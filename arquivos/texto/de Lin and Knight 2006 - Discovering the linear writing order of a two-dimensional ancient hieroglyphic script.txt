Artiﬁcial Intelligence 170 2006 409421 wwwelseviercomlocateartint Discovering linear writing order twodimensional ancient hieroglyphic script Shou Lin Kevin Knight Information Sciences Institute University Southern California USA Received 8 February 2005 received revised form 28 November 2005 accepted 6 December 2005 Available online 17 January 2006 Abstract This paper demonstrates machine learning methods applied deal realworld decipherment problem little background knowledge available The goal discover linear order twodimensional ancient script Hi eroglyphic Luwian This paper records complete decipherment process including encoding modeling parameter learning optimization evaluation The experiment shows proposed approach general recover linear order manually generated twodimensional scripts needing know advance language represent twodimensional scripts generated Since proposed method require domain speciﬁc knowledge applied language problems order discovery tasks domains biology chemistry 2005 Elsevier BV All rights reserved Keywords Ancient script Decipher Luwian Hieroglyphic Unsupervised learning Estimationmaximization Linear order Discovery Writing Natural language process 1 Introduction The Hieroglyphic Luwian script twodimensional script discovered middle Asia preserved rock dating 7001300 BCE 4 Unlike modern scripts possess clear linear order reading example leftto right topdown English Luwian symbols arranged regularly indicate speciﬁc writing order upper line Fig 1 This paper discusses general process discover writing order second line Fig 1 type twodimensional script So far convincing evidence precise order script A linguistic authority says leave doubt correct order reading 4 Up present day effort focused applying machine learning methods decipherment Knight Yamada 5 propose gen eral framework discover texttospeech relationships unknown scripts applying ﬁnitestate transducer model EM ExpectationMaximization algorithm learn parameters Sproat 7 claims orthography language represents consistent level linguistic representation mapping level surface spelling regular relation He proposes use rulebased realized ﬁnite state automata Corresponding author Email addresses sdlinisiedu S Lin knightisiedu K Knight 00043702 matter 2005 Elsevier BV All rights reserved doi101016jartint200512001 410 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 Fig 1 A snapshot twodimensional Luwian script upper sample linear order symbols lower encode regularities Both works aim ﬁnding relationships writing symbols sounds We seen computational approaches order discovery ancient writing including Hieroglyphic Luwian The challenges discovering linear order unknown scripts threefold 1 Modeling Without knowledge examples linear order abandon enlisting help supervised training rule writing This task difﬁcult asking somebody knows English ﬁgure linear order set English characters written twodimensional manner The challenge lies model problem handled machine unsupervised techniques 2 Complexity In general number possible orderings increases exponentially number symbols Hence face huge search space 3 Evaluation As kinds human discovery problems easily obtained linear data script makes hard verify results The section describes model linearorder discovery task unsupervised learning problem We provide intuition indicating proposed approach resembles human beings task In Section 3 model described Section 2 applied reﬁned deal real world problem Additionally discuss reduce search complexity polynomial We evaluation strategies results Section 4 conclude section 2 Modeling Our general strategy follows model way Luwians generated script Shannons noisy channel model 6 allows translate problem travelingsalesmanlike problem We apply EM algorithm learn parameters Luwian language model model Finally apply parameters compute associated probability plausible order order extract highest probability result 21 Noisy channel model We start exploiting noisychannel model shown Fig 2 We assume ancient people original linear script mind writing carve scripts twodimensional manner stone When time passed Luwians remains twodimensional scripts rocks observed Based Shannons model original linear script input X noisy channel The way Luwians wrote script treated noisy channel perturbs original onedimensional source text X twodimensional appearance Y This noisy channel represented PY X black box know twodimensional scripts generated Once S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 411 Fig 2 The noisy channel model representing Luwian people wrote script Fig 3 The twodimensional English bd Three arbitrary linear orders problem modeled noisy channel obvious orderdiscovery task represented ﬁnding X maximizes PY X This formula decomposed components PX PY X Bayes rule Eq 11 PX generated language model script essentially stands frequently script occur large sample ancient Luwian literature PY X treated noisy channel represents Luwian people wrote script Shannons model Bayes rule tells desired linear order X appear frequently Luwian literature high PX possess high chance producing observed twodimensional script Y 22 Linear order known language We like demonstrate idea described Section 21 toy example assume asked ﬁnd linear order known twodimensional language English Fig 3a told generated argmax X pX Y argmax pX PY X X 1 According Eq 1 need search order X maximizes PX PY X In example ignore PY X assume indication script generated Therefore need maximize PX In words whichever permutation sequence appears valid English plausible candidate People scramble characters mentally sooner later ﬁnd probably means good job Fig 3b permutation occurs frequently English speakers gdbojoo 3c goodjob 3d What happens human mind mimics procedure maximizing PX meaningful sentences possess higher PX meaningless ones This toy example demonstrates model extent reﬂects human beings solve puzzle A machine perform analysis knows English orders probable One way compute exploit ngram English language model easily obtained simple statistical analysis large English corpus Assuming English letterbigram probabilities given program machine enumerate possible orders compute associated probabilities pX pgoodjob pg po g po o pd o pj d pb j pb o p b After extract sequence highest probability result The probability 3c 3d lower 3b pd b 3c po 3d occur English In sense bigram language model known linearorder discovering problem similar traveling salesman problem graph nodes represent letters weighted links represent corresponding bigram probability letters salesman origin 1 In fact extra term PY Bayes rule deduction However PY ignored optimization process models probability observed output identical X 412 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 Fig 4 Applying EM reﬁne language model PX recursively 23 EM unknown scripts Section 22 shows possible recover linear order known script twodimensional form However approach applicable decipherment Luwian script language model unknown On hand order probability PX known possible apply method fractional counting generate ngram probabilities counting relatively times ngram appears possible ordering For example possible orders associated probability 20 30 50 bigram Pe s occurs twice respectively sequences fractional count Pe s 1 20 1 30 2 50 Finally language model constructed normalizing fractional count values Statistically speaking learning language model learning associated probability ordered sequences PX dual problems sense provide sufﬁcient information understand Unfortunately know Luwian In case propose use idea EM 2 learn models simultaneously shown Fig 4 For initialization assign certain probabilities uniform distribution ngram parameters language model In M step EM enumerate possible linear orders X use imperfect language model learned far generate associated probabilities PX In E step EM count frequently ngram appears possible orders X weighted associated probability PX generate reﬁned language model Then new language model exploited create precise PX It proven execute E step M step iteratively EM ultimately converge locally optimal solution parameter settings language model lead locally optimal PX Once language model learned ﬁnally apply compute associated probability order extract probable 24 Discussion In fact program similar asking human learn linear order script seen Why intuitively possible In section like provide insight showing proposed method generalization plausible decipherment process Thinking methods human beings use deal orderingdecipherment task ﬁrst try group symbols cooccur frequently neighborhood Based cooccurrence frequencies able hypothesize plausible order certain ambiguous areas corner areas Then propagate order information lessambiguous areas remaining areas reﬁne belief cooccurrence frequencies These steps applied repeatedly plausible order determined learn current hypotheses applicable remaining areas case need backtrack choosing hypotheses This decipherment process simpliﬁed version program The process choosing plausible direction based current beliefs symbols cooccur frequently corresponds maximization step expectationmaximization EM algorithm Using currently plausible order information modify existing beliefs resembles estimation step EM The major difference human beings small highly possible choices account machine able situations account furthermore weight possibilities occurrence better decision S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 413 Fig 5 The symbols faces adjacent rows face oppositely The power approach comes ability capture hidden subtle regularities Although program knows language beginning starts absorb information reﬁne understanding language perturbed scripts seen 3 Deciphering While Section 2 illustrates basic idea applying noisychannel model EM handle orderdiscovery problem section address practical issues faced implementing deciphering program 31 Assumptions In order simplify computation assumptions The ﬁrst assumption Luwians write symbol neighborhood current symbol The deﬁnition neighborhood symbols X Y twofold straight line center X Y touch symbol X Y adjacent columns c1 c2 c3 Fig 5 columns Under assumption jump symbol connect symbol This assumption plausible case writing systems2 With assumption complexity reduced On Obn b average branching factor average number neighbors Luwian Hieroglyphics symbol symbol average b candidates successor Furthermore linguists agree big picture external appearance follows socalled boustrophedon means alternate rows progress opposite directions plowing ﬁeld shown dotted line Fig 5 This hypothesis convincing symbols human animal shapes tend face different directions subsequent rows circled Fig 5 This assumption implies backward horizontally row In boustrophedon writing ﬁrst row lefttoright righttoleft We choose righttoleft experiment data clear starting point upperright corner script With assumptions reduce total number plausible orders O2k k total number columns Since external appearance known program determine precise progress direction locally The ﬁrst assumption implies choices topdown bottomup column plausible number orders O2k It exponential shown Section 34 assumptions allow apply forwardbackward algorithm fractional counting reducing overall complexity polynomial 2 There writing systems violate assumption example honoriﬁc inversion Egyptian writing 414 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 Fig 6 Digitizing Luwian script Fig 7 The graph representation data Fig 6 32 Encoding Encoding Luwian text reproductions friendly format manually There 9 rows separated rigid horizontal lines 753 symbols total 73 distinct page tested As shown evaluation section 753 symbols essentially determining order languages For symbol ﬁrst recognize assign symbol identiﬁer arrow carrot record twodimensional position shown middle table Fig 6 Once positions symbols determined generate relative direction symbol automatically use clock direction shown upper right Fig 6 The true input discovery program table recognized symbol identiﬁers relative directions neighboring symbols demonstrated Fig 7 The arrows plausible directions proceed For example way channel symbols S1 arrow S2 underscores Note link middle node node column connection S4 S1 progression inevitably violate assumptions Also cycle involving nodes different columns assumptions violated 33 Modeling weight links Once information recorded Fig 6 step generate associated weight automatically link sum weights path reﬂects possibility occurrence path We S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 415 models weights symbol Wn neighbor Wn1 borrowed modiﬁed described Section 2 The ﬁrst model 1 model weight bigram Luwian probability PWn1 Wn exactly Section 22 The limitation ignore noisy channel Luwians generated twodimensional script rely completely frequently sequence occurs literature To compensate limitation second model model 2 choose use PWn1 Wn PDnn1 represent weight PDnn1 direction current symbol The idea model assume ancient Luwian people preferred progressing directions preference independent symbols From decipherment point view model uses types independent information compensate weakness That symbols equally probable successors model choose preferred direction hand directions equally probable progressing model pick symbol plausible follow current For models apply EM algorithm learn associated parameters For initialization tried uniform distribution weights random assignments The results EM converges similar results initializations tried 34 Complexity As illustrated Section 31 assumptions complexity deciphering task exponential This EM phase need generate probabilities 2k different orders performing fractional counting The script experiment average 30 columns row unfortunately 230 computationally intractable However beneﬁt assumptions ease complexity higher order exponential Obn lower level exponential O2k We assumptions possible apply forwardbackward algorithm 1 fractional counting having list plausible orders reduces complexity polynomial The forwardbackward algorithm dynamic programming algorithm We propose use store alpha beta values link graph The alpha value aggregated probability starting point source link beta value aggregated probability destination link ending point In words alpha value link sum path probabilities reach link starting point beta value sum probabilities paths leaving link terminating ending point The fractional count link equal weight times alpha value times beta value divided alphaend alpha value node3 There types links graph One link progresses horizontally link S3 S5 Fig 8 link progress vertically link S5 S1 For link traverse horizontally alpha value inherits previous vertical link column Eq 2a shows alpha value link S3 S5 Although incoming links node S3 S1 S3 S2 S3 S4 S3 contribute alpha value link S3 S5 coming S1 S2 S3 node visit S4 instead S5 second assumption In words illegal progress S3 S5 nodes S4 column visited alpha value S3 S4 based alpha value S4 S3 Similarly beta value S3 S5 generated beta value S5 S1 link S5 S6 S5 S4 link second assumption tells moving S3 S5 node visit S1 S4 S6 Eq 2b Eqs 2a 2b tells horizontal links alpha value beta value independent dynamic programming condition holds Likewise links progress vertically S5 S1 alpha value weighted sum incoming alpha values previous column Eq 2c The beta value S5 S1 produced based beta value S1 S2 Eq 2d Eqs 2c 2d vertical link choice previous route alpha value affect choice future route beta value forwardbackward algorithm applied 3 Note long sequence computation alphaend value fractional count Since goal fractional count calculate relative occurrence frequency links necessary truly compute alphaend computation 416 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 Fig 8 How alpha value propagates graph AlphaS3 S5 AlphaS4 S3 PrS3 S4 BetaS3 S5 BetaS5 S1 PrS1 S5 AlphaS5 S1 AlphaS3 S5 PS5 S3 AlphaS4 S5 PS5 S4 BetaS5 S1 BetaS1 S2 PS2 S1 2a 2b 2c 2d The analysis shows given assumptions alpha beta values propagated ﬁrst order Markov manner interfering Once alphabeta values computed stored calculate speciﬁc link contributes associated bigram parameter multiplying transition probability alpha beta values The ultimate bigram probability accumulation identical bigram link probabilities Similarly fractional count PDnn1 table computed aggregating PDnn1 times alpha beta 35 Decoding As shown previous section alpha value depends neighbors alpha values assumptions Therefore EM converges apply dynamic programming extract path highest probability The output order generated model 2 experimental page shown Fig 9 In decoding phase modiﬁed dynamic programming algorithm bit recording best probability step secondbest probability Therefore dynamic programming algorithm ﬁnishes walking nodes graph order highest probability secondbest need evaluation 4 Evaluations Evaluation trickiest machine discovery problem The chickenandegg paradox arises sense evaluate results need know precise order hand aware precise order necessary pursue discovery In section indirect strategies evaluate results 41 Checking machines conﬁdence One method veriﬁcation check conﬁdence machine order picks We propose measure conﬁdence comparing probability best secondbest order returned machine In view fact machine makes decision based probability closeness best secondbest probabilities implies virtually machines point view This situation indicate lack data underﬁtting model Table 1 records best secondbest negative logprobability row model From table learn ﬁrst model bigram 29 rows distinguishable probabilities model 2 bigram plus independent direction rows distinguishable probability This implies S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 417 Fig 9 The writing order discovered second model 9 rows 753 symbols bigram information feel comfortable choice It gains conﬁdence direction factor considered Although evidence necessarily second model generate accurate results indicate provides sufﬁcient information machine conﬁdent decision 42 Checking consistency One typical way evaluate natural language learning program check learning curve data examine accuracy improved data added training In problem generating learning curve data possible gold standard answer available However draw similar graph results change new data introduced Fig 10 shows result alters data added line line For models similarity outputs remains steady reaches 90 7th line added implies stabilization 418 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 Table 1 The best secondbest log probabilities bold pairs distinguishable ones Rows row1 row2 row3 row4 row5 row6 row7 row8 row9 Models Model 1 best 149 193 165 193 160 150 157 178 189 2nd 150 194 165 193 161 247 260 179 190 Model 2 best 224 306 277 309 250 240 253 288 284 2nd 230 315 282 318 257 251 257 293 292 Fig 10 How changes data increases Fig 11 Ten different patterns generate twodimensional scripts 43 Evaluating methodology Instead verifying results directly section propose verify methodology checking works known languages writing systems The basic idea ﬁrst collect set meaningful sentences known languages secretly overlay symbols twodimensional Luwian scripts violating assumptions execute program recover original linear order This time know true order veriﬁcation We use 5 natural languages Chinese English Arabic Latin Spanish programming language Java testing We collected paragraph writing language We need 753 letters 753 characters Chinese tested Luwian script total 753 symbols To generate twodimensional scripts apply different patterns Fig 11 Note tenth pattern random follow rules Then replace symbols Luwian page letters paragraph exempliﬁed Fig 12 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 419 Fig 12 Replacing Luwian symbols English letters according pattern 1 Table 2 The accuracy 10 different writing patterns model Model Pattern 1 2 3 4 5 6 7 8 9 10 Avg 753 letters Latin 753 letters English 753 letter Arabic 753 letters Spanish 753 characters Chinese 753 letters Java Luwian Scripts Baseline 1 2 1 2 1 2 1 2 1 2 1 2 1 2 Random guess 73 74 78 77 77 78 76 75 84 85 85 84 86 84 87 85 79 98 76 97 77 99 72 82 87 83 85 85 100 100 91 80 87 84 89 88 79 77 88 76 76 77 78 79 89 77 82 79 85 87 74 72 84 98 74 76 73 71 84 72 78 78 79 77 68 66 79 95 76 75 76 77 88 72 75 77 80 82 75 73 82 99 65 60 70 66 76 67 68 72 71 74 67 67 73 87 82 85 87 86 84 100 100 90 84 83 87 88 86 83 84 89 52 60 54 52 52 54 57 65 44 45 63 53 50 54 80 50 50 50 50 49 28 50 69 66 56 50 56 56 67 67 61 55 66 50 50 85 76 50 75 72 72 76 50 85 76 85 50 50 86 75 76 84 76 These newly generated twodimensional scripts totally 60 different ones 6 different lan guages 10 different ways encoding serve input models generate linear results We evaluate results counting columns returned correct order The results shown Table 2 Note baseline randomly guess direction decision point reaches 50 patterns Table 2 shows models achieve better performance baseline writing patterns For case takes 1 minute program running 1 GHz PC converge stable result If script written according pattern 1 2 strong tendency directions second model discover order perfectly Chinese We believe perfect Chinese unlike Chinese script logographic number distinct symbols logographic writing higher alphabetic syllabic systems The left Table 3 describes PDnn1 learned model 2 pattern 1 This shows program learn fact prefers progressing 6 oclock direction 12 oclock direction Also learns distinguishable factor pattern 1 10 11 oclock direction preferable 7 8 oclock The right column Table 3 indicates pattern 3 model learned pr6 pr12 pr9 higher probability horizontal directions matches intrinsic characteristic pattern As patterns lessstrong preference directions model 2 reaches 80 accuracy languages Table 2 points model 1 accuracy ranges 70 85 Chinese depending language writing pattern This reasonable language model model noisy channel It tells script written randomly pattern 10 approach reproduce linear order accuracy higher 75 Next linear English written pattern 3 recovered model 2 While Section 2 iullartsets basic ideas combining nosihcy annel model EM handle orderdiscovery problem section uowl dleki dadress mose practical issues faced trying implement thed icephering program 420 S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 Table 3 PDnn1 learned model 2 Latin Pattern 1 pr6 0620 pr7 0001 pr8 0005 pr9 0030 pr10 0123 pr11 0224 pr12 0001 Pattern 3 pr6 0312 pr7 0020 pr8 0031 pr9 0259 pr10 0048 pr11 0024 pr12 0308 linear English written pattern 10 recovered model 1 difﬁcult challenge program human beings While Section 2 illuartsets basic edias oc fombining thon esihc yennal moled EM handle odrerdicsove yrorplb mein section uowl dlike address practical isusse faced yrting ti ompleme tnthed icerehping program The recovered scripts perfect understandable We perform similar human study giving human subjects4 twodimensional scripts written language know according patterns ask ﬁgure linear order The results samples sent ninety percent subjects perform better 50 The average accuracy 58 highest 73 far programs 79 average 97 highest score The result demonstrates people learn certain regularity data perform slightly better baseline better results task use EM algorithm powerful computational capability machine helpful The Table 2 demonstrates accuracy Luwian result assuming written according previous patterns5 For example written according pattern 2 results program generates reach 28 accuracy pattern 2 written bottomup manner discovery shows lines topdown It clear pattern 1 possesses higher accuracy 80 topdown model 2 This shows trust validity program conclude Luwian scripts written topdown After manually checking results shown Fig 9 bottomup writings come blurred area boundaries columns unclear Our theory Luwians general follow topdown manner writing However Luwian symbol different shape size situations desire better use space preventing writer following general rule Dr Hawkins addressed similar issues saying signs arranged vertical columns But signs come variety shapes long slender wide ﬂat problem means clear order signs read 3 The major contribution paper strengthen hypothesis general writing pattern topdown program provide plausible solution order ambiguous areas It ﬁnds regularities exceptions areas writing pattern abandoned hard task human beings Table 4 shows learned parameters PDnn1 certain bigram probabilities Luwian Hieroglyphic 5 Conclusion In paper unsupervised method discovering linear order twodimensional Hiero glyphic Luwian script We represent problem Shannons noisychannel model apply EM learn different parameter sets models Finally apply dynamic programming algorithm extract probable order 4 The subjects background including science computational linguistics psychology signal processing 5 Note sum probabilities opposite patterns pattern 1 pattern 2 100 It columns single symbol columns topdown bottomup direction regarded correct S Lin K Knight Artiﬁcial Intelligence 170 2006 409421 421 Table 4 The PDir bigram probabilities Luwian script pr6 0498 pr7 0009 pr8 0004 pr9 0146 pr10 0086 pr11 0138 pr12 0122 psmall triangle 025 pfour triangle 0249 pbuffalo triangle 0249 pswan triangle 0250 pcrocodile 0250 pdeer 0249 pthree 0499 We propose ways evaluate discovery problem We examine machine conﬁdence results produced comparing best secondbest results The results imply model 2 better ﬁt We generate consistency curve examine sufﬁciency data Finally evaluate methodology applying known languages The results program having pre requisite knowledge language twodimensional script generated recover linear script 80 accuracy As Luwian scripts experiment shows general written topdown identiﬁes certain exceptional areas The lesson learned modern computational power proper model machines capable grasping hidden regularities dealing tasks extremely difﬁcult human beings The contributions study ﬁnding writing order unknown ancient script demonstrate general unsupervised approach designed deal situation little background knowledge available learning Furthermore propose indirect general strategies evaluate discovery Since prior knowledge necessary approach potential applied natural language problems tasks related order discovery areas biology chemistry References 1 LE Baum An inequality associated maximization statistical estimation probabilistic functions Markov processes Inequali ties 627 3 1972 18 2 AD Dempster NM Laird DB Rubin Maximum likelihood incomplete data EM algorithm J Roy Statist Soc Ser B 39 1977 138 3 JD Hawkins Corpus Hieroglyphic Luwian Inscription vol I Walter Gruyter 1999 4 JD Hawkins The Luwians Scripts texts C Melchert Ed The Luwians Leiden 2003 5 K Knight K Yamada A computational approach deciphering unknown scripts Proceedings ACL Workshop Unsupervised Learning Natural Language Processing 1999 6 C Shannon A mathematical theory communication Bell Syst Tech J 27 3 1948 379423 7 R Sproat A Computational Theory Writing Systems Cambridge University Press Cambridge 2000