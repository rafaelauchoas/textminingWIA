Artificial Intelligence 77 1995 203247 Artificial Intelligence Reasoning connectionist nonmonotonicity learning networks capture propositional knowledge Gadi Pinkas Amdocs Inc 1161 Des Peres Rd Suite 170 St Louis MO 63131 USA Received March 1992 revised January 1994 Abstract The paper presents connectionist framework capable representing learning propo sitional knowledge An extended version propositional calculus developed demonstrated useful nonmonotonic reasoning dealing conflicting beliefs coping consistency generated unreliable knowledge sources Formulas extended calculus proved equivalent strong sense symmetric networks like Hopfield networks Boltzmann machines efficient algorithms given translating forth forms knowledge representation A fast learning procedure presented allows symmetric networks learn representations unknown logic formulas looking examples A connectionist inference engine sketched knowledge compiled symbolic representation learned inductively training examples Experiments large scale randomly generated formulas suggest parallel local search executed networks extremely fast average Finally shown extended logic highlevel specification language connectionist networks recent symbolic systems mapped The paper demonstrates rigorous bridge constructed ties opposing connectionist symbolic approaches 1 Intmduction Humans able reason surrounding world noisy incomplete knowledge remarkably high speed They astoundingly good infer ring useful reliable information conflicting beliefs knowledge selfcontradicting erroneous The work supported NSF grant R9008012 00043702950950 1995 Elsevier Science BV All rights reserved SSDIOOO437029400032V 204 C PinhArtxd Intelligence 77 1995 203247 decade AI realized analysis It reasoning mechanisms systems proposed formal models kind reasoning Some known examples logic 1531 circumscription task As result nonmonotonic 1321 default major Research nonmonotonic reasoning tried understand basic mechanisms rationale intuition dealing incomplete description world Recent nonmonotonic systems successful examples computational new situations symbolic deal exceptions 351 capturing intuitions IO 621 Most systems plagued intractable complexity intuitions The able revise rigid specialized develop personal constrained rules fuzzy approximate aspects knowledge knowledge noise inflexibility sensitivity adjust approach link They provide fast noise learn dynamically change systems performance accelerate achieve Connectionist representation traditional systems missing tolerant adaptive platform ability knowledge base robustness familiar situations While scientists powerful knowledge erful learning mechanisms cognitive tems learning efficient symbolic computations fuzzy heuristic naturally mapped adaptive capabilities sufficient adjusting Clearly learning procedures like compositionality expressive power This article concentrates symbolic AI concentrating systems connectionists concentrating adaptation mechanisms Connectionism criticized systematicity essential tasks easy symbolic approaches perform 8 We fast development pow lacking highlevel like sys capable 161 Once hope approach systems ultimate goal scientific approaches representationally powerful connectionist platform connectionist integrated expressive rigid symbolic approach somewhat neglected problem symbolic knowl studies networks networks edge representation represent learn unrestricted propositional One big difference connectionist connectionist In particular rules reason Connectionist control mechanism included networks symbolic knowledge process information represen expressed networks inter knowledge representation connectionist procedural knowledge systems need interpreter symbolic tations representation preter The interpreter represented We strive capable representing needed controlling information reasoning process different connectionist models Among I choose consider metric matrix weights This family models Boltzmann machines 171 harmony theory includes Hopfield networks field theory 63 mean sym 2021 J 151 Following Marvin Minskys synthesizing symbolic connectionist approaches 35 I The approach extended represent predicate logic I43 1 G PitusArtificial Intelligence 77 1995 203247 205 selecting symmetric connection networks SCNs variations The reasons following 1 7 networks energy characterized networks networks behavior functions express approximate symmetric easier specify symmetric 221i3 symmetric 42 works power restrict recent successful heuristic seen sequential variations symmetric paradigm net powerful lose expressive case 4 networks capable representing 33571 similar structure large set asymmetric hard problems symmetric repair techniques 2 3 4 Ideally like wide range logical satisfied paradigms mapped Also capable describing language networks connectionist nonmonotonic formal declarative network Such highlevel declarative programming level abstraction implementation My purpose connectionist highlevel article formalisms beneficial knowledge representable general encapsulated language specification It intermediate lowlevel neural cognitive processes networks 1 propositional knowledge 3 SCNs formulas looking examples truth nonmonotonic encapsulated logic 4 SCNs inference mechanisms 2 knowledge unknown propositional SCNs captured naturally SCN described extended version propositional learn representations assignments able capturing information procedural SCNs compared satisfiability knowledge control finally 5 algorithms bestknown formulas satisfy embedded favorably logic formulas algorithm logic propositional The paper energy paradigm organized proof usefulness connectionist following way Section 2 presents penalty theory The section demonstrates In Section 3 symmetric reasoning logic new networks reviewed Section 4 defines equivalence penalty proves strong equivalence logic SCNs logic Section 5 sketches semantics logic nonmonotonic introduced forms knowledge logic formulas SCNs It shows represent sentences penalty SCN described connectionist inference engine learning algorithm formulas performance approach Section 8 discusses discussed Section 6 discusses unknown propositional inductively Section 7 reports experiments enables SCNs learn representations related work Section 9 concludes uses representation performed evaluate representation extended s The TSP experiments Hopfield Tank 22 criticized Wilson Pawley 67 Hopfield architecture provided better energy function modifications later formulations encouraging results 4 Sometimes asymmetric consider restrict 22339 form symmetric network perform better efficiency symmetric case 206 G PinkusArtcial Intelligence 77 1995 203247 2 Penalty logic 111 extend propositional calculus order useful nonmonotonic Later calculus mapped reason SCNs logic networks revealed ing coping inconsistency strong relationship The extended calculus capable expressing variety interpretations adding real positive number knowledge penalty assigned likelihoods represent certainties 111 When entropy constraints represent measure reliability penalties penalties explicitly 5 I insist particular use interpretation intention reduced strength belief reliability sources belief This numbers 6611 priorities 3291 maximal knowledge penalty 5 I Note systems compute develop general framework possibly variety interpretations penalties logicist systems information systems penalty example sources unreliable let user specify explicit 21 Extending propositional calculus Definition 21 A penalty I finite set pairs Each pair composed real positive number called penalty standard pi 40 1 pi E lR pi propositional WFF formula called assumption logic formed belief PLOFF formula denoted 24l pi j pi pi E I The set beliefs n Example 22 The Nixon diamond stated 1000 N R Nixon republican 1000 NtQ Nixon quaker 10 Rt7P republicans tend pacifist 10 QP quakers tend pacifist 3000 N person reason Nixon An illustration The set beliefs example shown Fig I inconsistent penalties example proposition High penalty like states Nixon reflect strength believe republican We usually knowledge base like example given strict logic rules facts allow strict facts defeated The fact N states Nixon reason This fact receives evidence The evidence jump corrigible tend temporary rules tend pacifist Lower penalties given defeasible like states republicans highest penalty given The evidence case considered certain considered rules conclusions infallible s The penalties acquired learning subjective intuition captured G PinkasArtial Intelligence 77 1995 203247 201 Fig I An illustration Nixon diamond inheritance network nodes represent atomic propositions numbers penalties republican tend believe jumping conclusion blocked person know rule For example wouldnt like conclude republican explicitly mentioned pacifist Clearly Nixon Q like conclude like conclude adequate reason believe P default exception When know somebody pacifist person quaker person republican P P considered pacifist R quaker pacifism Nixon There person If want express ambiguous belief religious ideas stronger penalty political affilia Q P 15 tions influencing penalty leaving ones pacifism R TP unchanged increase 10 Example 23 1000 N R Nixon republican 1000 NQ Nixon quaker 10 15 RyP QP 3000N republican tend pacifist quakers tend pacifist person reason Nixon In revised set assumptions competing arguments One argument supports Nixon competition disagreeing ambiguous winning argument 301 pacifism Nixon supports case argument stronger argument negation The pacifism supports P wins defeat manages 22 Model theory There ways interpret I shall interpretation penalties assumptions convenient general formalism Given knowledge base Cc pi pi PLOFF 9 determines set possible models truth assignments n atomic propositions ranking This ranking 208 G PinkasArtijicial Intelligence 77 1995 203247 satisfy models better models goodness tend violated Two models equally good Even 601 By specifying mean informally assumptions associate possible models reflects normality satisfy world important fewer important assumptions Every models compared looking assumptions Q considered sum penalties sets sets assumptions models equally good A model normal model sum penalties violated assumptions sum penalties violated second For motivation 611 violate models violate different intersecting set assumptions summing penalties better This interpretation penalties possible models The ranking induces ranking function function assigns real value called induced rank violation rank 4 assigns rank truth assignments The Vrunk truth assignment Definition 24 The violation mnk PLOFF I function realvalued 2 computed summing assignment Vrunkk 2 cii p Cc violated assumptions penalties Vrank preferred Definition 25 The models models miny Vrunk y3 Vrank 2 The set preferred models denoted rl Vrankg function called minimize Definition 26 Let p 9 PLOFFs PLOFF preferred models 4 preferred models p r C r I semantically entails 40 rC k D iff Note sentence entails o iff model minimizes violation rank minimizes violation rank 9 In Nixon example preferred models N R Q P Examples valid conclusions conclusions ambiguous P holds preferred model 1P holds satisfied preferred models The pacifism Nixon N R Q P N R A Q 23 Merging PLOFFs evidence background knowledge The operator merge 6 metalanguage propositional logic It allows combine plays role A AND PLOFFs simply merging classic Definition 27 The merge operation 6 defined lcIl ii 2 rcII rcI2 u 2 1 U 2Pi Vi 1 pi9 Pi E I fl442 A Score number actually computed function hank This score later determine relative rank truth assignment G PitasArtificial Intelligence 77 1995 203247 The meaning merge PLOPPs appropriate Vrank functions The reader check simply obtained adding Vrank b Vrank Vrank The merge operation allows incremental update knowledge 209 Later equivalence networks logic formulas property allows add delete adding network updates occur relevant energy deleting terms There PLOPF established existing network new need recompute Nonmonotonic systems jump conclusions based given evidence conclusions knowledge based new evidence want reason retract decompose evidence easy way combine evidence In formalism simply merging functions ranking 10491 The background knowledge relatively evidence background adding It convenient background knowledge fixed evidence combining later Definition 28 Let tb e q PLOFPs Evidence e entails CY respect ground knowledge e cp iff 6 e b 9 The consequence relation induced I set pairs ecp e b p One special case definition evidence strict defeated certain This special case useful evidence validity evidence set Z consistent higher practically combination penalties infinite agent draws conclusions assigned lo To represent strict evidence e penalty reasoning validity systems based absolute logic assumptions background beliefs penalties Definition 29 Strict evidence consistent beliefs cc represents large penalty larger combination background PLOFF e ei set U ei Example 210 In Nixon diamond following beliefs considered background 1000 N R Nixon republican 1000 N Q Nixon quaker 10 RyP republicans tend pacifist 10 QP quakers tend pacifist The fact 3000 N strict evidence Another example strict evidence triggers example triggers 3000 Q conclusion Q A R conclusion TN Penalty new evidence logic nonmonotonic conclusions added In example evidence need retracted 210 G PinkosArtificial Intelligence 77 1995 203247 quaker conclusion Nixon need retract conclusion pacifist If addition add evidence Loyal goal genera1 possible 111 restrict evidence strict Such generalization phenomenon evidence noisy agent believe highly reliable evidence eyes facts background knowledge obtained sensory devices mere encountered formality practical direct uses Defeasible For example redundant conflicts applications unreliable evidence As evidence follows ambiguous Penalty need strict Most symbolic conclusions strict proposition treat follows background knowledge stated penalties Thus conclusions arrive example queries conclusion negation PLOFFs noisy channels The query wish prove redundant exactly evidence background knowledge Thus selfcontradicting background definition evidence knowledge preferred models query unreliable iff preferred models logic allows conclusions prove query systems 24 Proof theory A sound complete proof theory shown penalty based solely syntactic considerations process penalty logic gives clarifying logic This proof theory look reasoning Instead ranking rank consistent consistent preferred consistent subsets models best models reasoning process subsets assumptions use best preferred perform deduction A conclusion proof theory iff subsets entail Definition 211 A set T called theory PLOFF Cc iff T consistent subset assumptions set T C ZA satisfying model Definition 212 The penalty function theory T function obtained summing penabti CT CvEd P penalties assumptions included T A ranking induced set theories CI This ranking computed summing penalties missing assumptions Definition 213 A preferred function q The set preferred minspenal S 1 S theory T theory theory y3 theory T minimizes penalty theories 9 T T penaltyT Definition 214 Let rc q PLOFFs q let Tq T set preferred let T E set preferred theories 4p We entails7 theories rp Note deductive closure preferred definition entailment penalty logic resembles theories roughly resemble extensions entailment intersection extensions 531 The G PinkusArtcial Intelligence 77 1995 203247 211 denoted b rp iff preferred disjunction preferred theories rp theories c entail classical sense As special case consider case conclusion sp strict rp consistent rp iff preferred theory sense entailment propositional formed formula A PLOPS entails Cc entails cp classical In Nixon example 25 2 nonempty If rank consistent beliefs preferred T2NNQNRR belief IP These preferred IO missing I strength assumptions set subsets belief missing penalties missing theories TI N N Q N R Q P theories ranked 10 subsets summing theory conflicting inconsistent consistent Each preferred P 1P concluded The reasoning process intuitively subsets The subsets entailed Well need winners conclude lemmas theories entails obvious conclusions preferred understood competition like N Q A R theories agree consistent win theories minimal penalty A conclusion independently proof theory sound com plete Lemma 215 Let T C L consistent subset assumptions The subset T maximalconsistent model satisfies T violation rank x b T equal penalty T T maximalconsistent subset iff VZ penalty T Vranke 2 If T maximalconsistent assumptions assumptions Proof T Also assumption Therefore assumptions missing penalty T Vranke 2 T equal T maximalconsistent subset left violated model x satisfies T assumptions consistent T T maximal violated model x satisfies T T model x T set violated I Therefore set assumptions subset Assume model x satisfies T Vranke 2 penalty T If T T rank x T subsumes violated maximalconsistent assumption model x satisfying T new assumption The violation lower penalty T set assumptions included set assumptions 3 Vrank 2 penalye penalty T Vranke 2 violated x contains assumption This contradiction assumption included 0 8 A subset T maximalconsistent set consistency assumption added T preserving 212 G PinkasArficiul Intrllipm 77 1995 203247 The reader observe preferred penalty preferred LIti satisfying models This allows use prooftheoretic instead modeltheoretic theory I maximalconsistent theory equal violation function ranking subset rank pen Vrmk function relationship preferred models preferred The lemma establishes theories Lemma preferred 216 A model xf u preferred model PLOFF Cc iff model 2 satisfies theory Proof If x preferred model ti composed assumptions violated x exactly penaltytiT penalty T penalT Vrunk q penaltyd T conclude But T preferred included minimizes Vranke Let T set Vrunk 2 theory T v satisfy T theory By Lemma 215 models exists preferred T deduce Q satisfied X Since assumptions Vrunk q penaltyti CT penalty T Vrank 2 This contradiction minimality Vrunk 2 If model preferred theory T c T minimizes By Lemma 215 Vrank 2 penaltyT penalty function maximalconsistent preferred model Ic preferred model Vrank 2 Let T set assumptions penaltJti T Vrank 7 set assumptions set assumptions If x v Vrank j fl satisfied j The set T violated v equal T Therefore included penalty T Vrunk j7 Vrunk 2 penalty T contradiction minimality penalty T U Theorem 217 The proof procedure sound complete b p iff 14 I p Proof If I b rp preferred model preferred model p Based theory T lemma 216 preferred model 4 satisfies preferred model t4 satisfies preferred satisfies lemma 216 model disjunction theory T Cc preferred model 9 satisfies preferred theories p T I VTiETq T We conclude satisfies theory p Therefore theories p From preferred preferred preferred disjunction therefor IJ CD If IJ I p model satisfies preferred theory T satisfies theory T 9 From lemma 216 model satisfies T preferred preferred model cp model Based lemma 216 preferred model T rC satisfies preferred preferred model p le c r We conclude satisfies T preferred model cp theory kP 0 G PinkasArtificial Intelligence 77 1995 203247 213 This sound complete proof mechanism theories knowledge base defeasible competing useful reasoning For ex assume minority observations minority assumptions dealing inconsistency ample detect inconsistency usually want adopt theory maximum cardinality In logic penalties theories win maximal deed penalty defeated Thus minimum penalty cardinality maximal means maximum cardinality sources For theories decide defeasible conflicting set arguments A A1 supported better support A set arguments Al defeats conflicting theory theories systems principle reasoning sets arguments useful coping noisy knowledge logic generalization discussion argument notion conflicting cardinality Penalty erroneous Intuitively 30621 Example 218 Two levels blocking 31 1 10 meeting I usually Monday meeting sick lmeeting If Im sick I usually dont meeting 100 coldonly meeting If I cold I tend meeting 1000 coldonly sick If I cold means Im sick assumptions consistent assumption However given falsify meeting coldonly competing assumption infer evidence second theory theory drawn despite assumption If include evidence coldonly previously won loses new winner second assumption As result conclusion meeting evidence true Without additional meeting sick true prefer theories assumption greater penalty wins include theory include fact sick concluded 3 Symmetric models energy functions This section reviews symmetric paradigm Later relationship connectionist models energy minimization logic paradigm penalty 31 What symmetric networks SCN network connectionist A symmetric characterized weighted undirected graph nodes represent processing units arcs represent weighted Fig 2 There kinds arcs pairwise arcs link nodes nections monadic threshold weighted connection reverse sign given stored symmetric matrix diagonal bias single unit The weights connections zero The value j position single node A pairwise arc represents arcs attached wij monadic arc represents 214 G PinhsArtijicd Intelligence 77 1995 203247 output unit unit weight unit unit j equal weight directs weight connection matrix represents j The matrix symmetric unit j unit w w An SCN viewed searching called energy Each unit asynchronously adjust activation value energy decreases gradually reaches equilibrium computes settling local global minimum global minimum quadratic function gradient 9 function lo The network eventually There direct mapping networks quadratic energy functions appropriate network function minimize Given function construct given network generate variables function map nodes graph hidden variables mapped hidden units visible variables mapped symmetric energy term form 0x minimized The visible units Each node connected unit j weight w iff nonzero bias 8 term form viewed threshold WXXj A unit 0 function arcs units Unit tries minimize iff energy connected includes includes function A network fully specified energy function remainder paper functions The terms networks energy I distinguish interchangeably Fig 2 A symmettic 5TWSNR network represents function E 2RN 2NT XT 2WT WN 32 Activation functions Each unit network computes activation value follows The unit computes neighbors gradient energy function reverse sign weighted sum inputs Xi zero receives net The weighted sum inputs minus threshold actually partial derivative dEdX E energy In stochastic models noise introduced energy decreasing times function G PinkasArtcial Intelligence 77 1995 203247 215 The sum input activation nondecreasing function F usually nonlinear Xi Fneti task change connectionist models different viewed Some popular symmetric models described activation value according activation performing form gradient descent energy following landscape subsections energy steepness Different functions The network 321 The discrete Hopfield model The discrete Hopfield model zero The activation 20 uses binaryvalued function F units activation values Xi 1 0 ifWtiO This model searches corners hypercube corresponding units The discrete Hopfield model finds local minimum quickly escape possible values local minimum shallow network able deeper minimum times 322 The analog model Hopjield Tank 21 In Hopfield Tank networks activation values continuous zero search takes place interior hypercube By beginning network better near center cube searching gradient descent chances finding global minimum There guarantees global minimum good results reported implementation Tank use analog circuit slightly modified problems Hopfield function energy Ri input resistance unit gs sigmoidal function gain A 1 gs 1 e2As inverse g At high infinite g lie corners search space locations discrete Hopfield model The discrete analog models collapse infinite gain gain minima Fast average exponential worst case 241 216 G PinkusArtijiciul Intelligence 77 1995 203247 323 Boltzmann machines The Boltzmann machine important difference starting energy gradient neti determine high temperature activation 171 binary units discrete Hopfield model The annealed lower temperature The unit adopts state rule stochastic slowly cooling probability PXl 1 enetlT T temperature likely decrease monotonically frequency minima concentrating Boltzmann machine minimum schedules time exploring spending theoretically 9 annealing With stochastic adopt low energy states temperature cools The energy uphill randomly It search high temperature decreasing temperature wide range possibilities previous models rule network instead time deeper minima run long temperature A lower global guarantee easy sure annealing practice An annealing schedule designed time resources obtain providing anytime minimum probability lit given time quota Given bound lowest temperature bound global end time quota time given deeper minima finding global solution answer The guaranteed increases 324 Deterministic Boltzmann machinesmean jield theory A mean field method suggested Peterson Hartman 38 appears reduce time excessive The method unit encoding Boltzmann machines based deterministically wasted stochastic hill climbing probability Boltzmann approximating probability activation function X tanh F 1 times faster stochastic process stochastic Peterson performed field annealing Mean process lo30 somewhat better results mean field annealing designed desired anytime property similarly annealing Boltzmann machines Anderson Boltzmann machines reported deeper minima As Boltzmann machines providing fit time resources procedure 325 Heuristic repair methods Recently repair methods based local search proposed search problems The distributions 3357 problems techniques shown constraint successful satisfaction nqueen NPhard large scale hard scheduling 3SAT In methods distance function minimized current state goal measured local search Each variables problem G PinkasArtifcial Intelligence 77 1995 203247 217 checked change distance connectionist Heuristic effect changing distance reduces value distance executed When energy function function Usually taken sequential variation function heuristic repair considered algorithm implemented symmetric networks repair methods function formalism proposed sum weighted atomic problem variables nodes networks highorder energy function implement article The distance violated constraints propositions 33 Highorder energy functions To represent order connections convert Highorder arbitrary logic formulas network need power high hidden units This section reviews highorder networks shows networks introducing new hidden units networks sigmapi units pairwise standard connectionist nections Symmetric networks easily extended Naturally networks viewed minimizing 551 multiplicative handle highorder connections functions highorder energy 561 A korder energy function sum products product denoted function E 0 1 R expressed terms k variables A korder energy function Exx c 1 ilizitn WilitXil f k Quadratic order case energy functions secondorder functions special cases high c WijXiXj 1 ijn C WiXi In highorder model node assigned sigmapi unit updates value computing activation value accordingly partial derivative energy function activation update W dE wili c Illk jkij2i rI Xij 9 Ui FtMti ai Fneti extend standard update In discrete Hopfield model rule unique example Fneti model wish 1 neti 0 21x G PinknsArtrciul lntellrence 77 1995 203247 Fig 3 hypergraph k Frret 0 A highorder network translated k nodes The arcs directed order node arc weight arc weight determined weight corresponding opposite sign As quadratic case translation forth k symmetric highorder networks korder sigmapi units order energy hyperarcs connecting term energy functions function Fig 3 A cubic network u cubic hyperarc represents I NSW 2RN WN W S R N sigmapi units equivalent network Fig 2 hidden unit T We arbitrarily divide variables energy function variables hidden variables The hidden variables correspond network visible variables correspond hidden visible variables I visible variables Trepresents hidden variables visible units An energy function denoted usually function Ex sets visible hidden units zeros ones visible variables called visible reached considered state represents An assignment The values visible units equilibrium answer network Later article ables viewed atomic propositions 9s false Ill interpret visible states truth assignments visible vari 1 interpreted true 0 interpreted We set minimizing vectors projected visible variables visible solutions minimization problem 1T SEft minEJr I like Boltzmann machines harmony global minimum corresponding Models searching spurious memories exist In general local minima considered undesirable phenomena article ignore meaningful theory mean field theory viewed energy functions Local minima performance network This represent global usually cause degradation local minima knowledge I2 Several global minima exist energy level ED Ill algorithm G PinkmArtQicial Intelligence 77 1995 203247 219 Definition 31 Let E symmetric designates function ErunkEZ minyExy3 hidden variables The characteristic network energy function Ex 8 function network energy level obtained hidden units free The ErunkE function defines state values ErunkE units independent possible networks characteristic uses characteristic energy visible states The energy visible state visible units clamped reached This hidden topology original network There function The section equivalence different networks settle minimum networks behavior independent exact characterizes function function 34 The equivalence highorder networks loworder networks The following We energy functions subsection review results reported strongly equivalent 40 corresponding characteristic functions equal constant difference El M E2 iff ErunkE Erunk strongly equivalent set global Erunke c Networks minima similar energy ordering visible states st s2 visible states ordering means Etst iff Ezs1 IQ induce landscape convert highorder network strongly equiv alent loworder additional hidden units In addition energy function strongly equivalent hidden variables hidden units These algorithms network eliminating trade power sigmapi units computational versa As result expressive power highorder networks loworder networks hidden units higherorder allow simple units vice converted additional possibly Readers interested subsection They mind constructions directions possible technical details constructions skip Theorem 32 Any korder term w nt Xi NEGATIVE coeficient w replaced terms cf_ 2wXiT quadratic energy function additional hidden variable T Any korder term w nt Xi POSITIVE coefJicient w replaced terms 2k 1 wT generating strongly equivalent wXi 2wXiT 2WXkT2k_3WT generating strongly equivalent energy function omer k 1 additional hidden variable T 220 G PinkasArtijzciul lntellience 77 I 995 203247 The proof appears 44 1 Example 33 The following converted T T term XYZU It quadratic energy function additional hidden variables 4order energy function 4order XY XYZU XYXZ2XT2W2Z72UT5T 5 XYXY2XT 2YT2ZT3T2XT2IT2ZT 2UT 5T 2XT2kTT2ZT3T2X72YT_2ZT2UT5T The symmetric transformation nating subset variables eliminating hidden variables loworder possible highorder functions course interesting elimi To eliminate T bring energy function form E E oldterm oldterm C wi _ X T Consider assignments S variables X Xi Xi oldterm including T 0s CF w ni xl 0 Let Xi SX 1 1 Xi ifSXi O L expression 0 S The expression Li determines X 1 X depending variable assigned 1 state S expression disjunction represents new function E newterm T states cause reduction total energy The E oldterm include equivalent Example 34 Let T hidden variable eliminated ABfTACTA2TBTABTACA2B I The following assignments A B C cause p zero POOO 13 PO0 PlOO 2 I 13 PlOl 1 The new term equals G PinkmArtiJcial Intelligence 77 1995 203247 221 IAlBlC lAlBC 2Al Bl C A1 BC ABCABACAB1 Therefore ABTACTA2TBT ABC2ABACAB 4 The equivalence penalty logic energy minimization This section defines equivalence different forms knowledge representation relationships use rankedmodels penalty logic SCNs semantics use definition 41 Reasoning ranking functions A ranking model normality function set models function assigns real value rank set The ranking model considered grade goodness model As saw previous function ErankE Similarly function global minimum viewed ranking characterizes subsection ranking function function SCN E characterized equal ranking energy SCN I3 The search performed SCN search model highorder minimizes Penalty resentations mechanism logic formulas ranking independently classical functions logic WI It useful SCNs interpreted rep define reasoning knowledge representation form Definition 41 Let W 0 1 set models defined set n atomic propositions A ranking function k W R function reals A ranking large positive number A preferred model x ranking minimizes k kT minaky function k strict iff domain k 0 oo 00 represents function k model maps models The set preferred models k denoted rk Definition 42 Let f k e ranking rf f kebf entailed background knowledge k functions f entailed k k k f iff rk C iff evidence e e bk f The consequence relation induced k set pairs e f e f The modelbased reasoning mechanism consistent definitions defined logic Vranke taken ranking penalty Section 2 function IR There guarantee polynomial n number visible variables size network represents arbitrary ranking function 222 G PinkasArtciul Intelligence 77 I 995 203247 42 Calculi ranking functions rank Our step symbolically ing function This subsection defines function shows equivalence Sentences languages interpreted ranked models semantics representation allowed basic properties preserved knowledge languages knowledge encapsulated transformations describing ranking The following representation definitions meaning establish relationship form knowledge sentence language L The function ms function triple Definition 43 A calculus possible models m L k 1 k ranking ranking interpretation s x b s entails sentence m s Similarly interpreted addition corresponding s s k s ranking background sentence combination iff x preferred model ranking C m M L language M set assigns called s Let s s e k E L model x preferred model s function function function ms A sentence ranking sentence evidence entails function sentence e kk s iff ranking functions function ms mk b ms The consequence relation k set pairs e s 1 e b s Both classic predicate logic propositional logic viewed calculi languages strict ranking functions Example 44 Propositional propositional formed H x given formula s 03 represents large positive teristic function WFFs recursively defined L m 0 I L language oo 1 real H x charac ms outputs function formulas calculus WFFs X9 1 HX s X atomic proposition s G HT Kg x H s 31 A 2 H HX Hs 2 x H s st V 32 The reader easily observe function satisfy returns 0 truth assignments propositional WFF describes strict ranking satisfy WFF OCR assignments Example 45 Penalty logic calculus Cc m 0 1 m Vrcmk Definition 46 Let s E LI s E 1s sentences possibly different L m M L m M define kinds equivalence relations calculi G PinkasArbjkial Inrelligence 77 1995 203247 223 s s MS s iff corresponding ranking ms ms c We functions sequivalence s s 9 s iff associated ranking VT y ms equivalence preference functions ms induce iff y3 preserving 1 2 3 magnitude preserving s strongly equivalent equal constant difference equivalence s pequivalent ordering set models m s x m s 3 We pequivalence s weakly equivalent equivalence sets satisfying models minima preserving s s MW s iff corresponding ranking Tmcs Tmrst We functions wequivalence Observations 1 If background sentences strongly equivalent sentences iff m s m e 1 mc e corresponding s zS s evidence mc meaning Boltzmann machine induced consequence 5 entail set conclusions given evidence k c ms e conclusion Therefore relation associated strongly equivalent In addition ranking sentences probabilistic preserved function sentences entail 2 If background sentences s xp s conclusion mc If sentences set direct conclusions guarantee We guarantee property 3 set conclusions pequivalent strict evidence e 0 iff m s e k dame c ms e b mc property nonstrict evidence s s weakly equivalent ms b mc sentences iff m s b mc We entail hold try add evidence The reader easily observe sentences strongly equivalent pequivalent If want pequivalent weakly equivalent preserve set conclusions achievable minima strict evidence preserving preference If like able combine perform transformations knowledge use transformations preserve alence formed knowledge need need magnitude bine evidence probabilistic Most transformations strongly means constant difference We define equivalence Strong equivalence functions equivalent ranking calculi preserving strong equivalence transformed reminder paper magnitude interpretation forms knowledge piece weak equiv trans transformations We want com knowledge preserving representation induced representations Definition 47 A calculus Cl C 0 l m spw equivalent calculus 224 G PmkusArtiJzcicd Inteilqence 77 I 995 203247 C L 0 I m iff s E L exists spw s E L s E C exists spw equivalent equivalent s E C We use language C represent ranking language C vice versa In sections calculi function repre come I shall present knowledge embedded sentable equivalent SCNs 33 Some examples qf equivalent calculi energy functions Example 48 The calculus energy functions sumofproducts ing ranking E products mE ErankE Two special cases particular quadratic variables functions The calculus qf energy functions set strings calculus highorder describ E 0 l m functions written sumof calculus functions hidden The algebraic notation viewed language representing functions energy energy strongly equivalent functions hidden variables hidden variables We conclude In Section 34 algorithms given 1 convert highorder energy functions i4 loworder ones additional hidden variables 2 convert higherorder strongly equivalent energy calculus ones calculus highorder energy functions hidden units strongly equivalent functions Thus use language highorder energy functions quadratic hidden units symmetric connectionist SCN arbitrary number hidden units vice versa Note calculus SCNs language describes graphs weights thresholds course strongly equivalent calculus quadratic energy functions possibly network calculus calculus Example 49 Propositional propositional weak equivalence The energy function E algorithm 1 conjunction equivalent In 1401 I showed quadratic energy minimization obtained satisfiability I claim following 9 subformulas adding additional hidden atomic propositions new propositions For WFF Is This Convert variables naming binary subexpressions formula exampleAVBVlCDVEisconvertedintoTttAVBAT2H TVXAT2DVE result Assuming Ci H7Ps 3 H Convert loworder procedure Section 34 form A pi energy function characteristic function defined result cubic terms quadratic ones highorder computed Example 44 2 3 _ In papers concerned weak equivalence easily shown strong equivalence holds I5 In contrast familiar MAT connectives subformula limited disjunctions literals G PinuzsArtcial Intelligence 77 1995 203247 225 The global minima energy calculus highlevel 1 algorithm exist WFF Propositional quadratic energy functions However function equivalence set satisfying models evidence added probabilistic interpretation function exactly equal satisfying models calculus weakly equivalent SCNs language 40 converts energy function satisfiable WFF generate exponentially WFF energy long WFF 2 limitations preserved It means weak 44 The equivalence penalty logic SCNs This section shows penalty logic formula represented efficiently penalty logic formula logic SCNs strongly equivalent Every penalty SCN SCN described eficiently When PLOFF strongly equivalent network described energy function E 1 The set global minima E equal exactly set preferred models cp 2 Both knowledge representations induce s iff ErunkEs ErankEs order possible mod iff VrankJ s s better els Vrunk s 3 Knowledge update merging The equivalent operation energy new PLOFF network new piece knowledge cumulative An addition subtracting energy space adding knowledge base new PLOFF existing terms old The update representing energy deletion function modular simple 441 Representing penalty logic SCNs Theorem 410 For PLOFF Cc pi pi 1 1 n exists strongly exist constant c equivalent quadratic energy function Ex Vranke ErankE c The size network generated E order length I number symbols J following procedure Construction We construct E rC 1 Start set assumptions For pair pi cp create new hidden variable Ti cpi Ti cf pi add pairs 7 pi pi Ti The penalty 00 represents real value large force satisfied The original penalty pi causes 7s compete c pi holds CI strongly holds equivalent hidden variables 2 Construct J TiS considered energy function ci OOE cj pjTj Eq function high penalty 00 guarantees naming winning constraint theories generated algorithm described Example 49 226 C PinkasAriciul Intelligence 77 I 995 203247 Proof To Vranke ErankE c If hidden units T E free settle minimum minimum value c function setting 7 true pi satisfied false p violated Therefore clamping visible variables EOO obtains ErankE I e PC c p FI v FI I Ib pI c Vrank c 0 c 7tiq I function independently energy The naming step contribute number variables atomicity expect Thus needed assumption CD greater If case pi second step algorithm generate triple Each triple triples penalty constraint ranking generated wished The high penalty use function naming solutions constraints Once guarantee needed number variables guarantees zero penalty satisfied constraint preserved generated Thus constraint p triple happen satisfied energy function 7s compete original assumptions When satisfied splitting atomicity equal way construct naming triple penalty naming satisfy constraints causes The network generated search preferred sound complete proof theory seen seen performing theory IJ Ts win competition correspond model 9 According searching assumptions preferred preferred theory In following example assumptions variables naming needed Example 411 The Nixon diamond case Example 22 The PLOFF converted 9 300ON1000N t Q 1000 N R 10Q P 10 R 4 No naming needed 9 9 Each pairs converted energy function 1000 N f R 1000ENvR lOOO N NR 1000 NtQ 1000hQ lOOO N NQ 10 R TP 10E_R lORP IO QP lOFvp lOQ QP 3000 N 3000 EN 3000N G PinkasArtcial Intelligence 77 1995 203247 227 Summing energy terms E 1OOONQ 1OOONR 1ORP 1OQP lOOON 1OQ The corresponding network appears Fig 4 Fig 4 The network represents Nixon diamond example It corresponds energy function E 10001VQ IOOONR 1ORP 1OQP lOOON 1OQ Example 412 Converting case naming variables meeting example Table 1 general purposes assumptions demonstration The energy function summing energy assumptions 1000TM lOOOT3CM 1oooT4cs 2000TM 1OOOrs lOOOT2M 2OOOT3C lOOOT3M 2OOOT4C lOOOT4S 1OOOM 2000C 999rt 201 OT2 1 lOOT3 2OOOT4 It shown cubic symmetric assumptions Fig 5b Since generate simpler CipiE 1M strongly Fig 5a quadratic network network example variables function energy equivalent network lOOC CM lOOOC CS Fig 5c Table 1 Example 412 general case Penalty WFF x3 1000 1000 1000 1000 1 10 100 1000 Tl meeting T2 sick 7 meeting coldonly meeting T3 lOOOT 2TlM M lOOOT2SM 2T2 S M T2S T2M lOOO T3 C 2T3C M T3M T3CM T4 coldonly sick lOOO T4 C 2T4C S T4S T4CS Tl T2 T3 T4 lTl 1OT2 1OOT3 1 OOOT4 228 G PinkusArtijicul nfeliimcu 77 1995 203247 10 s 1000 M 4 1 100 21 Fig 5 Equivalent cubic b quadratic c quadratic simple conversion naming Ihr meeting example symmetric networks numbers circles thresholds possible theories Once preferred definition entailment A construction possible generate network searches preferred models construct network reason according Section 5 network described 442 Representing SCNs penalty logic formulas This subsection shows possible logic formula The motivation efficiently demonstrate compact language specification symmetric penalty network net connectionist logic penalty efficient works Theorem 413 Eve energy function E strongly equivalent exists constant c ErunkE Vrank c PLOFF 1 2 function Eliminate hidden variables Section 34 The energy ofproducts function form Construction The following algorithm generates strongly equivalent PLOFF energy energy function algorithm hidden variables converted PLOFF brought sum following way Let E I w nii x energy function We construct PLOFF The formula network The size formula linear number connections generated strongly equivalent energy order size original original function network G PiasArtificial Intelligence 77 1995 203247 229 Proof To Vrank ErankE c Vrank 2 c wi c Wl wOAjX WWx c c Wi c Wi wOl A x Wi c W WOAxF A x X wo wmj x c wrmm1 WI x I CwinXicwlnXcwi WiO It wrO n WI 40 ErankE c 0 Example 414 Looking network Fig 4 like PLOFF The energy function network E 1OOOhQ 1OOONR 1ORP 1OQP IOOON 1OQ The negative terms IOOOJVAQ 1000NR lOQAP 1000N The positive terms 10lR v 10 Q The final PLOFF lOOONrQ lOOONAR lOQAP lOOON 10RV 10Q Note usually meaningful case reversecompilation clear compact description formula exists network 5 A connectionist inference engine Suppose background PLOFF 9 evidence PLOFP e query like construct connectionist 1 tC u e k p 2 I U e k IP 3 network standard strict answer possible p 40 F 740 logic WFF 9 We answers ambiguous engine connectionist Intuitively built subnetworks satisfying model search trying biased preferred model satisfies p search preferred model satisfies rp If models exist biased second subnetwork rl e The subnetwork conclude satisfies p conclude 40 ambiguous 6 e entails 50 If preferred model U e k model satisfies asp 230 Table 2 fi U UC QUERYP UEQMRY G PinkasArtijiciul Intelligence 77 1995 203247 searches preferred model Cc satisfies P searches preferred model 14 satisfies YP PI I bias search model satisfies P bias I search model satisfies YP UE P A P AMBfGUOUSp satisfying models exist conclude AA4BIGCKlUS agree P Us P P AMBCUOUSp despite models conclude NOT ambiguous bias unable satisfying U e b p For simplicity conclude conjunction atomic proposition Later general solution atomic propositions let assume literals negation evidence e strict 9 single To implement rl naming copy intuition need atomic propositions duplicate background knowledge A A For query add initiate query P clamped user inquires P The unit answer It set TRUE Cc create atomic proposition P participate propositions QUERYp AMBIGUOUS QUERYp externally AMBIGUOUSp conclude Our inference G entails P 9 entails 7P engine described language penalty represents logic Table 2 Using network algorithm Theorem 410 generate generated Nixon example shown Fig 6 corresponding network The similar subnetwork One preferred model satisfies query searches preferred model falsifies Nixon diamond case rings represents Fig 6 Inference network searches query To initiate query P user externally clamps unit QUERYp This causes small positive bias E sent unit P small negative bias E sent P Each subnetworks Cc rV searches global minimum satisfying model original PLOFF The bias E small introduce new global minima subnetworks It constrain set G PinkasArtijcial Intelligence 77 1995 203247 231 satisfies If satisfying model set global minima bias exists global minima new set global minima The new set global minima preferred models IJ satisfy query If preferred model satisfies query models model set unaffected bias network searches succeeds conclude AMBIGUOUS models agree truth value query The AMBIGUOUS set false answer b p unit P If P true models Similarly bias rules satisfying proposition ti b rp b 5p P holds satisfying conclude satisfy answer models The network If tries P false conclude strict conjunction 4p literals evidence network simply clamping When background new evidence evigence 9 U e U P p querying P new atomic proposition user add appropriate atomic propositions arbitrary case need e arbitrary query D We building combine inference network observed general In schedule time Since problem manages time However attempts global minimum certain exponential The network generated converges correct answer instances global minimum An annealing search A slow annealing correct answer probably algorithm polynomial problem continuously knowledge representation time complexity sacrificed The inference mechanism time resources accuracy answer Only limited wish stop search limit reached The annealing planned process Although time resources given correct answer l6 like 171 NPhard AI use usually section 51 trades time resources given schedule given end guess 45 Traditionally language 281 accuracy answer accelerate example expressiveness answer incorrect systems allow able improve answer fit time special search limitation described trades 6 Learning propositional formulas So far seen networks compiled shows incrementally appeal connectionist models This section develop compiling Assume satisfying formulas network truth assignments representation tries learn unknown 4p For simplicity SCNs learn unknown propositional ability equal learn logic formulas However examples inductively formulas ones constructed formula p looking set formula let assume I6 There techniques improving chances escape local minima 15211 I7 Connectionist systems like 591 191 trade expressiveness time complexity 232 G PinkusArtificial lnrelligence 77 1995 203247 satisfiable WFF The task network learn way end learning process obtained translating 40 E Clearly set global minima energy function equal set satisfying models o r training set update function weights equal energy We stored memories look process learning associative memories Given set vectors satisfying models like construct network global formula assuming function exactly equal vectors presented described uses highorder units hyperarcs possible convert hyperarcs I8 pairwise adding hidden units Section 34 vectors unknown minima energy The algorithm reader remember connections Definition 61 A kCNF clause proposition negated WFF formed conjunction AND clauses OR k literals A literal atomic disjunction 7 atomic proposition For example contains A V 1B A A V C V D 3CNF composed clauses literals second contains I shall present new learning order arcs fast learning algorithm truth assignments realities unknown satisfy After presentation network satisfy formula These truth assignments rule symmetric possibly high connections learns unknown kCNF formula possible represent rule called examples presentations guaranteed set global minima updated corresponding exactly equal seen far Therefore assuming know k unknown desired network generated single scan training energy func set k tion presentations CNF formula set Note formula brought theory k large I9 set presentations kCNF form algorithm works formula p It practical unknown 61 A learning rule highorder symmetric connections x Let instantiation introduced clamping visible units XI x t 0 1 A presentation X vector zeros ones visible 5 XI rule units soon described update weight single lorder hyper arc It composed parts checks arc updated result current presentation second updates visible units Xi values xi The learning instantiation responsible weight Ix The memories network searches I9 Fortunately network contentaddressable complete rest bits given partial description stored vector expert domains regulated relatively short rules small k sufficient G PinkasArtcial Intelligence 77 1995 203247 233 seen time O A new combination 6 I I Checking update arc The idea certain bit patterns training set causes arcs connect units weights training updated updated A hyperarc rest units pattern active arc zero instantiated connects units involved set cause updating new pattern units pattern bit combination k bits participate 612 Updating The procedure weight update weight determined arc needs updated viewed extension Hebbian If number zero units participate familiar active inactive weight odd decrease For special case pairwise connection units activation value rule highorder connections weight rule increases increase hyperarc decreases weight 613 The kclause Let Arc Xi Given presentation learning rule formally Xii Zorder arc x x1 x instantiates visible units O 1 values iff exists new kbit pattern P Xi xi Xi lorder arc Arc updated Xi 9 Xjl 0 7 9 Xjk_1 0 seen earlier presentations includes zeroinstantiated number zero units Arc including 1 number zeros odd units Arc rest units Xj Xjh_ l If condition holds weight Arc incremented ones case decremented Example 62 Given updates presentation ABC 011 2clause rule causes following l The weight arc AB updated Nan 1 2bit combination l The weight BC A 0 B 1 new arc contains odd number zeros updated n c l 1 C 1 new arc arc contains zeros 2bit combination B l The bias unit B 2bit combination zerounits A 0 arc B includes zeros updated An l singleton A 0 B 1 new arc B extended arc B The bias A updated adding zero unit In similar way arc AC decremented C incremented extended bias B like 2bit combination like AB bias This oneshot multiple occurrences learning probability presentation learning pattern seen captured completely pattern provide information bit combination repetitions generate representation irrelevant occurs rule want needed longer Bayesian learn A single In contrast 234 G PirrkusArrrjicml lnrelligence 77 I 995 203247 62 Learning kCNF start connections example appears example The network grow linearly presentations weights change global minima includes examples presented Each time new regularities presentations presentations satisfy unknown cpq algorithm generates network global minima qf kCNF f ormula energy function truth assignments set presentations Theorem 63 v kCNF formula exactly equal E The network generated u single pass presentations Proof Only sketch proof given The proof based showing algorithm equivalent Valiants algorithm 681 Valiants algorithm starts list clauses learning kCNF possible kclauses n atomic propositions For presentation signment clauses unknown kCNF list satisfied example step positive example conjunction satisfies It proved formula presentations Therefore examples seen kCNF formula learned consistent presentations seen far exactly clauses list consists truth algorithm eliminates First need initial conjunction possible kclauses network zero weights represented Later clause elimination performed presentation initialization step equivalent step algorithm set weight updates An energy function zero constant energy function contradictions tautologies number clauses The initial conjunction algorithm zero weights exactly formula represents property list The step set weight updates performed kclause desired rule presentation Valiants algorithm corresponds disjunction Valiants elimination step A clause eliminated represents model satisfies exactly Valiants possible clauses starting point algorithm formulas G PinkasArtijcial Intelligence 77 1995 203247 235 example ofsomekbitpatternPXil instantiated performed network energy adding XinllIO OwheretheXisare energy space deleting Yis zero An elimination energy clause c terms E Since weights terms reverse signs weights actually updated If number zeros units Xis pattern The arcs updated result addition contain P instantiated ones rest units zeros P arc odd sign term n Xi fl c arc If number zeros corresponds sign positive weight exists new bit combination arc arc negative weight decremented rest bits combination incremented An arc updated P ones included arc zeros outside 0 Example 64 Learning truth assignments XOR formula ABC l 011101000 A B H C looking satisfying 110 We need 3clause rule express formula 3CNF The patterns look 3bit combinations presentations Given presentation ABC 011 nABc 1 odd number zeros ABC 1 number zeros Given n ABC 1 odd nc l Given ABC 000 nBc 1 c 1 n 1 B 1 n 1 1 nc1 Given ABC 110 Bc 1 AJB 1 236 G PinkasArtijicial Intelligence 77 1995 203247 Co Fig 7 The network highorder network XOR A e B C constructed quadratic equivalent hidden unit H b learning algorithm kclause learning If network passes cycle learning begins rule entire cycle test network performs stops k increased test algorithm 622 A general scheme learning kCNF k unknown rule examples training set network performs sufficiently stop learning 1 k 1 try monomials 2 Activate kclause 3 TEST 4 kkl 5 Goto 2 One approach TEST line 3 scheme discussed PAC 141 In learning correct approximately time network chances Valiants notion probably related model positive negative examples given arbitrary distribution The task polynomial distribution necessary 6 The approach guarantees polynomial exponential approach error rate larger E S arbitrary small E 8 We use k larger algorithm time number satisfying models PAC motivated similar conditions n 1 S 1 E like sure probability 141 satisfies polynomial needed style Another given complete examples The network unnecessary training task generate network set contains satisfying assignments performs ebad large k 6 compact probability positive probability As PAC case test case sequence samples network tests wrong global allows m errors r checks 2 If number fails k increased A behavior checks minimum The test procedure errors discussion literature learning m test succeeds lengthly reader referred theory approximation network converges testing theory techniques 2 m r computed 22 The paper carry contributions testing phase step general epsilondelta learning scheme bounds Chemoff bounds techniques related I suggest adaptation G PinkasArtifcial Intelligence 77 1995 203247 237 7 Experimental results Experiments randomly generated conjunctions managed satisfying solutions generated propositional 3variable clauses formulas The 3SAT 23 The large scale 3SAT problems speed Performance 57 provided comparison symmetric models formulas simulations remarkable GSAT algorithm 71 Simulations I machines tried mean field networks24 types algorithms inspired Hopfield networks Boltzmann 711 The Hopjield version tries solution Perform MAXTRIES In try set values units l Randomly l Perform Hopfield cycles MAXCYCLES MAXCONST continuous reducing A Hopfield cycle asynchronously solution energy zeros ones unit updated randomly updated selected cycles executed cycles performed updates units selecting unit need cycle updating value following way l If neti 0 unit l If neti 0 unit zero l If neti 0 unit value flipped 712 The Boltzmann version tries solution Do MAXT In try l Assign l Anneal zeroone random units starting templ cycle values tempO temp lTEMPSTEPS Perform Boltzmann Reduce When CLES continuous temperature tries performed2 cycles reduce energy zero Hopfield cycles executed MAXCY solution MAXCONST l If MAXTRIES tries executed solution annealing slows begins TEMPSTBPSTEMPSTEPSDELTAT annealing 27 3SAT problems convenient benchmark performance results algorithms available compare 24 William Chen assisted experiments ideas programming 25 The number cycles includes executed annealing 23x G PinkusArtificd Intelliencc 77 1995 203247 A Boltzmann cycle asynchronous random order function net temperature flipping unit visited update units value stochastically probability Section 323 713 The mean field version As Boltzmann version simulator slower annealing cycles Section 324 rest annealings tries MAXTRIES annealings time mean field theory annealing MFT Boltzmann cycles 26 A MFT cycle asynchronous random order unit turn updates update units Boltzmann cycle The activation units selected value activation function MET 72 The experiments Random 3SAT formulas II variables m clauses generated following way l Generate zeroone generated satisfied assignment truth assignment random vector n bits The formula l Starting formula m clauses added Randomly generate 3variable clause If clause new satisfied assignment selection 3 n variables add new clause formula conducted ratio 43 In experiments number variables fiability problems 100 120 200 variables 50 formulas generated variables The parameters simulations mn kept This ratio formulas generated number clauses generate hard satis 50 70 300 400 500 appear Table 3 One 34 During final stages experiments2 local search satisfiability similar networks known executed arc performed In flipped selected 13571 In try random tither solution flip variables randomly recent algorithms perform seen sequential variations Hopfield tries generated variable flips flips performed 57 maximum MAXTRIES MAXTRIES truth assignment In GSAT selected variables flipped cause largest flipping The variable xi Trying MFT cycles good strategy MFf 27 The way generate formulas different I34 1 Our generator forces formulas satisfiable I34 1 random formulas generated forced satisfiable later DavisPutnam algorithm 141 eliminate unsatisfiable formulas Our approach based resolution distribution generated easier 34 Nevertheless comparisons GSAT valid experiments conducted set B Selman private communication deterministic formulas It remains seen similar results happen unforced distributions 2X Our experimental design began presentation connectionist approach AAAI Spring Symposium 1991 different idea random generation satisfiability problems We changed benchmark design meet ratio reported 34 1 G PinkasArtcial Intelligence 77 1995 203247 239 m MAXTBIES MAXCYCLES MAXCONST TBMPSTBPS DELTAT 215 301 430 516 860 1275 1700 2150 50 50 100 250 500 2000 2500 3000 250 350 500 600 200 6000 8000 10000 20 20 60 60 60 120 170 200 8 11 15 14 28 35 50 77 m 215 301 430 516 860 1275 1700 2150 GSAT 26822 43643 109535 137447 4817 87718 16247 50664 Hopfield Boltzmann 2464 3337 6943 4999 8592 1052 15432 29782 2404 2836 8389 5904 8839 10168 12914 23354 1 I I 1 2 5 5 5 MFf 1873 1314 5559 3305 4678 5592 10626 15206 Table 3 n 50 70 100 120 200 300 400 500 Table 4 n 50 70 100 120 200 300 400 500 largest reported satisfiability approaches satisfied clauses better DavisPutnam absolute gradient GSAT increase perform significantly popular algorithms comparing satisfied clauses flipped The algorithm 131 directly close similarity comparison GSAT parameters taken experiments MAXCONST TEMPSTEPS problem 3o No fine tuning parameters algorithm 4 We implemented GSAT purpose number Hopfield version29 For purpose fair MAXCYCLES 571 The rest parameters n m MAXTRIES 131 variables DELTAT intuitively taken according implemented size values reported increase In Table 4 gives average number cycles algorithms solution Table 5 shows percentage experiments algorithms managed solution trial The reader note comparison GSAT based parallel execution run parallel time The number flips cycle counted A cycle GSAT flip single update units architecture constant assumed 2g The difference algorithm 3 The TEMPSTEPS Hopfield successful try fails truly random 131 nodes visited predefined order vector generated parameter taken approximately 075 average number cycles 240 Table S G PinktrsArticiul Intelligence 77 I 995 203247 V1 21s 301 330 5 I 6 860 1275 1700 2150 GSAT Hopfield Boltzmann 69 61 68 65 66 74 80 66 81 83 70 71 83 90 88 86 MFT 94 94 93 87 96 96 94 92 flips cycle parallel step constant GSAT designed different measured parallel execution flips cycles comparison time j Note For parallel execution connectionist approaches clearly leading Not surpris ingly MFT best performance firsthit 8 Related work discussion 8 I Connectimist approuches Derthick functions mundane 5 observed weighted reduction logical constraints implement reasoning skeptical The described energy basic differences called certain constraints translated subset language KL paper similarities Derthick uses different en logic 1 hidden units likely single model literature described implemented like Hopfield net architectures advantage hardware given network given ties massively parallel architecture Derthick special energy functions ONE The approach described Looking ergy functions Derthicks closer Section 82 standard works Boltzmann machines implementations networks described PLOFF reverse 4 A learning algorithm achieves loworder units relatively wellstudied algorithms It possible learning 3 Formal proofs twoway equivalence paper systems described cautious recent based finding symbolic nonmonotonic direct compilation developed networks 2 The obtained behavior Another connectionist nonmonotonic reasoning based maximum I use standard different likelihood lowlevel Shastri 581 It uses evidential reason inheritance networks My approach connectionist models restricted time cycle I Constant time parallel GSAT cycles constant average time suitable parallel architecture certainly obvious 1 conjecture true parallel connection approaches constant GSAT cycle computed G PinkasArtcial Intelligence 77 1995 203247 241 networks32 inheritance time complexity guaranteed trades correctness guaranteed work polynomial Shastris described tries solve intractable problem correct solution global minimum time This article shares 1195965 chance finding improves time given implementationalist 481 These small subsets predicate calculus spreading activation motivation implement considerations systems rule firing The expressive power mechanisms tractability structures attacking constraint extended limited performance complex In article I intention represent propositional networks cope naturally conflicting beliefs The technique multiplace predicates I intended stress problems representing syntax sensitivity problems firstorder predicate calculus representing 431 We look penalty logic layers abstraction highlevel logic seen level abstraction cognitive processes lowlevel neural higher needed implemen implementation language described l nice discussion multispan approach paper map systems mentioned symmetric networks possibly logic compile descriptions tations Thus penalty neural Using penalty sacrificing efficiency 421 82 Symbolic systems semantics binary 26 Pearl rational consequence results relationship preferential semantics 27 Lehmann paradigm A strict consequence lines work systems preferential Magidor 60 use ranked models 371 33 Lehmann relations induced relation strict evidence strict conclusion logic Penalty related particular like Lehmann Magidors ranked models applied PLOFF It strict WFFs Lehmann satisfies certain conditions rational strong conclusion implementable symmetric network Also symmetric network viewed implementing rational consequence relation We sure implementation connectionist set pairs Rg p 40 1 p cp p 40 relation function As result conclude Magidor defined rational consequence Every rational consequence iff defined ranking induces rational consequence rules proved consequence inference inference relation relation relation relation engine Iogic based Bayes semantics On surface Penalty preferential based Cox axioms However systems based preferential It developed compute probabilities notion discovered semantics systems based epsilon semantics 34 tight relationships reasoning 32 We easily extend approach predicates free variables Those variables bound user query M These systems related turn probabilistic reasoning 34 In epsflon semantics probabilities approach zero handle time inheritance networks looking atomic propositions Bayes systems means epsilon semantics 242 G PinasArrtjciul Intelligence 77 1995 203247 reduced directly 121 actually computes based Bayesian approach interpretation Pearl based maximal entropy considerations uses ranking penalties violated beliefs based probabilistic 10121 One penalty penalties given conditional logic Goldszmidt knowledge user specify penalty The summing article function described Penalty logic similarities systems based priorities 31 based levels reliability Brewkas approximately logic mapped architecture Poole penalty 50 beliefs One propositional large penalties Systems implemented generated automatically Another user specify nice properties hold specificity Penalty priority systems assigning priority Z entailed approximating base However logic knowledge drawn decisively bolder considered based priorities ambiguous In sense penalty based priorities conclusions logic penalty scaled penalties 35 Every conclusion penalties ghost cautious given logic selecting strict specificity 111 penalties 121 Z changes entailed penalty priority logic 47 121 We given birds penguins like Z able conclude wings case For example consider defaults birds penguins fly birds wings penguins logic approximate following fly Many systems based priorities penguins wings Penalty penguins wings despite intuitive deduction penalty normal fly wings wings dont fine preference 1 I Prioritybased logic contrast conclude according intuition fact penguins fly The reason models penguins logic considers models penguins fly ambiguous Nixon football For example Nixon case Example P 22 add consider fans fan football 1000V 4 FF 10FF systems skeptical tend pacifist Most nonmonotonic P P 1026303764 supporting P assumptions supporting 1P In particular case correct behavior changing penalty learns adjust nonmonotonic Our boldly contrast intuition like intuition Further network Q f P multiplying assumption develop autonomously penalties better defeat behavior decides Because allow arbitrary partial orders 10601 models fundamental rankedmodels cases intuition semantics s6 problematic examples boldly concludes systems skeptical tell skepticism right behavior systems 5 The penalties scaled penalty higher priority 76 Hector Geffner private communication subset lowpriority assumptions sum exceeds G PinkmArtcial Intelligence 77 1995 203247 243 The following possible wish prove example ranking clear function exists induces intuition intuitive behavior Example 81 Assume The intuition states following defeasible rules A D B TD C TD l Given A C D conclude B rank ABCD rank ABCD l Given A B C conclude D ambiguous rank ABCD rankABCD l Given A C D conclude B ambiguous rank ABC6 rank ABCD l Given A B C conclude D ambiguous rank ABCD rank ABCD This contradiction examples implemented ranked model runk ABCD runk ABCD Thus intuition stated 9 Conclusions The main contributions tions connectionist propositional knowledge knowledge 3 demonstrating large scale randomly generated 3SAT problems penalty I representation mechanisms experimentally paper 1 development inference 2 rigorously theoretical founda learning opposing capable representing engine relating unifying connectionist logic efficiency algorithm SCNs networks propositional lines Along sentences reasoning inconsistency penalty logic It possible functions SCNs Penalty introduced logic showed mappings logic framework defeasible handling Several systems mapped paradigm given usually matches intuition ranking expressed right penalties intuitions behavior settings penalties When suggest features nonmonotonic serves purposes logic symmetric networks 1 translate logic equivalent network serves basic construction A strong equivalence sentences penalty formally proved This twoway equivalence sentence penalty inference oscillating specification clarifying network language engine look dynamics networks described penalty higher 2 symmetric network non asymmetric logic sentences The logic gives level abstraction 244 G PlnkasArtijiciul Inrelligence 77 I 995 203247 energy functions 2 highorder logic finally Several equivalent highlevel functions hidden units energy tional 4 penalty SCN sentence languages SCN penalty languages Algorithms inference languages SCNs 1 quadratic 3 proposi expressive given translating languages capable answering query global minima attractive logic All languages logic properties translated constructed clamped engine An follows network correspond exactly knowledge formula When query correct answers The engine obtain inductively knowledge formula time length size training set providing k small constant algorithm examples Any unknown RCNF formula learned compiling equivalent powerful symbolic shown algorithm symbolic learning linear learning The developed PAC paradigm Revision knowledge adding new evidence easy tasks use penalty deleting PLOFF simply computing energy existing knowledge Thus local change function new PLOFF network adding describes adding PLOFF deleting terms logic energy function describing network The mappings given exceeds potential expressive languages I implemented Boltzmann machine minimum exist The technique evidence problems connectionist approach nqueens translated paper limited propositional like firstorder predicate nonmonotonic local change network propositional knowledge case allows higherlevel logic represented 431 I noticed problems local minima simulator toy problems like Nixon Penguins network managed global definitely scales large randomly generated 3SAT problems repair methods provide good results NPhard 361 repair speed 394546 add additional 331 Advances heuristic techniques energy minimization similar heuristic The ability networks learn adjustments energy landscape provide new research direction Learning algorithms speed local minima widening global minima 37 network convergence article If research build networks satisfaction accelerate techniques fast symbolic speed time described constraint time eliminating able successful perform Acknowledgment Thanks Jon Doyle Hector Geffner Sally Goldman Dan Kimura Stan Kwasny Fritz Lehmann Ron Loui Judea Pearl Dave Touretzky helpful discussions 37 Similar techniques tried improving local search 36 1 G PinkasArttficial Intelligence 77 1995 203247 24s References I I JA Branden Encoding complex symbolic data structures unusual connectiont techniques JA Branden JB Pollack eds Advances Connectionist Neural Computation Theory 1 Highlevel connectionist models Ablex New York 199 1 I21 RD Brandt Y Wang AJ Laub SK Mitra Alternative networks Proceedings IEEE International Conference solving Traveling Salesman Neural problem Networks San Diego CA 1988 list matching problem I31 G Brewka Preferred subtheories extended logical framework default reasoning Proceedings IJCAI89 Detroit MI 1989 10431048 141 M Davis H Putnam A computing procedure 151 M Dcrthick Mundane reasoning parallel constraint quantification satisfaction theory J ACM 7 1960 201215 PhD Thesis CMUCS88182 Carnegie Mellon University Pittsburgh PA 1988 I61 M Derthick Mundane reasoning parallel constraint satisfaction Artif Intell 46 I2 1990 107157 17 I JA Feldman Energy behavior connectionist models Technical Report TR155 Computer Science Department University Rochester Rochester NY 1985 IS JA Fodor ZW Pylyshyn Connectionism cognitive architecture critical analysis Cognition 28 1988 371 191 S Geman D Geman Stochastic relaxation Gibbs distributions Bayesian restoration images IEEE Trans Pattern Anal Mach Intell 6 1984 721741 1 101 H Geffner Defeasible reasoning causal conditional theories PhD Thesis Department Computer Science University California Los Angeles CA 1989 I I 11 M Goldszmidt P Morris J Pearl A maximum entropy approach nonmonotonic reasoning Proceedings AAAI90 Boston MA 1990 646652 121 M Goldszmidt J Pearl System Z formalism Proceedings AAAI91 Anaheim CA 1991 399404 reasoning variablestrength defaults local search large satisfiability problem SIGART Bull 3 1 1992 812 I 131 J Gu Efficient 141 D Haussler M Keams N Littlestone M Warmuth Equivalence Inf Cotnpur appear Proceedings Workshop Computational Learning Theory polynomial models leamibility 1988 4255 Technical Report UCSCCRL8806 1988 15 I GE Hinton Deterministic Boltzmann learning performs steepest descent weight space Neural Comput 1 1 1161 GE Hinton Preface 1989 Special Issue Connectionist Symbol Processing Art Intell 46 1990 14 17 I GE Hinton TJ Sejnowski Learning relearning Boltzman Machines JL McClelland DE Rumelhart PDP Research Group eds Parallel Distributed Processing Explorations Microstructure Cognition 1 MIT Press Cambridge MA 1986 282317 181 S Hiilldobler A structured connectionist unification algorithm Proceedings AAAI90 Boston MA 1990 ICSI Technical Report TR900 12 1990 1 191 S Hiilldobler CHCL connectionist limited resources inference Horn logic based connection method International Computer Science Institute TR90042 1990 1201 JJ Hopfield Neural networks physical Proc Nat Acad Sci 79 1982 25542558 systems emergent collective computational abilities 211 JJ Hopfield Neurons graded response collective twostate neurons Proc Nat Acad Sci 81 1984 30883092 decisions 221 JJ Hopfield DW Tank Neural computation computational properties like optimization problems Viol Cybern 52 144152 1231 AB Kahng Traveling salesman heuristics embedding dimension Hopfield model Proceedings International Joint Conference Neural Nehvorks 1989 513520 241 S Kasif S Banerjee A Delcher G Sullivan Some results computationrd networks Technical Report JHUCS8910 Department Computer Science complexity symmetric Johns Hopkins University Baltimore MD 1989 connectionist 246 G PinkasArtciul Intelligence 77 1995 203247 I25 1 TE Lang MG Dyer Highlevel inferencing connectionist network Connection Ser 1 2 1989 181217 I26 1 D Lehmann What conditional knowledge base entail Proceedings Inrernutionul Conference Knowledge Representution und Reusoning Toronto Ont 1989 212222 27 1 D Lehmann M Magidor Rational logics models study cumulative logic Technical Report TR8616 Leibnitz Center Computer Science Hebrew University Jerusalem 1281 HJ Levesque A fundamental tradeoff knowledge representation reasoning Israel 1988 Proceedings CSCSI84 London Ont 1984 141152 I29 I V Lifschitz Computing I30 I RP Loui Defeat arguments 131 I J McCarthy Programs commonsense Press Cambridge MA 1968 4034 18 circumscription Proceedings IJCAI85 Los Angeles CA 1985 defeasible inference Cornput Well 3 3 1987 M Minski ed Semantic information Processing MIT I32 1 J McCarthy Circumscriptiona I33 1 S Minton MD Johnson AB Phillips Solving form nonmonotonic problems heuristic repair method large scale constraint Proceedings AAAI90 Boston MA 1990 satisfaction scheduling 1724 reasoning Arfif fntell 13 1980 2739 I34 1 D Mitchell B Selman HJ Levesque Hard easy distribution SAT problems Proceedings AAAI92 San Jose CA 1992 459465 35 1 M Minsky Logical versus analogical symbolic versus connectionist neat versus scruffy AI Mug 12 2 1991 36 I P Morris The breakout method escaping local minima Proceedings AAAI93 Wasington DC 1993 4045 I37 J Pearl System Z natural ordering defaults tractable applications nonmonotonic reasoning Proceedings Theoreticul Aspects Reasoning Knowledge Pacific Grove CA 1990 12 1 135 learning algorithm Neural Networks 2 I38 I C Peterson E Hartman Explorations mean field theory 6 1989 39 I C Peterson B Siiderberg A new method mapping optimization problems neural networks In Int I Neurul Syst 1 1989 322 1401 G Pinkas Energy minimization calculus Neurul Cornput 3 2 1991 DS Touretzky JL Elman TJ Sejnowski GE Hinton eds Proceedings 1990 Connectionist Models Summer School Morgan Kaufmann San Mateo CA 1990 satistiability propositional I41 I G Pinkas Propositional nonmonotonic reasoning inconsistency symmetric neural networks Proceedings IJCAI9I Sydney Australia 199 I I42 I G Pinkas Converting binary threshold networks symmetric networks Technical Report WUCS9 3 1 Computer Science Department Washington University St Louis MO 1991 I43 1 G Pinkas Constructing proofs symmetric networks JE Moody 1J Hanson RI Lipman Advunces Neural Information Processing Systems IV 1992 2 17224 I44 I G Pinkas Logical inference symmetric connectionist networks Doctoral Thesis Washington University St Louis MO 1992 I45 1 G Pinkas R Dechter A new improved connectionist Proceedings AAAI92 San Jose CA 1992 434439 activation function energy minimization I46 I G Pinkas R Dechter On improving connectionist I47 I G Pinkas R Loui Reasoning energy minimization taxonomy principles I AI Research appear resolving conflicts Proceedings Third International Conference Principles Knowledge Representation und Reasoning Cambridge MA 1992 inconsistency I48 I S Pinker A Prince On language connectionism analysis parallel distributed processing model language acquisition Cognition 28 1988 73 193 I49 I D Poole On comparison theories preferring specific explanation Proceedings fJCAf85 Los Angeles CA 1985 144147 default reasoning Art Infell 36 1988 2747 inconsistent premises Theory Decision 1 1970 1792 17 framework I SO I D Poole A logical I 5 I I N Rescher R Manor On inference I52 I N Rescher Plausible Reasoning Van Gorcum 1976 IS3 I R Reiter A logic default reasoning Art IS4 I JA Robinson A machineoriented 2341 Infell 13 1980 81132 logic based resolution principle J ACM 12 I 1965 G PinkasArttjkial Intelligence 77 1995 203247 247 1551 DE Rumelhatt GE Hinton JL McClelland A general framework parallel distributed processing JL McClelland DE Rumelhart PDP Research Group eds Parallel Distributed Processing Explorarions rhe Microstructure Cognition 1 MIT Press Cambridge MA 1986 1561 TJ Sejnowski Higherorder Boltzman machines neural networks computing Proc Amer Inst Phys 151 Snowbird UT 1986 3984 1571 B Selman HJ Levesque D Mitchell A new method solving hard satisfiability problems Proceedings AAAI92 San Jose CA 1992 440446 I581 L Shastri Semantic Networks An Evidential Formulation Its Connection Realization Pitman London 1988 I591 L Shastri V Ajjanagadde A step modeling reflexive reasoning Behav Brain Sci 16 3 1993 477494 1601 Y Shoham Reasoning Change MIT Press Cambridge MA 1988 6 I EH Shortliffe ComputerBased Medical Consultation MYCIN Elsevier New York 1976 621 Cl Simari RP Loui Mathematics defeasible reasoning implementation Artif Intell 53 1992 125157 63 1 P Smolensky Information processing dynamic systems foundations harmony theory JL McClelland DE Rumelhart PDP Research Group eds Parallel Distributed Processing Explorations Microstructure Cognition 1 MIT Press Cambridge MA 1986 641 DS Touretzky The Mathemafics Inheritance Systems Fitman London 1986 I65 1 DS Touretzky GE Hinton A distributed connectionist production Cognirive Science 12 3 1988 423466 1661 RJ Williams The logic activation functions JL McClelland DE Rumelhart PDP Research Group eds Parallel Distribufed Processing Explorations Microstructure Cognition 1 MIT Press Cambridge MA 1986 167 1 GV Wilson GS Pawley On stability travelling salesman problem algorithm Hopfield Tank Biol Cybern 58 1988 6370 I68 LG Valiant A theory learnable Commun ACM 27 1984 1134l 142