Artiﬁcial Intelligence 174 2010 15401569 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Learning complex action models quantiﬁers logical implications Hankz Hankui Zhuo ab Qiang Yang b Derek Hao Hu b Lei Li Department Computer Science Sun Yatsen University Guangzhou China 510275 b Department Computer Science Engineering Hong Kong University Science Technology Clearwater Bay Kowloon Hong Kong r t c l e n f o b s t r c t Article history Received 2 January 2010 Received revised form 4 September 2010 Accepted 5 September 2010 Available online 29 September 2010 Keywords Action model learning Machine learning Knowledge engineering Automated planning Automated planning requires action models described languages Planning Domain Deﬁnition Language PDDL input building action models scratch diﬃcult timeconsuming task experts This diﬃcult formally conditions changes reﬂected preconditions effects action models In past algorithms automatically learn simple action models plan traces However cases real world need complicated expressions based universal existential quantiﬁers implications action models precisely underlying logical mechanisms actions Such complex action models learned previous algorithms In article present novel algorithm called LAMP Learning Action Models Plan traces learn action models quantiﬁers logical implications set observed plan traces partially observed intermediate state information The LAMP algorithm generates candidate formulas passed Markov Logic Network MLN selecting likely subsets candidate formulas The selected subset formulas transformed learned action models tweaked domain experts arrive ﬁnal models We evaluate approach planning domains demonstrate LAMP effective learning complex action models We analyze human effort saved LAMP helping create action models user study Finally apply LAMP realworld application domain software requirement engineering help engineers acquire software requirements LAMP help experts great deal realworld knowledgeengineering applications 2010 Elsevier BV All rights reserved 1 Introduction Automated planning systems achieve goals producing sequences actions given action models provided input 14 A typical way action models use action languages Planning Domain Deﬁnition Language PDDL 131114 specify precedence consequence actions A traditional way building action models ask domain experts analyze task domain manually construct domain scription includes set complete action models Planning systems proceed generate action sequences achieve goals However diﬃcult timeconsuming manually build action models given task domain experts This typical problem knowledgeengineering bottleneck experts ﬁnd diﬃcult articulate experiences formally completely Because researchers started explore ways reduce human Corresponding author Email addresses zhuohankmailsysueducn HH Zhuo qyangcseusthk Q Yang derekhhcseusthk DH Hu lnslileimailsysueducn L Li 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201009007 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1541 effort building action models learning observed examples plan traces Some researchers developed methods learn action models complete state information action example plan traces 4 153150 Others Yang et al 5139 proposed learn action models plan examples incomplete state information Yang et al 5152 developed approach known Action Relation Modeling System ARMS learn action models STRIPS STanford Research Institute Problem Solver 10 representation weighted MAXSATbased Maximum Satisﬁability approach Shahaf et al 39 proposed algorithm called Simultaneous Learning Filtering SLAF learn expressive action models consistencybased algorithms Despite success learning systems real world applications actions expressed expressive representation quantiﬁers logical implications For instance consider case different cases briefcase1 planning domain briefcase moved place briefcase color We model action PDDL follows2 action movec1 case l1 l2 location pre c1 l1 forall c2 case imply samecolor c2 c1not c2 l2 effect c1 l2 c1 l1 That want case c1 location l1 l2 c1 l1 ﬁrst case c2 color c1 l2 After action c1 l2 instead l1 Likewise consider pilot ﬂy place enemies We model action model ﬂy follows action pre ﬂyp1 pilot l1 l2 location p1 l1 forall p2 person imply enemy p2 p1not p2 l2 effect p1 l2 p1 l1 We examples need universal quantiﬁers logical implications precondition action precisely represent action compress action model compact form As example consider driver intends drive train Before start sure passengers gotten train After seat vacant start drive train We represent drivetrain action model PDDL follows action pre effect drivetraind driver t train free d forall p passenger p t exist s seat vacant s available t driving d tnot free d That driver d makes sure passengers p train t free time drive train t Furthermore seat s vacant consequence action drivetrain train set available passengers train Besides driver d state driving train driving d t free Such action model needs universal quantiﬁer describing preconditions existential quantiﬁer condition exist s seat vacant s conditional effect exist s seat vacant savailable t More examples require use quantiﬁers logical implications action models recent International Planning Competitions domains IPC53 trucks openstacks These complex action models represented PDDL learned existing algorithms proposed action model learning Our objective develop new algorithm learning complex action models quantiﬁers including conditional effects logical implications collection given example plan traces The input algorithm includes 1 set observed plan traces partially observed intermediate state information actions 2 list action headings composed action list parameters provided preconditions effects 3 list predicates corresponding parameters Our algorithm called LAMP Learn Action Models Plan traces outputs set action models quantiﬁers implications These action models summarizes plan traces possible domain experts need spend small time revising parts action models incorrect incomplete ﬁnalizing action models planning usage Compared previous approaches main contributions 1 LAMP learn quantiﬁers conform PDDL deﬁnition 1311 article shows action models PDDL quantiﬁers 2 LAMP learn action models implications preconditions improves expressiveness learned action models We require 1 httpwwwinformatikunifreiburgdekoehleripppddldomainstargz 2 A symbol preﬁx suggests symbol variable c1 suggests c1 variable certain constants values 3 httpzeusingunibsitipc5domainshtml 1542 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 existential quantiﬁers quantify lefthand conditional effect consistent constraints PDDL12 PDDL21 1311 3 Similar previous systems ARMS 51 52 LAMP learn action models plan traces partially observed intermediate state information This important real world situations record actions plan trace likely incomplete small number sensors record subset happens action executed In case number sensors cover possible new information sources events action Therefore state information observe incomplete One ask question large number plan traces provided descrip tion action models unavailable Here provide examples demonstrate validity requirements realworld applications One example batch commands operating consist command partial information directory location structure content If view applica tion planning application batch command list actions We easily action headings action names parameters UNIX history command However intermediate state information commands reading batch command ﬁle This corresponds example plan traces partial intermediate state information In domain speciﬁcation action models easily large number batch commands viewed plan examples partial state information Another example activity recognition 56 active research area integrates pervasive computing ma chine learning wireless sensor networks Activity recognition aims identify actions goals agents series observations agents actions environmental conditions One scenario sensorbased activity recognition 6019 use sensor readings pervasive computing environment understand activities carried collecting large number sensor reading sequences We perform activity recognition mapping sensorreading sequences corresponding activity sequences These activity sequences act input actionmodel learning algorithms obtain action models improve accu racy activity recognition However uncertainty sensor readings partial knowledge activities learned Thus state information activity sequences learned partially observ able Therefore activityrecognition domain large number plan sequences partially observed intermediate state information knowing model action A example comes Web services planning researchers attempted automate process linking Web services achieve complex tasks 17 Standard description languages SOAP 57 Simple Object Access Protocol syntax semantics services However standard protocols SOAP syntax Web services diﬃcult rely Webservice providers label semantic content service especially consider case Web services come different sources However easy check log data acquire large number examples learn Web service behaviors examples 42 Similarly scenario large number logs plan traces use learn Web service behaviors action models precise descriptions events happening service executed We overview LAMP algorithm At high level LAMP algorithm described steps Firstly encode input plan traces including observed states actions represented state transitions propositional formulas Secondly generate candidate formulas according predicate lists domain constraints Thirdly build Markov Logic Network MLN learning corresponding weights formulas select likely subset set candidate formulas Finally convert subset ﬁnal action models We conduct experiments planning domains LAMP effective learning action models quantiﬁers logical implications These action models incorrect incomplete Thus deﬁne quality measure learned action models We learned action models close handcrafted action models support quantiﬁers logical implication Furthermore conducting user studies demonstrate human experts need spend relatively small time revising learned action models compared writing action models scratch Finally apply LAMP software requirement engineering domain illustrate realworld applications proposed method The rest paper organized follows We ﬁrst discuss related works ﬁeld action model learning In particular discuss ARMS SLAF Since paper related Markov Logic Networks MLNs review papers theoretical foundations applications MLNs Next deﬁnition problem detailed steps LAMP algorithm We evaluate algorithm planning domains learn action models desirable properties learned action models Finally conclude paper discuss future works 2 Related work 21 Learning intermediate state information Recently methods proposed learn action models plan traces automatically The ﬁrst learn action models plan traces intermediate state information 6153150373233 Gil et al 15 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1543 called EXPO bootstrapped incomplete STRIPSlike domain description rest ﬁlled experience Oates et al 31 use general classiﬁcation learn effects preconditions actions identifying irrelevant variables Schmill et al 37 try learn operators approximate computation relevant domains assuming world fully observable Wang 50 describes approach automatically learn planning operators observing expert solution traces reﬁne operators practice learningbydoing paradigm It uses knowledge naturally observable experts solve problems Chrisman 6 shows learn stochastic actions conditional effects In 3233 probabilistic relational planning rule representation learned compactly model noisy nondeterministic action effects Holmes et al 18 model synthetic items based experience construct action models Walsh Littman 42 propose eﬃcient algorithm learning action schemas describing Web services Among methods limitations intermediate observations need known However real applications activity recognition wireless sensor networks biological applications AI Planning intelligent user interfaces Web services 14241612 obtain intermediate state information 22 Learning partial intermediate state information In past solutions developed learn action models intermediate state information fully observable These partially observable cases In area important algorithms ARMS SLAF ARMS 5152 automatically discover action models set successfully observed plan traces Unlike previous work action model learning assume complete knowledge states middle observed plan traces ARMS works partial intermediate states given These plan traces obtained observation agent know logical encoding actions state information actions Speciﬁcally ARMS gathers knowledge statistical distribution frequent sets actions plan traces It builds weighted propositional satisﬁability problem solves weighted MAXSAT solver It extracts constraints plan traces STRIPS models deals constraints weighted MAXSAT 5 Finally attains STRIPS models output weighted MAXSAT ARMS handle cases intermediate observations diﬃcult acquire learn action models quantiﬁers implications complex STRIPS models While ARMS learn STRIPS actions plan traces designed learn complex action models The precondi tions effects STRIPS action model literals makes easy build actionplan constraints PREADDDEL lists assign weights ARMS However formulas contain universal existential quantiﬁers number literals affected formulas unﬁxed extremely large One natural example way handle wildcard characters operating Such wildcard characters like corre spond ﬁles directory makes learning especially diﬃcult slow Thus contrast ARMS choose build LAMP algorithm based natural approach learning ﬁrstorder logic formulas Markov Logic Network MLN 23 Amir et al 1393830 present tractable exact SLAF problem identifying actions effects partially observable STRIPS domains It resembles version spaces logical ﬁltering identiﬁes models consistent observations It maintains outputs relational logical representation possible actionschema models sequence executed actions partial observations To improve performance Shahaf et al 39 propose eﬃcient algorithm learn preconditions effects deterministic action models For quantiﬁers Shahaf Amir 38 impose constraints learning result existential quantiﬁers appear preconditions universal quantiﬁers appear effects In realworld planning applications existential quantiﬁers appear effects actions universal quantiﬁers appear preconditions actions shown examples graduate Section 1 Besides learn action models preconditions include form implications requires complex action models real world These action models noted Section 1 needed model agents complex behaviors realworld applications Cresswell et al 44 present called LOCM designed carry automated induction action models sets example plans LOCM assumes planning domain consists sets called sorts object instances object behaves way object sort 45 Compared previous systems LOCM learn action models action sequences input shown work assumption output domain model represented objectcentered representation Our previous work 5354 established feasibility learning action models conditional effects transferring knowledge planning domains In 55 presented algorithm learn action models STRIPS descriptions Hierarchical Task Network HTN 59 methodpreconditions simultaneously In paper illustrate actionmodel learning LAMP learn complex action models including quantiﬁers logic implications conditional effects We present application example software engineering 58 real beneﬁt gained LAMP 23 Relation knowledge acquisition inductive logic programming In 4 action models acquired human expert interactions Simpson et al 4946 Graphical Interface Planning Objects called GIPO built investigate support knowledge engineering 1544 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 process building applied AI Planning systems Winner Veloso 47 present DISTILL learn program like plan templates example plans aim automatically acquire plan templates example plans templates planning directly The focus 47 extract plan templates example plans substitute planners The plan templates represent structural relations actions Another related work 48 programming demonstration PBD problem solved versionspace learning algorithm acquiring normal behavior terms repetitive tasks When user start repetitive task going sequence states use learned action sequence map initial goal states directly Different work aim PBD 48 learn action sequences instead action models Another related work 36 Sablon Bruynooghe present method learn action models experience observation domain expert It exploits idea concept induction ﬁrstorder predicate logic inductive logic programming ILP 29 allows utilize ILP noisehandling techniques learning losing representational power Similarly 2 presented learn precondition action TOP operator ILP The examples require positive negative examples propositions held states actions application ILP learn positive negative examples states target actions given However problem example traces partial states provided input To best knowledge work far apply ILP problem 24 Markov Logic Networks 241 Summary We use Markov Logic Networks MLNs 35923 help learn action models work MLN powerful framework combines probability ﬁrstorder logic An MLN consists set weighted formulas provide way soften hard constraints ﬁrstorder logic The main motivation MLNs soften hard constraints world violates formula knowledge base probable impossible Thus formula associated weight reﬂects strong constraint Each weight learned data variety methods including convex optimization likelihood related function iterative scaling margin maximization 27 The network structure learned input data typically performing greedy search conjunctions variables 212820 Furthermore different versions MLNs proposed For instance Wang Domingos 43 introduce hybrid MLNs continuous properties functions appear features Singla Domingos 41 extend Markov logic inﬁnite domains casting framework Gibbs measures 242 Weight learning MLNs Since major steps LAMP algorithm involves learning weights set candidate formulas standard techniques MLNs brieﬂy review commonly weight learning algorithms optimizing pseudo loglikelihood MLNs In work use idea weight learning MLNs presented 35 A brief description idea given follows A Markov Logic Network L set pairs F w F formula ﬁrstorder logic w real number Together ﬁnite set constants C c1 c2 cC deﬁnes Markov network M LC follows 1 M LC contains binary node possible grounding predicate appearing L The value node 1 grounded predicate true 0 2 M LC contains feature possible grounding formula F L The value feature 1 ground formula true 0 The weight feature w associated F L We assume formulas clausal form Let X set propositions describing world F set clauses MLN w weight associated clause f F We optimize pseudo likelihood 3 deﬁned w X x P ncid2 l1 cid3 cid4 Xl xlMBxXl P w Xl ground atom X MBx Xl state Markov blanket Xl n number ground atoms X In Markov network Markov blanket node deﬁned set neighboring nodes Similarly Markov blanket ground atom set ground atoms appear instantiation formula P w Xl xlMBx Xl computed cid3 cid4 Xl xlMBxXl P w C Xlxl exp cid5 f Fl w f C Xlxl C Xl0 C Xl1 cid3 cid4 Xl xl MBxXl HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1545 Fl set ground formulas Xl appears f Xl xl MBx Xl value 0 1 feature corresponding ith ground formula Xl xl Markov blanket state MBx Xl By optimizing pseudo log likelihood limitedmemory BFGS algorithm 26 learn weights MLNs MLNs applied realworld applications For instance Domingos 7 proposes apply Markov logic model real social networks evolve time multiple types arcs nodes affected actions multiple players Singla et al 40 build integrated solution based Markov logic solve entity resolution problem determine records database refer entities furthermore Poon et al 34 propose joint approach perform information extraction Markov logic existing algorithms segmentation records entity resolution performed single integrated inference process Domingos 8 gives theoretical discussion relationship data mining Markov logic Lowd et al 22 present unsupervised approach extract semantic networks large volumes text applying Markov logic 3 Problem deﬁnition A classical planning problem described P Σ s0 g Σ S A γ planning domain S set states A set action models γ deterministic transition function S A S s0 initial state g goal description An action model deﬁned action heading preconditions effects action heading composed action zero parameters A plan action sequence a0 a1 makes projection s0 g A plan trace deﬁned T s0 a0 s1 a1 sn g s1 sn partial observations intermediate states si subset world state These partial observations allowed Each ai instantiated action heading form action nameparameters moveL1 home action L1 home parameters describing locations Our learning problem stated follows We given set plan traces T set predicates set action headings A occur T As output LAMP outputs preconditions effects action heading A An example input desired output algorithm LAMP briefcase domain shown Tables 1 24 4 The LAMP algorithm In section detailed descriptions LAMP algorithm We ﬁrst overview algorithm shown Algorithm 1 Algorithm 1 Overview LAMP Input 1 A set predicates P 2 A set action headings A 3 A set plan traces T Output A set action models A 1 Encode plan trace set propositions denoted DB database plan traces denoted DBs DBs encode_tracesT 2 Generate candidate formulas F according correctness constraints F1 F5 F generate_formulasP A 3 Learn weights W candidate formulas W learn_weightsF DBs 4 Convert weighted candidate formulas action models A A attain_modelsW F 5 return A In following subsections detailed description step corresponds italic parts Algorithm 1 41 Step 1 encode plan traces In ﬁrst step Algorithm 1 procedure encode_traces encodes plan traces set proposition databases DBs plan traces T input To encode plan traces propositional formulas states state transitions need encoded A brief description procedure given Algorithm 2 In following detailed description main steps Steps 5 9 Algorithm 2 Encode state propositional formula In Step 5 Algorithm 2 idea use propositional formula represent facts hold state For instance consider briefcase domain Table 1 object o1 location l1 The state describing object o1 briefcase briefcase location l1 represented formula ino1 isatl1 If consider ino1 isatl1 propositional variables formula propositional 4 For clarity readers familiar domain brieﬂy predicates actions briefcase domain mean In domain portable objects locations briefcase moving objects place For predicates x portable l location means portable object x location l necessarily briefcase x portable means portable object x currently briefcase isat l location means briefcase location l For actions m location l location means person moved briefcase location m location l takeout x portable means portable object x taken briefcase putin x portable l location means portable object x briefcase briefcase location l 1546 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 1 The input LAMP x portable l location x portable isat l location Input predicates Input action headings m location l location takeout x portable putin x portable l location initial state Input plan traces Plan trace 1 Plan trace 2 Plan trace 3 isat l1 o1 l1 o2 l2 isat l2 o1 l2 o2 l1 o1 o2 isat home o1 l1 isat l1 o1 home o1 isat l2 o1 l2 o2 home isat l1 o1 home o2 l2 o1 o2 action 1 putin o1 l1 home l1 putin o1 l2 observation 1 isat l1 action 2 l1 l2 putin o1 l1 l2 l1 observation 2 o1 action 3 putin o2 l2 l1 home takeout o1 observation 3 o2 isat l2 action 4 l2 home l1 home observation 4 action 5 observation 5 action 6 goal state isat home o1 home o2 home isat home o1 home putin o2 home o2 home l2 isat l1 o1 l1 o2 l2 Table 2 The desired output LAMP preconditions effects preconditions effects preconditions effects m location l location isat m isat l isat m forall x portable x x lnot x m putin x portable l location x l isat l x takeout x portable x x formula A model propositional formula assigns true propositional variables ino1 isatl1 If location l2 propositional formula modiﬁed ino1 isatl1 isatl2 Notice use p referring logic formula p referring PDDL description isatl2 described isat l2 PDDL Encode action proposition In Step 9 Algorithm 2 idea encode action state transitions plan trace proposition way similar situation calculus 25 The behavior deterministic actions described transition function γ S A S For instance action movel1l2 Table 2 described γ s1 movel1 l2 s2 Algorithm 2 Encode plan traces encode_tracesT HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1547 Encode s formula conjunction propositions Put propositions conjunction DB Initialize DB state s pt Input A set plan traces T Output A set databases DBs 1 Initialize DBs 2 plan trace pt T 3 4 5 6 7 8 9 10 11 12 13 end 14 return DBs end action t end Put DB DBs Encode proposition Put proposition DB In s1 briefcase location l1 s2 l2 The states s1 s2 represented isatl1 isatl2 isatl1 isatl2 These formulas represent fact evolves state s1 state s2 We need propositional formula assert state s1 isatl1 isatl2 holds state s2 executing action isatl1 isatl2 holds We need different propositional variables hold different states specify fact holds state hold state In Step 9 Algorithm 2 generate situationlabels si ground atoms following way We denote trace s0 a0 an1 sn assign situationlabel si ground atom p p appears state si Thus ground atom isatl1 appears action a1 associated situationlabel s1 represented atoms parameters isatl1 s1 The parameters candidate formula determined parameters actions predicates composed original parameters actions predicates plus situationlabel variable denote si By introducing new state parameters si different literals isatl1 s1 isatl2 s2 action movel1 l2 s1 respectively The action states briefcase location l1 state s1 location l2 state s2 action movel1 l2 s1 Thus fact evolves state s1 state s2 represented statement isatl1 s1 isatl2 s1 isatl1 s2 isatl2 s2 This formula encodes transition state s1 s2 Furthermore represent fact action movel1 l2 causes transition propositional variable movel1 l2 s1 holds true action executed state s1 As result represent function γ s1 movel1 l2 movel1 l2 s1 isatl1 s1 isatl2 s1 isatl1 s2 isatl2 s2 Thus naturally encode plan traces propositional formulas For instance encode second plan trace Table 1 following formula isathome s0 ato1 l1 s0 isatl1 s0 ato1 home s0 ino1 s0 movehome l1 s0 putino1 l1 s1 ino1 s2 movel1 home s2 isathome s3 ato1 home s3 Since plan trace encoded conjunction grounded literals called facts equivalent encoding plan trace database referred DB Each record DB fact Records related conjunction Thus encode different plan traces different DBs As example plan traces Table 1 encoded DBs shown Table 3 state symbol si represented integer Notice use closed world assumption The truth value proposition recorded Table 3 left unknown We learn weights formulas counting number formulas satisﬁed known grounded literals recorded DBs We detailed description Section 43 Notice formulation imposing constraints intermediate state infor mation needs observed Since proportion intermediate state information representation encoded number facts DBs intermediate state information affect number records DBs However later experiments proportion intermediate state information input plan traces affect accurately learn action models later general intermediate state information better learning accuracy 1548 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 3 Encodings plan traces DBs notice proposition preceded notation means proposition false proposition existing DB means true proposition existing DB means unknown consistent representation MLNs input DB 1 isat l1 0 o1 l1 0 o2 l2 0 isat l2 0 o1 l2 0 o2 l1 0 o1 0 o2 0 putin o1 l1 0 isat l1 1 l1 l2 1 putin o2 l2 2 o2 3 isat l2 3 l2 home 3 isat home 4 o1 home 4 o2 home 4 DB 2 isat home 0 o1 l1 0 isat l1 0 o1 home 0 o1 0 home l1 0 putin o1 l1 1 o1 2 l1 home 2 isat home 3 o1 home 3 DB 3 isat l2 0 o1 l2 0 o2 home 0 isat l1 0 o1 home 0 o2 l2 0 o1 0 o2 0 putin o1 l2 0 l2 l1 1 takeout o1 2 l1 home 3 putin o2 home 4 o2 5 home l2 5 isat l1 6 o1 l1 6 o2 l2 6 42 Step 2 generating candidate formulas In previous step encoded plan trace propositional formula conjunction ground literals represented formula database records facts In step generate candidate formulas procedure generate_formulas set predicates P set action headings A input There ﬁve parts action model learned preconditions positive effects negative effects positive conditional effects negative conditional effects Our idea set formulas Suppose possible preconditions action set formulas F If formula f F form p holds action precondition p As overview basic algorithm generating candidate formulas described follows Algorithm 3 Impose speciﬁc correctness constraints written form b action b logical mula Encode speciﬁc requirements preconditions positive negative effects positive negative conditional effects form implications sure formulas generate reﬂect correctness quirements Enumerate possibilities ground candidate formulas previous step For example replace propo sitional variable predicate predicate list enumerate possibilities possible grounding This allows anticipate forms preconditions action model Enumerate cases parameters candidate formulas quantiﬁed From steps generate number candidate formulas quantiﬁers logical implications We employ MLNlearning algorithms select subset formulas convert action models These ideas described following formulas F1 F5 We conform requirements PDDL precondition literal implication universal quantiﬁer Preconditions conjunctive An actions effect positive negative literal universal quantiﬁers An effect conditional effect existential quantiﬁer quantify condition The relationship effects conjunctive Algorithm 3 Generate candidate formulas generate_formulas P A Input 1 A set predicates P 2 set actions A Output A set candidate formulas F 1 Initialize F 2 action A 3 4 5 end 6 return F Generate candidate formulas according F1F5 P Put generated candidate formulas F We detailed description Step 3 Algorithm 3 divided ﬁve parts formulas F1 F5 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1549 Table 4 Candidate formulas according formula 1a encoding preconditions ID 1 2 3 4 5 Formulas iptakeout o o p ixptakeout o x p iptakeout o isat p itakeout o o ixtakeout o x F1 Preconditions Any literal p precondition action state action plan trace According formulageneration method build candidate formulas form formulas 1a 1b For action literal p state generate following formula cid4 cid3 ai pξ iξ 1a ξ x1 x2 xn set parameters appear parameters exist ps parameters n cid2 0 ξ represents free variables precondition action given action heading ith state si likewise formulas 1b5 For simplicity omit parameters ps parameters shared representation Corresponding formula generate candidate formulas universal quantiﬁers precondi tions action Let c set parameters common p ξ x1 x2 xn parameters p appear parameter list We generate forall ξ p ξ c When includes parameters p generate p c candidate precondition We generate precondition action implication denoted p1 p2 PDDL equally denoted imply p1 p2 We generate following formula cid3 ai iξ cid3 p1ξ1 p2ξ2 cid4cid4 equivalently cid3cid3 cid4 ai p1ξ1 cid4 p2ξ2 iξ 1b ξ ξ1 ξ2 Likewise implication form p1 p2 p1 p2 similarly generate corresponding formulas accordingly Corresponding formula 1b build precondition implication forall ξ imply p1 ξ1 c1 p2 ξ2 c2 c1 c2 sets parameters appearing We consider implications form p1 p2 p1 p2 p1 p2 easy extend 1 implication complex form p 1 p2 PDDL denoted imply p p p 2 1 1 1 t 1 p 2 1 p t 1 p2 These formulas called candidate formulas chosen end preconditions corresponding actions We select subset candidate formulas transform action models Steps 3 4 In following example generate candidate formulas according formulas 1a 1b Example 1 We generate candidate formulas action takeout Table 1 according formulas 1a 1b The result shown Tables 4 5 The initial weights generated candidate formulas zeros Candidate formulas actions Table 1 generated similarly F2 Positive effects We generate positive effect formulas follows For literal p p positive effect action literal negation symbol p added state right plan trace For instance generate candidate formula isat l positive effect m l Table 2 isat l2 added l1 l2 executed plan trace 1 Table 1 To avoid generating lengthy effect lists action models generate preference constraint p hold p added effects redundantly generated Of course cases constraint false state constraint preferred preferences reﬂected weights attached formulas learned This motivation generate candidate formulas form formula 2 means pξ hold state si hold state si1 cid4 cid3 ai pξ pξ 1 iξ 2 Corresponding formula generate candidate formulas universal quantiﬁers Recall positive negative effects conditional effects support existential quantiﬁers PDDL deﬁnition We build positive effects follows forall ξ p ξ c When action includes parameters predicate p generate 1550 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 5 Candidate formulas according formula 1b encoding preconditions ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Formulas ixptakeout o o p x p iptakeout o o p isat p iptakeout o o p o ixptakeout o o p x ixptakeout o x p o p ixptakeout o x p isat p ixptakeout o x p o ixptakeout o x p x iptakeout o isat p o p ixptakeout o isat p x p iptakeout o isat p o ixptakeout o isat p x iptakeout o o o p ixptakeout o o x p iptakeout o o isat p ixtakeout o o x ixptakeout o x o p ixptakeout o x x p ixptakeout o x isat p ixtakeout o x o Table 6 Candidate formulas according formula 2 encoding positive effects ID 1 2 3 4 5 Formulas iptakeout o o p 1 o p ixptakeout o x p 1 x p iptakeout o isat p 1 isat p itakeout o o 1 o ixtakeout o x 1 x Table 7 Candidate formulas according formula 3 encoding negative effects ID 1 2 3 4 5 Formulas iptakeout o o p 1 o p ixptakeout o x p 1 x p iptakeout o isat p 1 isat p itakeout o o 1 o ixtakeout o x 1 x positive effects PDDL form p c Example 2 illustrates generate candidate formulas according formula 2 Example 2 For different literals p actions generate different formulas according formula 2 The initial weights generated candidate formulas zeros In Table 6 list candidate formulas generated action takeout F3 Negative effects For action generate p negative effect action condition p satisﬁed deleted state executed As case F2 limit action model size preference constraint p deleted p exists For instance isat m negative effect m l isat l1 satisﬁed state l1 l2 executed satisﬁed state Thus build corresponding formulas form formula 3 cid4 cid3 ai pξ pξ 1 iξ 3 Corresponding formula generate candidate formulas universal quantiﬁers build negative effects PDDL form forall ξ p ξ c When includes parameters p build negative effects PDDL form p c In following Example 3 generate candidate formulas according formula 3 Example 3 According formula 3 generate candidate formulas action takeout Table 1 The initial weights generated candidate formulas zeros The result shown Table 7 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1551 Table 8 Candidate formulas according formula 4 encoding positive conditional effects ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Formulas ixptakeout o o p x p x p 1 iptakeout o o p isat p isat p 1 iptakeout o o p o o 1 ixptakeout o o p x x 1 ipxtakeout o x p o p o p 1 ipxtakeout o x p isat p isat p 1 ixptakeout o x p o o 1 ixptakeout o x p x x 1 iptakeout o isat p o p o p 1 ixptakeout o isat p x p x p 1 iptakeout o isat p o o 1 ixptakeout o isat p x x 1 iptakeout o o o p o p i1 ixptakeout o o x p x p 1 iptakeout o o isat p isat p i1 ixtakeout o o x x 1 ipxtakeout o x o p o p 1 ixptakeout o x x p x p 1 ipxtakeout o x isat p isat p 1 ixtakeout o x o o i1 F4 Positive conditional effects For action generate positive conditional effects p1 p2 meaning p1 holds p2 hold executed We assert p1 p2 positive conditional effect action p2 holds state execution p1 holds state execution We preference constraint F2 p2 preferred hold execution Thus build following candidate formulas cid4 cid3 ai p1ξ ξ1 p2ξ ξ2 p2ξ ξ2 1 iξξ2ξ1 4 ξ ξ1 ξ2 sets parameters appearing parameters ξ ξ1 ξ ξ2 ξ1 ξ2 We generate positive conditional effect form shown formula 4 easy extend forms iξξ1ξ2ai p1ξ ξ1 p2ξ ξ2 p2ξ ξ2 1 complex forms corresponding positive conditional effects p1 1 p2 p2 1 Corresponding formula 4 generate candidate formulas conditional effects universal quantiﬁers existential quantiﬁers build positive conditional effects PDDL forall ξ ξ2 exist ξ1 p1 ξ ξ1 c1p2 ξ ξ2 c2 p1 c1p2 c2 ξ ξ1 ξ2 sets notice correspond different PDDL forms c1 c2 sets parameters appearing In following Example 4 generate candidate formulas based formula 4 Example 4 For action takeout Table 1 generate candidate formulas according formula 4 The initial weights generated candidate formulas zeros The result shown Table 8 F5 Negative conditional effects Similar F4 action generate negative conditional effects form p1 p2 meaning p1 holds execution p2 hold deleted executed Additionally add preference p2 deleted actions effect p2 exists precondition action We formulate follows cid4 cid3 ai p1ξ ξ1 p2ξ ξ2 p2ξ ξ2 1 iξξ2ξ1 5 We generate negative conditional effects form shown formula 5 similar F4 Corresponding formula 5 generate candidate formulas conditional effects universal existential quantiﬁers build negative conditional effects PDDL forall ξ ξ2 exist ξ1 p1 ξ ξ1 c1not p2 ξ ξ2 c2 p1 c1not p2 c2 ξ ξ1 ξ2 sets corresponding forms Likewise Example 5 generate candidate formulas based formula 5 Example 5 For action takeout Table 1 generate candidate formulas according formula 5 The initial weights generated candidate formulas zeros The result shown Table 9 The formulas responsible generating space possible candidate literals formulas action models terms preconditions effects constraints needed learning select set consistent formulas consistent training examples plan traces We impose following constraints 1552 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 9 Candidate formulas according formula 5 encoding negative conditional effects ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Formulas ixptakeout o o p x p x p 1 iptakeout o o p isat p isat p 1 iptakeout o o p o o 1 ixptakeout o o p x x 1 ipxtakeout o x p o p o p 1 ipxtakeout o x p isat p isat p 1 ixptakeout o x p o o 1 ixptakeout o x p x x 1 iptakeout o isat p o p o p 1 ixptakeout o isat p x p x p 1 iptakeout o isat p o o 1 ixptakeout o isat p x x 1 iptakeout o o o p o p i1 ixptakeout o o x p x p 1 iptakeout o o isat p isat p i1 ixtakeout o o x x 1 ipxtakeout o x o p o p 1 ixptakeout o x x p x p 1 ipxtakeout o x isat p isat p 1 ixtakeout o x o o i1 A1 Actionconsistency constraint First wish model learned conﬂict Thus action effect p action effect p state executed We formulate constraint follows For action literal p effect formulas f 1 L1 p f 2 L2 p L1 L2 conjunction literals require L1 L2 mutually exclusive Notice L1 L2 f 1 p f 2 p L1 L2 mutually exclusive For example x x chosen effects action takeout x time A2 Planconsistency constraint We require action models learned consistent training plan traces This constraint imposed relationship ordered actions plan traces ensures causal links plan traces broken That precondition p action j plan trace p initial state action ai j prior j adds p action ak k j ai j deletes p For literal q state s j q initial state s0 action ai s j adds q action ak deletes q We formulate constraint follows p PREa j p EFFai p EFFak q s j cid3 q s0 cid3 q EFFai q EFFak cid4cid4 6a 6b k j PREa j set preconditions action j state s j composed set propositions predicates For example action putin isat precondition putin initial state action adds deleted actions putin Otherwise action putin executed action A3 Nonempty constraint We wish avoid extreme situation plan trace action models learned actions trace nonempty preconditions effects Although action model incorrect undesirable To avoid cases require preconditions effects actions learn nonempty In words action model following formula hold PREa cid12 EFFa cid12 7 These constraints learning phase building action models MLN post processing constraints selecting effects learned Step 3 subsection In experiment constraints enforced 43 Step 3 learning weights candidate formulas In section construct Markov Logic Networks 3523 learn weights candidate formulas F based constraints A The formulas weights larger certain threshold chosen represent HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1553 preconditions effects learned action models This step procedure learn_weights candidate formulas F databases DBs input At ﬁrst glance maximum satisﬁabilitybased algorithm solve problem selecting good subset formulas However case The reason weighted satisﬁabilitybased algorithm assign nonuniform weights collection formulas correspond instantiations formulas grouped formulas expressing quantiﬁers implications complex formulas corresponds collection ground instantiations The weights instantiation different making impossible combine reconstruct ﬁrstorder formulas It natural consider weights ﬁrstorder logic formula entirety Therefore Markov Logic Network appropriate model learn weights ﬁrstorder logic formulas way soften hard constraints To learn weights formulas F exploit Alchemy System MLN 3523 calculate optimize score WPLL Weighted Pseudo LogLikelihood 3 With respect weights w database x DBs list possible worlds WPLL deﬁned follows WPLLw x log cid3 cid4 Xl xlMBxXl P w ncid2 l1 cid3 cid4 Xl xlMBxXl log P w ncid5 l1 cid3 P w cid4 Xl xlMBxXl cid6 C Xlxl C Xl0 C Xl1 f Fl C Xlxl exp w f Xl xl MBx Xl n number possible groundings atoms appearing formulas F Xl lth ground atom MBx Xl state Markov blanket Xl x x xi world state xi 1 0 denotes truth value corresponding ground atom true false respectively A Markov blanket ground atom set ground atoms appear grounding formula Fl set ground formulas Xl appears f Xl xl MBx Xl value 0 1 feature corresponding ith ground formula Xl xl Markov blanket state MBx Xl For instance formula px y qx F x A B y C D Then possible groundings p A C p A D pB C pB D q A qB n 6 Xl 0 l cid3 n viewed groundings F p AC F 1 p A C q A likewise Fl A Markov blanket p A C q A easily according F p AC likewise Markov blankets ground atoms Given world state x 1 1 1 0 1 0 value f X1 x1 MBx X1 1 note X1 p A C x1 1 x MBx X1 q A 1 x As result C X1x1 exp cid5 cid3 cid4 X1 x1 MBxX1 w f e w f F 1 Similarly calculate C X10 e w Furthermore WPLLw x cid3 cid4 Xl xlMBxXl log P w 6cid5 l1 log e w e w e w w w formula term log calculate corresponding values e wi e wi e wi calculated l 1 Likewise l 1 l cid3 6 The score WPLL measure likelihood weighted candidate formulas satisﬁed training data DBs The higher weight scores larger likelihood formulas Thus maximize likelihood try maximize score WPLL choosing proper weights candidate formulas We use gradientdescent based algorithm learn weights maximize WPLL score shown Algorithm 4 Notice Step 5 Algorithm 4 x selected DBs random order One speciﬁc random order corresponds DBs number elements DBs repetitions Steps 5 8 calculating value w i1 In implementation ﬁnal result w i1 given average values attained according ﬁve randomly generated orders When computing WPLL count true instantiations individual DB instead different DBs Steps 59 Algorithm 4 Each time computing WPLL update weight w i1 use DB compute WPLL updating w i1 forth In way objects state indices different plan traces affect counting true instantiations counting individual plan trace instead different plan traces 1554 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Algorithm 4 Weight learning algorithm learn_weightsF DBs N Input list formulas F A list databases DBs number iterations N Output Weights candidate formulas F 1 Initialize w 0 0 0 2 set number iterations N 3 0 N 1 w i1 w 4 Each database x DBs 5 Calculate WPLLw i1 x 6 w i1 w i1 λ WPLLw i1x Notice λ small constant w i1 7 8 9 10 end 11 return w N end Since DBs attained Step 1 formulas F generated F1F5 Step 2 Algorithm 4 learn weights formulas F Example 6 following demonstrate weight learning procedure Example 6 For simplicity use closed world assumption example Thus Table 3 DB2 denoted isat home 0 o1 l1 0 home l1 0 putin o1 l1 1 o1 2 l1 home 2 isat home 3 o1 home 3 propositions shown viewed false 0 1 2 3 stands s0 s1 s2 s3 Then x denoted 1 1 1 xi 0 shown means corresponding proposition appear DB2 xi 1 means corresponding proposition appears DB2 We assume locations home l1 portable o1 states s0 s1 s2 s3 Then number propositions n x 48 calculated counting groundings isat l o l o m l putin o l takeout o Notice use new parameter denote states literal Take candidate formula iptakeout o o p example assuming formula MLN We denote groundings contain proposition takeout o1 0 F takeout o1 0 F takeout o1 0 takeout o1 0 o1 l1 0 takeout o1 0 o1 home 0 Likewise calculate F takeout o1 1 F takeout o1 2 F takeout o1 3 F o1 l1 0 F o1 l1 1 F o1 l1 2 F o1 l1 3 F o1 home 0 F o1 home 1 F o1 home 2 F o1 home 3 When Xl takeout o1 0 Fl F takeout o1 0 C Xlxl exp cid5 cid3 cid4 Xl xl MBxXl w f f Fl Ctakeout o1 00 e2w Ctakeout o1 01 e w w weight candidate formula Note world state DB2 cid3 f cid3 f takeout o1 0 0 MBx cid3 takeout o1 0 cid4cid4 takeout o1 0 1 MBx takeout o1 0 o1 l1 0 cid3 takeout o1 0 cid4cid4 1 0 f F takeout o1 0 F takeout o1 1 F takeout o1 2 F takeout o1 3 F o1 l1 0 F o1 l1 1 F o1 l1 2 F o1 l1 3 F o1 home 0 F o1 home 1 F o1 home 2 F o1 home 3 Finally calculate WPLLw x likewise f WPLLw x ncid5 l1 log C Xlxl C Xl0 C Xl1 e2w e2w e w log log e2w e2w 1 e2w e2w 1 ﬁrst items attained Xl takeout o1 s0 takeout o1 s1 takeout o1 s2 takeout o1 s3 respectively item attained Xl o1 l1 s0 o1 l1 s1 o1 l1 s2 o1 l1 s2 o1 l1 s3 o1 home s0 o1 home s1 o1 home s2 o1 home s2 o1 home s3 respectively Note w weight candidate formula iptakeout o o p w w candidate formula MLN By setting weight w initial value calculate w iteratively Algorithm 4 e2w e2w e w e w e w e w 8 log log log As example learning result formulas Table 4 shown Table 10 Intuitively speaking larger weight formula probable formula true world state HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1555 Table 10 An example learned weights formulas generated F1 ID 1 2 3 4 5 Weights 05 12 15 08 09 Formulas iptakeout o o p ixptakeout o x p iptakeout o isat p itakeout o o ixtakeout o x Table 11 The generated action model action preconditions effects takeoutx portable x Table 12 The formulas selected Tables 5 7 8 ID 1 2 3 Selected formulas itakeout o o itakeout o o 1 o iptakeout o isat p o p o p i1 44 Step 4 generating action models All weights MLN learning initialized zero The optimization pseudo loglikelihood ensures number true groundings f larger generally corresponding weight f higher Hence ﬁnal weight formula MLN conﬁdence measure formula Intuitively larger weight formula probable formula true world description However generating ﬁnal action model formulas need determine threshold δ based accuracy action models learned choose set formulas MLN We steps generating action models Algorithm 5 Algorithm 5 Generate action models attain_modelsW F Input A set candidate formulas F weights W Output A set action models A 1 Initialize A 2 Test choose threshold value δ based error estimates plan correctness Section 51 evaluation criteria training plan traces 3 Select formulas F cid13 4 Convert F 5 return A cid13 F corresponding weights W larger δ action models A based F1F5 For instance formula generated F1 selected predicate p formula transformed precondi tion action formula The action model generation process seen Example 7 Example 7 From result Example 6 set zero threshold select formulas weights larger zero Table 10 The result itakeout o o weight 08 After converting formula action model result shown Table 11 ellipsis represents preconditions effects learned formulas Table 10 Furthermore demonstrate generate precondition positivenegative effect positivenegative conditional effect selected formula weight larger threshold δ assume formula selected Tables 4 7 8 respectively shown Table 12 We convert formulas corresponding action model shown Table 13 The ID numbers Tables 12 13 indicate corresponding conversion relation From Table 13 precondition o attained converting ﬁrst formula Table 12 This precondition means want portable o briefcase portable o briefcase Likewise explanation effects Table 13 Notice generate precondition action takeout implication form precondition exist real action model takeout However domain trucks learning result shown Table 20 experiment section precondition action loadppackage ttruck a1truckarea llocation implication form 1556 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 13 The generated action model according Table 12 action preconditions effects takeouto portable o o forall p location isat pat o p ID 1 2 3 cid3 cid3 cid4cid4 forall a2truckarea implycloser a2 a1 free a2 t learned candidate formula ia2load p t a1 l closer a2 a1 free a2 t weight high selected LAMP generated F1 This precondition suggests order load package p area a1 truck t area a2 t close a1 free 45 Properties action models 451 Action soundness plan consistency With candidate formulas generated F1F5 constraints A1A3 like certain proper ties satisﬁed LAMP algorithm Soundness Consider set action models We actions sound apply legal chain action sequence consistent initial state preconditions actions sequence satisﬁed preceding states resulting end state consistent state considered consistent contain literal p negation p A legal action means preconditions action satisﬁed state applied Deﬁnition 1 Soundness action models An action model said sound starting initial state S sequence P built legal forward chaining action models leads consistent state Theorem 1 Soundness property Imposing constraint A1 candidate formulas F1F5 ensures action models learned sound Proof This proven induction First initial state S consistent Assume action applicable S Consider state T applied S Then suppose T inconsistent p p true T There cases In case p true S p true T p removed T execution However possible semantics PDDL In second case p p righthand rules effect domain axioms infer p p Furthermore lefthand rules L1 L2 true T This means rules L1 p L2 p effects L1 L2 true However know constraint A1 allowed Thus T consistent By induction forwardchaining sequence lead consistent state cid2 Above shown learned action models sound A related question completeness plan exists planning problem planner generate solution problem However generating action models guarantee models complete completeness property planning action model action model Thus consider completeness action models Next wish learned action models suﬃciently expressive explain training plan traces This means action models order plan trace training set obtain goal conditions initial conditions This known plan consistency property Deﬁnition 2 Plan consistency We denote plan trace s0 a0 s1 sn g si state execution action ai denote corresponding action model ai Mai We learned action models consistent plan trace following conditions satisﬁed action ai plan trace 0 cid3 cid3 n 1 preconditions Mai satisﬁed state si 2 goals g achieved executing action sequence Theorem 2 Plan consistency The action models learned LAMPare consistent training plan traces HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1557 Table 14 The size problems Domains Briefcase Elevator Openstacks Trucks Predicates Actions Plan traces Average length 3 6 9 10 3 3 5 4 100 180 200 180 13 19 31 26 Proof According Deﬁnition 2 need verify conditions consistency satisﬁed From constraint A2 know preconditions action j plan trace added action ai prior exist initial state deleted preconditions action satisﬁed Furthermore goal literal goal state added action prior goal state exist initial state goal achieved executing action sequence Thus deﬁnition consistency follows conclusion holds cid2 452 Complexity analysis Comparing LAMP algorithm previous algorithms ARMS 52 SLAF 39 learning action models note ARMS uses weightedMAXSAT learn STRIPS models action models quantiﬁer preconditions effects SLAF imposes quantiﬁers learning result particular existential quantiﬁers appear preconditions universal quantiﬁers appear effects Such constraint arbitrary strong realworld action domains Our algorithm LAMP learns quantiﬁers way conforms deﬁnition PDDL preconditions quantiﬁed universal quantiﬁers effects literals quantiﬁed universal quantiﬁers effects conditional effects quantiﬁed existential universal quantiﬁers condition universal quantiﬁers effect Besides LAMP learns models implications preconditions makes learned action models expressive We analyze time complexity LAMP algorithm The running time LAMP algorithm depends running time step algorithm In ﬁrst step running time O tlg t denotes number plan traces l denotes maximum length plan traces g denotes maximal number propositions states including intermediate states initial states goal states In second step running time O apn n maximal number predicates form conditional effect implication formula p represent number predicates number actions respectively It takes O mntlgf step f denotes number formulas m denotes number iterations Finally takes O f fourth step Thus total running time LAMP O tlg O apn O mntlgf O f O mntlgf apn generally smaller mntlg f Likewise space complexity fourth steps O tlg O apn O ntlgf O f respectively Thus space complexity LAMP O tlg O apn O ntlgf O f O ntlgf apn generally smaller O ntlgf Comparing computational complexity note complexity SLAF O sk2P k1 s equivalent tl k minimum number preconditions kDNF form P set possible ﬂuents P larger g f Thus time complexity LAMP lower SLAF m assumed constant 5 Experiments 51 Datasets evaluation criteria To evaluate LAMP collected plan traces following planning domains briefcase elevator openstacks trucks briefcase homepage IPP1 elevator second International Planning Competition IPC25 openstacks trucks ﬁfth International Planning Competition IPC5 These domains char acteristics need evaluate LAMP algorithm briefcase elevator domains quantiﬁed conditional effects openstacks trucks quantiﬁers implications preconditions Using FF planner6 generated 100 plan traces briefcase 180 plan traces elevator 200 plan traces openstacks 180 plan traces trucks solving planning problems domain Notice number plan traces depends number planning problems downloaded IPP IPC2 IPC5 We size learning problems experiment Ta ble 14 second columns numbers different predicates actions domain fourth column number plan traces collected domain column average length plan traces domain We use plan traces training data learn action models use corresponding handwritten action models IPP IPC2 IPC5 ground truth action models compare learned action models The comparison gives error rates This method evaluation allows generate action models algorithm compare 5 httpwwwcstorontoeduaips2000 6 httpmembersderiatjoerghffhtml 1558 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 results handcrafted ones IPC5 collection The accuracy obtained gives assurances effectiveness action models learn real world observed states activity sequences sensors We deﬁne error rates learning algorithm difference learned action models hand written action models considered ground truth If precondition appears precondition list learned action model precondition list corresponding handwritten action model error count preconditions denoted Epre increases If precondition appears precondition list handwritten action model precondition list corresponding learned action model E pre increases Likewise error count actions effects denoted E eff Different quantiﬁers implications account difference Note preconditions action conjunctive viewed set element implication pre set pre universally quantiﬁed literal We denote set possible preconditions action model L0 preconditions learned action model L1 Then error count preconditions calculated comparing L1 pre set preconditions ground truth action model L2 pre Epre L1 pre L2 L1 L2 L2 pre pre pre pre Likewise effects action model conjunctive viewed set element eff eff Then universally quantiﬁed literal conditional effect We denote set possible effects action model L0 set effects learned action model L1 error count effects calculated E eff L1 eff L1 eff Furthermore denote total number possible preconditions effects action model T pre Here constrain implications conditional effects eff set effects ground truth action model L2 T eff respectively Then T pre L0 pre constant number different literals T pre T eff ﬁnite T eff L0 eff L2 eff L2 eff In experiments error rate action model deﬁned cid7 cid8 Ra 1 2 Epre T pre Eeff T eff assume error rates preconditions effects equally important range error rate Ra 0 1 Furthermore error rate action models A domain deﬁned R A 1 A cid5 A Ra A number As elements Using deﬁnition error rate present experimental results subsection We evaluate LAMP respect following criteria 1 relationship accuracy percentage observed intermediate states 2 relationship accuracy percentage propositions state 3 relationship accuracy number plan traces actionmodel learning 4 running time 5 human effort saved LAMP 6 application LAMP software requirement engineering 7 example output The detailed description criterion given subsection 52 Experimental results 521 Relationship accuracy percentage observed intermediate states To simulate partial observation actions plan trace plan traces randomly selected observed states speciﬁc percentage observations 1 5 likewise percentage values randomly selected observation ﬁve consecutive states plan trace We ran selection process ﬁve times Each time LAMP algorithm generated learned action models calculated error rate Finally calculated average error rate plan traces The results tests shown Fig 1 2 1 For percentage value 1 3 1 4 1 5 1 Fig 1 shows performance LAMP algorithm respect different threshold values δ selecting candidate formulas step algorithm set 001 01 05 10 respectively From results ﬁnd performance sensitive choice threshold threshold values large small A threshold large miss useful candidate formulas threshold small bring noisy candidate formulas affect overall accuracy algorithm In experiments seen threshold set 05 mean average accuracy optimal Fig 1II Furthermore error bars conﬁdence intervals algorithm performance stable We like know relationship accuracy learned model percentage observed intermediate states Since LAMP algorithm require intermediate state information seen learn useful information observations In experiment chose different percentages observations 1 2 1 produced corresponding errorrate results Our experimental results given Fig 1 cases observations lower error rate consistent intuition However cases threshold δ set 10 1 3 states observed error rate lower case 1 2 states given These cases consistent intuition possible observations obtained weights corresponding 4 1 3 1 5 1 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1559 Fig 1 The error rates respect different percentages observations formulas weights formulas considering overall learning process Thus threshold δ set 10 formulas chosen missed error rate higher Thus conclude cases need reduce value threshold correspondingly error rate decrease 522 Relationship accuracy percentage propositions state Another aspect partial observations simulated reducing percentage propositions known true state In section performed tests aspect partial observation We ﬁrst set percentage observations 1 3 tested different percentages propositions state calculate corresponding errors The propositions state randomly selected speciﬁc percentage value generate observations The action models learned evaluated ground truth models The results shown Fig 2 value 20 xcoordinate means 20 propositions state given plan trace likewise 40 60 80 From Fig 2 ﬁnd hand setting candidateselection threshold value δ 05 shown Fig 2II error rate generally lower thresholds shown Fig 2 I III IV consistent results Fig 1 On hand percentage propositions increases error rate generally decreases This explained larger percentage information available attained learning algorithm help improve learning result From Figs 1 2 ﬁnd error rate domain elevator generally larger domains suggests diﬃcult learn We observe elevator actions stop contain conditional effects From F4 F5 know conditional effects need literals represent conditions preconditions positive negative effects seen F1 F3 This fact conditional effects diﬃcult learn That error rate elevator larger The similar result briefcase conditional effects elevator openstacks trucks Its error rate generally larger openstacks trucks smaller elevator 523 Relationship accuracy number plan traces actionmodel learning To error rate affected number plan traces different number plan traces training data evaluate performance In tests assumed plan trace 1 5 fully observed interme diate states These observed states randomly selected The process generating state observations repeated ﬁve times time error rate generated different selections 1560 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Fig 2 The error rates respect different percentages propositions state We results Fig 3 suggests error rate generally decreases number plan traces increases At beginning error rate decreases quickly eventually goes slowly This means error rate sensitive number plan traces small large When comparing different curves ﬁgure error rate generally lower threshold δ 05 thresholds consistent result Fig 1 524 Running time We tested different number plan traces obtain corresponding CPU time learning The results shown Fig 4 percentage observed intermediate states set 1 5 From Fig 4 IIV CPU time goes number plan traces increases To relationship CPU time number plan traces ﬁt running result domain briefcase nonlinear curve shown Fig 5 The functional form curve Fig 5 00010x3 00795x2 147737x 1475333 means CPU time increases polynomially respect number plan traces A similar result domains elevator openstacks trucks 525 Human effort saved LAMP The question use LAMP create action models human effort reduced compared asking human experts manually encode domains scratch We invited groups people having size 10 build action models We took care separate groups testing Case 1 Case 2 Testers ﬁrst group given initial action models created action models scratch Testers second group given action models learned LAMP asked revise models These groups testers consisted people 21 31 years age In ﬁrst group people male female In second group people male female Among people ﬁrst group students faculties universities remaining companies In second group seven people universities companies All people general knowledge AI Planning PDDL language We interested seeing difference groups quality human effort action model construction HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1561 Fig 3 The error rates respect different number plan traces percentage observed intermediate states 1 5 Fig 4 The CPU time respect different number plan traces 1562 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Fig 5 The ﬁtting result CPU time briefcase Fig 6 Time saved error rate reduced LAMP For test case recorded testers individual times spent model building calculated corresponding error rates compared groundtruth models We present results Fig 6 indicates testing results ﬁrst case indicates ones second case Notice results shown Fig 6 average results people group From Fig 6I ﬁnd time cost ﬁrst case lower second case domains briefcase elevator openstacks trucks Similar results error rates Fig 6II In summary experiment LAMP algorithm help reduce human effort improve model quality construction action models 526 Comparison grounded actions In order method error counting learned actions quantiﬁers implications reasonable section compare error rates obtained grounded actions We test differences hand crafted action models learned action models grounding quantiﬁers implications conditional effects The results STRIPS action models For instance action model Table 2 grounded initial state plan trace 2 Table 1 result shown following home l1 preconditions effects isat home isat l1 isat home quantiﬁer conditional effect grounded results STRIPS action We count differences grounded handcrafted action models grounded learned action models generate error rates described Section 51 We longer need count differences quantiﬁers conditional effects We learn action models setting threshold δ 05 percentage observed intermediate states 1 5 We wish test differences learned action models handcrafted action models grounding plan HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1563 Table 15 Error rates actions quantiﬁers R A error rates grounded actions R cid13 A cid13 A R R A Briefcase 011 013 Elevator 014 015 Openstacks 008 010 Trucks 009 009 traces We collected 20 testing plan traces fully observed intermediate states domain including briefcase elevator openstacks trucks ground learned action models handcrafted action models states We calculate average error rate denoted R cid13 A STRIPS actions The result shown Table 15 cid13 A gives error rates generated grounding R A gives error rates generated cid13 A generally smaller R A We quantiﬁed actions From Table 15 error rate R table methods error counting result values error rates This justiﬁes previously error rate calculation method considering quantiﬁed actions In Table 15 R 527 Applying LAMPto software requirement engineering To demonstrate realworld application LAMP algorithm consider problem acquire software requirement speciﬁcation specifying onlineservice business process previous work 58 The software based actions learned LAMP operational As example successfully applied Bookstore System Guangxi province China A software requirement speciﬁcation complete description behaviors developed It includes set user cases interactions users In section main idea LAMP acquire software requirement speciﬁcation given follows 1 We ﬁrst extracted types objects belonged predicates represented relations types action schemas represented behaviors 2 After collected plan traces business process communicating business personnel 3 Finally learned action models predicates action headings plan traces input converted learned result software requirement speciﬁcation In following present example LAMP acquire software requirement speciﬁcation Bookstore Ordering System Example 8 A Bookstore Ordering System separated levels 1 Client ordering client orders books base store based list books generates ini tial_ordering_form 2 Base store ordering staff base store orders books province store based initial ordering forms generates base_ordering_form 3 Province store ordering staff province store orders books book supplier generates province_ordering_form The types predicates action headings extracted Bookstore Ordering System shown follows form x initial state form x waiting state form x dealt staff x free staff x busy staff form department object lbook ioform boform poform form init x form waiting x form dealt x form free x staff busy x staff x staff y form belong x staff y department types bstore pstore sdep department predicates action headings ClientOrder x staff y bstore ioform BaseOrder x staff y pstore boform ProvinceOrder x staff y sdep poform OrderDone x staff form b form staff x dealing form y staff x belongs department y 1564 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 16 Action models Bookstore Ordering System ClientOrder x staff y bstore ioform precondition effect free x init belong x y busy x free x init x forall b lbook waiting b x bnot waiting b BaseOrder x staff y pstore boform free x init x belong x y busy x free x init forall b ioform waiting b x bnot waiting b ProvinceOrder x staff y sdep poform free x init belong x y busy x free x init forall b boform waiting b x bnot waiting b OrderDone x staff form b form dealt b waiting busy x x x b dealt waiting b free x dealt b busy x x x b precondition effect precondition effect precondition effect staff form department types staff form department Bookstore Ordering System lbook ioform boform poform types list book initial ordering form base ordering form province ordering form respectively bstore pstore sdep types base store province store suppliers department respectively Corresponding business processes client ordering base store ordering province store ordering action headings ClientOrder BaseOrder ProvinceOrder Since executing actions process need action stop process BookDone Next collected plan traces book ordering domain An example plan trace shown follows Notice plan trace use composed initial state action sequence goal intermediate state This save time collecting plan traces init belong staff1 basestore1 belong staff2 provincestore1 belong staff3 sdep1 waiting lbook1 init ioform1 init boform1 init poform1 free staff1 free staff2 free staff3 actions goal dealt lbook1 dealt ioform1 dealt boform1 waiting poform1 ClientOrder staff1 basestore1 ioform1 OrderDone staff1 lbook1 ioform1 BaseOrder staff2 provincestore1 boform1 OrderDone staff2 ioform1 boform1 ProvinceOrder staff3 sdep1 poform1 OrderDone staff3 boform1 poform1 With extracted predicates action headings 20 plan traces input ran LAMP algorithm learn action models We example running result Table 16 italicized emphasized means deleted emphasized means added This action model ClientOrder means process client ordering executed staff x belongs base store free initial ordering form true initial state As result execution staff x busy book lists b dealt state waiting Likewise statements action models These statements software requirement speciﬁcation Bookstore Ordering System means software requirement speciﬁcation attained LAMP 528 Example output To help reader intuitive idea learned action models present resulting models learned LAMP algorithm setting threshold δ 05 percentage observed intermediate states 1 5 The results domains briefcase elevator openstacks trucks shown Tables 1720 condition italic means needed HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1565 Table 17 The action models learned domain briefcase action preconditions effects action preconditions effects action preconditions effects movem location l location isat m isat l isat l isat m forall x portable x x l forall x portable x x m takeoutx portable x x forall l location x lisat l putinx portable l location x x l isat l x isat l Table 18 The action models learned domain elevator action preconditions effects action preconditions effects action preconditions effects stopf ﬂoor liftat f forall p passenger boarded p destin p fnot boarded p forall p passenger boarded p destin p f served p forall p passenger origin p f notserved p boarded p liftat f upf1 ﬂoor f2 ﬂoor liftat f1 f1 f2 liftat f2 liftat f1 forall p boarded pdestin p f2 downf1 ﬂoor f2 ﬂoor liftat f1 f2 f1 liftat f2 liftat f1 f2 f1 forall p boarded porigin p f1 handwritten domain successfully learned learned result condition bold means needed handwritten domain incorrectly learned learned result In Tables 1720 missing conditions suggest corresponding weights high selected LAMP information constraints provided training data suﬃcient weights high additional conditions suggest corresponding weights high selected wrongly LAMP information constraints weights low Although results percent correct close handwritten action models real action models These results submitted human editors work domain analysis For instance Table 17 isat m isat l preconditions time briefcase places time according domain analysis guide remove ﬁnally remove isat l analysis domain Because low error rates human experts need spend lot time creating new domain based learning result 53 Discussion In subsection summarize major ﬁndings experimental results I In experiments ﬁrst validated performance LAMP algorithm varying proportion observed intermediate states From Fig 1 algorithm performance vary signiﬁcantly change observed state numbers 100 20 In general intermediate states observe better performance When validate experiment state perspective observe proportion propositions similar experiment results We vary percent propositions true state obtained similar results 1566 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 Table 19 The action models learned domain openstacks action preconditions effects action preconditions effects action preconditions effects action preconditions effects action preconditions effects setupmachinep product avail count machineavailablestacksavail availnot p forall o orderimply includes o p started o machineavailable machineconﬁgured p makeproductp product avail count machineconﬁgured pstacksavail avail forall o orderimply includes o pstarted o machineavailablenot machineconﬁgured p p startordero order avail newavail count waiting ostacksavail avail nextcount newavail avail forall p product imply includes o p p started ostacksavail newavail waiting onot stacksavail avail shipordero order avail newavail count started ostacksavail availnextcount avail newavail forall p productimply includes o p p shipped o stacksavail newavailnot started o nextcount avail newavail stacksavail avail opennewstackopen newopen count stacksavail opennextcount open newopen machineavailable stacksavail newopen stacksavail open nextcount open newopen Table 20 The action models learned domain trucks action preconditions effects action preconditions effects action preconditions effects action preconditions effects loadp package t truck a1 truckarea l location t l p l free a1 t forall a2 truckarea imply closer a2 a1 free a2 t p t a1not p lnot free a1 t unloadp package t truck a1 truckarea l location t l p t a1 free a1 t forall a2 truckareaimply closer a2 a1 free a2 t p t a1 free a1 t p l drivet truck location t1 t2 time t connected timenow t1 t1 t2 timenow t2 t t1 t2 t timenow t1 deliverp package l location t1 t2 time p l timenow t1 le t1 t2next t1 t2 delivered p l t2not t1 t2 atdestination p lnot p l II When vary parameter values candidate selection performance varies Choosing good threshold value important overall performance LAMP general threshold value large small lead signiﬁcant reduction performance refer Fig 1 Fig 2 As empirical result experiments best threshold value 05 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1567 III From Figs 1 2 notice experimental domains elevator diﬃcult learn domains This domains elevator actions contain conditional effects effects diﬃcult learn IV In experiment validate eﬃciency LAMP decreases plan traces refer Fig 3 We record running time use different numbers plan traces given input This running time information ﬁt curve empirically shows LAMP polynomial time complexity refer Fig 5 respect number plan traces V We LAMP algorithm help reduce human efforts creating action models seen Fig 6 Furthermore example application LAMP algorithm refer Example 8 LAMP applied realworld domains assoftware requirement engineering 6 Conclusions future work In paper presented novel approach learn action models quantiﬁers logical implications set observed plan traces support partially observable intermediate states Our LAMP learning algorithm makes use Markov Logic Networks learn action models automatically Our empirical tests planning domains LAMP algorithm effective We list possible directions follow future work Our current LAMP algorithm enumer ate possible preconditions effects candidate formulas according speciﬁc correctness constraints When actions predicates planning domain candidate formulas decrease eﬃciency LAMP In future consider incorporate domain knowledge ﬁlter im possible candidate formulas algorithm eﬃcient Domain analysis help reduce errors providing priori conditions important domain experts point view Another direction improve quality weight learning MLN Currently adopt generative learning approach LAMP maximize weighted pseudo loglikelihood Other weight learning approaches discriminative learning additional advantages A direction extend action model learning learn elaborate action representations including resources functions Finally consider plan traces contain false observations actions states ways ﬁlter noise training data Acknowledgements We thank support RGCNSFC project N_HKUST62409 research We grateful helpful comments editors reviewers journal References 1 Eyal Amir Learning partially observable deterministic action models Proceedings Nineteenth International Joint Conference Artiﬁcial Intelligence IJCAI 2005 2005 pp 14331439 2 Scott Benson Inductive learning reactive action models Proceedings Twelfth International Conference Machine Learning ICML 1995 1995 pp 4754 3 Julian Besag Statistical analysis nonlattice data The Statistician 24 1975 179195 4 Jim Blythe Jihie Kim Surya Ramachandran Yolanda Gil An integrated environment knowledge acquisition Proceedings Sixth International Conference Intelligent User Interfaces IUI 2001 2001 pp 1320 5 Brian Borchers Judith Furman A twophase exact algorithm MAXSAT weighted MAXSAT problems Journal Combinatorial Optimiza tion 2 4 1998 299306 6 Lonnie Chrisman Abstract probabilistic modeling action Proceedings First International Conference Artiﬁcial Intelligence Planning Systems AIPS 1992 1992 pp 2836 7 Pedro Domingos Mining Social networks viral marketing IEEE Intelligent Systems 20 1 2005 8082 8 Pedro Domingos Toward knowledgerich data mining Data Mining Knowledge Discovery 15 2007 2128 9 Pedro Domingos Stanley Kok Hoifung Poon Matthew Richardson Parag Singla Unifying logical statistical AI Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 2006 pp 27 10 Richard Fikes Nils J Nilsson STRIPS new approach application theorem proving problem solving Artiﬁcial Intelligence 2 34 1971 189208 11 Maria Fox Derek Long PDDL21 extension PDDL expressing temporal planning domains Journal Artiﬁcial Intelligence Research JAIR 20 2003 61124 12 Krzysztof Z Gajos Daniel S Weld Jacob O Wobbrock Decisiontheoretic user interface generation Proceedings TwentyThird AAAI Confer ence Artiﬁcial Intelligence AAAI 2008 2008 pp 15321536 13 Malik Ghallab Adele Howe Craig Knoblock Drew McDermott Ashwin Ram Manuela Veloso Daniel Weld David Wilkins PDDLthe planning domain deﬁnition language httpwwwinformatikuniulmdekiEduVorlesungenGdKIWS0203pddlpdf 1998 14 Malik Ghallab Dana Nau Paolo Traverso Automated Planning Theory Practice Morgan Kaufmann 2004 15 Yolanda Gil Learning experimentation incremental reﬁnement incomplete planning domains Proceedings Eleventh International Conference Machine Learning ICML 1994 1994 pp 8795 16 Thomas Hernandez Subbarao Kambhampati Integration biological sources current systems challenges ahead SIGMOD Record 33 3 2004 5160 17 Jorg Hoffmann Piergiorgio Bertoli Marco Pistore Web service composition planning revisited background theories initial state uncertainty Proceedings TwentySecond AAAI Conference Artiﬁcial Intelligence AAAI 2007 2007 pp 10131018 1568 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 18 Michael P Holmes Charles Lee Isbell Jr Schema learning experiencebased construction predictive action models Advances Neural Informa tion Processing Systems vol 17 NIPS 2004 2004 19 Derek Hao Hu Qiang Yang CIGAR concurrent interleaving goal activity recognition Proceedings TwentyThird AAAI Conference Artiﬁcial Intelligence AAAI 2008 2008 pp 13631368 20 Tuyen N Huynh Raymond J Mooney Discriminative structure parameter learning Markov logic networks Proceedings TwentyFifth International Conference Machine Learning ICML 2008 2008 pp 416423 21 Stanley Kok Pedro Domingos Learning structure Markov logic networks Proceedings TwentySecond International Conference Machine Learning ICML 2005 2005 pp 441448 22 Stanley Kok Pedro Domingos Extracting semantic networks text relational clustering Proceedings Nineteenth European Conference Machine Learning ECML 2008 2008 pp 624639 23 Stanley Kok Parag Singla Matthew Richardson Pedro Domingos The Alchemy System Statistical Relational AI University Washington Seattle 2005 24 Ugur Kuter Evren Sirin Bijan Parsia Dana Nau James Hendler Information gathering planning Web Service composition Journal Web Semantics JWS 3 23 2005 183205 25 Hector J Levesque Fiora Pirri Raymond Reiter Foundations situation calculus Electronic Transactions Artiﬁcial Intelligence 2 1998 159 178 26 Dong C Liu Jorge Nocedal On limited memory BFGS method large scale optimization Mathematical Programming 45 1989 503528 27 Daniel Lowd Pedro Domingos Eﬃcient weight learning Markov logic networks Proceedings Eleventh European Conference Principles Practice Knowledge Discovery Databases PKDD 2007 2007 pp 200211 28 Lilyana Mihalkova Raymond J Mooney Bottomup learning Markov logic network structure Proceedings TwentyFourth International Conference Machine Learning ICML 2007 2007 pp 625632 29 Stephen Muggleton Luc De Raedt Inductive logic programming theory methods Journal Logic Programming 1920 1994 629679 30 Megan Nance Adam Vogel Eyal Amir Reasoning partially observed actions Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 2006 pp 888893 31 Tim Oates Paul R Cohen Searching planning operators contextdependent probabilistic effects Proceedings Thirteenth National Conference Artiﬁcial Intelligence AAAI 1996 1996 pp 865868 32 Hanna M Pasula Luke S Zettlemoyer Leslie Pack Kaelbling Learning probabilistic relational planning rules Proceedings Fourteenth Interna tional Conference Automated Planning Scheduling ICAPS 2004 2004 pp 7382 33 Hanna M Pasula Luke S Zettlemoyer Leslie Pack Kaelbling Learning symbolic models stochastic domains Journal Artiﬁcial Intelligence Re search 29 2007 309352 34 Hoifung Poon Pedro Domingos Joint inference information extraction Proceedings TwentySecond National Conference Artiﬁcial Intelligence AAAI 2007 2007 pp 913918 35 Matthew Richardson Pedro Domingos Markov logic networks Machine Learning 62 12 2006 107136 36 Gunther Sablon Maurice Bruynooghe Using event calculus integrate planning learning intelligent autonomous agent Current Trends AI Planning 1994 pp 254265 37 Matthew D Schmill Tim Oates Paul R Cohen Learning planning operators realworld partially observable environments Proceedings Fifth International Conference Artiﬁcial Intelligence Planning Systems AIPS 2000 2000 pp 246253 38 Shahaf Dafna Eyal Amir Learning partially observable action schemas Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 2006 pp 913919 39 Shahaf Dafna Allen Chang Eyal Amir Learning partially observable action models eﬃcient algorithms Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 2006 pp 920926 40 Parag Singla Pedro Domingos Entity resolution Markov logic Proceedings Sixth IEEE International Conference Data Mining ICDM 2006 2006 pp 572582 41 Parag Singla Pedro Domingos Markov logic inﬁnite domains Proceedings TwentyThird Conference Uncertainty Artiﬁcial Intelligence UAI 2007 2007 pp 368375 42 Thomas J Walsh Michael L Littman Eﬃcient learning action schemas webservice descriptions Proceedings TwentyThird AAAI Conference Artiﬁcial Intelligence AAAI 2008 2008 pp 714719 43 Jue Wang Pedro Domingos Hybrid Markov logic networks Proceedings TwentyThird AAAI Conference Artiﬁcial Intelligence AAAI 2008 2008 pp 11061111 44 Stephen Cresswell Thomas Leo McCluskey Margaret Mary West Acquisition objectcentred domain models planning examples Proceedings Nineteenth International Conference Automated Planning Scheduling ICAPS 2009 2009 45 Ron M Simpson Diane E Kitchin TL McCluskey Planning domain deﬁnition GIPO Knowledge Engineering Review 22 2 2007 117134 46 TL McCluskey Donghong Liu Ron M Simpson GIPO II HTN planning toolsupported knowledge engineering environment Proceedings Thirteenth International Conference Automated Planning Scheduling ICAPS 2003 2003 pp 92101 47 Elly Winner Manuela Veloso Analyzing plans condition effects Proceedings Sixth International Conference AI Planning Schedul ing AIPS 2002 2002 48 Tessa Lau Pedro Domingos Daniel S Weld Version space algebra application programming demonstration Proceedings Seventeenth International Conference Machine Learning ICML 2000 Morgan Kaufmann San Francisco CA 2000 pp 527534 49 RM Simpson TL McCluskey W Zhao RS Aylett C Doniat GIPO integrated graphical tool support knowledge engineering AI planning Proceedings European Conference Planning Toledo Spain September 2001 50 Xuemei Wang Learning observation practice incremental approach planning operator acquisition Proceedings Twelfth Inter national Conference Machine Learning ICML 1995 1995 pp 549557 51 Qiang Yang Wu Kangheng Yunfei Jiang Learning action models plan examples incomplete knowledge Proceedings Fifteenth International Conference Automated Planning Scheduling ICAPS 2005 2005 pp 241250 52 Qiang Yang Wu Kangheng Yunfei Jiang Learning action models plan examples weighted MAXSAT Artiﬁcial Intelligence 171 23 2007 107143 53 Zhuo Hankui Qiang Yang Lei Li Transfer learning action models measuring similarity different domains Proceedings Thirteenth PaciﬁcAsia Conference Knowledge Discovery Data Mining PAKDD 2009 2009 pp 697704 54 Zhuo Hankui Qiang Yang Lei Li Transferring knowledge domain learning action models Proceedings Tenth Paciﬁc Rim International Conference Artiﬁcial Intelligence PRICAI 2008 2008 pp 11101115 55 Zhuo Hankz Hankui Derek Hao Hu Chad Hogg Qiang Yang Hector MunozAvila Learning HTN method preconditions action models partial observations Proceedings Nineteenth International Joint Conference Artiﬁcial Intelligence IJCAI 2009 2009 pp 18041810 56 Qiang Yang Activity recognition linking lowlevel sensors highlevel intelligence Proceedings Nineteenth International Joint Conference Artiﬁcial Intelligence IJCAI 2009 2009 pp 2025 HH Zhuo et al Artiﬁcial Intelligence 174 2010 15401569 1569 57 Martin Gudgin Marc Hadley Noah Mendelsohn JeanJacques Moreau Henrik Frystyk Nielsen Anish Karmarkar Yves Lafon SOAP Version 12 httpwwww3orgTRsoap12part1 58 Zhuo Hankui Lei Li Qiang Yang Rui Bian Learning action models quantiﬁed conditional effects software requirement speciﬁcation Proceedings Fourth International Conference Intelligent Computing ICIC 2008 2008 pp 874881 59 Dana Nau Au TszChiu Okhtay Ilghami Ugur Kuter Hector MunozAvila J William Murdock Dan Wu Fusun Yaman Applications SHOP SHOP2 IEEE Intelligent Systems 2005 3441 60 Jie Yin Xiaoyong Chai Qiang Yang Highlevel goal recognition wireless LAN Proceedings Nineteenth National Conference Artiﬁcial Intelligence AAAI 2004 2004 pp 578584