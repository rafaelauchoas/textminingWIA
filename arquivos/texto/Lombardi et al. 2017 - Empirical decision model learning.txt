Artiﬁcial Intelligence 244 2017 343367 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Empirical decision model learning Michele Lombardi DISI University Bologna Italy b DEI University Bologna Italy Michela Milano Andrea Bartolini b r t c l e n f o b s t r c t Article history Received revised form 21 December 2015 Accepted 10 January 2016 Available online 13 January 2016 Keywords Combinatorial optimization Machine learning Complex systems Local search Constraint programming Mixed integer nonlinear programming SAT modulo theories Artiﬁcial neural networks Decision trees One biggest challenges design realworld decision support systems coming good combinatorial optimization model Often accurate predictive models simulators devised complex slow employed combinatorial optimization In paper propose methodology called Empirical Model Learning EML relies Machine Learning obtaining components prescriptive model data extracted predictive model harvested real In way EML considered technique merge predictive prescriptive analytics All models introduce form approximation Citing GEP Box 1 Essentially models wrong useful In EML models useful provide adequate accuracy effectively exploited solvers ﬁnding highquality solutions We ground EML case study thermalaware workload dispatching We use learning methods Artiﬁcial Neural Networks Decision Trees encapsulate learned model number optimization techniques Local Search Constraint Programming Mixed Integer NonLinear Programming SAT Modulo Theories We demonstrate effectiveness EML approach comparing results obtained expertdesigned models 2016 Elsevier BV All rights reserved 1 Introduction Advances Combinatorial Optimization methods decades enabled successful application broad range industrial problems Many approaches rely availability declarative description This typically consists handcrafted mathematical model obtained thorough discussion domain experts introducing simplifying assumptions Devising good model complex task especially challenging dealing realworld systems A good model ﬁnds proper balance model complexity model accuracy hand excessive simpliﬁcation lead optimal completely useless solutions On hand incorporating details results extremely hard computational issues Despite number successful optimization approaches proposed literature Corresponding author Email addresses michelelombardi2uniboit M Lombardi michelamilanouniboit M Milano abartoliniuniboit A Bartolini httpdxdoiorg101016jartint201601005 00043702 2016 Elsevier BV All rights reserved 344 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 applied reallife industrial problems enabling cases1 huge savings terms resources time money machines energy Nevertheless systems impervious approaches Mixed Integer Linear Programming MILP Con straint Programming CP SAT propositional SATisﬁability modeling issues There basically kinds highcomplexity systems outofreach traditional combinatorial approaches 1 Complex Systems exhibit phenomena emerge collection interacting objects capable selforganization affected memory feedback 2 physical systems dynamic model known embedding combinatorial model computationally intractable A common way supporting decisionmaking systems design predictive model simulator based real data use whatif analysis 2 recent reference In whatif analysis decision maker repeatedly feeds scenarios sets decisions predictive model extract values certain observables quality measures Inevitably limited number scenarios investigated decision maker commits showing best behavior In combinatorial problems decision space large selecting scenarios manually isolation results farfromoptimal choices The aim paper bring highcomplexity systems reach combinatorial decision making optimiza tion The idea use Machine Learning ML learn approximate relation decisions impact In particular devise methodology called Empirical Model Learning EML 1 learns relations tween decidables observables2 data 2 encapsulates relations components optimization model objective functions constraints The training data learning techniques harvested real extracted predictive model simulator The integration model components merely matter encoding cases operational semantics eﬃcient use component ﬁned The ability integrate Machine Learning models combinatorial optimization potential play major role bridging gap predictive prescriptive analytics An EML based capable suggesting optimal decisions complex realworld setting taking advantage recent developments big data analysis predictive model design This paper provides main contributions First introduce Empirical Model Learning approach general fashion Second present number methods embedding Machine Learning models Decision Trees Artiﬁcial Neural Networks optimization techniques Local Search Mixed Integer NonLinear Programming Con straint Programming SAT Modulo Theories Some embedding techniques presented previous papers 45 Third despite main idea EML simple application requires care obtaining effective optimization approach We highlight main diﬃculties suggest possible solutions applying EML approach practical examples As motivating running examples use thermalaware workload dispatching problems deﬁned ex perimental multicore Intel CPU called Singlechip Cloud Computer SCC 6 Both problems consist mapping set heterogeneous jobs platform cores maximize cost metric involving platform eﬃciency The eﬃciency core affected number complex factors including thermal dynamics chip workload distribution presence lowlevel schedulers thermal controllers Although accurate simulator platform available inserted decision model high complexity large run time We EML allows considerable improvements simpler optimization approaches based expertdesigned heuristics expertdesigned models reﬁned function ﬁtting The paper structured follows Section 2 provide comparative analysis related work In Section 3 introduce example problems In Section 4 brief overview EML approach Section 5 presents techniques embedding Machine Learning models Combinatorial Optimization models Sections 6 7 discuss respectively design core combinatorial structure optimization problem extract model data cases example problems employed present process We provide experimental results Section 8 concluding remarks Section 9 2 Comparative analysis related work The EML approach combines elements Combinatorial Optimization Machine Learning Complex SystemsSimula tion In section provide brief overview approaches related integration research ﬁelds Loosely related approaches Researchers interested long time integration optimization techniques Machine Learning This surprising given training problems fundamentally peculiar optimization prob lems Works 78 studied core optimization problems ML algorithms proposed eﬃcient methods 1 The reader ﬁnd examples web page dedicated Franz Edelman Award https wwwinforms org RecognizeExcellence FranzEdelmanAward 2 The names decidables observables suggested Peter Flach 3 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 345 extracting knowledge huge volumes data Other works 910 presented 11 applied Constraint Programming Machine Learning tasks In optimization community substantial effort recently dedicated learning improve solution approach Clustering methods employed automatic algorithm selection 12 Several Machine Learning techniques predicting run time optimization algorithms aim perform algorithm selection 1314 A works focused learning customized optimization problem instances testing new techniques 15 Constraint acquisition Some papers 1618 focused learning set constraints match number positive negative examples task known constraint acquisition An extension approaches represented QuAcq 19 requires partial queries subsets problem variables need positive examples Other constraint acquisition approaches surveyed 20 All approaches attempt combine constraints given library build model compatible available data Conversely EML emphasis enabling integration standard Machine Learning model Neural Network combinatorial problem Our approach requires additional design effort ﬂexible better suited dealing practical applications especially cases good Machine Learning model available Many constraint acquisition approaches use active learning candidate solutions solver eval uated simulator serve new examples training set This kind approach allows tune model speciﬁc instance solved As drawback response times impractically large evaluating data point expensive Moreover simulator available active learning require deploy intermediate solutions real unreasonable practice In work assume training entirely offline Integrating active learning method possible left subject future research Surrogate models Surrogate models 21 excellent overview approximate models They typically employed design problems declarative models diﬃcult obtain simulation viable A surrogate model takes form function predeﬁned structure unknown parameters combination kernel functions Gaussian processes Surrogate models tuned training set typically employed blackbox optimization paragraph In approaches ALAMO 22 optimization simulation jointly employed explore alternative surrogate model structures tune parameters The idea surrogate models closely related propose important differences terms focus scope ideal generality Most works surrogate models focus 1 speciﬁc class Machine Learning techniques relatively simple compositions nonlinear continuous functions minority works considered Artiﬁcial Neural Networks 2324 2 speciﬁc problems typically continuous variables range straints 3 speciﬁc solution techniques Genetic Algorithms blackboxderivativefree optimization Mixed Integer NonLinear Programming Conversely EML aim enabling use Machine Learning tech niques possible optimization methods possible goal having ability choose adequate solution approach problem As consequence focus handling integration Ma chine Learning models optimization example emphasize importance model embedding techniques Section 5 methods exploit structure extracted model boosting search process This paper speciﬁcally focus problems Machine Learning models techniques outside typical scope surrogate models The LION approach Offline learning employed LION approach 25 extract approximate cost function abundant data The authors stress practical cases user preferences involved obvious numeric approach rank solutions In situation historical data way obtain approx imate cost function repeatedly querying user solution time The LION approach relies model ﬁtting obtain closetooptimal solutions makes diﬃcult target decision problems complex combinatorial struc ture Black box optimization Blackbox optimization approaches concerned ﬁnding solutions optimization problems having cost constraint functions unknown structure typical case systems lacking declarative model simulator available This use case surrogate models fact employed black box optimization If black box function fast evaluate local search approaches metaheuristics Genetic Algorithms directly applied 26 If black box evaluation expensive evaluate frequent simulators necessary employ advanced techniques limit number simulator calls Most methods black box optimization expensive cost functions developed ﬁeld derivative free optimization Within ﬁeld book 27 makes distinction Directional Search Model based approaches Informally speaking Directional Search methods survey 28 rely discretization limit number black box evaluations Modelbased approaches instead employ active learning ﬁt internal surrogate 346 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 model referred response surface The internal model guide search process In 29 Sim ulation Optimization 30 internal model stochastic process allows search algorithm account estimated solution quality estimated model accuracy order identify promising solutions The OptQuest 31 integrates closed loop simulation metaheuristics relies Neural Network quick approximate solution checking Black box optimization approaches differ EML important regards The important difference blackbox optimization approaches designed problems complex combinatorial structure Hence likely ineffective inapplicable problems discrete variables nontrivial constraints This ex actly class problems techniques CP MILP SAT interesting technique tend provide best results Second blackbox optimization relies performing simulation search pro cess excessively timeconsuming impossible simulator available In EML simulation time direct impact solver performance model extraction rely historical data exper iments performed real Third EML function describes behavior black box contrary structure Empirical Model known wish exploit boosting search process 3 Motivating example As case study motivating example consider problems related thermalaware workload dispatch ing experimental multicore CPU Intel called Singlechip Cloud Computer SCC 6 The problems share combinatorial structure different wrt objective function observables In section problems identify critical diﬃculties deﬁning model outline possible solution ap proaches Problem description The SCC platform 24 dualcore tiles arranged 4 6 grid overall 48 cores 8 6 grid Each core runs independent instance Linux kernel Intertile communication occurs message passing network interface tasks easily migrate cores The chip designed accept job batches external node separate Due large number cores packed single silicon die platform prone overheating Since chip prototype Intel chosen thermally stable overly large noisy fan implementing advanced thermal controller With aim studying temperature control policies SCC researchers University Bologna domain experts case devised accurate simulator based Matlab Hotspot thermal modeling tool 32 The simulator characterizes jobs terms CPI value Clocks Per Instruction maximum frequency jobs low CPI intense use CPU generate heat jobs high CPI comparatively colder On core simulated platform thermal control approaches introduced 1 A preemptive thermalaware scheduler interleaves execution hot cold jobs effort temperature stable Jobs classiﬁed run time based effect current temperature 2 A thermal controller dramatically lowers core frequency temperature exceeds safety threshold The jobs assumed run indeﬁnitely domain experts decided avoid overloading running number jobs core Problem variants Both dispatching problems consist mapping set heterogeneous jobs platform cores maximize objective related core eﬃciencies Due presence thermal controller eﬃciency core depends temperature number complex factors including 1 workload 2 temperature cores 3 core position die 4 action thermalaware scheduler threshold controller As mentioned consider problem variants 1 In ﬁrst variant referred WDPbal goal balance platform eﬃciency avoid occurrence hot spots abnormally warm cores Together domain experts formalized objective maximizing worstcase core eﬃciency 2 In second variant referred WDPmax goal portion platform large possible operates high eﬃciency Together domain experts formalized objective maximizing number cores having eﬃciency larger certain threshold Base models As preliminary step deﬁning solution approach introduce base model example problems In base models relation mapping decisions eﬃciency captured means generic functions Formally let n m respectively number jobs cores m 48 SCC Let introduce M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 347 set binary variables xik xik 1 iff job mapped core k 0 Then possible formulation WDPbal Base Model WDPbal max z min k0m1 cid2 cid3 eff k subject eff k hbal k x k 0m 1 m1cid4 xik 1 k0 n1cid4 xik n m i0 xi 0 1 0n 1 k 0m 1 0n 1 1 2 3 4 5 Constraints 3 ensure job mapped single core Constraints 4 force number jobs nm run core3 Constraints 2 deﬁne behavior target CPU This relying set functions hbal 0 1n 0 1 associates mapping jobs value eff k variable representing eﬃciency core k A base model second problem variant formulated follows k Base Model WDPmax m1cid4 max z heff k k0 subject heff k hmax k x k 0m 1 Constraints 3 4 5 6 7 8 The model identical WDPbal cost function functions hmax behavior In particular assume heff k binary heff k threshold value Therefore hmax 0 1n 0 1 function hmax deﬁne 1 iff eﬃciency core k k k k Modeling behavior The critical step deﬁning solution approach WDPbal WDPmax ﬁnding suitable embodiment hbal hmax functions Here survey main alternatives First hbal hmax functions evaluated exact fashion simply running SCC simulator This typical scenario black box optimization Section 2 viable approach GAs Local Search solution methods Unfortunately case simulation run requires minutes acceptable response time dispatcher order tens seconds Amortizing simulation time limiting simulator calls relying internal model solve issue direct evaluations simulator simply viable Second possible use heuristic measure proxy exact behavior Heuristics kind easy embed combinatorial model More importantly heuristics easy deﬁne domain experts typically base decisions kind simple rule For example case WDPbal designer simulator suggested use average CPI Clocks Per Instruction jobs mapped core proxy eﬃciency This approach considered experimentation Section 8 The main drawback heuristic usually impossible apriori quantify degree approximation This undesirable general adverse affect solution quality Third possible deﬁne hbal hmax functions linear regression ﬁtting expertdesign model rely knowledge domain expert deﬁning model structure extract parameter values data The accuracy model evaluated training set separate test set We considered approach fact simple form model learning experimentation 3 For sake simplicity assume n multiple m In general case computing features introduced Section 72 requires introduce simple nonlinear constraints This standard modeling constructs Local Search MINLP In CP computation Weighted Average Constraint 33 348 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Finally use Empirical Model Learning employ Machine Learning extract data structure parameters model This approach applicable structure hbal hmax obvious discussed remainder paper 4 The Empirical Model Learning approach In EML interested solving optimization problems deﬁned highcomplexity systems typically following structure min f x z st g jx z z hx xi D j J xi x 9 10 11 12 x vector decision variables xi domain D z vector observables related target We special assumption domains D general case The cost function f depend decision variables observables All problem variables subject constraints represented logical predicates g jx z The predicates corre spond classical inequalities Mathematical Programming combinatorial restrictions Global Constraints Constraint Programming alldiff element Equations 9 10 12 represent core combinatorial structure optimization problem The hx function describes approximate behavior highcomplexity speciﬁes observables depend decision variables The function h corresponds encoding Empirical Model obtained Machine Learning Designing optimization approach based EML requires care main activities 1 Deﬁning core combinatorial structure problem 2 Obtaining Machine Learning model 3 Embedding Empirical Model combinatorial problem The step critical ﬁrst presented Section 5 It possible embed Machine Learning model combinatorial problem suitable encoding deﬁned Such encoding designed exploited optimization approach boosting search process bound computation constraint propagation Step 1 consists deﬁning combinatorial model Step 2 classical Machine Learning task regression classiﬁ cation Both activities nontrivial time consuming extensively studied past In context EML require special attention discussed Sections 6 7 WDPbal WDPmax running examples 5 Embedding Empirical Model Embedding extracted EM optimization model requires 1 encode Empirical Model terms vari ables constraints 2 deﬁne operational semantics encoding By operational semantics refer procedure improve optimization process reasoning EM This operational semantics provided implicitly underlying solver convex envelope bounding MINLP solvers deﬁned explicitly encoding ad hoc ﬁltering algorithms CP In section embedding techniques types Machine Learning models Artiﬁcial Neural Networks ANNs Decision Trees DTs Combinatorial Optimization approaches Local Search LS Mixed Integer NonLinear Programming MINLP Constraint Programming CP SAT Modulo Theories SMT The exact combinations consider reported Table 1 provide technique integrating ANNs SMT solvers support nonlinear theories widely available We try encode Decision Trees MINLP require extensive linearization disjunctions likely leading poor bounds Additionally interested reader ﬁnd technique embedding Random Forests CP work 5 authors paper All encoding techniques general restricted example problems As baseline case consider embedding Machine Learning model Local Search The core idea Local Search improve iteratively incumbent solution exploring evaluating set neighbor solutions Local Search methods originally designed problems discrete variables unlike blackbox optimization methods deal nontrivial constraints violationbased cost functions incorporating constraints neighborhood deﬁnition LS approaches require ability evaluate cost constraint functions manipulate fully instantiated solutions For reasons embedding Machine Learning model requires simply implement function M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 349 Table 1 Supported combinations learning optimization techniques Artiﬁcial Neural Networks Decision Trees LS MINLP CP SMT evaluator As downside LS approaches limited ability exploit model structure boosting search process 51 Embedding artiﬁcial neural networks optimization methods We brieﬂy recall ANNs 34 comprehensive reference computational systems consisting networks basic units called artiﬁcial neurons Technically artiﬁcial neuron function vector input x scalar output y corresponding equation cid5 y φ b cid6 cid4 w xi 13 xi denotes single element x w terms input weights obtained training b bias The term φ monotonic nondecreasing nonlinear activation function Its argument known neuron activity 511 Embedding ANNs MINLP Mixed Integer NonLinear Programming MINLP 35 ﬁeld Mathematical Programming concerned ﬁnding extreme points nonlinear functions subject linear nonlinear integrality constraints Modern MINLP solvers advantage problem structure constraints cost function convex envelope approximation linearization cutting planes constraint propagation branching An ANN embedded MINLP model introducing variables model input output neuron directly inserting neuron equations model This easy long considered MINLP solver supports activation functions employed neurons Once EM encoded equations automatically taken account solver computing bounds generating cuts There aspects deserve care First MINLP solvers rely convexity providing globally optimal results Any MINLP solver converge local optimum possibly different global optimum neurons use nonlinear functions network layers Second numerical stability issue For example MINLP solvers need point invert model functions In principle activation function types inverted In practice ﬁnite precision underlying machine inversion possible restriction function domain possibly leading software crashes missed solutions The issue addressed restricting domain inputoutput variables neuron approach accidentally eliminate highquality solution risk low 512 Embedding ANNs CP Constraint Programming CP 36 Artiﬁcial Intelligence technique designed solve Constraint Satisfaction Problems CSP Constraint Optimization Problems COP A CSP deﬁned set variables subject set constraints Each constraint associated ﬁltering algorithm prune search time provably infeasible values variable domains Pruning value trigger ﬁltering algorithms process known propagation A straint solver combines ﬁltering propagation search highly customizable ﬁnd solutions combinatorial problem Optimization performed dynamically adding bounding constraints feasible solution We shown 4 ANN encoded CP introducing additional variables like MINLP encoding neuron global Neuron Constraint A Neuron Constraint signature φ y x w b 14 φ denotes activation function type y output variable x vector input variables w vector weights b bias Using constraint model neuron allows encode complex networks recurrent ones limited number basic modeling components constraint type activation function A Neuron Constraint maintains Bound Consistency Equation 13 ﬁltering algorithm capable pruning x y variables extremes domains guaranteed consistent Bound Consistency enforced Neuron Constraint ﬁltering neuron activity activation function separately Formally consider decomposition cid6 y φ y cid6 b cid4 16 15 y w xi 350 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Fig 1 An example Decision Tree cid6 y realvalued variable representing neuron activity The variable introduced explain ﬁltering method typically appear model Equation 16 linear expression classical ﬁltering techniques exist Since activation function monotonic nondecreasing Bound Consistency Equation 15 enforced means following rules cid6 updated y min y φ y y y updated y cid6 min y cid6 φ cid6 1 y 17 18 notation y y maximum y domain minima analogous cid6 cid6 The key idea domain narrowed triggers reduction domain maximum y viceversa The rules ﬁltering denotes maximum domain y y cid6 We recall φ function invertible practice restricted range values ﬁnite precision underlying machine This issue addressed replacing φ1 y Equation 18 expression maxv R φv y Moreover constraint solvers provide support realvalued variables necessary use integer variables ﬁniteprecision representation More details practical implementation issues 4 Using global constraint neuron viable CP encoding An alternative approach consists single global constraint capture network method ﬂexible provide stronger ﬁltering In 37 authors paper devised global propagator based Lagrangian relaxation common class networks twolayer feedforward ANNs For simplicity paper limit original approach constraint neuron 52 Embedding Decision Trees optimization methods Decision Trees DT type Machine Learning model typically employed classiﬁcation tasks 38 comprehensive overview Each leaf DT labeled class Each node labeled set attributes xi described DT input Attributes numeric symbolic The outgoing branches node labeled conditions attribute The conditions form partition attribute domain branch b j symbolic attribute labeled set Lb j acceptable symbolic values branch numeric attribute labeled splitting condition threshold θb j form xi θb j xi θb j A simple Decision Tree depicted Fig 1 An example classiﬁed starting root node traversing tree taking branches condition satisﬁed values attributes For fully speciﬁed example process leads single leaf corresponding predicted class 521 Embedding Decision Trees SMT Satisﬁability Modulo Theories SMT extension SAT allows include nonboolean predicates linear inequalities integer variables logical formula An SMT solver 39 combines SAT solver theory solvers providing support constraints dedicated approaches Simplex algorithm The SAT solver manages boolean representation taking care unit propagation conﬂict learning Several search strategies possible ranging lazy approaches boolean decisions instantiated theory variables complex schemes SAT theory solvers work interleaved fashion Optimiza tion performed posting bounding constraint solution restarting search process SMT allows treat constraints nonlogical domains boolean predicates enabling convenient encoding DTs First introduce variables model input attributes class let x y respectively Symbolic M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 351 attributes classes modeled integer variables Then obtain simple rulebased encoding based observation path π root leaf viewed logical implication cid7 b j π cstb j cid2 y Cπ cid3 19 notation cid2cid3 refers boolean predicate corresponding constraint enclosed brackets The term Cπ class corresponding leaf path π b j branch path Each expression cstb j form cid11 cstb j vLb j cid2xb j θb jcid3 cid2xb j θb jcid3 cid2xb j vcid3 xb j symbolic xb j numeric b j leftbranch xb j numeric b j rightbranch xb j refers case attribute variable associated branch b j Posting Clause 19 path tree suﬃcient encode DT However possible obtain formulation leading stronger propagation The key observation set leaves labeled certain class speciﬁes input conﬁgurations mapped class This allows encode tree set clauses class ch cid2 y chcid3 cid12 cid7 cstb j πkCπkch b j πk 20 In words class variable y takes value ch iff implications associated paths πk labeled ch true This formulation allows SMT solver perform powerful deductions unit propagation employ remainder paper 522 Embedding Decision Trees CP If Constraint Programming solver implementation supports logical expressions possible di rectly employ CP encoding Equation 20 Indeed encoding originally designed CP presented authors paper 5 If logical constraints supported possible obtain equivalent reformulation Equation 20 sepa rately modeling lefttoright righttoleft implications associated operator In particular righttoleft implication corresponds Equation 19 DT translates path πk cid17 b j πk cstb j cid2 y Cπkcid3 21 notation cid2cid3 refers reiﬁed constraint denotes 1 expression double square brackets true 0 expression false Informally Equation 21 forces class variable y value Cπk current domain attribute variables cstb j constraints necessarily satisﬁed Con versely y takes value Cπk conjunctions cstb j constraints true This leads class ch cid2 y chcid3 cid4 cid17 cstb j πkCπkch b j πk 22 None presented encodings capable enforcing Generalized Arc Consistency GAC GAC actually achieved CP encodings presented 5 respectively based table mdd constraint For sake simplicity paper restrict simple encodings presented refer interested reader 5 6 Design optimization model In section discuss Step 1 EML design process deﬁnition core combinatorial structure optimization problem Informally optimization model designed domain expert traditional fashion Deﬁning combinatorial structure requires identify input output Empirical Model deﬁne ﬁnal model extracted data This decision affects Machine Learning techniques extract Empirical Model 352 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 For example problems provided ﬁrst deﬁnition core combinatorial structure base models Section 3 In cases input Empirical Model given mapping variables The EM output WDPbal given eﬃciency core implies kind regression technique 1 iff core k high obtain Empirical Model The output WDPmax vector binary variables eff k eﬃciency case Empirical Model classiﬁer In remainder section present models core combinatorial structure WDPbal WDPmax variety optimization techniques LS MINLP CP SMT We refer model notation Tph T identiﬁes solution technique p problem type bal WDPbal max WDPmax The h term identiﬁes relation mapping eﬃciency variables modeled left unspeciﬁed section possible alternatives discussed Sections 7 Section 8 Core model Local Search The Local Search model WDPbal identical base model presented Section 3 reported convenience LSbalh max z min k0m1 effk subject eff k hkx empirical model k 0m 1 m1cid4 xik 1 k0 n1cid4 xik n m i0 xi 0 1 0n 1 k 0m 1 0n 1 23 24 25 26 27 recall n number jobs m number cores The hk functions represent Empirical Model discussed Section 7 Analogously LS model WDPmax LSmaxh max z m1cid4 k0 heffk subject heff k hkx empirical model k 0m 1 Constraints 25 26 27 28 29 Core model MINLP A MINLP model WDPbal obtained base model linearizing objective function MINLPbalh max z subject z effk k 0m 1 eff k hkx empirical model k 0m 1 Constraints 25 26 27 30 31 32 We design MINLP model second example problem deﬁned technique encoding MINLP Decision Tree Empirical Model employ WDPmax Section 7 Core model CP We designed CP models dispatching problems presented Section 3 Unlike LS MINLP case CP models represent mapping decisions integer variables xi 0 m 1 xi k iff job mapped core k This encoding ensures job mapped single core The model WDPbal follows M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 353 CPbalh max z min k0m1 effk subject eff k cid18 hkx empirical model k 0m 1 gcc x 0 m 1 xi 0 m 1 effk 0 1 cid19 n m 0n 1 k 0 m 1 33 34 35 36 37 Constraints 35 employ gcc Global Cardinality Constraint 40 ensure number jobs mapped core The gcc constraint forces value set 0 m 1 taken exactly nm x variables The model WDPmax identical cost function deﬁnition eﬃciency variables CPmaxh max z m1cid4 cid2 cid3 heff k k0 subject heff k hkx empirical model k 0m 1 Constraints 35 36 0 1 heff k k 0 m 1 38 39 40 41 Core model SMT We design SMT model WDPbal Empirical Model employ case Neural Network devised technique embedding ANNs SMT We deﬁne SMT model WDPmax In model represent mapping decisions integer variables xik xik 1 iff job runs core k similarly LS MINLP The resulting SMT model SMTmaxh max z m 1 cid2 cid3 heff k cid4 k0 subject heff k hkx empirical model k 0m 1 m1cid4 xik 1 k0 n1cid4 i0 xik n m cid2xik 0cid3 cid2xik 1cid3 0n 1 k 0m 1 0n 1 k 0m 1 42 43 44 45 46 cid2xik 0cid3 cid2xik 1cid3 boolean predicates associated nonboolean constraints case linear equations integer variables The boolean clauses 46 model binary domain xik variables 7 Extracting Empirical Model In section discuss Step 2 EML design process obtaining Empirical Model In principle extracting Empirical Model simply supervised learning task given training set containing known inputoutput pairs examples considered goal obtain function predict behavior unseen examples This goal achieved number powerful techniques Machine Learning domain In work limit Artiﬁcial Neural Networks ANNs Decision Trees DTs Machine Learning models embedding technique provided Section 5 In general extracting Machine Learning model requires 1 obtain training set 2 choose features input 3 choose Machine Learning technique 4 proceed training evaluation 354 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Extracting Empirical Model way step requires special care peculiarities EML context In remainder section identify peculiarities deal dispatching problems Section 3 running examples 71 EM extraction step 1 obtaining training set In EML training set obtained collecting historical data running adhoc experiments This costly operation needs performed once4 obtaining Empirical Model A good training set representative instances input conﬁgurations prediction needs In classical Machine Learning scenario set include instances likely occur practice introduce controlled bias simpliﬁes learning problem In EML instances evaluated generated optimization example problems given speciﬁc set jobs search engine try explore space possible mappings extensively possible In kind situation deﬁning input conﬁgurations likely generated extremely diﬃcult We address issue designing training set unbiased possible respect decisions search engine For example 1 collect groups jobs likely submitted 2 build training set generating mappings random cover set possible mappings uniformly possible This approach yields training set biased likely sets jobs unbiased wrt jobs mapped The training set example problems For dispatching problems learn different Machine Learning model core Using different models necessary cores nonhomogeneous behavior unique factors position chip variability manufacturing process Using model core single model platform allowed obtain high accuracy limited number inputs leading dramatic reduction training set size We built training set core generating random groups mapped jobs Then corresponding eﬃciency values obtained simulator The eﬃciency values employed directly training set WDPbal WDPmax eﬃciency discretized 01 class value 097 97 threshold The threshold chosen domain experts The eﬃciency measure consider average eﬃciency mapped jobs computed cpii cid20cpii effk 1 nm 47 cid4 job core k nm number jobs mapped core cpii nominal CPI job assumes maximum operating frequency cid20cpii job CPI measured simulation includes slowdown thermal controller The formula provides indication main factors affect core eﬃciency In particular observe 1 Because thermal controller eﬃciency core depends temperature affected running jobs stress CPU In particular known correlation temperature core average CPI running jobs Section 8 2 The temperature core subject thermal interactions cores particular nearest ones 3 The temperature core depends position silicon die affects ability disperse heat dependency taken account learning different model core 4 The temperature core affected room For sake simplicity disregard factor paper taken account example learning different models different room temperature ranges 5 Finally jobs affected thermal controller way Jobs higher cpii values spend lot clock cycles waiting memory operations conclude consequence actual CPI value affected thermal controller acts frequency CPU memory Conversely jobs low CPI affected frequency reductions Based observations obtained training sets factorial design similarly surrogate models 21 In particular core k generated multiple random sets 288 jobs 6 core cover possible values key parameters reported Table 2 Each parameter table approximation main factors affecting core eﬃciency The p0 p1 p2 parameters related average CPI jobs core proxy temperature The actual CPI values core k randomly generated following beta distribution 4 different parameterizations p3 table The CPI values remaining cores beta distributed single parameterization case chosen uniformly random considered core k 4 Unless active learning employed leave subject future research M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 355 Table 2 Factorial design training set core k Parameter Corresponding factor Values p0 p1 p2 p3 Average CPI jobs mapped core k Average CPI jobs mapped neighboring cores Average CPI jobs mapped remaining cores α β values beta distribution generating CPIs values Power consumption core k Power consumption nearby cores Power consumption faroff cores Effect individual CPI values jobs core k 3 21 3 4 α β pairs Overall core obtained 3 21 3 4 756 sets mapped jobs simulated order obtain corresponding eﬃciency values The process time consuming given simulation takes twotothree minutes computation time However training set needs generated setup performance EML based solution approach affected training time complexity extracted model All training sets publicly available5 72 EM extraction step 2 choosing input features In EML input Empirical Model consists principle decision variables In practice feeding decisions jobcore mapping directly Machine Learning model dependent problem size reduce generality method In Machine Learning issue usually addressed aggregation functions average standard deviation obtain features fed model In EML means Empirical Model constraints formulation Section 4 fact combination relations z hx equivalent cid21 y hfeatx z hEM y 48 y vector variables representing features hEM encoding Machine Learning model hfeat feature extraction constraints The need encode ﬁnal model feature extraction constraints lead accuracyeffectiveness tradeoffs example important feature diﬃcult encode optimization technique best suited combinatorial problem Input features example problems We designed tested input features based factors affecting eﬃciency identiﬁed Section 71 After attempts settled following list 1 The average CPI jobs core k avgcpik 2 The minimum CPI jobs core k mincpik 3 The average average CPI neighboring cores core hNk avgcpih neighcpik 1 cid22 Nk 4 The average average CPI cores cid22 othercpik 1 m1Nk core hcid11kh Nk avgcpih cid22 1 m minjob j k job j k cpi j cid2 cpi j cid3 Nk set cores having H distance maximum distances x y coordinates chip grid equal target k Most input features correspond directly parameters Table 2 natural The mincpik feature introduced eventually discarded ones provide Machine Learning model information individual CPI values The feature extraction constraints Local Search models maxcpi maxcpi cpii xik n1cid4 cpii xik avgcpik mincpik 1 m i0 min i0n1 1 Nk cid4 hNk neighcpik avgcpih othercpik 1 m 1 Nk cid4 hcid11kh Nk avgcpih k 0m 1 k 0m 1 k 0m 1 k 0m 1 5 In git repository https bitbucket org m _lombardi emlaij2015resources 49 50 51 52 356 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 avgcpik mincpik neighcpik othercpik realvalued variables maxcpi maximum possible CPI constant The feature extraction constraints CP models identical Local Search place xik variables use reiﬁed constraints form cid2xi kcid3 Constraints 49 51 52 directly inserted MINLP model This case Constraints 50 presence min operator It possible encode minimum operator MINLP integer variables linearization adverse effects bound quality For sake simplicity decided extract groups Empirical Models respectively mincpik input feature Only second group embedded MINLP In SMT model Section 6 linear predicates deﬁne avgcpik neighcpik othercpik obtained Con straints 49 51 52 replacing xik variables LIA Linear Integer Arithmetic predicates cid2xik 1cid3 For deﬁning feature extraction constraints mincpik ﬁrst build vector s containing job indices sorted increasing CPI value Then post hybrid linearlogical predicate mincpik itecid2xs0 1cid3 cpis0 itecid2xs1 1cid3 cpis1 53 ite ifthenelse statement ﬁrst parameter condition test second parameter expressions denoted condition respectively true false Normalization For Machine Learning approaches Artiﬁcial Neural Networks employ section beneﬁcial normalize input features ﬁxed range typically 1 1 In case formula cid6 avgcpi k 1 span1 avgcpik span0 k 0m 1 54 maxcpi mincpi2 avgcpik analogously features In formula span0 The value mincpi represents minimum possible CPI value assumed 0 slightly conservative fashion CPI values strictly positive The normalization Formula 54 included feature extraction constraints approaches ANNs Empirical Model maxcpi mincpi2 span1 73 EM extraction step 3 training quality assessment Designing training model studied topics Machine Learning 41 excellent overview Both steps require assess quality extracted model measured metrics Mean Squared Error number correctly classiﬁed instances The assessment typically separate test set crossvalidation In EML fact input Machine Learning model generated search approach makes quality assessment complicated Suppose example extracted model large prediction error certain range values input features frequent training set If values happen favorable terms cost problem model input conﬁgurations despite uncommon training set actively sought optimizer likely appear search time Because actual error ﬁnal chance similar maximum error test set This issue addressed example maximum error evaluation metric training Alter natively dynamically generate new examples similar ones leading largest errors repeat training For sake simplicity paper limit classical training techniques means example use Mean Squared Error evaluate quality ANNs training time Since Mean Squared Error assigns higher penalty errors large absolute value tends mitigate issue described In traditional Machine Learning applications evaluating model making prediction usually simple operation In EML extracted model simply evaluated exploited number ways order boost search process This makes size complexity extracted model particularly important determine computation effort required bounding constraint propagation This lead tradeoff model size accuracy critical ﬁnal optimization operates tight time constraints For example problems possible obtain accurate small EMs leave thorough evaluation tradeoff future research Empirical models WDPbal We Artiﬁcial Neural Networks ANNs Empirical Model WDPbal reasons First needed regression technique EM output eﬃciency value Moreover ANNs classical technique Machine Learning require little domain knowledge modular nature makes easier embed Combinatorial Optimization Section 5 We trained core feedforward network hidden layer We bipolar sigmoid neurons tanh activation function hidden output layers use alternative activation functions rectiﬁers considered future research The output network 1 1 range scaled order obtain easily understandable eﬃciency value We Encog framework 42 training ANNs M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 357 Fig 2 Mean prediction error main ANN0 Fig 3 Mean prediction error ANN1 lacking min input LevenbergMarquardt algorithm After experiments settled neurons hidden layer increasing size signiﬁcantly improve accuracy We trained ANN variants core having number type neurons The ﬁrst variant referred ANN0 takes inputs features Equations 4952 The second variant referred ANN1 lacks mincpik input diﬃcult encode Combinatorial Optimization We evaluated ANNs training sets separate test sets The test set core obtained sampling 50 examples random training sets cores Therefore test set contains 50 47 2350 unseen examples considerably different corresponding training set considerably challenging Fig 2 reports mean absolute values prediction errors ANN0 core The cores indexed rows 8 6 grid cores 05 refer ﬁrst row 611 second The errors refer eﬃciency values range 0 1 mean error 001 corresponds 1 eﬃciency difference The mean prediction errors low training test set Moreover eﬃciency cores ﬁrst row left right chip borders considerably easier predict Fig 3 reports mean absolute values prediction errors ANN1 networks mincpik input Their average accuracy good deﬁnitely worse ANN0 25 1 On hand ANN1 networks easier embed employed effective optimization techniques This kind tradeoff unique EML Rather common dealing real world problems involve approximate models In context EML provides ﬂexible approach balance model accuracy solver eﬃcacy Fig 4 reports error histogram training set ANN0 ANN1 networks corresponding core 20 located close center chip Actual errors considered absolute values In cases 358 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Fig 4 Prediction error histograms ANNs corresponding core 20 Fig 5 Percentage correctly classiﬁed instances Decision Trees networks provide accurate predictions larger error values order 10 likely occur ANN1 This potentially issue given importance maximum errors EML beginning Section 73 Finally investigated impact changing size training set accuracy ANN1 networks In particular tried 1 decrease size training set removing varying percentage instances chosen uniformly random 2 increase size training set adding varying number instances chosen uniformly random training sets The networks trained modiﬁed sets tested original test sets We removing adding little 10 instances signiﬁcant adverse effect accuracy leading larger average errors cores The rationale removing adding instances causes alteration factorial designed employed original training sets making region input space underrepresented The accuracy plots experimentation reported paper available online6 For second dispatching problem use classiﬁer Empirical Model trained Empirical models WDPmax distinguish low higheﬃciency cores Speciﬁcally Empirical Model consists Decision Tree DT core We chose DTs main reasons ﬁrst DTs accurate problem hand Second choice gives 6 In git repository https bitbucket org m _lombardi emlaij2015resources M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 359 opportunity discuss EML approach Machine Learning technique radically different Artiﬁcial Neural Networks We trained Decision Trees C45 algorithm 43 implemented Weka 44 J48 default parameters particular trees pruned The performance DTs assessed 10fold cross validation training set evaluation test sets ANNs The outcome evaluations reported Fig 5 shows percentage correctly classiﬁed instances DT corresponding core The accuracy actually high cases 8 Experimental results In section present experimental evaluation solution methods example problems We sider alternative modeling approaches discussed Section 3 1 heuristic proxy eﬃciency values 2 ﬁtting adjust parameters expertdesigned model ﬁnally 3 solution methods based EML The benchmark experimentation set 20 instances 288 jobs mapped 48 cores SCC platform The instances designed diverse possible As common trait instance includes majority lowCPI jobs plus small number jobs relatively high CPI benchmark challenging leaving time suﬃcient room improvements The instances detailed results publicly available download7 All solution approaches present based approximate models relying simulator search lead unacceptable response times As consequence necessary distinguish levels experimental comparisons 1 First compare effectiveness different solution approaches terms value cost function predicted solution quality 2 Second compare solution approaches terms real solution quality case obtained simulator The ﬁrst level comparison adopted combinatorial optimization papers allows assess effectiveness given approach ﬁnding solutions given model This comparison meaningful solution approaches based approximate model The second type comparison evaluates combined effectiveness solution method approximate model We use ﬁrst type comparison Section 83 order determine best solution method modeling approach heuristic model ﬁtting EML second type evaluation Section 84 compare optimization approaches based different models 81 Approximate models In section brieﬂy presentreview approximate models considered experimentation A heuristic proxy eﬃciency The computation workload job measured CPI value jobs executed core interleaved fashion thermal aware scheduler Section 3 As consequence reasonable expect core k correlation eﬃciency average job CPI The existence correlation checked empirically performing pilot experiments Fig 6 shows results core 41 contains scatter plot eﬃciency yaxis average CPI jobs xaxis Each point corresponds different set jobs color reﬂects density data points red denotes higher values typical region operation There strong correlation eﬃciency average CPI linear low average CPI values interesting region considered problems The results cores clean obtained core 41 good degree correlation present Based observations domain experts authors platform simulator suggested employ average job CPI core proxy eﬃciency WDPbal The resulting model eff hACPIx w T avgcpi 55 avgcpi vector avgcpik values speciﬁed feature deﬁnitions Equation 49 w vector weights identiﬁes sensitive core changes average CPI Assuming want avoid use model ﬁtting considered paragraph value w ﬁxed based knowledge domain expert In experiments considered possibilities 7 In git repository https bitbucket org m _lombardi emlaij2015resources 360 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Fig 6 Core eﬃciency average CPI mapped jobs For interpretation references color ﬁgure reader referred web version article Table 3 Solution approaches considered experimentation WDPbal WDPmax Model ACPI wACPI FEAT ANN0 ANN1 DT Opt technique LS MILP MINLP CP SMT 1 w 1 cores 2 w 1 internal cores w 075 cores chip borders w 05 cores chip corners ﬁrst choice referred ACPI model appropriate reliable information missing The second choice referred wACPI model takes account heat dissipation eﬃcient cores border silicon die We employed heuristic approach WPDbal problem Using ﬁtting adjust expertdesigned model A compromise EML relying heuristic consists employing function ﬁtting adjust parameters expertdesigned model With aim evaluate approach devised linear model WDPbal based features Section 7 The idea pilot experiments observations Section 71 suﬃcient deﬁne model structure relying powerful learning techniques The resulting model referred FEAT given eff hFEAT x waT avgcpi wmT mincpi wnT ngbcpi woT otrcpi wa wm wn wo vectors weights obtained linear regression Linear regression makes feasible obtain values parameters requires build training set overall design effort closer extracting Artiﬁcial Neural Network Still FEAT model cost function linear allow optimization approach considerably closer optimum predicted cost We employed model ﬁtting approach WDPbal problem Using Empirical Model Finally modeled behavior Empirical Model Learning The details Empirical Models given Section 7 Here simply recall employed ANN0 ANN1 Neural Networks WDPbal employed Decision Trees referred DT WDPmax We recall ANN1 model differs ANN0 lacks mincpik input feature 82 The solution approaches A summary solution approaches considered experimentation reported Table 3 The table highlights solution techniques problems applied WDPbal WDPmax models employed Algorithm 1 Find Initial Solutionn m cpi M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 361 sumk 0 cntk 0 k 0m 1 repeat lowest cpii select job map job sumk sumk cpii cntk cntk 1 core k jobs mapped min cntk break ties max sumk behavior Only solution techniques LS CP applied conﬁgurations remaining ones restricted linear formulations MILP speciﬁc models MINLP SMT We Localsolver v30 45 implement LS approach We deﬁned MINLP model GAMS 46 solved BARON Neos Servers Optimization 47 As CP chose Google ortools8 employed Neuron Constraint implementation developed 4 In CP model use integer variables ﬁxed precision factor represent realvalued variables Finally chose Z3 Microsoft Research SMT models 39 IBM ILOG CPLEX9 v125 MILP models All solvers chosen based effectiveness tested pilot experiments accessibility modeling interfaces We use default conﬁguration Localsolver CPLEX BARON solve LS MILP MINLP models The CP SMT models instead solved Randomized Adaptive Decomposition approach similar spirit Large Neighborhood Search described paragraph This order boost ability ﬁnd high quality solutions reasonably small time tens seconds case The decomposition technique applied LS Localsolver designed scalable MINLP access BARON slow httpbased API MILP topic future research Randomized adaptive decomposition Our CP SMT approaches wrapped decomposition method similar Large Neighborhood Search reduce time obtaining highquality solutions The method starts initial solution obtained simple greedy heuristic presented Algorithm 1 The heuristic works iteratively mapping computationintensive job lowest CPI loaded cores A core k0 considered loaded core k1 1 fewer jobs k0 k1 2 number jobs identical sum job CPIs k0 lower sum k1 This approach tries balance average CPI value core essentially following idea basis ACPI model Then decomposition method proceeds analogously Large Neighborhood Search iteratively 1 selecting small set C R cores 2 relaxing mapping decisions jobs running cores 3 solving restricted problem remap jobs cores C R aim improve solution quality The difference LNS solving restricted subproblem change objective function max z min kC R effk WDPbal max z cid4 effk WDPmax kC R cost function neglect eﬃciency cores set C R In words solving restricted problem iteration look local improvements global ones This particularly useful WDPbal happen search multiple cores similarly low eﬃciency In situation decomposition approach focus critical cores individually We stop iteration soon improving solution time limit reached In SMT approach restricted subproblem solved Z3 default parameters In CP case use restarts depth ﬁrst search random variable value selection For choosing cores relax select number nbad bad cores want improve eﬃciency plus number ngood good cores accept eﬃciency reduction In practice employ slightly different criterion example problems For WDPbal For WDPmax bad cores chosen randomly uniform probability minimum eﬃciency good cores chosen randomly rest selection probability given normalized ratio eff k avgcpi cid6 k equal avgcpik maxcpi avgcpi cid6 k bad cores chosen randomly uniform probability having low eﬃciency good cores chosen random rest selection probability given normalized value avgcpik In experiments nbad 1 ngood 3 WDPbal nbad 2 ngood 3 WDPmax 8 The software existing documentation available https developers google com optimization 9 See software page http www01ibm com software commerce optimization cplexoptimizer 362 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Table 4 Results WDPbal ACPI ANN0 ANN1 model 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ACPI LSbal obj 290 293 235 218 200 283 419 272 265 362 308 200 268 215 290 276 217 422 260 276 MILPbal obj 288 292 233 217 200 282 419 270 263 357 305 200 267 213 289 275 216 422 258 275 UB 293 358 239 218 200 288 505 276 301 434 380 200 271 216 296 280 222 510 262 312 83 Evaluation based predicted quality ANN0 CPbal obj 746 04 788 06 719 03 705 04 688 04 731 05 847 03 729 04 767 03 829 05 805 05 690 01 771 04 701 03 748 05 765 06 704 02 848 02 723 03 771 06 LSbal obj 736 11 755 21 715 13 680 46 682 08 735 10 778 55 733 08 734 32 764 40 753 19 682 11 727 15 699 08 745 07 742 31 699 18 789 35 722 08 744 18 ANN1 CPbal obj 700 02 774 04 669 07 662 01 645 01 692 03 858 04 682 07 730 05 836 06 797 05 645 01 727 06 657 03 702 01 735 06 661 01 857 04 673 08 749 07 LSbal obj 671 10 681 25 658 05 654 06 646 06 669 06 768 65 668 08 665 18 746 48 697 31 650 04 670 10 652 04 662 09 662 14 656 04 779 65 658 08 675 09 MINLPbal UB 721 840 854 869 662 723 952 710 833 954 919 663 855 667 723 878 838 872 699 791 In section compare solution approaches Table 3 terms predicted solution quality allow assess effectiveness approach ﬁnding good solutions speciﬁc models The wACPI FEAT models solved Local Search approach performed slightly better MILP similar ACPI model Results WDPbal We obtained LS approach based ACPI model WDPbal inserting hACPIx function core Local Search model Section 6 An analogous process followed obtain MILP approach We solved problem time limit 90 seconds Localsolver time signiﬁcant improvement obtained 1 hour CPLEX effort prove optimality The results experimentation reported Table 4 ACPI columns For approaches report minimum average CPI instance value objective function For MILP report value best upper bound end search process seen possible prove optimality time limit cases However bound values provide indication solutions approaches good Localsolver having slight edge Table 4 presents results EML approaches based ANN0 ANN1 model respectively ANN0 ANN1 columns The problem objective lowest core eﬃciency reported percentage value It important realize approaches operating model directly compared terms predicted quality Comparisons approaches based different models taken account Section 84 meaningless stage emphasized table doublelines separate ACPI ANN0 ANN1 columns We recall MINLP model designed Artiﬁcial Neural Network ANN0 simpliﬁed version mincpik input ANN1 Moreover rely remote server solving MILP problems impossible use MINLP model Randomized Adaptive Decomposition scheme severely affected performance approach Eventually decided different use MINLP model employed computing bound solution quality This removing integrality restriction mapping variables solving resulting NonLinear Programming model The best bound 2 hours shown Table 4 MINLPbalUB column values italic font denote optimal noninteger solutions We believe disappointing result provides actually good example ﬂexibility EML allows obtain valuable information inaccessible single solution technique Localsolver makes use randomized search algorithm decomposition method employ solving CP SMT model randomized Therefore LS CP approaches performed 10 runs instance time limit 90 seconds In CP timeout 2 seconds enforced iteration decomposition method For LS CP table reports obj columns average value objective function minimum core eﬃciency 10 runs round brackets standard deviation objective value 10 runs M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 363 Table 5 Results WDPmax 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 DT CPmax obj 2960 0917 3360 0800 2870 1005 2510 0700 2330 0458 3050 1118 3910 0700 2860 0800 3290 1300 3730 0640 3560 0917 2430 0781 3310 0831 2400 1000 2960 0663 3290 0831 2510 0700 3890 0539 2640 1020 3290 1136 LSmax obj 2770 0458 3240 0663 2810 0700 2510 0539 2350 0500 2900 0632 3690 0700 2790 1044 3070 0640 3490 1044 3320 0872 2400 0000 3130 0781 2400 0000 2860 0917 3080 0600 2470 0640 3660 1020 2520 0400 3180 0748 SMTmax obj 2360 0800 2480 0980 2250 1025 1810 0943 1500 2236 2330 0900 2620 1778 2300 1183 2410 0700 2580 1600 2410 0943 1500 1000 2400 1095 1740 1497 2320 0980 2370 1100 1720 2088 2690 1221 2180 0748 2470 1100 Both CP LS approaches managed obtain high quality solutions seen instances optimal NLP bound available The CP approach performed usually better gaps large 10 simpliﬁed model ANN1 large 7 model ANN0 signiﬁcant application The performance CP approach considerably robust different runs The difference performance stems main reasons ﬁrst propagation Neuron Constraints allows CP solver quickly terminate iterations decomposition method Second incorporation expert knowledge selection cores relaxed allows steer search promising regions solution space Results WDPmax Here present results second dispatching problem Section 3 As anticipated setting tested single Empirical Model Decision Trees extracted Section 7 We solved benchmark instances LS CP SMT The CP SMT approaches use decomposition method described beginning Section 8 Each instance solved 10 times different random seeds time limit 90 seconds Again time limit 2 seconds enforced iteration decomposition method Table 5 reports like previous cases average value problem objective 10 runs standard deviation Similarly WDPbal propagation domain knowledge allow CP approach perform generally better LS gap smaller problem LS slightly stable CP highlighted standard deviation values The SMT approach worst performer despite based decomposition method CP The reason Z3 solver diﬃculties proving infeasibility solving subproblems corresponding unlucky selection relaxed cores The CP approach conversely able close subproblems 2 seconds time limit 84 Evaluation real In section compare solutions provided different approaches terms quality target SCC simulator Section 3 By evaluate time optimization approach accuracy model This allows compare radically different approaches designed solve practical problem This evaluation restricted WDPbal We compare solutions obtained ACPI wACPI FEAT ANN0 ANN1 models For employed approach worked best terms predicted solution quality LS ACPI wACPI FEAT CP ANN0 ANN1 Each instance benchmark solved time limit 90 seconds 2 seconds decomposition iteration The results experimentation reported Table 6 For approach report minimum simulated core eﬃciency percentage We report brackets standard deviation core eﬃciencies standard deviation cost different runs instance solved The reason showing standard deviation minimum core eﬃciency quality measure real solutions robust wrt approximation errors balanced solution single large approximation error appear having poor quality Adding standard deviation allows detect kind situation 364 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 Table 6 Solution quality approaches measured target The values square brackets standard deviations eﬃciency platform cores single solution standard deviations taken multiple runs 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ACPI LS sim 535 143 568 111 539 138 502 137 610 108 525 132 581 125 519 133 603 111 601 111 618 116 592 118 570 119 500 110 516 149 553 116 518 121 605 123 510 128 543 125 wACPI LS sim 531 83 521 108 597 90 540 92 532 81 525 84 571 106 583 89 587 87 630 101 534 120 501 84 592 92 542 78 556 87 552 101 566 83 647 98 517 90 531 108 FEAT LS sim 585 77 628 75 564 91 487 78 500 85 567 76 566 81 596 85 564 105 600 72 562 90 488 83 561 98 551 84 572 89 602 96 508 79 600 72 556 80 566 84 ANN0 CP sim 638 63 634 67 584 77 643 70 600 72 657 67 629 73 651 64 634 65 677 57 652 67 571 61 652 78 639 64 595 77 631 73 639 80 675 62 635 68 638 83 ANN1 CP sim 608 70 650 75 580 85 609 91 467 107 592 79 602 91 672 70 652 77 654 66 629 79 560 103 635 83 591 85 652 73 617 71 569 96 657 80 665 73 660 85 Fig 7 Simulated eﬃciency values LSbalACPI For interpretation references color ﬁgure reader referred web version article The solution approaches based ANN models obtained general better results linear objective including FEAT model coeﬃcients obtained linear regression The advantage terms minimum eﬃciency standard deviation The results obtained accurate network ANN0 tend turn better obtained simpliﬁed ANN1 The results simulation ACPI ANN0 models observed Fig 7 Fig 8 Each subﬁgure corresponds instance tile subﬁgure corresponds core recall platform 8 6 layout Greenred tiles respectively denote higherlower eﬃciency values red corresponds 50 eﬃciency green 100 The LSbalACPI approach Fig 7 able obtain fairly balanced workloads However produced mappings occasionally lead abnormally low eﬃciency cores This happens cases M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 365 Fig 8 Simulated eﬃciency values CPbalANN0 For interpretation references color ﬁgure reader referred web version article solution close optimal crosschecking results Table 4 possible example low eﬃciency values occur instance 15 cost function value 276 bound 280 The eﬃciency maps Fig 8 corresponding solutions provided CPbalANN0 uniform By comparing results Table 6 Table 4 seen predicted minimum eﬃciency levels tend 510 higher real values On regard interesting facts observe ﬁrst despite signiﬁcant error level CP solutions ANN0 model balanced terms eﬃciency suggests overestimation tends similar cores Second error level order largest errors EM evaluation Section 7 This strengthens idea optimizer attracted solutions high predicted quality large margin error This behavior certainly ideal actually totally undesirable fact provide systematic approach Empirical Model robust including simulated solutions training set This method allow improve model robustness need increase dramatically training set size We leave investigation idea topic future research 9 Concluding remarks We proposed methodology called Empirical Model Learning merging Machine Learning optimization extracting decision model components data come simulator real Our emphasis deﬁning techniques embedding ML models Combinatorial Optimization designed optimization engine exploit ML model boosting search process We discussed main steps methodology motivating running examples thermalaware workload dispatching problems The ML techniques adopted Artiﬁcial Neural Network Decision Trees encoded Local Search Constraint Programming Mixed Integer NonLinear Programming ANNs SAT Modulo Theory DTs Designing good empirical model nontrivial task allows better accuracy wrt expert designed heuristics Experiments clear advantages dataextracted model terms quality ﬁnal solutions Constraint Programming shown particularly effective expressive modeling language ﬁltering algorithms Further improvements obtained increasing accuracy Empirical Models example Instruc tions Per Clock CPIs allows better characterize computationintensive jobs critical eﬃciency prediction We tested approach leads signiﬁcant improvements ANN1 ANN0 The Empirical Model Learning approach enables application optimization techniques complex real world prob lems hard impossible tackle As main beneﬁt approach opens new application areas In particular EML instrumental bridging gap predictive prescriptive analytics 366 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 As particular case EML perform decision making controlled In situation EML allows use highlevel optimizer steer behavior existing controller need know internal details potentially direct communication This property exploited ease integration legacy systems tackle large scale problems multilevel optimization Active learning incorporated EML approach periodically retraining ML model improve accu racy The approach employed design selfadapting systems capable tracking changes operating conditions In principle include disruptive events addition new components Finally interesting topic future research concerns relation predictive models uncertainty Machine Learning models designed deal uncertain data provide information probability distributions conﬁdence intervals robustness predictions These capabilities exploited design robust reliable decision making systems real world problems Acknowledgements This research partly funded Google Focused Grant Program Mathematical Optimization Combinato rial Optimization Europe title Model Learning Combinatorial Optimization A Case Study Thermal Aware Dispatching References 1 GE Box NR Draper Empirical ModelBuilding Response Surfaces Wiley New York 1987 p 424 2 S Brailsford L Churilov B Dangerﬁeld DiscreteEvent Simulation System Dynamics Management Decision Making John Wiley Sons 2014 3 P Flach personal communication 2014 4 A Bartolini M Lombardi M Milano L Benini Neuron constraints model complex realworld problems Principles Practice Constraint 5 A Bonﬁetti M Lombardi M Milano Embedding decision trees random forests constraint programming Integration AI OR Techniques 6 J Howard S Dighe et al A 48core IA32 messagepassing processor DVFS 45nm CMOS International SolidState Circuits Conference Programming CP 2011 pp 115129 Constraint Programming CPAIOR 2015 pp 7490 ISSCC 2010 pp 108109 7 KP Bennett E ParradoHernández The interplay optimization machine learning research J Mach Learn Res 7 2006 12651281 8 S Sra S Nowozin SJ Wright Optimization Machine Learning MIT Press 2011 9 LD Raedt T Guns S Nijssen Constraint programming data mining machine learning Proc 24th AAAI Conference Artiﬁcial Intelli gence AAAI 2010 pp 16711675 10 T Guns S Nijssen LD Raedt kPattern set mining constraints IEEE Trans Knowl Data Eng 25 2 2013 402418 11 LD Raedt S Nijssen B OSullivan PV Hentenryck Constraint programming meets machine learning data mining available httpvesta informatikrwthaachendeopusvolltexte20113207pdfdagrep_v001_i005_p061_s11201pdf 2011 12 Y Malitsky M Sellmann Instancespeciﬁc algorithm conﬁguration method nonmodelbased portfolio generation Integration AI OR Techniques Constraint Programming Combinatorial Optimization Problems CPAIOR 2012 pp 244259 13 F Hutter L Xu H Hoos K LeytonBrown Algorithm runtime prediction methods evaluation Artif Intell 206 2014 79111 14 L Kotthoff IP Gent I Miguel An evaluation machine learning algorithm selection search problems AI Commun 25 3 2012 257270 15 L Hernando A Mendiburu JA Lozano Generating customized landscapes permutationbased combinatorial optimization problems Learning Intelligent Optimization LION Lecture Notes Computer Science vol 7997 2013 pp 299303 16 C Bessière R Coletta EC Freuder B OSullivan Leveraging learning power examples automated constraint acquisition Principles Practice Constraint Programming CP 2004 pp 123137 17 C Bessière R Coletta B OSullivan M Paulin Querydriven constraint acquisition International Joint Conference Artiﬁcial Intelligence IJCAI 2007 pp 5055 Programming IJCAI 2012 pp 141157 18 N Beldiceanu H Simonis A model seeker Extracting global constraint models positive examples Principles Practice Constraint 19 C Bessiere R Coletta E Hebrard G Katsirelos N Lazaar N Narodytska CG Quimper T Walsh Constraint acquisition partial queries Inter national Joint Conference Artiﬁcial Intelligence 2013 pp 475481 20 B OSullivan Automated modelling solving constraint programming Proc 24th AAAI Conference 2010 pp 14931497 21 NV Queipo RT Haftka W Shyy T Goel R Vaidyanathan PK Tucker Surrogatebased analysis optimization Prog Aerosp Sci 41 1 2005 128 Transp Eng 136 6 2009 528536 43 6 1995 13491358 22 A Cozad NV Sahinidis DC Miller Learning surrogate models simulationbased optimization AIChE J 60 6 2014 22112227 23 K Gopalakrishnan AM Asce Neural network swarm intelligence hybrid nonlinear optimization algorithm pavement moduli backcalculation J 24 AH Zaabab Q Zhang M Nakhla A neural network modeling approach circuit optimization statistical design IEEE Trans Microw Theory Tech 25 R Battiti M Brunato The LION way machine learning plus intelligent optimization LIONlab University Trento 2014 26 M Gavanelli M Nonato A Peano S Alvisi M Franchini Genetic algorithms scheduling devices operation water distribution response contamination events Evolutionary Computation Combinatorial Optimization EvoCOP 2012 pp 124135 27 AR Conn K Scheinberg LN Vicente Introduction DerivativeFree Optimization MPSSIAM Series Optimization vol 8 SIAM 2009 28 C Audet A survey direct search methods blackbox optimization applications Mathematics Without Boundaries Springer 2014 pp 3156 29 DR Jones M Schonlau WJ Welch Eﬃcient global optimization expensive blackbox functions J Glob Optim 13 1998 455492 30 MC Fu Optimization simulation review Ann Oper Res 53 1994 199248 31 F Glover JP Kelly M Laguna New advances wedding optimization simulation Winter Simulation Conference vol 1 WSC IEEE 1999 32 W Huang S Ghosh S Velusamy Hotspot compact thermal modeling methodology earlystage VLSI design IEEE Trans Very Large Scale Integr pp 255260 VLSI Syst 14 5 2006 501513 M Lombardi et al Artiﬁcial Intelligence 244 2017 343367 367 33 A Bonﬁetti M Lombardi The weighted average constraint Principles Practice Constraint Programming CP 2012 pp 191206 34 S Haykin Neural Networks A Comprehensive Foundation 2nd edition Prentice Hall PTR Upper Saddle River NJ USA 1998 35 P Belotti C Kirches S Leyffer J Linderoth J Luedtke A Mahajan Mixedinteger nonlinear optimization Acta Numer 22 2013 1131 36 F Rossi PV Beek T Walsh Handbook Constraint Programming Elsevier 2006 37 M Lombardi S Gualandi A new propagator twolayer neural networks empirical model learning Principles Practice Constraint Programming CP 2013 pp 448463 38 L Rokach O Maimon Data Mining Decision Trees Theory Applications World Scientiﬁc Publishing Co Inc River Edge NJ USA 2008 39 LD Moura N Bjørner Z3 eﬃcient smt solver Tools Algorithms Construction Analysis Systems Springer 2008 pp 337340 40 JC Régin Generalized arc consistency global cardinality constraint Proc 13th National Conference Artiﬁcial Intelligence vol 1 AAAI 41 P Flach Machine Learning The Art Science Algorithms That Make Sense Data Cambridge University Press 2012 42 J Heaton Encog Java DotNet neural network framework Heaton Research Inc retrieved July 20 2010 43 J Quinlan C45 Programs Machine Learning Morgan Kaufmann 1993 44 M Hall E Frank G Holmes B Pfahringer P Reutemann IH Witten The WEKA data mining software update ACM SIGKDD Explor Newsl 11 1 45 T Benoist B Estellon F Gardi R Megel K Nouioua Localsolver 1x blackbox localsearch solver 01 programming 4OR 9 3 2011 299316 46 MR Bussieck A Pruessner Mixedinteger nonlinear programming SIAGOPT Newsl Views News 14 1 2003 1922 47 NV Sahinidis BARON 1210 global optimization mixedinteger nonlinear programs Users manual available httpwwwgamscomdddocs Press 1996 pp 209215 2009 1018 solversbaronpdf 2013