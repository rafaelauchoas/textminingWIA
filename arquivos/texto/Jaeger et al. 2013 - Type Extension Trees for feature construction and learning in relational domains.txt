Artiﬁcial Intelligence 204 2013 3055 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Type Extension Trees feature construction learning relational domains Manfred Jaeger Marco Lippi b Andrea Passerini c Paolo Frasconi d Institut Datalogi Aalborg Universitet Denmark b Dipartimento di Ingegneria dellInformazione e Scienze Matematiche Università degli Studi di Siena Italy c Dipartimento di Ingegneria e Scienza dellInformazione Università degli Studi di Trento Italy d Dipartimento di Ingegneria dellInformazione Università degli Studi di Firenze Italy r t c l e n f o b s t r c t Article history Received 23 October 2012 Received revised form 31 July 2013 Accepted 8 August 2013 Available online 20 August 2013 Keywords Statistical relational learning Inductive logic programming Feature discovery 1 Introduction Type Extension Trees powerful representation language countofcount features characterizing combinatorial structure neighborhoods entities relational domains In paper present learning algorithm Type Extension Trees TET discovers informative countofcount features supervised learning setting Experiments bibliographic data TETlearning able discover countof count feature underlying deﬁnition hindex inverse document frequency feature commonly information retrieval We introduce metric TET feature values This metric deﬁned recursive application WassersteinKantorovich metric Experiments kNN classiﬁer exploiting recursive countof count statistics encoded TET values improves classiﬁcation accuracy alternative methods based simple count statistics 2013 Elsevier BV All rights reserved Probabilistic logical relational models provide models properties relationships entities domains relational structure graphs networks generally kind structure relational database The prevalence type structured data challenges posed traditional machine learning methods based simple attributevalue data models led increasing past 10 years probabilistic logical models associated statisticalrelational learning techniques 106 When modeling entities embedded relational domain key question features entities relevant model predict properties Apart attributes given entities rela tional learning ability construct new features considering relational neighborhood entity Taking consideration related entities attributes obtains basically unlimited supply potential features A word terminology order attribute mean formal representation dataset property individual entities data column The color property ﬂower example formalized attributes color red green blue orange distinct attributes RGB_red RGB_green RGB_blue 0 255 The value space attribute typically simple data type like Boolean enumeration numeric In classic attribute value data feature synonym attribute By contrast use feature denote formalized properties broader sense First feature implicit data function explicit data For example brightness function RGB_red RGB_green RGB_blue feature attributevalue dataset containing attributes Corresponding author Email addresses jaegercsaaudk M Jaeger lippidiismunisiit M Lippi passerinidisiunitnit A Passerini paolofrasconiuniﬁit P Frasconi 00043702 matter 2013 Elsevier BV All rights reserved httpdxdoiorg101016jartint201308002 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 31 Fig 1 Bibliographic data fragment countofcount feature RGB_red RGB_green RGB_blue number friends older 22 feature person entity relational social network dataset Second relational data feature represent property relating multiple entities Thus titles having 3 words common feature pair paper entities bibliographic database Third unlike common frameworks statistical learning like example kernel methods paper terested simple numerical features focus features values complex combinatorial data structures representing loosely countsofcounts In cases require feature welldeﬁned formal speciﬁcation value space The language available formal speciﬁcations deﬁnes feature space Relational learning frameworks differ widely extent linked clearly deﬁned feature space extent feature selection feature construction integrated model learning process On hand techniques require availability features simple data type The features construction learning framework usually requires applicationdependent data preprocessing 42 Propositionalization approaches maintain strict separation feature construction learning speciﬁc frameworks representation languages feature speciﬁcation crucial ingredient 22 On extreme approaches feature construction model learning tightly integrated fact learned model essentially consists list features represented formal speciﬁcation language To category belong frameworks based predicate logic feature representation language 32021 In approaches feature construction integral learning process exact feature space accessible learner clearly delimited 118 A key component design relational features given tools available constructing features properties entities related entity chains relations Since number entities reached slotchains 8 varies instance instance feature construction usually involves form combination aggregation properties multiple related entities In nonprobabilistic inductive logic programming approaches aggregation usually based purely existential quantiﬁcation feature determines related entity certain attributes exists So example author entity bibliographic database deﬁne Boolean feature saying exists paper citing paper author A number frameworks closely linked relational databases 3113 construct features based aggregation operators Here possible example construct feature represents average count citations papers author received feature represents average price items purchased customer Recently approaches deﬁne probability distributions entire structures based pure count features popular 3643 Here probability relational structure given domain entities determined count entity tuples satisfy relational constraints typically expressed logical clause All approaches based features represent relatively simple summary statistics quantitative properties entitys relational neighborhood However prediction tasks detailed picture combi natorial countofcount features relevant Consider tiny bibliographic dataset shown Fig 1 instance It represents 5 different authors 10 different papers authors citation links papers Simple summary features author number papers hisher total average citation count However currently important attribute author hindex 14 To predict hindex attribute closely linked hindex like receiving professional award large research grant need consider detailed feature count papers given counts citations In Fig 1 values 5 authors countofcount feature shown right expression k l meaning l papers k citations As example count count features consider Internet Movie Database IMDB popular object investigation relational machine learning Here interested predicting attribute movie boxoﬃce success 3831 For prediction consider cast movie example terms size count actors cast previously received award nomination total number award nominations shared actors Again detailed countofcount feature informative ﬂat counts difference single actor cast award nominations single box oﬃce draw actor maybe peak hisher career actors nominations young allstar cast In information retrieval relevance measures document d given query q based counting terms ap pearing d q These counts usually weighted term weight inverse document frequency given number documents collection containing term Thus relevance measure computed 32 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 countofcount feature Similarly Fig 1 example querydocument pair d q feature value 3 1 10 2 7 1 expressing fact d q 1 term common appears total 3 documents 2 terms common appear 10 documents 1 term common appears 7 documents Finally consider relational domain consisting Webpages linksto relation An important attribute webpage pagerank 5 want estimate pagerank webpage based information local relational neighborhood Unlike hindex bibliographic example precisely determined relational cid4 P pagerank1 fully determined neighborhood radius 2 deﬁned chain authorOf A P citesP structure relational domain However useful approximation obtained local information Clearly relevant pagerank page P number incoming links Also important pagerank pages cid4 linking P number incoming links Furthermore important know pages P P linking P number outgoing links P pointing pages P determines inherited P Again relevant information given comprehensive countofcount pagerank P feature cid4 cid4 cid4 The purpose paper develop framework representation rich combinatorial countofcount features context relational learning The methodological contribution paper main components C1 The deﬁnition TypeExtension Trees TETs 7 formal representation language countofcount features C2 A method learning TypeExtension Trees conjunction simple predictive model based TETfeatures This gives method relational feature discovery baseline supervised learning framework In 7 TET learning employed classic notion information gain paper advantage recently introduced relational information gain 26 C3 The deﬁnition novel metric TET feature values enables distancebased learning techniques use countofcount features substantial manner To illustrate relationship signiﬁcance components consider analogy learning stan dard numerical attributevalue data instance fully characterized tuple numeric attributes class variable Fig 2 left shows small dataset numeric attributes A1 A2 N1 N4 binary class label values positive negative In dataset class correlated attributes A1 A2 N1 N4 random noise The plot left ﬁgure represents values A1 A2 C The relevant feature subset predicting class label A1 A2 space possible feature subsets Fig 2A The set A1 A2 called suﬃcient modelindependent feature predicting C No concrete type machine learning model use information represented feature A decision tree model instance use ﬁnitely Boolean features deﬁned lower upperbounds A1 A2values A linear classiﬁer use linear function A1 A2 We spaces featuresreduced modelspeciﬁc feature spaces Fig 2B A proper dis tinction suﬃcient reduced feature spaces important interpret result learning speciﬁc model terms feature discovery decision tree learned example dataset Fig 2C left uses Boolean features A1 079 A2 075 strictly speaking discovered exactly features However typically want generalize A1 A2 discovered feature suﬃcient feature space assume suitable reductions A1 A2 modelspeciﬁc feature spaces lead good performance types models Fitting logistic regression model data Fig 2C right directly leads construction linear function 128 221 A1 34N4 predictive feature Again abstract reduced feature try identify discovered modelindependent feature Here abstraction clearcut decision tree case Considering attributes coeﬃcients linear function highest absolute values inclusion feature subset subsets A1 A1 A2 A1 A2 N3 N4 discovered modelindependent feature Our objective C1 aims deﬁning relational data rich modelindependent feature space corresponds space attribute subsets Fig 2A includes complex countofcount features This deﬁnition developed Section 2 syntax semantics Type Extension Trees Section 21 The construction suﬃcient feature space relational data faces challenge basically unlimited supply possible features challenge diversity relational learning tasks attribute prediction individual entities link prediction classiﬁcation relational structures molecules require speciﬁcation features single entities pairs entities tuples entities global features relational dataset TET features provide uniform coherent framework cases Component C2 corresponds feature discovery process learning lightweight model illustrated Fig 2 This achieved ﬁrst deﬁning directly TETfeature values discriminant function turns TETfeature predictive model Section 3 Based discriminant function TET structure learning algorithm developed strictly speaking discover feature T f T TET f discriminant function deﬁned T trivial abstraction step extract modelindependent TET feature T Section 5 1 In fact pagerank applied Web searching bibliometrics 2 collaborative ﬁltering 27 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 33 Fig 2 Analogy numerical data A Suﬃcient feature space B Modelspeciﬁc feature spaces C lightweight learned models Given TET learned manually constructed based expert knowledge ﬁnally component C3 deﬁne predictive model based TET features makes use TETs countofcount values substantial sophisticated manner discriminant function For deﬁne metric TET values enables nearest neighbor classiﬁcation Section 4 2 Feature representation Type Extension Trees 21 TET syntax semantics In section review basic syntax semantics deﬁnitions Type Extension Trees 17 To simplify deﬁnitions assume attributes relations Boolean means multivalued attribute like color rise atomic propositions colorhat red assumed encoded Boolean attributes like color_redhat Relational data viewed model sense following deﬁnition Deﬁnition 21 Let R relational signature set relation symbols different arities A ﬁnite model R M M I consists ﬁnite domain M interpretation function I ra t f deﬁned ground atoms ra constructible relations r R arguments Marityr Throughout paper use f t shorthands false true Furthermore denote objects entities domain lowercase letters logical variables uppercase letters Only ﬁrstorder formulas acceptable variables stand objects Bold symbols denote tuples corresponding nonbold symbols foregoing deﬁnition a1 aarityr In logic programming terminology M Herbrand interpretation signature consisting R constant symbols elements M For convenience assume domain M partitioned objects different types arguments relations typed I deﬁned ground atoms arguments appropriate types For sake simplicity introduce special notation specifying types Rather examples typically use generic capital letters X Y Z U V W indicate variables range domain objects speciﬁc letters like A author P paper implicitly mean corresponding variables restricted subset objects With slight abuse notation τ complex ground sentence denote Iτ truth value interpretation I Deﬁnition 22 An Rliteral negated atom rV r R V tuple variable symbols We allow special literal cid7V evaluates t An Rtype conjunction Rliterals A type extension tree TET R tree nodes labeled Rtypes edges labeled possibly sets variables In following usually omit reference underlying signature R talk literals types Rliterals Rtypes Note according Deﬁnition 22 literal contain constant symbols arguments Since R assumed contain relation constant symbols consistent usual deﬁnition Rliteral The term type conjunction literals motivated distinct compatible existing uses type hand type entity commonly understood property expressed single unary predicate movieV personV Our deﬁnition generalizes types tuples objects types expressed conjunction literals On 34 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 hand type mathematical model theory consistent sets formulas free variables V 1 V n properties ntuples domain elements 15 Our deﬁnition special case type sense limiting single quantiﬁerfree conjunction literals Example 23 The following TET signature containing relations author authorOf cites author A P 1 authorOf A P 1 P 2 citesP 2 P 1 1 According semantics given TET represents feature suﬃcient computing hindex author example Example 24 A TET suﬃcient representing relevance features based inversedocument frequency weights cid7D Q T term_in_documentT D term_in_queryT Q cid4 D cid2 term_in_document T D cid3 cid4 2 Labeled edges TET related quantiﬁers predicate logic like quantiﬁer labeled edge binds occurrences variables associated edge subtree rooted edge The free variables TET variables bound edge label We TET propositional edge labels The TET 1 single free variable A 2 free variables D Q In cases root node essentially serves introduce free variables case 1 explicitly establish variable ranges entities type author If type constraints variables assumed implicit variable names root usually vacuous cid7 atom 2 We write T V denote TET free variables variables V necessarily contain We write cid4 τ V T V cid2 cid3 W 1 T 1V W 1 cid2 cid3cid5 W m TmV W m 3 denote TET root labeled τ V m subtrees T 1V W reached edges labeled variables W possibly A TET T V free variables V V 1 V k deﬁne feature ktuples domain entities model M Mk TET deﬁnes feature value V T Fig 1 right shows somewhat simpliﬁed form values V T a1 V T a5 TET T A 1 We general deﬁnition TET semantics steps ﬁrst deﬁne value space nested counts associated given TET T V actual mapping cid8 V T Deﬁnition 25 For set A denote multisets A set multisets A We denote a1 k1 kn multiset contains ki copies ai The value space VT TET T inductively deﬁned follows Base If T τ consists single node VT t f Induction If T τ W 1 T 1 W m Tm m VT f t cid2 cid3 VT multisets i1 We note according deﬁnition structure VT depends tree structure T labeling edges T types nodes T Example 26 1 graphical representation TET following 3 written T A cid4 author A cid2 cid4 cid2 cid4 cid5cid3cid5cid3cid5 P 1 citesP 2 P 1 4 The recursive deﬁnition VT grounded VcitesP 2 P 1 t f In words single node TET citesP 2 P 1 cid4 A P 1 represents Boolean feature pairs papers The inductive construction proceeds deﬁnition VT authorOf A P 1 P 2 cid4 T A P 1 cid4 authorOf A P 1 cid2 cid4 cid5cid3cid5 P 2 citesP 2 P 1 5 represents feature authorpaper pair A P 1 This value space constructed according inductive case Def inition 25 union f pairs form t A A multiset t f values Thus examples f M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 35 according Deﬁnition 27 feature value authorpaper pair p author p t f 9 t 1 feature value pairs a1 p1 a2 p4 Fig 1 t f 8 t 2 feature value a1 p2 Fig 1 Finally values TET 4 f t A A multiset values VT cid4 A P 1 Examples cid17 γ1 γ2 γ3 γ4 γ5 t t t t cid16 t f 8 t f 9 t 1 1 t f 8 t 2 1 f 7 t f 9 t 1 2 t f 10 1 f 7 t f 8 t 2 1 t f 10 2 f 8 t f 8 t 2 1 t f 10 1 f 7 t f 10 3 cid18cid19 6 Here better readability outer multisets written column vectors commaseparated linear form inner multisets These ﬁve values feature values ﬁve authors a1 a5 Fig 1 We usually use γ denote TET values We note t component value form t A A multiset redundant occurrence multiset preﬁxed t write A instead t A However adding explicit t embellishment value lets maintain clearer match structure TET T values γ VT example value t f 1 twolevel TET f 1 easily confused value f In preceding example introduced values TET feature 1 entities a1 a5 Fig 1 These values reﬁne informal countofcounts shown Fig 1 representing principled way recursive nature countofcounts including f counts case number papers written given author number papers citing given paper In following general deﬁnition feature value V T VT V speciﬁc tuple Deﬁnition 27 Let M M I model T V 1 V k TET Mk The value V T VT deﬁned follows Base If T V τ V consists single node V T Iτ Induction If T V τ V W 1 T 1V W 1 W m TmV W m If Iτ f V T f b If Iτ t cid2 cid3 T V cid3 cid2 t μa W 1 T 1 μa W m Tm μa W T multisetsVT given cid22cid21 cid22 cid21 cid21 cid20 cid21 cid20 cid2 γ b Mki st V cid3 T ia b γ 7 γ ranges values VT ki number variables W We remark original deﬁnitions given 17 treated cases Iτ f Iτ t symmetrically values according Deﬁnition 25 called pruned 7 recursive evaluation counts cut node evaluating f encountered TET branch We consider special case unlabeled edges TET particular purely propositional TETs Consider TET single unlabeled branch attached root T τ V T 1V The multiset 7 contains single value γ V T 1a Generally unlabeled edge TET induces recursive structure VT single value subTET T reached edge edge labeled variables induces multiset values The following example serves illustrate nature propositional TETs At time example shows different tree structures TET represent different logical properties cid4 36 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 Table 1 Propositional TETs values ev 1 v 2 ev 2 v 1 ev 1 v 2 ev 2 v 1 b ev 1 v 2 cid7v 1 v 2 cid7v 1 v 2 ev 2 v 1 c a1 a2 a1 a2 a1 a2 a1 a2 f t f f b f t f f t t c t f f t t f t f t t t t ev 1 v 2 ev 2 v 1 ev 1 v 2 ev 2 v 1 ev 1 v 2 ev 2 v 1 ev 1 v 2 ev 2 v 1 d d t f f f t t f t f f t f f t f t t f f f Example 28 Consider propositional TET T V V v 1 vn Since variables introduced edge labels nodes contain literals variables V V T a1 depends relational substructure induced a1 M By varying types nodes tree structure T V values V T represent variety different structural properties Consider relational signature contains single binary edge relation symbol e The upper Table 1 shows 4 different propositional TETs free variables v 1 v 2 In cases TET value T a1 a2 determined relational substructure induced a1 a2 The leftmost column Table 1 lists different possible substructures remaining columns values returned TETs ad substructures TET represents feature tests a1 a2 deﬁne substructure a1 a2 TET b twostage test possible edge relations If ﬁrst test fails edge ea1 a2 value false regardless presence absence converse edge Both c d use vacuous roottype cid7v 1 v 2 connect subTETs evaluated separately The values columns c d table list values different subTETs according toptobottom order branches deﬁned graphical representation Both TETs discriminate a1 a2structures sense structure unique value In sense d seen redundant version c However Section 4 c d exhibit distinct behavior respect metric deﬁne TET values 22 TET deﬁnable features In section illustrate TETs provide representation language rich class fundamental features usable variety learning frameworks TETfeatures play role suﬃcient feature space analogy Fig 2A Example 29 Inductive Logic Programming ILP probably oldest approach relational learning In ILP learns clas siﬁcation rules target predicate usually form logical clauses A classic example ILP Bongard problems scenes consisting geometric objects classiﬁed positive negative examples Here scene relational model domain consists geometric objects attributes like triangle X X Y deﬁned Then rule positive scene examples positive circleX triangleY inY X says scene positive contains triangle inside circle cf 35 Chapter 4 The feature classiﬁcation rule reduction TET feature cid7 XY circleX triangleY inY X Values simple TET form t f k t l giving counts object pairs satisfy respectively satisfy body rule The reduced ILP feature Boolean tests l 0 The original limitation Boolean features deﬁned existential quantiﬁcation ILP based approaches lifted introduction special literals contain aggregators quantitative information For example 44 intro duce aggregate conditions allow rule like cid20 positive circleX count cid22 Y triangleY inY X cid2 3 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 37 says scene positive contains circle triangles inside The Boolean feature rule reduction TET feature cid7 X circleX Y triangleY inY X 8 While explicitly explored 44 nested aggregate conditions possible cid20 cid20 cid22 cid21 cid21 circleX count cid21 cid22 cid21 triangleY inY X Y positive count X cid2 3 cid2 2 classify scene positive contains circles triangles inside This Boolean feature reduction underlying countofcount feature represented 8 Several frameworks proposed constructing numeric Boolean features iterated aggregation chains relational dependencies Some ILP tradition 22 arise combination relational extensions probabilistic graphical models decision trees 819131 While previous work nested aggregation considered frameworks require immediate aggregation step chain relations Thus considering bibliographic data example deﬁne author feature represents author seven publications seven citations However complex countofcount feature suﬃcient computing hindex outside scope feature construction methods Most approaches mentioned consider cases focus numeric predicates aggregation numeric values functions like mean max For purely categorical especially Boolean data consider paper common aggregation function count exists special case count cid2 1 In spite current restriction Boolean data basic TET architecture extended numeric data represent underlying suﬃcient combinatorial numerical information needed compute modelspeciﬁc aggregated features Example 210 In example focus MarkovLogic Networks MLNs 36 generative setting In case MLNs deﬁne distribution models M given signature R ﬁxed domain M This distribution deﬁned knowledge base KB containing ﬁrstorder logic formulas φi attached numeric weights w KB φ1X 1 w 1 φnX n wn 9 We refer 36 details MLN syntax semantics Relevant current context fact distribution deﬁned function count features adapting notation previous example written form cid20 count X cid21 cid22 cid21 φX 10 φX ﬁrstorder formula free variables X While similar appearance aggregate features preceding example essential differences ﬁrst MLNs depend ac tual integervalued count feature derived Boolean features form count cid2 k Second 10 takes count substitutions tuples domain elements free variables X φX As result countY triangleY inY X cid2 3 feature object X countX φX feature entire models M Concrete MLN implementations allow restricted class formulas φX model speciﬁcation quantiﬁer free formulas However use arbitrary ﬁrstorder formulas poses problem level semantic deﬁnitions consider models restrictions φX For count feature construct TET T integervalued feature 10 obtained reduction values V T We construct T form cid7 X T φX T φX TET free variables X truth value φa read TET value V T φa The construction T φ induction structure φ In atomic case φX rX T φ contains single node rX For conjunction φX φ1X 1 φ2X 2 lets T φX cid7 T φ1 X 1 T φ2 X 2 At point construction TET values V T φa encode information mere Boolean value φa contains individual truth values conjuncts φ1 φ2 cf c Table 1 For negation case φX ψX simply let T φ T ψ truth value ψa retrievable V T ψ truth value ψa In quantiﬁer case φX Y ψZ Y X Z Y deﬁnes T φX cid7 Y T ψ Z Y Again TET constructed step represents countingreﬁnement Boolean feature actually required note 10 outermost counting semantics variables X standard Boolean semantics quantiﬁers appearing internally φ Example 211 Graphs special kind relational models single binary edge relation e X Y case labeled graphs multiple unary relations li X representing different node labels An important feature 38 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 considered graph mining problems example deﬁne kernel functions graphs 9 number subgraphs speciﬁc structure H ﬁnite graph isomorphic H embedded G The unlabeled graph H nodes 1 k counts number subgraphs H described type contains variable Xi node H literal e Xi X j edge j H Then cid21 cid20 cid21 φH G cid4 H cid4 G H H cid22cid21 cid21 cid4 cid16 H cid4 cid7 X1 Xk cid23 eXi X j jei j TET feature corresponds φH G The case labeled graphs case H subgraph relationship edges G nodes matched nodes H present H treated similar manner cid4 cid4 cid4 G induced edges The preceding examples illustrate ability TET language represent coherent manner wide range features variety different relational learning frameworks There limitations TET language TETs essentially rooted ﬁrstorder logic reﬁned counting semantics replacing Boolean existential universal quantiﬁcation They represent features ﬁrstorder nature features deﬁned terms transitive closure relation deﬁned suitable extensions ﬁrstorder logic ﬁxedpoint logic transitive closure logic 11 For example paper p1 tracing chain citations starting paper p2 TETexpressible Also integervalued feature Erdös number author captured TET depends chain coauthorship relations undetermined length 3 TET discriminant function A TET deﬁnes feature objects relational domain TETdeﬁned features incorporated ways existing types predictive descriptive models For example deﬁne distance kernel functions TET value spaces VT making TET features usable standard clustering techniques SVM classiﬁers We introduce metric VT Section 4 In section ﬁrst build predictive model TET feature simple discriminant functions TET values functions form 7 d VT R Such discriminant functions directly lead binary classiﬁcation models We use denote binary class labels Then learn discriminant functions d d threshold value t assign class label tuple iff cid2 cid2 cid3cid3 cid2 cid2 cid3cid3 d V T d V T t 31 Deﬁning simple TET discriminant 11 We introduce simple type TETdiscriminant function The motivation particular form discriminant function propose twofold ﬁrst given TET discriminants eﬃcient learn evaluate Since discriminant function TET learning wrapper evaluation routine candidate TETs Section 5 eﬃciency important issue Second discriminant function deﬁnition motivated uniform generalization classic decision tree Naive Bayes models Deﬁnition 31 Let T TET A weight assignment β T assigns nonnegative real number nodes T A weight assignment written βr β 1 β m βr weight assigned root β weight assignment ith subtree For TET T node weights β deﬁne discriminant function dβ follows Let γ VT If γ f deﬁne dβ γ 0 If γ t T τ V consists single node β βr Deﬁne dβ γ βr If γ t μ1 μm μi multisetsVT deﬁne dβ γ βr mcid24 cid24 i1 γ cid4μi γ cid4cid17 f cid2 cid3 γ cid4 dβ 1 βr In following illustrate nature discriminant function We begin investigating TETs propositional setting turns simple discriminant closely related standard decision tree Naive Bayes models M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 39 Table 2 A propositional dataset b X c X t t f f t f t f X t 115 010 53 42 f 106 93 159 62 Fig 3 Decision tree TET discriminant d Fig 4 Naive Bayes model TET discriminant d Table 2 shows hypothetical dataset observations 100 cases Boolean attributes b c binary class label class values recorded To consistent relational notation view Boolean attributes unary relations X b X c X deﬁned observations X The entries table represent pairs n counts positive negative class given attribute value combination n Fig 3 left shows decision tree constructed data The nodes labeled counts positivenegative examples reach nodes An example e attribute values ae t f ce t n n instance classiﬁed positive precisely estimated positive probability 914 The right Fig 3 shows propositional TET b c This TET labeled weight assignment positive class examples weight node corresponds empirical frequency n satisfy conditions path root node For example ae t f ce t TET Fig 3 evaluates n n cid2 cid3 T e V cid2 t cid20cid2 t f t cid3cid22 f cid3 For value discriminant function evaluates cid2 cid2 cid3cid3 V T e β d 50100 1040 50100 914 1040 914 P cid2 cid21 cid21 ae t t cid3 12 A Naive Bayes model learned data Table 2 shown Fig 4 corresponding TET Again TET nodes labeled weight assignments corresponding empirical frequencies n Now example ae t f ce t evaluates TET value cid2 cid3 T e V cid2 t t f f t t f cid3 gives discriminant function value equal n n 40 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 cid2 cid2 cid3cid3 V T e β d 50100 1040 50100 3046 50100 P P P ae t P P ae t P ae t 3164 50100 P t P P t P t P ce t P P ce t P ce t 13 While 13 equivalent posterior probability P ae t t ce t obtain β β V T ed Naive Bayes model obtains ratio d weights equal odds ratio P ae ceP ae ce negative class frequencies n Naive Bayes model β similarly deﬁned d β V T e d n n The special cases 12 13 TETs emulating decision trees Naive Bayes models respectively generalize arbitrary propositional TETs follows To simplify matters consider TET T single free variable X vacuous root cid7 X Assume T n nodes root ith node labeled type τi X We denote φ p X Boolean feature conjunction types path root node including τi X Let node labeled weight P φ p Now consider example e It deﬁnes preﬁx T consisting nodes φ p e τie evaluates t Without loss generality assume nodes preﬁx 1 l l cid3 n Then discriminant function value e t τi t cid2 cid2 cid3cid3 V T e β d P lcid24 P φ p P φ p t τi t t lcid24 P P τi t φ p P τi t φ p t t i1 i1 Let ki cid2 0 number children node cid3 l preﬁx 1 l ki usually actual Boolean feature conjunction depends e deﬁnes relevant τi class number children τi TET T For j 1 ki let φ j preﬁx nodes contained subtree rooted jth child note φ j preﬁx If assume cid3 l child features φ j obtains j 1 ki independent given φ p P lcid24 i1 cid2 τi t P cid21 cid21 φ p cid3 t cid25 cid26 P τi t lcid23 i1 β V T e β V T e d d P P cid27 l cid27 l i1 τi t i1 τi t 14 β d β Thus way d interpreted odds ratio The independence assumption arrive interpretation appropriate decision tree Naive Bayes emulating TETs ﬁrst case actually vacuous preﬁx deﬁned example consists single branch For Naive Bayes TETs regular Naive Bayes assumption For TETs pure decision tree Naive Bayes structure independence assumption reasonable lead coherent probabilistic interpretation dd ratio Note easily construct TETs independence assumption infeasible logical dependencies different child features φ j Thus analysis leads general understanding nature discriminant function necessarily endow cases coherent probabilistic semantics cid4V W set branches T So far considered propositional TETs However analysis discriminant function case di rectly carries nonpropositional TETs For observe given concrete domain M transform TET T equivalent propositional grounding variableintroducing edges replacing cid4V M branch W T If groundings original branch labeled copies original weight assignment discriminant function deﬁned grounded TET discriminant function deﬁned original TET Thus interpretation discriminant function propositional TETs explains discriminant function general TETs additional assumptions observa cid4 deﬁned different substitutions tions general independence assumption implies features T cid4V W assumed independent dd ratio obtains approximation constants subTET T 14 weights deﬁned initial TET exact class frequencies ground features shared approximations obtained aggregating statistics groundings Section 5 cid4V T cid4V W Example 32 Consider following weight assignment TET Example 23 10 author A P 1 15 authorOf A P 1 P 2 20 citesP 2 P 1 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 41 We compute discriminant function value γ2 γ3 Example 26 f 7 t f 9 t 1 2 t f 10 1 γ3 t f 7 t f 8 t 2 1 t f 10 2 γ2 t Since underlying TET consists single branch m 1 levels recursive value deﬁnition At ﬁrst level recursion obtain dβ γ2 10 cid2 dβ cid2cid2 t f 9 t 1 cid3cid3cid3 2 dβ cid2cid2 t f 10 cid3cid3 To proceed compute values form t f k t l subTET T cid19 l cid16 cid2cid2 cid3cid3 dβ t f k t l 15 20 15 Plugging 15 gives cid16 cid16 dβ γ2 10 15 cid19 1 cid19 2 20 15 15 153 cid16 cid19 2 20 15 cid4 A P 1 cf 5 15 16 The rearrangement terms 16 read follows paper P 1 written author A contributes factor 15 discriminant function value citation paper A contributes factor 2015 Since γ3 total number authored papers citations papers 3 respectively 2 discriminant function value obtained dβ γ3 10 15 cid19 2 cid16 20 15 152 153 cid16 cid19 2 20 15 17 The preceding example points limitation discriminant function function value dγ depends certain ﬂat counts contained γ detailed countofcount structure On hand product factors determined simple counts turns TETs discriminant function emulate Markov Logic Networks following example illustrates Example 33 Consider MLN knowledge base 9 φi conjunctions literals The MLN deﬁnes weight model M cid28 n i1 countX φi X Mw e ncid24 cid2 e w cid3 countX φi X M i1 countX φiX iM value count feature 10 M The weight function models deﬁned discriminant function TET cid4 cid7 cid2 cid3 X 1 φ1X 1 cid2 cid3cid5 X n φ1X n weight assignment β 1 e w1 e wn MLNs formulas φi arbitrary quantiﬁerfree formulas emulated TET discriminant function For write φi disjunctive normal form k li jk mutually exclusive Then construction applied formulas φi j associated weights w yields discriminant function representation MLN weight function This representation size exponential length original formulas φi k li jk literals li jk individual disjuncts φi j cid29 cid27 cid27 j 4 TET metric After simple discriminant function previous section introduce second tool build predictive descriptive models directly TETdeﬁned features This consists deﬁnition metric value space VT TET T The metric deﬁned induction structure VT Following Deﬁnition 25 base case If VT t f deﬁne dtett f 1 dtett t dtet f f 0 42 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 The core induction step dtet consists speciﬁcation distance multisets values μ γ1 k1 γm μcid4 cid2 γ cid4 1 k cid4 1 γ cid4 lcid4 k cid4 l cid3 γi γ cid4 j come value space V T subTET T T dtet deﬁned V T After normalizing counts probability values pi kik1 p cid4 l view values probability distributions metric space V T dtet A standard way deﬁne metric distributions wellknown WassersteinKantorovich EarthMovers Distance k cid4 jk k cid4 1 cid4 j Deﬁnition 41 Let μ γ1 p1 γm pm μcid4 γ cid4 probability distributions p p 1 cid4 p cid4 1 γ cid4 lcid4 p cid4 l multisets V T counts normalized Let dtet metric V T The WassersteinKantorovich distance μ μcid4 cid3 cid30 cid2 cid2 cid3 cid2 dWK μ μcid4 inf q dtet j cid3 q γi γ cid4 j γi γ cid4 j inﬁmum taken probability distributions q V T V T marginal ﬁrst component equal p marginal second component equal p Note dWK valid metric ground distance dtet valid metric 4 cid4 We deﬁne dtetγ γ cid4 γ γ cid4 VT Deﬁnition 42 Let VT f t m γ γ cid4 VT Depending γ γ cid4 i1 multisetsVT assume dtet deﬁned VT 1 m Let f deﬁne dtetγ γ cid4 cid17 f γ γ cid4 f 0 1 f cid17 f 1 Eq 19 In nontrivial case γ γ cid4 cid17 f γ t μ1 μm γ cid4 t μcid4 1 μcid4 m μ1 μm cid2 1 μcid4 μcid4 m cid3 m multisets cid2 cid3 VT i1 Then cid2 γ γ cid4 cid3 dtet ωidWK cid2 cid3 μi μcid4 mcid30 i1 ω0 ωm 0 adjustable weight parameters cid28 ωi 1 Proposition 43 For T dtet metric VT values 0 1 18 19 Proof The statement clearly true base case VT t f For case VT f t m 0 1 VT 1 m Then dWK deﬁned multisets μi μcid4 convex combination 19 deﬁnes metric VT f By condition 0 1 i1 multisetsVT induction hypothesis dtet metric values VT metric 1 m ωi 1 values lie interval cid28 It remains extension 18 include f case satisﬁes properties metric dtetγ γ cid4 cid2 0 equality γ γ cid4 dtetγ γ cid4 cid3 1 symmetry dtetγ γ cid4 dtetγ cid4 γ clearly satisﬁed For triangle inequality consider dtetγ γ cid4 dtetγ cid4 γ cid4cid4 If γ γ cid4 γ cid4cid4 f sum zero equal dtetγ γ cid4cid4 If γ γ cid4 γ cid4cid4 f sum greater equal 1 greater equal dtetγ γ cid4cid4 cid2 The TET metric deﬁned computed transportation simplex algorithm specialized linear pro gramming algorithm solving transportation problem 37 Ling Okada 25 introduced faster algorithm computing Earth Movers Distance histogram However algorithm assumes ﬁxedsize histograms deal signatures distributions required recursive deﬁnition dtet In experiments CPU time required computing distances negligible compared CPU time computing TETvalues A simple theoretical analysis justiﬁes ﬁnding Assume want calculate distance TET values having simplicity shape uniform branching factor m height h Let n mh number nodes TET value assume transportation simplex needs computed TETvalue M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 43 Table 3 Distance matrices propositional TETs a1 a2 a1 a2 a1 a2 a1 a2 a1 a2 a1 a2 a1 a2 a1 a2 cid4 2 cid4 1 cid4 cid4 2 1 cid4 cid4 2 1 cid4 cid4 2 1 cid4 2 cid4 1 cid4 cid4 2 1 cid4 cid4 2 1 cid4 cid4 2 1 0 1 0 0 a1 a2 0 12 12 1 1 0 1 1 a1 a2 12 0 1 12 c 0 1 0 0 0 1 0 0 a1 a2 a1 a2 12 1 0 12 1 12 12 0 cid4 2 cid4 1 cid4 cid4 2 1 cid4 cid4 2 1 cid4 cid4 2 1 cid4 2 cid4 1 cid4 cid4 2 1 cid4 cid4 2 1 cid4 cid4 2 1 0 1 0 1 a1 a2 0 12 12 12 1 0 1 1 b a1 a2 12 0 12 12 d 0 1 0 1 1 1 1 0 a1 a2 a1 a2 12 12 0 12 12 12 12 0 Table 4 Author distance matrix normalization b false counts normalized a1 a2 a3 a4 a5 a1 0 a2 011 0 a3 011 002 0 a4 001 011 01 0 a5 013 002 002 012 0 a1 a2 a3 a4 a5 a1 0 a2 026 0 a3 037 022 0 b a4 023 019 014 0 a5 059 032 022 036 0 node takes polynomial time O mk k 37 k empirically 3 4 We recurrence running time T n TETdistance calculation T n mT cid19 cid16 n m cid3 cid2 mk O mT cid19 cid16 n m cid2 n k h cid3 O By master theorem h k T n O n h k T n O n k h We illustrate properties dtet metric Our ﬁrst example uses simple propositional TETs illustrate ﬂexibility dtet metric derives varying TET structures Example 44 Consider TETs ad Example 28 Table 1 gave possible conﬁgurations entity pairs a1 a2 associated TET values Table 3 shows distance matrices obtained evaluating dtet val ues For c d uniform weights different branches ωi 12 c ωi 14 d For better readability ease comparison rows columns matrices indexed a1 a2substructures entries table course function values We obtain following characteristics distance function deﬁned TETs This 01distance reference structure a1 a2 pair cid4 1 cid4 2 different structure distance 1 a1 a2 distance 0 pair reference structure b This metric identiﬁes structures a1 a2 cid4 1 cid4 2 assigns zero distance contains edge Otherwise structures distance 0 iff equal distance 1 c Here TET metric normalized edit distance relative primitive edit operations edge insertion edge deletion d This scaled 01 distance structures a1 a2 cid4 2 constant distance 0 iff different Note distinct structures distance 12 1 values agree TET The branches return distance 1 branches evaluate f ωi 14 weights gives total distance 12 cid4 1 Example 45 Table 4a gives distances values γi shown 6 distances authors ai Fig 1 deﬁned TET 1 44 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 The matrix shows according dtet clusters a1 a4 a2 a3 a5 authors distances authors groups order magnitude smaller distance authors different groups Comparing Fig 1 ﬁnds clusters deﬁned number papers written author ﬁrst cluster second Given difference number authored papers citation distribution secondary inﬂuence distance value example da1 a2 da1 a5 citation pattern papers a1 similar papers a2 papers a5 The preceding example highlights potential problem deﬁnition dtet differences toplevel counts appear dominating inﬂuence distance values While reasonable primary counts larger impact counts lower levels TET desirable control extent happening In following ﬁrst analyze general manner distances obtained certain TET values introduce method adjusting metric behavior adapted ﬁt closely needs speciﬁc applications We consider generic twolevel countofcount TET cid7V W rV W U sW U Assume variables W U typed W ranges subdomain size K U ranges sub domain size N We consider values form cid16 cid17 t f K k t f N n t n k cid18cid19 These values symmetric sense W rV W number n ssuccessors U cf values a2 a4 a5 6 Assuming K N ﬁxed values fully characterized parameters n k derive closedform expression pairs values form cid2 dWK n k cid3cid3 cid2 n cid4 cid4 k mink k K cid4 cid4 n n N cid4 k k K 20 cid4 cid4 cid18 N k k kk K This expression explains potential problems visible example Table 4a ﬁrst cid4 cid18 K distances tend small numbers Second distance dominated typically n n counts ﬁrst level TET Furthermore distance sensitive sizes difference depends K N domains variables range behavior 20 function actual counts n k n particular differences order magnitude K cid18 N N cid18 K Note issue akin situation standard attributevalue data certain numeric features dominate distance measure order magnitude measuring scale We address potential problems introducing normalization operation TET values k cid4 cid4 41 Value normalization In analogy standard normalization procedures numeric data introduce normalization operation TET values A standard normalization procedure numeric data linear transformation x cid8 ax b coeﬃcients b empirical distribution transformed dataset mean 0 variance 1 Note concrete coeﬃcients b depend dataset original empirical mean variance x normalization procedure general deﬁned hyperparameters 0 1 Instead standardizing numeric attributes 0 mean variance 1 assign different hyperparameters different attributes adjust impact different attributes overall distance function For TET values perform normalization scaling f counts replacing occurrences f k f ak The normalization guided dataindependent hyperparameters adjusted optimize behavior TET metric dtet speciﬁc purposes The concrete multiplicative factors depend hyperparameters empirical distribution TET values normalization performed The hyperparameters deﬁned normalization labeling sense following deﬁnition Deﬁnition 46 A normalization labeling TET cid2 cid2 cid4 τ V cid3 W 1 T 1V W 1 T V W m TmV W m cid3cid5 given vector y1 ym nonnegative real numbers normalization labeling subTETs T 1 m In following assume notational convenience m 1 T τ V W T cid4V W M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 Deﬁnition 47 Let T τ V W T Let Γ γ1 γn VT f cid4V W y y cid4 normalization labeling y cid4 normalization labeling T 45 cid4 We write γi cid2 t f k γi cid20 f γi1 ki1 γili cid22cid3 kili γi j cid17 f Deﬁne f avg 1n k ncid30 i1 k f cid17 f avg 1n k ncid30 licid30 i1 j1 ki j The normalization γi Γ hyperparameters y y cid4 given replacing k replacing γi j normalization γi j Γ cid4 γi j 1 n j 1 li hyperparameters y f yk f avgk f cid17 f avgk cid4 A normalization parameter y speciﬁes ratio total f non f counts values given dataset corre sponding branch labeled y Example 48 A normalization labeling basic bibliographic TET author A P 101 authorOf A P 1 P 210 citesP 2 P 1 The hyperparameters 01 10 experiments predicting hindex DBLP dataset cf Sec tion 61 Normalizing dataset consisting 5 values 6 gives normalized values cid17 γ 1 γ 2 γ 3 γ 4 γ 5 t t t t cid16 t f 0281 t f 0878 t 1 1 t f 078 t 2 1 f 0246 t f 0878 t 1 2 t f 0976 1 f 0246 t f 078 t 2 1 t f 0976 2 f 0281 t f 078 t 2 1 t f 0976 1 f 0246 t f 0976 3 cid18cid19 Observe 01 0281 0246 0246 0281 02462 3 3 2 3 1 2 0878 3 078 4 09761 2 1 2 2 The distance matrix obtained normalized values shown Table 4b One immediately sees range distance values spread interval 0 1 clustering according paper count disappeared The dissimilar authors a1 a5 papers citations vs papers citations However similar authors a3 a4 previously belonged different clusters Seeing a3 differs a4 addition paper citations makes intuitive sense distance measure optimized predicting hindex sees nearest neighbors 5 TET learning We ﬁrst learning problem want solve Our data consists model M sense Deﬁnition 21 In implementation M given relational database containing table r R table r contains tuples Marityr Ira true Furthermore given initial target table table consisting set examples class labels For example learning problem given data depicted Fig 1 a1 a3 a4 positive a2 a5 negative examples given 4 leftmost tables Table 5 Columns data tables headed synthetic identiﬁers Argi Columns target table class label 46 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 Table 5 Input data tables Arg1 a1 a2 a5 Arg1 p1 p2 p10 Arg1 Arg2 Arg1 Arg2 a1 a1 a5 p1 p2 p10 p3 p5 p9 p1 p2 p6 A a1 a2 a3 a4 a5 Label author paper authorOf cites target Table 6 Local target table A a1 a1 a2 a5 a5 a5 Label P p1 p2 p3 p8 p9 p10 target column headed variable names names free variables TET construct Thus given input want construct TET T A signature R author paper authorOf cites captures features A predictive class label given target Our general approach TET learning recursive topdown construction associates node local dis crimination task represented local target table In running example starting input data Table 5 initialize TET construction vacuous TET cid7 A If ﬁrst extension cid7 A P authorOf A P associate node authorOf A P local target table shown Table 6 This construction local target tables essentially construction local training sets FOIL 34 The construction new target table amounts problem transformation problem predicting label author transformed predicting label authorpaper pairs new target table effected taking consideration attributes authors papers additional relations authors papers exist data The exact speciﬁcation construction local target tables follows Let n TET node associated child n labeled type σ V W local target table ttnV L columns variables V label L Let n reached edge labeled variables W include possibility W 0 edge unlabeled Then n associated target table ttncid4 columns V W L deﬁned cid22 cid20 cid2 cid4 cid4 cid3 σ b true 21 ttncid4 V W L b l M V W l ttn I In case W 0 ttncid4 subset ttn containing elements σ true When building TET candidate treeextensions W σ V W scored based relational information gain RIG measure proposed 26 RIG values represent direct potential informativeness extension direct informativeness provided extensions step increase classpurity local target table Potential informativeness provided extensions introduce new entities target table subsequent steps enable discrimination positive negative examples additional features related new entities High RIG values unlike information gain decision tree learning example bounds guaranteed improvement classiﬁcation accuracy single construction step indicate potential improvement obtained construction steps After termination recursive subtree construction ﬁnal predictive accuracy gain subtree evaluated subtree pruned gain exceed given threshold The evaluation current TETs accuracy based concrete classiﬁcation model built TET feature It crucially use discriminant function model Section 3 weight assignment deﬁning discriminant function fast learn resulting discriminant function values validation set fast compute The overall wrapperevaluation discriminant function computationally eﬃcient If ﬁnal TET conjunction different classiﬁcation model discriminant function beneﬁcial use classiﬁcation model wrapperevaluation TET learning However complex models computationally expensive Furthermore appear case TET features learned discriminant function highly biased particular classiﬁcation model The discriminant function play role lightweight classiﬁcation model discover features useful complex model types similarly propositional case decision tree logistic regression model learning act feature selector subsequent use complex models like support vector machines cf Fig 2 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 47 Table 7 TET learning σ V W CAND build_TETData M Labeled table tt TET_node parent TET_node root 1 parentweight positive_class_frequencytt current_score predictive_scoreM root 2 3 EXTpossible_extensionsparent θvars θdepth σ V W EXT compute RIGtt σ V W 4 5 CAND candidate_extensionsEXTRIGvalues θRIG 6 7 8 9 10 11 12 13 14 cid4 construct_ttM tt σ V W tt nextChild new TET_nodeσ V W add nextChild child parent cid4 nextChild root build_TETM tt new_score predictive_scoreM root new_score current_score θscore remove nextChild parent current_scorenew_score Table 7 outlines TET learning algorithm It implemented procedure recursively expands initial TET It receives arguments data local target table tt classiﬁed pointer parent current node expanded pointer root TET constructed The initial build_TETM tt T T T new TET_Nodecid7V pointer initial TET vacuous root type cid7V The construction works follows Line 1 sets weight discriminant function d input node It relative frequency positive examples target table associated node weight d minus weight Thus discriminant function learned little extra cost parallel TET construction The function predictive_scoreM root called lines 2 11 performs global evaluation current TET based predictive performance conjunction chosen classiﬁcation model If model discriminant function calls predictive_scoreM root require computationally expensive model training current TET Lines 35 crucial subset possible extensions current node deﬁned types σ V W child nodes constructed exploration This operation analogous reﬁnement operators ILP Our construction steps ﬁrst step set possible extensions current node constructed function possible_ex tensions This function implement constraints language bias In implementation experiments restrict possible extensions terms number literals number new variables σ V W limiting numbers The function TILDEstyle userdeﬁned rmode declarations 3 force certain arguments new literal ﬁlled variables present parent node input variable new variable introduced extension output variable As common ILP algorithms type predicates specify type arguments literals In case learner expects unary predicate variable type true objects type In addition userdeﬁned bias force candidate extensions use latest introduced variables path root The rationale constraint extensions introducing new variables selected based RIG score likely potential direct informativeness By focusing search reﬁnements new variables force algorithm try making potential informativeness explicit When introducing new variable algorithm automatically adds equality constraints guaranteeing bound value root variables identifying entity TET represent features In case typed language bias inequality constraints added variables type In order control computational cost adding new variables constraint number variables path root leaf userdeﬁned maximum value θvars Finally possi ble_extensions implement termination condition depth current parent node TET reached user speciﬁed maximum depth θdepth possible_extensions return set In step relational information gain computed possible extensions function candidate_extensions performs selection based RIG values Our current implementation candidate_extensions selects extensions RIG value exceeds user deﬁned threshold θRIG A child node created candidate extension The function construct_tt constructs local target table child according 21 Lines 89 add new child labeled current candidate extension σ V W parent Line 10 continues recursive construction new child root new subtree Lines 1114 evaluate extension old TET new subtree accept reject based user deﬁned threshold required global score improvement θscore 48 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 Fig 5 The TET learned DBLP data set task hindex prediction The ﬁrst branch left represents relational features necessary suﬃcient compute hindices 6 Experiments 61 DBLP As stated introduction bibliometrics represents ideal domain test capability TETs learning countofcounts features particular focus task predicting hindices The hindex author A deﬁned maximum number h A authored h papers having h citations 14 The data set experiments taken DBLP Computer Science Bibliography 242 enhanced citation data 41 We extracted set facts relations form MySQL database original data set available httpwwwarnetminerorgcitation For sake reproducibility provide package scripts build data set experiments Supplementary Materials From original database extracted following tables author A providing id author paper P providing id paper author_of A P true A author P cites P 1 P 2 true P 1 cites P 2 Our ﬁrst goal learn TET able discriminate authors high low hindex abovebelow certain threshold To end extracted subgraph DBLP network Supplementary Materials consisting 8726 authors 244265 papers learning algorithm described Table 7 employing relational information gain 26 scoring function guiding search discriminant function Deﬁnition 31 evaluate TET score introduction new literal3 We chose hindex threshold h 7 deﬁne positive negative examples We ﬁxed θRIG 0 candidate extension nonzero RIG considered order given RIG score θscore 1e 4 order prune away branches low improvement θdepth 2 θvars 3 Fig 5 shows learned TET consists different branches ﬁrst corresponding countofcounts feature exactly compute hindex branches describing features correlated hindex author A number papers P cited paper P written A second branch number coauthors A paper P A One wonder learner return TET consisting ﬁrst branch feature suﬃcient predict hindex training examples exactly Note classiﬁcation accuracy training examples constrained prediction model maps TET feature value binary classiﬁcation The discriminant function learner virtually conceivable prediction model able exactly map feature value obtained ﬁrst branch binary threshold function h 7 additional features represented additional branches useful obtaining better ﬁt threshold cid4 cid4 The TET shown Fig 5 combination discriminant function guiding learning phase achieves F1 613 binary classiﬁcation task predicting authors h 7 As comparison employed TILDE 3 inductively learn logical decision tree setting The task turned diﬃcult TILDE author_of A P predicate potentially informative directly informative 26 language bias TET learner TILDE ended search tree We tried modify language bias allowing joint introduction predicates author_of A P cites P 1 P 2 result complex tree learned TILDE contained positive leaves covering examples heading F1 18 We tried deﬁne aggregates4 language bias counting number papers author number citations paper We TILDE option allows introduction multiple aggregates tree branch5 case search completed memory requirements We ﬁnally tried use exhaustive lookahead turned computationally expensive level lookahead TILDE ran 20 days terminating As comparison TET learning algorithm ran 7 minutes Finally order assess 2 httpdblpunitrierde 3 No particular rmode declaration speciﬁed 4 Aggregates speciﬁc rmodes TILDE deﬁning predicates represent aggregating functions count average min max 5 This option called aggregate_refinement M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 49 Fig 6 Results task hindex prediction years plot RMSE competitors function prediction year potential TET metric introduced Section 4 tested TET kNN classiﬁer leaveoneout setting TET metric TET values case F1 achieved classiﬁer value normalization 885 single neighbor 912 k 5 The second experiment hindex forecasting given data given year Y 0 predict hindex author forthcoming years In case aim measure discriminative power metric deﬁned TET values described Section 4 respect baselines prediction models plain counts The experimental framework constructed follows ﬁrst extracted DBLP set 8441 authors having hindex 3 Y 0 2000 split set 23 development set 13 test set development set split 23 training 13 validation Our predictor built follows simple TET shown ﬁrst branch Fig 5 describes suﬃcient features calculate hindex computed TET values authors run simple kNN algorithm employing TET metric distance values This predictor compared different competitors 1 predict future hindex equal current hindex SAME 2 predict future hindex test author average future hindices training authors having current hindex equal AVFUT 3 predict future hindex kNN algorithm distance linear combination plain counts features number papers npap number citations ncit hpred w pap npap w cit ncit kNN counts 4 predict future hindex nonlinear Support Vector Regressor taking plain counts features npap ncit input SVR 5 predict future hindex kNN algorithm TET metric learned TET represented Fig 5 compute TET values complete TET kNN Note albeit simple SAME AVFUT predictors signiﬁcant advantage methods direct use hindex materialized feature data methods given underlying paper citation counts The validation set perform model selection parameters model regularization parameter C 4 1 w pap w cit Gaussian kernel width γ tuned SVR C ranges 10 2 102 number neighbors k countsbased kNN normalization coeﬃcients ranging 10 y number neighbors k kNN TET metric6 Root Mean Squared Error RMSE measure performance predictor 2 102 γ ranges 10 Fig 6 shows results obtained function prediction horizon H years starting 2000 year TET values computed corresponding H 0 2009 H 9 The SAME AVFUT predictors construction 0 error H 0 remain accurate short prediction horizons dynamics hindices change slowly time The TET based predictor lower average prediction error starting H 5 outperforms methods based plain counts features It noticed 6 Model selection normalization coeﬃcients performed simple TET resulted normalization labeling shown Example 48 50 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 Table 8 F1 results CORA dataset Horizontal lines separate different learning parameter settings allowing increasingly complex TETs learned plus manually curated TET T hand Rows learning setting indicate different predictive model discriminant function DF kNN TET metric different values k 1 values producing nonnegligible differences reported Columns report results fold macroaveraged ﬁve folds θdepth θvars 1 1 2 2 3 3 3 3 2 2 3 3 4 4 4 4 hand hand hand hand hand Learn DF kNN1 DF kNN1 DF kNN5 kNN10 kNN100 DF kNN5 kNN10 kNN100 kNN500 1 892 892 892 892 792 880 894 866 892 936 942 947 976 2 906 906 906 906 906 906 957 957 906 992 989 997 997 3 901 901 908 903 589 709 786 908 908 968 971 980 980 4 952 952 952 952 896 888 942 958 952 966 977 993 993 5 880 880 880 880 880 880 811 811 880 913 945 953 953 avg 906 906 908 907 813 853 878 900 908 955 965 974 980 Table 9 Area recallprecision curve AURPC results CORA dataset Horizontal lines separate different learning parameter settings allowing increasingly complex TETs learned plus manually curated TET hand Rows learning setting indicate different predictive model discriminant function DF kNN TET metric different values k 1 values producing nonnegligible differences reported Columns report results fold macroaveraged ﬁve folds θdepth θvars 1 1 2 2 3 3 3 3 2 2 3 3 4 4 4 4 hand hand hand hand hand Learn DF kNN1 DF kNN1 DF kNN5 kNN10 kNN100 DF kNN5 kNN10 kNN100 kNN500 1 0926 0966 0926 0966 0872 0906 0932 0945 0926 0992 0992 0994 0995 2 0993 0993 0993 0993 0993 0993 0993 0993 0993 100 100 100 100 3 0940 0940 0946 0956 0986 0880 0928 0960 0946 0986 0995 0996 0992 4 0980 0980 0980 0980 0960 0925 0969 0982 0980 0998 0999 100 100 5 0957 0957 0957 0957 0957 0957 0957 0957 0957 0962 0962 0953 0971 avg 0959 0967 0960 0970 0954 0932 0956 0967 0960 0988 0990 0989 0992 kNN algorithm based complete learned TET performs slightly better simple TET longer prediction horizons This happens TET metric taking account countofcounts features performing consistently better plain countsbased metrics able exactly compute hindex starting features 62 Cora CORA dataset research papers citations originally collected Andrew McCallum different predictive tasks including hierarchical classiﬁcation information extraction citation matching Here focus task predicting bibliographic records refer paper We rely relational data representation experimental setting deﬁned Singla Domingos 40 The domain consists entities types title author venue given string value bibrec bibliographic record given author title venue ﬁelds title_word author_word venue_word constituent words appear ing title author venue Only ﬁrst author record considered setting The data set contains 1295 bibliographic records referring 132 different research papers 50 authors 103 venues Relations represent ing partof relationships title_of author_of venue_of link bibliographic records constituent ﬁelds word_in_title word_in_author word_in_venue link complete ﬁeld strings constituent words Note relations ﬁrst group onetoone second group onetomany The experimental setting Singla Domingos 40 consists ﬁve fold cross validation procedure plausible candidate pairs identiﬁed McCallum et als canopy approach 28 TFIDF cosine similarity measure This results 52923 overall candidate pairs 30971 positive 21952 negative pairs respectively The compiled dataset available alchemycswashingtonedu M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 51 Fig 7 Features CORA TETs entity resolution bibliographic records We ﬁxed θRIG 0 DBLP experiments θscore 1e 2 We ran TET learner ﬁve different training sets increasing values main parameters controlling size search space θdepth θvars Tables 8 9 report F1 Area recallprecision curve AURPC values fold macroaveraged ﬁve folds The simple discriminant function guiding TET learning phase DF compared kNN TET metric different values k 1 values producing nonnegligible differences reported We perform ﬁne tuning TET metric leaving hyperparameters Section 41 The ﬁrst apparent ﬁnding small TETs perform dataset increasing complexity pay Fig 7a shows TET learned 1 2 4 5 ﬁve folds θdepth 2 θvars 1 θdepth 3 θvars 2 parameter settings 2 5 folds θdepth 4 θvars 3 setting This represents basic feature records B 0 B1 having identical title andor venue ﬁelds It noteworthy TETs features involving author ﬁeld constructed This fact CORA dataset contains relatively different publications relatively small number ﬁrst authors author ﬁeld poor predictor identity papers Note simple pairs clearly different ﬁrst authors preliminarily excluded canopy construction 28 plausible candidates The difference simpler learning settings fold θdepth 3 θvars 2 setting learns TET Fig 7b TET learned simplest setting θdepth 2 θvars 1 lacks T W word_in_titleT T W branch The achieves slightly better results possibly correlation number title words likelihood entries refer paper Learning complex TETs provide improvements setting However inspection learned TETs gives interesting insights potential mined features Figs 7c d linear branches constitute main features complex TETs c ﬁrst appears fairly complex feature variable introductions represent threelevel hierarchical count However onetoone nature relations extensions V venue_of B0 V T title_of B T extension B venue_of B V introduces real counts 0 1 Roughly speaking feature c counts number records B venue B0 title B1 In TETs containing branch c dual branch roles B 0 B1 interchanged constructed Intuitively feature uses transitivity same_paper relation considering interpolating records B evidence equal B 0 agreement venue ﬁeld equal B1 agreement title ﬁeld viceversa Note dataset includes 103 distinct venues 132 papers makes venue discriminative title 52 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 Fig 8 Handcrafted CORA TET representing idflike countsofcounts features The dashed triangle indicates copy title subtree title replaced venue d reﬁnement left branch addition testing equality title ﬁeld d counts number additional records B title It appears introduction variable T 1 extension T 1 title_of B T 1 redundant T 0 place established unique title B simpler extension title_of B 0 T 0 express logical feature The reason roundabout way d ends taking deﬁning feature lies fact learning algorithm includes constraint variables introduced previous extension B case child node Section 5 With exception T W word_in_titleT T W branch TET b features discussed far use wordrelated relations Comparisons based identity title venue strings This different e shows complete TET learned fold θdepth 4 θvars 3 learning setting It reﬁnement sametitle feature time right subbranch introducing counts title words means T 1 word_in_titleT 1 T W title_of B0 T 1 countofcount feature incorporates inverse subsequent extension document frequency feature cf TET 2 Example 24 To understand meaning branch consider subTET rooted word_in_titleT 0 T W evaluates title word w w occur title B1 value f If w title B1 title B 0 value t t f k1 f k2 k1 k2 number titles domain contain respectively contain w If ﬁnally w occurs title B 1 B0 value t t t k1 f k2 k1 k2 The branch T W provides possible values count words w value In manner values right branch pro vide countofcount statistics required inversedocumentfrequency feature Note parsimonious representation 2 learnable current TET learner allow singleliteral nodes We learned complex TETs represent reasonable features In order better understand lack performance gain simpler TETs possibly lack predictive relevance features diﬃculty TET metric utilize feature information way presented TETs manually designed TET encodes idflike feature represented e way optimized TET metric Fig 8 It provides separate branches words titles venues dashed triangle indicates copy title subtree title replaced venue As shown rows Tables 8 9 TET achieves high performance paired kNN employing TETmetric perfect AURPC folds discriminant function fails exploit potential countsofcounts features We conclude TET learner able discover complex features high discriminative value The fact postprocessing representation features needed obtain best performance results kNN prediction surprising learner optimizing regard TET metric In future work kind postprocessing automated implementing eﬃcient techniques optimizing parameters TET metric weight parameters ω normalization labels y optimizing TET structure postpruning balancing operations The results learned TETs comparable achieved Markov Logic Network MLN like TET language independent contain rules referring speciﬁc strings occurring data achieves M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 53 AURPC 0971 40 Note MLN based approach 40 recent approaches achieving higher accuracy 3339 perform collective classiﬁcation exploit fact binary relation bibliographic records predicts equivalence relation The classiﬁcation models perform independent predictions pair bibliographic records expected achieve results competitive stateoftheart collective approaches It emphasized 3339 MLN structure set logical formulas carefully designed hand experiments TET structure learned data A carefully crafted TET described Fig 8 achieves macroaveraged AURPC 099 Additionally TET features equally connection collective classiﬁcation techniques We compared TET results achievable TILDE aggregates As DBLP case search procedure TILDE suffers lack direct informativeness single predicates plain TILDE returns tree folds However exhaustive lookahead allows overcome problem recover rules simple TET Fig 7a achieving substantially equivalent results macro averaged F1 910 slightly better simple TET Tilde learns rules fold More complex features like idflike ones Figs 7e 8 recovered plain Tilde adding aggregates language bias concerning counts author title venue words dramatically increases learning time search ﬁnish week CPU time As inductive logic programming relational rule learning approaches learning time strongly depends straints imposed search space exponential number candidate predicates Learning TETs takes roughly minutes hour 20 hours respectively average ﬁve folds increasingly complex learning settings θdepth 2 θvars 1 θdepth 3 θvars 2 θdepth 4 θvars 3 As matter comparison Tilde learns tree resembling TET Fig 7a 30 seconds 11 hours depending number exhaustive lookaheads allowed respectively 7 Conclusion Properties entities relational domain depend complex combinatorial countofcount features characterizing entities relational neighborhood Examples properties directly deﬁned terms countofcount features hindex author certain relevance measures widely information retrieval Type Extension Trees simple highly expressive representation language countofcount features In article presented method learning Type Extension Trees supervised learning settings means discovering countofcount features informative prediction class label Most existing frameworks statistical relational learning based simpler ﬂat count features use countofcount features implicit speciﬁcation conditional probability distributions include interpretable representation underlying features Examples frameworks ﬁrst kind Markov Logic Networks 36 cf Example 210 systems providing simple aggregation operators 11318 Examples frameworks second kind probabilistic relational models allow speciﬁcation conditional probability distribution nested combination functions 1630 Kernel methods applied implicitly extract features relational data The general framework volution kernels 12 originated wealth different approaches deﬁning similarity structured objects 45 references Features deﬁned kernels essentially count fragments substructures counts counts In cases methods aim develop suitable representation structured data subse quent learning discover features There previous works feature space learned relational data 2329 interpretable terms deﬁnite clauses In previous works relational feature construction integral particular learning paradigm Relational features right previously investigated 32 Here systematic view aggregationbased features different levels complexity developed However focus aggregation single level relational dependencies Discovered TET features variety classiﬁcation models integrated existing models relational probability trees 31 inductive logic programming systems simpler types count features 1 In paper considered approaches directly augmenting TET features prediction models The simple discriminant function fast learn evaluate makes limited use countofcount information provided TET feature value We introduced metric TET values deﬁned recursive application WassersteinKantorovich metric With metric distancebased methods supervised unsupervised learning directly applicable Our experiments shown TET learning algorithm able discover nontrivial interpretable countof count features A comparison classiﬁcation accuracies achieved discriminant function model knearest neighbor classiﬁcation based TET metric indicates TET features learned discriminant function support classiﬁcation models model exploits complex countofcount information outperforms models ﬂat counts 54 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 8 Supplementary material The software TET learning computation WassersteinKantorovich metric TET values data experiments presented paper downloaded httpwww3diismunisiitlippi researchTEThtml Acknowledgements AP ML PF partially supported PRIN grant 2009LNP494_002 References 1 A Van Assche C Vens H Blockeel S Dzeroski First order random forests Learning relational classiﬁers complex aggregates Mach Learn 64 2006 149182 2 Carl Bergstrom Measuring value prestige scholarly journals Coll Res Libr News 68 5 2007 314316 3 H Blockeel L De Raedt Topdown induction ﬁrstorder logical decision trees Artif Intell 101 12 1998 285297 4 Vladimir I Bogachev Aleksandr V Kolesnikov The MongeKantorovich problem achievements connections perspectives Russ Math Surv 67 5 2012 785 5 Sergey Brin Lawrence Page The anatomy largescale hypertextual web search engine Comput Netw 30 17 1998 107117 6 Luc De Raedt Paolo Frasconi Kristian Kersting Stephen Muggleton Eds Probabilistic Inductive Logic Programming Theory Applications Lecture Notes Computer Science vol 4911 Springer Berlin 2008 7 P Frasconi M Jaeger A Passerini Feature discovery type extension trees Proceedings 18th Int Conf Inductive Logic Programming ILP Lecture Notes Artiﬁcial Intelligence vol 5194 2008 pp 122139 8 N Friedman Lise Getoor D Koller A Pfeffer Learning probabilistic relational models Proceedings 16th International Joint Conference Artiﬁcial Intelligence IJCAI99 1999 9 T Gärtner P Flach S Wrobel On graph kernels Hardness results eﬃcient alternatives Proceedings 6th Annual Conference Compu tational Learning Theory 7th Kernel Workshop LNAI vol 2777 2003 pp 129143 10 Lise Getoor Ben Taskar Eds Introduction Statistical Relational Learning MIT Press Cambridge MA 2007 11 E Grädel Finite model theory descriptive complexity Finite Model Theory Its Applications Texts Theoretical Computer Science Springer 2007 Chapter 3 12 D Haussler Convolution kernels discrete structures Technical report 9910 UCSCCRL 1999 13 D Heckerman C Meek D Koller Probabilistic entityrelationship models PRMs plate models L Getoor B Taskar Eds Introduction Statistical Relational Learning MIT Press 2007 14 JE Hirsch An index quantify individuals scientiﬁc research output Proc Natl Acad Sci 102 46 2005 1656916572 15 Wilfrid Hodges Model Theory Cambridge University Press 1993 16 M Jaeger Relational Bayesian networks Proceedings 13th Conference Uncertainty Artiﬁcial Intelligence UAI13 Providence USA Morgan Kaufmann 1997 pp 266273 17 M Jaeger Type extension trees A uniﬁed framework relational feature construction Proceedings Mining Learning Graphs MLG06 2006 18 Y Kavurucu P Senkul IH Toroslu Concept discovery relational databases New techniques search space pruning rule quality improvement KnowlBased Syst 23 8 2010 743756 19 A Knobbe M Haas A Siebes Propositionalisation aggregates Proceedings PKDD 2001 2001 pp 277288 20 AJ Knobbe A Siebes D van der Wallen Multirelational decision tree induction Proceedings PKDD99 1999 pp 378383 21 Stanley Kok Pedro Domingos Learning structure Markov logic networks Luc De Raedt Stefan Wrobel Eds ICML ACM International Conference Proceeding Series ACM 2005 pp 441448 22 MA Krogel S Wrobel Transformationbased learning multirelational aggregation Proceedings ILP 2001 LNAI vol 2157 2001 pp 142155 23 N Landwehr A Passerini L De Raedt P Frasconi Fast learning relational kernels Mach Learn 78 3 2010 305342 24 Michael Ley The dblp science bibliography Evolution research issues perspectives Alberto HF Laender Arlindo L Oliveira Eds SPIRE Lecture Notes Computer Science vol 2476 Springer 2002 pp 110 25 Haibin Ling Kazunori Okada An eﬃcient earth movers distance algorithm robust histogram comparison IEEE Trans Pattern Anal Mach In tell 29 5 2007 840853 26 Marco Lippi Manfred Jaeger Paolo Frasconi Andrea Passerini Relational information gain Mach Learn 83 2 2011 219239 27 Nathan N Liu Qiang Yang Eigenrank rankingoriented approach collaborative ﬁltering Proceedings 31st Annual International ACM SIGIR Conference Research Development Information Retrieval 2008 pp 8390 28 Andrew McCallum Kamal Nigam Lyle H Ungar Eﬃcient clustering highdimensional data sets application reference matching Pro ceedings sixth ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD 00 New York NY USA ACM 2000 pp 169178 29 S Muggleton H Lodhi A Amini M Sternberg Support vector inductive logic programming Innov Mach Learn 2006 113135 30 S Natarajan P Tadepalli E Altendorf TG Dietterich A Fern A Restiﬁcar Learning ﬁrstorder probabilistic models combining rules Proceed ings 22nd International Conference Machine Learning ICML05 2005 pp 609616 31 Jennifer Neville David Jensen Lisa Friedland Michael Hay Learning relational probability trees Lise Getoor Ted E Senator Pedro Domingos Christos Faloutsos Eds KDD ACM 2003 pp 625630 32 C Perlich F Provost Aggregationbased feature invention relational concept classes Proceedings 9th ACM SIGKDD International Confer ence Knowledge Discovery Data Mining KDD2003 2003 33 Hoifung Poon Pedro Domingos Joint inference information extraction Proceedings AAAI07 2007 pp 913918 34 JR Quinlan Learning logical deﬁnitions relations Mach Learn 5 1990 239266 35 L De Raedt Logical Relational Learning Springer 2008 36 M Richardson P Domingos Markov logic networks Mach Learn 62 12 2006 107136 37 Yossi Rubner Carlo Tomasi Leonidas J Guibas The earth movers distance metric image retrieval Int J Comput Vis 40 2 2000 99121 38 Ajit Paul Singh Geoffrey J Gordon Relational learning collective matrix factorization Proceedings KDD08 2008 pp 650658 M Jaeger et al Artiﬁcial Intelligence 204 2013 3055 55 39 Sameer Singh Karl Schultz Andrew Mccallum Bidirectional joint inference entity resolution segmentation imperativelydeﬁned factor graphs Proceedings European Conference Machine Learning Knowledge Discovery Databases Part II ECML PKDD 09 Springer Verlag Berlin Heidelberg 2009 pp 414429 40 P Singla P Domingos Entity resolution Markov logic Proceedings ICDM06 2006 41 Jie Tang Jing Zhang Limin Yao Juanzi Li Li Zhang Zhong Su Arnetminer extraction mining academic social networks Proceedings KDD08 2008 pp 990998 42 Ben Taskar Carlos Guestrin Daphne Koller Maxmargin Markov networks Sebastian Thrun Lawrence Saul Bernhard Schölkopf Eds Advances Neural Information Processing Systems 16 MIT Press Cambridge MA 2004 43 G Van den Broeck N Taghipour W Meert J Davis L De Raedt Lifted probabilistic inference ﬁrstorder knowledge compilation Proceedings IJCAI 2011 2011 44 C Vens J Ramon H Blockeel Reﬁning aggregate conditions relational learning Proceedings PKDD 2006 LNAI vol 4213 2006 pp 383394 45 SVN Vishwanathan NN Schraudolph R Kondor KM Borgwardt Graph kernels J Mach Learn Res 99 2010 12011242