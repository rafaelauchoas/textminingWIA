Artiﬁcial Intelligence 156 2004 139176 wwwelseviercomlocateartint Theory revision queries Horn readonce parity formulas Judy Goldsmith a1 Robert H Sloan b2 Balázs Szörényi c György Turán cd3 Computer Science Department University Kentucky Lexington KY 405060046 USA b Department Computer Science University Illinois Chicago Chicago IL 606077053 USA c Hungarian Academy Sciences University Szeged Research Group Artiﬁcial Intelligence Aradi vértanúk tere 1 H6720 Szeged Hungary d Department Mathematics Statistics Computer Science University Illinois Chicago Chicago IL 606077045 USA Received 6 March 2003 received revised form 11 November 2003 accepted 15 January 2004 Abstract A theory context Boolean formula classify instances truth assignments Theories model realworld phenomena correctly The theory revision concept revision problem correct given roughly correct concept This problem considered model learning equivalence membership queries A revision algorithm considered efﬁcient number queries makes polynomial revision distance initial theory target theory polylogarithmic number variables size initial theory The revision distance minimal number syntactic revision operations deletion addition literals needed obtain target theory initial theory Efﬁcient revision algorithms given Horn formulas readonce formulas revision operators restricted deletions variables clauses parity formulas revision operators include deletions additions variables We query complexity readonce revision algorithm nearoptimal 2004 Published Elsevier BV Keywords Theory revision Knowledge revision Horn formulas Query learning Computational learning theory Boolean function learning Corresponding author Email addresses goldsmitcsukyedu J Goldsmith sloanuicedu RH Sloan szorenyirgaihu B Szörényi gytuicedu G Turán 1 Partially supported NSF grant CCR0100040 2 Partially supported NSF grants CCR9800070 CCR0100336 3 Partially supported NSF grants CCR0100336 CCR9800070 00043702 matter 2004 Published Elsevier BV doi101016jartint200401002 140 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 1 Introduction Sometimes model isnt right As scientists build models real world phenomena based limited data opinions sometimesfallible experts We verify begin use models discover correct Rather beginning modelbuilding phase prefer quickly simply revise current model continue project If initial model nearly correct efﬁcient The revision initial theory represented formula consists applying syntactic revision operators deletion addition literal For instance CUP theory1 presented Fig 1 revised accurate deleting literal white The revision distance target theory initial theory deﬁned minimal number revision operations speciﬁed ﬁxed set needed produce theory equivalent target starting initial theory As previous work 30 consider sets revision operators deletionsonly operators allow deletion literals clauses andor terms general operators allow addition literals Others implicitly explicitly considered models 3240 If target theory close initial theory efﬁcient revision algorithm ﬁnd quickly Thus revision distance relevant parameters deﬁning efﬁciency theory revision One way formalizing problem theory revision concept learning problem learn class concepts given revision distance initial theory A novel feature deﬁnition associates concept class concept sense assigns learning complexity individual concept precisely concept representation revision distance bound This help formalize intuitive elusive notion general hard easy target concepts learning theory For instance intuitively hard easy DNFs sense talk difﬁculty learning particular DNF On hand sense talk difﬁculty revising particular DNF So theory revision gives way quantify learning complexity DNF This article companion article 30 consider revision querybased learning models particular standard model learning membership equivalence queries denoted MQ EQ 5 This wellstudied model 2481113 15 nearly PAClearning In equivalence query learning algorithm proposes hypothesis theory h answer depends h c CUP hasconcavity white upwardpointingconcavity hasbottom ﬂatbottom lightweight hashandle widthsmall styrofoam Fig 1 Cup theoryconcept inspired Winston et al 57 Note additional variables current cup theory 1 Cups theory revision elephants computational learning theory AI general penguins nonmonotonic reasoning canonical toy example J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 141 c target theory If answer correct learning algorithm succeeded goal exact identiﬁcation target theory Otherwise answer counterexample instance x cx cid5 hx In membership query learning algorithm gives instance x answer 1 0 depending cx The query complexity learning algorithm number queries asks Note query complexity lower bound running time For running time count time required answer queries From formal theoretical point view assume oracles answer membership equivalence queries In practice membership queries need answered domain expert equivalence queries answered domain expert hypothesis waiting evidence error classiﬁcation It typical practical applications starts initial theory set counterexamples initial theory gives incorrect classiﬁcation The goal ﬁnd small modiﬁcation initial theory consistent examples In fact theory revision methods including algorithms presented work large number changes needed case efﬁcient learn scratch revising In setup simulate equivalence query running examples If ﬁnd counterexample current hypothesis continue simulation algorithm Otherwise terminate learning process current hypothesis serving ﬁnal revised theory In way efﬁcient equivalence membership query algorithm turned efﬁcient practical revision algorithm Besides motivation reasons speciﬁc theory revision justify use equivalence membership queries In practical applications case goal theory revision ﬁx initial theory provided expert It reasonable hope expert able answer queries classiﬁcation new instances For instance natural language applications possibility apparent everybody serve expert answering queries correctness sentences This means cases learning algorithms assumed use membership queries Another important reason study query model turns right model important learning problems That basic problems learning ﬁnite automata Horn formulas nontrivial efﬁcient learning algorithms model weaker models prove superpolynomial lower bounds In paper study important tractable classes formulas conjunctions Horn clauses readonce formulas Horn sentences tractable heart branches science The satisﬁability Horn sentences decided polynomialindeed lineartime 23 There combinatorial characterization functions expressed Horn sentences 21344454 Horn sentences applications For instance Horn sentences occur special cases logic logic programming databases Realworld reasoning causality described Horn theories If world like consequences 142 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 separately jointly Horn sentences model safe queries relational database theory 43 Given plethora Horn sentences imperative able mend broken The work presented paper ﬁrst step direction Similarly Horn formulas readonce formulas form nontrivial class tractable different aspects slight extensions intractable Boolean functions represented readonce formulas combinatorial characterization 3335 46 certain read restrictions CNF satisﬁability easily decidable polynomial time 38 It interesting tractable cases fault testing 39 Horn theory revision 2440 related readonce formulas The main results present paper revision algorithms Horn read formulas deletionsonly model revisions revision algorithm parity functions general model revisions Some lower bounds provided The ultimate goal work revise real expertsystem style theories Horn theories types queries argued feasible real human experts membership equivalence queries In course pursuit elusive goal achieved partial results restricted classes theories constrained revisions Our work parity formulas read formulas adds body theoretical work learning formulas queries 8 showcases techniques lowerbound proofs hope helpful later work They included mathematical elegance eventual applicability We include algorithms revision constrained deletions We note long history studying special case presumably greater tractability AI literature What deletions corresponds stuck faults usually studied diagnosing faulty circuits 1960s 1970s 39 instance case Koppel et al proved convergence empirical theory revision 1990s 40 We note scenarios deletionsonly useful intended application Say example mature expertsystem builder designed Hornformulabased theory sent apprentice populate theory The apprentice interviews experts enthusiastically writes expert says ignoring experts selfcorrections It turns model imperfect experts sound Thus expertsystem builder faced task revising theory From review apprentices methodology clear revisions require deletions As revision model expertsystem builder access experts ask types queries She use algorithm revise theory realizes efﬁcient algorithm available deletions case This precisely algorithm present Section 5 In section discuss previous work theory revision especially computational learning theory approaches theory revision Then Section 3 discuss meant learning revision propositional Horn formulas We formally deﬁne basic concepts learning queries Boolean formulas Section 4 In Section 5 revision algorithm propositional Horn J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 143 formulas We present revision algorithm readonce formulas Section 6 parity formulas Section 7 2 Previous work There extensive discussion related work theory revision computational learning theory literature actual AI systems literature companion paper 30 In section brieﬂy mention important somewhat arbitrarily selected articles In addition companion paper refer reader Wrobels overviews theory revision 5859 In section discuss depth papers given results called theory revision propositional Horn formulas different researchers actually considered different problems Mooney 45 initiated study theory revision computational learning theory approach based syntactic distances Mooney proposed considering PAC learnability class concepts having bounded syntactic distance given concept representation gave initial positive result sample complexity left computational efﬁciency open problem Numerous software systems built theory revision A representative examples EITHER 47 KBANN 52 PTR 40 STALKER 18 Many systems STALKER designed Carbonara Sleeman 18 called tweaking assumption initial theory fairly close correct theory This presumably case instance deployed expert errors On hand KBANN viewed solving essentially usual general concept learning problem backpropagation neural nets translating propositional Horn sentences possible starting learning ballpark concept instead default null concept It unclear KBANNs successes actually required initial theories tweak away correct Mooney implicitly assumes tweaking case theory revision Here appropriate learning resources number queries sample size depend polynomially revision distance subpolynomially size initial theory number variables domain consideration For instance want dependence Ologinitial theory size n n number variables domain The reason desirable tweaking assumption mean revision distance e cid6 maxinitial theory size n wish revise signiﬁcantly fewer resources learning scratch These considerations suggest relationship theory revision attribute efﬁcient learning 12151922 Attributeefﬁcient learning concerned learning concept scratch resources depend polynomially number variables target called number relevant variables logarithmically polylogarithmically total number variables universe Roughly speaking attributeefﬁcient learning special case theory revision initial theory default concept 144 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Angluin et al 7 querylearning algorithm Horn sentences Our revision algorithm given Section 5 modeled algorithm The primary difference learning revising Horn formulas formulas stringent query complexity bounds required revision opposed learning scratch For instance Angluin et als algorithm learn Horn formula n variables ask cid1n queries limited queries Readonce formulas efﬁciently learnable equivalence membership queries 8 While readtwice DNF formulas efﬁciently learnable 48 read thrice DNF formulas negative results 1 The query complexity learning algorithm readonce formulas On3 n number variables equivalently length formula In contrast revision algorithm readonce formulas uses Oe log n queries e revision distance initial target formulas There limited work theory revision predicate logic Greiner gives results theory revision predicate logic paper primarily revising propositional Horn formulas discuss section 32 In paper Greiner 31 gives results revision operators change order rules logic program These results ArgamonEngelson Koppel 10 Wrobel 58 theoretical results theory revision predicate logic 3 The dilemma Horns In literature learning propositional Horn sentences fact refers distinct deﬁnitions learning problems Although discussion issue 3 2025 think clariﬁcation possible sake clarify discussion related work especially Greiners related work 3132 1 Monotone circuit model Propositional Horn sentences variables observable problem classify instance given values observable variables The classiﬁcation instances observable variables depends sentence setting observable variables imply special output variable occurs head clauses This meaning EITHER theory revision 47 Mooneys theoretical PAC analysis theory revision 45 This model equivalent model complexity theorists monotone circuits 2 Assignments model Propositional Horn sentences classiﬁcation instance depends assignment variables agrees contradicts Horn sentence This meaning Angluin et al 7 article algorithm Section 5 3 Entailment Propositional Horn sentences instances Horn clauses clauses classiﬁcation depends entailed target Horn sentence This meaning Frazier Pitt work learning entailment 2526 Greiner 32 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 145 CUP UPRIGHT LIFTABLE OPEN GRASPABLE white UPRIGHT hasbottom upwardpointingconcavity LIFTABLE lightweight hashandle LIFTABLE widthsmall styrofoam OPEN upwardpointingconcavity GRASPABLE widthsmall GRASPABLE hashandle Fig 2 A monotonecircuit style Horn sentence CUP It deﬁne exactly set deﬁnition Fig 1 4 Atomic entailment The entailment setting 3 atoms instances Greiner considers case Consider Deﬁnition 1 An example Deﬁnition 1 style Horn formula CUP example Fig 1 readonce formula given Fig 2 The classiﬁcation variable CUP hidden variables UPRIGHT LIFTABLE OPEN GRASPABLE One sentence monotone circuit classiﬁcation variable corresponding output gate observables inputs hidden variables interior gates In fact monotone circuit equivalent Horn sentence Monotone circuits fairly rich class studied complexity theory Monotone circuits learnable equivalence queries smaller class monotone Boolean formulas learnable equivalence queries 372 To best knowledge open question monotone circuits learnable membership equivalence queries Deﬁnition 2 use revision result Section 5 The cup example Fig 2 follows Deﬁnition 2 variables including OPEN GRASPABLE visible In general Assignments model training data learning revising examples assignments variables In Deﬁnition 3 entailment hidden variables learned different We ask cup example Fig 2 following clauses entailed CUP LIFTABLE upwardpointingconcavity LIFTABLE lightweight Note example entailed Horn sentence Fig 2 The main point Model 4 entailment atoms use strong negative results Positive learning results useful Horn sentences n propositional variables unreasonably large set theories wants know n variables positive negative 2 One hardness learning monotone circuits equivalence queries applying standard variable substitution trick 36 cryptography result Goldreich et al things says class polynomialsize circuits learnable equivalence queries 27 146 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Angluin 3 provides discussion comparison cases Frazier 25 In particular Frazier shows convert query learning algorithm assignment model entailment model However Fraziers conversions general involve multiplicative blowup query complexity number variables domain n conversions automatically transfer theory revision results assignments model entailment model Further comparisons different approaches given De Raedt 20 Greiner 32 considers Models 3 4 entailment clauses atoms Loosely speaking Greiner shows nontweaking cases theory revision Models 3 4 NPcomplete absence membership queries general deletions model More precisely considers PAC model interested decision problem given sample Horn clauses labeled entailed entailed deciding Horn sentence stated revision distance d classify clauses Greiner shows entailment atoms model d cid1 ϕ ϕ size initial theory ϕ problem NP complete It nonapproximable sense ﬁnd Horn sentence agrees 90 classiﬁcations sample given usual complexity theory assumptions 32 Note Greiners hardness results contradict results First allow membership queries addition samplingequivalence queries Some classes exponential querysample complexity samplingequivalence queries polynomial query complexity membership equivalence queries Readonce Boolean formulas example class 837 On hand arbitrary Boolean formulas difﬁcult learn membership equivalence queries 9 Another distinction Greiners negative results primarily interested smaller values revision distance d cid1 ϕ 4 Preliminaries We use standard model membership equivalence queries counterexam ples denoted MQ EQ 5 In equivalence query learning algorithm proposes hypothesis concept h answer depends h c c target concept If answer correct learning algorithm succeeded goal exact identiﬁcation target concept Otherwise answer counterexample instance x cx cid5 hx For readonce formulas parity functions equiv alence queries proper hypothesis revision initial formula For Horn sentences equivalence queries proper meaning hypothesis conjunction Horn clauses Horn clause revision Horn clause initial formula revision single initial theory Horn clause hypothesis3 In membership query 3 Our lower bound Horn sentence revision permit proper equivalence queries J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 147 learning algorithm gives instance x answer 1 0 depending cx MQx cx c target concept We use standard notions propositional logic variable term monotone We assume true false formulas representation class formulas study paper The all0 vector denoted 0 all1 vector 1 We occasionally use standard partial order vectors x cid1 y component x equal corresponding component y The symbol denotes proper subset A Horn clause disjunction unnegated variable usually think implication clauses unnegated variable head negated variables body We write bodyC headC body head C respectively A clause unnegated variables considered head F written body F A Horn sentence conjunction Horn clauses For monotone terms s t use s t term product variables s t As example x1x2 x1x3 x1 Thus s t different s t product variables occurring s t When convenient treat Horn clause bodies monotone terms vectors 0 1n treat vectors subsets n If x 0 1n Horn clause C bodyC x x covers C Notice x falsiﬁes C x covers C headC x By deﬁnition F x Our Horn sentence revision algorithm makes frequent use fact x y cover clause C x y falsiﬁes C x y falsiﬁes C A Boolean formula ϕ readonce formula called µformula Boolean tree variable occurrence ϕ operations Such formula represented binary tree internal nodes labeled leaves labeled distinct variables constants 0 1 For technical reasons extend standard notion allow constants leaves The internal nodes correspond subformulas We subformula ϕ constant computes constant function A constant subformula maximal subformula constant subformula By Morgan rules assume negations applied variables As consider readonce formulas deletionsonly model know sign variablewe replace negated variables new variables keeping mind truth assignment handled accordingly Thus loss generality assume variable unnegated use readonce formulas A Boolean function read equivalent readonce formula A substitution partial function σ x1 xn cid4 0 1 Given substitution σ let ϕσ formula obtained replacing variable xi ϕ domain σ σ xi Substitutions σ1 σ2 equivalent respect ϕ ϕσ1 ϕσ2 compute function For lower bound revising readonce formulas shall use known notion VapnikChervonenkis dimension 55 Boolean functions Let C set Boolean 148 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 functions domain X We Y X shattered C Z Y cZ C cid1 cZx 1 0 x Z x Y Z Then VCdimC maxY Y X Y shattered C VCdimension C 41 Theory revision deﬁnitions Let ϕ Boolean formula variables x1 xn The concept represented ϕ set satisfying truth assignments ϕ For instance ϕ x1 x2 x1 x3 concept 110 101 111 With exception Section 7 revision operator ﬁxing occurrence variable formula constant 0 1 For instance ﬁx x2 ϕ 1 obtain revised formula x1 1 x1 x3 simpliﬁed x1 x1 x3 equivalent x1 If instead ﬁx second occurrence x1 0 obtain revised formula x1 x2 0 x3 simpliﬁed x1 x2 Because effect ﬁxing literal constant DNF CNF formulas delete literal clause term refer ﬁxing occurrence variable constant deletion Note readonce formulas instead corresponds stuckat faults fault detection For readonce formulas occurrence variables ﬁxed write revision substitution notation σ xi ci ci constant For example applying substitution σ x2 1 x3 1 formula ϕ2 x1 x2 x3 gives revised formula ϕ2σ x1 1 1 simpliﬁes x1 In Section 7 allow addition variable revision operator Handling operator difﬁcult consideration parity formulas provides example tractable class We denote Rϕ set formulas obtained ϕ ﬁxing occurrences variables constants The corresponding concept class denoted Cϕ The revision distance formula ϕ concept c Cϕ deﬁned minimum number applications speciﬁed set revision operators ϕ needed obtain formula c Thus example showed earlier revision distance ϕ x1 x2 x1 x3 concept represented x1 1 A revision algorithm formula ϕ access membership equivalence oracles unknown target concept c Cϕ return representation Rϕ target concept Our goal ﬁnd revision algorithms query complexity polynomial revision distance ϕ target polylogarithmic size ϕ size variable set The total running time algorithms polynomial size ϕ revision distance number attributes course instances read written We explicitly calculate exact asymptotic running times typically drastically worse query complexity number attributes times query complexity expect J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 149 query complexity generally training data constraining factor practical applications In fact results provide stronger revision algorithm The algorithms paper revise class concept classes That algorithms metaalgorithms formula ϕ speciﬁed class formulas read formulas function revision algorithm concept class Cϕ Notice choice revision operators plays double role First deﬁnes concept class things reachable speciﬁed formula revision operators Second determines revision distance gives performance metric 5 Revising propositional Horn sentences In section algorithm revising Horn sentences deletionsonly model Angluin et al 5 gave algorithm learning Horn sentences queries Their algorithm query complexity Onm2 n number variables m number clauses This complexity unacceptable revision task revision distance e smaller number variables n We algorithm REVISEHORN displayed Algorithm 1 query complexity Oem3 m4 independent n In following subsection details algorithm Section 52 lengthy example run algorithm The reader ﬁnd helpful switch forth subsections The analysis query complexity proof correctness Section 53 In Section 54 provide lower bound 51 Overview algorithm The highestlevel structure Algorithm REVISEHORN similar structure Angluin et als algorithm learning Horn sentences 5 DNF revision algorithm 30 making appropriate changes duality CNF form Horn sentences DNF form The presentation section selfcontained assume familiarity papers We start hypothesis conjunction classiﬁed true repeatedly outer loop lines 222 equivalence queries correct Horn sentence found4 Each negative counterexample help membership queries subroutine SHRINKEXAMPLE hypothesis restrictive positive counterexample hypothesis general We observe following fact negative instances implicit use 4 It somewhat surprising start conjunction initial theory unable ﬁnd revision algorithm good query complexity starts initial theory 150 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 hx 1 x negative counterexample 1 h hypothesis true 2 x EQh cid5 Correct 3 4 5 6 x SHRINKEXAMPLEx ϕ h clausegroup C h order bodyC x bodyC MQbodyC x 0 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 bodyC x bodyC headC cid5 F Add headC variable deleted bodyC head clause ϕ end break loop end end x wasnt shrink clausegroup h h h x F end x positive counterexample clause C h Cx 0 headC cid5 F Delete C h Change C clausegroup heads head clause ϕ x bodyC end end 23 24 25 26 end 27 return h end Algorithm 1 REVISEHORN Revises Horn sentence ϕ Proposition 1 Every negative instance CNF formula falsiﬁes clause CNF formula Each negative counterexample ﬁrst processed subroutine called SHRINKEX AMPLE Algorithm 2 discuss shortly In general subroutine change certain 1s 0s leaving negative counterexample negative counterexample current hypothesis Following Angluin et al ﬁnd convenient organize hypothesis distinct clause bodies We collection clauses Horn sentence body clausegroup Angluin et al called clausegroup meta clause We use notation body x1 x4 x5 denote clausegroup body body heads x1 x4 x5 shorthand conjunction clauses body x1 body x4 body x5 Algorithm REVISEHORN attempts use negative counterexample x returned SHRINKEXAMPLE deletions body existing hypothesis clause group C This ﬁrst bodyC x bodyC deletions bodyC We need bodyC x negative instance J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 151 true clause C0 ϕ bodyC0 cid5 x MQx x x false bodyC0 1 repeat 2 3 4 x 5 6 end 7 8 end 9 10 return x bodyC0 0 Algorithm 2 SHRINKEXAMPLEx ϕ h If update bodyC bodyC x variables deleting bodyC possible heads add variables heads C For instance negative counterexample x 11000011 hypothesis clause group x1x2x3x4 x7 x8 MQx x1x2x3x4 MQ11000000 0 x3 x4 heads initial theory clauses hypothesis clausegroup updated x1x2 x3 x4 x7 x8 If hypothesis clausegroup body edited way hypothesis restrictive adding new clause speciﬁcally x F Notice ﬁrst counterexample add new hypothesis clause Positive counterexamples hypothesis general We edit hypothesis clause falsiﬁed positive counterexample If positive counterexample falsiﬁes hypothesis clause head F clause simply deleted In practice effect deleting heads clausegroup multiple heads That fact follows lemmas prove Section 53 If instead positive counterexample x falsiﬁes clausegroup C head F means x covers C In case C heads added making general In fact add possible heads Speciﬁcally line 22 REVISEHORN adds heads clausegroup C heads clauses ϕ correspond 1s x bodyC 511 Shrinking negative examples The point Algorithm SHRINKEXAMPLE negative counterexample x current hypothesis decrease Hamming weight x Ideally x contain 1s positions corresponding body initial theory clause C0 target clause C x falsiﬁes derived Then use x introduce new hypothesis clause new hypothesis clause extraneous variables If instead use x deletions hypothesis clausegroup body smaller counterexample helpful produces deletions We following observation use help explain Algo rithm SHRINKEXAMPLE 152 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 clause C h 1 answer x y 2 repeat 3 4 5 6 7 8 answer changed end end x covers satisﬁes C answer falsiﬁes C Change headC 1 answer Algorithm 3 x y respect Horn sentence h Proposition 2 If target formula clause C revision initial theory clause C0 bodyC bodyC0 Proof This follows revision operator allow deletion cid1 Now notice negative counterexample x falsiﬁes target clause C revision initial theory clause C0 x bodyC0 falsiﬁes C Proposition 2 bodyC bodyC0 Thus like clause C0 initial theory MQx bodyC0 0 set x x bodyC0 However issue pay careful attention We need sure process intersecting x initial theory clause bodies change x example current hypothesis classiﬁes positive current hypothesis classiﬁes negative This use funny notation C0 instead x C0 lines 4 5 SHRINKEXAMPLE explain x Let h collection Horn clauses The operation respect h usually understood current constructed hypothesis formally deﬁned result pseudocode given Algorithm 3 The idea result x y result x y hypothesis clauses C x y covers bodyC x 1 position headC case 1 stays regardless y hypothesis clause Example Imagine current hypothesis h x1x2 x5 x1x2 x6 x5x6 x7 Notice way hypothesis contains clauses clausegroups Now 1111111 x1x2x3 1110000 111111 x1x2x3 1110111 operation set answer 1110110 second 1110111 The ﬁrst loop We easy observation operation prove operation property want terms making sure output satisﬁes hypothesis Proposition 3 For x y x y x y x J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 153 Lemma 4 If x satisﬁes hypothesis h y instance x satisﬁes h y respect h Proof There different ways instance satisfy Horn clause covering clause covering clause having clauses head set 1 We y know x satisﬁes clause ch h If x cover ch x x y x If x cover ch x headch set 1 x satisﬁes ch Now procedure y satisfy ch having headch set guarantees x y covers ch x 1 cid1 The interesting thing Algorithm SHRINKEXAMPLE repeatedly loops initial theory clauses continuing look deletions x pass initial theory clauses changing x We need repeated looping guarantee property output SHRINKEXAMPLE proved later Lemma 8 52 An example run REVISEHORN We example run REVISEHORN Suppose variable set x1 x2 x3 x4 x5 target formula ϕ target formula ψ given ϕ x1x2x3 x4 x2x4 x1 x2x4 x5 ψ x1x2x3 x4 x2x4 x1 x2 x5 Algorithm REVISEHORN initializes hypothesis h true conjunction Say EQh 11101 negative counterexample So SHRINKEXAMPLE11101 ϕ h It ﬁrst determines 11101 x1x2x3 11100 cid5 11101 asks query MQ11100 learns 11100 x2x4 01000 cid5 11100 negative instance x reset 11100 Next 11100 query MQ01000 0 x reset 01000 Since initial formula x2x4 010000 Now SHRINKEXAMPLE clause body second 01000 x1x2x3 01000 begins second iteration main loop This time 01000 01000 x2x4 01000 x altered value 01000 returned Accordingly hypothesis updated REVISEHORN h x2 F The main loop REVISEHORN makes equivalence query EQh time EQh 11111 positive counterexample Since hypothesis clause head F line 22 REVISEHORN puts possible heads updating hypothesis h x2 x1 x4 x5 154 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Now suppose EQh 11001 This positive counterexample causes hypothesis clause x2 x4 falsiﬁes deleted The hypothesis updated h x2 x1 x5 This time EQh 11101 Now SHRINKEXAMPLE 11101 x1x2x3 11101 11100 11101 falsify hypothesis clause x2 x5 Next x2x4 11001 membership query MQ11001 1 Since 11101 membership query returns 1 SHRINKEXAMPLE modify input returns 11101 hypothesis x2 x1 x5 x1x2x3x5 F Now EQh 11111 positive counterexample We change heads second hypothesis clausegroup hypothesis x2 x1 x5 x1x2x3x5 x4 Now EQh 01111 positive counterexample REVISEHORN removes x1 head ﬁrst hypothesis clausegroup updating hypothesis x2 x5 x1x2x3x5 x4 01111 Say time EQh 01111 When SHRINKEXAMPLE called ﬁrst determines x1x2x3 01101 MQ01101 1 change x Next x2x4 01011 MQ01011 0 x changed 01011 No changes 01111 x SHRINKEXAMPLE 01011 returned SHRINKEXAMPLE Now REVISEHORN x2 01011 x2 editing ﬁrst hypothesis clausegroup considered Next x1x2x3x5 01011 x2x5 membership query MQ01001 1 tried returns 1 second hypothesis clausegroup edited Instead new clausegroup added giving hypothesis x2 x5 x1x2x3x5 x4 x2x4x5 F Now EQh 11011 Then REVISEHORN use positive counterexample change clausegroup arrive x2 x5 x1x2x3x5 x4 x2x4x5 x1 Finally EQh Correct Notice way ﬁnal correct hypothesis exactly form stated equivalent resolution 53 Horn revision algorithm correctness Once established Algorithm REVISEHORN halts correctness follows form We prove bound query complexity series lemmas Several lemmas involve proving property hypothesis invariant We point places hypothesis changed place hypothesis clausegroup bodies created altered places set heads clausegroup altered One J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 155 positive counterexample edit hypothesis lines 1824 REVISEHORN The moving clausegroup body variable head clausegroup Line 9 REVISEHORN We begin observation heads hypothesis clauses We prove facts SHRINKEXAMPLE heart making query complexity independent number variables n Proposition 5 Every head hypothesis clausegroup F head initial theory clause We record following lemma fact x remains negative counterexample current hypothesis modiﬁed SHRINKEXAMPLE Lemma 6 If x negative instance satisfying Horn sentence h instance returned SHRINKEXAMPLEx ϕ h negative instance satisfying h Proof As algorithm proceeds x modiﬁed x membership query guarantees x returned instance negative bodyC0 immediately bodyC0 negative instance Thus Lemma 4 says x satisﬁes h x y y cid1 Next proceeding bound query complexity entire REVISEHORN algorithm bound query complexity SHRINKEXAMPLE algorithm Lemma 7 Algorithm SHRINKEXAMPLE makes Om2 membership queries m number clauses initial theory ϕ Proof Each iteration outer repeat loop makes query clause ϕ To prove lemma prove 2m 1 iterations outer repeat loop Each time iteration outer loop x altered The way x altered changing 1s 0s For given initial formula clause C0 ϕ set x x bodyC0 know set 0 position x bodyC0 head hypothesis clause Thus x altered head hypothesis clause plus initial theory clause The heads hypothesis clauses subset heads initial theory clauses m m initial theory clauses Thus x altered 2m times outer loop execute 2m 1 times desired cid1 Now output SHRINKEXAMPLE connected notion revision initial formula Lemma 8 Let x output SHRINKEXAMPLE For target clause C x falsiﬁes initial theory clause C0 C revision C0 position 156 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 1 x corresponds variable bodyC0 corresponds head initial theory clause Proof Assume contradiction C0 C x contains 1 position bodyC0 head initial theory clause Consider ﬁnal iteration outer repeat loop Algorithm SHRINKEXAMPLE Note x unchanged ﬁnal iteration algorithm We derive contradiction showing x changed bodyC0 cid5 x If By assumption x know x bodyC0 negative instance target contradiction x x modiﬁed line 5 SHRINKEXAMPLE forcing iteration outer repeat loop Now x falsiﬁes C x headC set 0 Therefore x bodyC0 bodyC0 covers C shown headC set 0 If x bodyC0 negative instance Because x falsiﬁes C x x covers C Since C revision C0 bodyC bodyC0 By Proposition 3 x bodyC0 x bodyC0 covers C cid1 bodyC0 x Now clausegroup body falsiﬁes clause target Horn sentence target clause falsiﬁed clause group body We ﬁrst prove ﬁrst facts prove lemmas need prove second Lemma 9 Each clausegroup body hypothesis falsiﬁes clause target concept Proof The body clausegroup negative instance target This true clausegroup ﬁrst added line 15 SHRINKEXAMPLE Lemma 6 maintained invariant guaranteed membership query immediately changing clausegroup body line 7 REVISEHORN cid1 Lemma 10 For hypothesis clausegroup C head F target clause C bodyC falsiﬁes headC heads C Proof When created hypothesis clausegroup head F When ﬁrst change clausegroup Cs head F know Lemma 9 bodyC falsiﬁes target clause Also know existence counterexample covers bodyC classiﬁed target positive target clauses falsiﬁed bodyC head F At point possible heads When delete variable clausegroup body possible head add We remove head positive counterexample guarantees removed cid1 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 157 From lemma clausegroup hypothesis altogether deleted hypothesis revised ways Corollary 11 No hypothesis clausegroup introduced deleted Proof The way potentially happen line 20 REVISEHORN removed clause head particular hypothesis clause group C Consider hypothesis clausegroup C If headC F head F operation remove If C heads F Lemma 9 bodyC falsiﬁes target clause C Now Lemma 10 heads clausegroup C headC There positive counterexample falsifying hypothesis clause body bodyC head headC head clausegroup C deleted cid1 Lemma 12 For given hypothesis clausegroup C variable added head Proof Heads initially added clausegroup head ﬁrst changed F line 22 REVISEHORN After happens clausegroup head F Thereafter heads added deleted body Because additions hypothesis bodies heads previously heads hypothesis clause body Once deleted body restored cid1 Lemma 13 No hypothesis clausegroup bodies falsify target clause Proof We follow proof Angluin et al 5 analogous statement algorithm learning Horn sentences scratch We ﬁrst following claim implies lemma prove claim Claim Consider clausegroup bodies b1 b2 bh hypothesis h order added For j bj falsiﬁes target clause C bi j covers C Assume claim true bodies hypothesis clausegroups Ck Ccid6 falsify target clause C WLOG k cid6 This contradicts claim bodyCk falsiﬁes C Now prove claim true induction number changes hypothesis It certainly vacuously true initial hypothesis We property remains invariant alter hypothesis Positive counterexamples change set clausegroup bodies need consider negative counterexamples Consider ﬁrst case modifying clausegroup body bj line 7 Algorithm RE VISEHORN setting bj x bj bj bodyCj After modiﬁcation bj cover clauses target formula need worry 158 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 clausegroups bi j Suppose contradiction bj falsiﬁes new target clause C bi covers C j It change bj covered C x falsiﬁed C Now bi x falsiﬁes C bi covers C x falsiﬁes C Therefore x edit bi loop lines 5 13 REVISEHORN long bi x bi What happens bi x bi Since bi x bi falsiﬁes C bi falsiﬁes C By Lemma 10 bis clausegroup head F headC heads Therefore x satisfy bi s clausegroup cause x covers bi x falsiﬁes C Lemma 6 says x satisfy clause hypothesis Next consider case adding new clausegroup body b x line 15 Algorithm REVISEHORN x returned Algorithm SHRINKEXAMPLE Suppose contradiction b falsiﬁes C hypothesis clausegroup body bi covers C Since b x falsiﬁes C bi covers C bi x falsiﬁes C statement line 6 directed algorithm use x edit bi long bi x bi If instead bi x bi bi falsiﬁes C Lemma 10 b satisfy bi s clausegroup contradicting assumption x satisﬁes hypothesis cid1 Theorem 14 Algorithm REVISEHORN uses Om3e m4 queries revise Horn sentence containing m clauses needing e revisions Proof First remember Corollary 11 particular clausegroup added deleted By Lemmas 9 13 number clausegroups number clauses target formula Thus worst case clausegroup C introduced hypothesis target clause C Let consider queries clausegroup C generate lifetime algorithm Its creation required Om2 queries SHRINKEXAMPLE negative counterexample plus Om main code REVISEHORN Next consider manipulation heads clausegroup There m heads introduced clause plus F By Lemma 12 removed moved exactly Each edit uses O1 queries Finally consider use negative counterexamples edit body clause group C By Lemma 7 negative counterexample cost Om2 queries We overall query bound showing number edits body C Om e At point run algorithm bodyC falsiﬁes target clause C Lemma 9 By Lemma 8 variables bodyC fall categories 1 Those bodyC deleted 2 Variables heads initial theory clause 3 Variables initial theory clause C0 C derived C That variables need revision Now m heads initial theory clauses e variables need deleted J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 159 This proof Lemma 9 says bodyC falsify body target clause target clause A negative counterexample cause bodyC change target clause body falsiﬁes We argue happen m 1 times bodyC ceases falsify particular target clause life falsify target clause This way clausegroup body stop falsifying target clause C having variable bodyC deleted clause group variable deleted clausegroup body Moreover m e edits bodyC accounted items 2 3 total entire life clausegroup C period clausegroup C associated particular target clause This m total number heads initial theory clauses deleted C lifetime heads deleted replaced Similarly e upper bound total number deletions initial theory clauses variables deleted replaced Since m hypothesis clausegroups total algorithm requires Oem3 m4 queries cid1 54 A lower bound revising Horn sentences In subsection lower bound query complexity revising Horn sentences The argument shows general escape dependence number clauses initial formula We Horn sentence cid1m queries required single deletion revision Note specify target function advance usual adversary arguments need sure adversarys responses consistent target function The technical argument similar lower bound revising DNF 30 transformed CNF form Horn sentences Consider variables x1 xn y1 yn let ϕn c1 cn 1 n ci x1 xi1xi1 xn yi F Theorem 15 The formula ϕn requires n 1 membership equivalence queries revised equivalence query conjunction Horn clauses Horn clause body revision body clause ϕn known exactly literal yi deleted Proof We adversary answer queries possible revision algorithm way forces revision algorithm claimed number queries Let ψi formula obtained ϕn deleting single occurrence variable yi Initially concept Ψ ψ1 ψn possible target concept adversary strategy eliminate concept Ψ query revision algorithm This implies claimed lower bound 160 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Let use ordered pairs x y denote truth assignments 2n variables ﬁrst component x truth assignment xi s second component truth assignment yi s A membership query x y answered follows If x n 2 bits 1 MQx y 1 This eliminate concepts Ψ If x n 1 bits 1 position xi 0 MQx y yi If yi 1 eliminate concept Ψ If yi 0 ψi eliminated Ψ If x 1 MQx y 0 This eliminate concept Ψ Now consider equivalence query EQθ θ conjunction Horn clauses clause C θ bodyC revised version body clause ϕn If θ contains clause C n 2 xis return positive counterexample 1 position bodyC 0s This eliminate concept Ψ If θ clause n 2 xs contains clause Ci bodyCi xs xi y return positive counterexample 1 position bodyC 0s This eliminates concept ψi Ψ The ﬁnal possibility clause θ exactly n 1 xs corresponding y This case includes case θ ϕn In case return negative counterexample 1n0n This eliminate concept Ψ cid1 6 Revising readonce formulas In section present revision algorithm class readonce formulas lower bounds showing algorithm close optimal In ﬁrst subsection preliminaries revision algorithm This followed description algorithm analysis detailed example The ﬁnal subsection gives lower bounds 61 Sensitization subformulas Our revision algorithm uses technique path sensitization fault analysis switching theory Kohavi 39 Assume like revise monotone readonce formula ϕ ϕ1 ϕ2 ϕ3 let target formula ψ ψ1 ψ2 ψ3 ψ obtained ϕ replacing certain variables constants Consider partial truth assignment α ﬁxes variables ϕ2 0 variables ϕ3 1 This ﬁxing variables called sensitizing ϕ1 α called sensitizing partial truth assignment ϕ1 Form vectors x0 x1 ﬁxing remaining variables 0 respectively 1 ask membership queries MQx0 MQx1 There possibilities J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 161 1 If MQx1 0 case ψ11 0 case ψ1 identically 0 ψ31 0 case target formula identically 0 2 If MQx0 1 case ψ10 1 case ψ1 identically 1 ψ20 1 case ψ2 identically 1 3 For revision algorithm important notice gain information case MQx0 0 MQx1 1 In case observe abnormality conclude truth assignment y variables ψ1 holds ψ1y MQy α Thus simulate membership queries subformula ψ1 membership queries target concept enables revision algorithm proceed recursion Also note case possible ψ21 0 andor ψ30 1 Now general deﬁnition sensitizing partial truth assignment Let ϕcid14 subformula ϕ Consider binary tree representing ϕ let P path leading root ϕ root ϕcid14 Then ϕ written ϕ ϕcid14 r ϕr r1 3 ϕ3 2 ϕ2 1 ϕ1 1 ϕ1 ϕr subformulas corresponding siblings nodes P 1 r In representation commutativity general ϕcid14 need leftmost subformula ϕ Let ψ obtained ϕ replacing certain variables constants Then 1 write ψ cid14 r ψr r1 3 ψ3 2 ψ2 1 ψ1 ψ ψ 2 Deﬁnition 16 Let ϕ readonce formula subformula ϕcid14 Write ϕ Eq 1 Let sets variables occurring ϕi Xi set variables occurring ϕcid14 Y Since ϕ readonce sets form partition x1 xn Now let α partial truth assignment assigns 1 respectively 0 variable Xi respectively 1 r Then α called partial truth assignment sensitizing ϕcid14 Generalizing remarks let α partial truth assignment sensitizing ϕcid14 Form truth assignments x0 0 α respectively x1 1 α extend α assigning 0 respectively 1 variables occurring ϕcid14 Now MQx1 0 follows monotonicity ψ ψ cid14 subformula ψi constant 0 In case subformula corresponding ψ cid14 r ψr r1 i1 ψi1 ψi target constant 0 subformula deleted replaced 0 The case similar MQx0 1 On hand MQx1 1 MQx0 0 sure partial truth assignment y variables ψ cid14 ψ cid14y MQy α This means ψ cid14 constant subformula These remarks summarized following lemma times later mentioning explicitly Lemma 17 Let ϕ initial formula ϕcid14 subformula ϕ let ψ ψ cid14 target formula respectively subformula corresponding ϕcid14 let α partial 162 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 truth assignment sensitizing ϕcid14 Then ψ cid14 constant subformula MQ0 α 1 MQ1 α 0 Otherwise ψ cid14y MQy α truth assignment y variables ϕcid14 b If ψ cid14 maximal constant subformula respectively ϕi1 1 respectively ϕi0 0 1 r In rest subsection formulate useful properties subformulas Two subformulas siblings corresponding nodes tree representation siblings The lemma follows directly deﬁnitions Lemma 18 Two maximal constant subformulas siblings The revision algorithm proceeds ﬁnding maximal constant subformulas important know identifying sufﬁcient learning Lemma 19 Substitutions σ1 σ2 equivalent formula ϕ maximal constant subformulas ϕσ1 ϕσ2 identical Proof If maximal constant subformulas identical replacing corresponding constants obtains formula Thus direction lemma holds For direction assume σ1 σ2 equivalent ϕ maximal constant subformulas identical There cases The ﬁrst case subformula ϕcid14 ϕ turns maximal constant subformula ϕσ1 ϕσ2 ϕcid14σ1 0 ϕcid14σ2 1 Let α partial truth assignment sensitizing ϕcid14 Then ϕσ11 α 0 ϕσ21 α 1 contradicting assumption σ1 σ2 equivalent In second case subformula maximal constant substitution Let ϕcid14 largest subformula We assume wlog ϕcid14σ1 maximal constant subformula computes constant 0 ϕcid14σ2 constant subformula Then ϕσ11 α 0 ϕσ21 α 1 contradicting assumption σ1 σ2 equivalent cid1 formula ϕ maximal constant Corollary 20 By ﬁnding revision subformulas identical target formula formula equivalent target formula The following lemma proved simple algorithm uses recursion structure formula ϕ Lemma 21 Given readonce formula ϕ constant c ﬁnd substitution σ ϕσ c σ ﬁxes minimal number variables polynomial time Let ϕ readonce formula subformula ϕcid14 We ϕcid14 approximately halfsize subformula ϕ contains onethird twothirds variables ϕ It standard fact subformula exists Wegener 56 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 163 1 x EQϕ cid5 correct σ FINDCONSTANTϕ x 2 ϕ ϕσ 3 4 end Algorithm 4 Algorithm REVISEREADONCEϕ For example minimal subformula contains onethird variables property If ϕ readonce formula subformula ϕcid14 ϕcid14partition truth assignment x x1 x2 x1 contains values x variables ϕcid14 x2 contains values x variables ϕ ϕcid14 62 The revision algorithm Now formulate main result section Algorithm REVISEREADONCE Algorithm 4 revises readonce formulas deletionsonly model revisions Theorem 22 Algorithm REVISEREADONCE uses Oe log n queries revise readonce formula containing n variables needing e revisions Proof Algorithm REVISEREADONCE consists loop checks target calls FINDCONSTANT In FINDCONSTANT REVISEREADONCE identify maximal constant subformula target formula ψ ﬁnd substitution ﬁxes subformula appropriate constant value The maximal constant subformula eliminated updated formula contains fewer variables As membership queries refer truth assignments original set variables new membership queries assign values eliminated variables The construction implies variables irrelevant values arbitrary FINDCONSTANT displayed Algorithm 5 recursive procedure takes formula ϕ counterexample x returns substitution σ The substitution ﬁxes subformula constant c It holds subformula maximal constant subformula computing constant c representation target concept5 FINDCONSTANT works recursively focusing faulty subformula subformula contains variables replaced constant previous levels formula This subformula proper subformula constant subformula constant subformula maximal constant subformula We assume property holds beginning recursion level maintain deeper recursion This guarantees eventually ﬁnd 5 In places proof property holds representation target concept Notice true information algorithm comes membership equivalence queries target responses queries independent particular representation 164 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 return substitution σ ﬁxing ϕ appropriate constant return substitution σ ﬁxing constant ϕx 1 ϕ variable 2 3 end 4 MQ0 1 MQ1 0 5 6 end 7 ϕcid14 approximately halfsize subformula ϕ 8 α partial truth assignment sensitizing ϕcid14 9 MQ0 α MQ1 α c return GROWFORMULAϕ ϕcid14 c 10 11 12 partition x x2 x21 x2r corresponding x1 x2 ϕcid14 subformulas MQx1 α cid5 ϕcid14x1 FINDCONSTANTϕcid14 x1 FINDFORMULAϕ ϕcid14 x FINDCONSTANTϕi x2i 13 14 15 16 17 18 19 end end look ϕcid14 Algorithm 5 The procedure FINDCONSTANTϕ x maximal constant subformula Once subformula use Lemma 21 return appropriate substitution As deeper recursion need ability ask membership queries concerning subformula target Therefore lower recursion level subformula ϕcid14 ϕ determine α partial truth assignment sensitizing ϕcid14 This way need membership query arises lower level truth assignment y need ask MQy α Recursion occurs MQ0 α 0 MQ1 α 1 sure MQy α equal value ψ cid14y ψ cid14 subformula target formula corresponding ϕcid14 From talking membership queries assume technique We write MQy instead MQy α α partial truth assignment sensitizing current subformula Now detailed description FINDCONSTANT explaining level recursion ﬁnds appropriate faulty subformula maintains counterexample x carried level counterexample The correctness algorithm follows discussion directly The complexity analysis requires point considered This Lemma 23 end proof Lines 13 We check current subformula ϕ consists single variable If ϕ vi thensince know ϕ proper constant subformula ϕ faultywe sure ϕ maximal constant subformula substitution vi c c ϕx appropriate maximal constant subformula From assume input formula variable J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 165 Lines 46 FINDCONSTANT examines MQ0 0 MQ1 1 If subformula identically true false Since ϕ property properly contained constant subformula ϕ maximal constant subformula Lines 78 We know ϕ constant subformula We determine approximately halfsize subformula ϕcid14 ϕ sensitizing partial truth assignment α Lines 910 We check MQ0 α MQ1 α c If case MQy α c partial truth assignment y variables ϕcid14 Thus ψ cid14 constant subformula maximal constant subformula properly contained ψ At point perform recursive calls The task left ﬁnd node path P root ψ root ψ cid14 root maximal constant subformula Procedure GROWFORMULA As GROWFORMULA implements simple binary search brief description displaying pseudocode The procedure gets input readonce formula ϕ subformula ϕcid14 constant c MQ0 α MQ1 α c Using Olog n membership queries outputs maximal subformula containing ϕcid14 corresponding subformula identical constant c representation target We assume c 1 case c 0 dual Using notation Deﬁnition 16 let αi 0 r partial truth assignment identical α X1 Xi leaves variables Y unassigned assigns 0 variables Then 0 0 0 α0 cid1 0 α1 cid1 0 α2 cid1 cid1 0 αr 0 α holds MQ0 α0 0 MQ0 αr 1 Asking membership queries MQ0 αj use binary search ﬁnd 1 cid1 cid1 r MQ0 αi1 0 MQ0 αi 1 The difference truth assignments 0 αi1 0 αi variables Xi 0 αi1 0 αi In fact 0 αi1 0 αi It follows Thus hand case ψi 0 0 ψi 1 1 representation target concept On hand case input child path P equal 1 cases As variables subformula set 0 subformula compute constant 1 function The inputs 0 αi1 0 αi demonstrate larger subformula computes constant function Thus subformula rooted i1 maximal constant subformula This completes discussion procedure GROWFORMULA Lines 1112 If line 11 know ψ cid14 constant subfor mula continue recursion ﬁnd ψ cid14 Using counterexample x form ϕcid14partition x line 12 In remainder procedure ﬁnd faulty subformula twothirds variables ϕ Lines 1314 Since α partial truth assignment sensitizing ϕcid14 ϕx1 α ϕcid14x1 Furthermore MQx1 α ψ cid14x1 ψ cid14 constant subformula If MQx1 α cid5 ϕx1 α ϕcid14x1 cid5 ψ cid14x1 ϕcid14 contains maximal constant subformula Thus carry ﬁnding faulty parts contribute faulty evaluation x recursive FINDCONSTANTϕcid14 x1 Lines 1517 The way point MQx1 α ψ cid14x1 ϕcid14x1 d faults subformula ϕi ϕ 1 2 r Let x2 x21 x22 x2r x2i x2 containing variables Xi 166 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Let yi respectively zi value computed ϕ respectively ψ input vector x 1 r Furthermore let yr1 d zr1 Then yi yi1 ϕix2i zi zi1 ψix2i 1 r Since x counterexample EQϕ holds y1 ϕx cid5 ψx z1 Since yr1 zr1 y1 cid5 z1 1 cid1 cid1 r yi1 zi1 yi cid5 zi Now let assume know special paragraph describes procedure FINDFORMULA ﬁnding Then follows ϕi faulty x2i counterexample equivalence ϕi ψi This means carry search faulty subformula ϕi This recursive FINDCONSTANT x2i counterexample As recursion simulate assignment y variables ψi MQy α α partial truth assignment sensitizing ϕi ϕ ϕcid14 constant subformula ϕi answer query value ψi y The search appropriate index procedure FINDFORMULA weighted binary search follows The yi values computed ϕ queries For computation zi let βi partial truth assignment assigns x2j variables Xj j r identical α Then zi MQx1 βi ϕcid14 contained constant subformula constant subformulas path 1 r j I wj In step ﬁnd index cid6 Let nj denote number variables ϕj deﬁne weight subformula wj nj 1 nj j 2 r In binary search use interval I b Initially 2 b r know y1 z1 yr1 zr1 For given I let s cid2 cid6 j wj dont need ask queries We determine ycid6 zcid6 query If ycid6 cid5 zcid6 let I cid6 1 b let I cid6 1 If I nonempty compute s continue search Otherwise search ycid6 cid5 zcid6 cid6 index looking cid6 1 This completes description analysis revision algorithm cid61 j wj s2 cid1 cid2 cid2 Since iteration ﬁnd maximal constant subformula ﬁnd minimal substitution ﬁx value computed subformula appropriate constant follows FINDCONSTANT called e times The claimed complexity bound follows following lemma Lemma 23 When called LEARNREADONCE Procedure FINDCONSTANT uses total Olog n membership queries Proof The general idea proof queries consumed FINDFOR MULA smaller recursive FINDCONSTANT Let examine procedure FINDFORMULA works Let u number variables subformula ϕ level recursion Since ϕcid14 approximately halfsize r j 2 2 uj subformula ϕ w1 wr 2 u 23 u 43 The value s reduce half iteration search k queries s 12k times initial value cid2 r j 1 uj cid1 u 23 initially s j I wj cid2 cid2 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 167 Thus u 43 12k We know index returned query weight ϕi weight ϕi1 appears s But contain ui query s cid2 ui In summary index t iterations t queries number variables ϕi ui cid1 u 43 12t 1 u3 2t 3 Thus t cid1 loguui 3 log 3 loguui 2 queries managed restrict location faulty subformula ϕi containing ui variables Thus recursion level use fewer K 2 loguui queries K number queries needed lines 4 9 13 The way enter recursion level line 14 need additional queries K Furthermore recursion need Olog u queries lines 110 Note level recursion size subformula twothirds size previous level Thus denoting size formula ith level recursion mi q log23 m0 levels recursion level excluding ﬁnal use K 2 logmimi1 queries Adding run FINDCONSTANT use cid4 cid3 cid4 cid3 K 2 log Olog mq Olog m0 mq1 mq K 2 log queries cid1 m0 m1 The proof Lemma 23 concludes proof theorem cid1 63 Example run revision algorithm readonce formulas Here detailed example showing readonce revision algorithm works Let formula revised ϕ y1 y2 y3 y4 y5 y6 y7 y8 y9 substitution giving target formula σ y3 1 y5 y6 y8 0 Thus target concept represented formula ψ y1 y2 1 y4 0 0 y7 0 y9 3 We start asking equivalence query EQϕ Let assume receive negative counterexample x 110011110 In Procedure FINDCONSTANT membership queries MQ0 0 MQ1 1 bring line 7 At point ﬁnd approximately halfsize subformula example ϕcid14 y1 y2 y3 y4 The corresponding subformula target ψ cid14 y1 y2 1 y4 Now form sensitizing truth assignment α ϕcid14 case simply sets variables ϕcid14 1 ask membership queries 0 α 1 α The answer MQ0 α 0 MQ1 α 1 continue line 12 We 168 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 x1 1100 x2 11110 By asking membership query MQx1 α ﬁnd ψ cid14x1 1 Knowing ϕ determine asking queries ϕcid14x1 1 As ψ cid14x1 ϕcid14x1 follows x2 counterexample responsible disagreement ϕx ψx In particular case variables x2 happen induce subformula ϕ FINDFORMULA need We substitute 1 ϕcid14 Then x2 11110 negative counterexample new target subformula ψ cid14cid14 target corresponding ϕcid14cid14 y5 y6 y7 y8 y9 It important note ψ cid14cid14y ψx1 y simulate membership queries new target membership queries original target continue procedure recursively As subsequent iterations illustrate additional cases steps algorithm example In FINDCONSTANTϕcid14cid14 x2 line 7 The half size subformula y5 y6 The sensitizing truth assignment subformula 010 Now membership queries 00 010 11 010 return 0 indicating y5 y6 subformula containing turned constant 0 Thus GROWFORMULA asks additional membership queries MQ11 110 0 MQ11 111 1 This shows y5 y6 y7 y8 maximal constant 0 subformula ϕcid14cid14 No recursive calls needed need compute minimal number variables turned 0 subformula identically 0 This achieved single substitution y8 0 Now completed procedure FINDCONSTANT main program The FINDCONSTANT start equivalence query formula obtained substitution cid14cid14cid14 y1 y2 y3 y4 y9 ϕ Let assume receive positive counterexample 000111111 restricted ﬁve variables ϕcid14cid14cid14 00011 We continue half size subformula y1 y2 divides counterexample 00 011 The sensitizing partial truth assignment ﬁrst half 001 We ﬁnd MQ00 001 0 MQ11 001 1 y1 y2 turned constant subformula Notice membership oracle needs inputs 0 19 fortunately values missing variables The membership query MQ00 001 0 tells ﬁrst half counterexample gives output y1 y2 corresponding subformula target To recurse ﬁnd subformula ϕcid14cid14cid14 contains constant subformula variables y3 y4 y9 induce subformula ϕcid14cid14cid14 This achieved procedure FINDFORMULA In case need consider subformulas y3 y4 y9 general cid1n subformulas necessitating binary search performed FINDFORMULA By deﬁnition ϕcid14cid14cid14 disagrees target counterexample concluded y1 y2 agrees counterexample So subformula y1 y2 y3 y4 ϕcid14cid14cid14 disagrees corresponding subformula target J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 169 subformula containing constant subformula y3 y4 Otherwise y9 To test subformula y1 y2 y3 y4 agrees target counterexample ask membership query instance formed setting y1 y2 y3 y4 values counterexample setting remaining variable y9 value sensitizing assignment y1 y2 That query MQ00011 1 Since ϕcid14cid14cid1400011 0 disagrees target constant subformula y3 y4 input subformula FINDCONSTANT That identify substitution y3 1 equivalence query formula y1 y2 y4 y9 ﬁnally identify target concept Notice actually revised fewer variables given Eq 3 The number variables revised small possible obtaining target concept 64 Lower bounds revising readonce formulas We prove lower bound query complexity revising readonce formulas giving example nvariable readonce formula cid1e logne equivalence membership queries required ﬁnd distance e revision If e On1ε ﬁxed ε 0 lower bound order magnitude upper bound provided REVISEREADONCE It shown types queries needed efﬁcient revision There nvariable readonce formulas n2 equivalence queries required order ﬁnd single revision For membership queries present stronger lower bound shows n e membership queries necessary instead equivalence queries allowed use fewer e equivalence queries As REVISEREADONCE uses exactly e equivalence queries ﬁnd distance e revision means allowing fewer equivalence query number membership queries required linear Bshouty Cleve Bshouty et al 1617 somewhat related constructions tradeoff results different query types Our ﬁrst lower bounds based readonce formulas form xi yi VCdimension respectively adversary argument lower bound uses adversary argument nvariable disjunction cid5 Theorem 24 The complexity revising readonce formulas deletiononly model cid1e log n e n number variables initial formula e revision distance initial formula target formula Proof Let assume n 2me m 2t 170 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 We use variables xij yij 1 cid1 cid1 e 0 cid1 j cid1 m 1 The initial formula ϕn ecid6 m1cid6 i1 j 0 xij yij Assume x y variables arranged respective e m matrices called X Y respectively We look class revisions ϕn row matrix X exactly variable ﬁxed 1 Let corresponding concept class Cn Lemma 25 VCdimCn cid2 e t Proof For 1 cid1 k cid1 e 1 cid1 cid6 cid1 t let Xkcid6 Ykcid6 truth assignment consists 0s exception positions kth row Y matrix positions k j cid6th bit binary representation j 1 Let set assignments S We claim S shattered Cn Consider subset A S For k 1 cid1 k cid1 e let ak tbit number describing truth assignments Xkcid6 Ykcid6 belong A That cid6th bit ak 1 iff Xkcid6 Ykcid6 A We look revision ϕA akth variable ﬁxed 1 row k matrix X It remains revision classiﬁes S required manner If Xkcid6 Ykcid6 A bit cid6 ak 1 By deﬁnition Ykcid6 1 position k ak In ϕA variable xkak ﬁxed 1 These observations imply ϕAXkcid6 Ykcid6 1 On hand Xkcid6 Ykcid6 A bit cid6 ak 0 The 1 components Xkcid6 Ykcid6 row k Y matrix positions k j cid6th bit binary representation j 1 Position k ak Thus corresponding xvariables ﬁxed 1 ϕA value 0 ϕAXkcid6 Ykcid6 0 cid1 By introducing dummy variables n right form VCdimCn cid2 e log cid7 cid8 n 2e The theorem follows general result VCdimCn provides lower bound number equivalence membership queries required learn Cn constant factor equivalence queries required proper 1142 cid1 cid9 cid10 n e The number formulas revision distance e given readonce formula 2e Thus allow equivalence queries necessarily proper standard halving algorithm 41 learn revision log2e Oe log n equivalence queries We result possible queries required proper cid10 n e cid9 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 171 Theorem 26 The complexity revising readonce formulas deletiononly model proper equivalence queries cid16n2cid17 n number variables initial formula assume single revision occurs Proof We use initial formula cid16n2cid17cid6 x2i1 x2i ϕn i1 4 Variables conjunction called partners The revisions considered ﬁx exactly variable 1 Let formula obtained ϕn ﬁxing xj 1 ϕj n let class Ccid14 n We adversary strategy forces learner use cid16n2cid17 equivalence queries n consist formulas ϕj It assumed hypotheses consistent previous counterexam ples previous counterexamples returned Let assume learner asks equivalence query EQψ If variable partner ﬁxed 1 ψ ψ 1 return 0 negative counterexample This rule concept Ccid14 n Otherwise variable xj ﬁxed 0 ψ return positive counterexample 0s xj partner value 1 Again rule concept Ccid14 n Otherwise variable xj ﬁxed 1 ψ partner return negative counterexample 0s partner xj value 1 This rules formula ϕj n Finally remains case ψ initial formula ψ ﬁrst query In case adversary looks set formulas ϕj n ruled If formulas j respectively odd returns positive counterexample 101010 respectively 010101 This rules formulas ϕj n j odd respectively rule j respectively odd n queries eliminate concept As long concept ruled learning process terminate lower bound follows cid1 The query eliminates cid16n2cid17 concepts Ccid14 Now present lower bound case membership queries allowed Actually consider general scenario learner allowed ask limited number equivalence queries In particular assume learner told advance target revision distance e initial theory number equivalence queries allowed e 1 Theorem 27 The number membership queries required revising readonce formulas deletiononly model n e n number variables initial formula e revision distance initial formula target formula assuming number equivalence queries fewer e 172 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Proof We start initial formula x1 xn consider class Ccid14cid14 n revisions ﬁx exactly e variables 0 The adversary maintains partition D U Q variables D stands deleted U stands undeleted Q stands In beginning D U Q x1 xn In course learning process holds concept Ccid14cid14 n variable D deleted variable U deleted consistent previous answers This implies learner identify target long D e D Q e For membership query MQx consider cases If xi 1 U MQx 1 sets changed Otherwise xi 1 Q MQx 1 variable xi moved Q U Otherwise MQx 0 sets changed For equivalence query EQψ consider following cases If ψ identically 1 respectively 0 0 respectively 1 vector given negative respectively positive counterexample sets changed If variable xi Q ψ vector 0s xi given negative counterexample xi moved Q D Otherwise characteristic vector Q returned positive counterexample sets changed Initially D 0 D increased equivalence query As fewer e equivalence queries D e Thus learning process terminate achieving D Q e But initially D Q n size decreased membership query Therefore n e membership queries needed cid1 7 Revising parity general revision model So far considered errors corresponding deletion literals terms In practical theory revision algorithms deal types errors replacement variable addition variable term Some error types hard deﬁne general careful deﬁnition particular cases 1040 Replacements additions appear harder handle deletions Let variables x1 xn given A parity function exclusiveor subset variables complement function Thus parity function speciﬁed giving cid20u 0 1n 0 1 writing parity function ϕ ϕx cid20u x denotes mod 2 inner product vectors Thus cid20u x mod 2 cid9cid2 cid10 n i1 uixi Given parity function allow deletion variable replacement variable constant variable addition variable parity addition constant 1 Given parity function ϕ denote Rϕ class parity functions obtained ϕ enlarged set revision operators denote Cϕ corresponding concept classes Thus Cϕ class parity functions variables x1 xn In cases unlike rest paper concept classes depend initial formula The role played revision J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 173 operator determine performance metric target concept obtained revisions identify queries Theorem 28 There revision algorithm parity functions general model revisions Oe log n queries e revision distance initial target concept Proof Let ϕx cid20uϕ x parity function revised let ψx cid20uψ x b target concept Since ψ0 b value b determined single equivalence query MQ0 If cid5 b change ϕ complement achieve b b 1 reverse labels Thus assumed b 0 The vectors cid20uϕ cid20uψ differ 2d bits The revision algorithm starts equivalence query ϕ Let x counterexample received query As b 0 holds x cid5 0 Our goal ﬁnd counterexample containing exactly 1 Let x1 x2 obtained x switching respectively ﬁrst second half 1 components x Notice x x1 x2 ϕx1 ϕx2 ϕx1 x2 ϕx cid5 ψx ψx1 x2 ψx1 ψx2 exactly ϕx1 cid5 ψx1 ϕx2 cid5 ψx2 hold Thus exactly x1 x2 counterexample membership query tell counterexample Continuing process counterexample single 1 component Olog n membership queries The variable corresponding 1 component variables ϕ ψ differ Hence ψ repeating procedure Oe times cid1 8 Concluding remarks Theory revision important know theory close desired theory learning scratch wasteful needlessly expensive This area received relatively little theoretical study We presented efﬁcient algorithms Horn readonce formulas deletionsonly revision model We given tight bounds revisions readonce formulas In addition given algorithm tight bounds general revision parity formulas These results prove formal model efﬁcient theory revision algorithms Additional results revising forms DNF formulas companion paper 30 Work revising Valiants class projective DNF functions 53 Sloan et al 49 The work presented means exhausts area theory revision learning theory point view There numerous open problems instance revision threshold formulas revision Horn formulas general model revision 174 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 Acknowledgements Preliminary versions results paper appeared conference papers 282950 masters thesis author 51 We thank Jignesh Doshi careful readings preliminary versions manuscript Robert Sloan wishes thank NSF providing time research working References 1 H Aizenstein T Hegedus L Hellerstein L Pitt Complexity theoretic results query learning Computational Complexity 7 1998 1953 2 H Aizenstein L Hellerstein L Pitt Readthrice DNF hard learn membership equivalence queries Proc 33rd Symposium Foundations Computer Science Pittsburgh PA IEEE Computer Society Press Los Alamitos CA 1992 pp 523532 3 D Angluin Learning propositional Horn sentences hints Technical Report YALEUDCSRR590 Department Computer Science Yale University New Haven CT 1987 4 D Angluin Learning regular sets queries counterexamples Inform Comput 75 2 1987 87 106 5 D Angluin Queries concept learning Machine Learning 2 4 1988 319342 6 D Angluin Negative results equivalence queries Machine Learning 5 1990 121150 7 D Angluin M Frazier L Pitt Learning conjunctions Horn clauses Machine Learning 9 1992 147164 8 D Angluin L Hellerstein M Karpinski Learning readonce formulas queries J ACM 40 1 1993 185210 9 D Angluin M Kharitonov When wont membership queries help J Comput System Sci 50 2 1995 336355 Earlier version appeared 23rd STOC 1991 10 S ArgamonEngelson M Koppel Tractability theory patching J Artiﬁcial Intelligence Res 8 1998 3965 11 P Auer PM Long Structural results online learning models queries Machine Learning 36 3 1999 147181 12 A Blum L Hellerstein N Littlestone Learning presence ﬁnitely inﬁnitely irrelevant attributes J Comput Syst Sci 50 1 1995 3240 Earlier version Proc Fourth Annual Workshop Computational Learning Theory COLT 1991 Santa Cruz CA 1991 pp 157166 13 A Blum S Rudich Fast learning kterm DNF formulas queries J Comput System Sci 51 3 1995 367373 14 N Bshouty Exact learning Boolean function monotone theory Inform Comput 123 1995 146 153 15 N Bshouty L Hellerstein Attributeefﬁcient learning query mistakebound models J Comput System Sci 56 3 1998 310319 16 NH Bshouty R Cleve On exact learning formulas parallel Proc 33rd Symposium Foundations Computer Science Pittsburgh PA IEEE Computer Society Press Los Alamitos CA 1992 pp 513522 17 NH Bshouty SA Goldman TR Hancock S Matar Asking questions minimize errors J Comput System Sci 52 2 1996 268286 Earlier version Proc Sixth Annual ACM Conference Computational Learning Theory COLT 1993 Santa Cruz CA 1993 pp 4150 18 L Carbonara D Sleeman Effective efﬁcient knowledge base reﬁnement Machine Learning 37 2 1999 143181 19 P Damaschke Adaptive versus nonadaptive attributeefﬁcient learning Machine Learning 41 2 2000 197215 20 L De Raedt Logical settings conceptlearning Artiﬁcial Intelligence 95 1997 187201 21 R Dechter J Pearl Structure identiﬁcation relational data Artiﬁcial Intelligence 58 1992 237270 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 175 22 A Dhagat L Hellerstein PAC learning irrelevant attributes Proc 35rd Annual Symposium Foundations Computer Science Santa Fe NM IEEE Computer Society Press Los Alamitos CA 1994 pp 6474 23 WF Dowling JH Gallier Lineartime algorithms testing satisﬁability propositional Horn formulae J Logic Programming 1 1984 267284 24 R Feldman Probabilistic revision logical domain theories PhD Thesis Cornell University Ithaca NY 1993 25 M Frazier Matters Horn features computational learning theory landscape The notion membership PhD Thesis Dept Computer Science University Illinois UrbanaChampaign Technical Report UIUCDCSR941858 1994 26 M Frazier L Pitt Learning entailment An application propositional Horn sentences Proc Tenth International Conf Machine Learning Amherst MA Morgan Kaufmann San Mateo CA 1993 pp 120127 27 O Goldreich S Goldwasser S Micali How construct random functions J ACM 33 4 1986 792807 28 J Goldsmith RH Sloan More theory revision queries extended abstract Proceedings ThirtySecond Annual ACM Symposium Theory Computing Portland OR May 2123 2000 ACM Press New York 2000 pp 441448 29 J Goldsmith RH Sloan B Szörényi G Turán Improved algorithms theory revision queries Proc 13th Annual Conference Computational Learning Theory Palo Alto CA Morgan Kaufmann San Francisco CA 2000 pp 236247 30 J Goldsmith RH Sloan G Turán Theory revision queries DNF formulas Machine Learning 47 23 2002 257295 31 R Greiner The complexity revising logic programs J Logic Programming 40 1999 273298 32 R Greiner The complexity theory revision Artiﬁcial Intelligence 107 1999 175217 33 V Gurvich On repetitionfree Boolean functions Uspekhi Mat Nauk 32 1 1977 183184 Russian 34 A Horn On sentences true direct unions algebras J Symbolic Logic 16 1951 1421 35 M Karchmer N Linial I Newman M Saks A Wigderson A combinatorial characterization readonce formulae Discrete Math 114 1993 275282 36 M Kearns M Li L Pitt L Valiant On learnability Boolean formulae Proc 19th Annual ACM Symp Theory Computing STOC 1987 New York ACM Press New York 1987 pp 285294 37 M Kearns L Valiant Cryptographic limitations learning Boolean formulae ﬁnite automata J ACM 41 1 1994 6795 38 H Kleine Buning T Lettmann Propositional Logic Deduction Algorithms Cambridge University Press Cambridge 1999 39 Z Kohavi Switching Finite Automata Theory second edition McGrawHill New York 1978 40 M Koppel R Feldman AM Segre Biasdriven revision logical domain theories J Artiﬁcial Intelligence Res 1 1994 159208 41 N Littlestone Learning quickly irrelevant attributes abound new linearthreshold algorithm Machine Learning 2 4 1988 285318 42 W Maass G Turán Lower bound methods separation results online learning models Machine Learning 9 1992 107145 43 JA Makowsky Model theory science An appetizer S Abramsky DM Gabbay TSE Maibaum Eds Handbook Logic Computer Science vol 1 Background Mathematical Structures Oxford University Press Oxford 1992 pp 763814 44 JCC McKinsey The decision problem classes quantiﬁers J Symbolic Logic 8 1943 6176 45 RJ Mooney A preliminary PAC analysis theory revision T Petsche Ed Computational Learning Theory Natural Learning Systems vol III Selecting Good Models MIT Press Cambridge MA 1995 pp 4353 Chapter 3 46 D Mundici Functions computed monotone Boolean formulas repeated variables Theoret Comput Sci 66 1989 113114 47 D Ourston RJ Mooney Theory reﬁnement combining analytical empirical methods Artiﬁcial Intelligence 66 1994 273309 176 J Goldsmith et al Artiﬁcial Intelligence 156 2004 139176 48 K Pillaipakkamnatt V Raghavan Readtwice DNF formulas properly learnable Computational Learning Theory EuroColt 93 The Institute Mathematics Applications Conference Series vol 53 Oxford University Press Oxford 1994 pp 121132 49 RH Sloan B Szörényi G Turán Projective DNF formulae revision Learning Theory Kernel Machines 16th Annual Conference Learning Theory 7th Kernel Workshop COLTKernel 2003 Washington DC August 2427 2003 Proceedings Lecture Notes Artiﬁcial Intelligence vol 2777 Springer Berlin 2003 pp 625639 50 RH Sloan G Turán On theory revision queries Proc 12th Annual Conf Computational Learning Theory Santa Cruz CA ACM Press New York 1999 pp 4152 51 B Szörényi Revision algorithms computational learning theory Masters Thesis Dept Computer Science University Szeged 2000 Hungarian 52 GG Towell JW Shavlik Knowledgebased artiﬁcial neural networks Artiﬁcial Intelligence 70 12 1994 119165 53 LG Valiant Projection learning Machine Learning 37 2 1999 115130 54 MH Van Emden RA Kowalski The semantics predicate logic programming language J ACM 23 1976 733742 55 VN Vapnik AY Chervonenkis On uniform convergence relative frequencies events probabilities Theor Probab Appl 16 2 1971 264280 56 I Wegener The Complexity Boolean Functions WileyTeubner 1987 57 PH Winston TO Binford B Katz M Lowry Learning physical descriptions functional deﬁnitions examples precedents Proc Natl Conf Artiﬁcial Intelligence AAAI83 Washington DC 1983 pp 433439 58 S Wrobel Concept Formation Knowledge Revision Kluwer Dordrecht 1994 59 S Wrobel First order theory reﬁnement L De Raedt Ed Advances ILP IOS Press Amsterdam 1995 pp 1433