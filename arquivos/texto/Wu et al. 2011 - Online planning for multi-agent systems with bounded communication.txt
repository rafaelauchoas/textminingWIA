Artiﬁcial Intelligence 175 2011 487511 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Online planning multiagent systems bounded communication Feng Wu ab Shlomo Zilberstein b Xiaoping Chen School Computer Science University Science Technology China Jinzhai Road 96 Hefei Anhui 230026 China b Department Computer Science University Massachusetts Amherst 140 Governors Drive Amherst MA 01003 USA r t c l e n f o b s t r c t Article history Received 2 February 2010 Received revised form 22 September 2010 Accepted 26 September 2010 Available online 29 September 2010 Keywords Decentralized POMDPs Cooperation collaboration Planning uncertainty Communication multiagent systems We propose online algorithm planning uncertainty multiagent settings modeled DECPOMDPs The algorithm helps overcome high computational complexity solving problems oﬄine The key challenges decentralized operation maintain coordinated behavior little communication communication allowed optimize value minimal communication The algorithm addresses challenges generating identical conditional plans based common knowledge communicating history inconsistency detected allowing communication postponed necessary To suitable online operation algorithm computes good local policies new fast local search method implemented linear programming Moreover bounds memory step applied problems arbitrary horizons The experimental results conﬁrm algorithm solve problems large best existing oﬄine planning algorithms outperforms best online method producing higher value communication cases The algorithm proves effective communication channel imperfect periodically unavailable These results contribute scalability decisiontheoretic planning multiagent settings 2010 Elsevier BV All rights reserved 1 Introduction A multiagent MAS consists multiple independent agents interact domain Each agent decision maker situated environment acts autonomously based observations domain knowledge accomplish certain goal A multiagent design beneﬁcial AI domains particularly composed multiple entities distributed functionally spatially Examples include multiple mobile robots space exploration rovers sensor networks weather tracking radars Collaboration enables different agents work eﬃciently complete activities able accomplish individually Even domains agents centrally controlled MAS improve performance robustness scalability selecting actions parallel In principle agents MAS different conﬂicting goals We interested fullycooperative MAS agents share common goal In cooperative setting agent selects actions individually resulting joint action produces outcome Coordination key aspect systems The goal coordination ensure individual decisions agents result nearoptimal decisions group This extremely challenging especially Corresponding author Department Computer Science University Massachusetts Amherst 140 Governors Drive Amherst MA 01003 USA Tel 1 413 545 1985 fax 1 413 545 1249 Email addresses wufengmailustceducn F Wu shlomocsumassedu S Zilberstein xpchenustceducn X Chen 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201009008 488 F Wu et al Artiﬁcial Intelligence 175 2011 487511 agents operate highlevel uncertainty For example domain robot soccer robot operates autonomously team cooperate members team play successfully The sensors actuators systems introduce considerable uncertainty What makes problems particularly challenging agent gets different stream observations runtime different partial view situation And agents able communicate sharing information time possible Besides agents domains need perform long sequence actions order reach goal Different mathematical models exist specify sequential decisionmaking problems Among decisiontheoretic models planning uncertainty studied extensively artiﬁcial intelligence operations research 1950s Decisiontheoretic planning problems formalized Markov decision processes MDPs single agent repeatedly interacts stochastically changing environment tries optimize performance measure based rewards costs Partiallyobservable Markov decision processes POMDPs extend MDP model handle sensor certainty incorporating observations probabilistic model occurrence In MAS individual agent different partial information agents state world Over decade different formal models problem proposed We adopt decentralized partiallyobservable Markov deci sion processes DECPOMDPs model team cooperative agents interact stochastic partiallyobservable environment It proved decentralized control multiple agents signiﬁcantly harder single agent control provably intractable In particular complexity solving twoagent ﬁnitehorizon DECPOMDP NEXPcomplete 12 In years promising approximation techniques developed 31117194647 The vast majority algorithms work oﬄine compute prior execution best action execute possible situations While oﬄine algorithms achieve good performance long time double exponential policy space explore For example PBIPIPG stateoftheart MBDPbased oﬄine algorithm takes 385 hours solve small problem Meeting 33 grid involves 81 states 5 actions 9 observations 3 Online algorithms hand plan step time given currently available information The potential achieving good scalability promising online algorithms But extremely challenging agents coordinated long period time oﬄine planning Recent developments online algorithms suggest combining online techniques selective communication communication possible eﬃcient way tackle large DECPOMDP problems The main goal paper present analyze evaluate online methods bounded communication present attractive alternative oﬄine techniques solving large DECPOMDPs The main contributions paper include 1 fast method searching policies online 2 innovative way agents remain coordinated maintaining shared pool histories 3 eﬃcient way bounding number possible histories agents need consider 4 new communication strategy cope bounded unreliable communication channels In presence multiple agents agent cope limited knowledge environment agents reason possible beliefs agents affects decisions Therefore possible situations consider selecting action given current knowledge We present new linear program formulation search space policies quickly Another challenge number possible histories situations grows rapidly time steps agents run memory quickly We introduce new approach merging histories bound size pool histories preserving solution quality Finally known appropriate amounts communication improve tractability performance multiagent systems When communication bounded true real world applications diﬃcult decide utilize limited communication resource eﬃciently In work agents communicate history inconsistency detected This presents new effective way initiate communication dynamically runtime The rest paper organized follows In Section 2 provide background introducing formal model discussing oﬄine online algorithms communication methods framework decentralized POMDPs In Section 3 present multiagent online planning communication algorithms including general framework policy search history merging communication strategy implementation issues In Section 4 report experimental results common benchmark problems challenging problem named grid soccer We report results cooperative box pushing domain imperfect communication settings In Section 5 survey existing online approaches communications applied decentralized POMDPs discuss strengths drawbacks Finally summarize contributions discuss limitations open questions work 2 Background In section provide formal description problem essential background We consider settings group agents coordinate discrete time steps The agents operate ﬁnite number steps T referred horizon At time step agent ﬁrst receives local observation environment takes action The combination agents actions causes stochastic change state environment produces joint reward new decision cycle starts In cooperative sequential F Wu et al Artiﬁcial Intelligence 175 2011 487511 489 decisionmaking settings agents come plan maximizes expected long term reward team Planning place oﬄine online In oﬄine planning computed plans distributed agent executed agent based local information Hence planning phase centralized long execution decentralized Online planning algorithms interleave planning execution Thus need ﬁnd actions current step instead generating plan oﬄine algorithms Agents usually share information communication But communication free resource applications We discuss communication model later section 21 Formal model decentralized POMDPs The Markov Decision Process MDP partially observable counterpart POMDP proved useful planning learning uncertainty The Decentralized POMDP offers natural extension frameworks operative multiagent settings We adopt DECPOMDP framework model multiagent systems approach results apply equivalent models MTDP 41 POIPSG 40 Deﬁnition 1 DECPOMDP A decentralized partially observable Markov decision process tuple cid3I S Ai Ωi P O R b0cid4 I ﬁnite set agents indexed 1 n Notice n 1 DECPOMDP equivalent singleagent POMDP S ﬁnite set states A state summarizes relevant features dynamical satisﬁes Markov property That probability state depends current state joint action previous states joint actions P st1s0 cid5a0 st1 cid5at1 st cid5at P st1st cid5at Ai ﬁnite set actions available agent cid5A iI Ai set joint actions cid5a cid3a1 ancid4 denotes set joint observations cid5o Ωi cid3o1 oncid4 denotes joint observation Every time step environment emits joint observation agent observes component ﬁnite set observations available agent cid5Ω iI Ωi joint action We assume agents observe actions taken time step P state transition probability table P s cid7 cid7s cid5a denotes probability taking joint action cid5a state s results P describes stochastic inﬂuence actions environment We assume transition transition state s probabilities stationary means independent time step O table observation probabilities O cid5os joint action cid5a reaching state s probability assumed stationary cid7 cid5a denotes probability observing joint observation cid5o taking O describes agents perceive state environment The observation cid7 R S cid5A cid9 reward function Rs cid5a denotes reward obtained taking joint action cid5a state s R spec iﬁes agents goal task It immediate reward agents taking joint action state The agents generally observe immediate reward time step local observation determine information b0 cid3S initial belief state distribution It vector speciﬁes discrete probability distribution S captures agents common knowledge starting state Formally deﬁne history agent hi sequence actions taken observations received agent At time step t cid2 o1 a0 a1 ot1 cid3 at1 ot ht history agent ht cid3ht cid4 joint history The term joint belief bh cid3S denotes probability distribution states induced joint history h Given set joint histories previous step computing set joint belief states current step straightforward Bayes rule 1 ht n cid2 cid3 scid7S bt O cid5ots scid7cid7S O cid5otscid7cid7 cid5at1 Throughout paper use bh shorthand joint belief bh cid7s cid5at1bt1sht1 sS P scid7cid7s cid5at1bt1sht1 sS P s cid4 cid7 cid5at1 cid7ht cid4 s cid4 1 A local deterministic policy δi mapping local histories actions Ai δihi ai And agent joint deterministic policy δ cid3δ1 δncid4 δh tuple local deterministic policies agent cid3δ1h1 δnhncid4 cid5a A deterministic policy represented policy tree nodes representing actions edges labeled observations A joint deterministic policy set policy trees Similarly local stochastic policy agent πiaihi mapping local history hi distribution Ai A joint stochastic policy π cid3π1 πncid4 tuple local stochastic policies 490 F Wu et al Artiﬁcial Intelligence 175 2011 487511 Solving DECPOMDP given horizon T start state s0 seen ﬁnding policy δ maximizes expected cumulative reward cid3 cid2 V δ s0 E cid8 cid2 R st cid5a t cid3cid7 cid7s0 cid5 T 1cid6 t0 Because recursive nature DECPOMDPs intuitive specify value function recursively cid2 V δ ht cid3 cid6 cid3 cid2 p stht cid3 cid2 R st cid5a cid6 cid2 P st1st cid5a cid3 cid2 cid5ost1 cid5a cid3 V cid2 O δ ht1 cid3 cid9 cid10 st st1cid5o cid5a δht ht1 ht cid5a cid5o state distribution pstht given history ht computed recursively follow cid3 cid2 p stht cid2 cid5ost cid5a O cid3cid6 cid2 P stst1 cid5a cid3 cid3 cid2 cid5aht1 p p cid2 st1ht1 cid3 2 3 4 st1 ht ht1 cid5a cid5o psh0 b0s s S A survey DECPOMDP models algorithms available 48 Previous studies identiﬁed different categories DECPOMDPs characterized different levels observability interaction The computational complexity solving problems ranges NEXP P 26 When individual observation agent identiﬁes true state uniquely DECPOMDP reduces multiagent MDP MMDP 15 When joint observation identiﬁes true state DECPOMDP referred DECMDP A DECMDP called transitionindependent DECMDP TIDECMDP transition models agents independent 9 Other special cases considered instance goaloriented DECPOMDPs 26 eventdriven DECMDPs 8 network distributed POMDPs NDPOMDPs 33 DECMDPs time resource constraints 131429 DECMDPs local interactions 51 factored DECPOMDPs additive rewards 36 In work consider general ﬁnitehorizon DECPOMDPs simplifying assumptions obser vations transitions reward functions The complexity general ﬁnitehorizon DECPOMDPs shown NEXPcomplete Due complexity results optimal algorithms theoretical signiﬁcance Current research efforts area focus ﬁnding scalable approximation techniques 31117194647 22 Oﬄine algorithms versus online algorithms Developing algorithms solving DECPOMDPs approximately thriving research area Most existing algo rithms operate oﬄine generating type complete policy execution begins The policy speciﬁes action possible runtime situation While good performance achieved algorithms signiﬁcant time day solve modest problems The reason need consider possible policies agents suﬃciently large set policies order preserve solution quality In domains robot soccer feasible consider possible strategies end Besides small changes environments dynamics require recomputing policy In contrast online algorithms need plan current action faster This long recognized competitive game playing chess A typical algorithm performs limited lookahead plans current step repeats process online observing opponents response Furthermore online planning better handle emergencies unforeseen situations allows online approaches applicable domains oﬄine approaches adequate Implementing online algorithms decentralized multiagent systems challenging Since underlying state observations agents available execution time agent reason possible histories observed agents affect action selection In cooperative multiagent domains agents ensure coordination Consider example robot soccer problem defenders D1 D2 trying mark attackers A1 A2 Each defender different observations environment compute different best joint plan online based local knowledge For example best joint plan based D1s knowledge cid3D1 A1 D2 A2cid4 best joint plan based D2s knowledge cid3D1 A2 D2 A1cid4 D A j denotes defender D marking attacker A j However actual joint action executed cid3D1 A1 D2 A1cid4 example miscoordination The outcome miscoordination arbitrarily bad cause severe failures teamwork 57 In example mentioned miscoordination leads undesired behavior defenders mark A1 A2 left unmarked In practice agent computes policies different private information risk resulting policies fail achieve intended effects On hand agents ignore private information policies openloop controllers considering local observations Thus deﬁne coordination follows Deﬁnition 2 coordination When agents maintain single shared plan joint policy execute action plan policy MAS exhibits coordination Otherwise exhibits miscoordination F Wu et al Artiﬁcial Intelligence 175 2011 487511 491 It diﬃcult guarantee coordination planning performed online agents different local informa tion Another major diﬃculty online algorithms meet realtime constraints greatly reducing available planning time compared oﬄine methods Due diﬃculties work online planning DECPOMDPs sparse In paper address key challenges multiagent online planning answer following questions 1 How guarantee coordination agents operate based local observations having different private infor mation 2 How meet planning time constraints bound memory usage agent reason vast number possible outcomes action choices agents 23 Communication decentralized POMDPs Agents limited source information teammates reason possible histories team members histories affect behaviors Communication alleviate problem sharing pri vate information sensory data Hence online algorithms incorporate communication improve performance However communication limited bandwidth costly unreliable For example robots work underground planet need certain locations initiate communication Even communication readily available cheap instance case indoor mobile robots limited bandwidth unreliability lead latency robots need wait period resend messages times critical information fully received In situations communication affect negatively performance Thus interesting challenging question integrate online planning communication use communication effectively Bounded communication recognized important characteristic multiagent systems factors 1 Agents regions coverage communication signal For example search rescue robots working subterranean tunnels subways caves wireless communications high frequency waves penetrate rock limiting radio communication areas line sight transceivers 30 Another example Mars rovers rover located blocking obstacle 6 In examples robots certain spots wireless connection available try communicate Obviously communication costly working sites far communication spots Agents bound frequency communication maintain coordination communication unavailable 2 Agents able share information agents time limitations communication channel computational device Consider example AIBO soccer robot 42 Communication latency wireless network high On average messages takes 05 seconds received teammate robots cases latency observed high 5 seconds This latency makes diﬃcult robots communicate recent information time The robots limited computational power They receive process images 20 frames second Because image contain relevant state features takes time build world model compute shared Besides receiving messages 20 Hz overloads robot operating systems message buffers After minutes attempting communicate private information updated robots motion controllers affected causing robots slow occasionally crash 3 Another important factor communication failures common wireless sensor networks 1 Failures require multiple attempts communicate information transmitted successfully In scenarios mentioned bounded communication preferable Additionally known free communication reduces DECPOMDP large single agent POMDP This having agent broadcast local observation agents time step When local observations known agent treated single joint observation giving complexity singleagent POMDP PSPACEcomplete When communication free ﬁnding optimal communication policy hard general DECPOMDP NEXPcomplete 41 In work adopt bounded communication suitable assumption domains interested There possible ways agents share information coordinate actions indirect communi cation direct communication common uncontrollable observed features In indirect communication agents actions affect observations agent Hence observations serve messages transmitted agents Generally observations agents dependent nonlocal information obser vation provides form indirect communication Thus general DECPOMDP includes form indirect communication policy determines communicate With direct communication information shared agents sending messages directly When components global state observed agents affected agents actions agents act common knowledge coordinate actions exchanging messages directly In work consider direct communication use sync communication model 61 agent broadcasts local information The communication language simply allows transmission agents action observation histories The tell query models 61 complex allowing way communication pair agents In designing general communication strategy determine communicate communicate send However sync model main question initiate communica tion Once communication initiated agent agents share local information Communication generally 492 F Wu et al Artiﬁcial Intelligence 175 2011 487511 Fig 1 Online planning framework agents assumed instantaneous message received delay soon sent But work consider impact possible stochastic delays The sync communication model essential simplify high complexity planning different sets partial information Nevertheless necessarily mean agents transmit local information agents Agents need transmit information relevant establishing coordination For example multiaccess broadcast channel problem 11 actual information communicated bytes indicate status buffer messages buffer megabytes gigabytes large Similarly common ﬁle sharing broadcast ﬁle names directory peers instead transmitting ﬁles In DECPOMDPs messages exchanged agents sequences highlevel observations sensor readings basis observation Additionally possible incorporate selective communication methods 43 model Generally choosing observations larger impact beliefs interesting direction left future work In practice usefulness selective communication depends observation model deﬁned Even selective communication question communicate remains open Another challenge simpliﬁed sync model choice communicate This particularly relevant agent interacts directly subset group entire team In paper focus general DECPOMDP settings assuming special interaction structure The domains investigate require coordination agents means action agent affects optimal choices The sync model suitable settings question communicate presents challenging Additionally use andcommunication 20 assuming separate communication action phases time step Thus communication facilitates better domainlevel action selection conditioning domain actions speciﬁc infor mation received replacing domainlevel actions orcommunication 20 We factor explicit cost communication work cost arbitrary particularly relevant appli cations interested In experimental results approach effective sense achieves better value communication compared existing online techniques communication This guarantees cost communication speciﬁed approach beneﬁcial cost 3 Multiagent online planning communication In section introduce new algorithm MultiAgent Online Planning Communication MAOPCOMM ﬁnding approximate solutions general DECPOMDPs This algorithm executed parallel agents team interleaving planning execution More precisely online algorithm divided planning phase executing phase updating phase applied consecutively time step An example involving agents shown Fig 1 Deﬁnition 3 belief pool A belief pool timestep t deﬁned tuple cid3H t agent Bt set joint belief states Bt bhtht Ht Ht iI Ht I Btcid4 Ht set histories To illustrate concept consider robot soccer problem mentioned Section 22 Suppose defense strategy defender mark nearest attacker Then belief pool contain knowledge nearest defender attacker As long defenders maintain belief pool use tiebreaking rule outcome plans compute determining defender marks attacker This guarantees strategies defenders coordinated F Wu et al Artiﬁcial Intelligence 175 2011 487511 493 Algorithm 1 Expand Histories Update Beliefs Input H t Bt δt H t1 Bt1 ht H t cid5o cid5Ω cid5a δt ht append cid5a cid5o end ht ht1 ht cid5a cid5o calculate distribution ht1 pht1 pcid5o cid5aht pht test ht1 reachable joint history pht1 0 H t1 H t1 ht1 compute belief state ht1 bt1ht1 Update belief bt ht cid5a cid5o add bt1 hash table indexed ht1 Bt1 Bt1 bt1ht1 return H t1 Bt1 Deﬁnition 4 local joint policies A local policy agent mapping set histories set actions δi H Ai δihi denotes action assigned history hi A joint policy set local policies δ cid3δ1 δ2 δncid4 agent δh denotes joint action assigned joint history h In planning phase agent computes joint policy δt possible history belief pool During ht1 ht appends action end ai After agent updates belief pool based plan δt shown Algorithm 1 executing phase agent adds new observation local history ht according component joint policy δt current local history ht local history ht continues step executes action ai δt ht ot As mentioned coordination important issue multiagent planning outcome uncoordinated policies arbitrarily bad In oﬄine planning coordination guaranteed distributing executing pre computed joint policy It diﬃcult achieve coordination online planning policy computed online agent receives different partial information environment In online algorithm agent maintains joint histories team This ensures agents ﬁnd joint policy remain coordinated While algorithm randomized ensures agent ﬁnds set joint policies pseudorandom number generator identical seed It important emphasize use common knowledge planning With belief pool randomization scheme agent generate exactly joint policy Each agents local observation policy execution Deﬁnition 5 belief inconsistency When agent believes p true agents observation implies p belief agent inconsistent Intuitively belief inconsistency occurs agents beliefs contradict observations obtains environ ment For example robot soccer problem mentioned Section 22 suppose D1 observes nearest attacker A2 A1 indicated inconsistent belief Then D1 communicate defenders inform ing nearest attacker D1 actually A2 update belief pools accurate information Hence team beneﬁts communication ﬁnding better plans based consistent beliefs This communication triggered algorithm detects belief inconsistency The agent initiates com munication soon communication resource available When communication occurs agent broadcasts local observation sequence agents sync model Consequently agent construct actual joint history calculate actual joint belief state The best joint action selected based new joint belief state And belief pool emptied replaced actual history Notice communication essential ensure coordination framework offers optional mecha nism improve performance This makes use communication ﬂexible particularly domains bounded communication resource Communication easily integrated framework considering plan ning phase Coordination guaranteed case communication updates common knowledge agents More precisely belief pools agents communication remain communication occurs The sync model work guarantees agent knowledge communication 494 F Wu et al Artiﬁcial Intelligence 175 2011 487511 Algorithm 2 MultiAgent Online Planning Communication Input b0 seed1T 1 foreach I parallel cid5a0 arg maxcid5a Q cid5a b0 Execute action a0 initialize h0 H 0 cid5a0 B0 b0 τcomm false t 1 T 1 Set random seed seedt H t Bt Expand histories beliefs H t1 Bt1 Get observation environment ot Update agent local history ot ht H t inconsistent ot τcomm true τcomm true communication available Synch ht τcomm false agents agents communicated ht Construct communicated joint history bt ht Calculate joint belief state ht cid5at arg maxcid5a Q cid5a bt ht H t ht Bt bt ht π t Search stochastic policy H t Bt Select action according π t aiht H t Bt Merge histories based π t Update agent local history ht Execute action Theorem 1 The multiagent online planning algorithm MAOPCOMM guarantees coordination agents communication Proof sketch We present mechanisms ensure agent maintains belief pool ﬁnds joint policy team coordinate online As shown Algorithm 1 common knowledge arising model update belief pool Agents run algorithm lockstep set predetermined random seeds ensure agents come randomized behavior The private information local history agent tracked remembering actions executed observations received previous steps This local history execute plan effect coordination mechanisms In Fig 1 inputs outputs plan update modules agents The execute module considers local information agent change belief pool joint policy lead miscoordination We use sync communication model reset belief pools histories joint belief state Thus agent maintains belief pool communication As long belief pools agent identical joint policies computed based belief pools Therefore agents remain coordinated cid2 Our online algorithm starts ﬁrst calculating executing best joint action initial belief state heuristic value function Then main planning loop shown Algorithm 2 executed Generally major challenges implementing framework 1 It NPhard problem ﬁnd decentralized policies possible history agent 58 In Section 31 provide approximate solution solving series linear programs 2 The belief pool extremely large making impossible managed online More precisely number possible histories grows exponentially time step In Section 32 introduce policybased techniques bound memory usage belief pool 3 Detecting inconsistency belief pool nontrivial given agents local information In Section 33 method address problem eﬃciently Finally Section 34 discuss data structures implementation store belief pools 31 Searching stochastic policies linear programming In decentralized multiagent systems agents knowledge observations agents reason possible belief states held affects action selection In order ﬁnd agent policy qi history hi agents need reason possible histories hi held F Wu et al Artiﬁcial Intelligence 175 2011 487511 495 Fig 2 Illustration similarity onestep online planning onestep oﬄine policy tree construction possible policies associated In words need ﬁnd joint policy δ maximizes value function cid6 cid6 cid2 cid3 V δ pshV δh s 5 hH sS psh state distribution given joint history h It important point joint policy created approach truly decentralized policy depends private information agent That resulting policy agent depends individual obser vation history δh cid3δ1h1 δnhncid4 The goal multiagent online planning agent independently calculates plan δh team executes share plan based local history For example agent execute action ai δihi The advantage decentralized policy agent execute plan independently based local information acquired far Finding decentralized policies online analogous onestep policy tree construction oﬄine DECPOMDP planning algorithms As shown Fig 2 histories h1 h2 h8 paths tree root current branches The target online oﬄine planning associate histories right subpolicies satisfy optimality criterion In oﬄine planning histories represented trees goal construct best complete policy In contrast online algorithms store histories sequence form evaluate policy current step Obviously number histories represented sequence form smaller key advantages online planning The straightforward way ﬁnding best joint policy enumerate possible mappings histories sub policies choose best However size joint space exponential number possible histories The number histories grows exponentially problem horizon In fact problem equivalent decentralized decision problem studied 58 proved NPhard 58 In algorithm ﬁnd approximate solution stochastic policies solving problem linear program The value joint stochastic policy π follows πiqihiQ cid2 cid3 cid5q bh 6 V π cid6 cid6 cid11 ph hH cid5q iI ph probability distribution history h bh belief state induced h Q cid5q bh value policy cid5q bh Note cid5q policy current time end problem Q cid5q bh value agents achieve future steps starting state distribution bh Unfortunately optimal value Q available online In fact subproblem original DECPOMDP new horizon T t t current time step So trying ﬁnd optimal value equivalent solving entire problem oﬄine simply setting t 0 But optimal value estimated certain heuristics Usually heuristic close optimal value time compute yield better performance vice versa We use onestep lookahead estimate value future steps It means consider policy action node In case approach provides set value functions V s deﬁne heuristic Ideally heuristic represent immediate value joint action expected future value As mentioned ﬁnding optimal value intractable requires work solving entire DECPOMDP One approach solution underlying MDP For onestep lookahead case qi cid5q simpliﬁed ai cid5a The QMDP heuristic 28 written follows Q cid5a b cid9 bs Rs cid5a cid6 sS cid6 scid7S cid2 P cid7s cid5a s cid3 V MDP cid10 cid3 cid2 cid7 s 7 496 F Wu et al Artiﬁcial Intelligence 175 2011 487511 Table 1 Improving policy linear programming Variables ε πi qi hi Objective maximize ε Improvement constraint cid4 V π ε cid2 hH ph cid4 cid5q πi qi hi cid12 kcid15i πkqkhkQ cid5q bh Probability constraints cid4 πi qi hi 1 hi H qi hi H qi πi qi hi cid3 0 V MDP value function underlying MDP The QMDP heuristic upper bound optimal value based assumption agents fully observe underlying state step A tighter bound QPOMDP heuristic 44 cid9 cid10 Q cid5a b bs Rs cid5a cid6 sS cid6 cid2 P scid7S cid7s cid5a s cid3 cid6 O cid5o cid5Ω cid3 cid2 cid5os cid7 cid5a V POMDP cid3 cid2 b cid5o cid5a 8 cid5o cid5a successor belief state b cid5a cid5o V POMDP value function underlying POMDP Intuitively b means agents share observations future step When concrete problem considered domain speciﬁc knowledge better estimate heuristic value In implementation use QMDP heuristic underlying MDPs tested domains solved quickly optimally The onestep lookahead extended multistep lookahead complex settings scope article To start search procedure local stochastic policy πi initialized deterministic selecting random action uniform distribution Then agent selected turn policy improved keeping agents policies ﬁxed This agent ﬁnding best parameters πiqihi satisfying following inequality cid10 cid2 cid3 cid5q bh πiqihiπiqihiQ V π cid2 cid9cid6 ph cid6 9 hH cid5q cid12 πiqihi kcid15i πkqkhk The linear program shown Table 1 ﬁnd new parameters πi The improvement procedure terminates returns π ε suﬃciently small agents Although algorithm terminate ﬁnite number iterations convergence optimality guaranteed It possible stuck suboptimal Nash equilibrium policy agent optimal respect In fact suboptimal solution achieve value arbitrarily far globally optimal Hence algorithms start arbitrary policy iterative improvements alternating agents produce polices guaranteed bound optimal value One simple technique use random restarts local maxima The observation different starting points converge different locally optimal values Hopefully random restarts values globally optimal close This process shown Algorithm 3 The number restarts determined online runtime constraint If time planning step long restarts better chance close globally optimal solution This simple technique works test domains experimented Note agent shares random seed agents randomized behavior It easy cid7 construct situations policies qi q value In order guarantee coordination agent choose policy according predetermined tiebreaking rule based canonical ordering policies qi q cid7 32 Bounding joint histories policybased merging Note underlying state observations agents available execution time DECPOMDPs Each agent reason possible histories observed agents affect action selection However number possible joint histories increases exponentially horizon ΩiT I assuming agent coordinates step knows joint policy For small problem 2 agents 2 observations number possible joint histories 100 steps 2100216 1060 infeasible planning algorithms Even storing memory impossible This presents major challenge developing online algorithms DECPOMDPs Optimal history merging A detailed analysis planning process shows joint histories kept memory useless One reason goal reasoning histories ﬁnd Clearly time sequence corresponds agent Because share private information agents maintain distribution histories based information As F Wu et al Artiﬁcial Intelligence 175 2011 487511 497 Algorithm 3 Search Stochastic Policies Random Restarts Input H B restarts select start point randomly π Initialize parameters deterministic random actions repeat ε 0 foreach I optimize policy alternatively πi εcid7 Solve linear program Table 1 H B πi ε ε εcid7 ε suﬃciently small π current best policy π π return π long distribution suﬃciently accurate eliminating histories early effect decision Another reason history segments early stage useless For example multiagent tiger problem 32 agents facing doors left right lies tiger lies untold riches The agents independently open door listen position tiger The reward function designed encourage coordination opening door sustain injury tiger present receive greater wealth riches present The listen action incurs small cost The position tiger reset randomly door opened The resulting policy horizon 6 agent listens twice opens door receives observation tigers position listens twice opens door received observations tigers position For histories agent opens door step best action time step affected observations received steps 1 3 Those observations provide useful information tigers position reset door opened From perspective team important thing agent probability opened door step 3 This quantity grouping similar histories Deﬁnition 6 Two histories hi h hi s bsh bsh cid7 agent probabilistically equivalent PE following holds hi ph ph cid7 cid7 h cid3hi hicid4 h cid7 cid3h cid7 hicid4 hi joint history agent The PE condition means histories agent distribution resulting belief state differ actionobservation sequences The following property established histories probabilistically equivalent Lemma 1 See 38 When histories PE bestresponse equivalent clustered history loss value Although PE condition nice theoretical properties applicable practice requires know hi possible history combination agents As mentioned earlier range values hi large intractable test possibility Considering possible hi important single value affect policy agent Hence worth analyzing values affect agent policy First agent form belief current state reasoning history agents hi Together history hi agent calculate state distribution bcid3hi hicid4 based joint history Second agent needs know policies choose policy complements In context goal reasoning histories agents compute policy If optimal policy q agent hi given agent follow policy considering history For example multiagent tiger problem agent knows optimal policy open door hears tigers roar door twice remembering happened necessary Deﬁnition 7 policy equivalent Two histories hi h identical optimal policy cid7 agent policy equivalent POE hi h cid7 Here policy means complete conditional plan current step step Usually policy represented tree nodes corresponding actions branches corresponding observations The optimal policy ﬁxed point policy space produces best team performance given agents follow optimal policies Therefore optimal policy agent optimal joint policy team In DECPOMDPs exists 498 F Wu et al Artiﬁcial Intelligence 175 2011 487511 optimal joint policy team The optimal joint policy small problems computed optimal oﬄine algorithms 5275556 Theorem 2 When histories hi h cid7 agent POE merged loss value policy generation keeping Proof At step 0 optimal policies steps 0 T given agents select optimal joint policy b0 At step t At future step t k assume agent merges histories ht kstep history hk If optimal policies cid7t cid7t ht different different subtrees contradicting assumption ht h optimal policy Due assumption optimality qtk At step t k given k agent ﬁnd optimal policy merging ht cid7t share optimal policy qt share optimal policy qtk subpolicy tree qt h cid7t The theorem holds steps induction cid2 h cid7t hk history ht h h hk Theorem 3 When histories agent PE POE meaning optimal policies Proof The proof based theorems 38 Suppose histories ht identical extensions htk PE Since htk h optimal action htk h ht h equal optimal policy POE cid2 cid7tk h cid7tk cid7tk cid7t PE step t At step t k cid7t created appending lengthk actionobservation sequence end ht PE equal optimal Qvalue function And agent select based optimal Qvalue function Hence easy build policy optimal cid7t considering possible extension optimal actions When histories agent PE h h However histories agent POE necessarily PE For example multiagent tiger domain suppose agent opens left door OL history hi opens right door OR history cid7 After door opened tigers position reset So agent follow optimal policy Obviously hi O L h O R POE Therefore POE condition general PE condition P E P O E PE h In singleagent POMDP policy tree evaluated given history agent compare policy value Since multiple optimal policies exist value value equivalent VE condition general P O E V E However multiple agents involved joint policy joint history evaluated It clear calculate exact value local policy given history single agent DECPOMDPs cid7 Approximate history merging In DECPOMDP oﬄine planning POE condition facilitates subpolicy reuse Reusing policy histories mapping histories single policy When handling policy referred reuse It called merging managing histories In optimal bottomup dynamic programming algorithm best policies current step available The number policies lower number possible histories small set policies reused build complex policies iteration When considering approximate solutions condition optimal policies relaxed For example consider robot sent clean 10 rooms building There type rooms 5 oﬃce rooms 3 meeting rooms 2 restrooms The robot needs know clean types rooms In words optimal policy type room reused times policy cleaning 10 rooms In scenario policy cleaning type room necessarily optimal In fact basic idea MBDP ﬁxed number policies reuse approximately building policies iteration The observation applied online planning way merge histories Unfortunately optimal policy agent available execution time Finding optimal policy hard solving entire problem But approximate future polices limited lookahead A kstep lookahead policy set policy trees depthk agent The kstep lookahead policy evaluated decomposing value function exact evaluation ksteps heuristic estimate remaining Then deﬁne similarity comparing structure depthk trees The kstep lookahead policy generated pre computed heuristics The POE condition approximated similarity kstep lookahead policies More precisely histories agent POE kstep lookahead policies similar structure In paper require depthk policy trees identical considered similar generalized measures similarity We present way Algorithm 4 maintain bounded size belief pool online use coordinate strategy team Bounding size histories important especially online planning planning time step limited Clustering methods guarantee reach desired size belief pool If size large algorithm exceed planning time miss action cycle Even worse policy way maintain belief pool In real applications cause damage special recovery process implemented In algorithm merge histories POE randomly choose history policy number histories retained bounded number policies generated deﬁnition similarity At step heuristics perform kstep lookahead create ﬁxed number policies F Wu et al Artiﬁcial Intelligence 175 2011 487511 499 Algorithm 4 PolicyBased History Merging Input H t Q t π t foreach I H t Hi hash table indexed qi Hiqi qi Q group histories based policy foreach hi H t policy hi according π t qi Select policy according π t add hi hash table key qi Hiqi Hiqi hi qihi generate new set histories foreach qi Q t history policy Hiqi hi Select history Hiqi randomly H t H t hi history set H t qi Select policy Q t Hiqi Q t randomly hi Select history Hiqi randomly H t H t hi return H t Fig 3 Communication model agents 33 Communicating inconsistency arises In online planning framework communication initiated planning phase shown Fig 3 First agent decide communication necessary check resource available If communication needed agents continue plan communication If communication needed resources unavailable agents postpone communication step Otherwise sync local information planning based information received teammates The local information communicated agents local observa tion sequence communication step current step The resources include local communication device agents communication channel All communication failures require recommunication step The decision communicate agents initiate communication communication postponed previous step inconsistency belief pool detected To verify inconsistency belief pool straightforward agents know current state However DECPOMDP model state unavailable online agent receive local observation execution time Fortunately agents local observation provides partial information state Hence detect problem examining inconsistency belief pool local observation agent 500 F Wu et al Artiﬁcial Intelligence 175 2011 487511 environment Intuitively agent tries action observes oi probability cid8 according belief pool likely wrong belief pool Let ht denote agent local history step t ot local observation agent receives environment local history maintain step t local actionobservation sequence We denote step t Note ht Bht set joint beliefs history component ht pool Deﬁnition 8 cid8inconsistency At time step t maintained belief pool Bt said cid8inconsistent agent local observation ot cid13cid6 max bot scid7S cid2 cid5o ts cid7 cid5a O cid3cid6 cid2 P sS cid14 cid3 bs cid7s cid5a s cid8 10 ot ot kcid15i Ωk cid5a joint action based cid5o t joint policy computed previous step cid5o t ot b Bht This deﬁnition monitor inconsistency agent local history observation provides indication history inconsistency pool The threshold cid8 determined structure observation function If uncertainty observation small cid8 small Note rule detect form inconsistency belief pool based current local observations And inconsistency detected means high likelihood problem belief pool The communication determined observation structure heuristic Intuitively agents right decision long belief pool contains joint history close real joint history However agents obtain observations agents execution time impossible know real joint history But check inconsistency history pool based local information If pool inconsistent agent refresh belief pool communicating agents synchronizing observation sequence After synchronization belief pool contains real joint history consistent Unlike approaches require instantaneous communication approach allows agents postpone commu nication resource unavailable They sacriﬁce value decisions communication The role communication improve performance possible When communication fails long time stateoftheart approaches continue decisions openloop methods totally ignore local ob servations 4452 localgreedy methods lead miscoordination different local information agent Obviously openloop localgreedy policies arbitrarily poor given problem suﬃciently long horizons Our approach computes joint policy conditioned local observation agent The common joint policy ensures agent coordinated actions utilizing local information Hence agents coordinate behavior communication It worthwhile point concept belief inconsistency fundamentally different idea belief divergence 60 The belief divergence approach ﬁrst establishes reference point belief agents synchronized knowledge Then compares current belief state reference point The distance belief points measured KLdivergence If divergence reaches threshold agents communicate synchronize observation sequences This approach assumes agents receive independently new observations beliefs remain static communication step 60 In contrast approach keeps updating joint beliefs given possible observations agents Belief inconsistency detected agents local observation match projected joint belief To summarize approach unique ability postpone communication time account new local observations maintain coordination While approaches allow easily modiﬁed al low postponement communication result signiﬁcant degradation performance Speciﬁcally DecComm leads openloop operation discarding local observations In approaches rely belief divergence postponing communication leads greater divergence beliefs increases miscoordination 34 Implementation considerations As mentioned earlier try onestep lookahead implementation So policy agent onenode policy tree associated certain action When storing history need save action observation sequence belief pool We need assign index history use hash table ht cid3 cid5θ btcid4 represent joint history cid5θ cid3θ1 θncid4 θi history index agent b joint belief induced joint history Eq 1 Since history policy history index agent represented tuple θi cid3qt1 observation current step policy tree index previous step ot cid4 qt1 ot F Wu et al Artiﬁcial Intelligence 175 2011 487511 501 Fig 4 Example history expansion updating The joint history cid3 cid5θ0 b0cid4 components cid3q1 cid4 cid3q2 cid4 pool expanded cid3 cid5θ1 b1cid4 cid3 cid5θ2 b2cid4 cid3 cid5θ3 b3cid4 cid3 cid5θ4 b4cid4 assigning possible joint observations Agent 1 gets observation o1 environment updates lo cal history cid3q1 cid4 cid3q1 o1cid4 Agent 2 gets observation o2 updates local history cid3q2 cid4 cid3q2 o2cid4 Fig 5 Example history merging policy execution The histories cid3q2 o1cid4 cid3q2 o2cid4 pool map policy q5 history randomly selected cid3q2 o1cid4 step index updated cid3q5 cid4 cid3q1 o1cid4 maps q3 updated index cid3q3 cid4 cid3q1 o2cid4 maps q4 updated index cid3q4 cid4 The policy local history agent 1 cid3q1 o2cid4 q3 agent 1 executes q3 updates local history cid3q3 cid4 Agent 2 executes q5 updates local history cid3q5 cid4 cid3q2 o2cid4 maps q5 For onestep lookahead case index simpliﬁed θi cid3at1 represent element belief pool ht Ht ot cid4 Therefore data structure use ht cid15cid15cid15 qt1 ot cid16 cid17cid18 cid19 1 1 θ1 cid16 cid20 cid20 qt1 ot cid16 cid17cid18 cid19 2 2 θ2 cid17cid18 cid5θ cid20 cid20 cid20 bt cid15 qt1 ot cid16 cid17cid18 cid19 n n θn cid19 At step update index joint belief state Fig 4 shows expand histories update index agents local history observation Fig 5 shows merge histories update index agents local history executing policy Note new history created appending ﬁrst new action new observation end previous history The history index observation represented cid3q cid4 ﬁgures intermediate histories update process hi ai missing observation It possible design different types indices history policy needed cid7 o a1 o2 at1 oicid4 qt policy agent executes oi cid4 pool change agent local history index cid3qt Each agents local history execution represented index At step update index agents local history cid3qt observation received environment current step t If history indexed cid3qt oicid4 merged represented cid7 cid4 Hence map agents local history cid3qt o history history pool For purpose communication agent stores actionobservation sequence cid3a0 o1 To summarize developed new data structures implement belief pool agents local history Instead storing observationaction sequences use indices represent histories Each index history cid3qi oicid4 contains parts pointer agent policy qi current local observation oi In Fig 4 expand belief pool joint observation update agents local history current observation received environment In Fig 5 expanded histories merged agents local history updated executing new policy These ﬁgures illustrate key operations new representation indices implementation cid4 includes actions executed observation received ot 502 F Wu et al Artiﬁcial Intelligence 175 2011 487511 4 Experimental results We implemented tested MAOPCOMM standard benchmark problems challenging prob lem called grid soccer In environments ﬁrst solved underlying centralized MDP provided resulting value function heuristic algorithm The reported results averages 20 runs algorithm problems We present average accumulated reward Reward average online runtime step Times average percentage communication steps Comm different horizons Horizon While communication limited minimizing important goal add explicit cost communication cost arbitrary particularly relevant applications The main purpose experiments test highvalued plans computed quickly online little communication Speciﬁcally goal achieve signiﬁcantly better value signiﬁcantly communication compared stateoftheart MAOPCOMM implemented Java ran 24 GHz Intel Core 2 Duo processor 2 GB RAM Linear programs solved lp_solve 55 Java wrapper All timing results CPU times resolution 001 second We try compare MAOPCOMM existing online planners use communication BaGAComm DecComm DecComm particle ﬁltering DecCommPF solve benchmark problems As mentioned earlier main reason limitation BaGAComm exact version DecComm bound size histories beliefs In experiments observed agents kept silent 10 steps Consequently number possible joint histories large 5210 2 agent problem 5 observations 10 steps communication Even BaGACluster approach reduce pool histories manageable size test domains BaGAComm exact version DecComm ran memory time quickly Therefore compared MAOPCOMM DecCommPF existing algorithm bounds memory In fact BaGAComm DecComm yield similar performance average values communication domains tractable Another fact pointed 44 DecComm exact tree representation joint beliefs DecCommPF approximates beliefs suﬃciently large particle ﬁlters provide substantial difference performance Therefore comparing algorithms DecCommPF leading communicative online algorithm applicable test problems presents best way assess beneﬁts approach To results perspective better understand role communication include results FULLCOMM case communication communicating observations step cid8 MAOP online approach communication inconsistency monitoring cid8 0 The monitoring threshold MAOPCOMM set cid8 001 number particles DecCommPF 100 According experiments 100 particles resulted substantial difference value obvious increase runtime DecCommPF We use discount factor experiments tested problems involve ﬁnite horizon It important emphasize FULLCOMM strategy expected outperform approaches use partial com munication Nonetheless provide results FULLCOMM establish upper bound online algorithm Although perfectly reliable instantaneous communication generally unrealistic interesting online communication approach performs uses MDP heuristic In implementation FULLCOMM runs POMDP joint actions observations The joint action selected step based onestep lookahead MDP heuristic It takes little time main computation Bayesian update joint belief state We consider time transmitting observations agents As discussed earlier FULLCOMM strategy simply reduces DECPOMDP POMDP easier solve Our goal experiments proposed approach online planning bounded communication effective compared FULLCOMM upper bound 41 Perfectly reliable communication channel Our initial set experiments involves communication channel perfectly reliable assumed available noise We relax assumptions following section Standard benchmark problems The ﬁrst set experiments involves standard benchmark problems Broadcast Chan nel 11 Meeting Grid 11 Cooperative Box Pushing 46 Stochastic Mars Rover 4 These benchmarks widely evaluate cooperative multiagent planning algorithms modeled DECPOMDPs1 Because number possible histories extremely large bounding size histories key contributions benchmark problems larger observation sets Other wellknown benchmark problems MultiAgent Tiger 32 Recycling Robots 2 Fire Fighting 35 2 observations The Broadcast Channel problem 11 simpliﬁed agent networking problem At time step agent choose send message If agents send messages collision gets This problem 4 states 2 actions 5 observations The results Table 2 problem methods achieved 1 Original domain descriptions available download DECPOMDP repository httpusersisristutlptmtjspaandecpomdpindex_enhtml F Wu et al Artiﬁcial Intelligence 175 2011 487511 503 Table 2 Benchmark results 20 trials Horizon Algorithm Broadcast channel S 4 Ai 2 Ωi 5 20 100 MAOP MAOPCOMM DecCommPF FULLCOMM MAOP MAOPCOMM DecCommPF FULLCOMM Meeting 33 Grid S 81 Ai 5 Ωi 7 20 100 MAOP MAOPCOMM DecCommPF FULLCOMM MAOP MAOPCOMM DecCommPF FULLCOMM Cooperative box pushing S 100 Ai 4 Ωi 5 20 100 MAOP MAOPCOMM DecCommPF FULLCOMM MAOP MAOPCOMM DecCommPF FULLCOMM Stochastic Mars Rover S 256 Ai 6 Ωi 8 20 100 MAOP MAOPCOMM DecCommPF FULLCOMM MAOP MAOPCOMM DecCommPF FULLCOMM Reward Time s Comm 1825 1835 1845 1890 8995 9035 900 9060 310 335 290 475 1530 1710 1490 2470 750 9930 13675 22250 160 44195 29650 88050 1819 4619 4502 6141 5115 22286 13304 32504 001 001 001 001 001 001 001 001 015 026 022 001 019 030 024 001 014 016 035 001 013 013 036 001 097 005 246 001 220 009 239 001 00 00 00 1000 00 00 00 1000 00 110 7050 1000 00 1210 7820 1000 00 1150 8350 1000 00 1226 5987 1000 00 170 220 1000 00 1800 3500 1000 similar values runtime 001 seconds Both MAOPCOMM DecCommPF initiated communication The performance communication case communication These results simple intuitive explanation The probability agents buffer ﬁll step 09 agent 01 Therefore easy coordinate case simply giving agent higher priority In Meeting Grid problem 11 robots navigate grid obstacles The goal robots spend time possible location In order problem challenging larger 33 grid simulated noisy sensor 09 chance perceiving right observation This problem 81 states robot 9 squares time Each robot 5 actions 7 legal observations sensing combination walls The results Table 2 MAOP online algorithm communication performed surprisedly case The onestep lookahead provided good heuristic problem agents meet problem resets The results FULLCOMM agents beneﬁt communication MAOPCOMM achieved higher value DecCommPF communication The runtimes MAOPCOMM MAOP DecCommPF short close In Cooperative Box Pushing domain 46 agents located 34 grid required push boxes small large box goal area The agents beneﬁt cooperation cooperatively push large box goal area high reward In order problem challenging agents transition random state problem resets We included uncertain observations domain 09 probability right observation 0025 probability This domain 100 states 4 goal states 96 non goal states Each agent 4 actions 5 observations The results Table 2 domain communication improve performance signiﬁcantly MAOP communication performed poorly The onestep lookahead longer good heuristic agents domain multiple goals large box small box For domain longer horizons 100 MAOPCOMM outperformed DecCommPF communication MAOP MAOPCOMM ran little faster DecCommPF 504 F Wu et al Artiﬁcial Intelligence 175 2011 487511 Fig 6 The 33 grid soccer domain 1 opponent 2 teammates Table 3 Grid soccer results 20 trials Horizon Grid soccer 23 S 3843 Ai 6 Ωi 11 Algorithm 20 100 MAOP MAOPCOMM DecCommPF FULLCOMM MAOP MAOPCOMM DecCommPF FULLCOMM Grid soccer 33 S 16131 Ai 6 Ωi 11 20 100 MAOP MAOPCOMM DecCommPF FULLCOMM MAOP MAOPCOMM DecCommPF FULLCOMM Reward 18050 2906 12950 37390 115780 193390 144160 193360 19070 29600 27140 3560 80360 167950 104440 180820 Time s Comm 025 028 136 001 014 016 128 001 190 230 1526 001 196 250 948 001 00 1480 335 1000 00 1540 3080 1000 00 270 590 1000 00 2690 7070 1000 The Stochastic Mars Rover 4 larger problem 256 states 2 agents Each agent 5 actions 8 ob servations The original problem deterministic observations We challenging introducing uncertainty observations Each agent observes 09 probability 0025 probability observation The results Table 2 communication critical domain Without communica tion MAOP gets value MAOPCOMM communicating version Again longer horizon MAOPCOMM produces higher value DecCommPF communications To summarize MAOPCOMM performed benchmark problems communication DecCommPF In domains Broadcast Channel Meeting Grid MAOP achieve high value communication Although tested horizon 100 approach solve problems larger horizons bound size histories step The experimental results varied little horizon problems longer horizons greater chance miscommunication error accumulation The parameter cid8 presents good way tradeoff communication overall value In domains Cooperative Box Pushing larger cid8 allow communication consequently improve performance The grid soccer domain To demonstrate scalability tested algorithm challenging problem called grid soccer Shown Fig 6 domain includes agents team opponent team Each agent 4 possible orientations left right The opponent observation reliable actions executes ﬁxed policy tries approach ball fast possible If opponent bumps agent ball ball game terminates reward 50 If agent ball enters goal grid game terminates reward 100 Each agent 6 actions north south east west stay pass Each action 09 probability success 01 probability having impact current state When agent executes pass action ball transferred agent step agent executes stay action time Otherwise ball goes ﬁeld game terminates reward 20 For step resulting nonterminal state penalty 2 After reaching terminal state problem reset Each agent gets 5 possible observations describing situation free wall teammate opponent goal 2 observations indicating controls ball Thus total number observations 11 The observation noisy 09 chance perceive correct observation 001 chance perceive observations We tested algorithm grid soccer problems 23 grid 3843 states 33 grid 16131 states These problems largest tackled far decisiontheoretic algorithms multiagent planning F Wu et al Artiﬁcial Intelligence 175 2011 487511 505 Fig 7 Result accumulated rewards different thresholds The results shown Table 3 MAOPCOMM achieved higher value DecCommPF performance MAOP competitive indicating MAOPCOMM smaller cid8 use communication produce good value The runtimes MAOPCOMM MAOP times faster DecCommPF One reason operators MAOPCOMM MAOP cheaper particle ﬁltering DecCommPF Another reason number histories kept MAOPCOMM MAOP smaller number particles Dec CommPF MAOPCOMM MAOP scaled 33 instance The statesensitive operator Bayesian update For problem 16131 states update loop takes hundreds seconds Fortunately transition functions sparse real applications including grid soccer allowing optimize Bayesian update algorithms including DecCommPF Incidentally problems larger state spaces runtime advantage keeping histories signiﬁcant 42 Imperfect communication channel In realworld applications communication unreliable noise poor reception For example wireless communication network connectivity intermittent messages retransmitted times Our work allows communication postponed communication channel available While communication postponed approaches downside severe DecComm 44 example simply ignore agents local observations run openloop policies The belief divergence approach 60 incorrect assumptions agents beliefs run greedy policies likely uncoordinated In contrast communication postponed approach computes conditional plan takes consideration possible observations As discussed Section 3 conditional plan remains coordinated agents In section present experimental results cooperative boxpushing domain imperfect commu nication channel We simulated intermittent communication drawing random variable uniform distribution time communication initiated If value greater threshold communication channel available unavailable We varied threshold 00 10 measured accumulated rewards percent age communication averaged 20 runs Communication available threshold 00 communication allowed threshold 10 We results MAOPCOMM variants MAOPCOMM POSTPONE postpones communication channel available MAOPCOMMDROP simply drops communication attempt channel available We results accumulated rewards Fig 7 results percentage communication Fig 8 different thresholds As expected reward communication goes MAOPCOMMDROP probability unavailable communication high It means communication steps truly critical operative box pushing domain simply dropping gradually decrease rewards Interestingly rewards communication grow certain range threshold values postpone communication available Interestingly threshold starts growing 0 04 agents implemented MAOPCOMMPOSTPONE com municate little bit better value The value declines threshold continue grow We tried analyze explain phenomenon counterintuitive There possible reasons contributing When communication postponed agents need decisions communication The inconsistent belief pool uncertain time goes The actions computed based uncertain beliefs introduce randomness And value function based heuristic overestimates future performance 506 F Wu et al Artiﬁcial Intelligence 175 2011 487511 Fig 8 Result percentage communication different thresholds randomness positive effect performance Another possible explanation problems multiple goals agents change current goals communication Once agent initiates communication agents team forced refresh belief pool Since heuristic estimate future value changing goals frequently based heuristic desirable Therefore agents lose value communicate change undergoing goal Consider example cooperative box pushing domain Suppose Agent 1 close small box decides push based current belief pool At time Agent 2 close large box initiates communication forces Agent 1 refresh belief pool Based new belief pool heuristic agents establish new goal decide push large box However new goal computed overestimating heuristic achievable old goal Agent 1 achieved limited horizon Thus agents lose value incorrectly estimating abilities abandoning current goals This reason communicate frequency communication important 20 We compared algorithms DecComm communication channel imperfect As results MAOPCOMMPOSTPONE performs better DecComm higher value communication Interestingly value DecComm grows certain uncertainty reliability communication channel introduced Overall experiments conﬁrm advantage MAOPCOMMPOSTPONE domains imperfect communication important factor realworld applications 5 Related work The literature planning communication DECPOMDPs equivalent models generally divided works compute oﬄine policies In section related work explicit communication involved More general surveys DECPOMDP solution methods recently published Seuken Zilberstein 48 Oliehoek et al 35 Bernstein et al 10 51 Oﬄine planning communication One group solution methods determines entire plan communication strategy oﬄine plan execution starts The stored plan runtime The COMMTDP model 41 DECPOMDPCOM model 25 equivalent complexity 48 provide theoretical frameworks reasoning communication oﬄine The COMMTDP model offers framework analyzing optimality team performance computational complexity agents decision problem In terms optimality analysis able encode existing teamwork theories models based joint intentions STEAM 57 provide novel algorithm outperforms earlier coordination strategies The DECPOMDPCOM model formalizes problem given communication language semantics examines value optimal policy action communication different cost models Pynadath Tambes work focuses optimality complexity analysis communication models The optimal solution provide particularly useful practice complexity result NEXPcomplete gen eral communication case Besides COMMTDP communication policies joint intention instantiations require designer specify joint persistent goal allow agent send message goal achieved 41 The DECPOMDPCOM model applies general problem agents allowed communicate optimize timing frequency communication Both models require priori semantics communi cated messages The semantics deﬁne type messages situations agents communicate F Wu et al Artiﬁcial Intelligence 175 2011 487511 507 Assuming message set Σ userdeﬁned ﬁxed semantics simpliﬁes optimization problem makes model design diﬃcult domaindependent Spaan et al 50 established new model messages sent agents action vectors received time step recipients observation vectors In contrast COMMTDP DECPOMDPCOM models require explicit communication language instead treats semantics communication optimization problem They present iterative method computing joint policy team decentralized fashion treating communication integral reasoning process Given set ﬁxed policies agents agent policy computation process converts DECPOMDP POMDP perspective factors expected contribution joint team reward policies agents The communication policy based heuristic method information entropy deﬁned policies states However model restricted transition independent agents agents observation depends agents local state Generally reasoning communication oﬄine requires enumeration possible messages effect team Unfortunately number messages grows exponentially large set possible observation histories The COMMUNICATIVE DPJESP technique integrates communication strategy K step components JESP algorithm ﬁnds Nash equilibrium policies multiple agents 31 The JESP Joint EquilibriumBased Search Policies approach tries ﬁnd policy maximizes joint expected reward agent time keeping policies agents ﬁxed process repeated equilibrium reached 32 DPJESP dynamic programming approach reason agents policy conjunction teammates The multiagent belief state deﬁned approach distribution current state observation history agents Since agent know exactly observations agents received runtime algorithm allows agents periodically synchronize observation histories reduce likelihood undesired behavior In order algorithm tractable uses ﬁxed communication decision enforces rule communication occur K steps Thus policy indexed observation history length greater K It uses sync model assume separate communication phase That decision cycle agent choose communication act Using information value theory value communication deﬁned net gain communicating difference expected improvement agents performance communication costs associated communication When value communication greater certain threshold agents beneﬁt communication sharing local information However computing exact value communication intractable especially multiagent systems communication constrained agent different partial information overall situation Under myopic assumption communication possible present time possible estimate eﬃciently value communication Becker et al 7 studied implications myopic assumptions developed myopic communication strategy transitionindependent DECMDPs Carlin Zilberstein 18 extended work general DECPOMDP models improved performance nonmyopic reasoning Both works use sync model communication determine communication strategy oﬄine 52 Online planning communication Online approaches provide important alternative decision communicate occurs execution time The approaches similar Bayesian Game Approximation Communication BaGAComm 20 Avoids Coordination Errors reasoning Possible Joint Beliefs Communication ACEPJBComm 42 known DecComm 44 In BaGAComm framework series smaller Bayesian games constructed solved BaGA 21 gener ate policies jointtype spaces timestep An alternatingmaximization algorithm ﬁnd locally optimal solutions Bayesian game To guarantee agent suﬃcient information independently construct game large type space corresponds possible joint histories maintained agent In order exert control size type space followup method BaGAClustering 22 introduced types clustering Low Probability Clustering LPC Minimum Distance Clustering MDC LPC removes clusters low probability merging nearest remaining neighbor MDC repeatedly ﬁnds similar pair clusters merges Both LPC MDC use worstcase expected loss similarity measure Although clustering methods alleviate exponential growth histories domains generally bound number histories kept memory In worst case size histories grows exponentially limited clustering possible Besides clustering methods timeconsuming To improve performance communication strategies share information agents The authors present types communication strategies ﬁxed policy Expected Value Difference EVD policy approach based Policy Difference PD The experimental evaluation method showed EVD PD result similar performance number communication acts better ﬁxed policy approach This work demonstrated important decide agent communicate achieve good performance 20 Unlike BaGACluster merges histories based similarity terms worstcase expected loss merge histories based similarity future policy structures Moreover approach bounds size histories memory step BaGACluster Thus algorithm solve problems agents 508 F Wu et al Artiﬁcial Intelligence 175 2011 487511 communicate long period time BaGAComm intractable quickly state observation spaces large For communication EmeryMontemerlo uses sync model But transmitting local observations agents broadcast current type team However type information suﬃciently accurate lossy clustering procedure applied In DecComm framework agent maintains distribution possible joint beliefs chooses communicate integrating observation history joint belief causes change joint action selected QPOMDP heuristic function This work emphasizes challenge avoiding coordination error reasoning possible joint beliefs However number possible joint beliefs grows rapidly feasible store memory To address Roth et al utilize ﬁxedsize method modeling distribution possible joint beliefs particle ﬁltering The approach requires ﬁxed memory use sampling Nevertheless domains number particles needed accurately model joint beliefs large With particle representation agents initiate communications real joint belief sampled Our approach better address situations initiates communication history inconsistency detected Basically communication DecComm based PD policy initiates communication policies communication different Additionally Roth et al use tell model communication allows subset agents broadcast mes sages Subsequent work addressed question communicate selects valuable subset observations agents observation history instead entire set broadcast message 43 53 Other relevant work communication Several different aspects communication special cases DECPOMDPs studied recent years Roth et al 45 proposed algorithm generate decentralized policies minimal communication factored DECMDPs It employs techniques solving factored MDPs generate centralized freecommunication plan team oﬄine At runtime agent executes factored policy traversing policy tree choosing branches according values state variables encounters reaches action leaf Communication needed facilitate execution portions policy contextspeciﬁc independence It utilizes query communication model agent asks teammates information needed Spaan Melo 51 introduced interactiondriven Markov games IDMGs assuming communication occurs interaction states It explicitly distinguishes situations agents interact situations act independently In noninteraction states agent chooses individual actions based simple heuristic approach completely disregards existence agents In interaction states agent communicates current individual state agents Then agent computes speciﬁc Nash equilibrium corresponding matrix game chooses actions accordingly Communication assumed unlimited noisefree Williamson et al 59 introduced dec_POMDP_Valued_Com model adds special communication reward func tion DECPOMDPs The impact communication measured KL Divergence difference information agents belief state communication They extended online approach called Real Time Belief Space Search RTBSS generate policy Early work RTBSS relied extensive domain knowledge encode explicitly agents coordinate assuming communication parallel activity actions 39 The online ap proach Williamson et al 59 similar DecComm algorithm algorithm uses local observations update joint belief states This work relies handtuned parameter value communication weighted sum communication reward original reward To overcome RS_dec_POMDP model approx imates valuation shaping reward based belief divergence 60 Like dec_POMDP_Valued_Com model RS_dec_POMDP model requires domaindependent communication reward function provided designer addon general DECPOMDP model Most importantly belief update work domaindependent In gen eral DECPOMDPs agents consider joint belief space takes account behaviors agents This joint belief space blows exponentially choices outcomes policies agents agent obtain observation Hence use policybased history merging technique bound memory usage But speciﬁc domains RoboCup rescue possible design adhoc belief monitoring mechanism algorithm maintains single belief state step agents online decisionmaking 59 Oliehoek et al 34 use QBG value function 37 ﬁnd communication policies domains communications onestep delay They model problem Bayesian Games adapt Persues approximate POMDP solver compute QBG value functions More recent work extends approach handle stochastic delays 52 They present model allows communication delayed time steps explicitly considers future probabilities successful communication The Q value function exact communication delays time step In situations delays longer time step agents coordinated decisions based openloop method Unlike work approach consider communicate There ways use communication multiagent systems Stone Veloso 53 utilize low bandwidth communication realtime task decomposition dynamic role assignment Xuan 61 consider com munication DECMDPs agent notices ambiguity plan Shen et al 49 formulate DECMDP twolayer Bayesian network ﬁnd nearoptimal communication strategy problems given structure Goldman et al 24 address problem potential misbehavior resulting misinterpretation messages F Wu et al Artiﬁcial Intelligence 175 2011 487511 509 exchanged establish formal framework identify collection properties allow agents interpret oth ers communicating Some researchers managed learn communication policies reinforcement learning settings require complete model known 2354 Most approaches focus fullyobservable multiagent domains based reinforcement learning relies trials 16 The major beneﬁt com munication improve coordination independent learner different models online 6 Conclusions We present new online algorithm planning uncertainty multiagent settings bounded communication The algorithm addresses key challenge keeping team agents coordinated maintaining shared pool histo ries allows agents choose local actions detect inconsistency arises The algorithm important advantages First use communication selectively recover inconsistency It delay communication resource available needed avoid communication long period time A second advantage scalability The algorithm solve existing benchmark problems faster best oﬄine algorithms solve larger problems scope oﬄine DECPOMDP planners Finally algorithm performs practice outperforming best existing online method producing better value communication The performance online planning highly dependent quality value function guides behavior agents interacting environment The optimal value function available execution time calculating computation time solving entire problem But approximated heuristic functions Qvalue function underlying MDP The effectiveness heuristics domain dependent In multiagent settings challenging ﬁnd heuristics account interdependence agents That actions agent affect observations agents This implicit form communication critical coordination How capture type information heuristic function important interesting research direction multiagent online planning As discussed earlier intractable possible histories team Therefore introduce technique bounding usage memory maintaining belief pool Obviously bounding memory introduce error pool making pool inconsistent real situation To best knowledge ﬁrst address type inconsistency explicit communication agents The algorithm monitors belief pool step refreshes pool sharing private information agents inconsistency detected agents It worth mentioning effectiveness detection method depends precision agents observations We verify belief pool agents local observation step If observation noisy hard tell detected inconsistency problems belief pool merely uncertainties local observation In applications sensor data accurate detect inconsistencies belief pool If agents silent exceeding usage communication resources In paper use communication share local information agents Another interesting type communication negotiate teams policy agents This type negotiation nontrivial scope paper More broadly paper tries draw attention AI community online methods viable alternative multiagent decisiontheoretical planning We online planning limited communication perform taking time Surprisingly work topic sparse 21224344 Our work contributes literature presenting complete framework multiagent online planning new methods bounding usage memory communication resources explores promising research directions planning learning multiagent systems Acknowledgements Special thanks Maayan Roth sharing source code DecComm reviewers helpful feedback suggestions This work supported China Scholarship Council Air Force Oﬃce Scientiﬁc Research Grant No FA95500810181 National Science Foundation Grant No IIS0812149 National Science Foundation China Grant No 60745002 National Hightech Project China Grant No 2008AA01Z150 References 1 I Akyildiz W Su Y Sankarasubramaniam E Cayirci Wireless sensor networks survey Computer Networks 38 4 2002 393422 2 C Amato DS Bernstein S Zilberstein Optimizing memorybounded controllers decentralized POMDPs Proceedings 23rd Conference Uncertainty Artiﬁcial Intelligence 2007 pp 18 3 C Amato JS Dibangoye S Zilberstein Incremental policy generation ﬁnitehorizon DECPOMDPs Proceedings 19th International Con ference Automated Planning Scheduling 2009 pp 29 4 C Amato S Zilberstein Achieving goals decentralized POMDPs Proceedings 8th International Joint Conference Autonomous Agents MultiAgent Systems 2009 pp 593600 5 R Aras A Dutech F Charpillet Mixed integer linear programming exact ﬁnitehorizon planning decentralized POMDPs Proceedings 17th International Conference Automated Planning Scheduling 2007 pp 1825 6 R Becker A Carlin V Lesser S Zilberstein Analyzing myopic approaches multiagent communication Computational Intelligence 25 1 2009 3150 510 F Wu et al Artiﬁcial Intelligence 175 2011 487511 7 R Becker VR Lesser S Zilberstein Analyzing myopic approaches multiagent communication Proceedings 2005 IEEEWICACM Interna tional Conference Intelligent Agent Technology 2005 pp 550557 8 R Becker S Zilberstein VR Lesser Decentralized Markov decision processes eventdriven interactions Proceedings 3rd International Joint Conference Autonomous Agents MultiAgent Systems 2004 pp 302309 9 R Becker S Zilberstein VR Lesser CV Goldman Solving transition independent decentralized Markov decision processes Journal Artiﬁcial Intelli gence Research 22 2004 423455 10 DS Bernstein C Amato EA Hansen S Zilberstein Policy iteration decentralized control Markov decision processes Journal Artiﬁcial Intelli gence Research 34 2009 89132 11 DS Bernstein EA Hansen S Zilberstein Bounded policy iteration decentralized POMDPs Proceedings 19th International Joint Conference Artiﬁcial Intelligence 2005 pp 12871292 12 DS Bernstein S Zilberstein N Immerman The complexity decentralized control Markov decision processes Proceedings 16th Confer ence Uncertainty Artiﬁcial Intelligence 2000 pp 3237 13 A Beynier A Mouaddib A polynomial algorithm decentralized Markov decision processes temporal constraints Proceedings 4th International Joint Conference Autonomous Agents Multiagent Systems 2005 pp 963969 14 A Beynier A Mouaddib An iterative algorithm solving constrained decentralized Markov decision processes Proceedings 21st National Conference Artiﬁcial Intelligence 2006 pp 10891094 15 C Boutilier Planning learning coordination multiagent decision processes Proceedings 6th Conference Theoretical Aspects Rationality Knowledge 1996 pp 195210 16 L Busoniu R Babuska BD Schutter A comprehensive survey multiagent reinforcement learning IEEE Transactions Systems Man Cybernet ics Part C Applications Reviews 38 2 2008 156172 17 A Carlin S Zilberstein Valuebased observation compression DECPOMDPs Proceedings 7th International Joint Conference Au tonomous Agents MultiAgent Systems 2008 pp 501508 18 A Carlin S Zilberstein Myopic nonmyopic communication partial observability Proceedings 2009 IEEEWICACM International Conference Intelligent Agent Technology 2009 pp 331338 19 JS Dibangoye A Mouaddib B Chaibdraa Pointbased incremental pruning heuristic solving ﬁnitehorizon DECPOMDPs Proceedings 8th International Joint Conference Autonomous Agents MultiAgent Systems 2009 pp 569576 20 R EmeryMontemerlo Gametheoretic control robot teams Doctoral Dissertation Robotics Institute Carnegie Mellon University August 2005 21 R EmeryMontemerlo GJ Gordon JG Schneider S Thrun Approximate solutions partially observable stochastic games common payoffs Proceedings 3rd International Joint Conference Autonomous Agents MultiAgent Systems 2004 pp 136143 22 R EmeryMontemerlo GJ Gordon JG Schneider S Thrun Game theoretic control robot teams Proceedings 2005 IEEE International Conference Robotics Automation 2005 pp 11631169 23 M Ghavamzadeh S Mahadevan Learning communicate act hierarchical reinforcement learning Proceedings 3rd International Joint Conference Autonomous Agents MultiAgent Systems 2004 pp 11141121 24 CV Goldman M Allen S Zilberstein Learning communicate decentralized environment Autonomous Agents MultiAgent Systems 15 1 2007 4790 25 CV Goldman S Zilberstein Optimizing information exchange cooperative multiagent systems Proceedings 2nd International Joint Conference Autonomous Agents MultiAgent Systems 2003 pp 137144 26 CV Goldman S Zilberstein Decentralized control cooperative systems Categorization complexity analysis Journal Artiﬁcial Intelligence Research 22 2004 143174 27 EA Hansen DS Bernstein S Zilberstein Dynamic programming partially observable stochastic games Proceedings 19th National Con ference Artiﬁcial Intelligence 2004 pp 709715 28 ML Littman AR Cassandra LP Kaelbling Learning policies partially observable environments Scaling Proceedings 12th International Conference Machine Learning 1995 pp 362370 29 J Marecki M Tambe On opportunistic techniques solving decentralized Markov decision processes temporal constraints Proceedings 6th International Joint Conference Autonomous Agents Multiagent Systems 2007 pp 825832 30 AC Morris D Ferguson Z Omohundro D Bradley D Silver C Baker S Thayer W Whittaker WRL Whittaker Recent developments subterranean robotics Journal Field Robotics 23 1 2006 3557 31 R Nair M Tambe M Roth M Yokoo Communication improving policy computation distributed POMDPs Proceedings 3rd International Joint Conference Autonomous Agents Multiagent Systems 2004 pp 10981105 32 R Nair M Tambe M Yokoo DV Pynadath S Marsella Taming decentralized POMDPs Towards eﬃcient policy computation multiagent settings Proceedings 18th International Joint Conference Artiﬁcial Intelligence 2003 pp 705711 33 R Nair P Varakantham M Tambe M Yokoo Networked distributed POMDPs A synthesis distributed constraint optimization POMDPs Proceedings 20th National Conference Artiﬁcial Intelligence 2005 pp 133139 34 FA Oliehoek MTJ Spaan N Vlassis DecPOMDPs delayed communication The 2nd Workshop Multiagent Sequential DecisionMaking Uncertain Domains 2007 35 FA Oliehoek MTJ Spaan N Vlassis Optimal approximate qvalue functions decentralized POMDPs Journal Artiﬁcial Intelligence Re search 32 2008 289353 36 FA Oliehoek MTJ Spaan S Whiteson NA Vlassis Exploiting locality interaction factored DecPOMDPs Proceedings 7th International Joint Conference Autonomous Agents Multiagent Systems 2008 pp 517524 37 FA Oliehoek N Vlassis Qvalue functions decentralized POMDPs Proceedings 6th International Joint Conference Autonomous Agents Multiagent Systems 2007 pp 833840 38 FA Oliehoek S Whiteson MTJ Spaan Lossless clustering histories decentralized POMDPs Proceedings 8th International Joint Confer ence Autonomous Agents MultiAgent Systems 2009 pp 577584 39 S Paquet L Tobin B Chaibdraa An online POMDP algorithm complex multiagent environments Proceedings 4th International Joint Conference Autonomous Agents MultiAgent Systems 2005 pp 970977 40 L Peshkin KE Kim N Meuleau LP Kaelbling Learning cooperate policy search Proceedings 16th Conference Uncertainty Artiﬁcial Intelligence 2000 pp 489496 41 DV Pynadath M Tambe The communicative multiagent team decision problem Analyzing teamwork theories models Journal Artiﬁcial Intel ligence Research 16 2002 389423 42 M Roth Executiontime communication decisions coordination multiagent teams PhD thesis The Robotics Institute Carnegie Mellon Univer sity 2007 43 M Roth R Simmons M Veloso What communicate Executiontime decision multiagent POMDPs Proceedings 8th International Symposium Distributed Autonomous Robotic Systems 2006 44 M Roth RG Simmons MM Veloso Reasoning joint beliefs executiontime communication decisions Proceedings 4th International Joint Conference Autonomous Agents Multiagent Systems 2005 pp 786793 F Wu et al Artiﬁcial Intelligence 175 2011 487511 511 45 M Roth RG Simmons MM Veloso Exploiting factored representations decentralized execution multiagent teams Proceedings 6th International Joint Conference Autonomous Agents Multiagent Systems 2007 pp 457463 46 S Seuken S Zilberstein Improved memorybounded dynamic programming decentralized POMDPs Proceedings 23rd Conference Uncertainty Artiﬁcial Intelligence 2007 pp 344351 47 S Seuken S Zilberstein Memorybounded dynamic programming DECPOMDPs Proceedings 20th International Joint Conference Artiﬁcial Intelligence 2007 pp 20092015 48 S Seuken S Zilberstein Formal models algorithms decentralized decision making uncertainty Journal Autonomous Agents MultiAgent Systems 17 2 2008 190250 49 J Shen VR Lesser N Carver Minimizing communication cost distributed bayesian network decentralized mdp Proceedings 2nd International Joint Conference Autonomous Agents MultiAgent Systems 2003 pp 678685 50 MTJ Spaan GJ Gordon N Vlassis Decentralized planning uncertainty teams communicating agents Proceedings 5th Interna tional Joint Conference Autonomous Agents Multiagent Systems 2006 pp 249256 51 MTJ Spaan FS Melo Interactiondriven Markov games decentralized multiagent planning uncertainty Proceedings 7th Interna tional Joint Conference Autonomous Agents Multiagent Systems 2008 pp 525532 52 MTJ Spaan FA Oliehoek N Vlassis Multiagent planning uncertainty stochastic communication delays Proceedings 18th International Conference Automated Planning Scheduling 2008 pp 338345 53 P Stone MM Veloso Task decomposition dynamic role assignment lowbandwidth communication realtime strategic teamwork Artiﬁcial Intelligence 110 2 1999 241273 54 D Szer F Charpillet Improving coordination communication multiagent reinforcement learning Proceedings 6th IEEE International Conference Tools Artiﬁcial Intelligence 2004 pp 436440 55 D Szer F Charpillet Pointbased dynamic programming DECPOMDPs Proceedings 21st National Conference Artiﬁcial Intelligence 2006 pp 12331238 56 D Szer F Charpillet S Zilberstein Maa A heuristic search algorithm solving decentralized POMDPs Proceedings 21st Conference Uncertainty Artiﬁcial Intelligence 2005 pp 576590 57 M Tambe Towards ﬂexible teamwork Journal Artiﬁcial Intelligence Research 7 1997 83124 58 J Tsitsiklis M Athans On complexity decentralized decision making detection problems IEEE Transaction Automatic Control 30 1985 440446 59 SA Williamson EH Gerding NR Jennings A principled information valuation communication multiagent coordination The 3rd Work shop Multiagent Sequential DecisionMaking Uncertain Domains 2008 60 SA Williamson EH Gerding NR Jennings Reward shaping valuing communications multiagent coordination Proceedings 8th International Joint Conference Autonomous Agents Multiagent Systems 2009 pp 641648 61 P Xuan V Lesser S Zilberstein Communication decisions multiagent cooperation Model experiments Proceedings 5th International Conference Autonomous Agents 2001 pp 616623