Artiﬁcial Intelligence 173 2009 696721 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Probabilistic planning clear preferences missing information Maxim Likhachev Anthony Stentz b Computer Information Science University Pennsylvania Philadelphia PA USA b The Robotics Institute Carnegie Mellon University Pittsburgh PA USA r t c l e n f o b s t r c t Article history Received 8 October 2007 Received revised form 27 October 2008 Accepted 29 October 2008 Available online 25 November 2008 Keywords Planning uncertainty Planning missing information Partially Observable Markov Decision Processes Planning Heuristic search For realworld problems environments time planning partially known For example robots navigate partiallyknown terrains planes scheduled changing weather conditions car routeﬁnders ﬁgure paths partial knowledge traﬃc congestions While general decisiontheoretic planning takes account uncertainty environment hard scale large problems problems exhibit special property clearly identify best called clearly preferred values variables represent unknowns environment For example robot navigation problem preferred ﬁnd initially unknown location traversable plane scheduling problem preferred weather remain good ﬂying weather routeﬁnding problem preferred road clear traﬃc It turns existence clear preferences construct eﬃcient planner called PPCP Probabilistic Planning Clear Preferences solves planning problems running series deterministic low dimensional Alike searches In paper formally deﬁne notion clear preferences missing information present PPCP algorithm extensive theoretical analysis useful extensions optimizations algorithm demonstrate usefulness PPCP applications robotics The theoretical analysis shows converged plan returned PPCP guaranteed optimal certain conditions The experimental analysis shows running series fast lowdimensional searches turns faster solving problem memory requirements lower deterministic searches orders magnitude faster probabilistic planning 2008 Elsevier BV All rights reserved 1 Introduction A common source uncertainty planning problems lack information environment A robot know traversability terrain traverse air traﬃc management able forecast certainty future weather conditions car routeﬁnder able predict future traﬃc congestions sure present traﬃc conditions shopping planner know particular item sale stores considers Ideally situations produce plan planner needs reason probability distribution possible instances environment Such planning known hard 12 Corresponding author Email address maximlseasupennedu M Likhachev 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200810014 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 697 For problems clearly best values variables represent unknowns environment We values clearly preferred values Thus robot navigation problem preferred ﬁnd initially unknown location traversable In air traﬃc management problem preferred good ﬂying weather In problem route planning partiallyknown traﬃc conditions preferred ﬁnd traﬃc road And ﬁnally shopping planning example preferred store hold sale item These believe large class planning problems exhibit clear preferences missing information One reasons knowledge clear preferences missing information knowledge best action state value optimal policy Instead know intuitive level best event traﬃc congestion sale independently choose use event All outcomes hand preference This intuitive information planning In paper present algorithm called PPCP Probabilistic Planning Clear Preferences able scale large problems exploiting fact preferences exist PPCP constructs reﬁnes plan running series deterministic Alike searches Furthermore making approximating assumption necessary retain information variables values discovered clearly preferred values PPCP keeps complexity search low independent missing information Each search extremely fast running series fast lowdimensional searches turns faster solving problem memory requirements lower deterministic searches orders magnitude faster probabilistic planning techniques While assumption PPCP makes need hold algorithm converge returned plan guaranteed optimal assumption hold The paper organized follows We ﬁrst brieﬂy A search explain ﬁnd leastcost paths graphs We explain planning problem changes information environment missing In Section 4 introduce notion clear preferences missing information brieﬂy talk problems exhibit In Section 5 explain PPCP algorithm makes use clear preferences The section gives extensive theoretical analysis PPCP includes correctness algorithm complexity results conditions optimality plan returned PPCP In Section 6 paper useful extensions algorithm interleave PPCP planning execution In section optimizations algorithm problems speed factor On experimental Section 7 shows PPCP enabled successfully solve path clearance problem important problem defense robotics The experimental results Section 81 hand evaluate performance PPCP problem robot navigation partiallyknown terrains They environments small solved methods guaranteed converge optimal solution RealTime Dynamic Programming 3 PPCP returns optimal policy faster The results PPCP able scale large costmaps size 500 500 cells environments thousands initially unknown locations The experimental results Section 82 hand PPCP solve large instances path clearance problem results substantial beneﬁts alternatives We ﬁnally conclude paper short survey related work discussion conclusions 2 Backward A search planning complete information Notations Let ﬁrst consider planning problem represented search path fully known deterministic graph G The fact graph G completely known time planning means missing information domain environment We use S denote state vertex graph terminology graph G State Sstart refers state agent time planning state Sgoal refers desired state agent We use AS represent set actions available agent state S G Each action AS corresponds transition edge graph G state S successor state denoted succS Each transition associated cost cS succS The costs need bounded small positive constant Backward A search The goal shortest path search algorithms A search 4 ﬁnd path S start Sgoal cumulative cost transitions path minimal The PPCP algorithm present paper based running series deterministic searches Each searches modiﬁed backward A searchthe A search searches Sgoal Sstart reversing edges graph In following brieﬂy operation backward A search S Suppose state S G knew cost leastcost path S S goal Let denote cost g Then leastcost path Sstart Sgoal easily followed starting Sstart executing action succS Consequently A search tries compute AS state S arg mina AScS succS g values In particular A maintains gvalues state visited far gS cost best path g far S Sgoal The pseudocode Fig 1 gives simple version backward A In version besta pointers store actions follow paths The code starts setting gSgoal 0 inserting Sgoal OPEN The code repeatedly selects states OPEN expands themexecutes lines 6 9 At point time OPEN set states candidates 698 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 1 gS start OPEN 2 gS goal 0 bestaS goal null 3 insert S goal OPEN priority equal gSgoal heurS goal 4 gS start minSOPENgS heurS 5 6 7 8 9 remove state S minimum priority gS heurS OPEN S succS action S cid6 cS cid6 gS cid6 cid6 S gS bestaS OPEN priority equal gS cid6 cid6 S gS search seen S gS insert S cid6 cS cid6 cid6 heurS cid6 cid6 Fig 1 Backward A search expansion These states new paths Sgoal propagated S predecessors As result expansion state S involves checking path predecessor state S improved path S Sgoal setting gvalue S cost new path b setting action bestaS OPEN The operation makes sure S Sgoal propagated predecessors S considered expansion expanded cost path S cid6 action leads state S c inserting S cid6 cid6 cid6 cid6 cid6 cid6 The goal A expand states order minimize number expansions required guarantee states leastcost paths Sstart Sgoal expanded Backward A expands states order gS heurS heurvalues estimate cost leastcost path S start S The heurvalues overestimate admissible A return suboptimal solution In order state cid6 G expanded heurvalues need consistent heurS start 0 states S S cid6 S cid2 heurS If heurvalues consistent time search S succS expands state S leastcost path S Sgoal better path later state reinserted OPEN Ordering expansions based summation g heurvalues makes search focus expansions states path S start Sgoal looks promising cid6 AS cid6 heurS cid6 cS The search terminates gSstartthe cost best path far Sstart Sgoalis small smallest summation g heur values OPEN Consequently OPEN longer contains states belong paths smaller costs gSstart This means A terminate guarantee path optimal The proof guarantee relies way fact g values states optimal path monotonically decreasing optimal path Sstart Sgoal contains transition S S action cid6 This monotonicity property later paper In particular general case optimal S g g plans domains missing information necessarily exhibit monotonicity state values case clear preferences state values optimal plans monotonic sense This allow use series backward A searches plan S cid6 Example To later explanations clearer let consider trivial planning problem shown Fig 2a Suppose agent needs buy wine cheese stores store A store B Both stores products different prices shown ﬁgure Initially agent home cost traveling home store stores translated money costs shown Fig 2a The planning problem agent purchase wine cheese minimal cost including cost travel return home Fig 2b shows problem represented graph Each state encodes position agent bought Thus Sstart Agent Home Bought Sgoal Agent Home Bought wine cheese Fig 2c shows gvalues heuristics priorities f g heur states computed backward A search ﬁnd leastcost path The path shown thicker lines The states expanded A shown grey For state S heuristic heurS cost moving home store agent state S plus cost purchasing items bought state S assuming price minimum possible price stores remember search backward heuristics estimate cost leastcost path start state state question Thus optimal plan agent store A buy cheese store B buy wine return home 3 Planning missing information In example graph G represents planning problem edge costs fully known By planning missing information hand refer case outcomes actions andor edge costs known time planning In particular hidden variables status affects outcomes andor costs certain actions The status hidden variables unknown agent time planning Instead agent probability distribution belief possible values hidden variables During execution agent sense hidden variables certain states Once senses actual values hidden variables known By sensing refer action results knowing hidden variable Sometimes explicit sensing action seeing region robot traversable In M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 699 Fig 2 Simple example planning complete information 700 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 cases value hidden variable deduced outcome action trying pick object realizing heavy picked single robot arm In case assume value hidden variable controls outcome action known action executed In terms explicit sensing corresponds assuming perfect sensing For example consider variation grocery shopping problem described The variation shown Fig 3a store A conduct 50 sale wine products store B conduct 50 sale cheese products The agent know sales actually happening estimates probability having sale store A 40 probability having sale store B 80 This modiﬁed version problem corresponds problem planning missing information underlying state includes additional boolean variables indicating sale corresponding store Let denote variables Sale A SaleB We use Sale A 1 0 mean store A doesnt sale wine similar notation SaleB The problem differs original deterministic planning agent know status variables Sale A SaleB visits stores A B respectively At time problem related narrower planning Partially Observable Markov Decision Problems POMDPs following assumptions Perfect sensing There noise sensing agent visits store knows sale Deterministic actions There uncertainty outcomes actions values hidden variables known agent moves deterministically cost purchase known status Sale A SaleB variables known Despite restrictions explain problem planning missing information notation termi nology POMDPs similar 5 Belief MDP formulation In POMDPs agent know state Instead probability distribution possible states A belief state particular value distribution dimensionality distribution high N 1 N number possible states original graph G Let denote belief states X For instance grocery shopping example initial belief state agent Xstart Agent Home Bought P Sale A 1 04 P SaleB 1 08 Note concisely represents probability distri bution possible states position agent items purchased known probability distribution possible store sales represented probability sale store assuming store sales independent events Fig 3b shows corresponding belief statespace The notation Sale A u SaleB u represents P Sale A 1 04 P SaleB 1 08 respectively The notation Sale A 0 Sale A 1 hand represents knowledge agent sale sale store A In words Sale A 0 equivalent belief P Sale A 1 0 Similar notation SaleB The belief statespace Markov Decision Process MDP After action agent knows precisely belief state Some actions probabilistic The outcomes actions depend actual status corre sponding hidden variables Essentially actions incorporate sensing hidden variables The probability outcomes actions follows probability distribution hidden variables sensed Because assume sensing perfect subgraphs result different outcomes single stochastic action disjoint This implies optimal policy belief statespace acyclic Let explicitly split X sets discrete ﬁniterange variables S X H X X S X H X S X set variables values observed These variables deﬁne state original graph G Fig 2 S X thought projection X statespace deﬁned completely observed variables H X set hidden variables represent missing information environment So example S X location agent agent purchased far H X status variables Sale A SaleB We use hi X denote ith variable H X The goal planner construct policy reaches state X S X S goal Agent Home Bought wine cheese minimizing expected cost execution Fig 3c shows policy example A policy single path actions probabilistic policy dictates agent state end execution According policy agent visit store A sale store A store B return store A sale store B This different single path case planning complete information Fig 2c Finding policy good quality diﬃcult reasons ﬁrst belief statespace probabilistic deterministic planners A typically apply second importantly size belief state space exponential number hidden variables More speciﬁcally given X S X H X size belief statespace roughly number states original graph G times number possible beliefs hidden variables In example 32 possible beliefsunknown 1 0about hidden variables Sale A SaleB M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 701 Fig 3 An example planning missing incomplete information 702 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 4 Clear preferences missing information In section introduce notion clear preferences This notion central idea PPCP We ﬁrst deﬁne notations assumptions For sake simplicity notation hi X u state X represent fact value hi unknown X If hi X cid8 u hand actual value hi known X sensing perfect Assuming hidden variable action We assume time action uncertainty outcomes uncertainty costs action involves sensing hidden variable uncertainty single hidden variable Thus execution single action result deducing value hidden variable In domains case factors control outcome action combined single hidden variable We allow single hidden variable control action For example example single hidden variable Sale 1 imply sale stores Denoting hidden variables We use h S Xa represent hidden variable controls outcomes costs action taken S X By h S Xa null denote case uncertainty outcome action taken state X In words variables H control outcomes executed S X underlying environment deterministic outcome Thus example ac tion moving store A home hAgentStoreABoughtwinecheeseamove null outcome depend values hidden variables The action moving store A home hand hAgentHomeBoughtamovetostoreA u agent enters store A ﬁnds sale Therefore action executed Xstart Agent Home Bought Sale A u SaleB u possible outcomes X1 Agent StoreA Bought Sale A 1 SaleB u X3 Agent StoreA Bought Sale A 0 SaleB u Denoting successors Sometimes need refer set successors belief statespace In cases use notation succ X denote set belief states Y SY succS X HY H X hS XaY known unknown X remains The function P XaY probability distribution outcomes action executed state X follows probability distribution h S Xa P h S Xa As mentioned action executed state X actual value h S Xa deduced assumed sensing perfect environment deterministic Thus example Xstart Agent Home Bought Sale A u SaleB u movetostoreA X1 Agent StoreA Bought Sale A 1 SaleB u P Xstarta X1 04 Assuming independence hidden variables PPCP assumes variables H considered independent In words sale event store A independent sale event store B Assuming clear preferences The main assumption PPCP makes clear preferences values hidden variables available It requires variable hi H given preferred value denoted b best This value deﬁned follows Deﬁnition 1 A clearly preferred value b hidden variable h S Xa value given state X action cid6 b h S Xa known h S Xa X u exists successor state X satisﬁes following cid2 h S Xa X cid6 X cid6 argmin Y succ Xa c cid3 S X SY v Y v Y expected cost executing optimal policy state Y We use notation succ X ab best successor denote state X h S Xa X u h S Xa X h S Xa X cid8 u There simply outcomes action executed X cid6 h S Xa X In case possible h S Xa X cid6 h S Xa X cid6 b cid6 cid8 b Our grocery shopping example satisﬁes property sensing action exist outcomes sale present preferred outcome Thus shown Fig 4 action movetostoreA executed Xstart Agent Home Bought Sale A u SaleB u preferred outcome succ X ab X1 Agent Fig 4 An example clear preferences better worse sale M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 703 StoreA Bought Sale A 1 SaleB u It trivial If X3 second outcome action X3 X1 X3 corresponds Sale A 0 outcome cS Xstart S X1 cS Xstart S X3 v exactly states difference X1 sale event store A X3 X1 cid3 v We believe wide range problems exhibit clear preferences People predict outcomes optimize coststogoal For example planning car route person clearly prefer single road free traﬃc Similarly choosing sequence connecting ﬂights person clearly prefer weather good ﬂying weather planes delays These preferences determined computing optimal values One frequent reasons fact particular value hidden variable commit agent particular action The agent free choose route sequence planes The requirement optimal plan case traﬃc good weather plane delays worse corresponding optimal plan case traﬃc bad weather plane delays Obviously wide range problems exhibit clear preferences For example case long ﬂight delay nonzero probability air carrier provide customers sort compensation ﬁrstclass upgrade free ticket In case clear person heshe prefers ﬂight ontime delayed The actual preference depends penalties incurred person ﬂight delayed missing connecting ﬂights dinners computed expected costs optimal plans outcomes computed Apart compensation nonobvious factors typically clearly preferred nondelayed ﬂight person ﬂying nondelayed ﬂight freedom stay arrival airport length possible delay optimal leave airport later Clear preferences capture fact cost optimal plan nondelayed ﬂight worse cost optimal plan delayed ﬂight These preferences known computing costs actual optimal plans 5 The PPCP algorithm The explanation algorithm split steps In ﬁrst section present planner constructs provably optimal policy series Alike searches belief statespace It gains eﬃciency search deterministic search use heuristics focus efforts Nevertheless search expensive size belief statespace exponential number hidden variables In following section planner extended use series searches underlying environment instead These resulting searches exponentially faster independent number hidden variables The overall solution guaranteed optimal certain conditions 51 Optimal planning repeated searches The algorithm works executing series deterministic searches We ﬁrst deterministic search works main function algorithm uses searches construct ﬁrst policy reﬁne The function deterministic search called ComputePath shown Fig 5 The search belief statespace It similar backward A It computes gvalues states uses heuristic values focus search performs repeated expansions states order gvalue plus heuristic known f values The meaning gvalues criterion solution minimizes somewhat different ones A search X minimum expected cost reaching goal state These estimates provided search function Main algorithm estimates cid6 0 associated It calculated nonnegative Then stateaction pair X cid6 gvalue action costs cS X preferred outcome state Y b succ X cid6 SY value estimates vY outcome states Y succ X Suppose state X estimate v X v cid6 deﬁned follows cid6 value Q vg X AS X cid6 cid6 cid3 SY vY c cid2 cid6 S X S cid2 cid3cid3 cid3cid3 cid2 Y b g Y b 1 Q vg X cid6 cid4 Y succ X cid6a cid6 ab Q vg X cid2 c cid2 S X P X cid6aY max To understand meaning equation consider ﬁrst standard deﬁnition undiscounted Q value action 6 terms costs rewards Q X cid6 cid4 P X cid6aY cid2 cid2 c S X cid6 cid3 SY cid3 vY Y succ X cid6a 2 cid6 gives expected cost optimal policy starts state X In words Q value expectation sum immediate cost plus value outcome values execution action If v cid6 underestimate Now consider Y Y vvalues possible outcomes action executed state X Q X values underestimates v deﬁnition clear preferences Deﬁnition 1 According cS X successor Y succ X If vvalues perfect estimates equal corresponding v cid6 cid6 This implies cS X values computed Q X cid6 SY b vY b cid3 cS X cid6 SY b v cid6 SY v cid6 SY v Y b cid3 cS X cid6 704 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 underestimates vY b cid3 v improve Q value estimate Eq 2 vvalue preferred outcome Y b Consequently current vvalues imperfect potentially Q vv X cid6 cid4 Y succ X cid6a P X cid6aY max cid2 cid2 c S X cid6 cid3 SY vY c cid2 cid6 S X S cid3cid3 cid2 Y b cid3cid3 cid2 Y b v 3 Finally ComputePath function computes gvalues states The property guarantees com puted gvalue underestimate corresponding v value These gvalues improvements previous vvalues think state values series specially ordered backups Thus instead vY b ComputePath function uses newly computed gvalue gY b This exactly Eq 1 It straightforward provided vvalues equal corresponding v values gY b Y b Eqs 1 2 identical As result plan minimum expected cost reaching equal v goal given simple strategy picking action smallest Q vg X current state X values ﬁrst Nevertheless search com values solution following ﬁxpoint equation In reality vvalues necessarily equal corresponding v putes gvalues based Q vg values In particular let deﬁne g cid5 g X 0 mina AS X Q vg X S X Sgoal 4 These g values expected costs optimal plans assumption vvalues nonpreferred outcomes expected costs optimal plans words vvalues nonpreferred outcomes assumed perfect estimates The gvalues computed search ComputePath function estimates g values In fact shown gvalues states expanded search exactly equal corresponding g values Theorem 3 Also max operator Eq 1 fact costs positive ﬁnite Q vg X strictly larger g values optimal ﬁnitecost paths monotonically decreasing g g cid6 arg mina AS X cid6 Q vg X gvalues sets gvalues relevant states corresponding g cid6 cid6 ab independently vvalues correct estimates This means cid6b X cid6 This monotonicity allows perform deterministic Alike search computes cid6 Q vg X succ X succ X cid6 g values cid6 cid6 The ComputePath function shown Fig 5 searches backwards belief statespace goal states state Xp called It important remember number goal states belief statespace exponential number hidden variables goal state state X S X S goal The trajectory Initially vvalues states need nonnegative smaller equal costs leastcost trajectories goal assumption hidden variables set clearly preferred values 1 procedure ComputePath Xp 2 g Xp OPEN 3 H element hi satisﬁes cid7 cid6 hi b hi Xp u OR hi hi Xp hi u X S goal H g X 0 besta X null insert X OPEN g X heur Xp X cid6 cid6 heur Xp X remove X smallest g X heur Xp X OPEN cid6 action state X st X succ X cid6 cid7 hi Xp cid8 u cid6 according formula 1 cid6 g X cid6 cid6 besta X cid6 OPEN priority g X cid6 Q vg X cid6 cid6 heur Xp X cid6 4 5 6 7 whileg Xp min X cid6OPEN g X 8 9 compute Q vg X 10 11 search seen X cid6 Q vg X g X 12 insertupdate X 13 14 procedure UpdateMDP Xpivot 15 X Xpivot 16 S X cid8 S goal v X g X 17 X succ X besta Xb 18 19 procedure Main 20 Xpivot Xstart 21 Xpivot null 22 23 UpdateMDP Xpivot 24 ComputePath Xpivot ﬁnd state X current policy S X cid8 Sgoal v X E X cid6succ Xbesta XcS X besta X S X set Xpivot X set Xpivot null cid6 v X cid6 25 26 Fig 5 Optimal planning repeated searches M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 705 search returns uses transitions correspond deterministic actions preferred outcomes stochastic actions This implemented starting search goal states hidden variables assume unknown preferred values unknown H Xp values equal corresponding hidden variables H Xp lines 36 The ﬁrst time ComputePath called Xp Xstart hidden variables unknown For subsequent calls Xp different state values hidden variables known Just like backward A search ComputePath function expands states order g plus heuristic expansion lines 813 updates gvalue besta pointer predecessor state expanded state The ComputePath function retain gvalues searches The states encountered ﬁrst time speciﬁc function gvalues reset lines 1112 It differs A gvalues computed according formula 1 In fact computation formula requires ComputePath function search backwards forwards Heuristics focus search Since search backward estimate cost following leastcost trajectory Xp state question In pseudocode userprovided function heur Xp X returns heuristic value state X These values need consistent following sense heur Xp Xp 0 state X action AS X heur Xp X cS X Ssucc X ab cid2 heur Xp succ X ab This reduces normal consistency requirement heuristics backward A statespace fully deterministic information missing time planning Section 2 For instance grocery shopping example heuristics planning complete information Section 2 computing heuristics use sale prices This equivalent computing heuristics assumption hidden variables known preferred values The search ﬁnishes soon gvalue Xp larger smallest priority states OPEN A min operator set assumed return inﬁnity The assumption expectation operator line 24 Once ComputePath function exits following holds path Xp goal state constructed picking action besta X state X moving state succ X besta Xb besta X outcome gvalue state trajectory equal g value state The Main function algorithm shown Fig 5 uses fact Starting Xstart repeatedly executes searches states reside current policy deﬁned besta pointers vvalues smaller according vvalues successors policy action line 24 The initial vvalues need nonnegative smaller equal costs leastcost trajectories goal assumption hidden variables equal b simple initialization zero suﬃces In particular values set heuristics admissible respect underlying graph generated values hidden variables set clearly preferred values The Main function terminates state current policy vvalue smaller according vvalues successors After search UpdateMDP function iterates path lines 1618 updates vvalues states path search setting gvalues line 17 As mentioned equal values The vvalues states retained termination algorithm On hand corresponding g increases vvalues guaranteed correct error vvalues states vvalues values long vvalues successors policy On hand g values As result algorithm converges time states policy overestimate v vvalues equal v values policy optimal proof theorems 7 values bounded v Theorem 1 The Main function Fig 5 terminates time expected cost policy deﬁned besta pointers given v Xstart equal minimum expected cost reaching goal Xstart The algorithm remains correct independently states satisfying condition line 24 selected Typically states satisfying condition beneﬁcial select ﬁrst state highest probability reached likely inﬂuence cost policy The probabilities reaching states policy computed single pass states policy starting start state In addition eﬃcient select states following simple optimization demonstrated Section 53 state X chosen Xpivot backtrack X policy encounter ﬁrst stochastic transition Xstart whichever comes ﬁrst point Xpivot chosen outcome transition transition resides branch X 52 Scaling searches Each search version algorithm presented slow operates belief statespace size exponential number hidden variables We actual version PPCP addresses ineﬃciency At high level main idea forget outcomes sensing particular search overall planning This means particular search values hidden variables tracked remain Consequently search performed original graph G underlying environment 706 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 Initially vvalues states need nonnegative smaller equal costs leastcost trajectories goal assumption hidden variables set clearly preferred values 1 procedure ComputePath Xp 2 gS Xp OPEN 3 gS goal 0 bestaS goal null 4 insert S goal OPEN gS goal heurS Xp S goal 5 whilegS Xp minS XOPEN gS X heurS Xp S X 6 7 remove S X smallest gS X heurS Xp S X OPEN cid6 H u Xp ab action S X compute Q vg S X search seen S X cid6 Q vg S X cid6 st S X SsuccS X cid6 according formula 5 cid6 gS X cid6 cid6 Q vg S X cid6 bestaS X cid6 gS X insertupdate S X 8 9 10 11 cid6 OPEN priority gS X cid6 heurS Xp S X cid6 v X gS X v X u gS X besta X bestaS X X succ X besta Xb 12 procedure UpdateMDP Xpivot 13 X Xpivot 14 S X cid8 S goal 15 16 17 procedure Main 18 Xpivot Xstart 19 Xpivot null 20 21 UpdateMDP Xpivot 22 ComputePath Xpivot ﬁnd state X current policy S X cid8 Sgoal v X E X cid6succ Xbesta XcS X besta X S X set Xpivot X set Xpivot null cid6 v X cid6 23 24 Fig 6 PPCP Planning repeated eﬃcient searches costs outcomes modiﬁed reﬂect initial settings hidden variables This graph exponentially smaller belief statespace In words search state consists S X variables size search statespace independent missing information This results drastic increase eﬃciency searches ability solve bigger problems running memory This eﬃciency comes expense optimality guaranteed conditions described later Theorem 7 In brief theorem states optimality guarantees require branch optimal policy executes actions rely hidden variable value clearly preferred value In words executing optimal policy necessary retain information variables values discovered clearly preferred values Suppose agent executes action outcome uncertain belief state X Suppose execution puts agent succ X ab This means agent deduce fact value hidden variable h S Xa represents missing information action b During search assume future agent need execute action action outcome dependent value hidden variable remember value single hidden variable allowed control outcomes action In case need execute action search assumes value hidden variable unknown As result search need remember h S Xa unknown known equal b In fact search needs compute Q vg value stochastic action assumes values hidden variables unknown known nonpreferred values belief state Xp state ComputePath function called Under assumption calculation Q vg Xvalue Eq 1 independent H X hidden variable H X value different variable H Xp equal b replaced u Thus search original graph G exponentially larger belief statespace The search longer needs track H states operates directly S states The ComputePath function shown Fig 6 performs search While function called belief state Xp searches backwards single state Sgoal single state S Xp states search statespace consist variables S The search assumes action outcome It assumes S X outcome cid6 H u Xp ab line 7 H u Xp deﬁned H Xp action executed S X hidden variable equal b set u This corresponds setting deterministic environment cid6a H Xp action executed S X cid6a unknown H Xp The heuristics need consistent known preferred value h S X respect environment set The ComputePath function performs backward A search statespace exception gvalues computed cid6 single outcome corresponding value hidden variable h S X cid6 S X SsuccS X To implement assumption agent memory preferred values hidden variables previously observed need compute Q vg values appropriately For stateaction pair S X AS X ComputePath function computes gvalues based Q vgS X instead Let X u denote belief state S X H u Xp We deﬁne Q vgS X M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 cid2 Q vg S X cid3 Q vg cid2 cid3 g cid2 succ cid2 X u cid3 cid3 b cid2 cid2 cid2 succ S g X u X u cid3cid3 cid3 b 707 5 According formula computation Q value action assume execute action belief state X u unaware hidden variables preferred values In calculation Q vg X u Eq 1 use vvalues corresponding belief states The calculation Q vg X u requires gvalue succ X u ab The search compute gvalues belief states Instead gsucc X u ab substituted Q vg implements assumption agent remember values gSsucc X u ab This computation hidden variables detected b For example grocery shopping problem described Section 3 ComputePath function corresponds search ing original graph G shown Fig 2 The search proceeds Sgoal state Agent Bought variables given S Xp The search graph G prices items sale corresponding values Sale A SaleB variables known 0 H Xp Otherwise prices assumed unknown computa tion gvalues taking expectation according Eq 5 Apart gvalues computed ComputePath functions identical backward A search performed graph G The Main UpdateMDP functions Fig 6 operate exact way exception state X UpdateMDP function updates needs update corresponding belief state X u line 15 Note UpdateMDP function updates actual policy executed Therefore successors besta X depend value hidden variable h S Xa H X necessarily equal set search environment value h S Xa H X u 53 Example We demonstrate operation PPCP algorithm described previous section problem robot navigation partiallyknown terrain example shown Fig 7 At time planning robot cell A4 goal cell F 4 In black shown cells untraversable There cells shaded grey status unknown robot cell B5 E4 For probability containing obstacle 05 In example restrict robot compass directions Whenever robot attempts enter unknown cell assume robot moves cell senses enters free returns The cost 1 cost moving unknown cell sensing returning 2 Fig 7b shows belief statespace robot navigation problem The fully observed X S X location robot hidden X H X status cells E4 B5 For example X4 R B4 E4 u B5 1 R B4 means robot cell B4 E4 u B5 1 means status cell E4 unknown Fig 7 The problem robot navigation partiallyknown terrain 708 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 Fig 8 An example PPCP operation cell B5 known blocked The robot navigation problem exhibits clear preferences sensing action exist outcomes cell blocked unblocked preferred outcome For example suppose robot state X R D4 E4 u B5 u Fig 7c Then action East outcomes Y 1 R D4 E4 1 B5 u Y 2 R E4 E4 0 B5 u The preferred outcome Y 2 succ X ab satisﬁes deﬁnition clear preferences robot E4 D4 cost 1 implying Y 2 1 v Figs 8 9 PPCP solves problem In left columns Figs 8 9 environment ComputePath function sets executed Xp speciﬁed underneath ﬁgure Thus executed Xp R D4 E4 1 B5 u Fig 8e ComputePath function assumes cell E4 blocked cell B5 free We heuristics shown hvalues gvalues path S Xp Sgoal shown grey dashed line computed search The heuristics Manhattan distances summation x y differences S Xp cell question Y 2 cS X SY 1 v Y 2 cS X SY 2 v Y 1 2 v Y 1 cid2 1 v Y 1 cid2 v In right column ﬁgure current policy PPCP UpdateMDP function incorporates results recent search shown left column row The states vvalues smaller supposed according successors outlined bold These states candidates Xpivot search iterations As mentioned earlier exercise following simple optimization M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 709 Fig 9 An example PPCP operation continued example experiments state X chosen Xpivot backtrack X policy encounter ﬁrst stochastic transition Xstart whichever comes ﬁrst point Xpivot chosen outcome transition resides branch X For example Fig 9d Xpivot chosen state R B4 E4 u B5 1 robot B4 cell E4 unknown cell B5 known contain obstacle result backtracking state R D4 E4 u B5 1 In computing Q vg vvalues belief states exist assumed equal Manhattan distances goal These distances initialize vvalues new belief states The difference way ComputePath function computes gvalues way A computes seen Fig 8g There gvalue cell C4 8 1 gD4 The gvalue cell D4 hand 7 despite fact gE4 1 710 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 reason vvalue state R D4 E4 1 B5 u corresponds bad outcome going east cell D4 10 The vvalue state updated previous iteration Fig 8f When computing Q vg according Eq 5 ComputePath function accounts vvalue As result path ComputePath function comes iteration going cell B5 different path produced running normal A assuming cells free In fact path exactly path computed ComputePath function ﬁrst iteration Fig 8c Fig 9h shows policy PPCP returns converges In general expected cost policy bounded cost policy robot forgets outcome sensing preferred outcome If optimal policy require remembering preferred outcomes policy returned PPCP guaranteed optimal In example memoryless property mean search planner assumes soon robot successfully enters cell E4 example ﬁnds free resets status cell E4 unknown cell The nonpreferred outcomes sensing reset robot ﬁnds cell E4 blocked information While example optimal plan need remember status cell robot successfully entered possible set environment suboptimal An optimal policy environment require robot sense cell come use fact cell free later time In Section 61 discuss memoryless assumption PPCP relaxed Independently memoryless property satisﬁed algorithm guaranteed converge ﬁnite time It important emphasize forgetting best outcomes hidden variables happening level search iteration execution ComputePath function The Main function constructs policy states forget Thus Fig 6 Main function calls UpdateMDP function updates policy belief statespace path search iteration This UpdateMDP function iterates path retaining values hidden variables preferred nonpreferred lines 1516 Fig 6 This seen Figs 89 example constructed policy forget preferred outcomes Instead forgetting preferred outcomes happens level search iteration 54 Theoretical properties We present theorems algorithm They supposed sense algorithm converges converges All theorems proofs 7 The ﬁrst theorems relate properties ComputePath function properties backward A search For example following theorem unsurprisingly states execution ComputePath function expands state guarantee A makes Here following comparing A assume heuristics consistent Theorem 2 No state expanded single execution ComputePath function The statement theorem similar property backward A maintains state f value summation gvalue heuristics smaller equal smallest f value states OPEN gvalue equal goal distance cost shortest path state goal While A computes cost shortest path ComputePath function computes cost path takes account vvalues values deﬁned Eq 4 bad outcomes To formally let redeﬁne g states X belief statespace For given vvalues given pivot state Xp g values solution following ﬁxpoint equation cid3 S X values states S X opposed g S X Sgoal 6 cid5 g cid2 0 mina AS X Q vg S X These g values goal distances ComputePath function gvalues computed states equal Theorem 3 Assuming function v nonnegative line 5 Fig 6 state S X heurS X gS X heurS X cid3 gS X cid6 OPEN holds gS X g cid6 heurS X S X cid6 S X Given terminating condition loop ComputePath fact hS Xp 0 heuristics S Xp Also A states consistent clear ComputePath terminates gS Xp g path g plus heurvalues smaller equal gvalue goal search S Xp Therefore values according Theorem 3 The proof going gvalues equal corresponding g theorem uses fact UpdateMDP sets vvalues states path gvalues This theorem shows updating vvalues states trajectory ComputePath makes states consistent vvalues policy successors In words update removes negative Bellman errors path returned ComputePath function M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 711 Theorem 4 Suppose ComputePath function executed state X true 0 cid3 v X cid3 v X u Then UpdateMDP returns holds state X vvalue updated UpdateMDP function holds v X 0 S X Sgoal v X cid2 E X cid6succ Xbesta XcS X besta X S X cid6 cid6 v X For purpose following theorem let deﬁne v u values somewhat similar v values v u X minimum expected cost policy preferred values hidden variables forgotten soon observed v u values upper bounds expected cost policy PPCP returns cid5 v u X 0 mina AS X Q v u v u X u S X Sgoal The theorem says vvalues iteration decrease The reason ComputePath function computes lowest possible gvalues g values states path UpdateMDP function sets vvalues Assuming properly initialized vvalues proof iteration vvalues increase induction executions UpdateMDP functions The theorem proves vvalues bounded corresponding v u values The proof statement induction based values ComputePath function sets gvalues larger v u values long v fact g values exceeding corresponding v u values ComputePath function executed Theorem 5 vvalues states monotonically nondecreasing bounded corresponding v u values After iteration algorithm value state Xp possibly corrected changing besta pointer making increase vvalue The number possible actions ﬁnite The increases hand bounded positive constant belief statespace ﬁnite assumed perfect sensing ﬁnite number possible values hidden variable Therefore algorithm terminates Moreover time termination vvalue state policy smaller expectation immediate cost plus vvalue successors Therefore expected cost policy larger v Xstart This summarized following theorem Theorem 6 PPCP terminates time cost policy deﬁned besta pointers bounded v Xstart turn larger v u Xstart The ﬁnal theorem gives conditions policy PPCP optimal It states policy We use ρ X optimal memory preferred outcomes required optimal policy notated ρ denote pointer action dictated policy ρ belief state X Theorem 7 Suppose exists minimum expected cost policy ρ holds h S Xρ X X cid8 b Then policy deﬁned besta pointers time PPCP terminates minimum expected cost policy satisﬁes following condition state X ρ The condition h S Xρ X X cid8 b means agent executes policy action ρ X state X hidden variable controls outcomes action known preferred outcome hidden variable controls outcomes uncertainty outcome action If property holds action policy need agent retain information values hidden variables observed clearly preferred values Typically means knowledge hidden variables exercised immediately need remember anymore Another way stating memoryless property PPCP branch optimal policy executes actions rely hidden variable assume value clearly preferred value PPCP guaranteed ﬁnd optimal policy 6 Extensions optimizations In following ﬁrst sections useful extensions PPCP algorithm The ﬁrst extension makes PPCP applicable problems acceptable policy exists forgets preferred outcomes sensing The second extension makes PPCP useable realtime systems making possible interleave planning PPCP execution The sections general optimizations PPCP algorithm prove effective path clearance problem Both optimizations leave theoretical properties algorithm convergence optimality certain conditions unchanged considered general optimizations PPCP 61 Overcoming memoryless property Each search PPCP assumes policy need remember outcomes sensing preferred outcomes In Fig 8 example instance robot senses cell trying enter If free robot 712 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 enters need remember free future path involve entering cell It needs remember cells turn blocked policies generated PPCP retain information There problems require remembering preferred outcomes simple approaches relaxing memoryless property PPCP Before trying ﬁrst thing sure sensing action action requires knowledge status hidden variable This essentially set robot navigation problem Fig 8 The robot senses cell enters backs single action Therefore preferred outcome sensing action sensing need remembered A slightly complex solution problem augment state S X k preferred outcomes sensing During search ComputePath function sensing operation results preferred outcome corresponding hidden variable pushed queue maximum size k Thereafter sensing need value hidden variable long remains queue pushed new results sensing For example robot navigation problem S X consist location robot plus k locations sensed robot The addition k preferred outcomes sensing makes execution ComputePath function expensive In particular size statespace search grows roughly factor Hk H number hidden variables Therefore k set small number complexity search low This approach good problems results recent sensing likely useful If addition k preferred outcomes sensing suﬃcient split vector hidden variables H X sets H X H X The ﬁrst hidden variables preferred outcomes Com putePath function track pseudocode ComputePath function Fig 5 The second set hidden variables H X ones preferred outcomes ComputePath function retain pseudocode ComputePath function Fig 6 The version PPCP uses approach generalizes PPCP given Sections 51 52 The pseudocode generalized version given 7 In fact version proofs derived The approach splitting H X sets suitable problems good idea preferred outcomes likely useful later 62 Interleaving planning execution In cases like able interleave planning execution The agent start executing current plan executing planner work improving plan This way agent need wait planner fully converge Interleaving planning PPCP execution follows The agent ﬁrst executes PPCP seconds The loop Main function PPCP suspended right UpdateMDP function returns line 21 Fig 6 agent starts following policy currently PPCP given besta pointers During agent Xstart state maintained PPCP updated current state agent main loop PPCP resumed seconds After suspended policy agent currently follows compared policy PPCP currently updated higher probability reaching goal location If policy agent currently follows higher probability reaching goal agent continues follow This way avoid changing policies time PPCP decides explore different policy explored outcomes The probabilities reaching goal acyclic policy computed single pass states policy topological order starting start state Once PPCP converges ﬁnal policy agent follow policy reexecuting PPCP agent deviates path actuation errors If agent deviate signiﬁcantly plan generated PPCP PPCP replan There need replan scratch Instead Xstart updated new state agent old policy discarded main loop PPCP resumed Note values state variables PPCP preserved It automatically reuse ﬁnd new policy current agent position faster PPCP reexecuted scratch 63 Reducing number search iterations As mentioned previously search ComputePath function encounters action executed state S X outcome known according H Xp evaluating Eq 1 ComputePath function uses vvalues nonpreferred outcomes The vvalues estimates goal distances If nonpreferred outcomes explored PPCP vvalues initial estimates costtogoal likely lower This means ComputePath function return path uses state action pair S X future iterations PPCP ﬁnd vvalue nonpreferred outcomes higher stateaction pair avoided Fig 10 gives example robot navigation partiallyknown environment problem When solving en vironment Fig 10a PPCP point invokes ComputePath function state Xp R A4 C 4 1 C 6 u M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 713 Fig 10 The comparison search PPCP b optimization vs c optimization described Section 63 Suppose time PPCP computed vvalue state R B6 C 4 1 C 6 1 12 This cost getting goal cell B6 cells C4 C6 blocked During current search comput ing gvalue cell C5 PPCP query vvalue state R C 5 C 4 1 C 6 1 needed Eq 1 If vvalue state computed previously PPCP initializes admissible estimate Man hattan distance C5 goal cell 4 Fig 10b After evaluating Eq 1 gvalue cell C5 6 05 max2 4 1 5 05 max1 5 1 5 Consequently search returns path shown Fig 10b goes cells C5 C6 One optimization propose use vvalues neighboring states obtain informative vvalues states explored Thus example deduce vvalue state R C 5 C 4 1 C 6 1 10 vvalue state R B6 C 4 1 C 6 1 12 minus upper bound minimum cost getting R B6 C 4 1 C 6 1 state R C 5 C 4 1 C 6 1 easily compute 2 More formally suppose interested estimating vvalue state X We small region R states X H H X Using state Y R upper bound cuY X getting state Y state X estimate v X v X max Y R cid2 cid3 vY cuY X 7 The upper bounds cu X computed single backward DepthFirst Search X In problems obtained priori To Eq 7 valid update v X consider following trivial proof v X remains admissible overestimate minimum expected cost getting goal provided admissible value vY Y R Let v Y denote minimum expected cost getting goal Y Then vY cid3 v Y cid3 cuY X v X X bounded vY cuY X setting v X guarantee admissibility v X Thus v The change algorithm Fig 6 line 15 vvalue update maximum old vvalue gvalue setting initial vvalues according Eq 7 result vvalues larger estimates computed search gvalues In words new line 15 follows 15 v X max v X g S X v X u max v X u cid2 cid2 cid2 cid3 cid2 cid2 cid3cid3 cid2 cid3 g cid3cid3 S X cid2 besta X besta cid3 S X Fig 10c shows operation ComputePath function uses optimization The gvalue cell C5 computed 9 05 max2 10 1 5 05 max1 5 1 5 uses vvalue state R B6 C 4 1 C 6 1 better estimate vvalue R C 5 C 4 1 C 6 1 nonpreferred outcome moving C5 C6 Consequently search returns different path PPCP explore path cells C5 C6 returned optimization The proposed optimization substantially cut overall number search iterations PPCP This signiﬁcantly overcomes expense computing better estimates vvalues nonpreferred outcomes 64 Speeding searches PPCP repeatedly executes Alike searches As result search efforts repeated beneﬁcial employ techniques D 8 D Lite 9 Adaptive A 10 known signiﬁcantly speed repeated A searches We use method guarantees perform work A search importantly requires little changes ComputePath function1 1 It interesting direction future work investigate ComputePath function PPCP uses incremental way D D Lite extended A incremental version 714 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 Fig 11 Path clearance problem The idea simple follows This optimization computes informed heuristic values heurS X focus search heurS X heuristic value estimates distance S Xp S X assumption hidden variables values unknown set b The heuristics need consistent Initially search iteration compute start distance cost leastcost path S Xstart state question state S assuming execution stochastic action results preferred outcome words search deterministic environment value hidden variable set b In Fig 8 example means compute distance cell A4 cell assuming cells E4 B5 free We computation single Dijkstras search Let denote computed value state S X heur S Xstart S X The computed value heur S Xstart S X perfect estimate start distance environment hidden variable set b Therefore good heuristic value use ComputePath function invoked Xp Xstart The problem ComputePath function time called ﬁnd path state Xp cid8 Xstart We employ principle 10 allows use heur values state S X heuristic value heurS X improved follows cid2 cid2 cid3cid3 cid2 cid2 cid2 cid3 S X heur max heur cid3 heur S X cid3 S Xstart S X heur S Xstart S Xp Note S X retain value heurS X search For reasoning 10 updated heurS X guaranteed overestimate actual distance S Xp S X remain consistent function 7 Application path clearance The problem path clearance problem planning robot task reach goal quickly possible detected adversary 1112 The robot know precise locations adversaries list possible locations When navigating robot come possible adversary location sense long range sensor area adversary detected cut area The example Fig 11 demonstrates path clearance problem Fig 11b shows traversability map satellite image 35 3 area shown Fig 11a The traversability map obtained converting image discretized 2D map cell size 5 5 meters traversable shown light grey color shown dark grey color The robot shown blue circle goal green circle2 Red circles possible adversary locations radii represent sensor range adversaries 100 meters example The radii vary location The locations speciﬁed manually automatically places narrow passages Each location comes probability containing adversary 50 location example likelihood location contains adversary The probabilities vary location The path robot follows change time robot senses possible adversary locations sensor range robot 105 meters example A planner needs reason possible outcomes sensing execution generate policy dictates path robot function outcome sensing Ideally generated policy minimize expected traversal distance Finding policy guarantees optimality corresponds planning missing information environment In fact path clearance problem equivalent problem planning robot navigating partiallyknown terrain The difference path clearance problem detecting adversary blocks large area resulting long detour An adversary location tendency placed places blocks path robot backup choose totally different route As result detours costlier case navigation partiallyknown terrain uncertainty 2 For colors web version article M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 715 Fig 12 Solving path clearance problem freespace assumption Solving path clearance assumptive planning To avoid computational complexity robot operating partiallyknown terrain performs assumptive planning 81314 In particular follows shortest path assumption unknown areas environment free robot sensed oth erwise This known freespace assumption 14 The robot follows path reaches goal senses new information environment In case robot recomputes starts following new shortest path freespace assumption The freespace assumption applicable path clearance problem The robot plan path assumption adversary present sensed This assumption makes path clearance deterministic planning problem solved eﬃciently The fact robot ignores uncertainty adver saries means risks having long detours detours path clearance problem tend longer problem navigation partiallyknown terrain previously explained For example Fig 12a shows path computed robot uses freespace assumption According path robot tries possible adversary location A shown Fig 11b shortest route goal As robot senses location A discovers adversary present red circle black sensing As result robot long detour Fig 12b shows actual path traversed robot reaches goal Solving path clearance PPCP planning Turns path clearance problem eﬃciently solved PPCP Same robot navigation partiallyknown terrain path clearance problem clear preferences values unknowns The unknowns m binary variables m possible adversary locations The preference variables value false adversary present Differently simple example Fig 7 path clearance robot long range sensor It sense adversary present actually reaching area covered adversary As result needs remember preferred outcomes wants sense adversaries long distance To address augment S X k 3 preferred outcomes sensing described Section 61 Fig 13 shows application PPCP path clearance example Fig 11 Before robot starts executing policy PPCP plans ﬁve seconds Fig 13a shows ﬁrst policy produced PPCP black color It single path goal fact exactly path planned planning freespace assumption Fig 12a PPCP produced path milliseconds ﬁrst iteration At step PPCP reﬁnes policy executing new search determines cost detour robot ﬁrst adversary location path contains adversary The result new policy Fig 13b PPCP continues manner end ﬁve seconds allocated planning generates policy shown Fig 13c This policy passed robot execution Each fork policy robot tries sense adversary chooses corresponding branch As explained Section 62 interleave planning execution Thus robot executes plan PPCP im proves relative current position robot Fig 13d shows new position robot robot travels speed 1 meter second current policy generated PPCP 15 seconds robot given goal Fig 13e shows position robot policy PPCP generated 30 seconds At point PPCP converged reﬁnement necessary Note generated policy makes robot area left number ways goal high chance available Unlike plan generated planning freespace assumption plan gener ated PPCP avoids going location A Fig 13f shows actual path traversed robot It 4123 meters long length trajectory traversed robot plans freespace assumption Fig 12b 4922 meters 716 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 Fig 13 Solving path clearance problem PPCP 8 Experimental analysis 81 Navigation partiallyknown terrain In section use problem robot navigation unknown terrain evaluate performance PPCP algo rithm optimizations In experiments randomly generated fractal environments model outdoor environments 15 A robot allowed directions cost traversable cells deﬁned distance centers corresponding cells times cost traversing target cell according fractal value The cost sensing discovering initially unknown cell untraversable set cost moving cell moving source cell In ﬁrst set experiments compared performance PPCP optimal algorithms VI value iteration LAO 16 RTDP 3 All plan ﬁnitesize belief statespaces shown competitive planners belief statespaces 5 To VI eﬃcient scalable ﬁrst performed simple reachability analysis initial belief state ran VI reachable portion belief state space Both PPCP LAO following admissible consistent heuristics estimate distances states coordinates x1 y1 x2 y2 cid2 x1 x2 y1 y2 cid2 x1 x2 y1 y2 cid2 x1 x2 y1 y2 cid2 max min 2 min cid3cid3 cid3 cid3 The heuristics initialize state values running VI RTDP algorithms Fig 14a shows time takes converge percent solved environments environments declared unsolved algorithm ran 15 minutes solution costs algorithms M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 717 Fig 14 Experimental results environments size 17 17 cells The number unknown locations increases 6 18 number results averaged 25 environments The ﬁgure shows PPCP converges faster algorithms differences speeds grow large fast increase number unknown locations More importantly PPCP able solve environments cases We numbers VI 6 unknowns LAO 10 unknowns running memory environments3 Fig 14a shows cases solution returned PPCP turned returned algorithms optimal solution An interesting potentially important byproduct results implication randomly generated environments optimal navigation partiallyknown environment need memorize cells turn free Finally Fig 14b shows rate convergence vvalue start state algorithms environments 6 unknowns note log scale time Besides algorithms compared PPCP eﬃcient algorithms HDP 17 MCP 18 FFreplan 19 FPG 20 plan ﬁnite belief statespaces While compared performance believe performance similar exhibited RTDP LAO perform planning belief statespaces exponential number unknowns The second set experiments shows PPCP applied problem robot navigation environments large size large number unknown locations Fig 14c compares performance PPCP strategy planning freespace assumption The comparison environments size 500 500 cells number unknown locations ranging 1000 04 overall size 25000 10 The size corresponding belief state spaces ranges 250000 31000 250000 325000 Unlike previous experiments ones robot moving given 1 second plan moves planning PPCP planning freespace assumption This time suﬃcient planning freespace assumption generate path The PPCP planning interleaved execution described Section 62 In experiments PPCP converged ﬁnal policy tens moves Fig 14c summarizes execution costs approaches 3 In domains LAO runs better VI In domain performance LAO comparable VI worse RTDP We believe reason fact heuristics informative costs cells larger ones If heuristics focus efforts VI reachability analysis eﬃcient LAO smaller overhead 718 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 averaged 25 randomly generated fractal environments row table The results cost trajectory traversed robot PPCP planning consistently smaller traversed robot freespace assumption planning 82 Path clearance In section study performance PPCP algorithm path clearance problem In experiments extended version PPCP allowed remember k 3 preferred outcomes described Section 61 In experiments randomly generated fractal environments model outdoor environments On fractal environments superimposed number randomly generated paths randomly generated pairs points The paths meant simulate roads forests valleys usually present outdoor terrains Figs 15a b typical environments experiments The lighter colors represent easily traversable areas All environments size 500 500 cells size cell 5 5 meters The test environments split groups Each group contained 25 environments For environment group I set 30 possible adversary locations randomly chosen coordinates areas traversable The size corresponding belief statespace 250000 330 Fig 15a shows plan PPCP algorithm optimizations described Sections 63 64 generated convergence environments group I For environment group II set 10 possible adversary locations The size corresponding belief statespace 250000 310 The coordinates locations chosen maximize length detours This meant simulate fact adversary set point robot long detour In words adversary set place robot likely traverse Thus environments group II challenging Fig 15b shows typical environment group II plan generated PPCP optimizations The shown plan 95 probability reaching goal words robot executing policy 5 chance encountering outcome plan generated In contrast plan Fig 15a plan environment group II complexthe detours longer harder compute For possible adversary location probability containing adversary set random value 01 09 We run sets experiments environments In ﬁrst set compared unoptimized PPCP al gorithm PPCP algorithm optimizations described Sections 63 64 Table 1 shows results group I averaged environments The algorithms run convergence order obtain comparison results According number states expanded unoptimized PPCP ﬁve times runtime close ﬁve times longer optimized PPCP The unoptimized PPCP converged environments 15 minutes In second set experiments compared execution cost robot planning optimized PPCP versus execution cost robot planning freespace assumption 14 Unlike previous experiments robot moving 5 seconds plan traversing 5 meter distance This time suﬃcient planning freespace assumption generate path The PPCP planning hand interleaved execution explained Section 62 Fig 15 The example environments testing plans generated PPCP Table 1 The comparison unoptimized optimized PPCP Group I environments The convergence times given environments algorithms converged 15 minutes expansions Time convergence secs Converged 15 minutes Unoptimized PPCP Optimized PPCP 59759717 11911585 28183 6081 64 92 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 719 Table 2 The overhead execution cost navigating planning freespace assumption navigating planning PPCP freespace freespace2 freespace3 Overhead execution cost Group I penalty 54 05 21 Group II penalty 52 49 43 Group I penalty 354 48 00 Group II penalty 216 170 127 Table 2 shows overhead execution cost incurred robot plans freespace assumption execution cost incurred robot uses PPCP planning The rows freespace2 freespace3 correspond making cost going cell belongs possible adversary location twice times higher respectively One scale costs way order bias paths generated planner freespace assumption away going possible adversary locations The results averaged 8 runs 25 environments group For run true status adversary location generated random according probability having adversary The ﬁgure shows planning PPCP results considerable execution cost savings The savings group I envi ronments small biasing freespace planner set 2 The problem biasing factor dependent actual environment way adversaries set sensor range adversary Thus overhead planning freespace group II environments considerable bias factors In columns introduced penalty discovering adversary It simulated fact robot runs risk detected adversary tries sense In experiments overhead planning freespace assump tion large Also note best bias factor freespace assumption shifted 3 indicating depend actual problem Overall results indicate planning PPCP signiﬁcant beneﬁts require tuning 9 Related work In general planning missing incomplete information environment sensing special class planning Partially Observable Markov Decision Processes POMDPs 5 As result theoretically algorithms solving POMDPs applicable solving problem planning missing information Unfortunately planning optimally POMDPs general planning missing information particular known intractable 12 Various approximations techniques proposed instead 2128 For example gridbased approaches 28 30 solve POMDPs putting specialized grids inﬁnite belief statespaces converting planning problem solving ﬁnitesize usually large MDP Pointbased approaches 24262731 approximate value function belief space computing relatively small set reachable points belief space Factorizationbased approaches 2123 use factored representation belief states Baral Son developed approximation techniques solving planning missing information problems 32 A number approaches capable planning missing information based idea heuris tic searches way 351618313335 For example LAO 16one algorithms experimentsis eﬃcient combination dynamic programming Alike extensions developed speciﬁcally planning MDPs It shown able ﬁnd policies belief statespaces 5 MCP 18 eﬃciently ﬁnd optimal policies running series Alike searches belief statespaces sparse stochasticity HSVI 31 FSVI 35 incorporate ideas heuristic searches pointbased approaches MAA 34 algorithm solving ﬁnitehorizon decentralized POMDPs optimally Alike processing Similarly experiments RTDP 3 ﬁnd solutions POMDP problems planning belief statespaces 5 Many abovementioned algorithms capable solving general POMDP problems It important realize problem addressing paper narrower simpler solving general POMDP For assume underlying problem deterministic uncertainty actions missing information environment We assume sensing perfect entails ﬁnite size belief statespace acyclic optimal policy Most importantly concentrate class problems clear preferences missing information The relevant work algorithm 36 developed problem robot navigation partiallyknown terrain Similarly deﬁnition clear preferences planner taken advantage idea cost plan cell free larger cost plan cell occupied Based idea proposed clever planner capable ﬁnding optimal policies faster optimal approaches The goal work avoid dealing exponentially large belief statespaces altogether required guarantee optimality solution This allows solve eﬃciently running memory large environments large missing information The cost solution optimality guarantee certain conditions 720 M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 10 Discussion future work Besides eﬃciency low memory requirements important advantages PPCP algorithm opin ion simplicity ease implementation PPCP easy implement running series A searches instances underlying problem deterministic making necessary assumptions pieces missing information For example path clearance problem PPCP reduced running series A searches exception gvalues computed ﬁnd paths environments Each environment adversaries present speciﬁed Xp Therefore implementation algorithm trivial The main disadvantage PPCP provide optimality guarantees certain conditions described Section 54 It hope possible derive general bounds suboptimality solutions returned PPCP cases conditions satisﬁed Interestingly experiments solutions returned PPCP optimal compared environments small solved algorithms ﬁnd provably optimal solutions Experimentally PPCP works problems clear preferences clear That partic ular outcome sensing thought preferred outcome satisfy Deﬁnition 1 In opinion valuable analyze behavior PPCP problems theoretical In particular interesting derive function relates suboptimality PPCP clear preferences satisﬁed Finally paper concentrated notion clear preferences missing information There common sources uncertainty One direction future research explore notion clear preferences extended cover types uncertainty sensor noise uncertainty actuation For instance case preferred outcomes actions Thus robot moving cliff clearly prefers slip Sometimes preferences clear nearly clear slip outcome turn good outcome end In case teresting investigate clear preferences assumed construct planner capable dealing realtime largescale problems exhibiting uncertainty actuation uncertainty environ ment 11 Conclusions Most good planning uncertainty When faced task try derive plan minimizes expected cost Instead typically reason contingencies assume cases fortune look The key able fact usually know assume ahead time good One goals paper formally deﬁne notion clear preferences missing information environment A second goal paper existence clear preferences construct eﬃcient planner PPCP By making use preferences PPCP solves planning problem running series deterministic Alike searches space original deterministic planning problem belief statespace exponential number unknowns The complexity searches complexity planning making assumptions unknowns common way real time planning possible This makes PPCP highly eﬃcient scalable largescale planning problems large amounts uncertainty In theoretical analysis shown converged plan returned PPCP guaranteed optimal certain conditions In experimental analysis shown PPCP successfully planning partiallyknown terrains solving path clearance problem important problems robotics For prob lems PPCP scale larger environments uncertainty previously possible We currently working applying PPCP planning problems robotics including navigation uncertainty position moving objects humans planning autonomous landing unmanned helicopters uncertainty safety multiple landing sites We hope paper stimulate research notion clear preferences uncertainty available eﬃcient algorithm probabilistic planning missing information ﬁnally encourage wider use planning uncertainty realtime robots operating largescale environments Acknowledgements This work sponsored US Army Research Laboratory contract Robotics Collaborative Technology Alliance contract number DAAD190120012 The views conclusions contained document authors interpreted representing oﬃcial policies expressed implied Army Research Laboratory US Government M Likhachev A Stentz Artiﬁcial Intelligence 173 2009 696721 721 References 1 CH Papadimitriou JN Tsitsiklis The complexity Markov Decision Processes Mathematics Operations Research 12 3 1987 441450 2 C Baral V Kreinovich R Trejo Computational complexity planning approximate planning presence incompleteness Artiﬁcial Intelli gence 122 12 2000 241267 3 A Barto S Bradtke S Singh Learning act realtime dynamic programming Artiﬁcial Intelligence 72 1995 81138 4 PE Hart NJ Nilsson B Raphael A formal basis heuristic determination minimum cost paths IEEE Transactions Systems Science Cybernetics SSC4 2 1968 100107 5 B Bonet H Geffner Planning incomplete information heuristic search belief space S Chien S Kambhampati C Knoblock Eds Proc 6th International Conf Artiﬁcial Intelligence Planning Scheduling AAAI Press Breckenridge CO 2000 pp 5261 6 M Puterman Markov Decision Processes Discrete Stochastic Dynamic Programming John Wiley Sons 1994 7 M Likhachev A Stentz PPCP The proofs Tech Rep University Pennsylvania Philadelphia PA 2008 8 A Stentz The focussed D algorithm realtime replanning Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 1995 9 S Koenig M Likhachev D Lite Proceedings Eighteenth National Conference Artiﬁcial Intelligence AAAI 2002 10 S Koenig M Likhachev Adaptive A Proceedings International Joint Conference Autonomous Agents Multiagent Systems AAMAS 2005 poster abstract 11 M Likhachev A Stentz Goal directed navigation uncertainty adversary locations Proceedings International Conference Intelligent Robots Systems IROS 2007 12 M Likhachev A Stentz Path clearance multiple scout robots Proceedings Army Science Conference ASC 2006 13 I Nourbakhsh M Genesereth Assumptive planning execution simple working robot architecture Autonomous Robots Journal 3 1 1996 4967 14 S Koenig Y Smirnov Sensorbased planning freespace assumption Proceedings IEEE International Conference Robotics Automation ICRA 1996 15 A Stentz Mapbased strategies robot navigation unknown environments AAAI Spring Symposium Planning Incomplete Information Robot Problems 1996 16 E Hansen S Zilberstein LAO A heuristic search algorithm ﬁnds solutions loops Artiﬁcial Intelligence 129 2001 3562 17 B Bonet H Geffner Faster heuristic search algorithms planning uncertainty feedback Proceedings 18th International Joint Conference Artiﬁcial Intelligence 2003 pp 12331238 18 M Likhachev G Gordon S Thrun Planning Markov Decision Processes sparse stochasticity Advances Neural Information Processing Systems NIPS 17 MIT Press Cambridge MA 2004 19 S Yoon A Fern R Givan FFreplan A baseline probabilistic planning Proceedings International Conference Automated Planning Scheduling ICAPS 2007 20 O Buffet D Aberdeen The factored policy gradient planner Proceedings Fifth International Planning Competition IPC 2006 21 C Boutilier D Poole Computing optimal policies partially observable decision processes compact representations Proceedings National Conference Artiﬁcial Intelligence AAAI96 AAAI PressThe MIT Press Portland OR 1996 pp 11681175 22 X Boyen D Koller Tractable inference complex stochastic processes Proceedings International Conference Uncertainty Artiﬁcial Intelligence UAI 1998 pp 3342 23 C Guestrin D Koller R Parr Solving factored pomdps linear value functions Proceedings Workshop Planning Uncertainty Incomplete Information 2001 24 K Poon A fast heuristic algorithm decisiontheoretic planning PhD thesis The Hong Kong University Science Technology 2001 25 N Roy G Gordon Exponential family pca belief compression POMDPs Advances Neural Information Processing Systems 2002 26 J Pineau G Gordon S Thrun Pointbased value iteration An anytime algorithm POMDPs Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 2003 27 M Spaan N Vlassis A pointbased POMDP algorithm robot planning Proceedings IEEE International Conference Robotics Au tomation 2004 pp 23992404 28 B Bonet An cid3optimal gridbased algorithm Partially Observable Markov Decision Processes Proceedings International Conference Machine Learning 2002 29 WS Lovejoy Computationally feasible bounds partially observed Markov Decision Processes Operations Research 39 1 1991 162175 30 R Zhou EA Hansen An improved gridbased approximation algorithm POMDPs Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 2001 31 T Smith RG Simmons Heuristic search value iteration POMDPs Proceedings International Conference Uncertainty Artiﬁcial Intelligence UAI 2004 32 Baral Son Approximate reasoning actions presence sensing incomplete information Proceedings International Logic Programming Symposium ILPS 1997 33 R Washington BIPOMDP Bounded incremental partiallyobservable Markovmodel planning Proceedings European Conference Planning ECP 1997 pp 440451 34 FCD Szer S Zilberstein Maa A heuristic search algorithm solving decentralized POMDPs Proceedings International Conference Uncertainty Artiﬁcial Intelligence UAI 2005 35 G Shani RI Brafman SE Shimony Forward search value iteration POMDPs Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 2007 36 D Ferguson A Stentz S Thrun PAO planning hidden state Proceedings 2004 IEEE International Conference Robotics Automation 2004