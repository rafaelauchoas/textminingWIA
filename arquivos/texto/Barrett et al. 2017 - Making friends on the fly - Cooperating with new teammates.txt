Artiﬁcial Intelligence 242 2017 132171 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Making friends ﬂy Cooperating new teammates Samuel Barrett a1 Avi Rosenfeld b Sarit Kraus cd Peter Stone e Cogitai Inc Anaheim CA 92808 USA b Dept Industrial Engineering Jerusalem College Technology Jerusalem 9116001 Israel c Department Computer Science BarIlan University Ramat Gan 5290002 Israel d Institute Advanced Computer Studies University Maryland College Park MD 20742 USA e Dept Computer Science The University Texas Austin Austin TX 78712 USA r t c l e n f o b s t r c t Article history Received 28 February 2015 Received revised form 10 October 2016 Accepted 17 October 2016 Available online 21 October 2016 Keywords Ad hoc teamwork Multiagent systems Multiagent cooperation Reinforcement learning Pursuit domain RoboCup soccer Robots deployed increasing variety environments longer periods time As number robots grows increasingly need interact robots Additionally number companies research laboratories producing robots increasing leading situation robots share common communication coordination protocol While standards coordination communication created expect robots need additionally reason intelligently teammates limited information This problem motivates area ad hoc teamwork agent potentially cooperate variety teammates order achieve shared goal This article focuses limited version ad hoc teamwork problem agent knows environmental dynamics past experiences teammates experiences representative current teammates To tackle problem article introduces new generalpurpose algorithm PLASTIC reuses knowledge learned previous teammates provided experts quickly adapt new teammates This algorithm instantiated forms 1 PLASTICModel builds models previous teammates behaviors plans behaviors online models 2 PLASTICPolicy learns policies cooperating previous teammates selects policies online We evaluate PLASTIC benchmark tasks pursuit domain robot soccer RoboCup 2D simulation domain Recognizing key requirement ad hoc teamwork adaptability previously unseen agents tests use 40 previously unknown teams ﬁrst task 7 previously unknown teams second While PLASTIC assumes degree similarity current past teammates behaviors steps taken experimental setup sure assumption holds The teammates created variety independent developers designed share similarities Nonetheless results PLASTIC able identify exploit similarities current past teammates behaviors allowing quickly adapt new teammates 2016 Elsevier BV All rights reserved This article contains material 4 prior conference papers 1114 Corresponding author Email addresses samcogitaicom S Barrett rosenfajctacil A Rosenfeld saritcsbiuacil S Kraus pstonecsutexasedu P Stone 1 This work performed Samuel Barrett graduate student University Texas Austin httpdxdoiorg101016jartint201610005 00043702 2016 Elsevier BV All rights reserved S Barrett et al Artiﬁcial Intelligence 242 2017 132171 133 1 Introduction Robots cheaper durable deployed environments longer periods time As robots continue proliferate way encounter interact variety kinds robots In cases interacting robots share set common goals case desirable cooperate In order effectively perform new environments changing teammates observe teammates adapt achieve shared goals For example disaster helpful use robots search site rescue survivors However robots come variety sources designed cooperate response 2011 Tohoku earthquake tsunami 43 555658 If robots preprogrammed cooperate share information areas searched worse unintentionally impede teammates efforts rescue survivors Therefore future desirable robots designed observe teammates adapt forming cohesive team quickly searches area rescues survivors This idea epitomizes spirit ad hoc teamwork In ad hoc teamwork settings agents encounter variety teammates try accomplish shared goal In ad hoc teamwork research researchers focus designing single agent subset agents cooperate variety teammates The desire agents designed ad hoc teamwork quickly learn teammates determine act new team achieve shared goals Agents reason ad hoc teamwork robust changes teammates addition changes environment This article focuses limited version ad hoc teamwork problem Speciﬁcally article investigates agent adapt new teammates given previously interacted teammates learned interactions However past interactions representative current teammates In article word agent refers entity repeatedly senses environment takes actions affect environment shown visually Fig 1a As shorthand terms ad hoc team agent ad hoc agent article refer agent reasons ad hoc teamwork The environment includes dynamics world agent interacts deﬁning observations received agent We treat agents domain teammates share set common goals fully cooperative terminology game theory Previous work teamwork largely assumed agents domain act uniﬁed team designed work speciﬁc teammates 25366668 Methods coordinating multiagent teams largely rely specifying standardized protocols communication shared algorithms coordination These approaches directly apply ad hoc teams strong assumptions sharing prior knowledge violated ad hoc teamwork scenario This view multiagent teams shown Fig 1b On hand article focus creating single agent cooperates teammates coming variety sources directly altering behavior teammates However agents share set common goals desirable act team In addition focusing single task agents face variety tasks task refers environment teams agents teams shared goals The differences article prior work presented visually Fig 1 Another existing area research agents behave reinforcement learning RL Generally RL problems revolve single agent learning interacting environment In RL problems agents receive sparse feedback quality sequences actions Generally RL algorithms model agents environment try learn best policy single agent given environment consider case team single designers control In addition RL algorithms usually learn scratch new environment ignoring information coming previous environments However growing body work applying transfer learning RL allow agents reuse prior experiences new domains 69 Fig 1a shows standard RL view agent interacting environment Fig 1b represents common multiagent view uniﬁed team interacting environment agents model teammates separate environment In case team designed deployed cooperate speciﬁc agents interact ﬁxed environment However agents rely knowing teammates usually require explicit communication andor coordination protocol shared team 365373 On hand article focus ad hoc teams drawn set possible teammates team tackles variety possible environments shown Fig 1c In case teammates programmed cooperate speciﬁc ad hoc agent treated given inalterable Instead research focuses enabling ad hoc agent cooperate variety teammates range possible environments In ad hoc team agents need able cooperate variety previously unseen teammates Rather developing protocols coordinating entire team ad hoc team research focuses developing agents cooperate teammates absence explicit protocols Therefore consider single agent cooperating teammates adapt behavior In scenario develop algorithms ad hoc team agent having direct control teammates In order responsive different teammates environments fully general ad hoc agent needs general classes capabilities 1 ability learn act environment maximize reward 2 ability reason teamwork learn teammates Previous work reinforcement learning largely focused agent learn dynamics environment 4967 addresses capability 1 Therefore article 134 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Fig 1 Foci agent based research leverage past research expand work new direction capability 2 reasoning team social knowledge required effective teamwork Speciﬁcally article explores limited version ad hoc teamwork problem ad hoc agent knows environmental dynamics encounters unknown teammates previous experience domain teammates However past experiences reﬂect current teammates behaviors To end introduce new algorithm ad hoc teamwork PLASTIC allows ad hoc agent reuse knowledge learned previous teammates quickly adapt teammates exhibiting unknown behaviors We analyze algorithm number different scenarios vary similar previous interactions current teammates behaviors The PLASTIC algorithm assumes similarities new old teammates behaviors true scenarios The experiments article enforce assumption creation teammates behaviors PLASTIC ﬁnds similarities exploit scenarios investigated article This article includes material originally presented 4 conference papers 1114 In addition contributions papers main new contribution article showing algorithms proposed papers form overarching approach PLASTIC This article describes PLASTIC algorithm applied ad hoc teamwork scenarios agent previous interactions domain In addition expands empirical results papers introducing detailed description type problems PLASTIC solve The remainder article organized follows Section 2 discusses problem ad hoc teamwork framework domains article evaluating ad hoc team agents We present background information required understand remainder article Section 3 In Section 4 present PLASTIC instantiations PLASTICModel PLASTICPolicy Section 5 describes empirical analyses PLASTIC pursuit domain Section 6 analyzes performance PLASTIC half ﬁeld offense 2D simulation domain Section 7 situates research literature Section 8 contains concluding remarks directions future research S Barrett et al Artiﬁcial Intelligence 242 2017 132171 135 2 Ad hoc teamwork This section presents problem ad hoc teamwork problem explored article framework evaluate ad hoc teamwork article We follow description grounding framework domains pursuit domain simulated robot soccer 21 Ad hoc teamwork description The problem ad hoc teamwork revolves agent cooperate teammates knows little These teammates share communication protocol expected share goals In ad hoc team setting agents assume teammates attempting accomplish goals opposed game theoretic settings agents reason ways opponents exploit behaviors In article assume teammates given ad hoc agent select teammates They selected earlier decision agent human intervention However ad hoc agent know teammates behaviors ahead time investigate ad hoc agent reuse knowledge past teammates learn quickly new teammates This article explores limited version greater ad hoc teamwork problem ad hoc agent behave knows environmental dynamics prior experiences teammates In general past teammates behaviors arbitrarily far current teammates behaviors However hypothesize similarities behaviors exploited speed learning To better understand ad hoc teamwork problem work past research compares identify dimensions ad hoc teamwork problems These dimensions affect ad hoc agent behave diﬃculty problem faces allow concretely specify problems article past works explore While possible ways scenarios vary size tasks state space stochasticity domain ﬁnd following dimensions informative differentiating algorithms existing literature 1 Team Knowledge Does ad hoc agent know teammates actions given state interacting 2 Environment Knowledge Does ad hoc agent know transition reward distribution given joint action state interacting environment 3 Reactivity teammates How ad hoc agents actions affect teammates These dimensions affect diﬃculty planning domain addition ad hoc agent needs explore environment teammates When ad hoc agent good knowledge plan consider ing exploration incomplete knowledge reason cost beneﬁts exploration The explorationexploitation problem studied previously notably reinforcement learning literature adding need explore teammates behaviors ability affect considerably alters tradeoff We believe scenarios lower amounts team knowledge environment knowledge higher amounts team mate reactivity representative ad hoc teamwork problem We expect agents capable dealing concerns robust ad hoc team agents able deal nearly ad hoc teamwork scenario Sections 211213 provide details dimensions measure impor tant ad hoc teamwork Note equations following sections use summations summations converted integrals domains continuous states actions We use dimensions article characterize experiments Sections 243 252 related work Section 751 We hypothesize problems similar characteristics dimensions approached similar algorithms The PLASTIC algorithm introduced article addresses subset problems In Section 22 specify subset 211 Team knowledge The ad hoc agents knowledge teammates behaviors gives insight diﬃculty planning domain The agents knowledge range knowing complete behaviors teammates knowing Settings partial information especially relevant real world problems exact behavior teammate known reasonable guidelines behaviors exist For example playing soccer usually assume teammate intentionally pass team shoot wrong goal If behaviors completely known agent reason fully teams actions behaviors unknown agent learn adapt ﬁnd good behavior To estimate ad hoc agents knowledge teammates behaviors compare actions ad hoc agent expects ground truth actions Speciﬁcally compare expected distribution teammate actions true distribution teammates follow To compute difference distributions use JensenShannon divergence measure chosen smoothed symmetric variant 136 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 popular KullbackLeibler divergence measure When ad hoc agent information teammates action assume uses uniform distribution represent actions Therefore deﬁne knowledge measure K T Pred 1 1 JST Pred JST U JSPred U JSU Point JST Pred 0 JST Pred JST U 1 T true distribution Pred predicted distribution U uniform distribution Point distribution weight point 1 0 0 JS JensenShannon divergence measure By deﬁnition K T T 1 knowledge complete ad hoc agent knows true distribution K T U 0 representing ad hoc agent knowledge relies uniform distribution Finally predicted distribution accurate uniform distribution K T Pred negative minimum value 1 This measure captures range 0 1 smoothly range 1 02 However generally expect prediction higher entropy distribution true distribution ad hoc agent ought correctly model uncertainty teammates behaviors conﬁdent wrong keeps measure range 0 1 We deﬁne ad hoc agents knowledge teammates behaviors TeamK 1 nk ncid6 kcid6 s1 t1 K TrueActionts PredActionts 1 s n state 1 t k speciﬁes teammate TrueActionts ground truth action distribution teammate t state s PredActionts action distribution ad hoc agent predicts teammate t select state s We assume PredActionts uniform distribution agent information teammate ts actions state s Thus ad hoc agent better information teammates behaviors distance distributions smaller TeamK higher If ad hoc agent limit possible actions teammates bias predictions likely actions TeamK higher For instance 100 possible actions ad hoc agent narrow teammates action choices TeamK relatively large 212 Environmental knowledge Another informative dimension knowledge ad hoc agent effects joint action given state example transition reward functions If ad hoc agent complete knowledge environment plan actions select simply consider unknown effects actions However incomplete knowledge explore actions face standard problem balancing exploring environment versus exploiting current knowledge Similarly teammate knowledge formally deﬁne ad hoc agents knowledge environments transitions TransK 1 nm ncid6 mcid6 s1 j1 K TrueTranss j PredTranss j 1 s n state 1 j m joint action K taken Equation 1 TrueTranss j ground truth transition distribution state s given joint action j PredTrans ad hoc agents predicted transition distribution If agent information transitions assume PredTranss j uniform distribution Intuitively ad hoc agent knows transition function distance TrueTrans PredTrans smaller result TransK higher We deﬁne agents knowledge environmental rewards similarly let EnvK TransK RewardK 213 Teammate reactivity The optimal behavior ad hoc agent depends teammates react actions If team mates actions depend ad hoc agent ad hoc agent simply choose actions maximize team reward single agent problem Considering actions teammates separately environment help computation factoring domain However teammates actions depend strongly ad hoc agents actions ad hoc agents reasoning consider teammates reactions If ad hoc agent modeling teammates teammates modeling ad hoc agent problem recursive directly addressed Gmytrasiewicz et als Recursive Modeling Method 35 2 One slight anomaly measure T uniform distribution 5 5 K 1 Pred exactly correct 55 negative For values T K smoothly spans range 1 1 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 137 A formal measure teammate reactivity needs capture different teammates actions ad hoc agent chooses different actions We measure distance resulting distributions teammate joint actions pairwise JensenShannon divergence measures However desirable distance 1 distributions overlap use normalizing constant log 2 Thus deﬁne reactivity domain state s Reactivitys 1 m 12 log 2 mcid6 mcid6 a1 acid41 JST s T s cid4 cid4 m actions available ad hoc agent T s JS JensenShannon divergence measure 1 distribution teammates joint actions given state s ad hoc agents action We use m 1 denominator exclude case numerator JS measure 0 case For cid7 overall reactivity domain average states resulting Reactivity 1 n s1 Reactivitys It possible n consider action affects teammates actions future restrict focus step reactivity paper cid4 Note sums formulation converted integrals continuous states actions This formu lation similar empowerment measure Jung et al 47 consider ad hoc agents ability change actions teammates environment state 22 Problem description In article focus exploring dimension teammate knowledge Speciﬁcally explore settings ad hoc agent prior experiences past teammates trying use knowledge experiences quickly adapt new teammates We assume teammates B drawn set possible teammate types A Then question ad hoc agent cooperate teammates However ad hoc agent know A B We explore different amounts prior knowledge given ad hoc agent We primarily focus setting ad hoc agent previously observed teams P operating current domain While access decisionmaking process ad hoc agent learn behaviors observing world states teams actions We hypothesize ad hoc agent use knowledge learns interactions quickly learn adapt new teammates behaviors exploiting similarities behaviors In article assume teammates ad hoc agent share common goal In addition explore environmental knowledge dimension assume ad hoc agent fully knows domain Further assume shared protocol explicit communication Speciﬁcally ad hoc agent directly communicate intentions teammates agents communicate movements world Finally assume teammates behaviors A P stationary teammates respond ad hoc agents immediate actions change behaviors learn time As algorithms forth article address small subset possible ad hoc teamwork scenarios leaving room fruitful future research The diﬃculty problem consider ad hoc agent knowledge teammates behaviors prior experiences teammates Nor agent shared communication protocol allows teammates explicitly communicate intentions Therefore ad hoc agent observe teammates determine behaviors Once knows behaviors teammates exhibit ad hoc agent adapt accordingly To speed process determining teammates behaviors ad hoc agent draw observations past teammates exploiting similarities current past teammates behaviors In article consider scenarios ad hoc agent different amounts prior knowledge team mates In terms dimensions ad hoc teamwork exploring team knowledge dimension varying accuracy PredAction value TeamK We consider scenario P A ad hoc agent seen potential teammate types know team drawn A We consider com plex scenario P A overlap P A This scenario occurs ad hoc agent previously attempted task types teammates current teammate type When P A accuracy PredAction lower ad hoc agents TeamK lower In scenarios article TeamK relatively high given ad hoc agents previous experiences fact current teammate types similar past teammates Note similarity assumed enforced authors Speciﬁcally use agent behaviors variety independent developers results PLASTIC similarities exploit scenarios explored article We hypothesize sort natural similarities occur settings While considering ad hoc agent knowledge teammates interesting think having prior knowledge form experience past teammates exhibiting behaviors realistic Agents encounter number teammates lifetime able draw experiences learn quickly encountering new teammate types similarities past teammate types In article focus scenario ad hoc agent knows environment EnvK 1 1 We believe expanding work scenarios ad hoc agent simultaneously learn environment 138 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 important avenue future research In terms dimensions believe future research focus ad hoc agents behave scenarios low values TeamK EnvK In addition article focuses problems limited reactivity teammates While agents react ad hoc agents actions learn time The ad hoc agents actions limited effects teammates values Reactivity low moderate domains article We believe future research teammates learn ad hoc agent needed ad hoc agents able deal higher amounts teammate reactivity 23 Evaluation framework Directly measuring teamwork far straightforward In cases easily measurable aspect overall performance team makes diﬃcult assign credit agent By placing agent variety teams measuring teams performances estimate good agent teamwork Therefore adopt evaluation framework introduced Stone et al 63 evaluates ad hoc team agent considering teammates domains encounter This framework speciﬁed Algorithm 1 According framework performance ad hoc team agent depends distribution problem domains D distribution possible teammates A cooperate For team B cooperating execute task d sB d scalar score representing effectiveness higher scores indicate better performance The algorithm takes sampling approach average agents performance range possible tasks teammates capture idea good ad hoc team player ought robust wide variety teamwork scenarios We use smin minimum acceptable reward team evaluated ad hoc team agent unable accomplish task teammates ineffective regardless abilities It mainly reduce number samples required evaluate ad hoc agents reduces noise comparisons Metrics sum rewards depending domain worstcase performance Algorithm 1 Ad hoc agent evaluation 1 function Evaluate inputs A D outputs r n params smin n 2 3 4 5 6 7 8 9 Initialize r 0 1 n Sample task d D Randomly draw subset agents B A EsB d smin Randomly select agent b B Create new team C Bb r r sC d return r n cid7 ad hoc agent cid7 set possible teammate agents cid7 set possible domains cid7 average performance reward cid7 minimal acceptable performance team cid7 number iterations 10 If Evaluatea0 A D Evaluatea1 A D difference signiﬁcant conclude a0 better ad hoc team agent a1 domains D set possible teammates A 24 Pursuit domain As discussed previous section evaluation ad hoc team agents depends strongly domains encounter Therefore section describes ﬁrst domain evaluating ad hoc team agents article The pursuit domain known predatorprey domain popular problem multiagent systems literature requires cooperation teammates capture prey remaining simple evaluate approaches 66 There versions pursuit domain different rules pursuit domain revolves set agents called predators trying capture agent called prey minimal time In version pursuit domain article world rectangular toroidal grid moving grid brings agent opposite Four predators attempt capture randomly moving prey surrounding sides time steps possible At time step agent select cardinal directions remain current position All agents pick actions simultaneously collisions S Barrett et al Artiﬁcial Intelligence 242 2017 132171 139 Fig 2 A view pursuit domain rectangle prey ovals predators oval star ad hoc predator evaluated handled priorities randomized time step In addition agent able observe positions agents A view domain shown Fig 2 videos domain viewed online3 241 Handcoded teammates In order meaningfully test proposed ad hoc teamwork algorithms handcoded predator algorithms vary ing representative properties The greedy GR predator moves nearest open cell neighbors prey ignoring teammates actions On hand teammateaware TA predator considers teammates allows predator farthest prey cell closest In addition teammateaware predator uses A path planning algorithm select actions greedy predator considers immediate collisions It expected differences teammates require ad hoc agent adapt reason actions inﬂuence teammates actions In addition deterministic agents stochastic agents select action distribution time step The greedy probabilistic GP predator moves similarly greedy predator chance taking longer path greedy destination Finally probabilistic destinations PD predator chooses new destination near prey time step slowly encircling prey converging These behaviors chosen provide spread representative behaviors The deterministic GR predator largely ignores teammates actions deterministic TA predator tries way teammates assumes way needed It expected ad hoc agent need cooperate differently types agents based reactivity In addition deterministic agents use stochastic GP PD agents We expect fairly trivial ad hoc agent differentiate deterministic agents harder differentiate stochastic agents Therefore ad hoc agent forced reason uncertainty teammates behaviors longer Furthermore behaviors signiﬁcantly different deterministic behaviors interacting requires reasoning noise future outcomes 242 Externallycreated teammates While set handcoded teammates attempts representative set limited possibly biased agents designed thinking ad hoc teamwork Therefore consider externallycreated team mates provide broader range agents created developers planning ad hoc teamwork scenarios Speciﬁcally use additional sets teammates article created undergraduate graduate science students These agents created assignment workshops agent design discussion ad hoc teams instead students asked create team predators captured prey quickly possible The agents produced varied wildly approaches effectiveness Both sets agents come workshop taught Sarit Kraus Bar Ilan University taught spring 2010 taught spring 2011 The ﬁrst set agents contains best 12 student agents taken ﬁrst class 41 students ﬁltered ability capture randomly moving prey 5 5 world 15 steps average smin 15 Algorithm 1 This set agents called StudentSelected The second set agents StudentBroad comes second offering course contains 29 agents class 31 students StudentBroad contains wider range performance StudentSelected ﬁltered heavily The better quality agents StudentBroad improvements directions architecture provided second class students based lessons learned ﬁrst offering course The ﬁltering set removing student team capturing prey second taking excessively long computation time 243 Dimension analysis Given problem description analyze pursuit domain described dimensions introduced Section 21 The ad hoc agents knowledge team TeamK varies different tests reactivity 3 http wwwcs utexas edu larg index php Ad _Hoc _Teamwork _Pursuit 140 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Table 1 TeamK Reactivity settings pursuit domain Teammate type Prior knowledge Handcoded Handcoded StudentSelected StudentSelected StudentBroad StudentBroad StudentBroad Handcoded Handcoded Handcoded Known learned set Handcoded Known learned set Leaveoneout known learned set World size 5 5 20 20 20 20 20 20 20 20 20 20 20 20 TeamK Reactivity 0719 0360 0156 0318 0098 0301 0280 0717 0801 0801 0801 0800 0800 0800 teammates When ad hoc agent knows teammates behaviors TeamK 1 A variety scenarios summarized Table 1 vary type teammates prior knowledge ad hoc agent teammates The ad hoc agent completely knows environment dynamics leading EnvK 1 1 The moderate reactivity values teammate types implies vital understand model ad hoc agents teammates However teammates learn time ad hoc agents actions The lower values team knowledge student teammates shows importance quickly narrowing ﬁeld models descriptive ones allow better planning This hypothesis approaches learning teammates explored empirically Section 5 25 Half ﬁeld offense 2D RoboCup simulator While pursuit domain provides interesting multiagent domain testing teamwork fairly simple com pared real world problems In order test scalability approach important consider complex problems Therefore consider simulated robot soccer domain 2D RoboCup Simulation League The 2D Simulation League oldest leagues RoboCup best studied competition research In domain teams 11 autonomous agents play soccer simulated 2D ﬁeld choose actions 100 ms Before action agents receive noisy sensory information location location ball locations nearby agents After processing information agents select abstract actions world dashing kicking turning The 2D simulation server manual includes perception action models online4 This domain provides testbed teamwork complex domain requiring focus areas vision legged locomotion Rather use 10 minute 11 11 game article instead uses quicker task half ﬁeld offense introduced Kalyanakrishnan et al 48 In Half Field Offense HFO set offensive agents attempt score set defensive agents including goalie letting defense capture ball A view game shown Fig 3 information videos online5 This task useful allows faster evaluation team performance running games providing simpler domain focus ways improve ball control In article consider versions HFO domain 1 limited version offensive agents defensive agents including goalie 2 version offensive agents ﬁve defensive agents including goalie Videos versions domain viewed online6 Hausknecht et al 39 built work article released open source version half ﬁeld offense domain7 If ball leaves offensive half ﬁeld defense captures ball offensive team loses If offensive team scores goal win In addition goal scored 500 simulation steps 50 seconds defense wins At beginning episode ball moved random location 25 offensive half closest midline Let length length soccer pitch Offensive players start randomly selected vertices forming square ball edge length 02 length added offset uniformly randomly selected 0 01 length The goalie begins center goal remaining defensive players start randomly half defensive half 251 Externallycreated teammates Unlike pursuit domain use handcoded teammates diﬃcult deﬁne set representative policies domain Instead productive consider agents created RoboCup competition It expected agents represent far better spread possible behaviors handcoded teammates given years improvements implemented competitions 4 http sourceforge net projects sserver 5 http wwwcs utexas edu AustinVilla sim halﬃeldoffense 6 http wwwcs utexas edu larg index php Ad _Hoc _Teamwork _HFO 7 https github com LARG HFO S Barrett et al Artiﬁcial Intelligence 242 2017 132171 141 Fig 3 A screenshot half ﬁeld offense 2D soccer simulation league The yellow agent number 11 control remaining yellow players externally created teammates These agents trying score blue defenders For interpretation references color ﬁgure legend reader referred web version article As 2D simulation league competition teams required release binary versions agents following competition Therefore use binary releases 2013 competition held Eindhoven Netherlands8 These agents provide excellent source externallycreated teammates test possible ad hoc team agents Speciﬁcally use 6 8 teams 2013 competition omitting 2 support playing games faster real time In addition use team provided code release Helios 3 commonly called agent2d serves basis teams league Therefore total 7 possible teams agent encounter agent2d aut axiom cyrus gliders helios yushan In order run teams RoboCup competition necessary ﬁeld entire 11 player team agents behave correctly Therefore necessary create entire team constrain additional players stay away play agents needed half ﬁeld offense These additional players moved ﬁeld time step This approach affect players HFO initial tests showed teams perform We choose ﬁxed set player numbers teammates based player numbers tended play offensive positions observations play initial experiments In limited HFO task defensive players use helios behavior HFO task use agent2d behavior 252 Dimension analysis In order better understand properties half ﬁeld offense domain teammates ad hoc agent encounter use dimensions described Section 21 We approximate JensenShannon divergence mea sure Monte Carlo sampling Recall Section 21 JSP Q 1 2 P Q KullbackLeibler divergence deﬁned 2 KLP M KLQ M M 1 cid8 KLP M P X log P x Mx The Monte Carlo approximation given ncid6 cid9KLP M 1 n P xi Mxi log Mxi 1 KL 2 P xi Q xi As number samples goes inﬁnity approximation converges true value Given actions continuous need consider inﬁnite number joint actions In addition ad hoc agent directly observe actions teammates Therefore use resulting locations agents estimate actions We assume effects actions noisy modeled Gaussian distribution standard deviation 40 40 distances angles respectively Applying methodology calculation introduced Section 211 leads approximate value 0425 TeamK limited HFO task In task ad hoc agent knows teammates behavior drawn set 7 potential behaviors In HFO task methodology calculates TeamK 0295 8 http wwwsocsim robocup org ﬁles 2D 142 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 We similarly approximate value Reactivity calculation introduced Section 213 Given 2D RoboCup simulator open source domain parameters passed players ad hoc agent completely knows environment dynamics In addition note opponents behaviors known ad hoc agent Therefore EnvK 1 1 However worth noting complex model domain tests ad hoc agent explicitly model HFO dynamics The 7 possible teams ad hoc agent encounter average reactivity Reactivity 0263 limited HFO task Reactivity 0507 version task The moderate reactivity means ad hoc agent help team consider actions affect teammates especially HFO domain However means teammates limited ability change based ad hoc agents actions For instance learning ad hoc agent time In addition perfect environmental knowledge means agent need explore environment On hand lower teammate knowledge means helpful explore teammates behaviors especially HFO domain space teammates behaviors larger Notice values close arising pursuit domain Therefore expect similar approach effective domain However complexity fully modeling domain means methods applied domains run issues Therefore expect modelfree approach effective teammate knowledge similarly effective The modelfree approach analyzed depth empirical results Section 6 3 Background While previous section describes general problem investigated article section describes math ematical model analyze problem In addition section presents existing algorithms approach builds 31 Markov decision process Agents need cooperate ad hoc teams need handle sequential decision making problems choose model problems Markov Decision Processes 67 An MDP 4tuple S A P R S set cid4s probability transitioning state s s states A set actions P s taking action Rs scalar reward given agent taking action state s In domains s S corresponds current positions agent A action ad hoc agent chooses In framework policy π mapping states actions deﬁnes agents behavior state The agents goal ﬁnd policy maximized s represents maximum long term reward long term expected rewards For stateaction pair Q obtained s deﬁned solving Bellman equation cid4 Q s Rs γ cid6 scid4 P s cid4s max acid4 cid4 cid4 s Q 2 0 γ 1 discount factor representing immediate rewards worth compared delayed rewards The optimal policy π derived choosing action maximizes Q Once model problem MDP clear agents objective maximize long term expected reward In setting translates ad hoc agent optimally cooperating teammates accomplish shared goals There number ways calculate actions result best long term expected reward Value Iteration VI 67 dynamic programming approach solve problem exactly Monte Carlo Tree Search MCTS al gorithms Upper Conﬁdence Bounds Tree UCT 51 approximately calculate actions maximize rewards sampling While VI requires model teammates actions domains transition function MCTS al gorithms require way sampling results agents actions In addition MCTS algorithms computationally tractable large domains VI A popular approach require explicit modeling domain Fitted Q Iteration FQI algorithm introduced Ernst et al 28 Similar VI FQI iteratively backs estimates values stateaction pairs However FQI employs samples states outcomes actions approximate values allows handle continuous domains s s S These algorithms explained greater depth Appendix A Other algorithms solving MDPs PLASTIC results Sections 5 6 employ methods 32 Transfer learning While previous section discusses methods ad hoc agent compute policy cooperating teammates given model teammates specify models come An approach employ article learn models past teammates treating supervised learning problem When ad hoc agent limited experiences current teammates addition extensive experiences past experiences able learn models speciﬁc current teammates Unfortunately limited experiences current teammates makes learning new model scratch infeasible However able reuse information learned S Barrett et al Artiﬁcial Intelligence 242 2017 132171 143 Fig 4 Overview PLASTIC cooperate unknown teammates past teammates addition knows current teammates learn new model teammates idea synonymous transfer learning In Transfer Learning TL goal reuse information learned source data set improve results target data set For TL performance target data matters source data training Following terminology ad hoc teamwork settings consider current teammates target set previously observed teammates source set The transfer learning algorithm TwoStageTransfer 14 article capable transferring knowledge multiple sources TwoStageTransfer inspired TwoStageTrAdaBoost algorithm 57 designed explicitly leverage multiple source data sets TwoStageTransfers goal ﬁnd best possible weighting set source data create classiﬁer weights Rather trying possible weightings TwoStageTransfer ﬁrst evaluates data source independently calculates ideal weight data source cross validation Then greedily adds data sources decreasing order calculated weights As adds data set ﬁnds optimal weighting set combined data added Finally adds data optimal weight repeats procedure data set 4 Planning Learning Adapt Swiftly Teammates Improve Cooperation PLASTIC This section introduces Planning Learning Adapt Swiftly Teammates Improve Cooperation PLASTIC algorithms enable ad hoc team agent cooperate variety different teammates One think appropriate thing ad hoc team agent ﬁt team following behavior teammates However teammates behaviors suboptimal approach limit ad hoc agent help team Therefore article adopt approach modeling possible teammate behaviors planning act based models This approach allows ad hoc agent reason models predict teammates actions allows convert predictions actions needs accomplish goals If models correct ad hoc agent given time plan approach lead optimal performance ad hoc agent helping team achieve best outcome algorithms UCT provably converge optimal behavior Note optimal performance team optimal ad hoc agent given behaviors teammates ﬁxed 41 Overview A visual overview PLASTIC given Fig 4 The short summary approach ad hoc agent learns set prior teammates given handcoded information possible teammates Then agent uses prior knowledge select actions update beliefs teammates observing reactions behavior 144 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 In article general approach realized algorithms One algorithm PLASTICModel focuses model based approach In approach ad hoc agent learns models past teammates selects models best predict current teammates uses models plan act order cooperate teammates The sec ond algorithm called PLASTICPolicy uses modelfree approach In variant ad hoc agent learns policy cooperate past teammates selects policies best match cooperate current teammates selects actions policies These algorithms described remainder section This gen eral approach speciﬁed Algorithm 2 The subroutines LearnAboutPriorTeammate SelectAction UpdateBeliefs described algorithms following section Algorithm 2 Pseudocode PLASTIC 1 function PLASTIC inputs PriorTeammates HandCodedKnowledge BehaviorPrior cid7 initialize knowledge information prior teammates PriorKnowledge HandCodedKnowledge t PriorTeammates cid7 past teammates agent encountered cid7 prior knowledge coded hand cid7 prior distribution prior knowledge 2 3 4 5 6 7 8 9 10 PriorKnowledge PriorKnowledge LearnAboutPriorTeammatet BehaviorDistr BehaviorPriorPriorKnowledge cid7 initialize beliefs cid7 act domain Initialize s s terminal SelectActionBehaviorDistr s Take action observe r s BehaviorDistr UpdateBeliefsBehaviorDistr s cid4 As shown Algorithm 2 PLASTIC begins initializing knowledge provided prior knowledge learned previous teammates Lines 25 LearnAboutPriorTeammate deﬁned differently vari ants algorithms learns information prior teammate encoding knowledge SelectAction subroutine Lines 610 PLASTIC selects agents actions PLASTIC updates beliefs teammate models policies observing actions UpdateBeliefs function implemented variants The core difference PLASTICModel PLASTICPolicy uses model based approach uses policy based approach The pursuit domain 4005 1013 states nearly deterministic actions nondeterminism reconciling collisions The half ﬁeld offense domain continuous states leading effectively inﬁnite base agents leading higher exponent In addition higherdimension state space actions nondeterministic leading higher branching factor The actions continuous discretized ad hoc agents actions Combining larger continuous state space higher branching factor hampers ability search best policy ad hoc agent However learning policies behaving type complex domains possible Speciﬁcally approaches Fitted Q Iteration handle learning policies large continuous state spaces Therefore PLASTICPolicy capable tackling complex problems PLASTICModel On hand PLASTICModel scalable diverse teams It stores model type teammate behavior team On hand PLASTICPolicy stores policy cooperating team depends behavior teammate In addition PLASTICModel better able handle situations explored article models teammate behaviors change time ongoing learning changes models As models change PLASTICModel simply adapt planning new behavior models PLASTICPolicy requires recalculating policy resulting MDP On hand main advantage PLASTICPolicy ability scale complex problems 42 PLASTICModel When agent good model environment use model plan good actions limited number interactions environment For ad hoc agent plan needs model teammates useful ad hoc agent build models teammates behaviors Given learning new models online takes samples useful reuse information learned past teammates This section describes PLASTICModel variant PLASTIC approach learns models prior teammates selects models best predict current teammates An overview approach given Fig 5 speciﬁcation LearnAboutPriorTeammate SelectAction UpdateBeliefs functions given Algorithm 3 These functions described depth remainder section 421 Model selection In Algorithm 3 necessary select set possible teammate models SelectAction Performing simulations Monte Carlo rollouts planners requires ad hoc agent model S Barrett et al Artiﬁcial Intelligence 242 2017 132171 145 Fig 5 Overview modelbased approach PLASTICModel cooperate unknown teammates teammates behave If presumably correct approximately correct single model behavior planning straightforward On hand ad hoc agent given possible models choose problem diﬃcult Assuming ad hoc agent starts prior belief distribution model correctly reﬂects teammates behaviors ad hoc agent update beliefs observing teammates In context P modelactions posterior probability model given observed actions value want determine Fortunately value calculated Bayes theorem P modelactions P actionsmodel P model P actions P actionsmodel probability ad hoc agent observing series teammates actions given teammates acting speciﬁed model P model prior probability model assume uniform models experiments This prior inject expert knowledge relative frequencies models P actions probability observing series teammates actions normalizing constant If correct model given set models ad hoc agents beliefs converge model set models differentiable model On hand correct model set Bayes rule drop good models posterior probability 0 single wrong prediction9 This update punish generally wellperforming models single mistake leaving poor models predict nearly randomly Therefore advantageous update probabilities conservatively Research regret minimization shown updating model probabilities polynomial weights algorithm near optimal examples chosen adversarially 17 Since expected ad hoc agents models perfect agent updates beliefs polynomial weights algorithm loss 1 P actionsmodel P modelactions 1 η loss P model η 05 parameter bounds maximum loss higher values converge quickly This scheme ensures good models prematurely removed reduce rate convergence In practice scheme performs observed examples teammates arbitrarily unrepresentative agents overall decision function If true teammate model set models polynomial weights algorithm guaranteed converge true teammate model 17 However general case ad hoc agent previously seen teammate true model teammate set models In case expect 9 P modelaction P actionmodelP model P action 0 P model P action 0 146 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Algorithm 3 Instantiation functions Algorithm 2 PLASTICModel 1 function UpdateBeliefs inputs BehaviorDistr s outputs BehaviorDistr params η m BehaviorDistr loss 1 P s BehaviorDistrm 1 ηloss Normalize BehaviorDistr return BehaviorDistr 2 3 4 5 6 7 function SelectAction inputs BehaviorDistr s outputs params p 8 9 ps return cid7 simulateAction derived known environment model cid7 sampling BehaviorDistr teammate behavior distribution 10 function LearnAboutPriorTeammate inputs t outputs m params Data repeat learnClassiﬁer Collect s t Data Data s m learnClassiﬁerData return m 11 12 13 14 15 16 cid7 probability distr possible teammate behaviors cid7 current environment state cid7 previously chosen action cid7 updated probability distr cid7 bounds maximum allowed loss cid7 probability distr possible teammate behaviors cid7 current environment state cid7 best action agent cid7 MDP planner selects actions UCT cid7 prior teammate cid7 model teammates behavior cid7 supervised learning algorithm forgiving update polynomial weights algorithm perform better pure Bayesian update informal tests supported belief 422 Planning This section describes SelectAction function Algorithm 3 When ad hoc agent model envi ronment teammates use model plan effects actions adapt teammates Formally ad hoc agent calculate P s state resulting ad hoc agent taking action s state s cid4s cid4 In article completely calculating probability ad hoc agent uses sample based planner approximate distribution Speciﬁcally ad hoc agent uses UCT quickly determine effects actions plan sequence actions beneﬁcial team UCT speed ability handle large action state spaces allowing scale large numbers teammates complex domains The modiﬁed version UCT algorithm article explained Appendix A2 Other planning algorithms Value Iteration VI approximate planners UCT chosen shows good empirical performance large domains 51 Given current belief distribution models ad hoc agent sample teammate models planning choosing model rollout similar approach adopted Silver Veness 60 Sampling model rollout desirable compared sampling model time step resampling lead states model predicts Ideally stateaction evaluations stored performed separately model require rollouts plan effectively Instead stateaction evaluations models combined improve generalization planning While UCT guaranteed converge optimal policy convergence exponential number rollouts In addition planning performed imperfect teammate models error UCTs calculated policy unbounded S Barrett et al Artiﬁcial Intelligence 242 2017 132171 147 error approaches VI Despite limitations UCT performs practice signiﬁcantly computationally tractable VI 423 Learning teammate models This section describes teammate models learned LearnAboutPriorTeammate function Algorithm 3 The previous sections described ad hoc agent select accurate model use planning specify source models One option ad hoc agent given handcoded models human experts shown Line 2 Algorithm 2 Fig 5 However source models models imperfect Therefore general solution ad hoc agent learn models Learning allows agent gain good set diverse models lifespan allowing better performance arbitrary new teammates The ad hoc agent builds models past teammates behaviors oﬄine selects learned models online cooperating new teammates It expected past teammates representative distribution future teammates future teammates seen In setting model teammate speciﬁes probability teammate taking action state MDP Formally model m teammate speciﬁes P aim s states s MDP If teammates actions depend ad hoc agents previous action based reactivity teammates ad hoc agents latest action included state order preserve Markov property MDP PLASTICModel treats building teammate models supervised learning problem goal predict teammates actions features extracted world state In setting ad hoc agent assumed observed past teammates actions form tuples s ai ai action teammate MDP state s The learning problem build model m captures P ais data The ad hoc agent model domain speciﬁes P s cid4s cid14a1 ancid15 cid14a1 ancid15 agents actions By combining models teammates m j model domain ad hoc agent index resulting state calculate probability action cid10 P s cid4s P s cid4s cid14a1 ai1 ai1 ancid15 P jm j s j Using equation ad hoc agent reason results actions far future In article agent uses C45 decision trees implemented Weka toolbox 37 learn models Several classiﬁers tried including SVMs naive Bayes decision lists nearest neighbor approaches boosted versions classiﬁers However initial tests domains considered article decision trees outperformed methods combination prediction accuracy training time On domains learning algorithms perform better PLASTIC depend decision trees All model learning performed oﬄine reﬂecting past experience domain ad hoc agent updates belief models online To capture notion ad hoc agent expected extensive prior general domain expertise assumed ad hoc teamwork setting speciﬁc teammates hand pretrain ad hoc agent observations pool past teammates We treat observations previous teammates experience given PLASTIC prior deploying ad hoc agent 424 Adapting existing teammate models The previous sections discuss ad hoc agent cooperate teammates interacted agent cooperate completely new teammates However cases ad hoc agent limited time observe current teammates interacts In addition extensive observations past interactions teammates For example pickup soccer scenario corresponds having past experience pickup soccer showing new game watching couple minutes joining This scenario ﬁts transfer learning TL paradigm requires ability leverage multiple sources related data In ad hoc teamwork scenario observations prior teammates correspond source data sets observations current teammates form target set In order integrate transfer learning PLASTICModel minor change needs Algorithm 2 Speciﬁcally Line 4 insert lines m LearnModelTLPriorKnowledge ObservationsTeammates PriorKnowledge PriorKnowledge m This alteration adds new model PriorKnowledge learned transfer learning combining information previous teammates limited observations new teammates In setting transfer learning problem starts observations past teammates For past team mate observations form Si st action teammate took state st t past timestep In TL terminology Si source data set When ad hoc agent encounters new teammate j observes small number teammates actions resulting set T st j called target data set Then ad hoc agent 148 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Fig 6 Overview modelfree approach PLASTICPolicy cooperate unknown teammates attempts use data sources build best model teammate j correctly predicts probability st The speciﬁcs agent combines source data sets teammate taking action target data set depends transfer learning algorithm j state st P j We discuss transfer learning algorithm TwoStageTransfer Section 32 possible transfer learning algorithms Section 74 A common approach problem determine source data set previous teammate similar target data set current teammate transfer knowledge source data set similarity refers probability teammates taking actions states In work ﬁnd TwoStageTransfer outperforms algorithms tested fact designed transfer knowledge multiple data sources simultaneously This capability allows TwoStageTransfer transfer knowledge previous teammates encountered increasing accuracy model teammates actions P j st 43 PLASTICPolicy In complex domains planning algorithms UCT perform poorly inaccuracies models environment In addition planning suﬃciently effective behavior computationally expensive employ scenarios Therefore desirable directly learn policy acting environment planning online Learning policy directly prevents ad hoc agent learning exploit actions work model real environment Given policy learned depend heavily teammates agent cooperating desirable learn policy type teammate Then ad hoc agent try pick policy best ﬁts new teammates encounters The remainder section describes PLASTICPolicy algorithm uses approach summarized Fig 6 The subroutines PLASTICPolicy speciﬁed Algorithm 4 431 Learning policy PLASTICPolicy learns teammates LearnAboutPriorTeammate function Rather explicitly modeling MDPs transition function PLASTICModel agent directly uses samples taken environment current teammates However online learning sequential long time learn useful policy complex domains Therefore desirable use distributed approach takes advantage ability run tests simultaneously To end PLASTICPolicy performs number interactions explores available actions parallel It stores experiences tuple cid14s r s resulting state cid4cid15 s original state action r reward s cid4 Using observations PLASTICPolicy learn policy cooperating teammates existing learning algorithms In article agent uses Fitted Q Iteration FQI 28 described Appendix A3 Alternative policy learn ing algorithms Qlearning 71 policy search 26 We chose use FQI good performance continuous domains require expert knowledge form parameterized policy S Barrett et al Artiﬁcial Intelligence 242 2017 132171 149 Algorithm 4 Instantiation functions Algorithm 2 PLASTICPolicy 1 function LearnAboutPriorTeammate inputs t outputs π m params Data repeat 2 3 4 5 6 7 8 Qlearning parameters α λ function approximation Collect s r s Data Data s r s t cid4 cid4 Learn policy π Data QLearning Learn nearest neighbors model m t Data return π m 9 function UpdateBeliefs inputs BehaviorDistr s outputs BehaviorDistr params η π m BehaviorDistr loss 1 P s BehaviorDistrm 1 ηloss Normalize BehaviorDistr return BehaviorDistr 10 11 12 13 14 15 function SelectAction inputs BehaviorDistr s outputs π m argmax BehaviorDistr π s return 16 17 18 432 Selecting policies cid7 prior teammate cid7 policy cooperating teammate t cid7 nearest neighbors model teammates behavior cid7 probability distr possible teammate behaviors cid7 previous environment state cid7 previously chosen action cid7 updated probability distr cid7 bounds maximum allowed loss cid7 probability distr possible teammate behaviors cid7 current environment state cid7 best action agent cid7 select likely policy This section describes UpdateBeliefs SelectAction functions Algorithm 4 When agent joins new team decide act teammates If copious amounts time learn policy cooperating teammates However time limited adapt eﬃciently We assume agent previously played number different teams agent learns policy teams When joins new team agent reuse knowledge learned teams adapt quickly new team One way reusing knowledge select learned policies If agent previously learned policy cooperating behaviors teammates exhibit knows teammates behaviors agent directly use learned policy However know behavior types teammates exhibiting agent select set learned policies determining previously seen behavior best matches teammates current behavior Many similar decisionmaking problems modeled multiarmed bandit problems problem stateless In setting selecting arm corresponds playing learned policies episode Over time agent estimate expected values expected chance scoring policy selecting policy number times observing outcome However type learning require large number trials outcomes playing policy noisy depending complexity domain Therefore desirable select policies quickly To end PLASTICPolicy employs approach based maintaining probability new team similar previously observed team At time step PLASTICPolicy takes action selected policy highest probability corresponding current teammates These probabilities updated observing actions team performs Bayes theorem However Bayes theorem drop posterior probability similar team 0 single wrong prediction Therefore Sec tion 421 PLASTICPolicy adopts approach updating probabilities polynomial weights algorithm regret minimization 17 150 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 loss 1 P actionsmodel P modelactions 1 η loss P model 3 η 05 parameter bounds maximum loss higher values converge quickly The learned policies directly probability past team taking action However experiences cid4cid15 learning policies help provide estimates teams transition function When cid14s r s update probability new team similar old agent observes state s state s team For old team agent ﬁnds stored state ˆs closest s state ˆs Then component cid4 state computes difference s We assume MDPs noise normal difference results probability drawn noise distribution Multiplying factors results point estimate probability previous team taking observed action ˆs cid4 cid4 cid4 Note domains state ordinal symbolic components different distance measures Also distance function performs experiments article possible states close measure far behavior level resulting bad estimate similarity The use polynomial weights algorithm mitigates effects small number poor similarity estimates overwhelmed measure frequently incorrect 5 Pursuit results This section presents empirical analysis PLASTICs performance pursuit domain introduced Section 24 The ﬁrst tests PLASTICModel effective cooperating known teammates UCT performs planning algorithm PLASTICModel These results PLASTICModel outperforms matching teammates behaviors The tests PLASTICModel learn cooperate number unknown teammates given prior handcoded models potential teammates behaviors HandCodedKnowledge following tests handcoded models enable PLASTICModel cooperate variety previously unseen teammates The initial tests evaluate parts PLASTICModel planning effective select set models The set tests investigates core question paper PLASTICModel reuse knowledge learns previous teammates order speed teammates exhibiting unknown behaviors Our ﬁrst tests demonstrate PLASTICModel effectively learn models past teammates use models quickly adapt unknown teammates Furthermore results PLASTICModel effective new teammates drawn substantially different set previous teammates Additional tests TwoStageTransfer effec tive learning models new teammates small observations teammate combined observations past teammates Using TwoStageTransfer PLASTICModel allows ad hoc agent cooperate variety unknown teammates outperforming reusing previously learned models 51 Methods In pursuit domain ad hoc agent uses PLASTICModel select actions To employ PLASTICModel model pursuit domain MDP States MDP current positions agents actions cardinal directions stay The transition function deterministic collisions handled based random priority assigned time step The reward function returns 10 prey captured 0 In Sections 55 56 PLASTICModel learns models teammates discussed Section 423 Learning allows agent gain good set diverse models lifespan allowing better performance arbitrary new teammates The ad hoc agent builds models past teammates behaviors oﬄine selects learned models online cooperating new teammates It expected past teammates representative distribution future teammates future teammates seen To predict past teammates actions ad hoc agent uses decision tree learning algorithm predict actions teammates given state Rather use single number represent state ad hoc agent uses factored representation We assume resulting features informative states similar features result similar actions teammates The features Table 2 relative locations agents domain The features include predator currently neighboring prey cells prey occupied predators gives information direction predator ﬁll spots Also include numbers predator assigned case agents team specialized based number Finally previous actions succinct imperfect summary predators intentions expect predators likely continue current direction learning algorithm ﬁgures history predicts action To capture notion ad hoc agent expected extensive prior general domain expertise assumed ad hoc teamwork setting speciﬁc teammates hand PLASTICModel observes number past teammates Speciﬁcally watches teams predators 50000 steps past teammate type builds separate model type teammate Preliminary tests data effective focus S Barrett et al Artiﬁcial Intelligence 242 2017 132171 151 Table 2 Features predicting teammates actions Positions relative teammate Description Num features Predator number Prey x position Prey y position Predatori x position Predatori y position Neighboring prey Cell neighboring prey occupied Previous actions 1 1 1 3 3 1 4 2 Values 0 1 2 3 10 10 10 10 10 10 10 10 truefalse truefalse research minimizing observations current teammates previous ones We treat observations previous teammates experience prior deploying ad hoc agent If observations current teammates available improve results transfer learning discussed Section 56 We evaluate PLASTICModel compares baseline directly copying teammates behaviors Copying teammates behaviors tests team perform teammate matched team ad hoc team agent Two possible baselines ad hoc agent select actions randomly result team capturing prey Therefore baselines article We use following performance metric given 500 steps times predators capture prey Whenever prey caught randomly relocated predators try capture prey Results averaged 1000 trials statistical signiﬁcance tested Wilcoxon signedrank test p 001 52 Cooperating known teammates Before analyzing PLASTICModel effective cooperating unknown teammates ﬁrst informative test cooperate known teammates known task Speciﬁcally test performance handcoded teammates presented Section 241 PLASTICModel given prior knowledge form correct handcoded policy teammates behaviors HandCodedKnowledge Although ad hoc team agent model teammates scenario ad hoc teamwork setting opportunity team coordinate prior starting task agent determine strategy online We hypothesize PLASTICModel effectively plan deal known teammates outperform matching suboptimal behaviors When teammates task known ﬁnding optimal behavior PLASTICModel simpliﬁes planning algorithm As presented Appendix A1 Value Iteration VI planning algorithm guaranteed compute optimal behavior ad hoc agent computationally intensive calculate In order scale larger problems desirable use eﬃcient approximate methods Upper Conﬁdence bounds Trees UCT discussed Appendix A2 Ideally approximate solutions lose compared optimal solutions Therefore look performance different planning algorithms PLASTICModel baseline matching teammates behaviors Results sizes worlds given Fig 7 These results ad hoc agent better copying behavior teammates PLASTICModel In 5 5 world following optimal havior VI captures prey average 9282 8104 times respectively cooperating Greedy Teammateaware teammates opposed 6777 6388 times mimicking behavior The improvements planning mimicking teammates increase worlds larger VI scale compu tationally calculate optimal behavior worlds For example 20 20 world PLASTICModel UCT allows agent capture prey average 1507 times 500 steps cooperating Greedy Probabilistic teammates compared 612 times mimicking teammates behavior Similarly agent PLASTICModel captures prey 1447 times 260 times paired Probabilistic Destinations teammates All differences statistically signiﬁcant p 001 However approximate planning UCT PLASTICModel compromise performs nearly VI despite computation time In 5 5 world agent captures prey 9168 8045 times Greedy Teammateaware agents planning UCT opposed 9282 81043 times VI The difference performance lowered playouts UCT cost computation time Given close approximation optimal UCT provides important difference methods time takes plan On 5 5 world entire UCT episode takes 10 seconds compared VIs 12 hour computation VI needs run episode Furthermore UCT anytime algorithm handle variable time constraints modify plan online models change Given good performance UCT computational eﬃciency use planning algorithm PLASTICModel remainder section 152 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Fig 7 Results known handcoded teammates 53 Cooperating teammates drawn known set While Section 52 considers case ad hoc agent knows behaviors teammates ad hoc agent informed Instead ad hoc agents need adapt new teammates ﬂy Therefore expand problem considering case ad hoc agent encounter handcoded predators teammates know behavior current teammates The ad hoc agent know teammates drawn set handcoded predators In words PLASTICModel receives handcoded behaviors HandCodedKnowledge needs determine best represents teammates online This setting closer general ad hoc teamwork scenario shows ad hoc agent knows teammates drawn larger set A possible teammates These evaluations test PLASTICModel determine type teammates encounters adapt We hypothesize PLASTICModel outperform matching behaviors perform marginally worse PLASTICModel knows behaviors interacting In Sections 5456 explore setting larger set possible teammates Ideally assuming PLASTICModel set possible models teammates input able determine model correct plan model appropriately In setting PLASTICModel uses polynomial weights method described Section 421 maintain beliefs teammates types PLASTICModel given uniform prior teammate types BehaviorPrior PLASTICModel knows teammates homogeneous teams agents following Greedy behavior following Teammateaware behavior The results scenario displayed Fig 8 Differentiating deterministic teammate behaviors straightforward soon action expected deterministic behavior incorrect model removed However stochastic teammate behaviors diﬃcult differentiate signiﬁcant overlap actions possible We compare PLASTICModel given handcoded teammate behaviors HandCodedKnowledge version PLASTICModel given correct model teammates HandCodedKnowledge We baseline trying ﬁt teammates predesigned team denoted Match The results shown Fig 8 PLASTICModel statistically signiﬁcantly better Match scenarios In 5 5 world PLASTICModelAll statistically signiﬁcantly worse PLASTICModelTrue GR GP PD teammates In 10 10 world PLASTICModelTrue signiﬁcantly better PLASTICModelAll GR teammates 20 20 world PLASTICModelTrue signiﬁcantly better PLASTICModelAll GR PD teammates These results PLASTICModel able quickly determine S Barrett et al Artiﬁcial Intelligence 242 2017 132171 153 Fig 8 Results unknown handcoded teammates behaviors teammates losing small compared knows correct teammate behavior ahead time 54 Unmodeled teammates To point PLASTICModel beneﬁt having correct model teammates HandCoded Knowledge HandCodedKnowledge includes incorrect models However PLASTICModel fortunate Therefore consider case agents A PLASTICModel correct model HandCodedKnowledge We PLASTICModel handcoded teammate behaviors HandCodedKnowledge ad hoc agent encounters teammates drawn set To sure biased creation agents better ad hoc teamwork externallycreated teammates described Section 242 StudentBroad PLASTICModel exploits similarities current teammates behaviors past teammates behaviors teammates explicitly designed similar Nonetheless PLASTICModel able identify exploit similarities teammates coming variety developers Note agents team produced student mix match agents different students However students teams agents use behavior For following tests focus 20 20 world complex interesting small worlds We hypothesize PLASTICModel able determine models best ﬁt teammates use plan effectively cooperate teammates Our expectation PLASTICModel outperform matching behaviors outperformed planning true behavior known As explained depth Section 42 PLASTICModel maintains probabilities known models samples distribution planning While models correct PLASTICModel tries determine behaviors best matches current teammates behaving We compare 3 possible strategies ad hoc agent 1 Match match teammates behaviors 2 PLASTICModelTrue use PLASTICModel HandCodedKnowledge initialized current teammates true behavior 3 PLASTICModelHC use PLASTICModel 4 handcoded models provided HandCodedKnowledge 154 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Fig 9 Results unobserved externallycreated teams StudentBroad 20 20 world Strategies 1 2 require ad hoc agent know true behavior current teammates possible These strategies serve baselines compare strategy 3 represents true ad hoc scenario encountering previously unseen teammates The results Fig 9 ad hoc agents despite incorrect models All differences statistically signiﬁcant For example PLASTICModel agent captures prey 1336 times 500 steps 997 times matched teammates behaviors This result surprising assume planning incorrect model perform worse playing behavior students agent ad hoc agent replaced While loss compared ad hoc agent knew true behavior teammates 4 handcoded models representative externallycreated teammate achieve good results This experiment shows possible agent cooperate unknown teammates set known representative models 55 Learning teammates In section explore scenario ad hoc agent previously observed number past teammates These past teammates expected similar current teammates Ideally ad hoc agent able use observations past teammates better cooperate current teammates To end section evaluates PLASTICModel learns models past teammates selects models Speciﬁcally PLASTIC Model observes past teammate total 50000 steps This number chosen agent excess information learning models care speed learning Then PLASTICModel learns decision tree represent behavior past teammate discussed Section 423 This section tests hypothesis PLASTICModel learn models past teammates reuse learned models cooperate new teammates The results approach marginally loses compared knowing teammates true behaviors outperforms matching behaviors The teammates section StudentBroad described Section 242 These teammates externallycreated designed students class assignment We consider 5 behaviors ad hoc agent 1 Match match teammates behaviors 2 PLASTICModelTrue use PLASTICModel HandCodedKnowledge initialized current teammates true behavior 3 PLASTICModelCorrectLearned use PLASTICModel PriorTeammates current teammates 4 PLASTICModelSetIncluding use PLASTICModel PriorTeammates including 29 possible teammates StudentBroad including current ones 5 PLASTICModelSetExcluding use PLASTICModel PriorTeammates including 28 possible teammates StudentBroad excluding current ones Once strategies 1 2 serve baselines require knowledge current teammates true behaviors PLASTIC ModelCorrectLearned evaluates performance learning algorithm PLASTICModel knows teammates agent cooperating uses past observations teammates learn model PLASTIC ModelSetIncluding evaluates general ad hoc teamwork scenario current type teammate unknown current teammates previously observed Finally PLASTICModelSetExcluding shows true ad hoc teamwork scenario ad hoc agent seen current teammates uses PLASTICModel reuse knowl edge learned previous teammates Fig 10 shows performance ﬁve approaches differences statistically signiﬁcant PLASTICModelTrue shows unattainable level performance requires perfect knowledge current teammates However learning model observing current teammates lose performance shown PLASTICModelCor S Barrett et al Artiﬁcial Intelligence 242 2017 132171 155 Fig 10 Results PLASTICModel learning models previously observed teammates encountering teams StudentBroad 20 20 world Fig 11 Results PLASTICModel learning models previously observed teammates encountering teams StudentSelected 20 20 world rectLearned line Furthermore having observed teammates needing select past teammates generate loss shown PLASTICModelSetIncluding line Finally PLASTICModelSetExcluding shows performance PLASTICModel encountering previously unseen teammate Its performance shows models learned previous teammates good job capturing behavior new teammates This problem true ad hoc teamwork problem ad hoc agent encounters teammates prior knowledge The gap PLASTICModelSetExcluding PLASTICModelSetIncluding shows room improve new teammates It possible agents created class biased similar agents StudentBroad share characteristics Therefore like test learned models allow PLASTICModel cooperate teammates drawn set In scenario learn models StudentSelected teammates Instead evaluate PLASTICModel performs given StudentBroad PriorTeammates encounters team mates StudentSelected Speciﬁcally PLASTICModel learns 29 models teammate behavior StudentBroad encounters 30th teammate drawn StudentSelected Fig 11 gives results tests differences statistically signiﬁcant Once PLASTICModelTrue shows upper bound performance PLASTICModel given information true behavior team mates accessible scenarios However PLASTICModelSetExcluding performs showing learned models generally useful This approach far outperforms matching teammates behaviors despite inaccuracies models For visual comparison videos ad hoc agents PLASTICModel adapt teammates videos ad hoc agents strategies online10 56 Learning new teammates The previous section assumes PLASTICModel observed previous teammates current teammates If instead PLASTICModel observes current teammates small number steps try use information learn new model teammates However given learning current teammates care speed learning Therefore PLASTICModel combines information coming previously observed 10 http wwwcs utexas edu larg index php Ad _Hoc _Teamwork _Pursuit 156 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Fig 12 Comparing different transfer learning algorithms PLASTICModel improve results ad hoc agents limited observations current teammates Tests 20 20 world StudentBroad teammates teammates learn better model Speciﬁcally setting permits PLASTICModel use transfer learning learn better model current teammates These evaluations test hypothesis transfer learning allows PLASTICModel narrow gap PLASTICModelSetExcluding PLASTICModelCorrectLearned seen previous section In tests assume ad hoc agent previously observed 50000 training steps past 28 team mates StudentBroad In addition seen 100 training steps current teammates Note signiﬁcantly testing time 500 steps testing begins PLASTICModel learning online adapting belief distribution possible models PLASTICModel improve models focus evaluating transfer learning algorithms ﬁxed observations current teammates Both past current teammates test taken StudentBroad We evaluate different transfer learning algorithms TwoStageTrAdaBoost TrAdaBoost TrBagg TwoStageTransfer discussed Section 32 PLASTICModel The goal transfer learning produce model performs target data set current teammates source data sets past teammates We hypothesize TwoStageTransfer perform best explicitly reasons fact source data coming multiple sources In addition compare transfer learning algorithms performance purely reusing previously learned teammate mod els PLASTICModelSetExcluding All transfer learning algorithms use decision trees base learning algorithm To evaluations fair possible TwoStageTransfer TwoStageTrAdaBoost 10 different weightings In TrAdaBoost TwoStageTrAdaBoost 10 boosting iterations For TrBagg total 1000 sets training classiﬁers Naive Bayes classiﬁer served fallback model Each algorithm set parame ters tuned values chosen preliminary tests based performance computational tractability Fig 12 shows results transfer learning algorithms subroutines PLASTICModel differ ences statistically signiﬁcant In PLASTICModel learning models performed oﬄine model selection happening online evaluation One baseline comparison PLASTICModel ignores previ ously observed teammates learns new model observed 100 steps current teammates shown PLASTICModelCorrectLearned100 As upper baseline compare unattainable performance ver sion PLASTICModel observes 50000 steps current teammate shown PLASTICModelCorrectLearned50000 represents best performance attainable models learned given large amounts data In results TwoStageTransfer statistically signiﬁcantly outperforms transfer learning algorithms In ad dition combining models learned TwoStageTransfer models learned representative teammates PLASTICModelTwoStageTransfer SetExcluding setting helps reaching results statistically signiﬁcantly bet ter PLASTICModelSetExcluding TrBagg performed poorly setting mistransferring information possibly fallback model balance target source data Several values parameters tested performance remained similar In addition important target data TwoStageTransfer needs perform Therefore vary order magnitude target data run PLASTICModelTwoStageTransferi target data provided Fig 13 shows results varying amounts target data constant amounts source data The difference results 1000 steps target data 100 statistically signiﬁcant differences 10000 1000 100 10 The results performance TwoStageTransfer improve target data improvement smooth These results observations 10 steps current teammates suﬃcient TwoStageTransfer perform produce useful models scenario We 100 steps Fig 12 transfer learning methods data perform adequately results TwoStageTransfer signiﬁcantly outperforms S Barrett et al Artiﬁcial Intelligence 242 2017 132171 157 Fig 13 Evaluating PLASTICModel TwoStageTransfer varying amounts target data Tests 20 20 world StudentBroad teammates 57 Summary This section showed PLASTICModel enables ad hoc team agents cooperate variety handcoded externallycreated teammates pursuit domain PLASTICModel gets good results given set handcoded behaviors HandCodedKnowledge experienced number previous teammates PriorTeammates PLASTICModel performs seen current teammates ad hoc agent experi ence previous teammates exhibit similar behaviors Furthermore transfer learning learn new models quickly allowing PLASTICModel quickly adapt new teammates Speciﬁcally combining models learned TwoStageTransfer models past teammates outperforms approaches considered PLASTICModel allows ad hoc agent adapt variety teammates Also agents behavior differs greatly cooperating different teammates indicating following single policy teammates These results PLASTICModel allows agents reuse knowledge previous teammates quickly adapt exploiting similarities observed teammates behaviors pursuit domain 6 Half ﬁeld offense results The previous section showed PLASTIC allows ad hoc agents cooperate variety teammates pursuit domain given prior experiences teammates However pursuit domain requires number agents cooperate simple compared realistic scenarios Therefore section looks scaling PLASTIC complex domain half ﬁeld offense HFO described Section 25 All past research HFO focused creating teams precoordinated section shows PLASTIC handle unknown teammates prior coordination Given complexity HFO planning UCT requires samples runs issues imperfect modeling environment Therefore evaluate PLASTICPolicy HFO PLASTICPolicy effective setting avoids complexity modeling domain teammates Instead PLASTICPolicy directly learns policies cooperating previous teammates selects policies online current teammates PLASTICPolicy described depth Section 43 The results section HFO domain PLASTICPolicy effective allowing ad hoc team agent reuse knowledge past teammates improve cooperation teammates Our tests evaluate PLASTICPolicy teams competed 2013 RoboCup 2D Simulation League complex developed years discussed Section 251 Despite complexity PLASTICPolicy able learn policies cooperat ing type teammate results policies specialized teammate type Therefore PLASTICPolicys approach maintaining probabilities teammate type selecting best policy signiﬁcantly outperforms approaches This section shows PLASTIC scale domains continuous states phisticated teammates 61 Grounding model Before discuss learn act HFO important understand model problem Therefore section describes model HFO domain MDP 611 State A state s S describes current positions orientations velocities agents position velocity ball In article use noiseless versions values permit simpler learning 158 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 612 Actions In 2D simulation league agents act selecting dash turn kick specify values power angle kick Combining actions accomplish desired results diﬃcult problem Therefore article builds code release Helios 3 This code release provides number high level actions passing shooting moving speciﬁed point We use 6 high level actions agent ball 1 Shoot shoot ball goal avoiding opponents 2 Short dribble dribble ball maintaining control 3 Long dribble kick ball chase 4 Pass0 pass teammate 0 5 Pass1 pass teammate 1 6 Pass2 pass teammate 2 Each action considers number possible movements ball evaluates effectiveness given locations agents opponents teammates Each action represents number possible actions reduced discrete actions agent2d evaluation function While high level actions restricts possibilities agent enables agent learn quickly prune ineffective actions allowing select intelligent actions fewer samples Additionally agent select moves away ball As agent continuous turn action continuous dash action time step helpful use set high level actions case 7 1 Stay current position 2 Move ball 3 Move opposing goal 4 Move nearest teammate 5 Move away nearest teammate 6 Move nearest opponent 7 Move away nearest opponent These actions provide agent number possible actions adapt changing environment constraining number possible actions 613 Transition reward functions The transition function deﬁned combination simulated physics domain actions selected agents The agent directly model function instead stores samples observed played games described Section 62 The reward function 1000 offense wins 1000 defense wins 1 time step taken episode The value 1000 chosen greater effects step rewards episode great completely outweigh effects Other values tested similar results 62 Methods PLASTICPolicy learns policies cooperating previously encountered team In article use Fitted Q Iteration FQI introduced Ernst et al 28 We treat action going agent possession ball action ends agent holds ball episode ended Given control single agent teammates follow policies The agent collects data actions teammates form cid4cid15 agents actions The agent directly store actions teammates instead storing cid14s r s resulting world states include effects teammates actions If controlled agents consider action teammates perspectives The agent observes 100000 episodes HFO type teammate These episodes contain agents actions agent ball away ball There ways represent state game half ﬁeld offense Ideally want compact representation allows agent learn quickly generalizing knowledge state similar states overconstraining policy Therefore select 20 features given 3 teammates X position agents x position ﬁeld Y position agents y position ﬁeld Orientation direction agent facing Goal opening angle size largest open angle agent goal shown θg Fig 14 Teammate goal opening angle teammates goal opening angle Distance opponent distance closest opponent S Barrett et al Artiﬁcial Intelligence 242 2017 132171 159 Fig 14 Open angle ball goal avoiding blue goalie open angle ball yellow teammate For interpretation references color ﬁgure legend reader referred web version article Distance teammate opponent distance teammate closest opponent Pass opening angle open angle available pass teammate shown θp Fig 14 63 Evaluation setup Results averaged 1000 trials consisting series games half ﬁeld offense In trial agent placed team randomly selected 7 teams described Section 251 Performance measured fraction time resulting team scores In article use variations HFO task 1 limited version offensive players attempting score defenders including goalie 2 version attackers attempting score ﬁve defenders In order run existing teams RoboCup competition necessary ﬁeld entire 11 player team agents behave correctly Therefore necessary create entire team constrain additional players stay away play agents needed half ﬁeld offense This approach alter behavior players HFO initial tests suggested resulting teams perform task We choose ﬁxed set player numbers teammates based player numbers tended play offensive positions observed play The defensive players use behavior created Helios limited version HFO In HFO defense uses agent2d behavior provided code release Helios 3 We compare strategies selecting policies learned playing previously encountered teammates The performance bounded Correct Policy line agent knows teammates behavior type policy use The lower bound performance given Random Policy line agent randomly selects policy use The Combined Policy line shows performance agent learns single policy data collected possible teammates representing agent treating single agent learning problem instead ad hoc teamwork problem Other options baselines ad hoc agent select actions randomly However setting behaviors result offense scoring Thus baselines excluded results We compare intelligent methods selecting models described Section 432 Speciﬁcally agent decide 7 policies follow know new teammates behavior type The Bandit line represents PLASTICPolicy uses cid8greedy bandit algorithm select policies Other bandit algorithms tested values cid8 cid8greedy cid8 01 linearly decreasing 0 length trial outperformed methods The PLASTICPolicy line shows performance approach lossbounded Bayesian updates maintain probabilities previously learned policy use We set η 01 updating probabilities models Equation 3 We model noise predicting actions normal distribution This noise affects loss function controlling probability function P actionsmodel For differences distance predictions use σ 40 orientation differences use σ 40 64 Limited half ﬁeld offense Our ﬁrst set results limited version HFO game uses 2 offensive players competing 2 defenders including goalie Therefore agent needs adapt single teammate This limited version problem reduces number state features 8 number actions holding ball 4 number actions away ball stays 7 These evaluations test hypothesis PLASTICPolicy quickly converge selecting best policy losing small compared correct policy In addition hypothesize PLASTICPolicy converge faster banditbased approach outperform combining data agents learn single combined policy The results shown Fig 15 error bars showing standard error The difference Correct Policy Random Policy lines shows selecting correct policy use important agent adapt teammates The gap Correct Policy Combined Policy shows knowing correct teammate better grouping teammates While Bandit line 160 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Fig 15 Scoring frequency limited half ﬁeld offense task Fig 16 Belief probability correct model P m calculated PLASTICPolicy limited HFO task s probability correct model having highest probability P p max pi s learning Fig 15 continue learning time Its performance converges scoring 0418 time approx imately 10000 episodes score approximately equal combined policy 0382 1750 episodes Its slow speed fact observations noisy estimates policies effectiveness received game HFO In addition scoring fractions different strategies fairly close determining best given noise diﬃcult On hand PLASTICPolicy line shows fast improvement converging performance Correct Policy line This quick adaptation factors 1 better estimations policy ﬁts teammates 2 frequency updates The estimations probabilities better measure agent moves noisy estimate policy performs The updates performed action episode updates frequent These factors combine result fast adaptation new teammates PLASTICPolicy The differences performance PLASTICPolicy Combined Policy Bandit statistically signiﬁcant population binomial test p 001 episodes shown Fig 15 Note PLASTICPolicy outperforms Correct Policy ﬁnal episodes difference noise statistically signiﬁcant Videos performance PLASTICPolicy compared strategies viewed online11 To understand learning PLASTICPolicy useful look beliefs shown Fig 16 This graph shows probability correct model current teammates probability correct model highest probability ties contributing probability tied While probability correct model takes 15 episodes reach 90 probability correct model maximal model 90 time 5 episodes This result explains taking maximal model gives good performance PLASTICPolicy Note choosing maximal model create premature convergence action teammates allows PLASTICPolicy update probability teammates 1 11 http wwwcs utexas edu larg index php Ad _Hoc _Teamwork _HFO S Barrett et al Artiﬁcial Intelligence 242 2017 132171 161 Fig 17 Scoring frequency half ﬁeld offense task Fig 18 Belief probability correct model P m PLASTICPolicy HFO task s probability correct model having highest probability P p max pi s In summary results section PLASTICPolicy effective cooperating unknown teammates complex domain continuous state continuous actions PLASTICPolicy able learn policies cooperating previous teammates quickly select policies eﬃciently cooperate new teammates 65 Full half ﬁeld offense Our second set results HFO game 4 offensive players versus 5 defenders including goalie In setting agent needs adapt teammates score ﬁve defenders This setting tests hypothesis PLASTICPolicy learn intelligent policies cooperating teammates quickly select policies cooperating unknown teammates We expect PLASTICPolicy outperform se lecting policies banditbased approach learning single policy cooperate teammates In addition hypothesize PLASTICPolicy lose marginally compared gold standard knowing best policy interacting teammates The results setting shown Fig 17 As Section 64 upper bound performance given Cor rect Policy lower bound given Random Policy The Bandit setting learns slowly reaching performance 0357 approximately 20000 episodes It outperforms combined policy 0350 12000 episodes Once PLASTICPolicy quickly converges correct policys performance outperforming Bandit Combined lines These results PLASTICPolicy quickly learns cooperate unknown teammates Using population binomial test p 005 PLASTICPolicys performance statistically signiﬁcantly better Combined Policy Bandit episode 3 For visual comparison videos ad hoc agents PLASTICPolicy strategies cooperate teammates viewed online12 12 http wwwcs utexas edu larg index php Ad _Hoc _Teamwork _HFO 162 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 We look PLASTICPolicys beliefs time Fig 18 In ﬁgure PLASTICPolicy takes episodes convinced correct model true model current teammates noise teams actions However greedily selecting highest probability models corresponding policy performs correct model maximal 90 time 5 episodes This result shows PLASTICPolicy learns quickly advantage continuing exploration teammates despite selecting believes current best policy 66 Summary The results section demonstrate PLASTICPolicy scale complex domains requiring coordinating teammates continuous states continuous actions PLASTICPolicy eﬃciently select good policies cooperat ing current teammates set policies learned cooperating past teammates Playing policies perform poorly spread possible teammates However PLASTICPolicy able quickly select good policy speciﬁc teammates cooperating 7 Related work While preceding sections discuss research problem approach tackle problem section focuses instead problem situating article literature We ﬁrst provide overviews areas closely related ad hoc teamwork beginning general multiagent coordination research moving research opponent modeling We follow discussion selected works use domains discussed article Finally conclude discussion current research ad hoc teamwork While large work related research article brings new ideas table With respect ad hoc teamwork article moves ad hoc teamwork empirical setting tackles complex problems studied previously Rather requiring shared communication coordination protocols like past research multiagent teams work describes agents cooperate shared protocols Compared opponent modeling work creates agents adapt quickly agents price making stronger assumption cooperating shared goal Furthermore article expands formulation pursuit RoboCup domains include teammates come variety sources need learn cooperate ﬂy 71 Multiagent teams A large body research coordinating multiagent teams exists specifying standardized protocols communication shared algorithms coordination One algorithm STEAM 68 team members build partial hierarchy joint actions In addition agents monitor progress plans adapting plans conditions change STEAM designed communicate selectively reducing communication required coordinate team STEAM shown effective number domains ranging simulated RoboCup soccer domains controlling number autonomous helicopters attack scenarios Another approach coordinating multiagent teams Generalized Partial Global Planning GPGP 25 GPGP works heterogeneous teams variety tasks allows number modular coordination mechanisms Decker Lesser ﬁnd different coordination mechanisms perform better different tasks endorse idea library coordination mechanisms choose new tasks STEAM GPGP expect agents know teams goal tasks required accomplish goal However line research explores domains information known instead agents learn tasks required accomplish goals An example approach Lauer Riedmillers work introduces Distributed Qlearning 53 learning multiagent fully cooperative settings The authors model problem multiagent MDP adopt model free approach In Distributed Qlearning agent maintains local policy Qvalue function depends actions Using approach agents policies converge optimal jointactions deterministic domains However approaches expect entire team shares coordination algorithm PLASTIC require shared communication coordination protocols assume teammates necessarily adapting ad hoc team agent 72 Opponent modeling The work discussed previous section assumes team cooperative trying accomplish shared goal Another scenario occur multiagent teams fully competitive setting agents attempt achieve mutually exclusive goals Opponent modeling research explores problem explicitly modeling agents domain While research cooperative teams appears similar ad hoc teamwork given agents trying accomplish shared goals settings opponent modeling similar ad hoc teamwork This similarity stems importance understanding reasoning agents domain opponent modeling S Barrett et al Artiﬁcial Intelligence 242 2017 132171 163 necessary robust ad hoc team agents Rather trying represent entire ﬁeld opponent modeling section summarizes major lines inquiry problem relevant article One line inquiry theoretically motivated exploring proven interacting opponents An algorithm shows reasoning AWESOME algorithm 23 AWESOME learning algorithm repeated normal form games When plays opponents use algorithm AWESOME agents converge playing Nash equilibrium optimal behavior agents rational When playing stationary opponents AWESOME agent learns exploit optimally These results algorithm exploit simple opponents getting exploited smart agents In vein theoretical analysis Chakraborty Stone developed CMLeS algorithm 21 CMLeS extends exploiting memorybounded teammates retaining convergence Nash equilibrium self play CMLeS reverts playing maximin strategy playing adversaries memorybounded retaining good payoffs worst case scenario Another exciting line opponent modeling research game theory solve real world security problems For example Korzhyk et al 52 discuss use Stackelberg game model design intelligent strategies This work applied deploying security LAX airport scheduling US Federal Air Marshals In Stackelberg games leaders act ﬁrst opponents observe actions responding The solution problem robust opponents actions minimizing risk This line research shows game theoretic approaches applied real world problems great effect minimizing resources required protect resource maximizing safety In research authors generally assume agents opponents act optimally ad hoc teamwork agents teammates act optimally One avenue research combines theoretical analysis empirical results area poker For example Bard et al 10 look adapt opponents strategy headsup limit Texas holdem poker The authors approximate Nash equilibrium strategy use counterfactual regret CFR 78 limits agent exploited To adapt weaknesses players remaining robust exploited Bard et al learn portfolio counter strategies limited best responses expected opponents behaviors 73 Experimental domains Isaacs performed seminal research pursuit evasion 44 problem explored Benda et al 15 Benda et al explore varying predators ability communicate considering central strategy communica tion carries cost In setting communication limited lower computation time taken step More research range research possibilities pursuit domain explained survey paper Stone Veloso 66 Most previous research focused developing coordinating predators deploying learning adapt unseen teammates For example MAPS 70 considers pursuit problem dynamic partially observable environments In work prey intelligently plans actions avoid capture MAPS uses coordination strategies position predators locations cover possible escape directions prey Ishiwaka et al 45 focus pursuit task predators learning online reinforcement learning The predators start homogeneous agents diverge learning process This work considers pursuit task partial observability continuous domain In addition research performed pursuit domain substantial amounts research RoboCup including walking positioning vision However relevant research RoboCup area team coordination One early exploration learning RoboCup simulation domain performed Stone 62 This book describes ﬂexible team structure novel learning approach called layered learning In particular introduces concept lockerroom agreement Lockerroom agreements predetermined multiagent protocols deﬁne ﬂexible teamwork structure interagent communication protocols However work relies entire team sharing lockerroom agreement assumed ad hoc teamwork Another aspect research RoboCup characterize teams behaviors complex multiagent systems Almeida et al 9 explore problem RoboCup 2D simulated robotic soccer league logs play input information The authors explore complexity teams behaviors discovering guidelines creating new plans teamwork However proposed approach requires substantial observations team usually available ad hoc teamwork scenarios One set online adaptations agents Small Size League SSL RoboCup Biswas et al 16 explore plan opponents strategies Their approach attempts pull opponents position leaving openings strategy exploit In addition detect potential threats based positions opponents adapt defend threats These online adaptations agents adapt agents domain ﬂy single game While general approach applied multiagent settings current version relies strong assumptions domain RoboCup encourages substantial research multiagent systems complex domains The majority research focuses coordinated teams agents directly applicable ad hoc teamwork 164 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 74 Transfer learning In Section 32 discuss transfer learning algorithm TwoStageTransfer 14 use transferring knowledge past teammates current ones However transfer learning algorithms TrAdaBoost 24 boostingbased algorithm source target data sets lumped model learned boosting TwoStageTrAdaBoost 57 designed response problems TrAdaBoost overﬁtting training data Therefore TwoStageTrAdaBoost ﬁrst searches set possible weightings source data points determines weighting best crossvalidation While transfer learning algorithms described focus boosting bagging approaches shown promise speciﬁcally form TrBagg 50 The TrBagg algorithm uses bootstrap sampling create number data sets taken combined source target data sets Then model learned data set models undergo ﬁltering phase cross validation determine models helpful There research transferring knowledge multiple sources Yao Doretto 74 introduced transfer learning algorithms handling multiple sources The ﬁrst MultiSourceTrAdaBoost uses instancetransfer approach reusing data source tasks training target classiﬁer Alternatively TaskTrAdaBoost employs parametertransfer approach assumed target data shares parameters source data sets Another approach transfer learning multiple source sets work Huang et al 42 They pro pose SharedBoost algorithm select best features prediction small number source data sets text classiﬁcation Zhuang et al 77 investigate autoencoders determine feature mapping allows train multiple classiﬁers source domain apply effectively target domain Similarly Fang et al 29 introduce algorithm determines shared subspace labels multiple sources sample given multiple labels Then subspace transfer knowledge target domain Another approach veloped Ge et al 30 The authors introduce online algorithm transfers knowledge multiple sources principled way achieving noregret guarantee compared oﬄine algorithm These algorithms provide promising step effectively handling multiple sources Further progress transfer learning multiple sources lead exciting advances area ad hoc teamwork incorporated learning component PLASTIC 75 Ad hoc teamwork A signiﬁcant portion research ad hoc teamwork takes theoretical approach problem These approaches focus simple settings bandit domain matrix games try prove optimality approach certain conditions Other researchers focus empirical approaches showing algorithms apply ad hoc teamwork problems practice Let ﬁrst consider theoretical contributions coming analysis ad hoc teamwork bandit domain One example theoretical approach bandit domain Stone Krauss research 65 In work authors consider multiagent version setting knowledgeable agent attempting teach novice teammate This work differs existing teaching literature teacher embedded domain teaching actions explicit cost This line research proves ad hoc agent acting teacher optimally lead teammate achieve best team payoff These papers differ work article assume novice teammates policy known TeamK 1 results directly applicable bandit domain consider domains large proven tractable similar fashion While bandit domain allows multiagent research majority work single agent focusing ad hoc teamwork problem A common domain looking interactions agents matrix games agents act simultaneously receive rewards This domain allows multiagent interactions remains simple theoretical analysis An early paper looks ad hoc teamwork matrix games Brafman Tennenholz 19 In paper investigate agents performing repeated joint task agent attempts teach novice agent The authors affect ad hoc agent agent acting teacher In work use gametheoretic framework consider teammates maximize expected utility use reinforcement learning Overall consider number strategies agent play including reinforcement learning agent established handcoded policies Building idea Stone et al 64 investigate ad hoc teamwork matrix games theoretical focus They explore ad hoc agent cooperate best response teammate maximizing teams shared rewards Best response agents choose action gives highest payoff assuming teammates continue playing observed action In work ad hoc agent knows payoff matrix teammates behavior TeamK 1 diﬃculty plan optimal path lead best response teammate best payoff This work expanded Agmon Stone 2 include teammate Agmon Stone best payoff reachable team larger agents come way describing optimal team payoff optimal steady cycle lead team cycle This work expanded case teammates behaviors fully known 1 TeamK 1 instead assuming ad hoc agent knows teammates behavior known set The authors algorithm REACT balances potential S Barrett et al Artiﬁcial Intelligence 242 2017 132171 165 costs different assumptions teammates behaviors REACT empirically performs large number matrices This line research differs article focus theoretical analysis limits work simple matrix game setting In addition article considers wider range possible teammate behaviors cases ad hoc agent knowledge teammates Stone et al 64 Agmon Stone 2 assume teammates behaviors known Ag mon et al 1 relax assumption instead assuming teammates behaviors drawn known set Chakraborty Stone 22 relax knowledge teammates behaviors ad hoc teamwork scenarios matrix games This work extends earlier work Chakraborty Stone 21 opponent modeling The authors propose new algorithm LCM tries achieve optimal performance teammates use limited features derived history With teammates LCM ensures team receives security value matrix game LCM determining features best explain teammates behavior set features explains behavior LCM verts playing safety strategy This approach performs optimally teammates form learning takes substantially longer PLASTIC Unlike PLASTIC LCM guarantee safety value teammates practice safety value low compared team receive ensuring safety value complex tasks requiring coordination impossible While matrix games serve good testbed looking interactions agents ad hoc teamwork scenarios limited stateless interactions Wu et al 72 scale theoretical analysis ad hoc teamwork complex theoretically tractable domains In work authors investigate ad hoc teamwork assumptions behaviors teammates Their ad hoc agent plans MCTS uses biased adaptive play predict actions teammates Biased adaptive play estimate policies teammates previous actions They test agent domains cooperative box pushing meeting 3 3 grid multichannel broadcast They consider case ad hoc agent knows environment teammates These teammates referred unknown teammates UTM types teammates domain UTM1 agents follow ﬁxed set actions UTM2 agents try play optimal behavior partial observations Their work shows approach adapt teammates accomplish tasks While work explores domains domains fairly simple Additionally ad hoc agent given large expert knowledge set possible teammates limited compared article Another approach scaling ad hoc teamwork matrix games work Albrecht Ramamoorthy 4 5 consider matrix games work In work consider case ad hoc agent given set possible types teammates introduce new formal model represent problem In setting problem ad hoc agent determine type teammates Their approach HBA combines idea Bayesian Nash equilibria Bellman optimality equation HBA maintains probability provided teammate types maximizes expected payoffs according Bellman principle In later research 6 Albrecht Ramamoorthy explore convergence bounds HBA Speciﬁcally prove convergence bounds ad hoc agent knows teammates drawn known set consider accurate expertprovided types need HBA solve task This line research closely related article differs notable ways Given similarity HBA PLASTIC expect analysis generalized PLASTIC However HBA assumes agent provided expert knowledge possible teammates In article PLASTIC learn previous teammates apply imperfect learned models cooperate new teammates Furthermore PLASTIC scales complex domains evaluate HBA Speciﬁcally reusing precalculated policies adapt new teammates allows PLASTICPolicy handle substantially harder problems explored Albrecht Ramamoorthy While theoretical analysis problems create exciting new algorithms ad hoc teamwork important question algorithms fare complex empirical analyses A work looks complex coordination ad hoc teams Bowling McCracken 18 In domain robot soccer Bowling McCracken measure performance ad hoc agents ad hoc agent given playbook differs teammates In paper play refers team plan speciﬁes play applies termination conditions roles players In domain teammates implicitly assign ad hoc agent role react teammate The ad hoc agent analyzes plays work best hundreds games predicts roles teammates play This work explores similar setting work learns signiﬁcantly longer time scale However focus agents constrained similarly designed different playbook designed completely independently In addition approach relies having playbook possible plays speciﬁes roles agents team In domains agents playbook approach directly applied domains Jones et al 46 consider robotic ad hoc teams expand analysis heterogeneous robots The authors explore ad hoc teams operating treasure hunt domain implement algorithms real heterogeneous robots searching new environments treasure The authors focus agents allocate roles team decen tralized fashion However assume agents share communication protocol use bid different roles auction shared coordination protocol assign tasks given communication This article explores scenarios shared protocols exist present ad hoc teamwork scenarios 166 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 While previous works mainly focus small teams agents research affect large teams agents Speciﬁcally researchers investigated use small number agents control ad hoc agents affect behavior ﬂocks agents ﬂocks birds ﬁsh The ad hoc agents encourage team reach speciﬁed goal An early paper area written Han et al 38 prior Stone et als formulation ad hoc teamwork This work focused adding shill agent ﬂock corresponds ad hoc team agent terminology This agent designed authors attempts ﬂock desired direction Further research line includes work Genter et al 32 Genter Stone 33 This research proves bounds number actions required control ﬂocks behavior In addition provide algorithm empirically outperforms methods shortterm lookahead planning The authors expand problem consider multiple ad hoc agents This line research differs article focuses scenarios teammates behaviors known TeamK 1 needing learn teammates While previous works consider ad hoc agent act improve performance team consideration choose agents form new team given larger set possible agents Liemhetcharat Veloso 54 explore idea selecting agents form new ad hoc team Given different agents better performing different roles team important select agents ﬁll roles beneﬁcial way In addition synergies team pairs agents work better agents These complexities lead interesting questions select teammates set agents The authors come novel representation problem called synergy graph learn graph While investigates ad hoc teamwork research focuses problem selecting agents ad hoc team question explored article agent act ad hoc team One way Interactive POMDP IPOMDP 34 IPOMDPs model adversarial interactions agents exam ining agent believes agents agents beliefs The graphical counterparts IPOMDPs known Interactive Dynamic Inﬂuence Diagrams IDIDs 27 IDIDs provide concise representations beliefs agents allow nesting IDIDs represent beliefs Both IPOMDPs IDIDs model problems studied article However issues potential exponential blowup beliefs size problem grows While work performed increase eﬃciency algorithms models 617576 remain computationally intractable size problems studied article IPOMDP Lite 41 especially relevant formulation IPOMDPs improves scalability modeling agents nested MDPs Current implementations IPOMDP Lite shown scale problems 18000 states In pursuit domain approximately 1013 states HFO domain states continuous inﬁnite Applying IPOMDP Lite ad hoc teamwork exciting area future research current instantiations shown scale size problems considered paper 751 Dimension analysis For related work ad hoc teamwork helpful informative consider problems fall dimensions described Sections 21 We like calculate exact values dimensions domains Sections 24 25 calculation requires information exact formulations domains teammates typically available publications available Therefore instead rough estimates problems lie dimensions The dimensions consider team knowledge environment knowledge reactivity teammates We begin discussing team knowledge TeamK ad hoc agents domains shows ad hoc agent knows teammates prior cooperating The majority related research considers cases teammates known TeamK 1 close 1 Notable exceptions include Liemhetcharat Velosos work 54 Wu et als work 72 consider completely unknown teammates TeamK 0 Also Agmon et al 1 Albrecht Ramamoorthy 5 assume teammates drawn known set TeamK 0 1 estimate TeamK lies range 03 07 works Additionally Bowling McCracken 18 consider situations teammates share codebook ad hoc agent We estimate TeamK fairly high settings range 06 08 given effective soccer plays similar compared random movement teammates From analysis existing works focus situations teammates fairly known consider scenarios teammates initially unknown Let consider environmental knowledge EnvK domains environmental knowledge ex plains ad hoc agent knows transition reward dynamics domain beginning The majority works section assume ad hoc agent knowledge domain EnvK 1 1 settings While noise domains ad hoc agent expected know level noise know true distribution states Exceptions include Jones et al 46 Liemhetcharat Veloso 54 ad hoc agent initially know reward function limited knowledge transition function We estimate knowledge transition function lies 07 10 works reward knowledge lies 0 05 This analysis suggests research ad hoc teamwork focused learning domain Instead agents assumed know domain instead focus learning teammates planning cooperate S Barrett et al Artiﬁcial Intelligence 242 2017 132171 167 The reactivity teammates domains Reactivity covers large spread values All domains assume teammates partially reactive ad hoc agents worth considering problem multiagent setting This reactivity varies signiﬁcantly based domain When ideal actions fairly obvious teammates interactions ad hoc agent unlikely change teammates behaviors leading Reactivity close 0 On opposite end spectrum teammates high uncertainty best actions ahead time ad hoc agents actions signiﬁcantly affect actions leading values Reactivity close 1 Given research assumes ad hoc agents know teammates domain majority focus plan cooperate effectively teammates Exploring planning ad hoc teamwork encourages researchers investigate settings varying amounts teammate reactivity dimension inﬂuential planning While calculating exact values dimensions TeamK EnvK Reactivity domain studied related work useful impossible calculate values complete knowledge domain teammates Even rough estimates dimensions problems lead interesting clusions Speciﬁcally existing research good job exploring plan cooperate teammates covering gamut teammate reactivity On hand ad hoc team research focused largely problems high team knowledge high environmental knowledge work exploring agents learn teammates domain Future work ad hoc teamwork address gap explore settings ad hoc agent needs learn teammates domain In real world scenarios robots need constantly adapting changing environments new teammates encounter Therefore important ad hoc teamwork research explore settings agents reason tradeoff exploring domain exploring teammates exploiting current knowledge 752 Summary In summary section presented survey research ad hoc teamwork relevant article A large works focus simple domains provide theoretical analyses In addition substantial number assume know teammates share communication coordination protocols works ad hoc teamwork agents designed developers provided protocols decentralized However works consider ad hoc teamwork problems investigated article teammates completely unknown prior coordination Finally article work aware learns previous teammates reuses knowledge quickly adapt new teammates 8 Conclusion Given growing numbers agents world form robots software agents necessity agents cooperate achieve goals However cooperating predesigned teams agents need cooperate variety teammates designed developers Therefore vital agents reason ad hoc teams agents adapt teammates ﬂy This article investigates limited version ad hoc teamwork problem ad hoc agent knows en vironmental dynamics previous interactions teammates exploit similarities teammates behaviors It ad hoc teamwork problem fact ad hoc agent know behav ior teammates past teammates necessarily representative current teammates PLASTIC reuses knowledge learned past teammates combines knowledge advice provided domain experts This approach allows PLASTIC quickly adapt new teammates ﬂy We PLASTIC performs disparate domains variety teammates differing amounts knowledge teammates While PLASTIC assumes current teammates behaviors similar past teammates behaviors experiments behaviors explicitly designed similar Instead teammates created variety independent developers Nonetheless PLASTIC able discover similarities teammates behaviors While article covers topics research ad hoc teams raises interesting questions The teammates article explicitly learning ad hoc agent interacts Therefore question learn teammates learning ad hoc agent One approach tackling problem ad hoc agent maintain additional models learning algorithms teammates These models learning algorithms updated new observations addition updating probabilities models relative Another approach consider problem recursive modeling setting 20 ad hoc agent reason teammates reasoning Additionally PLASTIC perform arbitrarily poorly encounters new teammates PLASTIC relies learned previous teammates new teammates behaviors arbitrarily far previous teammates PLASTICs performance arbitrarily bad Falling learning scratch promising slow time periods considered article Instead falling behavior known bounds teams performance like safety strategy promising However calculating behavior bounded performance ad hoc teamwork prob lems remains open question A interesting topic evaluate agents ad hoc teams learn environment learn teammates To simultaneously learn unknown tasks unknown teammates ad hoc team 168 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 agent need balance tradeoff exploiting current knowledge exploring dynamics task exploring behavior teammates A possible approach quickly learn scenario ad hoc agent consider set previous environmental models select update models time We look forward future research directions believe article takes signiﬁcant step making novel research ad hoc teamwork possible Acknowledgements This work taken place Learning Agents Research Group LARG Artiﬁcial Intelligence Laboratory The Uni versity Texas Austin LARG research supported grants National Science Foundation CNS1330072 CNS1305287 ONR 21C18401 AFOSR FA87501410070 FA95501410087 This research supported Israel Science Foundation grant No 148814 ERC Grant 267523 Peter Stone serves Board Directors Cogitai Inc The terms arrangement reviewed approved University Texas Austin accordance policy objectivity research Appendix A Markov decision process algorithms This appendix provides descriptions algorithms solve MDPs results presented Sections 5 6 Note algorithms solving MDPs PLASTIC recommend selecting algorithms suited domain considered A1 Value iteration One way calculate optimal policy Value Iteration VI 67 VI requires complete model environment speciﬁcally transition reward functions Given models VI calculate optimal value function Q Value iteration relies dynamic programming solve optimal stateaction values stateaction pairs VI initializes stateaction values arbitrarily improves estimates update version Bellman optimality equation given Equation 2 These updates repeated iteratively convergence ﬁnal calculated stateaction values guaranteed optimal s optimal policy π While VI provably converges optimal policy convergence substantial time VI diﬃculties scaling large domains requires visiting stateaction iterations In ad hoc teamwork scenarios problem especially costly number agents greatly increases state space In problems teams state space exponential size domain power proportional number agents positions agents Given symmetries speciﬁc problem possible reduce number possible states scaling poor For example initial tests ad hoc teamwork pursuit domain described Section 24 VI 5 5 world took approximately 12 hours University Texas Mastodon computing cluster In 5 5 world 255 1e7 states consider ignoring symmetries given 5 agents moving 25 world positions Scaling larger problem 20 20 world 4005 1e13 states entire team Thus million times states 5 5 world leading problem computationally infeasible Due exponential blowup size state space ad hoc teamwork problems suitable VI teammates behaviors fully known problem described MDP A2 Monte Carlo tree search Value Iteration approach solving MDPs allow ad hoc agent optimally cooperate teammates complete model teammates environment However VI infeasible run reasonable time requires complex model Rather calculating exact optimal value stateaction computationally tractable instead learn approximately optimal value relevant stateactions When state space large small sections relevant agent advantageous use samplebased approach approximating values actions Monte Carlo Tree Search MCTS Speciﬁcally MCTS algorithm called Upper Conﬁdence bounds Trees UCT 51 starting point creating primary planning algorithm article MCTS require complete model environment Rather knowing probability distribution states rewards resulting transition reward functions MCTS needs model allows sampling states rewards Furthermore treating stateactions equally likely UCT focuses calculating values relevant stateactions UCT performing number playouts step starting current state sampling actions environment end episode It uses playouts estimate values sampled stateaction pairs Also maintains count visits state actions estimates upper conﬁdence bound values balance exploration exploitation When selecting actions UCT greedily chooses action highest upper conﬁdence bound UCT shown effective domains S Barrett et al Artiﬁcial Intelligence 242 2017 132171 169 high branching factor Go 31 large POMDPs 60 As reasoned able handle branching factor caused number agents In article UCT modiﬁed use eligibility traces remove depth index help speed learning The pseudocode algorithm seen Algorithm 5 s current state Similar modiﬁcations Silver et al good success Go 59 In addition work Hester Stone 40 good results number reinforcement learning domains Algorithm 5 The modiﬁed version UCT article 1 function UCTSelect inputs s outputs params γ NumPlayouts c λ simulateActions 2 3 4 1 NumPlayouts Searchs return argmaxa Q s cid7 current state cid7 action selected UCT cid7 discount factor parameter MDP cid7 number Monte Carlo playouts perform cid7 weight given conﬁdence bound cid7 eligibility trace parameter affects backup cid7 environment model samples states cid4 5 function Searchs bestActions 6 s terminal 7 8 9 10 cid4 r simulateActions s cid4 bestActions es 1 cid7 Update Qvalues cid4 δ r γ Q s s Q s Q s λes cid4 11 12 13 14 15 cid4 Q s es cid4 s s es δ visitssa 16 function bestActions cid11 17 return argmaxa Q s c ln visitss visitss A3 Fitted Q iteration While UCT effective quickly computing approximately optimal policy MDP require model MDP permits sampling transition reward functions This model given learned given data VI requires stronger model model gives probability distribution states rewards transition reward functions However approaches attempt directly learn values stateactions model transition function approaches called modelfree Modelfree approaches require building model domain tractable hard model domains In addition modelfree algorithms computationally simpler In complex domains diﬃcult ad hoc agents compute model environment teammates useful ad hoc agent employ modelfree learning method ﬁnd good policy cooperating teammates In work agent uses Fitted Q Iteration FQI algorithm introduced Ernst et al 28 Similar Value Iteration VI FQI iteratively backs rewards improve estimates values states Rather looking state possible outcome state FQI uses samples states outcomes approximate values stateaction pairs This approximation allows FQI ﬁnd solutions complex continuous domains Alternative policy learning algorithms Qlearning 71 policy search 26 To collect samples domain agent ﬁrst performs number exploratory actions From action agent cid4cid15 s original state action r reward s stores tuple cid14s r s resulting state An advantage FQI algorithm data collected parallel number tests At iteration agent updates following equation tuple cid4 Q s r γ max acid4 Q s cid4 cid4 170 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 Q s initialized 0 Q estimate optimal value function Q estimate iteratively improved looping stored samples To handle continuous state spaces Q stored exactly table instead value estimated function approximation In article continuous state features converted set binary features CMAC tilecoding 78 estimate Q s given ˆQ s cid6 w f f ith binary feature w weight given feature updates split uniformly active features This approach uses set overlapping tilings cover space binary activations 67 The advantages tile coding include simple computation binary output good control generalization model References 1 N Agmon S Barrett P Stone Modeling uncertainty leading ad hoc teams Proceedings Thirteenth International Conference Autonomous Agents Multiagent Systems AAMAS May 2014 2 N Agmon P Stone Leading ad hoc agents joint action settings multiple teammates Proceedings Eleventh International Conference Autonomous Agents Multiagent Systems AAMAS June 2012 3 H Akiyama Agent2d base code release httpsourceforgejpprojectsrctools 2010 4 S Albrecht S Ramamoorthy A GameTheoretic Model BestResponse Learning Method Ad Hoc Coordination Multiagent Systems Tech rep School Informatics The University Edinburgh United Kingdom February 2013 5 S Albrecht S Ramamoorthy A gametheoretic model bestresponse learning method ad hoc coordination multiagent systems extended abstract Proceedings Twelfth International Conference Autonomous Agents Multiagent Systems AAMAS St Paul Minnesota USA May 2013 6 S Albrecht S Ramamoorthy On convergence optimality bestresponse learning policy types multiagent systems Proceedings 30th Conference Uncertainty Artiﬁcial Intelligence UAI Quebec City Canada July 2014 7 JS Albus A theory cerebellar function Math Biosci 10 12 1971 2561 8 JS Albus A new approach manipulator control cerebellar model articulation control CMAC Tran ASME J Dyn Syst Meas Control 97 9 1975 220227 9 F Almeida PH Abreu N Lau L Reis An automatic approach extract goal plans soccer simulated matches Soft Comput 17 5 2013 835848 10 N Bard M Johanson N Burch M Bowling Online implicit agent modelling Proceedings Twelfth International Conference Autonomous 11 S Barrett P Stone An analysis framework ad hoc teamwork tasks Proceedings Eleventh International Conference Autonomous Agents Agents Multiagent Systems AAMAS 2013 pp 255262 Multiagent Systems AAMAS June 2012 12 S Barrett P Stone Cooperating unknown teammates complex domains robot soccer case study ad hoc teamwork Proceedings 13 S Barrett P Stone S Kraus Empirical evaluation ad hoc teamwork pursuit domain Proceedings Tenth International Conference TwentyNinth Conference Artiﬁcial Intelligence AAAI January 2015 Autonomous Agents Multiagent Systems AAMAS May 2011 14 S Barrett P Stone S Kraus A Rosenfeld Teamwork limited knowledge teammates Proceedings TwentySeventh Conference Artiﬁcial Intelligence AAAI July 2013 15 M Benda V Jagannathan R Dodhiawala On Optimal Cooperation Knowledge Sources An Empirical Investigation Tech Rep BCSG201028 Boeing Advanced Technology Center Boeing Computing Services July 1986 16 J Biswas JP Mendoza D Zhu B Choi S Klee M Veloso Opponentdriven planning execution pass attack defense multirobot soccer team Proceedings Thirteenth International Conference Autonomous Agents Multiagent Systems AAMAS January 2014 17 A Blum Y Mansour Learning regret minimization equilibria Algorithmic Game Theory Cambridge University Press 2007 18 M Bowling P McCracken Coordination adaptation impromptu teams Proceedings Twentieth Conference Artiﬁcial Intelligence AAAI 2005 pp 5358 19 RI Brafman M Tennenholtz On partially controlled multiagent systems J Artif Intell Res 4 1996 477507 20 D Carmel S Markovitch Incorporating opponent models adversary search Proc AAAI 1996 pp 120125 21 D Chakraborty P Stone Convergence targeted optimality safety multiagent learning Proceedings TwentySeventh International 22 D Chakraborty P Stone Cooperating markovian ad hoc teammate Proceedings Twelfth International Conference Autonomous 23 V Conitzer T Sandholm AWESOME general multiagent learning algorithm converges selfplay learns best response stationary 24 W Dai Q Yang GR Xue Y Yu Boosting transfer learning Proceedings TwentyFourth International Conference Machine Learning 25 KS Decker VR Lesser Designing family coordination algorithms International Conference MultiAgent Systems ICMAS June 1995 Conference Machine Learning ICML June 2010 Agents Multiagent Systems AAMAS May 2013 opponents Mach Learn 67 May 2007 ICML 2007 pp 193200 pp 7380 26 MP Deisenroth G Neumann J Peters A survey policy search robotics Found Trends Robot 2 12 2013 1142 27 P Doshi Y Zeng Improved approximation interactive dynamic inﬂuence diagrams discriminative model updates Proceedings Eighth International Conference Autonomous Agents Multiagent Systems AAMAS 2009 28 D Ernst P Geurts L Wehenkel Treebased batch mode reinforcement learning J Mach Learn Res 2005 503556 29 M Fang Y Guo X Zhang X Li Multisource transfer learning based label shared subspace Pattern Recognit Lett 51 2015 101106 30 L Ge J Gao A Zhang OMSTL framework online multiple source transfer learning Proceedings 22nd ACM International Conference Information Knowledge Management CIKM 13 ACM New York NY USA 2013 pp 24232428 31 S Gelly Y Wang Exploration exploitation Go UCT MonteCarlo Go Advances Neural Information Processing Systems NIPS vol 19 32 K Genter N Agmon P Stone Ad hoc teamwork leading ﬂock Proceedings Twelfth International Conference Autonomous Agents 33 K Genter P Stone Inﬂuencing ﬂock ad hoc teamwork Proceedings Ninth International Conference Swarm Intelligence ANTS December 2006 September 2014 Multiagent Systems AAMAS May 2013 S Barrett et al Artiﬁcial Intelligence 242 2017 132171 171 34 PJ Gmytrasiewicz P Doshi A framework sequential planning multiagent settings J Artif Intell Res 24 1 Jul 2005 4979 35 PJ Gmytrasiewicz EH Durfee DK Wehe A decisiontheoretic approach coordinating multiagent interactions IJCAI vol 91 1991 pp 6368 36 B Grosz S Kraus Collaborative plans complex group actions Artif Intell 86 1996 269368 37 M Hall E Frank G Holmes B Pfahringer P Reutemann IH Witten The WEKA data mining software update ACM SIGKDD Explor Newsl 11 November 2009 1018 38 J Han M Li L Guo Soft control collective behavior group autonomous agents shill agent J Syst Sci Complex 19 2006 5462 39 M Hausknecht P Mupparaju S Subramanian S Kalyanakrishnan P Stone Half ﬁeld offense environment multiagent learning ad hoc teamwork AAMAS Adaptive Learning Agents ALA Workshop Singapore May 2016 40 T Hester P Stone TEXPLORE realtime sampleeﬃcient reinforcement learning robots Mach Learn 90 3 2013 385429 41 TN Hoang KH Low Interactive POMDP lite practical planning predict exploit intentions interacting selfinterested agents The 23th International Joint Conference Artiﬁcial Intelligence IJCAI AAAI Press 2013 pp 22982305 42 P Huang G Wang S Qin Boosting transfer learning multiple data sources Pattern Recognit Lett 33 5 2012 568579 43 YW Huang Y Sasaki Y Harakawa E Fukushima S Hirose Operation underwater rescue robot anchor diver III 2011 Tohoku earthquake tsunami OCEANS 2011 Sept 2011 pp 16 44 R Isaacs Differential Games A Mathematical Theory Applications Warfare Pursuit Control Optimization Dover Publications 1965 45 Y Ishiwaka T Sato Y Kakazu An approach pursuit problem heterogeneous multiagent reinforcement learning Robot Auton Syst 43 4 2003 245256 Science Department 2010 46 E Jones B Browning MB Dias B Argall MM Veloso AT Stentz Dynamically formed heterogeneous robot teams performing tightlycoordinated tasks Proceedings IEEE International Conference Robotics Automation ICRA May 2006 pp 570575 47 T Jung D Polani P Stone Empowerment Continuous AgentEnvironment Systems Tech Rep AI1003 The University Texas Austin Computer 48 S Kalyanakrishnan Y Liu P Stone Half ﬁeld offense RoboCup soccer multiagent reinforcement learning case study RoboCup2006 Robot Soccer World Cup X Lecture Notes Artiﬁcial Intelligence vol 4434 SpringerVerlag Berlin 2007 pp 7285 49 S Kalyanakrishnan P Stone Characterizing reinforcement learning methods parameterized learning problems Mach Learn 84 12 July 50 T Kamishima M Hamasaki S Akaho TrBagg simple transfer learning method application personalization collaborative tagging Ninth IEEE International Conference Data Mining ICDM Dec 2009 pp 219228 51 L Kocsis C Szepesvari Bandit based MonteCarlo planning Proceedings Seventeenth European Conference Machine Learning ECML 2011 205247 2006 52 D Korzhyk Z Yin C Kiekintveld V Conitzer M Tambe Stackelberg vs Nash security games extended investigation interchangeability equivalence uniqueness J Artif Intell Res 41 2 May 2011 297327 53 M Lauer M Riedmiller An algorithm distributed reinforcement learning cooperative multiagent systems Proceedings Seventeenth International Conference Machine Learning ICML Morgan Kaufmann 2000 pp 535542 54 S Liemhetcharat M Veloso Weighted synergy graphs effective team formation heterogeneous ad hoc agents Artif Intell 208 2014 4165 55 R Murphy K Dreger S Newsome J Rodocker E Steimle T Kimura K Makabe F Matsuno S Tadokoro K Kon Use remotely operated marine vehicles Minamisanriku Rikuzentakata Japan disaster recovery 2011 IEEE International Symposium Safety Security Rescue Robotics SSRR November 2011 pp 1925 56 K Nagatani S Kiribayashi Y Okada S Tadokoro T Nishimura T Yoshida E Koyanagi Y Hada Redesign rescue mobile robot Quince 2011 IEEE International Symposium Safety Security Rescue Robotics SSRR November 2011 pp 1318 57 D Pardoe P Stone Boosting regression transfer Proceedings TwentySeventh International Conference Machine Learning ICML June 2010 58 D Richardson Robots rescue Eng Technol 6 4 May 2011 5254 59 D Silver RS Sutton M Müller Samplebased learning search permanent transient memories Proceedings TwentyFifth International Conference Machine Learning ICML 2008 60 D Silver J Veness MonteCarlo planning large POMDPs Advances Neural Information Processing Systems NIPS vol 23 2010 61 E Sonu P Doshi Generalized bounded policy iteration ﬁnitelynested interactive POMDPs scaling Proceedings Eleventh Interna tional Conference Autonomous Agents Multiagent Systems AAMAS International Foundation Autonomous Agents Multiagent Systems Richland SC 2012 pp 10391048 62 P Stone Layered Learning Multiagent Systems A Winning Approach Robotic Soccer MIT Press 2000 63 P Stone GA Kaminka S Kraus JS Rosenschein Ad hoc autonomous agent teams collaboration precoordination Proceedings TwentyFourth Conference Artiﬁcial Intelligence AAAI July 2010 64 P Stone GA Kaminka JS Rosenschein Leading bestresponse teammate ad hoc team AAMAS Workshop Agent Mediated Electronic Commerce AMEC November 2010 65 P Stone S Kraus To teach teach Decision making uncertainty ad hoc teams Proceedings Ninth International Conference Autonomous Agents Multiagent Systems AAMAS May 2010 66 P Stone M Veloso Multiagent systems survey machine learning perspective Auton Robots 8 3 July 2000 345383 67 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press Cambridge MA USA 1998 68 M Tambe Towards ﬂexible teamwork J Artif Intell Res 7 1997 81124 69 ME Taylor P Stone Transfer learning reinforcement learning domains survey J Mach Learn Res 10 1 2009 16331685 70 C Undeger F Polat Multiagent realtime pursuit Auton Agents MultiAgent Syst 21 1 2010 69107 71 CJCH Watkins Learning Delayed Rewards PhD thesis Kings College Cambridge UK May 1989 72 F Wu S Zilberstein X Chen Online planning ad hoc autonomous agent teams The 22th International Joint Conference Artiﬁcial Intelligence IJCAI 2011 CVPR June 2010 73 P Xuan V Lesser S Zilberstein Communication decisions multiagent cooperation model experiments Proceedings Fifth International Conference Autonomous Agents AGENTS 2001 74 Y Yao G Doretto Boosting transfer learning multiple sources Proceedings Conference Computer Vision Pattern Recognition 75 Y Zeng Y Chen P Doshi Approximating model equivalence interactive dynamic inﬂuence diagrams k policy paths IEEEWICACM International Conference Web Intelligence Intelligent Agent Technology vol 2 2011 pp 208211 76 Y Zeng P Doshi Exploiting model equivalences solving interactive dynamic inﬂuence diagrams J Artif Intell Res 43 1 Jan 2012 211255 77 F Zhuang X Cheng S Pan W Yu Q He Z Shi Transfer learning multiple sources consensus regularized autoencoders T Calders F Esposito E Hüllermeier R Meo Eds Machine Learning Knowledge Discovery Databases Lecture Notes Computer Science vol 8726 Springer Berlin Heidelberg 2014 pp 417431 78 M Zinkevich M Johanson M Bowling C Piccione Regret minimization games incomplete information Advances Neural Information Processing Systems NIPS vol 20 2008 pp 905912