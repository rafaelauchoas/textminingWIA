Artiﬁcial Intelligence 239 2016 5469 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Extracting qualitative relations categorical data Jure Žabkar Ivan Bratko Janez Demšar Faculty Computer Information Science University Ljubljana Veˇcna pot 113 1000 Ljubljana Slovenia r t c l e n f o b s t r c t Article history Received 4 September 2014 Received revised form 23 June 2016 Accepted 28 June 2016 Available online 5 July 2016 Keywords Qualitative modeling Machine learning Ceteris paribus 1 Introduction Qualitative modeling traditionally concerned abstraction numerical data In numerical domains partial derivatives relation independent dependent variable qualitatively tell trend dependent variable In paper address problem extracting qualitative relations categorical domains We generalize notion partial derivative deﬁning probabilistic discrete qualitative partial derivative PDQ PD PDQ PD qualitative relation target class c discrete attribute derivative corresponds ordering attributes values ai P cai local neighborhood reference point respecting ceteris paribus principle We present algorithm computation PDQ PD labeled attributebased training data Machine learning algorithms induce models explain inﬂuence attributes values target class different subspaces attribute space 2016 Elsevier BV All rights reserved A data set collected Institute Oncology Ljubljana Slovenia contains data 1220 patients breast cancer Each patient given possible chemotherapy treatments CMF1 anthracyclinebased chemotherapy No treatment generally superior physicians role determine optimal treatment particular patient 1 The data contains demographic clinical data chosen treatment cancer recurred certain period How induce useful model data In general case physician need dependent independent decisions multiple objectives like patients survival comfort sideeffects optimize Which algorithms suitable analyzing data collected studies The problem resembles number different tasks Model induction belongs ﬁeld classical machine learning deal categorical numeric target variables attributes control need treated differently From perspective problem superﬁcially similar preference learning preferences explicitly given need extracted data The resulting model represented CPnet task derive preferences structure network We explore relation methods appropriate points later paper The methods present limited induction clinical general decision support systems like described Our venture point qualitative modeling Qualitative predictive models original form substitute gression models Instead predicting numerical value dependent variable predict direction Email addresses jurezabkarfriuniljsi J Žabkar ivanbratkofriuniljsi I Bratko janezdemsarfriuniljsi J Demšar Corresponding author 1 Cyclophosphamide methotrexate ﬂuorouracil httpdxdoiorg101016jartint201606007 00043702 2016 Elsevier BV All rights reserved J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 55 change respect changes independent variables They particularly useful complex problems instance economy 2 exact numerical relations unattainable qualitative models correctly example conditions decrease rates stimulate economic growth affect unemployment rate Kupiers 3 argued qualitative models reasoning continu ous phenomena incomplete unreliable knowledge Despite popularity qualitative models branches science notably economy gained little traction machine learning resulting algorithms automated induction data 45 Most qualitative models derived manually In mathematical terms qualitative models viewed models predict sign partial derivatives dubbed qualitative partial derivatives This makes attractive problem hand relations outcomes variables control resemble derivatives The dependent variables case discrete requires different deﬁnition derivative We assume variables differentiate categorical categorical variables common discrete values increase reliability spirit ﬁndings ﬁnally easier differentiate categorical variables respect categorical continuous variable The papers basic contributions contained Section 2 deﬁnition probabilistic discrete qualitative partial derivative b algorithm computation derivatives data point c induction models generalization machine learning derivatives In Section 21 deﬁne new type qualitative relation probabilistic discrete qualitative partial derivative PDQ PD relates categorical variables The deﬁnition based recasting qualitative partial derivative probabilistic terms instead predicting effect change numeric input variable numeric output predict effect certain change categorical input variable probability target class To end qualitative model interested change positive negative The ﬁrst step computation derivatives requires calculating conditional probabilities class values target attribute relevant attributes conditions The set relevant attributes data point deter mined greedy approach Section 22 Probabilities clustered obtain partial ordering Section 23 Both procedures constitute new algorithm Qube heuristic depend availability suitable data vicinity point derivative computed smoothing generalization necessary The paper puts emphasis point c By computed derivatives labels arbitrary machine learning method induction qualitative models Section 24 We experiment decision trees simplicity interpretability When aiming accuracy choose support vector machines neural networks modern methods Alternatively data produced Qube visualized construction CPnets Exploring options scope paper We demonstrate different uses method observe properties examples Section 3 We start artiﬁcial data set resembling described beginning known ground truth results evaluated Section 31 This example shows direct use machine learning algorithms data yield useful results We continue systematically observing behavior algorithm respect data size noise purely synthetic domain The case typical qualitative modeling data describes complex physical phenomenon modeling ﬁnd simple qualitative description case terms PDQ PDs The ﬁnal experiment run medical data control attributes This shows qualitative partial derivatives useful analysis standard machine learning data 2 Methods We start deﬁnition probabilistic discrete qualitative partial derivatives based conditional probabilities target class given attribute values The computation probabilities data requires selecting proper subsets examples Finally combine computed probabilities partial derivatives use induce qualitative models 21 Probabilistic discrete qualitative partial derivative The derivative function f x certain point x0 f cid3x0 tells informally change function value corresponding certain small change value functions argument f x0 cid2x f x0 f cid3 x0cid2x For functions multiple arguments denote f xi f x1 x2 xn compute derivatives argument xi separately 1 56 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 Qualitative derivatives denote f x similar ordinary derivatives direction change function increase decrease argument increases A qualitative derivative function positive negative zero continuous derivative positive negative zero Now consider multivariate distribution assigns probability y element Cartesian product A1 A2 An In machine learning a1 a2 A1 A2 An values discrete attributes A1 A2 An describing example example reference example The probability example belongs target class c y pca1 a2 2 Recall qualitative derivative f respect xi computed x1 x2 tells certain change xi increase decrease function value arguments remain constant Let probabilistic discrete qualitative partial deriva cid3 tive a1 a2 respect Ai tell change value attribute Ai ai increase decrease probability target class f Ai ai a1 cid3 cid3 pca1 ai pca1 cid3 pca1 ai pca1 cid3 pca1 ai pca1 Let deﬁne total order set Ai respect ﬁxed values j j cid7 ai cid3 pca1 ai pca1 cid3 This allows rewrite 3 f Ai ai cid3 a1 cid3 ai cid3 ai cid3 ai cid10 3 4 5 The derivative f Ai ai cid3 pair ai cid3 described total ordering attribute values Ai It known summer Mondays rain Spain stays mainly plain prainplain summer Monday Spain prainmountains summer Monday Spain Therefore rain location plain mountains summer Monday Spain going plain mountains decreases probability rain summer Mondays Spain Equivalently write mountains plain summer Mondays Spain The derivative deﬁned total ordering values Assuming types locations Spain seashore getting rain derivative equal seashore mountains plain The relation refers speciﬁc data point summer Mondays Spain Weather patterns Spanish winters summer Sundays Britain different 22 Computation conditional probabilities To compute derivative respect Ai estimate conditional probabilities pca1 single point E a1 a2 data sample different values Ai For example compute derivative rain likeli hood respect location summer Mondays Spain compute prainlocation summer Monday Spain location plain mountains seashore These probabilities estimated directly perfect Bayesian approach instance relative frequencies examples data match conditional We use cid3 naive Bayesian method reduce PDQ PD comparison pca1 pca 1 canceling terms corresponding values attributes naive Bayesian assumption conditional independence attributes given class implies derivative constant entire attribute space f Ai aia cid3 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 57 The problem requires seminaive Bayesian approach We replace condition pca1 relaxed condition P cai D D a1 includes attribute values conditionally dependent ai given class values attribute respect compute derivative Ignoring condi tionally independent values change computed derivative proof Appendix In running example discover day week plays role rain likelihood suﬃces compute probabilities prainlocation summer Spain D summer Spain2 We construct set conditions D greedy approach We start set D To check attribute A j added D try reject assumption value attribute conditionally given D independent A j value j p A j j Aic D p A j Ai jc Dp Aic D If assumption holds condition A j j unrelated Ai need added D Note check total independence A j Ai consider values Ai A j consider j vs values A j All values Ai need considered use set conditions D derivatives respect Ai certain reference example This ensures probabilities pcai D ai Ai 3 comparable useful deﬁning total ordering Ai For A j interested j substituted values A j affecting Ai The independence assumption checked χ 2 test We construct separate tables 2 Ai cells class c complement rows correspond A j j columns correspond values Ai From ﬁrst table compute expected absolute frequencies nc Dpa jc Dpvc D nc D1 pa jc Dpvc D v Ai nc D number examples class c satisfy conditions D Frequencies comple ment c computed analogously The observed frequencies computed training data taking conditions D account The sum χ 2 statistics tables distributed according χ 2 distribution 2Ai 1 degrees freedom The greedy algorithm construction D starts set At step tests independence assumption attributes D computes corresponding pvalue We select lowest value add D We stop procedure lowest pvalue speciﬁed threshold number examples matching conditions D falls given minimum This needed ensure reliability χ 2 statistics estimated conditional probabilities Our use pvalue require adjustments multiple hypotheses testing pvalue stopping criteria claim signiﬁcance alternative hypothesis After completing set conditions D compute pcai D ai Ai relative frequency Laplacean estimate mestimate 6 examples matching D The pseudo code algorithm shown Algorithm 1 2 Algorithm 1 Compute conditional probabilities Input Learning data set Output Conditional probabilities pcai D ai Ai example function compute_conditional_probabilities e data D repeat p_value A select_most_important_attribute D D A p_value p_limit compute pcai D ai Ai data end end function Although proposed greedy procedure simplistic works practice We mind selection attributes needs fast recomputed point compute derivative rules advanced search sets dependent values 2 This illustrated familiar derivatives continuous functions deﬁned f x1 x1 xn lim h0 f x1 h x2 xn f x1 x2 xn h When computing derivative respect x1 subtract function value points arguments x1 If function value point x1 xn depend x2 use different values x2 terms 58 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 Algorithm 2 Select relevant attribute Input index differentiated attribute Output relevant attribute X corresponding pvalue function select_most_relevant_attribute pmin 0 attribute A j cid7 Ai χ 2 0 v Ai E expected frequencies O observed frequencies target class c E nc Dpa jc Dpvc D χ 2 χ 2 O ai j E2E E nc D1 pa j c Dpvc D χ 2 χ 2 O ai j E2E nontarget class c E nc Dpa jc Dpvc D χ 2 χ 2 O ai j E2E E nc D1 pa j c Dpvc D χ 2 χ 2 O ai j E2E end p get_ p_valueχ 2 p pmin pmin p X A j end end end function 23 Computation derivatives The total order Ai determined order corresponding probabilities deﬁned 3 To handle noisy data treat probabilities corresponding values Ai equal differ userprovided threshold For use hierarchical clustering values average linkage 7 difference probabilities distances The clustering stopped distance closest clusters greater predeﬁned threshold experiments set 02 For example let Ai ﬁvevalued attribute values v 1 v 5 Probabilities pcv D values equal 01 02 03 05 06 respectively Let merging threshold 02 We recognize v 1 v 2 equivalent assign average probability 01 022 015 Next merge v 4 v 5 average probability 05 06 055 Finally merge v 1 v 2 v 3 average probability 01 02 033 02 We stop difference p 02 v 1 v 3 p 055 v 4 v 5 exceeds threshold 02 The resulting total ordering Ai v 1 v 2 v 3 v 4 v 5 In case ties clustering chooses pair merge random potentially resulting different possible results 24 Induction qualitative models To induce qualitative model respect certain attribute Ai ﬁrst compute PDQ PD entire learning set example compute set dependent values D ﬁnd total ordering attribute values Ai ex plained previous sections We replace original class labels partial derivatives total ordering induce model predicting ordering In principle learning algorithm task 3 Evaluation The described approach new aware method similar data similar fashion The aim section demonstrate different scenarios Qube qualitatively compare results direct use machine learning methods run type data observe behavior different conditions In experiments use Qube compute partial derivatives replace original target variable deriva tives induce model modiﬁed data This approach practical analyzing derivatives directly anticipated use Qube For modeling use classiﬁcation trees able compare mod els ground truth relations In practice symbolic methods interested interpretability subsymbolic methods preferred aim higher accuracy We start data resembles motivation beginning paper We constructed similar data set different treatments control group known groundtruth concept unlike J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 59 Table 1 The hidden model Rat data set Treatment 1 2 3 0 1 2 3 0 psurvival 100 65 85 100 20 Under condition A1 1 A3 2 A4 0 A1 1 A3 2 A4 0 A1 0 A2 1 2 A3 1 A3 cid7 1 Table 2 A small sample rat data set illustrating input data Qube algorithm The entire data set consists N 1000 learning examples Treatment 0 2 1 0 3 0 0 2 A1 1 1 1 1 0 0 0 0 A2 2 0 0 2 0 1 0 2 A3 0 0 1 0 1 0 0 2 A4 0 1 1 0 0 0 1 0 Survived 0 0 1 0 1 0 0 0 data motivation observe correctness model We Qube data appropriate inducing classiﬁcation trees directly data We use similar data set simpler target concept observe accuracy method different data sizes noise levels The scenario typical use case qualitative modeling The data generated complex physical model standard task qualitative modeling subsume simple qualitatively correct model With Qube model form qualitative partial derivatives The use case PDQ PD limited decision support preference induction useful general labeled attributebased data For case actual medical data generalized compute derivatives form rules asked medical doctors evaluate interpretability correctness rules We ran experiments parameters ﬁxed advance The threshold stopping clustering 02 described Section 23 The m mestimate 6 probabilities set common setting m 2 Classiﬁcation trees induced reimplementation C45 algorithm 8 following arguments max_depth5 mForPruning2 sameMajorityPruningTrue binarizationFalse max_majority 095 31 Rat data set 311 Data One thousand sick rats different genetic predispositions given treatments 1 2 3 treatment 0 The outcome describes rats survival 1 death 0 Four attributes describing important genetic markers A1 0 1 A2 0 1 2 A3 0 1 2 A4 0 1 constitute following hidden ground truth model The probability survival 100 rats genetic marker A3 1 disregarding treatment type Treatment 1 works perfectly combination markers A1 1 A3 2 A4 0 rats combination genetic markers receive treatment 2 65 chance survival Treatment 3 works rats A1 0 A2 1 2 probability survival group 85 If rat given treatment match necessary condition dies The survival rate rats receive treatment treatment 0 20 Table 2 presents small sample data set In real world scenario researchers studying effect treatments know model work reverse direction measure genetic parameters carry experiment compile data Table 2 goal learning relation treatment genetic markers Table 1 312 Experiment The desired output survived 1 selected variable clinicians interested treatment In formal ism described interested PDQ PD survived1 treatment We Qube calculate survived1 learning example original data set The result ordering treatment types increasing probability survival rat We replaced original outcome survival vs death orderings induced decision tree Fig 1 treatment 60 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 Fig 1 Treelike preference model survived treatment rats data set Fig 2 Decision tree prediction survival rats data set 313 Discussion Analyzing tree branch branch model perfectly matches underlying hidden model The leaf branch corresponding A1 1 A3 2 A4 0 states 0 3 2 1 This correct combination genetic markers treatment 1 psurvived 100 better treatment 2 psurvived 65 treatment 3 effect makes equally nondesirable treatment 0 Similarly branches A1 0 A2 1 A1 0 A2 2 end preference relation 0 1 2 3 agreement fact rats A1 0 A2 1 2 treated treatment 3 psurvived 85 Branch A1 1 A3 0 A2 0 states treatment 0 better treatments 1 2 3 1 2 3 0 This rule complies default rule ground truth model 20 rats receiving treatment 0 survive unlike mistreated rats die The remaining leaves state treatment type effect survival preferred treatments 0 1 2 3 combinations genetic markers In branches treatment equivalent treatments leaf covers rats susceptible treatments We contrasted model classical machine learning approach inducing model case classiﬁcation tree original data substituting outcomes partial derivatives The resulting tree shown Fig 2 The tree correctly predict survival researcher interested The attribute treatment emerge ﬁnal model If disable tree pruning treatment attribute appear tree necessarily leaves makes direct interpretation use decision making diﬃcult The tree induced data prepared Qube expresses preferences leaves natural way clinician interested appropriate treatment speciﬁc rat following attribute values root tree preference leaf tells optimal treatment 32 Equality data set 321 Data We study behavior algorithm varying number learning examples N number attributes atts level noise μ slightly simpler target concept allows automatically assess correctness model J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 61 Table 3 Groundtruth models Equality data set PDQ PD f 1 A1 f 1 A2 f 1 A3 Correct model A2 1 2 1 A2 2 1 2 A2 3 1 2 A1 1 2 3 1 A1 2 1 3 2 1 2 3 4 5 Fig 3 Classiﬁcation model original data set describing equality attributes A1 A2 The underlying concept f A2 1 2 3 cid5 f 1 A1 A2 0 A1 cid7 A2 Fig 4 Preference model f 1 A2 data set binary variable representing equality attributes A1 1 2 6 Other attributes unrelated output variable Attributes A3 A4 A5 ﬁve values respectively attributes Ai 5 random number values interval 2 5 We compute PDQ PDs target output value f 1 respect A1 A2 A3 Each attribute different number values examine inﬂuence attribute cardinality The correct models Table 3 intuitive rat data For f 1 A1 A2 equals 1 value A1 1 preferred A1 2 f 1 requires A1 A2 The situation inverted A2 2 A1 2 preferred A1 1 Since A1 A2 different number values interesting case appears A2 3 There value A1 increase likelihood f 1 A1 A2 effects A1 1 A1 2 qualitatively 1 2 similar A1 1 A2 1 increases likelihood f 1 comparison The case f 1 A2 values Analogously A1 2 A2 2 higher likelihood f 1 1 3 The model simplest f 1 A3 value A3 increases likelihood f 1 322 Experiment We randomly generated data sets different number learning examples N 100 500 1000 number attributes atts 5 10 30 50 level noise μ 0 5 10 30 Level noise refers fraction learning examples corrupted values output variable f For set parameters N atts μ sampled data sets computed PDQ PDs f 1 Ai 1 2 3 induced qualitative models C45 algorithm We compared obtained models theoretically correct models described Table 3 In following results treat obtained models correct perfectly match theoretical counterparts The models partially correct branch tree wrong treated wrong See Fig 3 Fig 4 Table 4 reports results different sizes data sets table contains fractions correctly reconstructed models symbol cid2 means models correct 10 runs 10 random runs f 1 A1 f 1 A3 f 1 A2 62 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 Table 4 The summary results randomly generated data sets underlying concept A1 A2 We varied number learning examples N number attributes atts level noise μ For combination fraction correct models learnt data set computed 10 runs The models correct marked cid2 models correct marked attributes 5 10 30 50 attributes 5 10 30 50 attributes 5 10 30 50 μ 0 cid2 cid2 cid2 cid2 μ 0 cid2 cid2 cid2 cid2 μ 0 cid2 cid2 cid2 cid2 cid2 cid2 cid2 08 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 N 100 cid2 cid2 09 09 N 500 cid2 cid2 cid2 cid2 N 1000 cid2 cid2 cid2 cid2 cid2 cid2 08 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 μ 5 cid2 09 cid2 09 μ 5 cid2 cid2 09 08 μ 5 cid2 cid2 cid2 09 μ 10 cid2 09 09 09 μ 10 cid2 08 09 05 μ 10 cid2 09 05 07 cid2 cid2 07 07 08 07 07 05 cid2 07 09 07 cid2 cid2 09 05 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 μ 30 06 07 01 01 μ 30 02 01 μ 30 05 02 01 04 04 02 02 01 02 09 09 07 05 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 Table 5 Time complexity calculation PDQ PD different number learning examples N number attributes atts All times seconds atts 5 10 30 50 N 100 004 006 01 02 500 04 07 19 31 1000 16 24 67 115 means model correct Throughout experiments pvalue set 005 threshold set 20 323 Discussion In data sets added noise Qube produces correct models values N atts fails 2 10 times smallest data set 50 dummy attributes In presence noise number induced incorrect models increases higher values added noise increasing number dummy attributes However effect noise dummy attributes diminishes increasing size data set N We evaluated complexity algorithm measuring time needed calculation PDQ PD including learning time classiﬁcation tree algorithm Table 5 summarizes average times seconds runs data set It shows quadratic growth N linear growth number attributes 33 Billiards 331 Data Billiards common table games played stick set balls snooker pool variants The goals games vary main idea common player uses stick stroke cue ball aiming ball achieve desired effect The friction table balls spin cue ball collision balls combine complex physical 9 However despite complexity amateur player learn basic principles stroke cue ball knowing physics In case study use method induce simple model human player ranking different shot types different ball settings Our goal learn shot preferences different circumstances simulated data Although possible pocket black ball different type shots shots appropriate increasing probability successful shot For example hole black ball cue ball collinear direct shot preferred railﬁrst shot diﬃcult correctly estimate reﬂection angles black ball pocket makes sense However presence balls present obstacle direct shot railﬁrst shot preferred J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 63 Fig 5 Blocking strokes Ball S blocks direct shot balls L R block left right shots respectively Table 6 Deﬁnitions different types shots Action Strong direct shot Weak direct shot Left railﬁrst shot Right railﬁrst shot Stick elevation 0 5 0 5 0 5 0 5 Stroke angle ϕ 02 ϕ 02 ϕ 02 ϕ 02 ϕ 02 ϕ 02 ϕ 02 ϕ 02 Stroke velocity 3 4 1 2 3 4 3 4 Stroke follow 01 01 01 01 01 01 01 01 We consider problem stroking cue ball order pocket black ball presence balls present obstacle players attempt desired stroke The balls ﬁxed position shown Fig 5 Usually player different options hit black ball 9 case study consider direct shots railﬁrst shots In white ball directly hits black ball white ball ﬁrst hits rail called cushion black ball For sake case study derived 4 possible actions player strong direct shot weak direct shot leftrailﬁrst shot rightrailﬁrst shot Strong weak direct shots Fig 6a differ force applied player visible different initial velocities white ball The difference left right railﬁrst shots shown Figs 6b 6c In leftrailﬁrst shot white ball pass black ball left hit rail hit black ball Similarly rightrailﬁrst shot white ball ﬁrst pass black ball right hit rail hit black ball We created data set 1000 shots learning examples example described following attributes S ball blocking straight shot present valuesyesno L ball blocking left shot present valuesyesno R ball blocking right shot present valuesyesno ShotType shot type valuesStrDir WeakDir LeftRail RightRail class variable black pocket values yes The goal target class method black pocket yes shot forced black ball pocket The attribute values single example randomly selected value class determined billiards simulator 10 Each shot simulator deﬁned shot direction stick elevation shot velocity shot follow The important property shot direction dominant effect direction black ball hit white ball First computed optimal stroke direction ϕ results pocketing black ball However account human imprecision added uniform noise interval 02 deg 02 deg Therefore value stroke direction simulator set random value selected interval ϕ 02 ϕ 02 We expect diﬃcult shots prone changes optimal stroke setting Similarly properties shot randomly chosen speciﬁed intervals require computation optimal values The exact speciﬁcations shot properties given Table 6 332 Experiment The induced preference model shown Fig 7 64 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 Fig 6 Examples different type shots case study Fig 7 The induced preference model suggesting best action situation balls table J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 65 333 Discussion Induced relations correct S L Balls S L table In case strong direct shot preferred weak direct shot velocity black ball suﬃcient black ball reach pocket weak shot A weak shot preferred left railﬁrst shot preferred right railﬁrst shot In general railﬁrst shots successful direct shots distortions caused cue ball hitting rail However left railﬁrst shot usually better path ball shorter path right railﬁrst shot It irrelevant ball R present block right railﬁrst shot lowest preference case S L yes Ball L blocking left railﬁrst shot makes direct shots preferred railﬁrst shots The preference direct shots railﬁrst shots equally preferred S yes L R Ball S blocking direct shots makes railﬁrst shots preferable direct shots equally bad Left railﬁrst shot preferred right reason S yes L R yes Balls S R blocking shots The left railﬁrst shot preferred shots open shot S yes L yes Balls S L blocking direct left railﬁrst shots There shot preferences shots equally hopeless It right railﬁrst shot preferred similar way left railﬁrst shot preferred previous situation Fig 6c indicates black ball travel longer distance compared case Fig 6b meaning shot stronger A strong railshot diﬃcult rarely succeeds explains preferences equal case 34 Bacterial infections elderly In experiment run algorithm real problem time usefulness domain interested partial derivatives able control variables 341 Data The aim realistic study qualitatively asses inﬂuence individual risk factors mortality bacterial infection elderly population The proportion elderly people developed world rapidly growing 11 It estimated 2020 elderly constitute 16 population USA 12 Bacterial infection common cause mortality aged nearly 14 hospital admissions elderly patients bacterial infection 13 account deaths older population 14 Compared younger population bacterial infection elderly usually presents different clinical symptoms The signs infection older patients apparent absent presents unique diagnostic challenges clinicians 11 The data collected Department Infectious Diseases University Medical Center Ljubljana Slovenia Patients enrolled study period June 1st 2004 December 31st 2005 following inclusion criteria age 65 years ii hospitalization bacterial infection iii routine laboratory tests performed Data included 602 patients having Creactive protein value 60 mgl admission hospital indicated bacterial infection Patients identiﬁed prospectively data collected prospectively time study assistants blinded study purpose An infectious diseases specialist unaware ﬁnal outcome reviewed charts excluded patients nonbacterial infections We observed mortality outcome class binary variable DEATH following distribution DEATH Yes 77602 128 DEATH No 525602 872 Data consists 31 categorical attributes Table 7 deﬁned clinicians carried study Demographic data included sex age nursing home residence immobility presence permanent urine catheter pres ence pressure ulcer presence prosthetic medical device artiﬁcial heart valve joint prosthesis Comorbidities included diabetes mellitus coronary artery disease congestive heart disease chronic obstructive pulmonary disease renal impairment liver disease cerebrovascular disease immunosupression Immunosupression deﬁned prolonged therapy 6 months corticosteroids treatment citostatic agents immunomodulatory agents patients transplanted organs tissues patients malignancy Vital signs collected nursing data included body temperature respiratory rate heart rate systolic blood pressure oxygen saturation We recorded patients expe rienced frailty onset disease Frailty deﬁned inability perform daily tasks feeding bathing patients able perform onset illness We recorded mental status changed onset current disease Mental status recorded normaloriented responsive disoriented unconscious Laboratory data included leukocyte count percentage band forms platelets blood creatinine urea value glucose value serum sodium concentration presence abnormal liver function tests We recorded microbiological specimens obtained ﬁnal diagnosis discharge The primary outcome study functional decline 2128 days hospital discharge Functional decline deﬁned discharge disposition nursing home care facility signif icant decline functional cognitive ability overall quality life observed patients relatives nursing staff We set DEATH Yes target class predicting risk posed different factors 66 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 Table 7 The observed attributes values Attribute Attribute values SEX AGE NH RESIDENT COMORBIDITIES DIABETES HEART D KIDNEY D LIVER D LUNG D IMMUNOSUPPRESSION NEUROLOGICAL D MOBILITY CONTINENCE PRESSURE ULCER URINE CATHETER BODY TEMP RESPIRATORY RATE SATURATION HEART RATE BLOOD PRESSURE MENTAL CHANGE UNCONSCIOUS FRAILTY LEUKOCYTE BAND FORMS THROMBOCYTES CREATININE UREA VALUE GLUCOSE Na SITE OF INFECTION M F 65 74 75 84 85 Yes No 0 1 MANY Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No 3780 3780 1000 1000 2000 2000 9000 9000 6000 6000 10000 10000 9000 9000 Yes No Yes No Yes No 400 400 1000 1000 1000 1000 10000 10000 9000 9000 600 600 400 400 750 750 13500 13500 14500 14500 Respiratory Other Gastrointerocolitis SoftTissue Urinary 342 Experiment We built qualitative tree entire data set taking PDQ PD class variable The resulting model explains qualitative relation mortality attribute Ai assuming ceteris paribus risk change change Ai attributes values ﬁxed Table 8 lists models attributes 343 Discussion Two clinical doctors evaluated models use clinical practise They models easy understand accordance domain knowledge following models DEATH Yes KIDNEY DISEASE DEATH Yes CATHETER While additional medical tests carried explain model kidney disease analysis models CATHETER revealed algorithm discovered subgroup immobile patients usually urine catheter COMORBIDITIES cid7 0 THROMBOCYTES 100 PRESSURE ULCER Yes This results ordering No Yes leaf tree The model correctly captures patterns data imply causality 4 Related work The motivation work comes ﬁeld qualitative reasoning Qube learning qual itative models categorical data Qualitative reasoning concerned qualitative physics 1519 In works model provided expert qualitative simulations There algorithms automated induction models 20 limited learning numerical data 45 There best knowledge algorithms learning qualitative models categorical data An important method deals relaxing strong independence assumption naive Bayesian approach There exist number methods purpose ﬁts context Kononenko introduced seminaive Bayes 21 Langley Sage 22 proposed Selective Bayesian Classiﬃer variant naive method uses subset attributes making predictions Since algorithm searching subset attributes yields highest classiﬁcation accuracy reveal attribute dependencies Kohavi 23 proposed NBTree algorithm induction hybrid decisiontree classiﬁers naive Bayes classiﬁers Friedman Goldszmidt introduced tree augmented naive Bayes TAN 24 allows attributes having attribute parent Bayesian network representation SuperParent TAN proposed Keogh Pazzani 25 improves classiﬁcation accuracy TAN introducing new heuristic exploring dependencies attributes In Bayesian network representation TAN allows attributes J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 67 Table 8 Qualitative models risk factors mortality elderly patients bacterial infections Attribute SEX AGE NH RESIDENT COMORBIDITIES DIABETES HEART D KIDNEY D LIVER LUNG D IMMUNOSUPPRESSION NEUROLOGICAL D MOBILITY CONTINENCE DECUBITUS CATHETER BODY TEMP RESP RATE SATURATION HEART RATE BLOOD PRESSURE MENTAL CHANGE UNCONSCIOUS FRAILTY LEUKOCYTE BAND FORMS THROMBOCYTES CREATININE UREA VALUE GLUCOSE Na SITE OF INFECTION Qualitative model M F IMMUNOSUPPRESSION Yes 75 84 85 65 74 IMMUNOSUPPRESSION No 65 74 75 84 85 No Yes COMORBIDITIES cid7 0 SITE OF INFECTION cid7 Other Yes No 0 1 MANY Yes No IMMUNOSUPPRESSION No BLOOD PR 90 Yes No Yes No No Yes RESP RATE 10 20 SATURATION 90 Yes No No Yes Yes No SATURATION 90 MENTAL CHANGE No Yes No No Yes No Yes SITE OF INFECTION cid7 Respiratory HEART RATE 100 IMMUNOSUPPRESSION No Yes No Yes No BAND FORMS 10 Na 145 BLOOD PRESSURE 90 Yes No Yes No No Yes BAND FORMS 10 SITE OF INFECTION cid7 Other Yes No COMORBIDITIES 0 No Yes COMORBIDITIES cid7 0 THROMBOCYTES 100 DECUBITUS Yes No Yes DECUBITUS No Yes No THROMBOCYTES 100 Yes No 378 378 MENTAL CHANGE No SITE OF INFECTION cid7 Other 378 378 10 20 20 10 9000 9000 BAND FORMS 10 60 100 100 60 BAND FORMS 10 SATURATION 90 60 100 60 100 SATURATION 90 60 100 100 60 9000 9000 No Yes No Yes Yes No Na 145 4 10 10 4 10 10 100 100 CATHETER Yes 90 90 CATHETER No 90 90 6 6 DECUBITUS No IMMUNOSUPPRESSION No 6 6 4 75 75 4 UREA VALUE 6 SITE OF INFECTION cid7 Other 4 75 75 4 13500 135 145 145 RESP RATE 10 20 RespiratoryOtherGastrointerocolitisUrinary SoftTissue RESP RATE 10 20 MENTAL CHANGE Yes Urinary RespiratoryOtherGastrointerocolitisSoftTissue MENTAL CHANGE No RespiratoryOtherGastrointerocolitisSoftTissueUrinary attribute parent Keogh Pazzani approximating underlying probability distribution best way improve classiﬁcation accuracy A different approach described 26 Apriori frequent pattern mining algorithm employed discover frequent itemsets arbitrary size class supports 68 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 A lazy algorithm named Locally Weighted Naive Bayes LWNB proposed 27 LWNB relaxes independence assumption learning local models prediction time The models learned weighted set training instances neighborhood test instance In LWNB test example neighborhood chosen knearest neighbors algorithm A step Lazy Bayesian Rules LBR algorithm 28 LBR search local neighborhood based global metric Instead test example LBR uses greedy search generate Bayesian rule antecedent matches test example The basic difference approaches methods concerned optimizing accuracy predictions estimations chosen attributes inﬂuence target class probability Although Qube learning preferences fundamentally different existing preference learning approaches 2931 Preference learning usually starts data describes preferences task learning algorithms limited generalization 3235 For contrast Qube calculates PDQ PDs learning example modeling preferences While continue standard preference learning approaches 3334 use simple machine learning algorithms treat preferences values new class variable Theoretically possible number class values exceeds reasonable practically controlled setting threshold parameter joining values size neighborhood reference example kind smoothing data The relevant preference learning method apply preference learning result Qube label ranking Similarly continue learning CPnets 36 represent type preferences The main difference approach represents preferences single attribute Ai conditioned attributes CPnets simultaneously represent preferences attribute combinations Our representation suitable control attribute shown Section 31 5 Conclusion Qualitative models established tool areas science notably economics They stirred surprisingly little machine learning community We presented knowledge ﬁrst machine learning method induction qualitative models categorical data The method solid theoretical background works practice In Rat domain shown Qube correctly captures qualitative relations underlying concept relatively complex Experiments synthetic domain performs robustly presence noise irrelevant attributes The presented case studies suggest scales complex domains In billiards case study Qube successfully modeled preferences shot type different settings balls table Finally medical case study shows usefulness general labeled data preference learning decision making Acknowledgement We grateful Prof Dr Eyke Hüllermeier kind valuable discussion helped position work respect ﬁeld preference learning We like thank Prof Dr Franc Strle Dr Jerneja Videˇcnik Zorman evaluating qualitative models medical domain Dr Tadej Janež help billiards simulator Appendix In seminaive Bayesian approach replace condition pca1 relaxed condition P cai D D a1 includes attribute values conditionally dependent ai given class Here prove ordering probabilities pca1 ai change omit conditional values conditionally independent ai given class c Let ﬁrst redeﬁne PDQ PD conditional log odds ratios f Ai ai cid3 a1 sgn ln pca1 ai anpca1 ai cid3 cid3 anpca1 pca1 7 c complement target class c It easy 7 equivalent 3 Let loss generality assume values a1 ak k conditionally independent values ak1 given class Applying Bayesian rule independence assumption canceling identical terms reapplying Bayesian rule turns 7 f Ai ai cid3 a1 sgn ln pcak1 ai anpcak1 ai cid3 pcak1 cid3 anpcak1 8 This equivalent 3 values a1 ak Therefore pca1 ai pca1 ai pcak1 cid3 cid3 pcak1 J Žabkar et al Artiﬁcial Intelligence 239 2016 5469 69 References 1 S Borštnar A Sadikov B Možina T ˇCufer High levels uPA PAI1 predict good response anthracyclines Breast Cancer Res Treat 121 2010 615624 2 PA Samuelson Foundations Economic Analysis enlarged edition Harvard University Press 1983 3 B Kuipers Using qualitative reasoning IEEE Expert 12 3 1997 9497 httpdxdoiorg101109MEX1997590090 4 J Žabkar M Možina I Bratko J Demšar Learning qualitative models numerical data Artif Intell 175 910 2011 16041619 5 I Bratko D Šuc Learning qualitative models AI Mag 24 4 2003 107119 6 B Cestnik Estimating probabilities crucial task machine learning ECAI 1990 pp 147149 7 RR Sokal CD Michener A statistical method evaluating systematic relationships Univ Kans Sci Bull 28 1958 14091438 8 J Demšar T Curk A Erjavec ˇCrt Gorup T Hoˇcevar M Milutinoviˇc M Možina M Polajnar M Toplak A Stariˇc M Štajdohar L Umek L Žagar J Žbontar M Žitnik B Zupan Orange data mining toolbox Python J Mach Learn Res 14 2013 23492353 9 DG Alciatore The Illustrated Principles Pool Billiards 1st edition Sterling 2004 10 D Papavasiliou Billiards manual Tech rep 2009 httpwwwnongnuorgbilliards 11 TT Yoshikawa Epidemiology unique aspects aging infectious diseases Clin Infect Dis 30 6 2000 931933 12 KP High Why infectious diseases community focus aging care older adult Clin Infect Dis 37 2 2003 196200 13 AT Curns RC Holman JJ Sejvar MF Owings LB Schonberger Infectious disease hospitalizations older adults united states 1990 2002 Arch Intern Med 165 21 2005 25142520 14 CP Mouton OV Bazaldua B Pierce DV Espino Common infections older adults Am Fam Phys 63 2 2001 257269 15 J Kleer JS Brown A qualitative physics based conﬂuences Artif Intell 24 1984 783 16 B Kuipers Qualitative simulation Artif Intell 29 1986 289338 17 B Kuipers Qualitative Reasoning Modeling Simulation Incomplete Knowledge MIT Press Massachusetts 1994 18 K Forbus Qualitative Reasoning CRC Press 1997 19 K Forbus Qualitative process theory Artif Intell 24 1984 85168 20 M Klenk K Forbus Analogical model formulation transfer learning AP physics Artif Intell 173 18 2009 16151638 21 I Kononenko Seminaive Bayesian classiﬁer EWSL91 Proceedings European Working Session Learning Machine Learning Springer Verlag New York Inc New York NY USA 1991 pp 206219 22 P Langley S Sage Induction selective Bayesian classiﬁers Proceedings Tenth Conference Uncertainty Artiﬁcial Intelligence Morgan 23 R Kohavi Scaling accuracy naiveBayes classiﬁers decisiontree hybrid Proceedings Second International Conference Knoledge 24 N Friedman M Goldszmidt Building classiﬁers Bayesian networks Proceedings Thirteenth National Conference Artiﬁcial Intelli Kaufmann 1994 pp 399406 Discovery Data Mining 1996 gence AAAI Press 1996 pp 12771284 25 EJ Keogh MJ Pazzani Learning augmented Bayesian classiﬁers comparison distributionbased classiﬁcationbased approaches Proceed ings International Workshop Artiﬁcial Intelligence Statistics 1999 pp 225230 26 D Meretakis B Wuthrich Extending naive Bayes classiﬁers long itemsets KDD 99 Proceedings Fifth ACM SIGKDD International Conference Knowledge Discovery Data Mining ACM New York NY USA 1999 pp 165174 27 E Frank M Hall B Pfahringer Locally weighted naive Bayes Proceedings Conference Uncertainty Artiﬁcial Intelligence UAI 2003 28 Z Zheng GI Webb KM Ting Lazy Bayesian rules lazy seminaive Bayesian learning technique competitive boosting decision trees Proc 16th International Conf Machine Learning Morgan Kaufmann San Francisco CA 1999 pp 493502 29 J Fürnkranz E Hüllermeier Preference Learning SpringerVerlag 2010 30 F Aiolli A Sperduti A preference optimization based framework supervised learning problems J Fürnkranz E Hüllermeier Eds Preference Learning SpringerVerlag 2010 pp 1942 31 F Rossi KB Venable T Walsh A Short Introduction Preferences Between Artiﬁcial Intelligence Social Choice Morgan Claypool Publishers 2003 2011 32 C Boutilier RI Brafman HH Hoos D Poole CPnets tool representing reasoning conditional ceteris paribus preference statements J Artif Intell Res 21 2003 2004 ICML 05 ACM New York NY USA 2005 pp 137144 33 W Chu Z Ghahramani Preference learning Gaussian processes Proceedings 22nd International Conference Machine Learning 34 E Brochu N Freitas A Ghosh Active preference learning discrete choice data Advances Neural Information Processing Systems 2007 35 W Cheng JC Huhn E Hüllermeier Decision tree instancebased learning label ranking Proceedings 26th Annual International Conference Machine Learning ICML 2009 Montreal Quebec Canada June 1418 2009 pp 161168 36 Y Chevaleyre F Koriche J Lang J Mengin B Zanuttini Learning ordinal preferences multiattribute domains case CPnets J Fürnkranz E Hüllermeier Eds Preference Learning SpringerVerlag 2009 pp 273296