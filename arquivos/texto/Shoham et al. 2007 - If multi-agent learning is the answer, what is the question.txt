Artiﬁcial Intelligence 171 2007 365377 wwwelseviercomlocateartint If multiagent learning answer question Yoav Shoham Rob Powers Trond Grenager Department Computer Science Stanford University Stanford CA 94305 USA Received 8 November 2005 received revised form 14 February 2006 accepted 16 February 2006 Available online 30 March 2007 Abstract The area learning multiagent systems today fertile grounds interaction game theory artiﬁcial intelligence We focus foundational questions interdisciplinary area identify distinct agendas ought argue separated The goal article start discussion research community result ﬁrmer foundations area1 2007 Published Elsevier BV 1 Introduction The topic learning multiagent systems multiagent learning MAL henceforth long history game theory long history game theory itself2 As early 1951 ﬁctitious play 10 proposed learning algorithm computing equilibria games proposals evaluate success learning rules going 23 5 Since time hundreds thousands articles published topic books 20 54 In Artiﬁcial Intelligence AI history singleagent learning rich richer thousands articles books compelling applications variety ﬁelds examples 2940 50 While recent years AI branched multiagent aspects learning Corresponding author Email addresses shohamcsstanfordedu Y Shoham powerscsstanfordedu R Powers grenagercsstanfordedu T Grenager 1 This article long history owes debts A ﬁrst version presented NIPS workshop MultiAgent Learning Theory Practice 2002 A later version presented AAAI Fall Symposium 2004 Y Shoham R Powers T Grenager On agendas research multiagent learning AAAI 2004 Symposium Artiﬁcial MultiAgent Learning FS0402 AAAI Press 2004 Over time gradually evolved current form result work area feedback colleagues We thank collectively special thanks members multiagent group Stanford past years Rakesh Vohra Michael Wellman provided detailed comments latest draft resulted substantive improvements responsible views forward This work supported NSF ITR grant IIS0205633 DARPA grant HR0011051 2 Another recent term area game theory interactive learning 00043702 matter 2007 Published Elsevier BV doi101016jartint200602006 366 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 vengeance If 2003 AI literature MAL enumerating relevant articles today longer possible The leading conferences routinely feature articles MAL journals3 While AI literature maintains certain ﬂavor distinguishes game theoretic literature commonalities greater differences Indeed alongside area mechanism design computational questions surrounding solution concepts Nash equilibrium MAL today arguably fertile interaction grounds science game theory The MAL research ﬁelds produced inspiring results We repeat comprehensive article subsequently interpreted belittling achievements area Yet alongside successes indications useful step ask basic questions area MAL One surface indication presence number frustrating dead ends For example AI literature attempting extend Bellmanstyle singleagent reinforcement learning tech niques particular Qlearning 53 multiagent setting fared zerosum repeated games 36 38 commonpayoff team repeated games 143152 generalsum sto chastic games 212637 reader unfamiliar line work cover brieﬂy Section 4 Indeed close examination clear foundations MAL beneﬁt explicit dis cussion What exact question questions MAL addressing What yardsticks measure answers questions The present article focuses foundational questions To start punch line following extensive look literature reached conclusions There different agendas pursued MAL literature They left implicit conﬂated result hard evaluate compare results We identify sense ﬁve distinct research agendas Not work ﬁeld falls ﬁve agendas identify This necessarily critique work doesnt simply means identify wellmotivated deﬁned problems addressed work We expect result throwing gauntlet additional problems deﬁned past work reevaluated reconstructed Certainly hope future work conducted evaluated welldeﬁned criteria guided article discussion engendered colleagues AI game theory In general view article ﬁnal statement start discussion In order punch line outlined proceed follows In section deﬁne formal setting focus In Section 3 illustrate question learning multiagent settings inherently complex singleagent setting places stress basic game theoretic notions In Section 4 provide concrete examples MAL approaches game theory AI This com prehensive coverage area selection value judgment Our intention anchor discussion concrete beneﬁt reader familiar area andwithin formal conﬁnes discuss Section 2the examples span space MAL reasonably In Section 5 identify ﬁve different agendas usually implicit literature argue explicit teased apart We end Section 6 summary main points article A ﬁnal remark order The reader ﬁnd material sections basic obvious different readers probably ﬁnd different parts We dont mean insult anyones intelligence err explicitness reasons First article addressed different communities somewhat different backgrounds Second goal contribute clariﬁcation foundational issues dont want guilty vagueness 3 We acknowledge simpliﬁcation history There deﬁnitely MAL work AI predates years relative deluge recent Similarly focus AI action days areas science feature MAL material mean include literature Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 367 2 The formal setting We couch discussion formal setting stochastic games aka Markov games Most MAL literature adopts setting focuses narrow class repeated games Fur thermore stochastic games generalize Markov Decision Problems MDPs setting relevant learning literature AI originates These deﬁned follows A stochastic game represented tuple N S cid3A cid3R T N set agents indexed 1 n S set nagent stage games cid3A A1 An Ai set actions pure strategies agent note assume agent strategy space games notational convenience substantive restriction cid3R R1 Rn Ri S cid3A R giving immediate reward function agent stage game S T S cid3A ΠS stochastic transition function specifying probability stage game played based game played actions taken We need deﬁne way agent aggregate set immediate rewards received state For ﬁnitely repeated games simply use sum average inﬁnite games common approaches t1 δt rt rt reward received use limit average sum discounted awards time t cid2 A repeated game stochastic game stage game MDP stochastic game agent While MAL literature lives happily setting remiss acknowledge literature Certainly discuss learning context extensiveform games incomplete andor imperfect information cf 28 We dont dwell distract main discussion lessons draw setting apply Although speciﬁcally include intend comments apply general level large population games evolutionary models particularly replicator dynamics RD 47 evolutionary stable strategies ESS 49 These deﬁned follows The replicator dynamic model assumes population homoge neous agents continuously plays twoplayer game agent Formally setting expressed tuple A P0 R A set possible pure strategiesactions agents indexed 1 m P0 m i1 P0i 1 R A A R immediate reward initial distribution agents possible strategies function agent Ra acid6 giving reward agent playing strategy agent playing strategy acid6 The population changes proportions according reward strategy compares average reward dt Pt Pt aut u Pt aut A strat t egy deﬁned evolutionary stable strategy cid4 0 strategies acid6 Ra 1 cid4a cid4acid6 Racid6 1 cid4a cid4acid6 acid6 Pt acid6Ra acid6 u ut cid2 cid2 cid2 t As names suggest way interpret settings building population genetics represent ing large population undergoing frequent pairwise interactions An alternative interpretation repeated game agents distribution strategies population representing agents mixed strategy homogeneous deﬁnition agents mixed strategy exist general deﬁ nitions agents nonidentical strategies The second interpretation reduces setting discuss The ﬁrst bears discussion brieﬂy Section 4 And stay framework stochastic games What learn games Here need explicit aspects stochastic games glossed far Do agents know stochastic game including stage games transition probabilities If know speciﬁc game played stage actions available What stage game playedonly rewards actions played agents Do magically agents mixed strategy stage game And In general games known play observable We focus known fully observable games agents strategy agents strategies known priori case prior distribution In restricted setting possible things learn First agent learn opponents opponents strategy strategies agent devise best good response Alternatively agent learn strategy opponents explicitly learning opponents strategy The ﬁrst called modelbased learning second model free learning 368 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 In broader settings learn In particular unknown games learn game Some argue restricted setting true learning setting current work MAL particularly game theory takes place setting b foundational issues wish tackle surface In particular comments intended apply work AI literature games unknown payoffs work builds success learning unknown MDPs We nature learning setting stochastic games following sections 3 On special characteristics multiagent learning Before launching speciﬁcs wish highlight special nature MAL There messages like aimed AI researchers speciﬁcally broadly Both lessons gleaned simple wellknown examples Consider game described Fig 1 In game row player strictly dominant strategy Down seemingly game But imagine repeated version game If row player repeatedly plays Down assuming column player paying attention column player start responding Left end repeated Down Left play If row player starts repeatedly playing Up assuming column player awake instead start responding playing Right players end repeated Up Right play The lesson simple profound In multiagent setting separate learning teaching In example playing dominated strategy row player taught column player play way beneﬁts Indeed reason appropriate speak neutrally multiagent adaptation learning We ﬁght linguistic battle point remains important especially scientists accustomed thinking interactive considerations game theorists In particular follows priori reason expect machine learning techniques proved successful AI singleagent settings prove relevant multiagent setting The second lesson draw wellknown game Rochambeau RockPaperScissors given Fig 2 As known zerosum game unique Nash equilibrium player randomizes uniformly strategies One conclude game But suppose entered Rochambeau tournament Would simply adopt equilibrium strategy If win competition This idle speculation competitions place routinely For example starting 2002 World Rock Papers Scissors Society WRPS standardized set rules ternational play overseen annual International World Championships regional national events year These championships attended players world tracted widespread international media attention The winners equilibrium players For example October 25th 2005 495 people entered competition Toronto countries diverse Norway Northern Ireland Cayman Islands Australia New Zealand UK The winner Toronto Lawyer Andrew Bergel beat Fig 1 Stackelberg stage game The payoff row player given ﬁrst cell payoff column player following Fig 2 RockPaperScissors Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 369 Californian Stan Long ﬁnals His strategy I read minds competitors ﬁgure thinking I dont believe planning throws meet opponent These tournaments course perfect match formal model repeated games However include example entertainment value The rules RPS tournaments matches players match consisting games game single play RPS The early matches adopted best format meaning player wins best set garners point requires points match The SemiFinals Final Match best ﬁve format meaning player wins best set garners point requires points match And competition consisted series repeated games longer others4 Entertainment aside learn We believe cautioning tale predictive prescriptive role equilibria complex games particular repeated games There examples games complex strategy spaces equilibrium analysis plays little roleincluding familiar parlor games Trading Agent Competition TAC computerized trading competition5 The strategy space repeated game generally stochastic game immenseall mappings past history mixed strategies stage game In complex games reasonable expect players contemplate entire strategy spacetheir opponents Thus Nash equilibria dont play great predictive prescriptive role Our cautioning words viewed countering default blind adoption equilibria driving concept complex games sweeping statement relevance equilibria cases The simpler stage game longer repetition instructive equilibria Indeed despite example believe players play repeated RPS game long tend converge equilibrium strategy particularly true programs dont share human difﬁculty throwing mental die Even complex games examples calculation approximate equilibria restricted strategy space provided valuable guidance constructing effective strategies This includes game Poker 33 4 exception general rule mentioned program competed Trading Agent Competition 13 Our point context complex games socalled bounded rationality deviation ideal behavior omniscient agents esoteric phenomenon brushed aside 4 A partial sample MAL work To discussion concrete useful look MAL work years The selection follows representative partial value judgment bias intended selection The reader familiar literature wish skip Section 43 general subjective comments Unless indicate examples drawn special case repeated twoperson games opposed stochastic nplayer games We ease exposition bulk literature focuses special case We divide coverage parts techniques results commentary 41 Some MAL techniques We discuss classes techniquesone representative work game theory typical work AI drawn equal attention communities 4 We acknowledge degree humor example The detailed rules httpwwwrpschampscomruleshtml additional entertaining reading note restriction strategy space Rock Paper Scissors explicitly ruling Any use Dy namite Bird Well Spock Water Match Fire God Lightning Bomb Texas Longhorn nonsanctioned throws result automatic disqualiﬁcation The overview RPS society tournaments adapted inimitable Wikipedia collaborative online ency clopedia available January 2 2006 Wikipedia goes list champions 2002 note comment male Torontonians The results speciﬁc competition cited drawn online edition Boise Weekly dated November 2 2005 The Boise Weekly starts piece If werent true wouldnt report 5 httptaceecsumichedu 370 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 411 Modelbased approaches The ﬁrst approach learning discuss common game theory literature modelbased It adopts following general scheme 1 Start model opponents strategy 2 Compute play best response 3 Observe opponents play update model strategy 4 Goto step 2 Among earliest probably bestknown instance scheme ﬁctitious play 10 The model simply count plays opponent past The opponent assumed playing stationary strategy observed frequencies taken represent opponents mixed strategy Thus ﬁve repetitions Rochambeau game opponent played R S P R P current model mixed strategy R 04 P 04 S 02 There exist variants general scheme example play exact best response step 2 This typically accomplished assigning probability playing pure strategy assigning best response highest probability allowing chance playing strategies A number proposals different ways assign probabilities smooth ﬁctitious play 18 exponential ﬁctitious play 19 A sophisticated version scheme seen rational learning 30 The model distribution repeatedgame strategies One starts prior distribution example repeated Rochambeau game prior state probability 05 opponent repeatedly plays equilibrium strategy stage game k 1 probability 2k plays R k times reverts repeated equilibrium strategy After play model updated posterior obtained Bayesian conditioning previous model For instance example ﬁrst nonR play opponent posterior places probability 1 repeated equilibrium play 412 Modelfree approaches An entirely different approach commonly pursued AI literature 29 modelfree avoids building explicit model opponents strategy Instead time learns ones possible actions fare This work takes place general heading reinforcement learning6 approaches roots Bellman equations 3 The basic algorithm solving best policy known MDP starts initializing value function V0 S R value state MDP The value function iteratively updated Bellman equation Vk1 Rs γ max T s s cid6 Vks cid6 cid3 scid6 The optimal policy obtained selecting action state s maximizes expected value cid2 scid6 T s scid6Vkscid6 Much work AI focused strategies rapid convergence large MDPs particular unknown partially observable MDPs While focus brieﬂy discuss unknown case literature leading current approaches stochastic games originated For MDPs unknown reward transition functions Qlearning algorithm 53 compute optimal policy Qs 1 αt Qs αt V s max Qs aA cid4 Rs γ V s cid6 cid5 As known certain assumptions way actions selected state time constraints learning rate schedule αt Qlearning shown converge optimal value function V 6 We note term somewhat differently game theory literature Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 371 The Qlearning algorithm extended multiagent stochastic game setting having agent simply ignore agents pretend environment passive Qis ai 1 αt Qis ai αt Vis max Qis ai ai Ai cid4 Ris cid3a γ Vis cid6 cid5 Several authors tested variations basic Qlearning algorithm MAL 48 However ap proach ignores multiagent nature setting entirely The Qvalues updated regard actions selected agents While justiﬁed opponents distributions actions stationary fail opponent adapt choice actions based past history game A ﬁrst step addressing problem deﬁne Qvalues function agents actions Qis cid3a 1 αQis cid3a α cid4 Ris cid3a γ Vis cid6 cid5 We left question update V given complex nature Qvalues For deﬁnition twoplayer zerosum SGs Littman suggests minimaxQ learning algorithm V updated minimax Q values 36 cid6 cid7 s a1 a2 V1s max P1a1Q1 cid3 P1ΠA1 min a2A2 a1A1 Later work jointaction learners 14 FriendorFoe Q algorithm 37 proposed update rules Q V functions focusing special case commonpayoff team games A stage game commonpayoff outcome agents receive payoff The payoff general different different outcomes agents problem coordination called games pure coordination The work zerosum commonpayoff games continues reﬁned extended 8323452 Much work concentrated provably optimal tradeoffs exploration exploitation unknown zero sum games fascinating topic germane focus More relevant recent efforts line research extend Bellman heritage generalsum games NashQ 25 CEQ 21 We cover reasons The description involved results satisfactory 413 Regret minimization approaches Our ﬁnal example prior work MAL noregret learning It interesting example reasons First unique properties distinguish work Second AI game theory communities appear converged independently The basic idea goes early work evaluate success learning rules 523 extended rediscovered numerous times years names universal consistency noregret learning Bayes envelope 16 overview history We algorithm proposed 24 representative body work We start deﬁning regret r t aj si agent playing sequence actions si instead playing action aj given opponents played sequence si r t aj sisi cid6 R tcid3 k1 aj sk cid7 R cid7 cid6 sk sk aj si 0 time step t 1 The agent selects actions probability proportional maxr t Recently ideas adopted researchers science community 172755 Note application approaches based regret minimization restricted case repeated games The difﬁculties extending concept stochastic games discussed 39 42 Some typical results One sees kinds results literature learning algorithms presented like These 372 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 1 Convergence strategy proﬁle Nash equilibrium stage game self play agents adopt learning procedure consideration 2 Successful learning opponents strategy opponents strategies 3 Obtaining payoffs exceed speciﬁed threshold Each types comes ﬂavors examples The ﬁrst type common literature game theory AI For example ﬁctitious play general converge Nash equilibrium stage game distribution play shown converge equilibrium zerosum games 46 2 2 games generic payoffs 41 games solved iterated elimination strictly dominated strategies 42 Similarly AI 38 minimaxQ learning proven converge limit correct Qvalues zerosum game guaranteeing convergence Nash equilibrium selfplay This result makes standard sumptions inﬁnite exploration conditions learning rates proofs convergence singleagent Qlearning Claus Boutilier 14 conjecture singleagent Qlearners beliefbased joint action learners proposed converge equilibrium common payoff games conditions selfplay decreasing exploration offer formal proof FriendorFoe Q NashQ shown converge Nash equilibrium set games slight generalization set zerosum common payoff games Rational learning exempliﬁes results second type The convergence shown correct beliefs opponents repeated game strategy follows agent adopts best response beliefs agent limit agents converge Nash equilibrium repeated game This im pressive result limited factors convergence depends strong assumption absolute continuity beliefs converged correct respect aspects history observable given strategies agents This involved topic reader referred literature tails The literature noregret learning provides example type result explicit criteria evaluating learning rules For example 19 criteria suggested The ﬁrst learning rule safe deﬁned requirement learning rule guarantee minimax payoff game The minimax payoff maximum expected value player guarantee possible opponent The second criterion rule consistent In order consistent learning rule guarantee best response empirical distribution play playing opponent play governed independent draws ﬁxed distribution They deﬁne universal consistency requirement learning rule best response empirical distribution regardless actual strategy opponent employing implies safety consistency modiﬁcation ﬁctitious play algorithm achieves requirement In 20 strengthen requirement requiring learning rule adapt simple patterns play opponent The requirement universal consistency fact equivalent requiring algorithm exhibit noregret generally deﬁned follows opponents cid4 0 cid8 cid9 lim tinf 1 t max aj Ai cid10 r t aj sisi cid4 cid11 In game theory artiﬁcial intelligence large number algorithms satisfy universal consistency noregret requirements In addition recent work 6 tried combine criteria resulting GIGAWoLF noregret algorithm provably achieves convergence Nash equilibrium selfplay games players actions player Meanwhile regret matching algorithm 24 described earlier guaran tees empirical distributions play converge set correlated equilibria game Other recent work 2 addressed concerns requiring guarantees behavior limit Their algorithm guaranteed achieve cid4noregret payoff guarantees small polynomial bounds time uses agents ability observe payoff receives action Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 373 43 Some observations questions We far described work comment step ask questions representative work Our ﬁrst comment concerns settings results presented While learning procedures apply broadly results focus self play agents adopt learning procedure consideration They tend focus games agents Why work particular focus Is technical convenience learning agents agents different learning procedures relevant reason Our second comment pertains nature results With exception work noregret learning results described investigate convergence equilibrium play stage game albeit twists Is pertinent yardstick If process self play agents converge equilibrium play disturbed More generally exception noregret learning work focuses play agents converge payoffs obtain Which right focus Noregret learning distinguished starting criteria successful learning learning pro cedure The question ask particular criteria adequate In particular requirement consistency ignores basic lesson learningvsteaching discussed Section 3 By measuring perfor mance stationary opponents allow possibility teaching opponents Thus example inﬁnitely repeated Prisoners Dilemma game noregret dictates strategy defecting precluding possibility cooperation example mutually reinforcing TitForTat strategies Our goal critique existing work shine spotlight assumptions ask questions basic issues addressed questions feel discussed clearly explicitly deserve In section propose organized way thinking questions 5 Five distinct agendas multiagent learning After examining MAL literaturethe work surveyed elsewe reached conclusion distinct agendas play left implicit conﬂated We believe prerequisite success ﬁeld explicit problem addressed We identify ﬁve distinct possible goals MAL research There ones identify They clear motivation success criterion allow researchers evaluate new contributions peoples judgments diverge relative importance success date They caricatured follows 1 Computational 2 Descriptive 3 Normative 4 Prescriptive cooperative 5 Prescriptive noncooperative We consider ﬁve turn The ﬁrst agenda computational nature It views learning algorithms iterative way compute properties game solution concepts As example ﬁctitious play originally proposed way comput ing sample Nash equilibrium zerosum games 10 replicator dynamics proposed computing sample Nash equilibrium symmetric games Other adaptive procedures proposed recently computing solution concepts example computing equilibria localeffect games 35 These tend efﬁcient computation methods constitute quickanddirty methods easily understood implemented The second agenda descriptiveit asks natural agents learn context learners The goal investigate formal models learning agree peoples behavior typically laboratory experiments possibly behaviors agents example animals organizations This agenda 374 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 taken apply largepopulation models interpreted representing populations This problem clearly important taken seriously calls strong justiﬁcation learning dynamics studied One approach apply experimental methodology social sciences There good examples approach economics game theory example 15 11 There supports studying given learning process For example extent accepts Bayesian model idealized model human decision making justify Kalai Lehrers model rational learning7 However rush investigate convergence properties motivated wish anchor central notion game theory process expense motivating process rigorously8 The centrality equilibria game theory underlies agenda identify MAL lack better term called normative focuses determining sets learning rules equilibrium More precisely ask repeatedgame strategies equilibrium happens repeated games strategies embody learning rule sort For example ask ﬁctitious play Qlearning appropriately initialized equilibrium repeated Prisoners Dilemma game Although expect game theory purists ﬂock approach examples In fact example know originates AI game theory 9 explicitly rejected game theorists 18 We consider legitimate normative theory Its practicality depends complexity stage game played length play connection discussion problematic role equilibria Section 3 The agendas prescriptive ask agents learn The ﬁrst involves distributed control dynamic systems There need desire decentralize control operating dynamic environment case local controllers adapt choices This direction naturally modeled repeated stochastic commonpayoff team game attracted attention AI recent years Proposed approaches evaluated based value achieved joint policy resources required terms computation communication time required learn policy In case rarely role equilibrium analysis agents freedom deviate prescribed algorithm Examples work include 121422 small sample Researchers interested agenda access large body existing work AI ﬁelds control theory distributed computing In ﬁnal agenda termed prescriptive noncooperative ask agent act obtain high reward repeated generally stochastic game It retains design stance AI asking design optimal effective agent given environment It happens environment characterized types agents inhabiting agents learning The objective agenda identify effective strategies environments An effective strategy achieves high reward environment main characteristics environment selected class possible opponents This class opponents motivated reasonable containing opponents Convergence equilibrium goal There possible instantiations term high reward One example noregret line work discussed It clearly deﬁnes means reward high exhibit regret discussed limitations criterion A recent example AI 7 This work puts forward criteria learning algorithm multiagent setting 1 The learning converge stationary policy 2 opponent converges stationary policy algorithm converge best response There possible critiques precise criteria They weak cases opponents converge stationary strategy And strong attaining precise best response constraint opponents strategy feasible But work knowledge marks ﬁrst time formal criterion forward AI A example agenda work recent years In 45 deﬁne criterion parameterized class target opponents parameter requirements learning algorithm 1 Targeted 7 Although scope article note question justify Bayesian approach interactive setting goes familiar contravening experimental data axiomatic justiﬁcation expectedutility approach extend naturally multiagent case 8 It noted game theory somewhat unusual having notion equilibrium associated dynamics rise equilibrium 1 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 375 optimality The algorithm achieve cid4optimal payoff target opponent 2 Safety The algorithm achieve payoff security level strategy minus cid4 opponent 3 Auto compatibility The algorithm perform selfplay precise technical condition omitted We demonstrate algorithm provably meets criteria target set set stationary opponents generalsum twoplayer repeated games More recent work extended results handle opponents play conditional recent history game 44 settings players 51 6 Summary In article following points 1 Learning MAS conceptually technically challenging 2 One needs crystal clear problem addressed associated evaluation criteria 3 For ﬁeld advance simply deﬁne arbitrary learning strategies analyze resulting dynamics converge certain cases Nash equilibrium solution concept stage game This motivated 4 We identiﬁed ﬁve coherent agendas 5 Not work ﬁeld falls buckets This means need buckets work needs revisited reconstructed grounded There point like didnt natural home previous sections view important It regards evaluation methodology We focused article formal criteria believe essential However known science algorithms meet formal criteria fail practice vice versa And advocate complementing formal evaluation experimental We included comprehensive bakeoff proposed algorithms leading contenders broad range games The algorithms coded games drawn GAMUT existing testbed 43 httpgamutstanfordedu GAMUT available community large It useful learningalgorithm repository To conclude reemphasize statement beginning This article meant beginning discussion ﬁeld end References 1 K Arrow Rationality self economic Journal Business 59 4 1986 2 B Banerjee J Peng Efﬁcient noregret multiagent learning AAAI 2005 3 R Bellman Dynamic Programming Princeton University Press 1957 4 D Billings N Burch A Davidson R Holte J Schaeffer T Schauenberg D Szafron Approximating gametheoretic optimal strategies fullscale poker The Eighteenth International Joint Conference Artiﬁcial Intelligence 2003 5 D Blackwell Controlled random walks Proceedings International Congress Mathematicians vol 3 NorthHolland Amsterdam 1956 pp 336338 6 M Bowling Convergence noregret multiagent learning Advances Neural Information Processing Systems vol 17 MIT Press Cambridge MA 2005 7 M Bowling M Veloso Rational convergent learning stochastic games Proceedings Seventeenth International Joint Confer ence Artiﬁcial Intelligence 2001 8 R Brafman M Tennenholtz Rmax general polynomial time algorithm nearoptimal reinforcement learning Journal Machine Learning Research 3 2002 213231 9 R Brafman M Tennenholtz Efﬁcient learning equilibrium Artiﬁcial Intelligence 159 12 2004 2747 10 G Brown Iterative solution games ﬁctitious play Activity Analysis Production Allocation John Wiley Sons New York 1951 11 C Camerer T Ho J Chong Sophisticated EWA learning strategic teaching repeated games Journal Economic Theory 104 2002 137188 12 YH Chang T Ho LP Kaelbling Mobilized adhoc networks A reinforcement learning approach 1st International Conference Autonomic Computing ICAC 2004 2004 pp 240247 13 SF Cheng E Leung KM Lochner K OMalley DM Reeves LJ Schvartzman MP Wellman Walverine A walrasian trading agent Decision Support Systems 39 2005 169184 376 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 14 C Claus C Boutilier The dynamics reinforcement learning cooperative multiagent systems Proceedings Fifteenth National Conference Artiﬁcial Intelligence 1998 pp 746752 15 I Erev AE Roth Predicting people play games reinforcement leaning experimental games unique mixed strategy equilibria The American Economic Review 88 4 1998 848881 16 D Foster R Vohra Regret online decision problem Games Economic Behavior 29 1999 736 17 Y Freund RE Schapire A decisiontheoretic generalization online learning application boosting Computational Learning Theory Proceedings Second European Conference SpringerVerlag Berlin 1995 pp 2337 18 D Fudenberg D Kreps Learning mixed equilibria Games Economic Behavior 5 1993 320367 19 D Fudenberg D Levine Universal consistency cautious ﬁctitious play Journal Economic Dynamics Control 19 1995 1065 1089 20 D Fudenberg DK Levine The Theory Learning Games MIT Press Cambridge MA 1998 21 A Greenwald K Hall Correlated Qlearning Proceedings Twentieth International Conference Machine Learning 2003 pp 242 249 22 C Guestrin D Koller R Parr Multiagent planning factored mdps Advances Neural Information Processing Systems NIPS14 2001 23 JF Hannan Approximation Bayes risk repeated plays Contributions Theory Games 3 1957 97139 24 S Hart A MasColell A simple adaptive procedure leading correlated equilibrium Econometrica 68 2000 11271150 25 J Hu M Wellman Nash Qlearning generalsum stochastic games Journal Machine Learning Research 4 2003 10391069 26 J Hu P Wellman Multiagent reinforcement learning Theoretical framework algorithm Proceedings Fifteenth International Conference Machine Learning 1998 pp 242250 27 A Jafari A Greenwald D Gondek G Ercal On noregret learning ﬁctitious play Nash equilibrium Proceedings Eighteenth International Conference Machine Learning 2001 28 P Jehiel D Samet Learning play games extensive form valuation NAJ Economics 3 2001 29 LP Kaelbling ML Littman AP Moore Reinforcement learning A survey Journal Artiﬁcial Intelligence Research 4 1996 237285 30 E Kalai E Lehrer Rational learning leads Nash equilibrium Econometrica 61 5 1993 10191045 31 S Kapetanakis D Kudenko Reinforcement learning coordination heterogeneous cooperative multiagent systems Proceedings Third Autonomous Agents MultiAgent Systems Conference 2004 32 M Kearns S Singh Nearoptimal reinforcement learning polynomial time Proceedings Fifteenth International Conference Machine Learning 1998 pp 260268 33 D Koller A Pfeffer Representations solutions gametheoretic problems Artiﬁcial Intelligence 94 1 1997 167215 34 M Lauer M Riedmiller An algorithm distributed reinforcement learning cooperative multiagent systems Proceedings 17th International Conference Machine Learning Morgan Kaufman 2000 pp 535542 35 K LeytonBrown M Tennenholtz Localeffect games Proceedings Eighteenth International Joint Conference Artiﬁcial Intelli gence 2003 pp 772780 36 ML Littman Markov games framework multiagent reinforcement learning Proceedings 11th International Conference Machine Learning 1994 pp 157163 37 ML Littman Friendorfoe Qlearning generalsum games Proceedings Eighteenth International Conference Machine Learn ing 2001 38 ML Littman C Szepesvari A generalized reinforcementlearning model Convergence applications Proceedings 13th Inter national Conference Machine Learning 1996 pp 310318 39 S Mannor N Shimkin The empirical Bayes envelope regret minimization competitive Markov decision processes Mathematics Operations Research 28 2 2003 327345 40 T Mitchell Machine Learning McGraw Hill 1997 41 K Miyasawa On convergence learning processes 2 2 nonzeroperson game Research Memo 33 1961 42 J Nachbar Evolutionary selection dynamics games Convergence limit properties International Journal Game Theory 19 1990 5989 43 E Nudelman J Wortman K LeytonBrown Y Shoham Run GAMUT A comprehensive approach evaluating gametheoretic algo rithms AAMAS 2004 44 R Powers Y Shoham Learning opponents bounded memory Proceedings Nineteenth International Joint Conference Artiﬁcial Intelligence 2005 45 R Powers Y Shoham New criteria new algorithm learning multiagent systems Advances Neural Information Processing Systems vol 17 MIT Press Cambridge MA 2005 46 J Robinson An iterative method solving game Annals Mathematics 54 1951 298301 47 P Schuster K Sigmund Replicator dynamics Journal Theoretical Biology 100 1983 533538 48 S Sen M Sekaran J Hale Learning coordinate sharing information Proceedings Twelfth National Conference Artiﬁcial Intelligence Seattle WA 1994 pp 426431 49 JM Smith Evolution Theory Games Cambridge University Press 1982 50 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press Cambridge MA 1998 51 T Vu R Powers Y Shoham Learning multiple opponents Proceedings Fifth International Joint Conference Autonomous Agents Multi Agent Systems 2006 52 X Wang T Sandholm Reinforcement learning play optimal Nash equilibrium team Markov games Advances Neural Informa tion Processing Systems vol 15 2002 Y Shoham et al Artiﬁcial Intelligence 171 2007 365377 377 53 C Watkins P Dayan Technical note Qlearning Machine Learning 8 34 1992 279292 54 HP Young Strategic Learning Its Limits Oxford University Press Oxford 2004 55 M Zinkevich Online convex programming generalized inﬁnitesimal gradient ascent ICML 2003