Artiﬁcial Intelligence 173 2009 11331149 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint The learnability voting rules Ariel D Procaccia a1 Aviv Zohar ba Yoni Peleg b Jeffrey S Rosenschein b Microsoft Israel RD Center 13 Shenkar Street Herzeliya 46725 Israel b School Computer Science Engineering The Hebrew University Jerusalem Jerusalem 91904 Israel r t c l e n f o b s t r c t Article history Received 16 May 2008 Received revised form 25 March 2009 Accepted 27 March 2009 Available online 9 April 2009 Keywords Computational social choice Computational learning theory Multiagent systems Scoring rules voting trees broad conciselyrepresentable classes voting rules scoring rules award points alternatives according position preferences voters voting trees iterative procedures select alternative based pairwise comparisons In paper investigate PAC learnability classes rules We demonstrate class scoring rules functions preferences alternatives eﬃciently learnable PAC model With respect voting trees general learning algorithm require exponential number samples number leaves polynomial size set alternatives polynomial training set suﬃces We apply results emerging theory automated design voting rules learning 2009 Elsevier BV All rights reserved 1 Introduction Voting wellstudied method preference aggregation terms theoretical properties computa tional aspects 621 practical implemented applications use voting exist 91213 In election n voters express preferences set m alternatives To precise voter assumed reveal linear preferencesa ranking alternatives The outcome election determined according voting rule In paper consider families voting rules scoring rules voting trees Scoring rules The predominantubiquitous evenvoting rule reallife elections Plurality rule Under Plurality voter awards point alternative ranks ﬁrst preferred alternative The alternative accumulated points summed voters wins election Another example voting rule Veto rule voter vetoes single alternative alternative vetoed fewest voters wins election Yet example Borda rule voter awards m 1 points topranked alternative m 2 points second choice forththe preferred alternative awarded points Once alternative points elected The abovementioned voting rules belong important family voting rules known scoring rules A scoring rule expressed vector parameters cid3α cid4α1 α2 αmcid5 αl real number α1 cid2 α2 cid2 cid2 αm Each voter awards α1 points mostpreferred alternative α2 secondmostpreferred alternative Predictably alternative points wins Under uniﬁed framework express rules This paper subsumes earlier conference papers AD Procaccia A Zohar Y Peleg JS Rosenschein Learning voting trees Proceedings 22nd AAAI Conference AI AAAI 2007 pp 110115 AD Procaccia A Zohar JS Rosenschein Automated design scoring rules learning examples Proceedings 7th International Joint Conference Autonomous Agents Multiagent Systems AAMAS 2008 pp 951958 Corresponding author Email addresses arielprogmailcom AD Procaccia avivzcshujiacil A Zohar jonipcshujiacil Y Peleg jeffcshujiacil JS Rosenschein 1 The author supported work Adams Fellowship Program Israel Academy Sciences Humanities 00043702 matter 2009 Elsevier BV All rights reserved doi101016jartint200903003 1134 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 Fig 1 A binary voting tree Plurality cid3α cid41 0 0cid5 Borda cid3α cid4m 1 m 2 0cid5 Veto cid3α cid41 1 0cid5 A good indication importance scoring rules given fact exactly family voting rules anonymous indifferent identities voters neutral indifferent identities alternatives consistent alternative elected separate sets voters elected overall 26 Voting trees Some voting rules rely concept pairwise elections alternative beats alternative b pairwise election b majority2 voters prefers b Ideally like select alternative beats alternative pairwise election alternative called Condorcet winner exist However prominent voting rules rely concept pairwise elections select alterna tive sense close Condorcet winner In Copeland rule example score alternative number alternatives beats pairwise election alternative highest score wins In Maximin rule score alternative number votes gets worst pairwise election number voters prefer alternative predictably winner alternative scores highest When discussing voting rules possible consider abstract setting A tournament T A com plete binary asymmetric relation A alternatives b aT b bT Clearly aforementioned majority relation induces tournament beats b pairwise election iff aT b More generally relation reﬂect reality goes strict voting scenario For example tournament represent basket ball league aT b team expected beat team b game We denote set tournaments A T T A So moment let look pairwise voting rules simply functions f T A The prominent class functions class binary voting trees Each function class represented binary tree leaves labeled alternatives At node alternatives children compete winner ascends node b compete aT b ascends The winnerdetermination procedure starts leaves proceeds upwards root alternative survives root winner election For example assume alternatives b c bT cT b aT c In tree given Fig 1 b beats subsequently beaten c right subtree beats c left subtree c ultimately compete root making winner election Notice allow alternative appear multiple leaves alternatives appear example singleton tree constant function Motivation setting We consider following setting entity refer designer mind voting rule reﬂect ethics society We assume designer able constellation voters preferences presented designate winning alternative considerable computational effort In particular think designers representation voting rule black box matches preference proﬁles winning alternatives This setting relevant example designer mind different properties wants rule satisfy case given preference proﬁle designer specify winning alternative compatible properties We like ﬁnd concise easily understandable representation voting rule designer mind We refer process automated design voting rules given speciﬁcation properties societal ethics ﬁnd elegant voting rule implements speciﬁcation In paper learning examples The designer presented different preference proﬁles drawn according ﬁxed distribution For proﬁle designer answers winning alternative The number queries presented designer intuitively small possible computations designer carry order handle query complex communication costly 2 We assume simplicity odd number voters AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1135 Now assume target voting rule designer mind given black box known belong family R voting rules We like produce voting rule R close possible target rule By close mean close respect ﬁxed distribution preference proﬁles More precisely like construct algorithm receives pairs form preferences winner drawn according ﬁxed distribution D preferences outputs rule R probability according D rule target rule agree high possible We wish fact learn rules R framework formal PAC Probably Approximately Correct learning model concise introduction model given Section 2 In paper look options choice R family scoring rules family voting trees These natural choices broad classes rules concise representations Choosing R designer principle translate possibly cumbersome unknown representation voting rule succinct easily understood computed Further justiﬁcation agenda given noting diﬃcult compute voting rule instances suﬃcient simply calculate elections result typical instances The distribution D chosen designer concentrate instances Our results The dimension function class combinatorial measure richness class dimension closely related number examples needed learn class We tight bounds dimension class scoring rules providing upper bound m lower bound m 3 m number alternatives election In addition given set examples eﬃciently construct scoring rule consistent examples exists Combined results imply following theorem Theorem 31 The class scoring rules n voters m alternatives eﬃciently learnable values n m In words given combination properties satisﬁed scoring rule possible construct close scoring rule polynomial time The situation respect learnability voting trees twofold general expressiveness possible complexity binary trees number examples required exponential m However assume number leaves polynomial m required number examples polynomial m In addition investigate computational complexity problems associated learning process It worthwhile ask possible extend approach Speciﬁcally pose question given class voting rules R designer general voting rule mind voting rule known belong R possible learn close rule R We prove natural deﬁnition approximation Theorem 53 Let Rn 1 δfraction voting rules f Ln x1 xm satisfy following property voting rule Rn approximation f m family voting rules size exponential n m Let cid3 δ 0 For large values n m m 12 cid3 In particular theorem holds scoring rules small voting trees answering question posed negative respect classes Related work Currently exists small body work learning economic settings Kalai 16 explores learn ability PAC model rationalizable choice functions These functions given set alternatives choose element maximal respect linear order Similarly PAC learning recently applied computing utility functions rationalizations given sequences prices demands 2 Another prominent example paper Lahaie Parkes 17 considers preference elicitation combina torial auctions The authors preference elicitation algorithms constructed basis existing learning algorithms The learning model exact learning differs PAC learning Conitzer Sandholm 3 studied automated mechanism design restricted setting agents numerical valuations different alternatives They propose automatically designing truthful mechanism preference aggregation setting However ﬁnd solution concepts determining exists deterministic mechanism guarantees certain social welfare N P complete problem The authors problem tractable designing randomized mechanism In recent work 5 Conitzer Sandholm forward eﬃcient algorithm designing deterministic mechanisms works limited scenarios In short setting goals methods completely differentin general voting context framing computational complexity questions problematic goal speciﬁed reference expected social welfare Some authors studied computational properties scoring rules For instance Conitzer et al 6 vestigated computational complexity coalitional manipulation problem scoring rules Procaccia Rosenschein 21 generalized results ﬁnally Hemaspaandra Hemaspaandra 14 gave characterization Many papers deal complexity manipulation control elections inter alia discuss scoring rules 148152227 1136 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 The computational properties voting trees investigated One prominent example work Lang et al 18 studied computational complexity selecting different types winners elections governed voting trees Fischer et al 10 investigated power voting trees approximating maximum degree tournament Structure paper In Section 2 introduction PAC model In Section 3 present results respect scoring rules In Section 4 investigate voting trees In Section 5 discuss possible extension approach We conclude Section 6 2 Preliminaries In section short introduction PAC model generalized dimension function class A comprehensive slightly formal overview model results concerning dimension 20 In PAC model learner attempting learn function f Z Y belongs class F functions Z Y The learner given training seta set z1 z2 zt points Z sampled iid independently identically distributed according distribution D sample space Z D unknown ﬁxed z exists given learning process In paper assume realizable case target function f training examples fact labeled target function zk f cid3 z k1 The error function f F deﬁned zkt f z cid9 f 1 cid2 err f Pr zD cid3 0 parameter given learner deﬁnes accuracy learning process like achieve 0 The learner given conﬁdence parameter δ 0 provides upper bound errh cid3 cid3 Notice err f probability errh cid3 cid2 Pr errh cid3 cid3 δ 2 We formalize discussion Deﬁnition 21 See 20 1 A learning algorithm L function set training examples F following property given cid3 δ 0 1 exists integer scid3 δthe sample complexitysuch distribution D X Z sample size s samples drawn iid according D probability 1 δ holds errLZ cid3 cid3 2 L eﬃcient learning algorithm runs time polynomial 1cid3 1δ size representations target function elements X elements Y 3 A function class F eﬃciently PAClearnable eﬃcient learning algorithm F The sample complexity learning algorithm F closely related measure classs combinatorial richness known generalized dimension Deﬁnition 22 See 20 Let F class functions Z Y We F shatters S Z exist functions f g F 1 For z S f z cid9 gz 2 For S1 S exists h F z S1 hz f z z S S1 hz gz Deﬁnition 23 See 20 Let F class functions set Z set Y The generalized dimension F denoted D G F greatest integer d exists set cardinality d shattered F Lemma 24 See 20 Lemma 51 Let Z Y ﬁnite sets let F set total functions Z Y If d D G F 2d cid3 F A functions generalized dimension provides upper lower bounds sample complexity algorithms Theorem 25 See 20 Theorem 51 Let F class functions Z Y generalized dimension d Let L algorithm F sampled iid according ﬁxed unknown zkk f given set t labeled examples zk f distribution instance space X produces output f F consistent training set Then L cid3 δlearning algorithm F provided sample size obeys cid4 s cid2 1 cid3 σ1 σ2 3d ln 2 ln cid4 cid5cid5 1 δ σ1 σ2 sizes representation elements Z Y respectively 3 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1137 Theorem 26 See 20 Theorem 52 Let F function class generalized dimension d cid2 8 Then cid3 δlearning algorithm F cid3 cid3 18 δ 14 use sample size s cid2 d 16cid3 3 Learnability scoring rules Before diving introduce notation Let N 1 2 n set voters let A x1 x2 xm set alternatives denote alternatives b c Let L L A set linear preferences3 A voter preferences cid11i L We denote preference proﬁle consisting voters preferences cid11N cid4cid111 cid112 cid11ncid5 A voting rule function f LN A maps preference proﬁles winning alternatives Let cid3α vector m nonnegative real numbers αl cid2 αl1 l 1 m 1 Let f cid3α LN C scoring rule deﬁned vector cid3α voter awards αl points alternative ranks lth place rule elects alternative points Since alternatives maximal scores election adopt method tiebreaking Our method works follows Ties broken favor alternative ranked ﬁrst voters alterna tives maximal scores ranked ﬁrst number voters tie broken favor alternative ranked second voters on4 Let Sn function f cid3α Sn distribution LN let x jk k denote π k x j place l Notice alternative x j s score preference proﬁle cid11N m class scoring rules n voters m alternatives Our goal learn PAC model target k drawn ﬁxed jl number voters ranked alternative k m To end learner receives training set cid11N k For proﬁle cid11N k k cid11N k f cid3α cid11N f cid3α cid11N l π k jlαl cid6 31 Eﬃcient learnability Sn m Our main goal section prove following theorem Theorem 31 For n m N class Sn m eﬃciently PAClearnable By Theorem 25 order prove Theorem 31 suﬃcient validate following claims 1 exists algorithm training set runs time polynomial n m size training set outputs scoring rule consistent training set assuming exists 2 generalized dimension class Sn m polynomial n m Remark 32 It possible prove Theorem 31 transformation scoring rules sets linear threshold functions Indeed known VC dimension restriction generalized dimension booleanvalued functions linear threshold functions Rd d 1 In principle possible transform scoring rule linear threshold function receives generally speaking vectors rankings alternatives input Given training set proﬁles transform training set rankings use learning algorithm However interested producing accurate scoring rule according distribution D preference proﬁles represents typical proﬁles It possible consider manytoone mapping distributions proﬁles distributions abovementioned vectors rankings Unfortunately procedure nontrivial guarantee learned voting rule succeeds according original distribution D Moreover procedure require increase sample complexity compared analysis given Therefore proceed direct agenda outlined detailed It straightforward construct eﬃcient algorithm outputs consistent scoring rules Given training set choose parameters scoring rule way example score designated winner large scores alternatives Moreover ties winner loser broken favor loser winners score strictly higher losers Our algorithm given Algorithm 1 simply formulates constraints linear inequalities solves resulting linear program The ﬁrst algorithm meant handle tiebreaking Recall x jk f cid3α cid11N k A linear program solved time polynomial number variables inequalities 24 follows Algorithm 1s running time polynomial n m size training set Remark 33 Notice vector cid3α standard representation rational coordinates numerator denominator integers represented polynomial number bits scaled equivalent vector 3 A binary relation antisymmetric transitive total 4 In case alternatives maximal scores identical rankings break ties arbitrarilysay favor alternative smallest index 1138 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 k 1 s Xk x j cid9 x jk cid3π k cid3π cid7 cid3π k j jk l0 minl π cid7 l π cid7 0 l0 Xk Xk x j cid9 0 end end cid2 x jk winner example k cid2 Ties broken favor x j end return feasible solution cid3α following linear program cid6 cid6 cid6 l π k l π k k x j Xk k x j Xk l 1 m 1 αl cid3 αl1 l αl cid3 0 jk lαl cid3 jk lαl cid3 cid6 l π k l π k jlαl 1 jlαl Algorithm 1 Given training set size s algorithm returns scoring rule consistent given examples exists integers polynomially representable In case scores integral Thus instead strict inequality LPs ﬁrst set constraints use weak inequality additive term 1 Remark 34 Although transformation learning scoring rules learning linear threshold functions mentioned Remark 32 drawbacks learning method conjecture results computational complexity learning linear threshold functions leveraged obtain computational eﬃciency Indeed wellknown algorithms Winnow 19 suit purpose Remark 35 Algorithm 1 check high probability voting rule designer mind scoring rule described different context Kalai 16 omit details This justiﬁes setting voting rule designer mind known scoring rule So remains demonstrate generalized dimension Sn m polynomial n m The following lemma shows Lemma 36 The generalized dimension class Sn m m cid8 cid7 Sn m D G cid3 m Proof According Deﬁnition 23 need set cardinality m 1 shattered Sn m Let m1 S cid11N k1 set let h g social choice functions disagree preference proﬁles S We k shall construct subset S1 S scoring rule f cid3α agrees h S1 agrees g S S1 1 x1 1 ties x1 x2 broken favor x1 Let cid3α parameter vector If 1 We shall assume loss generality hcid11N Let look ﬁrst preference proﬁle set cid11N gcid11N hcid11N mcid9 1 x2 cid11N 1 f cid3αcid11N mcid9 π 1 1l αl cid2 π 1 2l αl 1 hold l1 l1 wanted f cid3α agree g want opposite mcid9 l1 π 1 1l αl mcid9 l1 π 1 2l αl 4 5 More generally deﬁne respect proﬁle cid11N k vector cid3π k cid7 vector lth coordinate difference number times winner h winner g ranked lth place5 cid3π k cid7 cid3π k hcid11k cid3π k gcid11k 6 Now concisely write necessary conditions f cid3α agreeing cid11N k h g respectively writing6 5 There abuse notation hcid11N 6 In proﬁles cid11N k xl cid3π k 1 indifferent direction ties broken hcid11k mean cid3π k l AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 cid3π k cid7 cid3π k cid7 cid3α cid2 0 cid3α cid3 0 1139 7 8 Notice vector cid3π k cid7 exactly m coordinates Since m 1 vectors corresponding m 1 proﬁles S subset vectors linearly dependent We express vectors linear combination Without loss generality assume ﬁrst proﬁles vector written combination parameters βk 0 cid3π 1 cid7 m1cid9 k2 βk cid3π k cid7 Now shall construct subset S1 preference proﬁles follows cid11 cid10 k 2 m 1 βk cid2 0 S1 9 10 Suppose way contradiction f cid3α agrees h cid11N k k S1 g rest We shall examine value cid3π 1 cid7 cid3α cid3π 1 cid7 cid3α m1cid9 k2 βk cid3π k cid7 cid3α cid9 kS1 βk cid3π k cid7 cid3α cid9 k S11 βk cid3π k cid7 cid3α cid2 0 11 cid3α nonpositive f cid3α agrees The inequality construction S1whenever βk negative sign cid3π k cid7 g βk positive sign cid3π k cid7 1 cid9 x2 gcid11N Therefore Eq 5 f cid11N g outside S1this contradiction cid2 cid3α nonnegative agreement h 1 However holds 1 S1 assumed f cid3α agrees Theorem 31 proven The upper bound generalized dimension Sn m tight subsection lower bound m 3 32 Lower bound generalized dimension Sn m Theorem 26 implies lower bound generalized dimension function class directly connected complexity learning In particular tight bound dimension gives exact idea number examples required learn scoring rule Therefore wish bound D G Sn m Theorem 37 For n cid2 4 m cid2 4 D G Sn m cid2 m 3 Proof We shall produce example set size m 3 shattered Sn l 3 m 1 follows For l voters 1 n 1 rank alternative x j place j vote x1 cid11i l xm The preferences cid11n l deﬁned follows alternative x2 ranked place l l alternative x1 ranked place l 1 alternatives ranked arbitrarily voter n For example m 5 n 6 preference proﬁle cid11N m Deﬁne preference proﬁle cid11N l cid11i preferences voter n proﬁle cid11N l x2 cid11i l 3 cid111 3 x1 x2 x3 x4 x5 cid112 3 x1 x2 x3 x4 x5 cid113 3 x1 x2 x3 x4 x5 cid114 3 x1 x2 x3 x4 x5 cid115 3 x1 x2 x3 x4 x5 cid116 3 x3 x4 x2 x1 x5 Lemma 38 For scoring rule f cid3α α1 α2 cid2 2α3 holds cid12 cid8 cid7 cid11N l f cid3α x1 αl αl1 x2 αl αl1 Proof We shall ﬁrst verify x2 maximal score x2s score n 1α2 n 1α1 Let j cid2 3 x j s score n 1α3 α1 Thus difference n 1α1 α3 α1 Since α1 α2 cid2 2α3 n 1α12 α1 0 inequality holds n cid2 4 1140 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 Now preference proﬁle cid11N l x1s score n 1α1 αl1 x2s score n 1α1 αl If αl αl1 alternatives identical scores x1 ranked ﬁrst voters fact n 1 voters winner x1 If αl αl1 x2s score strictly higherhence case x2 winner cid2 Armed Lemma 38 prove set cid11N l 2α1 2α1 4 l x1 f cid3α2 cid11N m cid3α2 α1 l x2 cid2 2α1 α1 2 2α1 3 f cid3α1 cid11N 1 m1 l3 3 2α1 shattered Sn m Let cid3α1 α1 cid2 m By lemma l 3 m 1 α1 2 1 4 2α1 Let T 3 4 m 1 We exists cid3α f cid3αcid11N l x2 l T Indeed conﬁgure parameters α1 α2 2α3 αl αl1 iff l T The result follows directly Lemma 38 cid2 l x1 l T f cid3αcid11N 4 Learnability voting trees Recall dealing set alternatives A x1 xm denote alternatives b c A A tournament complete binary irreﬂexive relation T A denote set possible tournaments T T A A binary voting tree binary tree leaves labeled alternatives To determine winner election respect tournament T iteratively select siblings label parent winner according T remove siblings tree This process repeated root labeled label winner election A preference proﬁle cid11N set voters N induces tournament T T A follows aT b dominates b majority voters prefer b Thus voting tree particular voting rule deﬁned Section 3 However purposes section suﬃcient regard voting trees functions f T A A disregard set voters simply consider dominance relation T A We shall hereinafter refer functions f T A A pairwise voting rules Let denote class voting trees m alternatives Vm emphasize class depends m We like know sample complexity learning functions Vm To elaborate bit think voting trees functions T A sample space T 41 Large voting trees In section general answer question complexity exponential m We prove relying Theorem 26 theorem implies order prove claim suﬃcient demonstrate generalized dimension Vm exponential m This task presently turn Theorem 41 D G Vm exponential m Proof Without loss generality let m 2k 2 We associate distinct binary vector v cid4v 1 vkcid5 0 1k distinct example set tournaments S T To prove theorem Vm shatters set S size 2k Let set alternatives cid10 b x0 A 1 x1 1 x0 2 x1 2 x0 k x1 k cid11 T v bT v x0 For vector v 0 1k deﬁne tournament T v follows 1 k v 0 let x0 T v bT v x1 j j 0 1 beats x In addition tournaments T v 1 k v 1 x1 loses b We denote S set 2k tournaments7 Let f constant function b voting tree consists node b let g constant function We prove S 1 S voting tree b wins tournament S1 words tree agrees f wins tournament S S 1 tree agrees g Consider tree Fig 2 refer ith 2gadget 7 The relations described complete way completed consequence Fig 2 2gadget AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1141 Fig 3 vgadget Fig 4 vgadget With respect tree b wins tournament T v S iff v j Indeed v j x j b loses x v cid9 j x T v bT v x 1 j 1 j 1 j beats x j T v bT v x 1 j particular b Let v 0 1k We use 2gadget build tree b wins tournament T v S loses tournament S Consider balanced tree deepest nodes tree fact 2gadgets Fig 3 As b wins ith 2gadget iff v j We refer tree vgadget entire election On hand let v holds x0 Now notice b wins 2gadgets case tournament T v b winner cid9 v 1 Then proceeds win entire election j l alternative beats b survived lth cid16 cid9 v exists 1 k wlog 0 v wins ith 2gadget x0 implies x0 T vcid16 bT vcid16 x1 cid16 beaten stage alternative x 2gadget In case b win election Consider small extension Fig 4 vgadget lack better vgadget Recall tournament S beats alternative xi j loses b Therefore discussion tournament T v tournament S vgadget b wins election described vgadget alternative wins election We present tree prove required tournament S 1 b winner tournament S S1 prevails Let enumerate tournaments S1 S1 T v1 T vr We construct balanced tree Fig 5 levels consist vlgadgets l 1 r Let T vl vlgadget b proceeds win election Conversely let T v S S1 Then survives vlgadget proceeds win entire election S1 What result tournament election described tree First note b prevails The alternatives reach level gadgets b b beats Therefore l 1 r surely We shown Vm shatters S completing proof cid2 Remark 42 Even restrict attention class balanced voting trees corresponding playoff schedule dimension class exponential m Indeed unbalanced tree transformed identical voting rule balanced tree If trees height h replacing leaf depth d h labeled alternative balanced subtree height d h leaves labeled This implies class balanced trees shatter sample shattered Vm Remark 43 The proof completed Lemma 24 imply number different pairwise voting rules represented trees double exponential m highlights high expressiveness voting trees 1142 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 Theorem 41 coupled Theorem 26 implies sample complexity learning arbitrary voting trees expo Fig 5 The constructed tree nential n m 42 Small voting trees In previous section seen general large number examples needed order learn voting trees PAC model This result relied number leaves trees exponential number alternatives However realistic settings expect voting tree compactly represented particular usually expect number leaves polynomial m Let denote V k m class voting trees m alternatives k leaves Our goal section prove following theorem Theorem 44 D G V k m Ok log m This theorem implies particular number leaves k polynomial m dimension V k m polynomial m In turn implies Lemma 25 sample complexity V k m polynomial m In words polynomial pm 1cid3 1δ given training set size pm 1cid3 1δ algorithm returns tree consistent training set cid3 δlearning algorithm V k m To prove theorem require following straightforward lemma Lemma 45 V k m cid3 k mk Ck1 Ck kth Catalan number given cid4 cid5 Ck 1 k 1 2k k Proof The number voting trees exactly k leaves number binary tree structures multiplied number possible assignments alternatives leaves The number assignments clearly bounded mk Moreover known number rooted ordered binary trees k leaves k 1 Catalan number So total number voting trees exactly k leaves bounded mk Ck1 number voting trees k leaves k mk Ck1 cid2 We ready prove Theorem 44 Proof Theorem 44 By Lemma 45 cid13 cid13V k m cid13 cid13 cid3 k mk Ck1 Therefore Lemma 24 cid8 cid7 V k m D G cid3 log cid13 cid13V k m cid13 cid13 Ok log m cid2 43 Computational complexity In previous section restricted attention voting trees number leaves polynomial k We demonstrated dimension class polynomial m implies sample complexity AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1143 class polynomial m Therefore algorithm consistent training set polynomial size suitable learning algorithm Theorem 25 It signiﬁcant bottleneck especially setting automated voting rule design ﬁnding compact representation voting rule designer mind number queries posed designer regard satisﬁed realistic voting trees learnable Nonetheless contexts interested computational complexity given training set polynomial size computationally hard ﬁnd voting tree consistent training set In section explore question We assume hereinafter structure voting tree known priori This assumption observe balanced trees Theorems 41 44 hold regardless We shall try determine hard ﬁnd assignment leaves consistent training set We refer computational problem TreeSAT pun intended Deﬁnition 46 In TreeSAT problem given binary tree leaves labeled alternatives training set consists pairs T j xi j T j T xi j A We asked exists assignment alternatives rest leaves consistent training set j winner T j respect tree xi j Notice formulation problem leaves labeled However reasonable expect eﬃcient algorithm ﬁnds consistent tree given exists able solve TreeSAT problem Hence N P hardness result implies algorithm likely exist Theorem 47 TreeSat N P complete Proof It obvious TreeSAT N P In order N P hardness present reduction 3SAT In problem given conjunction clauses clause disjunction literals One asked given formula satisfying assignment It known 3SAT N P complete 11 Given instance 3SAT variables x1 xm clauses l j 1 l j 2 l j 3 k j1 construct instance TreeSat follows set alternatives A b x1 x1 c1 x2 x2 c2 xm xm cm For clause j deﬁne tournament T j tournament satisﬁes following restrictions j 1 l j 1 l j 2 l 2 loses l 3 beat alternative alternatives xi xi possibly excluding negations j 1 l 3 beats alternative alternatives xi xi j 2 l j In addition tournaments instance TreeSAT satisfy following conditions 1 b beats alternative corresponds literal loses 2 For 1 m xi beats xi 3 ci loses xi xi beats literal alternatives b The tournaments arbitrarily deﬁned respect competitions ci ck cid9 k Finally tournament require winner alternative b We proceed construct given partially assigned tree We start proof Theorem 41 deﬁning gadget igadget illustrated Fig 6 In subtree leaves assigned xi ci Now respect tournaments deﬁned assign xi leaf xi proceeds beat ci subsequently beats xi If assign xi leaf xi beats ci wins election If assign alternative ck k 1 m alternative defeated ci turn beaten xi Finally ck assigned loses ci xi winner beats ci proceeds beat xi To conclude point xi xi ck k cid9 survive igadget Using igadgets design tree complete construction TreeSAT instance tree described Fig 7 Fig 6 igadget 1144 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 Fig 7 The reduction We prove reduction We ﬁrst given 3SAT instance satisﬁable assignment leaves tree particular choices xi xi m tournaments winner b Consider satisfying assignment 3SAT instance For literal li assigned truth value assign label li unlabeled leaf igadget li survive igadget Now consider j tournament T j At literals l 3 true literals beat literals tournament T j literals reaches competition versus wins subsequently literal loses alternative b Therefore b winner election Since true j 1 m assignment consistent given tournaments j 2 l j 1 l In direction consider instance 3SAT satisﬁable ﬁx assignment leaves tree A ﬁrst case consider assignment ck survives igadget cid9 k ck beaten way root tree c alternative Hence b win constructed tournaments A second case consider igadget xi xi survives The corresponding assignment 3SAT j j 1 l 3 false This implies T j instance satisfying Therefore j l literal reaches tree compete loses Subsequently competes b wins making winner election respect tournament T j Hence assignment consistent tournamentsbut true respect assignment cid2 j 2 l Despite Theorem 47 practice solving TreeSat problem possible shall empirically demonstrate Our simulations carried follows Given ﬁxed tree structure randomly assigned alternatives pool 32 alternatives leaves tree We tree determine winners 20 random tournaments 32 alternatives Next measured time took ﬁnd assignment leaves tree necessarily original consistent training set 20 tournaments We repeated procedure 10 times number leaves 4 8 16 32 64 took average runs The problem ﬁnding consistent tree easily represented constraint satisfaction problem particular SAT problem Indeed node simply add constraint tournament involves node children To ﬁnd satisfying assignment SAT solver zChaff The simulations carried PC Pentium D dual core CPU running Linux 2 GB RAM 28 GHz clock speed We experimented different tree structures The ﬁrst seemingly simplesta binary tree close chain possible node leaf parent leaf refer trees caterpillars The second intuitively complicated balanced tree Notice given number leaves k number nodes cases 2k 1 The simulation results shown Fig 8 In case balanced trees hard ﬁnd consistent tree Adding sample tournaments add constraints task harder However elections number alternatives usually dozen problem solvable Furthermore problem far easier respect caterpillars reduction Theorem 47 builds trees caterpillars Therefore surmise tree structures practically possible terms computational effort ﬁnd consistent assignment input relatively large problem computationally hard practice AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1145 5 On learning voting rules close target rules Fig 8 Time ﬁnd satisfying assignment Heretofore concentrated learning voting rules known scoring rules voting trees In particular assumed scoring rule voting tree consistent given training set In section push envelope asking following question given examples consistent Mathematically actually asking exist target voting rules f general voting rule possible learn scoring rule small voting tree close target rule min f cid3α Sn err f cid3α err f large This course depends underlying distribution D In rest section implicit min f V m assumption D simplest nontrivial distribution proﬁles uniform distribution Nevertheless uniform distribution usually reﬂect real preferences voters assumption making sake analysis In light discussion deﬁnition distance voting rules going fraction preference proﬁles rules disagree m Deﬁnition 51 A voting rule f LN A capproximation voting rule g iff possible preference proﬁles cid10 cid13 cid13 cid11N LN f cid8 cid7 cid11N cid8cid11cid13 cid7 cid11N g cid13 cid2 c mn f g agree cfraction In words question given training set cid11N k f cid11N j k f LN A voting rule hard learn scoring rule voting tree capproximates f c close 1 It turns answer impossible We shall ﬁrst extreme example case scoring rules Indeed voting rules disagree scoring rule preference proﬁles target rule f rule impossible ﬁnd course impossible learn scoring rule close f In order consider following voting rule ﬂipped veto voter awards point alternative ranks winner alternative points In addition ties broken according lexicographic order alternative names This rule course reasonable preference aggregation method stillit valid voting rule Proposition 52 Let f cid3α scoring rule capproximation ﬂipped veto Then c cid3 1m A Deﬁne set Bcid11N LN Proof Let cid11N preference proﬁle f cid3αcid11N ﬂipped vetocid11N x follows proﬁle set obtained switching place alternative x A x cid9 x place x Bcid11N obtained switching ordering voter rank x x x score decrease result switches situation terms tiebreaking remained change In addition f cid3α situation x cid11N cid11N voters score alternatives remains unchanged switched alternatives ones rank x 1 respect score tiebreaking good situation x last8 For preference proﬁle cid11N 1 holds winner ﬂipped veto x x 8 It case voters ranked x tiebreaking assumption respect f cid3α 1146 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 Note case x x x follows f cid3αcid11N ranked voter cid11N f cid3αcid11N score cid11N 1 x Therefore preference proﬁle Bcid11N f cid3α ﬂipped veto agree 1 ﬂipped vetocid11N x 1 ties x x holds broken favor x It Bcid11N 2 We claim preference proﬁles cid11N Indeed assume exists cid11N Bcid11N case alternative switched x cid11N 1 cid11N Bcid11N 1 2 identical Therefore assume wlog x1 switched x 2 f cid3α ﬂipped veto agree holds Bcid11N 2 Assume ﬁrst winner proﬁles x order obtain cid11N cid11N 2 imply 1 rankings voters 2 But means x1 x2 winners cid11N f cid3α 1 cid11N It cid11N 1 1 cid11N rank x fact x x2 switched x 1 cid11N winner cid11N cid11N 2 contradiction 2 1 cid11N In addition preference proﬁles cid11N cid8 cid7 cid11N 1 cid7 cid11N ﬂipped veto 1 cid8 f cid3α x cid8 cid7 cid11N 2 f cid3α cid7 ﬂipped veto cid11N 2 Bcid11N cid8 x holds Bcid11N 1 proﬁles Bcid11N 1 It follows preference proﬁle f cid3α ﬂipped veto agree m 1 distinct proﬁles proﬁles Bcid11N 2 elects x 2 ﬂipped veto elects x voting rules disagree proves proposition cid2 We shall formulate main result section The theorem states voting rule 2 small family voting rules We shall subsequently theorem approximated factor better 1 holds small voting trees scoring rules Theorem 53 Let Rn 1 δfraction voting rules f Ln x1 xm satisfy following property voting rule Rn approximation f m family voting rules size exponential n m let cid3 δ 0 For large values n m m 12 cid3 Proof We surround voting rule f Rn 12 cid3approximation We union balls covers δfraction set space voting rules This implies 1 δfraction voting rules scoring rule 12 cid3 approximation m ball B f contains voting rules f For given f size B f As mn possible preference proﬁles ball contains rules agree f 12 cid3mn preference proﬁles For proﬁle disagreement m options set image disagreeing rule9 Therefore cid5 m12cid3mn cid13 cid13 cid13 cid3 cid13B f 12 cid4 mn 12 cid3mn How large expression Let B cid16 f set voting rules disagree f exactly 12 cid3mn preference proﬁles It holds cid4 cid13 cid13B cid16 cid13 cid13 f mn 12 cid3mn mn 12 cid3mn mn 12 cid3mn cid4 cid4 cid2 cid5 m 112cid3mn cid5 cid7 m 112cid3 cid8 12mn cid5 m12mn inequality holds large m But total number voting rules mmn number rules B cid16 f cid7 cid8 mn m12mn 12cid3mn cid8 m12cid3mn cid7 mn 12cid3mn cid2 cid2 B cid16 f B f mcid3mn mmn B f Therefore B f cid3 mmn mcid3mn m1cid3mn 9 This reasoning takes account voting rules agree f 12 cid3mn proﬁles 13 greater 14 15 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1147 If union balls cover δfraction set voting rules Rn m equivalently hold Rn m double exponential large values n m condition hold cid2 However assumption Rn m cid2 δ mcid3mn m1cid3mn cid2 δ mmn exponential n m Notice number distinct voting trees k leaves voting rules f LN A A m bounded number voters n expression given Lemma 45 k mk Ck1 Therefore corollary Theorem 53 Corollary 54 For large values n m voting rules approximated V k factor better 1 2 m k polynomial m In order obtain similar corollary scoring rules require following lemma indepen dent Lemma 55 There exists polynomial pn m n m N Sn m cid3 2pnm Proof It true inﬁnite number ways choose vector cid3α deﬁnes scoring rule Nevertheless interested number distinct scoring rules For instance cid3α1 2 cid3α2 f cid3α1 f cid3α2 vectors deﬁne voting rule It clear scoring rules f cid3α1 f cid3α2 distinct following condition holds exist alternatives C preference proﬁle cid11N f cid3α1 cid11N x j1 f cid3α2 cid11N x j2 This holds exist x j1 x j2 alternatives x j1 x j2 preference proﬁle cid11N cid3α1 x j1 s score strictly greater x j2 s cid3α2 x j2 s score greater alternatives tied tie broken favor x j2 Now assume cid11N induces rankings cid3π j1 cid3π j2 The conditions written cid9 l cid9 l π j1lα1 l π j1lα2 l cid3 cid9 l cid9 l π j2lα1 l π j2lα2 l 16 17 inequality equality ties broken favor x j2 l0 minl π j1l cid9 π j2l π j1l π j2l10 Let cid3πcid7 cid3π j1 cid3π j2 As proof Lemma 36 Eqs 16 17 concisely rewritten cid3πcid7 cid3α1 0 cid2 cid3πcid7 cid3α2 18 inequality equality ﬁrst nonzero position cid3πcid7 negative In order continue opt reinterpret discussion geometrically Each point Rm corresponds possible choice parameters cid3α Now possible choice cid3πcid7 normal hyperplane These hyperplanes partition space cells vectors interior cell agree signs dot products vectors cid3πcid7 More formally cid3α1 cid3α2 points interior cell vector cid3πcid7 cid3πcid7 cid3α1 0 cid3πcid7 cid3α2 0 By Eq 18 implies scoring rules f cid3α1 f cid3α2 cid3α1 cid3α2 interior cell identical What points residing intersection cells These vectors agree vectors cells ties broken according rankings induced preference proﬁle according parameters deﬁne hyperplanes Therefore points intersection conceptually annexed cells So reached conclusion number distinct scoring rules number cells Hence bound number cells claim number exponential n m Indeed cid3πcid7 mvector coordinate integer set n n 1 n 1 n It follows 2n 1m possible hyperplanes It known 7 given k hyperplanes ddimensional space number cells O kd In case k cid3 2n 1m d m obtained bound cid7 2log 3n cid8 m cid3 3nm2 cid7 2n 1m 2m2 log 3n cid8 m2 19 cid2 Remark 56 This lemma implies according Lemma 24 exists polynomial pn m n m N D G Sn m cid3 pn m However obtained tighter upper bound m Finally Theorem 53 Lemma 55 obtain 10 Wlog disregard case cid3π j1 factor cid3π j2 reader verify taking case account multiplies ﬁnal result exponential 1148 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 Corollary 57 For large values n m voting rules approximated Sn m factor better 1 2 Remark 58 Proposition 52 seemingly circumvented removing requirement scoring rule deﬁned vector cid3α αl cid2 αl1 l Indeed ﬂipped veto essentially scoring rule αm 1 αl 0 l cid9 m However constant voting rule elects alternative inapproximability ratio property scoring rules taken account Moreover Corollary 57 holds scoring rules assumed satisfy property 6 Discussion We demonstrated possibility learning scoring rules small voting trees We argued given black box speciﬁcation choice criteria society learning examples allows eﬃciently albeit approximately design rules The black box reﬂects ideal voting rule designer mind satisﬁes instance different desirable properties The designer essentially translates cumbersome representation voting rule hidden black box concisely represented voting rule easy understand apply In Section 5 explored possibility extending approach setting designer mind general voting rule scoring rule voting tree like ﬁnd scoring rule voting tree close possible Technically learningtheoretic results basically hold polynomial factors sample complexity setting situation diﬃcult terms computational complexity Unfortunately turns Corollaries 54 57 voting rules approximated scoring rules small voting trees However negative result relied implicitly assuming uniform distribution proﬁles More importantly case important families voting rules approximated scoring rules small voting trees Therefore rule point application approach designing general voting rules directly learning scoring rules small voting trees approximate Criticisms approach A possible concern given Corollaries 54 57 general motivation Indeed assume designer mind scoring rule argued designer aware fact knowledge parameters rule However recall class scoring rules exactly class anonymous neutral consistent voting rules 26 Hence designer selects winners way satisﬁes desiderata scoring rule unknown parameters obtained A similar case voting trees The underlying assumption literature implementation voting trees 10 references voting trees abstract model decision making voting rules fact represented voting trees transformation obvious For example Copeland rule selects alternative beats largest number alternatives pairwise elections represented elaborate voting tree seven alternatives 23 Hence designer voting rule represented voting tree unaware exact representation Let discuss additional possible criticisms general setting First notice multiagent environ ments number alternatives m large example agents voting joint plans 9 number alternatives signiﬁcantly larger number agents Hence complexity results depend num ber alternatives meaningful Second suggested designer ﬁnd easier express ethical properties considered mandatory express voting rule examples We argue rarely case Indeed diﬃcult concisely represent properties computational settings universal agreedupon language hard imagine creating language On hand specifying voting rule polynomial number examples provides concise description voting rule shown lead close approximation Future work We mention directions future research First imagine following scenario designer mind huge voting tree like know exists smaller voting tree implements voting rule The goes scoring rules designer mind scoring rule huge values components vector cid3α This setting closely related results hold alternative setting Second prove interesting study learnability larger families voting rules concise repre sentation One compelling example class generalized scoring rules recently proposed Xia Conitzer 25 Acknowledgements The authors wish thank anonymous AIJ AAAI AAMAS reviewers helpful comments This work partially supported Israel Science Foundation grant 89805 References 1 J Bartholdi CA Tovey MA Trick How hard control election Mathematical Computer Modelling 16 1992 2740 AD Procaccia et al Artiﬁcial Intelligence 173 2009 11331149 1149 2 E Beigman R Vohra Learning revealed preference Proceedings 7th ACM Conference Electronic Commerce ACMEC 2006 pp 3642 3 V Conitzer T Sandholm Complexity mechanism design Proceedings 18th Annual Conference Uncertainty Artiﬁcial Intelligence UAI 2002 pp 103110 4 V Conitzer T Sandholm Universal voting protocol tweaks manipulation hard Proceedings 18th International Joint Conference Artiﬁcial Intelligence IJCAI 2003 pp 781788 5 V Conitzer T Sandholm An algorithm automatically designing deterministic mechanisms payments Proceedings 3rd Interna tional Joint Conference Autonomous Agents Multiagent Systems AAMAS 2004 pp 128135 6 V Conitzer T Sandholm J Lang When elections candidates hard manipulate Journal ACM 54 2007 133 7 H Edelsbrunner Algorithms Combinatorial Geometry EATCS Monographs Theoretical Computer Science vol 10 Springer 1987 8 E Elkind H Lipmaa Small coalitions manipulate voting Proceedings Annual Conference Financial Cryptography FC Lecture Notes Computer Science SpringerVerlag 2005 9 E Ephrati JS Rosenschein A heuristic technique multiagent planning Annals Mathematics Artiﬁcial Intelligence 20 1997 1367 10 F Fischer AD Procaccia A Samorodnitsky A new perspective implementation voting trees Manuscript 2008 11 MR Garey DS Johnson Computers Intractability A Guide Theory NPCompleteness WH Freeman Company 1979 12 S Ghosh M Mundhe K Hernandez S Sen Voting movies The anatomy recommender Proceedings 3rd Annual Conference Autonomous Agents AGENTS 1999 pp 434435 13 T Haynes S Sen N Arora R Nadella An automated meeting scheduling utilizes user preferences Proceedings 1st Annual Conference Autonomous Agents AGENTS 1997 pp 308315 14 E Hemaspaandra L Hemaspaandra Dichotomy voting systems Journal Computer System Sciences 73 1 2007 7383 15 E Hemaspaandra LA Hemaspaandra J Rothe Anyone The complexity precluding alternative Artiﬁcial Intelligence 171 56 2007 255285 16 G Kalai Learnability rationality choice Journal Economic Theory 113 1 2003 104117 17 S Lahaie DC Parkes Applying learning algorithms preference elicitation Proceedings 5th ACM Conference Electronic Commerce ACMEC 2004 pp 180188 18 J Lang MS Pini F Rossi KB Venable T Walsh Winner determination sequential majority voting Proceedings 20th International Joint Conference Artiﬁcial Intelligence IJCAI 2007 pp 13721377 19 N Littlestone Learning quickly irrelevant attributes abound A new linearthreshold algorithm Machine Learning 2 1988 285318 20 BK Natarajan Machine Learning A Theoretical Approach Morgan Kaufmann 1991 21 AD Procaccia JS Rosenschein Junta distributions averagecase complexity manipulating elections Journal Artiﬁcial Intelligence Re search 28 2007 157181 22 AD Procaccia JS Rosenschein A Zohar Multiwinner elections Complexity manipulation control winnerdetermination Proceedings 20th International Joint Conference Artiﬁcial Intelligence IJCAI 2007 pp 14761481 23 S Srivastava MA Trick Sophisticated voting rules The case tournaments Social Choice Welfare 13 1996 275289 24 RJ Vanderbei Linear Programming Foundations Extensions second ed Springer 2001 25 L Xia V Conitzer Generalized scoring rules frequency coalitional manipulability Proceedings 9th ACM Conference Electronic Commerce ACMEC 2008 pp 109118 26 HP Young Social choice scoring functions SIAM Journal Applied Mathematics 28 4 1975 27 M Zuckerman AD Procaccia JS Rosenschein Algorithms coalitional manipulation problem The 19th ACMSIAM Symposium Discrete Algorithms SODA 2008 pp 277286