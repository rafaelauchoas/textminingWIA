Artiﬁcial Intelligence 228 2015 144 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A new probabilistic constraint logic programming language based generalised distribution semantics Steffen Michels Radboud University Institute Computing Information Sciences Nijmegen The Netherlands b Embedded Systems Innovation TNO The Netherlands Arjen Hommersom Peter JF Lucas Marina Velikova b r t c l e n f o b s t r c t Article history Received 7 April 2014 Received revised form 2 May 2015 Accepted 26 June 2015 Available online 4 July 2015 Keywords Probabilistic logic programming Imprecise probabilities Continuous probability distributions Exact probabilistic inference Probabilistic logics combine expressive power logic ability reason uncertainty Several probabilistic logic languages proposed past features We focus class probabilistic logic based Satos distribution semantics extends logic programming probability distributions binary random variables guarantees unique probability distribution For applications binary random variables suﬃcient requires random variables arbitrary ranges real numbers We tackle problem developing generalised distribution semantics new probabilistic constraint logic programming language In order perform exact inference imprecise probabilities taken starting point deal sets probability distributions single It shown given continuous distribution conditional probabilities events approximated arbitrarily close true probability Furthermore setting inference algorithm generalisation weighted model counting developed making use SMT solvers We inference similar complexity properties precise probabilistic inference unlike imprecise methods inference complex We experimentally conﬁrm algorithm able exploit local structure determinism reduces computational complexity 2015 Elsevier BV All rights reserved 1 Introduction Probabilistic logics gaining popularity modelling reasoning tools combine power logic represent knowledge ability probability theory deal uncertainty In addition ﬁeld statistical relational learning SRL 1 powerful machine learning methods developed probabilistic logical languages basis The need methods emerges fact areas data available imply uncertainty provides rich structure terms relations entities Probabilistic logics SRL methods applied wide range problem domains Examples include link node prediction This publication supported Dutch national program COMMIT The research work carried Metis project responsibility Embedded Systems Innovation TNO Thales Nederland BV carrying industrial partner Corresponding author Email addresses smichelssciencerunl S Michels arjenhcsrunl A Hommersom peterlcsrunl PJF Lucas marinavelikovatnonl M Velikova httpdxdoiorg101016jartint201506008 00043702 2015 Elsevier BV All rights reserved 2 S Michels et al Artiﬁcial Intelligence 228 2015 144 metabolic networks 2 discovering interactions genes 3 dealing potentially unknown number relations multiple objects robot 3 fusing information vessels North Sea modelling relations objects heterogeneous pieces information 4 Combining logic probability theory challenging involves dealing tradeoff expressivity eﬃciency inference The research described article focuses probabilistic logics adhering Satos distribution semantics 5 This semantics guarantees single unambiguously deﬁned probability distribution supports eﬃcient ference In semantic framework logic programming LP deﬁne probability distribution set binary facts Examples languages based semantics ProbLog 6 PRISM 7 ICL 8 CPlogic 9 The choice kind semantics motivated fact allows use probabilities local meaning provides modularity needed knowledge representation similar widely Bayesian networks 10 Some alternatives particular popular Markov Logic Networks use weights interpreted context entire theory 11 For realworld knowledgerepresentation problems binary random variables convenient suﬃcient random variables taking values arbitrary ranges needed integers real numbers In fact virtually deterministic realworld models include variables Examples data models involving age height persons temporal models use integer realvalued time physical models expressing quantities real numbers Since domains uncertainty present provision probabilistic representation models essential Furthermore domains typically relational An example involving relations uncertainty realvalued variables inferring map indoor environment based observations robots 12 While ﬁniteranged discrete random variables represented sets binary random variables random variables inﬁnite number values continuous variables signiﬁcant impact semantics probabilistic logics complexity inference problem Some previous attempts extend probabilistic logic programming realvalued variables heavily restrict use variables 13 powerful model real world problems Other approaches resort approximate methods inference sampling 1412 For problems current sampling techniques effective approaches fail guarantees results quality weak This especially problematic problems wrong decisions huge impact In article propose alternative exact inference sampling inherent advantages represent distributions exact inference infeasible time provide guarantees offered existing approximation methods We provide powerful general theoretic foundation probabilistic logic programs refer generalised distribution semantics practically useful language based semantic framework The theory supports approximating probability distributions arbitrarilyranged random variables continuous discrete equipped eﬃcient method inference The main contribution work expressive logical language deﬁnes events terms constraints random variables values inequalities realvalued random variables The logical language inspired constraint logic programming CLP 15 To allow exact computation probabilities probabilities imprecise introducing credal sets generalised distribution semantics Given credal sets ensured bounds marginals computed exactly Moreover virtue distribution semantics new framework ensures consistent distribution contrast probabilistic logics imprecisions probabilistic logic Nilsson 16 Based semantics introduce new probabilistic constraint logic programming PCLP language inde pendent probability distributions random variables deﬁned means set probabilityconstraint pairs Similar deterministic constraint logic programming CLP 15 PCLP family languages allowing use arbitrary straint theories unlike formalisms restricted certain kinds random variables ﬁnite discrete realvalued ones This possible thanks general semantic foundation As examples discuss discrete constants PCLPD real numbers PCLPR To knowledge ﬁrst usable framework combines imprecise prob abilities continuous variables An earlier version language presented previous occasion 17 In present paper place language context general semantic foundation provide proofs general properties We ﬁnally present inference algorithm generalisation recently emerged probabilistic inference methods based translating inference problems weighted model counting WMC problems This shown eﬃ cient inference method propositional formalisms Bayesian networks 18 In addition shown applicable probabilistic logics based distribution semantics 19 We exact inference new language dealt generalisation method This paper shows inference framework similar complexity characteristics precise probabilistic inference In fact complexity class precise inference unlike imprecise ap proaches Furthermore complexity bounded terms problem structure similar WMC Our generalised framework exploit additional structure determinism main advantage WMC compared classical inference algorithms 18 We experimentally conﬁrm determinism exploited generalised setting To knowledge ideas WMC offers novel method approximating continuous distributions A basic version algorithm described earlier work 17 provide improved general description additional experimental results S Michels et al Artiﬁcial Intelligence 228 2015 144 3 This article organised follows Motivating examples novel PCLP language given Section 2 We provide necessary background Section 3 theoretical results generalised dis tribution semantics Section 4 way deal credal sets context semantics Section 5 The PCLP language based theory described Section 6 inference language explored Section 7 We ﬁnally discuss related work Section 8 conclude article Section 9 2 Motivating examples To illustrate expressive power new language PCLP typical examples presented 21 Fruit selling We present example concerning likelihood consumers buy certain kind fruit based 20 Since ﬁrstorder formalism generalises easily arbitrary number kinds example apples bananas The main goal PCLP deal continuous distributions Yield fruit clearly relevant price We model yield fruit normally distributed random variables denoted string starting upper case letter bold Yieldapple N 12 0000 10000 Yieldbanana N 10 0000 15000 The price inﬂuenced government support modelled discrete random variables Supportapple 03 yes 07 Supportbanana 05 yes 05 The basic price linearly depends yield expressed deterministic logic fact basic_priceapple 250 0007 Yieldapple basic_pricebanana 200 0006 Yieldbanana In case price supported raised ﬁxed priceFruit BPrice 50 basic_priceFruit BPrice cid5SupportFruit yescid6 priceFruit BPrice basic_priceFruit BPrice cid5SupportFruit nocid6 Fruit logical variable denoted bold kinds fruit apple banana possible instantiations Here use special predicate cid5cid6 represents probabilistic events example cid5SupportFruit yescid6 true case random variable SupportFruit takes value yes In constraint logic programming CLP 15 similar predicate represent constraints At maximum price customers buy certain fruit modelled gamma distribution Max_priceapple cid2100 180 Max_pricebanana cid2120 100 Thus customer willing buy case price equal maximum price expressed following ﬁrstorder rule buyFruit priceFruit P cid5P Max_priceFruitcid6 The interesting question ask given knowledge base customers buy certain fruit As uncertain events speciﬁed occurrences cid5cid6 actually occur answer likely statements P buyapple P buyapple buybanana Another possible question probability customers buy apples given know maximum yield P buyapple cid5Yieldapple 10 0000cid6 Note probabilities computed straightforwardly require computing probabilities linear equalities different kinds continuous distributions general computable Our framework allows determine approximation known errors example P buyapple 0464 0031 P buybanana 0162 0031 P buyapple buybanana 0552 0054 The actual probabilities guaranteed computed maximum error determined approximation scheme arbitrarily small explained Section 56 4 S Michels et al Artiﬁcial Intelligence 228 2015 144 22 Diabetes mellitus The medical example shows PCLP model problems involving continuous distributions imprecise probabilities The means uses bounds precise probability estimates way handle situations concerned insuﬃcient data reliably estimate probabilities Such situations frequently occur clinical research A possible approach cases express uncertainty probabilities probability distribution secondorder probability distributions In contrast approach imprecise probabilities assumes possible probabilities speciﬁed range possible furthermore expresses complete ignorance actual probability So imprecise probabilities relieve specifying secondorder distribution requires knowledge data available price making hard choice probabilities possible The example concerns diabetes mellitus type 2 complex disorder metabolic control mecha nisms disturbed A ﬁrst step treatment regulate glucose metabolism diabetic patients glucose present abundance extracellular ﬂuid exception cerebrospinal ﬂuid unable cross cellular membrane cells lack usual energy resource called starvation amidst abundance A standard test check quality glucose control measurement fasting blood glucose levels Furthermore levels glycated hemoglobin HbA1c offer insight effectiveness longterm 8 12 weeks glucose control Clearly fasting blood glucose HbA1c measurements related average While type 2 diabetes related lifestylerelated factors recent biomedical research indicates genetic factors play role onset In 21 familial risk type 2 diabetes classiﬁed average moderate high In US population 698 average 227 moderate 75 high familial risk group In PCLP represented follows Predisposition 0698 average 0227 moderate 0075 high According 21 crude prevalences diabetes risk category 54 66 average risk group 131 167 moderate risk group 266 336 high risk group DM_if_AverageRisk 0054 yes 0934 DM_if_ModerateRisk 0131 yes 0833 DM_if_HighRisk 0266 yes 0664 The uncertainty conditional probabilities encoded leaving probability mass unspeciﬁed high risk group 0266 probability mass yes state 0664 state remainder unspeciﬁed An atom dm deﬁned indicate patient suffers diabetes deﬁning logical clauses case Predisposition dm cid5DM_if_AverageRisk yescid6 dm cid5DM_if_ModerateRisk yescid6 cid5Predisposition moderatecid6 dm cid5DM_if_HighRisk yescid6 cid5Predisposition averagecid6 cid5Predisposition highcid6 We compute example probability range probability diabetes yields 0087379 P dm 0109177 To illustrate combination continuous variables suppose level glucose represented normal distri butions N μ σ 2 μ σ denote mean standard deviation cases patient diabetes Gluc_if_DM N 75 38 Gluc_if_not_DM N 579 098 Another continuous variable represent level HbA1c assume linearly depend level glucose plus noise depends patient diabetic The noise variables modelled random variables HbA1c deﬁned Noise_if_DM N 00 33 Noise_if_not_DM N 00 03 hba1c14 092 Gluc_if_DM Noise_if_DM dm hba1c06 09 Gluc_if_not_DM Noise_if_not_DM notdm S Michels et al Artiﬁcial Intelligence 228 2015 144 5 Using representation possible instance compute bounds probability diabetes given level HbA1c larger 72 The evidence encoded following clause e hba1cH cid5H 72cid6 The following probability bounds computed 0416 P dm e 0554 Note imprecision results fact use imprecise probabilities fact continuous distributions approximated Again better approximation investing computation time 23 Running example ﬁre ship As ﬁnal example introduce following short case description illustrate concepts article Suppose ﬁre compartment ship The heat causes hull compartment warp ﬁre extinguished 125 minutes hull breach After 075 minutes ﬁre spread compartment This means ﬁre extinguished 075 minutes ship saved sure saved cid5Time_Comp1 075cid6 In compartment hull breach 0625 minutes ﬁre breaks In order reach second com partment ﬁre ﬁrst extinguished So ﬁres extinguished 075 0625 1375 minutes Additionally ﬁre ﬁrst compartment extinguished 125 minutes hull breaches The second compartment accessible ﬁreﬁghters extinguish ﬁre time means work times faster saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 Finally assume exponential distributions time durations available extinguish ﬁres Time_Comp1 Time_Comp2 Exp1 Exp1 The interesting question likely ship saved P saved required 3 Background In section discuss theoretical background build theory 31 Logic programming Logic programming LP based idea predicate logic programming language 22 Prolog far wellknown example language LP languages available Since LP based ﬁrstorder logic language suitable basis knowledge representation reasoning A logic program L consists set rules called clauses Rules implicitly universally quantiﬁed expressions form h l1 ln h called head collection literals l1 ln form body rule represent conjunction The head h atom expression form pt1 tm p predicate t1 tn terms If terms ti constants atom pt1 tm called ground The literals body atoms negated atoms nota Facts called unit clauses clauses body assumed true In following use convention constants denoted lowercase letters apple banana variables start uppercase letters Fruit BPrice Although syntax LP subset ﬁrstorder logic FOL semantics LP FOL differ The difference emerges semantics negations LP similarly closed world assumption CWA states certain conclusion false derived knowledge base In contrast FOL statement proved false negation statement implied knowledge base means statements decided true false This difference allows LP express nonground inductive deﬁnitions transitive closures possible FOL 23 For remainder paper combine LP probability theory require knowledge bases unique 2valued models Programs L negation characterised smallest model ML called Herbrand model consisting ground atoms entailed logic program L ML iff L cid12 Similarly logic programs cid2 6 S Michels et al Artiﬁcial Intelligence 228 2015 144 negations stratiﬁed meaning program disallows certain combinations recursion negations unique Herbrand model 24 Nonstratiﬁed programs unique model form nonpartial wellfounded models 25 stable models 26 We abstract class programs declarative semantics LP generalised distribution semantics required program unique model Thus following use M denote models given chosen semantics necessarily Herbrand model 32 Probability theory Probability theory offers widely wellfounded basis representing reasoning uncertainty The likelihood events expressed terms probabilities 0 1 0 means event certainly1 occur 1 opposite Probability theory provides deﬁned ways combine probabilities makes use partial knowledge state world conditioning evidence Often random variables represent probability distributions article denote bold uppercase letters X Y We assume random variable Vi range Rangei values Sometimes sets random variables indicated index set VI Vi I I index set In case ﬁnite number random variables ﬁnite ranges uniquely deﬁne joint probability distribution P assigning probability number joint assignment values v Rangei 1 n random variables P V1 v 1 Vn vn From probability distributions compute probability partial assignments marginalisation P VK v K P VK vk V J v J 1 v J J IK I 1 n Example 1 Assume random variable Predisposition example Section 22 view atom dm binary random variable DM The deﬁnition exactly deﬁne probability distribution contains imprecise knowledge deﬁne probability distribution consistent deﬁnition random variables following assignments P Predisposition average DM false 0652 P Predisposition average DM true 0046 P Predisposition moderate DM false 0189 P Predisposition moderate DM true 0038 P Predisposition high DM false 005 P Predisposition high DM true 0025 From compute probability instance P DM true P DM true P Predisposition average DM true P Predisposition moderate DM true P Predisposition high DM true 0109 This works discrete case continuous case random variables uncountable ranges requires sophisticated techniques In case range random variables real numbers probability measures intervals deﬁned terms probability density functions PDFs If f PDF random variable Vi probability Vi takes value interval b deﬁned cid3 cid4 Vi b P bcid5 f x dx 2 Generally random variables offer intuitive way look probability distributions suited construct complex probability distributions This especially true purpose want deal inﬁnitely 1 Note difference certainly certainly continuous distributions For instance probability temperature exactly 20 C 00 theoretically possible Therefore event certainly occur certainly occur S Michels et al Artiﬁcial Intelligence 228 2015 144 7 random variables different ranges countable uncountable For reason use measuretheoretic approach probability theory speciﬁcally Kolmogorovs probability theory 27 Probability spaces form basis modelling uncertain processes theory A probability space cid4 A P formally consists 1 A sample space cid4 An arbitrary set possible states model 2 An event space A A subset sample spaces powerset A cid4 σ algebra meaning contains set A closed ii complement e A cid12 cid4 e A iii countable union e1 e2 A cid12 e1 e2 A A consequence properties ii entire sample space cid4 event space Elements A called events probabilities assigned events 3 A probability measure P A function assigning number closed interval 0 1 event P countably additive means probability union countably pairwise disjoint events equal sum probabilities events P e1 P e2 P e1 e2 Additionally P assign 1 entire sample space P cid4 1 In setting random variables provide convenient view probability distribution actually functions mapping sample space variables range Vi cid4 Rangei This way probability measure P original probability space assigns probability event concerning random variable Vi In article use simple mapping sample spaces random variables We represent random variables values tuples element represents single random variables state The random variables functions mapping tuple speciﬁc element Example 2 The probability distribution Example 1 represented following probability space The sample space consists possible assignments random variables cid4 false average true average false moderate true moderate false high true high cid3 ω2 The event space pow ω1 ω2 The random variables map corresponding element tuple DM erset cid4 σ algebra countable sample spaces The probability assignments Example 1 translate following probability assignments events cid3 false average cid3 false moderate cid3 false high cid3 true average cid3 true moderate cid3 ω1 Predisposition cid3 true high ω1 ω2 0652 0038 0189 0046 0025 005 P P P P P P cid4 cid4 cid4 cid4 cid4 cid4 cid4 cid4 This uniquely deﬁnes probability measure P establishes probabilities events event space given properties probability measure The probability diabetes computed P cid4 cid3 true average true moderate true high 0109 Imprecise probability theory generalisation probability theory It avoids crisp probabilities allows express ignorance concerning probability distributions The theory possible obtain precise probabilities insuﬃcient availability data estimate probabilities probabilities estimated number experts providing range probabilities There different approaches imprecise probabilities varying expressiveness 28 In article consider convex sets probability distributions referred credal sets 29 This setting makes possible express probabilities events lower upper bound Formally denote credal set P assume associated sample event space consists collection probability measures consistent Kolmogorovs axioms For event e exists distribution P probability attains minimum maximum We denote probabilities P e P e respectively 33 Probabilistic logic programming Combining logic probability theory subject gained increased past years approaches associated software implementation explored We focus work Satos distribution semantics extends LP probabilities 5 The purpose distribution semantics extend semantics LP probabilistic semantics guaran teeing existence unique probability distribution consistent Kolmogorovs probability axioms The key property fact logic programs unique model Sato uses traditional model semantics concept generalised programs kind unique model deﬁned Suppose split program L rules R facts F L F R Facts rules body assume facts occur head rule Facts considered binary random variables taking values 8 S Michels et al Artiﬁcial Intelligence 228 2015 144 Table 1 Distribution semantics example ωF high_pred dm_hp dm_no_hp false false false false false true false true false false true true true false false true false true true true false true true true MLωF dm_no_hp dm dm_hp dm_hp dm_no_hp dm high_pred high_pred dm_no_hp high_pred dm_hp dm high_pred dm_hp dm_no_hp dm P LωF 071 003 006 002 002 005 01 001 true false A program called grounded variable replaced possible ground terms A grounded program potentially includes inﬁnite countable number ground facts For instance probabilistic process include random variables representing weather fact inﬁnite number future days Looking ground fact random variable sample space cid4F element example inﬁnite sequence Boolean values indicating weather day The truth value literal L determined truth assignment logic programs unique models discussed Section 31 We speak model associated element ωF cid4F denote MLωF Given probability measure P F deﬁned event space powerset cid4F possible extend distribution atoms occurring program unique way We refer resulting distribution P L The sample space cid4L deﬁned similarly event space facts includes atoms The event space sample spaces powerset To construct measure inﬁnite event space construct ﬁnite measures restricted ﬁrst n atoms extended measure inﬁnite event space extension theorem probability measures 30 III3 A ﬁnite measure ﬁrst n atoms deﬁned cid3 LωL def P F P n ωF cid4F MLωF cid12 ωL cid4 3 Here use element ωL sample space logical formula representing conjunction determines atom true false cid3 The probability measure deﬁned atoms makes possible compute probability arbitrary query sen Probabilities queries ﬁnite grounding computed exactly ﬁnite ωL cid4L ωL cid12 q cid4 tence q P L measures Example 3 We use simpliﬁed slightly changed version example Section 22 original distribution semantics deal precise probabilities discrete distributions For convenience consider states Predisposition distinguish high high Consider following rules indicating patient diabetes mellitus dm depending predisposition dm dm_high_pred high_predisposition dm dm_no_high_pred nothigh_predisposition Assume given probability measure P F Table 1 shows element event space F models probabilities associated element We compute instance probability P dm summing cases dm included model P dm 003 002 01 001 016 In way compute probability logical sentences Examples P dm high_predisposition 01 001 011 P dm high_predisposition 003 002 002 005 01 001 023 4 A generalised distribution semantics In section introduce generalised distribution semantics extending Satos original distribution semantics random variables arbitrary possibly inﬁnite ranges While original distribution semantics deﬁned binary probabilistic facts easily generalised random variables ﬁnite ranges implementation PRISM language supports random variables 31 However soon deal inﬁnite ranges generalisation semantically far straightforward particular ranges uncountable For example grounding real variable lead uncountable number ground atoms original distribution semantics provide welldeﬁned probability distribution We tackle problem augmenting logical formalism special constraints approach adopted terministic constraint logic programming CLP 15 satisﬁability modulo theories SMT solvers This leads expressive language queries exist closed form expression compute marginal probabilities exact S Michels et al Artiﬁcial Intelligence 228 2015 144 9 inference possible In second section discuss suﬃcient conditions exact inference possible extended language Note results section fundamentally different known solutions offered example Hybrid ProbLog 13 However ﬁrst time formalise approach general way As clear general theory act basis advanced work discussed remainder paper 41 Probability distributions constraint logic programs Whereas Satos distribution semantics assigns joint probability distribution ground atoms logic program probabilistic facts generalised distribution semantics use constraints deﬁne joint distri bution To represent constraints deﬁne general probabilistic constraint logic Subsequently generalised distribution semantics constraint logical language deﬁned 411 Constraint logic theories probability measures The basic idea language countably random variables V V1 V2 ranges sets elements arbitrary properties example discrete constants real numbers Hence number random variables inﬁnite similar original distributions semantics Note probabilities events involving inﬁnite number variables computable separate semantics issue performing inference order unnecessarily restrict generality semantics Constraints ϕ looked predicates state random variables ϕ function V1 v 1 V2 v 2 true false v element range Vi Equivalently constraint seen predicate sample space corresponds random variables states The subset sample space constraint holds called solution space constraint Deﬁnition 1 Constraint solution space The solution space constraint ϕ given sample space cid4 deﬁned CSSϕ def ω cid4 Vω v ϕv Vω v shorthand Viω vi In logical language use rules similar ones logic programming body use literals constraints form cid5ϕVcid6 The brackets indicate regular predicate constraint makes syntactically similar normal CLP 152 Formally deﬁne constraint logic theories follows Deﬁnition 2 Probabilistic constraint logic theory A probabilistic constraint logic theory T tuple V cid4V AV P V Constr L V V1 V2 countable set representing random variables associated nonempty ranges Range1 Range2 ﬁx enumeration random variable index cid4V sample space random variables V deﬁned Cartesian product random variables ranges cid4V def Range1 Range2 AV event space representing events concerning random variables V P V probability measure space deﬁned tuple cid4V AV P V probability space Constr set constraints closed conjunction disjunction negation CSSϕ ϕ Constr AV constraints correspond events L set logic programming rules constraints h l1 ln cid5ϕ1Vcid6 cid5 ϕmVcid6 ϕi Constr 1 m 2 We use cid5cid6 CLP avoid confusion set notation mathematical deﬁnitions 10 S Michels et al Artiﬁcial Intelligence 228 2015 144 In remainder section abstract actual constraint language In examples given section language deﬁne constraints meant illustration For example Vi continuous variables write cid5i Vi1 Vicid6 express Vi increases If interpret constraints simple ground atoms rules interpreted normal logic program associated logic programming semantics discussed Section 31 Note sample space deﬁned Cartesian product ranges random variables sim ple projections single tuple elements sample space So solution space arbitrary constraint ϕ equals ω cid4 ϕω Example 4 Consider running example Section 23 Time_Comp1 Exp1 Time_Comp2 Exp1 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 Suppose Time_Comp1 Time_Comp2 random variables enumeration The range variables real numbers R The constraint language includes linear inequalities probability measure ﬁrst variables Range1 independent distributed according exponential distribution Range2 412 Extending probability spaces entire theory While theory T deﬁnes probability distribution random variables probabilities events logical language speciﬁed directly As original distribution semantics probability distribution random variables uniquely extended distribution entire program Next shown extend sample space event space probability measure entire theory T We generalise distribution semantics looking probabilistic facts occurring rules special kind constraint probabilistic fact pf rule represents constraint pf true In order deﬁne semantics arbitrary constraints extend sample space considering atoms appearing logical theory L We assume countable number atoms treat random variables taking values true false denote set atoms A As variables V ﬁx enumeration deﬁne sample space cid4L Cartesian product values atoms Acid6 true false def cid4L i1 4 For event space logic theory AL sample spaces powerset sample space countable AL def cid4L 5 The sample space entire theory cid4T Cartesian product sample spaces random variables logical theory cid4T def cid4V cid4L 6 The event space entire theory AT built event spaces random variables logical theory Concretely tensorproduct σ algebra AT def AV AL 7 cid3 The tensorproduct σ algebra AV AL smallest σ algebra generated products elements AV AL We simply use product spaces product σcid4Vcid4L σ algebra eV eV eV AL eL AL cid4 Example 5 Consider following event spaces cid8 cid7 AV b c b c cid7 AL true false true false cid8 S Michels et al Artiﬁcial Intelligence 228 2015 144 11 The product eV eL eV AV eL AL cid7 true false true false b true c true b false c false b true b false c true c false true b true c true false b false c false cid8 true false b true b false c true c false This product σ algebra instance false b true c true included In tensor product minimal number elements added product σ algebra Finally probability measure P V extended probability measure entire theory yielding P T The way achieved distribution semantics builds observation determining truth values probabilistic facts uniquely determines truth values atoms entire theory In generalised setting similarly observe determining constraints true uniquely determines truth values atoms entire theory To formalise notion use set satisﬁableωV includes constraints satisﬁable given valuation ωV random variables We interpret set partial logic program assuming constraint occurs instantiation predicate cid5cid6 Example 6 Suppose ωV 0 ﬁrst random variable V1 takes value 0 Then satisﬁableωV include cid5V1 0cid6 include instance cid5V1 1cid6 An element sample space ωV yields logic theory L satisﬁableωV assume unique model discussed Section 31 This model denoted MLωV Example 7 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 The deﬁnition atom saved implies true constraint Time_Comp1 075 constraints Time_Comp1 125 Time_Comp1 025 Time_Comp2 1375 satisﬁed No model ML cid4 cid3 1 2 satisﬁable cid4 includes cid5Time_Comp1 075cid6 saved 05 1 includes cid5Time_Comp1 075cid6 cid5Time_Comp1 ML cid3 cid5Time_Comp1 125cid6 included models given clauses saved included ML 025 Time_Comp2 1375cid6 constraints In contrast 1 2 cid3 cid4 cid3 cid4 1 2 Given notions probability measure P T deﬁned extended event space The idea uniquely derive measure P V mapping elements entire event space elements random variables event space truth values logical atoms correspond unique models MLωV given valuations random variables ωV Deﬁnition 3 Entire theory probability measure The probability measure entire theory Ts event space deﬁned cid3 P Te def P V ωV ωV ωL e MLωV cid12 ωL cid4 Here use elements ωL logical theorys sample space logical formulas represent conjunctions determining atom true For example ωL 0 1 0 means b c b c atoms L It ensured events deﬁnition AV restriction MLωV cid12 ωL based compositions events AV We extend probability query atom q given probability measure P T follows Deﬁnition 4 Query probability The probability query q deﬁned cid3 P q def P T ωV ωL cid4T ωL cid12 q cid4 Note need restriction values random variables ωV Deﬁnition 4 Deﬁnition 3 ensures valuations random variables q hold contribute probability We know event deﬁned equation element event space AT restrictions 12 S Michels et al Artiﬁcial Intelligence 228 2015 144 values random variables event space concerning logic atoms deﬁned powerset sample space Equation 5 subset sample space event space Example 8 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 Suppose saved corresponds ﬁrst dimension cid4L ωL cid12 saved requires ﬁrst element sample true Then applying Deﬁnition 4 obtain cid3 ωV saved cid4T saved true P saved P T cid4 Then applying Deﬁnition 3 P saved P V cid3 ω1 ω1 saved cid4T MLω1 cid12 saved saved true cid4 Given clauses assuming Time_Comp1 Time_Comp2 correspond ﬁrst random variables MLω1 ω2 entails saved ω1 075 ω1 125 ω1 025 ω2 1375 P saved P V cid3 ω1 ω1 saved cid4T ω1 075 ω1 125 ω1 025 ω2 1375 cid4 cid3 ω1 cid4V ω1 075 ω1 125 ω1 025 ω2 1375 cid4 P V As Time_Comp1 Time_Comp2 cumulative distribution function F Exp1 probability P saved computed follows We denote associated density function f P Vω1 cid4V ω1 075 F 075 1 e 075 053 P Vω1 cid4V ω1 075 ω1 025 ω2 1375 075cid5 f xP Time_Comp2 4 1375 4xdx 0 075cid5 075cid5 f xF 55 4xdx x e3x55dx 052 e 0 0 P Vω1 cid4V ω1 125 ω1 025 ω2 1375 x e3x55dx 066 e 125cid5 0 To summarise P Vω1 cid4V ω1 075 ω1 125 ω1 025 ω2 1375 053 066 052 067 By combining Deﬁnitions 3 4 determine single event random variables probability space probability query q Such event referred following solution event In way separation achieved symbolic determining cases statement true logical reasoning probabilistic determining probability cases occurs Deﬁnition 5 Solution event The solution event query q deﬁned SEq def ωV cid4V MLωV cid12 q Lemma 1 The probability query q deﬁned Deﬁnition 4 computed solution event P q P V cid3 cid4 SEq All proofs provided Appendix A Deﬁnition 5 formulates solution event terms samples A different formulation terms constraints suited subsequent formulation conditions exact inference feasible Such mulation provided following lemma S Michels et al Artiﬁcial Intelligence 228 2015 144 13 Lemma 2 The solution event query q expressed SEq CSS cid9 cid10 MLcid8cid12q cid8Constr cid12 cid11 ϕcid8 ϕ cid8 denotes subsets constraints MLcid8 model theory L cid7 cid5ϕcid6 ϕ cid8 cid8 Intuitively disjunction represents collection possible models q true logic sense dealing meaning construct cid5cid6 All constraints cid8 true time order q true Thus conjunctions sets constraints combined Example 9 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 Given clauses ways prove saved Each model including saved include constraints enforced ﬁrst second clause corresponds disjunction Lemma 2 The second clause requires constraints hold combined conjunction Lemma 2 The solution event saved ω1 ω2 cid4V ω1 075 ω1 125 ω1 025 ω2 1375 Note event compute query probability Example 8 42 Exact inference conditions The semantics introduced general powerful computation event probabilities general infeasible Practically useful languages demand ﬁnding proper balance expressivity feasibility inference We discuss ways restrict general semantics way exact inference feasible The result provides basis analysing structured way properties allow exact inference different languages Proposition 1 The probability arbitrary query computed exactly following conditions hold 1 ﬁniterelevantconstraints condition There ﬁnitely constraint predicates cid5cid6 relevant determining truth fulness query atom Formally constraint predicate cid5ϕcid6 relevant query atom q exists set constraint predicates cid8 q Mcid8 L cid2 q Mcid5ϕcid6 cid8 L Intuitively exists set constraint predicates matters cid5ϕcid6 included program meaning cid5ϕcid6 relevant We assume ﬁnding relevant constraints predicates entailment checking ﬁnite time 2 ﬁnitedimensionalconstraints condition Constraints occurring clauses argument construct cid5cid6 concern ﬁnitelydimensional subset sample space This means constraints solution spaces form ω1 ωn ωn1 cid4V condω1 ωn cond arbitrary predicate n arguments constraint puts condi tion ﬁnite number variables 3 computablemeasure condition The probability ﬁnitedimensional events sense previous condition com putable The computablemeasure condition implies exactly compute ﬁnitedimensional integrals employed PDFs possible strong assumptions The conditions stated suﬃcient strictly necessary restrict kind continuous distribu tions employed instance Gaussian distributions work discussed Section 8 These conditions generalise restrictions typically enforced approaches based distribution semantics discussed The following example illustrates exact inference conditions Example 10 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 As shown Example 9 solution event derived ﬁnite time ﬁniterelevantconstraints condition ﬁnitedimensional constraints program ﬁnitedimensional ﬁnitedimensionalconstraints condition Assume Example 8 exactly compute probability ω1 ω2 cid4V ω1 075 ω1 125 ω1 025 ω2 1375 067 computablemeasure condition P saved 067 An example problem fulﬁl exact inference condition 14 S Michels et al Artiﬁcial Intelligence 228 2015 144 forever_sun X cid5Weather X sunnycid6 forever_sun X 1 The predicate forever_sun X intuitively represents sunny forever day X assuming inﬁnitely days future The probability forever_sun0 computed ﬁnite time inﬁnite number days considered means ﬁniterelevant constraints condition violated One usually limit probability forever_sun0 0 true possible probability measures Only assuming computablemeasure condition probability measure draw conclusion An alternative deﬁnition problem forever_sun cid5 iN Weatheri sunnycid6 Similarly probability forever_sun computable assumptions probability space generally possible ﬁnitedimensionalconstraints condition violated We brieﬂy discuss conditions context existing approaches The ﬁniterelevantconstraints condition reasonable condition allowing exact inference avoided In Satos original semantics condition called ﬁnitesupport condition required probabilistic facts similar ﬁniterelevantconstraints condition restricted positive programs Similarly ﬁnitedimensionalconstraints condition enforced Satos seman tics interpret probabilistic facts program constraints concerning single variable probabilistic fact required true false dependencies expressed structure rules So actually stronger variant ﬁnitedimensionalconstraints condition enforced constraints concern single variables Finally computable measure condition depends probability distribution deﬁned concrete language Most languages based distribution semantics satisfy property assuming random variables mutually independent This means probability measure deﬁned terms single probability variable consequently probability events consisting ﬁnite number variables computed ﬁnite time Again loss generality dependencies introduced structure logic program The situation complex continuous distributions considered The relevant language exact inference possible Hybrid ProbLog 13 extends Satos semantics continuous variables As Satos semantics Hybrid ProbLog allows constraints single variables This means instance cid5V1 0cid6 allowed cid5V1 V2cid6 This restriction ensures computablemeasure condition fulﬁlled assumption compute CDFs employed continuous distributions While binary case restriction constraints single variables restrict expressiveness language continuous case The main theory overcome restrictions discussed 5 Relaxing exact inference conditions credal sets The aim section provide new theory supports exact inference covering problems involving straints multiple variables As consequence general ﬁnitedimensionalconstraints condition required introduced previous section For example wish allow comparison realvalued random variables time avoiding severely restricting kind distributions fulﬁl computablemeasure condition This problem tackled introducing credal sets set probability distributions We idea makes possible compute bounds probabilities query conditions strict assumed We ﬁnally discuss important application theory approximation precise continuous distributions 51 Assigning probability mass events We ﬁrst overview basic idea The goal deﬁne joint probability distributions way probabilities computed ﬁnite sums queries ﬁnite proofs We achieve assigning probability masses number arbitrary events exhaustively atomic events As consequence deal credal sets single probability distribution However time guarantee set nonempty Furthermore ability express imprecise probabilities credal sets useful feature probabilistic knowledge available imprecise We deﬁne meaning assignment probability mass set values random variable prob ability mass distributed values As multiple possible ways probability mass distributed implies deﬁne unique probability distribution credal set illustrated following example Example 11 Assume single random variable real numbers range assign probability mass set values 1 3 Fig 1 depicts possible ways probability mass distributed distributed uniformly entire set Fig 1a parts Figs 1b 1c distributed complex manner Fig 1d These examples actually uncountably ways distribute probability mass set S Michels et al Artiﬁcial Intelligence 228 2015 144 15 Fig 1 Examples possible distributions probability mass real values 1 3 Deﬁning set distributions useful query lower bound upper bound success probability given set distributions computed We provide intuition bounds computed following example tackling problem formally Example 12 Consider following program q cid5V1 0cid6 Assume deﬁne probability distribution V1 assigning probability mass 01 set values smaller 1 Set 1 03 closed interval 1 1 Set 2 06 set values larger 1 Set 3 It clear matter probability mass distributed values Set 1 probability q derived 00 For Set 2 probability 00 case probability mass distributed values 00 03 opposite case For Set 3 query derived matter probability mass distributed We conclude probability q 06 09 52 Credal set speciﬁcations As discussed basic idea assign probability masses sets values Sets values correspond events consequently credal sets deﬁned terms probabilityevent pairs We refer kind deﬁnitions credal set speciﬁcations Such speciﬁcations desirable property guaranteed deﬁne nonempty credal sets sets probability measures Credal set speciﬁcations introduced section purely semantic level credal sets concrete syntactic level discussed later Since number random variables semantics inﬁnite allow potentially inﬁnite sets probabilityevent pairs Such speciﬁcations hard deﬁne directly clear construct probabil ity distributions consistent speciﬁcations Therefore deﬁne credal set speciﬁcations means sequence countably potentially inﬁnite number speciﬁcations deﬁning ﬁnitedimensional credal sets increasing dimen sionality We sure speciﬁcations contradict ensure property compatibility This allows use existing construction theorem construct inﬁnitedimensional probability distribu tions consistent credal set speciﬁcation given Before formally deﬁne concept credal set speciﬁcations introduce concept event projections We denote ith event projection event e natural number ei deﬁne πie def cid7 cid8 ω1 ωi ω1 ωi cid4 Event projections similarly deﬁned ﬁnite events 8 Deﬁnition 6 Credal set speciﬁcation A credal set speciﬁcation C consists countable number ﬁnitedimensional spec iﬁcations C1 C2 Each Ck form ﬁnite collection probabilityevent pairs p1 e1 p2 e2 pn en Ck 1 The events ﬁnitedimensional event space Ak 2 The sum probabilities 10 p 10 cid13 3 The events set peCk e cid22 peCk V sample space cid4k V def Range1 Range2 Rangek Additionally ﬁnite Ck compatible k Ck πkCk1 πlCk deﬁned πlCk def cid7 cid2 p e cid23 e cid23 πle p e Ck cid8 cid23 πlee peCk 16 S Michels et al Artiﬁcial Intelligence 228 2015 144 One look credal set speciﬁcations way split probability mass portions signed speciﬁc nonempty events given ﬁnite set random variables As said compatibility inductively construct consistent distributions random variables ensuring speciﬁcations different dimensionality contradict Example 13 Assume ﬁrst random variables sample space range set sun rain C1 probability sun tomorrow 80 cid14cid3 cid4 02 sun rain cid3 08 sun cid4cid15 Suppose means Consider following speciﬁcation concerning weather tomorrow weather day tomorrow cid14cid3 C2 02 sun sun sun rain cid3 cid4 08 sun sun cid4cid15 Both speciﬁcations compatible C2 ﬁxes probability sun tomorrow 10 conﬂicts C1 technically π1C2 cid14cid3 cid4cid15 10 sun cid23 C 2 cid22 C1 In contrast example compatible speciﬁcation cid4cid15 cid14cid3 02 sun sun sun rain rain sun cid4 cid3 08 sun sun Each Ck deﬁnes set probability measures given following deﬁnition V deﬁned Ck includes probability measures V consistent Kolmogorovs probability axioms additionally following condition holds A probability Deﬁnition 7 Finite credal sets The set probability measures Pk Ak measure P V Pk cid2 V event e Ak V cid2 p P Ve p decid22 pdCk pdCk The probabilities contributing lower bound related events subsets e certainly contribute probability e In contrast probabilities contributing upper bound relate events disjoint e possibly contribute probability e Example 14 For illustration twodimensional speciﬁcation variables range sun rain cid14cid3 C2 02 sun sun sun rain cid3 cid4 08 sun sun sun rain rain sun cid4cid15 Some distributions element resulting credal set sun sun sun rain rain sun rain rain P 1 V P 2 V P 3 V P 4 V P 5 V 10 00 05 02 01 00 10 05 03 01 00 00 00 05 08 00 00 00 00 00 With strict speciﬁcation restrict possible measures cid14cid3 C2 02 sun sun sun rain cid3 cid3 cid3 cid4 08 sun sun cid4cid15 In case P V cid4 rain sun P V cid4 rain rain 00 00 P V cid3 cid4 sun rain 02 P V cid3 cid4 sun sun 1 P V cid3 cid4 sun rain Next credal set speciﬁcation C deﬁnes credal set entire potentially inﬁnitedimensional random variables sample space set convex nonempty To prove fundamental property Ck C deﬁne nonempty set probability spaces entire event space AV satisfy Kolmogorovs probability axioms Lemma 3 For credal set speciﬁcation C C1 C2 exists nonempty credal set probability measures PV entire space cid4V AV measures P V PV agree C1 C2 sense events e natural numbers k S Michels et al Artiﬁcial Intelligence 228 2015 144 17 cid2 p P V cid3 cid4 πke cid2 p pdCk decid22 pdCk We require additional condition event space chosen possible construct inﬁnite dimensional probability measure inﬁnite number ﬁnite ones increasing dimensionality The technical condition enables construction probability measures poses practical restriction For stance condition fulﬁlled case event space random variables built Borel σ algebras Polish topological spaces 30 III3 This includes virtually possible event spaces relevant practical applications sub sets spaces discrete topology integers sets countably constants closed open intervals rational bounds real numbers Moreover credal set probability measures random variables extended credal set entire theory extending element credal set shown Section 412 Theorem 1 Each credal set speciﬁcation C deﬁnes nonempty credal set probability measures entire theory PT set corresponding query probability distributions P 53 Probability bounds We shown theory deﬁnes potentially inﬁnite set probability spaces means query compute potentially inﬁnite number probabilities Instead typically interested lower upper bounds probability query Deﬁnition 8 Probability bounds We deﬁne lower upper probability bounds query q follows P q min P P P q max P P P q P q We introduce formulas computing bounds ﬁnitedimensional queries Proposition 2 The lower upper probability bounds query q fulﬁlling ﬁnitedimensionalconstraints condition putting constraints ﬁrst k dimensions computed P q P q cid2 eSEq peCk cid2 p p eSEqcid22 peCk Example 15 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 We ﬁnite credal set speciﬁcation variables occurring clauses This way roughly matches exponential distributions example Time_Comp1 Time_Comp2 Exp1 How generate ﬁnite credal sets exactly matching continuous distributions general discussed Section 56 cid4 cid4 cid14cid3 C2 049 ω1 ω2 0 ω1 1 0 ω2 1 cid3 cid3 cid3 cid3 cid3 007 ω1 ω2 2 ω1 3 0 ω2 1 014 ω1 ω2 1 ω1 2 0 ω2 1 cid4 cid4 cid4 cid4 014 ω1 ω2 0 ω1 1 1 ω2 2 004 ω1 ω2 1 ω1 2 1 ω2 2 002 ω1 ω2 2 ω1 3 1 ω2 2 18 S Michels et al Artiﬁcial Intelligence 228 2015 144 Fig 2 Twodimensional sample space events solution event cid3 cid3 cid3 007 ω1 ω2 0 ω1 1 2 ω2 3 002 ω1 ω2 1 ω1 2 2 ω2 3 001 ω1 ω2 2 ω1 3 2 ω2 3 cid4 cid4 cid4cid15 The solution event saved ω1 ω2 cid4V ω1 075 ω1 125 ω1 025 ω2 1375 shown Example 9 The sample space events deﬁned credal set speciﬁcation visualised Fig 2 The solution event shown line inside solution event The ﬁrst event C2 shown left upper corner Fig 2 subset solution event This intuitively means matter distribute probability mass 049 inside event inside solution event Therefore P saved 049 All events represented grey areas Fig 2 disjoint solution event This means possible distribute probability mass way inside solution event We conclude P saved 049 014 007 014 004 088 We ﬁnally provide formulas probability bounds general case constraints ﬁnite dimensional Theorem 2 The lower upper probability bounds query q determined P q lim k P q lim k cid2 p eSEq peCk cid2 eSEqcid22 peCk p We ﬁnally present result relation lower upper bound follows theorem sum rule limits Corollary 1 The lower bound expressed terms upper bound vice versa P q 1 P q P q 1 P q 54 Dealing evidence Making use evidence crucial probabilistic reasoning Taking evidence account means exclude parts event space renormalise probability measure probabilities sum For single probability distribution probability query q given evidence e denoted P q e expressed terms nonconditional probabilities P q eP e In case credal set corresponding conditional probabilities ﬁnd minimum maximum Deﬁnition 9 Conditional probability bounds We deﬁne probability bounds given evidence P q e def min P P P q e def max P P P q e P q e S Michels et al Artiﬁcial Intelligence 228 2015 144 19 Fig 3 Twodimensional sample space events solution event solid evidence dashed Note alternative deﬁnitions conditional probabilities imprecise probability distributions Weichsel berger argues different notions conditional probabilities depending purpose 32 We restrict deﬁnition corresponds Weichselberger calls intuitive concept In contrast precise case deﬁne normalisation factor called partition function depends evidence query account This illustrated following example Example 16 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 Consider additional rule e cid5Time_Comp2 15cid6 Suppose like compute P saved e This illustrated Fig 3 events represented black areas excluded event space All events right column certainly excluded disjoint evidence It depends probability measure chosen credal set exclude events middle column The choice depends want compute lower upper bound illustrated ﬁgure The probability mass upper middle event included partition function e completely saved So excluding minimises probability The remaining events middle column contribute partition function lower bound Excluding increase result The events consequently excluded computing lower bound P saved e 049049 014 004 007 002 064 For upper bound events contributing numerator denominator probability included events contributing denominator excluded order obtain maximal probability The upper bound P saved e 049 014 014 004049 014 014 004 007 092 To compute conditional probabilities following proposition relates joint probability condi tional probability Proposition 3 The probability bounds query q given evidence e deﬁned Deﬁnition 9 computed follows P q e P q e P q e P q e P q e P q e P q e P q e 55 Exact inference conditions We introduce variant exact inference conditions Proposition 1 concerning probability bounds instead precise probabilities Theorem 3 The probability bounds arbitrary query computed ﬁnite time following conditions hold 1 ﬁniterelevantconstraints condition There ﬁnitely constraint predicates cid5cid6 relevant determining truthful ness query atom ﬁnding relevant constraints predicates entailment checking ﬁnite time The condition Proposition 1 2 ﬁnitedimensionalconstraints condition Events occurring clauses argument cid5cid6 concern ﬁnitelydimensional subset sample space The condition Proposition 1 20 S Michels et al Artiﬁcial Intelligence 228 2015 144 3 disjointeventsdecidability condition For ﬁnitedimensional events e1 e2 event space AV decide disjoint e1 e2 Note disjointeventsdecidability condition means decide event subset e1 e2 equivalent e1 cid4V e2 In condition replaced view strict computablemeasure condition disjoint eventsdecidability condition It fulﬁlled wide class possible ways deﬁne events linear inequalities integers excluding multiplication 33 inequalities real numbers including multiplication 34 Although able deﬁne arbitrary distributions deﬁne set distributions includes distribu tion For instance queries programs consisting linear constraints real numbers distributed according arbitrary continuous distributions approximated known maximal error shown Additionally possible deﬁne imprecise distributions case knowledge available actual distribution Example 17 cid7 025 ω1 0 ω2 0 Assume random variables V1 V2 range real numbers corresponding credal set speciﬁcation 025 ω1 0 ω2 0 025 ω1 0 ω2 0 025 ω1 0 ω2 0 Here use shorthand notation events denoting condition elements sample space This credal set speciﬁcation instance includes case ω1 ω2 independent normally distributed mean 00 arbitrary standard deviation Suppose want compute probability event 2ω1 ω2 We observe following statements linear constraints realvalued cid8 variables decidable disjointeventsdecidability condition 2ω1 ω2 cid2 ω1 0 ω2 0 2ω1 ω2 cid2 ω1 0 ω2 0 2ω1 ω2 ω1 0 ω2 0 2ω1 ω2 cid2 ω1 0 ω2 0 From compute P 2ω1 ω2 025 To determine upper bound observe following 2ω1 ω2 ω1 0 ω2 0 cid22 2ω1 ω2 ω1 0 ω2 0 2ω1 ω2 ω1 0 ω2 0 cid22 2ω1 ω2 ω1 0 ω2 0 cid22 From compute P 2ω1 ω2 075 56 Approximating continuous distributions credal sets The examples Section 2 contain continuous distributions translate inﬁnite credal set speciﬁcations exact inference possible Credal sets approximate combinations arbitrary continuous distributions known cumulative distribution function CDF This associating probabilities intervals deﬁning set distributions including actually intended To divide sample space single variable Vi n intervals l1 u1 ln j l j u j l j real number ui real number We deﬁne following onedimensional credal set P l1 Vi u1 l1 Vi u1 P ln Vi ln Vi To compute probabilities integrals use fact onedimensional integrals typical density functions computed negligible error example normal distributions integrals computed CDFs error function The credal sets independent random variables combined taking product credal sets means combinations elements taking product probabilities intersection events The technique applicable multivariate distributions instance dimensions rectangles instead intervals Providing speciﬁcation intervals restricts possible distributions means credal set arbitrarily close arbitrary single distribution However credal set speciﬁcation ﬁnite generally restricted arbitrary single distribution The probability bounds query determine maximum error approximation The number intervals determine precision gap probability bounds compute So precision increased arbitrarily Fig 4 gives example Gaussian distribution divided ﬁve intervals equal probability S Michels et al Artiﬁcial Intelligence 228 2015 144 21 Fig 4 Discretised PDF Gaussian distribution ϕv density value v dashed lines indicate intervals distribution discretised 6 A new probabilistic constraint logic programming language After introducing abstract semantics probabilistic logic programs section deals constructing actual language examples given Section 2 The structure rules ﬁxed semantics issue solved represent countable collection credal set speciﬁcations compatible concrete language In summary deﬁne credal sets independent sets random variables represent events constraints similar constraints occurring rules The resulting language called probabilistic constraint logic programming PCLP In following discuss constraint theories context PCLP Then syntax language link semantics discussed Finally discuss inference tasks performed PCLP 61 Constraint theories As mentioned PCLP based arbitrary constraint theories mild restrictions First constraints correspond subset event space fulﬁlling requirement Lemma 3 inﬁnite dimensional proba bility measures constructed inﬁnite number ﬁnite ones increasing dimensionality As discussed large class practically useful constraints inequalities integers real numbers fulﬁl requirement Further satisﬁability constraints decidable corresponds deciding disjointness events shown later Section 63 The requirement means disjointeventsdecidability condition holds It possible use socalled incomplete solvers exact bounds computed discuss later We refer instance PCLP based constraint language Constr PCLPConstr In article use constraint theories examples real numbers R discrete constants D In examples Section 2 article use combination refer language PCLPD R The combination different theories realised assuming constraint theory attached variable Statements variables constraint language corresponding constraint theory attached example realvalued random variables compared discrete ones Constraints different theories combined logical connectives In constraint theory D random variables values discrete constants The basic building blocks language set membership negation special cases equality inequality cid22 The constraint theory similar ﬁnite domain constraint theory CLPFD 35 difference requirement explicitly deﬁne range The range variables countablyinﬁnite number constants effectively restrict range assigning probability mass ﬁnite number constants Using D represent instance Bayesian networks 10 ﬁrstorder formalisms CPlogic 9 The R theory basically CLPR 36 Variables represent real numbers constraints consist linear equalities inequalities predicates cid22 functions One argument mul tiplication operator constant constraints linear guarantee decidability This theory approximate large class continuous distributions shown Section 56 62 Syntax An overview PCLP syntax given Table 2 Rules built according LP syntax additional cid5cid6constructs constraints deﬁned syntax constraint languages discussed previ ously New deﬁnition credal set speciﬁcations countable number variables sense Section 52 We deﬁne credal sets number random variable deﬁnitions Random variable deﬁnitions deﬁne credal sets groups 22 S Michels et al Artiﬁcial Intelligence 228 2015 144 Table 2 PCLP syntax overview Concept Random variable deﬁnition Rule Constraint ϕi Body element bi Atom ai Predicate d Term ti Logical variable Xi Random variable Vi Probability pi Syntax V1 Vnt1 tm p1 ϕ1 pl ϕl b1 bn element Constr V b V W ai notai cid5ϕi cid6 dt1 tn predicate starting lower case Prolog term 3 ab 3 contain logical variables X aX variable starting upper case random variable starting upper case bold 00 10 ﬁnitely random variables deﬁnition random variable depends ﬁnite number ones Deﬁnitions form V1 VnX1 Xm p1 ϕ1 pl ϕl Vi random variable label Xi parameter pi probability ϕi constraint The random variable deﬁnitions deﬁne random variables V1X1 Xm VnX1 Xm groundings X1 Xm All labels parameters sure Xi ground case single ground instance deﬁned random variables program There random variable label list labels distinct If brackets left Example 18 A single random variable instance deﬁned Temperature We deﬁne random variables representing temperature inﬁnitely future days TemperatureDay We ﬁnally deﬁne temperature humidity makes sense case modelled multivariate distribution depen dency random variables expressed logical structure Temperature HumidityDay In case multiple deﬁnitions deﬁne random variable deﬁnition occurring ﬁrst program deﬁnes variable Labels occurring deﬁnition occur deﬁnition identical list labels sure random variables deﬁned unambiguously Example 19 Suppose want specify distributions temperature future days want exception Day 0 We follows Temperature0 TemperatureDay The second line deﬁne Temperature0 case ﬁrst line occurs ﬁrst The following deﬁnition invalid TemperatureDay Temperature HumidityDay Temperature deﬁned ﬁrst line deﬁnition second line contradict deﬁnition A random variable deﬁnition p1 ϕ1 pl ϕl represents credal set speciﬁcation deﬁning ndimensional distri bution V1X1 Xm VnX1 Xm Each pi real number 0 1 p1 pl 1 The ϕi constraint deﬁnitions Vi placeholder random variables Each ϕi consistent sure probability mass assigned nonempty events The pi ϕi expressions X1 Xm S Michels et al Artiﬁcial Intelligence 228 2015 144 23 Example 20 Consider following valid deﬁnition TemperatureDay 02 Temperature 0 08 Temperature 0 The deﬁnition depend value logical variables Day TemperatureDay 02 Temperature Day1000 08 Temperature Day1000 The deﬁnition valid groundings Day restricted numbers It actually expresses temperature future increase average Note deﬁnitions examples Section 2 use syntactic sugar For example consider deﬁnition examples DM_if_HighRisk 0266 yes 0664 The idea kinds deﬁnitions leave probability mass express imprecision The deﬁnition actually abbreviation DM_if_HighRisk cid7 0266 DM_if_HighRisk yes 0664 DM_if_HighRisk 007 DM_if_HighRisk yes cid8 Deﬁnitions continuous distributions like N 00 10 This seen inﬁnite deﬁnitions In case approximation result ﬁnite deﬁnitions possible discussed Section 56 63 Semantics We start showing PCLP program deﬁnes countable sequence ﬁnite credal set speciﬁcations C1 length number random variables deﬁnes credal set speciﬁcation sense Deﬁnition 6 Deﬁnition 10 Credal set speciﬁcation PCLP program We associate single random variable deﬁnition random variable ﬁrst matching deﬁnition occurring program If matching deﬁnition associate speciﬁcation 10 true We ﬁx enumeration random variables denote family deﬁnitions ﬁrst n random variables Dn From deﬁne credal set speciﬁcations program Cn follows Cn πn cid9cid7 p CSSϕ p ϕ ˆcid6 dDn d cid8cid12 Here product random variable deﬁnitions d1 cid4 pd ϕd pe ϕe pd pe ϕd ϕe d ˆe def cid7 cid3 cid8 d e ˆd2 deﬁned We product deﬁnitions deal independent probabilities map constraints events making use solution space CSS The projection n dimensions πn necessary random variables ﬁrst n possibly included case deﬁned deﬁnition ﬁrst n Example 21 In Example 15 twodimensional credal set speciﬁcation variables Time_Comp1 Time_Comp2 given cid14cid3 C2 049 ω1 ω2 0 ω1 1 0 ω2 1 cid4 cid4 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 007 ω1 ω2 2 ω1 3 0 ω2 1 014 ω1 ω2 1 ω1 2 0 ω2 1 cid4 cid4 cid4 cid4 014 ω1 ω2 0 ω1 1 1 ω2 2 004 ω1 ω2 1 ω1 2 1 ω2 2 007 ω1 ω2 0 ω1 1 2 ω2 3 002 ω1 ω2 2 ω1 3 1 ω2 2 cid4 cid4 cid4cid15 002 ω1 ω2 1 ω1 2 2 ω2 3 001 ω1 ω2 2 ω1 3 2 ω2 3 24 S Michels et al Artiﬁcial Intelligence 228 2015 144 This credal set speciﬁcation represented PCLP Time_Comp1 07 0 Time_Comp1 1 02 1 Time_Comp1 2 01 2 Time_Comp1 3 Time_Comp2 07 0 Time_Comp2 1 02 1 Time_Comp2 2 01 2 Time_Comp2 3 Lemma 4 A PCLP program deﬁnes credal set random variables V1 V2 occurring To derive formulas probability bounds PCLP program ﬁrst introduce concept solution constraint equivalent solution event expressed terms constraints cf Lemma 2 Deﬁnition 11 Solution constraint The solution constraint query q deﬁned SCq def cid10 cid11 ϕ MLcid8cid12q cid8Constr ϕcid8 Example 22 Consider clauses running example Section 23 saved cid5Time_Comp1 075cid6 saved cid5Time_Comp1 125cid6 cid5Time_Comp1 025 Time_Comp2 1375cid6 The solution constraint query saved SCsaved Time_Comp1 075 Time_Comp1 125 Time_Comp1 025 Time_Comp2 1375 The way compute probabilities according Proposition 2 requires deciding disjointness events We substitute deciding satisﬁability constraints For introduce function checking satisﬁability constraints ﬁlled constraint checker concrete implementation We consider partially decidable constraint theories Nonlinear constraints instance solved general Deﬁnition 12 Satisﬁability check The function check Constr sat unsat unknown checks satisﬁability constraints The possible values function following meaning The constraint certainly satisﬁable solution The constraint certainly unsatisﬁable solution sat unsat unknown Satisﬁability decided said constraint In case function yields unknown constraint constraint theory fully decidable We express probability bounds query PCLP program satisﬁability check function Proposition 4 The lower upper probability bounds query q concerning ﬁrst n random variables PCLP program fulﬁlling exact inference conditions computed P q P q cid2 p checkϕSCqunsat pϕCn cid2 p checkϕSCqsat pϕCn S Michels et al Artiﬁcial Intelligence 228 2015 144 25 Table 3 Overview inference tasks Precise Pq Imprecise P q P q Discrete Hybrid Partially decidable constraints P q P q cid10 known arbitrary cid10 P q cid10 known bounded cid10 P q P q P q cid10 P q cid10 unknown arbitrary cid10 cid10 P q cid10 P q cid10 unknown bounded cid10 cid10 Here C credal set speciﬁcation deﬁned program Lemma 4 Corollary 2 For PCLP programs making use partially decidable constraints following holds P q P q cid2 p checkϕSCqunsat pϕCn cid2 p checkϕSCqcid22unsat pϕCn Corollary 3 Exact inference PCLPConstr possible queries ﬁnite number ﬁniterelevantconstraints condition ﬁnitedimensional constraints ﬁnitedimensionalconstraints condition case satisﬁability decided constraint language Constr 64 Inference tasks PCLP number inference tasks summarised Table 3 In case discrete distributions compute exact bounds query point probability precise distributions Proposition 4 In hybrid case able compute bounds Proposition 4 Such bounds approximations random variable deﬁnitions use approximations actual continuous distributions For precise case know bounds approximate point probability know maximal error approximation In imprecise case difference lower upper bound partially explained approximated continuous distributions partially imprecision distributions We know good approximation In cases approximation asymptotically equals actual result case inﬁnite computation time Finally case partially decidable constraint theory determine approximations Corollary 2 The achievable precision bounded constraints decidable 7 Inference generalised weighted model counting We shown certain conditions perform exact inference PCLP steps determining solution constraint Deﬁnition 11 computing probability Proposition 4 Both steps exponential time complexity naive way The solution constraint determined considering subsets constraints support set query Then probabilities computed summing possible choices exponentially number elements independent random variable deﬁnitions Inference eﬃciently making use problems structure We ﬁrst step Section 71 develop novel algorithm second step Section 72 We consider theoretical properties terms complexity proposed algorithm key result showing potential discuss results Section 73 We experimentally evaluate theoretical claims Section 74 71 Determining solution constraint We brieﬂy derive solution constraints variation existing techniques The algo rithm based SLD resolution wellknown way derive proofs queries LP We deal constraints bodies way operational semantics CLP collect constraints encounter SLD derivation 37 A derivation proves query case collected constraints true conjunction constraints true This means query derived conjunction constraints derived derivations true The solution constraint disjunction conjunctions relates deﬁnition solution constraints Deﬁnition 11 The main advantage SLD resolution restricts subsets constraints need considered prove query Note coincides way proofs collected ﬁrst implementation ProbLog 6 constraints probabilistic facts general constraints 26 S Michels et al Artiﬁcial Intelligence 228 2015 144 Negation added straightforward way simple SLD resolution suﬃcient case cycles present There solutions problem translating cyclic rules acyclic ones 38 tabled SLD resolution 39 We assume possible eﬃciently derive solution constraint query Possible optimisations applied CLP pruning derivations imposed constraints inconsistent discussed 72 Computing probabilities The proposed algorithm computing probability solution constraint generalisation probabilistic infer ence WMC We ﬁrst discuss way inference introducing generalisation 721 Probabilistic inference weighted model counting Real problems possess lot structure use speed inference Various inference methods developed exploit certain kinds structure Examples variable elimination 40 recursive conditioning 41 We focus performing probabilistic inference translation WMC problem This shown eﬃcient inference method propositional formalisms Bayesian networks 18 applicable probabilistic logics based distributions semantics 19 The approach exploits topological structure local structure determinism 42 contextspeciﬁc independence 43 The problem model counting basically means ﬁnd number models propositional knowledge base WMC straightforward generalisation problem model weight Those weights deﬁned terms weights attached different literals The weight model product weights literals included model Eﬃcient weighted model counting algorithms based observation counts components sharing vari ables computed independently 44 Model counting proceeds follows We assume theory expressed conjunctive normal form CNF In case disjunctions CNF split share variables weights computed independently weight entire CNF product weights step referred decomposition Otherwise case distinction single variable Then gets CNFs assumption variable false assumption true The weight computed sum weights CNFs represent theories disjoint models It happen structure problem determinism choice leads theory simpliﬁed potentially includes fewer variables true false means determinism problem exploited The choice variable order essential eﬃciency algorithm Different possible heuristics choice instance discussed Sang et al 45 Section 32 Example 23 In example illustrate basic idea binary model counting The example later compare generalised WMC algorithm We splitting omit decomposition identical generalised version We start solution constraint Example 22 convert CNF SCsaved Time_Comp1 075 Time_Comp1 125 Time_Comp1 025 Time_Comp2 1375 Time_Comp1 075 Time_Comp1 125 Time_Comp1 075 Time_Comp1 025 Time_Comp2 1375 In following abbreviate primitive constraints follows ϕ1 Time_Comp1 075 ϕ2 Time_Comp1 125 ϕ3 Time_Comp1 025 Time_Comp2 1375 Weighted model counting proceeds choosing variable split means continue branches assumption variable true false respectively This illustrated Fig 5 Note assuming truth values literals makes possible immediately simplify constraint case generalised version shown later The computation stops case true false derived To illustrate binary version comparison generalised assume example ϕ1 ϕ2 ϕ3 represent independent binary random variables probabilities 01 02 03 respectively To compute probabilities probability corresponding choice edge replace true 10 false 00 shown Fig 6 The probabilities remaining nodes computed bottomup taking sum probability associated children weighted probability associated edges Then probability root node probability query q S Michels et al Artiﬁcial Intelligence 228 2015 144 27 Fig 5 Binary WMC example deriving possible models Fig 6 Binary WMC example computing probabilities 722 Generalised weighted model counting involving constraints For general case longer restrict binary variables makes deciding constraints solutions complex Rather employ socalled satisﬁability modulo theories solvers The satisﬁability modulo theories SMT problem problem deciding given FOL theory including additional background theories satisﬁable SMT solver technology eﬃciency decide events disjoint conjunction constraints representing satisﬁable Very eﬃcient SMT solvers linear arithmetic currently available 4647 Since use solvers modiﬁcations discuss algorithms decide satisﬁability An SMT solver implementation function check Deﬁnition 12 Even constraint theory theoretically decidable practice decision procedure implemented We refer solvers decide satisﬁability instances cases return unknown incomplete solvers Incomplete solvers consequences inference discussed partially undecidable constraint theories Section 64 As way implement elements credal set speciﬁcations instead use choices random variables deﬁned follows Deﬁnition 13 Choice A choice ψ function selects random variable Vi probabilityconstraint pair deﬁnition Without loss generality restrict random variables actually occur solution straint query random variables inﬂuence probability query 28 S Michels et al Artiﬁcial Intelligence 228 2015 144 Algorithm 1 Generalised weighted model counting algorithm GWMC Input Constraint ϕ CNF partial choice constraint cid12 Result P ϕ cid12 P ϕ cid12 1 simplify ϕ assuming cid12 ϕ false return 00 00 2 3 ϕ true return 10 10 4 independent CNFs ϕ1 ϕ2 ϕ return GWMCϕ1 cid12 GWMCϕ2 cid12 5 6 random variable Vi ϕ occurring cid12 p GWMCϕ cid12 ψ 7 return cid13 ψVi pψ 8 return 00 10 We generalise steps WMC case distinction decomposition The resulting algorithm Algo rithm 1 compute bounds querys q probability GWMCSCq true discussed In case SMT solver incomplete bounds given Corollary 2 Case distinction We ﬁrst discuss process distinguishing cases atoms true false generalised Instead cases consider possible choices random variable level illustrated following example Example 24 We use solution constraint Example 23 SCsaved Time_Comp1 075 Time_Comp1 125 Time_Comp1 075 Time_Comp1 025 Time_Comp2 1375 ϕ1 Time_Comp1 075 ϕ2 Time_Comp1 125 ϕ3 Time_Comp1 025 Time_Comp2 1375 This time treat ϕ1 ϕ2 ϕ3 constraints instead binary literals consequence split cases literal true false consider choices given random variable deﬁnitions We use random variable deﬁnitions Example 21 Time_Comp1 07 0 Time_Comp1 1 02 1 Time_Comp1 2 01 2 Time_Comp1 3 Time_Comp2 07 0 Time_Comp2 1 02 1 Time_Comp2 2 01 2 Time_Comp2 3 We denote choices Time_Comp1 ψ11 ψ12 ψ13 choices Time_Comp2 ψ21 ψ22 ψ23 We level split choices variable shown Fig 7 The important difference binary WMC constraint immediately simpliﬁed Simpliﬁcation possible case constraints negation consequence choices Therefore branches choices random variables decided constraint true false This corresponds probability mass contributing upper lower bound On hand binary WMC cases examining choices necessary 3 implies ϕ1 rightmost branch beneath root node Fig 7 The choice ψ13 imposes 2 Time_Comp1 Time_Comp1 075 ϕ2 Time_Comp1 125 false entire solution constraint false This shows order variables chosen matters importantly generalised setting use determinism The tree Fig 7 9 leaves choices examined use structure reduce 7 In general subconstraint ϕ CNF consequence choices cid12 far cid12 cid12 ϕ replaced true Similarly negation consequence choices cid12 cid12 ϕ replaced false The SMT solver check conditions making use fact cid12 cid12 ϕ consequence checkϕ cid12 unsat cid12 cid12 ϕ consequence checkϕ cid12 unsat A subconstraint ϕ instance single primitive constraint CNF constraint represented entire CNF After substitution CNF simpliﬁed usual logical rules The simpliﬁed CNF enable decomposition random variables removed S Michels et al Artiﬁcial Intelligence 228 2015 144 29 Fig 7 Generalised WMC example Fig 8 Generalised WMC example computing probabilities To compute probability bounds proceed binary WMC use probability pairs represent lower upper bound Leaves corresponding true contribute bounds substituted 10 10 leaves corresponding false contribute substituted 00 00 ﬁnally leaves undecided contribute upper bound substituted 00 10 Each bound computed similar binary case Example 25 We continue computing probability bounds tree Example 24 illustrated Fig 8 The result equals bounds computed Example 15 applied semantic deﬁnition This idea exploited Algorithm 1 Lines 13 We use cid12 denote partial choice conjunction straints chosen random variable deﬁnitions possibly relevant random variables At Line 1 current solution constraint simpliﬁed discussed Note eﬃcient check possible subconstraints step There strategies subconstraints check checking primitive constraints choices checking entire solution constraint certain steps In implementation use experiments check primitive constraints choices step This strategy combination simple logical simpliﬁcations suﬃcient prove solution constraint false true Only case variables solution constraint chosen implementation check entire solution constraint If constraint simpliﬁed true false branch computation ﬁnished Lines 2 3 If case distinction Lines 6 7 eﬃciency computations depends order variables se lected Line 6 The main objective eliminate random variables enable decomposition binary WMC In generalised setting order variables determines determinism exploited As GWMC terminates case inconsistency simple heuristic order random variables Vi V j Vi occurs inequality constraint fewer variables average V j For example constraint 30 S Michels et al Artiﬁcial Intelligence 228 2015 144 Algorithm 2 Generalised weighted model counting algorithm optimisations OGWMC 1 Assuming initially global cache map cache Input Constraint ϕ CNF partial choice constraint cid12 Result P ϕ cid12 P ϕ cid12 occur cid12 occurring components ϕ variables occurring ϕcid23 occur cid12 ϕ variables occurring ϕcid23 ϕ cid12 cache return cacheϕ cid12 lower upper OGWMCϕ cid12 ϕcid23 result 00 upper 2 simplify ϕ assuming cid12 3 4 ϕ false return 00 00 5 ϕ true return 10 10 6 disjunction ϕcid23 7 8 9 primitive constraint ϕcid23 10 11 12 independent CNFs ϕ1 ϕ2 ϕ 13 14 random variable Vi ϕ occurring cid12 15 lower upper OGWMCϕ cid12 ϕcid23 result lower 10 result OGWMCϕ1 cid12 OGWMCϕ2 cid12 p OGWMCϕ cid12 ψ result cid13 ψVi pψ 16 return 00 10 17 cacheϕ cid12 cid2 result 18 return result X 0 X Y 0 ﬁrst select choices X choices constraint inconsis tent In implementation use experiments prioritise exploiting determinism use simple counting heuristic choosing variables Concretely count number disjunctions random variable occurs intuitively eliminating variables occurring disjunctions gives opportunities decomposition In case random variables count try exploit determinism choosing variable occurring primitive constraint number random variables Decomposition In spirit wellknown RelSAT algorithm 44 weighted model counting observe cases problem decomposed subconstraints share random variables Example 26 Consider solution constraint V1 0 V2 0 let cid12Vi 0 P V2 0 lower bound computed examining cid12V1 examined denote number choices random variable Vi In case P V1 V2 P V1 choices naively cid12V1 cid12V2 cid12V2 In Algorithm 1 decompose possible Line 4 recursively compute bounds CNFs Line 5 Other optimisations We discuss optimisation resulting reﬁned Algorithm 2 Some optimisations binary WMC straightforwardly generalised Consider example caching For binary WMC case CNF encountered twice count So computing subproblems multiple times avoided caching In generalised setting CNF contain random variables choice means CNF different counts We use caching reusing results equal CNFs combination equal choices random variables occurring CNF So ﬁrst check result cached Line 3 compute result store cache Line 17 The characteristics generalised problem require additional optimisations The fact making choices random variables truthfulness constraints involving undetermined dramatically hinder oppor tunities decomposition To counter implemented optimisations eliminating undetermined constraints First case variables occurring disjunction CNF chosen disjunction undetermined lower bound CNF 00 components CNF reduce true single disjunction lead undetermined CNF Conversely determining upper bound assume disjunction holds realised adding constraints imposed choices formalised Lines 68 Algorithm 2 This leads elimination disjunction simpliﬁed CNF It easily shown upper bound original CNF equals upper bound simpliﬁed CNF additional assumption included cid12 Secondly case disjunctions CNF undetermined constraint common follows upper bound 10 This constraint solution constraint true constraint proven false choices variables occurring In case lower bound equals lower bound CNF assuming negation constraint This formalised Lines 911 Algorithm 2 S Michels et al Artiﬁcial Intelligence 228 2015 144 31 73 Complexity analysis To obtain insight complexity inference problem PCLP analyse main source computational complexity computation probabilities solution constraint Very imprecise probabilistic infer ence general complex precise probabilistic inference For instance obtaining bound marginal probability given credal network NPPPcomplete PPcomplete Bayesian networks 48 In order com pare complexity PCLP credal networks ﬁrst decision problem associated PCLP inference similar Bayesian network inference practical constraint theories PP NPPPhard Of course reduced complexity comes cost expressiveness discussed Section 8 Subsequently proven PCLP complexity properties similar Bayesian network inference WMC treewidth problem bounded 731 Worstcase complexity By Corollary 1 clear computing lower upperbound PCLP query complexity Therefore problem consider given solution constraint query q probability p P q p We ﬁrst given complexity constraint checking C problem PPC Theorem 4 Given PCLP program P solution constraint ϕ particular query q determining bound P q PPC C complexity checking satisﬁability sets constraints P If complexity SMT solving polynomial C P problem PP Furthermore proving PPhardness trivial MAJSAT deciding half assignments propositional formula satisfy formula PPcomplete obviously reduced bound probability solution constraint This shows main PCLP inference problem complex precise discrete probabilistic inference checking constraints tractable deal random variables arbitrary domain perform particular kind imprecise inference In following abstract complexity SMT solving For interesting constraint domains equalities discrete constraints inequalities sums differences real numbers complexity SMT solving polynomial 4950 In theories desirable use incomplete solvers polynomial algorithms theories satisﬁability problem NPcomplete arithmetic integers 33 harder real number arithmetic including multiplication 34 undecidable general arithmetic integers including multiplication 51 Finally note typical problems size constraints small compared size entire probabilistic inference problem 732 Parametrised complexity terms treewidth Inference algorithms exploit structure problems For example upper bounds inference Bayesian networks precisely expressed terms networks treewidth t 52 O ct c maximum cardinality variables Instead cardinality variables use maximum number choices d context PCLP The number choices d bounded 2cn n number variables It smaller strength PCLP especially case c inﬁnite The treewidth measures treelike graph In case Bayesian network tree inference linear time A formal deﬁnition treewidth given In context want bound complexity terms property input CNF There different kinds graphs representing structure CNFs algorithms determining model count CNFs complexity bounded terms treewidth different kinds graphs known 53 We focus primal graph undirected graph CNFs atoms nodes nodes occurring disjunction CNF connected For PCLP inference problem slightly adapt deﬁnition primal graph First use random variables instead atoms Secondly consider additional dependencies random variables Those dependen cies emerge fact possible eliminate constraints spite fact choices random variables occurring discussed Section 722 We refer imprecise constraints constraints ϕ choice cid12 variables occurring ϕ simplify cid12 cid12 ϕ cid12 cid12 ϕ Deﬁnition 14 Constraint primal graph The constraint primal graph CNF including constraints undirected graph random variables occurring CNF nodes Two nodes X Y connected following conditions hold 1 There disjunction CNF including X Y 2 X occurs disjunction imprecise constraint ϕ Y occurs disjunction imprecise constraint χ ϕ χ share random variables 32 S Michels et al Artiﬁcial Intelligence 228 2015 144 Fig 9 Example constrained primal graphs Fig 10 Example tree decompositions Example 27 As example consider following CNF X 0 Y 0 Y Z The constrained primal graph case constraints imprecise given Fig 9a In case constraint Y Z imprecise additional edge added shown Fig 9b To measure complexity constraint primal graph introduce existing notions tree decomposition treewidth Deﬁnition 15 Tree decomposition See 54 A tree decomposition undirected graph G V E tree T W H nodes x1 xn xi subset V satisfying 1 T includes nodes G 2 All nodes connected edges G occur node T 3 If vertices T contain vertex G nodes tree path vertexes contain vertex G Deﬁnition 16 Treewidth See 54 The treewidth graph width tree decomposition minimal width The width tree decomposition size largest node minus 1 Example 28 A tree decomposition minimal width graph shown Fig 9a given Fig 10a Therefore treewidth graph 1 For graph Fig 9b tree decomposition possible shown Fig 10b The treewidth increased 2 Determining treewidth given graph NPcomplete 55 However treewidth known tree decomposition width constructed linear time 56 Finally present main result section relates input CNFs constraint primal graph treewidth complexity inference Theorem 5 Given input CNF bounded treewidth t complexity Algorithm 2 proper variable order O m t dt m number nodes tree decomposition d maximal number choices single variable probability mass assigned We emphasize complexity similar results ordinary model counting instance complexity terms primal graph Samer Szeider O o m t 2t o maximum number occurrences variables 733 Making use additional structure Chavira et al 18 argue standard algorithms complexity cid14ct worstcase bestcase complexity bounded exponentially treewidth In contrast WMC use local structure determinism 42 contextspeciﬁc independence 43 cases perform better Superiority WMC experimentally shown 57 The true generalised WMC algorithm Example 24 instance shows branches pruned means exponentially choices examined This evaluated experimentally following S Michels et al Artiﬁcial Intelligence 228 2015 144 33 74 Experiments In section provide insight behaviour proposed algorithm In particular ference algorithm competitive existing approaches discrete problems overhead handling constraints computing bounds instead single probabilities negligible We investigate scalability algorithm algorithm use determinism The implementation available httpwwwsteffenmichelsdepclp It makes use YAP Prolog 622 SMT solver Yices 201 47 supports linear arithmetic The experiments run Ubuntu 1404 Laptop Intel Core i3 24 GHz processor 4 GB RAM All runtimes averaged 10 runs 741 Comparison implementations We ﬁrst compare PCLP implementation implementations based similar inference algorithms Such implemen tations limited precise distributions discrete variables We use precise discrete problem compare The question want answer experiment scalability generalised inference algorithm compares existing approaches We expect overhead computing bounds instead point probabilities bounds equal precise problem use handling constraints However scalability generalised algorithm essentially similar We compare ProbLog 1 6 ProbLog 2 58 The ﬁrst version ProbLog works similar implementation First explanations collected SDD resolution resulting formula similar solution constraint The models formula counted compilation binarydecision diagrams BDD ProbLog 2 uses different approach All grounded rules translated equivalent propositional formulas deﬁned WMC problem This yield compact CNFs requires inclusion head atoms CNF instead probabilistic facts mentioned proofbased approach The CNF compiled deterministic decomposable negation normal form dDNNF Because performance approach mainly depends dDNNF compiler provide results difference compilers C2D 59 DSHARP compiler 60 A fair comparison PRISM possible makes strong assumption bodies head exclusive It solve diﬃcult problem computing probabilities possibly overlapping bodies according inclusionexclusion principle mentioned approaches For experiment use version fruit selling problem introduced Section 21 In original version compare actual price price customers willing pay buyFruit priceFruit P cid5P Max_priceFruitcid6 We replace simple discrete random variables indicating customer willing pay price government support Buys_with_supportFruit 03 yes 07 Buys_without_supportFruit 06 yes 04 The deﬁnition buy buyFruit cid5SupportFruit yescid6 cid5Buys_with_supportFruit yescid6 buyFruit cid5SupportFruit nocid6 cid5Buys_without_supportFruit yescid6 In experiment query buyfruit1 buyfruitn increasing n evaluate inference time Ideally inference time scale linearly query disjunction n independent formulas This requires eﬃcient caching elimination mechanism The result shown Fig 11 It clearly shows PCLP competitive spite overhead calling SMT solver computing bounds equal case instead point probabilities However achieve linear complexity ideally expected Only C2D scales linearly superior large n small n implementations perform better We adapt problem introduce continuous random variable fruits price The price customer willing pay uncertain makes constraint onedimensional This means dealt Hybrid ProbLog 13 integrated ProbLog 1 implementation experiment For PCLP implementation discretise continuous distributions points constants guarantees precise distribution All implementations previous comparison deal problem The price kind fruit deﬁned example Section 21 YieldFruit N 120000 10000 SupportFruit 03 yes 07 basic_priceFruit 250 0007 YieldFruit 34 S Michels et al Artiﬁcial Intelligence 228 2015 144 Fig 11 Comparison PCLP implementation discrete problem Fig 12 Comparison PCLP Hybrid ProbLog continuous problem priceFruit BPrice 50 basic_priceFruit BPrice cid5SupportFruit yescid6 priceFruit BPrice basic_priceFruit BPrice cid5SupportFruit nocid6 For price customers willing pay use constant The rule deﬁning buy buyFruit priceFruit P cid5P 1000cid6 We evaluated inference time query buyfruit1 buyfruitn increasing n The result Fig 12 shows performance PCLP competitive The fact PCLP superior conﬁrms better results previous experiment As time handling continuous distributions negligible result reﬂects higher overhead problem BDD package ProbLog compared PCLP 742 Scalability We consider original problem Section 21 means use following rule model customers buy certain kind fruit buyFruit priceFruit P cid5P Max_priceFruitcid6 Inference problem performed methods compared previously knowledge PCLP framework provide approximations known error To perform inference PCLP discre S Michels et al Artiﬁcial Intelligence 228 2015 144 35 Fig 13 Inference time vs maximum error query buyfruit1 buyfruitn tise continuous distributions described Section 56 naively choosing n intervals equal probability CDFs In experiments increase n decrease maximal error approximation increases inference time Note naive discretisation scheme purpose focus evaluating generalised WMC algorithm showing potential It expected better performance achieved sophisticated discretisation remains future work As compute P buyfruit1 buyfruitn time determine approximations The result shown Fig 13 relationship inference time maximum error The result shows interesting nonmonotonic behaviour n 2 inference expensive n 1 larger n eﬃciency inference increases increasing n For n 10 achieve substantially better performance small n The eﬃciency decreases n 1 n 2 general higher dimensional spaces ﬁner discretisation necessary achieve precision So naive inference approach maximum error higher larger n given inference time However making use problems structure inference time choice space subexponential particular subconstraints independent That effect dominates larger n error decrease given inference time The experiment shows potential inference approach structure problem sub problems concerning continuous variables estimated low precision order achieve satisﬁable precision entire problem 743 Exploiting determinism We ﬁnally inference algorithm use determinism comparing inference times query buyapple cid5Cropapple Xcid6 X The topological structure problems larger X query determined unsatisﬁable based choice random variable Cropapple The result shown Fig 14 relationship inference time maximum error For X 10 000 result similar result query buyapple probability excluded subspace Cropapple 10 000 small With higher X inference eﬃciency drastically increases achieve certain precision inference time needed The experiment conﬁrms inference algorithm exploits determinism similar WMC algorithms binary case 8 Related work This section organised follows We ﬁrst link work languages related PCLP After discuss inference methods relationship method presented paper 81 Related languages There paradigms probabilistic programming related paradigms nonprobabilistic pro gramming languages They range graphical approaches Bayesian networks 10 MultiEntity Bayesian networks 61 imperative objectoriented approaches FACTORIE 62 Figaro 63 purely functional approaches Church 64 logic programming approaches ICL 8 ProbLog 6 logic approaches BLOG 65 PCLP ﬁts 36 S Michels et al Artiﬁcial Intelligence 228 2015 144 Fig 14 Inference time vs maximum error query buyapple cid5Cropapple Xcid6 varying X Table 4 Related languages Precise Imprecise Discrete Bayesian Networks 10 ProbLog 6 PRISM 7 ICL 8 Hybrid Conditional Linear Gaussian 66 Hybrid Problog 13 Distributional clauses 14 BLOG 65 Locally Deﬁned Credal Networks 67 Probabilistic logic programming 68 Imprecise Probabilistic Horn Clause Logic 69 PCLP logic programming approach shown powerful knowledgerepresentation tool nonprobabilistic problems As result ﬁrst arguably best studied ﬁrstorder probabilistic programming paradigm wellfounded semantics In following focusing underlying programming paradigm discuss expressivity related languages kinds distributions represented This discussed dimensions discrete versus hybrid distributions precise versus imprecise distributions The resulting classes formalisms example formalisms shown Table 4 guide rest section As table illustrates able represent imprecise hybrid distributions knowledge unique feature PCLP 811 Precise discrete distributions PCLP clearly subsumes languages allowing deﬁne discrete precise probability distributions including propo sitional ones Bayesian networks 10 ﬁrstorder languages probabilistic logics based Satos distributions semantics Examples ICL 8 PRISM 7 ProbLog 6 CPlogic 9 Approaches weights stead probabilities Markov logic networks MLNs 70 different nature compared approaches sense parameters models necessarily direct probabilistic meaning A number approaches use constraints model discrete distributions This advantages representation learning mind There number languages PCLP based deterministic constraint logic program ming CLP One example language CLPBN 71 use constraints denote events deﬁne probability distributions considers probability distributions constraints The approach different approaches based distribution semantics illustrated examples Costa Paes 72 Especially dynamic models Hidden Markov Models represented PRISM compactly concisely This observation similarly holds approaches based distributions semantics including PCLP An approach similar CLPBN general clppdfy 73 restricted discrete random variables Another approach Riezler 74 sharing language assigns probabilities derivation choices The approach developed natural language processing applications mind consider continuous distributions Finally Satos CBPMs 75 based idea discrete joint probability distribution expressed independent binary random vari ables constrained arbitrary FOL formulas Constraints setting purely logical additional theories approach restricted discrete distributions cover countably inﬁnite domains integers S Michels et al Artiﬁcial Intelligence 228 2015 144 37 A limitation PCLP logic programming approaches directly support generative deﬁnitions random variables deﬁned terms value random variables For ﬁnite discrete distributions circumvented introducing distinct random variables related value random variable However possible inﬁnite distributions One example generative deﬁnitions useful modelling unknown number objects shown work BLOG 65 The number objects instance modelled Poisson distribution discrete ﬁnite Such generate process modelled PCLP number relevant constraints ﬁnite exact inference possible PCLP Theorem 3 812 Imprecise discrete distributions Approaches dealing imprecise probabilities typically represent complex imprecise distributions PCLP example 7668 Speciﬁcally PCLP express qualitative relations probabilities events express probability event greater probability event b However restricted way imprecise prob abilities deﬁned PCLP guarantees deﬁnitions inconsistent Theorem 1 Also languages guarantee consistency locally deﬁned credal networks LDCNs 67 relational variants 77 expressive PCLP express conditional credal sets Of course comes price inference complex cf Section 73 PCLP express open probability intervals realised Imprecise Probabilistic Horn Clause Logic 69 inﬁnitesimal probabilities 813 Precise hybrid distributions All approaches discuss general PCLP sense continuous distributions restricted real valued random variables The semantic foundation PCLP makes use general notion continuous distributions concerning random variables arbitrary uncountable ranges There general roughly different ways deal continuous distributions computing posterior continuous distributions continuous random variables computing probabilities events Examples ﬁrst approaches Conditional Linear Gaussian CLG models 66 ﬁrstorder approach 78 based distribution semantics The approach provides information continuous dis tributions possible probabilities events This instance computing expectations decision making However approach requires way represent resulting probability density functions distributions Those combinations Gaussian distributions ways represent functions mixtures truncated basis functions 79 piecewise polynomials 80 approximate usually employed distributions strong guar antees given approximation close true distribution Summarised determining result distributions achieved severely restricting distributions allowed conditional linear Gaussian distributions approximate representation The second approach deal continuous distributions adopted PCLP focuses computing probabilities events involving continuous variables It powerful decision making case discrete decisions Within category probabilistic languages based imperative objectoriented functional approaches support hybrid distributions A straightforward hybrid extension logic programming based approaches Hybrid ProbLog 13 allows realvalued random variables restricts use dimensional constraints instance X 0 This frame work expressive PCLP One general extensions logical semantics Distributional clauses DCs 14 supports arbitrary constraints realvalued random variables generative deﬁnitions vari ance distribution depend value Such generative deﬁnitions expressed PCLP However incorporating generative deﬁnitions context distribution semantics complicates semantics puts burden user language requiring number technical requirements program valid 14 Deﬁnition 3 Our extension distribution semantics view close original idea provides necessary properties analysis exact inference conditions credal set extension expressive 82 Related inference methods The main contribution presented paper idea deﬁne events terms decidable constraint theories deﬁne credal sets based constraint theories allow exact computation bounds In subsection consider related methods probabilistic inference subdivided exact approximate lifted inference 821 Exact inference There algorithms exact probabilistic inference deﬁned Bayesian Networks Examples variable elimination 40 recursive conditioning 41 As discussed base work approach translating inference WMC problems 18 method makes possible exploit structure determinism 42 context speciﬁc independence 43 occurs logical theories Such exact inference possible hybrid models restricted types distributions example Conditional Linear Gaussian CLG models 66 In probabilistic logic context inference Hybrid ProbLog 13 exactly Since constraints onedimensional distributions discretised points occurring inequalities proofs query turns inference discrete problem 38 S Michels et al Artiﬁcial Intelligence 228 2015 144 A general related approach inference probabilities constraints Dechter 81 focuses al gorithms inference problems All considered problems share property structure problem instances represented graphs Examples problems include probabilistic reasoning constraint satisﬁability exactly problem uniﬁed approach Dechters work basis interleaving probabilistic reasoning constraint checking eﬃcient way PCLP inference based uniform representation combining aspect inference problems In current approach probabilistic inference uses information random variables occur primitive constraints use structure constraints Exact inference algorithms developed imprecise case Since problems strictly complex precise case methods precise case applicable For instance resort methods multilinear programming 77 An exception Imprecise Probabilistic Horn Clause Logic IPHL 69 providing imprecise formalism complexity hard precise case However IPHL restricts language binary random variables Horn clauses PCLP impose restrictions 822 Approximate inference For Bayesian Networks class approximation algorithms based Pearls belief propagation algorithm 10 computes posterior probabilities passing messages nodes However algorithm terminates produces correct probabilities networks polytrees inference polynomial time For general case algorithm converges good approximation correct probabilities A lot work convergence quality approximation 8285 hard guarantees quality result provided contrast inference method Similarly PCLP work approximation schemes providing hard guarantees discrete distributions sim plifying problem 86 Recently Renkens et al shown effectiveness approach combination stateoftheart knowledge compilation techniques 87 The main difference PCLP work PCLP model reason continuous imprecise distributions approaches aimed approximate inference discrete precise problems As mentioned continuous distributions inference method use approximate representations distribu tions typically mixtures functions Examples approach mixtures truncated basis functions 79 piecewise polynomials 80 Such approximation functions compute posterior probabilities hard guarantees quality result given The accuracy approximation usually measured terms KullbackLeibler divergence 88 distributions directly related error com puted probabilities This makes hard draw conclusions quality probabilities expectations derived results essential decision making Methods virtually restrictions distributions based Markov chain Monte Carlo MCMC sampling Those methods effective handtailored speciﬁc problems There frameworks providing MCMC inference generic relational probabilistic languages 8990 Recently signiﬁcant progress improve known problems MCMC methods slow convergence highdimensional distri butions 91 distributions near deterministic probabilities 92 Despite improvements qualitative difference sampling methods method proposed work No hard guarantees given error sampling estimation Even perfect circumstances guarantees MCMC given terms instance Monte Carlo standard error gives probabilistic hard guarantee Even worse realistic cases known distributions guarantees given The approximation pseudo converge means converged solution actually case Determining Markov chain converged possible complicated theoretical conditions 93 In general known way sure Markov chain actually converged 94 p 21 In contrast PCLP pushes expressivity far possible allows measure quality making possible decide computation time effort acquire evidence invested compute better results 823 Lifted inference Another method deal intractability probabilistic problems lifted inference The idea symmetries inference problems especially present ﬁrstorder models exploited signiﬁcantly improve eﬃciency inference A survey lifted inference approaches provided Kersting 95 Lifted inference applied continuous distributions shown Choi et al 96 PCLP inference proﬁt lifted inference scope paper 9 Conclusions future work We introduced new generalised distribution semantics probabilistic logic programming making possible use random variables arbitrary ranges The semantics illustrate integration known techniques pro vide powerful formalism In particular combine ability logic represent relational knowledge probability theory deal uncertainty constraints representing conditions variables ranges S Michels et al Artiﬁcial Intelligence 228 2015 144 39 As commonly known exact probabilistic inference possible conditions pose strong restrictions distributions As alternative approximation methods propose use framework based credal sets lower upperbounds posteriors guaranteed Placing method theory imprecise probabilities provides clear view approach allowing explore properties compare approaches Also powerful applications approach straightforwardly deﬁned instance approximating probabilities events known arbitrarily small maximum error hybrid distributions We concrete language based concept perform exact inference similar complexity precise case unique property imprecise formalism We developed concrete inference algorithm based generalisation WMC The algorithm able exploit kinds structure ordinary WMC stateoftheart precise probabilistic inference This shows beneﬁt general semantic foundation integrating existing AI approaches The language straightforwardly extended arbitrary constraint theories ordinary constraint logic programming Furthermore performing inference use existing algorithms ﬁeld logic programming constraint satisfaction particular modern SMT solvers generalise probabilistic inference weighted model counting inheriting methods strengths In future important aspect PCLP practically useful implement eﬃcient inference mechanisms One research direction apply knowledge compilation techniques successfully applied standard WMC problem Also naive way discretise continuous distributions article probability improved enormously Further computing expectations random variables values important problem solve practical applications We ﬁnally plan apply PCLP building information fusion models context maritime safety security project METIS 9798 Appendix A Proofs Proof Lemma 1 We equation equivalent Deﬁnition 4 cid3 cid4 P q P T P V ωV ωL cid4T ωL cid12 q cid3 ωV ωV ωL cid4T MLωV cid12 ωL ωL cid12 q cid4 Deﬁnition 3 The condition MLωV cid12 ωL ωL cid12 q equivalent MLωV cid12 q MLωV uniquely determines ωL Therefore cid3 cid3 cid3 P q P V P V P V ωV ωV ωL cid4T MLωV cid12 q ωV cid4V MLωV cid12 q cid4 SEq Deﬁnition 5 cid2 cid4 cid4 Proof Lemma 2 By applying Deﬁnitions 1 5 prove ωV cid4V MLωV cid12 q cid7 ωV cid4V cid10 cid11 cid8 ϕωV MLcid8cid12q cid8Constr ϕcid8 Assume ωV element lefthand set MLωV cid12 q We choose cid8 MLcid8 MLωV Since ωV constraints cid8 satisﬁable ϕωV holds ωV element cid16 ϕcid8 righthand set Assume ωV element righthand set cid8 MLcid8 cid12 q cid16 ϕωV Because know MLωV includes cid5ϕcid6 constraints ϕ cid8 From MLcid8 cid12 q ϕcid8 conclude MLωV cid12 q monotonicity ﬁrstorder logic This means ωV element lefthand set cid2 Proof Proposition 1 Assuming conditions hold prove possible compute probability arbitrary query q analytic expression We solution event q ﬁnitedimensional means ﬁnite representation computed ﬁnite time The probability q computed according Lemma 1 assuming computablemeasure condition The solution event lemma Lemma 2 makes use subsets constraint language derives unique models The ﬁniterelevantconstraints condition means order ﬁnd cid8 MLcid8 cid12 q consider ﬁnite number occurrences cid5cid6 implies ﬁnite number constraints We consider ﬁnite number subsets event space built solution space combination ﬁnite number constraints The solution event consequently computed ﬁnite time Moreover constraints construction solution event ﬁnitedimensional ﬁnitedimensionalconstraints condition solution event ﬁnite cid2 40 S Michels et al Artiﬁcial Intelligence 228 2015 144 Proof Lemma 3 We element P V PV constructing ﬁnitedimensional Pk V The proof induction assuming number random variables inﬁnite Obviously proof holds ﬁnite V given arbitrary C1 2 given P k number random variables We 1 P 1 V consistent Ck compatible P k1 consistent Ck1 given Ck Ck1 compatible That P k V P k1 Ve We inﬁnite sequence compatible ﬁnitedimensional probability distributions construct probability measure entire event space given basic assumptions lemma compatible means kdimensional event e P k1 πk1e P k V P1 V V V In following complete construction probability measures general multiple distributions required probability Therefore ﬁx probability number disjoint events way makes possible construct valid probability measure We assign probability mass disjoint events probability mass rest probability space All possible measures assigning probabilities events way clearly assign 1 entire space countably additive valid probability measures 1 The disjoint events use construct P 1 V intersections relative complements events C1 set We sure events disjoint restricting minimal ones ones subset collection Intuitively seen areas Venn diagram include areas We denote events f 1 fn assign pi ei C1 probability pi f j ei There f j In case choose f j multiple ei sum probabilities What remains shown measure P 1 V constructed way obeys inequalities Deﬁnition 7 events e cid2 p P 1 Ve cid2 p decid22 pdC1 pdC1 cid2 pdC1 cid2 pdC1 p P 1 Ve p 1 P 1 Ve P 1 Ve cid2 p decid22 pdC1 First prove lower bound holds For event e consider subsets e C1 e1 em In construction P 1 V probability mass assigned e1 em assigned subsets f 1 fmcid23 e1 em turn subsets e Therefore Similarly probability mass ei ei e assigned f j e means 2 We assume given P k V obeying speciﬁcation Ck Ck1 compatible Ck From construct probability measure P k1 V compatible P k V For p e Ck p1 e1 pn en Ck1 πkei e 1 j n sum pi equals p Ck Ck1 compatible Further e f probability mass p assigned construction P k V We assign probability masses p1 pn intersections relative complements e1 f en f Such intersections disjoint nonempty denote f 1 fm We sure P k1 V f 1 fm events P k V assigns probabilities gk1 g f P k f cover entire probability space probability mass assigned compatible P k cid4 cid3 P k πk f j V For kdimensional f number k 1dimensional V f Therefore possible construct P k1 way V assigns g This makes measures compatible V The constructed probability measure obeys bounds deﬁned Deﬁnition 7 similar arguments P 1 V For event ei Ck1 disjoint events f 1 fm probability mass assigned All event Ck1 subsets f 1 fm certainly contribute probability e disjoint events certainly contribute cid2 Proof Theorem 1 The theorem follows directly Lemma 3 Deﬁnition 3 Deﬁnition 4 cid2 Proof Proposition 2 Bounds probabilities event deﬁned Deﬁnition 7 The theorem puts bounds querys solution event SEq probability equal querys probability Lemma 1 To bounds actually minimum maximum set probabilities tight bounds query q probability distribution P probability q equals lower 1 distribution probability equals upper bound 2 We proof construction similar proof Lemma 3 The construction applies construction V constructions P k1 We assign probability masses disjoint events f 1 fn e Ci V P 1 S Michels et al Artiﬁcial Intelligence 228 2015 144 41 assign probability mass arbitrary events The events f 1 fn minimal intersections relative complements events credal set speciﬁcation Additionally split events f subset disjoint solution event f SEq f SEq 1 For e subset solution event assign probability mass event f 1 fn disjoint solution event Such event exists f 1 fn subset e SEq split disjoint solution event This means e subsets solution event contribute probability consequently equals lower bound deﬁned proposition 2 We kind construction time assign events subsets solution event possible This leaves e disjoint solution event probability equals upper bound deﬁned proposition cid2 Proof Theorem 2 Using Proposition 2 bounds given ﬁnitedimensional Ck If increase k events e Ck split subsets e Events e probability contributes lower bound kdimensions higher dimensions events e possibly split subsets e Some subsets events contributing lower bound k dimensions higher dimensions This means lower bound overestimated ﬁnite k come closer actual lower bound higher dimensions Similarly upper bound underestimated ﬁnite k increasing k probability comes closer actual upper bound In case bounds underestimated overestimated caused fact event split higher k event compensated increasing higher k This means arbitrarily close actual bounds cid2 Proof Corollary 1 This follows directly Theorem 2 sum rule limits fact probabilities credal set speciﬁcations sum 10 Deﬁnition 6 fact SEq disjoint SEq SEq SEq cid4V cid2 Proof Proposition 3 We prove equation upper bound The proof lower bound similar We know P q e P q eP e P maxq e P maxq eZ Z partition function maximises P maxq e The partition function minimises P minq e P minq e P minq eZ By equivalence transformations substitution Z P maxq eP minq e P minq eP maxq e By Corollary 1 P maxq e P minq e 1 substitute P minq e P maxq e1 P maxq e P minq eP maxq e equivalent equation cid2 Proof Theorem 3 In proof Proposition 1 determine ﬁnitedimensional solution event case ﬁniterelevantconstraints condition ﬁnitedimensionalconstraints condition holds The bounds computed Proposition 2 The equations Proposition 2 require summing elements Ck This k chosen variables restricted solution event included A ﬁnite k ﬁnitedimensionalconstraints condition fulﬁlled We ﬁnally decide ﬁnite time event contained element ﬁnitedimensional credal set speciﬁcation disjoint subset solution event disjointeventsdecidability condition cid2 Proof Lemma 4 Follows directly fact ﬁnite credal set speciﬁcations compatible Lemma 3 cid2 Proof Proposition 4 The proof variation proof Proposition 2 Disjointness events corresponds unsatis ﬁability corresponding constraints conjunction event subset corresponds satisﬁability implication ϕ1 ϕ2 equivalent unsatisﬁability ϕ1 ϕ2 cid2 Proof Corollary 2 This follows directly Deﬁnition 12 Proposition 4 cid2 Proof Corollary 3 This follows directly Theorem 3 disjointeventsdecidability condition corresponds decidable satisﬁability constraints shown Proposition 4 cid2 Proof Theorem 4 We focus problem deciding P ϕ p probability 0 p 1 To prove member ship PPC decided Probabilistic Turing Machine M polynomial time given oracle checking satisﬁability constraints M computes joint probability choices constraints ϕ First M iterates variable Vi choosing particular constraint variable conform probability mass associated constraint Each computation path corresponds speciﬁc choice cid12 P Then M computes checkcid12 ϕ unsat p cid10 The probability If true state accepted probability 1 2 entering accepting state P ϕ 1 P ϕ cid10 p cid10 Now 2 probability arriving accepting state strictly larger 1 1 p cid10 probability 1 2 1 pcid10 1 P ϕ 1 2 p cid10 1 2 2 P ϕ p cid2 42 S Michels et al Artiﬁcial Intelligence 228 2015 144 Proof Theorem 5 We assume given tree decomposition treewidth t use determine variable order Suppose start arbitrary node s tree case distinctions variables node The number resulting CNFs bounded dt size node bounded t random variables d choices Furthermore number choices incorporated CNFs bounded t The complexity computation O t dt consider recursive calls OGWMC decomposed subCNFs The reason subCNFs different exploit caching Line 3 Algorithm 2 number distinct subCNFs bounded All subCNFs passed recursive calls include variables s Either eliminated simpliﬁcation included imprecise constraint In case disjunction imprecise constraints included choices random variables included optimisation Lines 68 Algorithm 2 applied For subCNF passed recursive new decomposition tree assigned decomposition applied Line 13 algorithm We eliminate s variables s tree number disconnected subtrees To subCNF exactly subtree assigned random variables included subCNF subtree The subtrees share variables Property 3 Deﬁnition 15 random variable remaining passed subCNFs present subtree Property 1 Deﬁnition 15 This means random variable present exactly subtree Furthermore subCNF exactly tree decomposition including variables subCNF Property 2 Deﬁnition 15 deﬁnition constraint primal graph Deﬁnition 14 Because apply caching number consecutive computations bounded number different inputs algorithm We consider number distinct CNFs variables occurring choices removed discussed For subtrees number distinct corresponding CNFs bounded dm m number variables subtree node n chosen previous step common If CNFs use node subtree originally connected n t m variables remaining Therefore complexity single CNF O t dtm dm different CNFs means complexity CNFs associated subtree O t dt This repeated m tree nodes eliminated At computation terminates Lines 4 5 16 algorithm We conclude total complexity O m t dt cid2 References 1 L Getoor B Taskar Eds An Introduction Statistical Relational Learning MIT Press 2007 2 A Kimmig F Costa Link node prediction metabolic networks probabilistic logic Bisociative Knowledge Discovery Springer 2012 pp 407426 3 B Moldovan P Moreno M van Otterlo J SantosVictor L De Raedt Learning relational affordance models robots multiobject manipulation tasks 2012 IEEE International Conference Robotics Automation ICRA IEEE 2012 pp 43734378 4 S Michels M Velikova A Hommersom PJ Lucas A decision support model uncertainty reasoning safety security tasks 2013 IEEE International Conference Systems Man Cybernetics SMC IEEE 2013 pp 663668 5 T Sato A statistical learning method logic programs distribution semantics ICLP 1995 pp 715729 6 LD Raedt A Kimmig H Toivonen ProbLog probabilistic prolog application link discovery Proc 20th International Joint Conference Artiﬁcial Intelligence AAAI Press 2007 pp 24682473 7 T Sato Y Kameya PRISM language symbolicstatistical modeling Proc 15th International Joint Conference Artiﬁcial Intelligence 1997 pp 13301335 Program 9 2009 245308 8 D Poole The independent choice logic L De Raedt P Frasconi K Kersting S Muggleton Eds Probabilistic Inductive Logic Program ming SpringerVerlag Berlin Heidelberg 2008 pp 222243 httpdlacmorgcitationcfmid17939561793966 9 J Vennekens M Denecker M Bruynooghe CPlogic language causal probabilistic events relation logic programming Theory Pract Log 11 2011 663680 ACM 1987 pp 111119 IJCAIAAAI 2013 10 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann 1988 11 M Richardson P Domingos Markov logic networks Mach Learn 62 12 2006 107136 12 J Wang P Domingos Hybrid Markov logic networks Proceedings 23rd National Conference Artiﬁcial Intelligence vol 2 AAAI08 AAAI Press 2008 pp 11061111 httpdlacmorgcitationcfmid16201631620244 13 B Gutmann M Jaeger L De Raedt Extending ProbLog continuous distributions P Frasconi FA Lisi Eds Proc 20th International Conference Inductive Logic Programming ILP10 Firenze Italy 2010 14 B Gutmann I Thon A Kimmig M Bruynooghe LD Raedt The magic logical inference probabilistic programming Theory Pract Log Program 15 J Jaffar J Lassez Constraint logic programming Proceedings 14th ACM SIGACTSIGPLAN Symposium Principles Programming Languages 16 N Nilsson Probabilistic logic Artif Intell 28 1 1986 7187 17 S Michels A Hommersom PJF Lucas M Velikova PWM Koopman Inference new probabilistic constraint logic F Rossi Ed IJCAI 18 M Chavira A Darwiche On probabilistic inference weighted model counting Artif Intell 172 67 2008 772799 19 D Fierens G Van den Broeck I Thon B Gutmann L De Raedt Inference probabilistic logic programs weighted CNFs F Gagliardi Cozman A Pfeffer Eds Proceedings 27th Conference Uncertainty Artiﬁcial Intelligence UAI 1417 July 2011 Barcelona Spain 2011 pp 211220 20 J Binder D Koller S Russell K Kanazawa P Smyth Adaptive probabilistic networks hidden variables Mach Learn 1997 213244 21 R Valdez P Yoon T Liu M Khoury Family history prevalence diabetes population 6year results national health nutrition examination survey 19992004 Diabetes Care 30 10 2007 25172522 22 J Lloyd Foundations Logic Programming 2nd edition Springer 1987 23 E Grädel On transitive closure logic Computer Science Logic 5th Workshop CSL91 vol 626 Springer 1992 pp 149163 24 KR Apt HA Blair A Walker Towards theory declarative knowledge IBM TJ Watson Research Center 1986 S Michels et al Artiﬁcial Intelligence 228 2015 144 43 25 A Van Gelder KA Ross JS Schlipf The wellfounded semantics general logic programs J ACM 38 3 1991 619649 26 M Gelfond V Lifschitz The stable model semantics logic programming ICLPSLP vol 88 1988 pp 10701080 27 AN Kolmogorov Foundations Theory Probability 1960 28 P Walley Towards uniﬁed theory imprecise probability Int J Approx Reason 24 2 2000 125148 29 I Levi The Enterprise Knowledge 1980 30 J Neveu Mathematical Foundations Calculus Probability HoldenDay San Francisco 1965 31 T Sato Y Kameya New advances logicbased probabilistic modeling prism Probabilistic Inductive Logic Programming Springer 2008 pp 118155 32 K Weichselberger T Augustin On symbiosis concepts conditional interval probability ISIPTA vol 3 2003 pp 608629 33 CH Papadimitriou On complexity integer programming J ACM 28 4 1981 765768 34 JH Davenport J Heintz Real quantiﬁer elimination doubly exponential J Symb Comput 5 1 1988 2935 35 PV Hentenryck Constraint Satisfaction Logic Programming The MIT Press 1989 36 J Jaffar S Michaylov PJ Stuckey RHC Yap The CLPR language ACM Trans Program Lang Syst 14 3 1992 339395 httpdxdoiorg 101145129393129398 37 J Jaffar MJ Maher K Marriott PJ Stuckey The semantics constraint logic programs J Funct Logic Program 37 13 1998 146 httpdxdoiorg 101016S074310669810002X 38 T Janhunen Representing normal programs clauses ECAI vol 16 2004 p 358 39 T Mantadelis G Janssens Dedicated tabling probabilistic setting ICLP Technical Communications 2010 pp 124133 40 D Poole NL Zhang Exploiting contextual independence probabilistic inference J Artif Intell Res 18 2003 263313 41 A Darwiche Recursive conditioning Artif Intell 126 1 2001 541 42 F Jensen S Anderson Approximations Bayesian belief universe knowledge based systems Proceedings Sixth Conference Annual Conference Uncertainty Artiﬁcial Intelligence UAI90 AUAI Press Corvallis Oregon 1990 pp 162169 43 C Boutilier N Friedman M Goldszmidt D Koller Contextspeciﬁc independence Bayesian networks Proceedings Twelfth International Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers Inc 1996 pp 115123 44 RJ Bayardo JD Pehoushek Counting models connected components AAAIIAAI 2000 pp 157162 45 T Sang P Beame H Kautz Heuristics fast exact model counting Theory Applications Satisﬁability Testing Springer 2005 pp 226240 46 L De Moura N Bjørner Z3 eﬃcient SMT solver Proceedings Theory Practice Software 14th International Conference Tools Algorithms Construction Analysis Systems TACAS08ETAPS08 SpringerVerlag Berlin Heidelberg 2008 pp 337340 httpdl acmorgcitationcfmid17927341792766 47 B Dutertre LD Moura The Yices SMT solver Tech rep Computer Science Laboratory SRI International 2006 48 CP De Campos FG Cozman The inferential complexity Bayesian credal networks IJCAI vol 5 2005 pp 13131318 49 PJ Downey R Sethi RE Tarjan Variations common subexpression problem J ACM 27 4 1980 758771 50 N Karmarkar A new polynomialtime algorithm linear programming Proceedings Sixteenth Annual ACM Symposium Theory Computing ACM 1984 pp 302311 Holland Publishing Company 1971 pp 171177 51 YV Matiyasevich Diophantine representation recursively enumerable predicates J Fenstad Ed Second Scandinavian Logic Symposium North 52 HL Bodlaender A tourist guide treewidth Technical report RUUCS 92 1992 53 M Samer S Szeider Algorithms propositional model counting J Discrete Algorithms 8 1 2010 5064 54 R Diestel Graph Theory 3rd edition Grad Texts Math vol 173 SpringerVerlag Berlin 2005 55 S Arnborg DG Corneil A Proskurowski Complexity ﬁnding embeddings aktree SIAM J Algebr Discrete Methods 8 2 1987 277284 56 HL Bodlaender A linear time algorithm ﬁnding treedecompositions small treewidth Proceedings TwentyFifth Annual ACM Sympo sium Theory Computing ACM 1993 pp 226234 Intelligence AAAI05 vol 1 2005 pp 475482 57 T Sang P Beame H Kautz Solving Bayesian networks weighted model counting Proceedings Twentieth National Conference Artiﬁcial 58 D Fierens G Van den Broeck J Renkens D Shterionov B Gutmann I Thon G Janssens L De Raedt Inference learning probabilistic logic programs weighted Boolean formulas Theory Pract Log Program 2014 144 First view 59 A Darwiche New advances compiling CNF decomposable negation normal form Proc ECAI Citeseer 2004 pp 328332 60 C Muise SA Mcilraith JC Beck E Hsu DSHARP fast dDNNF compilation sharpSAT Canadian Conference Artiﬁcial Intelligence 2012 61 K Laskey MEBN language ﬁrstorder Bayesian knowledge bases Artif Intell 172 23 2008 140178 62 A McCallum K Schultz S Singh Factorie probabilistic programming imperatively deﬁned factor graphs Advances Neural Information Processing Systems 2009 pp 12491257 63 A Pfeffer Figaro objectoriented probabilistic programming language Technical report Charles River Analytics 2009 p 137 64 N Goodman V Mansinghka D Roy K Bonawitz J Tenenbaum Church language generative models Proceedings 24th Conference Uncertainty Artiﬁcial Intelligence UAI 2008 2008 pp 220229 65 B Milch B Marthi SJ Russell D Sontag DL Ong A Kolobov BLOG probabilistic models unknown objects Proceedings Nineteenth In ternational Joint Conference Artiﬁcial Intelligence IJCAI05 July 30August 5 2005 Edinburgh Scotland UK 2005 pp 13521359 httpwwwijcai orgpapers1546pdf 66 SL Lauritzen Propagation probabilities means variances mixed graphical association models J Am Stat Assoc 87 420 1992 10981108 67 FG Cozman Credal networks Artif Intell 120 2 2000 199233 68 R Ng VS Subrahmanian Probabilistic logic programming Inf Comput 101 2 1992 150201 69 S Michels A Hommersom PJF Lucas M Velikova Imprecise probabilistic horn clause logic ECAI 2014 21st European Conference Artiﬁcial Intelligence Including Prestigious Applications Intelligent Systems PAIS 2014 1822 August 2014 Prague Czech Republic 2014 pp 621626 70 P Domingos S Kok D Lowd H Poon M Richardson P Singla Markov logic Probabilistic Inductive Logic Programming 2008 pp 92117 71 VS Costa D Page M Qazi J Cussens CLP BN constraint logic programming probabilistic knowledge Proceedings Nineteenth Confer ence Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers Inc 2002 pp 517524 72 VS Costa A Paes On relationship PRISM CLPBN Proc Int Workshop Statistical Relational Learning SRL2009 2009 73 N Angelopoulos clppdfy constraints probabilistic reasoning logic programming F Rossi Ed Principles Practice Constraint Pro gramming 2003 Lect Notes Comput Sci vol 2833 Springer Berlin Heidelberg 2003 pp 784788 74 S Reizler Probabilistic constraint logic programming PhD thesis University Tubingen Tubingen Germany 1998 75 T Sato M Ishihata K Inoue Constraintbased probabilistic modeling statistical abduction Mach Learn 83 2 2011 241264 76 L van der Gaag On probability intervals updating Technical report Department Computer Science University Utrecht 1990 http booksgooglenlbooksidnH9JGwAACAAJ 77 FG Cozman CP De Campos JS Ide JCF da Rocha Propositional relational Bayesian networks associated imprecise qualitative proba bilistic assessments Proceedings 20th Conference Uncertainty Artiﬁcial Intelligence AUAI Press 2004 pp 104111 44 S Michels et al Artiﬁcial Intelligence 228 2015 144 78 MA Islam CR Ramakrishnan IV Ramakrishnan Inference probabilistic logic programs continuous random variables Theory Pract Log Program 12 45 2012 505523 79 H Langseth TD Nielsen A Salmerón et al Mixtures truncated basis functions Int J Approx Reason 53 2 2012 212227 80 S Sanner E Abbasnejad Symbolic variable elimination discrete continuous graphical models AAAI 2012 81 R Dechter Reasoning probabilistic deterministic graphical models exact algorithms Synth Lect Artif Intell Mach Learn 7 3 2013 1191 82 KP Murphy Y Weiss MI Jordan Loopy belief propagation approximate inference empirical study Proceedings Fifteenth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers Inc 1999 pp 467475 83 JS Yedidia WT Freeman Y Weiss Constructing freeenergy approximations generalized belief propagation algorithms IEEE Trans Inf Theory 51 7 2005 22822312 84 SC Tatikonda MI Jordan Loopy belief propagation Gibbs measures Proceedings Eighteenth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers Inc 2002 pp 493500 85 JM Mooij HJ Kappen Suﬃcient conditions convergence loopy belief propagation Proceedings 21st Conference Uncertainty Artiﬁcial Intelligence UAI 05 July 2629 2005 Edinburgh Scotland 2005 pp 396403 86 D Poole Logic programming abduction probability New Gener Comput 11 34 1993 377400 87 J Renkens A Kimmig G Van den Broeck L De Raedt Explanationbased approximate weighted model counting probabilistic logics Proceedings 28th AAAI Conference Artiﬁcial Intelligence 2014 88 S Kullback RA Leibler On information suﬃciency Ann Math Stat 22 1 1951 7986 89 NS Arora R Salvo Braz EB Sudderth SJ Russell Gibbs sampling openuniverse stochastic languages Proceedings TwentySixth Conference Uncertainty Artiﬁcial Intelligence UAI 2010 July 811 2010 Catalina Island CA USA 2010 pp 3039 90 D Nitti TD Laet LD Raedt A particle ﬁlter hybrid relational domains 2013 IEEERSJ International Conference Intelligent Robots Systems November 37 2013 Tokyo Japan 2013 pp 27642771 91 MD Hoffman A Gelman The nouturn sampler adaptively setting path lengths Hamiltonian Monte Carlo J Mach Learn Res 15 1 2014 15931623 httpdlacmorgcitationcfmid2638586 Artiﬁcial Intelligence Statistics 2013 pp 397405 1996 223252 92 L Li B Ramsundar S Russell Dynamic scaled sampling deterministic constraints Proceedings Sixteenth International Conference 93 JG Propp DB Wilson Exact sampling coupled Markov chains applications statistical mechanics Random Struct Algorithms 9 12 94 S Brooks A Gelman G Jones XL Meng Handbook Markov Chain Monte Carlo CRC Press 2011 95 K Kersting Lifted probabilistic inference ECAI 2012 pp 3338 96 J Choi E Amir DJ Hill Lifted inference relational continuous models UAI vol 10 2010 pp 126134 97 T Hendriks P van Laar Metis dependable cooperative systems public safety Proc Comput Sci 16 2013 542551 98 M Velikova P Novák B Huijbrechts J Laarhuis J Hoeksma S Michels An integrated reconﬁgurable maritime situational awareness ECAI 2014 21st European Conference Artiﬁcial Intelligence Including Prestigious Applications Intelligent Systems PAIS 2014 1822 August 2014 Prague Czech Republic 2014 pp 11971202