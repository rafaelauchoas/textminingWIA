Artiﬁcial Intelligence 219 2015 6791 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Integrating representation learning skill learning humanlike intelligent agent Nan Li Noboru Matsuda William W Cohen Kenneth R Koedinger 5000 Forbes Ave Pittsburgh PA 15232 USA r t c l e n f o b s t r c t Article history Received 18 April 2012 Received revised form 2 July 2014 Accepted 5 November 2014 Available online 4 December 2014 Keywords Agent learning Representation learning Student modeling Building intelligent agent simulates human learning math science potentially beneﬁt cognitive science contributing understanding human learning artiﬁcial intelligence advancing goal creating human level intelligence However constructing learning agent currently requires manual encoding prior domain knowledge addition poor model human acquisition prior knowledge manual knowledgeencoding timeconsuming errorprone Previous research shown key factors differentiates experts novices different representations knowledge Experts view world terms deep functional features novices view terms shallow perceptual features Moreover performance learning algorithms sensitive representation deep features important achieving effective machine learning In paper present eﬃcient algorithm acquires representation knowledge form deep features demonstrate effectiveness domain algebra synthetic domains We integrate algorithm machine learning agent SimStudent learns procedural knowledge observing tutor solve sample problems getting feedback actively solving problems We learning deep features reduces requirements knowledge engineering Moreover propose approach automatically discovers student models extended SimStudent By ﬁtting discovered model real student learning curve data better student model humangenerated models demonstrate discovered model improve tutoring systems instructional strategy 2014 Elsevier BV All rights reserved 1 Introduction One fundamental goals artiﬁcial intelligence understand develop intelligent agents simulate humanlike intelligence A considerable effort 13 challenging task Further education 21st century increasingly helping students learn content better learners Thus second goal improving understanding humans acquire knowledge students vary abilities learn Corresponding author Email addresses nli1cscmuedu N Li NoboruMatsudacscmuedu N Matsuda wcohencscmuedu WW Cohen koedingercmuedu KR Koedinger httpdxdoiorg101016jartint201411002 00043702 2014 Elsevier BV All rights reserved 68 N Li et al Artiﬁcial Intelligence 219 2015 6791 To contribute goals recent efforts 47 developing intelligent agents model human learning math science second language Although agents produce intelligent behavior human knowl edge engineering remains nontrivial element knowledge engineering encoding prior domain knowledge given simulated student agent start learning process For example build algebra learning agent agent developer needs provide prior knowledge coding functions instance extract coeﬃcient add algebraic terms Such manual encoding prior knowledge timeconsuming constructed prior knowledge naturally correspond human students prior knowledge Since real students entering course usually substantial domainspeciﬁc domainrelevant prior knowl edge realistic model human learning assume knowledge given learned For example students learning algebra assume know coeﬃcient dif ference variable term constant term An intelligent models automatic knowledge acquisition small prior knowledge helpful reducing effort knowledge engineering intelligent systems advancing cognitive science human learning Previous work cognitive science 89 showed key factors differentiates experts novices ﬁeld different prior knowledge world state representation Experts view world terms deep functional fea tures coeﬃcient constant algebra novices view terms shallow perceptual features integer expression Deep features domainspeciﬁc shallow perceptual features domainindependent Having correct representation deep features aids process solving domain task For example algebra students need learn encode equation input terms coeﬃcients A shallow feature encoding coeﬃcient 5 5x number letter A deep feature encoding requires learner develop knowl edge include implicit perceptual processing capabilities recognize coeﬃcients generally 5 5x ax 3 3x2 1 x In general experts develop deep feature knowledge allows world way novices expert readers run word novices letters lines experts physics force contact points novices blocks inclined planes chess experts conﬁgurations pieces like knight fork novices pieces Such deep feature perception knowledge learned speciﬁc domains implicit experience explicit instruction In algebra students learn terms coeﬃcients equations building general prior knowledge numbers That prior knowledge basis initial shallow feature encoding example In general consider deep features rep resentation knowledge Representation knowledge organizes lowlevel perceptual input structured form assists agent understand solve problems particular domain Even perceptual input given dif ferent problem solving tasks different domains ideal representation world different different tasks Deep features viewed key features differentiate wellstructured representation poorlystructured representation Deep feature learning major component human expertise acquisition received attention AI Learning deep features changes representation future learning based improves future learning However deep features acquired clear Therefore recently developed learning algorithm acquires deep features automatically domainindependent knowledge integer input 10 We evaluated effectiveness algorithm learning deep features impact future skill learner In order evaluate deep feature learner affect future learning intelligent agent paper integrated deep feature learning algorithm SimStudent 11 agent learns problemsolving skills example feedback performance The original SimStudent relies handengineered representation encodes expert representation given prior knowledge This limits ability model novice students The extended SimStudent ﬁrst acquires representation problems deep feature learner Then makes use learned representation acquire skill knowledge later tasks Integrating deep feature learner original SimStudent reduces engineering effort builds better model student learning We extended SimStudent better representation learning performs better original Sim Student given domainspeciﬁc knowledge Furthermore compared original SimStudent domainspeciﬁc knowledge extended SimStudent able learn nearly given domainspeciﬁc knowledge For sake simplicity report experiment results algebra domain paper similar results observed domains 12 In addition use extended SimStudent automatically discover models real students discovered models better student models humangenerated models 13 Although reported use extended SimStudent better understand problem orders affect learning effectiveness inspecting SimStudents learning processes learning outcomes easily obtainable human subjects 14 To summarize main contributions paper twofold By integrating representation learning skill learn ing 1 reduce knowledge engineering effort required constructing intelligent agent 2 better model human behavior In following sections start brief review SimStudent We present deep feature learning algo rithm evaluation results Next integrate deep feature learner SimStudent illustrate algorithm example algebra After present experimental results original Sim N Li et al Artiﬁcial Intelligence 219 2015 6791 69 Fig 1 The interface SimStudent tutored equation solving domain The given problem 3x 2 8 SimStudent tutored subtract sides 2 After entering divide 3 SimStudent asks author usertutor step correct If SimStudent learned applicable production rule asks author demonstrate good step learns production rule reproduce steps like Student extended SimStudent trained problem sets real students learning algebra extended SimStudent able achieve performance comparable original SimStudent requiring domain speciﬁc knowledge input We present method SimStudent automatically discover student models student model discovered extended SimStudent better humangenerated models Finally discuss related work limitations possible future extensions 2 A brief review SimStudent SimStudent intelligent agent inductively learns skills solve problems demonstrated solutions problem solving experience It extension programming demonstration 16 inductive logic program ming 17 underlying learning techniques SimStudent theory learning academic domains makes following assumption 1 Problemsolving skills modeled conditionresponse patterns Much academic learning expertise development involves skill acquisition represented production rules schemas indicate mental external response howpart information needed wherepart preconditions met whenpart hierarchical representations perceived world 2 Multiple learning mechanisms skill learning Effective learning skills requires kinds generalizations 3 Skill learning perceptual grounding bias How input world represented perceptual abstraction hierarchy essential inductive bias skill learning This learning bias produces real differences learning success failure engineering perspective produces real differences quality predictions human academic learning scientiﬁc perspective 4 Representation learning modeled unsupervised induction perceptual structures An abstract perceptual representation acquired unsupervised way spatial temporal proximity information create chunks Later paper present approach implementing fourth tenet grammar induction SimStudent similar ACTR 18 Soar 19 theories learning Like theories SimStudent represents product learning production Unlike theories puts emphasis knowledgelevel learning cf 20 achieved induction positive negative examples SimStudent problemsolving agent theory learning academic domains emphasis learning skills representational basis As Soar ACTR theory concrete software architecture applied model learning problem solving skilled performance variety domains including math science second language 1221 Compared SOAR ACTR models created SimStudent models created cover new territory demonstrating knowledgelevel learning achieved Soar ACTR speciﬁcally domainspeciﬁc prior knowledge math science language skills Figs 1 2 screenshots SimStudent learning solve algebra equations Fig 1 interface teach SimStudent equation solving Fig 2 shows SimStudent keeps track demonstrated steps acquires skill knowledge based In paper use equation solving illustrative domain explain learning mechanisms However learning algorithms domain general In fact SimStudent tested domains including multicolumn addition fraction addition stoichiometry In rest section brieﬂy review learning mechanism SimStudent For details refer 6 70 N Li et al Artiﬁcial Intelligence 219 2015 6791 Fig 2 CTATs 15 behavior recorder SimStudent traces demonstrated step Each entry GUI element traced In example state changes steps entries table cells shown Fig 1 These traced existing production rule learn new production rule The conﬂict tree panel ﬁgure shows SimStudent applied skill divide incorrectly dividing sides 3 instead 3 21 Learning task SimStudent given set ideally simple feature predicates set ideally simple operator functions prior knowledge learning Each feature predicate boolean function describes relations objects main For example hascoeﬃcient 3x means 3x coeﬃcient Operator functions specify basic functions add numbers coeﬃcient SimStudent apply aspects problem representation Operator functions divided groups domainindependent operator func tions domainspeciﬁc operator functions Domainindependent operator functions multiple domains tend simpler like standard operations programming language Examples operator functions include adding numbers add 1 2 copying string copy 3x These operator functions useful solving equa tions domains multicolumn addition fraction addition Because domain general functions involved domains acquired algebra assume real students know prior algebra instruction Because domain general functions multiple domains potential engineering beneﬁt reducing eliminating need write new operator functions applying SimStudent new domain Domainspeciﬁc operator functions hand complicated functions getting coeﬃ cient term coeﬃcient 3x adding terms addterm 5x8 2x3 Performing operator functions implies domain expertise real students likely Domainspeciﬁc operator functions tend require knowledge engineering programming effort domain independent operator functions For example compare add domainindependent operator function add term domainspeciﬁc operator function Adding numbers step steps adding terms parsing input terms subterms applying addition strategy term format concatenating subterms Note operator functions different operators traditional planning systems operator functions explicit encoding preconditions produce correct results applied context For example dividing sides 3 divide 3 incorrect step problem 3x25 Thus SimStudent different traditional planning N Li et al Artiﬁcial Intelligence 219 2015 6791 71 Fig 3 A production rule divide algorithms engage speed learning SimStudent engages knowledge level learning 20 inductively acquires complex reasoning rules These rules represented production rules explain later During learning process given current state problem 3x6 SimStudent ﬁrst tries ﬁnd appro priate production rule proposes plan step coeﬃcient 3x coef divide coef If ﬁnds receives positive feedback continues step If proposed step incorrect negative feedback given SimStudent alternatives correct step demonstration provided SimStudent attempt mod ify learn production rules accordingly Although feedback mechanisms possible case feedback given existing automatic cognitive tutor CTAT 15 teach real students For demonstrated step tutor speciﬁes 1 perceptual information 3x 6 3x6 graphical user interface GUI showing ﬁnd information perform step 2 skill label divide corresponding type skill applied 3 step divide 3 problem 3x6 This simulates limited information available real students Taken gether pieces information form example action record indexed skill label R cid3label cid3percepts stepcid4cid4 In algebra example example action record R cid3divide cid33x 6 divide 3cid4cid4 For incorrect step proposed SimStudent example action record generated negative example During learning SimStudent typically ac quires production rule skill label l based set associated positive negative example action records gathered current step Rl R1 R2 Rn R ilabel l 1 2 n In summary like model real students tutored learning task presented SimStudent challenging First total number world states large In equation solving instance inﬁnite variety algebraic expressions entered possible alternative solution strategies Second operator functions given prior knowledge encode preconditions applicability search control postconditions Last semantics demonstrated step partially observable It usually takes operator function observed state observed state Correct intermediate outputs operator functions unobservable SimStudent Taken learning task SimStudent facing learning skill knowledge inﬁnite world states given incomplete operator function descriptions partially observable states 22 Production rules The output learning agent represented production rules 12 The left Fig 3 shows example production rule learned SimStudent simple English description shown right A production rule indicates look information interface perceptual information change problem state operator function sequence precondition apply rule set features indicting circumstances performing howpart useful For example rule divide sides 3x6 3 shown Fig 3 read given lefthand 3x righthand 6 equation lefthand 72 N Li et al Artiﬁcial Intelligence 219 2015 6791 constant term coeﬃcient term lefthand 3 write divide followed coeﬃcient divide 3 The perceptual information represents paths identify useful information GUI During execution Sim Student matches perceptual hierarchy acquired production rules With properlystructured perceptual hierarchy SimStudent able quickly focus attention essential information solving problem As discuss later extension proposed paper SimStudent able update perceptual hierarchy based inputs environment aids learning process The precondition Fig 3 includes set feature tests representing desired conditions apply production rule The Fig 3 operator function sequence computes output GUI 23 Learning mechanisms With challenges presented developed learning mechanisms SimStudent acquire parts production rules 6 The ﬁrst component perceptual learner learns wherepart production rule ﬁnding paths identify useful information GUI The elements interface typically organized tree structure The left Fig 10 shows example perceptual hierarchy algebra domain For example table node columns children column multiple cells children The percepts speciﬁed production rule cells associated sides algebra equation Cell 11 Cell 21 case Hence perceptual learners task ﬁnd right paths tree reach speciﬁed cell nodes There ways reach percept node interface 1 exact path exact position tree 2 generalized path set GUI elements speciﬁc relationship GUI element step entered cells step A generalized path levels tree bound node For example cell second column row Cell 23 generalized cell second column Cell 2 cell table Cell In example shown Fig 3 production rule overspeciﬁc wherepart produces step sides current step row SimStudent assumes example action records skill ﬁxed number percepts Therefore positive example action record associated skill label l R Rl percept ﬁeld R ipercepts mdimensional vector R ipercepts perceptsi1 perceptsi2perceptsim perceptsi j stands jth percept ith example action record Each percept vector perceptsi j GUI element1 The set percepts positive examples form n m matrix P R1percepts R2percepts RnperceptsT row Pi percepts ﬁeld example action record column P j composed percepts index example action records For position j set paths path reach percepts P j deﬁnes version space V j 22 subset hypotheses consistent observed training examples The learner searches general path version space V j This process bruteforce depthﬁrst search For example given example 3x6 row production rule learned shown Fig 3 overspeciﬁc wherepart If given examples rows 4x12 row wherepart generalized row table The second learning mechanism feature test learner learns whenpart production rule acquiring precondition production rule given feature predicates The acquired preconditions contain information applicability getting coeﬃcient applicable term 3x5 search control preferred add 5 sides problem 3x6 The feature test learner utilizes FOIL 23 inductive logic programming learns Horn clauses positive negative examples expressed relations FOIL acquire set feature tests desired situation ﬁre production rule For rule feature test learner creates new predicate corresponds precondition rule sets target relation FOIL learn The arguments new predicate associated percepts Each training action record serves positive negative example FOIL The task FOIL ﬁnd set feature tests best separate positive examples negative examples For example precondition divide percept1 percept2 precondition predicate associated production rule named divide preconditiondivide 3x 6 positive example The feature test learner computes truthfulness predicates bound possible permutations percept values sends input FOIL Given inputs FOIL acquire set clauses formed feature predicates describing precondition predicate The component operator function sequence learner acquires howpart production rule For positive example action record R learner takes percepts R ipercepts input sets step R istep output We operator function sequence explains perceptsstep pair cid3R ipercepts R istepcid4 takes R ipercepts input yields stepi applying operator functions For example SimStudent ﬁrst ceives perceptsstep pair cid32x 2 divide2cid4 operator function sequence directly divides sides righthand divide rhs sequence ﬁrst gets coeﬃcient divides sides coeﬃcient coeﬃcient lhs coef divide coef possible explanations given pair Since multiple example action records skill suﬃcient ﬁnd operator function sequence example action 1 In case equation solving domain percepts associated cells learning algorithm limited cells N Li et al Artiﬁcial Intelligence 219 2015 6791 73 Table 1 Probabilistic context free grammar coeﬃcient algebra Terminal symbols x Nonterminal symbols Expression SignedNumber Variable MinusSign Number Expression 10 SignedNumberVariable Variable 10 x SignedNumber 05 MinusSign Number SignedNumber 05 Number MinusSign 10 record Instead learner attempts ﬁnd shortest operator function sequence explains cid3percepts stepcid4 pairs iterativedeepening depthﬁrst search depthlimit As example divide rhs shorter coeﬃcient lhs coef divide coef SimStudent learn operator function sequence howpart Later meets example 3x6 receives perceptsstep pair cid33x 6 divide 3cid4 The operator func tion sequence divides sides righthand possible explanation Hence SimStudent modiﬁes howpart longer operator function sequence coeﬃcient lhs coef divide coef Last said SimStudent tries learn rule label new training action record added SimStudent fail learn single rule example action records In case SimStudent learns separate rule example action record More speciﬁcally SimStudent starts given set skill labels associated demonstrated steps SimStudent tries learn rule label It fail perceptual information learner ﬁnd path covers demonstrated steps operator sequence learner ﬁnd operator function sequence explains records In case SimStudent learns disjunctive rule record This effectively splits examples clusters Later new record SimStudent tries acquire rule clusters new record stops successfully learns rule clusters If new record added existing clusters SimStudent creates new cluster With approach number clusters increases records seen There merge operation clusters 3 Deep feature learning Having reviewed SimStudents production rule learning mechanisms discussion representation knowl edge acquisition deep feature learning As mentioned deep feature learning important human knowl edge acquisition achieving effective machine learning We carefully examined nature deep feature learning algebra equation solving discovered modeled unsupervised grammar induction problem given observational data expressions algebra Expressions formulated context free grammar CFG deep features modeled grammar nonterminal symbols particular positions grammar rule Table 1 illustrates por tion grammar algebra expressions modeling deep feature coeﬃcient nonterminal symbol grammar rules indicated square brackets SignedNumber Viewing feature learning tasks grammar induction provides general explanation experts acquire perceptual chunks 924 speciﬁc explanations novice errors In account novice errors result acquiring wrong grammar task domain Let use 3x example The correct grammar shown Table 1 produces correct parse tree shown left Fig 4 A novice acquire different grammar rules plausible lack experience negative numbers result incorrect parse tree shown right Fig 4 Instead grouping 3 grammar groups 3 x ﬁrst mistakenly considers 3 coeﬃcient In fact common strategic error students problem like 3x12 student divide sides 3 3 13 Based observations like built deep feature learner extending existing probabilistic context free grammar pCFG learner 25 support feature learning transfer learning Note deep feature learner domain general It currently supports domains student input represented string tokens modeled contextfree grammar algebra chemistry natural language processing Fig 4 Correct incorrect parse trees 3x 74 N Li et al Artiﬁcial Intelligence 219 2015 6791 Algorithm 1 GSH constructs initial set grammar rules S observation sequences O Input Observation Sequence Set O 1 S terminal symbol grammar rules 2 notallsequencesareparsableO S 3 hasrecursiveruleO 4 5 6 7 8 9 s generaterecursiverule O s generatemostfrequentruleO end S S s O updateplansetwithruleO S 10 end 11 S initializeprobabilitiesS 12 return S 31 A brief review pCFG learner Before introducing deep feature acquisition algorithm ﬁrst brieﬂy review pCFG learner 25 based The pCFG learner variant insideoutside algorithm 26 acquires probabilistic contextfree grammar pCFG expectationmaximization EM algorithm 27 The input pCFG learner set observation sequences O Each sequence string tokens directly user input The output pCFG generate input observation sequences high probabilities The consists parts greedy structure hypothesizer GSH creates nonterminal symbols associated grammar rules needed cover training examples Viterbi training step iteratively reﬁnes probabilities grammar rules 311 Greedy structure hypothesizer GSH Pseudo code GSH algorithm shown Algorithm 1 GSH creates xcontextfree grammar bottomup fashion It starts initializing rule set S rules associated terminal characters 3 x 3x observation sequences O Next algorithm line 4 detects possible recursive structures embedded observation sequence looking repeated symbols If algorithm learns recursive rule If algorithm fails ﬁnd recursive structures starts search character pair appears plans frequently line 6 constructs grammar rule character pair To build nonrecursive rule algorithm introduce new symbol set head new rule After getting new rule updates current observation set O rule replacing character pairs observations head rule line 9 After learning grammar rules structure learning algorithm assigns initial probabilities rules If k grammar rules head symbol assigned probability 1 k To break ties grammar rules head GSH adds small random number probability normalizes values This output GSH redundant set grammar rules sent Viterbi training phase 312 Reﬁning schema probabilities Viterbi training phase The probabilities associated initial set rules generated GSH phase tuned Viterbi training algorithm It considers parse tree associated observation sequence hidden variables Each iteration involves steps Algorithm 2 shows pseudo code Viterbi training phase In ﬁrst step algorithm computes probable parse tree observation example current rules Any subtree probable parse tree probable parse subtree Therefore observation sequence algorithm builds probable parse tree bottomup fashion reaching start symbol After getting parse trees observation examples algorithm moves second step In step algorithm updates selection probabilities associated grammar rules For grammar rule head ai new probability chosen simply total number times schema appears Viterbi parse trees divided total number times ai appears parse trees In comparison expectationmaximization 27 Viterbi training considered hard EM learning algorithm probable parse tree viewed parse tree observation sequence In contrast typical EM learning procedure possible parse trees updating grammar rule probabilities Therefore learning procedure fast approximation expectationmaximization approximates posterior distribution trees given parameters single MAP hypothesis After ﬁnishing second step algorithm starts new iteration convergence The output algorithm set probabilistic grammar rules As explain Subsection 332 trained prior tasks Viterbi training phase learning algorithm estimates rule frequency Dirichlet distribution N Li et al Artiﬁcial Intelligence 219 2015 6791 75 Algorithm 2 Viterbitraining updates probabilities associated set given grammar rules S obser vation sequences O Input Observation Sequence Set O Grammar Rule Set S 1 notconverged Compute probable parse trees observation sequences T φ forall oi O T T getthemostprobableparsingoi S end Update grammar rule probabilities according parse trees forall si S sel numberoftimesselectedsi T tol numberoftimesselectedsihead T si p sel tol 2 3 4 5 6 7 8 9 10 end 11 end 12 return S 32 Feature learning Having reviewed Li et als 25 pCFG learning algorithm embedding algorithm SimStudent lets ﬁrst extended support deep feature learning The input set pairs cid33x 3cid4 ﬁrst element input feature extraction mechanism ﬁnding coeﬃcient 3x second extraction output 3 coeﬃcient 3x The output pCFG nonterminal symbol rules set target feature shown SignedNumber Table 1 To produce output deep feature learner uses pCFG learner produce grammar searches nonterminal symbols correspond example extraction output 3 3x The process steps The ﬁrst builds parse trees observation sequences based acquired rules For instance algebra suppose acquired pCFG shown Table 1 The associated parse tree 3x shown left Fig 4 Next sequence learner traverses parse tree identify nonterminal symbol associated target feature extraction output rule nonterminal symbol belongs In case example nonterminal symbol SignedNumber associated feature extraction output 3 rule Expression 10 Signed Number Variable For sequences feature extraction output generated single nonterminal symbol happens acquired pCFG right structure For example parse tree shown right Fig 4 incorrect parse 3 nonterminal symbol associated 3 Last records frequency symbol rule pair picks pair matches training records learned feature For instance input records match SignedNumber Expression 10 SignedNumber Variable symbolrule pair considered target feature pattern After learning feature new problem comes ﬁrst build parse tree new problem based acquired grammar Then recognizes subsequence associated feature symbol parse tree returns target feature extraction output 5 5x 33 Transfer learning deep feature learning In order achieve effective learning extended feature learner support transfer learning domain domains Different grammars share grammar rules nonterminal symbols For example grammar equation solving grammar integer arithmetic problems contain subgrammar signed number We extended feature learning algorithm transfer solutions common subgrammars task Note tasks domain learning integer learning coeﬃcient different domains learning integer learning chemical formula We consider learning protocols tutor provides hints shared grammar highlighting subsequences associated nonterminal symbol shared grammar present hints provided For transfer learning subgrammar hints applied feature focus mechanism acquisition process For transfer learning subgrammar hints extended use grammar rule application frequencies previous tasks guide future learning explained 331 Explicitly labeled common subgrammars We ﬁrst consider situation SimStudents tutor provides hint shared subgrammar deep fea ture In original learning algorithm process grammar induction learner acquires grammar generates observation sequences differentiating potential feature subsequences coeﬃcients constant terms subsequences training examples It possible grammars generate set observation sequences grammar appropriate feature symbol embedded We sure original learner learn right 76 N Li et al Artiﬁcial Intelligence 219 2015 6791 However reasonable assume tutor explicitly highlights example subsequences targeted features teacher giving examples coeﬃcient indicating 3 coeﬃcient 3x 4 coeﬃcient 4x With assumption deep feature learner focus creating nonterminal symbols feature subsequences We developed feature focus mechanism follows First copy original learner learn subgrammar deep feature That extract feature subsequences training sequences learn subgrammar We replace feature subsequence special semantic terminal symbol invoke original learner problem Since semantic terminal symbol viewed terminal character phase learning properly embedded observation sequence Finally grammars combined semantic terminal relabeled nonterminal symbol associated start symbol grammar feature 332 Learning transfer common subgrammars hints Aiding transfer learning providing hints common subgrammars requires extra work tutor A powerful learning strategy able transfer knowledge adding work tutor Therefore considered second learning protocol shared grammar present hints provided An appropriate way transferring previously acquired knowledge later learning improve speed accuracy later learning The intuition perceptual chunks grammar acquired wholenumber experience aid grammar acquisition negative numbers turn aid algebra grammar acquisition Our solution involves transferring acquired grammar including application frequency grammar rule previous tasks future tasks More speciﬁcally acquisition grammar previous tasks learner records acquired grammar number times grammar rule appeared parse tree Since previous research algebra problem solv ing pointed computational parsimony important factor quality cognitive model human problem solving 28 faced new task learning algorithm uses existing grammar previous tasks build smallest number probable parse trees new records This process topdown fashion For sequencesubsequence algorithm ﬁrst tries given sequencesubsequence reduced single probable parse tree If succeeds algorithm returns fails algorithm separates sequencesubse quence subsequences recursively calls After building number probable parse trees training subsequences switches original GSH acquires new rules based partially parsed sequences For example grammar learner acquired signed number 3 previous task faced new task learning term 3x learner ﬁrst tries build parse tree term 3x But fails grammar signed number build parse trees subsequence 3 3x Nevertheless grammar learner partially parsed sequences partial reduced sequence 3x SignedNumber x calls original grammar learner partially parsed sequences During Viterbi training phase learning algorithm estimates rule frequency Dirichlet distribution based prior tasks adds applied rule frequency associated training problems current task recorded frequency previous tasks Note possible acquiring new rules new examples Viterbi training phase parse trees training examples previous tasks changed recorded frequencies longer accurate equivalent combining examples old task examples new task By recording frequencies instead rebuilding parse trees previous training examples cycle save space time learning One key SimStudent limitation depends having tutor provides examples feedback ﬁne grained stepbystep level The steps coarsegrained requiring lots inferences functions prior knowledge likely SimStudent effectively learn More generally clear SimStudents representation learning generalize needs accumulating relevant prior knowledge particularly new features whenlearning Although work begun explore issue 29 investigation needed Having acquired grammar deep features new problem given learner extract deep features ﬁrst building parse tree problem based acquired grammar extracting subsequences associated feature symbols parse tree target features This model presented far learns extract deep features unsupervised way goals context SimStudent problem solving Later extend ability integrating SimStudent 4 Empirical evaluation deep feature learner To evaluate proposed deep feature learner carried controlled experiments We compared alternatives proposed approach 1 transfer learning feature focus 2 transfer learning feature focus 3 transfer learning unlabeled subgrammars feature focus 4 transfer learning unlabeled subgrammars feature focus Learners labeled feature way knowing feature instead report accuracy obtained nonterminal symbol frequently corresponds feature subgrammar training examples Note compare proposed deep feature learner insideoutside algorithm Li et al 30 shown base learner learner extension N Li et al Artiﬁcial Intelligence 219 2015 6791 77 Fig 5 Learning curve synthetic domain subtask task transfer problems b overlapping tasks transfer problems outperforms insideoutside algorithm2 All experiments run 253 GHz Core 2 Duo Macbook 4 GB RAM 41 Experimental design synthetic domains In order understand generality scalability proposed approach ﬁrst designed carried ex periments synthetic domains We carried experiment types transfer nonrecursive domains subgrammar transfer overlappinggrammar transfer The transfer learners 3 4 trained ﬁrst grammar trained tested second grammar The nontransfer learners 1 2 trained tested second grammar In details nonrecursive domain formed randomly generated rules form binary andor tree The ﬁrst type subgrammar transfer We randomly generated sets nonrecursive rules S 1 S2 S1 subgrammar S2 The size terms number nonterminal symbols S1 roughly half S2 In order understand transfer learning affects learning eﬃciency carried second experiment previous grammar subset current grammar Rather second experiment tests overlapping grammar transfer We randomly generated nonrecursive domains S1 S2 S1 overlaps S2 feature subgrammar The transfer learners 3 4 trained ﬁrst S1 Each training record contains observation se quence set subsequences usually associated feature If S 1 contains n1 nonterminal symbols S1 number training records 10n1 Then learners trained tested S2 The number training records ranges zero If S2 contains n2 nonterminal symbols S2 number testing sequences 10n2 For testing record compared feature recognized oracle grammar recognized acquired grammar The score percentage times acquired grammar recognized feature oracle grammar averaged 100 randomly generated feature extraction tasks In results section score achieved learner given increasing number training examples learning curve 42 Experiment results synthetic domains We evaluated eﬃciency scalability proposed algorithm For learning eﬃciency measured learning curves learners For scalability looked accuracy time size acquired grammars different domain sizes Rate learning In order test learning speed randomly generated 100 subgrammargrammar pairs cid3S 1i S2icid4 1 2 100 For pair cid3S1i S2icid4 S2i 20 nonterminal symbols We measured scores learn ers subgrammargrammar pair given different numbers training sequences The results shown Fig 5a 5b We learner transfer learning feature focus Transfer Feature Focus steepest learning curve In subgrammar transfer case training records learner Transfer Feature Focus achieves score 099 higher score base learner transfer learner feature focus Transfer Feature Focus 065 Learners single extension Transfer Feature Focus Transfer Feature Focus slower learning curve comparing learner extensions Transfer Feature Focus outperform base learner Transfer Feature Focus 2 httprakaposhieasasuedunantistpdf 78 N Li et al Artiﬁcial Intelligence 219 2015 6791 Fig 6 Time learning different domain sizes subtask task transfer problems b overlapping tasks transfer problems Fig 7 Score different domain sizes subtask task transfer problems b overlapping tasks transfer problems Note transferonly learner Transfer Feature Focus better score small number training records learner feature focus Transfer Feature Focus But training records learner feature focus Transfer Feature Focus catches transferonly learner Transfer Feature Focus This situation appears transfer tasks Not surprisingly subgrammar transfer explicitly trained feature task switching task difference learners transfer learning larger overlapping task transfer Moreover randomly pick features randomly generated grammars overlapping task transfer possible selected feature relatively low level hierarchy corresponds short subsequences speciﬁc action sequence disjunctions In case targeted feature easy learn beneﬁt transfer diminished But extended learners Transfer Feature Focus Transfer Feature Focus Transfer Feature Focus outperform base learner Transfer Feature Focus Scalability learning algorithm In order understand scalability proposed algorithms tested performance learners terms accuracy time size acquired grammar different numbers domain sizes The scores learners domains ﬁve twentyﬁve nonterminal symbols shown Fig 7a 7b We training records learner extensions Transfer Feature Focus performs best learners base leaner Transfer Feature Focus shows fastest drop increasing size domains The learners single extension Transfer Feature Focus Transfer Feature Focus perform roughly equally With domains size 25 scores learners 3 apart As average time spent training record shown Fig 6a Fig 6b learners acquire targeted feature reasonable time While transferonly learner Transfer Feature Focus took 266 milliseconds training record domains size 25 learners Transfer Feature Focus Transfer Feature Focus Transfer Feature Focus took 1 millisecond training record learning We learner transfer learning unlabeled subgrammars feature focus Transfer Feature Focus runs slower domains larger sizes In fact needs 266 milliseconds training record This second task N Li et al Artiﬁcial Intelligence 219 2015 6791 79 maintaining grammar rules acquired previous task requires work Viterbi training step Besides feature focus mechanism enables learner separate sequence small subsequences focus small piece time learning Since transferonly learner Transfer Feature Focus consider feature focus learning takes longer learners We looked conciseness acquired grammar grammars larger sizes typically slow process future learning feature extraction To measure conciseness compute symbol ratio number symbols learned grammar original schema With subgrammar transfer learners acquired grammars roughly equal sizes With overlapping grammar transfer learners transfer learning unlabeled subgrammars Transfer Feature Focus Transfer Feature Focus need maintain knowledge acquired previous task higher symbol ratio learners This reasonable knowledge embedded grammar 43 Experimental design algebra In order understand proposed algorithm good model real students carried controlled simulation study algebra TransferFeature Focus learner Accelerated future learning learning proceeds effectively rapidly prior learning essential measure robust learning We little way precise understanding learning mechanisms yield results A computational model accelerated future learning ﬁts student learning data signiﬁcant achievement theoretical integration learning sciences reveal insights improving current education technologies In study focus proposed learning algorithm model better understanding accelerated future learning Two possible causes accelerated future learning better learning strategy stronger prior knowledge Learn ing feature focus example better learning strategy knowledge acquisition Transfer learning unlabeled subgrammars example developing stronger prior knowledge previous training prepare better future learning The objective study test 1 proposed model yield accelerated future learning stronger prior knowledge better learning strategies 2 prior knowledge learning strategies affect learning outcome In words model students learn later tasks effectively prior unsupervised semisupervised experience 431 Method In order understand behavior proposed model designed sequences learning tasks increas ing diﬃculties Here refer sequences curricula There tasks curricula Each learning task feature extraction task captured oracle grammar target feature represented nonterminal symbol grammar rule Task learn signed numbers Task learn recognize coeﬃcient expressions form SignedNumber x Task learn recognize constant lefthand equations form SignedNumber x Integer SignedNumber The curricula contain 1 task task 2 task task 3 task task task There 10 training sequences control difference training problems The training data randomly generated following grammar corresponding task For instance task twos grammar shown Table 1 In task learner given 10 training problems learner acquire representation knowledge learning task For task learner given ﬁve training records rate learning number training records increases To measure learning gain training condition separate testing phase carried In phase longer updated knowledge based given records It simply extracted features given records knowledge acquired training phase The quality knowledge evaluated accuracy extracted features Both systems tested 100 expressions form training data task For testing record compared feature extracted oracle grammars recognized acquired grammars Note task 4 testing problems task x x To assess accuracy model asked systems extract feature problem We oracle grammar evaluate correctness output A brief summary method shown Table 2 432 Measurements To assess learning outcome measured learning rate task curriculum evaluate effectiveness learners The experiment tested proposed model able yield accelerated future learning faster learning rate task transfer prior learning better learning strategy We compare learners previous simulations combinations transfer feature focus To evaluate learning rate report learning curves learners number training problems given task The accuracy feature extraction task averaged 10 training conditions This experiment focuses measuring learning rate In Section 7 test proposed model ﬁts real student data We integrating proposed model simulated student extended simulated student automatically discover student models The discovered model ﬁts real student data better 80 N Li et al Artiﬁcial Intelligence 219 2015 6791 Table 2 Method summary Three tasks Three curricula Number training condition Training size tasks raining size task Testing size T1 learn signed number T2 learn ﬁnd coeﬃcient expression T3 learn ﬁnd constant equation T1 T2 T2 T3 T1 T2 T3 10 10 1 2 3 4 5 100 humangenerated models This indicates extended simulated student simulates real student learning process 44 Experiment results algebra As shown Fig 8a curriculum learners acquired better knowledge training examples3 With ﬁve training problems transfer feature focus suﬃcient acquire knowledge score 096 base learner able achieve score 05 Both learners transfer learning Transfer Feature Focus Transfer Feature Focus steepest learning curve In fact reached score 096 training example The feature focus learner Transfer Feature Focus learns slowly learners transfer learning Transfer Feature Focus Transfer Feature Focus able reach score 096 ﬁve training ex amples Learners transfer prior grammar learning achieve faster future learning transfer learning The base learner Transfer Feature Focus learns slowly A careful inspection shows feature focus transfer learning base learner able acquire grammar rule nonterminal symbol generally corre sponding feature coeﬃcient learn identify positive coeﬃcients like novice students This causes failure identifying feature symbol Comparing base learner Transfer Feature Focus learner feature focus Transfer Feature Focus better learning strategy yields steeper learning curve Similar results observed curriculum curriculum shown Fig 8a 8b4 In curricu lum case case inspection shows conditions transfer learner Transfer Feature Focus remembers wrong knowledge acquired task transferred knowledge task learner perform worse learner prior knowledge Transfer Feature Focus This indicates knowledge necessarily lead steeper learning curves Transferring incorrect knowledge leads learning In curricula transfer learner Transfer Feature Focus outperforms learner semantic nonterminal constraint Transfer Feature Focus This suggests prior knowledge effective accelerating future learning learning strategy 5 Integrating deep feature representation learning SimStudent Given promising results shown believe proposed deep feature learner effective acquiring rep resentation knowledge good model real students To evaluate deep feature representation learner affect problemsolving learning intelligent agent section present integration deep fea ture learning agent SimStudent This extension integrates ideas theories perceptual chunking 31 basis improving knowledge representations turn facilitate better learning problem solving skills With current extension representation acquired deep feature learner skill learning Hence acquired representation change course skill acquisition As discuss later sections possible future step use feedback skill learning process reﬁne representation learning pro cess As mentioned SimStudent able acquire production rules solving complicated problems requires set operator functions given prior knowledge Some operator functions domainspeciﬁc require expert knowledge build In contrast feature learner acquires deep features essential ef fective learning requiring prior knowledge engineering In order reduce prior knowl edge engineering needed SimStudent build better model real students present novel approach integrates representation learner SimStudent Fig 9 shows comparison production rule ac quired original SimStudent corresponding production rule acquired extended SimStudent As 3 The Transfer Feature Focus Transfer Feature Focus overlapping 4 The Transfer Feature Focus Transfer Feature Focus overlapping Fig 8c N Li et al Artiﬁcial Intelligence 219 2015 6791 81 Fig 8 Learning curves task learners curriculum task task b task task c task task Both prior knowledge transfer feature focus strategy produce faster learning Fig 9 Original extended production rules divide readable format Grammar learning allows extraction information wherepart production rule eliminated need domainspeciﬁc function authoring getcoeﬃcient use howpart essential change coeﬃcient lefthand 3 included percep tual information extended production rule Therefore operator function sequence longer needs domainspeciﬁc operator getcoeﬃcient Then question algorithm learn include 3 perceptual information To achieve extended perceptual learning algorithm described low Previously perceptual information encoded production rules associated elements graphical user interface GUI text ﬁeld cells algebra equation solving interface This assumption limited granularity observation SimStudent achieve As suggested previous work 32 experts novices perceive mathematical expressions based syntactic structure In fact deep features discussed previously directly correspond 82 N Li et al Artiﬁcial Intelligence 219 2015 6791 Fig 10 Original extended perceptual hierarchy syntactic structure Representing deep perceptual features enhance performance learning agent eliminate reduce need authorsdevelopers manually encode domainspeciﬁc operator functions extract appropriate information appropriate parts perceptual input To improve perceptual representation extend percept hierarchy GUI elements include parse tree content leaf nodes text ﬁelds appending parse trees extension GUI path learning associated leaf nodes Fig 10 shows comparison original perceptual hierarchy extended perceptual hierarchy Since deep feature learner acquires probabilistic grammars parse trees given input In case use probable parse tree content leaf node All inserted nodes type subcell In algebra example extension means cells represent expressions corresponding sides equation extended SimStudent appends parse trees expressions cell nodes Lets use 3x example In case extended hierarchy includes parse tree 3x shown Fig 4 subtree connecting cell node associated 3x With extension coeﬃcient 3 3x explicitly represented percept hierarchy If extended SimStudent includes subcell percept production rules shown right Fig 9 new production rule need ﬁrst domainspeciﬁc operator function coeﬃcient However extending percept hierarchy presents challenges original perceptual learner First ex tended subcells associated GUI elements longer depend author specify relevant perceptual input SimStudent simply specify subcells parse trees relevant perceptual information acquired production rules include redundant information hurt generalization capability perceptual learner For example consider problems 3x6 4x8 Although examples explained dividing sides coeﬃcient 3x nodes parse tree 4x ﬁve nodes original perceptual learner able ﬁnd set generalized paths explain training examples Moreover subcells relevant percepts solving problem For example considering problem 3x6 inserted subcells 3 relevant percept solving problem Including unnecessary perceptual information production rules easily lead computational issues Second size parse tree input depends input length ﬁxed percept size assumption SimStudent longer holds Even number percepts inserted percepts ordered immediately clear To address challenges extend original perceptual learner support acquisition perceptual information redundant variablelength percept lists To SimStudent ﬁrst includes inserted subcells candidate percepts calls operator function sequence learner ﬁnd operator function sequence explains training examples In example operator function sequence divide 3 contain operator function divide 3 included candidate percept list The perceptual learner removes subcells operator function sequence candidate percept list Hence subcells 3 x included percept list Since training example action records share operator function sequence number percepts remaining example action record Next percept learner arranges remaining subcell percepts based order operator function sequences After process percept learner set percept lists contains ﬁxed number percepts ordered fashion We switch original percept learner ﬁnd general paths updated percept lists In example skill divide shown right Fig 9 perceptual information production rule contain elements lefthand righthand cells original rule coeﬃcient subcell corresponds left child variable term Note removed redundant subcells acquired production rule works 3x6 4x8 N Li et al Artiﬁcial Intelligence 219 2015 6791 83 Table 3 A list operator functions SimStudents Operator function Type GetOperand GenOne Copy Add Sub Multiply Divide Concat ReverseSign VarName Denominator Numerator SkillAdd SkillSubtract SkillMultiply SkillDivide SkillCltOperand SkillMtOperand EvalArithmetic AddTerm SubTerm DivTerm MulTerm Coeﬃcient FirstTerm LastTerm domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domaingeneral domainspeciﬁc domainspeciﬁc domainspeciﬁc domainspeciﬁc domainspeciﬁc domainspeciﬁc domainspeciﬁc domainspeciﬁc Example getoperand divide 3 3 generateone 1 copy 3 3 add 3 5 8 subtract 8 2 6 multiply 3 5 15 divide 8 3 83 concatenate 5 x 5x reversesign 8x 8x varname 8x2 x denominator 35x 5x numerator 35x 3 skilladd 3 add 3 skillsubtract 3 subtract 3 skillmultiply 3 multiply 3 skilldivide 3 divide 3 skillclt 3x45x2 clt 3x45x2 skillmt 3x mt 3x evalarithmetic 3x45x2 8x2 addterm 3x4 5x2 8x2 subtractterm 3x4 5x2 2x2 divideterm 8x2 2 4x1 multiplyterm 8x2 2 16x4 coeﬃcient 3x 3 ﬁrstterm 3x5 3x lastterm 3x5 5 As mentioned SimStudent holds perceptual grounding bias input world represented essential inductive bias skill learning With extension deep features explicitly represented perceptual hierarchy patterns extracting automatically acquired perceptual information learning algorithm In cases feature extraction operator functions needed predeﬁned order calculate deep features The production rule directly features matching learned paths perceptual information extended perceptual hierarchy SimStudent gets basic understanding term constant perceptual information paths This extension reduces number operator functions intelligent agent developer needs deﬁne order generate SimStudent maths In addition reduces number operator functions required production rules feature extraction operator functions longer needed eases production rule learning process 6 Experimental study SimStudent integrated deep feature learner In order evaluate extended SimStudent able acquire correct knowledge reduced prior knowl edge engineering carried experiment algebra domain We use algebra testing domain important learning tasks middle school students It relatively complicated similar domains multicolumn addition fraction addition Although reported demonstrated 12 extended SimStudent yields better learning performance knowledge engineering original SimStudent fraction addition stoichiometry 61 Experiment design Since goal build intelligent agent models skill acquisition real students instead randomly generated problems training sets problem sets select teach real students training sets More speciﬁcally problem sets high school students Carnegie Learning Algebra I Tutor The sizes training sets 13 14 35 35 problems We choose 10 problems real student data testing set We compare extended SimStudent original SimStudent given different amounts prior knowledge The extended SimStudent ﬁrst trained sequence deep feature learning tasks include learning signed number term expression We construct weak operator function set strong operator function set simulating weak strong prior knowledge The weak operator function set contains 24 domaingeneral operator functions copying string adding numbers The strong operator function set includes weak operator function set plus 12 domainspeciﬁc operator functions getting coeﬃcient adding terms Among given operator functions Table 3 shows list operator functions production rules acquired SimStudents Two original SimStudents extended SimStudent tested One 84 N Li et al Artiﬁcial Intelligence 219 2015 6791 Fig 11 Learning curves learners ﬁrstattempt accuracy b allattempt average accuracy original SimStudents given strong operator function set OStrong Ops provided weak operator function set OWeak Ops The extended SimStudent given weak operator function set EWeak Ops 62 Experiment results Evaluation learning speed The ﬁrst study carried focuses evaluation learning speed Since possible way solving algebra equation possible skill applicable time In order evaluate performance applicable skills use different measurements evaluating learning eﬃciency The ﬁrst measurement called ﬁrstattempt accuracy testing problem learner receives score 1 proposes correct step ﬁrst attempt gets 0 This measurement closest evaluation method real classroom settings student thought solving problem solution heshe writes graded The second measurement allattempt average accuracy focuses average performance applicable skills Instead counting ﬁrst attempt evaluator scores correctness applicable skills reports average score allattempt average accuracy The average learning curves SimStudents shown Figs 11a 11b The blue lines correspond original SimStudents black lines represent performance extended SimStudent As ﬁgures measurements huge gap original SimStudents OStrong Ops OWeak Ops strong operator functions Our focus test extended SimStudent able achieve performance comparable original SimStudent strong operator functions OStrong Ops given weak operator function set As result shows extended SimStudent EWeak Ops learns slower original SimStudent strong operator functions OStrong Ops beginning gradually catches original SimStudent With 35 training problems allattempt average accuracy extended SimStudent EWeak Ops reaches 90 5 lower original SimStudent strong operator functions OStrong Ops This suggests deep feature learner extended SimStudent able achieve comparable performance prior domainspeciﬁc knowledge engineering Evaluation knowledge engineering needed We evaluate learner performance measurements total knowledge learning speed For ﬁrst measurement look production rules acquired problem sets size 35 report average number domainspeciﬁc domaingeneral operator func tions rule sets Recall domainspeciﬁc operator functions usually require knowledge engineering domaingeneral operator functions As shown Table 4 SimStudent OStrong Ops given strong operator functions 8 domainspeciﬁc operator functions plus 75 domaingeneral operator functions average training sequences size 35 In contrast extended SimStudent EWeak Ops given domainspeciﬁc functions 12 domaingeneral operator functions indicates knowledge engineering effort In addi tion original SimStudent domaingeneral operator functions OWeak Ops 145 domaingeneral operator functions suggests needs larger prior knowledge engineering extended SimStudents However seen performs worse extended SimStudents To better understanding knowledge engineering effort reduced took closer look data As knew extended SimStudent EWeak Ops able achieve comparable performance domainspeciﬁc operators How feature learning process aid skill acquisition For example domainspeciﬁc operator subtractterm needed original SimStudent OStrong Ops skillsubtract perform operations N Li et al Artiﬁcial Intelligence 219 2015 6791 85 Table 4 Average number strong weak operator functions acquired production rules Operator functions Strong domainspeciﬁc Weak domaingeneral OriginalStrong Ops ExtendedWeak Ops OriginalWeak Ops 8 0 0 75 12 145 3x4 4 Since extended perceptual hierarchy 3x explicitly represented The extended Sim Student simply test constant term 3x4 4 equals subtrahend 4 If directly grabs 3x hierarchy returns result In case need operator subtractterm removed Instead equal feature test copy operator functions needed simpler subtractterm operator function 7 Using SimStudent discover better cognitive models As mentioned interested building learning agent like construct learning agent simulates students acquire knowledge In section going present approach automat ically discovers cognitive models extended SimStudent If discovered model turns good cognitive model able conclude extended SimStudent simulates real student learning process A cog nitive model set knowledge components KC encoded intelligent tutors model students solve problems 33 The set KCs includes component skills concepts percepts student acquire successful tar tasks For example KC algebra students proceed given problems form NvN 3x6 The cognitive model provides important information automated tutoring systems making instructional decisions Better cognitive models match real student learning behavior changes performance time They capable predicting task diﬃculty transfer learning related problems yield better instruction Traditional ways construct models include structured interviews thinkaloud protocols rational analysis However methods timeconsuming require expert input More importantly highly subjective Previous studies 3435 shown human engineering models ignores distinctions content learning important instructional implications Other methods Learning Factor Analysis LFA 36 apply automated search technique discover cognitive models It shown automated methods able ﬁnd better cognitive models humangenerated ones Nevertheless LFA requires set humanprovided factors given input These factors potential KCs LFA carries search process space factors If better model exists requires unknown factors LFA ﬁnd Therefore having approach able ﬁnd right set factors lead better predictions human data essential success cognitive modeling To address issue propose method automatically discovers cognitive models depending human provided factors The uses extended SimStudent acquire skill knowledge Each production rule corresponds KC students need learn The model labels observation real student based skill application 71 Method In order evaluate effectiveness proposed approach carried study algebra dataset We compared SimStudent model humangenerated KC model ﬁrst coding real student steps SimStu dent model humangenerated KC model testing model codings predict real student data Note DataShop 37 21 different KC models current dataset humangenerated KC model selected best models predicting human student behavior existing student models For humangenerated model real student steps ﬁrst coded action label associated correct step transaction action corresponds mathematical operations transform equation way makes progress solution As result default model contains KCs deﬁned called Action KC model add subtract multiply divide distribute clt combine like terms mt simplify multiplication rf reduce fraction Four KCs associated basic arithmetic operations add subtract multiply divide split KCs skill identify appropriate basic operator skill actually execute basic operator The called transformation skill typein skill As consequence 12 KCs deﬁned called ActionTypein KC model Not steps algebra dataset coded KC models steps transformation include Action KC model simplify division There 9487 steps coded KC models mentioned The default KC model deﬁned productions implemented cognitive tutor 6809 steps coded To fair comparison default ActionTypein KC models took intersection 9487 6809 steps As result 6507 steps coded default ActionTypein KC models We deﬁned new KC 86 N Li et al Artiﬁcial Intelligence 219 2015 6791 model called BalancedActionTypein KC model set KCs ActionTypein model associated 6507 steps KC model compare SimStudent model To generate SimStudent model SimStudent tutored solve linear equations interacting Carnegie Learning Algebra I Tutor like human student As training set SimStudent selected 40 problems teach real students Given acquired production rules step real student performed assigned applicable production rule KC associated step In cases applicable production rule coded step humangenerated KC model BalancedActionTypein Each time student encounters step KC considered opportunity student mastery KC learn KC practicing Having ﬁnished coding real student steps models SimStudent model humangenerated model Additive Factor Model AFM 36 validate coded steps AFM instance logistic regression models student success student KC KC opportunity interaction independent variables shown Equation 1 cid2 cid2 θi βk Q kj Q kjγk Nik 1 pi j 1 pi j ln Where k k represents student j represents step j k represents skill KC k probability student correct step j pi j coeﬃcient proﬁciency student θi βk coeﬃcient diﬃculty skill KC k Q kj γk coeﬃcient learning rate skill k Nik number practice opportunities student skill k Qmatrix cell step j skill k completing step j needs skill k AFM assumes student hisher proﬁciency learning start rate learning KC roughly students Given past behavior set students set tests AFM able predict probability success speciﬁc student speciﬁc problem step In case KC model deﬁning variables AFM model Once form AFM model KCs domain KCs ﬁt AFM model human student dataset test predicts future behavior students We utilized DataShop 37 large repository contains datasets educational domains set associated visualization analysis tools facilitate process evaluation includes generating learning curve visualization AFM parameter estimation evaluation statistics including AIC Akaike Information Criterion cross validation 72 Dataset We analyzed data 71 students Carnegie Learning Algebra I Tutor unit equation solving The students typical students vocationaltechnical school ruralsuburban area outside Pittsburgh PA The problems varied complexity example simpler problems like 3x6 harder problems like x572 A total 19683 transactions students Algebra Tutor recorded transaction represents attempt inquiry student feedback given tutor 73 Measurements To evaluate quality SimStudent model humangenerated KC model measured models predicting human student behavior The quality prediction generated KC models AFM measured AIC 3fold cross validation AIC measures ﬁt student data penalizing overﬁtting More speciﬁcally AIC measures log likelihood model comparing human data punishes model parameters The cross validation performed folds constraint training sets data points student KC We report root meansquared error RMSE averaged test sets 74 Experiment results The SimStudent model contains 21 KCs Both AIC 6448 cross validation RMSE 03997 lower humangenerated model AIC 6529 cross validation 04034 This indicates SimStudent model better predicts real student behavior N Li et al Artiﬁcial Intelligence 219 2015 6791 87 Fig 12 Different parse trees 3x x In order understand differences statistically reliable carried signiﬁcance tests The ﬁrst signiﬁcance test evaluates SimStudent model actually able better predictions humangenerated model During cross validation process student step test problem We took predicated error rates generated KC models step testing Then compared KC models predictions real student error rate 0 student correct ﬁrst attempt 1 After removing ties 6494 student steps SimStudent model better predictions humangenerated KC model 4260 steps A sign test shows SimStudent model signiﬁcantly p 0001 better predicting real student behavior humangenerated model In second test random nature assignment folds cross validation evaluated lower RMSE achieved SimStudent model consistent chance To repeated cross validation 20 times calculated RMSE models Across 20 runs SimStudent model consistently outperformed humangenerated model particular paired ttest shows SimStudent model signiﬁcantly p 0001 better humangenerated model5 Therefore conclude SimStudent model reliably better student model humangenerated KC model 75 Implications instructional decision We inspect data closely better qualitative understanding SimStudent model better implications improved instruction 38 Among 21 KCs learned SimStudent model 17 transformation KCs typein KCs It hard map SimStudent KC model directly humangenerated KC model Approximately speaking distribute clt combine like terms mt rf KCs typein KCs similar KCs deﬁned humangenerated KC model The transformation skills associated basic arithmetic operators add subtract multiply divide split ﬁner grain sizes based different problem forms One example split SimStudent created KCs division The ﬁrst KC simStdivide corresponds problems form AxB A B signed numbers second KC simStdivide1 speciﬁcally associated problems form xA A signed number This caused different parse trees Ax vs x shown Fig 12 To solve AxB SimStudent simply needs divide sides signed number A On hand x 1 represented explicitly parse tree SimStudent needs x 1x extract 1 coeﬃcient If SimStudent good model human learning expect true human students That real students greater diﬃculty making correct steps like x6 steps like 3x6 need convert mentally x 1x To evaluate hypothesis computed average error rates set problem types shown solid line Fig 13 problem types deﬁned forms like NvN Ns integer number v variable 3x6 instance NvN 6x instance Nv The problem types sorted increasing error rates In words problem types right harder human students left We calculated mean predicted error rates problem type humangenerated model SimStudent model Consistent hypothesis shown Fig 13 problems form AxB average error rate 0283 simpler problems form xA average error rate 0719 The humangenerated model predicts problem types similar error rates average predicted error rate AxB 0302 average predicted error rate xA 0334 fails capture diﬃculty difference problem types AxB xA The SimStudent model hand ﬁts real student error rates better It predicts higher error rates 0633 average problems form xA problems form AxB 0291 average SimStudents automatic split original division KC KCs simStdivide simStdivide1 suggests tutor teach real students solve types division problems separately In words tutoring students division problems include subsets problems subset corresponding simStdivide problems AxB speciﬁcally simStdivide1 problems xA We include explicit instruction highlights students x 1x 5 Note differences competitors KDD Cup 2010 httpspslcdatashopwebcmueduKDDCupLeaderboard range thousands RMSE 88 N Li et al Artiﬁcial Intelligence 219 2015 6791 Fig 13 Error rates real students predicted error rates student models 8 Related work The main contribution paper build humanlike intelligent agent reduce knowl edge engineering required integrating feature learning agent Previous work cognitive science shown chunking important component human knowledge acquisition Theories chunking mechanisms 93139 constructed EPAM 9 ﬁrst chunking theories proposed explain key phenomena expertise chess Learning occurs incremental growth discrimination network node network chunk It shown chunks suggest plans moves A later version EPAM EPAM IV 31 extends basic chunking mechanism support retrieval structure enables domainspeciﬁc material rapidly indexed In theories chunks usually refer perceptual chunks In addition CHREST 39 proposes template theory discrimination network contains perceptual chunks action chunks A detailed review works 40 Our work similar works modeling learning perceptual chunks kind deep feature learning differs theories theories uses pCFG learning model acquisition perceptual chunks There considerable research learning agent architectures Soar 19 uses chunking mechanism acquire knowledge constrains problemspace search Another architecture ACTR 2 creates new production rules compilation process gradually transforms declarative representations skill knowledge 18 Icarus 3 acquires complex conceptual predicates context problem solving Unlike theories SimStudent puts em phasis knowledgelevel learning cf 20 achieved induction positive negative examples It integrates ideas theories perceptual chunking 31 basis improving knowledge representations turn facilitate better learning problem solving skills Other research cognitive science attempts use probabilistic approaches model process human learning Kemp Xu 41 apply probabilistic model capture principles infant object perception Kemp Tenenbaum 42 use hierarchical generative model acquisition process domainspeciﬁc structural constraints But approaches tend use probabilistic model representation acquisition component learning agent Another closely related research area learning procedural knowledge observing behavior Classical ap proaches include explanationbased learning 4344 learning apprentices 45 programming demonstration 4616 Most approaches analytic methods acquire candidate procedures Other works transfer learn ing 4750 share resemblance work They focus improving performance learning transferring previously acquired knowledge domain However best knowledge approaches uses transfer learning learner acquire better representation reveals essential percept features integrate intelligent agent Additionally research deep architectures 51 shares clear resemblance work receiving creasing attention recently Theoretical results suggest order learn complicated functions AIlevel tasks deep architectures composed multiple levels nonlinear operation needed Although having stud ied machine learning literature diﬃculty optimization notable exceptions area including convolutional neural networks 5255 sigmoidal belief networks learned variational approxima tions 5659 deep belief networks 6061 While work deep architectures work interested modeling complicated functions nonlinear features tasks work different Deep architectures classiﬁcation tasks work focuses simulating human problem solving learning math science A lot efforts comparing quality alternative student models LFA automatically discovers student models limited space humanprovided factors Other works 6263 dependent N Li et al Artiﬁcial Intelligence 219 2015 6791 89 human labeling suffer challenges interpreting results In contrast SimStudent approach beneﬁt acquired production rules precise usually straightforward interpretation Baffes Mooney 64 apply theory reﬁnement problem modeling incorrect student behavior Other systems 6566 use Qmatrix ﬁnd knowledge structure student response data None approaches use simulated students construct student models Besides SimStudent lot work creating simulated students 76768 VanLehns 69 Sierra models impassedriven acquisition hierarchical procedures multicolumn subtraction sample solutions However work focused explaining origin bugs real students focus In addition Sierra given CFG parsing visual state subtraction problems automatically acquires pCFG Ohlsson 70 reviews different learning models employed different learning phases intelligent systems Our work integrating representation learning skill learning reﬂects learning mechanism able aid learning processes intelligent systems None approaches compared human learning curve data To best knowledge work ﬁrst combination use student model evaluation techniques assess quality simulated learner 9 Further discussion In spite promising results fruitful possibilities improve SimStudent First carry extensive studies domains evaluate generality proposed Moreover extended SimStudent uses domainindependent operator functions evaluate extension enables better transfer learning In words trained relevant task extended SimStudent needs fewer number extra operator functions original SimStudent new learning task We shown experimental results repeated domains fraction addition stoichiometry second language learn ing 1221 Basically domain represented set production rules potentially addressed SimStudent In future studies like similar results reproduced probabilistic based domains Since interested modeling real student learning like carry experiments comparing performance real student data In particular interested matching type errors SimStudent common errors real students We believe extensions able gain insights human learning advance process creating integrated intelligent agent Second mentioned pCFGs suitable representing 1dimensional information appropri ate choice maths domains algebra multicolumn addition There domains require representation 2dimensional space In response extended feature learning mechanism use twodimensional variant probabilistic contextfree grammar pCFG modeling user perceives structure user interface 71 The proposed 2dimensional pCFG learning algorithm models acquisition representation Our learning method exploits spatial layout interface temporal information users interact interface Since additional temporal spatial information representation acquisition proce dure search space reduced learning process remains relatively effective More speciﬁcally experimental studies carried synthetic domains tutoring domains including fraction addition equation solving stoichiometry We proposed layout learner able acquire highquality layouts small number training examples demonstrate intelligent agent acquired layouts able perform equally compar ing agent given manually constructed layouts We believe similar changes extend 2dimensional grammar support domains integral differential equations Additionally deep feature learning process currently carried SimStudent knowledge acquisition Only acquired grammar provide better representation SimStudent It possible feedback given SimStudent provide feedback integrated grammar For example deep feature learner initially acquired incorrect grammar caused lot failures SimStudent learning extended SimStudent potentially feed information deep feature learner ask revise grammar Moreover training records deep feature learner automatically generated steps demonstrated SimStudent By SimStudent able learn better representation knowledge skill knowledge acquisition The learning systems mutually assist achieving better performance Lastly SimStudents learning mechanism presents bias generalized skills speciﬁc ones considering computational cost This bias appears limitation SimStudent model student learning In cases general strategy invokes complicated procedure like dividing sides 1 solving xA human students prefer use general simple strategy moving minus sign lefthand righthand We recently developed conﬂict resolution strategy prefer skills smaller computational cost 21 This extension potentially addresses limitation SimStudent student model 10 Concluding remarks As shown previous studies 89 claim representation learning important aspects human knowledge acquisition modeled cognitive theories human learning In theory perceptual chunks 90 N Li et al Artiﬁcial Intelligence 219 2015 6791 correspond nodes contextfree grammar organizes perceptual knowledge Perceptual processing modeled parsing stimuli grammar result parse tree organizes constituents The learning process representation carried bottomup unsupervised fashion guided statistical regularities observed stimuli While results acquired representation able model types student errors forms representation discrimination networks 93139 connectionist models 72 possible We integrate proposed representation learner skill learning We claim speed skill acquisition depends quality representation given skill learner By integrating representation learning skill learning speed skill learning increases In later studies 13 showed extended agent discover better models student learning suggests extended agent better models human knowledge acquisition To sum building intelligent agent simulates humanlevel learning essential task AI cognitive science building systems requires manual encoding prior domain knowledge In paper proposed novel algorithm automatically acquires deep features observations annotation light anno tations We integrate standalone feature learning algorithm intelligent agent SimStudent extension perception module We showed integration extended SimStudent able achieve comparable performance requiring domainspeciﬁc operator function input In addition effective learner showed extended SimStudent discover better models real students Given results conclude extended SimStudent good humanlike intelligent agent requires small knowledge engineering Acknowledgement This research sponsored National Science Foundation grant numbers SBE0354420 DLR0910176 OMA0836012 EIA0205301 University California ONR grand number G0607ES008 Department Education grant number R305A090519 Mitre Corporation grant number 62459 The views conclu sions contained document author interpreted representing oﬃcial policies expressed implied sponsoring institution US government entity References 1 JE Laird A Newell PS Rosenbloom Soar architecture general intelligence Artif Intell 33 1 1987 164 2 JR Anderson Rules Mind Lawrence Erlbaum Associates Hillsdale New Jersey 1993 3 P Langley D Choi A uniﬁed cognitive architecture physical agents Proceedings TwentyFirst National Conference Artiﬁcial Intelligence 4 CM Neves JR Anderson Knowledge compilation mechanisms automatization cognitive skills Cognitive Skills Their Acquisition Boston 2006 1981 pp 5784 5 Y Anzai HA Simon The theory learning Psychol Rev 86 2 1979 124140 6 N Matsuda A Lee WW Cohen KR Koedinger A computational model learner errors arise weak prior knowledge Proceedings Conference Cognitive Science Society 2009 7 K Vanlehn S Ohlsson R Nason Applications simulated students exploration J Artif Intell Educ 5 1994 135175 8 MTH Chi PJ Feltovich R Glaser Categorization representation physics problems experts novices Cogn Sci 5 2 1981 121152 9 WG Chase HA Simon Perception chess Cogn Psychol 4 1 1973 5581 10 N Li WW Cohen KR Koedinger A computational model accelerated future learning feature recognition Proceedings 10th Interna 11 N Matsuda WW Cohen KR Koedinger Teaching teacher tutoring SimStudent leads effective cognitive tutor authoring Int J Artif Intell tional Conference Intelligent Tutoring Systems 2010 pp 368370 Educ 2014 httpdxdoiorg101007s4059301400201 12 N Li WW Cohen KR Koedinger Eﬃcient crossdomain learning complex skills Proceedings 11th International Conference Intelligent 13 N Li WW Cohen N Matsuda KR Koedinger A machine learning approach automatic student model discovery Proceedings 4th International Conference Educational Data Mining Eindhoven 2011 pp 3140 14 N Li WW Cohen KR Koedinger Problem order implications learning transfer Proceedings 11th International Conference Intelligent 15 V Aleven BM Mclaren J Sewall KR Koedinger A new paradigm intelligent tutoring systems exampletracing tutors Int J Artif Intell Educ 19 16 T Lau DS Weld Programming demonstration inductive learning formulation Proceedings 1999 International Conference Intelli gence User Interfaces 1998 pp 145152 17 S Muggleton L Raedt Inductive logic programming theory methods J Log Program 19 1994 629679 18 NA Taatgen FJ Lee Production compilation simple mechanism model complex skill acquisition Hum Factors 45 1 2003 6175 19 JE Laird PS Rosenbloom A Newell Chunking SOAR anatomy general learning mechanism Mach Learn 1 1986 1146 20 A Newell The knowledge level Artif Intell 18 1 1982 87127 21 N Li Y Tian WW Cohen KR Koedinger Integrating perceptual learning external world knowledge simulated student 16th International Conference Artiﬁcial Intelligence Education 2013 pp 400410 22 T Mitchell Generalization search Artif Intell 18 2 1982 203226 23 JR Quinlan Learning logical deﬁnitions relations Mach Learn 5 3 1990 239266 24 KR Koedinger JR Anderson Abstract planning perceptual chunks elements expertise geometry Cogn Sci 14 1990 511550 25 N Li S Kambhampati S Yoon Learning probabilistic hierarchical task networks capture user preferences Proceedings 21st International Joint Conference Artiﬁcial Intelligence Pasadena CA 2009 26 K Lari SJ Young The estimation stochastic contextfree grammars insideoutside algorithm Comput Speech Lang 4 1990 3556 27 AP Dempster NM Laird DB Rubin Maximum likelihood incomplete data EM algorithm J R Stat Soc B 39 1 1977 138 Tutoring Systems 2012 Tutoring Systems 2012 2009 105154 N Li et al Artiﬁcial Intelligence 219 2015 6791 91 28 KR Koedinger BA MacLaren Developing pedagogical domain theory early algebra problem solving CMUHCII Tech report 02100 Carnegie 29 N Li A Schreiber WW Cohen KR Koedinger Creating features learned grammar simulated student Proceedings 20th European Mellon University 2002 Conference Artiﬁcial Intelligence 2012 30 N Li W Cushing S Kambhampati S Yoon Learning probabilistic hierarchical task networks probabilistic contextfree grammars capture user preferences ACM Trans Intell Syst Technol 5 2 2014 29 31 HB Richman JJ Staszewski HA Simon HB Richman JJ Staszewski HA Simon Simulation expert memory EPAM IV Psychol Rev 1995 32 AR Jansen K Marriott GW Yelland Constituent structure mathematical expressions Proceedings 22th Annual Meeting Cognitive 33 KR Koedinger AT Corbett C Perfetti The knowledgelearninginstruction KLI framework bridging sciencepractice chasm enhance Science Society vol 22 Mahwah NJ 2000 p 238 robust student learning Cogn Sci 36 2012 757798 34 KR Koedinger MJ Nathan The real story story problems effects representations quantitative reasoning J Learn Sci 13 2 2004 305330 129164 35 KR Koedinger EA McLaughlin Seeing language learning inside math cognitive analysis yields transfer Proceedings 32nd Annual Conference Cognitive Science Society Austin TX 2010 pp 471476 36 H Cen K Koedinger B Junker Learning factors analysis general method cognitive model evaluation improvement Proceedings 8th International Conference Intelligent Tutoring Systems 2006 pp 164175 37 KR Koedinger RS Baker K Cunningham A Skogsholm B Leber J Stamper A data repository EDM community PSLC DataShop 38 KR Koedinger EA McLaughlin JC Stamper Automated student model improvement Proceedings 5th International Conference Educa Handbook Educational Data Mining 2010 tional Data Mining 2012 pp 1724 39 F Gobet HA Simon Five seconds Presentation time expert memory Cogn Sci 24 4 2000 651682 40 F Gobet Chunking models expertise implications education Appl Cogn Psychol 19 3 2005 183204 41 C Kemp F Xu An ideal observer model infant object perception D Koller D Schuurmans Y Bengio L Bottou Eds NIPS MIT Press 2008 pp 825832 42 C Kemp JBB Tenenbaum The discovery structural form Proc Nat Acad Sci USA 105 31 2008 1068710692 43 A Segre A learning apprentice mechanical assembly Proceedings Third IEEE Conference AI Applications 1987 pp 112117 44 RJ Mooney A General ExplanationBased Learning Mechanism Its Application Narrative Understanding Morgan Kaufmann San Mateo CA 1990 45 TM Mitchell S Mahadevan LI Steinberg Leap learning apprentice VLSI design Proceedings 9th International Joint Conference Artiﬁcial intelligence San Francisco CA 1985 pp 573580 46 A Cypher DC Halbert D Kurlander H Lieberman D Maulsby BA Myers A Turransky Eds Watch What I Do Programming Demonstration 47 R Raina AY Ng D Koller Constructing informative priors transfer learning Proceedings 23rd International Conference Machine 48 A NiculescuMizil R Caruana Inductive transfer bayesian network structure learning Proceedings 11th International Conference AI 49 L Torrey J Shavlik T Walker R Maclin Relational macros transfer reinforcement learning Proceedings 17th Conference Inductive 50 M Richardson P Domingos Markov logic networks Mach Learn 62 12 2006 107136 51 Y Bengio Learning deep architectures AI Found Trends Mach Learn 2 2009 1127 52 Y LeCun B Boser JS Denker D Henderson RE Howard W Hubbard LD Jackel Backpropagation applied handwritten zip code recognition Neural Comput 1 1989 541551 53 Y Lecun L Bottou Y Bengio P Haffner Gradientbased learning applied document recognition Proceedings IEEE 1998 pp 22782324 54 PY Simard D Steinkraus JC Platt Best practices convolutional neural networks applied visual document analysis Proceedings Seventh International Conference Document Analysis Recognition IEEE Computer Society Washington DC USA 2003 55 M Ranzato FJ Huang YL Boureau Y LeCun Unsupervised Learning Invariant Feature Hierarchies Applications Object Recognition Com puter Vision Pattern Recognition IEEE Computer Society Conference 5 2007 18 56 P Dayan GE Hinton RM Neal RS Zemel The Helmholtz machine Neural Comput 7 5 1995 889904 57 GE Hinton P Dayan BJ Frey RM Neal The wakesleep algorithm unsupervised neural networks Science 268 5214 1995 11581161 58 LK Saul T Jaakkola MI Jordan Mean ﬁeld theory sigmoid belief networks J Artif Intell Res 4 1996 6176 59 I Titov J Henderson Constituent parsing incremental sigmoid belief networks Proceedings 45th Annual Meeting Association Computational Linguistics Association Computational Linguistics Prague Czech Republic 2007 pp 632639 60 GE Hinton To recognize shapes ﬁrst learn generate images Prog Brain Res 165 2007 535547 61 Y Bengio O Delalleau C Simard Decision trees generalize new variations Comput Intell 26 4 2010 449467 62 PI Pavlik H Cen KR Koedinger Learning factors transfer analysis learning curve analysis automatically generate domain models Pro ceedings 2nd International Conference Educational Data Mining 2009 pp 121130 63 M Villano Probabilistic student models Bayesian belief networks knowledge space theory Proceedings 2nd International Conference Intelligent Tutoring Systems Heidelberg 1992 pp 491498 64 P Baffes R Mooney Reﬁnementbased student modeling automated bug library construction J Artif Intell Educ 7 1 1996 75116 65 KK Tatsuoka Rule space approach dealing misconceptions based item response theory J Educ Meas 20 4 1983 345354 66 T Barnes The Qmatrix method mining student response data knowledge Proceedings AAAI Workshop Educational Data Mining Pittsburgh 67 TW Chan CY Chou Exploring design supports reciprocal tutoring Int J Artif Intell Educ 8 1997 129 68 TN Pentti Hietala The competence learning companion agents Int J Artif Intell Educ 9 1998 178192 69 K VanLehn Mind Bugs The Origins Procedural Misconceptions MIT Press Cambridge MA USA 1990 70 S Ohlsson Computational Models Skill Acquisition Cambridge University Press 2008 pp 359395 Ch 13 71 N Li WW Cohen KR Koedinger Learning perceive twodimensional displays probabilistic grammars Proceedings 2012 European Conference Machine Learning Knowledge Discovery Databases Part II ECML PKDD12 Berlin Heidelberg 2012 pp 773788 72 JR Anderson A spreading activation theory memory J Verbal Learn Verbal Behav 22 1983 261295 PA 2005 pp 18 MIT Press Cambridge MA 1993 Learning New York NY 2006 pp 713720 Statistics 2007 Logic Programming Corvallis Oregon 2007