Artiﬁcial Intelligence 125 2001 209226 Research Note Robust Bayes classiﬁers Marco Ramoni acid3 Paola Sebastiani b Childrens Hospital Informatics Program Harvard Medical School 300 Longwood Avenue Boston MA 02115 USA b Department Mathematics Statistics University Massachusetts Amherst MA 01003 USA Received 12 November 1999 received revised form 24 October 2000 Abstract Naive Bayes classiﬁers provide efﬁcient scalable approach supervised classiﬁcation problems When entries training set missing methods exist learn classiﬁers assumptions pattern missing data Unfortunately reliable information pattern missing data readily available recent experimental results enforcement incorrect assumption pattern missing data produces dramatic decrease accuracy classiﬁer This paper introduces Robust Bayes Classiﬁer RBC able handle incomplete databases assumption pattern missing data In order avoid assumptions RBC bounds possible probability estimates intervals specialized estimation method These intervals classify new cases computing intervals posterior probability distributions classes given new case ranking intervals according criteria We provide scoring methods rank intervals decision theoretic approach trade risk erroneous classiﬁcation choice classifying unequivocally case This decision theoretic approach assess opportunity adopting assumptions pattern missing data The proposed approach evaluated publicly available databases cid211 2001 Elsevier Science BV All rights reserved Keywords Bayes classiﬁer Missing data Probability intervals 1 Introduction Supervised classiﬁcation task assigning class label unclassiﬁed cases described set attribute values This task typically performed ﬁrst training classiﬁer set classiﬁed cases label unclassiﬁed cases The Corresponding author Email addresses marco_ramoniharvardedu M Ramoni sebasmathumassedu P Sebastiani 0004370201 matter cid211 PII S 0 0 0 4 3 7 0 2 0 0 0 0 0 8 5 0 2001 Elsevier Science BV All rights reserved 210 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 supervisory component classiﬁer resides training signal provides classiﬁer way assess dependency measure attributes classes Naive Bayes classiﬁers NBCs 411 ﬁrst supervised classiﬁcation methods past years enjoyed renewed consideration 6 The training step NBC consists estimating conditional probability distributions attribute given class training data set Once trained NBC classiﬁes case computing posterior probability distribution classes Bayes Theorem assigning case class highest posterior probability NBCs assumes attributes conditionally independent given class assumption renders efﬁcient training classiﬁcation Unfortunately training set incomplete attribute values class reported unknown efﬁciency accuracy classiﬁer lost Simple solutions handle missing data ignore cases including unknown entries ascribe entries ad hoc dummy state respective variables 15 Both solutions known introduce potentially dangerous biases estimates 9 discussion In order overcome problem Friedman et al 6 suggest use EM algorithm 3 gradient descent 20 add Gibbs sampling 7 All methods rely assumption data Missing Random MAR 13 database left information infer missing entries recorded ones Unfortunately way verify data actually MAR particular database assumption violated estimation methods suffer dramatic decrease accuracy consequence jeopardizing performance resulting classiﬁer 21 This paper introduces new type NBC called Robust Bayes Classiﬁer RBC rely assumption missing data mechanism The RBC based Robust Bayes Estimator RBE 18 estimator returns intervals containing estimates induced possible completions incomplete database The intuition RBE information missing data mechanism incomplete data set constrain set estimates induced possible completions However situation estimator bound posterior probability classes The ﬁrst contribution paper provide specialized closedform intervalbased estimation procedure NBCs takes advantage conditional independence assumptions Once trained classiﬁers classify unlabeled cases Unfortunately Bayes Theorem straightforwardly extended standard pointvalued probabilities intervalvalued probabilities Nonetheless conditional independence assumptions underlying NBC allows closedform solution classiﬁcation task The second contribution paper new propagation algorithm compute posterior probability intervals containing class posterior probabilities obtained exact computation possible completions training set These intervals ranked according score new case assigned class associated highest ranked interval We provide scoring methods ﬁrst based strong dominance criterion 10 assigns case class minimum posterior probability higher maximum posterior probability classes This criterion preserves robustness classiﬁer leave cases unclassiﬁed provide weaker criterion improve coverage We introduce general decisiontheoretic M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 211 framework select appropriate criterion trading accuracy coverage As byproduct decisiontheoretic approach provides principled way asses viability MAR assumption given training set We database complete RBC estimates reduce standard Bayesian estimates RBC subsumes standard NBC special case This approach evaluated publicly available databases 2 Naive Bayes classiﬁers An NBC better understood regard m attributes set q mutually exclusive exhaustive classes discrete stochastic variables In way depict NBC Bayesian network 6a directed acyclic graph nodes represent stochastic variables arcs represent dependency relationships variables shown Fig 1 In network root node represents set C mutually exclusive exhaustive classes attribute child node Ai Each value cj variable C class attribute Ai bears set si values Ai D ak As shorthand denote C D cj cj Ai D ak aik The graphical structure Bayesian network representing NBC encodes assumption attribute Ai conditionally independent attributes given class The classiﬁer deﬁned marginal probability distribution fpcj g variable C set conditional probability distributions fpaik D cj g attribute Ai given class cj A consequence independence assumption distributions estimated training set D independently follows Let naik cj frequency cases training set D attribute Ai appears value aik class cj let ncj frequency cases training set class cj When training set D complete Bayesian estimates paik j cj pcj paik j cj D P cid11ij k C naik cj Tcid11ij h C naih cj U h pcj D P cid11j C ncj Tcid11l C nclU l 1 respectively The quantities cid11ij k cid11j regarded frequencies pair aik cj class cj respectively imaginary sample representing prior information distributions attributes classes The size cid11 imaginary sample called global prior precision Further details 17 Once classiﬁer Fig 1 A Bayesian network representing NBC attributes A1 A9 set C classes 212 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 trained use classiﬁcation new cases If represent case set attribute values e D fa1k amkg Bayes theorem yields posterior probability class cj given e pcj j e D Q pcj P q hD1 pch m iD1 paik j cj Q m iD1 paik j ch 2 case assigned class highest posterior probability From computational point view training classiﬁer reduces summarizing database D m contingency tables Ti dimension q cid2 si cell j k table Ti collecting frequency pair aik cj In way estimation q probability distributions attribute Ai conditional classes c1 cq locally frequencies naik cj table Ti frequencies nahk cj tables Th irrelevant ii estimation probability distribution attribute Ai conditional class cj independently classes frequencies naik cj row j iii estimation marginal distribution classes tables Ti row totals ncj In words estimation procedure performed table table table row row These properties termed global local parameter independence 22 source computational efﬁciency training process When entries training set D missing accuracy efﬁciency NBC threat The reasons situation clear regard incomplete database result deletion process occurred complete unknown database The received view missing data 13 based characterization deletion process According approach data Missing Completely Random MCAR probability entry missing independent observed unobserved values They Missing Random MAR probability function observed values database In cases data Informatively Missing Under assumption data MAR MCAR values unknown entries estimated observed ones deletion process called ignorable This property guarantees available data sufﬁcient train classiﬁer unfortunately enjoy longer properties global local parameter independence Indeed unknown entries induce types incomplete cases cases attribute Ai observed class missing ii cases class cj observed value attribute Ai missing iii cases value attribute Ai class missing We denote frequency cases naik n cj n respectively Suppose estimation method able compute estimates Eq 1 assigning proportion frequencies naik n cj n cell j k contingency table Ti As reconstructed marginal frequency class needs equal tables estimation locally longer properties local global parameter independence lost One exception arises class observed cases M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 213 Theorem 1 Suppose class observed cases training set D entries MAR Then estimates Eq 1 naik cj frequency fully observed pairs aik cj ncj class frequency exact Bayesian estimates The proof appears 22 When classes missing use approximate methods mentioned Introduction compute estimates Eq 1 However methods require deletion process ignorable When data informatively missing available entries longer sufﬁcient train classiﬁer Furthermore way check deletion process responsible missing data actually ignorable These motivations introduction Robust Bayesian Estimator RBE 18 application paper development robust version NBC 3 Robust estimation Recall NBC trained estimating conditional probability distributions fpaik j cj g fpcj g database D This section describes perform task database D incomplete We need following deﬁnitions Deﬁnition 1 Consistency Let D incomplete data set let px probability wish estimate D 1 A consistent completion D complete data set Dc D obtained deletion process 2 A consistent estimate px estimate computed consistent completion D 3 A consistent probability interval px interval Tpinf x psupxU containing consistent estimates A consistent interval nontrivial pinf x 0 psupx 1 4 A consistent probability interval tight smallest consistent probability interval Tpx pxU px The difference consistent tight consistent probability interval interval extreme points lower upper bounds set consistent estimates extreme points reached consistent completion database The rest section devoted construction tight consistent probability intervals quantities paik j cj pcj deﬁning NBC In order estimate conditional probability paik j cj incomplete training set D RBE collects frequencies naik n cj n incomplete cases virtual frequencies naik cj naik cj These frequencies compute extreme points tight consistent probability interval paik j cj The quantity naik cj maximum number incomplete cases Ai C completed aik cj given naik cj D n cj C naik C n 3 214 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 On hand virtual frequency naik cj maximum number incomplete cases Ai C ascribed cj increasing frequency naik cj naik cj D n cj C naih C n 4 X h6Dk The virtual frequencies compute values paik j cj paik j cj respectively minimum maximum estimate paik j cj consistent completions D cid11ij k C naik cj paik j cj D P paik j cj D P Tcid11j h C naih cj U C naik cj h cid11ij k C naik cj C naik cj Tcid11ij h C naih cj U C naik cj h 5 X It shown 18 interval Tpaik j cj paik j cj U tight consistent We consider estimation pcj note virtual frequencies ncj ncj equal number n cases D class observed We obtain tight consistent probability intervals pcj setting pcj D pcj D P P cid11j C ncj Tcid11l C nclU C n l cid11j C ncj C n Tcid11l C nclU C n l 6 When training set complete Eqs 5 6 reduce Eq 1 Each set given maximum probability class cj minimum probabilities classes f pcj pch h 6D j g deﬁnes probability distribution pcj C pch D 1 j 7 h6Dj probability intervals Tpcj pcj U reachable deﬁned 2 By deﬁnition probability paik j cj maximum value paik j cj virtual counter naik cj absorbs frequencies naik n pcj D pcj class ch paik j ch paik j ch pch D pcj Similarly probability paik j cj minimum value paik j cj class ch paik j ch paik j ch However class observed virtual frequencies naik cj naik cj equal n cj naik D n D 0 k In case probabilities paik j cj vary independently maxima minima reached time different classes cj 4 Robust classiﬁcation Once trained classiﬁer label unclassiﬁed cases Given new case NBC performs task steps ﬁrst computes posterior probability class M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 215 given attribute values assigns case class highest posterior probability In section ﬁrst compute posterior probability intervals class rank intervals classify new cases 41 Posterior probability intervals Let e D fa1k amkg attribute values case e wish classify With pointvalued probabilities expression posterior probability class cj given e given Eq 2 The Theorem identiﬁes nontrivial consistent probability intervals classes The result generalizes solution provided 18 Boolean classes We training set D reports class consistent intervals tight Theorem 2 Let D incomplete data set Then probability interval Tpinf cj j e psupcj j eU psupcj j e D pcj pinf cj j e D pcj Q pcj Q m iD1 paik j cj C m iD1 paik j cj P h6Dj pch Q m iD1 paik j ch Q Q m iD1 paik j cj pcj m iD1 paik j cj C maxffg g 6D j g 8 9 set ffg g 6D j g contains q cid0 1 quantities pcg mY iD1 paik j cg C X l6Djg pcl mY iD1 paik j cl g 6D j D 1 q nontrivially consistent Proof To prove theorem need interval Tpinf cj j e psupcj j eU contains posterior probabilities pcj j e derived possible completions training set pinf cj j e 0 psupcj j e 1 The inequalities simple consequence property 0 paik j cj 6 paik j cj 1 0 pcj 6 pcj 1 enjoyed robust estimates Hence sufﬁcient j pinf cj j e 6 pcj j e 6 psupcj j e quantity pcj j e class posterior probability computed consistent completions training set D From Eq 2 write pcj j e yj xj P h6Dj yhxh Q m yj D pcj xj D iD1 paik j cj For ﬁxed yj function f xj yj concave increasing xj decreasing xh h 6D j From standard convex analysis 19 follows variables xj constrained vary hyperrectangle maxima minima function obtained extreme points constrained region In particular function f xj yj maximized maximizing xj f xj yj D yj xj C 10 216 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 P P h6Dj xh minimized minimizing xj maximizing h6Dj xh minimizing This argument grounds intuition proof ﬁnd maxima minima function f xj yj hyperrectangle containing region deﬁnition variables xj yj ﬁxed maxima minima induce upper lower bounds function f xj yj We maximize minimize bounds respect yj The ﬁrst step ﬁnd hyperrectangle If probabilities paik j cj vary independently intervals Tpaik j cj paik j cj U variables xj vary independently Cartesian product C intervals T xj xj U D mY paik j cj mY paik j cj Q iD1 Q m iD1 paik j cj xh D iD1 Q Q m Thus setting xj D iD1 paik j ch yields maximum function f xj yj hyperrectangle C yj ﬁxed However noted Section 3 probabilities paik j cj vary independently function f xj yj deﬁned subset C quantity yj yj Q P P f1yj D m iD1 paik j cj Q m iD1 paik j cj C h6Dj yh upper bound Now maximize function f1yj respect yj subject j yj D 1 imposed fact probability intervals constraint Tpcj pcj U reachable shown Eq 7 This maximization yields upper bound Eq 8 The minimum function f yj xj hyperrectangle C m iD1 paik j cj maximizing yj ﬁxed given setting xj D h6Dj xh The Q m iD1 paik j ch quantities h6Dj m iD1 paik j ch maximized m iD1 paik j ch h6Dj P P P Q Q yj yj Q P f2yj D m iD1 paik j cj C m iD1 paik j cj Q h6Dj yh lower bound f yj xj We minimize function f2yj respect yj minimum given setting yj D pcj maximizing function f3 D P l pcl D 1 cid0 pcj The function f3 linear probabilities pch maximum evaluating extreme points constrained region lower bound Eq 9 follows 2 m iD1 paik j ch subject constraint pcg C m iD1 paik j ch h6Dj pch P Q When training set complete RBE intervals reduce point estimates given Section 2 quantities Eqs 8 9 identical posterior probability Eq 2 The interval Tpinf cj j e psupcj j eU consistent contains posterior probabilities pcj j e obtain applying Bayes Theorem consistent estimates paik j cj pcj The proof Theorem 2 uses constraints imposed class probability intervals Tpcj pcj U mixes maximum minimum probabilities coherently However probabilities paik j cj varying j minimized maximized independently general produce loose bounds Still class observed cases prove tightness bounds M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 217 Theorem 3 If class cj reported case e probability intervals deﬁned pcj j e D pcj pcj j e D pcj Q Q pcj m iD1 paik j cj C m iD1 paik j cj Q P l6Dj pcl m iD1 paik j cl Q pcj Q m iD1 paik j cj C m iD1 paik j cj P h6Dj pch Q m iD1 paik j ch 11 12 tight consistent Proof If class observed pcj D pcj D pcj noted Section 3 probabilities paik j cj vary independently j varies upper lower bounds Eqs 8 9 maximum minimum values function Eq 10 Note Eqs 8 9 reduce Eqs 11 12 2 42 Ranking intervals The previous section shown compute consistent posterior probability intervals classes given set e attribute values We use intervals assign case class associating interval score following classiﬁcation rule Deﬁnition 2 Intervalbased classiﬁcation rule Let e set attribute values let scj j e scores associated probability intervals Tpinf cj j e psupcj j eU Each case attribute values e assigned class associated largest score The intervalbased classiﬁcation rule based intuition score scj j e associated probability intervals Tpinf cj j e psupcj j eU meaningful summary global information contained probability intervals However unique requirement Since standard NBC classiﬁes cases basis posterior probabilities classes given attribute values require set scores associated probability intervals Tpinf cj j e psupcj j eU deﬁnes probability distribution X scj j e 0 j scj j e D 1 13 j Theorem 2 ensures interval Tpinf cj j e psupcj j eU contains possible conditional probabilities pcj j e computed consistent completions training set D variability intervals uncertainty missing data mechanism A conservative score derived strong dominance criterion 10 provides classiﬁcation rule require assumption missing data mechanism 218 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 Deﬁnition 3 Strong dominance score Given set q consistent posterior probability intervals Tpinf cj j e psupcj j eU deﬁne strong dominance score 8 sd cj j e D 1 pinf cj j e psupch j e h 6D j 0 pinf cj j e 6 psupch j e h 6D j The intervalbased classiﬁcation rule induced strong dominance score classiﬁes new case cj probability pinf cj j e larger probability psupch j e h 6D j Strong dominance safe criterion returns classiﬁcation obtain consistent completions training set D However probability intervals overlapping strong dominance score deﬁned face situation undecidability Moreover strong dominance score conservative condition pinf cj j e psupch j e h 6D j sufﬁcient yield classiﬁcation obtain complete training set known necessary In order increase coverage classiﬁer weaken criterion making minimal assumption missing data mechanisms equally possible making values intervals Tpinf cj j e psupcj j eU equally likely In way summarize interval average point deﬁning score sucj j e D psupcj j e cid0 k cid0 psupcj j e cid0 pinf cj j e cid1 D 1 cid0 kpsupcj j e C kpinf cj j e k chosen scores fsucj j satisfy properties Eq 13 Hence k D P P 1 cid0 h pinf ch j e hpsupch j e cid0 pinf ch j e A consequence consistency probability intervals Tpinf cj j e psupcj j eU extreme probabilities pinf cj j e psupcj j e probability pcj j e compute complete training set D relationship pinf cj j e 6 pcj j e 6 psupcj j e It follows j psupcj j e quantity k open interval 0 1 This ﬁnding guarantees score sucj j e interior interval Tpinf cj j e psupcj j eU consequently produce classiﬁcation rule correspond Eadmissible classiﬁcation rule compatible intervals Tpinf cj j e psupcj j eU 12 Note Hurwiczs OptimismPessimism criterionthe usual solution circumstances 1416does guarantee property As score sucj j e leads decision term completeadmissible score j pinf cj j e 6 1 6 P P Deﬁnition 4 Completeadmissible score Given set q consistent posterior probability intervals Tpinf cj j e psupcj j eU deﬁne quantity sucj j e D psupcj j e cid0 completeadmissible score psupcj j e cid0 pinf cj j e1 cid0 h pinf ch j e P hpsupch j e cid0 pinf ch j e P M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 219 It worth noting classiﬁcation based completeadmissible score sub sumes based strong dominance score pinf cj j e psupch j e h 6D j sucj j e j e h 6D j When condition apply strong dominance score hold completeadmissible score lets classify cases left unclassiﬁed strong dominance score This strategy result creased classiﬁcation coverage price lower accuracy 43 Which score Both strong dominance completeadmissible score provide sensible basis robust classiﬁcation Strong dominance safe price leaving cases unclassiﬁed completeadmissible score increases classiﬁcation coverage loosing robustness The choice intervalscoring method depends features problem hand section provide principled way choose best intervalbased classiﬁcation strategy A classiﬁcation typically evaluated basis classiﬁcation accuracy cid18 coverage cid13 The probability correctly classifying case probability classifying case Let cid18d cid13d respectively accuracy coverage RBC strong dominance score RBCd The accuracy cid18d independent missing data mechanism Similarly let cid18u accuracy RBC completeadmissible score RBCu The accuracy cid18u RBCu given components The ﬁrst component probability correctly classifying case use strong dominance score weighted coverage cid13d The second component probability correctly classifying case use strong dominance score weighted 1 cid0 cid13d Thus cid18u D cid18d cid13d C cid18ul1 cid0 cid13d 14 cid18ul classiﬁcation accuracy RBCu cases left unclassiﬁed RBCd term residual accuracy Residual accuracy provides measure gainloss classiﬁcation accuracy achieved RBCu relaxes strong dominance criterion increase coverage The decomposition Eq 14 provides ﬁrst basis choose scoring method For example simple rule adopt completeadmissible score cid18ul greater 1q cases left unclassiﬁed strong dominance score classiﬁed completeadmissible score better random The intuition rule accuracy valuable coverage prefer method classiﬁes randomly classiﬁes case The rationale expect consequence wrong classiﬁcation worse inability classify case This argument formally choose strong dominance completeadmissible score introducing misclassiﬁcation costs costs incurred inability classify case Suppose cost incurred able classify case attribute values e quantity Ci cost wrong classiﬁcation Cw Since event occurs probability 1 cid0 cid13d occurs probability 1 cid0 cid18d cid13d expected cost incurred RBCd CRBCd D Cw1 cid0 cid18d cid13d C Ci 1 cid0 cid13d 220 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 correct classiﬁcation yields cost On hand expected cost incurred RBCu achieving 100 coverage accuracy cid18u CRBCu D Cw1 cid0 cid18u In order minimize cost RBCd preferred RBCu CRBCd 6 CRBCu This true cid18u cid0 cid18d cid13d D cid18ul1 cid0 cid13d 6 1 cid0 cid13d 1 cid0 CiCw yields decision rule given theorem Theorem 4 Let Ci Cw denote respectively cost wrong classiﬁcation cost able classify case The interval based classiﬁcation rule uses strong dominance score yields minimum expected cost cid18ul 6 1 cid0 CiCw cid18ul accuracy RBCu cases left unclassiﬁed RBCd For example Ci D Cw best decision choose RBCd cid18ul 0 Compared simpler rule described decision takes account tradeoff accuracy coverage In practical applications quantities cid18d cid18u cid13d estimated available data cross validation shown section Suppose quantity cid18a accuracy NBCa trained incomplete data set assumption missing data mechanism For example cid18a accuracy NBC trained incomplete data set assumption data MAR We use decision rule help decide RBC strong dominance completeadmissible score yields minimum expected costs As byproduct decision rule interpreted evaluation consequences enforcing MAR assumption The comparison accuracy measures cid18a cid18u costindependent compare CRBCu D Cw1 cid0 cid18u CNBCa D Cw1 cid0 cid18a minimum expected cost achieved having highest accuracy If compare expected costs RBCd NBCa apply decision rule Theorem 4 NBCa preferred RBCd cid18ul 1 cid0 Ci Cw quantity 1 cid0 cid13d Tcid18ul cid0 1 cid0 Ci CwU cost incurred enforcing assumption missing data mechanism This solution easily extended cases mis classiﬁcation costs vary classes 5 Evaluation This section reports results experimental evaluation RBC incomplete data sets The aim evaluation compare performance RBC NBCs common solutions handle missing data 6 15 remove missing entries NBCm assign missing entries dummy state NBCcid3 Since data sets report classes case Theorem 1 NBCm faithful implementation MAR assumption NBCcid3 hand assumes knowledge missing data mechanism missing data treated category reported observed data M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 221 51 Materials methods The experimental evaluation conducted databases reported Table 1 available UCI Machine Learning repository 1 The database KDD99 consists 4704 cases 31 variables selected database 1999 KDD cup These databases offer variety different data types attributes database Voting record Vote binary attributes Breast Cancer Wisconsin BCancer Lung Cancer LCancer Bridge nominal attributes Hepatitis Mushrooms discrete example Annealing Credit Cylinder Horse Colic Sick offer good mixture continuous discrete nominal attributes The size databases ranges 32 case 56 attributes Lung Cancer 48842 cases 14 variables Mushrooms Continuous attributes discretized dividing observed range bins proportion entries Following current practice 8 compared accuracy classiﬁers running data set D 5 replicates 5fold cross validation experiment On database ran tests training NBC database missing entries removed NBCm assigning missing entries dummy state NBCcid3 strong dominance score RBCd completeadmissible score RBCu In cases computed estimates uniformly distributed global prior precision cid11 D 1 For test report values accuracyestimated average number cases correctly classiﬁed test setsand coveragegiven ratio number cases classiﬁed total number cases data set The 95 conﬁdence limits based Normal approximation proportion estimator 8 52 Results discussion Table 1 reports results The accuracy RBCd overall highest gain ranging 002 Breast Cancer 6 missing entries data set 699 cases 1677 Horse Colic data heavily missing Except Audiology Breast Cancer Lung Cancer accuracy gain RBCd statistically signiﬁcant cases shown non overlapping conﬁdence intervals This gain accuracy counterbalanced loss coverage small 651 Horse Colic The completeadmissible score increases coverage 100 price reducing accuracy Audiology Breast Cancer Credit outperformed standard NBC However difference accuracy sampling variability associated conﬁdence limits roughly probably data MAR data sets On hand accuracy gain RBCu NBCm NBCcid3 signiﬁcant data sets reaches 1019 Annealing data set conﬁrming potential danger wrongfully enforcing MAR assumption As noted Section 43 strong dominance score partitions data parts One comprises cases classiﬁcation ambiguity accuracy modeldependent The remaining comprises cases classiﬁed assumption missing data mechanism Using notation Section 43 accuracy cases systems achieving 100 coverage given quantity 222 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 Table 1 Accuracy NBCm NBCcid3 RBCd RBCu Maximum values reported boldface Database NBCm NBCcid3 RBCd RBCu 1 Adult 2 Annealing 3 Arythmia 4 Audiology Accuracy Accuracy Accuracy Coverage Accuracy 8174 cid6 023 8122 cid6 022 8651 cid6 021 8172 cid6 018 8250 cid6 020 8654 cid6 288 8088 cid6 332 9753 cid6 151 4912 cid6 487 9673 cid6 124 6440 cid6 225 6105 cid6 276 7609 cid6 325 3982 cid6 230 6619 cid6 233 5834 cid6 349 5550 cid6 351 6341 cid6 532 3478 cid6 348 5550 cid6 351 5 Automobile 6048 cid6 341 5805 cid6 345 6849 cid6 384 7122 cid6 316 6196 cid6 339 6 BCancer 9742 cid6 066 9742 cid6 066 9749 cid6 067 9965 cid6 523 9723 cid6 067 7 Bridge 8 Credit 9 Cylinder 6762 cid6 457 6476 cid6 466 8000 cid6 478 6667 cid6 460 6952 cid6 449 8488 cid6 130 8488 cid6 130 8748 cid6 172 9540 cid6 521 8470 cid6 131 7370 cid6 371 7300 cid6 374 9171 cid6 414 3130 cid6 697 7426 cid6 067 10 Echocardiogram 8723 cid6 294 8854 cid6 278 9358 cid6 235 8321 cid6 327 8854 cid6 278 11 HeartC 12 HeartH 13 HeartS 14 Hepatitis 5413 cid6 286 5380 cid6 286 5897 cid6 289 9571 cid6 116 5807 cid6 283 8333 cid6 200 8129 cid6 227 8588 cid6 211 8673 cid6 198 8367 cid6 211 3829 cid6 438 3659 cid6 434 4737 cid6 1145 1544 cid6 326 4228 cid6 445 8503 cid6 209 8516 cid6 208 9050 cid6 284 7645 cid6 973 8555 cid6 208 15 Horse Colic 7579 cid6 162 7579 cid6 163 9256 cid6 059 651 cid6 205 7773 cid6 161 16 KDD99 17 LCancer 8468 cid6 052 8480 cid6 050 8922 cid6 067 4542 cid6 070 8485 cid6 052 4375 cid6 1788 4375 cid6 1788 4667 cid6 1785 9375 cid6 866 4375 cid6 1788 18 Mushrooms 9853 cid6 012 9840 cid6 012 9904 cid6 015 9888 cid6 153 9870 cid6 011 19 Sick 20 Vote 9160 cid6 051 9087 cid6 053 9753 cid6 034 8630 cid6 229 9246 cid6 049 9002 cid6 105 9021 cid6 104 9205 cid6 175 9494 cid6 647 9021 cid6 104 cid18al D Ocid18a cid0 Ocid18d Ocid13d 1 cid0 Ocid13d Ocid18a estimated accuracy NBCm NBCcid3 RBCu Ocid18d Ocid13d estimated accuracy coverage RBCd Table 2 reports accuracy values NBCm NBCcid3 RBCu data sets experiment The sixth column reports maximum cost ratio Ci Cw RBCd best classiﬁcation terms minimum expected costs reference columns note proportion cases left unclassiﬁed RBCd size database If cost ratio higher reported value best highest accuracy cid18al reported bold face table In data sets BCancer Credit RBCd best choice Cw 444Ci Cw 145Ci respectively If conditions satisﬁed NBCm equivalently NBCcid3 best systems In BCancer data set completeadmissible score M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 223 Table 2 Residual accuracy NBCm NBCcid3 RBCu The sixth column reports maximum value cost ratio CiCw makes RBCd classiﬁcation minimum expected cost If cost ratio Ci Cw superior value corresponding boldfaced accuracy best choice The columns report percentage cases left unclassiﬁed RBCd database size Database 1 Adult 2 Annealing 3 Arythmia 4 Audiology 5 Automobile 6 BCancer 7 Bridge 8 Credit 9 Cylinder 10 Echocardiogram 11 HeartC 12 HeartH 13 HeartS 14 Hepatitis 15 Horse Colic 16 LCancer 17 KDD99 18 Mushrooms 19 Sick 20 Vote cid18ml 06040 07593 05666 05564 04066 07749 04286 03096 06448 05576 00000 06667 03663 06782 07462 00000 08090 04190 04892 05569 cid18cid3l 05757 06481 05100 05128 03221 07749 03428 03096 06549 06356 00000 05129 03462 06727 07462 00000 08112 05350 05424 05193 cid18ul 06457 09596 05964 05128 04580 02320 04856 02705 06631 06356 03799 06923 04135 06948 07670 00000 08121 06868 06052 05569 Ci Cw 1 cid0 cid13d 100 03543 00404 04036 04436 05420 02251 05144 06904 03369 03644 06201 03077 05865 03052 02330 10005 01878 03132 03948 04431 1828 5088 6018 6522 2878 035 3333 460 6870 1679 429 1327 8456 2355 9349 625 5458 122 1570 506 Size 48842 798 452 200 205 699 105 598 512 131 303 294 123 155 368 32 4704 8124 2800 435 performs poorly cases left unclassiﬁed strong dominance score enforcement MAR assumption allows standard NBC exploit information provided available data reaches accuracy 7749 This data set 6 cases missing entries In Credit data set NBCm NBCcid3 RBCu achieve accuracy lower 50 misclassiﬁcation cost lower 145Ci random assignment cases left unclassiﬁed RBCd preferable In Audiology RBCd minimum expected cost Cw 225Ci When condition cost ratio satisﬁed NBCm classiﬁcation adopt In data set LCancer accuracy cid18al null systems choice RBCd discussion This conﬁrmed fact maximum value cost ratio Ci Cw makes RBCd minimum expected cost 1005 Hence RBCd best Cw 0995Ci As data set medical nature 224 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 hardly imagine situation making automatic analysis costly making wrong In remaining data sets RBCu second best choice cost ratio CiCw superior value reported column table In data set Annealing example cost classifying case smaller 25 timesgiven 100404the cost wrong classiﬁcation RBCu best choice achieves accuracy 09596 cases left unclassiﬁed RBCd This gain 20 compared NBCm Again result conﬁrms MAR assumption data set negative effect accuracy A similar result shown Mushroom data set assigning missing entries dummy value enforcing MAR assumption yields essentially random classiﬁcation cases left unclassiﬁed RBCd use completeadmissible score rises residual accuracy 6868 The Sick data set reports similar result accuracy RBCu slightly superior NBCcid3 data sets Automobile Cylinder Hepatitis Horse Colic Vote data set These results suggest RBC based strong dominance criterion delivers highest accuracy risk decreased coverage The use completeadmissible score improves coverage decreasing accuracy appears achieve better results standard solutions proportion missing data small However consistently superior classiﬁer solution adopt needs account features data hand Nonetheless decision theoretic approach provides principled way choose appropriate solution 6 Conclusions This paper introduced RBC generalization standard NBC robust respect missing data mechanism The RBC performs training step incomplete data set resulting classiﬁcation quantiﬁed tight consistent probability intervals Then RBC classiﬁes new cases reasoning probability intervals We provided interval propagation algorithm identify bounds set classes posterior probabilities computed possible completions data scoring methods intervalbased classiﬁcation The choice scoring methods best suits problem hand based decisiontheoretic rule takes account costs misclassiﬁcation cost incurred able classify case extended costanalysis implications MAR assumption classiﬁcation accuracy The experimental evaluations showed gain accuracy achieved RBC compared standard solutions However results showed uniformly better classiﬁcation strategy data incomplete expect principled way choose solution best suits problem hand common practice real applications Although robust solution presented paper limited NBC straightforward extend treestructured classiﬁcation systems attributes binary classiﬁcation problem choose classes This training classiﬁer RBE computing bounds posterior probability classes 2U algorithm 5 The classiﬁcation M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 225 choosing intervalbased classiﬁcation rules presented principled way The extension general classiﬁcation models real challenge essentially requires development interval propagation algorithms returns loose bounds class posterior probability The methods described paper implemented program 1 distributed date 2000 copies Acknowledgements Authors grateful Paul Snow invaluable contribution develop ment completeadmissible score anonymous referees editor helpful suggestions References 1 C Blake E Keogh CJ Merz UCI Repository machine learning databases University California Irvine Department Information Computer Sciences 1998 2 L Campos J Huete S Moral Probability intervals A tool uncertain reasoning Internat J Uncertainty Fuzziness KnowledgeBased Systems 2 1994 167196 3 AP Dempster D Laird D Rubin Maximum likelihood incomplete data EM algorithm discussion J Royal Statist Soc Ser B 39 1977 138 4 RO Duda PE Hart Pattern Classiﬁcation Scene Analysis Wiley New York 1973 5 E Fagiuoli M Zaffalon 2U An exact interval propagation algorithm polytrees binary variables Artiﬁcial Intelligence 106 1998 77108 6 N Friedman D Geiger M Goldszmidt Bayesian network classiﬁers Machine Learning 29 1997 131 163 7 S Geman D Geman Stochastic relaxation Gibbs distributions Bayesian restoration images IEEE Transactions Pattern Analysis Machine Intelligence 6 1984 721741 8 R Kohavi A study crossvalidation bootstrap accuracy estimation model selection Proc IJCAI95 Montreal Quebec Morgan Kaufmann San Francisco CA 1995 pp 11461151 9 R Kohavi B Becker D Sommerﬁeld Improving simple Bayes M van Someren G Widmer Eds Poster Papers ECML97 Charles University Prague 1997 pp 7887 10 HE Kyburg Rational belief Behavioral Brain Sciences 6 1983 231273 11 P Langley W Iba K Thompson An analysis Bayesian classiﬁers Proc AAAI92 San Jose CA AAAI Press Menlo Park CA 1992 pp 223228 12 I Levi On indeterminate probabilities J Philos 71 1974 391418 13 RJA Little DB Rubin Statistical Analysis Missing Data Wiley New York 1987 14 M Pittarelli An algebra probabilistic databases IEEE Transactions Knowledge Data Engineering 6 2 1994 293303 15 JR Quinlan C45 Programs Machine Learning Morgan Kaufmann San Francisco CA 1993 16 M Ramoni Ignorant inﬂuence diagrams Proc IJCAI95 Montreal Quebec Morgan Kaufmann San Francisco CA 1995 pp 18081814 17 M Ramoni P Sebastiani Bayesian methods M Berthold DJ Hand Eds Intelligent Data Analysis An Introduction Springer New York 1999 pp 129166 18 M Ramoni P Sebastiani Robust learning missing data Machine Learning 2000 appear 19 RT Rockafellar Convex Analysis Princeton University Press Princeton NJ 1970 1 Available Bayesware Limited wwwbayeswarecom 226 M Ramoni P Sebastiani Artiﬁcial Intelligence 125 2001 209226 20 S Russell J Binder D Koller K Kanazawa Local learning probabilistic networks hidden variables Proc IJCAI95 Montreal Quebec Morgan Kaufmann San Francisco CA 1995 pp 1146 1151 21 DJ Spiegelhalter RG Cowell Learning probabilistic expert systems J Bernardo J Berger AP Dawid AFM Smith Eds Bayesian Statistics 4 Oxford University Press Oxford UK 1992 pp 447 466 22 DJ Spiegelhalter SL Lauritzen Sequential updating conditional probabilities directed graphical structures Networks 20 1990 157224