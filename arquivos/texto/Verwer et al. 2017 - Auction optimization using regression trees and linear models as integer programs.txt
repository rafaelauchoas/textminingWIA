Artiﬁcial Intelligence 244 2017 368395 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Auction optimization regression trees linear models integer programs Sicco Verwer Yingqian Zhang b Intelligent Systems Department Delft University Technology The Netherlands b Department Econometrics Erasmus University Rotterdam The Netherlands Qing Chuan Ye b r t c l e n f o b s t r c t Article history Received revised form 14 May 2015 Accepted 21 May 2015 Available online 29 May 2015 Keywords Auction design Machine learning Optimization Integer linear programming Regression In sequential auction multiple bidding agents problem determining ordering items sell order maximize expected revenue highly challenging The challenge largely fact autonomy private information agents heavily inﬂuence outcome auction The main contribution paper twofold First demonstrate apply machine learning techniques solve optimal ordering problem sequential auctions We learn regression models historical auctions subsequently predict expected value orderings new auctions Given learned models propose types optimization methods blackbox bestﬁrst search approach novel whitebox approach maps learned regression models integer linear programs ILP solved ILPsolver Although studied auction design problem hard proposed optimization methods obtain good orderings high revenues Our second main contribution insight internal structure regression models eﬃciently evaluated inside ILP solver optimization purposes To end provide eﬃcient encodings regression trees linear regression models ILP constraints This new way learned models optimization promising As experimental results signiﬁcantly outperforms blackbox bestﬁrst search nearly settings 2015 Elsevier BV All rights reserved 1 Introduction One main challenges mathematical optimization construct mathematical model describing properties When structure fully determined knowledge hand machine learning data mining techniques optimization instead knowledge They example order obtain decision values 1 ﬁtness functions 2 model parameters 3 Models learned data frequently blackbox manner predictions learned models internal structure It possible use models whitebox manner instance order determine search space cuts parameter bounds Neural networks way model unknown relations constraint programming 4 In paper develop whitebox optimization method regression models integer linear programming map entire models sets variables constraints solve shelf solver Corresponding author Email addresses SEVerwertudelftnl S Verwer yqzhangeseeurnl Y Zhang yeeseeurnl QC Ye httpdxdoiorg101016jartint201505004 00043702 2015 Elsevier BV All rights reserved S Verwer et al Artiﬁcial Intelligence 244 2017 368395 369 This whitebox method proposed blackbox method provides solution optimization problem key artiﬁcial intelligence operations research communities auction design We brieﬂy introduce problem domain going details methods 11 Sequential auction design Auctions increasingly popular allocating resources items businesstobusiness businessto customer markets Often sequential auctions 5 adopted practice items sold consecutively bidders Sequential auctions particular desirable number items sale large ﬂower auctions 6 buyers enter leave auction dynamically online auctions 7 In sequential auction auctioneer tune auction parameters inﬂuence outcome auction reserve prices items order sell In words design auctions purpose achieving predeﬁned goal In paper solve speciﬁc auction design problem deciding optimal ordering items sell sequential auction order maximize expected revenue OOSA short We assume bidders auctions budget constrained This highly relevant problem todays auctions bidders limited budget seen instance industrial procurement 8 Previous research shown presence budget constraints revenue collected auctioneer heavily dependent ordering items sell 911 This holds toy problem 2 items Let use simple example illustrate importance ordering cases Example 1 Two agents A1 A2 sequential auction items For sale items r1 r2 Suppose items sold means ﬁrstprice English auction1 Assume reserve prices lowest prices auctioneer willing sell times items 1 The agent A1 agent A2 willing pay items ν1r1 10 ν1r2 15 ν2r1 12 ν2r2 10 Furthermore budgets A1 A2 15 25 respectively We assume simple bidding strategy example The agents bid myopically item highest bid item lower value willing pay remaining budget The auctioneers goal maximize total sold price items Consider situation auctioneer sells ﬁrst r2 r1 A1 r2 overbids A2 11 r1 auctioned A1 bids maximally 4 budget limit A2 win item price 5 The total revenue 16 However selling sequence r1 r2 A2 win r1 bid 11 A2 win r2 price 11 The collected revenue 22 case cid2 Most current approaches ordering problem sequential auctions assume restricted market environ ment They study problem ordering items 1112 market homogeneous bidders 13 To best knowledge ﬁrst consider order items realistic auction settings hetero geneous bidders competing different items This problem highly complexa good design ordering needs care uncertainties For instance order evaluate revenue given ordering opti mization algorithm needs know bidders budgets preferences items usually private unshared Furthermore large variety possible bidding strategies bidders use auctions unknown This auction design problem typical example mathematical optimization model fully determined machine learning data mining techniques come play This exactly approach builds 12 Learning models whitebox blackbox optimization Nowadays auctions utilize information technology makes possible automatically store tailed information previous auctions selling sequences selling price auctioned item Our approach solving problem optimal ordering sequential auctions starts historical auction data We deﬁne compute relevant features use learn regression trees linear regression models expected revenue Given models propose approaches ﬁnd optimal ordering new set items 1 bestﬁrst search uses models blackbox evaluate different orderings items 2 novel whitebox optimization method translates models set items mixedinteger program MIP runs ILPsolver CPLEX Fig 1 displays general framework approaches optimization methods Just like traditional blackbox optimization approach 1415 bestﬁrst search ignorant internal structure models calls perform function evaluations predicting revenue ordering items Optimization possible means search procedure uses heuristics produce new orderings depending previously evaluated ones Our bestﬁrst search makes use dynamic programming cuts inspired sequential decision making order reduce search space 1 The English auction consider starting price reserve price bidders bid openly Each subsequent bid higher previous bid item sold highest bidder price equal bid 370 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 One main contributions paper realization learned regression models evaluated eﬃciently inside modern mathematical optimization solvers This evaluation includes computation feature values input machine learning evaluation features learned model output machine learning possible feedback evaluations new features In paper eﬃciently translate steps types learned models regression trees linear regression models mixedinteger constraints The resulting mixedinteger program evaluated modern integer linear programming ILP solver In way modern exact solvers instead heuristic search These solvers use advanced branchandbound methods cut search space compute optimize dual solution prove optimality testing possible solution This main beneﬁt whitebox method blackbox The downside learned model complex whitebox method lead large mathematical model diﬃcult optimize We compare approaches investigate tradeoff applying OOSA problem Contributions organization Although use sequential auction design illustrate method constructions general applied optimization setting unknown relations represented regression models learned data The constraint whitebox method feature values need computable integer linear functions intermediate solutions Our approach applied complex optimization settings entire orders schedules plans need constructed We list main contributions follows We demonstrate apply regression methods machine learning OOSA We eﬃcient encoding regression trees linear regressors MIP constraints We prove OOSA budget constrained bidders NPhard regression models We provide ﬁrst method tackles OOSA realistic settings We demonstrate experimentally whitebox methods outperform blackbox methods models overly complex In Section 2 formally introduce problem optimal ordering sequential auctions OOSA learn regression models historical auction data Section 3 standard machine learning methods Based learned models whitebox optimization method blackbox optimization introduced ﬁnd optimal ordering OOSA Section 4 Extensive experiments presented Section 5 compare performance proposed optimization methods learned models auction simulator Before conclude compare discuss related works Section 6 2 Optimal ordering sequential auctions OOSA We assume ﬁnite set bidders agents Let R r1 rl denote collection item types quantity item type 1 When clear context slightly abuse notation use S r1 r2 r1 denote multiset available items Each bidder agent valuation preference type item v R R In addition agent budget bi purchasing items desires win items auctioned budget limit In auction set n items S type set R cid4 R auctioned sequentially predetermined order We use s1 s2 sn denote ordering For example given types r1 r2 quantities 1 2 respectively possible orderings items s1 r1 s2 r2 s3 r2 s1 r2 s2 r1 s3 r2 s1 r2 s2 r2 s3 r1 For r j auctioned agent Ai puts bid r j minimum willing pay r j remaining budget We point case unconstrained budget maximum agent willing pay r j deﬁned νir j equal valuation v ir j Each item r j comes reserve price lowest price auctioneer willing sell r j If received bids reserve price r j r j sold Otherwise agent bids highest r j wins r j The winners items transfer payment auctioneer depending auction rule For example ﬁrstprice auction winner pays equal bid secondprice auction pays second highest bid reserve price item higher The revenue auctioneer sum total payment sold items total reserve values unsold items This sequential auction ends items auctioned agents run respective budgets We assume auction repeated time auction sells possibly different items S At end auction following information disposal 1 ordering auctioned items 2 allocation items agents payments The optimization problem study given set items budget constrained bidders ﬁnding optimal ordering items sequential auctions expected revenue maximized We problem OOSA We decision version optimization problem NPhard complete information bidders preferences strategic bid truthfully according preferences S Verwer et al Artiﬁcial Intelligence 244 2017 368395 371 Theorem 1 Given set items S preferences v S R exists ordering obtains revenue K R budgets bi bidder The problem deciding NPhard cid2 cid2 cid2 Proof By reduction wellknown NPhard partition problem 16 Given set integers I i1 I B We need bidders preferences v 1rk 2 ik dividable sets A B v 2rk 2 ik 1 1 k n The reserve price item k 1 k n ik The agents budgets b1 1 I 2 b2 I We claim I partitionable sets equal sums exists ordering obtains revenue K I There n items S K 3 2 A cid2 Given partition I sets A B sell items A ﬁrst B later In case agent 2 buy items A price 2 ik minimal bid win items agent 1 After buying items A agent 2 spent 2 I This entire budget agent 2 I total I This makes total All items B sold agent 1 reserve price ik Thus agent 1 pays revenue ik A ik makes cid2 ikB ik 1 ik A ik 1 I K agent 2 wins ﬁrst set items A costing 2 ik till uses budget Thus 2 cid2 Suppose ordering agent 1 2 spend budget K total This means I I Hence I Agent 1 pays ik remaining items B B I A uses money k A ik cid2 I 3 2 I 1 2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 2 2 k A ik 1 2 kB ik 1 2 partition I The construction clearly polynomial time cid2 cid2 cid2 A B Several related works deal type ordering optimization problem For example authors 11 investigate optimal ordering strategy case auctioneer items sell They items different value higher valued items auctioned ﬁrst order increase sellers revenue Pitchik 12 points presence budget constraints sealedbid sequential auction bidder wins ﬁrst good higher income expected revenue maximized These greatly simpliﬁed auction settings possible derive bidders equilibrium bidding strategies With assumptions distributions bidders budgets preferences optimal ordering theoretically derived However realworld auctions complex uncertain terms sizes itemsbidders agents preferences bidding strategies existing results applied In paper instead focus learning overall behaviors group bidders historical auction data machine learning techniques ﬁrst step solving OOSA In order apply ML techniques assume sequential auction set participating bidders char acteristics preferences budgets bidding strategies similar This simpliﬁes problem learning good ordering Instead learning individual valuationsbudgetbidding strategies agents treat agent population sin gle entity need ﬁnd single global function Obviously approach fail agents radically different auction However consider assumption sensible auctions industrial procurement auctions companies repeatedly join auctions similar interests Dutch ﬂower auction different bidders day seldom occurs day bidders interested roses day want tulips Although different participants interested different item types interests group participants remain stable 3 Learning predictive models OOSA At end sequential auction following information disposal 1 ordering auctioned items 2 price sold item Before build optimization model solve OOSA problem need ﬁnd suitable way model expected revenue given orderings auctioned items An ordering thought sequence items However best knowledge existing sequence models ﬁt auction setting Section 62 In work view prediction auctions outcome regression problem We split problem subproblems predicting value auctioned items We sum obtain overall objective function expected revenue P S given set S S n items P S cid3 1kn Gsk s j j k sl k l Gsk J L regression function determines expected value sk given J auctioned L auctioned The main beneﬁt representation modern machine learning methods learn function G data In addition item sold represents single sample auction contains samples learning reducing required data We study popular regressions functions 372 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 31 Two regression functions In paper use regression trees 17 absolute shrinkage selection operator LASSO 18 regression functions train features based items auctioned current item We ﬁrst brieﬂy introduce regressors Regression trees Regression trees form decision trees predicted values real numbers A decision tree popular predictive models mapping feature values target value It treeshaped graph root node interior nodes leaf nodes The root interior node contains Boolean test speciﬁc feature value f f 5 Every leaf node contains output value p It maps feature values output performing tests path root leaf For test performed outcome true f 5 path continued left branch outcome false f 5 path continued right branch Once leaf reached outputs value contains p A regression tree learner aims ﬁnd tree minimizes mean squared error predicted actual observed values Most regression tree learning algorithms follow greedy strategy splits interior nodes long decrease error signiﬁcant A split replaces leaf node interior node connected new leaf nodes The interior node receives Boolean constraint minimizes meansquared error resulting tree leaf nodes predict mean value observed data values end leaf mapping data samples leaf nodes LASSO LASSO method constructing linear regression function p f 1 fm c1 f 1 c2 f 2 cm fm d p value predict ci constants f feature values d intercept The standard approach ﬁnd function minimize mean squared error easy compute LASSO popular regularized version simple estimation penalizes absolute values regression coeﬃcients c1 cm Formally given dataset target values pd 1 d k k number samples uses convex optimization order features f d ﬁnd regression function solves following problem2 min cid3 1dk 1 2 k p f d 1 f d m pd2 α cid3 1im ci 0 α 1 parameter effect regularization Intuitively larger α larger penalty having large coeﬃcients ci Consequently larger value α drive coeﬃcients zero LASSO useful method correlated feature values ordinary squares model overﬁt values We use LASSO regression zero coeﬃcients implies need compute feature values order evaluate learned model positive effect optimization performance discuss Section 4 32 Learning regression functions predicting revenues We ﬁrst overview Fig 1 connection regression models optimization models solving OOSA Given historical auction data regression tree LASSO linear regression function learned item type The regression tree LASSO evaluate values selling different items based feature values computed given ordering items The learned regression trees LASSO functions ways model optimization problem OOSA 1 Blackbox optimization In paper use bestﬁrst search heuristic come orderings items use learned regression trees LASSO compute expected revenue orderings 2 Whitebox optimization We formulate optimization problem ﬁnding optimal ordering mixed integer linear program MIP shown automatically constructed learned regression tress LASSO functions We present details learning regression trees LASSO functions item types Currently provide following features regression models Feature 1 sold For item type r r items auctioned Feature 2 remain For item type r r items auctioned Feature 3 diff For pair item types r r cid4 difference r r cid4 items auc tioned Feature 4 sum For item type r value obtained auctioning r items overall sum Feature 5 index For item index auctioned Other sequential features sliding windows Ngrams 20 course added model However whitebox method computes values inside ILP solver requirement 2 This version implemented scikitlearn Python package 19 use learn models S Verwer et al Artiﬁcial Intelligence 244 2017 368395 373 Fig 1 Solving OOSA whitebox optimization blackbox optimization learned models Blackbox optimization calls predictive model evaluate possible orderings Whitebox optimization translates internal structure predictive model MIP constraints Table 1 The data set created past auctions r2 r1 r1 r2 Example 2 Type Value soldr1 soldr2 remainr1 remainr2 diffr1r2 sumr1 sumr2 sum index r2 r1 r1 r2 11 5 11 11 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 11 0 11 0 0 0 11 0 11 1 2 1 2 Fig 2 Two learned regression trees item types r1 r2 Example 2 The leaves left right tree output predicted value item type r1 r2 determined feature value soldr2 represented integer linear formulation Although diff feature determined ﬁrst add convenience learning regression tree requires nodes represent values The inﬂuence budget constraints directly modeled fourth feature paid r1 items reaches certain learned bound expect agents want r1 items budget Although exists indirect relation budget constraints ﬁrst features including beneﬁcial easier compute If regression model features reduce time needed solve auction design problem For similar reasons add feature Below example ordering obtained values transformed data set 5 types features Example 2 Consider setting Example 1 Assume auctions carried One sold r2 ﬁrst r1 The reversed As shown Example 1 ﬁrst auction obtain revenue 16 second auction receive 22 We compute feature values auctions depicted Table 1 Subsequently learn regression trees item types r1 r2 shown Fig 23 After learning regression trees optimize ordering new unseen multiset items r1 r1 r1 r2 trying orderings choosing maximum expected revenue r1 r1 r1 r2 gives 11 11 11 11 44 r1 r1 r2 r1 gives 11 11 11 5 38 r1 r2 r1 r1 gives 11 11 5 5 32 r2 r1 r1 r1 returns 11 5 5 5 26 Hence optimizer choose schedule r2 item r1 items cid2 3 The learned linear regression model straightforward Hence skip example 374 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 The example showed evaluate different orderings items learned regression trees In general trying possible orderings impossible multiset items S r1 rm m types total unique orderings blows quickly cid4 S ri ri S 1im 33 Modeling power tradeoff Our method regression modeling allows use regression method machine learning predicting unknown quantities optimization objective values parameters In addition regression function G uses values proposed solution input J L instead external parametersdata learned regression model represents unknown relations different values solution The model answers question What value X given Y opposed What value X answered ﬁtting model parameters Answering ﬁrst question allows interesting possibilities For instance use stochastic optimization ﬁtted parameters produce schedule use regression models predict effect schedule parameters use stochastic optimization newly estimated parameters This way use machine learning tools plan ahead Using whitebox method single optimization software Section 4 This loopback functionality provides lot power method comes risk Every time predictive models probability predictions inaccurate When loopback possible inaccura cies inﬂuence future predictions depend These future predictions inaccurate predicted overall objective value potentially diverge true value These cascading inaccuracies issue added modeling power makes We use sum feature relates predicted value predictions earlier auctioned items This feature important predicting budget constraints consequently regression models produce predictions 4 Whitebox blackbox optimization OOSA Given predictive models expected value item straightforward compute good ordering showed Example 2 For given ordering predict individual revenues items regression model sum obtain revenue ordering However testing possible orderings choosing highest revenue long time For instance want order 40 items 8 types experimental setting Section 5 5 type need test 40 58 In Appendix A provide hardness results demonstrate little hope P NP ﬁnding eﬃcient polynomialtime algorithm gives optimal ordering regression tree linear regression predictor In general better performing guided search space formed possible orderings We present searchbased optimization methods 1 novel whitebox optimization ILP model 2 blackbox heuristic bestﬁrst search 19 1031 possible unique orderings 41 Whitebox optimization ILP model Given regression tree linear regression models expected value item type automatically formulate problem ﬁnding optimal ordering mixed integer linear program MIP We discuss encoding se quential auction feature values objective function translating learned models regression tree linear regression respectively Ordering auction Given multiset S n items set possible types R use following free variables encode possible ordering S xir 0 1 Item type r xir 1 Thus x3r1 equal 1 means auctioned item type r1 We require index item type auctioned total number auctioned items type r equal number nr type r items S cid3 rR cid3 1in xir 1 1 n xir nr r R 1 2 Any assignment ones zeros x variables satisﬁes types constraints corresponds valid ordering items S The value ordering determined learned regression models S Verwer et al Artiﬁcial Intelligence 244 2017 368395 375 Translating feature values In order compute prediction regression model need translate models ILP constraints values features models4 Feature 1 2 3 5 computed linear functions x variables cid3 soldir x jr 1 n r R ji diffirrcid4 soldir soldircid4 cid3 remainir x jr ji indexi 1 n 1 n r R 1 n r r cid4 R r cid11 r cid4 3 4 5 6 For fourth type feature use additional variable p jr encodes expected value item auctioned index j type r If item index j type r p jr equal 0 Since p variables predictions regression functions provide deﬁnition deﬁning regression models sumir cid3 1 ji p jr 1 n r R 7 To aid ILP solver precompute minimum m f maximum M f obtainable values feature f index provide bounds solver Constructing objective function We aim maximize expected values pir cid3 cid3 max pir 1in rR 8 Although possible compute objective function sum values large sums x model variables described specifying parts sums intermediate continuous p variables signiﬁcantly reduces encoding size computation time Finally discuss encode learned regression tree linear regression model constraints ILP model Encoding regression trees We translate regression tree models ILP carefully constructed linear functions Our encoding requires new set 0 1 variables zilr representing leaf node l reached item type r index The internal decision nodes trees represented implicitly constraints new z variables Intuitively encode z variable false binary test parent nodes fails By additionally requiring exactly z variable true index fully encode learned regression trees Let Dr set decision nodes regression tree type r Every decision node Dr contains boolean constraint f c true feature f value equal constant c A key insight encoding boolean constraint directly inﬂuences value z variables true index z variables representing leafs right subtree false false represent leafs left subtree false In way require constraints boolean constraint order represent possible paths leaf nodes fv f M f c fv f m f c cid3 lL cid3 lLcid4 zilr M f 1 n r R f c Dr zilr m f 1 n r R f c Dr 9 10 calculation feature value f leaf nodes left right subtrees fv f index L L decision node constraint f c regression tree type r M f m f maximum minimum values feature f index For feature calculation simply replace fv f righthand sides corresponding feature deﬁnitions5 cid4 The constraints ensure zilr obtains value 1 binary test parent nodes path l tree type r return true index By construction regression trees ensures z 4 Including transformations applied 5 We ignore possibility feature f decision node obtained f changing behavior equal c features limited precision replace constants 376 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 variable true type r index We require exactly z variable true index6 This z type item sold index denoted x variables cid3 l zilr xir all1 n r R 11 This completes encoding regression trees The predictions trees index given z variable true index We multiply z variable constant prediction leaf node represents obtain prediction store p variables compute sum feature values pir cid3 lLr clr zilr 1 n r R clr constant prediction leaf l tree type r 12 Complexity Our translation regression trees eﬃcient It requires 2 constraints decision node Dr Equa tions 9 10 1 binary variable zilr leaf node L L Equations 9 10 11 tree To encode complete depth k tree 2k1 1 nodes requires 2 2k 1 2k1 2 constraints decision nodes 2k binary variables leaf nodes In addition require 1 constraint force exactly leaf vari able true type xir Equation 11 All variables constraints compute feature values pir variables computed directly storing result variable Consequently adds zero variables zero constraints translation cid4 In order encode entire OOSA problem new set constraints representing tree constructed item type item index In problem R types n items sale creates n R 2k1 1 constraints n R 2k variables encode complete tree depth k The ordering problem requires n R xir Equations 1 2 variables n R constraints This totals n R 2k 1 variables n R 2k1 1 n R constraints Since complete depth k tree 2k1 1 nodes linear number nodes tree We discuss encode linear regression model Encoding linear regression model Due linear nature implementing linear regression ILP straightforward We directly compute value p variables linear predictor function pir cid3 f Feat c f r fv f 1 n r R 13 Feat set features fv f feature f s values index c f r constant coeﬃcient feature f regression function type r The somewhat diﬃcult index regression function change depending auctioned item type r We implemented choice indicator functions CPLEX7 This changes formulation follows xir 1 pir cid3 f Feat c f r feat f 1 n r R xir 0 pir 0 1 n r R 14 15 It states xir true values pir determined regression function Otherwise value 0 These constraints needed fully implement linear regression function When LASSO regularization coeﬃcients receive value 0 These removed encoding making models smaller easier evaluate solver Complexity The translation regression functions straightforward requires 2 constraints item type indicator functions Equations 14 15 Because indicators need encode n Rpir variables No additional constraints variables required In order encode entire OOSA problem require n R 2 constraints n R 2 variables 6 Counterintuitively occur objective function discussed maximized z variable false index If small sum needed reach high revenue prediction beneﬁcial auction sell item 7 Many solvers similar constructions If constraints implemented bigM formulation similar use determine value z variables regression tree formulation S Verwer et al Artiﬁcial Intelligence 244 2017 368395 377 An example Now ILP models complete ready solve OOSA problem We following example illustrate formulation ILP works given learned regression trees Example 3 Given learned trees Example 2 suppose asked order new multiset items r1 r2 r2 We translate new set learned trees following integer linear program following 0 1 decision variables 1 3 xir1 xir2 zi1r1 zi2r1 zi1r2 cid3 max pir1 pir2 1i3 pir1 11zi1r1 11zi1r2 pir2 subject 1 3 5zi2r1 x1r1 x1r2 x2r1 x2r2 xir1 x3r1 x3r2 xir2 1 2 1 This denotes exactly x variable true index 2x variables true item type r2 1 type r1 This encodes possible orderings From compute feature values 1 3 soldir1 soldir2 x1r1 x1r2 xi1r1 xi1r2 constraints denoting Boolean tests internal nodes soldir2 100 05zi1r1 05zi2r1 soldir2 100 0 Msoldi 100 msoldi 0 The ﬁrst constraints encode zi1r1 soldir2 leaf succeed At require exactly z variable true index 1 05 Thus z variable true leaf Boolean tests internal nodes path root 05 zi2r1 1 soldir2 zi1r1 zi2r1 zi1r2 xir1 xir2 A satisfying assignment x variables x1r1 x2r2 x3r2 set 1 rest 0 corresponding ordering r1 r2 r2 1 Since sold1r2 1 This implies z11r1 results p1r1 1 For index x2r2 11 objective value 33 cid2 1 Similarly obtain z31r2 0 leads 995z11r1 100 05z12r1 1 forces z21r2 0 forcing z12r1 0 Since z11r1 11 p2r2 11 p3r2 z12r1 x1r1 x3r2 42 A blackbox heuristic bestﬁrst search algorithm We provide blackbox heuristic solving ordering problem 21 The traditional method overcome computational blowup caused sequential decision making use dynamic programming method Although lessens computational load combining different paths lead sets auctioned items search space large waiting solution long Instead employ bestﬁrst search strategy terminated anytime order return best solution far We bestﬁrst search strategy works Algorithm 1 The algorithm uses hashtable priority queue The hashtable exclude possibility visiting nodes twice obtained value like dynamic programming method These dynamic programming cuts sensible lose optimality rare occasions better sell earlier items leaving budget remaining ones The priority queue provides promising candidate nodes bestﬁrst strategy By computing random orderings remaining items learned models evaluate complete orderings items The best stored returned algorithm terminated Unfortunately result admissible heuristic A search procedure Hence algorithm pops solution queue necessarily optimal In experience random orderings remaining items heuristic provides good spread search space Although nodes unlucky obtain bad ordering remaining items multiple ways reach nodes search space unlikely possibilities unlucky 378 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 Algorithm 1 Blackbox heuristic solving OOSA bestﬁrst search Require A set items S historical data orderings values D maximum number iterations m Ensure Returned good high expected value ordering Transform D data set item type r Learn regression model D predicting value item type r end Initialize hashtable H priority queue Q Add data row Q Q size H m Pop row features F highest value p Q H contain F value p Add F value p H Let L set remaining items F item type r items L cid4 random ordering L ik Let ik item Type r L Let L Use models evaluate value p Create new features F Add F end Q value p p cid4 cid4 cid4 auctioning ik F cid4 auctioning ordering ik L cid4 F end end return The highest evaluated ordering 43 Discussion whitebox blackbox optimization The main difference abovementioned approaches Fig 1 whitebox method speciﬁes predictors entirely constraints infer bounds predictions cut search space The blackbox method instead uses predictors oracles ignorant predictions naturally eﬃcient compute infer search space cuts deduce ordering better testing Another key difference whitebox method results single optimization model run modern solver blackbox method requires use executable code produce predictions In blackbox setting harder use powerful solving methods available dedicated solvers problems integer linear optimization ILP satisﬁability SAT constraint programming CP Instead general search methods bestﬁrst search beamsearch metaheuristics genetic algorithms Both blackbox whitebox approaches advantages The main advantage blackbox performance large independent complexity regression model In contrast explicitly modeling regression model constraints whitebox complex regression models lead constraints dramatically increase time needed solve Another advantage blackbox optimization easy include additional cuts dynamic programming cuts discussed Such cuts added constraints LP formulation lead blowup runtime The main beneﬁt whitebox approach use modern exact solvers instead heuristic search These solvers use advanced branchandbound methods cut search space compute optimize dual solution prove optimality testing possible solution Our whitebox constructions easily integrated existing ILP formulations wide range applications instance Operations Research In way combine vast expert knowledge available applications knowledge readily available data The important downside whitebox evaluation translated models likely requires time running code blackbox especially models features somewhat complex In opinion advantages whitebox optimization largely outweigh blackbox optimization interesting topic research machine learning optimization 5 Experiments Designing optimal ordering sequential auctions diﬃcult heterogeneous bidders value items differently different budget constraints bid rationally irrationally bidding strategies To evaluate performance proposed optimization methods ideally collect real auction data build optimization models run realworld auctions real bidders different ordering items produced different methods compare resulting revenues Since evaluation method feasible main purpose paper opted widely accepted evaluation approach research community created S Verwer et al Artiﬁcial Intelligence 244 2017 368395 379 Fig 3 Our framework evaluating optimization learned models OOSA There simulator ways generate historic data evaluate OOSA solutions A train set unseen problem instances generated The train set learn regression models Random orderings problem instances test The instances learned models provided input ILPbased whitebox optimization bestﬁrst blackbox optimization The resulting orderings evaluated learned models simulator runs auction simulator simulates auctions agents We simulator generate auction data sets evaluate proposed method An overview process given Fig 3 51 The simulator Simulating auctions We simulate sequential auction settings simulator ﬁrst price auctions agents bid lower value willing pay item higher valuation item remaining budget 11 ii second price sealed bid auctions Vickrey auctions 22 Agents bid truthfully item round based valuations case insuﬃcient budget bid remaining budget 7 This bestresponse bidding strategy myopic utilitymaximizing agents consider current round auction8 iii Vickrey auctions agents bid smartly compare utility obtained end auction buying buying item place bid based difference Section 54 details On auctioned item bid truthfully budget allows9 Otherwise bid remaining budget Given bids item highest bid wins If multiple agents highest bid selected winner uniformly random With different auction settings intend method robust auction rules bidding strategies Below explain generated agents items settings parameters At end chapter method scales use different parameters generator Item types We use given set 8 items initialize auction simulator Every type ri gets assigned base value μi 25 5 1 8 reserve price ρi 1 2 25 5 Every type assigned popularity sparsity values denoted γi λi drawn uniformly 2 10 The popularity value measures degree desirability item type agents The sparsity measure frequency item type available auction In auction 40 items generated roulette wheel drawing scheme sparsity values Bidder agents The simulator starts 20 randomly generated bidding agents Every agent A j gets assigned bud b j 25 150 uniformly random They desire 1 5 8 item types popular types 8 Note sequential Vickrey auctions budget constrained agents truthtelling equilibrium bidding strategy 23 9 Vickrey 22 showed sequential auction unlimited budget weakly dominant strategy bidders bid true values auctioned item 380 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 higher probability selected drawn roulette wheel selection item types popularity values Every desired item type ri assigned agent A j given value ν jri β μi μi base value type ri β uniform random value 05 20 The value ν j ri agent A j willing pay ri ﬁrstprice auctions valuation A j ri secondprice auctions If value greater budget agent agents budget increased value 25 150 sampled uniformly adding budget This repeated budget suﬃcient item type Example 4 The following example agents A1 A8 item types r1 r2 r3 r4 generated smallscale experiments Section 551 item types reserve prices auctioneer ρ1 125 ρ2 15 ρ3 175 ρ4 20 sparsity values item types λ1 2 λ2 7 λ3 2 λ4 5 popularity values γ1 8 γ2 8 γ3 6 γ4 2 Every agents budget sampled 25 80 budget b νr1 νr2 νr3 νr4 A1 78 34 59 74 A2 37 24 30 A3 80 20 61 22 A4 60 41 21 30 A5 119 53 A6 103 38 58 A7 46 24 25 A8 63 42 In generated agents clearly item types popular Item type r4 popular desired agents caused low popularity value 2 The valuations sampled uniformly base values Agent A5 budget greater 80 budget resampling Five examples generated item sequences corresponding revenues index item price item price item price item price item price 1 r2 59 r2 59 r1 41 r3 61 r2 59 2 r4 22 r2 58 r1 38 r4 74 r3 61 3 r1 41 r2 45 r4 74 r1 41 r2 58 4 r2 58 r4 22 r4 22 r3 53 r4 20 5 r2 45 r4 22 r2 58 r4 20 r3 53 6 r1 24 r4 22 r4 22 r4 20 r3 53 7 r2 42 r2 42 r2 42 r4 20 r1 41 8 r4 22 r4 20 r2 30 r4 20 r3 19 9 r4 22 r2 30 r4 22 r3 53 r2 45 10 r2 25 r4 20 r2 25 r4 20 r3 19 11 r2 21 r4 20 r2 20 r2 58 r2 42 12 r4 20 r2 25 r4 20 r1 38 r4 20 13 r2 21 r2 21 r1 21 r2 42 r4 20 14 r2 19 r1 39 r2 21 r1 24 r3 175 15 r3 53 r4 20 r2 19 r3 19 r2 30 In sequences items type r1 r3 occur frequent low sparsity values cid2 Training For given set 20 random bidders simulator generates 1000 historical auctions The 40 items auctions generated scheme ordered randomly These items run simulator agents use abovementioned bidding strategies decide value bid order determine winners item selling prices The total selling price items sequential auction collected revenue For 20 generated bidders ﬁrst experiment effect different item orderings collected revenue trying 100 random orderings comparing smallest median largest collected revenue If difference largest smallest tenth median revenue 20 new bidders generated This process repeated ﬁnd set agents passes check typically occurs iterations By performing check remove irrelevant problem instances For 20 agents pass check generate 1000 auctions 40 items simulate auctions agents The resulting sequences itemprice pairs transformed features discussed Section 3 resulting data set train regression trees linear regressors Testing For set 20 bidders generate 5 sets 40 items testing First regressors tested comparing predictions revenues generated simulator 50 random orderings item sets Second translate item sets constraints blackbox whitebox optimization solvers The best ordering solvers compared based values regression model simulator 52 Experimental setup In experiment generate agents items described We use implementation regression trees LASSO scikitlearn machine learning module 19 Python learn evaluate regressors We learn trees different depths 3 5 8 tree3 tree5 tree8 set minimum number samples S Verwer et al Artiﬁcial Intelligence 244 2017 368395 381 required split internal node 10 The LASSO regressor run 3 different values α 10 01 0000001 lasso1 lasso2 lasso3 tolerance threshold test convergence set 00001 use maximum 100 000 iterations The resulting trees linear models translated ILP turn gets solved ILPsolver CPLEX 24 In addition provide solver initial solution best 1000 random orderings order start search set focus solver ﬁnding integer feasible solutions We set time limit ILP solver 15 minutes instance single thread Intel core i5 8 GB RAM record best ordering items ILP solver obtained The minute spent solution polishing local search procedure CPLEX We apply bestﬁrst search method problem instances running time limit Evaluations There levels evaluations involved problem Firstly determine quality learned regressors inﬂuence quality solution optimization For tested different regression trees different maximum depths linear regression models different parameters Next optimization methods evaluated terms quality produced ordering The optimization meth ods compare include proposed whitebox ILP model ﬁnds solution based abovementioned 6 regression models proposed blackbox bestﬁrst search evaluates solution based 6 regression models addition simple ordering methods auctioning valuable item ﬁrst mvf suggested 1110 ii random ordering strategy mean5000 seen realworld auctions purpose fairness It feasible compute best solution given problem size Thus obtain lower bound optimal solution follows given set items generate 5000 random orderings use true model simulator evaluate pick returning highest revenue We use mean value 5000 random orderings output random ordering strategy We evaluate 15 ordering methods ways Model evaluation use learned regression models evaluate solutions returned ordering methods compute predicted revenues Actual evaluation run auctions solutions orderings returned ordering methods simulator obtain corresponding revenues Note evaluation possible simulator available There total sets main experiments additional experiments presented paper Experiment 1 We simulate ﬁrstprice auction agents bid minimum value budget willing pay The winner pays winning bid Experiment 2 The simulator runs Vickrey auction myopic agents agents bid minimum true values item remaining budget The winner pays higher value second highest bid reserve price winning item Experiment 3 The simulator runs Vickrey auctions smart agents agents bid smartly based expected utility end auction Section 54 Experiment 4 In addition investigate scalability approach following experiments 1 ﬁrst test small instances close orderings learned models actual optimum 2 second generate randomized population bidders test method handle cases bidders different auction vary lot 3 use larger set items item types demonstrate approach expected perform larger auctions items greater diversity 53 Experiment 1 ﬁrstprice auctions We run 60 sets experiments For set experiment generate set agents generate run new sets items 5 times This results 300 models learning method Prediction accuracy We ﬁrst report performance learned trees linear models terms prediction accuracy Given set items learning randomly generate 50 permutations items orderings compute predicted values orderings learned models Thus set agents 50 5 40 10 000 bids predict These predictions compared evaluated values orderings simulator We report coeﬃcients determination R 2 scores Fig 4 This coeﬃcient standard measure comparing regression models deﬁned 10 We simply ordered items according base values We tested ordering based mean value data performed worse test 382 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 Fig 4 Prediction performance different learning models The yaxis depicts learning model obtained prediction accuracy R2 scores Equation 16 Each box contains 60 values 60 different sets experiments Table 2 The frequencies wins 60 runs method row method vs column method terms R2 scores tree3 tree5 tree8 lasso1 lasso2 lasso3 Total wins tree3 tree5 tree8 lasso1 lasso2 lasso3 0 60 60 33 39 38 0 0 58 0 2 2 R2 1 cid2 1dkpd p f d cid2 1 f d 1dkpd meanp2 m2 0 2 0 0 0 0 27 60 60 0 53 51 21 58 60 7 0 32 22 58 60 9 28 0 70 238 298 49 122 123 16 k number samples pd dth data value revenues returned simulator p f d m predicted values predicted revenues returned learning models tree3lp tree5lp tree8lp lasso1 lasso2 lasso3 meanp mean data values Each score computed 10000 values Fig 4 A large value close 10 means regressor perfect predictor smaller values indicate worse performance Fig 4 shows regression models lead good prediction lowest R 2 score 086 The learned trees depth 8 best performance followed trees depth 5 Intuitively larger tree better prediction The scores lasso2 lasso3 similar slightly better lasso1 This result makes sense LASSO higher regularization parameter α lasso1 implies use features prediction power The tree depth 3 shows worse performance larger trees average worse linear regression models This conﬁrmed frequencies wins comparing R 2 scores pairs Table 2 1 f d Performance ordering methods We discuss actual performance different ordering methods After ev ery ordering method returns best ordering orderings evaluated simulator corresponding revenues As ran 300 different instances 60 sets different agents 5 sets different items method 300 revenues We calculate frequencies wins comparing revenues pairs Table 3 One obvious conclusion table ordering heuristic mvf valuable item ﬁrst performs worst regardless method compared In fact heuristic performed worse random ordering strategy mean5000 123 wins vs 17411 This result contradicts theoretical ﬁnding concluded simpler auction settings Another observation given learned models developed whitebox methods win blackbox methods half time This holds consistently 12 proposed methods wins lp vs bf 171 197 178 185 195 189 It shows new way utilizing internal structure learned models optimization promising 11 Notice instance mean5000 better best5000 This random scheme selecting winners identical bids Consequently evaluating ordering twice simulator result different revenues We want point happen effect negligible S Verwer et al Artiﬁcial Intelligence 244 2017 368395 383 Table 3 The frequencies wins 300 runs method row method vs column method evaluated simulator p l 3 e e r t 0 125 192 142 188 170 193 168 224 171 214 176 68 282 57 f b 3 e e r t 171 0 213 163 217 192 226 181 236 200 231 210 73 284 65 p l 5 e e r t 105 83 0 97 142 130 150 127 184 141 169 131 51 268 27 f b 5 e e r t 152 129 197 0 193 168 202 158 225 177 204 182 61 284 58 p l 8 e e r t 107 81 149 105 0 121 160 113 182 134 167 134 38 277 13 f b 8 e e r t 125 105 168 122 178 0 183 139 205 163 188 161 48 271 30 p l 1 o s s l 102 71 144 94 137 112 0 100 174 115 155 124 36 257 31 f b 1 o s s l 130 116 169 134 181 157 185 0 204 155 201 154 65 280 48 p l 2 o s s l 71 61 111 74 113 91 113 88 0 94 126 89 34 255 12 f b 2 o s s l 127 94 156 118 160 133 178 135 195 0 184 156 55 278 26 p l 3 o s s l 82 67 128 90 128 106 136 91 159 107 0 96 41 252 18 f b 3 o s s l 119 86 164 115 162 138 170 131 199 124 189 0 54 280 24 f v m 231 226 249 238 260 251 263 233 265 243 257 245 0 291 174 tree3lp tree3bf tree5lp tree5bf tree8lp tree8bf lasso1lp lasso1bf lasso2lp lasso2bf lasso3lp lasso3bf mvf best5000 mean5000 0 0 0 5 t s e b 17 15 24 16 23 28 38 18 40 17 36 15 9 0 1 0 0 0 5 n e m 240 230 273 241 287 269 269 251 286 273 281 276 123 299 0 s n w l t o t 2068 1770 2632 2037 2668 2361 2756 2219 3073 2407 2899 2439 982 4158 864 Fig 5 The performance different ordering methods evaluated simulator compared mean5000 The simulated auctions ﬁrstprice auctions Each box contains 300 values If look results whitebox methods built learned regression trees tree3lp tree5lp tree8lp notice tree5lp tree8lp performed similarly slightly better tree3lp This result consistent higher R2 scores larger trees Interestingly despite lower R 2 scores linear regression LP methods return good orderings especially lasso2lp lasso3lp average better regression tree LP models These conﬁrmed Fig 5 depicts revenue differences simulator ordering methods mean5000 It obvious ﬁgure proposed linear regression LP models better optimization methods tree LP models practice We believe cascading inaccuracies caused sum feature relates predicted value predictions earlier auctioned items discussions Section 33 Due crisp boundaries regression trees effect errors solution evaluation greater linear regressors We believe effect smaller larger trees accurate Fig 6 shows performance difference LP model bestﬁrst search built learned regression model The solutions evaluated predictive models The value differences whitebox blackbox methods signiﬁcant smaller trees bigger trees tree3 vs tree5 vs tree8 linear models higher regularization parameter lower regularization parameter lasso1 vs lasso2 vs lasso3 This trend shown results expected The smaller tree leads smaller LP model easier optimize solver consequently gives better performance bestﬁrst search Similarly linear regression higher regularization parameter α implies feature values compute 384 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 Fig 6 The performance difference LP model bestﬁrst search evaluated predictive model Each box contains 300 values The simulated auctions ﬁrstprice auctions whitebox optimization advantage blackbox method obvious lasso2 lasso3 smaller values α We observe whitebox LP methods better blackbox methods LP models resulting trees depth 8 The depth 8 regression trees perform better bestﬁrst search evaluated model Fig 6 better LP evaluated simulator Fig 5 The likely reason strange behavior depth 8 trees bestﬁrst search outperforms LP harder predict instances12 Intuitively harder topredict instances typically result larger models harder optimize CPLEX We checked cause investigating R2 scores depth 8 regression tree correlated method performing better model evaluation The mean R2 scores 0950 LP performs better 0942 bestﬁrst performs better Although difference small signiﬁcant Moreover small difference R2 scores trees depth 5 depth 8 signiﬁcant effect difference model simulator evaluations cascading errors To demonstrate report Fig 7 solution differences ordering methods evaluated model simulator The purpose comparison test predicted outcome learned models corresponds actual outcome simulator This test important general simulators available evaluate solutions The ﬁgure demonstrates linear regression based optimization methods return reliable solutions solutions evaluated learned linear models closer solution values returned simulator Note logical values estimated optimization tries solve maximization problem The trees depth 5 8 signiﬁcant difference evaluation The depth 3 trees end highest evaluation difference overestimate solution values 54 Experiments 2 3 Vickrey auction myopic smart agents In order demonstrate method auction optimization learned models robust auction rule bidding strategies test secondprice auction Experiment 2 smart agents Experiment 3 We generate 10 sets agents settings main experiments For set agents run new sets items 5 times Figs 8 9 plots settings Figs 5 6 setting ﬁrst experiment In secondprice experiment test truthtelling bidders secondprice auctions The differences setting ﬁrst experiment bidding values payments As Fig 8 shows results similar ﬁrst experiments 1 methods outperform naive ordering strategies literature 2 whitebox outperforms blackbox methods 3 linear regression models perform best However difference best performing methods tree5 lasso2 longer signiﬁcant 12 Another possible cause LP solver makes small rounding errors Such errors unavoidable small large coeﬃcients andor precision problem formulation cause numerical instability We round mean values leaf predictions digits decimal point regression tree models translating LP Unfortunately crisp decision boundaries regression trees small errors large effect S Verwer et al Artiﬁcial Intelligence 244 2017 368395 385 Fig 7 The performance difference values evaluated model values evaluated simulator Each box contains 300 values The simulated auctions ﬁrstprice auctions Fig 8 Performance different ordering methods secondprice auctions truthtelling myopic agents The ﬁgures performance evaluated simulator model respectively Each box contains 50 values In experiment test smart agents aim maximize ﬁnal utility secondprice auction Speciﬁcally deciding value bid current item ri access auction simulator use cid4 ri assumed run remaining items I agents bid truthfully pay according secondprice rule runs For bid ri run simulator twice situation bid item ri valuations run1 buy item run2 They decide value bid according following rules cid4 ri For computational reasons running remaining items I If run2 agent remaining budget greater value item bids truthfully The intuition better buy item budget left end auction Else total utility run2 run1 bids truthfully When better buy item try obtain Else bids true value minus difference utility run2 run1 When x monetary units better buy item try obtain value minus x Using rules agents bid highest value expect increase utility 386 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 Fig 9 Performance different ordering methods secondprice auctions smart agents The ﬁgures performance evaluated simulator model respectively Each box contains 50 values As seen Fig 9 challenging setting method performs signiﬁcantly better naive ordering rules literature There larger variance performance different methods causing difference blackbox whitebox insigniﬁcant simulator When evaluated model whitebox better blackbox cases tree8 An interesting ﬁnal observation smart agents mvf method perform slightly better mean5000 signiﬁcantly experiments performed consistently worse 55 Experiment 4 practical issues Although optimizing orderings sequential auctions hard problem experiments demonstrate high revenues obtained signiﬁcantly outperforming naive methods proposed literature This holds presence smart bidders They advantage whitebox method optimization interesting tradeoff modeling optimization power better predictors necessarily better optimizers In section investigate practical issues approach We start easier instances close orderings learned models actual optimum Afterwards test method new settings randomized population bidders larger set items item types The randomized population mimics feature type realworld auctions agents bidding auction seen online auctions The larger instances aim demonstrate approach expected perform largescale auctions items greater diversity 551 Smaller auctions The smaller problem instances generated 1 maximum budget 80 2 maximum number 3 desired items 3 4 item types 4 15 items auction 5 8 agents All settings main experiments In addition added bruteforce method uses simulator blackbox tests possible ordering items Although bruteforce method infeasible 40 items case 15 items million possible orderings consider The results 10 sets agents tested 5 sets items shown Fig 10 The corresponding plots main experiments Figs 5 6 The R2 scores behavior similar main experiments range 08 09 Outliers depth 3 trees low 05 This score decrease likely caused decrease data size 15 instead 40 data rows auction The left plot Fig 10 shows results obtained simulator Any difference performance different methods insigniﬁcant depth 3 trees signiﬁcantly worse estimating ob tained revenues result lesser quality solutions The improvement performance methods random approximately 10 average whilst 20 obtained perfect models theory Frequently blackbox whitebox methods ﬁnd optimal solution learned models difference entirely prediction quality models It good performance mean5000 method indistinguish able optimal boxplot ﬁgure giving conﬁdence good approximation upper bound As seen right plot difference whitebox blackbox methods negligible small instances S Verwer et al Artiﬁcial Intelligence 244 2017 368395 387 Fig 10 Performance different ordering methods bruteforce search opt 15 items 4 types The ﬁgures performance evaluated simulator model respectively Each box contains 50 values tree3 tree5 tree8 lasso1 lasso2 lasso3 Total wins tree3 tree5 tree8 lasso1 lasso2 lasso3 0 10 2 13 13 13 9 0 4 15 14 14 17 15 0 19 19 19 6 4 0 0 1 1 6 5 0 18 0 2 6 5 0 18 17 0 44 39 6 83 64 49 Fig 11 Prediction performance different learning models randomized population including number wins table Each box contains 19 values 19 different sets experiments 552 Randomized population In set experiments obtain different sets agents creating large population size 60 drawing 20 random auction In addition generate random set 20 60 items auction sampled uniformly random Other use exact settings main experiments We results 19 sets agents increased set reduce noise results tested 5 sets items Fig 12 The R2scores obtained experiments shown Fig 11 388 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 Fig 12 Performance different ordering methods 20 60 items 8 types random population 20 agents selected pool 60 The ﬁgures performance evaluated simulator model respectively Each box contains 95 values From Fig 11 observations Firstly performance predictors worse main experiments median values 07 As expect revenue hard predict randomized setting heavily dependent agents auction Learning larger trees increase score In fact wins table shows depth 8 trees clearly worst predictor Secondly contrast main experiments linear regression appears better estimator regression tree difference small The simulator model results Fig 10 conﬁrm regression functions perform worse randomized population setting The linear regression models perform better random The difference expected value random solution small cases uncertain populations probably worthwhile model optimize problem The model results cases whitebox methods outperform blackbox ones exception depth 8 regression trees performance whitebox blackbox similar 553 Larger problem instances The larger problem instances generated settings main experiments expect 1 12 item types 2 80 items auction 3 40 agents In addition optimization problems harder timeout 30 minutes instead 15 The results 10 sets agents tested 5 sets items shown Figs 13 14 Notice performance depth 8 trees missing Fig 13 The reason CPLEX runs trees ran memory The encoding requires n R 2k 1 variables n R 2k 1 1 n R constraints If ﬁll depth 8 12 types 80 items end 46 720 variables 490 652 constraints This proved version CPLEX v1251 Although smaller trees run depth 5 trees outperformed blackbox approach For depth 3 trees better use whitebox A similar phenomenon visible Lasso results In previous results whitebox outperforms blackbox Lasso regressors Suddenly case smallest linear models coeﬃcients blackbox performs better These results match intuition whitebox approach beneﬁcial models overly complex In simulator results smaller models perform worse larger ones spite harder optimize We believe worse predictors seen wins table Fig 14 Interestingly larger linear models performance difference blackbox whitebox longer For depth 5 tree difference inverted believe chance However possible effect caused estimation For blackbox method average estimation solutions 237 111 whitebox method This caused fact CPLEX problems ﬁnding solutions large instances making harder ﬁnd speciﬁc solutions estimate solution value Investigating behavior left future work Fig 14 shows increase R2 scores median values 092 mark This likely cause increase training data twice items sold auction The tree models better regressors large instances optimization performance simulator worse In fact performance close random ordering The larger linear models perform better random far optimal S Verwer et al Artiﬁcial Intelligence 244 2017 368395 389 Fig 13 Performance different ordering methods 80 items 12 types 40 bidders The ﬁgures performance evaluated simulator model respectively Each box contains 50 values tree3 tree5 tree8 lasso1 lasso2 lasso3 Total wins tree3 tree5 tree8 lasso1 lasso2 lasso3 0 10 10 1 2 2 0 0 10 0 0 0 0 0 0 0 0 0 9 10 10 0 9 9 8 10 10 1 0 8 8 10 10 1 2 0 25 40 50 3 13 19 Fig 14 Prediction performance different learning models 80 items 12 types 40 bidders The yaxis depicts learning model obtained prediction accuracy R2 scores Equation 16 Each box contains 10 values 10 different sets experiments 6 Related work discussion We discuss related works work contributes related research communities 61 Interplay mathematical optimization machine learning Many studies investigated interplay data mining machine learning mathematical modeling tech niques overview 2527 Most investigate use data mining estimate value parameters 390 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 decision making models replace decision model structure fully determined hypotheses hand For instance Brijs et al 28 build decision model integer program maximizes product assortment retail store The decision model reﬁned incorporating additional decision attributes learned patterns recorded sales data Li Olafsson 3 use decision tree learn dispatching rules decide job dispatched ﬁrst These dispatching rules previously unknown assumed worth capture current practices previous data Gabel Riedmiller 1 model production scheduling problem multiagent reinforcement learning agent makes dispatching decisions reinforcement learning algorithm based neural network function approximation Another line work investigates use learning techniques optimization order learn properties good solutions For instance Defourny et al 29 combine estimation statistical models returning decision rule given state scenario tree techniques multistage stochastic programming This line work shares similarities ﬁeld blackbox optimization 141530 In blackbox optimization methods approximate function unknown analytical form typically expensive execute In contrast multistage stochastic programming form known stochastic An applied technique blackbox optimization use sur rogate methods 31 Surrogates approximations blackbox function expensive execute Typical examples include linearpolynomial regression neural networks methods machine learning These functions trained optimization possible blackbox function calls As learning tasks lead challenging optimization problems researchers applied mathematical optimization methods order increase learning eﬃciency For instance Bennett et al 32 use linear programming determining linear combination splits twoclass decision trees Chang et al 33 propose Constrained Conditional Model CCM framework incorporate domain knowledge conditional model structured learning form declarative constraints CCMs solve prediction problems In 34 authors build mixed integer program multiclass data clas siﬁcation A comprehensive overview optimization techniques learning given 35 Researchers interested mathematical optimization methods order ﬁnd entire models rules 3638 Our approach ﬁts ﬁrst line research interplay The proposed bestﬁrst search method uses regression models learn good orderings applied search evaluate solutions OOSA Hence similar works mentioned models learned data blackbox fashion This approach shares similarities surrogate methods blackbox optimization An important difference surrogate models learned data Furthermore proposed whitebox optimization method makes properties learned models visible optimization solver Bartolini et al 4 propose similar method translating neural networks constraint programming CP models Their approach simple effective allows model complex relations recurrent neural networks pair decision variables based data This powerful allows multiple trained neural networks plugged existing CP model constructed traditional means It designer relations variables model traditionally model based data In contrast use translation construct parts objective function existing MIP model regression functions To best knowledge ﬁrst combine regression functions learned data MIP model way Other closely related work CP literature considers problem constraint acquisition 3942 Given large set possible constraints L training data goal constraint acquisition compose constraint net work graphical representation CP model 43 constraints L classiﬁes training data correctly specify language includes positive excludes negative training examples Although hard problem effective approaches proposed based supervised 39 supervised 41 active 4042 learning These approaches based new learning algorithms proven performance bounds 42 Once learned constraints embedded existing CP model fully integrated optimization process From modeling perspective method beneﬁts downside On hand constraint acquisition allows modeler deﬁne dedicated constraint set L instead featuresvariables potentially resulting sensible models problem hand On hand learn ing problem arguably understood tackled existing algorithms machine learning potentially resulting quality classiﬁers In addition probabilistic inference interesting combination machine learning optimization proposed First probabilistic model learned traditional way Then computing posterior distribution target variables given new input data additional constraints added order limit possible assignments targets While original probabilistic inference problem solved dynamic programming methods additional constraints harder solve translated MIP This approach successfully applied order add expert knowledge conditional random ﬁelds semantic role labeling Roth Yih 44 Later principle model dependencies textual tokens textbased documents entity recognition Fersini et al 45 Interestingly work added constraints learned data added soft constraints MIP model S Verwer et al Artiﬁcial Intelligence 244 2017 368395 391 62 Sequence models As auction ordering essentially sequence items work related machine learning ap proaches sequence modeling To best knowledge existing sequence models ﬁts auction setting Language models deterministic automata 46 powerful model possible sequence inde pendently require data learn accurately Short sequence models hidden Markov models Ngrams 47 model dependence items sold long time sliding window length Markov decision processes MDPs 48 closest auction setting directly model expected price item come methods optimize expected total reward revenue However notice straightforward implementation auction design problem MDP possible Let try model auction design MDP Because Markov assumption state MDP contain relevant information auctioneers decision item auction set available items bidders valuation functions budgets strategies bidder items possesses In state q process auctioneer choose item auction multiset available items I The state q resulting auctioning item depends bidders valuations These unknown auctioneer probabilities cid4 provide distribution possible states estimate These probabilities P iq q corresponding rewards R iq q set available items equal I equal items q minus sold item The goal auctioneer maximize expected rewards revenue given set items I In state q action choosing item maximizes sum expected rewards V q I items I starting state q cid4 given auctioned In possible state q cid4 cid4 V q I arg max iI cid3 qcid4 cid4 P iq q R iq q cid4 V q cid4 I In equation separated I q highlight major hurdle needs overcome order represent auction design problem MDP I needs included MDP determines set available actions state However set items ﬁnite makes MDP acyclic large number possible subsets items I assuming effect ordering represented differently 2 In order learn rewards transition probabilities auctioneer need extremely large data sample I This intuitively shows diﬃcult represent auction design problem MDP However suitable factored representation states andor function approximation 48 rewards possible represent auction problem MDP In case major hurdle ﬁnd representation results Markovian states needed apply dynamic programming methods Since problem deciding good auction ordering exists NPcomplete Theorem 1 methods run polynomial time impossible exponentially large state space P NP Our method relies solvers search methods NPcomplete problems making polynomial state space possible requiring data estimate model parameters 63 Auction design In auction literature existing papers investigate impact ordering performance sequential auc tions One line related research focuses theoretical analysis In economics literature 91211 theoretical studies typically carried restricted markets The main research focus analyze equilibrium bidding strategies bidders compete usually items heterogeneous homogeneous gain sights impact ordering auction outcome based derived bidding behaviors For instance Elmaghraby 9 studies inﬂuence ordering eﬃciency sequential second price procurement auctions buyer outsources heterogeneous jobs suppliers capacity constraints Suppliers win 1 job setting The author shows speciﬁc sequences lower procurement costs identiﬁes class bidders cost functions eﬃcient orderings auction rewards jobs suppliers lowest total costs equilibrium bidding strategies exist Pitchik 12 points presence budget constraints sealedbid sequential auction bidders goods multiple symmetric equilibrium bidding functions ordering sale affects expected revenue If bidder wins ﬁrst good higher income expected revenue maximized Subramaniam Venkatesh 11 investigate optimal auctioning strategy revenuemaximizing seller auctions items complements substitutes They items different value higher valued item auctioned ﬁrst order increase sellers revenue A similar revenuemaximizing strategy proposed Benoit Krishna 49 complete information auction setting The authors conclude setting selling items budget constrained bidders better sell valued item ﬁrst However strategy optimize revenue anymore items auctioned Empirical research conducted test theoretical ﬁndings economics community Grether et al 10 report ﬁeld experiment tests ordering strategies seller sequential ascending automobile auctions They 392 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 conclude worst performing ordering terms revenue seller auction vehicles highest lowest values In science literature Elkind Fatima 13 study maximize revenue sequential auctions secondprice sealedbid rules bidders homogeneous valuations drawn public known uni form distributions want win item bid items In setting authors analyze equilibrium bids develop algorithm ﬁnds optimal agenda ordering Vetsikas et al 50 study similar auction setting unlike 13 assume valuations known bidders beginning auction The focus work compute equilibrium strategies bidders Later Vetsikas 23 analyzes bidding strategies budget constrained bidders sequential Vickrey auctions However challenge compute equilibrium strategies practice We ﬁrst consider learning previous auctions However difference lies fact existing work study bidders learn past information update bids Boutilier et al 51 propose learning model bidders update bidding policies sequential auctions resources complementarities The bidding strategies computed based estimated distribution prices modeled dynamic programming Goes et al 52 present empirical study real sequential online auctions They analyze data online auction retailer bidders learn update willingness pay repeated auctions item In 7 authors beneﬁts earlier auction data management sequential multiunit auctions seller needs split entire inventory sequential auctions smaller lots order increase proﬁt In work auction feedback mechanism developed based Bayesian model update auctioneers beliefs bidders valuation distribution Our contribution auction literature lies fact approach applied design optimal auctions based historical auction data The advantage machine learning data mining methods robust uncertainty noise high potential realworld auction design Moreover approach general applied different auction optimization problems ﬁnding best reserve price items sale maximizing social welfare instead revenue The necessary changes include selection features learning regression models encoding whitebox optimization model 7 Conclusions Mathematical optimization relies availability knowledge construct mathematical model problem hand This knowledge available For instance multiagent problems agents au tonomous unwilling share local information Frequently autonomy private information inﬂuence outcome optimization making ﬁnding optimal solution diﬃcult In paper adopt idea machine learning techniques estimate inﬂuences optimization problem unknowns optimal ordering sequential auctions OOSA problem We demonstrated approach transforming historical auctions data sets learning regression trees linear regression models subsequently predict expected value orderings new auctions We proposed types optimization methods learned models blackbox bestﬁrst search approach novel whitebox approach maps learned models integer linear programs ILP We built auction simulator set bidder agents simulate auction environment The simulator generating historical auction data evaluating orderings items returned methods We ran extensive set experiments different agents bidding strategies Although optimizing orderings sequential auctions hard problem proposed methods obtained high values signiﬁcantly outperforming naive methods proposed literature The experimental sults demonstrate advantage whitebox method optimization signiﬁcantly outperforms blackbox approach nearly settings In addition indicate learned model complex potentially results constraints consequently increase time needed solve problem whitebox fashion Since complex models potentially better predictors shows clear tradeoff modeling optimization power whitebox optimization In opinion beneﬁts whitebox approach largely outweigh beneﬁts blackbox optimization Finally extended experiments demonstrate encodings eﬃcient regression tree breaks data noisy An intriguing extension use regression forests instead individual trees These known handle noisy data better crisp boundaries individual trees The experiments method scale number items likely increase number trees need evaluated We expect method based regression forests require simpliﬁcations optimizations order feasible Besides improved performance big beneﬁt whitebox formulation provides new way obtaining traditional mathematical models Our method potential application areas especially problems data collected Even cases exists handcrafted optimization model model learned translated method easily integrated existing ILP formulations order determine objective function based data In way combine vast expert S Verwer et al Artiﬁcial Intelligence 244 2017 368395 393 Fig 15 The regression tree item type ri model partitioning problem knowledge available domains knowledge readily available data We like investigate combination future We chose relatively simple auction model ease explanation paper However approach works regression models able provide reliable predictions bidding values Hence believe applied auction formats complex valuation functions combinatorial preferences 53 complex bidding strategies The results method larger experiments 80 items shows scaling approach large realworld auctions require nontrivial simpliﬁcations Moreover paper learned regression trees linear models simulated data order test optimization performance When applying approach realworld data important test regressors assumptions satisﬁed If needed transform ﬁlter We plan discover simpliﬁcations test approach real auction data near future Our experiments highlight interesting properties whitebox method Firstly improvement performance number features reduced andor models complex It interesting investigate effect pruning feature selection reduction performance methods Secondly tendency regression tree optimizer overestimate ﬁnd orderings higher expected revenue revenue practice Intuitively solver abuses crisp nature regression tree order ﬁnd solution satisﬁes exactly right constraints Part problem constraints learned data uncertain solver treats exact Fortunately exists long history methods try optimize presence uncertainties area robust optimization 54 As future work investigate potential uses techniques learned models Recently regression tree models linear models leaf nodes successfully blackbox surrogate functions 55 Since straightforward translate trees given encodings replace leaf variables indicators linear function use interesting investigate possibility whitebox alternative Acknowledgement Sicco Verwer supported Technologiestichting STW VENI project 13136 MANTA We thank reviewers valuable suggestions comments signiﬁcantly improved quality paper Appendix A Hardness auction design learned predictors We predictive models instead agents utility functions reduce complexity problem remains NPcomplete regression trees linear regression predictors Lemma 1 Using regression trees problem exists ordering total predicted value K NPcomplete Proof The proof follows fact use simple regression trees model preferences agents Theorem 1 evaluating ordering trees polynomial time The regression tree item type ri shown Fig 15 cid2 Lemma 2 Using linear regression predictors problem exists ordering total predicted value K NPcomplete Proof We prove lemma construction computing value quadratic function linear functions ordering problem feature values The maximum value quadratic function forced coincide solution partition problem instance Given set integers I v 1 vn I dividable sets A B A cid2 cid2 B 394 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 From partition instance let k 1 2 tions v cid2 1in v construct following items linear regression predictors func n items type x1 xn vxi v v sum y 1 item type y v y 2k 1in sumxi cid2 2k cid2 The objective maximize ordering achieves value 2 1 2 k Let A B partition I cid2 cid2 A type x1 xn The following ordering gives value 1in v 1in vxi v y The corresponding decision problem ask exists B let a1 A b1 bB corresponding items cid2 cid2 cid2 In ordering cid2 1in sumxi value 2k v y k sum y k a1 A y b1 bB cid2 1i A vai k deﬁnition A sum y 0 item y auctioned Consequently cid2 1i A va1 v y obtains objective 1i A vai k giving v y 2k k k 1iB vb1 equal 1 Since 2 k 1iB vbi k ksum y 2 k proving ordering obtains value 2 1 cid2 2 k By deﬁnition B 1 2k To prove direction let analyze relation objective function 1in vxi v y auction ordering The term vxi predictors depends ordering sum y terms constants This term equal zero xi items auctioned y equal v y items auctioned y Similar let a1 A denote xi items y b1 bB y A B corresponding cid2 partition items I The objective value given 1iB vbi We analyze parts turn cid2 1iB vbi k kk 1i A vai v y k k 2 cid2 cid2 cid2 cid2 2k 1i A vai cid2 v y 2k cid2 1iB vbi v A v sum y 0 items v A v cid2 v v y 2k 1i A vai 2k v B v v B v v y v B cid2 cid2 cid2 cid2 v B v cid2 v B v v y2 2k cid2 cid2 Since v A v 2k v y v y2 2k v B v 2k overall objective function given maximized v y k k 0 value 2k k k2 cid2 2k v B v k The sets A B partition I cid2 2 1 2 k This exact value v y k obtained Remarks We proved NPcompleteness general case Lemma 2 However know com plexity holds realistic valuation functions bidders References 1 T Gabel M Riedmiller Adaptive reactive jobshop scheduling learning agents Int J Inf Technol Intell Comput 2 4 2008 130 2 A Huyet Optimization analysis aid datamining simulated production systems Eur J Oper Res 173 3 2006 827838 3 X Li S Ólafsson Discovering dispatching rules data mining J Sched 8 6 2005 515527 4 A Bartolini M Lombardi M Milano L Benini Neuron constraints model complex realworld problems JHM Lee Ed Principles Practice Constraint Programming Lecture Notes Computer Science vol 6876 Springer 2011 pp 115129 5 D Bernhardt D Scoones A note sequential auctions Am Econ Rev 84 3 1994 653657 6 E van Heck PMA Ribbers Experiences electronic auctions Dutch ﬂower industry EM 7 4 1997 2934 7 EJ Pinker A Seidmann Y Vakrat Using bid data management sequential multiunit online auctions uniformly distributed bidder valuations Eur J Oper Res 202 2 2010 574583 8 J Gallien LM Wein A smart market industrial procurement capacity constraints Manag Sci 51 2005 7691 9 W Elmaghraby The importance ordering sequential auctions Manag Sci 49 2003 673682 10 DM Grether CR Plott Sequencing strategies large competitive ascending price automobile auctions experimental examination J Econ Behav 11 R Subramaniam R Venkatesh Optimal bundling strategies multiobject auctions complements substitutes Mark Sci 28 2009 264273 Organ 71 2 2009 7588 httpdxdoiorg101287mksc10800394 12 C Pitchik Budgetconstrained sequential auctions incomplete information Games Econ Behav 66 2 2009 928949 13 E Elkind S Fatima Maximizing revenue sequential auctions Proceedings 3rd International Conference Internet Network Eco nomics WINE07 SpringerVerlag Berlin Heidelberg 2007 pp 491502 14 DR Jones M Schonlau WJ Welch Eﬃcient global optimization expensive blackbox functions J Glob Optim 13 4 1998 455492 15 S Shan GG Wang Survey modeling optimization strategies solve highdimensional design problems computationallyexpensive black box functions Struct Multidiscip Optim 41 2 2010 219241 16 MR Garey DS Johnson Computers Intractability A Guide Theory NPCompleteness WH Freeman Company 1979 17 L Breiman J Friedman R Olshen C Stone Classiﬁcation Regression Trees Wadsworth Brooks Monterey CA 1984 Oper Res 221 3 2012 469479 8 1 2004 723 Comput 25 3 2013 488501 2013 12471293 S Verwer et al Artiﬁcial Intelligence 244 2017 368395 395 18 R Tibshirani Regression shrinkage selection lasso J R Stat Soc B 58 1994 267288 19 scikitlearn machine learning Python httpscikitlearnorg 20 TG Dietterich Machine learning sequential data review Structural Syntactic Statistical Pattern Recognition Springer 2002 pp 1530 21 S Verwer Y Zhang Revenue prediction budgetconstrained sequential auctions complementarities AAMAS12 2012 pp 13991400 22 W Vickrey Counterspeculation auctions competitive sealed tenders J Finance 16 1 1961 837 23 IA Vetsikas Sequential auctions budgetconstrained bidders IEEE 10th International Conference eBusiness Engineering IEEE 2013 pp 1724 24 IBM ILOG CPLEX Optimizer httpwww01ibmcomsoftwareintegrationoptimizationcplexoptimizer 25 KP Bennett E ParradoHernández The interplay optimization machine learning research J Mach Learn Res 7 2006 12651281 26 S Meisel D Mattfeld Synergies operations research data mining Eur J Oper Res 206 1 2010 110 27 D Corne C Dhaenens L Jourdan Synergies operations research data mining emerging use multiobjective approaches Eur J 28 T Brijs G Swinnen K Vanhoof G Wets Building association rules framework improve product assortment decisions Data Min Knowl Discov 29 B Defourny D Ernst L Wehenkel Scenario trees policy selection multistage stochastic programming machine learning INFORMS J 30 LM Rios NV Sahinidis Derivativefree optimization review algorithms comparison software implementations J Glob Optim 56 3 31 S Koziel DE Ciaurri L Leifsson Surrogatebased methods Computational Optimization Methods Algorithms Springer 2011 pp 3359 32 KP Bennett OL Mangasarian Bilinear separation sets nspace Comput Optim Appl 2 1993 207227 33 M Chang L Ratinov D Roth Structured learning constrained conditional models Mach Learn 88 3 2012 399431 34 F Uney M Turkay A mixedinteger programming approach multiclass data classiﬁcation problem Eur J Oper Res 173 3 2006 910920 35 S Sra S Nowozin SJ Wright Optimization Machine Learning MIT Press 2012 36 E Carrizosa D Romero Morales Supervised classiﬁcation mathematical optimization Comput Oper Res 40 1 2013 150165 37 LD Raedt T Guns S Nijssen Constraint programming data mining machine learning AAAI 2010 pp 16711675 38 MJ Heule S Verwer Exact DFA identiﬁcation SAT solvers Grammatical Inference Theoretical Results Applications Lecture Notes Computer Science vol 6339 Springer Berlin Heidelberg 2010 pp 6679 39 C Bessiere R Coletta F Koriche B OSullivan A satbased version space algorithm acquiring constraint satisfaction problems Machine Learning ECML 2005 Springer 2005 pp 2334 40 C Bessière R Coletta B OSullivan M Paulin Querydriven constraint acquisition MM Veloso Ed Proceedings 20th International Joint Conference Artiﬁcial Intelligence IJCAI 2007 2007 pp 5055 41 N Beldiceanu H Simonis A model seeker extracting global constraint models positive examples M Milano Ed Principles Practice Constraint Programming Lecture Notes Computer Science vol 7514 Springer 2012 pp 141157 42 C Bessiere R Coletta E Hebrard G Katsirelos N Lazaar N Narodytska CG Quimper T Walsh Constraint acquisition partial queries Inter national Joint Conference Artiﬁcial Intelligence 2013 43 R Dechter Constraint Processing Morgan Kaufmann 2003 44 D Roth WT Yih Integer linear programming inference conditional random ﬁelds Proceedings 22nd International Conference Machine Learning ICML 05 ACM New York NY USA 2005 pp 736743 45 E Fersini E Messina G Felici D Roth Softconstrained inference named entity recognition Inf Process Manag 50 5 2014 807819 httpdxdoiorg101016jipm201404005 46 C la Higuera Grammatical Inference Learning Automata Grammars Cambridge University Press New York NY USA 2010 47 CM Bishop Pattern Recognition Machine Learning Information Science Statistics SpringerVerlag New York Inc Secaucus NJ USA 2006 48 ML Puterman Markov Decision Processes Discrete Stochastic Dynamic Programming vol 414 John Wiley Sons 2009 49 JP Benoit V Krishna Multipleobject auctions budget constrained bidders Rev Econ Stud 68 1 2001 155179 50 IA Vetsikas NR Jennings Sequential Auctions Partially Substitutable Goods Lecture Notes Business Information Processing vol 59 2009 pp 242258 2010 907924 51 C Boutilier M Goldszmidt B Sabata Sequential auctions allocation resources complementaritie Proceedings 16th Interna tional Joint Conference Artiﬁcal Intelligence vol 1 Morgan Kaufmann Publishers Inc San Francisco CA USA 1999 pp 527534 52 PB Goes GG Karuga AK Tripathi Understanding willingnesstopay formation repeat bidders sequential online auctions Inf Syst Res 21 53 P Cramton Y Shoham R Steinberg Combinatorial Auctions MIT Press 2006 54 A BenTal L El Ghaoui A Nemirovski Robust Optimization Princeton Series Applied Mathematics Princeton University Press 2009 55 D Verbeeck F Maes K De Grave H Blockeel Multiobjective optimization surrogate trees Proceeding Fifteenth Annual Conference Genetic Evolutionary Computation Conference ACM 2013 pp 679686