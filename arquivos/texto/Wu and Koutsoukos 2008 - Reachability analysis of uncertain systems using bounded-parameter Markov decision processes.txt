Artiﬁcial Intelligence 172 2008 945954 wwwelseviercomlocateartint Reachability analysis uncertain systems boundedparameter Markov decision processes Di Wu Xenofon Koutsoukos EECS Department Vanderbilt University Nashville TN 37235 USA Received 8 September 2006 received revised form 8 December 2007 accepted 17 December 2007 Available online 23 December 2007 Abstract Veriﬁcation reachability properties probabilistic systems usually based variants Markov processes Current methods assume exact model dynamic behavior suitable realistic systems operate presence uncertainty variability This research note extends existing methods Boundedparameter Markov Decision Processes BMDPs solve reachability problem BMDPs generalization MDPs allows modeling uncertainty Our results interval value iteration converges case undiscounted reward criterion required formulate prob lems maximizing probability reaching set desirable states minimizing probability reaching unsafe set Analysis computational complexity presented 2007 Elsevier BV All rights reserved Keywords Reachability analysis Uncertain systems Markov decision processes 1 Introduction Veriﬁcation reachability properties probabilistic systems usually based variants Markov processes Probabilistic veriﬁcation aims establishing bounds probabilities certain events Typical problems include maximum minimum probability reachability problems objective compute control policy maximizes probability reaching set desirable states minimize probability reaching unsafe set Such problems important application domains planning autonomous systems 1 biology 2 ﬁnance 3 Algorithms veriﬁcation MDPs presented 45 Several probabilistic models based variants MDPs considered 67 These methods assume exact values transition probabilities typically computed based detailed models discrete approximation techniques 8 estimated data 9 However realistic systems operate presence uncertainty variability modeling estimation errors affect transition probabilities impact solution Such systems best described uncertain transition probabilities An uncertain MDP describes routing aircraft based past weather data presented 10 Computing uncertain transition probabilities robot pathﬁnding Corresponding author Email addresses wudipkugmailcom D Wu xenofonkoutsoukosvanderbiltedu X Koutsoukos 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200712002 946 D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 example based model continuous dynamics described 11 Existing reachability analysis methods insufﬁcient dealing uncertainty This research note extends existing methods Boundedparameter Markov Decision Processes BMDPs solve reachability problem Proposed Givan Leach Dean 12 BMDPs generalization MDPs allows modeling uncertainty A BMDP viewed set exact MDPs sharing state action space speciﬁed intervals transition probabilities rewards policies compared basis interval value functions An overview BMDPs presented Section 2 The paper focuses problem maximizing probability reaching set desirable states The results presented 12 dynamic programming methods assuming discounted reward criterion A discount factor ensures convergence iterative methods interval value functions Probabilistic veriﬁcation formulated based Expected Total Reward Criterion ETRC 13 Under ETRC discount factor set 1 convergence iterative algorithms BMDPs involved contraction property iteration operators hold globally interval value function deﬁned proper restrictions intervals transition probabilities rewards applied The interval expected total reward BMDPs analyzed Section 3 Further proving polynomial computational complexity algorithm requires different method appropriate weighted norm Based ETRC paper presents detailed analysis convergence computational complexity maximum probability reachability problem Sections 4 5 respectively Minimum probability reachability problems based ETRC 13 addressed similar fashion A simpliﬁed robot pathﬁnding example numerical results illustrate approach 11 Optimal solutions variants uncertain MDP problems studied previously MDPs uncer tain transition probabilities discounted reward criterion considered 1415 Related methods consider discounted reward include work 16 computes optimal policy models compact convex uncertain sets approach 17 computes Pareto optimal policy maximizes average expected reward stationary policies speciﬁc partial order work 10 solves robust control problem The average reward problem BMDPs studied 18 similar average performance criterion considered 19 An algorithm based realtime dynamic programming uncertain stochastic shortest path problems presented 20 The algorithm requires goal state reachable visited state proposes reachability analysis preprocessing step based graph analysis Probabilistic reachability analysis uncertain MDPs signiﬁcant problem requires undiscounted reward criterion treated algorithms Probabilistic veriﬁcation uncertain systems addressed model checking methods A variant uncertain MDPs presented 2122 The main characteristic model uncertainty resolved nondeterminism step adversary picks probability distribution satisﬁes uncertain transition probabilities This differs BMDPs transition probabilities uncertain given action selected external agent The approach presented 21 computes probability distribution states ﬁnite number steps algorithms 22 reduce uncertain MDP larger size verifying subset probabilistic computation tree logic speciﬁcations steady state operators 2 Boundedparameter Markov decision processes We ﬁrst review basic notions BMDPs 12 establish notation A BMDP deﬁned M cid3Q A ˆF ˆRcid4 Q set states A set actions ˆR interval reward function maps q Q closed interval real values Rq Rq ˆF interval statetransition distribution p q Q α A F pq α cid2 PrXt1 qXt p Ut α cid2 F pq α For action α state p sum lower bounds ˆFpq α states q required equal 1 sum upper bounds required greater equal 1 A BMDP M deﬁnes set exact MDPs Let M cid3QM AM F M RM cid4 MDP If QM Q AM A RM p ˆRp F M pq α ˆFpq α α A p q Q M M To simplify presenta tion rewards assumed tight results easily generalized case interval rewards D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 947 Policies deﬁned π Q A restricted set stationary Markov policies Π Let V denote set value functions Q For exact MDP M policy π discount factor γ 0 1 value function solution equation VMπ p Rp γ cid4 cid3 VMπ q πp F M pq cid2 qQ computed iteratively applying policy evaluation operator denoted VIMπ V V For policy π state p interval value function BMDP M π p closed interval cid5 ˆVπ p inf MM VMπ p sup MM cid6 VMπ p 1 An MDP M M π maximizing M cid7 M VMπ cid3dom VM cid7π 1 likewise M π minimizing M cid7 M VMπ cid2dom VM cid7π It proved 12 Theorem 7 Corollary 1 policy π Π ordering states Q exist π maximizing MDP Mπ π minimizing MDP Mπ This implies input V V exists single MDP independent V V simultaneously maximizes minimizes VMπ p states p Q Therefore deﬁne interval policy evaluation operator cid7IVIπ cid7IVIπ ˆV p cid6 cid5 IVIπ V p IVIπ V p IVIπ V min MM VIMπ V VIMππ V IVIπ V max MM VIMπ V VIMππ V In order deﬁne optimal value function BMDP different orderings closed real intervals introduced l1 u1 cid2opt l2 u2 u1 u2 u1 u2 l1 cid2 l2 l1 u1 cid2pes l2 u2 l1 l2 l1 l2 u1 cid2 u2 In addition ˆU cid2opt ˆV q q Q Then optimistic optimal value function ˆVopt pessimistic optimal value function ˆVpes deﬁned upper bounds stationary policies cid2opt cid2pes respectively order interval value functions ˆV ˆU q cid2opt ˆV q ˆU q cid2pes ˆV ˆU cid2pes ˆVopt max πΠcid2opt ˆVπ ˆVpes min πΠcid2pes ˆVπ respectively The value iteration ˆVopt objective maximize upper bound V ˆVpes maximize lower bound V In subsequent sections focus optimistic case optimal interval value functions Results pessimistic case inferred analogously The interval value iteration operator cid7IVIopt state p deﬁned cid7IVIopt ˆV p max αAcid2opt min MM VIMαV p max MM cid5 cid6 VIMαV p 2 Due nature cid2opt cid7IVIopt evaluates actions primarily based interval upper bounds breaking ties lower bounds For state action maximizes lower bound chosen subset actions equally maximize upper bound To capture behavior deﬁne action selection function ρW p arg max αA max MM VIMαW p IVIoptV q max αA max MM VIMαV q IVI optV V q max αρV q min MM VIMαV q Then 2 rewritten cid7IVIopt ˆV cid6 cid5 IVI optV V IVIoptV 1 V cid3dom U q Q V q cid3 U q cid2dom deﬁned similarly 3 4 948 D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 3 Interval expected total reward BMDPs In paper primarily interested problem maximizing probability reach desirable set states By solving problem establish bounds probabilities reaching desirable conﬁgurations probabilistic veriﬁcation discrete systems This problem formulated Ex pected Total Reward Criterion ETRC BMDPs The ERTC viewed expected total discounted reward discount factor γ 1 For γ 1 convergence results 12 longer hold iteration operators cid7IVIπ cid7IVIopt cid7IVIpes global contraction mappings Furthermore interval value function deﬁned proper restrictions intervals transition probabilities rewards applied To simplify notation use vector notation R V column vectors ith element scalar reward value function ith state respectively FM transition probability function MDP M FMπ transition probability matrix given policy π For exact MDP M policy π value function ETRC solution equation VMπ R FMπ V computed policy evaluation operator VIMπ 13 The interval value function deﬁned Eq 1 similarly discounted case Further existence π minimizing π maximizing MDP depend discount factor 12 deﬁne π maximizing MDP Mπ π minimizing MDP Mπ M ETRC For MDP M policy π denote EMπ q expectation functionals given initial state q Under ETRC compare policies basis interval expected total reward ˆV V π V π q Q V π q EMππ q RXt V π q EMππ q cid9 cid8 cid2 t1 cid8 cid2 cid9 RXt t1 Let Rq maxRq 0 Rq maxRq 0 We deﬁne expected total rewards R R V π q lim N EMππ q cid8 cid4 cid3 Xt q R N 1cid2 t1 cid9 V ignores negative rewards V ignores positive rewards Since summands nonnegative limits exist2 The limit deﬁning V π q exists V π q ﬁnite π q V π q V case V π V π q V π q similarly deﬁned Noting impose following ﬁniteness assumption assures ˆVπ deﬁned π q V π q V Assumption 1 For π Π q Q V π q V π q ﬁnite b V π q V π q ﬁnite Let ˆVopt denote optimal interval value function ETRC The following theorem establishes optimality equation ETRC shows optimal interval value function solution optimality equation3 Theorem 1 Suppose Assumption 1 holds Then The upper bound optimal interval value function V opt satisﬁes equation V sup πΠ max MM VIMπ V sup πΠ R FMππ V IVIoptV b The lower bound optimal interval value function V optW satisﬁes equation V sup πρW min MM VIMπ V sup πρW R FMππ V IVI optW V value function W associated action selection function 3 2 This includes case limit 3 All proofs presented Appendix A readability D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 949 Based Theorem 1 value iteration operator cid7IVIopt deﬁned Eq 2 following lemma establishes monotonicity operator Lemma 2 Suppose U V value functions V U cid2dom V IVIoptU cid2dom IVIoptV b IVI optW U cid2dom IVI optW V value function W associated action selection function 3 Clearly Assumption 1 necessary computational approach In general case expected total reward criterion ETRC validate assumption holds However maximum probability reachability problem interval value function interpreted interval probability Assumption 1 easily validated shown Section 4 4 Maximum probability reachability problem The maximum probability reachability problem based special case class BMDP models known nonnegative models named similarly MDP models 13 A BMDP model called nonnegative satisﬁes Assumption 1 rewards nonnegative In order prove convergence value iteration consider following assumptions addition Assump tion 1 Assumption 2 For q Q Rq cid3 0 Assumption 3 For q Q π Π V π q If BMDP consistent Assumptions 2 3 called nonnegative BMDP model value function ETRC called nonnegative interval expected total reward Note Assumptions 2 3 imply Assumption 1 Theorem 1 Lemma 2 hold nonnegative BMDP models In following Lemma 3 shows ˆVopt solution optimality equation Theorem 4 establishes convergence result interval value iteration nonnegative BMDPs Lemma 3 Suppose Assumptions 2 3 hold Then V opt minimal solution V IVIoptV V V V V 0 cid2 V p p Q b V optW minimal solution V IVI optW V V value function W associated action selection function 3 Theorem 4 Suppose Assumptions 2 3 hold Then ˆV 0 0 0 sequence ˆV n deﬁned ˆV n cid7IVIn converges pointwise monotonically ˆVopt opt ˆV 0 An instance maximum probability reachability problem BMDPs consists BMDP M cid3Q A ˆF Rcid4 destination set T Q The objective maximum probability reachability problem determine p Q maximum interval probability starting p ﬁnally reaching state T ˆU max Moptp sup πΠcid2opt cid5 cid6 U Mπ p U Mπ p U Mπ p min MM PrMπ cid3 tXt p T cid4 U Mπ p max MM PrMπ cid3 tXt p T cid4 U Mπ U Mπ probabilities deﬁnition values 0 1 interval value function satisﬁes Assumption 1 Note U Mπ p computed recursively cid8 cid10 U Mπ p minMM 1 qQ F M pq πpU Mπ q p Q T p T 5 950 D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 In order transform maximum probability reachability problem problem solvable interval value iter ation add terminal state r transition probability 1 action let destination states T absorbed terminal state transition r probability 1 action set reward destination state 1 state 0 Thus form new BMDP model cid11M cid3 Q A F Rcid4 Q Q r Rp cid8 1 0 A A p q Q α A ˆFpq α 0 0 1 1 p T p T Fpq α p T r p T r q cid17 r p T r q r 6 Since Rr 0 structure Fpq clear V cid11Mπ r affected value function cid17 states For p Q V cid11Mπ p min M cid11M cid16 Rp cid2 Speciﬁcally p T V cid11Mπ p min M cid11M cid16 Rp cid4 cid3 πp F M pq q Q cid2 q Q cid4 cid3 πp F M pq V Mπ q cid17 V Mπ q Rp V cid11Mπ r 1 7 8 From 6 7 8 follows U Mπ equivalent V cid11Mπ Similarly U Mπ equivalent V cid11Mπ Therefore ˆV cid11Mopt sup πΠcid2opt cid5 V cid11Mπ V cid11Mπ cid6 cid5 U Mπ U Mπ cid6 sup πΠcid2opt ˆUMopt 9 The BMDP cid11M constructed satisﬁes Assumptions 2 3 interval value function state exists cid11M maximum probability reachability problem solved interval value iteration convergence assured Theorem 4 Note assume existence proper policy 23 Convergence guaranteed assump tion The reason twofold rewards 0 destination state ii destination state goes probability 1 terminal state absorbing rewardfree Therefore cycle add total reward 5 Computational complexity In section polynomial time complexity interval value iteration ETRC Our discussion based BMDP models rewardfree absorbing state terminal state Without loss generality assume q1 terminal state The interval value function destination state set 1 1 The initial interval value states set 0 0 States terminal state accessible affect result interval value iteration algorithm interval value functions remain 0 0 algorithm proceeds This terminal state accessible state destination states accessible In order prove polynomial complexity interval value iteration consider following assumption Assumption 4 The terminal state accessible states Given BMDP possible exists set states set entered exists policy state set In MDPs sets states called end components 5 controllably recurrent states 4 deﬁned similarly BMDPs In veriﬁcation problem considered paper end component incur zeroreward replaced state transformed model satisﬁes Assumption 4 5 End components computed polynomial time 5 interval value iteration algorithms polynomial Assumption 4 polynomial BMDP D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 951 Lemma 5 Let X x Rn x cid3dom 0 x1 0 suppose Assumption 4 holds Then IVIopt contraction mapping respect weighted norm cid18 cid18w X b value function W associated action selection function 3 IVIWopt contraction mapping respect weighted norm cid18 cid18w X The following theorem shows interval value iteration algorithm polynomial based similar argu ment 12 Theorem 6 Interval value iteration converges desired interval value function number steps polynomial number states number actions number bits represent BMDP parameters 6 Conclusions The results described paper interval value iteration proper restrictions reward transition functions solve BMDPs expected total reward criterion These results allow solve variety new problems BMDPs The paper focuses maximum probability reachability problem uncertain systems Additional problems extension probabilistic models veriﬁcation subject current future work Acknowledgements This work partially supported National Science Foundation NSF Career award CNS0347440 We like thank anonymous reviewer identifying errors presentation technical results Appendix A Proofs Proof Theorem 1 Denote e vector 1 1 1 For ε 0 exists π1 Π satisﬁes cid3dom V opt εe By deﬁnition V opt V opt cid3dom VIMππ V π1 R FMππ V π1 π Π V π1 It follows R FMππ V π1 V opt cid3dom sup πΠ VIMππ V opt εe IVIoptV opt εe cid3dom sup πΠ sup πΠ R FMππ V opt εe Hence V opt cid3dom IVIoptV opt For ﬁxed q Q ε 0 exists π Π satisﬁes V π q cid3 V optq ε It follows V optq ε cid2 VIMππ V π q cid2 VIMππ V optq cid2 sup πΠ VIMππ V optq IVIoptV optq Hence V opt cid2dom IVIoptV opt Since V opt cid2dom IVIoptV opt V opt cid3dom IVIoptV opt hold follows The proof b similar cid2 Proof Lemma 2 For ε 0 exist M1 M π1 Π IVIoptU sup πΠ VIMππ U cid2dom R FM1π1 U εe cid2dom R FM1π1 V εe cid2dom sup πΠ VIMππ V εeIVIoptV εe Since ε chosen arbitrarily holds The proof b similar cid2 952 D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 Proof Lemma 3 We denote F n The upper bound interval value function π Π deﬁned V π limN Assumption 2 limit deﬁned Further Assumption 3 V π Mπ nstep transition probability matrix V n sequence V n VIn n1 F n1 Mππ cid10 N Mπ V 0 R V opt sup πΠ V π cid3dom 0 A1 Suppose exists V V V IVIoptV By deﬁnition IVIopt V IVIoptV cid3dom VIMππ V R FMππ V π Π Hence V cid3dom R FMππ V cid3dom R FMππ R F 2 Mππ V Ncid2 cid3dom n1 Since V V F N Mππ F n1 Mππ R F N Mππ V V N 1 π F N Mππ V V cid3dom 0 V cid3dom V N 1 π N Thus V cid3dom Vπ π Π Hence V V V IVIoptV cid20 V cid3dom sup πΠ Vπ V opt A2 Theorem 1 shows V opt satisﬁes optimality equation Together A1 A2 V opt minimal solution V IVIoptV The proof b similar cid2 Proof Theorem 4 We ﬁrst proof sequence V n deﬁned V n IVIn optV 0 converges pointwise monotonically V opt By Assumption 2 IVIopt0 cid3dom 0 according Lemma 2a V n increases monoton ically Also q Q V nq ﬁnite Hence V nq monotonically increasing bounded series limn V nq supn V nq V q exists Since V cid3dom V n Lemma 2a implies IVIoptV cid3dom IVIoptV n V n1 n Therefore IVIoptV cid3dom V For π Π n cid10 IVIπ V n cid2dom IVIoptV n V n1 cid2dom V cid2dom IVIoptV A3 According monotone convergence theorem limn IVIπ V n IVIπ V π Π Together A3 IVIoptV supπΠ IVIπ V cid2dom V cid2dom IVIoptV implies V ﬁxed point IVIopt Choosing ε 0 sequence εn satisﬁes n1 εn ε exists policy π satis opt0 N Hence Vπ εe cid3dom V implying V cid2dom V opt By ﬁes Lemma 3a V opt minimal solution optimality equation V V opt Similarly prove sequence V n deﬁned V n IVIn optW V 0 converges pointwise monotonically V optW value func opt ˆV 0 tion W associated action selection function 3 By deﬁnition cid7IVIopt Section 2 ˆV n cid7IVIn converge pointwise monotonically ˆVopt cid2 N n1 εne cid3dom IVIN N n0 F n cid10 R Mππ cid10 Proof Lemma 5 We follow approach 24 Suppose cid7M cid3Q A ˆF Rcid4 BMDP q1 terminal state ˆFq1q1 α 1 1 α A Rq1 0 For ﬁxed M M set remaining states mM 1 cid2 cid2 mM p QM Qq1 q2 qn partitioned nonempty subsets QM α A exists state r q1 QM pr α 0 choice r depends 1 p α Construct set weights wM 1 QM i1 F M QM 2 wM n mMcid2 cid18 1 ηM 2i cid19IQM qj wM j i1 cid18 ηM min pq α p q Q α A F M F M pq α 0 cid19 A4 Since 0 ηM 1 A4 0 wM cid2 mM For α A exists state ql q1 QM 1 j 1 2 cid2 j cid2 n Suppose qj 2 cid2 j cid2 n QM 1 cid2 α 0 Let γ M QM i1 F M qj ql D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 953 1 η2mM 11 η2mM We cid20 cid21 cid20 cid22 ncid2 ncid2 k1 F M qj qk αwM k α F M qj ql αwM l cid21 cid22 wM j F M qj qk wM j cid2 cid2 k1kcid17l cid3 1 F M qj ql cid18 1 ηM 2i1 cid4cid22 αwM l cid19cid22 wM j cid2 wM 1 j cid18 1 ηM 2i1 cid3 1 ηM wM l cid19cid22cid18 1 ηM 2i 1 cid19 cid4cid22 wM j cid2 γ M second inequality follows fact F M wM l cid2 1 ηM 2i2 Let ˆU cid23V ˆV cid23V interval value functions For ﬁxed p cid17 q1 assume IVIopt ˆU p cid2 IVIopt ˆV p Select α cid3 ηM inequality follows fact qj ql M M α A maximize expression VIMαV p Then 0 cid2 IVIopt ˆV p IVIopt ˆU p max αA cid2 Rp VIMαV p max max αA MM cid4 Rp pq αV q max MM cid3 F M cid2 VIMαU p cid2 cid3 F M cid4 pq αU q cid2 qcid17q1 cid2 γ M wM qQ cid3 pq αwM F M q cid18cid3 p max q cid4cid3 cid4 wM U q V q q qQ cid4 U q V q cid19 wM q It follows cid24 cid24 cid24wM cid24IVIopt ˆU IVIopt ˆV cid2 γ M cid18U V cid18wM denotes maximum norm cid18 cid18 scaled wM 1 wM 2 n cid18 Note maximizing MDP M independent V γ M depends transition matrix n cid18xcid18wM cid18x1 x2wM cid18 cid18wM xnwM M IVIopt contraction mapping The proof b similar The construction weights wcid7M 1 wcid7M n little different wM sense set available choices actions state p Q longer A ρW p cid2 2 wcid7M 2 wM Proof Theorem 6 By Eq 4 iteration operator cid7IVIopt works following way upper bound interval value function ﬁrst converge upper bound converged iteration continue lower bound converges The ﬁrst stage polynomial By Lemma 5 IVIopt contraction mapping upper bound value function respect weighted norm subset Rn successive estimates V opt produced converge geometrically unique ﬁxedpoint b By Theorem 4 unique ﬁxed point desired upper bound value function c The upper bound true ˆVopt optimal value function π maximizing MDP cid7M d The parameters MDPs cid7M speciﬁed number bits polynomial number bits specify BMDP parameters e Since IVIopt contraction mapping argument similar 24 upper bound converge number steps polynomial number states number actions number bits represent BMDP parameters Similarly second stage polynomial Hence value iteration algorithm polynomial cid2 References 1 HLS Younes DJ Musliner Probabilistic plan veriﬁcation acceptance sampling Proceedings AIPS02 Workshop Planning Model Checking 2002 pp 8188 2 J Heath M Kwiatkowska G Norman D Parker O Tymchyshun Probabilistic model checking complex biological pathways Proc International Conference Computational Methods Systems Biology 2006 954 D Wu X Koutsoukos Artiﬁcial Intelligence 172 2008 945954 3 H Pham Minimizing shortfall risk applications ﬁnance insurance problems The Annals Applied Probability 312 1 2002 143172 4 C Courcoubetis M Yannakakis Markov decision processes regular events IEEE Transaction Automatic Control 43 10 1998 13991418 5 L Alfaro Formal veriﬁcation probabilistic systems PhD thesis Tech Rep STANCSTR981601 Stanford University Stanford CA 1997 6 J Rutten M Kwiatkowska G Norman D Parker Mathematical Techniques Analyzing Concurrent Probabilistic Systems CRM Monograph Series vol 23 American Mathematical Society 2004 7 M Jaeger Probabilistic decision graphscombining veriﬁcation AI techniques probabilistic inference International Journal Un certainty Fuzziness KnowledgeBased Systems 12 2004 1942 8 HJ Kushner P Dupuis Numerical Methods Stochastic Control Problems Continuous Time second ed SpringerVerlag New York 2001 9 E Feinberg A Shwartz Handbook Markov Decision Processes Methods Applications Kluwer Academic Publishers Boston MA 2002 10 A Nilim LE Ghaoui Robust control Markov decision processes uncertain transition matrices Operations Research 53 5 11 D Wu XD Koutsoukos Probabilistic veriﬁcation boundedparameter Markov decision processes V Torra Y Narukawa S Miyamoto Eds Modeling Decisions Artiﬁcial Intelligence MDAI 2006 Tarragona Catalonia Spain April 35 2006 Lecture Notes Artiﬁcial Intelligence vol 3885 Springer 2006 pp 283294 12 R Givan S Leach T Dean Boundedparameter Markov decision process Artiﬁcial Intelligence 122 12 2000 71109 13 ML Puterman Markov Decision Processes Discrete Stochastic Dynamic Programming John Wiley Sons Inc New York 1994 14 JK Satia RE Lave Markovian decision processes uncertain transition probabilities Operations Research 39 1953 10951100 15 CC White HK Eldeib Markov decision processes imprecise transition probabilities Operations Research 43 1994 739749 16 JA Bagnell AY Ng JG Schneider Solving uncertain Markov decision problems Tech Rep CMURITR0125 Robotics Institute Carnegie Mellon University Pittsburgh PA August 2001 17 M Kurano M Yasuda J Nakagami Interval methods uncertain Markov decision processes Markov Processes Controlled Markov Chains 2002 223232 18 A Tewari PL Bartlett Bounded parameter Markov decision processes average reward criterion Proceedings 20th Annual Conference Learning Theory Lecture Notes Computer Science vol 4539 Springer 2007 pp 263277 19 S Kalyanasundaram EKP Chong NB Shroff Markovian decision processes uncertain transition rates Sensitivity robust control Proc 41th IEEE Conference Decision Control 2002 20 O Buffet Reachability analysis uncertain ssps ICTAI 05 Proceedings 17th IEEE International Conference Tools Artiﬁcial Intelligence 2005 pp 515522 21 IO Kozine LV Utkin Intervalvalued ﬁnite Markov chains Reliable Computing 8 2 22 K Sen M Viswanathan G Agha Modelchecking Markov chains presence uncertainties TACAS Lecture Notes Computer Science vol 3920 Springer 2006 pp 394410 23 DP Bertsekas JN Tsitsiklis Parallel Distributed Computation Numerical Methods PrenticeHall Inc Upper Saddle River NJ 1989 24 P Tseng Solving Hhorizon stationary Markov decision problems time proportional logH Operations Research Letters 9 5 1990 287297