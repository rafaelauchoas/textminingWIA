Artiﬁcial Intelligence 117 2000 231253 Robust logics I Leslie G Valiant 1 Division Engineering Applied Sciences Harvard University Cambridge MA 02138 USA Received 14 October 1998 Abstract Suppose wish learn examples counterexamples criterion recognizing assembly wooden blocks constitutes arch Suppose prepro grammed recognizers relationships ontopofx y abovex y believe possibly complex expression terms base relationships sufﬁce approx imate desired notion arch How formulate relational learning problem exploit beneﬁts demonstrably available propositional learning attribute efﬁcient learning linear separators errorresilient learning We believe learning general setting allows multiple objects relations way fundamental key resolving following dilemma arises design intelligent systems Mathematical logic attractive language description clear semantics sound proof procedures However basis large programmed systems leads brittleness practice consistent usage predicate names guaranteed application areas mathematics viability axiomatic method demonstrated independently In paper develop following approach circumventing dilemma We suggest brittleness overcome new kind logic statement learnable By allowing learn rules empirically environment relative particular programs recognizing base predicates enable acquire set statements approximately consistent world need globally knowledgeable consistent programmer We illustrate approach describing simple logic sound efﬁcient proof procedure reasoning instances rendered robust having rules learnable The complexity accuracy learning deduction provably polynomial bounded cid211 2000 Elsevier Science BV All rights reserved I This research supported grants NSFCCR9504436 NSFCCR9877049 ONRN00014 9610550 ARODAAL0392G0115 A preliminary version paper appeared Proc 31st ACM Symposium Theory Computing Atlanta GA May 1999 pp 642651 1 Email valiantdeasharvardedu 0004370200 matter cid211 PII S 0 0 0 4 3 7 0 2 0 0 0 0 0 0 2 3 2000 Elsevier Science BV All rights reserved 232 LG Valiant Artiﬁcial Intelligence 117 2000 231253 Keywords Learning Reasoning Deduction Soundness Robustness Binding problem Learning rules Learning relations PAC learning PAC semantics 1 Introduction According Aristotle belief comes syllogism induc tion 3 Computational systems aspire exhibit characteristics intelligence need manipulate beliefs world It reasonable ask useful Aristotles dictum construction systems Our purpose argue duality expressed dictum fundamental In particular present mal encapsulates duality believe offers vehicle studying theoretical basis systems The history artiﬁcial intelligence interpreted having revolved duality beginning Since 1950s dominant paradigm advocated particularly McCarthy 28 knowledge programmed systems uniform logical language set rules logical inference procedures syllogisms draw new inferences As far learning Turing speculated earlier 1950 inductive learning build machines think 41 A years later pointed computational limitations logical reasoning 42 In years progress machine learning computational logic Nevertheless currently dominant theories phenomena inductive learning logical reasoning largely disparate notable exception area inductive logic programming 30 The purpose paper suggest formal reconciles principled deduction characteristic logic robustness characteristic learning More generally encompasses learning reasoning integrated way retains somewhat different crucial beneﬁts offer The particular beneﬁts mathematical logic wish retain existence clearly deﬁned semantics statement existence proof procedures enable new statements derived In particular deﬁned semantics makes possible proof procedures sound new statements derived true statements true The main beneﬁts learning need retained provides mechanism knowledge acquired fragments incomplete inconsistent inaccurate understanding current global state knowledge consistent language description world These beneﬁts learning somewhat irreconcilable standard logics rules expressed terms set predicates globally consistent meanings Such enforced global consistency achievable application areas known amenable axiomatization mathematics In areas world commonsense knowledge axiomatizations approach led systems brittle The minimal requirements shall robust logic existence LG Valiant Artiﬁcial Intelligence 117 2000 231253 233 Rules encode knowledge allow multiple objects relations ii A deﬁned semantics rules iii An inference procedure applying rules instances sound polynomial time iv Polynomial time algorithms learning rules examples Standard predicate calculus suitably constrained Horn clauses example satisﬁes ﬁrst requirements Part iv deﬁnition interpreted simply claiming logic robust provides means evaluating accuracy statement independently empirical process access raw data world In paper particular robust logic It arose efforts provide theoretical underpinnings neuroidal architecture described companion paper 47 It added beneﬁts derive architecture v The rules implemented ﬁxed network mirrors modularities dependencies inference procedure executed naturally network vi The classes connective functions allowed include class linear threshold functions ideal learnability properties exist efﬁcient learning algorithms attributeefﬁcient according experimental evidence errorresilient The main departure traditional logic semantics PAC semantics adopted machine learning This views inductive learning compu statistical phenomenon observations world provide empirical evidence rules hold generally world The learner chooses space possible rules according weight statistical evidence computational constraints choice computationally feasible This allows robust learning inaccurate inconsistent data accomplished rigorous foundation Most basically assumed instant set primitive sensors observations world regarded induced probability distribution D sets sensor readings This distribution D arbitrarily complex reﬂecting complexities world In general need know details D directly It instead aim deal rules simple truths sense hold D high probability A based logic given rules false time capability recognizing fact making empirical observations There exist alternative approaches address aspects learning reasoning simultaneously We shall discuss brieﬂy Section 4 The mechanisms needed learning reasoning particular robust logic computationally efﬁcient Their complexity depends polynomially number object variables number rules length exploit recursion The exponential dependence arity relations shall assume bounded small constant We believe set requirements satisﬁes indispensable formal viable basis computational intelligence 234 LG Valiant Artiﬁcial Intelligence 117 2000 231253 2 Scenes S Informally objects robust logic thought referring directly entities world instead representations internal image corresponds shortterm working memory The contents image called scene Scenes deﬁned terms pair A e A set fa1 ang objects tokens e set relations f eR1 eRt g objects The arity cid11i relation eRi 2 e number arguments The set relations e arity called ei cid18 e Hence e D union set K nonnegative integer values For simplicity shall assume sets A e arities ﬁnite deﬁne cid11 maximum arities cid11i 1 6 6 t A scene cid27 vector length L D LAe D iDj cid01 iD1 t iD1 ncid11i The ncid11j entries ﬁrst ncid11i regard j th group These truth values j th relation eRj ncid11j combinations cid11j objects selected n objects A cid11j arguments eRj Thus A D fa1 a2 a3g eR1 arity vector values 32 D 9 entries eR1a1 a1 eR1a1 a2 eR1a3 a3 corresponding entries eR2 eR3 eRt Some ﬁxed lexicographic ordering assumed group P P P t iD1 ncid11i The values vector set f0 1g For ﬁrst entry example 1 mean R1a1 a1 true 0 mean false The set P vectors A e shall denote cid5Ae cid5 short Clearly jcid5j D 2 t iD1 ncid11i We deﬁne set cid5 obscured scenes In vector elements value denotes corresponding relationship obscured There jcid5 j D 3 obscured scenes We assume space cid5 scenes A e associated probability distribution DAe cid5 In words scene cid27 2 cid5 distribution speciﬁes probability Dcid27 scene drawn randomly cid5 cid27 Clearly distribution deﬁnes probability condition scenes Thus D eR1a1 a2 D 1 probability random scene drawn according D eR1a1 a2 D 1 It eR1x1 x2 sum Dcid27 attaches probability conditions 9x18x2 cid27 condition holds When considering learnability useful distinguish distributions D cid5Ae unrestricted symmetric permutations A For scene cid27 2 cid5Ae exist nW cid0 1 scenes necessarily distinct obtained cid27 permuting tokens a1 2 A way We deﬁne distribution D cid5Ae symmetric scenes obtained permuting tokens way probability Since general intention regard A set tokens structure shall assume indicated distributions D symmetric However restriction appear required fundamental purpose results apply equally asymmetric case bounds learnability larger The interpretation general framework follows The distribution D deﬁned spirit PAC probably approximately correct learning imposed LG Valiant Artiﬁcial Intelligence 117 2000 231253 235 specifying possibly arbitrarily complex world The intelligent needs able strategies concepts work complex world theory PAC learning suggests strategies concepts learned examples drawn D needing D explicitly The set A tokens intended correspond representations image internal discussed 47 An important general point logic designed start cope partial knowledge The relation set e space base relations recognize time question For scene truth values relations explicitly available In real world room example object material depiction room information materials available shall speciﬁed We assume D imposes natural distribution knowledge speciﬁed Consider discussed unary relation speciﬁes object penguin 10 We want cid5 distinguish cases true false unspeciﬁed For reasons advantageous predicates logic values f0 1g In order allow implementation expect relations deﬁned encode value unspeciﬁed f0 1g A natural implementation unary predicates penguin penguincid3 penguincid3a1 D 0 mean scene specify a1 penguin If scene specify a1 penguin penguincid3a1 D 1 hold case penguina1 D 1 denote a1 penguin penguina1 D 0 By having binary versions primitive predicate way element cid5 effectively encode primitive predicate binding scene true false unspeciﬁed Since scenes drawn probability distribution D cid5 probability distribution imposed relationbinding combinations speciﬁed In words acknowledged start natural situations partial knowledge usually available The fact learning takes place situations partial knowledge offered explanation results PAC learning applied effectively situations partial knowledge This provides principled approach partial knowledge based learning suggested 4546 In practice bindings base relations unspeciﬁed value In case sake succinctness advantageous represent scene listing relations bindings speciﬁed This ensure size descriptions scenes economical possible We note notion obscured vector elements described earlier orthogonal idea unspeciﬁed vector elements Once scene drawn D f0 1g vector elements determine relations bindings speciﬁed We subsequently purposes analysis obscure hide arbitrary subset vector elements replacing symbols scene obscured For example performs planning wishes evaluate consequence situation construct scenes relations describing situation unobscured relating consequences obscured The deduction process evaluate likely values obscured relations 236 LG Valiant Artiﬁcial Intelligence 117 2000 231253 In context deduction shall distinguish vector elements determined Initially obscured element undetermined It determined deduction procedure deduced value 3 A robust logic 31 Syntax A rule intended statement precise sense holds certainty probability scenes drawn D Suppose scenes deﬁned A D fa1 ang e D f eR1 eRt g In describing rules shall deﬁne set symbolic relations D fR1 Rt g arity Ri eRi The role Ri model actual relation eRi Thus rule set rule 8x R2x cid17 R3x intention corresponding statement 8x eR2x cid17 eR3x hold high probability scenes drawn D Also suppose particular scene cid27 randomly drawn D certain given set relations e hold certain bindings Suppose relations bindings corresponding symbols replace e symbols premises rule set We want deduced rules R3a2 a4 example holds eR3a2 a4 hold high probability randomly drawn scene given set relations bindings hold In words want deductions rules lead conclusions semantically true randomly drawn scenes high probability To deﬁne syntax rules A need set C connectives set X object variable names standard symbols The set connectives C D S1 Ci set functions f 2 Ci Boolean function f f0 1gi f0 1g iD1 We shall choose C good learning properties In particular efﬁcient errorresilient attributeefﬁcient algorithms learning members Linear threshold functions prime example class 4572648 Attributeefﬁciency means number examples grows linearly number relevant variables logarithmically irrelevant ones This important current context envision variables generated automatically large numbers example represent relations possible bindings number available examples limited A rule q form 8x1 xs 2 A Qx1 xs cid27 cid2 cid0 f e1Ri1 eRi cid1 cid17 Ri0x1 xs cid3 x1 xs 2 X f 2 C 0 6 j 6 Rij independently quantiﬁed expression IQE form 2 Further 1 6 j 6 ej Rij 41yj1 4kj yjkj 2 A Rij zj1 zjmj mj arity cid11ij Rij 4h 2 f9 8g 1 6 h 6 kj yjh 2 X g cid18 X zjh 2 fx1 xsg fyj1 yjkj LG Valiant Artiﬁcial Intelligence 117 2000 231253 237 D yj2h2 j1 D j2 Also Q precondition arbitrary predicate yj1h1 value depends scene cid27 binding cid25 fx1 xsg A question Its value depend relation binding pairs values unobscured input cid27 determined deduction input Allowing precondition Q adds generality framework For example access examples concept certain subdomain characterize Q In case learning rule precondition appropriate especially reason believe concept simple deﬁnition broader domain Much analysis unchanged total precondition Q cid17 True case shall brevity suppress precondition We impose general restrictions Q learnability representability Even preconditions programmed human useful role Some natural restrictions aid mechanical use warrant investigation Q expressed succinctly computed easily terms C Q symmetric invariant permutations fa1 ang truth Q derived given set rules A simple example rule cid2 cid0 8x1 x2 2 A Th2 9y1R1x1 x2 y1 9y2R2x1 y2 9y38y4R3x2 y3 y4 cid17 R4x1 x2 cid3 cid1 total precondition Q cid17 True suppressed brevity The function Th2 threshold function arguments deﬁned true arguments true The expression left cid17 sign called lefthand q right righthand These abbreviated LHSq RHSq The important constraint deﬁnition example constituent quantiﬁed expressions LHSq quantiﬁed mutually disjoint sets variable names In example sets fy1g fy2g fy3 y4g The signiﬁcance constraints f e1 e evaluated scene cid27 binding cid25 fx1 xsg A value ei evaluated independently values y variables ei true bearing y variables ej true j 6D This notion equivalent realized connection bindings 47 This constraint implies example relationship expressed predicate calculus statement cid0 cid1 grandfatherx y cid17 9z fatherx z parentz y expressed directly single rule contains father parent conjunction The following pair rules sufﬁce 8x y z 2 A cid2 8x y 2 A cid2 cid3 fatherx z parentz y cid17 grandfather cid3x y z cid17 grandfatherx y 9z grandfather x y z cid3 cid3 The signiﬁcance disjointness constraint following For reasons computational economy want minimize costs enumerating bindings One approach relations use minimal number arguments sufﬁce 238 LG Valiant Artiﬁcial Intelligence 117 2000 231253 The grandfather predicate needs use higher level reasoning knowledge identity person z intermediate generation irrelevant On hand recognizing occurrence grandfather relationship ﬁrst place need establish existence intermediate person Hence level need relation having argument The formulation acknowledges realities economically possible This simple example illustrates general approach relies heavily assumption knowledge represented binding modularity terms relations small arity As second example consider notion dining room This deﬁned terms chairs tables relationships In turn chair deﬁned terms constituent parts relationships Binding modularity asserts arity relations needed kept small references identities parts needed deﬁne lower level notion chair redundant describing higher level notion dining room A issue variable bindings inequality constraints implied imposed In particular implied members fx1 xsg g bind distinct member A In formulation fyj1 yjkj speciﬁc combination inequality constraints Thus notation introduced specify example x1 x2 map distinct tokens x3 need distinct The general prohibition stated earlier distinct values j1 j2 bindings corresponding yj1h1 yj2h2 variables way constrain The following observation Fact 31 For relation R arity cid11 independently quantiﬁed expression eR value eR computed Oncid11 steps given values R ncid11 bindings cid11 arguments A Proof If deﬁnition eR order quantiﬁcation 41y1 42y2 4cid11ycid11 construct following rooted game tree depth cid11 Each node tree distance root corresponds substitution fy1 yig A Each node edges level C 1 nodes corresponding substitution extended possible substitutions yiC1 A Thus leaves tree depth cid11 correspond possible substitutions fy1 ycid11g A We label leaf value R substitution speciﬁed leaf We work starting leaves attach Boolean values node root reached The value attached node corresponding ﬁxed substitution fy1 yig A truth value 4iC1yiC1 4cid11ycid11R R denotes relation R given ﬁxed substitution It easy verify induction label node distance cid0 1 root 4i D 9 LG Valiant Artiﬁcial Intelligence 117 2000 231253 239 takes disjunction labels son nodes 4i D 8 takes conjunction labels son nodes Clearly similar procedure works inequality constraints imposed y2 6D y1 Also procedure works deﬁnition eR arguments ﬁxed values A quantiﬁcation occurs subset remaining variables 2 We note existential quantiﬁcations allow algorithm successfully label root node cases relation R determined bindings corresponds obscured value input deduction performed corresponding leaves game tree labels We shall eR determined procedure succeeds giving root node valid label We note discuss deduction Section 34 shall entertain possibility ﬁxed R ﬁxed binding evaluates values 0 1 case value written 01 The game tree evaluation procedure adapted case evaluate consequences 0 value 1 value To summarize leaf values 01 undetermined 01 The labelling procedure attach values internal tree nodes proceeds When evaluating expression e means procedure Fact 31 shall output undetermined value combination leaf values determines 0 1 value e ii 0 value combination determines 0 determines 1 iii 1 combination determines 1 0 iv 01 An important note process monotone sense e determined subsequently changing undetermined input value determined determined 01 e undetermined The size description kqk rule q number occurrences relation symbols deﬁnition The size description kSk set S rules sum kqk q 2 S The size jSj S number rules S A rule set S acyclic graph GS acyclic GS deﬁned following manner The graph GS directed graph G D V E V set nodes set E set edges subset cid2 In particular directed edge Ri Rj 2 E rule S contains Ri lefthand Rj righthand We denote class acyclic rule sets cid0 class arbitrary rule sets cid0 cid3 For cid11 D 1 2 denote cid0cid11 cid0 cid3 cid11 classes restricted rule sets maximum arity relation cid11 We allow rule share R 2 right hand This allows multiple deﬁnitions R possibly different preconditions depending different relation sets left hand Note rules R right hand Ri common left hand edge 240 LG Valiant Artiﬁcial Intelligence 117 2000 231253 GS arisen rule Of course multiple rules rise mutually contradictory deductions It turn results apply equally general graphs acyclic ones The importance allowing general graphs recursive rules expressed For example earlier expressions given adapted 8x y z 2 A cid2 8x y 2 A cid2 ancestorx z parentz y cid17 ancestor cid3x y z cid3 9z ancestor cid3x y z cid17 ancestorx y cid3 The rules deﬁned compared Horn clauses ﬁrst order predicate calculus The implication sign replaced equality conjunctions left hand generalized wider classes connectives including linear thresholds learning tractable robust On hand sake controlling computational complexity restrictions imposed A restriction fundamental shared example datalog model databases 43 function symbols predicate calculus excluded order prevent proliferation objects quantiﬁed This restriction enables complexity binding problem controlled For example constant Napoleon function parent expressions form parentiNapoleon refer unbounded number individuals ancestors Napoleon Another difference instead having constants refer individuals instead use unary predicates Thus instead constant Napoleon Napoleonai corresponds token ai qualiﬁed unary relation representing individual 32 Semantics In previous section deﬁned syntax rules rule sets terms triple A C set relations A set tokens C set connective functions Now scene cid27 2 cid5Ae binding cid25 fx1 xsg A rule q cid2 f 8x1 xs Qx1 xs cid27 cid0 e1Ri1 eRi cid17 Ri0 x1 xs cid1 cid3 examined truth values ej Rij lefthand cid0 cid1 f e1Ri1 eRi righthand Ri0 x1 xs We deﬁne f0 1gvalued functions lhsq cid27 cid25 rhsq cid27 cid25 follows The function lhsq cid27 cid25 D 1 binding cid25 makes f true cid27 e relations substituted relations The function rhsq cid27 cid25 D 1 eRi0 D 1 cid27 binding cid25 We deﬁne false positive false negative f0 1gvalued error functions errcid0q cid27 cid25 errCq cid27 cid25 follows errCq cid27 cid25 D 1 Qcid25 cid27 D 1 lhsq cid27 cid25 D 1 rhsq cid27 cid25 D 0 similarly errcid0q cid27 cid25 D 1 LG Valiant Artiﬁcial Intelligence 117 2000 231253 241 Qcid25 cid27 D 1 lhsq cid27 cid25 D 0 rhsq cid27 cid25 D 1 Note Q depends scene cid27 binding cid25 D fx1 xsg A slight abuse notation express Qx1 xs cid27 Qcid25 cid27 Finally deﬁne probability rule q predict false positives false negatives scenes cid27 drawn randomly cid5 D cid5eA according distribution D follows erDq D max cid25 X cid0 Dcid27 err cid27 Cq cid27 cid25 C err cid0q cid27 cid25 cid1 We note giving scene cid27 arbitrary probability deﬁned D restrict sum cid27 satisfy Q Also require small error binding deﬁne error measure maximum errors s D ncid11i0 cid0 bindings Note deﬁne analogously er D erD omitting errcid0 errC term respectively As summarizing simple measure accuracy use erD note case D symmetric maximization cid25 deﬁnition replaced substitution ﬁxed cid25 C D er Deﬁnition A rule q accurate D erDq 6 33 Learnability The goal logic provide framework reason knowledge learned Hence central minimal notion learnability rule sets This minimal notion capture idea complete speciﬁcation rule identity connective f 2 C approximation f learned PAC sense random examples scenes cid27 2 cid5 drawn according distribution D Clearly complete speciﬁcation requires speciﬁcations identities quantiﬁers relations Ri0 Ri1 Ri It requires speciﬁcations cid11ih argument positions Rih arguments x1 xs Ri0 substituted The remaining arguments distinct values h 1 6 h 6 l There additional inequality constraints speciﬁcation Deﬁnition For set rules rule q form 8x1 xs 2 A Qx1 xs cid27 cid0 e1Ri1 eRi f cid2 cid1 cid17 Ri0 x1 xs cid3 31 C A consider learning algorithms unit time obtain desired binding cid25 scene randomly chosen distribution D restricted scenes satisfy Qcid25 cid27 We class rules PAClearnable scenes learning algorithm rule q form 31 0 cid14 0 symmetric D runs time ﬁxed polynomial 1 1cid14 n size description q outputs probability 1 cid0 cid14 f 0 2 C makes rule accurate f 0 instantiated place f The oracle Qcid25 cid27 called arbitrary requested binding cid25 return unobscured cid27 randomly chosen according D 242 LG Valiant Artiﬁcial Intelligence 117 2000 231253 satisfying Qcid25 cid27 If Qcid25 cid27 false cid27 cid27 having nonzero probability D arbitrary f 0 returned In words seeking learn rule ﬁxed form following 8x1 x2 x3 Qx1 x2 x3 cid27 cid2 f 9y1R1x1 x2 y1 9y2R2x2 x3 y2 cid17 R1x1 x2 x3 cid3 The algorithm seek ﬁnd f 0 substituted place f ﬁxed form hold high probability random examples satisfying Q Note restriction f ﬁxed form ceases true restriction enumeration el c complete enumeration base relations bindings envisage viable C learnable attributeefﬁciently In symmetric case following Theorem 1 If C PAClearnable class Boolean functions algorithm M number L cid14 examples sufﬁcient learn C accuracy conﬁdence 1 cid0 cid14 A class rules constant arity C A PAClearnable scenes Also L cid14 examples sufﬁce learn accuracy conﬁdence 1 cid0 cid14 D symmetric cid3 Proof We assume rule 8x1 xs Qx1 xs cid27 cid2 f cid0 e1Ri1 eRi cid1 cid17 Ri0 x1 xs 32 descriptions ehRih 1 6 h 6 l known identity Ri0 Given set scenes cid27 2 cid5Ae drawn according D task learning algorithm derive f 0 2 C substituted f 32 rule conﬁdence 1 cid0 cid14 accurate The constant arity bound ensures Fact 31 value ehRih evaluated polynomial time scene Since D symmetric sufﬁce learn ﬁxed binding cid25 cid3xi D ai 1 6 6 s We shall assume sufﬁcient statement Theorem algorithm access source examples cid27 satisfy Qcid25 cid3 cid27 D 1 chosen binding cid25 cid3 For input scene cid27 shall consider truth value eRi0 ej eRij 1 6 j 6 substitution cid25 cid3 expressions This vector l truth values input f Boolean value output By assumption learning algorithm M C learn f 0 2 C accurate conﬁdence 1 cid0 cid14 L cid14 examples time polynomial cid01 cid14cid01 size description f But equivalent saying conﬁdence 1 cid0 cid14 rule q 0 learned rule q f 0 substituted f property erDq 0 cid25 cid3 D X cid0 Dcid27 err cid27 Cq 0 cid27 cid25 cid3 C err cid0q 0 cid27 cid25 cid3 6 cid1 Since D symmetric quantity erDq 0 cid25 cid3 invariant choice cid25 cid3 Since eDq deﬁned 33 maximum erDq cid25 cid25 conclude erDq 0 6 2 LG Valiant Artiﬁcial Intelligence 117 2000 231253 243 If D symmetric learnability deﬁned learning problem fundamentally difﬁcult However general learn distinct f 0 cid25 distinct cid25 fx1 xsg A The oracles Qcid25 cid27 deﬁned deﬁnition PAClearnability rules provides source examples needed cid25 However sample complexity potentially increase factor ns The following brief discussion intentions preconditions Q Clearly preconditions number valuable uses For example simple rule Rx exists known certain subdomain Q Alternatively small number subdomains cover positive occurrences R distinguishing contexts explicitly best way encoding relevant information How constraining preconditions allowed If regard distribution D referring base domain Q0 deﬁnitions intended ﬁrst instance refer case Q rule covers signiﬁcant inverse polynomial D If holds deﬁnition PAClearnability rules assumption examples oracle produce examples satisfy Q eliminated instead simply discard examples satisfy Q For rule set convenient deﬁne Q0 union preconditions rules set assuming rule satisfactory precondition Philosophically hypothesize universal precondition Qcid3 distribution Dcid3 Q0 D embedded In practice appears preferable discuss minimal Q0 D apply given rule set universal Qcid3 Dcid3 As mentioned earlier section important fact C choose useful classes learnable attributeefﬁciently Since number arguments f grow ncid11 possible quantiﬁed expressions arguments desirable sample complexity smaller quantity This possible example C chosen Boolean disjunctions terms Boolean functions deﬁned linear inequalities small integer coefﬁcients terms 2648 We note exact number possible quantiﬁed expressions written follows inequality constraints allowed If Ri0 righthand arity s relation arity cid12 6 cid11 number expressions arguments quantiﬁed 2iscid12cid0i Summing D 0 cid12 gives s C 2cid12 cid0 cid1 cid12 34 Deduction Soundness completeness In section discuss algorithms making deductions obscured scenes means set S rules thought capturing relevant knowledge We restrict deduction algorithms efﬁcient sense require polynomial time The crucial property desired deduction algorithm sound sense conclusion reached true high probability constituent rules accurate 244 LG Valiant Artiﬁcial Intelligence 117 2000 231253 We shall use following deﬁnitions Deﬁnition A deduction algorithm takes input set S rules respect A C ii relation Ri 2 righthand q 2 S iii binding cid25 arguments Ri A iv scene cid27 2 cid5 Ri binding cid25 eA obscured replacement values including The deduction algorithm outputs predicted f0 1 01g value Ri cid27 binding cid25 special predicted symbol Note deﬁnition allows deduction algorithm output predicted time In Section 343 shall introduce notion completeness disallow misuse possibility Deﬁnition A deduction algorithm polynomial time runs time bounded ﬁxed polynomial size description kSk rule set S size description scene cid27 size n D jAj A maximum computational complexity cS connectives C appearing S Deﬁnition A deduction algorithm accurate rule set S distribution D Ri 2 cid25 predicted f0 1 01g value Ri binding cid25 incorrect probability scene cid27 drawn randomly according D In order interpret deﬁnition note Ri cid25 cid27 drawn D things happen predicted value Ri cid25 incorrect value 01 taken incorrect 0 1 value incorrect differs eRi cid25 cid27 b predicted 0 1 value correct c predicted value The deﬁnition insists eventuality occurs probability It instructive subdivide c subcases In subcase c1 prediction rules total precondition actual preconditions restrictive apply In second subcase c2 prediction preconditions total This subcase arise example needed values obscured cid27 rules implications Ri Deﬁnition A deduction algorithm PACsound polynomial time ii polynomial p D 0 accurate rule set S rule pkSkjAjaccurate jAj number elements A kSk size description S Note accuracy weak requirement rule sets S rarely produce predictions PACsoundness applied procedures complete strong LG Valiant Artiﬁcial Intelligence 117 2000 231253 245 341 The acyclic case We deﬁne acyclic deduction algorithm applied acyclic rule set S respect A C scene cid27 2 cid5 eA The algorithm ﬁrst constructs graph GS rule set renumbers relations Ri associated eRi topologically sorted directed edge Ri Rj GS case j At node Rj maintained table contains entries ncid11j possible bindings Rj A Each entry value f0 1 01 g Initially values unobscured cid27 entered appropriate 0 1 values tables All remaining entries The algorithm considers turn jSj rules order increasing j Rj right hand relative ordering rules Rj righthand arbitrary cid2 cid0 cid1 cid3 cid17 Rj 8x1 xs 2 A Qx1 xs cid27 f When considering current rule RHS D Rj algorithm position following ns bindings cid25 x1 xs A s D cid11j e1Ri1 eRij It tests Qx1 xs cid27 D 0 update Otherwise Rih 1 6 h 6 j binding cid25 0 y variables arguments Rih bound cid25 takes value Rih determined unobscured input having evaluated combined binding cid25 cid25 0 evaluates ehRih Fact 31 If values Rih needed evaluate ehRih include values way eh evaluated cid25 entry cid25 Rj updated Otherwise substituting j values eh f gives deduced value Rj binding cid25 x1 xs A entered value cid25 table node Rj If Rj occurs righthand rule cid25 acquire 0 value rule 1 value In case table entry 01 Now Rih rule current consideration occurs RHS rule acquired 01 value cid25 eh acquire 01 value execution procedure Fact 31 The algorithm choose f0 1g vector consistent values eh lexicographically ﬁrst evaluates connective f vector When deduction algorithm terminates outputs table entry relation binding pair value requested value outputs predicted Theorem 2 For ﬁxed cid11 acyclic deduction algorithm PACsound acyclic rule sets cid0cid11 composed relations maximum arity cid11 Proof The algorithm repeats action described paragraph ncid11q times rule q 2 S Each action evaluates q values ehRih h performs evaluation connective function Hence overall complexity algorithm subsequent initialization tables upper bounded cid1 cid0 Evn cid11lq C cq ncid11q X q 246 LG Valiant Artiﬁcial Intelligence 117 2000 231253 P cq complexity evaluating connective f rule q Evn cid11 complexity evaluating quantiﬁed expression relation arity cid11 image n object variables If let kSk length description S deﬁned earlier q q C 1 Hence denote cS maximum complexity kSk D connectives S use Fact 31 upper bound Evn cid11 Oncid11 runtime algorithm upper bounded term linear description input cid27 plus term linear ncid11 cid0 kSkncid11 C jSjcS cid1 This establishes algorithm polynomial time cid11 regarded constant We algorithm sound sense rule S accurate D output algorithm inaccurate small probability random scene cid27 drawn according D Consider acyclic rule set S scene cid27 As algorithm evaluates S evaluates jSj rules considers sequence When evaluating connective f rule q binding cid25 constituent Rih lefthand uses values Rih different bindings consistent cid25 evaluated rules Rih righthand computed Now consider event cid27 drawn randomly according D eRj D 0 binding cid25 deduction algorithm outputs Rj D 1 binding alternatively eRj D 1 algorithm outputs Rj D 0 Then ﬁrst rule binding topological ordering mistake deduced values Rih lefthand equal real values eRih connective f produces answer different true value righthand But quantity erDq D max cid25 X cid0 Dcid27 err cid27 C q cid27 cid25 C err cid0 q cid27 cid25 cid1 upper bounds probability cid25 random cid27 D satisﬁes Q causes rule q err Let assume sufﬁcient deﬁnition PACsoundness probability jSjncid11 Then probability random cid27 cause rule S err binding jSjncid11 times quantity This establishes deduction algorithm PACsound 2 342 The general case We general deduction algorithm applied arbitrary rule sets acyclic ones For arbitrary rule set S deﬁne directed evaluation graph EGS D V E follows We assume rules deﬁned manner 31 ordered arbitrarily j D 1 jSj We denote right hand j th rule Rrj More rule share RHS case r onetoone The vertex set V EGS V D jSj j D1 cid8 cid9 rj cid25 j cid25 fx1 xcid11j g A In words vertex corresponds relation Rrj binding arguments A Hence certainly jV j 6 jSj cid1 ncid11S With regard edges E EGS LG Valiant Artiﬁcial Intelligence 117 2000 231253 247 rule j associated necessarily disjoint set directed edges vg vh vh D rj cid25 cid25 vg D k cid25 0 Rk appears LHSj More precisely presence vg vh E denotes order evaluate Rrj cid25 LHSj value Rk cid25 0 needed For example suppose rule j D 5 cid2 cid0 cid1 cid3 f 8x1 2 A 9y1 R1x1 y1 9y2 R2y2 x1 33 Also let denote binding cid25 fx1 xsg facid251 acid25sg Tcid251 cid25sU Then vh D 3 T4U vg D 2 T5 4U edge vg vh present E verify R3a4 D 1 need value R2y2 a4 value y2 2 A including y2 D a5 cid17 R3x1 Whether value R2y2 a4 needed particular evaluation depends details actual implementation For example suppose value R2a7 a4 previously obtained course evaluation If value 1 determined value 9y2R2y2 a4 value R2a5 a4 longer needed If value 0 hand value R2a5 a4 necessary determine value 9y2R2y2 a4 The indegree node determined simply syntax In example described expression 33 node 3 TiU 2n predecessors deriving rule half nodes 1 Ti j U 1 6 j 6 n half 2 Tk iU 1 6 k 6 n If R3 appears RHS rules indegree larger More formally predecessors EGS node j cid25 cid25 0 following property rule Rj righthand occurs Ri lefthand x variables occur common parameters Rj Ri rule bindings cid25 cid25 0 map 2 A We shall regard deduction algorithm procedure dynamically assigns label f0 1 01 g node EGS A node ﬁrst labels 01 01 considered set node labelled considered unset Initially node corresponds value obscured scene cid27 set f0 1g value speciﬁed cid27 All nodes initially given unset value The label value node v changed 0 1 labels predecessors v EGS fully determine arguments f determine f In example described 33 cid25 x1 a4 9y1R1a4 y1 9y2R2y2 a4 need determined example f parity function Note 9y1R1a4 y1 determined R1a4 ai set 1 ai 2 A R1a4 ai set 0 ai 2 A When node EGS set purposes algorithm description consider corresponding vector element cid27 originally obscured set value Clearly label node set 0 1 virtue rule label changed predecessor nodes arising rule set f0 1g values subsequently However node corresponds Ri occurs RHS rule contradiction tries set value 0 set 1 vice versa arise In case algorithm set value 01 As acyclic case node shall compute connective f ﬁrst time value deﬁned argument 01 label choose arbitrarily 248 LG Valiant Artiﬁcial Intelligence 117 2000 231253 The general deduction algorithm deﬁne follows The rules ordered arbitrary manner j D 1 jSj The labels nodes EGS unobscured 0 1 values given cid27 set values The algorithm scans rule set repeatedly terminates scan S completed updates occurred In scan enumerates rules j D 1 jSj considers updating In consideration enumerates bindings cid25 fx1 xcid11rj g A For binding precondition Q rule satisﬁed cid25 cid27 label node rj cid25 EGS changed rule fully determined labels predecessors EGS case previous scan algorithm evaluates f performs necessary update label In case 0 1 value assigned node set contrary 0 1 value previously virtue different rule set value changed 01 When algorithm terminates considers value label relation Ri binding cid25 speciﬁed deduction task outputs value If outputs predicted Theorem 3 For ﬁxed cid11 general deduction algorithm PACsound rule sets cid0 cid3 cid11 composed rules arity upper bounded cid11 Proof Each relation binding pair cid25 partake updateswhen ﬁrst set changed 01 Since cid25 pair initialized label start updated step follows 2tncid11 scans t relations t choices ncid11 choices cid25 Each scan considers node cid25 EGS times rules j rj D Hence performs jSjncid11 considerations In consideration examines j expressions LHS evaluates necessary Evn cid11 operations yield values computes connective f Hence ﬁrst upper bound runtime cid0 ncid11kSkEvn cid11 C ncid11jSjcS cid1 2tncid11 Now case considers algorithm globally number times connectives need evaluated number cid25 pairs jSjncid11 Also quantiﬁed expressions evaluated gametree manner Fact 31 reduced bound total work needed deduced global considerations Suppose modify algorithm node cid25 updated internal nodes game trees cid25 occurs leaf modiﬁed effects followed root game tree Following effects leaf costs cid11 operations accumulated updating game tree cid11na operations Using global considerations reduce complexity bound 2cid11tn2cid11 C jSjncid11cS The additional cost initializing labels linear quantity linear number nodes jSjncid11 EGS Hence algorithm satisﬁes polynomial time criterion cid11 constant LG Valiant Artiﬁcial Intelligence 117 2000 231253 249 For analyzing soundness suppose rule 0accurate This means binding cid25 RHS cid27 randomly drawn D case Qcid27 cid25 D 1 LHS 6D RHS probability 0 Now course evaluation general deduction algorithm performs jSjncid11 label updates If output algorithm incorrect given cid27 label updates rule j binding cid25 0 arguments LHSj computed correct cid27 applying function f speciﬁed rule j produced false value RHSj However ﬁxed j cid25 0 probability occurring 0 rule 0accurate assumption Hence probability false conclusions drawn algorithm jSjncid110 If 0 jSjncid11 follows output general deduction algorithm incorrect probability required 2 343 Completeness It clear deﬁnition accuracy need allow possibility deduction algorithm legitimately output predicted frequently It obscured scene cid27 speciﬁes values preconditions Q rules constraining On hand legitimate deduction algorithm output predicted circumstances valid impediment applies rules allow deduction accurate sound To resolve issue introduce following notions Deﬁnition For rule set S respect A C deduction sequence sequence triples qj cid25j bj j D 1 J qj 2 S cid25j binding f1 cid11j g A cid11j arity RHSqj bj 2 f0 1g Deﬁnition A deduction sequence valid input cid27 j 1 6 j 6 J following case The values valR cid25 pairs R cid25 unobscured input cid27 values bi taken values valRHSqi cid25i 1 6 j properties satisfy precondition Qj cid25j cid27 qj ii determine expressions LHSqj iii value connective fj rule qj speciﬁed point equals bj Deﬁnition A deduction algorithm complete class rule sets S respect A C following holds scene cid27 2 cid5 eA exists valid deduction sequence terminating q cid25 b b 2 f0 1g deduction algorithm input S Ri D RHSq cid25 cid27 outputs predicted value f0 1 01g If remains observe Proposition The acyclic deduction algorithm complete acyclic rule sets cid0 general deduction algorithm complete general rule sets cid0 cid3 250 LG Valiant Artiﬁcial Intelligence 117 2000 231253 Proof It easy induction J general deduction algorithm J th scan valid deduction sequences length J followed In words valid deduction sequence terminates qJ cid25J bJ algorithm given determined value f0 1 01g node RHSqJ cid25J The main point evaluation process described Fact 31 monotone guarantees node evaluation graph determined undetermined later By similar arguments acyclic deduction algorithm seen complete In case shows induction J RJ evaluated algorithm valid deduction sequences terminating qJ righthand considered 2 344 Compound relations Lexpressions In deﬁning rules chose sake simplicity quantiﬁed 2 Clearly base expression ej depending single relation Rij set 0 relations consist combinations members base set For example want arity 4 choose base set 0 binary relations deﬁne consist suitable conjunctions pairs The following class combinations noteworthy applications desirable reduce number bindings tried learning deduction We deﬁne labelled expression Lexpression existentially quantiﬁed expression Rx1 xsR1x1R2x2 cid1 cid1 cid1 Rs xs R arbitrary compound expression R1 Rs unary The ambiguity scene cid27 Lexpression e s variables number distinct bindings cid25 fx1 xsg A conjunction R1x1R2x2 cid1 cid1 cid1 Rs xs satisﬁed cid27 Clearly expression low ambiguity ambiguity 1 high probability cid27 2 D substantial search bindings necessary recognizing presence This particularly relevant modelling biological neural processes previously discussed 45 It provides efﬁcient accurate deduction For example suppose expressions occurring LHS rule set S Lexpressions ambiguity probability D Then kSk expressions occurs regarded separate relation new value 1 binding multiplicative factor ncid11 saved cost evaluating expressions It follow individual rule allowed correspondingly accurate factor ncid11jSjkSk guarantee accuracy deduction Finally observe notation described deﬁning rules express certain relations higher arity maximum arity base relations We example object a7 represent set identify members set ai relation membera7 ai holds If binary relation adjacent xj xk express relation completelyconnectedxi simulate relation arity n cid0 1 follows LG Valiant Artiﬁcial Intelligence 117 2000 231253 8x1 2 A cid2 8x28x3 cid0 memberx1 x2 _ memberx1 x3 _ adjacentx2 x3 cid17 completelyconnectedx1 cid3 251 cid1 If relations member adjacent predeﬁned LHS rule member allowed set compound relations rule completelyconnected learned examples evaluated 4 Related work The framework described relationships work areas Relational rules evaluation central example databases artiﬁcial intelligence learning studied numerous settings Our logic distinguished close way integrates learning reasoning insistence having measures computational complexity accuracy bounded polynomial functions Among numerous alternative approaches relevant brieﬂy mention Probabilistic logics deﬁned extend predicate calculus probabilistic phenomena 6 Some treat underlying distribution world impose space beliefs 12 PACsemantics aspects The underlying distribution world degree belief rule justiﬁed time quantiﬁed example conﬁdence parameter cid14 Inductive Logic Programming concerned learning rules predicate calculus standard semantics additional constraint background logical theory The limits polynomial time learnability explored 82130 Some systems exist learning rules 34 A discussion relationships approaches 20 The Learning Reason framework 1819 integrates learning reasoning similar spirit approach Its basic form oriented maintaining information set examples answering query processing examples anew Robust logics based rules This allows accept rule input beneﬁt experience permitting evaluate ﬁnetune rule basis examples experience Clearly systems based robust logics additionally memorize individual examples enjoy added beneﬁts gained 5 Conclusion We described formal representing knowledge deﬁned semantics efﬁcient sound complete deduction procedure instances It efﬁcient learning procedure learning representation rules time appropriate class connectives efﬁciently learnable The class 252 LG Valiant Artiﬁcial Intelligence 117 2000 231253 linear threshold functions appears particular good choice offers addition attributeefﬁcient errorresilient learning Clearly directions attempt extensions For example restricted discussion reasoning deductions single instances It interesting said deduction new rules Also PACsemantics based probability theory methods probabilistic inference brought bear 32 There remain questions logic applied pragmatically building systems perform computations cognitive nature The discussion 47 addresses questions References 1 JR Anderson Rules Mind Erlbaum Hillsdale NJ 1993 2 D Angluin P Laird Learning noisy examples Machine Learning 2 4 1988 343370 3 Aristotle Prior Analytics Book II Part 23 4 A Blum A Frieze R Kannan S Vempala A polynomial time algorithm learning noisy linear threshold functions Proc 37th IEEE Annual Symposium Foundations Computer Science FOCS96 1996 pp 330338 5 T Bylander Learning linear threshold functions presence classiﬁcation noise Proc 7th ACM Conference Computational Learning Theory 1994 pp 340347 6 R Carnap Logical Foundations Probability University Chicago Press Chicago IL 1950 7 E Cohen Learning noisy perceptrons perceptron polynomial time Proc 38th IEEE Symposium Foundation Computer Science 1997 pp 514523 8 WW Cohen CD Page Polynomial learnability inductive logic programming Methods results New Generation Computing 13 314 1995 369409 9 A Ehrenfeucht D Haussler M Kearns L Valiant A general lower bound number examples needed learning Inform Comput 82 3 1989 247266 10 ML Ginsberg Readings Nonmonotonic Reasoning Morgan Kaufmann Los Altos CA 1989 11 AR Golding D Roth Applying Winnow contextsensitive spelling correction Proc 13th Internat Conference Machine Learning Bari Italy Morgan Kaufmann San Mateo CA 1996 pp 182190 12 JY Halpern An analysis ﬁrstorder logics probability Artiﬁcial Intelligence 46 1990 311350 13 D Haussler Quantifying inductive bias AI learning algorithms Valiants learning framework Artiﬁcial Intelligence 36 1988 177221 14 MJ Kearns Efﬁcient noisetolerant learning statistical queries Proc 25th ACM Symposium Theory Computing ACM Press New York 1993 pp 392401 15 M Kearns M Li Learning presence malicious errors SIAM J Comput 22 4 1993 807837 16 MJ Kearns UV Vazirani An Introduction Computational Learning Theory MIT Press Cambridge MA 1994 17 R Khardon Learning actions Proc AAAI96 Portland OR 1996 pp 787792 18 R Khardon D Roth Learning reason restricted view Proc 8th ACM Conference Computational Learning Theory 1995 pp 301310 19 R Khardon D Roth Learning reason J ACM 44 5 1997 697772 20 R Khardon D Roth LG Valiant Relational learning NLP linear threshold elements Proc IJCAI99 Stockholm Sweden Morgan Kaufmann San Mateo CA 1999 pp 911917 21 JU Kietz S Dzeroski Inductive logic programming learnability SIGART Bulletin 5 1 1994 22 32 22 J Kivinen MK Warmuth P Auer The Perceptron algorithm versus Winnow Linear versus logarithmic mistake bounds input variables relevant Proc 8th ACM Conference Computational Learning Theory 1995 pp 289296 Artiﬁcial Intelligence 97 1997 325343 23 D Knuth The Art Computer Programming Vol 3 Addison Wesley Reading MA 1973 LG Valiant Artiﬁcial Intelligence 117 2000 231253 253 24 P Langley D Klahr R Neches Production System Models Learning Development MIT Press Cambridge MA 1987 25 DB Lenat et al CYC Toward programs common sense Comm ACM 33 8 1990 3049 26 N Littlestone Learning quickly irrelevant attributes abound A new linearthreshold algorithm Machine Learning 2 1988 285318 27 N Littlestone From online batch learning Proc 2nd Workshop Computational Learning Theory 1989 pp 269284 28 J McCarthy Programs commonsense Proc Teddington Conference Mechanization Thought Processes London 1959 HMSO 29 M Minsky S Papert Perceptrons MIT Press Cambridge MA 1969 30 S Muggleton L De Raedt Inductive logic programming Theory methods J Logic Programming 19 1994 629679 31 A Newell HA Simon Human Problem Solving PrenticeHall Englewood Cliffs NJ 1972 32 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann Los Altos CA 1988 33 L Pitt LG Valiant Computational limits learning examples J ACM 35 1988 965984 34 JR Quinlan Learning logical deﬁnitions relations Machine Learning 5 1990 239266 35 RL Rivest Learning decision lists Machine Learning 2 3 1987 229246 36 F Rosenblatt Principles Neurodynamics Spartan New York 1962 37 D Roth Learning reason The nonmonotonic case Proc IJCAI95 Montreal Quebec 1995 pp 11781184 38 D Roth A connectionist framework reasoning Reasoning examples Proc AAAI96 Portland OR 1996 pp 12561261 39 S Russell P Norvig Artiﬁcial Intelligence PrenticeHall Upper Saddle River NJ 1995 40 D Schuurmans R Greiner Learning default concepts Proc 10th Canadian Conference Artiﬁcial Intelligence CSCSI96 Toronto Ont 1994 pp 99106 41 AM Turing Computing machinery intelligence Mind 59 1950 433460 Reprinted DC Ince Ed Collected Works AM Turing Mechanical Intelligence NorthHolland Amsterdam 1992 42 AM Turing Solvable unsolvable problems Science News 31 1954 723 Reprinted DC Ince Ed Collected Works AM Turing Mechanical Intelligence NorthHolland Amsterdam 1992 43 JD Ullman Principles Database KnowledgeBase Systems Computer Science Press 1989 44 LG Valiant A theory learnable Comm ACM 27 11 1984 11341142 45 LG Valiant Circuits Mind Oxford University Press Oxford 1994 46 LG Valiant Rationality Proc 8th Annual Conference Computational Learning Theory ACM Press New York 1995 pp 314 47 LG Valiant A neuroidal architecture cognitive computation Lecture Notes Computer Science Vol 1443 Springer Berlin 1998 pp 642669 J ACM appear 48 LG Valiant Projection learning Machine Learning 37 2 1999 115130