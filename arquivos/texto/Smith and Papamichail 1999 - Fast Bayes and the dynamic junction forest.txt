Artificial Intelligence 107 1999 99124 Artificial Intelligence Fast Bayes dynamic junction forest JQ Smith KN Papamichail bl Department Statistics University Warwick Coventry CV4 7AL UK b School Informatics University Manchester Manchestel Ml3 9PL UK Received 10 Aprit 1997 received revised form 11 June 1998 Abstract It shown propagating probabilities conditional independence examples processes conditional augmented junction tree algorithms provide quick efficient method complex multivariate problems described fixed In paper formalise illustrate practical high dimensional applied structure underlying distributions independence propagation algorithms structure probabilistic passage time 0 1999 Elsevier Science BV All rights reserved Keywords Dynamic models Hellinger metric Influence diagrams Multivariate state space models Junction trees Probabilistic expert systems 1 Introduction Bayesian problem established Over years relationships variables represented influence diagrams causal networkssee expert systems work quickly accurately wide range practical problems provided fixed These underlying relationships 2812 263 l later paper However problems relationships coded graph eroded independencies evolve example information variables learning environment believe change learning point evolution structure ignoredfor needs different exception Kjaerulff 111 Learning issue However structure dependencies passage time Indeed gathered Alternatively dynamic environment inevitableso fundamentally conditional address remained machine largely Corresponding Email nadiapapamichmanakuk author Email jgsmithwanvickacuk OOW3702I99 PII SOOO4370298001039 matter 0 1999 Elsevier Science BV All rights reserved IO0 JQ Smith K N Pupumdwil Artifktrl Intrllimw IO7 IYYY 99124 enable company static number key ways illustrated relationships following example Suppose company produces number different brands products large market knows competitors brands There marketing produce causal network vectors techniques available 19201 This causal network codes sales given short time interval sales different brands time dependency structure transformed probability distributions sales product given precise information collections static environment causal These quick algorithms understood network remains unchanged timefor outlined tree 191 update efficiently helpful introduction section Jensen 9and junction variables sales information induce dependencies problems Firstly algorithms network For example company companys database causal network Instead However dynamic environment encounter Jensen usually employed directly complete observe directly typically incomplete noisy data fully disaggregated individual brands better information types imperfection example acquire perfect brands sales S 1 S2 dependent market Having observed S knowing S1 tells precisely aggregate S induces dependence possibly junction information Another type dependence course different causal network associated junction possible aggregates dynamic environment processing It possible preprocessed inferential necessary product sales competitors Unfortunately The simplest sum S sales different sections value S2 So S1 S2 The causal network light piece 3 11 Of tree drawn include pre available adjust absorb new information format influence diagram tree need adjusted induced missing data discussed It highly desirable conducive framework derived information A second problem generic components dynamic environments underlying dependence In application possible change competing brand Alternatively reposition brand different set competitors diagram represents choose dependency example competitor reprise readvertise structure periodically introduce new In case influence like structure need change In Section 4 paper formalise sequence graphs called junction experiences developing forecasting product sales competitive market 2 1221 forecasting spread contamination 4629 The second application nuclear accident coded operational dynamic junction description dynamic terms based authors forests This formalism fields application different forest 4 algorithms In Section 5 specify algorithm automatically adjusting graph calculations objects cliques form nodes graph work observation This allows propagation algorithmriginally designed JQ Smith KN Papamichail ArtQicial intelligence 107 1999 99124 101 relates states single clique original valid generally Such adjustments reduce subsequent efficiency new junction forest usually smaller number cliques larger size forestto In Section 6 introduce transformations junction forest automatically compact joint probability distribution changes representation efficient form The method modifies number states contained clique relationships discarding techniques transformation approximation This section concludes illustration described variables longer incorporated giving example conjunction Hellinger metric density nuclear example existence cliques In Section 7 outline approximate Section 3 We begin paper short review state space representations problems junction tree propagation algorithm 910 2 From dynamic influence diagrams junction graphs For simplicity slight loss generality shall assume dynamic time state space models equation time One standard class discrete terms observation 734 specified discrete defined example equation respectively Y Ft0 ut 8 Ge1 cot An illustration model given Fig 1 The error distribution ztot observation Yt time t 12 nearly chosen t satisfies conditional independent statement qJez 1 I 21 22 mutually independence 23 observation state vector time time t gives direct information values components I 1 t2 LI f41 let 24 states Markov timewe vector determine probability future need retain beliefs current state statements wish The random vectors ot 12 called state random vectors In dynamic systems state vector important object given value disregard Fig 1 A discrete time state space model 102 JQ Smith KN Pupamichail Artificial Intelligence 107 1999 99124 irrelevant predicting future observations Y In sense information In simple information need forecasting state vector contains relevant marketing example state random vectors vectors sales brand time t typically known precisely company The matrix G specifies time The simplest evolution G sales expected identity matrix stipulate t expected time period t 1 The error vector Wt captures likely random drift relationship Each vector Y t 12 set t read rows possibly component Y matrix F aggregate brands associated observational There usually measurement vector ut represents error associated observations incomplete data received sales brand time period interval develop A useful way coding conditional XI x causal independence network graph influence diagram set random vectors 9 Given collection conditional statements form independence statements 25 Xk I union n1 X2 directed acyclic graph nodes XI fi2k nk disjoint subsets XI x2 Xl x directed causal network edge x xk 1 6 k n Xi lies set nkThe elements flk called parents Xk A directed acyclic graph valid causal network statements Eq 25 true It easy conditional valid causal network nodes HI 01 22 form Fig 2 Deductive diagrams studiedsee HI associated equation rules causal networksinfluence I 3141724 independence In context example order develop networks important vector given time To illustrate allow nodes represent powerful methods learning causal individual components state return marketing example possible In time period 1 brands market A B C D E F G H Suppose asset sales collections expert judgment brands A B C D E F G H independent given sales learnt A B predictions sales brand C depend prediction brand sales D depend learnt sales A cited causal network C It straightforward Fig 2 judgments Now use equation extend structure add nodes representing dependency causal network sales future times For sake depict Fig 2 A causal network brands sales m time period JQ Smith KN Papamichail Artial Intelligence 107 1999 99124 103 ___________ y ____________________1 I I I Time Period 1 I 2 I 3 I 4 Fig 3 A causal network components states associated levels sales time periods data collected errors components independent t 12 components structure market valid second time period simplicity shall assume diagonal matrix G zeros diagonals error terms Given fourth rows independent competitive easy check valid causal network relating brand sale time periods time period 3 given corresponding E F G H assumed compete They predicted dependent represented subgraph associated period Within time period 4 brand H withdrawn gives rise adjusted pattern period The causal network process given Fig 3 advertising companies induced brands subgraph graph Fig 3 During There issues highlighted example First small problems increases number variables time necessary probability like I 04 JQ Smith KN Pupumichuil Artciul Intelligence 107 1999 99124 involved However Markov manipulations future situation structure need predictions current retrieved This storing probability distributions date information need states given data collected current data past enable estimate subset systems variables valid predictions remember causal network associated probability distribution A description achieved given later paper future Thus context sufficient distribution calculate possible probability check directly dseparation sales second period given information Let return example For time period 1 causal network Fig 2 valid By observation equations future events Now distribution variables suppose observe sales figures A B C D E F G H independent et al 3 l later paper errors Using known results Spiegelhalter variables causal easy obtain new distribution theorem network remains valid It possible follows 19171 time distribution equations able network Again observation network The predict probabilities data second period consists observations independent measurement errors A B C D E F G H It shown observe C D introduced separately obtain noisy estimate sum dependence C D dependence needs carried forward time period 3 This Section 5 paper The dependence E special case results discussed carried F G H induced advertising activity error period 3 A B marginal network The data independent measurement C D E F G product H having withdrawn A valid network variables time 4 shown given Fig 4 future events distribution Because Markov assumption future sales example depends data affected distribution current states past ones current states Past states distribution It follows important probability propagation algorithms significantly quicker need retain probabilities retaining In paper investigate efficiently code joint distribution subvector 0 particular application sufficient predictive vector 6 In environment r ti 02 requirements model In example example given Section 3 illustrate complicated More pIWiSely T high dimensional vector b vector length changes time index In practice effects observed causes causal network makes round raw causal network unsuitable framework It helpful define framework absorption information non directional When causal network dynamic probability propagation network follows common framework derived propagating new information causal tree I 91013 junction JQ Smith KN Papamichail Artijicial Intelligence 107 1999 99124 105 0 A A A B A A B D D C C E E F 8 G H 8 Time Period F 8 G H 8 2 C D A A B G E F 3 4 Fig 4 Margins states distinction WY chosen preserve shape causal network given data collected strictly time period brand sales aggregates given text plus independent errors index The data collected assumed measurements time unconnected cycles set parents parents node created producing First new directed edges joining ensure repeated causal new causal network This network node causal network process In causal network joined edge The resulting graph called decomposable B2 A3 Fig 3 directed edges need sets 5i B3 A4 nodes connected Ei Fi Gi Hi Ei 11 Fi 11 Gi 11 Hi l set 12 Z3 E3 F3 G3 H3 1941 F4 G4 Note edges need added On hand causal networks Fig 4 predictive dynamic probability need propagation parents connected modification added Bl A2 The cliques decomposable graph defined maximally connected subsets causal network Fig 2 5 cliques A B I C B D 15 F A B B C II E F G H The cliques causal network ordered mat intersection property 14321 2 m index For example G H In causal network ci 1 m decomposable satisfy running ri 1 ri il di ci f U cj di c ci 1 ci jl 106 JQ Smith KN Puprrmichail Artcial Intelligence 107 1999 99124 forest causal network nodes Henceforth shall label vector variables contained clique ci iI cliques 1 rn A junction network numbered consistently running intersection property clique c j connected undirected edge clique ci j j ri di ri di defined A junction tree connected subgraph junction forest Here di separator cliques ci cri denote 1 iJ vector variables contains It straightforward check iclcnldi 2i rn 26 ql denote marginal densities 41 respectively Let pii di 1 qi 1 1 m Then conditional independence relations given shown joint density p8 original variables written p marginal density subvector 1 ci q marginal density components 41 di defined equal 1 di Note causal networks undirected version rise breakdown given sense decomposition variables dependent causal order 27 literature principle second Gaussian In current propagation driving algorithms computational The fact junction tree structure setting family dependent causal quick usually assumed update usually light 933 Here distribution ordering makes ideal framework absorption information subset states observed error The algorithms discrete densities remaining states states typically slightly different setting reasons observe function states error The continuous algorithms equally difference causes problems involved valid continuous drift outside longer invalid significant particular case errors independent However To function states single clique straightforward create new clique ci observation yi contains states function Since c j contains observable observation states single clique c j conditional yi equation tree J J new junction 9 J instead J valid The conventional propagation apply directly Once propagation completed connecting edges J J removed loss contain unknown quantities contained detailed discussion procedure 5 tells connect cj undirected edge cj original 131 The second difference algorithm described tree J edges states follows cliques observations observation independence learning seenfor cliques algorithms implicit issues JQ Smith KN Papamichail Artificial 0 EF GH 1 0 Time Period 0 EF GH 0 2 Intelligence 107 1999 99124 107 AB x BCD AB x BCD 0 EFG 4 WSW 0 3 Fig 5 Marginal junction forests associated causal networks Fig 4 algorithm Henceforth assume accommodate example information sequence valid junction employed marketing networks tree adapted observation C D states lie clique Formal procedures tree type given body paper In causal time period 3 upper junction like described given making efficient adjustments forests corresponding Fig 4 given Fig 5 Notice time period collected called absorb Jensen tree J derived original extremely simple Thus consider In examples states observation Gaussian In case light observation Y updating marginal densities cliques subsequent adaptation tree J The joint density adaptation junction 16 appropriate equations states cj given Y Gaussian J The distribution cliques These states form separator cj junction algorithm following tree containing sequentially Once distribution cliques updated clique cj tree J discussed removed forest remain The distribution variables unchanged cliques trees junction cj updated loss original cj junction ci 41 clique let pj E4ji Let vector random variables Y j 12 Cl2 accommodated vector newly updated separator Then 7 Covrti Eq 1 Y updated mean separator _ZF Var Y updated covariance matrix read directly adjacent updated clique Since distribution 1 41 remains fixed updating usual multivariate normal JTjj Varji 10X JQ Smith KN Pupmichail Artijiciul Intelligence 107 1999 99124 theory 161 gives Y E42i 1 Y CLT2 Vari I Y given equations CT2 Cov il Y 42Ap C AC c2 X22 A CA 1291 Finally new cliques need calculate use distribution ci These subvectors ci So Gaussian case new mean vector continuous pass information X21 Cfi These results discussed A equations new separators contained step trivial need pick correct components nonGaussian covariance matrix Interestingly appropriate properties clique marginsas hyper Wishart families consuming discrete case analogous operation updating step analogous multiplying example 3this marginalising exists hyper Dirichlet step time calculation integral Of course problem exists simply summation On hand algebra given simplebeing functions casesunless involving 3 States efficient representation dynamic graphs structure probabilistic There usually representations terms graph critical graphical representation guides probability propagation time point efficiently possible Which graphical representation predictive computationally capability technological method currently available In paper like ones cited assume section process data determined considerations type timing quantity relevant updating use junction process information tree algorithm information including required efficient To achieve efficiency necessary define appropriate joint distribution random vector time statements t contains representation sequence random vectors indexed transparent time represented junction information tree distribution calculate required probability required future predictions time Thus need tree stores distribution state vector time t A sequence state vectors indexed time b A junction c Algorithms adjust distributionie associated distributions clique marginsin point t One algorithm described d An algorithm takes cliques junction adjust cliques junction tree light incoming data time previous section clique margins state vector time t 1 transforms cliques junction t123 class tree new associated distributions clique margins time t This process illustrated tree associated distributions example given JQ Smith KN Papamichail Artijicial Intelligence 107 1999 99124 109 Sometimes exact algorithms rise distributions states joint distributions compact graphical representation represented sparse graph associated cliques small number states In case expedient substitute slow exact ones Before formalise procedure example time environmental model relevant prediction trees evolve time structured way fast approximate algorithms junction Example 31 Pentificating Puffs steady release radioactivity fragment spread gaseous waste accident algorithms mass dangerous 28 29 details Briefly nuclear emitted building atmosphere wind field transported larger mass contains spread Once puff grows dispersion model sectors large puff This process predicting junction tree propagation accident puffs containing Each puff transported certain size time determined atmospheric conditions allows transported fragments eventually occur Each fragment deterministic associated release Readings Yt s air concentration taken periodically Physics mass fragments exist time t puff site slarger contributing error determined complicated covariates time t site s puffs function known environmental pieces different mass reading tells concentrations wind field depending independent linear combination sector lies Fragmentation formula observational Markov This process defined physicists future future mass joint distribution observationscan expressed function joint distribution mass fragments exist time T It follows valid state space time T sufficient provide distribution observations states time vector mass fragmentspuffs time T distribution fragmentsand sense existing time T large T typically extremely Unfortunately long vector However linear puff masses fragments parent puff joint 29 provided puffs released Markov process observations combinations distribution vector stored junction forest cliques masses puffs adjacent time emitted time T random vector length 2 b masses parent puff associated fragments 5 random vector length 6 Cliques linked edge junction forest component parent fragment common random vector This component efficient way puff emitted source Because cliques small dimension states An example tree given Fig 6 If storing joint distributions emissions independent state vector forest instead single tree forest tree puff emitted time T source puffs form need included 110 JQ Smith KN Pupmichail Artificial Intelligence 107 1999 99124 Fig 6 A Puff model junction tree c6 In Fig 6 simplicity puffs depicted splitting adjacent pairs Other cliques cliques associated 7 emissionscontaining order appeared Cliques c7 c8 c9 parents numbered fragments previously undivided puffs Cliques c 10 c1 l c 12 13 parents fragments fragments 2 5 cl information Now suppose time T observation linear combination error puff fragments components single clique vector given Then method accommodating Section 2 Gaussian network outlined tree state vector remain unchanged joint valid cliques junction distributions posterior observation Y linear combination components observing Y dependencies forest junction induced In case forest need modified data accommodated In Section 5 way cliques updated described However implicitly denied original vectors important In application note need determine source emissions Therefore subset states source masses transmit tree happened source predict future local contaminationa future mass fragments process essential mass fragments terminal information automatically cliques determining distributionsthe forest The states efficiently speeded simplifying removing completely transmitter forest In later sections discuss junction linear combination 4 A formalisation A state vector r time T random vector distribution time T given time T sufficient data D received including distributional information known priori determine The expected utility optimal policies utility function called time T JQ Smith KN Papamichail Artificial Intelligence 107 1999 99124 111 b The expected utility optimal policies utility function U called time T T function covariates values certain known time T1 We saw previous section representations appropriateness answered systemie example dynamic state vector needs r 6 time T sufficient allowed better computational state vector depended heavily questions needing state vector uniquely defined We saw utilities policies chosen For efficiency linear model Section 2 order estimate 13 t 123 t Or On hand store past distributions I wayeg asked involved let state vector time T 0 questions setting future A state vector T necessarily minimally sufficient determining b components included utilities quick propagation useful later write T 1 PT 2 4 lcalled minimal set components 4 satisfy requirements rest T2 called transmission L S algorithm Spiegelhalter Lauritzen states expected facilitate 14 It b essential statesare A state process t 1 t makes time sequence state vectors Note model sets states r3 0 Y t 23 Yt random variables observed t 12 _ state process time interval representation usually inefficient Each state vector time T state process tree associated triple TT CT ST consisting respectively junction edges set cliques labeling nodes tree set separators tree It set marginal probability distributions tree T Cl St P t 123 set PT pi ci E CT Henceforthcall tree state JT process associated state process time T iS Simply Tr CT PT T 1 2 3 A transmitter clique clique called essential clique components Pl lie parameterised storage usually terms values parameters For example case Gaussian process pi E PI stored terms mean vectors pi covariance matrices Zi labeling cliquesie ajunction When densities states A junction entirely t 123 t 123 transmitter family convenient specifies interval corresponding tree equation transformations 1 Transformations Because Markov structure state process time point t 123 specify junction composite different junction t 1 t Usually tree state Jt function Jt_l observations equation t time types In Example 31 state equations kind described use types transformation emitted illustration When new puff mass Qr Cr CT U c The time T forms new clique c QT QT1 CTI Set previous puff mass QT_I tree TT adding ST Sr_r U Qr_1 The tree TT formed junction node labeled c edge joining c c labeled QTr _ The set densities PT adds density clique c PT The clique margin c calculated multiplying lies clique c QT1 Qr2 clique margin cby density Qr_lcalculated chimney descriptive I I2 JQ Smith KN Prrpamichail Artificial Intelligence 107 1999 99124 conditional transformation transmission completely states time t Indeed redundant density QT Qr_1 states essential time known priori After Section 6 illustrate t 1 2 Transformations induced arrival observations In Section 2 cited tree process light observation method updating junction function states lying single clique This transformation particularly simple new set densities PT light observation Again states TT _ 1 redundant time Tfor surrogate data observed 3 Transformations redefine junction set TT CT ST TT1 CT _I changed PT_ example states acted feasible Exact versions discussed tree state compact Section 6 quick algorithms approximate methods described Section 7 Because transformations paper concentrate attention second Examples type discussed type 1 tend specific application Gargoum Smith 6 Gargoum 5 5 Transforming junction trees assimilate data c 11 c2 In Sections 1 2 gave examples observation function In section algorithm forest structure algorithms Such forest states different cliques probability propagation longer applicable modifies legitimately contain clique variables tree state new clique junction tree propagation D defined cr D usual junction use usual junction tree algorithms junction Let hb c T denote path nodes b c C T C S P hb c T repeated nodes Note hb c T exists unique forest period 2 Fig 5 b c lie tree forest In junction observe aggregate sales C D error note junction forest clique contains brands C D However path cliques b B C c B D repeatsnamely edge joining h c c r 1 C denote Let Tcllc21 crl LII CM paths hcicj smallest T 1 j r Let Tjclc2 trees comprise subforest cr containing 1 6 j 6 k briefly Tj denote forest k disconnected Tcl c2 cr Bj denote nodes Tj 1 j k Let Dj D n Bj Dj disjoint 1 j k union D We partition Bj Ci j Ci j b E Bj minpj hhb C Tj 1 lj hh number nodes b c path h lj maximum distance node Bj Dj Let Ci _il iJciCi j ci 1 6 lj 1 6 j k let Cj Ci j 1 fj c u Cj Ijk JQ Smith KN Papamichail Artijcial Intelligence 107 1999 99124 113 Let T forest nodes C C D U C U d d Uicl ci ii iii iv Two nodes b c C D connected edge T iff connected edge T A node b E C D connected b connected T The node ci j f C connected ori 1 The node d connected ljk node b E C edge iff b c 1 j j node ci j E C iff Ci j contains node node ci j E C iff j j 1 It straightforward check T forest Since cliques C separators S T defined remains distributions P define clique marginal probability Each clique C D given margin C C The margin ci j joint density states Tj T C S P taking product margins cO j construction appropriately calculated marginalising Finally clique d margin obtainable 1 j k calculated It straightforward junction figure forest T time period 2 Fig 5 gives junction check performing forest T period 3 Now following theorem Theorem 51 If junction tree state T C S P valid junction tree state T C S P dejined Proof This direct consequence dix A q Pearls 17 dseparation Theoremsee Appen Having transformed T T note observed variables clique d It valid use updating algorithm described new marginal probabilities states P lie Section 2 recalculate Note joint density pj r variables lying cliques B j given formula 27 pi 4 margin ci qiij 4 density separator si iassociated edge Tj linking ci ci Z j set separators Tj 51 In special case variables mean cliques covariance matrix variables easily calculated The mean vector read directly means densities pi 4 The covariance matrix obtained numbering graph undirected version Tj cliques numbered compatibly valid Now covariance matrix constructed regularly junction inductively numbering Bj jointly Gaussian cliques Bj Thus let 1 denote variables covariance matrix calculated single clique let 4 2 l2 4 s vector variables variables 114 JQ Smith KN Pupumichail Artificial Intelligence 107 IYYY 99124 clique adjacent 1 1 l 4s 4s r 1 G2 1 j residual j given Cj T having joint covariance matrix calculated j 12 If covariance matrix 4j random vector variables common WI c Clljl hs hjs Cs 1 Cl j covariance matrix 41 j Cs covariance matrix 4s covariance matrix 1 41 I l2 l3 given Cilll h12 IIS cl rT12 Cl2 h2s qw qs CS 1 Aj hj sE theory 16 This completes A I 2 A 1 CsAT s j 12 standard multivariate normal step replace 4l 4 1 repeat procedure covariance matrix It trivial matter Bj calculated variables read mean covariance matrices ci j mean covariance matrix vector calculated aboveand calculate new densities cliques contained inductive lying P In nonGaussian case calculation density ci j need integration summation slow process dramatically However loss efficiency replace T different junction tree having nodes replace C c Substituting tion c C constructions iiv gives valid junction tree representa Finally note use 51 algorithm described clique marginsbut work quickly seen generalisation Section 2 margins qiil4 retrieved updating closed form algorithm defined section requires conditions integration requires given Section 2 We finish section example Example 51 Deriving new junction tree probability propagation components You observation U distribution depends explicitly state vector lie cliques D 131 c7 c12 c15 lying c23 c24 To draw forest valid components trees T1 T2 defined union single clique d construct paths D T tree T given Fig 7 These subtrees given Fig 8 Using Eq 51 joint densities pj states Tj j 12 given cliques JQ Smith KN Papamichail ArtiJicial Intelligence 107 1999 99124 115 1 Fig 7 A junction observed c24 forest function variables set cliques D c3 c7 clZ c15 231 cW c16 c18c19c20 CL11 d41 d91 4121 Z a1 X gg CI ci 171 131 0 0 Tl c131 x 1141 X Z A 231 0 m41 23 151 0 subforest tree markingie Tl i21 Dl CD c7 121 c15 02 c23 c24 We set distance Fig 8 The constructed combine cliques observed nodes P2 P21 P23 P24 q2123q2124 From joint densities clique margins newly created cliques cO 11 cl 11 c2 11 cO 21 cl 21 Fig 10 The algorithm reconstructs tree depicted possible calculate Fig 9 Interestingly easy check c2 I cl 21 calculated avoiding step Thus clique margin c 12 clique margin c21 integration clique margin cO 21 given P21 P24 q2123q2124 116 JQ Smith KN Papamichail Artijiciul Intelligence 107 1999 99124 c 18c190 Fig 9 The reconstruction T 7 completed Here d cO 11 U cjO21 m21 c121 oo TI r ready construction 7 LO I c3 u C7 u c12 u Cl5 Fig 10 The transformation cl I c21Uc6 Uc13U141 12 I clUc41 cfO21 c23uc24 C12 c21 The density separator cO 21 c 121 union separators c21 c23 c21 c24 original junction tree given q2123q2124 r21 I lying separators equal r2t density variables set However Bl states tree Gaussian necessary place nodes single clique It experience nuclear coding fact construction smallindeed cliques different combines cliques trees parameter settings combine cliques parent trees forest 6 Other exact algorithms 6 I Splitting trees Section 2 tends It noted forest T split trees propagation work quickly The Chop algorithm algorithm described described time T state J T C S P forest trees It applied separator s E S degeneratefor example observed directly error If separator 512 E S cliques ct cz E C degenerate tree state J T C S P given transforms junction c2 independent The following valid transformation clearly et T C S P JQ Smith KN Papamichd Artijicial Intelligence 107 1999 99124 117 The Chop transformation sets S SI2 c CkL c21 u CT CT cr components T forest tree Tt2 E T containing containing 2 new clique edge set C S respectivelyso components ci components 12 12 cl 12 replaced treesone containing inevitably As time passes number cliques multiplies states process defined carefully This extent automatically The transmitter cliques It fact version Shachters transformation algorithm 23 Barren node reduction junction trees dynamic setting diagrams stated loses irrelevant decision influence takes junction The Chop transformation tree states J T C S P state J T C S P c E C C iff c essential clique c labels node T essential cliques T Separator essential cliques T C S P s E S S iff s edge lying path The forest T subforest T nodes C edges S C S defined For c E C C set pc pc pc E P lies path T Theorem 61 If J junction Proof See Appendix B q tree state process time T J defined If c E C margin given margin subvector c C containing margin conditional density ci given 12 1 2 12 degenerate distribution 30 use property discrete problems calling Implicitly Spiegelhalter global continuous property Queen Smith 21 use implicitly Lauritzen independence time series P If c E C C itwhich 62 Keeping transmitter cliques small It seen Section 5 adjusting data tendency information efficiency A junction small possible replacing transformation Thin takes junction state J T C S P follows tree transformation transmitter junction increase tree states allow accommodation loss clique size consequent Thin keeps size transmitter cliques clique union separators Thus tree tree state J T C S P junction C consists essential cliques C replaces transmitter clique c E C c c subvector c consisting components c contained separatorsorresponding corresponding c E CC node T c Set T T S S If c E C A C density unchanged density c appropriate margin pc c E C contains edges T connecting 118 JQ Smith KN Papamichail Artificial Intelligence 107 1999 99124 clique C As process evolves c subvector Notice Gaussian case set new clique margins trivial calculate given sub mean vector appropriate sub covariance matrix containing happen expediently lie path junction J T C S P let ncj E C called neighbours cj connected T let si j denote dimension vector states si j cliques removed essential cliques In C set cliques T edge Let si j E S edge connects ci cj transmitter tree cj 63 Combining transmitter cliques The final transformation section called Contract It acts mitter cliques cj T C S P cj E C adjacent T It takes C Ccjl ctjl U cj jl S Ssj jl U sj jl trans T C S P c j c j s j j denotes set edges S adjacent T sj j set edges nodes C C adjacent cj c j C connecting node new node cj j Let T graph nodes edge set respectively C S defined Let density c E C density c P Define density cj j c cj j P identical 13jPjr qjj pj pjl P densities cj c j respectively qj density sj j separator mean covariance matrix obtained formula 51 T Note variables cj j Gaussian efficient The Contract transformation works Thin shorten length paths essential cliques computationally efficiency obviously depends critically algorithms machine computing But larger cliques trees edges lead loss efficiency There candidates determine inevitably cliques One Contract simple criterion sample applicable spaceas transformation state vector components Gaussian caseis enacted Computational Contract iff sli jl C 4ilEncljl C lilEncjl si jl 2sj jl 6 max C si jl C si jl I clilencjl cLiJEncjl 1 This criterion uses Contract conservatively employed combining subsequent use Thin new clique bigger cliques cj cj JQ Smith KN Papamichail Artificial Intelligence 107 1999 99124 119 When separators dimension exactly edges T employed iff cJ cj 7 Transformations approximate quick forest simplify distribution Even exact approximation techniques forest gradually computational junction If inefficient necessary dynamic algorithms forest ones process information quickly times imperative approximate Obviously good approximation passage time rise distribution likely arise The inadequate There ways investigate problem illustrated efficacy approximation typical data streams exact calculation The second introduce metric distributions essential states given way analyticwe time use judge approximation A good metric use purpose Hellinger metric given invoked great care states look superficially future states later section checks essential p p 1 p PJ2T p p respectively topologically separation measure equivalent density approximating variation metric An alternative density This KullbachLeibler 5 junction The important point discussed Gargoum measures separation mentioned notice easy check p p densities local Thus example variables J differs clique c p p p p pC p marginal densities c density p p respectively Also margin variables p p apart dHpc pi Smith 27 proof results Thus particular margin pC c E C approximated p process left unaltered guarantee tree process pC p respectively induced substitution rise small changes true approximating It follows small approximations distributions states densities essential states clique margins A legitimate states Happily concern repeated use approximations problems truly significant true approximate densities expected time aggregate errors essential stochasticie distances distributions time So practice provided observations decrease geometrically predictive Smith 27 details Example 31 Here discuss easy generalise The Cut aggregation Gargoum Smith 6 Gargoum 5 discuss approximations tends problemsee consistent error state equations 120 JQ Smith KN Papamichuil Artificial Intelligence 107 1999 99124 transformation cliques ci cj E C follows T C S P T C S P deletes edge si j forest T set S Ssi j C Ckla u ljl jjcj2jandTisdefinedbysettingC wherecljljzljsi S defined node edge set If c E C c cj Finally density cj pc pc E P simply margin 2 j obtained density c j density pj cj product qjrj qj density 41 jJ rj density j The Hellinger distance approximation small t j j independent transformation replace Technically In Gaussian case rl 1 det l421 K Kp K I At I A2 1 I identity maps appropriate dimensions A 1 regression matrix AZ regression matrix Cpz j Cpl j So clearly 41 j j LH close zero A 1 A2 close zeroie learning 1 j expected hardly change mean vector j vice versa Note distribution essential state use Cut transformation conservativeit prohibits ignores certain learning 41 j j estimation processes information In particular predictionssomething avoided expected approximate inflate uncertainty learning systemssee expected j I 1 jit acknowledges Smith 25 Fig 11 gives junction forests obtained Cut transformation Forest 2 approximation forest originally single tree Forest 1 approximation approximate smaller Hellinger distance Tree 2 Forest 1 split degenerate cliquesTree whilst clique c5 disappeared completely Lop transformation transmitter clique separated essential cliques action Cut The forests modeling context process run long time cliques size trees appropriate settings Hellinger distance Cut typically size demands fewer trees Thus 5 Tree 7 Forest 2 spread nuclear contamination In simulation purposely let observed levels contamination larger forecasts model mathematical processes necessarily levels contamination prompt action approximate 03 forecasts associated true modelan far determining remarkably remain close original However forecasts forecasts deviated insignificant deviation runs different parameter settings model action concerned So methodology reason approximating robust In particular appropriate JQ Smith KN Papamichail Artijicial Intelligence 107 1999 99124 121 Forest 1 Tree1 s7 E Tree2 Tree 3 Tree 6 Tree7 ______________ Forest 2 sS15 Tree 1 s421 I Tree 3 Tree 4 TIWS Tree 2 Tree 6 Tree 7 Tree 8 Fig 11 Two forests cut administered different levels Hellinger distance approximation outlying data observed error forecast contamination appear escalate time 8 Conclusions The techniques dynamic probabilistic paper aid fast computation The automated husbandry belief structures variables fed user data particular useful regard learn The graphical structures They dependencies created destroyed passage independencies time learning type described The use structural approximation like Cut particularly mirrors human model choosing little chance affecting beliefs significantly learning For practice limit disregard data observed believe interesting scope statistical assuming variables date strongly independent evidence Running appropriate diagnostic prediction model albeit complex null hypothesisas Bayesian modelingis belief underpins inferential decision support dynamic systems providing stimuli problem solving space treating Bayesian current practical structure allows creativity Certainly creative entirely consistent taking approximations application implicit suggests Acknowledgements We grateful colleagues RODOS project discussions This work funded CEC Contract F14PCT9S0007 The views expressed RODOS project paper authors necessarily reflect EPSRC Grant GRK72254 Appendix A Proof Theorem 51 Theorem 51 f junctiorl state T C S P defined tree stde 7 C S P valid junction tree Srctim 5 independence Proof Since T forest running conditional graph It compatible ordering nodes introduces nodes B2 C nodes Bk subsequently T equivalently I statements intersection coded junction nodes Bl remaining nodes property RIP 321 ensures Since construction T forest RIP tells ci statements nodes C I nodes Ck 1 remaining nodes C coded junction graph 12 introduces equivalently nodes C2 C order I By dseparation Theorem diagramsinfluence 12 valid iff CI statements II There statements The IS 17181 junction graphs It 12 causal diagrams identical shared nodes CI statements Ck deduced C 11 implicit implicit 12 C I 1 different trees forest T The second class statements concern n But CI statements j 1 Cj cliques Bj independent sets variables variables sets This dseparation Theorem So result proved cliques adjacent I _ Ck 1 contain variates cliques lie dependencies variables Cj implied statement values given direct trivial consequence JQ Smith KN Papamichail ArtiJicial Intelligence 107 1999 99124 123 Appendix B Proof Theorem 61 Theorem 61 If junction tree I T C S P dejined Section 6 tree state J T C S P valid junction 1 j containing Tj running Tl T2 T3 Tk k k Tj trees T 11 T2 T3 T k T subtrees label nodes tree intersection property starting nodes Proof First check T k disconnected consists k disconnectedtrees subtreeof Tj 1 j k 1 j k trees Tj tree Tj Tj subtree T j way compatible directed tree undirected version node exactly parent root node 1 j k labeling way compatible Continue directed node exactly parent Tj root node 1 j k tree Tj label nodes tree undirected version j k It possible Tji Tj The junction graph G disconnected T 11 Tk compatible order valid Since nodes corresponding follows dseparation Theorem undirected version clique margins versions directed subforest T 11 Tj comprise ancestor set G graph G valid Since C contains states process C agree C construction trees undirected result follows 17 Gthe subjunction Tis directed q References I RG Cowell BAIESA JM JO Berger AP Dawid AFM Smith Eds Bayesian Statistics 4 Clarendon Press Oxford probabilistic expert shell qualitative quantitative learning Bemardo 1992 pp 595600 2 AI Dawid Applications Comput 2 1992 2536 general propagation algorithm probabilistic expert systems Statist 3 AP Dawid SL Lauritzen Hyper Markov laws statistical analysis decomposable graphical models Ann Statist 21 3 1993 12721317 4 S French DC Ranyard JQ Smith Uncertainty RODOS Research Report 9510 School Computer Studies University Leeds UK 1995 5 AS Gargoum Issues Bayesian Department University Warwick UK 1998 forecasting dispersal nuclear accident PhD Thesis Statistics 6 AS Gargoum JQ Smith Approximating dynamic Gaussian junction trees Research Report 279 Statistics Department University Warwick UK 1994 forecasting 7 PJ Harrison CF Stevens Bayesian 205247 discussion J Roy Statist Sot Ser B 38 1976 8 F Jensen FV Jensen SL Dittmer From influence diagrams junction trees Proceedings 10th Conference Uncertainty Artificial Intelligence San Francisco CA 1994 pp 367373 9 FV Jensen An Introduction Bayesian Networks UCL Press London 1996 IO FV Jensen KG Olesen SK Andersen An algebra Bayesian belief universes knowledgebased systems Networks 20 1990 637659 I l U Kjazrulff A computational scheme reasoning Artificial dynamic probabilistic networks Intelligence Stanford CA 1992 pp 121129 Proceedings 8th probabilities means variances mixed graphical association models Conference Uncertainty 121 SL Lamen Propagation J Amer Statist Assoc 87 1992 10981108 131 SI LaurnLen GraphIcal Mud Oxtord IIl SL Lauritzen DJ Spiegelhalter Local computations probabilities graphtcal structures Iluern Pm Oxford 1996 application expeli systems discussion J Roy Statist Sot Ser B SO 1988 157224 151 SI Lauritzen AP Dawid BN Larsen HG Leimer Independence properties directed Markov tields Networks 20 1990 49 I SOS Ihj KV Mardia JT Kent JM Bibby Muluvariate Analyr Academic Pre London 197Y I71 J Pearl Probabdistic 181 J Pearl TS Verma The logic representing dependencies hy directed graphs Proceedings 6th National Intelligent Systems Morgan Kaufmann San Mateo CA 1988 Inference Conference ArtitiGial Intelligence AAAIX7 Seattle WA 1987 pp 37437 IY CM Queen Using multiregreGon Statistician 43 I 1994 X79X dynamic model forecast salch m competitive market The 120 CM Queen Model elicitation m compctitie market 111 S French JQ Smith Eda The Practice Bayesian Analysis Arnold London 1097 pp 229243 121 I CM Queen JQ Smith Multiregression dynamic models J Roy Statist Sot Ser B 55 4 1993 X49 X70 1721 CM Queen JQ Smith DM Jame BaycGan lorccasts In markets overlapping structures Internat J Forecasting 10 lYY4 209237 1231 RD Shachter Evaluating mtlucncc diagram m A P Bau Ed Reliability Quality Control Elsevier NorthHolland Amsterdam IYXh pp 32 I344 1241 JQ Smith Influence diagrams statitictl modellinp Ann Statist 17 1980 654672 1251 JQ Smith Discussion ofLcarnmg probabilitlc systems J Spiegelhalter RG Coweli JM Bernardo JO Berger AP Dawid AFM Smith Ed Bayesian Statistics 4 Clarendon Press Oxford 1992 pp 460363 1261 JQ Smith Handlmg multiple source 01 variation um intluence diagram European J Oper Res X6 I995 1 I x9200 1171 JQ Smith BayeGan Appnxmaton Hcllmgcr metric J Roy Statlt Sot Ser B appear 12x1 JQ Smith S French Bnyesian updating fatmophcric dtspcrsion models ue accidental release radioactivity The Statistician 42 5 C IOO 5Ol5 I 1201 JQ Smith S French DC Ranyird An cfticicm graphtcal algorithm gaseous waste aficr accidental Belief Network Alfred Walker releac m A Gammerman IYYS pp 125140 updating ctimatcs dispersal Ed Probabilistic Reasoning Bayesian 30 DJ Spiegelhalter SL Lauritzcn Sequential updating conditional probabilities trn dlrected graphical structures Network 70 1990 57Y605 3 I I DJ Spiegelhalter AP Dawid SL Science X 1993 2 I Y246 Iauntzen Ri Covcll Bayesian analysis 111 expert systems Statistical 1321 RE Tttrjan M Yannakakis Simple lmcar time alpornhm IO tet chordality graphs test acyclicity ot hypergraphs selectively reduce acyclic hypegraph SIAM J Comput I3 19X4 566S7Y 1331 A Thomas DJ Spiegelhalter WR Gilks BUGS A program perform Bayesian inference Gibb ampling JM Bernardo JO Berger AP Dawid AFM Smith Eds Bayesian Statictick 4 Clarendon Pres Oxford 1992 pp 837X42 1341 M Weht PJ Harrison Bayesian Forccnting Dynamic Models Springer New York 1906