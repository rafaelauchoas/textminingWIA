Artiﬁcial Intelligence 175 2011 18561875 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint MonteCarlo tree search rapid action value estimation Go Sylvain Gelly a1 David Silver b Université Paris Sud LRI CNRS INRIA France b University College London UK r t c l e n f o b s t r c t Article history Received 27 October 2010 Received revised form 22 March 2011 Accepted 30 March 2011 Available online 6 April 2011 Keywords Computer Go MonteCarlo Search Reinforcement learning 1 Introduction A new paradigm search based MonteCarlo simulation revolutionised performance Go programs In article extensions MonteCarlo tree search algorithm signiﬁcantly improve effectiveness basic algorithm When applied extensions Go program MoGo ﬁrst program achieve dan master level 9 9 Go In article survey MonteCarlo revolution Go outline key ideas led success MoGo subsequent Go programs provide ﬁrst time comprehensive description theory practice extended framework MonteCarlo tree search 2011 Elsevier BV All rights reserved MonteCarlo tree search 1 new paradigm search revolutionised Go 23 rapidly replacing traditional search algorithms method choice challenging domains General Game Playing 4 Amazons 5 Lines Action 6 multiplayer card games 78 realtime strategy games 9 The key idea simulate thousands random games current position selfplay New positions added search tree node tree contains value predicts win position These predictions updated MonteCarlo simulation value node simply average outcome simulated games visit position The search tree guide simulations promising paths selecting child node highest potential value 10 This results highly selective search quickly identiﬁes good sequences The evaluation function MonteCarlo tree search depends observed outcomes simulations handcrafted evaluation functions traditional search algorithms The evaluation function continues improve additional simulations given inﬁnite memory computation converge optimal search tree 10 Furthermore MonteCarlo tree search develops highly selective bestﬁrst manner expanding promising regions search space deeply In article major enhancements MonteCarlo tree search The ﬁrst extension Rapid Action Value Estimation RAVE algorithm shares value actions subtree search tree RAVE forms fast rough estimate action value normal MonteCarlo slower accurate The MCRAVE algorithm combines value estimates principled fashion minimise mean squared error Corresponding author Email addresses sylvaingellym4xorg S Gelly davidstarsilvergooglemailcom D Silver 1 Now Google Zurich 00043702 matter 2011 Elsevier BV All rights reserved doi101016jartint201103007 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1857 The second extension heuristic MonteCarlo tree search uses heuristic function initialise values new positions search tree We demonstrate effective heuristic function learnt temporaldifference learning selfplay general heuristic provided algorithm We applied extensions Go program MoGo achieving signiﬁcant improvement performance 9 9 Go The resulting program ﬁrst program achieve dan master level ﬁrst program defeat human professional player This framework MonteCarlo tree search wide variety masterlevel Go programs including ﬁrst programs achieve dan level 19 19 Go This article provides ﬁrst comprehensive description extended framework MonteCarlo tree search It adds new theory results pseudocode discussion original presentation heuristic MCRAVE 11312 In addition include survey strongest Go programs based prior approaches strongest current programs based MonteCarlo methods 2 Simulationbased search 21 Twoplayer games We consider class twoplayer perfectinformation zerosum games chess checkers backgammon Go Without loss generality player ﬁrst Black player second White Black White alternate turns turn t selecting action Ast st S current state S ﬁnite state space As ﬁnite set legal actions state s The game ﬁnishes reaching terminal state outcome z Blacks goal maximise z Whites goal minimise z We deﬁne twoplayer policy π s Pras stochastic action selection strategy determines probability selecting actions given state It consists Black policy πB s Black moves White policy πW s White moves π cid4πB πW cid5 We deﬁne value function Q π s expected outcome playing action state s following policy π players termination2 Q π s Eπ z st s s S As 1 s value function maximises Blacks action value minimises Whites The minimax value function Q action value state action Q s max πB min πW Q π s s S As 2 A minimax policy deterministically plays Black moves maximise Q s This commonly called perfect play Q s plays White moves minimise 22 Simulation The basic idea simulationbased search 13 evaluate states online simulated games Each simulated game simulation starts root state s0 sequentially samples states actions backtracking game terminates At step t simulation simulation policy π s select action π st rules game generate state st1 The outcome z simulated game update values states actions encountered simulation 23 MonteCarlo simulation MonteCarlo simulation simple simulationbased search algorithm evaluating candidate actions root state s0 The search proceeds simulating complete games s0 termination ﬁxed simulation policy example selecting actions uniformly legal moves The value action s0 estimated mean outcome simulations starting candidate action MonteCarlo simulation provides simple method estimating root value Q π s0 Ns complete games simulated selfplay policy π state s The MonteCarlo value MC value Q s mean outcome simulations action selected state s Q s 1 Ns Nscid2 i1 Iis azi 3 2 In twoplayer games state usually called position action usually called The goodness positions moves estimated evaluation function We use terms informal discussions use state action value function precise sense 1858 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 zi outcome ith simulation Iis indicator function returning 1 action selected state cid3 s ith simulation 0 Ns Iis counts total number simulations action selected state s Ns i1 In basic form MonteCarlo simulation evaluate actions improve simulation policy However basic algorithm extended progressively favouring successful actions progressively pruning away successful actions 1415 In problems backgammon 16 Scrabble 17 Amazons 5 Lines Action 6 possible struct accurate evaluation function In cases beneﬁcial stop simulation end game bootstrap estimated value time stopping This approach known truncated MonteCarlo simulation increases simulation speed reduces variance MonteCarlo evaluation In challenging problems Go 15 hard construct accurate evaluation function In case truncating simulations usually increases evaluation bias reduces evaluation variance better simulate termination 24 MonteCarlo tree search MonteCarlo tree search MCTS uses MonteCarlo simulation evaluate nodes search tree 1 The values search tree select best action subsequent simulations MonteCarlo tree search sequentially bestﬁrst selects best child step simulation This allows search continually refocus attention simulation highest value regions state space As search tree grows larger values nodes approximate minimax value simulation policy approximates minimax policy The search tree T contains node ns corresponding state s seen simulations Each node contains total count state Ns action value Q s count Ns action A Simulations start root state s0 divided stages When state st represented search tree st T tree policy select actions Otherwise default policy roll simulations completion The simplest version algorithm greedy MCTS selects greedy action highest value ﬁrst stage argmaxa Q st selects actions uniformly random second stage Every state action search tree evaluated mean outcome simulations After simulation s0 a0 s1 a1 sT outcome z node search tree nst st T updates count updates action value Q st new MC value Eq 3 This update implemented incrementally reconsidering previous simulations incrementing count updating value outcome z3 Nst Nst 1 Nst Nst 1 Q st Q st z Q st Nst 4 5 6 In addition visited node added search tree In practice reduce memory requirements new nodes added simulation Typically new node added search tree simulation The ﬁrst state encountered represented tree added search tree If memory limitations issue possible wait simulations adding new node prune old nodes search progresses Fig 1 illustrates steps MCTS algorithm It possible compute statistics MonteCarlo tree search example max outcome evaluate positions rapidly sensitive outliers 15 intermediate statistic mean max outcome 1 However mean outcome proven robust effective statistic Go domains 25 UCT Greedy action selection ineﬃcient way construct search tree typically avoid searching actions poor outcomes signiﬁcant uncertainty value actions To explore search tree eﬃciently principle optimism face uncertainty applied favours actions greatest potential value To implement principle action value receives bonus corresponds uncertainty current value state action The UCT algorithm applies principle MonteCarlo tree search treating state search tree multi armed bandit action corresponds arm bandit 104 The tree policy selects actions UCB1 algorithm maximises upper conﬁdence bound value actions 18 Speciﬁcally action value 3 This incremental formulation accumulate error practice usually requires double precision 4 In fact search tree true multiarmed bandit real cost exploration planning In addition simulation policy continues change search tree updated means payoff nonstationary S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1859 Fig 1 Five simulations simple MonteCarlo tree search Each simulation outcome 1 black win 0 white win square At simulation new node star added search tree The value node search tree circles star updated count number black wins total number visits winsvisits augmented exploration bonus highest rarely visited stateaction pairs tree policy selects action maximising augmented value cid4 log Ns Ns Q s Q s c argmax Q s 7 8 c scalar exploration constant log natural logarithm Pseudocode UCT algorithm given Algorithm 1 1860 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 Algorithm 1 Twoplayer UCT procedure UctSearchs0 time available Simulateboard s0 end boardSetPositions0 return SelectMoveboard s0 0 end procedure procedure Simulateboard s0 boardSetPositions0 s0 sT SimTreeboard z SimDefaultboard Backups0 sT z end procedure procedure SimTreeboard c exploration constant t 0 boardGameOver st boardGetPosition st tree NewNodest return s0 st end SelectMoveboard st c boardPlaya t t 1 end return s0 st1 end procedure procedure SimDefaultboard boardGameOver DefaultPolicyboard boardPlaya end return boardBlackWins end procedure procedure SelectMoveboard s c legal boardLegal boardBlackToPlay argmaxalegal cid5 cid5 Q s c argminalegal Q s c cid6 cid6 cid7 cid7 log Ns Nsa log Ns Nsa end return end procedure procedure Backups0 sT z t 0 T Nst Nst 1 Nst 1 Q st zQ st Nst end end procedure procedure NewNodes treeInserts Ns 0 A Ns 0 Q s 0 end end procedure UCT proven converge minimax action value function 10 As number simulations N grows inﬁnity s0 Furthermore s0 O lognn probability selecting suboptimal action root values converge probability minimax action values A plimn Q s0 Q bias root values EQ s0 Q PrargmaxaA Q s0 cid12 argmaxaA Q s0 converges zero polynomial rate The performance UCT signiﬁcantly improved incorporating domain knowledge default policy 1920 The UCT algorithm carefully chosen default policy outperformed previous approaches search variety challenging games including Go 19 General Game Playing 4 Amazons 5 Lines Action 6 multiplayer card games 78 realtime strategy games 9 Much additional research MonteCarlo tree search developed context Go discussed section 3 Computer Go For years chess considered drosophila AI5 grand challenge task 21 It provided sandbox new ideas straightforward performance comparison algorithms measurable progress human capabilities With dominance alphabeta search programs human players conclusive chess 22 researchers sought new challenge Computer Go emerged new drosophila AI 21 task par excellence 23 grand challenge task generation 24 Go 10170 states 361 legal moves Its enormous search space orders magnitude big alphabeta search algorithms proven successful chess checkers Although rules simple emergent complexity game profound The longterm effect revealed 50 100 additional moves Professional Go players accumulate Go knowledge lifetime mankind accumulated Go knowl edge millennia For 30 years attempts encode knowledge machine usable form led positional understanding best comparable weak amateurlevel humans 5 Drosophila fruit ﬂy extensively studied organism genetics research S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1861 Fig 2 The White stones atari captured playing points marked A It illegal Black play B stone liberties Black play C capture stone D It illegal White recapture immediately playing D repeat position ko b The points marked E eyes Black The black groups left captured White alive The points marked F false eyes black stones right eventually captured White dead c Groups loosely connected white stones G removed board All surrounded intersections B W remaining stones black stones H d A ﬁnal position Dead stones B b w counted player If komi 65 Black wins 85 points example W Fig 3 Performance ranks Go increasing order strength left right 31 The rules Go The game Go usually played 19 19 grid 13 13 9 9 popular alternatives Black White play alternately placing single stone intersection grid Stones moved played captured Sets adjacent connected stones colour known blocks The intersections adjacent block called liberties If block reduced zero liberties opponent captured removed board Fig 2a A Stones remaining liberty said atari Playing stone zero liberties illegal Fig 2a B reduces opponent block zero liberties In case opponent block captured players stone remains board Fig 2a C Finally repeating previous board state illegal6 A situation repeat occur known ko Fig 2a D A connected set intersections wholly enclosed stones colour known eye One natural consequence rules block eyes captured opponent Fig 2b E Blocks captured described alive blocks certainly captured described dead Fig 2b F A loosely connected set stones described group Fig 2c G H Determining life death status group fundamental aspect Go strategy The game ends players pass Dead blocks removed board Fig 2d B In Chinese rules alive stones intersections enclosed player counted point territory player Fig 2d B W 7 Black plays ﬁrst Go White receives compensation known komi playing second The winner player greatest territory adding komi White W 32 Go ratings Human Go players rated threeclass scale divided kyu beginner dan master professional dan ranks Fig 3 Kyu ranks descending order strength dan professional dan ranks ascending order At amateur level difference rank corresponds number handicap stones required weaker player ensure game8 The majority Go programs compete Computer Go Server CGOS This server runs ongoing rapid play tournament 5 minute games 9 9 20 minute games 19 19 boards The Elo rating program server continually updated The Elo scale CGOS assumes logistic distribution winning probability Pr A beats B μ A μB Elo ratings player A player B respectively On scale difference 1 μB μ A 400 110 6 The exact deﬁnition repeating differs subtly different rule sets 7 The Japanese scoring somewhat different usually outcome 8 The difference 1 kyu 1 dan normally considered 1 stone 1862 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 200 Elo corresponds 75 winning rate stronger player difference 500 Elo corresponds 95 winning rate Following convention open source Go program GnuGo level 10 anchors scale rating 1800 Elo 33 Handcrafted heuristics In classic games handcrafted heuristic functions proven highly effective Basic heuristics ma terial count mobility provide reasonable estimates goodness checkers chess Othello 25 worthless Go Stronger heuristics proven surprisingly hard design despite decades endeavour 26 Until recently Go programs incorporated large quantities expert knowledge pattern database containing thousands manually inputted patterns typically including expert knowledge fuseki opening patterns joseki corner patterns tesuji tactical patterns Traditional Go programs use databases generate plausible moves match patterns The pattern database accounts large development effort traditional Go program requiring manyears effort expert Go players The Many Faces Go9 uses local alphabeta searches determine life death status blocks groups A global alphabeta search evaluate fullboard positions heuristic function local search results Pattern databases generate moves local global searches The program GnuGo10 uses pattern databases specialised search routines determine local subgoals capture connection eye formation The local status subgoal estimate overall beneﬁt legal 34 Reinforcement learning Go Reinforcement learning train value function predicts eventual outcome game given state The learning program rewarded score end game reward 1 Black wins 0 White wins Surprisingly informative binary signal proven successful 1 encourages agent favour risky moves calm moves ahead Expert Go players frequently play minimise uncertainty position judge ahead score behaviour replicated simply maximising expected score Despite shortcoming ﬁnal score widely reward signal 2730 Schraudolph et al 27 exploit symmetries Go board convolutional neural network The network predicts ﬁnal territory status particular target intersection It receives input intersection 1 0 1 White Empty Black respectively local region target outputs predicted territory target intersection The global position evaluated summing territory predictions intersections board Weights shared rotationally reﬂectionally symmetric patterns input features target intersections They train multilayer perceptron TD0 reward signal corresponding ﬁnal territory value intersection The network outperformed commercial Go program The Many Faces Go set low playing level 9 9 Go 3000 selfplay training games Dahls Honte 29 Enzenbergers NeuroGo III 30 use similar approach predicting ﬁnal territory However programs learn intermediate features input additional knowledge territory evaluation network Honte intermediate network predict local moves second network evaluate life death status groups NeuroGo III uses intermediate networks evaluate connectivity eyes Both programs achieved singledigit kyu ranks NeuroGo won silver medal 2003 9 9 Computer Go Olympiad RLGO 10 31 uses simpler computationally eﬃcient approach reinforcement learning It uses million local shape features enumerate possible 1 1 2 2 3 3 conﬁgurations Black White intersections possible location board The value state estimated linear combination local shape features matched state The weights features trained oﬄine temporaldifference learning games selfplay sharing weights symmetric local shape features The basic version RLGO rated 1350 Elo 9 9 Computer Go Server RLGO 24 3213 applies reinforcement learning approach online It applies temporaldifference learning simulated games selfplay start current state form simulationbased search At value function retrained realtime specialising tactics strategies relevant current position This approach boosted RLGOs rating 2100 Elo CGOS outperforming traditional Go programs resulting strongest 9 9 Go program based MonteCarlo tree search 35 MonteCarlo simulation Go In contrast traditional search methods MonteCarlo simulation evaluates current position dynamically storing knowledge positions static evaluation function This makes appealing choice Go seen number possible positions particularly large position evaluation particularly challenging 9 httpwwwsmartgamescommanyfaceshtml 10 httpwwwgnuorgsoftwaregnugo S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1863 The ﬁrst MonteCarlo Go program Gobble 33 simulated games selfplay current state s It combined MonteCarlo evaluation novel ideas allmovesasﬁrst heuristic ordered simulation The allmovesasﬁrst heuristic assumes value signiﬁcantly affected changes board The value playing action immediately estimated average outcome simulations action played time We formalise idea precisely Section 41 Gobble ordered simulation sort moves according estimated value This ordering randomly perturbed according annealing schedule cools additional simulations Each simulation plays moves prescribed order Gobble played weakly estimated rating 25 kyu Bouzy Helmstetter developed ﬁrst competitive Go programs based MonteCarlo simulation 15 Their basic framework simulates games selfplay current state s candidate action uniform random simulation policy value estimated average outcome simulations The domain knowledge prohibit moves eyes ensures games terminate reasonable timeframe Bouzy Helmstetter investigated number extensions MonteCarlo simulation precursors sophisticated algorithms 1 Progressive pruning technique statistically inferior moves removed consideration 34 2 The allmovesasﬁrst heuristic described 3 The temperature heuristic uses softmax simulation policy bias random moves strongest evalua e Q saτ blegal e Q sbτ τ constant temperature tions The softmax policy selects actions probability π s cid3 parameter controlling overall level randomness11 4 The minimax enhancement constructs width search tree separately evaluates node search tree MonteCarlo simulation Selective search enhancements tried 35 Bouzy tracked statistics ﬁnal territory status intersection simulation 36 This informa tion inﬂuence simulations disputed regions board avoiding playing intersections consistently players territory Bouzy incorporated pattern knowledge simulation player 20 Using enhancements program Indigo won bronze medal 2004 2006 19 19 Computer Go Olympiads It surprising MonteCarlo technique originally developed stochastic games backgammon 16 Poker 14 Scrabble 17 succeed Go Why evaluation based random play provide useful formation precise deterministic game Go The answer MonteCarlo methods successfully manage uncertainty evaluation A random simulation policy generates broad distribution simulated games repre senting possible futures uncertainty happen As search proceeds information accrued simulation policy reﬁned distribution simulated games narrows In contrast terministic play represents perfect conﬁdence future possible continuation If conﬁdence misplaced predictions based deterministic play unreliable misleading Abramson 37 ﬁrst demonstrate expected value games outcome random play powerful heuristic position evaluation deterministic games 36 MonteCarlo tree search Go MonteCarlo tree search ﬁrst introduced Go program Crazy Stone 1 The MonteCarlo value action s σ 2s During ﬁrst stage assumed normally distributed minimax value Q s N Q simulation tree policy selects action according estimated probability minimax value better During second stage simulation MonteCarlo value best action default policy selects moves probability proportional handcrafted urgency heuristic Using techniques Crazy Stone exceeded 1800 Elo CGOS achieving equivalent performance traditional Go programs GnuGo The Many Faces Go Crazy Stone won gold medal 2006 9 9 Computer Go Olympiad s Q s π s PrQ The Go program MoGo introduced UCT algorithm Go 1938 Instead Gaussian approximation Crazy Stone MoGo treats state search tree multiarmed bandit There arm bandit legal payoff arm outcome simulation starting During ﬁrst stage simulation tree policy selects actions UCB1 algorithm During second stage simulation MoGo uses default policy based specialised domain knowledge Unlike enormous pattern databases traditional Go programs MoGos patterns extremely simple Rather suggesting best situation patterns intended produce local sequences plausible moves They summarised applying prioritised rules opponent 1 If stones atari play saving random 2 Otherwise 8 intersections surrounding matches simple pattern cutting hane randomly play 11 Gradually reducing temperature simulated annealing beneﬁcial 1864 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 3 Otherwise opponent stone captured play capturing random 4 Otherwise play random The default policy MoGo handcrafted In contrast second version Crazy Stone uses supervised learning train pattern weights default policy 2 The relative strength patterns estimated assigning Elo rating pattern like tournament games players In approach pattern selected human player considered won alternative patterns Crazy Stone uses minorisationmaximisation algorithm estimate Elo rating simple 3 3 patterns features The default policy selected actions probability proportional matching pattern strengths A complicated set 17000 patterns harvested expert games progressively widen search tree Using UCT algorithm MoGo Crazy Stone signiﬁcantly outperformed previous 9 9 Go programs beginning new era Go 4 Rapid action value estimation MonteCarlo tree search separately estimates value state action search tree As result generalise related positions related moves To determine best simulations performed states actions The RAVE algorithm uses allmovesasﬁrst heuristic node search tree estimate value action RAVE provides simple way share knowledge related nodes search tree resulting rapid biased estimate action values This biased estimate determine best handful simulations signiﬁcantly improve performance search algorithm 41 Allmovesasﬁrst In incremental games Go value unaffected moves played board The underlying idea allmovesasﬁrst AMAF heuristic 33 Section 35 general value regardless played We deﬁne AMAF value function Q π s expected outcome z state s following joint policy π players given action selected subsequent turn Q π s Eπ z st s u cid2 t st au 9 The AMAF value function provides biased estimate true action value function The level bias Bs depends particular state s action Q π s Q π s Bs 10 MonteCarlo simulation approximate Q π s The allmovesasﬁrst value Q s mean outcome simulations action selected turn s encountered Q s 1 Ns Nscid2 i1 Iis azi 11 Iis indicator function returning 1 state s encountered step t ith simulation action Iis counts total number simulations selected step u cid2 t 0 Ns estimate AMAF value Note Black moves White moves considered distinct actions played intersection Ns i1 cid3 In order select best reasonable accuracy MonteCarlo simulation requires simulations candidate The AMAF heuristic provides orders magnitude information typically tried occasions handful simulations If value unaffected approximately moves played result faster rough estimate value 42 RAVE The RAVE algorithm Fig 4 combines MonteCarlo tree search allmovesasﬁrst heuristic Instead computing MC value Eq 3 node searchtree s T AMAF value Eq 11 node computed Every state search tree s T root subtree τ s S If simulation visits state st step t subsequent states visited simulation su u cid2 t subtree st su τ st This includes states su T visited default policy second stage simulation The basic idea RAVE generalise subtrees The assumption value action state s similar states subtree τ s Thus value estimated simulations starting s regardless exactly played S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1865 Fig 4 An example RAVE algorithm estimate value Black moves b state s Six simulations executed state s outcomes shown squares Playing immediately led losses MonteCarlo estimation favours b However playing subsequent time led wins ﬁve RAVE algorithm favours Note simulation starting root node belong subtree τ s contribute AMAF estimate Q s When AMAF values select action state st action maximum AMAF value subtree τ st Q sk ancestor Q st b In principle possible incorporate AMAF values selected argmaxb subtrees τ sk k t However experiments combining ancestor AMAF values appear confer advantage RAVE closely related history heuristic alphabeta search 39 During depthﬁrst traversal search tree history heuristic remembers success12 depths successful moves tried ﬁrst subsequent positions RAVE similar bestﬁrst depthﬁrst search store values subtree In addition RAVE takes account success moves outside search tree default policy 43 MCRAVE The RAVE algorithm learns quickly wrong The principal assumption RAVE particular value entire subtree frequently violated There situations example tactical battles nearby changes completely change value rendering redundant making vital Even distant moves signiﬁcantly affect value example playing ladder breaker corner radically alter value playing ladder opposite corner The MCRAVE algorithm overcomes issue combining rapid learning RAVE algorithm accuracy convergence guarantees MonteCarlo tree search There node ns state s search tree Each node contains total count Ns A MC value Q s AMAF value Q s MC count Ns AMAF count Ns To estimate overall value action state s use weighted sum Q cid5s MC value Q s AMAF value Q s cid5 Q cid5s cid7 1 βs Q s βs Q s 12 βs weighting parameter state s action It function statistics s stored node ns provides schedule combining MC AMAF values When simulations seen weight AMAF value highly βs 1 When simulations seen weight MonteCarlo value highly βs 0 As MonteCarlo tree search simulation divided stages During ﬁrst stage states search tree st T actions selected greedily maximise combined MC AMAF value argmaxb Q cid5st b During second stage simulation states search tree st T actions selected default policy 12 A successful alphabeta causes cutoff best minimax value 1866 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 After simulation s0 a0 s1 a1 sT outcome z MC AMAF values updated For state st simulation represented search tree st T values counts corresponding node nst updated Nst Nst 1 Nst Nst 1 Q st Q st z Q st Nst 13 14 15 In addition AMAF value updated subtree For state st simulation represented tree st T subsequent action simulation au colour play u cid2 t t u mod 2 AMAF value st au updated according simulation outcome z Nst au Nst au 1 Q st au Q st au z Q st au Nst au 16 17 If multiple moves played intersection simulation update performed ﬁrst intersection If action au legal state su illegal state st update performed 44 UCTRAVE The UCT algorithm extends MonteCarlo tree search use optimisminthefaceofuncertainty principle incor porating bonus based upper conﬁdence bound current value Similarly MCRAVE algorithm incorporate exploration bonus cid4 Q cid5 s Q cid5s c log Ns Ns Actions selected ﬁrst stage simulation maximise augmented value argmaxb Q We algorithm UCTRAVE13 If schedule decreases zero nodes s T A limN βs 0 asymptotic behaviour UCT RAVE equivalent UCT The asymptotic convergence properties UCT Section 2 apply UCTRAVE We different schedules property 45 Handselected schedule One handselected schedule MCRAVE uses equivalence parameter k cid4 βs k 3Ns k 19 k speciﬁes number simulations MonteCarlo value AMAF value given equal weight βs 1 2 cid4 k 3Ns k 1 2 1 k 3Ns k 4 k Ns 20 21 22 We tested MCRAVE Go program MoGo handselected schedule Eq 19 default policy described 19 different settings equivalence parameter k For setting played 2300 game match GnuGo 3710 level 10 The results shown Fig 5 compared MonteCarlo tree search 3000 simulations algorithms The winning rate MCRAVE varied 50 60 compared 24 13 The original UCTRAVE algorithm included RAVE count exploration term 11 However hard justify explicit RAVE exploration actions evaluated AMAF regardless action actually selected turn t 18 cid5 s b S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1867 Fig 5 Winning rate MCRAVE 3000 simulations GnuGo 3710 level 10 9 9 Go different settings equivalence parameter k The bars indicate standard error Each point plot average 2300 complete games RAVE Maximum performance achieved equivalence parameter 1000 indicating rapid action value estimate reliable standard MonteCarlo simulation thousand simulations executed state s 46 Minimum MSE schedule The schedule presented Eq 19 somewhat heuristic nature We develop principled schedule selects βs minimise mean squared error combined estimate Q cid5s 461 Assumptions To derive schedule simpliﬁed statistical model MCRAVE Our ﬁrst assumption policy π held constant Under assumption outcome MonteCarlo simulation playing action state s independent identically distributed iid Bernoulli random variable Furthermore outcome AMAF simulation playing action turn following state s iid Bernoulli random variable Prz 1 st s Q π s Prz 0 st s 1 Q π s Prz 1 st s u cid2 t st au Q π s Prz 0 st s u cid2 t st au 1 Q π s 23 24 25 26 It follows total number wins Ns simulations action played state s binomially distributed Similarly total number wins Ns simulations action played turn following state s binomially distributed Ns aQ s Binomial Ns Q s Binomial cid5 cid5 cid7 Ns Q π s cid7 Ns Q π s 27 28 Our second assumption distributions independent MC AMAF values uncorre lated In fact simulations compute MC value compute AMAF value means values certainly correlated Furthermore tree develops time simulation policy changes This means outcomes iid total number wins fact binomially distributed Nevertheless believe simpliﬁcations signiﬁcantly affect performance schedule practice 462 Derivation To simplify notation consider single state s action We denote number MonteCarlo simulations n Ns number simulations compute AMAF value n Ns abbreviate schedule 1868 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 β βs We denote estimated mean bias respect Q π s variance MC AMAF combined values respectively μ μ μcid5 b b bcid5 σ 2 σ 2 σ 2 cid5 mean squared error combined value e2 cid5 μ Q s μ Q s μcid5 Q cid5s b Q π s Q π s 0 b Q π s Q π s Bs bcid5 Q π cid5 s Q π s cid8cid5 σ 2 E 2 cid8cid5 cid8cid5 cid7 Q s Q π s cid7 Q s Q π s cid7 Q cid5s Q π cid5 s cid7 Q cid5s Q π s 2 2 2 cid10 cid9 cid9 Ns n cid9 cid10 cid9 Ns n cid9 cid9 Ns n Ns n cid9 cid10 cid9 Ns n Ns n cid10 σ 2 E E σ 2 cid5 E e2 cid5 cid8cid5 e2 cid5 σ 2 cid5 b2 cid5 1 β2σ 2 β 2 σ 2 1 β2σ 2 β 2 σ 2 β 2 b2 cid5 β b 1 βb cid7 2 Differentiating respect β setting zero 0 2β σ 2 21 βσ 2 2β b2 β σ 2 σ 2 σ 2 b2 σ 2 σ 2 Q π s a1 Q π s Ns Q π s a1 Q π s Ns n n n nnb2μcid51 μcid5 In roughly positions μcid5 1 β μcid51 μcid5 n μcid51 μcid5 n 2 simplify schedule β n n n 4nnb2 We start decomposing mean squared error combined value bias variance MC AMAF values respectively making use second assumption values independently distributed We use ﬁrst assumption MC AMAF values binomially distributed estimate variance 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 This equation includes unknown constant RAVE bias b This evaluated empirically testing performance algorithm constant values b machine learning learning predict error AMAF value MC value simulations The method simple effective method allow different biases identiﬁed different types position 463 Results We compared performance MCRAVE minimum MSE schedule approximation Eq 47 handselected schedule Eq 19 For minimum MSE schedule ﬁrst identiﬁed best constant RAVE bias empirical tests On 9 9 board performance MoGo minimum MSE schedule increased 80 Elo Table 1 On 19 19 board improvement 100 Elo S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1869 Table 1 Winning rate MoGo GnuGo 3710 level 10 number simulations increased MoGo competed CGOS heuristic MCRAVE handselected schedule February 2007 The versions 10 minutes game modify simulations according available time 300000 games opening 20000 endgame The asterisked version competed CGOS April 2007 minimum MSE schedule additional parameter tuning Schedule Handselected Handselected Handselected Minimum MSE Computation Wins vs GnuGo CGOS rating 3000 sims 10000 sims 10 minutes game 10 minutes game 69 82 92 97 1960 2110 2320 2480 5 Heuristic prior knowledge We introduce second extension MonteCarlo tree search heuristic MCTS If particular state s action rarely encountered simulation MonteCarlo value estimate highly uncertain unreliable Further search tree branches exponentially vast majority nodes tree experienced rarely The situation leaf nodes worst deﬁnition leaf node visited child node added In order reduce uncertainty rarely encountered positions incorporate prior knowledge heuristic evaluation function Hs heuristic conﬁdence function Cs When node ﬁrst added search tree initialised according heuristic function Q s Hs Ns Cs The conﬁdence heuristic function measured terms equivalent experience number simulations required order achieve MonteCarlo value similar accuracy heuristic value14 After initialisation value count updated usual standard MonteCarlo simulation 51 Heuristic MCRAVE The heuristic MonteCarlo tree search algorithm combined MCRAVE algorithm described pseudocode Algorithm 2 When new node ns added tree actions A initialise MC AMAF values heuristic evaluation function initialise counts heuristic conﬁdence functions C C respectively Q s Hs Ns Cs Q s Hs Ns Cs cid2 Ns Ns aA 48 49 50 51 52 We compare heuristic evaluation functions 9 9 Go heuristic MCRAVE algorithm program MoGo 1 The evengame heuristic Q evens 05 makes assumption positions encountered strong play ers likely close 2 The grandfather heuristic Q grandst Q st2 sets value node tree value grandfa ther This assumes value Black usually similar value time Black play 3 The handcrafted heuristic Q mogos based patternbased rules successfully MoGos default policy The heuristic designed moves matching good pattern assigned value 1 moves matching bad pattern given value 0 moves assigned value 05 The good bad patterns identical MoGo selecting moves greedily according heuristic breaking ties randomly exactly produce default policy πmogo 4 The local shape heuristic Q rlgos computed linear combination local shape features RLGO 10 Section 34 This heuristic learnt oﬄine temporal difference learning games selfplay For heuristic evaluation function assign heuristic conﬁdence Cs M constant values equiv alent experience M We played 2300 games MoGo GnuGo 3710 level 10 The MCRAVE algorithm executed 3000 simulations Fig 6 14 This equivalent beta prior binary outcomes 1870 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 Algorithm 2 Heuristic MCRAVE procedure McRaves0 time available Simulateboard s0 end boardSetPositions0 return SelectMoveboard s0 0 end procedure procedure Simulateboard s0 boardSetPositions0 s0 a0 sT aT SimTreeboard aT 1 aD z SimDefaultboard T Backups0 sT a0 aD z end procedure procedure SimDefaultboard T t T 1 boardGameOver DefaultPolicyboard boardPlayat t t 1 end z boardBlackWins return aT 1 at1 z end procedure procedure SimTreeboard t 0 boardGameOver st boardGetPosition st tree NewNodest DefaultPolicyboard return s0 a0 st end SelectMoveboard st boardPlayat t t 1 end return s0 a0 st1 at1 end procedure procedure SelectMoveboard s legal boardLegal boardBlackToPlay return argmaxalegal Evals return argminalegal Evals end end procedure procedure Evals b pretuned constant bias value Nsa β Nsa Nsa4Nsa Nsab2 return 1 βQ s β Q s end procedure procedure Backups0 sT a0 aD z t 0 T Nst 1 Q st zQ st Nst u t D step 2 au at2 au2 Nst au 1 Q st au z Q st Nst end end end end procedure procedure NewNodeboard s treeInserts boardLegal Ns Q s Ns Q s Heuristicboard end end procedure The value function learnt local shape features Q rlgo outperformed heuristics increased win ning rate MoGo 60 69 Maximum performance achieved equivalent experience M 50 indicates Q rlgo worth 50 simulations allmovesasﬁrst It likely results improved varying heuristic conﬁdence according particular position based variance heuristic evaluation function 52 Exploration exploitation The performance MonteCarlo tree search greatly improved carefully balancing exploration exploitation The UCT algorithm signiﬁcantly outperforms greedy tree policy Go 19 Surprisingly result appear extend heuristic UCTRAVE algorithm optimal exploration rate experiments zero greedy MCRAVE exploration tree policy We believe explanation lies nature RAVE algorithm Even action selected immediately position s played later point simulation This greatly reduces need explicit exploration values actions continually updated regardless initial selection However able run thorough tests tens thousands simulations It possible exploration important MCRAVE scaled millions simulations At point substantial number nodes dominated MC values RAVE values exploration nodes beneﬁcial S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1871 Fig 6 Winning rate MoGo heuristic MCRAVE algorithm 3000 simulations GnuGo 3710 level 10 9 9 Go Four different forms heuristic function text The bars indicate standard error Each point plot average 2300 complete games 53 Soft pruning Computer Go large branching factor pruning techniques selective search progressive widening Section 3 developed reduce size search space 40 Heuristic MCTS MCRAVE viewed soft pruning techniques focus highest valued regions search space permanently cutting branches search tree A heuristic function provides principled way use prior knowledge reduce effective branching factor Moves favoured heuristic function initialised high value tried moves low heuristic value However heuristic evaluation function incorrect initial value drop rate determined heuristic conﬁdence function moves explored The MCRAVE algorithm signiﬁcantly reduces effective branching factor RAVE forms fast rough estimate value Moves high RAVE values quickly favoured moves low RAVE values soft pruned search tree However RAVE values initially MCRAVE cuts branches permanently search tree Heuristic MCRAVE wrong The heuristic evaluation function inaccurate andor RAVE estimate misleading In case heuristic MCRAVE prioritise wrong moves best moves soft pruned tried simulations There guarantees algorithms help performance However practice help hurt average positions provide signiﬁcant performance advantage 54 Performance heuristic MCRAVE MoGo Our extensions MCTS heuristic MCTS MCRAVE increased winning rate MoGo GnuGo 24 UCT 69 heuristic MCRAVE However results based executing 3000 simulations handselected schedule Eq 19 When number simulations increased overall perfor mance MoGo improved correspondingly Table 1 shows performance heuristic MCRAVE scales additional computation The 2007 release version MoGo heuristic MCRAVE algorithm minimum MSE schedule Eq 47 improved handcrafted heuristic function15 The scalability release version shown Fig 7 based results combined study thousands hours 41 This version MoGo ﬁrst program achieve dan level 9 9 Go ﬁrst program beat professional human player 9 9 Go highest rated program Computer Go Server 9 9 19 19 Go gold medal winner 2007 19 19 Computer Go Olympiad achieved rating 2 kyu 19 19 Go human players Kiseido Go Server 15 Local shape features release version 1872 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 Fig 7 Scalability MoGo 2007 release version data collected members Computer Go mailing list 41 Elo ratings computed large tournament consisting thousand games version MoGo successive doublings number simulations Error bars indicate 95 conﬁdence intervals Elo rating 6 Survey subsequent work The results previous section achieved MoGo 2007 We brieﬂy survey subsequent work heuristic MCRAVE variety strong Go programs The heuristic function MoGo substantially enhanced initialising Hs Cs Cs handtuned values based handcrafted rules patterns 42 Supervised learning bias selection patterns favoured expert games In addition handcrafted default policy modiﬁed increase diversity simulations playing regions board ﬁx known issue lifeanddeath playing key point simple dead shapes known nakade Using 100000 simulations improved version MoGo achieved winning rate 55 9 9 boards 53 19 19 boards 2007 release version MoGo MoGo modiﬁed massively parallelising MCRAVE algorithm run cluster 43 In order avoid huge communication overheads memory shared shallowest nodes search tree The massively parallel version MoGo Titan run 800 processors Huygens Dutch national supercomputer MoGo Titan defeated 9 dan professional player JunXun Zhou 19 19 Go 7 stones handicap S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1873 Table 2 Approximate Elo ratings Computer Go Server 9 9 Go programs discussed text Year 2006 2006 2006 2006 2007 2007 2007 2009 2010 2010 Program Indigo GnuGo Many Faces NeuroGo RLGO MoGo Crazy Stone Fuego Many Faces Zen Description Pattern database MonteCarlo simulation Pattern database alphabeta search Temporaldifference learning neural network Temporaldifference search Variants heuristic MCRAVE Elo 1400 1800 1800 1850 2100 2500 2500 2700 2700 2700 The program Zen successfully combined MCRAVE sophisticated domain knowledge Zen ﬁrst program sustain dan rank fullsize 19 19 boards human players Kiseido Go Server KGS It currently ranked 4 dan placing 5 30000 ranked human players KGS Several strongest traditional Go programs combine existing tactical pattern knowledge heuristic MCRAVE framework including The Many Faces Go currently ranked 2 dan KGS Aya currently ranked 1 dan The open source program Fuego 44 extends MCRAVE algorithm use additional rapid value estimates variant minimum MSE schedule Eq 46 A parallelised version Fuego defeated 9 dan professional player 9 9 game defeated 6 dan amateur player 4 stones handicap size board16 The latest versions MoGo Crazy Stone The Many Faces Go achieved impressive victories professional players size boards Most recently program Erica combined heuristic MCRAVE new technique known simulation balancing 45 automatically tune parameters default policy 46 Previous machine learning approaches focused optimising strength default policy assumption stronger policy perform better Monte Carlo search 1 Unfortunately practice assumption incorrect 11 general diﬃcult ﬁnd default policy performs MonteCarlo search The key idea simulation balancing minimise error MonteCarlo value Q s oracle value computed deep search Erica simulation balancing train 2000 parameters default policy 9 9 Go Erica won gold medal 2010 19 19 Computer Go Olympiad currently ranked 3 dan KGS We provide summary current state art Go based ratings Computer Go Server Table 2 Kiseido Go Server Fig 8 Several programs described Section 3 included comparison 7 Conclusions For 30 years Go programs evaluated positions handcrafted heuristics based human expert knowledge shapes patterns rules However professional Go players play moves according intuitive feelings hard express quantify Precisely encoding knowledge machineunderstandable rules proven deadend classic example knowledge acquisition bottleneck Furthermore traditional search algorithms based handcrafted heuristics cope enormous state space branching factor game Go unable effective use additional computation time This approach led Go programs best comparable weak amateurlevel humans 2647 In contrast MonteCarlo tree search requires human knowledge order understand position Instead positions evaluated outcome thousands simulated games selfplay position These simulated games progressively reﬁned prioritise selection positions promising evaluations Over course simulations attention focused selectively narrow regions search space correlated successful outcomes Unlike traditional search algorithms approach scales size state space branching factor scale additional computation time In practice strongest programs extensive use expert human knowledge improve default policy deﬁne prior knowledge This knowledge accelerates progress search affect asymptotic optimality On Computer Go Server 9 9 13 13 19 19 board sizes traditional search programs rated 1800 Elo MonteCarlo programs enhanced RAVE heuristic knowledge rated 2500 Elo standard hardware17 Table 2 On Kiseido Go Server fullsize boards human opposition traditional search programs reached 5 kyu best MonteCarlo programs rated 4 dan Fig 8 The 16 See HumanComputer Go Challenges httpwwwcomputergoinfohcindexhtml 17 A difference 700 Elo corresponds 99 winning rate 1874 S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 Fig 8 Ranks Go programs discussed text Kiseido Go Server KGS Each point represents ﬁrst date program held given rank 20 consecutive games KGS Note program plays different time controls cause variations rank programs play regularly cause variations date18 Version GnuGo Aya Many Faces Go based traditional search MonteCarlo programs competitive human professionals 9 9 Go winning handicap games human professionals 19 19 Go In Go program MoGo doubling computation power led increase playing strength approximately 100 Elo points 13 13 Go Fig 7 19 19 Go The strongest programs lag far strongest humans improving rapidly Fig 8 shows initial jump performance achieved ﬁrst MonteCarlo programs Go programs continued improve rank year This new framework MonteCarlo tree search extends Go Variants heuristic MCRAVE performed previous search algorithms challenging games Hex 48 Havannah 49 The challenging properties Go characteristic hardest search planning decisionmaking problems Immedi ate actions delayed longterm consequences leading surprising complexity enormous search spaces intractable traditional search algorithms Variants heuristic MCTS MCRAVE outperforming previous approaches challenging search spaces feature selection 50 POMDP planning 51 natural language phrase generation 52 Understanding achieve high performance Go opening new possibilities high performance AI wide variety challenging problems References 1 R Coulom Eﬃcient selectivity backup operators MonteCarlo tree search 5th International Conference Computer Games pp 7283 2 R Coulom Computing Elo ratings patterns game Go International Computer Games Association Journal 30 2007 198208 3 S Gelly D Silver Achieving master level play 9 9 Go 23rd Conference Artiﬁcial Intelligence pp 15371540 4 H Finnsson Y Björnsson Simulationbased approach general game playing 23rd Conference Artiﬁcial Intelligence pp 259264 5 R Lorentz Amazons discover MonteCarlo 6th International Conference Computers Games pp 1324 6 M Winands YY Björnsson Evaluation function based MonteCarlo LOA 12th Advances Computer Games Conference pp 3344 7 J Schäfer The UCT algorithm applied games imperfect information Diploma thesis OttovonGuerickeUniversität Magdeburg 2008 8 N Sturtevant An analysis UCT multiplayer games 6th International Conference Computers Games pp 3749 9 R Balla A Fern UCT tactical assault planning realtime strategy games 21st International Joint Conference Artiﬁcial Intelligence pp 40 45 10 L Kocsis C Szepesvari Bandit based MonteCarlo planning 15th European Conference Machine Learning pp 282293 11 S Gelly D Silver Combining online oﬄine learning UCT 17th International Conference Machine Learning pp 273280 12 S Gelly A contribution reinforcement learning Application Go PhD thesis University South Paris 2007 13 D Silver Reinforcement learning simulationbased search game Go PhD thesis University Alberta 2009 14 D Billings LP Castillo J Schaeffer D Szafron Using probabilistic knowledge simulation play poker 16th National Conference Artiﬁcial Intelligence pp 697703 18 See httpsenseisxmpnetKGSBotRatings S Gelly D Silver Artiﬁcial Intelligence 175 2011 18561875 1875 15 B Bouzy B Helmstetter MonteCarlo Go developments 10th Advances Computer Games Conference pp 159174 16 G Tesauro G Galperin Online policy improvement MonteCarlo search Advances Neural Information Processing 9 pp 10681074 17 B Sheppard Worldchampionshipcaliber scrabble Artiﬁcial Intelligence 134 2002 241275 18 P Auer N CesaBianchi P Fischer Finitetime analysis multiarmed bandit problem Machine Learning 47 2002 235256 19 S Gelly Y Wang R Munos O Teytaud Modiﬁcation UCT patterns MonteCarlo Go Technical report 6062 INRIA 2006 20 B Bouzy Associating domaindependent knowledge Monte Carlo approaches Go program Information Sciences Heuristic Search Computer Game Playing IV 175 2005 247257 21 J McCarthy AI sport Science 276 1997 15181519 22 D McClain Once machine beats human champion chess New York Times December 5th 2006 23 A Harmon Queen captured mouse chess players use computers edge New York Times February 6th 2003 24 D Mechner All systems Go The Sciences 38 1998 25 J Schaeffer The games computers people play Advances Computers 50 2000 189266 26 M Müller Computer Go Artiﬁcial Intelligence 134 2002 145179 27 N Schraudolph P Dayan T Sejnowski Temporal difference learning position evaluation game Go Advances Neural Information Processing 6 pp 817824 28 M Enzenberger The integration priori knowledge Go playing neural network httpwwwcsualbertacaemarkusneurogoneurogo1996 html 1996 29 F Dahl Honte Goplaying program neural nets Machines Learn Play Games Nova Science 1999 pp 205223 30 M Enzenberger Evaluation Go neural network soft segmentation 10th Advances Computer Games Conference pp 97108 31 D Silver R Sutton M Müller Reinforcement learning local shape game Go 20th International Joint Conference Artiﬁcial Intelligence pp 10531058 32 D Silver R Sutton M Müller Samplebased learning search permanent transient memories 25th International Conference Machine Learning pp 968975 33 B Bruegmann MonteCarlo Go httpwwwcglucsfedugoProgramsGobblehtml 1993 34 B Bouzy Move pruning techniques MonteCarlo Go 11th Advances Computer Games Conference pp 104119 35 B Bouzy Associating shallow selective global tree search Monte Carlo 9 9 Go 4th International Conference Computers Games pp 6780 36 B Bouzy History territory heuristics MonteCarlo Go New Mathematics Natural Computation 2 2006 18 37 B Abramson Expectedoutcome A general model static evaluation IEEE Transactions Pattern Analysis Machine Intelligence 12 1990 182193 38 Y Wang S Gelly Modiﬁcations UCT sequencelike simulations MonteCarlo Go IEEE Symposium Computational Intelligence Games Honolulu Hawaii pp 175182 39 J Schaeffer The history heuristic alphabeta search enhancements practice IEEE Transactions Pattern Analysis Machine Intelligence PAMI11 1989 12031212 40 G Chaslot M Winands J Uiterwijk H van den Herik B Bouzy Progressive strategies MonteCarlo tree search New Mathematics Natural Computation 4 2008 343357 41 D Dailey 9 9 scalability study httpcgosboardspacenetstudyindexhtml 2008 42 G Chaslot L Chatriot C Fiter S Gelly J Hoock J Perez A Rimmel O Teytaud Combining expert online transient online knowledge Monte Carlo exploration 8th European Workshop Reinforcement Learning 43 S Gelly J Hoock A Rimmel O Teytaud Y Kalemkarian The parallelization MonteCarlo planning 6th International Conference Control Automation Robotics pp 244249 44 M Müller M Enzenberger B Arneson R Segal Fuego opensource framework board games Go engine based MonteCarlo tree search IEEE Transactions Computational Intelligence AI Games 2 2010 259270 45 D Silver G Tesauro MonteCarlo simulation balancing 26th International Conference Machine Learning pp 119126 46 S Huang R Coulom S Lin MonteCarlo simulation balancing practice 7th International Conference Computers Games pp 119126 47 B Bouzy T Cazenave Computer Go AIoriented survey Artiﬁcial Intelligence 132 2001 39103 48 B Arneson R Hayward P Henderson MoHex wins Hex tournament International Computer Games Association Journal 32 2009 114116 49 F Teytaud O Teytaud Creating upper conﬁdence tree program Havannah 12th Advances Computer Games Conference pp 6574 50 R Gaudel M Sebag Feature selection oneplayer game 27th International Conference Machine Learning pp 359366 51 D Silver J Veness Online MonteCarlo planning large POMDPs Advances Neural Information Processing Systems 24 52 J Chevelu T Lavergne Y Lepage T Moudenc Introduction new paraphrase generation tool based MonteCarlo sampling 47th Annual Meeting Association Computational Linguistics pp 249252