Artiﬁcial Intelligence 172 2008 454482 wwwelseviercomlocateartint Restricted gradientdescent algorithm valuefunction approximation reinforcement learning André da Motta Salles Barreto Charles W Anderson b Programa Engenharia CivilCOPPE Universidade Federal Rio Janeiro Rio Janeiro RJ Brazil b Department Computer Science Colorado State University Fort Collins CO 80523 USA Received 22 May 2006 received revised form 22 August 2007 accepted 23 August 2007 Available online 6 September 2007 Abstract This work presents restricted gradientdescent RGD algorithm training method local radialbasis function networks speciﬁcally developed context reinforcement learning The RGD algorithm seen way extract relevant features state space feed linear model computing approximation value function Its basic idea restrict way standard gradientdescent algorithm changes hidden units approximator results conservative modiﬁcations learning process prone divergence The algorithm able conﬁgure topology network important characteristic context reinforcement learning changing policy result different requirements approximator structure Computational experiments presented showing RGD algorithm consistently generates better valuefunction approximations standard gradientdescent method susceptible divergence In polebalancing Acrobot tasks RGD combined SARSA presents competitive results methods literature including evolutionary recent reinforcementlearning algorithms 2007 Elsevier BV All rights reserved Keywords Reinforcement learning Neurodynamic programming Valuefunction approximation Radialbasisfunction networks 1 Introduction Sutton Barto 77 Kaelbling et al 37 reinforcement learning class problems set techniques In paradigm agent learn act direct interaction environment The information available agent reinforcement signal providing evaluative feedback decisions This informal deﬁnition sufﬁcient explain increasing ﬁeld artiﬁcial intelli gence machine learning communities First framework crude appealing model actually happens nature analogy animal behavior irresistible 6476 From engineering perspec tive reinforcementlearning paradigm tempting transfers learning burden ﬁguring accomplish task This way instead providing set examples desired behavior Corresponding author Email addresses andremsblnccbr A da Motta Salles Barreto andersoncscolostateedu CW Anderson 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200708001 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 455 supervised learning designer left simpler task representing problem terms reward punishment signals If example wanted mobile robot travel point simply giving robot reward goal reached In order scenario true obstacles robots trajectory overcome One major difﬁculties arises combination reinforcement learning function approximation If wants solve realworld reinforcement learning taskswhere number possible states usually large allow exhaustive explorationit necessary provide agent ability generalize Nevertheless known completely understood combination reinforcement learning algorithms function approximators easily unstable ﬁnding feasible way merge paradigms today active area research machine learning This paper presents attempt direction training method local radialbasis function networks especially designed context reinforcement learning The restricted gradientdescent RGD algo rithm essentially modiﬁed version standard gradientdescent method inherits qualities drawbacks However restrictions imposed RGD application gradientdescents delta rule prone divergence In RGD algorithm center radial functions moved current state tends conﬁne convexhull formed training data Also widths radialbasis functions allowed shrink possibly diverge inﬁnity Obviously restrictions delta rule limit trajectory approximators parameter vector parameter space This lost ﬂexi bility compensated online allocation new hidden units results monotonically increasing approximation granularity This work organized follows Section 2 presents brief review reinforcement learning focusing methods rely concept value function address problem As mentioned stability methods affected use function approximators This issue discussed Section 3 One way alleviate instability caused use approximators adopt linear models operating features extracted original state space Section 4 discusses concept features paper way select local radialbasis functions Section 5 presents RGD algorithm seen strategy extract relevant features state space This algorithm applied Section 6 series computational experiments Particularly performance RGD compared standard gradientdescent method traditional reinforcementlearning algorithms techniques approximate value function Section 7 presents overall analysis experiments hypothesis explaining stable behavior restricted gradientdescent algorithm Finally Section 8 main conclusions research presented possible directions future work discussed 2 Reinforcement learning The goal agent reinforcement learning ﬁnd policy π mapping states actionsthat maximizes expected return The return Rt total reward agent receives long run time t Rt rt1 γ rt2 γ 2rt3 cid2 i0 γ irti1 γ 0 1 discount factor This parameter determines relative importance individual rewards depending far future The rewards r cid5 given environment agent time performs action Usually inter action happens discrete steps time step t agent selects action Ast function current state st S The sets S Ast environment represent possible states available actions state st As response action agent receives environment reward rt1 new state st1 This loop repeated indeﬁnitely agent reaches terminal state The interaction agent environment formalized Markov decision process 101454 One way address reinforcementlearning problem search good solution policy space directly This example parametrizing π evolutionary algorithm search good policies 3588 Another approach compute approximation gradient average reward respect parametrized policy perform gradient ascent 7873 456 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 Another way deal reinforcement learning problem use methods derived dynamic program ming 91454 One advantage approach fact dynamic programming studied long time supported strong understood theoretical basis Central dynamic programming approach concept value function The actionvalue function given policy π associates stateaction pair s expected return performing action state s following π Qπ s Eπ Rt st s Eπ denotes expected value following policy π Since notation widely adopted actionvalue function usually referred Qfunction Once Qfunction particular policy πk known derive new policy πk1 greedy respect Qπk s πk1s arg max aAs Qπk s 1 The policy πk1 guaranteed good better policy πk This fundamental idea reinforcement learning algorithms based dynamic programming given initial arbitrary policy π0 compute value function Qπ0 s generate better policy π1 greedy respect function The step compute Qπ1 s use generate new policy π2 Under certain assumptions successive alternation stepspolicy evaluation policy improvementcan shown converge optimal policy π maximizes expected return state Of course process executed different levels granularity It necessary example exact Qfunction Qπk s perform policy improvement step One compute rough approximation function use generate new policy πk1 The improvement step performed different levels Eq 1 update policy πk single state set entire state space S The exact way policy evaluation policy improvement steps performed deﬁnes different reinforcement learning algorithms 21 Computing value function A complete control problem broken stages policy evaluation policy improvement The policy improvement step usually easy compute This especially true Ast ﬁnite small st case reduced computation max operator 1 Therefore effort reinforcement learning research devoted evaluation problem given policy π compute value function Suppose state space ﬁnite set One way policy evaluation use sample trajectories compute average return associated stateaction pair 545 k s 1 Qπ k kcid2 i1 Ri t st s 2 t actual returns following visits s It shown sequence Qπ Ri asymp totically approaches true Qfunction π 77 One drawback approach fact naturally applied tasks interaction agent environment broken subsequences trials maze example new trial starts time agent reaches goal We subsequence episode 77 2 Qπ 1 Qπ Even task subdivided episodes 2 agent learn interaction collected data update Qπ k It possible agent learn interacting environment This recursive relation states known Bellman equation 9 cid3 rt1 γ Qπ cid7 sscid7 γ Qπ Ra P sscid7 cid4 cid5 st1 πst1 cid5cid8 cid7 Qπ s Eπ cid2 st s πs cid4 s 3 cid6 cid7 scid7 P reward transition sscid7 probability reaching state scid7 state s performing action Ra sscid7 expected A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 457 Eq 3 important results dynamic programming core valuefunction based reinforcementlearning methods First Qπ unique solution Bellman equation solving 3 stateaction pair corresponds ﬁnding true Qfunction π Furthermore Bellman equation makes possible update Qvalues state s based estimated value successors This Sutton Barto 77 bootstrapping basic mechanism agent learn interacting environment If environments dynamics completely knownthat P sscid7 giventhe Bellman equations stateaction pairs written linear equations solution true value function π One way solve use iterative methods approximation Qπ successively reﬁned based past estimates 1454 sscid7 Ra Qπ k1s P sscid7 sscid7 γ Qπ Ra k cid4 s cid7 πs cid7 cid5cid8 cid2 cid7 scid7 4 sscid7 Ra The algorithm known iterative policy evaluation It applied synchronously old values Qπ k s kept iteration GaussSeidel style recent available values targets updates In cases model required This restriction removed P sscid7 estimated sample transitions environment generative model This basic idea temporaldifference learning methods 74 update rule written cid7 Qπ 5 r reward received transition state s scid7 α 0 1 learning rate Temporal difference TD learning central ideas reinforcement learning It makes possible agent learn interacting environment knowledge dynamics With use eligibility traces TD method update Qvalue iteration This generates family algorithms known TDλ 67 cid7 r γ Qπ k k1s Qπ cid8 k s k s α πs cid4 s Qπ cid5 cid7 Eq 5 modiﬁed deal complete control problem evaluation step perform policy improvement step This incorporating max operator update rule resulting algorithm called Qlearning 83 If policy improvement step applied temporaldifference updatethat 1 5 applied alternatelyone SARSA algorithm 5775 The algorithms represented 4 5 control counterparts guaranteed converge true value function Qπ certain assumptions respected 14243666 One assumptions Qπ k stored lookuptable entry stateaction pair This makes application standard version methods large continuous state spaces infeasible 3 Reinforcement learning function approximation In realworld reinforcement learning tasks usually possible visit stateaction pair s agent able generalize limited experience In context dynamicprogramming based algorithms means value function represented function approximator Generalization examples studied artiﬁcial intelligence years great body research function approximation borrowed supervised learning However combination reinforcement learning function approximators straightforward task reasons 1 In reinforcement learning important learning occurs online agent interacts environ ment This requires methods able deal incrementally acquired data instead static training set 7785 2 Also bootstrapping method adopted target values training examplesthe righthand 4 5 exampleare highly nonstationary If policy changes learning process problem worse distribution training examples picked change time 55 3 As observed Boyan Moore 19 Gordon 32 function approximator able approximate ﬁnal value function Qπ able represent intermediary versions Qπ k function 458 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 4 In complete control problem good approximation optimal value function Q approximation error cause resulting policy π perform poorly 13586889 Despite difﬁculties reinforcement learning function approximators successfully applied domains Classical examples game playing Samuels checker player 6061 Tesauros TD Gammon program based reinforcement learning able play backgammon near level worlds strongest grandmasters 79 The reinforcement learning paradigm applied different sequential cision tasks shopschedule strategies NASA space shuttle missions 90 elevator dispatch control 23 dynamic channel allocation cellular telephone networks 65 regulation irrigation network 34 In ro botics reinforcement learning algorithms combined function approximation example navigation control 2541 help robots walk 12 play soccer 72 The contrast theoretical obstacles practical successes applying reinforcement learning function approximators resulted great area Some earlier works presented discouraging sults simple counterexamples popular methods fail dramatically 4193281 Later Sutton 75 showed counterexamples solved linear approximator line state sampling sampling transitions according current policy π Based observation Tsitsiklis Roy 82 proved TDλ conﬁguration converges probability 1 This result created strong tendency use linear models reinforcement learning theoretical analyses exist 53566378 The strongest results current literature convergence control algo rithms linear function approximators 33394849 Among kernelbased reinforcement learning 48 leastsquares policy iteration algorithm LSPI 39 deserve special attention discussed text 4 Features local basis functions As mentioned recent theoretical practical results reinforcementlearning literature consider use linear approximators One way represent Qfunction case discrete actions linear model possible action Qπ s mcid2 i1 wa θis As 6 wa cid8wa linear weights associated action θis m features representing state s common The concept feature Tsitsiklis Roy 82 corresponds kind information agent extracts environment The deﬁnition suitable feature space fundamental step reinforcement learning method completely taskdependent To illustrate statement compare known card games blackjack poker While ﬁrst values card ranks derive good strategy agent unable distinguish suits cards perform playing poker Usually identiﬁcation features trivial task If knowledge domain good strategy handcraft structure linear approximators adhere tightly possible frameworks guarantees exist If case desirable automate process alternative expensive trialanderror procedure 41 Local radialbasis functions One way extract features state space use basis functions 81 In way feature θis function mapping S cid5 It claimed literature functions θi local present signiﬁcant activation limited region state space 36980 The main motivation avoid phenomenon known interference 26 Interference happens update stateaction pair changes Qvalues pairs possibly wrong direction A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 459 Interference naturally associated generalization happens conventional supervised learning Nevertheless reinforcement learning paradigm effects tend harmful The reason twofold First combination interference bootstrapping easily unstable updates longer strictly local The convergence proofs algorithms derived 4 5 based fact operators contraction mappings successive application results sequence converging ﬁxed point solution Bellman equation 1436 When approximators asymptotic convergence lost update stateaction pair s change Qvalues pairs si aj si cid9 s aj cid9 In case approximation errors points si aj increase Qπ si aj subsequently target values updates 5 errors passed easily spreading entire domain Another source instability consequence fact online reinforcement learning distribution incoming data depends current policy Depending dynamics agent remain time region state space representative entire domain In situation learning algorithm allocate excessive resources function approximator represent region possibly forgetting previous stored information 85 One way alleviate interference problem use local function approximator The independent basis function severe problem limit basis function state corresponds lookuptable case 86 A class local functions widely approximation radial basis functions RBFs 52 The characteristic feature local RBFs fact value decreases monotonically distance central point called center cid8c radial function The width σ RBF determines fast value drops As ﬁrst noticed Broomhead Lowe 21 Poggio Girosi 51 radial basis function paradigm approximation structured artiﬁcial neural network In case RBFs activation functions networks hidden units It shown limit number radial functions available centers widths adapted training RBF network universal approximator 29 5 Restricted gradientdescent algorithm It possible use RBF network approximate reinforcementlearning value function In case deﬁning hidden layer network corresponds determining structure feature space linear model operate This trivial task conﬁguration RBFnetwork hidden layer requires deﬁnition number radial functions centers widths This section presents restricted gradientdescent algorithm training method local RBF networks developed context reinforcement learning This means operate online deal nonstationary data adapt network topology according complexity value function To achieve RGD algorithm relies basic mechanisms The ﬁrst discussed Section 51 strategy adopted allocate new units hidden layer Section 52 presents second ingredient RGD mechanism features valuefunction approximation determined Finally Section 53 complete RGD algorithm presented pseudocode case combined TD learning given The use local radial functions reinforcement learning means new idea See example 344 485859 cite Instead going details previous attempt use RBF networks approximate value function simply point similarities differences approach respect algorithm RGDs characteristics presented For review reader redirected Ratitchs PhD thesis 55 51 Dynamic allocation resources One decision strong impact performance RBF network number units hidden layer If units features model able represent value function necessary accuracy generate good policy On hand excessive number hidden nodes makes learning process slower potentially harming generalization capability network 460 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 It desirable able adapt topology approximating model according complexity value function approximated This especially true complete control problem value function changes time policyimprovement step performed When local RBF networks possible allocate new hidden units ondemand based pattern activity network One example place new RBFs regions state space approximation error unusually large 2750 The allocation new units based approximation error requires deﬁnition schedule threshold trigger process 59 Usually values domain speciﬁc Another idea try cover relevant areas state space equally assuring state visited agent results minimum activation level RBF network If presentation state cid8s activation network speciﬁc threshold τ new RBF added model center coincident cid8s 46551 One advantage strategy threshold τ deﬁned independently domain seen way control overlap interferencebetween RBFs The strategy advantage guarantee regular coverage state space granularity approximator dynamically increases widths RBFs decrease time This essential characteristic makes adaptive process RGD algorithm possible described section 52 Deﬁning features based approximation error As noted Sutton Barto 77 reinforcement learning structure approximator related complexity value function Different regions domain function require different levels granularity ideally approximators structure reﬂect requirement A strong indication need ﬁner granularity large approximation error However information determine network topology straightforward error function discontinuous respect number RBFs networks hidden layer Since gradient error function respect number hidden units computable try detect need ﬁner grain indirectly Particularly conventional gradientdescent method successively reduces RBFs widths new radial functions added areas state space left uncovered Given target value Qπ s obtained based 2 4 5 suppose objective minimize following weighted euclideannorm cid2 cid2 ξ1 ρε2 cid4 ρs Qπ s Qπ cid5 s 2 7 sa If εwa ρs distribution weighting errors different stateaction pairs In case incremental gradient descent algorithm change parameters local RBF distinct ways depending relationship approximation error ε RBFs output weights wa 0 width σi radial basis function enlarged center cid8ci moved current state cid8s shown Fig 1 If εwa 0 opposite changes performed RBFs width reduced center moved away current state Fig 2 A formal presentation update equations Gaussian function Appendix A The indiscriminate application updates described easily lead divergence In bootstrapping reinforcementlearning target values update Qπ depend current approximation Qfunction Notice example 4 5 computation new estimate Qπ k So source error updates error ampliﬁed iteration cumulative effect unbounded growth approximation error There evidence literature source error exaggeration Qvalues 32485680 The underlying idea combination max operator 1 inevitable imprecision estimate Qπ k1 upward In case local RBFs systematic overestimation result neverstopping increase functions widths Fig 1b In fact phenomenon observed preliminary experiments reinforcement learning RBF networks motivations development RGD algorithm k1 depends previous Qπ k likely bias Qπ 1 We use cid8s instead s want emphasize state s vector S cid5n A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 461 Original RBF b Width enlarged c Center moved current state Fig 1 Changes εwa 0 Original RBF b Width reduced c Center moved away current state Fig 2 Changes εwa 0 The idea RGD restrict modiﬁcations performed standard gradientdescent algorithm RBFs parameters If widths radial functions allowed shrink approximator granularity steadily increasing Obviously process leave areas state space uncovered compensated allocation new hidden units As reduction RBFs widths error driven resulting procedure training algorithm increases number features linear model based indirectly approximation error 53 Algorithm Algorithm 1 shows pseudocode RGD combined TD0 Notice essentially TDlearning algorithm linear approximator block code detached dotted lines The extension case λ 0 control algorithms straightforward As shown Algorithm 1 ﬁrst step RGD method check activated unit threshold τ If new RBF θm1 added network coincident current state cid8s The width σm1 new RBF determines overlap function neighbors deﬁnition τ allows control level interference functions If σm1 τ sufﬁciently small changes performed RGD algorithm hidden layer restricted activated unit θi activation level usually high The changes RBFs parameters depend εwa discussed If εwa 0 width radial function reduced changes center 0 cid8ci moved cid8s width left unaltered εwa Reducing RBFs widths desirable effects First decreases overlap functions helps maintain locality model Also reduction proportional approximation error size radial functions tends reﬂect shape value function narrower RBFs areas 462 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 loop initialize cid8s πcid8s repeat action given π cid8s cid8s2 Qπ perform action observe state cid8s2 reward r a2 πcid8s2 ε r γ Qπ compute TDerror a2 cid8wa cid8wa αε cid8θ update linear weights ﬁnd activated unit θi maxj θj cid8s θi τ cid8s allocate new hidden unit θm1 0 cid8ci cid8ci β ε2 cid8ci εwa σi σi β ε2 σi cid8ci cid8s decrease σi end cid8s cid8s2 a2 state cid8s terminal end loop Algorithm 1 RGDTD0 complex If learning rate β small moving centers radial functions direction current state tends conﬁne RBFs convexhull deﬁned training datapoints This desirable property approximators resources concentrated regions state space data lies 69 As mentioned widths RBFs deﬁned order guarantee overlap functions One way deﬁne σm1 activation new RBF θm1 equal τ center activated unit θi σm1 σ θm1cid8ci cid8cm1 σ τ 8 dependency θm1 parameters cid8cm1 σ emphasized Notice strategy width new RBF depend widths previous ones larger width neighbor θi larger σm1 threshold τ triggered cid8ci Suppose start single RBF network How deﬁne width σ1 function Clearly underestimated σ1 result excessive number hidden units ﬁnal network generated RGD As RBFs widths constantly reducing overestimated value parameter harm approximation This issue discussed Section 61 Another question arises naturally stop shrinking RBFs minimization leastsquares sense local minima generate approximation residuals directions alternate signs εwa result neverstopping decrease RBFs widths The decision stop shrinking process left user deﬁne appropriate stop criterion depending task This speciﬁc domain knowledge incorporated process example upper bound number hidden units threshold approximation error The section computational experiments different stop criteria 6 Computational experiments In section present empirical analysis RGD Four tasks evaluate different aspects algorithm In Section 61 use simple maze study general behavior RGD Particularly focus A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 463 Maze optimal policy The arrows represent sum unitlength vectors corresponding optimal actions The sym bol cid2 denote states actions valuefunction b Value functions gray map The brighter state higher value function Walls represent black squares Fig 3 Maze task c Value functions landscape The value internal walls consid ered minsa Qπ s 1 policy evaluation problem analyse main parameters RGD affect performance In Section 62 results RGD mountaincar task contrasted direct ancestor standard gradientdescent method The applied linear nonlinear models different sizes allows evaluate quality features selected RGD Finally Sections 63 64 RGD compared algorithms literature challenging control problems We use polebalancing Acrobot tasks check performance RGD evolutionary recent reinforcementlearning algorithms The conﬁguration RGD experiments discussed Appendix B 61 Maze Section 5 presented statements expected behavior RGD algorithm In section simple task verify assertions The task 25 25 maze presented Menache et al 44 shown Fig 3a The objective domain ﬁnd goal states marked G quickly possible The actions available state north south east west corresponding possible directions The arrows Fig 3a represent optimal policy π following reward function time agent performs action receives reward 05 ends goal states case gets reward 8 repositioned random state If agent runs wall limits maze dark gray squares Fig 3a remains state gets reward 05 Figs 3b 3c corresponding statevalue function2 case discount factor γ 09 The task ﬁrst experiment use RGD algorithm perform policy evaluation step given π use Algorithm 1 compute corresponding value function Qπ s In order assess quality approximation Qπ s report measure ξ1 deﬁned 7 However known small approximation errors result large deviations approximated policy π respect true π discussed Section 3 Since ﬁnal goal reinforcement learning ﬁnd good policy good valuefunction approximation deﬁned metric seen measure restore π approximate value function Qπ s The metric ξ2 deﬁned follows cid9 cid5 cid4 1 πs π s 0 πs cid9 πs cid5 πs π s ξ2 1 S δ cid2 δ cid4 s We deﬁned simple strategy control complexity models generated RGD algorithm number m units RBF network reaches limit mmax changes hidden layer 2 The value function state s deﬁned V π s maxa Qπ s 464 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 Fig 4 Results maze task σ1 30 The points correspond results achieved 1000000 transitions Averaged 50 runs ξ1 b ξ2 Fig 5 Results maze task τ 05 The points correspond results achieved 1000000 transitions Averaged 50 runs ξ1 b ξ2 new RBFs added ones model parameters changed Fig 4 shows performance RGD algorithm different values mmax τ initial width experiment σ1 30 leads activation level ﬁrst RBF greater 05 entire state space The ﬁrst thing notice analysing Fig 4 metrics ξ1 ξ2 fact correlated expect Even note intermediary value τ 05 generates best results measures The difference particularly relevant considers ξ2 τ 01 τ 09 policies π RGD vary error rate 30 40 regardless number RBFs τ 05 quality policy increases monotonically number hidden units Apparently high value τ increases interference radial functions low value decreases smoothness function computed RBF network We use value τ 05 experiment check performance RGD different values σ1 mmax If limit number hidden units smaller values initial width σ1 probably generate better results number RBFs ﬁnal networks hidden layer increase If resources limited larger widths better choice Fig 5 makes clear initial width σ1 1which generates activation level greater τ 05 state sdegrades results RGD values mmax Besides inﬂuencing widths new RBFs large σ1 increases number adaptation steps ﬁrst hidden unit provides initial approximation Qπ s But large σ1 Fig 5 shows A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 465 ξ1minimum 7505 b ξ2minimum 1507 Fig 6 Gray map representing results maze task mmax 15 The brighter square worse performance RGD algorithm The points correspond results achieved 1000000 transitions Averaged 50 runs Policy π generated agent The light gray areas rep resent states π s cid9 π s b Gray map value function generated agent c Landscape value function generated agent The value internal walls considered Qπ minsa s 1 Fig 7 Results maze task 1000000 transitions τ 05 σ1 15 difference results initial width changes 15 30 signiﬁcant Intuitively number transitions large performance RGD algorithm change σ1 threshold σ RBFs shrink right size The closer σ1 σ faster RGD ﬁnd reasonable solution The exact value σ domaindependent vary number transitions obviously number transitions ﬁxed large σ1 result poor performance RBFs widths time adjust The value ideal initial width σ depend parameter τ changes way σ1 affects widths RBFs created 8 To illustrate point performed experiments maximum number RBFs ﬁxed mmax 15 τ σ1 changed The results 1000000 transitions shown Fig 6 Notice extreme values τ σ1 result bad performance RGD algorithm In general conﬁgurations center Figs 6a 6b perform better edges Even metrics minimum exact center present best performances conﬁguration close σ1 15 τ 05 Fig 7 shows result single execution RGD algorithm τ 05 σ1 15 mmax 15 As shown Fig 7a 1000000 transitions agents policy π selects optimal action 80 state space Notice wrong choices concentrated areas concavity value function changes particularly internal walls The reason clear observes Fig 7b shows high values states close goal limits internal walls causing states opposite walls high Qvalues Even Fig 7c shows approximation Qπ s 466 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 Fig 8 Conﬁguration RBFs maze task 1000000 transitions The parameters τ 05 σ1 15 mmax 15 The contours drawn θi s τ 05 performed resulting RBF network able assimilate overall characteristic true value function Qπ s An interesting issue exactly approximation shown Fig 7c constructed It expected RBF networks structure reﬂects landscape value function larger number narrower RBFs areas value changes abruptly This expectation conﬁrmed Fig 8 shows distribution radial functions twodimensional state space run described terminated Notice RGD algorithm managed concentrate resources approximator areas surrounding goals internal walls value function complex Another point comes naturally regards susceptibility RGD presence noise In principle suspect RGD perform stochastic environments noise cause RBFs ratchet zero width We believe noise effect RGD residual errors intrinsic leastsquares approximation If stop criteria interrupt shrinking RBFs widths functions stop decreasing environment deterministic Therefore adequate stop criterion interrupt decreasing radial functions widths caused noise like deterministic case If true sensitivity RGD noise related strategy stop shrinking RBFs algorithms mechanism To test hypothesis reran experiment maze task noise added envi ronments dynamics Speciﬁcally agent selected possible directions north south east west moved right direction probability 1 η probability η noise randomly positioned 4 neighbor cells Fig 9 shows effect different levels noise performance RGD Notice mmax stop criterion presence noise affect quality approximation Actually easily ξ1 drops slightly η 04 η 05 probably new shape value function level noise η increased value function neighbor states closer actions lead neighbor cells 62 Mountain car When stop criterion interrupt adaptation RBFs RGD algorithm operates distinct stages In ﬁrst conﬁgures hidden units RBF network corresponds determining features output layer After stop criterion satisﬁed networks hidden layer kept ﬁxed RGD reduced standard gradientdescent method applied linear model An interesting question features selected RGD ﬁrst stage helpful valuefunction approximation second To investigate issue chose mountaincar task 67 In domain goal drive car valley The challenge relies fact cars engine strong pull slope facing way escape ﬁrst away goal slope inertia built carry car valley This task continuous state space state s represented position velocity car There possible actions state throttle forward throttle reverse A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 467 Fig 9 BoxandWhisker plots RGD results maze task different noise levels The values correspond approximation error achieved agent 1000000 transitions τ 05 σ1 15 mmax 15 The boxes graphics represent median lower upper quartiles The dotted lines extend extreme values Statistics computed 50 runs zero throttle The agent receives reward 1 time step moves past goal position mountain ends episode reward 0 An episode terminate agent reach goal 1000 steps Regardless previous terminated new episode started random position The equations governing implemented exactly described Sutton Barto 77 discount factor γ 1 input variables normalized lie interval 0 1 The mountain car complete control problem To deal task adopted mentioned SARSAλ algorithm eligibility traces λ 0 Appendix B details In order analyse quality features selected RGD combined SARSA algorithm standard gradientdescent method applied linear RBF networks different sizes The linear models 4 9 ﬁxed RBFs evenly distributed twodimensional statespace To guarantee comparison tween models size parameter mmax RGD algorithm assigned values recall mmax upper limit number hidden units The widths ﬁxed radial functions termined 8 activation level 05 The initial widths σ1 RGD determined 15 widths ﬁxed RBFs linear model size Based experiments section minimumactivation threshold RGD allocate new units set τ 05 To assess quality approximation constructed methods counted episode number steps taken agent escape 100 randomlyselected states set states experiments Fig 10 shows numbers RGD standard gradientdescent algorithm applied RBF networks ﬁxed hidden units Notice Fig 10a performance networks 4 ﬁxed hidden units degenerates episode 20000 observe shadowed regions representing 95 conﬁdence interval methods overlap episode 30000 With 9 ﬁxedunits networks different phenomenon occurs shown Fig 10b performance degrades episode 10000 episode 30000 starts improve The results RGD algorithm clear problem strategy position RBFs leads stable behavior Notice policy generated SARSARGD consistently keeps number steps escape 100 4 RBFs hidden layer In Section 4 mentioned indiscriminate application delta rule online gradientdescent algorithm lead divergence context reinforcement learning To verify statement repeated experiment standard gradientdescent algorithm applied RBF network tunable hidden layer The networks initialized exactly centers widths RBFs allowed adapt Table 1 shows average results achieved RGD standard gradientdescent method applied RBF networks ﬁxed adjustable RBFs The values correspond performance agent 50000 episodes Notice 4 units hidden layer RBF network adjustable units unable learn 468 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 4 RBFs b 9 RBFs Fig 10 Average number steps taken escape 100 randomlyselected initial states mountaincar task The shadowed gray regions correspond 95 conﬁdence interval computed 50 runs Table 1 Average number steps taken escape 100 randomlyselected initial states mountaincar task The values correspond results achieved 50000 episodes SARSA0 algorithm models shown The numbers left refer number hidden units RBF networks RGD algorithm value mmax Averaged 50 independent runs SD stands standard deviation 95 CI conﬁdence interval 95 probability level 4 9 Fixed RBFs Adjustable RBFs RGD σ1 085 Fixed RBFs Adjustable RBFs RGD σ1 058 Mean 17291 89069 7665 7385 20596 5204 Best 9358 89069 4933 6380 4611 4704 Worst 26930 89069 22234 9748 89069 7577 SD 6352 000 3430 945 30569 976 95 CI 1553019052 8906989069 67148616 71237647 1212329069 49335475 task performing number steps near maximum possible ﬁnal episodes runs theoretically failure escape 100 initial states result average 1000 steps The number 89069 probably reﬂects presence trivial states car able escape valley matter actions selected agent When number hidden units increased 9 performance model adjustable RBFs improves worse For 4 9 units hidden layer RGD algorithm presents results substantially better obtained standard gradientdescent algorithm This difference statistically signiﬁcant 95 probability level3 Notice worst result RGD corresponds average 22234 steps escape 100 initial states This smaller maximum number steps possible indicates convergence reasonable solution cases The experiments mountaincar task clear RGD able generate better results standard gradientdescent algorithm Why exactly happen Besides ondemand allocation new units RGD characteristics distinguish standard gradientdescent method widths radial functions allowed shrink centers current state activated unit parameters changed An interesting question features necessary interact To answer performed series experiments RGDs features turned The results shown Table 2 The ﬁrst row corresponds case RGD behaves similarly standard gradientdescent algorithm corresponds actual RGD 3 For statistical tests Students ttest utest Mann Whitney 42 depending data normally distributed A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 469 Table 2 Average number steps taken escape 100 randomly selected initial states mountaincar task The values correspond results achieved 5000 episodes SARSA0 algorithm combined RGD different conﬁgurations The parameters RGD algorithm set τ 05 σ1 058 mmax 9 Averaged 50 independent runs The columns W C U indicate restrictions conﬁguration W C refer changes performed algorithm widths center RBFs respectively The column U indicates activated unit changed iteration An means changes indiscriminately standardgradient algorithm cid3 indicates restricted changes RGD Restrictions Mean Best Worst SD 95 CI W cid3 cid3 cid3 cid3 C cid3 cid3 cid3 cid3 U cid3 cid3 cid3 cid3 1 2 3 4 5 6 7 8 54788 69566 54824 74228 7957 7252 6288 6171 5048 5581 4880 5011 4912 4814 4833 4735 89069 89069 89069 89069 16148 15616 47943 13794 39276 35072 40881 31428 2879 2260 6048 1913 4390165675 5984579287 4349266156 6551782939 71598755 66267878 46127964 5646701 The ﬁrst thing stands observing Table 2 big difference values shown ﬁrst rows ones This makes clear restriction changes RBFs widths main mechanism RGD achieves good performance When considering cases RBFs allowed shrink rows 5 8 moving centers radial functions current point good strategy notice means rows 7 8 smaller corresponding rows 5 6 Finally moving activated unit increase stability algorithm shown standard deviations Notice value drops row 5 6 7 8 row 1 2 3 4 As ﬁnal observation mentioned RGD achieved best performance features distinguish standardgradient algorithm notice row Table 2 presents best values statistics shown The differences mean shown row statistically signiﬁcant 95 level 7th row average number steps statistically identical standard RGD This indicates moving units activated strong effect ﬁnal results It important notice requires smaller number operations better choice 63 Pole balancing The polebalancing problem classic reinforcementlearning task studied authors 1645 The ob jective apply forces wheeled cart moving limited track order pole hinged cart falling At time step agent receives information position velocity cart angle cart pole angular velocity pole constitutes 4dimensional continuous state space There actions available state push cart left right force constant magnitude If pole falls past 12degree angle cart reaches boundaries track agent gets reward 1 new episode started At steps agent receives reward 0 Here use polebalancing task compare performance RGD algorithm methods literature As basis comparison chose work Moriarty Miikkulainen 47 authors evaluate performance techniques polebalancing problem including evolutionary traditional reinforcementlearning algorithms More speciﬁcally Symbiotic Adaptive NeuroEvolution algorithm SANE compared GENITOR Whitley et al 88 Adaptive Heuristic Critic AHC singlelayer 6 twolayer networks 2 lookuptable version Qlearning method Watkins Dayan 84 The discretization statespace singlelayer AHC Qlearning algorithms based work Barto et al 6 prior knowledge domain partition space 162 nonoverlapping regions The methods received continuous values state variables For exper iments adopted SARSAλ algorithm combined RGD Section 62 eligibility traces 470 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 Table 3 Results averaged 50 runs polebalancing task The episodes refer number attempts starts necessary balance pole 120000 steps starting pole straight car centered track 1layer AHC 2layer AHC Qlearning GENITOR SANE SARSARGD Episodes learn Failures Mean 232 8976 1975 1846 535 411 Best 32 3963 366 272 70 5 Worst 5003 41308 10164 7052 1910 924 SD 709 7573 1919 1396 329 237 0 4 0 0 0 0 Free parameters 162 2 35 2 162 35 40 186 29 decay rate λ 05 The initial width σ1 1 computed ﬁrst RBF activation level greater τ 05 entire state space In order fair comparison ran simulations version polebalancing task implemented exactly described Moriarty Miikkulainen 47 The task ﬁrst experiment ﬁnd network able balance pole 120000 time steps starting pole straight cart centered track Neither cart pole initial velocity A failure said occur agent able achieve goal 50000 episodes Notice task straightforward use ﬁxed exploration strategy SARSA adopted cid16greedy exploration ﬁxed cid16 detailed Appendix B Since algorithm explicit actorbut instead derives current approximation Qπ s athe way balance pole reasonable time reduce exploration rate exploratory moves easily end episode In context deﬁning decreasing schedule cid16 analysis RGDs performance difﬁcult success algorithm depend parameter4 Therefore instead gradually decreasing exploration rate simply tested performance current RBF network episode This test performed exploring learning like validation step supervised learning 62 If network able balance pole 120000 steps run terminated If learning process continued ﬁxed cid16greedy exploration Notice external criterion interrupt training process special care taken stop shrinking process RGD algorithm Table 3 shows statistics number episodes taken method balance pole Notice numbers extremely favorable RGD learning task faster algorithm presented stable behavior shown lower standard deviation These results impressive considers methods conﬁgure topology approximators instead models complexity known sufﬁcient problem Particularly 1layer AHCthe algorithm learn faster RGD averageused set features constructed based knowledge useful regions state space 6 theory makes task easier Notice models generated RGD average simpler algorithms shown number free parameters associated method AHCs adopted networks size actor critic The success RGD experiment consequence stop criterion algorithm task initialized start position checking current solution initial state corresponds verifying policy derived Qs able accomplish task This having validation set supervised learning coincident ﬁnal test set Indeed stop criterion generate highly specialized solutions able perform task restricted set states tested Even valid strategy tasks initialized way case version polebalancing problem Notice stop criterion adopted evolutionary methods outperformed RGD In experiment verify strategy stop learning process extends domains larger set initial states The agents trained experiment able balance pole speciﬁc start position pole straight car centered velocities equal zero A interesting challenge learn task 4 The strategy adopted Moriarty Miikkulainen 47 stop Qlearning algorithm clear A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 471 range initial states try generalize entire state space Our second experiment set order reproduce scenario conﬁgured exactly previous instead starting learning process state input variables randomly selected range possible values In work Moriarty Miikkulainen 47 algorithms interrupted pole balanced 120000 time steps initial random position To coherent decision decreasing exploration rate generalized stop criterion experiment process stopped network constructed RGD able balance pole 1000 steps n consecutive start positions failure This test model ﬁxed exploratory moves To verify quality solutions generated algorithm measured generalization ability 100 random initial states These values shown Table 4 number episodes taken method achieve goal The ﬁrst thing note 1 start position criterion stop RGDs learning process result specialized solutions discussed Notice strategy RGD algorithm presents reasonable learning rate bad generalization When number start positions stop criterion increased expected phenomenon occurs number episodes accomplish task increases resulting RBF networks present better generalization Notice results SARSARGD n 5 n 7 good specially considering initial states assess networks generalization represent irrecoverable situations states impossible balance pole 88 The values shown Table 4 clear RGD algorithm takes larger number episodes learn polebalancing task algorithms This true 1 initial state stop RGD Why happen The algorithms compared section divided categories according feature space work Qlearning 1layer AHC operate set features preselected based human knowledge problem The 2layer AHC GENITOR SANE extract features approximation mapping original input space higher dimensional hidden space ﬁxed size The RGD algorithm belongs category dimension hidden space learned We conjecture difference number episodes learn task related priori information given algorithm feature space Even information feature space RGD presented best generalization performance tested algorithms SARSARGD1 case algorithm clearly stopped prematurely At 95 probability level difference generalization algorithms statistically signiﬁcant 1layer AHC best Qlearning presented worst generalization When comparing RGD difference mean generalization signiﬁcant n cid4 5 Notice stop criterion RGD specially designed improve generalization principle possible come similar strategies algorithms The values shown Table 4 clearly indicate RGD stable algorithm polebalancing task Notice n cid4 3 algorithm presents low standard deviations generalization metric worst Table 4 Results averaged 50 runs polebalancing task The episodes refer number attempts necessary balance pole random initial position The generalization measured number initial states 100 randomlyselected ones trained agents able balance pole 1000 time steps The n SARSARGDn labels represent number states stop criteria number consecutive start positions agent supposed balance pole terminate episode 1layer AHC 2layer AHC Qlearning GENITOR SANE SARSARGD1 SARSARGD3 SARSARGD5 SARSARGD7 Episodes learn Mean 430 12513 2402 2578 1691 2844 6618 8455 11387 Best 80 3458 426 415 46 1102 2340 2249 2976 Worst 7373 45922 10056 12964 4461 5993 14846 23666 26851 SD 1071 9338 1903 2092 984 1367 2670 4250 5534 Generalization Mean Best Worst 50 44 41 48 48 15 50 56 58 76 76 61 81 81 61 68 74 80 2 5 13 2 1 1 16 17 40 SD 16 20 11 23 25 21 12 12 8 472 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 network 50 runs 3 conﬁgurations able balance pole 16 states tested bad compared methods Another indication RGDs stability fact failed balance pole 50 runs contrast 1layer AHC 3 failures 2layer AHC 14 failures5 Finally interesting note size RBF networks generated RGD compatible complexity models methods shown Table 3 spanning average 50 free parameters n 3 86 n 7 The conclusion RGD algorithm viable alternative problems similar polebalancing task If information statespace partition deﬁne right structure approximator methods shown upper half Table 4 expected perform similarly If information available RGD good choice A ﬁnal point mentioned performance evolutionary methods GENITOR SANE experiments polebalancing task Evolutionary algorithms particularly successful task early works 88 recently 31 references So pole balancing benchmark problem ﬁeld publication Moriarty Miikkulainens work difﬁcult versions task proposed successfully addressed 313571 In addition com parisons recent evolutionary methods traditional valuebased reinforcement learning algorithms indicate clear advantage ﬁrst second 316 However believe success fact polebalancing belongs class control problems particularly suitable optimization techniques As known search performed evolutionary algorithms based information gathered episodes This type search feasible tasks like balancing pole learning process spans large number short episodes However practical use evolutionary methods tasks episodes long example game chess The methods based temporaldifference learning hand tend sensitive aspect learning occur intra inter episodes Perhaps importantly polebalancing problem possible assess quality candidate solution failed accomplish task quality measured number steps solution balance pole Although problems similar formulation true In shortestpath problems little information gathered failure reinforcement signal ranking potential solutions longer trivial task In section present control problem characteristic present discussion concrete 64 Acrobot Underactuated mechanical systems degrees freedom actuators Examples systems clude manipulator arms diving vessels spacecrafts nonrigid body systems balancing systems unicycles dynamically stable legged robots 17 In section study Acrobot underactuated nonlinear studied control 70 machinelearning 1675 researchers The Acrobot teresting task dynamics complex yield challenging control problems simple permit complete mathematical analysis modeling In experiments simulator equations motion given 1770777 The Acrobot twolink robot arm powered elbow freeswinging shoulder It resembles gymnast swinging bar case joint corresponding gymnasts waist exert torque The goal swing gymnasts feet bar equal links fast possible reward 1 given agent time steps discounting The choice time step torque applied second joint Following Sutton 75 restricted options choices positive torque 1 Nm negative torque 1 Nm torque The statespace continuous 4dimensional 5 Following Moriarty Miikkulainen 47 failure cases included statistics Table 4 6 Gomez et al focus problem solving sequence increasingly difﬁcult versions polebalancing task solved valuefunction based methods 31 Unfortunately report measure generalization capability evolved networks dimension traditional reinforcementlearning algorithms produce competitive results 7 We 4th order RungeKutta integration time step 0005 seconds actions chosen 10 time steps reason choices given text The constants set 1775 restrict velocities links A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 473 variables representing joint positions representing joint angular velocities All episodes started stable position cid8s 0 0 0 0 terminate agent reaches goal maximum 1000 steps In simulations actions applied frequency 20 Hz contrasting usual choice 5 Hz This modiﬁcation makes task considerably harder explains larger number steps taken agents reach goal experiments compared previous results 1675 Notice Acrobot task formulated way relatively hard shortestpath problem presents characteristics discussed section In particular bad agentcontroller perform sequence actions lead goal results episode truncated arbitrary point Therefore information returned agent failed accomplish task failure signal Any optimization technique relies concept objective functionevolutionary methods includedwill problems ranking unsuccessful candidate solutions In fact successful solution luckily emerges process highly unlikely case optimization reduced random search In order check statement implemented evolutionary methods tested Acro bot task We tried conventional realcoded genetic algorithm 30 steadystate version similar GENITOR algorithm section 88 evolution strategies 1 10ES 1 10ES 15 recently shown excellent performance polebalancing task combined genetic program ming 87 In individual encoded linear weights RBF network 81 Gaussian units evenly distributed state space tried 10 different levels overlap functions detailed Besides RBFs adopted constant term As 6 linear model action candidate solution realvector length 3 82 246 The ﬁtness solution deﬁned 1000 ns ns number steps taken corresponding policy accomplish goal The genetic algorithms population 100 individuals methods interrupted 10000 evaluations carried out8 All choices standard 38 Each algorithm executed 10 times conﬁguration hidden layer resulting 100 independent runs able ﬁnd single individual capable achieving goal 1000 steps9 Given bad results achieved evolutionary methods proceeded try valuefunction based method leastsquares policy iteration algorithm LSPI 39 LSPI approximate policyiteration algorithm It extends beneﬁts leastsquares temporal difference LSTD 1820 control problem Like second makes efﬁcient use data eliminates learning parameters Unlike LSTD LSPI need model perform policy update data collected reasonable sample distribution The LSPI algorithm enjoys good convergence properties 39 applied problems 40 The ﬁrst step applying LSPI task collect data form transitions s r Usually sample trajectories generated exploratory policies 39 However random policies Acrobot task tends concentrate data equilibrium state cid8s 0 0 0 0 Sampling transitions random distribution trivial task Acrobot First variables representing links velocities bounded deﬁne interval pick samples More importantly want data concentrated relevant regions state space hard deﬁne priori In order overcome difﬁculties decided implement graphical interface let people play Acrobot Based movements Acrobot screen person choose time step torque applied like simple videogame The idea ﬁgure parts state space visited reasonable episode set baseline compare algorithms results Each person asked interact simulator heshe got familiarized systems dynamics After person played 8 This number set order number operations compatible performed methods applied task In particular evaluation takes approximately 1000 A m operations m 82 number features total cost run order magnitude 109 operations 9 Notice formulation Acrobot task deliberately designed illustrate difﬁculty applying conventional optimization techniques hard shortestpath problems The standard version problemin actions applied frequency 5 Hzis easily solved evolutionary methods 43 Notice information deﬁne ﬁtness individual ns number steps taken reach goal It course possible design elaborate ﬁtness functions 2291 requires domainspeciﬁc knowledge valuefunction based methods 474 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 Table 5 Results obtained 5 different people Acrobot simulator The values refer number steps taken swing Acrobots tip bar Averaged 10 episodes Person 1 2 3 4 5 Mean 4020 2906 3656 4534 6126 Best 302 232 299 245 391 Worst 472 630 673 651 996 SD 4916 12122 11062 10628 21768 10 episodes recorded data generated We helped 5 people resulted 21242 transitions The detailed information experiment given Table 5 Our ﬁrst attempt use data generated humans interaction directly LSPI able ﬁnd policy capable performing task dataset Thus generated larger dataset following way First bounded variables based humangenerated dataset limits intervals deﬁned minimum maximum values present data corresponding variable Then picked 10000 states cid8si evenly distributed hypercube deﬁned intervals cid8si applied 3 actions This resulted dataset 30000 transitions All results LSPI reported generated dataset10 We began experiments LSPI Gaussian RBF network 16 hidden units located 2 2 2 2 grid plus constant term Unfortunately experiments architecture successful increased granularity grid 81 RBFs We width σ hidden units tried different values parameter In particular 8 deﬁne levels overlap τ neighbor radial functions We started intuitive values τ 09 07 05 03 01 noticed smaller values generated better results tried τ 009 007 005 003 001 This resulted 10 different conﬁgurations hidden layer conﬁgurations experiments evolutionary methods LSPI executed 10 iterations iteration takes m2 30000 m A3 operations total number operations performed run order 109 Table 6 presents preliminary results LSPI Acrobot task different levels overlap RBFs Notice τ 09 τ 07 τ 05 10 runs able ﬁnd policy swing Acrobot 1000 steps This come surprise ﬁrst values expected better results τ 05 Anyway results improve signiﬁcantly smaller levels interference RBFs τ 005 01 average results obtained LSPI better achieved 4 5 people tried simulator The LSPI algorithm reaches best performance τ 005 smallest average standard deviation We conﬁguration run extra experiments LSPI described text The experiments RGD algorithm simpler deﬁne dataset We simply combined RGD SARSA0 executed online We cid16greedy exploration ﬁxed cid16 05 As previous experiment agent allowed explore learning explains large exploration rate Every time wanted check performance current RBF network ﬁxed cid16 0 corresponds greedy policy respect current valuefunction approximation All results RGD refer setting We adopted decaying learning rate starting α 103 going α 106 The number episodes performed learning deﬁned order computational cost SARSARGD compatible LSPIs Since current RGD implementation step takes approximately 8m operations considering episode takes 1000 steps set number episodes 600 16RBF case 10 We tried datasets example sampling larger number transitions described merging humangenerated data transitions uniformly sampled None resulted improvement LSPIs results A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 475 Table 6 Results LSPI Acrobot task Each run consisted 10 iterations Aver aged 10 runs τ 090 070 050 030 010 009 007 005 003 001 Mean 10000 10000 10000 9275 3533 3649 3848 3350 4224 5629 Best 1000 1000 1000 855 257 288 313 315 374 431 Worst 1000 1000 1000 1000 1000 1000 1000 343 800 1000 SD 000 000 000 7642 22760 22344 21618 1289 13322 17014 Table 7 Results obtained SARSARGD Acrobot task parameter conﬁgurations The value hidden layers learning rate β given relative α learning rate linear parameters Results averaged 10 runs mmax 16 RBFs Best Mean mmax 81 RBFs Best Mean Parameters Worst Worst SD SD β τ σ1 10 10 50 50 10 10 50 50 05 05 05 05 07 07 07 07 01α α 01α α 01α α 01α α 4712 6337 3279 3079 6472 4381 5026 4318 295 297 247 224 275 293 273 274 1000 1000 411 358 1000 1000 1000 1000 28054 31828 5625 3502 37196 29616 34343 29980 2642 2611 3004 2723 2566 5676 2545 2533 245 242 247 248 243 260 244 230 343 283 459 297 280 1000 285 282 2962 1469 7330 1673 1247 37238 1157 1504 3000 experiments 81 hidden units11 We experienced different values τ σ1 β hidden layers learning rate The results shown Table 7 The ﬁrst thing stands Table 7 fact SARSARGD able ﬁnd successful policies maximum 16 hidden units approximate Qfunction We believe strategy RGD conﬁgure hidden layer playing important role specially considering LSPI unable ﬁnd single successful solution network architecture Notice τ 05 σ1 50 results obtained SARSARGD 16RBF network competitive achieved LSPI 81 RBFs close best results human experience When mmax 81 hidden units SARSARGD consistently overcomes LSPI best results Table 5 Perhaps importantly RGD manages ﬁnd solution problem 10 runs 7 8 conﬁgurations In order reliable results reran experiments best conﬁguration algorithm averaging 50 executions LSPI applied RBF network 81 hidden units overlap τ 005 neighbor functions RGD parameters row Table 7 mmax 81 τ 07 σ1 50 β α The results shown Fig 11 Table 8 Fig 11 shows performance LSPI SARSARGD time Notice LSPI makes little progress ﬁrst iteration 5th 50 runs converge solution remains unaltered iteration At 95 conﬁdence level LSPI ﬁnd policy needs 332 steps accomplish task shown column Table 8 Contrasting LSPI RGDs results decrease monotonically episode 1500 point results bounce 300 steps The conﬁdence interval RGDs ﬁnal result 88 times wider LSPIs suggests variation algorithms behavior run Even clear advantage ﬁrst algorithm 11 The resulting numbers operations order magnitude performed 10 iterations LSPI models size order 107 109 respectively 476 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 Fig 11 Results achieved best conﬁgurations LSPI SARSARGD Acrobot task text details The shadowed gray regions correspond 95 conﬁdence interval computed 50 runs Table 8 Results achieved best conﬁgurations LSPI SARSARGD Acrobot task text details LSPIs numbers correspond results 10 iterations SARSARGD allowed run 3000 episodes values result algorithms performing number operations order magnitude 109 Averaged 50 runs LSPI SARSARGD Mean 33590 27656 Best 315 238 Worst 343 1000 SD 1211 10662 95 CI 3325433926 2470130611 second Notice Fig 11 RGDs curve crosses LSPIs episodes 600 900 corresponds second iterations LSPI episode 1200 conﬁdence intervals overlap anymore Note ﬁnal result RGD represents reduction 60 steps LSPI average As ﬁnal point mention RGD able ﬁnd policies able control Acrobot level proﬁciency skilled people tried simulator In experiments Acrobot SARSARGD algorithm able achieve better results LSPI performing roughly number operations We believe happened main reasons First way data collected algorithms different When merged SARSA RGD online algorithm actively gathers data according exploration strategy Normally greedy action respect current Qfunction chosen This results regions state space visited equivalent saying corresponding states assigned higher weights Qfunction approximation 7 In shortestpath problems characteristic crucial regions state space far path explored agent simply ignored With LSPI hard deﬁne states important usual choice uniform coverage state space Thus lot computational effort wasted approximation Qfunction regions state space visited agent practice12 However fact sufﬁcient explain superior performance SARSARGD com pared LSPI To check reran experiments SARSA standard gradientdescent algorithm instead RGD We RBF network 81 ﬁxed hiddenunits evenly distributed state space tried levels overlap functions τ 07 τ 05 All parameter values 12 One way remedy LSPI resample transitions s r iteration according current greedy policy 39 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 477 experiments RGD kept resulting online algorithm exactly exploration strategy For level interference RBFs executed SARSA 10 times None resulted policy able accomplish task Another characteristic RGD helped Acrobot task strategy deﬁne feature space way conﬁgures number basis functions model centers widths The results 16RBF networks supports hypothesis LSPI able ﬁnd single solution problem number hidden units equally spaced state space An interesting question nonlinear method Acrobot task circumstances RGD To answer performed test reran experiment let gradient descent method conﬁgure RBFs centers widths The functions initially positioned experiment tried values τ 07 05 As experiments RGD tested values hidden layers learning rate β α β 01α All remaining parameters set values We executed algorithm 10 runs conﬁguration resulted 40 independent runs Again runs resulted successful solution 7 Discussion It probably possible improve results RGD algorithms tasks studied speciﬁc conﬁgurations We prefer instead concentrate behavior algorithms tweaking use domain knowledge opinion better reproduces realworld scenario The experiments performed way shown RGD algorithm shares standard gradientdescent method desirable characteristics generality In domains tested RGD presented stable behavior able ﬁnd reasonable solutions parameter conﬁgurations The main difference RGD standard gradientdescent algorithm application delta rule In particular RGD algorithm widths RBFs allowed shrink centers current state These modiﬁcations conservative sense lead divergence parameters inﬁnity As long sufﬁciently small learning rate adopted widths radial functions asymptotically approach zero centers conﬁned convex hull deﬁned state space data Another difference RGD gradientdescent method possibility allocating new hidden units The strategy RGD add position radial functions follows basic philosophy socalled self organizing networks 2728 1 determine hidden unit closest current input vector 2 optionally knearest neighbors input vector 3 add new units ondemand according pre deﬁned insertion criterion One objective unsupervisedlearning methods dimensionality reduction 28 In principle states come agent interaction reinforcementlearning environment concentrated lowdimensional subspace original state space expect RGD restrict RBFs subspace Notice generally case RGD subjected Bellmans curse dimensionality 11 sense number RBFs approximator grow exponentially number dimensions state space 71 Kernelbased reinforcement learning We believe increasing number RBFs decreasing widths main mechanism RGD achieves good results Table 2 Although derived intuitive ideas strategy surprisingly similarities work Ormoneit Sen kernelbased reinforcement learning 48 theoretical perspective In work authors Qfunction approximated sum weighting kernels resembles local RBF network functions centered states cid8si Ormoneit Sen argue schema derive iterative update rule similar 4 prove resulting Qπ s converges probability true Qπ s number sample transitions approximation increases This desirable property authors consistency additional training data improve quality approximation Qπ s eventually leads optimal performance In reinforcement learning 478 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 context property hard establish conventional parametric approximators neural networks ﬁxed architecture trained standard gradientdescent algorithm 48 Besides simple assumptions reward transition kernel functions requirement convergence true weighting kernels shrink increasing sample size admissible rate Since kernelbased reinforcement learning kernel function sample transition statement equivalent saying widths functions decrease zero number increases Also reduction happen fast order guarantee certain degree overlap neighbor functions This similar reasoning RGD opposite direction kernelbased reinforcement learning number functions determines widths RGD algorithm opposite happens Notice having kernel sample transition feasible practice especially online learning RGDs strategy conﬁgure hidden layer regarded practical approach deﬁning kernel functions It somewhat surprising works following different lines thought come similar conclusions opinion strengthens empirical arguments More importantly opens new interesting possibilities future research We believe possible ﬁt RGD slightly modiﬁed version kernel based reinforcementlearning framework 8 Conclusions The restricted gradientdescent algorithm essentially strategy extract important features state space If information problem hand handcraft feature space certainly better choice use linear model perform valuefunction approximation On hand known domain RGD algorithm appealing alternative RGD basically modiﬁed version standard gradientdescent algorithm inherits qualities drawbacks In particular simple general wide applicability requires minimal use domain knowledge shown experiments Our algorithm presents advantages compared unrestricted form changes performed RGD conservative nonlinear parameters approximator diverge Besides RGD able conﬁgure topology RBF networks important feature situations On downside mention facts RGD makes inefﬁcient use data compared leastsquares methods like LSTD performance depends right deﬁnition parameters Also sensitive dimensionality state space meaning size RBF network usually grow exponentially dimension input space The study presented work fundamentally empirical analysis intended feasibility applying RGD algorithm reinforcementlearning benchmark tasks Speciﬁcally shown algorithm combined SARSA presents competitive results methods literature including evolutionary traditional reinforcementlearning algorithms When merged SARSA RGD online policy algorithm learning takes place agent interacts environment data collected according agents actual experience It difﬁcult think combinations rise algorithms different characteristics RGDQlearning example online offpolicy algorithm It conceivable use model environment aid learning process A theoretical analysis RGD desirable especially connections Ormoneit Sens kernelbased reinforcement learning Like RGD kernelbased learning relies local approximator open architecture cases increase number basis functions results decrease widths As theoretical framework kernelbased approach enjoys stronger convergence properties practical On hand RGD practical lacks theoretical performance guarantees We believe bridging gap beneﬁcial Acknowledgements The ﬁrst author like thank support provided Brazilian agency Coordenação Aperfeiçoa mento Pessoal Nível Superior CAPES current research possible We like thank A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 479 Helena Tati Bruninho Carol playing Acrobot simulator Finally thank reviewers Artiﬁcial Intelligence Journal suggestions great value preparation ﬁnal manuscript Appendix A Update equations Gaussian function The Gaussian function given cid10 θicid8s exp cid11 cid12cid8s cid8cicid122 σ 2 A1 cid12cid12 denotes Euclidean norm σi 0 The value function computed RBF network m Gaussian units following form given action Qπ cid8s mcid2 i1 wa θicid8s When online gradientdescent algorithm minimize 7 update rule RBFs widths cid17σi ε2 σi εwa 4θicid8s cid12 cid12cid8s cid8cicid122 σ 3 cid15 cid13cid14 cid20 It easy note nature change performed delta rule depend sign εwa terms nonnegative If εwa happens centers cid8ci ε2 cid8ci 0 width enlarged εwa 0 reduced Similar situation cid8s cid8ci εwa A2 cid17 cid8ci 4θicid8s σ 2 icid12 cid13cid14 cid15 cid20 In case εwa cid8s 0 center cid8ci moved current state cid8s εwa 0 moved away Appendix B RGD conﬁguration experiments The RBF networks generated RGD approximate value function Qπ s structure shown 6 output layer possible action The Gaussian function given A1 hidden units activation experiments We started learning process single unit networks hidden layer widths new RBFs deﬁned 8 The hidden layers learning rate β set 01α α learning rate output layer empirically β α results stable behavior RGD On experiments Acrobot tested α β discussed text The value α varied experiments As maze mountaincar tasks number steps learn focus analysis small learning rate α 104 The learning rates RBF networks ﬁxed tunable hidden units Section 62 In Section 63 hand learning performance RGD compared algorithms larger learning rate α 101 adopted On Acrobot task decaying learning rate discussed Section 64 No eligibility traces experiments polebalancing task decay rate λ 05 adopted With SARSA algorithm cid16greedy exploration strategy noted cid16 set 015 means action presenting largest value function selected 85 time remaining action picked uniformly random The value τ σ1 discussed text All parameters determined empirically based small set preliminary experiments 480 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 References 1 CW Anderson Learning problem solving multilayer connectionist systems PhD thesis Computer Information Science Uni versity Massachusetts 1986 2 CW Anderson Learning control inverted pendulum neural networks IEEE Control Systems Magazine 9 1989 3137 3 CW Anderson Qlearning hiddenunit restarting Advances Neural Information Processing Systems 1993 pp 8188 4 LC Baird Residual algorithms Reinforcement learning function approximation International Conference Machine Learning 1995 pp 3037 5 AG Barto M Duff Monte Carlo matrix inversion reinforcement learning Advances Neural Information Processing Systems vol 6 Morgan Kaufmann 1994 pp 687694 6 AG Barto RS Sutton CW Anderson Neuronlike adaptive elements solve difﬁcult learning control problems IEEE Transactions Systems Man Cybernetics 13 1983 834846 7 J Baxter P Bartlett Direct gradientbased reinforcement learning I Gradient estimation algorithms Technical report Research School Information Sciences Engineering Australian National University July 1999 8 J Baxter L Weaver P Bartlett Direct gradientbased reinforcement learning II Gradient ascent algorithms experiments Technical report Research School Information Sciences Engineering Australian National University July 1999 9 RE Bellman Dynamic Programming Princeton University Press 1957 10 RE Bellman A Markov decision process Journal Mathematical Mechanics 6 1957 679684 11 RE Bellman Adaptive Control Processes Princeton University Press 1961 12 H Benbrahim J Franklin Biped dynamic walking reinforcement learning Robotics Autonomous Systems Journal December 1997 13 DP Bertsekas Dynamic Programming Deterministic Stochastic Models PrenticeHall Inc Upper Saddle River NJ 1987 14 DP Bertsekas JN Tsitsiklis NeuroDynamic Programming Athena Scientiﬁc Belmont MA 1996 15 HG Beyer HP Schwefel Evolution strategies A comprehensive introduction Natural Computing 1 1 2002 352 16 G Boone Efﬁcient reinforcement learning Modelbased Acrobot control International Conference Robotics Automation vol 1 IEEE Robotics Automation Society Albuquerque NM 1997 pp 229234 17 G Boone Minimumtime control Acrobot International Conference Robotics Automation vol 1 IEEE Robotics Automation Society Albuquerque NM 1997 pp 32813287 18 JA Boyan Technical update Leastsquares temporal difference learning Machine Learning 49 2002 233246 19 JA Boyan AW Moore Generalization reinforcement learning Safely approximating value function Advances Neural Infor mation Processing Systems MIT Press Cambridge MA 1995 pp 369376 20 SJ Bradtke AG Barto Linear leastsquares algorithms temporal difference learning Machine Learning 22 123 1996 3357 21 DS Broomhead D Lowe Multivariable functional interpolation adaptive networks Complex Systems 2 1988 321355 22 SC Brown KM Passino Intelligent control Acrobot J Intell Robotics Syst 18 3 1997 209248 23 RH Crites AG Barto Improving elevator performance reinforcement learning Advances Neural Information Processing Sys tems vol 8 MIT Press Cambridge MA 1996 pp 10171023 24 P Dayan T Sejnowski TDλ converges probability 1 Machine Learning 14 1994 295301 25 M Dorigo M Colombetti Robot shaping Developing autonomous agents learning Artiﬁcial Intelligence 71 1994 321370 26 J Farrel T Berger On effects training sample density passive learning control American Control Conference 1995 pp 872 876 27 B Fritzke Growing cell structuresa selforganizing network unsupervised supervised learning Neural Networks 7 9 1994 1441 1460 28 B Fritzke A growing neural gas network learns topologies G Tesauro DS Touretzky TK Leen Eds Advances Neural Information Processing Systems vol 7 MIT Press Cambridge MA 1995 pp 625632 29 F Girosi T Poggio Networks best approximation property Technical Report AIM1164 Massachusetts Institute Technology Artiﬁcial Intelligence Laboratory Center Biological Information Processing Whitaker College 1989 30 D Goldberg Realcoded genetic algorithms virtual alphabets blocking Technical Report IlliGAL Report 90001 Illinois Genetic Algo rithms Laboratory Dept General EngineeringUniversity Illinois Urbana IL USA 1990 31 F Gomez J Schmidhuber R Miikkulainen Efﬁcient nonlinear control neuroevolution ECML 2006 17th European Conference Machine Learning Springer Berlin 2006 32 GJ Gordon Stable function approximation dynamic programming International Conference Machine Learning Morgan Kaufmann San Francisco CA 1995 pp 261268 33 GJ Gordon Reinforcement learning function approximation converges region Advances Neural Information Processing Systems 2000 pp 10401046 34 C Guestrin M Hauskrecht B Kveton Solving factored MDPs continuous discrete variables 20th Conference Uncertainty Artiﬁcial Intelligence 2004 35 C Igel Neuroevolution reinforcement learning evolution strategies Congress Evolutionary Computation CEC 2003 vol 4 IEEE Press 2003 pp 25882595 36 T Jaakkola MI Jordan SP Singh On convergence stochastic iterative dynamic programming algorithms Neural Computation 6 1994 37 LP Kaelbling ML Littman AP Moore Reinforcement learning A survey Journal Artiﬁcial Intelligence Research 4 1996 237285 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 481 38 M Keijzer JJ Merelo G Romero MG Schoenauer Evolving objects A general purpose evolutionary computation library Artiﬁcial Evolution 2310 2002 231242 39 MG Lagoudakis R Parr Leastsquares policy iteration Journal Machine Learning Research 4 2003 11071149 40 MG Lagoudakis R Parr ML Littman Leastsquares methods reinforcement learning control SETN 2002 pp 249260 41 LJ Lin Selfimproving reactive agents based reinforcement learning planning teaching Machine Learning 8 1992 293321 42 HB Mann DR Whitney On test 2 random variables stochastically larger Annals Mathematical Statistics 18 1947 5060 43 PH McQuesten Cultural enhancement neuroevolution PhD thesis The University Texas Austin 2002 44 I Menache S Mannor N Shimkin Basis function adaptation temporal difference reinforcement learning Annals Operations Research Special Issue Cross Entropy Method 134 2005 215238 45 D Michie R Chambers BOXES An experiment adaptivecontrol Machine Intelligence 2 1968 125133 46 JDR Millán D Posenato E Dedieu Continuousaction Qlearning Machine Learning 49 2002 247265 47 DE Moriarty R Miikkulainen Efﬁcient reinforcement learning symbiotic evolution Machine Learning 22 13 1996 1132 48 D Ormoneit S Sen Kernelbased reinforcement learning Machine Learning 49 2002 161178 49 TJ Perkins D Precup A convergent form approximate policy iteration Advances Neural Information Processing Systems vol 15 MIT Press Cambridge MA 2003 pp 15951602 50 JC Platt A resourceallocating network function interpolation Neural Computation 3 2 1991 213225 51 T Poggio F Girosi Network approximation learning Proceedings IEEE 78 9 September 1990 14811497 52 MJD Powell Radial basis functions multivariable interpolation A review JC Mason MG Cox Eds Algorithms Approxima tion Clarendon Press Oxford 1987 pp 143167 53 D Precup RS Sutton S Dasgupta Offpolicy temporaldifference learning function approximation 18th International Conference Machine Learning Morgan Kaufmann San Francisco CA 2001 pp 417424 54 ML Puterman Markov Decision ProcessesDiscrete Stochastic Dynamic Programming WileyInterscience 1994 55 B Ratitch On characteristics Markov decision processes reinforcement learning large domains PhD thesis School Computer Science McGill University Montréal 2004 56 SI Reynolds The stability general discounted reinforcement learning linear function approximation UK Workshop Computa tional Intelligence 2002 57 G Rummery M Niranjan Online qlearning connectionist systems Technical Report CUEDFINFENGTR 166 Cambridge UniversityEngineering Department 1994 58 PN Sabes Approximating Qvalues basis function representations 1993 Connectionist Models Summer School Lawrence Erlbaum Assoc Inc Hillsdale NJ 1993 59 K Samejima T Omori Adaptive internal state space construction method reinforcement learning realworld agent Neural Net works 12 1999 11431155 60 AL Samuel Some studies machine learning game checkers IBM Journal Research Development 3 1959 211229 61 AL Samuel Some studies machine learning game checkers iirecent advances IBM Journal Research Develop ment 11 1967 601617 62 W Sarle Stopped training remedies overﬁtting Proceedings 27th Symposium Interface 1995 63 R Schoknecht A Merke Convergent combinations reinforcement learning linear function approximation Advances Neural Information Processing Systems vol 15 MIT Press Cambridge MA 2003 pp 15791586 64 W Schultz P Dayan PR Montague A neural substrate prediction reward Science 275 1997 15931599 65 SP Singh D Bertsekas Reinforcement learning dynamic channel allocation cellular telephone systems Advances Neural Infor mation Processing Systems vol 9 MIT Press Cambridge MA 1997 p 974 66 SP Singh T Jaakkola ML Littman C Szepesvari Convergence results singlestep onpolicy reinforcementlearning algorithms Ma chine Learning 38 3 2000 287308 67 SP Singh RS Sutton Reinforcement learning replacing eligibility traces Machine Learning 22 13 1996 123158 68 SP Singh RC Yee An upper bound loss approximate optimalvalue functions Machine Learning 16 3 1994 227233 69 WD Smart LP Kaelbling Practical reinforcement learning continuous spaces International Conference Machine Learning 2000 pp 903910 70 MW Spong The swing control problem Acrobot IEEE Control Systems Magazine 15 1995 4955 Reprinted Neurocomputing Foundation Research 71 KO Stanley R Miikkulainen Efﬁcient reinforcement learning evolving neural network topologies GECCO 2002 Genetic Evolutionary Computation Conference Morgan Kaufmann New York 2002 pp 569577 72 P Stone RS Sutton Scaling reinforcement learning RoboCup soccer 18th International Conference Machine Learning Morgan Kaufmann San Francisco CA 2001 pp 537544 73 R Sutton D McAllester S Singh Y Mansour Policy gradient methods reinforcement learning function approximation Advances Neural Information Processing Systems 2000 pp 10571063 74 RS Sutton Learning predict methods temporal differences Machine Learning 3 1988 944 75 RS Sutton Generalization reinforcement learning Successful examples sparse coarse coding Advances Neural Information Processing Systems vol 8 MIT Press Cambridge MA 1996 pp 10381044 76 RS Sutton AG Barto Timederivative models Pavlovian reinforcement Learning Computational Neuroscience Foundations Adaptive Networks MIT Press Cambridge MA 1990 pp 497537 77 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press Cambridge MA 1998 A Bradford Book 482 A da Motta Salles Barreto CW Anderson Artiﬁcial Intelligence 172 2008 454482 78 V Tadic On convergence temporaldifference learning linear function approximation Machine Learning 42 3 2001 241267 79 GJ Tesauro TDGammon selfteaching backgammon program achieves masterlevel play Neural Computation 6 2 1994 215219 80 S Thrun A Schwartz Issues function approximation reinforcement learning Fourth Connectionist Models Summer School Lawrence Erlbaum Associates Hillsdale NJ 1993 81 JN Tsitsiklis B Van Roy Featurebased methods large scale dynamic programming Machine Learning 22 1996 5994 82 JN Tsitsiklis B Van Roy An analysis temporaldifference learning function approximation IEEE Transactions Automatic Con trol 42 May 1997 674690 83 C Watkins Learning delayed rewards PhD thesis University Cambridge England 1989 84 C Watkins PD Dayan Qlearning Machine Learning 8 1992 279292 85 SE Weaver LC Baird MM Polycarpou Preventing unlearning online training feedforward networks International Sympo sium Intelligent Control Gaithersburg 1998 pp 359364 86 DA White DA Sofge Handbook Intelligence Control Neural Fuzzy Adaptive Approaches Van Nostrand Reinhold New York 1992 Chapter Applied learning Optimal control manufacturing 87 D Whitley M Richards R Beveridge A da Motta Salles Barreto Alternative evolutionary algorithms evolving programs Evolution strategies steadystate GP Proceedings 8th Annual Conference Genetic Evolutionary Computation GECCO 2006 vol 1 ACM Press Seattle Washington 2006 pp 919926 Winner best GP paper 88 D Whitley S Dominic R Das CW Anderson Genetic reinforcement learning neurocontrol problems Machine Learning 13 23 1993 259284 89 RJ Williams LC Baird Tight performance bounds greedy policies based imperfect value functions Technical Report NUCCS9314 Northeastern University November 1993 90 W Zhang TG Dietterich A reinforcement learning approach jobshop scheduling International Joint Conference Artiﬁcial Intelli gence 1995 91 DZ Zhao J Yi GAbased control swing Acrobot limited torque Transactions Institute Measurement Control 28 1 2006 313