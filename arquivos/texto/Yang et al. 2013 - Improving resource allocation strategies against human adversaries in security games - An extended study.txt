Artiﬁcial Intelligence 195 2013 440469 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Improving resource allocation strategies human adversaries security games An extended study Rong Yang Christopher Kiekintveld b Fernando Ordóñez ac Milind Tambe Richard John University Southern California Los Angeles CA USA b University Texas El Paso El Paso TX USA c University Chile Santiago Chile r t c l e n f o b s t r c t Article history Received 16 December 2011 Received revised form 13 November 2012 Accepted 15 November 2012 Available online 20 November 2012 Keywords Bounded rationality Stackelberg games Decisionmaking Stackelberg games garnered signiﬁcant attention recent years given deploy ment real world security Most systems ARMOR IRIS GUARDS adopted standard gametheoretical assumption adversaries perfectly ra tional standard game theory literature This assumption hold realworld security problems bounded rationality human adversaries potentially reduce effectiveness systems In paper focus relaxing unrealistic assumption perfectly rational adver sary Stackelberg security games In particular present new mathematical models human adversaries behavior based fundamental theorymethod human decision making Prospect Theory PT stochastic discrete choice model We pro vide methods tuning parameters new models Additionally propose modiﬁcation standard quantal response based model inspired rankdependent expected utility theory We develop eﬃcient algorithms compute best response security forces playing different models adversaries In order evaluate effectiveness new models conduct comprehensive experiments human subjects webbased game comparing models previously proposed literature address perfect rationality assumption ad versary Our experimental results subjects responses follow assumptions new models closely previous perfect rationality assumption We defender strategy produced new stochastic discrete choice model outperform previous leading contender relaxing assumption perfect rational ity Furthermore separate set experiments beneﬁts modiﬁed stochastic model QRRU standard model QR1 2012 Elsevier BV All rights reserved Corresponding author Email address yangronguscedu R Yang 1 This paper signiﬁcantly extends previous conference paper Yang et al 2011 1 providing new methods setting parameters Prospect Theory model ii additional variant Quantal Response model new algorithm compute defender strategies new model iii comprehensive set experiments includes multiple new algorithms updated settings algorithms iv new analysis robustness different defender strategies predictive accuracy different models v additional discussion related work 00043702 matter 2012 Elsevier BV All rights reserved httpdxdoiorg101016jartint201211004 R Yang et al Artiﬁcial Intelligence 195 2013 440469 441 1 Introduction Stackelberg game models recently important tools analyzing realworld security resource allocation problems critical infrastructure protection 2 robot patrolling strategies 34 These models provide sophisti cated approach generating unpredictable randomized strategies mitigate ability attackers ﬁnd weaknesses surveillance The ARMOR 5 IRIS 6 GUARDS 7 PROTECT 8 systems notable examples approach develop decisionsupport systems realworld security problems One key sets assumptions systems attackers choose attack strategies based preferences observations security policy Typically systems applied standard gametheoretic assumption attackers fectly rational strictly maximize expected utility This reasonable starting point ﬁrst generation deployed systems Unfortunately standard gametheoretic assumption leaves open possibility defenders strategy robust attackers different decision procedures fails exploit known weaknesses decisionmaking human attackers It widely accepted standard gametheoretic assumptions perfect rationality ideal predicting behavior humans multiagent decision problems 910 A large variety alternative models studied behavioral game theory cognitive psychology capture deviations human decisions perfect rationality In multiagent systems community growing adopting models improve decisions agents interact humans provide better advice human decisionmakers multiagent decisionsupport systems 1112 Our work paper focuses integrating realistic models human behavior computational analysis Stackelberg game models security settings referred Stackelberg security games 1315 We provide case study general paradigm introducing realistic models human behavior game theoretic analysis While studies looking problem predicting human behavior examples actually included real decisionmaking Our work ﬁrst examples showing possible actually improves performance important class games In order perfect rationality assumptions integrate realistic models human decisionmaking realworld security systems address key challenges First literature introduced multitude potential models human decision making 1691710 models set assumptions little consensus model best different types domains Therefore important empirical question model best represents salient features human behavior important class applied security games Sec ond integrating proposed models decisionsupport purpose empirically evaluating model requires developing new algorithms computing solutions Stackelberg security games existing algorithms based mathematically optimal attackers 1819 One notable exception Cobra developed Pita et al 20 Cobra example modeling bounded rationality human adversaries taking account anchoring bias humans interpreting probabilities events 2122 ii limited computational ability humans lead deviation best response To best knowledge Cobra best performing strategy Stackelberg security games experiments human subjects Thus open question approaches allow fast solutions outperform Cobra addressing human behavior security games In paper signiﬁcantly expand previous work modeling human behavior Stackelberg security games implementing evaluating strategies based important methods literature modeling human decision making The ﬁrst relates Prospect Theory PT provides descriptive framework decisionmaking certainty accounts risk preferences loss aversion variations humans interpret probabilities weighting function 16 The method adapts ideas literature discrete choice problems 2326 gametheoretic framework basic premise humans choose better actions frequently noise decisionmaking process leads stochastic choice probabilities following logit distribution We ﬁrst propose mathematical models adversarys decisionmaking based Prospect Theory assumes adversary maximizes prospect decision making process assumes adversary makes bounded error computing prospect deviate suboptimal solution bound We propose mathemat ical models adversary makes decisions based logit discrete choice models One model QR couples quantal response adversary expected utility attacking target model QRRU modiﬁes expected utility adding extra weight target covered minimum resources inspired rankdependent expected utility theory 27 Based models adversary decision making computing defenders corresponding best response challenging involves solving nonconvex nonlinear optimization problems We develop new techniques address problems In particular develop Mixed Integer Linear Program compute defender optimal strategy PT based models representing nonlinear functions Prospect Theory piecewise approximations Furthermore present local search method random restarts compute defender optimal strategy stochastic models adversary 442 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table 1 Notations paper T xi qi Rd P d Ra P M Set targets ti T denotes ith target Probability target ti covered resource Probability target ti attacked adversary Defender reward covering ti attacked Defender penalty covering ti attack Attacker reward attacking ti covered Attacker penalty attacking ti covered Total number resources In order compare performance different adversary models conduct extensive empirical evaluation crowdsource platform Amazon Mechanical Turk2 AMT First design online game called The Guard Treasure simulate security scenario similar ARMOR program Los Angeles International LAX airport 5 We develop classiﬁcation techniques select payoff structures experiments models separated payoff structures representative game space We compare new methods robust baseline algorithm MAXIMIN perfect rationality baseline Dobss previous leading contender Cobra experiments Our experimental results new models accurately represent adversaries behavior security games previous methods ii strategies based new models lead statistically practically signiﬁcant higher defender expected utility previous leading contender Cobra Moreover identify situations QRRU model adversary leads signiﬁcantly better strategies QR model The rest paper organized follows Section 2 provides necessary background information Stackelberg security games deﬁnes notation paper Section 3 presents new models adversary decision making based Prospect Theory Quantal Response Equilibrium Following Section 4 describes algorithms developed compute optimal defender strategy new adversary models In Section 5 explain methods decide parameters different models Section 6 presents experimental setup results We discuss additional related work Section 7 summarize paper Section 8 2 Stackelberg security games In section ﬁrst deﬁne Stackelberg security games notation paper We introduce online game designed testbed collect data evaluate performance different algorithms introduced paper solving Stackelberg security games 21 Deﬁnition notation We consider Stackelberg Security Game SSG 128 single leader follower defender plays role leader adversary plays role follower The defender protect set targets attacked adversary The defender limited number resources need protect 8 targets 3 guards Each player set pure strategies In SSGs pure strategy adversary deﬁned attacking single target pure strategy defender deﬁned assignment security resources set targets assigning resources targets 1 3 6 An assignment security resource target referred covering target A mixedstrategy deﬁned probability distribution set possible pure strategies We use following notation SSG listed Table 1 defender total M resources protect set targets T ti The outcomes SSG depend attack successful Given target ti defender receives reward Rd adversary attacks target covered defender defender receives penalty P d A key property SSG games nonzerosum Rd resources cover target helps defender hurts attacker case reward Ra P P d case 28 In words adding Correspondingly attacker receives penalty P Ra We represent defenders mixedstrategy x describes probability target protected resource denote individual probabilities xi So x cid4xicid5 marginal distribution target In example defender protect 8 targets 3 resources guards defenders mixedstrategy written x cid4x1 x8cid5 We focus generating marginal distributions distributions original defender pure strategies original pure strategies improved algorithmic eﬃciency 1929 In paper consider case constraints assigning resources models important domains ARMOR deployed LAX 5 Korzhyk et al 29 marginal probability distribution covering target equivalent mixedstrategy original combinational defender pure strategies domains Moreover given marginal coverage target use technique called comb sampling 30 implement corresponding mixed strategy set actual assignments resources cid2 8 3 cid3 2 httpswwwmturkcom R Yang et al Artiﬁcial Intelligence 195 2013 440469 443 Fig 1 Game interface simulated online SSG In SSGs defender leader ﬁrst commits mixedstrategy assuming attacker follower decides pure strategy observing defenders strategy This models situation attacker conducts surveillance learn defenders mixedstrategy launches attack single target We denote attackers choice vector variables q cid4qicid5 ti T qi 0 1 represents probability target ti attacked Furthermore compute expected utility adversary assuming target ti attacked adversary xi xi P U 1 xiRa expected utility defender case xi xi Rd U d 1 xiP d 22 A simulated online SSG 1 2 We develop game called The Guards The Treasure simulate security model LAX airport terminals targeted attack 5 Fig1 shows interface game Players introduced game series explanatory screens describing game played In game instance subject asked choose gates open attack They told guards protecting gates ones The defenders mixed strategy represented marginal probability covering target cid4xicid5 given subjects At time subjects told reward successfully attacking target penalty getting caught target The gates protected guards drawn randomly probability shown game interface If subjects select gate protected guards receive penalty receive reward Subjects rewarded based rewardpenalty shown gate For example game shown Fig 1 probability gate 1 target 1 protected guard 059 Assuming subjects choose gate 1 heshe gets reward 8 gate 1 protected guard penalty 3 gate 1 protected guard 3 New models predicting attacker behaviors Existing models adversary behavior SSGs poor performance predicting behavior human adversaries 20 In order design better defender strategy better models adversary decisionmaking need developed In section present models adversarys behavior SSGs based Prospect Theory Quantal Response Equilibrium All models key parameters We section methodology setting parameters case 444 R Yang et al Artiﬁcial Intelligence 195 2013 440469 31 Prospect Theory Fig 2 Prospect Theory empirical function forms Prospect Theory provides descriptive model humans decision alternatives risk process maximizing prospect deﬁned soon expected utility More formally prospect certain alternative deﬁned cid4 l π xlV Cl 3 In Eq 3 xl denotes probability receiving Cl outcome The weighting function π describes proba bility xl perceived individuals An empirical function form π Eq 4 shown Fig 2a 31 π x xγ xγ 1 xγ 1 γ 4 The key concepts weighting function individuals overestimate low probability underestimate high proba bility 1631 Also π consistent deﬁnition probability π x π 1 x cid2 1 general The value function V Cl Eq 3 reﬂects value outcome Cl PT predicts individuals risk averse gain risk seeking loss implying Sshaped value function 1631 A key component Prospect Theory reference point Outcomes lower reference point considered loss higher gain cid5 V C C α C cid3 0 θCβ C 0 5 Eq 5 general form value function C relative outcome reference In Eq 5 assume reference point 0 α β determine extent nonlinearity curves If parameters α 10 β 10 function linear typical values α β 088 31 θ captures idea loss curve usually steeper gains curve typical value θ 225 31 reﬂects ﬁnding losses little twice painful gains pleasurable The function displayed Fig 2b 31 Given parameters henceforth denote value function V αβθ In SSG prospect attacking target ti adversary computed cid3 cid2 cid3 cid2 prospectti π xiV αβθ P π 1 xiV αβθ Ra According Prospect Theory subjects choose target highest prospect Thus cid5 qi prospectti cid3 prospectticid7 ticid7 T 1 0 6 7 32 Quantal Response Quantal Response Equilibrium QRE important solution concept behavioral game theory 17 It based long history work singleagent problems brings work gametheoretic setting 3233 It assumes instead strictly maximizing utility individuals respond stochastically games chance selecting nonoptimal R Yang et al Artiﬁcial Intelligence 195 2013 440469 445 strategy increases cost error decreases Given strategy proﬁle players response player modeled quantal response QR model heshe selects action probability given qix cid6 eλU x tkT eλU k x 8 x expected utility attacker selecting pure strategy Here λ 0 parameter U captures rational level player p extreme case λ 0 player p plays uniformly random extreme case λ quantal response identical best response Combining Eqs 8 1 qix cid6 P eλRa e tkT eλRa k e λRa λRa k xi P k xk 9 In applying QR model security game domain consider noise response adversary The defender uses decision support choose strategy able compute optimal strategy On hand attacker observes defenders strategy ﬁrst decides response hurt defender add noise response Recent work 33 shows Quantal Levelk 32 best suited predicting human behavior simultaneous games The key idea levelk humans perform bounded number iterations strategic reasoning level0 player plays randomly levelk k 1 player best response levelk 1 player We applied QR instead Quantal Levelk model attackers response Stackelberg security games attacker observes defenders strategy levelk reasoning applicable 33 Quantal Response Rankrelated Expected Utility We modify Quantal Response Model taking consideration fact individuals attracted extreme events uncertain highest payoff This idea inspired rankdependent Expected Utility Model 27 utilities choosing different alternatives based ranks We adapt idea security games consider effect target covered minimum resources That adversary prefer target covered minimum resources likely successful attacking target This signiﬁcantly reduce defenders reward case target fewest resources gives large penalty defender We modify QR model adding extra weight target covered minimum resources We refer modiﬁed model Quantal Response Rankrelated expected Utility QRRU model probability attacker attacks target ti computed qix cid6 xi eλs S x eλu U tkT eλu U k xkeλs Skx S ix 0 1 indicating ti covered resource cid5 S ix cid7 ticid7 T xi cid2 x 1 0 10 11 The denominator Eq 10 normalizing probability distribution qi sum 1 In numerator terms deciding probability target ti chosen adversary The ﬁrst term eλu U xi relates xi computed Eq 1 The parameter λu cid3 0 represents expected utility adversary choose target ti U level error adversarys computation expected utility equivalent λ Eq 8 The second term eλs S x relates adversarys preference covered target Note ti covered minimum resource term equals 1 extra weight added nonminimum covered targets ti covered minimum resource term cid3 1 adding extra weight probability adversary choose ti The parameter λs cid3 0 represents level adversarys preference minimum covered target λs 0 indicates preference minimum covered target As λs increase preference stronger 4 Computing optimal defender strategy Given new models adversary behavior SSG new algorithms need developed compute optimal fender strategy existing algorithms based assumption perfectly rational adversary We eﬃcient computation optimal defender mixed strategy assuming human adversary response follows models proposed Prospect Theory PTAdversary Quantal Response QRAdversary Quantal Response Rankrelated Utility QRRUAdversary 446 R Yang et al Artiﬁcial Intelligence 195 2013 440469 41 Computing PTadversary Assuming adversarys response follows Prospect Theory PTadversary developed methods compute optimal defender strategy 411 Brpt Best Response Prospect Theory Brpt mixed integer programming formulation computing optimal leader strategy players responses follow PT model We ﬁrst present abstract version formulation Brpt Eqs 1216 present detailed operational version Eqs 1729 uses piecewise linear approximation provide Brpt MILP Mixed Integer Linear Program max xqadz d st ncid4 i1 ncid4 xi cid2 M qi 1 cid2 i1 0 cid2 K 1 qi qi 0 1 cid2 cid3 cid2 cid3cid3 π xiV cid2 π 1 xiV P 1 xiP d Ra cid3 d cid3 xi Rd cid2 K 1 qi 12 13 14 15 16 The objective maximize d defenders expected utility Eq 13 enforces constraint total resources met In Eq 14 integer variables qi represent attackers pure strategy In Brpt qi constrained binary variable justiﬁed explained 18 assume adversary pure strategy best response qi 1 ti attacked 0 Eq 15 key decide attackers strategy given defenders mixed strategy x cid4xicid5 The variable represents attackers beneﬁt choosing pure strategy cid4qicid5 Since modeling attackers decision attackers prospect making Prospect Theory beneﬁt perceived adversary attacking target ti calculated π xiV P following Eq 3 The attacker tries maximize choosing target highest prospect enforced Eq 15 In particular inequality left Eq 15 enforces greater equal prospect attacking target On right hand Eq 15 constant parameter K large positive value For targets qi 0 upper bound difference prospect K bounds operational For target qi 1 target chosen attacker value forced equal actual prospect attacking target In Eq 16 constant parameter K enforces d constrained target attacked adversary qi 1 π 1 xiV Ra Ra cid7 V P We present Brpt MILP based piecewise linear approximation weighting function discussed earlier We use empirical functions introduced Section 31 weighting function π value function V Let P given input optimization formula Eqs 1316 The key challenge solve optimization problem π function nonlinear nonconvex If apply function directly solve nonlinear nonconvex mixedinteger optimization problem diﬃcult Therefore approximately solve problem representing nonlinear π function piecewise linear function This transforms problem MILP shown Eqs 1729 denote adversarys value penalty P reward Ra cid7 V Ra max xqadz d ncid4 5cid4 st xik cid2 M k1 i1 5cid4 xik xik 1 k1 0 cid2 xik xik cid2 ck ck1 k 1 5 zik ck ck1 cid2 xik k 1 4 zik ck ck1 cid2 xik k 1 4 xik1 cid2 zik k 1 4 xik1 cid2 zik k 1 4 zik zik 0 1 k 1 4 17 18 19 20 21 22 23 24 25 R Yang et al Artiﬁcial Intelligence 195 2013 440469 447 Fig 3 Piecewise approximation weighting function cid7 x 5cid4 k1 bkxik cid7 x 5cid4 k1 xik bk ncid4 qi 1 i1 0 cid2 cid2 cid2 cid7 x qi 0 1 cid3cid7 x P 5cid4 cid2 cid2 cid7 Ra M1 qi xik Rd xik P d cid3 cid3 d cid3cid7cid3 cid2 M1 qi 26 27 28 29 k1 Let π denote use piecewise linear approximation weighting function π shown Fig 3 We em pirically set 5 segments3 π This function deﬁned ckc0 0 c5 1 ck ck1 k 0 5 represent endpoints linear segments bk k 1 5 represent slope linear segment In order represent piecewise linear approximation π xi simultaneously π 1 xi partition xi 1 xi ﬁve cid7 segments denoted variables xik xik Therefore x equals π xi calculated sum linear function segment cid7 x π xi 5cid4 k1 bk xik shown Eq 26 At time enforce correctness partitioning xi 1 xi ensuring segment xik xik positive previous segment completely This enforced Eqs 1925 auxiliary integer variable zik zik zik 0 indicates kth segment xi xik completely cid7 π xi value following segments set 0 vice versa Eq 26 deﬁnes x cid7 piecewise linear approximation xi x π 1 xi value piecewise linear approximation 1 xi 412 Rpt RobustPT Rpt modiﬁes base Brpt method account possible uncertainty adversarys choice caused example imprecise computations 34 Similar Cobra Rpt assumes adversary choose strategy cid8 best choice deﬁned prospect action It optimizes worstcase outcome defender set strategies prospect attacker cid8 optimal prospect max xhqadz d st Constraints 1828 30 3 This piecewise linear representation π achieves small approximation error supz01 cid10π z π zcid10 cid2 003 448 R Yang et al Artiﬁcial Intelligence 195 2013 440469 ncid4 hi cid3 1 i1 hi 0 1 cid81 hi cid2 5cid4 cid2 M1 hi qi cid2 hi cid3cid7 x cid2 P cid7 x cid2 cid7 cid2 xik Rd xik P d Ra cid3 cid3cid7cid3 cid2 M1 hi cid8 cid3 d 31 32 33 34 k1 We modify Brpt optimization problem follows ﬁrst 11 constraints equivalent Brpt Eq 18 28 Eq 31 binary variable hi indicates cid8optimal strategy adversary cid8optimal assumption embedded Eq 33 forces hi 1 target ti leads prospect cid8 optimal prospect Eq 34 enforces d minimum expected utility defender targets lead cid8optimal prospect attacker Rpt attempts maximize minimum defender cid8optimal targets attacker providing robustness attacker human deviations cid8optimal set targets 42 Computing optimal strategy Quantal Response adversary Assuming adversary follows quantal response QRadversary present algorithm compute fenders optimal strategy QRadversary Given quantal response adversary described Eq 9 best response defender maximize expected utility U dx max x ncid4 i1 qixU d x Combined Eqs 9 2 problem ﬁnding optimal mixed strategy defender formulated cid6 max x P λRa ti T eλRa e cid6 tkT eλRa k e P d xi Rd P λRa k xk k xi P d ncid4 xi cid2 M st i1 0 cid2 xi cid2 1 j Algorithm 1 Brqr 1 opt g 2 1 IterN 3 4 5 6 7 8 end 9 return opt g xopt opt g end optl xopt x x0 randomly generate feasible starting point FindLocalMinimumx0 optl x opt g optl 35 36 37 Unfortunately objective function Eq 35 nonlinear nonconvex ﬁnding global optimum ex tremely diﬃcult Therefore focus methods ﬁnd local optima To compute approximately optimal strategy QRadversary eﬃciently develop Best Response Quantal Response Brqr heuristic described Algorithm 1 We ﬁrst negative Eq 35 converting maximization problem minimization problem In iteration ﬁnd local minimum fmincon function Matlab Interior Point Algorithm given starting point If multiple local minima randomly setting starting point iteration algorithm reach different local minima nonzero probability By increasing iteration number IterN probability reaching global minimum increases We empirically set IterN 300 experiments 43 Computing QRRUadversary We present algorithm compute defender optimal strategy assuming adversarys behavior follows QRRU model The adversarys response given model computed Eq 10 The optimal defender strategy QRRUadversary computed solving following optimization problem R Yang et al Artiﬁcial Intelligence 195 2013 440469 cid6 max xsxmin P λuRa ti T eλu Ra e cid6 tkT eλu Ra k e st Constraint 36 37 P d xi eλs si Rd λuRa k xk eλs sk k P xi P d xi 1 siK cid2 xmin cid2 xi ti T cid4 si 1 ti T si 0 1 ti T 449 38 39 40 41 integer variables si introduced represent function S ix shown Eq 11 In constraint 39 K constant large value Constraints 39 40 enforces xmin minimum value xi Simultaneously si set 1 target ti minimum coverage probability assigned set 0 The optimization problem nonlinear nonconvex mixed integer programming problem diﬃcult solve directly Therefore developed Best Response QRRUAdversary Brqrru algorithm iteratively computes defenders optimal strategy The iterative approach breaks mixedinteger nonlinear programming problem subproblems integer variables For subproblem target assumed covered target Then constraint maximum defender expected utility associated defender mixed strategy computed solving nonlinear programming problem similar Brqr Finally subproblem generating highest maximum defender expected utility actual optimal solution associated defender mixedstrategy optimal defender strategy assuming QRRUadversary FindOptimalDefenderStrategysicid7 1 Algorithm 2 Brqrru 1 opt g 2 ticid7 T optl x 3 opt g optl 4 5 6 7 end 8 return optg xopt opt g end optl xopt x Algorithm 2 shows pseudo code algorithm Algorithm 2 describes Brqrru In iteration target ticid7 conditioned covered minimum resource si 1 This reduces optimization problem following cid6 max x P λuRa ti T eλu Ra e cid6 tkT eλu Ra k e P d xi eλs si Rd λuRa k xk eλs sk k P xi P d st Constraint 36 37 xi cid2 xi ti T 42 43 integer variables involved si ti T predeﬁned parameters optimization problem Therefore solve method local search random restart Brqr Find Optimal Defender Strategysicid7 1 Line 3 Algorithm 2 calls Algorithm 1 solve optimization problem Eqs 4243 5 Parameter estimation In section methodology setting values parameters different models human behavior introduced previous section We set parameters later experiments data collected preliminary set experiments human subjects playing online game introduced Section 22 We posted game Amazon Mechanical Turk Human Intelligent Task HIT asked subjects play game Subjects played role adversary able observe defenders mixed strategy randomized allocation security resources In order avoid noncompliant participants allowed workers HIT approval rates greater 95 100 approved HITs participate experiment Let G denote game instance combination payoff structure Ra ti T defenders P T We include seven payoff structures strategy x Given game instance G denote choice jth subject τ G j experiments selected based classiﬁcation method explain Section 51 taken directly Pita et al 20 For payoff structure tested ﬁve different defender strategies This results 7 5 35 different game instances Each subjects played 35 games In total 80 subjects participated preliminary experiment P d Rd 450 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table 2 Apriori deﬁned features Feature 1 mean Ra P Feature 5 mean Ra P d Feature 2 std Ra P Feature 6 std Ra P d Feature 3 mean Rd P d Feature 7 mean Rd P Feature 4 std Rd P d Feature 8 std Rd P 51 Selecting payoff structures Fig 4 Payoff structure clusters color web version Even restricted class games security games inﬁnite number possible game instances depending speciﬁc values payoffs targets Since conduct experiments possible game instance need method select set payoffs structures use experiments Our main criteria selecting payoffs structures 1 select diverse set payoff structures cover different regions space possible security games 2 select payoff structures differentiate different behavioral models words models different predictions different test conditions In ﬁrst round goal select game instance distinguish key families prediction methods Brpt Rpt Brqr In second round selection need differentiate families Since wellunderstood method select game instances literature introduce procedure making selections P d 1 10 P We ﬁrst sample randomly 1000 different payoff structures 8 targets Ra integers drawn 10 1 This scale similar payoff structures 20 Z We use kmeans clustering group 1000 payoff structures clusters based features deﬁned Table 2 Intuitively features 1 2 good game adversary features 3 4 good game defender features 58 reﬂect level conﬂict players sense measure ratio players gain players loss integers drawn Z Rd In Fig 4 1000 payoff structures projected ﬁrst Principal Component Analysis PCA dimensions visualization The payoff structures 57 ﬁrst Pita et al 20 marked Fig 4 All payoff structures belong cluster 3 indicating game instances previous experiments similar terms features classiﬁcation4 To select speciﬁc payoff structures clusters ﬁrst generated ﬁve defender strategies based following families algorithms Dobss Cobra Brpt Rpt Brqr Here select algorithm family version Brqr At point preliminary data set parameters algorithms deciding payoff structures test Instead set parameters follows Dobss parameters Cobra 4 In 20 payoff structures use The fourth payoff structure zerosum game deployed Stackelberg security games zero sum 56 Furthermore zerosum games defenders strategies computed Dobss Cobra Maximin collapse turn identical R Yang et al Artiﬁcial Intelligence 195 2013 440469 451 use parameters drawn 20 Brpt Rpt use empirical parameter settings Prospect Theory 31 Brqr uses value λ 076 set data reported 20 method described Section 53 We use following criteria select payoff structures differentiate different families algorithms We deﬁne distance mixed strategies xk xl KullbackLeibler divergence Dxk xl xl For payoff structure Dxk xl measured pair strategies With ﬁve strategies 10 DKLxkxl DKLxlxk DKLxkxl cid6 n i1 xk logxk measurements We remove payoff structures mean minimum 10 quantities given threshold This results subset 250 payoff structures total clusters We select payoff structure closest cluster center subsets The payoff structures 14 selected different clusters marked Fig 4 52 Parameter estimation Prospect Theory An empirical setting parameter values suggested literature 31 based experiments conducted human subjects We include setting parameter values experiments evaluate benchmark performance prospect theory At time provide method estimate parameter values PT model set empirical response data collected SSG domain In section method estimating parameter values based grid search The empirical functions PT model adversary parameters speciﬁed α β θ γ shown Eqs 4 5 Varying values parameters change responses predicted PTmodel We denote weighting value function πγ V αβθ given set parameter values We deﬁne ﬁt parameter setting given data set subjects choices percentage subjects choose target predicted model The ﬁt computed Fitα β θ γ G 1 N cid4 j1N qτ G j α β θ γ G cid4 ti T Ni N qiα β θ γ G qi 0 1 indicates PT model predicts target ti chosen subjects computed Eq 7 Ni number subjects choose target ti N ti T Ni total number subjects We estimate parameter setting best ﬁt PT model maximizing ﬁt function 35 game cid6 instances max αβθγ cid4 G Fitα β θ γ G st 0 α β 1 θ cid3 1 0 γ 1 44 45 The constraints 45 restrict feasible range parameters deﬁned prospect theory model The objective function Eq 44 expressed closedform expression α β θ γ Without closed form diﬃcult apply gradient descent analytical search algorithm ﬁnd optimal solution Therefore use grid search 3536 solve problem follows 1 We ﬁrst uniformly sample set values parameter feasible ranges following grid intervals cid10α 005 cid10β 005 cid10γ 005 cid10θ 01 This gives set different values parameters For αl k1 cid10α αl lower simplicity represents sets sampled values following αk1 γl k4 cid10γ The feasible region θ θl k3 cid10θ γk4 bound region similarly βk2 upper bound set 5 twice suggested empirical value 31 βl k2 cid10β θk3 2 In total 20 20 20 40 320k different combinations parameter values We evaluate objective function combinations αk1 βk2 θk3 γk4 parameter combination best aggregate ﬁt solution cid2 α β θ γ cid3 arg max k1k2k3k4 cid4 G Fitαk1 βk2 θk3 γk4 G The parameter settings estimated method described cid2 α β θ γ cid3 10 06 22 06 452 R Yang et al Artiﬁcial Intelligence 195 2013 440469 53 Parameter estimation QR model We explain estimate parameter Quantal Response Model QR Model The parameter λ QR model represents level noise adversarys response function We employ Maximum Likelihood Estimation MLE ﬁt λ data collected Given game instance G N samples subjects choices τ jG j 1 N likelihood λ cid7 Lλ G λ G qτ G j j1N T denotes target attacked jth player qτ G τ G j player j attacks target t3 game G qτ G j j λ G computed Eq 9 For example λ G q3λ G Furthermore loglikelihood λ log Lλ G Ncid4 j1 log qτ j Gλ G cid4 ti T Ni log qiλ Combining Eq 8 log Lλ G λ cid4 ti T Ni U xi N log cid9 eλU x cid8 cid4 ti T We learn optimal parameter setting λ maximizing total loglikelihood 35 game instances log Lλ G cid4 max λ G st λ cid3 0 46 47 The objective function Eq 46 concave G log Lλ x concave function This demonstrated showing second order derivative log Lλ G nonpositive G cid6 j d2 log L dλ2 U xi U cid6 j x j2e xi2 eλU λU xi U j x j cid2 0 Therefore log Lλ x local maximum We use gradient descent solve optimization problem The MLE λ 055 λ 54 Parameter estimation QRRU model For QRRU Model need estimate parameters λu λs deﬁned Eq 10 We apply Maximum Likelihood Estimation similar method QR model Given game instance G responses N subjects τ jG j 1 N loglikelihood parameter setting λu λs log Lλu λs G Ncid4 j1 log qτ j Gλu λs G cid4 ti T Ni log qiλu λs Combining Eq 10 log Lλu λs G λu cid4 ti T Ni U xi λs cid4 ti T Ni S ix N log cid9 eλu U xi λs S x cid8 cid4 ti T We learn optimal parameter settings QRRU Model maximizing total loglikelihood 35 game instances cid4 max λuλs G log Lλu λs G st λu cid3 0 λs cid3 0 48 49 The objective function Eq 48 concave function G Hessian matrix log Lλu λs G negative semi deﬁnite We include details proof appendix cid4λu λscid5 R Yang et al Artiﬁcial Intelligence 195 2013 440469 453 cid4λu λscid5 Hλu λs G cid4λu λscid5T cid2 0 Hλu λs G Hessian matrix log Lλu λs G computed following Hλu λs G N cid6 Ai A j cid6 Ai A j cid6 U j S iS je cid6 T e Ai 2 ti Ai j S iS j2e T e Ai 2 xi λs S ix Therefore use gradient descent solve optimization problem Eqs 48 j U cid6 ti U j U cid6 ti U j 2e T e Ai 2 j S iS je T e Ai 2 j U cid6 ti A j A j cid6 Ai Ai λu U 49 The MLE parameters based data set cid3 cid2 λ u λ s 06 077 6 Experimental results discussion We evaluated performances defender strategies accuracy different adversary models human subjects online game The Guard The Treasure introduced Section 22 We conducted set evalua tions ﬁrst set includes 7 payoff structures experiments previous section second set focuses comparison QR model QRRU model 61 Experimental settings The design simulated game provided Section 22 We present detailed description experimental settings In total included 70 game instances comprising 7 payoff structures 10 strategies payoff structure ﬁrst set 12 game instances comprising 4 new payoff structures 3 strategies payoff structure second set To avoid confusion sets payoff structures number ﬁrst seven payoff structures 1117 2124 Each game instance played 80 different participants actual number subjects game stance ranges 80 91 Each subject asked play 40 70 games For purpose withinsubject comparison want subject play 10 different strategies payoff structure Therefore 40 games composed 4 payoff structures 10 defender strategies Furthermore order mitigate ordering effect subject responses randomize order game instances played subject We generated 40 different orderings games Latin square design The order played subject drawn uniformly ran domly 40 possible orderings To mitigate ordering effect feedback success failure given subjects end experiment As motivation subjects earn lose money based succeed attacking gate subject opens gate protected guards win lose The participants recruited Amazon Mechanical Turk Note participants differ played game provide data estimating parameter discussed previous section In order avoid noncompliant participants allowed workers HIT approval rates greater 95 100 ap proved HITs participate experiment They ﬁrst given detailed instruction game explaining game played Then practical rounds games provided help familiar game After learning practising given time ﬁnish games Each participant ﬁrst received 50 cents participating game Then gain bonus based outcomes games played point worth 1 cent On average subjects participated ﬁrst set experiment payoff 1117 received 145 bonus based total scores 40 game instances played subjects participated second set experiment payoff 2124 received 044 bonus based total scores 12 game instances played Participants given 5 hours total ﬁnish experiment shown suﬃciently long given average time spent 28 minutes ﬁrst set 40 games 8 minutes second set 12 games In following section ﬁrst parameter settings different leader strategies We provide experimental results follow analysis We compare quality different defender strategies human participants accuracy different adversary models sense human participants follow assumption models 62 Algorithm parameters For seven payoff structures 1117 introduced Section 5 tested different mixed strategies generated seven different algorithms Maximin Dobss 18 Cobra 20 Brpt Rpt Brqr Brqrru We include Maximin benchmark algorithm Maximin assumes adversary selects target worst defender Table 3 lists parameter settings strategies seven payoff structures 454 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table 3 Parameter settings different algorithms Payoff Cobraα Cobracid8 BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 11 12 13 015 20 015 29 015 25 α β θ γ 088 088 225 064 α β θ γ 088 088 225 064 cid8 25 α β θ γ 1 06 22 06 α β θ γ 1 06 22 06 cid8 25 λ 076 λ 055 λu λs 06 077 14 015 275 15 037 25 16 0 25 17 025 25 Dobss Maximin parameters For Cobra set parameters following methodology presented 20 closely possible payoff struc tures 1114 In particular values set α meet entropy heuristic discussed work For payoff structures 1517 identical payoff structures ﬁrst Pita et al use parameter settings work For BrptE RptE parameters Prospect Theory empirical values suggested literatures 31 For RptE empirically set cid8 25 maximum potential reward adversary 10 experimental settings We tried set parameters Prospect Theory learned ﬁrst set experiment described Section 52 We denote algorithms BrptL RptL For Brqr tried different values parameter λ λ 076 values learned data reported Pita et al 20 λ 055 value learned data collected ﬁrst set experiments participants Amazon Mechanical Turk We refer strategies resulting parameter settings Brqr algorithm Brqr76 Brqr55 respectively For Brqrru parameters learned data collected ﬁrst set experiments 63 Quality comparison We evaluated performance different defender strategies defenders expected utility statistical signiﬁcance results bootstrapt method 37 631 Average performance We ﬁrst evaluated average defender expected utility U d avgx different defender strategies based subjects choices avgx 1 U d N Ncid4 j1 U d τ j x 1 N cid4 ti T Ni U d xi τ j target selected jth subject Ni number subjects chose target ti N total number subjects Fig 5 displays U d avgx different strategies payoff structure We displayed normalized defender average expected utility different strategies payoff structure Fig 6 After normalization U d avgx defender strategy varies 0 1 highest U d avgx payoff structure scaled 1 lowest U d avgx scaled 0 Overall Brqr76 Brqr55 Brqrru performed better algorithms We compare performance algorithms seven algorithms report level statistical signiﬁcance Tables 4 5 6 We summarize results Maximin outperformed algorithms statistical signiﬁcance seven payoff structures Dobss outperformed algorithms statistical signiﬁcance payoff structure 16 In ﬁve seven payoff structures Cobra outperformed algorithms statistical signiﬁcance In payoff structure 13 performance Cobra close algorithms statistical signiﬁ cance way In payoff structure 15 Cobra outperformed algorithms statistical signiﬁcance achieved The algorithms outperform BrptE statistical signiﬁcance seven payoff structures Furthermore BrptL outperformed algorithms seven payoff structures statistical signiﬁcance cases payoff structure 16 In seven payoff structures RptE outperformed algorithms statistical signiﬁcance In payoff 13 RptE outperformed algorithms result statistical signiﬁcant In payoff structure R Yang et al Artiﬁcial Intelligence 195 2013 440469 455 Fig 5 Defender average expected utility achieved different strategies 14 RptE achieves similar performance Brqr55 outperformed Brqr76 Brqrru In payoff 15 RptE achieves similar performance Brqr76 outperformed Brqr55 Brqrru Furthermore RptL outperformed algorithms statistical signiﬁcance seven payoff structures payoff structure 12 result comparing Brqr76 Brqrru RptL doesnt statistical signiﬁcance Overall quantal response Brqr76 Brqr55 Brqrru strategies preferred strategies However performance strategies close set experiments In order differentiate strategies prove effectiveness QRRU model conducted separate set experiments We ﬁrst select new payoff structures 1000 random samples following rules cid6 n i1 xk logxk xl We ﬁrst measure distance Brqrru strategy Brqr strategies Kullback Leibler KL divergence Dxk xl DKLxkxl DKLxlxk DKLxkxl For payoff structure measure KL distance pair Brqrru Brqr76 pair Brqrru Brqr55 So measurements payoff structure We sort payoff structures descending order mean distance In 10 payoff structures select payoff structures targets assigned minimum coverage probability Brqr76 Brqr55 large penalty defender payoff structures penalty defender target small The details payoff structures defender strategies included appendix We conducted new set experiments human subjects payoff structures QR model based strategies payoff structure In total 4 3 12 game instances included experiments Each subject asked play 12 game instances 80 subjects involved experiments 456 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Fig 6 Defender average expected utility normalized 0 1 achieved different strategies Table 4 Level statistical signiﬁcance comparing Brqr76 algorithms p cid2 001 p cid2 005 p cid2 01 vs payoff 11 payoff 12 payoff 13 payoff 14 payoff 15 payoff 16 payoff 17 Dobss Maximin Cobra BrptE RptE BrptL RptL 020 096 026 021 025 099 013 015 Table 5 Level statistical signiﬁcance comparing Brqr55 algorithms p cid2 001 p cid2 005 p cid2 01 vs payoff 11 payoff 12 payoff 13 payoff 14 payoff 15 payoff 16 payoff 17 Dobss Maximin Cobra BrptE RptE BrptL RptL 016 086 037 016 095 012 011 R Yang et al Artiﬁcial Intelligence 195 2013 440469 457 Table 6 Level statistical signiﬁcance comparing Brqrru algorithms p cid2 001 p cid2 005 p cid2 01 vs payoff 11 payoff 12 payoff 13 payoff 14 payoff 15 payoff 16 payoff 17 Dobss Maximin Cobra BrptE RptE BrptL 015 099 040 027 018 033 011 RptL 027 Fig 7 Defender average expected utility achieved QR model based strategies Table 7 Statistical signiﬁcance p cid2 005 p cid2 001 payoff 21 payoff 22 payoff 23 payoff 24 Brqrru vs Brqr76 Brqrru vs Brqr55 Brqrru vs Brqr76 Brqrru vs Brqr55 Brqr76 vs Brqrru Brqr55 vs Brqrru Brqrru vs Brqr76 Brqr55 vs Brqrru 087 040 097 035 Fig 7 displays defender average expected utility achieved strategies We report statistical signiﬁ cance results Table 7 In payoff structures 21 22 Brqrru outperforms Brqr76 Brqr55 statistical signiﬁcance In payoff structures 23 24 strategies close performance No statistical signiﬁcance results reported Table 7 As noted earlier important feature payoff structures 21 22 compared payoff structures 23 24 target covered minimum resource Brqr76 Brqr55 target 3 payoff structure 21 target 3 payoff structure 22 large penalty cid2 6 defender In experiments payoff structures 21 22 10 subjects selected targets target 3 payoff structure 21 22 playing Brqr76 Brqr55 subjects chose target playing Brqrru Brqrru covers targets resources This main reason Brqrru signiﬁcantly outperforms Brqr payoff 21 payoff 22 In payoff 23 24 similar observation obtained subjects choice targets covered minimum resources Brqr76 Brqr55 selected frequently compared case Brqrru played However targets target 1 payoff 23 target 2 payoff 24 small penalty defender 1 Therefore signiﬁcant differences performance different Brqr strategies Based result sets experiments conclude stochastic model based strategies superior competitors Brqrru preferred strategy stochastic model based strategies In particular Brqrru achieves signiﬁcantly better performance Brqr target covered minimum resource Brqr po tentially large penalty defender performance similar stochastic model based strategies 458 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Fig 8 Distribution defenders expected utility individual subject 632 Performance distribution We analyze distribution performance defender strategy playing different adversaries subjects Given game instance G defender expected utility achieved playing strategy x subject j denoted U d x different defender strategies individual τ G j x Figs8 9 display distribution U d τ G j subjects payoff structure The yaxis shows range defenders expected utility different subjects Each box extended dash line ﬁgure shows distribution defender expected utility defender strategies dashed line speciﬁes range U d x band showing minimum value τ G j band showing maximum value box speciﬁed 25th 75th percentiles U d τ G j x showing 25th percentile value showing 75th value band inside box speciﬁes median 50th percentile U d x We compare distributions different defender strategies perspectives τ G j Range As presented Fig 8 Fig 9 general defender expected utility smallest range Maximin strategy played payoff structure 17 range defender expected utility RPTL played slightly smaller Maximin played Cobra Rpt Brqr Brqrru lead larger range defender expected utility Maximin Defender expected utility largest range Dobss Brpt played Worst case The lower band dashed line indicates worstcase defender expected utility different strategies played Maximin highest worstcase defender expected utility general payoff 15 worst case defender expected utility playing Brqr76 better playing Maximin Dobss Brpt lead lowest worstcase defender expected utility The worstcase defender expected utility playing Cobra Rpt Brqr Brqrru extreme cases Furthermore Brqr Brqrru lead higher worstcase defender expected utility Cobra Rpt In general playing Maximin defender expected utility individual adversary achieves smallest variance robust uncertainty adversarys choice However assuming adver sary select target making expected utility target equal Maximin exploit different preferences adversary different targets Brpt Dobss assume subjects select target maxi R Yang et al Artiﬁcial Intelligence 195 2013 440469 459 Fig 9 Distribution defenders expected utility individual subject mizes expected utility consider possibility deviations optimal choice adversary This leads arbitrarily lower defender expected utility adversary deviates predicted choice Cobra Rpt Brqr Brqrru try robust deviations Brqr Brqrru consider possibly small probability adversary attacking target softmax function In contrast Cobra Rpt separate targets groups cid8optimal set noncid8optimal set hard threshold They try maximize worst case defender assuming response cid8optimal set assign resources non cid8optimal targets When noncid8optimal targets high defender penalties Cobra Rpt vulnerable adversarys deviation For example target 6 payoff structure 12 small reward 1 large penalty 10 attacker Both Cobra Rpt consider target noncid8optimal set assign small probability cover target cid2 005 However approximately 10 subjects chosen target Since target high defender penalty 6 Cobra Rpt lose reward target Similar examples include target 5 payoff structure 14 target 8 payoff structure 11 64 Model prediction accuracy In section evaluate model predicts actual responses human participants differ ent metrics 38 mean square deviation MSD proportion inaccuracy POI Euclidean distance ED We ﬁrst extend deﬁnition MSD 38 designed 2action game order suit domain player 8 actions Given choices N subjects MSD model computed MSD cid1712 cid16 1 N Ncid4 pτ n 12 n1 50 τ n represents index target chosen subject n pi predicted probability model target chosen 460 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table 8 Ability behavioral models predict attacker decision Model Dobss PTE PTL QR76 QR55 QRRU Cobra RPTE RPTL Out sample In sample MSD 081 084 084 079 081 080 091 093 093 POI 067 071 071 067 067 065 083 035 087 052 086 049 ED 076 081 081 023 022 021 094 099 098 MSD 085 087 086 083 084 083 091 094 093 POI 073 075 074 073 073 070 083 042 088 056 086 054 ED 080 084 083 022 021 018 093 099 096 The POI score meant models deterministic prediction footing stochastic predic tion It treats target highest predicted probability predicted target computes proportion subjects didnt choose predicted target The POI score computed POI 1 N Ncid4 1 pτ n n1 51 τ n index target chosen subject n pτ n 1 τ n predicted target pτ n 0 Note models deterministic prediction POI score exactly equal square MSD value The Euclidean distance measures difference actual distribution subjects choices predic tion model It computed cid18cid4 cid2 cid3 ED pi pact 2 iT pi subjects chosen target 52 actually percentage probability predicted model target chosen pact Table 8 presents ability different models predict attacker decision measured different cri teria5 The measurements outofsample data 70 rounds games insample data 35 rounds games displayed table Better predictive power indicated lower MSD value POI score lower ED value The models deterministic prediction quantal response related models stochastic prediction The models Cobra RPTE RPTL dont strict deﬁnition prediction attackers behavior They modiﬁcations base models robustness For example Cobra modiﬁes Dobss assuming attacker deviate choosing target highest expected utility targets expected utilities cid8 highest value However subset possibly chosen targets model doesnt explicitly predict behavior attacker plays maximin strategy maximizing lowest expected utility RPTE RPTL modify PTE PTL similar ways Given property models compute POI score different ways different deﬁnitions model prediction The ﬁrst deﬁnition predicts single target lowest expected utility defender subset possible deviations Therefore POI score counts proportion subjects chosen targets The second deﬁnition predicts targets subset possible deviations Therefore POI score counts targets outside subset The POI score computed ﬁrst deﬁnition equal higher value computed second deﬁnition Note second deﬁnition doesnt satisfy property prediction sum predictions targets larger 1 We use deﬁnition mainly importance accounting deviation attackers decision The POI values computed second deﬁnition shown parentheses Table 8 The observations table summarized blow 1 For outofsample data 30 subjects selected target predicted PTE PTL words 70 subjects deviated prediction For Dobss average 67 subjects deviated predicted response Similar patterns observed insample data 2 Both Rpt Cobra consideration deviation subjects responses optimal action The percentage subjects deviate model prediction decreased signiﬁcantly outofsample data POI score 5 Maximin doesnt prediction adversary behavior exclude analysis R Yang et al Artiﬁcial Intelligence 195 2013 440469 461 Cobra 035 compared 067 Dobss POI score RPTE decreased 019 compared PTE POI score RPTL decreased 022 compared PTL Similar patterns observed insample data 3 The POI score QR76 QR55 Dobss This expected target predicted QR model chosen highest probability target highest expected utility attacker prediction Dobss In words QR76 QR55 predicted target Dobss At time QRRU lowest POI score models outofsample data insample data The MSD scores QRrelated models better lower models outofsample data QR55 score Dobss 4 The advantage QR related models signiﬁcant ED score represents error model predicting distribution subjects choices As shown Table 8 QRrelated models signiﬁcantly lower ED scores models This essentially reason models achieved signiﬁcantly better defender expected utility models 7 Related work In ﬁrst sections paper discussed recent developments gametheoretical approaches solve Stack elberg security games We discuss additional related work section Motivated realworld security problems algorithms developed compute optimal defender strategies Stackelberg games 191830 The ﬁrst algorithm real application Dobss Decomposed Optimal Bayesian Stackelberg Solver 18 central ARMOR 5 LAX airport GUARDS 7 built Transportation Security Administration Other works related Stackelberg security games include Agmon et al 339 Gatti et al 404 multirobot patrolling However important limitation work assumption perfectly rational adversary hold real world domains Recent work 20 developed new algorithm Cobra provided solution designing better defender strategies human adversaries modeling adversarys behavior taking consideration human deviation utility maximizing strategy ii human anchoring bias given limited observation defender mixed strategy Cobra signiﬁcantly outperforms Dobss experimental results human subjects We provided extensive comparison new approaches COBRA paper Another line related work Stackelberg security games trying design robust strategies deal different kinds uncertainties 4143 Yin et al 42 proposed uniﬁed eﬃcient algorithm addresses execution uncertainties defender observation uncertainties adversaries SSGs Kiekintveld et al 43 address payoff uncertainty introducing general model inﬁnite Bayesian Stackelberg security games allows payoffs represented continuous payoff distributions Although simulation based experiment showed promising result studies performances models real human subjects left unaddressed Our work differs efforts handle bounded rationality human adversary propose different models explicitly predict human decision making accounting bounded rationality Many models proposed capture human bounded rationality decision making psychology cognitive science 443445 A key challenge applying models gametheoretical framework help design better strategy transition descriptive model computational model On hand growing game theory literature develop realistic computational models human decision making games 91132 Most models ﬁnd empirical support data human playing games However research efforts attempted bridge gap computational game theory behavioral game theory fewer important area SSGs topic article In paper explore method developing computation models human adversary decision making based descriptive theories Prospect Theory rankdependent Expected Utility The outperforming result new models provide clear direction future work improve models human bounded rationality security games Outside area Stackelberg security games recent investigations human subjects inter acting agents For example Melo et al 46 investigate impact expression automated agents anger happiness human participant play game In repeated prisoners dilemma games agents expressions shown signiﬁcantly affect human subjects cooperation defection Similarly Azaria et al 47 focus road selection games advice automated provide human subjects Peled et al 48 focus bilateral bargaining games designing agents negotiate proﬁciently people Aside obvious difference focus SSGs key focus eﬃciently computing optimal mixed strategies defender 8 Summary future work There signiﬁcant gametheoretic techniques solve security problems Several realworld application based techniques deployed nation including ARMOR 5 IRIS 6 GUARDS 7 PRO TECT 8 These systems adopted traditional gametheoretical assumption perfectly rational adversaries While appropriate ﬁrst generation Stackelberg security game applications critical develop new 462 R Yang et al Artiﬁcial Intelligence 195 2013 440469 methods compute defender strategies addressing human bounded rationality particularly range SSG applica tions deployed systems face human adversaries continue grow New methods need developed compute defender strategy bounded rationality real human adversaries In paper address problem applying fundamental theories human decisionmaking Prospect Theory PT Quantal Response Equilibrium QRE model adversarys behavior security games The contributions article include proposing mathematical models adversarys decisionmaking based PT ii providing method adapt parameters PT PTbased methods iii proposing mathematical model adversarys decisionmaking based quantal response QR second uses modiﬁed QR based rank dependent utility iv developing eﬃcient algorithms compute optimal strategy defender adversary mod els v extensivetodate experiment verify effectiveness proposed approaches We compare new approaches benchmark algorithms Dobss Maximin Cobra seven different payoff structures Cobra leading contender addressing human bounded rationality presented previous work Experiment results new methods based QR model achieved statistically signiﬁcant better formance benchmark algorithms PTbased new methods Furthermore identify cases targets covered minimum resource large penalty defender modiﬁed QR model achieved sig niﬁcantly better performance basic QR model By providing new models better predicts behavior human adversary new algorithms computes strategies outperforming leading competitor paper advanced stateoftheart While research reported article takes important step forward addressing bounded rationality human adversaries context security games open topics future research One key area translate results obtained controlled experiments AMT speciﬁc realworld security applications Most issues related making transition unique work apply generally studies agenthuman interactions For example speciﬁc conditions tested lab way decisions presented likely exactly reﬂected real interactions population adversaries identical population adversaries realworld security setting However methods based fundamental features human decision making robustly supported large number behavioral studies methods translate realworld applications In addition parameters offer ability tune models time speciﬁc settings populations methodology provides techniques tuning parameters The parameter settings work serve initial settings real deployment adapted time Alternatively parameters initially set conservatively somewhat close settings result standard equilibrium adapted time starting point Another interesting possibility explored future work develop ways incorporate different sources information prior knowledge biases speciﬁc adversaries models general way Furthermore current game model abstraction realworld security scenario particular Los Angeles international airport The model reﬁned reﬂect details scenario For example current game assumes covering target single unit resources binary effect protecting targets resources protectednot protected An interesting direction future work explore effect having multiple units resources protect target At time interesting direction future work ex tend current game model deal domains continual online interaction defender attacker Acknowledgements This research supported Army Research Oﬃce grant W911NF1010185 We thank Mohit Goenka James Pita help developing webbased game F Ordonez like acknowledge support Conicyt Grant No ACT87 Appendix A Payoff structure information The payoff structures selected clustering groups displayed Table A9 The payoffs identical ﬁrst Pita et al shown Table A10 The payoff structures selected second evaluation set experiment comparing QR model based strategies listed Table A11 Table A9 Payoff structures Target defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty Table A10 Payoff structures Target defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty Table A11 Payoff structures Target defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty defender reward defender penalty subject reward subject penalty R Yang et al Artiﬁcial Intelligence 195 2013 440469 463 1 2 8 10 7 3 10 9 10 5 2 8 6 5 10 3 4 1 1 5 1 2 4 8 8 3 4 8 8 3 1 10 4 7 4 2 6 7 6 1 1 2 10 2 6 10 8 4 8 2 8 1 3 5 6 9 9 4 7 8 2 4 8 9 4 3 10 5 2 3 5 5 3 2 1 6 6 3 7 4 6 6 1 7 1 1 3 4 Payoff structure 11 7 3 3 6 7 1 7 8 b Payoff structure 12 9 5 2 10 9 1 9 8 c Payoff structure 13 8 4 1 3 3 6 3 7 d Payoff structure 14 10 9 3 5 2 3 9 8 3 4 Payoff structure 15 2 1 5 3 3 6 6 3 b Payoff structure 16 1 1 3 3 5 8 10 2 c Payoff structure 17 1 1 2 3 5 10 10 3 3 4 c Payoff structure 21 1 9 1 4 8 10 6 8 b Payoff structure 22 1 5 1 6 10 1 10 2 c Payoff structure 23 10 8 1 4 7 6 4 10 5 8 10 6 4 7 7 10 4 3 3 1 7 10 10 2 9 5 4 5 7 3 1 1 1 3 1 5 1 3 5 8 7 7 10 1 7 6 2 4 7 1 6 6 8 5 7 2 7 6 1 10 4 10 7 2 4 10 9 4 6 1 1 1 2 2 3 3 3 2 3 3 3 6 6 4 7 5 10 4 3 9 9 1 5 2 7 6 2 8 9 4 2 10 5 3 7 3 5 8 2 7 1 7 5 7 10 4 5 11 9 2 5 9 9 3 7 2 5 6 4 3 4 2 10 8 9 5 2 3 1 1 1 3 6 2 5 2 8 5 8 6 8 2 7 3 3 2 5 4 3 2 6 4 3 8 4 8 6 5 2 7 8 3 6 8 6 1 continued page 9 7 7 8 464 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table A11 continued Target defender reward defender penalty subject reward subject penalty 1 7 2 7 9 2 3 1 1 1 3 4 d Payoff structure 24 6 5 6 10 1 4 1 7 5 10 5 3 3 6 1 8 1 1 7 8 8 7 5 8 9 10 2 1 Appendix B Defender mixedstrategy The defenders mixedstrategy algorithm payoff structures displayed Tables B12B19 Table B12 Defenders mixedstrategy payoff structure 11 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 049 074 057 039 044 053 060 057 058 055 2 053 059 062 051 058 054 063 058 059 056 Table B13 Defenders mixedstrategy payoff structure 12 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 042 075 048 028 031 043 051 054 056 046 2 078 018 053 093 061 078 049 052 050 040 Table B14 Defenders mixedstrategy payoff structure 13 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 053 012 048 042 041 058 036 036 034 033 2 037 048 042 024 029 039 047 043 043 038 Table B15 Defenders mixedstrategy payoff structure 14 3 015 024 018 017 024 010 020 018 018 025 3 008 034 009 007 009 004 009 021 023 030 3 012 024 016 029 041 009 027 020 022 033 4 036 006 022 026 016 036 017 021 019 020 4 047 008 043 034 038 048 039 036 034 025 4 025 054 029 019 025 021 032 036 037 033 5 044 052 051 043 051 042 052 051 052 048 5 064 049 074 059 064 065 071 064 063 054 5 006 031 007 009 014 005 011 013 012 033 6 059 034 044 070 041 060 041 047 047 045 6 0 045 0 005 007 001 003 016 019 030 6 072 063 081 078 078 076 075 072 073 067 7 037 018 034 026 028 039 029 030 028 028 7 060 030 070 052 056 061 069 058 055 045 7 031 058 036 028 036 028 040 043 044 035 Target Dobss Maximin 1 022 059 2 037 021 3 019 041 4 044 036 5 005 044 6 058 063 7 069 0080 8 007 032 011 028 038 006 018 018 020 025 8 0 040 002 023 034 001 009 0 0 030 8 064 010 042 072 036 065 032 037 036 028 8 047 029 R Yang et al Artiﬁcial Intelligence 195 2013 440469 465 Table B15 continued Target Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 024 028 037 016 025 035 037 034 2 042 027 031 037 043 033 032 034 Table B16 Defenders mixedstrategy payoff structure 15 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 0 056 0 016 028 002 013 012 013 015 2 059 053 064 049 053 061 062 061 062 060 Table B17 Defenders mixedstrategy payoff structure 16 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 056 053 058 049 056 058 058 058 059 058 2 045 064 055 046 056 043 056 059 060 059 Table B18 Defenders mixedstrategy payoff structure 17 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 059 049 057 054 054 061 052 059 059 059 Table B19 Defenders mixedstrategy Target Brqrru Brqr55 Brqr76 1 030 031 031 2 044 036 048 041 044 042 041 044 044 044 2 045 050 052 3 021 022 029 014 021 030 032 034 3 045 0 023 041 012 043 013 016 012 010 3 019 0 0 021 001 015 0 0 0 005 3 010 0 0 019 0 008 0 0 0 0 3 035 017 013 4 050 033 037 048 053 044 041 039 4 051 049 063 046 052 050 060 055 056 052 4 068 049 053 067 054 073 054 060 061 060 4 065 052 059 060 057 070 055 063 064 064 4 Payoff structure 21 038 042 041 5 004 008 010 004 007 020 023 034 5 056 037 052 051 048 057 049 052 052 049 5 0 0 0 005 001 0 0 0 0 0 5 0 048 0 009 018 001 021 008 008 008 5 035 036 036 6 066 054 060 060 066 062 062 058 6 0 0 0 016 018 002 013 0 0 015 6 019 027 031 021 031 015 028 019 016 016 6 025 017 033 027 030 020 025 022 020 020 6 035 034 036 7 039 090 053 073 035 036 033 029 7 062 045 055 052 053 065 053 057 058 056 7 065 059 062 064 063 069 063 066 067 066 7 062 049 056 057 054 066 052 060 061 061 8 053 038 043 048 050 042 040 038 8 027 060 040 028 036 021 037 046 048 041 8 030 048 041 029 038 026 040 038 037 036 8 036 048 047 035 043 032 053 045 045 045 7 8 039 043 045 continued page 042 047 047 466 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table B19 continued Target Brqrru Brqr55 Brqr76 Brqrru Brqr55 Brqr76 Brqrru Brqr55 Brqr76 1 038 051 051 032 010 015 026 029 029 2 035 039 040 032 038 038 031 0 0 3 035 007 005 032 036 032 037 040 040 4 b Payoff structure 22 030 036 041 c Payoff structure 23 032 036 035 d Payoff structure 24 031 019 020 5 043 063 063 032 030 027 036 042 041 6 035 022 020 027 032 033 031 047 050 7 035 016 014 066 071 071 052 055 054 8 051 066 065 044 047 047 056 069 065 Appendix C Distribution subjects choices The distributions subjects choices playing payoffstrategy combination displayed Tables C20 C27 Table C20 Distribution subjects choices payoff structure 11 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 1744 581 814 2326 1163 814 465 349 581 814 2 581 349 233 814 116 465 581 930 116 233 3 698 233 116 2209 233 2442 233 814 930 116 Table C21 Distribution subjects choices payoff structure 12 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 1333 222 889 2333 2000 1333 889 111 000 222 2 1111 3000 2222 1000 2556 1333 4000 2111 3556 3222 3 444 111 1111 889 778 1444 889 444 000 111 Table C22 Distribution subjects choices payoff structure 13 Target Dobss Maximin Cobra BrptE RptE BrptL RptL 1 2308 4176 2308 3187 2418 1978 4615 2 1099 110 879 1758 2088 1099 110 3 1868 000 1099 440 000 2308 1319 4 116 5000 2674 465 4302 581 4535 3023 3605 3605 4 333 3111 1444 1444 1000 444 444 1778 1444 3556 4 220 000 330 659 330 549 330 5 465 000 000 349 116 814 581 116 233 233 5 1556 1222 1000 1333 1222 1444 1333 1556 667 889 5 1209 000 879 1099 879 1099 1209 6 2326 2791 3140 1628 3837 2209 1860 3256 2907 3372 6 778 000 1444 889 889 556 778 222 556 111 6 1978 2088 1758 1978 1648 1538 1209 7 465 1047 1047 1977 233 581 233 930 930 814 7 1778 2222 1222 2000 1556 1667 1111 1667 1778 1778 7 549 000 330 659 220 769 440 8 3605 000 1977 233 000 2093 1512 581 698 814 8 2667 111 667 111 000 1778 556 2111 2000 111 8 769 3626 2418 220 2418 659 769 R Yang et al Artiﬁcial Intelligence 195 2013 440469 467 Table C22 continued Target Brqr76 Brqr55 Brqrru 1 3187 3846 3297 2 110 220 000 Table C23 Distribution subjects choices payoff structure 14 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 543 000 435 326 000 1087 326 000 000 109 2 652 761 543 1413 1087 1413 217 1196 870 217 Table C24 Distribution subjects choices payoff structure 15 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 1477 114 795 1250 000 1136 682 341 000 000 2 568 114 114 1932 682 682 455 227 114 114 Table C25 Distribution subjects choices payoff structure 16 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 833 333 000 2667 833 500 000 333 333 333 2 1167 000 167 167 167 1333 167 000 000 000 3 440 330 000 3 870 000 543 435 217 1087 870 000 109 000 3 1364 4773 4091 114 5455 1364 5455 3864 5341 5114 3 1333 4167 4667 1000 4000 1500 1333 4500 4500 3000 Table C26 Distribution subjects choices payoff structure 17 Target Dobss Maximin Cobra BrptE RptE BrptL RptL Brqr76 Brqr55 Brqrru 1 610 732 244 610 610 1220 1707 732 610 488 2 1098 976 854 976 1220 1098 122 1341 1341 1220 3 1951 3293 4512 1220 3902 1463 2195 4146 3780 2927 4 220 000 220 4 1087 761 1196 1739 2283 1304 978 978 1304 1087 4 114 227 000 455 341 568 227 000 000 114 4 2833 5000 4333 2500 3500 2667 6333 4333 4167 4500 4 2317 1951 2439 3293 2683 1829 2317 1707 2805 2317 5 1429 1099 000 5 2174 000 1522 1957 1413 1522 1739 1304 1630 109 5 795 1023 795 114 341 341 114 455 568 909 5 1833 167 333 2000 333 1333 1500 333 167 1500 5 1951 122 366 1829 000 1463 000 122 122 488 6 1099 1868 1538 6 2065 870 1087 1848 978 1087 761 1196 870 1087 6 1705 114 1136 682 114 1250 795 1932 1250 341 6 833 333 333 1000 167 333 333 333 500 167 6 1220 976 610 976 122 1341 2195 732 610 1829 7 000 000 000 7 2283 7065 4130 1304 3804 2283 4891 5326 4891 6739 7 2841 3636 3068 3864 3068 2500 2273 3182 2727 3409 7 667 000 000 500 667 667 333 167 167 333 7 122 1951 854 488 854 488 1463 1220 732 366 8 3516 2637 4945 8 326 543 543 978 217 217 217 000 326 652 8 1136 000 000 1591 000 2159 000 000 000 000 8 500 000 167 167 333 1667 000 000 167 167 8 732 000 122 610 610 1098 000 000 000 366 468 R Yang et al Artiﬁcial Intelligence 195 2013 440469 Table C27 Distribution subjects choices Target 1 7294 5412 5882 235 235 235 000 1412 1412 4941 3529 4118 Brqrru Brqr55 Brqr76 Brqrru Brqr55 Brqr76 Brqrru Brqr55 Brqr76 Brqrru Brqr55 Brqr76 References 2 1059 1647 941 353 588 588 471 235 235 941 3529 2941 3 000 1059 1176 000 1059 1647 118 235 000 824 235 471 4 Payoff structure 21 236 118 353 b Payoff structure 22 8471 6824 5765 c Payoff structure 23 235 235 118 d Payoff structure 24 235 118 000 5 353 706 588 118 118 353 000 000 118 471 588 706 6 7 8 941 706 824 000 235 706 6706 4353 4824 588 353 118 118 235 235 353 118 235 1529 2235 1647 1529 1647 1529 000 118 000 471 824 471 941 1294 1647 471 000 118 1 R Yang C Kiekintveld F Ordonez M Tambe R John Improving resource allocation strategy human adversaries security games IJCAI 2011 pp 458464 2 N Gatti Game theoretical insights strategic patrolling Model algorithm normalform ECAI08 2008 pp 403407 3 N Agmon S Kraus GA Kaminka Multirobot perimeter patrol adversarial settings ICAT 2008 pp 23392345 4 N Basiloco N Gatti F Amigoni Leaderfollower strategies robotic patrolling environments arbitrary topologies AAMAS 2009 pp 5764 5 J Pita M Jain F Ordonez C Portway M Tambe C Western P Paruchuri S Kraus Deployed armor protection The application game theoretic model security Los Angeles international airport AAMAS 2008 pp 125132 6 J Tsai S Rathi C Kiekintveld F Ordonez M Tambe IRIS tool strategic security allocation transportation networks AAMAS 2009 pp 3744 7 J Pita M Tambe C Kiekintveld S Cullen E Steigerwald GUARDS game theoretic security allocation national scale AAMAS 2011 pp 3744 8 B An J Pita E Shieh M Tambe C Kiekintveld J Marecki GUARDS PROTECT Next generation applications security games ACM SIGecom Exchanges 10 2011 3134 9 CF Camerer T Ho J Chongn A cognitive hierarchy model games QJE 119 2004 861898 10 M CostaGomes VP Crawford B Broseta Cognition behavior normalform games An experimental study Econometrica 69 2001 11931235 11 S Ficici A Pfeffer Simultaneously modeling humans preferences beliefs preferences AAMAS 2008 pp 323330 12 Y Gal A Pfeffer Modeling reciprocal behavior human bilateral negotiation AAAI 2007 13 M Tambe Security Game Theory Algorithms Deployed Systems Lessons Learned Cambridge University Press New York NY 2011 14 D Korzhyk V Conitzer R Parr Security games multiple attacker resources IJCAI 2011 pp 273279 15 J Letchford Y Vorobeychik Computing randomized security strategies networked domains AARM Workshop AAAI 2011 16 D Kahneman A Tvesky Prospect theory An analysis decision risk Econometrica 47 1979 263292 17 RD McKelvey TR Palfrey Quantal response equilibria normal form games Games Economic Behavior 2 1995 638 18 P Paruchuri JP Pearce J Marecki M Tambe F Ordonez S Kraus Playing games security An eﬃcient exact algorithm solving bayesian Stackelberg games AAMAS 2008 pp 895902 19 C Kiekintveld M Jain J Tsai J Pita F Ordonez M Tambe Computing optimal randomized resource allocations massive security games AAMAS 2009 pp 689696 20 J Pita M Jain F Ordonez M Tambe S Kraus Solving Stackelberg games realworld Addressing bounded rationality limited observations human preference models Artiﬁcial Intelligence Journal 174 2010 11421171 21 C Fox R Clemen Subjective probability assessment decision analysis Partition dependence bias ignorance prior Management Science 51 2005 14171432 22 KE See CR Fox YS Rottenstreich Between ignorance truth Partition dependence learning judgment uncertainty Journal Experimental Psychology Learning Memory Cognition 32 2006 13851402 23 DL McFadden Econometric analysis qualitative response models Z Griliches MD Intriligator Eds Handbook Econometrics vol 2 Elsevier 1984 pp 13951457 24 K Train Discrete Choice Methods Simulation Cambridge University Press Cambridge UK 2003 25 DL McFadden Quantal choice analysis A survey Annals Economic Social Measurement 5 1976 369390 26 DL McFadden A method simulated moments estimation discrete choice models numerical integration Econometrica 57 1989 9951026 27 E Diecidue PP Wakker On intuition rankdependent utility The Journal Risk Uncertainty 23 2001 281289 28 D Korzhyk Z Yin C Kiekintveld V Conitzer M Tambe Stackelberg vs Nash security games An extended investigation interchangeability equivalence uniqueness JAIR 2011 pp 297327 29 D Korzhyk V Conitzer R Parr Complexity computing optimal Stackelberg strategies security resource allocation games AAAI 2010 30 J Tsai Z Yin J Kwak D Kempe C Kiekintveld M Tambe Urban security Gametheoretic resource allocation networked physical domains AAAI 2010 31 D Kahneman A Tvesky Advances prospect theory Cumulative representation uncertainty Journal Risk Uncertainty 5 1992 297322 32 DO Stahl PW Wilson Experimental evidence players models players JEBO 25 1994 309327 33 JR Wright K LeytonBrown Beyond equilibrium Predicting human behavior normalform games AAAI 2010 34 H Simon Rational choice structure environment Psychological Review 63 1956 129138 35 MK Sen PL Stoffa Global Optimization Methods Geophysical Inversion Elsevier New York 1995 36 JC Becsey L Berke JR Callan Nonlinear squares methods A direct grid search approach Journal Chemical Education 45 1968 728 R Yang et al Artiﬁcial Intelligence 195 2013 440469 469 37 RR Wilcox Applying Contemporary Statistical Techniques Academic Press 2003 38 N Feltovich Reinformentbased vs beliefbased learning models experimental asymmetricinformation games Econometrica 68 2000 605641 39 N Agmon S Kraus GA Kaminka V Sadow Adversarial uncertainty multirobot patrol IJCAI 2009 pp 18111817 40 N Gatti Game theoretical insights strategic patrolling model algorithm normalform ECAI 2008 pp 403407 41 M Aghassi D Bertsimas Robust game theory Mathematical Programming 107 2006 231273 42 Z Yin M Jain M Tambe F Ordonez Riskaverse strategies security games execution observational uncertainty AAAI 2011 43 C Kiekintveld J Marecki M Tambe Approximation methods inﬁnite bayesian Stackelberg games Modeling distributional payoff uncertainty AAMAS 2011 pp 10051012 44 R Hastie RM Dawes Rational Choice Uncertain World Psychology Judgement Decision Making Sage Publications Thousand Oaks 2001 45 C Starmer Developments nonexpected utility theory The hunt descriptive theory choice risk Journal Economic Literature XXXVIII 2000 332382 46 C Melo P Carnevale J Gratch The effect expression anger happiness agents negotiations humans AAMAS 2011 pp 937944 47 A Azaria Z Rabinovich S Kraus CV Goldman Strategic information disclosure people multiple alternatives AAAI 2011 pp 594600 48 N Peled Y Gal S Kraus A study computational human strategies revelation games AAMAS 2011 pp 345352