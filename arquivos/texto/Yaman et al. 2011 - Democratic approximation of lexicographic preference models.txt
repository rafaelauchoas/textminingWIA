Artiﬁcial Intelligence 175 2011 12901307 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Democratic approximation lexicographic preference models Fusun Yaman Thomas J Walsh b Michael L Littman c Marie desJardins d BBN Technologies 10 Moulton St Cambridge MA 02138 USA b University Arizona Department Computer Science Tucson AZ 85721 USA c Rutgers University Department Computer Science Piscataway NJ 08854 USA d University Maryland Baltimore County Computer Science Electrical Engineering Department Baltimore MD 21250 USA r t c l e n f o b s t r c t Article history Received 27 February 2009 Received revised form 5 August 2010 Accepted 5 August 2010 Available online 2 December 2010 Keywords Lexicographic models Preference learning Bayesian methods 1 Introduction Lexicographic preference models LPMs intuitive representation corresponds realworld preferences exhibited human decision makers Previous algorithms learning LPMs produce best guess LPM consistent observations Our approach democratic commit single LPM Instead approximate target votes collection consistent LPMs We present variations methodvariable voting model votingand empirically democratic algorithms outperform existing methods Versions democratic algorithms presented case preferred values attributes known case unknown We introduce intuitive powerful form background knowledge prune possible LPMs We demonstrate background knowledge incorporated variable model voting improves performance signiﬁcantly especially number observations small 2010 Elsevier BV All rights reserved Lexicographic preference models LPMs simplest intuitive preference representations An LPM deﬁnes order importance variables objects domain uses order preference decisions For example meal preference vegetarian weak stomach represented LPM vegetarian dish preferred nonvegetarian dish vegetarian nonvegetarian items mild dishes preferred spicy ones Despite simplicity lexicographic LPMs studies human decision making 4209 experimentally demon strate humans decisions lexicographic reasoning instead mathematically sophisticated meth ods linear additive value maximization 6 Previous work learning LPMs set preference observations limited autocratic approaches possible consistent LPMs picked heuristically future decisions However highly likely autocratic methods produce poor approximations target observations In paper present democratic approach LPM learning commit single LPM Instead approximate target preference votes collection consistent LPMs We present variations method variable voting model voting Variable voting operates variable level samples consistent LPMs implicitly The learning algorithm based variable voting learns weak order variables linearization corresponds LPM consistent observations Model voting explicitly samples consistent LPMs Corresponding author Email addresses fusunbbncom F Yaman twalshcsarizonaedu TJ Walsh mlittmancsrutgersedu ML Littman mariedjcsumbcedu M desJardins 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201011012 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1291 employs weighted vote weights computed Bayesian priors The additional complexity votingbased algorithms compared autocratic methods tolerable algorithms loworder polynomial time complexity Our experiments democratic algorithms outperform average worstcase performance state oftheart autocratic algorithm We investigate effect imperfect data learning algorithms We consider kinds imperfections faulty observations noise hidden ties ties broken arbitrarily Our empirical evaluation demonstrates algorithms consider robust presence hidden ties However small number faulty observations signiﬁcantly reduce performance voting algorithms On hand greedy algorithm resilient performance decline proportional noise data We lesson adapting voting methods consider noise environment empirically resulting heuristic par greedy approach case noisy observations To improve performance learning algorithms number observations small introduce intuitive powerful form background knowledge The background knowledge deﬁnes equivalence classes variables indicating important set variables second important set This representation permits user designer provide partial information LPM class LPMs learner reduce search space We demonstrate background knowledge variable model voting improves performance signiﬁcantly especially number observations small In rest paper background LPMs Section 2 votingbased methods Sec tion 3 After introducing methods case preferred values attributes known present extensions algorithms case preferred values known priori Section 4 We introduce background knowledge representation generalize voting methods exploit background knowl edge Section 5 present approach handling noisy data Section 6 present experimental results work Section 7 Finally present related work Section 8 discuss future work conclusions Section 9 2 Lexicographic preference models In section brieﬂy introduce lexicographic preference model LPM summarize previous results learn ing LPMs In work consider binary variables domain 0 11 For clarity introduction algorithms assume preferred value variable known This assumption removed Section 4 Without loss generality assume 1 preferred 0 Given set variables X X1 Xn object A X vector form x1 xn We use notation A Xi refer value Xi object A A lexicographic preference model L X total order subset R X We denote total order cid2L Any variable R relevant respect L similarly variable I X R irrelevant respect L If variable A appears earlier total order B A B A said important smaller rank B If A B objects preferred object given L determined follows Find smallest important variable X cid2L X different values A B The object value 1 X preferred If relevant variables L value A B objects equally preferred tie Example 1 Suppose X1 X2 X3 total order deﬁned LPM L consider objects A 1 0 1 1 B 0 1 0 0 C 0 0 1 1 D 0 0 1 0 A preferred B A X1 1 X1 important variable L B preferred C B X2 1 objects value X1 Finally C D equally preferred values relevant variables An observation o A B ordered pair objects connoting A preferred B In practical applications preference observations gathered demonstration expert breaks ties arbitrarily That presented situation decision choice expert judges alternatives equally good expert fact indifferent equally likely choose alternative Thus observations A B actually tied preference order determine directly observations Therefore LPM L said consistent observation A B iff L implies A preferred B A B equally preferred The problem learning LPM deﬁned follows Given set observations ﬁnd LPM L consistent observations Previous work learning LPMs limited case variables relevant This assumption entails observation A B A strictly preferred B ties happen irrelevant attributes 1 The representation easily generalized monotonic preferences ordinal variables 1 corresponds preference values increasing order 0 decreasing order shown Yaman desJardins 21 conditional preference networks CPnets 1292 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 Algorithm 1 greedyPermutation Require A set variables X set observations O Ensure An LPM consistent O exists 1 1 n 2 Arbitrarily pick X j X MISS X j O min Xk X MISS Xk O Rank X j assign rank X j Remove X j X Remove observations A B O A X j cid4 B X j 5 6 Return total order cid2 X Xi X j iff Rank Xi Rank X j 4 3 The best published algorithm learning LPMs observations presented Schmitt Martignon 16 proposed greedy variablepermutation algorithm guaranteed ﬁnd LPMs consistent observations exists They shown noisy data case ﬁnding LPM violate constant number observations NPcomplete We use greedy algorithm shown Algorithm 1 performance baseline The algorithm refers function MISS Xi O deﬁned A B O B Xi preferred A Xi number observations violated O important variable selected Xi Basically algorithm greedily constructs total order choosing variable step causes minimum number inconsistencies observations If multiple variables minimum chosen arbitrarily The algorithm runs polynomial time speciﬁcally O n2m n number variables m number observations Dombi et al 7 shown n variables relevant O n log n queries oracle suﬃce learn LPM Furthermore possible learn LPM O n2 observations pairs differ variables They proposed algorithm ﬁnd unique LPM induced observations In case noise irrelevant attributes ties reported arbitrarily algorithm return answer In net section investigate following problem Given set observations noise possibly arbitrarily broken ties ﬁnd rule predicting preferences agrees target LPM produced observations Later paper relax assumption permit noisy data Section 6 3 Voting algorithms We propose democratic approach approximating target LPM produced set observations Instead ﬁnding consistent LPMs reasons collection LPMs consistent observations Given objects approach prefers majority models prefer A naive implementation voting algorithm enumerate LPMs consistent set observations However number models consistent set observations exponential naive implementation infeasible In section methodsvariable voting model votingthat sample set consistent LPMs use voting predict preferred object Unlike existing algorithms learn LPMs methods require variables relevant observations tiefree The following subsections explain variablevoting model voting methods summarize theoretical results 31 Variable voting Variable voting uses generalization LPM representation Instead total order variables variable voting reasons weak order2 cid2 ﬁnd preferred object given pair Among variables differ objects ones smallest rank salient weak order vote choose preferred object The object 1 values voting variables declared preferred If votes equal objects equally preferred Deﬁnition 1 Variable voting Suppose X set variables cid2 weak order X Given objects A B variablevoting process respect cid2 determining objects preferred Deﬁne D set variables differ A B Deﬁne D Deﬁne N A number variables D number variables D favor B set variables D smallest rank D respect cid2 If N A N B A preferred If N A N B B preferred Otherwise equally preferred favor A value 1 A 0 B N B 2 A weak order asymmetric reﬂexive transitive order In words weak order deﬁnes ordering sets objects set objects unordered respect F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1293 Algorithm 2 learnVariableRank Require A set variables X set observations O Ensure A weak order X 1 Π x 1 x X 2 Π changed iteration Every observation A B O 3 4 5 6 7 8 9 x D y D Π x cid2 Π y Ax 1 Bx 1 D x Ax cid4 Bx D V A x D V B x D VariableVote predicts preferred object based V A V B x V B Π x X Π x Π x 1 10 11 Return weak order cid3 X x cid3 y iff Π x Π y Table 1 The rank variables iteration forloop line 3 algorithm learnVariableRank Observations Initially 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 X1 1 2 2 2 X2 1 1 1 1 X3 1 1 1 1 X4 1 2 2 3 X5 1 2 2 3 Example 2 Suppose cid2 weak order X2 X3 X1 X4 X5 Consider objects A 0 1 1 0 0 B 0 0 1 0 1 D X2 X5 D X2 X2 smallest ranking variable D respect cid2 X2 favors A A X2 1 Thus variable voting cid2 prefers A B Algorithm 2 presents algorithm learnVariableRank learns weak order cid2 variables set observations variable voting respect cid2 correctly predict preferred objects observations Speciﬁcally ﬁnds weak orders deﬁne equivalence classes set variables The algorithm maintains min imum possible rank variable violate observation respect variable voting Initially variables considered equally important rank 1 The algorithm loops set observations ranks converge At iteration pair variable voting predicts winner allows use algorithm onlinelearning setting examples O need classiﬁed learning process Regardless pre diction ranks variables voted wrong object incremented reducing importance Finally algorithm builds weak order cid2 based ranks x cid2 y x lower rank y In oﬄinelearning setting O set training examples weak order given directly variable voting classify examples test set Example 3 Suppose X X1 X2 X3 X4 X5 O consists 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 Table 1 illustrates ranks variable X iteration forloop line 3 algorithm learnVariableRank The ranks variables stay second iteration loop loop terminates The weak order cid2 based ranks variables order given Example 2 We summarize theoretical results algorithm learnVariableRank Correctness Suppose cid2 weak order returned learnVariableRank X O Any LPM L based corresponding topo logical sort cid2L cid2 consistent observation set O This proven simply contradiction Suppose observation oi existed majority variables existing class led incorrect classiﬁcation oi based returned weak order cid2 Since learning algorithm loops observations process resulted increment value algorithm completed current cid2 LPM consistent cid2 consistent O Furthermore learnVariableRank increments ranks relevant variables actual rank target LPM This seen considering cases relevant ir relevant variables First relevant variables number times variable set variables actually vote potentially incremented votes incorrectly simply true rank t Once reaches true rank deﬁnition vote incorrectly variable vote correctly values tied true rank Second ranks irrelevant variables incremented far number variables algorithm guaranteed terminate presence irrelevant variables 1294 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 In onlinelearning setting O incrementally fed learner oﬄine setting O pro Convergence vided batch learnVariableRank mistakebound O n2 n number variables To consider cases given observation ot time t assuming ease exposition A preferred B First V A V B mistake variables rank increased voted wrong way The second case V A V B case mistake need limit number cases Ignoring increments ﬁrst case mistake increases sum potential ranks 1 sum ranks target LPM induces O n2 second case occur O n2 times This bound guarantees given observations described background section learnVariableRank converge weak order cid2 conjunction variable voting consistently classiﬁes preferred objects respect target LPM Furthermore incrementing ranks case 1 gives stronger result mentioned topological sort cid2 preﬁx total order induced target LPM If variables relevant cid2 converge total order induced target LPM Computational complexity We consider computational complexity learnVariableRank oﬄinelearning setting O provided training set A loose upper bound time complexity learnVariableRank O n3m n number variables m number observations This bound holds whileloop line 2 runs O n2 times max sum maximum possible ranks forloop line 3 runs m observations deﬁnition The time complexity iteration forloop O n variables need considered worst case overall complexity O n3m We leave investigation tighter bounds improved data structures average case analysis future work 32 Model voting The second method present employs Bayesian approach This method randomly generates sample set S distinct LPMs consistent observations When pair objects presented preferred predicted weighted voting That L S casts vote object prefers vote weighted according posterior probability P LS Deﬁnition 2 Model voting Let U set LPMs O set observations S U set LPMs consistent O Given objects A B model voting prefers A B respect S cid2 cid2 P LSV L AB P LSV L B A 1 LU L AB 1 A preferred respect L 0 V V posterior probability L target LPM given S calculated discussed LU L B A deﬁned analogously P LS We ﬁrst assume LPMs equally likely priori In case given sample LPMs S size k posterior probability LPM L 1k L S 0 Note S maximal case degener ates naive voting algorithm However generally feasible enumerate consistent LPMsin practice sample small feasible large representative In constructing S exploit fact consistent LPMs share preﬁxes total order deﬁne variables We wish discover compactly represent LPMs To end introduce idea aggregated LPMs An aggregated LPM X1 X2 Xk represents set LPMs deﬁne total order preﬁx X1 X2 Xk Intuitively aggregated LPM states possible completion preﬁx consistent observations The algorithm sampleModels Algorithm 3 implements smart sampling approach constructing LPM consistent given observations returning aggregated LPM possible We start arbitrary consistent LPM set consistent add variable orderings extending input LPM We ﬁrst identify variables extending preﬁxthat variables Xi observation Xi 1 preferred object Xi objects We select variables randomly extend preﬁx Finally remove observations explained selection continue rest observations If point observations remain return aggregated form preﬁx completion preﬁx consistent null observation Running sampleModels times eliminating duplicates produce set possibly aggregated LPMs Example 4 Consider set observations O Example 3 Then aggregated LPMs consistent O follows X2 X2 X3 X2 X3 X1 X3 X3 X1 X3 X2 X3 X2 X1 To illustrate set LPMs aggregate LPM represents consider X2 X3 X1 total 5 extensions X2 X3 X1 X2 X3 X1 X4 X2 X3 X1 X5 X2 X3 X1 X4 X5 X2 X3 X1 X5 X4 Every time algorithm sampleModels runs set observations O Example 3 randomly generate aggregated LPMs X2 X3 X1 X3 X1 X3 X2 X1 Note shorter models produced sampleModels subpreﬁxes aggregated LPMs easy modify sampleModels return models F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1295 Algorithm 3 sampleModels Require A set variables X set observations O rulePreﬁx LPM extended Ensure An LPM possibly aggregated consistent O 1 candidates set variables Y Y rulePreﬁx A B O AY 1 AY BY 2 candidates cid4 3 O return rulePreﬁx 4 5 6 7 Randomly remove variable Z candidates Remove observation C D O C Z cid4 D Z Extend rulePreﬁx rulePreﬁx rulePreﬁx Z Recompute candidates 8 9 return rulePreﬁx Table 2 The posterior probabilities number votes LPMs Example 5 LPMs X2 X2 X3 X2 X3 X1 X3 X3 X1 X3 X2 X3 X2 X1 P LS 1 131 131 131 531 131 1631 131 531 P LS 2 0 0 0 526 0 1626 0 526 N L AB 0 1 1 5 0 7 1 5 N L B A 0 0 0 0 0 7 0 0 An aggregate LPM sample saves having enumerate possible extensions preﬁx introduces complications computing weights posteriors LPMs votes For example comparing objects A B extensions aggregate LPM vote A B Thus need ﬁnd total number LPMs aggregate LPM represents determine proportion favor A B vice versa enumerating extensions Suppose n variables L aggregated LPM preﬁx length k Then number extensions L denoted FL equal fnk fm deﬁned fm mcid2 i0 cid3 cid4 m mcid2 i0 m m 2 Intuitively fm counts possible permutation m items Note fm computed eﬃciently number possible LPMs n variables given fn While formula calculates total number extensions need determine votes aggregate LPM L X1 X2 Xk allocate compared objects A B We variables X1 Xk preﬁx variables If A B different values preﬁx variable extensions vote accordance smallest variable Suppose preﬁx variables tied m set nonpreﬁx variables Then m composed disjoint sets b w set variables favor A b set variables favor B w set variables neutral value A B An extension Lcid8 L produce tie iff variables b irrelevant Lcid8 The number extensions fw The number extensions favor A B directly proportional aa b Therefore number L extensions L vote A B denoted N AB N L AB b fm fw The number extensions L vote B A computed similarly Note computation N FL linear time caching recurring values 3 L AB N L B A Example 5 Suppose X O deﬁned Example 3 The ﬁrst column Table 2 lists LPMs sistent O The second column gives posterior probabilities models given sample S 1 set consistent LPMs The column posterior probability models given sample S 2 X2 X3 X1 X3 X1 X3 X2 X1 Given objects A 0 1 1 0 0 B 0 0 1 0 1 number votes object based LPM given columns Note total number votes A B add total number extensions X3 X1 extensions X3 X1 X3 X1 X4prefer A B equally Algorithm 4 describes modelVote takes sample consistent LPMs produced instance sampleModels pair objects input predicts preferred object weighted votes LPMs sample 1296 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 Algorithm 4 modelVote Require A set LPMs S objects A B Ensure Returns A B tie 1 Initialize sampleSize number nonaggregated LPMs S 2 aggregated LPM L S sampleSize F L 3 4 Vote A 0 VoteB 0 5 LPM L S 6 L aggregate rule 7 8 9 10 11 12 13 14 15 winner object L prefers A B Increment Votewinner 1sampleSize A B differ preﬁx variable L prefers A B L extension L winner object L Votewinner F LsampleSize Vote A N L VoteB N L AB sampleSize B AsampleSize 16 17 Vote A VoteB 18 19 20 Return tie Return object obj highest Voteobj Returning Example 5 reader verify model voting prefer A B Next present theoretical results sampleModels modelVote algorithms Complexity The time complexity sampleModels bounded O n2m n number variables m number observations whileloop line 2 runs n times worst case variable needs removed time candidates At iteration process observation time performing computations O n time If sampleModels s times generate sample size s total complexity sampling O sn2m For constant s s bounded polynomial function relevant quantities n m bound polynomial Similarly complexity modelVote O sn considers s rules sample counting votes rule O n time Comparison variable voting The set LPMs sampled learnVariableRank subset LPMs sampleMod els produce cases relationship strict modelslearnVariableRank modelssampleModels For inclusion sampleModels aggregates considers possible models variable considered location LPM recursively Thus LPMs consistent weak order returned learnVariableRank subset models The strictness shown running example paper demonstrates sampleModels generate LPM X3 X1 extensions consistent weak order returned learnVariableRank 4 Learning preferred attribute values In previous sections assumed preferred value binary variable known order importance variables needed learned In section generalize deﬁnition LPM explicitly state preferred value relevant variable The motivation generalization preferred value variable known priori For example meal preference learning situation different groups people prefer different values spicy variable To represent larger model space use pair literals represent variable similar trick learning Boolean formulae binary variables There literals l based variable X variable X positive literal negation X negative literal Given set variables V let LV set literals based variables V A generalized LPM L LV total order subset R LV R contain positive negative literal based variable If A B objects preferred object given L determined follows Find smallest literal l L X variable l based A X B X different values If l positive literal object value 1 X preferred object value 0 X preferred If relevant variables L value A B objects equally preferred tie F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1297 Algorithm 5 genLearnVariableRank Require A set variables X set observations O Ensure A weak order literals based variables X 1 Π x 1 Π x 1 x X 2 Π change 3 observation A B O 4 5 6 7 8 x D y D Π x cid2 Π y D set literals based variables differ A B D V A set positive negative literals D V B set positive negative literals D x V B Π x X 1 1 0 A 1 0 B Π x Π x 1 9 10 Return weak order cid3 X x cid3 y iff Π x Π y Example 6 Suppose X1 X2 X3 total order deﬁned generalized LPM L consider objects A 1 0 1 1 B 0 1 0 0 C 0 0 1 1 B preferred A B X1 0 X1 important literal L B preferred C B X2 1 objects value X1 Next adapt voting algorithms described previous sections learn generalized LPMs 41 Generalized variable voting We adapt deﬁnition variable voting Deﬁnition 1 similar way LPM generalization Essentially need deﬁne weak order cid2 set literals instead set variables We need modify way count votes N A N B voting literals positive negative literals vote object 1 0 variable literal based To avoid repetition formally deﬁne generalized variable voting following example demonstrates new votecounting procedure Example 7 Suppose cid2 weak order X2 X3 cid2 X1 X2 X3 X4 X5 cid2 X1 X4 X5 Consider objects A 1 0 1 1 0 B 0 0 1 0 1 The literals based variables different A B D X1 X1 X4 X4 X5 X5 The literals vote X1 X4 X5 smallest ranking variables D respect cid2 X1 votes A X1 positive literal A X1 1 Similarly X4 votes A X5 votes B negative literal B X5 0 Therefore variable voting cid2 prefers A B As previous example demonstrates literal votes complement smaller rank Furthermore literal complement ranking votes cancel affect preference decision We note case positive negative literals appear variable ranking special care taken constructing LPM weak ordering topic return end section Algorithm 5 presents algorithm genLearnVariableRank Given set observations algorithm learns ranking literal returns weak order cid2 subset literals Generalized variable voting outlined respect cid2 correctly predict preferred objects observations The genLearnVariableRank algorithm similar learnVariableRank major difference genLearnVariableRank ranking function Π possible literals updated prediction wrong correct unanimous The rank literal incremented number variables Example 8 Suppose X X1 X2 X3 X4 X5 O consists 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 Table 3 illustrates ranks literal based variables X iteration forloop line 3 algorithm genLearnVariableRank The whileloop line 2 algorithm terminates iter ations The algorithm genLearnVariableRank returns weak order X2 X3 cid2 X1 X2 X3 X4 X5 cid2 X1 X4 X5 The asymptotic bounds complexity convergence learnVariableRank hold genLearnVariableRank However correctness relationship weak order cid2 returned genLearnVariableRank generalized LPMs consistent observations needs revised Speciﬁcally topological sort cid2 valid generalized LPM contain literals negations To correct problem simply discard literal higher rank opposite pair literals appears bin discard A topological sort performed resulting weak order produce generalized LPM L Any LPM produced manner consistent O discarded variables learning algorithm inﬂuence prediction 1298 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 Table 3 The rank literals iteration forloop line 3 algorithm genLearnVariableRank Observations X1 X2 X3 X4 X5 X1 X2 X3 X4 X5 Initially 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2 2 3 1 1 1 2 2 2 3 1 1 2 2 2 2 2 1 2 2 2 2 2 2 1 1 1 2 2 2 3 1 1 2 2 2 2 2 Algorithm 6 genSampleModels Require A set variables X set observations O rulePreﬁx LPM extended Ensure An LPM possibly aggregated consistent O set positive literals Y Y Y rulePreﬁx A B O AY 1 AY BY set negative literals Y Y Y rulePreﬁx A B O AY 0 AY BY 1 candidates 2 candidates 3 candidates candidates 4 candidates cid4 5 O return rulePreﬁx candidates 6 7 8 9 Randomly remove variable Z candidates Remove observation C D O C Z cid4 D Z Extend rulePreﬁx rulePreﬁx rulePreﬁx Z Recompute candidates 10 11 return rulePreﬁx 42 Generalized model voting The modiﬁcations model voting considered parts First need extend algorithm sampleModels produce generalized LPMs Algorithm 6 presents algorithm genSampleModels operates level literals returns possibly aggregated generalized LPMs Similar sampleModels positive literal X considered candidate rule extension observation X 1 preferred object objects A negative literal X candidate observation X 0 preferred object objects Note produce valid LPM need ensure preﬁx contains literal based variable Second need generalize counting model extensions aggregate LPMs distribution votes comparing objects The number extensions L denoted FL equal fnk n set variables literals based k length preﬁx L fm redeﬁned fm mcid2 cid4 cid3 m i0 2i mcid2 i0 m 2i m 4 The new deﬁnition combination literals positive negative variable fm extra 2i term inside summation extensions ﬁxed variables include Now consider pair objects A B aggregated generalized LPM L As A B different values preﬁx literal L extensions vote accordance smallest literal However preﬁx variables tied generalized model voting votes divided equally equal number extensions positive negative literals based rest variables Thus algorithm generalized model voting modelVote ﬁrst line algorithm genSampleModels lines 14 16 compute distribution votes aggregate LPMs preﬁx variables tied deleted 5 Introducing background knowledge In general training examples learning algorithm space consistent LPMs large In case possible ﬁnd good approximation target model To overcome problem introduce background knowledge indicating certain solutions favored In section propose form background knowledge consisting equivalence classes set attributes These equivalence classes indicate set important attributes second important attributes For example buying car people consider important attributes car mileage year car The second important set attributes color number doors body type Finally important properties interior color wheel covers Throughout section assume preferred value F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1299 variable known given background knowledge We formally deﬁne representation background knowledge means LPM consistent background knowledge Deﬁnition 3 Background knowledge The background knowledge B learning lexicographic preference model set variables X weak order total order partition X B form E 1 E2 Ek E X B deﬁnes weak order X variables x E y E j x y iff E E j We denote weak order cid2B cid5 Deﬁnition 4 Suppose X X1 Xn set variables B background knowledge L LPM L consis tent B iff total order cid2L consistent weak order cid2B Intuitively LPM consistent background knowledge B respects variable orderings induced B The background knowledge prunes space possible LPMs The size partition determines strength B example single variable set B deﬁnes speciﬁc LPM In general number LPMs consistent background knowledge form E 1 E2 Ek computed following recursive formula cid7 cid6 cid6 cid6 cid7 e1 ek G f e1 e1 G cid7 e2 ek 1 5 ei E base case recursion G 1 The ﬁrst term formula counts number possible LPMs variables E1 important variables The deﬁnition consistency entails variable appear cid2L iff important variables cid2L term e1 Note recursion G limited number sets partition bounded number variables computed linear time caching precomputed values f To illustrate potential power background knowledge consider learning problem variables Without background knowledge total number LPMs 905970 If background knowledge B partitions variables sets elements number LPMs consistent B 646 If B sets ﬁrst set variables rest limits number 190 We easily generalize learnVariableRank algorithm utilize background knowledge changing ﬁrst line learnVariableRank initializes ranks variables Given background knowledge form S 1 Sk generalized algorithm assigns rank 1 important rank variables S 1 rank S1 1 S2 forth This initialization ensures observation A B learning order variables class S A B values variables classes S1 S i1 different values variable S The algorithm modelVote generalized use background knowledge B In sample generation phase use sampleModels presented earlier eliminate rules preﬁxes consistent B Note preﬁx aggregated LPM L consistent B case extension L Thus LB algorithm modelVote need change references FL N B A respectively L B A F LB AB N L AB N B L N F N B L number extensions L consistent B LB AB number extensions L consistent B prefer A N LB B A analogous Suppose B given E1 Em Let Y denote preﬁx variables aggregate LPM L let Ek ﬁrst set variable Ek Y Then F B L GEk Y Ek1 Y Em Y When counting number extensions L consistent B prefer A need examine case preﬁx variables equally prefer objects Suppose Y deﬁned D denotes set difference E Y Let D j ﬁrst nonempty set Dk ﬁrst set variable Dk different values objects Obviously variables Dk inﬂuence prediction preferred object If di D cardinality D set variables Dk favor A b set variables Dk favor B w set variables Dk neutral N LB AB number extensions L consistent B prefer A computed follows N LB AB b cid6 B L G F cid6cid8 d j dk1 w cid9cid7cid7 6 1300 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 6 Handling noisy data Although inferring LPM noisy data NPcomplete 16 moderate empirical success greedy algorithm seen Section 77 intuition heuristic solution voting developed Speciﬁcally greedy approach iteratively constructed LPM added attribute violated fewest number observations We borrow intuition build heuristic extensions learnVariableRank modelVote If expected number noisy observations cid3 data set provided new algorithm NoiseAware Model Vote NAMV changes sampleModels consider variable candidate adding violate total variables cid3 observations Notice remains stochastic LPM construction consider LPMs greedy approach Following similar path develop noiseaware version learnVariableRank ranks variables updated cid3 observations mispredicted If know expected noise observations employ hybrid approach greedyPermutation modelVote greedyVote simply replacing sampling algorithm sampleModels greedyPermutation In losing advantage aggregate LPMs greedyPermutation produces single LPM conﬁning samples ones generated minimum mistakes heuristic We investigate resilience extensions empirically Section 77 7 Experiments In section explain experimental methodology discuss results empirical evaluations We deﬁne prediction performance algorithm P respect set test observations T performanceP T CorrectP T 05 TieP T T 7 CorrectP T number observations T predicted correctly P including prediction t T t actually tie TieP T number observations T P predicted tie object actually preferred Note LPM returned greedyPermutation returns tie In contrast variable voting respect weak order variable equally important return ties overall performance 05 better randomly selecting preferred objects We use MV VV G denote model voting variable voting greedy approximations LPM Similarly use NAMV GV denote NoiseAware Model Vote greedyVote Given sets training test observations O T measure average worst performances VV MV G When combined learnVariableRank VV deterministic algorithm average worst performances VV However case MV sampling sampleModels randomized Even training test data O T performance MV vary To mitigate effect ran MV 10 times O T pair called sampleModels S times run sample size S recording average worst performance The greedy algorithm G randomized line 2 variable picked arbitrarily ran G 200 times O T recording average worst performance In ﬁgures data points averages 20 runs different O T We employed twotailed Ttest 95 conﬁdence interval test signiﬁcance In discussion results applicable note statistically signiﬁcant differences For experiments control variables R number relevant variables target LPM I number irrelevant variables N O number training observations N T number test observations For MV experiments sample size S control parameter For ﬁxed values R I LPM L randomly generated If background knowledge B given L consistent B Unless noted Section 75 preferred value attribute 1 preferred value attribute chosen randomly We randomly generated N O N T pairs objects I R variables Finally labeled preferred objects according L In order allow researchers replicate results posted data ﬁles scripts data generation web httpmaplecsumbceduLPM 71 Comparison MV VV G Fig 1a shows average performance G MV sample size S 200 VV R 15 I 0 N T 20 N O ranges 2 20 Fig 1b shows worst performance algorithm In ﬁgures data points averages 20 different pairs training test sets O T The average performance VV MV better average performance G difference signiﬁcant data point Also note worstcase performance G seeing observations 03 suggests poor approximation target VV MVs worstcase performance better worstcase performance G justifying additional complexity algorithms MV VV F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1301 Fig 1 Average prediction performance b worst prediction performance greedy algorithm variable voting model voting Fig 2 The average worst case prediction performance greedyVote compared model voting variable voting greedy approaches noisefree data 72 Greedy voting Even proposed greedyVote noiseaware adaptation model vote investigated performance noisefree case For experiment data set control variables explained 71 In addition sample size GV set 200 MV Fig 2 contains average performance MV VV G worst performance MV reported Fig 1a b Fig 2 demonstrates average worst case performance GV Just like MV GV randomized algorithm data set ran GV times average runs prediction performance An interesting result GVs average performance close VVs performance worst case performance average performance G Therefore virtue votingbased algorithm GV demonstrates better worst case performance greedy algorithm worst case performance signiﬁcantly worse MV VV We believe behavior occurs GV uses greedy approach sampling space consistent LPMs reﬂected tight overlap worst GV average G performances 1302 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 Fig 3 Left The average prediction performance right worst prediction performance model voting different sample sizes 73 Effect sample size MV performance Fig 3 shows worst average prediction performance MV sample sizes S 10 S 50 S 200 S 1200 problems 10 relevant variables R 10 irrelevant variables I 0 The number observations No increases 2 20 xaxis In general sample size increases prediction performance increases The effect sample size worstcase performance evident average performance 74 Irrelevant variables Irrelevant variables hamper pruning space possible LPMs Of votingbased algorithms MV ability ignore variables early sampleModels produces LPMs use variables VV hand operates entire set variables given observations eventually discovers irrelevant ones In experiments compared average performance MV VV cases total number attributes constant number irrelevant attributes varied Our results showed algorithms robust presence irrelevant variables Furthermore test cases number irrelevant variables dominated number relevant variables total number attributes small performance algorithms improved case relevant variables appeared That things equal algorithms easier learn ignore irrelevant variable use relevant Fig 4 depicts cases The graph left shows average prediction performance MV VV R 2 I 3 R 5 I 0 The data sets fewer relevant variables learned easily The right shows results comparison pattern results attributes R 3 I 12 R 15 I 0 We seen pattern similar situations Note graphs model voting outperforming variable voting observed difference statistically signiﬁcant Our experiments indicate algorithms robust presence irrelevant variables achieving high accuracy values relatively samples 75 Learning preferred variable values We implemented generalized versions MV VV refer gMV gVV respectively We tested performance gMV gVV different data sets The ﬁrst data set comparing MV VV R 15 I 0 variables 1 preferred 0 Fig 5a shows average worst performance MV VV A quick comparison Fig 1 reveals decreased prediction performance statistically signiﬁcant This expected result given increase search space The second data set R 5 I 10 preferred value variables chosen randomly Fig 5b demonstrates performance MV VV similar pattern Fig 5a 76 Effect hidden ties Fig 6 shows prediction performance VV G average worst case problems 10 relevant variables R 10 ﬁve irrelevant variables I 5 The number observations No 50 number observations hidden ties varies 0 45 xaxis F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1303 Fig 4 Average MV performance VV performance different numbers relevant irrelevant variables Fig 5 Comparison gMV gVV data set 15 relevant variables irrelevant variables 1 preferred value b Comparison gMV gVV data set 5 relevant 10 irrelevant variables preferred value attribute chosen randomly In general number hidden ties increase observations prediction performance degrades This result expected number useful observations help algorithms learn ranking relevant attributes decreases number hidden ties increases The performance MV data sets similar VV omitted readability A interesting result existence hidden ties data actually improves performance algorithms compared smaller dataset hidden tie observations omitted ﬁltered versions Fig 6 Our explanation phenomenon hidden ties provide useful information learning order relevant attributes existence helps algorithms identify irrelevant attributes hidden ties increase number mistakes caused irrelevant attributes push ranking decreasing importance allowing observations clarify ordering identiﬁed relevant attributes 1304 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 Fig 6 The prediction performance VV G varying number hidden ties 50 observations The results ﬁltered VV G obtained eliminating hidden ties observations 77 Effect noise Fig 7 shows average prediction performance MV G problems 10 relevant variables R 10 ﬁve irrelevant variables I 5 The total number observations No 50 number observations faulty varies 0 45 xaxis Fig 8 shows worst performance setting The results average worst performance MV operates assumption noisefree data signiﬁcantly compromised small amounts noise interprets refutation correct model correct models The asymptotic performance 05 reﬂects fact noise causes MV eliminate models version space causing predict tie testing observation essentially making random selection algorithm We omitted results VV ﬁgure VVs behavior closely resembled MV The performance G decays far gracefully MV VV G allows observations discarded In Figs 7 8 compare average worst performance NAMV GV algorithms presented paper including ﬁltered versions noisy data omitted provided baselines Notice unlike original modelVote confounded small noise gentle decays NAMV GV mirror greedy approachs robustness noise perform comparably data set Asymptotically observations noisy NAMV performs better modelVote average case statistically signiﬁcant result eliminate possible models instead defaulting random selection favors test observation 1s Among noise adaptations MV GV demonstrates best worstcase performance noise levels 10 25 difference GV NAMV statistically signiﬁcant 78 Effect background knowledge performance Fig 9 shows positive effect incorporating background knowledge performance voting algorithms I 0 N T 20 N O ranges 2 20 In addition experiment aims background R 10 knowledge undermine advantage voting algorithms held greedy algorithm knowledge free case To end trivially generalized G produce LPMs consistent given background knowledge B The data points averages 20 different pairs training test sets O T We arbitrar ily picked weak orderings use background knowledge B1 X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 B2 X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 The performance VV improved greatly introduction ordering background knowledge B2 stronger B1 prunes space consistent LPMs B1 As result performance gain B2 greater B1 The difference perfor mance background knowledge background knowledge statistically signiﬁcant point Note background knowledge particularly effective number training observations small The worstcase performance G background knowledge B1 B2 shown Fig 9 In cases worstcase performance G signiﬁcantly lower performance VV corresponding background knowledge Using experimental scenario obtained similar results MV seen Fig 10 In summary worst case performance greedy algorithm background knowledge B2 outperforms average performance MV background knowledge However weaker knowledge B1 average performance MV better G B2 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1305 Fig 7 The average prediction performance model voting greedy NAMV GV number noisy observations increases Fig 8 The worst prediction performance model voting greedy NAMV GV number noisy observations increases 8 Related work The concept lexicographic comparisons commonplace everyday life Expressions safety ﬁrst harm quality job evoke lexicographic preferences Lexicographic rankings sporting events For example countries competing Olympics typically ranked total medals gold medals silver medals 17 The winner Netﬂix prize chosen ranking submissions ﬁrst minimum test error earliest submission 13 Lexicographic utilities applied understanding human preferences 1911 They extensive mathemat ical foundation studied economics psychological management science literature 8 Lexicographic orders preference models utilized research areas including multicriteria optimization 1 linear programming 5 game theory 14 The relevant existing work learning andor approximating LPMs Schmitt Martignon 16 Dombi et al 7 summarized Section 2 1306 F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 Fig 9 The effect background knowledge VV G arbitrarily selected weak orderings background knowledge B2 stronger constraining B1 Fig 10 The effect background knowledge average MV performance arbitrarily selected weak orderings background knowledge B2 stronger B1 In general preferences ranking similar The ranking problem described Cohen et al 3 similar problem learning LPM However line work poses learning optimization problem goal ﬁnding single ranking maximally agrees given preference function In particular approach constructs collection domainspeciﬁc ranking experts predictions combined model voting scheme The voting concept similar spirit approach underlying representations different Torrey et al 18 employ inductive logic programming approach learn multiattribute ranking rules In principle rules represent lexicographic preference models Fürnkranz Hüllermeier 10 investigate pairwise preference learning ranking problem The observations set partially ranked objects goal learn rank new set objects Their approach reduce original problem number binary classiﬁcation problems pair labels Hence assumptions underlying preference model Boutilier et al 2 consider preference learning algorithm representation CPnets modeling preferences ceteris paribus equal assumption However representation necessarily capture lexicographic preference models directly applicable problem considered Another analogy described Schmitt Martignon 16 LPMs decision lists 15 Speciﬁcally shown LPMs special case 2decision lists algorithms learning classes models directly applicable 9 Conclusions future work In paper presented democratic approximation methods learning lexicographic preference models LPMs given set preference observations Instead committing consistent LPMs maintain set models predict based majority votes We described methods variable voting model voting We showed methods implemented polynomial time exhibit better worst averagecase F Yaman et al Artiﬁcial Intelligence 175 2011 12901307 1307 performance existing methods Finally deﬁned form background knowledge improve performance number observations small incorporated background knowledge votingbased methods signiﬁcantly improving empirical performance Future directions work allow number extensions theoretical investigations Many theoretical bounds presented paper tightened complexity bound learnVariableRank potentially deﬁned terms simpler parameters sample size parameter model vote maintain ing performance guarantees We recently extended basic LPM representation learning techniques support contextdependent preferences form branching LPMs including methods learning branching LPMs noisy data 12 We continuing investigate heuristics like NAMV greedyVote robust noise While problem learning LPMs noisy data NPcomplete superior performance voting algo rithms greedy method noisefree case indicates possible identify characterize restricted problem settings heuristic extensions NAMV greedyVote signiﬁcantly outperform stateoftheart greedy approach Acknowledgements This work supported Defense Advanced Research Projects Agency US Air Force BBN Tech nologies Corp contract number FA865006C7606 References 1 D Bertsekas J Tsitsiklis Parallel Distributed Computation Numerical Methods Athena Scientiﬁc 1997 2 C Boutilier RI Brafman C Domshlak HH Hoos D Poole CPnets A tool representing reasoning conditional ceteris paribus preference statements Journal Artiﬁcial Intelligence Research 21 2004 135191 3 W Cohen R Schapire Y Singer Learning order things Journal Artiﬁcial Intelligence Research 10 1999 243270 4 AM Colman JA Stirk Singleton bias lexicographic preferences equally valued alternatives Journal Economic Behavior Organiza tion 40 4 December 1999 337351 5 G Dantzig A Orden P Wolfe The generalized simplex method minimizing linear form linear inequality restraints Paciﬁc Journal Mathematics 5 1955 183195 6 RM Dawes The robust beauty improper linear models decision making American Psychologist 34 1979 571582 7 J Dombi C Imreh N Vincze Learning lexicographic orders European Journal Operational Research 183 2 2007 748756 8 P Fishburn Lexicographic orders utilities decision rules A survey Management Science 20 11 1974 14421471 9 JK Ford N Schmitt SL Schechtman BM Hults M Doherty Process tracing methods contributions problems neglected research issues Organi zational Behavior Human Decision Processes 43 1989 75117 10 J Fürnkranz E Hüllermeier Pairwise preference learning ranking Proc ECML03 CavtatDubrovnik SpringerVerlag 2003 pp 145156 11 G Gigerenzer DG Goldstein Reasoning fast frugal way Models bounded rationality Psychological Review 103 4 1996 650669 12 J Jones M desJardins Learning branching lexicographic preference models Tech Rep TRCS1005 UMBC Dept CSEE June 2010 13 Netﬂix The Netﬂix prize rules httpwwwnetﬂixprizecomrules 2009 14 A Quesada Negative results theory games lexicographic utilities Economics Bulletin 3 20 2003 17 15 R Rivest Learning decision lists Machine Learning 2 3 1987 229246 16 M Schmitt L Martignon On complexity learning lexicographic strategies Journal Machine Learning Research 7 2006 5583 17 Sports illustrated httpsportsillustratedcnncomolympics2008medalstracker 2008 18 L Torrey J Shavlik T Walker R Maclin Relational macros transfer reinforcement learning Proceedings Seventeenth Conference Inductive Logic Programming Corvallis Oregon 2007 19 A Tversky Intransitivity preferences Psychological Review 76 1969 3148 20 MR Westenberg P Koele Multiattribute evaluation processes methodological conceptual issues Acta Psychologica 87 1994 6584 21 F Yaman M desJardins Moreorless CPnetworks Proceedings 23rd Conference Uncertainty Artiﬁcial Intelligence 2007