Artiﬁcial Intelligence 171 2007 382391 wwwelseviercomlocateartint Perspectives multiagent learning Tuomas Sandholm Carnegie Mellon University Computer Science Department Pittsburgh PA 15213 USA Received 18 May 2006 received revised form 27 February 2007 accepted 27 February 2007 Available online 30 March 2007 Abstract I lay slight reﬁnement Shoham et als taxonomy agendas I consider sensible multiagent learning MAL research It intended rigid senseless work agendas additional sensible agendas arise Within agenda I identify issues suggest directions In computational agenda direct algorithms efﬁcient MAL plays role especially rules game unknown direct algorithms known class games In descriptive agenda emphasis placed establishing classes learning rules actually model learning multiple humans animals Also agenda way circular This positive verify learning models In prescriptive agendas desiderata need clear guide design MAL algorithms The algorithms need mimic humans animals learning I discuss worthy desiderata literature motivated The learning problem interesting cooperative noncooperative settings concerns different For noncooperative settings future work increasingly consider learning strategically Lower bounds cut agendas They derived computational complexity number interactions needed 2007 Elsevier BV All rights reserved Keywords Multiagent learning Learning games Reinforcement learning Game theory 1 Introduction Learning experience key capability able devise good strategy plan possible contingencies advanceeven help computers In multiagent settings learning needed opponents strategies unknown rules game unknown computationally complex solve good strategy means Multiagent learning MAL complicated fact agents learning changing exploration behavior 61 making environment nonstationary learner MAL studied different objectives different restrictions game learners observe This work supported National Science Foundation ITR grants IIS0121678 IIS0427858 Sloan Fellowship Email address sandholmcscmuedu 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200702004 T Sandholm Artiﬁcial Intelligence 171 2007 382391 383 Fig 1 Taxonomy agendas multiagent learning research 2 On agendas multiagent learning In paper If multiagent learning answer question Shoham et al important contribution laying ﬁve agendas MAL I present reﬁnement taxonomy Fig 1 It consistent view adds hierarchy renames normative agenda learning algorithms equilibrium In rest section I comment agendas taxonomy 21 Computational agenda While MAL algorithms principle equilibrium ﬁnding tend signiﬁcantly efﬁcient best direct equilibriumﬁnding algorithms For example MAL algorithms able solve tiny poker games direct techniques able ﬁnd exact equilibrium poker games including game Rhode Island Holdem 31 billion nodes game tree 32 If ones goal equilibrium ﬁnding little reason use MAL algorithms There tremen dous recent progress faster algorithms equilibrium ﬁnding normal form games 5862 graphical games 42 71 sequential games perfect information 64 sequential games imperfect information 3132 Why want advantage efﬁcient direct techniques One settings efﬁcient direct techniques known For example best programs Backgammon developed MAL speciﬁcally Qlearning 69 Also MAL algorithms cases simpler program direct techniques arguably preferable code direct algorithm readily available scalability MAL sufﬁces setting Finally conceivable equilibriumﬁnding problems MALbased algorithms turn efﬁcient For example ﬁnding minimax solution twoplayer zerosum game MAL speciﬁcally noregret learning efﬁcient alternative linear programming game huge number pure strategies payoffs small 2 page 73 Most importantly MAL algorithms needed agents know structure payoffs game goal exploit weak opponent equilibrium strategy exploit oppo nent 22 Descriptive agenda The descriptive agenda MAL grew largely concern people required rationality reasoning capability act according gametheoretic equilibrium This turn calls question descriptive 384 T Sandholm Artiﬁcial Intelligence 171 2007 382391 power game theory To mitigate economists studied MAL techniques lead equilibrium play1 If justiﬁcation equilibrium reached learning require sophisticated gametheoretic reasoning direct equilibriumﬁnding algorithms This simplicity argument equilibrium selection easytolearn equilibria likely reasonable justifyingselecting solution concepts concepts associated convergent learning algorithms descriptive Hu mans animals learning learning yields equilibrium scope descriptive MAL agenda There critical shortcomings descriptive MAL agenda ways pursued date The important shortcoming illdeﬁned counts MAL algorithm agenda descriptive conclusions depend completely algorithms count For example elabo rate work MAL algorithms converge equilibrium However trivial algorithm achieves goal players ﬁrst explore cell payoff matrix order individually computes equilibrium agent algorithm ﬁnd equilibrium case multiple equilibria play equilibrium forever Researchers pursuing descriptive agenda consider straw man MAL algorithm Why Unfortunately clear deﬁnitions algorithms count usually ar ticulated Instead variety algorithms proposed naturalness argued based intuition taste algorithmbyalgorithm basis Since descriptive conclusions hinge algorithms count I think clear deﬁnitions MAL algorithms count The deﬁnitions preferably admit exclude classes algorithmsdeﬁned properties algorithmic stepsrather individual algorithms What algorithms admitted versus excluded guided experiments humans animals population descriptive conclusions drawn Unfortunately gives rise potential circularity agenda order descriptive conclusions humans behave need start understanding humans behave The circularity constructively try verify approach working One observe behavior setting select learning models accordingly test models different setting check descriptive conclusions align observed behavior setting Another downside experimental results context dependent One approach admit deﬁnition algorithms simple opposed complex execute While intuitively appealing suffers difﬁculties First clear simple human reasoning coincide Turing machine model clear complexity measures devised Second MAL algorithms admitted descriptive approach today involve numerous sophisticated calculations Another shortcoming descriptive agenda work assumes people use learning rule better behaving differently learning process In words people reduced assumption following certain learning algorithm guarantees agent follows algorithm eventually equilibrium reached A related shortcoming person assumed follow learning rule behaving differently drive population converge different equilibrium beneﬁcial 23 Prescriptive agendas In prescriptive agendas interested multiple agents learn It largely irrelevant learning algorithms mimic humans animals learn The goal develop learning algo rithms satisfy given desiderata given performance measures It paramount desiderata important speciﬁed clearly upfront This contrast designing MAL algorithm ﬁrst deﬁning idiosyncratic desiderata justify algorithm The commonly desideratum convergence empirical distribution play Nash equilibrium meaningful play far optimal step stability guarantees equilibrium present agents far equilibrium step I desideratum guaranteeing 1 For example Kalai Lehrer 41 agent knows payoff matrix repeated game rationally updates beliefs strategies agents converge Nash equilibriumif measurable set outcomes positive probability actual strategies positive probability agents prior belief It argued requirement unreasonably restrictive 53 54 games learning comes close equilibrium 25 T Sandholm Artiﬁcial Intelligence 171 2007 382391 385 satisﬁcing performance utility bound optimal highly important agent want In experience fruitful approach prescriptive agendas use desiderata guide algorithm development This tends lead algorithms intuitive obvious choices form better I include examples subsections I discuss strands prescriptive agenda 231 Prescriptive cooperative agenda The prescriptive cooperative agenda team games agents payoffs identical incentives aligned Why multiagent setting Why consider agents agent Indeed team problems MAL applied probably better andor easily solved centralized approach For example designing controller bank elevators consider elevator separate agent On hand applications induce inherent distribution problem The data inherently distributed centralized changes quickly Certain problems networks constitute typical class settings 14 In settings modeling problem multiagent problem sense agents identical payoffs If game known team setting learning setting Each agent principle compute optimal joint policy use If agent uses deterministic algorithm agents ﬁnd optimal policy multiplicity optima issue However computing optimal joint policy complex settingssuch control artiﬁcial predator teams 6368 robot soccer 67 opponent teams strategy knownso learning algorithm come joint policy reasonable alternative This establishes connection computational MAL agenda The team setting simpler fact designer gets design agents need worry agents follow prescribed learning technique If game unknown problem highly nontrivial involves following learning tasks learning game learning play The agents simultaneously identify game maximize utility discounted sum rewards It possible learn game play building explicit model Another complication team games multiple equilibria suboptimal Research interesting history In repeated games learning automata 5570 converge equilibrium local optimum In stochastic aka Markov games problem difﬁcult Littman 47 introduced FriendorFoe Qlearning learns play Nash equilibrium overall stochastic game global optimum strategy proﬁle maximizes payoff agent saddle point Nash equilibrium agent deviates better The algorithm needs know settings operating sufﬁces learn ones Qfunction The algorithm variants called FriendQ FoeQ aka minimax Q respectively The settings scope theoretical convergence results earlier MAL algorithm NashQ 38 Those convergence guarantees require global optimality property saddle point property satisﬁed overall stochastic game internal representation learned far table Qvalues 3847 This condition satisﬁed practice algorithm told global optimum setting saddle point setting knows use appropriate updating rule Without told NashQ converges setting restricted stage game unique equilibrium 47 multiple equilibria payoff vector zerosum games Note saddle point setting adversarial sense algorithms pertain prescriptive noncooperative agenda discussed Claus Boutilier 16 developed joint action learner agents converge Nash equilibrium team games outcome suboptimal equilibrium repeated stochastic team games Learning play optimal Nash equilibrium team Markov games posed important open problem 48 Wang Sandholm 72 developed optimal adaptive learning provably accomplishes goal That work exempliﬁed lesson desideratumnot idiosyncratic notions naturalnessshould guide 386 T Sandholm Artiﬁcial Intelligence 171 2007 382391 algorithm design The algorithm encompasses unintuitive features incomplete sampling ones memory action selection biased particular way apart stochasticity needed exploration Both essential algorithm meet desideratum Most recently nearoptimal polynomialtime algorithms problem presented 10 232 Prescriptive noncooperative agenda The prescriptive noncooperative agenda widely applicable signiﬁcantly challenging agents nonidentical payoffs cause different incentives The special case constantsum purely adversarial games 46 easier generalsum case 61 It clear goal MAL setting Much work focused learning play equilibrium While sensible desideratum omnipotent For example agents playing equilibrium agent better deviating equilibrium strategy Bowling Veloso 8 suggested learning algorithm following properties P1 convergence Nash equilibrium selfplay Remarks One postulate notions equilibrium target For example papers written MAL correlated equilibrium target 33 Furthermore postulate target equilibrium stage game equilibrium remaining repeated game On hand equilibria yield better outcomes Iterated Prisoners Dilemma 61 On hand settings stage game play trying learn For example practicing soccer chess Go opponent course year ones goal usually possible end Also constitutes equilibrium repeated game depends agents discounting common knowledge P2 convergence bestresponse stationary opponent opponent eventually comes stationary Remarks This agent learning maximally exploit irrational stationary opponents In cooper ative games better worded learning despite agents irrationality Alternatively wish learn bestrespond larger classes opponents 5960 One view properties minimal desiderata sense desirable MAL algorithm satisfy One desire strengthen P1 convergence Pareto efﬁcient equilibrium The WoLFIGA algorithm 8 improvement earlier algorithm 65 uses higher learning rate losing lower winning It satisﬁes P1 P2 2person 2action repeated games assuming agents observe mixed strategies assuming gradientfollowing steps inﬁnitesimal The AWESOME Adapt When Everybody Stationary Otherwise Move Equilibrium algorithm 22 satisﬁes P1 P2 nperson naction repeated games assuming mixed strategies observable inﬁnitesimal steps It showed desiderata drive MAL algorithm design The idea AWESOME adapt agents strategies appear stationary retreat pre computed equilibrium strategy At point time AWESOME maintains null hypotheses playing precomputed equilibrium stationary Whenever hypotheses rejected AWESOME restarts AWESOME reject hypothesis based actions played epoch Over time epoch length carefully increased criterion hypothesis rejection tightened obtain convergence guarantee AWESOME selfaware detects actions signal nonstationarity restarts synchronization That AWESOME player mindful teaches The techniques prove properties AWESOME fundamentally different prior algorithms help analyzing future MAL algorithms AWESOME viewed skeletonthat guarantees satisfaction P1 P2on additional techniques guarantee additional desirable properties T Sandholm Artiﬁcial Intelligence 171 2007 382391 387 AWESOME makes assumptions theoretical work attempting attain P1 P2 3865 First deals repeated games stochastic games state Second assumes game known learned This assumed game theory literature learning review 27 signiﬁcant MAL research science attempts agents learn game 49111617193338464749577273 So far research able claims satisfying P1 P2 In fact continuoustime dynamics knowledge players payoffs necessary converge Nash equilibrium 37 If game known initially agents observe realized payoffs agents given agents learning algorithm conceivably collaboratively explore game learn game learn play The assumption agents compute Nash equilibrium It assumed multiple AWESOME players compute Nash equilibrium identical algorithms It unknown Nash equilibrium worstcase polynomial time certain related questions hard worst case 18 30 In practice Nash equilibria reasonably large games 445862 The lesson desiderata guide MAL algorithm design emerged developing efﬁcient provably nearoptimal algorithms learning play Paretooptimal strict Nash equilibria agents prefer different equilibria repeated coordination games nonidentical 73 A key future direction develop MAL algorithms converge optimal Nash equilibrium generalsum repeated games possible In addition properties end result learning desire properties entire learning process For example like P3 low regret compared best ﬁxed strategy averaged time Several MAL algorithms property average regret tends zero regardless opponents repeated games 2726283436394574 unattainable general stochastic games 50 The ReDVaLeR algorithm 3 satisﬁes P1 P2 relies agents observing mixed strategies inﬁnitesimal steps For different setting parameter achieves constantbounded regret Stepping right desiderata Consider HeadsUp poker 2player zerosum game played repeatedly If wants build agent unbeatable expectation draws cards compute minimax strategy 3132 use If interested accruing money possible better principle learning exploit opponents irrationality 66 That risky opponent teach agent play certain way later exploit agents learned way playing One reasonable approach start playing minimax strategy gradually modifying strategy advantage opponent agent learns opponent To avoid taught exploited problem mentioned agent example deviate minimax play extent risks expectation worst possible opponent strategy equal agents winnings far Or agent want risk McCracken Bowling 51 present related approach At iteration game strategy selected strategies yield payoff worse safety level minimax payoff minus constant If certain opponent modeling algorithm selecting set adjusting constant limit average payoff worse safety level In summary type setting payoffs real money learning process view learning process strategically2 Algorithms average regret tends zero address extent In zerosum games algorithm play nearly optimally rational minimax opponent nearly optimally stationary opponent stationary population indistinguishable stationary opponents However algorithms completely satisfactory First regret grow bound average regret tends zero Second average regret zero immediately approaches zero certain rates Third zero average regret imply agents learned play Nash equilibrium These learners generally oblivious players payoffs Thus reason incentives steer interaction winwin use threats However 2 For settings player zerosum repeated game potentially stochastic payoffs know game payoffs learn order play Conitzer Sandholm 17 introduced framework BLWoLF analyzing inherent disadvantage player The framework allows probabilistic approximate learning 388 T Sandholm Artiﬁcial Intelligence 171 2007 382391 certain classes games low average regret imply cid2Nashequilibrium 6 Fourth agent zero average regret solution far Pareto optimal Fifth games large numbers stagegame pure strategiessuch poker tiny artiﬁcial variantsit clear algorithms helpful implementable computational perspective If algorithm learns repetition game expected payoff pure strategy cid2 average regret guaranteed conducting number repetitions proportional log n faster general n number pure strategies 26 In applications n huge inﬁnite certain structured domains adaptively choosing paths network strategy space compactly represented optimized yielding faster learning speed 4074 Unfortunately applications algorithm observes payoff strategy actually played aka bandit setting 2 In setting number repetitions needed best algorithms On log n algorithm require cid3n iterations needs play strategy Again certain applications n huge inﬁnite strategy space compactly represented optimized yielding faster learning 2452 This exciting future direction 233 Agenda learning algorithms equilibrium One conceptual difﬁculty noregret algorithms player better different repeatedgame strategy So agent use given learning strategy Furthermore given strategies lead equilibrium multiple equilibria agent able drive better equilibrium playing different strategy This motivates idea learning algorithms equilibrium I suggested luncheon IJCAI95 Workshop Adap tation Learning Multiagent Systems Independently Brafman Tennenholtz 1213 pursued agenda signiﬁcant progress On hand holy grail prescriptive noncooperative agenda On hand frail If agent fails play prescribed learning strategy better playing differently Second work assumes agents ways discounting known Third multiple equilibria learning algorithms picked Stepping learning computing equilibrium repeated game One possibility payoff matrices unknown Brafman Tennenholtz 1213 proposed efﬁcient learning equilibrium learning algorithms equilibrium deviations irrational polynomial number steps payoffs approach Nash equilibrium polynomial number steps sticks learning algorithm Such equilibria exist surprisingly broad class settings Future work strive MAL algorithms achieve desirable properties related viewing learn ing process strategicallyperhaps lines discussed aboveand versions P1 P2 andor P3 3 Research cuts agendas Lower bounds There research direction cuts agendas lower bounds They derived computational complexity ﬁnding solution satisfying given solution concept These bounds apply computational complexity MAL algorithms algorithms ﬁnd solution The complexity ﬁnding Nash equilibrium unknown polynomially equivalent nperson games rational numbers payoffs 2person games 01 payoffs 115 Finding good Nash equilibria hard related targets 2player games 1830 Good correlated equilibria easy ﬁnd 2player games 30 concisely represented nperson games 56 The complexity solution concepts determined 5182021232943 One caveat computational lower bounds usually derived Turing machine model computation conceivable humans animals powerful computational apparatus case lower bounds apply descriptive agenda Another important complexity measure MAL number interactions rounds cost needed ﬁnd solution Conitzer Sandholm 19 proposed methodology based communication complexity deriving lower bounds They derived bounds function number actions available game solution concepts Lower bounds derived methodology apply MAL algorithms opponents payoffs unknown establish inherent limitations MAL The bounds rely particular T Sandholm Artiﬁcial Intelligence 171 2007 382391 389 model computation apply descriptive agenda Recently methodology nagent games 35 4 Conclusions I laid slight reﬁnement Shoham et als taxonomy agendas I consider sensible multiagent learning MAL My goal spur discussion research ultimately provide ﬁeld clarity The taxonomy intended rigid senseless work agendas new sensible agendas emerge Within agenda I brieﬂy identiﬁed issues suggested directions future research In computational agenda direct algorithms efﬁcient MAL MAL plays role especially rules game unknown efﬁcient direct algorithms known class games In descriptive agenda emphasis placed establishing classes learning rules actually model learning multiple humans animals Also agenda potential circularity This positive verify learning models In prescriptive agendas desiderata need clear guide MAL algorithm design The algo rithms need mimic humans animals learning I discussed worthy desiderata pointed literature motivated The learning problem interesting cooperative noncooperative settings issues concerns different For noncooperative settings future work increasingly consider learning strategically Lower bounds cut agendas Lower bounds derived computational complexity learning solution assuming example Turing machine model Lower bounds derived number interactions cost needed learn solution tools communication complexity These bounds independent computational model apply human animal learning Acknowledgements I thank Rakesh Vohra Michael Wellman organizing important special issue I thank anonymous reviewer helpful suggestions I thank Avrim Blum interesting communications regret learning Michael Littman illuminating discussion NashQ FriendorFoeQ References 1 T Abbott D Kane P Valiant On complexity twoplayer winlose games Symposium Foundations Computer Science 2005 2 P Auer N CesaBianchi Y Freund RE Schapire The nonstochastic multiarmed bandit problem SIAM Journal Computing 32 2002 4877 3 B Banerjee J Peng Performance bounded reinforcement learning strategic interactions National Conf Artiﬁcial Intelligence 2004 4 B Banerjee S Sen J Peng Fast concurrent reinforcement learners Internat Joint Conf Artiﬁcial Intelligence 2001 5 M Benisch G Davis T Sandholm Algorithms rationalizability CURB sets National Conf Artiﬁcial Intelligence 2006 6 A Blum E EvenDar K Ligett Routing regret On convergence Nash equilibria regretminimizing algorithms routing games ACM Symposium Principles Distributed Computing 2006 7 M Bowling Convergence noregret multiagent learning Conf Neural Information Processing Systems 2005 8 M Bowling M Veloso Multiagent learning variable learning rate Artiﬁcial Intelligence 136 2002 215250 9 R Brafman M Tennenholtz A nearoptimal polynomial time algorithm learning certain classes stochastic games Artiﬁcial Intelli gence 121 2000 3147 10 R Brafman M Tennenholtz Learning coordinate efﬁciently A modelbased approach Journal Artiﬁcial Intelligence Research 19 2003 1123 11 R Brafman M Tennenholtz Rmaxa general polynomial time algorithm nearoptimal reinforcement learning Journal Machine Learning Research 3 2003 213231 12 R Brafman M Tennenholtz Efﬁcient learning equilibrium Artiﬁcial Intelligence 159 2004 2747 Earlier version NIPS02 13 R Brafman M Tennenholtz Optimal efﬁcient learning equilibrium Imperfect monitoring symmetric games National Conf Artiﬁcial Intelligence 2005 14 YH Chang T Ho L Kaelbling Mobilized adhoc networks A reinforcement learning approach Internat Conf Autonomic Comput ing 2004 15 X Chen X Deng Settling complexity 2player Nash equilibrium Electronic Colloquium Computational Complexity Report No 150 2005 390 T Sandholm Artiﬁcial Intelligence 171 2007 382391 16 C Claus C Boutilier The dynamics reinforcement learning cooperative multiagent systems National Conf Artiﬁcial Intelligence 1998 17 V Conitzer T Sandholm BLWoLF A framework lossbounded learnability zerosum games Internat Conf Machine Learning 2003 18 V Conitzer T Sandholm Complexity results Nash equilibria Internat Joint Conf Artiﬁcial Intelligence 2003 19 V Conitzer T Sandholm Communication complexity lower bound learning games Internat Conf Machine Learning 2004 20 V Conitzer T Sandholm Complexity iterated dominance ACM Conf Electronic Commerce 2005 21 V Conitzer T Sandholm A generalized strategy eliminability criterion computational methods applying National Conf Artiﬁcial Intelligence 2005 22 V Conitzer T Sandholm AWESOME A general multiagent learning algorithm converges selfplay learns best response stationary opponents Special issue Learning Computational Game Theory Machine Learning 67 2007 2343 Short version ICML03 23 V Conitzer T Sandholm Computing optimal strategy commit ACM Conf Electronic Commerce 2006 24 A Flaxman A Kalai B McMahan Online convex optimization bandit setting Gradient descent gradient ACMSIAM Symposium Discrete Algorithms 2005 25 DP Foster HP Young On impossibility predicting behavior rational agents Proceedings National Academy Sciences 98 2001 1284812853 26 Y Freund R Schapire Adaptive game playing multiplicative weights Games Economic Behavior 29 1999 79103 27 D Fudenberg D Levine The Theory Learning Games MIT Press 1998 28 D Fudenberg DK Levine Consistency cautious ﬁctitious play Journal Economic Dynamics Control 19 1995 10651089 29 I Gilboa E Kalai E Zemel The complexity eliminating dominated strategies Mathematics Operation Research 18 1993 553565 30 I Gilboa E Zemel Nash correlated equilibria Some complexity considerations Games Economic Behavior 1 1989 8093 31 A Gilpin S Hoda J Peña T Sandholm Gradientbased algorithms ﬁnding Nash equilibria extensive form games Mimeo 2007 32 A Gilpin T Sandholm Finding equilibria large sequential games imperfect information ACM Conf Electronic Commerce 2006 33 A Greenwald K Hall Correlated Qlearning Internat Conf Machine Learning 2003 34 A Greenwald A Jafari A general class noregret learning algorithms gametheoretic equilibria Conf Learning Theory 2003 35 S Hart Y Mansour The communication complexity uncoupled Nash equilibrium procedures 2006 Draft 36 S Hart A MasColell A simple adaptive procedure leading correlated equilibrium Econometrica 68 2000 11271150 37 S Hart A MasColell Uncoupled dynamics lead Nash equilibrium American Economic Review 93 2003 18301836 38 J Hu MP Wellman Nash Qlearning generalsum stochastic games Journal Machine Learning Research 4 2003 10391069 39 A Jafari A Greenwald D Gondek G Ercal On noregret learning ﬁctitious play Nash equilibrium Internat Conf Machine Learning 2001 40 A Kalai S Vempala Efﬁcient algorithms online decision problems Journal Computer System Sciences 71 2005 291307 41 E Kalai E Lehrer Rational learning leads Nash equilibrium Econometrica 61 5 1993 10191045 42 M Kearns M Littman S Singh Graphical models game theory Conf Uncertainty Artiﬁcial Intelligence 2001 43 DE Knuth CH Papadimitriou JN Tsitsiklis A note strategy elimination bimatrix games Operations Research Letters 7 1988 103107 44 C Lemke J Howson Equilibrium points bimatrix games Journal Society Industrial Applied Mathematics 12 1964 413423 45 N Littlestone MK Warmuth The weighted majority algorithm Information Computation 108 2 1994 212261 46 M Littman Markov games framework multiagent reinforcement learning Internat Conf Machine Learning 1994 47 M Littman Friend foe Qlearning generalsum Markov games Internat Conf Machine Learning 2001 48 M Littman Valuefunction reinforcement learning Markov games Journal Cognitive Systems Research 2 2001 5566 49 M Littman C Szepesvári A generalized reinforcementlearning model Convergence applications Internat Conf Machine Learn ing 1996 50 S Mannor N Shimkin The empirical Bayes envelope regret minimization competitive Markov decision processes Mathematics Operations Research 28 2 2003 327345 51 P McCracken M Bowling Safe strategies agent modelling games AAAI Fall Symposium Artiﬁcial Multiagent Learning 2004 52 B McMahan A Blum Online geometric optimization bandit setting adaptive adversary Conf Learning Theory 2004 53 J Nachbar Prediction optimization learning games Econometrica 65 1997 275309 54 J Nachbar Bayesian learning repeated games incomplete information Social Choice Welfare 18 2001 303326 55 KS Narendra MAL Thathachar Learning Automata An Introduction Prentice Hall 1989 56 C Papadimitriou T Roughgarden Computing equilibria multiplayer games Symposium Discrete Algorithms 2005 57 K Pivazyan Y Shoham Polynomialtime reinforcement learning nearoptimal policies National Conf Artiﬁcial Intelligence 2002 58 R Porter E Nudelman Y Shoham Simple search methods ﬁnding Nash equilibrium National Conf Artiﬁcial Intelligence 2004 59 R Powers Y Shoham Learning opponents bounded memory Internat Joint Conf Artiﬁcial Intelligence 2005 60 R Powers Y Shoham New criteria new algorithm learning multiagent systems Conf Neural Information Processing Systems 2005 61 T Sandholm R Crites Multiagent reinforcement learning iterated prisoners dilemma Biosystems 37 1996 147166 special issue Prisoners Dilemma Early version IJCAI95 Workshop Adaptation Learning Multiagent Systems 62 T Sandholm A Gilpin V Conitzer Mixedinteger programming methods ﬁnding Nash equilibria National Conf Artiﬁcial Intelli gence 2005 63 T Sandholm MV Nagendra Prasad Learning pursuit strategies Project CmpSci 698 Machine Learning Computer Science Department University Massachusetts Amherst Spring 1993 T Sandholm Artiﬁcial Intelligence 171 2007 382391 391 64 J Schaeffer Y Björnsson N Burch A Kishimoto M Müller R Lake P Lu S Sutphen Solving checkers Internat Joint Conf Artiﬁcial Intelligence 2005 65 S Singh M Kearns Y Mansour Nash convergence gradient dynamics generalsum games Conf Uncertainty Artiﬁcial Intelligence 2000 66 F Southey M Bowling B Larson C Piccione N Burch D Billings C Rayner Bayes bluff Opponent modelling poker Conf Uncertainty Artiﬁcial Intelligence 2005 67 P Stone M Veloso Towards collaborative adversarial learning A case study robotic soccer International Journal Human Computer Studies 48 1998 68 M Tan Multiagent reinforcement learning Independent vs cooperative agents Internat Conf Machine Learning 1993 69 G Tesauro Temporal difference learning TDgammon Communications ACM 38 3 1995 70 ML Tsetlin Automaton Theory Modelling Biological Systems Academic Press 1973 71 D Vickrey D Koller Multiagent algorithms solving graphical games National Conf Artiﬁcial Intelligence 2002 72 X Wang T Sandholm Reinforcement learning play optimal Nash equilibrium team Markov games Conf Neural Information Processing Systems 2002 73 X Wang T Sandholm Learning nearParetooptimal conventions polynomial time Conf Neural Information Processing Systems 2003 74 M Zinkevich Online convex programming generalized inﬁnitesimal gradient ascent Internat Conf Machine Learning 2003