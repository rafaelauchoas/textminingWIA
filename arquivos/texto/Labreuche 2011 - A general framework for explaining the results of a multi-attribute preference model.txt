Artiﬁcial Intelligence 175 2011 14101448 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A general framework explaining results multiattribute preference model Christophe Labreuche Thales Research Technology RD128 91767 Palaiseau cedex France r t c l e n f o b s t r c t Article history Received 27 February 2009 Received revised form 13 August 2010 Accepted 13 August 2010 Available online 1 December 2010 Keywords Preferences Decision theory Argumentation Weight 1 Introduction The automatic generation explanation prescription multiattribute decision model crucial applications recommender systems This task complex quantitative models designed easily explainable The major limitation previous research formal justiﬁcation arguments selected explanation The goal paper deﬁne general framework justify arguments shall selected case decision model based weights assigned attributes Due complexity explaining preference model based utility theory explanation reasonings necessary cover cases ranging situations prescription trivial situations prescription tight The set selected arguments framework nondominated element combinatorial structure sense order relation Our general approach instantiated precisely models probabilistic expected utility model qualitative pessimistic minmax model concordance rule constructed weight vector 2010 Elsevier BV All rights reserved In decision making uncertainty social choice multicriteria decision making main domains decision theory 4048 explicit analytical models constructed represent preferences decision maker combine dimensions states nature voters criteria respectively Decision theory mainly focuses specifying rational agent behave results justiﬁcation axiomatic characterizations decision models 54513741 Another welldeveloped research area decision theory concerns elicitation decision maker preferences led design elaborate elicitation methods Decision models traditionally mainly quantitative case instance expected utility model 54 51 More recently qualitative models developed AI order overcome diﬃculty elicitation information necessary models 24191220 A wide class quantitative qualitative models parameterized weight vector weight assigned dimension 2649 We interested models paper The ﬁnal decision process model elicited usually studied decision theory It generally reduced application decision model options If individual constructs decision model convinced relevance necessary explain result application decision model However practical situations decision needs justiﬁed actors participate construction These actors interested technicality decision model On Email address christophelabreuchethalesgroupcom 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201011008 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1411 contrary wish synthetic explanation decision It shall automatically generated According Klein 35 explanation intuitive comprehensive persuasive The automatic generation explanation outcome decision model easy task models decision theory designed provide reasons support recommendation 35 By contrast decision frameworks AI construction ability naturally provide explanation This instance case beliefdesireintention representation architecture 11 argumentation 25 conditional preference networks 9 In 1832 argumentationbased framework making explaining decision proposed A preference relation candidate options derived positive negative acceptable arguments support option ordering arguments Another extension argumentation incorporate preferences 42 In context multicriteria decision making speciﬁcally multiattribute value theory MAVT 263430 works aim generating explanation outcome decision model 351443 The gener ation process split parts selection arguments criteria presented structuration expression selected arguments natural language The second welldeveloped Ref 14 One note expression selected content natural language extensively studied literature instance 1327 cite Concerning selection previously mentioned works 351443 use idea It consists selecting k k parameter criteria largest contribution overall utility This approach satisfactory following reasons First textual explanation mention weights criteria 14 This major drawback weights essential MAVT model Secondly formal justiﬁcation arguments selected particular choice k parameter The aim paper develop formal framework justiﬁes selection arguments This work especially dedicated situations recipient explanation individual designed decision model This general framework designed decision model based weights It adapts automatically complexity decision The easier decision simpler explanation Premises work conference papers 3839 Section 2 describes weighted decision models We introduce particular models validate framework expected utility model 5451 weighted minmax model 22 weighted majority model 44 The general explanation framework presented Section 3 In order adapt complexity situation argumentation reasonings introduced They called anchors analogy concept anchor deﬁned Grize refer implicit information convince audience 32 A subset criteria selected explanation criteria decisive sense depending anchor Such subset called explanation set The set explanation sets forms combinatorial structure One aims ﬁnding nondominated explanation sets structure sense order relation expressing simplicity explanation set In following Sections 4 7 general framework thoroughly developed anchor weighted decision models From properties satisﬁed nondominated explanation sets explanation generated derived A method algorithm compute nondominated explanation set given case We deal expression selected arguments natural language However examples texts generated presented Some experimental results presented Section 8 The proofs results given Section 10 2 Decision models based weight vector Decision theory 4048 interested preference representation gathers different domains MultiCriteria Decision Making MCDM 492810 Decision Making Uncertainty DMU 3651 Social Choice SC 629 The typical decision problem studied decision theory consists selecting alternative set X candidate options alternatives described dimensions This selection obtained construction preference relation X The set ﬁnite dimensions denoted N 1 n alternatives characterized dimension N value set Xi In MCDM N set decision criteria Xi attribute representing criterion alternative characterized value attribute X X1 Xn The criteria conﬂicting 49 As example cost criteria performance criteria met time The main diﬃculty ﬁnd good compromise criteria In DMU elements N states representing possible situations X1 Xn C set possible consequences alternative called act mapping N C X C N The consequence selecting particular alternative depends state nature occur Moreover attitude decision maker DM uncertainty inﬂuences choice strategy 51 In SC N set voters X1 Xn C set candidates alternatives candidates X C The diﬃculty ﬁnd fair consensus opinions voters 5 The preference relation alternatives X usually constructed preference relation cid3i set Xi We denote cid4i asymmetric symmetric parts cid3i In MCDM cid3i represents preferences DM elements attribute Xi DMU cid31 cid3n depict preferences DM set C consequences SC cid3i models preferences voter set C candidates There exist preference models overall preference alternative y X alternative x X obtained weighing pros cons y x N yi xi preference The sets A positive negative null arguments respectively concerning preference y x dimensions N The y x N xi cid4i yi A y x N yi cid4i xi A 1412 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 y x A y x based priorities allotted dimensions This quantiﬁed arbitrage A weight w assigned N aggregation function depending w w 1 wn 264951 The semantics weights depends method Basically w interpreted importance criterion MCDM probability possibility state nature DMU power voter SC The weight vector w normalized sense depending method We denote min max operators respectively We interested families preference relations characterized weight vector w w 1 wn For given family F preference models set weights denoted WF corresponding preference relation w w WF We denote cid4F denoted cid3F w We interested models derived aggregated value w asymmetric symmetric parts cid3F w F x y X y cid3F F w y x cid2 0 A classical representation summarize option x X overall utility h F w x h w x H F w x This gives H F w y x h F w y cid2 w x There exist accurate elicitation methods construct quantitative model hEU The wellknown family corresponds expectedutility model labelled EU von Neumann iN w uixi w 0 1 ui Xi 0 1 value function mea Morgenstern 54 Savage 51 hEU suring attractiveness elements Xi The value function ui quantiﬁes preference relation cid3i b Xi cid3i b uia cid2 uib In MCDM model MultiAttribute Value Theory Model 2634 One WEU 0 1n w 750 The main drawback methods complexity ﬁt applications Qualitative decision theory deﬁned AI overcome diﬃculty 24191220 A pessimistic weighted extension labelled Pess Wald minmax iN uixi 1 w w interpreted possibility distribution function 56 deﬁned 22 hPess DMU 23 This model MCDM One WPess 0 1n For simplicity consider interval 0 1 Pess model linearly ordered scale elements w x The EU Pess models require existence value representation cid3i commensurateness preference scale weightuncertainty scale usually easy satisfy practice The majority rule labelled Maj deﬁned SC 44 gets rid assumptions It studied AI general framework 20 It reads H Maj A yx w This model limitations Arrows theorem 5 The majority rule known MCDM concordance rule 49 One WMaj Rn A yx w w y x cid2 cid2 cid3 Despite limitations models EU Pess Maj described applications covering different domains For reason focus models cid4 cid2 iN w 1 WPess w 0 1n iN w 1 Let mention EU model normalization condition The set normalized weights wrt model F denoted WF For EU Maj models WEU WMaj w 0 1n iN w 1 w α α α α 0 1 31 The weights normalized model equivalent idempotency property hEU iN w 1 22 For models exists Pess characterized property dimensions symmetric particular normalized weight vector w F n These weights apply absence information aggregation process Therefore w They results application principles insuﬃcient reason maximum entropy minimum speciﬁcity In Bayesian approach lack information represented uniform probability w MCDM SC criteria voters assigned weight reason proceed differently The vector w called reference weight vector For models Maj EU obtain N F w F 1 cid2 cid4 F F w Maj w EU 1 n 1 cid3Maj cid2 wMaj corresponds Condorcet majority rule hEU iN w 1 value w Maj w EU 1 wEU arithmetic mean Let w WEU w WMaj Since n corresponds mean importance dimension w Hence relation w 1 n n resp w 1 said important resp unimportant dimension n means dimension important resp important average value 1 For model Pess N 1 2 w Pess hPess wPess min operator 3 General explanation framework 31 Why generating automatically explanation From assume F EU Pess Maj ﬁxed family models weight vector v WF speciﬁed Let cid5 DF x y v X X WF y cid4F cid6 v x C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1413 We consider options x y X assume x y v DF We aim explaining y strictly preferred x according model If individual constructs decision model cid4F v convinced relevance necessary explain v x This true actors involved decision process shown comparison y cid4F following examples The individual referred decision maker DM usually person responsible decision He explain decision actors instance chief executive board shareholders These actors interested technicality decision model In order convince merits decision synthetic explanation needs given ii A decision support usually generates ordered list recommended options user decides choose Since user expert decision model elicited recommendation shall explained Moreover user necessarily highly qualiﬁed time constraints operator needs decisions Hence explanation shall nontechnical simple fast understand user An example multicriteria decision function recommends assignment priorities radar tasks radar management 8 iii There situations AI artiﬁcial agents preferences decision models This instance case negotiation In negotiation protocols offer actor iteration protocol actors feedback 451 They reveal preference models actors want models private Each actor provides ones clues offer instance satisfactory In previous examples explanation recommendations produced decision model generated According Klein 35 explanation simply EU model y preferred x overall score cid2 iN v xi x appeal intuition compare iN v yi y larger overall score cid2 alternatives explicitly Yet examples ii iii synthetic explanation needs automatically generated produced human When number criteria relatively large cognitive load interpret ﬁgures x y v relatively high This load high situations decision activity repetitive case radar management example ii operator perform activity hours raw In example iii values v sent agents privacy reasons synthetic explanation generated artiﬁcial agent The arguments explanation elements N The explanation based satisfaction degrees uix ui y x y dimension models EU Pess value functions ui exist Hence precise form expression value functions ui matter As result sake simplicity notation assume paper value functions ui identity function Hence set X 0 1n models EU Pess Our framework apply domains MCDM DMU SC However sake conciseness restrict MCDM interpretations results In particular N correspond decision criteria For x X xi mark score x criterion For models EU Pess xi 0 1 interpreted satisfaction degree value 1 perfectly satisfactory criterion value 0 unacceptable Our explanation framework transposed DMU SC 32 Notation deﬁnitions 1 A y x sgni For F EU Pess X 0 1n deﬁne cid3 Rn cid3i yi xi N For F Maj deﬁne y x For S N sgn 1 0 1N sgni S 1 s S s S Zπ Z Z RN deﬁne π Z S s Throughout paper apply S 1 deﬁnition vectors cid3 x y v leading π cid3 S π v S respectively Let ΠS set permutations coalition S N For π ΠN w WF π w weight vector deﬁned π wi wπ N For cid14 NS w S w w w S w Let cid14 WF S N deﬁne compound weight vector w S w cid14 NS WF w S w y x sgni cid14 1 A cid3 cid3 Zπ Z 0 A cid14 NS w S π y S π x cid5 Ex A1 A p p N cid16 A1 A p N Ai A j cid16 j cid6 A A exists A cid14 Acid14 A A cid14 We write A Acid14 For A Acid14 Ex write A cid18 Acid14 A A A Acid14 For permutation π ΠN determine ﬁnest partition sense cid18 Aπ A1 A p N Ai invariant π π Ai Ai For 1 p write 1414 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 k2 π k1 k3 π k2 kqi π kqi 1 k1 π kqi Ai k1 kqi qi Ai By abuse language Ai called cycle π One taxonomy arguments sign strength The sign argument follows n EU Maj models argument cid3i Section 2 The strength argument related sign v 1 n v 1 strong medium weak v cid19 1 n v cid21 1 n respectively 33 Motivation approach EU model To general idea approach let ﬁrst focus EU model Ideally like produce argumentation relation y cid4EU v x There diﬃculties extend argumentation quantitative setting In logicbased argumentation argument pair cid22H hcid23 h conclusion H contains minimal elements knowledge base logically entails h H cid24 h 52 Here like construct argument h statement y cid4EU v x H N subset criteria Intuitively criteria H explain decision decision remains unchanged values x y criteria N H It writes H cid24 cid7 cid8 v x y cid4EU cid14 x NH y cid14 NH 0 1NH cid9 y H y cid14 NH cid9 cid10 cid4EU v cid14 xH x NH cid10 This equivalent y H 0NH cid4EU xH 1NH This condition strong satisﬁed H v equal N The reason criteria compensate EU model Section 2 rare criterion completely useless decision The diﬃculty mentioned earlier expressed fallacy composi tiondivision applies divisible objects deﬁned concatenation smaller parts 33 Quantitative models characterized presence concatenation operators From measurement standpoint value func tions ui obtained quantitative scale constructed concatenation operator Xi preference relation 37 The aggregation marks hEU results concatenation materialized v arithmetic operator Let π ΠN vπ 1cid3π 1 cid3 cid3 vπ ncid3π n The explanations preference y cid4EU v x proposed 351443 similar consist displaying user criteria π q π q 1 π n q 1 n parameter This idea simplifying explanation reasoning v icid3i focusing subset arguments originated theory argumentation The quantity compeli measuring contribution criterion overall evaluation H EU v y x called compellingness Klein 35 There main limitations approach Firstly formal justiﬁcation selecting particular criteria explanation Secondly textual explanation generated refer weight vector v Yet behaviour decision model highly inﬂuenced weight vector v explanation preference y cid4F v x mention weights The weights play central role approach However shown following example circumstances necessary mention speciﬁcity weights explanation Example 1 The simple situation arises y strictly better x criteria There negative null argument Clearly y cid4F v x F EU Pess Maj v WF There need trivial situation mention weights v explanation In Example 1 w WF yields comparison y x This suggests look general case set V F y x weights w WF y strictly preferred x In following lemma restrict EU model w x Let cid3 cid3i 0 cid3 cid3i 0 N If A y x cid16 Lemma 1 Let V EU y x w WEU y cid4EU A y x cid16 cid11 cid11 cid11 A cid11 1 A y x cid11 cid11 cid11 A cid11 1 A y x If If y x y x cid5 cid5 cid6 w w V EU y x cid6 w w V EU y x 0 1 cid12 j A y x cid5 cid6 w j w V EU y x cid14 0 max A yx cid3 cid3 cid3 j cid3 j cid3 j cid3 cid13 1 min j A yx cid15 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1415 It easy construct intervals obtained Lemma 1 general explanation particular cases As said Section 31 recipient explanation wish generate supposed prior speci ﬁcities decision model weight vector v Anyway recipient wishes prior idea decision x y consciously unconsciously use speciﬁc weights reference weight w EU options x y Hence x better marks y average x preferred y according cid3EU wEU recipient priori expect x preferred y He surprised relation y cid4EU v x Hence explanation shall focus explaining weights v w EU yield opposite decisions Example 2 Assume x 07 07 05 y 1 0 05 v 07 02 01 gives y cid4EU wEU y Hence weights v play important role outcome y cid4EU v x One feels x better y average y preferred x weight vector v criteria y better x important criteria n resp v 1 x better y important Recall criterion important resp unimportant v 1 n y x 3 The weight vector v important positive argument One A ﬁrst criterion important negative argument second criterion The conjunction phenomena actually explains decision Finally criterion impact decision removed explanation y x 2 A y x 1 A v x x cid3EU In Lemma 1 studied change weight vector v maintaining preference x y We idea instantiate different manner We look changes weights yield switch preference x y F Following Example 2 possible reasoning select arguments consists understanding use reference weight vector w leads opposite comparison x y compare weight vector v This switch preference F necessarily comes criteria v signiﬁcantly different w N y remains preferred x speciﬁcity criterion necessary explanation discarded explanation If wording x average good y contained generated explanation mistake reasoning consisting mentioning explicitly criteria explanation If possible change v w F We compare v weights following example y x 1 3 A Example 3 Assume x 0 1 06 y 1 0 1 v 04 02 04 yields y cid4EU v x We notice y y x 2 One feels explanation average strictly better x One A previous example weight ﬁrst criterion positive argument important weight second criterion negative argument important Even criterion positive discarded explanation The intuition ﬁrst criterion suﬃciently positive argument cid31 1 cid33 04 v 1 04 v 3 How justify formally criterion removed explanation Here possible justiﬁcation The weight vector v assigned criteria different manner If v 1 assigned second criterion v 2 assigned ﬁrst criterion leading vcid14 y The inversion preferences weight v x y comes switch weight criteria 1 2 criterion 3 played role inversion preference cid14 02 04 04 decision opposite x cid3EU The explanation based identiﬁcation decisive criteria As suggested Example 3 simple way determine set criteria decisive look inﬂuence permuting weights If positioning weights crucial permutation invert decision Our example shows explanations generated given choice y cid4EU v x y x 3 One proceed Example 3 If switches weights criteria 1 3 leading v y x 1 2 Example 4 Assume x 05 05 1 y 1 1 0 v 04 04 02 gives y cid4EU cid14 A 02 04 04 obtain x cid3EU v x y better x criterion 1 important x better y criterion 3 important Another explanation generated One switch criteria 2 3 explanation focus criteria 2 3 vcid14 y Hence explanation y cid4EU v x One A 34 Description general framework Based previous Examples 1 4 following points raised First different types explanation reasonings previous examples Example 1 Example 2 Examples 3 4 The presentation type explanation user requires different reasoning This based implicit information related concept 1416 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 anchor In argumentation anchor refers admitted facts knowledge world common sense rules 553246 The anchor corresponds implicit reasoning rules explicitly quoted argumentation 55 The missing causal relations argumentation drawn automatically audience In case use term anchor denote generic way reasoning explanation The set anchors denoted Ψ An anchor situations value x y v We denote DF ψ set values x y v DF anchor ψ applied Secondly Examples 3 4 explanation concerns permutation weights criteria In complex situations cycles necessary In case explanation composed set disjoint coalitions criteria element Ex Each coalition criteria set called elementary argument Let Exx y v F ψ Ex set explanation sets allowed anchor ψ Ψ explain preference y cid4F v x We Exx y v F ψ x y v DF ψ Deﬁnition 1 For x y v DF explanation set prescription y cid4F v x element following set Exx y v F cid5 cid6 ψ A ψ Ψ A Exx y v F ψ For ψ A Exx y v F A A respectively explanation A absent explanation This means A necessarily equal N y x positive null negative arguments SA S In Examples 2 3 4 seen arguments y x A A y x A A cid16 We paragraph formalism use justify selection arguments explanation removal ones The set Exx y v F combinatorial structure composed elements In order select simplest explanation set deﬁne order relation cid2 Ex Relation ex cid2 ex means explanation set ex complex cid14 The anchors simple understand There order relation cid2 Ψ We understand ex denote cid3 asymmetric cid2 symmetric cid2 One prefers unconditionally explanation set simple anchor explanation set complex anchor The order relation cid2 Exx y v F deﬁned follows ψ A cid2 ψ cid14 Acid14 following conditions met cid14 ψ cid3 ψ cid14 ψ ψ cid14 A cid18 Acid14 A simpler Acid14 We interested simplest explanation sets y cid4F cid2 v x minimal elements Exx y v F sense The set Ex forms combinatorial structure The number partitions k blocks set p elements Stirling number Sk p second kind deﬁned 4 cid12 cid15 Sk p 1 k kcid17 i0 1ki k p Hence cardinality Ex ncid17 pcid17 p1 k1 Sk p The exploration Ex ﬁnd simplest explanation set expected complex 35 Descriptions anchors There remains anchors Generalizing Examples 1 2 3 4 set Ψ composed anchors Ψ ψALL ψNOA ψIVT ψRMG Anchor ψALL generalization Example 1 When y preferred x criteria A y x A y x y preferred x according preference model Example 1 DF ψALL cid5 x y v DF A y x N cid6 No speciﬁcity cid3F grand coalition N Exx y v F ψALL N v needs quoted explanation There elementary argument C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1417 Anchor average ψNOA generalization Example 2 We interested case reference F weight w v x x cid3F leads opposite decision Example 2 y cid4F F act anchor recipient explanation think w Hence wF y The reference weights w F DF ψNOA cid5 x y v DF x cid3F wF y cid6 y remains preferred x discarded Following Example 2 criteria v replaced w F explanation Therefore explanation set A y strictly preferred x weights vA w NA NA normalized matter The criteria N A decisive Note weights vA w sense weights inﬂuencing decision close reference weight If criteria A described explanation N A mentioned explanation The set elementary arguments coalition structure cid22Ncid23 composed singletons N Hence x cid3F deﬁne wF y F F Exx y v F ψNOA cid5 A cid22Ncid23 y cid4F vAw cid6 x F NA set Exx y v F ψNOA Anchor invert ψIVT generalization Examples 3 4 The idea anchor decision weights v switched Hence set cid5 cid6 DF ψIVT x y v DF π ΠN x cid3F π v y The elementary arguments cycles π In explanation generated implicit justiﬁcation arguments discarded shall given In Examples 3 4 criteria explanation original weight v We denote A explanation set The preference x y switched weights assigned criteria A π v remaining criteria N A original weight v The set A union cycles π A Aπ Therefore deﬁne Exx y v F ψIVT cid5 A π ΠN x cid3F cid6 π vAv NA y A Aπ The analysis situations different assignment weights criteria yields inversion preference x y helps understand decisive criteria Anchor remaining ψRMG This case occurs previous anchors applies Hence cid6 π v x cid5 x y v DF A wF x π ΠN y cid4F y x cid16 N y cid4F DF ψRMG As Section 7 elementary argument grand coalition N Exx y v F ψRMG N All previous anchors ordered according intrinsic complexity ψALL cid3 ψNOA cid3 ψIVT cid3 ψRMG The anchors derived previous examples In rest paper prove intuition given examples Section 33 EU model holds situations x y v intuition generalizes models Pess Maj More precisely ﬁrst derive important properties minimal elements Exx y v F From properties convincing explanations generated cases Sections 4 5 6 7 study anchors ψALL ψNOA ψIVT ψRMG respectively In section decision models EU Maj Pess analysed 4 Anchor y x N fact y cid4F When A The following sentence generated explain y cid4F v x anchor v x trivial y preferred x according decision model y preferred x y better x ALL criteria 5 Anchor average We assume section anchor average holds y cid27F obtained understanding weight vectors v w F wF x The explanation synthesis yield different decisions x y 1418 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 F representation priors recipient explanation weights Our approach identiﬁes minimal y w set criteria need original weight criteria weight value w remains preferred x F For sake simplicity notation cid22Ncid23 assimilated N section Hence elements Exx y v F ψNOA assimilated coalitions N The minimal explanation sets anchor ψNOA according cid2 exactly minimal coalitions Exx y v F ψNOA according Lemma 2 A coalition S N minimal Exx y v F ψNOA S Exx y v F ψNOA k S S k Exx y v F ψNOA 3 Note N Exx y v F ψNOA Exx y v F ψNOA 51 Expected utility model Proposition 1 Let S Exx y v EU ψNOA minimal sense Then necessarily vk cid16 1 A y x Moreover k S y x A n k S S k A y x vk k A y x vk 1 n 1 n y x weak importance lower 1 y x strong importance larger mean weight 1 Interpretation 1 From Proposition 1 selected criteria S clearly decisive S contains null argument positive arguments S A n negative arguments S A n We shown intuition Example 2 true general case Hence arguments selected presented natural way recipient If set S N selected reason criteria mentioned outcome criteria medium importance Hence general sentence situation criteria N S needs speciﬁcity criteria S We interested generation explanation user The explanation automatically generated wEU x disproved followed v x list S Exx y v EU ψNOA S minimal arguments This wEU x means x better y average The following following structure concession reverse preference y cid27EU statement preference y cid4EU structure classical argumentation 17 Relation y cid27EU sentence generated explain y cid4EU v x Even x better y average y preferred x y better y x important y worse x x criteria S A criteria S A y x important The following additional sentence added cid2 iNS v icid3i 0 Moreover y average better x criteria This explanation aims convincing audience priori think x preferred y x better average opposite preference holds This illustrated Section 821 examples Section 821 presents comparison approach Klein method Let simple method compute minimal element Exx y v EU ψNOA Let π ΠN cid15 cid12 vπ 1 1 n cid3π 1 cid3 cid3 cid12 vπ n 1 n cid15 cid3π n Proposition 2 Let p largest integer 1 n π p π n Exx y v EU ψNOA Then π p π n minimal Exx y v EU ψNOA Moreover minimal coalition Exx y v EU ψNOA strictly smaller cardinality π p π n C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1419 52 Weighted majority model Proposition 3 Let S Exx y v Maj ψNOA minimal Then necessarily vk cid16 1 Moreover k S n k S S A y x A y x k A y x vk k A y x vk 1 n 1 n From Proposition 3 minimal explanation set positive arguments strong negative arguments weak This similar Interpretation 1 The explanation preference y x follows structure Section 51 Relation y cid27Maj wMaj x means criteria x preferred y criteria y preferred x The following sentence generated explain y cid4Maj x v Even criteria x preferred y criteria y preferred x y preferred x criteria y x y x y better x important criteria S A S A x better y important Let simple method compute minimal element Exx y v Maj ψNOA Let π ΠN cid12 cid15 vπ 1 1 n sgnπ 1 cid3 cid3 cid12 cid15 vπ n 1 n sgnπ n Proposition 4 Let p largest element 1 n π p π n Exx y v Maj ψNOA Then π p π n minimal Exx y v Maj ψNOA Moreover minimal coalition Exx y v Maj ψNOA strictly smaller cardi nality π p π n 53 Pessimistic qualitative model cid3 Let hPessS w z iS zi 1 w S N In weighted minimum aggregation function hPess x contribution criterion N xi 1 v The value 1 v small criterion important Hence bad score important criterion saved criteria overall score necessarily bad On contrary bad score unimportant criterion saved small weight v Proposition 5 Let S Exx y v Pess ψNOA minimal Then S A cid18 cid18 cid18 hPessS v x cid2 hPessS v y xi yi xi iNS iNS iNS y x A y x k S yk 1 vk Moreover Finally hPessS v x cid2 hPessS v y h PessNS v x h PessNS v y h PessNS v x Relation yk 1 vk k S means weight criterion k hides bad score yk Hence S consists weak negative null arguments This proposition shows worse mark x remaining criteria N S lower score y N S lower aggregated score y S Moreover hPess x attained N S v A positive argument selected Exx y v Pess ψNOA set consists negative null argu ments The following proposition shows criteria S S Exx y v Pess ψNOA advantageously added explanation Proposition 6 Assume hPess v Let M k N yk cid3 hPess x attained kx N hPess x xkx 1 vkx Then kx A x For S Exx y v Pess ψNOA minimal S M Moreover 1 vk xkx 1 vkx y x ykx 1 vkx v v k M Interpretation 2 Thanks Propositions 5 6 anchor relatively simple The set arguments divided parts On hand selected arguments S negative weak arguments On criteria y worse score x saved relatively small value weight yk 1 vk The value weight crucial criteria On hand nonselected arguments N S criteria value 1420 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 v weight decisive close reference weight 1 The decisive criteria N S kx overall score hPess x x attained This criterion suﬃciently important hide score x conditions 1 vk Proposition 6 The remaining criteria N S kx mentioned If wishes provide x comprehensive explanation add bad evaluation y criteria M M S saved relatively small importance Indeed Proposition 6 1 vk xkx 1 vkx k M contain negative null arguments set time composed positive arguments Finally y relatively good remaining criteria N S M k N S yk cid3 hPess relatively unimportant Note M criteria M kx v The following sentences generated explain y cid4Pess v x Even worst score y worse x y preferred x relatively bad scores y compared x criteria S saved relatively small importance The overall score x attained criterion kx x worse y relatively large importance save x On criteria M bad evaluation y saved relatively small importance criteria Finally y sufficiently good remaining criteria N S M kx The sentence means score y criteria N S M x An illustration proposed Example 7 Section 822 kx needs larger overall score The following proposition provides simple way compute minimal explanation set Proposition 7 Let p smallest integer 1 n π y deﬁned beginning Section 3 Then π y N 1 π y N p minimal Exx y v Pess ψNOA N 1 π y N p Exx y v Pess ψNOA π y N ΠN It surprising x taken account permutation compute minimal explanation set y x A y x S yi clear π y N 1 A S xi In fact cid3 cid3 6 Anchor invert We assume section exists π ΠN y cid27F π v x We aim determining arguments ex π v x The minimal arguments anchor ψIVT exactly minimal elements plaining y cid4F Exx y v F ψIVT sense cid18 v x y cid27F Proposition 8 Let A Exx y v F ψIVT minimal sense cid18 Then S A S cid2 2 61 Expected utility model We need following result Lemma 3 Let S N s S Then maxπ ΠS j S Moreover minπ ΠS cid2 iS vπ icid3i attained π S ΠS deﬁned π S j π v S s π cid3 S 1 j 1 j S iS vπ icid3i attained π S ΠS deﬁned π S j π v S π cid3 S 1 j cid2 The minimal explanation sets characterized proposition Proposition 9 Let A Exx y v EU ψIVT minimal sense cid18 Let π ΠN x cid3EU We choose π π N A Let S A Then k j S j cid16 k cid3k cid16 cid3 j vπ j cid16 vπ k π vAv NA y A Aπ Moreover cid3π cid3 vπ j vπ k cid3k cid3 j 0 S 2 cid3π cid3 S 2 vπ π cid3 S 1 cid3π cid3 S 1 vπ π cid3 Hence j S π j π S j Finally cid17 vπ π cid3 cid17 S S S S v jcid3 j vπ S jcid3 j jS jS 4 5 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1421 From relation cid3π cid3 S S null argument S A Moreover π π S S A allocation weights criteria according π favourable relatively inversion comparison x y S 2 cid3π cid3 S 1 cid3π cid3 We interested generation explanation user Let A Exx y v F ψIVT minimal sense cid18 Relation 5 means cycle S π ordering π assigns largest weights negative criteria criteria smallest value cid3 smallest weights positive criteria Proposition 9 shows weights assigned differently cycle S preference x y reversed According Proposition 9 j S cid3i cid3 j vπ vπ j We y cid4EU weight v j assigned criterion j The explanation focus following pairs v x exist j S cid3i cid3 j weight v assigned criterion larger cid5 R S j S 2 cid3i cid3 j v v j cid6 For R S 2 R seen binary relation deﬁne transitive closure R R smallest subset S 2 R R cid7 j R j k R k R cid8 The set R S stable transitive closure Let denote R For explanation restrict pairs R transitivity Let R cid16 SA R S S S pairs R S R R S smallest subset R S 2 R R S S S deduced R The explanation consist concatenation elementary text pair R We organize arguments sets arguments CPS positive strong CPRS positive relatively strong CNW negative weak R CNRW negative relatively weak set pairs arguments CPN positive negative argument Let j R We cases ii j A j A y x A y x v v j This situation illustrated Example 10 Section 831 The strength positive argument j larger negative argument If v j cid2 1 n j added CPS added CNW If previous condition hold v cid21 v j pair j added CPN Finally previous conditions hold j added CPS v j cid2 1 n Note previous cases necessarily holds n added CNW v cid3 1 n v cid3 1 y x cid3 j cid3i v v j This situation illustrated Example 10 Section 831 The fact pair j selected emphasises fact criterion j stronger positive If criterion j smaller weight v criterion criterion larger weight v j decision opposite Criterion present selected pair j comparison situation criterion j Hence criterion mentioned explanation Therefore v j cid2 1 n j added CPS j added CPRS y x cid3 j cid3i v v j The situation dual ii Likewise striking criterion negative weaker j Criterion j present emphasis difference mentioned Therefore v j cid3 1 n added CNW added CNRW iii j A Interpretation 3 In anchor ψIVT positive reasons option y compared situation anchor ψNOA y better average x We looking permutations assignment weights criteria invert decision These permutations identify decisive criteria More precisely Proposition 9 permutation π selected assigns cycle π largest weights negative criteria smallest weights positive criteria Hence criterion property satisﬁed permutation selected The criteria likely selected positive arguments strong negative ones weak Yet selection process ψIVT seen relaxation ψNOA consider positive criterion weight lower 1 n negative criterion weight greater 1 n The arguments displayed elements R contained C cid22CPS CPRS CNW CNRW CPNcid23 Several examples computation C given Section 831 The explanation following y preferred x y better x criteria CPS important criteria CPRS relatively important x better y criteria CNW important criteria CNRW important criterion j y better x important criterion y worse xfor jCPN The argument brackets repeated j CPN 1422 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 In weight π vA v NA weights remain nonselected criteria N A In case unlike anchor ψNOA property nonselected criteria It possible consider minimal explanation sets A smallest number elements concatenate explanation sets This illustrated Example 15 Section 84 We wish compute minimal element A Exx y v EU ψIVT First search A search ing permutation π Proposition 9 shows A minimal restriction π S equal π S S A π N A Let deﬁne S N DEU S cid17 jS v jcid3 j min π Π S cid17 jS vπ jcid3 j By Lemma 3 DEU S candidate coalitions A We set cid2 jS v j vπ S jcid3 j By Proposition 9 DEU S 0 S A The subsets S DEU S 0 cid5 S S N DEU S 0 Aπ S S cid6 If Aπ S cid4 S coalition structure Aπ S S1 Sr decomposed subcoalitions S1 Sr S coalition S replaced simpler coalitions S1 Sr S discarded This Since DEU S explains condition Aπ S S deﬁnition S The elements S labelled following order DEU Sr DEU S1 S T 1 T 2 T p p S T 1 cid2lexi T 2 cid2lexi cid2lexi T p cid2lexi deﬁned S cid2lexi T S T cid7 S T DEU S cid8 cid2 DEU T This lexicographic ordering looks ﬁrst cardinality value DEU It interpreted terms simplicity presenting coalition explanation This order relation extended explanation sets We deﬁne cid4discri Ex follows For A1 Aq B1 Br Ex A1 cid2lexi cid2lexi Aq B1 cid2lexi cid2lexi Br A1 Aq cid4 B1 Br discri cid7 k 1 t Aq Br Aqk1 Brk1 Aqk Brk cid8 cid7 Aq Br q r A1 Brq1 cid8 t q r A1 Aq cid4discri B1 Br A1 Aq cid4 discri B1 Br q r A1 B1 Aq Bq k 1 t D A1 D B1 D Ak1 D Bk1 D Ak D Bk In explanation set A1 Aq coalitions explained user Aq complex coalition set Hence comparing explanation sets compare complex coalition ﬁrst explanation set second case equality second complex coalition This described cid4 discri When explanations sets coalitions cardinality looks value D This depicted cid4discri We deﬁne discri Ex follows notation set A1 Aq discri B1 Br q r A1 B1 Aq Bq D A1 D B1 D Aq D Bq Finally deﬁne cid18discri A cid18discri B A cid4discri B A discri B The order cid18discri complete order reﬁnes cid18 sense 21 A cid4 B A cid4discri B ii A B Ex A cid16cid4 B B cid16cid4 A A cid4discri B We deﬁne Algorithm AlgoEU Table 1 determine minimal explanation set A In algorithm B contains best explanation set sense complete order cid18discri far Proposition 10 Algorithm AlgoEU returns nonempty explanation set Moreover outcome Algorithm AlgoEU element Exx y v EU ψIVT minimal sense cid18 Finally outcome Algorithm AlgoEU minimal argumenta tion set Exx y v EU ψIVT sense cid18discri C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1423 Table 1 Algorithm determination A Algorithm AlgoEU For k 1 p If T A cid2 If The algorithm returns output Algo 0 AlgoA B k L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 cid2 H EU D EU SA D EU T S C A T Branching return C AlgoA T B Updating best explanation set If C cid16 B C cid2discri B B C Bounding If B cid16 A T cid16cid2discri B return B v y x To end section Algorithm AlgoEU easily modiﬁed generate minimal explanation sets sense partial order cid4 discri As said earlier wish enrich generated text considering explanation sets sets shall minimal elements Exx y v EU ψIVT sense cid4 discri To end cid4discri replaced cid4 discri algorithm Moreover output B algorithm unique explanation set collection explanation sets Accordingly B collection explanation sets Then line L8 replaced following lines If C cid16 B B C If C cid16 B cid16 cid2D B D cid4 C B B D B C cid4 discri D C discri Moreover line L10 replaced If B cid16 D B D cid4 A T return B discri Extending Proposition 10 obtain modiﬁed Algorithm AlgoEU returns minimal elements Exx y v EU ψIVT sense cid4 discri 62 Weighted majority model Proposition 11 Let A Exx y v Maj ψIVT minimal sense cid18 Let π ΠN x cid3EU Aπ Let S A Then 2 cid3 S cid3 3 For k j S k cid16 j k j belong set A Moreover y x A π vAv NA y A y x y x A If j A If k A y x k A y x j A y x j A y x k A y x k A y x j A y x j A y x k A y x k A y x j A y x vπ k vπ j y x vπ k vπ j y x A From Proposition 11 following permutation π largest weights coalition S assigned negative arguments smallest weights assigned positive arguments When S 3 S y x The idea criteria S largest weight assigned A negative argument smallest weight assigned positive argument The explanation previous section First set R computed Then ﬁve sets arguments CPS CPRS CNW CNRW CPN constructed The generated text similar previous section y x A Algorithm AlgoEU described Section 61 adapted minor changes determine minimal explanation set Maj model One needs change DEU cid17 DMaj S v j sgn j min π Π S cid17 jS jS cid2 vπ j sgn j v j vπ S j sgn j S cid17 jS condition SB DEU S DEU S cid2 H EU v y x cid2 SB D Maj S D Maj S cid2 H Maj v y x 63 Pessimistic qualitative model Proposition 12 Let A Exx y v Pess ψIVT minimal sense cid18 Let π ΠN x cid3Pess Aπ For S A exist K S A y x A y x S J S A y x S π vAv NA y A 1424 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 cid5 vπ cid9 A y x S cid10 cid6 J S vπ k k K S cid2 vπ j j J S cid10 y x A y x A vπ cid9cid9 cid5 cid10 S cid6 K S The sets K S J S S A satisfy following properties There exists subset S A K S cid16 For coalition S K S 1 For S A J S K S If S N K S J S cid16 N Let S A If K S cid16 K S k PessNk π v h x h PessN J S k π v x The relation Proposition 12 means criteria J S count overall evaluations x y Interpretation 4 Without permutation π situation described Proposition 12 Hence largest weights coalition S A assigned negative arguments J S smallest weights assigned positive arguments K S The idea selected criteria positive arguments larger weight negative ones easy understand As Section 831 compute R pair A S B S A S S A negative j positive null v v j The explanation following y x J S B S S A y x A C cid22CPS CPRS CNW CNRW CPNcid23 More precisely S A deﬁne y x K S A S j B S y preferred x criteria A S y better x important criteria B S y worse xfor SA When K S J S S explanation given From inequalities contained 17 proof Propo sition 12 marks x y positive argument k bad saved small weight criterion k 1 vπ j relatively large j S k Moreover marks x y negative arguments J S good From relation Proposition 12 relatively bad overall utility x attained S criteria The explanation following bad scores x y positive argument k saved small weight marks x y criteria S good relatively bad overall utility x compared y criteria SS Let turn computation minimal element Exx y v Pess ψIVT It necessary explore combinatorial structure We use characterization situation anchor ψRMG described Proposition 19 given Section 73 According proposition exists permutation inverts preference x y conditions ii iii Proposition 19 violated Proposition 20 gives equivalent condition satisfaction This condition easily checked In proof proposition permutation π cid8 1 constructed violated In case y cid27Pess π cid8 1 x construction Condition ii Proposition 19 easily checked practice When violated permutation π cid8 2 constructed proof proposition It satisﬁes y cid27Pess π cid8 2 x construction Condition iii Proposition 19 equivalent relation 12 according Proposition 21 Condition 12 easily checked practice This condition conjunction conditions When ﬁrst 12 violated y cid27Pess x π cid8 3 π cid8 3 deﬁned proof Proposition 21 When second 12 violated y cid27Pess π cid8 4 x π cid8 4 deﬁned proof Proposition 21 A permutation ensures switch preferences easily constructed cases One reﬁne permu tations obtain cycles smallest cardinality This provides minimal element Exx y v Pess ψIVT 7 Anchor remaining case We assume section anchor ψRMG applies A y x cid16 N y cid4F wF x π ΠN y cid4F π v x The following result shows relation 6 implies middle relation 6 holds 6 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1425 Lemma 4 For models cid3Maj w cid3Pess w cid3EU w π ΠN y cid4F π v x y cid4F wF x It proves anchor invert apply decision reference weights Further preference y x probably strong reference weights 71 Expected utility model By Lemma 3 exist π ΠN H EU π v y x cid3 0 0 min π Π N π v y x H EU H EU π N v y x A necessary suﬃcient condition anchor ψRMG applied A y x cid16 N H EU π N v y x 0 Lemma 5 One v WEU z 0 1n w EU z 1 hEU n cid17 π Π N hEU π v z There geometrical interpretation result The set weights π v π permutation forms vertices polyhedron interpreted set normalized weights The arithmetic mean centre gravity vertices Proposition 13 We π N v y x cid3 H EU H EU w EU y x Moreover assume j N cid3i cid16 cid3 j π N v y x H EU Then H EU wEU y x N v 1 n 7 Lemma 4 F EU follows ﬁrst Proposition 13 When anchor ψRMG applies positive negative arguments Hence 7 holds Proposition 13 suggests basically cases explain y remains preferred x weights switched The ﬁrst case occurs weights v w EU A permutation weights impact comparison x y In case weights signiﬁcantly different Hence y remains preferred x permutation weights y expected better x average We need following propositions understand separation previous cases More precisely establish relationship closeness v reference weight w EU closeness H EU wEU y x H EU π N v y x Proposition 14 If 7 satisﬁed v WEU different w EU w EU y x minπ Π N H EU H EU maxπ Π N H EU π v y x π v y x minπ Π N H EU π v y x cid2 1 n Moreover inequality sharp sense ﬁnd x y v previous relation satisﬁed equality Proposition 15 We set ν max iN Then cid11 cid11 cid11v 1 cid11 n cid11 cid11 cid11 cid11 δ max j cid3i cid3 j χ EU cid11 cid11H EU w EU y x H EU cid11 cid11 π N v y x ν cid3 n 1 n π N v y x H EU H EU π N v y x δ ν cid3 n 1χ EU δ Moreover inequalities sharp 1426 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Following Propositions 14 15 cases ν 0 ν cid3 εν practice εν parameter Interpretation 5 The weights v criteria neutral sense speciﬁcity favour criterion disfavour criterion Hence H EU wEU This means v y cid4EU equal arithmetic mean H EU wEU x This relation easy understand checked user v x follows relation y cid4EU The explanation following y preferred x y average better x criteria weights ν cid16 0 ν εν practice Interpretation 6 When weights criteria different large weights assigned indifferently positive negative arguments inverting preference y x One feels intuitively comes fact value cid3 A y x average larger value cid3 A y x χ EU δ From Proposition 15 ν εν quantity small In ratio denominator δ acts normalization We focus absolute values difference H EU y x small value χ EU necessarily mean comparison x y obvious It result fact options x y relatively close marks Here H EU wEU y x signiﬁcantly larger H EU wEU y x signiﬁcantly larger zero vπ N means positive arguments favour y average signiﬁcantly larger negative arguments We consider subcases Firstly consider case ν relatively small εν ν εν practice εν parameter The explanation following y x 0 The fact H EU wEU y x H EU vπ N y preferred x intensity preference y x A significantly larger intensity preference x y A criteria weights y x y x There remains consider case ν large ν cid2 εν practice Then Proposition 15 quantity large The explanation following χ EU δ y preferred x intensity preference y x A larger intensity preference x y A y x y x 72 Weighted majority model Lemma 6 One v WMaj w Maj y x 1 H Maj n cid17 π Π N H Maj π v y x Proposition 16 Consider alternatives x y X We H Maj w Maj y x cid2 H Maj π N v y x Assume A y x cid16 N A y x cid16 N A y x cid16 N Then H Maj π N v y x H Maj wMaj y x v w Maj 8 Condition 8 means x dominates strictly y criteria y dominates strictly x criteria x identical y criteria A necessary suﬃcient condition anchor ψRMG applied C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1427 A y x cid16 N H Maj π N v y x 0 Lemma 4 F Maj follows ﬁrst Proposition 16 Proposition 17 Let χ Maj H cid11 cid11 cid11 cid11 cid3 4nn 1χ Maj cid11 cid11 cid11v 1 cid11 n Maj wMaj x y H Maj vπ N x y Then N The explanation generated similar Section 71 We following cases ν 0 ν cid3 εν practice Hence H EU v equal arithmetic mean H EU wEU This means y cid4Maj v x follows relation y cid4Maj wEU x The explanation following y preferred x criteria y better x criteria x better y aggregation model majority rule ν cid16 0 ν εν practice Proposition 17 quantity χ Maj small Hence H EU wMaj y x signiﬁcantly larger Maj π N v y x Hence decision majority rule strong The explanation tells little bit H speciﬁcities x y y preferred x criteria y better x ON AVERAGE MUCH stronger criteria x better y 73 Pessimistic qualitative model Lemma 4 F Pess follows following proposition Proposition 18 Let v WPess If permutation π ΠN hPess π v y hPess π v x ncid18 i1 ncid18 yi xi i1 For w WPess hPess z h w z h anchor ψRMG Pess model following propositions z h Pess A w Pess A w yx yx yx Pess A w z We characterization Proposition 19 Let cid5 E 1 xi A cid6 y x cid5 1 vπ v N j j cid5 cid11 cid11 A cid11 cid11 y x cid6cid6 1 Let e median value discrete set E We π ΠN h Pess A π v yx x cid3 e Moreover H Pess π v y x 0 π ΠN following propositions hold Pess A π ΠN h π v y x A ii A iii π ΠN hPess yx y h y x yi e Pess A π v π v x h yx Pess A π v yx x Pess A π v yx x x h cid3 Proposition 20 Let V cid16 1 vk cid3 kV A y x A yx yi yi Let V k N 1 vk cid2 yi k 1 vk The following propositions equivalent Pess A π ΠN h π v ii V cid11 cid5 cid11 V yx y h Pess A π v yx x y x x j 1 vk cid6cid11 cid11 j A 9 V deﬁned 10 1428 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Proposition 21 Let j cid5 I A y x xi x j A y x A y x x j cid3 j A yx A yx x j Let cid6 Then following relations equivalent π ΠN hPess π v x h Pess A π v yx x h Pess A π v yx x I cid16 cid11 cid11k N 1 vk cid2 x j cid11 cid11 I 11 12 A necessary suﬃcient condition having y cid4Pess π v x π ΠN satisfaction condition ii Proposition 19 condition ii Proposition 20 relation 12 Proposition 21 From properties easily check anchor ψRMG applied We interested generation explanation user From Proposition 19 conditions ii iii hold According Proposition 20 V 10 Assume ﬁrst V The worse scores x A y x clearly smaller yi When V y x worse score x This weight hide difference worst score yi y A means weights large aggregation function close minimum The ﬁrst explanation follows y preferred x weight hide worse score x compared y A y strictly better x A y x The aggregation function Minimum Recall y x y x x The case 10 holds The set L j A y x If V L bad score The set V gathers weights hide worst score yi y A weight worse score x compared y saved The ﬁrst explanation follows y x x j 1 vk contains criteria A y preferred x weight hide worse y x The aggregation function close scores x compared y A Minimum Recall y strictly better x A y x The midpart explanation concerns point ii Proposition 19 The overall score x lower e Thus y y x The explanation following y x A better overall utility x A y better overall score x criteria A y x y better x criteria A y x A y x A y x Recall The explanation concerns point iii Proposition 19 Proposition 21 From relation I cid16 y x From relation k N 1 vk cid2 x j I weight y x The end explanation y x saved scores x A y x A worse scores x attained A worse scores x A follows Finally worse scores x attained A weight worse scores x A x A y x A y x The aggregation function close Minimum y x saved scores y x There 8 Illustration experimental results The aim section threefold Firstly process generating explanation summarized Section 81 Secondly wish approach helps user better understand decision tries The anchors seen metaexplanations explanation schemas 57 It aims selecting decisive criteria comparison y cid4F v x The decisive criteria inﬂuencing criteria decision determination helps user better understand decision actions decision improvement actions We illustrate anchors behave examples The illustrations focus models EU Pess The reason model Maj considered Maj model seen particular case EU model applied binary alternatives Indeed C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1429 y cid4Maj v x y cid4EU v x y x y deﬁned x y relations x 0 Sections 82 83 discuss anchors ψNOA ψIVT respectively illustrative examples We consider speciﬁcally anchors ψALL ψRMG interpretation straightforward particular require selection subset criteria Moreover Section 84 presents comparative study explanations anchors ψNOA ψIVT ψRMG EU model 1 xi cid2 yi x 0 y 1 yi cid2 xi The aim section present experimental results concerning practical determination explanation Section 85 81 Generation process explanations The generation explanation given instance x y v DF obtained follows One aims determining nondominated element Exx y v F wrt cid2 Due lexicographic structure cid2 Exx y v F ﬁrst identiﬁes anchor applied The conditions anchor applied presented paper More precisely ψ Ψ necessity suﬃcient conditions x y v x y v DF ψ We identify preferred anchor ˆψ set ψ Ψ x y v DF ψ according cid2 Then compute nondominated explanation set ˆA Exx y v F ˆψ anchor ˆψ This presented previous sections Considering EU model trivial ψALL ψRMG anchors requires sort vector ψNOA anchor solved Algorithm AlgoEU anchor ψIVT Finally ˆψ ˆA corresponding textual explanation generated Examples text presented earlier paper 82 Illustration anchor average 821 Model EU Let ﬁrst compare Klein 35 approach In approach selected criterion anchor ψNOA necessarily fulﬁls v 1 n cid3i 0 Proposition 1 A criterion likely selected negative weak argument positive strong argument On hand Klein approach criteria cid3i 0 v 0 unlikely selected There main situations approaches different results The ﬁrst ones speciﬁc anchor ψNOA speciﬁc anchor In ﬁrst criterion weak negative argument It intuitive criterion Our approach selects criterion Kleins approach usually In second situation criterion strong negative argument Kleins approach selects criterion This nondecisive criterion compensated positive arguments y preferred x Hence necessary criterion user synthesis In situation criterion medium argument v 1 n cid3i cid19 0 Kleins approach selects criterion usually The strength argument corresponds reference weight 1 n mind Since different prior useful explicitly user It better focus criteria standard weights small large We examples illustrate main properties approach As shown Example 5 approach selects arguments Example 5 decision clear cut On opposite decision tight criteria selected approach Example 6 It intuitive tighter decision arguments selected This occur Klein approach Examples 5 6 Other examples criteria Section 84 Example 5 The decision relatively clearcut approaches select opposite arguments Consider situation x 042 066 066 057 y 054 004 089 076 v 041 006 024 029 Then cid3 012 062 023 019 y signiﬁcantly preferred x hEU v y 066 hEU v x 054 Criteria v 1 compeli n cid3i v cid3i 1 2 0019 0049 0118 0037 3 0002 0055 4 0008 0055 The Klein approach selects arguments case criteria 1 3 4 decision relatively clearcut On hand approach selects argument criterion 2 example Looking example criterion 2 main argument x better average y y preferred x decision model 1430 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Example 6 The decision tight approach returns arguments Let consider x 095 067 064 027 039 y 03 037 041 094 049 v 018 011 012 024 035 Then cid3 065 03 023 067 01 y slightly preferred x hEU v y 054 hEU v x 052 Criteria v 1 compeli n cid3i 1 2 3 4 5 0013 0117 0027 0033 0018 0028 0027 0161 0015 0035 The Klein approach selects criteria 1 4 On hand approach criteria 2 3 4 5 selected 822 Model Pess Several examples given Example 7 illustrates case M clearcut number selected criteria small The general case M This example shows decision cid16 represented Example 8 Example 7 Clearcut decision 10 criteria Consider following values x y v y 01 0 066 097 045 071 057 077 009 069 x 014 046 026 08 069 064 007 017 094 045 v 013 053 045 012 083 057 10 019 058 023 v v x 007 hPess y 042 y cid4Pess x The selected criterion 2 Moreover criterion kx 7 decisive We hPess The worse score x reached criterion positive strong argument We M 2 M generic text generated S kx M The sentence generic explanation text y suﬃciently good remaining criteria 1 3 4 5 6 8 9 10 We remark scores y criteria 1 9 good 01 x 007 x The decision changed weight 009 larger score hPess 1 assigned criteria 1 9 It turns criteria medium importance w Pess implies overall score y larger x But necessary explanation v v Example 8 Tight decision 10 criteria Consider following values x y v y 036 001 022 011 061 04 006 007 043 061 x 025 021 014 008 053 015 02 053 087 075 v 10 002 086 023 091 053 036 017 026 053 We hPess M v x 014 hPess y 022 y cid4Pess x The selected criteria 2 7 8 Moreover kx 3 M 2 4 7 8 4 Indeed worse score x reached criterion 4 positive strong argument v v 83 Illustration anchor invert 831 Model EU In cases encountered cycles minimal permutation cardinality 2 A cycle size 3 presented Example 9 The second example contains 10 criteria It possible consider explanation sets minimal cardinality order generate textual explanation Example 15 later Example 9 Permutation criteria Let x 089 003 007 032 038 y 036 076 06 025 075 v 006 011 021 029 033 Then cid3 053 073 053 007 037 We compel 0031 008 0111 002 0122 The Klein approach selects criteria 5 3 On hand minimal element Exx y v EU ψIVT 1 3 5 For associated permutation π score criteria 1 5 3 assigned weight criteria 5 3 1 respectively π 5 1 π 3 5 π 1 3 We note π disfavourable permutation y 1 3 5 1 3 1 5 Finally CPS 3 5 CNW 1 sets tuple C It easy R Section 61 Example 10 Example 10 criteria Let y 045 064 086 076 087 054 017 004 055 005 x 061 028 008 002 081 015 016 038 024 075 cid3 016 036 078 074 006 039 001 034 031 07 v 013 004 012 01 007 019 015 003 001 016 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1431 We compel 0021 0014 0094 0074 0004 0074 0001 001 0003 0112 The Klein approach selects criteria 10 3 6 4 On hand approach explanation set 6 8 3 9 Regarding pair 6 8 criterion 6 positive important criterion 8 negative important Regarding pair 3 9 criteria positive arguments criterion 3 positive important criterion 9 Criterion 9 mentioned Finally CPS 3 6 CNW 8 sets tuple C 832 Model Pess The following examples illustrate different situations size minimal explanation sets minimal explanation set composed subset cardinality 3 Example 11 subsets cardinality 2 Example 12 Example 11 A selected set cardinality 3 Let y 083 066 019 055 094 075 x 063 00 015 098 097 081 v 082 08 10 01 028 026 We y cid4Pess x The minimum selected coalitions Exx y v Pess ψIVT cardinality 3 2 3 4 2 3 5 2 3 6 We note criteria 2 3 present selected sets These criteria positive arguments large weight The overall score x y attained criterion 3 large importance The criteria 4 5 6 appear selected criteria negative arguments small weight v Example 12 Example 9 criteria Consider following values x y v y 043 057 028 05 046 03 06 048 083 x 005 089 022 00 082 074 086 017 076 v 10 07 067 029 027 029 078 087 067 v We y cid4Pess x The selected set composed cycles 1 2 6 8 In cycle 1 2 criterion 1 positive argument large importance criterion 2 negative argument importance lower criterion 1 Moreover overall score x attained important criterion 1 x small score Concerning cycle 6 8 criterion 6 negative argument small weight criterion 8 positive argument large weight The explanation says criterion 1 positive argument important criterion 2 negative argument criterion 8 positive argument important criterion 6 negative argument Finally remaining criteria y good evaluations 84 Comparison anchors EU model In order compare anchors ﬁx vectors x y consider different values weight v Clearly anchor ψALL holds weight vector v anchor holds value v Hence focus anchors ψNOA ψIVT ψRMG We consider following values x y y 099 035 031 051 062 057 052 x 05 006 003 095 087 02 095 Option y average better x The following examples v given Example 13 Anchor ψNOA Let v 006 011 019 011 031 008 014 Option x preferred y The Klein approach selects criteria 5 7 The selected criteria anchor criteria 1 5 Criterion 1 weak negative argument criterion 5 positive strong argument The criteria explicitly mentioned importance relatively close mean importance 1 n Example 14 Anchor ψNOA Let v 014 005 017 023 017 011 013 Option x preferred y The Klein approach highlights criteria 4 1 Our approach selects criteria 2 4 Criterion 2 weak negative argument criterion 4 positive strong argument The criteria previous case importance close mean importance 1 n explicitly mentioned Example 15 Anchor ψIVT Let v 011 014 013 002 027 025 008 Option y preferred x The Klein approach highlights criteria 6 5 There elements Exx y v EU ψIVT cardinality 2 4 6 6 7 In selection 4 6 criterion 6 positive strong argument criterion 4 weak negative argument We note criterion 6 belongs second selection criterion 7 second selection weak negative argument At end criteria 4 6 7 displayed explanation 1432 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Fig 1 Results experimental study The columns occur percentage occurrence anchor The mean computation time presented column mean execut time The mean percentage tree explored search appears column perc tree explor Finally column 1st coalition expl set corresponds percentage times algorithm terminates ﬁrst step Example 16 Anchor ψIVT Let v 024 02 025 006 002 019 004 Option y preferred x The Klein approach highlights criteria 1 6 The preferred explanation set Exx y v EU ψIVT 1 7 3 4 In pair 1 7 criterion 1 positive strong argument criterion 7 weak negative argument In pair 3 4 criterion 3 positive strong argument criterion 4 weak negative argument The criteria 1 3 4 7 displayed explanation Example 17 Anchor ψRMG Let v 016 014 015 01 016 015 014 Option y preferred x The Klein approach selects criteria 1 7 We situation Interpretation 6 intensity preference y x positive criteria 1 2 3 6 signiﬁcantly larger intensity preference x y negative criteria 4 5 7 Moreover criteria weights Example 18 Anchor ψRMG Let v 012 016 015 016 015 014 012 Option y preferred x The Klein approach selects criteria 4 1 We situation Interpretation 5 Compared previous situation weights closer arithmetic mean Even y average signiﬁcantly better x need use argument We seen anchors triggered different values v The seven criteria selected previous examples anchors ψNOA ψIVT This shows inﬂuence weights selection process 85 Experimental results We present section results experimentations conducted approach The tests mainly concerned computational performance determination nondominated explanation set Among models EU Maj Pess restrict case EU model We seen methods algorithms select explanation set similar models EU Maj Hence necessary perform experiment models Moreover determination nondominated explanation set hand model Pess end Section 63 requires computation time The computation necessary determine nondominated explanation set time consuming anchors ψALL ψNOA ψRMG needs sort vector n components By contrast Algorithm AlgoEU anchor ψIVT requires search explanation large tree The computation time shown anchor ψIVT Our approach implemented Java tested Intel Pentium Core 2 266 GHz The experimentations performed randomly generated instances set DEU Fig 1 shows percentage occurrence anchor experiment Concerning Algorithm AlgoEU mean execution time given To analyse eﬃciency branching strategy algorithm Fig 1 indicates mean percentage tree explored search We noticed algorithm terminates ﬁrst iteration This situation arises T 1 explanation set T 1 smallest element S according cid2lexi Fig 1 presents results values n 4 20 These commonly encountered values number attributes practice According Fig 1 algorithm terminates ﬁrst coalition S case This shows strategy chosen ranking coalitions S according cid2lexi eﬃcient Moreover percentage search tree explored algorithm decreases rapidly n The worse scenario Algorithm AlgoEU occurs explanation set grand coalition N permutation π Aπ N In case 2 subsets S S cid3 2n 1 explored ﬁnding explanation set S C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1433 For small values n anchors occur likely ψIVT ψRMG The probability occurrence anchor 2n1 decreases rapidly n For larger values n preponderant case anchor ψIVT 1 ψALL If ν 1 n weight vector v clearly close reference weight w EU Concerning anchor ψRMG obtained good separation subcases anchor values εν 015 n εν 03 n 9 Conclusion We propose approach select arguments explanation prescription multi attribute decision model parameterized weights assigned criteria It based analysis values weights relative scores options compared The general approach applied decision models expected utility model EU weighted majority model Maj weighted minmax model Pess These models wellknown weightbased models decision theory Moreover represent different visions decision theory The idea approach look changes weight vector v yield inversion prescription decision model The explanation focuses criteria weight vector changed The remaining criteria play role inversion prescription mentioned explanation Not change weights change shall explainable user In paper strategies permutation modiﬁcation weights considered replacement v reference weights w weights v criteria These strategies lead different explanation strategies They called anchors ψNOA ψIVT respectively F There possible changes weights compatible anchor All admissible changes repre sented single combinatorial structure Exx y v F containing explanation sets Since interested simplest explanation simplest changes sought In structure Exx y v F expressed order relation cid18 One looks nondominated elements Exx y v F sense cid18 The properties nondominated explanation sets studied anchors ψNOA ψIVT Concerning anchor ψNOA shown models EU Maj Pess positive selected arguments turn strong negative selected arguments weak Moreover selected arguments qualitative Pess model necessarily negative This comes fact min operator expresses principle elimination criteria An explanation easily generated properties The computation particular nondominated explanation set easy needs rank vector Concerning anchor ψIVT obtain roughly idea weaker form More precisely allocation weights criteria unfavourable relatively inversion prescription models EU Maj This implies positive arguments important weights negative arguments A branch bound algorithm proposed compute particular non dominated explanation set The underlying combinatorial structure large isomorphic set coalition structures In algorithm coalition structures coalitions small cardinality explored ﬁrst Experimental tests shown strategy enables ﬁnd quickly explanation set cases The anchors ψNOA ψIVT cover cases x y v DF Another case simplest ψALL occurs positive arguments The explanation need mention speciﬁcities decision model trivial situation The case ψRMG gathers situations covered anchors Interestingly able divide DF ψIVT typical subcases For instance EU model weights close reference weights clearly positive arguments negative ones We shown numerous illustrative examples explanation generated helps understand decision This comes fact approach selects decisive criteria criteria given change weight switches decision The type relevant change dependant anchor This work beneﬁt extensions First framework applied weightbased decision models An example model weighted maxmin function 22 This model optimistic counterpart weighted minmax function hPess Some preliminary results given 39 suggest extension approach possible model Secondly approach extended models complex parameters simple weight assigned criterion One think Choquet Sugeno integrals extend EU Pess models respectively 1553 The parameters discrete integrals correspond concept capacity contains 2n coeﬃcients 15 The generation explanation Choquet integral wrt particular case capacity addressed needs studied 3843 In paper attention paper structuration expression selected arguments The textual explanations proposed paper given type explanation anchor yields This improved incorporating dedicated techniques generation natural language The explanations resulting approach aim ﬁnding arguments root prescription The explanation produce complex generated 143543 explanation reasoning sound We analyse deeply speciﬁcity model deduce set selected arguments For reason approach suited applications high addedvalue decision model expected This means recipients approach domain experts simple users Internet To cite example application mention decision aid selection candidate architectural options design complex 1434 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 systems requires elaborate decision models 47 However shown explanation generated far complex EU Maj models An interesting property framework explanation adapts automatically complexity prescription On hand explanation complex Pess model The min max binary operators extensions Boolean AND OR operator nonBoolean spaces The weighted minmax seen complex condition combining AND OR operators This indicates explanation complex needs arguments An extension work generate simpliﬁed explanation model 10 Proofs 101 Proof Section 3 Proof Lemma 1 In order compute maximum minimum values component elements polytope V EU y x consider vertices V EU y x polytope 16 First following set cid5 w WEU A y x A cid6 included V EU y x Hence A 0 1 interval y x w 0 y x 1 set admissible values weight w A y x cid2 For ε 0 let U ε set w Rn deﬁned n 1 inequalities w 1 cid2 0 wn cid2 0 kN cid3k wk cid2 ε ε0 U ε A point w U ε vertex U ε n 1 inequalities equality replaced corresponding equalities 16 Theorem 181 Let j 1 n 1 index inequalities transformed equalities kN wk 1 Then V EU y x cid16 The ﬁrst case j 1 n For ε small necessarily A y x j A y x From cid2 relations cid3 j w j ε w w j 1 obtain w cid3 cid3 cid3 j ε cid3 j ε cid3 j w j cid3 cid3 w k N j wk 0 The second case arises j n 1 We obtain A y x w 1 k N wk 0 Assume A y x cid16 A y x cid16 Hence w min j A yx 1 A y x w j cid3 y x The lemma shown relations hold ε 0 By construction 0 maxi A yx boundaries intervals reached intervals sharp cid5 j A cid3 cid3 ε cid3 j ε cid3 j cid3 j 102 Proofs Section 5 Proof Lemma 2 The lemma clear Let Consider S Exx y v F ψNOA assume 3 holds When F EU proof Proposition 1 3 holds vk 1 n cid3k 0 k S Let T S T cid16 S For k S T 13 H EU v T w EU NT y x H EU v Skw EU NSk y x cid12 cid17 iST k cid15 cid3i v 1 n By 3 H EU v SkwEU NSk y x cid3 0 We conclude H EU v T wEU NT y x 0 T Exx y v EU ψNOA This proves coalitions satisfying 3 necessary minimal Exx y v EU ψNOA The proof similar F Maj thanks relation 14 given Lastly consider case F Pess Let T S T cid16 S From 3 hPess v Skw Maj NSk x cid2 y k S T Lemma 7 shown holds strict inequalities replaced nonstrict hPess v Skw Maj NSk inequalities We obtain cid18 kST For z X hPess v Skw x cid2 Maj NSk cid18 kST hPess v Skw Maj NSk y C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1435 cid18 kST hPess v Skw Maj NSk z cid18 cid9 iT cid18 cid9 cid10 zi 1 v cid10 zi 1 v iT hPess v T w z Maj NT cid18 S cid18 S zi zi cid18 cid7cid9 cid10 zk 1 vk zk cid8 kST cid18 kST zk Hence conclude hPess v T w Maj NT x cid2 hPess v T w y Maj NT This proves T Exx y v Pess ψNOA cid5 Proof Proposition 1 Let S Exx y v EU ψNOA minimal k S Clearly 3 satisﬁed Hence H EU v S wEU NS H EU v SkwEU NSk y x cid3 0 gives H EU v S wEU NS cid12 y x H EU cid15 v SkwEU NSk H EU v S w EU NS y x H EU v Skw EU NSk y x vk 1 n cid3k y x 0 From relation y x 0 13 infer vk 1 n cid3k 0 This concludes proof cid5 Proof Proposition 2 The integer p deﬁned Proposition 2 exists N Exx y v EU ψNOA Exx y v EU ψNOA We set S π p π n Let k S We write vk 1 n vπ p 1 n π 1k cid2 p y x H EU cid3k 13 y x y x v Skw EU cid3 H EU NSk cid3π p v S w EU v S w EU H EU NS NS cid15 cid15 cid12 cid12 H EU v Sπ pw EU cid12 NSπ p y x H EU v S w EU NS y x H EU v Sπ pw EU NSπ p y x cid12 vπ p 1 n cid15 cid15 cid3π p H EU v Sπ pw EU NSπ p y x 13 cid3 0 S π p Exx y v EU ψNOA Hence S minimal Exx y v EU ψNOA By 13 S N H EU v S w EU NS y x H EU w EU y x cid12 cid17 kS vk 1 n cid15 cid3k kS vk 1 By deﬁnition p clear S set smallest cardinality minimal element Exx y v EU ψNOA strictly lower cardinality S cid5 n cid3k H EU wEU y x Hence cid2 Proof Proposition 3 Let S Exx y v Maj ψNOA minimal k S We H Maj v S w Maj NS y x H Maj v Skw Maj NSk y x Hence result shown thanks 3 cid5 cid12 cid15 vk 1 n sgnk 14 Proof Proposition 4 Thanks 14 proof similar Proposition 2 cid5 cid3 Proof Proposition 5 Let S Exx y v Pess ψNOA minimal Let k S We set b S xi We obtain 3 iSkxi 1 v cid3 cid9 cid10 yk 1 vk yk cid3 b xk b cid9 cid10 xk 1 vk cid3 iSk yi 1 v cid3 S yi 1436 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 We wish infer inequality relations b xk yk 1 vk previous relations It possible standard reasoning inequalities However tedious lengthy easy We propose systematic check possible cases comparison variables b xk yk 1 vk Since previous relations contain min max operators analysis depend numerical values b xk yk 1 vk relative ordering Hence performed As result generate possible comparisons lower equal greater variables b xk yk 1 vk previous inequalities satisﬁed At end pair variables analyse comparisons variables compatible inequalities Considering instance variables b outcome relation b following comparisons b cid3 b b cid2 b b This approach proof Proposition 12 We obtain following invariant comparisons yk cid3 b 1 vk yk cid3 xk notation b 1 vk means b b 1 vk Hence k A y x A y x Applying relation k S obtain S A y x A y x Since yk 1 vk k weak negative argument We cases S cardinality 1 S k Then yk 1 vk cid2 1 vk b cid18 cid18 yi b xi S S cid18 S xi ii S cardinality 2 Since b cid18 cid9 cid10 yi 1 v iSk cid18 S cid18 cid9 cid10 xi 1 v yi iSk cid18 S xi Since yi cid3 xi S Lemma 7 cid18 cid9 cid10 yi 1 v cid3 cid18 cid9 cid10 xi 1 v iSk iSk Hence k S cid18 cid18 xi yi S S cid18 S cid18 cid9 cid10 yi 1 v xi iSk This gives cid18 cid18 cid18 cid9 cid10 yi 1 v xi S kS iSk cid10 yi 1 v cid18 cid9 iS In cases cid18 S xi cid18 S yi cid18 S xi cid18 cid9 iS cid10 yi 1 v cid5 Proof Proposition 6 Assume hPess vkx xkx 1 vkx We obtain contradiction ykx cid3 1 vkx Hence ykx 1 vkx Furthermore ykx xkx x attained kx N Since hPess x particular ykx 1 y hPess v v v Moreover k M yk 1 vk xkx 1 vkx 1 vk xkx 1 vkx Let S Exx y v Pess ψNOA minimal assume S M cid16 Let k S M Then yk hPess hPess v S wPess v SkwPess NS minimal Proposition 5 hPess y hPess NSk y yk Since S Exx y v Pess ψNOA hPess v S wPess NS cid3 iNS xi Hence iNS xi yk hPess PessNS v y hPess x h x x cid2 cid3 v v S wPess NS v x We x Since S hPess v Skw Pess NSk Since k A y x A v S wPess NS cid18 y xi iNS y x xk cid2 yk cid3 iNS xi cid3 iNS xi cid3 iNSk xi Therefore C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1437 hPess v Skw Pess NSk y cid18 iNSk xi cid2 hPess v Skw Pess NSk x Hence S k Exx y v Pess ψNOA contradicts fact S minimal We conclude S M cid5 Proof Proposition 7 By deﬁnition S π y Exx y v Pess ψNOA To S minimal need 1 p 2 D S π y Exx y v Pess ψNOA cid3 N p Exx y v Pess ψNOA π y cid14 D C One C Exx y v Pess ψNOA Moreover N p 1 N N 1 π y N 1 π y N 1 π y N 1 C Let 1 p 2 Set C π y jNC y j yπ y N cid9 cid18 hPess v C w Pess NC y cid10 y j 1 v j yπ y N jC cid18 cid9 jC Hence hPess v D w Pess ND y cid10 y j 1 v j yπ y N cid10 y j 1 v j cid18 cid9 jC cid14 hPess v C w Pess NC y cid2 hPess v D w Pess ND y Furthermore option z relation C D implies following inequality hPess v C w Pess NC z cid3 hPess v D w Pess ND z Applying z y conclude hPess v C w Pess NC y hPess v D w Pess ND y Since C Exx y v Pess ψNOA 15 applied x hPess v D w Pess ND x cid2 hPess v C w Pess NC cid2 hPess v C w Pess NC x y hPess v D w Pess ND y 15 Hence D Exx y v Pess ψNOA This proves π y N 1 π y N p minimal Exx y v Pess ψNOA cid5 103 Proofs Section 6 Proof Proposition 8 Let A Exx y v F ψIVT minimal sense cid18 Let π ΠN A Aπ Then x cid3F π vAv NA y Assume contradiction exists S A S 1 The condition S 1 implies π S Hence x cid3F π vASvNAS y This contradicts minimality A A S cid4 A A S Aπ cid5 Proof Lemma 3 The proof induction s S Let H S w When s 2 cid2 iS w icid3i w WEU H S π S v π S v H S S 1cid3π cid3 S 1 S 2 vπ v Hence result proved s 2 vπ v vπ v S 2cid3π cid3 vπ v S 1 cid3π cid3 S 2 vπ v cid3π cid3 S 2 S 1 cid2 0 S 2cid3π cid3 S 1 vπ v S 1cid3π cid3 S 2 Assume result shown subsets cardinality strictly lower s Let S N S s Let π ΠS Deﬁne σ π π S 1 k π cid3 S s k cid14 π v S s π S k Value cid3k largest number set cid14 cid14 k σ cid14σ 1k cid14 σ k cid14 cid3i S vkcid14 largest number set v S Deﬁne σ cid14 ΠS σ cid14k σ cid14i σ S k cid14 Fig 2 Set π cid14 σ cid14 π S ΠS cid14 σ 1k 1438 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Fig 2 Description σ σ cid14 Fig 3 Description π π cid14 cid17 vσ π S icid3i cid17 iS i1s cid14 Hence vσ π v S icid3π cid3 S Let i1 π v cid14 s i2 π v S 1σ 1k One cid17 H S π v vπ icid3i iS S 1k H S H S π cid14v π v S i1cid3π cid3 vσ cid14π v vσ cid14π v vkcid14 vσ kcid14 cid3k cid3π cid3 S i1 S i2cid3π cid3 S i2 S i2 cid2 0 vσ π v S i1cid3π cid3 S i1 vσ π v S i2cid3π cid3 S i2 deﬁnition k k cid14 Moreover H S induction assumption H π S v H S Sk π S v π cid14v value vkcid14 cid3k criterion k Hence H S π S v H S π cid14v H Sk π S v H Sk π cid14v By cid2 H Sk π cid14v Hence shown H S π S v cid2 H S π cid14v cid2 H S π v Similarly H S π S v cid3 H S π v Hence induction assumption holds s cid5 Proof Proposition 9 Let A Exx y v EU ψIVT minimal sense cid18 Let π ΠN x cid3EU A Aπ Since x cid3EU v NA π v π vAv NA y π vAv NA y choose π way N A π Hence π vA Let S A By Proposition 8 S cid2 2 Let k j S k cid16 j Let π cid14 ΠN deﬁned π cid14 j π k π cid14k π j π cid14l π l l N k j Fig 3 Clearly Aπ cid14 cid4 Aπ Let Acid14 A S S cycles π cid14 minimal element shall Acid14 Exx y v EU ψIVT Hence y cid4EU A Acid14 π cid14l l l N A N Acid14 π cid14 v π cid14 vAcid14 v NAcid14 Hence case y cid4EU To sumup cid14cid14 k π cid14k π cid14 π cid14k containing j k respectively One clearly Acid14 cid4 A Acid14 Aπ cid14 Hence order A NAcid14 x Since π cid14v x cid14 j π cid14 j π cid14 π cid14 j S π cid14v x y cid4EU cid14cid14 S π cid14vAcid14 v cid14 S x cid3EU π v y y cid4EU π cid14v x We obtain 16 16 0 H EU π cid14v y x H EU π v y x vπ j vπ k cid3k cid3 j Therefore 4 holds From 4 cid3π cid3 S l vπ π cid3 π j π S j j S S l vπ π cid3 S l1 l 1 S 1 Hence cid3π cid3 cid16 cid3π cid3 S l1 l 1 S 1 Hence 5 holds We vπ v S l cid3π cid3 S l1 By 4 previous relation implies S S We conclude S 1 cid3 cid3 vπ v Finally inequality proposition follows relation 5 Lemma 3 cid5 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1439 cid2 Proof Proposition 10 Assume contradiction algorithm ends returning explanation set Hence B step search This implies search tree pruned step Bounding subsets elements S explored search This means exist A Ex composed elements S π v y x cid3 0 This contradicts cid2 H EU basic assumption beginning Section 6 v y x Hence exist π ΠN H EU SA DEU S When bounding condition triggered AlgoEU A T cid16cid4discri B Since cid18discri complete order conclude B cid18discri A T In combinatorial structure Ex endowed cid18discri subtree node A T search AlgoA B k iteration pruned elements Ex subtree form A T i1 m cid2 1 T cid2discri T i1 By relation T im B cid18discri A T cid4discri A T T i1 T im sees way strictly better argumentation set Ex node A T We shown bounding condition justiﬁed algorithm Assume algorithm terminates returns B We deﬁne π ΠN π π S S S B π N B We cid17 π v y x H EU H EU v y x DEU S cid3 0 SB Hence B Exx y v EU ψIVT All elements Ex strictly better B explored Algorithm AlgoEU previous steps Since algorithm terminate earlier means C Exx y v EU ψIVT C cid4discri B Hence B minimal sense cid4discri The explanation set B minimal sense cid4 cid4discri reﬁnement cid4 cid5 Proof Proposition 11 The proof follows Proposition 9 In particular k j S k cid16 j deﬁne π cid14 Hence 0 H Maj π cid14v y x H Maj One necessarily sgnk element S sets A vπ j statement proposition follow previous inequality cid5 π v y x vπ j vπ k sgnk cid16 sgn j Hence k j belong set A y x A y x A sgn j y x There y x A y x Hence S cid3 3 The relations vπ k y x A Proof Proposition 12 We proceed proof Proposition 9 In particular k j S k cid16 j deﬁne π cid14 Let h PessN jk π v y b h PessN jk π v x Proceeding proof Proposition 9 obtain relations y cid4Pess cid9 cid9 π cid14v x y cid27Pess π v x 16 Hence α1 b α2 cid3 b cid10 y j 1 vπ k cid10 x j 1 vπ k cid10 y j 1 vπ j cid10 x j 1 vπ j cid9 cid9 cid9 cid9 cid10 yk 1 vπ j cid10 β1 xk 1 vπ j cid10 cid9 yk 1 vπ k cid10 xk 1 vπ k β2 cid9 We interested case k A y x A y x j A y x We cases The ﬁrst case 1 vπ k 1 vπ j The larger weight vπ k vπ j assigned negative argument smaller assigned positive argument The second case 1 vπ k cid3 1 vπ j We proceed exactly proof Proposition 5 We perform sys tematic check possible cases comparison variables b xk x j yk y j 1 vπ j 1 vπ k determine comparisons variables hold We obtain PessN jk xk cid3 yk cid3 h π v 1 vπ k cid3 h PessN jk π v x x h h PessN jk π v PessN jk π v PessN jk π v x y j x j PessN jk x h π v x 1 vπ j y Let K S J S sets indices k j respectively satisfying 17 Let cid14 j contradiction exist pairs k j k k cid14 N j k obtain h k ykcid14 1 vπ kcid14 h cid14 K Scid14 J Scid14 S S PessN jk π v cid14k PessN j π v cid14 y cid3 yk 1 vπ k Hence contradiction raised h 17 SA K S cid3 1 Assume y x k j K S J S cid14 A PessN jk cid14 A satisfying 17 k cid16 k cid14 y Since π v y cid3 ykcid14 1 vπ kcid14 yk 1 vπ k ykcid14 1 vπ kcid14 Similarly y x A y x A From 17 yk 1 vπ k h cid14 j cid2 1440 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 We assume K S cid16 We set K S k Assume S N Assume contradiction J S N k Let j J S Pess J S j x xi 1 vπ From 17 applied pair π v Pess J S π v h k j xi 1 vπ h x x j 1 vπ j Let J S j h PessNk j π v x 1 vπ j 1 vπ 1 vπ j From 17 applied pair k x j 1 vπ j h PessNki π v x 1 vπ 1 vπ j 1 vπ The previous relations clearly contradictory Hence J S cid16 N k PessNk Using previous argument easily cases h π v x xi 1 vπ N J S k Hence PessNk π v h x h PessN J S k π v x y x K S j Let S A To sumup vπ j vπ A y x J S Moreover vπ j cid3 vπ K S j J S The weights assigned positive arguments smallest A ones criterion K S y x K S j A y x A cid5 vπ cid9 A y x S cid10 cid6 J S vπ k k K S cid2 vπ j j J S cid10 y x A y x A vπ cid9cid9 cid5 cid10 S cid6 K S cid5 104 Proofs Section 7 Proof Lemma 5 Clearly z 0 1n 1 n cid17 π Π N π v z hEU cid12 ncid17 i1 1 n cid17 kN cid15 vk zi ncid17 i1 1 n zi hEU w EU z cid5 Proof Proposition 13 By deﬁnition π N permutations π ΠN π N v y x cid3 H EU H EU π v y x Hence Lemma 5 H EU proved wEU y x cid2 1 n cid2 π ΠN H EU π N v y x H EU π N v y x Consequently ﬁrst lemma 18 The second lemma obvious Let Assume H EU wEU y x From 18 Lemma 5 previous relation implies permutation π H EU π N v y x H EU π N v y x π v y x Thus H EU permutation π ΠN H EU v y x H EU π v y x Let cid16 j N π ΠN permutation permuting j leaving elements N Previous relation applied π gives v icid3i v jcid3 j v jcid3i v icid3 j Hence v v j cid3i cid3 j 0 19 Assume 7 holds Let k N Ak N cid3i cid3k Thus N Ak cid16 For Ak j N Ak cid3i cid16 cid3 j v v j 19 We conclude v value Since weights normalized obtained wished result cid5 Proof Proposition 14 Let HEU 1 inf xRn yRn vRn v1vn1 w EU y x minπ Π N H EU H EU maxπ Π N H EU π v y x π v y x minπ Π N H EU π v y x Since H EU v y x hEU v cid3 obtain HEU 1 cid3Rn vRn inf v1vn1 Let cid3 Rn w EU cid3 minπ Π N hEU hEU maxπ Π N hEU π v cid3 π v cid3 minπ Π N hEU π v cid3 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1441 α 1 maxπ Π N hEU π v cid3 minπ Π N hEU π v cid3 β α min iN cid3i Let cid3cid14 αcid3β By deﬁnition β cid3cid14 Rn π v cid3cid14 minπ ΠN hEU β 31 maxπ ΠN hEU Since hEU π v cid3cid14 1 Hence w stable aﬃne transformations hEU w αcid3β αhEU w cid3 HEU 1 inf cid3Rn vRn v1vn1 maxπ ΠN hEU π v cid3minπ ΠN hEU π v cid31 cid19 w EU cid3 min hEU π Π N cid20 hEU π v cid3 Without loss generality assume v 1 cid3 cid3 vn cid31 cid3 cid3 cid3n Then Lemma 3 min π Π N π v cid3 hEU max π Π N π v cid3 hEU ncid17 i1 ncid17 i1 vni1cid3i v icid3i HEU 1 cid3Rn vRn v1cid3cid3vn cid31cid3cid3cid3n v1vn1 inf cid2 n i1v vni1cid3i 1 cid15 vni1 cid3i cid12 ncid17 i1 1 n From Assertions 1 2 HEU 1 1 n Assertion 1 For v WEU w EU v 1 cid3 cid3 vn cid3Rn cid31cid3cid3cid3n i1v vni1cid3i 1 inf cid2 n i1 cid12 ncid17 1 n cid15 vni1 cid3i min k2n cid2 n cid2 n n vni1 ik 1 ikv vni1 numerator denominator positive Proof Let cid21 U cid3 Rn 0 cid3 cid31 cid31 cid3 cid32 cid3n1 cid3 cid3n ncid17 v vni1cid3i 1 cid22 i1 From 16 Theorem 181 cid3 vertex U iff n 1 inequalities n inequalities U transformed equalities Let k 1 n index inequality transformed equality One k 1 i1v vni1 0 Hence k 2 n Then 0 cid31 cid3k1 cid3k cid3n αk One cid2 n ncid17 v vni1cid3i αk 1 i1 ncid17 v vni1 ik From normalization condition obtain αk 1 ikv vni1 Hence relation assertion proved cid2 n We w EU cid3 min hEU π Π N π v cid3 hEU cid15 vni1 cid12 ncid17 ik 1 n max π Π N π v cid3 min hEU π Π N π v cid3 hEU ncid17 v vni1 ik cid3 deﬁned The signs numerator denominator follow Proposition 13 cid5 1442 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Assertion 2 inf vWEUw EU v1cid3cid3vn min k2n cid2 n cid2 n n vni1 ik 1 ikv vni1 1 n Proof Let HEU 2 left hand expression lemma Then HEU 2 vRn v1cid3cid3vn v1vn1 inf min k2n cid2 cid2 n ik 1 n cid2 n jN v j vni1 ikv vni1 We notice ratio homogeneous v previous relation Hence constraint v 1 vn 1 removed HEU 2 inf vRn v1cid3cid3vn min k2n cid2 cid2 n ik 1 n cid2 n jN v j vni1 ikv vni1 We interested v different arithmetic mean From Lemma 1 denominator strictly positive case Hence arbitrarily set value 1 vRn v1cid3cid3vn ikv vni11 inf cid2 n cid12 ncid17 min k2n ik 1 n cid17 jN cid15 v j vni1 HEU 2 One ncid17 v vni1 ncid17 v vni1 ik cid14 maxk n k 1 ikcid14 cid21 k Let U v Rn 0 cid3 v 1 v 1 cid3 v 2 vn1 cid3 vn ncid17 cid22 v vni1 1 ikcid14 From 16 Theorem 181 v vertex U iff n 1 inequalities U transformed equalities Let p 1 n index inequality transformed equality One p 1 equality constraint U satisﬁed Hence p 2 n Then 0 v 1 v p1 v p vn αp One cid12 ncid17 ik 1 n cid17 jN cid15 v j vni1 cid12 nk1cid17 i1 n p 1 n cid15 αp v F There cases p cid2 n k 1 Hence functional F F n p 1n k 1 αp n ncid17 v vni1 1 ikcid14 ncid17 ikcid14 cid9 n max cid9 cid10 cid14 cid10 1 p k v αp Hence F greater equal 1 n The minimal value 1 n attained p k n p n k 1 The functional F F n p 1n k 1 n cid7 αp n k 1 p 1 cid8 αp ncid17 v vni1 1 cid9 n k cid14 1 cid10 αp cid7cid9 n k cid14 1 cid10 cid8 αp pαp p ikcid14 Hence functional C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1443 F p n cid7 cid8 n p 1n k 1 nn k p 2 cid7 p 1k 1 cid8 p n cid2 1 n The minimal value 1 n cid5 The proof Proposition 14 completed cid5 Proof Proposition 15 Let λEU H EU π N v y x H EU π N v y x For π π cid14 ΠN cid11 cid11H EU π v y x H EU cid11 cid11 cid3 λEU π cid14v y x 20 Let icid8 icid8 jcid8 jcid8 N miniN cid3i cid3icid8 maxiN cid3i cid3icid8 min jN v j v jcid8 max jN v j v jcid8 Deﬁne π π cid14 ΠN π icid8 jcid8 π icid8 jcid8 π cid14icid8 jcid8 π cid14icid8 jcid8 π π cid14i N icid8 icid8 From 20 obtain cid11 cid11 cid11v jcid8 cid3icid8 v jcid8 cid3icid8 v jcid8 cid3icid8 v jcid8 cid3icid8 cid11 cid3 λEU Hence v jcid8 v jcid8 cid3icid8 cid3icid8 cid3 λEU By deﬁnition icid8 icid8 cid3icid8 cid3icid8 δ This proves v jcid8 v jcid8 cid3 λEU δ By deﬁnition jcid8 jcid8 j N v v j cid3 v jcid8 v jcid8 cid3 λEU δ Finally N cid11 cid11 cid11v 1 cid11 n cid11 cid11 cid11v 1 cid11 n cid11 cid11 cid11 cid11 cid11 cid11 cid11 1 cid11 n cid11 cid11 cid11 cid11 v j cid17 jN cid17 cid11 cid11 cid11 v j v jNi cid11 cid3 n 1 n λEU δ By Proposition 14 λEU cid3 nχ EU Hence inequalities Proposition 15 proved These inequalities reached x 0 0 y 0 0 1 v 0 0 1 obtain ν n1 n δ 1 λEU 1 χ EU 1 n cid5 Proof Lemma 6 One cid17 cid17 H Maj π v y x 1 n π Π N A yx cid17 A yx π Π N cid17 A yx 1 n cid17 vπ cid17 A yx 1 n cid17 π Π N vπ H Maj w Maj y x cid5 1 n 1 n Proof Proposition 16 For π ΠN H Maj wMaj y x cid2 H Maj The second lemma obvious Let Assume H H Maj wMaj y x Then π ΠN H Maj j leaving criteria We π N v y x y x Let cid16 j N let π permutation permuting π N v y x Hence Lemma 6 H Maj π v y x cid2 H Maj π v y x cid2 H Maj π N v y x Maj v π v y x y x H Maj H Maj v 0 2v v j 2v j v v v j v j v y x A y x cid16 If A By 8 A Hence v v j j N Now A Hence v v j j N cid5 y x j A y x j A y x j A A A A A y x j A y x j A y x j A y x j A y x y x y x A y x A y x j A y x j A y x y x y x cid16 obtain v v j A y x cid16 obtain v v j A y x j A y x j A y x A y x A y x y x 1444 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 Proof Proposition 17 We want lower bound HMaj min x y 8 min vWMajw Maj One H Maj w Maj y x minπ Π N H maxπ Π N H Maj π v y x minπ Π N H Maj π v y x Maj π v y x max x y 8 max vWMajw Maj max π Π N H Maj π v y x 1 min x y 8 min vWMajw Maj min π Π N H Maj π v y x 1 attained v 1 1 v 0 cid16 1 A relation Hence y x 1 ﬁrst relation A y x 1 second cid19 max x y 8 max vWMajw Maj max π Π N H Maj π v y x min π Π N cid20 π v y x H Maj cid3 2 HMaj cid2 1 2 min x y 8 Setting L v A A min vWMajw Maj cid2 cid2 A v cid19 H Maj w Maj y x min π Π N H cid20 Maj π v y x HMaj cid2 1 2 min A AQN 8 min vWMajw Maj L w Maj cid9 A A cid10 min π Π N Lπ v cid10cid20 cid9 A A A v cid19 QN A A 2N 2N A A Without loss generality assume v 1 cid3 cid3 vn 1 p A A q n p q 0 n 1 Then following relations hold cid10 cid9 A L w Maj A cid9 min π Π N Lπ v A p n q 1 n cid10 A v 1 v p vq vn Condition 8 p q p n q 1 p cid2 1 q cid3 n 21 Hence HMaj cid2 1 2 min pq 21 min vWMajw Maj v1cid3cid3vn F F pnq1 n v 1 v p vq vn We write HMaj cid2 1 2 min pq 21 min v 0cid3v1cid3cid3vn v1vn1 F Proceeding exactly proof Proposition 14 minimum necessarily attained vertex polytope cid21 U v Rn 0 cid3 v 1 cid3 cid3 vn cid22 v 1 ncid17 i1 A vector v U vertex U iff exists r 1 n v 1 vr1 0 vr vn α Since v normalized α 1 nr1 One r cid2 2 v different w Maj One C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1445 cid27 cid27 v 1 v p vq vn p r 0 p r 1α p cid2 r n q 1α q cid2 r n r 1α q r We following cases 1 p r q cid2 r We F p n q 1 n n q 1 n r 1 p n n q 1r 1 nn r 1 cid2 1 nn 1 p 0 r 2 q n 2 p r q r We F p n q 1 n n r 1 n r 1 p q 1 n cid2 1 n 3 p cid2 r q cid2 r We F p n q 1 n p r 1 n q 1 n r 1 r 1n p q 2 n r 1 cid2 1 n 4 p cid2 r q r This situation impossible q r cid2 θ 1 2nn 1 Maj π v y x We shown χ Maj maxπ Π N H Then H Maj π v y x H Maj π v y x minπ Π N H π cid14v y x cid3 χ Maj Maj θ tion 15 If j set A j N v v j cid3 2χ Maj cid11 cid11 cid11 1 cid11 n Therefore cid11 cid11 cid11 v j v cid11 cid11 cid11v 1 cid11 n cid11 cid3 n 1 cid11 cid17 cid11 cid11 cid11 2χ Maj θ n θ jcid16i cid3 2χ Maj θ cid5 π π cid14 ΠN Let j N Deﬁne π π cid14 proof Proposi y x A y x A y x obtain v v j cid3 χ Maj θ Hence For proof Proposition 18 need following result Lemma 7 Let p N b Rp If ai bi 1 p pcid18 i1 pcid18 ai bi i1 Proof By associativity operator prove lemma p 2 One a1 b1 cid2 b1 b2 a2 b2 cid2 b1 b2 Hence a1 a2 b1 b2 proves result cid5 Proof Proposition 18 One π ΠN cid10 yi 1 vπ ncid18 cid9 i1 cid10 xi 1 vπ ncid18 cid9 i1 By Lemma 7 cid18 cid28 ncid18 cid9 cid10 yi 1 vπ cid29 cid18 cid28 ncid18 cid9 cid10 xi 1 vπ cid29 π Π N i1 π Π N i1 Hence cid14 cid18 ncid18 cid9 cid10 yi 1 vπ cid13 cid14 cid18 ncid18 cid9 cid10 xi 1 vπ cid13 i1 π Π N i1 π Π N 1446 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 cid28 ncid18 ncid18 cid9 i1 j1 cid10 yi 1 v j cid29 cid28 ncid18 ncid18 cid9 i1 j1 cid10 yi 1 v j cid29 Since exists k vk 1 cid10 yi 1 v j yi ncid18 cid9 j1 This gives ncid18 i1 ncid18 yi xi i1 cid5 Proof Proposition 19 Let t E t e The cardinality set E 2 A elements E 1 greater equal t Hence cid11 cid11i N 1 v cid2 t α E 1 α cid2 t y x xi cid2 t cid6cid11 cid11 cid11 cid11 A cid11 cid5 cid11 cid11 cid5 cid11 cid6cid11 cid11 cid11 cid11 A cid11 cid11 y x y x 1 There A y x 1 Therefore exist π ΠN yx Pess A π v h x cid2 t Hence 9 holds Only proposition We assume H Pess yx By deﬁnition A x h yx Pess A π v Pess A h π v π v y cid3 h hPess Pess A π v y h y x A y x A yx y cid3 h yx yx Pess A π v y cid3 hPess Pess A π v π v y x 0 π ΠN y x h yx Pess A π v x If assume hPess π v x h y cid2 h Pess A π v Pess A π v yx yx x h x h Pess A π v Pess A π v yx yx y x π v x contradicts H Pess π v y x 0 Hence π v x h hPess hPess π v x h Pess A π v Pess A π v yx yx x x h Pess A π v yx x yx Pess A π v y h Pess A π v yx x H Pess π v y x cid3 0 contradiction raised Hence iii holds If assume h Hence holds Let L cid14 A y x A vm 1 Consider π cid8 2 y x In permutation π cid8 A precisely A y x yi cid3 e Assume contradiction L cid14 cid16 Since v normalized exists m N A yx1i 2 largest values 1 v assigned smallest values x vice versa More 2 k m k L ΠN π cid8 2 π v π cid8 π x N cid14 y x xi cid2 e 1 vπ cid8 2 cid2 e Hence max π Π N yx Pess A π v h x h Pess A v π cid8 2 yx x e Since π cid8 2 k m k L cid14 yx h Pess A v π cid8 2 y h Pess A v π cid8 2 yx y cid3 e h yx Pess A v π cid8 2 x hPess π cid8 2 v x This contradicts H Pess π cid8 2 v y x 0 Hence L cid14 If proof Assume ii iii hold Let π ΠN Then h π v x 9 iii By ii h x hPess y h Pess A π v Pess A π v Pess A π v yx yx e cid2 h yx Pess A π v yx y h y e Hence hPess Pess A π v yx x hPess π v x π v x cid5 π v y hPess Proof Proposition 20 Assume V 10 holds Let π ΠN When π yx Pess A h π v x yi xi yi 1 vπ y yi xi 1 vπ cid2 h yx Pess A π v V V cid16 Then 10 Suppose π cid5 Lπ j A y x x j 1 vk 1 vπ j yi cid6 cid16 V For j Lπ 1 vπ j yi cid3 y j y j 1 vπ j y j x j 1 vπ j Moreover j Lπ x j 1 vπ j 1 vk x j 1 vk 1 vπ j yi cid3 1 vk Hence C Labreuche Artiﬁcial Intelligence 175 2011 14101448 1447 y x Lπ y j x j cid2 1 vk 1 vπ j cid2 yi 1 vπ j cid2 1 vk hPessLπ π v y hPessLπ On hand Hence h Pess A π v yxLπ π v π v x hPessLπ j A y cid2 1 vk y cid2 1 vk hPessLπ x 1 vk yx Pess A π v h x π v Conversely assume V cid16 10 satisﬁed We π v y hPessLπ x cid2 h Pess A π v yx xi yi cid3 1 vk Since V cid2 L exists π cid8 1 hPessL 1 vπ cid8 π cid8 1 1 cid2 1 vk Since π cid8 1 yxL y cid2 1 vk h y x L h k A Pess A v π cid8 1 Pess A v π cid8 1 L j A y x x j 1 vk 1 cid2 yi L 1 vπ cid8 ΠN π cid8 1 v y 1 vk hPessL v x 1 vk Since yi xi cid2 1 vk yxL x cid5 x cid2 1 vk Hence h y h k yx yx π cid8 1 Pess A v π cid8 1 Pess A v π cid8 1 Proof Proposition 21 Assume I Since v normalized exists m N vm 1 Let π cid8 3 cid3 ΠN π cid8 3 j m Since iN xi x j hPess π cid8 3 v x x j contradicts 11 ΠN I 1 vπ cid8 Assume I cid16 k N 1 vk cid2 x j cid2 I Then exists π cid8 4 Pess A x h v π cid8 4 m We obtain h x cid2 x j h Pess A v π cid8 4 Pess A v π cid8 4 π cid8 yx yx yx 4 j 4 cid2 x j x x j This contradicts 11 yx Assume 12 holds Then π ΠN exists I 1 vπ x j Hence h yx yx xi 1 vπ x j h Pess A π v x h Pess A π v x cid2 x j Hence 11 holds cid5 Pess A π v x cid3 References 1 L Amgoud S Belabbes H Prade Towards formal framework search consensus autonomous agents 4th International Joint Conference Autonomous Agents Multiagent Systems AAMAS Utrecht 2005 pp 537543 2 L Amgoud JF Bonnefon H Prade An argumentationbased approach multiple criteria decision 8th European Conference Symbolic Quantitative Approaches Reasoning Uncertainty ECSQARU2005 Barcelona 2005 pp 269280 3 L Amgoud H Prade Using arguments making explaining decisions Artiﬁcial Intelligence 173 2009 413436 4 GE Andrews The Theory Partitions Encyclopedia Mathematics Its Applications 2nd edition AddisonWesley 1976 5 K Arrow Social Choice Individual Values 2nd edition Wiley 1963 6 KJ Arrow AK Sen K Suzumura Handbook Social Choice Welfare Handbooks Economics Elsevier 2002 7 CA Bana e Costa JC Vansnick A theoretical framework Measuring Attractiveness Categorical Based Evaluation TecHnique MACBETH Proc XIth Int Conf MultiCriteria Decision Making Coimbra Portugal August 1994 pp 1524 8 F Barbaresco JC Deltour G Desodt B Durand T Guenais Ch Labreuche Intelligent M3R radar time resources management Advanced cognition agility autonomy capabilities International Radar Conference Bordeaux France October 1216 2009 9 C Boutilier R Brafman C Domshlak H Hoos D Poole CPnets tool representing reasoning conditional Ceteris Paribus preference statements Journal Artiﬁcial Intelligence Research 21 2004 135191 10 D Bouyssou T Marchant M Pirlot A Tsoukiàs Ph Vincke Evaluation Decision Models Multiple Criteria Stepping Stones Analyst International Series Operations Research Management Science Springer 2006 11 R Brafman Intentions Plans Practical Reason Harvard University Press Massachusetts 1987 12 R Brafman M Tennenholtz An axiomatic treatment qualitative decision criteria J ACM 47 2000 452482 13 CB Callaway JC Lester Narrative prose generation Artiﬁcial Intelligence 139 2002 213252 14 G Carenini JD Moore Generating evaluating evaluative arguments Artiﬁcial Intelligence 170 2006 925952 15 G Choquet Theory capacities Annales lInstitut Fourier 5 1953 131295 16 V Chvatal Linear Programming WH Freeman Company New York 1983 17 EP Corbett RJ Connors Classical Rhetoric Modern Student Oxford University Press Oxford 1999 18 Y Dimopoulos P Moraitis L Amgoud Theoretical computational properties preferencebased argumentation 18th European Conference Artiﬁcial Intelligence ECAI08 Patras Greece 2008 pp 463467 19 J Doyle R Thomason Background qualitative decision theory The AI Magazine 20 1999 5568 20 D Dubois H Fargier P Perny Qualitative decision theory preference relations comparative uncertainty An axiomatic approach Artiﬁcial Intelligence 148 2003 219260 21 D Dubois H Fargier H Prade Reﬁnements maximin approach decisionmaking fuzzy environment Fuzzy Sets Systems 81 1996 103122 22 D Dubois H Prade Weighted minimum maximum operations fuzzy set theory Information Sciences 39 1986 205210 23 D Dubois H Prade Possibility Theory An Approach Computerized Processing Uncertainty Plenum Press New York 1988 24 D Dubois H Prade Possibility theory basis qualitative decision theory Proc Int Joint Conf AI IJCAI95 Montreal Canada August 1995 pp 1925 25 P Dung On acceptability arguments fundamental role nonmonotonic reasoning logic programming nperson games Artiﬁcial Intelligence 77 1995 321357 26 W Edwards JR Newman Multiattribute Evaluation Sage Publications Cambridge 1983 27 M Elhadad Using argumentation text generation Journal Pragmatics 24 1995 189220 28 J Figueira S Greco M Ehrgott Eds Multiple Criteria Decision Analysis State Art Surveys Kluwer Academic Publishers 2005 29 P Fishburn The Theory Social Choice Princeton University Press 1973 30 P Fishburn Semiorders choice functions Econometrica 43 1975 975977 31 J Fodor M Roubens Fuzzy Preference Modelling MultiCriteria Decision Aid Kluwer Academic Publishers 1994 32 JB Grize Matériaux pour une logique naturelle Technical report Travaux du Centre Recherche Semiologique No 29 Université Neuchâtel Suisse 1976 1448 C Labreuche Artiﬁcial Intelligence 175 2011 14101448 33 CL Hamblin Fallacies Methuen London 1970 34 RL Keeney H Raiffa Decision Multiple Objectives Wiley New York 1976 35 DA Klein Decision Analytic Intelligent Systems Automated Explanation Knowledge Acquisition Lawrence Erlbaum Associates 1994 36 FH Knight Risk Uncertainty Proﬁt Houghton Miﬄin Boston New York 1921 37 DH Krantz RD Luce P Suppes A Tversky Foundations Measurement vol 1 Additive Polynomial Representations Academic Press 1971 38 Ch Labreuche Argumentation results multicriteria evaluation model individual group decision aiding Int Conf Euro Society Fuzzy Logic Technology EUSFLAT Barcelona Spain September 79 2005 pp 482487 39 Ch Labreuche Argumentation decision aggregation operators based weights Int Conf Information Processing Management Uncertainty KnowledgeBased Systems IPMU Paris France July 27 2006 pp 683691 40 RD Luce H Raiffa Games Decisions Wiley New York 1957 41 Th Marchant Towards theory MCDM Stepping away social choice theory Mathematical Social Choice 45 2003 343363 42 S Modgil Reasoning preferences argumentation frameworks Artiﬁcial Intelligence 173 2009 901934 43 J Montmain G Mauris A Akharraz Elucidation decisional risk multi criteria decision based Choquet integral aggregation A cybernetic framework International Journal MultiCriteria Decision Analysis 13 2005 239258 44 H Moulin Axioms Cooperative Decision Making Cambridge University Press 1988 45 SD Parsons NR Jennings Negotiation argumentation preliminary report Proc 2nd Int Conf MultiAgent Systems ICMAS Kyoto Japan 1996 pp 267274 46 C Perelman L OlbrechtsTyteca Traité lArgumentation PUF Paris 1958 47 JP Pignon Ch Labreuche A methodological approach operational technical experimentation based evaluation systems systems architec tures Int Conference Software Systems Engineering Their Applications ICSSEA Paris France December 46 2007 48 A Rapoport Decision Theory Decision Behaviour Kluwer Academic Publishers Dordrecht 1989 49 B Roy Multicriteria Methodology Decision Aiding Kluwer Academic Publishers Dordrecht 1996 50 TL Saaty A scaling method priorities hierarchical structures J Math Psychology 15 1977 234281 51 LJ Savage The Foundations Statistics 2nd edition Dover 1972 52 M Schroeder R Schweimeier Notions attack justiﬁed arguments extended logic programs F van Harmelen Ed Proceedings European Conference Artiﬁcial Intelligence ECAI02 Amsterdam 2002 pp 536540 53 M Sugeno Theory fuzzy integrals applications PhD thesis Tokyo Institute Technology 1974 54 J Von Neumann O Morgenstern Theory Games Economic Behavior Princeton University Press 1944 55 WA Wagenaar PJ van Koppen HFM Crombag Anchored Narratives The Psychology Criminal Evidence St Martins Press New York 1993 56 A Wald Statistical Decision Functions Wiley New York 1950 57 D Walton Argumentation Schemes Presumptive Reasoning Erlbaum Mahwah NJ 1996