Artiﬁcial Intelligence 316 2023 103829 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint Alessandro Allievi ab Holger Banzhaf c Felix Schmitt d Reward Misdesign autonomous driving W Bradley Knox ab Peter Stone Robert Bosch LLC United States America b The University Texas Austin United States America c Robert Bosch GmbH Germany d Bosch Center Artiﬁcial Intelligence Germany e Sony AI United States America r t c l e n f o b s t r c t Article history Received 10 January 2022 Received revised form 30 September 2022 Accepted 24 November 2022 Available online 13 December 2022 Keywords Reinforcement learning Reward design Utility Cost Safety Risk Autonomous driving 1 Introduction This article considers problem diagnosing certain common errors reward design Its insights applicable design cost functions performance metrics generally To diagnose common errors develop 8 simple sanity checks identifying ﬂaws reward functions We survey research published tier venues focuses reinforcement learning RL autonomous driving AD Speciﬁcally closely examine reported reward function publication present reward functions complete standardized format appendix Wherever suﬃcient information apply 8 sanity checks surveyed reward function revealing nearuniversal ﬂaws reward design AD exist pervasively reward design tasks Lastly explore promising directions aid design reward functions AD subsequent research following process inquiry adapted domains 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 Treatments reinforcement learning assume reward function given ﬁxed However practice correct reward function sequential decisionmaking problem rarely clear Unfortunately process designing reward function reward designdespite criticality specifying problem solvedis given scant attention introductory texts1 For example Sutton Bartos standard text reinforcement learning 45 pp 5354 469 devotes merely 4 paragraphs reward design absence known performance metric Anecdotally reward design widely acknowledged diﬃcult task especially people considerable experience Further DulacArnold et al 14 recently highlighted learning multiobjective poorly speciﬁed reward functions critical obstacle hampering application reinforcement learning realworld problems Additionally problem reward design highly related general problem designing performance metrics optimizationwhether manual auto mated optimizationand equivalent designing cost functions planning control Section 2 making discussion This paper Special Issue Riskaware Autonomous Systems Theory Practice Corresponding author Email address bradknoxcsutexasedu WB Knox 1 Unless noted discussion reward design focuses speciﬁcation environmental reward shaping rewards added We focus default manual reward speciﬁcation differs inverse reinforcement learning methods learning reward functions However discuss application work methods Section 54 httpsdoiorg101016jartint2022103829 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 reward design relevant reinforcement learning This article contributes important step reward design evaluating proposed reward function independently reinforcement learning algorithms chosen optimize respect To end Section 4 develop 8 sanity checks identifying ﬂaws reward functions Throughout article use autonomous driving motivating example reward design AD serves source reward functions demonstrate application 8 sanity checks In Section 3 challenges reward design autonomous driving The sanity checks Section 4 reveal pervasive issues published reward functions AD Speciﬁcally majority reward functionsoften themfail tests ﬁrst 3 sanity checks Sections 4143 In Section 5 explore obstacles reward design AD initial attempts design attributes AD reward function uncovering obstacles designer consider Section 5 includes review governmentmandated performance metrics discussions reward learning AD multiobjective optimization AD proposal designing reward AD ﬁnancial currency unit Given high envisioned impact autonomous driving RLforAD community needs consider reward design carefully reinforcement learning signiﬁcant role development AD We seek condemn past efforts reviewwhich excellent regardsbut provide understanding common issues reward design guidance identifying Additionally paper general survey reinforcement learning autonomous driving surveys conducted Zhu Zhao 59 Kiran et al 26 The contributions article include deep discussion reward design AD entail development 8 simple sanity checks reward cost functions application sanity checks identify prevalent ﬂaws published reward functions AD ﬂaws anecdotally appear common RL AD identifying pervasive usage trialanderror reward design Section 46 revealing obstacles arise initial attempt design reward AD In particular claim solve problem reward design AD Instead provide guidance future efforts design reward performance metrics autonomous driving speciﬁcally tasks undeﬁned reward functions 2 Background objectives utility functions reward functions To support discussion reward design ﬁrst review reward function When attempting decide alternative decisionmaking control algorithmsor equivalently discussion policies map state probability distribution actionsa common intuitive approach deﬁne performance metric J scores policy π according J π R If J π A J πB π A better according J 2 Deﬁnition performance metric J creates ranking policies identiﬁes set optimal policies helpful optimizing behavior sequential decisionmaking tasks However different deﬁnitions performance metric typically create different rankings sets optimal policies generally change result learning algorithm optimizing according J Put simply bad design performance metric function creates misalignment designeror stakeholdersconsiders good behavior learning algorithm A perfect optimization algorithm3 good performance metric optimizing J π typically deﬁned expectation outcomes created π outcomes directly observed J π More speciﬁcally episodic framings tasks J π deﬁned expectation Gτ performance trajectory τ created policy π Episodic task framings norm reinforcement learning AD assumed article noted Speciﬁcally J π Eπ DT Gτ τ trajectory generated following π starting state drawn according initial state distribution D tasks state transition probabilities T The mean Gτ trajectories generated serve practical unbiased estimator J π The policyperformance metric J trajectoryperformance metric G names ﬁelds address sequential decisionmaking problem Fig 1 shows terms In subﬁelds J called objective deﬁnition explicitly includes goal maximizing minimizing contexts In utility theory G utility function J π expected utility In planning control theory J π Gτ referred cost optimization seeks minimize cost maximize assume In evolutionary algorithms J π referred ﬁtness policy Fitness simply mean Gτ n samples τ expectation Gτ In reinforcement learning RL transition trajectory elicits reward signal according reward function R S A S R The input R state action state transition In undiscounted settingwhich implied episodic tasks article assumes defaultthe sum trajectory T 1 τ s rewards return reinforcement learnings word Gτ Therefore Gτ t1 Rst st1 trajectory τ s1 a1 s2 a2 sT 1 aT 1 sT Policyimprovement algorithms RL seek maximize expected return J π cid2 2 Roughly speaking learning algorithms sequentialdecision making tasks thought looping steps 1 Policy evaluation approximately evaluate J π policies gathered experience 2 Policy improvement use evaluation step 1 choose new policy π new policies evaluate 3 We use learning optimization interchangeably article 2 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Fig 1 Relationships terminology related ﬁelds Note ﬁtness objective similar meaning return identical Section 2 Terms like ﬁtness utility accompanying terminology rightmost column instead necessarily performance metrics expressed Markovian reward functions See Abel et al recent work expressing non Markovian performance metrics Markovian reward functions 1 We largely use terminology RL refer trajectorylevel performance metrics utility functions return function common concept Also feasible articles discussions focus trajectorylevel utility functions Gs reward functions We clarity having judged subjectively consequences utility function analytically accessible reward function 3 The challenge reward design autonomous driving We consider challenges reward design AD We note challenges apply widely tasks particularly occur physical world effects typically envisioned scope task Utility function depends numerous attributes First driving multiattribute problem meaning encompasses nu merous attributes contribute utility driving These attributes include measurements progress destination time spent driving collisions obeying law fuel consumption vehicle wear passenger experience impacts world outside vehicle These external impacts include people cars pedestrians bicyclists government entity builds maintains driving infrastructure pollution climate broadly Deﬁning trajectoryperformance metric G AD requires 1 identifying attributes specifying quantitatively 2 combining utility function outputs single realvalued number4 Utility function depends large contextdependent set stakeholders Second utility function G conform stakeholders interests In AD stakeholders include users passengers end consumers partnering businesses like taxi ridesharing companies automotive manufacturers governmental regulators providers research funding nearby pedestrians passengers bicyclists residents broader society These stakeholders weight given interests differ vehicles different manufacturers differ car different contexts One context affect G driving region values preferences driving culture differ substantially Therefore ideal reward design AD require designing numerous reward functions Gs generally Alternatively aforementioned stakeholderbased context observation signal permitting single monolithic reward function Such reward function allow policy learned numerous stakeholderbased contexts generalize new contexts Lack rigorous methods evaluating utility function Third choosing utility function algorithmic optimization contexts critical question arises given set stakeholders utility function deemed better worse We research measure degree utility functions conformity stakeholders interests We formal documentation current common practice evaluating utility function Anecdotally evaluation involves subjectively judging policies trained candidate utility functions reﬂecting utility function contributing undesirable behavior observed trained policies Setting aside dangers allowing speciﬁc learning algorithms inform design utility function discussion trialanderror reward design Section 46 common approach distilled speciﬁc process carefully examined practice varies substantially different designers reward functions utility functions However sanity checks Section 4 represent useful steps direction 4 Multiattribute utility functions single performance metric optimization unlike multiobjective utility functions discuss Section 55 3 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Table 1 Sanity checks perform ensure reward function suffer certain common problems Each sanity check described problematic characteristic look Failure ﬁrst 5 sanity checks identiﬁes problems reward function failure 3 checks considered warning 1 2 3 4 5 6 7 8 Sanity check failures Unsafe reward shaping Brief explanation If reward includes guidance behavior deviates measuring desired outcomes reward shaping exists Mismatch peoples reward functions preference orderings Undesired risk tolerance indifference points Learnable loopholes Missing attributes Redundant attributes Trialanderror reward design If human consensus trajectory better reward function agree Assess reward functions risk tolerance indifference points compare humanderived acceptable risk tolerance If learned policies pattern undesirable behavior consider explicitly encouraged reward If desired outcomes reward function indifferent Two reward function attributes include measurements outcome Tuning reward function improve RL agents performances unexamined consequences Incomplete description problem speciﬁcation Missing descriptions reward function termination conditions discount factor time step duration indicate insuﬃcient consideration problem speciﬁcation Potential interventions Separately deﬁne true reward function shaping reward Report true return shaped return Change applicable safe reward shaping method Remove reward shaping Change reward function align preferences human consensus Change reward function align risk tolerance humanderived level Remove encouragement loopholes reward function Add missing attributes Eliminate redundancy Only use observations behavior improve reward functions measurement task outcomes tune separately deﬁned shaping reward In research publications write problem speciﬁcation chosen The process reveal issues Elicits naïve reward shaping A fourth diﬃculty speciﬁc designing perstep feedback function like reward function cost function Driving task domain delayed feedback utility drive contained end based goal reached For drives minutes hours delayed information performance renders credit assignment behavior trajectory diﬃcult In diﬃculty reward shaping appears tempting naïve application extremely common research reviewed Section 41 4 Sanity checks reward functions In section develop set 8 conceptually simple sanity checks critiquing improving reward functions cost functions equivalently Table 1 summarizes sanity checks Many tests apply broadly trajectorylevel utility function We demonstrate usage critically reviewing reward functions RL AD research The 19 publications review include publication RL autonomous driving published beginning survey process toptier conference journal focusing robotics machine learning 1323292485321483728302520 publications respected venues focus autonomous driving 3295533464 Of 19 publications arbitrarily designated 10 focus papers strove exhaustively characterize reward function related aspects task description typically detailed correspondence authors Section 46 These 10 focus papers detailed Appendix A Appendix C 4 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 We present 8 sanity checks 5 detailed subsections 3 ﬁnal subsection Section 46 These tests overlap problems uncover sanity check entails distinct inquiry Their application 19 publications reveals multiple prevalent patterns problematic reward design 41 Identifying unsafe reward shaping In standard text artiﬁcial intelligence Russell Norvig assert As general rule better design formance metrics according actually wants achieved environment according thinks agent behave 42 p 39 In standard text RL Sutton Barto 45 p 54 agree phrasing adding imparting knowledge effective behavior better initial policy initial value function More succinctly specify measure outcomes achieve Exceptions rule thoughtfully justiﬁed Yet rewards encourage hint generally desirable behavioroften intention making learning eﬃcient tractable informative reward infrequent inaccessible policiesis intuitively appealing This practice formalized reward shaping learning agents received reward sum true reward shaping reward The boundary types rewards clear true objective given AD Nonetheless rewards clearly types The dangers reward shaping documented 403527 These dangers include creating optimal policies perform catastrophically Perhaps worse reward shaping appear help increasing learning speed reward designer realizing roughly speaking decreased upper bound performance changing reward functions preference ordering policies There small canon rewardshaping research focuses perform reward shaping certain safety guarantees 355651218 Safety means reward shaping guarantee harm learning differs colloquial deﬁnition avoiding harm people property A common safety guarantee policy invariance having set optimal policies shaping rewards We generally recommend attempts shape rewards informed literature safe reward shaping For techniques guar antees shaping rewards designed separately true reward function Also possible utility function G arises true reward equivalent main performance metric evaluating learned policies We formulate exposition sanity check Unsafe reward shaping identiﬁed ﬁrst identifying reward shapingwithout regard safetyand determining designers reward function following known safe reward shaping method persuasive argument shaped rewards safe Application AD Acknowledging subjectivity classifying reward shaped shaping explicitly discussed conﬁdently judge 19 publications surveyed 13 included reward shaping tributes reward functions 1329243253921483728304620 Another 2 included reward attributes arguably considered reward shaping 544 Examples behavior encouraged reward shaping 13 publications staying close center lane 24 passing vehicles 32 changing lanes 21 increasing distances vehicles 53 avoiding overlap oppositedirection lane 1329 steering straight times 9 Other examples Appendix B All encouraged behaviors heuristics achieve good driv ingviolating aforementioned advice Russell Norvig Sutton Bartoand easy construct scenarios discourage good driving For example reward shaping attribute penalizes changing lanes 21 discourage moving lanes farther vehicles pedestrians including acting unpredictably Many examples behaviors encouraged shaping rewards viewed metrics attributes true utility function highly positively correlated performance As Amodei et al 3 discussed rewarding behavior correlated performance backﬁre strongly optimizing reward result policies trade increased accumulation shaping rewards large reductions performancerelated outcomes driving overall performance This concept memorably aphorized Goodharts law When proxy measure target ceases good measure 4417 Ostensibly similar criticism aimed measures true reward function For instance assume reducing gas cost avoiding collisions attributes true utility function Then reducing gas cost discourage accelerating avoid potential collision The critical difference attributes measuring desired outcomes rewardshaping attributes trading true attributes utility function result higher overall utility desirable In example reducing correctly weighted gas cost presumably negligible effect frequency collisions beneﬁt avoiding collisions far outweigh beneﬁt reducing gas cost Another perspective difference effective optimization increase rewardshaping attributes expense overall utility effective optimization increase true utility attributes expense utility attributes expense overall utility Of 13 publications use reward functions conﬁdent shaped 8 set 10 focus papers 13 2924325392148 None 8 papers explicitly described separation shaping rewards true rewards discussed policy invariance guarantees safety reward shaping Of 8 papers Jaritz et al Toromanoff et al acknowledged usage reward shaping 5 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Fig 2 Illustrations abstract trajectories Sections 42 43 discussed undesirable consequences Jaritz et al write bots achieve optimal trajectories car try remain track center reward function explicitly incentivizes Further 8 papers reward shaping performance learned policies compared terms return according performance metrics obscuring undesirable behavior frequent collisions result RL algorithms imperfect optimization reward function optimizing In work learning reward functions Ibarz et al 22 provide useful examples analyze alternative reward function returns true reward function 3 8 papers report return 53219 42 Comparing preference orderings Although diﬃcult humans score trajectories policies ways consistent utility theory simply judging trajectory better easy Accordingly method critiquing utility function compare utility functions trajectory preferences groundtruth preferences expressed single human decision multiple stakeholders issuance regulations This preference comparison test utility function G τ A τB Gτ A GτB means preferred Finding τ A τB statement false indicates ﬂaw utility function evaluate severity ﬂaw However severity implied trajectory strongly preferred Note focus evaluating utility function differs learning utility function preferences trajectories subtrajectories Section 54 sanity check applied Application AD We apply comparison choosing trajectories nonexceptional circumstances trajectory strongly preferred We speciﬁcally let τcrash drive successful crashing halfway destination let τidle safe trajectory vehicle choosing stay motionless parked Fig 2 illustrates τcrash τidle Of 10 focus papers 9 permit estimating Gτcrash Gτidle Huegle et al 21 Appendix C For calculation utilities later article assume reward temporally discounted problem speciﬁcationwhich generally considered correct episodic tasks 45 p 68 like thesedespite nearly papers adherence current best practice discounting future reward aid deep reinforcement learning solutions discussed Pohlen et al 39 We presume appropriate set stakeholders prefer vehicle left idle proceed certain collision τcrash τidle Yet 9 evaluated reward functions 2 reward functions correct preference 7 reward functions prefer τcrash collision These 7 papers identiﬁed left Fig 3 τidle τcrash We calculate utilities reward functions 9 papers set focus papers examination reward functions suggests similar proportion likewise incorrect ordering Calculation returns trajectories allows muchneeded sanity check researchers conducting RLforAD projects avoiding reward functions egregiously dangerous particular manner 43 Comparing indifference points A complex form preference comparison reveals problems 2 papers passed test Section 42 For trajectories τ A τB τC continuity axiom utility theory states probability p rational agent indifferent 1 τB 2 sampling Bernoulli distribution τ A τC τC occurs probability p 52 GτB pGτC 1 pGτ A 1 This indifference point p compared groundtruth indifference point derived human stakeholders reveals risk tolerance The use indifference points AD exemplar widely applicable methodology testing reward function This methodology entails choosing τB trajectory achieved certainty default 6 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Fig 3 Estimates kilometers collision published reward functions indifferent prefer safely declining driving certain collision rate Higher values indicate stronger safety requirements Publications referenced ﬁrst 3 letters ﬁrst authors digits publication year Che19 refers publication Jianyu Chen The 3 points right designate estimates actual collision age group US drivers collisions 5060 year olds 1617 year olds 47 rough estimate collision drunk 1617 year old applying 37x risk blood alcohol concentration 008 estimated Peck et al 38 The task domain Jaritz et al 24 presented racing video game judged realworld safety standards behavior inaction τ A τC chosen contain possible outcomes risky departure default behavior inaction τC successful risky outcome τ A unsuccessful risky outcome From spective τ A losing gamble τB gambling τC winning gamble One set examples τ A τB τC domain AD includes trajectories video game agent deterministically ﬁnish level action seeking points dying time runs ﬁnishing level seeking additional points getting points ﬁnishing level time runs Application AD To apply test preferences probabilistic outcomes AD add τsucc τcrash τidle Section 42 τsucc trajectory successfully reaches destination τcrash τidle τsucc Therefore choosing p amounts setting permissible risk crashing successful trips In words policy higher risk threshold p preferred refuses drive Human drivers appear conduct similar analyses refusing drive faced signiﬁcant probability collision severe weather Fig 3 displays calculated p converted interpretable metric collision5 driving equally preferable deploying vehicle For comparison plot estimates policereported collisions categories humans These humanderived indifference points provide rough bounds US societys indifference point drunk driving considered illegal 1617 year old US citizens permitted drive As ﬁgure shows 9 focus papers permit form analysis 0 require driving safely legally drunk US 1617 year old teenager The riskaverse reward function metric 8 approve driving policy crashes 2000 times estimate drunk 1617 year old US drivers An argument testand broadly requiring utility function enforce driving humanlevel safetyis penalizing collisions cause RL algorithm correctly learn current policy safe driving causing stuck conservative local optimum moving This issue potentially overcome creating suﬃciently good starting policies performing reward shaping explicitly rigorously Further signiﬁcant issue argument argument reward function encourage RL algorithm gather driving experience extremely lenient collisions In particular speciﬁc weighting collision penalty effectively discourage collisions making vehicle avoid driving pendent performance current policy As RL algorithm improves driving effective collisionweighting values generally need increase The true reward function task speciﬁcation change function policy However dynamic weighting achieved deﬁning true reward function risk tolerance desirable realworld situations adjusting weight given collisions form dynamic reward shaping collisions weight start small gradually increased scaffold learning policy improves eventually reaching true weight value In strategy reward shaping temporary policy invariance achieved shaping ends 44 Identifying learnable loopholes Once designed reward function learning policy observable patterns undesirable behavior emerge When behavior increases utility referred reward hacking speciﬁcation gaming terms implicitly unfairly blame agent correctly optimizing ﬂawed utility function Colloquially technically legal violations 5 Kilometers crash indifference point p1 p 05 distance successful path p1 p τsucc trajectories halflength τcrash trajectory indifference point 7 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Fig 4 Illustration common learnable loophole agent moves circles repeatedly collect reward progress reaching goal spirit law called loopholes The RL literature provides numerous catastrophic examples loopholes 40235 involve learned trajectories actually loop physical space repeatedly accrue reward prevent longterm progress Fig 46 However loopholes subtle dominating performance nonetheless limiting cases loopholes diﬃcult ﬁnd observations learned behavior In general subtle blatant loopholes observing undesirable behavior reﬂecting reward function encourages behavior A rigorous improvement reﬂection estimate utility return equivalently observed trajectory contains undesirable behavior estimate utility trajectory modiﬁed minimally contain undesirable behavior desirable trajectory receives utility loophole likely exists An alternative perspective improvement method ﬁnding trajectories perform sanity check comparing preference orderings Section 42 Application AD We observe blatantly catastrophic loopholes RLforAD literature reward functions AD far appear designed trial error Section 46 trialanderror design catastrophic loopholes likely caught reward function tuned avoid However learnedpolicy limitation Jaritz et al 24 discussed Section 41 example learned loophole RL AD 45 Missing attributes Utility functions lack attributes needed holistically judge performance trajectory Speciﬁcally discussed Amodei et al 3 omission attribute implicitly expresses indifference attributes mea surements outcomes A simple example autonomous driving ignore passenger experience including attribute increases utility making progress destination learned behavior exhibit high accelera tion jerk tend harm passenger experience directly affect example utility function Section 3 contains list potential attributes autonomous driving help evaluation Identifying missing attributes adding solution When inclusion missing attributes intractable Amodei et al propose penalizing policys impact environment However approach called impact regularization shares potential issues reward shaping Application AD Since set required reward function attributes AD undetermined assume sake analysis abstract attributes emboldened Section 3 complete necessary set AD assess surveyed reward functions missing attributes This exercise highly subjective given desired attributes mathematically speciﬁed For instance Cai et al 8 penalize acceleration time reaching goal argue approximately cover fuel consumption And argue acceleration addresses passenger experience include seemingly important aspects passenger experience jerk illusions danger braking safely later human By judgment 10 reward functions Appendix A missing attribute The attributes commonly included form time spent driving collisions progress destination combination exempliﬁed purely Isele et als simple reward function 23 46 Sanity checks failure warning In addition sanity checks described methods raise red ﬂags indicating potential issues reward function warrant investigation Because descriptions sanity checks relatively short discussions applications AD explicitly separated previously described sanity checks Redundant attributes Similarly utility function redundant attributes encourage discourage comes Such overlap overly encourage constitutes desirable outcomes The overlap complicate reward designers understanding attributes combined impact utility function For instance consider au tonomous driving utility function includes attributes penalizes collisions penalizes repair 6 More examples webpage Speciﬁcation gaming examples AI 8 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 costs ego vehicle Both attributes penalize damage ego vehicle collision Yet attribute includes measurement outcomes collision penalty discourage harm people external ob jects repaircosts penalty discourages driving styles increase rate wear When redundant attributes exist utility function solutions include separating redundant aspects multiple attributes new attribute removing redundant aspects attribute Executing solutions straightforward In ex ample collision penalty separated components measure harm humans animals harm external objects increased repair costs ego vehicles maintainer Then repaircosts component collision penalty removed contained penalty overall repair costs Trialanderror reward design If publication presents reward function describing reward design process suspect likely designed process trial error This trialanderror reward design process involves designing reward function testing RL agent observa tions agents learning tune reward function repeating testing tuning process satisﬁed This process described Sutton Barto 45 p 469 Since reward function revised typically performance metrics employed evaluate learned policies Those performance metrics based subjective judgment explicitly deﬁned One issue trialanderror reward design speciﬁcation reinforcement learning problem principle adjusted beneﬁt candidate solution problem More practically issue manual rewardoptimization process overﬁt In words trialanderror reward design improve ward functions eﬃcacywhether measured reward designers subjective evaluation performance metric otherwisein speciﬁc context tested resultant reward function untested contexts effects unknown Factors affecting trialanderror reward design include RL al gorithm duration training assessing learned policy In particular designer chooses ﬁnal reward function suspect typically allow agent train longer duration multiple reward functions evaluated trialanderror design Further comparison multiple RL algorithms tend unfairly favor algorithm trialanderror design reward function speciﬁcally tuned improve performance RL algorithm Two types trialanderror reward design appear appropriate The ﬁrst type intentionally designing ward shaping function reward shaping RL solution changing RL problem The second type observations learned behavior change designers understanding tasks trajectorylevel performance metric reward function changed align new understanding Trialanderror reward design AD widespread 8 publications authors shared reward design process correspondence 8 reported following version deﬁning linear reward function manually tuning weights revising attributes trial error RL algorithm learns satisfying policy 13 238213295348 Based informal conversations numerous researchers suspect trialanderror reward design widespread domains Yet unaware research examined consequences ad hoc rewardoptimization process Incomplete problem speciﬁcation research presentations Many publications involving RL exactly specify aspects RL problem speciﬁcations Only 1 10 focus papers thoroughly described reward function discount factor termination conditions time step duration 29 We learned 9 papers speciﬁcations corre spondence authors We conjecture broader analysis published reward functions ﬁnd omission problem speciﬁcation details positively correlated reward design issues In absence conﬁr matory analysis nonetheless encourage authors write problem speciﬁcation readers beneﬁt practice writing problem details provide insights 5 Exploring design effective reward function In contrast previous sectionwhich focuses design reward functionthis section instead considers design This section contains exploration intend preliminary recommendation speciﬁc reward function process creating Again AD serves running example section 51 Performance metrics RL One potential source rewarddesign inspiration performance metrics created communities RL researchers For AD speciﬁcally prominent metrics regulatory agencies companies developing AD technology Many metrics express distance undesirable event incorporates critical utilityfunction tributes making progress avoiding failure states like collisions Speciﬁcally California Department Motor Vehicles DMV requires reporting miles disengagement disengagement deﬁned deactivation vehicles autonomous mode andor safety driver taking control autonomous Criticisms metric include 9 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 ignores important context complexity driving scenarios software releases tested sever ity outcomes averted safety drivers interventions The California DMV requires report ﬁled collision miles collision measure calculated 15 This metric vulnerable criticisms Also note disengagements prevent collisions safety drivers disengagement preferences decrease miles disengagement increasing miles collision vice versa complementary metrics combined somewhat clearer understanding safety Another metric miles fatality ad dresses ambiguity collisions severity In contrast 8 attributes listed Section 3 important AD utility function metrics covers 2 attributesdistance traveled count undesirable event Performance metrics AD designed robotics artiﬁcial intelligence communities In particu lar pertimestep cost functions developed planning control communities converted reward functions multiplying outputs 1 Additionally insight gained examining reward functions learned techniques reviewed Section 54 However review cost functions learned reward functions scope article 52 An exercise designing utility function attributes To sense challenges involved expressing reward function attributes let consider attributes listed previously Section 3 progress destination obeying law passenger experience We assume attribute component linear reward function following common practice For simplicity focus scenario driving passengers consideration cargo We obvious candidate progress destination7 This candidate attribute proportion progress destination expressed value 1 calculated time step t route length position time t route length position time t 1 route length start adds 1 entire successful trajectory Route length calculated ways distance shortest legal path planned This approach initially appears reasonable nonetheless signiﬁcant issue equaldistance incre ment progress impact attribute To illustrate consider policies One policy stops exactly halfway route The policy reaches destination half trips start trip With respect progresstothedestination attribute proposed polices expected performance Yet performance policies equivalent For commutes authors certainly prefer ridesharing service cancels half time service drops halfway destination This dilemma leaves open question calculate progress destination attribute Perhaps based utility derived passengers transported location We suspect utility generally lowest dropoff point far pickup location destination A highly accurate assessment utility seemingly require information passengers dropoff location including destination For instance drug store destination passengers utility dropoff locations differ greatly based plan purchase urgently needed medication buy snack Such information unlikely available autonomous vehicles nonetheless coarse estimate passengers utility speciﬁc dropoff location information available Also passengers opt share reaching destination highly consequential allowing driving policy reﬂect important information Obeying law trickier deﬁne precisely attribute This attribute expressed penalties incurred breaking laws Unfortunately penalties come different units obvious conversion unitssuch ﬁnes time lost interacting law enforcement court systems maybe time spent incarceratedmaking diﬃcult combination penalties single numeric attribute An additional issue modeling penalties ignores cost including law enforcement systems If external costs included attribute designer want careful external costs redundantly expressed attribute collisions attribute A challenge obtaining regionbyregion encoding driving laws penalties Passengers critical stakeholders driving trajectory passenger experience appears important experiences captured metrics like collisions For example people experienced fear passengers driver brakes later prefer creating prebraking moment uncertainty driver aware need slow vehicle Though situation actually safe hindsight late application brakes created unpleasant experience passenger To deﬁne passenger experience attribute candidate solution calculate passengerexperience score surveys passenger satisfaction 7 For reasons hinted subsection focus simpler binary attribute reaching destination ignores dropoff points destination different effects passengers utilities 10 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 averaging passengers numeric ratings end ride However surveys rely biased self report disruptive deploy trajectory In experimental design surveys asking respondents predictions behavior advisable consider instead experiment rely observations behavior guideline prompts question future passenger behaviorsuch choosing use AD service againmight useful attribute direct surveys Lastly passenger experience underdeﬁned concept despite conﬁdence important include overall AD utility function leaving open question exactly measure Combining attributes diﬃcult units attribute straightforwardly convertible attributes The reward function commonly linear combination attributes linearity assumption incorrect example utility function needs result conjunction attributes binary utility function success deﬁned reaching destination collision Also weight assignment linearly expressed reward functions trial error Section 46 possibly researchers lack principled way weigh attributes different units 53 A ﬁnancial utility function AD One potential solution challenge combine attributes express attributes unit added weights Speciﬁcally ﬁnancial utility function output change expectation net proﬁt stakeholders caused τ Utilities expressed currency units common RL proﬁt cost reduction explicit goals task stock trading 34 tax collection 31 unaware usage optimization objective AD To create ﬁnancial utility function AD nonﬁnancial outcomes need mapped ﬁnancial values assessment peoples willingness pay outcomes We surprised ﬁnd nonﬁnancial outcomes driving straightforward ﬁnancial expression initially expected providing optimism strategy reward design For example effort gone establishing value statistical life allows calculation monetary value reduction small risk fatalities The value statistical life numerous governmental agencies decisions involve ﬁnancial costs risks fatality The US Department Transportations value statistical life 116 million US Dollars 2020 5150 54 Methods learning reward function Instead manually designing reward function instead learn types data Methods learn ing reward functions include inverse reinforcement learning demonstrations 3660 learning reward functions preferences trajectory segments 57107 inverse reward design trialanderror reward design multiple instances task domain 19 Traditionally approaches assume reward linear function prespeciﬁed attributes weights learned challenges choosing attributes remain Approaches instead model reward expressive representations like deep neural networks 581016 avoid challenge An issue apparent way evaluate result reward learning knowing utility function G known tasks like autonomous driving blindly trusting results learning partic ularly unacceptable safetycritical applications like AD For methods nearly sanity checks present Section 4 provide partial evaluation learned reward functions checks agnostic reward function created The exception seventh sanity check trialanderror reward design potential issues overﬁtting reward function ad hoc design present learned reward functions result optimizing trajectorylevel performance measure 43 Although manual design reward functions learning reward functions appear mutually exclusive ap proaches complementarily For instance aforementioned approach inverse reward design 19 involves learning single reward function explains aspects multiple manually designed reward functions More generally research ﬁnds learning reward functions performant manual reward design nonetheless suspect manual reward design provide information helpfully informs learning process providing prior reward functions 55 Multiobjective reinforcement learning Multiobjective approaches 41 alternative deﬁning single utility reward function optimization In particular combination attributes utility function left undeﬁned learning Such approach ﬁt autonomous driving attributes change time For example frequent price changes petroleum gasoline electricity accounted proportionally reweighting fuel costs attribute Many multiobjective reinforcement learning algorithms evaluate sets policies best policy speciﬁc utility function parametrization later estimated learning For linear utility functionsincluding arise linear reward functionssuccessor features generalized policy improvement 116 promising techniques decisions changing utility function requiring task experience Much 11 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 articles inquiry sanity checks Section 4 apply choice utilityfunction parametrization multiobjective RL enable task execution 6 Conclusion In US 19 trillion miles driven 2019 49 Once autonomous vehicles prevalent generate massive amounts experiential data Techniques like reinforcement learning leverage data optimize autonomous driving competing methods behavioral cloning laborintensive manual design decisionmaking Despite suitability RL AD play large role development welldesigned objectives optimize By AD motivating example article sheds light problematic state reward design provides arguments set sanity checks jump start improvements reward design We hope article provokes conversation reward speciﬁcationfor autonomous driving broadlyand adds momentum muchneeded sustained investigation topic From article impactful directions work First speciﬁcally AD craft reward function utility function AD passes sanity checks includes attributes incorporate rel evant outcomes addresses issues discussed Section 52 passes tests deemed critical AD utility function The second direction supports practicality learning utility function passes ﬁrst sanity checks Speciﬁcally work reviewed challenges exploration reward sparsity partially addressed reward shaping low penalties collisions One empirically demonstrate learning reward function passes corresponding sanity checks challenges instead addressed methods Third broad application sanity checks numerous tasks likely lead insights reﬁnement sanity checks Fourth develop comprehensive methods evaluating utility functions respect human stakeholders interests Fifth trialanderror reward design common AD extrapolate informal conversations researchers currently dominant form reward design RL tasks general Inves tigation consequences ad hoc technique shed light unsanctioned highly impactful practice Finally community beneﬁt research constructs best practices manual design reward functions arbitrary tasks Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Acknowledgements We thank Ishan Durugkar Garrett Warnell Sam Devlin Yunshu Du James MacGlashan Brian Christian valuable feedback We thank authors focus papers generously answered questions publications details This work collaboration Bosch Learning Agents Research Group LARG UT Austin LARG research supported NSF CPS1739964 IIS1724157 FAIN2019844 ONR N00014182243 ARO W911NF1920333 DARPA Lockheed Martin GM Bosch UT Austins Good Systems grand challenge Peter Stone serves Executive Director Sony AI America receives ﬁnancial compensation work The terms arrangement reviewed approved University Texas Austin accordance policy objec tivity research Appendix A Reward functions 10 focus papers In Appendix section reward functions problem speciﬁcation details 10 papers evaluate Section 4 We closely examined 9 publications patterns 10 focus papers consistent pursue clarifying 9 additional publications problem speciﬁcations suﬃciently able characterize conﬁdence level focus papers described In section papers listed alphabetically ﬁrst authors A marks information obtained fully correspondence author paper Additionally include time limit information typically report RL agent updated terminal transition time expires episode stopped rarely information However suspect time limits meant training feasible actually problem speciﬁcation suspect agents correctly updated terminal transition time exhaustion A1 LeTSdrive driving crowd learning tree search 8 Reward function The reward function unweighted sum 3 attributes authors stated purpose brackets 12 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 eﬃciency 01 nonterminal time step 0 terminal transition smoothness 01 action includes nonzero acceleration 0 safety 1000 v 2 05 collision pedestrian static obstacle v driving speed ms 0 Time step duration Time steps 100 ms Discount factor The discount factor γ 1 Episodiccontinuing time limit termination criteria The task episodic Regarding time limit episodes computa tionally stopped 120 seconds simulation time 1200 time steps calculating success rate However trajectory stopped 120 seconds trajectory update value function making somewhat optimistic Termination criteria collisions agent reaching goal A2 Modelfree deep reinforcement learning urban autonomous driving 9 Reward function The reward function unweighted sum 5 attributes minspeed 10 speed ego vehicles speed ms penalty going fast 05 steering angle2 penalty magnitude steering angle radians 10 collision 0 1 leaving lane incurred distance ego vehicles center closest point provided routes polyline greater 2 m 0 01 step encourage quickly reaching goal Reward calculated 100 ms CARLA time step received agent decision points 400 ms That received reward sum 100 ms rewards Time step duration Time steps effectively 400 ms frame skip chosen action unchanged 4 frames 100 ms research CARLA simulator Discount factor The discount factor γ 099 applied decision point 400 ms Episodiccontinuing time limit termination criteria The task episodic The time limit 500 time steps 50 s Termination criteria getting goal state collisions leaving lane running time A3 CARLA open urban driving simulator 13 Reward function The reward function weighted sum 5 attributes r 1cid5d 005cid5v 000002cid5c 2cid5s 2cid5o These attributes cid5d change distance traveled meters shortest path start goal regularly calculated egos current position cid5v change speed kmh cid5c change collision damage expressed range 0 1 cid5s change proportion ego vehicle currently overlaps sidewalk cid5o change proportion ego vehicle currently overlaps lane Time step duration Time step information described paper 01 s time step duration reported papers use CARLA simulator report time step duration Discount factor The ﬁrst author suspects γ 099 typical A3C Episodiccontinuing time limit termination criteria The task episodic The time limit episode time car follow shortest path start state goal state driving 10 kmh Termination occurs collision reaching goal A4 Dynamic input deep reinforcement learning autonomous driving 21 Reward function The reward function unweighted sum 3 attributes 13 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 1 given step vvdesired 001 current action lane change 0 vdesired Time step duration Time steps agent 2 seconds Discount factor The discount factor γ 099 Episodiccontinuing time limit termination criteria The task continuing There time limit termination criterion A5 Navigating occluded intersections autonomous vehicles deep reinforcement learning 23 Reward function The reward function unweighted sum 3 attributes 001 given step 10 collision occurred 0 1 agent successfully reaches destination intersection 0 Time step duration Time steps 200 ms actions 2 4 8 time steps agent frame skip Discount factor The discount factor γ 099 Episodiccontinuing time limit termination criteria The task episodic In unoccluded scenarios episodes limited 20 s In occluded scenarios episodes limited 60 s Termination occurs success running time collision A6 Endtoend race driving deep reinforcement learning 24 Reward function Four different reward functions evaluated new intentionally add reward shap ing r v r vcos α d Ours r vcos α maxd 05w 0 Ours w margin r vcos α 1exp 4d05w Ours sigmoid 1 Above v vehicle velocity component direction lanes center line d distance middle road α difference vehicles heading lanes heading w road width Time step duration Time step duration 33 ms 1 step 30 FPS frame The game pause await RL agents action Discount factor The discount factor γ 099 Episodiccontinuing time limit termination criteria The task episodic No time limit enforced The termination criteria vehicle stops progressing vehicle goes offroad wrong direction A7 CIRL controllable imitative reinforcement learning visionbased selfdriving 29 Reward function The reward function unweighted sum 5 attributes penalty steering angles ranges assumed incorrect current command going left turnright com mand speed kmh penalties going fast turn limits speedbased reward turning speed limit 100 collision vehicles pedestrians 50 collision trees poles 0 100 overlapping sidewalk 0 100 overlapping oppositedirection lane 0 14 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Time step duration Time steps 100 ms reported research conducted CARLA simulator Discount factor The discount factor γ 09 Episodiccontinuing time limit termination criteria The task episodic The time allotted episode time follow optimal path goal 10 kmh Termination occurs successfully reaching destination having collision exhausting allotted time A8 Deep distributional reinforcement learning based highlevel driving policy determination 32 Reward function The reward function unweighted sum 4 attributes 40 v speed hour allowed range 40 80 kmh v40 05 ego vehicle overtakes vehicle 0 025 ego vehicle changes lane 0 10 ego vehicle collides 0 Time step duration The time step duration time frames Unitybased simulator The correspondence frame seconds simulated world unknown ﬁrst author Discount factor The discount factor γ 099 Episodiccontinuing time limit termination criteria The task episodic There time limit Termination occurs collision vehicle ego vehicle travels track length 2500 Unity spatial units effectively reaching goal A9 Learning hierarchical behavior motion planning autonomous driving 53 Reward function The reward function deﬁned separately transitions terminal nonterminal states For transi tions terminal states reward following 100 goal reached 50 collision running time 10 red light violation 1 ego vehicle wrong lane The reward single nonterminal highlevel behavioral step negative sum costs shorter steps best trajectory motion planner executes highlevel action Expressed additive inverse cost reward highlevel behavioral step unweighted sum 3 attributes cid2 t t2vre f vt t t2 cid2 1 cid2 t rewards speeds close desired speed vre f 1 cid2 vt rewards based distance traveled t 002 dolont 01 dolat t rewards keeping larger distances obstacles Distances meters speed ms Desired speed vre f determined RL agents highlevel actions speed_up speed_down change vre f ﬁxed percentage vt current speed Time t count 1 time steps planned trajectory dolon dolat respectively longitudinal lateral distance closest vehicle 20 meters measured centers cars If vehicle present range dolon 20 dolat 35 Time step duration A new highlevel action chosen RL agent planned trajectory previous highlevel action completed In simulator results average time step 1 s RLbased behavior policy The lowerlevel motion planner averages 100 ms planning iteration Discount factor Discount factor γ 099 Episodiccontinuing time limit termination criteria The task episodic The time limit time required travel length randomly generated route CARLA town 10 kmh Episodes terminated goal reached following occur collision driving lane red light violation 15 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 A10 Endtoend modelfree reinforcement learning urban driving implicit affordances 48 Reward function The reward function outputs r rspeed 05 rdist 05 rheading 1 1 rspeed 1 sdesired sego40 attribute 0 1 40 kmh maximum sdesired inversely proportional absolute difference actual speed desired speed rdist dpathdmax attribute 1 0 dmax 20 dpath distance meters closest point optimal paths spline rheading clip1 0 b θego θpath attribute 1 0 θego ego vehicles heading θpath heading optimal path spline closest point 1 termination 0 To determine reward optimal path created waypoint API This path optimal simplifying assumptions In training waypointgenerated optimal path generated randomly choosing turn directions intersections The unit speeds kmh unit angles degrees The desired speed sdesired hard coded based presence traﬃc lights obstacles priveleged information available training dmax half width lane apparently 2 m CARLA In formula rheading b 110 optimal paths accompanying highlevel command follow lane straight intersection b 125 command turn left right intersection We note reward function appears somewhat supervisory teaching target speed contexts stay close precomputed path Further context required calculate reward available agent testing Time step duration Time steps 100 ms reported research conducted CARLA simulator Discount factor Discount factor γ 099 Episodiccontinuing time limit termination criteria The task episodic There time limit training During training successful terminations reaching destination instead driving continued pro cedurally generated route undesirable termination condition met Termination conditions agent optimal path dmax collisions running red light having 0 speed obstacle waiting red traﬃc light Appendix B Reward shaping examples paper Of 19 papers reviewed highly conﬁdent 13 include reward shaping Below example paper behavior discouraged encouraged reward shaping The following examples discouraged behavior penalized negative reward deviating center lane 24483746 changing lanes 21 overlapping oppositedirection lane 1329 overlapping lane boundary 20 delaying entering intersection ego vehicles right way stop sign 28 deviating steering straight 9 sideways drifting race track 30 getting close vehicles 20 having turn signal 46 These following examples behaviors encouraged positive reward passing vehicles 32 increasing distances vehicles 53 Additionally 2 papers reward attributes somewhat defensible constituting reward shaping The reward function 55 includes penalty correlated lateral distance center lane reinforcement learning algorithm explicitly lowerlevel module receives command lane argue subtask module stay inside lane However perfectly centered lane highlevel command think argument considering reward shaping stronger argument alternative A reward attribute 4 encourages rightward lane car way 16 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 ﬁts laws US states require drivers right passing Therefore reward function includes reward shaping depends laws location ego vehicle Lastly conﬁdent 4 papers include reward shaping 2382533 Appendix C Calculation trajectory returns This appendix section describes estimate return trajectories reward function These calculations related sanity checks reward functions comparing preference orderings Section 42 comparing indifference points Section 43 Recall estimate returns 3 different types trajectories τcrash drive successful crashing halfway destination τidle safe trajectory vehicle choosing stay motionless parked τsucc trajectory successfully reaches destination We additionally remind readers indifference point calculated solving following equation p τ A τB τC GτB pGτC 1 pGτ A In paper calculate speciﬁcally Gτidle pGτsucc 1 pGτcrash And p safety indifference point expressed crash calculated p1 p 05 path length path length length successful trajectory Note p1 p expresses τsucc trajectories halflength τcrash trajectory indifference point making p1 p 05 path lengths driven collision To illustrate assume successfuluntilcollision τcrash Gτcrash 10 motionless τidle Gτidle 5 successful τsucc Gτsucc 10 Recall indifference point p utility function preference τidle lottery τcrash τsucc according pGτsucc 1 pGτcrash For example 5 p 10 1 p 10 solving equation results p 025 Therefore utility function prefer driving driving ratio higher 1 success 3 crashes Let assume path length 1 Therefore indifference point collision car drive 033 successful drives half 1 drive collision p 1 p 05 path lengths collision 1 path length 025 075 05 083 collision Calculations safety indifference points given papers τcrash τidle τsucc 823 In descriptions try signiﬁcant assumption Readers ﬁnd useful choose different assumptions test sensitive analysis assumptions expect reader ﬁnd changes qualitative results arise quantitative analysis C1 General methodology assumptions To estimate return trajectory τi fully ground sequence stateaction pairs As remainder appendix section exact stateaction sequence needed estimating certain trajectories returns reward functions For calculating reward time step adhere following methodology To determine returnutility successful portion trajectory assume unnecessary penalties incurred driving sidewalk For positive attributes reward choose value gives maximum outcome If maximum unclear choose outcomes attributes good better bestreported experimental results Lastly experimental results include measure reward attribute attempt calculate value better expect typical human driver Path lengths given length papers driving tasks estimation If given information insuﬃ cient estimate path length assume 1 For papers paper write units term equations return trajectory We encourage reader refer papers corresponding subsection Appendix A understand calculations return Additionally aid reader use speciﬁc colors terms expressing time limit path length time step duration speed For reward functions sums attributes return calculations maintain order 17 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 attributes described Appendix A include 0 terms attributes affect return corresponding trajectory C2 Assumptions calculations paper LeTSdrive driving crowd learning tree search 8 Driving maps 40 m 40 m single intersection curve map size assume path length 40 m This assumption appears reasonable car spawned random location For successful path τsucc successful portion path τcrash use mean task time number deceleration events reported best algorithm The motionless τidle involves time running actually termination event method treated The driving speed collision occurs assumed mean speed LeTSDrive calculated Table 1s Timetogoal path length Path length 004 A trajectory successful collision Gτcrash 51571 01 reward time step 10 time step s 296 s successful trajectory 05 path length traveled 01 reward acceleration event 182 mean acceleration events successful episode 05 path length traveled 1000 40 m path length296 s successful trajectory2 05 A motionless trajectory Gτidle 120 01 eﬃciency reward time step 10 time step s 120 s time limit reached 01 smoothness reward acceleration event 0 acceleration events episode 0 safety reward A successful trajectory Gτsucc 3142 01 eﬃciency reward time step 10 time step s 296 s successful trajectory 01 smoothness reward acceleration event 182 mean acceleration events successful episode 0 collision Calculation indifference point Gτidle pGτsucc 1 pGτcrash p Gτidle Gτcrash Gτsucc Gτcrash 120 51571 3142 51571 p p 09617 Calculation collision indifference point p 1 p 05 path lengths collision 004 path length 09617 1 09617 004 05 102 collision 18 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Modelfree deep reinforcement learning urban autonomous driving 9 We assume constant speed 5 ms 18 kmh rewardgiving speed speedbased reward attribute This paper focuses speciﬁc roundabout navigation task presumably shorter route common CARLA benchmarks ﬁrst established Dosovitskiy et al 13 Accordingly differing assumptions CARLA evaluations assume 0125 path length distance achieved speed exactly half permitted 50 s time limit We assume steering angle 0 avoiding penalty agent leaves lane collision Also recall reward accrued 100 ms time steps discounting applied 400 ms time steps Path length 0125 A trajectory successful collision Gτcrash 6015 5 m s 125 m path length 05 path length traveled 5 m s 0 deviations 0 steering angle 10 collision 1 leaving lane collision 01 constant reward time step 10 time step s 125 m path length 05 path length traveled 5 m s 10 time step s A motionless trajectory Gτidle 500 0 0 speed 0 deviations 0 steering angle 0 collision 0 leaving lane 01 constant reward time step 50 s time limit 10 time step s A successful trajectory Gτsucc 12250 5 m s 125 m path length 5 m s 10 time step s 0 deviations 0 steering angle 0 collision 0 leaving lane 01 constant reward time step 125 m path length 5 m s 10 time step s CARLA open urban driving simulator 13 For successful drive assume 1 path change speed start ﬁnish 60 kmh overlap occurs sidewalk lane For trajectory collision assume collision damage total 1 ego vehicle completely overlaps sidewalk lane 60 kmh Path length 1 A trajectory successful collision Gτcrash 50100 500 m path length 1 reward m traveled 005 60 h increased trajectory 000002 1 collision damage 0 overlapping sidewalk 2 1 fully leaving lane collision 19 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 A motionless trajectory Gτidle 0 0 distance traveled 0 h increased trajectory 0 collision damage 0 overlapping sidewalk 0 leaving lane A successful trajectory Gτsucc 1003 1000 m path length 1 reward m traveled 005 60 h increased trajectory 0 collision damage 0 overlapping sidewalk 0 leaving lane Dynamic input deep reinforcement learning autonomous driving 21 Because collision appears impossible task reward function involved analysis preference orderings indifference points Navigating occluded intersections autonomous vehicles deep reinforcement learning 23 We assume path 6 lane widths roughly Left2 turn requires lane 335 m wide based 11 feet appearing common httpsmutcd fhwa dot gov rpt tcstoll chapter443 htm For successful drive assume 4 s required We focus unoccluded scenario 20 s time limit Path length 335 m lane width 6 lane widths path length1000 m 002 A trajectory successful collision Gτcrash 101 05 path length traveled 4 s successful trajectory02 s time step 001 reward time step 10 collision 0 reaching goal A motionless trajectory Gτidle 1 20 s time limit02 s time step 001 reward time step 0 collision 0 reaching goal A successful trajectory Gτsucc 08 4 s successful trajectory02 s time step 001 reward time step 0 collision 1 reaching goal Calculation indifference point Gτidle pGτsucc 1 pGτcrash p Gτidle Gτcrash Gτsucc Gτcrash 1 101 08 101 p p 08349 20 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Calculation collision indifference point p 1 p 05 path lengths collision 002 1 path length 08349 1 08349 011 collision 05 002 Endtoend race driving deep reinforcement learning 24 Note domain carracing video game safety constraints differ autonomous driving For successful driving assume 7288 kmh average speed reported 987 track We assume ego vehicles heading aligned lane car center lane Path length 987 A trajectory successful collision Gτcrash 532980 987 path length 05 path length traveled 17288 h 3600 s h 30 time steps s 7288 h 1 reward h 0 heading aligned lane 0 lane center A motionless trajectory Gτidle 0 0 0 h 0 heading aligned lane 0 lane center A successful trajectory Gτsucc 1065960 987 path length 17288 h 3600 s h 30 time steps s 7288 h 1 reward h 0 heading aligned lane 0 lane center CIRL controllable imitative reinforcement learning visionbased selfdriving 29 For successful drives assume 60 h speed speed limit We assume penalties incurred When collision occurs assume overlap oppositedirection lane 1 s equivalent impact overlap sidewalk 1 s 60 h speed speed limit steeringangle penalty collision vehicle speciﬁcally As CARLAbased research assume 1 successful trajectory creates 6 minute time limit Path length 1 A trajectory successful collision Gτcrash 16900 0 always0 steering angle 60 h 1 path length 05 path length traveled60 h 3600 s h 10 times steps s 100 collision vehicle 0 overlapping sidewalk 100 overlapping oppositedirection lane 1 s overlap 10 times steps s A motionless trajectory Gτidle 0 0 always0 steering angle 0 0 h 0 collision 0 overlapping sidewalk 0 overlapping oppositedirection lane 21 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 A successful trajectory Gτsucc 36000 0 always0 steering angle 60 h 1 path length60 h 3600 s h 10 times steps s 0 collision 0 overlapping sidewalk 0 overlapping oppositedirection lane Deep distributional reinforcement learning based highlevel driving policy determination 32 For successful trajectory successful portion trajectory collision assume 17 overtakes based Distance meters Num overtake statistics shown Fig 7 paper assuming statistics taken good trajectory average 1 lane change overtake We assume car driving 80 h speed accrues reward Since minimum speed 40 h stopping papers taskhighway drivingwould unsafe instead assume vehicle decline deployed 0 return Since ﬁrst author know duration time step simulator time assume common Unity default 30 frames second 30 time steps second Unity processing time equals simulator time consequently 0033 s time steps assumed We assume 1 path ﬁrst author access path length task Path length 1 A trajectory successful collision Gτcrash 6739 80 h 4040 05 path length traveled 1 path length 80 h 3600 s h 30 time steps s 05 reward overtake 17 overtakes 1 path length 05 path length traveled 025 reward lane change 17 overtakes 1 lane change overtake 1 path length 05 path length traveled 10 collision A motionless trajectory Gτidle 0 A successful trajectory Gτsucc 13579 80 h 40 40 1 path length 80 h 3600 s h 30 time steps s 05 reward overtake 17 overtakes 1 path length 025 reward lane change 17 overtakes 1 lane change overtake 1 path length 0 collision Learning hierarchical behavior motion planning autonomous driving 53 We assume 1 path length speed 60 h 1667 m s CARLA evaluations We assume desired speed vre f 60 h making ﬁrst pertimestep component result 0 reward step highlevel RL time steps exactly 1 s reported mean duration Lastly assume vehicles closer 20 m ego vehicle Because path length assumed 1 time limit 360 s based time limit information Appendix A 22 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 Path length 1 A trajectory successful collision Gτcrash 1748 0 matching vre f 1 path length 05 path length traveled 60 h 3600 s h 1 RL time steps s 1 1 10 planning time steps RL time step 1667 m s 1 path length 05 path length traveled 60 h 3600 s h 1 RL time steps s 10 planning time steps RL time step 002 20 m 01 35 m 50 collision A motionless trajectory Gτidle 37112 360 s time limit 1 RL time steps s 1667 m s 0 m s 1 remaining terms constant speed differential moved outside summation 360 s time limit 1 RL time steps s 11 10 planning time steps RL time step 0 h 360 s time limit 1 RL time steps s 10 planning time steps RL time step 002 20 m 01 35 m 50 running time A successful trajectory Gτsucc 5496 0 matching vre f 1 path length60 h 3600 s h 1 RL time steps s 11 10 planning time steps RL time step 1667 m s 1 path length60 h 3600 s h 1 RL time steps s 10 planning time steps RL time step 002 20 m 01 35 m 100 reaching goal Endtoend modelfree reinforcement learning urban driving implicit affordances 48 As CARLA evalu ations assume 1 path length We assume speed 30 h based ﬁrst authors report 40 kmh maximum speed We assume termination occurs reaching destination true testing training involves driving termination failure No additional reward given successful termi nation For τcrash τsucc assume ego vehicle moving speed location heading We assume 0speed termination condition applied immediately 10 s later assumption avoid terminating starting time step vehicle spawned speed near 0 h Path length 1 23 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 A trajectory successful collision Gτcrash 599 1 reward sego matching sdesired 1 path length 05 path length traveled 130 h 3600 s h 10 time steps s 0 dpath 0 0 θego matching θpath 1 collision A motionless trajectory Gτidle 25 1 0 h 30 h sdesired 40 10 s 0speed termination condition applied 10 time steps s 0 dpath 0 0 θego matching θpath A successful trajectory Gτsucc 1200 1 reward sego matching sdesired 1 path length30 h 3600 s h 10 time steps s 0 dpath 0 0 θego matching θpath References 1 D Abel A Barreto M Bowling W Dabney S Hansen A Harutyunyan MK Ho R Kumar ML Littman D Precup et al Expressing nonMarkov reward Markov agent 2022 2 D Amodei J Clark Faulty reward functions wild httpsblog openai com faultyreward functions Accessed 11 March 2022 2016 3 D Amodei C Olah J Steinhardt P Christiano J Schulman D Mané Concrete problems ai safety arXiv preprint arXiv1606 06565 2016 4 S Aradi T Becsi P Gaspar Policy gradient based reinforcement learning approach autonomous highway driving IEEE Conference Control Technology Applications CCTA IEEE 2018 pp 670675 5 J Asmuth ML Littman R Zinkov Potentialbased shaping modelbased reinforcement learning TwentyThird AAAI Conference Artiﬁcial 6 A Barreto W Dabney R Munos JJ Hunt T Schaul H Van Hasselt D Silver Successor features transfer reinforcement learning arXiv preprint Intelligence 2008 pp 604609 arXiv1606 05312 2016 7 D Brown W Goo P Nagarajan S Niekum Extrapolating suboptimal demonstrations inverse reinforcement learning observations International Conference Machine Learning ICML PMLR 2019 pp 783792 8 P Cai Y Luo A Saxena D Hsu WS Lee Letsdrive driving crowd learning tree search Robotics Science Systems RSS 2019 9 J Chen B Yuan M Tomizuka Modelfree deep reinforcement learning urban autonomous driving IEEE Intelligent Transportation Systems Conference ITSC IEEE 2019 pp 27652771 mation Processing Systems NIPS 2017 pp 42994307 10 PF Christiano J Leike T Brown M Martic S Legg D Amodei Deep reinforcement learning human preferences Advances Neural Infor 11 P Dayan Improving generalization temporal difference learning successor representation Neural Comput 5 1993 613624 12 SM Devlin D Kudenko Dynamic potentialbased reward shaping 11th International Conference Autonomous Agents Multiagent Systems IFAAMAS 2012 pp 433440 13 A Dosovitskiy G Ros F Codevilla A Lopez V Koltun Carla open urban driving simulator Conference Robot Learning CoRL 2017 14 G DulacArnold N Levine DJ Mankowitz J Li C Paduraru S Gowal T Hester Challenges realworld reinforcement learning deﬁnitions bench marks analysis Mach Learn 2021 150 15 FM Favarò N Nader SO Eurich M Tripp N Varadaraju Examining accident reports involving autonomous vehicles California PLoS ONE 12 2017 16 J Fu K Luo S Levine Learning robust rewards adversarial inverse reinforcement learning International Conference Learning Representa 17 CA Goodhart Problems monetary management UK experience Monetary Theory Practice Springer 1984 pp 91121 18 M Grzes Reward shaping episodic reinforcement learning AAAI Conference Artiﬁcial Intelligence ACM 2017 19 D HadﬁeldMenell S Milli P Abbeel SJ Russell A Dragan Inverse reward design Advances Neural Information Processing Systems NIPS 20 M Henaff Y LeCun A Canziani Modelpredictive policy learning uncertainty regularization driving dense traﬃc 7th International Conference Learning Representation ICLR 2019 e0184952 tions ICLR 2018 2017 pp 67656774 24 WB Knox A Allievi H Banzhaf et al Artiﬁcial Intelligence 316 2023 103829 21 M Huegle G Kalweit B Mirchevska M Werling J Boedecker Dynamic input deep reinforcement learning autonomous driving IEEERSJ International Conference Intelligent Robots Systems IROS 2019 22 B Ibarz J Leike T Pohlen G Irving S Legg D Amodei Reward learning human preferences demonstrations Atari arXiv preprint arXiv181106521 2018 23 D Isele R Rahimi A Cosgun K Subramanian K Fujimura Navigating occluded intersections autonomous vehicles deep reinforcement learning IEEE International Conference Robotics Automation ICRA IEEE 2018 pp 20342039 24 M Jaritz R De Charette M Toromanoff E Perot F Nashashibi Endtoend race driving deep reinforcement learning IEEE International Conference Robotics Automation ICRA IEEE 2018 pp 20702075 25 A Kendall J Hawke D Janz P Mazur D Reda JM Allen VD Lam A Bewley A Shah Learning drive day International Conference Robotics Automation ICRA IEEE 2019 pp 82488254 26 BR Kiran I Sobh V Talpaert P Mannion AA Al Sallab S Yogamani P Pérez Deep reinforcement learning autonomous driving survey IEEE 27 WB Knox P Stone Reinforcement learning human MDP reward 11th International Conference Autonomous Agents Multiagent Trans Intell Transp Syst 2021 Systems AAMAS IFAAMAS 2012 28 C Li K Czarnecki Urban driving multiobjective deep reinforcement learning 18th International Conference Autonomous Agents MultiAgent Systems AAMAS IFAAMAS 2019 pp 359367 29 X Liang T Wang L Yang E Xing CIRL controllable imitative reinforcement learning visionbased selfdriving European Conference Com 30 GH Liu A Siravuru S Prabhakar M Veloso G Kantor Learning endtoend multimodal sensor policies autonomous navigation Conference 31 G Miller M Weatherwax T Gardinier N Abe P Melville C Pendus D Jensen CK Reddy V Thomas J Bennett et al Tax collections optimization 32 K Min H Kim K Huh Deep distributional reinforcement learning based highlevel driving policy determination IEEE Trans Intell Veh 4 2019 puter Vision ECCV 2018 pp 584599 Robot Learning CoRL 2017 pp 249261 New York state Interfaces 42 2012 7484 416424 2006 pp 673680 33 B Mirchevska C Pek M Werling M Althoff J Boedecker Highlevel decision making safe reasonable autonomous lane changing reinforcement learning 21st International Conference Intelligent Transportation Systems ITSC IEEE 2018 pp 21562162 34 Y Nevmyvaka Y Feng M Kearns Reinforcement learning optimized trade execution 23rd International Conference Machine Learning ICML 35 A Ng D Harada S Russell Policy invariance reward transformations theory application reward shaping Sixteenth International Conference Machine Learning ICML 1999 36 A Ng S Russell Algorithms inverse reinforcement learning Seventeenth International Conference Machine Learning ICML 2000 37 C Paxton V Raman GD Hager M Kobilarov Combining neural networks tree search task motion planning challenging environments IEEERSJ International Conference Intelligent Robots Systems IROS IEEE 2017 pp 60596066 38 R Peck M Gebers R Voas E Romano Improved methods estimating relative crash risk casecontrol study blood alcohol levels Joint Meeting International Council Alcohol Drugs Traﬃc Safety ICADTS International Association Forensic Toxicologists TIAFT 2007 39 T Pohlen B Piot T Hester MG Azar D Horgan D Budden G BarthMaron H Van Hasselt J Quan M Veˇcerík et al Observe look achieving consistent performance atari arXiv1805 11593 2018 40 J Randløv P Alstrøm Learning drive bicycle reinforcement learning shaping Fifteenth International Conference Machine Learning 41 DM Roijers P Vamplew S Whiteson R Dazeley A survey multiobjective sequential decisionmaking J Artif Intell Res 48 2013 67113 42 S Russell P Norvig Artiﬁcial Intelligence Modern Approach 2020 43 S Singh RL Lewis AG Barto Where rewards come Proceedings Annual Conference Cognitive Science Society Cognitive ICML Citeseer 1998 pp 463471 Science Society 2009 pp 26012606 44 M Strathern Improving ratings audit British university Eur Rev 5 1997 305321 45 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press 2018 46 Y Tang Towards learning multiagent negotiations selfplay IEEECVF International Conference Computer Vision Workshops ICCVW 2019 47 B Tefft Rates motor vehicle crashes injuries deaths relation driver age United States 20142015 AAA Foundation Traﬃc Safety httpsaaafoundation org rates motorvehicle crashes injuries deaths relation driverage united states 2014 2015 2017 Accessed 11 March 2022 48 M Toromanoff E Wirbel F Moutarde Endtoend modelfree reinforcement learning urban driving implicit affordances IEEECVF Con ference Computer Vision Pattern Recognition 2020 pp 71537162 49 US Dept Transportation Traﬃc volume trends httpswwwfhwa dot gov policyinformation travel _monitoring 20jultvt 20jultvt pdf 2020 Ac cessed 11 March 2022 50 US Dept Transportation Departmental guidance treatment value preventing fatalities injuries preparing economic analyses https wwwtransportation gov sites dot gov ﬁles 2021 03 DOT _VSL _Guidance 2021 _Update pdf 2021 Accessed 11 March 2022 51 US Dept Transportation Departmental guidance valuation statistical life economic analysis httpswwwtransportation gov oﬃce policy transportation policy revised departmental guidance valuation statistical life economic analysis 2022 Accessed 11 March 2022 52 J Von Neumann O Morgenstern Theory Games Economic Behavior Princeton University Press 1944 53 J Wang Y Wang D Zhang Y Yang R Xiong Learning hierarchical behavior motion planning autonomous driving IEEERSJ International 54 P Wang H Li CY Chan Continuous control automated lane change behavior based deep deterministic policy gradient algorithm IEEE Conference Intelligent Robots Systems IROS 2020 Intelligent Vehicles Symposium IV IEEE 2019 pp 14541460 55 P Wang H Li CY Chan Quadratic qnetwork learning continuous control autonomous vehicles Machine Learning Autonomous Driving Workshop 33rd Conference Neural Information Processing Systems NeurIPS 2019 56 E Wiewiora Potentialbased shaping Qvalue initialization equivalent J Artif Intell Res 19 2003 205208 57 C Wirth R Akrour G Neumann J Fürnkranz et al A survey preferencebased reinforcement learning methods J Mach Learn Res 18 2017 146 58 M Wulfmeier P Ondruska I Posner Maximum entropy deep inverse reinforcement learning arXiv preprint arXiv150704888 2015 59 Z Zhu H Zhao A survey deep rl il autonomous driving policy learning arXiv preprint arXiv210101993 2021 60 BD Ziebart AL Maas JA Bagnell AK Dey Maximum entropy inverse reinforcement learning TwentyThird AAAI Conference Artiﬁcial Intelli gence 2008 pp 14331438 25