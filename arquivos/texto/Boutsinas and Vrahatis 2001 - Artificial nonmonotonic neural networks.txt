Artiﬁcial Intelligence 132 2001 138 Artiﬁcial nonmonotonic neural networks B Boutsinas ac MN Vrahatis bc Department Computer Engineering Informatics University Patras GR26500 Patras Greece b Department Mathematics University Patras GR26500 Patras Greece c University Patras Artiﬁcial Intelligence Research Center UPAIRC University Patras GR26500 Patras Greece Received 30 August 1999 received revised form 8 January 2001 Abstract In paper introduce Artiﬁcial Nonmonotonic Neural Networks ANNNs kind hybrid learning systems capable nonmonotonic reasoning Nonmonotonic reasoning plays important role development artiﬁcial intelligent systems try mimic common sense reasoning exhibited humans On hand hybrid learning provides explanation capability trained Neural Networks acquiring symbolic knowledge domain reﬁning set classiﬁed examples Connectionist learning techniques ﬁnally extracting comprehensible symbolic information Artiﬁcial Nonmonotonic Neural Networks acquire knowledge represented multiple inheritance scheme exceptions nonmonotonic inheritance networks extract reﬁned knowledge scheme The key idea use special cell operation training order preserve symbolic meaning initial inheritance scheme Methods knowledge initialization knowledge reﬁnement knowledge extraction introduced We prove methods address perfectly constraints imposed nonmonotonicity Finally performance ANNNs compared wellknown hybrid systems extensive empirical tests 2001 Elsevier Science BV All rights reserved Keywords Nonmonotonic reasoning Neural networks Hybrid systems Inheritance networks Unconstrained optimization DNA sequence analysis Corresponding author Email address vutsinasceidupatrasgr B Boutsinas 0004370201 matter 2001 Elsevier Science BV All rights reserved PII S 0 0 0 4 3 7 0 2 0 1 0 0 1 2 6 6 2 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 1 Introduction 11 Motivation background Nonmonotonic reasoning plays important role development systems try mimic common sense reasoning exhibited humans Human beings constantly forced decisions reach conclusions ambiguous world The knowledge acquired observation inherently incomplete contain conﬂicting information exceptions general rules Many formalisms proposed literature capable representing knowledge multiple inheritance scheme exceptions 3691926485054 A nonmonotonic reasoner face general problems knowledgebased systems strong dependency correctness domain knowledge lack domain independent effective learning algorithms Due domain knowledge altered manually necessary Moreover nonmonotonic reasoner usually problems dealing multiple extensions theory In situations extensions treated cases ambiguity The way treated depends credulous skeptical view adopted 55 Besides attempt resolve possible conﬂicts cases 52 On hand examplebased systems Artiﬁcial Neural Networks ANNs need large set training examples strong dependency features examples They lack explanation capability generated outputs consequently decisions reached Moreover longstanding problem connectionist modelling representation structured objects Although attempts address problem 47 proposed systems efﬁcient 27 On contrary explanation capability representation structured objects easily provided knowledgebased Recently signiﬁcant attention paid development hybrid systems based neuralsymbolic integration aiming exploring advantages constituent paradigm There promising early attempts 11 combination explanation capabilities powerful declarative knowledge representation symbolic approach massive parallelism generalization capabilities connectionist approach Neuralsymbolic hybrid systems use Artiﬁcial Neural Network examplebased learning symbolic knowledge representation scheme domain knowledge The important contributions interesting ﬁeld 181012182229304256 Following McCarthys observation 37 hybrid systems neural net works inferential processing based logicbased propositional knowledge repre sentation schemes Such logicbased formalisms lack important properties nonmonotonic reasoning 6 important property stability called cumulativity This generally true known nonmonotonic formalisms Reiters Default Logic 6 Special extensions Default Logic introduced order tackle stability problem Cumulative Default Logic 434 Moreover shown decision problems intractable undecidable stated context formal logicbased systems B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 3 On hand symbolicconnectionist systems nonmonotonic domain importance theory practice Pathbased formalisms great representation schemes based semantic networks They introduce alternative approach nonmonotonic reasoning They try tackle algorithmic intractability correct treatment incomplete contradictory knowledge Although exists lot criticism semantics semantic networks 64 accepted effectively representation inference scheme nonmonotonic domain 54 Inheritance networks pathbased formalism widespread use nonmonotonic reasoning systems 55 Hybrid systems usually based logicbased knowledge representation schemes far domain knowledge concerned The proposed paper Artiﬁcial Nonmonotonic Neural Networks ANNNs hybrid systems use inheritance networks nonmonotonic multiple inheritance knowledge representation scheme domain knowledge Artiﬁcial Neural Networks learning mechanism The supported proper training method suits perfectly approach applied changing selected weights epoch ANNNs based energy minimization spreading activation metaphor The input cells connectionist externally activated based known facts domain knowledge spreading activation forces output cells activated deactivated It activation output cells guides reasoning process 12 Related work In 23 logicbased method presented inserting propositional general logic program P threelayer feedforward Artiﬁcial Neural Network binary threshold neurons It proved network transformed recurrent network connecting output units corresponding input units falls unique stable state corresponds unique stable model semantics P The potential impact result new massively parallel computational model logic programming derived The presented 56 KnowledgeBased Artiﬁcial Neural Network capable inserting ifthen rules neural network reﬁning rules backpropaga tion based learning algorithm extracting rules neural network It empirically shown revise domain knowledge efﬁciently learn new rules examples domain knowledge Following key idea 23 12 Connectionist Inductive Learning Logic Programming System integrates inductive learning examples domain knowledge deductive learning Logic Programming Propositional logic programs inserted feedforward Artiﬁcial Neural Network bipolar semi linear threshold neurons The network trained standard backpropagation learning algorithm revised logic program extracted Moreover shown 4042 satisﬁability problem propositional logic reduced problem ﬁnding global minima energy function symmetric network The consequence important result symmetric neural networks 4 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 applied solving lot hard problems optimization problems constraint satisfaction problems 16 There efforts direction hybrid systems use knowledge represen tation schemes based ﬁrstorder logic Unfortunately systems proposi tional 2425 In 21 automated reasoning ﬁrstorder Horn clauses pre sented Connectionist Horn Clause Logic implemented feedforward neural network In 24 extension method 23 presented inserting ﬁrstorder logic programs threelayer recurrent Artiﬁcial Neural Networks correspond approximation semantics programs Finally 43 analogous result 42 shown according ﬁrstorder resolution proof ﬁxed predetermined length reduced global minimum energy function symmetric network The ﬁrst result hybrid symbolicconnectionist nonmonotonic domain Pinkas In 44 shown minimization problem energy function symmetric network applied propositional nonmonotonic reasoning fact exceeding 45 As shown 46 initial domain knowledge represented logicbased scheme Penalty Logic To propositional formula representing domain knowledge assumption positive real number assigned penalty order form penalty logic formed formula This penalty paid assignment satisfy assumption An assignment preferred pays higher penalty Assignments pay minimum penalty preferred models preferred Pinkas showed minimization problem energy function symmetric network reduced problem ﬁnding preferred models given set assumptions representing initial domain knowledge In rest paper ﬁrst ANNNs Then present knowledge initialization knowledge reﬁnement knowledge extraction method ANNNs Consequently test performance ANNNs wellknown hybrid systems ﬁnally discuss critical issues 2 Artiﬁcial Nonmonotonic Neural Networks Artiﬁcial Nonmonotonic Neural Networks neuralsymbolic hybrid systems They possess domain knowledge initialization Artiﬁcial Neural Network The examplebased learning mechanism reﬁnement initial knowledge processing set classiﬁed examples After reﬁnement acquired knowledge extracted substituting initial domain knowledge At state new cycle knowledge initializationreﬁnementextraction start new set examples need considered Fig 1 Domain knowledge represented nonmonotonic multiple inheritance scheme allowing exceptions More speciﬁcally use Nonmonotonic Inheritance Networks NINs based semantic networks There clash intuitions 55 nonmonotonic inheritance networks concerning treatment nonmonotonicity onpath versus offpath preemption treatment competing extensions forms skepticism versus forms credulity direction represented inheritance followed B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 5 Fig 1 The cycle knowledge initialization reﬁnement extraction upward versus downward view reasoning Their performance closely related intuitions 51 The reasoning process proved NPhard onpathoff path credulous downward reasoners onpath upward credulous reasoners However proved polynomial time complexity skeptical reasoners In NINs knowledge represented attaching node directed acyclic graph label denotes object class objects property possessed objects domain discourse establishing desired relationships insertion proper directed edges When insert edge emanates node incident mean class objects represented node inherits deﬁning properties class represented If inheritance exception exists indicated exception link For example NIN shown left Fig 2 represents facts Es Ds kinds Ds like Ds 3 Ds Cs Moreover reasoning conclude Es Cs Ds Finally exception link indicated dotted line represents Ds Bs Cs There main operations associated NINs The ﬁrst operation consists answering object possesses particular property The second consists ﬁnding objects satisfying particular set properties Developing efﬁcient algorithms operations great importance AI applications Both operations reduced computing transitive relationships objects nonmonotonic inheritance network The objective knowledge initialization phase construct Artiﬁcial Neural Network provides answers nonmonotonic inheritance network far operations concerned The knowledge initialization methodology employ presented section After knowledge initialization phase constructed Artiﬁcial Neural Network trained set classiﬁed examples order reﬁne initial knowledge The reﬁnement achieved changing initial weights The 6 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 way construct Artiﬁcial Neural Network initialization phase imposes constraints training method use We propose new training method suits perfectly approach satisfying requirements The method presented Section 4 Finally reﬁned knowledge extracted Artiﬁcial Neural Network represented nonmonotonic inheritance network replaces initial The proposed knowledge extraction method presented Section 5 3 The knowledge initialization method The representation nonmonotonic inheritance network corresponds domain knowledge based directed acyclic graphs set descriptions The knowledge initialization phase formally deﬁned follows Given A directed acyclic graph G V E R POS NEG V set nodes represent objects domain discourse E set edges represent relations objects Set R consists edges represent ordinary relations set POS consists edges represent exceptional positive relations set NEG consists edges represent exceptional negative relations Finally assumed R POS NEG set relations R r r cid8q scid9 type semantics isa ako isl 3 type ii set exceptions X cid8q n1 nk p1 plcid9 q n1 nk p1 pl represent objects domain discourse ni exceptional negative relation q pi exceptional positive relation q initialize Artiﬁcial Neural Network set k N cells U u1 uk receives network binary input gives network binary output Moreover cell ui performs operation S described later inputs set integer weights W wij j cid1 k proper activation function σ applied network outputs cid1 σ x 1 1 x 0 x cid1 0 1 Cell input activation assumed discrete Section 4 justiﬁcation In following use interchangeably notion link inheritance network notion weighted connection neural network We ﬁrst present knowledge initialization method respect main characteristics common sense reasoning nonmonotonicity redundancy ambiguity B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 7 Fig 2 An ANN initialized NIN 31 Inserting symbolic knowledge exceptions Exceptions inheritance properties general classes speciﬁc ones introduce nonmonotonicity reasoning represented knowledge The existence speciﬁc knowledge object discourse change previously valid conclusions Consider example nonmonotonic inheritance network shown left Fig 2 An interpretation symbols following A stands HOVER B stands FLY C stands BIRD D stands PENGUIN E stands MALE PENGUIN The fact object discourse bird leads conclusion ﬂies But speciﬁc fact penguin forces conclusion ﬂy The initial nonmonotonic inheritance network transformed Artiﬁcial Neural Network shown right Fig 2 The equivalence ANN initial NIN far intended meaning concerned requires simulation inheritance cancelling exceptions represented negative links This achieved attaching negative weight connection Artiﬁcial Neural Network corresponds negative link symbolic domain knowledge shown right Fig 2 In way activation cell u4 u5 corresponding fact object discourse penguin male penguin prevent cells u2 u1 getting activated corresponding conclusion object discourse ﬂies hovers The considered standard activation function cell operation 32 Inserting symbolic knowledge redundancies Since aim simulate common sense reasoning closely possible important nonmonotonic reasoner handle redundant information consistent manner Of course long reasoners knowledge world remains unchanged 8 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Fig 3 A NIN supplemented redundant atomic statement time kind information recognized subsequently removed preprocessing phase reasoners life cycle However reasoners knowledge continuously subjected revisions case possible redundant contradictory information introduced effect revision process Redundant information closely related stability cumulativity reasoner Suppose reasoner knowledge represented nonmonotonic inheritance network supports conclusion isaX Y We reasoner stable inserting knowledge base redundant information isaX Y continues support conclusions supported insertion piece information Consider example nonmonotonic inheritance network shown left Fig 3 interpretation symbols previous example Clearly redundant fact male penguin bird change previous conclusion taking redundant fact consideration This satisﬁed activation cells right Fig 3 This kind stability called atomic stability It obvious Artiﬁcial Neural Network initialized proposed methodology supports atomic stability Atomic stability viewed 26 acceptability criterion inheritance reasoner However kind stability called generic stability viewed criterion In opinion atomic generic stability viewed acceptability criteria inheritance reasoner 23 Consider example nonmonotonic inheritance network shown left Fig 4 interpretation symbols previous example Clearly redundant fact bird hovers change previous conclusion taking redundant fact consideration This satisﬁed activation cells right Fig 4 Without connection cell u3 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 9 Fig 4 A NIN supplemented redundant generic statement cell u1 activation cell u4 u5 prevents activation cells u2 u1 However longer true presence connection Therefore Artiﬁcial Neural Network initialized methodology proposed far support generic stability In order overcome problem negative weight 2 attached connections correspond negative links But technique offer solution negative weight absorbed activation function standard type function weighted sum inputs cell To address problem deﬁne cell operation S computation maximum absolute value inputs cell Thus ui Si in1 inm in1 cid16 in2 cid16 cid16 inm cid16 binary operator deﬁned x cid16 y xyxy 2 xyxy 2 1 xεy 0 xεy 0 x y 0 xεy xεy 2 3 in1 inm inputs cell ui uj connected ui inj derived function cid1 inj 0 wji cid16 Sj σ Sj 1 σ Sj 1 4 Intuitively cell operation S guarantees output cell identical input maximum absolute value In case input equal absolute value output identical negative This achieved subtracting small positive number ε inputs determining resolution distinguishing x y 10 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Therefore Fig 4 w42 2 output cell u2 2 u4 activated Eventually output cell u2 output cell u1 Considering positive outputs activating cell Artiﬁcial Neural Network Fig 4 behaves correctly satisfying important property stability 33 Satisfying inferential distance ordering It case negative link preempted speciﬁc positive vice versa Therefore ordering exception links negative positive according speciﬁc class refer The speciﬁc referring class higher priority exception link This ordering referred literature inferential distance ordering 54 viewed acceptability criterion nonmonotonic reasoning It ensures conﬂicts exception links resolved favor exceptions higher priority according inferential distance ordering favor exceptions concerning speciﬁc classes In order properly initialize Artiﬁcial Neural Network preserve inferential distance ordering But negative links represented negative weight positive links positive weight resolve conﬂict favor speciﬁc We overcome problem assigning negative positive weight negative positive link respectively magnitude relative topological ordering cells According topological ordering function f U N assigns integer cell cell ui ancestor cell uj sense exists connection path ui uj f ui f uj We assign weight exception link starting links having tail nodes lowest topological order If links having tail nodes topological ordering start negative links To negative link negative weight equal topological order tail node assigned To positive link positive weight equal topological order tail node examined negative link path assigned Exceptionally positive link case head node topological order lower equal head node examined negative link examined negative links tail node positive weight equal topological order tail node assigned The methodology applied example Fig 5 formally presented Initialization Algorithm Section 35 Intuitively methodology guarantees path combination positive negative exception links satisﬁes stability inferential distance ordering Section 54 proof 34 Inserting symbolic knowledge conﬂicts Nonmonotonic multiple inheritance frequently introduces multiple extensions theory We quote 9 extension set beliefs sense justiﬁed reasonable light known world B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 11 Fig 5 Assigning negative positive weights according topological ordering These multiple extensions conﬂicts represented knowledge Usually cases multiple inheritance treated cases ambiguity 54 effort resolve conﬂicts exception 52 The way multiple extensions treated depends credulous skeptical view adopted 26 The proposed methodology deﬁnition recognize multiple extensions supports preference conclusions represented paths containing connections weights maximum absolute value Consider example nonmonotonic inheritance network shown left Fig 6 An interpretation symbols following A stands PACIFIST B stands QUAKER C stands REPUBLICAN D stands NIXON The activation cells right network Fig 6 supports conclusion deﬁnitely Ds As Cs opposite conclusion supported As Bs This conclusion activation cell u3 prevents u1 activation Generally path contains connection maximum absolute value transfer value prevalent Therefore weight negativepositive negativepositive conclusion derived One main advantages Artiﬁcial Nonmonotonic Neural Networks knowledge reﬁnement phase explained later section conﬂicts resolved favor probable ones respect training set 12 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Fig 6 Conﬂicted extensions 341 Inserting symbolic coupled knowledge In nonmonotonic reasoning usually objects coupled immediate succes sors This means object allowed possess property possessed immediate successors object possesses property strictly belongs particular class Of course kind nonmonotonic reasoning skeptical reasoning instance object coupled immediate succes sors In proposed methodology coupling decoupling assured This conclusion relied maximum absolute value connections Consider example nonmonotonic inheritance network shown left Fig 7 Clearly Ds Es As negative weights w32 w53 A skeptical reasoner contrary supports conclusions Es As Ds As allowing decoupling Es Ds Of course knowledge reﬁnement phase coupling decoupling preferred favor probable respect training set 35 The initialization algorithm According methodology presented sections initialization algorithm follows Given direct acyclic graph GV E R POS NEG set relations R r r cid8q scid9 set exceptions X cid8q n1 nk p1 pkcid9 initialize Artiﬁcial Neural Network set cells U u1 uk set weights W wij j cid1 k B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 13 Fig 7 Another example conﬂicted extensions executing following algorithm The initialization algorithm 1 vi V distinct q referred R deﬁne cell ui U 2 e j E r j R deﬁne connection ui uj wij 1 3 n j NEG cid8i j cid9 X ascending topological order deﬁne connection ui uj wij d d positive integer order allow contiguous weights 4 p j POS cid8i j cid9 X ascending topological order deﬁne connection ui uj wij d d closed path edge n x y NEG p f j cid2 f y Otherwise deﬁne connection ui uj wij d x 4 A knowledge reﬁnement method Knowledge reﬁnement based training corresponding Artiﬁcial Neural Network At time knowledge reﬁnement aims effective knowledge extrac tion To end need training method preserves symbolic meaning initialized Artiﬁcial Neural Network In case accomplished restricting weights set selected ones ﬁxed architecture Notice training algorithms specialized train particular type network architecture 14 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 In case network considered network neurons discrete states Thus consider Discrete Multilayer Neural Network DMNN consisting L layers ﬁrst layer denotes input L output intermediate layers hidden layers It assumed l 1th layer Nl1 units These units operate according following equations net l j Nl1cid6 i1 wl1l ij yl1 θ l j yl j σ l cid7 cid8 net l j 5 j net input j th unit lth layer wl1l net l connection weight ith unit l 1th layer j th unit lth layer yl denotes output ith unit belonging lth layer θ l j denotes threshold j th unit lth layer σ activation function We consider units σ netl discrete activation function We especially focus units output states usually called binary hardlimiting units 38 cid9 ij cid8 cid7 net l j σ l true false cid2 0 net l j net l j 0 6 Although units discrete activation function superseded large extent computationally powerful units analog activation function DMNNs important handle inherently binary tasks neural networks Their internal representation clearly interpretable computationally simpler understand networks sigmoid units provide starting point study neural network properties Furthermore hardlimiting units understand better relationship size network training complexity 17 In 13 demonstrated DMNNs hidden layer create decision region expressed ﬁnite union polyhedral sets unit input layer Moreover artiﬁcially created examples given networks create non convex disjoint decision regions Finally discrete activation functions facilitate neural network implementations digital hardware costly fabricate The common feed forward neural network FNN training algorithm propagation BP 49 makes use gradient descent directly applied networks units discrete output states discrete activation functions hardlimiters nondifferentiable This holds modiﬁcations BP method 3132 However modiﬁcations gradient descent approach presented literature 145363 In 15 approximation gradient descent socalled pseudogradient training method proposed This method uses gradient sigmoid heuristic hint instead true gradient Experimental results validated effectiveness approach We derive apply new training method DMNNs makes use gradient approximation introduced 15 Our method exploits imprecise information error function approximated gradient like pseudogradient method improved convergence speed potential train B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 15 DMNNs situations according experiments pseudogradient method fails converge For comparative results method BP 33 41 Problem formulation proposed solution We consider units discrete output states use convention f f false t t true instead classical 0 1 1 1 f t real positive numbers f t Real positive values prevent units saturating logic false power inﬂuence layer DMNN help justiﬁcation approximated gradient value employ First let deﬁne error discrete unit follows ej t dj t yL j t j 1 2 NL 7 dj t desired response j th neuron output layer input pattern t yL j t output kth neuron output layer L Notice N refers number output cells semantically related input cells More speciﬁcally output cells eventually activated input cells Thus consider subgraph initial inheritance network For ﬁxed ﬁnite set input output cases square error training set contains T representative cases E Tcid6 t 1 Et Tcid6 NLcid6 t 1 j 1 e2 j t 8 The idea pseudogradient ﬁrst introduced training discrete recurrent neural networks 6566 extended DMNNs 15 The method approximates true gradient Ew error function Ew respect weights w introducing analog set values outputs hidden layer units output layer units j Eq 5 written Thus assumed yl cid7 cid7 S cid8cid8 yl j cid10σ l net l j S deﬁned 0 1 true false S deﬁned 1 1 x cid2 05 x 05 cid10σ x cid1 cid1 cid10σ x true false x cid2 0 x 0 Using chain rule pseudogradient computed cid11E wl1l ij cid10 j yl1 δl backpropagating error signal cid10δ output layer cid7 netL j cid7 dj S netL j scid25 cid8 cid8cid8 cid10 δL j cid7 9 10 11 12 13 16 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 hidden layers l 2 L 1 cid8cid6 cid10 δl j scid25 cid7 net l j wll1 j n cid12 δl1 n n 14 In expressions scid25net l j derivative analog activation function By real positive values true false ensure pseudogradient reduce zero output false Notice use σ cid25 zero nonexistent zero Instead use scid25 positive cid10 δl j gives indication direction magnitude step function net l j error surface E However pointed 15 value pseudogradient accurate gradient descent based training DMNNs considerably slow compared BP training FNNs In order alleviate problem propose alternative pseudogradient training method procedure The proposed training method applied changing selected weights epoch useful approach It based recently proposed unconstrained optimization methods 5961 Next present proposed training method For simplicity index se lected weights sequence wi 1 2 n Thus order ﬁnd point w w n minimizes given error function 1 w 2 w E D Rn R 15 1 w0 2 w0 2 w0 speciﬁc bounded domain D try obtain sequence points wk k 0 1 converges w So arbitrary chosen starting vector weights w0 n D subminimize E w1 direction Now cid13w1 w0 subminimizer course point cid13w1 w0 n possesses smaller function value cid13w1 We repeat process ﬁnd subminimizer point w0 Then set w1 1 cid13w2 w2 direction starting vector weights w1 2 w0 n Thus compute point w1 3 starting vector weights w1 n formed Now replacing starting point w0 w1 repeat process compute w2 ﬁnal estimated point w computed according predetermined accuracy Notice use onedimensional subminimization techniques minimize error function For details techniques 736395758 1 w0 2 way compute point w1 n point w1 w1 2 w0 2 w1 1 w1 1 w1 With discussion mind provide high level description algorithm E indicates error function w0 w0 n starting weights h h1 hn starting stepsizes coordinate direction MEP maximum number epochs required δ ε predetermined desired accuracy 1 w0 The training algorithm 1 Input E w0 h MEP δ ε 2 Set k 1 3 Set 0 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 17 4 If k MEP replace k k 1 step Step 12 5 Replace 1 continue 6 Compute subminimizer cid13wi accuracy δ ith direction applying onedimensional subminimization iterative scheme 7 If cid13wi subminimizer E ith direction set wk1 cid13wi wk set wk1 8 If n Step 5 9 If Ewk1 cid1 Ewk Step 3 set y0 wk continue 10 Apply step pseudogradient training method utilizing starting value y0 output value ySUB 11 If Ewk1 EySUB set wk1 ySUB return Step 3 12 Output wk Ewk Our experience cases problems studied 33 6061 application subprocedure Step 10 necessary We placed algorithm sake completeness For proof convergence algorithm related ones 5961 5 The knowledge extraction method One main advantages hybrid comes fact symbolic access reﬁned knowledge Therefore knowledge extraction phase reﬁnement knowledge provided trained Artiﬁcial Neural Network extracted comprehensible symbolic scheme great importance reliability hybrid The proposed knowledge extraction method Artiﬁcial Nonmonotonic Neural Net works heavily relies reversing initialization phase The cells Artiﬁcial Neural Network simply transformed nodes Nonmonotonic Inheritance Network But reﬁnement initialized knowledge actually represented changes weights connections Artiﬁcial Neural Network These changes actually alize reﬁnement initialized knowledge resolving conﬂicts The initial knowledge inheritance network changed insertions deletions exception links As known unique trained set weights w veriﬁes corresponding error function To ensure changes weights concern connections contribute knowledge reﬁnement apply training algorithm selected weights described Section 53 51 Resolving conﬂicts In resolving conﬂicts technique based identiﬁcation extension supported set classiﬁed examples reﬁnement phase employed Extensions actually represented paths inheritance network 18 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 connectionist network The identiﬁcation prevalent extensions reﬁnement phase achieved identiﬁcation changes weights attached connections In initialized connectionist network prevalent extensions represented path containing connections weights maximum absolute value Since negative weights support negations decrease weight gives precedence prevalent extension supporting negations On hand increase weight contributes cancelling negative weight gives precedence prevalent extension supporting positive result An increase decrease weight wij deﬁned regard value reﬁnement phase w0 ij When prevalent extension identiﬁed change weight extracted inheritance network modiﬁed order support extension This achieved adding inheritance network proper exception link positive negative depending result supported identiﬁed extension The added exception link concerns extension clearly concern exception link corresponding connection changed Therefore added exception link attached path represents extension Consider example trained Artiﬁcial Neural Network Fig 8 Suppose decrease weight w32 6 w0 3 identiﬁes extension 32 F D C A prevalent Then negative exception link F A added supports prevalent extension Notice added link affect existed negative exception link C A remain The added link course priority existing according inferential distance measure In order add exception links properly need identify path represents certain extension Since prevalent extension identiﬁed change weight connection represents existing exception link assume extension Fig 8 Adding exception links B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 19 represented path containing tail head nodes exception link Such path F D C A Fig 8 containing tail node F head node A existing exception link C A In general exists path node representing input cell consideration node representing output node consideration path contains changed weight add exception link nodes The exception link positive negative increase decrease negative positive weight decrease increase positive negative weight 52 Preemption exception links It case existing exception link preempted supported set training examples Such exception edges identiﬁed changes weights corresponding connections Consider example trained Artiﬁcial Neural Network Fig 9 initialized shown Fig 5 Suppose weight w64 decreased Therefore positive exception link F D added Notice added link introduces conﬂict This conﬂict resolved implementation level existing exception link F D deleted extraction algorithm subsection Fig 9 Preempted exception links 20 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 53 The extraction algorithm To attack cases reﬁnement phase training method applied selected weights Mainly weights ones correspond exception links Therefore change weight guarantees prevalence validation exception link Meanwhile resolving conﬂicts extensions represented paths exceptions identiﬁcation prevalent extension possible weights correspond exception links considered Actually case initialized connectionist network trained We handle problem allowing addition weights set selected weights correspond ordinary links These weights connections adjacent output cells represent nodes incoming exception links Consider example trained Artiﬁcial Neural Network Fig 11 The training algorithm allows addition weight w563561 set selected ones apart w560559 w561560 The extraction algorithm ﬁrst transforms cells connections nodes edges Then algorithm adds proper negative positive exception links prevalent extension identiﬁed The extraction algorithm presented Given trained connectionist network set cells U u1 uk set weights W wij j cid1 k initial set weights W 0 w0 ij j cid1 k reﬁnement phase Revise Inheritance Network set nodes V set edges represent ordinary relations R set edges represent exceptional positive relations POS set edges represent exceptional negative relations NEG Executing following algorithm The extraction algorithm 1 cell ui U construct node vi V 2 connection ui uj wij 1 construct edge e j R wij 0 construct edge e j NEG wij 0 construct edge e j POS 3 connection ui uj w0 cid28 wij ij path h t containing j exists h node representing input consideration t node representing output consideration B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 21 w0 ij wij add exist exception link e h t POS delete existing exception link e h t NEG elseif w0 ij wij add exist exception link e h t NEG delete existing exception link e h t POS 54 Soundness completeness The proposed methodology sound complete proved following In general consider sound output valid complete valid output produced The proof soundness completeness ANNNs actually reduced proof soundness completeness initialization revision extraction phase separately The completeness initialization phase obvious comes straightforward initialization algorithm Section 35 Every inheritance network transformed neural network type described Section 3 This correspondence sets nodes V edges E inheritance network sets cells U connections W neural network respectively Obviously topological ordering nodes inheritance network possible assign weights connections neural network The soundness initialization phase guarantees equivalence neural network intended meaning background knowledge We consider equivalence guaranteed neural network satisﬁes inheritance stability inferential distance ordering described Section 3 We prove soundness initialization phase cases Lemmas 1 2 guarantee satisfaction inheritance property Satisfaction inferential distance metric stability property guaranteed Theorems 3 6 Notice conﬂict resolution property guaranteed reﬁnement phase following proofs refer intra extension intrapath domain Lemma 1 In neural network constructed inheritance network exceptions positive ones algorithm Section 35 cell o U activated iff cell j k U connected o path connections p wj 0j 1 wj 1j 2 wj j 0 j n U wj 0j 1 wj 1j 2 wj W activated Proof Suppose cell j k 1 cid1 k cid1 n activated Thus Eq 2 obtain Sj k1 inj k cid16 inj k cid16 j k activated Eq 4 obtain 22 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 σ Sj k 1 inj k wj kj k1 cid16 Sj k Given exist positive exceptions wj kj k1 cid2 1 Sj k1 cid2 1 σ Sj k1 1 Similarly σ Sj k2 1 ﬁnally σ So 1 Lemma 2 In neural network constructed inheritance network negative exception paths positive ones algorithm Section 35 activation cell t U exception link starts prevents activation cell connected head h U exception link path connections p whj 1 wj 1j 2 j 1 j 2 U whj 1 wj 1j 2 W Proof Since cell t activated exists negative exception wth St cid2 1 σ St 1 Thus Eq 2 obtain Sh int cid16 int cid16 means Eq 4 int wth cid16 St Because exists negative exception wth 1 f t f h f h cid1 1 Thus int 1 Sh 1 σ Sh 1 Similarly Lemma 1 j k included p σ Sj k 1 Theorem 3 In neural network constructed inheritance network exceptions wf tf h wstsh paths p wj 0j 1 wj 1j 2 wj j 0 j n U wj 0j 1 wj 1j 2 W inferential distance metric stability property satisﬁed The following cases exist 1 Both negative positive In case following sub cases distinguished exclusion f f t f f h cid1 f st f sh b inclusion f f t cid1 f st f sh cid1 f f h c intersection f f t cid1 f st f f h f sh 2 The ﬁrst positive negative In case following sub cases distinguished exclusion f f t f f h cid1 f st f sh b inclusion f f t cid1 f st f sh cid1 f f h c intersection f f t cid1 f st f f h f sh This subcase actually introduces atomic stability 3 The ﬁrst negative positive In case following sub cases distinguished exclusion f f t f f h cid1 f st f sh b inclusion f f t cid1 f st f sh cid1 f f h c intersection f f t cid1 f st f f h f sh This subcase actually introduces generic stability Proof It straightforward Eqs 2 3 4 subcases considering arbitrary input output cells output cells properly activateddeactivated inferential distance metric stability property satisﬁed In following prove claim subcases The proofs rest subcases similar Consider case iia Suppose cell f t activated Then Lemma 1 node f f f t f f f sh σ Sf 1 implied It clear fourth step initialization algorithm B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 23 Section 35 wf tf h wstsh wf tf h 0 wstsh 0 Therefore output cell sh Sshinsm inst insm cid16 inst sm immediate ancestor sh Then insm wsmsh cid16 Ssm wf tf h Also inst wstsh cid16 Sst wstsh ε Eq 3 Therefore Ssh wstsh ε Eq 3 Finally σ Ssh 1 Deﬁnition 4 If m mt mh n nt nh exception links deﬁne dominant exception link link priority according inferential distance metric stability property respect Theorem 3 Formally m dominant n denoted m cid29 n signSnh signwm f mt cid2 f nt f mh cid2 f nh signwm cid28 signwn signwm signwn 16 cid1 Lemma 5 The dominant operator cid29 deﬁnes relation D set connections W Relation D reﬂexive antisymmetric Proof Obviously m mt mh W m cid29 m f mt f mt Therefore D reﬂexive Obviously m mt mh n nt nh W m cid29 n n cid29 m m n implied Therefore D antisymmetric Theorem 6 The general case neural network constructed inheritance network reduced case Theorem 3 exceptions paths allowed This accomplished successively replacing pairs exceptions m n dominant exception starting pair lower topological order tail m Under constraint prove D transitive permits successive substitutions Proof We prove l lt lh m mt mh n nt nh W f lt f lh f mt f mh f nt f nh f lt cid2 f mt cid2 f lh cid2 f mh f mt cid2 f nt cid2 f mh cid2 f nh l cid29 m m cid29 n l cid29 n implied Hence D transitive If signwl signwm negative positive Deﬁnition 4 f lt cid2 f mt f lh cid2 f mh If signwm signwn signwl signwm signwn Deﬁnition 4 f mt cid2 f nt f mh cid2 f nh So f lt cid2 f nt f lh cid2 f nh second case Deﬁnition 4 l cid29 n Else signwm cid28 signwn signwl cid28 signwn m cid29 n signSnh signwm Therefore signSnh signwl ﬁrst case Deﬁnition 4 l cid29 n Consider case signwl cid28 signwm If signwm cid28 signwn signwl signwn Since constraint f lt cid2 f nt f lh cid2 f nh second case Deﬁnition 4 l cid29 n Finally signwm signwn signwl cid28 signwn Since l cid29 m signSmh signwl Moreover signSnh determined signwm signSmh But signSmh signwl signSnh determined dominant l m l Therefore signSnh signwl ﬁrst case Deﬁnition 4 l cid29 n Completeness soundness reﬁnement phase reduced convergence training algorithm For proof convergence algorithm related results 5961 24 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Completeness extraction phase obvious comes straightforward extraction algorithm Section 53 The extraction algorithm guarantees output cell o U reached input cell U path p wij 1 wj 1j 2 wj j 1 j n U wij 1 wj W includes weight wpq belongs set selected weights Therefore change weight belongs set selected weights reﬁnement phase transformed addition exception link head tail paths changed weight assigned Soundness extraction phase obvious An increase negative weight decrease positive weight actually decreases absolute value weight gives precedence conﬂicted path positive negative weight respectively Similarly decrease negative weight increase positive weight gives precedence path contains 6 Experimental results In order test effectiveness efﬁciency ANNNs learning need reﬁne initial knowledge pure nonmonotonic domain Instead choosing scarcely mentioned literature domain appropriate comparative results tested ANNNs problem considered deﬁned nonmonotonic domain literature treated The problem extraction classiﬁcation rules dataset considered scope work reduced extraction general patterns exceptions Classiﬁcation problem monotonic domain attacked symbolic connectionist techniques There hybrid systems mentioned introduction evaluated The key idea ANNNs classiﬁcation problem pseudo nonmonotonic domain consider initial arbitrarily chosen classiﬁcation rules introduce conﬂicts reﬁne resolving conﬂicts Therefore exceptions initial knowledge actually artiﬁcially deﬁned The reader bear mind ANNNs capable reﬁning initial knowledge nonmonotonic domain To knowledge exists hybrid apart Pinkass symmetric networks based translation Penalty Logic heavily relies user deﬁned penalties turn require kind preprocessing conﬂicts classifying examples belong nonmonotonic domain Of course relation established Logic Programming nonmonotonic reasoning default autoepistemic logics based treating negation 35 Thus hybrid systems like 23 12 nonmonotonic domain However practical problems consideration establishing relation Logic Programming nonmonotonic reasoning default interpretation negation NPcomplete task Therefore heuristics negation failure prove adopted Moreover consideration hurdles imposed logicbased formalisms general lack stability property mentioned introduction B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 25 In order evaluate ANNNs obtaining comparative results hybrid systems transform monotonic domain artiﬁcial nonmonotonic It actually transformation prevents ANNNs outperforming alternative systems classiﬁcation monotonic domain The main problem ANNNs inherently treat noise exceptions tend overspecialize Therefore ANNNs accurate unseen examples applied monotonic domain But obtained experimental results presented follows comparable hybrid systems superior wellknown pure symbolic connectionist systems In subsection compare ANNNs pure symbolic connectionist systems problem classifying realworld datasets test datasets More speciﬁcally evaluate ANNNs realworld dataset representing customer base big telecommunications company realworld datasets domain Molecular Biology especially DNA sequence analysis promoter recognition splicejunction determination problems 62 Notice DNA sequence analysis problems benchmarks comparing performance learning systems 61 Extracting customer proﬁles Classiﬁcation rules extracted supervised learning methods classify data predeﬁned classes described set concepts attributes In cases independently adopted representation scheme set classiﬁcation rules describes class deﬁning general pattern exceptions A subset rules deﬁnes general pattern young loan applicants high risk rest deﬁne exceptions young loan applicants high income low risk Consider example sample rules shown Fig 10 These rules constructed CN2 algorithm 5 behavior customer base big telecommunications company 1 The conditions rules certain attributes customer base predeﬁned classes denote proﬁt related behavior It obvious rules deﬁne general patterns ﬁrst exception rules Therefore represent general patterns initial knowledge nonmonotonic inheritance scheme ﬁnd exceptions reﬁning initial knowledge respect database records training examples Initial knowledge usually provided experts In tests performed ANNNs evaluation chosen randomly If consider general patterns shown Fig 10 initial knowledge initial inheritance network shown Fig 11 constructed Consider following interpretation 2 symbols A stands BAD B stands COMMON C stands GOOD 1 For conﬁdentiality reasons classes rules actual ones 2 For conﬁdentiality reasons attribute values actual ones 26 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Fig 10 A sample decision list Fig 11 An example D stands EMPLOYEES OF GOVERNMENT COMPANIES E stands RESIDENTS OF AEGEAN ISLANDS F stands YOUNG PEOPLE G stands YOUNG PEOPLE EMPLOYEES OF GOVERNMENT COMPA NIES RESIDENTS OF AEGEAN ISLANDS The inheritance network turn initialize connectionist network shown ﬁgure The trained relational data training examples Finally reﬁned rules extracted form new inheritance network shown Fig 12 This reﬁned inheritance network infer Employees B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 27 Fig 12 Resolving conﬂicts government companies residents Aegean Islands common customers This holds added positive exception link G B negative exception link G C The ﬁrst supports fact Gs Bs previously conﬂicted extension Gs Bs Cs removed second Notice query actually combination attribute values considered exceptional reﬁned training Hence combinations attribute values considered potential exceptions determining pseudo nonmonotonic domain The initial weights previous example shown Fig 11 During reﬁnement phase initial weights changed trying minimize error function The error function given output cells input cell takes following form E 3cid6 i1 cid14 σ NOUTi 2DOUTi 1 cid15 2 Thus E cid14cid7 w560559 cid16 S560 cid16 w564559 cid16 S564 2DOUT559 1 cid7 w561560 cid16 S561 cid16 w562560 cid16 S562 2DOUT560 1 cid7 w563561 cid16 S561 2DOUT563 1 cid8 2 cid15 17 cid8 2 cid8 2 A table 30000 records training set This table CN2 algorithm rules Fig 10 extracted 31 records table form young_people empl_gov_co res_Aegean_Isl bad encoded in565 1 DOUT561 0 DOUT560 0 DOUT559 1 129 records encoded in565 1 DOUT561 0 DOUT560 1 DOUT559 0 108 records encoded in565 1 DOUT561 1 DOUT560 0 DOUT559 0 28 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Table 1 Instances trace execution w0 563561 1 1 1 566 1 1 1 w0 561560 561 w0 560559 560 561 561 561 561 566 2 566 566 560 560 560 560 E 1280 2104 1280 2144 1072 1072 1112 Some epochs reﬁnement process exhibited Table 1 Theoretically weight w560559 changed properly interval ζ ζ ζ big positive integer We consider ζ number assigned cell topological ordering plus Actually weight w560559 changed taking values set 566 566 Since minimization error function algorithm proceeds changing weights Eventually weight w563561 changed A decrease minimizes error function Meanwhile case possibly general case outputs deactivated conﬂict resolution In case ignore decrement error function proceed keeping change weight This case outputs activated According experience view tests case encountered Then weight w561560 changed An increase weight minimizes error function respect initial value More changes minimize error function reﬁnement phase ﬁnishes The revised inheritance network shown Fig 12 There increase weight w561560 identiﬁes extension G E B prevalent Therefore positive exception link G B added supports prevalent extension Notice added link affect existed negative exception link B A retained Moreover decrease weight w563561 Therefore negative exception link G C added Notice revised inheritance network negative exception link G A added order directly exclude extension G D A Meanwhile positive exception link G B resolve conﬂict indirectly adopting skeptical view 26 Alternatively conﬂicts resolved implementation level Thus positive exception link added extraction algorithm negative exception links added rest output nodes Initial knowledge represented inheritance network shown Fig 11 consists general rules refer values different attributes characterizing training set nodes D E F input node G represents potential exception refers combination attribute values If consider possible general rules different attribute value arbitrarily classify examples classes A B C use ANNNs reﬁne initial knowledge resolving artiﬁcial B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 29 Fig 13 Testset performance conﬂicts Obviously consider exceptions rules formed combination values taken pair attributes try resolve conﬂicts We repeatedly consider exceptions exceptions larger depth formed combination values taken triple attributes Fig 11 Thus end obtained inheritance network represents classiﬁcation rules resolved conﬂicts classiﬁcation process We compared ANNNs extraction customer proﬁles symbolic connectionist techniques We chosen CN2 wellknown symbolic algorithm genuine connectionist learning techniques SelfOrganized Map algorithm 28 Radial Basis Function networks The overall classiﬁcation accuracy tested techniques shown Fig 13 set 30000 patterns chosen set 60000 patterns In case formed training set choosing randomly 80 initial set test set remaining 20 Using SOM best results obtained number neurons range 50 150 Within range classiﬁcation performance strongly dependent number neurons The training set size large For instance 10000 training patterns taken training set obtained performance 36 However 30000 training patterns 50000 patterns tried classiﬁcation performance varied signiﬁcantly 46 When class information appended input training patterns classiﬁcation performance improved 24 48 50 The LVQ phase supervised reﬁnement class boundaries improves classiﬁcation results 5658 Radial Basis Function RBF networks taking centers weight vectors neurons obtained SOM LVQ Then RBF network training algorithm learns locally classiﬁcation function accounting patterns fall near training center 20 However practically numerical solution formulated equations RBF training problematic large size problem The currently achieved performance RBF networks 37 The poor performance attributed large size training set involves large matrices sparse 30 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 As far symbolic algorithm CN2 concerned overcomes low classiﬁcation performance problem connectionist techniques Actually construction rules related predeﬁned measure classiﬁcation performance entropy However suffers bad time space complexity The example previous section shown Fig 11 concerns particular combination attribute values determining potential exception probably resolved This example actually subset complete solution provided mentioned approaches The overall time complexity dominated number different combinations values subset input attributes examined This Z cid16 Scid6 S cid6 cid14 i2 j 1 Ak1 Akj cid17 cid15 18 S number attributes Akn number different values assigned attribute ﬁeld Therefore time complexity particular application ANNNs classiﬁcation problem directly proportional number different attributes number different values attributes On hand number attributes participating combination depth exception impact classiﬁcation accuracy Large depths tend overspecialize 62 Analyzing DNA sequence We evaluate ANNNs promoter recognition splicejunction determination problem Initial knowledge problem rule sets 12 56 The initial knowledge promoter recognition problem represented NIN shown Fig 14 sequence location arbitrarily connected promoter output node promoter These output nodes connected exception link The approach adopted splicejunction determination problem We crossvalidation testing methodology More speciﬁcally promoter recognition leavingoneout crossvalidation set 106 examples permuted divided 106 sets example set In evaluation phase set seen learning testing Fig 14 The initial NIN promoter recognition problem B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 31 Fig 15 Testset performance promoter recognition Fig 16 Testset performance splicejunction determination examples remaining 105 sets training Hence evaluation process requires 106 training phases Testset performance promoter recognition ANNNs shown Fig 15 number recognized positive negative examples depicted Testset performance splicejunction determination 10fold crossvalidation 1000 examples chosen randomly standard set 3190 examples illustrated Fig 16 Testset performance ANNNs compared empirical learning algorithms methods Stormo ONeill 56 suggested biologists Notice replicate tests algorithms performed 56 The overall classiﬁcation accuracy ANNNs promoter recognition splice junction determination compared systems learn strictly examples examples background knowledge shown Fig 17 Fig 18 respectively Notice replicate tests algorithms performed 12 Performance ANNNs promoter recognition tested apart leaving oneout 10fold 20fold cross validation Fig 17 shows 10fold methodology exhibits best results 20fold worst One hypothesis 32 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 Fig 17 Classiﬁcation accuracy promoter recognition Fig 18 Classiﬁcation accuracy splicejunction determination explain result trade decrease accuracy insufﬁcient training examples 20fold overspecialization leavingoneout increase accuracy optimum training set 10fold We evaluate generalization ability ANNNs learning small sets examples The testing methodology consists splitting initial set 106 examples promoter recognition problem subsets containing approximately 25 examples 26 remaining examples 80 The set B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 33 Fig 19 Classiﬁcation accuracy learning small sets Fig 20 Classiﬁcation accuracy function depth exceptions partitioned sets increasing size smaller sets subsets larger ones In evaluation phase subsets training set 26 examples testing The classiﬁcation accuracy ANNNs phase depicted Fig 19 compared hybrid systems backpropagation Notice replicate tests performed 12 The result holds splicejunction determination problem Depth exceptions straightforward impact classiﬁcation accuracy ANNNs particular context pseudo nonmonotonic domain On hand depth exceptions straightforward impact time complexity Note large depths exceptions desirable pseudo nonmonotonic domain introduce overspecialization On contrary desirable pure nonmonotonic domain order capture exceptions Also notice tests presented far performed depth equals 2 combinations attributes Of course nonmonotonic domain larger depth better classiﬁcation accuracy worse time complexity In Fig 20 34 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 classiﬁcation accuracy promoter recognition problem depicted function depth exceptions approximately 25 examples 26 testing remaining examples training 7 Discussion conclusion research Hybrid systems typically reﬁnement domain knowledge After reﬁnement phase trained Artiﬁcial Neural Network hybrid reasoning order access reﬁned knowledge In general Artiﬁcial Neural Network construction supports access operations ISA type queries infers object possesses particular property Notice complexity ISA queries Artiﬁcial Neural Network comparable complexity ISA queries pathbased networks effective compression techniques example 2 Of course true consider enumeration activation function unit cost Moreover Artiﬁcial Neural Network support access operations concerning recognition problem ﬁnd objects satisfying particular set properties However case operations effective The main advantages hybrid systems come symbolic access reﬁned knowledge Apart effective access operations symbolic explanation capability generated outputs In utilization hybrid systems reﬁned knowledge extracted order feed domain knowledge Therefore reliability extraction phase critical factor effectiveness hybrid systems We tried use proper initialization method proper training method proper extraction algorithm ANNNs We proved preserve symbolic meaning Initial Inheritance Network effectively transform trained Artiﬁcial Neural Network comprehensible nonmonotonic inheritance network Moreover evaluated ANNNs applying classiﬁcation problem pseudo nonmonotonic domain exceptions initial knowledge artiﬁcially deﬁned We followed evaluating procedure order obtain comparative results knowledge available benchmarks pure nonmonotonic domains On hand monotonic domain known widely benchmarks 56 We empirically proved despite ANNNs capable reﬁning initial knowledge nonmonotonic domain performance classiﬁcation problem comparable hybrid monotone systems better wellknown monotone pure symbolic connectionist systems Of course monotone systems ANNNs compared classifying examples belong nonmonotonic domain The main problem observed experiments ANNNs inherently treat noise exceptions tend overspecialize We currently applying ANNNs different data sets constitute pure nonmonotonic domain incomplete data data drifting evolving concepts data arriving time We try improve performance ANNNs monotonic domains reduced pseudo nonmonotonic domains presented B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 35 experimental tests To end try identify malfunctions cases biased noisy data exists ANNNs pseudo nonmonotonic domains usually exhibit lower performance compared systems In general ANNNs treating noise exceptions sensitive noise training domain knowledge especially ANNNs applied monotonic domain This result obvious presented experiments We propose attack problem properly selecting training set able deﬁne desired outputs possible That training forced contain examples resolving extensions deleting exceptions examples support represented knowledge Moreover continuous analogous reﬁnement process powerful tackling noise satisfy prerequisites imposed proposed approach To end intent modify approach continuous analogous reﬁnement processes applied instead In conclusion ANNNs deﬁned capable reﬁning initial knowledge nonmonotonic domain We applied ANNNs monotonic domain reducing pseudo nonmonotonic domain extraction classiﬁcation rules large relational databases Notice common monotonic knowledge representation schemes production rules traditional Expert Systems weaker nonmonotonic inheritance networks domain knowledge hybrid domain knowledge traditional Expert System In case initializationreﬁnement extraction cycle considered knowledge acquisition inferring methodology traditional Expert Systems capable justifying produced conclusions Finally interested examining behavior training methods preserve symbolic meaning initialized Artiﬁcial Neural Network restricted changing weights add hidden layers connections The semantics NINs extracted trained Artiﬁcial Neural Network research Acknowledgements We wish thank anonymous referees constructive comments suggestions invaluable criticisms helped improve paper We wish thank Dr S Papadimitriou Mr G Petalas invaluable help tests Dr P Peppas useful discussions References 1 R Andrews J Diederich AB Tickle Survey critique techniques extracting rules trained artiﬁcial neural networks KnowledgeBased Systems 8 1995 373389 2 B Boutsinas On managing nonmonotonic transitive relationships Proc 8th IEEE International Conference Tools Artiﬁcial Intelligence ICTAI Toulouse France 1996 pp 374382 3 B Boutsinas YC Stamatiou G Pavlides Massively parallel support nonmonotonic reasoning J Geller H Kitano C Suttner Eds Parallel Processing Artiﬁcial Intelligence Elsevier Science Amsterdam 1997 pp 4167 36 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 4 G Brewka Cumulative default logic In defense nonmonotonic inference rules Artiﬁcial Intelli gence 50 2 1991 183205 5 P Clark T Niblett The CN2 induction algorithm Machine Learning 3 1989 261283 6 JP Delgrande T Schaub W Ken Jackson Alternative approaches default logic Artiﬁcial Intelligence 70 1994 167237 7 JE Dennis Jr RB Schnabel Numerical Methods Unconstrained Optimization Nonlinear Equations PrenticeHall Englewood Cliffs NJ 1983 8 D Dubois H Prade Possibilistic logic preferential models nonmonotonicity related issues Proc IJCAI91 Sydney Australia 1991 pp 419424 9 D Etherington Formalizing nonmonotonic reasoning systems Artiﬁcial Intelligence 31 1987 4185 10 L Fu Introduction knowledgebased neural networks KnowledgeBased Systems 8 1995 299300 11 SI Gallant Connectionist expert systems Comm ACM 31 1988 152169 12 AA Garcez G Zaverucha The connectionist inductive learning logic programming Appl Intelligence 11 1999 5977 13 GJ Gibson FN Cowan On decision regions multilayer perceptrons Proc IEEE 78 1990 1590 1594 14 EM Gorwin AM Logar WJB Oldham An iterative method training multilayer networks threshold functions IEEE Trans Neural Networks 5 1994 507508 15 R Goodman Z Zeng A learning algorithm multilayer perceptrons hardlimiting threshold units Proc IEEE Neural Networks Signal Processing 1994 pp 219228 16 HW Güsgen S Hölldobler Connectionist inference systems B Fronhofer G Wrightson Eds Parallelization Inference Systems Lecture Notes Artiﬁcial Intelligence Vol 590 Springer Berlin 1992 pp 82120 17 SE Hampson DJ Volper Representing learning Boolean functions multivalued features IEEE Trans Systems Man Cybernet 20 1990 6780 18 I Hatzilygeroudis J Prentzas Constructing modular hybrid knowledge bases expert systems Internat J Artiﬁcial Intelligence Tools 10 12 2001 87105 19 I Hatzilygeroudis H Reichgelt Handling inheritance integrating logic objects Data Knowledge Engineering 21 1997 253280 20 S Haykin Neural Networks 2nd edn Macmillan Publishing 1999 21 S Hölldobler F Kurfess CHCLA connectionist inference B Fronhöfer G Wrightson Eds Parallelization Inference Systems Lecture Notes Artiﬁcial Intelligence Vol 590 Springer Berlin 1992 pp 318342 22 S Hölldobler Automated inferencing connectionist models Post PhD Thesis Intellektik Informatic TH Darmstadt 1993 23 S Hölldobler Y Kalinke Towards massively parallel computational model logic programming Proc ECAI94 Workshop Combining Symbolic Connectionist Processing 1994 pp 6877 24 S Hölldobler Y Kalinke H Störr Approximating semantics logic programs recurrent neural networks Appl Intelligence 11 1999 4558 25 S Hölldobler Challenge problems integration logic connectionist systems Technical Report WV9903 AI Institute Dept Computer Science Dresden University Technology 1999 26 J Horty R Thomason D Touretzky A skeptical theory inheritance nonmonotonic semantic networks Artiﬁcial Intelligence 42 1990 311318 27 Y Kalinke Using connectionist term representation ﬁrstorder deductionA critical view F Maire R Hayward J Diederich Eds Connectionist Systems Knowledge Representation Deduction Queensland Univ Tech 1997 28 T Kohonen SelfOrganized Maps Springer Berlin 1997 29 R Maclin Learning instruction experience Methods incorporating procedural domain theories knowledgebased neural networks Technical Report UW CSTR951285 1995 30 R Maclin JW Shavlik Reﬁning domain theories expressed ﬁnitestate automata Proc 8th International Machine Learning Workshop San Mateo CA 1991 31 GD Magoulas MN Vrahatis GS Androulakis Effective backpropagation training variable stepsize Neural Networks 10 1997 6982 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 37 32 GD Magoulas MN Vrahatis GS Androulakis Increasing convergence rate error backpropa gation algorithm learning rate adaptation methods Neural Computation 11 1999 17691796 33 GD Magoulas MN Vrahatis TN Grapsa GS Androulakis A training method discrete multilayer neural networks SW Ellacott JC Mason IJ Anderson Eds Mathematics Neural Networks Models Algorithms Applications Kluwer Academic Boston 1997 pp 250254 Chapter 42 34 D Makinson General theory cumulative inference M Reinfrank Ed Proc 2nd International Workshop nonmonotonic Reasoning Lecture Notes Artiﬁcial Intelligence Vol 346 Springer Berlin 1989 pp 118 35 VW Marek M Truszczynski Nonmonotonic Logic Springer Berlin 1993 pp 141187 36 GE Manoussakis MN Vrahatis GS Androulakis New unconstrained optimization methods based onedimensional rootﬁnding D Bainov A Dishliev Eds Proc 3rd International Colloquium Numerical Analysis Science Culture Technology Publishing Oxford Graphic Printers 1995 pp 127136 37 J McCarthy Epistemological challenges connectionism Behavioural Brain Sciences 11 1988 44 38 W McCullough WH Pitts A logical calculus ideas imminent nervous activity Bull Math Biophysics 5 1943 115133 39 JM Ortega WC Rheinboldt Iterative Solution Nonlinear Equations Several Variables Academic Press New York 1970 40 G Pinkas The equivalence connectionist energy minimization propositional calculus satisﬁability Technical Report WU CS 9003 1990 41 G Pinkas Energy minimization satisﬁability propositional logic D Touretzky J Elman T Sejnowski G Hinton Eds Proc Connectionist Models School Morgan Kaufmann San Mateo CA 1990 42 G Pinkas Symmetric neural networks propositional logic satisﬁability Neural Computation 3 1991 282291 43 G Pinkas Expressing ﬁrstorder logic symmetric connectionist networks LN Kanal CB Suttner Eds Informal Proc Internat Workshop Parallel Processing AI Sydney Australia 1991 pp 155 160 44 G Pinkas Propositional nonmonotonic reasoning inconsistency symmetric neural networks Proc IJCAI91 Sydney Australia 1991 pp 525530 45 G Pinkas Constructing syntactic proofs symmetric networks Advances Neural Information Processing Systems IV NIPS91 1992 217224 46 G Pinkas Reasoning nonmonotonicity learning connectionist networks capture propositional knowledge Artiﬁcial Intelligence 77 1995 203247 47 J Pollack Recursive distributed representations Artiﬁcial Intelligence 46 1990 77105 48 R Reiter A logic default reasoning Artiﬁcial Intelligence 13 1980 81132 49 DE Rumelhart GE Hinton RJ Williams Learning internal representations error propagation DE Rumelhart JL McClelland Eds Parallel Distributed Processing Explorations Microstructure Cognition Vol 1 MIT Press Cambridge MA 1986 pp 318363 50 E Sandewall Nonmonotonic inference rules multiple inheritance exceptions Proc IEEE 74 1986 13451353 51 B Selman HJ Levesque The tractability pathbased inheritance Proc IJCAI89 Detroit MI 1989 52 L Shastri Default reasoning semantic networks A formalization recognition inheritance Artiﬁcial Intelligence 39 1989 283355 53 DJ Tom Training binary node feed forward neural networks backpropagation error Electronics Letters 26 1990 17451746 54 D Touretzky The Mathematics Inheritance Systems Morgan Kaufmann Los Altos CA 1986 55 D Touretzky J Horty R Thomason A clash intuitions The current state nonmonotonic multiple inheritance systems Proc IJCAI87 Milan Italy 1987 pp 476482 56 GG Towell JW Shavlik Knowledge based artiﬁcial neural networks Artiﬁcial Intelligence 40 1994 119165 57 MN Vrahatis Solving systems nonlinear equations nonzero value topological degree ACM Trans Math Software 14 1988 312329 58 MN Vrahatis CHABIS A mathematical software package locating evaluating roots systems nonlinear equations ACM Trans Math Software 14 1988 330336 38 B Boutsinas MN Vrahatis Artiﬁcial Intelligence 132 2001 138 59 MN Vrahatis GS Androulakis JN Lambrinos GD Magoulas A class gradient unconstrained minimization algorithms adaptive stepsize J Comput Appl Math 114 2000 367386 60 MN Vrahatis GS Androulakis GE Manoussakis A new unconstrained optimization method imprecise problems D Bainov A Dishliev Eds Proc 3rd International Colloquium Numerical Analysis Science Culture Technology Publishing Oxford Graphic Printers 995 pp 185194 61 MN Vrahatis GS Androulakis GE Manoussakis A new unconstrained optimization method imprecise function gradient values J Math Anal Appl197 1996 586607 62 JD Watson NH Hopkins JW Roberts JA Steitz AM Weiner Molecular Biology Gene Vol 1 Benjamin Cummings Menlo Park CA 1987 63 B Widrow R Winter Neural nets adaptive ﬁltering adaptive pattern recognition IEEE Computer March 1988 2539 64 W Woods Whats link Foundations semantic networks R Brachman H Levesque Eds Readings Knowledge Representation Morgan Kaufmann Los Altos CA 1985 pp 217241 65 Z Zeng R Goodman P Smyth Learning ﬁnite state machines selfclustering recurrent networks Neural Comput 5 1993 976990 66 Z Zeng R Goodman P Smyth Discrete recurrent neural networks grammatical inference IEEE Trans Neural Networks 5 1994 320330