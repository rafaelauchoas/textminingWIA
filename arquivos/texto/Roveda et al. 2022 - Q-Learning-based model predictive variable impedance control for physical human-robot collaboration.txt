Artiﬁcial Intelligence 312 2022 103771 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint QLearningbased model predictive variable impedance control physical humanrobot collaboration Loris Roveda Dario Piga Istituto Dalle Molle di studi sullIntelligenza Artiﬁciale IDSIA Scuola Universitaria Professionale della Svizzera Italiana SUPSI Università della Svizzera italiana USI la Santa 1 6962 Lugano Switzerland b Politecnico di Milano Department Mechanical Engineering La Masa 1 20156 Milano Italy Andrea Testa b Asad Ali Shahid Francesco Braghin b r t c l e n f o b s t r c t Article history Received 21 September 2021 Received revised form 18 July 2022 Accepted 6 August 2022 Available online 11 August 2022 Keywords Physical humanrobot collaboration Industry 40 Machine learning Modelbased reinforcement learning control Neural networks QLearning Stability Variable impedance control Physical humanrobot collaboration increasingly required contexts industrial rehabilitation applications The robot needs interact human perform target task relieving user workload To robot able recognize humans intentions guarantee safe adaptive behavior intended motion directions The robotcontrol strategies attributes particularly demanded industrial ﬁeld operator guides robot manually manipulate heavy parts teaching speciﬁc task With aim work proposes QLearningbased Model Predictive Variable Impedance Control QLMPVIC assist operators physical humanrobot collaboration pHRC tasks A Cartesian impedance control loop designed implement decoupled compliant robot dynamics The impedance control parameters setpoint damping parameters optimized online order maximize performance pHRC For purpose ensemble neural networks designed learn modeling humanrobot interaction dynamics capturing associated uncertainties The derived modeling exploited model predictive controller MPC enhanced stability guarantees means Lyapunov constraints The MPC solved making use QLearning method online implementation uses actorcritic algorithm approximate exact solution Indeed Qlearning method provides accurate highly eﬃcient solution terms computational time resources The proposed approach validated experimental tests Franka EMIKA panda robot test platform Each user asked interact robot controlled vertical z Cartesian direction The proposed controller compared modelbased reinforcement learning variable impedance controller MBRLC previously developed authors order evaluate performance As highlighted achieved results proposed controller able improve pHRC performance Additionally industrial tasks collaborative assembly collaborative deposition task demonstrated prove applicability proposed solution real industrial scenarios 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 This paper Special Issue Riskaware Autonomous Systems Theory Practice Corresponding author francescobraghinpolimiit F Braghin dariopigasupsich D Piga Email addresses lorisrovedaidsiach L Roveda andrea8testamailpolimiit A Testa asadalishahididsiach AA Shahid httpsdoiorg101016jartint2022103771 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 1 Introduction 11 Context To meet customers needs oriented tailormade products companies updat ing production processes means new ﬂexible agile tools 1 In context collaborative robotics plays key role 2 providing powerful solutions assist operators execution different activities comanipulation 34 tasks knowledge transfer robotic 56 easy programmable deployable applications 7 Physical humanrobot collaboration pHRC currently investigated topics 8 In fact pHRC nowadays demanded ﬁelds applications collaborative robots 9 exoskeletons 10 However open issues state art overcome particular considering safetystability guarantees humanrobot inter action humanrobot dynamics modeling humans intention recognition active assistanceempowering purposes computation eﬃciency realtime control adaptation optimization To tackle mentioned issues pHRC scenario paper proposes QLearningbased Model Predictive Variable Impedance Control QLMPVIC assist operator physically interacting collaborative robot Based Cartesian impedance control providing controlled manipulator compliant decoupled behavior Carte sian space MPC designed order online optimize parameters setpoint damping parameters assist user detected intended motion directions maximizing collaboration performance The MPC exploits learned humanrobot interaction dynamics model obtained means ensemble neural networks Therefore lack sophisticated analytical models humanrobot interaction dynamics overcome employing method capable capture complexity uncertainties dynamics An MPC objective function designed order minimize users effort collaboration robot Indeed users intention motion detected making possible assist himher intended directions motion The designed MPC enhanced stability guarantees means Lyapunov constraints In way safetystability issues tackled proposed methodology The MPC online solved making use QLearning method exploiting actorcritic algorithm approximate exact solution The obtained solution accurate highly eﬃcient able tackle issue related computation eﬃciency compromise implementation controller real applications In following Section state art related pHRC control addressed highlight open issues ﬁeld solutions provided proposed approach 12 Related work Among strategies 1112 physical humanrobot collaboration pHRC commonly enabled implementing low level impedance controller 13 provides robot safe compliant behavior suitable interacting surrounding environment including human subjects 14 The impedance control parameters massinertia stiffness damping setpoint tunedadapted means highlevel control strategies execution task 15 achieve humanlike adaptability skills 1617 maximize humanrobot collaboration performance 18 Such highlevel control strategies designed analytical models humanenvironment interaction 19 However solutions realized methods limited speciﬁc modeling adapted impossibility models capture complex interaction dynamics Therefore machine learning MLbased approaches investigated implement ﬂexible controllers Two types MLbased solutions available state art model based ML approaches 20 modelfree ML approaches 21 Modelbased ML approaches provide powerful algorithms control tuning purposes capable capturing complex uncertain interaction dynamics The main drawback strategies consists limited variation task conditions faced proposed controllers In order effective adopted models accurately represent target scenario losing generalizability 22 Modelfree approaches hand allow achieve acceptable results wide set scenarios exploiting autonomous tuning trialanderror However tuning procedures costly terms computational resources time requiring vast trails achieve target performance 4 Many efforts development combined solutions exploiting advantages model based modelfree ML solutions In 23 implemented systematic approach optimize gains admittance controller online prior knowledge target position task characteristics A fuzzy QLearning algo rithm regulate damping robot trajectory approaches minimum jerk cooperation effective The partitioning robot state fuzzy sets eﬃcient method deal curse dimen sionality continuous space However requires number parameters manually tuned works selecting deterministic action small set fuzzy Q values obtain quicker convergence According 24 restricting search optimal action agents action set restricted set Q values myopic idea Herein proposal employ genetic algorithm stochastic optimizer action selection stage Fuzzy QLearningbased controller The associated increase computational burden mitigated improvement performance In 25 Gaussian Process GP model learned approximation dynamical predict longterm state evolution internal simulation The impedance controller parameters optimized exploiting learned modeling However computation minutes suitable realtime implementation 2 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 controller industrial applications Thus suggested approach able provide accurate model interaction dynamics slow large data loss functions In addition modelbased approaches generally perform better assumption smooth continuous dynamics far observed humanrobot collaboration tasks 20 Due described reasons authors focused approximating humanrobot interaction simpliﬁed models deﬁned priori 26 based research 27 shows humans central nervous follows latent desired trajectory regulated impedance control scheme In paper authors model dynamics human arm approach estimat ing impedance parameters desired trajectory GPs Instead 28 exploits theoretical study arm movements operators involved dyadic manipulation task The resultant interaction model able estimate interaction force reaching movement function measured force applied human With strategies possible design variable impedance controllers realtime However methods able capture real nonlinear dynamics humanrobot interaction In 29 proposed novel framework exploits electromyography EMG dynamic manipulability measurements human arm provide robot information human motor behavior task requirements Through information robot adapt behavior phasedependent regulation trajectories force impedance selfcomplyingstiffening provide humanlike complementary assistive response This method proves intuitive effective requires special measurement implemented adapts impedance control stiffness predeﬁned range In addition parameters speciﬁed oﬄine deﬁne task expected behavior robot In 30 motion intention human deﬁned desired trajectory employed human limb model estimated radial basis function neural networks RBFNN The estimated motion intention integrated impedance control robot actively following human partner However small datasets RBFNN suffer overﬁtting generating errors estimation setpoint Moreover impedance parameters adapted obtain compliant behavior 20 focused estimating nonlinear dynamics humanrobot interaction An ensemble Artiﬁcial Neural Networks ANNs learn interaction dynamics model design ModelBased Reinforcement Learning MBRL variable impedance controller pHRC ANNs trained oﬄine embed HRC dynam ics As data problem overﬁtting ANN recur proposed approach uses ensemble ﬁve ANNs overcome issue allowing capture uncertainties real HRC dynamics improving modeling performance prediction purposes The trained model kept updated execution collabo rative task This model Model Predictive Controller MPC Cross Entropy Method CEM realtime optimization computation performed control step impedance control parameters damping stiffness parameters considering objective minimization human effort HRC task execution minimizing force applied human robot However proposed approach drawbacks The impedance control setpoint update law predeﬁned optimization considered Furthermore number samples iterations CEM limited requirement conclude computation speciﬁed time enforced control frequency Moreover constrains deﬁned MPC enforce stability The perfor mance controller strictly dependent weights cost function setpoint update function theoretical guarantee stability As highlighted discussion major open issues design pHRC controllers related stability guarantees Stability imposed including appropriately designed terminal cost andor terminal constraints control problem deﬁnition 31 inherited Control Lyapunov Functions CLFs CLFs commonly synthesize stabilizing controllers 32 However despite optimizationbased formulation fail achieve longterm optimal behavior This deﬁciency arises fact cost optimization problems fails incorporate future behavior instead pointwise optimal 33 In contrast Nonlinear Model Predictive Control NMPC emphasizes performance solving optimal control problem online despite additional assumptions met certify closed loop stability Thus integration control methodologies extensively discussed literature try combine beneﬁts compensate drawbacks Lyapunov methods construct stabilizing terminal conditions NMPCs 34 analyze stability absence thereof 35 Another approach incorporates stability condition required CLF prediction horizon NMPC 36 As noted 36 approach desirable properties absence terminal cost stability horizon length In 37 focus practical computational aspects uniﬁcation methodologies control robotic platform The performed tests combination leads improved performance CLF methods signiﬁcantly reducing tuning prediction horizon length terminal conditions NMPC methods However integration reported far considers known simple dynamics proposing algorithms suitable humanrobot interaction problem In fact conventional MPC algorithms encounter problems related large computational burden caused high nonlinearity considered dynamics requiring accurate models An interesting implementation industrial controller based MPC stabilized Lyapunov constrains described 38 The controller applied continuous nonlinear complex dynamics The accurate mathematical model hard obtained To solve issue modeling realized exploiting Reinforcement Learning RL technique approximate solution Lyapunov MPC LMPC Speciﬁcally RL algorithms QLearning ANNs exploited 3 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 approximate agent Qfunction deal continuous state space Thus computational eﬃciency control law guaranteed regardless complexity dynamics The reviewed updating strategies online tuning impedance control parameters pHRC context characterized diﬃculties simultaneous optimization impedance parameters The online opti mization setpoint impedance parameters fact important obtain active targetoriented compliant behavior manipulator pHRC task In addition commonly diﬃcult ensure stability op timization problem Moreover reliable model target dynamics available making diﬃcult optimize collaboration Besides reviewed strategies based internal simulation state evolution exploiting prediction capabilities dynamic models Qfunctions However accurate eﬃcient implemented real applications execution usually approximated fuzzy logic requiring precise setting fuzzy rules membership functions solve curse dimensionality problems 13 Paper contribution The aim paper design variable impedance controller guaranteed stability pHRC applications This controller capable modeling complex humanrobot interaction dynamics exploiting online opti mization lowlevel impedance control parameters The employed optimization methodology similarly strategy described 20 based resolution MPC The MPC objective function designed minimize users effort interaction manipulator simulating evolution ensemble artiﬁcial neural networks ANNs In fact GP models lose eﬃciency high dimensional spaces 25 ensemble ANNs represents interesting choice accurately model unknown dynamics making possible capture uncertainties prediction The MPC enhanced stability guarantees means Lya punov constraints The MPC solved online means QLearning method uses actorcritic algorithm approximate exact solution The proposed controller designed ﬁx following main open issues present reviewed state art approaches especially taking consideration controller developed 20 authors computational burden ii lack demonstrated stability closed loop solution iii performancedependence weights cost function setpoint update strategy tackled avoiding solve MPC problem CEM algorithm control step A Qlearning algorithm described 38 instead Based measured state manipulator Qlearning algorithm embeds neural networks returns estimation optimal impedance parameters computed MPC lower computational requirements After exploration phase Qfunction learns utility stateaction tuples allowing actor choose desirable control policy according speciﬁed targets During training phase actor lighter CEM algorithm solve receding horizon control problem update actor network In way prediction capability ANN exploited estimate state evolution obtain ﬁnal result accurate obtained selecting action associated highest Qvalue In fact selecting action highest Qvalue account model uncertainty proposed implementation ii solved combining MPC Lyapunovbased controller obtained pointwise mininorm method 33 introduces speciﬁed constraints control step These approaches suboptimal strategies solve optimization problem proper technical conditions equivalent respectively EulerLagrange optimization solution HamiltonJacobiBellman equation 36 Thus proposed approach prac tical viewpoint provides guaranteed stability properties CLFs methods online optimization capabilities performance receding horizon controllers Since Lyapunovbased pointwise mininorm controller solves inverse optimal control problem tunes choice best control action weights performance index Lyapunov functions In combination MPC leads reduce inﬂuence cost weights ﬁnal control action 37 tackling iii This issue addressed considering setpoint Cartesian impedance control damping parameters optimization variables avoiding need manually tune setpoint control law related gains shown 20 Remark 1 The authors developed proposed QLMPVIC working controlaﬃne model robot assuming humans interaction affect property The authors conﬁdent operator modify actuation law controller designed reduce hisher effort heshe interacting robot endeffector This modeling choice common literature nonaﬃne systems considered control clearly nonlinear As matter example considered aircraft dynamics underwater vehicles active magnetic bearings electromagnetic levitation 39 4 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 The proposed controller validated experimental tests considering pHRC application Franka EMIKA panda robot test platform Two scenarios considered interaction hu man robot endeffector tool robot endeffector ii interaction human sealing gun installed robots endeffector A total 20 subjects 10 subjects scenario involved experimental campaign In scenarios user asked interact robot controlled z vertical direction To assess performance proposed controller compared modelbased reinforcement learn ing variable impedance controller previously developed authors 20 The pHRC performance validated basis questionnaire described 420 Achieved results highlight improved performance pro posed controller wrt 20 Furthermore training process proposed controller validated order verify applicability real industrial context speciﬁc operator user trains separate controller 5 additional users asked perform training controller ease procedure customizing controller speciﬁc user Achieved results effectiveness training process Finally proposed approach applied complex usecases collaborative assembly task collaborative deposition task order applicability real industrial tasks 14 Paper layout The paper structured follows In Section 2 detailed description control methodology proposed The low level impedance controller introduced Section 21 Then pHRC dynamics detailed Section 22 The description adopted methodology online modeling estimation pHRC dynamics provided Section 23 The Lyapunovbased MPC detailed Section 24 The Qlearning methodology stated Section 25 actor critic implementation Section 26 The CEM algorithm described Section 27 Finally complete QLMPC control framework shown Section 28 Section 3 provides achieved experimental results demonstrating improved pHRC performance wrt controller proposed 20 Finally Section 4 contains conclusions future work 2 Methodology The proposed QLearningbased Model Predictive Variable Impedance Control QLMPVIC pHRC tasks main levels In ﬁrst lowlevel control loop variable Cartesian impedance controller realized outer highlevel controller work considering manipulator decoupled massspringdamper Cartesian space The outer highlevel controller update setpoint damping parameters inner controller order optimize pHRC performance minimize interaction force human robot operators effort The outer highlevel controller composed actor critic ANN implements QLearning algorithm resolution nonlinear optimal control problem An ensemble ANNs ex ploited estimate model An MPC enhanced stability guarantees means Lyapunov constraints implemented online compute lowlevel impedance control parameters maximizing pHRC performance Fig 1 shows proposed QLMPVIC schema highlighting element composing proposed methodology In following elements composing QLMPVIC lowlevel Cartesian impedance control humanrobot interaction dynamics modeling estimation methodology based ensemble ANNs LMPC Qlearning methodology actorcritic ANNs CEM algorithm described 21 Cartesian impedance control As described 40 Cartesian impedance controller designed perform compliant task providing reference inner position controller On basis interaction force acting manipulator impedance control p related translational degrees freedom allows calculate robot accelerations ximp p ϕcd DoFs ϕcd related rotational DoFs described intrinsic Euler angles representation 1 p M t 1 M ϕ ϕcd Dtcid2 p Kt cid2p ft cid2 Dϕ ϕcd ST Kϕ ϕcd ωϕcd τ ϕ cid3 1 Considering translational impedance control Mt target mass matrix Dt target damping matrix Kt target stiffness matrix ft external forces vector p actual Cartesian positions vector cid2p p pd cid2 p p pd pd target positions vector pd target velocity vector Considering rotational impedance control Mϕ target inertia matrix Dϕ target damping matrix Kϕ target stiffness matrix ϕcd set Euler angles extracted Rd d Rc describing mutual orientation compliant frame Rc c coincident robot endeffector reference frame target frame Rd τ ϕ external torques vector referred target frame Matrix Sωϕcd deﬁnes transformation derivatives Euler angles angular velocities ω Sωϕcd ϕcd 41 The DoFs impedance control results RT 5 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 1 Overall QLMPVIC scheme The training operations inside dashed square performed buffer described section 26 leading update QLMPC controller regularly sends optimal setpoint damping parameters impedance controller The optimization variables Cartesian impedance control setpoint xd damping Dr control input u computed basis robot state z Mr xC Dr cid2xC Kr cid2xC fext 2 Mr Dr Kr impedance diagonal matrices composed translational rotational parts cid2xC xC C DoFs position reference impedance controller cid2xC xC xd xd cid2p ϕcd fext ft ST Kr Mr ζ r damping ratio diagonal matrix cid2xt cid2xr xd ωϕcd τ ϕ It underlined damping matrix computed follows Dr 2ζ r 22 pHRC model description The proposed QLMPVIC designed pHRC tasks operator considered contact manipulator motion required Thus pHRC dynamics considered result interaction agents robot human described following model Mtot x Dtot x Ktotx xd Khxd x 3 2 modiﬁed introducing operators dynamics additional unknown contributions Mh total mass Mr Mh Mtot additional unknown contribution Dh total damping Dr Dh Dtot additional unknown contri bution Kh total stiffness Kr Kh Ktot Considering operator wants reach certain target position xd external wrench fext human applied wrench fh applied proportion actual error xd x human stiffness Kh As proposed paper highlevel controller optimize impedance control setpoint xd refers variables object optimization follows operators target position xd impedance control damping Dr order achieve smooth motion task execution The stiffness parameter impedance control considered optimization purposes In fact optimization affect achieved collaboration signiﬁcantly 6 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 simultaneous optimization impedance control setpoint stiffness necessary multiplied robot dynamics 2 Therefore ﬁnal controlled ideally behave follows Mtot x Dr Dhx Ktot x xd fh 0 fh Kh cid5 cid4 xd x 4 The external force fh applied human included state The state space representation written follows cid10 1 tot Dr Dh dx dt Ktotx xd fh cid11 M dx dt x dx dt fh Kh cid2 dxd dt dx dt cid3 5 For sake simplicity Cartesian impedance control decouples Cartesian degrees freedom DoFs considered DoF following analysis In addition translational DoFs considered paper common management DoFs pHRC wrt rotational DoFs 9 The dimensional translational DoF written follows dx dt Ktot Mtot x fh Mtot Ktot Mtot xd Dr Mtot dx dt Dh Mtot dx dt x dx dt fh Kh cid2 cid3 dxd dt dx dt The formulation rewritten zt fzt g1ztu1t g2ztu2t zt x x fhT Z R3 U ui Rcid6umin ui umax u1t U 1 u2t U 2 6 7 optimization variables seen input u state z represented robot position x robot velocity x external force fh Since parameters describing human dynamics dependent characteristics interaction varying task execution outer controller access known dynamics modeling Therefore functions f g1 g2 nonlinear need estimated updated periodically In order simplify considered problem losing generality possible consider target position xd piecewise constant function In fact reduced bandwidth human motion wrt lowlevel robot controller Hz kHz 20 reasonable consider dxd 0 In addition important underline considered dynamics controlaﬃne Besides assumed fzt gizt smooth vector ﬁelds origin equilibrium point unforced nominal implies f0 0 Moreover assumed state z considered sampled synchronously wrt inputs control variables time instants state measurement samplings taken indicated time sequence tk 0 tk t0 kcid2 k 0 1 2 t0 initial time cid2 sampling time dt 23 Model estimation To implement proposed modelbased reinforcement learning approach unknown dynamics pHRC detailed previous Section needs modeled To avoid introducing relevant errors delicate modeling phase better avert use simpliﬁed models 2628 A possible solution issue adoption Artiﬁcial Neural Networks ANNs modeling pHRC dynamics accurate eﬃcient way However ANNs tend overﬁt regions state space training data To solve issue possible use ensemble ANNs order capture model uncertainties data available 42 In fact ANNs agree regions data available disagree regions data For reason dynamics uncertainties captured considering ensemble prediction CEM algorithm online resolution MPC Lyapunovbased controller Additionally ANNs easily scale data thanks parallelization modern GPUs This functionality exploited present work objectives paper reduce computational burden order algorithm feasible implementation pHRC tasks moderate computational power The following procedure implemented order train update ensemble ANNs 7 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Table 1 Parameters CEM ANNs Parameter Value Hardware conﬁguration Training buffer size Model Approximators Number hidden layers Number hidden units Size ensemble Activation function Learning rate Actor Optimizer Number hidden layers Number hidden units Activation function Learning rate Critic Optimizer Number hidden layers Number hidden units Activation function Learning rate CEM Discount factor γ No samples No iterations No elites Prediction horizon 1 NVIDIA GPU 12 CPU cores 5 5 512 2 ReLU 1e3 Adam 3 64 ReLU 1e4 5e5 SGD 3 128 ReLU 1e3 09 16 3 4 7 1 The parameters network architecture taken 42 ensemble size testing multiple values 2 5 simulation experimental tests 20 The parameters networks listed Table 1 2 The networks learn target dynamics fzt ut predicting change state time step duration cid2 zttcid2 zt fzt ut 8 This choice fact control input little effect output successive states zt zttcid2 similar 43 3 The inputs outputs normalized mean standard deviation The initial values mean standard deviation variable roughly estimated knowledge target task During ﬁrst run actual robot variables measured update previous estimation Data normalization highly impacts way neural network interprets different states manipulator crucial obtain good performance reducing distribution shift training interaction samples 4 The ensemble ANNs updated periodically adapt new human physiological parameters measured data task execution In way adaptability approach improved operator changes hisher collaboration modalities robot heshe changes target task gets tired In particular period deﬁned actual state ˆz x x fh measured stored buffer At end period values compared output neural network z em ˆztk ztk Em 1 2 cid6emcid62 A stochastic gradient descend method update layer weights Thus proposed ANNs able provide model function fzt ut represents controlaﬃne fzt ut fzt g1ztu1t g2ztu2t 9 10 However general model function deﬁnition Lyapunovbased controller adopted stabilize MPC Indeed important obtain speciﬁc functions f g1 g2 instant f evaluating fzt ut inputs null fzt 0 0 fzt g1zt0 g2zt0 fzt 11 8 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Function g1 obtained subtracting f fzt ut evaluated u1 1 u2 0 fzt 1 0 fzt fzt g1zt1 g2zt0 fzt g1zt Function g2 obtained subtracting f fzt ut evaluated u1 0 u2 1 fzt 0 1 fzt fzt g1zt0 g2zt1 fzt g2zt 12 13 These functions f g1 g2 Sections guarantee stability controlled 24 Lyapunovbased model predictive control The MPC online control methodology time step tk actual state ztk controlled sampled initial state optimal control problem The solved open loop speciﬁed time horizon T tk tk1 tk2 tkN series optimal control actions uk uk1 uk2 ukN returned output Only ﬁrst term series input control action applied time step controlled evolves new measured state starting point optimization problem While controller work indeﬁnite time deﬁned time horizon limited T Moreover controller subjected constraints input open loop behavior evaluated step differ closed loop case model perfect In addition instability occur The stability feasibility solution optimal control problem enforced introducing proper terminal cost terminal constraint Other approaches literature particular considering Lyapunovbased Model Predictive Controllers stability robustness properties inherited Lyapunovbased controller combining online optimization performance properties receding horizon method 4445373638 Thus suboptimal strategy providing excellent results obtained resolution reduced complexity optimization problem Considering previously deﬁned nonlinear continuoustime 7 optimal control problem solved openloopEulerLagrange fashion adopting receding horizon control technique produce state feedback control law However approach optimal control problem framework original objective function adapted consequent modiﬁcation problem follows cid12 0 tkNcid12 rdτ cid9 rdτ ϕztkN f ϕ cid9 V tk 14 represents value function obtained optimization original cost functional r instan V taneous cost function ϕ terminal weight Moreover solution obtained locally optimal To solve issues complex computations referring 36 possible generate inverse optimal control law h given properly chosen CLF V L pointwise minimization control effort hT h qz min h st V Lz z fz g1zh1 g2zh2 σ z 15 16 σ z continuous positive deﬁnite function q R3 R continuously differentiable function q0 0 qz 0 z cid9 0 Since optimization limited current time step state z known control action h imposed Thus presence term qz pointless optimization purpose The resulting Lyapunovbased controller proper technical conditions 36 equivalent globally optimal solu tion HamiltonJacobiBellman equation Extending pointwise controller receding horizon methodology objective function account state z constraint 16 extended time steps inside horizon Thus optimization problem tkNcid12 rzτ uτ dτ u U min uScid2 tk st zt fzt g1ztu1t g2ztu2t ztk ˆztk V Lz z fz g1zu1 g2zu2 σ z 9 17 18 19 20 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Scid2 family piecewise constant functions sampling period cid2 N prediction horizon r T zt predicted trajectory open loop ˆzt cost function problem set fh fh state measured time tk The proposed methodology interpreted conceptual blend HJB EulerLagrange philosophies N 0 equivalent pointwise mini norm problem N goes original optimal control problem In case ﬁnite N stable solution chosen conditions 36 respected optimal controller recovered case ﬁnite time horizon Once controller h computed better rearrange constraint 20 way associated limits control action clearly emerge V Lz z fz g1zu1 g2zu2 V Lz z fz g1zh1 g2zh2 21 The existence controller hz h1z h2zT provides stability equilibrium point origin satisfying input constraints states inside given stability region generally guaranteed associated property stabilizability 46 According Artsteins theorem 47 dynamical differentiable Control Lyapunov Function CLF exists regular global asymptotically stabilizing feedback Thus given set class K functions αi 1 2 3 4 CLF V Lz inverse optimal control problem surely solved Deﬁnition 21 A class K function continuous function α 0 R 0 α0 0 α strictly monotonically increasing denoted α K If limr αr α said class K function 37 Deﬁnition 22 A CLF continuously differentiable function V L z following inequalities valid 48 α1z V Lz α2z fz g1zh1 g2zh2 α3z V Lz z inf h cid13 cid13 cid13 cid13 V Lz z cid13 cid13 cid13 cid13 α4z 22 23 24 z O z R2 O z open neighborhood origin We V L weak CLF inequality 23 nonstrict V Lz z inf h fz g1zh1 g2zh2 0 25 The existence weak CLF contemplated Artsteins theorem doesnt guarantee global stabilizability existence CLF Nevertheless cases weak CLF design globally stabilizing control law case present work To deﬁne inverse optimal control problem speciﬁed 1516 necessary choose CLF V L function σ z The selected order obtain pointwise mininorm Sontag controller 49 provides smooth real analytic feedback stabilizer generic aﬃne Denoting Lie derivatives V L az V Lz fz b1z V Lz g1z b2z V Lz g2z βz 2cid14 i1 b2 z σ z chosen cid15 σsz a2 qββ 2 yield resulting control law cid16 hi bi hi 0 cid17 a2β2 qβ β β cid9 0 β 0 10 26 27 28 29 30 31 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 If point CLF V L chosen shape lines optimal value function V technical conditions obtain inverse optimal controller satisﬁed Thus V L corresponds solution HJB equation globally optimal 50 The obvious candidate CLF quadratic function includes fundamental variables measure distance researched point So deﬁned V L 1 2 xP 1 xT 1 2 fh P 2 f T h 32 P 1 P 2 weights select signiﬁcance term For choice previously deﬁned Lie derivatives expressed az V Lz fz P 1 x f1 P 2 fh f3 P 1 x cid18 Dh Mtot x Ktot Mtot x fh Mtot cid19 P 2 fh Kh x b1z V Lz g1z P 1 x g11 P 2 fh g13 P 1 x Ktot Mtot b2z V Lz g2z P 1 x g21 P 2 fh g23 P 1 x x Mtot βz 2cid14 i1 z P 2 b2 1 x2 cid20 2 Ktot Mtot x Mtot cid21 2 33 34 35 36 However way resultant control law h continuous In fact shape qβ example set β 2 49 respect small control property Thus continuity proven general necessary look conditions β 0 x 0 understand stabilizer behaves Considering unknown human parameters constant z 0 noticed az 0 z2 b1z 0 zi βz 0 z2 b2z 0 z2 h1z b1 h2z b2 cid17 cid17 a2 β 4 β a2 β 4 β 0 zi 0 z2 Indeed assumption Lyapunovbased controller results continuous z 0 However known human impedance parameters Mh Dh Kh actually nonlinear functions state previous statement valid necessary Dh Since Mh considered HRC scenario assumption reasonable controller considered continuous point Instead x 0 β 0 x fh cid9 0 assuming impedance parameters vary considering xd 0 fh Khx P 2 cid14 P 1 actually implemented noticed diverge inﬁnity z 0 diverge slower 1 zi 1 Mh Ktot Mh x 0 x 0 az 0 b1z 0 b2z 0 βz 0 x 0 x 0 az 0 b1z 0 b2z 0 βz 0 x x x2 x2 x x x2 x2 x 0 x 0 az 0 b1z 0 b2z 0 βz 0 x 0 x 0 az 0 b1z 0 b2z 0 βz 0 x x x2 x2 x x x2 x2 11 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 x 0 x 0 h1z b1 h2z b2 x 0 x 0 h1z b1 h2z b2 x 0 x 0 h1z b1 h2z b2 x 0 x 0 cid17 cid17 a2β4 β a2β4 β cid17 cid17 a2β4 β a2β4 β cid17 cid17 a2β4 β a2β4 β cid17 cid17 a2β4 β a2β4 β h1z b1 h2z b2 0 quicker x 0 quicker x c 0 x c 0 x 0 quicker x 0 quicker x Fig 2 First component Lyapunovbased controller h1 constant values human parameters The Figure underlines discontinuity control action x 0 exception origin c ﬁnite value For reason points Lyapunovbased controller continuous However remains piecewise continuous inside family feasible input actions proposed Fig 2 Fig 3 shows components Sontags controller considering feasible values human parameters 51 Considering analysis resulting closed loop obtained application Sontags stabilizer shows nonsmooth dynamics handled classical existence theorems ordinary differential equa tions requiring vector ﬁelds Lipschitz continuous Instead analyze reference work Filippov 52 needs developed solution concept differential equations righthand sides required Lebesgue measurable state time variables Deﬁnition 23 Filippov Considering vector differential equation x f x t measurable locally bounded vector function x called solution t0 t1 x absolutely continuous t0 t1 t t0 t1 x K f x t K f x t cid22 cid22 δ0 μN0 f Bx δ N 12 37 38 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 3 Second component Lyapunovbased controller h2 constant values human parameters cid23 μN0 denotes intersection sets N Lebesgue measure zero represents convex closure set Bx δ ball radius δ centered x K f x t multivalued map able associate multiple values couple x t Through introduction differential equation generalized differential inclusion conceptually possible deal discontinuous x In deﬁnition important sets measure zero discarded This technical allows obtained solutions deﬁned points vector ﬁeld deﬁned interface regions piece wise deﬁned vector ﬁeld Theorem 21 Chain rule Let x Filippov solution x f x t interval containing t V Rn R R smooth function generally dependent time Then V xt t absolutely continuous V xt t V x t d dt V x t cid25 cid24 cid24 V x V t K f x t 1 cid25 39 40 V x represents gradient function V respect vector x V t stands partial derivative V respect t The demonstration provided 53 The deﬁnition set value map V x t necessary study equilibria differential inclusions generalized Lyapunov criterion deﬁned demonstrated 54 Theorem 22 Lyapunov stability Let x f x t essentially locally bounded 0 K f x t region D x Rn cid6 x cid6 δ tt0 t Also let V Rn R R regular function satisfying V 0 t 0 0 α1cid6 x cid6 V x t α2cid6 x cid6 f x cid9 0 D α1 α2 class K Then V x t 0 D implies xt 0 uniformly stable solution V weak CLF 1 2 addition class K function α3 D property V x t α3cid6 x cid6 0 solution xt 0 uniformly asymptotically stable V CLF 13 41 42 43 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 For purposes present work set V Lx t V Lz fz g1zh1 g2zh2 deﬁned cid16 cid17 V Lx t 0 a2 β 2 qβ 0 β cid9 0 β 0 44 chosen V L weak CLF doesnt guarantee existence stabilizing controller Nevertheless solution accepted proceeding inverse optimal design global asymptotic stability proven extension LaSalles theorem originally demonstrated continuous differential equations Theorem 23 LaSalle Let cid12 compact set Filippov solution autonomous x f x x0 xt0 starting cid12 unique remains cid12 t t0 Let V cid12 R regular function v 0 v V Deﬁne cid13 x cid120 V Then trajectory cid12 converges largest invariant set M closure cid13 V L Proof Let zt Filippov solution considered starting cid12 Since V L zt absolutely continuous bounded zero V L bounded zero V Lz tends constant t Uniqueness trajectories zt invariant set Moreover implies continuous dependence initial conditions positive limit set L cid12 cid12 closed By continuity V L V Lp p L Given L solutions inside L contained largest invariant set cid13 theorem proved solutions attracted M largest invariant set x 0 53 cid2 Since L dt V L 0 follows L absolutely continuous characterized d invariant 0 V L L cid13 Since L Now set M trajectories consistent differential inclusion K f z t solutions different z 0 remain M equilibrium position considered asymptotically stable M K f m m cid9 0 M 45 In considered M invariant subset characterized x 0 general x fh K f m obtained evaluating K f z t points M Span K f m 0 1 1 Ktot Kh Mtot 0 0 46 Since z 0 point intersection sets possible conclude remains M z 0 Thus designed Lyapunovbased controller h able asymptotically stabilize attract state equilibrium point graphically shown Fig 4 However doesnt guarantee stability implemented zeroorder hold fashion In case result function sampling period cid2 weights P 1 P 2 At point possibility replace receding horizon constraint derived continuoustime implementation 21 variant adapted sampled However substitution leads degradation performance 37 constraint 21 MPC inherit properties Lyapunovbased controller 25 QLearning Reinforcement Learning RL refers actor agent modiﬁes action based stimuli received interacting environment order maximize notion cumulative reward 55 It eﬃcient technique realize optimal control systems high complexity industrial process control chemical process control 5557 Among RL algorithms Qlearning typical method developed Watkins Dayan 58 known action dependent heuristic dynamic programming One advantages Qlearning obtain optimal control policy directly minimizing Qfunction relying information states inputs prior knowledge dynamics An example RL algorithm approximate quasiinﬁnitehorizon nonlinear MPC discretetime nonlinear developed 59 Based 38 predictive controller nonlinear continuoustime systems designed taking consideration pHRC modeling control purposes presented method Due presence sources disturbance large unmodeled variables pHRC controlled modeled Markov decision process MDP The deﬁned 4tuple ηX U F M R M X state space U action space F M X U X state transition function describes process controlled R M represents reward function The function F M modeling evolution state depending actual state input stochastic Thus associates states X certain probability acquired actual conditions The goal RL ﬁnd optimal control policy π X U 14 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 4 Representation vector ﬁeld differential inclusion associated streamlines represent trajectory solution different initial conditions optimizing cumulative total reward function 60 In paper proposed cost function minimized deﬁned integral utility function time instant tk inﬁnity tcid12 R M V ztk rzτ uτ dτ tk 47 utility function r equal previously deﬁned quadratic cost function 17 The control action u belongs piecewise constant functions ut utk t tk tk1 chosen control policy π depending state z For reason V function state The goal RL resolution optimization problem deﬁne series input actions minimizes value function tcid12 V ztk min u tk ψ nrzτ uτ dτ 48 ψ discounted factor Bellman function According Bellman principle reformulated follows tk1cid12 V ztk min u tk ψ nrzτ uτ dτ V ztk1 49 In framework Qlearning reward function optimized expressed function Q dependent present state input action Thus previous expression rewritten tk1cid12 Q ztk utk min u rzτ uτ dτ Q ztk1 utk1 optimal control law u tk tk results u tk arg min u Q ztk utk st ztk1 F M ztk utk 15 50 51 52 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 These equations derive Qlearningbased algorithm obtain optimal control policy LMPC problem iteratively 38 The algorithm implementation related described methodology summarized pseudo code Algorithm 1 QLearning The set U input action optimized deﬁned U U LU LU set inputs respect Lyapunov constraint expressed LMPC described previous Section Enforcing deﬁned Lyapunov constraint algorithm converged policy inherit stability robustness Lyapunov based controller The initialization Qfunction step 4 Algorithm 1 facilitates calculation initial policy following step 5 In case considering learning process iterative Qfunction initialization method affect update convergence Theoretically convergence reached However feasible practical applications limited computing resources realtime computing requirements Indeed cases suboptimal control law obtained ﬁnite number iterations Besides approximating Q function control policy neural networks online update additional ﬁtting error introduced Nevertheless method allows reach high accuracy control 38 Algorithm 1 QLearning 1 Initialize error threshold cid17 0 2 k k0 k0 1 3 4 5 Measure current state vector ztk set 0 Initialize Q 0 0 Calculate initial control u0tk tk1cid12 u0tk arg min u U tk rzτ uτ dτ Q 0 6 cid30 cid30 cid30Q i1ztk ui tk Q ztk ui1ztk cid30 cid30 cid30 cid17 7 Update sequence action policies tk1cid12 uitk arg min u U tk rzτ uτ dτ Q iztk1 ui1tk1 8 Update sequence Qfunctions tk1cid12 Q i1ztk ui tk rzτ ui τ dτ Q iztk1 ui1ztk1 9 Apply ut ui tk t tk tk1 tk 53 54 55 56 In contrast ordinary QLearning algorithm proposed approach agent adopts action policy update Qfunction In literature kind algorithm named SARSA Considering greedy policy adopted considered Qlearning algorithm This peculiarity depends fact proposed pHRC scenario possible identify separate timewindows exploration environment waiting function converge optimal control Indeed best control available time immediately applied The convergence proposed QLearning algorithm need proved Exploiting lemma proved ztk uitk estimated tk ut utk t tk tk1 A similar demon sequence Q i1ztk uitk tends optimal Qfunction Q control policy sequence uitk tends optimal control policy u stration available 38 Lemma 24 Let ai arbitrary control sequence subjected control constraint U The corresponding value function cid18i1 updated cid18i1ztk aitk rzτ aiτ dτ cid18iztk1 ai1ztk1 57 tk1cid12 cid180 0 Then Q i1ztk uitk cid18i1ztk aitk satisﬁed iterative number tk 16 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Proof Note Q i1ztk uitk result minimization optimal input uitk cid18i1ztk aitk achieved arbitrary control input aitk concluded Q i1ztk uitk cid18i1ztk aitk cid2 Lemma 25 Consider Qfunction sequence Q i1ztk uitk deﬁned algorithm corresponding input sequence uitk Then follows Q Q i1 Proof Setting arbitrary control sequence aitk ui1tk cid18i1 cid18i1ztk ui1tk rzτ ui1τ dτ cid18iztk1 uiztk1 tk1cid12 tk tk1cid12 Q i2ztk ui1tk rzτ ui1τ dτ Q i1ztk1 uiztk1 Considering initialized value functions Q 0 cid180 0 0 tk Q 1ztk u0tk cid180 rzτ u0τ dτ 0 tk1cid12 tk 58 59 60 This difference kept Q i1 cid18i terms increased quantity cid31 rzτ uiτ dτ Indeed Q i1 cid18i Knowing Lemma 24 Q cid18i concluded tk1 tk Q i1 cid18i Q Q i1 Q cid2 Lemma 26 Consider Qfunction sequence Q i1ztk uitk deﬁned algorithm There admissible control policy makes control law corresponding state z 0 limited time t N f There exists upper bound Y tk Q i1ztk uitk Q tk Y tk ztk u Proof Let μz arbitrary admissible control policy set ai μztk At point cid18i1ztk μztk expressed cid18i1ztk μztk tk1cid12 rzτ μzτ dτ cid18iztk1 μztk1 tk tk2cid12 rzτ μzτ dτ cid18i1ztk2 μztk2 tk tki1cid12 rzτ μzτ dτ cid180ztki1 μztki1 tk tcid12 rzτ μzτ dτ V ztk tk This valid admissible control policy Substituting general μ u Q ztk implies ztk u 61 possible Q i1ztk u ztk Q i1ztk uitk Q ztk u ztk 62 Besides let μ0 admissible control policy makes control law corresponding state z 0 limited time tN f It optimal 17 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 ztk u Q ztk rzτ μ0zτ dτ tcid12 tN fcid12 tk tcid12 rzτ μ0zτ dτ rzτ μ0zτ dτ tk tN fcid12 tN f 1 rzτ μ0zτ dτ Y tk tk 63 Therefore condition Q i1ztk uitk Q ztk u tk Y tk veriﬁed cid2 Theorem 27 Consider iterative control law ui Q i1 deﬁned Algorithm 1 When ui u Q i1 Q Proof According Lemma 25 deﬁnition optimal Qfunction results ztk u Q tk lim Q i1ztk uitk According Lemma 26 possible write Q i1ztk uitk Q ztk u tk Y tk Thus Q i1ztk uitk Q ztk u tk lim The theorem demonstrated cid2 26 ActorCritic neural networks 64 65 66 The Algorithm 1 implemented online proper threshold cid17 limit number iterations However way constrained minimization problems need solved time instant result processes ineﬃcient computational point view Therefore overcome problem QLearning implemented couple neural networks approximate Qfunction policy π The actor neural network responsible update variable impedance parameters represents action choosing policy QLearning It approximates LMPC control law u LM P C tk ztk processing vector measured state ˆz x x fhT 3 hidden layers return optimized impedance parameters u xd DhT setpoint damping parameters The structure following cid4 cid5 unm tanh A3 σR LU A2 σR LU A1 ˆz b1 b2 b3 67 unm indicates nonnormalized output ANN The couples Ai bi represent linear layers neural network transform input vector matrix multiplication adding bias Since 64 units layer dimensions matrices A1 R643 A2 R6464 A3 R264 b1 R64 b2 R64 b3 R2 The activation function σR LU Rectiﬁed Linear Unit deﬁned follows σR LU x x max0 x Due hyperbolic tangent output constrained range 1 1 denormalized u unm ustd uavr uavr umax umin2 ustd umax umin2 68 69 70 71 These constraints represent saturation limit input good method enforce implemen tation proposed controller real applications 18 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 During training output actor neural network u compared u result minimization Qfunction approximated critic neural network Q utk arg min u U Q ztk utk ea utk utk Ea 1 2 cid6eacid62 72 73 The stand gradient descent algorithm update weight actor network implemented follows j1 A A j la j1 b b j la Ea utk Ea utk utk Ai utk bi la learning rate actor The critic neural network approximate Qfunction general expressed tcid12 Q rzτ uτ dτ tk 74 75 76 h u directly present functional utk approximated actor neural network Since r fh f T However determines future states values fh tk1 onward In fact network needs approximate sum costs associated states probably covered according greedy control policy requires input state zt ut T able estimate future evolution Then 3 hidden layers 128 neurons elaborate state return estimation Qfunction The adopted structure follows Q C3 σR LU C2 σR LU C1zt utT d1 d2 d3 C1 R1285 C2 R128128 C3 R128 d1 R128 d2 R128 d3 R Then approximated error squared error critic network respectively deﬁned ec Ec ec Q tk Q tk Ec 1 2 cid6eccid62 Q result Bellman equation Q tk tk1cid12 rdτ Q tk1 tk 77 78 79 Again variant gradient descent algorithm update weight training critic network schematized follows Q tk Ci Q tk di Ec Q tk Ec Q tk j1 j1 lc lc d C 81 80 d C j j lc learning rate critic 27 Cross entropy method In previous section minimization Q described necessary training actor deﬁne error ea Thus optimization problem introduced solved discrete optimal control problem application input piecewise constant state sampled To choose best input action utk evolution interaction force fh suggested ensemble neural networks time horizon long considered This problem formalized 19 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 uk arg min u U Q zk uk arg min u U N1cid14 k0 r Q zN1 uN1 g2zku2k st zk1 fzk g1zku1k z0 ztk uk U k 0 1 N 1 82 83 84 The exact computation optimum possible high nonlinearity problem uncertainties come modeling In order compute approximated solution ﬁnitehorizon control problem CrossEntropy Method CEM exploited 61 CEM probabilistic optimization longing ﬁeld Stochastic Optimization The strategy algorithm sample problem space approximating distribution suitable solutions This achieved assuming distribution problem space Gaussian sampling distribution generate possible candidate solutions Then distribution updated basis best candidate solutions As algorithm progresses distribution reﬁned focuses area optimal solutions The pseudocode describing implementation given Algorithm 2 Cross Entropy Method Algorithm 2 Cross entropy method 1 Initialize mean μ standard deviation σ stochastic distributions 2 iter 0 1 Nit 3 4 5 6 Sample sequence outputs u j Saturate values limit range k 0 1 N 1 Predict evolution sample j ensemble neural networks z j k 7 8 9 10 11 j j hk N1 k0 r f Q z Evaluate cost j Order samples according cost j Choose best s elite samples Compute mean μcp standard deviation σcp Use update stochastic distributions considering smoothing factor α N1 u j N1 μi1 1 αμi αμcp σ i1 1 ασ ασcp 12 Return best sample u j It important highlight value function minimized considered Bellman equation contributions closest time directly evaluated cost function In way prediction capabilities model approximator exploited estimate state evolution obtain ﬁnal result accurate achieved algorithm relays Qfunction The limit range generated input cut previously deﬁned set U It intersection U includes input actions respect saturation limits set LU include inputs respect Lyapunov constraint expressed LMPC In fact constraints decoupled transferred upper lower bounds input The conditions present deﬁnition Model Predictive Control stricter reformulated cid25 gitkz hiztk V Lztk z 85 1 2 input imposed Simplifying wrt common term possible write V L ztk z R uitk R In way stability respect single single inputs cid24 V Lztk z cid10 cid25 cid24 gitkz uitk cid11 gitkz uitk hiztk uitk hiztk cid10 cid10 V L ztk z V L ztk z cid11 gitkz cid11 gitkz 0 0 Combining limits saturation constraints gets cid10 uitk hiztk umin hiztk uitk umax cid10 V L ztk z V L ztk z cid11 gitkz cid11 gitkz 0 0 20 86 87 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 explicit deﬁnition set U Note case x 0 cid10 V L ztk z cid11 gitkz 0 V Lz z fz g1zu1 g2zu2 V Lz z fz g1zh1 g2zh2 0 Therefore necessary introduce limitations set U 88 28 Online updating QLMPC All ANNs employed controller trained online provide algorithm capability dynamically adapting new data patterns In RL framework online learning approach opposed batch learning allows immediate knowledge transfer agent valuable guide data acquisition convenient direction This method results higher exploitation sensitivity learning parameters verge 62 Besides memory critical tasks preserved data shortly acquisition The design implementation online controller effectively eﬃciently update impedance control parameters proposed considering nonlinear unknown dynamics introduced Fig 1 The training neural networks described Section 26 recalls update sequences Q i1ztk uitk uitk shown Algorithm 1 Therefore proposed QLMPC summarized pseudocode Algorithm 3 fair approximation QLearning networks converged tends result LMPC problem Algorithm 3 QLMPC 1 Initialize weight actor neural network critic neural network 2 Initialize weight neural networks model approximator ensemble 3 k k0 k0 1 4 5 6 Measure current state vector ztk x x fhT Store transition ztk1 utk1 ztk inside buffer D Calculate control law utk equations 67 69 unm tanh cid4 A3 σR LU A2 σR LU A1 ˆz b1 b2 b3 cid5 u unm ustd uavr Apply utk kNh integer Get Nh transitions zn zn1 D n 1 2 Nh Update model approximators transition zn zn1 Nh Use zn compute Q n Use zn1 compute un1 actor neural network Use zn1 un1 compute Q n1 Apply Bellman equation ﬁnd Q n Calculate ec Ec update critic weight transition zn zn1 Nh Find un1 minimizing Q zn1 CEM Calculate ea Ea update actor weight 7 8 9 10 11 12 13 14 15 16 17 18 19 The catastrophic interference issue actorcritic network training addressed adapting learning rate ac cording measured state In way operating scenarios provide nonmeaningful information detected reducing relevance learning As matter example learning rate actornetwork decreased operator robot interact Due limited computational capability Linux rt kernel possible use GPUs possible update networks iteration 6 Hz requiring slow frequency 12 Hz During training record states previously covered zn zn1 performed buffer D size Nh When buffer ﬁlls training phase activated operations inside dashed square Fig 1 performed The data stored buffer provided training framework order acquired It important buffer small size shorten idle time training process beneﬁt online learning properties reduce variance data subsequent training steps Indeed data perceived sampled different distributions sudden variation control policy occur leading unstable behavior This issue tackled designing proper data normalization experimental results assess successful outcome solutions From computational point view training actor expensive algorithm application CEM Therefore converges possible stop training increasing update frequency controller 3 Experimental results In Section experimental validation proposed QLMPVIC pHRC tasks described The proposed troller compared wrt 20 previously developed authors Both controllers 21 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 5 Franka EMIKA panda Robot involved validation experiments Considering experimental scenario user interacts manipulator endeffector Considering experimental scenario ii sealing gun attached robot endeffector user interacts robot Fig 13 implemented tested Franka EMIKA panda robot Fig 5 test platform execution target pHRC task In following considered target pHRC tasks validation purposes ﬁrstly described Then evaluation protocol introduced The participants later detailed Next implementation details QLMPVIC given Finally experimental results shown 31 Target pHRC tasks Two experimental scenarios considered test validation proposed QLMPVIC compared approach 20 interaction human robot endeffector tool robot endeffector Fig 5 ii interaction human sealing gun installed robots endeffector The subjects supposed operate robot vertical direction z proposed QLMPVIC approach 20 activated Two virtual walls z axis set 07 m 03 m implemented order constraint robot motion Each subject performs 10 complete motions deﬁned virtual walls The experimental scenarios tested adopting pretraining proposed algorithm 20 Such pretraining performed authors Andrea Testa The running order control algorithms randomized balance evaluation results In addition training process proposed QLMPVIC approach tested considering experimental scenario asking participants perform training section order evaluate training procedure proposed methodology 32 Evaluation protocol The pretrained controllers QLMPVIC controller 20 evaluated basis qualitative evaluation framework 420 Indeed performance tested control methods assessed questionnaire end evaluation experiments subjects The following metrics 22 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 deﬁned address physical humanrobot interactions pHRI cognitive humanrobot interactions cHRI completely address human expectations task goals 63 naturalness humans overall likability normalcy ease use convenience non complexity operation collabora tion smoothness movement smooth characterized limited jerk effort effort hardship endeavor required achieve performance level satisfying mental physical temporal demand motion nature object velocity acceleration felt human velocity acceleration low high compared human expectation stability presenceabsence oscillations sudden inactivity effects manipulation object structure environment detection intention robot follows human intention accelerating deaccelerating motion performance overall performance lifting object desired position speciﬁed time attempting avoid userunfriendly events Each proposed metrics ranking 0 minimum ranking negative review 4 maximum ranking positive review The ratings assigned completion experiment explored scenario allow order feedback structured framework Ratings based subjective opinion participants gained feeling controllers indicative mainly relative performance As volunteers expert professionals merely curious questionnaire collected collection points view So room reviews built deep knowhow considered closer absolute independent evaluation controllers The training process evaluated order verify usability proposed controller scratch trained speciﬁc user real context application The following criteria considered proposed evaluation userfriendliness training intuitive necessary actions train robot complex andor far ordinary way use robot duration time required achieve acceptable performance controller satisfaction operator able achieve target performance test Each proposed metrics ranking 0 minimum ranking 4 maximum ranking 33 Participants The following subjects involved experimental evaluation different experimental scenarios scenario pretraining 10 healthy subjects 6 males 4 females mean age 31 6 years physical problem scenario ii pretraining 10 healthy subjects 10 males mean age 24 1 years physical problem scenario iii pretraining 5 healthy subjects 3 males 2 females mean age 26 3 years physical problem Prior testing subjects informed evaluation scenario testing procedure 34 Implementation QLMPVIC The code required implement run proposed QLMPVIC detailed instructions available following GitHub repository httpsgithub com Andrea8Testa Laboratorio git The provided readme ﬁle contains precise description hyperparameters deﬁne train ANNs sake repeatability The proposed QLMPVIC implemented Franka EMIKA panda robot exploiting ROS framework The im plementation available Franka torque control based libfranka functionalities exploiting Franka Control Interface FCI making possible send motor signals manipulator realtime 1 kHz The end effector interaction force fundamental evaluation cost function provided converting measurements torque sensors installed vendor actuated joint robot employed experimental evaluation However important underline forcetorque sensor installed endeffector robot joint torque sensors available employed manipulator 23 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 341 Lowlevel Cartesian impedance control The lowlevel Cartesian impedance control runs 1 kHz frequency Franka EMIKA panda robot torque controller parameters imposed follows mass parameters set 10 inertia parameters set 10 m2 translational rotational stiffness parameters set 500 Nm 50 N mrad respectively The damping ration parameters nonoptimized Cartesian impedance control DoFs set 07 The setpoint damping parameters computed online controlled Cartesian direction z exploiting proposed QLMPVIC Cartesian DoF z The Cartesian impedance control implemented exploiting measured interaction wrench fext fh applied human operator provided manipulator making use internal joint torques measurements 342 Lyapunovbased controller As mentioned Section 24 parameters Lyapunov function chosen empirically based experimental testing V L 1 2 P 1 10 xP 1 xT 1 2 5 fh P 2 f T h 4 P 2 4 10 343 ANNs The architecture ensemble model approximators actorcritic neural networks discussed respectively Section 23 Section 26 To update weights critic neural network training SGD algorithm Pytorch 64 Instead Adam algorithm algorithm ﬁrstorder 3 gradientbased optimization stochastic objective functions 65 employed The learning rates set 10 5 actor network according model approximators critic networks goes 10 magnitude measured fh The buffer store transitions required train ANNs size Nh 5 4 5 10 344 CEM The CEM solve discrete optimal control problem section 27 characterized smoothing rate α 09 prediction horizon N 7 Nit 4 Nsamples 15 s 4 elite samples The saturation limits input set cid24 cid25 cid24 cid25 umax 07 50 umin 03 01 The control frequency highlevel QLMPC set equal 6 Hz This value mainly determined number ANNs settings CEM 35 Experimental results The experimental results related humanrobot interaction dynamics modeling comparison proposed QLMPVIC MBRLC 20 evaluation training process proposed controller shown Section 351 Humanrobot interaction dynamic modeling evaluation The accuracy learned humanrobot interaction dynamic model fundamental effectiveness proposed QLMPVIC In fact basis learned model MPC computes optimized impedance control parameters applied humanrobot collaboration task Therefore model affects task dynamics human perception collaboration robot It mandatory proposed approach capable model complex humanrobot interaction dynamics succeed In order validate proposed modeling Fig 6 shows training loss test loss employed ANNs The behavior pretrained model approximators recorded experimental scenario extracting random training testing subsets collected data Two new ANNs trained ﬁrst subset evaluated second Such plots highlight proposed couple ANNs capable model humanrobot interaction dynamics Fig 7 shows comparison measured predicted state variables end aforementioned training highlighting capabilities modeling predict interaction dynamics control purposes In fact seen especially considering force estimation estimation error range modelbased state art observers 6669 importantly shown 20 352 QLMPVIC vs MBRLC experimental scenario Considering experimental scenario deﬁned Section 31 obtained results related ﬁrst performance indicators detailed Section 32 shown Fig 8 tested controllers The overall performance indicator 24 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 6 Training loss red line test loss blue line 2 ANNs For interpretation colors ﬁgures reader referred web version article Fig 7 Comparison measured predicted state variables instead shown Fig 9 tested controllers The controllers achieved similar result wrt stability criterion shown Fig 8e metrics QLMPC controller performing better The important improvements achieved wrt effort criterion shown Fig 8c motion criterion shown Fig 8d naturalness criterion shown Fig 8a In fact novel QLMPVIC perceived lighter responsive MBRLC This result highlighted overlapping evolution interaction force optimized setpoint Fig 10 shows evolutions highlight reactivity proposed controller able quickly adapt Cartesian impedance setpoint humanrobot interaction 353 QLMPVIC vs MBRLC experimental scenario ii Considering experimental scenario ii deﬁned Section 31 obtained results related ﬁrst performance indicators detailed Section 32 shown Fig 11 tested controllers The overall performance indicator instead shown Fig 12 tested controllers The achieved results considered scenario ii aligned ones achieved scenario proposed QLMPVIC outperforms MBRLC In particular considering extra payload attached robot endeffector sealing gun effort criterion highlights better performance proposed QLMPVIC wrt ones MBRLC shown Fig 11c In fact proposed QLMPVIC demonstrated reactive promptly following intended motion operator quickly adapting setpoint applied force decays time instants prompt displacement endeffector 354 QLMPVIC training process validation Considering validation procedure QLMPVIC training process deﬁned Section 31 obtained results shown Fig 14 The involved subjects globally satisﬁed training process QLMPVIC While subjects considered ﬁrst minute training boring controller proved learn quickly improving feedback training All subjects procedure smooth simple cases 5 minutes achieve acceptable performance A completely satisfactory controller behavior commonly reached 10 minutes training 25 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 8 Naturalness b Smoothness c Effort d Motion e Stability f Detection Intention criteria tested controllers scenario 36 Validation usecases Besides performance comparison showing improved achieved performance proposed approach wrt 20 developed controller employed industrial tasks provide insights usage real production scenarios In particular collaborative assembly task gear shaft 5 Fig 13 ii collaborative deposition task 6 Fig 13 b considered target usecases considers robot equipped gripper manipulating gear collaboratively assembled ii considers robot equipped sealing gun operated human deposition path strictly related activities developed H2020 CS2 ASSASSINN 26 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 9 Overall performance criterion tested controllers scenario Fig 10 Interaction force optimized setpoint evolutions considered scenario speciﬁc subject The delay interaction force setpoint represents reaction time manipulator project Both tasks executed demonstrate application robot context programmingby demonstration collaboratively perform The selected tasks allow assessment adoption proposed controller real humanrobot collaborative industrial tasks In fact tasks require smooth humanrobot interaction having robot capable quickly reacting intention motion human order properly execute target task guaranteeing stability interaction In fact instabilities vibrations delays controller reaction cause task failures unacceptable output quality Indeed proposed usecases allow evaluation effectiveness developed controller complex humanrobot collaborative applications A video showing performed assembly task available link httpsyoutube com shorts Mc VMzbGfpA feature share video showing performed deposition task available link httpsyoutube com shorts QQ1hLD9Va6o feature share As highlighted proposed controller allows implementation smooth reactive robot behavior reﬂecting performance evaluation provided previous Sections making possible form complex humanrobot collaborative tasks Thus proposed controller proven applicable real industrial tasks 27 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 11 Naturalness b Smoothness c Effort d Motion e Stability f Detection Intention criteria tested controllers Makita Cordless Caulking Gun mounted manipulator scenario ii 37 Discussion 371 Pros The Lyapunovbased approach addressed lack stability issue performanceweight sensitivity present previous related works 20 As consequence proposed QLMPVIC outperforms approach 20 minimal tuning effort The ensemble ANNs solves challenges related diﬃculty effort required obtaining accurate model nonlinear providing reliable estimation humanrobot interaction behavior The integration CEM Qlearning LMPC problem resolution combines prediction capabilities 28 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 12 Overall performance criterion tested controllers scenario ii Fig 13 The tested assembly task gear assembled shaft b deposition task material deposited highlighted path shown 29 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Fig 14 Results training analysis offered ensemble limited computational burden associated RL techniques In fact controller run Linux rt kernel allow usage acceptable frequency 372 Cons The hardware limitations constraints computational time generated necessity online learning limit number ANNs algorithm handle By reducing dimension model approximator ensemble higher risk overﬁtting shown regions available data performance deterioration beginning training Besides interaction force operators effort evaluation task ergonomics analyses collaboration performance carried 4 Conclusions The presented work paper proposes QLearningbased Model Predictive Variable Impedance Control QLMPVIC assist operators physical humanrobot collaboration pHRC tasks The proposed methodology composed control loops lowlevel Cartesian impedance controller highlevel methodology online optimization impedance control parameters setpoint damping parameters allowing minimize humanrobot teraction force collaboration The outer highlevel controller composed actor critic ANN implement QLearning algorithm resolution nonlinear optimal control problem An ensemble ANNs exploited estimate model The MPC enhanced stability guarantees means Lyapunov straints implemented compute lowlevel impedance control parameters online The proposed QLMPVIC tested assess performance comparing wrt MBRL variable impedance control implemented 20 The proposed QLMPVIC proven improve pHRC performance The training process proposed approach validated showing highquality performance limited training time Two complex usecases collaborative assembly task collaborative deposition task additionally setup applicability proposed approach real industrial tasks Future work devoted increase highlevel control loop frequency imposed equal 6 Hz paper order improve pHRC performance To solve issue parallelization modern GPUs exploited In addition external sensors EMGs exploited better capture humanrobot interaction modeling collaboration performance The proposed strategy adapted terms modeling control objectives applied exoskeleton device Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper 30 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 Acknowledgement The work developed project ASSASSINN funded H2020 CleanSky 2 grant agreement n 886977 The work developed project ExoRescue funded EUREKA Eurostars grant agreement n E115182 References 1 G Fragapane D Ivanov M Peron F Sgarbossa JO Strandhagen Increasing ﬂexibility productivity industry 40 production networks autonomous mobile robots smart intralogistics Ann Oper Res 2020 119 2 S Makris Cooperating Robots Flexible Manufacturing Springer 2021 3 L Roveda N Castaman S Ghidoni P Franceschi N Boscolo E Pagello N Pedrocchi Humanrobot cooperative interaction control installation heavy bulky components 2018 IEEE International Conference Systems Man Cybernetics SMC IEEE 2018 pp 339344 4 L Roveda S Haghshenas M Caimmi N Pedrocchi L Molinari Tosatti Assisting operators heavy industrial tasks design optimized cooperative impedance fuzzycontroller embedded safety rules Frontiers Robotics AI 6 2019 75 5 L Roveda M Magni M Cantoni D Piga G Bucca Humanrobot collaboration sensorless assembly task learning enhanced uncertainties adapta 6 L Roveda B Maggioni E Marescotti A Shahid AM Zanchettin A Bemporad D Piga Pairwise preferencesbased optimization pathbased tion bayesian optimization Robot Auton Syst 136 2021 103711 velocity planner robotic sealing tasks IEEE Robot Autom Lett 2021 7 F Vicentini N Pedrocchi M Beschi M Giussani N Iannacci P Magnoni S Pellegrinelli L Roveda E Villagrossi M Askarpour et al Piros cooperative safe reconﬁgurable robotic companion cnc pallets loadunload stations Bringing Innovative Robotic Technologies Research Labs Industrial EndUsers Springer 2020 pp 5796 8 RR Galin RV Meshcheryakov Humanrobot interaction eﬃciency humanrobot collaboration Robotics Industry 40 Issues New Intelligent Control Paradigms Springer 2020 pp 5563 9 L Roveda S Haghshenas A Prini T Dinon N Pedrocchi F Braghin LM Tosatti Fuzzy impedance control enhancing capabilities humans onerous tasks execution 2018 15th International Conference Ubiquitous Robots UR IEEE 2018 pp 406411 10 A Mauri J Lettori G Fusi D Fausti M Mor F Braghin G Legnani L Roveda Mechanical control design industrial exoskeleton advanced human empowering heavy parts manipulation tasks Robotics 8 3 2019 65 11 E Magrini A De Luca Hybrid forcevelocity control physical humanrobot collaboration tasks 2016 IEEERSJ International Conference 12 A Martinez B Lawson C Durrough M Goldfarb A velocityﬁeldbased controller assisting leg movement walking bilateral hip Intelligent Robots Systems IROS IEEE 2016 pp 857863 knee lower limb exoskeleton IEEE Trans Robot 35 2 2018 307316 13 N Hogan Impedance control approach manipulation 1984 American Control Conference IEEE 1984 pp 304313 14 L Roveda A userintention based adaptive manual guidance forcetracking capabilities applied walkthrough programming industrial robots 2018 15th International Conference Ubiquitous Robots UR IEEE 2018 pp 369376 15 SG Khan G Herrmann M Al Graﬁ T Pipe C Melhuish Compliance control humanrobot interaction Part 1survey Int J Humanoid Robot 11 03 2014 1430001 16 P Liang C Yang N Wang Z Li R Li E Burdet Implementation test humanoperated humanlike adaptive impedance controls Baxter robot Conference Towards Autonomous Robotic Systems Springer 2014 pp 109119 17 C Yang C Zeng C Fang W He Z Li A dmpsbased framework robot learning generalization humanlike variable impedance skills IEEEASME Trans Mechatron 23 3 2018 11931203 power tools Robot ComputIntegr Manuf 68 2021 102084 Robot Syst 13 5 2016 1729881416662771 18 W Kim L Peternel M Lorenzini J Babiˇc A Ajoudani A humanrobot collaboration framework improving ergonomics dexterous operation 19 L Roveda N Pedrocchi LM Tosatti Exploiting impedance shaping approaches overcome force overshoots delicate interaction tasks Int J Adv 20 L Roveda J Maskani P Franceschi A Abdi F Braghin LM Tosatti N Pedrocchi Modelbased reinforcement learning variable impedance control humanrobot collaboration J Intell Robot Syst 100 2 2020 417433 21 S Cremer SK Das IB Wijayasinghe DO Popa FL Lewis Modelfree online neuroadaptive controller intent estimation physical humanrobot interaction IEEE Trans Robot 36 1 2019 240253 22 C Gaz E Magrini A De Luca A modelbased residual approach humanrobot collaboration manual polishing operations Mechatronics 55 2018 234247 2018 2539 23 F Dimeas N Aspragathos Reinforcement learning variable admittance control humanrobot comanipulation 2015 IEEERSJ International Conference Intelligent Robots Systems IROS IEEE 2015 pp 10111016 24 A Kukker R Sharma Stochastic genetic algorithmassisted fuzzy qlearning robotic manipulators Arab J Sci Eng 2021 113 25 C Li Z Zhang G Xia X Xie Q Zhu Eﬃcient force control learning industrial robots based variable impedance control Sensors 18 8 26 JR Medina H Börner S Endo S Hirche Impedancebased gaussian processes modeling human motor behavior physical nonphysical interaction IEEE Trans Biomed Eng 66 9 2019 24992511 27 H Gomi R Osu Taskdependent viscoelasticity human multijoint arm spatial characteristics interaction environments J Neurosci 28 E Noohi M Žefran JL Patton A model humanhuman collaborative object manipulation application humanrobot interaction IEEE 29 L Peternel N Tsagarakis A Ajoudani A humanrobot comanipulation approach based human sensorimotor information IEEE Trans Neural Syst 18 21 1998 89658978 Trans Robot 32 4 2016 880896 Rehabil Eng 25 7 2017 811822 30 Y Li SS Ge Humanrobot collaboration based motion intention estimation IEEEASME Trans Mechatron 19 3 2013 10071014 31 L Grüne J Pannek Nonlinear model predictive control Nonlinear Model Predictive Control Springer 2017 pp 4569 32 WL Ma S Kolathaya ER Ambrose CM Hubicki AD Ames Bipedal robotic running durus2d bridging gap theory experi ment Proceedings 20th International Conference Hybrid Systems Computation Control 2017 pp 265274 33 RA Freeman PV Kokotovic Inverse optimality robust stabilization SIAM J Control Optim 34 4 1996 13651391 34 A Jadbabaie J Yu J Hauser Unconstrained recedinghorizon control nonlinear systems IEEE Trans Autom Control 46 5 2001 776783 35 A Jadbabaie J Hauser On stability receding horizon control general terminal cost IEEE Trans Autom Control 50 2005 674678 31 L Roveda A Testa AA Shahid et al Artiﬁcial Intelligence 312 2022 103771 36 JA Primbs V Nevistic JC Doyle A receding horizon generalization pointwise minnorm controllers IEEE Trans Autom Control 45 5 2000 37 R Grandia AJ Taylor A Singletary M Hutter AD Ames Nonlinear model predictive control robotic systems control Lyapunov functions 898909 preprint arXiv2006 01229 2020 38 H Zhang S Li Y Zheng Qlearningbased model predictive control nonlinear continuoustime systems Ind Eng Chem Res 59 40 2020 1798717999 39 T Binazadeh MA Rahgoshay Robust output tracking class nonaﬃne systems Syst Sci Control Eng 5 1 2017 426433 40 F Caccavale C Natale B Siciliano L Villani Sixdof impedance control based angleaxis representations IEEE Trans Robot Autom 15 2 1999 41 L Sciavicco B Siciliano Modelling Control Robot Manipulators Springer Science Business Media 2012 42 K Chua R Calandra R McAllister S Levine Deep reinforcement learning handful trials probabilistic dynamics models preprint arXiv 289300 1805 12114 2018 43 A Nagabandi G Kahn RS Fearing S Levine Neural network dynamics modelbased deep reinforcement learning modelfree ﬁnetuning 2018 IEEE International Conference Robotics Automation ICRA IEEE 2018 pp 75597566 44 P Mhaskar NH ElFarra PD Christoﬁdes Predictive control switched nonlinear systems scheduled mode transitions IEEE Trans Autom 45 P Mhaskar NH ElFarra PD Christoﬁdes Stabilization nonlinear systems state control constraints Lyapunovbased predictive control 46 M Heidarinejad J Liu PD Christoﬁdes Economic model predictive control nonlinear process systems Lyapunov techniques AIChE J 58 3 47 Z Artstein Stabilization relaxed controls Nonlinear Anal Theory Methods Appl 7 11 1983 11631173 48 D Munoz la Pena PD Christoﬁdes Lyapunovbased model predictive control nonlinear systems subject data losses IEEE Trans Autom Control 49 ED Sontag A universal construction Artsteins theorem nonlinear stabilization Syst Control Lett 13 2 1989 117123 50 RA Freeman JA Primbs Control Lyapunov functions new ideas old source Proceedings 35th IEEE Conference Decision Control Control 50 11 2005 16701680 Syst Control Lett 55 8 2006 650659 2012 855870 53 9 2008 20762089 vol 4 IEEE 1996 pp 39263931 51 D Lakatos F Petit P Van Der Smagt Conditioning vs excitation time estimating impedance parameters human arm 2011 11th IEEERAS International Conference Humanoid Robots IEEE 2011 pp 636642 52 AF Filippov Differential Equations Discontinuous Righthand Sides Control Systems vol 18 Springer Science Business Media 2013 53 D Shevitz B Paden Lyapunov stability theory nonsmooth systems IEEE Trans Autom Control 39 9 1994 19101914 54 M Vidyasagar Nonlinear Systems Analysis SIAM 2002 55 FL Lewis D Vrabie Reinforcement learning adaptive dynamic programming feedback control IEEE Circuits Syst Mag 9 3 2009 3250 56 Q Wei D Liu G Shi A novel dual iterative qlearning method optimal battery management smart residential environments IEEE Trans Ind 57 R Padhi N Unnikrishnan X Wang S Balakrishnan A single network adaptive critic snac architecture optimal control synthesis class Electron 62 4 2014 25092518 nonlinear systems Neural Netw 19 10 2006 16481660 58 CJCH Watkins P Dayan QLearning Mach Learn 8 1992 279292 59 X Xu H Chen C Lian D Li Learningbased predictive control discretetime nonlinear systems stochastic disturbances IEEE Trans Neural Netw Learn Syst 29 12 2018 62026213 60 O Sprangers R Babuška SP Nageshrao GA Lopes Reinforcement learning portHamiltonian systems IEEE Trans Cybern 45 5 2014 10171027 61 J Brownlee Clever algorithms natureinspired programming recipes Jason Brownlee 2011 62 GA Rummery M Niranjan OnLine QLearning Using Connectionist Systems vol 37 Citeseer 1994 63 S Mizanoor Rahman R Ikeura Cognitionbased control optimization algorithms optimizing humanrobot interactions powerassisted object manipulation J Inf Sci Eng 32 5 2016 64 L Bottou Stochastic gradient descent tricks Neural Networks Tricks Trade Springer 2012 pp 421436 65 DP Kingma J Ba Adam method stochastic optimization preprint arXiv1412 6980 2014 66 L Roveda A Bussolan F Braghin D Piga 6d virtual sensor wrench estimation robotized interaction tasks exploiting extended Kalman ﬁlter 67 L Roveda D Piga Sensorless environment stiffness interaction force estimation impedance control tuning robotized interaction tasks Auton 68 L Roveda AA Shahid N Iannacci D Piga Sensorless optimal interaction control exploiting environment stiffness estimation IEEE Trans Control Syst 69 L Roveda A Bussolan F Braghin D Piga Robot joint friction compensation learning enhanced 6d virtual sensor Int J Robust Nonlinear Control Machines 8 4 2020 67 Robots 45 3 2021 371388 Technol 30 1 2021 218233 2022 32