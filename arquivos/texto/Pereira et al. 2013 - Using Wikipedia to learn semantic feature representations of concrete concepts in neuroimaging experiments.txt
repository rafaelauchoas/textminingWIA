Artiﬁcial Intelligence 194 2013 240252 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Using Wikipedia learn semantic feature representations concrete concepts neuroimaging experiments Francisco Pereira Matthew Botvinick Greg Detre Psychology Department Princeton Neuroscience Institute Princeton University Princeton NJ 08540 United States r t c l e n f o b s t r c t Article history Available online 10 July 2012 Keywords Wikipedia Matrix factorization fMRI Semantic features 1 Introduction In paper corpus thousand Wikipedia articles concrete visualizable concepts produce lowdimensional semantic feature representation concepts The purpose representation serve model mental context subject functional magnetic resonance imaging fMRI experiments A recent study Mitchell et al 2008 19 showed possible predict fMRI data acquired subjects thought concrete concept given representation concepts terms semantic features obtained human supervision We use topic models corpus learn semantic features text unsupervised manner features outperform Mitchell et al 2008 19 demanding 12way 60way classiﬁcation tasks We features uncover similarity relations brain activation different concepts parallel relations behavioral data human subjects 2012 Elsevier BV All rights reserved Over years machine learning classiﬁers increasingly demonstrate pattern brain activation measured functional magnetic resonance imaging fMRI contains information stimuli seen subject decisions aspects task performance 918261029 Recently expanded discovering information present encoded testing hypotheses encoding One approach postulate model information created response stimuli learning mapping information brain activation patterns model tested new stimuli building true brain activation patterns known elegant example visual cortex 11 Conversely test models trying reproduce stimulus gave rise brain activation patterns patterns Examples reconstruction simple visual stimulus 20 pattern dots mentally visualized subject 32 producing structural semantic description stimulus scene 23 All examples pertain visual cortex pictorial stimuli models information processing carried visual cortex But model consider interested meaning concept opposed visual representation When considering representation meaning concept someones mind possible view representation semantic features present varying degrees Examples alive versus inanimate manmade artifact versus natural Features shared Corresponding author Email address fpereiraprincetonedu F Pereira 00043702 matter 2012 Elsevier BV All rights reserved httpdxdoiorg101016jartint201206005 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 241 concepts belonging semantic category expect saw hammer share virtue tools A pioneering study 19 showed predict brain activation response line drawing concrete concept noun naming given semantic feature values concept patterns brain activation concepts The authors introduced procedure obtaining semantic feature values text corpus required specifying number verbs computing occurrence nouns naming concepts large corpus Our paper close spirit motivated related questions The ﬁrst discover semantic space represent concrete concepts learning semantic features relatively small corpus Our ﬁrst contribution corpus containing Wikipedia articles deﬁning concepts instances words naming concepts case standard corpora Furthermore use topic models 3 means number features produced principle sidestepping need specify verbs The second question determine corpus reﬂects degree semantic representations concepts mind human subjects fMRI data For use semantic feature representation learned predict semantic feature values brain activation instead brain activation semantic feature values This semantic feature representation decode subjects mental context reveal similarity structure representations related concepts readily apparent solely consider fMRI data 2 Related work There theories semantic information represented brain 22 extensive review Almost theories rely extent notion features attributes particular concrete concept 16 alive wood From perspective features including excluding concepts particular categories judging similarity concepts making semantic judgments conjunction categorical taxonomic structure One way obtaining features painstakingly asking subjects produce different concrete concepts tallying named deemed important distinguishing concepts categories The result known semantic feature production norm 16 This guarantee relevant feature generated fact distinctive likely come problem data available concepts included norm It possible address issue resorting subjects making assumption semantic features distinguish meanings concepts reﬂected usage statistics nouns naming large text corpus This relies notion features shared people thinking concept talking concepts chair table requires common understanding characteristics concept The pioneering paper inspired work 19 uses approach relying assumption Some theories treat semantic knowledge stored amodally independent perception action relevant acquisition use knowledge 6 Others postulate knowledge stored involving sensory functional processing areas furthermore making semantic judgment require retrieval interaction perception situation judgment possibly simulation What peach feel like held What happens dropped 1 Motivated perspective 19 assumed key semantic features meaning concept cor respond basic sensory motor activities actions performed objects actions involving changes spatial relationships They handpicked 25 verbs1 related activities actions computed cooccurrence noun naming concept 25 verbs large text corpus Google ngram corpus httpngramsgooglelabscom The 25 cooccurrence counts concept semantic feature values normalization unit length vec tor The hypothesis underlying procedure 25 verbs good proxy main characteristics concept frequent cooccurrence corresponding noun text means different sources people association mind noun The authors showed features corresponded degree information present brain subject accomplished showing predict brain activation response line drawing concrete concept noun naming given semantic feature values concept patterns activation concepts There multiple approaches learning features text data Latent Semantic Analysis LSA 13 best known tradition perform psychological tasks tests word stimuli 318 21 applications EEG instance This work seen analytically operating wordbydocument matrix derive lowerdimensional vector space simplex words reside excellent review 1 See hear listen taste smell eat touch rub lift manipulate run push ﬁll ride fear open approach near enter drive wear break clean 242 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 Fig 1 A complex pattern activation expressed combination basic patterns related vector space approaches 33 12 shown features similar 6 form conceptrelation feature extracted subset 500 articles Wikipedia concepts study showing addition deﬁnitional text carried information purpose general purpose corpus independently work early version 28 appeared workshop In 7 method extract features British National Corpus data set 14 replicate analysis 19 goal validating features extracted 15 learns semantic features matrix cooccurrences 5000 common words English perform prediction task 19 data set Google ngram corpus This closely related work approach producing semantic features classiﬁcation tasks use different 4 uses semantic features derived 6 feature norm predict fMRI activation The approach deploy Latent Dirichlet Allocation LDA 3 LDA produces generative probabilistic model text corpora document viewed bagofwords words appear matters word drawn ﬁnite mixture underlying set topics corresponds probability distribution vocabulary words known topic model Note LSA LDA modeled documents terms learned semantic features speciﬁc approach models psychological predictions words concepts refer A particularly relevant example use topic models purpose 8 works text corpus containing educational text school grades The authors topic model corpus capable capturing structure human semantic representation evidenced ability predict human subject word association patterns 25 success number tasks involve memory linguistic processing human subjects While work LSA extract representation vector document LDA preferable practical reasons topic probability vectors forced orthogonal restricted add 1 making presence topic detract presence conceptual ones LDA deal certain aspects word association troublesome LSA detailed 8 We analyze data set 19 generously public authors In experiment task trial think meaning given concept seconds seeing line drawing concept noun naming Before experiment subjects asked think certain aspects concept mind properties interaction write corresponding line drawingnoun concept Subjects reported helped reliably think things response stimuli precise notion mental context use The problem want address mental representation concept present brain measured fMRI If accept mental representation composed semantic features envisage decomposing pattern brain activation thinking concept combination basis patterns corresponding key semantic features stimulus This illustrated Fig 1 complex pattern split simpler ones forming basis The value semantic feature indicates degree basis pattern present given values patterns learned fMRI data As described earlier 19 ﬁnds semantic feature values computing 25 cooccurrence values concept This approach limited fact requires stipulating 25 verbs The verbs selected capture range characteris tics described guarantee ones relevant concrete concepts We use topic models learn semantic features text corpus selected purpose Section 3 This happens unsupervised manner need specify verbs proxy indicator The essential characteristic corpus composed Wikipedia httpenwikipediaorg articles concrete visualizable concepts including corresponding 60 19 Articles deﬁnitional style refer concrete concepts edited people contain essential shared knowledge subject article We assumption instance language particularly suitable reﬂect structure real world represented multiple minds 30 conclusively demonstrated A important advantage deﬁnitional articles semantic feature representation article concept topic model topic probabilities document use directly semantic feature representation corresponding stimulus concept This contrast representing words lowdimensional space representation word naming concept representation concept LSA This relieves burden having perform word sense disambiguation 24 generating features word different meanings different documents From 33 perspective deriving document representation termdocument matrix 12 instance wordcontext matrix F Pereira et al Artiﬁcial Intelligence 194 2013 240252 243 In order semantic features capture relevant semantic structure 1 learn mapping feature brain activation pattern 2 classify brain images taken subject sees novel concept training set predicting values semantic features present 3 use model uncover similarity relations brain activation paralleling similarity structures human semantic representations 3 Materials methods 31 Data We use 9 subjects data set 19 The experimental task line drawing concept noun naming seconds thinking properties The stimulus set contained 60 concepts 12 categories animals body parts buildings building parts clothing furniture insects kitchen manmade objects tools vegetables vehicles The experiment 360 trials divided 6 epochs 60 concepts appeared stimuli total 6 presentations concept An example image average images taken 47 seconds stimulus onset trial labels concept category belongs 32 Semantic features The experiments described paper rely different kinds semantic features Wikipedia Semantic Features WSF Google Cooccurrence Features GCF 19 These act lowdimensional representations fMRI data decomposing example constituent basis images To obtain Wikipedia Semantic Features started classical lists words 27 2 modern revisionsextensions 5 34 compiled words corresponding concepts deemed concrete imageable score lists editorial decision We identiﬁed corresponding Wikipedia article titles airplane FixedWing Aircraft compiled related articles linked Aircraft cabin If words original lists multiple meanings included articles meanings suggested disambiguation pages free association including Bear_claw_pastry Claw Bear We stopped process list roughly 3500 concepts corresponding articles We restrict number articles included reasons The ﬁrst hadnt developed good semiautomatic way ﬁnding Wikipedia articles concepts knew concrete visualizable related air plane FixedWing Aircraft Wikipedia want include Airplane Seat Airplane Cabin The sec ond training topic models quickly computationally demanding number articles increased We Wikipedia Extractor2 remove HTML wiki formatting annotations processed resulting text morphological analysis tool Morpha 17 lemmatize words basic stems taste tasted taster tastes word 25 The resulting text corpus processed topic modeling software 3 build LDA models The ar ticles converted required format keeping words appeared articles words excluded resorting custom stopword list We run software varying number topics allowed 10 100 increments 5 setting α parameter topics following 8 range multiples inverse num ber topics yielded comparable experiment results terms peak accuracy albeit different numbers topics Intuitively α controls semantically diverse documents number different topics document tend represented having For given number topics K yielded distributions vocabulary topic vector topic probabilities articleconcept vector lowdimensional representation concept depicted Fig 2A Note probabilities add 1 presence semantic feature trades presence desirable expressing brain image combination basis images weighted features The 60 75 matrix right Fig 3 shows value features 75 topic model 60 concepts considered sorted category A visualization topic representations concepts word distributions associated 40topic model httpwwwprincetonedumatthewbwikipedia The Google Cooccurrence Features semantic features 19 represent given stimulus They obtained considering cooccurrence counts noun naming stimulus concept 25 verbs text corpus yielding vector 25 counts normalized unit length The lowdimensional representation brain image given concept 25dimensional vector The 60 25 matrix left Fig 3 shows value features 60 concepts considered Finally consider question similar GCF WSF representations given GCF feature values examples similar given WSF feature We computed correlation possible pair GCF WSF features 60 concepts shown center Fig 3 Qualitatively speaking half GCF high correlation WSF rest direct counterpart GCF subset WSF The fact 2 httpmedialabdiunipiitwikiWikipedia_extractor 244 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 Fig 2 A The Wikipedia subcorpus transformed article associated vector topic probabilities topic probability distribution words B The 4 stages topic probabilities 1 learning basis images 2 predicting topic probabilities test images 3 classiﬁcation 4 comparing similarity predicted topic probabilities test images concepts This iteration crossvalidation loop example images hammer test set Fig 3 The value semantic features 60 concepts considered GCF 25 verbs left WSF 75 topics right The 60 concepts belong 12 categories arranged sequence 5 animals followed 5 body parts followed 5 buildings Between GCF WSF center matrix correlations pair GCF WSF vectors predicted features concepts remaining features correlate partially WSF suggests sparse correspond groupings examples clearly distinguished GCF F Pereira et al Artiﬁcial Intelligence 194 2013 240252 245 Fig 4 Top An example brain image written row vector combination linear combination row vectors Bottom A data set contains brain images forming matrix X rows examples lowdimensional representation matrix Z Fig 5 Left The semantic feature classiﬁcation experiment requires learning image basis set training examples respective semantic feature representations This predict semantic feature values test set examples classify known se mantic feature values 12 categories 60 concepts Right The voxel classiﬁcation experiment replicates main 19 Semantic feature representations 2 test concepts conjunction image basis learned training set predict respective test examples use prediction 2way classiﬁcation 33 Using semantic features fMRI data Overview Fig 2B shows 4 stages semantic features play role experiments described later These 4 stages place inside crossvalidation loop example images hammer concept left test set example images remaining concepts training set In stage 1 topic probability representations articles corresponding training set concepts example images learn image basis image topic In stage 2 image basis example images test concept predict topic representation example image In stage 3 classify considering predicted topic representation closest topic representation test concept concept classiﬁcation concepts semantic category category classiﬁcation The predicted topic representation stored different purpose stage 4 comparing predicted topic representations examples test set Notation As example 3D image divided grid voxels unfolded vector x entries voxels containing cortex A data set n m matrix X row example vector xi Similarly 19 example x expressed linear combination basis images b1 bK dimensionality weights given semantic feature vector z z1 zK depicted Fig 4 The lowdimensional representation data set X n K matrix Z row semantic feature vector zi corresponding basis images K m matrix B row k corresponds basis image bk shown Fig 4 If referring columns matrices column j X use notation X j The notation x indicates transpose vector x cid3 Learning basis images given matrices X Z left Fig 5 decomposed set Learning prediction j examples X j predicted independent regression problems voxel Z regression coeﬃcients B j values voxel j basis images Predicting semantic feature vector z example x regression problem x regression coeﬃcients cid3 z1 zK cid3 z For WSF prediction semantic feature vector additional constraint values need add 1 probabilities Any situation linear regression unfeasible square matrix normal equations invertible addressed ridge term tradeoff parameter set 1 j values voxel predicted B cid3 cid3 246 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 Fig 6 A comparison performance GCF WSF 10100 topics accuracy category left concept right classiﬁcation tasks Curves average 9 subjects In plot WSF red line GCF blue constant dashed line chance level black constant dotted line These results obtained leaveoneconceptout crossvalidation For interpretation colors ﬁgure reader referred web version article 4 Results 41 Classiﬁcation experiments Classiﬁcation semantic features We like ascertain information category identity stimulus example image coming single task trial We predicting semantic features WSF GCF example classifying category 12way concept 60way As illustrated left Fig 5 training examples semantic feature representation learn set basis images goal reconstructing training examples possible The basis images turn predict semantic feature values test examples determining essence semantic features active test example Classiﬁcation assigning label example original data similar semantic feature values judged correlation We use 60fold leaveoneconceptout crossvalidation testing 6 examples withheld concept perform ing following steps fold 1 training set Xtrain corresponding semantic features Z train select 1000 reproducible voxels learn image basis B information selection criterion 2 use test set Xtest basis B predict semantic feature representation Z pred examples 3 use nearestneighbor classiﬁcation predict labels examples Xtest comparing Zpred example known semantic features Z There semantic feature vector different concept Z This procedure unbiased tested empirically permutation test examples permuted epoch verify accuracy results task situation chance level Fig 6 shows results category left concept right classiﬁcation plot contrasts accuracy ob tained GCF obtained WSF 10100 topics increments 5 averaged subjects We performed persubject comparison number topics testing WSF better GCF deemed paired ttest 005 signiﬁcance level uncorrected In general WSF signiﬁcantly better slightly GCF 00167 category concept classiﬁcation Chance levels categoryconcept tasks 1 12 thresholds pvalue 001 null hypothesis accuracy chance level 0120035 respectively One ask improvement solely ability generate 25 features fact LDA address section 00833 1 60 Voxel selection necessary obtain best results experiment said results chance category task seen Fig 7 The reproducibility criterion identiﬁes voxels activa tion levels training set concepts bear relationship epochs mathematically vector activation levels sorted concepts highly correlated epochs As 19 points expect activation differentially task related uniformly present conditions consistent presentations stimulus concept We chose use 1000 500 producible voxels results somewhat better comparable 2000 voxels legitimate consider sensitive results choice Given reproducibility criterion selecting voxels essentially correlation computation ﬁnd threshold null hypothesis correlation given F Pereira et al Artiﬁcial Intelligence 194 2013 240252 247 Fig 7 Same Fig 6 voxel selection Fig 8 Same Fig 6 statistical criterion determining voxels select subject pvalue Fisher transformation For instance given 59 voxel values compared 6 5pairs runs observed correlation r 01 pvalue 001 true correlation ρ 0 Using threshold gives different number voxels subject ranging approximately 200 2000 results similar obtained 1000 voxels seen Fig 8 Classiﬁcation cooccurrence features learned corpus A reasonable question ask necessary use topic models corpus order learn new features useful classiﬁcation In order test applied approach 19 corpus computing cooccurrence counts 60 nouns naming concepts 25 verbs chose generating normalized features Wikipedia Cooccurrence Features WCF A verb noun deemed cooccur appeared 5 words Table 1 shows results category concept tasks GCF WCF Across subjects performance worse WCF GCF especially category task A possible explanation ngram cooccurrence carry information shown GCF requires large corpus yield reasonable cooccurrence estimates WSF uses cooccurrence multiple words article determining probability distribution topic appears eﬃcient use information available 248 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 Table 1 Classiﬁcation accuracy category 12way concept 60way tasks 25 semantic features 19 GCF 25 semantic features derived corpus approach WCF 9 participants P1 P2 P3 P4 P5 P6 P7 P8 P9 Category task GCF WCF Noun task GCF WCF 0217 0181 0019 0022 0169 0128 0031 0017 0133 0125 0025 0008 0211 0217 0028 0033 0144 0103 0044 0022 0122 0089 0031 0011 0119 0125 0008 0031 0108 0083 0017 0014 0133 0139 0019 0017 42 Comparison lowdimensional representations In previous section concerned accuracy quantitative gauge given lowdimensional representation fMRI data decompose basis images generalization power Intuitively feature representation examples given concept test time allows classiﬁcation basis images predict representation capture patterns brain activation correspond semantic features underpinning representation concept In order analyze model subject data yielded highest classiﬁcation accuracies subject P1 close 4010 categoryconcept prediction WSF model 75 topics A nuanced measure good model assigns semantic feature representations concepts mirror minds subjects In particular interested concepts related practice represented model similar ways In experiment relatedness maps roughly semantic category semantic categories connected way Buildings Building parts While having direct access mental representations concept relatedness consider behavioral data word association norms 25 In particular experiment subjects given cue words asked produce new word associated cue The end result probability distribution associates cue word These data 31 produce lowdimensional Word Association Space WAS word represented vector words similar association patterns placed similar regions space vector distance reﬂects similarity association pattern One way use information purpose compute matrix word similarities summary structure semantic association We selected WAS vectors nouns naming 60 concepts considered computed correlations result shown leftmost column Fig 9 There salient blocks diagonal correspond concepts category similarity stretches categories body parts tools buildings building parts manmade objects yellow orange intense brown The WAS similarity matrix act reference pair concepts related Before considering fMRI data examine similarity structure WSF GCF representations 60 concepts obtained text This shown middle left column Fig 9 WSF GCF For concept plot shows correlation semantic feature vector semantic feature vectors concepts There diagonal blocks representations corresponding withincategory similarity pattern confusion categories similar WSF WAS We priori expectation GCF WSF closer WAS possible nouns naming related concepts appear close verbs giving rise similar feature values These results indicate WSF representation reﬂects relatedness concepts GCF representations similar unrelated concepts indicated density high similarity betweencategories concept pairs Armed examine extent semantic feature values predicted fMRI example GCF WSF image basis similarity structure WAS matrices derived solely text This shown middle right column Fig 9 WSF GCF The matrices contain similarity semantic features predicted concept test set 6 presentations concept test set time examples test set These matrices 360 360 sorted category 6 presentations concept adjacent WSF recovers similarity structure concept seen text fMRI data single trial This indicates basis images learned training set generalize extent help predict topic probabilities new image category level It far clear GCF features recover similarity structure obtained text Finally look directly similarity fMRI patterns different concepts shown rightmost column Fig 9 Each entry matrix similarity example test set examples training set obtained computing correlation 1000 voxels selected The ﬁrst thing worth noting similarity high pairs concepts possibly voxels selected stability informative distinguishing concepts The second matrix looks similar F Pereira et al Artiﬁcial Intelligence 194 2013 240252 249 Fig 9 This ﬁgure compares representations 360 examples 6 presentations concept 60 concepts fMRI average 6 presentations The conceptsexamples plot arranged belonging semantic category appear sequence ﬁrst member sequence labeled category considering examples multiple presentations concept adjacent Left Correlation lowdimensional representation nouns naming concepts Word Association Space derived human subject behavior Middle left Correlation lowdimensional representation concepts WSF75 GCF learned text corpora Middle right Correlation lowdimensional representations predicted test examples WSF75 GCF fMRI data plots 360 360 Right Correlation example image test time examples computed 1000 voxels selected plot 360 360 For interpretation colors ﬁgure reader referred web version article GCF matrix produced fMRI data suggesting case GCF text model inﬂuence representations predicted happens WSF This analysis attempt understanding structure similarity concept representation giving sense echo relatedness concepts measured behavioral data systematic study Although presenting single subject overall results remain similar subjects degrading peak accuracy categoryconcept prediction degrades One conceive measures semantic behavior model generate predictions word association suggested 8 421 Replication 2way classiﬁcation experiment voxel values 19 Classiﬁcation voxel values The main experiment 19 entailed predicting fMRI activation unseen stimuli perform forcedchoice 2way classiﬁcation predicted brain image schematized right Fig 5 Note completely different prediction task described previous section terms predicted fMRI activation semantic features 2way We replicated experiment GCF WSF representations In authors considered 60 average examples stimulus concept averaging 6 pre sentations turn left 1770 possible pairs average examples For leftout pair learned set basis images remaining 58 examples respective GCF representations They GCF representation leftout examples basis generate predicted example These twoway matching task actual average examples left outcome deemed correct predicted image concept closer image corresponding leftout concept concept Note learning basis making prediction entire brain 250 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 Table 2 Results replication leave2conceptsout 2way classiﬁcation experiment 19 For subjects P1P9 GCF represents mean accuracy obtained GCF 1770 leave2out pairs Org mean accuracy reported 19 remaining columns mean accuracy obtained WSF 25 50 75 100 topics GCF Org WSF25 WSF50 WSF75 WSF100 P1 079 083 078 084 088 074 P2 075 076 060 063 073 065 P3 073 078 066 068 078 067 P4 079 072 083 085 087 077 P5 082 078 067 068 074 064 P6 073 085 071 072 074 067 P7 075 073 079 077 079 074 P8 074 068 056 067 071 065 P9 070 082 064 073 074 071 Fig 10 Results replication leave2conceptsout 2way classiﬁcation experiment 19 Average score 9 subjects numbers topics selection 500 stable voxels determined computing reproducibility 58 examples 1770 training sets Table 2 shows mean accuracy 1770 leave2out pairs GCF mean accuracy reported 19 mean accuracy WSF 25 50 75 100 topics 9 subjects Fig 10 shows results averaged subjects We able exactly reproduce GCF numbers 19 despite data preprocessing far ascertain supplementary materials personal communication authors The data preprocessing example mean 0 standard deviation 1 prior averaging repetitions concept subtracting mean average examples We voxel selection procedure 500 voxels code yields voxel ranking ridge regression function 19 mention value ridge parameter λ assumed 1 It takes 75 topics subjects display performance equal better GCF think different aspects models sparse The ﬁrst topics probability close 0 concepts topics represent parts corpus The second concepts tend represented topics category related ones later number topic increases possibly conceptspeciﬁc We believe factors combine fewer 4050 topics basis image learned correspond categoryfocused topic share common activation concepts category conceptspeciﬁc As wont topics available probability close 0 regardless topics model makes sense happen This lead predicted patterns brain activation concepts category similar models 4050 topics Given 2way classiﬁcation task involves matching predicted brain images 2 leftout nouns actual brain images inability predict conceptspeciﬁc brain activation disproportionately affect classiﬁcation predicting topic probabilities brain images category granularity classiﬁcation problems main paper helped In contrast 19 uses fewer features involved representation concept basis images effectively capture conceptspeciﬁc brain activation 5 Discussion As discussed earlier work motivated related questions The ﬁrst learn semantic space represent concrete concepts relatively small corpus corpus contains articles deﬁning concepts The second determine corpus reﬂects degree semantic representations concepts mind human subjects F Pereira et al Artiﬁcial Intelligence 194 2013 240252 251 The fact extract corpus order magnitude smaller 13 8 suggests deﬁnitional text informative That said demonstrated conclusively learning topic models corpus size deﬁnitional document representation associated concept The contrast semantic feature representation concepts 19 obtained considering cooccurrence nouns naming concepts verbs 1 2 3 4 5grams massive corpus In case fact learn indicates power leveraging cooccurrence multiple words document fact topic account large set words Our data set naturally sparse 3500 articles 50 000 words topic easily account words appearing related articles In order tackle second question turned fMRI images obtained subject thought different concepts Together corresponding topic probability representation concepts obtained text derive basis fMRI images corresponding brain activation pattern elicited topic To basis images generalize crossvalidated fashion learn basis images 59 60 concepts fMRI data set The basis images predict topic probability representation example images leftout concept Any evaluation relying predicted representation evaluation basis images generalize new concepts function good test model extent process basis images derive topic probabilities fMRI data predicts topic probabilities similar model We evaluated model quantitatively classiﬁcation tasks concept category individual concept levels category classiﬁcation 12class problem 30 examples class concept classiﬁcation 60class problem 6 examples class Both numbers topics clearly capture category level structure expected substantially easier proved case Performing chance task indicates conceptspeciﬁc information topic probability representation relatively low accuracy 5 subjects indicates absent concepts harder extract fMRI data From qualitative angle wanting consider criterion accuracy looked similarity topic probability representations concepts strength association concepts havioral task As 8 suggests topic model predict associations words directly providing quantitative measure instead scope current paper That said interesting avenue research attempt predict results classical norms rankings concreteness visualizability 275 common items semantic category 234 models subsets results norms This provide completely different source constraints evaluations quality representation embodied model We compared topic model concept representation 19 obvious benchmark For predicting concept representation brain images new concepts approach good better predicting brain images new concepts takes topics reach performance 19 This comparison model performed reasonably Our main showing improvement respect 19 demonstrating approach feasible Given specify verbs obtain semantic features obtain models number topics certain practical constraints lot room improvement One possibility topics correspond semantic features ﬁner grain category An example wood topic place heavy probability words brown grain pine oak In situation far topics spreading probability fewer words concept represented assigning probability topics currently case It conceptually straightforward modify parameters topic models yield characteristics model currently working direction A second possibility fact type model opens door fMRI experiments subject read instead having purely visual stimuli Starting probability assignment topics suggested task topic model provides formal mechanism updating assignment stimulus word read model mental context image image basis The ability read allow experiments metaphor processing given example images concrete concepts underpinning metaphors components meaning possibly abstract concepts straightforward produce topic representation respective Wikipedia articles know useful representations We currently piloting reading experiment validate idea tracking mental context topic model Acknowledgements We like thank Tom Mitchell Marcel Just generously agreeing share data set Dave Blei help building interpreting topic models Ken Norman Sam Gershman discussions presentation material We like thank reviewers constructive feedback Matthew Botvinick Francisco Pereira supported National Institute Neurological Disease Stroke NINDS grant number NS053366 252 F Pereira et al Artiﬁcial Intelligence 194 2013 240252 References 1 LW Barsalou Grounded cognition Annual Review Psychology 59 2008 617645 2 WF Battig WE Montague Category norms verbal items 56 categories Journal Experimental Psychology 80 3 1969 146 3 D Blei M Jordan AY Ng Latent Dirichlet allocation Journal Machine Learning Research 3 2003 9931022 4 Kaimin Kevin Chang Tom Mitchell Marcel Adam Just Quantitative modeling neural representation objects How semantic feature norms account fMRI activation NeuroImage 56 2 May 2011 716727 5 JM Clark A Paivio Extensions Paivio Yuille Madigan 1968 norms Behavior Research Methods Instruments Computers A Journal Psychonomic Society Inc 36 3 August 2004 371383 6 George S Cree Ken McRae Analyzing factors underlying structure computation meaning chipmunk cherry chisel cheese cello concrete nouns Journal Experimental Psychology General 132 2 2003 163201 7 Barry Devereux Colin Kelly Anna Korhonen Using fMRI activation conceptual stimuli evaluate methods extracting conceptual representations corpora Proceedings NAACL HLT 2010 First Workshop Computational Neurolinguistics Association Computational Linguistics 2010 pp 7078 8 TL Griﬃths M Steyvers JB Tenenbaum Topics semantic representation Psychological Review 114 2 April 2007 211244 9 JV Haxby MI Gobbini ML Furey A Ishai JL Schouten P Pietrini Distributed overlapping representations faces objects ventral temporal cortex Science 293 5539 2001 2425 10 J Haynes G Rees Decoding mental states brain activity humans Nature Reviews Neuroscience 7 7 2006 523534 11 KN Kay T Naselaris RJ Prenger JL Gallant Identifying natural images human brain activity Nature 452 7185 2008 352355 12 Colin Kelly Barry Devereux Anna Korhonen Acquiring humanlike featurebased conceptual representations corpora Proceedings NAACL HLT 2010 First Workshop Computational Neurolinguistics Association Computational Linguistics 2010 pp 6169 13 Thomas Landauer Susan Dumais A solution Platos problem The latent semantic analysis theory acquisition induction representation knowledge Psychological Review 104 2 1997 211240 14 Geoffrey Leech Roger Garside Michael Bryant CLAWS4 The tagging British National Corpus Proceedings 15th Conference Com putational Linguistics 1994 pp 622628 15 Han Liu Mark Palatucci Jian Zhang Blockwise coordinate descent procedures multitask lasso applications neural semantic basis discovery Proceedings 26th Annual International Conference Machine Learning ICML09 2009 pp 18 16 Ken McRae George S Cree Mark S Seidenberg Chris McNorgan Semantic feature production norms large set living nonliving things Behavior Research Methods 37 4 November 2005 547559 17 G Minnen J Carroll D Pearce Applied morphological processing English Natural Language Engineering 7 03 2001 207223 18 TM Mitchell R Hutchinson RS Niculescu F Pereira X Wang M Just S Newman Learning decode cognitive states brain images Machine Learning 57 12 October 2004 145175 19 TM Mitchell SV Shinkareva A Carlson K Chang VL Malave RA Mason MA Just Predicting human brain activity associated meanings nouns Science 320 5880 2008 11911195 20 Y Miyawaki H Uchida O Yamashita M Sato Y Morito H Tanabe N Sadato Y Kamitani Visual image reconstruction human brain activity combination multiscale local image decoders Neuron 60 5 2008 915929 21 Brian Murphy Marco Baroni Massimo Poesio EEG responds conceptual stimuli corpus semantics Proceedings Conference Empirical Methods Natural Language Processing 2009 pp 619627 22 Gregory Murphy The Big Book Concepts 1st edition MIT Press Cambridge MA 2004 23 T Naselaris RJ Prenger KN Kay M Oliver JL Gallant Bayesian reconstruction natural images human brain activity Neuron 63 6 2009 902915 24 Roberto Navigli Word sense disambiguation ACM Computing Surveys 41 2 February 2009 169 25 Douglas Nelson Cathy McEvoy Simon Dennis What free association measure Memory Cognition 28 6 September 2000 887899 26 KA Norman SM Polyn GJ Detre JV Haxby Beyond mindreading Multivoxel pattern analysis fMRI data Trends Cognitive Sciences 10 9 2006 424430 27 A Paivio JC Yuille SA Madigan Concreteness imagery meaningfulness values 925 nouns Journal Experimental Psychology 76 1 1968 125 28 F Pereira M Botvinick G Detre Learning semantic features fMRI data deﬁnitional text Proceedings NAACL HLT 2010 First Workshop Computational Neurolinguistics Association Computational Linguistics 2010 pp 19 29 F Pereira TM Mitchell M Botvinick Machine learning classiﬁers fMRI A tutorial overview NeuroImage 45 1 Suppl March 2009 S199S209 30 D Roy E Reiter Connecting language world Artiﬁcial Intelligence 167 12 2005 112 31 Mark Steyvers Richard Shiffrin Douglas Nelson Word association spaces predicting semantic similarity effects episodic memory A Healy Ed Experimental Cognitive Psychology Its Applications American Psychological Association 2005 pp 19 32 B Thirion E Duchesnay E Hubbard J Dubois JB Poline Denis Lebihan Stanislas Dehaene Inverse retinotopy Inferring visual content images brain activation patterns NeuroImage 33 2006 11041116 33 Peter D Turney Patrick Pantel From frequency meaning Vector space models semantics Journal Artiﬁcial Intelligence Research 37 2010 141188 34 J Van Overschelde Category norms An updated expanded version Battig Montague 1969 norms Journal Memory Lan guage 50 3 2004 289335