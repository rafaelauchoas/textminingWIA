Artiﬁcial Intelligence 172 2008 14001427 wwwelseviercomlocateartint Imprecise probability trees Bridging theories imprecise probability Gert Cooman Filip Hermans Ghent University SYSTeMS Research Group Technologiepark Zwijnaarde 914 9052 Zwijnaarde Belgium Received 30 March 2007 received revised form 12 February 2008 accepted 3 March 2008 Available online 18 March 2008 Abstract We overview approaches probability theory lower upper probabilities probabilities Walleys behavioural theory imprecise probabilities Shafer Vovks gametheoretic account probability We theories closely related suspected ﬁrst sight establish correspondence interesting interpretation ii allows freely import results theory Our approach leads account probability trees random processes framework Walleys theory We indicate results reduce computational complexity dealing imprecision probability trees prove interesting general version weak law large numbers 2008 Elsevier BV All rights reserved Keywords Gametheoretic probability Imprecise probabilities Coherence Conglomerability Event tree Probability tree Imprecise probability tree Lower prevision Immediate prediction Prequential Principle Law large numbers Hoeffdings inequality Markov chain Random process 1 Introduction In recent years witnessed growth number theories uncertainty imprecise lower upper probabilities previsions precise pointvalued probabilities previsions central Here consider Glenn Shafer Vladimir Vovks gametheoretic account probability 29 introduced Section 2 Peter Walleys behavioural theory 33 outlined Section 3 These different interpretation certainly inﬂuenced different schools thought Walley follows tradition Frank Ramsey 22 Bruno Finetti 11 Peter Williams 39 trying establish rational model subjects beliefs terms behaviour Shafer Vovk follow approach inﬂuences strongly coloured ideas gambling systems martingales They use Cournots Principle interpret lower upper probabilities 28 29 Chapter 2 nice historical overview Walleys approach lower upper probabilities deﬁned terms subjects betting rates Corresponding author Email addresses gertdecoomanUGentbe G Cooman ﬁliphermansUGentbe F Hermans 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200803001 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1401 What set 1 particular Sections 4 5 practical situations approaches strongly connected2 This implies results valid theory automatically converted reinterpreted terms Moreover shall develop account coherent immediate prediction context Walleys behavioural theory prove Section 6 weak law large numbers intuitively appealing interpretation We use weak law Section 7 suggest way scoring predictive model satisﬁes A Philip Dawids Prequential Principle 56 Why believe results important relevant AI Probabilistic models intended represent agents beliefs world operating determine actions diversity situations Probability theory provides normative reasoning making decisions face uncertainty Bayesian precise probability models property completely decisive Bayesian agent optimal choice faced number alternatives state information While view advantage realistic Imprecise probability models try deal problem explicitly allowing indecision retaining normative coherentist stance Bayesian approach We refer 83334 discussions Imprecise probability models appear number AIrelated ﬁelds For instance probabilistic logic known George Boole 1 result probabilistic inferences set probabilities imprecise probability model single probability This important dealing missing incomplete data leading socalled partial identiﬁcation probabilities instance 919 There growing literature socalled credal nets 34 essentially Bayesian nets imprecise conditional probabilities We convinced mainly mathematical computational complexity associated imprecise probability models keeping widely tool modelling uncertainty But believe results reported help inroads reducing complexity Indeed upshot able connect Walleys approach Shafer Vovks twofold First develop theory imprecise probability trees probability trees transition node children described imprecise probability model Walleys sense Our results provide necessary apparatus making inferences trees And probability trees closely related random processes effectively brings position start developing theory eventdriven random processes uncertainty described imprecise probability models We illustrate Examples 1 3 Section 8 Secondly able prove socalled Marginal Extension results Theorems 3 7 Proposition 9 lead backwards recursion dynamic programminglike methods allow exponential reduction computational complexity making inferences imprecise probability trees This illustrated Examples 3 Section 8 For precise probability trees similar techniques described Shafers book causal reasoning 26 They Christiaan Huygens drew ﬁrst probability tree showed reason solution Pascal Fermats Problem Points3 2 Shafer Vovks gametheoretic approach probability In gametheoretic approach probability 29 Shafer Vovk consider games typical players Reality Sceptic play according certain protocols They obtain interesting results coherent probability protocols This section devoted explaining means roles Reality Sceptic rules play gameplay related probabilistic reasoning 21 Realitys event tree We begin ﬁrst basic assumption dealing ﬁrst player Reality plays 1 An earlier condensed version paper discussion proofs presented ISIPTA 07 Conference 7 2 Our line reasoning contrasted 28 Shafer et al use gametheoretic framework developed 29 construct theory predictive upper lower previsions interpretation based Cournots Principle See comments near end Section 5 3 See Section 8 details precise references 1402 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Fig 1 A simple event tree Reality displaying initial situation cid2 nonterminal situations t grey circles paths terminal situations ω black circles Also depicted cut U u1 u2 u3 u4 cid2 Observe t strictly precedes u1 t cid2 u1 Ct u1 u2 children cut t G1 Reality makes number moves possible moves depend previous moves way depend previous moves Sceptic This means represent gameplay event tree 2627 information event trees We restrict discussion bounded protocols Reality makes ﬁnite bounded number moves beginning end game happens But dont exclude possibility point tree Reality choice inﬁnite number moves We shall come assumptions appropriate notational tools explicit4 Let establish terminology related Realitys event tree 211 Paths situations events A path tree represents possible sequence moves Reality beginning end game We denote set possible paths ω Ω sample space game A situation t connected segment path initial starts root tree It identiﬁes moves Reality certain point identiﬁed node tree We denote set situations Ω It includes set Ω terminal situations identiﬁed paths All situations called nonterminal initial situation cid2 represents initial segment See Fig 1 simple graphical example explaining notions If situations s t s initial segment t s precedes t t follows s write s cid3 t alternatively t cid4 s If ω path t cid3 ω path ω goes situation t We write s cid2 t s strictly precedes t s cid3 t s cid5 t An event A set paths words subset sample space A Ω With event A associate indicator IA realvalued map Ω assumes value 1 A 0 We denote t ω Ω t cid3 ω set paths t t event corresponds Reality getting situation t It clear events type t Shafer 26 calls events type exact Further Section 4 exact events events legitimately conditioned events foreseen occur Realitys gameplay 212 Cuts situation Call cut U situation t set situations follow t paths ω t unique u U ω goes In words u U u cid4 t ii ω cid4 tu U ω cid4 u 4 Essentially width tree inﬁnite depth ﬁnite G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1403 Fig 2 An event tree Reality space Wt corresponding children cut Ct nonterminal situation t Fig 1 Alternatively set U situations cut t corresponding set u u U exact events partition exact event t A cut interpreted complete stopping time If situation s cid4 t precedes follows element cut U t s precedes follows U write s cid3 U s cid4 U Similarly strictly precedes follows For cuts U V t U precedes V element U followed element V A child nonterminal situation t situation immediately follows The set Ct children t constitutes cut t called children cut Also set Ω terminal situations cut cid2 called terminal cut The event t corresponding terminal cut situation t 213 Realitys spaces We w Reality nonterminal situation t arc connects t children s Ct meaning s tw concatenation segment t arc w See Fig 2 Realitys space t set Wt moves w Reality t Wt w tw Ct We mentioned Wt countably uncountably inﬁnite situations reality choice inﬁnity moves But Wt contain elements choice Reality situation t 22 Processes variables We necessary tools represent Realitys gameplay This gameplay seen basis eventdriven timedriven account theory uncertain random processes The driving events course moves Reality makes5 In theory processes generally consider things depend succession moves This leads following deﬁnitions Any partial function set situations Ω called process process domain includes situations follow situation t called tprocess Of course tprocess sprocess s cid4 t sprocess means restricting attention values situations follow s A special example tprocess distance dt situation s cid4 t returns number steps dt s tree t s When said considering bounded protocols meant natural number D dt s cid3 D situations t s cid4 t Similarly partial function set paths Ω called variable variable Ω domain includes paths situation t called tvariable If restrict tprocess F set t terminal situations follow t obtain tvariable denote FΩ If U cut t tvariable g U measurable u U g assumes value gu gω paths ω u In case consider g variable U denote gU If F tprocess cut U t associate tvariable FU assumes value FU ω Fu ω follow u U This tvariable clearly U measurable considered variable U This notation consistent notation FΩ introduced earlier 5 These socalled Humean events shouldnt confused Moivrean events considered subsets sample space Ω See Shafer 26 Chapter 1 terminology explanation 1404 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Fig 3 The event tree associated successive coin ﬂips Also depicted cuts X1 U initial situation Similarly associate F new U stopped tprocess U F follows cid2 U Fs Fs Fu t cid3 s cid3 U u U u cid3 s The tvariable U FΩ U measurable actually equal FU U FΩ FU The following intuitive example clarify notions 1 Example 1 Flipping coins Consider ﬂipping coins This leads event tree depicted Fig 3 The identifying labels situations intuitively clear initial situation cid2 coins ﬂipped nonterminal situation h ﬁrst coin landed heads second coin ﬂipped terminal situation t t coins ﬂipped landed tails First consider real process N situation s returns number N s heads obtained far N 0 N h 1 If restrict process N set Ω terminal elements real variable NΩ values NΩ h h 2 NΩ h t NΩ t h 1 NΩ t t 0 Consider cut U initial situation corresponds following stopping time stop ﬂips soon outcome heads Fig 3 The values corresponding variable NU given NU h h NU h t 1 NU t h 1 NU t t 0 So NU U measurable considered map elements h t h t t U particular NU h 1 Next consider processes F F 1 F 2 Ω h t deﬁned follows s Fs F 1s F 2s h h h t t t h h h h h h t t h t t h h t h t t t t t F returns outcome latest F 1 outcome ﬁrst F 2 second coin ﬂip The associated variables F 1 Ω element sample space respective outcomes ﬁrst second coin ﬂips Ω F 2 The variable F 1 Ω X1measurable soon reach situation cut X1 value completely determined know outcome ﬁrst coin ﬂip Fig 3 deﬁnition X1 We associate process F variable F X1 X1measurable returns element sample space outcome ﬁrst coin ﬂip Alternatively stop process F coin ﬂip leads X1stopped process X1F This new process course equal F 1 corresponding variable Ω X1FΩ F 1 F 1 Ω X1 Eq 1 F G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1405 23 Sceptics gameplay We turn player Sceptic His possible moves depend previous moves Reality following sense In nonterminal situation t set St moves s available called Sceptics space t We following assumption G2 In nonterminal situation t positive negative zero gain Sceptic associated possible moves s St Sceptic This gain depends situation t w Reality This means nonterminal situation t gain function λt St Wt R λt s w represents change Sceptics capital situation t makes s Reality makes w 231 Strategies capital processes Let introduce notions terminology related Sceptics gameplay A strategy P Sceptic partial process deﬁned set Ω Ω nonterminal situations Pt St corresponding Sceptic nonterminal situation t If Sceptic conceives strategy P means determines Reality leaves situation cid2 Pt nonterminal situation t With strategy P corresponds capital process KP value situation t gives Sceptics capital accumulated far starts zero capital cid2 plays according strategy P It given recursion relation KP tw KP t λt Pt w w Wt initial condition KP cid2 0 Of course Sceptic starts cid2 capital α uses strategy P corresponding accumulated capital given process α KP In terminal situations accumulated capital given real variable α KP Ω If start nonterminal situation t cid2 consider tstrategies P tell Sceptic starting t onwards corresponding capital process KP tprocess tells capital Sceptic accumulated starting zero capital situation t tstrategy P 232 Lower upper prices The assumptions G1 G2 outlined determine socalled gambling protocols They sufﬁcient able deﬁne lower upper prices real variables Consider nonterminal situation t real tvariable f The upper price Et f f t deﬁned inﬁmum capital α Sceptic start t order tstrategy P accumulated capital α KP allows end game hedge f moves Reality makes t cid4 2 cid4 f taken mean α KP ω cid4 f ω terminal situations ω t Similarly cid3 α α KP Et f inf Ω cid4 f tstrategy P α KP Ω lower price E cid3 t f sup E t f f t α α KP Ω 3 t f Et f If start initial situation t cid2 simply upper lower prices real cid3 f tstrategy P E variable f denote Ef Ef cid4 233 Coherent probability protocols Requirements G1 G2 gambling protocols allow moves spaces gain functions Sceptic We impose conditions Sceptics spaces A gambling protocol called probability protocol G1 G2 requirements satisﬁed P1 For nonterminal situation t Sceptics space St convex cone linear space a1s1 a2s2 St nonnegative real numbers a1 a2 s1 s2 St 1406 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 P2 For nonterminal situation t Sceptics gain function λt following linearity property λt a1s1 a2s2 w a1λt s1 w a2λt s2 w nonnegative real numbers a1 a2 s1 s2 St w Wt Finally probability protocol called coherent6 C For nonterminal situation t s St w Wt λt s w cid3 0 It clear requirement means nonterminal situation Reality strategy playing t onwards Sceptic strictly increase capital t onwards tstrategy use For coherent probability protocols Shafer Vovk prove number interesting properties corre sponding lower upper prices We list number For real tvariable f associate U E cut U t special U measurable tvariable E uf paths ω t u unique situation U ω goes For real tvariables f1 f2 f1 cid3 f2 taken mean f1ω cid3 f2ω paths ω t U f ω E Proposition 1 Properties lower upper prices coherent probability protocol 29 Consider coherent probability protocol let t nonterminal situation f f1 f2 real tvariables U cut t Then t f cid3 Et f cid3 supωt f ω convexity t f1 E t f2 superadditivity t f real λ cid4 0 nonnegative homogeneity t f α real α constant additivity 1 infωt f ω cid3 E t f1 f2 cid4 E 2 E t λf λE 3 E t f α E 4 E t α α real α normalisation 5 E 6 f1 cid3 f2 implies E 7 E U f law iterated expectation t f2 monotonicity t f1 cid3 E t f E t E What Shafer Vovk use speciﬁc instances coherent probability protocols prove limit theorems law large numbers central limit theorem law iterated logarithm derive special cases wellknown measuretheoretic versions We shall come Section 6 The gametheoretic account probability described far general But pay little attention beliefs Sceptic additional players games entertain Reality event tree This strange according personalist epistemicist school probability beliefs In order ﬁnd incorporate beliefs game theoretic framework turn Walleys imprecise probability models 3 Walleys behavioural approach probability In book behavioural theory imprecise probabilities 33 Walley considers different types related uncertainty models We shall restrict general powerful turns easiest explain coherent sets desirable gambles 35 Consider nonempty set Ω possible alternatives ω actually obtains obtain assume possible principle determine alternative Also consider subject uncertain possible alternative actually obtains obtain A gamble Ω realvalued map Ω interpreted uncertain reward expressed units predetermined linear utility scale ω actually obtains reward f ω positive negative zero We use notation GΩ set gambles Ω Walley 33 assumes gambles bounded We boundedness assumption here7 6 For discussion use coherent refer 28 Appendix C 7 The concept desirable gamble formally allows generalisation coherence axioms real desirability hinge boundedness assumption technical mathematical point view G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1407 If subject accepts gamble f taken mean willing engage transaction ﬁrst determined ω obtains ii receives reward f ω We try model subjects beliefs Ω considering gambles accepts 31 Coherent sets desirable gambles Suppose subject speciﬁes set R gambles accepts called set desirable gambles Such set called coherent satisﬁes following rationality requirements D1 f 0 f cid5 R avoiding partial loss D2 f cid4 0 f R accepting partial gain D3 f1 f2 belong R pointwise sum f1 f2 belongs R combination D4 f belongs R pointwise scalar product λf belongs R nonnegative real numbers λ scaling Here f 0 means f cid3 0 f 0 Walley argued D1D4 sets desirable gambles satisfy additional axiom D5 R Bconglomerable partition B Ω IB f R B B f R conglomerabil ity When set Ω ﬁnite partitions ﬁnite conglomerability direct sequence ﬁnitary combination axiom D3 But Ω inﬁnite partitions inﬁnite conglomerability strong additional requirement controversy If model R B conglomerable means certain inconsistency problems conditioning elements B B avoided 33 details examples Conglomerability belief models wasnt required forerunners Walley Williams 398 Finetti 11 While agree Walley conglomerability desirable property sets desirable gambles believe conglomerability necessary need require conglomerability respect partitions actually intend condition model on9 This path shall follow Section 4 32 Conditional lower upper previsions Given coherent set desirable gambles deﬁne conditional lower upper previsions follows gamble f nonempty subset B Ω indicator IB cid4 cid3 α IB α f R P f B inf cid4 cid3 α IB f α R P f B sup 4 5 P f B P f B lower prevision P f B f conditional B supremum price α subject buy gamble f accept gamble f α contingent occurrence B Similarly upper prevision P f B f conditional B inﬁmum price α subject sell gamble f accept gamble α f contingent occurrence B For event A deﬁne conditional lower probability P AB P IAB subjects supremum rate betting event A contingent occurrence B similarly P AB P IAB We want stress deﬁnition Eq 5 P f B conditional lower prevision Walley 33 Section 61 called contingent interpretation supremum acceptable price buying gamble f contingent occurrence B meaning subject accepts contingent gambles IB f P f B cid6 8 Axioms related D1D4 D5 actually suggested Williams bounded gambles But need weaker form D5 cut conglomerability D5 9 The view expressed related Shafers sketched near end 25 Appendix 1 considered derive main results Theorems 3 6 cid12 1408 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 cid6 0 called B occurs This contrasted updating interpretation condi tional lower prevision P f B subjects present occurrence B supremum acceptable price buying f receiving information B occurred Walleys Updating Principle 33 Section 616 shall accept use Section 4 essentially states conditional lower previsions interpretations There way looking conditional lower prevision P f B shall dynamic interpretation P f B stands subjects supre mum acceptable buying price f gets know B occurred For precise conditional previsions interpretation considered 132324 It far obvious relation ﬁrst interpretations10 We shall brieﬂy come distinction following sections cid5 BB IB P f B gamble Ω element ω B For partition B Ω let P f B assumes value P f B B element B The following properties conditional lower upper previsions associated coherent set desirable bounded gambles essentially proved Walley 33 Williams 39 We extension potentially unbounded gambles Proposition 2 Properties conditional lower upper previsions 33 Consider coherent set desirable gambles R let B nonempty subset Ω let f f1 f2 gambles Ω Then11 1 infωB f ω cid3 P f B cid3 P f B cid3 supωB f ω convexity 2 P f1 f2B cid4 P f1B P f2B superadditivity 3 P λf B λP f B real λ cid4 0 nonnegative homogeneity 4 P f αB P f B α real α constant additivity 5 P αB α real α normalisation 6 f1 cid3 f2 implies P f1B cid3 P f2B monotonicity 7 B partition Ω reﬁnes partition B Bc R Bconglomerable P f B cid4 P P f BB conglomerative property The analogy Propositions 1 2 striking equality Proposition 17 inequality Proposition 2712 In section set identify exact correspondence models We shall ﬁnd speciﬁc situation applying Walleys theory leads equalities general inequalities Proposition 2713 We strict inequality Proposition 27 Example 2 Consider urn red green blue balls ball drawn random Our subject uncertain colour ball Ω r g b Assume assesses willing bet colour red rates including 14 accepts gamble Ir 14 Similarly colours accepts gambles Ig 14 Ib 14 It difﬁcult prove coherence requirements D1D4 Eq 5 smallest coherent set desirable gambles R includes assessments satisﬁes f R P f cid4 0 P f 3 4 f r f g f b 3 1 4 cid3 min cid4 f r f g f b 10 In 28 authors confuse updating interpretation dynamic interpretation claim new understanding lower upper previsions justiﬁes Peter Walleys updating principle 11 Here Proposition 1 implicitly assume write welldeﬁned meaning instance sums appear function P f B realvalued inﬁnite Shafer Vovk dont mention need 12 Concatenation inequalities lower prices appear general context described 28 13 This happen generally called marginal extension situation immediate prediction meaning start extend initial model condition increasingly ﬁner partitions initial conditional model partition deals gambles measurable respect ﬁner partitions 33 Theorem 672 20 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1409 For partition B b r g Daltonist observed colour ball tells subject follows Eq 5 manipulations cid7 cid6 f b P f b P cid6 f r g cid7 2 3 f r f g 2 1 3 cid4 cid3 f r f g min If consider f Ig particular P gb 0 P gr g 13 P gB 13Irg cid6 cid7cid7 P cid6 gB 13 13 3 P g 14 P g P P gB 0 1 6 3 4 1 4 P The difference P f B P f B inﬁmum selling supremum buying prices gambles f repre sents imprecision present subjects belief model If look inequalities Proposition 21 led consider extreme cases One extreme maximises degrees imprecision P f B P f B letting P f B infωB f ω P f B supωB f ω This leads socalled vacuous model corresponding R f f cid4 0 intended represent complete ignorance subjects The extreme minimises degrees imprecision P f B P f B letting P f B P f B The common value P f B called prevision fair price f conditional B We corresponding functional P B conditional linear prevision Linear previsions precise probability models considered Finetti 11 They course properties lower upper previsions listed Proposition 2 equality inequality statements 2 7 The restriction linear prevision indicators events ﬁnitely additive probability measure 4 Connecting approaches In order lay bare connections gametheoretic behavioural approach enter Shafer Vovks world consider player called Forecaster situation cid2 certain piecewise beliefs moves Reality 41 Forecasters local beliefs More speciﬁcally nonterminal situation t Ω Ω beliefs situation cid2 w Reality choose set Wt moves available gets t We suppose represents beliefs form coherent14 set Rt desirable gambles Wt These beliefs conditional updating interpretation sense represent Forecasters beliefs situation cid2 Reality immediately gets situation t We speciﬁcation coherent Rt t Ω Ω immediate prediction model Forecaster We want stress Rt interpreted dynamically set gambles Wt Forecaster accepts situation t We shall generally event tree provided local predictive belief models nonterminal situations t imprecise probability tree These local belief models coherent sets desirable gambles Rt But lower previsions P t derived sets Rt When local belief models precise previsions equivalently ﬁnitely additive probability measures simply probability tree Shafers 26 Chapter 3 sense 42 From local global beliefs We ask behavioural implications conditional assessments Rt imme diate prediction model For instance tell Forecaster accept certain gambles15 Ω set possible paths Reality In words beliefs cid2 14 Since dont immediately envisage conditioning local model subsets Wt impose extra conglomerability requirements coherence conditions D1D4 15 In Shafer Vovks language gambles real variables 1410 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Fig 4 In nonterminal situation t consider gamble ht Realitys space Wt Forecaster accepts turn process denoted ht The values ht s situations s cid3 t indicated curly arrows Reality nonterminal situation t combined coherently beliefs cid2 Realitys complete sequence moves In order investigate use Walleys general powerful method natural extension conservative coherent reasoning construct local pieces information Rt set desirable gambles Ω Forecaster situation cid2 coherent ii small possible meaning gambles accepted actually required coherence 421 Collecting pieces Consider nonterminal situation t Ω Ω gamble ht Rt With ht associate tgamble16 denoted ht deﬁned cid7 cid6 ωt ht ω ht ω cid4 t denote ωt unique element Wt tωt cid3 ω The tgamble ht U measurable cut U t nontrivial U cid5 t This implies interpret ht map U In fact shall associate gamble ht Wt tprocess denoted ht letting ht s ht ωt s cid4 t ω terminal situation follows s Fig 4 It ht represents gamble Ω called Reality ends situation t isnt called depends Realitys immediately t gives value ht w paths ω tw The fact Forecaster situation cid2 accepts ht Wt conditional Realitys getting t translates immediately fact Forecaster accepts contingent gamble It ht Ω Walleys Updating Principle We end set cid8 R It ht ht Rt tΩ Ω gambles Ω Forecaster accepts situation cid2 The thing left ﬁnd smallest coherent set ER desirable gambles includes R coherent set Here coherence refer conditions D1D4 D5cid12 variation D5 refers conglomerability respect partitions actually intend condition suggested Section 3 422 Cut conglomerability These partitions cut partitions Consider cut U initial situation cid2 The set events BU u u U partition Ω called U partition D5cid12 requires set desirable gambles cut conglomerable conglomerable respect cut partition BU 17 16 Just variables deﬁne t gamble partial gamble domain includes t 17 Again Realitys spaces Wt ﬁnite cut conglomerability D5 consequence D3 needs extra attention But spaces inﬁnite cut U contain inﬁnite number elements corresponding cut partition BU inﬁnite making cut conglomerability nontrivial additional requirement cid12 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1411 Fig 5 The t selection S event tree process deﬁned nonterminal situations t s selects situations desirable gamble Forecaster The values corresponding gamble process GS indicated curly arrows D5cid12 R BU conglomerable cutpartition BU Ω Iuf R u cut U cid2 f R cut conglomerability Why require conglomerability cut partitions Simply interested predictive infer ence eventually want ﬁnd gambles Ω Forecaster accepts situation cid2 conditional contingent Reality getting situation t This related ﬁnding lower previsions Forecaster conditional corresponding events t A collection t t T events constitutes partition sample space Ω T cut cid2 cid5 Because require cut conglomerability follows particular ER contain sums gambles g uU Iuhu nonterminal cuts U cid2 choices hu Ru u U This Iug Iuhu R u U Because ER convex cone D3 D4 sum sums uU Iuhu ﬁnite number nonterminal cuts U belong ER But case bounded protocols discussing Reality bounded ﬁnite number moves Ω Ω ﬁnite union uΩ Ω Iuhu belong ER choices hu Ru u Ω Ω nonterminal cuts sums cid5 cid5 423 Selections gamble processes cid9 GS s Consider nonterminal situation t tselection partial process S deﬁned nonterminal s cid4 t Ss Rs With tselection S associate tprocess GS called gamble process Sus 6 tcid3ucid2s situations s cid4 t Fig 5 Alternatively GS given recursion relation GS sw GS s Ssw w Ws cid9 GS Ω nonterminal s cid4 t initial value GS t 0 In particular leads tgamble GS terminal situations ω follow t letting Ω deﬁned IuSu 7 tcid3uuΩ Ω Then argued gambles GS Ω belong ER nonterminal situations t tselections S As strategy capital processes cid2selection S simply selection cid2gamble process simply gamble process 424 The Marginal Extension Theorem It technical step prove Theorem 3 It signiﬁcant generalisation terms sets desirable gambles coherent lower previsions18 Marginal Extension Theorem ﬁrst proved Walley 33 Theorem 672 subsequently extended De Cooman Miranda 20 It essentially tells t gamble globally desirable dominates tgamble obtained additively piecing locally desirable gambles manner described 18 The difference language obscure generalisation But Theorem 7 expressions terms predictive lower previsions connection clearer 1412 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Theorem 3 Marginal Extension Theorem There smallest set gambles satisﬁes D1D4 D5cid12 includes R This natural extension R given ER cid3 g g cid4 GS Ω selection S cid4 Moreover nonterminal situation t tgamble g holds It g ER tselection St g cid4 GSt Ω ω terminal situations ω follow t Ω taken mean gω cid4 GSt Ω g cid4 GSt 43 Predictive lower upper previsions 8 We use coherent set desirable gambles ER deﬁne special lower previsions P t P t Forecaster situation cid2 conditional event t Reality getting situation t explained Section 319 We shall conditional lower previsions predictive lower previsions We Eq 5 Theorem 3 nonterminal situation t P f t sup sup cid3 α It f α ER cid3 α f α cid4 GS cid4 cid4 9 We use notation P f P f cid2 sup α f α ER It stressed Eq 8 valid terminal situations t Eq 9 clearly isnt Ω tselection S Besides properties Proposition 2 hold general conditional lower upper previsions predictive lower upper previsions consider satisfy number additional properties listed Propositions 4 5 Proposition 4 Additional properties predictive lower upper previsions Let t situation let f f1 f2 gambles Ω 1 If t terminal situation ω P f ω P f ω f ω 2 P f t P f It t P f t P f It t 3 f1 cid3 f2 t implies P f1t cid3 P f2t monotonicity Before important point stressed clariﬁed It immediate consequence Proposition 42 f g gambles coincide t P f t P gt This means P f t completely determined values f assumes t allows deﬁne P t gambles necessarily deﬁned t tgambles We shall freely follows For cut U situation t deﬁne tgamble P f U gamble assumes value P f u ω cid4 u u U This tgamble U measurable construction considered gamble U Proposition 5 Separate coherence Let t situation let U cut t let f g tgambles g U measurable 1 P tt 1 2 P gU gU 3 P f gU gU P f U 4 g nonnegative P gf U gU P f U 44 Correspondence immediate prediction models coherent probability protocols There appears close correspondence expressions 3 lower prices E t f associated coherent probability protocols 9 predictive lower previsions P f t based 19 We stress conditional lower previsions contingentupdating interpretation G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1413 immediate prediction model Say given coherent probability protocol given immediate prediction model match lead identical corresponding lower prices E t predictive lower previsions P t nonterminal t Ω Ω The following theorem marks culmination search correspondence Walleys Shafer Vovks approaches probability theory Theorem 6 Matching Theorem For coherent probability protocol immediate prediction model match conversely immediate prediction model coherent probability protocol match The ideas underlying proof theorem clear If coherent probability protocol spaces St gain functions λt Sceptic deﬁne immediate prediction model Forecaster essentially Rt λs s St If conversely immediate prediction model Forecaster consisting sets Rt deﬁne spaces Sceptic St Rt gain functions λt h h h Rt We discuss interpretation correspondence Section 5 45 Calculating predictive lower prevision backwards recursion The Marginal Extension Theorem allows calculate conservative global belief model ER corre sponds local immediate prediction models Rt Here beliefs expressed terms sets desirable gambles Can derive result allows similar corresponding lower previsions To question entails ﬁrst consider local model Rs set desirable gambles Ws s Ω Ω Using Eq 5 associate Rs lower prevision P s GWs Each gamble gs Ws seen uncertain reward outcome gsw depends unknown w Ws Reality gets situation s And Forecasters local predictive lower prevision P sgs sup α gs α Rs 10 gs supremum acceptable price cid2 buying gs Reality gets s But seen Section 43 situation t derive global predictive lower previsions P t Forecaster global model ER Eq 8 For tgamble f P f t Forecaster inferred supremum acceptable price cid2 buying f contingent Reality getting t Is way construct global predictive lower previsions P t directly local predictive lower previsions P s We infer following theorem Propositions 8 9 Theorem 7 Concatenation Formula Consider cuts U V situation t U precedes V For tgambles f Ω20 1 P f t P P f U t 2 P f U P P f V U To clear following Proposition 8 implies consider tselection S deﬁne U called tselection SU selection mimics S U begin select zero gambles nonterminal situation s cid4 t let SU s Ss s strictly precedes element U let SU s 0 Rs If stop gamble process GS cut U readily infer Eq 6 U stopped process U GS U GS GSU Eq 1 GS U GSU Ω 11 20 Here implicitly assumed expressions welldeﬁned second statement P f v real number v V making sure P f V gamble 1414 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Fig 6 The event tree uncertain process involving n successive coin ﬂips described Example 3 We stopped gamble processes gamble processes correspond selections called soon Reality reaches cut This means actually restrict selections S U called Proposition 8 Proposition 8 Let t nonterminal situation let U cut t Then U measurable tgamble f It f ER tselection S It f cid4 GSU U Consequently Ω equivalently fU cid4 GS cid3 α fU α cid4 GS U tselection S cid4 P f t sup cid3 α f α cid4 GSU Ω tselection S sup cid4 If tgamble h measurable respect children cut Ct nonterminal situation t interpret gamble Wt For gambles following immediate corollary Proposition 8 tells predictive lower previsions P ht completely determined local model Rt Proposition 9 Let t nonterminal situation consider Ctmeasurable gamble h Then P ht P t h These results tells predictive lower upper previsions calculated backwards recursion starting trivial predictive previsions P f Ω P f Ω f terminal cut Ω local models P t This illustrated following simple example We shall come idea Section 8 Example 3 Suppose n 0 coins We begin ﬂipping ﬁrst coin tails stop ﬂip second coin Again stop tails ﬂip coin In words continue ﬂipping new coins tails n coins ﬂipped This leads event tree depicted Fig 6 Its sample space Ω t1 t2 tn hn We consider cuts U1 t1 h1 cid2 U2 t2 h2 h1 U3 t3 h3 h2 Un tn hn hn1 It convenient introduce notation h0 initial situation cid2 For nonterminal situations hk k 0 1 n 1 Forecaster beliefs cid2 Reality situation outcome k 1th coin ﬂip These beliefs expressed terms set desirable gambles Rhk Realitys space Whk hk Each space Whk clearly identiﬁed children cut Uk1 hk For purpose example consider local predictive lower previsions P hk GUk1 associated Rhk Eq 10 Forecaster assumes coins approximately fair sense assesses probability heads ﬂip lies 1 2 This assessment 2 leads following local predictive lower previsions21 1 2 cid4 cid3 ghk1 gtk1 cid10 g 1 2δ ghk1 1 2 δ 0 δ 1 δ 1 2 2δ min gtk1 P hk 12 cid11 g gamble Uk1 21 These socalled linearvacuous mixtures contamination models natural extensions probability assessments P hk 1 2 δ 33 Chapters 34 details δ P hk hk1 1 2 hk1 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1415 Let instance calculate local predictive models P hk predictive lower probabil ities P hns gamble f Ω situation s tree First terminal situations clear Proposition 41 cid6 hntk 13 We turn calculation P hnhn1 It follows Proposition 9 P hnhn1 P hn1 cid6 hnhn P 1 0 P cid7 cid7 hn substituting g Ihn Eq 12 k n 1 cid6 hnhn1 δ cid7 P 14 1 2 To calculate P hnhn2 consider hn1 cid3 Un1 cid6 hnUn1 cid6 hnUn1 cid6 hnhn2 cid7 hn2 P P P P cid6 cid6 cid7 cid7 P hn2 cid7cid7 ﬁrst equality follows Theorem 7 second Proposition 9 taking account gn1 P hnUn1 gamble children cut Un1 hn2 It follows Eq 13 gn1tn1 P hntn1 0 Eq 14 gn1hn1 P hnhn1 1 δ Substituting g gn1 Eq 12 2 k n 2 ﬁnd cid6 hnhn2 cid7 P cid12 cid13 2 1 2 δ Repeating course reasoning ﬁnd generally nk cid12 cid13 cid6 hnhk cid7 P 1 2 δ k 0 n 1 15 16 This illustrates use backwards recursion procedure calculate global local predictive lower previsions22 5 Interpretation Matching Theorem In Shafer Vovks approach appears Reality Sceptic player called Forecaster Her role consists determining Sceptics space St gain function λt non terminal situation t Shafer Vovk leave largely unspeciﬁed Forecaster makes approach general abstract But Matching Theorem tells connect approach Walleys inject notion belief modelling gametheoretic framework We speciﬁc Forecaster determine Sceptics spaces St gain functions λt determined Forecast ers beliefs cid2 Reality immediately getting nonterminal situations t23 Let explain carefully Suppose Forecaster certain beliefs situation cid2 Reality non terminal situation t suppose models beliefs specifying coherent set Rt desirable gambles Wt This brings situation described previous section When Forecaster speciﬁes set making certain behavioural commitments committing accepting situation cid2 gamble Rt contingent Reality getting situation t accepting combination gambles according combination axioms D3 D4 D5cid12 This implies derive predictive lower previsions P t following interpretation situation cid2 P f t supremum price 22 It indicates need work general language lower previsions gambles familiar lower probabilities events want calculate global predictive lower probability recursion step need start working lower previsions gambles More discussion previsiongamble versus probabilityevent issue 33 Chapter 4 23 The germ idea case Forecasters beliefs expressed precise probability models GWt present Shafers work instance 29 Chapter 8 25 Appendix 1 We extend idea Walleys imprecise probability models 1416 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Forecaster buy tgamble f conditional Realitys getting t basis commitments initial situation cid2 What Sceptic Forecaster commitments This means situation cid2 use selection S nonterminal situation t selects gamble equivalently nonnegative linear combination gambles St ht Rt offer corresponding gamble GS Ω Ω Forecaster bound accept If Realitys situation t w Wt changes Sceptics capital positive negative zero ht w In words space St identiﬁed convex set gambles Rt gain function λt given λt ht ht But selection S identiﬁed strategy P Sceptic KP Ω essence proof Theorem 6 tells led Ω coherent probability protocol corresponding lower prices E t Sceptic coincide Forecasters predictive lower previsions P t GS In nice paper 28 Shafer Gillett Scherl discuss ways introducing interpreting lower previsions gametheoretic framework terms prices subject willing pay gamble terms subject believes lot money utility prices They consider conditional lower previsions contingent dynamic interpretation argue equality certain cases Here decided stick usual interpretation lower upper previsions concentrated contingentupdating interpretation We approach gametheoretic framework useful This particular relevance laws large numbers Shafer Vovk derive gametheoretic framework laws given behavioural interpretation terms Forecasters predictive lower upper previsions To example turn deriving general weak law large numbers 6 A general weak law large numbers Consider nonterminal situation t cut U t Deﬁne tvariable nU nU ω distance dt u measured moves tree t unique situation u U ω goes nU clearly U measurable nU u simply distance dt u t u We assume nU u 0 u U words U cid5 t Of course bounded protocols considering nU bounded denote minimum NU Now consider s t U bounded gamble hs real number ms hs ms Rs meaning Forecaster situation cid2 accepts buy hs ms contingent Reality getting situation s Let B 0 common upper bound sup hs inf hs t cid3 s cid2 U It follows coherence Rs D1 ms cid3 sup hs To things interesting shall assume inf hs cid3 ms hs ms cid4 0 accepting gamble represents real commitment Forecasters As result hs ms cid3 sup hs inf hs cid3 B We interested following tgamble GU given GU 1 nU cid9 tcid3scid2U Ishs ms provides measure average gambles hs yield outcome Forecasters accepted buying prices ms segments tree starting t ending right U In words GU measures average gain Forecaster segments t U associated commitments taken Reality segments This gamble GU U measurable We interpret GU gamble U Also hs u U know s cid2 u hs value hsu hsωs ω u This allows write GU u 1 nU u cid9 cid14 hsu ms cid15 tcid3scid2u We like study Forecasters beliefs initial situation cid2 contingent Reality getting t occurrence event cid3 ω t GU ω cid4 cid6 GU cid4 cid6 cid4 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1417 cid6 0 In words want know P GU cid4 cid6t Forecasters supremum rate betting event average gain t U cid6 contingent Realitys getting t Theorem 10 Weak Law Large Numbers For cid6 0 cid6 GU cid4 cid6t cid7 P cid4 1 exp cid12 cid13 NU cid62 4B2 We NU increases lower bound increases theorem loosely formulated follows As horizon recedes Forecaster coherent believe increasingly strongly average gain path present horizon wont negative This general version weak law large numbers It seen generalisation Hoeff dings inequality martingale differences 14 37 Chapter 4 30 Appendix A7 coherent lower previsions event trees 7 Scoring predictive model We look interesting consequence Theorem 10 shall score predictive model manner satisﬁes Dawids Prequential Principle 56 We consider special case Theorem 10 t cid2 Suppose Reality follows path situation uo U leads average gain GU uo Forecaster Suppose average gain negative GU uo 0 We uo GU cid6 0 cid6 GU uo events GU cid6 actually occurred uo On hand Forecasters upper probability cid2 occurrence satisﬁes P GU cid6 cid3 exp NU cid62 4B2 Theorem 10 Coherence tells Forecasters upper probability cid2 event uo actually occurred SNU γU uo cid12 cid13 SN x exp N 4 x2 γU u GU uo B Observe γU uo number 1 0 assumption Coherence requires Forecaster local predictive commitments forced Sceptic chooses strategy bet occurrence event uo rate 1 SNU γU uo So Forecaster losing utility local predictive commitments Just depends close γU uo lies 1 large NU Fig 7 The upper bound SNU γU uo constructed upper probability uo interesting property try explicit Indeed calculate Forecasters upper probability P uo uo directly Eq 9 value generally depend Forecasters predictive assessments Rs situations s dont precede uo Reality got We shall case upper bound SNU γU uo constructed Theorem 10 Fig 7 What Forecaster pay 1 SN x function x γU u different values N NU 1418 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Consider situation s U path uo meaning Reality got situation s Therefore corresponding gamble hs ms expression GU isnt calculating value GU uo change obtain value GU uo Indeed consider predictive model thing ask Rcid12 U alternative predictive model restriction let hcid12 s coincide Rs s chosen arbitrarily coherently Now construct new average ms U uo GU uo new upper probability hs mcid12 s s precede uo For s Rcid12 gain gamble Gcid12 s precedes uo We know reasoning Gcid12 event uo observed s cid12 cid13 cid12 cid13 SNU Gcid12 U uo B SNU GU uo B SNU cid6 cid7 γU uo In words upper bound SN γU u Forecasters upper probability Reality getting situation uo depends Forecasters local predictive assessments Rs situations s Reality actually got assessments situations This means method scoring predictive model satisﬁes Dawids Prequential Principle instance 56 8 Concatenation backwards recursion As discovered Section 45 Theorem 7 Proposition 9 enable calculate global predic tive lower previsions P t imprecise probability trees local predictive lower previsions P s s cid4 t backwards recursion method That possible probability trees probability models precise pre visions wellknown24 arguably discovered Christiaan Huygens middle 17th century25 It allows exponential dynamic programminglike reduction complexity calculating previsions ex pectations essentially phenomenon leads computational efﬁciency machine learning tools instance Needleman Wunschs 21 sequence alignment algorithm In section want illustration exponential reduction complexity looking problem involving Markov chains Assume state Xn consecutive times n 1 2 N assume value ﬁnite set X Forecaster beliefs state X1 time 1 leading coherent lower prevision P 1 GX She assesses jumps state Xn xn new state Xn 1 goes depend state Xn time n states Xk previous times k 1 2 n 1 Her beliefs Xn xn time n 1 represented lower prevision P xn GX The time evolution modelled Reality traversing event tree An example tree X b N 3 given Fig 8 The situations tree form x1 xk X k k 0 1 N k 0 gives abuse notation let X 0 cid2 In cut Xk X k cid2 value Xk state time k revealed This leads imprecise probability tree local predictive models P cid3 P 1 P x1xk P xk 17 expressing usual Markov conditional independence condition terms lower previsions For notational convenience introduce generally nonlinear lower transition operator T linear space GX follows T GX GX x cid15 P xf 24 See Chapter 3 Shafers book 26 causal reasoning probability trees This chapter contains number propositions calculating probabilities expectations probability trees ﬁnd generalisations Sections 43 45 For instance Theorem 7 generalises Proposition 311 26 imprecise probability trees 25 See Appendix A Shafers book 26 Shafer discusses Huygenss treatment special case socalled Problem Points Huygens draws probably ﬁrst recorded probability tree solves problem backwards calculation expectations tree Huygenss treatment Appendix VI 15 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1419 Fig 8 The event tree time evolution states b change state time instant n 1 2 3 Also depicted respective cuts X 1 X 2 cid2 state times 1 2 revealed words Tf gamble X value Tf x state x X given P xf The lower transition operator T completely describes Forecasters beliefs changes state instant We want ﬁnd corresponding model Forecasters beliefs cid2 state time n So let consider gamble fn X N actually depends value Xn X time n We want calculate lower prevision P fn P fncid2 Consider time instant k 0 1 n 1 situation x1 xk X k For children cut Cx1 xk x1 xk xk1 xk1 X x1 xk P fnCx1 xk gamble depends value Xk 1 X value xk1 given P fnx1 xk1 We ﬁnd P fnx1 xk P P cid6 cid7 cid6 fnCx1 xk cid7 x1 xk P xk cid6 cid6 fnCx1 xk cid7cid7 P 18 ﬁrst equality follows Theorem 7 second Proposition 9 Eq 17 We ﬁrst apply Eq 18 k n 1 By Proposition 52 P fnCx1 xn1 fn led P fnx1 xn1 P xn1 fn Tfnxn1 Tfn cid6 cid7 fnCx1 xn2 P Substituting Eq 18 k n 2 yields P fnx1 xn2 P xn2 cid6 cid7 fnCx1 xn3 P T2fn Tfn Proceeding fashion k 1 P fnCcid2 Tn1fn going step k 0 Eq 18 yields P fncid2 P cid3 P fnCcid2 cid7 19 cid6 Tn1fn P fn P 1 We complexity calculating P fn way essentially linear number time steps n In literature imprecise probability models Markov chains 2173132 socalled credal set set probabilities approach generally calculate P fn The point want approach typically worse exponential complexity number time steps To recall 33 lower prevision P GX derived coherent set desirable gambles corresponds convex closed set M P probability mass functions p X called credal set given cid3 p cid4 cid7 cid6 g GX P g cid3 Epg cid5 M P let Epg xX pxgx expectation gamble g associated mass function p Ep linear prevision language Section 32 It holds gambles g X cid4 cid3 Epg p ext M P ext M P set extreme points convex closed set M P Typically approach extM P assumed ﬁnite M P called ﬁnitely generated credal set See instance 34 discussion credal sets applications Bayesian networks cid4 cid3 Epg p M P P g min min 1420 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 Then P fn calculated follows26 Choose nonterminal situation t x1 xk X k k 0 1 n 1 mass function pt set M P t given Eq 17 equivalently set extreme points ext M P t This leads precise probability tree calculate corresponding expectation fn Then P fn minimum expectations calculated possible assignments mass functions nodes We roughly speaking M P t typical number extreme points M complexity calculating P fn essentially M n exponential number time steps This shows lower prevision approach problems lead efﬁcient algorithms credal set approach This especially relevant probabilistic inferences involving graphical models credal networks 34 Another nice example phenomenon concerned checking coherence precise imprecise probability models Walley et al 36 9 Additional remarks We proved correspondence approaches event trees bounded horizon For games inﬁnite horizon correspondence immediate Shafer Vovk implicitly use coherence axioms stronger D1D4 D5cid12 leading lower prices dominate corresponding predictive lower previsions Exact matching restored course provided argue additional requirements rational subject comply This interesting topic research We havent paid attention special case coherent lower previsions conjugate upper previsions coincide precise previsions fair prices Finettis 11 sense When local predictive models P t Proposition 9 happen precise meaning P t f P t f P t f gambles f Wt immediate prediction model described Section 4 closely related arguably identical probability trees introduced studied Shafer 26 Indeed predictive previsions P s obtained concatenation local models Pt guaranteed Theorem 727 Moreover indicated Section 8 possible prove lower envelope theorems effect local lower previsions P t correspond lower envelopes sets Mt local previsions Pt ii possible choice previsions Pt Mt nonterminal situations t leads compatible probability tree Shafers 26 sense corresponding predictive previsions P s iii predictive lower previsions P s lower envelopes predictive previsions P s compatible probability trees Of course law large numbers Section 6 remains valid probability trees Finally want recall Theorem 7 Proposition 9 allow calculation predictive models P s local models backwards recursion manner strongly reminiscent dynamic program ming techniques This allow efﬁcient computation predictive models approach exploits lower envelope theorems sets probabilitiesprevisions We think lessons learnt dealing types graphical models credal networks 34 What makes efﬁcient approach possible ultimately Marginal Extension Theorem Theorem 3 leads Concatenation Formula Theorem 7 speciﬁc equality general inequal ities Proposition 27 Generally speaking instance 33 Section 67 20 marginal extension results proved models Forecaster speciﬁes local immediate prediction models relate beliefs nonterminal situation t Reality going immediately getting t Acknowledgements This paper presents research results BOFproject 01107505 We like thank Enrique Miranda Marco Zaffalon Glenn Shafer Vladimir Vovk Didier Dubois discussing questioning views ex pressed discussions took place years ago Sébastien Destercke 26 An explicit proof statement far immediate application Theorems 3 4 20 27 This instance compared Proposition 311 26 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1421 Erik Quaeghebeur read commented earlier drafts We grateful insightful generous comments reviewers led better discuss signiﬁcance potential applications results helped improve readability paper Appendix A Proofs main results In appendix gathered proofs important results paper A1 Proofs results Section 3 We begin proof Proposition 2 Although similar results proved bounded gambles Walley 33 Williams 39 proof works extension possibly unbounded gambles considering paper Proof Proposition 2 For ﬁrst statement proof ﬁrst inequalities The proof remaining inequality similar For ﬁrst inequality assume loss generality inf ω B f ω real number denote β So know IB f β cid4 0 IB f β R D2 It follows Eq 5 β cid3 P f B To prove second inequality assume ex absurdo P f B P f B follows Eqs 4 5 real α β β α IB f α R IB β f R By D3 IB β α IB f α IB β f R contradicts D1 IB β α 0 We turn second statement As announced Footnote 11 assume sum terms P f1B P f2B welldeﬁned If terms equal resulting inequality holds trivially assume loss generality terms strictly greater Consider real α P f1B β P f2B Eq 5 IB f1 α R IB f2 β R Hence IB f1 f2 α β R D3 P f1 f2B cid4 α β Eq 5 Taking supremum real α P f1B β P f2B leads desired inequality To prove statement ﬁrst consider λ 0 Since D4 IB λf α R IB f αλ R Eq 5 P λf B sup cid4 cid3 α IB λf α R sup cid4 cid3 λβ IB f β R λP f B For λ 0 consider P 0B sup α IB α R 0 equality follows D1 D2 For fourth statement use Eq 5 ﬁnd cid4 cid3 β IB f α β R P f αB sup sup cid4 cid3 α γ IB f γ R α P f B The ﬁfth statement immediate consequence ﬁrst To prove sixth statement observe f1 cid3 f2 implies IB f2 f1 cid4 0 IB f2 f1 R D2 Now consider real α IB f1 α R D3 IB f2 α IB f1 α IB f2 f1 R Hence cid4 cid3 α IB f1 α R cid4 cid3 α IB f2 α R taking suprema considering Eq 5 deduce P f1B cid3 P f2B For ﬁnal statement assume P f C real number C B Also observe P f D P f IDD nonempty D Deﬁne gamble g follows gω P f C ω C C B We prove P gB cid3 P f B We assume loss generality P gB inequality holds trivially Fix cid6 0 consider gamble IB f g cid6 Also consider C B If C B ICIB f g cid6 ICf P f C cid6 R Eq 5 If C B ICIB f g cid6 0 R D2 Since R Bconglomerable follows IB f g cid6 R P f gB cid4 cid6 Eq 5 Hence P hB cid4 0 h f g Consequently P f B P h gB cid4 P hB P gB cid4 P gB use second statement fact P gB P hB cid4 0 implies sum righthand inequality welldeﬁned extended real number cid4 1422 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 A2 Proofs results Section 4 Proof Theorem 3 We argued coherent set desirable gambles includes R contain gambles GS D3 D5cid12 By D2 D3 include set ER If ER coherent satisﬁes D1D4 D5cid12 proved ER natural extension R This set We ﬁrst D1 satisﬁed It clearly sufﬁces selection S holds GS Ω 0 This follows Lemma 2 To prove D2 holds consider selection S0 0 GS0 0 f cid4 0 follows f cid4 GS0 f ER To prove D3 D4 hold consider f1 f2 ER nonnegative real numbers a1 a2 We know selections S1 S2 f1 cid4 GS1 f2 cid4 GS2 But a1S1 a2S2 selection Rt satisfy D3 D4 Ga1S1a2S2 a1GS1 a2GS2 cid3 a1f1 a2f2 a1f1 a2f2 ER To conclude D5cid12 satisﬁed Consider cut U cid2 Consider gamble f assume Iuf ER u U We prove f ER Let Ut U Ω Unt U Ω U disjoint union Ut Unt For ω Ut Iωf Iωf ω ER implies f ω cid4 0 D1 For u Unt invoke Lemma 3 ﬁnd uselection Su Iuf cid4 GSu Ω Now construct selection S follows Consider s Ω Ω If u cid3 s unique U cut u Unt let Ss Sus Otherwise let Ss 0 Then cid9 GS IuGSu cid3 cid9 cid9 Iuf cid3 Iuf f uUnt uUnt uU f ER ﬁrst equality seen immediate consequence Lemma 1 second inequality holds shown f ω cid4 0 ω Ut The rest proof follows Lemma 3 cid4 Lemma 1 Let t nonterminal situation let U cut t Consider tselection S let u U Ω Su uselection given Sus Ss nonterminal situation s follows u Sus 0 Moreover let SU U called tselection S deﬁned Theorem 7 Then u GSu Ω IuGS cid14 GS u GS Ω Iu cid9 cid9 cid15 uU Ω GS U cid9 uU Ω uU Ω GSU Ω IuGSu Ω cid9 uU Ω IuGSu Ω Proof It immediate second equality holds Eq 11 For ﬁrst equality obviously sufﬁces consider values left righthand sides ω u u U Ω The value righthand Eqs 6 7 GS u GSu Ω ω cid9 cid9 cid9 Ssu Ssω tcid3scid2u ucid3scid2ω tcid3scid2ω Ssω GS Ω ω cid4 Lemma 2 Consider nonterminal situation t tselection S Then doesnt hold GS corollary consider cut U t gamble GS GS U 0 U Ω 0 t As U u GS u Then doesnt hold U U deﬁned GS Proof Deﬁne set PS s Ω Ω t cid3 s Ss cid4 0 relative complement NS s Ω Ω t cid3 s Ss cid5cid4 0 If NS GS cid4 0 Eq 7 assume loss generality NS Ω nonempty Consider minimal element t1 NS meaning s NS s cid2 t1 minimal element NS bounded horizon assumption So t cid3 s cid2 t1 Ss cid4 0 Choose w1 Wt1 St1w1 0 possible Rt1 satisﬁes D1 This brings situation t2 t1w1 If t2 NS choose w2 Wt2 St2w2 0 possible D1 If t2 PS G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1423 know St2w2 cid4 0 choice w2 Wt2 We continue way reach terminal situation ω t1w1w2 ﬁnite number steps bounded horizon assumption Moreover GS Ω ω cid9 tcid2t1 cid7 cid6 St ωt cid9 It hold GS k Ω 0 t Stkwk cid4 0 St1w1 0 0 ω Ω GS Ω ω cid3 0 ω cid5cid4 t t GSt GS To prove second statement consider U called tselection SU derived S letting SU s Ss Ω ω s follows t strictly precedes u U zero Then GS u ω u u U Eq 11 Now apply result tselection SU cid4 Ssu GSU tcid3scid2u cid5 Lemma 3 Consider nonterminal situation t gamble f Then It f ER tselection St It f cid4 GSt Ω t Proof It clearly sufﬁces prove necessity Assume It f ER meaning deﬁnition set ER selection S It f cid4 GS Ω Let St tselection deﬁned letting St s Ss t cid3 s zero It follows Lemma 1 use cut cid2 t terminal situations follow t It f cid4 GS It GS Iωcid12GS cid9 cid12 t GSt Ω Ω ω Ω ωcid12cid5t A1 Ω ω cid3 f ω ω cid4 t A2 Then A2 proof complete prove GS t cid4 0 Assume ex absurdo GS t 0 Consider cut cid2 t terminal situations dont follow t Applying Lemma 2 cut initial situation cid2 ω Ω t GS Ω ω 0 But contradicts A1 cid4 Proof Proposition 4 For ﬁrst statement consider terminal situation ω gamble f Ω Then ω ω Iωf α Iωf ω α ER α cid3 f ω D1 D2 Using Eq 8 ﬁnd P f ω f ω By conjugacy P f ω P f ω f ω f ω For second statement use Eq 8 observe It f α It f It α The statement immediate consequence second Proposition 26 cid4 Proof Proposition 5 The ﬁrst statement follows Eq 8 observe It It α It 1 α ER α cid3 1 D1 D2 For second statement consider u U P gu gU u But U measurability g tells Iug α IugU u α gamble belongs ER α cid3 gU u D1 D2 Now use Eq 8 The proofs fourth statements similar based observation Iuf g α Iuf gU u α Iugf α IugU uf α cid4 Proof Theorem 6 First consider immediate prediction model Rt t Ω Ω Deﬁne Sceptics spaces St Rt gain functions λt St Wt λt h w hw h Rt w Wt Clearly P1 P2 satisﬁed Rt convex cone D3 D4 But coherence requirement C Indeed werent satisﬁed nonterminal situation t gamble h Rt hw 0 w Wt contradicting coherence requirement D1 Rt We led coherent probability protocol We matching Consider nonterminal situation t tselection S For terminal situations ω cid4 t GS Ω ω cid9 cid7 cid6 Su ωu cid9 cid7 cid6 Su ωu λu KS Ω ω tcid3ucid2ω tcid3ucid2ω 1424 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 P t words selections strategies onetoone correspondence actually things corresponding gamble capital processes additive inverses It immediate Eqs 3 9 E t Conversely consider coherent probability protocol spaces St gain functions λt St Wt λt s s St By similar argument P cid12t E nonterminal t Deﬁne Rcid12 t t P cid12t predictive lower previsions associated sets Rcid12 t convex cone gambles P1 P2 C know nonterminal situations t gambles h Rcid12 t w Wt hw cid4 0 This means conditions Lemma 4 satisﬁed P cid12t P t P t predictive lower previsions associated immediate prediction model Rt smallest convex cone containing nonnegative gambles including λt s δ s St δ 0 cid4 t But Rcid12 Lemma 4 Consider nonterminal situation t Ω Ω set gambles Rcid12 convex cone ii h Rcid12 t αh δ f h Rcid12 dictive lower previsions obtained sets Rt coincide ones obtained Rcid12 t t Wt Rcid12 t w Wt hw cid4 0 Then set Rt t δ 0 f cid4 0 α cid4 0 coherent set desirable gambles Wt Moreover pre Proof Fix nonterminal situation t We ﬁrst Rt coherent set desirable gambles D1D4 satisﬁed Observe Rt smallest convex cone gambles including set h δ h Rcid12 t δ 0 containing nonnegative gambles So D2D4 satisﬁed To prove D1 holds consider g 0 assume ex absurdo g Rt Then h Rcid12 t δ 0 f cid4 0 α cid4 0 0 g αh δ f αh δ 0 α 0 h δ 0 But ii w Wt hw cid4 0 hw δ 0 This contradicts h δ 0 We second Consider gamble f Ω Fix t Ω Ω cid6 0 First consider tselection Scid12 associated Rcid12 s s cid4 t Since Reality ﬁnite bounded number moves happens possible choose δs 0 nonterminal s cid4 t cid5 tcid3scid2ω δs cid6 ω Ω follow t Deﬁne tselection S associated Rs Ss Scid12sδs Rs s Scid12s Rcid12 cid12 nonterminal s follow t Clearly GS Ω cid3 α f α cid4 GScid12 Ω cid4 cid3 α f α cid4 GS Ω f t sup Scid12 sup Scid12 sup sup P cid4 cid3 sup Scid12 cid6 cid3 P f t cid6 cid3 cid6 GScid12 Ω cid3 α f α cid6 cid4 GS Ω sup cid4 Since inequality holds cid6 0 ﬁnd P cid12f t cid3 P f t Conversely consider tselection S associated Rs For s cid4 t hs Rcid12 s s δs 0 fs cid4 0 αs cid4 0 Ss αshs δs fs Deﬁne tselection Scid12 associated Rcid12 Scid12s αshs Ss αsδs fs cid3 Ss Clearly GScid12 cid3 α f α cid4 GS Ω cid3 GS Ω cid4 α f α cid4 GScid12 cid12 Ω cid3 sup f t cid3 P Ω cid4 P f t sup S cid3 sup S This proves P cid12f t P f t cid4 sup Proof Theorem 7 It isnt difﬁcult second statement consequence ﬁrst prove ﬁrst statement Consider tgamble f Ω Recall implicitly assumed P f U tgamble Then prove P f t P P f U t Let ease notation g P f U tgamble g U measurable prove P f t P gt Now possibilities First t terminal situation ω hand P f t f ω Proposition 41 On hand Proposition 41 P gt gω P f U ω Now U cut t ω unique element u U t ω goes u ω P f U ω P f ω f ω Proposition 41 This tells case P f t P gt G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1425 Secondly suppose t terminal situation Then follows Proposition 27 cut conglom erability ER P f t cid4 P P f U t P gt recall P t P t P U P BU It remains prove converse inequality P f t cid3 P gt Choose cid6 0 Eq 9 tselection S f P f t cid6 cid4 GS Ω paths t Invoke Lemma 1 notations introduced ﬁnd f P f t cid6 cid4 GS U IuGSu Ω t A3 uU Ω Now consider u U If u terminal situation ω Proposition 41 gu P f ω f ω Eq A3 yields cid9 gω P f t cid6 cid4 GSU taking account GS U yields Ω ω GSU Ω Eq 11 If u terminal situation ω u Eq A3 A4 f ω P f t cid6 cid4 GS U u GSu Ω ω Su uselection inequality Eq 9 tells P f u cid4 P f t cid6 GS ω u U u gω P f t cid6 cid4 GSU A5 If combine inequalities A4 A5 recall Eq 9 P gt cid4 P f t cid6 Since holds cid6 0 conclude P gt cid4 P f t cid4 Ω ω Proof Proposition 8 The condition clearly sufﬁcient let necessary Suppose It f ER tselection S f cid4 GS Ω Theorem 3 Lemma 3 Deﬁne u U Ω selection Su follows Sus Ss s cid4 u Sus 0 Then Lemma 1 cid9 GS Ω GS U IuGSu Ω uU Ω Now ﬁx u U If u terminal situation ω follows equality fU u f ω cid4 GS U u If u terminal situation ω u fU u f ω cid4 GS U u GSu Ω ω taking supremum ω u fU u cid4 GS U u sup ωu GSu Ω ω cid4 GS U u inequality follows supωu U u equivalent It f cid4 GSU fU cid4 GS GSu Ω ω cid4 0 Lemma 2 t u S Su Now recall Ω Eq 11 cid4 A3 Proofs results Section 6 Proof Theorem 10 This proof builds intriguing idea Shafer Vovk different situation form 29 Lemma 33 Because hs ms cid3 B t cid3 s cid2 u follows GU u cid4 B sufﬁces prove GU cid6 t complementary event Δc tcid6 inequality cid6 B We work upper probability P Δc tcid6 It given cid3 α α GS inf Ω cid4 IΔc tcid6 tselection S cid4 A6 1426 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 tcid6 event U In expression A6 tcid6 tselection S follow cid4 IΔc cid3 α 0 contradicting Lemma 2 Fix α 0 δ 0 consider selection S Ss Because GU U measurable consider Δc assume α cid4 0 Indeed α 0 α GS Ω GS Ω λshs ms Rs t cid3 s cid2 U let Ss zero Here cid7cid15 cid6 mv hvu cid6 mv hvs cid14 1 δ cid14 1 δ λs αδ αδ A7 cid16 cid16 cid7cid15 tcid3vcid2s tcid3vcid2s u element U follows s Recall B cid3 hs ms cid3 B choose δ 1 2B certainly guaranteed λs 0 λshs ms Rs After elementary manipulations u U ω u GS Ω ω cid9 cid6 tcid3scid2u hsu ms cid9 cid7 λs cid6 hsu ms cid7 αδ cid16 cid14 1 δ cid6 mv hvu cid7cid15 tcid3scid2u tcid3vcid2s second equality follows Eq A7 The gamble GS ease notation Ω U measurable If let ξs ms hsu GS Ω u α cid9 cid16 δξs 1 δξv α cid9 cid16 1 δξv α cid9 cid16 1 δξv tcid3scid2u cid16 α α tcid3vcid2s 1 δξv α α tcid3scid2u cid16 cid14 1 δ tcid3vcid2s cid6 mv hvu cid7cid15 tcid3scid2u tcid3vcid3s tcid3vcid2u tcid3vcid2u u U Then follows A6 ﬁnd α cid4 0 cid16 α cid14 1 δ cid6 mv hvu cid7cid15 cid4 1 tcid3vcid2u u belongs Δc inequality equivalent condition tcid6 α upper bound P Δc tcid6 t By taking logarithms sides ln α cid9 tcid3scid2u cid14 1 δ ln cid6 ms hsu cid7cid15 cid4 0 A8 Since ln1 x cid4 x x2 x 1 cid9 cid6 cid14 1 δ ln cid7cid15 ms hsu tcid3scid2u 2 δms hsu cid4 δB 1 cid6 ms hsu cid7 cid6 ms hsu cid14 δ cid9 cid9 δ cid4 cid7cid15 2 2 previous restrictions δ ﬁnd tcid3scid2u cid9 cid4 δ cid14 cid15 ms hsu tcid3scid2u δ2nU uB2 tcid3scid2u nU uδ cid14 GU u B2δ cid15 But u Δc cid9 tcid6 GU u cid6 u cid7cid15 cid6 ms hsu nU uδcid6 B2δ cid14 1 δ ln tcid3scid2u If choose α u U ln α nU uδcid6 B2δ cid4 0 equivalently α cid4 expnU uδcid6 B2δ condition A8 satisﬁed u Δc tcid6 α upper bound P Δc 2B2 Replacing nU tcid6 minimum NU allows rid udependence P Δc 4B2 We previously tcid6 2B use value δ ﬁnd proved inequality cid6 B cid4 required δ 1 t The tightest smallest upper bound u U achieved δ cid6 t cid3 exp NU cid62 G Cooman F Hermans Artiﬁcial Intelligence 172 2008 14001427 1427 References 1 G Boole The Laws Thought Dover Publications New York 1847 reprint 1961 2 MA Campos GP Dimuro AC da Rocha Costa V Kreinovich Computing 2step predictions intervalvalued ﬁnite stationary Markov chains Technical Report UTEPCS0320a University Texas El Paso 2003 3 FG Cozman Credal networks Artiﬁcial Intelligence 120 2000 199233 4 FG Cozman Graphical models imprecise probabilities International Journal Approximate Reasoning 39 23 2005 167184 5 APh Dawid Statistical theory The prequential approach Journal Royal Statistical Society Series A 147 1984 278292 6 APh Dawid VG Vovk Prequential probability Principles properties Bernoulli 5 1999 125162 7 G Cooman F Hermans On coherent immediate prediction Connecting theories imprecise probability G Cooman J Vejnarova M Zaffalon Eds ISIPTA 07Proceedings Fifth International Symposium Imprecise Probability Theories Ap plications SIPTA 2007 pp 107116 8 G Cooman E Miranda Symmetry models versus models symmetry WL Harper GR Wheeler Eds Probability Inference Essays Honor Henry E Kyburg Jr Kings College Publications 2007 pp 67149 9 G Cooman M Zaffalon Updating beliefs incomplete observations Artiﬁcial Intelligence 159 12 2004 75125 10 B Finetti Teoria delle Probabilità Einaudi Turin 1970 11 B Finetti Theory Probability A Critical Introductory Treatment John Wiley Sons Chichester 19741975 English translation 10 volumes 12 P Gärdenfors NE Sahlin Decision Probability Utility Cambridge University Press Cambridge 1988 13 M Goldstein The prevision prevision Journal American Statistical Society 87 1983 817819 14 W Hoeffding Probability inequalities sums bounded random variables Journal American Statistical Association 58 1963 1330 15 Ch Huygens Van Rekeningh Spelen van Geluck 16561657 Reprinted vol XIV 16 16 Ch Huygens Œuvres complètes Christiaan Huygens Martinus Nijhoff Den Haag 18881950 Twentytwo volumes Available digitised form Bibliothèque nationale France httpgallicabnffr 17 IO Kozine LV Utkin Intervalvalued ﬁnite Markov chains Reliable Computing 8 2 2002 97113 18 HE Kyburg Jr HE Smokler Eds Studies Subjective Probability Wiley New York 1964 Second edition new material 1980 19 C Manski Partial Identiﬁcation Probability Distributions SpringerVerlag New York 2003 20 E Miranda G Cooman Marginal extension theory coherent lower previsions International Journal Approximate Reason ing 46 1 2007 188225 21 SB Needleman CD Wunsch A general method applicable search similarities amino acid sequence proteins Journal Molecular Biology 48 1970 443453 22 FP Ramsey Truth probability 1926 RB Braithwaite Ed The Foundations Mathematics Logical Essays Kegan Paul Trench Trubner Co London 1931 pp 156198 Chapter VII reprinted 18 12 23 G Shafer Bayess arguments rule conditioning The Annals Statistics 10 1982 10751089 24 G Shafer A subjective interpretation conditional probability Journal Philosophical Logic 12 1983 453466 25 G Shafer Conditional probability International Statistical Review 53 1985 261277 26 G Shafer The Art Causal Conjecture The MIT Press Cambridge MA 1996 27 G Shafer PR Gillett R Scherl The logic events Annals Mathematics Artiﬁcial Intelligence 28 2000 315389 28 G Shafer PR Gillett RB Scherl A new understanding subjective probability generalization lower upper prevision International Journal Approximate Reasoning 33 2003 149 29 G Shafer V Vovk Probability Finance Its Only Game Wiley New York 2001 30 V Vovk A Gammerman G Shafer Algorithmic Learning Random World Springer New York 2005 31 D Škulj Finite discrete time Markov chains interval probabilities J Lawry E Miranda A Bugarin S Li MA Gil P Grze gorzewski O Hryniewicz Eds Soft Methods Integrated Uncertainty Modelling Springer Berlin 2006 pp 299306 32 D Škulj Regular ﬁnite Markov chains interval probabilities G Cooman J Vejnarová M Zaffalon Eds ISIPTA 07 Proceedings Fifth International Symposium Imprecise Probability Theories Applications SIPTA 2007 pp 405413 33 P Walley Statistical Reasoning Imprecise Probabilities Chapman Hall London 1991 34 P Walley Measures uncertainty expert systems Artiﬁcial Intelligence 83 1 1996 158 35 P Walley Towards uniﬁed theory imprecise probability International Journal Approximate Reasoning 24 2000 125148 36 P Walley R Pelessoni P Vicig Direct algorithms checking consistency making inferences conditional probability assessments Journal Statistical Planning Inference 126 2004 119151 37 L Wasserman All Statistics Springer New York 2004 38 PM Williams Notes conditional previsions Technical report School Mathematical Physical Science University Sussex UK 1975 39 PM Williams Notes conditional previsions International Journal Approximate Reasoning 44 2007 366383 Revised journal version 38