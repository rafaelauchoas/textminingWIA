Artiﬁcial Intelligence 316 2023 103840 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint On robustness sparse counterfactual explanations adverse perturbations Marco Virgolin Evolutionary Intelligence Group Centrum Wiskunde Informatica Science Park 123 1098 XG Amsterdam Netherlands b Department Mathematics Geosciences University Trieste Weiss 2 34128 Trieste Italy Saverio Fracaros b r t c l e n f o b s t r c t Article history Received 8 April 2022 Received revised form 25 November 2022 Accepted 12 December 2022 Available online 16 December 2022 Keywords Counterfactual explanation Explainable machine learning Explainable artiﬁcial intelligence Robustness Uncertainty Counterfactual explanations CEs powerful means understanding decisions algorithms changed Researchers proposed number desiderata CEs meet practically useful requiring minimal effort enact complying causal models In paper consider interplay desiderata robustness enacting CEs remains feasible costeffective adverse events place sparsity CEs require subset features changed In particular study effect addressing robustness separately features recommended changed We provide def initions robustness sparse CEs workable incorporated penalty terms loss functions discovering CEs To carry experiments create release code ﬁve data sets commonly ﬁeld fair explainable machine learning enriched featurespeciﬁc anno tations sample meaningful perturbations Our experiments CEs robust adverse perturbations place worstcase intervention prescribe require larger cost anticipated impossible However accounting robustness search process easily allows discovering robust CEs systematically Robust CEs ad ditional intervention contrast perturbations costly nonrobust CEs We ﬁnd robustness easier achieve features change posing impor tant point consideration choice counterfactual explanation best user Our code available httpsgithub com marcovirgolin robust counterfactuals 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 1 Introduction Modern Artiﬁcial Intelligence AI systems rely machine learning models ensembles decision trees deep neural networks 13 contain thousands billions parameters These large models appealing proper training regularization regimes unmatched smaller models 45 However large models perform myriads computations diﬃcult interpret predict behavior Because large models called blackbox models ensuring use highstakes applications medicine ﬁnance fair responsible challenging 67 Corresponding author Email address marcovirgolincwinl M Virgolin httpsdoiorg101016jartint2022103840 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 The ﬁeld eXplainable AI XAI studies methods dissect analyze blackbox models 89 methods generate interpretable models possible 10 Famous methods XAI include feature relevance attribution 11 12 explanation analogy prototypes 1314 focus work counterfactual explanations Counterfactual explanations enable reason contrast analogy ways input given black box model needs changed model different decision 1516 A classic example counterfactual explanation Your loan request rejected If salary 60 000 instead 50 000 debt 2500 instead 5000 request approved A user obtains unfavorable decision attempt overturn intervening according counterfactual explanation Normally search counterfactual explanations formulated optimization problem Sec 21 formal description Given feature values user starting point seek minimal changes feature values result point blackbox model makes different oftentimes speciﬁc favorable deci sion We wish changes minimal reasons learn behavior blackbox model neighborhood data points assess fairness guaranteed general 17 hope putting counterfactual explanation practice means reallife intervention require minimal effort For counterfactual explanations useful desiderata requiring minimal feature changes need taken account Sec 9 18 In paper consider desideratum important usability counterfactual explanations ro bustness adverse perturbations By adverse perturbations mean changes feature values happen unforeseen circumstances users control making reaching desired outcome longer possible requiring user effort originally anticipated These unforeseen circumstances origins time delays mea surement corrections biological processes For example counterfactual explanation improving patients heart condition prescribes lowering patients blood pressure chosen treatment need employed longer turn futile patient genetic predisposition resist treatment examples Sec 51 choices coding experiments robust_cfedataprocpy We adverse perturbations happen seek counterfactual explanations ro bust perturbations A particular novelty work distinguish perturbations impact features counterfactual explanations prescribe change note features irrele vant changed differently prescribed counterfactual explanation address Sec 23 This counterfactual explanations normally required sparse terms intervention prescribe subset features changed better usability Sec 21 As shown making discrim ination allows improve effectiveness eﬃciency robustness accounted Consequently need consider carefully counterfactual explanation pursue based robust features change In summary paper makes following contributions 1 We propose workable deﬁnitions robustness counterfactual explanations concern respectively fea tures prescribed changed kept 2 We release code support investigations ﬁve existing data sets annotated perturbations plausibility constraints tailored features type user seeking recourse 3 We provide experimental evidence accounting robustness important prevent adverse perturbations making hard impossible achieve recourse counterfactual explanations adverse perturbations sampled distribution necessarily worstcase ones 4 We robustness features change far reliable computationally eﬃcient account robustness features 5 Additionally propose simple effective genetic algorithm outperforms existing gradientfree search algorithms discovery counterfactual explanations The algorithm supports plausibility constraints imple ments proposed deﬁnitions robustness 2 Preliminaries In following introduce preliminary concepts reasoning robustness counterfactual explanations sparse sense In particular problem statement searching counterfactual explanations ii present notions perturbation robustness general terms iii introduce deﬁnitions C K sets partition features counterfactual explanation The following Secs 3 4 present main contribution paper notions robustness tailored sparse counterfactual explanations speciﬁc C K 21 Problem statement Let assume given point x x1 xd d number features Each feature takes values subset R case numerical feature subset N case categorical 2 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 feature For categorical features use natural numbers convenient way identify categories disregard ordering For example categorical feature gender 0 mean male 1 mean female 2 mean nonbinary Thus x Rd1 Nd2 d1 d2 d A counterfactual example1 point x point z Rd1 Nd2 given classiﬁcation blackbox machine learning model f Rd1 Nd2 c1 c2 ci decision class f z cid5 f x We wish z close x meaningful distance function δ problemspeciﬁc meets desiderata Sec 9 For example commonly distances capable handling numerical categorical features variants Gowers distance 19 Eq 9 20 variant thereof Often dealing classes impose f z t target class desire z Other times wish ﬁnd set counterfactual examples z1 zk possibly different classes obtain multiple means recourse simply gain information decision boundary f nearby x explain f s local behavior 152122 For sake readability provide formal deﬁnitions case features numerical x Rd d1 d d2 0 For completeness include explanations deal categorical features running text Furthermore assume feature independence While assumption rarely entirely met realworld practice commonly literature lack causal models works consider causality Sec 9 allows greatly simplify introduction concepts presented We discuss limitations arise assumption Sec 8 A counterfactual explanation represented description x needs changed obtain z In words counterfactual explanation prescription interventions reach respective counterfactual example For example assumption independence allnumerical features difference z x typically considered counterfactual explanation reach z x What particular form counterfactual explanations crucial discourse use z x simplicity We proceed considering following traditional setting simplicity exposition loss generality assume features preprocessed difference unit terms feature equivalent difference unit feature j users effort commensurate different features Alternatively account computation distance Eq 9 We seek explanation relative optimal zcid3 zcid3argminzδz x δz x z x1 λz x0 subject f z t z x P 1 In words δ linear combination weighed λ sum absolute distances feature values x z count feature values different x z Note zcid3 needs unique multiple optima exist Moreover difference z x abide plausibility constraints speciﬁed collection P We model plausibility constraints set speciﬁcations relative feature concerning zi xi allowed 0 0 cid5 0 feature increase decrease change categorical features consider For example private individual wishes granted loan constraints specify reasonably intervene change value currency feature called mutable actionable counterfactual explanations zi xi 0 representing currency value Similarly individuals age increase decrease zi xi 0 representing age We particularly consider L1norm term 1 δ Eq 1 reasonable think independent features total cost intervention effort user sum costs intervention feature separately costs grow linearly Some works 2023 choose L2norm 2 known Euclidean norm instead L1norm deﬁnitions robustness given paper easily adapted L2norm Regarding L0norm term 0 δ Eq 1 term explicitly promotes form sparsity seeks minimize features different value z x This desirable oftentimes user reasonably focus intervene limited number features amounts larger total cost terms L1 compared intervention features 24 22 Perturbations robustness Unforeseen circumstances inﬂation lead different intervention needed compared originally prescribed counterfactual explanation increase savings 1000 granted credit access Thus instead reaching z intended counterfactual explanation different point z obtained Note effects unforeseen circumstances impact feature values circumstances need encoded feature values In fact focus extent feature values perturbed circumstances cid6 cid6 1 Many authors use x manuscript readability represent counterfactual example x instead z We chose z overload notation superscripts later 3 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 1 Example considering robustness perturbations seeking counterfactual examples The red green areas respectively represent high risk low risk classiﬁcations cardiac condition according model f The patient represented x high risk Three possible counterfactual examples shown different interventions white arrows zb treating blood pressure zv treating vitamin deﬁciency zbv treating We assume know maximal extent perturbation reasonable risk blood pressure vitamin deﬁciency natural physiological events This allows deﬁne blue areas surrounding counterfactual example Perturbations w counterfactual examples lead point blue area zv best terms proximity x blue area partly overlaps red area This means exist w zv w leads point red area invalidating counterfactual explanation In cases important estimate additional intervention possible zv reached cost For interpretation colors ﬁgures reader referred web version article cid6 p cid2 cid3 p 1 1 d p Let deﬁne vector w z z perturbation counterfactual example z We assume perturbations impact feature sampled distribution P w interested controlling robust large magnitude perturbations reasonable risk For example normallydistributed perturbations want consider values sampled 95th 99th percentile We assume deﬁne vector p 0 represent respectively smallest negative largest positive perturbations reasonably happen ith feature For example ith feature represents tells blood pressure lower conse blood pressure patient p quence dehydration p tells blood pressure raise consequence antiinﬂammatory drug intake Clinicians able deﬁne information experience retrieve need p Note ith medical literature In general magnitudes p feature categorical decreases increases p explained numerical features longer meaningful For categorical features assume p contains elements represent categorical perturbations possible feature pi set indices represent categories p p 0 p p cid5 p d 1 p Under problem setting considered Sec 21 perturbations impact counterfactual explanation deﬁne reached z perturbations An example illustrated cid6 box hyperrectangle possible points z Fig 1 We deﬁne concept pneighborhood z follows Deﬁnition 1 pneighborhood pneighbors counterfactual example Given model f point x respective counter factual example z vector possible perturbations p pneighborhood z set cid4 N cid6 z z cid6 zi p zi p cid5 A point z cid6 N z cid6 cid5 z called pneighbor z 2 Not perturbations problematic Our goal study robustness adverse perturbations cid6 cid5 t In words wish seek counterfactual examples z fewest possible f z w f z pneighbors perturbations cause classiﬁcation performed f different t When happens counterfactual explanation invalidated perturbation However case invalidation permanent exist intervention new counterfactual explanation adheres constraints P allows overcome invalidation Therefore work seek discover counterfactual explanations robust sense invalidated additional intervention remains possible ii cost additional intervention small Unforeseenly f assumed general model necessarily linear following argument holds 4 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Proposition 1 For general f information classiﬁcation pneighbor f z provides information classiﬁcation pneighbor f z cid6 cid5 f z cid6cid6 z cid6 f z z cid6cid6 cid6 boundary N interior N Proof We preclude model f rem 25 f represent function Thus f represent Swiss cheeselike function example f z z example neural network Under universal approximation theo cid6cid6 cid6 e e ε1 εd different zerovector small εi cid2 cid6 cid5 f z cid6cid6 z cid6 This proposition means information regularity smoothness f available check cid6 cid5 pneighbor z t Checking neighbors typically feasible soon features real valued Thus best approximate approach For example MonteCarlo sampling approach batch random points N considered hoping batch representative points N As sections better strategy designed sparsity considered z assess invalidate explanation z f z cid6 We conclude section noting perturbations described far absolute independent starting point x counterfactual consideration z intervention entailed counterfactual explanation z x Pertur bations feature depend xi zi sampled distribution P w ixi zi For example market ﬂuctuations return investment smaller anticipated 5 expected value Such type relative perturbations entail different pneighborhoods different x z For simplicity loss generality proceed assuming perturbations absolute We explain included relative perturbations annotations experiments Sec 5 23 Sparsity features C K We use form sparsity mentioned Sec 21 partition features sets As mentioned sparsity important desideratum reasonable expect user realistically intervene track features achieve recourse Given speciﬁc counterfactual explanation z point x set containing indices features values change C 1 d zi cid5 xi complement set indices features values kept K 1 d zi xi Typically suﬃciently large λ plausibility constraints speciﬁed P K cid5 Note proposed partitioning C K implicitly assumes features relevant counter factual explanation If certain features irrelevant perturbations features effect f s decision features need accounted assessing robustness This means accounting irrelevant features makes assessing robustness computationally expensive needed However f considered blackbox cautiously assume features relevant assessing robustness We proceed accounting perturbations respective robustness separately features C K Accounting robustness separately important assessing robustness features C far eﬃciently effective features K Knowing multiple counterfactual explanations user want choose counterfactual explanation ﬁts himher best based robustness exhibits terms C K In section present ﬁrst notion robustness concerns C 3 Robustness C We begin focusing features counterfactual explanation instructs change features indices C Recall assume vector maximal perturbation magnitudes p deﬁned This leads following deﬁnition Deﬁnition 2 Cperturbation Given point x respective counterfactual example z vector maximal magnitude perturbations p Cperturbation counterfactual explanation z x vector 1 wc d p wc wc cid6 p wc wc 0 C C 3 4 wc 0 wc zerovector In words Cperturbation perturbation acts features C features Next use concept Cperturbations introduce Csetbacks 5 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 2 Example Csetbacks The red green areas represent high risk low risk classiﬁcations cardiac condition according model f The patient represented x high risk Two treatments possible administered jointly drug incompatibility sparsity intervention needed treatments pursued The closest optimal counterfactual example zv concerns treating vitamin deﬁciency white arrow pointing Another counterfactual example zb concerns treating blood pressure white arrow pointing left Maximal Csetbacks shown counterfactual examples blue dashed segments The setbacks counterfactual examples invalid However counterfactual examples reached additional intervention treatment administration Indeed accounts possibility maximal Csetbacks place total cost original intervention additional intervention remedy perturbation 3 25 reach zv larger total cost reach zb 4 1 Fig 3 Onedimensional example Csetback advantageous magnitude perturbation exceeds magnitude intervention Deﬁnition 3 Csetback A Csetback counterfactual explanation z x Cperturbation wc p p 0 wc 0 wc 0 zi xi 0 zi xi 0 C 5 We denote Csetbacks wcs p In words Csetback Cperturbation element perturbation wcs opposite sign counterfactual explanation zi xi We interpret meaning Csetbacks wcs vectors push user away z x direction intervention Furthermore maximal Csetback denoted wcs zi xi 0 wcs max Csetback elements correspond features C maximal magnitude wcs zi xi 0 An example given Fig 2 Csetbacks arguably interesting Cperturbations Csetbacks subset perturbations plays user In fact certain Cperturbations advantageous enabling reach z intervention zi xi matches To account robustness originally provisioned sign wc interested understanding perturbations prevent reach z proceed focusing exclusively Csetbacks It important note Csetbacks advantageous allows perturbations larger magnitude intervention wcs zi xi Csetback lead point precedes x terms direction intervention For point intervention costly originally planned entirely needed point target class Fig 3 Advantageous situations interesting robustness counterfactual explanations overturned perturbations interesting pursue We consider Csetback elements capped wcs zi xi allowed 26 discuss aspect In nutshell wcs zi xi Perhaps interesting scenario considering Csetbacks dealing zcid3 counterfactual example p optimal minimizes Eq 1 ideal outcome The following simple result holds zcid3 Proposition 2 For Csetback wcs zcid3 wcs zcid3 xi f zcid3 wcs cid5 t 6 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Proof We use reduction ad absurdum Let assume opposite said Proposition 2 exists wcs cid6 x δzcid3 wcs x δzcid3 x f zcid3 wcs t Let z cid6 In words z target class closer x zcid3 This contradicts fact zcid3 optimal cid2 cid6 t By construction wcs δz cid6 zcid3 wcs f z Now Proposition 2 guaranteed Csetback wcs happens zcid3 resulting point longer classiﬁed t Intuitively natural consequence fact optimal counterfactual examples lay border decision boundary optimal Also zcid3 optimal respective L0 component distance x zcid3 minimal features C wcs relevant classiﬁcation Given premises important understand invalidation zcid3 averted additional intervention cost intervention minimized It important note invalidation counterfactual explanation Csetback averted additional intervention reach intended zi C possible To consider fact intervention entailed counterfactual explanation z x adhere plausibility constraints speciﬁed P z x possible counterfactual explanation Since Csetbacks aligned direction original intervention point z wcs x x z meet P It suﬃces apply additional intervention originallyintended direction recover desired counterfactual example Under L1norm choice δ Eq 1 cost associated additional intervention needed overcome Csetback wcs simply wcs1 Since invalidation Csetbacks dealt additional intervention reasonably assume user keeps track value xi changes C course intervention know stop intervention follows necessity counterfactual examples far decision boundary terms features C Note contrast prior work aspects robustness counterfactuals possibility additional intervention considered counterfactual examples required far decision boundary terms features Sec 9 Thus seeking counterfactual examples invalidated Csetbacks seek counterfactual examples additional intervention needed contrast Csetbacks minimal To end use Proposition 2 order seek counterfactual examples optimal require minimum intervention cost additional cost contrast maximal Csetbacks wcs max factored In following deﬁnition highlight Csetbacks depend speciﬁc z x determine C avoid confusion use function notation W cs maxz x place wcs max Deﬁnition 4 Optimal counterfactual example Csetbacks Given model f point x vector p point zcid3c cid11 z W cs maxz x x cid12 6 zcid3c W cs maxzcid3c x argmincid11 maxzx optimal counterfactual example Csetbacks zW cs cid12δ This deﬁnition gives way seek multiple exist counterfactual explanation entails minimal intervention cost accounting maximal Csetbacks Indeed suﬃces equip given search algorithm Eq 6 perform following steps 1 z evaluated compute respective wcs max 2 instead computing δz x compute δz wcs max x 3 end search return point minimizes distance zcid3c Performing computations mentioned takes linear time number features O d need build wcs max step 1 subtract z prior computing δ step 2 given z f evaluated z This relatively fast demonstrated B32 especially compared situation described Sec 22 need use f predict class number neighbors z Note Eq 6 setbacks subtracted counterfactual examples computing δ account fact cost increase recall construction Csetbacks Deﬁnition 3 4 Robustness K We consider K set concerning features kept current value Mirroring notion K 0 K Similarly cast concept neighborhood Deﬁnition 1 consider Kperturbations Cperturbation Deﬁnition 2 deﬁne Kperturbation vector wk p wk leading wk p Deﬁnition 5 Kneighborhood Kneighbors counterfactual example Given model f point x respective counter factual example z vector possible perturbations p Kneighborhood z p set cid14 cid13 7 K zi p zi cid6 cid6 z z cid6 z cid6 K z A point z zi p K cid6 cid5 z called Kneighbor z 7 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 4 Example Kperturbations The counterfactual example zv vulnerable Kperturbations lead red area true zb If plausible reduce blood pressure Kperturbations zv lead permanent invalidity Else resolved additional intervention terms blood pressure For categorical feature K neighborhood built swapping zi possibilities listed pi p pi set containing categories perturbations lead Next use K deﬁne concept vulnerability Kperturbations Deﬁnition 6 Vulnerability Kperturbations Given model f point x vector p counterfactual example z vulnerable Kperturbations z cid6 Nz p f z cid6 cid5 f z Informally deﬁnition says z vulnerable Kperturbations decision boundary surrounding z suf ﬁciently loose respect features K Fig 4 shows example The reason vulnerability Kperturbations particularly important differently case Cperturbations Kperturbation invalidate counter factual explanation permanently In fact Kperturbation changes z different direction intervention Thus Kperturbation lead point z exists plausible intervention reach originally intended z cid6 For example consider feature represent inﬂation mutable actionable feature feature changed global market trends user P state user intervention exist change P imposes zi xi 0 However unforeseen circumstance ﬁnancial crisis 2008 lead large inﬂation increase p 0 Consequently impossible user obtain desired loan bank hand certain loans inﬂation high Now recall reason Deﬁnition 4 case Cperturbations Proposition 2 holds exist points class t x optimal counterfactual example zcid3 The hold Kperturbations features K orthogonal direction intervention happen maximal cid6 t nonmaximal perturbation feature perturbation feature K leads point z cid6cid6 cid5 t Thus checking maximal perturbations longer suﬃcient check lead point z instead points Kneighborhood K f z f z cid6cid6 cid6 As mentioned Sec 22 checking point neighborhood feasible Thus propose approximate assessment Krobust nonvulnerable perturbations K counterfactual explanations MonteCarlo sampling Let 1 f z K 0 1 indicator function returns 1 Kneighbors share class z f z 0 Taken random sample m Kneighbors deﬁne following score Krobustness scorez m 1 m mcid15 i1 cid6 1 f zz 8 We remark Krobustness scorez m 1 guaranteed z Krobust score approximation Still score determine counterfactual examples preferable pursue associated smaller risk adverse perturbations invalidate permanently 5 Experimental setup In section ﬁrstly preparation data sets experiments Secondly search algorithms considered ﬁnding nearoptimal counterfactual explanations Lastly loss function considered incorporate proposed notions robustness 8 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table 1 Considered data sets n d resp d2 indicate number observations features categorical preprocessing The column t target class simulated user Plausib constr reports number plausibility constraints allow features increase remain equal decrease The column Perturb reports number perturbations concerning numerical N categorical C features Finally Accrf Accnn report average ﬁve folds test accuracy random forest neural network models Data set abbrev Credit risk Cre Income Inc House price Hou Productivity Pro Recidivism risk Rec n 1000 1883 506 1196 2000 d 20 12 13 12 10 d2 Classes User 6 7 1 5 6 High low High low High low High med low High low Individual Individual Municipality Company Inmate t Low High Low High Low Plausib constr 3 8 0 2 3 0 0 3 1 0 0 0 2 2 0 Perturb Accrf Accnn N6 C0 N4 C4 N11 C0 N5 C2 N3 C2 076 083 093 079 080 075 082 093 070 078 Table 2 Examples perturbations manually annotated considered data sets We relative perturbations respect value feature intended counterfactual example z consideration search algorithm Dset Feature Cre Inc Hou Pro Rec Savings Marital status Crime rate Overtime Age Decrease Increase Categories 10 10 single married widowed 1 3 0 5 3 2 Note Might happen save relative intended Unforeseen change proposal divorce death Relative increase decrease Up 3 days overtime needed Judicial delays 2 years 51 Data sets Table 1 summarizes data sets consider For data set assumption type user seeks recourse user private individual seeking increase income company seeking improve productivity employees Based manually deﬁne target class t set plausibility constraints P interventions reasonably plausible collection p maximal magnitudes perturbations sampled consider uniform normal distributions We named data sets Table 1 represent purpose Originally Credit risk abbreviated Cre known South German Credit Data 27 recent update corrects inconsistencies popular Statlog German Credit Data 28 Income Inc called Adult Census income 2930 Housing price Hou known Boston housing 31 research fairness interpretability features raises ethical concerns 32 Productivity Pro concerns productivity levels employees producing garments 33 Lastly Recidivism Rec data set collected investigation ProPublica possible racial bias commercial software COMPAS intends estimate risk inmate reoffend 34 Examples recent works fair explainable machine learning adopted data sets 203540 We preprocess data sets similarly literature This includes removal redundant features observations missing values limiting number observations considered Rec Regarding annotations perturbations numerical features perturbations increase decrease feature value absolute relative terms compute relative perturbations respect z For example numerical feature capitalgain Inc assume perturbations happen lead relative 5 increase 10 decrease feature based value achieve feature For categorical features deﬁne absolute perturbations possible changes category conditioned current category The choices build p subjective elaborate Sec 8 We sample perturbation uniform normal distribution indicated Sec 7 Table 2 shows examples maximal perturbations annotated As mentioned deﬁne plausibility constraints P data set Each constraint speciﬁc feature For ith numerical feature possible constraints zi xi 0 zi xi 0 zi xi 0 For ith categorical feature possible constraints zi xi Full details preprocessing deﬁnition p P documented form comments code robust_cfedataprocpy 52 Blackbox models We consider random forest neural networks standard multilayer perceptron architecture blackbox ma chine learning models f We use Scikitlearns implementations 41 We assume access predictions f information model parameters gradients Our experiments repeated stratiﬁed ﬁve fold crossvalidation model obtained gridsearch hyperparameter tuning Once trained models obtain test accuracy varying 70 90 average different data sets meaningful decision bound aries learned See Appendix A details hyperparameter tuning accuracy models different data sets For discovery counterfactual examples consider observations x f x cid5 t test sets crossvalidation 9 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table 3 Settings considered counterfactual search algorithms For NeMe set max imum number iterations 100 achieve commensurate runtimes CoGS settings default For DiCE consider conﬁgurations b The loss DiCE Eq 9 CoGS Setting Population size Num generations Tournament size smut Value 1000 100 2 25 Setting Method Total CEs Max iterations Loss weights DiCE b Value Genetic 20 b 100 500 b 100 Default b 05 prox 05 spars 0 div GrSp Setting Num layer First radius Decrease radius Sparse Value 2000 01 10 True LORE Setting Population size Num generations Discrete use probabilities Continuous function estim Value 1000 10 False False 53 Counterfactual search algorithms To provide experimental results concerning robustness Sec 7 ﬁrstly seek counterfactual search algorithm performs best overall candidates To end consider benchmark following algorithms literature operate blackbox f Diverse Counterfactual Explanations DiCE 22 Growing Spheres GrSp 23 LOcal Rulebased Explanations LORE 2042 NelderMead method NeMe 4344 Furthermore devise algorithm genetic algorithm Counterfactual Genetic Search CoGS2 The settings algorithms reported Table 3 We algorithms Note algorithms heuristics guarantee discovering optimal minimal distance counterfactual examples given nature search problem general blackbox f 531 DiCE DiCE actually library includes algorithms random sampling KDtree search fastretrieval data structure built points training set genetic algorithm Of consider performed substantially better preliminary experiments simply refer DiCE DiCE conﬁgured return collection counterfactual examples single However algorithms consider return single counterfactual example Thus compare algorithms equal footing set DiCE return single counterfactual example We achieve ranking counterfactual example collection according loss function consideration explained Sec 54 picking bestranking point We consider different conﬁgurations DiCE Conﬁguration uses default settings allowing longer number iteration match computational budget given algorithms Conﬁguration b uses custom settings aligned similar CoGS DiCE CoGS genetic algorithms 532 GrSp GrSp greedy algorithm iteratively samples neighbors starting point x spheres L2 sense increasing radius counterfactual examples GrSp includes feature selection promote sparsity Unforeseenly GrSp handle numerical features To able use GrSp comparison let GrSp operate categorical features numerical ones categories encoded integers At end optimization transform numerical values categories rounding Note suboptimal artiﬁcial ordering introduced categories 533 LORE LORE works generating neighborhood x random search genetic algorithm ﬁnding multiple counterfactual explanations different distance We consider variant adopts genetic algorithm performed substantially better preliminary experiments After neighborhood determined LORE ﬁts decision tree 2 httpsgithub com marcovirgolin cogs 10 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 5 Runtimes means 95 conﬁdence intervals counterfactual search algorithms considered data sets blackboxes random forest neural network The right plots zoomedout versions left ones Since path root decision tree leaf represents classiﬁcation rule AGE 34 SALARY_CATEGORY HIGH t LORE essentially returns multiple counterfactual explanations expressed rules To able compare algorithms return single counterfactual example build counterfactual example z taking shortest rule returned LORE applying rule starting point x rule set xs age salary 34 high respectively We conﬁrmed discussion authors applying LOREs rules result points actually classiﬁed t When happens perform 15 attempts generating counterfactual example shortest returned rule focusing numerical features prescribed certain value In particular applying rule x add subtract prescribed value term cid6 initially 3 doubled attempt Moreover LORE computationally expensive run set 10 Fig 5 fraction computation budget allowed algorithms Table 3 534 NeMe NeMe classic simplexbased algorithm gradientfree optimization Like GrSp NeMe naturally handle categorical features Thus use approximation GrSp encode categories integers let NeMe treat categories numerical values map values integers categories rounding end We use SciPys implementation default parameters 45 535 CoGS We design CoGS relatively standard genetic algorithm adapted search points neighboring x especially terms L0norm CoGS operates follows First initial population candidate solutions generated sampling feature values uniformly interval numerical features possible categories categorical features These intervals speciﬁed taken automatically training set With probability 2d d total number features feature value candidate solution copied x sampled Every iteration algorithm jargon evolutionary computation generation offspring solutions produced current population crossover mutation Following survival ﬁttest applied form population generation Our version crossover produces offspring solutions simply swapping feature values random par ents uniformly random Our version mutation produces offspring solution parent solution randomly altering feature values A feature value altered probability 1d left untouched If feature alter categorical category swapped category uniformly random If feature alter nu merical ﬁrstly random number r sampled uniformly random smut2 smut2 smut 0 1 hyperparameter represents maximal extent allowed mutations secondly original feature value changed adding r maxi mini maxi mini respectively maximum minimum values possible feature After crossover mutation quality ﬁtness offspring solutions evaluated loss function Eq 9 ﬁtness function minimization sought Finally use tournament selection 66 form population generation 11 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 We set CoGS allow plausibility constraints P speciﬁed If plausibility constraints mutation restricted plausible changes feature represents age increase If mutation makes numerical feature obtain value bigger maxi resp smaller mini value feature set maxi resp mini CoGS written Python relies heavily NumPy 67 speeding key computations For example popu lation encoded NumPy matrix crossover mutation implemented matrix operations 54 Loss We use following loss drive search counterfactual examples f z t treated integers 1 2 γ z x 1 2 γ z x 1 d z x0 d d1cid15 f z t0 zi xi maxi mini d2cid15 j z j x j0 9 10 The function γ equation Gowers distance 1946 features indexed numerical indexed j categorical values treated integers maximal minimal values numerical feature maxi mini taken training data set case provided extra annotations data sets The term z x0d promotes sparsity intervention like Gowers distance ranges zero The term requires execution machine learning model f simply returns zero f z t f z cid5 t 541 Incorporating robustness loss To seek robust counterfactual examples use notions described Sec 3 Sec 4 When optimizing robustness perturbations concerning C use Deﬁnition 4 maximal Csetbacks computed ﬂy candidate z contribution update contribution γ loss function When optimizing robust 2 1 Krobustness score ness perturbations concerning K compute Krobustness score Eq 8 add 1 loss In results presented use m 64 compute Krobustness score analysis impact m provided B3 6 Preliminary results choosing suitable counterfactual search algorithm This section reports benchmarking considered search algorithms identify overall best We repeat execution algorithm ﬁve times consider bestfound counterfactual example ﬁve repetitions We search counterfactual example x test sets ﬁve crossvalidation x f x cid5 t Since LORE takes longer execute algorithms Fig 5 perform repetitions instead ﬁve consider ﬁrst ﬁve x test set ﬁve folds Since DiCE CoGS support plausibility constraints use plausibility constraints comparison comparison DiCE COGS plausibility constraints provided Appendix B1 61 Runtimes Fig 5 shows runtime algorithms different data sets irrespective succeed fail ﬁnd counterfactual example point f predicts t The experiments run cluster computing nodes slightly different CPUs invite consider order magnitude runtimes exact numbers The ﬁgure shows random forest CoGS DiCE conﬁguration fastest algorithms fastest implementations GrSp NeMe competitive LORE slower execute algorithms When neural network inference times generally faster CoGS DiCEa GrSp NeMe competitive 62 Success discovering counterfactual examples Table 4 shows frequency counterfactual search algorithms succeed ﬁnding counterfactual ex ample point f predicts t CoGS variants DiCE succeed systematically algorithms GrSp performs thirdbest overall In particular GrSp ﬁnds counterfactual examples Hou data set single categorical feature Since GrSp intended operate solely numerical features result nicely supports hypothesis GrSp works features numerical Although LORE supports numerical categorical features perform better GrSp data sets limited number runs conducted LORE excessive runtime explained Lastly NeMe performs substantially worse algorithms 12 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table 4 Mean standard deviation ﬁve crossvalidation folds frequency counterfactual search algorithms succeed ﬁnding counterfactual example Plausibility straints considered algorithms support Alg t CoGS s e r o f DiCEa Cre 100 100 Inc 100 100 m o d n r f k r o w t e n l r u e n f DiCEb GrSp LORE NeMe CoGS DiCEa DiCEb GrSp LORE NeMe 100 046 011 056 020 008 003 100 089 006 020 013 005 002 100 100 100 100 100 087 007 052 020 014 007 100 025 004 028 016 011 003 Hou 100 100 100 100 068 020 004 005 100 100 100 100 068 010 009 001 Pro 100 100 Rec 100 100 100 086 004 024 020 003 001 100 030 015 060 038 014 002 100 100 100 100 100 051 012 076 023 011 004 100 049 010 084 023 051 003 Fig 6 Boxplots relative change loss respect CoGS GrSp LORE NeMe different data sets blackboxes success cases 63 Quality discovered counterfactual examples As benchmarking effort consider algorithm manages produce nearoptimal counterfactual examples smallest loss In particular report relative change loss bestfound counterfactual example respect loss obtained CoGS success cases Since consider successes term loss Eq 9 null f z t0 0 The relative change loss respect CoGS algorithm Alg LAlgz LCoGSz LCoGSz Fig 6 shows relative change loss DiCE GrSp LORE NeMe respect CoGS DiCE GrSp LORE typically ﬁnd points larger loss CoGS NeMe performs similarly CoGS NeMe seldom succeeds cf Table 4 This suggests NeMe explore small neighborhood x particularly fails counterfactual examples relatively distant x 13 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table 5 Mean standard deviation frequency bestfound ﬁve search repetitions counterfactual example accounting robustness accidentally robust wrt C K For numerical features consider match value tolerance level Tol 1 5 10 range feature Robustness Only C Only K Both C K Only C Only K Both C K t s e r o f m o d n r f k r o w t e n l r u e n f Tol 1 5 10 1 5 10 1 5 10 1 5 10 1 5 10 1 5 10 Cre Inc Hou Pro Rec 040 006 042 007 043 007 037 001 044 003 046 004 023 004 027 003 030 005 025 012 027 012 029 011 013 007 026 008 039 002 002 002 006 003 012 008 002 004 002 005 002 006 002 040 008 058 007 000 000 000 001 001 002 001 002 001 035 002 052 003 070 004 000 000 000 076 010 084 009 085 009 033 024 063 017 067 016 021 021 054 021 060 019 096 002 097 002 097 002 007 005 080 012 093 004 007 005 069 009 093 004 053 005 057 006 058 006 026 005 037 006 046 007 019 006 026 005 034 006 087 005 089 005 089 005 008 006 042 019 058 014 006 006 038 018 052 014 027 006 037 007 040 009 004 004 008 003 012 002 003 003 006 004 008 004 050 008 056 005 057 004 001 001 001 002 002 000 001 001 001 001 002 64 Conclusion benchmarking The results overall CoGS performs best DiCE particular DiCEa closest competitor terms speed success rates algorithm ﬁnds counterfactual examples substantially distant x larger loss CoGS GrSp good runtime generally ﬁnds closer counterfactual examples lower loss DiCE remains inferior CoGS terms distance loss success rate LORE worse success rate GrSp NeME worse Therefore use CoGS following experiments robustness We remark DiCE like CoGS supports speciﬁcation plausibility constraints We CoGS performs better DiCE plausibility constraints Appendix B1 7 Experimental results robustness We proceed presenting experimental results robustness perturbations C K jointly We focus results allow answer believe important research questions RQ1 Do need account robustness discover robust counterfactual examples RQ2 Does lack robustness compromise feasibility correcting perturbations additional intervention RQ3 Are robust counterfactual explanations advantageous terms additional tervention cost These questions addressed order subsections Because space limitations number additional results reported Appendix B including runtime taken account robustness wrt C K effect varying m computing Krobustness score We account plausibility constraints P following experiments We remark experiments CoGS succeeded discovering counterfactual example f predicts t having mean success rate 99 stdev 1 Rec data set f implemented neural network 71 RQ1 Do need account robustness discover robust counterfactual examples Table 5 shows frequency robust counterfactual examples discovered accidentally To realize compare bestfound counterfactual example discovered CoGS robustness accounted C Krobustness accounted indicated Sec 541 We frequency match indication robust counterfactual examples discovered accident Since numerical feature values differ slightly bestfound counterfactual examples consider values match suﬃciently close according tolerance level 1 5 10 range feature As reasonable expect results larger tolerance level zcid3 discovered accounting robustness matches respective discovered accounting robustness In general result depends data set consideration albeit arguably random forest neural network blackbox model f For brevity focus tolerance level 5 random forest On Inc bestfound counterfactual examples rarely match discovered accounting Crobustness 4 average tolerance 5 14 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 7 Mean frequency plausible additional intervention exists contrast perturbations reach intended counterfactual example uniformlydistributed categorical changes normally uniformlydistributed numerical changes f random forest Darker colors represent worse cases vice versa happens Hou 84 average tolerance For Krobustness like Crobustness result depends data set Importantly data sets frequencies high Crobustness Krobustness necessarily On Inc bestfound counterfactual examples rarely optimal Csetbacks match counterfactual examples discovered penalizing low Krobustness scores 40 average tolerance 5 This surprising C Krobustness orthogonal assumption feature independence The row shows bestfound counterfactual examples happen robust perturbations C K The frequencies clearly lower previous triplets rows Hou data set frequency discovering counterfactual example happens robust wrt C K chance relatively large 50 tolerance 5 When neural network instead random forest trends mentioned remain speciﬁc magnitudes differ For example accidental discovery robust counterfactual examples wrt C andor K lower Cre neural network compared random forest opposite holds Hou exceptions tolerance level 1 C Krobustness sought Overall result indicates lucky cases Hou f neural network unlikely discover robust counterfactual examples chance Hence wishes achieve robustness search explicitly instructed end In sections investigate achieving robustness actually important 72 RQ2 Does lack robustness compromise feasibility correcting perturbations additional intervention At point current works robustness counterfactual explanations typically consider extent ro bustness helps preventing invalidation counterfactual explanations Sec 9 In words consider point z given perturbing bestfound counterfactual example classiﬁed t For completeness report B2 Current works consider additional intervention allows correct perturbation obtain t exist cid6 Figs 7 8 frequency achieving intended counterfactual explanation remains possible random perturbations place The frequency computed applying counterfactual explanation outcome search 100 perturbations sampled uniformly random categorical possibilities categorical features normally stdev 01 uniformly numerical intervals numerical features deﬁned p We note similar results obtained choosing random forest neural network f As expected possible contrast Csetbacks happen direction intervention In stead perturbations concerning K lead z plausible intervention exists reach originally intended counterfactual example We report result perturbations concerning C K time cid6 15 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 8 Mean frequency plausible additional intervention exists contrast perturbations reach intended counterfactual example uniformlydistributed categorical changes normally uniformlydistributed numerical changes f neural network Darker colors represent worse cases construction result perturbations concerning K Like results Sec 71 extent perturbations K reduce possibility additional intervention depends data set On Pro perturbations contrasted additional intervention plausibility constraints Table 1 Conversely Rec perturbations K impossible reach originallyintended counterfactual example Krobustness accounted In fact accounting Krobustness generally improves chances additional intervention possible times substantially Inc Hou Rec Cre represents exception accounting Krobustness performs similar worse accounting This suggests cision boundary learned f data set smooth making use Krobustness score coarse approximation helpful Generally accounting perturbations C help achieving substantial robustness perturbations K Hou This suggests Hou f learns decision boundaries incor porate interesting interactions certain features Importantly accounting Crobustness accounting Krobustness substantially compromise gains obtained accounting Krobustness perturbations C admit additional intervention Overall results accounting robustness crucial ensure perturbations happen additional intervention obtain t remains possible 73 RQ3 Are robust counterfactual explanations advantageous terms additional intervention cost We present following results terms relative cost ratio cost intervention reach intended z random perturbations place initial cost reaching z x plus cost reaching z perturbed z ideal cost cost incurred complete absence perturbations cost reaching z x We compute relative cost notions robustness accounted The ideal cost computed accounting robustness The cost modeled 1 ﬁrst Eq 9 cid6 t assume additional intervention needed additional cost zero Moreover f z relative cost 1 2 γ z x 1 zx0 d 2 cid6 Figs 9 10 random forest neural network respectively robustness accounted leftmost triplets boxes plot relative cost dramatically large In words additional interven tion correct perturbations extremely costly Whether relative cost increases perturbations C blue boxes K orange boxes depends data set For example perturbations K largest effect Rec C largest effect Inc far types distribution types f For random forest neural network relative cost ranges 5 10 ideal cost 100 Inc perturbations C accounting robustness 16 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 9 Cost terms different conﬁgurations accounting robustness different perturbations relative ideal cost random forest Due perturbations t perturbations relative cost notion robustness accounted label None typically larger right notion robustness accounted matching color box label The vertical axis Inc logarithmic scale Fig 10 Cost terms different conﬁgurations accounting robustness different perturbations relative ideal cost neural network Due perturbations relative cost notion robustness accounted label None typically larger right notion robustness accounted matching color box label The vertical axis Inc logarithmic scale When accounts notion robustness meant deal respective type perturbation relative cost decreases substantially Accounting Crobustness second blue box left plot counters perturbations C data sets On Inc particular relative cost improves orders magnitude As Sec 72 accounting perturbations K Krobustness score remain insuﬃcient observed Cre Inc types f Again likely limitation simple heuristic K robustness score deal Krobustness Accounting robustness wrt C resp K general lead smaller relative cost perturbations K resp C We conﬁrm general trend statistical testing Appendix C Lastly 17 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig 11 Cost accounting robustness relative accounting robustness ideal cost perturbations place Note rare relative costs smaller 1 lack optimality search algorithm Fig 12 Reduction sparsity relative number features caused accounting robustness Note different vertical axes accounting C Krobustness rightmost triplets boxes plot offers protection lower relative cost situations types perturbations place In general data sets types f distribution relative costs perturbations C K place C Krobustness accounted right green box plot better distribution perturbations place notion robustness accounted leftmost green box plot Since ideal cost computed notion robustness accounted relative costs robustness accounted comes fact robust counterfactual examples generally farther away x nonrobust ones Fig 11 shows cost increase comes solely accounting robustness considered data sets types f perturbation taking place We remark values smaller 1 happen discovered counterfactual examples suboptimal Importantly ﬁnd cost accounting robustness 1 7 ideal cost accounting robustness In general signiﬁcantly smaller increase incurred perturbations place robustness accounted reported generally 5 10 ideal cost 100 Lastly Fig 12 shows cost increase comes counterfactuals sparse For certain data sets Cre random forest neural network robust counterfactual explanations sparse nonrobust ones In general robust counterfactual explanations tend sparse depending choice f 18 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 type robustness accounted The reduction sparsity moderate substantial For example 10 features need change account K perturbations Hou random forest approximately feature Instead 30 features need change account K perturbations Rec neural network approximately features The fact sparsity decreases seeking robust counterfactuals natural consequence adopting linearization objectives Sec 54 Thus reduction sparsity tackled tuning weight attributed L0norm Eq 9 These results conﬁrm robust counterfactual explanations principle costly pursue nonrobust ones random perturbations place robust counterfactual explanations require additional inter vention nonrobust ones 8 Discussion Our experimental results provide positive answer research questions In general counterfactual explanations robust terms features value prescribed changed Crobustness value prescribed kept Krobustness Moreover nonrobust counterfactual explanations susceptible impossible user remedy perturbations additional intervention cost additional intervention larger nonrobust counterfactual explanations robust ones Ultimately clear accounting robustness important Our experimental results suggest accounting robustness features C tempers perturbations C simi larly accounting robustness features K tempers perturbations K Moreover f learn nonlinear feature interactions accounting C K limited effect contrasting perturbations K resp C Only cases Hou robustness wrt C substantial repercussions effect perturbations K vice versa In addition counterfactual search algorithm guarantee discovered counterfactual example optimal experimentally incorporating Deﬁnition 4 loss Sec 541 produces strong resilience additional cost Sec 73 perturbations features C Besides effective implementation Deﬁnition 4 eﬃcient B32 What results seeking robustness respect features K problematic This Proposition 1 fact features K aligned direction intervention Thus proposed control Krobustness approximation Krobustness score We seeking counterfactual examples maximize Krobustness score suﬃcient obtain good resilience perturbations features K Moreover Krobustness score requires sample evaluate f multiple points far expensive computing Deﬁnition 4 Therefore future work consider better method Krobustness score For example information f available information provide guarantees neighborhood z Theorem 2 40 linear f The assumption features independent simplistic literature small number works assume causal model available 4748 Under assumption feature independence models neighborhood counterfactual example box L1 hypersphere L2 However certain features causal dependency features neighborhood morphs possibly complex shapes dependency linear Importantly feature depends j change j having implicitly changes Similarly perturbation happening j implicitly alter As framework currently assumes independence important study extent separation C K remains possible meaningful For realworld problems reasonable expect exist groups features truly independent groups features Thus study robustness C K carried higher level feature groups future work There number aspects worth mentioning wishes implement research work like coun terfactual explanations practice including work For example use L1norm Gowers distance measure intervention cost In fact literature works typically choose distance measure Gowers L2 norm instead variants Sec 9 Of course realistic implementation intervention cost need reﬁned mixing different types norms Similarly wish use different distributions sample meaning ful perturbations opposed uniform normal synthetic experiments different functions deﬁne maximal extent perturbation account distribution feature values For example smaller dense areas Other desiderata need denser areas feature p included seeking counterfactuals practice 4950 including accounting multiple types robustness time related uncertainties f 5152 p Lastly subjective choices deﬁne perturbations p plausibility constraints P data sets We choices best based reading metainformation web sources papers data sets We doubt domain experts better choices Nevertheless argue important limitation long community agrees choices reasonable suﬃce provide sensible test bed benchmarking robustness Hopefully researchers ﬁnd annotations useful future experiments robustness counterfactual explanations Similarly hope researchers ﬁnd CoGS interesting algorithm benchmark 19 M Virgolin S Fracaros 9 Related work Artiﬁcial Intelligence 316 2023 103840 A number works literature propose new desiderata largely orthogonal notions robust ness important enhance practical usability counterfactual explanations For example Dandl et al 49 consider proximity z x according different distances training points x suﬃciently close z reasonably belong training data distribution A similar desideratum considered 21 53 work employs neural autoencoders end 54 remarks importance sparsity explanations concepts pertinent negatives minimal features different conﬁdently predict given class pertinent positives minimal features help correctly identifying class Laugel et al 3650 require z reached training point x having cross decision boundary f z result artifact decision boundary f In 47 22 counterfactual explanations studied lens causality For recent surveys counterfactual explanations reader referred 551656 cid6 cid6 We focus works deal notion robustness andor perturbations explicitly Artelt et al 57 present theoretical results effect perturbations linear f evaluate effect different type perturbations Gaussian uniform masking classiﬁers ﬁnd counterfactual explanations obey plausibility straints robust counterfactual explanations Differently Artelt et al consider sparsity optimize robustness The work Karimi et al 48 extends 47 consider possible uncertainties causal modeling In 17 shown malicious actor principle jointly optimize small perturbations model f applying perturbations points speciﬁc group white males respective counterfactual explanations costly normal fact counterfactual explanations conceptually similar adversarial examples 5860 Some works consider forms robustness counterfactual explanations respect changes f z classiﬁed t f instead f 5161 updates f data distribution shift temporal geospatial nature 6252 In 63 robustness counterfactual explanations stud ied context differentiallyprivate support vector machines Dominguez et al 40 consider counterfactual explanations remain valid presence uncertainty x account causality We note Dominiguez et al consider neighborhood uncertainty x akin Deﬁnition 1 fact sort neighborhoods common tools posthoc explanation methods Anchor explainer 64 seeks representative points class assessing prediction f points neighborhood Zhang et al 65 propose counter factual search method based linear programming works neural networks ReLU activations work seen lens robustness method produces regions points share desired class Finally contemporary work Fokkema et al 26 provide important theoretical results counterfactual explanations XAI methods feature attribution ones dramatically different small perturbations applied starting point x general point explain cid6 To best knowledge exists work prior attempts exploit sparsity assessing robustness sparsity important property counterfactual explanations Moreover existing works typically consider robustness helps preventing counterfactual explanations invalid consider additional intervention possible assess associated cost 10 Conclusion Counterfactual explanations help understand blackbox AI systems reach certain decisions intervention possible alter decisions For counterfactual explanations useful practice studied robust adverse perturbations naturally happen unforeseen circumstances ensure intervention prescribe remains valid potential additional intervention cost needed remains limited We presented novel notions robustness concern adverse perturbations features counterfactual explanation prescribes change Crobustness Krobustness respectively We annotated ﬁve existing data sets reasonable perturbations plausibility constraints developed competitive counterfactual search algorithm search robust counterfactual explanations Our experimental results counterfactual explanations happen robust accident Consequently adverse perturbations place counterfactual explanations require larger cost realized anticipated impossible user achieve recourse Our deﬁnitions robustness incorporated search process robust counter factual explanations discovered We shown Crobustness accounted eﬃciently effectively true Krobustness This aspect taken account choosing coun terfactual explanation best user Overall robust counterfactual explanations resilient invalidation require smaller additional intervention contrast perturbations Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper 20 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table A1 Hyperparameter settings considered tuning random forest Name No trees Min samples split Max features Options 50 500 2 8 d d Table A2 Hyperparameter settings considered tuning neural network Name Learning rate Max iterations Solver Options 00001 001 200 1000 Adam SGD Table A3 Test accuracy hyperparametertuned random forests acting considered data sets ﬁvefold blackbox models f crossvalidation Fold 0 1 2 3 4 Avg Cre 071 078 078 074 076 076 Inc 086 082 079 082 083 083 Hou 093 090 091 091 097 093 Pro 079 077 078 082 078 079 Rec 080 082 078 077 080 080 Table A4 Test accuracy hyperparametertuned neural networks acting considered data sets ﬁvefold blackbox models f crossvalidation Fold 0 1 2 3 4 Avg Cre 074 078 073 078 074 075 Inc 083 082 080 082 082 082 Hou 094 093 091 091 096 093 Pro 062 070 069 077 072 070 Rec 078 079 075 078 079 078 Data availability The data available github repository linked abstract Acknowledgements We thank dr Stef C Maree insightful early discussions This work use Dutch national einfrastructure support SURF Cooperative grant EINF2512 Funding This publication project Ro bust Counterfactual Explanations project number EINF2512 research program Computing Time National Computer Facilities partly ﬁnanced Dutch Research Council NWO Appendix A Hyperparameter optimization random forest neural network To obtain blackbox model f given crossvalidation fold train random forest model neural network multi layer perceptron classiﬁcation optimized gridsearch hyperparameter tuning ﬁvefold crossvalidation training set The hyperparameter settings considered listed Tables A1 A2 Scikit learns default v 101 For random forest onehot encode categorical features training querying random forest model code robust_cfeblackbox_with_preprocpy For neural network additionally scale numerical features mean zero standard deviation The performance tuned random forest folds shown Table A3 respective neural network shown Table A4 21 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table B1 Mean standard deviation ﬁve crossvalidation folds frequency CoGS variants DiCE succeed ﬁnding counterfactual example plau sibility constraints f r f n n f Alg CoGS DiCEa DiCEb CoGS DiCEa DiCEb Cre 100 100 100 100 100 100 Inc 100 100 100 100 100 004 100 Hou 100 100 100 100 100 100 Pro Rec 100 100 020 400 100 100 100 100 099 006 100 005 100 004 100 080 400 Fig B1 Boxplots relative change loss respect CoGS variants DiCE plausibility constraints different data sets blackboxes success cases Appendix B Additional results We provide additional results These comparison DiCE CoGS plausibility constraints enforced ii possibly nonpermanent invalidity caused perturbations typically literature robustness iii effect increasing m computation Krobustness score B1 DiCE vs CoGS plausibility constraints Table B1 shows average standard deviation success rate times counterfactual desired class variants DiCE CoGS plausibility constraints active The algorithms comparable succeed cases However DiCEb perform substantially worse cases data set Pro random forest data set Rec neural network Lastly Fig B1 shows relative change loss obtained algorithms As seen DiCE comes close performance CoGS Rec Thus overall CoGS remains superior DiCE plausibility constraints enforced We attribute fact differently CoGS DiCE inherently designed discover diverse set counter factuals instead single closestpossible counterfactual B2 Invalidity counterfactual explanations We fact bestfound counterfactual explanations typically robust associated point zcid3 greater chance perturbations invalid f z shifted perturbation Here consider additional intervention possible Fig B2 shows average frequency perturbations cause invalidity The frequencies computed applying cid6 cid5 t z cid6 22 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig B2 Mean frequency invalidity counterfactual explanations different types perturbations accounting different types robustness Darker colors represent worse scenarios larger average invalidity discovered counterfactual example 100 perturbations sampled uniformly random categorical features categorical possibilities uniformly normally stdev 01 numerical features numerical intervals The ﬁgure shows notion robustness accounted perturbations generally larger chance causing invalidity counterfactual explanation Regarding perturbations features C Csetbacks recall accounting respective notion robustness intended provide counterfactual explanations minimal additional intervention cost maximal Csetback happen Ideally returned counterfactual example optimal near x possible means example border decision boundary f Thus optimality guarantees f zcid3 wcs cid5 t Proposition 2 This means Csetback result invalidity entries perturbations C report 1 This happen Fig B2 CoGS guarantee discover optimal counterfactual examples cases returned example boundary Csetback small cross boundary The frequency phenomenon depends data set Also accounting robustness wrt C theory decrease invalidity rate additional intervention costly conﬁrmed Sec 73 23 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig B3 Approximated groundtruth Krobustness scores 1000 samples increasing m determine value m needed good robust ness perturbations K Shaded areas represent standard deviations ﬁnd accounting robustness wrt C lowers invalidity rate Hou evident types f normallydistributed perturbations When Krobustness accounted bestfound counterfactual explanation supposed region decision boundary relatively loose respect features K Consequently accounting Krobustness fact counter invalidity wish risking impossible carry additional intervention plausibility constraints The ﬁgure shows general substantial gain lowering invalidity accounting Krobustness At times accounting Krobustness allows reach zero invalidity cell corresponds robustness K perturbations K Inc Hou Pro types f sampling distributions However case Krobustness helps heuristic nature Krobustness score Cre Lastly observe frequency invalidity raise notions robustness accounted time Inc uniformlydistributed perturbations neural network Note necessarily problem invalidity perturbations C expected high goal robustness wrt C able minimize additional intervention cost B3 Setting m Krobustness We report results setting hyperparameter m computing Krobustness scores Eq 8 In particular run CoGS accounting Krobustness loss function m 0 4 16 64 Note m 0 corresponds accounting Krobustness B31 Achieved Krobustness We consider increasing m improves Krobustness approximated groundtruth We approximate ground truth true Krobustness calculating Krobustness score 1000 samples counterfactual example discovered speciﬁc m Fig B3 shows results obtained experiment We consider case Crobustness accounted If Krobustness accounted m 0 approximated groundtruth Krobustness discovered counterfactual examples low Rec random forest score approximately 04 neural network score 04 As soon samples considered m 4 Krobustness increases substantially Cre Further increasing m diminishing returns note m increased exponentially Accounting Crobustness largely orthogonal meaning effect terms Krobustness B32 Additional required runtime Fig B4 shows additional runtime incurred runs CoGS account notion robustness runs account particular increasing m calculation Krobustness score The ﬁgure shows accounting Crobustness comes signiﬁcant extra cost runtime This follows fact use Deﬁnition 4 need compute maximal Csetback Conversely accounting Krobustness come relatively large additional cost runtime appears linear m note m grows exponentially plots Fortunately experimental results B3 suggest small values m suﬃcient obtain good Krobustness scores 24 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Fig B4 Additional runtime CoGS different conﬁgurations accounting robustness respect runtime accounting robust ness Fig B5 Cost accounting robustness relative accounting robustness ideal cost perturbations place different values m Note rare relative costs smaller 1 lack optimality search algorithm B33 Additional cost accounting robustness Fig B5 expands results reported Fig 11 including different values m We ﬁnd major differences based setting m computing Krobustness score tails respective distributions Hou slightly Pro types f Accounting C Krobustness time leads larger costs accounting reasonable expect On average cost comes accounting robustness limited 65 ideal cost Inc especially light results 25 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table C1 Result pairwise comparison effect accounting different types robustness data set Cre different perturbations random forest neural network uniform normal sampling distributions The displayed p values obtained MannWhitney U test HolmBonferroni correction post KruskalWallis test rejecting null hypothesis pvalue cid13 001 Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K None 1000 0000 0000 0000 None 1000 0520 0000 0000 None 1000 0000 0000 0000 None 1000 0000 0000 0000 Cre perturbations C Only C Only K Both C K 0000 1000 0000 0125 0000 0000 1000 0000 0000 0125 0000 1000 Cre perturbations K Only C Only K Both C K 0520 1000 0000 0000 0000 0000 1000 0912 Cre perturbations C K Only C Only K None 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 0003 0000 0000 0912 1000 Both C K 0000 0000 0003 1000 Inc perturbations C Only C Only K Both C K 0000 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 Inc perturbations K Only C Only K Both C K 0000 1000 0000 0000 0000 0000 1000 0000 Inc perturbations C K Only C Only K None 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 Both C K 0000 0000 0000 1000 Table C2 Result pairwise comparison effect accounting different types robustness data set Inc different perturbations random forest neural network uniform normal sampling distributions The displayed p values obtained MannWhitney U test HolmBonferroni correction post KruskalWallis test rejecting null hypothesis pvalue cid13 001 perturbations place described Sec 73 additional intervention perturbations lead 100 times larger costs nonrobust counterfactual explanations Inc Fig 9 Appendix C Statistical signiﬁcance We report statistical signiﬁcance results displayed Sec 73 For data set type perturbation perform KruskallWallis tests assume normality determine signiﬁcant differences present relative cost induced applying different notion robustness In cases outcome test signiﬁcant differences present pvalue cid13 001 Next perform posthoc pairwise comparisons MannWhitneyU test assess notion robustness protects considered perturbation signiﬁcantly differently The result pairwise comparison analysis shown Tables C1 C5 26 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table C3 Result pairwise comparison effect accounting different types ro bustness data set Hou different perturbations random forest neural network uniform normal sampling distributions The displayed p values obtained MannWhitney U test HolmBonferroni correction post KruskalWallis test rejecting null hypothesis pvalue cid13 001 Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K None 1000 0000 0000 0000 None 1000 0000 0000 0000 None 1000 0000 0000 0000 None 1000 0000 0000 0000 Hou perturbations C Only C Only K Both C K 0000 1000 0261 0006 0000 0261 1000 0083 0000 0006 0083 1000 Hou perturbations K Only C Only K Both C K 0000 1000 0000 0000 0000 0000 1000 0070 Hou perturbations C K Only C Only K None 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 0008 0000 0000 0070 1000 Both C K 0000 0000 0008 1000 Pro perturbations C Only C Only K Both C K 0000 1000 0000 0008 0000 0000 1000 0000 0000 0008 0000 1000 Pro perturbations K Only C Only K Both C K 0000 1000 0000 0000 0000 0000 1000 0760 Pro perturbations C K Only C Only K None 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 0014 0000 0000 0760 1000 Both C K 0000 0000 0014 1000 Table C4 Result pairwise comparison effect accounting different types robustness data set Pro different perturbations random forest neural network uniform normal sampling distributions The displayed p values obtained MannWhitney U test HolmBonferroni correction post KruskalWallis test rejecting null hypothesis pvalue cid13 001 On Cre Kperturbations middle Table C1 accounting robustness wrt C signiﬁcantly different accounting notion robustness pvalue 052 001 similarly accounting robustness wrt K induces relative cost accounting robustness wrt C K On Hou Cperturbations Table C3 accounting C signiﬁcantly different accounting K pvalue 052 001 accounting K signiﬁcantly different accounting C K pvalue 0083 001 When perturbations happen C K Pro Rec respective tables accounting K signiﬁcantly different accounting C K pvalue 0014 001 pvalue 0271 001 respectively In general results match seen Figs 9 10 Also note majority cases accounting notion robustness signiﬁcantly different accounting 27 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 Table C5 Result pairwise comparison effect accounting different types robustness data set Rec different perturbations random forest neural network uniform normal sampling distributions The displayed p values obtained MannWhitney U test HolmBonferroni correction post KruskalWallis test rejecting null hypothesis pvalue cid13 001 None 1000 0000 0000 0000 None 1000 0000 0000 0000 Rec perturbations C Only C Only K Both C K 0000 1000 0000 0000 0000 0000 1000 0115 0000 0000 0115 1000 Rec perturbations K Only C Only K Both C K 0000 1000 0000 0000 0000 0000 1000 0651 Rec perturbations C K Only C Only K None 1000 0000 0000 0000 0000 1000 0000 0000 0000 0000 1000 0271 0000 0000 0651 1000 Both C K 0000 0000 0271 1000 Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K Robustness None Only C Only K Both C K References Syst 30 2017 31463154 116 32 2019 1584915854 Exp 2021 12 2021 124003 1 JH Friedman Greedy function approximation gradient boosting machine Ann Stat 29 2001 11891232 2 G Ke Q Meng T Finley T Wang W Chen W Ma Q Ye TY Liu LightGBM highly eﬃcient gradient boosting decision tree Adv Neural Inf Process 3 Y LeCun Y Bengio G Hinton Deep learning Nature 521 7553 2015 436444 4 M Belkin D Hsu S Ma S Mandal Reconciling modern machinelearning practice classical biasvariance tradeoff Proc Natl Acad Sci 5 P Nakkiran G Kaplun Y Bansal T Yang B Barak I Sutskever Deep double descent bigger models data hurt J Stat Mech Theory 6 B Goodman S Flaxman European Union regulations algorithmic decisionmaking right explanation AI Mag 38 3 2017 5057 7 A Jobin M Ienca E Vayena The global landscape AI ethics guidelines Nat Mach Intell 1 9 2019 389399 8 A Adadi M Berrada Peeking inside blackbox survey eXplainable Artiﬁcial Intelligence XAI IEEE Access 6 2018 5213852160 9 R Guidotti A Monreale S Ruggieri F Turini F Giannotti D Pedreschi A survey methods explaining black box models ACM Comput Surv 51 5 2018 142 2019 206215 pp 47684777 Systems vol 29 10 C Rudin Stop explaining black box machine learning models high stakes decisions use interpretable models instead Nat Mach Intell 1 5 11 MT Ribeiro S Singh C Guestrin Why I trust explaining predictions classiﬁer Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining 2016 pp 11351144 12 SM Lundberg SI Lee A uniﬁed approach interpreting model predictions Advances Neural Information Processing Systems 2017 13 B Kim R Khanna OO Koyejo Examples learn criticize Criticism interpretability Advances Neural Information Processing 14 C Chen O Li D Tao A Barnett C Rudin JK Su This looks like deep learning interpretable image recognition H Wallach H Larochelle A Beygelzimer F dAlchéBuc E Fox R Garnett Eds Advances Neural Information Processing Systems vol 32 2019 15 S Wachter B Mittelstadt C Russell Counterfactual explanations opening black box automated decisions GDPR Harv J Law Technol 31 2017 841 intelligence IEEE Access 9 2021 1197412001 16 I Stepin JM Alonso A Catala M PereiraFariña A survey contrastive counterfactual explanation generation methods explainable artiﬁcial 17 D Slack S Hilgard H Lakkaraju S Singh Counterfactual explanations manipulated arXiv preprint arXiv2106 02666 18 S Barocas AD Selbst M Raghavan The hidden assumptions counterfactual explanations principal reasons Proceedings 2020 Conference Fairness Accountability Transparency 2020 pp 8089 19 JC Gower A general coeﬃcient similarity properties Biometrics 1971 857871 20 R Guidotti A Monreale S Ruggieri D Pedreschi F Turini F Giannotti Local rulebased explanations black box decision systems arXiv preprint 21 S Sharma J Henderson J Ghosh CERTIFAI common framework provide explanations analyse fairness robustness blackbox models Proceedings AAAIACM Conference AI Ethics Society 2020 pp 166172 22 RK Mothilal A Sharma C Tan Explaining machine learning classiﬁers diverse counterfactual explanations Proceedings 2020 Conference Fairness Accountability Transparency 2020 pp 607617 arXiv1805 10820 28 M Virgolin S Fracaros Artiﬁcial Intelligence 316 2023 103840 23 T Laugel MJ Lesot C Marsala X Renard M Detyniecki Comparisonbased inverse classiﬁcation interpretability machine learning Interna tional Conference Information Processing Management Uncertainty KnowledgeBased Systems Springer 2018 pp 100111 24 MT Keane B Smyth Good counterfactuals ﬁnd casebased technique generating counterfactuals explainable AI XAI International Conference CaseBased Reasoning Springer 2020 pp 163178 25 K Hornik M Stinchcombe H White Multilayer feedforward networks universal approximators Neural Netw 2 5 1989 359366 26 H Fokkema R Heide T van Erven Attributionbased explanations provide recourse robust arXiv preprint arXiv2205 15834 27 U Grömping South German credit data correcting widely data set report 42019 Reports Mathematics Physics Chemistry Department II Beuth University Applied Sciences Berlin 2019 httpsarchive ics uci edu ml datasets South German Credit 28UPDATE 29 28 H Hofmann Statlog German credit data httpsarchive ics uci edu ml datasets Statlog German Credit Data 1994 29 R Kohavi B Becker Census income httpsarchive ics uci edu ml datasets adult 1996 30 R Kohavi Scaling accuracy naiveBayes classiﬁers decisiontree hybrid Proceedings Second International Conference Knowl edge Discovery Data Mining vol 96 1996 pp 202207 31 D Harrison Jr DL Rubinfeld Hedonic housing prices demand clean air J Environ Econ Manag 5 1 1978 81102 32 M Carlisle Racist data destruction httpsmedium com docintangible racist data destruction 113e3eff54a8 2019 33 AA Imran MS Rahim T Ahmed Mining productivity data garment industry Int J Bus Intell Data Min 19 3 2021 319342 34 J Larson S Mattu L Kirchner J Angwin How analyzed COMPAS recidivism algorithm httpswwwpropublica org article howwe analyzed compas recidivism algorithm 2016 Machine Learning PMLR 2018 pp 25642572 preprint arXiv190709294 Processing Systems 2021 35 M Kearns S Neel A Roth ZS Wu Preventing fairness gerrymandering auditing learning subgroup fairness International Conference 36 T Laugel MJ Lesot C Marsala X Renard M Detyniecki The dangers posthoc interpretability unjustiﬁed counterfactual explanations arXiv 37 F Ding M Hardt J Miller L Schmidt Retiring adult new datasets fair machine learning ThirtyFifth Conference Neural Information 38 W La Cava JH Moore Genetic programming approaches learning fair classiﬁers Proceedings 2020 Genetic Evolutionary Computation Conference GECCO 20 Association Computing Machinery New York NY USA 2020 pp 967975 39 M Virgolin A De Lorenzo F Randone E Medvet M Wahde Model learning personalized interpretability estimation mlpie Proceedings Genetic Evolutionary Computation Conference Companion GECCO 21 Association Computing Machinery New York NY USA 2021 pp 13551364 40 R DominguezOlmedo AH Karimi B Schölkopf On adversarial robustness causal algorithmic recourse Proceedings 39th International Conference Machine Learning vol 162 PMLR 2022 pp 53245342 41 F Pedregosa G Varoquaux A Gramfort V Michel B Thirion O Grisel M Blondel P Prettenhofer R Weiss V Dubourg J Vanderplas A Passos D Cournapeau M Brucher M Perrot E Duchesnay Scikitlearn machine learning Python J Mach Learn Res 12 2011 28252830 42 R Guidotti A Monreale F Giannotti D Pedreschi S Ruggieri F Turini Factual counterfactual explanations black box decision making IEEE Intell Syst 34 6 2019 1423 43 JA Nelder R Mead A simplex method function minimization Comput J 7 4 1965 308313 44 F Gao L Han Implementing NelderMead simplex algorithm adaptive parameters Comput Optim Appl 51 1 2012 259277 45 P Virtanen R Gommers TE Oliphant M Haberland T Reddy D Cournapeau E Burovski P Peterson W Weckesser J Bright SJ van der Walt M Brett J Wilson KJ Millman N Mayorov ARJ Nelson E Jones R Kern E Larson CJ Carey I Polat Y Feng EW Moore J VanderPlas D Laxalde J Perktold R Cimrman I Henriksen EA Quintero CR Harris AM Archibald AH Ribeiro F Pedregosa P van Mulbregt SciPy 10 Contributors SciPy 10 fundamental algorithms scientiﬁc computing Python Nat Methods 17 2020 261272 46 M DOrazio Distances mixed type variables modiﬁed Gowers coeﬃcients arXiv preprint arXiv210102481 47 AH Karimi J Von Kügelgen B Schölkopf I Valera Algorithmic recourse imperfect causal knowledge probabilistic approach arXiv preprint arXiv2006 06831 48 AH Karimi B Schölkopf I Valera Algorithmic recourse counterfactual explanations interventions Proceedings 2021 ACM Confer ence Fairness Accountability Transparency FAccT 21 Association Computing Machinery New York NY USA 2021 pp 353362 49 S Dandl C Molnar M Binder B Bischl Multiobjective counterfactual explanations International Conference Parallel Problem Solving Nature Springer 2020 pp 448469 50 T Laugel MJ Lesot C Marsala X Renard M Detyniecki Unjustiﬁed classiﬁcation regions counterfactual explanations machine learning Joint European Conference Machine Learning Knowledge Discovery Databases Springer 2019 pp 3754 51 M Pawelczyk K Broelemann G Kasneci On counterfactual explanations predictive multiplicity Conference Uncertainty Artiﬁcial Intelligence PMLR 2020 pp 809818 52 K Rawal E Kamar H Lakkaraju Algorithmic recourse wild understanding impact data model shifts arXiv preprint arXiv2012 11788 53 A Van Looveren J Klaise Interpretable counterfactual explanations guided prototypes arXiv preprint arXiv190702584 54 A Dhurandhar PY Chen R Luss CC Tu P Ting K Shanmugam P Das Explanations based missing contrastive explanations pertinent negatives Advances Neural Information Processing Systems vol 31 55 S Verma J Dickerson K Hines Counterfactual explanations machine learning review arXiv preprint arXiv2010 10596 56 AH Karimi G Barthe B Schölkopf I Valera A survey algorithmic recourse contrastive explanations consequential recommendations ACM 57 A Artelt V Vaquet R Velioglu F Hinder J Brinkrolf M Schilling B Hammer Evaluating robustness counterfactual explanations IEEE Sympo 58 M Pawelczyk C Agarwal S Joshi S Upadhyay H Lakkaraju Exploring counterfactual explanations lens adversarial examples theo Computing Surveys CSUR sium Series Computational Intelligence IEEE 2021 pp 0109 retical empirical analysis arXiv preprint arXiv2106 09992 59 V Ballet X Renard J Aigrain T Laugel P Frossard M Detyniecki Imperceptible adversarial attacks tabular data arXiv preprint arXiv191103274 60 T Freiesleben The intriguing relation counterfactual explanations adversarial examples Minds Mach 2021 133 61 A Ferrario M Loi The robustness counterfactual explanations time IEEE Access 62 A Ferrario M Loi A series unfortunate counterfactual events role time counterfactual explanations arXiv preprint arXiv2010 04687 63 R Mochaourab S Sinha S Greenstein P Papapetrou Robust counterfactual explanations privacypreserving SVM International Conference Machine Learning ICML 2021 Workshop Socially Responsible Machine Learning 2021 64 MT Ribeiro S Singh C Guestrin Anchors Highprecision modelagnostic explanations Proceedings AAAI Conference Artiﬁcial Intelligence 65 X Zhang A SolarLezama R Singh Interpreting neural network judgments minimal stable symbolic corrections Advances Neural vol 32 2018 Information Processing Systems vol 31 29 M Virgolin S Fracaros Appendix references Artiﬁcial Intelligence 316 2023 103840 66 BL Miller DE Goldberg Genetic algorithms tournament selection effects noise Complex Syst 9 3 1995 193212 67 CR Harris KJ Millman SJ van der Walt R Gommers P Virtanen D Cournapeau E Wieser J Taylor S Berg NJ Smith R Kern M Picus S Hoyer MH van Kerkwijk M Brett A Haldane JF del Río M Wiebe P Peterson P GérardMarchant K Sheppard T Reddy W Weckesser H Abbasi C Gohlke TE Oliphant Array programming NumPy Nature 585 7825 2020 357362 30