ELSEVIER Amficial Intelligence 97 1997 245271 Artificial Intelligence Selection relevant features examples machine Avrim L Bluma Pat Langley b1 School Computer Science Carnegie Mellon University Pittsburgh PA 152133891 USA h Institute Study Learning Expertise 2164 Staunton Court Palo Alto CA 94306 USA Received October 1995 revised May 1996 Abstract In survey review work machine large amounts irrelevant relevant features problem selecting topics present general framework challenges future work area 1997 Elsevier Science BV learning methods handling data sets containing problem selecting empirical theoretical work machine learning use compare different methods We close information We focus key issues relevant examples We advances Keywords Relevant features Relevant examples Machine learning 1 Introduction As machine learning aims address larger complex tasks problem focusing relevant information potentially overwhelming quantity data increasingly important For instance data mining corporate scientific records involves dealing features examples internet World Wide Web huge volume lowquality information easy access learning Similar issues arise personalization filtering systems information retrieval electronic mail netnews like In paper address specific aspects focusing task received significant attention AI literature problem focusing relevant Corresponding author Email avrimcscmucdu Also affiliated Intelligent Systems Laboratory DahnlerBenz Research Technology Center 1510 Page Mill Road Palo Alto CA 94304 USA Email laagleyOisleorg 00043702971700 1997 Blsevier Science BV All rights reserved PII SOOO4370297000635 246 AL Blum F LangleyArtificial intelligence 97 1997 245271 features examples general use representing relevant drive learning process We review recent work topics presenting use compare contrast different approaches data problem selecting frameworks important problem features notions relevance We begin problem focusing relevant In Section 2 present task filter wrapper based schemes We turn Section 3 problem focusing relevant filtering labeled unlabeled data We conclude future work empirical relate general goals featureselection developed approaches weighting examples describing methods Section 4 open problems challenges theoretical algorithms We report methods embedded characterizing explicit compare featureselection techniques fronts Before proceeding clarify learning computational ods results There substantial work feature recognition theory philosophy work approaches discuss areas readers statistics data selection aware scope survey focuses meth learning fields pattern information theory experimental machine fields statistics selection science Although space cover similarities 2 The problem irrelevant features At conceptual level divide task concept subtasks concept deciding combine learning selection relevant features elimination use describing deciding features features irrelevant ones algorithms incorporate In view central problems approach addressing At practical level like induction features More specifically irrelevant reach desired achieve good performance training examples needed complexity grow slowly number features present needed classification small fraction crucial work machine developing Induction algorithms desirable properties task represent examples theoretical For instance learningboth experimental algorithms 62631 machine learning induction algorithms scale domains goal like number level accuracy called sample text uncommon lo4 lo7 attributes expectation In recent years growing focused naturehas differ considerably emphasis focusing relevant test lies simple nearestneighbor method classifies features At extreme instances retrieving nearest stored training example available attributes distance computations Although Cover Hart 25 showed approach accuracy little thought reveals presence irrelevant excellent asymptotic In fact Langley Ibas 58 attributes averagecase number training exam ples needed analysis simple nearestneighbor similar reach given accuracy PAC notion sample complexity slow rate learning considerably indicates AL Blunt R LangleyArtcial Intelligence 97 1997 245271 247 grows exponentially number irrelevant concepts Experimental discouraging studies nearestneighbor conclusion At extreme lie induction methods explicitly attempt attributes tar conjunctive 1611 consistent logical descriptions example approach sophisticated methods relevant attributes learning nearestneighbor augment features reject irrelevant ones Techniques simplest identifying including encouraging small subset features algorithm hypotheses consideration size sufficient extremes featureweighting methods features aim achieve good scaling behavior experimental theoretical Theoretical For instance good generalization guarantee results improve induction method methods results focusing number sample middle select subsets explicitly reduce reduction 131 Somewhat corresponding significantly select relevant constitute learning terminology We structure context supervised formal notions relevance problem characterizing algorithms We turn discussing remainder section follows We begin describing important introducing featureselection developed wrapper approaches based relation algorithm This decomposition induction helps comparing approaches belong compare explicit tackle In addition definitions help illustrate general goals filter selection scheme basic trends different seen certain ways similar motivations We schemes somewhat different perspective category based weighting embedded reflects historical methods problem featureselection techniques 21 Deitions relevance features machine relevant The reason There number different definitions literature generally means depends question point different definitions appropriate depending ones goals Here important In hope illustrate definitions relevance discuss taken issues involved variety motivations literature learning variety significance More approaches relevant let consider setting For concreteness examples feature feature Boolean continuous F x F2 x x F The learning algorithm data point Boolean multiple valued continuous example paired associated whatwavelength isred discrete multiple An example point domain Fi For n features attributes instance whatcolor values instance space given set S training data label classification Although learning additional algorithm quantities sees postulate 46 probability fixed sample S helpful PAC learning model c distribution D instance space target function 248 AL Blunt f LangleyArtijicial Intelligence 97 1997 245271 examples labels We model sample S having produced repeatedly selecting examples D labeling according function c The target function c deterministic probabilistic case example A cA probability distribution labels single label Note use distribution D model integrity constraints data For instance suppose representing decimal digit boolean features feature 1 digit greater equal We model having D assign examples 101010101 probability zero target function c defined examples Given setup simplest notion relevance notion relevant target concept Definition 1 Relevant target A feature xi relevant target concept c exists pair examples A B instance space A B differ assignment Xi cA cB Another way stating definition feature xi relevant exists example instance space twiddling value xi affects classification given target concept Notice notion drawback learning algorithm given access sample S necessarily determine feature xi relevant Even worse encoding features redundant feature repeated twice possible examples differ feature examples probability zero D On hand definition choice theoretical analyses learning algorithms notion relevance prove convergence properties algorithm algorithm The definition useful situations target function c real object learning algo rithm actively query inputs choosing learning algorithm trying reverse engineer piece hardware convenient fiction To remedy drawbacks definition John Kohavi Pfleger 42 define notions termed relevance respect distribu tion nice interpretation notion relevance respect sample Definition 2 Strongly relevant sampledistribution A feature Xi strongly relevant sample S exist examples A B S differ assignment Xi different labels different distributions labels appear S multiple times Similarly xi strongly relevant target c dis tribution D exist examples A B having nonzero probability D differ assignment xi satisfy cA cB In words like Definition 1 A B required S nonzero probability AL Blum P LungleyArtijicial Intelligence 97 1997 245271 249 Definition 3 Weakly relevant vant sample S target c distribution D relevant features xi sampledistribution strongly A feature Xi weakly rele remove subset possible features important useful ignore Features These notions relevance viewpoint learning decide relevant generally strongly algorithm attempting matter sense strongly removing weakly features ignored account statistical variations For instance special case Definition 3 feature X relevant weakly relevant account correlated target function features given finite sample want relevant important In practice wish adjust variance statistical sample Features feature adds ambiguity Xi strongly depending definitions removed significance relevant In somewhat different vein definitions cases features relevant simply want caring exactly measure complexity That want use relevance function features want notion relevance complexity measure respect sample data S set concepts C useful select subset low For purpose complicated perform use relevance requiring algorithm quantity explicitly Definition 4 Relevance complexity measure set concepts C let r S C number features concept features Given sample data S 1 C error S fewest relevant relevant Definition S concept In words asking smallest number features needed achieve concept point view information contained useless error optimal performance class C feature persons socialsecurity highly respect modified definition S produces smaller sorts concepts consideration allow concepts relevant set C The reason specifying C nearly minimal For additional robustness number relevant The notions relevance independent specific There guarantee useful notion usefulness algorithm feature relevant vice versa Caruana Freitag term incremental usefulness learning algorithm necessarily 191 explicit simply Definition 5 Incremental L feature set A feature Xi incrementally accuracy hypothesis L produces accuracy achieved feature set A usefulness Given sample data S learning algorithm useful L respect A feature set xi U A better 250 AL Hum P LangleyArtificial Intelligence 97 1997 245271 This notion especially natural feature subsets incrementally instance To follow general disjunctions examples featureselection adding removing algorithms features search space current setfor definitions clear consider features xt V x3 V x7 suppose expressed learning algorithm sees framework described concepts Section 22 100000000000000000000000000000 111111111100000000000000000000 000000000011111111110000000000 000000000000000000001111111111 000000000000000000000000000000 note The relevant relevant x weakly 1 depend true target concept features Definition feature Using Def c include consistent target disjunction relevant rest weakly strongly initions 2 3 xi relevant relevant removing xi x3 x10 Using Definition 4 simply relevant relevant smallest consistent disjunction The notion incremental Definition 5 depends learning algorithm presumably given feature set 12 feature useful features xii question Definition 5 related end Section 22 discuss simple specific algorithm x30 We revisit number features strongly r S C 3 usefulness features There variety natural extensions definitions For Definition 4 S consider features individual lowestdimensional relevant In case analogy linear combinations features space projecting existence good function statistical approaches examples class C This notion Indeed methods learning finding 44 commonly heuristics instance relevant ask What space preserves relevance principal lowdimensional natural component analysis subspaces 22 Feature selection heuristic search featureselection We turn discussing viewing approaches algorithms generally algorithms large numbers irrelevant attributes A convenient perform explicit heuristic search state search space specifying view characterize feature nature terms stance basic issues determine dealing data sets contain paradigm feature selection subset possible selection method heuristic features According especially influences As Fig 1 depicts having exactly direction search operators generate points starting point turn states natural partial ordering space child start feature space successor parents This suggests search process First determine AL Blum P LangleyArtificial Intelligence 97 1997 245271 251 Fig 1 Each state space feature subsets specifies states right including attribute case involving space features dark circles parents induction Note attributes partially ordered states children use 27 remove involves organization The approach successively successively partial ordering Devijver Kittler takes away genetic operators types connectivity A second decision add attributes start attributes called forward selection known backward elimination One use variations adds k features somewhat different report operator like crossover produce search Clearly exhaustive subsets attributes A space At point selects approach known stepwise selection features decision point track search options consider states generated operators accuracy greedy scheme sophisticated search space impractical realistic approach search considers iterates For instance elimination path Within select current methods domains best simply choose set One replace bestfirst search expensive tractable hillclimbing adding removing lets retract earlier decision keeping explicit current set attributes state improves relies greedy method exist 2 possible local changes traverse considers metric issue concerns A One commonly occur information separate evaluation interacts basic strategy evaluate alternative training data Many involves attributes ability algorithms induction discriminate incorporate theory directly measure accuracy training issue concerns featureselection set A broader induction algorithm discuss shortly subsets attributes classes criterion based set strategy 252 AL Blum l LangleyArtcial Intelligence 97 1997 245271 accuracy continue Finally decide criterion halting attributes alternatives search For example revise feature set long sets candidate end search space select best One simple halting selected attributes maps stop combination robust uses stop adding removing estimate classification accuracy degrade continue generating reaching criterion single class value alternative parameter assumes noisefree training data A features according relevancy breakpoint simply orders determine values improves score Note design decisions induction carries feature techniques developed selection Thus address To concrete algorithm describing problem refer repeatedly provide useful dimensions let revisit scenario given end Section 21 features Boolean considering disjunction simple strategy known greedy setcover algorithm concepts expressible zero features Begin disjunction convention tive example Then features present example choose inclusion classified safe positives outputs nega negative number correctly number correctly classified safe current hypothesis features halt ties arbitrarily Repeat add hypothesis breaking examples increases increase positive With respect framework algorithm begins leftmost point incrementally moves training set step strictly rightward evaluates infinite penalty misclassifying negative examples improves evaluated performance subsets based performance Fig 1 halts Given data points xi xii exists disjunction In fact number features selected method times larger listed end Section 21 algorithm xpt halt It hard set method Olog ISI features Definition 4 39451 2 number relevant consistent training illustrate relationships section For instance We use algorithm previous Definition nitions algorithm necessarily strongly cause misclassify ignores feature algorithms negative example On hand data consistent disjunction defi 2 ignored algorithm true In fact data consistent disjunction features useful 3 converse 5 weakly relevant incrementally conservative Definition Definition features relevant nature 2 This hard follows fact exist feature add captures 1 r S C fraction stillmisclassified positive examples In direction finding smallest disjunction consistent given set data NPhard 351 polynomialtime algorithm disjunctions clog n times larger smallest c l4 place NP quasipolynomial time 1711 AL Blum I LangleyArtijicial Intelligence 97 1997 245271 253 strongly placed algorithms hypothesis feature strongly relevant features incrementally algorithm prefer weakly relevant eventually useful relevant evaluation criterion We review specific featureselection methods grouped embed selection basic induction classes use feature selection selection wrapper tofilter features passed induction induction process treat feature algorithm 23 Embedded approaches feature selection notes achieve slightly better bounds provide logical conjunctions Methods inducing logical descriptions selection methods embedded basic induction algorithm inducing given little add remove features concept description response Fig 1 describes ordering clearest example feature In fact algorithms greedy setcover algorithm partial ordering typically use errors new instances For methods space hypotheses algorithms prediction 7599102 organize results search concept descriptions learning pure conjunctive Theoretical pure disjunctive training factor larger halfspaces encouraging As mentioned logarithmic communication halting earlier hypothesis guaranteed cally number irrelevant list functions produced induction learning intersections learning DNF formulas results Pazzani Sarrett learning conjunctive Similar operations inducing complex combining induction greedy search function They partition subset extending select attribute Quinlans features concepts greedy setcover approach finds hypothesis smallest possible In fact Warmuth personal features These results apply directly PAC setting resulting examples misclassified Because fairly small sample complexity grows logarithmi settings algorithm Situations form include 14 1 algorithms 98 The free worst case spaces uniform distribution constantdimensional time conjunction disjunction n gn greedy setcover method distribution report averagecase logarithmic 78 imply adding removing logical concepts methods involve richer descriptions For example features analysis simpler methods growth certain product distributions core methods routines recursive partitioning methods 151 carry 8 11 CART form ID3 space decision 801 C45 discriminate best ability training data based attribute repeat tree downward discrimination trees stage evaluation classes process possible target concept characterized apply complex Dhagat Hellerstein fashion recursive kalternation set attributes reasonably documents decision functions techniques 28 extended greedy set cover kterm DNF formulas satisfies example good model dealing text contain small number possible long individual 8 describes methods unbounded small number instance lists Blum 254 AL Blum P LungleyArtiial Intelligence 97 1997 245271 words embedded complex algorithm dictionary For cases featureselection process clearly Separateandconquer methods similar manner These selection feature helps distinguish conjunctive classes remaining remove Clearly partitioning training cases rule C They repeat lists 227379 learning decision techniques use evaluation feature select test single rule excludes members members C rule covers repeat process class C add resulting process function embed features methods explicitly separateandconquer reason expect branch rule preference select features appear relevant scale irrelevant results exist methods studies Langley Sage 611 suggest decisiontree methods scale target concepts logical studies targets concepts theoretical features certain features Although domains growth nearestneighbor Experiments Almuallim 31 Kira Rendell 471 substantial features introduced decreases selected irrelevant inclusion irrelevant For involve experimental linearly number irrelevant conjunctions However exhibit Dietterich accuracy Boolean given sample size target concepts exponential involves The standard explanation effect discriminate greedy selection attributes domains little interaction tive concepts However feature isolation nificant problems situation arises target concepts3 presence attribute look discriminating scheme Parity concepts constitute reliance algorithms classes This approach works relevant attributes conjunc interactions lead relevant irrelevant cause sig extreme example lookahead remedy researchers problems search carries significant attempted techniques Some search extensive responded selectively defining new features combinations greedy search powerful letting 791 However approach directly evaluated handle analysis greedy 771 success Of course cost Others computational existing ones 72 larger steps terms ability experiment large numbers theoretical replacing irrelevant features increase 24 Filter approaches feature selection A second general purpose Pfleger attributes approach occurs 421 termed selection feature basic induction jilter methods separate process introduces step For reason John Kohavi filter irrelevant step uses general characteristics occurs The preprocessing induction 3 Note problem disappear increasing sample size Embedded rely greedy entire instance search distinguish space available relevant irrelevant features early selection methods search process AL Blum R LangleyArtcial Intelligence 97 1997 245271 255 set select features exclude Thus filtering methods use output algorithm training independent induction combined method simplest Perhaps filtering scheme evaluate feature mutual individually based information measure correlation target function select determined categorization nearestneighbor k features testing holdout set This method 6263 classification scheme achieved good empirical highest value The best choice k commonly text combination naive Bayes success tasks Kira Rendells rates complex decision reports 471 RELIEF algorithm featureevaluation function Their follows general paradigm incorpo uses ID3 induce 55 Almuallim minimal Dietterich extensions method tree training data selected features Kononenko types features feature selection handle general 3 filtering approach attributes involves greater degree search looks classes This method begins looking feature features pure partitions FOCUS passes original algorithm features feature space Their FOCUS algorithm pairs generates instances different classes training examples described selected perfectly discriminate isolation triples forth halting finds combination decisiontree training combinations turns induction set Comparative studies regular decisiontree method examples randomly number training unaffected introduction decisiontree method degraded significantly Schlimmer approach space feature sets starting set adding features combination consistent training data irrelevant attributes carries systematic avoid revisiting selected Boolean given target concepts FOCUS accuracy 871 describes related finds showed search states Although Focus RELIEF follow feature selection decisiontree features nearestneighbor use naive Bayesian relies embedded course use induction methods For instance Cardie preprocessor retrieval Kubat Flotzinger 56 filter decisiontree method produce reduced informationtheoretic Koller Sahami Markov blankets features In somewhat different vein Greiner Grove Kogan settings helpful Table 1 characterizes tutor filters conditionally recent work filter methods set attributes More recently Singh Provan classifier selection inclusion filter features metrics issue 541 employed crossentropy measure designed use naive Bayes decisiontree irrelevant attributes Bayesian Interestingly scheme filter 93 network induction 371 consider section induction algorithm feature set The typical described earlier reduced selection methods Most experiments unknown experimentally number irrelevant effect artificially results improvement focused natural domains features researchers features introducing terms dimensions takes advantage embedded contain 3471 studied construction 171 uses filtering Pfurtscheller 256 AL Blum P LangleyArtijicial Intelligence 97 1997 245271 Table 1 Characterization space feature sets recent work filter approaches feature selection terms heuristic search Authors Starting point Search control Almuallim FOCUS Cardie Keller Sahami Kira Rendell RELIEF Kubat et al Schlimmer Singh Provan None None All None None None Breadth Greedy Greedy Ordering Greedy Systematic Greedy Halting criterion Consistency Consistency Threshold Threshold Consistency Consistency Induction algorithm Dec tree Near neigh TreeBayes Dec tree Naive Bayes None No info gain Bayes net Another class filter methods actually constructs higherorder terms variance features orig best original inal ones orders features The statistical example approach generates orthogonal reduced dimensionality scribe theoretical intersection tribution The similar nal halfspaces guarantees technique principal components linear combinations principal space Empirically variety learning explain selects analysis features vectors components tasks Blum Kannan 441 bestknown successfully methods form target function sufficiently 24 analysis 121 benign dis incorporates orthogo examples chosen related method independent component ideas insists new features independent 25 Wrapper approaches feature selection A generic approach feature selection occurs outside basic induction method subroutine refer wrapper approaches 5 11 The typical wrapper algorithm Fig 1 embedded induction searches postprocessor For paper filter methods evaluates algorithm training data wrapper topic accuracy resulting classifier metric4 Actually literature statistics pattern recognition problem feature selection long active research method uses reason John et al 42 Kohavi John issue space feature subsets alternative estimated scheme long history 271 use machine The general argument sets running learning relatively recent wrapper approaches induction method use feature subset provide better estimate accuracy separate 4 One natural metric measuring running involves induction algorithm entire training data given set John et al crossvalidation method provides better measure expected accuracy novel accuracy learned structure training data However features argue convincingly test cases AL Blunt P LangleyArtificial Intelligence 97 1997 245271 257 favor wrapper method inductive bias For example Doak forward comparisons improve induction Doak reports experimental elimination John et al present similar comparative impact different studies including 181 report set empirical searchcontrol effect studies entirely different measure 291 John et al 42 argue behavior decisiontree selection backward techniques wrappers versus filters Caruana Freitag focusing decision The major disadvantage trees explore variations wrapper methods wrapper methods filter methods algorithm calling induction invent ingenious formers com feature set speed time instead speeds feature selection Freitag scheme techniques search larger spaces reasonable scheme evaluation cost results putational considered This cost led researchers ing process caching decision Moore Lee 761 alternative reducing Certainly trees lets algorithms In particular Caruana percentage training cases evaluation wrapper work Indeed expect methods induction account attributes benefit featureselection wrappers algorithms substantial learning framework focused decisiontree default like nearestneighbor schemes This expectation led nearestneighbor embedded body work wrapper methods casebased incorporate Let consider approach behavior Langley Sages 591 OBLIWON algorithm combines method assigns decisions learning The featureselection taking new instances account wrapper idea simple nearestneighbor class nearest case stored memory process effectively alters distance metric ignoring features relevant judged search elimination estimated OBLIVION carries backward sets starting features iteratively est improvement estimated method training data measure uses leaveoneout novel space feature leads great actually declines We characterize OBLIVION wrapper accuracy feature set running accuracy alternative crossvalidation nearestneighbor feature sets In particular accuracy The continues evaluation metric process estimate test cases removing accuracy involves Although 761 approach computationally expensive OBLIVION uses technique accuracy N training cases holding case turn constructing classifier correctly sight Moore Lee estimates classifier based remaining N 1 cases seeing predicts simply stores cessively expensive remaining ones classify accuracy training set case averaging training leave suc This scheme results N cases Because nearestneighbor case estimating cases memory implement tractable 5 The leaveoneout removing 5 Kohavi similarities 501 incorporated OBLIVION idea technique inducing decision tables 258 AL Blunt l LaneyArttjkial Intelligence 97 1997 245271 Table 2 Characterization recent work wrapper approaches feature selection terms heuristic search space feature sets Authors Starting point Search control Halting criterion Aha Bankert Beam Caruana Freitag CAP Doak John Kohavi Pfleger Langley Sage OBLIVION Langley Sage Sel Bayes Moore Lee Race Singh Provan K2AS Skalak TownsendWeber Kibler Comparison Comparison Comparison All None Comparison None Random All Comparison Greedy Comparison Greedy Greedy Greedy Greedy Greedy Mutation Comparison No better All Not better No better Worse Worse No better Worse Enough times No better Induction algorithm Near neigh Dec tree TreeBayes Dec tree Near neigh Naive Bayes Near neigh Bayes net Near neigh Near neigh Langley Sage designed number experiments suggest domains evaluate Results irrelevant OBLIVION features classifiers synthetic learns highaccuracy However suggesting Holtes highly correlated completely chess end games predicting domains contain 40 features irrelevant fewer instances simple nearestneighbor effect absent finding accuracy onelevel decision cause difficulty UC1 data sets trees better classifying class giving evidence nearestneighbor irrelevant ones OBLIVION fare significantly words semantic features Other researchers developed wrapper methods use nearestneighbor Most research wrapper methods focused classification For instance Aha Bankert starts randomly beam search cloud classification feature selection greedy search random hill climbing nearestneighbor option subset features includes 21 report technique like OBLIVION selected 94 work feature set replaces specified number cycles Moore idea knearest impressive features Skalaks starts random continues improvements Kibler greedy decisions They report task involves 200 numeric 97 combine induction methods highly sensitive numeric prediction Also work emphasized redundant features benefit basic approach Lee 76 TownsendWeber neighbor feature selection features However Langley Sage 60 shown naive Bayesian classifier sensitive Doaks idea learning complex earlier work Singh Provan Bayesian networks This suggests presence algorithms behavior induction feature irrelevant selection sense Definition 5 921 extended techniques variety situations feature selection improve focus finding attributes necessarily attributes As Caruana Freitag useful finding 191 argue methods relevant ones performance advantages irrelevant AL Blum P LangleyArtificial Intelligence 97 1997 245271 259 Table 2 characterizes recent efforts wrapper methods terms dimensions discussed earlier induction method case direct search researchers developed process The table shows diversity techniques variant methods Unfortunately comparison heavy reliance experimental deal increasing algorithms experiments numbers irrelevant ability results available features theoretical directly study 26 Featureweighting methods So far discussed algorithms explicitly attempt select relevant effect assigning features explicit subset features However approach especially apply weighting function relevance We separated motivations selection humans fed algorithm Weighting schemes online considerations Weighting uses generally natural schemes viewed methods incremental result tend embedded algorithms featureselection degrees perceived approach feature understood different Explicit intended tend easier implement settings generally purely motivated performance featureselection methods However feature sets approaches search For instance training lead simultaneous common instances changes Perhaps bestknown attributeweighting weights method terms heuristic search viewed explicit weight space lacks partial ordering forms rely different form gradient descent feature weighting instances The 741 adds subtracts weights linear training backpropagation additive changes Pomerleau features timevarying 841 generalization set weights leastmean issue squares algorithm perceptron threshold unit response updating rule errors 11011 linear units involve set 6 Baluja domains multilayer neural networks reduce error training 7 discuss neural network approach truly paper Kivinen Warmuth Auer 661 developed WINNOW rule Y additively perceptron online stream data consistent disjunction settings dominated algorithm degrees relevance difficulty Perceptronweighting techniques showed features multiplicative manner instance 49 In response Littlestone irrelevant issue dates weights Littlestone features WINNOW makes 0 rlogn mistakes relevance given number irrelevant achieves formulas negative examples logarithmic Definition degradation 4 Thus threshold features linear This effectively uses notion behavior degrades logarithmically target concept More generally WINNOW kDNF concept classes conjunctions positive functions good separation h While work embedded weighting schemes neuralnetwork driven method weights embedded nearestneighbor learner modifies fiavor Aha 11 reports error distance metric altering 260 AL Blum P LungleyArijicial Intelligence 97 1997 245271 For concreteness present version WINNOW algorithm disjunction learning scenario discussed Sections 21 22 proof Littlestones theorem The Winnow algorithm simple version I Initialize weights WI w features 1 2 Given example xi x output 1 1x1 wx n output 0 3 If algorithm makes mistake If algorithm predicts negative positive example xi equal 1 double value wi b If algorithm predicts positive negative example xi equal 1 cut value Wi half 4 Go 2 Theorem 6 WINNOW makes 2 3r 1 lg n mistakes sequence examples consistent disjunction r features Proof Let bound number mistakes positive examples Any mistake positive example double weights target function relevant weights mistake negative example halve weights definition disjunction Furthermore relevant weights doubled 1 lgn times weights n doubled Therefore WINNOW makes r 1 lg n mistakes positive examples Now bound number mistakes negative examples The total weight summed features initially n Each mistake positive example increases total weight n doubling wx n On hand mistake negative example WlXl decreases total weight n2 halving wixi wx 2 n The total weight drops zero Therefore number mistakes negative examples twice number mistakes positive examples plus 2 2 2r 1 lg n Adding bound number mistakes positive examples yields theorem 0 The general approach WINNOW algorithms developed Littlestone Warmuth 691 Vovk 1001 Littlestone Long Warmuth 671 CesaBianchi et al 21 Kivinen Warmuth 48 relations approaches additive updating methods mean squares algorithm In fact multiplicative updating schemes similar kind multi plicative probability updates occur Bayesian methods results provide bounds performance Bayesian updating probabilistic assumptions approach met Experimental tests WINNOW related multiplicative methods natural domains revealed good behavior 693 stud ies synthetic data scale domains thousands irrelevant features 681 AL Blum P LangleyArtcial Intelligence 97 1997 245271 261 More generally weighting methods cast ways merging advice sources knowledge weighting process plays interesting dual role respect different light methods discussed earlier Filter approaches pass output blackbox classifiers combine generated blackbox predictions determine generated algorithms learning learning algorithm weighting approaches input best way learning In filter set selected features On hand direct analogs filter wrapper approaches exist use filterlike methods 96 probability distributions determining weights Stanfill 1951 Ting conditional features based et al 26 present different weighting informationtheoretic end Finally Kohavi Langley Yun 52 adapted wrapper method way search discretized weight space explored feature sets Each approaches use features reports comparisons simple selection attributes metric use scores produced RELIEF 47 weight attributes scheme nearestneighbor Daelemans shows improvement normalizes 3 The problem irrelevant examples This suggests second broad useful Just attributes process aid learning concerns examples selection Some work assumed informative instances However robust approach training examples near misses provides involves letting briefly consider presence benevolent examples better type relevance techniques tutor gives 1021 learning select focus ideal training sequences intensive algorithm computationally efficiency Another labels obtained training data available makes sense learn examples proposed reasons learning Researchers learning One sufficient purposes computational experts unlabeled available easy generate Yet reason example selection rate learning focusing space hypotheses Here distinguish examples relevant viewpoint infomtation ones algorithm Most work emphasizes purpose selecting examples case high examples increase aiding search ones relevant viewpoint informationbased measures cost labeling attention informative examples reason As featureselection schemes separate exampleselection methods selection process induction embed examples passing selection dimension methods select unlabeled learning successive calls instead organize select relevant examples instances filter wrap example algorithm learning process technique Although refer section distinction labeled training instances ones 262 AL Blum I LangleyArtificial Intelligence 97 1997 245271 31 Selecting labeled data The generic approach assumes examples equally useful As training data available set labeled process example selection use learning noted embed algorithm simple ceptron algorithm methods learn example current hypothesis misclassifies embedded methods hypothesis edited nearestneighbor methods incremental approach For instance called conservative correct algorithms induction schemes Such ignore examples basic learning conjunctive If assumes guarantee high probability success criteria testing bution overall relevant examples conservative cases achieves 10 error ignore 90 data training data test data taken single fixed distri data training 141 As learning progresses useful For instance algorithm 20 error rate ignore 80 training certain parts input space increases portion space learners knowledge wellunderstood roughly Although wishes time proportional constant Thus In PAC model le algorithms use explicit example learning induction methods seen order halve error rate 143486 However conservative number examples actually learning number new examples algorithm error rate remains achieve error rate E logarithmic result holds conservative learning algorithms need roughly double number examples algorithms error rate halve number examples actually linear embed example achieve adjusts training data based algorithms input accuracy learners current hypothesis near random focuses currently hard data Schapire use examples described logarithmic improved accuracy neural network methods tasks involving optical like This approach selection process similar effects wrapper method distribution behavior The basic distribution guessing As result shown general boosting improve character recognition backpropagation training expensive prediction learning process lets achieve idea learning progresses In particular Schapire Drucker et al 30311 shown 33341 booster samples removing especially experimental takes generic called boosting technique On 86 describes techniques boosting Freund appropriate conditions algorithm selection learning given 7 Littlestone Mesterharm deal better irrelevant This shows exist interactions 68 shown variant naive Bayes standard version updates features learns errors statistics example problems feature selection example selection x Although boosting clear empirical uses originally developed learning weak perform somewhat better guessing distribution function learned boost performance implies strong learning PAC model In words produce highquality theoretical goal showing algorithm hard core predictions AL Blunt P LangleyArtificial Intelligence 97 1997 245271 263 time needed selects random Another class wrapper methods induction Quinlan construct study decisiontree reduce Windowing tree uses tree classify remaining cases method selects new decision classifies reduction describes sets John Langley proper size randomly experimental example selection originated technique designed SO reports windowing sets training large trees decision sample training data induce initial decision misclassified original sample constructs process tree correctly led substantial reports 20 training training processing wrapper method called peepholing tree forth repeating data Quinlan time large collection chess endgames report simpler use wrappers random set augment examples From larger windowing determine Catlett designed selected training 43 sample 641 filter approach learning machine common Lewis Catlett techniques methods One imagine simple techniques inconsistent examples widely Onepass research leaned windowing selection labeled data embedded wrapper literature training data removing identical class methods filtering sampling training data constitute like boosting iterative versions sampling cleaning 32 Selecting unlabeled data 891 Given unlabeled The learner select data labeled This useful expensive scenarios unlabeled data plentiful labeling process problem embedded induction algo One generic approach set hypotheses consistent training data called query rithm maintains committee random label instance The basic idea informative relevant examples likely way Unfortunately pass test hypotheses classify requires stronger constraints obtain theoretical method requires ability space hypotheses sample random consistent hypotheses difficult major method selects set different predictions query committee boosting Specifically hypotheses requests topic algorithmic consistent There larger body work algorithms 327091 instance research results sort known example effect classification walk generate examples query algorithms empirical community A common slightly For instance turn changes heading membership determine feature values algorithms experimentation choosing theoretical community technique alter determine determine class methods effectively designs critical experiments eliminate hypotheses 75 learning examples different point letting task Mitchell relevant features competitors suggested tying labels desired classification reduce informationtheoretic complexity approach example earlier discussion Another competing distinguish 264 AL Blum I LangleyArtijicial Intelligence 97 1997 245271 selection Sammut Banerji demonstrated continued successful variance generate queries greatly enlarges guarantee polynomialtime tradition results In parallel theoretical learning 851 Gross advantage empirically More recently work active instance Cohn Ghahramani selects examples designed researchers 45164183 types concept classes 381 formal methods learning 23 report learners shown ability Jordan reduce Although work queries experimentation learning efforts addressed complex emphasized learning simple classifi tasks For example 53 let grammarinduction query oracle strings distinguish 571 KEKADA Rajamoneys hypotheses competing competing hypotheses Kulkarni 821 COAST design critical experiments scientific domains Finally Shen learning action uses experimentation cation Knobe Knobe legality candidate Simons distinguish Simon models 901 Gil planning learning Other 361 explored tasks systems incorporate strategies encountered domain For example Scott Markovitch learning methods situations space vised exploring unfamiliar considerably increase learning parts state space rates random presentations obtain reinforcement Most work selecting querying unlabeled exploring portions instance information representative 881 adapt idea unsuper include bias 65 Both approaches learning Angluin query method membership learning number irrelevant features known determine queries place new relevant et al 5 Blum et al Ill applied queries available theoretical algorithm Specifically algorithm polynomial mistake bound data embedded methods results wrapper automated way idea gradually grow set use feature dependence relevant algorithm makes mistake relevant features present The basic results set mistake feature missing reasonable concept class converted number mistakes plus queries logarithmic 4 Challenges future relevance research Despite relevant challenges recent activity features examples associated progress remain directions improve study theoretical important empirical problems Here outline learning communities methods machine selecting learning research 41 Theoretical challenges We claim sense central open theoretical problems learning wellknown revolve questions finding relevant features For instance consider question polynomialtime algorithms guarantee machine AL Blum l LangleyArtcial Intelligence 97 1997 245271 265 DNF formulas similar question polynomialsize learning polynomialsize Or consider model These questions include case PAC uniform models trees learnable following open problem special decision distribution Does exist polynomialtime functions 0 ln logn distribution models algorithm learning relevant features class Boolean PAC uniform This special case function tree small DNF representation knew priori log n relevant definition written truth table having n entries small decision trivial hand algorithm proven issues finding hard features learning problem log n variables relevant 9 On sense class lo Thus classes difficult special case For instance features core makes learn statistical query model Kearns problem need unusual impossible relevant problem solve appears note As practical matter test proposed algorithm given In fact functions truth tables class generally easy To allow easier experimental unclear experimentally targetfunctions distribution following hard uniform problem specic distribution target random examples convenience problem random testing algorithms functions number relevant features 2 log n parity bits indexed S Select random X compute number ones T contain ones zeroes results disjoint sets S T c 1 n size log n On input S contain odd exclusiveor function bits indexed T majority output lo ability lists parity class challenge complex A second theoretical develop algorithms focusing WINNOW apply functions problems target classes decision functions This greatly extend online settings In framework example selection important direction general exist positive threshold results linear connect work query models advantage generally algorithmic membership assume filtering unlabeled require solving computationally arbitrary points input space probed work instances apply fixed data stream available hard subproblem Another challenge In fact class algorithm learn Indeed exact easy examples choosing membership larger class general DNF formulas membership queries respect I For instance positive bits number ones making bits ones zeros majority algorithm Bshouty leaning model recent algorithm Jackson classification active T 456 S 123 function queries learns 41 uniform distribution example 011101001010 parity 0 I XOR quantities I queries trees 161 learns larger class decision membership 266 AL Blum l LangleyArtcial Intelligence 97 1997 245271 theoretically process analyze ways example selection aid featureselection 42 Empirical challenges 204 attributes 1675 attributes data sets For instance Considerable work remains empirical urgent needs studies challenging domains date involved 40 features Two exceptions Aha Bankerts study cloud classification retrieval Moreover Langley widelyused UC1 data sets completely In hindsight relevant world domains substantial feature selection Koller Sahamis work information dealt far fewer features results nearestneighbor method suggest attributes ask real data sets test adequately ideas ignore ones However believe features character attributes diagnostic domains typical experiments fraction irrelevant natural experts want Sages irrelevant tend 61 roles important methods Such data sets number relevant Experiments synthetic data featureselection factors constant gorithms function factors showing features However distinguish irrelevant systematic experiments Monks problems study vary factors attributes holding al sample complexity scale domains use synthetic data reliance isolated artificial data sets irrelevant In way directly measure ability let systematically useful play improvements More challenging efficiency feature selection Although increase eliminate number states examined caused exponential problems domains features higher proportion irrelevant constant growth terms heuristic ones require sophisticated methods increases factor search problems number feature sets However viewing suggests places In general invent better techniques look solutions selecting initial feature set start search formulate searchcontrol methods improved feature sets design better frameworks improve efficiency sacrificing accuracy Future research halting criteria area compare carefully leaving attributeweighting approach advantages informed open question experiment relevance experiments preferably test specific hypotheses approaches space feature sets devise alternative advantage structure usefulness schemes Presumably best answered behavior featureselection evaluating designed feature example More generally selection related need studies designed intimately relationship Much empirical work example dealt approach potential sort promises field machine involving learning occupied lowdimensional domains spaces irrelevant selection tasks help understand quantify 2338 selection clearly holds greater issues years come features Resolving basic AL Blunt II LangleyArtificial Intelligence 97 1997 245271 267 Acknowledgements This research supported Grant No CCR9357793 Sloan Foundation Research Fellowship National Grant No active Office Naval Research Many researchers directly indirectly referees editors paper We like thank Science Foundation l0505 N0001494 area feature example ideas presented issue helpful comments suggestions contributed selection References I 11 D Aha A study instancebased algorithms supervised learning evaluations Doctoral Dissertation Department Information tasks mathematical empirical Computer Science psychological University California Irvine CA 1990 21 DW Aha RL Bankert A comparative D Fisher JH Lenz eds Arttficial evaluation sequential feature selection algorithms Intelligence Statistics V Springer New York 1996 3 H Almuallim TG Dietterich Learning irrelevant features Proceedings AAAI91 Anaheim CA AAAI Press 1991 547552 141 D Angluin Learning regular sets queries counterexamples Inform Compuf 75 1987 87106 5 1 D Angluin L Hellerstein M Karpinski Learning readonce formulas queries J ACM 40 1993 185210 61 R Armstrong D Freitag T Joachims T Mitchell Webwatcher Wide Web Distributed Environments 1993 Proceedings AAAI Spring Symposium Information Gathering learning apprentice World Heterogeneous networks 71 S Baluja D Pomerleau Dynamic Technical Note Artificial functions WINNOW weightedmajority Intelhgence 97 1997 38 l395 81 A Blum Learning Boolean infinite attribute space Muchine Learning 9 1992 373386 support 9 A Blum Empirical results calendar Proceedings 12th International Conference Machine Learning Lake Tahoe domain focus attention artificial neural relevance visionbased based algorithms issue scheduling CA Morgan Kaufmann San Mateo CA 1995 6472 IO A Blum M Fur J Jackson M Kearns Y Mansour characterizing Symposium Theory Computing Montreal Que 1994 253262 Fourier analysis statistical learning query S Rudic Weakly Proceedings learning DNF 26th Annual ACM 111 A Blum L Hellerstein irrelevant attributes N Littlestone Learning J Comput System Sci 50 1995 3240 presence finitely infinitely I 121 A Blum R Kannan Learning intersection k halfspaces uniform distribution J ACM 36 1989 929965 Proceedings 34th Annual IEEE Symposium Foundations Computer Science Palo Alto CA IEEE 1993 312320 A Blumer A Ehrenfeucht D Haussler MK Warmuth Occams 1987 377380 A Blumer A Ehrenfeucht D Haussler MK Warmuth Learnability dimension L Breiman JH Friedman RA Olshen CJ Stone Classification Regression Trees Wadsworth Belmont CA 1984 NH Bshouty Exact learning monotone Computer Science Palo Alto CA IEEE 1993 3023 11 trees C Cardie Using decision Conference Machine Learning Amherst MA Morgan Kaufmann San Mateo CA 1993 2532 RA Caruana D Freitag Greedy attribute selection Machine Learning New Brunswick NJ Morgan Kaufmann San Mateo CA 1994 2836 IEEE Symposium Foundations razor Inform Process Lett 24 VapnikChervonenkis International Conference improve casebased theory Proceedings 10th International Proceedings Proceedings learning Ilth I131 I141 1151 1161 I171 1181 268 AL Blum P LangleyArtificial Intelligence 97 1997 245271 I 19 1 RA Caruana D Freitag How useful relevance Working Motes AAAI Fall Symposium Relevance New Orleans LA AAAI Press 1994 2529 201 J Catlett Peepholing choosing attributes efficiently Conference Machine Learning Aberdeen Scotland 54 megainduction Proceedings 9th btternntionnl Morgan Kaufmann San Mateo CA 1992 49 1211 N CesaBianchi Y Freund DP Helmbold D Haussler RE Schapire MK Warmuth How use expert advice Proceedings Annual ACM Symposium Theory Computing 1993 382391 221 P Clark T Niblett The CN2 induction algorithm Machine Learning 3 1989 261284 23 DA Cohn Z Ghahramani learning statistical models MI Jordan Active J Artif Intell Research 4 1996 129145 1241 P Comon component 2S I TM Cover PE Hart Nearest neighbor pattern classification Independent analysis new concept Signal Process 36 1994 287314 IEEE Trans Inform Theory 13 1967 2127 261 W Daelemans S Gillis G Durieux The acquisition stress dataoriented approach Comput Linguistics 20 3 1994 421451 271 PA Devijver J Kittler Pattern Recognition A Sfatistical Approach PrenticeHall Englewood Cliffs NJ 1982 28 A Dhagat L Hellerstein PAC learning irrelevant attributes Proceedings IEEE Symposium Foundations Computer Science IEEE 1994 6474 I29 1 J Doak An evaluation featureselection methods application security Tech Rept CSE9218 Department Computer Science University California Davis 1992 1301 H Drucker R Schapire Improving performance Advances Neural Information Processing Systems Vol 4 Morgan Kaufmann neural networks boosting San P Simard algorithm Mateo CA 1992 31 H Drucker C Cortes LD Jackel Y LcCun V Vapnik Boosting learning Proceedings 11th International Conference Machine Learning New Brunswick NJ machine algorithms Morgan Kaufmann San Mateo CA 1994 5361 32 1 M Dyer A Frieze R Kannan A random polynomial volume Proceedings Annual ACM Symposium Theory Computing 1989 375 approximating time algorithm convex bodies 381 331 Y Freund Boosting weak learning algorithm majority Proceedings 3rd Annunl Workshop Compututional Learning Theory San Francisco CA Morgan Kaufmann San Mateo CA 1990 202216 34 Y Freund An improved boosting algorithm implications learning complexity Proceedings 5th Annual ACM Workshop Computational Learning Theory Pittsburgh PA ACM Press 1992 391398 1351 M Garey D Johnson Computers Intractability A Guide Theory NPCompleteness WH Freeman San Francisco CA 1979 1361 Y Gil Efficient domainindependent experimentation Machine Learning Amherst MA Morgan Kaufmann San Mateo CA 1993 128134 Proceedings Jh International Conference 137 I R Greiner AJ Grove A Kogan Knowing doesnt matter exploiting omission irrelevant data Artificial Intelligence 97 1997 345380 issue 38 1 KP Gross Concept acquisition attribute evolution experiment selection Doctoral Dissertation School Computer Science Carnegie Mellon University Pittsburgh PA 199 1 391 D Haussler Quantifying inductive bias concept learning Proceedings AAAI86 Philadelphia PA AAAI Press 1986 485489 40 R Hohe Very simple classification Learning I1 1993 6391 rules perform commonly domains Machine 411 J Jackson An efficient membershipquery algorithm learning DNF respect uniform distribution Proceedings IEEE Symposium Foundations Computer Science IEEE 1994 421 GH John R Kohavi K Pfleger Irrelevant features subset selection problem I1 rh International Conference Machine Learning New Brunswick NJ Morgan Kaufmann Mateo CA 1994 121129 Proceedings San AL Blum P LangleyArtificial Intelligence 97 1997 245271 269 43 GH John P Langley Static VS dynamic sampling Conference Knowledge Discovery Data Mining Portland OR AAAI Press 1996 367370 Proceedings 2nd International data mining 1441 LT Jolliffe Principal Component Analysis Springer New York 1986 45 I DS Johnson Approximation combinatorial algorithms problems J Comput System Sci 9 1974 256278 46 MJ Keams UV Vazirani An Introduction Computational Learning Theory MIT Press Cambridge MA 1994 1471 K Kira L Rendell A practical approach feature selection Conference Machine Learning Aberdeen Scotland 249256 Morgan Kaufmann Proceedings 9th international San Mateo CA 1992 48 J Kivinen MK Warmuth Additive versus exponentiated Proceedings 27th Annual ACM Symposium Theory Computing New York ACM Press 1995 2092 18 linear prediction gradient updates 491 J Kivinen MK Warmuth linear versus logarithmic mistake bounds input variables relevant Technical Note Artificial Intelligence 97 1997 325343 issue 50 1 R Kohavi The power decision tables Proceedings 8th European Conference Machine Learning P Auer The Perceptron versus Winnow algorithm 1995 1511 R Kohavi CH John Wrappers issue 273324 feature subset selection Artificial Intelligence 97 1997 521 R Kohavi P Langley Y Yun The utility feature weighting nearestneighbor algorithms Proceedings 9th European Conference Machine Learning Prague Springer Berlin 1997 531 B Knobe K Knobe A method inferring contextfree grammars Inform Control 3 I 1977 129146 1541 D Koller M Sahami Toward optimal feature selection Proceedings 13th International Conference Machine Learning Bari Italy Morgan Kaufmann San Mateo CA 1996 551 1 Kononenko Estimating attributes analysis extensions RELIEF Proceedings 7th European Conference Machine Learning 1994 561 M Kubat D Flotzinger methods Berlin 1993 367371 G Pfurtscheller Discovering study Proceedings 1993 European Conference Machine Learning Vienna Springer EEG signals comparative patterns 57 1 D Kulkami HA Simon Experimentation J Shrager P Langley eds Computational Models Scientc Discovery Theory Formation Morgan Kaufmann San Mateo CA 1990 machine discovery 581 P Langley W Iba Averagecase 1993 889894 Chambery France analysis nearest neighbor algorithm Proceedings IJCAI93 I59 P Langley S Sage Oblivious decision trees abstract cases Working Notes af AAAI94 Workshop CaseBased Reasoning Seattle WA AAAI Press 1994 113l 17 601 P Langley S Sage Induction selective Bayesian classifiers Proceedings 10th Conference Uncertainty Artificial Intelligence Seattle WA Morgan Kaufmann San Mateo CA 1994 399406 161 1 F Langley S Sage Scaling ed Computational Learning Theory Natural Learning Systems Vol 4 MIT Press Cambridge MA 1997 domains R Greiner irrelevant features 1621 DD Lewis Representation learning information retrieval Doctoral Dissertation Tech Rept UM CS1991093 Department Computer Science University Massachusetts Amherst MA 1992 1631 DD Lewis Feature selection feature extraction text categorization Proceedings Speech Natural Language Workshop San Francisco 64 DD Lewis J Catlett Heterogeneous Morgan Kaufmann San Mateo CA 1992 2 122 17 uncertainty sampling Proceedings Zlth International San Mateo CA 1994 Conference Machine Learning New Brunswick NJ Morgan Kaufmann 148156 I65 1 LJ Lin Selfimproving reactive agents based reinforcement learning planning teaching Machine Learning 8 1992 293321 I66 1 N Limestone Learning quickly irrelevant attributes abound new linear threshold algorithm Machine Learning 2 1988 285318 270 AL Blum P LangleyArtificial Intelligence 97 1997 245271 671 N Littlestone PM Long MK Warmuth Online learning linear functions Proceedings 23rd Annual ACM Symposium Theory Computing New Orleans LA ACM Press 1991 465475 68 N Littlestone C Mesterharm An apobayesian relative WINNOW MC Mozer MI Jordan T Petsche eds Advances Neural Information Processing Systems Vol 9 MIT Press Cambridge MA 1997 69 N Littlestone MK Warmuth The weighted majority algorithm Inform Comput 108 1994 212261 701 L Lovasz M Simonovits On randomized complexity volume diameter Proceedings IEEE Symposium Foundations Computer Science IEEE 1992 48249 1 7 1 C Lund M Yannakakis On hardness approximating minimization problems Proceedings Annual ACM Symposium Theory Computing 1993 286293 72 CJ Matheus LA Rendell Constructive induction decision trees Proceedings IJCAI89 Detroit MI Morgan Kaufmann San Mateo CA 1989 645650 73 RS Michalski Pattern recognition ruleguided inductive inference IEEE Trans Pattern Anal Machine Intell 2 1980 349361 74 M Minsky S Papert Perceptrons An Introduction Computational Geometry MIT Press Cambridge MA 1969 75 TM Mitchell Generalization search Arfial reprinted JW Shavlik TG Dietterich eds Readings Machine Learning Morgan Kaufmann San Mateo CA 1990 Intelligence 18 1982 203226 761 AW Moore MS Lee Efficient algorithms minimizing cross validation error Proceedings Ilfh International Conference Machine Learning New Brunswick NJ Morgan Kaufmann San Mateo CA 1994 190198 771 SW Norton Generating better decision trees Proceedings IJCAI89 Detroit MI Morgan Kaufmann San Mateo CA 1989 800805 78 MJ Pazzani W Sarrett A framework average case analysis conjunctive learning algorithms Machine Learning 9 1992 349372 791 G Pagallo D Haussler Boolean feature discovery empirical learning Machine Learning 5 1990 7199 801 JR Quinlan Learning efficient classification procedures application chess end games RS Michalski JG Carbonell TM Mitchell eds Machine Learning An Artificial Intelligence Approach Morgan Kaufmann San Mateo CA 1983 8 I JR Quinlan C45 Programs Machine Learning Morgan Kaufmann San Mateo CA 1993 82 S Rajamoney A computational approach theory revision J Shrager P Langley eds Computational Models Scientific Discovery Theory Formation Morgan Kaufmann San Mateo CA 1990 83 RL Rivest RE Schapire Inference finite automata homing sequences Inform Comput 103 1993 299347 841 DE Rumelhart G Hinton RJ Williams Learning internal representations error propagation DE Rumelhart JL McClelland PDP Research Group eds Parallel Distributed Processing Explorations Microstructure Cognition Vol 1 MIT Press Cambridge MA 1986 1851 C Sammut RB Banerji Learning concepts asking questions RS Michalski JG Carbonell TM Mitchell eds Machine Learning An Artificial Intelligence Approach Vol 2 Morgan Kaufmann San Mateo CA 1986 861 RE Schapire The strength weak learnability Machine Learning 5 1990 197227 871 JC Schlimmer Efficiently inducing determinations complete efficient search algorithm uses optimal pruning Proceedings 10th International Conference Machine Learning Amherst MA Morgan Kaufmann San Mateo CA 1993 284290 881 PD Scott S Markovitz Representation generation exploratory learning DH Fisher Unsupervised MJ Pazzani P Langley eds Concept Formation Knowledge Experience Learning Morgan Kaufmann San Mateo CA 1991 89 HS Seung M Opper H Sompolinsky Query committee Proceedings 5th Annual Workshop Computational Learning Theory Pittsburgh PA ACM Press New York 1992 287294 AL Blum P LangleyArtificial Intelligence 97 1997 245271 271 1901 WM Shen HA Simon Rule creation rule learning environmental exploration Proceedings IJCAI89 Detroit Ml Morgan Kaufmann San Mateo CA 1989 675680 911 A Sinclair M Jerrum Approximate counting uniform generation rapidly mixing Markov chains Inform Comput 82 1989 93133 1921 M Singh GM Provan A comparison selective nonselective Proceedings 12th Infernational Conference Machine Learning lake Tahoe induction algorithms Bayesian classifiers CA Morgan Kaufmann San Mateo CA 1995 497505 93 M Singh GM Provan Efficient Proceedings 13th International Conference Machine Learning Bari Italy Morgan Kaufmann San Mateo CA 1996 learning selective Bayesian network classifiers 1941 DB Skalak Prototype feature selection sampling random mutation hillclimbing algorithms Proceedings 11th International Conference Machine Learning New Brunswick NJ Morgan Kaufmann San Mateo CA 1994 293301 95 CW Stanfill Memorybased reasoning applied English pronunciation Proceedings AAAI87 Seattle WA AAAI Press 1987 577581 961 KM Ting Discretization continuousvalued attributes instancebased learning Tech Rept No 49 1 Basser Department Computer Science University Sydney D Kibler Instancebased 97 1 T TownsendWeber 1994 prediction continuous values Working Notes AAAI94 Workshop CaseBased Reasoning Seattle WA AAAI Press 1994 3035 98 1 K Verbeurgt Learning DNF uniform distribution polynomial Annual Workshop Computational Learning Theory San Francisco CA Mateo CA 1990 314325 1991 SA Vere Induction concepts predicate calculus time Morgan Kaufmann Proceedings 3rd San Proceedings IJCAI75 Tbilisi Georgia Morgan Kaufmann San Mateo CA 1975 281287 1 1001 V Vovk Aggregating strategies Proceedings 3rd Annual Workshop Computational Learning Theory San Francisco CA Morgan Kaufmann San Mateo CA 1990 371383 1011 B Widrow ME Hoff Adaptive switching circuits IRE WESCON Convention Record 1960 96104 1 1021 PH Winston Learning structural descriptions examples PH Winston ed The Psychology Computer Vision McGrawHill New York 1975