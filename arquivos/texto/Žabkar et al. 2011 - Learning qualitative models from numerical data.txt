Artiﬁcial Intelligence 175 2011 16041619 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Learning qualitative models numerical data Jure Žabkar Martin Možina Ivan Bratko Janez Demšar Faculty Computer Information Science University Ljubljana Tržaška 25 1000 Ljubljana Slovenia r t c l e n f o b s t r c t Article history Received 16 March 2010 Received revised form 23 February 2011 Accepted 23 February 2011 Available online 2 March 2011 Keywords Qualitative modelling Regression Partial derivatives Monotone models 1 Introduction Qualitative models relations observed quantities qualitative terms In predictive modelling qualitative model tells output increases decreases input We Padé new method qualitative learning estimates partial derivatives target function training data uses induce qualitative models target function We formulated methods computation derivatives based linear regression local neighbourhoods The methods empirically tested artiﬁcial realworld data We provide case study shows developed methods practice 2011 Elsevier BV All rights reserved People reason qualitatively For example playing simple pendulum ﬁve year old child discovers period pendulum increases uses longer rope Although later taught accurate numerical model describing behaviour T 2π lg relying operational qualitative relation everyday life Still despite Turings proposition artiﬁcial intelligence mimic human intelligence work far trying learn models data cid2 We formally relation period pendulum T length l gravitational acceleration g T Q l g meaning period increases l decreases g Different deﬁnitions qualitative relations discussed related work We base partial derivatives function f positive negative qualitative relation x region R partial derivative f respect x positive negative entire R f Q Rx x0 R f Q Rx x0 R f x f x x0 0 x0 0 1 2 Qualitative predictive models qualitative relations input variables continuous output instance z 0 x 0 f Q x z 0 x 0 f Q x For sake clarity omitted specifying region obvious context Corresponding author Tel 386 1 4768 154 fax 386 1 4768 386 Email addresses jurezabkarfriuniljsi J Žabkar martinmozinafriuniljsi M Možina ivanbratkofriuniljsi I Bratko janezdemsarfriuniljsi J Demšar 00043702 matter 2011 Elsevier BV All rights reserved doi101016jartint201102004 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1605 f x1 x2 x1x2 sampled ﬁve points The ﬁrst step relabels Fig 1 General outline twostep method simple example The target function examples replacing value target function sign estimated derivative f wrt x1 The second step induces classiﬁcation model data The paper includes major contributions The ﬁrst idea transforming problem learning qualitative models learning ordinary predictive models described section This followed new method called Padé1 computing partial derivatives data typical machine learning We ways computing derivatives based variations local linear regression Finally provide extensive experimental evaluation proposed setup 2 General outline method We propose new twostep approach induction qualitative models data presented Fig 1 Input data describes sampled function given set examples x f x attribute values f function value In ﬁrst step estimate partial derivative point covered learning example We replace value output f example sign corresponding derivative q Each relabelled example x q describes qualitative behaviour function single point In second step generalpurpose machine learning algorithm generalise relabelled data resulting qualitative model describing functions qualitative behaviour entire domain Such models relation output single input variable dependence attributes In Fig 1 modelled effect x1 y depends values x1 x2 model tells f decreases increasing x1 x2 negative increases increasing x1 x2 positive In real life model economic conditions rates public debt taxation inﬂation unemployment increase rates increasedecrease unemployment To inﬂuence multiple inputs effect rates inﬂation taxation unemployment induce separate model independent attribute In paper use C45 tree inducer construction models resulting models qualitative trees essentially classiﬁcation trees predicting qualitative behaviour While decided trees interpretability classiﬁcation model instance naive Bayesian classiﬁer random forest SVM induce qualitative models 3 Related work The beginnings ﬁeld qualitative reasoning early work outside AI Jeffries 1 May 2 troduced qualitative stability ecology Samuelson 3 discussed use qualitative reasoning economics Examples qualitative reasoning economics include early work Simon Ando 45 The papers Forbus 6 Kleer Brown 7 Kuipers 8 approaches foundations quali tative reasoning work AI Kalagnanam et al 911 contributed mathematical foundations qualitative reason ing There number approaches qualitative identiﬁcation known learning qualitative models data Most work concerned learning QDE Qualitative Differential Equations models One approach translate numerical systems behaviour time qualitative representation check QDE straints QSIMtype constraints 8 satisﬁed qualitative behaviour The resulting constraints constitute qualitative model Examples approach programs GENMODEL 1213 MISQ 1415 QSI 16 A similar approach carried general purpose ILP Inductive Logic Programming induce model qualitative behaviours enjoys advantages power ﬂexibility ILP 1719 Džeroski Todorovski developed approaches discovering dynamics QMN 20 generates models form qualitative differential equations LAGRANGE 21 LAGRAMGE 22 observed behaviour form differential equations PADLES 23 extends approach learning partial differential equations SQUID 24 ﬁnds models terms semiquantitative differential equations Gerçeker Say 25 ﬁt polynomials numerical data use induce qualitative models We believe algorithm LYQUID static systems experiment dynamic systems 1 An acronym partial derivative famous French mathematician 1606 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 Fig 2 The neighbourhoods locally weighted regression τ regression b parallel pairs c Qualitative models dynamic systems necessarily based QDE For example KARDIO model heart 26 uses symbolic descriptions logic instead QuMas 2728 learned models example qualitative haviours time approach similar ILP The QUIN algorithm 2931 induces qualitative trees similar classiﬁcation trees different leaf x y meaning z increases x nodes The leaves qualitative tree contain qualitative constraints z M decreases y depends variables QUIN constructs trees computing qualitative change vectors pairs points data recursively splitting space regions share common qualitative properties given Despite similarities Padé QUIN signiﬁcantly different Padé acts preprocessor numerical data combination attributevalue learning Padé computes qualitative partial derivatives example data points derivatives class values subsequent learning For example Padé combined decision tree learner produce qualitative trees similar induced QUIN However Padé combined rule learner produces model form qualitative rules The main algorithmic difference systems Padé computes quantitative qualitative partial derivatives example point QUIN computes degree consistency subregion numerical data principle possible qualitative monotonicity constraint Padé knowledge algorithm computing partial derivatives point cloud data Another impor tant difference Padé mentioned algorithms Padé essentially preprocessor algorithms induce model Padé merely augments learning examples additional labels later appropriate algorithms induction classiﬁcation regression models visualisation This results number Padés advantages For instance algorithms learning qualitative models handle numerical attributes Since Padé combined machine learning method learner use discrete attributes Besides symbolic methods interesting approaches qualitative modelling inspired biological systems 3233 Outside artiﬁcial intelligence numerical methods computation derivatives given point Such numerical derivatives modelling qualitative behaviour Unfortunately numerical differentiation computes function value points chosen algorithm limited given data sample 4 Computation partial derivatives We denote learning example x y x x1 x2 xn y value unknown sampled function y f x We introduce methods estimation partial derivative f point x0 The simplest assumes function linear small hypersphere x0 Fig 2a It computes locally weighted linear regression examples lying hypersphere considers computed coeﬃcients partial derivatives The second method τ regression computes single partial derivative time To avoid inﬂuence arguments function considers points sphere lie hypertube axis differentiation Fig 2b The derivative computed weighted univariate regression Finally parallel pairs method replaces single hypertube set pairs aligned axis differentiation Fig 2c allows focus direction differentiation decreasing number examples considered computation 41 Locally weighted regression Let N x0 set examples xm ym xmi x0i Fig 2a According Taylors theorem differ entiable function approximately linear neighbourhood x0 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 f xm f x0 f x0 xm x0 R2 1607 3 Our task ﬁnd vector partial derivatives f x0 We solve linear regression problem rephrasing 3 linear model xm ym N x0 ym β0 β Txm x0 cid6m 4 task ﬁnd β β0 minimal sum squared errors cid6m N x0 The error term cid6m covers remainder Taylor expansion R2 noise data The size neighbourhood N x0 reﬂect density examples amplitude noise Instead setting predeﬁned radius cid10xm x0cid10 δ consider neighbourhood k nearest examples weigh points according distance x0 wm e cid10xmx0cid102σ 2 5 The parameter σ 2 ﬁtted farthest example negligible weight 0001 This transforms problem locally weighted regression LWR 34 regression coeﬃcients represent partial derivatives cid3 cid4 β0 β cid5 X T W X cid61 X T W Y X 1 x11 1 xk1 x1n xkn W w 1 0 0 wk Y y1 yk 6 7 The computed β estimates vector partial derivatives f x0 As usual linear regression inverse 6 replaced pseudoinverse increase stability method Fig 3a shows simple example computation derivative f x1 x2 x2 2 point 10 10 neighbour 1 x2 hood k 5 examples 42 τ regression The τ regression algorithm differs LWR shape neighbourhood reference point It starts κ examples hypersphere generally larger LWR keeps k examples lie closest axis differentiation Fig 2b Let assume loss generality compute derivative regard ﬁrst argument x1 The neighbourhood N x0 contains k examples smallest distance cid10xm x0cid101 chosen κ examples smallest distance cid10xm x0cid10 cid10 cid10i represents distance computed dimensions ith With suitable selection κ k assume xm1 x01 cid11 xmi x0i 1 examples xm If assume partial derivatives regard different arguments comparable size f x1xm1 x01 cid11 f xixmi x0i 1 We omit dimensions ﬁrst scalar product 3 f xm f x0 f x1 xm ym N x0 We set linear model x0xm1 x01 R2 ym β0 β1xm1 x01 cid6m β1 approximates derivative f x1 x0 The task ﬁnd value β1 minimises error N x0 Examples N x0 weighted according distance x0 axis differentiation wm e xm1x012σ 2 σ set farthest example weight 0001 The described linear model solved weighted univariate linear regression neighbourhood N x0 f x1 cid13 x0 β1 cid13 stands xmN x0 cid13 cid13 wmxm1 ym wmx2 m1 cid13 cid13 cid13 wmxm1 cid13 wm ym cid13 wmxm12 wm Fig 3b shows computation derivative f x1 x2 x2 1 wm x2 2 point 10 10 κ 7 k 5 8 9 10 11 1608 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 Fig 3 Computation partial derivatives y x2 1 x2 2 point 10 10 43 Parallel pairs Consider examples xm ym xj y j close x0 aligned x1 axis cid10xm xjcid10 xm1 x j1 Under premises suppose examples correspond linear model 9 coeﬃ cients β0 β1 Subtracting 9 ym y j gives ym y j β1xm1 x j1 cid6m cid6 j ym j β1xm j1 cid6m j 12 13 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1609 ym j ym y j xm j1 xm1 x j1 The difference ym y j linear difference attribute values xm1 x j1 Coeﬃcient β1 approximates derivative f x1 To compute derivative 12 spherical neighbourhood like ﬁrst method LWR For x0 Note model intercept term β0 pair compute alignment x1 axis scalar product base vector e1 αm j cid14 cid14 cid14 cid14 xm xjTe1 cid10xm xjcid10cid10e1cid10 cid14 cid14 cid14 cid14 xm1 x j1 cid10xm xjcid10 14 We select k best aligned pairs κ points hypersphere x0 Fig 2c assign weights corre sponding alignment wm j e 1αm j2σ 2 σ 2 set smallest weight equals 0001 The derivative computed univariate linear regression f x1 x0 β1 cid13 cid13 xmxjN x0 wm jxm1 x j1 ym y j xmxjN x0 wm jxm1 x j12 15 16 Fig 3c illustrates computation k 5 pairs κ 7 points Note regression formula uses xm j1 independent ym j1 dependent variable 44 Time complexity Let data set contain N points Described methods compute partial derivative single point ﬁrst ﬁnding k nearest neighbours distances computed values n attributes requires O Nn ln k The costliest remaining step LWR method computation inverse k k matrix X T W X takes O k3 brute force O k2376 CoppersmithWinograd algorithm τ regression parallel pairs use univariate linear regression running times O k O κ respectively Finding nearest neighbours usually dominates time complexity steps The time complexity computing derivatives points data set rises quadratically number points O N 2n ln k The time complexity second step schema building qualitative model examples labelled quali tative derivatives depends machine learning algorithm settings 5 Experiments We ﬁrst performed extensive set experiments artiﬁcially constructed problems These data sets test Padé respect accuracy effect irrelevant attributes effect noise data To assess accuracy induced models compute derivatives model entire data set We check predictions model match analytically computed partial derivatives We deﬁne accuracy Padé proportion examples correctly predicted qualitative partial derivatives Note procedure require separate training testing data set correct answer prediction compared induction model Where stated experimental results represent averages trials Another set experiments performed realworld data Since ground truth correct partial derivative data sets necessarily known compute predictive accuracy model In cases stability measure quality machine learning method 35 Stability measures inﬂuence variation input output The motivation analysis design robust systems affected noise randomness sampling In experiments parallel pairs method reduced number arguments ﬁtted setting κ k Since ﬁrst batch experiments artiﬁcial data sets suggested parameter settings signiﬁcantly affect performance methods use constant values experiments use k 20 LWR parallel pairs κ 100 k 20 τ regression realworld data sets decreased κ 50 smaller number examples For LWR ridge regression compute illposed inverse 6 We C45 reimplemented Or ange 36 induction qualitative models We preferred algorithm potentially accurate methods like SVM goals induce comprehensible models Besides artiﬁcial data sets simple require singlenode tree tree leaves C45 run default settings noted 1610 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 Table 1 Results experiments f x y x2 y2 LWR Accuracy qualitative derivatives k 10 20 30 40 50 0991 0993 0992 0993 0994 Pairs 0986 0992 0992 0993 0993 b Accuracy qualitative models induced C405 k 10 0997 0996 20 0996 30 0995 40 0998 50 0993 0995 0994 0995 0995 Table 2 Results experiments f x y x3 y LWR Accuracy qualitative derivatives k 10 20 30 40 50 0727 0725 0751 0740 0725 Pairs 0690 0792 0879 0919 0966 b Accuracy qualitative models induced C405 k 10 1000 1000 20 0978 30 0971 40 0956 50 0898 0930 0954 0964 0986 51 Accuracy artiﬁcial data sets τ regression κ 30 0948 0929 0909 0990 0983 0983 τ regression κ 30 0597 0576 0545 0737 0745 0752 50 0968 0960 0953 0950 0935 0993 0991 0987 0978 0977 50 0626 0593 0571 0556 0541 0880 0743 0599 0780 0906 70 0971 0972 0969 0964 0961 0990 0986 0988 0978 0987 70 0639 0619 0609 0588 0558 0890 0811 0813 0732 0805 100 0980 0981 0978 0974 0972 0991 0991 0993 0990 0991 100 0647 0646 0618 0613 0612 0864 0860 0777 0700 0760 We performed experiments mathematical functions f x y x2 y2 We sampled uniform randomly 1000 points range 10 10 10 10 f x y x3 y f x y sin x sin y Function f x y x2 y2 standard test function 30 Its partial derivative wrt x f x 2x f Q x x 0 f Q x x 0 Since functions behaviour respect y similar observed results f x The accuracy methods close 100 Table 1 Changing values parameters major effect τ regression short κ 30 κ 50 wide k cid2 10 tubes better accuracy long tubes κ 100 accuracy decreases k The indicate longer tubes reach boundary positive negative values x Induced tree models high accuracy Function f x y x3 y globally monotone increasing x decreasing y region The function interesting stronger dependency x obscure role y All methods 100 accuracy regard x Prediction functions behaviour wrt y proves diﬃcult accuracy τ regression 5060 accuracy LWR 70 Table 2 Parallel pairs undisturbed inﬂuence x estimate sign f y accuracy 95 proper parameter settings An interesting observation accuracy induced qualitative tree models highly exceeds pointwise partial derivatives For instance qualitative models derivatives LWR reach 95100 despite low 70 accuracy estimates derivative When generalising labels denoting qualitative derivatives incorrect labels scattered randomly C45 recognises noise induces tree single node Function f x y sin x sin y partial derivatives f x cos x sin y f y cos y sin x change signs multiple times observed region The accuracy methods 80 90 degrading larger neighbourhoods Table 3 However accuracy C45 barely exceeds 50 making random pre dictions Rather limitation Padé shows expected inability C45 learn chessboardlike concept Since qualitative modelling traditionally interested examples physics tested Padés ability rediscover simple physical laws generated data Each domain described equation obtained 1000 uniform random samples J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1611 Table 3 Results experiments f x y sin x sin y LWR Accuracy qualitative derivatives k 10 20 30 40 50 0882 0870 0862 0844 0814 Pairs 0858 0841 0823 0796 0754 b Accuracy qualitative models induced C405 k 10 0509 0515 20 0515 30 0521 40 0516 50 0519 0531 0523 0555 0583 τ regression κ 30 0863 0820 0769 0516 0509 0510 50 0885 0865 0837 0799 0717 0512 0518 0513 0511 0507 70 0890 0880 0861 0839 0810 0510 0522 0514 0507 0508 100 0886 0882 0877 0853 0845 0509 0510 0515 0511 0515 Table 4 Accuracy computed partial derivatives domains physics Centripetal force Variable LWR τ regression Parallel pairs b Gravitational force Variable LWR τ regression Parallel pairs c Pendulum Variable LWR τ regression Parallel pairs m 100 086 100 m1 096 088 096 l 100 100 100 r 100 094 098 r 100 099 100 v 100 097 100 m2 096 087 096 ϕ 098 083 097 511 Centripetal force The centripetal force F object mass m moving speed v circular path radius r F mv 2 r We prepared data set target concept m sampled randomly 01 1 r 01 2 v 1 10 Apart failure τ regression F m explain proportion correctly computed qual itative partial derivatives Table 4a 100 C45 correctly induced singlenode trees predicting F Q m F Q v F Q r 512 Gravitation Newtons law universal gravitation states point mass attracts point mass force pointing line intersecting points force equal F G m1m2 r2 F gravitational force G gravitational constant m1 m2 points masses r distance point masses We prepared data set m1 m2 sampled randomly 01 1 r 01 2 Padé reached 100 accuracy Table 4b C45 induced correct models F Q m1 F Q m2 F Q r 1612 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 Fig 4 The qualitative model describing relation T ϕ Fig 5 Accuracy different number additional irrelevant attributes 513 Pendulum The period T ﬁrst swing pendulum wrt length l mass bob m initial angle ϕ0 is2 cid15 T 2π cid16 l g 1 1 16 ϕ2 0 11 3072 ϕ4 0 173 737 280 cid17 ϕ6 0 The mass chosen randomly 01 1 length 01 2 angle 180 180 g 981 Table 4c shows accuracies dependency period length angle All methods achieved 100 accuracy length learning example qualitative derivative equals ground truth The qualitative model describing relation T ϕ depends value ϕ C45 induced qualitative tree shown Fig 4 data labelled LWR The models based τ regression parallel pairs difference split threshold root node 27 parallel pairs The tree states negative angles period T decreases ϕ positive angles increases ϕ In words period increases absolute value ϕ τ regression 228 52 Effect irrelevant attributes Many realworld data sets include large number attributes effect function value The following experi ment shows robustness Padé cases We consider data sets obtained sampling f x y x2 y2 f x y xy described observe correctness partial derivatives computed Padé 050 random attributes interval x y 10 10 added domain The graphs Fig 5 accuracy drops increasing number added irrelevant attributes method We observe steep drop accuracy LWR ascribe fact LWR multivariate solving illconditioned number attributes approaches k Parallel pairs τ regression avoid problem computing derivative single attribute time Similar results algorithms f x y xy reﬂect fact deal problem gradient function aligned axis differentiation problem gets worse increasing number additional random variables The algorithms behave differently f x y x2 y2 Here hyperplane x 0 represents boundary positive negative derivatives wrt x τ regression uses larger neighbourhood parallel pairs generally disadvantage case approximately number 2 httpenwikipediaorgwikiPendulum J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1613 Table 5 Effect noise accuracy computed qualitative partial derivatives f x y x2 y2 random noise N0 σ σ 0 Correctness computed derivatives LWR τ regression parallel pairs 0993 0981 0992 b Correctness qualitative models induced C45 LWR τ regression parallel pairs 0996 0991 0995 σ 10 0962 0945 0924 0978 0976 0966 σ 30 0878 0848 0771 0956 0949 0949 σ 50 0795 0760 0680 0922 0917 0901 examples sides boundary added qualitative predictions come correct Parallel pairs hand choose pairs according angles axis differentiation These random number random variables increases derivative computed random direction It diﬃcult estimate distinction methods affects performance realworld problems We performed experiment number function method consistently outperforms 53 Coping noise In experiment add amounts noise function value The target function f x y x2 y2 deﬁned 100 100 We added Gaussian noise mean 0 standard deviation 10 10 10 10 puts f 0 10 30 50 noise noise range comparable signal We measured accuracy derivatives models induced C45 Since data contains noise set C45s parameter m minimal number examples leaf 10 examples data set m 100 We repeated experiment noise 100 times computed average accuracies The results shown Table 5 Padé robust despite huge noise As experiments artiﬁcial data sets observed C45 able learn perfect models despite drop correctness derivatives higher noise levels 54 Stability realworld data sets In measurements stability ran Padé UCI ML repository 37 data sets servo imports85 galaxy prostate housing autompg artiﬁcial data sets xy x2 y2 x3 y sin x sin y previous experiments For data set created 100 bootstrap samples sample computed qualitative partial derivatives class wrt continuous attributes example We deﬁned stability wrt attribute xi exam ple xm βxm xi proportion prevailing derivatives Q xm xi Q xm xi positive negative derivatives predicted xm βxm xi maxQ xm xi Q xm xi Q xm xi Q xm xi 17 The total stability regard attribute xi average stability examples data set We prefer β close 1 happens qualitative predictions derivative The lowest possible β 05 The settings methods k 20 κ 50 τ regression The lower value κ smaller number examples Results Table 6 average stability partial derivative outcome wrt attribute Altogether 46 continuous attributes domains We ranked performances methods attribute rank 1 assigned best method rank 3 worst Averaging attributes showed parallel pairs method stable rank 180 slightly τ regression method 184 LWR method achieved considerably worse rank 235 55 Comparison Padé QUIN We conclude experiments comparison Padé QUIN The algorithms similar pur poses diﬃcult bring ground While output QUIN tree models Padé computes partial derivatives constructing trees kinds models visualisations For sake comparison observed trees constructed C45 data relabelled Padé paper Padés aim ﬁnd partial derivative wrt given variable given point QUIN identiﬁes areas qualitatively invariant behaviour function wrt variables speciﬁed advance For objectivity followed deﬁnition 38 stating absence 1614 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 Table 6 Stability real artiﬁcial data sets The numbers correspond stabilities LWR τ regression parallel pairs respectively Data setattribute LWR τ PP Data setattribute LWR τ PP Auto displacement horsepower weight acceleration Imports85 normalizedlosses wheelbase length width curbweight height enginesize bore compressionratio stroke horsepower peakrpm highwaympg citympg Housing crim indus nox rm age dis tax ptratio b lstat 076 071 077 075 068 066 072 067 088 072 071 067 068 065 064 066 066 065 071 068 070 087 082 071 070 067 068 071 080 084 081 076 076 085 085 082 088 079 080 078 081 080 089 083 087 087 080 085 080 088 079 081 088 085 076 091 075 075 078 077 072 089 082 094 096 075 091 085 079 081 093 075 084 088 078 081 085 092 079 081 084 084 075 093 Galaxy eastwest northsouth radialposition Prostate lcavol lweight age lbph lcp 093 093 093 092 078 069 069 071 090 090 090 082 080 079 078 077 088 080 083 093 080 079 078 071 Servo xy x3 y x2 y2 sin x sin y pgain vgain 095 088 094 072 100 085 x y x y x y x y 099 099 095 094 099 098 100 077 100 075 100 078 099 099 095 096 099 099 091 091 088 089 086 086 variable leaf QUINs tree means derivative regard variable zero practise considered insigniﬁcant undeﬁned Padé predict zero derivatives Instead comparing accuracy algorithms confusion matrices results Padé column We report results obtained τ regression performance parallel pairs similar We parameters paper κ 100 k 20 QUIN run default parameters We performed comparison artiﬁcial functions x2 y2 x3 y xy different conditions noiselessdata including relevant variables x y data Gaussian noise σ 10 described Sec tion 53 noiseless data additional random variables unrelated output We constructed 1000 training 1000 testing instances domain We repeated experiment 10 times report averages Results given Table 7 For instance noiseless data 500 points positive derivative f x y x2 y2 wrt x QUIN correctly predicted positive derivative 482 points zero derivative 17 missing example rounding error Both algorithms achieve good results noiseless noisy data QUINs overlooking effect y f x y x3 y Padé makes correct prediction 71 cases problem It impossible tell happen QUIN forced predicting increase decrease Padé allowed abstain QUIN QUIN occasionally gives different results x y symmetric functions f x y x2 y2 f x y xy ascribed algorithm explained implementational issue The performance algorithms signiﬁcantly diverges presence irrelevant variables Padé affected f x y x3 y inferior inﬂuence y additionally obscured random variables QUINs results degrade signiﬁcantly This explained different nature algorithms Padés τ regression parallel pairs univariate methods QUIN multivariate models effect attributes Padé observes values attributes deﬁnition neighbourhood QUIN target concept 6 Case study billiards To practical use proposed methods analyse problem billiards Billiards common table games played cue set balls snooker pool variants The goals games vary main idea common player uses cue stroke cue ball aiming ball achieve J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1615 Table 7 Confusion matrices QUIN Padé test data sets f x y f x QUIN Padé Noiseless data relevant attributes 482 17 0 498 2 x2 y2 x3 y xy 0 62 437 2 497 1000 0 0 0 0 0 1000 0 0 0 495 0 4 497 1 1 0 498 3 496 b Data Gaussian noise added function value σ 10 x2 y2 497 2 0 481 18 30 28 440 10 489 x3 y xy 1000 0 0 0 0 0 1000 0 0 0 496 0 8 484 21 0 0 494 20 474 c Data additional irrelevant variables 487 15 x2 y2 103 0 399 106 0 390 4 492 x3 y xy 1000 0 0 0 0 0 1000 0 0 0 0 343 159 0 342 154 499 3 5 491 f y QUIN Padé 500 0 0 493 7 23 0 476 2 497 0 0 0 0 1000 0 0 0 282 717 500 0 0 495 4 9 0 490 4 495 494 0 0 483 10 25 0 479 28 476 0 0 0 0 1000 0 0 0 290 709 171 320 8 469 30 0 0 499 5 494 175 0 321 178 0 324 490 6 4 498 0 0 0 0 1000 0 0 0 339 660 502 0 0 108 0 388 494 8 9 487 Fig 6 Strokes different fullness hit azimuth Fig 7 Example trace simulator desired effect The friction table balls spin cue ball collision balls combine complex physical 39 However amateur player learn basic principles stroke cue ball knowing physics Our goal induce qualitative model describing relation azimuth degree fullness hit Fig 6 reﬂection angle cue ball collision ball We deﬁne reﬂection angle 1616 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 Table 8 The observed variables billiards case study Name Azimuth Elevation Velocity Follow Angle Range 15 15 deg 0 30 deg 3 5 ms 05r 05r 180 180 deg Description The horizontal angle cue The vertical angle cue Velocity cue Forwardbackward spin caused hitting ball abovebelow centre The reﬂection angle cue ball Fig 8 Spinning cue ball follow centre draw centre Dots represent point cue ball hit cue Fig 9 The qualitative model describing relation reﬂection angle azimuth angle stroke direction line connecting positions cue balls collision black ball collision cushion black ball Fig 7 We model relation azimuth angle depends attributes Table 8 horizontal vertical angle cue called azimuth elevation respectively follow forwardbackward spin Fig 8 cue ball velocity stroke We billiards simulator 40 collect sample 5000 randomly chosen scenarios We chose use τ regression parameters usual κ 100 k 20 We induced qualitative tree C45 asked experts inter pretation model The tree induced C45 shown Fig 9a The experts model easier understand set equations knowledge easier transfer actual game They able recast original tree simpler model Fig 9b As serve conceptualisation domain useful beginners The tree ﬁrst splits attribute follow For negative values cue ball hit centre backward spin results inversely proportional relation reﬂection angle azimuth increasing azimuth decreases angle vice versa The exception azimuth zero artefact coordinate reﬂection angle azimuth crosses line discontinuity 180 recorded decrease 179 179 increase 179 181 Positive values follow cue ball extra forward spin adds spin cue ball acquires rolling In case azimuth different effect black ball Having forward spin cue ball tendency roll forward collision For azimuth approximately 8 8 increasing decreasing J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1617 Fig 10 The plots followazimuth different sample sizes azimuth increases decreases reﬂection angle For greater absolute values azimuth cue ball partially hits black ball causes reﬂection angle negatively related azimuth Our model correctly recognised important attributes follow azimuth The elevation velocity effect reﬂection angle context according expert opinion We consequently tested obtained qualitative relations simulator simulating small changes azimuth leaving values attributes constant observing reﬂection angle We rules correct apart small deviations numerical values splits qualitative tree Finally smaller random samples 100 1000 examples step 100 domain assess practical usefulness Padé sparse data We ﬁrst excluded tree learning algorithm experiment plotting scatter plots examples marked respective qualitative derivatives Scatter plots follow azimuth results computed random samples 1000 Fig 10a 200 examples Fig 10b smallest sample Padé worked reasonably However tried induce qualitative trees data observed unreliable samples 1000 examples 7 Discussion conclusion The methods computation derivatives differ theoretical background leads differences performance LWR multivariate method computing derivatives univariate This puts LWR disadvantage LWR sensitive size data set number attributes comparable greater number points regression computed case number examples neighbourhood k total size data set LWR solving illconditioned This problem clearly surfaces experiments irrelevant attributes The methods attribute time run problem When number attributes small LWR gives satisfactory results LWR fared considerably worse realworld data sets evaluated methods indirectly testing stability The lower stability LWR reﬂects higher complexity Based multivariate regression number param eters LWR optimises equals number attributes The higher complexity LWR method allows ﬁt data better lower bias cost higher variance Therefore LWR theory better estimate derivatives compared overﬁt data attributes This property manifested experiments LWR performed considerably worse domains attributes housing imports85 scored best ﬁve domains attributes Being univariate τ regression parallel pairs methods select examples lying direction dif ferentiation exclude inﬂuence attributes function value To gather number examples LWR expand neighbourhood The τ regression extending neighbourhood direction axis differentiation parallel pairs method differentiates lines parallel actual axis differentiation The ac curacy computed derivatives proﬁt excluding attributes suffer extending neighbourhood The effect visible target functions x3 y sin x sin y The tests method copes effects x differentiating y LWR fails parallel pairs work The concept requires small neighbourhood LWR fares better difference performance large concept The difference τ regression parallel pairs subtle The τ regression computes derivative inside somewhat longer tube length governed parameter κ typically set 50100 It assumes function linear longer region error comes violating assumption Parallel pairs use smaller vicinity experiments set κ k 20 The function required linear close x0 The price pay derivative smoothed hyperdisc perpendicular axis differentiation instead 1618 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 computed x0 τ regression This cause problems derivative strongly depends arguments function We unable experimentally ﬁnd practical consequences theoretical difference τ regression parallel pairs As general rule LWR preferred data sets small number attributes 5 sparse sample On hand τ regression high dimensional data dense sample Parallel pairs appropriate assume attributes different inﬂuence class variable Experiments demonstrate computation partial derivatives diﬃcult prone errors unreliable estimates usually result correct qualitative models Even modest learning algorithm C45 able reduce noise computed derivatives build models perfectly match target function Apart ignoring noisy labels C45 task trivial artiﬁcial domains required inducing tree leaves It test case study billiards It induced correct model demonstrates basic advantage qualitative modelling standard regression modelling model induced presents general principles player regression model complex numerical relationships probably diﬃcult comprehend certainly impossible use billiards table We experimented machine learning algorithms rule learning naive Bayesian classiﬁer got similar results Since C45 proved adequate needs systematically tested published behaviour Padé combination algorithm While presented models small simple realworld qualitative models induced experts Samuelson 3 Jeffries 1 May 2 complex It world simpler expected described qualitatively humans reason simple models skip details included speciﬁc models particular contexts In summary introduced new approach learning qualitative models differs existing approaches trick translating learning problem classiﬁcation problem applying generalpurpose learning methods solve To focus paper explored ﬁrst step involves estimation partial derivatives The second step opens number interesting research problems explicitly discussed One exploration powerful learning algorithms Is assumption simplicity world qualitative terms correct support vector machines result better models Another interesting question combine models derivatives respect individual arguments single model describing functions behaviour respect arguments One approach use multilabel learning methods model derivatives We leave questions open research area Acknowledgements This work supported Slovenian research agency ARRS J22194 P20209 European project XMEDIA EC grant number ISTFP6026978 We thank Tadej Janež Lan Žagar Jure Žbontar expert help billiards case study References 1 C Jeffries Qualitative stability digraphs model ecosystems Ecology 55 6 1974 14151419 2 RM May Qualitative stability model ecosystems Ecology 54 3 1973 638641 3 PA Samuelson Foundations Economic Analysis enlarged edition Harvard University Press 1983 4 HA Simon On deﬁnition causal relation The Journal Philosophy 49 1952 517528 Reprinted HA Simon Models Discovery D Reidel Boston 1977 5 HA Simon A Ando Aggregation variables dynamic systems Econometrica 29 1961 111138 6 K Forbus Qualitative process theory Artiﬁcial Intelligence 24 1984 85168 7 J Kleer JS Brown A qualitative physics based conﬂuences Artiﬁcial Intelligence 24 1984 783 8 B Kuipers Qualitative simulation Artiﬁcial Intelligence 29 1986 289338 9 J Kalagnanam HA Simon Y Iwasaki The mathematical bases qualitative reasoning IEEE Intelligent Systems 6 2 1991 1119 10 J Kalagnanam Qualitative analysis behaviour PhD thesis Pittsburgh PA USA 1992 11 J Kalagnanam HA Simon Directions qualitative reasoning Computational Intelligence 8 2 1992 308315 12 E Coiera Generating qualitative models example behaviours Tech rep 8901 University New South Wales 1989 13 D Hau E Coiera Learning qualitative models dynamic systems Machine Learning Journal 26 1997 177211 14 S Ramachandran R Mooney B Kuipers Learning qualitative models systems multiple operating regions Working Papers 8th International Workshop Qualitative Reasoning Physical Systems Japan 1994 15 B Richards I Kraan B Kuipers Automatic abduction qualitative models Proceedings National Conference Artiﬁcial Intelligence AAAIMIT Press 1992 16 A Say S Kuru Qualitative identiﬁcation deriving structure behavior Artiﬁcial Intelligence 83 1996 75141 17 I Bratko S Muggleton A Varšek Learning qualitative models dynamic systems Proceeding 1st International Workshop Inductive Logic Programming ILP 91 1991 pp 207224 18 GM Coghill SM Garrett RD King Learning qualitative models presence noise Proceedings QR02 Workshop Qualitative Reasoning Workshop 2002 19 GM Coghill A Srinivasan RD King Qualitative identiﬁcation imperfect data Journal Artiﬁcial Intelligence Research 32 1 2008 825877 J Žabkar et al Artiﬁcial Intelligence 175 2011 16041619 1619 20 S Džeroski L Todorovski Discovering dynamics inductive logic programming machine discovery Journal Intelligent Information Systems 4 1995 89108 21 S Džeroski L Todorovski Discovering dynamics International Conference Machine Learning 1993 pp 97103 22 L Todorovski Using domain knowledge automated modeling dynamic systems equation discovery PhD thesis University Ljubljana Ljubljana Slovenia 2003 23 L Todorovski S Džeroski A Srinivasan J Whiteley D Gavaghan Discovering structure partial differential equations example behavior Proceedings Seventeenth International Conference Machine Learning 2000 24 H Kay B Rinner B Kuipers Semiquantitative identiﬁcation Artiﬁcial Intelligence 119 2000 103140 25 RK Gerçeker A Say Using polynomial approximations discover qualitative models Proceedings 20th International Workshop Qualita tive Reasoning Hanover New Hampshire 2006 26 I Bratko I Mozetiˇc N Lavraˇc KARDIO Study Deep Qualitative Knowledge Expert Systems MIT Press Cambridge MA USA 1989 27 I Mozetiˇc Learning qualitative models EWSL 1987 pp 201217 28 I Mozetiˇc The role abstractions learning qualitative models Proceedings Fourth International Workshop Machine Learning Morgan Kaufmann 1987 29 D Šuc I Bratko Induction qualitative trees L De Raedt P Flach Eds Proceedings 12th European Conference Machine Learning Springer Freiburg Germany 2001 pp 442453 30 D Šuc Machine Reconstruction Human Control Strategies Frontiers Artiﬁcial Intelligence Applications vol 99 IOS Press Amsterdam The Netherlands 2003 31 I Bratko D Šuc Learning qualitative models AI Magazine 24 4 2003 107119 32 A Varsek Qualitative model evolution Proceedings Twelfth International Joint Conference Artiﬁcial Intelligence 1991 pp 13111316 33 W Pang G Coghill An immuneinspired approach qualitative identiﬁcation biological pathways Natural Computing 2010 119 doi101007s1104701092122 34 C Atkeson A Moore S Schaal Locally weighted learning Artiﬁcial Intelligence Review 11 1997 1173 35 S BenDavid U von Luxburg D Pal A sober look clustering stability 19th Annual Conference Learning Theory Springer Berlin Germany 2006 pp 519 36 J Demšar B Zupan G Leban T Curk Orange experimental machine learning interactive data mining Proceedings PKDD 2004 pp 537 539 37 A Asuncion DJ Newman UCI machine learning repository 2007 38 D Šuc D Vladušiˇc I Bratko Qualitatively faithful quantitative prediction Artiﬁcial Intelligence 158 2 2004 189214 39 DG Alciatore The Illustrated Principles Pool Billiards ﬁrst edition Sterling 2004 40 D Papavasiliou Billiards manual Tech rep 2009 httpwwwnongnuorgbilliards