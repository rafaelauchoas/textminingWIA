Artificial Intelligence 106 1998 335352 Artificial Intelligence Worstcase analysis Perceptron Exponentiated Update algorithms Tom Bylander Division Computer Science The University Texas San Antonio San Antonio TX 78249 USA Received 6 April 1998 received revised form 21 September 1998 Abstract The absolute loss absolute difference desired predicted outcome This paper demonstrates worstcase upper bounds absolute loss Perceptron learning algorithm Exponentiated Update learning algorithm related Weighted Majority algorithm behavior algorithms sequence trials trial The bounds characterize acceptable consists example desired outcome loss outcome The worstcase absolute best linear function comparison class plus constant dependent initial weight vector plus pertrial loss The pertrial allowed tolerance desired outcome For concept worstcase bounds lead mistake bounds comparable past results 0 1998 Elsevier Science BV All rights reserved interval value interval bounded absolute learning algorithm loss algorithms loss eliminated learning Keywords Learning algorithms Absolute loss bounds Mistake bounds Randomized classification algorithms 1 Introduction Linear linear learning Although achieve good empirical networks For concept learning threshold linear functions important class functions machine results 12261 standard components neural represent limited functions mistake bounds known linear Perceptron threshold algorithm perfect classifier function 2225 Winnow This paper revised extended version Bylander Email bylandercsutsaedu 7 0004370298Z matter 0 1998 Elsevier Science BV All rights reserved PII SOOO437029800098S 336 T Bylander Art1 Zntelligence 106 1998 335352 l1921 There results algorithms Weighted Majority algorithms types noise 361020 However previous results characterize behavior algorithms sequence examples related Weighted Majority absolute This paper shows minimizing absolute learning linear threshold algorithms Exponentiated Update algorithm sum absolute differences worstcase absolute best linear function weight vector plus pertrial algorithm additional loss algorithms comparison loss The pertrial functions loss characterizes online behavior Perceptron algorithm loss desired predicted outcomes The loss class plus constant dependent initial learning In case total bounded sum absolute loss eliminated allowed tolerance loss bounded constant sequence length desired outcome The results paper hold sequence examples assumptions direct relationship loss number classification mistakes single misclassification distribution examples Unfortunately absolute correspond bounds derived linearly separable case small large absolute loss Nevertheless interesting mistake loss A previous results based absolute specialized cases Duda Hart 1 l derive Perceptron update rule Perceptron criterion algorithm function specialization absolute examples learning decreasing 161 A version converges linear Weighted Majority algorithm best input Theorems 2 3 8 independently paper shows modify absolute series stationary distribution minimum proved results similar algorithms rate harmonic function loss function WMC absolute loss square loss loss The Perceptron 21 CesaBianchi loss comparable absolute loss 917 The performance follows pattern similar The analysis square algorithms hypothesis online algorithms current hypothesis algorithms analysis comparison loss minus worstcase algorithm analyses online compared class The bounds based distance target hypothesis changes chosen proportion facilitate targets loss The distance measure linear best The desired outcome example implemented positivenegative allowed learning positivenegative examples In case absolute loss bounds lead mistake bounds algorithms similar randomized versions algorithms literature Also expected mistake bounds obtained previous outcome real interval Thus concept 2 Preliminaries A trial ordered pair x I consisting real vector x E IV example I outcome A prediction F example x weight real interval 7 Bylander Artcial Intelligence 106 1998 335352 331 vector w E IR computing weight vector w trial x I determined dot product 7 w x EYE wxi The absolute loss AbsLossw x I e yn 0 I Yhi I if7E I I ify ylo inf y yhi supYe y That desired prediction outcome weight vector algorithm argument loss argument trial sequence trials second denote absolute interval The AbsLoss notation For online algorithm A comparison weight vector u trial sequence S bounds form AbsLossA S AbsLossu S expression based characteristics algorithm A trial sequence S Before trial S algorithm hypothesizes weight vector wt The bounds based demonstrating trial S AbsLosswt S AbsLossu S 5r summing additional obviously ct 0 chosen The cases covered following loss c1 trials When AbsLossw Lemma 1 When y w x I given rrial St x I AbsLossw S AbsLossu S u x y u x w x WhenwIforagiventrialSxIthen AbsLossw S AbsLossu St 6 7 u x w x u x Proof Let y10 infyt y When y10 7 ws absolute ylu u x absolute 0 inequality similar loss ylo u x absolute j I inequality follows fact loss u s x ylo loss The proof second 3 Absolute loss bounds Worstcase absolute loss bounds derived Perceptron Exponentiated Update algorithms followed discussion 3 I Bounds Perceptron The Perceptron algorithm weight vector s typically rule applied vector w incremented outcome prediction zero vector 0 learning given Fig 1 The Perceptron algorithm inputs initial rate 17 The Perceptron update current weight decremented qx prediction y low high The use j outside outcome interval interval generalizes standard Perceptron algorithm S 0 lemma 1 2 338 7 Bylander Articial Intelligence 106 1998 335352 Algorithm Perceptron s q Parameters s start vector s E IRn q learning rate q 0 Initialization Before trial set w 1 s Prediction Upon receiving prediction tth example xt j wt xr Update Upon receiving tth outcome update weight vector interval Zr ifyr It ifFt E It ifFf It Fig I Perceptron algorithm The behavior Perceptron algorithm bounded following theorem 11x 11 m denotes Euclidean norm vector x Theorem 2 Let S sequence 1 trials Let XP 3 maxr llxt I maximum vector length Then comparison vector u llu 11 UP AbsLossPerceptron0 q S AbsLossu S G 2 u2 171X2 Choosing v UPX leads AbsLossPerceptron0 q S AbsLossu S lJX Proof Let du w llu w12 I ui wi2 Consider tth trial S xI It Let yf wy xt If Tt E It wtl wt du wt du wl 0 If yr It wtl wt qxr follows du Wt du WI kui Wtij2 kui Wrli12 il il k Ui Wij2 Cui il il Wti Vti12 2ru Xr Wt Xl q211x112 3 2yu xy Wf Xt x2 From Lemma 1 fact llxt I Xp follows T Bylander Artcial Intelligence 106 1998 335352 339 AbsLossPerceptronwt q St AbsLossu S ux _w f t x wwduAl t 27l I rg 2 Similarly yt It follows AbsLossPerceptronwt rl St AbsLossu S du wt du l1 I 2rl 2 By summing 1 trials AbsLossPerceptron0 q S AbsLossu S c AbsLossPerceptronmr t1 rl St AbsLossu S du 0 du w1 211 W2 2 duO c62 2rl X2 2 2V 2 proves inequality theorem The second inequality ately choice v II follows immedi 32 Boundsfor Exponentiated Update learning given Fig 2 The EU algorithm The EU Exponentiated Update algorithm inputs start vector s positive rate q positive number U Every weight vector consists positive weights sum U Normally weight start weight vector set Un For trial prediction T outside outcome divided eqxi prediction 7 weight Zui current weight vector w multiplied sum U low high The updated weights normalized Weighted Majority algorithm The EU algorithm implement 21 xti E 0 l I Weighted Majoritys update parameter intervals 0 l2 EU decisions Weighted Majority algorithm q In lp U 1 use outcome respectively With parameters classification l negative positive examples interval ln Assuming set s ln l2 algorithm makes The difference weights normalized sum U The EU algorithm instantiated U 1 real value outcomes closely related generalized EG algorithm absolute 17 If EG EU algorithm obtains loss function instead real interval outcomes Kivinen Warmuth 171 analyze generalized EC algorithm square loss function 340 T Bylander Artificial Intelligence 106 1998 335352 Algorithm EUsqU Parameters S start vector Cyt s U si 0 II learning U sum weights weight vector U 0 rate n 0 Initialization Before trial set w 1 si Prediction Upon receiving prediction yr wI xt tth example xt Update Upon receiving tth outcome update weight vector interval It Wtli Wti ifYt E It I UEwtie7Tti y wtiexi 1 Fig 2 Exponentiated Update algorithm This papers analysis borrows 17 weights sum U relative entropy distance ideas analysis EG algorithm normalization function The behavior EU algorithm bounded following theorem ln start vector Theorem 3 Let S sequence 1 trials Let s In Let X maxti Ixti 1 maximum magnitude value example Then comparison vector II cy ui U ui 3 0 AbsLossEUs n U S AbsLossu S U Inn II VX2 2 Choosing q 22innXE leads AbsLossEUs r U S 6 AbsLossu S UX2211nn Proof Let S 1 s XE U defined theorem Let du W eui lnuiwi il 0 In0 0 definition du w 0 Note If sum weights equal sum ws weights dusuilnuilnuilnUlnn il E il il I Bylunder Artiial Intelligence 106 1998 335352 341 Consider du w du wtl 0 If It tth trial St xt I Then Ft wt x t Now E It wrl wt follows dZ4 Wt du Wtl 2 Z4i In 2 Ui In 2 il k ui il k ui il il lnwl UilIlWi il lne7X _ k ui In 2 f3Ti il jl In Appendix A shown k o G VW xt I v il t E This implies du WI du wt1 3 vu Xt rlwt Xt j V2uX2 Using Lemma I follows AbsLossEUwt q U St AbsLossu S u 1 x _w t t t x dKWtduwtl v dX2 2 Similarly jt It follows AbsLossEUwt q U S AbsLossu St du WI du wt1 dg 2 II By summing 1 trials 342 T Bvlander Artificial Intelligence 106 1998 335352 AbsLossEUs n U S AbsLossu S CAbsLossEUwr I U St AbsLossu S I 1 c du Wt du I fI 11 VX2 2 du s du w1 rlwd2 17 2 6 dus I rlfJE rl Inn v 2 eJEx I 2 proves inequality theorem The second ately choice 7 q inequality follows immedi 33 Discussion Theorems 2 3 provide similar results They form AbsLossA S AbsLossu S O1 1 length trial sequence If 1 known advance good choice learning rate q leads allowed vary parameters fixed AbsLossA S AbsLossu S Od lengths length approach small absolute Because sequence bounds depend 1 It hard generate trial sequences bounds The bound loss trial matter Perceptron twonorms algorithm depends Up X bound respective best weight vector example vectors The bound EU algorithm depends U onenorm best weight vector sum weights X infinitynorm example vectors maximum magnitude square loss case 9 value example Inn 171 previous mistake bound analyses outperform Perceptron algorithm best comparison weight vector small weights example vectors small values term Thus similar 191 EU algorithm appears The bound EU algorithm vector nonnegative comparison comparison transformation upper bound sum weights absolute values example x doubled appending transformation weights simple class include negative weights U length example This values x number weights change inn term In 2n UE However 171 Specifically expand restrictive doubles sum I Bylander ArtQicial Intelligence 106 1998 335352 343 4 Mistake bounds To analyze concept learning consider trial sequences consist classication trials outcome trial positive negative absolute version online algorithm A classification algorithm classifies example positive T 0 That outcome label The classification loss version y 0 negative interval 0 interval 0 negative examples distinguished F 0 making classification positive examples outcome performed No updating classification threshold apply outcome convenient example classified correctly The choice 0 analysis note Theorems 2 3 intervals classification threshold trials absolute loss algorithm uses outcome interval To relate 0 1 loss classification useful An absolute intervals positive examples outcome loss algorithm performs updating loss absolute classification classification subsequence trial sequence omits zero trials change ordering remaining loss slightly different outcome l interval 11 negative examples An absolute y correct interval As result absolute loss algorithm given trial greater equal Ol loss algorithm weight vector Ol loss trial 1 incorrect 0 correct For following observation algorithm trials cc Observation 4 Let S classihcation trial sequence If classification algorithm makes m mistakes S subsequence S length m corresponding absolute subsequence S length m absolute loss algorithm absolute loss m classification algorithm fewer loss m Equivalently loss algorithm absolute m mistakes S Based observation mistake bounds derived The notation AbsLoss algorithm 0 1 Loss absolute 0 1 loss classification loss absolute algorithm Perceptron EU algorithms loss Theorem 5 Let S sequence 1 classification trials Let XP maxt xt I Suppose exists vector u IIu 11 6 UP AbsLossu S 0 Let S subsequence S length m Then m UP2Xz implies AbsLossPerceptron0 lX S m implies 01LossPerceptron0 lX S cm Proof Using Theorem 2 AbsLossu S 0 n lX m UX AbsLossPerceptron0 n S AbsLossu S F 2 u rlmX2 4 2 TgTrn 344 I Bylander Artificial Intellipwe 106 1998 335352 Because subsequence Observation 4 implies 01LossPerceptron0 q S m q length m absolute loss m Actually value learning rate affect mistake bound start It affects relative vector zero vector 0 classification length current weight vector This weight vector learning rate q times sum subset example vectors q positive affect sign dot product threshold separation 1 0 classification threshold The mistake bound corresponds previous mistake bounds exists vector u example suppose means outcome positive negative example respectively This corresponds Now u transformed literature For llull U AbsLossu S 0 This 1 1 example x unit vector Xp 1 mistake bound identical bound Minsky Papert 22 unit vector separation 6 l U If U 1J2 Now consider EU algorithm Theorem 6 Let S sequence 1 classication Suppose AbsLossu S 0 Let s In m Then m 2UkX2 Inn implies trials Let X 3 maxti IXtiI exists vector u nonnegative weights cy ui U Let S subsequence S length AbsLossEUs lXi S cm implies 01LossEUs lUXz S cm Proof Using Theorem 3 AbsLossu S 0 q lUXz m 2lJ2X2 Inn AbsLossEUs q U S AbsLoss S Inn amX n 2 U2X21nn m cm E E 2 Because Observation 4 implies 01LossEUs subsequence q U S C m q length m absolute loss m While learning rate important tion U unnecessary The normalization relative sizes EU classification normaliza algorithm affects sum weights This mistake bound corresponds Balanced algorithm mistake bounds Weighted Majority algorithm equivalence 191 4 Demonstrating Littlestone Block 2 Novikoff 23 Papert 24 generally credited providing proofs mistake bound 4 In Littlestone 191 Weighted Majority algorithm analyzed general linear threshold learning algorithm addition analysis master algorithm Littlestone Warmuth 21 7 BylanderArtijiciul Intelligence 106 1998 335352 345 somewhat tedious superficial differences algorithms bounds However bigoh equivalence analysis X 1 comparison vectors separation 6 weights sum 1 To separation bounds 1 sum weights needs iYE 1 S Under conditions paper 2lXz Inn 2 In n6 The Oln ns mistake bound agrees Littlestone easily shown In Littlestones Mistake bounds derived best comparison vector makes trial loss theorem EU mistakes Note comparison vector makes mistake classification deviate threshold UX implies absolute UX 1 absolute algorithm loss algorithm This leads following Theorem 7 Let S sequence 1 classification trials Let X 3 maxi lxy I exists vector u nonnegative weights cy ut UE Suppose 01Lossu S k Suppose AbsLossu S 0 trials k Let S subsequence qf S length m Let n mistakes Lets learning rate n C 2X Then m LX 1k y 1 _ Vx 2 implies AbsLossEUs q UE S m implies 01LossEUs 7 U S C m Proof k mistakes corresponding If 01Lossu S k AbsLossu S 0 AbsLossu S LX 1k mistake trials absolute loss rX 1 To use Theorem 3 want obtain AbsLossEUs n U S AbsLos S UE Inn I7mGXZ r7 2 crX 1k CInn pp rl rlmGX2 2 The expression m 17 2X2 m LX 1k y dJk l7 Because Observation 4 implies 01LossEUs 1 q U S C m q length m absolute subsequence loss m One special case U 1 X 1 This corresponds 19211 That inputs EU algorithm EU algorithm master algorithm produced outputs learning algorithms turn trained sequence observations Suppose EUs inputs produced algorithm makes k fewer mistakes 1 encoding negative positive respectively Then mistake bound 267k 267 Inn obtained predictions 1 346 7 Bylunder Artijicial Intelligence 106 1998 335352 r 05 This close Weighted Majority mistake bound 264k 264 Inn j3 el 21 5 5 Toleranced absolute loss The analysis leads pertrial goal come tolerance directly hitting nonnegative ismodifiedtoZZftwhereyZifandonlyifytyyforsomeyZ The absolute interval indicates outcome The notation AbsLoss S t tolerance t outcome loss algorithms consider extension r interval Z trial trial sequence S interval accordance modified outcome loss calculated For Perceptron EU algorithms loss qX22 77X22 respectively pertrial worstcase 3 generalized loss eliminated obtain following leads additional pertrial analysis If t equal values turns independent length sequence The proofs Theorems 2 leaving constant additional loss sequence theorems intervals Theorem 8 Let S sequence 1 trials t positive real number Let XP 3 maxt llxt 11 r 2rX2 Then comparison vector u lull UP AbsLossPerceptron0 Proof Let II S 5 AbsLossu S 3 lJ2X2 4t du w 11 wl12 eui il Consider tth trial S Xr It Let y wt xt If E Zr f 5 wrl wt du wt du WI 0 If j Zt f x wtl wr qxt In proof Theorem 2 shown du wt du wtl 3 2qu xt wy Xt q2X From Lemma 1 fact llxr II 6 Xp follows AbsLossPerceptronwt r St r AbsLossu S AbsLossPerceptronwt q St T AbsLossu S uxtwtxtt du wt du wtl 2 2rl Similarly yt It f 5 follows 5 One obtain analogue Theorem 7 Perceptron worse Ok Inn algorithm case Okfi algorithm bounds master Z Bylander Artificial Intelligence 106 1998 335352 341 AbsLossPerceptronw n St r AbsLossu S dW W du 27 r1 I llx2 2 By letting t r7X22 summing 1 trials AbsLossPerceptron0 n S r AbsLossu S CAbsLossPerceptronruf q S r AbsLossu S fl proves inequality theorem q real number Let s In start vector Let X maxti Ixti 1 q 2tUEX2 Then Theorem 9 Let S sequence 1 trials t positive In comparison vector U Cy ai U ai 0 AbsLossEUs n U S r AbsLossu S E E 2r U2X2 Inn Proof Let S 1 s XE U defined theorem Let du W kui lnuiwi il 0 In0 0 definition d u W 0 Recall proof Theorem 3 If sum weights equal sum ws weights du s U Inn Considerthe du wt du wtl 0 If yr A Zt f r tthtrial St xt It ThenF Wr xt Now ifyt E It ft rut1 Wt UE Wt iPri Wti tjecBi In proof Theorem 3 shown du wt du Wtl 3 VU xt qtu xt p Q2LIXZ 2 Using Lemma 1 follows AbsLossEUx q U St t AbsLossu St AbsLossEUxt r U St r AbsLossu St uxtwtxtt du wt du wti XZT rl 2 348 7 Bylunder Artificial Intellipm 106 I 998 335352 Similarly yt It f r follows AbsLossEUwr rj U S T AbsLossu S du wt du w1 rl WEXE2 2 By letting t qX22 summing 1 trials AbsLossEUs q U S t AbsLossu S CAbsLossEUw I U S t AbsLossu S 11 proves inequality theorem q absolute For algorithms absolute toleranced loss best comparison vector constant nontoleranced If best comparison vector zero sequence matter long sequence loss bounded constant absolute sequence These results strongly support claim Perceptron EU algorithms online algorithms loss toleranced absolute loss algorithm minimizing absolute exceeds loss 6 Randomized classification algorithms classication To apply Theorems 8 9 consider concept sequences A randomized algorithm defined follows The prediction 7 converted predicting positive 7 l2 negative positive probability F randomizing fixed randomized prediction trial trial sequence prediction predict 6 l2 If l2 y l2 l2 predict negative It assumed method intervals outcome classification classification independent outcome learning classification prediction Under randomized prediction l 00 vals spectively toleranced absolute m l2 Note classification prediction l2 However tolerance t l2 loss determined based outcome classification 11 outcomes converted positive negative classification outcome inter trials added intervals l2 regardless performed F l2 updating correct The idea randomized algorithm borrowed 2 11 analyzes randomized version Weighted Majority algorithm This papers randomization ranges 7 positive negative predictions deterministic differs Refer Section 4 definition classification trial sequence T Bylander Artcinl Intelligence 106 1998 335352 349 Note toleranced absolute loss randomized classification algorithm F prediction trial referring prediction classification classification correct classification predictions 1 incorrect predictions toleranced absolute supports following observation loss 0 In cases loss greater equal expected value 0 1 loss This equal probability incorrect toleranced absolute l2 7 l2 Otherwise Observation 10 Let S classification trial sequence Then toleranced absolute loss randomized classification algorithm S greater equal expected value algorithms 0 1 loss S The notation AbsLoss randomized classification l2 algorithm 0 1 Loss l2 toleranced absolute 0 1 loss loss Theorem 11 Let S sequence 1 classification exists vector u Iu II UP AbsLossu S 0 Then trials Let Xr 3 maxt IJXt I Suppose AbsLossPerceptron0 S T implies E0ILossPerceptron0 S k 6 q Proof Using Theorem 8 AbsLossu S 0 r 1 X r l2 AbsLossPerceptron0 n S r AbsLossu S p U2X2 4r p U2X2 2 Observation 10 implies EOlLossPerceptron0 n S r U2X22 q Theorem 12 Let S sequence 1 classijkation trials Let X 3 maxti Ixti I Suppose exists vector u nonnegative weights c ui Ue AbsLossu S 0 Lets Ln Ln Then AbsLossEUs S lJXilnn implies FO1LossEUs S JJ X2lnn Proof Using Theorem 9 AbsLossu S 0 n lUXz r l2 AbsLossEUs n S r AbsLossu S U X2 In n 2r U2X Inn EE Observation 10 implies EO1LossEUs q S t 6 UXlnn q 350 Bylander Artijicial Intelligence 106 1998 335352 For randomized algorithms worstcase bounds expected 0 1 loss half worstcase mistake bounds deterministic improve worsecase bounds factor 2 value 7 close 0 0 1 loss 1 deterministic worst case expected 0 1 loss close l2 randomized algorithms Roughly randomization algorithms 7 Conclusion This paper presented shows online algorithms algorithms sequence trials examples Specifically loss online algorithms comparable comparison vectors analysis Perceptron minimizing Exponentiated Update loss paper shows worstcase absolute optimal weight vector class absolute The analysis fully general No assumptions probability distribution trials The Perceptron analysis refers maximum vector length example maximum vector length comparison vector The Exponentiated Update analysis refers maximum magnitude value example sum weights comparison vector linear separability trial sequence When classification loss bounds closely paper shown absolute needed deterministic study classification behavior algorithms target comparison vector allowed drift linearly separable nonseparable known mistake bounds research randomized versions algorithms Additional linearly related separable case 7 Based minimizing absolute loss possible derive backpropagation algorithm suitable multiple layers linear threshold units It interesting initial conditions parameters lead good performance learning determine Acknowledgements Thanks Manfred Warmuth anonymous reviewers comments paper This based work supported Texas Advanced Research Program material Grant No 1997010115225 Appendix A Inequality exponentiated update A general version Lemma A 1 shown Hoeffding 15 p 221 It presented completeness Lemma Al Let w E 8 consist nonnegative weights cy wi U Let x E IP X 3 maXi xi 1 Let q real number Then following inequality holds ln 2 ii x I Vz il E 5 See 11314 interesting research lines I Bylander Artificial Intelligence 106 1998 335352 351 Proof Define f fswXlnfyg il E Now differentiate f twice respect 4 When 17 0 f r wx 0 afaq w XI With regard second partial derivative following bound holds second partial derivative Hence Taylors theorem inequality lemma q References l P Auer M Warmuth Tracking best disjunction Machine Learning 2 HD Block The Perceptron model brain functioning Reviews Modem Physics 34 1 1962 123 appear 135 3 A Blum A Frieze R Kannan S Vempala A polynomialtime algorithm learning noisy linear threshold functions Proceedings 37th IEEE Annual Symposium Foundations Computer Science 1996 4 T Bylander Learning linearthreshold functions presence classification noise Proceedings 7th Annual ACM Conference Computational Learning Theory 1994 pp 340347 5 T Bylander Leaming linear threshold approximations Perceptrons Neural Computation 7 1995 370379 6 T Bylander Learning probabilistically linear threshold Conference Computational Learning Theory 1997 pp 485490 consistent functions Proceedings 10th Annual 7 T Bylander Worstcase absolute Providence RI 1997 pp 485490 loss bounds linear learning algorithms Proceedings AAAI97 S N CesaBianchi Analysis gradientbased algorithms online regression Proceedings 10th Annual Conference Computational Learning Theory 1997 pp 163170 9 N CesaBianchi PM Long MK Warmuth Worstcase quadratic rule IEEE Trans Neural Networks 7 1996 604619 WidrowHoff loss bounds generalization IO E Cohen Learning noisy Perceptrons Perceptron polynomial time Proceedings 38th IEEE Annual Symposium Foundations Computer Science 1997 111 RO Duda PE Hart Pattern Classification 121 SI Gallant Perceptronbased 131 M Herbster M Warmuth Tracking 14 M Herbster M Warmuth Tracking Scene Analysis Wiley New York 1973 learning algorithms IEEE Trans Neural Networks 1 1990 179191 best expert Machine Learning appear best regressor Proceedings 1 lth Annual Conference Computational Learning Theory 1998 1151 W Hoeffding Probability inequalities sums bounded variables J Amer Statist Assoc 58 1963 1330 16 RL Kashyap Algorithms pattern classification JM Mendel KS Fu Eds Adaptive Learning Pattern Recognition Systems Theory Applications Academic Press New York 1970 pp 8 ll 13 352 L BJv Artificial Intellipm 106 1998 335352 171 J Kivinen MK Warmuth Exponentiated gradient versus gradient descent linear predictors Information Computation 132 1997 I63 irrelevant attributes abound new linearthreshold algorithm IS N Littlestone Learning quickly Machine Learning 2 1988 2853 18 19 N Littlestone Mistake bounds logarithmic linearthreshold learning algorithms PhD Thesis University California Santa Cruz CA 1989 20 N Littlestone Redundant noisy attributes attribute errors linearthreshold learning Winnow Proceedings 4th Annual Workshop Computational Learning Theory 1991 pp 1477156 21 N Littlestone MK Warmuth The weighted majority algorithm Information Computation 108 1994 212261 22 ML Minsky SA Papert Perceptrons MIT Press Cambridge MA 1969 23 ABJ Novikoff On convergence proofs Perceptrons Theory Automata Vol XII 1962 pp 615622 Proceedings Symposium Mathematical 24 S Papert Some mathematical models learning Proceedings 4th London Symposium Information Theory 196 1 25 F Rosenblatt Principles Neurodynamics 26 J Shavlik RJ Mooney G Towell Symbolic neural learning programs experimental Spartan Books New York 1962 comparison Machine Learning 6 1991 I1 l143