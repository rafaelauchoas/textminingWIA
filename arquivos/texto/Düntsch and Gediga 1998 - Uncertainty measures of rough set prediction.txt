Artificial Intelligence 106 1998 109137 Artificial Intelligence Uncertainty measures rough set prediction Ivo Diintsch ab2 Giinther Gediga btc 12 School Inforrnarion Sofbvare Engineering University Ulster Newtownabbey BT 37 OQB UK b Instirut fir Semantische Infomationsverarbeitung Universitiit Osnabriick 49069 Osmbriick Germany FB PsychologieMethodenlehre 49069 Osnabriick Germany Universitiit Osmbtick Received 14 July 1997 received revised form 19 March 1998 Abstract The main statistics rough set data analysis approximation quality limited value choice competing models predicting decision variable In keeping rough set phillosophy noninvasive data analysis present model selection criteria information length principle Our main spirit minimum description procedure based principle indifference combined maximum entropy principle minimum The applicability proposed method keeping external model assumptions demonstrated comparison error rates results C45 14 published data sets 1998 Elsevier Science BV All rights reserved theoretic entropy Keywords Rough set model Minimum description length principle Attribute prediction 1 Introduction Most commonly procedures observed phenomena presuppose subject analysis random influences regression correlation applied data prediction require parameters outside character order statistical methods variance properties quantitative One methiod avoids external parameters RSDA developed Z Pawlak coworkers early 1970s 15162021 recently received wider attention means data analysis 23 The rationale rough set model observation rough set dutu analysis author Email iduentschulstacuk Corresponding 1 Email ggetigaLucePsychoUniOsnabrueckDE implied 2 Equal authorship 0004370298 PII SOOO4370298000915 matter 0 1998 Elsevier Science BV All rights reserved 110 I Diintsch G Gediga Artijicial Intelligence 106 1998 109137 The imprecision granularity ambiguity information decision usually vague uncertainty Vagueness caused information Granularity sources coming representation explanation prescription based vague information introduce 24 In words original concept described roughly object property model realization sets 0 CERTAINLY l POSSIBLY 0 CERTAINLY NOT function internal knowledge This looks conspicuously logical level algebraic semantic rough set logic corresponds fuzzy logic threevalued membership like fuzzy membership function algebraic 819 Rough set analysis uses rely prior model fuzzy set methods probabilistic models In words instead assumptions rough set analysis utilizes solely external numbers additional parameters granularity structure given data expressed classes suitable equivalence relations Of course mean RSDA model assumptions example indicate statistical model RSDA pnnciple indifference However model assumptions ignorance happens region indiscernibility Section 21 given granularity information admit complete The results RSDA seen background possible mind rough set model structural aspects data attribute serve valuable information contextual minimum tries extract information neglecting domains This keeps model assumptions indicator direction pure form numerical possible analysis The relationship RSDA statistical modeling complementary Table l discussed lo Knowledge representation tabular form OBJECT ATTRIBUTE VALUE relationship databases Section 22 rough set model information systems similar relational If Q set predictor features d decision attribute RSDA generates rules form xqmqxdmVxdmVVxdm A qEQ X attribute value object n respect attribute r 11 Table 1 RSDA versus statistical modeling RSDA Statistical models Many featuresattributes data points Few variables data points Describing redundancy Top reducing attribute set Reducing uncertainty Bottom introducing new variables I Diintsch G Gediga Artijcial Intelligence 106 1998 109137 111 We rough set model right hand 11 proper disjunction If term right hand rule deterministic Whereas RSDA handles rules deterministic remains unclear status indeterministic straightforward manner rules indeterministic sense rules If rules based observations granularity high rule chance In order test significance rules use randomization methods null hypothesis conditional probability rule assuming compute objects randomly assigned decision classes simple procedures based randomization true In I 11 developed techniques evaluate validity prediction based principle indifference Section 24 statistics RSDA briefly described underlying technique Although randomization methods useful expensive resources applicable conditional testing scheme tell rule chance provide metric comparison different rules Q d R d let different models uncertainty Thus need different criterion model selection minimum description length MDLP 2728 states best theory explain given phenomenon principle d minimizes sum l binary l binary predictor length encoding hypothesis Q length encoding decision data d hypothesis Q In sequel present different probability distributions frame M attractiveness approach rules 11 considered aggregate different ways model selection RSDA based spirit MDLP Within model uncertainty context selection criterion H M Q d information l effort coding hypothesis Q expressed entropy function HQ l uncertainty classify randomly chosen observation given hypothesis expressed suitable entropy terms optimal number decisions guessing Hd I Q basic tools RSDA The paper organized follows main properties usage entropy functions Section 3 contains approaches known data sets Finally Section 5 consists summary outlook Section 4 applies main approach Section 2 uncertainty 112 I Diintsch G Gediga Artijicial Intelligence 106 1998 IO137 2 Basic tools constructions 2 I Approximation spaces An equivalence 8 set U transitive reflexive symmetric binary relation pair U 0 approximation space In context shall equivalence spaces core mathematical information idea granulation relation relation indiscernibility relation Approximation described classes indiscernibility concept RSDA usage reflects Recall partition P set U family nonempty pairwise disjoint subsets relation 8 associate partition PO U b E U class PQ aOb The classes PO U union U With equivalence specifying form b E U aeb By abuse language speak classes equivalence mean classes associated partition 8a class ofa modo 8 relation The interpretation membership limited classes 6 unions This leads following definition rough set theory knowledge objects U extends classes 8 knowledge subset X U For X C U lower approximation positive region X XEf Uex x E xl upper approximation possible region X If X C U given predicate P x E U 1 x E X means x certainly property P 2 x E x means x possibly property P 3 x E U x means x definitely property P The area uncertainty extends XX area certainty XUx 22 Information systems Knowledge representation RSDA relational tables An information z UT vq fq consists I Diintsch G Gediga Artificial Intelligence 106 1998 109137 113 Boundary Set X Lower approximation X _ _ _ Difference upper lower approximation X Fig 1 Rough approximation 1 finite set U objects 2 finite set 0 attributes 3 q E i2 l set V attribute values l information function f4 U V In sequel shall use Z generic P Q R G 0 We write x4 instead x x respect attribute q Furthermore suppose want predict attribute sets Q R G 2 information 1 U 1 n value denote d E fZ decision attribute Example 1 We use small information concepts developed illustrate shall predicted variables Smoker given Table 2 running example sequel An attribute Heart Disease HD S Body Mass Index MI With Q associate equivalence relation 8Q U defining partition induced 8Q denoted P6Q simply PQ 114 I Diintsch G Gediga Artzcial Intelligence IO6 1998 109137 Table 2 An example information No 1 2 3 4 5 6 I 8 9 S yes yes yes yes BM HD normal obese normal obese normal normal obese obese normal yes yes yes The interpretation Q following view world U limited attributes given Q able distinguish objects equivalence Classes Of 8Q Example 1 Continued The classes 8Q 6d follows Q iSI Classes 6 123491 56781 WW 113569 24781 IS BMIl 11391 2413 1561 78 d IHDI 11234791 15681 We use definition upper respectively lower approximation sets 8Q defined previous section It hard Y c U uxEU 8QxnYd upper approximation Y respect Q IQ x E u 8Q c Y 21 22 lower approximation 1 Y respect Q If Q understood write r The equivalence Let Q d G PQ x Pd relation relations 8Q obtain rules following way XYEQXY Q Observe 21 XcyQ ifandonlyif XOyQO ifandonlyif XflY0 I Diintsch G Gediga Artificial Intelligence 106 1998 109137 115 XYQdexnY0 23 Observe determine knowledge gained Q X n Y 0 alsoby 22whether X C Y A pair X Y E Q d called Qdrule rule Q d understood X Y By abuse language shall Q d rule identify singleton sets element usually written danger confusion normally contain If X Y E Q d X corresponds left hand implication 1 l Y corresponds disjuncts right hand Example 1 Continued The rule S HD consists pairs 1123491 11234791 567g 11234791 1567gl 56 gl BMZ HD pairs 113569 1234791 Il 35S 56 gl 247gl 11234791 247g 56 g S 13MZ HD obtain 1391 11234791 I241 1123479 561 56 g 7gl 11234791 17g 5681 The deterministicorfunctionalpart Q d written Q 2 d set X I E Q d X s Y If X Y E Q 2 d class X called ddeterministic deterministic d understood values x respect attribute values d In case values x E U attributes Q uniquely determine Example 1 Continued The deterministic 56 deterministic deterministic class BMZ HD classes S BMZ HD 139 class S HD 12349 24 116 I Diintsch G Gediga Artificial Intelligence 106 1998 109137 If Q d Q 2 d Q d function Q d deterministic write Q d case d dependent Q It hard Q j d 8Q G ed terminology line usual convention RSDA A special role played deterministic Q d define vQd u x E PQ x Y E Q 2 d In words vQd understood shall write V instead vQd Note union ddetetministic eQ classes If Q d nVI0 nIVI2 24 singleton class 6Q deterministic definable dejinable Q understood d A class Y ed called Q Y C V Example 1 Continued The deterministic parts easily seen VSHD 123491 VBMIHD 0 VIXBMIJHD I 234569 Even RSDA symbolic method implicitly makes statistical assumptions briefly want start looking single equivalence U The inherent metric approximation relation 8 U 0 approximation quality yex def ll pl 25 22 p 16ffl If 0 understood shall usually omit subscripts The value yX relative frequency objects U correctly classified systems 22 p 221 choose different equivalent definition quality Q respect knowledge given 8 X The function y generalized information suited purpose As measure approximation d define approximationfunction yQ d 1 UX l PQ X ddeterministic IUI 26 Note vQ4IUl IVI Qkd ifandonlyif yQdl Example 1 Continued We YSHD YBkf HD YSBMIHD 5 I Diintsch G Gediga Artial Intelligence 106 1998 109137 117 It hard statistical principle underlying approximation functions principle indifference l information equally assumed likely occurrence basic events Q called reduct ofd minimal respect property Reducts particular importance rough set theory means feature reduction y Q d 1 23 DataJiltering discretization Even RSDA inherent categorization mechanism data satisfactorily 121 based information provided indiscernibility ways One method keeps close minimum continuous RSDA philosophy keeping outside assumptions described relations This technique collects values feature single value taking union class decision deterministic equivalence attribute statistical basis rule enlarged Ithis way underlying significance rule increased classes totally contained Section 24 possible handle filtering procedure For example attribute q rule collect 2 35 single attribute value q feature procedure The important internal dependency Even method procedure fixed example kept intact need additional parameters step regarded operationahzation cheap standard algorithm decision attribute set engine GROBIAN 9 simple investigations Browne et al 4 Browne 3 indicate Nevertheless scheme cope effectively complex sophisticated RSDA use external parameters restrictive modelling invite reader consult Bazan l Nguyen Nguyen interactions works surprisingly discretization discretization methods For methods applicable continuous variable structure In words implemented rough assumptions 181 references The claim RSDA applicable real life problems handle continuous variables open problem fact The success applications fuzzy controlling requires discretization continuous data shows distinction continuous data versus discrete data necessarily discrete imply types data We refer reader Section 4 methods wlhich prediction quality RSDA based methods explored data sets consists continuous variables need different continuous methods different respectively handle 118 I Diintsch G Gediga Artcial Intelligence IO6 1998 109137 24 Signijkance testing Suppose want test statistical significance rule Q d Let _X set permutations U For cr E C define new set feature vectors Xf X r d 27 xd values according In way permute constant The resulting distribution v Q ad CT E E evaluate strength prediction Q d The value p y Q d 1 Ho measures quality defined rule denoted Q cd We use permutation extremeness observed approximation leaving r pvQ 4 I Ho lb E x vQ ad 2 vQ 411 IUI 28 If cx py Q d 1 Ho low traditionally 5 reject null hypothesis reject null hypothesis rule signzjkant casual Failure mean true randomization tests necessary condition significance discussion 6 Randomization statistical technique require representative population theoretical generalization sampling study randomization accord stated objective This aspect contrast techniques Even technique needs parametric suppose estimators latent probabilities equivalence classes population sample procedure uses information given sample statistical classes suitable percentages observed equivalence bootstrap assumptions Example 1 Continued Table 3 tells approximation sets S BMZ S BMZ prediction HD example Table 2 qualities significance information The best approximation quality attained combination significance terms statistical attributes S BMZ However significant predictor prediction S smaller S HD set S preferred unlikely outcome HD success chance Therefore success chance prediction predicting set S BMZ quality predict HD evidence approximation Table 3 Approximation quality significance predicting attributes Attribute set Significance Interpretation 0556 0000 0778 0047 locm 0144 Not casual Y 5 Casual u 5 Casual Y 5 I Diintsch G Gediga Artcial Intelligence 106 1998 109137 119 In applications observe reducts attribute sets acceptable approximation statistical validity sets comparable good statistical foundations quality Thus need additional criterion model selection laid section testing gives information quality Significance 25 Partitions information measures Let P partition U classes Xi k having cardinality compliance statistical assumption elements U randomly distributed element x class Xi ri n We define entropy P rough set model assume classes P probability ri In HPlog io I 29 If 8 equivalence instead HP Furthermore instead H6Q relation U P induced partition write H8 usually write HQ Q set attributes The entropy estimates mean number comparisons minimally necessary retrieve equivalence class information randomly chosen element x E U We think class entropy P measure granularity partition HP reaches maximum information gain fixed n In words universal class guess correct class element partition specific class hardest predict inclusion element contains singletons information gain maximized 0 P corresponds tv HP identity relation For partitions PI P2 U associated equivalence relations 8t write Pr P2 81 02 The following Lemma known Lemma 21 IfPl P2 HP1 3 HP2 Proof Since class P2 union classes Pt suppose generality associated P2 p1 2 p3 pm Now loss associated Pr pr pm m 3 probabilities HPl P2 P3 pm 1 2 H Pl ___ Pl p2 P2 ___ Pl p2 H2plpzH b HP2 example 114 p 211 Corollary 122 If R C Q C 52 HR HQ 120 I Diintsch G Gediga Artcial Intelligence 106 1998 109137 More classes automatically mean higher entropy need hypothesis Pt P2 example 1585 H f f H 4 6 1447 For later use mention 14 p 211 entropy function property strong additivity Lemma 23 Suppose ii t lij j 6 ni sets positive parameters CJi C 6ij 1 ini Then C C Eti Ili j loiT2 jiai klOf C fiij10g2 jn 3 Rough set prediction The problem want address variant classical prediction problem l given decision attribute d best attribute set Q 2 fi predict Q dvalue object x given values x features contained We variant RSDA rules determined equivalence classes combining prediction quality partitions U involvedsee feature reduction 11 23and The prediction problem raises questions l Which subsets Q 52 candidates l What metric look like determine select best attribute set best attribute set prediction approximation quality y defined In conventional RSDA success conditional measurement researcher However approximation use different feature sets Q R prediction d To define unconditional measure prediction success use MDLP idea combining rule RSDA 26 measure choice attributes qualities compared deterministic measure uncertainty applying indeterministic l program complexity l statistical uncertainty rule global measure prediction success In way dependent independent treated similarly attributes In sequel discuss type uncertainty based informationtheoretic entropy functions Section 25 Our model selection criterion entropy value H Q d aggregates set Q attributes different models M handle l complexity coding hypothesis Q measured entropy HQ partition associated equivalence relation 6Q 29 I Diintsch G GedigaArtcial Intelligence 106 1998 109137 121 l conditional coding complexity Hd Q d given values attributes Q H d HQ Hd I Q 31 The estimator Hd 1 Q measures given class 8Q important knowledge given Q uncertainty predict membership want gauge success model conditioned class 6d The importance H Q d fact aggregates uncertainty predicting elements Hd 1 Q effort HQ coding This enables researcher compare different attribute sets Qi terms common unit measurement conditional measure prediction success like y Hd Q hypothesis Since entropies defined probability measures arise partitions nelement set remarks 29 upper bound log In order able compare different entropies model M define 0 las Hd log follows normalized entropy measurebounded d identity if8Q If H 4 5 log2 n sQdfl HQ d Hd logzn Hd 32 The measures SM Q d constructed approximation quality way comparable l S Q d 1 entropy measure good possible SM Q d near 0 shows coding information indicates poor model predicting attribute d near theoretical maximum Similarly based bounds 0 6 Hd Q 6 log2n normalize Hd I Q bY SdI Ql Hd I Q lon We shall later measures Sd identicalto We assume approximation prediction requires quality y models presented distinguished respective associated parameters I Q comparableand special case 33 specification probability distribution choice distributions Throughout suppose classes 6Q X0 Xt ri f Xi 1 classes Yo Y Furthermore let c t VXuUUX 122 I Diintsch G Gediga Artificial Intelligence 106 1998 109137 Xi c exactly deterministic classes 8 In accordance previous observations assume principle indifference Also shall write y instead y Q d Q d understood set r2i dzf ri n t 31 Prediction I knowing The approach based assumption structure uncertainty estimated interaction d Q In case class X 8Q determines probability distributions based intersection classes This assumes know 1 classes 2 classes 8Q 3 interaction intersections It follows representative rough set approach 111 order justify prediction assume data set sample This general problem data mining discussed Uncertainty sense model predominantly feature predictor set Q intended RSDA local feature intersection equivalence classes X E 8Q Y E 6d We shall procedure code rules apply complexity simple procedure guess kQ n odd point view words guided purely viewed identical statistical view Although effort adopt approach RSDA context 30 shall discuss aspects work RSDA approach different The partition induced 19 dAf 6Q fled nonempty sets Xi n Yj t j s associated parameters defined n Uij IXi n Yjl n Thus ffeOC xx j log2 js Now define HQ d f HB 34 35 In information emphasize theory HQ d usually written HQ d use notation view world consists Q want predict d One problem approach symmetry HQ d docQ n Hocd Q We shall discuss problem instead refer reader Jumarie p 49ffl Li Vitartyi 17 p 65ffl The proof following proposition straightforward left reader 14 p 24ff I Dtitsch G Gediga Artiial Intelligence 106 1998 109137 123 Proposition 31 Let d Q C L Then 1 HioCQ 4 2 Hd 2 HCQ d HQ ifand ifdQ C 6d Applying 1132 normalized lotentropy measure SOc Q d definable andgiven Hd c log2 nwe obtain SQ d 1 HOc Q f d Hd log Hd ForeachiGt jslet lij df IXi n Yil ri This estimated probability element Xi class Xi fl Yj In words conditional probability x E Yj given n E Xi Observe CTii Cfij 1 js parameters furthermore fri fiij satisfy hypotheses Lemma 23 Eti riij lXinYjl cij n I Substituting 36 35 applying Lemma 23 obtain HOD4dHQCiCqij10g2 js HQ 2 icl si C js rjij 1 sitme Gij 1 6 c The conditional entropy d given Q Zidocd 1 Q fEf k fi C6ij log2 icl js 36 37 This usual statistical definition conditional expression entropy Its normalization leads SOCd I Q 1 HOCd I Q log2n Example 1 Continued Table 4 shows statistical quality example information Table 2 information analysis prediction Although measures SOc Q d SOCd I Q vote S best predicting set given dataand convergence need true general case line results significance test Table 3 124 I Diintsch G Gediga ArtiJicial Intelligence 106 1998 109137 Table 4 Statistical information measures predicting quality Attribute set HOcQ d SOcQ d fd I Q SOc I Q IS1 VW IS BW 1352 1891 2197 0808 0568 0432 0361 0900 0222 0835 0587 0814 Y 0556 0000 0778 simple examples Example 2 Some measures Hloc approximation shall demonstrate average uncertainty quality y work differ Suppose q1 d values 0 1 suppose observe probabilities 41 I 411 dO dl c l4 l4 l2 l4 114 l2 c l2 112 1 We calculate HOcql d 2 HOCd I ql 1 Now consider attribute q2 values 0 3 observed probabilities 42 o 42 1 92 2 923 dO dl c 114 0 l4 l16 l16 3116 3116 114 114 l8 l8 l4 c l2 l2 1 Whereas q2 enables predict 25 cases deterministically rule ifqzO d 0 q1 predict d Comparing entropy measures observe HOCq2 d 26556 HOcql d 2 HOCd 1 q2 06556 HOCd 1 ql 1 entropy measure HOc Q d favors 41 conditional example large classes predict obviously entropy measure Whereas HOCd 1 Q votes q2 better predicting attribute The explanation effect simple encoding small number classes effectively The prediction success second table overruled large number small classes high uncertainty causing high coding complexity coding complexity predicting attribute effect high coding effort eliminated better prediction success q2 results smaller conditional entropy measure If subtract I Diintsch G Gediga Artcial Intelligence IO6 1998 109137 125 A tabsle presents example Hiocd 1 Q optimal rough set prediction certain circumstances 43 o 431 ll6 l16 l16 l2 7116 l2 c l2 l2 1 dO dl c bet based q2 Having conditional measure HOcd 1 q3 Although q3 predicts outcome deterministically 05436 better HOcd 1 q2 06556 The essence result bet given q3 preferable knowledge q3 0 enables predict outcome d 0 likely d 1 q3 1 predicts d 1 value q2 1 comparably time With attribute 42 bets given given observation bad Although 1 II dataset knowledge d value di search set dvalues success q3 bad ql consequently In terms RSDA prediction betting situation given q3 satisfactory 93i 0 knowing yql d q yq3 d 0 As examples special layout rough prediction problem lotmodel optimizes guessing outcome dependent variable necessarily perfect prediction statistical entropy measures account In sections present entropy measures integrated rough set approach suitable rough set prediction The earliest paper concern entropy rough set analysis Wong et al 30 In Theorem 2 later restated Teghem Benjelloun strong connection RSDA entropy measurement 29 Proposition 6 following claimed translated connection terminology Claim Suppose thatfor c t IXi n Yj 1 di j S Then ftCd I Q Ir YQl n forall j s Consider Suppose U 0 1 7 partition given d sets following counterexample YiZ2i2i1 4 partition given Q x0 l 357 X1 0246 126 I Diintsch G Gediga Artcial Intelligence 106 1998 109137 Now Fe U Y Q 0 j 4 IF yil n 1 Furthermore satisfied We IXi n Yj 1 1 2 j 4 hypothesis claim Izi 1 2 rjijl 4 follows ijij log2 L_ lOgz4 17ij C jt4 lij log2 2 j Thus ZfOCd e Ci 2 2 ii2 contradicts claim We generalize assumptions Wong et al 30 value HOCd Q depend y number classes 6d Qdefinable example class 8 deterministic elements Proposition 32 Suppose Xi uniformly distributed classes Yj t j s IXi II Yj I di Then FPd I Q logzs 1 Proof By hypothesis t j G s Xi n Yj di follows Cj qij 1 qij r di 1 sl 1 Thus HOcd I Q C si C kj log2 ilOgzSl j c s2i lOgzS 1 lOgzS I proves claim q I Diintsch G GedigaArtcial Intelligence 106 1998 109137 127 32 Prediction II playing safe Whereas optimal guessing entropy measures previous strategies based measures distributions d x Q rough set approach based guessing knowing This means observations predicted perfectly assumed realization systematic process nature indeterministic section good candidates estimated parameters crossclassification researcher rules assumed unknown Based arguments given class Y 19d observation y region jJQ 1 Q result random process characteristics unknown uncertainty words given data partition obtained Q know world equivalence classes 6 Given assumption information y data set help classify element y E U V conclude requires rule class In case element U V viewed realization unknown probability distribution uncertainty log2n Note unlike previous approach assumes classes 6Q observed approach requires sample orin representative rroe estimates 20 classes 6Q Thus regard probability distribution relation 6Q given data accord Q associated equivalence principles RSDA know upper respectively class Y Of d lower Qapproximation terms parameterse ln It follows apply deterministic Q d ignore rules Thus use classes 8Q y E U V class In words maximum entropy principle worst case look equivalence gained indeterministic contained assume relation Odet defined V assume x edt y x y exists 6 c x y E Xi Its associated probability distribution given c 1 U VI We define entropy deterministic rough prediction respect Q d 38 ffde Q d dzf H Odet c log2 Hde Q d C jZi log2 ic c ic r2i log2 Knowledge Guessing 128 I Diintsch G Gediga Artcial Intelligence IO6 1998 109137 This gives d 1 Q gf HdeQ d HQ 1 ylogn cli log iC Since 0 _C 8Q fled note 8Q fled classes 0 fOc Q d c iti lo ic i2i log2 ic I Hdet Q d 1 v log implies HOCd 1 Q Hded 1 Q If compare Hde Q d Hloc Q f d terms necessary parameters assume computation indeterministic population deterministic indeterministic Indeed rules rules representative Hloc measures distinguishup sample underlying values quantitative HQ d deterministic rules In contrast Hde Q d requires representativeness indeterministic rule rules deterministic valid m objects consists m assumes unique individual rules gathered random world replicated The proof following straightforward left reader Proposition 33 Let d Q C f2 Then 1 HdetQ d 3 Hd 2 HdetQ f d HQ 3 Hdet Q f d log2n ifand if6Q C tf ifand V 0 V union singletons ofe identity relation explained Q 8Q The extremes Hdet Q d l y Q d 0 guessing In cases HdetQ d log2n The following gives bounds Hded 1 Q varies Proposition 34 1 y Hdetd 1 Q 1 v logzn VI Proof First observe 24 log2 n I VI logz2 1 The minimum value CiC Si log Eti obtained c t 1 case c i2i log2 Ezi ic nIV1 log2 n Therefore n ___ nIV1 1ylog2 I Diintsch G Gediga Arhcial Intelligence 106 1998 109137 129 Hded Q 1 y lon Isi izc lo 0 1 ni 1 y logn 1 Yb2 u Ymn log2 1 y lo41 v 1 v log2n IVI class X For direction note nondeterministic class exactly fii log21 elements anld Cic elements class elements class elements loss Since value Cilc generality greater case assume n 1 VI maximum f2i log2 1 Therefore Hdet I Q 3 1 y logn 1 v log 5 1 Y 12n log2 5 1 Y log2W 1y proves claim q We Hded I Q deterministic distributionof classes leading lower Hded I Q We use nondeterministic independent granularityie probability classes 8Q dependent granularity rules higher granularity classes Propositionr 35 If Q g R Hdetd I R 6 Hdetd I Q Proof By remark assume deterministic 6 This implies Be G e class 8Q class HdeR d f HdetQ d Since furthsrmore HQ 6 HR Corollary 22 conclusion follows q A similar result hold Hdet Q d example given Table 5 shows HdetIqlJ IPI 15 2 HdeIql q21 IPI Hdetq21 PI 130 I Diintsch G Gediga Artijcial Intelligence 106 1998 109137 Table 5 HdetQ d u 1 2 3 4 42 1 2 3 4 41 1 1 2 2 P 1 2 2 2 As 32 define normalized relative deterministic prediction success Sdet Q d normalized rough entropy NRE let m Hd log2 n Then Otherwise Hd logzn set SdetQ d itf 1 HdetQ d Hd lwn Hd 39 310 success RSDA In way obtain measure prediction coding complexity compare different prediction uncertainty Sde Q d 1 worst case Sdet Q d 0 Sdet unconditional measure complexity rules uncertainty measure terms combination rules sense perfect prediction predictions merged results The question arises approximation function y positioned model Proposition 34 shows fixed Q max Hdetd l R yR 4 YQ 4 I ylog IVI denote value H det d 1 Q The following vQ d Hd I Q strictly inversely monotone result tells fixed d oposition36 yQ d YR d w Hjid 1 R Hd 1 Q The hypothesis 7 Q d y R f d implies I VQdI 5 I VRdl Proof Thus Hd 1 R 1 YR dlqzfi IVRtd 1 yQ dlogn IvQdl Hzd 1 Q First note k 1 klog2kkllogkl 311 1 Diintsch G GedigaArtijicial Intelligence 106 1998 109137 131 We assume 0 Hzd 1 R U VRd 8 Now Hxti I W Hxd I Q 1 yR dlogn iVRdo 1 vQ db IvQedb n IVRdIlog2n IVRdl iVQdllon IvQdl n IVRdo n IVQdl 311 IvQdl IVRtdI YQ 4 YR 4 This completes proof q We observe l RSDA tries maximize y procedure minimize maximum conditional entropy deterministic terms conditional In rough prediction uncertainty view y v Q d crude approximation measure normalized prediction success SEX1 Q 1 Hd I Q minHxd I R R g f2 maxHLxd I R R g f2 minHixd R R g i2 _HJdIQbo lon 0 y _ 1 _ log21 v log2 n yo 1 log2h Proposition result similar equivalence 35 extend hypothesis 36 hold following relations d 8Q 6 following partitions v Q d y R d shows consider example od I 31Pk 56 OQ 11341 25 36 OR 111 23456 Then yQdOyRd On hand Hded Q log26 log23 1 log25 log26 log2 2 Hded 1 R Example 1 Continued Table 6 presents rough information example given Table 2 We skipped presentation Hdetd Qmeasures becauseas prediction success different attribute sets The results NRE Sdet Q d identical y purpose comparing analysis data shown abovethey 132 I Diintsch G GedigaAicial Intelligence 106 1998 109137 Table 6 Rough information measures predicting quality example information Attribute set HdetQ f d SdetQ d SQ d ISI WfIl IS BW 1880 3170 2197 0573 0000 0432 0808 0568 0432 Y 0556 0000 0778 Significance 0047 1000 0144 evaluate good candidate produces limitations significance defects SOc Q d repaired order goodness predictability test Inspecting rough prediction quality attribute sets test results BMZ Table 6 significance 33 Prediction III living In Section 32 prediction HdetQ d consists parts absolute correct deterministic union lower bound approximations random The prediction random elementtoclass mapping uncertain observation predicted given available source assumption data If willing rules offered RSDA restricted rules need entropy estimation use information provided indeterministic uncertainty recognizes distribution method interpreted sense indeterministic This approach handle uncertainty sequel spare reader somewhat prediction uncertainty Xi class 8Q lead deterministic 13d induces structure U V classes rule I Yik Od k 1 Xi Yij E Q d Xi intersects Yij V KO Uncertainty given Xi measured uncertainty Yic V Yik induced The V requires knowledge probability rule produces certain assumption degree imprecision based solely uncertainty d interact Q Even pure rough set theory certainly consistent procedure describes upper bounds sets defined terms suitable probability distribution As shall involved definitions resulting entropy measures HQ d Hd 1 Q We shall entropy Hd 1 Q mention anti monotone This result drawback relationship C measure approximation search process use Hd measures y Hdetd Hmeasure deterministic approach assumes representative Hloc assumes conditional probability distribution representative Hdet assumes ClaSS representative practical value assumption H rough entropy Hdet statistical entropy HI probability distributions upper bound class probability distribution lower bound pOpUhtiOn method subsequent quality natural As consequence like conditional unlike Hded 1 Q HOcd 1 Q conditional I R Hlocd 1 I Therefore takes representativeness limited I R stop criterion We shall investigate monotone research I Diintsch G Gediga Artificial Intelligence 106 1998 109137 133 4 Data analysis validation The approach closest noninvasive philosophy RSDA entropy rough prediction Hde Q d combines deterministic maximum entropy principle basic aim use assumptions outside data possible principle indifference RSDA context We advocate type entropy Although principle maximum entropy suggests largest entropy possibilities Using appropriate definitions shown sense L incorporates information measures Al consistent know adopt p additional 131 To obtain objective measurement use normalized rough entropy NRE 3 lo SdetQ _ d 1 HdetQ d Hd loglUI Hd 41 If NRE value near 1 entropy use favorable value near 0 indicates casualness The normalization moving standards long change decision attribute d Therefore comparison NRE values different predicting attribute sets makes sense given fixed decision attribute low chosen attribute combination The implemented procedure searches attribute sets high NRE finding algorithm expensive use geneticlike NRE feature set computationally determine sets high NRE We named method SORES acronym Searching timal Rough Entropy Sets SORES implemented rough set engine GROBIAN 9 3 4 I Validation test procedure 14 datasets available In order repository4 appropriate subset datasets Quinlan UC1 references origin obtained These 26 test Release 8 C45 The validation training equal sizes 100 times assuming balanced distribution set method performed splitting data set randomly training testing data IT2 method The mean error value measure prediction success settesting We choose half set training purposes order basis testing predictive power resulting attribute sets Because data sets contained continuous attributes missing values preprocessing apply SORES algorithm data sets Missing values replaced mean step necessary 3 All material relating SORES datasets description algorithm GROBIAN obtainedfromourwebsitehttpwwwpsychouniosnabrueckdesores 4httpMurwicsuciedumlearnMLRepositoryhtml 134 I Diintsch G Gediga Artificial Intelligence IO6 1998 IO9137 value case ordinal attributes frequent value mode The preprocessing continuous data different global discretization methods minimal granularity filtering method described Method 1 consists global Section 23 NRE affect y influence dependency influences decision structure This results values attribute attribute The discretization methods cluster number objects The respectively HOcbased methods local discretization method refined transforming discretization attributes given Catlett 5 Dougherty et al 7 proposed Hdet measure This task needs outside scope current introductory classes approximately attributes respect continuous article In Table 7 list basic parameters data sets compare C45 performance given Quinlan 26 uses IOfold cross validation CVlO data sets optimized SORES results 26 This taken care dividing data blocks cases similar size class distribution 26 p 8 1 footnote 31 Table 7 Datasets SORES validation Dataset SORES C458 Name Cases Classes Attributes Cont Discr Anneal Auto BreastW Colic CreditA CreditG Diabetes Glass HeartC HeartH Hepatitis IliS Sonar Vehicle Std deviation 798 205 683 368 690 1000 768 214 303 294 155 150 208 846 6 6 2 2 2 2 2 6 2 2 2 3 2 4 9 15 9 10 6 7 8 9 8 8 6 4 60 18 29 10 12 9 13 _ 15 15 13 _ No prcd attr Error Error 11 626 167 2 2 4 5 6 3 3 2 5 3 3 3 2 1128 1770 574 526 2155 1500 1810 1470 3292 2840 3186 2540 2179 3250 225 1 2300 1943 2150 1721 2040 433 480 2594 2560 3584 2710 1033 877 I Diintsch G Gediga Artificial Intelligence 106 1998 109137 135 Because TT2 tends result smaller prediction success rates CVlO comparison SORBS C45 based conservative estimate The SORES column No pred attr records number attributes feature RSDA cases actually prediction considerably prominent number attributes indicate The results learning procedure SORBS present version viewed effective compares machine established C45 produces better results However standard deviation error percentages SORES higher C45 conclude current SORES C45 slightly better performance odds 77 given 14 problems performance 45 method 5 Sunuuay outlook In paper proposed context RSDA entropy approaches estimate unconditional measures prediction success The statistical entropy measure suited assumption symmetric predicted given attributes discussed exchange predicting given deterministic information RSDA frame Two modifications information information The approach H additionally distributions indeterministic suitabb measure complexity expected prediction uncertainty rules assumes atomlike Hde relies structure uses knowledge rules drawback lacking monotony csonditional measure Hd 1 Q The measure Hde Q d compare attribute sets Ql Qk terms combined coding In second paper applied method searching optimal rough entropy sets SORBS real life data sets The method applicable C45 performs better SORES 7 14 problems C45 line tuned version Release 8 SORBS present raw following steps leastthe Fine tuning SORBS procedure consist ofat l Both types measuresHQ f d Hd 1 Q model M usedare weighted sum extent suitable measures finding optimal sets prediction HQ d w wHQ d 1 wHd 1 Q 0 w 1 suitable measure If u 1 weight effort searching rule high effort reducing uncertainty dependent attribute If w 0 chosen effort coding rules neglected Finally 0 w 1 estimates relative effort finding rule respect finding object uncertainty The methods Section 3 based w 1 procedure worthwhile results procedures w z 1 compare l The proposed methodas symbolic data analysis procedureis consuming In order enhance applicability procedure time real life data sets 136 I Diintsch G Gediga Arrcial Intelligence IO6 1998 109137 optimization optimization implemented The theory dynamic enhancement performed big samples kind subsample 12 step reducts l The discretization continuous attributes problem solved symbolic data analysis procedures described work presented numerical examples local discretization procedure optimizes directly expected produce better prediction quality global discretization chosen criteriong technique Although Hdet Q point like procedures developed Finally discretization methods external parameters representation approaches Thus model assumptions serve preprocessing mechanism harder computational methods applied global numerical Section 3 use stated procedures statistical kept minimum assumptions Acknowledgement We like thank anonymous referees constructive remarks helped improve clarity completeness paper References l JG Bazan A comparison dynamic nondynamic rough set methods extracting laws decision tables Institute Mathematics University Rzeszdw 1997 Preprint Z J Bazan A Skowron P Synak Dynamic reducts tool extracting laws decision Proceedings Symposium Methodologies Artificial Intelligence Springer Berlin 1994 pp 346355 Intelligent Systems Charlotte NC Lecture Notes tables 3 C Browne Enhanced rough set data analysis Pima Indian diabetes data Proceedings 8th Ireland Conference Artificial Intelligence Deny 1997 pp 3239 4 C Browne I Diintsch G Gediga IRIS revisited comparison discriminant enhanced rough set data Eds Rough Sets Knowledge Discovery Physica Heidelberg L Polkowski A Skowron analysis 1998 Vol 2 pp 345368 5 J Catlett On changing continuous Proceedings European Working Session LearningEWSL91 attributes ordered discrete attributes Y Kodratoff Ed Springer Berlin 1991 pp 164178 6 J Cohen Things I learned far American Psychologist 45 1990 13041312 7 J Dougherty R Kohavi M Sahami Supervised unsupervised discretization continuous 12th International Conference Machine Learning Morgan Kaufmann San Francisco CA features Proceedings 1995 pp 194202 8 I Dtlntsch A logic rough sets Theoret Comput Sci 179 1997 427436 9 I Diintsch G Gediga The rough set engine GROBIAN A Sydow Ed Proceedings 15th IMACS World Congress Vol 4 Wissenschaft und Tech Berlin 1997 pp 613618 lo I Dtintsch G Gediga ROUGHIANrough information analysis 15th IMACS World Congress Vol 4 Wissenschaft Proceedings 636Fullpaperavailablefromhttpwwwinfjulstacukcccz23papersroughian html I Diintsch G Gediga Statistical evaluation rough set dependency HumanComputer Studies 46 1997 589604 ll extended abstract A Sydow Ed und Technik Berlin 1997 pp 631 analysis International Journal 1 Diintsch G GedigaArtcial Intelligence 106 1998 109137 137 12 I Dtintsch G Gediga Simple data filtering rough set systems Intemat J Approx Reason 18 1998 93106 13 ET Jaynes Information 14 G Jumarie Relative Information Springer Berlin 1990 15 E Konrad E Grlowska Z Pawlak Knowledge theory statistical mechanics Physical Review 106 1957 62630 representation systemsdefinability informations ICS Research Report 433 Polish Academy Sciences 1981 16 E Konrad E Orlowska Z Pawlak On approximate concept learning Tech Report 817 Technische Universitat Berlin 1981 17 M Li P Vitrhryi An Introduction Kolmogorov Complexity Its Applications Texts Monographs Computer Science Springer Berlin 1993 18 HS Nguyen SH Nguyen Discretization methods data mining L Polkowski A Skowron Eds Rough Sets Knowledge Discovery Physica Heidelberg 1998 19 P Paglianl Rough InformationRough 20 Z Pawlak Mathematical Sciences 1973 theory sets Set Analysis Physica Heidelberg 1997 pp 109190 logicalgebraic structures E Odowska Ed Incomplete foundations information retrieval ICS Research Report 101 Polish Academy 21 Z Pawlak Rough sets Intemat J Comput Inform Sci 11 1982 341356 22 Z Pawlak Rough sets theoretical aspects reasoning data System Theory Knowledge Engineering Problem Solving Vol 9 Kluwer Dordrecht 1991 23 Z Pawlak JW GrzymdaBusse 24 Z Pawlak R Slowiriski Rough set approach Warsaw University Technology 1993 R Slowiiiski W Ziarko Rough sets Comm ACM 38 1995 8995 multiattribute decision analysis ICS Research Report 36 25 L Polkowski A Skowron Eds Rough Sets Knowledge Discovery Physica Heidelberg 1998 appear 26 R Quinlan Improved use continuous attributes C45 Journal Artificial Intelligence Research 4 1996 7790 27 I Rissanen Modeling shortest data description Automatica 14 1978 465471 28 J Rissanen Minimumdescriptionlength S Kotz NL Johnson principle Eds Encyclopedia Statistical Sciences Wiley New York 1985 pp 523527 29 J Teghem M Benjelloun Some experiments R Siowinski Theory System Theory Knowledge Engineering pp 267284 Ed Intelligent Decision Support Handbook Applications compare rough sets theory ordinal statistical methods Advances Rough Set Problem Solving Vol 11 Kluwer Dordrecht 1992 30 SKM Wong W Ziarko RL Ye Comparison roughset statistical methods inductive learning Intemat J MarMach Stud 24 1986 5372