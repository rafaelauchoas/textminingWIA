Artiﬁcial Intelligence 259 2018 110146 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint The complexity generality learning answer set programs Mark Law Alessandra Russo Krysia Broda Department Computing Imperial College London SW7 2AZ United Kingdom r t c l e n f o b s t r c t Article history Received 2 September 2016 Received revised form 20 February 2018 Accepted 15 March 2018 Available online 21 March 2018 Keywords Nonmonotonic logicbased learning Answer Set Programming Complexity nonmonotonic learning Traditionally work ﬁeld Inductive Logic Programming ILP ad dressed problem learning Prolog programs On hand Answer Set Program ming increasingly powerful language knowledge representation reasoning gaining increasing attention industry Consequently research activity ILP widened area Answer Set Programming witnessing pro posal new learning frameworks extended ILP learning answer set programs In paper investigate theoretical properties existing frame works learning programs answer set semantics Speciﬁcally present detailed analysis computational complexity frameworks spect decision problems deciding hypothesis solution learning task deciding learning task solutions We introduce new notion generality learning framework enables deﬁne framework general terms able distinguish ASP hypothesis solution set incorrect ASP programs Based notion formally prove generality relation set existing frameworks learning programs answer set se mantics In particular recently proposed framework Contextdependent Learning Ordered Answer Sets general brave induction induction stable models cautious induction maintains complexity cautious induction highest complexity frameworks 2018 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 1 Introduction Over decades growing Inductive Logic Programming ILP 1 goal learn logic program called hypothesis given background knowledge base explains set examples The main advantage ILP traditional statistical machine learning approaches learned hypotheses easily expressed plain English explained human user facilitating closer interaction tween humans machines Traditional ILP frameworks focused learning deﬁnite logic programs 16 normal logic programs 78 On hand Answer Set Programming 9 powerful language knowledge representation reasoning ASP closely related declarative paradigms SAT SMT Constraint Programming inductive reasoning 1012 Compared paradigms nonmonotonicity ASP particularly suited commonsense reasoning 1315 Because expressiveness eﬃcient solving ASP Corresponding author Department Computing Huxley Building 180 Queens Gate Imperial College London London SW7 2AZ United Kingdom Email address marklaw09 imperial ac uk M Law httpsdoiorg101016jartint201803005 00043702 2018 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 M Law et al Artiﬁcial Intelligence 259 2018 110146 111 increasingly gaining attention industry 16 example decision support systems 17 etourism 18 product conﬁguration 19 Consequently scope ILP recently extended learning answer set programs examples partial solutions given problem intention provide algorithms support automated learning complex declarative knowledge Learning ASP programs allows learn variety declarative nonmonotonic commonsense theories including instance Event Calculus 20 theories 21 theories scheduling problems agents preference models real user data 22 synthetic data 2324 Learning ASP programs advantages compared learning Prolog programs Firstly learning Prolog programs goal directed SLDNF procedure Prolog taken account Speciﬁcally learning programs negation ensured programs stratiﬁed learned program loop certain queries As ASP declarative consideration need taken account learning ASP programs A second fundamental advantage learning ASP programs theory learned expressed extra types rules available Prolog choice rules weak constraints Learning choice rules allows learn nondeterministic concepts instance learn coin nondeterministically land heads tails This achieved learning simple choice rule 1heads tails1 Learning choice rules different probabilistic ILP settings 2527 similar coins problems focus learning probabilities outcomes coin Learning weak constraints enables natural extension ILP preference learning 23 resulted effective problem domains learning preference models scheduling 23 urban mobility 24 Several algorithms aimed learning answer set semantics different frameworks learning ASP pro grams recently introduced literature 28 presented notions brave induction I L P b cautious induction I L P c based respectively established notions entailment answer set semantics 1329 brave entailment atom true answer set cautious entailment atom true answer sets In brave induction answer set cover examples cautious induction answer set cover examples Brave induction actually special case earlier learning framework called duction stable models I L P sm 30 examples partial interpretations A hypothesis solution induction stable models task example partial interpretations answer set hypothesis combined background knowledge covers partial interpretation Brave induction equivalent induction stable models exactly partial interpretation example Each frameworks learning ASP programs unable learn types ASP programs 31 exam ple brave induction learn programs containing hard constraints In 31 presented learning framework called Learning Answer Sets I L P L A S uniﬁes brave cautious induction able learn ASP programs containing normal rules choice rules hard constraints In spite increased expressivity ap proaches learn weak constraints able capture preference learning Informally learning weak constraints consists identifying conditions ordering answer sets The learning task case require examples derings partial interpretations To tackle aspect learning ASP programs extended Learning Answer Sets framework Learning Ordered Answer Sets I L P L O A S 23 demonstrated algorithm1 able learn preferences scheduling domain More recently extended I L P L O A S framework I L P context L O A S contextdependent examples come extra contextual information 24 In paper explore expressive power computational complexity framework The important allows identify class problems framework solve gives indication price paid framework We characterise expressive power framework terms new notions called onetoonedistinguishability onetomanydistinguishability manytomanydistinguishability The intuition onetoonedistinguishability given ﬁxed background knowledge B suﬃcient examples framework able distinguish target hypotheses H 1 unwanted hypotheses H 2 This means task T given framework background knowledge B H 1 solution 1F set T H2 We characterise onetoonedistinguishability class framework F written D1 tuples cid3B H1 H2cid4 Bs H1s H2s state framework F1 D1 1 general F2 F2s onetoonedistinguishability class strict subset F1s onetoonedistinguishability class Onetomanydistinguishability relates task ﬁnding single target hypothesis set possi ble hypotheses It upgrades notion onetoonedistinguishability classes onetomanydistinguishability classes These tuples form cid3B H Scid4 framework task includes H unwanted hypotheses S inductive solution Manytomanydistinguishability upgrades notion manyto manydistinguishability classes These contain tuples form cid3B S 1 S2cid4 S1 set target hypotheses framework task accepts hypothesis S 1 hypothesis S2 inductive solution We measures I L P context general I L P L O A S general I L P L A S We L O A S I L P L A S general I L P sm I L P c Although I L P sm equally D1 1 general I L P b I L P sm general I L P b onetomany manytomany generality measures 1 Our ILASP solving I L P L O A S tasks available download 32 112 M Law et al Artiﬁcial Intelligence 259 2018 110146 Despite different generalities I L P c I L P L A S I L P L O A S I L P context L O A S computational complexity frameworks decision problem verifying given hypothesis solution given learning task problem deciding given learning task solutions Similarly I L P sm I L P b computational complexities decision problems despite general generality measures We begin Section 2 reviewing background material necessary rest paper In Section 3 recall deﬁnitions learning frameworks Sections 4 5 prove complexities generalities respectively learning framework We conclude paper discussion related future work 2 Background 21 Answer Set Programming In section introduce concepts needed paper Given atoms h h1 h k b1 b n c1 c m h b1 b n c1 cm called normal rule h head b1 b n c1 c m col lectively body represents negation failure rule b1 b n c1 c m head hard constraint choice rule rule lh1 h ku b1 b n c1 c m l u integers head called aggregate A rule R safe variable R occurs positive literal body R In paper use ASP ch denote set choice programs P programs composed safe normal rules choice rules hard constraints Given rule R write headR denote head R bodyR denote R denote atoms occur positively resp negatively body R body R body Given program P write AtomsP denote atoms P We extend notation fragments program resp body The Herbrand Base program P ASP ch denoted H B P set variable free ground atoms formed predicates constants P The subsets H B P called Herbrand interpretations P A ground aggregate lh1 h ku satisﬁed interpretation I iff l I h1 h k u As restrict programs sets normal rules hard constraints choice rules use simpliﬁed deﬁnitions reduct choice rules presented 33 Given program P Herbrand interpretation I H B P reduct P I constructed groundP set ground instances rules P 4 steps ﬁrstly remove rules bodies contain negation atom I secondly remove negative literals remaining rules thirdly replace head hard constraint choice rule head satisﬁed I H B P ﬁnally replace remaining choice rule lh1 h mu b1 b n set rules hi b1 b n hi I h1 hm Any I H B P answer set P minimal model reduct P I Throughout paper denote set answer sets program P A SP We program P bravely entails atom written P b answer set A P A Similarly P cautiously entails written P c answer set A P A Unlike hard constraints ASP weak constraints affect answer set program P Hence deﬁnitions apply programs weak constraints Weak constraints create ordering A SP specifying answer sets preferred A weak constraint form b1 b n c1 c mwl t1 t k b1 b n c1 c m atoms w l terms specifying weight level t1 tk terms A weak constraint W safe variable W occurs positive literal body W At priority level l aim discard answer set min imise sum weights ground weak constraints level l bodies true The higher levels minimised ﬁrst The terms t1 t k specify ground weak constraints considered unique 34 For program P interpretation A weakP A set tuples w l t1 t k b1 b n c1 c mwl t1 t k groundP A satisﬁes b1 b n c1 c m For level l score interpretation A sum weights tuples level l formally P l cid2 A wlt1tkweakP A w For A1 A2 A SP A1 dominates A2 written A1 cid12P A2 iff l P l An answer set A A SP optimal dominated A2 A SP P l A2 A1 m l P m A1 P m A2 Example 1 Let P program 0p1 p2 p31 P 8 answer sets combinations making p atoms true false Consider weak constraints pX11 pX11 X The ﬁrst weak constraint states p atoms true penalty paid This penalty paid regardless 1 2 3 p atoms true Conversely second weak constraint says penalty 1 paid p atoms true In cases optimal answer set ﬁrst case remaining answer sets dominate second case answer sets p atom dominate 2 p atoms turn dominate single answer set 3 p atoms Note deﬁnition weak constraints paper line recent ASP standard established 34 The syntax previous deﬁnitions weak constraints 13 include terms t1 t k considered M Law et al Artiﬁcial Intelligence 259 2018 110146 113 ground instance weak constraint individually This semantics achieved notion weak straints 34 Any weak constraint bodyw l2 mapped weak constraint bodywl V1 V n V1 Vn set variables occur body If multiple weak constraints exactly preserve semantics 13 unique term added weak constraint For example pX1 1 qX1 1 pX11 X 1 qX11 X 2 With additional term W eakP pa qa P program containing weak constraints equal 1 1 1 1 1 2 leading score 2 level 1 additional term W eakP pa qa equal 1 1 leading score 1 level 1 Unless stated refer ASP program paper mean program consisting ﬁnite set normal rules choice rules hard weak constraints We introduce extra notation useful later sections Given set interpretations S set ordP S captures ordering interpretations given weak constraints P It generalises dom inates relation includes cid3 A1 A2 cid4 A1 cid12P A2 includes tuples binary compari son operators Formally cid3 A1 A2 cid4 ordP S A1 A2 S A1 cid12P A2 cid3 A1 A2 cid4 ordP S A1 A2 S A2 cid12P A1 cid3 A1 A2 cid4 ordP S A1 A2 S A2 cid2P A1 cid3 A1 A2 cid4 ordP S A1 A2 S A1 cid2P A2 cid3 A1 A2 cid4 ordP S A1 A2 S A1 cid2P A2 A2 cid2P A1 cid3 A1 A2 cid17cid4 ordP S A1 A2 S A1 cid12P A2 A2 cid12P A1 Given ASP program write ordP shorthand ordP A SP Two ASP programs P Q strongly equivalent written P s Q ASP program R A SP R A SQ R We recall splitting set theorem 35 use proofs paper This theorem relies notions splitting set partial evaluation logic program Given program P set U H B P splitting set P rule R groundP AtomsheadR U cid17 AtomsR U Given ground rule R set atoms U write RU denote rule R positive negative occurrences atoms U removed body R Given program P splitting set U P set X U partial evaluation P R U respect U X written eU P X program RU R groundP AtomsheadR U body X body R X Theorem 1 Given ground ASP program P splitting set U P A SP X Y X A SR P AtomsheadR U cid17 Y A SeU P X The intuition splitting set theorem set atoms U known split program P ﬁnd answer sets subprogram deﬁnes atoms U ﬁrst For answer sets X partially evaluate P X solve partially evaluated program answer sets The splitting set theorem guarantees answer set Y partially evaluated program X Y answer set P Furthermore answer set P constructed way 22 Complexity theory We assume reader familiar fundamental concepts complexity Turing machines reductions detailed explanation 36 Many decision problems ASP known complete classes polynomial hierarchy 37 The classes polynomial hierarchy deﬁned follows P class problems solved polynomial cid4P time Deterministic Turing Machine DTM cid2P class problems k1 0 0 N P cid2P solved DTM polynomial time cid2P class problems solved nondeterministic Turing Machine polynomial time cid2P k oracle ﬁnally cid3P class problems complement solved nondeterministic Turing Machine polynomial time cid2P 1 cid3P 1 N P coN P respectively N P class problems solved nondeterministic Turing machine polynomial time coN P class problems complement N P problem cid3P 0 k oracle cid2P k oracle cid2P coN P cid2P P cid4P P cid2P D P class problems D mapped pair problems D1 D2 D1 N P D2 coN P instance I D I answers yes mapped instances I1 I2 D1 D2 respectively answer yes It known 36 following inclusions hold P N P D P cid4P 2 P 2 coN P D P cid4P 2 cid3P 2 cid2P k1 k1 k k k 3 Learning frameworks In section deﬁnitions learning frameworks analyse paper The ﬁrst brave induction cautious induction induction stable models We reformulate preserve meaning original deﬁnitions easier comparison 2 In 13 purpose 114 M Law et al Artiﬁcial Intelligence 259 2018 110146 It common ILP task hypothesis space set rules appear hypotheses The purpose hypothesis space twofold ﬁrstly allows task restricted solutions way interesting secondly aids computational search inductive solutions Tasks brave cautious induction induction stable models originally presented hypothesis space 2830 mainly considered theoretically speciﬁcations eﬃcient algorithmic computations The publicly available algorithms brave induction 3839 use hypothesis space deﬁned mode declarations 40 In paper upgrade brave induction cautious induction induction stable models hypothesis space S M 31 Notation terminology An ILP learning framework F deﬁnes learning task F inductive solution given learning task F For framework task tuple cid3B S M Ecid4 B ASP program called background knowledge S M set ASP rules called hypothesis space E tuple called examples The structure E depends type ILP framework Each papers 28 30 31 23 presented learning frameworks different languages B S M example induction stable models presented normal logic programs It unfair induction stable models general learn programs choice rules simply considered original paper fact induction stable models general learn programs choice rules For fair comparison assume paper learning framework background knowledge B hypothesis space S M consist normal rules choice rules hard constraints weak constraints Given framework F learning task T F cid3B S M Ecid4 F hypothesis subset hypothesis space S M In Section 5 consider tasks unrestricted hypothesis spaces written cid3B Ecid4 case ASP program called hypothesis An inductive solution hypothesis background knowledge B satisﬁes conditions E given particular learning framework F We write I L P F T F denote set inductive solutions T F Throughout paper use term covers apply kind example given F task cid3B S M Ecid4 hypothesis H covers example e element component E meets particular conditions framework F puts H e 32 Framework deﬁnitions Brave induction I L P b ﬁrst presented 28 deﬁnes inductive task examples ground atoms covered answer set entailed brave entailment ASP The original deﬁnition consider atoms present answer set negative examples The publicly available algo rithms realise brave induction hand allow negative examples We upgrade deﬁnition paper allow negative examples3 follows cid4cid4 B ASP program called Deﬁnition 1 A brave induction I L P b task T b tuple cid3B S M cid3E sets ground atoms called positive negative ground knowledge S M hypothesis space E examples respectively A hypothesis H S M said inductive solution T b written H I L P bT b A A SB H E A E A E E Cautious induction I L P c ﬁrst presented 28 It deﬁnes inductive task examples covered answer set entailed cautious entailment ASP B H satisﬁable answer set Similarly brave induction original deﬁnition consider negative examples Deﬁnition 2 upgrade framework include negative examples cid4cid4 B ASP program called Deﬁnition 2 A cautious induction I L P c task T c tuple cid3B S M cid3E sets ground atoms called positive negative ground knowledge S M hypothesis space E examples respectively A hypothesis H S M said inductive solution T c written H I L P cT c A SB H cid17 A A SB H E A E A E E Brave induction reason true false single answer set B H It specify brave tasks enforcing atoms bravely entailed necessarily answer set Induction stable models 30 I L P sm hand generalises notion brave induction shown Deﬁnition 4 The following terminology ﬁrst introduced Deﬁnition 3 A partial interpretation e pair sets ground atoms cid3einc eexccid4 An interpretation I said extend e iff einc I eexc I 3 Note I L P b negative example ei easily simulated adding rule ai ei background knowledge giving ai positive example ai new atom unique ei appear original task M Law et al Artiﬁcial Intelligence 259 2018 110146 115 Deﬁnition 4 An induction stable models I L P sm task T sm tuple cid3B S M cid3Ecid4cid4 B ASP program called background knowledge S M hypothesis space E set partial interpretations called examples A hypothesis H said inductive solution T sm written H I L P smT sm H S M e E A A SB H A extends e Note brave induction task thought special case induction stable models exactly partial interpretation example We consider Learning Answer Sets framework introduced 31 This ﬁrst framework capable unifying concepts brave cautious induction The idea use examples partial interpretations extended answer sets B H Deﬁnition 5 A Learning Answer Sets task tuple T cid3B S M cid3E E ground knowledge S M hypothesis space E positive negative examples A hypothesis H S M inductive solution T written H I L P L A S T cid4cid4 B ASP program called sets partial interpretations called respectively E 1 e 2 e E E A A SB H A extends e cid3 A A SB H A extends e Note deﬁnition combines properties brave cautious semantics positive examples bravely entailed negation negative example cautiously entailed Example 2 Consider I L P L A S learning task background knowledge B contains deﬁnitions structure 4x4 Sudoku board deﬁnitions cell same_row same_col same_block same_row same_col same_block true different cells row column block B cell1 1 cell1 2 cell4 4 same_rowX1 Y X2 Y cellX1 Y cellX2 Y X1 cid17 X2 same_colX Y1 X Y2 cellX Y1 cellX Y2 Y1 cid17 Y2 block1 1 1 block1 2 1 block2 1 1 block2 2 1 block3 1 2 block3 2 2 block4 1 2 block4 2 2 block1 3 3 block1 4 3 block2 3 3 block2 4 3 block3 3 4 block3 4 4 block4 3 4 block4 4 4 same_blockC1 C2 blockC1 B blockC2 B C1 cid17 C2 For purposes example consider small hypothesis space S M practice larger4 S M E E cid10 0valueC 1 valueC 2 valueC 3 valueC 41 cellC 1valueC 1 valueC 2 valueC 3 valueC 41 cellC 1valueC 1 valueC 2 valueC 3 valueC 42 cellC same_rowC1 C2 valueC1 V valueC2 V same_colC1 C2 valueC1 V valueC2 V same_blockC1 C2 valueC1 V valueC2 V cid3value1 1 1 cid4 cid3value1 1 1 value1 3 1 cid4 cid3value1 1 1 value3 1 1 cid4 cid3value1 1 1 value2 2 1 cid4 cid3value1 1 1 value1 1 2 cid4 cid3 value1 1 1 value1 1 2 value1 1 3 value1 1 4cid4 cid11 We need able answer set assigns value cell hypothesis suﬃcient This captured positive example causes choice rules solution order covered Our ﬁrst negative examples require constraints included solution Without negative examples constraint left solution The fourth negative example means upper bound counting aggregate choice rule 1 answer sets cell 1 1 assigned 1 2 Finally ﬁfth negative 4 A larger version learning task hypothesis space 542 rules manual learning algorithm ILASP 41 116 M Law et al Artiﬁcial Intelligence 259 2018 110146 example forces lower bound choice rule 1 answer sets 1 1 assigned values 1 4 Hence possible inductive solution H 1valueC 1 valueC 2 valueC 3 valueC 41 cellC same_rowC1 C2 valueC1 V valueC2 V same_colC1 C2 valueC1 V valueC2 V same_blockC1 C2 valueC1 V valueC2 V The solutions hypothesis space S M contain H extra redundant choice rules 0valueC 1 valueC 2 valueC 3 valueC 41 cellC Note need I L P L A S s combination brave cautious induction separate correct hypothesis incorrect hypotheses E If instead use brave induction whichever examples use H solution choice containing choice rule cid4 H A As A SB H rules solution For instance consider hypothesis H 0valueC 1 valueC 2 valueC 3 valueC 41 cellC For examples cid3E I L P bcid3B cid3E A SB H cid4cid4 answer set A B H E cid20 answer set answer set B H solution task A E H If use cautious induction examples true answer set false answer set Therefore examples value predicate atom valuex y x y range 1 4 answer set B H contains valuex y means valuex y given positive negative example H solution task This means I L P c task T c H solution subset hypothesis space S M solution T c E cid20 cid20 cid20 Note learning frameworks considered far I L P L A S included incentivise learning weak constraint This frameworks examples answer sets B H Any solution H containing weak constraint W answer sets W removed HW shorter optimal5 solution The notion ordering examples needed incentivise learning weak constraints order enforce answer sets B H dominate answer sets Deﬁnition 6 An ordering example tuple o cid3e1 e2 opcid4 e1 e2 partial interpretations op binary comparison operator cid17 An ASP program P bravely respects o iff A1 A2 A SP following conditions hold A1 extends e1 ii A2 extends e2 iii cid3 A1 A2 opcid4 ordP P cautiously respects o iff cid3 A1 A2 A SP following conditions hold A1 extends e1 ii A2 extends e2 iii cid3 A1 A2 opcid4 ordP Note Deﬁnition 6 generalises initial deﬁnition ordering examples given 23 ordering examples operator express examples pairs answer sets equally preferred In Section 5 extension allows learn wider class programs We deﬁne notion Learning Ordered Answer Sets I L P L O A S Deﬁnition 7 A Learning Ordered Answer Sets task tuple T cid3B S M cid3E called background knowledge S M hypothesis space E tively positive negative examples O b O c sets ordering examples E orderings A hypothesis H S M inductive solution T written H I L P L O A S T O b O ccid4cid4 B ASP program sets partial interpretations called respec called brave cautious E E 1 H I L P L A S cid3B S M cid3E 2 o O b B H bravely respects o 3 o O c B H cautiously respects o cid4cid4 E Note orderings positive examples We chose restriction appear scenario hypothesis need respect orderings extended pair answer sets B H Example 3 Consider I L P L O A S task T cid3B S M cid3E follows E O b O ccid4cid4 individual components task 5 It common practice ILP search optimal shortest solutions M Law et al Artiﬁcial Intelligence 259 2018 110146 117 cid11 1 e B 0p q2 S M unrestricted S M set normal rules choice rules hard weak constraints cid10 E e E cid10 O b cid10 O c e cid11 2 cid4 cid3e 1 e cid11 1 cid4 cid3e 1 e cid3p cid4 e cid3 pcid4 2 2 1 The positive examples task satisﬁed background knowledge answer sets p q p q As negative examples remains ﬁnd set weak constraints answer set contains p preferred answer set contain p answer sets contain p equally optimal One hypothesis single weak constraint p11 The frameworks discussed far examples express properties learned hypothesis H ﬁxed background knowledge B These properties answer sets B H ordering answer sets In 24 presented new learning framework uses contextdependent examples Each example comes context ASP ch program C Examples express properties B H C meaning multiple examples different contexts express B H C1 properties B H C2 different properties Deﬁnition 8 A contextdependent partial interpretation CDPI pair cid3e Ccid4 e partial interpretation C ASP ch program called context A contextdependent ordering example CDOE o tuple cid3cid3e1 C1cid4 cid3e2 C2cid4 opcid4 ﬁrst elements CDPIs op binary comparison operator cid17 P said bravely respect o A1 A SP C1 A2 A SP C2 A1 extends e1 A2 extends e2 cid3 A1 A2 opcid4 ordP A SP C1 A SP C2 A program P said cautiously respect o A1 A SP C1 A2 A SP C2 A1 extends e1 A2 extends e2 cid3 A1 A2 opcid4 ordP A SP C1 A SP C2 When examples given contexts equivalent examples I L P L O A S Note contexts contain weak constraints In fact operator cid12P deﬁnes ordering answer sets based weak constraints program P So given CDOE cid3cid3e1 C1cid4 cid3e2 C2cid4cid4 C1 C2 contain different weak constraints clear program consider computing ordering answer sets checked weak constraints P P C1 P C2 P C1 C2 We present formal deﬁnition I L P context L O A S framework O b O ccid4cid4 Deﬁnition 9 A Contextdependent Learning Ordered Answer Sets I L P context B ASP program called background knowledge S M set rules allowed hypotheses ﬁnite sets CDPIs called respectively positive negative examples O b O c hypothesis space E called respectively brave cautious contextdependent orderings A hypothesis H S M ﬁnite sets CDOEs E inductive solution T written H I L P context L O A S task tuple T cid3B S M cid3E E E L O A S T A A SB C H st A extends e cid3 A A SB C H st A extends e Ccid4 E Ccid4 E 1 cid3e 2 cid3e 3 o O b B H bravely respects o 4 o O c B H cautiously respects o In 24 showed contextdependent examples simplify encoding certain tasks splitting background knowledge contexts relevant particular examples Although I L P context task L O A S transformed I L P L O A S task general requires parts examples encoded background knowledge Example 4 shows transformation Example 4 Consider simple scenario machine single conﬁguration parameter allowed natural number value A user allowed input natural number b b machine beep Two example scenarios encoded contextdependent positive examples cid3cid3beep cid4 valuea 3 valueb 2cid4 cid3cid3 beepcid4 valuea 4 valueb 20cid4 A task containing examples ground knowledge requires inductive solution combined context ﬁrst example answer set containing beep combined second example answer set containing beep If expressing task I L P L O A S scenarios represented considering background knowledge 118 M Law et al Artiﬁcial Intelligence 259 2018 110146 Table 1 A summary available systems learning answer set se mantics Framework Systems I L P b I L P sm I L P c I L P L A S I L P L O A S I L P context L O A S XHAIL 42 ASPAL 38 RASPAL 43 ILASP 32 ILASP 32 ILASP 32 Table 2 A summary complexity learning frameworks Framework Complexity veriﬁcation Complexity deciding satisﬁability I L P b I L P sm I L P c I L P L A S I L P L O A S I L P context L O A S N P complete N P complete D P complete D P complete D P complete D P complete N P complete N P complete cid2P 2 complete cid2P 2 complete cid2P 2 complete cid2P 2 complete cid10 B 1valuea 3 valuea 411valueb 2 valueb 201 cid11 The contextdependent examples mapped non contextdependent examples cid3valuea 3 valueb 2 beep cid4 cid3valuea 4 valueb 20 beepcid4 In fact 24 general map ping I L P context I L P L O A S This mapping simpliﬁed mapping depends encoding examples L O A S background knowledge abuses purpose background knowledge The contexts contextdependent examples allow instead separate information truly background knowledge applies scenarios information particular example 33 Systems learning answer set semantics The current publicly available systems ILP categorised according 6 frameworks presented section Table 1 It noted systems directly solve I L P c I L P sm tasks simply translated I L P L A S tasks solved ILASP The ILED 21 incremental extension XHAIL speciﬁcally targeted learning Event Calculus 20 theories The underlying mechanism based brave induction examples terms sequential time points 4 Complexity In section discuss complexity learning frameworks presented Section 3 respect decision problems veriﬁcation deciding given hypothesis H inductive solution task T satisﬁability deciding learning task T inductive solutions A summary results shown Table 2 To aid readability proofs propositions stated section given appendix All complexities discussed section propositional versions frameworks background knowledge hypothesis space learning task ground 41 Learning answer sets stratiﬁed summing aggregates As existing results complexity solving aggregate stratiﬁed programs useful introduce new learning framework I L P s L A S generalization I L P L A S allows summing aggregates bodies rules long stratiﬁed The existing results complexity programs allow prove complexity I L P s L A S Hence I L P L O A S reduces I L P s L A S helpful proving complexity I L P L O A S A summing aggregate s form lsuma1 w1 n wnu l u w1 wn integers u W S set wi a1 atoms s satisﬁed interpretation I l 0n ai I We recall deﬁnition aggregate stratiﬁcation 44 We slightly simplify deﬁnition considering propositional programs disjunction wiW S w cid12cid2 cid13 Deﬁnition 10 A propositional logic program P aggregates occur bodies rules stratiﬁed aggregate agg level mapping cid22 cid22 AtomsP ordinals rule R P following holds M Law et al Artiﬁcial Intelligence 259 2018 110146 119 1 b AtomsbodyR b headR 2 If agg bodyR b Atomsagg b headR P said aggregate stratiﬁed stratiﬁed aggregate P The intuition aggregate stratiﬁcation forbids recursion aggregates In general aggregate stratiﬁed programs lower complexity nonaggregate stratiﬁed programs Aggregate stratiﬁcation negation failure program aggregate stratiﬁed unrelated stratiﬁed usual sense Note constraints choice rules added aggregate stratiﬁed program breaking stratiﬁcation long atoms head choice rule lower level atom body This illustrated following example Example 5 Any constraint b1 b n c1 c m rewritten s b1 b n c1 c m s s new atom s mapped higher level atom A choice rule lh1 h ou b1 b n c1 cm rewritten h1 b1 b n c1 cm hcid20 1 hcid20 1 b1 b n c1 cm h1 ho b1 b n c1 cm hcid20 o hcid20 o b1 b n c1 cm ho s b1 bn c1 c m h1 hnl 1 s scid20 b1 b n c1 c m u 1h1 h n scid20 o s scid20 1 h cid20 new atoms s scid20 hcid20 given level hi occur previous program given new level s scid20 Provided previous program aggregate stratiﬁed new To avoid constantly mapping refer programs choice rules constraints aggregate stratiﬁed given new highest level hcid20 Lemma 1 44 Deciding aggregate stratiﬁed propositional program disjunction cautiously entails atom coN P complete Corollary 1 Deciding aggregate stratiﬁed propositional program disjunction bravely entails atom N P complete Proof We ﬁrst deciding aggregate stratiﬁed propositional program disjunction bravely en tails atom N P We showing polynomial reduction problem complement problem Lemma 1 deﬁnition coN P N P The complement problem Lemma 1 ciding non disjunctive aggregate stratiﬁed program cautiously entail atom Take nondisjunctive aggregate stratiﬁed program P atom let neg_a atom occur P P b P neg_a cid17c neg_a So decision problem N P It remains deciding aggregate stratiﬁed propositional program disjunction bravely entails atom N P hard We showing problem N P reduced polynomial time deciding satisﬁability aggregate stratiﬁed propositional program disjunction Consider arbitrary N P problem D The complement D D coN P deﬁnition coN P Hence Lemma 1 polynomial reduction D deciding aggregate stratiﬁed propositional program disjunction cautiously entails atom We deﬁne polynomial reduction D deciding aggregate stratiﬁed propositional program disjunction bravely entails atom follows instance I D let P program atom given polynomial reduction complement I deciding cautious entailment cid20 program P neg_a neg_a new atom I returns true P cid17c deﬁne P cid20 b neg_a Hence P aggregate stratiﬁed new atom neg_a strata P polynomial reduction D deciding aggregate stratiﬁed propositional program disjunction bravely entails atom Hence decision problem N P hard cid2 cid20 We introduce extra learning task Learning Answer Sets Stratiﬁed Aggregates I L P s L A S It Learning Answer Sets allowing summing aggregates bodies rules B S M long 120 M Law et al Artiﬁcial Intelligence 259 2018 110146 Fig 1 Chains polynomial reductions arrow denotes polynomial reduction framework B S M aggregate stratiﬁed Note condition B S M aggregate stratiﬁed implies hypothesis H S M B H aggregate stratiﬁed 42 Relationships learning tasks In section prove decision problems I L P b I L P sm reduce polynomially We decision problems chain polynomial reductions I L P c I L P L A S I L P context L O A S I L P L O A S I L P s L A S This chain reductions proving tasks share complexity decision problems By proving I L P c Ohard I L P s L A S O complexity class O prove tasks Ocomplete Similarly I L P b I L P sm reduce polynomially decision problems problems I L P b Ocomplete class I L P sm The chains reductions shown Fig 1 Proposition 1 shows complexity I L P b I L P sm coincide decision problems Proposition 1 1 Deciding veriﬁcation satisﬁability I L P b reduces polynomially corresponding I L P sm decision problem 2 Deciding veriﬁcation satisﬁability I L P sm reduces polynomially corresponding I L P b decision problem Proposition 2 shows chain polynomial reductions I L P c I L P L A S I L P L O A S I L P context L O A S L A S decision problems I L P s Proposition 2 1 Deciding veriﬁcation satisﬁability I L P c reduces polynomially corresponding I L P L A S decision problem 2 Deciding veriﬁcation satisﬁability I L P L A S reduces polynomially corresponding I L P context 3 Deciding veriﬁcation satisﬁability I L P context L O A S 4 Deciding veriﬁcation satisﬁability I L P L O A S reduces polynomially corresponding I L P s L O A S decision problem reduces polynomially corresponding I L P L O A S decision problem L A S decision problem 43 Complexity deciding veriﬁcation satisﬁability framework For learning frameworks prove complexity deciding veriﬁcation satisﬁability We start I L P b I L P sm frameworks decision problems N P complete Proposition 3 Verifying given H inductive solution general I L P b task N P complete Corollary 2 Verifying given H inductive solution general I L P sm task N P complete Proposition 4 Deciding satisﬁability general I L P b task N P complete Corollary 3 Deciding satisﬁability general I L P sm task N P complete We proven complexity deciding veriﬁcation satisﬁability I L P b I L P sm proving corre sponding entries Table 2 It remains complexities I L P c I L P L A S I L P L O A S I L P context L O A S reduces I L P s As shown I L P c reduces I L P L A S turn reduces I L P L O A S reduces I L P context L O A S I L P context L A S polynomial time prove complexity verifying hypothesis framework L O A S suﬃces I L P c D P hard proving hardness frameworks I L P s L A S member D P proving membership frameworks This shows framework member D P D P hard D P complete Proposition 5 Deciding veriﬁcation I L P s L A S member D P M Law et al Artiﬁcial Intelligence 259 2018 110146 121 Proposition 6 Deciding veriﬁcation I L P c D P hard We prove complexity deciding veriﬁcation I L P c I L P L A S I L P L O A S This proves corresponding entries Table 2 Theorem 2 Deciding given H solution I L P c I L P L A S I L P L O A S I L P context L O A S task D P complete case Proof By Proposition 6 deciding veriﬁcation I L P c D P hard By Proposition 2 deciding veriﬁcation I L P c reduces deciding veriﬁcation I L P L A S turn reduces deciding veriﬁcation I L P context L O A S reduces deciding satisﬁability I L P L O A S reduces deciding veriﬁcation I L P s L A S Proposition 5 deciding veriﬁcation I L P s L A S member D P Deciding veriﬁcation learning frameworks member D P D P hard Hence deciding veriﬁcation framework D P complete cid2 Similarly deciding satisﬁability cid2P 2 complete framework need I L P s L A S member cid2P 2 I L P c cid2P 2 hard Proposition 7 Deciding satisﬁability I L P s L A S cid2P 2 Proposition 8 Deciding satisﬁability I L P c cid2P 2 hard We prove complexity deciding satisﬁability I L P c I L P L A S I L P L O A S This proves remaining entries Table 2 Theorem 3 Deciding satisﬁability I L P c I L P L A S I L P L O A S I L P context L O A S task cid2P 2 complete case Proof similar proof Theorem 2 By Proposition 8 deciding satisﬁability I L P c cid2P 2 hard By Proposition 2 deciding satisﬁability I L P c reduces deciding satisﬁability I L P L A S turn reduces deciding satisﬁability I L P context L A S By Proposition 7 deciding satisﬁability I L P s 2 Deciding satisﬁability learning frameworks member cid2P L O A S reduces deciding satisﬁability I L P L O A S reduces deciding satisﬁability I L P s 2 hard Hence deciding satisﬁability framework cid2P 2 complete cid2 L A S cid2P 2 cid2P 44 Considering noisy examples Although frameworks considered paper originally presented assumption examples perfectly labeled noise examples systems solving tasks consider noise searching optimal solution A common approach XHAIL 42 ILASP 32 penalise hypotheses examples cover In ILASP examples labeled penalty paid hypothesis cover example Any example labeled penalty covered inductive solution Given set examples E score hypothesis H said H pH E H length hypothesis pH E sum weights examples covered H As hypothesis inductive solution covers examples labeled penalty labeled explicit penalty decision problems veriﬁcation satisﬁability reduced corresponding decisions nonnoisy tasks simply removing example penalty 5 Generality In section present new notion generality learning framework The aim sense class ASP programs framework capable learning given suﬃcient examples Language biases tend general impose restrictions classes program learned They primarily aid performance computation capture intrinsic properties learning framework In section consider learning tasks unrestricted hypothesis spaces hypotheses constructed set ﬁrst order normal rules choice rules hard weak constraints We assume learning framework F task consisting pair cid3B EF cid4 B ﬁrst order ASP background knowledge EF tuple consisting examples framework example E L A S cid3E sets partial interpretations cid4 E E E Allowing unrestricted hypothesis space raises question learning framework general deﬁne tasks lead particular set hypotheses inductive solutions On ﬁrst instance framework F general learn hypothesis H task T F framework H inductive solution T F However shown Example 6 loose notion generality lead trivial learning framework learning tasks examples general framework possible 122 M Law et al Artiﬁcial Intelligence 259 2018 110146 Example 6 Consider trivial learning framework I L P cid23 learning tasks pairs cid3B Ecid23cid4 Ecid23 tuple B ASP program I L P cid23cid3B Ecid23cid4 set ASP programs ASP program solution I L P cid23 task Although hypothesis H given background knowledge B clearly set examples Ecid23 H I L P cid23cid3B Ecid23cid4 possible hypothesis solution task making impossible distinguish hypothesis It clearly suﬃcient framework general learn target hypothesis denoted H T ﬁnd learning task H T solution What deﬁnition lacks way express H T solution task T unwanted hypothesis solution T To capture property learning framework able task T distinguish hypothesis H T unwanted hypothesis Pairs target unwanted hypotheses distinguished interesting starting point considering generality learning framework But property generality Frameworks cid20 2 brave induction distinguish target hypothesis H T unwanted hypotheses H separate learning tasks single learning task capable accepting H T inductive solution H cid20 2 Consider instance following example cid20 1 H cid20 1 H Example 7 Imagine scenario observing coin tossed times Obviously comes like learn ASP program answer sets correspond different outcomes Consider background knowledge B atoms heads tails true coin lands heads tails respectively Our target hypothesis H T ASP program A SB H heads tails One hypothesis program H T 1heads tails1 Consider hypotheses H heads tails correspond coin landing heads tails respectively Neither hypothe H sis correctly represent behaviour coin unwanted hypotheses There answer set heads cid20 B H 2 separately tasks cid3B cid3tails cid4cid4 cid3B cid3heads cid4cid4 respectively But learning task I L P b H T inductive solution H cid20 1 answer set tails B H cid20 2 I L P b distinguish H T H cid20 1 H cid20 2 cid20 1 cid20 1 H cid20 2 A general notion generality learning framework considered looks distinguishing target hy pothesis H T set unwanted hypotheses S In Section 52 introduce notion onetomanydistinguishability class learning framework This corresponds class pairs single hypothesis H T s set Ss hypotheses learning framework task distinguishes H T hypothesis S Informally notion expresses generality framework ﬁnding single target hypothesis presence unwanted hypotheses In Section 53 extend onetomanydistinguishability class learning framework manytomanydistinguishability turns captures notion distinguishing set target hypotheses S 1 set unwanted hypothe ses S2 single task In remainder section explore new measures generality expressed different learning problems Onetoonedistinguishability determines hypotheses framework general learn ruling unwanted hypothesis onetomanydistinguishability determines hypotheses learned space unwanted hypotheses ﬁnally manytomanydistinguishability determines exactly sets hypotheses learned We prove properties classes generalities making use deﬁnition strong reduction framework Strong reduction different concept reduction presented 45 Deﬁnitions 11 12 present respectively reformulation notion reduction introduced 45 new concept strong reduction Deﬁnition 11 A framework F1 reduces F2 written F1 r F2 F1 task T F1 F2 task T F2 I L P F1 T F1 I L P F2 T F2 A framework F1 rgeneral F2 F2 r F1 F1 rgeneral F2 F2 r F1 F1 cid4r F2 cid4cid4 maps Example 8 Consider I L P b I L P c learning frameworks I L P b r I L P c I L P b task cid3B cid3E cid3 cid4cid4 I L P c reduce I L P b Consider I L P c task cid3B e e E cid4cid4 I L P bT b I L P cT c instance I L P c task T c cid3 cid3p cid4cid4 assume task T b cid3B cid3E The hypothesis H1 p I L P cT c given assumption H 1 I L P bT b But consider hypothesis cid4 B H2 Thus H2 0p1 Since A SB H1 A SB H2 B H1 answer set extending cid3E H1 I L P bT b H2 I L P bT b But H2 I L P bT b easy H2 I L P cT c making I L P bT b equal I L P cT c Hence I L P c reduce I L P b I L P c rgeneral I L P b e e E E E E We discuss relationship reductions measures generality Section 6 Our notion strong reduction differs notion reduction fact reduced task background knowledge original task M Law et al Artiﬁcial Intelligence 259 2018 110146 123 Deﬁnition 12 A framework F1 strongly reduces F2 written F1 sr F2 F1 task T F1 F2 cid3B EF2 F1 srgeneral F2 F2 sr F1 F1 cid4sr F2 cid4 cid4 task T F2 I L P F1 T F1 I L P F2 T F2 A framework F1 srgeneral F2 F2 sr F1 cid3B EF1 Proposition 9 shows strong reduction relations frameworks considered paper Note I L P c rgeneral I L P b shown Example 8 srgeneral I L P b This changing background knowledge I L P c represent I L P b tasks Proposition 9 1 I L P b sr I L P sm sr I L P L A S sr I L P L O A S sr I L P context L O A S 2 I L P c sr I L P L A S Proof 1 For I L P b task T b cid3B cid3E E cid4cid4 I L P bT b I L P smcid3B cid3cid3E E cid4cid4cid4 For I L P sm task T sm cid3B cid3e1 encid4cid4 I L P smT sm I L P L A S cid3B cid3e1 en cid4cid4 cid4cid4 For I L P L A S task T L A S cid3B cid3E For partial interpretation e let ce CDPI cid3e cid4 For I L P L O A S task T L O A S cid3B cid3E I L P L O A S T L O A S I L P context O ccid4cid4 O b O ccid4cid4 cid3ce1 ce2cid4 cid3e1 e2cid4 O b cid3ce1 ce2cid4 cid3e1 e2cid4 cid4cid4 I L P L A S T L A S I L P L O A S cid3B cid3E ce e E E E E n 2 For I L P c task T c cid3B cid3e cid4 cid3e cid4cid4cid4 Note I L P L A S positive example enforces answer set I L P c positive negative examples mapped I L P L A S negative examples enforce case positive resp negative examples false resp true answer set true resp false answer set cid2 cid4cid4 I L P cT c I L P L A S cid3B cid3cid3 cid4 cid3 e cid4 cid3 e cid4 cid3e m 1 L O A S cid3B cid3ce e E 1 e 1 e e m n 1 51 Distinguishability A onetoonedistinguishability class captures pairs hypotheses H 1 H2 distinguished respect given possible background knowledge Deﬁnition 13 The onetoonedistinguishability class learning framework F denoted D1 1F set tuples cid3B H1 H2cid4 ASP programs task T F cid3B EF cid4 H1 F T F H2 F T F For cid3B H1 H2cid4 D1 1F T F said distinguish H1 H2 respect B Given frameworks F1 F2 F1 resp D1 1 general resp F2 D1 1F1 resp D1 1F2 D1 1F 2 D1 1F 1 Note onetoonedistinguishability relationship symmetric pairs hypotheses H 1 H2 given background knowledge B H 1 distinguished H 2 H2 distinguished H 1 This illustrated Example 9 Example 9 Consider background knowledge B deﬁnes concepts cell same_block same_row same_column 4x4 Sudoku grid Let H1 incomplete description Sudoku rules 1 valueC 1 valueC 2 valueC 3 valueC 4 1 cellC valueC1 V valueC2 V same_rowC1 C2 valueC1 V valueC2 V same_colC1 C2 Also let H2 complete description Sudoku rules 1 valueC 1 valueC 2 valueC 3 valueC 4 1 cellC valueC1 V valueC2 V same_rowC1 C2 valueC1 V valueC2 V same_colC1 C2 valueC1 V valueC2 V same_blockC1 C2 I L P b distinguish H1 H2 respect B This seen task cid3B cid3value1 1 1 value2 2 1 cid4cid4 On hand I L P b distinguish H2 H1 Whatever examples given learning A A answer set B H 2 But answer sets task learn H2 case E B H2 answer sets B H1 So A answer set B H 1 implies H1 satisﬁes examples solution learning task A E 124 M Law et al Artiﬁcial Intelligence 259 2018 110146 Table 3 A summary suﬃcient necessary conditions learning framework hypothesis H1 distinguishable hypothesis H2 respect background knowledge B Framework F I L P cid23 I L P b I L P sm I L P c I L P L A S I L P L O A S I L P context L O A S 1F Suﬃcientnecessary condition cid3B H1 H2cid4 D1 A SB H1 cid2 A SB H2 A SB H1 cid2 A SB H2 A SB H1 cid17 A SB H2 EcB H1 cid2 EcB H2 A SB H1 cid17 A SB H2 A SB H1 cid17 A SB H2 ordB H1 cid17 ordB H2 B H1 cid17s B H2 C ASP ch st ordB H1 C cid17 ordB H2 C In fact Proposition 10 generalises Example 9 showing I L P b distinguish program containing constraint program constraint Proposition 10 I L P b distinguish hypothesis H contains constraint C HC respect background knowledge Proof Assume contradiction hypothesis H H cid3B cid3E cid4cid4 H I L P bT b H cid20 I L P bT b E cid20 C C constraint I L P b task T b A A SB H E A E A But C constraint A SB H A SB H cid20 A cid20 A SB H A A SB H H cid20 I L P bT b Contradiction cid2 cid20 E A E A One useful property strong reduction framework F1 framework F2 1 general F1 case 1F1 D1 1F2 Note F2 guaranteed D1 D1 reduction F2 F1 Proposition 11 For frameworks F1 F2 F1 sr F2 D1 1F1 D1 1F2 Proof Assume F1 sr F2 Take cid3B H1 H2cid4 D1 1F1 There task T F1 background knowledge B H1 I L P F1 T F1 H2 I L P F1 T F1 Hence F1 sr F2 task T F2 background knowledge B H1 I L P F2 T F2 H2 I L P F2 T F2 So cid3B H1 H2cid4 D1 1F2 Hence D1 1F1 D1 1F2 cid2 As clear strong reductions shown Proposition 9 ordering onetoonedistinguishability classes frameworks emerges shown Corollary 4 Corollary 4 1 D1 2 D1 1I L P b D1 1I L P c D1 1I L P sm D1 1I L P L A S 1I L P L A S D1 1I L P L O A S D1 1I L P context L O A S While information ordering power frameworks distinguish hy potheses tell example relationship distinguishability classes I L P b I L P c It tell s strict fact D1 1I L P sm rest strict subset lations For framework Table 3 shows necessary suﬃcient condition needed able distinguish hypotheses In case cautious induction framework condition makes use new notation Given program P EbP i1 im e1 en A A SP st i1 m A e1 e n A EbP denotes set conjunctions literals true answer set P Similarly use EcP denote set conjunctions literals true answer set P The following property holds 1 I L P b D1 Proposition 12 For programs P 1 P 2 EbP 1 EbP 2 A SP 1 A SP 2 Propositions 13 18 prove onetoonedistinguishability classes I L P b I L P sm I L P c I L P L A S I L P L O A S L O A S showing suﬃcient necessary conditions distinguishability presented Table 3 To aid readability I L P context proofs appendix main paper cid10 cid14 cid11 cid14 A SB H1 cid5 A SB H2 cid3B H1 H2cid4 Proposition 13 D1 1I L P b M Law et al Artiﬁcial Intelligence 259 2018 110146 125 Interestingly I L P sm cid4sr I L P b D1 1I L P sm This shown Proposition 14 The reason I L P sm distinguish hypothesis H 1 hypothesis H 2 task T sm H1 solution T sm H2 This means H1 cover examples T sm partial interpretation example T sm covered H2 This partial interpretation example given set positive negative examples I L P b task This I L P b task distinguish H 1 H2 1I L P b D1 Proposition 14 D1 1I L P b D1 1I L P sm To better compare conditions I L P b I L P c express necessary suﬃcient condition I L P b terms notion EbP Speciﬁcally I L P b hypothesis H1 distinguishable hypothesis H 2 respect background knowledge B necessary suﬃcient EbB H1 contain conjunction EbB H2 This extra conjunction generate set examples covered H1 H2 This demonstrated Example 10 Example 10 Consider programs B H 1 1heads tails1 H2 heads EbB H1 contains conjunction heads tails EbB H2 This conjunction mapped positive example tails negative example heads B H 1 covers B H2 task cid3B cid3tails headscid4cid4 distinguishes H1 H2 So onetoonedistinguishability condition I L P b expressed EbB H1 cid5 EbB H2 expected onetoonedistinguishability condition I L P c EcB H1 cid5 EcB H2 Indeed case extra condition I L P c imposes inductive solution inductive solution H B H satisﬁable Although extra condition unnecessary ﬁrst sight importance clear considering distinguishability Without extra condition hypothesis distinguishable hypothesis given constraint hypothesis H cid3B H cid4 D1 1I L P c B This answer set B cover examples answer sets As I L P c extra condition B H satisﬁable distinguishability condition slightly complicated EcB H1 cid5 EcB H2 shown Proposition 15 Proposition 15 D1 1I L P c cid15 cid14 cid14 cid14 cid3B H1 H2cid4 cid14 A SB H1 cid17 A SB H2 EcB H2 cid5 EcB H1 cid16 We prove onetoonedistinguishability classes frameworks I L P L A S I L P L O A S D1 1I L P L A S 1I L P c I L P L A S distinguish hypotheses combined background tains D1 1I L P b D1 knowledge different answer sets Proposition 16 D1 1I L P L A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 As shown Theorem 4 I L P L O A S D1 1 general I L P L A S This I L P L O A S able use ordering examples distinguish hypotheses combined background knowledge order answer sets differently programs answer sets cid15 cid16 Proposition 17 D1 1I L P L O A S cid14 cid14 cid14 cid3B H1 H2cid4 cid14 A SB H1 cid17 A SB H2 ordB H1 cid17 ordB H2 Note assume I L P L O A S able ordering examples binary ordering operators The slightly restrictive version I L P L O A S presented 23 operator smaller onetoone distinguishability class This shown Example 11 Example 11 Consider heads tails problem B cid10 1heads tails1 cid11 potential hypotheses H1 H2 heads11 A SB H1 A SB H2 heads tails If consider restricted I L P L O A S operator express ordering examples H 2 distinguished H 1 H1 H2 This answer sets B H1 equally optimal cid3tails heads cid4 cid3heads tails cid4 ordB H 1 In contrast allow use binary ordering operators consider task ordering example cid3tails heads cid4 able distinguish H 1 H2 The learned hypothesis H1 weak constraints 126 M Law et al Artiﬁcial Intelligence 259 2018 110146 answer sets equally optimal ordering example respected H 1 H2 prefers tails heads I L P L O A S distinguish hypotheses combined ﬁxed background knowledge behave dif ferently It distinguish hypotheses different behave respect background knowl edge This means hypotheses strongly equivalent combined background knowledge I L P L O A S distinguish We I L P context distinguish L O A S hypotheses H1 H2 combined background knowledge strongly equivalent program C ASP ch consisting normal rules choice rules hard constraints ordB H1 C cid17 ordB H2 C Proposition 18 D1 1I L P context L O A S cid15 cid3B H1 H2cid4 cid14 cid14 cid14 cid14 B H1 cid17s B H2 C ASP ch ordB H1 C cid17 ordB H2 C cid16 Now proven distinguishability classes learning framework strengthen statement Corollary 4 precisely state relationship distinguishability classes frameworks Apart case I L P b I L P sm subset relations Corollary 4 fact strict subsets Theorem 4 Consider learning frameworks I L P b I L P c I L P sm I L P L A S I L P L O A S I L P context L O A S 1 D1 2 D1 1I L P b D1 1I L P c D1 1I L P sm D1 1I L P L A S 1I L P L A S D1 1I L P L O A S D1 1I L P context L O A S Proof 1I L P b D1 1I L P context 1 The fact D1 1I L P sm shown Proposition 14 By Corollary 4 D1 1I L P sm D1 1I L P L O A S D1 1I L P context L O A S L O A S remains D1 D1 D1 Consider tuple cid3B H1 H2cid4 B H1 p H2 1p q1 A SB H1 A SB H2 1I L P sm It cid3B H1 H2cid4 satisfy condition given Table 3 necessary D1 satisfy condition D1 1I L P L A S cid17 D1 1I L P sm cid17 D1 Consider tuple cid3B H1 H2cid4 B 1p q1 H1 H2 p11 A SB H1 A SB H2 1I L P L O A S ordB H1 cid17 ordB H2 Hence conditions Table 3 cid3B H1 H2cid4 D1 D1 1I L P L O A S 1I L P L A S Therefore D1 Consider tuple cid3B H1 H2cid4 B H1 H2 pq Also consider program P q A SB H1 A SB H2 ordB H1 ordB H2 A SB H1 P cid17 A SB H2 P shows B H1 cid17s B H2 Hence conditions Table 3 cid3B H1 H2cid4 D1 1I L P L O A S Therefore D1 L O A S D1 1I L P L A S cid17 D1 1I L P context 1I L P L A S Hence D1 1I L P sm cid17 D1 1I L P L A S 1I L P L A S 1I L P L O A S cid17 2 By Corollary 4 D1 1I L P L A S Consider tuple cid3B H1 H2cid4 B p p H1 H2 p A SB H1 A SB H1 cid17 A SB H2 By conditions Table 3 cid3B H1 H2cid4 D1 1I L P L A S Hence remains D1 1I L P c cid17 D1 1I L P L A S D1 1I L P c Hence D1 1I L P L A S cid2 1I L P c cid17 D1 1I L P L O A S cid17 D1 1I L P context L O A S 1I L P c D1 52 The onetomanydistinguishability class learning framework In practice ILP task search space possible hypotheses important know cases particular hypothesis distinguished rest In follows analyse conditions learning framework distinguish hypothesis set hypotheses As mentioned beginning Section 5 corresponds new notion onetomanydistinguishability class learning framework generalisation notion onetoonedistinguishability class described Deﬁnition 14 The onetomanydistinguishability class learning framework F denoted D1 mF set tu ples cid3B H H1 Hncid4 task T F distinguishes H H respect B Given frameworks F1 F2 F1 resp D1 mF1 resp D1 mgeneral F2 D1 mF2 D1 mF 2 D1 mF 1 The onetomanydistinguishability class tells circumstances framework general distinguish target hypothesis set unwanted hypotheses Note tuples oneto M Law et al Artiﬁcial Intelligence 259 2018 110146 127 manydistinguishability class singleton set argument correspond tuples onetoone mgeneral F2 F1 distinguishability class framework case F1 D1 D1 1 general F2 For example I L P sm D1 mgeneral I L P b 1 general Proposition 19 shows F1 shown Proposition 14 I L P b I L P sm equally D1 1 general F2 D1 mgeneral F2 F1 D1 Proposition 19 For frameworks F1 F2 F1 D1 D1 mF1 D1 mF2 D1 1F2 D1 1F1 mgeneral F2 F1 D1 1 general F2 Proof Assume F1 D1 D1 1 general F2 cid3B H1 H2cid4 D1 As cid3B H1 H2cid4 D1 cid3B H1 H2cid4 D1 1F2 cid3B H1 H2cid4 D1 1F1 cid2 1F1 mgeneral F2 let cid3B H1 H2cid4 D1 1F2 To F1 mF2 F1 D1 mgeneral F2 cid3B H1 H2cid4 D1 mF1 We seen strong reduction F1 F2 F2 D1 1 general F1 1 generality strong reduction mgeneral F1 case strong reduction mgenerality Similarly D1 Proposition 20 shows similar result holds D1 F1 F2 imply F2 D1 F2 F1 Proposition 20 For frameworks F1 F2 F1 sr F2 D1 mF1 D1 mF2 Proof Assume F1 sr F2 Take cid3B H Scid4 D1 mF1 There task T F1 background knowledge B H I L P F1 T F1 S I L P F1 T F1 Hence F1 sr F2 F2 task T F2 background knowledge B H I L P F2 T F2 S I L P F2 T F2 So cid3B H Scid4 D1 mF2 Hence D1 mF1 D1 mF2 cid2 Due strong reductions shown Proposition 9 ordering onetomanydistinguishability classes frameworks emerges shown Corollary 5 Corollary 5 1 D1 2 D1 mI L P b D1 mI L P c D1 mI L P sm D1 mI L P L A S mI L P L A S D1 mI L P L O A S D1 mI L P context L O A S This time s Corollary 5 upgraded strict Rather proving onetomanydistinguishability classes scratch present useful result For frameworks onetoone distinguishability class learning framework construct onetomanydistinguishability class This case framework closed onetomanydistinguishability formalised Deﬁnition 15 Proposition 21 Corollary 6 onetomanydistinguishability class framework constructed onetoone distinguishability class closed onetomanydistinguishability Deﬁnition 15 Given learning framework F closure onetomanydistinguishability class written D1 set cid3B H S1 Sncid4 cid3B H S1cid4 cid3B H Sncid4 D1 D1 mF mF We F closed onetomanydistinguishability mF D1 mF Proposition 21 For learning framework F D1 Corollary 6 For learning framework F D1 holds F closed onetomanydistinguishability mF mF cid10 cid10 cid14 cid14cid3B H H1cid4 cid3 B H Hncid4 D1 cid3B H H1 Hncid4 cid14 cid14cid3B H H1cid4 cid3 B H Hncid4 D1 cid3B H H1 Hncid4 cid11 1F cid11 1F The equality Note learning frameworks closed onetomanydistinguishability instance Example 12 shows brave induction We induction stable models hand closed onetomany distinguishability Example 12 I L P b closed onetomanydistinguishability We reconsidering programs B H 1heads tails1 H1 heads H2 tails cid3B H H1cid4 D1 mI L P b cid3B cid3tails cid4cid4 distin guishes H H1 wrt background knowledge B Similarly cid3B H H 2cid4 D1 mI L P b cid3B cid3heads cid4cid4 distinguishes H H2 wrt background knowledge B If I L P b closed onetomanydistinguishability cid3B H H 1 H2cid4 128 M Law et al Artiﬁcial Intelligence 259 2018 110146 D1 cid3B H H1 H2cid4 D1 H1 H2 I L P bT b mI L P b I L P b closed onetomanydistinguishability suﬃcient cid4cid4 H I L P bT b mI L P b Hence remains task T b cid3B cid3E E Assume contradiction task T b As H I L P bT b A SB H heads tails E heads tails equal heads tails H solution heads tails E Case 1 E Case E Case b E heads Then H1 H2 inductive solutions This contradiction H 1 H2 I L P bT b Then H2 inductive solution T b Contradiction Case c E tails Then H1 inductive solution T b Contradiction Case 2 E heads Case 3 E tails heads E inductive solution regardless E task solutions know H solution In case H 1 Contradiction Similarly case tails E inductive solution regardless E Contradiction task solutions In case H 2 Hence task T b cid3B cid3E closed onetomanydistinguishability E cid4cid4 H I L P bT b H1 H2 I L P bT b I L P b In contrast I L P b I L P sm closed onetomanydistinguishability distinguish H H1 H2 task cid3B cid3cid3heads cid4 cid3tails cid4cid4cid4 Note combination brave tasks distinguish H H1 H2 We ability combine tasks way suﬃcient condition framework closed onetomanydistinguishability Proposition 22 shows onetomanydistinguishability class I L P b Proposition 22 D1 mI L P b cid10 cid14 cid11 cid14 A SB H cid5 A SB h1 A SB hm cid3B H h1 hmcid4 Proof 1 Let B H h1 hm ASP programs A SB H cid5 A SB h1 A SB hm This implies interpretation A answer set B H answer set programs B h1 B hm Let L set atoms occur answer set programs B H B h1 B hm B H answer set extends cid3 A L Acid4 B h1 B hm So task cid3B cid3 A L Acid4cid4 distinguishes H h1 hm Hence cid3B H h1 hmcid4 D1 2 Assume cid3B H h1 hmcid4 D1 set extending cid3E answer set B h1 B hm Therefore A SB H cid5 A SB h1 A SB hm cid2 cid4cid4 B H answer cid4 B h1 B hm Hence answer set B H mI L P b Then I L P b task T b cid3B cid3E mI L P b E E For framework F closed onetomanydistinguishability suﬃcient necessary F tasks F task solutions exactly hypotheses solutions original tasks This formalised proved Lemma 2 This condition necessary general holds frameworks considered paper closed onetomanydistinguishability Lemma 2 For learning framework F closed onetomanydistinguishability suﬃcient pair learn F ing tasks T 1 I L P F T 1 F cid4 possible construct new learning task T 3 F cid4 I L P F T 3 F cid3B E 1 F I L P F T 2 F cid4 T 2 F F cid3B E 2 F cid3B E 3 Proof Assume pair learning tasks T 1 learning task T 3 F closed onetomanydistinguishability cid3B H S 1 Sncid4 D1 showing mathematical induction k 1n cid3B H S1 Skcid4 D1 F cid4 I L P F T 3 F cid4 possible construct new mF To prove mF We prove F Let cid3B H S1cid4 cid3B H Sncid4 D1 F I L P F T 1 F I L P F T 2 F cid4 T 2 F cid3B E 1 F cid3B E 2 F cid3B E 3 mF Base Case k 1 cid3B H S1cid4 D1 Inductive Hypothesis Assume 0 k n cid3B H S1 Skcid4 D1 Inductive Step We cid3B H S1 Sk1cid4 D1 mF initial assumptions mF mF M Law et al Artiﬁcial Intelligence 259 2018 110146 129 As cid3B H S1 Skcid4 D1 S1 Sk I L P F T 1 H I L P F T 2 I L P F T 3 fore H I L P F T 3 F I L P F T 1 F I L P F T 2 F Sk1 I L P F T 2 mF inductive hypothesis learning task T 1 F H I L P F T 1 F As cid3B H Sk1cid4 D1 F By initial assumption learning task T 3 F S1 Sk I L P F T 3 mF learning task T 2 F cid3B E 3 F Sk1 I L P F T 3 F So H I L P F T 3 F F F cid4 F There F S1 Sk1 I L P F T 3 F Hence cid3B H S1 Sk1cid4 D1 mF cid2 Proposition 23 I L P c I L P sm I L P L A S I L P L O A S I L P context L O A S closed onetomanydistinguishability Theorem 5 Given frameworks F1 F2 D1 mF1 D1 mF2 D1 1F1 D1 1F2 Proof D1 mF1 D1 mF2 cid14 cid14 cid14 cid14 cid3B H H1 Hncid4 cid14 cid14 cid14 cid14 cid14 cid14 cid3B H H1 Hncid4 cid14 cid14 cid3B H H1cid4 D1 1F1 cid3B H Hncid4 D1 1F1 cid3B H H1cid4 D1 cid3B H Hncid4 D1 1F2 1F2 Proposition 21 D1 1F1 D1 1F2 cid2 Corollary 7 Given frameworks F1 F2 closed onetomanydistinguishability D1 D1 1F1 D1 1F2 mF1 D1 mF2 Theorem 6 Consider learning frameworks I L P b I L P c I L P sm I L P L A S I L P L O A S I L P context L O A S 1 D1 2 D1 mI L P b D1 mI L P c D1 mI L P sm D1 mI L P L A S mI L P L A S D1 mI L P L O A S D1 mI L P context L O A S Proof Firstly shown Example 12 D1 D1 follow Corollary 7 Proposition 23 cid2 mI L P sm I L P sm closed onetomanydistinguishability D1 mI L P b strict subset D1 mI L P b Hence Theorem 5 D1 mI L P b mI L P sm The results mI L P b D1 Even frameworks F1 F2 closed onetomanydistinguishability case combination closed onetomanydistinguishability Example 13 shows example case I L P sm I L P c We deﬁne ﬁrst mean combination framework constructed given frameworks Deﬁnition 16 Given frameworks F1 F2 combination framework combF1 F2 allows task cid3B cid31 E 1cid4cid4 cid3B E 1cid4 F1 task task cid3B cid32 E 2cid4cid4 cid3B E 2cid4 F2 task cid15 Given combF1 F2 task T cid3B cid3x Ecid4cid4 I L P combF1F2T I L P F1 cid3B Ecid4 I L P F2 cid3B Ecid4 x 1 x 2 1I L P c Hence Deﬁnition 16 D1 1I L P c task cid3B cid3 qcid4cid4 This shows cid3B H H 1cid4 cid3B H H2cid4 D1 Example 13 Consider frameworks I L P sm I L P c closed onetomanydistinguishability Consider programs B H 0p1 H1 H2 0p q1 cid3B H H1cid4 D1 1I L P sm task cid3B cid3cid3p cid4cid4cid4 cid3B H H2cid4 D1 1I L P sm D1 1combI L P sm I L P c But distinguishability conditions proven previous section seen framework distinguish H H 1 H2 Therefore cid3B H H1 H2cid4 D1 mI L P L A S This D1 mI L P c closed onetomanydistinguishability contain cid3B H H 1 H2cid4 mI L P L A S contains cid3B H H1cid4 cid3B H H2cid4 contains D1 mcombI L P sm I L P c This means D1 mI L P c strict subset D1 mI L P sm D1 mI L P sm D1 53 The manytomanydistinguishability class learning framework So far considered main classes deﬁne general learning framework Firstly discussed cid20cid4 framework distinguish H onetoonedistinguishability class tuples cid3B H H respect B We showed limitations separate I L P b I L P sm I L P b clearly H special case I L P sm This motivated upgrading notion onetoonedistinguishability class changing cid20 130 M Law et al Artiﬁcial Intelligence 259 2018 110146 element tuple single hypothesis set hypotheses notion onetomanydistinguishability class This naturally leads question possible upgrade generality classes allowing second element tuple set hypotheses Each tuple form cid3B S 1 S2cid4 B background knowledge S1 S2 sets hypotheses For tuple new class framework required task T background knowledge B hypothesis S 1 inductive solution T hypothesis S2 inductive solution T Deﬁnition 17 formalises manytomanydistinguishability class Deﬁnition 17 The manytomanydistinguishability class learning framework F denoted Dm mF set tuples cid3B S1 S2cid4 B program S1 S2 sets hypotheses task T F background knowledge B S1 I L P F T F S2 I L P F T F Given frameworks F1 F2 F1 resp Dm m general F2 Dm mF1 resp Dm m F2 Dm mF2 Dm mF1 We seen frameworks F1 F2 F1 sr F2 D1 We seen D1 F1 subset relations necessarily strict Proposition 24 Corollary 8 Dm strong reductions 1F2 mgenerality corresponding strong reduction F2 m generality equivalent 1 generality D1 mF2 D1 mF1 D1 1F1 D1 Proposition 24 For learning frameworks F1 F2 F1 sr F2 Dm mF1 Dm mF2 Proof mF2 2 Assume Dm 1 Assume F1 sr F2 Let cid3B S1 S2cid4 arbitrary element Dm mF1 task T F1 background knowledge B S1 I L P F1 T F1 S2 I L P F1 T F1 Hence F1 sr F2 F2 task T F2 background knowledge B S1 I L P F2 T F2 S2 I L P F2 T F2 Hence cid3B S1 S2cid4 Dm m F1 By deﬁnition Dm m F1 Dm mF2 Let T F1 arbitrary F1 task We F2 task background knowledge inductive solutions Let B background knowledge T F1 S1 I L P F1 T F1 S2 possibly inﬁnite set ASP programs S1 cid3B S1 S2cid4 Dm mF1 cid3B S1 S2cid4 Dm mF2 Therefore task T F2 background knowledge B I L P F2 T F2 S1 Hence F1 sr F2 cid2 Corollary 8 For learning frameworks F1 F2 F1 Dm m general F2 F2 sr F1 F1 cid4sr F2 Proposition 25 For frameworks F1 F2 Dm mF1 Dm mF2 D1 mF1 D1 mF2 Proof Assume Dm Dm mF2 Hence cid3B H Scid4 D1 mF1 Dm mF2 cid2 mF2 let cid3B H Scid4 D1 mF1 Then cid3B H Scid4 Dm mF1 cid3B H Scid4 Theorem 7 shows framework D1 mgeneral implies Dm m general strong reduction second framework ﬁrst Theorem 7 For frameworks F1 F2 F1 D1 F2 mgeneral F2 F2 sr F1 F1 Dm m general Proof Assume F1 D1 F2 It remains F2 Dm Dm D1 m general F1 Then Proposition 25 F2 D1 mgeneral F2 cid2 mgeneral F2 F2 sr F1 By Proposition 24 F1 Dm m general m general F1 Assume contradiction F2 mgeneral F1 contradicting fact F1 Corollary 9 Consider learning frameworks I L P b I L P c I L P sm I L P L A S I L P L O A S I L P context L O A S 1 Dm 2 Dm mI L P b Dm mI L P c Dm mI L P sm Dm mI L P L A S mI L P L A S Dm mI L P L O A S Dm mI L P context L O A S Proof Each result follows directly Theorem 6 Theorem 7 Proposition 9 cid2 Note pair frameworks discussed paper D1 m general implies Dm m general result hold general Example 14 shows pair frameworks M Law et al Artiﬁcial Intelligence 259 2018 110146 131 Property F1 F2 equal D1 Table 4 A summary relationships different measures generality paper Consequences property F1 F2 equal D1 1 F1 F2 equal D1 2 F1 F2 equal D1 Either F1 D1 F1 F2 equal Dm 1 general F2 F1 D1 D1 m generality mgenerality mgenerality 1 generality 1 generality mgenerality mgeneral F2 F1 F2 incomparable F1 D1 mgeneral F2 F1 Dm m general F2 F1 D1 F1 Dm mgeneral F2 m general F2 F1 F2 different D1 1 generality F1 F2 different D1 mgenerality F1 F2 incomparable D1 1 generality F1 F2 incomparable D1 mgenerality m general F2 F1 F2 incomparable m generality 1 general F2 1 general F2 1 general F2 mgeneral F2 1 Either F1 Dm Dm 2 F1 D1 1 F1 D1 2 F1 D1 F1 D1 1 F1 D1 2 F1 D1 1 F1 F2 different D1 2 F1 F2 different Dm F1 F2 different Dm 1 F1 F2 incomparable D1 2 F1 F2 incomparable Dm F1 F2 incomparable Dm 1 general F2 mgeneral F2 mgenerality m generality m generality mgenerality m generality m generality Example 14 Consider new learning framework I L P d takes examples pair sets atoms E hypothesis H inductive solution task B H exactly answer set answer set contains 1I L P c This seen follows E assume cid3B H H 1I L P d Then task Td cid3B cid3E s The onetoonedistinguishability class D1 s E cid20cid4 D1 cid4cid4 H I L P dTd H 1I L P d D1 cid20 I L P dTd E E Case 1 A SB H Let T c cid3B cid3E A SB H Case 2 B H cid20 E cid20cid4 D1 cid20 H cid20 exactly answer set answer set cover examples cid4cid4 As B H exactly answer set answer set covers examples H I L P cT c As cid20 I L P cT c Hence cid3B H H 1I L P c E cid4cid4 As B H exactly answer set answer set covers examples H I L P cT c As answer set cover examples H cid20 I L P cT c Hence cid3B H H cid20cid4 D1 1I L P c Let T c cid3B cid3E cid20 B H Case 3 B H cid20 multiple answer sets There answer set A set There atom A unique answer set B H In ﬁrst case let E E H B H unique answer set B H atom A answer set B H B H answer c cid4cid4 H I L P cT c answer set B H covers examples answer set cover examples Hence cid3B H H In second case let E Then let T c cid3B cid3E cid20 I L P cT c B H E cid20cid4 D1 c E c c c c cid20 cid20 1I L P c I L P c 1I L P c D1 1I L P d strict subset D1 cid20cid4 B H multiple answer In fact D1 sets As I L P c closed onetomanydistinguishability onetomanydistinguishability classes subsets closure means I L P c D1 1I L P d elements cid3B H H mgeneral I L P d Theorem 5 Dm m general I L P d Take instance tuple t cid3 heads tails 1heads tails1cid4 The set examples suﬃcient I L P d distinguish hypotheses containing facts choice rule choice rule multiple answer sets However I L P c task facts solutions choice rule Hence t Dm mI L P d t Dm m general m general I L P c incomparable Dm I L P d In fact I L P d Dm mI L P c I L P c Dm m generalities Example 14 shows Dm m generality able compare frameworks clear mgenerality relation In section discuss relationships relative merits D1 measure generality 132 M Law et al Artiﬁcial Intelligence 259 2018 110146 54 Discussion Table 4 summarises relationships different measures generality presented paper It shows equal onetoonedistinguishability weaker equal onetomanydistinguishability weaker equal tomanydistinguishability This seen ﬁrst section table equal manytomanydistinguishability implies equal onetomanydistinguishability implies equal onetoonedistinguishability converse implica tions hold general On hand different onetoonedistinguishability stronger different oneto manydistinguishability turn stronger different manytomanydistinguishability This means tomanydistinguishability resp onetomanydistinguishability able separate frameworks onetomany distinguishability resp onetoonedistinguishability frameworks incomparable manytomanydistinguishability resp onetomanydistinguishability onetomanydistinguishability resp oneto onedistinguishability The different notions generalities inconsistent sense F1 general F2 says F2 general F1 It useful explain tasks different measures generality correspond 1 Onetoonedistinguishability describes general framework distinguishing hypothesis 2 Onetomanydistinguishability describes general framework task identifying target hypothesis space unwanted hypotheses 3 Manytomanydistinguishability describes general framework task identifying set target hy potheses background knowledge B set hypotheses S task T F background knowledge B I L P F T F S cid3B S Scid4 Dm mF S inﬁnite set hypotheses S In practice ILP usually addresses task ﬁnding single target hypothesis space hypotheses tomanydistinguishability likely useful measure onetoonedistinguishability classes useful ﬁnding onetomanydistinguishability classes frameworks manytomanydistinguishability interesting theoretical property 541 More general learning frameworks We shown section I L P context L O A S general measure tasks pre sented learning answer set semantics The obvious question possible deﬁne general learning tasks The D1 mgenerality 1 general learning task possible able distinguish different ASP programs H 1 H2 respect background knowledge B This require learning task distinguish programs strongly equivalent p q p p q q We argue level onetoone distinguishability unnecessary ILP aim learn programs output explains examples As strongly equivalent programs output combined additional programs providing context reason going D1 L O A S closed onetomanydistinguishability argument D1 1 generality As I L P context One outstanding question worth going Dm m generality Note possible deﬁne notion closure manytomanydistinguishability classes frameworks considered paper closed manytomanydistinguishability It unclear having closed manytomanydistinguishability desirable property framework Closed onetomanydistinguishability means framework distinguish target hypothesis H set hypotheses S distinguish H element S means sets examples distinguish H element S combined form single set examples ruling element S For framework closed manytomanydistinguishability given target hypotheses h1 h2 distinguished undesirable hypothesis h3 need able ﬁnd task distinguished h1 h2 h3 For example cid3 heads 1heads tails1cid4 cid3 tails 1heads tails1cid4 D1 1I L P L A S I L P L A S closed manytomanydistinguishability need able ﬁnd task background knowledge distinguishes heads tails 1heads tails1 It diﬃcult imagine scenario learn hypothesis coin heads tails choice rule desirable hypothesis 542 The generality noisy frameworks As discussed Section 44 learning systems able solve tasks examples potentially noisy case examples necessarily covered trade maximising coverage overﬁtting examples One method XHAIL 42 ILASP 32 systems penalise hypothesis example covered Examples given positive integer penalty paid example covered M Law et al Artiﬁcial Intelligence 259 2018 110146 133 The measures generality presented section extended cover noisy tasks For instance case onetoonedistinguishability deﬁne noisy onetoonedistinguishability class learning framework set tuples cid3B H1 H2cid4 set examples E pH 1 E pH2 E pH E total penalty paid hypothesis H background knowledge B examples E In fact extended notion onetoonedistinguishability class equivalent standard nonnoisy onetoonedistinguishability class cid20 pH2 E cid20 E cid20 As penalties positive pH 1 E pH2 E implies pH1 E set examples cid20 cid20 H2 This E covered H Hence set examples E means cid3B H1 H2cid4 noisy onetoonedistinguishability class standard nonnoisy onetoonedistinguishability class Similarly tuple cid3B H 1 H2cid4 standard onetoonedistinguishability class set examples E H 1 covers example E H 2 pH1 E pH2 E cid3B H1 H2cid4 noisy onetoonedistinguishability class H1 example E A similar argument holds onetomanydistinguishability class worth noting hold true manytomanydistinguishability class If upgrade manytomanydistinguishability class way tuples noisy manytomanydistinguishability class framework standard manytomanydistinguishability class Take instance example discussed previous section cid3 heads tails 1heads tails1cid4 Dm mI L P L A S However consider I L P L A S examples E cid3 cid3heads cid4 cid3tails cid4cid4 pheads E 1 ptails E 1 p1heads tails1 E 2 meaning cid3 heads tails 1heads tails1cid4 noisy Dm mI L P L A S 6 Related work The complexity I L P b I L P c veriﬁcation satisﬁability investigated 28 However work results satisﬁability deciding task solutions restrictions hypothesis space This means I L P b I L P c deciding task satisﬁable equivalent checking model B examples covered simpler decision problem For reason complexity satisﬁability I L P c 28 N P complete cid2P 2 complete The complexities given veriﬁcation hypothesis given 28 different ones paper consider different language B H They consider disjunctive logic programs investigated complexity learning programs disjunction The reason chose consider disjunctive logic programs systems available ILP answer set semantics allow disjunction For example systems I L P b 3839 allow disjunction allowing disjunction raise complexity complexity tasks actually solved practice existing systems As discussed Section 5 generality learning framework investigated In 45 author deﬁned generality terms reductions framework F1 said general framework F2 F2 r F1 F1 cid4r F2 We showed Section 5 ﬁnal notion generality manytomanydistinguishability coincides similar notion strong reductions The difference strong reductions compared reductions 45 strong reductions allow background knowledge modiﬁed reduction We showed Example 8 I L P b reduces I L P c I L P b strongly reduce I L P c This reduction I L P b I L P c encode examples background knowledge argue abuses purpose background knowledge Aside differences strong reductions reductions discussed Section 5 onetomany distinguishability relevant comparing generalities frameworks respect task ﬁnding single hypothesis space hypotheses The reductions 45 closer notion manytomany distinguishability compare set solutions One key advantage notions generality strong reductions reductions comparing relative generalities frameworks strongly reduce For instance seen I L P b I L P c incomparable Dgenerality reason I L P b Dgeneral distinguish hypothesis containing constraint hypothesis constraint On hand I L P c Dgeneral example I L P c distinguish p respect background knowledge 0p1 task cid30p1 cid3 pcid4cid4 61 Other learning frameworks Traditional ILP aims learn Prolog style logic programs restricted learning deﬁnite programs negation failure For shared subset languages learned ILP frameworks ASP frameworks deﬁnite rules including lists deﬁnite learning task expressed brave cautious task examples deﬁnite task hypothesis space restricted deﬁnite logic programs As frameworks support features choice rules constraints negation ASP frameworks support lists comparison generality informative A review early efforts extend ILP learn normal logic programs presented 8 The techniques discussed 8 operate stable model answer set semantics require examples covered stable models answer sets This corresponds cautious induction 134 M Law et al Artiﬁcial Intelligence 259 2018 110146 We discussed frameworks ILP work answer set semantics shown sections 4 5 complexity generality frameworks compare frameworks In particular shown complexities learning frameworks I L P L A S I L P L O A S I L P context L O A S cautious induction learning problems represented learning answer sets represented brave cautious induction One example learning rules Sudoku This brave induction incentivise learning constraints rules Sudoku useful examples given cautious learner values cells cell value valid Sudoku board Another early work learning frameworks answer set semantics Induction Answer Sets 46 In paper learning algorithms I A S pos I A Sneg presented The task I A S pos learn hypothesis cautiously entails set examples This corresponds task cautious induction I A Sneg hand aims ﬁnd hypothesis cautiously entail set examples answer set contain example This sense reversed brave induction As shown paper general I A S pos I A Sneg procedures combined general compute correct hypothesis Another framework supported model semantics answer set semantics Learning Inter pretation Transitions LFIT 47 In LFIT examples pairs interpretations cid3I J cid4 J set immediate consequences I given B H In 24 presented mapping LFIT task I L P context task This shows complexity deciding satisﬁability veriﬁcation LFIT cid2P 2 complete The generality hand different tasks considered programs strongly equivalent answer sets semantics different supported models Example 15 demonstrates pair programs example learning interpretations use distinguish L A S Example 15 Consider programs P 1 P 2 P 1 p p P 2 P 1 P 2 strongly equivalent answer set semantics However P 1 supported model p P 2 LFIT distinguish P 1 P 2 respect background knowledge example cid3p pcid4 Example 15 shows I L P context L O A S distinguishability class contain LFITs distinguishability class Conversely LFIT distinguishability class contains D1 1I L P context L O A S distinguish hypotheses containing weak constraints hypotheses weak constraints In fact contain D1 1I L P L A S shown Example 16 Example 16 Consider programs P 1 P 2 cid10 cid15 P 1 P 2 cid11 p p p p p cid16 set p This means For programs P 1 P 2 immediate consequences interpretation I example possibly distinguish P 1 P 2 respect background knowledge Under answer set semantics P 1 answer set p P 2 answer sets I L P L A S distinguish P 1 P 2 respect background knowledge positive example cid3p cid4 I L P L A S I L P L O A S I L P context different distinguishability classes LFIT L O A S D1 1 general LFIT This interesting observation demonstrates frameworks incom parable measures generality reason individual distinguishability classes discuss hypotheses framework powerful distinguish For instance I L P L A S distinguish hypotheses strongly equivalent answer set semantics Example 15 shows cases I L P L F I T 62 Relation probabilistic ILP One advantages learning ASP programs Prolog programs ASP allows modeling non determinism unstratiﬁed negation choice rules The seen coin examples paper shown I L P L A S framework learn coin heads tails M Law et al Artiﬁcial Intelligence 259 2018 110146 135 Another method achieving nondeterminism ILP adding probabilities Probabilistic Inductive Logic Program ming 48 combination ILP probabilistic reasoning Its aim learn logic program annotated probabilities The task PILP divided structure learning underlying logic program learned parameter estimation weight learning probabilities learned A key difference I L P L A S PILP aim learn programs nondeterministic I L P L A S aims learn programs answer sets capture set possibilities PILP aims learn probability distribution possibilities Although signiﬁcant progress ﬁeld PILP 25495126 learning annotated Prolog programs PILP answer set semantics relatively young approaches PrASP 525327 considers problem weight learning fact uses similar example learning coins This example illustrates difference weight learning standard ILP In ILP task learn exactly possibilities heads tails weight learning goal estimate probabilities possibility PROBXHAIL 54 attempt combine structure learning weight learning learn deﬁnite logic programs While coin example paper viewed inherently probabilistic situations practice wish learn nondeterministic programs considering probability instance policy learning A policy permit valid actions given scenario impose constraints actions The task learn program answer sets reﬂect set valid options estimate probability action taken 7 Conclusion In paper investigated complexity generality state art frameworks learning answer set programs We shown decision problems veriﬁcation hypothesis inductive solution task deciding given task satisﬁable brave induction I L P b induction stable models I L P sm complexities cautious induction I L P c learning answer sets I L P L A S learning ordered answer sets I L P L O A S context dependent learning ordered answer sets I L P context L O A S complexities higher I L P b I L P sm Studying complexity decision problems learning frameworks important gives sense price paid choosing particular framework In contrast generality important shows advantages choosing framework specifying hypotheses learned framework When ILP practice trade complexity generality framework The generality classes presented paper inform decision likely inﬂuenced class programs learned 1 generality D1 mgenerality Dm We introduced new measures generality D1 m generality shown measures generality concept strong reductions ordering generalities frameworks considered paper Although I L P c I L P L A S I L P L O A S I L P context L O A S computational complexities I L P c general I L P L A S general I L P L O A S general I L P context L O A S measure generality This ordering seen strong reductions measures They allow reason framework D1 1 general example studying class tuples frameworks distinguishability class They allow discuss generalities frameworks incomparable strong reductions example strong reduction I L P c I L P b I L P b I L P c Our measures allow I L P b D1 1 general distinguish hypothesis containing constraint program constraint cases I L P c D1 1 general In paper results presented addressed nonnoisy learning frameworks In general ILASP systems support noise allowing examples labeled penalty In case ILASP searches hypothesis minimises sum H pH E pH E sum examples set E covered hypothesis H Such hypothesis called optimal solution For decision problems veriﬁcation satisﬁability shown complexity results unaffected In current work investigating complexities nonnoisy frameworks noisy frameworks differ decision problem verifying hypothesis optimal solution given task In future work hope upgrade propositional complexity results presented paper apply learning ﬁrst order answer set programs Acknowledgements This research partially funded EPSRC project EPK0335221 Privacy Dynamics EPSRC Doctoral Training Account EPL5047861 136 M Law et al Artiﬁcial Intelligence 259 2018 110146 Appendix A Proofs A1 Proofs Section 4 In section present proofs omitted Section 4 In proofs use predicate symbols avoid continually introducing new atoms aid readability As section restricted propositional programs ﬁrst order atom interpreted new propositional atom Proposition 1 1 Deciding veriﬁcation satisﬁability I L P b reduces polynomially corresponding I L P sm decision problem 2 Deciding veriﬁcation satisﬁability I L P sm reduces polynomially corresponding I L P b decision problem Proof 1 Let T b cid3B S M cid3E E cid4cid4 arbitrary I L P b task E cid4cid4cid4 H H I L P smT sm H I L P bT b deciding Consider task T sm cid3B S M cid3cid3E veriﬁcation I L P b reduces polynomially deciding veriﬁcation I L P sm Similarly I L P smT sm I L P bT b T sm satisﬁable T b satisﬁable deciding satisﬁability I L P b reduces deciding satisﬁability I L P sm 2 Let T sm cid3B S M cid3Ecid4cid4 arbitrary I L P sm task Let n E E e1 en For integer 1 n let f function maps atom B S M new atom ai We extend notation work sets atoms rules parts rules replacing atom set rule f ia For rule R S M deﬁne new atom in_hR Consider task T b cid3Bb Sb atom appended body cid4cid4 components task follows appendR rule R M cid3E E cid17 append f 1R in_hR append fnR in_hR R S M cid14 cid14 cid14 cid14 cid14 cid14 Bb f 1R fnR Sb M in_hR E E fiinc fiexc cid10 cid11 R B cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14R S M cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 ei E ei cid3einc eexc einc ei E ei cid3einc eexc exc eexc cid4 cid4 For solution H T b deﬁne gH R in_hR H We I L P smT sm gH Assume H I L P smT sm H S M ei E A A SB H A extends ei H S M ei E A A S f iB H A extends cid3 f iinc einc H S M A A S f iB H 1 n A extends cid3E E f iexc exc eexc cid4 atoms sub program cid4 cid20 H cid20 I L P bT b disjoint H S M A A SBb in_hR R H A extends cid3E E cid4 splitting set theorem in_hR R H splitting set cid20 Sb M gH cid20 I L P bT b gH cid20 H A A SBb H H H H gH H H I L P smT sm gH I L P bT b deciding veriﬁcation I L P sm reduces polynomially deciding veriﬁcation I L P b Similarly I L P smT sm gH H I L P bT b T sm satisﬁable T b satisﬁable deciding satisﬁability I L P b reduces deciding satisﬁability I L P sm cid2 cid20 A extends cid3E cid20 I L P bT b cid20 H cid20 H E cid4 Proposition 2 1 Deciding veriﬁcation satisﬁability I L P c reduces polynomially corresponding I L P L A S decision problem 2 Deciding veriﬁcation satisﬁability I L P L A S reduces polynomially corresponding I L P context L O A S decision problem M Law et al Artiﬁcial Intelligence 259 2018 110146 137 3 Deciding veriﬁcation satisﬁability I L P context L O A S 4 Deciding veriﬁcation satisﬁability I L P L O A S reduces polynomially corresponding I L P s reduces polynomially corresponding I L P L O A S decision problem L A S decision problem Proof 1 Let T c I L P c task cid3B S M cid3E E cid4cid4 E cid4 e cid3e cid4 e Consider I L P L A S task T L A S cid3B S M cid3cid3 cid4 cid3 e By deﬁnition I L P L A S H I L P L A S H S M A A SB H A extends cid3 cid4 e cid4 cid3 A A SB H A extends cid3 e A A SB H This true H S M B H satisﬁable e A This deﬁnition H member I L P cT c I L P cT c I L P L A S T L A S e As I L P cT c I L P L A S T L A S T c satisﬁable T L A S satisﬁable Hence deciding satisﬁability I L P c task reduced deciding satisﬁability I L P L A S task polynomial time Similarly hypothesis H H I L P cT c H I L P L A S T L A S deciding veriﬁcation I L P c reduces polynomially deciding veriﬁcation I L P L A S cid3 A A SB H A extends cid3e A A SB H e cid4 ﬁnally e A e E E E E E cid4cid4 2 Let T L A S I L P L A S task cid3B S M cid3E E cid4cid4 Consider I L P L O A S task T L O A S cid3B S M cid3E E cid4cid4 I L P L A S T L A S I L P L O A S T L O A S T L A S satisﬁable T L O A S satisﬁable Hence deciding satisﬁability I L P L A S reduces polynomially deciding satisﬁability I L P L O A S Similarly hypothesis H H I L P L A S T L A S H I L P L O A S T L O A S deciding veriﬁcation I L P L A S reduces polynomially deciding veriﬁcation I L P L O A S 3 In 24 presented mapping I L P context L O A S task I L P L O A S task The correctness mapping proven Theorem 1 24 Given I L P context task decide satisﬁability mapping checking L O A S satisﬁability resulting I L P L O A S task Similarly given hypothesis I L P context task verify L O A S hypothesis inductive solution task mapping Hence satisﬁability veriﬁcation I L P context L O A S reduce satisﬁability veriﬁcation respectively I L P L O A S E 4 We translating arbitrary I L P L O A S task T L O A S cid3B S M cid3E deﬁne new atoms meta representation For 1 2 let f function maps atom B S M new atom ai We extend notation work sets atoms rules parts rules replacing atom set rule f ia For rule R S M deﬁne new atom in_h R For weak constraint W B S M let id1W id2W new propositional atoms let wtW weight W priorit yW priority level W For terms t1 t2 dominatest1 t2 deﬁned L A S task Before O b O ccid4cid4 I L P s dominatest1 t2 dom_lvt1 t2 l sumid1W1 wtW1 id 1Wn wtWn id2W1 wtW1 id 2Wn wtWn 0 non_dom_lvt1 t2 l sumid1W1 wtW1 id 1Wn wtWn id2W1 wtW1 id 2Wn wtWn 0 domt1 t2 dom_lvt1 t2 l non_beft1 t2 l cid15 l priority level B S M W 1 W n weak constraints B S M level l cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid16 cid14 cid14 cid14 cid14 L A S cid20 S cid3B non_beft1 t2 l1 non_dom_lvt1 t2 l2 l1 l2 levels B S M l1 l2 cid20cid4cid4 individual components deﬁned For positive Consider task T s cid20 negative examples simple reiﬁcation examples relate new B M The brave orderings mapped positive examples covered hypothesis H B H bravely respects ordering cid20 represent answer sets M f 1 f 2 single answer set B example For hypothesis H B H H hypothesis S M corresponding H Similarly cautious orderings mapped negative cid20 examples answer set B extends example pair answer sets corresponding B H ordered incorrectly B H cautiously respect ordering cid20 M cid3E S cid20 H cid20 H cid20 E cid20 S cid20 cid20 cid20 cid20 f iRR B R weak constraint 1 2 B idiW f ibodyW W weak constraint B 1 2 append f iR in_h R R S M R weak constraint idiW append f ibodyW in_hW W weak constraint S M 1 2 138 M Law et al Artiﬁcial Intelligence 259 2018 110146 dominates1 2 dominates2 1 dom dom1 2 dom dom2 1 in_h R R S M cid14 cid14e E cid10 cid11 f 1e cid20 M cid20 S E cid18 cid20 E cid18 cid18 cid18 cid18 cid18 cid10 cid18 cid18 cid18 cid18 cid18 cid18 cid3 f 1einc 1 f 2einc 2 dom1 2 f 1eexc cid3 f 1einc 1 f 2einc 2 f 1eexc 1 f 2eexc cid3 f 1einc 1 f 2einc 2 dom f 1eexc 1 f 2eexc cid3 f 1einc 1 f 2einc 2 dom2 1 f 1eexc cid3 f 1einc 1 f 2einc 2 f 1eexc 1 f 2eexc 1 f 2einc cid3 f 1einc 2 f 1eexc cid14 cid11 cid14e E f 1e 1 f 2eexc cid3 f 1einc 1 f 2einc 2 f 1eexc 1 f 2eexc cid3 f 1einc 1 f 2einc 2 dom2 1 f 1eexc cid3 f 1einc 1 f 2einc 2 f 1eexc 1 f 2eexc cid3 f 1einc 1 f 2einc 2 f 1eexc 1 f 2eexc 2 cid4 1 f 2eexc cid14 cid14 cid14cid3cid3einc cid14 cid14 cid14cid3cid3einc 2 dom2 1cid4 cid14 cid14 cid14cid3cid3einc 1 eexc cid14 cid14 cid14cid3cid3einc cid14 cid14 cid14cid3cid3einc 2 dom1 2cid4 1 f 2eexc 2 cid4 2 cid4 1 cid14 cid14 cid14cid3cid3einc 2 domcid4 2 cid4 cid14 cid14 2 dom1 2cid4 cid14cid3cid3einc cid14 cid14 cid14cid3cid3einc 1 f 2eexc cid14 cid14 cid14cid3cid3einc 2 domcid4 1 eexc cid14 cid14 cid14cid3cid3einc 2 dom2 1cid4 cid14 cid14 cid14cid3cid3einc 1 f 2eexc 2 cid4 1 cid3 f 1einc 1 f 2einc 2 dom1 2 f 1eexc cid3 f 1einc 1 f 2einc 2 dom f 1eexc 1 f 2eexc 2 cid4 1 eexc 1 1 eexc 1 cid4 cid3einc 2 eexc 2 cid4 cid3einc 2 eexc 2 cid4 cid4 O b cid4 cid4 O b cid19 cid4 cid3einc 2 eexc 2 cid4 cid17cid4 O b 1 eexc 1 1 eexc 1 cid4 cid3einc 2 eexc 2 cid4 cid3einc 2 eexc 2 cid4 cid4 O b cid4 cid4 O b cid19 1 eexc 1 cid4 cid3einc 2 eexc 2 cid4 cid4 O b 1 eexc 1 1 eexc 1 cid4 cid3einc 2 eexc 2 cid4 cid3einc 2 eexc 2 cid4 cid4 O c cid4 cid4 O c cid19 cid4 cid3einc 2 eexc 2 cid4 cid17cid4 O c 1 eexc 1 1 eexc 1 cid4 cid3einc 2 eexc 2 cid4 cid3einc 2 eexc 2 cid4 cid4 O c cid4 cid4 O c cid19 cid4 cid3einc 2 eexc 2 cid4 cid4 O c cid14 cid14 cid14cid3cid3einc 1 1 eexc cid20 S cid20 M cid19 cid19 cid19 cid19 cid19 cid19 cid19 cid19 By splitting set theorem 35 shown H A SB cid20 H cid20 A A S A cid14 cid14 cid14 cid14 cid14 cid14 f 1 A1 f 2 A2 dominates1 2 dominates2 1 dom dom1 2 dom dom2 1 A1 A2 A SB H cid20 cid20 cid20 cid10 A cid20 cid20 cid20 S cid20 H cid20 H A correspond mapped example E A augmented dom dom1 2 cid20 M let H corresponding hypothesis S M The answer sets B ensuring pairs answer sets Note answer set B H ﬁrst element pairs Hence rules dominatest1 t2 exactly behaviour weak constraints B H answer sets domt1 t2 true ﬁrst answer set dominates second cid14 cid11 cid14 A f 1 A1 f 2 A2 A1 A2 A SB H A SB A1 dominates A2 dom dom2 1 A2 dominates A1 For hypothesis H pairs answer sets B H E Each positive example e ﬁrst answer set covers e true B H covers positive example E Similarly negative example e mapped example E sets ﬁrst answer set covers e Each brave ordering example cid3e1 e2 opcid4 O b mapped positive example ensuring pair answer sets cid3 A1 A2cid4 B H A1 covers e1 A2 covers e2 cid3 A1 A2 opcid4 ordB H This true B H bravely respects ordering example Each cautious ordering example cid3e1 e2 opcid4 O c mapped negative example ensuring pair answer sets cid3 A1 A2cid4 B H A1 covers e1 A2 covers e2 cid3 A1 A2 opcid4 ordB H This true B H cautiously respects ordering example cid20 S Hence H I L P L O A S cid3B S M cid3E This means check satisﬁability I L P L O A S task similarly verify solution mapping task I L P s As mapping polynomial size original task means veriﬁcation satisﬁability I L P L O A S reduces polynomially corresponding decision problem I L P s This true B H cover negative examples inductive solution I L P s O b O ccid4cid4 cid20 cid4cid4 H inductive solution L A S task Note deﬁned I L P s L A S task B contains stratiﬁed aggregates ensuring pairs answer L A S cid3B cid20 M cid3E E E cid20 cid20 cid20 L A S cid2 Proposition 3 Verifying given H inductive solution general I L P b task N P complete M Law et al Artiﬁcial Intelligence 259 2018 110146 139 e e cid4cid4 For H S M H I L P bT b B H e Proof Let T b I L P b task cid3B S M cid3E satisﬁable As deciding satisﬁability program N P complete B H contains E e normal rules choice rules constraints program constructed polynomial time means deciding veriﬁcation I L P b N P E E It remains deciding veriﬁcation N P hard We showing deciding satisﬁability ASP program P containing normal rules choice rules constraints reduced polynomially deciding veriﬁcation I L P b task Consider I L P b task T b cid3P cid3 cid4cid4 Let H H I L P bT b answer set P H P satisﬁable cid2 Proposition 4 Deciding satisﬁability general I L P b task N P complete E Proof First deciding satisﬁability general I L P b task N P We mapping arbitrary cid4cid4 ASP program answer sets mapped solutions T This program task T cid3B S M cid3E satisﬁable T satisﬁable program aggregate stratiﬁed checking program satisﬁable N P Hence construct program proved deciding satisﬁability I L P b N P For R S M deﬁne new atom in_hRi Also let metaR rule R additional atom in_hRi added body We deﬁne meta encoding Tmeta follows Tmeta B metaR R S M 0in_hR1 _hRSM SM e e E e e E For answer set A let M1 A R R S M in_hRi A A A STmeta Ain_hRi R S M A SB M1 A e e E seen splitting set theorem in_hRi R S M splitting set e e E This Hence A A STmeta H S M H M1 A Ain_hRi R S M A SB H A respects Hence Tmeta satisﬁable H S M A A SB H A respects examples This examples case T satisﬁable It remains deciding satisﬁability general I L P b task N P hard Deciding satisﬁability normal logic program N P hard demonstrating deciding satisﬁability normal program P mapped I L P b task suﬃcient Let P normal logic program Let T I L P b task cid3P cid3 cid4cid4 T satisﬁable H A A SP H A A This true P satisﬁable Hence deciding satisﬁability general I L P b task N P complete cid2 Proposition 5 Deciding veriﬁcation I L P s L A S member D P Proof Checking H inductive solution I L P s T aggregate stratiﬁed ASP programs P P cautiously atom P L A S task T cid3B S M cid3E E H I L P L A S T P cid4cid4 achieved mapping bravely entails atom 1 Let n integer E For integer 1 n let f function mapping atoms B H new atoms ai We extend notation allow f act ASP programs substituting atoms program Let P program coveredi fieinc f ieinc m f ieexc 1 o fieexc 1 cid16cid14 cid14 cid14 cid14ei cid3einc 1 einc m eexc 1 eexc o cid4 E cid16 cid15 cid15 f iB H split n sub programs P 1 P n program P contains rules containing atoms generated P f plus rule coveredi As atoms sub program disjoint atoms subprograms A SP A SP 1 An A SP n This follows applying splitting set theorem n 1 times For 1 n P b coveredi A B H A extends ei ei tive example Hence P examples covered Therefore checking positive examples covered N P Corollary 1 As checking H S M polynomial time means checking H S M positive examples covered N P ith posi covered covered1 coveredn b covered positive A1 An A1 140 M Law et al Artiﬁcial Intelligence 259 2018 110146 2 Let P program cid17 cid15 B H covered neg_violated neg_violated einc e o 1 e exc eexc m 1 c covered cid3 A A SB H e P negative examples covered coN P Lemma 1 cid14 cid14 cid14 cid14cid3einc 1 einc m eexc 1 eexc o cid16 cid4 E E A extends e Hence checking Hence H I L P s L A S T H S M positive examples covered negative examples L A S T reduced checking problem N P problem coN P This covered verifying H I L P s means verifying hypothesis solution I L P s L A S task D P cid2 Proposition 6 Deciding veriﬁcation I L P c D P hard Proof To prove veriﬁcation hypothesis solution I L P c task D P hard prove problem D P reduced veriﬁcation task Let D arbitrary decision problem D P By deﬁnition D P case exist decision problems D1 D2 D1 N P D2 coN P D returns yes D1 D2 return yes By Lemma 1 Corollary 1 case programs P 1 P 2 atoms a1 a2 P 1 b a1 P 2 c a2 D returns yes Without loss generality assume atoms P 1 a1 disjoint atoms P 2 a2 E cid4cid4 individual components task deﬁned follows Take T c I L P c task cid3B S M cid3E B P 1 appendP 2 a3 a1 0a31 a2 a3 assume a3 new atom appendP add atom body rules P S M E E a2 I L P cT c P 1 a1 satisﬁable appendP 2 a3 0a31 a2 a3 c a2 This case subprograms P 1 a1 appendP 2 a3 0a31 a2 a3 disjoint guaranteed satisﬁable answer set a2 Hence I L P cT c P 1 b a1 P 2 c a2 But case D returns yes Hence problem D P reduced verifying hypothesis inductive solution I L P c task Hence veriﬁcation I L P c D P hard cid2 Proposition 7 Deciding satisﬁability I L P s L A S cid2P 2 Proof Given I L P s N P oracle check satisﬁability T polynomial time L A S task T cid3B S M cid3E E cid4cid4 nondeterministic Turing Machine access A nondeterministic Turing Machine S M choices corresponding selecting rule hypothesis This hypothesis veriﬁed polynomial time N P oracle queries similar N P query complement coN P query Proposition 5 answering yes ﬁrst query returned yes second query returned Such Turing Machine terminate answering yes task satisﬁable path Turing Machine answers yes hypothesis S M inductive solution task Hence deciding existence solution I L P s L A S task cid2P 2 cid2 Proposition 8 Deciding satisﬁability I L P c cid2P 2 hard Proof We reducing known cid2P disjunctive logic program 55 I L P c task 2 complete problem deciding existence answer set ground Take ground disjunctive logic program P We deﬁne I L P c task T P solution P answer set Let Atoms set atoms P Let P program constructed replacing negative literal literal in_asa in_as new predicate replacing head h1 hm counting aggregate 1h1 hmm heads mapped 10 equivalent cid20 M Law et al Artiﬁcial Intelligence 259 2018 110146 141 We deﬁne learning task T P follows not_minimal new atom cid15 B P cid20 in_asa not_minimal in_asa S M in_asa Atoms not_minimal E E cid16 cid14 cid14 cid14 cid14a Atoms This task solution exists H S M B H satisﬁable answer set B H contains not_minimal H S M st A A S 1h1 h mm b1 bn cid14 cid14 cid14 cid14 cid14 cid14 1h1 h mm b1 b n in_asc1 _asco in_asc1 in_asco H P cid20 A in_asa H answer set B H contains not_minimal H S M st A A S 1h1 h mm b1 b n cid14 cid14 cid14 cid14 cid14 cid14 1h1 hmm b1 b n in_asc1 _asco in_asc1 in_asco H P cid20 A in_asa H strict subset A answer set answer set B H contains not_minimal H S M st in_asa H minimal model h1 hm b1 b n 1h1 h mm b1 bn in_asc1 in_asco in_asc1 in_asco H P cid20 H S M st in_asa H minimal model h1 hm b1 b n h1 hm b1 bn c1 c o P in_asc1 in_asco H A Atoms st A minimal model h1 hm b1 b n h1 hm b1 bn c1 c o c1 A P cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 cid14 A Atoms A minimal model P A A Atoms A answer set P P satisﬁable Hence deciding disjunctive logic program satisﬁable general mapped decision problem checking existence solutions learning answer sets task Therefore deciding existence solutions ground I L P c task cid2P 2 hard cid2 A2 Proofs Section 5 Proposition 12 For programs P 1 P 2 EbP 1 EbP 2 A SP 1 A SP 2 Proof Assume A SP 1 A SP 2 A A SP 1 A A SP 2 Assume c i1 im e1 e n EbP 1 Then answer set A P 1 contains es Hence answer set P 2 contains es Hence c EbP 2 142 M Law et al Artiﬁcial Intelligence 259 2018 110146 Conversely assume EbP 1 EbP 2 Let A A SP 1 A A SP 2 H B P 2 Let L set H B P 1 As A A SP 1 c i1 im e1 e n EbP 1 set atoms A es set atoms L A As c EbP 1 c EbP 2 answer set A P 2 contains A atom e L A H B P 2 cid10 cid20 A Hence A A SP 2 cid2 L A cid20 Proposition 13 D1 1I L P b cid14 cid11 cid14 A SB H1 cid5 A SB H2 cid3B H1 H2cid4 Proof We prove showing D1 cid14 cid11 cid10 cid14 A SB H1 cid5 A SB H2 cid3B H1 H2cid4 1I L P b Proposition 12 cid10 cid14 cid11 cid14EbB H1 cid5 EbB H2 cid3B H1 H2cid4 equal set We ﬁrst cid3B H1 H2cid4 D1 1I L P b conjunction EbB H1 EbB H2 1I L P b I L P b task T cid3B cid3i1 im e1 encid4cid4 H1 I L P bT H2 As cid3B H1 H2cid4 D1 I L P bT Hence answer set B H 1 i1 im A e1 em A answer set B H2 Hence conjunction c i1 im e1 en EbB H1 c EbB H2 Next exists conjunction c i1 im e1 en c EbB H1 c EbB H2 cid3B H1 H2cid4 D1 Assume conjunction c Then B H 1 answer set extends cid3i1 im e1 encid4 B H2 Hence H1 I L P bcid3B cid3i1 im e1 encid4cid4 H2 I L P bcid3B cid3i1 im e1 encid4cid4 So cid3B H1 H2cid4 D1 1I L P b 1I L P b cid2 Proposition 14 D1 1I L P b D1 1I L P sm Proof First D1 1I L P b D1 1I L P sm Assume cid3B H1 H2cid4 D1 H1 I L P bT b H2 I L P bT b Let T sm cid3B cid3E cid3B H1 H2cid4 D1 1I L P sm 1I L P b Then task T b cid3B cid3E cid4cid4 cid4cid4 H1 I L P smT sm H2 I L P smT sm Hence E E 1I L P b D1 Next D1 n cid3 E cid3E E swer set B H2 Hence letting T b cid3B cid3E cid4 cid4cid4 H1 I L P smT sm H2 I L P smT sm There partial interpretation A 1I L P b cid2 cid4 answer set A B H 1 E cid4cid4 H1 I L P bT b H2 I L P bT b So cid3B H1 H2cid4 D1 1I L P sm There task T sm cid3B cid3E 1I L P sm Assume cid3B H1 H2cid4 D1 A E n E E 1 E 1 Proposition 15 D1 1I L P c cid15 cid14 cid14 cid14 cid3B H1 H2cid4 cid14 A SB H1 cid17 A SB H2 EcB H2 cid5 EcB H1 cid16 Proof First cid3B H 1 H2cid4 D1 1I L P c A SB H1 cid17 A SB H2 EcB H1 cid5 EcB H2 Let cid3B H1 H2cid4 arbitrary element D1 EcB H2 We A SB H2 As cid3B H1 H2cid4 D1 H2 I L P cT c EcB As H1 I L P cT c A A SB H1 E H1 initial assumption EcB H1 EcB H2 conjunction EcB H2 A A SB H2 E 1I L P c As H1 I L P cT c A SB H1 cid17 Assume EcB H1 cid4cid4 H1 I L P cT c A But H2 I L P cT c means A SB H2 A cid17 conjunction E 1I L P c T c cid3B cid3E A E A E e E e E We B H 1 H2 A SB H1 cid17 A SB H2 EcB H1 cid5 EcB H2 cid3B H1 H2cid4 D1 Case 1 A SB H1 cid17 A SB H2 1I L P c Consider task T c cid3B cid3 cid4cid4 H1 I L P cT c A SB H1 cid17 A A SB H1 A A H2 I L P cT c A SB H2 Case 2 A SB H1 cid17 c i1 im e1 en EcB H1 c EcB H2 Consider task T c cid3B cid3i1 im e1 encid4cid4 H1 I L P cT c H2 I L P cT c cid2 Proposition 16 D1 1I L P L A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 Proof M Law et al Artiﬁcial Intelligence 259 2018 110146 143 We ﬁrst D1 I L P L A S task T cid3B cid3E Case 1 e E As H1 I L P L A S T A H1 cid17 A SB H2 As H1 I L P L A S T A H1 cid17 A SB H2 It remains D1 1I L P L A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 For cid3B H1 H2cid4 D1 E cid4cid4 H1 I L P L A S T H2 I L P L A S T 1I L P L A S A A SB H2 A extend e cid20 A SB H1 A cid20 extends e Hence A cid20 A SB H2 A SB Case 2 e E A A SB H2 A extends e cid20 A SB H1 A cid20 extend e Hence A A SB H 1 A SB 1I L P L A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 Take B H1 H2 ASP programs A SB H1 cid17 A SB H2 Let L set atoms appear answer sets B H 1 B H2 Case 1 A A SB H1 A A SB H2 Let e A cid3 A L Acid4 A interpretation A SB H 1 A SB H2 extends e A e A completely deﬁned atoms L Hence answer set B H 1 extends e A answer set B H2 Hence H1 I L P L A S cid3B cid3e A cid4cid4 H2 I L P L A S cid3B cid3e A cid4cid4 So cid3B H1 H2cid4 D1 1I L P L A S Case 2 A A SB H2 A A SB H1 Let e A cid3 A L Acid4 A interpretation A SB H 1 A SB H2 extends e A Hence answer set B H1 extends e A answer set B H 2 Hence H1 I L P L A S cid3B cid3 e Acid4cid4 H2 I L P L A S cid3B cid3 e Acid4cid4 So cid3B H1 H2cid4 D1 1I L P L A S cid2 Proposition 17 D1 1I L P L O A S cid15 cid14 cid14 cid14 cid3B H1 H2cid4 cid14 A SB H1 cid17 A SB H2 ordB H1 cid17 ordB H2 cid16 Proof cid3B H1 H2cid4 D1 H2 I L P L O A S T Case 1 e E As H1 I L P L O A S T A H1 cid17 A SB H2 As H1 I L P L O A S T A H1 cid17 A SB H2 We ﬁrst D1 1I L P L O A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 ordB H1 cid17 ordB H2 For O b O ccid4cid4 H1 I L P L O A S T task T cid3B cid3E 1I L P L O A S I L P L O A S E A A SB H2 A extend e cid20 A SB H1 A cid20 extends e Hence A cid20 A SB H2 A SB Case 2 e E A A SB H2 A extends e cid20 A SB H1 A cid20 extend e Hence A A SB H 1 A SB Case 3 cid3e1 e2 opcid4 O b covered H1 H2 Assume A SB H1 A SB H2 A1 A2 A SB H1 A1 extends e1 A2 extends e2 cid3 A1 A2 opcid4 ordB H1 H1 covers ordering example cid3 A1 A2 opcid4 ordB H2 H2 cover ordering example ordB H 1 cid17 ordB H2 Case 4 cid3e1 e2 opcid4 O c covered H1 H2 Assume A SB H1 A SB H2 A1 A2 A SB H2 A1 extends e1 A2 extends e2 cid3 A1 A2 opcid4 ordB H2 H2 cover ordering example cid3 A1 A2 opcid4 ordB H1 H1 cover ordering example ordB H 1 cid17 ordB H2 Hence cases A SB H 1 cid17 A SB H2 ordB H1 cid17 ordB H2 It remains D1 1I L P L O A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 ordB H1 cid17 ordB H2 Take B H1 H2 ASP programs A SB H 1 cid17 A SB H2 ordB H1 cid17 ordB H2 Let L set literals appear answer sets B H 1 B H2 Case 1 A SB H1 cid17 A SB H2 cid3B H1 H2cid4 D1 H1 I L P L A S T L A S H2 I L P L A S T L A S Let T L O A S cid3B cid3E I L P L O A S T L O A S 1I L P L A S Proposition 16 Hence I L P L A S task T L A S cid3B cid3E E cid4cid4 cid4cid4 H1 I L P L O A S T L O A S H2 E Case 2 A SB H1 A SB H2 ordB H1 cid17 ordB H2 A1 A2 A SB H1 equal A SB H2 binary operator op cid3 A1 A2 opcid4 ordB H1 cid3 A1 A2 opcid4 ordB H26 Let e1 cid3 A1 L A1cid4 e2 cid3 A2 L A2cid4 L 6 Note need consider case tuple cid3 A1 A2 opcid4 ordB H2 cid3 A1 A2 opcid4 ordB H1 case cid3 A1 A2 op 1cid4 ordB H2 1 1 1 cid17 1 1 cid171 1cid4 ordB H1 cid3 A1 A2 op 144 M Law et al Artiﬁcial Intelligence 259 2018 110146 set atoms answer sets B H 1 Consider I L P L O A S task T L O A S cid3B cid3e1 e2 cid3e1 e2 opcid4 cid4cid4 H1 I L P L O A S T L O A S H2 I L P L O A S T L O A S Hence cases cid3B H1 H2cid4 D1 1I L P L O A S cid2 Proposition 18 D1 1I L P context L O A S cid15 cid14 cid14 cid14 cid3B H1 H2cid4 cid14 B H1 cid17s B H2 C ASP ch ordB H1 C cid17 ordB H2 C cid16 Proof We ﬁrst D1 1I L P context L O A S L O A S I L P context L O A S 1I L P context cid14 cid15 cid14 cid14 cid3B H1 H2cid4 cid14 B H1 cid17s B H2 C ASP ch st ordB H1 C cid17 ordB H2 C task T cid3B cid3E E O b O ccid4cid4 H1 I L P context L O A S T cid16 For cid3B H1 H2cid4 D1 H2 I L P context L O A S T Case 1 cid3e Ccid4 E A A SB H2 C A extend e cid20 A SB H1 C A L O A S T A As H1 I L P context A SB H1 C cid17 A SB H2 C Hence B H1 cid17s B H2 cid20 Case 2 cid3e Ccid4 E A A SB H2 C A extends e cid20 A SB H1 C A L O A S T A As H1 I L P context A SB H1 C cid17 A SB H2 C Hence B H1 cid17s B H2 Case 3 cid3cid3e1 C1cid4 cid3e2 C2cid4 opcid4 O b covered H1 H2 cid20 extends e Hence A cid20 A SB H2 C extend e Hence A A SB H 1 C Assume B H1 s B H2 Let S set A SB H1 C1 A SB H1 C2 equal set A SB H 2 C1 A SB H2 C2 B H1 s B H2 A1 A SB H1 C1 A2 A SB H1 C2 A1 extends e1 A2 extends e2 cid3 A1 A2 opcid4 ordB H1 S H1 covers ordering example cid3 A1 A2 opcid4 ordB H2 S H2 cover ordering example Let C ASP ch program appendC1 a1 appendC2 a2 1a1 a21 a1 a2 new atoms appendP appends atom body rule P A SB H 1 C A a1 A A SB H1 C1 A a2 A A SB H1 C2 t cid3 A1 a1 A2 a2 opcid4 ordB H1 C t ordB H2 C Hence C ASP ch ordB H1 C cid17 ordB H2 C Case 4 cid3e1 e2 opcid4 O c covered H1 H2 Assume B H1 s B H2 Let S set A SB H1 C1 A SB H1 C2 equal set A SB H 2 C1 A SB H2 C2 B H1 s B H2 A1 A SB H1 C1 A2 A SB H1 C2 A1 extends e1 A2 extends e2 cid3 A1 A2 opcid4 ordB H2 S H2 cover ordering example cid3 A1 A2 opcid4 ordB H1 S H1 cover ordering example Let C ASP ch program appendC1 a1 appendC2 a2 1a1 a21 a1 a2 new atoms appendP appends atom body rule P A SB H 1 C A a1 A A SB H1 C1 A a2 A A SB H1 C2 t cid3 A1 a1 A2 a2 opcid4 ordB H1 C t ordB H2 C Hence C ASP ch ordB H1 C cid17 ordB H2 C Hence cases B H 1 s B H2 C ASP ch ordB H1 C cid17 ordB H2 C It remains D1 1I L P L O A S cid3B H1 H2cid4 A SB H1 cid17 A SB H2 ordB H1 cid17 ordB H2 Take B H1 H2 ASP programs A SB H 1 cid17 A SB H2 ordB H1 cid17 ordB H2 Case 1 B H1 cid17s B H2 There program C A SB H 1 C cid17 A SB H2 C Case A A SB H1 C A A SB H2 C Let L set atoms answer sets B H 1 C B H2 C let e A partial interpretation cid3 A L Acid4 Then B H 1 C answer set extends e A B H2 C H1 I L P context L O A S cid3B S M cid3cid3e A Ccid4 cid4cid4 H2 Case ii A A SB H2 C A A SB H1 C Let L set atoms answer sets B H 1 C B H2 C let e A partial interpretation cid3 A L Acid4 Then B H 2 C answer set extends e A B H1 C H1 I L P context L O A S cid3B S M cid3 cid3e A Ccid4 cid4cid4 H2 Case 2 B H1 s B H2 C ASP ch ordB H1 C cid17 ordB H2 C A1 A2 A SB H1 C equal A SB H2 C binary operator op cid3 A1 A2 opcid4 ordB H1 C cid3 A1 A2 opcid4 ordB H2 C Let e1 cid3 A1 L A1cid4 e2 cid3 A2 L A2cid4 L set atoms answer sets B H 1 C Consider I L P context L O A S cid3B cid3e1 e2 cid3cid3e1 Ccid4 cid3e2 Ccid4 opcid4 cid4cid4 H1 I L P context L O A S L O A S H2 I L P context L O A S T context L O A S T context task T context L O A S Hence cases cid3B H1 H2cid4 D1 1I L P context L O A S cid2 Proposition 19 For learning framework F D1 mF cid14 cid10 cid14cid3B H H1cid4 cid3 B H Hncid4 D1 cid3B H H1 Hncid4 cid11 1F M Law et al Artiﬁcial Intelligence 259 2018 110146 145 Proof We ﬁrst cid3B H Scid4 D1 need H Take arbitrary H subset S Hence H cid20 S H cid20 cid3B H H We D1 cid20 S H1 Hncid4 cid3B H H icid4 D1 mF mF We cid3B H Scid4 cid14 cid11 cid14cid3B H H1cid4 cid3 B H Hncid4 D1 1F cid3B H H1 Hncid4 cid14 cid14cid3B H H1cid4 cid3B H Hncid4 D1 cid3B H H1 Hncid4 D1 Take arbitrary cid11 1F To cid10 cid10 cid20 S cid3B H H cid20cid4 D1 1F cid20 S It remains cid3B H H cid20cid4 D1 1F By deﬁnition D1 cid20cid4 D1 cid20 S cid20 cid3B H S cid20cid4 D1 1F cid10 mF cid14 cid10 cid14cid3B H H1cid4 cid3 B H Hncid4 D1 cid3B H H1 Hncid4 cid14 cid14cid3B H H1cid4 cid3B H Hncid4 D1 cid3B H H1 Hncid4 cid11 1F mF cid3B H H1 Hncid4 D1 mF Hence deﬁnition D1 cid11 1F mF Hence T F H I L P F T F S Take arbitrary cid3B H 1F For 1n cid3B H H icid4 D1 mF cid2 mF cid20 I L P F T F Proposition 20 I L P c I L P sm I L P L A S I L P L O A S I L P context L O A S closed onetomanydistinguishability Proof 1 Consider I L P c tasks T 1 c H I L P cT 1 E E 2 Hence Lemma 2 I L P c closed onetomanydistinguishability A This case E cid3B cid3E 1 c A SB H nonempty A A SB H E 1 2 A E E 2 E A A E 2 2 A holds H I L P cT 3 c E 1 2 A E c I L P cT 2 cid4cid4 T 2 c cid4cid4 Let T 3 c cid3B cid3E cid3B cid3E 2 E 1 E cid4cid4 1 E 2 1 1 1 2 For tasks T 1 sm 1 e1 n cid4 T 2 sm cid3B e2 1 e2 m cid4 let T 3 sm cid3B e1 1 e1 n e2 1 e2 m cid4 I L P smT 3 sm cid3B e1 sm cid3B cid3E L A S sm I L P smT 2 I L P smT 1 Hence Lemma 2 I L P sm closed onetomanydistinguishability 1 cid4cid4 T 2 3 For tasks T 1 cid3B cid3E 1 E cid4cid4 let T 3 2 E 2 L A S L A S O b L A S I L P L A S T 2 4 For tasks T 1 I L P L A S T 1 Hence Lemma 2 I L P L A S closed onetomanydistinguishability cid3B cid3E cid4cid4 T 2 1 O b 2 E L O A S I L P L O A S T 2 L O A S I L P L O A S T 1 2 O b E Hence Lemma 2 I L P L O A S closed onetomanydistinguishability cid3B cid3E 2 O b 2 O c 1 O b 2 E L O A S I L P context L O A S T 2context L O A S closed onetomanydistinguishability cid2 2 O c 2 O b E 1 Hence Lemma 2 I L P context 1 O c L O A S I L P context L O A S T 3context 5 For tasks T 1context L O A S O c O b 2 cid3B cid3E cid4cid4 I L P L O A S T 3 cid3B cid3E 1 E cid4cid4 I L P context cid4cid4 T 2context L O A S 2 O b L O A S L O A S T 1context L O A S O c 2 2 O c 2 O c 1 O c 1 E L O A S 1 1 1 1 2 1 cid3B cid3E 1 E 2 E 1 E 2 cid4cid4 I L P L A S T 3 L A S L A S cid4cid4 let T 3 L O A S cid3B cid3E 1 E 2 E 1 cid4cid4 let T 3context 2 L O A S L O A S cid3B cid3E 1 E 2 E 1 References 1 S Muggleton Inductive logic programming New Gener Comput 8 4 1991 295318 2 O Ray K Broda A Russo A hybrid abductive inductive proof procedure Log J IGPL 12 5 2004 371397 3 S Muggleton L De Raedt D Poole I Bratko P Flach K Inoue A Srinivasan ILP turns 20 Mach Learn 86 1 2012 323 4 S Muggleton D Lin Metainterpretive learning higherorder dyadic datalog predicate invention revisited Proceedings TwentyThird International Joint Conference Artiﬁcial Intelligence AAAI Press 2013 pp 15511557 5 A Srinivasan The Aleph Manual Machine Learning Computing Laboratory Oxford University 2000 6 H Blockeel L De Raedt Topdown induction ﬁrstorder logical decision trees Artif Intell 101 1 1998 285297 7 D Corapi A Russo E Lupu Inductive logic programming abductive search ICLP Technical Communications 2010 pp 5463 8 C Sakama Nonmonotomic inductive logic programming International Conference Logic Programming Nonmonotonic Reasoning Springer 9 M Gelfond V Lifschitz The stable model semantics logic programming ICLPSLP vol 88 1988 pp 10701080 10 S Kolb Learning constraints optimization criteria Proceedings First Workshop Declarative Learning Based Programming 2016 11 C Jordan Ł Kaiser Machine learning guarantees descriptive complexity smt solvers preprint arXiv1609 02664 12 M Sebag C Rouveirol Constraint inductive logic programming 13 T Eiter G Ianni T Krennwallner Answer set programming primer Reasoning Web Semantic Technologies Information Systems Springer 14 ET Mueller Commonsense Reasoning An Event Calculus Based Approach Morgan Kaufmann 2014 15 M Gelfond Y Kahl Knowledge Representation Reasoning Design Intelligent Agents The AnswerSet Programming Approach Cambridge 2001 pp 6280 2009 pp 40110 University Press 2014 16 E Erdem M Gelfond N Leone Applications answer set programming AI Mag 37 3 2016 5368 17 M Nogueira M Balduccini M Gelfond R Watson M Barry An aprolog decision support space shuttle International Symposium Practical Aspects Declarative Languages Springer 2001 pp 169183 18 F Ricca A Dimasi G Grasso SM Ielpa S Iiritano M Manna N Leone A logicbased etourism Fundam Inform 105 12 2010 3555 19 T Soininen I Niemelä Developing declarative rule language applications product conﬁguration International Symposium Practical Aspects Declarative Languages Springer 1999 pp 305319 20 R Kowalski M Sergot A logicbased calculus events New Gener Comput 4 1 1986 6795 146 M Law et al Artiﬁcial Intelligence 259 2018 110146 21 N Katzouris A Artikis G Paliouras Incremental learning event deﬁnitions inductive logic programming J Mach Learn Res 100 23 2015 555585 22 D Athakravi Inductive Logic Programming Using Bounded Hypothesis Space PhD thesis Imperial College London 2015 23 M Law A Russo K Broda Learning weak constraints answer set programming Theory Pract Log Program 15 45 2015 511525 24 M Law A Russo K Broda Iterative learning answer set programs context dependent examples Theory Pract Log Program 16 56 2016 834848 25 L De Raedt A Kimmig H Toivonen Problog probabilistic prolog application link discovery IJCAI vol 7 2007 pp 24622467 26 F Riguzzi E Bellodi R Zese A history probabilistic inductive logic programming Front Robot AI 1 2014 6 27 M Nickles PrASP report preprint arXiv1612 09591 28 C Sakama K Inoue Brave induction logical framework learning incomplete information J Mach Learn Res 76 1 2009 335 29 M Alviano W Faber N Leone S Perri G Pfeifer G Terracina The disjunctive datalog DLV Datalog Reloaded Springer 2011 pp 282301 30 RP Otero Induction stable models Inductive Logic Programming Springer 2001 pp 193205 31 M Law A Russo K Broda Inductive learning answer set programs Logics Artiﬁcial Intelligence JELIA 2014 Springer 2014 32 M Law A Russo K Broda The ILASP learning answer set programs httpswwwdoc ic ac uk ml1909 ILASP 2015 33 M Law A Russo K Broda Simpliﬁed Reduct Choice Rules ASP Tech Rep DTR20152 Imperial College Science Technology Medicine Department Computing 2015 34 F Calimeri W Faber M Gebser G Ianni R Kaminski T Krennwallner N Leone F Ricca T Schaub ASPCore2 input language format https wwwmat unical aspcomp2013 ﬁles ASPCORE 2 0 pdf 2013 35 V Lifschitz H Turner Splitting logic program ICLP vol 94 1994 pp 2337 36 CH Papadimitriou Computational Complexity John Wiley Sons Ltd 2003 37 LJ Stockmeyer The polynomialtime hierarchy Theor Comput Sci 3 1 1976 122 38 D Corapi A Russo E Lupu Inductive logic programming answer set programming Inductive Logic Programming Springer 2012 pp 9197 39 O Ray Nonmonotonic abductive inductive learning J Appl Log 7 3 2009 329340 40 S Muggleton Inverse entailment progol New Gener Comput 13 34 1995 245286 41 M Law A Russo K Broda Inductive learning answer set programs v260 42 S Bragaglia O Ray Nonmonotonic learning large biological networks Inductive Logic Programming Springer 2015 pp 3348 43 D Athakravi D Corapi K Broda A Russo Learning hypothesis reﬁnement answer set programming International Conference Inductive Logic Programming Springer 2013 pp 3146 44 W Faber G Pfeifer N Leone Semantics complexity recursive aggregates answer set programming Artif Intell 175 1 2011 278298 45 L De Raedt Logical settings conceptlearning Artif Intell 95 1 1997 187201 46 C Sakama Induction answer sets nonmonotonic logic programs ACM Trans Comput Log 6 2 2005 203231 47 K Inoue T Ribeiro C Sakama Learning interpretation transition J Mach Learn Res 94 1 2014 5179 48 L De Raedt K Kersting Probabilistic inductive logic programming International Conference Algorithmic Learning Theory Springer 2004 pp 1936 49 L De Raedt I Thon Probabilistic rule learning International Conference Inductive Logic Programming Springer 2010 pp 4758 50 E Bellodi F Riguzzi Structure learning probabilistic logic programs searching clause space Theory Pract Log Program 15 02 2015 169212 Artiﬁcial Intelligence 2016 51 F Riguzzi E Bellodi R Zese G Cota E Lamma Scaling structure learning probabilistic logic programs MapReduce European Conference 52 M Nickles A Mileo Probabilistic inductive logic programming based answer set programming preprint arXiv1405 0720 53 M Nickles A Mileo A probabilistic inductive answer set programming International Conference Scalable Uncertainty Management Springer International Publishing 2015 pp 99105 54 S Dragiev A Russo K Broda M Law PROBXHAIL An abductiveinductive algorithm probabilistic inductive logic programming 55 T Eiter G Gottlob On computational cost disjunctive logic programming propositional case Ann Math Artif Intell 15 34 1995 289323