Artiﬁcial Intelligence 302 2022 103602 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Analyzing Differentiable Fuzzy Logic Operators Emile van Krieken Vrije Universiteit Amsterdam Netherlands b Civic AI Lab Amsterdam Netherlands Erman Acar ab Frank van Harmelen r t c l e n f o b s t r c t Article history Received 10 June 2020 Received revised form 27 September 2021 Accepted 28 September 2021 Available online 7 October 2021 Keywords Fuzzy logic Neuralsymbolic AI Learning constraints The AI community increasingly putting attention combining symbolic neural approaches argued strengths weaknesses approaches complementary One recent trend literature weakly supervised learning techniques employ operators fuzzy logics In particular use prior background knowledge described logics help training neural network unlabeled noisy data By interpreting logical symbols neural networks background knowledge added regular loss functions making reasoning learning We study formally empirically large collection logical operators fuzzy logic literature behave differentiable learning setting We ﬁnd operators including wellknown highly unsuitable setting A ﬁnding concerns treatment implication fuzzy logics shows strong imbalance gradients driven antecedent consequent implication Furthermore introduce new family fuzzy implications called sigmoidal implications tackle phenomenon Finally empirically possible use Differentiable Fuzzy Logics semisupervised learning compare different operators behave practice We ﬁnd achieve largest performance improvement supervised baseline resort nonstandard combinations logical operators perform learning longer satisfy usual logical laws 2021 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 1 Introduction In recent years integrating symbolic statistical approaches Artiﬁcial Intelligence AI gained considerable attention 226 This research line gained traction recent inﬂuential critiques purely statistical deep learning 4759 focus AI community decade While deep learning brought important breakthroughs vision 8 natural language processing 61 reinforcement learning 68 concern progress halted shortcomings dealt Among massive amounts data deep learning models need learn simple concept In contrast symbolic AI easily reuse concepts express domain knowledge single logical statement Finally easier integrate background knowledge symbolic AI Corresponding author Email addresses evankriekenvunl E van Krieken ermanacarvunl E Acar FrankvanHarmelenvunl F van Harmelen httpsdoiorg101016jartint2021103602 00043702 2021 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 However symbolic AI issues scalability dealing large amounts data performing complex reasoning task able deal noise ambiguity sensory data The related wellknown symbol grounding problem Harnad 30 deﬁnes semantic interpretation formal symbol intrinsic parasitic meanings heads In particular symbols refer concepts intrinsic meaning humans computers manipulating symbols understand ground meaning On hand properly trained deep learning model excels modeling complex sensory data These models bridge gap symbolic systems real world Therefore recent approaches 1823674620 aim interpreting symbols logicbased systems deep learning models These ﬁrst systems implement hybrid nonsymbolicsymbolic elementary symbols grounded nonsymbolic representations pick proximal sensory projections distal object categories elementary symbols refer Harnad 30 11 Reasoning learning gradient descent We introduce Differentiable Fuzzy Logics DFL aims integrate reasoning learning logical formulas expressing background knowledge The symbols formulas interpreted deep learning model parameters learned DFL constructs differentiable loss functions based formulas minimized gradient descent This ensures deep learning model acts manner consistent background knowledge backpropagate parameters deep learning model To ensure loss functions differentiable DFL uses fuzzy logic semantics 41 Predicates functions constants interpreted deep learning model By maximizing degree truth background knowledge gradient descent learning reasoning performed parallel We apply loss functions constructed DFL challenging machine learning tasks purely supervised learning These methods fall umbrella weakly supervised learning 76 For example semisupervised learning 7432 detect noisy inaccurate supervision 19 For problems DFL corrects predictions deep learning model logically inconsistent background knowledge To understanding losses present paper analysis choice operators com pute logical connectives DFL For example functions called tnorms connect fuzzy propositions 41 Because return degree truth event propositions true tnorms generalize classical conjunction Similarly fuzzy implication generalizes classical implication Most operators differentiable enable use DFL Interestingly derivatives operators determine DFL corrects deep learn ing model predictions inconsistent background knowledge We qualitative properties derivatives integral theory practice DFL We approach problem view symbolic statistical approaches AI bridge conceptual gap views This provides insights overlooked 12 Contributions The main contribution article answer following question What fuzzy logic operators existential quan tiﬁcation universal quantiﬁcation conjunction disjunction implication convenient theoretical properties gradient descent We analyze theoretically empirically effect choice operators compute logical connectives Differentiable Fuzzy Logics learning behavior DFL To end We introduce Differentiable Fuzzy Logics Section 4 combines fuzzy logic gradientbased learning analyze behavior different choices fuzzy logic operators Section 3 We analyze theoretical properties aggregation functions compute universal quantiﬁer existential quantiﬁer tnorms tconorms compute connectives fuzzy implications compute connective We introduce new family fuzzy implications called sigmoidal implications Section 5 insights analyses We perform experiments compare fuzzy logic operators semisupervised experiment Section 9 We recommendations choices operators 2 Differentiable Logics Loss functions realvalued functions represent cost minimized Differentiable Logics DL logics differentiable loss functions constructed compute truth value given formulas semantics logic These logics use background knowledge deduce truth value statements unlabeled poorly labeled data allowing use data learning possibly normal labeled data This beneﬁcial unlabeled poorly labeled partially labeled data cheaper easier come This approach differs Inductive Logic Programming 54 derives formulas data DL instead informs data 2 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 1 In running example image objects o1 o2 We motivate use DL following classiﬁcation scenario consider analysis Assume agent A goal scene image It gets feedback supervisor S exact description images available However S background knowledge base K concepts contained images The intuition Differentiable Logics S correct As descriptions scenes consistent knowledge base K Example 1 We illustrate idea following example Agent A image I Fig 1 contains objects o1 o2 A supervisor S consider unary class predicates chair cushion armRest binary predicate partOf Since S description I correct A based knowledge base K A describes image conﬁdence value 0 1 observation For instance pchairo1 indicates conﬁdence A assigns chairo1 o1 chair pchairo1 09 pcushiono1 005 parmResto1 005 ppartOfo1 o1 0001 ppartOfo1 o2 001 pchairo2 04 pcushiono2 05 parmResto2 01 ppartOfo2 o2 0001 ppartOfo2 o1 095 Suppose K contains following logic formula says parts chair cushions armrests x y chairx partOf y x cushion y armRest y S reason A relatively conﬁdent chairo1 partOfo2 o1 antecedent formula satisﬁed cushiono2 armResto2 hold Since pcushiono2 parmResto2 possible correction tell A increase degree belief cushiono2 We like automate kind supervision S performs previous example To end study Differentiable Fuzzy Logics DFL family Differentiable Logics literature based fuzzy logic Here term Differentiable Logics refers logic translation scheme logical expressions differentiable loss functions Then Differentiable Fuzzy Logics stands case logic fuzzy logic translation scheme applies logical expressions include fuzzy operators Note numerical character fuzzy logic obvious candidate differentiable logic Therefore DFL truth values ground atoms numbers 0 1 logical connectives interpreted function truth values Examples logics family Logic Tensor Networks 67 similarly named Deep Fuzzy Logic 50 logics underlying Semantic Based Regularization 18 LYRICS 48 KALE 26 compare Section 111 This family stands orthogonal wellstudied mathematical classiﬁcation fuzzy logics landscape 12 Instead analysis use variety individual T norms different properties combined variety aggregation functions 3 Background We assume basic familiarity syntax semantics ﬁrstorder logic We shall denote predicates sans serif font cushion set V variables denoted x y z x1 x2 x3 set O domain objects denoted o1 o2 constants b c We limit functionfree formulas prenex normal form start quantiﬁers followed quantiﬁerfree subformula An example formula prenex form x y Px y Qx R y An atom Pt1 tm t1 tm terms If t1 tm constants ground atom 3 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Table 1 The tnorms Name Gödel minimum Product Łukasiewicz Tnorm T G b mina b T P b b T L K b maxa b 1 0 cid2 Drastic product T D b Nilpotent minimum TnM b mina b 0 cid2 1 b 1 b 1 0 mina b Properties idempotent continuous strict continuous leftcontinuous Yager T Y b max1 1 ap 1 bp 1 p 0 p 1 continuous Table 2 The tconorms Name Gödel maximum Product probabilistic sum Łukasiewicz Tconorm S G b maxa b S P b b b S L K b mina b 1 cid2 Properties idempotent continuous strict continuous Drastic sum Nilpotent maximum Yager S D b SnM b maxa b 1 cid2 0 b 0 b 1 1 maxa b 1 p 1 p 1 S Y b minap b p rightcontinuous continuous Table 3 Some common aggregation operators Name Generalizes Minimum Product Łukasiewicz Maximum Probabilistic sum Bounded sum T G T P T L K S G S G S L K Aggregation operator A T G x1 xn minx1 xn cid3 A T P x1 xn n i1 xi cid4 A T L K x1 xn max n E S G x1 xn maxx1 xn cid3 i11 xi E S P x1 xn 1 n cid6 cid5cid4 E S L K x1 xn min n i1 xi 1 i1 xi n 1 0 Fuzzy logic manyvalued logic truth values real numbers 0 1 0 denotes completely false 1 denotes completely true It model reasoning presence vagueness sharp boundaries imprecisely classify concepts tall person small number 2856 We look predicate fuzzy logics particular extend propositional fuzzy logics universal existential quantiﬁcation For brief background fuzzy logic operators Appendix A extensive treatment mathematical fuzzy logic refer reader standard textbooks include 28 56 12 In paper exclusively use strong negation Na 1 Conjunctions b generalized Tnorms function T 0 12 0 1 commutative associative monotonic T 1 In fuzzy logic conjunctions tnorm denoted logical formula level b However semantic level T b b refer truth values corresponding logical formulas b An overview common Tnorms given Table 1 alongside common properties deﬁned Appendix A3 Tconorms generalize disjunctions denoted b fuzzy logic literature They formed tnorm T Sa b 1 T 1 1 b following DeMorgans law b b The common tconorms given Table 2 Universal existential quantiﬁcation generalized increasing aggregation operators A E 0 1n 0 1 A1 1 E1 1 1 A0 0 E0 0 0 10 Universal aggregators constructed tnorm T existential aggregators tconorm S nN cid7 A T 1 E S 0 A T x1 xn T x1 A T x2 xn E S x1 xn Sx1 E S x2 xn 1 2 An overview common aggregation operators given Table 3 A formal introduction Appendix A4 Finally implications generalized fuzzy implications 38 functions I 0 12 0 1 creasing respect ﬁrst argument decreasing respect second Furthermore assume boundary conditions I0 0 I1 1 1 I1 0 0 follows classical implication Fuzzy implications additional properties discuss Appendix A5 We discuss classes fuzzy implications The ﬁrst Simplications Appendix A51 examples Table 4 formed tconorm generalizing material impli 4 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Table 4 Simplications formed c common tconorms Table 2 Name Tconorm Gödel KleeneDienes Product Reichenbach Łukasiewicz DubouisPrade S G S P S L K S D Simplication I K D c max1 c I RC c 1 c I L K c min1 c 1 1 c 1 c 0 1 I D P c cid2 Nilpotent Fodor S Nm I F D c 1 max1 c c Table 5 The Rimplications constructed tnorms Table 1 Properties All IP All IP All All All Name Gödel product Goguen Łukasiewicz Weber Tnorm T G T P T L K T D cid2 I G c I G G c Rimplication c 1 c cid2 c 1 c I L K c min1 c 1 cid2 1 1 c I W B c cid2 Properties LN EP IP LN EP IP All LN EP IP Nilpotent Fodor T Nm I F D c 1 max1 c c All cation c c I S c SNa c The second Rimplications standard choice tnorm fuzzy logics Appendix A52 examples Table 5 These constructed tnorms I T c supb01 T b c 4 Differentiable Fuzzy Logics As mentioned earlier Differentiable Fuzzy Logics DFL Differentiable Logics based fuzzy logic Truth values ground atoms continuous logical connectives interpreted differentiable fuzzy operators In principle DFL handle predicates functions To ease discussion analyze functions constants leave discussion1 41 Semantics DFL deﬁnes new semantics vector embeddings functions vectors place classical semantics In classical logic structure consists domain discourse interpretation function meaning predicates DFL deﬁnes structures embedded interpretations2 instead Deﬁnition 1 A Differentiable Fuzzy Logics structure tuple S cid13O η θcid14 O ﬁnite unbounded set called domain discourse o O ddimensional3 vector η P RW O m 0 1 embedded interpretation θ RW parameters η maps predicate symbols P P arity m function m objects truth value 0 1 That ηP θ O m 0 1 We use notation ηθ P denote ηP θ To address symbol grounding problem 30 objects domain discourse ddimensional vectors reals Their semantics come underlying semantics vector space terms interpreted real valued world 67 Predicates interpreted functions mapping vectors fuzzy truth value Embedded interpretations implemented neural network models4 trainable network parameters θ Note different values parameters θ produce different DFL structures Next deﬁne truth values formulas DFL 1 Functions constants modeled 67 48 2 Seraﬁni Garcez 67 uses term semantic grounding symbol grounding 51 instead embedded interpretation emphasize fact L interpreted real world ﬁnd potentially confusing refer groundings Herbrand semantics Furthermore word interpretation highlight parallel classical logical interpretations 3 Without loss generality ﬁx dimensionality vectors representing objects Extensions varying number dimensions straight forward introducing types 2 4 We use models refer deep learning models like neural networks models model theory 5 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Deﬁnition 2 Let S cid13O η θ cid14 DFL structure N fuzzy negation T tnorm S tconorm I fuzzy implication A E universal existential aggregators respectively Furthermore let μ V O variable assignment use cid15 cid15 μx μx o refer new assignment x mapped domain object o μx ox o μx ox cid15 Then S satisﬁes formula ϕ L wrt μ S μ ϕ degree eθ ϕ truth x cid16 x value ϕ eθ V O L 0 1 valuation function deﬁned inductively structure ϕ follows eθ μ Px1 xm ηθ P μx1 μxm eθ μ φ Neθ μ φ eθ μ φ ψ T eθ μ φ eθ μ ψ eθ μ φ ψ Seθ μ φ eθ μ ψ eθ μ φ ψ Ieθ μ φ eθ μ ψ eθ μ x φ A eθ μ x φ E oO oO eθ μx o φ eθ μx o φ 3 4 5 6 7 8 9 Equation 3 deﬁnes fuzzy truth value atomic formula μ assigns objects terms x1 xm resulting list ddimensional vectors These inputs interpretation ηθ predicate symbol P ηθ P fuzzy truth value Equations 4 7 deﬁne truth values connectives operators N T S I Equations 8 9 deﬁne truth value universally quantiﬁed formulas x φ existentially quantiﬁed formulas x φ This enumerating domain discourse o O evaluating truth value φ o assigned x μ combining truth values aggregation operators A E Note assumption ﬁniteness domain pragmatic It reﬂects ﬁniteness data machine learning settings Hence fundamental results realm mathematical fuzzy logic hold general logic deﬁned 12 42 Relaxing quantiﬁers For inﬁnite domains domains large compute semantics quanti ﬁers choose sample batch b objects O approximate computation valuation This replacing Equation 8 eθ μ x φ b A i1 eθ μx oi φ o1 ob chosen O 10 Choosing batch objects ways One approach sample realworld distri bution domain discourse O available For example domain discourse natural images realworld distribution distribution natural images A common approach assume access dataset D independent samples distribution 25p109 choose minibatches dataset Note relaxing quantiﬁers sampling lose soundness computation different batches different truth values formulas 43 Learning fuzzy maximum satisﬁability In DFL fuzzy maximum satisﬁability 19 problem ﬁnding parameters θ maximize valuation knowledge base K Deﬁnition 3 Let K knowledge base formulas S DFL structure K eθ valuation function Then Differentiable Fuzzy Logics loss LD F L knowledge base formulas K computed LD F LS K LD F Lcid13O η θ cid14 K cid12 ϕK eθ ϕ The fuzzy maximum satisﬁability problem problem ﬁnding parameters θ minimize Equation 11 θ argminθ LD F LS K 6 11 12 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 This optimization problem solved gradient descent method If operators N T S I A E differentiable repeatedly apply chain rule reversemode differentiation DFL loss LD F LS K This LD F L SK procedure ﬁnds derivative respect truth values ground atoms ηθ Po1om We use partial derivatives update parameter θ n iteration n optimization process chain rule resulting different embedded interpretation ηθ n1 This procedure computed follows ith parameter ηθ Po1 om θ ni θ n1i θ ni cid8 LD F LSn K LD F LSn K ηθ Po1 om θ ni cid8 θ ni 13 cid12 Po1om cid8 learning rate Note parameters θ n implicitly passed L structure Sn cid13O η θ ncid14 We refer implementation details Appendix B Example 2 To illustrate computation valuation function eθ return problem Example 1 The domain discourse set objects natural images We access dataset objects D o1 o2 The valuation formula ϕ x y chairx partOf y x cushion y armRest y eθ μ ϕ A AIT ηθ chairo1 ηθ partOfo1 o1 Sηθ cushiono1 ηθ armResto1 IT ηθ chairo1 ηθ partOfo2 o1 Sηθ cushiono2 ηθ armResto2 AIT ηθ chairo2 ηθ partOfo1 o2 Sηθ cushiono1 ηθ armResto1 IT ηθ chairo2 ηθ partOfo2 o2 Sηθ cushiono2 ηθ armResto2 Next choose operators T T P S S P A A T P I I RC cid13 eθ μ ϕ 1 ηθ chairx ηθ partOf y x 1 ηθ cushion y1 ηθ armRest y x yC 04261 If interpret predicate functions conﬁdence values Example 1 ηθ Px pPx ﬁnd eθ ϕ 0612 Taking K ϕ ﬁnd chain rule LD F LS K ηθ chairo2 LD F LS K ηθ cushiono2 LD F LS K ηθ armResto2 LD F LS K ηθ partOfo2 o2 LD F LS K ηθ partOfo2 o1 LD F LS K ηθ chairo1 LD F LS K ηθ cushiono1 LD F LS K ηθ armResto1 LD F LS K ηθ partOfo1 o1 LD F LS K ηθ partOfo1 o2 04031 02219 04978 01103 00058 04257 07662 00029 00029 We gradient update step update conﬁdence values Example 1 ﬁnd partial derivative parameters θ deep learning model pθ Equation 13 One particularly interesting property Differentiable Fuzzy Logics partial derivatives subformulas respect satisfaction knowledge base somewhat explainable meaning For example hypothesized Example 1 computed partial derivatives reﬂect increase pcushiono2 absolute largest partial derivative 5 Derivatives operators We choice operators logical connectives actually determines ferences DFL If different set operators Example 2 gotten different derivatives These cases sense cases Furthermore easier ﬁnd global minimum fuzzy maximum satisﬁability problem Equation 12 operators This smoothness operators In section analyze wide variety functions logical reasoning present properties determine useful inferences illustrated We discuss varieties fuzzy negations strong negation NC 1 continuous intuitive simple derivatives 7 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Deﬁnition 4 A function f R R said vanishing b R b c b f c 0 interval function 0 Otherwise function nonvanishing A function f Rn R vanishing derivative a1 R 1 n f a1an vanishing ai Whenever derivative operator vanishes loses learning signal This deﬁnition include functions pass 0 product tconorm ﬁnd derivative S P 1 0 1 2 Furthermore partial derivatives connectives backward pass valuation function ground atoms multiplied If partial derivatives 1 product approach 0 This happen instance large sequence conjunctions product tnorm The drastic product T D operators derived drastic sum S D DuboisPrade Weber implications I D P I W B vanishing derivatives The output conﬁdence values deep learning models result transformations real numbers functions like sigmoid softmax result truth values 0 1 The operators derived T D nonvanishing derivatives inputs exactly 0 1 invalidating use application Deﬁnition 5 A function f Rn R said singlepassing nonzero derivatives input argument That x1 xn 0 1 holds cid16 0 1 n cid16cid14 cid14 cid14 1 cid14 cid14 cid14 cid15 cid14 cid14 cid14 f x1xn xi Using singlepassing Fuzzy Logic operators ineﬃcient input nonzero derivative learning signal complete forward pass computed ﬁnd input In particular hold choosing operators based Gödel tnorm Proposition 1 Any composition singlepassing functions singlepassing For proof Appendix C1 Concluding logical operator usable learning task need nonvanishing derivative majority input signals contribute learning signal ideally singlepassing contribute effectively learning signal 6 Aggregation After global considerations previous section analyze aggregation operators universal existential quantiﬁcation separately outline beneﬁts disadvantages DFL 61 Minimum maximum aggregators The minimum aggregator given A T G x1 xn minx1 xn The partial derivatives cid2 A T G x1 xn xi 1 0 argmin j1nx j 14 It singlepassing nonzero gradient input lowest truth value Many practical formulas exceptions An exception formula like x Ravenx Blackx raven bucket red paint thrown The minimum aggregator derivative exception red raven correctly predicted Additionally ineﬃcient compute forward pass inputs feedback signal The partial derivatives maximum aggregator E S G x1 xn maxx1 xn similar increase input highest truth value instead This reasonable aggregator existential quantiﬁcation reinforce belief input conﬁdent correct existential quantiﬁer true A downside consider input despite fact condition hold multiple inputs 62 Łukasiewicz aggregator The Łukasiewicz aggregator given A T LU x1 xn max cid2 A T LU x1 xn xi 1 0 cid4 n i1 xi n 1 8 cid5cid4 n cid6 i1 xi n 1 0 The partial derivatives given 15 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 The gradient nonvanishing limn n1 n 1 larger values n inputs high hold i1 xi n 1 average value xi larger n1 n 57 As For proposition refer fraction inputs condition holds The probability condition holds point uniformly sampled 0 1n fraction cid4 n Proposition 2 The fraction inputs x1 xn 0 1 derivative A T LU nonvanishing 1 n For proof Appendix C21 Clearly majority inputs vanishing gradient implying universal aggregator useful DFL learning setting A similar argument existential Łukasiewicz aggregator bounded sum E S L K x1 xn cid4 n i1 xi 1 nonvanishing derivatives 1 argument average value xi smaller min n Like Łukasiewicz aggregator nonvanishing derivatives fraction 1 1 n domain This useful existential aggregator The agent learn inputs close 0 63 Yager aggregator The Yager universal aggregator given A T Y x1 xn max 1 cid19 ncid12 1 xip cid20 1 p 0 p 0 16 i1 Here p 1 corresponds Łukasiewicz aggregator p corresponds minimum aggregator p 0 corre sponds aggregator formed drastic product A T D The derivative Yager aggregator A T Y x1 xn xi cid23cid4 n j11 x jp cid24 1 1 p 1 xip1 0 cid23cid4 n cid23cid4 n j11 x jp j11 x jp cid24 1 p 1 cid24 1 p 1 17 This derivative vanishes Therefore corresponds drastic aggregator 1 p j11 x jp 1 As 1 xi 0 1 1 xip decreasing function respect p i11 xip 1 holds larger fraction inputs p increases fraction 0 p 0 cid4 n cid4 n The exact fraction inputs nonvanishing derivative hard express general case5 However ﬁnd closedform expression Euclidean case p 2 Proposition 3 The fraction inputs x1 xn 0 1 derivative A T Y p 2 nonvanishing n 2 π 2 n 1 2ncid10 1 2 cid10 Gamma function See Appendix C22 proof We plot fraction nonvanishing derivatives values p Fig 2 For fairly small p vast majority inputs vanishing derivative similar high n showing aggregator little use learning context cid25 cid6 1 cid5cid4 n i1 x p cid26 p 1 nonvanishing Similarly derivatives Yager existential aggregator E S Y x1 xn min fraction inputs Yager universal aggregator 64 Generalized Mean Generalized Mean Error If concerned maximizing truth value A T Y simply remove max constraint resulting aggregator nonvanishing derivative However codomain function longer 0 1 We aﬃne transformation function ensure case Appendix D1 obtain Generalized Mean Error 5 Assume x1 xn U 0 1 independently standard uniformly distributed Note zi 1 xi standard uniformly distributed z distributed beta distribution Beta1p 1 27 Y i1 zi sum n betadistributed variables Unfortunately closedform expression probability density function sums independent beta random variables 60 A suitable approximation use central limit theorem z1 zn identically independently distributed cid4 n p 9 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 2 The fraction inputs Yager aggregator A T Y derivatives The values dotted lines estimated Monte Carlo simulation values p nilpotent minimum aggregator A TnM nonvanishing Deﬁnition 6 For p 0 Generalized Mean Error AG M E deﬁned AG M E x1 xn 1 cid19 1 n ncid12 1 xip cid20 1 p i1 18 The error difference predicted value xi ground truth value truth value 1 This function following derivative AG M E x1 xn xi 1 xip1 1 n 1 p ncid12 1 x jp j1 1 1 p 19 When p 1 derivative greatest inputs lowest speed optimization sensitive outliers For p 1 opposite true A special case p 1 ncid12 20 A M A E x1 xn 1 1 n 1 xi i1 having simple derivative A M A E x1xn associated Łukasiewicz tnorm Another special case p 2 1 xi n This measure equal 1 minus mean absolute error MAE A R M S E x1 xn 1 cid27 cid28 cid28 cid29 1 n ncid12 1 xi2 i1 21 This function equal 1 minus rootmeansquare error RMSE commonly regression tasks heavily weights outliers We Yager existential aggregator Appendix D1 Deﬁnition 7 For p 0 Generalized Mean deﬁned E G M x1 xn cid19 cid20 1 p 1 n ncid12 i1 x p 22 p 1 corresponds arithmetic mean p 2 geometric mean In contrast Generalized Mean Error cid23 cid24 1 p 1 cid4 n j1 x p j p1 x 1 n derivative 1 greater values smaller inputs p 1 lower values p 1 n Since want ensure derivative high inputs high truth values reinforce likely inputs conﬁrm formula want use p 1 Note arithmetic mean E G M derivative mean absolute error A M A E meaning p 1 universal existential quantiﬁcation distinguished Furthermore increasing inputs equally great idea existential quantiﬁer likely inputs formula holds Also note unlike existential aggregators directly formed tconorms maximum aggregator x1 xn 1 However long reach optimum low inputs 10 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 65 Product aggregator probabilistic sum The product aggregator given A T P x1 xn dent events It following partial derivatives A T P x1 xn xi ncid13 x j j1icid16 j cid3 n i1 xi This probability intersection n indepen 23 This derivative vanishes xi 0 Furthermore derivative xi decreased input x j low Finally compute aggregator practice numerical underﬂow multiplying small numbers Noting argmax f x argmax log f x observe logproduct aggregator Alog T P x1 xn log A T P x1 xn ncid12 i1 logxi 24 formulas prenex normal form truth value universal quantiﬁers connective Unlike aggregators codomain nonpositive numbers instead 0 1 Furthermore log product aggregator seen loglikelihood function correct label 1 similar crossentropy minimization The partial derivatives Alog T P x1 xn xi 1 xi 25 In contrast Equation 23 values inputs irrelevant derivatives respect lowervalued inputs far greater singularity x 0 value inﬁnite We conclude product aggregator particularly promising nonvanishing handle outliers The logproduct aggregator combines generalized mean aggregator Equation 7 formulas form x y The logarithm reduces outer exponentiation resulting derivative x icid4 p1 The probabilistic sum aggregator E S P x1 xn 1 i11 xi trickier Its derivatives x p cid3 n E S P xi ncid13 j1 jcid16i 1 x j 26 This derivative intuitive It increases xi inputs x j low However account value xi If inputs low increase inputs equally Since logarithm distribute addition use trick product aggregator care taken computing log E S P prevent numerical underﬂow errors 66 Nilpotent aggregators The Nilpotent tnorm given TnM b aggregator A TnM equal cid2 cid2 mina b 0 b 1 In Appendix D3 Nilpotent A TnM x1 xn minx1 xn 0 xi x j 1 xi x j lowest values x1 xn 27 The derivative follows cid2 A TnM x1 xn xi argmin j x j xi x j 1 x j second lowest value x1 xn 1 0 28 Like minimum aggregator singlepassing like Łukasiewicz aggregator derivative vanishes majority input space sum smallest values lower 1 Proposition 4 The fraction inputs derivative A TnM nonvanishing 1 2n1 For proof Appendix C23 The fraction inputs nonvanishing derivative plotted Fig 2 Again means larger numbers inputs n aggregator vanish input useful construction learning context 11 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Similarly Nilpotent existential aggregator given cid2 E SnM x1 xn maxx1 xn 1 xi x j 1 xi x j largest values x1 xn 29 It similar derivative increases largest input largest inputs lower 1 This somewhat similar maximum aggregator behaves additional condition stop increasing largest input high 67 Summary The minimum aggregator computationally ineﬃcient handle exceptions Universal aggregation opera tors vanish receiving large inputs scale include operators based Yager family tnorms nilpotent aggregator Removing bounds Yager aggregators introduces interesting connections loss functions classical machine learning literature This case logarithmic ver sion product aggregator corresponds crossentropy loss function They natural means dealing outliers promising practical use We options existential quantiﬁcation problems vanishing gradients important care ensuring formula true input instead 7 Conjunction disjunction Next analyze partial derivatives tnorms tconorms conjunction disjunction Fuzzy Logics In tnorm Fuzzy Logics weak disjunction maxa b Gödel tconorm instead dual tconorm Suppose tnorm T tconorm S We deﬁne following quantities choice taking partial derivative loss generality T S commutative deﬁnition dT b T b dS b Sa b 30 1a 1T 1a1b T 1a1b 1a It noted Deﬁnition 14 T a1 1 Sa 0 tconorm S Furthermore note S tconorm NC dual tnorm T 1T 1a1b 1 T 1 tnorm T Deﬁnition 16 Sa0 The main difference analyzing tnorms tconorms maximum T b 1 arguments b 1 In contrast tconorms inﬁnite number maxima exist Some maxima desirable Referring formula Example 1 showed preferable increase truth value cushion y armRest y Similarly conjunct negated appears antecedent implication like aforementioned formula choose conjuncts decrease By noting T ab ﬁnd tnorm chooses way dual tconorm choose Similarly disjunction negated minimize arguments way dual tnorm maximize arguments S1a1b 1a Example 3 We introduce running example analyze behavior different tnorms We optimize b c gradient descent The truth value expression computed f b c ST b T 1 c Using boundary conditions Deﬁnition 14 16 ﬁnd global optima 10 b 10 00 c 10 The derivative function chain rule f b c ST b T 1 c T b T b ST b T 1 c T 1 c T 1 c 71 Gödel tnorm The Gödel tnorm T G b mina b Gödel tconorm S G b maxa b We ﬁnd cid2 cid2 T G b 1 0 b b S G b 1 0 b b 31 32 Both T G S G singlepassing derivatives deﬁned b A beneﬁt magnitude deriva tive nearly 1 exploding vanishing gradients caused multiple repeated applications chain rule 12 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 b 1 c 1 c 1 c b 1 0 1 1 0 1 0 1 Fig 3 Decision tree derivative S G T G b T G 1 c respect Example 4 Filling Equation 31 representing b c T G S G gives f b c 1minabmin1ac 1ab 1min1acminab 11ac 1amin1acab 11aminab1ac 1c indicator function This corresponds decision tree Fig 3 The value modiﬁed increase truth conjunctions In order choose true compares 1 If 1 increase ﬁrst conjunct increasing Gradient ascent ﬁnds global optimum formula A small perturbation truth values inputs ﬂip derivative For instance b 1 c increase value 0501 decrease 0499 Furthermore cause gradient ascent stuck local optima For instance ϕ b c 04 b 02 c 01 gradient ascent increases 05 point gradient ﬂips decreases 05 Experiments optimizing gradient descent ﬁnd global optimum 888 random initializations b c 72 Łukasiewicz tnorm The Łukasiewicz tnorm T L K b maxa b 1 0 Łukasiewicz tconorm S L K b mina b 1 The partial derivatives cid2 cid2 T L K b 1 0 b 1 b 1 S L K b 1 0 b 1 b 1 33 These derivatives vanish half domain Proposition 2 However like Gödel tnorm gradient large cause vanishing exploding gradients Example 5 Using Łukasiewicz tnorm tconorm Equation 31 gives rise following computation f b c 1maxab10maxca01 cid5 1ab1 1ca0 cid6 Choosing random values initialize b c gradient descent able ﬁnd global optimum 835 initializations 73 Yager tnorm cid5 ap b p S Y b min cid2cid5 T Y b The family Yager tnorms 75 T Y b max1 p 0 family Yager tconorms cid6 1 p 1 p 0 We plot p 2 Fig 4 The derivatives given cid5 1 ap 1 bp cid6 1 1 ap 1 bp 0 cid2cid5 cid6 1 p 1 ap1 S Y b ap b p 0 cid6 1 p 1 1 ap1 1 ap 1 bp 1 1 ap 1 bp 1 ap b p 1 ap b p 1 34 35 We plot derivatives Fig 5 showing vanishing derivative nonnegligible section domain Using method described footnote 5 Section 63 Mathematica ﬁnds closed form expression fraction inputs cid23 cid24 Yager tnorm nonvanishing 1 p cid24 Observe p cid16 1 derivative T Y undeﬁned π 4 1p cid10 cid23 pcid10 1 2 1 p b 1 derivative S Y undeﬁned b 0 This requires care implementation prevent numerical issues For p 1 lower truth values higher derivative tnorm tconorm 13 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 4 Left The Yager tnorm Right The Yager tconorm For p 2 Fig 5 Left The derivative Yager tnorm Right The derivative Yager snorm For p 2 higher truth values higher derivative As p increases T Y S Y behave like T G S G Note p 1 tnorm higher derivatives higher inputs derivative singularity lima1 b 1 74 Product tnorm The product tnorm tconorm visualized Fig 6 T P b b S P b b b Their derivatives T P b b S P b 1 b 36 The derivative tnorm 0 b 0 similarly b 1 tconorm The derivative tnorm interpreted follows If wish increase b increased proportion b This sensible learning strategy If b small case conjunction certainly satisﬁed derivative low instead high The derivative tconorm intuitive says If wish increase b increased proportion 1 b If b true deﬁnitely want true Example 6 By product tnorm tconorm Equation 31 f b c 1 1 c b 1 b c As explained increase proportion b true c true decrease proportion c true b true 14 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 6 Left The product tnorm Right The product tconorm 75 Summary The Gödel tnorm tconorm simple effective having strong derivatives However brittle making binary choices The Łukasiewicz tnorm tconorm strong derivatives vanish half domain The Yager family tnorms tconorms vanish signiﬁcant domain The derivative tnorm larger lower values sensible learning strategy This case product tnorm derivative dependent input value However product tconorm intuitive corresponds intuition input true 8 Implication Finally consider functions suitable modeling implication We start discussing particular challenges associated implication operator 81 Challenges material implication A signiﬁcant proportion background knowledge written universally quantiﬁed implications Examples statements humans mortal laptops consist screen processor keyboard humans wear clothes These formulas form x φx ψx φx antecedent ψx consequent The implication known rules inference classical logic Modus ponens inference says x φx ψx know φx true ψx true Modus tollens inference contraposition says x φx ψx know ψx false φx false ψx Unlike sequences conjunctions formulas true agent predicts scene implication false supervisor multiple choices Consider implication ravens black There 4 categories formula black ravens BR nonblack nonravens NBNR black nonravens BNR nonblack ravens NBR Assume agent observes NBR options consider 1 Modus Ponens MP The antecedent true modus ponens consequent true We trust agents observation raven believe black raven BR 2 Modus Tollens MT The consequent false modus tollens antecedent false We trust agents observation nonblack object believe raven NBNR 3 Distrust We think agent wrong observations conclude black nonraven BNR 4 Exception We trust agent observing nonblack raven NBR ignore fact observation goes background knowledge ravens black6 The distrust option somewhat useless The exception option correct know exception agents observations 6 This option completely ludicrous white ravens fact exist However rare 15 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 We assume far nonblack objects ravens ravens Thus statistical perspective likely agent observed NBNR This shows imbalance associated implication ﬁrst noted 42 Reichenbach implication It similar class imbalance problem Ma chine Learning 37 sense far contrapositive examples positive examples background knowledge This problem closely related Raven paradox 317242 ﬁeld conﬁrmation theory ponders evidence conﬁrm statement like ravens black It usually stated follows 1 Premise 1 Observing examples statement contributes positive evidence statement 2 Premise 2 Evidence statement evidence logically equivalent statements 3 Conclusion Observing examples nonblack nonravens evidence ravens black The conclusion follows nonblack objects nonravens logically equivalent ravens black For DFL similar thing happens When correct observation NBR BR difference truth value equal correct NBNR More precisely representing ravens black Ia b example I1 1 corresponds BR Ax1 I1 0 xn Ax1 I1 1 xn Ax1 I1 0 xn Ax1 I0 0 xn I0 0 I1 1 1 When agent observes thousand BRs single NBR agent observes thousand NBNRs single NBR truth value ravens black equal The ﬁrst agent seen ravens single black The second observed non ravens single raven black Intuitively ﬁrst agents beliefs line background knowledge We proceed analyze number implication operators light discussion 82 Analyzing implication operators In section choose negation derivative respect antecedent makes easier compare fuzzy implications monotonically decreasing respect antecedent Deﬁnition 8 A fuzzy implication I contrapositive differentiable symmetric Iac c I1c1a 1c c 0 1 A consequence contrapositive differentiable symmetry c 1 derivatives respect This tecedent consequent negation Iac seen distrust option increases consequent negated antecedent equally I11ac I1c1a Iac 11a 1c c Proposition 5 If fuzzy implication I Ncontrapositive symmetric c 0 1 Ia c INc Na N classical negation contrapositive differentiable symmetric By proposition Simplications contrapositive differentiable symmetric This property says difference implication handles derivatives respect consequent antecedent Proposition 6 If implication I leftneutral c 0 1 I1 c c I1c contrapositive differentiable symmetric Ia0 1 c 1 If addition I The proofs propositions Appendix C3 All Simplications Rimplications leftneutral Simplications contrapositive differentiable symmetric The derivatives Rimplications vanish c half domain This necessarily bad property depends highly sort application use DFL If example 0499 c 05 implication ravens black expresses state uncertainty probably learned However possible implication vanishes 83 Gödelbased implications Implications based Gödel tnorm strong discrete choices singlepassing As I K D c max1 c derivatives cid2 I K D c c 1 0 1 c 1 c I K D b 1 c 1 c cid2 1 0 16 37 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 7 Left The Kleene Dienes implication Right The Gödel implication Plots section rotated smallest value help understand shape functions In particular plots derivatives implications rotated 180 degrees compared implications Or simply conﬁdent truth consequent truth negated antecedent increase truth consequent Otherwise decrease truth antecedent This decision somewhat arbitrary account imbalance modus ponens modus tollens cid2 The Gödel implication simple Rimplication I G c c 1 c Its derivatives cid2 I G c c 1 c 0 I G b 0 38 These implications shown Fig 7 The Gödel implication increases consequent c antecedent changed This makes poorly performing implication practice For example consider 01 c 0 Then Gödel implication increases consequent agent fairly certain true Furthermore derivative respect negated antecedent 0 choose modus tollens correction argued actually best choice 84 Łukasiewicz Yagerbased implications The Łukasiewicz implication S Rimplication It given I L K c min1 c 1 simple derivatives I L K c c I L K c cid2 c 1 0 39 Whenever implication satisﬁed antecedent higher consequent simply increase negated antecedent consequent lower This seen distrust choice observations agent equally corrected account imbalance modus ponens modus tollens cases The derivatives Gödel implication I G equal I L K I G zero derivative negated antecedent cid26 p 1 p 0 We plot I Y p 2 Fig 8 p 1 cid25 cid5 1 ap c p I L K p 0 I D P p I K D The derivatives computed The Yager Simplication given I Y c min cid6 1 cid2cid5 I Y c c I Y c 1 ap c p 0 cid2cid5 1 ap c p 0 cid6 1 p cid6 1 p 1 c p1 1 1 ap1 1 ap c p 1 1 ap c p 1 We plot derivatives p 2 Fig 9 For p limc0 I Y 1c c 1 Furthermore p 1 lima1 17 40 41 I Y ac c cid14 cid14 cid14 c0 0 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 8 Left The Yager Simplication Right The Yager Rimplication For p 2 Fig 9 Plots derivatives Yager Simplication p 2 For p 1 I Y understood increasingly smooth version p 1 lima1 KleeneDienes implication I K D Lastly derivative like T Y S Y Section 73 nonvanishing fraction I Y a0 0 cid23 cid24 cid14 cid14 cid14 c0 π 4 1p cid10 cid23 pcid10 1 2 1 p 1 p cid24 input space The Yager Rimplication Appendix D4 I T Y c cid2 1 1 cid5 1 cp 1 ap cid6 1 c p We plot I T Y p 2 Fig 8 As expected p 1 reduces I L K p 0 reduces I W B p reduces I G It contrapositive symmetric p 1 The derivatives implication cid2 I T Y c c 1 cp 1 ap 0 cid2 I T Y c 1 cp 1 ap 0 1 p 1 1 cp1 c 1 p 1 1 ap1 c 42 43 We plot Fig 10 Note p 1 c 1 holds limac I T Y ac c approaches c 1 cp 1 ap approaches 0 giving singularity 0 singularities makes training unstable practice 1 p limac I T Y ac 1 undeﬁned This collection 18 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 10 Plots derivatives Yager Rimplication p 2 Fig 11 Left The Reichenbach implication Right The Goguen implication 85 Productbased implications The product Simplication known Reichenbach implication given I RC c 1 c We plot Fig 11 Its derivatives given I RC c I RC c c 1 c 44 These derivatives closely follow modus ponens modus tollens rules When antecedent high increase consequent consequent low decrease antecedent However 1 c derivative equal distrust option chosen This result counterintuitive behavior For example agent predicts 06 raven 05 black use gradient descent ﬁnd maximum end 03 raven 1 black We end increasing conﬁdence black raven high However additional modus tollens reasoning raven barely true Furthermore agent time predicts values 0 c 0 result modus tollens case common majority gradient decreases antecedent I RC a0 1 We identify methods counteract behavior We introduce second method Section 851 The ﬁrst method counteracting corner behavior notes different aggregators change derivatives implications behave truth value high For instance ﬁnd derivatives respect negated antecedent logproduct aggregator RMSE aggregator cid14 cid14 cid14 a0 log A P I RC a1 c1 I RC cn 1 ai 1 ci 1 ai ai ci c c 45 19 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 12 Left The antecedent derivative Reichenbach implication logproduct aggregator Right The antecedent derivative Reichenbach implication RMSE aggregator Fig 13 The derivatives Goguen implication Note plot log scale A R M S E I RC a1 c1 I RC cn 1 ai 1 ciai ai ci j1a j j c j2 cid30 n cid4 n cid30 n ciai ci cid4 n j1a j c j2 46 We plot functions Fig 12 For RMSE aggregator choose n 2 a1 c1 a1 a1 c12 09 Note derivative respect negated antecedent RMSE aggregator 0 ai 0 ci 0 ai ai ci 0 logproduct aggregator derivative 1 By differentiable contrapositive symmetry consequent derivative 0 aggregators This shows RMSE aggregator derivatives vanish corners 0 c 0 1 c 1 logproduct aggregator c gradient cid2 The Rimplication product tnorm Goguen implication given I G G c implication Fig 11 The derivatives I G G cid2 I G G c c c 0 1 I G G c cid2 c 0 c a2 c 1 c We plot 47 We plot Fig 13 This derivative useful First modus ponens modus tollens derivatives increase This opposite modus ponens rule antecedent low increases consequent For example raven 01 black 0 derivative respect black 10 singularity approaches 0 20 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 14 The consequent derivatives logReichenbach logReichenbachsigmoidal s 9 implications The ﬁgure plotted log scale 851 Sigmoidal implications For second method tackling corner problem introduce new class fuzzy implications formed transforming fuzzy implications sigmoid function translating boundary conditions hold The derivation proofs properties Appendix D2 Deﬁnition 9 If I fuzzy implication I sigmoidal implication σI given s 0 b0 R σI c 1 e s1b0 eb0s es1b0 cid23cid23 1 e cid24 b0s σ s Ia c b0 1 cid24 48 σ x 1 1ex denotes sigmoid function Here b0 parameter controls position sigmoidal curve s controls spread curve σI function σ s Ia c b0 linearly transformed codomain closed interval 0 1 For common value b0 1 2 simpler form exists σI c 1 cid25cid23 s 2 1 e cid25 cid25 cid24 s 2 1 e σ s cid26cid26 cid26 1 Ia c 1 2 Next derivative σI Substituting d 1e s1b0 s1b0 h sb0 e e cid5 1 e cid6 sb0 ﬁnd σI c Ia c d h s σ s Ia c b0 1 σ s Ia c b0 49 50 This keeps properties original function smoothens gradient higher values s As derivative sigmoid function positive derivative vanishes derivative I vanishes We plot derivatives Reichenbachsigmoidal implication σI RC Fig 15 As expected Proposition 16 differentiable contrapositive symmetric Compared derivatives Reichenbach implication small gradient corners When logproduct aggregator derivative antecedent respect total valuation divided truth implication In Fig 14 compare consequent derivative normal Reichenbach implication Reichenbachsigmoidal implication log function Clearly singularity 1 c 0 implication 0 derivative log function inﬁnite A signiﬁcant difference sigmoidal variant ﬂat normal Reichenbach implication This useful means larger gradient values c implication true In particular gradient modus ponens case 1 c 1 modus tollens case 0 c 0 far smaller help balancing effective total gradient solving corner problem Reichenbach implication brought Section 85 These derivatives smaller higher values s In Fig 16 plot Reichenbachsigmoidal implication different values hyperparameters b0 s Comparing 16a 16b larger values b0 sigmoidal shape center lower input values Note s 001 Fig 16c plotted function indiscernible plot Reichenbach implication Fig 11 interval sigmoid acts extremely small sigmoidal transformation linear For high values s like 16d S shape thinner larger domain low derivative 21 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 15 The derivatives Reichenbachsigmoidal implication s 9 86 Summary We analyzed fuzzy implications theoretical perspective keeping challenges caused mate rial implication mind As result analysis ﬁnd popular Rimplications particular Gödel implication Yager Rimplication Goguen implication work differentiable setting The analyzed implications intuitive derivatives practical issues like nonsmoothness 9 Experimental setup To insights behavior operators practice perform series simple experiments analyze We discuss experiments MNIST dataset handwritten digits 43 investigate behavior different fuzzy operators introduced paper The goal experiments method state art problem semisupervised learning MNIST able insights fuzzy operators behave differentiable setting7 91 Measures To investigate performance different conﬁgurations DFL ﬁrst introduce useful metrics These insight different operators behave In section assume dealing formulas form ϕ x1 xm φ ψ Deﬁnition 10 The consequent magnitude cons antecedent magnitude ant knowledge base K deﬁned sum partial derivatives consequent antecedent respect DFL loss cons cid12 cid12 ϕK μMϕ eθ μ ϕ eθ μ ψ ant cid12 cid12 ϕK μMϕ eθ μ ϕ eθ μ φ 51 Mϕ set instances universally quantiﬁed formula ϕ ψ φ evaluated instantiation μ The consequent ratio cons sum consequent magnitudes divided sum consequent antecedent magnitudes cons cons consant Deﬁnition 11 Given labeling function l returns truth value formula according data instance μ consequent antecedent correctly updated magnitudes sum partial derivatives consequent negated antecedent true cucons cid12 cid12 ϕK μMϕ lψ μ eθ μ ϕ eθ μ ψ cuant cid12 cid12 ϕK μMϕ lφ μ eθ μ ϕ eθ μ φ 52 7 Code available httpsgithub com HEmile differentiable fuzzylogics 22 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 16 The Reichenbachsigmoidal implication different values b0 s That consequent true data measure magnitude derivative respect consequent To evaluate quantities deﬁne ratios similar precision metric Deﬁnition 12 The correctly updated ratios consequent antecedent deﬁned cucons cid4 cid4 ϕK cuconsϕ ϕK consϕ cuant cid4 cid4 ϕK cuantϕ ϕK antϕ 53 These quantify fraction updates going right direction When ratios approach 1 DFL increase truth value consequent negated antecedent correctly8 Otherwise increasing truth values subformulas wrong Ideally want measures high 8 It change truth value ground atom wrongly φ ψ atomic formulas 23 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 92 Formulas We use knowledge base K universally quantiﬁed logic formulas There predicate digit zero For example zerox true x handwritten digit labeled 0 We sets formulas learn additional binary predicate 921 The problem The problem simple problem test different operators implication universal aggregation We use binary predicate true arguments digit We formulas use 1 x y zerox zero y samex y x y ninex y samex y If x y handwritten zeros example represent digit 2 x y zerox samex y zero y x y ninex samex y y If x y represent digit represents zero 3 x y samex y y x This formula encodes symmetry predicate We ﬁnd Appendix F1 set operators better random guessing consequent updates cucons 01 know conﬁdence set operators better random cuant 099 922 The sum9 problem In second problem use binary predicate sum9 true arguments sum 9 We use problem test existential quantiﬁcation conjunction disjunction The formulas 1 x y sum9x y For digit sum 99 2 x y sum9x y zerox y onex y ninex zero y This formula deﬁnes sum9 predicate 93 Experimental methodology We split MNIST dataset 1 labeled 99 unlabeled Given handwritten digit x labeled digit y pθ yx computes distribution 10 possible labels We use 2 convolutional layers max pooling ﬁrst 10 second 20 ﬁlters Then follows fully connected hidden layers 320 50 nodes softmax output layer The probability samex1 x2 handwritten digits x1 x2 holds modeled pθ samex1 x2 This takes 50dimensional embeddings x1 x2 fully connected hidden layer ex1 ex2 These network architecture called Neural Tensor Network 69 cid25 cid26cid26 cid25 cid31 pθ samex1 x2 σ cid2 u tanh cid2 x1 W e 1k V ex2 ex1 ex2 b 54 1k Rddk bilinear tensor product V Rk2d concatenated embeddings b Rk W bias vector We use k 50 size hidden layer u Rk compute output logit goes sigmoid function σ conﬁdence value The loss function use split parts ﬁrst unlabeled dataset Du labeled dataset Dl Lθ w D F L LD F Lcid13Du η θ cid14 K cid12 x yDl log pθ yx cid12 x1 y1x2 y2 DlDl log pθ 1 y1 y2 x1 x2 55 The ﬁrst term DFL loss weighted DFL weight w D F L The second supervised cross entropy loss batch size 64 The supervised binary cross entropy loss learn recognize samex y10 This loss log pθ sum9 1 y1 y29x1 x2 sum9 problem As far negative examples positive examples undersample negative examples Note supervised losses seen universal aggregation logical facts logproduct aggregator Alog T P pθ y1x1 pθ yDlxDl For optimization ADAM 39 learning rate 0001 9 We sample minibatches 64 digits means negligible probability exists digit minibatch match 00117 precise 10 It possible use loss term learn predicate formulas challenging works good set fuzzy operators 24 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Table 6 Results problem symmetric conﬁgurations Simplications R implications For w D F L 1 T P w D F L 10 SImplications RImplications Accuracy cons T G T P T L K T Y p 15 T Y p 2 T Y p 20 T Nm 953 965 949 952 777 956 952 032 008 05 020 002 cucons 031 072 086 051 054 cuant 083 099 012 075 075 Accuracy cons 950 948 949 952 950 955 952 1 062 05 062 053 cucons 011 004 086 061 001 cuant 096 012 046 099 10 Results We ran experiments combinations operators aim showing discussed insights present practice We report accuracy recognizing digits test set consequent ratio cons consequent antecedent correctly updated ratios cucons cuant We train 70000 iterations convergence The purely supervised baseline test accuracy 9518 0204 runs 35 minutes Semisupervised methods improve baseline useful Our implementation including DFL runs 1 hour 52 minutes 101 Symmetric conﬁgurations First consider symmetric conﬁgurations conjunction tnorm T disjunction dual tconorm T universal aggregation extended tnorm A T existential aggregation extended tconorm E S implication Simplication based tconorm Rimplication based tnorm For example T P use log A T P aggregation I RC implication Symmetric conﬁgurations T P conjunction S P disjunction Alog T P retain equivalence relations fuzzy logic unlike choose arbitrary conﬁguration operators 1011 Symmetric conﬁgurations problem All conﬁgurations run w D F L 1 T P run w D F L 10 The results problem Table 6 One general observation Simplications work better Rimplications The conﬁguration Rimplications outperform supervised baseline T Y p 20 Simplication performs similar We hypothesize derivatives Rimplications vanish c The Gödel tnorm performs par supervised baseline This min aggregator singlepassing like conﬁguration The single instance receives derivative exception argued Section 61 evident low values cucons cuant The Łukasiewicz tnorm performs worse supervised baseline Since A L K derivative 0 1 ev erywhere total gradient large vanish By deﬁnition I L K cons 1 2 consequent negated antecedent derivatives equal Equation 39 cucons low 001 worse random guessing As half gradient MP reasoning half nearly incorrect The performance Yager tnorm highly dependent choice parameter p For p 20 performance bit higher baseline The lower value p likely derivative universal aggregator vanishes However p 2 results worse Łukasiewicz tnorm corresponds p 1 derivative p 15 simply vanishes run The product tnorm performs best highest values cucons cuant To large extent logproduct aggregator effective symmetric conﬁgurations perform better Appendix F2 Finally Nilpotent tnorm performs exactly like supervised baseline derivative universal aggregator vanished complete training run 1012 Symmetric conﬁgurations sum9 problem In addition problem run symmetric conﬁgurations sum9 problem able account existential quantiﬁcation disjunction behave The results Table 7 These closely reﬂect results problem Again conﬁgurations clearly outperform supervised baseline product tnorm Yager tnorm p 20 product tnorm promising candidate Furthermore addition Nilpotent tnorm derivatives Łukasiewicz tnorm Yager tnorm p 2 vanish run 102 Individual operators We perform experiments investigate contribution speciﬁc fuzzy operators regard resulting conﬁgurations sensible logical sense We better understand operator 25 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Table 7 Results sum9 problem symmetric conﬁgu rations Simplication w D F L 1 T P w D F L 10 Accuracy cons 031 013 cucons 044 080 cuant 078 095 099 082 071 T G T P T L K T Y p 15 T Y p 2 T Y p 20 T Nm 952 961 952 952 952 955 952 Table 8 Left Results problem varying universal aggregator For w D F L 10 Right Results sum9 problem varying existential aggregator Accuracy A T G Alog T P A T L K A T Y p 15 A T Y p 2 A T Y p 20 AG M E p 15 A R M S E AG M E p 20 A T Nm 866 963 782 795 833 840 961 962 955 798 cons Universal aggregation cucons 065 053 094 077 083 081 043 042 038 050 037 014 000 000 000 000 043 046 045 034 Existential aggregation cuant 045 096 099 098 098 098 076 072 070 058 Accuracy cons E S G E S P E S L K E S Y E S Y E S Y E G M E G M E G M E S Nm 953 961 959 959 963 963 969 967 964 955 038 015 017 021 027 037 029 029 039 031 cucons 060 068 076 064 059 062 013 014 064 058 cuant 066 094 087 085 081 070 095 094 070 078 Table 9 Results sum9 problem varying tnorm t conorm Accuracy cons T G T P T L K T Y p 15 T Y p 2 T Y p 20 T Nm 204 203 970 970 968 949 969 047 046 031 029 030 066 031 cucons 015 016 015 011 011 016 017 cuant 090 092 093 096 096 086 093 contributes learning process Throughout section ﬁx universal aggregation operator logproduct aggregator Alog T P existential aggregation operator generalized mean E G M p 15 conjunction disjunction T Y S Y p 15 implication Reichenbachsigmoidal implication s 9 b 05 We select promise initial experiments 1021 Aggregation Table 8 shows results varying universal aggregator problem varying existential aggregator sum9 problem The logproduct operator w D F L 10 best universal aggregator A R M S E trailing slightly Other generalized mean errors effective We singlepassing aggregators minimum Nilpotent minimum aggregators aggregators vanish large domain Yagerbased Nilpotent minimum perform poorly However curiously Yagerbased aggregators high cucons cuant The generalized means best result existential aggregation p 15 performing best They manage properly select inputs existential quantiﬁer true softly increasing largest inputs Unlike universal aggregation Yager existential aggregator decent choice The maximum aggregator Nilpotent maximum aggregator somewhat outperform supervised baseline single passing 1022 Conjunction disjunction In Table 9 compare different tnorms corresponding tconorms Here operators vanish large domain work best Yager tnorms Nilpotent minimum These work better aggregation inputs smaller reducing probability derivative vanishes The product tnorm Gödel tnorm corresponds weak conjunction disjunction perform poorly In table clear relation lower cons perform better It likely 26 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Table 10 Results problem symmetric conﬁgurations Simplications Rimplications SImplications RImplications Accuracy cons I K D I RC I L K I Y p 15 I Y p 2 I Y p 20 I F D σI RC 957 963 960 961 962 951 961 963 008 009 05 014 013 057 019 014 cucons 082 073 007 077 082 048 068 053 cuant 098 098 093 096 097 098 091 096 Accuracy cons I G I G G I L K I R Y I R Y I R Y I F D 905 936 960 961 957 960 961 1 099 05 014 013 014 019 cucons 005 000 007 014 064 065 068 cuant 087 093 066 038 038 091 lower Yager tnorm disjunction consequent 1 happens sum Yager tnorm hits boundary 1023 Implications In Table 10 compare different fuzzy implications problem The Reichenbach implication Yager Simplication work having accuracy 97 The Kleene Dienes Yager Rimplications surpass baseline In experiments sigmoidalReichenbach implication run s 9 b 1 2 forms normal Reichenbach implication However ﬁnd Appendix F3 SGD algorithm implication outperforms Reichenbach implication reaching 973 accuracy As argued Sections 83 85 Gödel implication Goguen implication worse performance supervised baseline making incorrect modus ponens inferences While derivatives I L K I G differ I G disables derivatives respect negated antecedent I L K performs better I G worst test implication suggesting derivatives respect negated antecedent required successfully applying DFL Note Simplications tend perform better Rimplications particular Gödel tnorm product tnorm This inherently balance derivatives respect consequent negated antecedent contrapositive differentiable symmetric 1024 Additional experiments In Appendix F3 investigate parameters s b0 sigmoidalReichenbach implication We ﬁnd best performing implication problem vanilla SGD optimizer reaching 973 accuracy Furthermore Appendix F4 investigate problem inﬂuence rule learning process We ran sum9 problem vanilla SGD optimizer ﬁxed conﬁguration Section 102 reaches 977 accuracy This signiﬁcantly higher ADAM conﬁrming ﬁndings problem Sec tion 1023 Finally ran settings formulas problem sum9 problem This best accuracy ﬁnd experiments 980 conﬁrms adding background knowledge increases ﬁnal performance 103 Analysis We plot accuracy different conﬁgurations respect cucons cuant Figs 17a 17b The blue dots represent runs problem red dots represent runs sum9 problem Fig 17b shows positive correlation suggesting vital learning process updates going antecedent correct Although slight positive correlation Fig 17a problem pronounced Furthermore sum9 correlation negative instead conﬁgurations highest accuracy low values cucons We plot experimental values cons values cucons cuant Figs 17c 17d For negative correlation Apparently larger consequent ratio decreases correctness updates In Appendix F3 ﬁnd experimenting value s lower values cons smaller portion reasoning happens corners 0 c 0 1 c 1 instances agent certain Since Simplications strong derivatives corners Proposition 6 phenomenon likely present Simplications This suggests need properly balance contribution updates antecedent consequent Since usually reasoned Section 81 derivatives respect antecedent common balance reﬂected experimental ratio updates 104 Conclusions We run experiments conﬁgurations hyperparameters explore works The performing fully symmetric option product tnorm Reichenbach implication If willing 27 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig 17 We plot analytical measures ﬁnd relations Blue dots represent runs problem red dots represent runs sum9 problem For interpretation colors ﬁgures reader referred web version article forego symmetry ﬁnd choice aggregators important factor performance For universal aggregation recommend logproduct aggregator existential quantiﬁcation recommend generalized mean value p 1 2 We especially important choose aggregation operators vanish large domain singlepassing In experiments tuned sigmoidalReichenbach implication coupled vanilla SGD proved effective fuzzy implication In general recommend choosing Simplications Rimplications For conjunction disjunction recommend tuning Yager tnorm value dependent complexity formulas prevent derivative vanishing run Although Differentiable Fuzzy Logics signiﬁcantly improves supervised baseline suited semi supervised learning currently competitive stateoftheart methods like Ladder Networks 62 accuracy 989 100 labeled pictures 992 1000 11 Related work Differentiable Fuzzy Logics falls discipline Statistical Relational Learning 24 concerns models reason uncertainty learn relational structures like graphs 111 Differentiable Fuzzy Logics Special cases DFL researched papers different names Logic Tensor Networks LTN 267 implements function symbols uses neural model interpret predicates LTN applied weakly supervised learning Scene Graph Parsing 19 transfer learning Reinforcement Learning 3 Semanticbased regularization SBR 17 applies DFL kernel machines They use Rimplications mean aggre gator Sen et al 66 applies SBR collective classiﬁcation predicting trained deep learning model optimizes DFL loss ﬁnd new truth values This ensures predictions consistent formulas testtime Marra et al 50 uses tnorm Fuzzy Logics Rimplication alongside weak disjunction By tnorms based generator functions satisﬁability computation simpliﬁed generalizations common loss functions Marra et al 48 applies DFL image generation It uses product tnorm logproduct aggregator Goguen implication By function symbols represent generator neural networks create constraints create semantic description image generation problem Rocktäschel et al 65 uses product tnorm Reichenbach implication relation extraction eﬃcient matrix embedding rules Guo et al 26 extends link prediction triple classiﬁcation marginbased ranking loss implications Demeester et al 16 uses regularization technique equivalent Łukasiewicz implication Instead existing data ﬁnds loss function iterate objects guarantee rules hold This scalable model simple implications A promising approach adversarial sets 52 set objects 28 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 domain satisfy knowledge base These probably informative objects It uses gradient descent ﬁnd objects minimize satisﬁability The parameters deep learning model updated predicts consistent knowledge base adversarial set A beneﬁt approach iterate instances satisfy constraints Adversarial sets applied natural language interpretation 53 Both papers use Łukasiewicz implication Gödel tnorm tconorm They able infer new labels existing unlabeled data use artiﬁcial data methods orthogonal jointly 112 Neurosymbolic methods fuzzy logic operators Posterior regularization 2132 framework weaklysupervised learning structured data It projects output deep learning model ruleregularized subspace consistent knowledge base This output label deep learning model imitate Unlike paper compute derivatives computation satisfaction knowledge base Marra et al 49 Daniele Seraﬁni 13 instead use gradient descent projection Therefore unlike earlier methods posterior regularization derivatives respect operators They learn relative formula weights jointly parameters deep learning model Arakelyan et al 1 uses tnorms tconorms existential quantiﬁcation answer queries ﬁnding entity embedding highest truth value given query This search gradient descent By comparing entity embedding best ﬁts optimized entity embedding authors answer complex FOL queries The authors use product Gödel tnorms Another recent work employs fuzzy logic operators neurosymbolic setting Logical Neural Networks 63 This work stands orthogonal work foremost distinction employ logics lowlevel logical connectives neurons neural activation functions employ higher level deﬁning loss function They limit work propositional level simpliﬁcation purposes argue extending relational level straightforward ILP 20 differentiable inductive logic programming uses product tnorm tconorm differentiable inference The Neural Theorem Prover 64 differentiable proving queries combines different proof paths Gödel tnorm tconorm Šourek et al 70 introduces method differentiable query proving learnable weights formulas They use operators inspired fuzzy logic transformed sigmoid function There vast literature Fuzzy Neural Networks 353644 replace standard neural network neurons neu rons based fuzzy logic Some neurons use fuzzy logic operators differentiated networks trained backpropagation 113 Differentiable probabilistic logics Some approaches use probabilistic logics instead fuzzy logics interpret predicates probabilistically As deep learn ing classiﬁers model probability distributions probabilistic logics natural choice fuzzy logics DeepProbLog 46 probabilistic logic programming language neural predicates compute probabilities ground atoms It supports automatic differentiation backpropagate loss query predi cate deep learning models implement neural predicates similar DFL It supports probabilistic rules handle exceptions rules We compare differentiable probabilistic logic called Semantic Loss 74 Appendix E similarities DFL operators based product tnorm This similarity suggests practical problems DPFL present Semantic Loss They apply Semantic Loss MNIST semi supervised learning different knowledge base As inference exponential size grounding probabilistic logics approaches use advanced compilation technique 14 inference feasible larger problems 12 Discussion This paper presented theoretical results Differentiable Fuzzy Logics operators evaluated behavior semisupervised learning We discuss problems deploying solutions DFL DFL seen form multiobjective optimization 33 In DFL loss Equation 11 sum valuations different formulas separate objective Each objectives weighted differently resulting wildly varying loss landscapes Having objectives requires signiﬁcant hyperparameter tuning A method capable learning relative formula weights jointly like 491370 solve problem A second challenge related class imbalance problem 379 We argued Section 81 signiﬁcant portion commonsense background knowledge modus tollens case far common Our problem showed wellperforming implications far larger derivative respect negated antecedent consequent This imbalance increase complex problems However simply removing derivatives respect antecedent solution A reason usually correct unlike derivatives respect consequent In fact Appendix F4 formula digits 29 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 antecedent performs better formula digits consequent model learn new positive examples Although focused experimenting accuracy derivatives implication noted derivatives disjunction operator choice For example agent observes walking object supervisor knows humans animals walk supervisor supposed choose human animal Here similar imbalances exist different possible classes There images humans animals Further pose important choose operators based performance task hand based logical properties The best conﬁguration uses operators based product Yager tnorms The product tnorm viable symmetric choice experiments The largest beneﬁt symmetric choice operators truth value formulas logically equivalent classical logic equal This makes easier analyze background knowledge behave require putting particular form As ﬁnal remark noteworthy interpretation truth values As aforementioned logic use fuzzy logic originally aimed address logical reasoning presence vagueness probabilistic uncertainty The truth values derived fuzzy operators probabilistic instance 28 p 411 However considerably large problems addressed machine learning literature probabilistic mathematical origins statistics classiﬁcation task running example probabilistic origin With choice aimed respect recent literature Applications fuzzy operators general set problems necessarily fuzzy uncommon neurosymbolic AI Examples include 67 63 1 cited Section 11 13 Conclusion We analyzed Differentiable Fuzzy Logics order understand reasoning logical formulas behaves differentiable setting We examined properties large different operators affect DFL We substantial differences properties large number Differentiable Fuzzy Logics operators showed including popular operators highly unsuitable use differentiable learning setting By analyzing aggregation functions logproduct aggregator RMSE aggregator convenient connections fuzzy logic machine learning deal outliers Next analyzed conjunction disjunction operators strong candidates In particular Gödel tnorm tconorm simple choice Yager tnorm product tconorm intuitive derivatives We noted interesting imbalance derivatives respect negated antecedent consequent implication Because modus tollens case common conclude large useful inferences MNIST experiments decreasing antecedent modus tollens reasoning Furthermore derivatives respect consequent increase truth value false consequent false majority times Therefore argue modus tollens reasoning embraced future research As possible solution problems caused imbalance introduced smoothed fuzzy implication called Reichenbachsigmoidal implication Experimentally product tnorm tnorm base choices operators The product tconorm Reichenbach implication intuitive derivatives correspond inference rules classical logic logproduct aggregator effective universal aggregation operator In order gain largest improvements supervised baseline abandon normal symmetric conﬁgurations norms tnorms tconorms implications aggregation operators satisfy usual algebraic relations Instead resort nonsymmetric conﬁgurations operators based different tnorms com bined The Reichenbachsigmoidal implication performs best experiments Its hyperparameters tweaked decrease imbalance derivatives respect negated antecedent consequent For existential quantiﬁca tion general mean error performs best conjunction disjunction family Yager tnorms Nilpotent minimum highest ﬁnal accuracy We believe proper empirical comparison different methods introduce background knowledge logic useful properly understand details performance possible applications challenges method Sec ondly believe work required background knowledge help deep models train realworld problems One research direction develop methods properly deal exceptions An approach formula importance weights learned distinguish relevant irrelevant formulas background knowledge probabilistic instead fuzzy logics natural ﬁt Lastly additional research vast space fuzzy logic operators ﬁnd properties useful DFL 11 Indeed reasoning belief fuzzy logic semantics instead probabilistic logic semantics straight outofthebox yield undesirable results Consider event pa probability 05 Now consider disjunction pa value 05 However Łukasiewicz logic Sa yield 1 30 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Acknowledgements We sincerely thank anonymous reviewers comments substantially improved content quality manuscript This work partly funded MaestroGraph research programme project number 612001552 ﬁnanced Dutch Research Council NWO Erman Acar generously funded Hybrid Intelligence Project ﬁnanced Dutch Ministry Education Culture Science project number 024004022 This work supported DAS5 distributed supercomputer 4 Appendix A Background fuzzy logic operators In section introduce semantics fuzzy operators tnorm tconorm negation connect truth values fuzzy predicates semantics quantiﬁer We follow 38 section refer proofs additional results A1 Fuzzy negation The functions compute negation truth value formula called fuzzy negations Deﬁnition 13 A fuzzy negation decreasing function N 0 1 0 1 N1 0 x NNx x 11 N called strict strictly decreasing continuous strong 0 1 NNa A consequence conditions N0 1 Throughout paper use N refer classical negation Na 1 A2 Triangular norms The functions compute conjunction truth values called tnorms For rigorous overview 40 Deﬁnition 14 A tnorm triangular norm function T 0 12 0 1 commutative associative 1 Monotonicity For 0 1 T increasing 2 Neutrality For 0 1 T 1 The phrase T increasing means 0 b1 b2 1 T b1 T b2 Deﬁnition 15 A tnorm T following properties Continuity A continuous tnorm continuous arguments b Leftcontinuity A leftcontinuous tnorm leftcontinuous arguments That b 0 1 limxa T x b T b limit T x b x increases approaches c Idempotency An idempotent tnorm property 0 1 T d Strictmonotony A strictly monotone tnorm property 0 1 T strictly increasing e Strict A strict tnorm continuous strictly monotone Table 1 shows basic tnorms tnorms alongside properties A3 Triangular conorms The functions compute disjunction truth values called tconorms snorms Deﬁnition 16 A tconorm triangular conorm known snorm function S 0 12 0 1 commutative associative 1 Monotonicity For 0 1 Sa increasing 31 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 2 Neutrality For 0 1 S0 Tconorms obtained tnorms De Morgans laws classical logic p q p q Therefore T tnorm NC strong negation T s NC dual S calculated Sa b 1 T 1 1 b A1 Table 2 shows common tconorms derived Equation A1 tnorms Table 1 alongside optional properties tnorms Deﬁnition 15 A4 Aggregation operators The functions compute quantiﬁers like aggregation operators 45 Deﬁnition 17 An aggregation operator 10 function A argument A0 0 0 A1 1 1 cid7 nN 0 1n 0 1 nondecreasing respect Aggregation operators variadic functions functions deﬁned sequence arguments For i1 xi Ax1 xn Table 3 shows common aggregation operators reason use notation An talk Furthermore consider symmetric aggregation operators invariant permutation sequence The quantiﬁer interpreted conjunction arguments x Therefore extend tnorm T 2dimensional inputs ndimensional inputs commutative associative 40 A T 0 A T x1 x2 xn T x1 A T x2 xn A2 These operators straightforward choice modeling quantiﬁer seen series conjunctions All operators constructed way symmetric aggregation operators output value ordering arguments This generalizes commutativity We tconorm S model quantiﬁer E S 0 E S x1 x2 xn Sx1 A S x2 xn A5 Fuzzy implications A3 The functions compute truth value p q called fuzzy implications p called antecedent q consequent implication We follow 38 refer details proofs Deﬁnition 18 A fuzzy implication function I 0 12 0 1 c 0 1 I c decreasing Ia increasing I0 0 1 I1 1 1 I1 0 0 From deﬁnition follows I0 1 1 Deﬁnition 19 Let N fuzzy negation A fuzzy implication I satisﬁes leftneutrality LN c 0 1 I1 c c b exchange principle EP b c 0 1 Ia Ib c Ib Ia c c identity principle IP 0 1 Ia 1 d Ncontrapositive symmetry CP c 0 1 Ia c INc Na e Nleftcontrapositive symmetry LCP c 0 1 INa c INc f Nrightcontrapositive symmetry RCP c 0 1 Ia Nc Ic Na All statements generalize law classical logic Left neutrality generalizes 1 p p exchange principle generalizes p q r q p r identity principle generalizes p p tautology Furthermore N contrapositive symmetry generalizes p q q p Nleftcontrapositive symmetry generalizes p q q p Nrightcontrapositive symmetry generalizes p q q p 32 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 A51 Simplications In classical logic material implication deﬁned follows p q p q Using deﬁnition use tconorm S fuzzy negation N construct fuzzy implication Deﬁnition 20 Let S tconorm N fuzzy negation The function I SN 0 12 0 1 called S Nimplication deﬁned c 0 1 I SN c SNa c A4 If N strong fuzzy negation I SN called Simplication strong implication As consider strong negation NC omit N use I S refer I SNC All Simplications I S fuzzy implications satisfy LN EP RCP Additionally negation N strong satisﬁes CP addition strict satisﬁes LCP In Table 4 Simplications use strong fuzzy negation NC common tconorms Table 2 Note Simplications rotations tconorms A52 Rimplications Rimplications way constructing implication operators They standard choice tnorm fuzzy logics Deﬁnition 21 Let T tnorm The function I T 0 12 0 1 called Rimplication deﬁned I T c supb 0 1T b c A5 The supremum set A denoted sup A lowest upper bound A All Rimplications fuzzy implications satisfy LN IP EP T leftcontinuous tnorm supremum replaced maximum function Note c I T c 1 We looking Equation A5 The largest value b possible 1 neutrality property tnorms T 1 c Table 5 shows Rimplications created common Tnorms Note I L K I F D appear tables They Simplications Rimplications Appendix B Implementation Differentiable Fuzzy Logics The computation satisfaction shown pseudocode form Algorithm 1 By ﬁrst computing dictionary g contains truth values ground atoms12 reduce forward passes computations truth values ground atoms required compute satisfaction This algorithm fairly easily parallelized eﬃcient computation GPU noting individual terms aggregated lines 12 14 different instances quantiﬁers dependent By noting formulas prenex normal form set dictionary g tensor operations recursion formula This applying fuzzy operators elementwise vectors truth values instead single truth value element vector represents variable assignment The complexity computation O K P bd K set formulas P predicates formula d maximum depth nesting universal quantiﬁers formulas K known quantiﬁer rank This exponential quantiﬁers object constants C iterated lines 12 14 mentioned earlier mitigated somewhat eﬃcient parallelization Still computing valuation transitive rules x y z Qx z Rz y Px y example far demanding antisymmetry formulas x y Px y P y x Appendix C Proofs C1 Singlepassing Proposition 7 Any composition singlepassing functions singlepassing 12 The dictionary g seen fuzzy Herbrand interpretation assigns truth value ground atoms 33 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 ϕ φ ϕ φ ψ ϕ φ ψ ϕ φ ψ return Neφ g C μ ϕ Px1 xm return gP μx1 μxm return Ieφ g C μ eψ g C μ return Seφ g C μ eψ g C μ return T eφ g C μ eψ g C μ cid23 The valuation function computes Fuzzy truth value ϕ cid23 Find truth value ground atom dictionary g Algorithm 1 Computation Differentiable Fuzzy Logics loss First computes fuzzy Herbrand interpretation g given current embedded interpretation ηθ This performs forward pass neural networks interpret predicates Then computes valuation formula ϕ knowledge base K implementing Equations 38 1 function eϕ g C μ 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 end function 18 19 procedure DFLηθ P K O N T S I A E C o1 ob sampled O 20 g dict 21 P P 22 23 24 25 26 27 28 end procedure end return AϕK wϕ eθ ϕ g C cid23 Calculate valuation formulas ϕ Start variable assignment This implements Equation 11 cid23 Computes Differentiable Fuzzy Logics loss cid23 Sample b constants use pass cid23 Collects truth values ground atoms return AoC eφ g C μ x o return EoC eφ g C μ x o cid23 Apply universal aggregation operator cid23 Each assignment seen instance ϕ cid23 Calculate truth values ground atoms gP o1 oαP ηθ Po1 oαP o1 oαP C ϕ x φ ϕ x φ end end Proof We prove structural induction Let f Rn R singlepassing function let x1 xn R Then clearly f x1 xn singlepassing Next assume induction g Rn R composition singlepassing functions assume single passing Let y Rm R singlepassing function Let Z set inputs y deﬁne xi yZ We composition g x1 yZ xn singlepassing For z Z holds g x1 xn z g x1 xn xi yZ z C1 As g singlepassing 1 number j 1 n gx1xn k 1 m gx1xn If j assumption yZ singlepassing 1 k 1 m y Z z Equation C1 1 input gx1xn singlepassing cid2 cid16 0 If 0 0 If 1 j cid16 direct input x j cid16 0 cid16 0 We conclude composition g x1 yZ xn cid16 0 gx1xn x j xi x z C2 Nonvanishing fractions C21 Łukasiewicz aggregator Proposition 8 The fraction inputs x1 xn 0 1 derivative A T LU nonvanishing 1 n Proof Consider standard uniformly distributed random variables x1 xn U 0 1 The sum Y distributed 3429 The cumulative density function distribution F Y y 1 n cid27 ycid28cid12 1k k0 cid26 y kn1 cid25 n k cid4 n i1 xi IrwinHall C2 cid27cid28 ﬂoor function The derivative A T LU nonvanishing Y n 1 equivalently IrwinHall distribution symmetric Y 1 Using F Y gives F Y 1 1 n cid6 1 0n 11 cid6 11 1n cid5 10 n cid2 cid5 n 1 cid5 n 0 1 cid6 This result bounded sum aggregator condition Y 1 34 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 C22 Yager aggregator Proposition 9 The fraction inputs x1 xn 0 1 derivative A T Y p 2 nonvanishing n 2 π 2 n 1 2ncid10 1 2 cid10 Gamma function Proof The points x1 xn R volume 5p5 cid4 n i11 xi2 1 hold describes volume nball13 radius 1 This V n 2 π n 2 n 1 cid10 1 C3 We interested volume x1 xn 0 1 single orthant14 nball The orthants nball lies 2n15 Thus volume nball x1 xn 0 1 V n As total volume points lying 0 1n 1 fraction points derivative A T Y p 2 nonvanishing cid2 π 2 n 1 2ncid10 1 2 2n n 2 C23 Nilpotent aggregator Proposition 10 The fraction inputs x1 xn 0 1 derivative A TnM nonvanishing 1 2n1 Proof Consider n standard uniformly distributed random variables x1 xn U 0 1 We interested probability x1 x2 1 xk kth smallest sample known kth order statistic 15 Weisberg 73 derives cumulative density function linear combinations standard uniform order statistics Let k0 0 k1 kS kn i1 di xi v Let rs ks ks1 S integers indicating coeﬃcients di 0 We aim calculate probability 1 S let r S1 n kS Finally let c S1 0 cs cs1 di m largest integer v cm Then Weisberg 73 ﬁnds cid4 S cid2 Scid12 P dsxks v s1 mcid12 g s1 cid6 cid5 rs1 cs s rs 1 g s ith order derivative gsc c vn S1 i1icid16sc ciri cid3 c C4 C5 Filling case ﬁnd S 2 k1 1 k2 2 d1 d2 1 Therefore r1 r2 1 r3 n 2 c1 2 c2 1 c3 0 The largest integer m 1 cm 2 Filling ﬁnd P x1 x2 1 g 11 2 1 1 1 g 11 1 2 1 1 2 1n 22 112 0n2 1 1n 11 21 0n2 1 2n1 cid2 C6 C7 C3 Implications Proposition 11 If fuzzy implication I NC contrapositive symmetric NC strong negation contra positive differentiable symmetric Proof Say implication I NC contrapositive symmetric Because I NC contrapositive symmetric I1 c 1 Ia c Thus I1c1a Iac cid2 1c Iac 1c c 13 An nball generalization concept ball dimension region enclosed n 1 hypersphere For example 3ball ball surrounded sphere 2sphere Similarly 2ball disk surrounded circle 1sphere A hypersphere radius 1 set points distance 1 center 14 An orthant n dimensions generalization quadrant dimensions octant dimensions 15 To help understand consider n 2 The 1ball circle center 0 0 The area circle evenly distributed quadrants 35 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Proposition 12 If implication I leftneutral I1c Ia0 1 c 1 If addition I contrapositive differentiable symmetric Proof First assume I leftneutral Then c 0 1 I1 c c Taking derivative respect c clear I1c 1 As 1 c 0 1 Ia0 1 Next assume I contrapositive differentiable symmetric Then I1c I1c11 I1c0 1 cid2 1c 1c c c Appendix D Derivations functions D1 perror aggregators The unbounded Yager aggregator AU Y x1 xn 1 cid19 ncid12 1 xip cid20 1 p p 0 D1 i1 We aﬃne transformation w AU Y x1 xn h function ensure boundary conditions Deﬁnition 17 hold w AU Y 0 0 h 0 w AU Y 1 1 h 1 Solving Equation D2 D3 h ﬁnd cid19 w 1 cid20 1 p ncid12 1 0p h 0 i1 cid23 w 1 n 1 p cid24 h 0 w 1 cid19 ncid12 1 1p h w w p n h 1 cid20 1 p i1 w 1 0 h 1 h w 1 Equating D4 D5 solving h ﬁnd w w p n w 1 w 1 p n And h 1 p n 1 Filling simplifying ﬁnd A p E x1 xn 1 p n 1 cid19 ncid12 1 xip i1 cid20 1 p cid25 cid26 1 p n 1 cid19 ncid12 1 xip cid20 1 p i1 1 1 p n cid19 1 n 1 ncid12 1 xip i1 cid20 1 p cid5cid4 n i1 x p cid6 1 p p 0 36 Similarly tconorm AU Y S x1 xn D2 D3 D4 D5 D6 D7 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 cid19 ncid12 i1 w cid20 1 p 0p h 0 w 0 h 0 h 0 cid20 1 p 1p h 1 cid19 ncid12 i1 w w n 1 p h 1 w 1 p n cid19 A pM E AN x1 xn 1 n D2 Sigmoidal functions cid20 1 p ncid12 i1 x p p 0 D8 D9 D10 In Machine Learning logistic function sigmoid function σ x 1 common activation function 25p65 66 This inspired 71 introduce parameterized families aggregation functions MaxSigmoid activation functions 1ex cid19 cid19 cid20cid20 cid19 cid19 cid20cid20 cid15 σ x1 xn σ A s xi n 1 b0 cid15 σ x1 xn σ A s xi b0 D11 ncid12 i1 ncid12 i1 We generalize transformation function f 0 1n R symmetric increasing A cid15 σ f x1 xn σ s f x1 xn b0 D12 This aggregation function according Deﬁnition 17 σ 0 1 boundary conditions cid15 cid15 σ 1 1 1 A σ 0 0 hold We solve adding linear parameters w h redeﬁning A σ f σ f x1 xn w σ s f x1 xn b0 h D13 For need sure lowest value f domain 0 1n maps 0 highest 1 For deﬁne inf f inf f x1 xnx1 xn 0 1 sup f sup f x1 xnx1 xn 0 1 This gives following equations Aσ 0 0 w σ s inf f b0 h 0 Aσ 1 1 w σ s sup f b0 h 1 First solve equations w starting Equation D14 w σ s inf f b0 h 0 h w w h 1 e sinf f b0 1 e 1 sinf f b0 Likewise Equation D15 w σ s sup f b0 h 1 1 1 e ssup f b0 1 h w Now solve h equating Equations D17 D16 Then w 1 h 1 e ssup f b0 37 D14 D15 D16 D17 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 1 h 1 e ssup f b0 h 1 e sinf f b0 h 1 e ssup f b0 1 e sinf f b0 1 e ssup f b0 We following formula Aσ x1 xn e ssup f b0 1 e sinf f b0 e ssup f b0 cid23cid23 1 e cid24 sinf f b0 σ s f x1 xn b0 1 D18 cid24 If f fuzzy logic operator outputs 0 1 straightforward choice b0 1 2 This uses symmetric sigmoid function For function f translates outputs f 1 0 1n 0 1 ﬁnd following simpliﬁcation noting supremum f 1 inﬁmum 0 2 1 2 σ f x1 xn 1 e es0 1 2 e s1 1 2 s1 1 2 cid25 cid24 cid25cid23 1 e s0 1 2 σ cid25 s cid25cid23 f x1 xn 1 2 cid25 cid24 cid25 cid26cid26 cid26 1 s 2 σ s cid25 cid24 s 2 σ s 1 e cid25cid23 1 e cid25 cid25 f x1 xn 1 2 cid25 f x1 xn 1 2 cid26cid26 cid26 cid26cid26 cid26 1 1 e s 2 σ s f x1 xn 1 2 1 s s 2 1 e 2 1 e s 2 1 e 2 e e s s 2 s 2 e 2 e s s 2 e s 2 e cid25cid23 e 1 e s 2 1 s 2 1 cid24 Next prove properties sigmoidal implication Proposition 13 For a1 c1 a2 c2 0 1 1 Ia1 c1 Ia2 c2 σI a1 c1 σI a2 c2 2 Ia1 c1 Ia2 c2 σI a1 c1 σI a2 c2 D19 D20 D21 D22 cid24 2 cid26cid26 cid26 1 cid23 Proof 1 We note σI written σI c w σ s Ia c b0 h constants w s1b0 s1b0 As s 0 s b0 s 1 b0 Therefore e sb0 e 1 e 1e sb0 e cid23 e 1 e 0 w 0 As s 0 s Ia1 c1 b0 s Ia2 c2 b0 Ia1 c1 Ia2 c2 Next note sigmoid function σ monotonically increasing function Using w 0 ﬁnd σI a1 c1 w σ s Ia1 c1 b0 w σ s Ia2 c2 b0 σI a2 c2 e s1b0 0 Furthermore e s1b0 h 2 0 certainly s1b0 0 0 As e sb0 e cid24 cid24 2 s 2 s 2 cid23 2 s1b0 1e sb0 e s 2 σI a1 c1 w σ s Ia1 c1 b0 h w σ s Ia2 c2 b0 h σI a2 c2 cid2 Proposition 14 σI c 1 Ia c 1 Similarly σI c 0 Ia c 0 Proof Assume c 0 1 Ia c 1 By construction σI c 1 Appendix D2 Now assume a1 c1 0 1 σI a1 c1 1 Now consider a2 c2 Ia2 c2 1 By construction σI σI a2 c2 1 For sake contradiction assume Ia1 c1 1 However Proposition 13 Ia1 c1 Ia2 c2 σI a1 c1 σI a2 c2 hold This contradiction σI a1 c1 σI a2 c2 1 assumption Ia1 c1 1 wrong Ia1 c1 1 The proof Ia c 0 analogous cid2 Proposition 15 For fuzzy implications I σI fuzzy implication Proof By Deﬁnition 18 I c decreasing Ia increasing Therefore Proposition 131 σI c decreasing σI increasing Furthermore I0 0 1 I1 1 1 I1 0 0 We ﬁnd Proposition 14 σI 0 0 1 σI 1 1 1 σI 1 0 0 cid2 38 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 I sigmoidal implications satisfy leftneutrality I leftneutral s approaches 0 Proposition 16 If fuzzy implication I contrapositive symmetric respect N σI Proof Assume implication I contrapositive symmetric c 0 1 Ia c INc Na By Proposition 132 σI c σI Nc Na Thus σI contrapositive symmetric respect N cid2 By proposition I Simplication σI contrapositive symmetric contrapositive differentiable symmetric Proposition 17 If I satisﬁes identity principle σI satisﬁes identity principle Proof Assume fuzzy implication I satisﬁes identity principle Then Ia 1 By Proposition 14 holds σI 1 cid2 D3 Nilpotent aggregator Proposition 18 Equation A2 equal Nilpotent tnorm cid2 A TnM x1 xn minx1 xn 0 xi x j 1 xi x j lowest values x1 xn D23 Proof We prove induction Base case Assume n 2 Then A TnM x1 x2 TnM x1 x2 x1 x2 lowest values x1 x2 condition Equation D23 change x1 x2 1 Inductive step We assume Equation D23 holds n 2 Then Equation A2 A TnM x1 xn1 TnM A TnM x1 xn xn1 Note A TnM x1 xn 0 A TnM x1 xn1 0 xn1 0 1 0 xn1 1 hold We identify cases 1 If xn1 lowest value x1 xn1 A TnM x1 xn second lowest value x1 xn1 0 If 0 sum second lowest values greater 1 sum lowest values If TnM A TnM x1 xn xn1 ﬁrst compares A TnM x1 xn xn1 1 sum lowest values x1 xn1 higher 1 returns xn1 holds 0 2 If xn1 second lowest value x1 xn1 A TnM x1 xn lowest value x1 xn1 0 If 0 sum ﬁrst lowest values greater 1 sum lowest values If TnM A TnM x1 xn xn1 ﬁrst compares A TnM x1 xn xn1 1 sum lowest values x1 xn1 higher 1 returns A TnM x1 xn holds 0 3 If xn1 lowest second lowest value x1 xn1 sum s lowest values x1 xn sum lowest values x1 xn1 If A TnM x1 xn 0 s greater 1 A TnM x1 xn1 0 If s 1 A TnM x1 xn lowest value surely A TnM x1 xn xn1 1 xn1 large second lowest value cid2 By considering E SnM x1 xn 1 A TnM 1 x1 1 xn easy cid2 E SnM x1 xn maxx1 xn 1 xi x j 1 xi x j largest values x1 xn D24 D4 Yager Rimplication The Yager tnorm deﬁned T Y b 1 cid5 1 ap 1 bp cid6 1 p The Yager Rimplication deﬁned Deﬁ nition 21 I T Y c supb 0 1T Y b c When c I T Y 1 T Y 1 c Assuming c ﬁnd ﬁlling I T Y c supb 0 11 cid5 1 ap 1 bp cid6 1 p c c To closedform solution I T Y ﬁnd largest b 1 inequality b ﬁnd 39 D25 D26 cid5 1 ap 1 bp cid6 1 p c Solving E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 cid5 c 1 1 ap 1 bp cid6 1 p 1 cp 1 ap 1 bp cid5 1 b 1 cp 1 ap cid6 1 p cid5 cid6 1 b 1 1 cp 1 ap D27 If c 1 cp 1 ap 1 cp 1 ap 0 Furthermore c 0 1 1 cp 1 ap 1 cid6 1 Therefore true 1 p 0 1 The largest value b 0 1 condition holds equal 1 cid6 1 p 0 1 satisﬁes inequality cid5 1 cp 1 ap 1 cp 1 ap cid5 p Combining earlier observation c I T Y cid2 I T Y c 1 1 cid5 1 cp 1 ap cid6 1 c p 1 ﬁnd following Rimplication D28 p 2 Fig 8 As expected p 1 reduces Łukasiewicz implication The derivatives We plot I T Y implication cid2 cid2 I T Y c 1 cp 1 ap 0 1 p 1 1 c c I T Y c 1 cp 1 ap 0 Appendix E Differentiable Product Fuzzy Logic 1 p 1 1 c D29 D30 We compare Differentiable Product Fuzzy Logic DPFL uses product t tconorm T P S P Reichenbach implication I RC logproduct aggregator Alog T P probabilistic logic method called Semantic Loss 74 Deﬁnition 22 Let P set predicates O domain discourse ηθ embedded interpretation L K knowledge base background knowledge The Semantic Loss deﬁned cid13 cid12 cid13 LS θ K log ηθ Po1 om wK wPo1om wPo1om 1 ηθ Po1 om E1 w world known Herbrand interpretation assigns binary truth value ground atom ηθ Po1 om probability ground atom This computes logarithm sum probabilities worlds K holds probability sampling world consistent K The probability ground atom Po1 om true ηθ Po1 om The different ground atoms assumed independent By marginalizing world w injecting background knowledge unsupervised semisupervised learning Compared DPFL Semantic Loss exponential size ground atoms sum valid worlds computed Equivalent formulas equal Semantic Loss knowledge base consisting conjunction facts equal crossentropy loss function DPFL connected interesting way Semantic Loss It corresponds single iteration loopy belief propaga tion algorithm 55 Proposition 19 Let ϕ closed formula P1o11 o1m P2o21 o2m ground atoms appearing ϕ P1o11 o1m P2o21 o2m Then holds LS θ ϕ eθ ϕ T T P S S P I I RC A Alog T P As loops ground atom appears uniquely ϕ factor graph loopy belief propaga tion tree As eθ ϕ corresponds single iteration loopy belief propagation equal regular belief propagation exact method computing queries probabilistic models 58 Clearly condition ϕ strong Although loopy belief propagation known good approximation empirically 55 degree DPFL approximates Semantic Loss requires research guarantee However DPFL approximates Semantic Loss strong alternative exponential computation However means problems DPFL present Semantic Loss For example formula ravenx blackx grounding knowledge base contain repeated ground atoms Semantic Loss DPFL equivalent share diﬃculties related imbalance modus ponens modus tollens 40 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 E1 Proof In section prove Proposition 19 Without loss generality assume K ϕ x1 xn φ Slightly rewriting Equation E1 ﬁnd probability distribution Semantic Loss pϕηθ pϕwpwηθ E2 cid12 w deﬁne valuation probability pϕw Iw ϕ world probability cid13 cid13 pwηθ ηθ Po1 ok wPo1ok wPo1ok 1 ηθ Po1 ok cid3 n cid3 A Bayesian network joint distribution factorized px i1 pxixpai xpai set random vari ables parents xi In particular interested joint distribution pϕ wηθ We use compositional structure ϕ expand pϕw Let pφchφ probability φ subformula ϕ true according binary truth table conditioned truth value direct subformulas chφ φ For example φ α β pφα β 1 α β 1 pφα β 0 For atomic formula Po1 ok lookup world w pPo1 okw Po1ok w Po1ok Let cid14 set subformulas ϕ We express joint distribution pcid14 wηθ pwηθ pφchφ E3 cid13 φcid14 cid3 φcid14 pφchφ 1 Note distribution pϕw A speciﬁc world w uniquely determines single cid14 φcid14 pφchφ forms polytree directed tree logical expression formed tree From Bayesian net work deﬁne factor graph belief propagation For brevity denote speciﬁc ground atom Po1 ok PO We note use m denote instantiation instead μ section μ denote messages customary belief propagation There variable node w PO ground atom PO appearing grounding ϕ Additionally factor node f wPo1ok w PO ηθ Po1 okwPO 1 ηθ Po1 ok1wPO There variable node φ factor node fφ subformula φ cid14 For φ PO φ cid14 fφφ w PO Iφ w PO For φ α φ cid14 fφφ α Iφ 1 α For φ α β φ cid14 fφφ α β Iφ α β Let ϕ x1 xn φ node Denote set instances ϕ M Then fϕϕ m1 mM Iϕ cid3 mM αm eμ random variable corresponding instantiation m φ We ignore connectives formed classical logic DPFL Next com pute messages belief propagation We start world variable nodes w P O computation tree ϕ The messages factors variables given 7 μ f sxx yne f sx μ y f s y nex set neighbors node x The messages variables factors given μx f s x X f sx X μ flxx cid4 cid3 cid3 lnex f s μ f wPO b μw PO w PO fφ w PO μ f wPO w PO ηθ Po1 okw PO 1 ηθ Po1 ok1wPO factor variable ground atom cid3 w PO w PO w PO ground atom variable atomic formula factor φ Po1 ok We assume incoming messages μ fψ w PO atomic formulas ground atom PO initialized 1 We able compute graph contain loops ψψPO φcid16ψ μ fψ w PO c μ fφ φφ Iφ 1μw PO able atomic formulas d μφ fα α μ fααα subformula variables factors subformulas α As φ factors fφ 0 ηθ Po1 okφ 1 ηθ Po1 ok1φ factor vari fφ 1 Iφ 0μw PO w PO μ f wPO simply passes downstream message e μ fφ φφ μ fαα11φ μ fαα0φ factor variable negated subformulas φ α f μ fφ φφ cid6 cid5 μ fαα1 μ fβ β 1 cid6 cid5cid3 αm 1 tor variable conjunctions φ α β mM μ fαm cid6 cid5 μ fαα0 μ fβ β 0 μ fαα1 μ fβ β 0 μ fαα0 μ fβ β 1 cid24 αm αm factor universally quan tiﬁed formula ϕ x1 xn φ αm subformula corresponding instantiation m The second term sum cases subformula truth value 0 α1αMiαi 0 mM μ fαm g μ fϕ ϕϕ 1φ fac cid23cid4 1ϕ ϕ φ cid3 41 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 cid3 pφ We wish know marginal probability pϕ 1ηθ A marginal variable φ factor graph sneφ μ f sxφ The variable node ϕ factor node fϕ neighbor g ﬁnd16 pϕ 1ηθ cid13 mM μαm fϕ 1 E4 Next use induction prove computation μ fαm αm 1 equal Differentiable Product Fuzzy Logic Let φ subformula αm instantiation m ϕ Let eθ valuation function Deﬁnition 2 T T P N NC We prove μ fφ φ1 eθ m φ μ fφ φ0 1 eθ m φ Base case φ PO By c μ fφ φ1 ηθ Po1 ok μ fφ φ0 1 ηθ Po1 ok By Equation 317 eθ m φ ηθ Po1 ok Inductive step φ α By e inductive hypothesis μ fφ φ1 μ fαα0 1 eθ α m μ fφ φ0 μ fαα1 eθ m α By Equation 4 eθ m φ 1 eθ m α Inductive step φ α β By f inductive hypothesis μ fφ φ1 μ fαα1 μ fβ β 1 eθ m α eθ m β μ fφ φ0 μ fαα0 μ fβ β 0 μ fαα1 μ fβ β 0 μ fαα0 μ fβ β 1 1 eθ m α1 eθ m β eθ m α1 eθ m β 1 eθ m αeθ m β 1 eθ m α eθ m β By Equation 5 T P c c eθ m φ eθ m α eθ m β Using Equation E4 ﬁnd pϕ 1ηθ Logic computation universal quantiﬁer Equation 8 cid3 mM eθ m φm equal Differentiable Product Fuzzy Importantly computation logic tree loops caused ground atoms appearing multiple subformulas Therefore ground atom appears single formula Differentiable Product Fuzzy Logic computes probability Semantic Loss E2 Example For example formula P corresponds variable node P possible values 1 0 factor node factor ηθ P P true 1 ηθ P If consider formula γ P P Q ﬁnd following factor graph IQ w Q f Q Q v Q Iα P Q fα P v P1 IP1 w P f P1 w P v w P IP2 w P f P2 v w Q w Q f w Q ηθ Q ηθ P f w P v P2 P vα P Q vβ Iβ 1 α vβ P Q fγ Iγ P β vγ P P Q Here box nodes correspond factor nodes circle nodes correspond variable nodes As γ formula messages passed Note single loop present atom P twice f P2 The ﬁrst incorrect access f P1 μv wP formula This causes incorrect messages μv wP incoming message μ f P2v wP With Differentiable Product Fuzzy Logic ﬁnd expression ηθ P 1 ηθ P ηθ Q correct prob puts 1 ability P 1 Q 0 ηθ P 1 ηθ Q Appendix F Additional experiments In Appendix report additional experiments problem Throughout Appendix use vanilla stochastic gradient descent learning rate 001 05 momentum instead ADAM The reason best conﬁgurations perform better optimizer conﬁgurations perform better ADAM 16 We use approximation equality simpliﬁcation Note necessarily true loopy belief propagation 17 Equation 3 refers ungrounded case grounding lookup function l required 42 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Table F11 Conﬁgurations RMSE aggregator w D F L 1 log product aggregator wdf l 10 Accuracy T G T L K T P T Y p 2 T Y p 20 T Nm 963 966 967 964 960 954 Alog T P w D F L 10 cucons cons 089 033 048 054 051 044 010 05 044 040 029 029 cuant 097 067 069 074 078 084 Accuracy 962 969 970 966 959 954 A R M S E w D F L 1 cucons cons 089 006 081 087 082 062 010 05 008 012 018 003 cuant 097 095 098 097 098 099 F1 Comparing different formulas problem We note values cucons cuant roughly formulas pick random 1 x y zerox zero y samex y x y ninex y samex y For formula cucons 1 10 100 1 minus probability x y zero The modus 10 cases distrust option 100 cases modus tollens casein 9 distribution samex y18 cuant 99 ponens case true 1 9 100 cases 2 x y zerox samex y zero y x y ninex samex y y For formula cucons 1 100 The modus ponens cases true 1 100 cases probability digit represents zero cuant 99 10 cases distrust option 9 modus tollens 9 10 100 cases 3 x y samex y y x As biimplication cucons 1 10 cuant 9 10 The distrust option possible formula From set operators better random guessing consequent updates cucons 01 It diﬃcult value cuant good random guessing probabilities upper bounded lowest bound 09 We know set operators better random cuant 099 F2 Varying aggregators In section analyze symmetric conﬁgurations problem use aggregators formed extending tnorm In particular consider RMSE aggregator AG M E p 2 logproduct aggregator Alog T P Table F11 shows results RMSE aggregator DFL weight 1 log product aggregator DFL weight 10 Nearly conﬁgurations perform signiﬁcantly better aggregators symmetric aggregator In particular Gödel Łukasiewicz Yager tnorms outperform baseline aggregators differentiable handle outliers The product tnorm slightly worse RMSE aggregator logproduct aggregator Like discussed Section 85 cons higher aggregator corners ai 0 ci 0 ai 1 ci 1 gradient RMSE aggregator However values cucons cuant lower logproduct aggregator This previously point As longer gradient 1 corners 0 c 0 1 c 1 large gradients agent conﬁdent prediction This case inherently riskier contributes information It informative increase conﬁdence 0 low The Łukasiewicz tnorm particularly high accuracy 969 log product level performance product tnorm However low value cucons 006 relatively low value cuant Interest ingly conﬁguration cucons higher RMSE aggregator logproduct aggregator F3 Reichenbachsigmoidal implication The newly introduced Reichenbachsigmoidal implication σI RC promising candidate choice implication argued Section 851 To better understanding implication investigate effect parameters problem We ﬁx aggregator logproduct conjunction operator Yager tnorm p 2 use DFL weight w D F L 10 18 It slightly 1 10 minibatch examples Therefore reﬂexive pairs samex x common 43 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 Fig F18 The results Reichenbachsigmoidal implication σI RC Alog T P aggregator T Y p 2 w D F L 10 Left shows results values s keeping b0 ﬁxed 05 right shows results values b0 keeping s ﬁxed 9 Table F12 The results T Y p 2 conjunction Alog T P Rimplications These run vanilla SGD instead ADAM aggregator w 1 10 Simplications Accuracy cons I K D I L K I RC I Y p 2 I F D 961 970 969 966 963 010 05 008 012 007 cucons 088 003 085 087 065 cuant 097 097 099 097 096 Accuracy cons I G I L K I G G I T Y I F D I T Y p 05 906 970 940 954 963 963 1 05 086 058 007 018 cucons 007 003 001 003 065 065 cuant 097 097 099 096 037 Table F13 implication s 9 The results σI RC 2 T Y p 2 conjunction Alog T P b0 1 aggregator w D F L 10 leaving formulas problem The numbers indicated formulas present training Formulas Accuracy cons 1 2 2 3 1 3 1 2 3 971 959 963 956 952 958 005 012 015 005 003 019 cucons 054 075 052 059 078 064 cuant 099 095 098 100 099 095 On left plot Fig F18 ﬁnd results experiment parameter s keeping b0 ﬁxed 1 2 Note s approaches 0 Reichenbachsigmoidal implication I RC The value 9 gives best results 973 accuracy This outperforms implications vanilla SGD optimizer displayed Table F12 Interestingly clear trends values cons cucons cuant Increasing s increase cons This antecedent derivative corner 0 c 0 low argued Section 851 When s increases corners smoothed Furthermore cucons cuant decrease s increases This corners derivatives small Updates corner likely correct model conﬁdent For higher value s gradient magnitude instances model conﬁdent We note happened RMSE aggregator product tnorm Regardless best parameter value clearly values cucons cuant highest Reichenbach implication On right plot Fig F18 experiment value b0 Clearly 1 2 works best having highest accuracy cucons F4 Inﬂuence individual formulas Finally compare inﬂuence different formulas problem Table F13 Removing reﬂexivity formula 3 largely impact performance The biggest drop performance removing formula 1 deﬁnes predicate Using formula 1 gets slightly better performance formula 2 despite fact positive labeled examples formula 1 predicates zero consequent Since 95 derivatives respect negated antecedent formula contributes ﬁnding 44 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 additional counterexamples Furthermore improving accuracy predicate improves accuracy digit recognition Just reﬂexivity formula 3 highest accuracy individually use digit predicates References 1 E Arakelyan D Daza P Minervini M Cochez Complex query answering neural link predictors International Conference Learning Repre sentations 2021 preprint arXiv1906 06576 2019 2 S Badreddine A dAvila Garcez L Seraﬁni M Spranger Logic tensor networks arXiv2012 13635 cs httparxivorg abs 2012 13635 2020 3 S Badreddine M Spranger Injecting prior knowledge transfer learning reinforcement learning algorithms logic tensor networks arXiv 4 H Bal D Epema C Laat R van Nieuwpoort J Romein F Seinstra C Snoek H Wijshoff A mediumscale distributed science research infrastructure long term Computer 49 2016 5463 5 K Ball An elementary introduction modern convex geometry Flavors Geometry vol 31 1997 pp 158 6 TR Besold Ad Garcez S Bader H Bowman P Domingos P Hitzler KU Kuehnberger LC Lamb D Lowd PMV Lima L Penning G Pinkas H Poon G Zaverucha Neuralsymbolic learning reasoning survey interpretation arXiv preprint arXiv171103902 httparxivorg abs 1711 03902 2017 7 CM Bishop Pattern Recognition Machine Learning 2006 8 A Brock J Donahue K Simonyan Large scale GAN training high ﬁdelity natural image synthesis arXiv preprint arXiv1809 11096 2018 9 M Buda A Maki MA Mazurowski A systematic study class imbalance problem convolutional neural networks Neural Netw 106 2018 10 T Calvo A Kolesárová M Komorníková R Mesiar Aggregation operators properties classes construction methods T Calvo G Mayor R Mesiar Eds Aggregation Operators New Trends Applications PhysicaVerlag HD Heidelberg 2002 pp 3104 11 R Cignoli F Esteva L Godo F Montagna On class leftcontinuous tnorms Fuzzy Sets Syst 131 2002 283296 httpsdoi org 10 1016 S0165 12 P Cintula P Hájek C Nogura Handbook Mathematical Fuzzy Logic vol 1 College Publications 2011 13 A Daniele L Seraﬁni Knowledge enhanced neural networks AC Nayak A Sharma Eds PRICAI 2019 Trends Artiﬁcial Intelligence Springer International Publishing Cham 2019 pp 542554 14 A Darwiche SDD new canonical representation propositional knowledge bases IJCAI International Joint Conference Artiﬁcial Intelligence 249259 011401 00215 9 2011 pp 819826 15 HA David HN Nagaraja Order Statistics edition Encyclopedia Statistical Sciences 2003 16 T Demeester T Rocktäschel S Riedel Lifted rule injection relation embeddings Proceedings 2016 Conference Empirical Methods Natural Language Processing Association Computational Linguistics 2016 pp 13891399 httpaclweb org anthology D16 1146 17 M Diligenti M Gori C Sacca Semanticbased regularization learning inference Artif Intell 244 2017 143165 18 M Diligenti S Roychowdhury M Gori Integrating prior knowledge deep learning Machine Learning Applications ICMLA 2017 16th 19 I Donadello L Seraﬁni Ad Garcez Logic tensor networks semantic image interpretation IJCAI International Joint Conference Artiﬁcial IEEE International Conference IEEE 2017 pp 920923 Intelligence 2017 pp 15961602 httparxivorg abs 1705 08968 20 R Evans E Grefenstette Learning explanatory rules noisy data J Artif Intell Res 61 2018 65170 httpsdoi org 10 1613 jair5477 21 K Ganchev J Gillenwater Posterior regularization structured latent variable models J Mach Learn Res 11 2010 20012049 httpdl acm org citation cfm id 1756006 1859918 22 Ad Garcez KB Broda DM Gabbay NeuralSymbolic Learning Systems Foundations Applications Springer Science Business Media 2012 23 M Garnelo K Arulkumaran M Shanahan Towards deep symbolic reinforcement learning arXiv preprint arXiv1609 05518 2016 24 L Getoor B Taskar Introduction Statistical Relational Learning vol 1 MIT Press Cambridge 2007 25 I Goodfellow Y Bengio A Courville Y Bengio Deep Learning vol 1 MIT Press Cambridge 2016 26 S Guo Q Wang L Wang B Wang L Guo Jointly embedding knowledge graphs logical rules Proceedings 2016 Conference Empirical Methods Natural Language Processing 2016 pp 192202 27 AK Gupta S Nadarajah Handbook Beta Distribution Its Applications CRC Press 2004 28 P Hájek The Metamathematics Fuzzy Logic Kluwer 1998 29 P Hall The distribution means samples size N drawn population variate takes values 0 1 values equally probable Biometrika 19 1927 240245 httpwwwjstororg stable 2331961 30 S Harnad The symbol grounding problem Physica D 42 1990 335346 31 CG Hempel Studies Logic Conﬁrmation II Mind vol 54 1945 pp 97121 httpwwwjstororg stable 2250948 32 Z Hu X Ma Z Liu E Hovy E Xing Harnessing deep neural networks logic rules Proceedings 54th Annual Meeting Association Computational Linguistics Volume 1 Long Papers Association Computational Linguistics 2016 pp 24102420 httpaclweb org anthology P16 1228 33 CL Hwang ASM Masud Multiple Objective Decision MakingMethods Applications A StateoftheArt Survey vol 164 Springer Science Business Media 2012 34 JO Irwin On frequency distribution means samples population having law frequency ﬁnite moments special reference Pearsons type II Biometrika 1927 225239 35 JSR Jang ANFIS adaptivenetworkbased fuzzy inference IEEE Trans Syst Man Cybern 1993 httpsdoi org 10 1109 21256541 36 JSR Jang CT Sun E Mizutani NeuroFuzzy Soft Computing A Computational Approach Learning Machine Intelligence 1997 37 N Japkowicz S Stephen The class imbalance problem systematic study Intell Data Anal 6 2002 429449 38 B Jayaram M Baczynski Fuzzy Implications vol 231 Springer Berlin Heidelberg 2008 httplinkspringercom 10 1007 978 3 540 69082 5 39 DP Kingma J Ba Adam method stochastic optimization arXiv1412 6980 cs httparxivorg abs 1412 6980 2017 40 EP Klement R Mesiar E Pap Triangular Norms vol 8 Springer Science Business Media 2013 41 G Klir B Yuan Fuzzy Sets Fuzzy Logic vol 4 Prentice Hall New Jersey 1995 42 E van Krieken E Acar F van Harmelen Semisupervised learning differentiable reasoning IfCoLog J Log Appl 6 2019 633653 43 Y LeCun C Cortes MNIST handwritten digit database httpyann lecun com exdb mnist 2010 44 CT Lin GC Lee Neuralnetworkbased fuzzy logic control decision IEEE Trans Comput 1991 httpsdoi org 10 1109 12 106218 45 Y Liu E Kerre An overview fuzzy quantiﬁers I Interpretations Fuzzy Sets Syst 95 1998 121 46 R Manhaeve S Dumanˇci c A Kimmig T Demeester L De Raedt DeepProbLog neural probabilistic logic programming S Bengio HM Wallach H Larochelle K Grauman N CesaBianchi R Garnett Eds Advances Neural Information Processing Systems 31 Annual Conference Neural Information Processing Systems 2018 NeurIPS 2018 38 December 2018 Montréal Canada 2018 httparxivorg abs 1805 10872 httppapers nips cc book advances neural information processing systems 31 2018 2018 45 E van Krieken E Acar F van Harmelen Artiﬁcial Intelligence 302 2022 103602 47 G Marcus Deep learning critical appraisal arXiv preprint arXiv180100631 2018 48 G Marra F Giannini M Diligenti M Gori Constraintbased visual generation arXiv preprint arXiv180709202 2018 49 G Marra F Giannini M Diligenti M Gori Integrating learning reasoning deep logic models arXiv preprint arXiv190104195 pp 117 httpsarxivorg pdf 190104195v1pdf 2019 50 G Marra F Giannini M Diligenti M Maggini M Gori Learning Tnorms theory arXiv preprint arXiv190711468 2019 51 M Mayo Symbol grounding implications artiﬁcial intelligence BT TwentySixth Australasian Computer Science Conference ACSC2003 52 P Minervini T Demeester T Rocktäschel S Riedel Adversarial sets regularising neural link predictors Uncertainty Artiﬁcial Intelligence 16 2003 pp 5560 httpcrpit com confpapers CRPITV16Mayo pdf Proceedings 33rd Conference UAI 2017 2017 Computational Natural Language Learning 2018 pp 6574 53 P Minervini S Riedel Adversarially regularising neural NLI models integrate logical background knowledge Proceedings 22nd Conference 54 S Muggleton L Raedt Inductive logic programming theory methods J Log Program 1920 1994 629679 httpsdoi org 10 1016 0743 106694 90035 3 httpwwwsciencedirect com science article pii 0743106694900353 55 KP Murphy Y Weiss MI Jordan Loopy belief propagation approximate inference empirical study Proceedings Fifteenth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers Inc 2013 pp 467475 httparxivorg abs 13016725 56 V Novák I Perﬁlieva J Moˇckoˇr Mathematical Principles Fuzzy Logic Springer US 1999 httpsdoi org 10 1007 978 1 4615 5217 8 57 H Páll Jónsson Real Logic Logical Tensor Networks University Amsterdam 2018 MSc thesis 45 58 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Elsevier 1988 59 J Pearl Theoretical impediments machine learning seven sparks causal revolution Proceedings Eleventh ACM International Conference Web Search Data Mining ACM 2018 p 3 60 TG Pham N Turkkan Reliability standby betadistributed component lives IEEE Trans Reliab 1994 httpsdoi org 10 1109 24 61 A Radford J Wu R Child D Luan D Amodei I Sutskever Language Models Are Unsupervised Multitask Learners 2019 62 A Rasmus M Berglund M Honkala H Valpola T Raiko Semisupervised learning ladder networks Advances Neural Information Process 285114 ing Systems 2015 pp 35463554 63 R Riegel A Gray F Luus N Khan N Makondo IY Akhalwaya H Qian R Fagin F Barahona U Sharma S Ikbal H Karanam S Neelam A Likhyani S Srivastava Logical neural networks arXiv2006 13155 2020 64 T Rocktäschel S Riedel Endtoend differentiable proving httpsarxivorg pdf 1705 11040 pdf httparxivorg abs 1705 11040 2017 65 T Rocktäschel S Singh S Riedel Injecting logical background knowledge embeddings relation extraction Proceedings 2015 Con ference North American Chapter Association Computational Linguistics Human Language Technologies 2015 pp 11191129 https rockt github io pdf rocktaschel2015injecting pdf httpwwwaclweb org anthology N15 1118 httpaclweb org anthology N15 1118 66 P Sen G Namata M Bilgic L Getoor B Galligher T EliassiRad Collective classiﬁcation network data AI Mag 29 2008 93 67 L Seraﬁni AD Garcez Logic tensor networks deep learning logical reasoning data knowledge CEUR Workshop Proceedings 1768 2016 05128 pdf 3 545 68 D Silver J Schrittwieser K Simonyan I Antonoglou A Huang A Guez T Hubert L Baker M Lai A Bolton et al Mastering game Go human knowledge Nature 550 2017 354 69 R Socher D Chen CD Manning AY Ng Reasoning neural tensor networks knowledge base completion Proc NIPS13 2013 pp 110 70 G Šourek V Aschenbrenner F Železný O Kuželka Lifted relational neural networks CEUR Workshop Proceedings 2015 httpsarxivorg pdf 1508 71 G šourek V Aschenbrenner F Železný S Schockaert O Kuželka Lifted relational neural networks eﬃcient learning latent relational structures J Artif Intell Res 62 2018 69100 httpsdoi org 10 1613 jair111203 72 PB Vranas Hempels raven paradox lacuna standard Bayesian solution Br J Philos Sci 55 2004 545560 httpsdoi org 10 1093 bjps 55 73 H Weisberg The distribution linear combinations order statistics uniform distribution Ann Math Stat 42 1971 704709 https doi org 10 1214 aoms 1177693419 httpsdoi org 10 1214 aoms 1177693419 74 J Xu Z Zhang T Friedman Y Liang G den Broeck A semantic loss function deep learning symbolic knowledge J Dy A Krause Eds Proceedings 35th International Conference Machine Learning PMLR Stockholmsmässan Stockholm Sweden 2018 pp 55025511 http proceedings mlrpress v80 xu18h html 75 RR Yager On general class fuzzy connectives Fuzzy Sets Syst 4 1980 235242 76 ZH Zhou A brief introduction weakly supervised learning Nat Sci Rev 5 2017 4453 46