Artiﬁcial Intelligence 176 2012 22912320 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Multiinstance multilabel learning ZhiHua Zhou MinLing Zhang ShengJun Huang YuFeng Li National Key Laboratory Novel Software Technology Nanjing University Nanjing 210046 China r t c l e n f o b s t r c t Article history Received 29 April 2010 Received revised form 25 September 2011 Accepted 13 October 2011 Available online 18 October 2011 Keywords Machine learning Multiinstance multilabel learning MIML Multilabel learning Multiinstance learning In paper propose MIML MultiInstance MultiLabel learning framework example described multiple instances associated multiple class labels Compared traditional learning frameworks MIML framework convenient natural representing complicated objects multiple semantic meanings To learn MIML examples propose MimlBoost MimlSvm algorithms based simple degeneration strategy experiments solving problems involving complicated objects multiple semantic meanings MIML framework lead good performance Considering degeneration process lose information propose DMimlSvm algorithm tackles MIML problems directly regularization framework Moreover access real objects capture information real objects MIML representation MIML useful We propose InsDif SubCod algorithms InsDif works transforming singleinstances MIML representation learning SubCod works transforming singlelabel examples MIML representation learning Experiments tasks able achieve better performance learning singleinstances singlelabel examples directly 2011 Elsevier BV All rights reserved 1 Introduction In traditional supervised learning object represented instance feature vector associated class label Formally let X denote instance space feature space Y set class labels The task learn function f X Y given data set x1 y1 x2 y2 xm ym xi X instance yi Y known label xi Although formalization prevailing successful realworld problems ﬁt framework In particular object framework belongs concept corresponding instance associated single class label However realworld objects complicated belong multiple concepts simultaneously For example image belong classes simultaneously grasslands lions Africa text document classiﬁed categories viewed different aspects scientiﬁc novel Jules Vernes writing books traveling web page recognized news page sports page soccer page In speciﬁc real task maybe multiple concepts right semantic meaning For example image retrieval user interested image lions interested concept lions instead concepts grasslands Africa associated image The diﬃculty caused objects involve multiple concepts To choose right semantic meaning objects speciﬁc scenario fundamental diﬃculty tasks In contrast starting large universe possible concepts involved task helpful subset concepts associated concerned object ﬁrst choice Corresponding author Email address zhouzhlamdanjueducn ZH Zhou 00043702 matter 2011 Elsevier BV All rights reserved doi101016jartint201110002 2292 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 small subset later However getting subset concepts assigning proper class labels objects challenging task We notice alternative representing object single instance cases possible represent complicated object set instances For example multiple patches extracted image patch described instance image represented set instances multiple sections extracted document section described instance document represented set instances multiple links extracted web page link described instance web page represented set instances Using multiple instances represent complicated objects helpful inherent patterns closely related labels explicit clearer In paper propose MIML MultiInstance MultiLabel learning framework example described multiple instances associated multiple class labels Compared traditional learning frameworks MIML framework convenient natural representing complicated objects To exploit advantages MIML representation new learning algorithms needed We propose MimlBoost algorithm MimlSvm algorithm based simple degeneration strategy experiments solving problems involving complicated objects multiple semantic meanings MIML framework lead good performance Considering degeneration process lose information propose DMimlSvm Direct MimlSvm algorithm tackles MIML problems directly regularization framework Experiments direct algorithm outperforms indirect MimlSvm algorithm In practical tasks access real objects real images real web pages instead given observational data real object represented single instance Thus cases capture information real objects MIML representation Even situation MIML useful We propose InsDif INStance DIFferentiation algorithm transforms singleinstances MIML examples learning This algorithm able achieve better performance learning singleinstances directly tasks This strange object associated multiple class labels described single instance information corresponding labels mixed diﬃcult learning transform singleinstance set instances proper ways mixed information detached extent diﬃcult learning MIML helpful learning singlelabel objects We propose SubCod SUBCOncept Discovery algo rithm works discovering subconcepts target concept ﬁrst transforming data MIML examples learning This algorithm able achieve better performance learning singlelabel examples di rectly tasks This strange label corresponding highlevel complicated concept diﬃcult learn concept directly different lowerlevel concepts mixed transform singlelabel set labels corresponding subconcepts relatively clearer easier learning learn labels ﬁrst derive highlevel complicated label based diﬃculty The rest paper organized follows In Section 2 review related work In Section 3 propose MIML framework In Section 4 propose MimlBoost MimlSvm algorithms apply tasks objects represented MIML examples In Section 5 present DMimlSvm algorithm compare indirect MimlSvm algorithm In Sections 6 7 study usefulness MIML access real objects Concretely Section 6 propose InsDif algorithm MIML better learning singleinstances directly Section 7 propose SubCod algorithm MIML better learning singlelabel examples directly Finally conclude paper Section 8 2 Related work Much work devoted learning multilabel examples umbrella multilabel learning Note multilabel learning studies problem realworld object described instance associated number class labels1 different multiclass learning multitask learning 28 In multiclass learning object associated single label multitask learning different tasks involve different domains different data sets Actually traditional twoclass multiclass problems cast multilabel problems restricting instance label The generality multilabel problems inevitably makes diﬃcult address One famous approach solving multilabel problems Schapire Singers AdaBoostMH 56 extension AdaBoost core successful multilabel learning BoosTexter 56 This approach maintains set weights training examples labels training phase training examples corresponding labels hard easy predict incrementally higher lower weights Later De Comité et al 22 alternating decision trees 30 powerful decision stumps BoosTexter handle multilabel data obtained AdtBoostMH algorithm Probabilistic generative models useful multilabel learning 1 Most work multilabel learning assumes instance associated multiple valid labels work assuming labels associated instance correct 35 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2293 McCallum 47 proposed Bayesian approach multilabel document classiﬁcation mixture probabilistic model mixture component category assumed generate document EM algorithm employed learn mixture weights word distributions mixture component Ueda Saito 65 presented generative approach assumes multilabel text mixture characteristic words appearing singlelabel text belonging multilabels It noteworthy generative models 47 65 based learning text frequencies documents speciﬁc text applications Many multilabel learning algorithms developed decision trees neural networks knearest neighbor classiﬁers support vector machines Clare King 21 developed multilabel version C45 decision trees modifying deﬁnition entropy Zhang Zhou 79 presented multilabel neural network BpMll derived Backpropagation algorithm employing error function capture fact labels belonging instance ranked higher belonging instance Zhang Zhou 80 proposed Mlknn algorithm identiﬁes k nearest neighbors concerned instance assigns labels according maximum posteriori principle Elisseeff Weston 27 proposed RankSvm algorithm multilabel learning deﬁning speciﬁc cost function corresponding margin multilabel models Other kinds multilabel Svms developed Boutell et al 11 Godbole Sarawagi 33 In particular hierarchically approximating Bayes optimal classiﬁer Hloss CesaBianchi et al 15 proposed algorithm outperforms simple hierarchical Svms Recently nonnegative matrix factorization applied multilabel learning 43 multilabel dimensionality reduction methods developed 7485 Roughly speaking earlier approaches multilabel learning attempt divide multilabel learning number class classiﬁcation problems 3672 transform label ranking problem 2756 later approaches try exploit correlation labels 436585 Most studies multilabel learning focus text categorization 22333947566574 studies aim im prove performance text categorization systems exploiting additional information given hierarchical structure classes 141553 unlabeled data 43 In addition text categorization multilabel learning use ful tasks scene classiﬁcation 11 image video annotation 3848 bioinformatics 712132127 association rule mining 5063 There lot research multiinstance learning studies problem realworld object described number instances associated single class label Here training set composed bags containing multiple instances bag labeled positively contains positive instance negatively The goal label unseen bags correctly Note training bags labeled labels instances unknown This learning framework formalized Dietterich et al 24 investigating drug activity prediction Long Tan 44 studied Paclearnability multiinstance learning showed instances bags independently drawn product distribution Apr AxisParallel Rectangle proposed Dietterich et al 24 Paclearnable Auer et al 5 showed instances bags independent Apr learning multiinstance learning framework NPhard Moreover presented theoretical algorithm require prod uct distribution transformed practical algorithm named Multinst 4 Blum Kalai 10 described reduction Paclearning multiinstance learning framework Paclearning onesided random classiﬁ cation noise They presented algorithm smaller sample complexity algorithm Auer et al 5 Many multiinstance learning algorithms developed past decade To Diverse Density 45 Emdd 83 knearest neighbor algorithms Citationknn Bayesianknn 67 decision tree algorithms Relic 54 Miti 9 neural network algorithms Bpmip extensions 7790 Rbfmip 78 rule learning algorithm Ripper mi 20 support vector machines kernel methods miSvm MiSvm 3 DdSvm 18 MissSvm 88 MiKernel 32 BagInstance Kernel 19 Marginalized MiKernel 42 convexhull method ChFd 31 ensemble algorithms MiEnsemble 91 MiBoosting 70 MilBoosting 6 logistic regression algorithm Milr 51 Actually popular machine learning algorithms multiinstance versions Most algorithms attempt adapt singleinstance supervised learning algorithms multiinstance representation shifting focus discrimination instances discrimination bags 91 Recently proposal adapting multiinstance representation single instance algorithms representation transformation 93 It worth mentioning standard multiinstance learning 24 assumes bag contains positive instance bag positive implies exists key instance positive bag Many algorithms designed based assumption For example point maximal diverse density identiﬁed Diverse Density algorithm 45 actually corresponds key instance Svm algorithms deﬁned margin positive bag margin positive instance 319 As research multiinstance learning goes assumptions introduced 29 For example contrast assuming key instance work assumed key instance instance contributes bag label 1770 There argument instances bags treated independently 88 All assumptions umbrella multi instance learning generally tackling real tasks diﬃcult know assumption ﬁttest In words different tasks multiinstance learning algorithms based different assumptions different superiorities In early years research multiinstance learning work considered multiinstance classiﬁcation discretevalued outputs Later multiinstance regression realvalued outputs studied 252 different versions generalized multiinstance learning deﬁned 5868 The main difference standard multiinstance 2294 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Fig 1 Four different learning frameworks learning generalized multiinstance learning standard multiinstance learning single concept bag positive instance satisfying concept generalized multiinstance learning 5868 multiple concepts bag positive concepts satisﬁed bag contains instances concept Recently research multiinstance clustering 82 multiinstance semisupervised learning 49 multi instance active learning 60 reported Multiinstance learning attracted attention Ilp community It suggested multiinstance problems regarded bias inductive logic programming multiinstance paradigm key propositional relational representations expressive easier learn 23 Alphonse Matwin 1 approximated relational learning problem multiinstance problem fed resulting data feature selection techniques adapted propositional representations transformed ﬁltered data relational representation relational learner Thus expressive power relational representation ease feature selection propositional representation gracefully combined This work conﬁrms multi instance learning act bridge propositional relational learning Multiinstance learning techniques applied diverse applications including image categorization 17 18 image retrieval 7184 text categorization 360 web mining 86 spam detection 37 security 54 face detection 6676 computeraided medical diagnosis 31 3 The MIML framework Let X denote instance space Y set class labels Then formally MIML task deﬁned MIML multiinstance multilabel learning To learn function f 2 Xm Ym Xi X set instances xi1 xi2 xini labels yi1 yi2 yili labels Y X 2 Y given data set X1 Y 1 X2 Y 2 xi j X j 1 2 ni Y Y set yik Y k 1 2 li Here ni denotes number instances Xi li number It interesting compare MIML existing frameworks traditional supervised learning multiinstance learning multilabel learning Traditional supervised learning singleinstance singlelabel learning To learn function f X Y given data set x1 y1 x2 y2 xm ym xi X instance yi Y known label xi Multiinstance learning multiinstance singlelabel learning To learn function f 2 X1 y1 X2 y2 Xm ym Xi X set instances xi1 xi2 xini yi Y label Xi 2 Here ni denotes number instances Xi Multilabel learning singleinstance multilabel learning To learn function f X 2 Y X Y given data set xi j X j 1 2 ni x1 Y 1 x2 Y 2 xm Ym xi X instance Y Y set labels yi1 yi2 yili k 1 2 li Here li denotes number labels Y given data set yik Y 2 According notions multiinstance learning Xi yi labeled bag Xi unlabeled bag ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2295 Fig 2 MIML helpful learning singlelabel examples involving complicated highlevel concepts From Fig 1 differences learning frameworks In fact multilearning frameworks resulted ambiguities representing realworld objects Multiinstance learning studies ambiguity input space instance space object alternative input descriptions instances multilabel learning studies ambiguity output space label space object alternative output descriptions labels MIML considers ambiguities input output spaces simultaneously In solving realworld problems having good representation important having strong learning algorithm good representation capture meaningful information learning task easier tackle Since real objects inherited input ambiguity output ambiguity MIML natural convenient tasks involving objects It worth mentioning MIML reasonable singleinstance multilabel learning cases Suppose multilabel object described instance associated l number class labels label1 label2 labell If represent multilabel object set n instances instance1 instance2 instancen underlying information single instance easier exploit label number training instances signiﬁcantly increased So transforming multilabel examples MIML examples learning beneﬁcial tasks shown Section 6 Moreover representing multilabel object set instances relation input patterns semantic meanings easily discoverable Note cases understanding particular object certain class label important simply making accurate prediction MIML offers possibility purpose For example MIML representation discover object label1 contains instancen labell contains instancei occurrence instance1 instancei triggers label j MIML helpful learning singlelabel examples involving complicated highlevel concepts For example Fig 2a shows concept Africa broad connotation images belonging Africa great variance easy classify topleft image Fig 2a Africa class correctly However exploit lowlevel subconcepts ambiguous easier learn tree lions elephant grassland shown Fig 2b possible induce concept Africa easier learning concept Africa directly The usefulness MIML process shown Section 7 2296 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Fig 3 The general degeneration solutions 4 Solving MIML problems degeneration It evident traditional supervised learning degenerated version multiinstance learning degener ated version multilabel learning traditional supervised learning multiinstance learning multilabel learning degenerated versions MIML So simple idea tackle MIML identify equivalence traditional supervised learning framework multiinstance learning multilabel learning bridge shown Fig 3 X Y 1 1 For y Y Solution A Using multiinstance learning bridge The MIML learning task learn function f 2 transformed multiinstance learning task f MIL Xi y 1 y Y 1 The learn function f MIL 2 y 1 This multi proper labels new example X instance learning task transformed traditional supervised learning task learn function f SISL X Y 1 1 constraint specifying derive f MIL Xi y f SISLxi j y j 1 2 ni For ni y Y f SISLxi j y 1 y Y 1 Here constraint f MIL Xi y sign j1 f SISLxi j y Xu Frank 70 transforming multiinstance learning tasks traditional supervised learning tasks Note kinds constraint determined according Y y sign f MIL X Y X 2 cid2 Solution B Using multilabel learning bridge The MIML learning task learn function f 2 transformed multilabel learning task X Z The proper labels Y learn function f MLL Z 2 This multilabel learning task new example X transformed traditional supervised learning task learn function f SISL Z Y 1 1 For y Y f SISLzi y 1 y Y 1 That f MLLzi y f SISLzi y 1 Here mapping φ implemented constructive clustering proposed Zhou Zhang 93 transforming multiinstance bags traditional singleinstances Note kinds mappings X 2 Y f MLLzi f MIML Xi zi φ Xi φ 2 determined according Y For zi Z f MLLφ X In rest section propose MIML algorithms MimlBoost MimlSvm MimlBoost illustration Solution A uses categorywise decomposition A1 step Fig 3 MiBoosting A2 MimlSvm illustration Solution B uses clusteringbased representation transformation B1 step MlSvm B2 Other MIML algorithms developed taking alternative options Both MimlBoost MimlSvm simple We dealing complicated objects multiple semantic meanings good performance obtained MIML framework simple algorithms This demonstrates MIML framework promising expect better performance achieved future researchers forward powerful MIML algorithms 41 MimlBoost Now propose MimlBoost algorithm according ﬁrst solution mentioned identifying equivalence traditional supervised learning framework multiinstance learning bridge Note strategy derive kinds MIML algorithms Given set Ω let Ω denote size number elements Ω given predicate π let cid2π cid3 1 π holds 0 given Xi Y y Y let Ψ Xi y 1 y Y 1 Ψ function X Y 1 1 judges label y proper label Xi The basic assumption MimlBoost Ψ 2 labels independent MIML task decomposed series multiinstance learning tasks solve treating label task The pseudocode MimlBoost summarized Appendix A Table A1 Y In ﬁrst step MimlBoost MIML example Xu Y u u 1 2 m transformed set number multiinstance bags Xu y1 Ψ Xu y1 Xu y2 Ψ Xu y2 Xu yY Ψ Xu yY Note Xu y v Ψ Xu y v v 1 2 Y labeled multiinstance bag Xu y v bag containing nu number instances xu1 y v xu2 y v xunu y v Ψ Xu y v 1 1 label bag ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2297 Thus original MIML data set transformed multiinstance data set containing m Y number bags We order X1 y1 Ψ X1 y1 X1 yY Ψ X1 yY X2 y1 Ψ X2 y1 Xm yY Ψ Xm yY let X yi Ψ X yi denote ith m Y number bags contains ni number instances Then data set multiinstance learning function f MIL learned accomplish desired MIML y 1 In paper MiBoosting algorithm 70 im function f MIML X plement f MIL Note MiBoosting MimlBoost algorithm assumes instances bag contribute independently equal way label bag y sign f MIL X For convenience let B g denote bag X y Ψ X y B B g G E denotes expectation Then goal learn function F B minimizing baglevel exponential loss EB EGBexpgF B ultimately estimates baglevel logodds function 1 Prg1B training set In boosting round aim expand F B F B c f B adding new weak classiﬁer exponential loss minimized Assuming j hb j derived instances bag contribute equally independently bags label hb j 1 1 prediction instancelevel classiﬁer h jth instance bag B nB number instances B 2 log Prg1B f B 1 nB cid2 cid2 1 ni ni j1 W gihb It shown 70 best cid2 f B added achieved seeking h maximizes j given baglevel weights W expgF B By assigning instance label bag corresponding weight W ini h learned minimizing weighted instancelevel classiﬁcation error This actually corresponds Step 3a MimlBoost When f B best multiplier c 0 got directly optimizing exponential loss cid4 cid3 exp EB EGB gFB c cid4 g f B cid5cid5cid6 cid7 cid7 cid8 cid9 W exp c gi cid2 W exp cid3cid4 2ei 1 cid5 c cid10cid11 j 1 j hb ni cid6 cid2 j cid2hb j cid6 gicid3 computed Step 3b Minimization expectation actually corresponds Step 3d ei 1 ni numeric optimization techniques quasiNewton method Note Step 3c ei cid2 05 Boosting process stop 89 Finally baglevel weights updated Step 3f according additive structure F B 42 MimlSvm Now propose MimlSvm algorithm according second solution mentioned identifying equivalence traditional supervised learning framework multilabel learning bridge Note strategy derive kinds MIML algorithms Again given set Ω let Ω denote size number elements Ω given Xi Y zi φ Xi X Z y Y let Φzi y 1 y Y 1 Φ function Φ Z Y 1 1 φ 2 The basic assumption MimlSvm spatial distribution bags carries relevant information information helpful label discrimination discovered measuring closeness bag representative bags identiﬁed clustering The pseudocode MimlSvm summarized Appendix A Table A2 In ﬁrst step MimlSvm Xu MIML example Xu Y u u 1 2 m collected data set Γ Then second step kmedoids clustering performed Γ Since data item Γ Xu unlabeled multiinstance bag instead single instance Hausdorff distance 26 employed measure distance The Hausdorff distance famous metric measuring distance bags points vision tasks techniques measure distance bags points set kernel 32 Hausdorff distance A In given bags A a1 a2 A B deﬁned B b1 b2 bnB cid12 dH A B max max A min bB cid7a bcid7 max bB min A cid13 cid7b acid7 2 cid7a bcid7 measures distance instances b takes form Euclidean distance After clustering process data set Γ divided k partitions medoids Mt t 1 2 k spectively With help medoids original multiinstance example Xu transformed kdimensional numerical vector zu ith 1 2 k component zu distance Xu Mi dH Xu Mi In words zui encodes structure information data relationship Xu ith partition Γ This process reassembles constructive clustering process Zhou Zhang 93 transforming multiinstance examples singleinstance examples 93 clustering executed instance level executed bag level Thus original MIML examples Xu Y u u 1 2 m trans formed multilabel examples zu Y u u 1 2 m corresponds Step 3 MimlSvm 2298 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 f MLLz Then data set multilabel learning function f MLL learned accomplish desired MIML In paper MlSvm algorithm 11 implement f MLL Concretely function f MIML X MlSvm decomposes multilabel learning problem multiple independent binary classiﬁcation problems class example associated label set Y regarded positive example building Svm class y Y regarded negative example building Svm class y Y shown Step 4 MimlSvm In making predictions TCriterion 11 actually corresponds Step 5 MimlSvm algorithm That test example labeled class labels positive Svm scores Svm scores negative test example labeled class label negative score 43 Experiments 431 Multilabel evaluation criteria In traditional supervised learning object class label accuracy performance evaluation criterion Typically accuracy deﬁned percentage test examples correctly classiﬁed When learning complicated objects associated multiple labels simultaneously accuracy mean ingful For example approach A missed proper label approach B missed proper labels test example having ﬁve labels obvious A better B accuracy A B identical incorrectly classiﬁed test example Five criteria evaluating performance learning multilabel examples 5692 ham ming loss oneerror coverage ranking loss average precision Using denotation Sections 3 4 given test set S X1 Y 1 X2 Y 2 X p Y p ﬁve criteria deﬁned Here h Xi returns set proper labels Xi h Xi y returns realvalue indicating conﬁdence y proper label Xi rankh Xi y returns rank y derived h Xi y cid2 p i1 cid2 p i1 hlossS h 1 p Y h Xicid8Y cid8 stands symmetric difference sets The hamming loss 1 evaluates times objectlabel pair misclassiﬁed proper label missed wrong label pre dicted The performance perfect hlossS h 0 smaller value hlossS h better performance h cid2 oneerrorS h 1 p cid2arg max yY h Xi y Y icid3 The oneerror evaluates times topranked label proper label object The performance perfect oneerror S h 0 smaller value errorS h better performance h p i1 max yY rankh Xi y 1 The coverage evaluates far needed average list labels order cover proper labels object It loosely related precision level perfect recall The smaller value coverageS h better performance h coverageS h 1 rlossS h 1 p y1 y2h Xi y1 cid3 h Xi y2 y1 y2 Y Y Y denotes complementary set Y Y The ranking loss evaluates average fraction label pairs misordered object The performance perfect rlossS h 0 smaller value rlossS h better performance h 1 Y Y p i1 cid2 p avgprecS h 1 The average precision evaluates average fraction proper labels ranked particular label y Y The performance perfect avgprecS h 1 larger value avgprecS h better performance h yY p y cid8rankh Xi y cid8cid2rankh Xi y y rankh Xi y cid8Y cid2 cid2 p i1 1 Y In addition criteria design new multilabel criteria average recall average F1 cid2 avgreclS h 1 p i1 yrankh Xi ycid2h Xi yY Y The average recall evaluates average fraction proper labels predicted The performance perfect avgreclS h 1 larger value avgreclS h better performance h p avgF1S h 2avgprecS havgreclS h avgprecS havgreclS h The average F1 expresses tradeoff average precision average recall The performance perfect avgF1S h 1 larger value avgF1S h better performance h Note criteria measure performance different aspects diﬃcult algorithm outperform criteria In following study performance MIML algorithms tasks involving complicated objects multiple semantic meanings We tasks MIML good choice good performance achieved simple MIML algorithms MimlBoost MimlSvm 432 Scene classiﬁcation The scene classiﬁcation data set consists 2000 natural scene images belonging classes desert mountains sea sunset trees Over 22 images belong multiple classes simultaneously Each image represented ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2299 Table 1 Results meanstd scene classiﬁcation data set indicates smaller better indicates larger better Compared algorithms MimlBoost MimlSvm MimlSvmmi MimlNn AdtBoostMH RankSvm MlSvm Mlknn Evaluation criteria hloss 193007 189009 195008 185008 211006 210024 232004 191006 oneerror 347019 354022 317018 351026 436019 395075 447023 370017 coverage 984049 1087047 1068052 1057054 1223050 1161154 1217054 1085048 rloss 178011 201011 197011 196013 NA 221040 233012 203010 aveprec 779012 765013 783011 771015 718012 746044 712013 759011 averecl 433027 556020 587019 509022 NA 529068 073010 407026 aveF1 556023 644018 671015 613020 NA 620059 132017 529023 bag instances generated Sbn method 46 uses Gaussian ﬁlter smooth image subsamples image 8 8 matrix color blobs blob 2 2 set pixels matrix An instance corresponds combination single blob neighboring blobs left right described 15 features The ﬁrst features represent mean R G B values central blob remaining features express differences mean color values central blob neighboring blobs respectively3 We evaluate performance MIML algorithms MimlBoost MimlSvm Note MimlBoost MimlSvm merely proposed illustrate general degeneration solutions MIML problems shown Fig 3 We claim best algorithms developed degeneration paths There exist processes transforming MIML examples multiinstance singlelabel MISL examples singleinstance multilabel SIML examples Even degeneration process MimlBoost MimlSvm alternatives realize second step For example miSvm 3 replace MiBoosting Miml Boost twolayer neural network structure 81 replace MlSvm MimlSvm MimlSvmmi MimlNn respectively Their performance evaluated experiments We compare MIML algorithms stateoftheart algorithms learning multilabel examples cluding AdtBoostMH 22 RankSvm 27 MlSvm 11 Mlknn 80 algorithms introduced brieﬂy Section 2 Note singleinstance algorithms regard image 135dimensional feature vector obtained concatenating instances direction upperleft rightbottom The parameter conﬁgurations RankSvm MlSvm Mlknn set considering strategies adopted 1127 80 respectively For RankSvm polynomial kernel polynomial degrees 2 9 considered 27 chosen holdout tests training sets For MlSvm Gaussian kernel For Mlknn number nearest neighbors considered set 10 The boosting rounds AdtBoostMH MimlBoost set 25 50 respectively The performance algorithms different boosting rounds shown Appendix B Fig B1 observed rounds performance algorithms stable Gaussian kernel Libsvm 16 Step 3a MimlBoost The MimlSvm MimlSvmmi realized Gaussian kernels The parameter k MimlSvm set 20 number training images The performance algorithm different k values shown Appendix B Fig B2 observed setting k signiﬁcantly affect performance MimlSvm Note Appendix B Figs B1 B2 plot 1 average precision 1 average recall 1 average F1 ﬁgures lower curve better performance Here experiments 1500 images training examples remaining 500 images testing Experiments repeated thirty runs random trainingtest partitions average standard deviation summarized Table 14 best performance criterion highlighted boldface Pairwise ttests 95 signiﬁcance level disclose MIML algorithms signiﬁcantly better Adt BoostMH MlSvm seven evaluation criteria This impressive mentioned evaluation criteria measure learning performance different aspects algorithm rarely outperforms algorithm criteria MimlSvm MimlSvmmi signiﬁcantly better RankSvm evaluation criteria MimlBoost MimlNn signiﬁcantly better RankSvm ﬁrst ﬁve criteria MimlNn signiﬁcantly better Mlknn evaluation criteria Both MimlBoost MimlSvmmi signiﬁcantly better Mlknn criteria hamming loss MimlSvm signiﬁcantly better Mlknn oneerror average precision average recall average F1 ties criteria Moreover note best performance evaluation criteria attained MIML algorithms Overall comparison scene classiﬁcation task shows MIML algorithms signiﬁcantly better nonMIML algorithms validates powerfulness MIML framework 3 The data set available httplamdanjueducndata_MIMLimageashx 4 For shared implementation AdtBoostMH httpwwwgrappaunivlille3frgrappaen_indexphp3infosoftware ranking loss average recall average F1 available programs outputs 2300 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Table 2 Results meanstd text categorization data set indicates smaller better indicates larger better Compared algorithms MimlBoost MimlSvm MimlSvmmi MimlNn AdtBoostMH RankSvm MlSvm Mlknn Evaluation criteria hloss 053004 033003 041004 038002 055005 120013 050003 049003 oneerror 094014 066011 055009 080010 120017 196126 081011 126012 coverage 387037 313035 284030 320030 409047 695466 329029 440035 rloss 035005 023004 020003 025003 NA 085077 026003 045004 aveprec 937008 956006 965005 950006 926011 868092 949006 920007 averecl 792010 925010 921012 834011 NA 411059 777016 821021 aveF1 858008 940008 942007 888008 NA 556068 854011 867013 433 Text categorization The Reuters21578 data set experiment The seven frequent categories considered After moving documents labels main texts randomly removing documents label data set containing 2000 documents obtained 149 documents multiple labels Each document represented bag instances according method 3 Brieﬂy instances obtained splitting document passages overlapping windows maximal 50 words As result 2000 bags number instances bag varies 2 26 36 average The instances represented based term frequency The words high frequencies considered excluding function words removed vocabulary Smart stoplist 55 It based document frequency dimensionality data set reduced 110 loss effectiveness 73 Thus use 2 frequent words instance 243dimensional feature vector5 The parameter conﬁgurations RankSvm MlSvm Mlknn set way Section 432 The boosting rounds AdtBoostMH MimlBoost set 25 50 respectively Linear kernels The parameter k MimlSvm set 20 number training images The singleinstance algorithms regard document 243dimensional feature vector obtained aggregating instances bag equivalent represent document sole term frequency feature vector Here experiments 1500 documents training examples remaining 500 documents testing Experiments repeated thirty runs random trainingtest partitions average standard deviation summarized Table 2 best performance criterion highlighted boldface Pairwise ttests 95 signiﬁcance level disclose impressively MimlSvm MimlSvmmi signiﬁcantly better nonMIML algorithms MimlNn signiﬁcantly better AdtBoostMH RankSvm Mlknn evaluation criteria signiﬁcantly better MlSvm hamming loss average recall average F1 ties criteria MimlBoost signiﬁcantly better AdtBoostMH criteria tie hamming loss signiﬁcantly better RankSvm criteria signiﬁcantly better MlSvm average recall tie average F1 signiﬁcantly better Mlknn oneerror coverage ranking loss average precision Moreover note best performance evaluation criteria attained MIML algorithms Overall comparison text categorization task shows MIML algorithms better nonMIML algorithms validates powerfulness MIML framework 5 Solving MIML problems regularization The degeneration methods presented Section 4 lose information degeneration process direct MIML algorithm desirable In section propose regularization method MIML In contrast MimlSvm MimlSvmmi method developed regularization framework directly DMimlSvm The basic assumption DMimlSvm labels associated example relatedness formance classifying bags depends loss labels predictions bags constituent instances Moreover considering class label number positive examples smaller negative examples method incorporates mechanism deal class imbalance We employ constrained concaveconvex procedure Cccp wellstudied convergence properties 62 solve resultant nonconvex optimization problem We present cutting plane algorithm ﬁnds solution eﬃciently 51 The loss function Given set MIML training examples X1 Y 1 X2 Y 2 Xm Ym goal DMimlSvm learn map proper label set bag X X corresponds f X Y Speciﬁcally DMimlSvm Y X 2 ping f 2 5 The data set available httplamdanjueducndata_MIMLtextashx ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2301 f f 1 f 2 f T T number labels label space chooses instantiate f T functions X R determines belongingness lt X Y l1 l2 lT Here tth function ft 2 f X lt ft X 0 1 cid3 t cid3 T In addition single instance x X bag X viewed bag x containing instance f x ftx simpliﬁed f x ftx rest section f x f 1x f 2x f T x welldeﬁned function For convenience To train component functions ft 1 cid3 t cid3 T f DMimlSvm employs following empirical loss function V involving terms balanced λ cid4 cid5 cid4 V Xim i1 Y im i1 f V 1 Xim i1 Y im i1 f cid5 cid4 λ V 2 Xim i1 f cid5 3 Here ﬁrst term V 1 considers loss groundtruth label set training bag Xi Y predicted label set f Xi Let yit 1 lt Y holds 1 cid3 cid3 m 1 cid3 t cid3 T Otherwise yit 1 Furthermore let z max0 z denote hinge loss function Accordingly ﬁrst loss term V 1 deﬁned cid4 V 1 Xim i1 Y im i1 f cid5 1 mT mcid7 Tcid7 i1 t1 cid4 cid5 1 yit ftXi 4 The second term V 2 considers loss f Xi predictions Xi s constituent instances f xi j 1 cid3 j cid3 Here common assumption ni reﬂects relationships bag Xi instances xi1 xi2 xini multiinstance learning strength Xi hold label equal maximum strength instances hold label ft Xi max j1ni ftxi j6 Accordingly second loss term V 2 deﬁned cid4 V 2 Xim i1 f cid5 1 mT mcid7 Tcid7 cid4 l i1 t1 ftXi max j1ni cid5 ftxi j 5 Here lv 1 v 2 deﬁned ways set l1 loss paper lv 1 v 2 v 1 v 2 By combining Eq 4 Eq 5 empirical loss function V Eq 3 speciﬁed cid4 V Xim i1 Y im i1 f cid5 1 mT mcid7 Tcid7 cid4 cid5 1 yit ftXi λ mT 52 Representer theorem MIML i1 t1 mcid7 Tcid7 i1 t1 cid4 l ftXi max j1ni cid5 ftxi j 6 For simplicity assume function ft linear model ftx cid11wt φxcid12 φ feature map induced kernel function k cid11cid12 denotes standard inner product Reproducing Kernel Hilbert Space RKHS H induced kernel k We recall instance regarded bag containing instance kernel k kernel deﬁned set instances set kernel 32 In case classiﬁcation objects bags instances classiﬁed according sign ft DMimlSvm assumes labels associated bag relatedness associated bag simultaneously To reﬂect basic assumption DMimlSvm regularizes empirical loss function Eq 6 additional term Ω f cid5 cid4 Ω f γ V Xim i1 Y im i1 f 7 Here γ regularization parameter balancing model complexity Ω f empirical risk V Inspired 28 assume relatedness labels measured mean function w 0 w 0 1 T Tcid7 t1 wt The original idea 28 minimize cid2 T t1 cid7wt w 0cid72 minimize cid7w 0cid72 set regularizer 8 Ω f 1 T Tcid7 t1 cid7wt w 0cid72 ηcid7w 0cid72 9 6 Note assumption restrictive extent There cases label bag rely instance maximum predictions discussed Section 2 In addition classiﬁcation sign prediction important 19 sign ft Xi signmax j1ni ft xi j However paper common assumption adopted popularity simplicity 10 11 2302 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 According Eq 8 ﬁrst term RHS Eq 9 rewritten 1 T Tcid7 t1 cid7wt w 0cid72 1 T Tcid7 t1 cid7wtcid72 cid7w 0cid72 Therefore substituting Eq 10 Eq 9 regularizer simpliﬁed Ω f 1 T Tcid7 t1 cid7wtcid72 μcid7w 0cid72 Further note cid7wtcid72 cid7 ftcid72 framework DMimlSvm follows H cid7w 0cid72 cid7 cid2 T t1 ft T cid72 H substituting Eq 11 Eq 7 regularization min f H 1 T Tcid7 t1 cid14 cid14 cid14 H μ cid14 cid7 ftcid72 cid2 T t1 ft T cid14 cid14 cid14 cid14 2 H cid4 γ V Xim i1 Y im i1 f cid5 12 Here μ parameter trade discrepancy commonness labels similar dissimilar H μcid7 wt s Refer Eq 10 Ω f 1 T Intuitively μ 1 μ large minimization Eq 12 force cid7 labels important μ 1 μ small minimization Eq 12 force cid7 ft tend zero commonness labels important 28 T t1 cid72 H tend zero discrepancy cid72 H H 1 cid72 T cid2 T t1 ft T H μ 1cid7 cid72 T t1 ft T T t1 ft T T t1 ft T T t1 ft T cid7 ft cid7 ftcid72 cid72 H T t1 cid2 cid2 cid2 cid2 cid2 cid2 Given setup prove following representer theorem Theorem 1 The minimizer optimization problem 12 admits expansion ftx αti0kx Xi αti jkx xi j cid15 mcid7 i1 αti0 αti j R nicid7 j1 cid16 cid10 Proof Analogous 28 ﬁrst introduce combined feature map cid9 φx r Ψ x t 0 0 cid17 cid18cid19 cid20 t1 decision function ˆf x t cid11 ˆw Ψ x tcid12 φx 0 0 cid17 cid18cid19 cid20 T t ˆw r w 0 w 1 w 0 w T w 0 Here r μT T Let ˆk denote kernel function induced Ψ ˆH corresponding RKHS We Eqs 13 14 cid21 ˆf x t cid22 ˆw Ψ x t Tcid7 cid7 ˆf cid72 ˆH cid7 ˆwcid72 i1 cid22 cid21 w 0 wt w 0 φx cid21 cid22 wt φx ftx cid7wt w 0cid72 rcid7w 0cid72 Tcid7 i1 cid7wtcid72 μT cid7w 0cid72 Therefore loss function Eq 6 represented ˆV Xim i1 Y im i1 ˆf cid4 ˆV Xim i1 Y im i1 ˆf cid5 1 mT mcid7 Tcid7 cid4 1 yit cid5 ˆf Xi t i1 t1 mcid7 Tcid7 i1 t1 cid4 l ˆf Xi t max j1ni cid5 ˆf xi j t λ mT Thus Eq 12 equivalent cid4 cid7 ˆf cid72 ˆH γ ˆV 1 T min ˆf ˆH Xim i1 Y im i1 ˆf cid5 13 14 15 16 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2303 Note cid7 ˆf cid72 ˆH 57 minimizer ˆf functional risk Eq 16 admits representation form 0 R strictly monotonically increasing function According representer theorem Theorem 42 ˆf x t cid15 mcid7 Tcid7 t1 i1 cid4 cid5 Xi t x t ˆk βti0 cid4 cid5 xi j t x t ˆk βti j cid16 nicid7 j1 βti j R corresponding weight vector ˆw represented Tcid7 ˆw cid15 mcid7 βti0Ψ Xi t t1 i1 cid16 βti jΨ xi j t nicid7 j1 Finally Eqs 13 18 cid22 w Ψ x t nicid7 cid22 wt φx cid15 mcid7 ftx cid21 cid21 cid16 αti0kx Xi αti jkx xi j 17 18 19 i1 j1 αti j 1 r cid2 t βti j βti jr cid2 Note x Eq 19 regarded bag Xi instance xi j In words ft Xi ftxi j obtained Eq 19 53 Optimization Considering use l1 loss lv 1 v 2 Eq 12 rewritten cid14 cid14 cid14 H μ cid14 cid7 ftcid72 cid2 T t1 ft T cid14 cid14 cid14 cid14 2 H 1 T Tcid7 t1 min f Hξ δ γ mT ξ cid8 1 γ λ mT δcid8 1 st yit ftXi cid2 1 ξit ξ cid2 0 δit cid3 ftXi max j1ni ξ ξ11 ξ12 ξit ξmT cid8 δit δmT cid8 ftxi j cid3 δit 1 m t 1 T 20 slack variables errors training bags label δ δ11 δ12 0 1 allzero allone vector respectively Without loss generality assume bags instances ordered X1 Xm x11 x1n1 xm1 xmnm Thus object bag instance training set indexed following function I cid23 IXi Ixi j m cid2 i1 l1 nl j j 1 ni 1 m With ordering obtain m n m n kernel matrix K deﬁned objects training set n m i1 ni Denote ith column K ki According Theorem 1 ft Xi k Ixi j αt bt Here bias bt label included According deﬁnition ft Eq 19 Eq 20 cast optimization problem I Xi αt bt ftxi j k cid2 cid8 cid8 cid8 1 cid8 A K A1 γ mT ξ cid8 1 γ λ mT δcid8 1 Tcid7 1 min Aξ δb t K αt μ αcid8 T 2 cid5 st cid2 1 ξit t1 cid8 I Xi αt bt 2T cid4 k yit ξ cid2 0 cid8 Ixi jαt δit cid3 k I Xi αt max j1ni A α1 α2 α T b b1 b2 bT cid8 cid8 I Xi αt cid8 k Ixi j αt cid3 δit k k cid8 21 2304 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 The optimization problem nonconvex optimization problem constraint nonconvex Note nonconvex constraint difference convex functions optimization problem solved Cccp 1962 standard techniques solve kind nonconvex optimization problems Cccp guaranteed converge local minimum 75 cases converge global solution 25 Here solving optimization problem 21 Cccp works solving sequential convex quadratic problems Con cid8 Ixi j αt solve following convex quadratic cid8 Ixi j αt max j1ni k cretely given initial subgradient ni j1 ρi jt k cid2 optimization QP problem cid8 1 cid8 A K A1 γ mT ξ cid8 1 γ λ mT δcid8 1 min Aξ δb st Tcid7 1 t K αt μ αcid8 T 2 cid5 t1 cid8 I Xi αt bt 2T cid4 yit k ξ cid2 0 cid8 Ixi jαt δit cid3 k k cid8 I Xi αt cid2 1 ξit k cid8 I Xi αt nicid7 j1 ρi jtk cid8 Ixi j αt cid3 δit 22 Then iteration update ρi jk according Ixi j αt cid6 maxk1ni k cid23 cid8 cid8 Ixikαt ρi jt 0 k 1nd nd number active xi j s It holds guaranteed converge local minimum 54 Handling classimbalance cid2 ni j1 ρi jt 1 ts The iteration continues procedure The solution improved explicitly account instancelevel classimbalance class label number positive instances smaller number negative instances MIML problems We roughly estimate imbalance rate ratio number positive instances negative instances class label strategy adopted 41 In speciﬁc label y Y divide training bags X1 Y 1 X2 Y 2 Xm Ym subsets A1 Xi Y y Y A2 Xi Y y Y It obvious instances A2 negative y Then Xi Y A1 assuming instances different Y Y returns labels roughly equally distributed number positive instances y Xi Y roughly ni 1 number labels Y Thus imbalance rate y ibr y mcid7 i1 yY ni Y 1cid2 m i1 ni mcid7 i1 yY ni n Y There classimbalance learning methods 69 One popular effective methods rescaling 87 incorporated framework easily In short obtaining estimated imbalance rate class label use rates modulate loss caused different misclassiﬁcations In ξ Eq 22 directly related hinge loss 1 yit ft Xi According rescaling method 87 loss generality rewrite loss function Eq 23 cid9 yit 1 2 cid10 yit ibr yit cid4 cid5 1 yit ftXi 23 Let τ τ11 τ12 τit τmT τit yit 1 yit ibr yit Then minimize loss deﬁned Eq 23 Eq 22 Eq 24 Here ξ cid8τ indicates weighted loss considering instancelevel classimbalance It evident problem Eq 24 standard QP problem 2 Tcid7 1 t K αt μ αcid8 T 2 cid5 t1 cid8 I Xi αt bt min Aξ δb st 2T cid4 yit k ξ cid2 0 cid2 1 ξit cid8 1 cid8 A K A1 γ mT ξ cid8τ γ λ mT δcid8 1 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 k cid8 Ixi jαt δit cid3 k cid8 I Xi αt k cid8 I Xi αt nicid7 j1 ρi jtk cid8 Ixi j αt cid3 δit 55 Eﬃcient algorithm 2305 24 Eq 24 largescale quadratic programming problem involves constraints variables To tractable scalable observing constraints Eq 24 redundant present eﬃcient al gorithm constructs nested sequence tighter relaxations original problem cutting plane method 40 Similar use structured prediction 64 add constraint cut violated current solution ﬁnd solution updated feasible region Such procedure converge optimal εsuboptimal solution original problem Moreover Eq 24 supports natural problem decomposition constraint matrix block diagonal matrix block corresponds label The pseudocode algorithm summarized Appendix A Table A3 We ﬁrst initialize working sets St s sets solutions zeros line 1 Then instead testing constraints expensive lots constraints use speedup heuristic described 61 use p constraints approximate constraints line 4 Smola Schölkopf 61 shown p larger 59 selected violated constraint probability 095 5 violated constraints constraints The Lossi line 5 x d u d linear coeﬃcients bias ith linear constraint respectively If calculated max0 u 4 experiments update maximal Loss lower given stopping criteria ε simply set ε 10 taken working set St constraint maximal Loss added St lines 8 9 Once new constraint added solution recomputed respect St solving smaller quadratic program problem line 10 The algorithm stops update St s cid8 56 Experiments The previous experiments Section 43 shown different MIML algorithms different advantages differ ent performance measures In section propose DMimlSvm algorithm We claim DMimlSvm best MIML algorithm What want contrast heuristically solving MIML problem degeneration developing algorithms regularization framework directly offers better choice So meaningful comparison DMimlSvm MimlSvm MimlSvmmi algorithms derived regularization framework directly To study behavior DMimlSvm MimlSvm MimlSvmmi different amounts multilabel data derive ﬁve data sets scene data Section 432 By randomly removing singlelabel images obtain data set 30 40 50 images belonging multiple classes simultaneously randomly removing multi label images obtain data set 10 20 images belong multiple classes simultaneously A similar process applied text data Section 433 derive ﬁve data sets On derived data sets use 25 data training remaining 75 data testing experiments repeated thirty runs random trainingtest partitions The parameters DMimlSvm MimlSvm MimlSvmmi set holdout tests training sets Since DMimlSvm needs solve large optimization problem incorporated advanced mechanisms cuttingplane algorithm current DMimlSvm deal moderate training set sizes The seven criteria introduced Section 431 evaluate performance The average standard deviation plotted Figs 4 5 Note ﬁgures plot 1 average precision 1 average recall 1 average F1 ﬁgures lower curve better performance As shown Figs 4 5 performance DMimlSvm better MimlSvm MimlSvmmi cases Speciﬁcally pairwise ttests 95 signiﬁcance level disclose On scene classiﬁcation task 35 conﬁgurations 7 evaluation criteria 5 percentages multilabel bags performance DMimlSvm superior MimlSvm MimlSvmmi 88 80 cases comparable 6 20 cases inferior 6 cases b On text categorization task 35 conﬁgurations performance DMimlSvm superior MimlSvm MimlSvmmi 82 82 cases comparable 9 18 cases inferior 9 cases The results suggest DMimlSvm good choice learning moderate number MIML examples 57 Discussion The regularization framework presented section important assumption class labels share commonness w 0 Eq 8 This assumption makes regularization easier realize simpliﬁes real scenario In fact real applications rare class labels share commonness 2306 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Fig 4 Results scene classiﬁcation data set different percentage multilabel data The lower curve better performance Fig 5 Results text categorization data set different percentage multilabel data The lower curve better performance typical class labels share commonness commonness shared different labels different For example class label y1 share class label y2 y2 share y3 maybe y1 shares y3 So reasonable assumption different pairs labels share different things By considering assumption powerful method developed Actually diﬃcult modify framework Eq 12 replacing role w 0 W element W j expresses relatedness ith jth class labels min 1 2T 2 cid7 j cid7w W jcid72 1 T 2 cid7 j μi jcid7W jcid72 γ V Note W tensor W j vector 25 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2307 To minimize Eq 25 taking derivative W j w W j w j W ji 2μi j W j 2μ ji W ji 0 Considering W j W ji μi j μ ji w W j w j W j 4μi j W j 0 W j w w j 4μ ji 2 Put Eq 26 Eq 25 cid14 cid14 cid14 cid14 4μi j 1w w j 4μi j 2 1 2T 2 min cid7 j 2 cid14 cid14 cid14 cid14 1 T 2 cid14 cid14 cid14 cid14 μi j cid7 j w w j 4μi j 2 After simpliﬁcation Eq 25 min 1 8T 2 1 4T 2 cid9 cid7 j cid7 j 10μi j 1 16μ2 j 2μi j 12 cid7w icid72 2μi j 1 2μi j 12 cid7w jcid72 2μi j 1 2μi j 12 cid11w w jcid12 γ V γ V 2 cid14 cid14 cid14 cid14 cid10 So new optimization task min Aξ δb 1 8T 2 Tcid7 cid9 Tcid7 10μi j 1 16μ2 j 2μi j 12 K αi 2μi j 1 αcid8 2μi j 12 αcid8 j K α j cid10 K α j γ αcid8 mT ξ cid8 1 γ λ mT δcid8 1 2μi j 1 2μi j 12 cid5 cid2 1 ξit st j1 i1 Tcid7 Tcid7 i1 j1 cid8 I Xi αt bt 1 4T 2 cid4 yit k ξ cid2 0 cid8 Ixi jαt δit cid3 k I Xi αt max j1ni k k cid8 cid8 I Xi αt cid8 k Ixi j αt cid3 δit 26 27 28 By solving Eq 28 MIML learner understanding relatedness pairs labels W j understanding different importance W j s determining concerned class label μi j s helpful understanding complicated concepts underlying task Eq 28 diﬃcult solve involves variables Thus exploitunderstand pairwise relatedness different pairs labels remains open problem 6 Solving singleinstance multilabel problems MIML transformation The previous sections access real objects able represent complicated objects MIML examples MIML framework beneﬁcial However practical tasks given observational data object represented single instance access real objects In case capture information real objects MIML representation Even situation MIML useful Here propose InsDif INStance DIFferentiation algorithm transforms single instance multilabel examples MIML examples exploit power MIML 61 InsDif For object associated multiple class labels described single instance information corre sponding labels mixed diﬃcult learn The basic assumption InsDif spatial distribution instances different labels encodes information helpful discriminating labels information explicit breaking singleinstances number instances corresponds label 2308 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 InsDif twostage algorithm based instance differentiation In ﬁrst stage InsDif transforms example bag instances deriving instance class label order explicitly express ambiguity example input space second stage MIML learner utilized learn transformed data set For consistency previous description algorithm 81 current version InsDif use twolevel classiﬁcation strategy note MIML algorithms DMimlSvm applied Using denotation Sections 3 4 given data set S x1 Y 1 x2 Y 2 xm Ym yik Y k 1 2 li Here li denotes number xi X instance Y Y set labels yi1 yi2 yili labels Y In ﬁrst stage InsDif derives prototype vector vl class label l Y averaging training instances belonging l vl 1 Sl cid9 cid7 cid10 xi xi Sl cid24 Sl xi xi Y S l Y cid25 l Y 29 Here vl approximately regarded proﬁlestyle vector describing common characteristics class l Actually kind prototype vectors shown usefulness solving text categorization problems For example Rocchio method 3459 forms prototype vector class averaging documents represented weight vectors class classiﬁes test document calculating dotproducts weight vector rep resenting document prototype vectors Here use prototype vectors facilitate bag generation After obtaining prototype vectors example xi rerepresented bag instances B instance B expresses difference xi prototype vector according Eq 30 In way example transformed bag size equals number class labels B xi vl l Y 30 In fact process attempts exploit spatial distribution xi vl Eq 30 kind distance xi vl The transformation realized ways For example referring prototype vector class consider following approach For possible class l identify knearest neighbors xi training instances l proper label Then mean vector neighbors regarded instance bag Note transformation single instance bag instances realized general preprocessing method plugged learning systems In second stage InsDif learns transformed training set S B1 Y 1 B2 Y 2 Bm Ym This task realized MIML learning algorithm By default use MimlNn algorithm introduced Section 432 The use MIML algorithms stage studied section The pseudocode InsDif summarized Appendix A Table A4 In ﬁrst stage Steps 1 2 InsDif transforms example bag instances querying class prototype vectors In second stage Step 3 MIML algorithm learn transformed data set A test example x transformed corresponding bag representation B fed learned MIML model 62 Experiments We compare InsDif stateoftheart multilabel learning algorithms including AdtBoostMH 22 RankSvm 27 MlSvm 11 Mlknn 80 Cnmf 43 algorithms introduced brieﬂy Section 2 In addition MimlBoost MimlSvm MimlSvmmi respectively replace MimlNn realizing second stage InsDif variants InsDif InsDifMIMLBOOST InsDifMIMLSVM InsDifMIMLSVMmi These variants evaluated comparison Note experiments different Sections 43 56 In Sections 43 56 assumed data MIML examples section assumed given observational data real object represented single instance In words section trying learn singleinstance multilabel examples experimental data sets different Sections 43 56 621 Yeast gene functional analysis The task predict gene functional classes Yeast Saccharomyces cerevisiae best studied organisms Speciﬁcally Yeast data set investigated 2780 studied Each gene represented ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2309 Table 3 Results meanstd Yeast gene data set indicates smaller better indicates larger better Compared algorithms InsDif InsDifMIMLSVM InsDifMIMLSVMmi AdtBoostMH RankSvm MlSvm Mlknn Cnmf Evaluation criteria hloss 189010 189009 196011 212008 207013 199009 194010 NA oneerror 214030 232040 238043 247029 243039 227032 230030 354184 coverage 62880240 66250261 63960206 63850151 70900502 72200338 62750240 79301089 rloss 163017 179015 172012 NA 195021 201019 167016 268062 aveprec 774019 763021 765019 739022 750026 749021 765021 668093 avgrecl 602026 591023 655024 NA 500047 572023 574022 NA avgF1 677023 666022 706017 NA 600041 649022 656021 NA 103dimensional feature vector generated concatenating gene expression vector corresponding phylogenetic proﬁle Each 79element gene expression vector reﬂects expression levels particular gene different ex perimental conditions phylogenetic proﬁle Boolean string bit indicating concerned gene close homolog corresponding genome Each gene associated set functional classes maximum size potentially 190 Elisseeff Weston 27 preprocessed data set known structure functional classes In fact set functional classes structured hierarchies 4 levels deep7 Illustrations ﬁrst level hierarchy generate Yeast data 277980 The resulting multilabel data set contains 2417 genes fourteen possible class labels average number labels gene 424 157 For InsDif parameter M set 20 size training set The performance algorithm different M settings shown Appendix B Fig B3 performance sensitive setting M The boosting rounds AdtBoostMH set 25 The performance algorithm different boosting rounds shown Appendix B Fig B4 observed round performance stable Similar observations Section 622 For RankSvm polynomial kernel degree 8 suggested 27 For MlSvm features For Cnmf normalized Gaussian kernel Gaussian kernel default Libsvm setting kernel width recommended 43 compute pairwise class similarity For Mlknn number nearest neighbors considered set 10 The criteria introduced Section 431 evaluate learning performance Tenfold crossvalidation conducted data set results summarized Table 38 best performance criterion highlighted boldface 1 Table 3 shows InsDif variants achieve good performance Yeast gene functional data set Pairwise ttests 95 signiﬁcance level disclose InsDif signiﬁcantly better compared multilabel learning algorithms second Table 3 criteria coverage worse Mlknn difference statistically signiﬁcant9 b InsDifMIMLSVM signiﬁcantly better compared multilabel learning algorithms 68 cases signiﬁcantly inferior 11 cases c InsDifMIMLSVMmi signiﬁcantly better compared multilabel learning algorithms 65 cases signiﬁcantly inferior Speciﬁcally InsDifMIMLSVMmi outperforms compared algorithms terms average recall average F1 It noteworthy Cnmf performs poorly compared algorithms test set information The reason key assumption Cnmf examples high similarity input space tend large overlap output space hold gene data genes functions different physical appearances similar Overall results Yeast gene functional analysis task suggest MIML useful given observa tional data complicated object represented single instance 622 Web page categorization The web page categorization task studied 396580 The web pages collected yahoocom domain divided 11 data sets based Yahoos toplevel categories10 After page classiﬁed number Yahoos secondlevel subcategories Each data set contains 2000 training documents 3000 test docu ments The simple term selection method based document frequency number documents containing speciﬁc 7 See httpmipsgsfdeprojyeastcataloguesfuncat details 8 Hamming loss average recall average F1 available Cnmf ranking loss average recall average F1 available AdtBoostMH The performance InsDifMIMLBOOST reported algorithm terminate reasonable time data 9 Note implementation RankSvm obtained help authors 27 results somewhat worse best results reported 27 We think performance gap caused minor implementation differences different experimental data partitions Nevertheless worth mentioning results InsDif better best results RankSvm 27 terms hamming loss oneerror average precision best results RankSvm 27 terms ranking loss 10 Data set available httpwwwkeclnttcojpasmembersuedayahootargz 2310 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Table 4 Results meanstd web page categorization data sets indicates smaller better indicates larger better Compared algorithms InsDif InsDifMIMLSVM AdtBoostMH RankSvm MlSvm Mlknn Cnmf Evaluation criteria hloss 039013 043015 044014 043014 042015 043015 NA oneerror 381118 395119 477144 424135 375119 471157 509142 coverage 45451285 68231623 41771261 72282442 69191767 40971236 67171588 rloss 102037 166045 NA 182057 168047 102045 171058 aveprec 686091 653093 621108 621108 660093 625116 561114 avgrecl 377163 501105 NA 252172 378167 292189 NA aveF1 479154 566102 NA 345177 472156 381196 NA term applied data set reduce dimensionality Actually 2 words highest document frequency retained ﬁnal vocabulary Other term selection methods information gain mutual formation adopted After term selection document data set described feature vector BagofWords representation feature expresses number times vocabulary word appearing document Characteristics web page data sets summarized Appendix C Table C1 Compared Yeast data Section 621 instances represented higherdimensional feature vectors large portion 2045 multilabeled Moreover number categories 2140 larger rare categories 2055 So web page data sets diﬃcult Yeast data learn The parameter settings similar Section 621 That InsDif parameter M set 20 size training set boosting rounds AdtBoostMH set 25 RankSvm polynomial kernel polynomial degrees 2 9 considered 27 chosen holdout tests training sets MlSvm Cnmf linear Gaussian kernel respectively Mlknn number nearest neighbors considered set 10 Results data sets shown Appendix C Fig C1 average results summarized Table 4 best performance criterion highlighted boldface11 Table 4 shows InsDif InsDifMIMLSVM perform Yahoo data Pairwise ttests 95 signiﬁcance level disclose InsDif inferior AdtBoostMH Mlknn terms coverage inferior MlSvm terms oneerror comparable Mlknn terms ranking loss comparable MlSvm terms average recall average F1 Under circumstances 79 cases performance InsDif signiﬁcantly better compared multilabel learning algorithms second Table 4 b InsDifMIMLSVM signiﬁcantly better compared multilabel learning algorithms 44 cases signiﬁcantly inferior 18 cases Speciﬁcally InsDifMIMLSVM achieves best performance terms average recall average F1 oneerror inferior MlSvm signiﬁcantly superior compared multilabel learning algo rithms Overall results web page categorization task suggest MIML useful given observational data complicated object represented single instance 7 Solving multiinstance singlelabel problems MIML transformation In tasks given observational data object represented multiinstance singlelabel example access real objects In case capture information real objects MIML representation Even situation MIML useful Here propose SubCod SUBCOncept Discovery algorithm transforms multiinstance singlelabel examples MIML examples exploit power MIML 71 SubCod For object described multiinstances associated label corresponding highlevel complicated concept Africa Fig 2a diﬃcult learn concept directly The basic assumption SubCod highlevel complicated concepts derived number lowerlevel subconcepts relatively clearer easier learning transform singlelabel set labels corresponds 11 The performance InsDifMIMLBOOST InsDifMIMLSVMmi reported algorithms terminate reasonable time data Note signiﬁcant differences numbers table subtle ﬁrst glance InsDif vs RankSvm terms oneerror statistical tests based detailed information online supplementary ﬁle justify signiﬁcance ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2311 subconcept Therefore learn labels ﬁrst derive highlevel complicated label based illustrated Fig 2b SubCod twostage algorithm based subconcept discovery In ﬁrst stage SubCod transforms singlelabel example multilabel example discovering exploiting subconcepts involved original label realized constructing multiple labels unsupervised clustering instances treating cluster set instances separate subconcept In second stage outputs learned transformed data set derive original labels predicted realized supervised learning algorithm predict original labels subconcepts predicted MIML learner Using denotation Sections 3 4 given data set X1 y1 X2 y2 Xm ym xi j X j 1 2 ni yi Y label Xi Here ni denotes Xi X set instances xi1 xi2 xini number instances Xi In ﬁrst stage SubCod collects instances bags compose data set D x11 x1n1 x21 x2n2 xm1 xmnm cid2 For ease discussion let N i1 ni reindex instances D x1 x2 xN A Gaussian mixture model M mixture components learned D EM algorithm mixture components regarded subconcepts The parameters mixture components means μk covariances Σk mixing coeﬃcients πk k 1 2 M randomly initialized initial value loglikelihood evaluated In Estep responsibilities measured according m γik cid2 πkN xiμk Σk j1 π jN xiμ j Σ j M 1 2 N In Mstep parameters reestimated according μnew k Σ new k π new k cid2 cid2 N i1 γikxi cid2 N i1 γik i1 γikxi μnew N k N i1 γik cid2 cid2 N i1 γik N xi μnew k T loglikelihood evaluated according ln pDμ Σ π Ncid7 cid9 Mcid7 ln i1 k1 cid4 π new k N xi μnew k Σ new k cid10 cid5 31 32 33 34 35 After convergence EM process prespeciﬁed number iterations estimate associated subconcept instance xi D 1 2 N scxi arg max γik k 1 2 M k 36 Then derive multilabel Xi 1 2 m considering subconcept belongingness Let ci denote Mdimensional binary vector element 1 1 For j 1 2 M ci j 1 means subconcept corresponding jth Gaussian mixture component appears Xi ci j 1 means sub concept appear Xi Here value ci j determined according simple rule ci j 1 Xi instance takes jth subconcept satisfying Eq 36 ci j 1 Note examples identical singlelabel derived multilabels different The process works unsupervised way consider original labels bags Xi s Thus derived multilabels ci need polished incorporating relation subconcepts original label Xi Here maximum margin criterion In consider vector zi elements zi j 10 10 j 1 2 M zi j 1 means label ci j modiﬁed zi j 1 means label ci j ci cid16 zi j 1 2 M qi j ci j zi j Let θ denote smallest number labels inverted Denote qi inverted SubCod attempts optimize objective 1 2 cid4 min wbξ Z st cid7wcid72 2 C mcid7 ξi i1 w Tci cid16 zi b yi ξ cid2 0 1 cid3 zi j cid3 1 cid5 cid2 1 ξi 2312 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Table 5 Predictive accuracy Musk1 Musk2 Elephant Tiger Fox data sets Compared algorithms SubCod SubCodMIMLNN SubCodMIMLSVMmi Diverse Density Emdd miSvm MiSvm ChFd Data sets Musk1 08500035 08590025 08700023 0880 0848 0874 0779 0888 cid7 j zi j cid2 2θ mM Z z1 z2 zm Musk2 09210014 08880022 08690020 0840 0849 0836 0843 0857 Elephant 08360010 08150023 08050017 NA 0783 0820 0814 0824 Tiger 08080013 07950018 07870016 NA 0721 0789 0840 0822 Fox 06160020 05990032 05900015 NA 0561 0582 0594 0604 37 By solving Eq 37 vector zi maximizes margin prediction proper labels Xi Here solve Eq 37 iteratively We initialize Z 1s First ﬁx Z optimal w b standard QP problem Then ﬁx w b optimal Z standard LP problem These steps iterated till convergence Finally set multilabel vectors elements correspond positive ci j zi j s 1 2 m j 1 2 M 1 set remaining ones 1 Thus polished multilabel vec tors ci bags Xi Thus original data set X1 y1 X2 y2 Xm ym transformed MIML data set X1 c1 X2 c2 Xm cm MIML algorithms applied To map multilabels predicted MIML classiﬁer test example original singlelabels y Y second stage SubCod traditional classiﬁer f 1 1M Y generated data set c1 y1 c2 y2 cm ym This relatively simple traditional supervised learning algorithms applied The pseudocode SubCod summarized Appendix A Table A5 In ﬁrst stage Steps 1 3 SubCod derives multilabels subconcept discovery transforms singlelabel examples MIML examples MIML learner generated In second stage Step 4 traditional classiﬁer trained map derived multilabels fed MIML learner multilabels multilabels fed original singlelabels Test example X supervised classiﬁer label y predicted X 72 Experiments We compare SubCod stateoftheart multiinstance learning algorithms including Diverse Density 45 Em dd 83 miSvm MiSvm 3 ChFd 31 algorithms introduced brieﬂy Section 2 For SubCod MIML learner Step 3 realized MimlSvm classiﬁer Step 4 realized Smo default parameters In addition MimlNn MimlSvmmi respectively replace MimlSvm realizing Step 3 SubCod variants SubCod SubCodMIMLNN SubCodMIMLSVMmi They evaluated comparison12 Note experiments different Sections 43 56 62 Both Sections 43 56 deal learning MIML examples Section 62 deals learning singleinstance multilabel examples section deals learning multiinstance singlelabel examples experimental data sets section different Sections 43 56 62 Five benchmark multiinstance learning data sets including Musk1 Musk2 Elephant Tiger Fox Both Musk1 Musk2 drug activity prediction data sets publicly available UCI machine learning repository 8 Here ev ery bag corresponds molecule instance corresponds lowenergy shape molecule 24 Musk1 contains 47 positive bags 45 negative bags number instances contained bag ranges 2 40 Musk2 contains 39 positive bags 63 negative bags number instances contained bag ranges 1 1044 Each instance 166dimensional feature vector Elephant Tiger Fox image annotation data sets generated 3 multiinstance learning Here bag image instance corresponds segmented region image 3 Each data set contains 100 positive 100 negative bags instance 230dimensional feature vector These data sets popularly evaluating performance multiinstance learning algorithms Parameters SubCod determined holdout tests training sets Speciﬁcally candidate values M number Gaussian mixture components range 10 70 candidate values θ smallest number labels 12 We evaluated variant SubCodMIMLBOOST obtained employing MimlBoost replace MimlSvm terminate reasonable time performance reported section ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2313 inverted range mM 10 mM 70 Ten runs tenfold cross validation performed results summarized Table 5 best performance data set highlighted boldface Note results compared algorithms second Table 5 best performance reported literatures 3 3113 Table 5 shows SubCod variants competitive stateoftheart multiinstance learning algorithms In particular Musk2 performance better algorithms This expectable Musk2 complicated data set largest number instances data set subconcept discovery process SubCod effective Overall experimental results suggest MIML useful given observational data object represented multiinstance singlelabel example 8 Conclusion This paper extends preliminary work 8192 formalize MIML MultiInstance MultiLabel learning framework example described multiple instances associated multiple class labels It inspired recognition solving realworld problems having good representation important having strong learning algorithm good representation capture meaningful information learning task easier tackle Since real objects inherited input ambiguity output ambiguity MIML natural convenient tasks involving objects To exploit advantages MIML representation propose MimlBoost algorithm MimlSvm algorithm based simple degeneration strategy Experiments scene classiﬁcation text categorization solving problems involving complicated objects multiple semantic meanings MIML framework lead good performance Considering degeneration process lose information propose DMimlSvm algorithm tackles MIML problems directly regularization framework Experiments direct Svm algorithm outperforms indirect MimlSvm algorithm In practical tasks given observational data complicated object represented single instance access real objects capture information real objects MIML representation For scenario propose InsDif algorithm transforms single instances MIML examples learn Experiments Yeast gene functional analysis web page categorization algorithm able achieve better performance learning singleinstances directly This diﬃcult understand Actually representing multilabel object multiinstances structure information collapsed traditional singleinstance representation easier exploit label number training instances signiﬁcantly increased So transforming multilabel examples MIML examples learning beneﬁcial tasks MIML helpful learning singlelabel examples involving complicated highlevel concepts Usually diﬃcult learn concepts directly different lowerlevel concepts mixed If transform singlelabel set labels corresponding subconcepts relatively clearer easier learn learn labels ﬁrst derive highlevel complicated label based Inspired recognition propose SubCod algorithm works discovering subconcepts target concept ﬁrst transforming data MIML examples learn Experiments algorithm able achieve better performance learning singlelabel examples directly tasks We believe semantics exist connections atomic input patterns atomic output patterns prominent usefulness MIML realized paper possibility identifying connection As stated Section 3 MIML framework possible understand concerned object certain class label important simply making accurate prediction results helpful understanding source ambiguous semantics Acknowledgements The authors want thank anonymous reviewers helpful comments suggestions The development MimlSvmmi InsDifMIMLSVM InsDifMIMLSVMmi SubCodMIMLNN SubCodMIMLSVMmi owes reviewers suggestions The au thors want thank DeChuan Zhan James Kwok help DMimlSvm Yang Yu help SubCod André Elisseeff Jason Weston providing Yeast data implementation details RankSvm A preliminary Chinese version presented Chinese Workshop Machine Learning Applications 2009 This research sup ported National Fundamental Research Program China 2010CB327903 National Science Foundation China 61073097 61021062 13 The tradition multiinstance learning community compare best performance reported literature Since detailed results available 317183132456783 perform statistical signiﬁcance tests 2314 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Appendix A Pseudocodes learning algorithms Table A1 The MimlBoost algorithm 1 Transform MIML example Xu Y u u 1 2 m Y number multiinstance bags Xu y1 Ψ Xu y1 Xu yY Ψ Xu yY Thus original data set transformed multiinstance data set containing m Y number multiinstance bags denoted X yi Ψ X yi 1 2 m Y 2 Initialize weight bag W 1 3 Repeat t 1 2 T iterations mY 1 2 m Y 3a Assign bags label Ψ X yi instances x j yi 1 2 m Y j 1 2 ni set weight jth instance ith bag W j W ini build instancelevel predictor ht x j yi 1 1 3b For ith bag compute error rate ei 0 1 counting number misclassiﬁed instances bag cid2ni ei cid2ht x j yicid6Ψ X yicid3 j1 ni 3c If ei 05 1 2 m Y Step 4 cid2 mY 3d Compute ct arg minct i1 W exp2ei 1ct 3e If ct cid2 0 Step 4 3f Set W W exp2ei 1ct 1 2 m Y renormalize 0 cid2 W cid2 1 j X j y 1 x y sign s jth instance t ct ht x cid2 cid2 j 4 Return Y cid2 mY i1 W 1 Table A2 The MimlSvm algorithm 1 For MIML examples Xu Y u u 1 2 m Γ Xu u 1 2 m 2 Randomly select k elements Γ initialize medoids Mt t 1 2 k repeat Mt change 2a Γt Mt t 1 2 k 2b Repeat Xu Γ Mt t 1 2 k index argmint1k dH Xu Mt Γindex Γindex Xu dH A B t 1 2 k cid2 2c Mt arg min AΓt BΓt 3 Transform Xu Y u multilabel example zu Y u u 1 2 m zu zu1 zu2 zuk dH Xu M1 dH Xu M2 dH Xu Mk 4 For y Y derive data set D y zu Φzu yu 1 2 m train Svm h y SVMTrainD y Mk cid3 0 y Y z 5 Return Y arg max yY h y z M2 dH X yh y z M1 dH X dH X Table A3 Eﬃcient algorithm Eq 24 t ξ t1 ξ tm δt1 δtm bt 0 Pick p indexes constraints St randomly denoted I Compute Lossi constraint I ﬁnd cutting plane q arg maxiI Lossi If Lossq ε For t 1 T Input K λ μ γ ε Xi Y m i1 1 t St vt α T 2 Repeat 3 4 5 6 7 8 9 10 11 12 13 Until St changes End If End For St St q vt optimized St Table A4 The InsDif algorithm 1 For singleinstance multilabel examples xu Y u u 1 2 m compute prototype vectors vl l Y Eq 29 2 Derive new training set S 3 Learning S B 1 Y 1 B 2 Y 2 Bm Ym MIML algorithm transforming xi bag instances B Eq 30 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2315 Table A5 The SubCod algorithm 1 For multiinstance singlelabel examples Xu yu u 1 2 m collect instances x Xu identify Gaussian mixture com ponents EM process detailed Eqs 31 35 2 Determine subconcept instance x Xu according Eq 36 derive label vector cu Xu 3 Make corrections cu optimizing Eq 37 results cu Xu train MIML learner ht X Xu cu u 1 2 m 4 Train classiﬁer h y c cu yu u 1 2 m maps derived multilabels original singlelabels 5 Return y h y ht X Appendix B Parameter settings learning algorithms Fig B1 Performance MimlBoost AdtBoostMH different rounds scene classiﬁcation data set Fig B2 Performance MimlSvm different k values scene classiﬁcation data set 2316 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 Fig B3 Performance InsDif different M settings Yeast gene data set Fig B4 Performance AdtBoostMH different rounds Yeast gene data set Appendix C Web page data sets Table C1 Characteristics web page data sets term selection PMC denotes percentage documents belonging category ANL denotes average number labels document PRC denotes percentage rare categories kind category 1 instances data set belong Data Set ArtsHumanities BusinessEconomy ComputersInternet Education Entertainment Health RecreationSports Reference Science SocialScience SocietyCulture Number categories Vocabulary size 26 30 33 33 21 32 22 33 40 39 27 462 438 681 550 640 612 606 793 743 1047 636 Training set PMC 4450 4220 2960 3350 2930 4805 3020 1375 3485 2095 4190 ANL 1627 1590 1487 1465 1426 1667 1414 1159 1489 1274 1705 PRC 1923 5000 3939 5758 2857 5313 1818 5152 3500 5641 2593 Test set PMC 4363 4193 3127 3373 2820 4720 3120 1460 3057 2283 3997 ANL 1642 1586 1522 1458 1417 1659 1429 1177 1425 1290 1684 PRC 1923 4333 3636 5758 3333 5313 1818 5455 4000 5897 2222 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2317 Fig C1 Results Yahoo data sets Supplementary material The online version article contains additional supplementary material Please visit doi101016jartint201110002 2318 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 References 1 É Alphonse S Matwin Filtering multiinstance problems reduce dimensionality relational learning Journal Intelligent Information Sys tems 22 1 2004 2340 2 RA Amar DR Dooly SA Goldman Q Zhang Multipleinstance learning realvalued data Proceedings 18th International Conference Machine Learning Williamstown MA 2001 pp 310 3 S Andrews I Tsochantaridis T Hofmann Support vector machines multipleinstance learning S Becker S Thrun K Obermayer Eds Advances Neural Information Processing Systems vol 15 MIT Press Cambridge MA 2003 pp 561568 4 P Auer On learning multiinstance examples Empirical evaluation theoretical approach Proceedings 14th International Conference Machine Learning Nashville TN 1997 pp 2129 5 P Auer PM Long A Srinivasan Approximating hyperrectangles Learning pseudorandom sets Journal Computer System Sciences 57 3 1998 376388 6 P Auer R Ortner A boosting approach multiple instance learning Proceedings 15th European Conference Machine Learning Pisa Italy 2004 pp 6374 7 Z Barutcuoglu RE Schapire OG Troyanskaya Hierarchical multilabel prediction gene function Bioinformatics 22 7 2006 830836 8 C Blake E Keogh CJ Merz UCI repository machine learning databases Department Information Computer Science University California Irvine CA 1998 httpwwwicsuciedumlearnMLRepositoryhtml 9 H Blockeel D Page A Srinivasan Multiinstance tree learning Proceedings 22nd International Conference Machine Learning Bonn Germany 2005 pp 5764 10 A Blum A Kalai A note learning multipleinstance examples Machine Learning 30 1 1998 2329 11 MR Boutell J Luo X Shen CM Brown Learning multilabel scene classiﬁcation Pattern Recognition 37 9 2004 17571771 12 K Brinker J Fürnkranz E Hüllermeier A uniﬁed model multilabel classiﬁcation ranking Proceedings 17th European Conference Artiﬁcial Intelligence Riva del Garda Italy 2006 pp 489493 13 K Brinker E Hüllermeier Casebased multilabel ranking Proceedings 20th International Joint Conference Artiﬁcial Intelligence Hydrabad India 2007 pp 702707 14 L Cai T Hofmann Hierarchical document categorization support vector machines Proceedings 13th ACM International Conference Information Knowledge Management Washington DC 2004 pp 7887 15 N CesaBianchi C Gentile L Zaniboni Hierarchical classiﬁcation Combining Bayes SVM Proceedings 23rd International Conference Machine Learning Pittsburgh PA 2006 pp 177184 16 CC Chang CJ Lin LIBSVM A library support vector machines Technical report Department Computer Science Information Engineering National Taiwan University Taipei 2001 17 Y Chen J Bi JZ Wang MILES Multipleinstance learning embedded instance selection IEEE Transactions Pattern Analysis Machine Intel ligence 28 12 2006 19311947 18 Y Chen JZ Wang Image categorization learning reasoning regions Journal Machine Learning Research 5 2004 913939 19 PM Cheung JT Kwok A regularization framework multipleinstance learning Proceedings 23rd International Conference Machine Learning Pittsburgh PA 2006 pp 193200 20 Y Chevaleyre JD Zucker A framework learning rules multiple instance data Proceedings 12th European Conference Machine Learning Freiburg Germany 2001 pp 4960 21 A Clare RD King Knowledge discovery multilabel phenotype data Proceedings 5th European Conference Principles Data Mining Knowledge Discovery Freiburg Germany 2001 pp 4253 22 F De Comité R Gilleron M Tommasi Learning multilabel altenating decision tree texts data Proceedings 3rd International Conference Machine Learning Data Mining Pattern Recognition Leipzig Germany 2003 pp 3549 23 L De Raedt Attributevalue learning versus inductive logic programming The missing links Proceedings 8th International Workshop Inductive Logic Programming Madison WI 1998 pp 18 24 TG Dietterich RH Lathrop T LozanoPérez Solving multipleinstance problem axisparallel rectangles Artiﬁcial Intelligence 89 12 1997 3171 25 T Pham Dinh HA Le Thi A DC optimization algorithm solving trustregion subproblem SIAM Journal Optimization 8 2 1998 476505 26 GA Edgar Measure Topology Fractal Geometry Springer Berlin 1990 27 A Elisseeff J Weston A kernel method multilabelled classiﬁcation TG Dietterich S Becker Z Ghahramani Eds Advances Neural Infor mation Processing Systems vol 14 MIT Press Cambridge MA 2002 pp 681687 28 T Evgeniou CA Micchelli M Pontil Learning multiple tasks kernel methods Journal Machine Learning Research 6 2005 615637 29 J Foulds E Frank A review multiinstance learning assumptions Knowledge Engineering Review 25 1 2010 125 30 Y Freund L Mason The alternating decision tree learning algorithm Proceedings 16th International Conference Machine Learning Bled Slovenia 1999 pp 124133 31 G Fung M Dundar B Krishnappuram RB Rao Multiple instance learning aided diagnosis B Schölkopf J Platt T Hofmann Eds Advances Neural Information Processing Systems vol 19 MIT Press Cambridge MA 2007 pp 425432 32 T Gärtner PA Flach A Kowalczyk AJ Smola Multiinstance kernels Proceedings 19th International Conference Machine Learning Sydney Australia 2002 pp 179186 33 S Godbole S Sarawagi Discriminative methods multilabeled classiﬁcation Proceedings 8th PaciﬁcAsia Conference Knowledge Discovery Data Mining Sydney Australia 2004 pp 2230 34 DJ Ittner DD Lewis DD Ahn Text categorization low quality images Proceedings 4th Annual Symposium Document Analysis Information Retrieval Las Vegas NV 1995 pp 301315 35 R Jin Z Ghahramani Learning multiple labels S Becker S Thrun K Obermayer Eds Advances Neural Information Processing Systems vol 15 MIT Press Cambridge MA 2003 pp 897904 36 T Joachims Text categorization support vector machines Learning relevant features Proceedings 10th European Conference Machine Learning Chemnitz Germany 1998 pp 137142 37 Z Jorgensen Y Zhou M Inge A multiple instance learning strategy combating good word attacks spam ﬁlters Journal Machine Learning Research 8 2008 9931019 38 F Kang R Jin R Sukthankar Correlated label propagation application multilabel learning Proceedings IEEE Computer Society Conference Computer Vision Pattern Recognition New York NY 2006 pp 17191726 39 H Kazawa T Izumitani H Taira E Maeda Maximal margin labeling multitopic text categorization LK Saul Y Weiss L Bottou Eds Advances Neural Information Processing Systems vol 17 MIT Press Cambridge MA 2005 pp 649656 40 JE Kelley The cuttingplane method solving convex programs Journal Society Industrial Applied Mathematics 8 4 1960 703712 41 H Kück N Freitas Learning individuals group statistics Proceedings 21st Conference Uncertainty Artiﬁcial Intelligence Edinburgh Scotland 2005 pp 332339 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 2319 42 JT Kwok PM Cheung Marginalized multiinstance kernels Proceedings 20th International Joint Conference Artiﬁcial Intelligence Hydrabad India 2007 pp 901906 43 Y Liu R Jin L Yang Semisupervised multilabel learning constrained nonnegative matrix factorization Proceedings 21st National Conference Artiﬁcial Intelligence Boston MA 2006 pp 421426 44 PM Long L Tan PAC learning axisaligned rectangles respect product distributions multipleinstance examples Machine Learning 30 1 1998 721 45 O Maron T LozanoPérez A framework multipleinstance learning MI Jordan MJ Kearns SA Solla Eds Advances Neural Information Processing Systems vol 10 MIT Press Cambridge MA 1998 pp 570576 46 O Maron AL Ratan Multipleinstance learning natural scene classiﬁcation Proceedings 15th International Conference Machine Learning Madison MI 1998 pp 341349 47 A McCallum Multilabel text classiﬁcation mixture model trained EM Working Notes AAAI99 Workshop Text Learning Orlando FL 1999 48 GJ Qi XS Hua Y Rui J Tang T Mei HJ Zhang Correlative multilabel video annotation Proceedings 15th ACM International Conference Multimedia Augsburg Germany 2007 pp 1726 49 R Rahmani SA Goldman MISSL Multipleinstance semisupervised learning Proceedings 23rd International Conference Machine Learn ing Pittsburgh PA 2006 pp 705712 50 R Rak L Kurgan M Reformat Multilabel associative classiﬁcation medical documents medline Proceedings 4th International Conference Machine Learning Applications Los Angeles CA 2005 pp 177186 51 S Ray M Craven Supervised versus multiple instance learning An empirical comparison Proceedings 22nd International Conference Machine Learning Bonn Germany 2005 pp 697704 52 S Ray D Page Multiple instance regression Proceedings 18th International Conference Machine Learning Williamstown MA 2001 pp 425432 53 J Rousu C Saunders S Szedmak J ShaweTaylor Learning hierarchical multicategory text classiﬁcation models Proceedings 22nd Interna tional Conference Machine Learning Bonn Germany 2005 pp 774751 54 G Ruffo Learning single multiple instance decision trees security applications PhD thesis Department Computer Science Univer sity Turin Torino Italy 2000 55 G Salton Automatic Text Processing The Transformation Analysis Retrieval Information Computer AddisonWesley Reading MA 1989 56 RE Schapire Y Singer BoosTexter A boostingbased text categorization Machine Learning 39 23 2000 135168 57 B Schölkopf AJ Smola Learning With Kernels Support Vector Machines Regularization Optimization Beyond MIT Press Cambridge MA 2002 58 SD Scott J Zhang J Brown On generalized multipleinstance learning Technical Report UNLCSE20035 Department Computer Science University Nebraska Lincoln NE 2003 59 F Sebastiani Machine learning automated text categorization ACM Computing Surveys 34 1 2002 147 60 B Settles M Craven S Ray Multipleinstance active learning JC Platt D Koller Y Singer S Roweis Eds Advances Neural Information Processing Systems vol 20 MIT Press Cambridge MA 2008 pp 12891296 61 AJ Smola B Schölkopf Sparse greedy matrix approximation machine learning Proceedings 17th International Conference Machine Learning San Francisco CA 2000 pp 911918 62 AJ Smola SVN Vishwanathan T Hofmann Kernel methods missing variables Proceedings 10th International Workshop Artiﬁcial Intelligence Statistics Savannah Hotel Barbados 2005 pp 325332 63 FA Thabtah PI Cowling Y Peng MMAC A new multiclass multilabel associative classiﬁcation approach Proceedings 4th IEEE Interna tional Conference Data Mining Brighton UK 2004 pp 217224 64 I Tsochantaridis T Joachims T Hofmann Y Altun Large margin methods structured interdependent output variables Journal Machine Learning Research 6 2005 14531484 65 N Ueda K Saito Parametric mixture models multilabeled text S Becker S Thrun K Obermayer Eds Advances Neural Information Processing Systems vol 15 MIT Press Cambridge MA 2003 pp 721728 66 P Viola J Platt C Zhang Multiple instance boosting object detection Y Weiss B Schölkopf J Platt Eds Advances Neural Information Processing Systems vol 18 MIT Press Cambridge MA 2006 pp 14191426 67 J Wang JD Zucker Solving multiinstance problem A lazy learning approach Proceedings 17th International Conference Machine Learning San Francisco CA 2000 pp 11191125 68 N Weidmann E Frank B Pfahringer A twolevel learning method generalized multiinstance problem Proceedings 14th European Conference Machine Learning CavtatDubrovnik Croatia 2003 pp 468479 69 GM Weiss Mining rarity problems solutions A unifying framework SIGKDD Explorations 6 1 2004 719 70 X Xu E Frank Logistic regression boosting labeled bags instances Proceedings 8th PaciﬁcAsia Conference Knowledge Discovery Data Mining Sydney Australia 2004 pp 272281 71 C Yang T LozanoPérez Image database retrieval multipleinstance learning techniques Proceedings 16th International Conference Data Engineering San Diego CA 2000 pp 233243 72 Y Yang An evaluation statistical approaches text categorization Information Retrieval 1 12 1999 6788 73 Y Yang JO Pedersen A comparative study feature selection text categorization Proceedings 14th International Conference Machine Learning Nashville TN 1997 pp 412420 74 K Yu S Yu V Tresp Multilabel informed latent semantic indexing Proceedings 28th Annual International ACM SIGIR Conference Research Development Information Retrieval Salvador Brazil 2005 pp 258265 75 AL Yuille A Rangarajan The concaveconvex procedure Neural Computation 15 4 2003 915936 76 C Zhang P Viola Multipleinstance pruning learning eﬃcient cascade detectors JC Platt D Koller Y Singer S Roweis Eds Advances Neural Information Processing Systems vol 20 MIT Press Cambridge MA 2008 pp 16811688 77 ML Zhang ZH Zhou Improve multiinstance neural networks feature selection Neural Processing Letters 19 1 2004 110 78 ML Zhang ZH Zhou Adapting RBF neural networks multiinstance learning Neural Processing Letters 23 1 2006 126 79 ML Zhang ZH Zhou Multilabel neural networks applications functional genomics text categorization IEEE Transactions Knowledge Data Engineering 18 10 2006 13381351 80 ML Zhang ZH Zhou MLkNN A lazy learning approach multilabel learning Pattern Recognition 40 7 2007 20382048 81 ML Zhang ZH Zhou Multilabel learning instance differentiation Proceedings 22nd AAAI Conference Artiﬁcial Intelligence Van couver Canada 2007 pp 669674 82 ML Zhang ZH Zhou Multiinstance clustering applications multiinstance prediction Applied Intelligence 31 1 2009 4768 83 Q Zhang SA Goldman EMDD An improved multiinstance learning technique TG Dietterich S Becker Z Ghahramani Eds Advances Neural Information Processing Systems vol 14 MIT Press Cambridge MA 2002 pp 10731080 84 Q Zhang W Yu SA Goldman JE Fritts Contentbased image retrieval multipleinstance learning Proceedings 19th International Conference Machine Learning Sydney Australia 2002 pp 682689 2320 ZH Zhou et al Artiﬁcial Intelligence 176 2012 22912320 85 Y Zhang ZH Zhou Multilabel dimensionality reduction dependency maximization ACM Transactions Knowledge Discovery Data 4 3 2010 Article 14 86 ZH Zhou K Jiang M Li Multiinstance learning based web mining Applied Intelligence 22 2 2005 135147 87 ZH Zhou XY Liu On multiclass costsensitive learning Proceeding 21st National Conference Artiﬁcial Intelligence Boston WA 2006 pp 567572 88 ZH Zhou JM Xu On relation multiinstance learning semisupervised learning Proceeding 24th International Conference Machine Learning Corvallis OR 2007 pp 11671174 89 ZH Zhou YYu AdaBoost X Wu V Kumar Eds The Top Ten Algorithms Data Mining Chapman Hall Boca Raton FL 2009 pp 127149 90 ZH Zhou ML Zhang Neural networks multiinstance learning Technical report AI Lab Department Computer Science Technology Nanjing University Nanjing China August 2002 91 ZH Zhou ML Zhang Ensembles multiinstance learners Proceeding 14th European Conference Machine Learning CavtatDubrovnik Croatia 2003 pp 492502 92 ZH Zhou ML Zhang Multiinstance multilabel learning application scene classiﬁcation B Schölkopf J Platt T Hofmann Eds Ad vances Neural Information Processing Systems vol 19 MIT Press Cambridge MA 2007 pp 16091616 93 ZH Zhou ML Zhang Solving multiinstance problems classiﬁer ensemble based constructive clustering Knowledge Information Sys tems 11 2 2007 155170