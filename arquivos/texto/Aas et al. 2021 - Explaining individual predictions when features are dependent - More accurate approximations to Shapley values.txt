artiﬁcial intelligence 298 2021 103502 content list available sciencedirect artiﬁcial intelligence wwwelseviercomlocateartint explaining individual prediction feature dependent accurate approximation shapley value kjersti aa martin jullum anders løland norwegian computing center po box 114 blindern n0314 oslo norway r t c l e n f o b s t r c t article history received 3 october 2019 received revised form 5 january 2021 accepted 29 march 2021 available online 31 march 2021 keywords feature attribution shapley value kernel shap dependence explaining complex seemingly simple machine learning model important practical problem want explain individual prediction model learning simple interpretable explanation shapley value game theoretic concept purpose shapley value framework series desirable theoretical property principle handle predictive model kernel shap computationally eﬃcient approximation shapley value higher dimension like existing method approach assumes feature independent shapley value currently suffer inclusion unrealistic data instance feature correlated explanation misleading case simple linear model prediction paper extend kernel shap method handle dependent feature provide example linear nonlinear model degree feature dependence method give accurate approximation true shapley value 2021 author published elsevier bv open access article cc byncnd license httpcreativecommonsorglicensesbyncnd40 1 introduction interpretability crucial complex machine learning model applied area medicine 1 fraud detection 2 credit scoring 3 application complex hardtointerpret machine learning model like deep neural network random forest gradient boosting machine currently outperforming traditional extent interpretable linearlogistic regression model clear tradeoff model complexity model interpretability meaning hard understand sophisticated model perform lack explanation constitutes practical issue trust model 4 legal issue develop model required law explain model exposed automated decision general data protection regulation 5 response new line research emerged focus helping user interpret prediction advanced machine learning method existing work explaining complex model divided main category global local explanation try model term variablesfeatures inﬂuenced general model common method overall explanation permutation based feature importance 6 partial depen dence plot 7 local explanation hand try identify different input variablesfeatures inﬂuenced speciﬁc predictionoutput model referred individual prediction explanation method paper special issue explainable ai corresponding author email address kjerstiaasnrno k aa martinjullumnrno m jullum anderslolandnrno løland httpsdoiorg101016jartint2021103502 00043702 2021 author published elsevier bv open access article cc byncnd license httpcreativecommonsorglicensesbyncnd40 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 explanation particularly useful complex model behave different different feature combination meaning global explanation representative local behavior local explanation method divided category modelspeciﬁc modelagnostic general ex planation method paper focus method category usually try explain individual prediction determining simple interpretable explanation model speciﬁcally given prediction exam ples explanation vector 8 lime local interpretable modelagnostic explanation 4 shapley value 911 approach build concept cooperative game theory 12 series desirable theoretical property 11 shapley value method originally invented assigning payouts player depending contribution total payout explanation setting feature player prediction total payout framework difference prediction average prediction perfectly distributed feature property distinguishes shapley value method like example lime guarantee perfectly distributed effect noted lime shapley value actually explain different thing instance prediction explained probability person crashing car sum shapley value feature equal difference prediction mean probability person crashing car mean taken person having driver license sum lime value equal difference prediction mean probability mean taken person similar person shapley value explain difference prediction global average prediction lime explains difference prediction local average prediction appropriate model explanation consistent human understand model study 11 stronger agreement human explanation shapley value lime shapley value measuring global feature importance instance partition r2 quantity d feature linear regression model shapley regression value assuming independent feature 13 recently dependent feature 1416 general shapley framework global additive importance measure suggested 17 main disadvantage shapley value computational complexity grows exponentially tractable feature led approximation like shapley sampling value 910 kernel shap 11 requires computational power obtain similar approximation accuracy paper focus kernel shap method having desirable property method assumes feature independence observational study machine learning problem rare feature statistically independent meaning shapley value method suffer inclusion prediction based unrealistic data instance feature correlated case simple linear model main contribution paper extend kernel shap method handle dependent feature methodology implemented rpackage shapr available cran 18 paper revised version unpublished paper 19 best knowledge ﬁrst address account dependence shapley value based individual prediction explanation later paper discussing difference approach original kernel shap method termed respectively observational interventional approach succeeding literature lundberg lee 11 advocate observational approach us interventional approach computational reason janzing et al 20 argue causal interpretation shapley value replace conventional conditioning observation conditioning intervention pearl docalculus 21 frye et al 22 suggest socalled asymmetric shapley val ues way incorporate causal knowledge real world restricting possible permutation feature computing shapley value consistent partial causal ordering line approach apply conventional conditioning observation sure explanation respect multivariate distribution data denote data manifold heskes et al 23 generalize work causal shapley value partly based approach deﬁne approach symmetric conditional shapley value chen et al 24 argue preferable general choice observational true data interventional true model approach application dependent abovementioned approach modelagnostic sense explain machine learning method addition method method called treeshap 25 specially designed tree ensemble method like xgboost 26 according author treeshap account feature dependence apparent simulation experiment method inaccurate feature dependent rest paper organized follows section 2 review shapley value kernel shap method sec tion 3 contains main contribution paper proposed approach accounting dependence section 4 give result experiment validating method finally section 5 contains concluding remark 2 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 2 exact shapley value kernel shap approximation section ﬁrst deﬁnition shapley value game theory section 21 explain use context explaining individual prediction section 22 section 23 ﬁnally kernel shap method 21 exact shapley value cooperative game theory consider cooperative game m player aiming maximizing payoff let s m 1 m subset consisting s player assume contribution function v map subset player real number called worth contribution coalition s describes total expected sum payoff member s obtain cooperation shapley value 12 way distribute total gain player assuming collaborate fair distribution sense distribution certain desirable property listed according shapley value player j get φ jv φ j cid2 sm j sm s 1 m v j v j 1 m 1 weighted mean contribution function difference subset s player containing player j note set s sum formula interpreted follows imagine coalition formed player time player demanding contribution v j v fair compensation player compute average contribution permutation possible coalition yielding weighted mean unique coalition illustrate application 1 let consider game player m 1 2 3 possible subset 1 2 3 1 2 1 3 2 3 1 2 3 1 shapley value player given φ1 1 3 φ2 1 3 φ3 1 3 cid3 cid3 cid3 cid4 v1 2 3 v2 3 cid4 v1 2 3 v1 3 cid4 v1 2 3 v1 2 1 6 1 6 1 6 cid3 cid3 cid3 cid4 v1 2 v2 cid4 v1 2 v1 cid4 v1 3 v1 1 6 1 6 1 6 cid3 cid3 cid3 cid4 v1 3 v3 cid4 v2 3 v3 cid4 v2 3 v2 cid3 cid3 cid3 cid4 v1 v cid4 v2 v cid4 v3 v 1 3 1 3 1 3 let deﬁne nondistributed gain φ0 v ﬁxed payoff associated action player zero coalition game summarizing right hand side easily add total worth game φ0 φ1 φ2 φ3 v1 2 3 shapley value following desirable property eﬃciency total gain distributed mcid2 j0 φ j vm symmetry j player contribute equally possible coalition v v j subset s contains j shapley value identical φi φ j v j v player j coalition s m j φ j 0 coalition game described gain function v w combined distributed gain correspond dummy player linearity gain derived v gain derived w φiv w φiv φiw real number φia v aφiv shapley value set value satisfying property 12 27 proof 3 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 22 shapley value prediction explanation consider classical machine learning scenario training set yi xii1ntrain size ntrain train predictive model f x attempting resemble response value y closely possible assume want explain prediction model f x štrumbel kononenko 910 lundberg lee 11 suggest shapley value moving game theory decomposing individual prediction feature contribution single prediction take place payout feature place player prediction f x speciﬁc feature vector x x decomposed follows f x φ0 mcid2 j1 φ j φ0 e f x φ shapley value explain difference global average prediction model form additive feature attribution method prediction y additive feature attribution method adhers property listed section 21 11 appendix discus property useful prediction explanation setting φ j prediction x x f x j able compute shapley value prediction explanation setting need deﬁne contribution know value function v certain subset s function resemble value f x subset s feature quantify follow 11 use expected output predictive model conditional feature value x x s subset v e f xx x s 2 measure conditional median appropriate conditional expectation summarizes probability distribution common estimator prediction application viewed prediction f x minimizer commonly squared error loss function appendix b predictive model linear regression model f x β0 feature x j j 1 m independent 2 shapley value simple form cid5 m j1 β j x j φ0 β0 mcid2 j1 β j ex j φ j β j x j ex j j 1 m 3 note ease notation rest paper dropped superscript φ j value prediction f x explained result different set φ j value best knowledge explicit formula like 3 exists general case dependent feature non linear model m feature number possible subset involved 1 2m number possible subset increase exponentially m increase meaning exact solution problem computationally tractable feature section 23 shall clever approximation method partly overcome issue addition computationally tractable approximation computing shapley value applying method practice requires estimate expectation 2 x main methodological contribution paper describing developing comparing methodology appropriately estimate expectation section 232 stateofart method determining expectation proposed approach section 3 23 kernel shap kernel shap method 11 aim estimating shapley value 2 practical situation method divided separate part clever computationally tractable approximation computing shapley value 1 ii simple method estimating v 11 method presented somewhat limited form order facilitate understanding method consecutive improvement method shall section carefully restate kernel shap method ﬁrst present assuming v known present method estimating v kernel shap 231 approximated weighted square alternative equivalent formula shapley value charnes et al 28 later lundberg lee 11 deﬁne shapley value optimal solution certain weighted square wls problem 4 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 simplest form wls problem stated problem minimizing cid2 sm v φ0 cid2 j φ j2km s 4 cid3 respect φ0 φm s m 1 s m s denoted shapley kernel weight let write z 2m m 1 binary matrix representing possible combination inclusionexclusion m feature ﬁrst column 1 row entry j 1 row l 1 feature j included combination l 0 let v vector containing v w 2m 2m diagonal matrix containing s s case resembles feature combination corresponding row z letting φ vector containing φ0 φm 4 rewritten cid4 m s v z φt w v z φ solution cid71 cid6 φ z t w z z t w v 5 6 m cid5 practice inﬁnite shapley kernel weight m 0 set large constant c example c 106 imposing constraint φ0 v j0 φ j vm problem model contains feature m computing right hand 6 computationally expensive kernel shap trick use weighted square formulation approximate 6 shapley kernel weight different size meaning majority subset s row z contributes little shapley value assuming proper approximation element v consistent approximation obtained sampling replacement subset d m probability distribution following shapley weighting kernel row z d z element v d v computation shapley kernel weight sampling sampled subset weighted equally new square problem note s s m excluded sampling procedure corresponding z row appended z d vselements appended vd diagonal matrix w d extended diagonal element equal c procedure give following approximation 61 cid71 cid8cid6 cid9 φ z t d w d z d z t d w d v d rd v d 7 practical consequence 7 opposed 1 explaining prediction typically case matrix operation producing m 1 d matrix rd carried provided vd precomputed needed explain different prediction model perform matrix multiplication rd vd different v d 232 estimating contribution function feature independence computing vector v need v value possible feature subset represented matrix z z d use approximation 7 stated section 21 contribution value v certain subset s deﬁned v e f xx x s let s denote complement s x s x x expected value computed follows e f xx x s s e f x s x x x cid10 s px s x x f x s x s dx s 8 s able compute exact v s conditional distribution x s given x x px s x x value need conditional distribution px s x x s seldom known step procedure kernel shap method assumes feature independence replacing px s x x s px s 8 assuming feature independence kernel shap intervenes feature breaking dependence feature s s produce interventional shapley value 1 bulk information approximation wls problem obtained personal communication scott lundberg 5 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 training set data train model f empirical distribution x integral 8 approximated monte carlo integration v kershaps 1 k kcid2 k1 f xk s s x 9 s k 1 k sample training data independence assumption sampled indepen xk dently x 3 incorporating dependence kernel shap method feature given model highly dependent kernel shap method correct answer prediction nonrepresentative data instance stated section 1 rare feature real datasets statistically independent place kernel shap method independence assumption px s x px s approximating integral 8 apart rough assumption kernel shap framework stand clever fruitful way approximate shapley value desirable incorporate dependence kernel shap method avoiding independence assumption estimatingapproximating px s x x s directly generate sample distribution instead generating independently x described section 232 propose approach estimating px s x x s assuming gaussian distri bution px ii assuming gaussian copula distribution px iii approximating px s x x s empirical conditional distribution iv combination empirical approach gaussian gaussian copula approach 31 multivariate gaussian distribution assume feature vector x stem multivariate gaussian distribution mean vector μ s multivariate gaussian particular writing covariance matrix cid2 conditional distribution px s x x px px x s nm μ cid2 μ μs μ s cid8 cid9 cid8 cid2 cid2ss cid2s s cid2 s cid2 s s s n sμ s cid2 s give px s x x μ s μ s cid2 s cid2 1 s x s μs cid2 s cid2 s s cid2 s cid2 1 s cid2s s 10 11 instead sampling marginal distribution x s sample gaussian distribution ex pectation vector covariance matrix given 10 11 expectation vector μ covariance matrix cid2 estimated sample mean covariance matrix training data respectively sample s k 1 k conditional distribution integral 8 ﬁnally approximated 9 xk 32 gaussian copula feature far multivariate gaussian distributed instead represent marginals empirical distribution model dependence structure gaussian copula deﬁnition ddimensional copula multivariate distribution c uniformly distributed marginals u01 01 sklars theorem 29 state multivariate distribution f marginals f 1 f 2 fd written f x1 xd cf 1x1 f 2x2 fdxd 12 appropriate ddimensional copula c fact copula 12 expression cu1 ud f f 1 1 u1 f 1 2 u2 f 1 d ud f copula beneﬁt use analytical expression conditionals 10 11 1 j s inverse distribution function marginals copula gaussian assuming gaussian copula use following procedure generating sample px s x x s 6 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 convert marginal x j feature distribution x gaussian feature v j v j cid41 ˆf j x j ˆf j empirical distribution function marginal j assume v distributed according multivariate gaussian2 sample conditional distribution pv s v v s method described section 31 convert margin v j conditional distribution original distribution ˆx j ˆf 1 j cid4v j series sample generated described integral 8 ﬁnally approximated 9 33 empirical conditional distribution dependence structure marginal distribution x far gaussian aforementioned method expected work situation propose nonparametric approach classical method nonparametric density estimation kernel estimator 30 decade following introduction reﬁned developed direction example 3133 kernel estimator suffers greatly curse dimensionality quickly inhibits use multivariate problem method exist nonparametric estimation conditional density especially x x s dimensional finally kernel estimation approach give nonparametric density estimate need able generate sample estimated distribution developed empirical conditional approach sample approximately px s x s method s informative conditional distribution motivated idea sample x s x x close x px s x s consists following step 1 compute distance instance x explained training instance xi 1 ntrain distance x instance computed cid11 d x xi x s xi 1 s x s t cid5 s s xi s 13 cid5s sample covariance matrix ntrain instance x compute distance use element subset s equation 13 viewed scaled version mahalanobis distance 34 2 compute weight training instance xi 1 ntrain distance similarly gaussian distribution kernel cid12 w x xi exp d x xi2 2σ 2 cid13 σ viewed smoothing parameter bandwidth need speciﬁed xi increasing order let x k training instance corresponding kth largest 3 sort weight w x weight 4 approximate integral 8 weighted version 9 v condkershaps cid5 x k k1 w x cid5 k k f x k1 w x xk k s x s 14 s sampled replacement training data weight w xi note 9 xk 1 ntrain approach sampling effective us training observation us weight integral computation input sampling number sample k approximate prediction step 4 instance chosen total sum weight accounted k largest weight cid14 cid5 k l k1 w x cid5 ntrain i1 w x xi x k min ln cid15 η 15 η set instance 09 k 15 exceeds certain limit instance 5000 set limit essentially kernel based estimation procedure kernel density estimation require selection bandwidth parameter method exception choice bandwidth parameter σ viewed bias variance tradeoff small σ put weight closest training observation give low 2 quality assumption depend close gaussian copula true copula 7 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 bias high variance large σ spread weight higher number distant training observation give high bias low variance typically feature highly dependent small σ needed bias dominate feature essentially independent bias σ larger σ preferable σ method approximates original kernel shap method section 232 viewing estimation e f xx x s 1 ntrain turn empirical conditional distribution approach k ntrain equivalent nadaraya watson estimator 35 hurvich et al 36 developed smallsamplesize corrected version akaike information criterion aicc select bandwidth parameter nonparametric regression problem connection nadaraya watson estimator allows apply aicc directly select σ s regression problem response f xi s covariates xi s x strategy aicc ﬁnd suitable smoothing parameter choose σ minimizer aicc log ˆτ 2 cid4h ˆτ 2 1 ntrain ntraincid2 i1 f xi s x s 16 2 cid5 ntrain j1 w x j xi f x cid5 ntrain j1 w x j xi j s s x cid4h 1 trhntrain 1 trh 22 h ntrain ntrain matrix index h j cid5 w x j xi ntrain l1 w xl xi commonly called smoother hat matrix trh trace h select σ compute aicc 16 σ value select σ corresponding smallest aicc value subset s new observation x explained meaning computa tionally intensive approach reduce computational burden experimented different approximation ended σ assumed value subset s size s approach denoted approximate aicc method section 4 sum aicc value subset size minimized instead aicc value subset approximate aicc method time consuming section 4 experimented ﬁxed σ 01 subset s 34 combined approach performing experiment described section 4 turned empirical conditional distribution method work dimension x d small number outperformed d multivariate gaussian method gaussian copula method condition feature accordance previous literature instance 37 reference paper attempt estimate f zx x d 3 dimension z onedimensional higher dimension previously proposed method typically rely prior dimension reduction step result signiﬁcant loss information wise combine empirical approach multivariate gaussian gaussian copula approach simulating conditional distribution dimxs d empirical method conditional distribution parametric method combined approach partly avoid curse dimensionality empirical approach conditioning feature conditional distribution analytically computed gaussian approach determine d use instance cross validation experiment determined d based experience 4 experiment problem evaluating prediction explanation method generally ground truth verify approach accurate original kernel shap method described section 232 dependent feature turn simulated data compute true shapley value computing exact shapley value single prediction requires solving o 2m integral type 8 dimension 1 m 1 perform accurate experiment high dimensional setting perform experiment low dimensional m 3 moderate dimensional m 10 setting multivariate distribution feature x sampling model yx form predictive model f experiment 8 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 m 3 m 10 treated section 42 43 respectively lowmoderate dimension feature experiment computationally tractable use exact version kernel shap 6 turn approximation 7 aicc dependent approximation method directly dependent sampled training set run experi ments 10 batch batch sample new training set size ntrain 2 000 use model ﬁtted training data explain prediction test set size 100 mean quality conditional expectation approximation measured based total ntest 10 100 1 000 test observation sampling new training data batch reduces inﬂuence exact form ﬁtted predictive model compared single training set simulation shapley value approximation compare original kernel shap method original kernel shap gaussian conditional distribution gaussian kernel shap gaussian copula empirical margin copula kernel shap empirical conditional distribution determining σ exact aicc empiricalaiccexact kernel shap empirical conditional distribution determining σ approximate aicc empiricalaicc approx kernel shap empirical conditional distribution setting σ 01 conditional distribution empirical 01 kernel shap combined approach empirical approach subset dimension 3 gaussian approach empirical01gaussian empiricalaiccapproxgaussian kernel shap combined approach empirical approach subset dimension 3 copula approach empirical01copula empiricalaiccapproxcopula computational complexity empiricalaiccexact method 3 dimensional experiment fur thermore combined approach 10 dimensional experiment experiment xgboost ﬁt predictive model include socalled treeshap method 25 comparison 41 evaluation measure quantify accuracy different method rely mean absolute error mae shapley value approximation averaged feature test sample maemethod q 1 mntest mcid2 ntestcid2 j1 i1 φi jtrue φi jq φi jq denotes shapley value feature j prediction computed approximation method q φi jtrue corresponding true value order determine superiority proposed method original kernel shap method rely socalled skill score 38 associated aforementioned mae skill score method q take form skillmae method q maemethod q maeoriginal maeoptimal maeoriginal 1 maemethod q maeoriginal maeoptimal mae optimal method equal zero skill score measure superiority method compared reference method original kernel shap standardized way take value 1 perfect approximation 0 reference method approximation method worse reference method skill score negative compare method equal term method restricted use k 1 000 sample training set feature combination test observation 42 dimension 3 dimensional setting use different sampling model yx linear piecewise constant combine different multivariate distribution feature x gaussian generalized hyperbolic distribu tion gaussian mixture total experimental setup af section 421423 multivariate feature distribution sampling model discussed section 424 425 finally section 426 contains result noise term εi common experiment assumed follow distribution εi low dimension exact shapley value 8 computed numerical integration d ε n0 012 9 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 421 gaussian distributed feature ﬁrst feature distribution px shall consider multivariate gaussian distribution px n30 cid2ρ covariance matrix cid2ρ take form cid2ρ 17 1 ρ ρ ρ 1 ρ ρ ρ 1 experiment correlation coeﬃcient ρ varies 0 098 representing increasing positive correlation feature 422 skewed heavytailed distributed feature second feature distribution px considered generalized hyperbolicghdistribution following 39 random vector x said follow ghdistribution index parameter λ concentration parameter ω location vector μ dispersion matrix cid2 skewness vector β denoted x ghλ ω μ cid2 β represented x μ w β w u w gigλ ω ω u n0 cid2 w independent u gig generalized inverse gaussian distribution introduced 40 appendix c contains detail distribution use following parameter value experiment λ 1 ω 05 cid2 cid20 diag1 β 14κ 1 μ 0 ew 14κ skewness coeﬃcient κ varies 1 10 different experiment resulting increasingly skewed heavytailed positively correlated distribution ew denotes mean gig distribution equal approximately 456 parameter value special form μ chosen ghdistribution mean zero 423 multimodal distributed feature feature distribution mixture gaussian distribution different mean px 2cid2 k1 πknμkγ cid202 mixture probability π1 π2 05 mean function μ1γ μ2γ γ 1 05 1cid8 covariance matrix cid2 assumed form 17 γ parameter represents distance gaussian distribution range 05 10 different experiment 424 linear model ﬁrst sampling model yx simple linear model y gx x1 x2 x3 ε data distribution modeled ﬁtting parameter β j j 0 3 linear model f x β0 β1 x1 β2 x2 β3 x3 ε ordinary linear regression 425 piecewise constant model second sampling model yx piecewise constant model constructed summing different piecewise stant function y gx fun1x1 fun2x2 fun3x3 ε function fun1 fun2 fun3 displayed fig 1 ﬁt model gx xgboost framework 26 default value hyperparameters histogram tree learning method 50 boosting iteration 10 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 1 piecewise constant function experiment d e f interpretation color ﬁgures reader referred web version article 426 result result 3dexperiments visualized fig 2 3 experiment linear sampling model gaussian feature seen upper row fig 2 original kernel shap method work feature independent outperformed method ρ greater 005 gaussian model generally show best performance noted aicc method determining σ work better ﬁxed value 01 experiment b linear sampling model skewed heavytailed feature like previous experiment original kernel shap method outperformed approach shown middle row fig 2 κ increase mae value copula gaussian method reduced increased variance feature come byproduct increasing κ parameter experiment empirical approach ﬁxed σ 01 performs uniformly better based aicc experiment c sampling model bimodal gaussian mixture distributed feature method perform uniformly better original kernel shap method lower row fig 2 show empirical method outperform gaussian copula method especially γ distance mode feature distribution large experiment df use feature distribution ac experiment use piecewise constant model instead linear result largely linear model case original kernel shap method outperformed approach gaussian model best gaussian feature empirical approach best feature distribution bimodal case skewed heavytailed feature gaussian copula method preferable larger value κ experiment piecewise constant model treeshap method addition one shown fig 3 performance method slightly better original kernel shap method experiment d e worse experiment f surprising treeshap method supposed handle dependence better original kernel shap method overall 3 dimensional experiment clearly important account dependence feature computing shapley value suggested method best depends underlying feature distribution result empirical method approximate aicc version fairly similar case better slower exact version recommend 43 dimension 10 10 dimensional case restricted type experiment ﬁrst use gaussian distributed feature form described 3 dimensional case section 421 px n100 cid2ρ cid2ρ 10 dimensional extension 17 feature variance 1 pairwise correlation feature ρ ﬁrst experiment us sampling model yx directly extending 3 dimensional linear model section 424 y gx 9cid2 j1 x j ε 11 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 2 dimension 3 mae skill score linear model upper row gaussian feature middle row ghdistributed feature lower row gaussian mixture distributed feature 12 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 3 dimension 3 mae skill score piecewise constant model upper row gaussian feature middle row ghdistributed feature lower row gaussian mixture distributed feature 13 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 4 dimension 10 mae skill score linear model gaussian distributed feature use error term εi case y modeled ordinary linear regression d ε n0 012 3 dimensional experiment analogous 3 dimensional f x 10cid2 j1 β j x j ε second experiment extends 3 dimensional experiment described section 425 taking form y gx cid2 fun1x j cid2 fun2x j cid2 fun3x j ε j 123 j 456 j 789 excluding effect 10th feature 3 dimensional case model gx ﬁtted xgboost framework setting note setting feature 10 direct inﬂuence y experiment simulate data 10 dimensional ghdistribution following parameter value λ 1 ω 05 μ 3 3 3 3 3 3 3 3 3 3 cid2 diag1 2 3 1 2 3 1 2 3 3 β 1 1 1 1 1 05 05 05 05 05 sampling model use piecewise constant model described didnt use empiricalaiccexact approach 10 dimensional experiment previously stated approach computationally intensive 3 dimensional experiment showed performance empiricalaiccexact empiricalaiccapprox approach similar 3 dimensional experiment generalized hyperbolic feature piecewise constant model mae skill score empirical approach equal meaning necessary use signiﬁcantly computational heavy aicc approach empirical approach experiment empirical01 method combined approach use empirical approach subset dimension 3 bandwidth parameter σ deter mined approximate aicc method linear case ﬁxed 01 result simulation experiment shown fig 4 5 table 1 numerical integration compute exact shapley value 3 dimensional experiment turn monte carlo integration 10 dimensional case use 9 computing true shapley value sample true conditional distribution instead sample estimated conditional distribution 14 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 5 dimension 10 mae skill score piecewise constant model gaussian distributed feature table 1 dimension 10 mae skill score piecewise stant model ghdistributed feature approach original gaussian copula empirical01 empirical01gaussian empirical01copula treeshap mae 1182 0377 0526 0307 0199 0236 1181 skill score 0000 0633 0504 0737 0821 0791 0014 ﬁgures result gaussian distributed feature dimension 10 3 dimensional counterpart fig 2 gaussian method generally showing best performance combined empirical gaussiancopula approach work piecewise constant model treeshap method behaves 3 dimensional case slightly better original kernel shap method small medium sized dependence worse high dependence feature skewed heavytailed data table 1 show empirical01gaussian method give best perfor mance having slightly better mae value skill score empirical01copula method finally like experiment suggested approach outperform original kernel shap treeshap method 44 real data example example use real data set data set consists 28 feature extracted 6 transaction time series previously predicting mortgage default relating probability default transaction information 3 transaction information consists daily balance consumer credit kk checking br saving sk account addition daily number transaction checking account tr transferred checking account inn sum checking saving credit card account sum time series length 365 day mean mean maximum max minimum min standard deviation std coeﬃcient variation cv computed resulting 28 feature scaled mean 0 standard deviation 1 computation fig 6 show histogram feature remaining feature similar distribution feature distri butions skewed heavytailed pairwise rank correlation measured kendall τ 41 shown fig 7 use kendall τ instead linear correlation coeﬃcient variable far linearly dependent fig 9 example rank correlation fig 7 close 0 group feature high mutual corre lations expect approach accurate approximation true shapley value original kernel shap method assumes independent feature 15 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 6 histogram variable real data set data set divided training set test set containing 12696 1921 observation respectively ﬁtted xgboost model 50 tree training data default parameter setting resulting auc test data 088 example section 43 showed case skewed heavytailed feature piecewise stant model combined approach superior empirical01gaussian approach best performing method assume case real data set compare performance method original kernel shap approach fig 8 show shapleyvalues individual data set shapley value different indi vidual large difference variable br_mean br_min br_max variable sum_min sum_mean sum_max kendall τ matrix fig 7 variable positively correlated variable surprising shapley value method different indi vidual b observe large difference variable sum_min sum_mean sum_max addition worth noticing shapley value variable kk_cv opposite sign variable strongly negatively correlated variable kk_std addition positively correlated kk_min kk_mean kk_max previously stated problem evaluating shapley value real data ground truth justify result way shown 1 shapley value weighted sum difference v j v subset s method original kernel shap method v estimated 9 method differ original kernel shap method assumes feature independence generating sample conditional distribution px s x x s able sample conditional distribution generated method representative sample generated original kernel shap method likely shapley value obtained method accurate obtained original kernel shap method conditional distribution involved shapley formula 28 variable impossible included example illustrate method give correct approximation true conditional distribution original kernel shap approach fig 9 show plot 16 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 7 kendall τ correlation matrix real data set br_min br_max br_std br_max black dot training data turquoise dot sample conditional distribution variable xaxis given br_max equal zero generated method red dot corresponding sample generated original kernel shap approach original kernel shap approach generates sample unrealistic sense far outside range observed training data known evaluation predictive machine learning model far domain trained lead spurious prediction important explanation method evaluating predictive model appropriate feature combination sample generated method inside range observed training data fig 10 study different conditional distribution involved shapley formula conditional distribution sum_min sum_mean sum_max given variable conditional distribution variable inn_cv given inn_cv conditional distribution variable kk_mean br_max given kk_mean br_max distribution generate 1000 sample individual test data set method original kernel shap approach condition different set value combination individual conditional distribution method compute mean mahalanobis distance sample nearest training sample resulting 1000 different mean distance panel fig 10 show probability density mean distance scaling mean distance average original method distance sized relative mean distance original method generated sample realistic expect majority mean distance small starting leftmost column mode density corresponding method smaller corresponding original kernel shap approach indicates sample generated approach realistic generated original kernel shap approach kendall τ matrix fig 7 variable sum_min sum_mean sum_max positively correlated variable make sense method provides better estimate conditional distribution 17 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 8 shapley value person real data set computed method original kernel shap method proceed second column density approach similar surprising shown fig 7 variable inn_cv uncorrelated variable finally rightmost column mode density corresponding method smaller corresponding original kernel shap approach difference large ﬁrst column believe explained fact variable condition kk_mean strongly correlated variable summarize illustrated shapley value computed method original method different tried justify fact method give correct approximation true conditional distribution original kernel shap approach 5 summary discussion shapley value modelagnostic method explaining individual prediction solid theoretical foundation main disadvantage method computational complexity grows exponentially number feature led approximation kernel shap method known key ingredient kernel shap method conditional distribution subset s feature conditional feature s subset original version kernel shap method implicitly assumed feature subset independent meaning conditional distribution replaced marginal distribution 18 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 fig 9 plot br_min br_max left br_std br_max right black dot training data turquoise dot sample conditional distribution variable xaxis given br_max equal zero generated method red dot corresponding sample generated original kernel shap approach fig 10 probability density mean mahalanobis distance different conditional distribution different individual text description 19 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 feature s high degree dependence feature independence assumption lead unrealistic combination feature value evaluation predictive machine learning model far domain trained lead spurious prediction resulting shapley value correct paper introduces modiﬁed version kernel shap method handle dependent feature proposed different approach estimating conditional distribution assuming gaussian multivariate distribution feature assuming gaussian copula empirical margin empirical approach combination empirical approach gaussian gaussian copula approach performed comprehensive simulation study linear nonlinear model gaussian nongaussian distribution dimension 3 10 method accurate approximation true shapley value original kernel shap approach nonlinear model method clearly outperformed treeshap method best knowledge shapley approach try handle dependence feature prediction explanation setting performing experiment turned nonparametric approach superior conditioning small number feature outperformed gaussian copula method condition variable regard combined approach promising proposed method approach applied real case 28 variable prediction explained produced xgboost classiﬁer designed predict mortgage default case true shapley value known provide result indicate combined approach provides sensible approximation original kernel shap method having desirable property method obvious drawback computational time timeconsuming aicc computation bandwidth parameter nonparametric approach ﬁxed bandwidth parameter instead computational time combined approach original kernel shap method recently attempt underlying graph structure input data reduce computational complexity 4243 conditional independence requirement satisﬁed graph ﬁrst divided separate community kernel shap method applied community individually method proposed 4243 tested prediction obtained text image classiﬁcation setting data graph structure enabling factorization separate community main challenge method tabular data potentially huge number test conditional dependence performed deﬁnitely worth closer look paper assume aim explain actual prediction model model predic tion probability shapley framework decompose probability summing subset φi value theory produce value range 01 case natural assume importance feature additive log odds space space probability problem solution straightforward human interpret log odds contribution probability decompose practical mathematical question tabular data contain ordered nonordered categorical data proposed nonparametric approach categorical variable converted numerical one simplest solution use onehotencoding large data set handle categorical feature hundred category meaning method need handle large number binary attribute alternative approach use idea clustering literature 44 45 deﬁning distance function handle categorical mixedtype feature generalization mahalanobis distance treating data mixture nominal ordinal continuous variable instead approach described instance 46 come parametric approach categorical data represents greater challenge promising alternative use entity embedding 47 convert categorical feature numerical one treat feature similarly numerical feature declaration competing author declare known competing ﬁnancial interest personal relationship appeared inﬂuence work reported paper acknowledgement author grateful scott lundberg valuable advice concerning kernel shap method want thank ingrid hobæk haff suggesting copula approach nikolai sellereite help implementing method rpackage shapr work supported norwegian research council grant 237718 big insight appendix shapley property prediction explanation setting shapley value prediction explanation φ0 v e f x actually play important role quantifying prediction feature merely global average prediction φ0 large absolute value compared φ j j cid13 0 feature said unimportant speciﬁc prediction 20 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 prediction explanation setting interpretation property shapley value discussed section 21 follows m eﬃciency sum shapley value different feature equal difference prediction e f x property ensures prediction global average prediction value explained global mean prediction fully devoted explained feature φ j j 1 m compared different prediction f x j1 φ j f x cid5 symmetry shapley value feature equal combined subset feature contribute equally prediction failure fulﬁll criterion contribution different feature explanation inconsistent untrustworthy explanation dummy player feature change prediction matter feature combined shapley value 0 assigning nonzero explanation value feature inﬂuence prediction odd natural requirement linearity prediction function consists sum prediction function shapley value feature identical sum feature shapley value individual prediction function hold linear combination prediction function property ensures model form random forest structurally simple ensemble model explained interpreted individually failure fulﬁll basic advantageous property give odd undesirable inconsistent explanation frame work additive explanation method shapley value satisﬁes criterion 11 appendix b shapley value model linear section ﬁrst proof explicit formula shapley value predictive model linear regression model feature independent obtain contribution function v model linear feature dependent b1 linear model independent feature v e f xx x s predictive model linear regression model feature independent shapley value simple form φ j β j x j ex j j 1 m proof derive expression v case v e f xx x cid10 s f x s x s px s x x s dx s cid10 cid2 βi xi cid2 s cid10 cid2 s cid2 s βi xi pxi dxi βi exi βi x cid2 βi x px s x x s dx s cid10 cid2 βi x px s dx s b1 b2 b3 transition step b1 b2 follows assumption linear model transition b2 b3 follows independent feature assumption having computed v expression v j simply v j v β j x j β j ex j meaning v j v β j cid6 x j cid7 ex j case difference v j v independent s shapley formula written 21 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 φ j β j cid6 x j cid7 cid2 ex j sm j sm s 1 m sum shapley weight 1 φ j β j cid6 x j cid7 ex j j 1 m cid2 b2 linear model dependent feature model linear feature independent v instead derived follows v e f xx x cid10 s cid2 s cid2 s f x s x s px s x x s dx s cid10 cid2 βi xi cid2 s cid10 βi s dxi xi pxixs x s βi exixs x cid2 βi x f x s ex s x x s x x s βi x px s x x s dx s cid10 cid2 βi x px s dx s b4 b5 b6 b7 case avoid timeconsuming simulation able analytically obtain proper estimate ex s x x s straightforward general appendix c generalized hyperbolic distribution density ddimensional generalized hyperbolic random vector x cid8 px ω δx μ cid2 ω β t cid21β ω δx μ cid2ω β t cid21β cid27 2π d2cid212 kλω exp x μt cid21β cid24cid25 cid9 λd2 2 kλd2 cid26 cid28 δx μ cid2 x μt cid21x μ squared mahalanobis distance x μ kλ modiﬁed bessel function kind index λ mean vector covariance matrix x ex μ ew β varx ew cid2 varw ββ t shown 48 x partitioned x t 2 t x 1 d1dimensional x 2 d2dimensional conditional distribution x 2 given x 1 x1 generalized hyperbolic distributed x 2x 1 x1 gh λ21 χ21 ψ21 μ21 cid221 β 21 1 x t λ21 λ d12 χ21 ω x1 μ1t cid2 ψ21 ω β t 11β 1 1 cid5t 1 11 x1 μ1 cid2t μ2 μ21 cid221 cid222 cid2t cid2t β 2 1 11 x1 μ1 12cid2 1 11 cid212 12cid2 1 12cid2 11 β 1 β 21 gh slightly different parameterization gh distribution conditional distribution technical reason use parameterization proposed 49 cid24cid25 cid26 cid8 px χ δx μ cid2 ψ β t cid21β cid9 λd2 2 ψχ λ2 kλd2 2π d2cid212 kλ χ δx μ cid2ψ β t cid21β cid27 cid28 χ ψ exp x μt cid21β 22 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 reference 1 z obermeyer ej emanuel predicting future big data machine learning clinical medicine n engl j med 375 2016 1216 2 sudjianto s nair m yuan zhang d kern f celadíaz statistical method ﬁghting ﬁnancial crime technometrics 52 2010 519 3 h kvamme n sellereite k aa s sjursen predicting mortgage default convolutional neural network expert syst appl 102 2018 207217 4 mt ribeiro s singh c guestrin trust explaining prediction classiﬁer proceeding 22nd acm sigkdd interna tional conference knowledge discovery data mining kdd acm 2016 pp 11351144 5 european union regulation eu 2016679 european parliament council 27 april 2016 protection natural person regard processing personal data free movement data repealing directive 9546ec general data protection regulation j eur union 59 2016 6 fisher c rudin f dominici model wrong useful learning variable importance studying entire class prediction model simultaneously j mach learn re 20 2019 181 7 c molnar interpretable machine learning guide making black box model explainable httpschristophm github io interpretable ml book 2020 11 2010 18031831 8 d baehrens t schroeter s harmeling m kawanabe k hansen kr müller explain individual classiﬁcation decision j mach learn re 9 e štrumbel kononenko eﬃcient explanation individual classiﬁcations game theory j mach learn re 11 2010 118 10 e štrumbel kononenko explaining prediction model individual prediction feature contribution knowl inf syst 41 2014 647665 11 sm lundberg si lee uniﬁed approach interpreting model prediction advance neural information processing system curram asso ciates 2017 pp 47684777 12 l shapley value nperson game contribution theory game vol 2 1953 pp 307317 13 ab owen sobol index shapley value siamasa j uncertain quantiﬁcat 2 2014 245251 14 e song bl nelson j staum shapley effect global sensitivity analysis theory computation siamasa j uncertain quantiﬁcat 4 2016 10601083 15 ab owen c prieur shapley value measuring importance dependent input siamasa j uncertain quantiﬁcat 5 2017 9861002 16 p giudici e raﬃnetti shapleylorenz explainable artiﬁcial intelligence expert syst appl 2020 114104 17 covert sm lundberg si lee understanding global feature contribution additive importance measure adv neural inf process syst 33 18 n sellereite m jullum shapr rpackage explaining machine learning model dependenceaware shapley value j open sour softw 5 19 k aa m jullum al land explaining individual prediction feature dependent accurate approximation shapley value arxiv 2020 2020 2027 1903 10464v1 2019 20 d janzing l minorics p blöbaum feature relevance quantiﬁcation explainable ai causal problem international conference artiﬁcial intelligence statistic pmlr 2020 pp 29072916 21 j pearl causality cambridge university press 2009 22 c frye c rowat feige asymmetric shapley value incorporating causal knowledge modelagnostic explainability adv neural inf process syst 23 t heskes e sijben ig bucur t claassen causal shapley value exploiting causal knowledge explain individual prediction complex model 24 h chen jd janizek s lundberg si lee true model true data 2020 icml workshop human interpretability machine 25 sm lundberg si lee consistent feature attribution tree ensemble proceeding 34th international conference machine learning 33 2020 adv neural inf process syst 33 2020 learning whi 2020 2020 jmlr wcp 2017 pp 1521 26 t chen c guestrin xgboost scalable tree boosting proceeding 22nd acm sigkdd international conference knowledge discovery data mining kdd acm 2016 pp 785794 27 hp young monotonic solution cooperative game int j game theory 14 1985 6572 28 charnes b golany m keane j rousseau extremal principle solution game characteristic function form core chebychev shapley value generalization springer netherlands dordrecht 1988 pp 123133 29 sklar fonctions répartition à n dimension et leurs marge publ inst stat univ paris 8 1959 229231 30 m rosenblatt remark nonparametric estimate density function ann math stat 27 1956 832837 31 mp holmes ag gray cl isbell fast kernel conditional density estimation dualtree monte carlo approach comput stat data anal 54 2010 17071718 32 k bertin c lacour v rivoirard adaptive pointwise estimation conditional density function ann inst henri poincaré probab stat 52 2016 939980 33 r izbicki ab lee converting highdimensional regression highdimensional conditional density estimation electron j stat 11 2017 28002831 34 pc mahalanobis generalised distance statistic proceeding national institute science india 1936 pp 4955 35 hj bierens nadarayawatson kernel regression function estimator topic advanced econometrics cambridge university press 1994 36 cm hurvich j simonoff cl tsai smoothing parameter selection nonparametric regression improved akaike information criterion j r pp 212247 stat soc ser b stat methodol 60 1998 271293 37 r izbicki ab lee converting highdimensional regression highdimensional conditional density estimation electron j stat 11 2017 28002831 38 t gneiting ae raftery strictly proper scoring rule prediction estimation j stat assoc 102 2007 359378 39 rp browne pd mcnicholas mixture generalized hyperbolic distribution j stat 43 2015 176198 40 ij good population frequency specie estimation population parameter biometrika 40 1953 237264 41 mg kendall new measure rank correlation biometrika 30 1938 8193 42 j chen l song mj wainwright mi jordan lshapley cshapley eﬃcient model interpretation structured data corr arxiv1808 02610 ab 2018 43 x li nc dvornek y zhou j zhuang p ventola j duncan eﬃcient interpretation deep learning model graph structure cooperative game theory application asd biomarker discovery international conference information processing medical imaging springer 2019 pp 718730 44 z huang clustering large data set mixed numeric categorical variable proceeding paciﬁc asia knowledge discovery data mining conference 1997 pp 2134 45 z huang extension kmeans algorithm clustering large data set categorical variable data min knowl discov 2 1998 283304 46 ar leon kc carriere generalized mahalanobis distance mixed data j multivar anal 92 2005 174185 23 k aa m jullum løland artiﬁcial intelligence 298 2021 103502 47 c guo f berkhahn entity embeddings categorical variable arxiv1604 06737v1 2016 48 y wei y tang pd mcnicholas mixture generalized hyperbolic distribution mixture skewt distribution modelbased clustering incomplete data comput stat data anal 130 2019 1841 49 aj mcneil r frey p embrechts quantitative risk management concept technique tool princeton university press 2006 24 