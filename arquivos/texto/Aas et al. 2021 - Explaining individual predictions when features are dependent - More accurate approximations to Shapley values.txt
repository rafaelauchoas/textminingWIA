Artiﬁcial Intelligence 298 2021 103502 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Explaining individual predictions features dependent More accurate approximations Shapley values Kjersti Aas Martin Jullum Anders Løland Norwegian Computing Center PO Box 114 Blindern N0314 Oslo Norway r t c l e n f o b s t r c t Article history Received 3 October 2019 Received revised form 5 January 2021 Accepted 29 March 2021 Available online 31 March 2021 Keywords Feature attribution Shapley values Kernel SHAP Dependence Explaining complex seemingly simple machine learning models important practical problem We want explain individual predictions models learning simple interpretable explanations Shapley value game theoretic concept purpose The Shapley value framework series desirable theoretical properties principle handle predictive model Kernel SHAP computationally eﬃcient approximation Shapley values higher dimensions Like existing methods approach assumes features independent Since Shapley values currently suffer inclusion unrealistic data instances features correlated explanations misleading This case simple linear model predictions In paper extend Kernel SHAP method handle dependent features We provide examples linear nonlinear models degrees feature dependence method gives accurate approximations true Shapley values 2021 The Authors Published Elsevier BV This open access article CC BYNCND license httpcreativecommonsorglicensesbyncnd40 1 Introduction Interpretability crucial complex machine learning model applied areas medicine 1 fraud detection 2 credit scoring 3 In applications complex hardtointerpret machine learning models like deep neural networks random forests gradient boosting machines currently outperforming traditional extent interpretable linearlogistic regression models However clear tradeoff model complexity model interpretability meaning hard understand sophisticated models perform This lack explanation constitutes practical issue I trust model 4 legal issue develop model required law explain model exposed automated decisions General Data Protection Regulation 5 In response new line research emerged focuses helping users interpret predictions advanced machine learning methods Existing work explaining complex models divided main categories global local explanations The tries model terms variablesfeatures inﬂuenced general model Two common methods overall explanation permutation based feature importance 6 partial depen dence plots 7 Local explanations hand try identify different input variablesfeatures inﬂuenced speciﬁc predictionoutput model referred individual prediction explanation methods Such This paper Special Issue Explainable AI Corresponding author Email addresses kjerstiaasnrno K Aas martinjullumnrno M Jullum anderslolandnrno A Løland httpsdoiorg101016jartint2021103502 00043702 2021 The Authors Published Elsevier BV This open access article CC BYNCND license httpcreativecommonsorglicensesbyncnd40 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 explanations particularly useful complex models behave different different feature combinations meaning global explanation representative local behavior Local explanation methods divided categories modelspeciﬁc modelagnostic general ex planation methods In paper focus The methods category usually try explain individual predictions determining simple interpretable explanations model speciﬁcally given prediction Three exam ples Explanation Vectors 8 LIME Local Interpretable Modelagnostic Explanations 4 Shapley values 911 The approach builds concepts cooperative game theory 12 series desirable theoretical properties 11 The Shapley value method originally invented assigning payouts players depending contribution total payout In explanation setting features players prediction total payout In framework difference prediction average prediction perfectly distributed features This property distinguishes Shapley values methods like example LIME guarantee perfectly distributed effects It noted LIME Shapley values actually explain different things For instance prediction explained probability person A crashing car sum Shapley values features equal difference prediction mean probability person crashing car mean taken persons having driver license The sum LIME values equal difference prediction mean probability mean taken persons similar person A That Shapley values explain difference prediction global average prediction LIME explains difference prediction local average prediction Appropriate model explanations consistent humans understand model In study 11 stronger agreement human explanations Shapley values LIME Shapley values measuring global feature importance For instance partition R2 quantity d features linear regression model Shapley regression values assuming independent features 13 recently dependent features 1416 A general Shapley framework global additive importance measures suggested 17 The main disadvantage Shapley value computational complexity grows exponentially tractable features This led approximations like Shapley Sampling Values 910 Kernel SHAP 11 The requires computational power obtain similar approximation accuracy Hence paper focus Kernel SHAP method While having desirable properties method assumes feature independence In observational studies machine learning problems rare features statistically independent meaning Shapley value methods suffer inclusion predictions based unrealistic data instances features correlated This case simple linear model The main contribution paper extend Kernel SHAP method handle dependent features The methodology implemented Rpackage shapr available CRAN 18 Our paper revised version unpublished paper 19 best knowledge ﬁrst address account dependence Shapley value based individual prediction explanation Later papers discussing difference approach original Kernel SHAP method termed respectively observational interventional approach succeeding literature Lundberg Lee 11 advocate observational approach uses interventional approach computational reasons Janzing et al 20 argue causal interpretation Shapley values replace conventional conditioning observation conditioning intervention Pearls docalculus 21 Frye et al 22 suggest socalled asymmetric Shapley val ues way incorporate causal knowledge real world restricting possible permutations features computing Shapley values consistent partial causal ordering In line approach apply conventional conditioning observation sure explanations respect multivariate distribution data denote data manifold Heskes et al 23 generalize work causal Shapley values partly based approach They deﬁne approach symmetric conditional Shapley values Chen et al 24 argue preferable general choice observational true data interventional true model approach application dependent All abovementioned approaches modelagnostic sense explain machine learning method In addition methods method called TreeSHAP 25 specially designed tree ensemble methods like XGBoost 26 According authors TreeSHAP accounts feature dependence As apparent simulation experiments method inaccurate features dependent The rest paper organized follows Section 2 reviews Shapley values Kernel SHAP method Sec tion 3 contains main contribution paper proposed approaches accounting dependence Section 4 gives results experiments validating methods Finally Section 5 contains concluding remarks 2 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 2 The exact Shapley value Kernel SHAP approximation In section ﬁrst deﬁnition Shapley value game theory Section 21 explain use context explaining individual predictions Section 22 In Section 23 ﬁnally Kernel SHAP method 21 The exact Shapley value cooperative game theory Consider cooperative game M players aiming maximizing payoff let S M 1 M subset consisting S players Assume contribution function vS maps subsets players real numbers called worth contribution coalition S It describes total expected sum payoffs members S obtain cooperation The Shapley value 12 way distribute total gains players assuming collaborate It fair distribution sense distribution certain desirable properties listed According Shapley value player j gets φ jv φ j cid2 SM j SM S 1 M vS j vS j 1 M 1 weighted mean contribution function differences subsets S players containing player j Note set S sum The formula interpreted follows Imagine coalition formed player time player demanding contribution vS j vS fair compensation Then player compute average contribution permutations possible coalitions yielding weighted mean unique coalitions To illustrate application 1 let consider game players M 1 2 3 Then possible subsets 1 2 3 1 2 1 3 2 3 1 2 3 Using 1 Shapley values players given φ1 1 3 φ2 1 3 φ3 1 3 cid3 cid3 cid3 cid4 v1 2 3 v2 3 cid4 v1 2 3 v1 3 cid4 v1 2 3 v1 2 1 6 1 6 1 6 cid3 cid3 cid3 cid4 v1 2 v2 cid4 v1 2 v1 cid4 v1 3 v1 1 6 1 6 1 6 cid3 cid3 cid3 cid4 v1 3 v3 cid4 v2 3 v3 cid4 v2 3 v2 cid3 cid3 cid3 cid4 v1 v cid4 v2 v cid4 v3 v 1 3 1 3 1 3 Let deﬁne nondistributed gain φ0 v ﬁxed payoff associated actions players zero coalition games By summarizing right hand sides easily add total worth game φ0 φ1 φ2 φ3 v1 2 3 The Shapley value following desirable properties Eﬃciency The total gain distributed Mcid2 j0 φ j vM Symmetry If j players contribute equally possible coalitions vS vS j subset S contains j Shapley values identical φi φ j If vS j vS player j coalitions S M j φ j 0 If coalition games described gain functions v w combined distributed gains correspond Dummy player Linearity gains derived v gains derived w φiv w φiv φiw Also real number φia v aφiv The Shapley values set values satisfying properties 12 27 proofs 3 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 22 Shapley values prediction explanation Consider classical machine learning scenario training set yi xii1ntrain size ntrain train predictive model f x attempting resemble response value y closely possible Assume want explain prediction model f x Štrumbel Kononenko 910 Lundberg Lee 11 suggest Shapley values By moving game theory decomposing individual prediction feature contributions single prediction takes place payout features place players We prediction f x speciﬁc feature vector x x decomposed follows f x φ0 Mcid2 j1 φ j φ0 E f x φ That Shapley values explain difference global average prediction A model form additive feature attribution method prediction y additive feature attribution method adhers properties listed Section 21 11 In Appendix A discuss properties useful prediction explanation setting φ j prediction x x f x j To able compute Shapley values prediction explanation setting need deﬁne contribution know value function vS certain subset S This function resemble value f x subset S features To quantify follow 11 use expected output predictive model conditional feature values xS x S subset vS E f xxS x S 2 Other measures conditional median appropriate However conditional expectation summarizes probability distribution common estimator prediction applications When viewed prediction f x minimizer commonly squared error loss function We Appendix B predictive model linear regression model f x β0 features x j j 1 M independent 2 Shapley values simple form cid5 M j1 β j x j φ0 β0 Mcid2 j1 β j Ex j φ j β j x j Ex j j 1 M 3 Note ease notation rest paper dropped superscript φ j values Every prediction f x explained result different sets φ j values To best knowledge explicit formula like 3 exists general case dependent features non linear models With M features number possible subsets involved 1 2M Hence number possible subsets increases exponentially M increases meaning exact solution problem computationally tractable features In Section 23 shall clever approximation method partly overcome issue In addition computationally tractable approximation computing Shapley values applying method practice requires estimate expectation 2 xS The main methodological contribution paper describing developing comparing methodology appropriately estimate expectations In Section 232 stateofart method determining expectations proposed approaches Section 3 23 Kernel SHAP The Kernel SHAP method 11 aims estimating Shapley values 2 practical situations The method divided separate parts A clever computationally tractable approximation computing Shapley values 1 ii A simple method estimating vS In 11 method presented somewhat limited form In order facilitate understanding method consecutive improvements method shall section carefully restate Kernel SHAP method We ﬁrst present assuming vS known present method estimating vS Kernel SHAP 231 Approximated weighted squares There alternative equivalent formulas Shapley values Charnes et al 28 later Lundberg Lee 11 deﬁne Shapley values optimal solution certain weighted squares WLS problem 4 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 In simplest form WLS problem stated problem minimizing cid2 SM vS φ0 cid2 jS φ j2kM S 4 cid3 respect φ0 φM kM S M 1 S M S denoted Shapley kernel weights Let write Z 2M M 1 binary matrix representing possible combinations inclusionexclusion M features ﬁrst column 1 row entry j 1 row l 1 feature j included combination l 0 Let v vector containing vS W 2M 2M diagonal matrix containing kM S S cases resembles feature combinations corresponding row Z Letting φ vector containing φ0 φM 4 rewritten cid4 M S v Z φT W v Z φ solution cid71 cid6 φ Z T W Z Z T W v 5 6 M cid5 In practice inﬁnite Shapley kernel weights kM M kM 0 set large constant c example c 106 imposing constraints φ0 v j0 φ j vM problem When model contains features M computing right hand 6 computationally expensive The Kernel SHAP trick use weighted squares formulation approximate 6 The Shapley kernel weights different sizes meaning majority subsets S rows Z contributes little Shapley value Hence assuming proper approximation elements v consistent approximation obtained sampling replacement subset D M probability distribution following Shapley weighting kernel rows Z D Z elements v D v computation As Shapley kernel weights sampling sampled subsets weighted equally new squares problem Note S S M excluded sampling procedure As corresponding Z rows appended Z D vSelements appended vD diagonal matrix W D extended diagonal elements equal c This procedure gives following approximation 61 cid71 cid8cid6 cid9 φ Z T D W D Z D Z T D W D v D RD v D 7 A practical consequence 7 opposed 1 explaining predictions typically case matrix operations producing M 1 D matrix RD carried Provided vD precomputed needed explain different predictions model perform matrix multiplication RD vD different v D 232 Estimating contribution function feature independence When computing vector v need vS values possible feature subsets represented matrix Z Z D use approximation 7 As stated Section 21 contribution value vS certain subset S deﬁned vS E f xxS x S Let S denote complement S x S x xS Then expected value computed follows E f xxS x S S E f x S xS xS x cid10 S px S xS x f x S x S dx S 8 S Hence able compute exact vS S conditional distribution x S given xS x px S xS x values need conditional distribution px S xS x S seldom known In step procedure Kernel SHAP method assumes feature independence replacing px S xS x S px S 8 Hence assuming feature independence Kernel SHAP intervenes features breaking dependence features S S produces interventional Shapley values 1 The bulk information approximation WLS problem obtained personal communication Scott Lundberg 5 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Using training set data train model f empirical distribution x integral 8 approximated Monte Carlo integration v KerSHAPS 1 K Kcid2 k1 f xk S S x 9 S k 1 K samples training data Due independence assumption sampled indepen Here xk dently xS 3 Incorporating dependence Kernel SHAP method If features given model highly dependent Kernel SHAP method correct answer predictions nonrepresentative data instances As stated Section 1 rare features real datasets statistically independent The place Kernel SHAP method independence assumption px S xS px S approximating integral 8 Apart rough assumption Kernel SHAP framework stands clever fruitful way approximate Shapley values It desirable incorporate dependence Kernel SHAP method avoiding independence assumption This estimatingapproximating px S xS x S directly generate samples distribution instead generating independently xS described Section 232 We propose approaches estimating px S xS x S assuming Gaussian distri bution px ii assuming Gaussian copula distribution px iii approximating px S xS x S empirical conditional distribution iv combination empirical approach Gaussian Gaussian copula approach 31 Multivariate Gaussian distribution If assume feature vector x stems multivariate Gaussian distribution mean vector μ S multivariate Gaussian In particular writing covariance matrix cid2 conditional distribution px S xS x px pxS x S NM μ cid2 μ μS μ S cid8 cid9 cid8 cid2 cid2SS cid2S S cid2 SS cid2 S S S N Sμ SS cid2 SS gives px S xS x μ SS μ S cid2 SS cid2 1 SS x S μS cid2 SS cid2 S S cid2 SS cid2 1 SS cid2S S 10 11 Hence instead sampling marginal distribution x S sample Gaussian distribution ex pectation vector covariance matrix given 10 11 expectation vector μ covariance matrix cid2 estimated sample mean covariance matrix training data respectively Using samples S k 1 K conditional distribution integral 8 ﬁnally approximated 9 xk 32 Gaussian copula When features far multivariate Gaussian distributed instead represent marginals empirical distributions model dependence structure Gaussian copula The deﬁnition ddimensional copula multivariate distribution C uniformly distributed marginals U01 01 Sklars theorem 29 states multivariate distribution F marginals F 1 F 2 Fd written F x1 xd CF 1x1 F 2x2 Fdxd 12 appropriate ddimensional copula C In fact copula 12 expression Cu1 ud F F 1 1 u1 F 1 2 u2 F 1 d ud F copula beneﬁt use analytical expressions conditionals 10 11 1 j s inverse distribution functions marginals While copulas Gaussian Assuming Gaussian copula use following procedure generating samples px S xS x S 6 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Convert marginal X j feature distribution X Gaussian feature V j V j cid41 ˆF j X j ˆF j empirical distribution function marginal j Assume V distributed according multivariate Gaussian2 sample conditional distribution pv S vS v S method described Section 31 Convert margins V j conditional distribution original distribution ˆX j ˆF 1 j cid4V j With series samples generated described integral 8 ﬁnally approximated 9 33 Empirical conditional distribution If dependence structure marginal distributions x far Gaussian aforementioned methods expected work For situations propose nonparametric approach The classical method nonparametric density estimation kernel estimator 30 decades following introduction reﬁned developed directions example 3133 However kernel estimator suffers greatly curse dimensionality quickly inhibits use multivariate problems Moreover methods exist nonparametric estimation conditional densities especially xS x S dimensional Finally kernel estimation approaches gives nonparametric density estimate need able generate samples estimated distribution Hence developed empirical conditional approach sample approximately px S x S The method S informative conditional distribution motivated idea samples x S xS xS close x px S x S consists following steps 1 Compute distance instance x explained training instances xi 1 ntrain The distance x instance computed cid11 DS x xi x S xi 1 S x S T cid5 S S xi S 13 cid5S sample covariance matrix ntrain instances xS That compute distance use elements subset S Equation 13 viewed scaled version Mahalanobis distance 34 2 Compute weights training instances xi 1 ntrain distances similarly Gaussian distribution kernel cid12 wS x xi exp DS x xi2 2σ 2 cid13 σ viewed smoothing parameter bandwidth needs speciﬁed xi increasing order let x k training instance corresponding kth largest 3 Sort weights wS x weight 4 Approximate integral 8 weighted version 9 v condKerSHAPS cid5 x K k1 wS x cid5 K k f x k1 wS x xk k S x S 14 S sampled replacement training data weights wS xi Note 9 xk 1 ntrain Our approach sampling effective uses training observation uses weights integral computation input sampling The number samples K approximate prediction step 4 instance chosen total sum weights accounted K largest weights cid14 cid5 k L k1 wS x cid5 ntrain i1 wS x xi x K min LN cid15 η 15 η set instance 09 If K 15 exceeds certain limit instance 5000 set limit Essentially kernel based estimation procedures kernel density estimation require selection bandwidth parameters Our method exception The choice bandwidth parameter σ viewed bias variance tradeoff A small σ puts weight closest training observations gives low 2 The quality assumption depend close Gaussian copula true copula 7 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 bias high variance A large σ spreads weight higher number distant training observations gives high bias low variance Typically features highly dependent small σ needed bias dominate When features essentially independent bias σ larger σ preferable As σ method approximates original Kernel SHAP method Section 232 By viewing estimation E f xxS x S 1 ntrain turns empirical conditional distribution approach K ntrain equivalent Nadaraya Watson estimator 35 Hurvich et al 36 developed smallsamplesize corrected version Akaike information criterion AICc select bandwidth parameters nonparametric regression problems The connection Nadaraya Watson estimator allows apply AICc directly select σ S regression problem response f xi S covariates xi S x The strategy AICc ﬁnd suitable smoothing parameter choose σ minimizer AICc log ˆτ 2 cid4H ˆτ 2 1 ntrain ntraincid2 i1 f xi S x S 16 2 cid5 ntrain j1 wS x j xi f x cid5 ntrain j1 wS x j xi j S S x cid4H 1 trHntrain 1 trH 22 Here H ntrain ntrain matrix indexes H j cid5 wS x j xi ntrain l1 wS xl xi commonly called smoother hat matrix trH trace H Thus select σ compute AICc 16 σ values select σ corresponding smallest AICc value This subsets S new observation x explained meaning computa tionally intensive approach To reduce computational burden experimented different approximations ended σ assumed value subsets S size S In approach denoted approximate AICc method Section 4 sum AICc values subsets size minimized instead AICc values subset Even approximate AICc method time consuming Hence Section 4 experimented ﬁxed σ 01 subsets S 34 A combined approach When performing experiments described Section 4 turned empirical conditional distribution method works dimension xS D small number outperformed D multivariate Gaussian method Gaussian copula method condition features This accordance previous literature instance 37 references Very papers attempt estimate f zx x D 3 dimensions z onedimensional In higher dimensions previously proposed methods typically rely prior dimension reduction step result signiﬁcant loss information Hence wise combine empirical approach multivariate Gaussian Gaussian copula approach simulating conditional distributions dimxS D empirical method conditional distributions parametric method In combined approach partly avoid curse dimensionality empirical approach conditioning features conditional distributions analytically computed Gaussian approach To determine D use instance cross validation In experiments determined D based experience 4 Experiments A problem evaluating prediction explanation methods generally ground truth Hence verify approaches accurate original Kernel SHAP method described Section 232 dependent features turn simulated data compute true Shapley values As computing exact Shapley values single prediction requires solving O 2M integrals type 8 dimension 1 M 1 perform accurate experiments high dimensional settings We perform experiments low dimensional M 3 moderate dimensional M 10 setting multivariate distributions features x sampling models yx forms predictive model f The experiments 8 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 M 3 M 10 treated Sections 42 43 respectively Due lowmoderate dimension features experiments computationally tractable use exact version Kernel SHAP 6 Hence turn approximation 7 As AICc dependent approximation methods directly dependent sampled training set run experi ments 10 batches In batch sample new training set size ntrain 2 000 use model ﬁtted training data explain predictions test set size 100 This means quality conditional expectation approximations measured based total ntest 10 100 1 000 test observations Sampling new training data batch reduces inﬂuence exact form ﬁtted predictive model compared single training set simulations The Shapley value approximations compare original Kernel SHAP method original The Kernel SHAP Gaussian conditional distribution Gaussian The Kernel SHAP Gaussian copula empirical margins copula The Kernel SHAP empirical conditional distribution determining σ exact AICc empiricalAICcexact The Kernel SHAP empirical conditional distribution determining σ approximate AICc empiricalAICc approx The Kernel SHAP empirical conditional distribution setting σ 01 conditional distributions empirical 01 The Kernel SHAP combined approach empirical approach subsets dimension 3 Gaussian approach empirical01Gaussian empiricalAICcapproxGaussian The Kernel SHAP combined approach empirical approach subsets dimension 3 copula approach empirical01copula empiricalAICcapproxcopula Due computational complexity empiricalAICcexact method 3 dimensional experiments Fur thermore combined approaches 10 dimensional experiments For experiments XGBoost ﬁt predictive model include socalled TreeSHAP method 25 comparison 41 Evaluation measures To quantify accuracy different methods rely mean absolute error MAE Shapley value approximations averaged features test samples MAEmethod q 1 Mntest Mcid2 ntestcid2 j1 i1 φi jtrue φi jq φi jq denotes Shapley value feature j prediction computed approximation method q φi jtrue corresponding true value In order determine superiority proposed methods original Kernel SHAP method rely socalled skill score 38 associated aforementioned MAE The skill score method q takes form SkillMAE method q MAEmethod q MAEoriginal MAEoptimal MAEoriginal 1 MAEmethod q MAEoriginal MAEoptimal MAE optimal method equal zero The skill score measures superiority method compared reference method original Kernel SHAP It standardized way takes value 1 perfect approximation 0 reference method When approximation method worse reference method skill score negative To compare methods equal terms methods restricted use K 1 000 samples training set feature combination test observation 42 Dimension 3 For dimensional setting use different sampling models yx linear piecewise constant combine different multivariate distributions features x Gaussian Generalized Hyperbolic distribu tion Gaussian mixture total experimental setups AF In Sections 421423 multivariate feature distributions sampling models discussed Sections 424 425 Finally Section 426 contains results The noise term εi common experiments assumed follow distribution εi low dimension exact Shapley value 8 computed numerical integration d ε N0 012 Due 9 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 421 Gaussian distributed features The ﬁrst feature distribution px shall consider multivariate Gaussian distribution px N30 cid2ρ covariance matrix cid2ρ takes form cid2ρ 17 1 ρ ρ ρ 1 ρ ρ ρ 1 In experiments correlation coeﬃcient ρ varies 0 098 representing increasing positive correlation features 422 Skewed heavytailed distributed features The second feature distribution px considered Generalized HyperbolicGHdistribution Following 39 random vector X said follow GHdistribution index parameter λ concentration parameter ω location vector μ dispersion matrix cid2 skewness vector β denoted X GHλ ω μ cid2 β represented X μ W β W U W GIGλ ω ω U N0 cid2 W independent U GIG Generalized Inverse Gaussian distribution introduced 40 Appendix C contains details distribution We use following parameter values experiments λ 1 ω 05 cid2 cid20 diag1 β 14κ 1 μ 0 EW 14κ skewness coeﬃcient κ varies 1 10 different experiments resulting increasingly skewed heavytailed positively correlated distribution EW denotes mean GIG distribution equal approximately 456 parameter values The special form μ chosen GHdistribution mean zero 423 Multimodal distributed features The feature distribution mixture Gaussian distributions different means That px 2cid2 k1 πkNμkγ cid202 mixture probabilities π1 π2 05 mean functions μ1γ μ2γ γ 1 05 1cid8 The covariance matrix cid2 assumed form 17 The γ parameter represents distance Gaussian distributions range 05 10 different experiments 424 Linear model The ﬁrst sampling model yx simple linear model y gx x1 x2 x3 ε Data distribution modeled ﬁtting parameters β j j 0 3 linear model f x β0 β1 x1 β2 x2 β3 x3 ε ordinary linear regression 425 Piecewise constant model The second sampling model yx piecewise constant model constructed summing different piecewise stant functions y gx fun1x1 fun2x2 fun3x3 ε The functions fun1 fun2 fun3 displayed Fig 1 We ﬁt model gx XGBoost framework 26 default values hyperparameters histogram tree learning method 50 boosting iterations 10 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 1 Piecewise constant functions experiment D E F For interpretation colors ﬁgures reader referred web version article 426 Results The results 3Dexperiments visualized Figs 2 3 In experiment A linear sampling model Gaussian features As seen upper row Fig 2 original Kernel SHAP method works features independent outperformed methods ρ greater 005 The Gaussian model generally shows best performance It noted AICc method determining σ works better ﬁxed value 01 In experiment B linear sampling model skewed heavytailed features Like previous experiment original Kernel SHAP method outperformed approaches As shown middle row Fig 2 κ increases MAE values copula Gaussian methods reduced This increased variance features comes byproduct increasing κ parameter For experiment empirical approach ﬁxed σ 01 performs uniformly better based AICc In experiment C sampling model bimodal Gaussian mixture distributed features Again methods perform uniformly better original Kernel SHAP method The lower row Fig 2 shows empirical methods outperform Gaussian copula methods especially γ distance modes feature distribution large In experiments DF use feature distributions AC experiments use piecewise constant model instead linear The results largely linear model case The original Kernel SHAP method outperformed approaches Further Gaussian model best Gaussian features empirical approaches best feature distribution bimodal In case skewed heavytailed features Gaussian copula methods preferable larger values κ For experiments piecewise constant model TreeSHAP method addition ones As shown Fig 3 performance method slightly better original kernel SHAP method experiments D E worse experiment F This surprising TreeSHAP method supposed handle dependence better original kernel SHAP method Overall 3 dimensional experiments clearly important account dependence features computing Shapley values Which suggested methods best depends underlying feature distribution Further results empirical method approximate AICc version fairly similar cases better slower exact version recommend 43 Dimension 10 In 10 dimensional case restricted types experiments In ﬁrst use Gaussian distributed features form described 3 dimensional case Section 421 That px N100 cid2ρ cid2ρ 10 dimensional extension 17 features variance 1 pairwise correlation features ρ The ﬁrst experiment uses sampling model yx directly extending 3 dimensional linear model Section 424 y gx 9cid2 j1 x j ε 11 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 2 Dimension 3 MAE skill score linear model Upper row Gaussian features Middle row GHdistributed features Lower row Gaussian mixture distributed features 12 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 3 Dimension 3 MAE skill score piecewise constant model Upper row Gaussian features Middle row GHdistributed features Lower row Gaussian mixture distributed features 13 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 4 Dimension 10 MAE skill score linear model Gaussian distributed features use error term εi case y modeled ordinary linear regression d ε N0 012 3 dimensional experiments Analogous 3 dimensional f x 10cid2 j1 β j x j ε The second experiment extends 3 dimensional experiment described Section 425 taking form y gx cid2 fun1x j cid2 fun2x j cid2 fun3x j ε j 123 j 456 j 789 excluding effect 10th feature As 3 dimensional case model gx ﬁtted XGBoost framework settings Note settings feature 10 direct inﬂuence y In experiment simulate data 10 dimensional GHdistribution following parameter values λ 1 ω 05 μ 3 3 3 3 3 3 3 3 3 3 cid2 diag1 2 3 1 2 3 1 2 3 3 β 1 1 1 1 1 05 05 05 05 05 As sampling model use piecewise constant model described We didnt use empiricalAICcexact approach 10 dimensional experiments As previously stated approach computationally intensive Moreover 3 dimensional experiments showed performance empiricalAICcexact empiricalAICcapprox approaches similar For 3 dimensional experiment Generalized Hyperbolic features piecewise constant model MAE skill scores empirical approaches equal meaning necessary use signiﬁcantly computational heavy AICc approach Hence empirical approach experiment empirical01 method In combined approaches use empirical approach subsets dimension 3 bandwidth parameter σ deter mined approximate AICc method linear case ﬁxed 01 The results simulation experiments shown Figs 4 5 Table 1 While numerical integration compute exact Shapley values 3 dimensional experiments turn Monte Carlo integration 10 dimensional case That use 9 computing true Shapley value samples true conditional distribution instead samples estimated conditional distribution 14 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 5 Dimension 10 MAE skill score piecewise constant model Gaussian distributed features Table 1 Dimension 10 MAE skill score piecewise stant model GHdistributed features Approach original Gaussian copula empirical01 empirical01Gaussian empirical01Copula TreeSHAP MAE 1182 0377 0526 0307 0199 0236 1181 Skill score 0000 0633 0504 0737 0821 0791 0014 From ﬁgures results Gaussian distributed features dimension 10 3 dimensional counterparts Fig 2 Gaussian method generally showing best performance The combined empirical Gaussiancopula approaches work For piecewise constant model TreeSHAP method behaves 3 dimensional case slightly better original kernel SHAP method small medium sized dependence worse high dependence features For skewed heavytailed data Table 1 shows empirical01Gaussian method gives best perfor mance having slightly better MAE values skill scores empirical01copula method Finally like experiments suggested approaches outperform original Kernel SHAP TreeSHAP method 44 Real data example In example use real data set The data set consists 28 features extracted 6 transaction time series It previously predicting mortgage default relating probability default transaction information 3 The transaction information consists daily balances consumers credit kk checking br savings sk accounts addition daily number transactions checking account tr transferred checking account inn sum checking savings credit card accounts sum For time series length 365 days mean mean maximum max minimum min standard deviation std coeﬃcient variation cv computed resulting 28 features These scaled mean 0 standard deviation 1 computations Fig 6 shows histograms features remaining features similar distributions The feature distri butions skewed heavytailed The pairwise rank correlations measured Kendalls τ 41 shown Fig 7 We use Kendalls τ instead linear correlation coeﬃcient variables far linearly dependent Fig 9 examples Most rank correlations Fig 7 close 0 groups features high mutual corre lations Hence expect approach accurate approximations true Shapley values original Kernel SHAP method assumes independent features 15 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 6 Histograms variables real data set The data set divided training set test set containing 12696 1921 observations respectively We ﬁtted XGBoost model 50 trees training data default parameter settings The resulting AUC test data 088 The example Section 43 showed case skewed heavytailed features piecewise stant model combined approaches superior empirical01Gaussian approach best performing method We assume case real data set compare performance method original Kernel SHAP approach Fig 8 shows Shapleyvalues individuals data set The Shapley values different For indi vidual A large differences variables br_mean br_min br_max variables sum_min sum_mean sum_max From Kendalls τ matrix Fig 7 variables positively correlated variables Hence surprising Shapley values methods different For indi vidual B observe large differences variables sum_min sum_mean sum_max In addition worth noticing Shapley values variable kk_cv opposite signs This variable strongly negatively correlated variable kk_std In addition positively correlated kk_min kk_mean kk_max As previously stated problem evaluating Shapley values real data ground truth Hence justify results ways As shown 1 Shapley value weighted sum differences vS j vS subsets S Both method original Kernel SHAP method vS estimated 9 The methods differ original Kernel SHAP method assumes feature independence generating samples conditional distribution px S xS x S Hence able samples conditional distributions generated method representative samples generated original Kernel SHAP method likely Shapley values obtained method accurate obtained original Kernel SHAP method Since conditional distributions involved Shapley formula 28 variables impossible However included examples illustrate method gives correct approximations true conditional distributions original Kernel SHAP approach First Fig 9 shows plots 16 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 7 Kendalls τ correlation matrix real data set br_min br_max br_std br_max The black dots training data The turquoise dots samples conditional distribution variable xaxis given br_max equal zero generated method red dots corresponding samples generated original Kernel SHAP approach The original Kernel SHAP approach generates samples unrealistic sense far outside range observed training data It known evaluation predictive machine learning models far domain trained lead spurious predictions Thus important explanation methods evaluating predictive model appropriate feature combinations The samples generated method inside range observed training data Further Fig 10 study different conditional distributions involved Shapley formula The conditional distribution sum_min sum_mean sum_max given variables The conditional distribution variables inn_cv given inn_cv The conditional distribution variables kk_mean br_max given kk_mean br_max For distributions generate 1000 samples individuals test data set method original Kernel SHAP approach That condition different sets values For combination individual conditional distribution method compute mean Mahalanobis distance sample nearest training samples resulting 1000 different mean distances Each panel Fig 10 shows probability density mean distances scaling mean distances average original method Hence distances sized relative mean distance original method If generated samples realistic expect majority mean distances small Starting leftmost column mode density corresponding method smaller corresponding original Kernel SHAP approach This indicates samples generated approach realistic generated original Kernel SHAP approach From Kendalls τ matrix Fig 7 variables sum_min sum_mean sum_max positively correlated variables Hence makes sense method provides better estimate conditional distribution 17 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 8 Shapley values persons real data set computed method original Kernel SHAP method If proceed second column densities approaches similar This surprising shown Fig 7 variable inn_cv uncorrelated variables Finally rightmost column mode density corresponding method smaller corresponding original Kernel SHAP approach differences large ﬁrst column We believe explained fact variables condition kk_mean strongly correlated variables To summarize illustrated Shapley values computed method original method different We tried justify fact method gives correct approximations true conditional distributions original Kernel SHAP approach 5 Summary discussion Shapley values modelagnostic method explaining individual predictions solid theoretical foundation The main disadvantage method computational complexity grows exponentially number features This led approximations Kernel SHAP method known A key ingredient Kernel SHAP method conditional distribution subset S features conditional features S subset In original version Kernel SHAP method implicitly assumed features subsets independent meaning conditional distribution replaced marginal distribution 18 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 Fig 9 Plots br_min br_max left br_std br_max right The black dots training data The turquoise dots samples conditional distribution variable xaxis given br_max equal zero generated method red dots corresponding samples generated original Kernel SHAP approach Fig 10 Probability densities mean Mahalanobis distances different conditional distributions different individuals See text description 19 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 features S If high degree dependence features independence assumption lead unrealistic combinations feature values Since evaluation predictive machine learning models far domain trained lead spurious predictions resulting Shapley values correct This paper introduces modiﬁed version Kernel SHAP method handles dependent features We proposed different approaches estimating conditional distribution assuming Gaussian multivariate distribution features assuming Gaussian copula empirical margins empirical approach combination empirical approach Gaussian Gaussian copula approach We performed comprehensive simulation study linear nonlinear models Gaussian nonGaussian distributions dimensions 3 10 methods accurate approximations true Shapley values original Kernel SHAP approach For nonlinear models methods clearly outperformed TreeSHAP method best knowledge Shapley approach tries handle dependence features prediction explanation setting When performing experiments turned nonparametric approach superior conditioning small number features outperformed Gaussian copula methods condition variables Hence regard combined approach promising proposed methods This approach applied real case 28 variables predictions explained produced XGBoost classiﬁer designed predict mortgage default In case true Shapley values known However provide results indicate combined approach provides sensible approximations original Kernel SHAP methods While having desirable properties method obvious drawback computational time The timeconsuming AICc computation bandwidth parameter nonparametric approach Using ﬁxed bandwidth parameter instead computational time combined approach original Kernel SHAP method Recently attempts underlying graph structure input data reduce computational complexity 4243 If conditional independence requirements satisﬁed graph ﬁrst divided separate communities Kernel SHAP method applied community individually The methods proposed 4243 tested predictions obtained text image classiﬁcation In settings data graph structure enabling factorization separate communities The main challenge method tabular data potentially huge number tests conditional dependence performed However deﬁnitely worth closer look In paper assume aim explain actual predictions model For models predic tion probability If Shapley framework decompose probabilities summing subset φi values theory produce values range 01 Hence cases natural assume importance features additive log odds space space probabilities There problems solution straightforward human interpret log odds contribution probability Hence decompose practical mathematical question Tabular data contain ordered nonordered categorical data The proposed nonparametric approach categorical variables converted numerical ones The simplest solution use onehotencoding However large data sets handle categorical features hundreds categories meaning method needs handle large number binary attributes An alternative approach use ideas clustering literature 44 45 deﬁning distance functions handle categorical mixedtype features There generalizations Mahalanobis distance treating data mixture nominal ordinal continuous variables instead approach described instance 46 When comes parametric approaches categorical data represents greater challenge The promising alternative use entity embedding 47 convert categorical features numerical ones treat features similarly numerical features Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Acknowledgements The authors grateful Scott Lundberg valuable advice concerning Kernel SHAP method We want thank Ingrid Hobæk Haff suggesting copula approach Nikolai Sellereite help implementing methods Rpackage shapr This work supported Norwegian Research Council grant 237718 Big Insight Appendix A Shapley properties prediction explanation setting When Shapley values prediction explanation φ0 v E f x actually plays important role quantifying prediction features merely global average prediction If φ0 large absolute value compared φ j j cid13 0 features said unimportant speciﬁc prediction Moreover 20 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 prediction explanation setting interpretations properties Shapley value discussed Section 21 follows M Eﬃciency The sum Shapley values different features equal difference prediction E f x This property ensures prediction global average prediction value explained global mean prediction fully devoted explained features φ j j 1 M compared different predictions f x j1 φ j f x cid5 Symmetry The Shapley values features equal combined subsets features contribute equally prediction Failure fulﬁll criterion contribution different features explanation inconsistent untrustworthy explanations Dummy player A feature change prediction matter features combined Shapley value 0 Assigning nonzero explanation value feature inﬂuence prediction odd natural requirement Linearity When prediction function consists sum prediction functions Shapley value feature identical sum features Shapley values individual prediction functions This holds linear combinations prediction functions This property ensures models form Random Forests structurally simple ensemble models explained interpreted individually Failure fulﬁll basic advantageous properties gives odd undesirable inconsistent explanation frame work There additive explanation method Shapley values satisﬁes criteria 11 Appendix B Shapley values model linear In section ﬁrst proof explicit formula Shapley values predictive model linear regression model features independent Then obtain contribution function vS model linear features dependent B1 Linear model independent features When vS E f xxS x S predictive model linear regression model features independent Shapley values simple form φ j β j x j Ex j j 1 M Proof First derive expression vS case vS E f xxS x cid10 S f x S x S px S xS x S dx S cid10 cid2 βi xi cid2 iS S cid10 cid2 S cid2 S βi xi pxi dxi βi Exi iS βi x cid2 iS βi x px S xS x S dx S cid10 cid2 βi x px S dx S B1 B2 B3 The transition step B1 B2 follows assumption linear model transition B2 B3 follows independent features assumption Having computed vS expression vS j simply vS j vS β j x j β j Ex j meaning vS j vS β j cid6 x j cid7 Ex j From case difference vS j vS independent S Hence Shapley formula written 21 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 φ j β j cid6 x j cid7 cid2 Ex j SM j SM S 1 M Since sum Shapley weights 1 φ j β j cid6 x j cid7 Ex j j 1 M cid2 B2 Linear model dependent features If model linear features independent vS instead derived follows vS E f xxS x cid10 S cid2 S cid2 S f x S x S px S xS x S dx S cid10 cid2 βi xi cid2 iS S cid10 βi S dxi xi pxixS x S βi ExixS x cid2 βi x f x S Ex S xS x iS S xS x S βi x px S xS x S dx S cid10 cid2 iS βi x px S dx S B4 B5 B6 B7 In case avoid timeconsuming simulations able analytically obtain proper estimate Ex S xS x S This straightforward general Appendix C Generalized hyperbolic distribution The density ddimensional Generalized Hyperbolic random vector X cid8 px ω δx μ cid2 ω β T cid21β ω δx μ cid2ω β T cid21β cid27 2π d2cid212 Kλω exp x μT cid21β cid24cid25 cid9 λd2 2 Kλd2 cid26 cid28 δx μ cid2 x μT cid21x μ squared Mahalanobis distance x μ Kλ modiﬁed Bessel function kind index λ The mean vector covariance matrix X EX μ EW β VarX EW cid2 VarW ββ T It shown 48 X partitioned X T 2 T X 1 d1dimensional X 2 d2dimensional conditional distribution X 2 given X 1 x1 Generalized Hyperbolic distributed X 2X 1 x1 GH λ21 χ21 ψ21 μ21 cid221 β 21 1 X T λ21 λ d12 χ21 ω x1 μ1T cid2 ψ21 ω β T 11β 1 1 cid5T 1 11 x1 μ1 cid2T μ2 μ21 cid221 cid222 cid2T cid2T β 2 1 11 x1 μ1 12cid2 1 11 cid212 12cid2 1 12cid2 11 β 1 β 21 Here GH slightly different parameterization GH distribution conditional distribution For technical reasons use parameterization proposed 49 cid24cid25 cid26 cid8 px χ δx μ cid2 ψ β T cid21β cid9 λd2 2 ψχ λ2 Kλd2 2π d2cid212 Kλ χ δx μ cid2ψ β T cid21β cid28 cid27 χ ψ exp x μT cid21β 22 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 References 1 Z Obermeyer EJ Emanuel Predicting future big data machine learning clinical medicine N Engl J Med 375 2016 1216 2 A Sudjianto S Nair M Yuan A Zhang D Kern F CelaDíaz Statistical methods ﬁghting ﬁnancial crimes Technometrics 52 2010 519 3 H Kvamme N Sellereite K Aas S Sjursen Predicting mortgage default convolutional neural networks Expert Syst Appl 102 2018 207217 4 MT Ribeiro S Singh C Guestrin Why I trust Explaining predictions classiﬁer Proceedings 22nd ACM SIGKDD Interna tional Conference Knowledge Discovery Data Mining KDD ACM 2016 pp 11351144 5 European Union Regulation EU 2016679 European Parliament council 27 April 2016 protection natural persons regard processing personal data free movement data repealing directive 9546EC general data protection regulation Off J Eur Union 59 2016 6 A Fisher C Rudin F Dominici All models wrong useful learning variables importance studying entire class prediction models simultaneously J Mach Learn Res 20 2019 181 7 C Molnar Interpretable machine learning guide making black box models explainable httpschristophm github io interpretable ml book 2020 11 2010 18031831 8 D Baehrens T Schroeter S Harmeling M Kawanabe K Hansen KR Müller How explain individual classiﬁcation decisions J Mach Learn Res 9 E Štrumbel I Kononenko An eﬃcient explanation individual classiﬁcations game theory J Mach Learn Res 11 2010 118 10 E Štrumbel I Kononenko Explaining prediction models individual predictions feature contributions Knowl Inf Syst 41 2014 647665 11 SM Lundberg SI Lee A uniﬁed approach interpreting model predictions Advances Neural Information Processing Systems Curram Asso ciates Inc 2017 pp 47684777 12 LS Shapley A value Nperson games Contributions Theory Games vol 2 1953 pp 307317 13 AB Owen Sobol indices Shapley value SIAMASA J Uncertain Quantiﬁcat 2 2014 245251 14 E Song BL Nelson J Staum Shapley effects global sensitivity analysis theory computation SIAMASA J Uncertain Quantiﬁcat 4 2016 10601083 15 AB Owen C Prieur On Shapley value measuring importance dependent inputs SIAMASA J Uncertain Quantiﬁcat 5 2017 9861002 16 P Giudici E Raﬃnetti ShapleyLorenz explainable artiﬁcial intelligence Expert Syst Appl 2020 114104 17 I Covert SM Lundberg SI Lee Understanding global feature contributions additive importance measures Adv Neural Inf Process Syst 33 18 N Sellereite M Jullum shapr rpackage explaining machine learning models dependenceaware Shapley values J Open Sour Softw 5 19 K Aas M Jullum AL land Explaining individual predictions features dependent accurate approximations Shapley values arXiv 2020 2020 2027 1903 10464v1 2019 20 D Janzing L Minorics P Blöbaum Feature relevance quantiﬁcation explainable AI causal problem International Conference Artiﬁcial Intelligence Statistics PMLR 2020 pp 29072916 21 J Pearl Causality Cambridge University Press 2009 22 C Frye C Rowat I Feige Asymmetric Shapley values incorporating causal knowledge modelagnostic explainability Adv Neural Inf Process Syst 23 T Heskes E Sijben IG Bucur T Claassen Causal Shapley values exploiting causal knowledge explain individual predictions complex models 24 H Chen JD Janizek S Lundberg SI Lee True model true data 2020 ICML Workshop Human Interpretability Machine 25 SM Lundberg SI Lee Consistent feature attribution tree ensembles Proceedings 34th International Conference Machine Learning 33 2020 Adv Neural Inf Process Syst 33 2020 Learning WHI 2020 2020 JMLR WCP 2017 pp 1521 26 T Chen C Guestrin XGBoost scalable tree boosting Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD ACM 2016 pp 785794 27 HP Young Monotonic solutions cooperative games Int J Game Theory 14 1985 6572 28 A Charnes B Golany M Keane J Rousseau Extremal principle solutions games characteristic function form core Chebychev Shapley Value Generalizations Springer Netherlands Dordrecht 1988 pp 123133 29 A Sklar Fonctions répartition à n dimensions et leurs marges Publ Inst Stat Univ Paris 8 1959 229231 30 M Rosenblatt Remarks nonparametric estimates density function Ann Math Stat 27 1956 832837 31 MP Holmes AG Gray CL Isbell Fast kernel conditional density estimation dualtree Monte Carlo approach Comput Stat Data Anal 54 2010 17071718 32 K Bertin C Lacour V Rivoirard Adaptive pointwise estimation conditional density function Ann Inst Henri Poincaré Probab Stat 52 2016 939980 33 R Izbicki AB Lee Converting highdimensional regression highdimensional conditional density estimation Electron J Stat 11 2017 28002831 34 PC Mahalanobis On generalised distance statistics Proceedings National Institute Sciences India 1936 pp 4955 35 HJ Bierens The NadarayaWatson kernel regression function estimator Topics Advanced Econometrics Cambridge University Press 1994 36 CM Hurvich JS Simonoff CL Tsai Smoothing parameter selection nonparametric regression improved Akaike information criterion J R pp 212247 Stat Soc Ser B Stat Methodol 60 1998 271293 37 R Izbicki AB Lee Converting highdimensional regression highdimensional conditional density estimation Electron J Stat 11 2017 28002831 38 T Gneiting AE Raftery Strictly proper scoring rules prediction estimation J Am Stat Assoc 102 2007 359378 39 RP Browne PD McNicholas A mixture generalized hyperbolic distributions Can J Stat 43 2015 176198 40 IJ Good The population frequencies species estimation population parameters Biometrika 40 1953 237264 41 MG Kendall A new measure rank correlation Biometrika 30 1938 8193 42 J Chen L Song MJ Wainwright MI Jordan LShapley CShapley eﬃcient model interpretation structured data CoRR arXiv1808 02610 abs 2018 43 X Li NC Dvornek Y Zhou J Zhuang P Ventola JS Duncan Eﬃcient interpretation deep learning models graph structure cooperative game theory application ASD biomarker discovery International Conference Information Processing Medical Imaging Springer 2019 pp 718730 44 Z Huang Clustering large data sets mixed numeric categorical variables Proceedings First Paciﬁc Asia Knowledge Discovery Data Mining Conference 1997 pp 2134 45 Z Huang Extensions kmeans algorithm clustering large data sets categorical variables Data Min Knowl Discov 2 1998 283304 46 AR Leon KC Carriere A generalized Mahalanobis distance mixed data J Multivar Anal 92 2005 174185 23 K Aas M Jullum A Løland Artiﬁcial Intelligence 298 2021 103502 47 C Guo F Berkhahn Entity embeddings categorical variables arXiv1604 06737v1 2016 48 Y Wei Y Tang PD McNicholas Mixtures generalized hyperbolic distributions mixtures skewt distributions modelbased clustering incomplete data Comput Stat Data Anal 130 2019 1841 49 AJ McNeil R Frey P Embrechts Quantitative Risk Management Concepts Techniques Tools Princeton University Press 2006 24