Artiﬁcial Intelligence 171 2007 434439 wwwelseviercomlocateartint No regrets noregret YuHan Chang Intelligent Systems Division USC Information Sciences Institute 4676 Admiralty Way Marina del Rey CA 90292 USA Received 16 May 2006 received revised form 26 October 2006 accepted 13 December 2006 Available online 13 February 2007 Abstract Noregret described framework game theorists scientists converged designing evaluating multiagent learning algorithms However Shoham Powers Grenager point framework deﬁciencies behaving suboptimally certain reactive opponents But lost With simple modiﬁcations regretminimizing algorithms perform ways wish multiagent learning algorithms perform providing safety adaptability reactive opponents We argue research community regrets noregret methods 2007 Elsevier BV All rights reserved Keywords Multiagent learning Regretminimization Game theory 1 Introduction Traditional noregret algorithms perform suboptimally regret criterion compares algorithms performance possible alternate outcomes individual stage games repeated game assuming opponents action stays ﬁxed For example typical noregret agent playing Prisoners Dilemma end defecting minimizes algorithms regret relative possible action cooperating given observed sequence opponent actions However relatively simple extend standard noregret approaches handle avoid suboptimal cases In fact I argue modiﬁed noregret algorithms hold promise future direction multiagent learning Regret minimization methods referred experts algorithms hedging algorithms provide clearest method evaluating agent performance general multiagent settings Since like assume opponent multiagent learning problems unknown usually difﬁcult evaluate agent performance depends type opponent agent ends playing Regretminimization approaches circumvent problem deﬁning performance terms comparison class possible strategies agent capable executing Thus assumptions need opponents strategy Furthermore Shoham et al stated regretminimizing algorithms guarantee safety addition universal consistency These beneﬁts extended case agent faces reactive opponents Instead considering single actions time step modiﬁed regret framework considers multiperiod strategies dividing Email address ychangISIEDU 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200612007 YH Chang Artiﬁcial Intelligence 171 2007 434439 435 sequence games intervals The regret criterion account reactive strategies Titfor Tat Two issues arise setup 1 actions potential reward longer observed action actually played reward depends opponents current reactive strategy observed 2 computational complexity grows exponentially consider longer intervals larger set complex strategies The ﬁrst issue resolved noregret algorithm Auer CesaBianchi Freund Schapires EXP3 algorithm 1 extends Freund Schapires multiplicative weight algorithm 6 case partial information We propose alleviate second issue choosing set possible strategies carefully incorporating learning algorithms experts modiﬁed version EXP3 algorithm 4 2 Mathematical background For article focus attention repeated games techniques described po tentially extended stochastic games The repeated game setting captures complexity multiagent learning problem need focus ability learn react opponent In stochastic game need learn adapt external environment Here focus modeling states opponent set aside problem additionally modeling states external environment time During stage game repeated game player simultaneously chooses play particular action ai Ai receives reward based joint action taken We use terms policy strategy interchangeably While Nash equilibrium accepted solution concept single stage game repeated game stochastic game settings modern game theory takes general view optimality view gained acceptance machine learning community 35 The key difference treatment history actions taken game Here deﬁne behavioral strategy β H Ai H t H t H t set possible histories length t Histories observations joint actions ht ai ai ht1 For simplicity assume A A1 A2 cid2 Deﬁnition 1 A τ length behavioral strategy βτ H τ A mapping set possible histories H τ actions A Let Bτ set possible τ length behavioral strategies βτ We note Bτ AA2τ In case H t H consider learning algorithms possible behavioral strategy playing repeated game This deﬁnition strategy space clearly powerful allows deﬁne larger set potential equilibria However opponent rational longer advantageous ﬁnd play equilibrium strategy In fact given arbitrary opponent Nash equilibrium strategy return lower payoff action Indeed payoff worse original Nash equilibrium value Thus turn regret minimization algorithms 21 Regretminimization In repeated games standard regret minimization framework enables perform best action single best action played time period We frequently refer EXP3 algorithm variants explored Auer et al 1 example type algorithm In original formulation EXP3 choose single actions play observe rewards received chosen different actions We need bit precise deﬁnition regret Auer et al consider adversarial setting reward function actually sequence reward functions change time period We denote cumulative reward executing algorithm H T time periods RH compared cumulative reward agent received chosen execute ﬁxed action T time periods Rmax maxa Ra Since algorithm H randomized discussing expected regret Rmax ERH The authors T K ln K K number action performance EXP3 exhibits expected regret bound 2 choices T number rounds play game In situations rewards possible actions observed period upper bound expected regret reduced O T ln K e 1 436 YH Chang Artiﬁcial Intelligence 171 2007 434439 Generally speaking regretminimizing algorithms hedge possible actions keeping weight action updated according actions historical performance The probability playing action fraction total weights mixed uniform distribution Intuitively better experts perform better assigned higher weight played Sometimes algorithms called experts algorithms think actions recommended set experts This set referred comparison class This comparison class provides clear means evaluating algorithms performance pegging evaluation metric set strategies assumptions know execute It important note existing methods compare performance strategies best responses called oblivious myopic opponents That opponent learn react actions instead playing preselected possibly arbitrary ﬁxed string actions Under circumstances expect intelligent opponent change strategy observe sequence plays For example consider game repeated Prisoners Dilemma If follow oblivious opponent assumption best choice action hindsight defect assuming oblivious opponent change action response defection This approach assign low scores high regret cooperative strategies miss chance earn higher rewards cooperating opponents TitforTat opponent cooperates long cooperate These opponents called reactive opponents Our extension regret framework deals reactive opponents expanding comparison class strate gies include reactive behavioral strategies Now instead comparing best action stage game assuming opponents action stays ﬁxed compare performance strategies TitforTat assumption opponent chooses actions response strategy To precise divide sequence games phases length λ phase play havioral strategy λ stage games Thus instead choosing actions A stage game instead choosing behavioral strategy Bλ play phase Now choose strategy TitforTat able observe produces higher cumulative rewards λ stage games Always Defect strategy length phase In related work Mannor Shimkin 8 propose similar supergame framework extending regret minimization framework handle stochastic games In supergame framework phases allow agent observe external environments reaction strategy Our basic extensions EXP3 follow lines deal speciﬁcally modeling reactive opponent Markov external environment We propose extensions deal overwhelming computational complexity setup Mannor Shimkin focus alternate method based empirical Bayes envelope provide efﬁcient algorithm slightly weaker guarantees supergame setup Farias Meggido 5 explore problem optimizing behavior reactive opponent use different performance metric regret 3 Extending experts framework We basic extensions experts framework allow capture added power behavioral strategies Instead choosing actions A choose behavioral strategies Bτ Bτ replaces A comparison class essentially forcing compare performance complex possibly better performing strategies While executing βτ Bτ phase consisting λ stage games agent receives reward time step observe rewards received played possible strategies This reasonable opponent adapt differently particular strategy played causing different cumulative outcome λ time periods Thus opponent arbitrary blackbox opponent ﬁxed ﬁnite automaton For example consider opponent action choices depend previous τ length history joint actions Thus construct Markov model M opponent set possible τ length histories state space If optimal policy β MDP ergodic use cid5return mixing time νβM 7 induced Markov chain choice λ good idea average rewards possible policy long run We usually assume given ν maxβBτ νβM simply choose sufﬁciently large ﬁxed λ YH Chang Artiﬁcial Intelligence 171 2007 434439 437 Thus executing policy β learned particular opponent model M run policy νβM time periods properly estimate beneﬁt policy Setting experts commitment time horizon λ ν ensures expert receives good estimates policys value λlength phase denote total reward phase rβ t t λ 1 Over T time periods cumulative reward Rβ rβ 1λ 1 iλ cid3cid5T λcid6 i1 Alternatively given ﬁxed phase length λ like able evaluate possible strategies order choose optimal strategy This entail enumerating possible behavioral strategies λ periods A hedging algorithm H switches set Bλ strategies picks βi phase cumulative reward denoted RH cid5T λcid6cid4 i1 cid5 1λ 1 iλ cid6 rβi The expected regret following algorithm H maxβBλ Rβ ERH However AA2λ possible strategies evaluate Not long time try possible strategy regret bounds exceedingly weak The expected regret T time periods cid7 2 e 1AλA2λ2 T λ ln A Clearly amounts computationally infeasible approach problem In traditional MDP solution tech niques saved Markov property state space reduces number strategies need evaluate allowing reuse information learned state Without assumptions opponents behavior classic regret minimization framework beneﬁts 4 Learning algorithms experts However imagine policies useful fruitful ones explore given choice ﬁxed commitment length λ In fact cases probably rough idea types opponent models appropriate given domain For example Prisoners Dilemma example expect opponent TitforTat player AlwaysDefect AlwaysCooperate player Given particular assumptions possible opponent models able use learning algorithm estimate model parameters based observed history These learning algorithms viewed experts rec ommend particular behavioral strategy β played game phase The behavioral strategies discussed simply experts recommend strategy βi played phase The traditional static experts recommend action played stage game game phase Deﬁnition 2 Given set experts E regret obtain following algorithm H deﬁned maxeE Re ERH expert e outputs recommended behavioral strategy β Bλ phase repeated game For example use learning expert believe opponent Markov τ length history joint actions discussed previous section We construct Markov model opponent use efﬁcient learning algorithm E3 Kearns Singh 7 learn cid5optimal policy time polynomial number states A2τ We able efﬁciently learn best response strategy opponents shown Fig 1 case long choose τ cid2 4 In contrast hedging algorithm needs evaluate exponentially large number possible policies AA2τ possible policies Fig 1 A possible opponent model ﬁve states Each state corresponds number consecutive Cooperate C actions played D stands Defect 438 YH Chang Artiﬁcial Intelligence 171 2007 434439 Of course small number learning experts entire set behavioral strategies longer guarantee regret minimization possible policies As discuss following section choose subset ﬁxed policies compare performance learning algorithms decide use guarantee noregret relative subset ﬁxed policies relative recommended strings actions produced learning algorithms In ways learning algorithms experts simply offloads exploration experts framework individual learning algorithm The computational savings occur learning algorithm makes particular assumptions structure world opponent enabling expert learn efﬁciently hedging possible strategies 5 The hedged learner Since chosen learning algorithms fail output good policies propose incorporate experts inside hedging algorithm hedges set experts includes learners This allows hedging algorithm switch experts particular learning algorithm fails It fail incorrect opponent assumptions previous sections example chose τ 4 learning algorithm simply illsuited particular domain Here outline method adding learning experts regretminimization algorithm Auer et als EXP3 alongside static experts It straightforward extend results variants EXP3 EXP3P1 guarantees similar bounds hold uniformly time probability We given K static experts plays single pure action stage game We need add L reactive experts learning algorithms behavioral strategies For evaluation K static experts νk 1 k K assume opponents action string independent agents action choices For l L assume given νl 1 based policies experts produce assumptions opponent model Since true mixing time νl commitment length required expert l When clear context write K L number experts sets K L respectively Hierarchical hedging Let H0 denote toplevel hedging algorithm Construct secondlevel hedging algorithm H1 composed original K static strategies Use H1 learning algorithms L 1 experts H0 hedges In contrast naive approach adding learning experts run K L experts time longest commitment phase required static experts learning algorithms λmax maxiKL νi This naive approach suffers main drawbacks stemming issue Because naive approach follows experts λmax periods follows static experts longer necessary Intuitively slows algorithms adaptation rate Furthermore lose safety beneﬁt comes hedging pure actions Whereas hedging algorithm set pure action static experts λ 1 able guarantee attain safety minimax value stage game longer true naive approach λ 1 included possible λmaxlength behavioral experts This removes uncertainty action opponent strategies able exploit determinism For example consider opponent runs behavioral strategy simply plays best response periods observed action Each K static experts incur high loss run λmax periods Hierarchical Hedging addresses issues Proposition 3 Suppose set K static experts set L reactive experts commitment lengths λi maxi λi K We devise algorithm regret bound cid7 cid5 2 e 1 T K ln K λmaxT L 1 lnL 1 cid6 Proposition 4 The Hierarchical Hedging algorithm HH attain asymptotic average reward close safety value minimax value v stage game For cid5 0 lim infT RHHT v cid2 cid5 surely YH Chang Artiﬁcial Intelligence 171 2007 434439 439 Hierarchical hedging provides method devising computationally tractable regretminimizing algo rithms able perform large set reactive opponents achieve noregret arbitrary oppo nents guarantee safety 6 Issues overcome conclusion As satisfying noregret framework isnt panacea complex problems need deal multiagent learning problems As opponent strategies grow complex recognition process possible strategic responses grow complex Computational time grows accordingly We need pursue develop techniques reduce computational load Work direction includes constructing efﬁcient learners depend VC dimension concept class number possible experts class 9 We try initially use small opponent models enlarge models statesplitting techniques necessary 2 However given challenges beneﬁt regretminimization approach evaluates performance relative comparison class strategies know execute Thus given ﬁxed compu tational constraints use noregret approaches achieve agent performance comparable better best strategy set strategies know compute constraints This capability holds matter type opponent agent ends facing game Weve seen regretminimization approaches provide number beneﬁts applied multi agent learning problem Speciﬁcally techniques likely useful ﬁfth area research Shoham et al prescriptive noncooperative problems These techniques allow create algorithms guarantee safety exhibit noregret behavior arbitrary opponents Moreover extensions weve described article allow inclusion behavioral strategies learning algorithms comparison class ensuring agent performs static opponents reactive opponents Noregret algorithms said perform comparison class measured By opening door larger comparison classes develop regretminimizing algorithms exhibit regret truly perform wide variety settings References 1 P Auer N CesaBianchi Y Freund RE Schapire Gambling rigged casino adversarial multiarmed bandit problem Proceedings 36th Symposium Foundations Computer Science 1995 2 Y Chang PR Cohen CT Morrison W Kerr RS Amant The Jean Proceedings International Conference Development Learning 2006 3 Y Chang LP Kaelbling Playing believing The role beliefs multiagent learning Advances Neural Information Processing Systems 2001 4 Y Chang LP Kaelbling Hedged learning Regretminimization learning experts International Conference Machine Learning 2005 5 DP Farias N Meggido How combine expert novice advice actions impact environment Advances Neural Information Processing Systems 2004 6 Y Freund RE Schapire Adaptive game playing multiplicative weights Games Economic Behavior 29 1999 79103 7 M Kearns S Singh Nearoptimal reinforcement learning polynomial time Proceedings International Conference Machine Learning 1998 8 S Mannor N Shimkin The empirical Bayes envelope approach regret minimization Technion Technical Report EENo1261 2000 9 A Strehl C Mesterharm ML Littman H Hirsh Experienceefﬁcient learning associative bandit problems Proceedings Interna tional Conference Machine Learning 2006