Artiﬁcial Intelligence 260 2018 135 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Learning machine Random backpropagation deep learning channel Pierre Baldi Peter Sadowski Zhiqin Lu b Department Computer Science University California Irvine United States b Department Mathematics University California Irvine United States r t c l e n f o b s t r c t Article history Received 7 December 2016 Received revised form 21 December 2017 Accepted 15 March 2018 Available online 3 April 2018 Keywords Deep learning Neural networks Backpropagation Local learning Random backpropagation RBP variant backpropagation algorithm training neural networks transpose forward matrices replaced ﬁxed random matrices calculation weight updates It remarkable effectiveness spite random matrices communicate error information completely removes taxing requirement maintaining symmetric weights physical neural To better understand random backpropagation ﬁrst connect notions local learning learning channels Through connection derive alternatives RBP including skipped RBP SRBP adaptive RBP ARBP sparse RBP combinations ASRBP analyze computational complexity We study behavior simulations MNIST CIFAR10 benchmark datasets These simulations variants work robustly backpropagation multiplication derivatives activation functions important As followup study lowend number bits required communicate error information learning channel We provide partial intuitive explanations remarkable properties RBP variations Finally prove mathematical results RBP variants including 1 convergence optimal ﬁxed points linear chains arbitrary length 2 convergence ﬁxed points linear autoencoders decorrelated data 3 longterm existence solutions linear systems single hidden layer convergence special cases 4 convergence ﬁxed points nonlinear chains derivative activation functions included 2018 Elsevier BV All rights reserved 1 Introduction Over years question biological plausibility backpropagation algorithm implements stochastic gradient descent neural networks raised times The question gained relevance numerous successes achieved backpropagation variety problems ranging vision 21313014 speech recognition 12 engineering high energy physics 726 biology 8321 natural sciences recent results optimality backpropagation 6 There known issues facing bio logical neural networks relation backpropagation include 1 continuous realvalued nature gradient Corresponding author Email address pfbaldi uci edu P Baldi httpsdoiorg101016jartint201803003 00043702 2018 Elsevier BV All rights reserved 2 P Baldi et al Artiﬁcial Intelligence 260 2018 135 information ability change sign violating Dales Law 2 need kind teachers signal provide tar gets 3 need implementing linear operations involved backpropagation 4 need multiplying backpropagated signal derivatives forward activations time layer traversed 5 need precise alternation forward backward passes 6 complicated geometry biological neurons problem transmitting error signals precision individual synapses However formidable obstacle standard backpropagation algorithm requires propagating error signals backwards synaptic weights identical corresponding forward weights Furthermore related problem suﬃciently recognized weight symmetry maintained times learning early neural develop ment It hard imagine mechanisms biological neurons create maintain perfect symmetry However recent simulations 24 surprisingly indicate symmetry required fact backpropagation works random weights backpropagate errors Our general goal investigate backpropagation random weights better understand works The foundation better understanding random backpropagation RBP provided concepts local learning deep learning channels introduced 6 Thus begin introducing notations connecting RBP concepts In turn leads derivation alternatives RBP study simulations known benchmark datasets proceeding formal analyses 2 Setting notations learning channel Throughout paper consider layered feedforward neural networks supervised learning tasks We denote architecture AN0 Nh N L 1 N0 size input layer Nh size hidden layer h N L size output layer We assume j denote weight connecting neuron j layer h 1 neuron layer h layers fully connected let wh The output O h neuron layer h computed Sh j O h1 wh cid2 Sh j O h f h 2 j The transfer functions f h usually neurons typical exceptions output layer usually monotonic increasing functions The typical functions artiﬁcial neural networks identity logistic hyperbolic tangent rectiﬁed linear softmax We assume training set M examples consisting input outputtarget pairs It T t t 1 M Iit refers ith component tth input training example similarly target T In addition error function E minimized learning process In general assume standard error functions squared error case regression identity transfer functions output layer relative entropy case classiﬁcation logistic single class softmax multiclass units output layer essential point While focus supervised learning worth noting unsupervised learning algorithms neural net works autoencoders neural autoregressive distribution estimators generative adversarial networks come output targets fall framework 21 Standard backpropagation BP Standard backpropagation implements gradient descent E applied stochastic fashion online mini batches batch form summing averaging training examples For single example omitting t index simplicity standard backpropagation learning rule easily obtained applying chain rule given cid2wh j η E wh j ηBh O h1 j 3 η learning rate O h1 easy backpropagated error satisﬁes recurrence relation presynaptic activity Bh j backpropagated error Using chain rule Bh E Sh cid3 f h cid2 k k wh1 Bh1 ki boundary condition B L Ei S L T O L 4 5 P Baldi et al Artiﬁcial Intelligence 260 2018 135 3 Thus short errors propagated backwards essentially linear fashion transpose forward ma trices symmetry weights multiplication derivative corresponding forward activations time layer traversed 22 Standard random backpropagation RBP Standard random backpropagation operates exactly like backpropagation weights backward pass completely random ﬁxed Thus learning rule cid2wh j ηRh O h1 j randomly backpropagated error satisﬁes recurrence relation cid2 Rh cid3 f h Rh1 k ch1 ki k weights ch1 ki random ﬁxed The boundary condition remains R L Ei S L T O L Thus RBP weights layer architecture updated gradient descent identically BP case 23 The critical equations 6 7 8 Within supervised learning framework considered goal ﬁnd optimal set weights wh j The equa tions weights satisfy critical point simply E wh j cid2 t tO h1 Bh j t 0 9 Thus general optimal weights depend input targets weights network And learning viewed lossy storage procedure transferring information contained training set weights architecture The critical Equation 9 shows necessary forward information inputs lower weights leading layer h 1 subsumed term O h1 t Thus framework separate channel communicating information inputs deep weights necessary Thus focus feedback information targets contained term Bh t physical neural transmitted dedicated channel j Note Bh t depends output O Lt target T t weights layers h fully connected case weight path unit layer h output units ways O Lt backpropagation process In addition Bh t depends upper derivatives derivatives activations functions neurons unit layer h fully connected case derivatives path unit layer h output units Thus general j depend O h1 solution critical equations weights wh outputs targets upper weights upper derivatives Backpropagation shows suﬃcient weights depend O h1 T O upper weights upper derivatives j j 24 Local learning Ultimately optimal learning information required reach critical point E appear learning rule deep weights In physical neural learning rules local 6 sense involve variables available locally space time simplicity focus locality space Thus typically present formalism local learning rule deep layer form cid2wh j cid2w L j F O h O h1 j wh j F T O L O L1 j w L j 10 11 assuming targets local variables layer Among things allows organize stratify learning rules instance considering polynomial learning rules degree forth 4 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Deep local learning term use use local learning adaptive layers feedforward architecture Note Hebbian learning 15 form local learning deep local learning proposed instance Fukushima 10 train neocognitron architecture essentially feed forward convolutional neural network inspired earlier neurophysiological work Hubel Wiesel 18 However deep local learning information targets propagated deep layers general deep local learning ﬁnd solutions critical equations succeed learning complex functions 6 25 The deep learning channel From critical equations optimal neural network learning algorithm capable communicating information outputs targets upper weights deep weights physical neural communication channel 2827 exist communicate information This deep learning channel learning channel short 6 studied tools information complexity theory In physical systems learning channel correspond physical channel leads important considerations nature instance uses forward connections reverse direction different set connections Here focus primarily information coded sent channel In general information outputs targets communicated channel wh j denoted Ih jT O L Although backpropagation propagates information layer deep layers staged way necessary Ih jT O L sent directly deep layer h skipping layers This observation leads immediately skipped variant RBP described section It important note r l h However standard backpropagation principle information form Ih shows possible send information synapses impinging neuron r l h targeting possible learn simpler type information form Ih postsynaptic neuron This class algorithms channels deep targets algorithms equivalent providing target deep neuron Furthermore backpropagation shows necessary information outputs targets contained term T O L need Ih r l h Standard backpropagation uses information upper weights ways 1 output O L appears error terms T O L 2 backpropagation process Random backpropagation crucially shows information upper weights contained backpropagation process necessary Thus ultimately focus r l h r denotes set exclusively information simple form Ih ﬁxed random weights T O L wl T O L rl rs l h f rs l h f rs l h f rs l h f jT O L wl T O L wl cid3Sl cid3Sl cid3Sl cid3Sl Thus learning channel interested local learning rules form cid2wh j F O h O h1 j wh j Ih T O L rl rs l h f cid3 Sl r l h In fact shall focus exclusively learning rules multiplicative form cid2wh j ηIh T O L rl rs l h f cid3 Sl r l hO h1 j 12 13 corresponding product presynaptic activity kind backpropagated error information standard BP RBP special cases Obvious important questions seek partial answers include 1 r l h shall multiple possibilities 2 kinds forms Ih corresponding tradeoffs forms instance terms computational complexity information trans mission 3 upper derivatives necessary T O L rl rs l h f cid3Sl 3 Random backpropagation algorithms computational complexity We going focus algorithms information required deep weight updates I h r l h produced essentially linear process vector T t O t computed output layer processed linear operations additions multiplications constants include multiplication upper derivatives Standard backpropagation algorithm possible ones We interested case matrices random However restricted setting possibilities depending instance 1 information progressively propagated layers case BP broadcasted directly deep layers 2 multiplication derivatives forward activations included 3 properties matrices learning channel sparse vs dense This leads new algorithms Here use following notations T O L f cid3Sl BP standard backpropagation RBP random backpropagation transpose feedforward matrices replaced random matrices SRBP skipped random backpropagation backpropagated signal arriving layer h given C hT O P Baldi et al Artiﬁcial Intelligence 260 2018 135 5 random matrix C h directly connecting output layer L layer h layer h ARBP adaptive random backpropagation matrices learning channel initialized randomly progressively adapted learning product corresponding forward backward signals cid2cl r R denotes randomly backpropagated error In case forward channel rs learning channel backward weights ηRl1 s O l ASRBP adaptive skipped random backpropagation combines adaptation skipped random backpropagation The default algorithm involves multiplication layer derivative forward activation functions The variants multiplication omitted denoted f cid3 The default algorithm involves dense random matrices generated instance sampling normal ized Gaussian weight But consider case random 1 0 1 binary matrices distributions including sparse versions As shall random weights sign forward weights essential lead improvements speed stability Thus use word congruent weights case Note ﬁxed random matrices learning channel initialized congruently congruence lost learning sign forward weight changes cid3Sl SRBP introduced information theoretic reasons happens error information communicated directly facilitate mathematical analyses avoids backpropagation process However sections empirically SRBP viable learning algorithm practice work better RBP Importantly simulation results suggest learning synaptic weight wh j information upper derivatives f r l h needed However immediate l h derivative f Note suggests possible algorithm skipped backpropagation SBP In case training exam ple epoch matrix feedback channel product corresponding transposed forward matrices ignoring multiplication derivative forward transfer functions layers layer consideration Multiplication derivative forward transfer functions applied layer consideration Another possibility combination RBP SRBP learning channel implemented combination longranged connections carrying SRBP signals shortrange connections carrying backpropagation procedure longrange signals available This relevant biology combinations longranged shortranged feedback connections common biological neural systems In general case linear networks f cid3 1 including excluding derivative terms makes dif ference Furthermore linear architecture AN N N layers size RBP equivalent SRBP However layers size layer sizes introduce rank constraints information backpropagated RBP differ information propagated SRBP In linear nonlinear cases network depth 3 L 3 RBP equivalent SRBP random matrix needed cid3Sh Additional variations obtained dropout multiple sets random matrices learning channel instance averaging purposes Another variation skipped case cascading allowing backward matrices learning channel pairs layers Note notion cascading increases number weights computations interesting exploratory robustness point view 31 Computational complexity considerations The number computations required send error information learning channel fundamental quantity depends computational model cost associated operations Obviously equal computational cost BP RBP basically differ value weights However subtle differences appear algorithms SRBP To illustrate consider architecture AN0 Nh N L fully connected let W total number weights In general primary cost BP multiplication synaptic weight corresponding signal backward pass Thus easy bulk operations required BP compute backpropagated signals scale like O W fact cid5W W N0 N1 N1 N2 N L1 N L L1cid2 k0 Nk Nk1 14 Note biases added separately equivalently implemented adding unit clamped layer change scaling Likewise adding costs associated sums computed neuron multiplications derivatives activation functions change scaling long operations costs constant multiplicative factor cost multiplications signals synaptic weights 6 P Baldi et al Artiﬁcial Intelligence 260 2018 135 As mentioned scaling RBP obviously different matrices However corresponding term SRBP given cid3 N L N1 N L N2 N L N L1 N L W kL1cid2 Nk k1 15 In sense computational complexity BP SRBP identical layers size signiﬁcantly different especially taking consideration tapering associated architectures cid3 practice In classiﬁcation problem instance N L 1 random matrices SRBP rank 1 W scales like total number neurons total number forward connections Thus provided leads effective learning SRBP lead computational savings digital However physical neural spite savings scaling complexity BP SRBP end This physical neural backpropagated signal reached neuron layer h communicated synapse A physical model specify cost communication Assuming unit cost BP SRBP require cid5W operations entire architecture Finally analysis physical account costs associated wiring possibly differential costs long short wires instance SRBP requires longer wires standard BP RBP 4 Algorithm simulations In section simulate algorithms standard benchmark datasets The primary focus achieving stateoftheart results better understanding new algorithms break The results summarized Table 1 end 41 MNIST Several learning algorithms ﬁrst compared MNIST 22 classiﬁcation task The neural network architecture consisted 784 inputs fullyconnected hidden layers 100 tanh units followed 10 softmax output units Weights initialized sampling scaled normal distribution 11 Training performed 100 epochs mini 6 update momentum batches size 100 initial learning rate 01 decaying factor 10 In Fig 1 performance algorithm shown training set 60000 examples test set 10000 exam ples Results adaptive versions random propagation algorithms shown Fig 2 results sparse versions shown Figs 3 4 The main conclusion general concept RBP robust works BP Performance unaffected degrades gracefully random backwards weights initialized different distributions change training The skipped versions algorithms work slightly better nonskipped versions Very deep networks trained SRBP problem shown Finally RBP variants different neuron activation functions Multiplication derivatives activation functions associated layer updated locally available play important role 42 Additional MNIST experiments In addition experiments presented following observations training MNIST variations algorithms 1 If matrices learning channel RBP randomly changed stochastic minibatch update sampled distribution mean 0 performance poor similar training layer 2 If matrices learning channel RBP randomly changed stochastic minibatch update backwards weight constrained sign corresponding forward weight training error goes 0 This signconcordance algorithm explored Liao et al 23 3 If elements matrices learning channel RBP SRBP sampled uniform normal distribu tion nonzero mean performance unchanged This consistent sparsity experiments means sampling distributions zero 4 Updates deep layer RBP SRBP appear require updates precedent layers learning channel If ﬁx weights layer h updating rest layers SRBP performance worse ﬁx layers l h 5 If remove magnitude information SRBP updates keeping sign performance better Top Layer Only algorithm good SRBP This explored section 6 If remove sign information SRBP updates keeping absolute value things work 7 If different random backward weight send error signal individual weight hidden neuron updates incoming weights things work P Baldi et al Artiﬁcial Intelligence 260 2018 135 7 Fig 1 MNIST training upper test lower accuracy function epoch different learning algorithms backpropagation BP skip BP SBP random BP RBP skip random BP SRBP version algorithm error signal multiplied derivative postsynaptic case layer trained lower layer weights ﬁxed Top Layer Only Note transfer function f algorithms differ backpropagate error signals lower layers layer updated according typical gradient descent rule Models trained ﬁve times different weight initializations trajectory mean shown cid3 8 The RBP learning rules work different transfer functions including linear logistic ReLU rectiﬁed linear units 43 CIFAR10 To test validity results performed similar simulations convolutional architecture CIFAR10 dataset 20 The speciﬁc architecture based previous work 16 consisted 3 sets convolution maxpooling layers followed denselyconnected layer 1024 tanh units softmax output layer The input consists 32by32 pixel 3channel images convolution layer consists 64 tanh channels 5 5 kernel shape 1 1 strides maxpooling layers 3 3 receptive ﬁelds 2 2 strides All weights initialized sampling scaled normal distribution 11 updated stochastic gradient descent minibatches size 128 5 update During training momentum 09 The learning rate started 001 decreased factor 10 training images randomly translated 10 direction horizontally vertically ﬂipped horizontally probability p 05 Examples results obtained 2D convolutional architectures shown Figs 5 6 Overall similar obtained MNIST dataset 8 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Fig 2 MNIST training upper test lower accuracy function training epoch adaptive versions RBP algorithm ARBP SRBP algorithm ASRBP In simulations adaption slightly improves performance SRBP speeds training For ARBP algorithm learning rate reduced factor 01 experiments weights growing quickly Models trained ﬁve times different weight initializations trajectory mean shown 5 Bit precision learning channel 51 Lowprecision error signals In following experiment investigate nature learning channel quantizing error signals BP RBP SRBP algorithms This distinct work uses quantization reduce computation 17 memory 13 costs Quantization applied forward activations weights quantization applied T O L weight update quantization given backpropagated signal received hidden neuron J h cid2wh j Ih T O L O h1 cid4 cid3 T O L J h Quantize j f h cid3 O h1 j 16 17 P Baldi et al Artiﬁcial Intelligence 260 2018 135 9 Fig 3 MNIST training upper test lower accuracy function training epoch sparse versions RBP SRBP algorithms Experiments run different levels sparsity controlling expected number n nonzero connections sent neuron layer connected backward learning channel The random backpropagation matrix connecting layers created sampling entry 0 1 Bernoulli distribution element 1 probability p nfan 0 For example SRBP Sparse1 10 softmax outputs sends nonzero weight equal 1 connection average neuron hidden layers We compare Normal versions RBP SRBP elements matrices initialized standard Normal distribution scaled way forward weight matrices 11 Models trained ﬁve times different weight initializations trajectory mean shown cid3 f h derivative activation function T O L J h Ih cid3 T O L f h nonquantized update We deﬁne quantization formula Quantizeαbitsx α signx 2round cid5 cliplog2 x α bits1 0 cid6 18 19 bits number bits needed represent 2bits possible values α scale factor quantized values fall range α α Note deﬁnition identical quantization function deﬁned Hubara et al 17 deﬁnition general α constrained power 2 In BP RBP quantization occurs error signal backpropagated previous layers quantization 3 varied bit width bits Fig 7 shows errors accumulate In experiments ﬁxed scale parameter α 2 performance degrades gracefully precision error signal decreases small values larger values bits 10 performance indistinguishable unquantized updates 32bit ﬂoats 10 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Fig 4 MNIST posttraining accuracy sparse versions SRBP algorithm For extreme values n sparse SRBP fails n 0 backward weights set zero error signals sent n 100 backward weights set 1 neurons given layer receive error signal The performance algorithm surprisingly robust extremes For sparse RBP shown backward weights scaled factor 1 n avoid exponential growth error signals lower layers Table 1 Summary experimental results showing ﬁnal test accuracy percentages RBP algorithms 100 epochs training MNIST CIFAR10 For experiments section training repeated ﬁve times different weight initializations cases mean provided sample standard deviation parentheses Also included quantization results Section 5 experiments applying dropout learning channel Section 6 cid3 MNIST Baseline No f Adaptive Sparse8 Sparse2 Sparse1 Quantized error 5bit Quantized error 3bit Quantized error 1bit Quantized update 5bit Quantized update 3bit Quantized update 1bit LC Dropout 10 LC Dropout 20 LC Dropout 50 cid3 CIFAR10 Baseline No f Sparse8 Sparse2 Sparse1 BP 979 01 899 03 976 965 946 952 965 925 977 978 977 834 02 548 36 RBP 972 01 883 11 973 01 960 04 963 05 903 11 954 925 898 940 910 96 965 967 967 702 11 327 62 463 43 629 09 567 26 SRBP 972 02 884 07 973 01 969 01 958 02 946 06 951 932 916 933 922 907 971 972 971 727 08 399 39 709 07 657 19 626 18 Top layer 847 07 479 04 52 Lowprecision weight updates The idea lowprecision weight updates new 25 Liao et al 23 recently explored use lowprecision updates RBP In following experiment investigate robustness RBP SRBP low precision weight updates controlling degree quantization Equation 19 quantization 6 weight updates need small The quantization applied error signals scale factor reduced α 2 backpropagated hidden layers summing minibatch previous experiments use minibatch updates size 100 nondecaying learning rate 01 momentum term Fig 8 The main conclusion lowprecision updates weights train MNIST classiﬁer 90 accuracy lowprecision weight updates appear degrade performance BP RBP SRBP roughly way P Baldi et al Artiﬁcial Intelligence 260 2018 135 11 Fig 5 CIFAR10 training upper test lower accuracy function training epoch different learning algorithms backpropagation BP skip BP SBP random BP RBP skip random BP SRBP version algorithm error signal multiplied derivative case layer trained lower layer weights ﬁxed Top Layer Only Models postsynaptic transfer function f trained ﬁve times different weight initializations trajectory mean shown cid3 6 Observations In section provide number simple observations provide intuition previous simu lation results RBP variations work Some observations focused SRBP general easier study standard RBP Fact 1 In RBPs algorithms Llayer parameters w L j follows gradient trained like BP random feedback weights learning layer In words BP RBP SRBP layer Fact 2 For given input sign T O changed weights updates changed opposite direction This true algorithms considered BP RBP variants derivatives activations included Fact 3 In RBP algorithms T O 0 online batch mode weights cid2wh j mode 0 line batch 12 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Fig 6 CIFAR10 training upper test lower accuracy sparse versions RBP SRBP algorithms Experiments run different levels sparsity controlling expected number n nonzero connections sent neuron layer connected backward learning channel The random backpropagation matrix connecting layers created sampling entry 0 1 Bernoulli distribution element 1 probability p nfan 0 We compare Normal versions RBP SRBP elements matrices initialized standard Normal distribution scaled way forward weight matrices 11 Models trained ﬁve times different weight initializations trajectory mean shown Fact 4 Congruence weights necessary However helpful speed learning This easily seen simple cases For instance consider linear nonlinear AN0 N1 1 architecture coherent weights denote weights layer b weights layer c weights learning channel Then variants RBP weights updates direction gradient This obvious ηT O ci I j similar layer Fact 1 For ﬁrst layer weights changes given cid2w 1 j ηT O bi I j ci bi assumed coherent So change produced gradient descent cid21 j dynamics lower layer exactly gradient direction orthant gradient downhill respect error function Additional examples showing positive necessary effect coherence given Section 7 Fact 5 SRBP perform showing upper derivatives needed However derivative corresponding layer matter In general activation functions considered derivatives tend 0 1 Thus learning attenuated neurons saturated So ingredient matter let close 0 synapses neurons saturated change synapses neurons saturated f cid3 P Baldi et al Artiﬁcial Intelligence 260 2018 135 13 Fig 7 MNIST training upper test lower accuracy function training epoch sparse versions RBP SRBP algorithms Experiments run different levels quantization error signal controlling bitwidth bits according formula given text Equation 19 Fact 6 Consider multiclass classiﬁcation problem MNIST All elements class tend receive backpropagated signal tend unison For instance consider beginning learning small random weights forward network All images tend produce uniform output vector similar 01 01 01 Thus images 0 class tend produce uniform error vector similar 09 01 01 images 1 class tend produce uniform error vector similar 01 09 01 essentially orthogonal previous error vector forth In words 10 classes associated 10 roughly orthogonal error vectors When vectors multiplied ﬁxed random matrix SRBP tend produce 10 approximately orthogonal vectors corresponding hidden layer Thus backpropagated error signals tend similar digit class orthogonal different digit classes At beginning learning expect roughly half 5 digits 10 MNIST case direction BP Thus conclusion intuitive picture RBP work 1 random weights introduce ﬁxed coupling learning dynamics forward weights mathematical analyses 2 layer weights follows gradient descent stirs learning dynamic right direction 3 learning dynamic tends cluster inputs associated response away similar clusters Next discuss possible connection dropout 14 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Fig 8 MNIST training upper test lower accuracy function training epoch sparse versions RBP SRBP algorithms Experi ments carried different levels quantization weight updates controlling bitwidth bits according formula given text Equation 19 Quantization applied examplespeciﬁc update summing updates minibatch 61 Connections dropout Dropout 165 different training algorithm based form randomness Here explore possible connections RBP First observe BP equations viewed form dropout averaging equations sense ﬁxed example compute ensemble average activity units learning channel The ensemble average taken possible backpropagation networks unit dropped stochastically unit layer h dropped probability 1 f o assuming derivatives transfer functions 0 1 inclusively case standard transfer functions logistic rectiﬁed linear transfer functions rescaling necessary Note way dropout probabilities change example units saturated likely dropped consistently remark saturated units learn cid3Sh In view kinds noise 1 choice dropout probabilities vary example 2 actual dropout procedure Consider adding type noise symmetric weights backward pass form wh j ξ h j 20 j 0 The distribution noise Gaussian instance essential assume Eξ h The important point noise weight independent noise weights independent P Baldi et al Artiﬁcial Intelligence 260 2018 135 15 Fig 9 MNIST training upper test lower accuracy function training epoch BP RBP SRBP different dropout probabilities learning channel 0 dropout 10 20 50 For dropout probability p error signals dropped scaled 11 p As dropout forward propagation large dropout probabilities lead slower training hurting ﬁnal performance dropout noise units Under assumptions shown 5 expected value activity unit backward pass exactly given standard BP equations equal Bh unit layer h In words standard backpropagation viewed computing exact average backpropagation processes implemented stochastic realizations backward network forms noise described Thus reverse argument consider RBP approximates average BP averaging ﬁrst kinds noise instead averaging random realization weights selected ﬁxed epochs This connection suggests intermediate RBP variants samples weights single Finally possible use dropout backward pass The forward pass robust dropping neurons fact dropout procedure beneﬁcial 165 Here apply dropout procedure neurons learning channel backward pass The results simulations reported Fig 9 conﬁrm BP RBP SRBP robust respect dropout 7 Mathematical analysis 71 General considerations The general strategy try derive precise mathematical results proceed simple architectures complex architectures linear case nonlinear case The linear case amenable analysis 16 P Baldi et al Artiﬁcial Intelligence 260 2018 135 case RBP SRBP equivalent hidden layer layers size Thus study convergence RBP optimal solutions linear architectures increasing complexity A1 1 1 A1 1 1 1 A1 1 1 A1 N 1 AN 1 N general AN0 N1 N2 case single hidden layer This followed study nonlinear A1 1 1 case For kind linear network set standard assumptions derive set nonlinear fact polynomial autonomous ordinary differential equations ODEs average batch time evolution synaptic weights RBP SRBP algorithm As soon variable nonlinear general theory understand corresponding behavior In fact dimensions problem understanding upper bound number relative position limit cycles form dxdt P x y dydt Q x y P Q polynomials degree n open fact Hilberts 16th problem ﬁeld dynamical systems 2919 When considering speciﬁc systems arising RBPSRBP learning equations ﬁrst prove systems longterm solution Note polynomial ODEs longterm solutions dxdt xα x0 cid7 0 longterm solutions α 1 If trajectories bounded longterm solutions exist We particularly interested longterm solutions converge ﬁxed point opposed limit cycles behaviors A number interesting cases reduced polynomial differential equations dimension These understood following theorem Theorem 1 Let dxdt Q x k0 k1x knxn ﬁrst order polynomial differential equation dimension degree n 1 let r1 r2 rk k n ordered list distinct real roots Q ﬁxed points If x0 ri xt ri solution constant If ri x0 ri1 xt ri Q 0 ri ri1 xt ri1 Q 0 ri ri1 If x0 r1 Q 0 corresponding interval xt r1 Otherwise Q 0 corresponding interval long time solution xt diverges ﬁnite horizon If x0 rk Q 0 corresponding interval xt rk Otherwise Q 0 corresponding interval long time solution xt diverges ﬁnite horizon A necessary suﬃcient condition dynamics converge ﬁxed point degree n odd leading coeﬃcient negative Proof The proof theorem straightforward visualized plotting function Q Finally general matrices forward channel denoted A1 A2 matrices learning channel denoted C1 C2 Theorems stated concise form additional important facts contained proofs 72 The simplest linear chain A1 1 1 Derivation The simplest case correspond linear A1 1 1 architecture Fig 10 Let denote a1 a2 weights ﬁrst second layer c1 random weight learning channel In case O t a1a2 It learning equations given α EI T β EI 2 With proper scaling learning rate η cid2t leads nonlinear coupled differential equations temporal evolution a1 a2 learning cid2a1 ηc1T O I ηc1T a1a2 II cid2a2 ηT O a1 I ηT a1a2 Ia1 I When averaged training set cid7 Ecid2a1 ηc1 EI T ηc1a1a2 EI 2 ηc1α ηc1a1a2β Ecid2a2 ηa1 EI T ηa2 1a2 EI 2 ηa1α ηa2 1a2β cid7 cid8 da1 dt da2 dt αc1 βc1a1a2 c1α βa1a2 αa1 βa2 1a2 a1α βa1a2 Note dynamic P a1a2 given d P dt a1 da2 dt a2 da1 dt a2 1 a2c1α β P The error given E 1 2 ET P I2 1 2 ET 2 1 2 P 2β P α 1 2 ET 2 1 2β α β P 2 α2 2β 21 22 23 24 25 P Baldi et al Artiﬁcial Intelligence 260 2018 135 17 Fig 10 Left A1 1 1 architecture The weights a1 a2 adjustable feedback weight c1 constant Right A1 1 1 1 architecture The weights a1 a2 a3 adjustable feedback weights c1 c2 constant dE d P α β P E ai α β P P ai equality requires ai cid7 0 26 Theorem 2 The Equation 23 converges ﬁxed point Furthermore trivial cases associated c1 0 starting initial conditions converges ﬁxed point corresponding global minimum quadratic error function All ﬁxed points located hyperbolas given α β P 0 global minima error function All ﬁxed points attractors interior certain parabola For starting point ﬁnal ﬁxed point calculated solving cubic equation Proof As ﬁrst example ﬁrst deal trivial cases For subsequent systems skip trivial cases entirely Trivial cases 1 If β 0 I 0 α 0 As result activity input hidden output neuron 0 Therefore weights a1 a2 remain constant da1dt da2dt 0 equal initial values a10 a20 The error remain constant equal 0 T 0 Thus assume β 0 2 If c1 0 lower weight a1 changes remains equal initial value If initial value satisﬁes a10 0 activity hidden output unit remains equal 0 times a2 remains constant equal initial value a2 a20 The error remains constant equal 0 T 0 If a10 cid7 0 error simple quadratic convex function a2 rule adjusting a2 simply gradient descent value a2 converge optimal value given a2 αβa10 General case Thus assume β 0 c1 cid7 0 Furthermore easy check changing sign α corresponds reﬂection a2axis Likewise changing sign c1 corresponds reﬂection origin a1 a2 axis Thus short suﬃcient focus case α 0 β 0 c1 0 In case critical points a1 a2 given P a1a2 α β EI T EI 2 0 27 corresponds hyperbolas twodimensional a1 a2 plane ﬁrst quadrant α EI T 0 Note critical points depend feedback weight c1 All critical points correspond global minima error function E 1 2 ET O 2 Furthermore critical points P include parabola a2 1 a2c1 0 a2 a2 1c1 28 Fig 11 These critical points dependent weights learning channel This parabola intersects hyperbola a1a2 P αβ point coordinates a1 c1αβ13 a2 α23c In upper half plane a2 c1 congruent positive dynamics simple understand For instance ﬁrst quadrant a1 a2 c1 0 α β P 0 da1dt 0 da2dt 0 d P dt 0 13 1 β 23 18 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Fig 11 Vector ﬁeld A1 1 1 linear case c1 1 α 1 β 1 a1 correspond horizontal axis a2 correspond vertical axis The critical points correspond hyperbolas critical points ﬁxed points global minima error functions Arrows colored 1c1 unstable All critical points attractors according value d P dt showing critical points inside parabola a2 a2 Reversing sign α leads reﬂection a2axis reversing sign c1 leads reﬂection a1 a2 axes For interpretation colors ﬁgures reader referred web version article gradient vector ﬂow directed hyperbola critical points If started region a1 a2 P grow monotonically critical point reached error decrease monotonically global minimum If α β P 0 da1dt 0 da2dt 0 d P dt 0 vector ﬂow directed hyperbola critical points If started region a1 a2 P decrease monotonically critical point reached error decrease monotonically global minimum A similar situation observed fourth quadrant a1 0 a2 0 More generally a2 c1 sign congruent BP a2 1 a2c1 0 P increase α β P 0 decrease α β P 0 Note true general c1 small regardless a2c1 positive This remains true c1 varies sign relative a1 a2 case true a2 1 long small When c1 small dynamics dominated layer The lower layer changes slowly layer adapts rapidly converges global minimum When a2 c1 recovers convergent dynamic BP d P dt sign α β P However lower half plane situation slightly complicated Fig 11 To solve dynamics general case Equation 23 1 a1 da2 dt c1 gives a2 1 2c1 da1 dt a2 1 C ﬁnally a2 1 2c1 a2 1 b0 1 2c1 a2 10 29 30 Given starting point a10 a20 follow trajectory given parabola Equation 30 converges critical point global optimum da1dt da2dt 0 To ﬁnd speciﬁc critical point converges Equations 30 27 satisﬁed simultaneously leads depressed cubic equation a3 1 2c1a20 a102a1 2 c1α β 0 31 solved standard formula roots cubic equations Note parabolic trajectories tained upper half plane intersect critical hyperbola point equation single real root In lower half plane parabolas associated trajectories intersect hyperbolas 1 2 3 distinct P Baldi et al Artiﬁcial Intelligence 260 2018 135 19 points corresponding 1 real root 2 real roots 1 double 3 real roots The double root corresponds point c1αβ13 associated intersection parabola Equation 30 hyperbola critical points a1a2 αβ parabola additional critical points P given Equation 28 When multiple roots convergence point trajectory easily identiﬁed looking derivative vector ﬂow Fig 11 Note ﬁgure points critical hyperbolas stable attractors lower halfplane satisfy a1a2 αβ a2c1 a2 1 0 This shown linearizing critical points Linearization critical points If consider small deviation a1 u a2 v critical point a1 a2 satisfying α βa1a2 0 linearize corresponding cid8 du dt dv dt βc1a2u a1 v βa1a2u a1 v a1a2 αβ If let w a2u a1 v dw dt βc1a2 a2 1w w w0e βc1a2a2 1t 32 33 Thus βc1a2 a2 small c1 sign a2 If βc1a2 a2 1 0 w constant described If βc1a2 a2 1 0 w converges zero a1 a2 attractor In particular case c1 1 0 w diverges corresponds unstable critical points Finally note cases instance trajectories upper half plane value P trajectories increases decreases monotonically global optimum value However case trajectories d P dt changes sign happen 73 Adding depth linear chain A1 1 1 1 Derivation In case linear A1 1 1 1 architecture notational simplicity let denote a1 a2 a3 forward weights c1 c2 random weights learning channel note index equal target layer In case O t a1a2a3 It P It The learning equations cid2a1 ηc1T O I ηc1T a1a2a3 II cid2a2 ηc2T O a1 I ηc2T a1a2a3 Ia1 I cid2a3 ηT O a1a2 I ηT a1a2a3 Ia1a2 I When averaged training set Ecid2a1 ηc1 EI T ηc1 P EI 2 ηc1α ηc1 P β Ecid2a2 ηc2a1 EI T ηc2a1 P EI 2 ηc2a1α ηc2a1 P β Ecid2a3 ηa1a2 EI T ηa1a2 P EI 2 ηa1a2β ηa1a2 P β 34 35 P a1a2a3 With proper scaling learning rate η cid2t leads nonlinear coupled differential equations temporal evolution a1 a2 a3 learning da1 dt da2 dt da3 dt c1α β P c2a1α β P a1a2α β P The dynamic P a1a2a3 given d P dt a1a2 da3 dt a2a3 da1 dt a1a3 da2 dt c1a2a3 c2a2 1a3α β P 36 37 a2 1a2 2 Theorem 3 Except trivial cases associated c1 0 c2 0 starting initial conditions Equation 36 converges ﬁxed point corresponding global minimum quadratic error function All ﬁxed points located hypersurface given α β P 0 global minima error function Along trajectory ai1 quadratic function ai For starting point ﬁnal ﬁxed point calculated solving polynomial equation degree seven 20 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Fig 12 Left A1 1 architecture The weights ai adjustable feedback weight ci ﬁxed The index parameter associated corresponding target layer Proof If c1 0 a1 remains constant linear case A1 1 1 architecture inputs I replaced a1 I Likewise c2 0 a2 remains constant problem reduced A1 1 case proper adjustments Thus rest section assume c1 cid7 0 c2 cid7 0 The critical points correspond α β P 0 depend weights learning channel These critical points correspond global minima error function These critical points critical points 1a3 0 a1 a2 a3 product P Additional critical points P provided hypersurface a2 R3 c1a2a3 c2a2 1a2 2 The dynamics solved noting Equation 36 yields da2 dt a1c2 c1 da1 dt da3 dt a2 c2 da2 dt As result a2 c2 2c1 a2 1 C1 C1 a20 c2 2c1 a102 a3 1 2c2 a2 2 C2 C2 a30 1 2c2 a202 Substituting results ﬁrst equation gives da1 dt c1α βa1 da1 dt c1α βa1 c2 2c1 c2 2c1 a2 1 C1 1 2c2 a2 2 C2 a2 1 C1 1 2c2 c2 2c1 a2 1 C12 C2 38 39 40 41 42 In short da1dt Q a1 Q polynomial degree 7 a1 By expanding simplifying Equation 42 easy leading term Q negative given βc2 1 Therefore Theorem 1 initial conditions a10 a1t converges ﬁnite ﬁxed point Since a2 quadratic function a1 converges ﬁnite ﬁxed point similarly a3 Thus general case converges global minimum error function satisfying α β P 0 The hypersurface a2 1a3 0 depends c1 c2 provides additional critical points product P It shown linearization hypersurface separates stable unstable ﬁxed points As previous case small weights congruent weights help learning necessary In particular c1 1a3 0 d P dt c2 small c1 small c2 congruent a3 a2 sign α β P c1a2a3 c2a2 c1a2a3 c2a2 216c2 1a2 1a2 2 2 74 The general linear chain A1 1 Derivation The analysis extended immediately linear chain architecture A1 1 arbitrary length Fig 12 In case let a1 a2 aL denote forward weights c1 cL1 denote feedback weights Using derivation previous cases letting O P I a1a2 aL I gives cid2ai ηciT O a1a2 ai1 I 43 P Baldi et al Artiﬁcial Intelligence 260 2018 135 21 1 L Taking expectations usual leads set differential equations c1α β P c2a1α β P da1 dt da2 dt daL1 dt daL dt cL1a1a2 aL2α β P a1 aL1α β P compact form dai dt ci ki1cid13 k1 akα β P 1 L cL 1 As usual P cid14 L i1 ai α ET I β EI 2 A simple calculation yields d P dt Lcid2 i1 P ai dai dt α β P Lcid2 i1 P ci ai i1cid13 k1 ak equality requiring ai cid7 0 44 45 46 Theorem 4 Except trivial cases starting initial conditions Equation 44 converges ﬁxed point corre sponding global minimum quadratic error function All ﬁxed points located hypersurface given α β P 0 global minima error function Along trajectory ai1 quadratic function ai For starting point ﬁnal ﬁxed point calculated solving polynomial equation degree 2L 1 Proof Again weights learning channel nonzero critical points correspond curve α β P 0 These critical points independent weights learning channel correspond global minima i1 error function Additional critical points product P a1 aL given surface k1 ak 0 These critical points dependent weights learning channel If ci small congruent j 0 d P dt sign α β P Thus respective feedforward weights small congruent weights help learning necessary icid7k aicLk i1 P ci ai jk1 j1 L k1 cid15 cid15 cid14 cid14 cid14 L To convergence Equation 45 ci dai1 dt ci1ai dai dt 47 Note derivatives daidt zero zero limit cycles Since general case ci nonzero ai1 ci1 2ci a2 C 48 showing quadratic relationship ai1 ai linear term Thus ai expressed polynomial function a1 degree 2i1 containing terms ai k0 k1a2 1 ki1a2i1 1 ki1 ci 2ci1 ci1 2ci2 2 ci2 2ci3 4 2i1 c3 2c2 49 50 By substituting relationships equation derivative a1 da1dt Q a1 Q polynomial odd degree n given n 1 2 4 2L1 2L 1 51 Furthermore Equation 50 seen leading coeﬃcient negative Theorem 1 set initial conditions converge ﬁnite ﬁxed point For given initial condition point convergence solved looking nearby roots polynomial Q degree n 22 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Gradient descent equations For comparison gradient descent equations dai dt aL ai1a1 ai1α β P P ai α β P E ai equality middle requires ai cid7 0 In case coupling neighboring terms given ai dai dt ai1 dai1 dt Solving equation yields da2 dt da2 i1 dt a2 i1 a2 C 75 Adding width expansive A1 N 1 52 53 54 Derivation Consider linear A1 N 1 architecture Fig 13 For notational simplicity let a1 aN weights lower layer b1 bN weights upper layer c1 cN random weights learning channel In case O t aibi The learning equations aibi It We let P cid15 cid15 cid2ai ηciT O I ηciT cid15 cid2bi ηT O ai I ηT aibi II aibi Iai I cid15 When averaged training set cid8 Ecid2ai ηci EI T ηci P EI 2 ηciα ηci P β Ecid2bi ηai EI T ηai P EI 2 ηaiα ηai P β cid8 cid8 αci βci P ciα β P αai βai P aiα β P dai dt dbi dt The dynamic P aibi given cid15 d P dt cid2 ai dbi dt bi dai dt α β P cid2 bici a2 α EI T β EI 2 With proper scaling learning rate η cid2t leads nonlinear coupled differential equations temporal evolution ai bi learning 55 56 57 58 Theorem 5 Except trivial cases starting initial conditions Equation 57 converges ﬁxed point corre sponding global minimum quadratic error function All ﬁxed points located hypersurface given α β P 0 global minima error function Along trajectory bi quadratic polynomial function ai Each ai aﬃne function j For starting point ﬁnal ﬁxed point calculated solving polynomial differential equation degree 3 Proof Many features linear chain similar analyses In general case weights learning channel nonzero critical points given surface α β P 0 correspond global optima These critical points independent weights learning channel Additional critical cid15 bici 0 depends weights learning points product P bici 0 d P dt sign channel If ci s small congruent respective bi s α β P aibi given surface a2 a2 cid15 cid15 To address convergence Equation 57 leads vertical coupling ai bi ai dai dt ci dbi dt bi 1 2ci a2 Ci 59 1 N Thus dynamics ai variables completely determines dynamics bi variables needs understand behavior ai variables In addition vertical coupling ai bi horizontal coupling ai variables given Equation 57 resulting dai1 dt ci1 ci dai dt ai1 ci1 ci ai K i1 60 P Baldi et al Artiﬁcial Intelligence 260 2018 135 23 Fig 13 Left Expansive A1 N 1 architecture Right Compressive AN 1N architecture In cases parameters ai bi adjustable parameters ci ﬁxed Thus iterating variables ai expressed aﬃne functions a1 form ai ci c1 a1 K cid3 1 N 61 Thus solving entire reduced solving a1 The differential equation a1 form da1dt Q a1 Q polynomial degree 3 Its leading term leading term c1β P To ﬁnd leading term cid2 P aibi cid2 a3 2ci ciai leading term Q given K a3 1 K βc1 1 2c1 1 2c2 c3 2 c3 1 1 2cN c3 N c3 1 β 2 1 c2 1 Ncid2 1 c2 62 63 Thus leading term Q negative coeﬃcient a1 converges ﬁnite ﬁxed point variables 76 Adding width compressive AN 1 N Derivation Consider linear AN 1 N architecture Fig 13 The online learning equations given cid15 N cid2ai η cid2bi ηT O k1 ckTk O kIi cid15 N k1 ak Ik cid8 cid8 64 1 N As usual taking expectations matrix notation small learning rate leads differential equations d A dt dB dt Ccid9T I B Acid9I I cid9T I B Acid9I I At 65 Here A 1 N matrix B N 1 matrix C 1 N matrix Mt denotes transpose matrix M cid9I I EI It cid9T I ET It N N matrices associated data Lemma 1 Along ﬂow deﬁned Equation 65 solution satisﬁes C B 1 2 A2 K K constant depending initial values Proof The proof immediate C dB dt d A dt At cid2 ci dbi dt cid2 ai dai dt cid2 ci dbi dt 1 2 d A2 dt A2 a2 1 a2 N The theorem obtained integration 66 67 24 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Theorem 6 In case autoencoder uncorrelated normalized data Equation 68 converges ﬁxed point satisfying A βC β positive root particular cubic equation At ﬁxed point B C tβC2 product P B A converges C t CC2 Proof For autoencoder uncorrelated normalized data cid9T I cid9I I Id In case written 68 69 70 cid8 d A dt dB dt We deﬁne CId B A Id B A At σ t 1 2 cid10 Acid102 K C σ t A dt let A0 A0 Note σ t K We assume C A0 linearly independent proof easier Then d A Therefore solution At form At f tC gt A0 yields cid3 f t 1 σ t f t t σ tgt f 0 0 g0 1 cid3 g gt e cid16 t 0 σ sds f t e cid16 t 0 σ sds cid16 e r 0 σ sdsdr tcid17 0 From expressions know f g nonnegative We f t gt tcid17 0 1 gr dr Since σ t K gt bounded cid17 0 1 gr dr 71 72 73 74 75 By general theorem shown section know cid10 Acid10 bounded f Using Equation 74 implies gt 0 t Now consider equation bounded cid3 1 σ f f Now consider cubic equation 1 1 2 t2cid10Ccid102 K t 0 For t large gt 0 σ t 1 2 f 2cid10V cid102 K Thus Equation 76 close polynomial differential equation cid3 1 h 1 2 h2cid10Ccid102 K h 76 77 78 79 P Baldi et al Artiﬁcial Intelligence 260 2018 135 25 Fig 14 General linear case architecture AN0 N L Each forward matrix Ai adjustable size Ni Ni1 In SRBP feedback matrices Ci ﬁxed size Ni N L By Theorem 1 convergent positive root Equation 77 comparison Equation 76 converge This proves f t β t combination gt 0 t shows A converges βC As A converges ﬁxed point error function converges convex function B performs gradient descent convex function approach ﬁxed point By results 23 solution satisfy B A At At When A βC gives B C t βC C t C t βcid10Ccid102 In case product P B A converges ﬁxed point C t Ccid10Ccid102 The proof easily adapted slightly general case cid9I I diagonal matrix 77 The general linear case AN0 N1 N L Derivation Although provide solution case useful derive equations We assume general feedforward linear architecture Fig 14 AN0 N1 N L adjustable forward matrices A1 A L ﬁxed feedback matrices C1 C L1 C L Id Each matrix Ai size Ni Ni1 SRBP matrix Ci size Ni N L As usual O t P It cid14 L i1 AiIt Assuming learning rate matrix notation cid2 Ai ηCiT O Ai1 A1 It ηCiT O It At 1 At i1 taking averages leads differential equations d Ai dt Cicid9T I P cid9I I At 1 At i1 80 81 P A L A L1 A1 cid9T I ET It cid9I I EI It cid9T I N L N0 matrix cid9I I N0 N0 matrix In case autoencoder T I cid9T I cid9I I Equation 81 true 1 L C L Id Id identity matrix These equations establish coupling layers d Ai1 dt Ci1cid9T I P cid9I I At 1 At When layers sizes coupling written C 1 i1 d Ai1 dt assume random matrices Ci invertible square matrices d Ai1 dt Ci1C d Ai dt d Ai dt C 1 1 At At Gradient descent equations For comparison gradient descent equations given d Ai dt At i1 At resulting coupling Lcid9T I P cid9I I At 1 At i1 At i1 Ai1 dt d Ai dt At deﬁnition E Ai d Ai dt E ET P I22 82 83 84 85 86 26 P Baldi et al Artiﬁcial Intelligence 260 2018 135 RBP equations Note case RBP backward matrices C1 C L1 opposed SRBP differential equations d Ai dt Ci C L1cid9T I P cid9I I At 1 At i1 87 By letting B Ci C L1 obtains SRBP equations size layers impose constraints rank matrices B 78 The general threelayer linear case AN0 N1 N2 Derivation Here let A1 N1 N0 matrix weights lower layer A2 N2 N1 matrix weights upper layer C1 N1 N2 random matrix weights learning channel In case O t A2 A1 It P It cid9I I EI It N0 N0 cid9T I ET It N2 N1 The learning equations given cid8 d A2 dt d A1 dt cid9T I P cid9I I At 1 C1cid9T I P cid9I I resulting coupling C1 d A2 dt d A1 dt At 1 88 89 The corresponding gradient descent equations obtained immediately replacing C1 At 2 Note twolayer linear case corresponds classical Least Square Method understood The general theory threelayer linear case understood In section signiﬁcant step providing complete treatment case One main results deﬁned Equation 88 longterm existence C1 P C1 A2 A1 convergent short able learn However imply matrix valued functions A1t A2t individually convergent We prove special cases like AN 1 N A1 N 1 studied previous sections A2 2 2 We begin following theorem Theorem 7 The general layer linear Equation 88 longterm solutions Moreover cid10 A1cid10 bounded Proof As Lemma 1 dC A2 dt Thus Ccid9T I A2 A1cid9I I At 1 d A1 dt At 1 dC A2 C A2t dt d A1 dt At 1 A1 d At 1 dt d dt A1 At 1 It follows C0 C A2 C A2t A1 At 1 C0 constant matrix Let f Tr A1 At 1 Using Lemma 2 df dt 2 Tr d A1 dt 1 2 TrCcid9T I At At 1 C A2 A1cid9I I At 1 c3cid10 A1cid10 2 TrC A2 A1cid9I I At 1 Since 2 TrC A2 A1cid9I I At 1 TrC A2 A1cid9I I At 1 Tr A1cid9I I At 1C A2t 2 TrC A2 A1cid9I I At 1 TrC A2 A1cid9I I At 1 TrC A2t A1cid9I I At 1 Equation 92 2 TrC A2 A1cid9I I At 1 Tr A1 At 1 A1cid9I I At 1 TrC0 A1cid9I I At 1 90 91 92 93 94 95 96 97 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Using second inequality Lemma 2 df dt c3 A1 TrC0 A1cid9I I At 1 c1 f 2 c3 cid18 f c4 f c1 f 2 c5 1 2 c1 f 2 27 98 positive constants c1 c5 Since A1 longterm existence f Note possible f increasing t f f 2c5 implies f 2 c1 f 2 0 f bounded c1 increasing local maximum point f f cid3t 0 c5 1 c1 But f 2c5 c1 2c5 Lemma 2 There constant c1 0 f c1cid10 A1cid102 1 2 Tr A1 At 1 A1cid9I I At 1 c1 f 2 Proof The ﬁrst statement obvious To prove second observe Tr A1 At 1 A1cid9I I At 1 Tr At 1 A1cid9I I At 1 A1 c2 Tr At 1 A1 At 1 A1 c1 f 2 constants c1 c2 0 To complete proof Theorem 7 estimate A2 sure diverge ﬁnite time Let h 1 2 Tr A2 At 2 Then dh dt dh dt Trcid9T I A2 A1cid9I I At 1 At 2 Trcid9T I At 1 At 2 Tr A2 A1cid9I I At 1 At 2 Trcid9T I At 1 At 2 Since shown A1 bounded dh dt Trcid9T I At 1 At 2 K A2 K h constant K As result h K1t2 K2 cid18 A2 K1t2 K2 K3t K4 99 100 101 102 103 104 Since t 1 A2 bounded longterm solutions The main result section follows Theorem 8 Partial Convergence Theorem Along ﬂow Equation 88 A1 C1 A2 uniformly bounded Moreover C1 A2 A1 C1cid9T I cid9 1 I I t cid17 0 cid10C1 A2 A1 C1cid9T I cid9 1 I I cid102dt Proof Let U C1cid9T I A2 A1cid9I I cid9 1 I I Then U cid9I I dC1 A2 dt U cid9I I A T 1 d A1 dt It follows dC1 A2 dt C1 A2T U cid9I I A T 1 C1 A2T U cid9I I C1 A2 A1T d A1C1cid9T I cid9 dt 1 I I T U cid9I I C1cid9T I cid9 1 I I T U C1cid9T I T 105 106 107 28 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Thus dC1 A2 dt C1 A2T d A1C1cid9T I cid9 dt 1 I I T U cid9I I U T 0 Here matrices X Y write X Y Y X semipositive matrix Let V C1 A2C1 A2T A1C1cid9T I cid9 1 I I T C1cid9T I cid9 1 I I A T 1 Then dV dt 0 By Theorem 7 lower bound matrix V V A1C1cid9T I cid9 1 I I T C1cid9T I cid9 1 I I A T 1 C constant matrix C Thus t V V t convergent Using inequality expression C1 A2C1 A2T A1C1cid9T I cid9 1 I I T C1cid9T I cid9 1 I I A T 1 108 109 monotonically decreasing Since A1 bounded Theorem 7 A2 A T 2 nonnegative expression convergent In particular C1 A2 bounded ﬂow By 108 A1 C1 A2 L2 integrable Thus fact pointwise convergence C1 A2 A1 Since C1 rank partial convergence If C1 rank general case C1 A2 A1 convergent A2 A1 When partial convergence imply convergences solution A1t A2t The following result gives suﬃ cient condition Theorem 9 If set matrices A1 A2 satisfying 1 I I C1 A2 A1 C1cid9T I cid9 C1 A2 C1 A2T A1 A T 1 A1C1cid9T I T C1cid9T I A T C1 A2 A T 1 2 C T K 1 L 110 discrete A1t C1 A2t convergent Proof By proof Theorem 8 know A1t C1 A2t bounded limiting points pair A1t C1 A2t satisfy relationships Equation 110 If set discrete limit unique A1t C1 A2t converge If C1 rank Equation 88 convergent assumptions Theorem 9 satisﬁed Applying result A1 N 1 AN 1 N cases provides alternative proofs Theorem 3 Theorem 6 The details omitted Beyond cases algebraic set deﬁned Equation 110 complicated study The ﬁrst nontrivial case analyzed corresponds A2 2 2 architecture In special case solve convergence problem entirely follows For sake simplicity assume cid9I I cid9T I C1 I Then associated Equation 88 simpliﬁed cid8 dB dt d A dt I B A At I B A 111 At Bt 2 2 matrix functions By Theorem 7 know Bt At convergent In order prove Bt At individually convergent prove following result Theorem 10 Let F set 2 2 matrices A B satisfying equations B B T A A T K A A T B B T L A B I 112 K L ﬁxed matrices Then F discrete set deﬁned Equation 111 convergent P Baldi et al Artiﬁcial Intelligence 260 2018 135 29 Proof The proof somewhat long technical given Appendix It uses basic tools algebraic geometry Theorem 10 provides evidence general algebraic set deﬁned Equation 110 discrete Although moment able prove discreteness general case question separate mathemat ics real algebraic geometry The deﬁned Equation 110 overdetermined algebraic equations For example At Bt n n matrices C nonsingular contains nn 1 equations n2 unknowns One deﬁne Koszul complex 9 associated equations Using complex given speciﬁc matrices C cid9T I cid9I I K L constructive algorithmic way determine set discrete If corresponding ODE convergent1 79 A nonlinear case As expected case nonlinear networks challenging analyze mathematically In linear case transfer functions identity derivatives transfer functions equal 1 play role The simulations reported provide evidence nonlinear case derivatives activation functions play role RBP SRBP Here study simple nonlinear case provides evidence We consider simple A1 1 1 architecture single power function nonlinearity power μ cid7 1 hidden layer O 1S 1 S 1μ The ﬁnal output neuron linear O 2S 2 S 2 overall inputoutput relationship O a2a1 Iμ Setting μ 13 instance provides Sshaped transfer function hidden layer setting μ 1 corresponds linear case analyzed previous section The weights a1 a2 forward network c1 learning channel cid8 Derivation derivatives When derivatives included obtains μ ET Iμ a2a 1 α βa2a μ μ 1 EIμ1 c1γ δa2a 1 c1ET I a2a μ 1 EμI 2 da2 dt da1 dt μ 1 μ 1 113 α ET Iμ β EI 2μ γ ET I δ EIμ1 Except trivial cases ﬁxed μ points general a2a 1 γ δ time αβ a2a μ 1 Derivation derivatives In contrast derivative forward activation included cid8 da2 dt da1 dt μ 1 c1μa ET Iμ a2a μ 1 EI 2μ 2μ1 1 μ 1 α βa2a EI 2μ μ 1 μ1 1 ET Iμ a2c1μa μ1 1 This leads coupling a1 da1 dt c1μ da2 dt a2 a2 1 2c1μ K c1μα βa2a μ 1 114 115 excluding usual trivial cases c1 0 μ 0 Here K constant depending a10 a20 The coupling shows da1dt 0 da2dt 0 general limit cycles possible The critical points given equation α βa2a μ 1 0 a2 α βa μ 1 116 depend weight learning channel Thus nontrivial cases a2 hyperbolic function μ 1 It easy cases converges ﬁxed point For instance α 0 c1 0 μ 1 a10 a20 small positive da1dt 0 da2dt 0 derivatives monotonically μ increasing α βa2a 1 decreases monotonically convergence critical point Thus general including derivatives forward activations simpler better behaved In fact general theorem Theorem 12 Assume α 0 β 0 c1 0 μ 1 Then positive initial values a10 0 a20 0 described Equation 114 convergent positive roots equation t α β t2 2c1μ K tμ 0 1 We thank Professor Vladimir Baranovsky providing information 117 30 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Table 2 Postsynaptic information required deep synapses optimal learning Ih j represents signal carried deep learning channel postsynaptic term learning rules considered Different algorithms reveal essential ingredients signal simpliﬁed In row function F implemented sparse adaptive matrices carry low precision signals include nonlinear transformations learning channel 4 cid3l h cid3l h Information Ih Ih Ih Ih Ih Ih Ih Ih rsl h f rsl h f j T O wl T O wl cid3l h T O wl rsl h f rsl h 1 wh1 T O wl T O rl rsl h 1 rh cid3l h T O rh ki f cid3l h T O rh ki f cid3l h F T O f Ih j Ih j Ih j Ih j Ih j Ih j Ih j Ih j ki ki f cid3l h f cid3l h Algorithm General Form BP symmetric weights BP symmetric weights BP symmetric weights RBP random weights SRBP random skipped weights SRBP random skipped weights F sparselowprecadaptivenonlin Proof Using Equation 115 differential equation a1 rewritten da1 dt μa μ1 1 c1α β a2 1 2c1μ K μ 1 Q a1 118 When μ integer Q a1 polynomial odd degree leading coeﬃcient negative Theorem 1 convergent If μ integer let r1 rk positive roots function Q The proof proceeds similarly proof Theorem 1 That differential equation Equation 118 convergent nonnegative roots Q t However a10 0 careful analysis shows a1 converge zero Thus a1 converge positive root Equation 117 Gradient descent equations Finally comparison case gradient descent given cid8 ET Iμ a2a da2 dt da1 dt μ 1 a2μa μ1 1 ET Iμ a2 μ 1 EI 2μ 2μ1 2μa 1 μ 1 α βa2a EI 2μ μ 1 μ1 1 a2μα βa2a μ 1 119 Except trivial cases critical points given Equation 116 converges critical point 8 Conclusion Training deep architectures backpropagation digital computers useful practical applications easier creation software packages automatic differentiation capabilities This convenience misleading hampers thinking constraints learning physical neural systems merely mimicked digital computers Thinking learning physical systems useful ways leads notion local learning rules turn identiﬁes fundamental problems facing propagation physical systems First backpropagation local learning channel required communicate error information output layer deep weights Second backpropagation requires symmetric weights sig niﬁcant challenge physical systems use forward channel reverse direction requiring different pathway communicate errors deep weights RBP mode communicating information learning channel completely bypasses need symmetric weights ﬁxed random weights instead However RBP possibility ones harnessing randomness learning channel Here derived variants RBP studied simulations mathematical analyses Additional variants studied followup paper 4 considers additional symmetry issues having learning channel architecture symmetric version forward architecture having nonlinear units learning channel similar nonlinear units forward architecture In combination main emerging picture general concept RBP remarkably robust variants lead robust learning RBP variants practical role digital simulations lead slower learning useful future better understand biological neural systems implement new neural physical systems silicon substrates In supervised learning critical equations principle deep weights depend training examples weights network Backpropagation shows possible derive effective learning rules form cid2wh j role lower network subsumed presynaptic activity j term O h1 j signal communicated deep learning channel carries information outputs j O h1 Ih ηIh j P Baldi et al Artiﬁcial Intelligence 260 2018 135 31 targets deep synapses Here studied kind information carried signal I h j simpliﬁed Table 2 The main conclusion postsynaptic terms 1 implement gradient descent layer random weights learning channel layer work 2 represents local derivatives activations deep layer h form f units layer h derivatives layers necessary F reasonable function error T O By reasonable mean function F linear composition linear propagation nonlinear activation functions ﬁxed slowly varying matrices involved random sparse As expected better matrices rank gracious degradation opposed catastrophic failure observed matrices deviate rank case Furthermore function F satisfy F 0 0 prevent weight changes error zero ﬁrst order approximation F linear biases F T O f cid3 cid3 The robustness properties algorithms explanations general principles We pro vided intuitive formal explanations properties On mathematical polynomial learning rules linear networks lead systems polynomial differential equations We shown cases cor responding ODEs converge optimal solution However polynomial systems ODEs rapidly complex results provided useful complete providing directions future research Acknowledgements Work supported NSF grant IIS1550705 DARPA grant D17AP00002 NIH grant NIH GM123558 PB NSF grant DMS1547878 ZL We grateful hardware donation NVIDIA Corporation Appendix A Proof Theorem 10 Assume A B F If near A B F discrete real analytic matrixvalued functions At Bt F small t 0 A0 B0 A B Moreover write At A t E t2 F t3 6 cid3cid3cid3 B 2 cid3cid3 A cid3 A G ot3 120 E cid7 0 We use A egy prove E 0 case E cid7 0 higher order derivatives reach contradiction denote A cid30 A cid3cid30 A cid3cid3cid30 B cid3cid30 B cid30 B cid3cid3 B cid3 B cid3cid3cid30 respectively The general strat cid3cid3cid3 It easy compute cid3 A 1 A cid3 1 B E B A B By taking derivative ﬁrst relations Equation 112 B E B B E BT E A T A E T 0 E E T B E B B T BB E BT 0 Let X E A T B E B Y E B E B B T 121 122 123 Then equations X Y skew symmetric Y A T X If Y cid7 0 orthogonal transfor mation scaling assume cid20 cid19 Y 0 1 0 1 Write cid19 cid20 b c d A Then Y A T cid20 cid19 b d c 124 125 126 Since X skewsymmetric b c 0 d Thus A aI real number cid7 0 As result K 2 a2I L 2a 1 a2 I 127 32 P Baldi et al Artiﬁcial Intelligence 260 2018 135 A B aI At Bt Since K L proportional identity At Bt F Now let write 1 I Let At Bt upper triangular matrices obtained orthogonal transformation At cid19 cid20 b 0 d Then equation B B T A A T K equivalent following 2a 2d ba 1 a2 b2 2a 1 d2 2a 1 a2 1 d 1 b d 0 1 a2 128 129 Since t small At suﬃciently close aI From second equation d If b 0 conclude ﬁrst equation At aI This implies At Bt A B So case E 0 Things complicated b cid7 0 We ﬁrst assume cid7 1 In case equation 2 distinct Thus case b zero If 1 d 1 1 Using ﬁrst equation b 0 At Bt At Bt A B conclude E 0 1 d 0 Since d cid7 1 suﬃciently small t d 2 From results know Y cid7 0 A proportional identity near A B F elements F F discrete When X Y 0 possible E cid7 0 However following Lemma 1 d Lemma 3 If X Y 0 A cid7 I E invertible matrix Proof By contradiction assume E invertible Then X 0 A E B B T E 1 By taking determinant sides det A det A detB B T Thus 130 131 det A 1 132 Since A similar negative deﬁnite matrix B B T eigenvalues λ1 λ2 A negative Since λ1λ2 det A 1 λ1 λ2 2 Using matrix representation Equation 125 d Tr A Tr B B T a2 b2 c2 d2 However 133 134 a2 b2 c2 d2 d2 b c2 2 d2 2 d 135 equality true b c d 2 Since λ1 λ2 2 λ1λ2 1 eigenvalues A 1 1 implies b c 0 Thus A I impossible assumption Next consider remaining case X Y 0 E invertible equal zero A proportional identity In case order derivatives reach conclusion By taking derivatives ﬁrst relations Equation 112 P P T Q Q T R R T S S T 0 T cid3 cid3 cid3cid3 P B Q A R B S A cid3cid3 A cid3cid3 B cid3cid3 cid3cid3cid3 A cid3cid3cid3 cid3cid3cid3 B A T A cid3 B T B cid3cid3cid3 B A T 3 A cid3cid3 B T 3B A cid3 T cid3cid3 B cid3 T A cid3 T 136 137 P Baldi et al Artiﬁcial Intelligence 260 2018 135 Similar relations matrices X Y Q A T P B cid3 cid3 B T A T A cid3 AT Since A B I cid3 B T A T B T A cid3 T Thus Q A T P B cid3 B T A T E A T B E B B T A T E A T 0 X 0 Since A proportional identity P Q 0 case X Y The relationship R S complicated computed idea We ﬁrst S A T R 3B cid3cid3 cid3 B T A T 3 A cid3cid3 cid3 T A Using Equation 139 fact P 0 S A T R 3 A cid3 cid3 A T B T A cid3 T 3E E T B T E T Since E invertible assume E cid7 0 E ξηT column vectors ξ η From fact Y 0 conclude ξηT BξηT B B T 0 Bξ cid10ηcid102 cid10B T ηcid102 ξ Thus compute E E T B T E T cid10ηcid104 cid13ξ ηcid14 cid10B T ηcid102 ξ ξ T S A T R 3 cid10ηcid104 cid13ξ ηcid14 cid10B T ηcid102 ξ ξ T If cid13ξ ηcid14 cid7 0 S cid7 0 Thus A T S 1 R 3 cid10ηcid104 cid13ξ ηcid14 cid10B T ηcid102 1ξ ξ T S 33 138 139 140 141 142 143 144 145 146 147 148 For matrix S S R skewsymmetric matrices S identical eigenvalues Let λ eigenvalue A 1ξ ξ T trace determinant zero So eigenvalues zero On hand 1 R proportional identity As result matrix A T A Tr A 2λ det A λ2 Taking trace ﬁrst relations Equation 112 1 cid10 Acid102 Tr K 2cid10 Acid102 Tr L 4λ 4λ λ 149 150 Thus ﬁxed K L λ cid10 Acid10 assume discrete values Since t small At Q t A Q tT orthogonal matrix Q t Let write cid19 cid20 cid19 cid20 A λ b 0 λ Q cid3 0 0 1 1 0 151 34 P Baldi et al Artiﬁcial Intelligence 260 2018 135 cid30 equal Then E A cid20 cid19 E b 0 0 b 152 By Lemma 3 E invertible Thus b 0 But b 0 A proportional identity case discussed We deal case cid13ξ ηcid14 0 Without loss generality assume cid19 cid20 cid19 cid20 ξ 1 0 η 0 1 By checking equation A E E B B T conclude cid19 cid20 2 0 d d 0 A 153 154 2 d d cid7 0 Again taking trace ﬁrst In fact t small eigenvalues At d relations Equation 112 1 cid10 Acid102 TrK 2 2d d2cid10 Acid102 TrL 2d2 2d 2d Therefore d locally uniquely determined K L Finally write At Q t A Q tT assume cid3 0 Q cid19 cid20 0 1 0 1 cid19 E 0 d d 2 cid20 2 d d 0 Since E singular d 1 A I This case covered proof Theorem 10 complete References 2016 i8i17 1 F Agostinelli N Ceglia B Shahbaba P SassoneCorsi P Baldi What time Deep learning approaches circadian rhythms Bioinformatics 32 12 2 P Baldi K Hornik Neural networks principal component analysis learning examples local minima Neural Netw 2 1 1988 5358 3 P Baldi Z Lu Complexvalued autoencoders Neural Netw 33 2012 136147 4 P Baldi Z Lu P Sadowski Learning machine symmetries deep learning channel Neural Netw 95 2017 110133 5 P Baldi P Sadowski The dropout learning algorithm Artif Intell 210C 2014 78122 6 P Baldi P Sadowski A theory local learning learning channel optimality backpropagation Neural Netw 83 2016 6174 7 P Baldi P Sadowski D Whiteson Searching exotic particles highenergy physics deep learning Nat Commun 5 2014 8 P Di Lena K Nagata P Baldi Deep architectures protein contact map prediction Bioinformatics 28 2012 24492457 httpsdoi org 10 1093 bioinformatics bts475 First published online July 30 2012 9 D Eisenbud Commutative Algebra View Toward Algebraic Geometry Grad Texts Math vol 150 SpringerVerlag New York 1995 10 K Fukushima Neocognitron selforganizing neural network model mechanism pattern recognition unaffected shift position Biol Cybern 36 4 1980 193202 11 X Glorot Y Bengio Understanding diﬃculty training deep feedforward neural networks Proceedings International Conference Artiﬁcial Intelligence Statistics AISTATS10 Society Artiﬁcial Intelligence Statistics 2010 12 A Graves AR Mohamed G Hinton Speech recognition deep recurrent neural networks 2013 IEEE International Conference Acoustics Speech Signal Processing ICASSP IEEE 2013 pp 66456649 13 S Han H Mao WJ Dally Deep compression compressing deep neural network pruning trained quantization Huffman coding CoRR abs 1510 00149 2015 14 K He X Zhang S Ren J Sun Deep residual learning image recognition arXiv preprint arXiv1512 03385 2015 15 D Hebb The Organization Behavior A Neuropsychological Study Wiley Interscience New York 1949 16 GE Hinton N Srivastava A Krizhevsky I Sutskever RR Salakhutdinov Improving neural networks preventing coadaptation feature detectors 17 I Hubara M Courbariaux D Soudry R ElYaniv Y Bengio Quantized neural networks training neural networks low precision weights arXiv12070580 July 2012 activations CoRR arXiv1609 07061 2016 18 DH Hubel TN Wiesel Receptive ﬁelds binocular interaction functional architecture cats visual cortex J Physiol 160 1 1962 106 19 Y Ilyashenko Centennial history Hilberts 16th problem Bull Am Math Soc 39 3 2002 301354 20 A Krizhevsky G Hinton Learning Multiple Layers Features Tiny Images 2009 21 A Krizhevsky I Sutskever GE Hinton Imagenet classiﬁcation deep convolutional neural networks Advances Neural Information Processing Systems 2012 pp 10971105 22 Y LeCun L Bottou Y Bengio P Haffner Gradientbased learning applied document recognition Proc IEEE 86 11 1998 22782324 155 156 157 P Baldi et al Artiﬁcial Intelligence 260 2018 135 35 23 Q Liao J Leibo T Poggio How important weight symmetry backpropagation Proceedings Thirtieth AAAI Conference Artiﬁcial 24 TP Lillicrap D Cownden DB Tweed CJ Akerman Random feedback weights support learning deep neural networks arXiv14110247 2014 25 M Riedmiller H Braun A direct adaptive method faster backpropagation learning RPROP algorithm IEEE International Conference 26 P Sadowski J Collado D Whiteson P Baldi Deep learning dark knowledge dark matter Workshop Conference Proceedings J Mach Intelligence 2016 pp 18371844 Neural Networks vol 1 1993 pp 586591 Learn Res 42 2015 8197 27 CE Shannon A mathematical theory communication III Bell Syst Tech J XXVII 1948 623656 28 CE Shannon A mathematical theory communication parts I II Bell Syst Tech J XXVII 1948 379423 29 S Smale Mathematical problems century Math Intell 20 2 1998 715 30 RK Srivastava K Greff J Schmidhuber Training deep networks Advances Neural Information Processing Systems 2015 pp 23682376 31 C Szegedy W Liu Y Jia P Sermanet S Reed D Anguelov D Erhan V Vanhoucke A Rabinovich Going deeper convolutions Proceedings IEEE Conference Computer Vision Pattern Recognition 2015 pp 19 32 J Zhou OG Troyanskaya Predicting effects noncoding variants deep learningbased sequence model Nat Methods 12 10 2015 931934