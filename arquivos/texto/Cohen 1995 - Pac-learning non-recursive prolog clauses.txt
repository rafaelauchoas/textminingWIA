ELSEVIER Artificial Intelligence 79 1995 138 Artificial Intelligence Patlearning nonrecursive Prolog clauses William W Cohen ATT Bell Laboratories 600 Mountain Avenue Murray Hill NJ 07974 USA Received August 1993 revised April 1994 Abstract ILP Recently logic programming increasing research learning concepts expressed subsets Prolog term inductive growing body research This paper seeks expand theoretical foundations ILP investigating patlearnability logic programs We focus programs consisting single functionfree nonrecursive clause focus generalizations language known patlearnable language determinate functionfree clauses constant depth We demonstrate number syntactic generalizations language hard learn language generalized clauses constant locality allowing patlearnability More specifically determinate clauses log depth regardless language represent hypotheses We investigate patlearnable effect allowing indeterminacy clause clauses k indeterminate variables hard learn DNP We restricted language clauses bounded indeterminacy learnable CNF represent hypotheses restricting locality clause constant allows patlearnability arbitrary indeterminacy allowed This result shown strict generalization previous result determinate functionfree clauses constant depth Finally present extensions results logic programs multiple clauses Keywords Machine learning inductive logic programming patlearning 1 Introduction Recently increasing research learning concepts ex firstorder description pressed logics 5295 I researchers logics While researchers considered concepts representation logics specialpurpose examples logic representation standard firstorder Email wcohenresearchattcom 00043702950950 SSDIOOO437029400034X 1995 Elsevier Science BV All rights reserved particular restricted subsets Prolog 737394046 The term inductive logic programming represent ILP language cepts growing body research learning systems Prolog semantics One advantage basing particular number previous com plexity mathematically understood This offers hope learning systems based rigorously analyzed A number formal results fact obtained results expand logic programs representational words wish determine logic programs This paper seeks patlearnability degree investigating In particular goal investigate carefully researchers derived 16212327 imposed certain practical systems necessary Valiants theoretical analyze existing 50 mode1 patlearnability representational ILP learning systems learnability boundaries foundations learnability restrictions In paper WC consider primarily logic programs consisting single function results obtain extensions clauses programs straightforward clause We focus single clauses multipleclause 10 161 We consider nonrecursive requires somewhat different free nonrecursive able learnability results single clauses analysis recursive programs note recursion important ods realworld problems 1 17303538 1 A final restriction knowledge allowed ground unit clauses practical analysis allow background theories known database model This restriction 374046 learning systems 8 IO forma1 machinery applications ILP meth background In paper define patlearnability learnability important purpose paper review previous results logic programs shows single determinate 161 We investigate language determinate We represent hypotheses language functionfree clause constant depth patlearnable number generalizations clauses logarithmic depth patlearnable regardless language language beginning constant depth We investigate indeterminacy indeterminate clauses We effect allowing clause obtain clauses single language predictable represent hypotheses Both results negative demonstrate series results free variable hard learn DNF slightly restricted clauses bounded kCNF apparently bounding indeterminacy Our result hard learn However arbitrary locality clause allows patlearnability patlearnable clauses concerns surprisingly indeterminacy indeterminate allowed reasonable languages clauses clauses bounded clause rewritten clause locality greater relative expressive power locality fixed sense reasonable language clauses bounded locality language iideterminate clauses ijdeterminate j ijdeterminate I Thus I strict generalization 3 These restrictions precisely defined 111 Sedan 2 5 NW CohenArcial Intelligence 79 1995 138 3 To summarize results obvious syntactic generaliza tions ijdeterminacy language clauses bounded fail produce patlearnable locality yield patlearnable results learnability languages generalizing language multipleclause non Finally state additional recursive programs discuss A number results related work conclude paper previously presented form formal issue 8912 The patlearnability recursive programs considered depth companion papers 13141 preliminary interesting 2 Preliminaries 2 I Logic programming In section overview logic programming As considering particular coincide usual ones case nonrecursive logic programs overview simplified accordingly singleclause reader Prolog programs For complete description logic referred standard texts 341 simple definitions functionfree programming Logic programs represent letters represent uppercase symbol Xl arity literal A fact written alphabet constant symbols represent letters like tl t2 alphabet predicate symbols like p q r alphabet variables letters A literal written p X1 Xk X variables The number arguments written p tl tk p arity fact letters 0 sets 0 Xl tl X t2 X t A result replacing variable X A 81 usually usually p predicate called k literal symbol tl tk constant predicate number arguments constant g necessary write literal use A0 denote constant 02 substitutions use A X mapped 8 extending denote symbols variables represent symbol Xi mapped A A fact f instance literal A substitution substitutions Greek function mapping variables If 8 substitution k A substitution ti constant symbols notation partial slightly symbol If 01 82 substitutions 02 Notice 81 general instances A01 superset set instances A 8 A0 F 81 C 02 81 general 02 literal A set Finally denite clause written AB A A Bl A Bl Bl literals A called head clause conjunction B1 A A B1 called body clause extension clause ABl A A BI respect facts f databasethen database DB set If DB set factswhich l fEDBor l exists substitution clause B E DB 0 AB f Bi body In case substitution clause For brevity let ertCDB database DB 8 proves f extension 01 extension C respect denote For technical reasons equali predicatethat constant assumption IO database polynomial f appearing convenient predicate assume database DB contains symbol equal equal ti ti E DB ti ti This predicate added DB eqqualt tj DB loss generality increase Readers familiar logic programming notice definition extension semantics Prolog programs size coincides usual programs considered Prolog programs ground background define tixpoint minimalmodel paper singleclause nonrecursive functionfree theory Hence succinctly extension clause C respect DB f C A DB t f Again familiar lustorder logic clause A B A t A B thought logical statement VX X TB V c lBl A I X variables XI clause respect statement conjunction DB simply facts DB appear clause Then set facts e follow extension logical Example If DB set DB mother annbob father bobjulie father bobchris extension clause grandmother X Y mother X Z father Z Y respect DB set DB U grandmother annjulie grandmother annchris Notice predicate prove additional adopted f X Y function f X Y true iff Y f X The general substitutions facts convention represented f f 1 grandmother annjulie fz grandmother annchris extension 6 X ann Y julie Z bob 01 X ann Y Chris Z bob WM CohenArtcial Intelligence 79 1995 138 5 22 Models learnability formal analysis learnability paper case functionfree course Our goal determine formal analysis subsets Prolog efficiently Prolog In section models pac learnable focus Any means basic models learnability learnability Pitt Warmuth requires explicit model slight modifications 50 polynomial efficiently language predictability nonrecursive Valiant learnable introduced introduced domain Define concept C X representation 441 Let X set called polynomially subset X language LANG set concepts Associated X LANG size complexity measures We write size complexity e E X IC 11 Ilel assume concept C E LANG instance measure represent C e We use notation X respectively LANG stand set elements X respectively LANG size complexity greater n In paper concept set represents casual risk confusion refer set represented concept C extension C number bits needed distinction related form One measure Example For example boolean variables normal vector measure C Thus xl AZ V FAxg instance let DNF language boolean let X domain binary vectors interpreted assignments disjunctive vector e E X length formula C number literals le 5 concept C e 00110 complexity complexity formulae lCl 4 An example C pair e b b 1 e E C b 0 distribution probability pair multisets S S draw n f rom domain X according positive examples C S containing basic learning models function If D sample C X drawn according D negative ones We define D Sf containing Definition 1 Polynomially predictable A language LANG polynomially predictable function m n n iff algorithm PACPREDICT polynomial n 0 n 0 C E LANG E 0 e 1 6 0 6 1 probability function D PACPREDICT following behavior distribution 1 given sample SS C f rom X drawn according m d n n examples PACPREDICT outputs hypothesis H D containing ProbDH C DC H E 6 probability PACPREDICT PACPREDICT taken possible samples Sf S randomized algorithm coin flips 2 PACPREDIC ples runs time polynomial 111 h II n number exam 3 H evaluated polynomial The algorithm PACPREDIC called prediction algorithm time LANG function m f II n called rampe complexity PACPREDICT predictability predictability We The condition abbreviate polynomial definition merely states low measured usually training examples drawn The second condition sample size polynomial polynomial The final condition weak sense predictions worstcase inputs learner error hypothesis probability distribution D stipulation time learner time Notice learning model definition allows adversarial choice hypothesis usable total running polynomial requires ensures simply The model polynomial predictability studied 44 weaker version Valiants 501 criterion putlrarnabilit Definition 2 Paleurnable rithm PACLEARN A language LANCE palearnable iff algo I PACLEARN satisfies requirements definition polynomial pre dictability 2 inputs S Sm PACLJXRN outputs hypothesis H E LANG Thus language Predictability patlearnable important predictable language words nonpredictable language adding additional language indicates strong negative result supersct language property shared patlearnability IS predictable converse need true In language predictable generalizing predictable restrictions Showing language sense expressive predictable learn efficiently On hand considered desirable ILP contexts logic programs polynomial output hy algorithm potheses patlearning output hypotheses algorithm Thus ideally like positive results given patlearning model negative paper possible use predictability prediction model In patlearning model polynomial results format desirable positive results arbitrary given negative prediction primarily results 23 Background knowledge extending rtundard models So far formalization standard However provides set examples background useful constructing hypothesis typical user theory defining set predicates ILP task learner WW CohenArtificial Intelligence 79 1995 138 7 logic program P P background data theory good model To account background learnability One way target concept 231 Dieroski models knowledge model entailed paper follow Haussler et al 161 closely related formalism directly allow examples necessary clauses 202745 extend However typical use background knowledge ILP systems set pairs form If LANG set definite clauses DB database denotes represents extension C respect DB defined facts e C A DB k e If DB set databases denotes called language family The set definite clauses LANG called clause language LANG DB C DB C E LANG Each pair set LANG VB set languages LANGDB DB E VB Such set languages Section 21ie In paper consider primarily learnability language families usual set notions patlearnability addition algorithms learning accept database training examples The following definitions new setting polynomial predictability input extend Definition 3 A language DB E DB prediction family LANG DB polynomially predictable iff algorithm PACPREDICTDB LANGDB A language nomially predictable time polynomial fixed DB prediction family LANGVB uniformly polynomially predictable iff poly algorithm PACPREDICTDB Sf S runs inputs PACPREDICT argument algorithm LANGDB The uniform patlearnability language family defined analogously Intuitively language family predictable predicted regardless family uniformly predictable single prediction database DB language algorithm Notice works databases PACPFEDICTDB S S size database DB Thus uniform predictability requires prediction learning algorithm run time polynomial pac scale size including inputs learnability background database DB Finally Most results let define aDB set databases containing facts arity paper following uniformly patlearnable l For fixed constant LANGDB forms LANGDB patlearnable Such result means clauses works database DB furthermore polynomial exponential positive allows adversary choice database DB learning algorithm time requires time require strong size database DB However arity facts database This known algorithm maximum learnability result LANG clauses l For 2 a0 a0 small fixed constant a0 3 LANG aDB predictable Such result means LANGDB representation clauses databases DB E aDB predictable patlearnable clauses regardless hypotheses This negative result learnability LANG The notions uniform patlearnability additional predictability odd allow adversarial extend standard models input learner The standard target concepts simply learning model worst case possible choices database At database DB target concept DB expressible extremely learning accurate hypothesis time sample complexity grow size target concept actually worst case databases DB target concept ILP setting database models worst case distributions lirst glance reasonable expressed predicates defined large concept quickly C E LANGDB appropriate We Thus model sense concise size database DB The parameters choice database This typically use II denote predicates defined required representation DB IZ 11 II measure requiring value keeping consider learner results sense polynomial size learning problem size measures casual reader easier different measures separate terms singlesize measure II lip nb n Example As example muternalgrandmother user provide database ILP learning problem learn predicate DB father charliewilliam mothercharliesusan father susandan father williammaurice mother susanruth mother williamcaroline father rachelmaurice mother rachelcaroline father elizabethwarren motherelizabethrachel examples S maternal_grandmother charlieruth maternal_grandmother elizabethcaroline S maternal_grandmother charliedan maternal_grandmother williamcaroline maternal_grandmother ruthdan maternal_grandmother mauricesusan In problem size examples IDEPTHDETERM users database DB 2DB size database nr 2 An ILP learning clause definition produce hypothesis Q IO language H maternaI_grandmother X Y tmother X Z A mother Z Y WW CohenArcial Intelligence 79 1995 138 9 If learning patlearning S S sufficiently guarantees user provides inputs S S database DB large drawn known sample complexity fixed distribution error rate E learner Note 24 Sample complexity learning logic programs In typical ILP problems examples predicate symbol p arity n effect predicate arity head target clause given One important fact note following Theorem 4 Let DATALoC language functionfree clauses head predicate symbol p arity ne Then jixed stant DB E aDB DATALOC DB n nt nb nb size DB VapnikChervonenkis polynomial nonrecursive dimension 4 Proof We establish upper bound number semantically different clauses A DATALOG clause size n contain n distinct variables head variables appear body n variables appear n antc possible clause heads Since nb database literal consists predicate predicates appear appear symbol fewer variables body clause bounds succeeds database DB Putting total number semantically nbn different clauses literals n nbnp antan The VC dimension bounded logarithm quantity n log nY log nb ah polynomial nb n nt 0 Blumer et al 4 concept class polynomial VC dimension sample size consistent hypothesis H minimal near size high confidence low error More specifically algorithm size certain polynomial minimal A outputs consistent smallest consistent hypothesis satisfy requirements Thus patlearningexcept time requirement following learner runs polynomial polynomial hypothesis DATALOG simple procedure satisfy requirements requirement uniform learning program increasing order consistent sample Since paper nonvacuous DATALOG clauses restrictions DATALOG means computational paper patlearnable considered ignored languages polynomialtime learning possible time enumerate patlearnability polynomial size return siders languages complexity The central question address clause 25 Constantdepth determinacy arid previous results input variables literal B variables appearing useful restrictions If A RI A A B ordered definite definite Bi Bi called possible A B_t variables appearing respect DB introduced Muggleton I37 1 Feng clauses clause appear clause ABl A output variables A literal B determinate substitution B_g E DB substitution literal binding input variables determinate liter determinate A clause g unifies A fact P Bla E DB BT E DB 13 B E DB Less formally output variables possible binding given DB determinate Informally determinate Next define depth variable appearing clauses evaluated backtracking clause ABl A head clause depth zero Otherwise interpreter A B follows let B variable V let d maximal depth input variables maxima1 depth Variables appearing literal containing Bi depth V d I The depth clause variable clause Prolog Example The clause maternal_grandmother C G mother C M mother M G assuming mother determinate variable M clause depth Assuming enclosedpaper length determinate functional clause The maximum depth variable predicates unwelcomemail E envelope E I enclosed_paper E P A mustreviewP length L 1 gt50 L A determinate variable L depth depth The variable P Irom clause depth An interesting class logic programs following Definition 5 ijdeterminate database DB E jVB A determinate clause depth bounded constant called ijdeterminate learning program GOLEM applied ijdeterminate programs Closely inductive logic programming number practical related restrictions systems including The problems adopted FOIL 173038 1 learns 471 LINUS 321 nonrecursive The learnability ijdeterminate clauses formally ied 161 For notation depth iDEPTHDETERM language jDB 1 One important family ijdeterminate clauses result following let iDEPTHDETERM language determinate stud clauses denoted W W CohedArtijicial Intelligence 79 1995 138 I1 Theorem 6 Dieroski Muggleton guage family iDEFTHDETErw jZB uniformly patlearnable Russell 161 For xed j fan worse _ shown patlearnable single clause NPhard negative condition hold specifically Other previous work established clauses fixed depth language determinate ijdeterminacy clauses language indeterminate 271 The proof facts based showing arbitrary depth patlearnable sets examples finding single clause language consistent examples Unfortunately learning consistent examples Most ILP learning clauses single clause results learning expressive times called representationdependent 2 One goals paper positive representationindependent orem 6 These analytic importance output single clause learn set results develop result The results limited practical required complement shortly results developed tool obtain systems hard Such negative intractable representation learner results learnability learning results 26 Reducibility prediction problems Pitt Warmuth problems analogous method showing 44 introduced notion reducibility notion reducibility decision problems prediction commonly prove problem NPhard Predictionpreserving reducibility essentially language harder predict reducibility Let LANGE language Definition 7 Predictionpreserving main Xi LANG2 language domain X2 We predicting LANGi reduces predicting LANG2 denoted LANG 9 LANG2 fi Xi f X2 henceforth henceforth instance mapping function LANGE LANG2 function called called 1 x E C fi x E fc C concept mapping following hold concept membership preserved fc mappings size complexity size concept fi x computed 2 3 fcC polynomial size complexity Cie representations preserved polynomial factor polynomial time Note fc need computable fi computed polynomial time fix preserve size polynomial factor Intuitively fc Ci returns concept C2 f LANG2 decisions concept membershipon emulate Clie prepro examples The prototypical broader setting hypotheses learning richer language kCNF 421 example learning problem learning kterm DNE Assuming hard representationdependent setting RP NP patlearning kterm DNF intractable kterm DNF tractable hypotheses expressed scheme possible LANG exists predicting LANG2 fi If predicting LANG reduces cessed function learning algorithm learning concepts LANG following First convert examples unknown cept Cl domain Xl examples domain X2 instance mapping f If conditions definition hold Cl consistent original concept fc Cl consistent image f running examples LANG produce hypothesis H good learning approximation map H f difficult impossible However original Cl given example x original H predict membership domain Xl simply predict language LANG computing f Cl Of course E Cl true possible algorithm Pitt Warmuth algorithm prediction 43 rigorous argument following LANGI leading fx E H approach theorem leads Theorem 8 Pitt Warmuth Asrurre rhar LANG 9 LANG Therz following hold l Jf LANG polyonliall l f LANC rrot polytmnial predictuble rhrr LANG 1 polyomially predictuble predictable tlzetl LANGE polynomially pre dictable The second case theorem allows useful language prediction secure The case theorem gives means obtaining LANG given prediction hard breakin g cryptographic LANG schemes number languages transfer hardness results known prediction algorithm widely assumed If f onetoone ible patlearnable preserving case shown For example reduction reduction LANG patlearnable said bc invert LANG proof Theorem 6 based invertible prediction Ijdeterminate clauses monotone monomials algorithm f _ computable 3 Logdepth clauses hard learn In sections investigate models described patlearnability point Theorem Theorem 6 generalizing corresponding languages 6in learnable polynomial particular consider generalizing learnability definite clauses predictability Our starting result ways seeing definition ijdeterminacy relaxing restriction list clauses We consider 371 argue practically useful programs limited clauses constant depth Mug depth case complex clauses greater depth It clause depth d slowly size measured clause size q database size gleton Feng frequently plausibly argued reasonable growing fq example size II provide examples function problem argument support assume WW CohenArtificial Intelligence 79 1995 138 13 output I I I Y2 I Y5 I OR Y4 I OR I Yl NOT AND I Xl J x2 L x3 Y3 OR J L x4 X5 circuit Xl X2X3X4X5 A notXlYl X2X3Y2 A X4X5Y3 A A orYlY2Y4 orY2Y3Y5 A Y4YSOutput A true Output Fig 1 Constructing determinate clause equivalent circuit The key result section increasing depth bound logarithmic size examples makes determinate clauses hard predict Theorem 9 For constant 3 language family log n DEPTHDETERM aVB polynomially predictable cryptographic assumptions 3 boolean circuits Proof The proof based predictionpreserving depth d determinate language depthd boolean circuits n binary variables containing AND OR NOT gates usual semantics complexity measures 4 We fanin exists database DBcrn E 3VB containing atomic facts clauses depth d Let dCIRcurr reduction dCIRCUIT dDEPTHDETERM DtB 1 The expressive power depthbounded 51 particular studied predict cryptographic assumptions immediately reduction Theorem 8 boolean circuits learnability known logdepth circuits hard follows 25 Theorem 41 Thus theorem The construction language binary vector bl b converted instance mapping Fig 1 An example reduction illustrated circuit s More precisely prediction problem intractable quadratic problems widely conjectured residue problem inverting RSA encryption intractable following intractable function factoring Blum integers solving 251 These 4 Specifically complexity assignment extension circuit C set variable assignments instance number inputs C outputs l length binary vector representing inputs complexity circuit number gates circuit j atom form circuit 111 I 1 For example converted functions und definition unary predicate I I succeeds vector 1001 I circuit I OO I I The database DB CIR contains definitions boolean true argument DBR utld 0 OJ CZH 0 I 0 00 O or0 I 1 tot 0 I td10Oc7ttrl11Ior1I01r111 tzotlO true I Finally concept mapping gate C circuit single f 14 indicated literal L single output variable F defined figure To precise ut7d Zil z2 ib AND gate L E orZiZzK G OR gate tzotZl K G ib NOT gate Assume inputs cast Z variables correspond loss generality gate G G ordering numbering clause fcC simply Gi inputs G puts f c s citruif XI X 1 Notice construction preserves depth The algorithm presented Muggleton Feng learning single ijdeterminate depth clause The result shows bound depth The result holds learning systems clauses exists improves representation hypotheses systems approximate determinate doubly exponential clause learning algorithm singly exponential USC alternative clause Recent work shown drawn uniform distribution tions distribution predictable logdepth circuits arc hard predict examples fairly strong assump clauses examples logdepth determinate 126 Thus making 4 Hardtolearn indeterminate clauses The results Section 3 indicate iideterminate clauses increasing likely able generalize depth bound We consider class relaxing This exe requires additional cryptographic assumptum solving II x II Itc subset sum hard W W CohenArtificial Intelligence 79 I 995 l38 15 second key aspect ijdeterminacy determinate While problems determinacy realworld problems determinate able relax restriction literals appropriate background knowledge restriction condition clauses restriction accessed 111 practical reasons useful plausible ways lead languages In particular consider bounding relax determinacy hard patlearn way leads In section consider restriction relaxations cases hard predict depth clause restricting language variables predict DNF We consider alternative indeterminacy patlearnable indeterminate bounded clause language set restrictions language hard predict We consider bounding number free exactly hard degree predictable 41 Constantdepth indeterminate clauses The obvious way relaxing determinate Unfortunately clauses hard learn Letting DEPTH denote following result ijdeterminacy consider constantdepth leads language language clauses depth k family Theorem 10 For 3 k 3 1 language predictable NP C PPoly family kDEPTHaDB 47 shows language LANG polynomially Proof Schapire C E LANG emulated polynomialsized C E IDEPTHDB construct polynomial testing membership circuit To prove theorem sized database DB E 3VB C NPhard sufficient predictable following predicates Let DB contain l The predicate booleanX l For k l n predicate true X 0 X 1 linkkMvX true M E n n V E 0 l X E 0 1 following conditions 1 M E l holds M k X V M M k M k k X TV X V values l Finally predicate satV1 V2 V3 true K E 0 1 VI V2 V equal 1 Now consider 3sat formula 4 Aa li V li V li n variables x1 rn We encode formula following arity3n atom mi k Ii xk mi k li q Now consider clause CSQ W W CohenArfificul lntelligencr 79 I 995 I3X SUtMIMlz3Ml MMM A boolean Xk A LI possible values variables xr assigned values introduce The sets literals XLS correspond xs correspond 4 The set literals ensures V Xk complements consistent assignment ensures 4 satisfied variables sets depthl indeterminate x 4 defined literals I x1 v Xk l z Vs values literals conjunction 4 succeeds l appear ensures ss Finally conjunction values given clause Thus conclude 4 satisfiable iff clause Csr succeeds instance e determining CST succeeds NPhard Finally notice boolean predicate requires seven facts define linkk predicate 8n facts define IDBJI bounded polynomial II This completes link predicates proof requires facts define sat predicate requires 2n 2 2 require 8n2 facts define Hence II It clear CSU size polynomial 9 Therefore remainder section consider learnability guages strictly restrictive clauses bounded number free variables kDEPTH We consider learnability lan 42 Clauses k free variables Let free variables clause variables clause head One reasonable small number free variables This restriction Haussler restriction 231 appear impose body consider clauses imposed analogous We consider learnability recursive clauses containing necessarily depth k restricting clauses evaluated simple notice polynomial language kFREE defined clauses non FREE time While glance language true number free variables ensures clause pX q X Y classifies example pa k free variables Notice WE CohenArtcial Intelligence 79 1995 l38 17 Database fori lk trueiby falsei b y b y b 0 y E 1 r y forallbybloryElrbutyi DNF formula qAUjAu4 v EATg v U Auq Equivalent clause dnfCX2X3 A falselX3Y A truelX4Y A truelXlY false X2Y A false2 X3Y A truesXlY A false3QLY Fig 2 Constructing indeterminate clause equivalent DNF formula exactly qa 61 E DB V V qa b E DB bl b possible variables allow bindings disjunctive expressed single clause indeterminate variable Y Thus indeterminate encode boolean expression suitable database This leads following expressive power indeterminate disjunctive normal form 6 single kfree clause theorem free variables concepts As turns exploit Theorem 11 For constants kFRm aVB predictable DNF predictable 2 2 k 3 1 language family Proof As Theorem 9 statement theorem reduction follows directly single Lemma 12 Let rTlItMDNF database DB size polynomial denote language rterm DNFformulae There r rB_NF 1FREEDBI To theorem DNF formula n n terms DNF formula fewer n DNF exactly n terms adding terms form ulK lemma notice follows complexity terms padded size n predicted lfree clauses database DB Proof The construction DB contain sufficient atomic facts define binary predicates false behave follows reduction based illustrated Fig 2 Let true1 falsel true l trueiXYsucceedsifXlorifYEl l falseiXY succeedsifXOorifYEl ilil ilil r r 6 Recall boolean formulae form Vi Aj lij said disjunctive normal form We denote length number literais number variables size measure formula examples size measure assignment DNF language bit vector encoding contains Since 2r 1 facts required OG detine predicates total size DB We define instance mapping f map assignment 01 map formula form b atom drfc 0 O The concept mapping f defined clause f4 drfX xA I I Lit dclined true x Y fulse x Y il l L I I Lir Z fe Clearly polynomial size reduction polynomial fd size J 4 respectively DB Next notice fcd clause fc 4 d true assignment succeed bound variable Y appearing head r values 1 r Thus 111 b term T rT I true Y bound case ria Liti succeeds succeeds Y bound T fails possible hinding Y conjunction A Liti fail Thus concept membership value rL Lf On hand 4 false assignment preserved mapping q It noted rterm DNF expression patlearning kterm DNF clause 111 IFREEDB Lemma 12 existing hardness translated results 241 leads following result Observation 13 For 2 languagefLmily IFREE aDB patlearnable It straightforward obtain number similar representationdependent clauses results patlearning 271 However hardness rem 1 Kietz limited results We turn instead harder learn DNF The answer FREE somewhat lines Theo learning DNF hard given Theorem 1 I develop FREE question languages question accepts conjecture Theorem kFREE UDB 14 v DNF predictable fijr ull constants k language family uniformly predictable It suffices Proof DB E NDB constants 1 k background theory KM CohenArtial Intelligence 79 1995 138 19 kFEEEDB _a DNF reduction holds use hypothesized prediction Below reduction algorithm arbitrary DNF predict kFEEEDB database DB Let C clause determined contains equality predicate assume C distinct7 Thus head C determined positive examples assume kFEEE DB The predicate symbol arity head C DB head variables examples Notice clause n k variables variables serve arguments database DB size nb Since database DB contains nb n kO possible literal Let nb literals Bi Bnbkp n k atuples background predicate appear symbols Now let C AtB body kFREE clause A A B clause kFEEE DB Recall C covers example e iff exists substitution 8 BTEDBAABBEDB 1 theory DB size nb predicates 0 general substitution ground unb constants k free variables Thus let introduce DB AB e However arity amF anb k possible substitutions 01 boolean variables Uij ranges nb nk literal j ranges anbk represents substitution represents Notice size set variables fie instance mapping follows uij true Q Finally DNF formula let concept mapping example e return assignment Q polynomial n ne We define variables BigjO E DB dc defined fc C map clause C AB A A B anL2 1 fcC E V 1ucj jl il Since polynomial proof 0 1 polynomial unbk size It verified mapping preserves concept membership This completes fc C true exactly Eq 1 true reduction n nb nt formula fcC The predictability theory years Thus result actually settle question DNF open problem computational learning More precisely equivalent clause C constraints larger represented conditions C target clause C variables head C distinct necessary equality body C It easy C need polynomially head distinct variables 20 W W CohenArfd Intellrgmce 79 1995 l38 indeterminate require substantial theoretical advance clauses predictable answering question 43 Clauses bounded indeterminaq If believes DNF hard predict suggested result clause suggest possible restriction indeterminacy needed emulate bounding lead predictable predictability introduced language restrictions lead learnable based observation result negative languages The degree closely related number terms DNF formula fixed k Hence associated clause kterm DNF predictable number possible language Such result useful decreases gradually learnability indeterminacy substitutions intuitively investigate particular In section It turns intuition language certain correct result learnability clauses bounded weaker model predictability We present fairly general version result consider concrete result Theorem 6 extended This gives positive general result restriction indeterminacy instantiations 43 I Bounding indeterminacy u clause We want talk clauses deterministic following definition Definition 15 Effectively tively kindeterminate dure SUBST e DB having kindeterminate respect X A language LANG DB iff polytime given e E X computes set substitutions computable called eflec proce 01 0 following properties I bounded k l number substitutions l C E LANG DB e extension C general set 0 proves e extension C included substitution 0s generated SUBST Note duplications allowed 8i 13 loss generality I k j assume language Informally kindeterminate substitutions small set candidate necessary As example language lindeterminate prover Some additional SUBST implemented examples given Section 432 single substitution generate suffice theorem proving ijdeterminate given instance e produce clauses effectively theorem Prolog style proves e extension C The following property important Definition 16 Polynomial nomial literal support literal support A language iff X DB E VB family LANGZB poly set literals LIT WW CohedArtijicial Inrelligence 79 1995 l38 21 partial order LIT LIT polynomial l cardinality l LANGDB n IIDBII exactly clauses AtBl A A B A fixed Bi members LIT body clause satisfies BiBj Bj body clause clause appears left Bj following restriction Bi body One example language polynomial case polynomial determinate clause obtained simple counting argument relationship clauses literal support bound number literals 371 ordering language ij function Bi5Bi iff input variables Bj bound Bi key property fact generalizes clauses patlearnable case ordering This definition ijdeterminate literal support ensure The example kfree clauses shows polynomial ensure clauses linked learnability determinism makes The language kfree clauses polynomial function function constant way reduce indeterminacy literal support sufficient The principle result section restrictions cult extend Section 433 yields predictable predictability result theorem shows imposing diffi result This issue discussed language clauses Unfortunately patlearning Theorem 17 Let kINDETELS support family kINDETERMPLS aVB literal effectively kindeterminate Then jixed language language polynomial clause uniformly predictable Proof The proof analogous learning DB E aDB kINDETERhLS clause proof Theorem 14 reduce kterm DNF expression learning kINDETERMPLS aDB kRMDNF kterm DNF predictable theorem The follows kCNF hypothesis language Since immediately space kINDETERhLS polynomial 421 set literals B1 B clause C language written C I A B As introduce set variables Ucj ci ranges AB 1 n encodes literal B j ranges 1 k encodes substitution instance mapping map example e assignment Q literal support kn The follows First variables effective kindeterminacy ordering substitutions Q constructed procedure SUBST e DB guaranteed generate set k substitutions definition 81 ok The arbitrary shortly An assignment Ucj true Bj E DB 0 22 W W CoherrArtficictI Itttelligewr 79 I 995 I 3X Finally define concept mapping f map clause C AtB A A B kterm DNF formula Note clause C covers example e 0 makes clause true terms jC true conversely C doesnt cover e terms f C true So mappings preserve concept membership Notice different different examples ordering 0 irrelevant E 432 Languages satisjying restrictions Although result stated generally restrictions enforce support devise natural syntactic polynomial language kFREEDB clauses databases constant databases size equal I suggested proof Theorem d t jlDBjlk In e erminate effectively nonetheless key restrictions 14 shows Thus language difficult clauses One possible language family kfree set size I predictable Thus letting VB denote effectively kindeterminate Observation 18 For jixed k 1 tlze language predictuble jbmily kFREEVBl uniformly Note time complexity natural prediction algorithm predicts kterm DNF kCNF practical algorithm Also severe restriction restricting 0 11 high size background database Another possibly useful way defining language meeting restrictions follows l First specify tuple II variables The head clause language arguments For instance fix arguments learning tuple T l Next specify small set output output function clause Continuing head clause specify example given family relationships like grandfather nephew variables X Y order literals S L L ordering literal d possible bindings manner consistent X Y arguments following set c 6 output literals S LI parentXAL2 parentyB L3 parent A C Ld pnrent B D L5 spouse X E L6 spouse P F following ordering s WW CohenArtificial Intelligence 79 1995 138 23 L2 w4 ordering Given bindings assuming binding set S d 2 constraint person parents literals Ll L2 L3 L4 literals L5 L5 assuming person spouse Thus l Finally define language SOUTPUT list T bodies argument set S order consistent set clauses contain output literals selected heads SOUTPUT generating It easy clause procedure free variables substitutions arity language SOUTPUT polynomial free variables Hence substitutions simply effectively ISI dindeterminate generate possible literal set S Also databases fixed literal support alS backtrack Observation 19 For SOUTPUT aDB S d bindings uniformly predictable provided constants c d ISI c literal set S literal The time complexity kCNFbased It noted priori way choose literal set S ordering language SOUTPUT requires additional user user function 4s Thus practice specifying input For example specify learning problem given examples background database DB family addition Onecd relationship prediction algorithm l pair variables X Y appear l set indeterminate literals S parent A X appear hy head hypothesis clause pothesis clause l ordering In respect FREE require function 4s clause language SOUTPUT differs iDEPTHDETERM little user input specify This result generalized somewhat One generalization clauses polynomial literal support ijdeterminate combine new predictable language ijdeterminate clauses language SOUTPUT language clauses form It possible based fact obtain AtBl A ABADkAAD A A B ijdeterminate AtBl provides way introducing ijdeterminate clauses making prediction intractable ADl A A D SOUTPUT This result language small nondeterminism 433 Further discussion It emphasized reasons result positive result weak First shown number language kterm DNF hard patlearn result learning reasoning result appears difficult predictable extend disadvantage target clause This way obtaining clause ultimate goal based logic programs concept second kterm DNF easily reversed The kterm DNF yields 31 learns kterm DNF general DNF classes example example logic programs patlearnability prediction algorithm learnable learning approximates patlearnable accurately integrate Furthermore reasons mapping reduce clause hypotheses describes algorithm distributions impossible polynomial literal support fact means patlearn clauses effectively kindeterminate kterm DNF directly easily converted 331 language A second problem known algorithms time exponential tolerated section different k This suggests imposing additional restriction indeterminate clauses small indeterminism restrictions For reasons consider predicting kterm DNF require 5 Learnable indeterminate clauses 5 I Highly local clauses leutruble We consider alternative language indeterminate patlearnable restriction indeterminate aim predictable clauses clauses The construction natural question makes indeterminate general idea develop ask limiting Lemma 12 requires free variable clauses easier learn This restriction unfortunately appears literal number occurrences free variable help related restriction learning easier The basic limit length chain linked variables closely restriction notion formally Definition 20 Locale Let Vi Vz free variables clause ABl A A B We V touches V2 appear literal V inuences V2 touches V touches variables V influences t The locale variable V set literals Bi Bi contain V variable influenced V appearing Thus injbences touches symmetric reflexive relations injuences transitive closure touches Informally variable VI influences variable V V2 choice binding testing ground fact e extension C The locality clause possible choices bindings VI affect The problem database contains equality predicate variables copied arbitrary number times W W CohenArtificial Intelligence 79 I 995 I38 25 size largest set literals illustrate locality influenced free variable The following examples Example free variable underlined In following clauses free variables highlighted locale father E S son S F A husband F W no_payment_due S tenlist S PC A peace_corpsPC draftableS ccitizen S C A unitedstates C A ageSA A A 2 18 A A 26 clause influence variable S influenced C age S A locale relation applies free variables Notice c Finally let locality clause cardinality largest locale free language clauses locality k clause let LOCAL denote variable The principle result section following Theorem 21 For anyfied k languagefamily kLOCti aVB patlearnable uniformly new variables A n distinct variables As new literal labeled klocal clause ABl A A Bl As predicate symbol arity A known body size k locale contain n ak database locality predicate symbol arguments nbn ak different appear locality p nb n akO k different 9 localities LOCI LOC Note Proof Let Sf S sample proof Theorem 17 assume arguments introduce distinct variables Also note nb distinct predicates DB Since literal literals localities length k Let denote constant k number distinct n ak variables localities p polynomial n nb Now notice clause C locality k written form ALOCi 3 s LOCi p possible LOC Since free variables locales interact e E exrCDB locales free variable appears shared locales exactly e E In words C decom use form ALOCij One Loci different extALOCiDB posed Valiants 50 conjunction technique e E extALOCi DB components monomials learn C 9 Up renaming variables In bit following algorithm patlearn klocal clauses The learner initially hypothesizes specific klocal clause ALOC LOC The learner esis LOC e exf ABLOC DB 3 DB E LOCOru 8 general substitution checked To condition ak free variables Ok substitutions Valiants procedure examines positive example e In turn deletes hypoth Note e extension exactly AB e time recall u contain DB contain constants argument 0 Following target concept g need checked polynomial algorithm patlearn polynomial Again algorithm result bc extended somewhat language clauses form example patlearning ABl A B ijdeterminate AD A A D klocal 52 The expressive power locul clauses Theorem 21 positive result shows klocal clauses efficiently learned formal model The importance result depends great clauses correspond usefulness locality klocal clauses representation ijdeterminate typically tasks In section attempt evaluate list manipulation language note logic programs reasonable deal usefulness klocal clauses unlike sorts clauses programming bias 521 Experimental results clause empirically language Among applying learning 1 I 1 In experiments One way evaluate bias different versions experimental uses experiments sort reported ILP learned programs clauses ij clause klocal clauses These different versions Grendel compared literature These experiments problems simple useful problems notably list However benchmarks bias benchmark problems Some preliminary Grendel constructed different determinate set benchmark confirmed ijdeterminacy recursive programs significantly imposing important cases useful way restriction results suggest indicate better results obtained discarding instead locality relax restriction Thus restriction like append considered determinacy determinacy locality learning languages taken WW CohenArtcial Intelligence 79 1995 138 21 522 Locality generalizes ijdeterminacy locality usefulness formally analyze languages For instance A second way evaluate ex pressive power klocal clauses The easiest way comparing LOCAL k clearly depth k 1 kLOCAL restriction language clauses constant language clauses depth However bounded construction Lemma 12 lengthn locality n similarly number free variables To note language kLOCAL incomparable clause locality clause single free variable clause locality n free variables To summarize k kLOCAL g kFREE kFREE g kLOCAL kLocAt_ c k lDEPTH A interesting question clauses Clearly clauses ijdeterminate depth unbounded klocal clauses include relationship klocal clauses indeterminate It case determinate ijdeterminate literals klocal clauses bounded locality As example consider clause pXsuccessorXY AqlY AAqY surprising However ijdeterminate bound locality language clauses constant determinate More precisely clauses constant depth following relationship languages clause rewritten clause bounded relationship function j Thus reasonable locality strict generalization turns locality sense language holds languages For aDB Theorem 22 dDEErHDETERM DB clause C E kLOCAE DB C equivalent C IlCll kllCl k ad DB clause C E d E Proof Let C AcBl A A B clause We literal Bi directly supports literal Bj iff output variable Bi input variable Bj literal Bi indirectly supports Bj iff Bi directly supports Bj Bi directly supports Bk indirectly transitive closure directly supports supports Bj Thus indirectly supports Now Bi body C let LOCi conjunction LOCi Bj A A Bjk A Bi Bj literals C support Bi directly indirectly order appeared C Next let introduce appearing 1 r substitution vi Y x Y variable occurring LOCi A 28 W W CohenArtijicid lntelltgence 79 1995 138 The determinate clause C Below learned actual problem clause t more_active DrugADrugB functionfree 301 version depth2 determinate structure DrugAXYZ A not_equal_toh X A polarity YP A equal_to2 P A structure DrugBTUV A equal_toh V B B2 Bx B4 B5 Bb The support relationships BI directly supported literals B2 directly supported BI B3 directly supported BI B4 directly supported B3 indirectly supported BI B5 directly supported literals B6 directly supported Bs The phase construction LOCI structure DrugAXYZ LOCz structure DrugAXYZ Anottequal_to_h X LOC3 structureDrugAXYZ LOC4 structure DrugAXYZ Apolarity YP Aequal_to2 P LOCs structure DrugBTUV LO structure DrugBTUV Aequal_toh V Apolarity YP The constructed clause C After renaming free variables appear clause obtain following clause C single conjunction variables conjunctions single ZI 1 P more_active DrugADrugB structureDrugAXIY structure DrugAXzYzZz structureDrugAXsYJZI structureDrugAX4Y4Apolarity structure DrugBTSUsVS 1 A Anotequal_toh ApolarityY3Py collecting X2 A A Y4P4Aequal_to_2P4 A StIUCtUle DrugBT6UsVg kqldtOhV6 Fig 3 Constructing local clause equivalent determinate clause We define LOC LOCiai copies LOCI LOC variables LOC different clause effect step LOC variables renamed free variables LOC Finally LOC free let C AtLOCi A A LOC An example construction example point given Fig 3 We suggest reader refer C klocal We claim furthermore C determinate remainder proof establish k udi C k times C extension claims size C C In W U CohenArtiial Intelligence 79 1995 138 29 claims sufficient To establish C k ad equivalently maximum number literals depth d Clearly function Nd LOCi upper bound k C klocal k times number literals let define Nd size LOC Bi input variables bounded k To establish LOCi corresponding DB E aDB C E dDEPrHDETERMDB The function Nd bounded following lemma Lemma 23 For DB E aVB Nd c ad Proof By induction d For d 0 literals support Bi locality LOCi contain literal Bi N 0 1 Now assume depth d Notice lemma holds d 1 consider literal Bi inputs LOCi larger conjunction A iL3j directly supports B LOCj A Bi literal Bj sup ort Bi Also directly Nd 1 cjl dpl directly supports Bi depth d 1 input variables Bi different Bj Putting inductive hypothesis NdaNdlla By induction lemma holds 0 Now consider second claim determinate C C constructed extension The direction equivalence clause C actually holds Lemma 24 If fact extension C respect DB f extension C respect DB f Proof We wish f E ext C DB ing literals f E extCDB body clause change f E extCDB f E ext C DB Since duplicat sufficient extension C ALOCl A A LOC Consider variable substitutions introduced LOCi given distinct C Since construction LOC ui onetoone mapping free variables LOG distinct substitution u Ub ai free defined As example clause C Fig 3 u X x x2 x x3 x xj x Y xY2xY3yY4X z z z z z z ZJ z P3 p P4 P i5 T T6 T lJiJiJiJyvv It easy applying renaming variablesie Cu C substitution C simply undo effect Now assume literals literals f E extCDB substitution body clause CH DB Clearly substitution body clause CB DB f E ext C DB definition 8 8 TO 0 0 We finally establish converse Lemma 24 This direction equivalence requires C determinate Lemma 25 If u jt rmte theta f extension C nith respe1t DB f extetlsion fC respect DB arid C determi C determinate C determinate easily proved picking Proof If f E ext C DB 0 proves Let define variable Y C copy Y E C Y renaming Y Y Y rrl defined Certainly easy determinate C 0 map copy Y constant ty This copies Y 5 Y induction depth Y bound constant true copies For variables depth d 0 statement literals B B depthO variables For variables depth d 0 consider contain Y I output variables apply input variables bindings C shows Y Yi bound determinism inductive hypothesis constant vacuously Hence let define substitution H Y ty copies Y C bound rr 0 I r LOCH LOCH Clearly 0 proves f E ext C DB El 0 proves f t extCDB Proof Theorem 22 continued We established bounded size equivalent C This concludes proof theorem 0 C klocal We note proof technique Theorem 22 similar Dieroski Muggleton Russell particular Dieroski Muggleton rewritten conjunction LOC conjunctions 161 ijdeterminate Russell showed ijdeterminate clauses learnable clause corresponds closely boolean propositions LOC introduced proof Finally shown ijdeterminate constant observed constant clauses locality bounded example fairly large NW CohenArtcial Intelligence 79 1995 l38 31 j 3 bound locality k 34 81 Hence ijdeterminate Theorem 21 need best algorithm learning algorithm clauses 6 Extensions multipleclause programs So far results programs containing consider extending clause This important containing multiple clearly useful clauses results presented programs topic practical understanding limitations single clause We contain learn programs systems systems Still considering programs fixed database immediate nonrecursive learning arbitrary severe restrictions hard We use proof usual semantics logic program logic programs cryptographically 341 result Theorem 26 For 1 language nonrecursive multipleclause taining clauses cryptographic following assumptions language families polynomially predictable programs l 0DEPTHDETERM aDB l 0FREEaZB 0 OLOCAL aDB This true background database restricted contain single fact straightforward Proof The proof reduce predicting learning multipleclause OR gates lo The instance mapping circuit program P follows boolean circuit program We assume ILP adaptation proof Theorem 9 case learning problem circuit contains AND Theorem 9 The concept mapping maps l For AND gate Gi program P contain clause piXlt Xnh A L2 Lit Liz defined follows true jth input gate Gi variable Xk j pkXlXn jth input gate Gi output gate Gk l For OR gate Gi program P contain clauses X1 h PiXIXnb2 Lit Liz defined analogously lo This possible force NOTs negations input variables constructing new variables equivalent Z eliminate Yi reduction repeatedly applying LkMorgans NOT gates instance mapping laws circuit introduces n W W CoherfArrficid lntellenw 79 1995 I38 output AND I gs I I I g3 OR I OR g4 gl AND L t OR g2 J L circuit kll X2X4X5 ps Xl X2X3X4X5 3 s iX2Xii4XS pXlX2X3X4XS A pJX1X2X3X4X5 p4 X 1 X2X3X4X5 pj X1X2X3X4X5 1 X 1 X2X3X4X5 pi Xl X2X3X4X5 x Xl X2X3X4X5 p2 Xl X2X3X4X5 pr X 1 X2X3X4X5 f _p p r pj XI X2X3X4X5 pr X 1 X2X3X4X5 trueX1 11 X I X2X3X4X5 truc X4 true X5 true X2 A true X3 Fig 4 Constructing logtc program equivalent circuit l Finally P contains single clause ckGr XI x _I x X pn gate output output circuit An example construction shown Fig 4 It easy verify construction clause programs types named trUe I 0 reduces learning circuits learning multiple theorem database DB logic programs hard lcarn henceforth heads clauses restriction restrict program predicate learning 6374046 We programs multipleclause predicate Jinitions Since arbitrary cases symbol arity systems For case predicate definition Ck extension P respect database DB union extensions C respect DB Again number practical coincides usual semantics simply set clauses P Cl simple multipleclause representation nonrecursive semantics remains Many preceding results extend immediately multipleclause nitions completeness state extensions programs predicate defi W U CohedArtijicial Intelligence 79 1995 l38 33 Observation 27 For Z 3 language multipleclause predicate definitions taining clauses log n DEPTHDETERM aDB polynomiallypredictable cryptographic assumptions This follows directly nonpredictability single clauses Observation 28 For b 2 k 2 1 language multipleclause predicate dejnitions containing clauses kFREE aDB uniformly predictable DNF predictable This follows directly set DNF formula DNF Theorems 11 14 fact disjunction Observation 29 Let kINDEIERhIS clause language polynomial literal support containing effectively kindeterminate clauses Then Ned k 1 language multipleclause predicate definitions containing 1 clauses kINDETERMPIS aDB uniformly predictable This follows directly Theorem 17 fact union 1 distinct kterm DNF formulas 1 constant k lterm DNF formula extensions Theorems clauses klocal clauses respectively Again It remains consider ijdeterminate reduction appropriate based previous boolean monomials set boolean learnability sions straightforward invertible constructing features converting While Theorem 21 proved directly reduction monomial hard learn general DNF easily obtain known monomials t Since 6 21 pac exten results The proof Theorem 6 based clause learned learning monotone monomial clause monomial ijdeterminate proved invertible monotone ijdeterminate features distributionindependent setting monotone DNF cases single clause reduces result Observation 30 For fixed j language multipleclause predicate def iff DNF initions containing clauses iDEFIMDETl3thJ jZB patlearnable patlearnable For anyfied k language multipleclause predicate definitions containing clauses kLOCAL aVB patlearnable iff DNF patlearnable Some positive results obtainable lterm DNF learnable Russell observed simple distribution simple distributions learning algorithm learn lclause It known constant 1 monotone lterm DNF predicate definition 331 I2 Dieroski Muggleton monotone ijdeterminate construct p variables 1 I1 Specifically ui true iff e E exr ALOCi DB I Simple distributions broad class probability distributions include computable distributions map example e assignment Q 34 W W CdwdArtijiciul Intrllience 79 I 995 I38 constructing DNF features finally converting predicate definition Thus appropriate set boolean features learning lterm formula ijdeterminate Theorem 31 Dieroski Muggleton Russell 161 For anyxed und j g14uge multipleclause predicate definitions containing 1 clauses iDEPTHDETERM jDB unfirmly palearnable simple distributions The proof technique applied predicate definitions containing LOCAL clauses following corollary Theorems 21 3 1 Observation 32 For uny fixed k und u language multipleclause predicate dej nitions containing 1 clauses kLOCAL uDB simple distributions uniformly putlearnable One problem applying results practice proofs learnability simple distributions sample certain universal distribution Implemented systems heuristic methods completely constructive particular computable learn multiple clauses learning algorithm 331 7 Concluding remarks Most implemented firstorder learning systems use restricted mathematically understood resent concepts An obvious advantage representation complexity suggests logics mathematically theoretical foundations subfield tigating model polynomial predictability analyzing encourages power learnability learnability restricted analyzed This paper sought inductive logic programming logic programs Most analysis introduced Pitt Warmuth language characterizing logic programs semantics rep learning systems inves 441 This model expressive formally expand In paper characterized extensions language determinate clauses constant depth 16371 These results summarized logdepth First reduction circuits showed single logdepth determinate clause patlearnable Next relaxed indeterminate condition determinacy clauses constant depth shown obtained Since considered free variables indeterminacy clause restrictions language We showed hard learn DNF We showed restricting language polynomial clause leads predictability literal support patlearnability hard number results predict clause k degree W W CohenArtificial Intelligence 79 1995 138 35 nDEPTHDETERM kDEPTH log nDEPTHDETERM kDEPTHDETERM SOUTPUT Fig 5 Summary results single clauses Above heavy line languages heavy line languages patlearnable Predicting boxed predictable All predictable hard predict languages kINDETERMPU language kFREE equivalent predicting DNF open problem Finally showed restricting especially learnability This result strict generalization j ijdeterminate j locality clause constant k leads pac interesting klocal clauses shown ijdeterminate sense fixed clause rewritten clause locality greater following clauses indeterminate 16271 note These results summarized previous clauses Fig 5 shows languages considered paper partially ordered expressive power l3 Some results previous work determinate constantdepth relaxed The analysis languages heavy polynomially cryptographic shown boxed question original The case kINDETEFUvLS heavy line hard predict supersets languages The language kfree open line learning results previous predictable The languages assumptions dotted associated line patlearnable results representational literature ex predictable theory arbitrarydepth computational predictable remaining assumptions languages iff DNF results 23 raises In obtaining tended Haussler concepts k variables easy existential indeterminate clause general hard predict DNF immediate question learnability existential conjunctive representationindependent conjunctive predictability setting It concept expressed single result Theorem 17 concepts More recently Kietz 27 shown constantdepth hard patlearn learn These results strengthened particular presented representationindependent arbitrarydepth indeterminate determinate clauses clauses hard pac number ways paper hardness results determinate I3 Strictly speaking kINDETERMPLS set languages single Set need depth k However kINDETERMPU language considered bounded language languages depth related 16 WW CohenArtijicial Intelligence 79 1995 138 clauses log depth arbitrary depth indeterminate depth clauses constant We investigated nate clauses One result indepth subclass indeterminate strict generalization clauses learnability subclasses indetermi investigation isolation interesting patlearnable class klocal clauses class ijdeterminate clauses Kietz Dieroski 28 investigated complexity closely LANGE hypothesis t VB LANGE LANGE theory DB E DB set examples results ILP problem possible task called ZLP problem The ILP problem given background LANGE consistent examples respect provability k One corollary Theorem 21 ILP problem klocal clauses The connection negative complex As formalized Kietz Dieroski polynomial time algorithm number examplesfor expressive hypothesize Thus possible Theorem 4 language LANG polynomially solves ii examples corresponding hypothesis relationship tractable somewhat grows quickly language LANGE sufficiently positive examples language solvable hard predict However results Blumer et al 4 algorithm A time number I4 Thus imply ILP problems tractably solved way yields concise predictable A run polynomial ILP problem size hypotheses linearly results paper returned A grows nearly generates hypothesis negative predictability solve ILP problem LANG ILP problem hypothesis table containing language principle lookup example area appear 10131419 suggest The learnability recursive logic A number questions challenging problem results difficult continued 18223 programs The learnability multipleclause analysis DNF encouraging programs genera1 logic programs settings considered Angluin Frazier Pitt 2 work remains relating learnable predicate definitions progress learnability I 1 The learnability learnable firstorder largely open issue fairly general classes restricted classes genera1 logic open area Finally languages firstorder clauses analogous languages 15411 restricted settings Acknowledgments The author like thank Haym Hirsh Rob Schapire presentation Mike Kearns JgUwe Kietz Rob Schapire discussions technical content reviewers helpful comments presentation comments number helpful I4 More precisely hypothesis size sample m examples ma 1 W W CohenArtificial Intelligence 79 1995 I38 31 References 11 AV Aho JE Hopcroft JD Ullman The Design Analysis Computer Algorithms Addison Wesley Reading MA 1974 121 D Angluin M Frazier L Pitt Learning conjunctions horn clauses Mach Learn 9 23 1992 131 A Blum M Singh Learning functions k terms Computational Learning Theory Rochester NY Morgan Kaufmann Los Altos CA 1990 Proceedings Third Annual Workshop I41 A Blumer A Ehrenfeucht D Haussler M Warmuth Classifying learnable concepts VapnikChervonenkis dimension J ACM 36 1989 929965 IS R Boppana M Sipser The complexity finite functions J van Leeuwen ed Handbook Theoretical Computer Science NorthHolland Amsterdam 1990 758804 I61 WW Cohen Grammatically biased language Artif Intell 68 1994 303366 learning learning description logic programs explicit antecedent I71 WW Cohen Compiling knowledge Proceedings Ninth International Conference Machine Learning Aberdeen explicit bias 1992 I81 WW Cohen Cryptographic limitations learning oneclause logic programs Proceedings Tenth National Conference Artijcial Intelligence Washington DC 1993 191 WW Cohen Learnability restricted logic programs Proceedings Third International Workshop Inductive Logic Programming Bled 1993 I IO WW Cohen A patlearning algorithm restricted class recursive logic programs Proceedings Tenth National Conference Artacial Intelligence Washington DC 1993 111 WW Cohen Rapid prototyping ILP systems explicit bias Proceedings 1993 IJCAI Workshop 1993 Inductive Logic Programming Chambery 121 WW Cohen Patlearning nondeterminate Artificial Intelligence Seattle WA 1994 clauses Proceedings Eleventh National Conference I 131 WW Cohen Patleaming I 141 WW Cohen Patlearning 151 WW Cohen H Hirsh Learnability recursive recursive logic programs efficient algorithms logic programs negative results J Artif Intell Res 2 541573 J Artif Intell Res 2 500539 description logics Proceedings Fourth Annual Workshop Computational Learning Theory Pittsburgh PA ACM New York 1992 161 S Dieroski S Muggleton S Russell Patlearnability determinate logic programs Proceedings 171 C Feng 1992 Workshop Computational Learning Theory Pittsburgh PA 1992 fault diagnosic Press New York 1992 Programming Academic temporal Inducing rules qualitative model Inductive Logic I 181 M Flammini A MarchettiSpaccamela distributions probability Pittsburgh PA ACM New York 1992 classes Proceedings Fourth Annual Workshop Computational Learning Theory L Kera Learning DNF formulae I 191 M Frazier techniques 1993 CD Page Learnability results Proceedings Third International Workshop Inductive Logic Programming Bled theories Some basic nondeterminate recursive 201 M Frazier L Pitt Learning entailment An application propositional horn sentences Proceedings Tenth International Conference Machine Learning Amherst MA Morgan Kaufmann Los Altos CA 1993 12 I A Frisch CD Page Learning constrained atoms Proceedings Eighth International Workshop Machine Learning Ithaca NY Morgan Kaufmann Los Altos CA 1991 I 22 T Hancock Learning kp decision trees uniform distribution Proceedings Sixth Annual ACM Conference Computational Learning Theory Santa Cruz CA ACM New York 1993 231 D Haussler Learning conjunctive 241 M Kearns M Li L Pitt L Valiant On learnability boolean concepts structural domains Mach Learn 4 1 1989 formulae Proceedings 19th ACM Symposium Theory Computing New York 1987 251 M Kearns L Valiant Cryptographic limitations learning Boolean formulae finite automata Proceedings 21th ACM Symposium Theory Computing New York 1989 126 M Kharitonov Cryptographic lower bounds learnability boolean Proceedings Fourth Annuul Workshop Compufutionul functions uniform Learning Theory Pittsburgh distribution PA ACM New York 1992 127 1 JU Kietz Some computational programming Proceedings lower bounds inductive 1993 Europeun Conference Machine Leurning Vienna 1993 computational complexity logic inductive binding j 28 1 JU Kietz Dieroski 129 I JU Kietz K Morik Constructive Inductive logic programming induction background learnability SIGART Bull 5 1994 2231 Proceedings Workshop knowledge OII Evuluating und Chunging Representution Muchine Leurning ut IJCAI91 Sydney I991 I30 1 RD King S Muggleton RA Lewis MJE Stemberg Drug design machine learning use logic programming model structureactivity relationships trimethoprim analogues dihydrofolate reductase Proc Nut Acud Sci 89 1992 1 3 I 1 E Kushilevitz D Roth On learning visual concepts DNF formulae Proceedings Sixth Annuul tCM Conference OH Cornpututionul I 32 I N Lavra S Dieroski Background fpurning Theor Santa Cruz CA ACM New York 1993 knowledge declarative bias inductive concept learning Internutionul Workshop A1192 Lecture Notes KP Jantke ed Anulocul Artificial Intelligence 642 Springer Berlin 1992 ctrrd Inductive Inferem 33 I M Li P Vitanyi Learning simple concepts simple distributions SIAM J Comput 20 5 1991 31 1 JW Lloyd Fimndutons Lrg Proqrunrminji 135 1 BDS Muggleton The application inductive Irductive Logic Progrutnmin Springer Berlin 2nd ed 1987 logic programming Academic Press New York 1992 finiteelement mesh design Inductive Logic Programming Academic Press New 1371 Inductive logic programming I36 S Muggleton York 1992 S Muggleton C Feng Eticient Academic Press New York 1992 S Muggleton RD King MJE Sternberg Protein secondary machine SH Muggleton ed Inductive Logic Progrummin M Pazzani D Kibler The utility knowledge L Pitt M Frazier Classic learning Protein Enqrq 5 1992 647657 1391 1401 I41 I 1381 Academic Press New York 1992 inductive learning Much Leurn 9 I 1992 Sevenfh Anmud ACM Conference Proceedings Theory New Brunswick NJ ACM New York 1994 learning limitations learning examples J ACM 35 1988 965984 prediction problems On difficulty predicting Complexity Theory Washington EEE Conjtirence Structure burning Proceedinys 3rd Annual Cornpututionul L Pitt L Valiant Computational L Pitt MK Warmuth Reductions automata DC IEEE Computer Society Press Silver Spring MD 1988 L Pitt M Warmuth Predictionpreserving GD Plotkin A note inductive generalization Much JR Quinlan Learning JR Quinlan Determinate logical definitions literals relations Much burn logic programming reducibility J Compu Syst Sci 41 1990 430467 Intell 5 1969 153163 5 3 1990 inductive Proceedings Eighth Ithaca NY Morgan Kaufmann Los Altos CA 1991 lnternutionul Workshop Muchine Leurning RE Schapire The strength weak learnability Much Leurn 5 2 E Shapiro Algorithmic LC Valiant A theory learnable Commurl ACM 27 11 M Vilain P Koton M Chase On analytical Progrum Debugging 1984 MIT Press Cambridge MA 1982 1990 Eighth Nutionul Conference Artchl Intellipncr similaritybased Proceedings classification Boston MA MIT Press Cambridge MA 1990 WI I23 I I441 1451 I461 1471 1481 1491 ISOl 151 I induction logic programs Inductive Logic Prugrurnrning structure prediction logicbased