Artiﬁcial Intelligence 222 2015 157181 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Playing knowledge A virtual player Who Wants Be Millionaire leverages question answering techniques Piero Molino Pasquale Lops Pierpaolo Basile Giovanni Semeraro Marco Gemmis Department Computer Science University Bari Aldo Moro Via E Orabona 4 I70125 Bari Italy r t c l e n f o b s t r c t Article history Received 5 November 2013 Received revised form 29 January 2015 Accepted 6 February 2015 Available online 12 February 2015 Keywords Language game Question answering Natural language processing Artiﬁcial intelligence Decision making This paper describes techniques build virtual player popular TV game Who Wants Be Millionaire The player answer series multiple choice questions posed natural language selecting correct answer different choices The architecture virtual player consists 1 Question Answering QA module leverages Wikipedia DBpedia datasources retrieve relevant passages text useful identify correct answer question 2 Answer Scoring AS module assigns score candidate answer according different criteria based passages text retrieved Question Answering module 3 Decision Making DM module chooses strategy playing game according speciﬁc rules scores assigned candidate answers We evaluated accuracy virtual player correctly answer questions game ability play real games order earn money The experiments carried questions coming oﬃcial Italian English boardgames The average accuracy virtual player Italian 7964 signiﬁcantly better performance human players equal 5133 The average accuracy virtual player English 7641 The comparison human players carried English playing successfully game heavily depends players knowledge popular culture experiment involved sample Italian players As regards ability play real games involves deﬁnition proper strategy usage lifelines order decide answer question condition uncertainty retire game taking earned money virtual player earns e 114531 average Italian e 88878 English exceeds average earned human players greater extent e 5926 Italian 2015 Elsevier BV All rights reserved Corresponding authors marcodegemmisunibait M Gemmis pierpaolobasileunibait P Basile Email addresses pieromolinounibait P Molino pasqualelopsunibait P Lops giovannisemerarounibait G Semeraro httpdxdoiorg101016jartint201502003 00043702 2015 Elsevier BV All rights reserved 158 P Molino et al Artiﬁcial Intelligence 222 2015 157181 1 Introduction The work intelligent games long history successful visible results Artiﬁcial Intelligence research 34 Indeed today artiﬁcial systems able compete challenge human players complex games Most games closed world ones meaning ﬁnite number possible choices allows researchers solve formal way hard play exponential dimensions search spaces A challenging type games represented open world games sport games crosswords structured states game actions player easily enumerated making search space possible solutions practically unfeasible One recent results ﬁeld success Watson opendomain question answering built IBM Research February 2011 beat highest ranked players quiz Jeopardy 1718 We particularly interested games related human language They classiﬁed word games word meanings important language games word meanings play important role 27 Language games generally require wide linguistic common sense knowledge Who Wants Be Millionaire WWBM perfect example language game player provides answer question posed natural language selecting correct answer possible ones Even number possible answers limited able successfully play game heavily depends players knowledge understanding questions ability balance conﬁdence answer risk taken answering This article describes architecture Virtual Player WWBM game leverages Question Answering QA techniques Wikipedia DBpedia open knowledge sources order incorporate knowledge useful playing game A preliminary work describes architecture virtual player presented 37 The current work extends previous work following directions use DBpedia decision making strategy integrated manage lifelines characterizing game possibility retire game use machine learning techniques improve process scoring candidate answers question Extended related work question answering answer validation language games provided extensive experiments Italian English versions game Motivated challenge develop effective virtual player WWBM game paper address issues related general topic designing effective QA systems concerns speciﬁc aspects game Hence investigate following research questions RQ1 To extent QA designed languageindependent way preserving effectiveness We cope question proposing general architecture QA Answer Scoring AS framework exploits resources algorithms speciﬁcally designed given language exclusively basic NLP operations partofspeech tagging stemming In order assess effectiveness framework different languages performed experiments English Italian RQ2 Can Wikipedia DBpedia serve effective knowledge bases answering WWBM game questions We address question developing virtual player based proposed QA framework leverages Wikipedia DBpedia open knowledge sources ﬁnd correct answers Besides QA framework virtual player adopts decision making strategy play game rules usage lifelines answering condition uncertainty retiring game taking earned money Experiments performed compare accuracy virtual player human players The paper organized follows Section 2 describes rules game related work areas language games QA AS presented Section 3 The architecture virtual player details QA AS modules decision making strategy adopted play game provided Sections 47 Section 8 reports results extensive evaluation performed Italian English versions game Finally conclusions reported Section 9 2 Rules game WWBM language game broadcast TV channels countries player correctly answer series 15 multiplechoice questions increasing diﬃculty Questions posed natural language correct answer selected possible choices Fig 1 shows example question Who directed Blade Runner possible answers A Harrison Ford B Ridley Scott C Philip Dick D James Cameron There time limits answer questions Moreover contestants read question advance time decide attempt answer quit game keeping earned money Each question certain monetary value level 1 e 500 level 2 e 1000 level 3 e 1500 level 4 e 2000 level 5 e 3000 level 6 e 5000 level 7 e 7000 level 8 e 10000 level 9 e 15000 level 10 e 20000 level 11 e 30000 level 12 e 70000 level 13 e 150000 level 14 e 300000 level 15 e 1000000 If answer correct player earns certain money continues play answering questions increasing diﬃculty reaches question retires game taking earned money There guarantee points money banked lost player gives incorrect answer P Molino et al Artiﬁcial Intelligence 222 2015 157181 159 Fig 1 An example Who Wants Be Millionaire question questions 3000 20000 1000000 Euros corresponding milestone questions 5 10 15 respectively At point contestant use lifelines provide form assistance 5050 lifeline removes wrong answers leaving player binary choice correct answer incorrect Poll Audience player asks studio audience pronounce correct answer The percentages audience 4 different answers given player word choice answer Phone Friend player 60 seconds phone friend read question possible choices order suggestion right choice 3 Related work 31 Natural Language Processing language games Language games usually require big knowledge deep reasoning capabilities compete human level Artiﬁcial players language games adopt Natural Language Processing NLP technologies order manage com plexity ambiguity language storing manipulating complex representations knowledge involved game A popular language game solving crossword puzzles Besides linguistic knowledge solving crosswords requires satisfaction constraints possible answers The ﬁrst experience reported literature Proverb 28 exploits large libraries clues solutions past crossword puzzles WebCrow 15 ﬁrst solver Italian crosswords exploits Web main source information set previously solved games WebCrow based sequential combination clue answering grid ﬁlling solution radically different human approach In order ﬁnd best candidate words WebCrow queries Google search engine queries reformulations original deﬁnitions obtained enriching morphological forms keywords varying number gender nouns tense verbs adding synonyms hypernyms WordNet order querying process effective The text retrieved pages analyzed NLP techniques classiﬁer chooses probable partofspeech depending deﬁnition order reduce number candidate words Finally list best words deﬁnition given matched letter constraints given grid WebCrow achieves 688 correct words 799 correct letters showing potential Web resource complex language games Another interesting language game Guillotine game broadcast Italian National TV company It involves single player given set ﬁve words clues linked way speciﬁc word represents unique solution game Words unrelated strongly related word representing solution For example given ﬁve words sin Newton doctor pie New York solution apple apple symbol original sin Christian theology Newton discovered gravity means apple apple day keeps doctor away famous proverb apple pie fruit pie New York city called big apple In 483 authors present OTTHO On Tip THOught artiﬁcial player Guillotine game The idea OTTHO deﬁne knowledge infusion process analyzes unstructured information stored open knowledge sources Web create memory linguistic competencies world facts effectively exploited deeper understanding information deals The knowledge infusion process adopts NLP techniques build knowledge base similarly approach described article extracts information mainly Wikipedia A reasoning mechanism based spreading activation algorithm adopted retrieve appropriate pieces knowledge useful ﬁnd possible solutions An approach implement virtual player Who Wants Be Millionaire game pro posed 25 The authors exploit huge knowledge Web use NLP techniques reformulate questions order create different queries The queries sent Google search engine number results 160 P Molino et al Artiﬁcial Intelligence 222 2015 157181 ranking mechanism exploits redundancy information sources 25 A decision making module combines results spirit ensemble learning adaptive weighting scheme tries maximize earned money respect risk answering The reaches accuracy 75 showing unstruc tured data useful kind task fails questions require common sense reasoning access structured information The main differences respect work adopt selected sources information available Web Wikipedia DBpedia Web attempt improve reliability answers adopt QA framework instead search engine order improve process selecting reliable passages In February 2011 IBM Watson supercomputer adopting technology DeepQA project 18 beaten champions Jeopardy TV quiz In Jeopardy player given question expressed answer ﬁnd answer expressed question Watson applies different NLP Information Retrieval IR Machine Learning ML techniques focusing opendomain QA answering questions domain constraints Watson analyzed 200 millions content elements structured unstructured including text Wikipedia The steps Watson answering process summarized follows 1 acquires knowledge different data sources encyclopedias dictionaries thesauri journal articles databases taxonomies ontologies 2 input problem treated question analyzed NLP algorithms lastly classiﬁed 3 candidate answers generated previously acquired knowledge 4 candidate answers pass threshold ﬁltered Several scoring algorithms rank candidate answers order evidence quality Lastly scoring criteria combined select ﬁnal candidate answer Similarly approach implemented Watson adopt QA techniques solving WWBM game use process different scoring criteria eventually combined return best candidate answer 32 Question answering The task Question Answering ﬁnd correct answers users questions expressed natural language The traditional QA pipeline relies following steps Question Analysis Passage Retrieval Answer Extraction Answer Se lectionValidation Closeddomain QA refers QA tasks pertaining speciﬁc limited domains medicine Dealing questions closeddomain generally easier task kind domainspeciﬁc knowledge ex ploited limited type questions accepted Opendomain QA refer speciﬁc domain deals general questions This usually requires use world knowledge answer extracted The Web generally source knowledge order exploit redundancy information selecting answers according frequency search results 1426 This technique complemented textual pattern extraction matching order ﬁnd exact answers rank conﬁdence 2339 NLP methods understanding users questions matching passages extracted documents 2224 As reported 10 adoption NLP plays key role likely answers users questions way expressed signiﬁcantly differ question Thus NLP essential discovering complex lexical syntactic semantic relationships questions candidate answers The commonly adopted linguistic analysis steps include stopword removal stemming lemmatization partofspeech tagging parsing named entity recognition word sense disambiguation semantic role labeling Other approaches rely different kind knowledge extract answers questions In 16 approach opendomain QA massive knowledge bases KBs carried decomposing open QA problem smaller subproblems including question paraphrasing query reformulation In ﬁrst step question rewritten paraphrase operator mined large corpus questions order reduce variance input questions example How tell ﬂu reformulated What signs ﬂu The second step uses handwritten templates parse paraphrased question speciﬁc KB query reformulated queryrewrite operator cope vocabulary mismatch question words KB symbols In work adopt Wikipedia DBpedia knowledge sources extract answers questions integrate common linguistic analysis steps The novelty adoption Distributional Semantic Models DSM 47 computing semantic similarity questions documents This novel QA ﬁeld especially task answer reranking In 4 authors propose semantic parser maps questions answers latent logical forms They ﬁrst generate number logical forms exploiting lexicon maps logical predicates phrases built knowledge base large text corpus They calculate probability candidate logical form loglinear model exploits lexical logical features trained questionanswer pairs This approach differs rely mainly lexical features retrieving passages mapping questions logical form try map questions DBpedia triples exploiting predicate lexicalization commonalities lexicon construction step 33 Question answering machine reading answer validation QA systems generally deal simple questions require inference ﬁnd correct answers real understanding documents performed This historically led QA architectures based Information Retrieval P Molino et al Artiﬁcial Intelligence 222 2015 157181 161 techniques ﬁnal answers obtained focusing selected portions retrieved documents matching sentence fragments sentence parse trees Other systems perform deeper analysis texts solve tasks involve kind reasoning For example Machine Reading task 40 goal answer questions require deep knowledge individual short texts systems required choose answer analyzing corresponding test document conjunction background text collections Other complex tasks include Recognizing Textual Entailment RTE 121 Answer Validation Exercise AVE 452 In RTE decide meaning text T entails meaning different text H hypothesis Differently AVE consists deciding answer question correct according given text Systems receive set triplets question answer supporting text return value triplet validated answer correct supported selected selected answer validated chosen output rejected answer incorrect evidence correctness In order solve tasks techniques adopted based use lexical processing syntactic processing Named Entities 44 In 6 answer validation carried computing overlap response question stemmed content words humangenerated answer key The intuition strategy good answer expected contain certain keywords exact phrasing matter This reason authors stopwords removal stemming In 32 answer validation based intuition knowledge nects answer question estimated exploiting redundancy Web information More speciﬁcally hypothesis number documents retrieved Web question answer cooccur good indicator validity answer More complex strategies propose solutions answer validation based lightweight process abduction starting paragraphs text answers 21 based use semantics For example Castillo 9 builds model Support Vector Machines deﬁne implication holds set lexical semantic measures compute similarity hypothesis text pairs Features based 1 overlap text hypothesis computed taking account single words stems bigrams trigrams 2 cosine similarity Levenshtein distance text hypothesis 3 semantic similarity Wordnet Glöckner 20 shallow feature extraction like lexical overlap validate answers A local score computed aggregation determine combined score answer captures joint evidence snippets supporting answer Ahn et al 2 exploited semantic representations inference techniques process answer extraction selected passages Answer candidates extracted relaxed uniﬁcation method takes advantage Prolog uniﬁcation allows assign high scores perfect matches terms question passage low scores perfect matches obtained relaxed uniﬁcation Less perfect matches granted different semantic types predicates different argument order terms symbols semantically related hypernymy according WordNet We inspired previous approaches build virtual player WWBM game borrowed techniques adopted AVE task techniques based lexical processing computation approximate match passages text candidate answers Section 6 34 Question answering linked data The rapid growth semantic information published Web particular linked data initiative 5 poses new challenges supporting users query large amounts heterogeneous structured semantic data natural language interfaces This led rise ontologybased QA new paradigm able exploit expressive power ontologies representation user information needs keywordbased queries FREyA 13 allows users enter queries form In ﬁrst step generates syntactic parse tree order identify answer type The processing starts lookup annotating query terms ontology concepts ontologybased gazetteer OntoRoot Next basis ontological mappings triples produced ﬁnally combined generate SPARQL query In similar way PowerAqua 30 transforms query set triples cid3subject property objectcid4 means linguistic processing maps triples suitable semantic resources ontologies likely query terms Given semantic resources set ontology triples jointly cover query derived combined complete answer merging ranking interpretations produced different ontologies QAKiS 8 QA DBpedia focuses bridging gap natural language expressions labels ontology concepts means WikiFramework repository This repository built automatically extracting relational patterns Wikipedia free text specify possible lexicalizations properties DBpedia For example natural language patterns express relation birthDate born The approach pattern repository represents promising solution bridging lexical gap natural language expressions ontology labels work Section 51 Similarly QAKiS SemSek 1 focuses matching natural language expressions ontology concepts This relies steps linguistic analysis query annotation 1 http pascallin ecs soton ac uk Challenges RTE 2 http nlp uned es clefqa ave 162 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Fig 2 Virtual player architecture semantic similarity The query annotation mainly looks entities classes DBpedia index match expressions occurring natural language question This process guided syntactic parse tree provided linguistic analysis Starting plausible identiﬁed resources classes SemSek retrieves ordered list terms following dependency tree In order match terms DBpedia concepts SemSek uses semantic similarity measures based Explicit Semantic Analysis 19 based WordNet In 31 evaluation systems context challenges question answering systems linked data QALD presented Results encouraging QA linked data deliver answers complex information needs expressed natural language heterogeneous semantic data task mapping natural language formal queries trivial problems 11 4 Virtual player architecture The architecture virtual player WWBM game presented Fig 2 consists modules Game Manager manages user interface selects question level game logs information game different players Question Answering leverages Wikipedia DBpedia retrieve rank relevant passages text useful identify correct answer question More details provided Section 5 Answer Scoring leverages list text passages extracted Question Answering module adopts criteria assign score possible answers question A detailed description provided Section 6 Decision Making takes ﬁnal decision answering question retiring game considering current level game available lifelines score computed possible answer Answer Scoring A detailed description provided Section 7 According current level diﬃculty game Game Manager selects question possible answers passed Question Answering module This module exploits knowledge contained Wikipedia DBpedia select ranked list passages text likely contain correct answer question These passages processed Answer Scoring implements set heuristics come score possible answer question Finally Decision Making module decides provide speciﬁc answer retire game taking account scores candidate answers available lifelines current level game In order better explain process answer selection use following running example paper Let consider question Fig 1 Who directed Blade Runner correct answer B Ridley Scott Let consider ranked list text passages provided Question Answering module Table 1 Each passage contains title Wikipedia page extracted score computed Question Answering module More formally given cid3q A B C Dcid4 q question A B C D possible answers Question Answering module returns list results Rq cid3t1 p1 w 1cid4 cid3tRq pRq wRqcid4 triple cid3ti pi w icid4 corresponds title ti Wikipedia page containing passage pi w score P Molino et al Artiﬁcial Intelligence 222 2015 157181 163 Table 1 List passages returned Question Answering module question Who directed Blade Runner Each passage reports Wikipedia page extracted relevance score passage respect question Wikipedia page ti Ridley Scott Blade Runner Blade Runner Blade Runner Blade Runner Passage pi Sir Ridley Scott born 30 November 1937 English ﬁlm director producer Following commercial breakthrough Alien 1979 bestknown works sciﬁ classic Blade Runner 1982 best picture Oscarwinner Gladiator 2000 Blade Runner 1982 American dystopian science ﬁction action ﬁlm directed Ridley Scott starring Harrison Ford Rutger Hauer Sean Young The screenplay written Hampton Fancher David Peoples loosely based novel Do Androids Dream Electric Sheep Philip K Dick Director Ridley Scott ﬁlms producers spent months meeting discussing role Dustin Hoffman eventually departed differences vision Harrison Ford ultimately chosen reasons The screenplay Hampton Fancher optioned 1977 Producer Michael Deeley interested Fanchers draft convinced director Ridley Scott ﬁlm Interest adapting Philip K Dicks novel Do Androids Dream Electric Sheep developed shortly 1968 publication Director Martin Scorsese interested ﬁlming novel optioned QA score w 532 51 5 49 12 indicating relevance passage respect question q The list Rq Question Answering module able ﬁnd passages relevant question This happen information contained adopted knowledge sources Wikipedia DBpedia information contained adopted knowledge sources possible ﬁnd match textual semantic question passages containing answer For example answer question When Leonardo da Vinci born contained Wikipedia diﬃcult ﬁnd reported Leonardo da Vinci April 15 1452May 2 1519 We solve problem implementing strategy based use DBpedia Section 51 question falls categories remain unanswerable Section 811 5 Question answering In order provide virtual player knowledge useful ﬁnding correct answers questions game exploited data coming open knowledge sources Wikipedia DBpedia Their knowledge processed multilingual QA framework called QuestionCube 3536 able retrieve relevant passages text likely contain correct answer questions game Fig 3 depicts high level architecture QA framework It works separate steps indexing time builds different indexes documents passages belonging document query time question analyzed NLP pipeline tagged linguistic annotations passed set search engines Finally passages belonging documents retrieved search engines scored ﬁltered returned ranked list More details follow Question analysis pipeline NLP analyzers adopted tag questions linguistic annotations We adopt fol lowing pipeline stemming partofspeech tagging lemmatization Named Entity Recognition NER This representation input search engines ﬁlter need information carry analysis linguistic level question So far NLP analyzers English Italian developed Search engines use multiple engines allows integration retrieval strategies data sources Each search engine query generation component exploit different tags added query adopt different query syntax When new question comes engine activated lists retrieved documents merged single list maintaining reference provenance document corresponding score computed engine Starting retrieved documents passage index obtain list passages input Filters We adopt different search engines engines based BM25 model 42 running keywords lemmas extracted Wikipedia pages respec tively engine searching DBpedia triples described Section 51 Filters pipeline ﬁlters adopted assign score text passage belonging documents retrieved previous step Zero ﬁlter removes passages having score equal 0 TopN ﬁlter selects N topranked passages highest score 164 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Fig 3 QuestionCube architecture Terms ﬁlter assigns score passage based frequency occurrence question terms passage Exact sequence ﬁlter assigns score passage based number terms occur longest overlapping sequence question passage Normalization ﬁlter assigns normalized score passage based passage length Both simple normalization ﬁlter considering number terms called Bytesize Normalization ﬁlter based Pivoted Normalized Document Length technique implemented More details 3350 Ngrams ﬁlter assigns score passage based overlapping ngrams question passage Density ﬁlter assigns score passage based distance question terms inside passage The closer question terms passage higher score The density calculated modiﬁed version Minimal Span Weighting schema 38 cid3 cid2 q d 1 maxmms minmms q d set terms query document respectively speciﬁcally query question document passage maxmms minmms initial ﬁnal location sequence document terms containing query terms Distributional ﬁlter assigns score passage based similarity question computed Distribu tional Semantic Model DSM DSM built applying Latent Semantic Analysis termterm matrix Wikipedia pages We construct matrix containing 100000 frequent terms Wikipedia stopwords removed multiword expressions allowed In DSMs given vector representation words u u1 u2 uncid6 v v 1 v 2 vncid6 vectors 400 dimensions possible compute similarity cosine angle However question passages sentences composed terms order compute similarity need method compose words occurring sentences It possible combine words vector addition This operator similar superposition deﬁned connec tionist systems 51 corresponds pointwise sum components Addition commutative operator means account order underlying structures existing words More complex methods combine word vectors exploited Formally q q1q2 qn p p1 p2 pm question passage respectively build vectors q p represent question passage semantic P Molino et al Artiﬁcial Intelligence 222 2015 157181 165 space respectively Vector representations built applying addition operator vector representation words belonging Fig 4 Leonardo da Vinci infobox q q1 q2 qn p p1 p2 pm 1 The similarity q p computed cosine similarity This similarity score assigned passage ZScore ﬁlter assigns score passage based ZScore normalization 49 scores assigned search engines ﬁlters CombSum ﬁlter assigns score passage summing scores assigned search engines ﬁlters 49 Terms ﬁlter Density ﬁlter enhanced version adopts combination lemmas partofspeech tags features instead terms A boost factor assigned ﬁlter order increase decrease strength 51 Using DBpedia knowledge source The question When Leonardo da Vinci born shows diﬃculty extract correct answer infor mation contained Wikipedia In case date birth April 15 1452 identiﬁed adopting classical passage retrieval process implemented Question Answering module In order manage kind questions speciﬁc search engine leverages knowledge contained DBpedia DBpedia includes struc tured information embedded Wikipedia articles infobox Fig 4 reports infobox Leonardo da Vinci contains useful information birth date death date nationality DBpedia represents resources prop erties relations resources RDF triples contain components subject predicate object For example RDF triple represents birth date Leonardo da Vinci 166 P Molino et al Artiﬁcial Intelligence 222 2015 157181 httpdbpediaorgresourceLeonardo_da_Vinci dbpediaowlbirthDate 14520415 DBpedia allows query relations properties associated Wikipedia resources birth date Leonardo da Vinci accessing property dbpediaowlbirthDate In order leverage knowledge contained DBpedia manually created mapping 50 frequent DBpedia properties different lexical izations question form asking speciﬁc properties For example property dbpediaowlbirthDate mapped questions When born What date birth similar wordings In way obtained datasets containing 347 questions Italian 312 English question tagged corresponding DBpedia property Each dataset train Rocchio classiﬁer 43 given question able predict DBpedia property mapped The features train classiﬁer words occurring questions Stopwords removed words When How Where useful hints guide classiﬁer correct classiﬁcation performance details classiﬁer reported Section 811 In order retrieve relevant information DBpedia queried additional search engine containing documents lexicalization RDF triples subject form cid3label subject label predicate label objectcid4 value object case literals The lexicalization previous example cid3Leonardo da Vinci date birth 14520415cid4 Only triples related 50 selected properties lexicalized way Each document additional ﬁeld reporting DBpedia properties contains dbpediaowlbirthDate When query question sent DBpedia search engine ﬁrst classiﬁed Rocchio classiﬁer order identify property refers selected property added query named entities occurring question The query submitted search engine retrieves set documents relevant query Starting documents extracts corresponding list passages RDF triples scored pipeline ﬁlters described Section 5 additional DBpedia property ﬁlter This ﬁlter scores triples containing property returned classiﬁer conﬁdence triples containing 0 It worth note question submitted search engines working Wikipedia DBpedia order retrieve results knowledge sources 6 Answer scoring The main goal Answer Scoring module assign score possible answers question Similarly approaches context Answer Validation Exercise 45 adopted ﬁve criteria based analysis passages returned Question Answering module Each criterion returns score possible answer normalized sum scores possible answers More formally given cid3q A B C Dcid4 criterion computes cid3q c A c B cC c D cid4 c X score assigned candidate answer X In following criterion described taking account running example Title Levenshtein TL criterion It computes Levenshtein distance candidate answer X title ti Wikipedia page returned Question Answering module The Levenshtein distance measures difference strings deﬁned minimum number singlecharacter edits insertion deletion substitution required change string As Levenshtein distance distance measure similarity measure compute maxlenX lenti levX ti maxlenX lenti len function computing length string lev X ti Levenshtein distance X ti normalization factor scores 0 1 interval In running example taking account ﬁrst passage returned Question Answering module Table 1 answer B Ridley Scott occurs title page containing passage gets maximum score 1 answer A Harrison Ford gets score equal 1312 0167 answer D James Cameron gets score equal 1312 0077 answer C Philip Dick gets score equal 1210 12 0077 The normalized score answer 13 13 c A c B cC c D 0077 0077101670077 1 0077101670077 0167 0077101670077 0077 0077101670077 0058 0757 0126 0058 Longest Common Subsequence LCS criterion It computes Longest Common Subsequence candidate answer X passage text pi returned Question Answering module title ti Wikipedia page P Molino et al Artiﬁcial Intelligence 222 2015 157181 167 passage pi extracted In running example taking account second passage answer A Harrison Ford gets score equal 13 answer B Ridley Scott gets score equal 12 answer C Philip Dick gets score equal 11 answer D James Cameron gets score equal 0 occur passage The normalized score answer c A c B cC c D 13 1312110 12 1312110 11 1312110 0 1312110 0361 0333 0305 0 Overlap criterion It computes Jaccard index set terms candidate answer X set terms passage text pi returned Question Answering module The Jaccard index measures similarity sets deﬁned size intersection divided size union sets In running example taking 0041 answer D gets score account second passage answers A B C score 2 49 equal 0 The normalized score answer c A c B cC c D 0 0041004100410 0 0041 0041004100410 0333 Exact Substring ES criterion It computes length characters longest common substring candidate answer X passage text pi returned Question Answering module normalized length didate answer In running example taking account second passage returned Question Answering 1 answer module answer A Harrison Ford gets score 13 13 055 answer D James Cameron gets score equal 0 The normalized score C Philip Dick gets score 6 11 answer 1 answer B Ridley Scott gets score 12 12 c A c B cC c D 1 110550 1 110550 055 110550 0 110550 0392 0392 0215 0 Density criterion It computes density terms candidate answer X inside passage text pi returned Question Answering module modiﬁed version minimal overlapping span method 38 reported Section 5 In running example taking account second passage returned Question Answering 066 passage reports module answers A B score equal 1 answer C gets score equal 2 3 Philip K Dick adding extra token tokens candidate answer answer D gets 0 The normalized score answer c A c B cC c D 0376 0376 0248 0 Each criterion parameters set 1 110660 1 110660 066 110660 0 110660 1 number processed passages case score answer computed topn passages returned Question Answering module ﬁnal score average values 2 use weight w passages returned Question Answering module case average computed previous point weighted score w passage This strategy allows assign higher weights passages deemed relevant question Question Answering module 3 level linguistic analysis adopt processing passages returned Question Answering module Passages represented keywords lemmas stems stopword removal 4 use question expansion asks different questions obtained concatenating original possible candidate answers In running example virtual player queries Question Answering module following questions Who directed Blade Runner Harrison Ford Who directed Blade Runner Ridley Scott Who directed Blade Runner Philip Dick Who directed Blade Runner James Cameron If criterion adopts question expansion uses different sets passages question instead set passages 168 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Besides mentioned criteria Answer Scoring distributional ﬁlter Section 5 adopted comparing answers retrieved passages We ﬁnally decided avoid ﬁlter selection relevant passages speciﬁc question expect candidate answer contained passages ﬁlters based pure lexical comparison suﬃcient properly select score Moreover answers usually contain words named entities numbers dates hardly identiﬁed distributional ﬁlter It worth mention managed speciﬁc type questions formulated negative form Quale di questi supercriminali non e uno storico nemico di Batman English Which villains historical enemy Batman Let suppose candidate answers Dottor Destino Mister Freeze Pinguino Joker The Question Answering module returned passages containing Joker Pinguino Mister Freeze correct question raised positive form In fact question raised negative form topranked passages contain candidate answers actually discarded For reason detects negative question strategy scoring candidate answers reversed meaning highest score plausible vice versa The process detecting questions raised negative form leverages heuristics based use regular expressions Performance details reverse scoring method regular expressions correctly detecting negative questions reported Section 811 7 Decision making The Decision Making module responsible decision answering speciﬁc question retiring game available lifelines The decision strategy evaluates uncertainty information provided Question Answering module Answer Scoring available lifelines level question order ﬁnal decision The decision making strategy devised encapsulates heuristics manage following situations uncertainty 1 maximum score computed Answer Scoring candidate answers low This means quality passages retrieved Question Answering module low passages useful ﬁnd correct answer question criteria adopted Answer Scoring assigning score candidate answers satisfactory 2 difference score best candidate answer second best candidate answer small This means virtual player able provide clear evidence likely candidate answer candidate answers In situation virtual player conﬁdence candidate answers answer question lifeline In situation uncertainty decision making algorithm following decisions 1 retire game 2 use available lifelines 3 continue play providing random answer The random strategy useful situations particular speciﬁc levels game 6th 11th question wrong answer causes loss earned The decision making algorithm implements simple strategy managing lifelines meaning order usage independent level current question The leverages ﬁrst Poll Audience lifeline return candidate answer good conﬁdence Phone Friend explored ﬁnally 5050 Other authors faced problem deﬁning dynamic decision making strategy WWBM game In 25 decision making module constructs decision tree encodes probabilities utilities potential future state game The tree consists decision forks choosing answer question use lifeline walk away chance forks encode uncertainty correctly answering questions The best choice obtained action maximizes expected utility The strategy able parameterize risk virtual player exhibit risk averse behavior risk neutral Probabilities assigned nodes tree based historical past performance sample questions associated diﬃculty level A different strategy based dynamic programming adopted 41 analyze different objectives 1 maximize expected reward 2 maximize probability reaching given question An analysis results presented work allowed deﬁne order decision making module check use available lifelines More speciﬁcally Poll Audience checked speciﬁc indications provided 5050 Phone Friend The functions decision making algorithm Algorithm 1 following BestAnswers SecondBestAnswers return best second best candidate answer question q respec tively CanUselifeline returns true speciﬁc lifeline current game returns false Uselifeline returns new set answers scores obtained adopting speciﬁc lifeline CanRisk returns true current question allows player provide wrong answer losing earned money 6th 11th question returns false P Molino et al Artiﬁcial Intelligence 222 2015 157181 169 Algorithm 1 Decision making algorithm 1 procedure Decision Making q c A c B cC c D lifelines cid7 Decision strategy based scores candidate answers question q cid7 Selection best second best candidate answers cid7 Situation uncertainty conﬁdent best answer undecided best second best candidate answer available lifelines 2 3 4 BestAnswer Best q c A c B cC c D SecondBestAnswer SecondBest q c A c B cC c D BestAnswerscore 0 BestAnswerscore SecondBestAnswerscore BestAnswerscore threshold CanUsePoll Audience audienceAnswers UsePoll Audience lifelines lifelines Poll Audience audienceAnswersscore 0 Return BestaudienceAnswers end CanUsePhone Friend friendAnswer UsePhone Friend lifelines lifelines Phone Friend friendAnswer cid9 null Return friendAnswer end CanUse5050 CanRisk 50 50answers Use5050 lifelines lifelines 5050 50 50answersscore 0 Return Best5050answers Return Random5050answers end CanRisk Return Randomanswers end end 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 end 34 35 end procedure end Retire end Return BestAnswer cid7 No lifelines player risk RandomAnswers allows virtual player provide random answer question Retire allows virtual player retire win earned money The virtual player uses lifelines conﬁdence answers statement step 4 In case provided answer highest score step 33 If virtual player situation uncertainty explores available lifelines starting Poll Audience steps 511 If player conﬁdence answer provided lifeline step 8 returns answer step 9 continues explore available lifelines Steps 1218 manage Phone Friend lifeline allows return answer possibly provided friend step 16 The usage 5050 lifeline related level game reached player If user risk meaning money lost providing wrong answer question step 19 player uses 5050 lifeline step 20 The player chooses answer highest score step 23 conﬁdence answer randomly returns remaining answers step 25 If player rely lifelines conﬁdence answers provided lifelines returns randomly chosen answer case user risk steps 2830 retires game step 31 Given game played board game variant implemented speciﬁc strategy simulate actual way lifelines use board game implies interaction players 5050 simulated way real game removing wrong answers candidate answers As real game Phone Friend Poll Audience lifelines able return correct answer As regards Phone Friend worth notice usually higher level game diﬃcult answer question Hence lifeline works follows returns correct answer levels 1 5 randomly chooses alternatives providing correct answer returning answer levels 6 10 randomly chooses alternatives providing correct answer returning answer returning wrong answer levels 11 15 On Poll Audience simulated order distribute votes coming audience percentage candidate answers Without real audience simulated distribution votes strategy takes account current level question degree randomness We ﬁrst assign percentage votes correct answer representing percentage 170 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Fig 5 Percentage audience voting correct answer Poll Audience lifeline audience know correct answer randomly distribute remaining votes candidate answers The percentage votes assigned correct answer baseline inversely proportional level game diﬃcult question lower conﬁdence baseline percentage votes randomly perturbed according level game lower upper bound depicted Fig 5 It worth noting perturbation baseline percentage different questions level 1 5 6 10 11 15 For example player uses Poll Audience level 3 baseline percentage equal 54 randomly perturbed 49 74 remaining votes 51 26 randomly distributed remaining answers lifeline level 14 baseline percentage 32 randomly perturbed 22 37 remaining votes 78 63 randomly distributed remaining answers 8 Experimental evaluation The goal evaluation twofold 1 assess effectiveness Question Answering Answer Scoring modules answer questions game compare results obtained human players The experiment discussed Section 81 2 evaluate accuracy virtual player play game taking account strategy implemented Decision Making module compare results obtained human players The experiment discussed Section 82 We datasets containing questions WWBM oﬃcial Italian boardgame containing ques tions English boardgame3 Both datasets contain 1960 questions subdivided 15 groups level game The ﬁrst 10 levels contain 160 questions levels 11 12 13 14 15 contain 120 90 70 50 30 questions respectively 113 questions Italian 84 English negative form The distribution negative questions level game reported Fig 6 While previous results compare available English version game dataset aware previous results Italian baseline compare The metric adopted evaluation accuracy proportion correctly answered questions computed ratio number correct answers nc total number questions n accuracy nc n The signiﬁcance results assessed McNemars test application Bonferroni correction 81 Experiment 1 evaluation performance QA answer scoring The goal experiment assess accuracy ﬁve criteria answer scoring properly conﬁgured parameters described Section 6 number processed passages tested 10 conﬁgurations topn n 1 2 3 4 5 10 15 20 25 30 pas sages returned QA module use score w passages returned QA module tested 2 conﬁgurations weight passage returned QA module 3 Datasets available contacting authors P Molino et al Artiﬁcial Intelligence 222 2015 157181 171 Fig 6 Distribution negative questions level game Table 2 Performance top15 conﬁgurations averaged questions Acronyms P passages Lex Lexicalization KWD Keywords LEM Lemmas ST Stems Score Use score passage SW Use stopword removal QE Use question expansion Rank Criterion Italian dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 English dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Overlap Overlap Density Density Density Overlap Overlap Overlap Overlap Density Density Density Overlap Overlap Overlap Overlap Overlap Density Overlap Density Density Overlap Density Overlap Density Overlap Density Overlap Overlap Overlap P 25 25 3 30 30 20 20 30 30 20 20 25 15 15 20 25 25 3 20 30 30 30 20 15 25 30 20 20 20 15 Lex ST LEM KWD ST LEM ST LEM ST LEM ST LEM KWD ST LEM ST LEM ST KWD ST ST LEM ST ST LEM KWD LEM LEM LEM ST ST Score SW QE Accuracy Y Y Y Y Y Y Y Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y Y N Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y N N Y N N N N N N N N N N N N N N Y N N N N N N N N N N N N 6429 6429 6403 6403 6403 6378 6378 6378 6378 6327 6327 6301 6276 6276 6276 5947 5938 5926 5922 5908 5908 5899 5884 5872 5837 5835 5821 5814 5799 5797 level linguistic analysis adopted process passages returned QA module tested 3 conﬁgurations represent passages keywords stems lemmas We run 2 conﬁgurations stopwords removal use question expansion tested 2 conﬁgurations question expansion Overall set 1200 different conﬁgurations assign score possible answers question game starting scores answer highest selected The Question Answering conﬁgured ﬁlters listed Section 5 Table 2 reports accuracy best 15 conﬁgurations averaged set 1960 questions Italian English datasets 172 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Table 3 Best worst performance single criterion conﬁguration averaged questions Acronyms P passages Lex Lexi calization Score Use score passage SW Use stopword removal QE Use query expansion ES Exact Substring LCS Longest Common Subsequence TL Title Levenshtein KWD Keywords LEM Lemmas ST Stems Rank Criterion Italian dataset BEST WORST BEST WORST BEST WORST BEST WORST BEST WORST English dataset BEST WORST BEST WORST BEST WORST BEST WORST BEST WORST Overlap Overlap Density Density ES ES LCS LCS TL TL Overlap Overlap Density Density ES ES LCS LCS TL TL P 25 1 3 1 30 1 3 1 1 1 25 1 3 1 30 1 3 1 1 1 Lex ST KWD KWD LEM KWD KWD KWD LEM KWD LEM LEM KWD KWD ST KWD KWD KWD LEM KWD LEM Score SW QE Accuracy Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N Y N N N Y N N Y Y N Y N N N Y N N Y Y N N N Y N N N Y N Y N N N Y N N N Y N Y N 6429 4260 6403 4337 5918 4209 5714 4107 4005 2015 5947 3774 5926 3858 5462 3746 5219 3604 3517 1512 It worth note Overlap Density best performing criteria Italian English datasets top85 best conﬁgurations obtained criteria The analysis results unveils usefulness taking account considerable number passages usefulness leveraging score passages returned QA module effectiveness process stopwords removal Finally question expansion process negatively affect accuracy The worst criterion Title Levenshtein Italian English worst 100 results obtained criterion In order clear picture accuracy set conﬁgurations Table 3 reports criterion best worst conﬁguration We observe best worst conﬁgurations criterion Italian English pretty accuracy obtained English dataset 5 lower obtained Italian average Statistical tests signiﬁcant difference best conﬁguration Overlap best conﬁguration Density accuracy signiﬁcantly better best conﬁgurations criteria regardless language p 001 Starting accuracy single conﬁgurations tried combine order improve overall accuracy Answer Scoring Differently greedy combination adopted 37 work propose combination based pointwise learning rank approach regressionbased algorithms 29 question answer pairs q labeled relevance judgments answer respect question q In setting correct answer question game labeled relevance judgment 1 incorrect answers labeled 0 Each training example represented feature vector consisting level question 1 15 1200 single scores obtained mentioned conﬁgurations We opted Random Forests RF 7 algorithm ensemble learning method combining different tree regressors built different samples training data random subsets data features The ﬁnal result obtained averaging output single trees The use different data samples distribution different feature sets learning individual trees prevent overﬁtting We adopted implementation provided RankLib library4 Questions dataset split training set Tr test set Ts The methodology adopted obtaining Tr Ts stratiﬁed 5fold cross validation stratiﬁcation process ensures fold contains distribution questions different levels game Given size dataset 1960 questions applying 5fold cross validation means dataset divided 5 disjoint partitions containing 392 questions The experiment performed 5 steps At step 4 partitions training set Tr 1568 questions remaining partition test set Ts The steps repeated 5 disjoint partitions Ts 4 http sourceforge net p lemur wiki RankLib P Molino et al Artiﬁcial Intelligence 222 2015 157181 173 Table 4 Error analysis Italian English Error type Questions concepts occur WikipediaDBpedia Questions concepts explicit WikipediaDBpedia Questions involve numbers time math Questions require language proﬁciency Questions require comparison Questions require knowledge visual properties Other errors 13 15 13 14 750 750 30 results averaged 5 runs In order tune parameters learning rank number trees learning rate subsampling rate validation set obtained sampling questions training set run We sampled 125 training set 196 questions case took account distribution questions different levels stratiﬁcation To sum step training set contains 1372 questions validation set contains 196 questions remaining 392 questions test set The ﬁnal accuracy obtained combining individual conﬁgurations learning rank strategy equal 7964 Italian 7641 English averaged ﬁve runs signiﬁcantly better p cid10 00001 accuracy best single conﬁguration 6429 Italian 5947 English The result obtained English similar achieved 25 correctly answered 75 questions different smaller dataset More details accuracy different level diﬃculty game presented discussed Section 813 811 Unanswerable questions error analysis In order understand questions able provide correct answer classiﬁed speciﬁc categories performed proper error analysis The following list contains categories questions remain unanswerable Questions concepts occurring WikipediaDBpedia questions concern astrology proverbs sayings religion Questions concepts explicit WikipediaDBpedia able provide answer questions necessary information contained knowledge sources For example question Quale di questi attori non e ﬁglio darte English Which actors actor son actor father require match concept ﬁglio darte actor son actor father explicit WikipediaDBpedia Questions answers involve special numbers periods time mathematical computations fails provide answer contains Roman numerals implies computation time frames mathematical computations For example candidate answers question In quale secolo fu costruita la prima penna sfera English In century ﬁrst ballpoint pen built A XX B XVII C XVIII D XIX able ﬁnd match candidate answers question How long William Harrison oﬃce ninth president USA involves computation time frame able carry question How degrees angles isosceles triangle angle 120 requires knows total angles Euclidean triangle sum 180 Questions require language proﬁciency questions Nella lingua latina il vocativo plurale e sempre uguale che cosa English In Latin language vocative plural equal require speciﬁc knowledge encoded WikipediaDBpedia Questions require comparison questions Quale tra queste regioni italiane ha la superﬁcie minore English Which regions Italy lowest surface require comparison selection minimum value Questions require knowledge visual properties questions Quale di queste squadre di calcio non ha come simbolo esemplare di lupo Which football teams wolf symbol require knowledge visual properties The error analysis carried results experiments previous section depicted Table 4 30 errors wrong scoring answers factors heuristics recognize manage negative questions Section 6 Hence order evaluate effective strategy evaluated hand accuracy regular expressions correctly detecting negative questions hand effectiveness reverse scoring method The accuracy heuristics based use regular expressions F 1 8923 Italian dataset F 1 9820 English reverse scoring method evaluated taking account percentage negative questions reverse scoring strategy useful answer lowest score correct harmful answer lowest score wrong indifferent correct answer highest score lowest score Italian dataset percentages 80 174 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Fig 7 Decrease accuracy ablation feature groups For group number features removed reported 9 11 respectively English dataset percentages 96 1 3 respectively This means reverse scoring method effective practice Finally order details DBpedia beneﬁt computed number questions QA module returned passages coming DBpedia 290 questions Italian dataset 293 English rely passages coming DBpedia coming Wikipedia means answer questions potentially extracted DBpedia For sake completeness evaluated accuracy Rocchio classiﬁer described Section 51 allows map different lexicalizations questions asking speciﬁc property corresponding DBpedia property The accuracy classiﬁer computed leaveoneout 8559 English dataset 8357 Italian 812 Ablation tests To gain insights power different parameters conﬁgure answer scoring criterion formed feature selection ablation tests removing single features groups features As regards ablation single features trained different model removing feature related single conﬁguration measuring corresponding predictive accuracy learning rank model process peated 1200 times The analysis results unveils 160 1200 times accuracy learned models lower obtained best conﬁguration regardless language adopted The maximum decrease accuracy 484 Italian 500 English It interesting note 160 learned models feature corresponding conﬁguration adopting keywords representing passages removed Hence signal brought keywords relevant brought stems lemmas We performed ablation tests removing groups features We deﬁned following different groups Five groups corresponding different answer scoring criterion group contains features corre sponding conﬁgurations single criterion Three groups corresponding different level linguistic analysis adopted processing passages text returned QA module Two groups containing conﬁgurations question expansion strategy Two groups containing conﬁgurations number passages 1 5 10 30 respec tively Fig 7 reports decrease accuracy obtained different models trained removing single group features Groups higher values better meaning features higher impact overall performance model ablation leads higher decrease accuracy All results statistically signiﬁcant compared best conﬁguration obtained learning rank strategy p cid10 00001 P Molino et al Artiﬁcial Intelligence 222 2015 157181 175 Fig 8 Per level accuracy human performance Italian As regards groups corresponding answer scoring criteria best performance obtained LCS followed TL Density Overlap ES A comparison performance single conﬁgurations reported Table 2 Table 3 shows Overlap Density criteria best performing individually individually removed overall performance signiﬁcantly different This supports ﬁnding signal bring overlapping On TL criterion worst performing individually individually removed leads decrease overall accuracy greater 5 As regards linguistic analysis adopted processing passages text returned QA module keywordbased representations best performance followed stems lemmas behave similar way This observed removing conﬁguration time ablation conﬁgurations representations based stems lemmas lead decrease accuracy As regards question expansion mechanism methods adopting slightly better performance 1 adopting line performance single conﬁgurations observed question expansion impact accuracy Finally conﬁgurations fewer number passages 1 5 better passages 10 30 This contradict ﬁnding stemmed analysis top15 conﬁgurations Table 2 best performance obtained 15 30 passages A possible interpretation information overlap existing 15 30 passages higher existing 1 5 passages led combination effective 813 Per level analysis human performance We compared performance QA Answer Scoring human players provide answers questions different levels diﬃculty Playing successfully WWBM game heavily depends players knowledge popular culture comparison human players performed Italian dataset To purpose involved 98 human players selected availability sampling strategy 46 Italian students graduates The dataset 1960 Italian questions randomly split 98 disjoint sets 20 questions set assigned different user provided answers having possibility consult Web knowledge sources level question disclosed users As baseline queried Google question retrieved top30 snippets text returned search engine Hence computed score candidate answer multiplying number times answer occurred snippet inverse rank snippet Finally selected candidate answer highest score random selection adopted break ties We refer Google baseline As Google queries Web use information available Wikipedia decided compare fairer baseline We queried Google restricting results pages coming Wikipedia retrieving top30 snippets scoring way Google baseline We refer Wikipedia baseline Figs 8 9 report level game 1 accuracy best conﬁguration obtained learning rank strategy Italian English average accuracy 7964 σ 607 Italian 7641 σ 145 English 2 accuracy achieved human players 5133 σ 1761 Italian 3 accuracy Google baseline 6713 Italian 7180 English 4 accuracy Wikipedia baseline 5171 Italian 6065 English The accuracy signiﬁcantly better Google baseline Wikipedia baseline accuracy human players p cid10 00001 The similar performance levels game As regards Italian dataset best perfor mance obtained level 2 worst obtained levels 7 13 However error analysis report 176 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Fig 9 Per level accuracy performance English speciﬁc problem levels observed higher concentration questions able deal Section 811 ﬁve folds As regards English performance constant levels game standard deviation low Accuracy human players Italian decreases monotonically best performance obtained ﬁrst level game worst level 14 This observation coherent fact lower levels game correspond easier questions higher levels correspond complex questions deeper knowledge required provide correct answers The Italian signiﬁcantly outperforms humans 15 levels game average It worth analyze performance players groups questions 1 5 6 10 11 15 respectively reach guarantee points money earned banked As regards ﬁrst group questions humans obtain worst performance fourth ﬁfth question 6188 surprising ﬁrst point players guarantee earned money Even level questions disclosed human players hypothesis milestone questions likely diﬃcult way game designed Surprisingly behavior observed second milestone question albeit performance question average Results obtained humans group questions average comparable obtained absolute difference performance ranging 38 56 The Google baseline better human players Italian accurate absolute difference performance amounts 1251 Italian 461 English average The Wikipedia baseline accurate Google baseline Italian English slightly accurate human players average Italian To sum virtual player built QA Answer Scoring modules potential beat human players playing real game rules able correctly answer questions different levels diﬃculty We observe similar performance regardless language Question Answering framework Answer Scoring criteria work way Italian English The small differences performance likely different number documents extracted Wikipedia million Italian millions English impact performance search engines QA framework course fact datasets comparable However playing real game complex task requires proper strategy manage lifelines decide answer question retire game taking earned money This purpose experiment described section 82 Experiment 2 evaluation virtual player The goal experiment evaluate ability virtual player play game implementing proper strategy use lifelines decide answer question condition uncertainty retire game taking earning money We ran experiment comparing performance human players performance achieved best conﬁguration obtained learning rank approach strategy play game deﬁned decision making algorithm described Section 7 The comparison carried evaluating level reached game money earned players5 As previous experiment comparison performance virtual player humans carried Italian 5 The Italian English versions game different currency different distribution monetary values level game For sake comparison decided use currency monetary values Italian P Molino et al Artiﬁcial Intelligence 222 2015 157181 177 Fig 10 Plot average income different values threshold measuring difference score best second best candidate answer Fig 11 Distribution levels reached game money earned players log scale Upper lower ends boxes represent 3rd 1st quartile respectively Whiskers extend extreme data point 15 times interquartile range Median values depicted solid lines mean values solid points We involved 35 subjects different involved previous experiment overall played 325 games Each game consists questions selected Game Manager randomly sampled question level game proceeds games played human player different questions level On virtual player played 160 games questions selected Game Manager Italian English datasets An important setting conﬁgure virtual player value threshold decision making algorithm assessing situation uncertainty difference score best second best candidate answer Section 7 The threshold value optimized empirically varying value selecting led virtual player obtain highest average income questions validation set ﬁrst fold Threshold ﬁnally set 02 Italian English plot showing different values affect average income depicted Fig 10 Fig 11 shows boxplots levels reached game money earned players Figs 12 13 report distribution games reaching speciﬁc level distribution games ended income speciﬁc interval All players able reach level game average level reached humans ﬁve 565 respect virtual player reaches level seven 788 760 Italian English respectively This highlighted median value ﬁfth level humans seventh virtual player More half times 5138 human players ended game reaching levels 1 5 40 times reached levels 6 10 Few times 862 humans able reach levels level 15 reached The distribution games ended virtual player groups questions uniform Italian English slightly better performance Italian coherent results Figs 8 9 accuracy similar levels game 301 325 games 9261 played humans ended error response 24 times 739 players ended game retiring taking earned money cases players retired game level 6 11 wrong answer effect earned money Moreover human players ended game maximum prize Differently humans virtual player able successfully complete game Indeed earned e 1000000 17 times 1062 Italian 12 times 750 English 116 games 7250 ended wrong answer virtual player Italian 27 times 1687 retired game 178 P Molino et al Artiﬁcial Intelligence 222 2015 157181 Fig 12 Distribution games reaching speciﬁc level Fig 13 Distribution games ended income speciﬁc interval Fig 14 Distribution lifelines game providing answer The virtual player English ended 114 games 7125 wrong answer retired game 34 times 2125 The highest percentage games ended virtual player providing answer highlights conservative low risk strategy It interesting note decision making algorithm allow virtual player end game level 6 11 The money earned humans e 5926 average average income virtual player signiﬁcantly higher Indeed earned e 114531 Italian e 88878 English The detailed ﬁgures shown Fig 13 games ended null income reported games ended interval corresponding milestone questions Most games played humans 8830 ended null income answering questions ﬁrst group This means reached groups questions 1170 times differently virtual player reached groups questions 40 times Italian English The fewer percentage games ended null income virtual player conﬁrms risk averse behavior albeit differences Italian English exist coupled ability provide correct answers regardless level game allows end games higher average income Fig 14 reports use lifelines different stages game The comparison strategies adopted humans virtual player fair uses static order available lifelines deﬁned decision making algorithm Poll Audience Phone Friend 5050 Despite limitation overall support provided lifelines virtual player valuable Indeed virtual player able provide correct answer 114 times P Molino et al Artiﬁcial Intelligence 222 2015 157181 179 Italian 141 times English thanks lifelines Human players virtual player use single lifeline game average behave similar way During ﬁrst stages game frequency usage Poll Audience virtual player 4286 Italian English cumulative frequency use available lifelines human players 4111 This means virtual player forced use lifeline deﬁned decision making algorithm human players use available lifelines prefer Poll Audience ﬁrst stages game The frequency use Phone Friend humans virtual player similar As expected 5050 virtual player ﬁrst levels game humans uniformly levels game To sum expected results Experiment 1 virtual player able outperform humans adopts simple decision making strategy The better performance terms average reached level earned money end game The performance virtual player Italian English pretty average income signiﬁcantly better Italian English As observed performance QA Answer Scoring likely different size knowledge sources Italian English course different datasets experiment 9 Conclusions In work investigated issues To extent QA designed languageindependent way preserving effectiveness Can Wikipedia DBpedia serve effective knowledge bases answering WWBM game questions As regards ﬁrst issue work actually led deﬁnition effective languageindependent framework QA Question Answering Answer Scoring modules deﬁned set ﬁlters related speciﬁc language We deﬁned effective strategy combine different criteria scoring candidate answers machine learning techniques Experiments performed Italian English showed effectiveness ap proach As regards second issue built virtual player WWBM game based following modules Question Answering able retrieve passages text relevant speciﬁc question expressed natural language Wikipedia DBpedia open knowledge sources Answer Scoring implements heuristics based analysis results returned Question Answering module order assign score candidate answers Decision Making chooses strategy play game exploiting scores candidate answers availability lifelines current level game The virtual player outperformed human players terms average accuracy correctly answering questions game terms ability play real games rules We plan enhance decision making algorithm order allow smarter management lifelines conservative strategy lead risk neutral player hope reﬁnements improve overall accuracy Moreover speciﬁc heuristics devised answer questions currently unanswerable Section 811 order improve performance References 1 N Aggarwal P Buitelaar A description natural language query DBpedia C Unger P Cimiano V Lopez E Motta P Buitelaar R Cyganiak Eds Proceedings Interacting Linked Data ILD 2012 Workshop CoLocated ESWC 2012 CEUR Workshop Proceedings vol 913 2012 pp 9699 2 K Ahn J Bos D Kor M Nissim BL Webber JR Curran Question answering QED TREC 2005 EM Voorhees LP Buckland Eds Pro ceedings Fourteenth Text REtrieval Conference TREC 2005 Special Publications vol 500266 National Institute Standards Technology NIST 2005 3 P Basile M Gemmis P Lops G Semeraro Solving complex language game knowledgebased word associations discovery IEEE Trans Comput Intell AI Games 2015 httpdxdoiorg101109TCIAIG20142355859 press 4 J Berant A Chou R Frostig P Liang Semantic parsing freebase questionanswer pairs Proceedings 2013 Conference Empirical Methods Natural Language Processing EMNLP 2013 1821 October 2013 Grand Hyatt Seattle Seattle Washington USA ACL 2013 pp 15331544 meeting SIGDAT special group ACL 5 C Bizer The emerging Web linked data IEEE Intell Syst 24 2009 8792 6 E Breck JD Burger L Ferro L Hirschman D House M Light I Mani How evaluate question answering day real work Proceedings Second International Conference Language Resources Evaluation LREC 2000 European Language Resources Association 2000 pp 14951500 7 L Breiman Random forests Mach Learn 45 2001 532 8 E Cabrio AP Aprosio J Cojan B Magnini F Gandon A Lavelli QAKiSQALD2 C Unger P Cimiano V Lopez E Motta P Buitelaar R Cyganiak Eds Proceedings Interacting Linked Data ILD 2012 Workshop CoLocated EWSC 2012 CEUR Workshop Proceedings vol 913 2012 pp 8795 180 P Molino et al Artiﬁcial Intelligence 222 2015 157181 9 JJ Castillo The contribution FaMAF QACLEF 2008 answer validation exercise Working Notes CLEF 2008 Workshop 2008 pp 1719 10 J Chen A Diekema MD Taffet NJ McCracken NE Ozgencil O Yilmazel ED Liddy Question answering CNLP TREC10 question answering track Text Retrieval Conference Special Publications vol 500274 National Institute Standards Technology NIST 2001 11 P Cimiano M Minock Natural language interfaces problem datadriven quantitative analysis H Horacek E Métais R Muñoz M Wolska Eds Natural Language Processing Information Systems 14th International Conference Applications Natural Language Infor mation Systems NLDB 2009 Lecture Notes Computer Science vol 5723 Springer 2010 pp 192206 revised papers 12 I Dagan O Glickman B Magnini The PASCAL recognising textual entailment challenge JQ Candela I Dagan B Magnini F dAlché Buc Eds Machine Learning Challenges Evaluating Predictive Uncertainty Visual Object Classiﬁcation Recognizing Textual Entailment First PASCAL Machine Learning Challenges Workshop Lecture Notes Computer Science vol 3944 Springer 2005 revised selected papers 13 D Damljanovic M Agatonovic H Cunningham FREyA interactive way querying linked data natural language R GarciaCastro D Fensel G Antoniou Eds The Semantic Web ESWC 2011 Workshops Lecture Notes Computer Science vol 7117 Springer 2012 pp 125138 revised selected papers 14 S Dumais M Banko E Brill J Lin A Ng Web question answering better Proceedings 25th ACM SIGIR Conference SIGIR02 15 M Ernandes G Angelini M Gori WebCrow Webbased crossword solving MM Veloso S Kambhampati Eds AAAI AAAI PressThe ACM New York NY USA 2002 pp 291298 MIT Press 2005 pp 14121417 16 A Fader L Zettlemoyer O Etzioni Open question answering curated extracted knowledge bases SA Macskassy C Perlich J Leskovec W Wang R Ghani Eds The 20th ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD 2014 ACM 2014 pp 11561165 17 D Ferrucci A Levas S Bagchi D Gondek ET Mueller Watson Jeopardy Artif Intell 2013 93105 18 DA Ferrucci EW Brown J ChuCarroll J Fan D Gondek A Kalyanpur A Lally JW Murdock E Nyberg JM Prager N Schlaefer CA Welty Building Watson overview DeepQA project AI Mag 31 2010 5979 19 E Gabrilovich S Markovitch Computing semantic relatedness Wikipediabased explicit semantic analysis MM Veloso Ed Proceed ings 20th International Joint Conference Artiﬁcial Intelligence IJCAI 2007 Hyderabad India January 612 2007 Morgan Kaufmann 2007 pp 16061611 20 I Glöckner University Hagen QACLEF answer validation exercise Working Notes CLEF 2008 Workshop 2008 pp 1719 21 SM Harabagiu S Maiorano Finding answers large collections texts paragraph indexing abductive inference Proceedings AAAI Fall Symposium Question Answering AAAI 1999 pp 6371 22 SM Harabagiu DI Moldovan M Pa sca R Mihalcea M Surdeanu RC Bunescu R Girju V Rus P Morarescu FALCON boosting knowledge answer engines Text Retrieval Conference Special Publications vol 500249 National Institute Standards Technology NIST 2000 pp 479488 23 SM Harabagiu M Pa sca SJ Maiorano Experiments opendomain textual question answering Proceedings 18th Conference Com putational Linguistics Volume 1 COLING00 Association Computational Linguistics Stroudsburg PA USA 2000 pp 292298 24 EH Hovy L Gerber U Hermjakob M Junk CY Lin Question answering Webclopedia Text Retrieval Conference Special Publications vol 500249 National Institute Standards Technology NIST 2000 pp 655664 25 SK Lam DM Pennock D Cosley S Lawrence 1 billion pages 1 million dollars Mining Web play Who Wants Be Millionaire C Meek U Kjærulff Eds Proceedings 19th Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann 2003 pp 337345 26 JJ Lin An exploration principles underlying redundancybased factoid question answering ACM Trans Inf Syst 25 2007 27 ML Littman Review language games TA Marsland I Frank Eds Computers Games 2nd Int Conf Lecture Notes Computer Science vol 2063 Springer 2000 pp 396404 revised papers 28 ML Littman GA Keim N Shazeer A probabilistic approach solving crossword puzzles Artif Intell 134 2002 2355 29 T Liu Learning Rank Information Retrieval Springer 2011 30 V Lopez M Fernández E Motta N Stieler PowerAqua supporting users querying exploring Semantic Web Semant Web 3 2012 249265 31 V Lopez C Unger P Cimiano E Motta Evaluating question answering linked data J Web Semant 21 2013 313 32 B Magnini M Negri R Prevete H Tanev Is right answer Exploiting Web redundancy answer validation Proceedings 40th Annual Meeting Association Computational Linguistics ACL 2002 pp 425432 33 CD Manning P Raghavan H Schtze Introduction Information Retrieval Cambridge University Press New York NY USA 2008 34 M Maybury O Stock W Wahlster Intelligent interactive entertainment grand challenges IEEE Intell Syst 21 2006 1418 35 P Molino P Basile QuestionCube framework question answering G Amati C Carpineto G Semeraro Eds Proceedings 3rd Italian Information Retrieval IIR Workshop Bari Italy January 2627 2012 CEUR Workshop Proceedings vol 835 CEURWSorg 2012 pp 167178 36 P Molino P Basile A Caputo P Lops G Semeraro Exploiting distributional semantic models question answering IEEE International Conference Semantic Computing IEEE 2012 pp 146153 37 P Molino P Basile C Santoro P Lops M Gemmis G Semeraro A virtual player Who Wants Be Millionaire based question answering M Baldoni C Baroglio G Boella R Micalizio Eds AIIA 2013 Advances Artiﬁcial Intelligence XIIIth International Conference Italian Association Artiﬁcial Intelligence Lecture Notes Computer Science vol 8249 Springer 2013 pp 205216 38 C Monz Minimal span weighting retrieval question answering R Gaizauskas M Greenwood M Hepple Eds Proceedings SIGIR 2004 Workshop Information Retrieval Question Answering pp 2330 39 M Pa sca OpenDomain Question Answering Large Text Collections Studies Computational Linguistics CSLI Publications 2003 40 A Peñas EH Hovy P Forner Á Rodrigo RFE Sutcliffe R Morante QA4MRE 20112013 overview question answering machine reading evaluation P Forner H Müller R Paredes P Rosso B Stein Eds Information Access Evaluation Multilinguality Multimodality Visualization 4th International Conference CLEF Initiative CLEF 2013 Lecture Notes Computer Science vol 8138 Springer 2013 pp 303320 41 F Perea J Puerto Dynamic programming analysis TV game Who Wants Be Millionaire Eur J Oper Res 183 2007 805811 42 S Robertson H Zaragoza The probabilistic relevance framework BM25 Found Trends Inf Retr 3 2009 333389 43 J Rocchio Relevance feedback information retrieval G Salton Ed The SMART Retrieval System Experiments Automated Document Processing PrenticeHall Englewood Cliffs NJ 1971 pp 313323 44 Á Rodrigo A Peñas F Verdejo UNED answer validation exercise 2007 C Peters V Jijkoun T Mandl H Müller DW Oard A Peñas V Petras D Santos Eds Advances Multilingual Multimodal Information Retrieval 8th Workshop CrossLanguage Evaluation Forum CLEF 2007 Lecture Notes Computer Science vol 5152 Springer 2008 pp 404409 revised selected papers 45 Á Rodrigo A Peñas F Verdejo Overview answer validation exercise 2008 C Peters T Deselaers N Ferro J Gonzalo GJF Jones M Kurimo T Mandl A Peñas V Petras Eds Evaluating Systems Multilingual Multimodal Information Access 9th Workshop CrossLanguage Evaluation Forum CLEF 2008 Lecture Notes Computer Science vol 5706 Springer 2009 pp 296313 revised selected papers 46 S Royce B Straits Approaches Social Research 3rd edition Oxford University Press New York 1999 47 M Sahlgren An introduction random indexing Methods Applications Semantic Indexing Workshop 7th International Conference Terminology Knowledge Engineering 2005 P Molino et al Artiﬁcial Intelligence 222 2015 157181 181 48 G Semeraro M Gemmis P Lops P Basile An artiﬁcial player language game IEEE Intell Syst 27 2012 3643 49 JA Shaw EA Fox Combination multiple searches Proceedings Second Text REtrieval Conference TREC2 Special Publications vol 500215 National Institute Standards Technology NIST 1994 pp 243252 50 A Singhal C Buckley M Mitra Pivoted document length normalization Proceedings 19th ACM SIGIR Conference SIGIR96 ACM New York NY USA 1996 pp 2129 51 P Smolensky Tensor product variable binding representation symbolic structures connectionist systems Artif Intell 46 1990 159216