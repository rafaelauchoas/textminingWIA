Artiﬁcial Intelligence 227 2015 165189 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Differential evolution noisy multiobjective optimization Pratyusha Rakshit Amit Konar Electronics Telecommunication Engineering Department Jadavpur University Kolkata 700032 India r t c l e n f o b s t r c t Article history Received 20 December 2013 Received revised form 15 June 2015 Accepted 15 June 2015 Available online 18 June 2015 Keywords Noise Differential evolution multiobjective optimization Sampling Interquartile range Skewness Dominance probability We propose extension multiobjective optimization realized differential evolution algorithm handle effect noise objective functions The proposed extension offers merits respect traditional counterpart First adaptive selection sample size periodic ﬁtness evaluation trial solution based ﬁtness variance local neighborhood proposed This avoids computational complexity associated unnecessary reevaluation quality solutions disregarding necessary evaluations relatively poor solutions ensure accuracy ﬁtness estimates The second strategy concerned determining expected value noisy ﬁtness samples basis distribution instead conventional averaging ﬁtness measure trial solutions Finally new crowdingdistance induced probabilistic selection criterion devised promote quality solutions rank candidate pool generation ensuring population quality diversity objective spaces Computer simulations performed noisy version wellknown set 23 benchmark functions reveal proposed algorithm outperforms competitors respect inverted generational distance spacing error ratio hypervolume ratio metrics 2015 Elsevier BV All rights reserved 1 Introduction The multiobjective optimization MOO literature witnessed radically different perspective solving realworld problems evolutionary computing methods A MOO concerned mathematical optimization problems involving complex nonlinear conﬂicting objectives optimized simultaneously Usually derivativefree single objective optimization algorithm generates new trial solutions biased better region objective space weeds poor solutions competitive selection iterations However nontrivial MOO problem exists single solution simultaneously optimizes objective To jointly optimize multiple objective functions MOO 1 selection trial solutions performed Pareto ranking concerned judiciously identify ing nondominated trial solutions rest population Pareto ranking induced ﬁtness measure objective functions individual trial solutions The objectives functions certain variables describing speciﬁc problem usually return unique value variables argument However scientiﬁcengineering problems observed measurements variables remain constant objective functions return different values noiseinduced dynamic variation objective surfaces This class problem referred noisy optimization problem Noise Corresponding author Tel 91 9477399645 Email address pratyushar1gmailcom P Rakshit httpdxdoiorg101016jartint201506004 00043702 2015 Elsevier BV All rights reserved 166 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 creeps picture technological limitations modeling errors incomplete data leading different results repeated evaluations set parameter values objective functions In circumstances quality trial solution MOO deprived promoted generation poor noisy ﬁtness estimates deceptive solution illusive good ﬁtness discarded current population 23 This paper addresses issues uncertainty management selection qualitative trial solutions MOO presence noise incorporating following policies adaptation sample size trial solution periodic ﬁtness evaluation expected ﬁtness estimation measured noisy ﬁtness samples crowdingdistance induced stochastic selection First sample size periodic ﬁtness evaluation trial solution adapted means ﬁtness variance local neighborhood Sampling refers periodic ﬁtness evaluation trial solution diminish risk promoting inferior solutions noisy environment It worth mentioning adaptive selection sample size momentous increasing sample size augments quality measure ﬁtness cost additional runtime Here nonlinear form capturing relationship sample size trial solution ﬁtness variance local neighborhood induced exponential function regarded eﬃciently balance tradeoff runtime complexity computational accuracy Second measuring ﬁtness trial solution traditional methods 46 refer average ﬁtness samples However average ﬁtness presumes equal probability occurrence ﬁtness samples returns poor ﬁtness estimate noise variance ﬁtness measure solutions local neighborhood selected trial solution large This problem circumvented referring expected value ﬁtness samples true ﬁtness estimate trial solution The expected ﬁtness concerned occurrence probability ﬁtness samples better ﬁtness measure given trial solution We introduce novel strategy evaluate expected ﬁtness trial solutions distribution ﬁtness samples entire sample space In present context densitybased nonuniform partitioning ﬁtness sample space employed capture uncertainty involved ﬁtness measurement noisy ﬁtness samples Finally develop probabilistic selection PS policy encapsulate diversity quality non dominated trial solutions noisy ﬁtness landscapes It observed deterministic selection scheme crowdingdistancebased sorting promoting trial solutions rank candidate pool gen eration employed traditional MOO algorithms lead suboptimal misleading sets nondominated solutions noisy environment sampling 7 The selection strategy depends density nondominated solutions surrounding individual objective space reliability mea sured ﬁtness samples We develop new probabilistic measure reliability based skewness distribution ﬁtness samples The degree asymmetry distribution ﬁtness samples captured skewness Con sequently provides unique approach identifying rare ﬁtness samples lying tail distribution These infrequent samples far away expected ﬁtness assumed occur creeping noise ﬁtness landscapes The rarer occurrence infrequent samples closer ﬁtness samples expected value small skewness distribution greater degree credibility ﬁtness estimates given trial lution The trial solutions having greater crowding distance high grade reliability assessed probability occurrence rare samples given precedence ranking solutions The evolutionary component proposed noisy MOO algorithm realized differential evolution MOO DEMO 8 algorithm proven merits global optimization Some attractive features DEMO justi fying selection design proposed noisy optimization algorithm include simplicity structure leading ease coding control parameters faster convergence 4849 comparison MOO algorithms Performance analysis proposed noisy optimization algorithm realized DEMOreferred differential evolution noisy MOO DENMO henceforthis studied noisy version set 23 benchmark functions Exper iments undertaken compare potency proposed algorithm differential evolution MOO noise DEMON 9 nondominated sorting genetic algorithm II NSGAII αdominance operator NSGAIIA 10 conﬁdence based dynamic resampling CDR 11 simulated annealing noisy MOO 12 elitist evolutionary multiagent 13 multiobjective evolutionary algorithm robust features MOEARF 14 modiﬁed NSGAII 7 noisetolerant strength Pareto evolutionary algorithm 15 Pareto fronteﬃcient global optimization 16 In study objective functions contaminated noise samples taken ﬁve noise distributionsnamely Gaussian Poisson Rayleigh exponential random positive negative expeditions noise amplitude 25 true ﬁtness function values Experiments reveal proposed realization outperforms algorithms important performance metricsthat inverted generational distance IGD spacing error ratio ER hypervolume ratio HVR The paper divided seven sections Section 2 brieﬂy reviews literature strategies adopted evolu tionary algorithms solve noisy MOO problems In Section 3 provide overview MOO DEMO algorithm Section 4 provides noise handling mechanism DENMO The experimental settings benchmarks simu lation strategies explained Sections 5 6 respectively Conclusions given Section 7 2 Literature review Recently researchers interested developing robust MOO algorithms search optimal solu tions deceived noise Stagge et al 17 employed concept sampling ﬁtness reevaluation P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 167 trial solutions enhance objective estimates presence noise The average ﬁtness samples provides relatively safe measure ﬁtness trial solution reducing possible risk certifying lowgrade trial solution enter optimal Pareto Syberfeldt et al 11 proposed CDR strategy adaptively select sample size Welch conﬁdence interval test For userdeﬁned conﬁdence level resampling solutions performed iteratively noise suﬃciently reduced assessed Welch conﬁdence interval Inspired role momentum term backpropagation neural networks Goh Tan 14 adopted experientiallearningdirected perturbation strategy govern candidate movement direction ﬁtness improvement basis information acquired generations faster convergence Moreover tradeoff explorative exploitative capabilities MOO balanced proposed gene adaptation selection strategy Finally pos sibility necessity measure induced possibilistic archiving method resolve uncertainty including solution nondominated set Babbar et al 7 recommended new Pareto ranking strategy approve ingress apparently inferior trial solutions optimal Pareto satisfying statistical comparison The comparison concerned difference mean values ﬁtness samples individual solutions respect scaled average ﬁtness variance The executed ranking policy prevents probable dismissal quality solutions optimal Pareto true potential solutions incomprehensible noise However inadequacy strategy lies context uniform sampling The scope new noiseaware αdominance operator explored Boonma et al 10 evaluate dominance relationship conﬁdence level α individuals basis measurement noisy ﬁtness samples The merit proposed strategy lies prerequisite knowledge noise distribution objective space needed The soft selection scheme multiagent employed Siwik et al 13 ensures survival quality solution agent seemingly poor ﬁtness estimates noisy uncertain environment Some resources denoted life energy assigned solution agent basis ﬁtness life energy indirect measure agents survival offspringgeneration capabilities reduced reallocation resources dominating agents vice versa Hence quality solution dominated loses ﬁnite energy prolong functionality energy reduced zero The strategy guarantees little inﬂuence noise quality solution opportunity enhance energy future communicating agents dominates Buche et al 15 proposed noisetolerant MOO algorithm Here population member assigned dominancedependant lifetime varies inversely number solutions dominates Thus strategy defends overall population impact illusive ﬁtness unreliable solutions Conversely survival quality solutions ensured reevaluating archived solutions expired lifetime adding current population Finally loss information noisy environments hindered appending solutions nonexpired lifetime archive nondominated members Rakshit et al 9 implemented nonuniform sampling strategy utilizing linear relationship sample size trial solution ﬁtness variance local neighborhood Moreover statistical expectation ﬁtness samples referred ﬁtness measure trial solutions On basis proposed strategies Goldbergs method 7 extended verify possible accommodation slightly inferior population members optimal Pareto Das et al 18 integrated thresholdbased selection policy randomscalefactorinduced stochastic model differential evolution proﬁciently explore dynamic ﬁtness landscape contaminated noise The strategy promotes offspring vector generation ﬁtness better respective parent certain threshold value This turn circumvents approval misleading solutions generations A probabilistic dominance approach adopted Hughes 19 deal uncertainty decisions concerned supremacy trial solution context noisy ﬁtness landscapes The placement trial solutions Pareto fronts governed quantiﬁed dominance probability Singh et al 20 utilized hypothesis test based Student t distribution stochastic selection nondominated solutions Knowles Hughes 16 Gaussian processassisted surrogate model discover competent solution possessing largest expected improvement minimum cost It designed aim optimization restricted evaluation budget weighing predicted ﬁtness solutions associated error Knowles et al 21 proposed ﬁtnessdistancebased correlation strategy quality performance MOO problem presence noise constrained limited evaluation budget Mattila et al 12 devised new criterion selectively include candidate solutions optimal Pareto based probability solution dominated nondominated solutions current generation In addition generation offsprings guided information content nondominated solutions previous generations The strategy proposed realized context simulated annealing 3 Preliminaries In section provide overview important conceptsMOO DEMOwhich referred frequently rest paper develop solution noisy MOO problems 168 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 31 Multiobjective optimization The principal objective MOO algorithms attain Pareto optimal solutions single run The following deﬁnitions explain DEMO extension Deﬁnition 1 Let consider trial solutions cid3Xi cid3X j MOO problem N objectives need minimized The kth objectives cid3Xi cid3X j represented fk cid3Xi fk cid3X j respectively Then cid3Xi said dominate cid3X j denoted cid3Xi cid3X j following conditions jointly satisﬁed 1 cid3Xi worse cid3X j objective functionsthat fk cid3Xi fk cid3X j k 1 N 2 cid3Xi strictly better cid3X j objective functionthat fl cid3Xi fl cid3X j l 1 N Deﬁnition 2 In MOO trial solution cid3X said nondominated cid3X dominates cid3X Deﬁnition 3 Let P set potential solutions MOO problem P members P dominated member P cid7 cid7 P called optimal Pareto set solutions Deﬁnition 4 Let f 1 f 2 objective functions MOO problem let cid3X cid3Xi cid3X j members nondominated list solutions Furthermore cid3Xi cid3X j nearest neighbors cid3X objective spaces The crowding distance trial solution cid3X nondominated set depicts perimeter hypercube formed nearest neighborsthat cid3Xi cid3X j vertices ﬁtness landscapes In words crowding distance cid3X computed CD cid3X f 1 cid3Xi f 1 cid3X j f 2 cid3Xi f 2 cid3X j 32 Differential evolution MOO DEMO 8 employs evolutionary strategy utilizes advantages differential evolution 22 mech anisms Paretobased ranking crowding distance sorting An overview main steps DEMO algorithm presented cid2 cid3 Initialization The ith member cid3Xit current population P t uniformly randomizing individuals range cid3X min cid3X max cid3X min xmin xmax 1 xmin 2 jth component ith member t 0 initialized xmax xmax 2 D 1 1 NP generation t 0 selected cid3X max xmin D xi j0 xmin randi j0 1 j 1 j 1 D Here randi j0 1 uniformly distributed random number lying 0 1 The crossover rate initialized 0 1 The kth objective function fk cid3Xi0 evaluated target vector cid3Xi0 1 NP k 1 N b Mutation A donor vector cid3V corresponding cid3Xit created randomly selecting members xmin j xmax j cid3Xrand1t cid3Xrand2t cid3Xrand3ti cid9 rand1 cid9 rand2 cid9 rand3 P t cid3V cid3Xrand1t F cid2 cid3 cid3Xrand2t cid3Xrand3t 2 F scaling factor 0 2 The mutation operation given 2 referred DErand1 23 This 1 NP There mutation operations details available 23 c Crossover There types crossoverbinomial exponential 23 In case binomial crossover trial vector cid3U generated pair cid3V cid3Xit following operation cid4 cid4 ui jt v jt xi jt randi j CR j jrand j 1 D 3 jrand 1 D randomly chosen index In case exponential crossover randomly select integer n 1 D It represents starting point target vector cid3Xit commence exchange components corresponding donor vector cid3V t We select integer L 1 D L denotes number components donor vector contributes target The selection L given 24 Now obtain trial vector cid3U ui jt v jt xi jt j cid11ncid12D cid11n 1cid12D cid11 n L 1cid12D j 1 D j 1 D 4 cid11 cid12D denotes modulo function modulus D The kth objective function fk cid3U evaluated trial vector cid3U 1 NP k 1 N P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 169 d Selection Now trial vector cid3U replaces corresponding target vector cid3Xit cid3U cid3Xit based mea sures objective functions However cid3U cid3Xit nondominated cid3U inserted current population P t cid3U discarded This assists faster convergence true Pareto This step repeated trial vectors population solution vectors obtained size S lying NP 2NP e Nondominated sorting The population P t size S NP S 2NP obtained sorted number Pareto fronts according nondominating criteria All nondominated solutions current population given rank 1 named optimal Pareto Front_Set1 The second formed identifying nondominated solutions set P t Front_Set1 The ranking process continues nondominated sets identiﬁed ranked Front_Set1 Front_Set2 Front_Set3 The pseudocode nondominated sorting given 25 f Truncation extended population crowdingdistancebased ranking The parent population iteration P t1 constructed selecting nondominated sets solutions P t according ascending order Pareto ranking Let Front_Setl set set accommodated P t1 adding Front_Setl P t1 exceeds NP Then solutions Front_Set l sorted descending order crowding distance To ensure diversity population solutions highest crowding distances included P t1 Front_Setl P t1 NP g Convergence After evolutionary step repeat process step b following conditions convergence satisﬁed The stop criteria include bound number iterations achieving suﬃciently low error aggregations thereof 4 Optimization presence noise This paper concerned realworld MOO problems measured variables contaminated stochastic noise aging sensors andor measurement inaccuracies The objectives MOO functions measurement variables contaminated noise lack precision Traditional deterministic MOO algorithms unable capture global optima ﬁtness landscapes presence stochastic noise 7 This section extends traditional DEMO algorithm improve optimization eﬃcacy presence stochastic noise 2627 ways described following sections 41 Adaptive selection sample size First repercussion noise ﬁtness measurements diminished taking repeated samples objective functions trial solution Because objective functions noisy MOO contaminated noise single measure ﬁtness estimate likely largely inﬂuenced noise To ensure ﬁtness estimates minimally inﬂuenced noise approach large sample size ﬁtness estimates Apparently larger sample size higher precision ﬁtness estimates given trial solution However large sample size adds computational overhead contribution improving ﬁtness estimates quality solution Contrarily samples considered ﬁtness estimates inaccurate leading selection deceptive trial solution qualitative Sample size selection constrained tradeoff accurate ﬁtness estimation computational com plexity Consequently incorporation method adaptive selection sample size MOO problem presence noise envisaged We attempt realize strategy noisy MOO algorithm introducing test criterion ﬁtness variance local neighborhood trial solution control sample size In words essence current work induce trial solution information sample size periodic ﬁtness evaluation acquired ﬁtness variance local neighborhood The design philosophy adopted relies underlying premise possible measure creeping noise neighborhood trial solution anticipated ﬁtness variance subpopulation A large ﬁtness variance trial solutions subpopulation indicates largescale detrimental effect noise local neighborhood given trial solution ﬁtness landscape Under situ ation obvious draw large sample size accurately estimate ﬁtness given trial solution On hand small ﬁtness variance local neighborhood portrays smaller contamination effect noise turn requires small sample size reduce computational cost sacriﬁcing quality ﬁtness measurement Mathematical model sample size adaptation Several formulations sample size selection adopted maintaining smaller sample size lower ﬁtness variance larger sample size larger ﬁtness variance local neighborhood trial solution One simple approach proportional selection sample size n proportional ﬁtness variance v local neighborhood trial solution However simply setting propor tional law demands large sample size n large variance v economic contexts particularly practical MOO problems Alternatively logistic function employed serve purpose Ex pression 5 refers model sample size nki corresponding kth objective function fk cid3Xi trial solution cid3Xi k 1 N having lower bound nki nmin largest bound nki nmax As shown Fig 1 function bounds continuous nondecreasing function ﬁtness variance vki local neighborhood cid3Xi corresponding kth objective controls value nki nmin nmax k 1 N 170 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Fig 1 The nonlinearity adapt sample size ﬁtness variance local neighborhood In Fig 1 v max population kth objective k 1 N k denotes maximum ﬁtness variance local neighborhood trial solutions current nki nmin cid2 nmax nmin cid3 cid2 cid3 1 expvki 5 b Local neighborhood selection To determine neighborhood Ddimensional trial solution cid3Xi population ﬁrst divided equally spaced intervals NP j 1 D Now neighborhood cid3Xi denoted Ni constructed selecting trial NP members parameter space search range xmin cid3x j xmax solutions lying hyperspace bounded cid3 cid3X min cid3 cid3X max xmin j xmax j j j cid3 cid3X min cid3 cid3X max xi1 cid3x1 xi2 cid3x2 xiD cid3xD xi1 cid3x1 xi2 cid3x2 xiD cid3xD 62 c Fitness variance local neighborhood Once trial solutions cid3X j local neighborhood Ni cid3Xi identiﬁed ﬁtness variance Ni determined follows 61 cid5 vki nk j sk j cid3X j Ni cid5 cid3X j Ni nk j 7 nk j symbolizes sample size sk j represents spread samples fk cid3X j k 1 N discussed shortly 42 Sampledistributionbased ﬁtness estimation Because adaptive selection sample size multiple ﬁtness measurements given trial solution These measurements general different values nonzero variance Traditional noisy MOO algorithms estimate true ﬁtness estimate given trial solution averaging noisy ﬁtness samples However averaging strategy presumes equal probability occurrence ﬁtness samples offering poor ﬁtness estimate real situations This circumvented second extension traditional DEMO algorithm paper The alternative approach proposed concerned ﬁtnesssampledensitybased partitioning sample space referring expected value entire sample space true ﬁtness estimate trial solution In words ﬁtness estimate trial solution biased ﬁtness samples crowded regions sample space imposing importance rare ﬁtness samples It assumed ﬁtness samples sparse zones sample space result noise contamination Sampledistributionbased ﬁtness estimation SDFE includes main steps k k cid3Xi f max cid3Xi respectively Now entire range f min Fitness interval selection sample space We ﬁrst record variance V k cid3Xi measured samples kth ﬁtness trial solution cid3Xi We note minimum maximum values observed ﬁtness samples cid3Xi ﬁrst divided intervals f min cid3Xi respectively equal length The resulting intervals represented f min cid3Xi2 If variance ﬁtness samples lying ﬁrst interval f mid cid3Xi greater V k cid3Xinki subdivided equal intervals represented f min cid3Xi2 The approach cid3Xi f min f mid1 k second interval The procedure repeated subsequent intervals variance ﬁtness sam ples interval falls V k cid3Xinki As consequence entire sample space f min cid3Xiis divided L intervals unequal length indicated Fig 2 cid3Xi respectively f mid1 cid3Xi f mid cid3Xi f min cid3Xi f max cid3Xi f mid cid3Xi f mid1 cid3Xi f max cid3Xi f max cid3Xi f max cid3Xi f mid cid3Xi f mid k k k k k k k k k k k k k k k k k P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 171 Fig 2 Fitness intervals sample space Fig 3 Expected ﬁtness calculation b Occurrence probability ﬁtness samples It evident proposed nonuniform partitioning sam ple space ﬁtness samples longer intervals rare samples probably owing noise contribution true ﬁtness estimate cid3Xi regarded similarly ﬁtness samples smaller interval We measure contribution ﬁtness samples interval j ﬁtness estimate trial solution cid3Xi probability measure p j p j n j kinki j 1 L 8 n j ki represents number samples fk cid3Xi jth interval j 1 L c Expected ﬁtness estimation Let f j k cid3Xi median value ﬁtness samples cid3Xi jth interval j 1 L We obtain expected valuesay E fk cid3Xi f k cid3Xiof kth ﬁtness cid3Xi 9 f k cid3Xi Lcid6 j1 p j f j k cid3Xi 9 The expected value obtained provides unique measure kth objective cid3Xi noisy local distribution ﬁtness samples wide space The ﬁtness samples particular interval represented median median value frequency distribution prone noisy measurements 28 A schematic diagram evaluation expected ﬁtness kth objective given Fig 3 The entire procedure performed objectivethat k 1 N d Spread ﬁtness samples The level contamination noise fk cid3Xi captured spread sample values fk cid3Xi away f k cid3Xi denoted ski k 1 N To evaluate ski median values ﬁtness samples L intervals sorted ascending order magnitude The lower upper quartiles sorted list identiﬁed recorded Q k025 cid3Xi Q k075 cid3Xi respectively The interquartile range corresponding fk cid3Xi measure ski deﬁned 10 k 1 N ski Q k075 cid3X Q k025 cid3X 10 To eliminate impact extreme values noisy ﬁtness samples interquartile range referred measure spread samples fk cid3Xi respective expected measurement f k cid3Xi instead sample variance 43 PS truncation extended population The nextgeneration population size NP MOO formed identifying trial solutions higher Pareto ranks merged population size lying NP 2NP current generation It apparent en 172 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 tire accommodation nondominated members deﬁnite granted generation nextgeneration population size ﬁxed NP A greedy selection process employed promote members generation ensuring diversity distribution trial solutions objective spaces To rank solutions crowding distance metric The crowding dis tance solution cid3Xi denoted CD cid3Xi offers estimate density nondominated solutions neighboring cid3Xi It sum distances ﬁtness measures nearest neighboring solutions objective spaces In traditional MOO conqueror samerank solutions having greater crowding distance This turn improves diversity performance algorithm enriching explorative capabil ity This conventional process ranking solutions crowding distance metric account uncertainty measurement noisy ﬁtness samples The possible promotion trial solu tion cid3Xi generation samerank candidate pool presence noise depends impor tant issues First greater crowding distance measure CD cid3Xi higher chance selecting cid3Xi competitors residing The uncertainty involved ﬁtness measurement taken care second criterionthe probability nonoccurrence rare samples fk cid3Xi A higher value probability ensures proximity measured ﬁtness samples fk cid3Xi sample space The intermittent samples far away fk cid3Xi supposed result contamination effect noise evidently occurrence produces longer tail sample distribution Finally new selection strategy induced crowding distance prob ability nonoccurrence rare samples devised promote quality solutions generation The contamination effect noise ﬁtness samples realized quartile skewness distribution ﬁtness samples capture possible occurrence rare ﬁtness samples away expected ﬁtness estimate The quartile skewness γki deﬁned 11 provides robust measure degree asymmetry distribution ﬁtness samples fk cid3Xi respect expected value fk cid3Xi k 1 N γki Q k025 cid3Xi fk cid3Xi fk cid3Xi Q k075 cid3Xi Q k075 cid3Xi Q k025 cid3Xi 11 It apparent 11 γki 0 indicates tail left ﬁtness sample distribution pro nounced tail right signifying ﬁtness samples values comparatively fk cid3Xi rare The reverse case true γki 0 It apparent γki approaching 1 γki approaching 1 fre quency occurrence ﬁtness samples lying left right tail ﬁtness sample distribution far away fk cid3Xi extremely small regarded noisy samples 29 It evident skewness greatly inﬂuenced occurrence rare noisy ﬁtness samples far away expected value fk cid3Xi Hence expected fk cid3Xi affected noise measured ﬁtness samples close fk cid3Xi γki 0 This observation motivated denote 0 γki 1 probability occurrence rare samples en compassing uncertainty involved measurement ﬁtness samples fk cid3Xi Thus deﬁne probability nonoccurrence rare samples providing measure degree reliability samples fk cid3Xi fol lows pki 1 γki 12 The normalized measure CD cid3Xi denoted CD cid3Xi given 13 cid3Xi cid3X j lie Pareto CD cid3Xi CD cid3Xi j CD cid3X j cid5 13 Now treating CD cid3Xi like probability presuming pki k 1 N CD cid3Xi independent deﬁne selection probability cid3Xi generation psi Ncid7 k1 pki CD cid3Xi 14 The product function introduced 14 reveals increase pki k 1 N CD cid3Xi ensures increase psi The pseudocode proposed DENMO algorithm N objectives given Termination criteria al gorithm include bound maximum number function evaluations FEs achievement suﬃciently low difference performance metric values discussed Section 53 successive generations aggregations thereof P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 173 Procedure DENMO Begin I Initialization t 0 Ia Randomly initialize population NP Ddimensional individuals P t cid3X1t cid3X2t cid3XNPt following 1 Ib Set sample size nki kth objective function fk cid3Xit corresponding target vector cid3Xit nmin Ic Evaluate expected ﬁtness fk cid3Xit spread ski measured ﬁtness samples fk cid3Xit 1 NP k 1 N 1 NP k 1 N 9 10 respectively II While stopping criterion reached begin IIa For 1 NP begin 1 Mutation Generate donor vector cid3V corresponding ith target vector cid3Xit 2 2 Crossover Generate trial vector cid3U pair cid3Xit cid3V binomial crossover 3 3 Neighborhood selection The neighborhood cid3U obtained selecting members hyper space bounded cid3 cid3U min cid3 cid3U max given 6 End For IIb Formation merged population For 1 NP begin 1 Adaptive selection sample size cid3U Evaluate sample size fk cid3U k 1 N 5 2 Fitness estimation cid3U Evaluate expected ﬁtness spread measured ﬁtness samples fk cid3U k 1 N 9 10 respectively 3 Selection cid3U Include cid3U current generation discard basis dominance relationship cid3Xit cid3U discussed Section 32 list item d End For IIc Sort solutions P t number Pareto fronts denoted Front_Set following nondominated sorting strategy given Section 32 list item e IId Stochastic selection samerank solutions 1 Set P t1 NULL l 1 2 Repeat P t1 P t1 Front_Setl l l 1 Until P t1 Front_Setl NP 3 Calculate selection probability trial solutions Front_Setl 14 sort descend ing order selection probability solutions rank 1 Front_Setl highest selection probabilities respectively 4 Set P t1 P t1 NP P t1 trial solutions Front_Setl highest selection probability IIe Increase counter value t t 1 End While End 44 Comparative framework parameter setting The MOO algorithms comparative study included DEMON 9 NSGAIIA 10 CDR 11 simulated annealing noisy MOO 12 elitist evolutionary multiagent 13 MOEARF 14 modiﬁed NSGAII 7 noisetolerant strength Pareto evolutionary algorithm 15 Pareto fronteﬃcient global optimization 16 These algorithms selected comparative framework wide popularity realm noisy MOO The population size maximum number FEs algorithms ﬁxed 50 104 D Ddimensional problem respectively To comparison fair population algorithms benchmark functions discussed Section 51 initialized random seeds The best parametric setups algorithms chosen following respective sources In proposed DENMO algorithm minimum maximum sample sizes considered nmin 10 nmax 30 respectively crossover rate 09 The parameter settings given Tables 1 2 5 Benchmark functions evaluation metrics 51 Benchmark functions The challenging issue substantiating performance MOO algorithm identify right benchmark functions diverse characteristics multimodality deception isolation particularly location true 174 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Table 1 Parameter settings Ddimensional search space DENMO DEMON Parameter Value CR nmin nmax 09 10 30 Parameter Crossover factor Value 09 Minimum sample size Maximum sample size Initial value neighborhood restriction factor 10 30 10 CDR Parameter Number offsprings Mutation step size Crossover operator Crossover probability SA Value Parameter Value 25 05 Single point 08 1 5 30 20 Temperature Minimum sample size Maximum sample size Number nondominated solutions generating candidate solutions NSGAIIA Parameter Number samples Crossover rate Crossover operator Crossover distribution index Mutation rate Mutation operator Distribution index mutation SVM type SVM kernel Stopping criterion SVM C parameter SVM Minimum conﬁdence level Maximum conﬁdence level Value 30 09 SBX 20 1D Polyno mial 20 Csupport vector classiﬁca tion Linear 1e3 1 09 099 CDR conﬁdencebased dynamic resampling CR crossover rate DEMON differential evolution multiobjective optimization noise DENMO differ ential evolution noisy multiobjective optimization NSGAIIA nondominated sorting genetic algorithm II αdominance operator SA simulated annealing SBX simulated binary crossover SVM support vector machine optimal Pareto objective spaces resemble complicated realworld problems Traditional benchmark functions 3132 usually global optima located center parameter space search bounds Natu rally benchmark functions insuﬃcient profoundly validate performance MOO algorithm To surmount problem set benchmark functions 30 recommended Congress Evolutionary Computation CEC2009 conference The proposed benchmarks include extension stretching rotation objective functions assimilating diversity optimization problems traditional benchmark functions The performance proposed DENMO algorithm analyzed respect noisyversion 23 CEC2009 recommended multiobjective benchmark functions 30 Among benchmarks seven UF1UF7 twoobjective UF8UF10 threeobjective UF11UF13 ﬁveobjective unconstrained boundconstrained test functions UF11UF13 extended rotated versions immensely popular test suitesthe DebThieleLaumannsZitzler test suites 3132and test function Walking Fish Group test suite 33 The set 23 benchmark functions includes seven CF1CF7 twoobjective CF8CF10 threeobjective general constrained test instances 52 Noise models The noisy version kth objective fk cid3X trial solution cid3X given fknoisy cid3X fk cid3X ηk k 1 N 15 ηk represents injected stochastic noise amplitude follows certain probability distribution function PDF The following ﬁve variants ηk considered Gaussian ηk Gaussian PDF given P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 175 Table 2 Parameter settings Ddimensional search space MOEARF Parameter Archive size Value 50 Modiﬁed NSGAII NTSPEA ParEGO elEMAS Value Parameter Value Parameter Crossover probability Value 08 Parameter Death threshold Selection operator Binary tournament Mutation probability 1D Reproduction threshold Crossover operator Uniform 10 Migration threshold Initial value neighbor hood restriction factor Crossover rate 08 Niche size 02 Mutation operator Bit ﬂip Mutation rate 1D Ranking scheme Diversity operator Pareto ranking Niche count radius 001 Normalized Objective Space Niche adaptation factor Crowding factor Migration chance 099975 2 02 Reproduction chance 055 0 50 20 μ λ Archive size objectives Archive size objectives Recombi nation operator Mutation operator c1 c2 60 60 20 50 01 03 Energy transferred 0100 Maximum lifetime 4 Value 11D1 11 15 Parameter Initial population Latin hypercube Number scalarizing vectors objectives Number scalarizing vectors objectives Scalarizing function Augmented Tchebycheff Internal GA evaluations iteration Crossover probability Realvalue mutation probability Realvalue SBX parameter Realvalue mutation parameter 200000 01 1D 10 50 elEMAS elitist evolutionary multiagent GA genetic algorithm MOEARF multiobjective evolutionary algorithm robust features NSGAII non dominated sorting genetic algorithm II NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SBX simulated binary crossover pdfηk 1 σ 2π ηk m2 2σ 2 e 16 m σ 2 stand mean variance Gaussian PDF We wellknown technique called BoxMuller method 34 inject ηk fk cid3X following Gaussian distribution b Poisson ηk Poisson PDF given pdfηk ληk e ηk λ 17 λ represents mean variance Poisson distribution The injection Poisson noise ηk fk cid3X performed Knuths algorithm 35 c Rayleigh ηk Rayleigh PDF given ηk ηk 0 b2 expη2 0 pdfηk k 2b2 cid4 mean variance noise distribution m b injection Rayleigh noise ηk fk cid3X performed inverse transform sampling 36 π 2 σ 2 b24 π 2 respectively The d Exponential ηk exponential PDF given cid4 pdfηk expaηk 0 ηk 0 19 18 176 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 mean variance noise distribution m 1a σ 2 1a2 respectively We Ziggurat method 37 inject ηk fk cid3X following exponential distribution e Random Lastly consider ηk random noise maximum noise amplitude 25 true ﬁtness amplitudes fk cid3X Linear congruential pseudorandom number generator code 38 generate random noise 53 Performance metrics Inverted generational distance IGD Let P set uniformly distributed points true optimal Pareto let A approximate set optimal Pareto deﬁned objective space MOO problem Then average distance P A 30 deﬁned follows cid5 cid2 IGD A P cid3 vP dv A P 20 Here dv A represents minimum Euclidean distance v points A A lower value IGD ensures approximate Pareto A obtained proposed MOO close optimal Pareto b Spacing cid3 Schott 39 proposed metric measure range variance neighboring vectors nondom inated solutions obtained algorithm question The metric provides distinctive measure spread distribution vectors The metric deﬁned follows cid8 cid9 cid9 cid10 1 M 1 cid3 i1 cid5 Mcid6 d di2 d 1 M Mcid6 i1 di 21 f k cid3Xi f k cid3X j M nondominated vectors method Here cid3Xi cid3X j Here di minM nondominated vectors belonging approximate Pareto A A value zero metric signiﬁes members approximate Pareto equidistantly spaced j1 jcid9i N k1 c Error ratio ER This metric introduced van Veldhuizen 40 deﬁned follows ER cid5 M i1 ei M cid4 ei 0 1 cid3Xi A cid3Xi P cid3Xi A cid3Xi P 22 An ideal value zero metric designates nondominated solutions approximate Pareto A belong optimal Pareto P d Hypervolume ratio HVR This metric proposed Coello et al 41 deﬁned follows HVR A HV A HVP 23 Here HV A HV P denote hypervolume approximate Pareto A optimal Pareto P respectively The size objective spaces covered set nondominated solutions S termed hypervol ume HVS Mathematically HVS Λ cid3X cid7 cid3X cid3X cid7 cid3Xref cid15 cid14 cid11cid12 cid13 cid3XS 24 Here Λ symbolizes Lebesgue measure cid3Xref denotes reference point lowest ﬁtness maximum objective function value case minimization problem HVR A attains maximum ideal value 1 42 provided nondominated vectors belonging A objective spaces identical members optimal Pareto P However circumstances noisy ﬁtness landscapes need address uncertainty related ﬁtness measurements determining hypervolume approximate Pareto A The uncertainty problem resolved weighting size objective space covered nondominated solution cid3X probability cid3X dominates population member The higher probability reliable measure objective space covered cid3X For MOO problem objectives minimized probability trial solution cid3X dominating solution cid3X j given 25 p cid3X cid3X j Ncid7 k1 cid2 cid3 f k cid3X j f k cid3X p Ncid7 k1 1 1 expc f k cid3X j f k cid3X The validation probability distribution evident following observations 25 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 177 1 If c approaches f k cid3X j f k cid3X k 1 N p cid3X cid3X j 1 indicating dominance cid3X cid3X j 2 If c approaches f k cid3X j f k cid3X k 1 N p cid3X cid3X j 0 indicating dominance cid3X j cid3X 3 If f k cid3X j f k cid3X k 1 N p cid3X cid3X j 12N indicating nondominance relationship cid3X cid3X j It apparent probability population member dominated cid3X given 26 1 p cid3X NPcid7 j1 cid3X j cid9 cid3X p cid3X j cid3X 26 Hence hypervolume approximate A evaluated 24 objective space covered trial solution cid3X weighted p cid3X cid3X A 6 Simulation results This section provides relative performance individual extensions DEMO introduced simulations A comparative analysis performance DENMO performance standard algorithms undertaken section 61 Effectiveness different extensions DENMO In Section 4 extended DEMO introducing strategiesnamely adaptive selection sample size ASSS SDFE PSto suit application noisy MOO problems The ASSS SDFE PS strategies mutually independent deal different issues context noisy optimization To speciﬁc ASSS strategy concerned sample size adaptation SDFE strategy deals expected ﬁtness trial solutions based distribution ﬁtness samples PS strategy takes care relative comparison trial solutions rank ensure diversity quality solutions population Naturally incorporation strategies DEMO necessarily require exclusion rest The following groups strategies considered experiments DEMOASSS Traditional DEMO extended ASSS strategy The mean value ﬁtness samples ﬁtness estimate given trial solution The crowdingdistanceinduced selection scheme employed truncation extended population b DEMOSDFE Here SDFE strategy integrated traditional DEMO equal sample size size 20 signed trial solutions ﬁtness reevaluation Here crowdingdistanceinduced selection scheme employed selection quality solutions samerank c DEMOPS The performance traditional DEMO improved PS approach Here sample size trial solutions ﬁxed 20 estimate mean ﬁtness d DEMOASSSSDFE Traditional DEMO utilizes beneﬁts ASSS strategy SDFE strategy Trunca tion extended population crowding distance sorting traditional DEMO remains unchanged e DEMOASSSPS The extension encompasses ASSS PS strategies govern diversity quality solutions population f DEMOSDFEPS Traditional DEMO extended SDFE PS The sample size size 20 assigned trial solution g DENMO All extensions integrated traditional DEMO The mean standard deviation bestofrun IGD metric values 50 independent runs seven variants considered section traditional counterpart DEMO presented Table 3 23 30dimensional CEC2009recommended multiobjective benchmark functions 30 contaminated random noise restricted amplitude To obtain results shown Table 3 algorithms run initial population run The relative ranking different extensions examined respect remaining performance metrics different noise settings The results obtained follow trend similar reported Table 3 given space restrictions Friedman twoway analysis variances ranks 43 nonparametric statistical test performed mean IGD metric values 50 independent runs variant reported Table 3 The null hypothesis states algorithms equivalent individual ranks equal The row Table 3 summarizes rankings obtained Friedman procedure It evident DENMO outperforms extended versions traditional DEMO handling noise ﬁtness landscapes With level signiﬁcance α 005 Friedman statistic exhibits signiﬁcant differences performance competing algorithms test value 161 The results highlight DENMO best algorithm post hoc analysis 44 applied DENMO control method Although provide results IGD metric trend remains 178 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Table 3 Ranking different extensions introduced traditional differential evolution multiobjective optimization DEMO respect mean inverted generational distances standard deviation parentheses contaminating benchmark functions random noise amplitude 25 true objective function values UF1UF8 Functions Groups DEMO DEMOASSS DEMOSDFE DEMOPS DEMOASSSSDFE DEMOASSSPS DEMOSDFEPS DENMO UF1 UF2 UF3 UF4 UF5 UF6 UF7 UF8 UF9 UF10 UF11 UF12 UF13 CF1 CF2 CF3 CF4 CF5 CF6 CF7 CF8 CF9 CF10 07420 0358 07622 0405 07631 0320 06940 0389 06252 0425 06812 0476 07887 0375 07731 0354 06856 0301 06257 0420 07527 0271 06228 0468 07555 0412 06497 0416 06436 0389 07329 0387 08125 0307 05903 0374 05854 0373 05632 0381 05344 0373 07763 0386 06036 0416 06573 0238 06289 0374 06912 0303 06336 0357 05218 0299 06280 0369 06967 0306 06335 0346 05224 0260 05461 0369 06601 0264 05059 0325 06794 0321 05525 0375 05473 0375 05490 0378 06536 0300 04875 0354 05060 0337 04922 0331 04947 0308 06915 0251 05197 0370 Friedman ranking 8 7 03269 0181 03913 0261 05347 0296 03279 0271 04180 0210 02151 0317 03192 0286 04417 0325 04950 0241 04255 0293 05588 0153 04576 0298 05144 0214 03862 0354 02837 0164 04863 0272 03413 0241 04269 0181 03718 0308 02215 0269 03904 0195 04344 0156 04798 0357 5 04537 0223 04639 0305 05727 0297 03653 0281 05153 0218 03092 0368 03537 0300 05440 0340 05107 0252 04847 0336 05592 0256 04786 0320 05180 0320 04493 0359 03391 0246 05306 0306 05382 0287 04713 0290 04422 0313 02222 0306 04357 0222 04788 0204 05306 0366 00937 0143 01239 0096 02901 0070 01246 0065 01289 0102 00500 0103 00711 0156 00764 0177 02015 0089 00895 0106 00619 0072 00924 0212 02565 0091 01442 0213 00733 0055 03027 0134 01312 0204 00911 0123 00761 0151 01177 0094 01291 0039 01804 0071 02059 0172 6 3 02330 0171 03122 0183 04948 0272 02517 0266 03933 0166 01697 0261 02324 0268 03736 0315 04675 0168 03896 0290 05140 0149 04485 0250 04795 0180 02717 0313 02058 0141 04588 0270 02772 0233 03162 0165 02961 0291 01757 0249 03890 0077 02816 0135 03714 0324 4 00216 0134 00827 0049 02014 0042 01229 0061 00938 0072 00376 0065 00434 0091 00629 0126 00339 0056 00733 0073 00510 0035 00822 0153 01643 0034 01439 0026 00704 0042 02040 0118 00768 0164 00877 0087 00646 0017 00994 0089 01163 0039 01193 0040 01127 0156 2 00176 0119 00379 0009 00701 0039 00396 0014 00728 0048 00092 0008 00324 0025 00566 0024 00072 0010 00431 0070 00359 0018 00772 0008 00603 0011 00606 0007 00640 0016 00785 0097 00260 0033 00415 0050 00054 0003 00690 0070 00541 0017 00517 0021 01080 0055 1 The best metric value obtained case shown bold ASSS adaptive selection sample size DENMO differential evolution noisy multiobjective optimization PS probabilistic selection SDFE sample distributionbased ﬁtness estimation remaining performance metricsthat cid3 ER HVRand ﬁve variants stochastic noise In post hoc analysis Holm test 44 employed results Friedman procedure DENMO control algorithm results given Table 4 The null hypothesis considers performance DENMO seven competitor algorithms equally good However outcome analysis indicates DEMOSDFEPS null hypothesis rejected benchmark functions The performance DENMO considered signiﬁcantly better variants present context 62 Performance analysis DENMO The comparative analysis relative performance proposed DENMO algorithm competitors discussed section Although experiments performed ﬁve variants noise noise variance σ 2 0 1 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 179 Table 4 Holm test applied results Table 3 differential evolution noisy multiobjective optimization DENMO control algorithm k Variants DEMO z p αk α 005 7 6 5 4 3 2 1 Traditional DEMO DEMOASSS DEMOPS DEMOSDFE DEMOASSSPS DEMOASSSSDF DEMOSDFEPS 0000010 0000010 0000010 0000010 0000033 0005626 0166236 k number extended versions DEMO 8 rank extended version descending ascending order z values p values DEMO differential evolution multiobjective optimization ASSS adaptive selection sample size PS probabilistic selection SDFE sampledistribution based ﬁtness estimation 969106 830662 692218 553774 415331 276887 138443 000714 000833 001000 001250 001666 002500 005000 Acceptancerejection null hypothesis Reject Reject Reject Reject Reject Reject Accept 10 30 50dimensional problems report results ﬁnite values σ 2 speciﬁc problem dimensions save space The results omitted follow trend similar tables ﬁgures section 621 Comparative performance algorithms respect IGD metric The mean standard deviation IGD metric 50 independent runs 300000 FEs 30dimensional problem algorithms presented Table 5 ηk Poisson noise mean 025 variance 025 Since algorithms commence initial population problem instance use paired t tests compare means results produced best secondbest algorithms 45 The statistical signiﬁcance level difference means best algorithms presented 12th column Tables 58 Here plus sign designates t value 49 degrees freedom signiﬁcant 005 level signiﬁcance twotailed test minus sign denotes difference means statistically signiﬁcant applicable refers cases algorithms achieve best accuracy results The statistical signiﬁcance refers comparison DENMO best remaining algorithms The simulation results Table 5 DENMO outperformed competitors 21 23 benchmark functions Of 21 20 functions difference mean IGDs DENMO nearest competitor statisti cally signiﬁcant DEMON remains secondbest algorithm achieves best average IGD outperforming DENMO case UF7 Plots ﬁnal approximation sets smallest IGD objective spaces test instances threeobjectives shown Fig 4 It evident Fig 4 approximate Pareto DENMO algorithm closer optimal Pareto comparison competing algorithms 622 Comparative performance algorithms respect cid3 metric The mean standard deviation cid3 metric 50 independent runs 500000 FEs 50dimensional problem algorithms presented Table 6 ηk Rayleigh noise mean 03 variance 0025 Table 6 indicates DENMO outperformed contender algorithms statistically signiﬁcant fashion 20 functions respect cid3 metric It remains secondbest algorithm benchmark function UF11 outperformed DEMON In case benchmark functions UF13 CF7 performance DENMO remains comparable DEMON However noteworthy UF13 DENMO achieves lowest standard deviation 623 Comparative performance algorithms respect ER metric The mean standard deviation ER metric 50 independent runs 300000 FEs 30dimensional problem algorithms presented Table 7 ηk exponential noise mean 086 variance 075 Close scrutiny Table 7 reveals DENMO outperformed evolutionaryswarm noisy MOO algorithms statis tically signiﬁcant fashion 19 23 test functions achieving near optimal values ER metric It yielded results statistically equivalent DEMON case benchmark functions UF3 CF2 It achieved secondbest rank contender algorithms benchmarks UF7 UF13 624 Comparative performance algorithms respect HVR metric The mean standard deviation HVR metric 50 independent runs 300000 FEs 30dimensional problem algorithms presented Table 8 ηk Gaussian noise mean 0 variance 04 Close inspection Table 8 indicates performance proposed DENMO algorithm remained sistently superior noisy MOO algorithms respect HVR metric Of 23 benchmark functions 20 cases DENMO outperforms nearestneighbor competitor statistically signiﬁcant fashion 625 Effect varying noise variance In subsection scrutinize effect varying noise variance performance algorithms Fig 5 shows evolution average cid3 ER metric values population noise variance algorithms 180 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Table 5 Mean inverted generational distances standard deviation parentheses 50 independent runs Poisson noise mean variance 025 Function DENMO DEMON NSGAIIA CDR SA elEMAS MOEARF Mod NSGAII NTSPEA ParEGO Statistical signiﬁcance CF1 UF7 UF9 UF1 UF2 UF4 UF5 UF8 UF3 UF6 UF10 UF11 UF12 UF13 01202 0139 01573 0201 02081 0053 01254 0168 02314 0131 02578 0096 00843 0240 03309 0278 03314 0059 01218 0101 00712 0086 02025 0085 01743 0101 01115 0049 00558 0052 02512 0062 00698 0177 01349 0073 01875 0129 00768 0052 01286 0102 01645 0170 02261 0317 01522 0206 02998 0262 0313 0106 03431 0190 02542 0160 02711 0197 01971 0271 03437 0284 03970 0093 01324 0131 00957 0193 02065 0160 02178 0162 01287 0095 02053 0163 03527 0266 00743 0282 02408 0111 02607 0132 01935 0061 01103 0089 01087 0162 01882 0222 02138 0265 03056 0282 03564 0177 04100 0232 02850 0189 02997 0249 03065 0282 03641 0303 03979 0101 01816 0202 01121 0248 02281 0168 02510 0227 01467 0149 00620 0142 03334 0235 00835 0317 02418 0115 02782 0192 02633 0065 01426 0152 02532 0257 02658 0359 01102 0091 01391 0169 01732 0051 00906 0127 01564 0095 02238 0081 00556 0026 01572 0119 00827 0024 00944 0099 00552 0064 00211 0057 01078 0085 00811 0045 00428 0023 01825 0050 00560 0113 01347 0048 00722 0090 00364 0044 00931 0080 00985 0142 00285 0153 00957 0044 00795 0124 00780 0030 00612 0072 00286 0009 01476 0075 00691 0071 00003 0040 00116 0015 00274 0060 00350 0030 00172 0026 00607 0050 00432 0006 00428 0023 00390 0024 00461 0041 00567 0044 00674 0053 00228 0003 00646 0037 00472 0078 00253 0116 02968 0279 04045 0297 04183 0196 04549 0252 04402 0205 04587 0285 03305 0337 04227 0320 05263 0175 03216 0237 03865 0380 03201 0216 02815 0253 01815 0161 02059 0258 04388 0294 04054 0356 02543 0142 03882 0236 02769 0205 01645 0270 02566 0289 02669 0360 The best metric value obtained case shown bold A plus sign designates t value 49 degrees freedom signiﬁcant 005 level signiﬁcance twotailed test minus sign denotes difference means statistically signiﬁcant CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent MOEARF multiobjective evolutionary algorithm robust features Mod NSGAII modiﬁed nondominated sorting genetic algorithm II NA applicable referring cases algorithms achieve best accuracy results NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing 04629 0331 04725 0403 05692 0349 06091 0359 04743 0398 05524 0428 04889 0426 06824 0358 06354 0298 04093 0413 06274 0453 05772 0435 04955 0438 04457 0435 05257 0361 04952 0431 05052 0420 05057 0311 05045 0386 04269 0317 04610 0443 04365 0403 03511 0453 04524 0314 04523 0373 04235 0253 05041 0325 04535 0373 05401 0418 04634 0412 05972 0353 06097 0267 03471 0310 04909 0417 03831 0261 04627 0361 03615 0221 04357 0323 04624 0359 04573 0363 04488 0257 04876 0372 04127 0306 02081 0342 03960 0367 02846 0370 05751 0371 05296 0408 06150 0481 06352 0417 04863 0430 06487 0480 05110 0450 06831 0409 06723 0305 04480 0423 06387 0477 06436 0479 06053 0442 05417 0482 05366 0400 04944 0387 05968 0470 05171 0379 06203 0452 04736 0350 05853 0479 06271 0422 04670 0464 03807 0312 04235 0321 04224 0217 04985 0284 04430 0208 05159 0372 04200 0395 05529 0323 05905 0237 03453 0274 04183 0416 04442 0321 04281 0327 03401 0214 03490 0292 04482 0356 04327 0358 03917 0234 04469 0317 03738 0257 02074 0301 02567 0332 02700 0368 CF10 CF2 CF5 CF7 CF3 CF4 CF8 CF6 CF9 NA number number generations ﬁxed 300000 problem dimension D 30 Poisson exponential noise respectively It evident Fig 5 noisy MOO algorithms eventually lose accuracy achieving values cid3 ER close ideal value zero noise increasing variance creeps ﬁtness landscapes However DENMO appears effective achieving approximate Pareto lower values cid3 ER metrics noise predominant factor ﬁtness landscape complex 626 Effect varying problem dimension Plots average HVR IGD metric values problem dimension 10 100 given Fig 6 Rayleigh random distribution noise respectively We note Fig 6 HVR IGD metrics nonincreasing nondecreasing functions respectively problem dimension D speciﬁc noise settings An intuitive interpretation phenomenon increase D complex terrain needs explored population members obtain Pareto optimal solutions decreasing HVR increasing IGD metric values However P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 181 Fig 4 The ﬁnal approximation set smallest inverted generational distance objective space UF3 contaminated zero mean Gaussian noise σ 2 06 b UF10 contaminated Poisson noise σ 2 10 corresponding symbols algorithms CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent MOEARF multiobjective evolutionary algorithm robust features NSGAII nondominated sorting genetic algorithm II NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing instances DENMO achieves best rank contestants providing largest smallest values HVR IGD metrics substantiated Fig 6 627 Convergence characteristics To compare relative speed convergence quality solution DENMO competitors cid3 ER metric values median run algorithm versus number FEs plotted Fig 7 Fig 7 shows DENMO outperformed contender algorithms 628 Robustness runtime analysis The experiment compared consistent performance speed different algorithms spect HVR metric First threshold value HVR metric selected corresponding benchmark function Then algorithms run benchmark function different settings noise problem dimen sion The algorithm terminated best value HVR metric achieved algorithm fell predeﬁned threshold maximum number FEs 300000 30D problems 500000 50D problems reached whichever occurred earlier The termination algorithm occurrence ﬁrst condition indi cates algorithm succeeds ﬁnding approximate Pareto prescribed tolerance limit HVR metric speciﬁc run reaching maximum number FEs said successful run The number successful runs total 50 runs recorded algorithm respect predeﬁned threshold value HVR metric A lower number unsuccessful runs total runs minus successful runs algo rithm corresponds robustness performance In Fig 8 present plot number unsuccessful runs versus runtime terms expected number FEs 50 independent runs respect HVR tolerance limit 075 algorithms UF5 tested different settings problem dimension stochastic noise The plot provides visual means elucidating eﬃcacy algorithms respect robustness runtime terms expected number FEs The x y coordinates scaled properly uniformity magnitude The performance algorithm assessed measuring distance representative point origin The smaller measure better performance algorithm terms robustness expected runtime The rankings obtained different dimensional problems given Table 9 The experiment repeated bench mark functions different settings stochastic noise problem dimension respect remaining metrics The results omitted economy space follow trend similar Fig 8 Table 9 It observed Fig 8 DEMON MOEARF simulated annealing case 30D problem zero mean Gaussian noise σ 2 035 DEMON NSGAIIA CDR case 50D problem Poisson noise σ 2 07 manage attain smaller number unsuccessful runs DENMO cost increased expected runtime 629 Nonparametric statistical analysis performance A series nonparametric statistical tests performed mean performance metrics 50 independent runs competitor algorithms reported Tables 58 The ﬁrst test Friedman twoway analysis variances ranks 43 Additionally ImanDavenport test variant Friedman test 46 The objective Friedman ImanDavenport tests signiﬁcant statistical difference different algorithms Table 10 summarizes rankings obtained Friedman procedure highlighting DENMO best algorithm With level signiﬁcance α 005 Friedman statistics ImanDavenport statistics 182 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Table 6 Mean spacing cid3 standard deviation parentheses 50 independent runs Rayleigh noise mean 03 variance 0025 Function DENMO DEMON NSGAIIA CDR SA elEMAS MOEARF Mod NSGAII NTSPEA ParEGO Statistical signiﬁcance CF1 UF9 UF1 UF7 UF4 UF5 UF8 UF3 UF6 UF2 UF10 UF11 UF12 UF13 01033 0080 01523 0135 02696 0186 01100 0095 02459 0130 00587 0072 02130 0269 02033 0118 01414 0052 01428 0205 02700 0189 01798 0126 02476 0099 00935 0064 01147 0247 01990 0150 03008 0196 02998 0237 01793 0205 02681 0197 00534 0174 02637 0136 02722 0224 01340 0284 02378 0303 04132 0318 02314 0139 04025 0237 04581 0114 04281 0307 02333 0365 02856 0101 02656 0325 04952 0289 03991 0183 03311 0308 02039 0387 01782 0286 03076 0294 03699 0344 04683 0293 02654 0302 02795 0209 01039 0291 02916 0280 03289 0239 01105 0276 01943 0231 03254 0233 02213 0117 02463 0144 03578 0095 04191 0290 02280 0199 01609 0094 02304 0255 03743 0217 02280 0148 03039 0190 01421 0180 03419 0295 02344 0158 03511 0312 04127 0239 01856 0262 02685 0208 00851 0271 02875 0265 02887 0226 00669 0063 01077 0120 02514 0118 00849 0046 02229 0119 00448 0056 01869 0216 01433 0022 01059 0052 00690 0180 00845 0173 01472 0072 02388 0022 00750 0049 00321 0228 01740 0070 02508 0108 02581 0129 01340 0171 00114 0129 00383 0077 00474 0106 02206 0203 00622 0019 00122 0033 02042 0031 00150 0023 00138 0015 00121 0022 01545 0043 00827 0005 00645 0023 00371 0072 02186 0172 00204 0025 02388 0010 00515 0024 00271 0150 00897 0011 00174 0037 02517 0078 00800 0149 00114 0129 00074 0050 00432 0100 00995 0189 02926 0357 02475 0353 04602 0332 02516 0196 04786 0282 05250 0186 04852 0317 02721 0370 03731 0164 02934 0375 04382 0259 04234 0237 03407 0326 01650 0286 04028 0376 03259 0314 03885 0351 05018 0315 04328 0320 04132 0271 01912 0300 03288 0312 03541 0365 The best metric value obtained case shown bold A plus sign designates t value 49 degrees freedom signiﬁcant 005 level signiﬁcance twotailed test minus sign denotes difference means statistically signiﬁcant CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent MOEARF multiobjective evolutionary algorithm robust features Mod NSGAII modiﬁed nondominated sorting genetic algorithm II NA applicable referring cases algorithms achieve best accuracy results NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing 05202 0426 05508 0418 05252 0500 04388 0486 05217 0423 06293 0329 06228 0395 05836 0443 04963 0406 04275 0483 05662 0398 05354 0384 05659 0427 04004 0465 05153 0485 05047 0494 05955 0459 06014 0473 05750 0439 04989 0346 04579 0512 05527 0472 04581 0408 04305 0394 03898 0403 05242 0456 04337 0380 05137 0302 05373 0216 05983 0378 02993 0384 04026 0257 03467 0470 05648 0365 04723 0376 05456 0355 02480 0418 04237 0441 04845 0471 04907 0447 05778 0464 04969 0435 04415 0295 02536 0452 05473 0431 04424 0386 04206 0369 03258 0395 05078 0343 03412 0337 04239 0279 05931 0324 04304 0307 04779 0405 04587 0293 03407 0427 05083 0326 04544 0347 03978 0351 01585 0210 04227 0421 04183 0421 04354 0436 05348 0412 04387 0365 04371 0287 02055 0332 04101 0322 03944 0385 05649 0501 05561 0484 05484 0524 05029 0504 05843 0519 06626 0494 06253 0524 06393 0485 05590 0471 06161 0488 06345 0512 06097 0450 05931 0512 05993 0489 05246 0518 05097 0516 06358 0491 06114 0481 05862 0471 05416 0500 05273 0524 05935 0490 06146 0402 CF10 CF2 CF6 CF4 CF3 CF9 CF8 CF5 CF7 NA NA signiﬁcant differences performance contender algorithms test values presented Table 10 p 0001 So post hoc analysis performed DENMO control method In post hoc analysis applied BonferroniDunn test 47 ranking results Friedman procedure The analysis indicates level signiﬁcance superiority control algorithm remaining algorithms For BonferroniDunn test critical difference 44 calculated data 22677 The interpretation measure performance algorithms signiﬁcantly different corre sponding average Friedman ranks differ critical difference depicted Fig 9 Fig 9 shows bar graph height bar proportional average Friedman ranking obtained representative algorithm We chose smallest corresponding bestcontrol algorithm DENMO summed height critical difference obtained BonferroniDunn test The result represented cutline going bars Then conclude behaviors algorithms characterized bars cutline sig P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 183 Table 7 Mean error ratio standard deviation parentheses 50 independent runs exponential noise mean 086 variance 075 Function DENMO DEMON NSGAIIA CDR SA elEMAS MOEARF Mod NSGAII NTSPEA ParEGO Statistical signiﬁcance NA CF1 UF9 UF2 UF7 UF5 UF3 UF6 UF4 UF8 UF1 UF10 UF13 UF11 UF12 00789 0052 00406 0072 00588 0166 02095 0079 00023 0092 00185 0152 00241 0125 01102 0294 00838 0038 00846 0124 00642 0148 01234 0060 00753 0171 00620 0098 00235 0176 01366 0049 02790 0077 02091 0130 02648 0252 00484 0020 00343 0232 01237 0074 00881 0129 01650 0081 00408 0095 01213 0295 02723 0093 00510 0148 01238 0213 03047 0161 01257 0336 01006 0187 02725 0129 00920 0186 01694 0154 03099 0195 01336 0113 02606 0207 02960 0129 03316 0265 02968 0132 02840 0287 00632 0085 00629 0290 02268 0139 00950 0231 02814 0146 00708 0338 01925 0302 02801 0172 00981 0177 03066 0248 03668 0162 01349 0363 01747 0356 03282 0183 01653 0332 01825 0215 03628 0208 01994 0139 02738 0220 05451 0395 03443 0358 03648 0172 02865 0399 01237 0139 01523 0309 02718 0247 02012 0280 03315 0335 01106 0346 03603 0356 02817 0174 01302 0251 03107 0269 04075 0184 02262 0385 01348 0263 04239 0190 02632 0339 04291 0270 03815 0351 03096 0213 03229 0277 05086 0374 03581 0374 03714 0186 03313 0470 02115 0237 01666 0359 04315 0272 03344 0418 00363 0010 00142 0003 00588 0160 00786 0058 00008 0045 00052 0146 00228 0032 00018 0192 00627 0016 00420 0101 00077 0041 00645 0040 02178 0180 00427 0089 00235 0176 01301 0026 01271 0034 00114 0113 01260 0224 00063 0005 00340 0108 00878 0015 00308 0104 04264 0382 03156 0374 03668 0371 02839 0199 01519 0323 04843 0370 04936 0215 02576 0418 02363 0449 04246 0228 02789 0381 04434 0342 04974 0428 05791 0449 03809 0393 03477 0289 04695 0472 03728 0218 04327 0492 03152 0292 03379 0387 04380 0295 04869 0494 The best metric value obtained case shown bold A plus sign designates t value 49 degrees freedom signiﬁcant 005 level signiﬁcance twotailed test minus sign denotes difference means statistically signiﬁcant CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent MOEARF multiobjective evolutionary algorithm robust features Mod NSGAII modiﬁed nondominated sorting genetic algorithm II NA applicable referring cases algorithms achieve best accuracy results NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing 05006 0401 03773 0513 04155 0439 03098 0284 01677 0350 05028 0407 05877 0225 05206 0432 02936 0560 04738 0261 03055 0430 05338 0385 05156 0475 04091 0382 03951 0435 05598 0501 05912 0506 03986 0247 05519 0505 03673 0292 04106 0428 05553 0367 04914 0505 05092 0448 04478 0568 04694 0455 04041 0287 01846 0401 05430 0517 06230 0270 05853 0500 03432 0607 05005 0398 04026 0560 06138 0485 05408 0569 04090 0318 05650 0508 05645 0511 06466 0644 04895 0320 05938 0551 03691 0304 05816 0479 06115 0406 06088 0539 05659 0449 06036 0614 05412 0562 05817 0331 02712 0603 06799 0573 06263 0367 06838 0510 04725 0650 05244 0625 04317 0567 06513 0543 05689 0610 05865 0453 06498 0597 06944 0634 06655 0645 05503 0443 06168 0574 04371 0487 05926 0510 06784 0492 06462 0548 06821 0578 06266 0661 06659 0631 06051 0473 05407 0629 06848 0596 06270 0370 06689 0510 04574 0633 05299 0644 05547 0650 06844 0662 05886 0651 06633 0553 05760 0583 06924 0609 06876 0666 06192 0589 06813 0593 06222 0522 06661 0614 06912 0540 06956 0602 CF10 CF8 CF9 CF7 CF6 CF2 CF4 CF3 CF5 NA niﬁcantly inferior behavior control algorithm Only DEMON NSGAIIA null hypothesis rejected tests α 005 metrics However seven algorithms regarded signiﬁcantly poorer DENMO level signiﬁcance α 005 7 Conclusion We proposed novel approach management uncertainty ranking trial solutions possible creeping noise ﬁtness landscapes MOO problem Although traditional evolutionary MOO algorithms extended principles uncertainty management introduced realized strategies DEMO algorithm quality performance respect runtime accuracy computational complexity 184 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Table 8 Mean hypervolume ratio standard deviation parentheses 50 independent runs Gaussian noise mean 0 variance 04 Function DENMO DEMON NSGAIIA CDR SA elEMAS MOEARF Mod NSGAII NTSPEA ParEGO Statistical signiﬁcance CF1 UF9 UF1 UF4 UF8 UF5 UF7 UF3 UF6 UF2 UF10 UF11 UF13 UF12 08309 0147 08172 0228 08065 0060 08470 0266 07875 0148 08587 0241 08527 0061 08812 0273 08363 0115 07767 0103 08048 0058 08753 0073 08120 0227 07670 0101 07413 0055 08487 0158 08216 0048 08187 0201 08054 0057 08425 0103 08154 0059 07015 0101 08189 0161 07943 0158 08072 0252 08045 0120 08443 0286 07415 0181 08461 0369 08395 0074 08769 0290 08335 0266 07566 0155 07807 0078 08748 0162 07788 0255 07499 0116 06983 0163 08019 0289 07945 0266 08092 0203 07765 0083 08223 0146 07951 0069 07012 0116 07986 0192 07515 0234 08034 0337 07863 0287 08009 0288 07149 0217 08416 0422 07881 0084 08722 0316 08132 0319 07159 0244 07569 0091 08513 0165 07427 0296 07150 0164 06775 0164 07708 0331 07781 0302 08050 0328 07242 0130 08097 0150 07445 0073 06803 0172 07287 0209 08362 0103 08269 0192 08783 0058 08985 0216 08200 0108 08629 0180 08641 0049 08974 0263 08850 0066 07967 0080 08796 0024 08990 0016 08733 0158 08161 0073 08863 0034 08642 0026 08372 0040 08312 0129 08470 0055 08510 0060 08472 0004 08250 0091 09161 0088 08935 0050 09024 0141 09007 0034 09112 0051 08415 0010 08840 0091 08829 0030 09164 0125 09143 0009 08015 0023 08729 0045 09150 0013 08875 0055 08896 0057 08871 0025 08896 0005 08496 0028 08397 0047 08525 0050 09086 0020 08228 0050 08857 0042 09161 0084 07091 0300 07943 0364 07730 0299 07434 0291 07112 0233 07375 0434 07646 0104 08609 0336 08115 0319 06657 0274 07334 0315 08361 0183 07113 0388 07074 0309 06629 0182 07081 0367 07268 0403 07866 0376 07176 0161 07931 0217 07374 0232 06696 0305 07286 0291 The best metric value obtained case shown bold A plus sign designates t value 49 degrees freedom signiﬁcant 005 level signiﬁcance twotailed test minus sign denotes difference means statistically signiﬁcant CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent MOEARF multiobjective evolutionary algorithm robust features Mod NSGAII modiﬁed nondominated sorting genetic algorithm II NA applicable referring cases algorithms achieve best accuracy results NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing 06777 0316 07440 0364 07534 0299 07294 0369 06838 0243 07269 0458 07302 0272 08229 0440 08003 0422 06616 0294 06864 0341 07642 0256 07058 0420 06668 0328 06298 0250 06940 0388 07080 0439 07052 0404 06732 0222 07384 0268 07010 0292 06627 0306 07269 0377 06613 0354 06755 0423 07017 0356 07170 0382 06776 0375 07251 0474 07019 0272 07864 0464 07875 0464 06393 0296 06667 0400 07275 0356 06566 0450 06569 0404 06228 0277 06349 0401 06409 0439 06422 0407 06659 0266 07383 0422 06416 0347 06538 0342 06824 0416 06562 0375 06668 0458 06847 0396 06614 0407 06479 0387 07207 0522 06639 0293 07793 0495 06789 0474 06158 0349 06659 0512 06568 0463 06378 0543 06562 0410 06172 0418 06287 0410 06335 0489 06375 0476 06658 0291 06933 0438 06401 0360 06454 0388 06545 0457 06488 0421 06382 0463 06381 0546 06444 0473 06142 0404 06691 0544 06495 0373 06001 0544 06267 0510 06102 0382 06396 0521 06202 0516 06173 0549 06181 0501 06042 0547 06258 0429 06197 0518 06353 0533 06277 0430 06359 0513 06190 0397 06315 0502 06236 0479 CF10 CF4 CF2 CF7 CF3 CF9 CF5 CF6 CF8 NA The integrity work lies following 1 adaptive selection sample size periodic ﬁtness evaluation trial solution based ﬁtness variance subpopulation surrounding 2 evaluation expected value ﬁtness samples trial solution based nonuniform ﬁtness sample distribution entire sample space 3 enhancing robustness selecting trial solutions samerank candidate pool based selection probability inﬂuenced jointly crowding distance metric probability nonoccurrence rare probably noisy ﬁtness samples The adaptive selection sample size ﬁtness reevaluation trial solution signiﬁcant eﬃcient tradeoff runtime complexity computational accuracy A nonlinear functional form induced function 1 expv proﬁciently capture variation sample size ﬁtness variance v indirect implication extent noise contamination local neighborhood given trial solution The second noise handling strategy provides unique ﬁtness estimate trial solution based expectation measured noisy ﬁtness samples P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 185 Fig 5 Average spacing versus Poisson noise variance σ 2 UF4 b error ratio versus exponential noise variance σ 2 CF8 300000 function evaluations 30D problem CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent Mod NSGAII modiﬁed nondominated sorting genetic algorithm II MOEARF multiobjective evolutionary algorithm robust features NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing Fig 6 Average hypervolume ratio versus problem dimension Rayleigh σ 2 025 noise contaminating CF5 b inverted generational distance versus problem dimension random limitedamplitude noise contaminating UF10 CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary mul tiagent Mod NSGAII modiﬁed nondominated sorting genetic algorithm II MOEARF multiobjective evolutionary algorithm robust features NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing It differs conventional averaging approach considers equal probability occurrence ﬁtness samples Moreover proposed strategy recommends novel method prioritizing ﬁtness samples crowded regions sample space reducing impact noisy rare ﬁtness samples evaluation expected ﬁtness estimate The PS strategy adopted extension traditional DEMO algorithm paves way decisive selection quality solutions nextgeneration population simultaneously ensuring population diversity objective spaces The stratagem circumvents dismissal quality solutions generations limiting impact misleading individuals noisy environment We undertook comparative study proposed DENMO algorithm stateoftheart noisy MOO algorithms The eﬃcacy contender algorithms handle uncertainty scrutinized respect noisy version test suite 23 CEC2009 benchmark functions The performance algorithms compared basis performance metrics IGD cid3 ER HVR Statistical signiﬁcance results judged nonparametric Friedman test ImanDavenport statistic Holm test BonferroniDunn post hoc analysis The experimental study clearly reveals DENMO outperforms competitor algorithms statistically signiﬁcant manner respect standard metrics presence ﬁve different stochastic noise distributions Gaussian Poisson Rayleigh exponential random noise limited amplitude In addition fundamental claim paper DENMO outperforms competitors consistency quality performance expected runtime 186 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Fig 7 Average spacing versus number function evaluations Poisson σ 2 065 noise contaminating CF4 b error ratio versus number function evaluations exponential σ 2 05 noise contaminating UF8 CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent sys tem Mod NSGAII modiﬁed nondominated sorting genetic algorithm II MOEARF multiobjective evolutionary algorithm robust features NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing Fig 8 Relative performance terms unsuccessful runs versus expected runtime UF5 different settings noise problem dimension D CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent MOEARF multiobjective evolutionary algorithm robust features NSGAII nondominated sorting genetic algorithm II NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing Table 9 Rankings obtained Fig 8 Ranking 1 2 3 4 5 6 7 8 9 10 30D UF5 zero mean Gaussian noise σ 2 035 DENMO DEMON CDR MOEARF NSGAIIA SA elEMAS Modiﬁed NSGAII NTSPEA ParEGO 50D UF5 Poisson noise σ 2 07 DENMO DEMON NSGAIIA CDR elEMAS MOEARF SA Modiﬁed NSGAII ParEGO NTSPEA CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary mul tiagent MOEARF multiobjective evolutionary algorithm robust features NSGAII nondominated sorting genetic algorithm II NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 187 Table 10 Average rankings obtained Friedmans test Algorithm Friedman ranking obtained DENMO DEMON NSGAIIA CDR SA elEMAS MOEARF Modiﬁed NSGAII NTSPEA ParEGO Table 5 10652 19347 31304 39565 49130 60000 70434 79565 90434 99565 Table 6 10869 19130 30000 40434 50869 60434 69565 78695 90000 100000 Table 7 10869 19130 30000 41304 49565 60000 70000 79130 91739 98260 Table 8 11087 18913 30000 40000 50000 60000 70000 80000 90000 100000 Friedman statistic 2052410 2042680 2039748 2065138 ImanDavenport statistic 2566971 1644910 1483353 9344515 Critical difference α 005 22677 CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary mul tiagent MOEARF multiobjective evolutionary algorithm robust features NSGAII nondominated sorting genetic algorithm II NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing Fig 9 Graphical representation BonferroniDunns procedure DENMO control method results given Table 10 corresponding values inverted generational distance Table 5 b spacing cid3 Table 6 c error ratio Table 7 d hypervolume ratio Table 8 CDR conﬁdencebased dynamic resampling DEMON differential evolution multiobjective optimization noise DENMO differential evolution noisy multiobjective optimization elEMAS elitist evolutionary multiagent Mod NSGAII modiﬁed nondominated sorting genetic algorithm II MOEARF multiobjective evolutionary algorithm robust features NSGAIIA nondominated sorting genetic algorithm II αdominance operator NTSPEA noisetolerant strength Pareto evolutionary algorithm ParEGO Pareto fronteﬃcient global optimization SA simulated annealing 188 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 Acknowledgements Funding Council Scientiﬁc Industrial Research award Senior Research Fellowship Pratyusha Rakshit Acknowledgment No 1433922K121 FILE NO 090967812013EMRI UGCUPEII Cognitive Science Program Jadavpur University gratefully acknowledged References 1 A Chowdhury A Konar P Rakshit AK Nagar A multiobjective evolutionary approach evaluate designing perspective proteinprotein interaction network J Netw Innov Comput 1 2013 270289 2 AN Aizawa BW Wah Scheduling genetic algorithms noisy environment Evol Comput 2 2 1994 97122 3 J Branke C Schmidt Selection presence noise Genetic Evolutionary Computation Conference Lecture Notes Computer Science vol 2723 SpringerVerlag Berlin June 2003 pp 766777 4 BL Miller Noise sampling eﬃcient genetic algorithms Doctor Philosophy thesis University Illinois UrbanaChampaign 1997 5 S Markon D Arnold T Back T Beislstein HG Beyer Thresholdinga selection operator noisy ES Proceedings IEEE Congress Evolutionary Computation vol 1 May 2001 pp 465472 6 BL Miller DE Goldberg Genetic algorithms selection schemes varying effects noise Evol Comput 4 2 1996 113131 7 M Babbar A Lakshmikantha DE Goldberg A modiﬁed NSGAII solve noisy multiobjective problems Genetic Evolutionary Computation Conference AAAI Chicago 2003 pp 2127 LateBreaking Papers 8 T Robic B Philipic DEMO differential evolution multiobjective optimization The Third International Conference Evolutionary MultiCriterion Optimization Lecture Notes Computer Science vol 3410 SpringerVerlag Berlin 2005 pp 520533 9 P Rakshit A Konar S Das LC Jain AK Nagar Uncertainty management differential evolution induced multiobjective optimization presence measurement noise IEEE Trans Syst Man Cybern Syst 44 7 July 2014 922937 10 P Boonma J Suzuki A conﬁdencebased dominance operator evolutionary algorithms noisy multiobjective optimization problems Proceed ings 21st IEEE International Conference Tools Artiﬁcial Intelligence November 2009 pp 387394 11 A Syberfeldt A Ng RI John P Moore Evolutionary optimisation noisy multiobjective problems conﬁdencebased dynamic resampling Eur J Oper Res 204 3 2010 533544 2013 255276 12 V Mattila K Virtanen RP Hämäläinen A simulated annealing algorithm noisy multiobjective optimization J MultiCriteria Decis Anal 20 56 13 L Siwik S Natanek Elitist evolutionary multiagent solving noisy multiobjective optimization problems Proceedings IEEE Congress Evolutionary Computation June 2008 pp 33193326 14 CK Goh KC Tan An investigation noisy environments evolutionary multiobjective optimization IEEE Trans Evol Comput 11 3 2007 354381 15 D Buche P Stall R Dornberger P Koumoutsakos Multiobjective evolutionary algorithm optimization noisy combustion processes IEEE Trans Syst Man Cybern Part C Appl Rev 32 4 November 2002 460473 16 J Knowles EJ Hughes Multiobjective optimization budget 250 evaluations Evolutionary MultiCriterion Optimization Lecture Notes 17 P Stagge Averaging eﬃciently presence noise The Fifth International Conference Parallel Problem Solving Nature Lecture Computer Science vol 3410 SpringerVerlag Berlin 2005 pp 176190 Notes Computer Science vol 1498 SpringerVerlag 1998 pp 188197 18 S Das A Konar UK Chakraborty Improved differential evolution algorithms handling noisy optimization problems Proceedings IEEE Congress Evolutionary Computation vol 2 September 2005 pp 16911698 19 EJ Hughes Evolutionary multiobjective ranking uncertainty noise Evolutionary MultiCriterion Optimization Lecture Notes Com puter Science vol 1993 SpringerVerlag Berlin Heidelberg July 2001 pp 329343 20 A Singh B Minsker Uncertainty based multiobjective optimization groundwater remediation umatilla chemical depot Critical Transitions Water Environmental Resources Management Environmental Water Resources Ground Water Issues Symposium 2004 pp 110 21 J Knowles D Corne A Reynolds Noisy multiobjective optimization budget 250 evaluations Evolutionary MultiCriterion Optimization Lecture Notes Computer Science vol 5467 Springer Berlin April 2009 pp 3650 22 P Rakshit A Konar P Bhowmik I Goswami S Das LC Jain AK Nagar Realization adaptive memetic algorithm differential evolution Qlearning case study multirobot path planning IEEE Trans Syst Man Cybern Syst 43 4 July 2013 814831 23 R Storn KV Price Differential evolutiona simple eﬃcient heuristic global optimization continuous spaces J Glob Optim 11 4 1997 24 S Das A Abraham UK Chakraborty A Konar Differential evolution neighborhoodbased mutation operator IEEE Trans Evol Comput 13 3 341359 June 2009 526553 25 K Deb A Pratap S Agarwal T Meyarivan A fast elitist multiobjective genetic algorithm NSGA II IEEE Trans Evol Comput 6 2 April 2002 182197 26 JM Fitzpatrick JJ Greffenstette Genetic algorithms noisy environments Mach Learn 3 23 October 1988 101120 27 E Mendel RA Krohling M Campos Swarm algorithms chaotic jumps applied noisy optimization problems Inf Sci 181 20 2011 44944514 28 MK Gupta AM Gun B Dasgupta Measures Central Tendency Fundam Stat vol 1 World Press Pvt Ltd 2008 29 S Heymann M Latapy C Magnien Outskewer skewness spot outliers samples time series Proceedings 2012 International Conference Advances Social Networks Analysis Mining IEEE Computer Society August 2012 pp 527534 30 Q Zhang A Zhou S Zhao PN Suganthan W Liu S Tiwari Multiobjective optimization test instances CEC 2009 special session competi tion Working Report CES887 School Computer Science Electrical Engineering University Essex Colchester UK Nanyang Technological University Singapore April 20 2009 Special Session Performance Assessment MultiObjective Optimization Algorithms Technical Report 2008 31 K Deb L Thiele M Laumanns E Zitzler Scalable multiobjective optimization test problems Proceedings IEEE Congress Evolutionary Com putation vol 1 May 2002 pp 825830 32 E Zitzler K Deb L Thiele Comparison multiobjective evolutionary algorithms empirical results Evol Comput 8 2 June 2000 173195 33 S Huband P Hingston L Barone L While A review multiobjective test problems scalable test problem toolkit IEEE Trans Evol Comput 34 GEP Box ME Muller A note generation random deviates Ann Math Stat 29 2 1958 610611 35 DE Knuth The Art Computer Programming vol 2 Seminumerical Algorithms 3rd ed AddisonWesley Longman Publishing Co Inc Boston MA 10 5 October 2006 477506 USA 1997 P Rakshit A Konar Artiﬁcial Intelligence 227 2015 165189 189 36 W Hörmann J Leydold G Derﬂinger General Principles Random Variate Generation Automatic Nonuniform Random Variate Generation Springer 37 G Marsaglia WW Tsang The Ziggurat method generating random variables J Stat Softw 5 8 2000 17 38 S Tezuka Uniform Random Numbers Theory Practice The Springer International Series Engineering Computer Science vol 315 1995 XII 39 JR Schott Fault tolerant design single multicriteria genetic algorithm optimization Master Science thesis Department Aeronautics Astronautics Massachusetts Institute Technology Cambridge Massachusetts May 1995 40 DA van Veldhuizen GB Lamont Multiobjective evolutionary algorithms analyzing stateoftheart Evol Comput 8 2 2000 125147 41 CA Coello Coello GB Lamont DA van Veldhuizen Evolutionary Algorithms Solving MultiObjective Problems 2nd ed Genetic Evolutionary Berlin 2004 pp 1341 p 209 Computation Series 2007 42 M Fleischer The measure pareto optima Applications multiobjective metaheuristics Second International Conference onEvolutionary Multi Criterion Optimization Springer Lecture Notes Computer Science vol 2632 SpringerVerlag Berlin April 2003 pp 519533 43 D Sheskin Handbook Parametric Nonparametric Statistical Procedures 4th edition Chapman HallCRC 2007 44 S Picek M Golub D Jakobovic Evaluation crossover operator performance genetic algorithms binary representation The Seventh International Conference Intelligent Computing BioInspired Computing Applications Lecture Notes Computer Science vol 6840 SpringerVerlag Berlin 2012 pp 223230 45 B Flury A First Course Multivariate Statistics Springer Texts Statistics Series 1997 46 J Derrac S Garcia D Molina F Herrera A practical tutorial use nonparametric statistical tests methodology comparing evolutionary swarm intelligence algorithms Swarm Evol Comput 1 1 March 2011 318 47 SO García AG Fernández J Luengo F Herrera Advanced nonparametric tests multiple comparisons design experiments computational intelligence data mining experimental analysis power Inf Sci 180 10 May 2010 20442064 48 LV SantanaQuintero CA Coello Coello An algorithm based differential evolution multiobjective problems Int J Comput Intell Res 1 1 49 P Rakshit AK Sadhu A Halder A Konar R Janarthanan Multirobot boxpushing differential evolution algorithm multiobjective optimiza tion Proceedings International Conference Soft Computing Problem Solving Advances Intelligent Soft Computing vol 130 2012 pp 355365 2005 151169