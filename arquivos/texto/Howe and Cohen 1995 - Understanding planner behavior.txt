Artificial Intelligence 76 1995 125166 Artificial Intelligence Understanding planner behavior Adele E Howe Paul R Cohen bl Computer Science Department Colorado State Universiry Fort Collins CO 80523 USA b Computer Science Department University Massachusetts Amherst MA 01003 USA Received June 1993 revised March 1994 Abstract As planners environments increasingly complex planner behavior increasingly difficult understand We understand causes fail debug failures understand allows succeed design generation This paper describes partially automated methodology understanding planner behavior long periods time The methodology calkd Dependency Interpretation uses statistical dependency detection identify interesting patterns behavior execution traces interprets patterns weak model planners interaction explain patterns caused planner Dependency environment Interpretation applied identify possible causes plan failures Phoenix planner By analyzing sets execution traces gathered 400 runs Phoenix planner showed statistical dependencies patterns behavior sensitive version planner increasing temporal separation events dependency detection degrades predictably number available execution traces decreases noise introduced execution traces Dependency Interpretation appropriate complete correct model planner environment available execution traces available 1 Introduction AI planners long introspective They sit thinking actions interact plans efficient issue plan Some planners execute plans planning complete The introspective nature planners difficult understand act especially Corresponding author Emaik howecscolostakedu 1 Email cohencsumassedu 00043702950950 1995 Elsevier Science BV All rights reserved SSDIOOO4370294000832 126 AE Howe iR CohedArtijicial Intelligence 76 1995 125166 regularities regularities planner includes interacting acting interleaved behavior frequently trouble connecting components planning dynamic environment Although know planners designed record design combine represent skips staggering permits predict explain modifications Our approach traces To understand planner behavior traces way effects produce complex execution internal workings planner external execution A planners actions events environment planner understanding tasks environment statistical planner behavior responsive actions actionsunexpectedly arise follow strict order Some dependencies traces use weak model planner explain frequent infrequent reasons mundane For example trace frequently execution execution First dependencies occurrences Dependencies action frequently alert includes actions subplan suggests problems example trace planner trace mundane plans executed search plans structures actions strict linear ordering actions When structures dependencies All steps use help debug completely failurerecovery long sequence obstacleavoidance trapped To explain dependencies index library explanations result cooccurrences automated semiautomated applied execution component Phoenix planner statistical dependencies relationships identify execution It straightforward slightly difficult However dependencies weak knowledge interpret dependencies Our approach fragments execution traces terms detailed histories internal external states explain statistical plans signify The contribution overlap dependencies paper planner environment terms general structures explain particular planning traces tendencies episodes 121 I 1 Related research taken approach Much research effort understanding particular understanding plans planner behavior focused plan fail plan planner failures debugged Researchers general positions plan planner debugging One bugs failures anticipated looking structure uncover pathologies simulating plan execution plans actually executing plans Sussmans HACKER earliest example approach 191 critics detect look structure plan level development potential problems Instead simulating increasingly execution identify satisfy actions actually simulating plan lead bugs For years researchers built planners discover bugs HACKER recognize structures opportunities inform level plan development complex constraints execution 181 AE Howe PR CohenArtcial Intelligence 76 1995 125166 127 161 debugs plans simulating causal model GORDIUS Many subsequent efforts relied simulation execution Hammonds lo simulates execution produce execution trace plan includes CHEF relationships plan actions resulting states CHEF chains backward execution trace determine steps caused failure classifies failure cause based explanation happened indexes set general repair strategies fix plan avoid observed failure Like CHEF Simmons traces GOFUXUS plan assumptions regressing desired outcomes values states causal dependency structure generated simulation plan identifying mis matches The repairs faulty assumptions set general repair strategies Both Hammonds Simmons approaches assume simulator correct model domain approaches differ kinds flaws detect strategies apply repair faults Related efforts rely causal models planner include Hudlicka Lessers work diagnosing failures execution 141 Birnbaum et al proposal enhance modelbased diagnosis plans 31 Most research addresses debugging plans debugging planner While related explaining plan fail step explaining planner favor plan little planner debugging The notable exception Hammonds CHEF learned plan failures CHEF remember repaired plans bugs subsequent planning use newly modified plans account bugs previously Others exploited idea modelbased explanation failures bugs errors arise execution traces learn new plans 251 Our position amalgam structural simulationexecution approaches focused debugging planner We think difficult debug planner identifying individual bug explaining arose terms history states variable bindings environmental events details Instead rely statistical dependencies point pieces plans look structural features pieces explain dependencies After modifications test explanations correct executing plans seeing particular dependencies disappear reduced Our approach parallels software testing program analysis For example Bates Wileden l developed language describing salient abstractions systems behavior module monitors systems behavior extracts event traces based desired abstractions Gupta describes knowledgebased selectively collecting analyzing traces interprocess messages 91 In systems human programmer examine resulting traces localized failures determine software failed debug As statistical approach technique hardware fault diagnosis called correspondence analysis classifies failure modes causes analyzing contingency tables test results 151 As knowledgebased approach DAACS Dump Analysis And Consulting System takes snapshot particular type fatal program error contents minidump matches information dump belief network canonical diagnoses 41 The result set hypotheses source failure 128 AE Howe RR CohenArtijcial Intelligence 76 1995 125166 Most research AI debugging explains particular failures order debug plan research software testing addresses finding patterns behavior order debug program Our approach combines explain patterns behavior time order debug program planner In particular use statistical technique called dependency detection identify patterns behavior followed knowledge based interpretation phase construct explanations planner produced behaviors The sections statistics interpretation 2 Dependency detection In section statistical dependencies execution traces A dependency unexpectedly frequent infrequent cooccurrence For example let A B C actions plan consider execution trace BABCCCBBCBABABCABAC One thing notice A occurs B following imme diately occurrence A followed C We represent contingency table action subsequence identifies row second action identifies column B 4 3 7 E5 A X Totals E 1 10 11 Totals 5 13 18 The table shows subsequence AB occurred times subsequence occurred In addition A followed B denoted fi table shows 3 occurrences D 10 occurrences m Apparently odds seeing B second element twoelement subsequence depends element A If element A odds seeing B 310 odds 4l element A In words presence A appears B likely This impression erroneous execution trace short numbers contingency table small apparent relationship A B accidental pattern random sequence letters Statistical tests contingency tables tell probability apparent relationships dependency A B chance We underlying probabilistic justification tests 7 Chapter 21 The common test contingency tables chisquare test 171 described later The test statistic use closelyrelated G test additive contingency table G 2 C fij In cells 1 AE Howe PR CohedArtiJicial Intelligence 76 1995 125166 129 fij number occurrences frequency cell iJ fij expected frequency cell ij Expected frequencies arrived ways specified extrinsically specify expected frequency males females sample calculated contingency table assumption row column variables independent In case G test test goodness fit extrinsic frequency distribution second test independence Dependency detection based tests independence 1 l71 Expected frequencies tests independence derived row column sums In table total numbers occurrences B B 7 11 respectively best estimate population probabilities B B 7711 l respectively By logic best estimates population probabilities A A 5 513 13 5 13 respectively Now occurrence B independent precursor A probability sequence AB product probabilities A B That assuming independence PrAB PrA x PrB 518 x 718 The expected frequency AB probability AB times 18 number items contingency table This product 518 x 718 x 18 simplifies 5 x 718 In general expected frequency cell row column j fij fi fj f l 9 fi f j totals row column j respectively f sum cells But remember formula gives expected frequency cells assumption column factor independent row factor Equation 1 simply sums deviations expected frequencies actual frequencies contingency table The larger value G lieve independence assumption Substituting expected actual frequencies contingency table Eq 1 obtain Gz2b1n lln 3ln lOln 5007 When value referred chisquare distribution degree freedom probability attaining result greater equal G assumption occurrence B independent occurrence A 00265 Thus claim B depends A probability wrong 100 The G test independence tells ratio B B significantly different row contingency table Thus called heterogeneity test tells ratios cell frequencies row heterogenenous In example heterogeneity significant Although dependency A B probably spurious dependency execution trace easy explain subsume Note example longer subsequence dependencies BB occurs times A place wildcard Perhaps We symbol designate single wildcard symbol 130 AE Howe PR CohenArttjkial Intelligence 76 1995 125166 10 603 90 10 603 90 10 603 90 Fig 1 An example dependency A B dependency depend previous action trace telling B tends followed intervening execution chance A intervening BB subsumes AB AB occurs primarily BAB relationships consider look longer sequences actions large number sequences execution For example BAB consider dependency action action times The question subsequence trace includes unexpectedly possibilities A B dependency instead AI3 independent existence Subsumption depend 1 There preceding action B 2 There dependency B B separated action dependency depend intervening action A 3 There dependency An example kind B A B shown Fig 1 The lower twoaction level represents resent upper With tree like dependency pends precursor odds seeing B x 130 action precedes expectedly CAB levels tree rep sequences considered possible precursors A B C sequences A B In Fig 1 odds seeing B A 16 ratios depend subsequences In words Fig 1 shows A B subsumed AAB BAB earlier AB AB AB B high cooccurrence twoaction Fig 2 shows example second kind AB dependency substitute depend In case odds odds seeing xB 130 odds wildcard action intervening seeing AB 16 depend intervening action A B C Sometimes dependencies strong propensity strong propensity overall contingency B following A 05 probability cancel Fig 3 shows case sequence AAB B follow AA threeaction B follow A sequences BAB CAB However A B probability B following x 05 table shows dependency A E Howe PR CohedArtijicial Intelligence 76 1995 125l 66 131 B B 10 60 10 Ml0 60 3 903 SO3 So Fig 2 An example dependency A B separated action dependency depend intervening action 24 6 132 132 7 1966 66 5 11 66 66 Fig 3 Although contingency table shows dependency A B B shows strong propensity follow A sequence AAB opposite effect holds sequences BAB CAB 26 162 2 246 2 9 4 13 2 9 3 12 Fig 4 The AB dependency appears previous action A 132 AE Howe RR CohenArtificial Intelligence 76 1995 125166 Lastly Fig 4 shows case AB dependency shows actions threeaction sequence AA dependency dominates data There evidence Fig 4 general twoaction AB dependency B appear depend A threeaction sequences BAB CAB With G tests differentiate cases illustrated Figs 1 2 3 4 To able test twoitem dependencies AB threeitem dependencies AAB When doesnt matter item substitute twoitem dependencies significant When dependencies cancel threeitem dependencies significant Usually threeitem dependencies significant Let build contingency tables test cases We begin contingency table Fig 4 reproduce Table 1 Table 1 The Fig 4 contingency table Ii mi Totals 39 450 Because Eq 1 sum differences expected actual frequencies partition components For instance calculate G statistic A row Table 1 G statistic x row add statistics G table Noting row sums 210 279 column sums 39 450 respectively calculate expected frequencies The G statistic row The G statistic second row calculated way The sum statistics 2028 exactly apply Eq 1 run test independence contingency table Gp 2 AE Howe PR CohenArtificial Intelligence 76 1995 125166 133 Theres magical uiy property consequence Eq 1 sum However decomposing G tells GA 940 G 1088 row contributes roughly equally total G value 2028 Said dif ferently frequencies 30 180 deviate expectations little frequencies 9 270 For convenience Gp denote G statistic Table 1 P stands pooled packed contrast G statistics calculate later Comparing Gp 2028 chisquare distribution degree freedom highly significant This means occurrence B B depends preceded A x says little possible effects item precursor denoted For instance tested possibility suggested tree Fig 4 dependency B A reflects strong propensity B follow AA 162 188 cases tendency g follow BA CA appears weaker relatively cases To test threeitem dependencies build slightly different contingency tables summarize frequencies AAB m Note row contingency table summarize contingency table structure lost summarized In Fig 4 example frequencies Al3 ti BAB BAi CAB CAB We unpack 30 occurrences AB 26 occurrences AAB occurrences BAB CAB Similarly unpack 180 occurrences We unpack second row Table lthe isnt necessary immediate purposes Unpacking row suffice yields Table 2 270 occurrences Bbut occurrences B Table 2 The unpacked contingency table B 26 2 2 9 AA BA CA x Totals 39 ix 162 9 9 270 450 The total G statistic Table 2 necessarily exceed Gp 2028 calculated Remember G measures heterogeneity degree ratios frequencies rows differ In general heterogeneity table increases row unpacked row replaced new rows apt heterogeneous Only new rows contain frequencies exactly proportions unpacked row heterogeneity remain constant example shortly The G statistic Table 2 denoted GJ unpacked easily computed withEq 1 134 AE Howe RR CohenArtificial Intelligence 76 1995 125166 Gu2 As expected Gu Gp We let Gu GJ Gp 0287 denote increase heterogeneity unpacking Because Gu small know ratios new rows Table 2 AA BA CA differ little themselvesthe frequencies similar In fact ratio row AA 26162 roughly 16 ratios rows BA CA 145 The raw row frequencies different Row AA boasts 188 occurrences B h rows contain 11 occurrences The contributions row Gu reflect differences We derived results First strong dependency exists precursors A A subsequent occurrence B B Gp 2028 Second heterogeneity introduced unpacking A AA BA CA negligible Gu 0287 Third precursor AA contributes Gu BA CA How interpret results We Table 1 B likely follow A A probability observing B l6 following A l30 following A We dont know 1 presence absence A precursor responsible 2 presence absence particular item occurs place respon sible 3 interaction particular substitution subsequent item precursor responsible In fact results support interpretation B follows A frequently A know presence absence A second item precursor increases likelihood observing B But story Unpacking AE Howe RR CohenArtificial Intelligence 76 1995 125166 135 propensity ratios frequencies AA BA A row Table 1 affects heterogeneity little similar CA rows similar similar A row In words propensity B ratio frequencies follow A similar follow AA BA CA This suggests item twoitem precursor little influence occurrence B second fact 188 210 cases A row Table 1 actually AA precursors BA CA contribute little Thus conclude B likely weaker The conclusion AA influences B precursor AA strongly affects occurrence observing B A influences B general effects precursors BA CA consistent precursor A However ignores item We ready quantify conclusions Keep mind G statistics additive As result construct summary results like Statistic G GAA A A GX GJ GH Value 2028 7327 118 118 1088 20567 0287 Significance significant significant significant significant significant significant significant estimate cell frequencies The result Gp 2028 tells A influences B row sum Similarly sums fixed use results significant conclude item precedes A specific test 2 x 2 table degree freedom The row doesnt matter This column row contains C columns C 1 free total sum table Thus vary column R constrained rows R 1 degrees freedom Thus R x C table R 1 x C 1 degrees freedom The result table Gu 20567 4 x 2 unpacked degrees freedom significant It tells ratios row frequen different Because G additive cies rows rows AA heterogeneity Clearly unpacked ask row pectations grees freedom cies constrains G columns ex test row C 1 frequen tests GAA GA GCA row A contribute The rows BA CA insignificant degree freedom contingency cell frequencies Each measured G answer estimate expected Table 2 significantly Table 2 responsible frequencies row sum tables row The different We different results calculate G statistics Fig 1 For example statistics execution traces Fig 1 summarized 136 AE Howe IR CohenArtial Intelligence 76 1995 125166 Statistic GP GAA GBA GCA Gx Value 940 3133 3133 3133 1088 Significance significant significant significant significant significant Gv GH 94 0 significant significant The interpretation results significant dependency A B twoaction subsequences effect identical 0 substitutionsA B Cfor In fact individual threeaction dependen cies significant example ratio occurrences B B sequences AAB ti significantly different expected population ratio GAA 3133 In short second action precursor A action influence occurrence B It turns situation Fig 2 yields identical G statistics similar interpretation case Fig 1 The difference look differences precursors AA AB AC instead precursors AA BA CA However ratio B B influenced actions AA AB AC cases ratio 16 Thus 0 p Er 94 We conclude data dependency A B action intervenes intervening action influence dependency Finally consider situation dependencies cancel shown Fig 3 The G statistics case Statistic GP GAA GBA A GGi GJ GH Value 0 11565 5754 2306 0 19625 19625 Significance significant significant significant significant significant significant significant When precursor AA ratio B B subsequent action significantly different expectation GAA 11565 Similarly precursor BA ratio B B significantly different expectation GBA 5754 If precursor CA effect evident GCA 2306 Despite strong effects precursors effect evident data pooled Gp 0 This threeitem dependencies cancel The entails AA likely followed B second says BA AE Howe PR CohenArtificial Intelligence 76 1995 125166 137 likely followed B These opposing tendencies yield highly heterogeneous table A row unpacked Gn 19625 course invisible A row GA 0 We note passing test Gn degrees freedom rows result unpacking A row sums constrained A row sum An alternative formulation run ordinary test independence table rows AA BA CA produce Gn clearly degrees freedom In sum subsequence XYZ successive actions execution trace differentiate cases Z depends Y irrespective action X Fig 1 Z depends X irrespective action Y Fig 2 dependency Z Y depends X Figs 3 4 3 Interpretation Dependencies raise questions answer They suggest relationships expect confirm knew spurious Interpretation distinguishes situations explains relationships suggested dependency terms planner environment Interpretation parts identifying plans involved dependencies constructing explanations plans produced dependencies In contrast detecting dependencies parts rely knowledge planner environment 3 I Identifying suggestive plan structures For interpretation search means producing dependen cyhow event action influence occurrence The purpose step identify planning structures knowledge mechanisms influence particular events suggest means dependency If event depends medium interact search description plans We detect unusually low cooccurrence events present try explain lack desired cooccurrence Determining produce dependencies requires knowledge planner environment Yet conflicts goal minimizing reliance model planner environment Our solution supplement available execution information knowledge structures available planner weak model planner interaction environment The process identifying suggestive plan structures starts selecting depen dency attention We select dependency pragmatic reasons First present search suggestive structures supported set Lisp functions type suggestive structure tedious run functions dependency Second step fully automated designer wade lot information structures explanations Focusing attention single dependency reduces possible deluge information 138 AE Howe PR CohenArtificial Intelligence 76 1995 125166 One careful selecting dependency interpretation Spurious dependen low dependency cies easily avoided selecting strong dependencies probability chance based dependency tenacity tions utility observed outcomes significant Dependencies changes appear impervious desirability judgment unexpected designer decide ranked condi observed behavior value environmental instances Determining Having attention selected dency actions actions plans determine precursor influenced event depen dependency plans structural descriptions interaction dependency Roughly speaking situate dependency dependent associate remains Associating dependencies actions plans dependency First need locate localize produces composed plan actions environment behavior Dependencies role planner producing planner Plan actions need associated plans appear Environment plan actions ongoing plans Consequently specify ways events dependency plan actions events To deter dependencies need relate parts typically information events detected directly indirectly plan actions detected ways initiate sensor activation processing relate sensory dependency sets actions dependency mapped appear The mapping searched dependency possible space includes predecessor Action A dependency represent hierarchical longer concentrate precursor antecedent dependency dependency solid relationship dependency indicate lines plans plans demarcates interactions For example Fig 5 shows plan portion plan Action P temporal precedence dotted antecedent lines process From point interpretation parts plans actions dependency involved short longterm memory planners shortterm memory Finding suggestive structures identified actions know Having pendency We search The problem searching tories environmental runs planner irrelevant The problem searching plans plans described wrong actions strategies inferring behavior timetoo procedural actions instantiated plans little plan fragments actions included events overwhelming longterm memory plan expansions variable bindings recorded planners different plans representation planning largely AE Howe PR CohenArtcial Intelligence 76 1995 125166 139 Fig 5 A dependency mapped actions plan The shaded region indicates portion plan possibly relating dependency actions We supplement The suggestive plan structures available declarative plan representation structural knowl form executable combines plan fragments formed plans shared commitments edge planner plans relationships combinations nate actions expectations gest means events planners structures efforts separate plans agents constrain decision making efficiency world They called suggestive plan fragments result coordi idioms course action shared sug In cases plans structures interact promoting plan reuse help coordinate plan languages support generalizing hopefully improving languagebased specific variable actions included instantiations structure structures To identify relationships particular dependencies search Fig 5 structural In case temporal structure predecessor guaranteed plan dependencies suggestive follow actions These structures basic types temporal control antecedent necessarily called Sequential Actions plan One easily distinguish suggestive parts plans demarcated data interactions follows action control data relationships action determines later action included hierarchical expansion parent determines actions included lower suggestive plan A data interaction For example actions structure uses plans constitute control structure level called Selection Constructs action adds plan actions share information plan actions set values plan variables plan called Shared Variables action sets variable figure A control structure plan For example actions relationship structure structure While planners construct plans types structural relationships set suggestive ways planner suggestive structures structures depends information incorporates fundamentally environment plan language plans The set planner described Section 42 140 AE Howe PR CohedArtijcial Intelligence 76 1995 125166 32 Explaining suggestive structures produce dependencies The second interpretation tures produced stories happened suggestive structures combine dependencies They precisely determine hypotheses source observed dependency observed dependencies The explanations struc brief cause cause dependency suggestive explanations The explanations intended explain caused antecedent This subtle distinction cooccurrences causal share cause emphasize ways events cooccur dependencies explain relationships Two events cooccur chance The occurred precursor Dependencies explanations cause dependency An example explanation parts different plans Resource Contention describes happens plans vie resource able acquire access Resource contention common multiple agents plans compete limited resources occur plans forced temporal sequencing controlled An example explanation Overcommitment describes plan inappropriate retracted environment suggest overcommitment environment environment strong temporal sequences data dependencies plan actions control decisions based possibly old information need particular ways Suggestive structures dependency actions environment resource management changes The result interpretation phase set hypotheses planner observed dependencies Given set suggestive structures try phase limited knowledge interpretation correct Just human programmers phase explanations produced explanations guaranteed behavior program planner 4 One application Failure Recovery Analysis We applied dependency interpretation planner Phoenix led specialized This section describes target behavior vironment planner Recovery Analysis explain cases words assist debugging recovery process planner form dependency plan failure understand plans fail Phoenix target en called Failure identify exacerbate cause failure environment interpretation includes FRA The purpose Failure Recovery Analysis plans influence time failure Most planners operate recovery Even reactive systems planner recognizes dynamic environments include reactions repairs plan include responding Failure execution form failure desirable situations AE Howe PR CohenArtificial Intelligence 76 1995 125166 141 1 Run Planner Execution Traces FRFRF 5 Modify Planner Explanations Dependencies FFRFFRF Suggestive Structures Failure Recovery Analysis designer iteratively debugs planner testing fig 6 The cycle planner interactions We chose focus failure recovery aspect planner environment record analyze reasons First failure recovery influences failures occur Minor changes design failure recovery produce significant changes number types failures discovered series experiments Phoenix 131 Second failure recovery uses plans ways explicitly foreseen designers forbidden prevented Failure recovery repairs plans adding replacing portions As result plan include plan fragments juxtaposed orders contexts envisioned designers Consequently failure recovery test limits planner FRA proceeds iterative process designer tests planner interprets dependencies modifies planner based dependency interpretation Fig 6 The process continues designer satisfied resulting planner The designer starts running planner environment The Phoenix automates step automated experiment controller varies environment predefined ranges collects execution traces failures occurred repaired An example execution trace j R F Rp Fn R_yp Fip R Fnrs Fs failure types Rs recovery methods The subscripts indicate individuals set F means failure type ner Second execution traces searched dependencies recovery efforts failures These dependencies tell designer recovery actions influence failure 3 occurs failure influences For FRA dependencies consist combinations failure types recovery actions repaired We 3 The failure refers failure appears execution removes time stamps temporal separation unknown execution immediate hours later trace Because trace 142 AE Howe lR CohenArtcial Intelligence 76 1995 125166 types FF detect dependencies RF recovery action followed failure type FRF failure recovery method described equations list execution type followed failure type type accept Section 2 implemented traces input return list dependencies followed failure Dependency detection Lisp functions repaired output failure selects dependencies attention tries designer planners Third determine The dependencies suggestive failure To suggestive structures structures involve mapped structures actions caused observed dependency actions plans plans searched susceptible actions known designer runs set Lisp functions check Fourth designer matches suggestive modifications At present possible explanations signer look set explanations The explanations according modifications described structures set dependency step automated indexed suggestive structures terms listed general modifications indicate explanations designer planner chooses modifies structures At end cycle understanding suspected modification incidence failures changed achieved flaw The cycle begins designer running designer search flaws fix determine desired effect observed dependency disappeared planner likely explanation remove based planner Next time better fully automated In contrast approaches designer ultimately attention generality Because explanations planners guarantee failure fix planner uses weak model debugging FRA procedure applied human focus The designer decides trades power structures bugs actual cause suggestive bug FRA repair inherent localize broad range bugs appropriate 41 Phoenix target planner environment 81 serves laboratory Fig 7 Phoenix provides The Phoenix system4 interpretation As shown agent architecture set plan knowledge bases type agent experimental collecting data Its environment application dependency Yellowstone National Park automatically simulated environment experiments controlling interface forest fighting The goal forest fighting irregular spread moisture content wind speed direction natural boundaries large roads Fires contained removing fires efficiently possible Forest fires rates function ground cover elevation bodies water fuel paths causing shapes variable contain 4 Phoenix refers entire simulator components distinguish planner Whenever possible use Phoenix planner comprise planner agents including parts AE Howe PR CohenArtificial Intelligence 76 1995 125166 143 Simulation Controller V Forest Fire Simulator 4 Fig 7 Diagram separate processes comprise Phoenix The arcs processes indicate transfer data processes fireline One agent fireboss coordinates burn called building field agents bulldozers possible Other agents watchtowers gasoline carriers helicopters activities fireboss bulldozers gasoline fireline exploiting natural boundaries support delivering refuel agents gathering field activities surround information Fig 8 shows interface simulator The map upper display depicts Yellowstone National Park north Yellowstone Lake Features wind speed direction shown window features types shown light lines grey shaded areas Four rivers roads terrain fireline near center figure A watchtower bulldozers building visible near center upper left geographic reflexes planner Sensors perceive environment change All Phoenix agents agent architecture consists sensors ef local planner level compe particular cognitive state environment Each reflexes occur component Together control layer provides faster actions avoids detrimental fectors agent effecters form twolayer tence Reflexes address changes respond planner coordinates The planner plans skeletal structure plan conditions plans executed information environment environment monitoring developing library plans partially detailed uninstantiated interactions timeline searches task needed state Plans expanded instantiated refinement new tasks added progress planner available plans appropriate plan By measure Phoenix environment challenging AI planners As result Phoenix plans fail lots reasons The environment planner fail Phoenix plans fail plans fail situations bases plan slow change abilities agents limited based obsolete uncertain include bugs change unpredictably environment sense plan Phoenix possible environment information tested 144 AE Howe PR CohenArtificial Intelligence 76 1995 125166 Fig 8 View Phoenix simulator bulldozers fighting 42 Phoenixspecific knowledge FRA FRA uses kinds domainspecific knowledge types failures recovery actions suggestive structures explanations At present Phoenix planner recog nizes 11 types failures applies recovery actions repair failures The failures types listed Table 3 recovery actions Table 4 While failure types obviously specific Phoenix planner recovery actions sugges tive structures explanations designed generalize Phoenix We believe apply characterize planners aptly Phoenix process acquiring planners analyzing structures apply Following Sussmans lead plans 191 envision defining canonical bugs fixes classes planners Suggestive structures Suggestive structures determined plan language The Phoenix plan language fairly impoverished representation goals effects actions largely procedural language reasoning opaque The suggestive structures Phoenix planner exploit information available structure plans The following listing suggestive structures acquired far Phoenix planner AE Howe PR CohenArtificial Intelligence 76 1995 125166 145 Table 3 Failure pes Phoenix planner CCP ccv CFP FNE NER NRS PRJ PTR RU IP ZBT Cant Calculate Path The planner safe path points map Cant Calculate Variable This failure type catchall unable assign value variable Cant Find Plan Planning operates searching plan library plan fits requirements context plan appropriate current state environment This failure type detected plan meets criteria Fire Not Encircled Each firefighting plan includes step check contained failure detected fully encircled fireline Not Enough Resources Firefighting plans estimate resources required contain particular fires failure detected large available resources No Remaining Segments This failure indicates mismatch allocation work individual agents Cant Calculate Projection This failure detected size projected future time bugs code anomalies available information Cant Calculate Path Road Normally paths calculated points variant path calculation calculate path point nearest road Resource Unavailable Resources assumed available initial planning actually unavailable plan later attempts allocate Insufficient Progress Envelopes detect differences observed expected progress plans failure detected observed progress inadequate Zero Build Time In examining particular planner contained requires fireline Table 4 Recovery actions Phoenix planner WATA Wait try failed action RV RA SA SP AR RP RT Recalculate variable failed action Recalculate variables failed action Substitute similar plan step failed action Substitute projection action failed Assign additional resources plan Abort current plan replan parent level level plan immediately Abort current plan replan level redo entire plan uses mismatch Shared Variables One action sets variable uses Shared vari value getting failure variety reasons changes ables vulnerable propagated assumptions value set confusion Shared Resources Two actions allocate use resource Separate plans access bulldozers global instance frames The Phoenix planner arbitrate resource use fuel carriers resources Resources units variable types data structures come served refer physical shared resources variables 146 AE Howe PR CohenArtificial Intelligence 76 1995 125166 l Sequential Actions One action orderings Sequential barring inexorable extremely weak information determining action completely unrelated guaranteed follow vulnerable progression replanning intercession flaws planner typically precedes plan action This structure merely useful plan opposed plans l Unordered Actions Two actions unordered plan Actions actions reason dering constraints opportunistically independent mistake designers modification independent interleaved planner run time Typically resulting order produce effects unrelated failure recovery designer interactions actions meaning indicate expects dolist Phoenix plan underlying languages iteration constructs The assumption l Iteration Constructs Multiple actions added plan decision language supports action Like programming constructs language wish similar action example construct adds plan number includes action different variable binding Because action action share assumptions vulnerable l Repeated Actions One action diflerent plans gets times diflerent contexts Some planning plans For example path calculation time agent moved place These oft repeated actions difficult difficult program predict advance possible situations tend programmed actions executed actions repeated rest environment included incorrectly l Environment References One action senses result actions share assumptions state environment calculate variables planning Some plan actions sense environment If decision calculation decisions based condition environment assumes constancy environmental calculations decisions vulnerable environment passes conditions failure Explanations Dependencies result action causing later failure incidental relations structural list cataloging planners plans The catalog explanations influences failure intended ways complete failure dependencies Phoenix planner identifies Phoenix planner As suggestive structures Phoenix serve starting point produced l Overly Constrained Environment Assumptions A sequence plan actions assumes environment account changes stability types failures conditions vulnerable constancy environmental conditions Actions higher frequency certain environment resulting assume changes AE Howe lR CohedArtiJScial Intelligence 76 1995 125166 147 A suggestive structure suggests explanation planner modifications actions coordinate update actions environment remove references depen model environment stay require environment explanation Given dency add monitoring reorder actions l Implicit AssumptionxTwo actions different assumptions value plan variable extent later actions requirements successful explicit plan variables execution violated Some plan expectations values plan underlying implicit variables assumptions Some suggestive structures suggest combined shared variables unordered tions iteration constructs planner plan description incompatible assumptions remove actions plans combined shared variables Some modifications dependency explicit change add new variables plans explanation actions actions combined repeated ac sequential l Resource ContentionSeveral plans vie resource treats ongoing plans independent minimizes able acquire access Resource contention agents plans vie limited planner actions allows reuse plans makes resource contention likely parallel actions combined remove common multiple resources For Phoenix search inter A suggestive structure explanation planner suggests resources Some modifications institute reservation policy reserve shared dependency resource availability tervals plans interleaved sequence contending immediately plans decision resources based resource decisions use create protection coordination flexibility moderating plan overly constrained l 0vercommitmentA set actions determined far advance trade discover state world fail early structured planner change sequence accommodate change Plan designers efficiency plans Sometimes information information Some suggestive actions combined shared variables parallel actions combined shared resources Some planner recovery modifications explicit knowledge gleaned shared assumptions planning decision making change subplan means later plan different choice try thing failed structures remove failure explanation dependency later decisions suggest parallel influence built l Temporal Sequencing Plans When actions strictly ordered plans failure detects detected early plan follow plan restarted plans occurrences match order A failure occur action executed A failure detected later actions type interleaved 148 AE Howe IR CohenArtcial Intelligence 76 1995 125166 A suggestive explanation replace structure suggests explanation planner modification remove sequential actions Given dependency temporal structure opportunistic control l High Base Frequency Planning actions high base frequency failures detected fre strict actions repeated dependencies opportunity detect failure Some planning plan This lead high base frequency quent actions lead spurious dependencies ordering effects A suggestive ner modification remove action different situations dependency suggests structure repeated actions A plan create different versions explanation times l BandAid SolutionsA recovery action repair immediate failure failure symptomatic deeper problem leads subsequent fail ures In sense symptom detected failure inevitable causes makes Some suggestive structures suggest explanation RF dependency combined unordered actions shared variables FRF combined sequential references FRF dependency Some planner recovery modifications limit application suspect recovery action force replan earlier add new plan structure related recovery methods failures identified actions environment failure change remove dependency dependency repair l Downstream Failures Recovery actions disrupt flow control plan actions cause later failures The recovery method alter fail subsequent plan actions conditions Recovery cause later failures making poor repair updating plan variables expectations causing related structures repair suggest Some suggestive explanation actions shared variables local repair method RF dependency combined sequential WATA RV RAV SA FRF dependency actions environment remove action narrow detrimental relationship dependency scope changes failure recovery effects modify repair subsequent plan description combined sequential recovery produce explicit suspect references Some planner recovery modifications application limit failure R l Stealing A recovery action significantly modes expectations constraints assumptions plan preclude host related failures making unrelated failures likely occur When failures prevented avoided relaxing constraints internal model match environment plan modifications appear frequent In effect recovery method steals failures normal flow causing replaced Some suggestive structures suggest explanation plan updating failures related dependency R replan substitution FRF modified prevent RF dependency action The planner good effect AE Howe RR CohenArtcial Intelligence 76 1995 125166 149 5 Empirical evaluation Phoenix planner help programmers useful need assess information Phoenix planner technique effort required To determine FRA feasible need know procedure works learn know applying FRA To determine particular applying demonstrate usefulness FRA determining information execution effort expended sensitivity underlying gained In section FRA feasible applying FRA Phoenix planner assess gained different sets relationship traces Phoenix planner estimating execution gain information traces collected statistics 51 Demonstrating FRA Phoenix feasibility To demonstrate FRA procedure applied help debug traces 94 Phoenix planner fought refer set traces Base Case The fires set hour intervals current version Phoenix planner First collected execution experiment fires course 60 simulation execution wind speed direction allowed change roughly hour included 968 failures hours trials From failure recovery analyze 15 FF dependencies Fip expensive RSp F We selected dependency Fig 9 The dependency Third ran Lisp functions Second ran Lisp functions previously added improve execution efforts failures We 46 dependencies 16 FRF dependencies traces statistical depen 15 RF dencies set dependencies fail selected interpretation repair precursor ure frequently occurring includes failure recovery method recovery performance interacting detrimentally parts plan This R followed Fip observed 52 times set execution relationship common pattern behavior traces suggests map dependency includes recovery action precursor R transforms failed indirect attack plan abbreviated PiO shown failure successor Fip R type fireline projection calculation repaired plan PA substituting actions action calculating model projections multiplefkedshell Ap_fs tightshell A based Ap_mb RSp replaces Failure FQ detected plan monitoring time remains structure 11 I called indirectattackenvelope Aen comparing The projection calculation suggestive indirect attack plans All indirect attack plans include different structures Shared Variable Sequential Ordering All projection calculation actions set variable attackprojection envelope action The envelope action follows plan Fip detected envelope action actions envelope action appear failed The Phoenix plan suggestive plan structures insufficient complete expected indirect attack plans progress actual progress planner different projection different calculation indicates includes library action 150 AE Howe RR CohenArtcial Intelligence 76 1995 125166 Dependency SharedVariable Fig 9 Mapping dependency suggestive smxtures Fourth looked explanations observed dependency suggestive structures pro repair flaw structures suggestive dependency identified possible modifications different explanations Xmplicit Assumptions duced Combinations structures ing underlie The Shared Variable cause failure sets variable differently plicit assumptions properly monitored violate monitoring Alternatively symptom deeper failure raging control available lead explanations The suggestive Rsp fip Shared Variable Sequential Order Bandaid Solutions action calculation im acceptable progress recovery action R lead FQ recovery action repairing task substituted projection expected envelope action violating resources inadequate projection specified bandaid cure envelope assumptions structures implement suggestive solution limit repairs bandaid FRA procedure planner With FRA select The results analysis explanations dependency dependency At stage possible designer look designer decides change repair The stereotypical repairs needs stereotypical application recovery action R add new recovery action failure Fe In case recovery failures action Rsp added improve Fpv Fner removing levels Alternatively adding new recovery method produce new dependencies For reasons rejected The stereotypical constructive use explicit defining additional variables We checked involving type dependency failure use attackprojection In particular variables code code envelope code code detects variable modifications unlikely repairs implicit assumptions promising precursor dependency set performance recovery performance assumptions expensive previous assumptions repair helpful projection The projection code calculates point future The versions projection estimate projection extent code differ calculation AE Howe PR CohenArtcial Intelligence 76 1995 125166 151 available projections search resources The estimates resource performance summary expectations progress fight estimate combined likely performance fireline time building include factors bundled construct summary variable variable The envelope code uses plan code obvious actions differed performance estimates set estimated fireline rate travel startup envelope assumed rate building refueling overhead Because rate building fireline conditions different projection actions To accommodate actions restructured envelope action combines set separate variables separate variables signaling define By examining projection capabilities included times new instructions summaries failures effectively varied differences capabilities expected progress reflected projection observed intended described As changes minutes treated seconds A code projection Fpes previously actions optimistic estimated sixtieth desired value Because variables unlikely FRA buggythat Consequently fixed bugs indicated needed local projection FRA bugs detected fixed actions The bugs certainly caused data led projection activities parameter estimate bugs code recorded plan use parameters code buggy suggested way variables set consistently aspect planner projection projection parameters fixed bugs FRA indicated involved leading modifications effective RspF dependency traces overall rate failures decreases To test 87 trials fires set traces dependencies The dependencies included 12 FF dependencies traces 87 trials execution We tell new execution execution detected modifications ran modified planner 12 hour intervals analyzed detected 12 RF dependencies detected hour5 hour yields significant The modifications new execution result z 1311 zero FRF dependencies The Rspl dependency traces rate failures went 0414 failures mean failures 0333 failures hour z test differences p 00001 different type Fp accounted planner resulted type Failure 03 new execution type detected projection FCC decreased 25 increased incidences failure Base Case experiment failure 16 12 increased traces significantly Table 5 lists incidence failures Base Case 208 failures traces Similarly calculation 15 12 Yet overall percentage counts failures F Fprr execution incidence F 42 decreased I 5 Failures hour measure general measure progress activities agents 152 AE Howe PR CohenArtificial Intelligence 76 1995 125166 Table 5 Failure counts Base Case modified planner execution traces Failure type Base Case Modified planner CCP ccv CFP FNE 1P NER NRS PFR PRJ mR RDU ZBT Number 232 5 143 I 381 246 3 0 321 76 122 4 1540 0151 0003 0093 0005 0247 0160 0002 0 0208 0049 0079 0003 1000 Number 142 0 92 5 415 122 15 2 1 147 129 7 1137 0125 0 0081 0004 0418 0107 0013 0002 0001 0129 0113 0006 1 Ooo counts different types calculation An optimistic bugs code implementing traces modified planner Most changes failures probably removing changes projection failure types removing detected early fighting plans failures data experiment enhanced failure optimistic record measure progress failures plans getting plan obvious Without explanation If fighting plan cause early plan failures detect general detected later points trend progress know increase difficult detected explanation correct procedure fewer failures suggestive structures explanations Based dependencies following This example shows designer apply FRA help identify repair flaws planning modifications FFL4 directed examine modify portion code Phoenix planner The modified planner detected avoided modifications Because fixed bugs identified FRA virtually change motivated finding bugs FRA pointed source impossible matched problems explanation led examine relationship improvement applying FRA For complexity analyze code thought help FRA FRA useful assumed previously fix problem exhibit impossible previously dependency selected correct conclude trials These new execution As example performed cycle FXA First collected execu tion traces 102 experiment conditions time intervened thors Explorer experiment moved University Massachusetts Colorado State University The dependency failure 23 overlap previous example au traces collected considerable sets new traces similar included 1043 failures 0261 failureshour significantly lower went rate AE Howe PR CohenArtcial Intelligence 76 1995 125166 153 0333 failureshour installing Colorado probably general fixes course new plan fight F selected attention We 23 Rn F We selected dependency data cells contingency dependencies value upper left repair cost ranking repair Third suggestive action aborts plan expensive replanning bulldozers resources Shared Variables detected projection inadequate setting assessment available state Fourth explanations Overly variables weather explanations explicit examine decision coordinate assumptions Implicit Assumptions Because Second searched dependencies set selected expensive The suggestive dependency Re based reasonable available structures dependencies considering table 5 ranking cell picking So Rn Fner common structures progress searches planner discovers contain variable describing bulldozers current weather conditions Constrained Environment Assumptions involved inclined environment projection select plan replanning environment recommendations replanning combine assumptions hold replanning referred likely state environment code Our intuition sighted replanning decision sync possibly demanding criteria state Sequential Structure Environment References reference extremely differed related changes reliance available Based analysis set closely limited flexibility varying environment Phoenix criteria select plans assigned variables based plan selection Phoenix select general easy condi chosen use resources available selected resources tight estimated aspects state world selected condi average selected random The planner We changed assumptions fighting plans Of situations Although tions randomly The selection accurate tions differed considerably characteristics analysis code Additionally available explicit plan selection plan observation number bulldozers remaining stingy resources criteria modified time assignment bulldozers plans assessed pilot experiments assumption able We ran percentage trials We analyzed conditions results collected execution 82 ex periment disappeared 1 However 0346 failures hour We choices point We change try different solution sume change needs better tuned run pilot experiments test set ones selection criteria examine left programmer Rn Fner dependency 9 failures introduced change fix The decision F reduced dependency rate increased failure traces 154 AE Howe RR CohenArtificial Intelligence 76 1995 125166 cases dependency detection assess results IXA suggest additional changes 52 Utility dependencies We assess utility PRA estimating information gained relative effort required We focus dependency detection core technique IRA dependencies information drives rest procedure effort required dominated collecting execution traces We wished determine quality information dependencies capture weak strong relationships dependencies summarize abstractly interaction planner environment We estimate information gained describing strength dependencies series experiments Phoenix showing dependencies change planner modified time precursor antecedent increased In words dependencies characterize strong effects dependencies reflect planner time passes planners actions dependent event We estimate effort required detect dependencies estimating sensitivity dependency detection data available noise data summarizing computation required Our measurements based set sets execution traces Phoenix planner gathered series experiments In experi ments failure recovery portion Phoenix planner incrementally modified The experiment Base Case Section 51 fourth experiment volved modified planner described section 521 Information gained As planner environment changes changed type strength dependencies changes Prior gathering sets execution traces described Section 51 gathered execution traces previous versions planner versions successively simpler versions failure recovery analyzed dependencies Amount information If dependency detection discovers dependencies designer swamped information volume useless If dependencies strong meaning probability noise chance low designer confidence dependencies accurate characterization relationship precursor failure type Different dependencies detected execution traces versions planner In terms evaluating designer likely appears dependencies detected swamped dependencies experiments execution traces different number dependencies detected overwhelming The dependencies detected experiment 46 Base Case Section 51 fewest dependencies detected 24 The total AE Howe RR CohenArtificial Intelligence 76 1995 125166 155 COI Jnt 60 60 40 20 c05 I0 C20 20 Probability given G contingency table Fig 10 Histogram p values precursor failure combinations FF RF FRF data sets p values probabilities G value contingency table noise chance sets execution traces 125 To sense dependencies likely detected Fig 10 shows distribution p values data sets We detect dependency precursor failure type p probability G chance Y Y 005 As expect overwhelming majority p values LY In figure column count precursorfailure combinations p upper limit previous column limit listed column 10 means count 005 p 10 The strength dependency measured terms probability ratios observed dependency arose chance noise The majority dependencies detected sets execution traces Y threshold In fact histogram Fig 11 shows half dependencies p 001 indicating dependencies detected highly significant Sensitivity dependencies planner version To determine dependencies reflect changes planner tested overlap dependencies detected sets execution traces Additionally tested temporal persistence dependencies analyzing exe cution traces dependencies precursor failure occurs later execution trace The incidence failure types changed experiments Consequently expect dependencies detected execution traces experiment change We tested expectation comparing sets dependencies detected combinations experiments counting overlap The results summarized Table 6 The planner experiment variant planner experiment variant experiment variant experiment Thus expect adjacent experiments share relatively dependencies 156 AE Howe RR CohedArtijkial Intelligence 76 1995 125166 Fig 11 Histogram p values dependencies The dependencies detected execution traces experiments Probability given G contingency table Table 6 Number deoendencies shared bv combinations exneriments l2 23 34 l3 24 l4 123 234 1234 RF FF FRF 1 9 1 Totals 11 1 5 1 I 4 4 0 8 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 1 0 1 nonadjacent experiments share relatively dependencies Data different experiments share dependencies dependencies capture structure interactions planner planner environment planner changes dependencies change In fact execution traces experiments exhibit phenomenon experiments dependencies common set fewest common Sensitivity dependencies temporal separation Dependencies represent downstream influences precursors later failures A RsP fiP dependency indicates RsP influences makes likely occurrence Z failure Detecting downstream influences implies expect effects precursors failure types recovery methods persistence long actions detected failures repaired finished In evaluating dependencies capture downstream influences need determine long steps execution traces precursors influence failures downstream FRA detects dependencies precursor failure immediately fol lows The algorithm designed detect dependencies precursor AE Howe FR CohenArtificial Intelligence 76 1995 125166 157 limited number failure immediately following pragmatic reasonsit patterns needed tested detection code Dependency detection runs G tests possible patterns precursors failures Thus number patterns increases combinatorially add earlier failures recovery methods precur sors making computationally expensive consider longer earlier precursors Practically need know missing downstream influences limit patterns include previous failure We divide concern missing information parts Should precursors include single pair failure recovery method repaired Do precursors influence failures failure To answer question modified dependency detection test longer patterns patterns failure recovery method looked dependencies detected precursors longer complicated To answer second question modified dependency detection test dependencies failure later failure example failures later As question looked dependencies detected temporal separation increases precursor failure FRA tests dependencies types FF RF FRF failures recovery methods appear execution traces For example execution traces experiment 1 contained 10 different failure types different recovery methods 600 possible patterns FRF 600 different patterns length tested Given number different failure types execution traces number different recovery methods calculate number possible patterns length precursor For example precursor length RFRF produces 4620 possible patterns 3600 RFRF 600 RFF 360 R RF 60 R wF The stands wild card value meaning matter value position long execution trace appears values R F means particular recovery method R followed failure recovery method finally particular failure F Testing possible patterns FRFRF require 59280 tests execu tion traces experiment The execution traces experiments included 3900 failures collected 15000 simulation hours suggests computation time concern execution traces contain patterns So need determine worth testing longer patterns execution traces We modifying dependency detection code test length precursor examining rates detection Table 7 shows results testing dependencies precursor length FRFRF execution traces experiment 1 Clearly number possible combinations failure types recovery methods quickly outpaces size execution traces length precursor increases percentage dependencies detected patterns execution traces decreases 23 RF 9 FRFRF Table 7 suggests expect influence precursors decays time A smaller percentage long precursors detected dependencies number significant combinations short precursors For example GFound divided number different combinations execution traces 158 AE Howe FR CohenArtificial Intelligence 76 1995 125166 Table 7 Dependency search space precursors FRFR Combinations Found G test GCombinations GFound RF FRF RFRF FRFR F Totals 60 700 4620 53900 59280 40 160 531 1737 2468 9 36 50 162 257 0150 0050 0011 0003 023 023 009 009 Table 8 Dependency search space temporally separated failures Execution traces experiment Combinations Nonempty G test GComb GNonempty FF FF F F F rF 760 760 760 760 225 220 207 199 43 20 33 32 006 003 004 004 019 009 016 016 values F R decreases 023 short precursor R decreases GCombinations number significant combinations dependencies The decrease significantly detect 162 dependencies 9 dependencies lower missing dependencies account percentages length limiting 015 0003 009 longest precursor FRFR divided number combinations indicating long combinations tested Yet program higher numbers relatively test longer dependencies length count considerably small proportion possible suggests limiting proportion dependencies precursors length 2 So Alternatively evaluate influence precursors diminishes time failures gathered longer temporal separations failure Fy Table 8 shows results distance traces analyzed dependencies failure F followed number failures temporal separations failure significant looking simpler precursors The data set earliest set execution FFF meaning pairs particular followed particular failures increases dependencies decrease likely longer dependencies It appears number patterns decreases slightly longer decreases intervals missing dependencies 19 FF proportion significant dependencies precursor slightly proportion 16 F wF small considering Because conclusion dependencies finding intervals The future work section describes proposed approaches 522 Effort required sensitivity dependency detection size data set test means The G test statistical sensitive dependency detection execution traces For example data execution available For G tests performed data refers number patterns trace AE Howe RR CohenArtificial Intelligence 76 1995 125166 159 includes FRF patterns The contingency table pattern FpqRspFip structed execution trace shown Table 9 The total number patterns The ratio pattern FpjRspFi execution trace 1l ratio count precursor followed target failure count precursor followed target failure ratio precursor FpjRsp followed failure Fip 12 The difference ratios sufficient detect dependence precursor failure G test contingency table yields G 0236 p 0627 If 20 times patterns execution traces 100 patterns traces ratios 2020 2040 G test significant G 4711 p 003 dependency detected Table 9 Contingency table dependency Rsp The total number patterns ratios patterns contingency table influences results G test Imagine significant contingency table p cy ways change table change result p vary total count table n vary ratios row row The change addresses sensitivity test size execution traces second addresses sensitivity noise difference required detect dependency longer The changes independent understand influence examine separately The sensitivity test size execution traces examined analytically sensitivity noise examined empirically section To determine effect sample size detecting dependencies examine equation underlying G test detect dependencies heterogeneity test The nature G test G values subsets sample added G value superset property exploited pruning If ratios remain total number counts contingency table double G value contingency table doubles For example G value contingency table Table 9 0236 G value contingency table 20 times patterns 4711 roughly close gets round error 20 times 0236 To explain arises consider alternative mathematically equivalent form equation heterogeneity G2 aIn bln 5 oihlnah 160 AE Howe RR CohenArtcial Intelligence 76 1995 125166 count upper left contingency table b count upper right contingency table c count lower left contingency table d count lower right contingency table f cc d j d c d Assuming hold constant ratios ab cd vary total count b c d replace b x making ratio c y d making ratio cy c simplify equation G2al xlnl y xln 1 xlnl x1 Y 2 illustrates hold ratios constant x y vary increase total value G increases linearly increases This explanation valid simple version G heterogeneity detect dependencies place Linearity desirable effect data predictable tends reduce likelihood surprising results trials collected We know execution traces collected likely detect depen dencies new dependencies detected likely borderline addition data The line G test strong dependencies given execution traces patterns given patterns rare dependencies So user FRA interested detecting dependencies execution traces adequate user wishes rare obscure dependencies necessary gather execution traces The level effort expended gathering execution traces depends kinds dependencies wishes The preceding analysis tells G changes expend effort gather execution traces analysis assumes ratios contingency table constant Consequently effect noise given patterns execution traces To rephrase concern problems fewer patterns execution traces particular failure random event Formal analysis concern difficult Instead tested effect noise empirically inserting random events execution traces testing dependen cies original traces remained The counts shown Table 10 About 65 dependencies remain introducing noise This means 35 dependencies detected disappear patterns included execution traces As expect dependencies vul nerable tweaking based execution traces included instances precursorfailure pattern 23 44 52 dependencies disappeared based contingency tables f 5 The implication sensitivity dependency detection noise execution traces rare patterns patterns based instances execution traces especially sensitive noise SO viewed skeptically When evaluating cost executing dependency detection need consider factors First dependency detection simple fully automated calculations test fast run batch mode The computation time required collect dependencies sets execution traces far shorter AE Howe IR CohenArcial Intelligence 76 1995 125166 161 Table 10 Dependencies remaining tweaking contingency tables Contingency tables tested sensitivity minor changes ratios The table includes number dependencies remaining tweaking total number deoendencies execution traces exoeriments Experiment 1 Experiment 2 Experiment 3 Experiment 4 RF FF FRF Total o4 9113 517 14124 418 15119 314 22131 1015 II15 1116 28146 II12 1012 oo 17124 time required gather execution traces place minutes dependency detection weeks data collection Second complexity dependency detection algorithm mitigated practical reduction number patterns considered step The patterns considered appeared execution traces experiments number patterns observed fourfifths possible Of remained step dependency detection reduced set thirds second step half From practical standpoint cost executing dependency detection fairly low 6 Future work We described technique identifying explaining dependencies planner behavior applied explain sources failure Phoenix planner Obviously beginning We envision directions future research First intend expand definition dependencies encompass longer time intervals longer combinations actions events Second examine dependencies characterize environments explore markers identifying similarities apparently different environments Third apply dependency interpretation planners Phoenix behaviors plan failures 61 Detect longer dependencies Dependency detection based assumption recent precursors likely influence failure occurs phrase differently precursors influence persist failure Empirical evidence suggests probably missing dependencies extending temporal extent precursors Unfortunately combinatorial nature dependency detection appears preclude identifying arbitrarily long sequences significant precursors Unless gather incredibly long execution traces quickly run instances pattern precursor failure Additionally considering patterns means consuming computation time 162 AE Howe RR CohedArtiJScial Intelligence 76 1995 125166 However combinatorics dependency detection based searching ai trarily long sequences We manage complexity focusing search long sequences searching execution traces pattern precursor failure search particularly interesting ones Sets longer dependencies accumulated controlling collection data selectively test particular dependencies experiment design heuristically controlling construction comparison dependencies supplementing G test A new experiment design selectively eliminate recovery methods plan actions test precipitates avoids particular failures 6 For analysis remove action recovery method consideration results execution traces free interactions missing action Consequently examining possible chains method member dependency detection involve comparing dependency sets generated execution traces action By comparing dependency sets generated way infer dependencies interactions missing methods For example action R removed frequency Fip relative failures decreases analysis determine Rsp produced additional Fip failures Was Rsp producing failures Does R conjunction recovery methods RRFip dependencies account surplus Fip failures Does R influence Fip longer intervals Briefly new experiment design determine possible explanations account additional Fip failures comparing counts particular failures action changes counts differ uniformly depend previous failure If counts differ uniformly conjecture difference solely action removed check failure counts compared counts actions removed looking cases actions behaved similarly action X action Y interact removing produce similar results Longer combinations require changes application G test determining pools partitions longer precursors comparing results separate G tests The pool partitions represent different hypotheses producing observed dependency example F influence FY F conjunction particular recovery methods R Rb influence FY If consider possible pools partitions faced combinatorial algorithm If consider pools partitions based significant applications G test use results smaller chains determine larger chains explore Our intuition suggests set partitions account little variance pool probably worth looking longer precursors include partitions For example RspFip favored FR Fip based G test longer precursors FRFxRFip unlikely account variance To develop approach need run G test pools pairs FRF partitions triples RFRF ad FFRF probably longer pools partitions test intuition supported data Then need derive heuristics selecting pools partitions based results previous G tests AE Howe RR CohenArtificial Intelligence 76 1995 125166 163 Another approach finding longer dependencies rephrase problem finding possible long dependencies finding highly significant dependencies In case use local search techniques explore large space possible long dependencies By local search strongest related dependen cies need pruning step easily tune time spent searching modifying number random starts define search operators match intuition constitutes neighborhood dependencies Working logistics detecting longer dependencies step broadening set events included execution traces Execution traces FFL4 include failures recovery methods events actions influence types failures occur Instrumentation available Phoenix collect influences changes weather initiating new plans observations new fires currently FRA analyze execution traces Enhancing dependency detection consider longer chains means events added execution traces inundated possible dependencies 62 Construct equivalence classes environments Execution traces experiments yielded different dependencies The pendencies different degree difference appears depend failure recovery planner differed experiment Based analyzing execution traces Phoenix planner similar versions versions differed single change implementation failure recovery result similar sets dependencies Comparisons dependencies detected execution traces pilot studies sug gest dependencies sensitive degree similarity environment Perhaps dependencies function markers particular characteristics environ ments For example severely resourceconstrained environments characterized repetitions resource contention failures producing observed dependency resource contention failure leads Additionally failure recovery meth ods perform shortterm load balancing reduce contention constrained resource simply cause different type resource contention failure producing dependency shortterm load balancing recovery methods particular types resource contention failures One expect dependencies appear type resourceconstrained environment superficially different transportation planner air traffic control forest fighter dispatcher Looking markers requires lingua franca dependency sets differ ent environments compared One option hierarchy failures methods general classes resource contention domain specific instances bulldozer unavailable Phoenix The benefit constructing equivalence classes environments predict easily given planner perform environment equivalence class From design standpoint design planners new environments borrowing heavily previous designs known work similar environments From scientific standpoint able tell differences environments 164 AE Howe PR CohenArtcial Intelligence 76 1995 125166 superficial allowing compare planners knowledge bases designed different environments 63 Apply dependency interpretation systems Applying Dependency Interpretation systems means enhancing systems collect execution traces acquiring supportive knowledge specific systems For planners embedded simulated environments collecting execution traces supported simulator planners simulated environments planner need augmented collect execution traces Phoenixspecific knowledge applied step FRA new need acquire types knowledge l environment event types plan actions planner l suggestive plan structures explanations new planner The easy planners predefined methods recogniz ing salient environment events The second difficult The intuition suggestive structures people program particular systems structures check trying track bugs understand program behaves For example resource contention failures probably look structures involved managing resources pairings reserverelease actions actions consume resources decisions resources preconditions actions We recorded structures suspect Phoenix planner believe structures apply equally planners We intend test conjecture 7 Conclusion Part challenge designing new planners understanding work The lessons previous systems basis future designs Unfortunately planners host environments increasingly complex increasingly difficult understand This paper describes experiments method understanding havior planners The approach combines domainindependent syntactic technique summarizing behavior identifying interesting patterns domaindependent semantic technique interpreting patterns The strength approach lies application statistical techniques reduce tremendous amounts execu tion information salient patterns reliance weak domainplanner knowledge interpretation Statistical dependency detection prunes overwhelming largely uninteresting space behavioral data dependency interpretation ex ploits knowledge interactions small parts planner explain behavior The future planning depends ability explain behavior increasingly complex systems For meant changing focus explaining individual decisions particular planning episodes explaining statistical anomalies AE Howe lR CohenArtificial Intelligence 76 1995 125166 165 episodes Also explanations terms highly specific state information terms general plan structures programming idioms Consequently debug Phoenix planner given execution trace single planning episode pinpoint source bug particular line code Nor We think unlikely write program automatically localize bugs arbitrary complex systems given execution traces buggy Dependency detection interpretation hand explain statistical anomalies indicate bugs planners particularly designed environments programmer easily previously unsuspected bugs designer extrapolate good design new environment Acknowledgments This research supported ARPAAFOSR contracts F4962089C00113 F3060293C0100 grant Office Naval Research University Re search Initiative NOOO1486K076 National Science Foundation Issues RealTime Computing grant CDA8922572 National Science Foundation Research Initiation Award RIA IRI9308573 The US Government authorized reproduce distribute reprints governmental purposes notwithstanding copyright tation hereon This research conducted authors PhD Thesis research University Massachusetts We wish thank Cynthia Loiselle carefully reading translating portions document LaTex David Hart David Westbrook helping run experiments Phoenix We wish thank reviewers careful review insightful comments References 1 PC Bates JC Wileden Highlevel debugging distributed systems behavioral abstraction approach Department Computer Information Science 8329 University Massachusetts Amherst MA 1983 2 SW Bennett Learning approximate plans use real world A Segre ed Proceedings rhe Sixth Zntemational Workshop Machine Learning Ithaca NY Morgan Kaufmann San Mateo CA 1989 224228 3 L Bimbaum G Collins M Freed B Krnlwich Modelbased diagnosis planning failures Proceedings AAAI90 Boston MA 1990 318323 4 LJ Bumell SE Talbot Incorporating probabilistic reasoning reactive program debugging Proceedings Ninth Conference Articial Intelligencefor Applicaiions Orlando FL 1993 321327 5 SA Chien Learning analyzing fortuitous occurences A Segre ed Proceedings rhe Sixrh International Workshop Machine Learning Ithaca NY Morgan Kaufmann San Mateo CA 1989 249251 6 PR Cohen An experiment test additivity effects Technical Report Memo No 20 University Massachusetts Experimental Knowedge Systems Laboratory Amherst MA 1991 171 PR Cohen Empirical Methodsfor Arhjicial Infelligence MIT Press Cambridge MA 1993 8 PR Cohen M Greenberg DM Hart AE Howe Trial understanding design requirements agents complex environments AI Mag 10 3 1989 3248 166 AE Howe PR CohenArtcial Intelligence 76 1995 125166 91 NK Gupta RE Seviora An expert approach real time debugging Proceedings IEEE Computer Society Conference AI Applications Denver CO 1984 336343 1101 KJ Hammond Explaining repairing plans fail Proceedings IJCAI87 Milan Italy 1987 109l 14 Art Intell 45 1990 173228 111 DM Hart PR Cohen SD Anderson Envelopes vehicle improving efficiency plan execution KP Sycara ed Proceedings Workshop Innovative Approaches Planning Scheduling Control Morgan Kaufmann San Mateo CA 1990 7176 121 AE Howe Accepting inevitable role failure recovery design planners PhD Thesis University Massachusetts Department Computer Science Amherst MA 1993 131 AE Howe PR Cohen Failure recovery model experiments Proceedings AAAI91 Anaheim CA 1991 801808 141 E Hudlicka V Lesser Modeling diagnosing problemsolving behavior IEEE Trans Syst Man Cybern 17 3 1987 407419 1151 LF Pau Failure Diagnosis Petormance Monitoring Control Systems Theory 11 Marcel Dekker New York 198 1 161 RG Simmons A theory debugging plans interpretations Proceedings AAAI88 Minneapolis MN 1988 9499 171 RR Sokal FJ Rohlf Biometry The Principles Practice Statistics Biological Research Freeman New York 2nd ed 1981 181 M Stefik Planning constraints MOLGEN Part l ArtiJ Intell 16 1981 111139 191 GA Sussman A computational model skill acquisition Technical Report Memo AITR297 MIT AI Lab Cambridge MA 1973