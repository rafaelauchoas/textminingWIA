Artiﬁcial Intelligence 173 2009 593618 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A heuristic search approach planning temporally extended preferences Jorge A Baier ab Fahiem Bacchus Sheila A McIlraith Department Computer Science University Toronto Canada b Department Computer Science Pontiﬁcia Universidad Católica Chile Chile r t c l e n f o b s t r c t Article history Received 27 October 2007 Received revised form 28 November 2008 Accepted 28 November 2008 Available online 6 December 2008 Keywords Planning preferences Temporally extended preferences PDDL3 Planning preferences involves ﬁnding plan achieves goal requires ﬁnding preferred plan achieves goal preferences plans speciﬁed planners input In paper provide technique accomplishing objective Our technique deal rich class preferences including socalled temporally extended preferences TEPs Unlike simple preferences express desired properties ﬁnal state achieved plan TEPs express desired properties entire sequence states traversed plan allowing user express richer set preferences Our technique involves converting planning problem TEPs equivalent planning problem containing simple preferences This conversion accomplished augmenting inputed planning domain new set predicates actions updating predicates We provide collection new heuristics specialized search algorithm guide planner preferred plans Under fairly general conditions method able ﬁnd preferred planie optimal plan It accomplish having resort admissible heuristics perform poorly practice Nor technique require assumption restricted plan length makespan We implemented approach HPlanP planning compete 5th International Planning Competition achieved distinguished performance Qualitative Preferences track 2008 Elsevier BV All rights reserved 1 Introduction Classical planning requires planner ﬁnd plan achieves speciﬁed goal In practice plan achieves goal equally desirable Preferences allow user provide planner information use discriminate successful plans information allows planner distinguish successful plans based plan quality Planning preferences involves ﬁnding plan achieves goal requires ﬁnding achieves goal optimizing users preferences Unfortunately ﬁnding optimal plan computationally expensive In cases like planner direct search reasonably preferred plan In paper provide technique accomplishing objective Our technique able deal rich class preferences Most notably class includes temporally extended preferences TEPs The difference TEP socalled simple preference simple preference expresses desired property ﬁnal state achieved Corresponding author Department Computer Science University Toronto Canada Email addresses jabaiercstorontoedu JA Baier fbacchuscstorontoedu F Bacchus sheilacstorontoedu SA McIlraith 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200811011 594 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 plan TEP expresses desired property sequence states traversed plan For example preference shift worker work 2 overtime shifts week temporally extended preference It expresses condition sequence daily schedules constructed plan Planning TEPs subject recent research 61235 It theme 5th International Planning Competition IPC5 The technique provide paper able plan class preferences includes speciﬁed planning domain deﬁnition language PDDL3 23 PDDL3 speciﬁcally designed IPC5 It extends PDDL22 include things facilities expressing temporally extended simple preferences temporally extended preferences described subset linear temporal logic LTL It supports quantifying value achieving different preferences speciﬁcation metric function The metric function assigns plan value dependent speciﬁc preferences plan satisﬁes The aim solving PDDL3 planning instance generate plan satisﬁes hard goals constraints achieving best possible metric value optimizing value possible returning high value plan optimization infeasible Our technique approach The ﬁrst exploits existing work 2 convert planning problems TEPs equivalent problems containing simple preferences deﬁned extended planning domain The second main contribution work develop set new heuristics search algorithm exploit heuristics guide planner preferred plans Many heuristics extracted relaxed planning graph technique previously compute heuristics classical planning Previous heuristics classical planning suited planning preferences The heuristics present speciﬁcally designed address tradeoffs arise planning achieve preferences Our search algorithm different previous algorithms planning As number attractive properties including ability ﬁnd optimal plans having resort admissible heuristics This important admissible heuristics generally lead unacceptable search performance Our method able ﬁnd optimal plans requiring restriction plan length makespan This important restrictions generally allow planner ﬁnd globally optimal plan In addition search algorithm incremental ﬁnds sequence plans improving previous This important practice necessary trade computation time plan quality The ﬁrst plans sequence plans generated fairly quickly provide user working plan act immediately If time available algorithm continue search better plan The incremental search process employs pruning technique incremental search eﬃcient The heuristics search algorithm presented easily employed planning systems An additional contribution paper brought ideas working planning called HPlanP Our planner built extension TLPlan 1 The basic TLPlan uses LTL formulae express domain control knowledge LTL formulae serve prune search space However TLPlan mechanism providing heuristic guidance search In contrast implementation extends TLPlan heuristic search mechanism guides planner plans satisfy TEPs pruning partial plans violate hard constraints We exploit TLPlans ability evaluate quantiﬁed formulae avoid having convert preference statements quantiﬁed collection ground instances This important grounding preferences yield intractably large domain descriptions We use implementation evaluate performance algorithm analyze relative performance different heuristics problems IPC5 Simple Qualitative Preferences tracks In rest paper ﬁrst provide necessary background This includes brief description features PDDL3 approach handle In Section 3 ﬁrst approacha method compiling domain temporally extended preferences solely terms simple ﬁnal state preferences Section 4 describes heuristics search algorithm developed It presents number formal properties algorithm including characterizing conditions algorithm guaranteed return optimal plans Section 5 presents extensive empirical evaluation technique including analysis effectiveness combinations heuristics presented Section 4 Section 6 presents discussion approach Section 7 summarizes contributions discusses related work provide ﬁnal conclusions 2 Background This section reviews background needed understand paper Section 21 presents basic planning deﬁ nitions brief description planning domain deﬁnition language PDDL Section 22 describes variation wellknown approach computing domainindependent heuristics based computation relaxed plans planner compute heuristics As opposed wellknown approaches method able handle ADL mains directly having precompile domain STRIPS domain Section 23 describes planning domain deﬁnition language PDDL3 recent version PDDL enables deﬁnition hard constraints preferences metric functions JA Baier et al Artiﬁcial Intelligence 173 2009 593618 595 21 An overview planning formalisms languages A classical planning instance tuple I Objs Oper Init Goal Objs ﬁnite set objects Oper ﬁnite set planning operators Init initial state ﬁnite set ground literalsor simply factsdescribing initial state Goal describes set goal states In STRIPS planning instances 19 set Oper contains operator descriptions form preo addo delo preo list precondition facts operator o addothe add listis list facts positive effects operator o delothe delete listis list facts negative effects operator o Finally Goal set goal facts In expressive ADL formalism 31 operators preconditions effects simple lists ground literals ADL preconditions arbitrary boolean formulae existentially universally quantiﬁed set objects Objs ADL effects conditional means adds deletes conditioned satisfaction arbitrary boolean formulae Effects universal sense affect objects satisfy certain condition For example assume describing domain objects contain objects Further assume action movex y z moves object x location y location z process moves objects x z The precondition action atx y object x location y effects deﬁned list cid2 Eff add atx z v cid3 cid4 inv x add atv z del atx y v inv x del atv y cid3 cid4cid5 Thus location object x objects inside x changes z In addition expressive preconditions effects ADL allows representation functions This means states contain addition propositional facts sentences form f cid5c z f function c tuple objects Objs z object Objs Actions change functions assigning f cid5c different value add effect Finally ADL Goal formula possibly quantiﬁed describes condition satisﬁed goal state For details ADL refer reader 31 Although STRIPS ADL provide formal descriptions classical planning instances standard input language planners precise syntactical form standardized The Planning Domain Deﬁnition Language PDDL 30 hand speciﬁcally designed provide uniform syntax describing planning problems context 1998 International Planning Competition PDDL currently facto standard describing planning problems extended subsequent versions IPC Recent versions PDDL enable deﬁnition planning instances superset ADL For example PDDL21 20 extends ADL enabling explicit representation time Among features allows speciﬁcation actions duration On hand PDDL22 16 extends PDDL21 allowing derived predicates predicates deﬁned axiomati cally timed literals literals true speciﬁed time instant PDDL3 Section 23 extends PDDL22 hard constraints preferences metric functions The planning problem STRIPS ADL settings problem ﬁnding legal sequence actions ground operatorsthat executed initial state lead state goal condition Goal satisﬁed 22 Planning heuristic search Many stateoftheart domainindependent planners use domainindependent heuristics guide search plan Heuristics estimate cost achieving goal certain state They standard search algorithms usually key good performance They typically computed solving relaxed version original problem One popular domainindependent relaxations corresponds ignoring negative effects actions This approach taken planners hsp 8 ff 27 In STRIPS formalism corresponds ignoring delete lists In paper exploit heuristic search plan preferences The heuristics presented based known technique computing relaxed planning graph 27 graph generated Graphplan 7 STRIPS relaxed planning instance ignores negative effects This graph composed fact layersor relaxed worldsand action layers The action layer level n contains actions possible relaxed world depth n The relaxed world depth n 1 contains facts hold layer n 1 generated applying positive effects actions action layer n The graph expanded goal satisﬁed ﬁnal relaxed world ﬁxed point reached Once graph expanded compute relaxed plan goals regression goal facts graph initial state The length plan heuristic estimator cost achieving goal In rest paper assume familiarity extraction relaxed plans For details refer reader article Hoffmann Nebel 27 221 Relaxed plans functionfree ADL domains To compute heuristics functionfree ADL domains ﬁrst transform domain STRIPS wellknown procedure described Gazen Knoblock 21 compute heuristic usual This approach taken 596 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 systems ff unfortunately procedure lead considerable blow size original instance Our planner handles ADL domains takes different approach In particular computes relaxed planning graph directly ADL instance approach similar taken Marvin planning 11 To effectively handle relaxed ADL domains effects conditioned negative facts relaxed worlds represent facts true facts false executing set actions To end relaxed worlds divided parts positive represents added facts negative represents deleted facts n 0 0 k n F n F 0 F When computing relaxed planning graph state s set relaxed worlds sequence pairs fact sets sc sc set facts s complement s Further F 0 F action appears action layer depth n facts added included positive relaxed world depth F k1 Moreover facts layer k copied layer F k 1 F k1 facts deleted added F k1 F s F k1 F Special care taken evaluation preconditions conditions conditional effects actions negations appear conditions To evaluate formula relaxed world evaluate negation normal form NNF instead In NNF negations appear right atomic formulae A formula easily converted NNF pushing negations standard rules f f f f f 1 f 2 f 1 f 2 f 1 f 2 f 1 f 2 f f Now assume want determine formula φ true relaxed state F k graph NNF φ To evaluate φ instead evaluate φcid11 relaxed worlds F recursively standard way interpreting quantiﬁers boolean binary operators usual When evaluating positive fact f return truth value f F k On hand evaluating negative fact f return truth value f F k In short f true depth k f deleted action false initial state More formally n Furthermore let φcid11 k F 0 F k F k F 0 F n F Deﬁnition 1 Truth NNF formula relaxed state Let relaxed planning graph constructed initial state s 0 F k F problem set objects problem Objs F k The following cases deﬁne φ 0 F k cid13 φ k F true level k relaxed planning graph denoted F If φ atomic formula F k F If φ f f atomic formula F k cid13 ψ F k cid13 φ iff F If φ ψ ξ F k F k F k cid13 φ iff F If φ ψ ξ F k cid13 ψ F k F k F k F k cid13 φ iff o Objs F If φ xψ F k F k F k cid13 φ iff φ F k F k k cid13 φ iff φ F k cid13 ξ k F k cid13 ξ k cid13 ψxo ψxo formula ψ free k instances x replaced o1 If φ xψ o Objs F k F k cid13 ψxo The standard relaxed plan extraction modiﬁed slightly ADL case Now actions conditional effects fact f true action particular set facts responsible addition precondition condition conditional effect true When recursing subgoal f add new subgoals facts responsible addition f relaxed world As case STRIPS relaxed planning graphs fact f reachable state performing certain sequence legal actions f eventually appears fact layer graph The happens relaxed planning graphs This proven following proposition Proposition 2 Let s planning state R F 1 F m relaxed planning graph constructed s ﬁxed point φ NNF formula If φ true performing legal sequence actions a1 s exists k cid2 m F 0 F 0 F 1 F m F k F k cid13 φ Proof See Appendix A cid2 This proposition veriﬁes relaxed planning graph fact relaxation problem In particular says goal reachable relaxed planning graph achievable real plan Besides desirable property reachability result key interesting properties search algorithm In particular later essential proving bounding functions employ prune optimal solution certain reasonable assumptions 1 In implementation bounded quantiﬁcation condition checked eﬃciently In particular means object Objs need checked JA Baier et al Artiﬁcial Intelligence 173 2009 593618 597 1 2 3 4 5 6 s0s1 sn cid13 φ s0s1 sn cid13 φ s0s1 sn cid13 end φ s0s1 sn cid13 sometimeafter φψ s0s1 sn cid13 sometimebefore φψ s0s1 sn cid13 atmostonce φ sn cid13 φ iff 0 cid2 cid2 n si cid13 φ iff 0 cid2 cid2 n si cid13 φ iff iff si cid13 φ j cid2 j cid2 n s j cid13 ψ iff si cid13 φ j 0 cid2 j s j cid13 ψ iff 0 cid2 n S cid13 φ j j cid3 k k j sk cid13 φ Fig 1 Semantics PDDL3s temporally extended formulae mention explicit time The trajectory s0s1 sn represents sequence states results execution sequence actions a1 23 Brief description PDDL3 PDDL3 introduced Gerevini Long 23 5th International Planning Competition It extends PDDL22 enabling speciﬁcation preferences hard constraints It provides way deﬁning metric function deﬁnes quality plan dependent satisfaction preferences The current version planner handles nontemporal nonnumeric subset PDDL3 language Qualitative Preferences track IPC5 In subset temporal features language durative actions timed ﬂuents supported Moreover preference formulae mention explicit times operators alwayswithin supported Numeric functions PDDL ﬂuents supported The rest section brieﬂy describes new elements introduced PDDL3 support 231 Temporally extended preferences constraints PDDL3 speciﬁes TEPs temporally extended hard constraints subset quantiﬁed linear temporal logic LTL 32 These LTL formulae interpreted trajectories nontemporal subset PDDL3 sequences states result execution legal sequence actions Fig 1 shows semantics LTLbased operators temporally extended formulae The ﬁrst operators standard LTL remaining ones abbreviations deﬁned terms standard LTL operators 232 Temporally extended preferences constraints Preferences constraints viewed preferences satisﬁed declared constraints construct Each preference given declaration allow later reference By way illustration following PDDL3 code deﬁnes preferences hard constraint constraints preference cautious forall o heavyobject sometimeafter holding o rechargingstation1 forall l light preference plight turnoff l forall x explosive holding x The cautious preference suggests agent recharging station held heavy ob ject plight suggests agent eventually turn lights Finally unnamed hard constraint establishes explosive object held agent point valid plan When preference externally universally quantiﬁed deﬁnes family preferences containing individual pref erence binding variables quantiﬁer Therefore preference plight deﬁnes individual preference object type light domain Preferences quantiﬁed externally like cautious seen deﬁning family containing single preference Temporal operators nested PDDL3 Our approach handle general case nested temporal operators 233 Precondition preferences Precondition preferences atemporal formulae expressing conditions ideally hold state action performed They deﬁned actions precondition For example preference labeled econ speciﬁes preference picking objects heavy action pickup parameters b block precondition clear b effect holding b preference econ heavy b 598 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 Precondition preferences behave like conditional action costs They violated time action exe cuted state condition hold In example econ violated time heavy block picked plan Therefore preferences violated number times 234 Simple preferences Simple preferences atemporal formulae express preference certain conditions hold ﬁnal state plan They declared goal For example following PDDL3 code goal delivered pck1 depot1 preference truck truck depot1 speciﬁes hard goal pck1 delivered depot1 simple preference truck depot1 Simple preferences externally quantiﬁed case represent family individual preferences 235 Metric function The metric function deﬁnes quality plan generally depending preferences achieved plan To end PDDL3 expression isviolated returns number individual preferences family preferences violated plan When refers precondition preference expression returns number times precondition preference violated execution plan The quality metric depend function totaltime nontemporal subset PDDL3 returns plan length actual duration plan expressive settings Finally possible deﬁne want maximize minimize metric want weigh different components For example PDDL3 metric function metric minimize totaltime 40 isviolated econ 20 isviolated truck speciﬁes twice important satisfy preference econ satisfy preference truck impor tant useful ﬁnd short plan In article focus metric functions mention totaltime isviolated functions allow function symbols planning domain 3 Preprocessing PDDL3 As described previous section PDDL3 supports deﬁnition temporally extended preferences subset LTL A brute force method generating preferred plan generate plans realize goal rank respect PDDL3 metric function However evaluating plans generated eﬃcient plans achieve goal Instead need able provide heuristic guidance planner direct generation highquality plans This involves estimating merit partial plans estimating TEPs potentially satisﬁed extensions estimating metric value potentially achieved extension With heuristic information planner direct search effort growing promising partial plans To actively guide search plans satisfy problems TEPs develop twopart approach The ﬁrst component approach exploit techniques presented Baier McIlraith 2 convert planning domain containing TEPs containing equivalent set simple ﬁnalstate preferences Simple preferences similar standard goals express soft goals conversion enables second approach extend existing heuristic approaches classical goals obtain heuristics suitable guiding planner achievement new set simple preferences The development evaluation new heuristics simple preferences main contributions work described section That section presents new search strategy effective exploiting heuristics In section ﬁrst approach techniques Baier McIlraith 2 exploited compile planning domain containing TEPs domain containing simple preferences Besides conversion TEPs deal features PDDL3 support described previous section 31 Temporally extended preferences constraints Baier McIlraith 2 presented technique construct automaton Aϕ temporally extended formula ϕ The automaton Aϕ property accepts sequence states sequence states generated plan sequence states satisﬁes original formula ϕ The technique works rich subset ﬁrstorder JA Baier et al Artiﬁcial Intelligence 173 2009 593618 599 linear temporal logic formulas includes PDDL3s TEPs It includes TEPs temporal operators nested allowed PDDL3 To encode PDDL3 preference formulae preference formula represented automaton Reaching accepting condition automaton corresponds satisfying associated preference formula The automaton Aϕ embedded planning domain extending domain new predicates representing state automaton Thus initial state planning problem predicates capture fact automaton starting initial state inputed initial state problem The technique modiﬁes domains actions properly update automatastate predicates When sequence actions applied starting initial state automatastate predicates updated capture progress actions satisfying preference formula automaton corresponds Hence determine sequence actions satisﬁed ϕ simply testing automatastate predicates ﬁnal state arising actions indicate automaton accepting state In words technique allows convert temporally extended condition ϕ condition ﬁnal state automaton state predicates indicate Aϕ accepting state One important feature compilation technique exploit construct parameterized automata That need expand quantiﬁed ﬁrstorder temporal extended formula ϕ larger propositional formula computing ground instantiations This means technique generates compact domains avoiding grounding quantiﬁed preferences Generating compact compiled problem key good performance Section 5 Although general size automaton results compiling arbitrary LTL formula ϕ exponential ϕ case restricted subset LTL allowed PDDL3 formulae allow nestings temporal operators exponential blowup occur Baier McIlraiths original paper aimed planning temporally extended goals preferences Up construction automata temporally extended formula approach identical taken How Baier McIlraith 2 propose derived predicates embed automata planning domain In work chosen different approach compatible underlying TLPlan employed implementation In rest section details construction automata way embed automata planning domain Further details automata construction 2 311 Parameterized ﬁnite state automata The compilation process ﬁrst constructs parameterized nondeterministic ﬁnitestate automaton PNFA Aϕ temporally extended preference hard constraint expressed LTL formula ϕ The PDDL3 operators presented Fig 1 abbreviations ﬁrst expanded standard LTL operators following Gerevini Long 23 The PNFA represents family nondeterministic ﬁnitestate automata Its transitions labeled ﬁrstorder formulae input language set strings plan states A PNFA Aϕ accepts sequence plan states iff sequence satisﬁes ϕ Fig 2 shows examples PNFA ﬁrstorder LTL formulae Parameters automaton appear LTL formula externally quantiﬁed Fig 2b The intuition different objects tuples objects different states automaton Tuples objects transition labeled automaton reads plan state s iff transition q q state q state q formula satisﬁed s cid11 cid11 As example consider transportation domain packages A B initially loaded vehicle Focusing formula Fig 2b objects start initial state q0 Then automaton inputs initial state planning problem That state satisﬁes formula implies loaded x delivered x packages A B loaded initial state Hence packages transition state q2 stay state q0 automata nondeterministic This means initially objects satisfy temporal formula automatons accepting state q2 That null plan satisﬁes formula b Fig 2 Now assume perform action load A Truck In resulting state B stays q0 moves q0 q2 A moves q0 q1 Hence A longer satisﬁes formula satisfy plan reaches state deli vered A true A PNFA useful computing heuristics effectively represents different paths goal achieve certain property states intuitively monitor progress satisfying original temporal formula Therefore expanding relaxed planning graph computing heuristics implicitly considering possible relaxed ways satisfying property 312 Representing PNFA planning problem After PNFA constructed embedded planning domain This accomplished ex tending original planning problem additional predicates represent state automaton plan state If planning domain multiple TEPs usually case PNFA constructed TEP formula embedded planning domain automatonspeciﬁc automatastate predicates That ﬁnal planning problem contain distinct sets automatastate predicates embedded automaton 600 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 Fig 2 PNFA exists c cafe c c b forall x sometimeafter loaded x delivered x In PNFA q0 initial state accepting states indicated double circle border b To represent automaton domain deﬁne predicate specifying automatons current set states When automaton parameterized predicate arguments representing current set automaton states particular tuple objects In example fact autstate q0 A represents object A automaton state q0 Moreover automaton deﬁne accepting predicate The accepting predicate true tuple objects plan satisﬁed temporal formula tuple Rather modify domains actions automata state properly updated actions executed Baier McIlraith 2 instead modiﬁed underlying TLPlan action automatically apply speciﬁed set automata updates Automata updates work like pseudoactions performed automatically new successor generated When generating successor s performing action planner adding deleting effects When ﬁnished processes automata updates builds new state s regarded actual successor s performing The s compilation process avoid changes domains actions instead insert conditions needed transition automata state selfcontained addition domain speciﬁcation generating new successor s The state s cid11cid11 cid11cid11 cid11 cid11 Syntactically automata updates encoded domain ﬁrstorder formulae contain add del keywords like regular TLPlan action effect speciﬁcations For automata Fig 2b update include rules forall x implies autstate q0 x loaded x add autstate q1 x That object x moves state q0 q1 loaded x true Analogously deﬁne update accepting predicate performed immediately automata updateif automaton reaches accepting state add accepting predicate world state In addition specifying automata states updated need specify objects au tomata states initial state problem This means augment problems initial state adding collection automata facts Given original initial state automaton planner computes states relevant tuple objects automaton inputed problems initial state adds corre sponding facts new problem In example initial state new compiled problem contains facts stating A B states q0 q2 If temporally extended formula originally described hard constraint accepting condition automaton treated additional mandatory goal During search use TLPlans ability incrementally check temporal constraints prune search space plans violated constraint 32 Precondition preferences Precondition preferences different TEPs atemporal associated execution actions If precondition preference p violated n times plan PDDL3 function isviolated p returns n Therefore compiled problem contains new domain function isviolatedcounterp precondition preference family p This function keeps track times preference violated It initialized zero conditionally incremented associated action performed state violates atemporal pref erence formula In case preference quantiﬁed function parameterized allows compute number times different objects violated preference For example consider PDDL3 pickup action given In compiled domain original declaration placed JA Baier et al Artiﬁcial Intelligence 173 2009 593618 601 action pickup parameters b block precondition clear b effect heavy b increase isviolatedcounterecon1 holding b add holding b 33 Simple preferences As TEPs add new accepting predicates compiled domain simple preference We deﬁne updates analogous automata updates accepting predicates Accepting predicates true iff prefer ence satisﬁed Moreover preference quantiﬁed accepting predicates parameterized true tuples objects time false tuples 34 Metric function For preference family deﬁne new domain function isviolatedname The return values functions deﬁned terms accepting predicates temporally extended simple preferences terms violation counters precondition preferences If preference p quantiﬁed isviolatedp function counts number object tuples fail satisfy preference By way illustration TLPlan code generated preference plight deﬁned Section 232 defdefinedfunction isviolatedplight localvars x x 0 forall l light l x local variable x initialized 0 implies preference_plight_satisfied l x x 1 increase x 1 preference satisfied isviolatedplight x return total sum preference_plight_satisfied accepting predicate deﬁned preference plight Note translation avoids grounding quantiﬁcation refer objects type light If original metric function contains PDDL3 function totaltime replace occurrence TLPlan function planlength counts number actions plan Thus actions implicitly associated unitary duration The metric function resulting instance deﬁned PDDL3 deﬁnition making reference new functions If objective maximize function invert sign function body Therefore henceforth assume metric minimized In remainder paper use notation isviolatedp N refer value isviolatedp search node N We refer metric function M use MN denote value metric search node N 4 Planning preferences heuristic search Starting work unpop 29 hsp 8 ff 27 forwardchaining search guided heuristics proved powerful useful paradigm solving planning problems As shown automata encoding temporally extended preferences allows automatically augment domain additional predicates serve track partial plans progress achieving TEPs The central advantage approach converts planning domain simple preferences In particular achievement TEP marked achievement accepting predicate TEP syntactically identical standard goal predicate This means converted domain standard techniques computing heuristic distances goal predicates utilized obtain heuristic distances TEP accepting predicates For example standard technique based relaxed planning graph 27 approximates distance goal TEP accepting predicate heuristically guide forwardchaining search Nevertheless standard methods fairly easily modiﬁed manner aim develop search strategy suitable problem planning TEPs In particular approach aims provide search algorithm main features First planner ﬁnd good plans optimize supplied metric function Second able generate optimal plans able generate improvement existing plan Finally contexts hard achieve optimal planand great deal search effort requiredwe want algorithm ﬁnd plan quickly possible Heuristic search nonadmissible heuristics like relaxed goal distances employed planners like ff effective quickly ﬁnding plan However offer assurances quality plan ﬁnd On hand admissible heuristic plan guaranteed optimal assuming heuristic 602 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 admissible respect supplied plan metric Unfortunately admissible heuristics typically perform poorly practice 8 Hence admissible heuristic plan fails ﬁnd plan This typically unacceptable practice In section develop heuristic search technique exploits special structure translated planning domains order ﬁnd plan fairly rapidly nonadmissible heuristic b generate sequence improved plans fairly general conditions terminates optimal plan bounding technique In partic ular search technique allows generate better plansor optimal plansif suﬃcient computational resources available It allows improve existing plan prove plan optimal In rest section begin describing set different heuristic functions serve guide search satisfying goals preferences Then search algorithm analyze properties 41 Heuristics functions planning preferences Our algorithm performs forward search space states guided heuristics Most heuristic functions given computed search node N constructing relaxed planning graph described Section 221 The graph expanded planning state corresponding N grown goal facts preference facts instances accepting predicates appear relaxed state ﬁxed point reached The goal facts correspond hard goals preference facts correspond instantiations accepting predicates converted TEPs Since compiled domain need update automata predicates procedure Section 221 modiﬁed apply automata updates action layers regular actions performed On hand new compiled domain functions addition modify procedure Section 221 ignore effects directly affect value function This means relaxed worlds preference counters value initial state s Note preference counters appear conditions conditional effects preconditions actions Proposition 2 continues hold relational facts particular holds accepting predicates Below suite heuristics computed relaxed planning graph plan ning preferences They designed guide search 1 satisfying goal 2 satisfying highly valued preferences preferences given higher weight metric function However highly valued preferences hard achieve guiding planner achievement preferences yield acceptable performance To avoid problem approach tries account diﬃculty satisfying preferences value ultimately attempting achieve tradeoff factors 411 Goal distance function G This function returns estimate number actions needed achieve goal planning problems contain hard achieve goal collection preferences G heuristic ff planner modiﬁed ADL case The value returned G number actions contained relaxed plan achieves goal 412 Preference distance function P This function measure hard reach preference facts It based heuristic proposed Zhu Givan 37 conjunctive hard goals adapted case preferences Let P set preference facts appear relaxed planning graph let d f depth f ﬁrst appears construction graph Then P N f P d f k parameter k Notice unreachable preference facts appearing graph affect P s value cid6 413 Optimistic metric function O The O function estimate metric value achievable search node N search space O require constructing relaxed planning graph Rather compute assuming 1 precondition preferences violated future 2 TEPs violated proved unachievable N regarded false 3 remaining preferences regarded satisﬁed 4 value totaltime evaluated length plan corresponding N To prove TEP p unachievable N O uses suﬃcient condition It checks automaton p currently state path accepting state Examples LTL formulae detected technique falsiﬁed future form ϕ Indeed soon ϕ false state automatons current set states possible reach accepting state Although O clearly underestimates set preferences violated plan extending N neces sarily lower bound metric value plan extending N It lower bound metric function nondecreasing number violated preferences As later lower bounds metric function soundly prune search space speed search JA Baier et al Artiﬁcial Intelligence 173 2009 593618 603 Deﬁnition 3 NDVPL metric functions Let I preprocessed PDDL3 planning instance let set Γ contain prefer ences let lengthN length sequence action generated N A metric function M nondecreasing number violated preferences plan length NDVPL iff nodes N N holds cid11 1 If lengthN cid3 lengthN 2 If totaltime appears M lengthN lengthN cid11 p Γ isviolatedp N cid3 isviolatedp N cid11 cid11 p Γ isviolatedp N cid3 cid11 MN cid3 MN isviolatedp N cid11 MN MN cid11 NDVPL metrics natural objective problem minimize metric function preprocessed instances Problems NDVPL metrics violating preferences improves metric plan Furthermore adding actions plan fail satisfy new preferences improve metric Below Remark 16 additive metrics metrics IPC5 satisfy condition Proposition 4 If metric function NDVPL O N guaranteed lower bound metric value plan extend ing N Proof The optimistic metric regards violated preferences provably violated successor N state reachable N sequence actions It regards satisﬁed remaining preferences That cid11 O evaluating metric hypothetical node N O node N reachable N cid11 Furthermore O evaluates plan length N p Γ isviolatedp N O cid2 isviolatedp N cid11 Since hypothetical node lengthN O lengthN lengthN O cid2 lengthN cid11 It follows metric function NDVPL follows Deﬁnition 3 successor N O N returns lower bound metric value plan extending N cid2 N MN O cid2 MN cid11 The O function variant optimistic weight heuristic PPlan planner 6 PPlan progresses LTL preferences deﬁned Bacchus Kabanza 1 node search space The optimistic weight assumes falsiﬁed LTL preferences progressed false 414 Best relaxed metric function B The B function estimate metric value achievable extending node N It utilizes relaxed planning graph grown state corresponding N obtain estimate In particular evaluate metric function relaxed worlds planning graph B minimum values The metric function evaluated relaxed world w Mw evaluates isviolated functions directly w evaluates totaltime length sequence actions corresponds N For case NDVPL metric functions B similar O return tighter estimates Indeed note layer relaxed planning graph contains superset preference facts true successor current state Also counters precondition preferences updated expanding graph value isviolated functions precondition preferences constant relaxed states This represents implicit assumption precondition preferences violated The metric value relaxed worlds increase actually decreases number preference facts increases deeper relaxed worlds As result metric deepest relaxed world returned B This value corresponds evaluating metric function relaxed state 1 isviolated functions precondition preferences identical ones N 2 preference facts appear relaxed planning graph regarded violated 3 remaining preferences regarded satisﬁed This condition 2 stronger condition 2 deﬁnition O Indeed preference detected unsatisﬁable method described O appear relaxed planning graph path accepting state preference Hence action add accepting predicate preference By relaxed planning graph B detect preferences satisﬁable successor N spotted O s method For example consider preference ϕ f consider fact f reachable current state The myopic O function regard preference satisﬁable possible reach ﬁnal state automaton formula ϕ automaton f looks like Fig 2a On hand f appear graphbecause f unreachable current stateand B regard ϕ unsatisﬁable These observations lead conclusion BN lower bound metric value successor N NDVPL condition Proposition 5 If metric function NDVPL BN guaranteed lower bound metric value plan extend ing N Proof Proposition 2 implies preference facts achieved successors N eventually appear deepest relaxed world Because metric NDVPL implies metric value deepest relaxed 604 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 world minimum value returned B function Now apply argument proof Proposition 4 returned metric value corresponds evaluating metric hypothetical node isviolated counters lower equal plan extending N cid2 415 Discounted metric function Dr The D function weighting metric function evaluated relaxed worlds Assume w 0 w 1 wn relaxed worlds relaxed planning graph w depth w 0 s sc positive negative facts state Dr evaluated Then discounted metric Dr Dr Mw 0 cid9 Mw i1 Mw ri n1cid7 cid8 i0 1 Mw metric function evaluated relaxed world w r discount factor 0 cid2 r cid2 1 The D function optimistic respect preferences appear earlier relaxed planning graph prefer ences easy pessimistic respect preferences appear later preferences hard Intuitively D function estimates metric value plans extending current state believing satisfaction preferences appear easier Observe Mw i1 Mw metric value gained passing relaxed world w w i1 This multiplied ri decreases increases Observe metric gains discounted preferences weighted higher PDDL3 metric higher impact value D That D achieves desired tradeoff ease achieving preference value achieving A computational advantage D function easy compute As opposed approaches heuristic needs explicit selection preferences pursued planner Finally observe r close 1 effect discounting low close 0 metric quickly discounted When r close 0 D function myopic sense discounts heavily preferences appear deeper graph 42 The planning algorithm Our planning algorithm searches plan series episodes The purpose episodes ﬁnd plan goal better value best far In planning episode bestﬁrst search plan initiated heuristics proposed The episode ends soon ﬁnds plan quality better plan previous episode The search terminates search frontier The algorithm shown Algorithm 1 When search started plan algorithm uses goal distance function G heuristic standard bestﬁrst search The heuristics ignored ﬁrst planning episode This motivated fact goal hard condition satisﬁed In problems heuristics guide planner achieving preferred plan conﬂict achieving goal cause search diﬃcult 1 function SearchHPlanPinitial state init goal formula goal set hard constraints hConstraints metric function MetricFn heuristic function UserHeuristic cid2 initialize search frontier cid2 pruning bounding cid2 search restarted frontier InitFrontierinit closed bestMetric worst case upper bound HeuristicFn G frontier current Best element frontier according HeuristicFn Closedcurrent closed current satisﬁes hConstraints MetricBoundFncurrent bestMetric current satisﬁes goal metric bestMetric Output plan current ﬁrst plan HeuristicFn UserHeuristicFn frontier InitFrontierinit Reinitialize closed List end bestMetric MetricFncurrent end succ successors current frontier merge succ frontier closed closed current 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 end 23 24 end 25 end function end Algorithm 1 HPlanPs search algorithm JA Baier et al Artiﬁcial Intelligence 173 2009 593618 605 After ﬁnding ﬁrst plan algorithm restarts search scratch time uses combination heuristics guide planner preferred plan Let UserHeuristic denote combination UserHeuris tic combination heuristic functions Nevertheless paper consider small subset possible combinations In particular consider prioritized sequences heuristics lower priority heuristics break ties higher priority heuristics Since achieving goal remains mandatory UserHeuristic uses G ﬁrst priority heuristics lower priority For example consider prioritization sequence G D03O When comparing states frontier planner ﬁrst looks G function The best state lower G value lower distance goal However tie uses D03 best state smaller value Finally tie uses O function break In Section 5 investigate effectiveness prioritized heuristics sequences 421 Pruning search space Once completed ﬁrst planning episode G want ensure subsequent planning episode yields better plan Whenever plan returned metric lower plan line 10 Moreover episode use metric value previously plan prune search space improve search performance In planning episode algorithm prunes search space node N estimate reach better plan best plan far This estimate provided function Met ricBoundFN given argument search algorithm MetricBoundFNN compute estimate lowerbound metric plan extending N Pruning realized algorithm line 1 condition false As value bestMetric gets updated line 17 pruning constraint imposes tighter bound causing partial plans rejected The O B heuristic functions deﬁned wellsuited MetricBoundFN Indeed tried experiments On hand simple turnoff pruning simply passing null function MetricBoundFN 422 Discarding nodes closed list Under certain conditions algorithm prune nodes revisit plan state appeared previously expanded node This eﬃciency allows algorithm avoid considering plans cycles The algorithm keeps list nodes expanded variable closed standard best ﬁrst search Furthermore current extracted search frontier state checked set closed nodes line 8 If exists node closed list state better equal heuristic value Closedcurrent closed true node current pruned search space Note states identical compiled planning instance boolean predicate incide values assigned ground function coincide In particular means isviolatedcounters identical states identical preferences equally satisﬁed Nevertheless search nodes identical states assigned different heuristic values Given way deﬁned User Heuristic different heuristic values assigned nodes identical states metric function depends totaltime If totaltime function appears positively metric metric wise equally preferred plans longer ones preferred shorter ones discarding nodes prune node leads optimal plan We discuss section Finally note cycles eliminating occur compiled instance occurring original instance Indeed original instance LTL preferences satisﬁed visiting state twice For example consider preference eventually turn light switch turn Any plan contains action turnon immediately followed turnoff satisﬁes preference visits state twice In compiled domains plan produce cycle pruned This set current states preferences automatonrepresented automata domain predicateschanges performing actions changes nonaccepting state accepting state 43 Properties algorithm In section certain conditions search algorithm guaranteed return optimal koptimal plans We prove result imposing restriction UserHeuristic function In particular ensure optimality function inadmissible In planning important inadmissible heuristics typically required adequate search performance The ﬁrst requirement proofs pruning performed algorithm sound Deﬁnition 6 Sound Pruning The pruning performed Algorithm 1 sound iff node N pruned line 1 metric value plan extending N exceeds current bound bestMetric 606 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 When Algorithm 1 uses sound pruning state incorrectly pruned search space That node N pruned search space plan extending achieve metricvalue superior current bound To guarantee algorithm performs sound pruning suﬃces provide lowerbound function input algorithm Theorem 7 If MetricBoundFNN lower bound metric value plan extending N Algorithm 1 performs sound pruning Proof If node N closed pruned search space MetricBoundFNN cid3 bestMetric If Met ricBoundFN lower bound metric value plan extending N b MetricBoundFNN cid2 MN p solution node N p extending N By putting b obtain N closed pruned MN p cid3 bestMetric solution node N p extending N pruning sound cid2 As proven previously Section 41 metric function NDVPL O B lower bound functions provide sound pruning Notice turning pruning having MetricBoundFN return value bestMetric provides sound pruning The second requirement optimality discarding closed nodes performed line 8 To preserve optimality algorithm remove node lead plan preferred plan achieved extending nodes discarded Formally Deﬁnition 8 Discarding Closed Nodes Preserves Optimality The discarding nodes Algorithm 1 preserves optimality iff node N discarded line 8 optimal node plan N O closed list exists node N frontier extended plan optimal quality The condition deﬁned holds NDVPL metrics fairly general conditions In particular holds NDVPL metric independent totaltime It holds NDVPL metric depends totaltime O B ﬁrst tie breaker G P UserHeuristic Finally hold D ﬁrst tie breaker NDVPL metric functions additive totaltime Deﬁnition 9 Additive totaltime ATT A metric function M additive total time ATT iff MN M P N M T N M P N expression mention function totaltime M T N expression plandependent function totaltime Intuitively ATT metric sum function depends isviolated functions function includes totaltime include isviolated functions Now ready state result formally Theorem 10 The discarding nodes Algorithm 1 preserves optimality algorithm performs sound pruning metric function M NDVPL 1 M independent totaltime 2 M dependent totaltime O B ﬁrst tie breaker UserHeuristic G P 3 M ATT D ﬁrst tie breaker UserHeuristic G P Proof See Appendix B cid2 An important fact sound pruning prunes optimal plans search space optimal plan An important consequence fact search algorithm able ﬁnd optimal plans fairly general conditions Our ﬁrst result says sound pruning optimality guaranteed algorithm terminates Theorem 11 Assume Algorithm 1 performs sound pruning node discarding preserves optimality If terminates plan returned optimal Proof Each planning episode returned better plan algorithm stops ﬁnal planning episode rejected possible plans Since algorithm prunes discards node extended optimal optimal plan plan better returned exists cid2 Theorem 11 guarantee optimal solution algorithm terminate To guarantee impose conditions restrict explored search space ﬁnite Once conditions optimality easy prove search eventually terminate JA Baier et al Artiﬁcial Intelligence 173 2009 593618 607 Theorem 12 Assume following conditions hold 1 The initial value bestMetric worst case upper bound Algorithm 1 ﬁnite 2 The set cyclefree nodes N MetricBoundFNN initial value best Metric ﬁnite 3 Algorithm 1 performs sound pruning 4 Node discarding Algorithm 1 preserves optimality Then Algorithm 1 guaranteed ﬁnd optimal plan exists Proof Each planning episode examines nodes estimated metric valuegiven MetricBoundFNthat bestMetric By assumption 2 ﬁnite set nodes episode complete algorithm eventually terminate Now result follows Theorem 11 cid2 In Theorem 12 condition 1 satisﬁed implementation algorithm uses suﬃciently large number initial value bestMetric Moreover Theorem 7 shows condition 3 satisﬁed Theorem 10 shows condition 4 satisﬁed Condition 2 falsiﬁed PDDL3 instance In particular metric function deﬁned way value improves number violated precondition preferences increases Under metric function plans metric values improve bound plan length increases This mean number plans metric value initial bound bestMetric unbounded condition 2 violated We avoid cases like metric function bounded precondition prefer ences Deﬁnition 13 BPP metrics Let individual precondition preferences planning instance P Γ let U denote initial value bestMetric A metric function bounded precondition preferences BPP exists value ri precondition preference pi Γ node N MetricBoundFNN U pi violated ri times BPP metrics isviolated functions smaller ﬁxed bound node metric value lower U This property guarantees ﬁnite number plans value U ultimately enables prove optimality result Corollary 14 Assume metric function planning instance P BPP assume conditions 1 3 4 Theorem 12 hold Then Algorithm 1 ﬁnds optimal plan P Proof We need prove set nodes N MetricBoundFNN bestMetric ﬁnite This satisfy condi tion 2 allow apply Theorem 12 The BPP condition ensures precondition function pi N value range 0ri ﬁxed value ri Since precondition functions functions planning instance remaining elements state boolean predicates means ﬁnite number different states property cid2 Note NDVPL property use satisfy condition 4 Theorem 12 imply necessarily BPP property As example suppose domain precPref precondition preference goalPref1 goalPref2 ﬁnalstate preferences Assume B function MetricBoundFN metric node N deﬁned MN isviolatedgoalPref1 N isviolatedprecPref N isviolatedgoalPref2 N 2 M clearly NDVPL decrease plans violate preferences However M necessarily increase preferences violated lead situations inﬁnite set goal nodes metric value Indeed assume goalPref2 unreachable preference detected relaxed planning graph wont detected B bounding function Moreover assume planner node satisﬁes goalPref1 Assuming precPref violated action planning instance inﬁnite plans generated violate precPref repeatedly satisfying goalPref1 Because isviolated functions represented state plans eliminated algorithm produce cycles The BPP NDVPL properties natural conditions metric function Indeed reasonable assume violated preferences undesirable Hence plan arbitrarily worse number preferences violates arbitrarily larger Such property suﬃcient guarantee NDVPL BPP conditions The additive family metric functions satisﬁes conditions deﬁned follows Deﬁnition 15 Additive metric function A PDDL3 metric function additive isviolatedpi ci cid3 0 form M cid6 n i0 ci 608 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 Remark 16 Additive metric functions satisfy NDVPL condition satisfy BPP condition MetricBoundFN B O Additive metric functions problems qualitative preference track IPC5 Therefore algorithmwhen O B pruningis guaranteed ﬁnd optimal solution problems given suﬃcient time memory In practice restrictions time memory algorithm ﬁnds optimal solution simple problems On larger problems returned best plan completed planning episodes time alloted 431 kOptimality Instead searching optimal plan set valid plans interested restricting attention subset valid plans For example resource usage limitations constrain set plans willing accept This case shift worker asked work overtime shift days plane log certain number continuous kilometers If set plans interested characterized temporally extended property suﬃces add property set hard constraints The optimality results presented allow planner ﬁnd optimal plan restricted set plans regardless property For interesting properties ﬁnd optimal plans weaker conditions metric function required general case This case example interested plans length bounded certain value Several existing preference planners able ﬁnd plans optimal set plans restricted length makespan For example PPlan 6 given bound k able ﬁnd optimal plan length k Similarly Brafman Chernyavsky 10 SatplanP 24 return optimal plans plans makespan n n parameter It noted plans need globally optimal That plans longer length makespan higher value plan returned systems Our algorithm hand return globally optimal plan conditions described If interested plans restricted length algorithm return koptimal plans weaker conditions Deﬁnition 17 koptimal plan A plan koptimal iff optimal set plans length cid2 k To achieve koptimality force algorithm search space plans length smaller equal k imposing additional hard constraint restricts length plan Theorem 18 Assume Algorithm 1 uses sound pruning set initial hard constraints contains formula totaltime cid2 k Then returned plan koptimal Proof Since space plans length k ﬁnite planning episode terminate improved plan exists Because sound pruning node wrongly pruned search space Hence returned plan optimal cid2 Note result require restrictions metric function condition 2 Theorem 12 Thus result satisﬁed broader family metric functions satisfy Theorem 12 example satisﬁed NDVPL metrics Eq 2 5 Implementation evaluation We implemented ideas planner HPlanP HPlanP consists modules The ﬁrst preprocessor reads PDDL3 problems generates planning problem simple preferences expressed TLPlan domain The second module modiﬁed version TLPlan able compute heuristic functions implements algorithm Section 4 Recall key elements algorithm iterative pruning strategy heuristics planning In following subsections evaluate effectiveness planner obtaining good quality plans combinations heuristics As testbed use problems qualitative preferences track IPC5 contain TEPs The IPC5 domains composed transportation domains TPP trucks production domain openstacks domain involves moving objects machines restrictions storage ﬁnally rovers models rover collect experiments details refer reader IPC5 booklet 13 Each domain consists 20 problems The problems trucks openstacks rovers domains hard goals preferences The remaining problems preferences Preferences domains impose interesting restrictions plans usually plan achieve At end section compare planner planners participated IPC5 The results based data available IPC5 22 experiments JA Baier et al Artiﬁcial Intelligence 173 2009 593618 609 51 The effect iterative pruning To evaluate effectiveness iterative pruning compared performance pruning functions optimistic metric O best relaxed metric B pruning From experiments conclude time pruning produce better results pruning overall pruning B usually produces better results pruning O To compare different strategies ran IPC5 problems O pruning 30minute timeout The heuristics experiments topperforming strategies domain pruning B The impact pruning varies different domains In domains impact pruning little In storage TPP domains pruning effect practice In rovers domain impact slim O performs good B pruning average produces solutions 005 increase metric An increased impact observed trucks domain topperforming heuristics improve metric ﬁrst plan 3060 B pruning O pruning metric improved 2802 average pruning 2133 average Finally greatest impact observed openstacks domain Here B produces 1363 improvement average pruning pruning O produce 162 improvement In general pruning noticeable impact search frequently proven certain preferences satisﬁed In case openstacks domain example preferences require certain products associated orders delivered On hand goal usually requires number orders shipped To ship order required start order ship However deliver product associated order o needs product o started o shipped Thus order o shipped B function automatically regards unsatisﬁable preferences involved delivery unmade product associated o This occurs frequently search plans domain The initial solution ignores preferences produces plan makeproduct actions As search progresses states ﬁnish order early constantly pruned away turn favours adding makeproduct actions A effect pruning prove conditions Theorem 11 met optimal solution Indeed algorithm stops simplest problems domains proving optimal plan If pruning search generally terminate 52 Performance heuristics To determine effectiveness prioritized heuristic sequences Section 41 compared 42 heuristic se quences B pruning function allowing planner run 15 minutes 80 IPC5 problem instances All heuristics G highest priority omit G names Speciﬁcally experi mented O B O P P O B P P B B Dr DrB O Dr DrO r 0 001 005 01 03 05 07 09 1 In general heuristic better produces plans better quality quality measured metric plans To evaluate good heuristic measure percent improvement metric plan respect metric ﬁrst plan Thus ﬁrst plan metric 100 metric 20 percent improvement 80 Since ﬁrst plan G metric value regardless heuristic choose Hence measure objectively compare performance Table 1 shows best worst performing heuristics domains tested In domains heuris tics yield similar performance Moreover conclude heuristic functions use relaxed planning graph key good performance In problems save TPP heuristics relaxed planning graph best performance The case TPP pathological qualitative preference track However looking actual plans traversed search observed case O good heuristic problem O totally blind states O equal 0 Rather turns heuristics based relaxed planning graph poor domain misguiding search In Section 6 explain scenarios heuristics perform badly details TPP cases 53 Comparison approaches We entered HPlanP IPC5 Qualitative Preferences track 22 achieving second place SGPlan5 28 Despite HPlanPs distinguished standing SGPlan5s performance superior HPlanPs ﬁnding better quality plans generally solving problems solving faster SGPlan5s superior performance unique pref erences tracks SGPlan5 dominated 6 tracks IPC5 satisﬁcing planner competition As conjecture superior performance attributed partitioning techniques use speciﬁc planning preferences techniques combined HPlanP This supported fact HPlan P similar better performance SGPlan5 simple planning instances experiments shown end section HPlanP consistently performed better mipsbdd 17 mipsxxl 15 HPlanP usually ﬁnd plans bet ter quality solve problems mipsbdd mipsxxl use related techniques based propositional Büchi 610 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 Table 1 Performance different heuristics problems Qualitative Preferences track IPC5 The second column shows number problems plan The shows plans subsequently improved planner The average percent metric improvement respect ﬁrst plan shown square brackets Domain openstacks trucks storage rovers TPP 1 Plan 18 5 16 11 20 4 9 9 1 Plan 14 Best heuristics BP1377 DO11363 DB11363 BD11363 B1363 Worst heuristics D0B756 r 001 005 01 DOr763 DBr763 D0O3068 OD03068 PB535 OP535 PO535 O1202 BO37 OB37 B37 O37 BD0053562 OD0053555 BD03542 PO2104 PB2104 BP2418 OP2418 D01O1715 D01B1715 D03B1691 D03O1691 O001D1647 O005D1647 20 O4032 BO3202 B3202 OB3397 BP697 OP716 B1085 OB1085 BO1085 O1085 r cid2 09 BDr903 OD091098 Table 2 Relative performance HPlanPs best heuristics simple preferences compared IPC5 participants Ratio compares performance particular planner HPlanPs Ratio 1 means HPlanP superior Ratio 1 means S number problems solved means planner compete domain Domain HPlanP SGPlan5 PS Yochan mipsbdd mipsxxl TPP openstacks storage pathways S 20 20 20 20 Ratio 1 1 1 1 S 20 20 20 20 Ratio 07808 089092 074076 077 S 11 5 4 Ratio 102107 386395 102 S 9 2 4 10 Ratio 094099 25 1 079 S 9 18 4 16 Ratio 168178 645681 1541 119121 automata handle LTL preferences We think superior performance explained compi lation ground LTL formulae avoiding blowups heuristics easy compute For example mipsxxl mipsbdd able solve ﬁrst problems smallest openstacks domain HPlanP quickly ﬁnd plans In domain number preferences typically high instance contains 120 preferences On hand similar occurs storage mains In domain fewer preferences quantiﬁed More details results IPC5 22 While enter Simple Preferences track experiments performed competition indicate HPlanP track To perform comparison ran planner 15 min2 ﬁrst 20 instances3 domain In Table 2 performance HPlanPs best heuristics compared participants domains planners solved problem HPlanP able solve 20 problems domains trucks solve 5 simpler instances Table 3 details trucks domain In table S number problems solved approach Ratio average ratio metric value obtained particular planner metric obtained planner Thus values 1 indicate planner ﬁnding better plans values 1 indicate opposite The results HPlanP obtained IntelR XeonTM CPU 266 GHz machine running Linux timeout 15 min Results planners extracted IPC5 oﬃcial results generated Linux IntelR XeonTM CPU 300 GHz machine 30 min timeout Memory limited 1 GB processes We conclude SGPlan5 typically outperforms HPlanP SGPlan5 average obtains plans 25 better terms metric value obtained HPlanP Moreover simple instances usually HPlanP PS equally better SGPlan5 Table 3 HPlanP solve instances solved Yochan mipsxxl mipsbdd Furthermore outperforms Yochan mipsxxl terms achieved plan quality HPlanPs performance comparable mipsbdd problems solved planners Finally observed bestperforming heuristics domains TPP use relaxed planning graph particular D heuristic PS We ran ﬁnal comparison SGPlan5 HPlanP openstacksnce domain 25 openstacksnce formulation original openstacks simplepreferences domain include actions conditional effects These domains essentially equivalent sense plans domain corresponding plan equal quality The results shown Table 4 We observe HPlanP consistently outperforms SGPlan5 2 In IPC5 planners given 30 min similar machine 3 Only pathways domain 20 problems JA Baier et al Artiﬁcial Intelligence 173 2009 593618 611 Table 3 Plan quality metric HPlanPs heuristics compared IPC5 Simple Preferences participants simpler nonmetric problems ns means instance solved planner means planner compete domain Instance PS Yochan mipsbdd mipsxxl SGPlan5 HPlanP TPP01 TPP02 TPP03 TPP04 TPP05 TPP06 TPP07 openstacks01 openstacks02 openstacks03 openstacks04 openstacks05 openstacks06 openstacks07 trucks01 trucks02 trucks03 trucks04 trucks05 storage01 storage02 storage03 storage04 storage05 storage06 storage07 pathways01 pathways02 pathways03 pathways04 pathways05 pathways06 pathways07 22 36 24 45 103 133 124 0 3 0 0 1 6 11 49 51 165 ns ns 2 3 3 3 ns ns ns 16 24 29 35 89 110 126 12 12 ns ns ns ns ns 0 0 0 0 ns 3 5 6 9 ns ns ns 2 3 3 2 7 8 11 16 24 29 35 223 275 322 63 63 88 98 133 133 285 0 0 0 ns ns 18 37 158 197 ns ns ns 3 5 47 3 102 129 125 16 24 29 35 79 101 100 13 16 12 26 36 33 67 1 0 0 0 0 5 8 14 17 87 124 160 2 3 3 2 65 10 8 O 16 24 29 39 103 120 124 6 4 36 47 25 21 87 0 0 0 3 0 3 5 6 9 97 161 274 2 3 3 2 85 129 125 ODr 05 ODr 0 ODr 1 16 24 29 35 79 118 135 6 4 30 44 21 18 74 0 0 0 1 0 3 5 6 9 130 195 281 2 4 37 2 9 129 125 16 24 29 35 87 114 135 6 4 36 45 25 21 87 0 0 0 3 0 3 5 6 9 130 195 307 2 4 37 2 102 129 125 16 24 29 42 105 120 135 6 4 30 49 21 18 74 0 0 0 4 0 3 5 6 9 97 161 274 2 4 37 2 102 129 125 Table 4 Metric values obtained HPlanPs heuristics SGPlan5 openstacks openstacksnce 25 domains Instance SGPlan5 HPlanP openstacksnce SGPlan5 HPlanP 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 70 70 90 100 140 140 300 620 620 120 120 153 223 65 210 210 450 930 1581 1348 O 11 7 38 48 48 35 98 140 154 30 36 80 190 47 125 133 224 588 1581 1348 OD5 11 11 42 49 48 41 98 152 155 25 26 81 172 22 123 133 255 558 1581 1348 openstacksnce OD0 11 7 37 46 48 34 98 148 154 30 36 80 181 47 125 133 269 929 1581 1348 OD1 11 11 41 49 48 41 98 148 154 20 22 73 174 24 126 133 254 557 1581 1348 13 16 12 26 36 33 67 123 121 20 21 23 48 6 0 0 0 0 254 424 O 6 4 36 47 25 21 87 86 109 19 19 52 171 32 74 74 209 557 1581 1348 openstacks OD5 6 4 30 44 21 18 74 78 123 11 22 45 167 23 67 63 179 464 1581 1348 openstacks OD0 6 4 36 45 25 21 87 86 109 10 23 45 167 21 67 67 179 464 1581 1348 OD1 6 4 30 49 21 18 74 78 123 13 12 51 167 21 67 63 180 493 1581 1348 612 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 instances domain obtaining plans usually 50 better quality We observe perfor mance HPlanP consistent formulations case SGPlan5 6 Discussion In previous sections proposed collection heuristics planning TEPs simple prefer ences conjunction incremental search algorithm In experimental evaluation saw domains heuristics utilize relaxed planning graph provide best performance Given limited num ber domains opportunity test planner hardand impossibleto conclude best combination heuristics use It hard justiﬁed recipe use However situations heuristics perform poorly identiﬁed analyzed Below reasons potential poor performance The ﬁrst reason potentially poor performance choice prioritized sequences heuristics We chosen goal distance G appear ﬁrst priority guide planner satisfying mustachieve goals pragmatic reason goal important thing achieve However design decision makes search algorithm focus excessively goal achievement detriment preference satisfaction This issue particularly relevant interactions goal preferences Consider example situation preference p achieved achieving goal Furthermore assume goal g conjunction f 1 f 2 assume prior achieving p f 2 false In cases like algorithm ﬁnds plan goal hardly ﬁnd plan satisﬁes p When extending plan g planner f 2 action invalidates f 2 action choose action invalidate subgoal available This goal distance G search node f 2 false strictly greater goal distance f 1 f 2 true As consequence algorithm trouble achieving p actually achieve p extending plan g actions invalidate f 2 available Unfortunately way getting situation implies exhausting search space plans extend plan g invalidating g The second source poor performance loss structure incur computing heuristic planning instance actions deletes negative effects ignored The inaccurate reachability information provided relaxation signiﬁcantly affect performance heuristics based relaxed planning graph P B D Consider example instance hard goals preferences p1 p2 Assume p2 preference easy achieve state violated order achieve p1 Assume state p2 satisﬁed p1 need perform actions achieve p1 p2 Let actions b c makes p2 false p1 true ﬁnally action b followed c reestablish p1 shown Fig 3 Moreover assume action e applicable s leads s2a state p1 p2 reached sequence actions Because D heuristic computed delete relaxation D prefer expand s2 instead s1 A relaxed solution s2 achieve preferences depth 1 preference p2 satisﬁed depth 0 On hand relaxed solution s1 achieve preferences depth 2 s1 actions needed reestablish p2 Once algorithm expands s2 action applicable s2 analogous e steer search away s3 It precisely situation similar described makes heuristics based relaxed planning graph especially D P perform poorly TPP domain TPP transportation problem trucks markets depots transporting goods A good truck performing load followed store Stored goods unloaded truck performing unload Once market buy object ready load In problems TPP domain preference states good eventually loaded truck p1 On hand preference states trucks unloaded end plan p2 Once considered moving truck market bought certain good good1 plan preﬁx achieved p2 p1 A reasonable course action achieve preferences load good1 truck followed store followed unload However state results performing load preferred planner like Fig 3 load invalidates p2 making p1 true Instead action preserves Fig 3 A situation D heuristics prefers node lead quick satisfaction p1 p2 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 613 p2 property buy good preferred This leads planner consider possible combinations sequences buy good considering load Even worse performing possible buys similar reason search prefers use truck market buying products 7 Related work There signiﬁcant work planning preferences related varying degrees method presented We organize work groups ﬁrst planners able plan preferences nonPDDL3 preference languages soft goals second work focuses PDDL3 language In rest section review literature categories 71 Other preference languages PPlan 6 planning exploits progression plan directly TEPs heuristic search In contrast HPlanP incremental PPlan returns optimal plan length bounded planlength parameter koptimal Unfortunately PPlan uses admissible heuristic far informative heuristics pro posed As far eﬃcient The heuristic PPlan similar O heuristic provide estimate cost achieving unsatisﬁed preferences PPlan developed prior deﬁnition PDDL3 exploits qualitative preference language LPP deﬁne preferences LPP supports rich TEPs including nested LTL formulae unlike PDDL3 specifying metric objective function LPP objective expressed logical formula PPlans LPP language extension improvement PP language proposed Son Pontelli 35 The HPlanQP planner 3 proposed answer shortcomings PPlan It extension HPlanP allowing planning qualitative TEPs guided heuristics similar proposed paper The preference language based LPP language PPlan HPlanQP guides search actively satisfaction preferences unlike PPlan like HPlanP guarantees optimality plan given suﬃcient resources Also related work partial satisfaction planning problems PSPs oversubscription planning 3436 PSPs understood planning problem hard goals collection soft goals associated utility actions costs associated Some existing planners PSPs 1433 incremental use pruning techniques However general offer optimality guarantees Recently Benton et al 5 developed incremental planner bboplp uses branchandbound pruning PSP planning similar approach bboplp able offer optimality guarantees given suﬃcient resources However contrast HPlanP uses different techniques obtaining heuristics To compute heuristics ﬁrst relaxes original planning problem creates integer programming IP model new problem It computes heuristics linearprogramming relaxation IP model Lastly Feldmann et al 18 propose planner PSPs iteratively invokes MetricFF ﬁnd better plans Bonet Geffner 9 proposed framework planning action costs costsrewards associated ﬂuents Their cost model represent PSPs simple preferences subset PDDL3 They propose admissible heuristics optimal algorithm planning model Heuristics obtained compiling relaxed instance problem dDNNF algorithm modiﬁcation A The approach scale large planning instances need employ admissible heuristic Finally work casts preferencebased planning problem answer set programming problem ASP constraint satisfaction problem CSP satisﬁability SAT instance The paper Son Pontelli 35 proposed ﬁrst languages preferencebased planning PP cast planning problem optimization ASP problem Their PP language includes TEPs expressed LTL Brafman Chernyavsky 10 proposed CSP approach planning ﬁnalstate qualitative preferences speciﬁed TCPnets Additionally Giunchiglia Maratea 24 pro posed compilation preferencebased planning problems SAT None approaches exploits heuristic search fundamentally different form approach proposed The approaches guide search solution imposing variablevalue ordering attempt produce preferred solutions ﬁrst Because works recasting problem different formalism explore different search space approach Note conversion ASP CSP SAT requires assuming ﬁxed bound plan length limiting approach best ﬁnding koptimal plans 72 IPC5 competitors Most related work approaches taken planners competed IPC5 PDDL3 language form heuristic search Yochan 4 heuristic planner simple preferences based Sapaps 36 Our approach similar theirs sense use relaxed planning graph obtain heuristic estimate Yochan incremental planner employing heuristics geared classical goals However compute heuristic explicitly selects subset preferences achieve treats subset classical goal This process costly presence preferences PS PS 614 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 mipsxxl 17 mipsbdd 15 use Büchi automata plan temporally extended preferences While approach compiling away TEPs constructs automata approach translation process generates grounded preference formulae This makes translation algorithm prone unmanageable blowup Further search techniques planners different exploit mipsxxl iteratively invokes modiﬁed MetricFF 26 forcing plans decreasing metric values mipsbdd hand performs costoptimal breath ﬁrst search employ heuristic Finally winner preferences tracks IPC5 SGPlan5 28 uses completely different approach It partitions planning problem subproblems It uses modiﬁed version ff solve subproblems ﬁnally integrates subsolutions solution entire problem During integration process attempts minimize metric function SGPlan5 incremental suffer nonrobustness performance shown results given Table 4 performance reformulated equivalent domain changes dramatically 8 Conclusions future research In paper presented new technique planning preferences deal simple preferences temporally extended preferences hard constraints The core technique new set heuristics incremental search algorithm amenable integration variety classical simplepreference planners The compi lation technique converting TEPs simple preferences work planners method embedding constructed automata utilize need modiﬁcation dependent facilities available planner Our method embedding constructed automata utilized TLPlans ability deal numeric functions quantiﬁcation In particular TLPlans ability handle quantiﬁcation allowed utilize parameterized representation preferences generated compilation leading considerably compact domain encoding We presented number different heuristics planning preferences These heuristics feature account value achieved unsatisﬁed preferences account diﬃculty actually achieving preferences Our method combining different types guidance simple tiebreaking sophisticated combinations related heuristics investigated More generally question identifying domain features particular heuristics suitable interesting direction future work We presented incremental bestﬁrst search planning algorithm A key feature algorithm use heuristic bounding functions prune search space incremental planning episodes We proved fairly natural conditions algorithm generate optimal plans It worth noting conditions require algorithm utilize admissible heuristics Nor require imposing priori restrictions plan size length makespan allow algorithm achieve koptimality global optimality The algorithm employ different heuristics incremental planning episode exploit ﬁrst planning episode ignoring preferences asking planner search plan achieving goals The motivation want working plan hand trying ﬁnd preferred plan In experiments remaining planning episodes executed ﬁxed heuristic More ﬂexible schedules heuristics investigated future work Finally implemented method extending TLPlan planning performed extensive experiments IPC5 problems evaluate effectiveness heuristic functions search algorithm While heuristic dominated test cases clearly provided superior guidance good solutions In particular use relaxed planning graph way proved effective domains Experiments conﬁrmed essential role pruning solving large problems HPlanP scales better approaches planning preferences attribute superior performance fact ground planning problems Although proposed heuristics perform reasonably benchmarks tested identiﬁed cases perform poorly In cases computing heuristics delete relaxation provide bad guidance presence preferences The resolution issues raised open interesting avenues future research Acknowledgements We gratefully acknowledge funding Natural Sciences Engineering Research Council Canada NSERC Ontario Ministry Research Innovation Early Researcher Award We thank Christian Fritz helpful discussions development planner anonymous reviewers useful feedback Appendix A Proof Proposition 2 In section prove Proposition 2 First prove intermediate results ﬁnal proof JA Baier et al Artiﬁcial Intelligence 173 2009 593618 615 The ﬁrst intermediate result says NNF formula φ P true state s denoted s cid13 φ φ proposition true s true relaxed state This proven F true relaxed state F following lemma Lemma 19 Let P set propositions φ NNF formula s F F P states Then s cid13 φ F F 1 F 2 F F F cid13 p p s cid13 p p sc F F cid13 φ Proof The proof follows induction structure φ Base cases φ p φ p They follow directly conditions lemma Induction We following cases If φ ψ ξ s cid13 ψ s cid13 ξ By inductive hypothesis F F cid13 ψ F F cid13 ξ It follows Deﬁnition 1 F F cid13 φ If φ ψ ξ proof analogous previous case If φ xψ o Objs s cid13 ψxo By inductive hypothesis o Objs F F cid13 ψxo Deﬁnition 1 F If φ xψ proof analogous previous case cid2 F cid13 φ The ﬁnal intermediate result actually version Proposition 2 simple facts Lemma 20 Let s planning state R F m relaxed planning graph constructed s ﬁxed point Moreover let sn state results performing legal sequence actions a1 s exists k cid2 m F k cid13 f f s F k cid13 f f sc k F k F m F 1 F m1F m1 F 0 F 1 F 0 F m m 0 Moreover assume Proof Since R constructed ﬁxed point F set states generated performing action sequence s s1 sn state si generated performing sequence actions a1 ai s The following proof lemma induction length action sequence n m F F F Base case n 0 We prove case consider k 0 In case sequence actions performed s s F sc Let f arbitrary fact By deﬁnition construction R F F F m1 m1 k 0 0 k 1 f s Then Deﬁnition 1 F 2 f sc Then Deﬁnition 1 obtain F k cid13 f k 0 concluding proof case k cid13 f k 0 k F k F Induction Let assume theorem true n 1 We prove true n We divide proof cases Again assume f arbitrary fact 1 f sn f sn1 This case trivial inductive hypothesis F 2 f sn f sn1 Again induction hypothesis F k F 3 f sn f sn1 Then added fact k cid13 f k cid2 m f performed sn1 We prove action cid11 cid2 m 1 relaxed planning graph add fact f graph level executable level k cid11 1 cid2 m k Let assume precondition action ϕP condition conditional effect adds f ϕc Then formulae satisﬁed sn1 k F k cid13 f k cid2 m sn1 cid13 ϕP ϕc Moreover inductive hypothesis exists k cid11 cid2 m F F kcid11 F kcid11 F kcid11 cid13 p kcid11 cid13 p p sn1 p sc n1 A1 A2 A3 At point safely assume k cid11 m 1 graph constructed ﬁxed point k cid11 m k cid11 equal m A2 A3 hold 616 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 Now combine Eqs A2 A3 A1 Lemma 19 conclude F executable level k f kcid11 cid13 ϕP ϕc Action relaxed planning graph condition ϕc enables conditional effect adds cid11 1 cid2 m concluding proof case added graph level k k true level k Therefore f kcid11 F cid11 cid11 4 f sn f sn1 Proof analogous previous case cid2 Now ready prove result Proof Proposition 2 By Lemma 20 know exists k cid2 m p sn F k cid13 p Because sn cid13 φ follows Lemma 19 F p sc F k cid13 φ cid2 k F k F k F k cid13 p Appendix B Proof Theorem 10 Before start proof prove lemma establishes conditions Theorem 10 nodes exactly state different B D O metric value lengths differ analogously Lemma 21 Let N1 N2 search nodes correspond planning state s Furthermore let metric M instance NDVPL depend totaltime If RN1 cid2 RN2 1 R O B 2 M ATT R D lengthN1 cid2 lengthN2 Proof We divide proof cases Case 1 R O B Then RN1 MN possibly preferences satisﬁed Analogously RN2 MN Therefore cid11 1 N cid11 1 hypothetical node length N1 cid11 2 length N2 cid11 2 node N MN cid11 1 cid2 MN cid11 2 B1 cid11 Because planning state associated N1 N2 identical know N 1 satisfy exactly preferences Γ set preferences planning instance p Γ cid11 isviolatedp N 2 Now contrapositive implication 2 NDVPL deﬁnition cid11 2 This implies lengthN1 cid2 lengthN2 Deﬁnition 3 Eq B1 lengthN concludes proof case cid11 1 isviolatedp N cid11 1 cid2 lengthN cid11 2 N Case 2 R D M ATT Because M ATT Eq 1 DN1 MN1 R1 R1 expression depend totaltime depends N1s state Likewise DN2 MN2 R2 R2 depends state N2 Since states corresponding N1 N2 equal R1 R2 Hence DN1 cid2 DN2 MN1 cid2 MN2 contrapositive implication 2 NDVPL deﬁnition Deﬁnition 3 implies lengthN1 cid2 lengthN2 This concludes case ﬁnishing proof cid2 Now ready prove result First note search restarted scratch ﬁrst plan This means closed list reinitialized Second note nodes N1 N2 state associated G P functions evaluated nodes return value There fore UserHeuristicN1 cid2 UserHeuristicN2 means tie breaker functions R RN1 cid2 RN2 R O B D The sketch proof follows We assume node N leads optimal plan discarded algorithm Then prove happens optimal node frontier extended optimal plan Assume exists optimal plan p1 a1a2 traverses sequence states s0s1 sn Let N1 node formed applying p1 s0 Because metric NDVPL assume plan contains cycles plan contained cycles removing worse Suppose point search node N generated applying a1a2 j initial state j n discarded algorithm line 8 This means exists closed node NC associated state N UserHeuristicNC cid2 UserHeuristicN B2 Both nodes associated state s j isviolated counters identical preference This means NC constructed s0 sequence actions b1b2 bk This sequence actions gets state s j sequence p2 b1b2 bka j1 plan JA Baier et al Artiﬁcial Intelligence 173 2009 593618 617 Let N2 node constructed applying p2 s0 Now prove N2 corresponds optimal plan We cases Case 1 The metric depends totaltime Because inequality B2 implies RNC cid2 RN R ei ther O D B Lemma 21 lengthNC cid2 lengthN k cid2 j We clearly lengthN2 cid2 lengthN1 furthermore precondition counters identical follows NDVPL condi tion MN2 cid2 MN1 Given N1 represents optimal plan conclude MN2 MN1 N2 represents optimal plan Case 2 The metric depend totaltime Therefore node N2 reaches state N1 M depends properties encoded state MN1 MN2 N2 represents optimal plan This concludes case 2 Now know NC predecessor N2 expanded algorithm following things happen 1 A successor NC frontier In case condition Deﬁnition 8 follows immediately 2 N2 closed list This implies condition Deﬁnition 8 satisﬁed 3 A successor NC discarded algorithm In case successor leads optimal plan This means apply argument proof node leading eventually satisfy condition Deﬁnition 8 algorithm visited ﬁnitely nodes References 1 F Bacchus F Kabanza Planning temporally extended goals Annals Mathematics Artiﬁcial Intelligence 22 12 1998 527 2 JA Baier SA McIlraith Planning ﬁrstorder temporally extended goals heuristic search Proceedings 21st National Conference Artiﬁcial Intelligence AAAI Boston MA 2006 pp 788795 3 JA Baier SA McIlraith On domainindependent heuristics planning qualitative preferences 7th Workshop Nonmonotonic Reasoning Action Change NRAC 2007 4 J Benton S Kambhampati MB Do YochanPS PDDL3 simple preferences partial satisfaction planning 5th International Planning Competition Booklet IPC2006 Lake District England July 2006 pp 5457 5 J Benton M van den Briel S Kambhampati A hybrid linear programming relaxed plan heuristic partial satisfaction problems Proceedings 17th International Conference Automated Planning Scheduling ICAPS Providence RI September 2007 pp 3441 6 M Bienvenu C Fritz S McIlraith Planning qualitative temporal preferences Proceedings 10th International Conference Knowledge Representation Reasoning KR Lake District England 2006 pp 134144 7 A Blum ML Furst Fast planning planning graph analysis Artiﬁcial Intelligence 90 12 1997 281300 8 B Bonet H Geffner Planning heuristic search Artiﬁcial Intelligence 129 12 2001 533 9 B Bonet H Geffner Heuristics planning penalties rewards compiled knowledge Proceedings 10th International Conference Knowledge Representation Reasoning KR 2006 pp 452462 10 R Brafman Y Chernyavsky Planning goal preferences constraints Proceedings 15th International Conference Automated Planning Scheduling ICAPS Monterey CA June 2005 pp 182191 11 AI Coles AJ Smith Marvin A heuristic search planner online macroaction learning Journal Artiﬁcial Intelligence Research 28 February 2007 119156 12 JP Delgrande T Schaub H Tompits Domainspeciﬁc preferences causal reasoning planning Proceedings 14th International Confer ence Automated Planning Scheduling ICAPS Whistler Canada June 2004 pp 6372 13 Y Dimopoulos A Gerevini P Haslum A Saetti The benchmark domains deterministic ipc5 httpzeusingunibsitipc5 July 2006 14 MB Do J Benton M van den Briel S Kambhampati Planning goal utility dependencies Proceedings 20th International Joint Confer ence Artiﬁcial Intelligence IJCAI Hyderabad India 2007 pp 18721878 15 S Edelkamp Optimal symbolic PDDL3 planning MIPSBDD 5th International Planning Competition Booklet IPC2006 Lake District England July 2006 pp 3133 16 S Edelkamp J Hoffmann PDDL22 The language classical 4th International Planning Competition Tech Rep 195 Computer Science Department University Freiburg 2004 17 S Edelkamp S Jabbar M Naizih Largescale optimal PDDL3 planning MIPSXXL 5th International Planning Competition Booklet IPC2006 Lake District England July 2006 pp 2830 18 R Feldmann G Brewka S Wenzel Planning prioritized goals Proceedings 10th International Conference Knowledge Representation Reasoning KR Lake District England July 2006 pp 503514 19 R Fikes NJ Nilsson STRIPS A new approach application theorem proving problem solving Artiﬁcial Intelligence 2 34 1971 189208 20 M Fox D Long PDDL21 An extension PDDL expressing temporal planning domains Journal Artiﬁcial Intelligence Research 20 2003 61124 21 BC Gazen CA Knoblock Combining expressivity UCPOP eﬃciency graphplan ECP97 Toulouse France September 1997 pp 221233 22 A Gerevini Y Dimopoulos P Haslum A Saetti 5th International Planning Competition httpzeusingunibsitipc5 July 2006 23 A Gerevini D Long Plan constraints preferences PDDL3 Tech Rep 20050807 Department Electronics Automation University Brescia Brescia Italy 2005 24 E Giunchiglia M Maratea Planning satisﬁability preferences Proceedings 22nd AAAI Conference Artiﬁcial Intelligence AAAI Vancouver British Columbia 2007 pp 987992 25 P Haslum Openstacks SPNCE domain httpusersrsiseanueduaupatrikipc5html 2007 26 J Hoffmann The MetricFF planning Translating ignoring delete lists numeric state variables Journal Artiﬁcial Intelligence Research 20 2003 291341 27 J Hoffmann B Nebel The FF planning Fast plan generation heuristic search Journal Artiﬁcial Intelligence Research 14 2001 253302 28 CW Hsu B Wah R Huang Y Chen Constraint partitioning solving planning problems trajectory constraints goal preferences Proceedings 20th International Joint Conference Artiﬁcial Intelligence IJCAI Hyderabad India January 2007 pp 19241929 29 DV McDermott A heuristic estimator meansends analysis planning AIPS96 1996 pp 142149 30 DV McDermott PDDLThe Planning Domain Deﬁnition Language Tech Rep TR98003DCS TR1165 Yale Center Computational Vision Con trol 1998 618 JA Baier et al Artiﬁcial Intelligence 173 2009 593618 31 EPD Pednault ADL Exploring middle ground STRIPS situation calculus Proceedings 1st International Conference Knowledge Representation Reasoning KR Toronto Canada May 1989 pp 324332 32 A Pnueli The temporal logic programs Proceedings 18th IEEE Symposium Foundations Computer Science FOCS 1977 pp 4657 33 R Sanchez S Kambhampati Planning graph heuristics selecting objectives oversubscription planning problems Proceedings 15th International Conference Automated Planning Scheduling ICAPS Monterey CA 2005 pp 192201 34 DE Smith Choosing objectives oversubscription planning Proceedings 14th International Conference Automated Planning Scheduling ICAPS Whistler Canada 2004 pp 393401 35 TC Son E Pontelli Planning preferences logic programming V Lifschitz I Niemela Eds Proceedings 7th International Confer ence Logic Programming Nonmonotonic Reasoning LPNMR LNCS vol 2923 Springer 2004 pp 247260 36 M van den Briel RS Nigenda MB Do S Kambhampati Effective approaches partial satisfaction oversubscription planning Proceedings 19th National Conference Artiﬁcial Intelligence AAAI 2004 pp 562569 37 L Zhu R Givan Simultaneous heuristic search conjunctive subgoals Proceedings 20th National Conference Artiﬁcial Intelligence AAAI Pittsburgh Pennsylvania USA July 913 2005 pp 12351241