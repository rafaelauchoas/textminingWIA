Artiﬁcial Intelligence 171 2007 805837 wwwelseviercomlocateartint Negotiating rewards Sarvapali D Ramchurn Carles Sierra b Lluís Godo b Nicholas R Jennings IAM Group School Electronics Computer Science University Southampton UK b IIIAArtiﬁcial Intelligence Research Institute CSIC Bellaterra Spain Received 8 November 2006 received revised form 2 April 2007 accepted 2 April 2007 Available online 6 May 2007 Abstract Negotiation fundamental interaction mechanism multiagent systems allows selfinterested agents come mutually beneﬁcial agreements partition resources efﬁciently effectively Now situations agents need negotiate times developing strategies effective repeated interactions important challenge Against background growing body work examined use Persuasive Negotiation PN involves negotiating rhetorical arguments threats rewards appeals trying convince opponent accept given offer Such mechanisms especially suited repeated encounters allow agents inﬂuence outcomes future negotiations negotiating deal present aim producing results beneﬁcial parties To end paper develop comprehensive PN mechanism repeated interactions makes use rewards asked given Our mechanism consists parts First novel protocol structures interaction capturing commitments agents incur rewards Second new reward generation algorithm constructs promises rewards future interactions means permitting agents reach better agreements shorter time present encounter We develop speciﬁc negotiation tactic based reward generation algorithm achieve signiﬁcantly better outcomes existing benchmark tactics use inducements Speciﬁcally empirical evaluation MultiMove Prisoners Dilemma setting tactic lead 26 improvement utility deals 21 times fewer messages need exchanged order achieve 2007 Elsevier BV All rights reserved Keywords Persuasive negotiation Repeated negotiations Negotiation tactics Bargaining Bilateral negotiation 1 Introduction Negotiation fundamental concept multiagent systems MAS enables selfinterested agents ﬁnd agreements partition resources efﬁciently effectively In cases negotiation proceeds series offers counteroffers 20 These offers generally indicate preferred outcome proponent opponent accept counteroffer beneﬁcial outcome reject Now cases Corresponding author Email addresses sdrecssotonacuk SD Ramchurn sierraiiiacsices C Sierra godoiiiacsices L Godo nrjecssotonacuk NR Jennings 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200704014 806 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 agents involved need negotiate times However repeated encounters rarely dealt multiagent systems literature Section 7 details One main reasons repeated encounters require additional mechanisms structures required single shot encounters fully account repeated nature interaction In particular offers generated inﬂuence present encounter future ones better deals long run 925 To end argumentbased negotiation ABN arguments support offers persuade opponent accept advocated effective means achieve 3036 approach explore paper In ABN techniques aim enable agents achieve better agreements faster allowing explore larger space possible solutions andor express update evolve preferences single multiple shot interactions 21 They providing additional explanations justify offer 1 identifying goals satisﬁed offer opponent aware 31 offering additional incentives conditional acceptance offer 22239 While approaches capture way notion persuasiveness number focused speciﬁcally use rhetorical arguments threats rewards appeals 3284144 To clear categorise argument acts persuasive elements aim force entice convince opponent accept given offer Section 7 details In particular categorise approaches general term Persuasive Negotiation PN denote fact try ﬁnd additional incentives opposed justifying elaborating goals offer opponent accept given offer 3036 In order implement PN mechanism critical exchanges negotiating agents follow given pattern ensuring agents seen execute propose negotiation terminates agents endowed appropriate techniques generate exchanges evaluate offers counteroffers negotiation process These requirements met speciﬁcation protocol dictates agents allowed offer commit execute reasoning mechanism allows agents sense offers exchanged accordingly determine best response 30 Given present novel protocol reasoning mechanism pairs agents engage PN context repeated games participating agents negotiate number issues times In particular focus exchange rewards opposed threats appeals We rewards clear beneﬁt agent receiving entail direct commitment agent giving continue long term relationship likely beneﬁcial participating agents1 In addition standard use rewards offered prize gift model allows agents ask rewards attempt secure better outcomes future conceding current encounter closing deal quickly This perspective common humantohuman negotiations participants ask subsequent favour return agreeing concede current round 1733 Being speciﬁc PN mechanism constructs possible rewards terms constraints issues negotiated future encounters protocol extends Rubinsteins 37 alternating offers protocol allow agents negotiate exchanging arguments offers form promises future rewards requests promises future encounters Example A car seller reward buyer prefers red cars promise buyer ask reward discount 10 constraint price seller propose time price yearly car servicing agrees buy blue instead demanded price buyers asking price red car low seller Now buyer accepts better outcome parties buyer beneﬁts able savings future match preference red car seller beneﬁts reduces stock obtains immediate proﬁt 1 The use appeals threats poses number problems For example use appeals usually assumes agents implement deductive mechanism overly constraining assumption cases appeals impact directly agents beliefs goals means appeals need adopt commonly understood belief goal representation 1322 Threats turn tend break relationships guaranteed enforced makes harder assess negotiation encounter 19 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 807 We believe promises important repeated interactions number reasons First agents able reach agreement faster present game providing guarantees outcome subsequent games Thus agents ﬁnd current offer reward worth counteroffer delays agreement future games Second involving issues future negotiations present game cost servicing example effectively expand negotiation space considered provide possibilities ﬁnding better agreements long run 20 For example agents value future outcomes lower discount factors opponent able obtain higher utility future games opponent values immediate rewards quickly Thirdly reward guarantees range possible outcomes game corresponding negotiation space constrained reward reduce number offers exchanged search space time elapsed agreement reached Continuing example buyer starts advantage time wants negotiate price service car need negotiate long reasonable agreement Against background work advances state art following ways First provide new alternating offers protocol extends alternating offers protocol builds Bentahar et al 6 specify commitments agents engaging persuasive negotiations rewards Speciﬁcally protocol details dynamic logic commitments arise retracted result agents promising rewards making offers Thus protocol possible track commitments ensure enact rewards offers commit The protocol standardises agent allowed expect receive opponent turn allows focus making important negotiation decisions Second agents reasoning mechanism develop Reward Generation Algorithm RGA calculates constraints act rewards resources negotiated future games The RGA provides ﬁrst heuristic compute select rewards given asked Third develop speciﬁc Reward Based Tactic RBT uses RGA generate combinations offers rewards In provide ﬁrst PN tactic considers repeated nature interactions generating offers rewards We RBT reach better agreements 26 utility time 21 times fewer messages standard nonpersuasive negotiation tactics The remainder paper structured follows Section 2 describes basic deﬁnitions repeated negotia tion games properties agents Section 3 details PN protocol Section 4 presents RGA functions agents evaluate incoming offers rewards Given Section 5 describes RBT algo rithm In Section 6 empirically evaluate RBT benchmark standard negotiation algorithms Section 7 details related work Section 8 concludes 2 Repeated negotiation games In section formalise repeated negotiation games apply PN Thus let Ag set agents X set negotiable issues Agents negotiate issues x1 xn X value vi domain D1 Dn Then contract O O set issuevalue pairs noted O x1 v1 xm vm O set contracts2 We note set issues involved contract O XO X During negotiation agent limit range values accept issue termed negotiation range noted vxi max Without loss generality require variable xi contract occurs number variables values taken ﬁnite min vxi Given basic deﬁnitions negotiation game agent starts making offer O x1 v1 xm vm rewards set issues x1 xm X opponent counteroffer accept The agents counteroffering agreement reached deadline tdead reached superscript agent identiﬁer needed3 While possible consider inﬁnitely 2 Other operators cid2 cid3 This means agents specify range values enact speciﬁc value This important need specify rewards Section 42 3 If agreement reached agents committed enacting deal settled according protocol deﬁned Section 3 Note forced enact deal trust model 3443 check behaviour agent altered accordingly However case scope work 808 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 Table 1 Summary notation xi max xi min v Ag O U O Oα X v t θ τ cid7α ecid7α θ t ecid7α τ t t α dead Lα cid2 δU β xi set agents usually α β set contracts contract O O utility contract contract xi XOα δU α xi set negotiated issues x1 x2 negotiation range given issue xi time ﬁrst negotiation game started delay negotiation games time offers discount factor agent α discount games agent α discount offers agent α deadline negotiation game α target utility agent α ﬁnitely repeated games focus base case repetition work aim understand foundational level impact promises future rewards encounters We constrain games differentiate case agents play game time independently ﬁrst allowing second game happen current game successful outcome agreement reached agents deadlines In possibility agents negotiate outcomes negotiation round The agents come agreement ﬁrst game fail reach second game case obtain utility outcome ﬁrst game This believe closely models realistic applications agents engage longterm relationships ﬁnd beneﬁt given result previous agreement reach agreements prior continuing relationship Such approaches common longterm contracting relationships deﬁned economic literature 925 Negotiation games played sequence delay θ end ﬁrst game beginning second Moreover game time transmitted offer noted τ In negotiation game agents assess value offers exchanged utility function Each agent 0 1 utility contract U O 0 1 privately known utility function issue Uxi Dxi deﬁned 1 cid2 U O wiUxi vi i1m cid3 O x1 v1 xm vm wi weight given issue xi wi 1 We consider agents α β Ag having utility functions designed MultiMove Prisoners Dilemma MMPD game chosen canonical ubiquitous naturesee Appendix A details 5746 According game αs marginal utility δU higher absolute scale βs issues note Oα noted Oβ Oα Oβ O4 Moreover given delays exist games agents utilities discounted follows In games discount computed ecid7θt offers ecid7τ t t time negotiation started note expect θ cid8 τ generally cid7 known discount factor agent5 The value cid7 scales impact delays higher value means signiﬁcant discounting offer lower value means lower discounting effect Finally agent assumed target utility achieve games noted L 0 2 This target regarded agents 4 By establishing relationship agents utility functions aim model applicable realistic settings Also believe unreasonable assume agents estimate issues important higher δU opponent In case mechanism applies case agents marginal utilities sum zero case agents play common zerosum game 25 5 The exponential decay function commonly bargaining theory capture cumulative discounting effect delays offers Other functions according particular application context chosen SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 809 aspiration level combined outcomes games 13 This target equal sum maximum achievable utility games 2 case agent cid7 0 exploits games completely L cid3 1 ecid7θt 1 maximum achievable utility undiscounted game Having deﬁned basic constructs repeated negotiation games summarise notation Table 1 In section negotiation protocol To end build notation presented section order clearly specify semantics interaction 3 The negotiation protocol As discussed earlier negotiation proceeds exchange offers counteroffers 37 In general pro tocol speciﬁcation interaction simple type commitment upheld agent time enacting proposal offer accepted However extending protocol en capsulate persuasive elements rewards means commitments pertaining enactment content rewards speciﬁed agents issuing rewards 62347 We term commitments social commitments pledges agents virtue publicly visible actions utterances These commitments checked institution arbitrator sure agents supposed provide guarantees proper behaviour 30 There number representations specify commitments retracted illocutions agents actions agents 30 However given rewards likely result large number states state transitions enactment rewards requires clear semantics actions performed specify protocol Harels dynamic logic DL 18 This type actionbased logic particularly suitable specifying programs sets actions start termination conditions constructs similar negotiation encounter Speciﬁcally build work 6 cater rewards To end ﬁrst provide brief overview constructs dynamic logic specify syntax semantics language protocol Finally axioms capture impact illocutions actions taken agents negotiation encounter 31 Preliminaries Dynamic logic proposed multimodal logical semantics programs A program conceived combination actions change state world The main components DL set atomic programs a0 a1 Π0 set modal formulae Φ world states 18 details The atomic actions basic indivisible execute single step Given program Π generated composing actions number operators b Π b Π signiﬁes b performed sequential composition Π represents iteration indeterminate number times ϕ Π tests formula ϕ Φ satisﬁed current state b Π speciﬁes nondeterministic execution b Moreover aϕ denotes program Π executed necessary ϕ true cid9acid10ϕ denotes program Π executed possible ϕ true The propositional operators 1 deﬁned 0 usual way DL semantics based Kripkestyle structures M S τ ρ S represent set states τ Φ 2S gives states formula true ρ Π 2SS function taking program argument giving corresponding set pairs starting end states program connects In following subsections deﬁne particular theory called PN persuasive negotiation DL model persuasive negotiation dialogue To ﬁrst language set Π0 illocutionary actions agents interchange set formulae Φ state negotiation encounter Given provide set axioms express constraints apply persuasive negotiation protocol 810 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 32 The PN language In section main components language We ﬁrst formalise notion contracts action agents execute Second illocutions exchanged dialogue predicates represent state world 321 Contracts The central element PN contract agents negotiate We extend notion contract given Section 2 capture fact agents execute elements contract To end note set formulae ASG Φ consisting atomic assignments form xi vi conjunctions atomic assignments x1 v1 x2 v2 xn vn6 We introduce operator Do represent contracts atomic actions consistent logical language representation section Thus deﬁne contract x1 v1 xm vm equivalent Dox1 v1 xm vm Moreover union contracts x1 v1 x2 v2 x3 v3 x4 v4 x1 v1 x2 v2 x3 v3 x4 v4 equivalent conjunction contents contracts Dox1 v1 x2 v2 x3 v3 x4 v47 Given deﬁnitions contract Doϕ O ϕ ASG represents action making assignment ϕ true8 322 Illocutions Agents negotiate sending illocutions represent offers counteroffers These illocutions considered actions setting speechact theory 438 Illocutions generally talk illocutions sent later time contracts pair negotiating agents Here set illocutions I Π0 consists general classes The ﬁrst consists proper negotiation illocutions Ineg second contains illocutions Ipers added form persuasive negotiation We denote Iα Iβ set illocutions α β send respectively First negotiation illocutions Ineg general form proposeα β pdenotes α sends proposal β accept deal given p O acceptα β pdenotes α accepts enact contract p O Second persuasive illocutions Ipers general form rewardα β p qdenotes α reward β q O Iα β accepts contract p O p enacted As seen q deal favourable β illocution help β future enhance reputation β unconditional accept deal presented later time askrewardα β p qdenotes α asks reward q O Iβ β β accepts offer presented p O p enacted 323 World description As discussed Section 31 actions programs performed agents result changes state world In model programs consist number illocutions contract executions To represent consequences theses actions exploit theory presented 6 In model authors prescribe commitments hold different states world agents able navigate different states actions perform In short actions lead commitments true false We extend work Bentahar et al incorporate notion commitment framework persuasive negotiation To end ﬁrst conceive 6 Other mathematical operations cid3 cid2 contracts discussed Section 2 7 Actually committing execution contract agent α commits true variable bindings issues agents control issues Xα However simplify notation represent agent socially committed contract 8 Whenever apply operator formula action like Doϕ later propose reward SC actually mean application operator term representing formula This represented Gödel quotes Docid15ϕcid16 We abuse notation omit quotes SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 811 set social commitments dialogue result illocutions uttered retracted illocutions uttered actions executed At beginning negotiation dialogue agent says commitments false As negotiation proceeds true active false inactive according illocutions sent Some commitments false actions performed negotiation In order represent commitments negotiation state need introduce special operators SCα β ϕ q Φ denotes commitment α β enact q given ϕ satisﬁed Here q O Iα ϕ Donea1 Donean Φ denote commitment conditional enactment number actions a1 ϕ true denote commitment unconditional Donea Φ Π denote action performed For instance SCα β Doneproposeα β p acceptβ α p p means case β accepts contract p pro posed α α committed β contract Moreover arbitrary compound formulae Φ constructed atomic formulae formulae ASG standard connectives For example SCα β Doneacceptβ α p Donep q means α committed q β accepted offer p p Building basic elements set states S DL framework determined setting truth values types formulae assignments values issues x v ii instances Done predicates Donep iii instances SC predicates SCα β ϕ p Thus state world described possibly partial assignment values issues actions performed social commitments active Given deﬁnition semantics PN language axioms support basic rules persuasive negotiation protocol 33 The PN axioms We ﬁrst explain basic axioms meaning operators Do Done Doϕϕafter execution Doϕ necessarily ϕ true aDoneaafter executing action necessarily formula Donea true Donea b Donea Donebthe execution action sequence b implies b performed Next capture relationship illocutions social commitments We avoid rules depicting turntaking procedure normally happens negotiation order focus essential features commit ments respect enactment proposals rewards9 proposeα β pSCα β Doneacceptβ α p p This means proposeα β p uttered α commits enact p β accepts proposal rewardα β p qSCα β Doneacceptβ α p p SCα β Doneacceptβ α p Donep q This means rewardα β p q uttered α commits deal p β accepts deal p Moreover α commits reward q O Iα happen contract p true askrewardα β p qSCα β Doneacceptβ α p p SCβ α Doneacceptβ α p Donep q This means askrewardα β p q uttered α commits deal p β accepts tract p Moreover β commits reward q O Iβ happen contract p true 9 The rules encounter use ones described Section 2 The logical representation rules formalised DL ﬁner levels granularity turntaking deadlines send new proposals rewards withdrawal negotiation Examples negotiation protocols cater rules 2327 However choose focus believe bare essentials protocol respect persuasive negotiation 812 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 We outline axioms specify dynamics social commitments actions performed cid4 cid4 Unconditionally committing enacting contract reward SC α β Donea p cid5 cid4 SC cid4 α β Donea p cid5 cid5 SCα β true p In case action α committed enacting p contract reward conditions This usually case accept offer p contract executed reward p given Conditionally committing enacting contract reward SC α β Donea ϕ p cid5 cid4 SC cid4 α β Donea ϕ p cid5 cid5 SCα β ϕ p In case action α commits p ϕ true This happen example reward p offered ϕ represents enactment offer accepted action conditional reward p enacted Enacting contract reward SCα β true p pSCα β true p This simply means commitment enact contract reward revoked contract reward enacted We ﬁnally basic axioms ensure agents commit uptodate contract rewards Committing contract time proposeα β pSCα β Doneacceptβ α pcid17 pcid17 pcid17 cid18 p rewardα β p qSCα β Doneacceptβ α pcid17 pcid17 pcid17 cid18 p askrewardα β p qSCα β Doneacceptβ α pcid17 pcid17 pcid17 cid18 p These mean commitment previous offer retracted new contract offered reward given asked new offer Committing reward time rewardα β p qSCα β Doneacceptβ α p Donep qcid17 qcid17 cid18 q askrewardα β p qSCβ α Doneacceptβ α p Donep qcid17 qcid17 cid18 q These mean commitment previous reward retracted new reward given asked Using axioms possible automatically check agents allowed point negotiation dialogue negotiation ended This achieved storing commitment incurred dialogue commitment store new illocutions issued checked commitment store accepted certain existing commitments active inactive Such mechanism easily built electronic institution automated checking 101130 4 The persuasive negotiation strategy The protocol described previous section structures interactions agents allows understand messages exchanged commitments negotiating However protocols indication content offers rewards agents need devise order reach good agreements indicate send offers rewards determine agents strategy Therefore complement protocol important devise mechanisms generate evaluate offers rewards committed enact In particular respect following requirements 21 1 Techniques exist generating proposals providing supporting argumentsthis demands agents endowed strategies generate offers Here assume prior information oppo nent knowledge conﬂict preferences domain discourse SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 813 models area 1315 In situations heuristicbased approach proven track record eliciting good outcomes approach adopted Generally mechanisms assume knowledge opponent decide offers counteroffers according behaviour opponent behaviour dependent tactics deadline agent timedependent tactics resources available resourcedependent tactics 12 In section later ones develop heuristic tailored problem repeated negotiations 2 Techniques exist assessing proposals associated supporting argumentsthis means agents need able evaluate beneﬁt proposals rewards This normally captured evaluating incoming offers agents preference structure utility function However repeated encounters agents know outcome future games priori exists uncertainty outcomes This uncertainty needs taken account decision making agents prior games Currently negotiation technique deals strategies speciﬁcally tailored repeated encounters aim use persuasive negotiation order reduce uncertainty future outcomes use rewards 3 Techniques exist responding proposals associated supporting argumentshere heuristicbased models shown provide good responses offers counteroffers In particular special attention heuristicbased models try achieve Paretoefﬁciency bargaining encounter models shown time come better agreements overall 13 In aim develop bargaining mechanism seeks efﬁcient partitioning resources In general persuasive negotiation agents means inﬂuencing future negotiations wards exchanging offers counteroffers impact outcome present encounter Given negotiation normally occurs partitioning resource rewards case aim strain partition imposing bounds agreements achieved future negotiations Thus promises rewards asked given partially determine partitioning resources negotiated later time example Section 1 To end section develop Reward Generation Algorithm RGA generates rewards based offers calculated techniques resource behaviourbased tactics Moreover Section 5 develop speciﬁc persuasive negotiation strategy builds RGA generate offers rewards From section onwards focus speciﬁc features repeated negotiation games described Section 2 abuse notation slightly denote set outcomes ﬁrst game O1 second O2 On general case Thus speciﬁc setting consider proposal p reward q speciﬁed persuasive illocutions rewardα β p q askrewardα β p q p O1 q O210 In represented reward Section 3 example q O reward given α translated set constraints operators cid3 cid2 α abide contract O2 O2 Similarly normal negotiation illocutions proposeα β p acceptα β p consider offers ﬁrst game p O1 Given ﬁrst discuss rewards justiﬁably persuade opponent rewards generated combining different components RGA Finally devise evaluation functions assess utility obtained rewards offers support In agents decide counteroffer accept given offer 41 When use rewards In PN agents try rewards ask rewards order opponent accept particular offer Rewards giving higher utility outcome opponent given higher utility agent asking second game Given rewards speciﬁed second game terms range values issue Thus giving reward equates specifying range vx 05 issue x O2 O2 agent utility increases increasing values x Conversely asking reward means specifying range vx 04 O2 asking agent utility increases decreasing values x Now agents ﬁnd advantageous 10 Here consider rewards illocutions suggested Section 3 easily implemented extending proposed solution Such extension considered future work 814 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 Fig 1 Determining outcome second game according offer ﬁrst game accept rewards costs counteroffer discount factor risk passing deadline opponents Here deal issues related agents promises tackle uncertainty underlying simply assume focus reasoning mechanism agents require order negotiate rewards In reward given asked following contexts A reward proposed agent manage achieve target L reaching agreement giving reward This happen agent α asking β concede ﬁrst game giving α utility ﬁrst game Agent α afford forsake utility second game values discounting effects It conceding second game acts reward Note reward cost sender needs estimate cost reward respect Lα properly committing giving reward A reward asked agent able concede ﬁrst game catch second In case agent asking reward costs conceding ﬁrst game entices opponent pledge return concession second game concession ﬁrst game The agent asking reward needs ask reward commensurate target level concession making The reasoning captured Fig 1 As seen given contract O1 offered α reward α β propose negotiation range favourable β offers high utility β second game The agreement reached ﬁrst game higher utility α The converse applies agent α asks β reward These procedures seen tradeoff mechanism negotiation agents tradeoff gains present future return gains future present 33 In general main ways agents stand gain rewards manner 1 Agents able reach agreement faster ﬁrst game providing guarantees outcome second game For example α speciﬁes negotiate pie second game β prefer accept offer instead delaying negotiation result ﬁrst second pie worth signiﬁcant target Lβ This turn reduces negotiation time discounted outcome ﬁrst second games 2 The negotiation mechanism efﬁcient allows agents explore larger negotiation space different preferences This happen particularly α lower discount factor β For example β tradeoff second pie opponent values higher proﬁts ﬁrst game 3 Agents able reach agreement faster second game negotiation space left searched reward given asked For example α β negotiate pie second game pie SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 815 Require O1 O1 L 1 Compute concessions Oα 1 Oβ 1 Here agent determines agents concede issues higher lower δU opponent 2 Select O2 O2 matches level concession O1 3 Check combination O1 O2 satisﬁes L adjust vmin vmax second game according values O2 send offer reward 42 The reward generation algorithm Algorithm 1 Main steps RGA Building reasoning mechanism presented Section 41 develop reward generation algorithm RGA Its role determine level concession ﬁrst game set value corre sponding reward decide send First assume agent means generating offers O1 comply negotiation ranges issue These generated termed negotiation tactic 12 In line work negotiating presence deadlines assume agents negotiation tactic concedes extent agreement reached deadline passed Then step negotiation based concessions offer O1 O1 RGA computes reward O2 O2 decides asked given In Algorithm 1 outlines main steps RGA detailed following subsections 421 Step 1 Compute concession degrees In context degree agent concedes game equivalent value loses issues 1 O1 value min negotiation range define opponent relative opponent loses issues Assuming x vx issue x vx max vx cid5 cid4 vx 1 cid4 cid4 cid6 vx vx max Ux Ux max min cid4 cid5 cid4 cid6 vx vx min Ux Ux max min U x 1 U x U x Ux max cid5cid7 cid5cid7 min cid5 From compute maximum agent cid2 Umax wxU x max xXO1 minimum Umin cid2 wxU x min xXO1 actual utility cid2 U1 wxU x 1 xXO1 cid3 wx 1 These weights ascribed values given wx αs relative weight issue x weight issue utility function Eq 1 normalised number issues considered Then concession degree offer O1 computed conO1 Umax U1 Umax Umin 2 It possible calculate concessions issues higher lower δU α conαOα 1 1 conαOβ respectively Then complement conαOα β concedes α αs perspective α exploits β 1 1 conαOα 1 conαOβ 1 conαOβ 1 1 represents 816 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 422 Step 2 Determine rewards To determine agent concedes game given play MMPD α needs compare degree concession issues higher δU β Oβ 1 zero sum game calculated issues This means determining different conditions conαOβ compared concession 1 conαOα β perceived α To end deﬁne conditions refer case α concedes β COOP cooperates concedes β CONC concedes concedes β EXPL exploits respectively follows 1 lower δU β Oα COOP true conαOα CONC true conαOα EXPL true conαOα 1 conαOβ 1 conαOβ 1 conαOβ 1 1 α grounds ask reward 1 1 α ask reward 1 1 α reward The conditions capture fact agent ask reward conceding ﬁrst game exploiting ﬁrst game It possible envisage variations rules agents want reward opponent exploiting ﬁrst game want ask conceding However behaviours modelled complex strategies consider future work But agent risk failed negotiation Here focus basic rules ensure agents try maximise chances reaching proﬁtable outcome Now having determined argument sent reward asked given determine value reward Given agent α aims achieve target Lα value chosen 1 conαOβ reward depend L conαOα 1 degrees concession agent We consider points turn ignore agent identiﬁer clear context Given O1 ﬁrst game standing offer minimum utility α needs second game l2 L U O1 We need consider following cases remember ecid7θt maximum obtained second game discounts 1 If l2 cid3 ecid7θτ t possible α reach target second game provided agents reach agreement ﬁrst ask rewards The larger l2 likely rewards given conceded second game achieve L Note τ added discounting effect denote agent time send illocution 2 If l2 ecid7θτ t possible reward agent ask attempt achieve value close possible l2 l2 For assuming know l2 cid3 ecid7θτ t possible determine necessary adjust negotiation ranges issues O2 order achieve l2 Speciﬁcally agent calculates undiscounted ecid7θτ t needs second game Then needs decide going adjust utility minimum utility needs issue equivalent bound vout issue order achieve ecid7θτ t Here choose distribute utility obtained evenly issues Other approaches involve assigning higher vout higher utility issues higher weight utility function In vout constrain agents ranges issues negotiation ranges overlap opponent result possible agreement Our approach tries reduce risk Thus required outcome vout issue second game computed cid9 cid8 l2 3 vout U 1 x l2 ecid7θτ t Having computed constraint vout agent needs determine reward ask To end agent computes contract O satisﬁes following properties cid4 Oβ 2 conα cid4 Oα 2 cid4 Oα 1 conα conα conα Oβ 1 cid4 cid5 cid5 cid5 cid5 This equivalent heuristic described Section 41 level concession exploitation offer 1 mapped reward asked given second O2 ﬁrst game O1 Oα 1 Oβ SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 817 Oβ Oα 2 Then assuming linear utility functions ﬁnite domains values issues procedure 2 equivalent reﬂecting level concession issues higher δU α higher δU β This inverting Eq 2 given known Umax Umin deﬁned step 1 ﬁnding vx 1 assigning U x 1 issue procedure linear time respect number issues considered 1 Let assume issue x results bound vr maximum minimum according type argument sent Thus O2 α obtains bounds issues rewards ask β Given consider send reward based vr vout compare U1 inverting U x 423 Step 3 Decide send offers rewards Assume α prefers high values x β prefers low ones determined reward offered procedure asking reward broadly similar highlight differences necessary Now α determine reward actually given value according following constraints 1 vr cid2 vout α promise reward deﬁning upper bound vr second game implying α ask vr This target vout vr α negotiate revised upper bound vcid17 vout When asking reward α ask lower bound vr max vcid17 vr negotiate normal upper bound vmax order achieve utility target vr lower bound vcid17 min min 2 vr vout α achieve target offers reward commensurate asks β concede ﬁrst game In case α revises negotiation ranges vcid17 vout vmax remaining Thus agent send reward simply modiﬁes negotiation ranges Now supposed ask reward α achieve target deserved reward However ask β reward vr lower bound privately bound future negotiation vcid17 vout keeping upper bound vmax In tries gain utility possible11 min min Now coming case l2 ecid7θτ t implying vout vr agent intends ask reward able constrain negotiation range achieve target point 2 In cases negotiation range modiﬁed reward asked CONC true Given ﬁnal conditions summarise rules dictate particular illocutions negotiation ranges adjusted assuming offer O1 calculated O2 represents associated reward shown Algorithm 2 With place section describes recipient illocutions reasons contents 43 Evaluating offers rewards Having discussed agents generate rewards agent evaluates offers rewards receives Generally agents negotiate standard alternating offers protocol proponent accepts offer opponent offer proponent forward lower discounted time utility offer presented opponent This expressed Rule 1 However agents persuasive negotiation evaluate incoming offer reward asked given From previous section generally infer reward implies value vr deﬁnes lower upper bound given issue negotiation game For example reward given seller guaranteed discount lower limit price purchase current buyer reward requested buyer Therefore given bound agent infer outcome given issue lie vcid17 min vcid17 equivalent different agents normal negotiation ranges vmin vmax account agents target vout given target l2 value vr max 11 The difference constraint applied reward target reward applies constraint agents applies separately agent according individual targets 818 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 COOP EXPL vout vr x XO2 proposeα β O1 end CONC l2 cid3 ecid7θ τ t askrewardα β O1 O2 modify vmin vmax second game end CONC l2 ecid7θ τ t askrewardα β O1 O2 end EXPL vout cid3 vr x XO2 rewardα β O1 O2 modify vmin vmax second game end Algorithm 2 Step 3 RGA U Onext ecid7β τ t cid3 U Ogiven e α Onext βs possible offer cid7β t U Ogiven offer given acceptβ α Ogiven end Rule 1 Accepting offer usual case max min vcid17 Generally assume given negotiation range vcid17 agent able deﬁne expected outcome range probability distribution uniform normal gamma reasoning based negotiation strategy conciliatory strategy expect lower utility gain second game compared nonconciliatory faced nonconciliatory opponent This probability distribution estimated previous interactions opponent knowing behaviour opponents bargaining strategy relationship agents bargaining position 1733 Given expected outcome issue agent calculate expected utility determined according bounds set reward reward utility offer tagged Moreover procedure calculate expected utility reward offer want send By comparing sets utilities decision accept counteroffer step We procedure follows Assume β agent recipient reward given asked β prefers small values 2 issue x considered Then let βs negotiable range vmin vmax issue x βs target lβ second game implies needs vβ issue second game Now β receives rewardα β O Oa askrewardα β O Ocid17 r upper bound proposed α issue x Oa vα In meantime β calculated offer Onew reward Ob bound vβ meaning Oa reward second game Oa implies vα r lower bound Ocid17 r given issue x Ob Then issue x β calculates negotiable ranges second game given vα r asked calculates vβ vmin vα r We assume r β calculate probabilistic technique expected outcome range evα vα vmax Given expected outcomes issue overall expected outcomes EOa EOb second game calculated type reward respectively cid2 r minvout vmax case Ocid17 r minvout vmax Ocid17 vmax given vβ x vmin vα r evβ r minvβ r minvβ x vβ vα U EOa wx U cid5 cid4 evα x U EOb xXEOa cid2 xXEOb wx U cid5 cid4 evβ x 4 5 EOa expected outcome reward given α EOb expected outcome reward given β cid3 wx 1 wx weight given issue utility function Eq 1 These weights second game different evaluating offers ﬁrst game known advance SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 819 U Onew e cid7β τ t U EOb e cid7β θ τ t cid3 U O e cid7β t U EOa e cid7β θ t acceptβ α O rewardβ α Onew Ob askrewardβ α Onew Ob end Rule 2 Evaluating received reward ask reward U Ocid17 new e acceptβ α O cid7β τ t U EOcid17 b e cid7β θ τ t cid3 U O e cid7β t U EOa e cid7β θ t proposeβ α Ocid17 new end Rule 3 Evaluating received reward send normal offer agent compute value expected outcomes second game future weights order consistent Given expected outcomes calculated agent decides accept counteroffer Rule 2 This evaluates offer generated offer received decide accept offer received send reward illocution note addition discount factors reﬂect time till game sending counteroffer Note principle applies agent send askreward instead Finally consider case agent β received persuasive offer reply offer argument In case β calculates expected outcome second game constraints negotiation range vmin vmax elicit EOcid17 b Rule 3 compares utility offer received utility offer generated outcome expected game decide propose accept Note second game left uncertain case bounds changed reward This means agent guarantee meet target result agents taking time reach agreement second game case nonpersuasive tactics Section 6 As seen section generation rewards evaluation offers assume offer based rewards computed Given section discuss remove assumption developing novel tactic uses RGA generate offers rewards 5 The rewardbased tactic As described previous section RGA requires offer generated negotiation tactic order generate accompanying reward In vein common heuristicbased tactics classiﬁed behaviourbased BBusing form titfortat ii timebasedusing Boulware BW concedes little beginning conceding signiﬁcantly deadline Conceder CO starts high concession concedes little deadline 1212 Now tactics engage positional bargaining 17 starting high utility offer proponent α gradually conceding lower utility ones In turn procedure automatically causes RGA start promising rewards gradually asking rewards This tactics generate offers exploitative beginning negotiation As agent gradually concedes initial offer negotiation reward generation mechanism ask rewards instead Thus possible tactics ask rewards beginning negotiation This signiﬁcantly reduce efﬁciency terms sum utilities agents negotiation encounter agents better conceding second game low discount factor cid7 return exploit ﬁrst game discussed earlier Section 1 This mean patient agent lower 12 Other negotiation tactics resourcebased dependent factors The tactics select chosen common studied literature 1233 820 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 discount factor cid7 ask reward second game agent offer reward second game To ground work present novel rewardbased tactic RBT based Faratins tradeoff tactic 13 asks gives reward point negotiation order reach agreement To agent needs know evaluate incoming offers generate counteroffers accordingly We consider main cases calculating best response offer reward These 1 An offer reward received possible counteroffer reward 2 It possible counteroffer reward offer involved rewards 3 It possible counteroffer reward offer involve rewards We algorithm deals cases turn 51 Case 1 Counteroffering reward In case offer reward received possible counteroffer reward according RGA Thus agent α needs calculate combinations rewards asked given offers choose combination deems appropriate send β To calculate combinations α ﬁrst needs determine overall utility combination To achieve use hill climbing method similar Faratin et als 13 model In method agent tries ﬁnd offer believes favourable opponent necessarily conceding In case particularly utility functions based MMPD procedure equates agent trying gain utility issues higher δU lower δU β13 In strategy tries maximise joint gains repeated negotiation encounter Therefore calculate best combination offer reward agent α send hillclimbing approach α ﬁrst calculates utility offer intends send ﬁnds offer reward optimally match utility value By optimality case mean offer reward favourable β Thus utility offer calculated according difference exists αs previous offer sent β step utility α wishes previous offer The size utility step arbitrarily set Given step size f 1 utility step calculated function Su O1 O2 O1 O2 1 0 2 follows SuO1 O2 O cid17 1 O cid17 2 f ecid7t U O1e2cid7τ U EO2ecid7θ2τ U Ocid17 1ecid7τ U EOcid17 2ecid7θτ f 6 2 current offer expected outcome βs reward Ocid17 O1 EO2 αs previous offer expected outcome second game αs reward O2 respectively 1 EOcid17 Ocid17 2 respectively In case Su returns zero negative value α accept offer reward applying evaluation rules deﬁned Section 43 When reward speciﬁed agents utility calculated function considers offers agent remove U EOcid17 U EOcid17 2 calculation Given utility step Su possible calculate utility Nu combination offer reward following equation Nu U O1e cid72τ t U EO2e cid7θ2τ t SuO1 O2 O cid17 1 O cid17 2 f 7 Given rewards specify bounds negotiation second game combination offered step represents space possible agreements second game given offer ﬁrst Therefore ﬁnding combination closely matches opponents offer reward equates ﬁnding space offers close opponents space covers latest offer reward This procedure pictured Fig 2 13 Note different point discussed Section 422 constrain negotiation ranges search offers proﬁtable parties SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 821 Fig 2 The hill climbing performed RBT agent α ﬁnd appropriate reward offer response offer reward agent β The shaded semi circles represent spaces different offers rewards utility α Each new offer α closer agent βs previous offer Given previously received proposed offers rewards ﬁnd O1 O2 maximise conαOα maximise conαOβ 2 reward β 2 ask reward β subject U αO1 O2 Nu x v O1 O2 vmin cid3 v cid3 vmax values need negotiation range Optimisation Model 1 Computing best counteroffer reward As seen ﬁgure tactic α calculates favourable combination offer reward agent β achieves utility Nu In tactic aims offers closest preferred β steps losing utility In calculating reward given account fact MMPD opponent likes issues maximising opponents gain issues ensure reward attractive opponent In way reward asked associated offer calculated values issues offer favourable opponent issues prefers according MMPD To calculate offers rewards solve problem deﬁned Optimisation Model 1 Linear Programming techniques order calculate reward favourable β α Algorithm 1 runs RGA ﬁnd best possible rewards associated offers combined utility equal Nu However Algorithm 1 fail ﬁnd optimal output result constraints strong target L high optimiser able ﬁnd solution speciﬁed number steps cases resort procedure described case 2 52 Case 2 Counteroffering rewards given previous rewards In case agent ﬁnd combination offer reward utility matches Nu Therefore agent calculates offer standard heuristicbased tactics outlined beginning section In case BB tactics appropriate generate offer given previous offers opponent offers associated rewards This means offers BB calculate offer exactly depict concessions agent leading BB tactics misunderstanding behaviour opponent This turn lead offer BB agent concedes Therefore BW CO generate offer independent previous offers opponent 822 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 Given previously received proposed offers ﬁnd O1 maximise conαOα 1 maximise αs concessions issues β high δU subject U αO1 Nu x v O1 vmin cid3 v cid3 vmax values need negotiation range Optimisation Model 2 Computing best counteroffer 53 Case 3 Counteroffering rewards given previous rewards In event β proposes offer rewards tactic needs able respond similar procedure case 1 order continue stepwise search agreement In case tactic calculates offer utility equal Nu U EOcid17 2 Eq 7 Moreover offer calculated similar offer β This achieved solving problem deﬁned Optimisation Model 2 This calculates offer O1 O1 maximises level concession opponent likes previous case achieving Nu In case issues negotiated qualitative nature similarity based algorithm 13 6 Experimental evaluation In section series experiments aim evaluate effectiveness efﬁciency PN reasoning mechanism To end pitch agents RGA RBT number nonpersuasive negotiation tactics standard benchmark metrics We ﬁrst experimental settings types agents benchmark algorithm metrics tests Given provide results tests analyse performance RBT different parameter settings 61 Experimental settings The scenario consider involves agents playing negotiation games rules discussed Section 2 The general settings apply negotiation games follows The pair negotiating agents utility functions shaped MMPD discussed Appendix A The actual utility opponent obtains particular values issues known utilities private Thus agents α β negotiate 4 issues x1 x4 x1 x2 price bandwidth valued α β x3 x4 usage service time payment valued β α The agents utility functions U α U β speciﬁed issue Table 2 As noted weights gradients utility functions chosen respect conditions MMPD detailed Appendix A The maximum time negotiation game place tmax set 2 seconds allows 300 dead illocutions exchanged agents14 Unless stated agents deadlines t α t β dead deﬁned according uniform distribution 0 2 seconds cid7α cid7β discount factors set value 0 1 drawn uniform distribution stated Lα Lβ targets agents drawn uniform distribution 0 2 stated θ τ θ set 05 seconds meaning second game discounted e05cid7 agent τ set 00001 meaning utility offer discounted e00001cid7 simulate instantaneous replies stated 14 Experiments run MATLAB 71 2 GHz Intel PC 1 GB RAM Preliminary experiments negotiation tactics suggest agents come agreement time period achieve agreement maximum negotiation time extended SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 823 Table 2 Utility functions weights issues agent Agent α β Utility function weight issue Ux1 wx1 04x1 05 1 02x1 04 Ux2 wx2 09x2 02 1 06x2 01 Ux3 wx3 1 02x1 02 09x2 03 Ux4 wx4 1 06x2 01 04x1 02 vmin vmaxthe negotiation range issue agent deﬁned privately known λ degree alignment negotiation ranges For example λ 1 negotiation ranges overlap completely degree alignment 0 negotiation ranges overlap The degree alignment arbitrarily set 08 represent fact agents reasonably large set possible agreements reach achieve target We assume ﬁrst offer agent makes negotiation selected random having high utility agent Also ﬁrst agent start negotiation chosen random This random choice reduces possible ﬁrstmover advantage strategy loses utility discount factors Moreover order calculate expected outcome second game discussed Section 43 agents draw outcome issue normal distribution mean centred middle agents negotiation range second game variance equal 05 Finally experiments use ANOVA ANalysis Of VAriance test statistical signiﬁcance results obtained 62 Populations negotiating agents In order benchmark RBT standard negotiation tactics create groups agents First create agents use RBT negotiate ﬁrst game These agents use standard tactics discussed Section 5 second game Second create group agents called PNT Persuasive Negotiation Tactics use RGA rewards They generating offers standard tactics BB BW CO deﬁned Section 5 plug offers RGA obtained compatible rewards In second game PNT agents simply use standard tactics generate offers Third create group agents called NT Negotiation Tactics use standard negotiation tactics generate offers games 12 implement standard tactics In following experiments use homogeneous populations 80 agents NT PNT RBT create heterogeneous population equal numbers RBT PNT agents 40 refer PNTRBT study RBT PNT agents perform 63 Efﬁciency metrics As argued Section 1 goals PN achieve better agreements faster standard negotiation mech anisms To test PN model achieves use following metrics Average number offersthis average number offers agents need exchange coming agreement To calculate record number offers time agreement reached calculate average total number negotiations Note time offer short time τ elapses A lower average equates shorter time agents come agreement mutatis mutandis average high Moreover lower average lower loss utility result discount factors cid7 Thus deﬁne timeefﬁcient tactic takes relatively small number offers reach agreement Success ratethis ratio agreements reached pairs games number times agents meet negotiate The larger success rate better negotiation tactic ﬁnding attractive offer opponent Average utility agreementthis sum utilities negotiating agents agreements divided number agreements reached The higher value better strategy ﬁnding 824 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 outcome brings high utility participating agents Thus deﬁne socially efﬁcient negotiation tactic brings high sum utility outcome Expected utilitythis equal average utility weighted probability agreement reached The probability calculated dividing total number agreements number encounters agents Thus agents ﬁnd agreement encounters probability 1 come agreement future encounter A strategy high expected utility likely reach high utility agreements time meets strategies Having deﬁned evaluation metrics results experiments 64 Comparing persuasive nonpersuasive strategies When agents play negotiation games ﬁrst NT reward generation mechanism able offers evaluate offers PNT able generate evaluate offers rewards Given persuasive strategies like PNT RBT constrain rewards according target L shown Section 422 need allow nonpersuasive tactics constrain ranges accordingly ensure fair comparison Thus allow tactics constrain ranges issues second game according target reach agreements use rewards propose illocution The procedure similar described Section 422 vout calculated Eq 3 bound negotiation range second game use rewards Given postulate number hypotheses performance RGA RBT results validate Hypothesis 1 Negotiation tactics use RGA time efﬁcient dead dead t β This hypothesis follows fact expect rewards help agents ﬁnd agreement faster We impose 1 s cid7α cid7β 01 θ 1 s λ 08 following basic settings interactions Lα Lβ 08 t α These settings chosen represent symmetric conditions agents impose relatively constraints negotiation games agents play The symmetric nature interaction ensures tactic advantageous position opponent Here recorded average number offers lower number time efﬁcient agents agent makes order reach agreement For populations tactics agent meets agent 50 times repeated 15 times results averaged We recorded results Table 3 Thus NT takes average 547 offers reach agreement PNT agents 58 combined PNT RBT population takes 56 offers agreement The performance RBT agents signiﬁcantly better populations reach agreements 26 offers NT factor 2115 These results validate Hypothesis 1 Now reason superior performance persuasive tactics general rewards offers attractive expected shrinkage negotiation ranges second game following application rewards reduces negotiation space searched agreement The additional improvement RBT attributed fact RBT agent calculates rewards offers hillclimbing algorithm utility opponents issues higher marginal utility explained Section 5 Hence faster PNTRBT party RBT performs hillclimbing These results suggest outcomes RBT PNT populations discounted reach agreements time reach agreement agents deadlines However clear utility agreements reached signiﬁcantly higher NT agents This leads following hypothesis Hypothesis 2 Negotiation tactics use RGA achieve higher success rate expected utility average utility 15 Using ANOVA sample size 15 population α 005 F 2210 Fcrit p 8 10 results statistically signiﬁcant difference means distribution 74 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 825 Table 3 Benchmark results Tactic No offers Success rate Average utility Expected utility RBT PNTRBT PNT NT 26 56 58 547 10 10 099 087 202 195 19 184 202 195 188 16 To test hypothesis run experiments previous case record average utility agreement number agreements reached Thus possible calculate expected utility average utility encounter success rate game explained earlier These recorded Table 3 Thus success rate persuasive strategies generally higher NT strategies 087en counter nonpersuasive strategies 099encounter PNT strategies 10encounter RBT PNT 10encounter RBT only16 This result clearly shows use RGA increases probability reaching agreement The similar performance RBT PNTRBT difference PNTRBT PNT shows RBT agents able ﬁnd agreements readily similar counterparts able persuade PNT agents attractive offers This conﬁrmed fact average utility persuasive strategies generally higher 19encounter PNT 195encounter PNTRBT 202encounter RBT NT 184encounter Note difference utility NT tactics greater discount factors cid7α cid7β bigger given high average number offers NT uses 547 Given trend success rate average utility expected utility follows similar trend NT agents obtaining 16encounter PNT 188encounter RBT PNT 195encounter 202encounter RBT agents only17 Generally speaking results conclude RGA basic tactics allows agents reach better agreements faster These results suggest PNT agents reach broadly similar agreements terms utility NT agents discount fact rewards signiﬁcantly reduce time reach agreements increase probability reaching agreement Now discussed Section 5 PNT agents usually generate offers ﬁrst starting high utility ones NT agents calculate rewards accordingly Given agents tend start giving rewards end asking rewards As negotiation proceeds offers accepted offers generally converge point agents concede nearly equally issues irrespective marginal utilities agents rewards converge similar point This turn results lower overall utility games agent exploits game turn Now rewards selected intelligent fashion RBT agents reach higher overall utility general This agents exploit issues higher marginal utility opponent This demonstrated results RBT agents suggest reach agreements high utility participating agents It noticed performance mixed populations RBT PNT agents perform RBT agents slightly better pure PNT population results This suggests RBT agents ﬁnd agreements convince PNT opponent quickly able propose better rewards offers PNT agents However apparent RBT agents able avoid exploited PNT counterparts agreements RBT tries favourable PNT agents described Section 5 Given postulate following hypothesis Hypothesis 3 Agents RBT able avoid exploitation standard tactics connected RGA PNT 16 Using ANOVA sample size 15 population PNT PNT RBT PNT α 005 F 88 4 These results conﬁrm signiﬁcant difference means PNT strategies Fcrit 315 p 441 10 The success rate NT agents lower populations elements sample 17 These results validated statistically ANOVA F 3971 Fcrit 273 p 736 10 size 15 population α 005 These results mean signiﬁcant difference means populations 80 sample 826 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 In order determine tactic exploited recorded PNTs RBTs average utility separately18 Thus average RBT PNT agents obtained average utility agreement 096encounter This result validates hypothesis suggests hillclimbing mechanism RBT agents calculates offers convince opponent reducing utility RBT PNT agents signiﬁcantly small steps maximises joint gains Algorithm 1 In general experiments empirically demonstrated usefulness rewards bar gaining Thus achieved initial aim PN enable agents achieve better agreements faster In following section study RBT affected different conditions environment understand important factors affect efﬁciency persuasive negotiation strategy 65 Evaluating reward based tactic In section explore properties RBT studying behaviour key attributes agents varied As deduced Section 4 large number attributes affect behaviour RBT focus following main ones believe signiﬁcant impact reward generation component behaviour RBT These attributes 1 Lthe target determines size reward given asked determined vout Eq 3 procedure described Section 422 Given varying L allows study effectiveness PN general possibility asking giving reward changes Moreover aim study effect agent having lower higher target opponent outcomes negotiations 2 cid7the discount factor dictates utility offers rewards In particular aim RBT reward generation mechanism help agents different discount factors ﬁnd good agreements 3 θ delay second game played determines value reward Increasing value signiﬁcantly reduce value reward agent By varying θ aim impacts use rewards negotiation affects outcome game In experiments compute 95 conﬁdence interval result plot error bars appropriate graphs order statistical signiﬁcance results19 First investigate impact negotiation target L outcome negotiations In context L decide reward sent negotiation ranges agent second game Section 422 The higher value L agents likely able construct rewards This agent shrink negotiation range second game order achieve higher L games Therefore expect agents achieve fewer deals corresponding lower overall expected utility Moreover case agent high L opponents rewards likely accepted rewards likely allow agent achieve target agents likely come agreements offers come agreement In case expect agent higher L negotiate strongly constrain second game higher utility opponent To investigate intuitions consider pair agents α β use RBT postulate following experimental hypothesis Hypothesis 4 The higher value Lα relative Lβ higher average utility α compared β To test Hypothesis 4 ran experiment agents negotiate similar settings previous section fact Lα varied 0 15 Lβ kept ﬁxed 05 The results experiment shown Fig 3 18 We validated result ANOVA sample size 15 strategy α 005 Thus null hypothesis equal means samples validated F 013 Fcrit 410 p 071 005 19 If error bars overlap points indicates signiﬁcant difference points Otherwise signiﬁcant difference 95 conﬁdence level SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 827 b Fig 3 Expected utility average number offers average utility agents Lα varied Expected utility α β Lβ 05 Lα varied b Average number offers α β Lα varied c Average utility α β Lβ 05 Lα varied c As seen Fig 3a overall expected utility agents rises sharply Lα 07 sharp rise number offers exchanged agents Fig 3b Moreover success rate agents drop The main cause jump expected utility rise average number offers explained results shown Fig 3c As seen Lα 07 αs utility gradually rises βs utility sharply falls This means α exploits β issues negotiated In order obtain Lα 07 α need exploit β ﬁrst game issues prefers β exploit β issues likes β second game This deduced weights utility functions shown Table 2 Therefore point α β likely exploit maximally issues prefer game This results high point utility represents cooperatecooperate point MMPD peak Fig 3a When Lα 07 agents ﬁnd agreements completely exploiting opponent issue agree proposals rewards result lower overall utility outcome lies away cooperatecooperate point MMPD Beyond Lα 07 harder α ask rewards This Lα increases use rewards decreases αs ability concede game decreases needs achieve high target α constrain negotiation ranges second game trying achieve target discussed Section 423 However given Lβ 05 Lα β afford exploited α manage reach target games Hence success rate agents decrease However given stringent demands α agents likely exchange larger number offers β conceding signiﬁcant number times agreement reached 828 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 In general results validate Hypothesis 4 conﬁrm intuition αs bargaining power increase respect target Given results expected second game discounted α started exploiting β higher value 075 We explore discounting effects negotiation investigate effect increasing agents targets time general behaviour discounts targets varied Before study effect discount factor cid7α outcome negotiation keeping cid7β 05 In case low value cid7α equates low discounting effect outcome games conversely high value cid7α Therefore expect cid7α gets higher agreements reached games discounted result lower overall expected utility Moreover higher cid7 values agents ﬁnd harder achieve target L value offers counteroffers rewards Agents likely offers reach agreement reach fewer agreements In case cid7α varied expect agent higher discount factor likely accept offer opponent counteroffering time discounts offer offered opponent This means patient agent likely offers easily accepted fewer numbers offers average exploit opponent Hence predicted game theoretic models bargaining 25 patient agent gets increasingly higher average utility patient opponent difference discount factors increases We postulate following hypothesis Hypothesis 5 The higher value cid7α relative cid7β agents likely reach agreements offers reach agreement To test hypothesis ran similar experiment apart fact kept target agents Lα Lβ 05 varied cid7α 0 4 keeping cid7β 05 In context obvious overall expected utility agents decrease cid7α increases utility α gets decreases result discounting effect Given recorded average utility agent number offers reach agreement The results shown Fig 4 As seen Fig 4a βs utility slightly decreases cid7α rises The number offers agents rises signiﬁcantly cid7α increases 144 This cid7α 144 discounting second game worth 05 assuming α exploits issues second game Thus impossible α ask rewards rely giving rewards Moreover discounting effect increases harder β convince α Eventually time passes agents rely offers α constrains negotiation ranges game achieve target Given negotiations time second game previous experiment Therefore target slightly reduces advantage βs patience having lower discount factor type game It success rate b Fig 4 Average utility average number offers cid7α varied Average utility α β cid7α varied b Average number offers α β cid7α varied SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 829 b Fig 5 Varying target discount factor α β resulting expected utility number agreements reached Expected utility b Success rate agents signiﬁcantly decrease 1 0999 cid7α 144 This suggests agents run time trying convince This happen poor agreement reached ﬁrst game α constrains negotiation ranges second game agreement possible These results validate Hypothesis 5 Given results expect combined effect increasing target increasing discount factor signiﬁcantly reduce expected utility agents increase number offers need come agreement We postulate following hypotheses Hypothesis 6 The higher value Lα Lβ lower expected utility agents Hypothesis 7 The higher value cid7α cid7β agents likely reach agreements offers reach agreement Therefore varied agents discount factors targets stronger effect negotiation outcomes The plot expected utility success rate shown Fig 5 As seen Fig 5a expected utility signiﬁcantly affected Lα Lβ 20 The results conﬁrm Hypotheses 6 7 A jump utility experiment Hypothesis 4 noticed particular values agents target corresponding points agents need try exploit maximally constrain negotiation ranges second game achieve However certain point agents able exploit maximally use rewards achieve target This results decrease number agreements reached shown Fig 5b Moreover notice point expected utility drops relative target values decreases cid7 This conﬁrms initial intuition discount factor inﬂuences extent effect target expected utility We recorded average number offers agents impact target discount factors The results shown Fig 6 As seen drop expected utility reﬂected jump number offers The region peak occurs corresponds values targets discount factors agents able use rewards persuade signiﬁcantly shrink negotiation ranges second game reach target Beyond peak higher values targets particular agents ﬁnd agreements ﬁrst game according hill climbing mechanism RBT guarantees meet number steps Note plateau low values L lower value high values L suggesting rewards signiﬁcantly reduce number offers reach agreement compared offers hill climbing method 20 Note jumps success rate 1 similarly jumps expected utility 2 curve ﬁtting actual results 830 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 Fig 6 Impact L cid7 average number offers b Fig 7 Impact offers rewards varying θ Average number offers encounter θ increased b Percentage agreements rewards θ varied Finally given higher values cid7 decrease probability agents reach agreement increase number offers exchanged expect similar effect higher values delay This longer delay decreases value rewards agents reduces probability reaching agents target L Therefore expect longer delay θ lower success rate agents higher average number offers needed reach agreement Given postulate following hypothesis Hypothesis 8 The higher value θ likely agents use rewards offers reach agreement As hypotheses ran similar experiment keeping Lα Lβ 05 cid7α cid7β 05 varied θ 0 5 seconds recorded expected utility agents The success rate agents decrease signiﬁcantly number offers signiﬁcantly increased θ increased 3 seconds shown Fig 7a These results conﬁrm Hypothesis 8 The reason jump number offers θ 3 similar explanation previous experiment cid7α 144 Indeed θ 3 total value second game decreases 05 decreases value rewards given asked This results agents able offers rewards increase constraints second negotiation turn increases number offers needed reach agreement To conﬁrm results recorded number agreements reached use rewards As shown Fig 7b number agreements reached use rewards increases θ increases SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 831 7 Related work In paper dealt repeated negotiations PN We previously presented preliminary version PN strategy 36 In paper elaborated protocol discuss evaluation functions thoroughly evaluate associated reasoning mechanism In following subsections survey main work carried areas distinguish 71 Repeated negotiations Repeated negotiations repeated games long studied game theory 26 In particular closest work area Muthoo 2425 analysed equilibrium offers arise agents bargain repeatedly number issues In similar vein Busch Hortsmann 9 analysed equilibrium offers arise agents need decide negotiate terms long term relationship settle agreement incrementally different points time Their results imply better cases short term deals long term ones imply lower negotiation costs In case heuristics employ RGA follow similar line thought outcome second game completely negotiated ﬁrst This turn reduces time come agreement agents lose signiﬁcant utility discounting effects In multiagent agent systems area repeated negotiations considered terms repeated sequential auctions 81416 These works looked equilibrium strategies agents use auctions settings complete information Our work differs game theoretic approaches general look decentralised bargaining interaction agents knowledge opponent need ﬁnd best agreements possible This believe realistic situation requires turn heuristic methods empirical evaluations analytical solutions proofs 72 Persuasive negotiation A number approaches PN considered aspects problem years seminal work Sycara 4042 challenges identiﬁed Tohmé 45 Jennings et al 21 First note work language domain content rewards communicate persuasive arguments 222739 In work mainly build 39 order construct domain communication languages use rewards However work differs additionally consider rewards asked specify social commitments entailed illocutions exchanged negotiations Moreover additionally specify reasoning mechanism tactic PN Second terms reasoning mechanisms PN note work 22 speciﬁes arguments threats rewards appeals terms logic statements However semantics arguments completely speciﬁed choice argument send according number ad hoc rules Building 35 proposed reasoning mechanism considered threats rewards appeals In case arguments abstract elements gave utility agents The choice arguments send determined according trustworthy opponent number fuzzy rules 34 More recently 23 provided formal model arguments threats rewards explanatory arguments logic determine force argument They specify mechanism identify conﬂicts threats rewards appeals Their conception rewards similar capture gains reward terms gains goals reward achieves However specify negotiation protocol negotiation algorithm determines offers send rewards threats Moreover study threats rewards bring better negotiation outcomes In general approaches concretely instantiated arguments terms standard negoti ation scenario Moreover algorithms benchmarked standard negotiation algorithms gains claim generate properly quantiﬁed In contrast shown approach generate signiﬁcant gains standard negotiation tactics respects 832 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 8 Conclusions In paper presented comprehensive model persuasive negotiation enables agents achieve better deals repeated encounters previously possible standard negotiation tactics In particular focus use rewards rhetorical arguments given asked Speciﬁcally rewards deﬁne constraints imposed set possible agreements future negotiation games contingent opponent agreeing offer support current encounter The model consists parts protocol reasoning mechanism In terms protocol dynamic logic specify commitments arise persuasive negotiation based exchange rewards In ensure negotiation dialogue agents checked consistency ensuing commitments stored Our PN protocol ﬁrst consider commitments result asking giving rewards negotiation encounter In terms reasoning mechanism deﬁne agent generate select evaluate rewards offers This decision making model composed Reward Generation Algorithm computes rewards asked given opponent set functions permit evaluation incoming outgoing offers rewards The RGA based simple principle concessions previous games need compensated future rewards We shown RGA easily connected nonpersuasive negotiation tactics order generate rewards repeated encounters Building decision making model developed new Reward Based Tactic permits generation rewards asked given opponent point negotiation The RBT strives achieve Paretoefﬁcient deals ensuring preferred outcomes selected negotiating agents In shown reduce number offers agents need come agreement enable agents achieve higher utility deals standard benchmark tactics MMPD domain In particular results RGA enable agents standard negotiation tactics 17 gain utility repeated encounters More importantly RBT shown generate agreements 26 better standard tactics 21 times fewer messages Note results indicative possible improvement PN bring agents interact speciﬁc setting MMPD Other settings envisaged expect similarly positive results MMPD generally considered capture canonical properties interactions aim apply PN Moreover analysed RBTs properties shown important factor impacts number offers exchanged average utility achieved target agents set achieve An agents target determines aggressively try come agreement offer ask rewards Thus higher target likely able rewards likely ask rewards In extreme case given principle apply RGA agents able claim rewards avoid making concession order achieve target In general work raises number theoretical practical issues First allowing rewards repeated encounters extend bargaining problem initially posed Rubinstein 37 Now problems usually studied deduce equilibrium properties bargaining theory 25 This important order understand interplay factors agents targets discount factors impact negotiation outcome However believe PN mechanisms like undoubtedly generate complex interaction scenarios These scenarios raise number complex theoretical issues need addressed Second fact agents reasoning mechanism sophisticated standard negoti ation tactics indicates design agents likely challenging complexity arguments exchange increases This means structured approaches terms methodologies frame works needed designing PN agents 32 Such approaches help deﬁne standardise reasoning mechanism agents way different types arguments protocols decision making functionalities interconnected adapted ﬁt particular application contexts Third shown PN beneﬁcial constituent agents important study systemwide properties emerge PN mechanisms In vein usually expected decentralised bilateral negotiations based standard negotiation tactics presented rarely achieve level efﬁciency guaranteed centralised auctionbased approaches However given PN techniques support richer interactions existing automated bilateral negotiation mechanisms possible exchange SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 833 meaningful information lead agents achieve better deals case This turn lead better efﬁciency level Hence important study beneﬁcial PN mechanisms relative auctionbased approaches identify tradeoffs result use Finally RBT shown better standard tactics MMPDbased repeated encounters tactics envisaged future different similar contexts Given interesting use techniques evolutionary game theory genetic algorithms strategies change performance agents pitted different strategies 48 This help determine strategy choose agent placed given population Acknowledgements Carles Sierra Lluís Godo partially supported OpenKnowledge European STREP project Spanish project EIA TIN200615662C0201 We wish thank Pilar Dellunde valuable comments logical aspects paper Prof Abhinay Muthoo Dr Pablo Noriega Prof dr John Jules Ch Meyer construc tive comments advice negotiation model related work We thank anonymous reviewers valuable comments Appendix A Devising utility functions The prisoners dilemma PD known applicability general forms interactions 5 In devising utility functions according PD aim build realistic interesting interaction scenarios zero sum games 2537 In particular characterisation agents utility functions terms PD model general interactions agent pair prefers issues counterpart This commonly case example highvolume traders able enjoy economies scale value price goods sell individual customers probably Another example car seller high costs getting car special colour buyer strong feelings colour With respect PD case bargain cooperation means agent agrees concede defection means agent exploits opponent In order devise utility functions appropriate work assumes values enacted issue require moves Cooperate Defect present standard version PD In particular need continuous scale cooperation extremes To end extend prisoners dilemma multimove prisoners dilemma MMPD 72946 In MMPD actions moves considered enactment contents contract paying goods delivering goods Both interaction partners actions dictated contract enact seller delivers goods buyer pays goods given time Agents issue care delivery goods ensuring certain quality issue discrete number possible values given paying 3 days 4 days delivering 1 month 2 months In following section ﬁrst deﬁne action set possible moves agents interact MMPD Then provide formal deﬁnition MMPD respect multiissue contracts The subsection shows devise utility functions agents engage MMPD These utility functions agents experiments Section 6 A1 The action set Whenever contract signed agent given contract enact In order simplify notation note Oα issues α enacts contract Oβ β enacts slight modiﬁcation formalism introduced Section 2 In effect achievement issuevalue pairs xi vi agents contract action game Thus agent α generate action set OOα MMPD deﬁning possible assignments values issues controls This expressed cid4 Oα cid6 Oα x1 v1 xn vn xi X cid5 vi Dxi cid4 Oα A1 O cid7 cid5 834 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 Table A1 Multimove prisoners dilemma αs partβs Oβ k Oβ l Oα U β Oα U β Oα Oβ Oβ k U αOα l U αOα Oβ k Oβ l Oα j U β Oα j U β Oα j Oβ Oβ k U αOα j l U αOα j Oβ k Oβ l Each agent possible actions deﬁned actions result payoff agent similar prisoners dilemma discrete multiaction set opposed binary action set A2 The game αs action Oβ The MMPD represented matrix row column corresponds particular degree operation agents Therefore contract O agents α β represented point k βs action O Oα matrix Oα k The subindexes different contracts correspond row column k respectively matrix We assume total order applies possible contracts matrix according utility contract agent concerned moving single row column This means agent α Oα j j possible executions Oα j defection exploitation α cooperative concession β resulting greater utility α utility loss β β agrees Oβ k staying column Let Oα set contracts handled α Oβ similarly β Oα Oβ We deﬁne multimove prisoners dilemma follows Oα j representing defection Oα α Oβ l representing defection Oβ k β Deﬁnition 9 Two agents α β engage multiplemove prisoners dilemma MMPD contracts Oβ choose iff points matrix Oα U β Oβ Oα j l following rules respected Oα U αOα k U β Oβ j Oβ U αOα k Oβ l 1 Defection Rules agent exploit anothers cooperation defecting exploiting ends lower payoff defects cid5 cid5 cid4 cid4 Oα cid4 Oα U α U β Oβ l Oβ l cid5 U α U β cid4 Oα j Oα j Oβ l Oβ l cid5 U α U β cid4 Oα cid4 Oα cid5 cid5 Oβ k Oβ k U α U β cid4 Oα j cid4 Oα j cid5 cid5 Oβ k Oβ k 2 Pareto Efﬁciency Rules sum rewards cooperate concede higher sum obtained agents defect exploit cid4 Oα cid4 Oα j U α U α cid5 cid5 Oβ k Oβ k U β U β cid4 Oα cid4 Oα j cid5 cid5 Oβ k Oβ k U α U α cid4 Oα j cid4 Oα j cid5 cid5 Oβ k Oβ l U β U β cid4 Oα j cid4 Oα j cid5 cid5 Oβ k Oβ l From rules possible derive following payoff matrix pair possible contracts chosen agents We deﬁne utility functions respect payoff structure MMPD To end propose following theorem Theorem 10 Let X given set issues α β agents Xα issues αs control Xβ issues βs control X Xα Xβ Assume utility α contract O x1 v1 xn vn issues XO X form U αO vi analogously agent β U β O xi utility functions α β individual issue xi xi XO ωα x xi vi U α xi xi XO ωβ U β x U β U α xi cid3 cid3 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 835 Moreover assume U α y Xβ O respectively differentiable strictly decreasing x v U β y u differentiable strictly increasing functions x XαO Then U α U β respect aforementioned defection Paretoefﬁciency rules multimove prisoners dilemma following conditions satisﬁed ii cid9 cid8 dU β x dx ωβ x ωα x dU α x dx issues x XαO cid8 cid9 ωα y ωβ y dU α y dy dU β y dy issues y Xβ O A2 A3 inequalities pointwise Proof Without loss generality assume XO x y Xα x Xβ y Let O x v y u agreed contract We begin considering defection agent α issue x value v value vcid17 U αvcid17 U αv given remains For easier notation write U αv u denote utility agent α contract x v y u similarly agent β U v u U αv u U β v u From defection Paretoefﬁciency rules MMPD condition U v u U v cid17 u assumptions utilities U α U β Eqs A2 A3 means x U β cid17 cid17 x v x U α x U β x U α ωα x v ωβ x v ωα equivalent condition required cid5 x v Now general assumptions cid4 x v U β U β cid5 x v U α cid4 U α ωβ ωα x x v x v ωβ x cid17 cid17 cid17 U α x v U α x v vcid17cid10 v dU α x dx dx x v U β U β x v cid17 vcid17cid10 v dU β x dx dx A4 A5 A6 A7 Hence applying condition expressed Eq A2 theorem Eqs A6 A7 Eq A5 satisﬁed U v u U vcid17 u ucid17 defection α u Similarly procedure applied Eqs A6 A7 Eq A3 defection agent β changing agreed value y u new value y ucid17 U β ucid17 U β u given opponent defect case yields U v u U v ucid17 Finally agents defect x vcid17 y ucid17 U αvcid17 U αv U β ucid17 U β u given stays obviously desired inequalities actually express Paretoefﬁciency rules U v u max cid4 U v cid17 cid17 u U v u cid5 cid4 cid2 min U v cid17 cid17 u U v u cid5 having following defection rules satisﬁed U α x vcid17 given stays cid2 x v U β U β x v U α cid17 cid17 U v u x vcid17 U β y u U β y ucid17 U α y u U α A8 y ucid17 If utility function agent α issue contract satisﬁes conditions expressed Eqs A2 A3 respect opponent β agents follow prisoners dilemma These utility functions 836 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 generally mean α higher marginal utility β issues issues y Theorem 10 lower marginal utility issues issues x Theorem 10 Then agreement reach represents different degree exploitation concession parties concerned The degree concession determined difference exists maximum value agent obtain exploited opponent issues value agreement chosen Eq 2 The higher exploitation higher utility loss expected particular contract opponent References 1 L Amgoud S Kaci On generation bipolar goals argumentationbased negotiation I Rahwan P Moraitis C Reed Eds Argumentation MultiAgent Systems State Art Survey Lecture Notes Artiﬁcial Intelligence vol 3366 Springer 2004 pp 192207 2 L Amgoud H Prade Formal handling threats rewards negotiation dialogue Proceedings Fourth International Joint Conference Autonomous Agents MultiAgent Systems ACM Press 2005 pp 529536 3 L Amgoud H Prade Handling threats rewards explanatory arguments uniﬁed setting International Journal Intelligent Sys tems 20 12 2005 11951218 4 JL Austin How Do Things Words Harvard University Press 1975 5 R Axelrod The Evolution Cooperation Basic Books New York 1984 6 J Bentahar B Moulin JC Meyer B Chaibdraa A logical model commitment argument network agent communication C Sierra L Sonenberg NR Jennings M Tambe Eds Proceedings Third International Conference Autonomous Agents MultiAgent Systems 2004 pp 792799 7 A Birk Boosting cooperation evolving trust Applied Artiﬁcial Intelligence 14 8 2000 769784 8 F Brandt G Weiss Vicious strategies Vickrey auctions AGENTS 01 Proceedings Fifth International Conference Au tonomous Agents ACM Press 2001 pp 7172 9 L Busch IJ Hortsmann Endogenous incomplete contracts A bargaining approach Canadian Journal Economics 32 4 1999 956975 10 M Esteva JA Rodríguez B Rosell JL Arcos Ameli An agentbased middleware electronic institutions Third International Joint Conference Autonomous Agents MultiAgent Systems 2004 pp 236243 11 M Esteva JA RodríguezAguilar C Sierra P García JL Arcos On formal speciﬁcation electronic institutions F Dignum C Sierra Eds Agent Mediated Electronic Commerce Lecture Notes Artiﬁcial Intelligence vol 1991 Springer 2001 pp 126147 12 P Faratin C Sierra NR Jennings Negotiation decision functions autonomous agents International Journal Robotics Autonomous Systems 24 34 1998 159182 13 P Faratin C Sierra NR Jennings Using similarity criteria tradeoffs automated negotiations Artiﬁcial Intelligence 142 2 2002 205237 14 S Fatima M Wooldridge NR Jennings Optimal negotiation strategies agents incomplete information JJ Meyer M Tambe Eds Intelligent Agent Series VIII Proceedings 8th International Workshop Agent Theories Architectures Languages ATAL 2001 Lecture Notes Computer Science vol 2333 Springer 2001 pp 5368 15 S Fatima M Wooldridge NR Jennings An agendabased framework multiissue negotiation Artiﬁcial Intelligence 152 1 2004 145 16 S Fatima M Wooldridge NR Jennings Sequential auctions objects common private values Proceedings Fourth International Joint Conference Autonomous Agents MultiAgent Systems 2005 pp 635642 17 R Fisher W Ury Getting Yes Negotiating Agreement Without Giving In Penguin Books New York 1983 18 D Harel Dynamic logic D Gabbay F Guenther Eds Handbook Philosophical Logic Volume II D Reidel Publishing Company 1984 pp 497604 19 J Hovi Games Threats TreatiesUnderstanding Commitments International Relations Pinter 1998 20 NR Jennings P Faratin AR Lomuscio S Parsons C Sierra M Wooldridge Automated negotiation Prospects methods challenges International Journal Group Decision Negotiation 10 2 2001 199215 21 NR Jennings S Parsons P Noriega C Sierra On argumentationbased negotiation Proceedings International Workshop MultiAgent Systems Boston USA 1998 22 S Kraus K Sycara A Evenchik Reaching agreements argumentation A logical model implementation Artiﬁcial Intelli gence 104 12 1998 169 23 P McBurney RM van Eijk S Parsons L Amgoud A dialoguegame protocol agent purchase negotiations Journal Autonomous Agents MultiAgent Systems 7 3 2003 235273 24 A Muthoo Bargaining longterm relationship endogenous termination Journal Economic Theory 66 1995 590598 25 A Muthoo Bargaining Theory Applications Cambridge University Press 1999 26 MJ Osborne A Rubinstein Bargaining Markets Academic Press 1990 27 S Parsons C Sierra NR Jennings Agents reason negotiate arguing Journal Logic Computation 8 3 1998 261292 28 C Perelman The Realm Rhetoric ﬁrst ed University Notre Dame Press 1982 29 L Prechelt INCA A multichoice model cooperation restricted communication BioSystems 37 12 1996 127134 30 I Rahwan SD Ramchurn NR Jennings P McBurney SD Parsons L Sonenberg Argumentationbased negotiation The Knowledge Engineering Review 18 4 1996 343375 31 I Rahwan L Sonenberg F Dignum Towards interestbased negotiation JS Rosenschein T Sandholm M Wooldridge M Yokoo Eds Proceedings 2nd International Joint Conference Autonomous Agents MultiAgent Systems Melbourne Australia 2003 pp 773780 SD Ramchurn et al Artiﬁcial Intelligence 171 2007 805837 837 32 I Rahwan L Sonenberg NR Jennings P McBurney Stratum A methodology designing agent negotiation strategies Applied Artiﬁcial Intelligence 21 10 2007 33 H Raiffa The Art Science Negotiation Belknapp 1982 34 SD Ramchurn D Huynh NR Jennings Trust multiagent systems The Knowledge Engineering Review 19 1 2004 125 35 SD Ramchurn NR Jennings C Sierra Persuasive negotiation autonomous agents A rhetorical approach C Reed Ed Workshop Computational Models Natural Argument IJCAI 2003 pp 918 36 SD Ramchurn C Sierra L Godo NR Jennings Negotiating rewards Proceedings Fifth International Joint Conference Autonomous Agents MultiAgent Systems ACM Press 2006 pp 400407 37 A Rubinstein Perfect equilibrium bargaining model Econometrica 50 1982 97109 38 J Searle Speech Acts An Essay Philosophy Language Cambridge University Press New York 1969 39 C Sierra NR Jennings P Noriega S Parsons A framework argumentationbased negotiation M Singh A Rao M Wooldridge Eds Intelligent Agent IV 4th International Workshop Agent Theories Architectures Languages ATAL 1997 Lecture Notes Computer Science vol 1365 Springer 1998 pp 177192 40 K Sycara Arguments persuasion labour mediation Proceedings Ninth International Joint Conference Artiﬁcial Intelligence 1985 pp 294296 41 K Sycara Persuasive argumentation negotiation Theory Decision 18 3 1990 203242 42 K Sycara The PERSUADER D Shapiro Ed The Encyclopedia Artiﬁcial Intelligence John Wiley Sons 1992 43 WTL Teacy J Patel NR Jennings M Luck Travos Trust reputation context inaccurate information sources Autonomous Agents MultiAgent Systems 12 2 2006 183198 44 C Tindale Acts Arguing Rhetorical Model Argument State University Press New York Albany NY 1999 45 F Tohmé Negotiation defeasible reasons choice Proceedings Stanford Spring Symposium Qualitative Preferences Deliberation Practical Reasoning 1997 pp 95102 46 G Tsebelis Are sanctions effective A game theoretic analysis Journal Conﬂict Resolution 34 1990 328 47 DN Walton ECW Krabbe Commitment Dialogue Basic Concepts Interpersonal Reasoning SUNY Press Albany NY 1995 48 J Weibull Evolutionary Game Theory The MIT Press 1995