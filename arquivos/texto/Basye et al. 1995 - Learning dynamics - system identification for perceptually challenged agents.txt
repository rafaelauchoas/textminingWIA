Artificial Intelligence 72 1995 139 171 Artificial Intelligence Learning dynamics identification perceptually challenged agents Kenneth Basye Thomas Dean bi Leslie Pack Kaelbling b2 Department Mathematics Computer Science Clark Universiiy 950 Main Street Worcester MA 01610 USA Department Computer Science Box 1910 Brown University Providence RI 029121910 USA Received September 1992 revised March 1993 Abstract From perspective agent inputoutput behavior environment embedded described dynamical Inputs correspond actions executable agent making transitions states environment Outputs correspond perceptual information available agent particular states environment We view dynamical identification inference deterministic finitestate automata sequences inputoutput pairs The agent influence sequence inputoutput pairs presented pursuing strategy exploring environment We identify sorts perceptual errors errors perceiving output state errors perceiving inputs actually carried learning making transition state We present efficient highprobability algorithms number identification problems involving errors We present results empirical investigations applying algorithms learning spatial representations 1 Introduction System identification refers inferring model dynamics governing agents For instance wish infer model interaction environment Corresponding author Email kbasyegammaclarkuedu This work supported National Science Foundation Presidential Young Investigator Award IRI8957601 Air Force Advanced Research Projects Agency Department Defense Contract No E3060291C0041 National Science foundation conjunction Advanced Research Projects Agency Department Defense Contract No IRI8905436 2 This work supported National Science Foundation National Young Investigator Award 00043702950950 1995 Elsevier Science BV All rights reserved SSDl000437029400023T 140 K Btrsy YI dArrijkkr Inrrllipxce 72 1995 139I 71 fluctuations assembly output parts supplier affect production factory robot interacts devices work cell rules set states transition probabilities differential set stochastic process agent predict consequences performing spatial inference equations Such predictions planning The inferred model correspond production The model useful insofar enables actions diagnostic System reasoning identification environment studied variety disciplines theory We focus learning theory neural networks automata environments characterized large literature restricted problem portion summarized paper Our results address Our objective probability outputs complexity infer accurate models high inputs effects uncertainty algorithms interaction environment learning time faced noise computational deterministic determine polynomial produce observing agents finitestate including representations automata There control We interested agents interact environments interactions We chosen particular focus identification observation complicates appears critical uncertainty identification It clear studying identification observation annoying hard cope However structure Useful structure sequences distinctive plays supporting identification facilitating wide range interactions means end believe problems isolation provides insight role Our basic findings uncertainty requires somewhat bookkeeping asymptotically lacking hopeless landmarks short form reasonably distributed environments learning features makes function learning adequately relatively easy use having sort model learning space support path planning A dynamical model speed clearest example utility model optimally 221 allowing agent simulate tradeoffs involved learning dynamical models exactly learning depend tasks agent Again avoid addressing 9 order focus basic paper issues actions environments It certainly possible model There environments help enormously Perhaps maps largescale learning plans reactions Clearly worth uncertainty observation tradeoffs affects learning 2 Modeling dynamical systems automata We model dynamical finitestate automata systems deterministic graph DFA shows statetransition agents actions outputs learning interested defined terms agents perceptual capabilities Discernability information state agent uniquely exists sequence actions observations structure real environments discernible require state DFA agents perceptual discernible inputs available identify DFAs Fig 1 DFA inputs We K Basye et alArtcial intelligence 72 1995 139171 141 1 0 Ii x CJ interacting environment Fig 1 An agent Y Y 1 0 X X x agent distinguish states For instance states automaton correspond agent locations office building hallways meet In case observed outputs correspond hallways hallways As example consider Here states correspond user outputs announcements state menus services actions traversing structure voicemail junctions number incident keys pressed junction correspond inputs actions locations learning incident The DFA need represent agents interaction environment separate models different aspects interaction We assume size careful choice perception state space reduced limit action primitives Actions agents options small equivalence abstract behaviors given state The set possible observations response use perceptual apparatus experiences relations perceptual act filters encapsulated manageable serve kept introducing relatively determining states We admit requires great deal insight specifying set need correspond set states cross product sets values state state variables desirable Historically AI researchers kept state space implicit In view agents observations state variables fluents observations automaton need correspond variables We world consisting distinguishable action primitives general advice obtain primitives We claim primitives Even state uniquely stochastic results tions available particularly agent observing process output state realizing action attempting Fig 2 illustrates extreme arises fact observa state In paper sources uncertainty We allow noise true execute action results apply relatively sorts errors Our polynomialtime forms uncertainty problem offer small number perceptually perception interested occasionally predict fairly poor performance learning difficult statespacereducing deterministic case uncertainty performance determine 142 K Basye et al Artificial Intelligence 72 1995 139171 Fig 2 Noisy observations inputs outputs benign environments Our empirical perform better current sort expect robots encounter real world investigations algorithms theoretical bounds predict benign environments indicate 3 Formal model In order automaton inferring identification model structure finitestate varieties finitestate au tomata introduce extension familiar definition finitestate automata versions output Recall follows action state reached action depends action taken previous state In Moore model output depends state reached Mealy model output depends previous state action action taken Alternatively depend current state previous action In terms model appropriate depends nature agents sensing particular sensations depend Moore automata regardless got state systems way previous action We use given state think Mealy model having outputs model implies sensations identification In model explicitly distinguish actual states actions automaton agents possibly erroneous view Thus given true state agent observes label accurate environment environment reflection state Similarly commands real actions corresponding action nominal correspondence agent generates commands taken s It technically correct define output alphabet outputs generate observable states outputs simplicity states rise directly following treatment chosen possibly erroneous collapse labels automaton function mapping labels correct For giving account processes K Basye et alArtcial Intelligence 72 1995 139l 71 143 In order use probabilistic functions means modelling uncertainly introduce following notation finite set S let flfs OJlfsl s set probability density functions PDFs S The structure agents environment interaction environment specified tuple E Q B L C 6 q5 k l Q finite nonempty l B finite nonempty l L finite nonempty l C finite nonempty l 6 Q x B Q state transition function l 4 probabilistic set states set basic actions set observable set executable commands observation labels function 4 Q f FL mapping state distribution possible observed labels l probabilistic action function C FB mapping executable command distribution possible actions We write B We extend IZ Ql We write A B set finite sequences basic actions usual way defining 6q A q S q ab 6 6q 6 q E Q E A b E B A sequence We write qa shorthand 6q state resulting function 6 sequences state transition execution sequence state q action executed We assume state function 6 induces set E labelled edges 41 b q2 41 q2 E Q b E B An automaton strongly connected qla q2 states 41 q2 E Q sequence E A We write qa denote executing example sequence starting bobI b2 b qa qqboqbobl qa sequence outputs length state q beginning output state q For al 1 resulting Q B 6 specify deterministic concerned worlds systems represent function structure The remaining environment When distribution dq c performs action drawn distribution apart agent Note state transition fixed observe act agents ability agent reaches state q E Q observes label drawn perform command Similarly state q attempts elements c We assume correct observation function action command action command observation deterministic state inference procedures wider applicability stochastic expect exclude state correct It useful distinguish correct action function thought mapping 1 Clearly assumptions action However certainly able learn presence arbitrarily malicious noise Instead functions behave noisy version function We write d observation PDF element probability functions governing observation situations assuming observation require action 144 K Busye et ulArtiJicinl intelligence 72 1995 139I 71 When sidering outputs understood function vation states sequences obser When state different output correct observation correctly probability errors remaining lifetime agent restrictions error distribution In cases cases stationary threshold value distributions governing change impose additional explicit required function present agents functions provide additional unique While environments necessary function infer automata allows agents observation unique observations shall unique observation structure environments unique observation uncertainty Even case states unique outputs We refer states landmarks use landmarks capable determining capable recognizing observation unique presence stochastic landmarks landmarks outputs sequences states In following discussion landmarks necessary distinguish functions forms In order agent able unique observation arise deterministic observations assume automata indistinguishable mean sequence automaton M unique The class automata environments attempting experiment perform c A sequence u E A said distinguish q1 q2 Th e notion distinguishability single sequence E A distinguishes extended state q Mt following qta 92a way Mt M2 distinguishable MT action sequence states q M2 Ml qa qu An automa pairs states q1 q2 E Q exists action ton said reduced sequence given distinguishes stronglyconnected reduced member isomorphism 161 automaton minimum number states strongly connected learn reduced We consistently tell Note requirement pairs states However non identical pairs states called distinguishing sequence M More precisely E A distinguishing The outputs signature determined began executing provides unique signature precisely sequence E A homing sequence ql u q2 qla qza Every distinguishing knowing started executing quence sequence result having executed know end sequence Both distinguishing quences preset adaptive action tree actions branches correspond fixed executed blindly regard outputs way sequence provide unique given state way knowing sequence state reached end execution More M 41 q2 E Q homing sequence sequence se distinguishing implies homing se execution distinguishing state automaton note signature 111 An adaptive sequence determined previous outputs q1 q2 E Q qla 92a H q1 q2 possible outputs A preset sequence sequence A homing sequence state providing distinguishes sequence leaving sequence resulting The usual definition finitestate automata includes start state machine K Basye et alArtcial Intelligence 72 1995 139171 145 prior assumed agent ability environments actions Some systems feature state reset For return assumption reset realistic 4 Theoretical results purely deterministic case results paper methods input observation The main stochastic section review automata purely deterministic functions important previous case learning automata In order results results concerning presence context inference data inferring sequences automaton automaton allowed automaton automaton construction observed pairs assumed inferring Some writers distinction automaton For sequence inputoutput terms Ql agrees given set data Moore behaves isomorphic trivial agrees data constructed building chain states build tree 161 come reduced strongly inference smallest consistent automaton yields result correct identically observed automaton long data If multiple start state root For reason research concentrated smallest automaton showed inputoutput connected isomorphic inference Gold inputs generating putoutput produces description automa recording produced ton Inference algorithm eventu ally case way detect happened Golds algorithm relies learner having initial state time sequence descriptions converge description correct automaton automaton Thus inferring samples original inference method resulting outputs periodically isomorphic 121 provides automata automaton case behaviorally behavior The algorithm reset automaton limit means guaranteed limit ability finding The general problem inferring smallest 1131 automaton consistent given Indeed finding automaton intractable NPcomplete assuming P NP 171 21 building work Gold provides polynomialtime pairs close smallest set inputoutput polynomially Angluin inferring smallest source counterexamples automaton provides sequence inputs hypothesized generate different outputs Rivest Schapire ability given In model point source counterexamples indicates automaton algorithm reset automaton algorithm hypothesize correct actual automata 191 use homing sequence substitute reset dispense reset source counterexamples sequence case distinguishing polynomial time 18201 Several provided learned approached researchers finitestate learning et al 211 recurrent network systems neural net learn works For example ServanSchreiber 146 K Busye et trl Artijkiul Intelligence 72 I 995 139I 71 grammars finitestate Rivest Schapire algorithms mentioned mance Bachrach 31 neural net implement This work stressed perfor issues issue noise inputs outputs considered 5 Theoretical results stochastic case In section provide polynomialtime settings stochastic different agents actions allow error observations observations action function agent impose restriction special landmark In assume algorithms identification automata error In second assume states unique perfect allow error actions outputs In final case allow error observations states nominal label unique 51 Deterministic actions stochastic observations In section consider situation labelled states We relying correct action function knowledge distin guishing functions learned output function sequence automata nonunique uniquely observation noisy4 X11 Structural interaction properties Structurally automaton agent parts We assume states environment requirements learned strongly connected We avoid possibility algorithm weak require reach agent knows upper bound number environment trapped With regard interactions assume agent moves deterministically perfect Observations assumed correct observation probability greater independent sequence events Finally assume environment determine straightforward distinguishing learner For manmade execution actions restrictions observations preset distinguishing natural environments example junctions office environments environment short sequence turns serve distinguish noisy provided sequence For 512 Algorithms The algorithm present uses subroutine moves agent statistics labels observes The procedure provides procedure collecting environment output signature homing sequence end sequence state reached end movement Recall distinguishing output uniquely determines sequence state reached output The work described section carried jointly Dana Angluin Sean Engelson described 7 I K Basye et alArtificial Intelligence 72 1995 139I 71 147 state homing sequences sequence begun In environment signatures correctly This algorithm makes use procedure effect having automaton achieves terminates probably correct The procedure sequence sequence That state given distinguishing way run longer order guarantee correctness general observed uniquely determines stochastic observations called LOCALIZE probably state returns signature parameterized higher probability We begin explaining correct homing LOCAL procedure learn environments properties discussed The localization procedure works exploiting fact movement deterministic sequence period cycle locations separate given distinguishing execute agent The basic idea execute number times collect cycle certain output By finding observed outputs use statistics outputs observed state These statistics correct outputs state determine cycle signature returned distinguishing case state agent agent supplying deterministic high probability repeatedly localize sequence In order determine statistics statistics analyzed alternative For environment bound IQ The set outputs observing Tii j symbol Zj given hypotheses determine high probability period cycle After period repetition walk high probability walk period cycle E states Q 41 q2 qn let m given upper probability agent state qi Let P denote lower bound L Eo II Zk Let Pji denote Let s blb2 blsl preset distinguishing actions For integer Let qi state reached executing sufficient agent guarantee 0 let si represent sequence sequence consisting times sequence Pfi The m repetitions sequence states period cycle s repeated cycle Thus 4OqlqC9 value wish periodic Let p denote As execute sequence statistics separately position eo s qibl b2 For e sequence d period p second walk 3 prefix track position For offset e actions q periodic IsI 1 let di state reached qi executing For C consider sequence correct outputs states di dqi The output sequence 4P periodic unique p pt However s distinguishing period dividing p Since outputs necessarily sequence p 148 K Basye et alAmjzcial Intelligence 72 1995 139I 71 Table I Sequences visited states QT 4 Step States visited LCM pcs Thus suffice common multiple values PJJ LCM In fact high probability values qe p divides qt qe divides p LCM q p We sequence qo qi 42 analogous procedure Consider candidate period rr 6 m let g gcdp z For 0 T 1 s starting m sequence states visited rr repetitions consider repetitions s This sequence states Since qi k Olpg states periodic period p sequence visits state set qi I order continues repeat cycle pg Table 1 shows example p 6 rr 4g 2 In case row r gives indices states visited repeating s qo 91 states visited repeating s starting qs qr assuming In special case 7r p shown Table 2 row r consist exclusively visits state q It case wish distinguish We observe states observe agent visits The algorithm repeat times N chosen close probabilities form table r rows numbered 0 r label ri During time observe sequence distinguishing high probability observed labels state s total JV frequencies sampled distribution For candidate period r 6 m 1 k columns possible j second walk increment table row r column ensure label l After relative frequency entry row Let p P second walk compute entries label separation p threshold lower bound correct observation threshold When row table r value table sequence rr outputs table said plausible For plausible determined largest value r rows minimum period r sequence We LCM IT candidate 4 We use value p We present procedure precise manner Procedure Localize 1 For simplicity assume integers 1 possible outputs known correspond k Build table T r t Y j size m x 1st x m x k Initialize cycle K Basye et alArtcial Intelligence 72 1995 139I7I 149 Table 2 Sequences visited states 6 Step States visited 0 1 2 3 4 5 0 I 2 3 4 5 0 1 2 3 4 5 0 I 2 3 4 s 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 2 3 4 zero 5 s ensure sequence traverse table entries Execute continually Initialize Execute s N times incrementing individual Let C c mod IsI j label observed long continues step following agent closed walk execute S 6 sequence counter R c 0 step counter c c 0 R time After executing immediately following exe cution b Foreachn12 m 1 increment j table entry TG e R mod 7rTT 1 Increment c Let 5 step counter c t c 1 6 7 8 list C c period isep build For 7r f consider large elements atg maxj F q f r j outputs corresponding n 1 Find table F T e If r 7r row r table contains sequence outputs length n larger Initialize dimensional element taking rOl Let ZS7 LCM 7r E L Conclude R mod 17 threedimensional table FI7 esis correct outputs distinguishing sequence outputs argmaxj FnJ single output arg maxj F IZ 1 SI 1 r 1 mod ZI j row r return hypoth s state period sequence add L e 0 1 IsI 1 concatenated located state currently agent sequence r j Table 3 shows tables resulted run LOCALIZE environment Fig 3 The distinguishing shown T conjectured sequence S r index cycle length r j observed tables procedure bb Recall label We r 2 C 0 r 2 7 4 plausible When number executions s I index sequence given period length 5 If labels known table constructed incrementally adding new labels observed 6 Following Step 2 action action s IS0 K Btrsye rf 11 Rrtijkicil lnrellijiencr 72 I 995 139I 71 alb Fig 3 A simple environment I short distinguishing sequence Table 3 Tables built LOCALIZE environment Fig 3 7r I 7r2 3 r4 j 0 jl jO jl jO j I jO jl I 0 26052 0 0 0 24048 0 0 0 I I 4408X 6012 0 0 0 0 0 0 21084 4 016 2008 SO2 0 0 0 0 21084 4016 23092 2008 0 0 0 0 100588 70412 100588 70412 70438 90562 0 0 20118 lS0882 160942 lO0588 130812 30188 0 0 llO846 20154 20514 llO846 lOO833 2OI67 3025 13l 120923 SO667 40333 llO917 9075 0 lO0769 lO0833 sequence 01 period 2 r 2 1 period r 4 e 0 period set periods returned algorithm 010 1 1 The LCM agent starts qo finishes In case r 0 signature qo 000 In case I 1 signature 2 n 4 e 1 period qo R q2 R odd 2 Ilr 2 In example 513 Analysis A formal proof correctness algorithm proof complexity paper Dean et al 81 We cite final complexity refer reader paper complete proof terms number steps appears result concerning Theorem 1 In order provide correct signature probability execute sequence s distinguishing 1 E LOCALIZE 8m2 2sJkm3 m 2P l times The number steps taken K Basye et alArtifcial Intelligence 72 1995 139171 151 Mm2 P 2 In IsI length distinguishing Recall number states correct output probability automaton P lower bound probability sequence m upper bound state k number possible outputs E bound procedure fail returns sequence agent moment The complete distinguishing underlying 81 Suppose proof correctness procedure employs straightforward relatively s learning LOCALIZE subroutine sketched The detailed algorithm LOCALIZE state agent determine state visited In case agent learn depth connectivity state transition graph The agent actually search manage depthfirst traverse executes sequences actions search root corresponding depthfirst course search state recognized arc added inferred automaton completely having visited appropriate path Instead state transition graph starting root time LOCALIZE When automaton performing amounts search backtracks general paths search tree returning backtrack statetransition automatons depthfirst explored fashion graph The actual inference algorithm states encounter necessarily able immediately problem state root knows high probability state landed depthfirst number executions distinguishing identify complicated localization procedure agent final state search The identify parallel possible agent executed step solved given starting state solved performing searches LOCALIZE ends Whenever LOCALIZE search state root node The second problem depthfirst sequence state high probability possible It easily shown learn automaton high probability number visits fixed starting state LOCALIZE return number times time polynomial state required number times state state agent return sure returned starting given polynomial agent executing LOCALIZE Of course agent achieve required probability reciprocal probability agent absolutely success number steps polynomial measures problem size The requirement general distinguishing In particular Dean et al 8 learning sequence provided polynomial avoided general considering possible computing Further provide sequences preset distinguishing efficient way compute adaptive distinguishing clear adaptive sequence work place preset LOCALIZE procedure 23 general environments indicate hard results Yannakakis time environments sequences However sequence Lee unlikely I52 K Bmye et al Artijiiciul Intelligence 72 I 995 139I 71 The procedure certain With adaptive action relies ability follow sequence deterministically right states perceives incorrect outputs sequence correct movement longer assured perform point depend previous possibly way incorrect output 52 Stochastic actions deterministic observations allows A landmark environment fairly strong condition certain states unique determine observed labels detected That labels unique think agent label unique having detector Although people good Indeed people select landmarks precisely sure present unique aspect surety comes having examined San locations For example identify Francisco assures learned agents unusual ability The problem ability interesting scope work outside good landmark based background knowledge building section assume learned programmed appearance This ability amounts Transamerica pyramid count landmark certainly unique having styles 521 Structural interaction properties In addition landmark property assume environment learned possible reverses agents observation movement assume effect While action result selftransition reversible This means action perfect restriction property Thus With regard probability assume action requirement ability obviously structure labelling landmark states landmarks label intended action takes 13 f allow static distribution actions error Finally agent perfect knowledge action reverse location We agent This agent know actions succeed fail cause selftransitions reverse movement certainty We assume detect actions implies way arrived current refer degree state knows perfectly agent For convenience define D subset Q consisting landmark states We define state I r 0 Z states landmarks We local connectivity radius r 4 E D shortest path 9 landmark radius r 4 We global connectivity environment E constant states q1 q2 D provide path q1 q2 states I subset Q consisting nonlandmark landmark distribution parameter r maximum distance landmark nearest procedure provide procedure factor learns learns The work described section carried jointly Jeff Vitter described 141 K Basye et al Articial Intelligence 72 1995 139171 153 Fig 4 A path landmarks A D length constant q2 The path constructed landmarks Fig 4 factor length shortest path q1 paths locally connected Thus summarize agents capabilities perfect serves function half time At state action taken state landmark agent agent knows action follows The agents action intended direction agent knows reverse In addition intended action unique 522 Algorithms We begin presenting learns error local connectivity incurred procedure multiplicative environ ment We answer global path queries kept low local error kept low transition local uncertainty measure complexity polynomial procedure directs exploration accurate small constant learning build answer global path queries global uncertainty measure increase begins search environment factor optimal high probability factor We conclude map building possible local connectivity The procedure trying cr c 2 integer paths locate landmarks Once candidate paths located second phases verified The need twostage process arises possibility combination landmarks statistically traversals occurring paths 1 algorithm looks short landmarks This process paths connect reverse certainty frequently fact We exploiting true paths errors By attempting ensure high probability sets directions corresponding traversals actually correspond procedure appeared movement errors perceived distinguish paths result The learning algorithm broken steps landmark identification selection filtering step agent determines candidate paths agent agent finds set candidate paths E connecting step step candidate set landmarks finds identifies candidate landmarks correspond We present actual paths 1 procedure learning local connectivity Procedure Connect I Landmark location A uniform random walk environment landmarks encountered arc added list 2 For landmark A list step I For sequence directions length cr multiple A recording path defined sequence direction reverse direction step After traversal A random walk Add path reaches landmark list candidate paths traversals intended return starting 3 For candidate path list step head list comparing landmark times beginning directions observed recorded comparisons traversal landmark random walk For path maintain traversals failed traversal If landmark landmark successful 2 execute path step 2 Any failure path multiple reverse reached failure reached return beginning count successful 4 Return paths list count step 3 threshold 523 Analysis We state proof detailed proof leading result algorithm lemmas complexity correct polynomial 161 series sample Lemma 2 For FI 0 CONNECI procedure cr qf state polynomial 1 cl 1 I 28 sire qf E exponential lundmark environment probability learns cr local connectivity I EI time Lemma 3 Let E landmark environment distribution parameter r let c integei c 2 Given procedure cr landmark possible time polynomial result c c 2 times length optimal path I 1 I cR F 0 I F size environment Any global path returned 1 0 learns local connectivity lr probability learn global connectivity probability time polJnomia1 Theorem 4 It possible probability exponenfial r I F time polynomial learn global connectivity landmark environment I 20 size E Ic l Theorem 4 simple consequence problem learning landmarks length I order establish Lemmas 2 3 It immediate application environment states r 0 need explore paths environment Because global connectivity In case parameter global connectivity K Basye ef alArtcial Intelligence 72 1995 139I 71 155 candidate path length process works reverse certainty 5 It possible Corollary distinguishable size G reverse uncertainty locations probability connectivity learn 1 E time polynomial environment E 28 1 E ll defined require environment The notion global connectivity learned necessary recover structure entire environment indistinguishable states completely assumed directions imagine For equivalence equivalence approach try completely local neighborhoods situations instance traverse direct path indistinguishable indistinguishable classes uniquely designate class radius particular global landmark street Chrysler building It far provide easy bookstore learning landmarks But states paths states partitioned In 5 J modify learn environment state specifying completely landmark 53 Stochastic actions observations In section consider observations required algorithm case error agents actions sample complexity restrictions In order provide algorithm polynomial restrictions environment We present sketch proof correctness complexity 531 Structural interaction properties reversible corresponding property Even oneway action leading action q2 91 Virtually streets normally Recall environment 41 q2 q1 Z q2 navigational corresponding undirected assumption environments parallel streets running opposite direction providing essentially environment Here assume action agent knows environment reverse given action reversible The conductance environment informally environment harder learn environment work environments longer bottlenecks We assume If lot bottlenecks measure ways random exploration The algorithm presented lower conductance conductance labelling environment IQ1 IL number states equal number labels simplicity correct presentation assume label state qi Let Pi q3 qi li probability li state qi For j let Pji qi lj probability agent let P lower bound mistakenly higher apparent probabilities state qi Finally correct observation Pi It intuitively IQ IL1 label Ii nominally agent correctly observes unique know observes lj label 1 K Basw et 01 ArtiJicicd Intelligence 72 1995 139I 71 correct observation sooner agent able correctly probability identify underlying environment commands actions The relationship labels states We assume agent correctly performs action executes command c Let 0 lower bound fi agent 8 In addition assume choose commands agent generate random walk environment action function way choose actions uniformly allowing IAl ICI Let 19 c probability analogous With regard observation assume j Pi P The agent likely mistake state 4 state q As seen later point requirement limit frequency given label seen states observation incorrect observation probabilities rejlexive mistake state qr state qi 532 Algorithms Given restrictions structure envi ronment learned simple algorithm The result algorithm high probability original environment previous section isomorphic environment underlying The algorithm uses random strategy explore graph records pair l la command Ji number times observation li label edge labels followed performing graph extracted command ci observing action ai state q state qk underlying graph observation lk After exploration c resulted statistics If lk frequently observed label 1 assume More formally statetransition graph learned following way Algorithm CMFO 1 For command Choose Most Frequent Observation c E C construct twodimensional table T indexed dimension Initialize labels L entries tables 0 2 3 Begin executing uniform tion state label 1 observed I observed command c increment value T 1 I random walk environment Whenever transi label state 4 After n action steps stop return edge set E containing edge qr ah qi T I li Tc lj lk k equal j Fig 5 shows results sample run algorithm simple graph The vertex large tables indicate label pairs The underlying graph encoded frequency sequential perception probabilities small tables specify action largest element row table boldface type 533 Analysis In section state specific conditions ceeds provide bound number observations given confidence identifying entire graph correctly algorithm CMFO suc agent order K Basye et ulArtcial Intelligence 72 1995 139171 157 Fig 5 Results running graphidentification algorithm simple environment shown I 000 19 08 In following abuse notation vertex qi Zi event perceiving event visiting qi result executing event perceiving constructing c given label Zi seen prior probability The observation agent This addressed estimates probabilities li result executing later proof probabilities slightly let qi stand event visiting label Zi addition let oqi stand action oli stand action The algorithm seen label Zj seen executing command notated ProZj 1 Zi AC depend details random walk executed An important quantity bounding necessary number trials separation label incorrect successor c 6qia j k separation Zk label equal si sij k Zj correct successor command probabilities Pr 0Zj 1 Zi Pr OZk 1 Zi given command c executed For defined sij k Pr oZj Zi h oZk 1 Zi 8 particular Iabel label Z given label qj Zk action corresponding likely Zi maximizes Pr OZk 1 Zi lower bound si written s If value s high incorrect ones likely For CMFO algorithm positive reflexive corresponding particular derives construct observation situations impossible possible set noise models high IQ1 1 P positive separation high values P The reflexivity requirement In Section 6 discuss observed probabilities making possible correct The requirement need Consider unrealistically error Cik Pki Without restriction work correctly guarantee sum probabilities incorrect separation requirement correct observation situations eliminate requirements satisfy transitions transition likely s Because separation respect particular command suppress parameter 158 K Basye et al Artid intelligence 72 1995 139171 We characterize number observations separation The proof Hoeffdings likely observation largest sample frequency largest omitted large number samples required algorithm function It uses observation true frequency inequality 6141 Lemma 6 lf separation 0 output algorithm CMFO correct probability 1 E vertex visited N times N polynomial le IQ JAI s greater ls We use random walk explore turn attention question long walk needed Not guarantee high times able charac probability terize distribution affects transition probabilities states visited state visitations environment separation allows random walk environment earlier assumption We actions chosen agent knows distribution com walk Markov process transition A uniform equiprobably mands matrix R defined r gAl g number edges E qi qt edges qi qj In reversible qj qi R symmetric rows sum implies doubly stochastic 1 making uniform stationary probability IO limit state visited probability matrix doubly stochastic Any matrix distribution columns environment ljQl The lemma concerns rate distribution uniform stationary distribution Let _i t probability t let ri stationary probability L2 norm difference state visitations ap process state qi Define rr Xt This result proaches state qi time discrepancy direct consequence l Mihails result 151 convergence Markov chains Lemma 7 Let t number time steps needed guarantee state distribution time t stationary distribution executing discrepancy 5 process determined transition matrix R Then t bounded merging conductance process It defined min S ScQqtd K Basye et alArtijicial Intelligence 72 1995 139171 159 Fig 6 A plot 12OP 1 area positive separation Now bound separation assuming bound discrepancy 1 IQ 1 fl Lemma 8 Let upper bound discrepancy t steps let z 1 IQ fl let P lower bound probability making correct observation Pi let 8 lower bound probability taking cor rect action et If probabilities making incorrect observations reflexive tlx y PxY PYx actions initial labels li random walk length t 1 siOifPlJ882148and 2 St P28P 1Z P 1 Note long walk reasonable approximate requirement P lm intuitively simple 8 lower bound correct execution commands greater complexity case separation algorithm CMFO contains factor l 20P2 1 Fig 6 shows plot factor values P B positive The plateau area figure represents portions P 0 space bound separation bounded 2tP2 1 exploration area separation negative In z 1 yields pleasing consequence Theorem 9 The output CMFO algorithm correct probability 1 s 1 2 uniform random walk length polynomial P2eP iiz P 1 l2 I IQl 1 Al P lowest probability correct obser 1e vation 8 lowest probability correct action Cp conductance z 160 K Bmyr et trl Artificicd Intullience 72 1995 139I 71 1 IQldm tribution wheneBer 1 IQlLm ad upper bound discrepancy state dis I PlzJz288z2z148 2 uniform distribution C induces uniform distribution A 3 Vx y C PYX environment 4 reversible 6 Empirical results sections develop particular simulations algorithms In following results empirical cases results demonstrate ments results automata actual number samples needed class noise mode1 present Sections 51 53 In realworld environ representing plausible far predicted theoretical 61 Noise models probability correct action Two results Section 5 require require threshold Two probability In addition algorithm CMFO requires threshold error reflexive observation A large number possible noise models satisfy constraints algorithm different noise models observations choice noise model certainly affect actual performance In experiments noise mode1 actions correct observation probability random actions constructed Our error model chosen uniformly simple uniform model When incorrect action taken incorrect actions For observations developed general class noise models called similarity partition noise models subsets Such model Ql Q2 Qk Intuitively represent look alike Each state given partition Q observation gives correct answer probability P gives label state partition probability 1 PQ Th e uniform error mode1 special case scheme occurs partition element follows The set Q states partitioned sets states function elements partition covers Q 62 Deterministic actions stochastic observations The polynomial Section 51 pessimistic We functions shown bound performance algorithms results experiments results complete There similar described LOCALIZE indicate inference automaton algorithm provided 8 1 Our result requires environments distinguishing hallway environments natural environments tinguishing environments determined sequences To test hypothesis constructed length shortest distinguishing sequences We hypothesize particular possess short dis variety hallway sequence assum K Basye et alArticial Intelligence 72 1995 139I 71 161 il iv 111 Fig 7 Graphs hallway environments sequence shown Fig 7i Figs 7ii statetransition II pairs adjacent directions graph n edges selecting The lengths shortest distinguishing graph fifth ing sequence existed Fig 7i depicts floor Brown CS Department Three graphs typical ones experiments 7iv The length shortest distinguishing sequences Figs 7 ii 7 iii 7 iv respectively We generated large number graphs starting d x d grid locations constructing uniform distribution replacement The actions available location consisted N E W S axes grid movement edge particular direction The labels orientation degenerate model probability probability fixed d n range d d2 length shortest distinguishing nearly constant For graphs distinguishing sequence states Fig 8 shows number states environment location adjacent corridors A uniform error label P observed label correct k 1 P For sequence looked length shortest roughly square root number sequence function type Lshaped Tshaped including locations label corresponding facing N E W S total sixteen length shortest distinguishing averaging sets environments action corresponded agent observed locations according selftransition junction correct increase encoded labels 162 K Btuye et al Articiul Intelligence 72 I 995 139I 71 10 20 30 40 50 60 Fig 8 Length shortest distinguishing sequence function number states environment 80 20 40 60 80 Fig 9 Percentage correct state identifications sequence distinguishing LOCALIZE function number repetitions The theoretical results indicate DFA consisting needs 76206 steps P 08 In simulations successful 100 time sequence length We observed insensitive P low 05 We believe uniformly performance LOCALIZE largely perform 100 accuracy having executed 50 steps largely fact errors distributed error 50 steps distinguishing straightforward P continuing incorrect construct alternative labels 21 states LOCALIZE LOCALIZE K Busye et dArtijicial Intelligence 72 1995 139171 163 Fig 10 An abstract environment test CMFO algorithm distributions graph percentage environment sequence This graph graphs considered require lot work LOCALIZE Fig 9 shows LOCALIZE running correct state identifications Fig 7i function number repetitions distinguishing typifies performance experiments LOCALIZE running range 63 Stochastic actions observations In section consider performance CMFO algorithm simulation simulation These environments The environments hallway environments North South East West As actions location experiments University parameters P 8 abstract models moving given called CZT 4 Brown 21 states The experiments performed range values constructed actions corresponding applicable models floor Computer Science Department Fig 10 shows environment selftransitions result With regard CMFO algorithm uniform error model size problem actually helps large number states virtually guarantees benign This uniform distribution frequent observation correct uniform noise model increased increasing size requires longer walk gather data By smaller partitions noise models created For example partitioning Q pairs pernicious assured It easy significant competitor stated similarity partition noise models satisfy proof regard correct answer terms frequency Indeed reflexivity requirement different second uniform model based hallway structure environment In experiments error model cated partitions locations partitioned world For example UT 4 corner junctions Tjunctions agent noise model partitioned junction South West hallways distinguished East hallways similarity partition error models The compli In models represented hall junctions If sensors conform types corner North In CZT 4 environment type junction type junction locations junction junction considering orientation type Junction partitions For example oriented junctiontype according detect partition reliably able 164 K BJZ et 11 ArtiJiciol Intelligence 72 1995 139I 71 number singleton sults triples elements results number pairs h3 I Results time algorithm The experiments 8 random consisted multiple correct action 0 probability runs algorithm CMFO CIT 4 envi algorithm run simulation correct observa issued random value 0 1 generated compared 0 If ronment For error models different values probability tion P number steps In simulations command value generated greater executed number generated P random ment current state Otherwise Values 0 P 05 I 01 increments varied 1000 10000 The walk length combination map 50 tries combination uniform error model incorrect action chosen uniformly correct action executed After execution second compared P If number generated greater similarity partition ele label current state returned walk length range algorithms overall performance went bad nearly perfect For walk length 8 P algorithm given 20 tries construct incorrect observation returned interval chosen correct Figs 1113 performance algorithm CMFO UT 4 environment junctiontype improves obtained junctiontype error models performance combinations Each figure shows action observation algorithm P lm In oriented partition model performance success theorem This unoriented algorithms performance uniform oriented steadily spectively uncertainty decrease number steps creases The figures provide comparison different error models Although data theorem guarantees In case uniform collected values P 0 beginning error model parame complete result benign nature ters disallowed poor uniform noise model met particular P reaches til requirements model comes closer theorem The unoriented pessimistic model performs closely size 6 In addi fact partitions tion suggested theorem roughly 2 orders magnitude The success algorithm looseness shorter walks sult concerning data gathered different needed algorithm shown different values P infer correct map failed 8 The plateau fig perfect answers failures ures Fig 6 reproduced unexpected separa tion large model smaller walk lengths simulation shorter factors mentioned random walks proof Fig 14 shows assumptions uniform model attributable 10000 steps Comparison right plot shows number steps areas plots trials represent low negative algorithm form Here region occur thousands areas la K Basye et alArtcial Intelligence 72 1995 139171 165 1 e 06 e 07 e 08 e 09 e 10 Fig 11 Number successes plotted function p walk length different values 0 algorithm CMFO CIT 4 environment uniform error model 9 05 9 06 P 1 P 5 20 15 0 9 07 9 08 e 09 e 10 Fig 12 Number successes plotted funcuon walk length different values 0 algorithm CMFO CfT 4 environment oriented junctiontype similarity partition error model K Busye et dArtificial Intelligence 72 1995 139171 167 6 05 t 06 e 07 0 08 steps e 09 e 10 Fig 13 Number successes plotted function p walk algorithm CMFO CIT 4 environment unoriented junctiontype length different values 0 similarity partition error model 168 K Basye et crlArtcicd Intelligence 72 1995 139I 71 CIT4 Unoriented CIT4 Oriented CIT4 Uniform ls Fig 14 Number steps 100 success N 20 plotted function P walk length different values 9 algorithm CMFO CIT 4 environment similarity partition error models right plot 1 s factor Theorem 2 64 Application real mapping In earlier analyses simplifications assume observation sake mathematical movement errors assumptions simplifying independent For example error Despite models relevant variety interesting faced mobile tractability ignore sources systematic lieve Our simulations based problem spatial environment map building based CMFO algorithm Recall nominally threshold This requirement satisfied combining tation CMFO algorithm addition required junction features unique output state observation probability orien type position location The robot equipped structural information In section briefly real robotic available Section 53 tasks environments learning robots 61 K Basye et alArtcial Intelligence 72 1995 139171 169 odometer generate procedures algorithms strategies transducers sonar Robust highlevel movement procedures implemented traversal sensing movement ronment algorithm modified allowed efficient exploration fail operate CIT 4 simulation version presented corresponding procedures designed avoided slightly combinations information simpler actions The robots common hallway envi environment The CMFO modifications taking actions predicted The goal modifications identification development strategies number steps required analysis Section 53 order small This goal achieved The sensing movement environment environment number states allow correct constant robot successfully number times built models 3Q steps 7 Discussion open problems The identification problems discussed thought points large space problems properties Our goal work provide solutions problems problems These space derives directly real world noise unavoidable results provide indications portion space characterized noisy inputs andor outputs Our solving identification set structural interaction representative robotic systems For example proached current direct comparisons ical empirical advantage The robotic mind With regard problems tions allow better results complexity results suggest described having nominally involving problems realistically ap provided problems explored theoret enormous unique labels earlier designed knowledge assump spatial exploration The solutions problems involving nonunique sequences Gill 111 provides outputs generally involve sequences way construct preset sequences exist length exponential distinguishing particular distinguishing number states algorithm Yannakakis preset distinguishing constructive guishing questions algorithm Lee sequence As regards uncertainty resolved remain requires complete description automaton 231 problem determining automaton efficient adaptive distin important determining automaton PSPACEcomplete perception following sequence l Suppose distinguishing sequences exist observation movement uncer tain Are adaptive sequences easy l Suppose agent given adaptive distinguishing underlying automaton observation sequence movement uncertainty Is easy identify I70 8 Conclusions algorithms agents investigates deterministic corresponding linite automaton relatively inputoutput In model identify environment high degree predictive accuracy expediency means simplifying This paper dynamical represented states actions While admit automata provided Biological serve believe require agent interaction environment modeled different automata We claim action automata behavior small number real world modeled requires nature senses routines real world We impossibly hard In addition approach range different aspects interaction given appropriate perceptual terms inferring systems appear abstract considerably equipped robust perceptual huge amounts data available lacking routines learn automaton model makes sense complexity identification represent motor routines learning reduce errors In paper address problem dealing inevitable movement means establishing movement Given model errors approximation A genera1 method dealing errors oc perception easy contend goal polynomial cur perception affect movement time highprobability observation truth basis filtering hypotheses far eluded We provided algorithms work case envi got ronment agent means determining necessarily We provided came case states unique signatures observation noisy algorithms movement landmarks distributed knowing ground indicate In additional theoretical results performed extensive class relatively benign experimental realistic en bounds conservative Ultimately seeking algorithms environmental studies vironments learn highprobability model presence occasional errors In work real mobile robots approaching goal factor size underlying mode1 time small constant correct underlying approximation Acknowledgments Dana Angluin Sean Engelson provided proof Section 51 Oded Maron Evangelos Kokkevis participated discussions participated pointers provided helpful suggestions algorithm results Jeffrey Vitter provided helpful simulation landmark algorithm Philip Klein provided useful reviewers literature random walks graphs Several anonymous algorithms development insights corrections K Basye et alArtcial Intelligence 72 1995 139l 71 171 References 1 I D Angluin On complexity minimum 2 D Angluin Learning 131 JR Bachrach Department MA 1992 Connectionist modeling Computer regular sets queries counterexamples inference regular sets Inf Control 39 1978 337350 I Comput 75 1987 87106 Tech Report 926 Amherst Amherst control finite state environments Information Science University Massachusetts 141 K Basye T Dean JS Vitter Coping uncertainty map learning Proceedings IJCAI89 Detroit MI 1989 151 K Basye T Dean JS Vitter Coping uncertainty map learning Tech Report CS8927 Department Computer Science Brown University Providence RI 1989 I61 KJ Basye Aframeworktr map construction PhD Thesis Department Computer Science Brown University Providence RI 1992 71 T Dean D Angluin K Basye S Engelson L Kaelbling E Kokkevis 0 Maron finite Proceedings AAAI92 Inferring automata stochastic output functions application San Jose CA 1992 map learning 181 T Dean D Angluin K Basye S Engelson L Kaelbling E Kokkevis 0 Maron finite map learning Tech Report CS9227 Inferring automata stochastic output Department Computer Science Brown University Providence RI 1992 functions application 19 T Dean K Basye R Chekaluk S Hyun M Lejter M Randazza Coping uncertainty control navigation exploration Proceedings AAAI90 Boston MA 1990 I 10 I W Feller An Introduction Probability Theory Applications Wiley New York 3d ed 1970 revised printing I II A Gill Stateidentification I 121 EM Gold System I 131 EM Gold Complexity I 141 L Kaelbling K Basye T Dean Learning automaton identification experiments finite automata Inf Comput 4 1961 132154 state characterization Automatica 8 1972 621636 identification given sets IY Control 37 1978 302320 labelled graphs noisy data Proceedings Seventh Yale Workshop Adaptive Learning Systems 1992 I 151 M Mihail Conductance convergence Markov chains combinatorial treatment expanders Proceedings 31st ACM Symposium Foundations Computer Science 1989 I 161 EE Moore Gedankenexperiments sequential machines Automata Studies Princeton University Press Princeton NJ 1956 129153 I 171 L Pitt MK Warmuth The minimum consistent DFA problem approximated polynomial Proceedings 21st Annual ACM Symposium Theoretical Computing 1989 I 181 RL Rivest RE Schapire Diversitybased inference finite automata Proceedings 29th ACM Symposium Foundations Computer Science 1987 191 RL Rivest RE Schapire Inference finite automata homing sequences Proceedings 2lst Annual ACM Symposium Theoretical Computing 1989 1201 RE Schapire The design analysis efficient learning algorithms Tech Report MITLCSTR493 199 I JL McClelland A Cleeremans simple D Touretzky ed Advances Neural Information Processing Vol 1 Morgan sequential Learning structure learning planning reacting based approximating dynamic Proceedings Seventh International Conference Machine Learning 1990 D Lee Testing finite state machines Proceedings 23rd ACM Symposium Computer Science MIT Laboratory I21 D ServanSchreiber recurrent networks Kaufmann San Mateo CA 1989 Integrated architectures 221 RS Sutton programming I23 1 M Yannakakis Theoretical Computing 199 1