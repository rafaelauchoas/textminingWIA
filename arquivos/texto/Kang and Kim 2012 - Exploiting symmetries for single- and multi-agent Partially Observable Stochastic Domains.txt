Artiﬁcial Intelligence 182183 2012 3257 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Exploiting symmetries single multiagent Partially Observable Stochastic Domains Byung Kon Kang KeeEung Kim Department Computer Science KAIST 3731 Guseongdong Yuseonggu Daejeon 305701 Republic Korea r t c l e n f o b s t r c t Article history Received 29 January 2010 Received revised form 26 January 2012 Accepted 26 January 2012 Available online 30 January 2012 Keywords POMDP POSG Symmetry Graph automorphism 1 Introduction While Partially Observable Markov Decision Processes POMDPs multiagent extension Partially Observable Stochastic Games POSGs provide natural systematic approach modeling sequential decision making problems uncertainty computational complexity solutions computed known prohibitively expensive In paper high computational resource requirements alleviated use symmetries present problem The problem ﬁnding symmetries cast graph automorphism GA problem graphical representation problem We demonstrate symmetries exploited order speed solution computation provide computational complexity results 2012 Elsevier BV All rights reserved Markov Decision Processes MDPs classical mathematical framework sequential decision making prob lems agent action decisions based environment states The number steps agent decisions ﬁnite inﬁnite leading ﬁnitehorizon inﬁnitehorizon problems respectively However computationally tractable MDPs shown inadequate successfully model agents noisy perception environment state In order incorporate uncertainty state perception inherent agent extended formalism called Partially Observable MDPs POMDPs emerged 123431 POMDPs provide model singleagent sequential decision making state uncertainty turning decision making problem planning 12 Different MDPs POMDPs provide agent observability states Instead agent infer state based noisy observations This results deﬁning probability distribution states deﬁned belief state represent uncertainty states With single extra assumption computational complexity solving POMDP problem jumps Pcomplete MDP PSPACE complete ﬁnitehorizon POMDPs 23 Solving inﬁnitehorizon POMDPs known undecidable 17 There lot work alleviating intractability means computing approximate solutions One wellknown works shows practicality theoretical guarantees PointBased Value Iteration PBVI Pineau et al 25 PBVI proceeds sampling reachable belief states according heuristics order avoid curse dimensionality induced continuous nature belief states The value backups performed sampled belief states collecting additional belief states The main factor determines performance PBVI belief point selection heuristic The heuristics intended capture reachability belief points avoiding unnecessary computation unreachable beliefs One popular heuristic Greedy Error Reduction Corresponding author Tel 82 42 350 3536 fax 82 42 350 3510 Email addresses bkkangaikaistackr BK Kang kekimcskaistackr KE Kim 00043702 matter 2012 Elsevier BV All rights reserved doi101016jartint201201003 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 33 heuristic samples belief points result largest error bound PBVI belongs class algorithms called pointbased methods value computation performed ﬁnite set belief states called belief points1 Heuristic Search Value Iteration HSVI Smith Simmons 30 pointbased method approximates value function heuristic exploration belief states It maintains upper lowerbound true value function decreases bound gaps recursively selecting belief states The upper bound initialized corners belief simplex maintained point set Update bound performed adding new belief point value computed projection convex hull formed beliefvalue pairs The lower bound vector set meaning value updated newly added belief like updates performed PBVI The belief point added selected depthﬁrst search initial belief Another approach intractability POMDP solution eased practical manner advantage structural regularities present POMDPs One popular method uses concept homomorphism reduce state space forming equivalent model potentially smaller size This technique called model minimization extensively studied MDP domain making use stochastic bisimulation states Informally bisimilar states grouped form smaller state space original MDP optimal policies original reduced MDP directly related The structural characteristics allow state grouping reward equivalence block transition equivalence The property states states group yield reward given action grouped states transition probability group original destination states It known optimal policy reduced MDP lifted converted optimal policy original MDP model reduction results computation 810 A different structural feature automorphism An automorphism model homomorphism By ﬁnding automorphisms symmetries present model expect reduce possibly redundant computation performed symmetric portion solution space It feature propose use POMDPs order reduce computational resources needed computing optimal solutions In particular interested POMDP symmetry related reducing size model nonetheless exploited speed conventional pointbased POMDP algorithms introduced The subject symmetry sequential decision making carried actively exceptions Ravin dran Barto 26 ﬁrst extend model minimization method cover symmetries MDPs More recently Narayanamurthy Ravindran 20 constructively proved problem ﬁnding symmetries MDPs belongs complexity class graph isomorphismcomplete GIcomplete In work authors use graphbased encoding MDP cast problem ﬁnding MDP symmetries graph automorphism GA Our work similar reduce problem discovering GA provides simpler intuitive approach practical guide applying symmetries existing algorithms We extend domain multiagent settings Another work similar permutable POMDPs Doshi Roy 9 This work presented context preference elicitation belief states permutable state distribution Similarly approach present permutable POMDP framework based idea value functions certain classes POMDPs permutable respect state permutation That components value function permuted according permutation corresponding states maintaining value invariance While overall idea league approach important differences First permutable POMDP considers speciﬁc type symmetry preference elicitation problems models similar More speciﬁcally certain preference elicitation problems set exhibit symmetric properties That ﬁrst provide certain conditions state permutation satisfy preference elicitation POMDP parameters set order satisfy stated conditions As opposed setting research aims provide algorithmic framework symmetries discovered exploited general POMDP problems Second symmetry deﬁnition requires equality condition hold n permutations n number states This strict condition suitable limited set problems On hand formulation relaxes restriction considering state action observation permutations groups Partially Observable Stochastic Games POSGs multiagent extension POMDPs actions obser vations collective form agents This change induces leap complexity hierarchy planning ﬁnitehorizon Decentralized POMDPs DECPOMDPs special class POSGs common payoffs known NEXPcomplete 3 Planning inﬁnitehorizon DECPOMDP undecidable DECPOMDPs generalization POMDPs Hansen et al 11 exact algorithm solving POSGs means MultiAgent Dynamic Programming MADP MADP performs dynamic programming backups extended multiagent belief space distribution latent state policies agents In order memory usage check notion dominance prune unnecessary intermediate solutions iteration In paper extended version previous work 13 extend algorithm exploiting symmetries POSGs In particular notion symmetries extended multiagent case affects gametheoretic concepts POSGs 1 In sequel use terms belief points belief states interchangeably 34 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 2 Formal models POMDPs POSGs Before present main results algorithms ﬁrst review preliminaries formal models single multiagent sequential decision making problems partially observable environments paper We deﬁne optimal solutions models representations solutions 21 POMDPs The partially observable Markov decision process POMDP 12 model sequential decision making problems single agent settings It generalization MDP model relaxes assumption agent complete information environment states Formally POMDP deﬁned tuple cid3S A Z T O R b0cid4 S set environment states A set actions available agent Z set possible observations T S A S 0 1 transition function T s s cid6 s state s executing action cid6 P s cid6s denoting probability changing state O S A Z 0 1 observation function O s z P zs denoting probability making obser vation z executing action arriving state s R S A cid7 reward function Rs denotes immediate reward received agent executing action state s b0 initial state distribution b0s denoting probability environment starts state s Since agent directly observe states consider history past actions observations decide current action The history time t deﬁned ht a0 z1 a1 z2 at1 zt The action determined policy π function maps histories actions For ﬁnitehorizon problems assume agent execute actions ﬁnite time steps policy represented policy tree node labeled action execute edge labeled observation agent receive time step Following observation edge agent faces level subtree root node speciﬁes action execute time step The sequence action nodes observation edges traversed executing policy naturally history The history leads deﬁnition belief state probability distribution states given history actions observations bts P st sht b0 Upon executing action receiving observation zt1 belief state bt1 τ bt zt1 time step computed Bayes rule cid3 cid2 cid6 s O s bt1 cid4 cid6 zt1 sS T s s cid6bts P zt1bt P zt1bt cid2 cid6 s O cid5 scid6S cid3cid5 cid2 zt1 T s s cid6 cid3 bts sS The belief state bt constitutes suﬃcient statistic history ht represented Sdimensional vector We redeﬁne policy mapping belief states actions The value policy expected discounted sum rewards following policy starting certain belief state The optimal value function obtained following optimal policy deﬁned recursively given t 1step optimal value function tstep optimal value function deﬁned cid6 V t b max Rb γ P zb aV t1 cid7 cid2 cid3 τ b z cid5 z Z 1 Rb cid4 s bsRs γ 0 1 discount factor BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 35 22 POSGs The partially observable stochastic game POSG 311 extension POMDP framework multiagent settings More formally POSG n agents deﬁned tuple cid3I S b0 Ai Z T O R icid4 I ﬁnite set agents indexed 1 n S ﬁnite set environment states b0 initial state distribution b0s probability environment starts state s Ai ﬁnite set actions available agent Also set joint actions speciﬁed cid9A Z ﬁnite set observations available agent Similarly set joint observations deﬁned cid9Z T transition function T s cid9a s cid6s cid9a probability resulting state s cid6 P s iI Ai cid8 cid6 cid8 iI Z executing joint O observation function O s cid9a cid9z P cid9zcid9a s probability making joint observation cid9z executing action cid9a state s joint action cid9a arriving state s R individual reward function R cid9a denotes reward payoff received agent joint action cid9a executed state s If restrict agent share individual reward function model Decentralized POMDP DECPOMDP 3 In POSGs agent independently makes decision based local information available agent The local information time t agent represented local history hit ai0 zi1 ai1 zi2 ait1 zit actions ai observations zi set Ai Z respectively The local policy strategy πi executed agent essentially mapping local histories local actions A joint policy set local policies agent Algorithms POSGs ﬁnd joint policy set local policies cid9π π1 πn agent solution concepts Nash equilibrium correlated equilibrium In case DECPOMDPs agents cooperate algorithms search optimal joint policy maximizes expected sum rewards planning horizon The agents POSGs reason agents policies true state collectively affect rewards state transitions value This leads deﬁnition multiagent belief state probability distribution hidden states agents policies 19 Hence dynamic programming methods POMDPs involve belief states value vectors deﬁned states methods POSGs involve multi agent belief states value vectors deﬁned joint space states agents policies Thus dimension S cid9Πi cid9Πi set policies policy π Πi agent exists value vector V π agents agent In paper focus ﬁnitehorizon problems assume local policy represented decision tree Formally agent tstep value function executing policy π executing policy cid9πi deﬁned V π cid9πi R cid9a cid9π γ O s cid9a cid9π cid9z cid5 cid9z cid9Z cid2 cid5 T scid6S cid3 s cid9a cid9π s cid6 V π zi it1 cid2 cid3 cid9πicid9zi cid6 s 2 cid5 cid5 cid9π π cid9πi joint policy formed π agent cid9πi agents cid9a cid9π joint action current time step prescribed policy cid9π π zi t 1step local policy agent observation zi cid9πicid9zi t 1step joint policy agents observation cid9zi For given multiagent belief state bi agent value executing local policy π deﬁned V π itbi bis cid9πiV π cid9πi 3 sS cid9πi cid9Πi 3 Solution methods In section brieﬂy review important solution techniques POMDPs POSGs There exists wealth literature presenting algorithms matter paper discuss pointbased value iteration PBVI 25 POMDPs multiagent dynamic programming MADP 11 POSGs discussed later sections 31 PBVI POMDPs The deﬁnition optimal value function Eq 1 leads dynamic programming update obtain tstep t1 The dynamic programming update optimal value function V represented backup operator H value functions given belief state b t 1step optimal value function V t 36 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Table 1 The PBVI algorithm Require B init initial set belief states K maximum number belief state expansions T maximum number backups B B init Γ k 1 K t 1 T Γ BACKUPB Γ end B new EXPANDB Γ B B B new end Return Γ cid6 V tb H V t1b max Rb γ cid5 z Z P za bV t1 cid2 cid3 τ b z cid7 Since belief states provide suﬃcient statistic histories treated states continuous MDP belief state MDP One shortcoming approach belief state continuous simply use tabular representation value functions discrete state space MDPs naively performing backup operation possible belief state intractable However Sondik 31 pointed value function horizon t represented set Γt α0 αm αvectors value particular belief state b calculated cid5 V tb max αΓt αsbs sS The construction Γt carried series intermediate Γ generation cid11 s Rs cid5 αa cid12 Γ cid13 cid9 t cid2 cid3 cid6 cid2 cid6 O s z cid3 cid2 cid3 cid6 s αi αi Γt1 Γ az t αaz s γ T s s scid6S cid10 cid10 αa cid10 cid10 cid10 αaz cid14 z Z Γ az t Γ t t Γ cid15 Γt Γ t A crosssum operator sets A B deﬁned A B b A b B However Γt order O AΓt1 Z worst case leading high computational cost The doubly exponential growth Γt t alleviated pruning dominated αvectors possible belief states effect pruning limited practice This mainly fact backup possible belief states Pointbased value iteration PBVI 25 attempts limit growth performing backups ﬁnite set B reachable belief states Hence ﬁnding Γt V t PBVI constructs Γ ab A b B elements calculated t cid12 Γ ab t αa b cid10 cid10 cid10 αa bs Rs cid16 cid5 argmax αΓ az t z Z cid13 cid17 α b s ﬁnally compute best action belief state cid12 Γ B t α cid2 cid10 cid10 α argmax Γ ab A αa t b αa b b cid3 b B cid13 Using Γt Γ B approximation V t policy simply takes form choosing action associated t argmaxαΓt α b Table 1 outlines PBVI The BACKUP routine refers process creating Γ B described The EXPAND routine characterizes heuristic aspect PBVI task collect reachable belief states given set B beliefs Heuristics EXPAND include greedy error reduction belief states reduce expected error bound greedily chosen stochastic simulation explorative action belief states reduce maximum distance sampled belief states greedily chosen In later sections modify BACKUP routine order exploit symmetries POMDPs t BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 37 32 MADP POSGs Hansen et al 11 propose multiagent dynamic programming MADP algorithm POSGs The dynamic programming update MADP consists stages ﬁrst enumerating tstep policies t 1step policies evaluating policies obtain value functions eliminating policies useful multiagent belief state Note multiagent value function Eq 3 represented set S cid9Πidimensional vectors While dynamic programming methods POMDPs PBVI involve belief states value vectors deﬁned environment states methods POSGs involve multiagent belief states value vectors deﬁned joint space environment states agents policies Hence dimension value vectors vary policy eliminated second stage dynamic programming update A convenient way represent value prepare value vector joint policy cid9π cid9Πt state value vectors belief vectors ﬁxed dimension S cid3 cid3 cid5 cid5 cid2 cid3 cid2 cid3 cid2 cid2 V cid9π R s cid9a cid9π γ O s cid9a cid9π cid9z T s cid9a cid9π s cid6 cid9π cid9z it1 V cid6 s cid9z cid9Z The corresponding value function speciﬁc belief b 0 1S scid6S V cid9π itb cid5 sS bsV cid9π 4 5 Notice given Eq 2 convert Eq 4 concatenating cid9πi π construct joint policy cid9π Also given joint policy state belief vector dimension S computed horizon t based given initial state distribution b0 actionobservation history time t Thus Eq 3 represented Eq 5 We use Eq 5 represent value rest section ease exposition Given set cid9Πt1 Π1t1 Πit1 Πnt1 t 1step joint policies value vectors V cid9π it1 cid9π cid9Πt1 ﬁrst stage dynamic programming update exhaustively generates Πit Πit1 agent set tstep local policies agent Assuming tree representations policies tstep local policy agent created preparing Ai root action nodes appending possible combinations t 1 step local policies observation edges root action node The number exhaustively generated tstep local Combining Πit agents yields set tstep joint policies cid9Πt size policies Πit AiΠit1 Z Π1tΠ2t Πit Πnt The ﬁrst stage dynamic programming update concluded computing values joint policies V cid9π cid9Πt agent Eq 5 cid9π With necessary policy backup value computation completed update continues second stage weakly dominated policies pruned A local policy π agent said weakly dominated agent decrease value switching local policy maintain local policies exists cid9πi cid9Πit switching away π strictly increases agent value A weakly dominated policy weak dominance relation holds existence requirement strict improvement value The test weak dominance local policy π agent determined checking existence probability distribution p policies Πitπ cid5 π cid6Πit π cid3 cid2 p π cid6 π cid6 cid9πi V s cid2 V π cid9πi s s S cid9πi cid9Πit 6 π cid9πi value vector joint policy formed π agent cid9πi agents If exists V distribution π prunable possible agent stochastic policy determined p achieving value worse π This test dominance carried linear programming LP A weakly dominated policy safely pruned concern loss value The pruning proceeds iterated fashion agent alternately tests dominance prunes accordingly This iteration stops agent prune local policies Table 2 outlines MADP algorithm computing set T step joint policies Note algorithm requires additional computation select joint policy depending solution concept Nash equilibrium For DEC POMDPs assume cooperative settings joint policy maximum value initial state distribution b0 selected optimal joint policy 4 Symmetries POMDPs POSGs In section symmetries deﬁned POMDPs POSGs We ﬁnding symmetries botch cases graph isomorphism complete GIcomplete problem complexity class ﬁnding automorphisms general graphs We present graph encoding given POMDP POSG order apply algorithms ﬁnding graph automorphisms We POMDP POSG algorithms extended exploit symmetries discovered models 38 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Table 2 The MADP algorithm Require Πi0 Vi0 cid90 initial value function agent t 1 T The ﬁrst stage dynamic programming backup 1 n Perform backup t 1step local policies Πit1 produce exhaustive set tstep local policies Πit end Let cid9Πt Π1t Πit Πnt cid9π cid9Πt Compute V cid9π Eq 5 add value vector Vit end The second stage dynamic programming backup repeat 1 n π Πit Prune π weakly dominated Eq 6 end end local policy pruned loop end return Sets T step policies ΠiT corresponding value vectors ViT agent 41 Deﬁnition symmetries POMDPs There number works past advantage underlying structure decision theoretic plan ning models Perhaps extensively studied types structural regularities homomorphism It directly related abstraction model minimization techniques try reduce size model cid6 maps actions φ Z Z Z A homomorphism φ POMDP deﬁned cid3φS φ A φ Z cid4 φS S S cid6 function maps states φ A cid6cid4 cid6 cid3S A A reduced model M mappings manytoone Because property model minimization methods POMDPs search homomorphism φ maps M equivalent POMDP M minimal model size Depending deﬁnition homomorphism φ obtain different deﬁnitions minimal model maps observations Note mapped POMDP M cid6 O cid6 A cid6 R cid6 Z cid6 T A simple extension MDP model minimization 10 POMDPs leads homomorphism φ form cid3φS 1 1cid4 1 φS satisfy following constraints denotes identity mapping In order hold equivalence M M cid2 cid5 cid3cid3 cid2 cid2 cid3 cid6 cid6 cid6 cid6 T cid6 R cid6 O φS s φS s cid6 T s s scid6cid6φ 1 S scid6 cid6cid6 cid2 cid3 φS s cid2 Rs cid3 φS s z O s z Pineau et al 24 extend approach case task hierarchy given expert achieve reduction state space actions irrelevant task hierarchy Wolfe 36 extends minimization method compute homomorphism general form cid3φS φ A φ Z cid4 observation mapping φ Z change depending action The constraints equivalence given cid2 cid6 T φS s φ Aa φS cid3cid3 cid2 cid6 s cid5 cid2 T s s cid3 cid6cid6 scid6cid6φ 1 S scid6 cid2 cid3 φS s φ Aa cid2 φS s φ Aa φa Rs cid3 Z z cid6 R cid6 O O s z Note methods interested ﬁnding manytoone mappings order ﬁnd model reduced size Hence focus computing partitions state action observation spaces blocks represent aggregates equivalent states actions observations respectively Once partitions employ conventional POMDP algorithms abstract POMDP reduced number states actions observations effect reduces computational complexities algorithms In paper interested automorphism special class homomorphism Deﬁnition 1 An automorphism φ deﬁned cid3φS φ A φ Z cid4 state mapping φS S S action mapping φ A A A observation mapping φ Z Z Z onetoone mappings satisfying BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 39 cid6 s s sLEFT s sRIGHT T s aLISTEN s cid6 T s aLEFT s cid6 T s aRIGHT s cid6 sLEFT sRIGHT sLEFT sRIGHT sLEFT sRIGHT 10 00 00 10 05 05 05 05 05 05 05 05 Fig 1 Transition probabilities tiger domain O s aLISTEN z O s aLEFT z O s aRIGHT z z s sLEFT s sRIGHT zLEFT 085 015 zRIGHT zLEFT zRIGHT zLEFT zRIGHT 015 085 05 05 05 05 05 05 05 05 Fig 2 Observation probabilities tiger domain s sLEFT s sRIGHT Rs aLISTEN 1 1 aLEFT 100 10 aRIGHT 10 100 Fig 3 Reward function tiger domain cid3cid3 cid2 T s s cid3 cid6 T O s z O cid6 cid2 cid2 s φS s φ Aa φS cid2 cid3 φS s φ Aa φ Z z cid2 cid3 φS s φ Aa Rs R Hence φ maps original POMDP assumption reduction size model The classic tiger domain 12 best examples automorphisms POMDPs The state space S tiger domain deﬁned sLEFT sRIGHT representing state world tiger left door right door respectively The action space A deﬁned aLEFT aRIGHT aLISTEN representing actions opening left door opening right door listening respectively The observation space Z deﬁned zLEFT zRIGHT representing hearing sound tiger left door right door respectively The speciﬁcations transition probabilities observation probabilities rewards given Figs 1 2 3 The initial belief given b0sLEFT b0sRIGHT 05 Note tiger domain compact sense minimization methods previously mentioned reduce size model examining reward function aggregate aLEFT aRIGHT rewards different depending current state sLEFT sRIGHT By similar argument reduce state space observation space However sLEFT sRIGHT interchanged yield equivalent POMDP simultaneously changing corre sponding actions observations cid12 φS s cid18 s sLEFT s sRIGHT sRIGHT sLEFT aLISTEN aLISTEN aRIGHT aLEFT aLEFT aRIGHT z zLEFT z zRIGHT φ Aa cid12 φ Z z zRIGHT zLEFT Furthermore property yields symmetries belief states αvectors tiger domain seen Fig 4 The automorphism POMDPs type regularity intend discover exploit paper symmetry model necessarily help model minimization algorithm reduce size model Hence computing partitions focus computing possible automorphisms original POMDP Note original POMDP reduced size exponentially automorphisms number blocks partition For example model minimization yields state partition K blocks 2 states number automorphisms 2K Hence advisable compute automorphism compute minimal model POMDP 40 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Fig 4 Value function tiger domain obtained PBVI 5 belief states b1 b5 symmetric corresponding αvectors α1 α5 symmetric The argument applies b2 b4 Although illustration uses approximate value function computed PBVI value function exact methods phenomenon 42 Properties symmetries POMDPs As shown tiger domain automorphisms POMDPs reveal symmetries present belief states α vectors given POMDP M automorphism φ cid3φS φ A φ Z cid4 let Γ set αvectors optimal value function In setting provide following theorems exploited computing solution given POMDP By slight abuse notation vector v dimension S let φS v transformed vector elements permuted φS Theorem 1 If b reachable belief state φS b reachable belief state Proof First given φ cid3φS φ A φ Z cid4 note zs b ba φ A φZ z cid2 cid3 φS s cid6 O s z O φS s φ Aa φ Z z This cid6 T φS s φ Aa φS s automorphism ensures T s s means symmetric image reachable belief vector b φS b reachable initial belief b0 executing symmetric policy action mapped φ Aa In words b reachable initial belief state b0 executing policy tree φS b reached executing policy tree action nodes relabeled φ Aa observation edges relabeled φ Z z cid2 Theorem 2 If α Γ φS α Γ Proof We prove induction horizon t Γt By deﬁnition automorphism Rs RφS s φ Aa Hence α Γ0 φS α Γ0 Suppose argument holds Γt1 This implies α Γ az φS α Γ φ A aφ Z z t t deﬁnition Γ az If t α Γt deﬁnition b cid2 cid5 αs αa bs Rs αcid6 b cid3 argmax αcid6Γ az t z Z Consider symmetric image deﬁned cid2 cid2 cid5 cid3 φS s α R cid3 φS s φ Aa argmax φZ z Z φ A aφZ z αcid6cid6Γ t cid2 cid3 αcid6cid6 φS b For observation φ Z z argmax select αcid6cid6 b Hence φS α Γt cid2 symmetric image αcid6 selected argmaxαcid6Γ az t αcid6 In work specialize PBVI algorithm exploit symmetries shown later sections However theorems provide general applied variety different value functionbased algorithms We BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 41 Joint action a1LISTEN a2LISTEN Other sLEFT sRIGHT sLEFT sLEFT sRIGHT sLEFT sRIGHT sRIGHT 0 05 1 05 0 05 1 05 Fig 5 State transition probabilities DecTiger domain The second row shows transition probabilities joint actions composed nonlisten individual action argue unifying theme value functionbased algorithms dependence αvectors andor belief points theorems presented indicate symmetric images sampled belief points αvectors contribute equivalently overall value For example randomized pointbased backup Perseus 32 beneﬁt results having perform redundant backup operation symmetric beliefs Symmetries exploited search based methods HSVI Forward Search Value Iteration FSVI 29 similar manner In particular multiple backups performed single sampled belief point taking symmetric image sampled belief The gist different value functionbased methods provide different sampling approaches framework universally applied enhance sampling procedure 43 Deﬁnition symmetries POSGs Extending deﬁnition POSGs introduces agenttoagent mappings local actions observations agent mapped agent Formally automorphism POSGs deﬁned follows Deﬁnition 2 An automorphism φ agent POSG tuple cid3φI φS φ cid9A φ cid9Z I agent mapping φI I I state mapping φS S S action mappings φ Ai φ Z Z ZφI bijections satisfying cid3cid3 cid2 cid2 cid3 cid2 cid4 φ cid9A φ Ai I φ cid9Z Ai AφI observation mappings φ Z cid6 s cid9a s T T O s cid9a cid9z O R cid9a RφI cid9a cid9z cid6 cid6 φS s φ cid9Acid9a φS s cid2 cid3 φS s φ cid9Acid9a φ cid9Z cid9z cid2 cid3 φS s φ cid9Acid9a s s A special case agent mapping φI identity mapping φ said intraagent automorphism On hand φI nonidentity mapping said interagent automorphism Informally speaking interagent automorphism allows interchanging agents long local actions observations interchanged accordingly On hand intraagent automorphism conﬁned interchanging local actions observations agent It thought intraagent automorphism captures symmetry present singleagent POMDP level interagent automorphism extends symmetry multiagent level To illustrate present decentralized tiger DecTiger domain 19 DecTiger multiagent extension classical tiger domain There agents setting agent set I 1 2 sequence decisions open door jointly separately listen The states tiger domain sLEFT sRIGHT Each agent set actions equivalent single agent case aiLISTEN aiRIGHT aiLEFT 1 2 ai X indicates action X agent The observation space duplicated singleagent case ziLEFT ziRIGHT 1 2 notations deﬁned similarly If agent performs open action state resets 05 probability If continue listen action change state Each agent individually observes tiger correct room probability 085 performing listen action When agents perform joint listen action resulting joint observation probability computed product individual probabilities All joint actions agent performs nonlisten action result uniform distribution joint observations Rewards given equally agents designed encourage cooperation The maximum reward attained cooperatively opening tigerfree room If agent chooses different room high penalty given If cooperatively open tiger room suffer penalty lesser cost Jointly listening costs small penalty opening tigerfree room agent listens result small reward If hand agent opens tiger room listening incur worst possible penalty The transition probabilities observation probabilities rewards summarized Figs 5 6 7 respectively One possible symmetry exhibits interagent mapping presented Fig 8 For complete list symmetries DecTiger invite reader consult Fig 15 Section 7 report experimental results 42 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Joint observation z1LEFT z2LEFT z1LEFT z2RIGHT z1RIGHT z2LEFT z1RIGHT z2RIGHT sLEFT 07225 01275 01275 00225 sRIGHT 00225 01275 01275 07225 Fig 6 Observation probabilities DecTiger domain joint action a1LISTEN a2LISTEN The probabilities joint actions uniform omitted Joint action a1RIGHT a2RIGHT a1LEFT a2LEFT a1RIGHT a2LEFT a1LEFT a2RIGHT a1LISTEN a2LISTEN a1LISTEN a2RIGHT a1RIGHT a2LISTEN a1LISTEN a2LEFT a1LEFT a2LISTEN sLEFT sRIGHT 20 20 0 0 100 100 100 100 2 2 9 9 9 9 101 101 101 101 0 0 20 20 100 100 100 100 2 2 101 101 101 101 9 9 9 9 Fig 7 Individual rewards DecTiger domain cid12 φI Agent 2 Agent 1 Agent 1 Agent 2 φS s Identity mapping φ A1 φ A2 cid12 φZ1 z cid12 φZ2 z a2LISTEN a1LISTEN a1RIGHT a2RIGHT a1LEFT a2LEFT a1LISTEN a2LISTEN a2RIGHT a1RIGHT a2LEFT a1LEFT z z1LEFT z z1RIGHT z z2LEFT z z2RIGHT z2RIGHT z2LEFT z1RIGHT z1LEFT Fig 8 An example interagent symmetry DecTiger 44 Properties symmetries POSGs As case POMDPs symmetries POSGs reveal useful regularities present model In section formally state properties symmetries POSGs extend MADP later sections Again slight abuse notation extend domain φ local joint policy trees output policy tree actions observations permuted accordingly That φπ policy tree π permuted policy tree action nodes mapped π A observation edges permuted π Z Theorem 3 Given automorphism φ cid3φI φS φ cid9A φ cid9Z cid4 V cid9π V φ cid9π φI cid2 cid3 φS s s time steps 1 cid3 t cid3 T Proof We prove induction t For t 1 immediate reward matters V cid9π i1s R cid9a RφI cid2 cid3 φS s φ cid9Acid9a V φ cid9π φI i1 cid2 cid3 φS s The ﬁrst equalities follow fact 1step policy tree simply single action node The second equality holds deﬁnition automorphism Assume theorem holds ts t k 1 policy trees depth k 1 For t k Bellman equation unfolds BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 R cid9a γ cid5 cid2 T scid6Scid9z cid9Z cid2 cid3 cid6 O cid6 s cid9a cid9z cid3 s cid9a s cid9π cid9z ik1 V cid3 cid2 cid6 s RφI cid2 cid3 φS s φ cid9Acid9a γ cid5 cid6S φS s φ cid9Z cid9z cid9Z 43 T φS s φ cid9Acid9a φS s O φS s V cid6 cid6 φ cid9Acid9a φ cid9Z cid9z cid6 φS s φ cid9π φ cid9Z cid9z φI ik1 All terms V shown equal deﬁnition automorphism The equality nextstep value term established inductive hypothesis subtrees k 1level subtrees encountered following cid9z cid9π symmetric ones encountered following φ cid9Z cid9z φ cid9π Therefore equality holds t cid2 1 cid2 Because Theorem 3 holds values t henceforth drop horizon superscript t possible Based theorem following statement weak dominance presence symmetries Theorem 4 If local policy π agent weakly dominated local policy φπ agent φI weakly dominated automorphism φ Proof From Eq 6 local policy π agent weakly dominated exists probability distribution p local policies Πiπ cid2 π cid6 s s cid9πi cid9Πi s cid2 V cid5 V p cid3 π cid6 cid9πi π cid9πi π cid6Πi π Consider local policy φπ agent φI In order prove φπ weakly dominated exists probability distribution p π cid6cid6 cid9πφI φI agent φI local policies ΠφI iφπ s s cid9πφI cid9ΠφI φπ cid9πφI φI s cid2 V π cid6cid6 cid5 V p cid2 cid3 cid6 cid6 π cid6cid6ΠφI iφπ Note local policy π cid6 ΠφI iφπ ﬁnd π cid6 Πiπ π cid6cid6 φπ cid6 φ bijective If set p π cid6cid6 φπ cid6 probability distribution p agent corresponds local policy φπ cid6 agent φI Hence π cid6cid6 cid6π cid6cid6 pπ cid6 satisﬁes inequality cid2 cid6 From Theorem 4 follows policy tree symmetric images pruned loss value known weakly dominated Corollary 1 If policy π pruned φπ pruned As case POMDPs adopt MADP demonstrate usefulness symmetries POSGs While approach algorithmspeciﬁc argue theoretical basis exploitations general applied algorithms For example signiﬁcant work solving DECPOMDPs recent years including Bounded Policy Iteration BPI 4 MemoryBounded Dynamic Programming MBDP 28 Heuristic Policy Iteration HPI 2 PointBased Bounded Policy Iteration PBBPI 14 PointBased Policy Generation PBPG 37 Constraint Based Policy Backup CBPB Team Decision problem based Policy Iteration TDPI 15 These algorithms share common computational steps exhaustive partial dynamic programming backup policies pruning dominated policies improving policies mathematical programming The theoretical results reduce number policies generated dynamic programming backup number mathematical programs solve We apply recent results exploiting symmetries reduce sizes mathematical programs 5 details left future work The symmetries impacts game theoretic analysis given POSG To facilitate discussion convert given POSG normal form game We adhere term policy sake consistency strategy widely adopted game theory As pointed Hansen et al 11 POSG time horizon t converted normal form game enlisting policy trees possible actions We include initial state distribution order scalar payoffs Sdimensional vector payoffs This taking inner cid9π initial state distribution b0 This inner product payoff entry product value vector V converted game We denote payoff joint policy cid9π agent ui cid9π equivalently uiπi cid9πi It follows ui cid9π uφI iφ cid9π Eq 7 cid5 cid5 b0sV s cid9π s s cid2 cid3 φS s b0 V φ cid9π φI cid2 cid3 φS s 7 44 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 For discussion symmetries game theoretic solution concepts begin Nash equilibrium A pure strategy Nash equilibrium joint policy ﬁxed agent agent incentive unilaterally switch policy provided change theirs Proposition 1 If joint policy cid9π Nash equilibrium normal form representation given POSG symmetric image φ cid9π constitutes Nash equilibrium Proof Given Nash equilibrium cid9π following inequality holds deﬁnition cid11cid3 cid11cid3 cid2cid9 cid2cid9 ui cid9π π cid2 ui πi cid9π πi cid13 π The automorphism guarantees ui cid9π uφI iφ cid9π joint policy cid9π Therefore uφI iφ cid9π cid2 uφI iφπi φ cid9π πi cid13 π establishes fact φ cid9π Nash equilibrium cid2 Proposition 1 easily generalizes mixedstrategy Nash equilibrium Note notion symmetries generalize deﬁnition classical symmetric games requires exists invariant action mapping φ A observation mapping φ Z possible permutations agents Our theoretical results making game solvers scalable widening applicability techniques Cheng et al 7 The facts presented section lead eﬃcient procedure ﬁnding equilibria symmetric POSGs Instead searching single equilibrium present POSGs speed process applying symmetries POSGs equilibria discovered The correlated equilibrium CE 21 generalizes mixedstrategy Nash equilibrium Whereas mixedstrategy Nash equilibrium deﬁned independent probability local policies CE probability joint policies allowing dependencies agents local policies That probability p joint policies CE cid5 cid9πi p cid9π ui cid9π cid2 cid5 cid9πi p cid9π ui cid2cid9 π cid6 cid9πi cid11cid3 π cid6 cid13 πi 8 With symmetries present normal form game representation POSG prove symmetric property CE Proposition 2 Let p CE normal form representation given POSG Then exists possibly CE p cid6φ cid9π p cid9π automorphism φ given POSG joint policy cid9π p cid6 Proof Given CE p rewrite Eq 8 cid2 cid5 cid5 p cid9π uφI cid3 φ cid9π cid2 p cid9π uφI cid2cid9 cid2 φ cid3 π cid6 cid11cid3 φ cid9πi π cid6 cid13 πi cid9πφI Note π cid6 probability φ cid9π chosen Therefore exists CE assigns probability p cid9π φ cid9π cid2 cid13 φπi φ bijective This modiﬁed form states p cid9π cid13 πi φπ cid6 cid9πφI 5 Symmetry discovery models In section ﬁnding symmetries present POMDPs POSGs graph isomorphism GI com plete problem computational complexity class ﬁnding automorphism groups general graphs We present graph encoding given POMDP POSG order use graph automorphism algorithm ﬁnding symmetries model 51 Graph encoding POMDP We ﬁrst cast problem ﬁnding automorphisms POMDPs ﬁnding automorphisms graphs Speciﬁcally encode given POMDP vertexcolored graph automorphism graph corresponds automorphism POMDP Our approach prove useful discuss computational complexity discovering POMDP automorphisms later section A vertexcolored graph G speciﬁed cid3V E C ψcid4 V denotes set vertices E denotes set edges cid3v v jcid4 C set colors ψ V C denotes color associated vertex An automorphism φ V V permutation V property edge cid3v v jcid4 E cid3φv φv jcid4 E vertex v V ψv ψφv We encode POMDP vertexcolored graph order apply graph automorphism algorithms The encoded graph composed following classes vertices edges counts presented parentheses BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 45 Fig 9 Encoding tiger domain vertexcolored graph Two vertices color shapes ﬁllings States S vertices state s prepare vertex v s vertex share unique color s S ψv s cstate A ψva caction Actions A vertices action prepare vertex va vertex share unique color Next states S vertices S edges state s cid6 unique color s cid6 S ψv scid6 cstatecid6 We connect nextstate vertex v scid6 state vertex v s s cid6 s Observations Z vertices observation z prepare vertex v z vertex share unique prepare vertex v scid6 vertex share color z Z ψv z cobs cid6 z cid6 cid6cid6 cid6 s cid6 T s Transition probabilities S2 A vertices 3S2 A edges triplet s s cid6 prepare vertex v T sascid6 cid6 assign colors vertices share color represents transition probability T s s cid6cid6cid6 cid6cid6cid6 ψv T sascid6 ψv T scid6cid6acid6scid6cid6 iff T s s cid6 s transition probabilities s s We connect transition probability vertex v T sascid6 corresponding state action nextstate vertices v s va v scid6 Observation probabilities S A Z vertices 3S A Z edges triplet s z prepare vertex v O saz represents observation probability O s z assign colors vertices share color cid6 ψv O saz ψv O scid6acid6zcid6 iff O s z observation probabilities s z s cid6 We connect observation probability vertex v O saz corresponding state action observation O s vertices v s va v z Reward function S A vertices 2S A edges pair s prepare vertex v Rsa represents reward Rs assign colors vertices share color rewards cid6 We connect reward vertex v Rsa corresponding s s state action vertices v s va cid6 ψv Rsa ψv Rscid6acid6 iff Rs Rs Initial state distribution S vertices S edges state s prepare vertex vb0s represents initial state probability b0s assign colors vertices share color initial state cid6 We connect initial state probability vertex probabilities s s vb0s corresponding state vertex v s ψvb0s ψvb0scid6 iff b0s b0s cid6cid6 cid6 cid6 cid6 cid6 z cid6 s cid6 The graph encoding process mechanical colors edges carefully prepared order preserve equiv alence model graph automorphism Fig 9 shows result graph encoding process tiger domain The encoded graph sparse consisting O S2 A Z vertices O S2 A Z edges number edges linear number vertices Despite superpolynomial running time worst case typical graph automorphism solvers eﬃcient sparse graphs As report Section 7 nauty 18 graph automorphism solver quickly automorphisms encoded graphs benchmark POMDP domains 6 107 vertices As minor remark note choose colors ψv T sascid6 cid13 ψv O saz T s s cid6 O s z This prevent transition probability permuted observation probability vertices Similar restrictions apply vertices different classes 46 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 52 Graph encoding POSG Similar POMDP case problem ﬁnding POSG automorphisms reduced ﬁnding automorphism group properly encoded graph The graph encoding use different POMDP approach exception vertices reﬂect multiagent aspects The encoded graph composed following classes vertices edges Agents I vertices prepare vertex agent assigning unique color States S vertices prepare vertex state assigning unique color Next states S vertices S edges prepare vertex state assigning unique color different color state vertices We connect nextstate vertex corresponding state vertex permuting state vertices yields permuting nextstate vertices order Actions Ai vertices Ai edges prepare vertex action assigning unique color We cid4 cid4 cid4 cid4 connect action vertex corresponding agent vertex represent agent action available Observations Z edges prepare vertex observation assigning unique color We connect observation vertex corresponding agent vertex represent agent observation available Z vertices Transition probabilities S2 Ai edges prepare vertex transition prob ability assigning unique color probability We connect transition probability vertex corresponding state nextstate action vertices Ai vertices I 2S2 cid8 cid8 Observation probabilities S Ai Z edges prepare vertex obser vation probability assigning color probability We connect observation probability vertex corresponding state action observation vertices Ai Z vertices 2I 1S Individual reward functions IS Ai edges prepare vertex individual reward assigning color reward We connect reward vertex corresponding agent state action vertices Ai vertices I 2IS cid8 cid8 Initial state distribution S vertices S edges prepare vertices corresponding vb0s way cid8 cid8 POMDPs The resulting graph O IS2 share reward function O IS2 number vertices Ai Z vertices O I2S2 cid8 cid8 cid8 Ai Z edges For DECPOMDPs agents Ai Z edges number edges linear 53 Computational complexity A recent study computational complexity ﬁnding MDP symmetries 20 showed problem ﬁnding symmetries given MDP polynomially reduced problem ﬁnding automorphisms corresponding graph encoding Hence known computational complexity ﬁnding symmetries MDP belongs graph isomorphismcomplete GIcomplete class In section extend result MDPs POMDPs POSGs taking similar slightly different approach For ease exposition provide lemmas useful proving main theorem results POSGs We use following deﬁnitions proof ﬁrst lemma Deﬁnition 3 Given POSG M G M denotes vertexcolored undirected graph representation M The model vertices G M vertices corresponding state action observation agents M The parameter vertices G M ones corresponding transition observation reward functions M We adjust notations symmetries order prevent confusion Symmetries pertaining graphs denoted φG G subscript symmetries POSGs retain notations introduced Deﬁnitions 1 2 Lemma 1 A symmetry M corresponds unique automorphism G M vice versa Proof First assume symmetry φ M given From prove existence unique automorphism φG G M To construct unique φG φ proceed ﬁrst mapping model vertices according φ For example given action vertex vai set φG vai vφ Aφi aφi Note mapping agent vertices simultaneously maintains edge connectivity corresponding action observation vertices mapped accordingly Next permute parameter vertices connected model vertices This permutation possible φ pre serves probabilities rewards corresponding vertex colors To specify permutations cid6cid4 By construction G M parameter vertices consider pair tuples t1 cid3s cid9a cid9z s cid6 respectively share vertices v T scid9ascid6 v T φS sφ cid9A cid9aφS scid6 corresponding T s cid9a s cid6cid4 t2 cid3φS s φ cid9Acid9a φ cid9Z cid9z φS s cid6 T φS s φ cid9Acid9a φS s BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 47 color probabilities equal φ This assertion holds arbitrary choice t1 deﬁnition POSG symmetry The components connected relevant parameter vertices participating model vertices Therefore φG v T scid9ascid6 v T φS sφ cid9A cid9aφS scid6 preserves color edge constraints graph automorphism The argument applies observation probability reward vertices completing construction φG The construction φG tailored speciﬁc φ different φcid6 cid13 φ POSG symmetries bijections Thus φG speciﬁc φ To direction assume given φG G M Consider tuples t1 cid3v s vai cid3φG v s φG vai φG v zi φG v scid6 cid4 The set vertices tuples run agents I The vertices v s v scid6 vai t1 connected transition probability vertex v T scid9ascid6 corresponds T s cid9a s concatenating actions corresponding vertices vai v T φS sφ cid9A cid9aφS scid6 t2 Because v T scid9ascid6 triple s cid9a s v T φS sφ cid9A cid9aφS scid6 It follows following equalities hold v scid6 cid4 t2 cid6 cid9a joint action formed The analogue holds transition probability vertex cid6 vertices t2 connected v zi φG v T scid9ascid6 v T φS sφ cid9A cid9aφS scid6 cid3 cid2 φG v T scid9ascid6 ψ ψv T φS sφ cid9A cid9aφS scid6 If true exist vertex v T cid6cid6 mapped v T scid9ascid6 Then property graph auto morphism corresponding vertices t2 connected v T cid6cid6 vertices t1 mapped t2 However contradiction way G M constructed vertices t2 connected cid6 transition probability vertices Therefore exists automorphism φ T s cid9a s φG v s vφS s φG vai vφ cid9A aφI I ai Ai s S The analogous equalities observation reward functions proved similarly Furthermore similar proof reverse direction φ unique given φG φG bijection cid2 cid6 T φS s φ cid9Acid9a φS s We given vertexcolored undirected graph G construct POSG automorphism G corresponds unique symmetry POSG vice versa The constructed POSG consists single agent action observation Each state POSG corresponds vertex G In construction follows Prepare POSG state v V With slight abuse notation use notations states vertices interchangeably We agent set singleton set There single action POSG transition probabilities determined follows Let degv denote degree vertex v Then T v u 1 degv v u E A selftransition probability 1 implicitly assigned zerodegree vertices Therefore transition probability assignment need O V D time D maxvV degv This complexity upperbounded O V 2 V 1 edges connected given vertex There observation z leading identical observation probability function 1 s pairs That O v z 1 v V This assignment O V time For reward component assign reward according color vertex action taken That Rv Nψv N C cid7 taken bijection maps colors real numbers Deﬁnition 4 Given vertexcolored undirected graph G cid3V E C ψcid4 MG denotes POSG representation G construction steps described Lemma 2 An automorphism MG corresponds unique symmetry M vice versa Proof First unique symmetry φ MG automorphism φG G Because vertices constitute state space MG states permuted By edgepreserving property φG degv degφG v vertices G It follows T v u T φG v φG u v u E By colorpreserving property φG Rv ψv ψφG v RφG v holds Lastly observation probability remains invariant automorphism constant states Therefore construct φ permuting states way permuted φG Notice φG v cid13 φcid6 To prove direction assume symmetry φ MG given By deﬁnition φ T s s cid6 Rs RφS s holds The equivalence transition probabilities implies degv s degvφs vertex v s corresponding state s This equality holds v V To end set φG v s vφs unique φG To φG supports edgepreservation v u E Let sv su states mapped v u respectively Then T sv su 1 T φsv φsu The fact term nonzero indicates φG v φG u E Also v u E T sv su T φG v φG u 0 φG v φG u E cid2 cid6 T φS s φS s cid13 φG φ unique G v φcid6 1 degφG v degv G We state main theorem computational complexity ﬁnding symmetries POSGs We denote problem ﬁnding generators2 automorphism groups graph G AGENG problem ﬁnding 2 Simply automorphism generator graph set permutations vertices applied yields permuted graphs 48 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 symmetries given POSG M PSYMMM It known AGENG belongs class GIcomplete 6 We use fact prove computational complexity PSYMMM GIcomplete To prove PSYMMM GIcomplete need PSYMMM cid3p AGENG M AGENG cid3p PSYMMMG A cid3p B denotes polynomial reducibility problem A problem B Theorem 5 PSYMMM belongs class GIcomplete Ai Z Proof We ﬁrst PSYMMM cid3p AGENG M The number vertices G M O I2S2 polynomial number agents states individual actions observations Since complexity constructing undirected graph n vertices O n2 case complete graph takes polynomial time convert POSG corresponding vertex colored undirected graph By Lemma 1 symmetries M automorphisms G M equivalent The second proof aims AGENG cid3p PSYMMMG For purpose parallel argument assume given graph vertexcolored argument specialized noncolored graphs Given vertexcolored undirected graph G cid3V E C ψcid4 construct corresponding POSG M G Note takes polynomial time convert graph G POSG MG By Lemma 2 automorphisms G symmetries MG equivalent cid2 cid8 By setting I 1 POSG POMDP arguments presented proof Theorem 5 carries modiﬁcation Hence state result arbitrary POMDP computational complexity Corollary 2 The problem ﬁnding symmetries POMDP belongs class GIcomplete Although class GIcomplete belongs NP known P NPcomplete It known low hierarchy class NP number implementations solve GI problems eﬃciently 6 Exploiting symmetries solution methods In section present algorithms POMDPs POSGs taking advantage symmetries present model We ﬁrst extend PBVI characteristics POMDP symmetries discussed Section 42 We present extended version MADP POSGs properties POSG symmetries discussed Section 44 61 Extending PBVI symmetry exploitation POMDPs With set automorphisms Φ represents set symmetries present model modify PBVI advantage symmetries belief states αvectors First sample set belief states heuristics PBVI select belief state farthest cid15 cid151 distance belief state B Since readily know values symmetric images belief state modify cid15 cid151 distance computation handle symmetries cid15b b 1 distance This allows exclude symmetrically identical belief states Second B exclude symmetrically identical belief states modify backup operation include symmetric images αvectors Γ t Table 3 shows pseudocode performing symmetric backup operation cid6cid151 We select belief state farthest cid15 cid15Φ minφφcid6Φ cid15φb φcid6b cid6cid15Φ 1 We added small important improvement symmetric backup αvectors belief states symmetric image b φb For belief states unnecessary add φαb Γt φαb relevant belief state φb b φb We identiﬁed automorphisms yield b cid13 φb belief state b included symmetric images αvectors automorphisms 62 Extending MADP symmetry exploitation POSGs We apply approach POSGs Using results Section 44 expect certain leverages performance MADP In particular use symmetries stages method Value computation stage The ﬁrst major speed bottleneck occurs value computation evaluate joint policies generated exhaustive backup However Theorem 3 states given joint policy value vector merely permutation value vector symmetric image Thus value computation policies avoided simply permute symmetric value vector need Note case interagent symmetries value vectors agent obtained permuting value vectors symmetric agent The total number value vectors decreases factor Φ Pruning stage An greater slowdown LP routines pruning The existence symmetries allows reduce number LP invocations First local policy π agent pruned Corollary 1 states local policy φπ agent φI pruned φ Second π pruned φπ s pruned Therefore LP need performed local policies BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 49 Table 3 The backup operation PBVI taking account Φ set symmetries Require Γt backupB Γt1 Φ cid6 zαi s cid6 zZ argmaxαΓ az t α b cid4 cid4 cid6O s αaz scid6S T s s A z Z αi Γt1 s S αaz s γ end cid28 Γ az t end Γt b B A s S b s Rs αa end argmaxaαa αb αa b αb Γt Γt Γt αb φ cid3 f g hcid4 Φ f αb Γt Γt Γt f αb end end b b end end Table 4 Dynamic programming POSG symmetries NoPrunei agent maintains list policy trees prunable symmetry Require Sets tstep policies Πit corresponding value vectors Vit agent set symmetries Φ The ﬁrst stage dynamic programming backup Perform exhaustive backups Πit1 cid9π cid9Πt1 φ cid9π cid3φ Φ V it1 computed cid9π it1 Eq 5 add value vector Vit1 Compute V end end The second stage dynamic programming backup agent prunable policy NoPrunek k I π Πit1 π NoPrunei π pruned Eq 6 Πit1 Πit1π φ Φ ΠφI it1 ΠφI it1φπ end π pruned NoPrunei NoPrunei π φ Φ NoPruneφI NoPruneφI φπ end end end end return Sets t 1step policies Πit1 corresponding value vectors Vit1 agent The procedure multiagent dynamic programming operator exploits symmetry outlined Table 4 7 Experiments In section empirically symmetries POMDPs POSGs help reduce burdens computational resources required compute solutions The experiments conducted number standard benchmark domains POMDPs POSGs 71 POMDP experiments Before demonstrate performance gain PBVI algorithm symmetric backup operator ﬁrst report test results existence automorphisms standard POMDP benchmark domains Most benchmark domains compact sense model minimization algorithm able reduce size 50 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Domain Tiger Tigergrid 2Cityticketing perr 0 2Cityticketing perr 01 3Cityticketing perr 0 3Cityticketing perr 01 S 2 36 397 397 1945 1945 Min S V Nauty exec time 2 35 397 397 1945 1945 39 9814 1 624 545 1 624 545 61 123 604 61 123 604 0004 s 0061 s 31872 s 23873 s 2585770 s 2601543 s Φ 2 4 4 4 12 12 Fig 10 Model minimization graph automorphism results benchmark domains S number states original model Min S number states minimized model V number vertices graph encoding model Φ number automorphisms nauty including identity mapping Domain Tiger Tigergrid 2cityticketing perr 0 2Cityticketing perr 01 3Cityticketing perr 0 3Cityticketing perr 01 Algorithm PBVI SymmPBVI PBVI SymmPBVI PBVI SymmPBVI PBVI SymmPBVI PBVI SymmPBVI PBVI SymmPBVI B 19 10 590 300 51 17 104 30 261 36 275 30 Γ 5 5 532 529 5 5 9 10 37 42 39 133 Iter 89 89 88 85 167 168 167 167 91 91 91 91 Exec time 007 s 005 s 35969 s 19609 s 15780 s 5760 s 54604 s 20197 s 43 09432 s 9 39506 s 43 28692 s 16 79117 s V b0 cid11 640 640 080 080 874 874 776 773 808 808 695 694 001 003 002 002 100 100 Fig 11 Performance comparisons PBVI algorithm automorphisms SymmPBVI PBVI algorithm exploiting automorphisms sym metric belief collection symmetric backup B number belief states given algorithms Γ number αvectors comprising policy Iter number iterations convergence V b0 average return policy starting initial belief b0 cid11 convergence criteria algorithm running maxbB V nb V n1b cid2 cid11 All V b0s 95 conﬁdence interval optimal domains For tigergrid domain 16 able reduce size ﬁnd symmetries For tiger domain 12 able reduce size ﬁnd symmetries We tested automorphism existence larger domains In spoken dialogue management domain Williams et al 35 user trying buy ticket travel city city machine request conﬁrm information user order issue correct ticket These dialog management problems denoted ncityticketing In domain n cities human user trying book ﬂight cities The agent automated response needs following actions greet askfromaskto conftoxconffromx submitx y x y n cities The users response treated observation agent x fromx tox fromxtoy yes null x y refer cities The observation function dependent speech recognition model performs The states factored components Whether speciﬁed Whether destination speciﬁed Whether current turn ﬁrst turn We instantiated domain n 2 n 3 possible cities different rates speech recognition errors perr perr 0 assumes speech recognition error perr 01 assumes error rate 10 Note case perr 0 domain POMDP user provide partial information request origin city All problems reduced size symmetries Regardless value perr graphs encoding POMDP models exactly The small differences nauty execution times differences orderings vertices graph Fig 10 summarizes result automorphism ﬁnding experiments Next experimented PBVI algorithms benchmark domains discovered automorphisms First sampled ﬁxed number symmetric belief states 300 tigergrid ran symmetric version PBVI We checked number unique belief states symmetric belief states expanded automorphisms We set number 590 tigergrid number belief states non symmetric version PBVI ran algorithm setting automorphisms Note implementation PBVI slightly differs original version original PBVI interleaves belief state exploration value iteration ﬁxing belief states onset execution We gathered belief states simply breadthﬁrst traversal instead stochastic simulation This analyze eﬃciency symmetric backup isolated effects symmetric belief state exploration Fig 11 shows results experiments In summary automorphisms help signiﬁcantly improve performance PBVI running time sacriﬁcing quality policy BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 51 Fig 12 GridSmall environment 72 POSG experiments Fig 13 GridSmall3x3 environment There wellknown benchmark domains general POSGs wealth benchmark domains DEC POMDPs Hence report results symmetry exploitation MADP DECPOMDPs DecTiger 19 Grid Small 1 BoxPushing 27 By focusing DECPOMDPs rule issues equilibrium selection problem generalsum games The DecTiger domain multiagent extension wellknown Tiger domain introduced Sec tion 43 The main difference agents suffer gain coordinating actions penalty severe agent unilaterally opening door leads tiger opening door tiger The standard GridSmall domain set 2by2 grid world agents 1 2 spend time possible grid cell There total 16 states grid cell agent ﬁve actions agent aiUP aiDOWN aiRIGHT aiLEFT aiSTAY observations agent denoted ziLEFT ziRIGHT indicate agent senses wall left right respectively The 16 states encoded s X Y X Y A B C D given Fig 12 The X indicates cell agent 1 resides Y agent 2 s A D given Fig 12 An extended version GridSmall played 3by3 grid world There total 81 states grid cells A B C D E F G H I The action set remains 2by2 case There additional observation sensing wall denoted ziNOTHING agent A visual representation state s AC given Fig 13 The BoxPushing domain requires agents 1 2 push small boxes large box goal state The large box heavy single agent coordinate actions order jointly push large box There actions robot aiLEFT aiRIGHT aiMOVE aiSTAY ﬁve observations robot ziSMALL ziLARGE ziWALL ziEMPTY ziOTHER 100 states goal states The robots choose place small boxes individually goal state receive small reward cooperatively push large box receive greater reward The initial state BoxPushing domain depicted Fig 14 In domain robots R1 R2 start facing 3by4 grid Notice location R1 left R2 This order R1 left R2 ﬁrst upwards But robots boxes moving upward cause box positioned goal region terminating domain This accounts fact column coordinates labeled 0 3 suﬃce positions R1 R2 impossible robot rows grid having domain terminated Thus adopt alphanumeric encoding denote particular nongoal state All nongoal states form s X X Y Y ﬁrst X s column coordinates R1 R2 order Y s values r l u d indicating robot facing right left respectively Eg s03rl depicts initial state given Fig 14 The goal states correspond left small box goal region sLBox right small box goal region sRBox small boxes goal region sLRBox large box goal region sLargeBox Prior executing symmetric MADP algorithm ran nauty graph encoding DECPOMDP domain The automorphisms DecTiger presented Fig 15 The automorphisms discovered included trivial automorphism identity mapping There nontrivial automorphisms interagent The interagent intraagent automorphisms GridSmall domain shown Figs 17 18 respectively Appendix A This domain contains automorphisms including identity mapping Of seven nontrivial automorphisms inter 52 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Fig 14 Initial conﬁguration BoxPushing domain Immediately robots left right small gray boxes black large box The hatched region grid goal region Symm type Interagent State Identity mapping sLEFT sRIGHT Intraagent sLEFT sRIGHT Action a1LISTEN a2LISTEN a1LEFT a2LEFT a1RIGHT a2RIGHT a1LISTEN a2LISTEN a1LEFT a2RIGHT a1RIGHT a2LEFT a1LEFT a1RIGHT a2LEFT a2RIGHT a1LISTEN a1LISTEN a2LISTEN a2LISTEN Obs z0LEFT z1LEFT z1RIGHT z2RIGHT z1LEFT z2RIGHT z1RIGHT z2LEFT z1LEFT z1RIGHT z2LEFT z2RIGHT Fig 15 Nontrivial automorphisms DecTiger The notation X Y indicates X symmetric Y Domain Algorithm T 1 T 2 LP Time V DecTiger GridSmall GridSmall3x3 BoxPushing SymmMADP MADP SymmMADP MADP SymmMADP MADP SymmMADP MADP 4 6 2 8 1 4 10 12 1 s 0 s 1 s 0 s 3 s 1 s 2 s 1 s 4 9 1 1 1 1 8 16 LP 42 84 6 10 5 5 156 290 Time 1 s 1 s 0 s 1 s 1 s 1 s 1271 s 3505 s V 207 810 11 50 25 50 768 1536 T 3 LP 1022 2371 124 410 189 V 86 175 344 250 2631 20 000 99 809 Time 326 s 1215 s 24 s 65 s 172800 s NA NA Fig 16 Performance comparisons domains symmetry exploitation LP number LP invocations V set value vectors produced end iteration The ﬁrst row domain shows results symmetry exploitation second row shows results symmetry All time records rounded nearest second agent Similarly GridSmall3x3 seven nontrivial automorphisms These shown Figs 19 21 Appendix A For BoxPushing domain nontrivial automorphism interagent automorphism shown Fig 23 Appendix A One notable symmetry domain interchange states indicating left right small boxes goal region sLBox sRbox In addition agents corresponding actions observations swapped After computing symmetries compared proposed algorithm MADP algorithm domain We measured memory usage counting number value vectors created end iteration We counted number LP invocations horizon As seen Fig 16 algorithms able complete horizons domains BoxPushing domain respectively For GridSmall3x3 domain MADP complete horizon SymmMADP The running time symmetryexploiting algorithms include time taken compute symmetries nauty explains SymmMADP takes slightly longer complete ﬁrst time horizon cases A separate ﬁeld nauty execution time omitted negligible 25 s compared overall running time Notice exploitation symmetries proceeding horizon attained MADP spatially constrained For DecTiger domain value vectors 70 GB memory end value computation horizon 4 symmetry exploitation Such tendency fact memory usage experiences exponential increase symmetry helps linear factor best However issue addressed approximate algorithms bound memory usage experimenting symmetric versions left future work Earlier horizons exhibit beneﬁt symmetries policy trees generated However horizon effect symmetry exploitation While number value vectors BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 53 reduced approximately proportional symmetries present domain number LP invocations execution time necessarily follow trend This 1 existence selfsymmetric policy trees contribute multiple removal LP avoidance 2 differing LP sizes LP solvers execution time varies The size LP important factor inﬂuences execution time The size governed policy trees created exhaustive backup domain size For example BoxPushing domain utilizes relatively larger LPs 12 800 constraints amplifying effect symmetries Since LP solvers usually highorder polynomial time reducing linear number variables constraints LPs attain superlinear improvement time Symm Type Interagent State s A B sB A s AC sC A s A D sD A sBC sC B sB D sD B sC D sDC s A A sC C s A B sDC s A D sBC sB A sC D sB B sD D sC B sD A s A A sB B s AC sD B s A D sC B sBC sD A sB D sC A sC C sD D s A A sD D s A B sC D s AC sB D sB A sDC sB B sC C sC A sD B Action a1UP a2UP a1DOWN a2DOWN a1LEFT a2LEFT a1RIGHT a2RIGHT a1STAY a2STAY a1UP a2DOWN a1DOWN a2UP a1LEFT a2LEFT a1RIGHT a2RIGHT a1STAY a2STAY a1UP a2UP a1DOWN a2DOWN a1LEFT a2RIGHT a1RIGHT a2LEFT a1STAY a2STAY a1UP a2DOWN a1DOWN a2UP a1LEFT a2RIGHT a1RIGHT a2LEFT a1STAY a2STAY Obs z1LEFT z2LEFT z1RIGHT z2RIGHT z1LEFT z2LEFT z1RIGHT z2RIGHT z1LEFT z2RIGHT z1RIGHT z2LEFT z1LEFT z2RIGHT z1RIGHT z2LEFT Fig 17 Nontrivial interagent automorphisms GridSmall Symm type Intraagent Action a1UP a1DOWN a2UP a2DOWN Obs Identity mapping a1LEFT a1RIGHT a2LEFT a2RIGHT z1LEFT z1RIGHT z2LEFT z2RIGHT a1UP a1DOWN a2UP a2DOWN a1LEFT a1RIGHT a2LEFT a2RIGHT z1LEFT z1RIGHT z2LEFT z2RIGHT State s A A sC C s A B sC D s AC sC A s A D sC B sB A sDC sB B sD D sBC sD A sB D sD B s A A sB B s A B sB A s AC sB D s A D sBC sC A sD B sC B sD A sC C sD D sC D sDC s A A sD D s A B sDC s AC sD B s A D sD A sB A sC D sB B sC C sBC sC B sB D sC A Fig 18 Nontrivial intraagent automorphisms GridSmall 54 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 8 Conclusion We presented graphtheoretical framework computing exploiting symmetries POMDPs POSGs In addition shown experiments actual running time space signiﬁcantly reduced exploiting symmetries The computation symmetries ﬁrst encoding problems appropriate graph structures The automorphisms graphs mapped problem domain represent symmetries problem In provided theoretical result relates computational complexity symmetry computation graph isomorphism computation class GIcomplete Additionally extended concept symmetry multiagent setting introducing POSG symmetries Because multiagent nature symmetries POSGs yield Symm type Interagent Action a1UP a2UP a1DOWN a2DOWN a1LEFT a2LEFT a1RIGHT a2RIGHT a1STAY a2STAY Obs z1LEFT z2LEFT z1RIGHT z2RIGHT z1NOTHING z2NOTHING a1UP a2UP a1DOWN a2DOWN a1LEFT a2LEFT a1RIGHT a2RIGHT a1STAY a2STAY z1LEFT z2RIGHT z1RIGHT z2RIGHT z1NOTHING z2NOTHING a1UP a2DOWN a1DOWN a2UP a1LEFT a2LEFT a1RIGHT a2RIGHT a1STAY a2STAY z1LEFT z2LEFT z1RIGHT z2RIGHT z1NOTHING z2NOTHING State s A B sB A s AC sC A s A D sD A s A E sE A s A F s F A s AG sG A s A H sH A s A I sI A sBC sC B sB D sD B sB E sE B sB F s F B sBG sG B sB H sH B sB I sI B sC D sDC sC E sEC sC F s F C sC G sGC sC H sH C sC I sI C sD E sE D sD F s F D sDG sG D sD H sH D sD I sI D sE F s F E sE G sG E sE H sH E sE I sI E s F G sG F s F H sH F s F I sI F sG H sH G sG I sI G sH I sI H s A A sC C s A B sBC s A D s F C s A E sEC s A F sDC s AG sI C s A H sH C s A I sGC sB A sC B sB D s F B sB E sE B sB F sD B sBG sI B sB H sH B sB I sG B sC D s F A sC E sE A sC F sD A sC G sI A sC H sH A sC I sG A sD D s F F sD E sE F sDG sI F sD H sH F sD I sG F sE D s F E sE G sI E sE H sH E sE I sG E s F G sI D s F H sH D s F I sG D sG G sI I sG H sH I sH G sI H s A A sG G s A B sH G s AC sI G s A D sDG s A E sE G s A F s F G s A H sBG s A I sC G sB A sG H sB B sH H sBC sI H sB D sD H sB E sE H sB F s F H sB I sC H sC A sG I sC B sH I sC C sI I sC D sD I sC E sE I sC F s F I sD A sG D sD B sH D sDC sI D sD E sE D sD F s F D sE A sG E sE B sH E sEC sI E sE F s F E s F A sG F s F B sH F s F C sI F sG B sH A sGC sI A sH C sI B Fig 19 Nontrivial interagent automorphism GridSmall3x3 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 55 implications area game theory We presented gametheoretic properties exhibited presence symmetries Our algorithms exploit symmetries presented These algorithms modiﬁcations previous known algorithms PBVI MADP POMDPs POSGs respectively Although demonstrated eﬃciency symmetry exploitation PBVI MADP idea readily extended algorithms For example symmetries impact solution techniques use heuristic search MAA 33 Qvalue functions Action a1UP a2DOWN a1DOWN a2UP a1LEFT a2RIGHT a1RIGHT a2LEFT a1STAY a2STAY Obs z1LEFT z2RIGHT z1RIGHT z2LEFT z1NOTHING z2NOTHING Symm type Inter contd State s A A sI I s A B sH I s AC sG I s A D s F I s A E sE I s A F sD I s AG sC I s A H sB I sB A sI H sB B sH H sBC sG H sB D s F H sB E sE H sB F sD H sBG sC H sC A sI G sC B sH G sC C sG G sC D s F G sC E sE G sC F sDG sD A sI F sD B sH F sDC sG F sD D s F F sD E sE F sE A sI E sE B sH E sEC sG E sE D s F E s F A sI D s F B sH D s F C sG D sG A sI C sG B sH C sH A sI B Fig 20 Nontrivial interagent automorphism GridSmall3x3 continued Action a1LEFT a1RIGHT a2LEFT a2RIGHT Obs z1LEFT z1RIGHT z2LEFT z2RIGHT a1UP a1DOWN a2UP a2DOWN Identity mapping Symm type Intraagent State s A A sC C s A B sC B s AC sC A s A D sC F s A E sC E s A F sC D s AG sC I s A H sC H s A I sC G sB A sBC sB D sB F sBG sB I sD A s F C sD B s F B sDC s F A sD D s F F sD E s F E sD F s F D sDG s F I sD H s F H sD I s F G sE A sEC sE D sE F sE G sE I sG A sI C sG B sI B sGC sI A sG D sI F sG E sI E sG F sI D sG G sI I sG H sI H sG I sI G sH A sH C sH D sH F sH G sH I s A A sG G s A B sG H s AC sG I s A D sG D s A E sG E s A F sG F s AG sG A s A H sG B s A I sGC sB A sH G sB B sH H sBC sH I sB D sH D sB E sH E sB F sH F sBG sH A sB H sH B sB I sH C sC A sI G sC B sI H sC C sI I sC D sI D sC E sI E sC F sI F sC G sI A sC H sI B sC I sI C sD A sDG sD B sD H sDC sD I sE A sE G sE B sE H sEC sE I s F A s F G s F B s F H s F C s F I Fig 21 Nontrivial intraagent automorphism GridSmall3x3 56 BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 Action a1UP a1DOWN a2UP a2DOWN a1LEFT a1RIGHT a2LEFT a2RIGHT Obs z1LEFT z1RIGHT z2LEFT z2RIGHT Symm type Intra contd State s A A sI I s A B sI H s AC sI G s A D sI F s A E sI E s A F sI D s AG sI C s A H sI B s A I sI A sB A sH I sB B sH H sBC sH G sB D sH F sB E sH E sB F sH D sBG sH C sB H sH B sB I sH A sC A sG I sC B sG H sC C sG G sC D sG F sC E sG E sC F sG D sC G sGC sC H sG B sC I sG A sD A s F I sD B s F H sDC s F G sD D s F F sD E s F E sD F s F D sDG s F C sD H s F B sD I s F A sE A sE I sE B sE H sEC sE G sE D sE F Fig 22 Nontrivial intraagent automorphism GridSmall3x3 Action a1LEFT a2LEFT a1RIGHT a2RIGHT a1MOVE a2MOVE a1STAY a2STAY Obs z1EMPTY z2EMPTY z1OTHER z2OTHER z1SMALL z2SMALL z1LARGE z2LARGE State sLBox sRBox s01uu s23uu s01ud s23du s01ul s23ru s01ur s23lu s01du s23ud s01dd s23dd s01dl s23rd s01dr s23ld s01lu s23ur s01ld s23dr s01ll s23rr s01lr s23lr s01ru s23ul s01rd s23dl s01rl s23rl s01rr s23ll s02uu s13uu s02ud s13du s02ul s13ru s02ur s13lu s02du s13ud s02dd s13dd s02dl s13rd s02dr s13ld s02lu s13ur s02ld s13dr s02ll s13rr s02lr s13lr s02ru s13ul s02rd s13dl s02rl s13rl s02rr s13ll s03ud s03du s03ul s03ru s03ur s03lu s03dl s03rd s03dr s03ld s03ll s03rr s12ud s12du s12ul s12ru s12ur s12lu s12dl s12rd s12dr s12ld s12ll s12rr Fig 23 Nontrivial automorphism BoxPushing It interagent automorphism DECPOMDPs 22 Another interesting area application apply symmetries ﬁnite controller representation policies 1 While symmetry exploitation greatly reduces computational spatial burden solving POMDPs POSGs limited fact problems come symmetries One promising direction research compute approximate symmetries theoretical error bound Acknowledgements We indebted anonymous reviewers helpful comments improving article This work sup ported Korea Research Foundation Grant KRFD00527 Defense Acquisition Program Administration Agency Defense Development Korea contract UD080042AD Appendix A Appendix Figs 1723 automorphisms GridSmall GridSmall3x3 BoxPushing accompanying results experiments section BK Kang KE Kim Artiﬁcial Intelligence 182183 2012 3257 57 References 1 C Amato S Zilberstein Heuristic policy iteration inﬁnitehorizon decentralized POMDPs Proceedings AAMAS 2008 Workshop Multi Agent Sequential Decision Making Uncertain Domains 2008 pp 115 2 DS Bernstein C Amato EA Hansen S Zilberstein Policy iteration decentralized control Markov decision processes Journal Artiﬁcial Intelli gence Research 34 2009 89132 3 DS Bernstein R Givan N Immerman S Zilberstein The complexity decentralized control Markov decision processes Mathematics Operations Research 27 4 2002 819840 4 DS Bernstein EA Hansen S Zilberstein Bounded policy iteration decentralized POMDPs Proceedings 19th International Joint Conference Artiﬁcial Intelligence 2005 pp 12871292 5 R Bödi K Herr M Joswig Algorithms highly symmetric linear integer programs Tech Rep arXiv10124941 6 KS Booth CJ Colbourn Problems polynomially equivalent graph isomorphism Tech Rep CS7704 University Waterloo 1979 7 SF Cheng DM Reeves Y Vorobeychik MP Wellman Notes equilibria symmetric games Proceedings AAMAS 2004 Workshop Game Theoretic Decision Theoretic Agents 2004 pp 2328 8 T Dean R Givan Model minimization Markov decision processes Proceedings 14th National Conference Artiﬁcial Intelligence 1997 pp 106111 9 F Doshi N Roy The permutable POMDP fast solutions POMDPs preference elicitation Proceedings 7th International Conference Autonomous Agents MultiAgent Systems 2008 pp 493500 10 R Givan T Dean M Greig Equivalence notions model minimization Markov decision processes Artiﬁcial Intelligence 147 12 2003 163223 11 EA Hansen DS Bernstein S Zilberstein Dynamic programming partially observable stochastic games Proceedings 19th National Con ference Artiﬁcial Intelligence 2004 pp 709715 12 LP Kaelbling ML Littman AR Cassandra Planning acting partially observable stochastic domains Artiﬁcial Intelligence 101 12 1998 99134 13 KE Kim Exploiting symmetries POMDPs pointbased algorithms Proceedings 23rd AAAI Conference Artiﬁcial Intelligence 2008 pp 10431048 14 Y Kim KE Kim Pointbased policy iteration decentralized POMDPs Proceedings 11th Paciﬁc Rim International Conference Artiﬁcial Intelligence 2010 pp 614619 15 A Kumar S Zilberstein Pointbased backup decentralized POMDPs Complexity new algorithms Proceedings 9th International Conference Autonomous Agents MultiAgent Systems 2010 pp 13151322 16 ML Littman AR Cassandra LP Kaelbling Learning policies partially observable environments Scaling Proceedings 12th International Conference Machine Learning 1995 pp 362370 17 O Madani S Hanks A Condon On undecidability probabilistic planning related stochastic optimization problems Artiﬁcial Intelli gence 147 12 2003 534 18 BD McKay Nauty users guide version 24 online document httpcsanueduaubdmnautynugpdf 2007 19 R Nair M Tambe M Yokoo D Pynadath S Marsella Taming decentralized POMDPs Towards eﬃcient policy computation multiagent settings Proceedings 18th International Joint Conference Artiﬁcial Intelligence 2003 pp 705711 20 SM Narayanamurthy B Ravindran On hardness ﬁnding symmetries Markov decision processes Proceedings 25th International Conference Machine Learning 2008 pp 688695 21 N Nisan T Roughgarden E Tardos VV Vazirani Algorithmic Game Theory Cambridge University Press New York NY USA 2007 22 FA Oliehoek N Vlassis Qvalue functions decentralized POMDPs Proceedings 6th International Conference Autonomous Agents MultiAgent Systems 2007 pp 838845 23 CH Papadimitriou JN Tsitsiklis The complexity Markov decision processes Mathematics Operations Research 12 3 1987 441450 24 J Pineau G Gordon S Thrun Policycontingent abstraction robust robot control Proceedings 19th Conference Uncertainty Artiﬁcial Intelligence 2003 pp 477484 25 J Pineau G Gordon S Thrun Anytime pointbased approximation large POMDPs Journal Artiﬁcial Intelligence Research 27 2006 335380 26 B Ravindran AG Barto Symmetries model minimization Markov decision processes Tech Rep CMPSCI 0143 University Massachusetts Amherst 2001 27 S Seuken S Zilberstein Improved memorybounded dynamic programming decentralized POMDPs Proceedings 23rd Conference Uncertainty Artiﬁcial Intelligence 2007 pp 344351 28 S Seuken S Zilberstein Memorybounded dynamic programming DECPOMDPs Proceedings 20th International Joint Conference Artiﬁcial Intelligence 2007 pp 20092015 29 G Shani RI Brafman SE Shimony Forward search value iteration POMDPs Proceedings 20th International Joint Conference Artiﬁcial Intelligence 2007 pp 26172624 30 T Smith R Simmons Heuristic search value iteration POMDPs Proceedings 20th Conference Uncertainty Artiﬁcial Intelligence 2004 pp 520527 31 EJ Sondik The optimal control partially observable Markov decision processes PhD thesis Department Electrical Engineering Stanford University 1971 32 MTJ Spaan N Vlassis Perseus Randomized pointbased value iteration POMDPs Journal Artiﬁcial Intelligence Research 24 2005 195220 33 D Szer F Charpillet S Zilberstein MAA A heuristic search algorithm solving decentralized POMDPs Proceedings 21st Conference Uncertainty Artiﬁcial Intelligence 2005 pp 576583 34 C White A survey solution techniques partially observed Markov decision process Annals Operations Research 32 14 1991 215230 35 JD Williams P Poupart S Young Factored partially observable Markov decision processes dialogue management Proceedings IJCAI 2005 Workshop Knowledge Reasoning Practical Dialogue Systems 2005 pp 7682 36 AP Wolfe POMDP homomorphisms Proceedings NIPS 2006 Workshop Grounding Perception Knowledge Cognition SensoryMotor Experience 2006 37 F Wu S Zilberstein X Chen Pointbased policy generation decentralized POMDPs Proceedings 9th International Conference Au tonomous Agents MultiAgent Systems 2010 pp 13071314