Artiﬁcial Intelligence 267 2019 3957 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Probably bounded suboptimal heuristic search Roni Stern Ben Gurion University Negev Israel b University Toronto Canada Gal Dreiman Richard Valenzano b r t c l e n f o b s t r c t Article history Received 10 March 2017 Received revised form 15 June 2018 Accepted 8 August 2018 Available online 25 October 2018 Keywords Artiﬁcial intelligence Heuristic search Finding optimal solution search problem desirable diﬃcult cases A common approach cases try ﬁnd solution suboptimality bounded parameter cid2 deﬁnes far optimal solution acceptable A scarcely studied alternative try ﬁnd solution probably optimal parameter δ deﬁnes conﬁdence required solutions optimality This paper explores option introduces concept probably boundedsuboptimal search pBS search algorithm Such search algorithm accepts parameters cid2 δ outputs solution probability 1 δ costs 1 cid2 times optimal solution A general algorithmic framework pBS search algorithms proposed Several instances framework described analyzed theoretically experimentally range search domains Results pBS search algorithms faster stateoftheart boundedsuboptimal search algorithm This shows practice ﬁnding solutions satisfy given suboptimality bound high probability faster ﬁnding solutions satisfy suboptimality bound certainty 2018 Elsevier BV All rights reserved 1 Introduction Consider search problem ﬁnd path state space given initial state goal state Given memory running time standard heuristic search algorithms like A algorithm 1 solve given search problem optimally ﬁnd lowestcost path initial state goal However case memory runtime ﬁnd optimal solution For cases range search algorithms proposed return suboptimal solutions 25 In particular boundedsuboptimal search algorithms algorithms guaranteed return solution cost 1 cid2 times optimal solution Ideal boundedsuboptimal algorithms introduce natural tradeoff solution quality search runtime cid2 high solutions returned quickly poorer quality cid2 low solutions harder ﬁnd usually higher quality lower cost Boundedsuboptimal search algorithms strict sense cost solution return 1 cid2 times cost optimal solution Current techniques achieve guarantee rely admissible heuristics heuristics lower bound optimal solution cost Such heuristics inaccurate resulting increased search runtime Corresponding author Email addresses sternronpostbguacil R Stern galdreimangmailcom G Dreiman rvalenzanocstorontoedu R Valenzano httpsdoiorg101016jartint201808005 00043702 2018 Elsevier BV All rights reserved 40 R Stern et al Artiﬁcial Intelligence 267 2019 3957 In paper propose alternative type solution quality guarantee Search algorithms quality guarantee return solutions optimal high probability Instead controlling suboptimality returned solution cid2 algorithms accept parameter δ controls conﬁdence returned solution optimal δ allows similar tradeoff solution quality runtime increasing δ expected decrease search effort cost increasing likelihood suboptimal solution returned Both types solution quality guarantees combined novel notion probably boundedsuboptimal search pBS search A pBS search algorithm given parameters cid2 δ required return solution 1 cid2 times optimal solution probability higher 1 δ We 1 cid2 desired suboptimality 1 δ required conﬁdence Introducing deﬁning concept pBS search ﬁrst main contribution work In domains solution current boundedsuboptimal search algorithms lower better suboptimality practice suboptimality guaranteed bound Since usually tradeoff runtime solution quality means user search algorithm paid runtime needed desired solution quality A key beneﬁt novel form bounded suboptimality propose provides users search algorithms control timequality tradeoff We observed experimentally pBS algorithms propose possible obtain solution desired suboptimality signiﬁcantly faster slightly relaxing required conﬁdence 10 09 The second contribution general framework developing pBS algorithms called Psf Psf main building blocks solution generator stopping condition The solution generator algorithm produces sequence solutions The stopping condition responsible identifying current solution suﬃcient sense desired suboptimality required conﬁdence We propose stopping conditions Absolute hratio Openbased prove conditions results pBS search algorithm For Absolute hratio conditions propose solution generator speciﬁcally designed satisfy stopping conditions quickly These stopping conditions new solution generator contribution work Finally evaluate different instances Psf experimentally search domains Pancakes puzzle Dockyard robot Vacuum cleaner gridbased pathﬁnding The experiments diverse set domains varying cid2 δ offers ﬂexible control solution quality versus runtime tradeoff general increasing cid2 δ results smaller running time lower solution quality In particular setting δ 0 allows ﬁnd solutions faster Some material work previously published proceedings Symposium Combinatorial Search SoCS 67 This paper summarizes goes conference papers In particular extends prior works The experimental evaluation prior conference publications considered domain 15puzzle This work signiﬁcantly extends evaluation implementation evaluation Psf instances additional domains Section 5 The theoretical basis pBS search properly deﬁned analyzed This includes corrections original work Appendix B For Absolute hratio stopping conditions propose solution generator speciﬁcally designed based recent boundedcost search algorithms Section 6 In addition previous work 67 line research referred Probably Approximately Correct PAC search inspired notion PAC learning theoretical machine learning literature 8 Given signiﬁcant differences concept PAC solution quality guarantees considered paper changed probably boundedsuboptimal search avoid confusion A comparison related concepts Section 8 2 Preliminaries background A graph search problem search problem deﬁned graph G source vertex s V nonempty set target vertices T V The edges G associated nonnegative cost denoted ce cost path p G denoted cp sum costs constituent edges A solution search problem P path G s vertex T A solution called optimal solution lower cost Let Opt P denote cost optimal solution P The suboptimality solution ratio cost Opt P Hence suboptimality optimal solution 1 Note alternative forms suboptimality introduced 9 concepts paper easily extended For clarity focus paper aforementioned deﬁnition suboptimality A search algorithm procedure accepts search problem tries return solution Note cases underlying graph G given search algorithm explicitly large ﬁt memory Instead search algorithm given source vertex s set state transition operators implicitly deﬁne G set states reachable applying sequences state transition operators s 21 Properties search algorithms R Stern et al Artiﬁcial Intelligence 267 2019 3957 41 Let cost A P denote cost solution returned algorithm A given problem P A search algorithm A called optimal search algorithm search problem P holds cost A P OptP A search algorithm A called boundedsuboptimal search algorithm accepts parameter cid2 problem P holds cost A P 1 cid2 OptP suboptimality solutions returns 1 cid2 A 10 RBFS 11 examples optimal search algorithms Weighted A cid2 3 Explicit Estimation Search 4 Dynamic Potential Search 5 examples boundedsuboptimal search algorithms 1 IDA 2 A Two additional types search algorithms use paper anytime search algorithms boundedcost search algorithms Anytime search algorithms continue search better solutions initial solution 12 BeamStack given running time Prominent examples anytime search algorithms Anytime Weighted A Search 13 Anytime Window A A search algorithm boundedcost search BCS algorithm accepts parameter B returns solution ANA cost B solution exist Potential Search PTS 15 Boundedcost Explicit Estimation Search BEES 16 examples BCS algorithms 14 Anytime Potential Search APTS 15 known Anytime Nonparametric A Note focus work graph search problems ﬁnding solution smaller cost desired Search algorithms applied settings goal ﬁnd solution maximal cost 17 We expect extending results settings diﬃcult 22 Bestﬁrst search Many heuristic search algorithm ﬁt general framework bestﬁrst search Bestﬁrst search BFS iterative algorithm maintains list nodes called Open On iteration algorithm chooses single node Open expand expanding node means generating children inserting Open Initially Open contains initial state si BFS algorithms maintain list nodes called Closed contains previously expanded nodes Closed avoid generating states multiple times needed Bestﬁrst search algorithms differ choose node expand Open For example A algo rithm 1 chooses expand node Open minimal g h value g value node n denoted gn lowest cost path far si n hn heuristic estimate cost n goal node If heuristic function admissible lower bound A guaranteed ﬁnd optimal lowestcost solution 1 3 Probably bounded suboptimal pBS search In section deﬁne notion probably boundedsuboptimal search Let P set search problems let D distribution P D deﬁned random variable drawn P according distribution D Following notation cost A P D random variable cost solution returned search algorithm A given search problem drawn P according distribution D Similarly OptP D random variable optimal solution cost problems drawn P according D Deﬁnition 1 pBS search algorithm A search algorithm pBS search algorithm wrt P D iff accepts input cid2 0 1 δ 0 probability 1 δ outputs solution suboptimality 1 cid2 cid2 cost A P D 1 cid2 OptP D cid3 P r 1 δ 1 We refer 1 cid2 desired suboptimality refer 1 δ required conﬁdence Thus pBS search algorithm search algorithm returns solution desired suboptimality required conﬁdence Classical search algorithms viewed special cases pBS search algorithm Optimal search algorithms pBS search algorithms cost A P OptP D clearly Equation 1 holds A nonnegative cid2 δ Similarly boundedsuboptimal search algorithm pBS search algorithm cost A P 1 cid2 OptP D clearly Equation 1 holds nonnegative δ However classes algorithms ignore value δ miss potential opportunities speeding search Next propose general framework pBS search algorithm considers cid2 δ 31 A framework pBS search algorithms The pBS search framework propose abbreviated Psf major building blocks solution generator stopping condition The role solution generator ﬁnd solutions solution lower cost previous The role stopping condition decide best solution far incumbent solution desired suboptimality required conﬁdence 42 R Stern et al Artiﬁcial Intelligence 267 2019 3957 Algorithm 1 pBS search algorithm framework Psf Input 1 cid2 desired suboptimality Input 1 δ required conﬁdence NewSolution GenerateSolutionU NewSolution return Incumbent 1 U 2 Incumbent None 3 Improving U possible 4 5 6 7 8 9 10 Incumbent NewSolution U NewSolutionCost ShouldStopU return Incumbent 11 end 12 end Psf uses building blocks follows pseudo code Algorithm 1 First solution generator line 4 Algorithm 1 run ﬁnd initial solution If successfully ﬁnds solution solution cost stored variables Incumbent U respectively Following stopping condition ShouldStop line 10 checks U desired suboptimality required conﬁdence1 If Incumbent returned Otherwise iteration begins solution generator ﬁnds new solution updating Incumbent U stopping condition invoked check U desired suboptimality required conﬁdence lines 311 This iterative process continues incumbent solution satisﬁed stopping condition solution generator ﬁnd solutions Note solution generator accepts U parameter seeks ﬁnd solution cost lower Incumbents cost If solution exists Incumbent returned Returning Incumbent case safe solution generator complete ﬁnd solution cost lower U Incumbent optimal The key question implementing Psf implement solution generator stopping condition Anytime search algorithms especially suitable serve solution generators given time return sequence solutions subsequent solution better predecessor Moreover straightforward modify existing anytime search algorithms accept cost incumbent solution U prune parts search space result improving incumbent solution For example anytime search algorithm uses admissible heuristic prune nodes g h U Some anytime algorithms guaranteed eventually ﬁnd optimal solution If Psf uses anytime algorithm solution generator exists stopping condition Psf sound complete value cid2 δ ﬁnd solution desired suboptimality required conﬁdence An example stopping condition stopping condition satisﬁed halts search In case solution generator eventually ﬁnd optimal solution satisﬁes desired suboptimality requirements For discussions assume solution generator converges optimal solution 4 pBS stopping conditions Clearly stopping condition halts ineﬃcient In section propose better stopping conditions halt search earlier maintain pBS search guarantees Deﬁnition 1 Such stopping conditions referred pBS conditions Deﬁnition 2 pBS condition A pBS condition stopping condition Psf algorithm guarantees instance Psf stopping condition complete solution generator pBS search algorithm The notion pBS conditions pBS search deﬁned respect set problems P distribution D To develop effective pBS conditions assume preprocessing stage given set search problems sampled independently distribution D sampled iid manner suﬃcient resources solve optimally We refer given set search problems training set Having representative training set able solve optimally problems certainly possible However reasonable assumptions realistic scenarios See discussion Section 7 41 The Absolute condition Having sample problems optimal solutions allows approximate cumulative distribution function CDF random variable OptP D This example counting number problems optimal 1 As later discuss creating stopping condition nontrivial large parts paper devoted proposing stopping conditions R Stern et al Artiﬁcial Intelligence 267 2019 3957 43 Fig 1 The distribution F v 4 domains experimental results The x axis shows optimal solution costs denoted v y axis shows F v value solution higher v range v values Other statistically valid curve ﬁtting techniques Let F v denote resulting CDF How close F v actual CDF OPTP D depends size training set approximation technique For simplicity assume paper F v actual CDF OPT P D assume F v P rv OptP D Future work study number problems create CDF affects resulting inaccuracy modify δ incorporate error Fig 1 shows CDFs experiments generated simple problemcounting method mentioned The x axis shows possible values Opt P D y axis shows cumulative distribution given x value shows value P rx OptP D See Section 5 description different domains For given CDF F v deﬁne threshold value T cid2 δ maximal number v F v 1cid2 equal larger 1 δ Formally cid4 cid4 T cid2 δ argmax v0 F cid5 cid5 1 δ v 1 cid2 T cid2 δ key ﬁrst pBS condition introduce Absolute condition According condition search halts U T cid2 δ Theorem 1 The Absolute stopping condition pBS condition Proof Let P problem drawn P according distribution D let A instance Psf uses Absolute condition When A halts P optimal solution line 6 Algorithm 1 halted Absolute satisﬁed line 10 In worst case A halts reason In case cost A P T cid2 δ This means cid5 cid4 F cost A P 1 cid2 1 δ P r cost A P 1 cid2 OptP D 1 δ P rcost A P 1 cid2 OptP D 1 δ 2 3 4 Since P sampled P D follows optimal solution OptP sampled distribution optimal solutions OptP D P rcost A P 1 cid2 OptP 1 δ cid2 5 The Absolute condition attractive properties First easy implement simply halt cumbent solution T cid2 δ Thus condition easy integrate existing search algorithms Second requires 44 R Stern et al Artiﬁcial Intelligence 267 2019 3957 virtually additional memory running time threshold value T cid2 δ depend problem hand P computed problems domain The Absolute condition consider information search problem currently solved To illustrate wasteful consider following example Example 1 Assume admissible heuristic function h trying solve search problem P start state si hsi 40 Since h admissible know Opt P 40 Thus ﬁnd incumbent solution cost 40 immediately halt regardless value δ However training set contained state optimal solution 35 F 40 0 values δ Absolute condition halt search Moreover assume heuristic underestimates optimal solution 5 In case happen ﬁnd solution cost 44 likely optimal halt Again Absolute condition agnostic exploit information 42 Heuristicaware pBS conditions We present complementary solutions issues identiﬁed example 421 Bounding optimal solution 1 WA Many search algorithms maintain lower bound optimal solution For example search algorithm 18 APTS 5 use fmin maintains generated nodes open list A minnOpengn hn lower bound optimal cost h admissible The value fmin increase crease inconsistent heuristics 19 nodes expanded generated So obtain tightest lower bound search algorithms maintain maximal fmin observed far We denote value Max f min Clearly incumbent solution equal smaller 1 cid2 Max f min desired suboptimality complete certainty δ 0 This stopping condition refer Max f min condition wellknown literature anytime search algorithms 12 boundedsuboptimal search algorithms 453 Impor tantly Max f min condition pBS condition halting search condition satisﬁed We implemented additional stopping condition pBS conditions Observe Max f min condition resolves ﬁrst problem Example 1 hsi 40 cost incumbent solution 40 hsi 40 Max f min 40 2 ARA 422 hratio condition The following pBS condition called hratio condition aims resolve second problem Example 1 consid ering error heuristic function start state For heuristic function h search problem P corresponding start state si deﬁne error h ratio hsi P D The hsi P D OptP hsi The error h random search problem P D random variable denoted Opt statistics needed hratio condition CDF random variable denoted F R v P rv Opt As Absolute condition deﬁne threshold value T R cid2 δ cid4 cid4 cid5 cid5 T R cid2 δ argmax v0 F R v 1 cid2 1 δ The hratio condition satisﬁed incumbent solution U holds U hsi T R cid2 δ Theorem 2 The hratio condition pBS condition The proof Theorem 2 straightforward adaptation proof Theorem 1 It given Appendix A com pleteness Similar Absolute condition hratio condition Psf incurs overhead Simply compute hsi T R cid2 δ beginning search check U smaller case halt search The hratio condition special case general pBS condition considers likelihood search problem having optimal solution value given heuristic value initial state To implement condition require cumulative distribution function Fht v P rOpt vhsi t The corresponding threshold parameter cid4 cid5 argmax v0 Fhhsi v 1 cid2 1 δ resulting stopping condition halt search incumbent solution equal smaller threshold value Implementing pBS condition properly requires larger training set hratio requires suﬃcient problems training heuristic value R Stern et al Artiﬁcial Intelligence 267 2019 3957 45 43 Openbased condition The pBS stopping conditions proposed far agnostic solution generator Indeed knowl edge underlying search problem stopping conditions consider statistics generated training set heuristic current start state While makes simpler implement means incumbent solution U satisfy pBS conditions search halt new incumbent solution Moreover pBS conditions consider solution generator ﬁnds solutions Thus ignore information discovered underlying problem solution generator collects search The Max f min condition remedies extent considers lower bound obtained solution generator halt lower bound increased condition ignores δ increasing lower bound diﬃcult In section propose pBS condition monitors search performed solution generator based information collects underlying search problem decide halt search new incumbent solution This pBS condition called Openbased condition designed speciﬁc type solution generators solution generators based bestﬁrst search Many anytime search algorithms bestﬁrst searches 12 APTS 15 The Openbased condition monitors nodes Open searching including ARA new incumbent solution decides halt likely node Open solution incumbent solution desired suboptimality 18 AWA We explain Openbased condition details First deﬁnitions introduced Let h lowest cost path n goal Thus search problem P start state si h n denote cost si OptP Deﬁnition 3 Reject A node n rejects cost U wrt cid2 cid6 gn h cid7 n 1 cid2 U Intuitively node n rejects cost U solution underlying search problem passes node n solution cost small reject hypothesis U desired suboptimality 1 cid2 Lemma 1 If optimal solution node n Open reject solution cost U U achieves desired suboptimality Proof This proof contradiction Assume U achieve desired suboptimality U 1 cid2 OptP Let n denote optimal path initial state si node n Since optimal solution g m Since si gm h node m Open optimal solution gm g cid7 m 1 cid2 gm h m Open assumed nodes Open reject cost U follows U Thus U 1 cid2 OptP contradicting assumption U desired suboptimality cid2 m 1 Thus OptP h cid6 When node n Open value h n known Thus determining node n rejects cost U feasible Given appropriate statistics possible estimate probability node rejects cost U To end collect consider statistics heuristic error search nodes encountered solution generator These statistics similar hratio represented CDF function F n v P r h v There ways approximate CDF In experiments grouping nodes h value random variable Nhv represents drawing node hn v distribution nodes observed search h value equal v Then approximate F n v P r h v denoted Fhhn v See Section 52 details collected information generate statistics Nhhn hNhhn n hn Corollary 1 The probability randomly drawn node rejects cost U given cid5cid5 cid4 cid4 Fh hn 1 hn Proof By deﬁnition Fh Nhhn h hNhhn P r cid4 cid4 P r 1 cid2 gn U 1 cid2 cid2 hn 1 hn cid4 cid2 cid3cid3 U 1cid2 gn equal cid5cid5 1 U 1 cid2 hn cid4 gn hn h gn Nhhn hNhhn cid5 cid5 U 6 7 probability n rejects U Deﬁnition 3 cid2 For given node n denote value 1 hn U 1cid2 gn P U n cid2 cid2 clear context omit write P U n Thus Corollary 1 states P U n probability node n rejects cost U 46 R Stern et al Artiﬁcial Intelligence 267 2019 3957 Deﬁnition 4 Openbased The Openbased condition satisﬁed cid8 nOpen log1 P U n log1 δ 8 The correctness Openbased condition relies assumption nodes n n n rejects U negatively correlated event n n n reject U product individual probabilities occur cid9 Open event rejects U Under assumption probability cid9 cid9 cid6 P r n rejects U n cid9 cid7 rejects U 1 P U n 1 P U n cid9 Theorem 3 The Openbased condition pBS condition assumption Equation 9 holds Proof If Openbased condition satisﬁed cid8 log1 P U n log1 δ nOpen cid9 nOpen 1 P U n 1 δ Due Equation 9 P rn Open n rejects U cid9 nOpen 1 P U n 1 δ 9 10 11 12 Following Lemma 1 means U desired suboptimality required conﬁdence cid2 Note Openbased condition deﬁned Equation 11 logarithm We logarithm avoid precision issues Algorithm 2 Openbased pBS condition update rule Input m node currently expanded 1 m goal node U decreased 2 3 ˆP U 0 foreach node m cid9 Open ˆP U ˆP U log1 P U m cid9 4 5 6 7 8 9 10 11 12 13 14 15 16 end end ˆP U ˆP U log1 P U m foreach child m gm m cid9 m cid9 cid9 updated m expanded previously Open ˆP U ˆP U log1 P oldU m cid9 end ˆP U ˆP U log1 P U m cid9 end end A signiﬁcant advantage Openbased condition Absolute hratio conditions satisﬁed expanding node Open Max f min U changed Thus especially important check condition eﬃciently The runtime required check Openbased condition satisﬁed dominated runtime log1 P U n The straightforward way compute ˆP U required calculate log1 P U n Let ˆP U cid8 cid8 nOpen nOpen iterate nodes Open Computing ˆP U way node expanded clearly eﬃcient Open large Fortunately ˆP U computed incrementally eﬃcient manner node m removed Open decrease ˆP U log1 P U m node m added Open increase ˆP U log1 P U m The details incremental computation P U n given Algorithm 2 explained Let m nongoal node selected expansion When m expanded removed Open ˆP U decrease log1 P U m line 7 Algorithm 2 The nodes generated m added Open need update ˆP U accordingly Speciﬁcally generated child m cid9 falls following cases Case 1 m cid9 cid9 line 7 cid9 P U m R Stern et al Artiﬁcial Intelligence 267 2019 3957 47 Open Closed In case m cid9 added Open Consequently ˆP U increased log1 Case 2 m Open When generating node Open g value updated path cid9 change ˆP U remains unchanged cid9 cid9 line 7 m lower cost current g value If gm If gm calculated old new g value respectively After generating m adding P ne w U m cid9 change ˆP U updated follows Let P oldU m update ˆP U removing P oldU m cid9 denote value P U m cid9 P ne w U m cid9 line 13 cid9 Case 3 m Closed previously expanded In case g value m cid9 cid9 m lower gm cid9 updated m reinserted cid9 The node m cid9 expanded cases cost path m cid9 ˆP U 2 Open need add P ne w U m Updating ˆP U incremental manner incurs O 1 overhead time node generated In contrast goal node expanded better incumbent solution U decreases Consequently calculating ˆP U value log1 P U n updated node n Open lines 15 This requires O Open overhead However occurs new incumbent solution If number times incumbent solution updated D overhead updating ˆP U amortized cost generating node Open incurring additional D operations generated node 5 Experimental results In preliminary work pBS search evaluated pBS search framework pBS conditions known 15puzzle Here extend evaluation additional domains All source code run experiments publicly available See Appendix D details obtain run 51 Domains The ﬁrst domains pancakes gridbased pathﬁnding standard heuristic search benchmarks The domains vacuum cleaner dockyard robot inspired classical planning problems previously Thayer Ruml 4 520 evaluating search algorithms For brevity domains Pancakes Pathﬁnding Vacuum Dockyard 511 The pancakes puzzle Pancakes In domain k pancakes different sizes represented unique number 1 k A state domain pile k pancakes represented permutation numbers 1 k There single goal state state pancakes sorted largest smallest There k 1 state transition operators operator reverses ﬁrst pancakes permutation The heuristic domain GAP heuristic 21 adds 1 adjacent pancakes consecutive heuristic 4pancake state 1 3 2 4 2 gaps pancakes 1 2 In experiments experimented 40pancake puzzle Problems generated randomly performing 1000 random ﬂips goal state 512 Gridbased pathﬁnding Pathﬁnding In domain given grid cells task ﬁnd path cell As grid brc202d map popular Dragon Age Origins video game publicly available movingaicom repository 22 A state cell grid allowed transitions cardinal directions Problems generated randomly selecting cells given map The heuristic domain Manhattan distance Grid pathﬁnding signiﬁcantly different pancakes puzzle graph represents state space given explicitly input graph represents state space 40pancakes puzzle given implicitly start state allowed set operators As result pancake statespace combinatorially large Naturally means solving grid pathﬁnding problems general easier solving pancake puzzle 513 The vacuum cleaner Vacuum This domain inspired ﬁrst statespace presented Russell Norvigs textbook 23 A vacuum cleaner working grid 200 200 obstacles 35 cells number dirty spots The cleaner ﬁnd tour cleans dirty spots This problem variant traveling salesman problem TSP heuristic based minimum spanning tree3 Problems generated randomly putting vacuum cleaner 5 dirty locations 200 200 grid obstacles The number location obstacles grid different problem set randomly 2 There search algorithms reinsert nodes Case 3 For example Anytime Repairing A inconsistent list future usages In cases compute ˆP U union Open inconsistent list 3 There variant robot heavier carrying dirt unitcost version domain 18 stores nodes separated list called 48 R Stern et al Artiﬁcial Intelligence 267 2019 3957 hratios generated training set 40Pancakes puzzle left Fig 2 A sample cumulative distribution functions CDF h Vacuum cleaner domain right The different lines plots correspond CDFs generated different ranges h values This plot shows heuristic function domains accurate small h large h values accurate h values extremes 514 The dockyard robot Dockyard This domain inspired Ghallab et al 24 depots domain International Planning Competition IPC The task containers dockyard location Similar classic blocksworld domain containers stacked different piles topmost container pile accessed time Stacking unstacking container onfrom pile crane Moving container X pile Y pile Z involves actions unstacking X Y crane loading X mobile robot moving location Y location Z unloading X robot ﬁnally stacking X Z crane Each action associated cost Stacking unstacking pile costs 5 plus height pile Loading unloading container robot costs 1 Moving robot piles costs distance piles See Thayer Ruml 4 details heuristic computed The problems generated 5 cranes 8 containers stacked 5 piles average distance piles 5 52 Statistics collection In 4 domains 50 problems training set generating statistics required pBS conditions The remaining problems 40 problems dockyard domain 50 problems domains evaluate performance different pBS conditions search algorithms This set problems referred testing set The distribution needed Absolute condition F v created optimally solving problems training set For hratio condition computed h value start states problems Generating dis tribution needed Openbased condition Fhhn v complex requires gathering statistics states observed search To obtain statistics run APTS problems training set optimal solution Then sampled states generated APTS runs obtain states range h values h value ﬁnding optimal path corresponding goal For states computed h h values grouped bins according h values The resulting data set states h h h value bin signiﬁcantly different average bin contains 50 problems average h h values Appendix C describes details distribution generation process details h To illustrate outcome process Fig 2 shows cumulative distribution functions generated process h ratio y axis cumulative distribution Pancakes left Vacuum right In plots x axis h h smaller function CDF ratio computed portion problems training set h x axis value As noted grouped states close h values This shown Fig 2 curve corresponds h h CDFs generated different ranges h values The importance generating different CDFs different ranges h values evident plots domains observe h small large tends accurate h extremes For example Pancakes domain heuristic equal smaller 4 accurate But h 4 8 approximately 80 problems training set heuristic wrong 20 h 12 Understanding GAP heuristic accurate h small h large requires deeper h study domain GAP heuristic 53 Results In batch experiments evaluated basic pBS search framework uses offtheshelf anytime search algorithm decides halt according proposed pBS conditions Absolute hratio Openbased The anytime search algorithm APTS simple implement provides stateoftheart results 15 We R Stern et al Artiﬁcial Intelligence 267 2019 3957 49 Fig 3 The gain terms number nodes expanded Max f min stopping condition Dockyard left Pancakes right The xaxis shows cid2 values δ set 01 experiment Fig 4 The gain terms number nodes expanded Max f min stopping condition Pathﬁnding left Vacuum right The xaxis shows cid2 values δ set 01 compared proposed pBS conditions Absolute hratio Openbased baseline stopping conditions Max f min condition described Section 421 Oracle stopping condition described The Oracle stopping condition halts solution generator suboptimality incumbent solution equal lower desired suboptimality This means Oracle halts incumbent solution equal smaller 1 cid2 times optimal solution Thus use Oracle stopping condition ﬁrst solve optimally search problem hand Oracle given comparison purposes practical stopping condition requires solving given problem optimally Both baseline conditions Oracle Max f min serve extent lower upper bound considering δ ﬁnd solutions suboptimality 1 cid2 use Max f min stopping condition hope guarantee 1 cid2 suboptimality anytime search expanding fewer states Oracle stopping condition 531 Algorithm runtime comparison It common heuristic search literature compare runtime search algorithms comparing number nodes expand Such comparison valid runtime expanding node approximately constant equal evaluated algorithms In case added runtime hratio Absolute conditions negligible add single check goal The added runtime Openbased condition large adds nonconstant overhead new incumbent solution occur frequently domains Indeed observed experimentally average time node expansion evaluated algorithms stopping conditions differences negligible Therefore compared runtime performance different algorithms comparing number nodes ex panded solution Speciﬁcally computed ratio number nodes expanded baseline Max f min condition evaluated pBS conditions We refer ratio gain evaluated condition Figs 3 4 average gains obtained different pBS conditions mains δ 01 cid2 0 01 025 05 075 104 The xaxis desired suboptimality cid2 yaxis aforementioned gain Max f min 4 Note possible set cid2 values higher 1 domains chosen cid2 values suﬃcient display trends 50 R Stern et al Artiﬁcial Intelligence 267 2019 3957 Fig 5 The impact changing δ hratio pBS condition Results shown Vacuum left Dockyard right axes Fig 3 One curve shows results Oracle condition curves results hratio condition different values δ First pBS conditions Absolute hratio Openbased provide gain Max f min cases In domain pBS condition gain 2 value cid2 For example Dockyard Absolute hratio provide factor 10 speedup Max f min In domains Dockyard certain value cid2 gain Max f min diminishes eventually pBS conditions converge performance Max f min This values cid2 suﬃciently high Max f min suﬃcient halt search immediately ﬁrst solution In particular hs 1 cid2 equal larger ﬁrst solution anytime search algorithm pBS conditions including Optimal expand exactly number states Thus rate gain Max f min decreases 1 cid2 increases depends accuracy heuristic This explains gain pBS conditions converged quickly 1 Pancakes cid2 small 05 heuristic domain GAP heuristic known accurate optimal solutions tested problems larger 40 In contrast heuristic Dockyard accurate optimal solutions signiﬁcantly larger factor heuristic initial state Thus require larger values cid2 converge Max f min Dockyard It clear results proposed pBS conditions achieves best results Openbased performs grid pathﬁnding domain Pancakes puzzle achieves performance Optimal However domains performs poorly Absolute hratio perform similarly domains hratio better Vacuum cleaner domain poorer dockyard domain These results pBS conditions dominates deeper understanding underlying domain needed identify best pBS condition use given problem This left future work Let consider results Oracle stopping condition As expected Oracle serves upper bound performance pBS conditions However cases onpar pBS conditions Pancakes cid2 025 gain slightly smaller gain Openbased This Oracle returns solutions 1 cid2suboptimal pBS condition return solutions 1 cid2suboptimal 1 δ cases This effect noticeable results discussed far generated small value δ 01 532 Changing required conﬁdence 1 δ Next analyzed impact increasing δ parameter Setting δ higher 01 leads cases pBS conditions achieve better gains Oracle Speciﬁcally experimented δ 025 05 075 1005 Fig 5 shows gain Max f min hratio pBS condition Vacuum left Dockyard right domains Each curve shows results hratio stopping condition different δ values For reference added curve Oracle results As expected increasing δ causes hratio stopping condition halt earlier yielding higher gains runtime This veriﬁes key property effective pBS stopping condition considers effectively cid2 δ parameters Moreover high δ values results hratio stopping condition surpasses Oracle stopping condition However occurs δ values higher 025 The trends discussed observed domains pBS stopping conditions 533 Solution quality practice Next consider actual cost solutions pBS framework different pBS conditions In particular analyzed actual suboptimality solutions compared desired suboptimality required conﬁdence follows For combination pBS condition cid2 δ computed actual suboptimality solution obtained problem benchmark We calculated ratio problems 5 Note setting δ 100 possible setting δ 100 corresponds halting immediately ﬁrst solution R Stern et al Artiﬁcial Intelligence 267 2019 3957 51 Table 1 Dockyard ratio problems actual solution suboptimality smaller given value columns different pBS conditions values cid2 1 δ 09 table 1 δ 075 table Absolute hRatio Openbased 110 100 125 1 cid2 Required conﬁdence 1 δ 09 100 110 125 150 175 200 094 060 012 000 000 000 100 100 100 086 076 066 100 100 084 040 006 004 Required conﬁdence 1 δ 075 000 010 025 050 075 100 094 084 068 018 006 004 100 100 092 086 072 062 076 046 012 000 000 000 150 175 200 100 110 125 150 175 200 100 110 125 150 175 200 100 100 100 100 100 100 100 100 100 100 100 098 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 094 046 002 000 000 000 078 036 000 000 000 000 100 098 064 010 004 004 098 088 050 008 004 004 100 100 098 078 064 062 100 100 096 074 064 060 100 100 100 100 100 098 100 100 100 100 100 096 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 076 018 000 000 000 100 076 018 000 000 000 100 100 100 062 018 006 100 100 100 062 018 006 100 100 100 100 088 070 100 100 100 100 088 070 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 actual suboptimality larger ﬁxed value V set 10 11 125 15 175 20 These results Dockyard domain presented Table 1 Each column corresponds combination pBS condition value V Each row corresponds different desired suboptimality 1 cid2 The tables corresponds required conﬁdence 1 δ 09 075 respectively For example consider second entry Absolute pBS condition V 110 row 1 cid2 125 table 1 δ 09 The value entry 084 This means 84 problems actual suboptimality obtained Absolute pBS condition setting 1 δ 09 1 cid2 125 resulted suboptimality equal greater 110 V Similarly entry right indicates actual suboptimality 100 problems solved conﬁguration equal greater 125 We highlighted bold table entries V equal desired suboptimality 1 cid2 By Deﬁnition 1 pBS search algorithm value 1 δ entries As seen holds values cid2 δ experimented proposed pBS conditions In fact values bold entries usually larger corresponding 1 δ values We observed similar behavior domains This suggests room future research come stronger pBS search algorithm halt earlier maintain desired guarantees 6 cid2Aware solution generator So far considered instances Psf solution generator anytime search algorithm com pletely decoupled behavior search algorithm node expand stopping condition In section propose solution generator speciﬁcally designed ﬁnd solutions satisfy stopping condition Consider Absolute hratio conditions Both compute threshold value beginning search halt incumbent solution equal lower threshold Thus Psf instances use conditions propose use boundedcost search BCS algorithm solution generator setting costbound stopping conditions threshold value In experiments Potential Search BCS algorithm 15 However BCS algorithm solution generator way possible Openbased condition constant threshold Developing solution generator speciﬁcally tuned Openbased condition topic future work 61 Experimental results We evaluate experimentally Psf instances including use BCS algorithm solution generator In ad dition compare performance Psf instances Dynamic Potential Search DPS recently introduced stateoftheart boundedsuboptimal search algorithm 5 Here use gain Max f min APTS solu tion generator metric comparison In domain value cid2 highlight bold best performing algorithm highest gain Tables 2 3 gain results δ 01 evaluated algorithms The results domains values cid2 best performing algorithm pBS search algorithm An exception Pathﬁnding small values cid2 DPS outperformed pBS search algorithms However starting cid2 025 higher pBS search algorithm best performance As results presented earlier universal winner different pBS search algorithms suitable different settings Openbased particularly useful Pancakes Pathﬁnding domains The boundedcost pBS search performs Dockyard general robust effective 52 R Stern et al Artiﬁcial Intelligence 267 2019 3957 Table 2 The gain Max f min Pancakes Dockyard δ 01 The columns O R A represent Openbased hratio Absolute respectively Domain search cid2 Bounded cost Bounded cost Dockyard Pancakes Anytime Anytime O R A R A DPS O R A R A DPS 000 010 025 050 075 100 918 296 159 100 100 100 377 100 100 100 100 100 377 100 100 100 100 100 263 514 134 099 101 101 213 203 136 098 101 100 084 206 123 098 100 100 108 167 233 230 260 195 157 338 462 658 1278 1094 221 339 801 1719 1393 1190 274 688 660 539 454 1428 313 534 624 654 1643 1785 169 300 417 364 273 318 Table 3 The gain Max f min Pathﬁnding Vacuum δ 01 The columns O R A represent Openbased hratio Absolute respectively Domain search cid2 Bounded cost Bounded cost Pathﬁnding Anytime Anytime Vacuum O R A R A DPS O R A R A DPS 000 010 025 050 075 100 101 131 403 217 109 100 104 134 152 196 109 100 108 139 132 126 100 100 565 386 245 132 097 092 604 395 254 126 093 090 643 453 267 139 099 094 100 148 149 190 163 118 143 510 633 545 198 119 148 333 349 243 175 111 275 633 599 330 131 090 255 481 452 197 122 091 138 323 281 169 110 088 7 Discussion The key assumptions work summarized 1 Problems observed past provide representative statistics problems given future 2 The search algorithm access representative sample past problems 3 It possible obtain optimal solutions sampled problems Here discuss assumptions reasonable 71 A distribution problems As evident Deﬁnition 1 notion pBS search algorithm deﬁned respect distribution set search problems Thus pBS search relevant cases single search problem needs solved We argue realistic search problems oneshot problems For example consider navigation applications Waze Google Maps These applications constantly solving pathﬁnding search problems road map searching paths different sourcedestination pairs By analyzing historical data estimate navigation requests need handled future This true applications compute minimal number hops proﬁles social web 25 navigating robots warehouse 26 72 The availability representative training set Having representative training set common assumption analysis machine learning algorithms Even heuristic search literature prior works assumed having representative training set especially context algorithms predict search effort 2730 Indeed settings having training set natural pathﬁnding example mentioned In cases required generate training set actively sampling state space The procedure sample state space needs designed distribution sampled states similar real distribution start states In domains diﬃcult domains sampling states distribution easy For example sampling 15puzzles problems uniform distribution state space generating random permutation 15 tiles verifying mathematically resulting permutation represents solvable 15puzzle problem 31 A general approach sampling states perform sequences random walks set known start states While random walks guaranteed provide representative selection possible startgoal pairs generate sample states search space commonly approach R Stern et al Artiﬁcial Intelligence 267 2019 3957 53 73 Solving optimally training set instances While search problems hard solve optimally solved optimally time consum ing Our work focuses cases assume time consuming process solving training set optimally preprocessing step Performing costly preprocessing step order speed runtime solving future problems common search algorithms 32 ﬁelds Future work investigate handle cases solve training set problems 8 Related work This work pBS search related previously studied topics First discuss relation pBS search PAC learning 8 learning heuristic search Section 81 Then discuss relation pBS search Pearl Kims notion δrisk admissibility 3 Section 82 Finally discuss works ﬁelds consider similar form probably boundedsuboptimal solution guarantee Section 83 81 Learning heuristic search The notion pBS search inspired notion PAC learning 8 earlier versions work referred pBS search PAC search The terminology change PAC search pBS search emphasize pBS search PAC learning signiﬁcantly different A learning algorithm called PAC learning algorithm probability higher 1 δ learns classiﬁer correct probability higher 1 cid2 The 1 δ deﬁnition refers probability drawing representative samples training 1 cid2 refers probability training classiﬁer draw instance classiﬁed correctly In pBS search parameters cid2 δ refer happens training 1 δ probability drawing problem pBS search algorithm output solution 1 cid2 times optimal solution Thus δ PAC learning corresponds cid2 pBS Also notion solution quality PAC learning instance classiﬁed correctly As true equivalent PAC learning cid2 parameter pBS search The possible connection PAC learning framework heuristic search previously noted literature Ernandes Gori 33 They artiﬁcial neural network generate heuristic function ˆh ˆh heuristic solve likely admissible admissible high probability Experimentally showed A 15puzzle quickly return optimal solutions cases This viewed special case pBS search concept cid2 0 δ 0 In addition bounded quality returned solution function parameters 1 P ˆh probability ˆh overestimating optimal cost 2 d length number hops optimal path ˆh heuristic optimal given 1 P ˆh d goal Speciﬁcally probability path A However formula given theoretical observation In practice length optimal path goal d known problem solved optimally bound identify solution probably optimal practice Other search algorithms use machine learning techniques generate accurate heuristics pro posed 3435 Samadi et al 34 artiﬁcial neural network learn accurate heuristic values heuristics features This heuristic A search To improve quality solutions learned heuristic learning process biased generating admissible heuristic Experimental A results shown quality solutions A learned heuristic close optimal While learned heuristic shown highly effective practice theoretical analysis given suboptimality achieved For large state spaces hard obtain training examples bootstrapping approach proposed 35 An initial heuristic learned examples solved easily Following learned heuristic search solutions harder examples The solutions training set learning improved heuristic This process repeated leading learning heuristics increasing accuracy The ﬁnal heuristic generated process current stateoftheart heuristic domains 35 For work learned heuristic shown effective practice theoretical analysis given suboptimality achieved 82 δRisk admissibility The concept pBS search reminiscent δrisk admissibility concept deﬁned seminal work semi admissible heuristic search Pearl Kim 3 They deﬁned δrisk admissible search algorithm algorithm informally bounds δ risk solution returns optimal To quantify risk introduced concept risk function proposed possible risk functions A δrisk admissible search algorithm deﬁned respect speciﬁc risk function search algorithm halt node Open risk larger δ They introduced actual δriskadmissible algorithm called R bestﬁrst search expands nodes according risk 54 R Stern et al Artiﬁcial Intelligence 267 2019 3957 One risk functions proposed Pearl Kim probability node n f n C C incumbent solution cost While appearing similar risk function major difference pBS search algorithm δriskadmissible algorithm A δriskadmissible algorithm verify n Open P r f n C δ pBS search algorithm needs verify cid10 P r nOpen f n C δ As δriskadmissible algorithm necessarily pBS search algorithm R algorithm pBS search 83 Probably suboptimal guarantees related ﬁelds To best knowledge ﬁrst characterize propose heuristic search algorithms guarantee return solution boundedsuboptimal high probability This kind guarantee discussed ﬁelds For example term probably approximately correct search information retrieval ﬁeld problem searching document collection set independent computers 36 A correct search deﬁned result deterministic search set computers A randomized distributed search algorithm proposed returns search results approximately correct high probability Another example probably suboptimal guarantee discussed ﬁeld experiment planning context problem called satisﬁcing search problem 37 This problem consists set experiments performed order Each experiment cost set possible outcomes fail success The distribution possible outcomes experiment given goal obtain satisfying conﬁguration experiment outcomes The task build strategy choosing experiments perform A probably approximately optimal algorithm proposed returns strategy high probability approximately optimal terms expected experiment cost A somewhat related topic complexity theory approximation algorithms probabilistically checkable proofs PCP 3839 A key question complexity theory problems eﬃciently approximated eﬃcient algorithm guaranteed return solution suboptimality bounded Complementing question correctness solution problem veriﬁed probabilistically having proof correctness The relation pBS search boundedsuboptimal search algorithm fact approximation algorithm albeit possibly eﬃcient verifying probabilistically solution optimal corresponds pBS guarantee 9 Conclusion future work In paper introduced novel form boundedsuboptimality search algorithms called probably bounded suboptimal search pBS search A pBS search algorithm provides control suboptimality solutions ﬁnds conﬁdence achieving suboptimality A general framework named Psf introduced basis devel opment pBS search algorithms instances framework proposed Key development instances use statistics generated analyzing training set problems optimal solutions Some Psf instances propose require representative sample problems require involved statis tics nodes generated search A thorough experimental evaluation shows Psf instance dominates success depends properties domain Since ﬁrst work subject possible directions future work Some directions mentioned Section 7 Another interesting direction future work consider state features meaningful statistics predicting heuristic error For example pathﬁnding Manhattan distance heuristic accurate areas obstacles extremely accurate open spaces We introduced pBS conditions proved holds individually explored pBS conditions combined A naive combination pBS condition form disjunction halt rules says The resulting pBS condition guaranteed satisfy suboptimality conﬁdence different pBS conditions accept solutions suboptimality higher desired suboptimality different problems However future work explore construct ensemble pBS conditions valid way Also deeper inspection thresholdbased pBS conditions shows exhibit runtime improve ments easier instances harder ones To suppose computed threshold T δ 0 δ 0 Recall intuitively means proportion problems training set optimal cost T δ 0 δ Now consider problem optimal solution cost larger T δ 0 Using pBS search algorithm uses Absolute pBS condition solve problem δ cid2 0 end halting Max f min condition true This happens problem proven solved optimally R Stern et al Artiﬁcial Intelligence 267 2019 3957 55 speedup problem compared baseline case δ 0 In contrast Absolute pBS condition halt earlier problems optimal solution smaller T δ 0 Thus Absolute pBS condition effective prob lems smaller optimal solution cost generally problems easier solve A similar effect occurs hratio pBS condition problems heuristic value start state signiﬁcantly smaller optimal solution cost In words pBS conditions risk missing desired suboptimality easier problems achieving harder problems While desirable cases cases Future research needed devise pBS conditions control bias An exciting line future work explore search strategies heuristics speciﬁcally designed pBS search All Psf instances proposed admissible heuristic heuristic followed fairly standard bestﬁrst search framework However develop pBS conditions consider inadmissible heuristics statistics instead addition admissible heuristic This especially interesting modern methods use machine learning generate heuristics accurate inadmissible 35 In addition consider search strategies bestﬁrst search solution generators For example depthﬁrst branch bound depthﬁrst anytime search algorithm known effective domains An open question implement stopping conditions like Openbased condition Max f min algorithm Acknowledgements The authors wish thank Ariel Felner Robert Holte assistance advice preliminaries version work We thank Daniel Gilon Vitali Sepetnisky Mathew Hatem providing source code basic framework implemented ran experiments Appendix A Correctness hRATIO condition The proof Theorem 2 follows lines proof Theorem 1 given completeness Proof Let P problem drawn P according distribution D let A instance Psf uses hratio condition When A halts P Max f min condition hratio condition satisﬁed If condition satisﬁed incumbent solution guaranteed desired suboptimality value δ If A halted hratio condition means cost A P hsi T R cid2 δ consequently cost AP T R cid2 δ Since hsi 1 δ By deﬁnition T R cid2 δ deﬁned largest value v F R v F R 1cid2 1 δ follows F R cost AP hsi 1cid2 cid3 cid2 cid4 P r cost A P hsi 1 cid2 Opt hsi cid5 P D 1 δ cid2 Appendix B The incorrectness lowerbounded ratiobased pBS condition In preliminary work pBS search referred PAC search 7 proposed additional pBS condition referred LowerBounded RatioBased PAC Condition This condition reﬁnement hratio condition instead considering value F R U 1 cid2 P r U 1 cid2 Opt hsi P D following P r Max f min hsi U 1 cid2 Opt hsi P D The intuitive explanation Max f min lower bound Opt lower bound suboptimality U However condition turns incorrect The reason requires knowing error distribution hsi given speciﬁc value Max f min This creates coupling stopping condition underlying search algorithm Max f min lower bound improve hratio pBS condition valid pBS condition Appendix C Sampling states different h values Here distribution Openbased condition Fhhn v generated experiments This process consists steps sampling solving binning Step 1 Sampling Let P 1 P 50 denote 50 problems training set We run APTS problems optimal solution Let HP denote set unique h values observed states generated solving 56 R Stern et al Artiﬁcial Intelligence 267 2019 3957 P let GenP h t denote set states h value equal t generated APTS solving problem P For problem P h value t HP sampled single state GenP h ht Thus APTS generated state h 3 50 problem sampled exactly 50 states h 3 GenP 1 h 3 GenP 2 h 3 Note sample uniformly GenP t need actually store states GenP t It suﬃcient maintain search number states generated far h t denoted Ct states denoted st When ﬁrst state h t generated st set state Ct set 1 Afterwards At end state s search st exactly state sampled uniformly GenP t cid9 t t increment Ct 1 set st state probability 1 Ct cid9 t expanded hs Step 2 Solving For sampled states compute h run optimal solver That state s sampled GenP t compute h h search problem s initial state goal goal P h Computing h sampled state trivial To compute ﬁnding optimal solution Step 3 Binning The resulting data set states h h values grouped bins according h values states This way resulted bin having 50 problems To avoid having bins h value bins similar distributions grouped adjacent bins difference average h h values The lefthand Fig 2 shows 001 Then compute bin CDF h CDFs generated Pancakes domain Appendix D Reproducing experimental results All source code run experiments described paper publicly available Github repository httpsgithub com galdreiman j PAC heuristic search git The speciﬁc version code experiments paper git tag aijfinal To build source code use Gradle build tool httpsgradle org run command gradle fatJar The resulting JAR ﬁle run following command line arguments CollectOpenBased This generates statistics Openbased condition CollectThresholdBased This generates statistics Absolute hratio conditions Run This runs Psf APTS solution generator Absolute hratio Max f min conditions problem set RunOracle This runs Psf APTS solution generator Oracle stopping condition problem set RunDPS This runs DPS problem set RunOpenBased Psf APTS solution generator Openbased condition problem set BoundedCostBased Psf PTS solution generator Absolute hratio conditions explained Section 6 References 1 PE Hart NJ Nilsson B Raphael A formal basis heuristic determination minimum cost paths IEEE Trans Syst Sci Cybern 4 2 1968 2 I Pohl Heuristic search viewed path ﬁnding graph Artif Intell 1 3 1970 193204 3 J Pearl JH Kim Studies semiadmissible heuristics IEEE Trans Pattern Anal Mach Intell 4 1982 392399 4 JT Thayer W Ruml Bounded suboptimal search direct approach inadmissible estimates International Joint Conference Artiﬁcial Intel ligence vol 2011 IJCAI 2011 pp 674679 5 D Gilon A Felner R Stern Dynamic potential search new bounded suboptimal search Symposium Combinatorial Search SoCS 2016 6 R Stern A Felner R Holte Probably approximately correct heuristic search Symposium Combinatorial Search SoCS 2011 7 R Stern A Felner RC Holte Searchaware conditions probably approximately correct heuristic search The Symposium Combinatorial Search 100107 pp 119123 SoCS 2012 8 LG Valiant A theory learnable Commun ACM 27 1984 11341142 9 RA Valenzano S Jabbari Arfaee JT Thayer R Stern NR Sturtevant Using alternative suboptimality bounds heuristic search International Conference Automated Planning Scheduling ICAPS 2013 pp 233241 10 RE Korf Depthﬁrst iterativedeepening optimal admissible tree search Artif Intell 27 1 1985 97109 11 RE Korf Linearspace bestﬁrst search Artif Intell 62 1 1993 4178 12 EA Hansen R Zhou Anytime heuristic search J Artif Intell Res 28 2007 267297 13 R Zhou EA Hansen Beamstack search integrating backtracking beam search International Conference Automated Planning Schedul ing ICAPS 2005 pp 9098 14 S Aine P Chakrabarti R Kumar AWA Intelligence IJCAI 2007 pp 22502255 214 2014 125 Scheduling ICAPS 2012 pp 270278 window constrained anytime heuristic search algorithm International Joint Conference Artiﬁcial 15 R Stern A Felner J van den Berg R Puzis R Shah K Goldberg Potentialbased boundedcost search anytime nonparametric A Artif Intell 16 JT Thayer R Stern A Felner W Ruml Faster boundedcost search inadmissible estimates International Conference Automated Planning R Stern et al Artiﬁcial Intelligence 267 2019 3957 57 17 R Stern S Kiesel R Puzis A Felner W Ruml Max min solving maximization problems heuristic search Annual Symposium anytime A provable bounds suboptimality The Conference Neural Information Processing Combinatorial Search SoCS 2014 pp 146156 18 M Likhachev GJ Gordon S Thrun ARA Systems NIPS 2003 pp 767774 19 A Felner U Zahavi R Holte J Schaeffer NR Sturtevant Z Zhang Inconsistent heuristics theory practice Artif Intell 175 910 2011 20 V Sepetnitsky A Felner R Stern Repair policies reopening nodes different search settings The Symposium Combinatorial Search 21 M Helmert Landmark heuristics pancake problem The Symposium Combinatorial Search SoCS 2010 pp 109110 22 N Sturtevant Benchmarks gridbased pathﬁnding IEEE Trans Comput Intell AI Games 4 2 2012 144148 httpweb cs du edu sturtevant 15701603 SoCS 2016 pp 8188 papers benchmarks pdf 23 S Russell P Norvig Artiﬁcial Intelligence A Modern Approach 3rd edition Pearson 2009 24 M Ghallab D Nau P Traverso Automated Planning Theory Practice Elsevier 2004 25 R Bakhshandeh M Samadi Z Azimifar J Schaeffer Degrees separation social networks Symposium Combinatorial Search SoCS 2011 pp 1823 26 PR Wurman R DAndrea M Mountz Coordinating hundreds cooperative autonomous vehicles warehouses AI Mag 29 1 2008 920 27 RE Korf M Reid S Edelkamp Time complexity iterativedeepeningA 28 U Zahavi A Felner N Burch RC Holte Predicting performance IDA 29 LH Lelis S Zilles RC Holte Predicting size IDA 30 L Lelis S Zilles RC Holte Improved prediction IDA s search tree Artif Intell 196 2013 5376 s performance epsilontruncation The Symposium Combinatorial Search SoCS 2011 conditional distributions J Artif Intell Res 37 2010 4183 Artif Intell 129 12 2001 199218 pp 108116 31 WW Johnson WE Story et al Notes 15 puzzle Am J Math 2 4 1879 397404 32 RE Korf A Felner Disjoint pattern database heuristics Artif Intell 134 12 2002 922 33 M Ernandes M Gori Likelyadmissible subsymbolic heuristics European Conference Artiﬁcial Intelligence ECAI 2004 pp 613617 34 M Samadi A Felner J Schaeffer Learning multiple heuristics AAAI Conference Artiﬁcial Intelligence 2008 pp 357362 35 S Jabbari Arfaee S Zilles RC Holte Learning heuristic functions large state spaces Artif Intell 175 1617 2011 20752098 36 I Cox R Fu L Hansen Probably approximately correct search Advances Information Retrieval Theory Lecture Notes Computer Science vol 5766 Springer 2009 pp 216 37 R Greiner P Orponen Probably approximately optimal satisﬁcing strategies Artif Intell 82 1990 2144 38 I Dinur Probabilistically checkable proofs codes International Congress Mathematicians 2010 p 265 39 S Arora S Safra Probabilistic checking proofs new characterization NP J ACM 45 1 1998 70122