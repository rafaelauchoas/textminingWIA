Artiﬁcial Intelligence 322 2023 103948 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint How ﬁnd good explanation clustering Sayan Bandyapadhyay Fedor V Fomin b Petr A Golovach b Nidhi Purohit b Kirill Simonov d Department Computer Science Portland State University United States America b Department Informatics University Bergen Norway c LIRMM Université Montpellier CNRS Montpellier France d Hasso Plattner Institute University Potsdam Germany r t c l e n f o b s t r c t William Lochet c Article history Received 2 November 2022 Received revised form 3 March 2023 Accepted 22 May 2023 Available online 1 June 2023 Keywords Explainable clustering Clustering outliers Multivariate analysis kmeans kmedian clustering powerful unsupervised machine learning techniques However complicated dependencies features challenging interpret resulting cluster assignments Moshkovitz Dasgupta Rashtchian Frost proposed elegant model explainable kmeans kmedian clustering ICML 2020 In model decision tree k leaves provides straightforward characterization data set clusters We study natural algorithmic questions explainable clustering 1 For given clustering ﬁnd best explanation decision tree k leaves 2 For given set points ﬁnd decision tree k leaves minimizing kmeansmedian objective resulting explainable clustering To address ﬁrst question introduce new model explainable clustering Our model inspired notion outliers robust statistics following We seeking small number points outliers removal makes existing clustering wellexplainable For addressing second question initiate study model Moshkovitz et al perspective multivariate complexity Our rigorous algorithmic analysis sheds light inﬂuence parameters like input size dimension data number outliers number clusters approximation ratio computational complexity explainable clustering 2023 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommons org licenses 4 0 1 Introduction Interpretation explanation decisions produced learning models including clustering signiﬁcant direction machine learning ML artiﬁcial intelligence AI given rise subﬁeld Explainable AI Explainable AI attracted lot attention researchers recent years surveys Carvalho et al 6 Marcinkeviˇcs Vogt 39 All works divided main categories premodeling 4847261634 postmodeling A preliminary version paper appeared extended abstract proceedings AAAI 2022 The research leading results supported Research Council Norway project BWCA grant 314528 European Research Council ERC grant LOPPRE reference 819416 DFG Research Group ADYN grant DFG 411362735 Corresponding author University Bergen PB 7803 N5020 Bergen Norway W Lochet kirillsimonovgmailcom K Simonov Email addresses sayanbpdxedu S Bandyapadhyay fedorfominuibno FV Fomin petrgolovachuibno PA Golovach williamlochetgmailcom httpsdoiorg101016jartint2023103948 00043702 2023 The Authors Published Elsevier BV This open access article CC BY license http creativecommons org licenses 4 0 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Fig 1 An example optimal solution 5means b An explainable 5means clustering c corresponding threshold tree For interpretation colors ﬁgures reader referred web version article 434544635 explainability While postmodeling explainability focuses giving reasoning decisions black box models premodeling explainability deals ML systems inherently understandable perceivable humans One canonical approaches premodeling explainability builds decision trees 4042 In fact signiﬁcant work explainable clustering based unsupervised decision trees 31923243341 In node decision tree data partitioned according features threshold value While threshold tree provides clear interpretation resulting clustering cost measured standard kmeansmedian objective signiﬁcantly worse cost optimal clustering Thus hand eﬃcient algorithms developed kmeansmedian clustering 1 challenging explain On hand easily explainable models output costly clusterings Subsequently Moshkovitz et al 41 fundamental work posed natural algorithmic question possible kill birds stone To precise possible design eﬃcient procedure clustering Is explainable small decision tree Does cost signiﬁcantly cost optimal kmeansmedian clustering To address question Moshkovitz et al 41 introduced explainable kmeansmedian clustering In scheme clustering represented binary threshold tree leaves correspond clusters internal node corre sponds partitioning collection points threshold ﬁxed coordinate Thus number leaves tree k number clusters sought Also cluster assignment explained thresholds corresponding rootleaf path For example consider Fig 1 Fig 1a shows optimal 5means clustering 2D data set Fig 1b shows explainable 5means clustering data set The threshold tree inducing explainable clustering shown Fig 1c The tree ﬁve leaves corresponding 5 clusters Note model explainability clustering clear geometric interpretation cluster formed set axisaligned cuts deﬁned tree As Moshkovitz et al argue classical kmeans clustering algorithm leads complicated clusters threshold tree leads easy explanation The advantage explainable approach evident higher dimensions feature values kmeans contribute formation clusters Moshkovitz et al 41 deﬁne quality explainable clustering cost explainability ratio cost explainable clustering cost optimal clustering Subsequently obtain eﬃcient algorithms computing explainable clusterings cost explainability Ok kmedian Ok2 kmeans In work propose new model explaining clustering called Clustering Explanation Our Our contributions approach explainability inspired research robustness statistics machine learning especially vast ﬁeld outlier detection removal context clustering 102017972530 In model given k meansmedian clustering like explain clustering threshold tree removing subset points To precise interested ﬁnding subset points S removed threshold tree T explainable clustering induced leaves T exactly given clustering removing points S For given clustering deﬁne optimal best explainable clustering minimizes size S given clustering explained removing minimum number points Thus Clustering Explanation measure explainability number outlying points removal turns given clustering explainable clustering The reasoning new measure cluster explainability following In certain situations satisﬁed small decision tree explaining clustering outlying data points We note given clustering explainable clustering explained threshold tree size S 0 In Fig 2 provide example optimal 5means clustering exactly data set Fig 1 However new explainable clustering obtained different way If remove small number points Fig 2b 9 red larger points explainable clustering optimal clustering removing 9 points 2 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Fig 2 An optimal 5clustering b explainable clustering ﬁts clustering removing larger red points We note Clustering Explanation corresponds classical machine learning setting interpreting blackbox model lies scope postmodeling explainability Surprisingly area widely unexplored comes rigorous algorithmic analysis clustering explanation Consequently study Clustering Explanation perspective computational complexity Our new model naturally raises following algorithmic questions Given clustering eﬃciently decide clustering explained threshold tree removing points ii Given clustering integer s eﬃciently decide clustering explained removing s points In work design polynomial time algorithm resolves ﬁrst question Regarding second question algorithm time 22 minsk n2d dnO1 decides given clustering n points Rd O1 time k 1approximation algorithm Clustering Explanation explained removing s points We n That polynomial time algorithm returns solution set sk 1 points removed best explainable clustering removes s points Moreover provide eﬃcient data reduction procedure reduces instance Clustering Explanation equivalent instance r 2s 1dk points Rd integer coordinates range 1 r The procedure speed algorithm Clustering Explanation long n 2s 1dk We complement algorithms showing hardness lower bound In particular Clustering Explanation approximated factor F s time f sndos computable functions F f Exponential Time Hypothesis ETH 28 fails All results appear Section 3 We provide new insight computational complexity model Moshkovitz et al 41 While vanilla kmedian kmeans problems NPhard k 2 21412 d 2 36 case explainable clustering We design simple algorithms computing optimal best explainable clustering kmeansmedian objective run time 4ndkO1 n2d n O1 respectively Hence constant k constant d optimal explainable clustering computed polynomial time The research approximation algorithms cost explainability 41815223237 implicitly assumes solving problem exactly NPhard However ﬁnd proof fact literature To ﬁll gap obtain following hardness lower bound An optimal explainable clustering f k nok time computable function f Exponential Time Hypothesis ETH fails This lower bound demonstrates asymptotically running times simple algorithms unlikely improved Our reduction yields problem NPhard These results described Section 4 Finally combine explainability models obtain Approximate Explainable Clustering model For collection n points Rd positive real constant ε 1 seek identify εn outliers cost explainable kmeansmedian remaining points exceed optimal cost explainable kmeansmedian clustering original data set Thus allowed remove small number points good original optimal solution While hardness result Section 4 holds explaining dataset sacriﬁcing small fraction points possible solve problem eﬃciently And model obtain algorithm running time 4dkcid3 O1 signiﬁcantly better dependence d k For k n example compare time bounds 4ndkO1 n2d dnO1 This algorithm appears Section 5 See Table 1 summary results cid3 Related work A recent series work 81522323738 investigated cost explainability ratio cost explainable clustering cost optimal clustering The work papers signiﬁcantly improved bound Ok kmedian Ok2 kmeans shown Moshkovitz et al 41 The state oftheart bound kmedian O poly log k kmeans O k poly log k We note works based random sampling orthogonal cuts different techniques use In related work Izza et al 29 studied redundancy explanations provided decision trees gave eﬃcient algorithms eliminate 3 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Table 1 A summary results Model Clustering Explanation Explainable Clustering Approximate Explainable Clustering O1 AlgorithmsUpper bounds 22 minsk n2dn k 1approximation Reduction Osdk points 4ndkO1 n2d n 4dkcid3 k n cid3 O1 O1 HardnessLower bounds No F sapprox f sndos f k nok redundancy We note investigation redundancy scope work Finally mention recent survey CañeteSifuentes Monroy MedinaPérez 5 experimental results multivariate decision trees 2 Preliminaries kmeansmedian Given collection X x1 xn n points Rd positive integer k task kclustering partition X k parts C1 Ck called clusters cost clustering minimized We follow convention previous work 41 deﬁning cost In particular kmeans consider Euclidean distance cid3 kmedian Manhattan distance For collection points X Rd deﬁne cid5c xcid52 2 1 cid3 cost2X min cRd cid2 xXcid3 point c simply means cost cost2C1 Ck cid3 cost1X kmedian simply median cost clustering cid3 Rd minimizing sum 1 mean X For clustering C1 Ck X Rd kmeans cid3 k i1 cost2Ci With respect Manhattan distance deﬁne analogously cid3 k i1 cost1Ci cid3 xXcid3 cid5c xcid51 minimized median X cost1C1 Ck cid3 mincRd Explainable clustering For vector x Rd use xi denote ith element coordinate vector 1 d Let X collection points Rd For 1 d θ R deﬁne Cutiθ X X1 X2 X1 X2 partition X X1 x X xi θ X2 x X xi θ Then given collection X Rd positive integer k cluster X follows If k 1 X unique cluster If k 2 choose 1 d θ R construct clusters C1 C2 C1 C2 Cutiθ X For k 2 select 1 d θ R construct partition X1 X2 Cutiθ X X Then clustering X deﬁned recursively union k1clustering X1 k2clustering X2 integers k1 k2 k1 k2 k We clustering C1 Ck explainable kclustering collection points X Rd C1 Ck constructed described procedure Threshold tree It useful represent explainable kclustering triple T k ϕ called threshold tree T rooted binary tree k leaves nonleaf node children called left right respectively ϕ U 1 d R U set nonleaf nodes T For node v T deﬁne collection points Xv X For root r Xr X Let v nonleaf node T let u w left right children respectively assume Xv constructed We set Xu Xw CutϕvX If v leaf Xv cluster A clustering C1 Ck explainable kclustering collection points X Rd threshold tree T k ϕ C1 Ck clusters corresponding leaves T Note T binary tree k leaves total number trees k 1th Catalan number upper bounded 4k Note binary tree k leaves exactly k 1 nonleaf nodes For collection X x1 xn n points 1 d denote coordiX set distinct values ith coordinates x ji j 1 n It easy observe construction threshold tree set points X Rd suﬃcient consider cuts Cutiθ θ coordiX values θ cuts canonical We threshold tree T k ϕ collection points X Rd canonical nonleaf node u V T ϕu θ θ coordiX Throughout paper consider canonical threshold trees Parameterized complexity ETH A parameterized problem cid6 subset cid7 N cid7 ﬁnite alphabet Thus instance cid6 pair I k I cid7 k nonnegative integer called parameter A parameterized problem cid6 ﬁxedparameter tractable FPT solved f k IO1 time computable function f Parameterized complexity theory provides tools refute existence FPT algorithm parameterized problem The standard way considered problem hard parameterized complexity classes W1 W2 We refer book 11 formal deﬁnitions parameterized complexity classes The basic complexity assumption theory 4 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Fig 3 An example 3means clustering b The ﬁrst vertical cut c The second horizontal cut class FPT formed parameterized ﬁxedparameter tractable problems FPT W1 W2 The hardness proved demonstrating parameterized reduction problem known hard considered complexity class A parameterized reduction manyone reduction takes input I k ﬁrst problem f kIO1 time cid3 gk f g computable functions outputs equivalent instance I Another way obtain lower bounds use Exponential Time Hypothesis ETH formulated Impagliazzo Paturi Zane 2728 For integer k 3 let δk inﬁmum real numbers c kSatisﬁability problem solved time O2cn n number variables The Exponential Time Hypothesis states δ3 0 In particular ETH implies kSatisﬁability solved time 2onn cid3 second problem k O1 cid3 k A kernelization kernel parametrized problem cid6 polynomial time algorithm given instance I k cid3 f k computable cid6 outputs instance I function f The function f called kernel size kernel polynomial f polynomial It shown decidable FPT problem admits kernel However unlikely FPT problems polynomial kernels We refer 11 recent book kernelization Fomin et al 18 details cid3 cid6 I k cid6 I cid3 cid6 ii I cid3 k cid3 k cid3 k 3 Clustering explanation Clustering explanation In Clustering Explanation problem input contains kclustering C1 Ck X Rd nonnegative integer s task decide collection points W X W s C1 W Ck W explainable kclustering Note Ci W 31 A polynomialtime k 1approximation In optimization version Clustering Explanation given kclustering C C1 Ck X Rd goal ﬁnd minimumsized subset W X C1 W Ck W explainable clustering In following design approximation algorithm problem based greedy scheme For subset W X let C W C1 W Ck W Also subset Y X deﬁne clustering induced Y CY C1 Y Ck Y Denote OPTY size minimumsized subset W clustering CY W explainable First following simple observation follows trivially deﬁnition OPT Observation 1 For subset Y X OPTY OPTX For cut θ 1 d θ coordiX let Li θ x Rd xi θ Ri θ x Rd xi θ Lemma 1 Consider subset Y X CY contains nonempty clusters It possible select cut θ 1 d θ coordiY subset W Y polynomial time cluster CY W fully contained Li θ Ri θ ii cluster CY W Li θ iii cluster CY W Ri θ iv size W OPTY Before prove lemma use design desired approximation algorithm The algorithm We start set points X We apply algorithm Lemma 1 Y X ﬁnd cut θ subset W 1 X cluster CX W 1 fully contained Li θ Ri θ Let X1 X W 1 Li θ X2 X W 1 Ri θ We recursively apply step X1 X2 separately If level point set subset single cluster simply return An illustration algorithm example clusters shown Fig 3 Here horizontal vertical cut separates point cluster We select ﬁrst cut vertical coordinate value 4 W 1 contains square red point Fig 3b After cut X1 contains cluster return recursive branch X2 contains clusters So divide point set choosing horizontal cut 5 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 coordinate value 4 This cut separates point cluster shown Fig 3c Thus case W 1 square red point After cut contains single cluster return branches Hence obtain explainable clustering removing 2 points The correctness algorithm trivially follows Lemma 1 In particular recursion tree algorithm gives rise desired threshold tree Also algorithm runs polynomial time successful cut θ polynomial time algorithm ﬁnds k 1 cuts separate clusters The claim follows properties ii iii Lemma 1 Consider threshold tree generated algorithm For internal node u let Xu corresponding points W u points removed Xu ﬁnding explainable clustering points Xu W u Note k 1 nodes The total number points removed X ﬁnding explainable clustering W u By Lemma 1 cid3 u W u OPTXu Now Xu X Observation 1 OPT Xu OPTX It follows cid2 W u k 1 OPTX u Theorem 1 There polynomialtime k 1approximation algorithm optimization version Clustering Explanation By noting OPTX 0 C explainable clustering obtain following corollary Corollary 1 Explainability given kclustering Rd tested polynomial time Proof Lemma 1 We probe possible choices cuts θ 1 d θ coordiY select incurs minimum cost We select subset W points removed wrt cut The cost cut exactly size W Fix cut θ We following cases In ﬁrst case clusters CY strictly half points contained Li θ In case select cluster C minimum intersection Li θ Put cid3 Ri θ W The second case points C Li θ W Also cluster C symmetric ﬁrst clusters CY strictly half points contained Ri θ In case select cluster C minimum intersection Ri θ Put points C Ri θ W cid3 Li θ W In cases ﬁrst desired Also cluster C properties satisﬁed CY W In case cluster C CY add smaller C Li θ C Ri θ W In case C Li θ C Ri θ break tie way properties ii iii satisﬁed As CY contains clusters Moreover property trivially satisﬁed cid3 CY points C cid3 CY points C cid3 cid3 In showed choices cuts possible select W ﬁrst properties satisﬁed Let wm minimum size set W cuts As select cut size W minimized suﬃcient wm OPTY Y CY W clusters C CY In words CY W number clusters CY Consider optimal set W θ In ﬁrst abovementioned cases suppose W Let k explainable Let θ canonical cut corresponding root threshold tree corresponding explainable clustering Such cut exists CY contains clusters Let cid4W set selected algorithm correspond CY W θ ing cut θ But fully k θ choosing root cut cuts arrive contains points k CY In case algorithm adds contradiction Hence C θ minimized C CY cluster points C Li OPTY The proof second case cid3 CY points C C ﬁrst case We discuss proof case Consider clusters C CY θ nonempty Note clusters points cid4W But C Li θ For cluster C add W smaller C Li θ C Ri θ cid4W Hence case cid4W W OPTY The lemma follows noting wm cid4W cid2 contain points parts C Li clusters However deﬁnition threshold tree use k θ cid4W size C Li contains points C Li cuts separate points Y W θ fully contained W θ cid4W Thus cid4W W contain C Li θ need k θ C Ri θ C Ri cid3 Li C Li cid3 Ri cid3 cid3 32 Exact algorithm Our 22 minsk n2d dnO1 time algorithm based novel dynamic programming scheme Here brieﬂy algorithm Our ﬁrst observation subproblem deﬁned wrt bounding box Rd cut split point set threshold tree axisparallel hyperplane The number distinct bounding boxes 6 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Fig 4 The collection X interval b Z outside b b splits Y Subfamilies X Y X bproper subfamilies X Y Z X Y Z 1feasible respect b Y split interval n2d dimension box speciﬁed bounding values This explains n2d factor running time Now consider ﬁxed bounding box corresponding subproblem containing number given clusters maybe partially If new canonical cut splits cluster resulting parts removed choice passed dynamic programming As remove s points number clusters k number distinct choices bounded 22 minsk This roughly gives following theorem Theorem 2 Clustering Explanation solved 22 minsk n2d dnO1 time Before formal proof theorem let introduce speciﬁc notations illustrated Fig 4 Let b Rd b We denote b x Rd x b b interval For collection points X Rd X b X b X outside b X b b splits X X b cid13 X b cid13 Let X family disjoint collections points Rd A subfamily Y X said bproper X X b Y ii X X outside b included Y Note X X split b Y Y The truncation X respect b family trabX X b X X st X b cid13 For integer s 0 X sfeasible respect b b splits s collections X cid5 k Proof Theorem 2 Let C s instance Clustering Explanation C C1 Ck disjoint collections points Ci Rd Let X i1 Ci We vector z R d canonical zi coordiX 1 d Recall suﬃces consider canonical cuts construction decision tree canonical vectors represent sequences canonical cuts dimensions W S W S For pair canonical vectors b b C sfeasible respect b b proper S S1 Scid9 C denote ωa b S minimum size collection points W X b trabS explainable cid9clustering We assume ωa b S 0 S S Intuitively tuple b S deﬁnes subproblem dynamic programming bounded box b clusters S explained That S bproper necessarily contains clusters b clusters split b split clusters S assumed contained b removing extra points Clearly S sfeasible respect b simply accounting clusters split b exceeds budget s cid3 1 S cid3 1 cid3 cid9 cid3 cid9 Now instead ωa b S compute following slightly different value cid2 cid2 wa b S ωa b S Ci b Ci b 2 Ci S Ci CS Observe wa b S additionally accounts cost removing extra points clusters split b ωa b S computes cost explanation truncation S b Since interested clustering obtained deleting s points assume wa b S value bigger s This slightly informal deﬁnition simpliﬁes arguments In particular observe sums 2 value bigger s S sfeasible respect b In fact reason sums included 2 Notice C s yesinstance Clustering Explanation wa b C s 1 d b We proceed deﬁne computation The values wa b S computed different ways depending cid9 S C j b If cid9 1 ωa b S 0 deﬁnition Then If cid9 0 S ωa b S 0 wa b S cid3 S Ci 1 k Ci b cid13 C j C wa b S Ci b cid2 C j CCi C j b 7 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Assume cid9 2 values ωa For 1 d θ coordiX ai θ bi deﬁne vectors aiθ biθ setting cid3 Scid3 computed Scid3 cid9 cid3 b cid6 aiθ j θ j j j cid13 biθ j cid6 θ b j j j cid13 That biθ aiθ b intervals formed splitting b cit θ We θ s feasible C sfeasible respect biθ aiθ b For sfeasible θ partition S1 S2 S θproper S1 S2 biθ aiθ bproper respectively We deﬁne δiθ S Ci b cid2 Ci S Ci abiθ cid13 Ci aiθ bcid13 total size clusters C split cut θ b We compute ωa b S following recurrence wa b S min 3 right denoted minCi b cid2 C j cid2 C j b Ci S C j SCi C j CS minwa biθ S1 waiθ b S2 δiθ S 1 d θ coordiX S1 S2 partition S st ai θ bi θ sfeasible S1 S2 θproper We assume triple θ S1 S2 satisfying conditions deﬁnition set We assume value proves bigger s Intuitively term corresponds trying feasible cuts level subproblem reducing respective smaller subproblems The term additionally covers case remaining clusters solution covered following cuts play role case We prove correctness 3 showing inequalities left rights parts directions First wa b S This trivial wa b S Assume case Then Ci CS Ci b Let r ωa b S trabS assumption wa b S s Recall wa b S ωa b S let W X b collection r points S cid3 S explainable cid9clustering Assume S Ci1 Cicid9 Ci j cid3 cid3 cid9 1 b Let Wi W S Ci S Ci b W S Notice happen W j S cid3 j cid3 j j 1 cid9 Then Ci j b W j cid9 1 values j Suppose h 1 cid9 Cih h 1 cid9 j cid13 h In case obtain S W S cid3 1 S cid3 1 cid9 b W j Observe Ci j b cid13 Wh Ci j b W j cid3 cid3 cid3 cid9 ωa b S cid2 C j SCih C j b wa b S Cih b cid2 cid2 C j C j SCih C j CS C j b Then wa b S Assume case S j W j cid13 distinct indices j 1 cid9 Then wa b S cid3 cid3 j Because separate nonempty collections points deﬁnition explainable clustering implies 1 d θ coordiX ai θ bi partition I1 I2 1 cid9 property ˆS1 S W j j I1 explainable cid91 I1clustering clusters biθ ii ˆS2 W j j I2 explainable cid92 I2clustering clusters aiθ b ˆS1 ˆS2 contain nonempty S collections points Moreover assume S j W j j 1 cid9 S j W j placed ˆS1 S j point biθ S j points aiθ b placed ˆS2 cid5 We deﬁne S1 Ci j jI1 aiθ b W j We set W1 Observe W1 R1 W2 R2 partition W sets Denote w 1 W1 w 2 W2 let r1 R1 r2 R2 Clearly w 1 w 2 r1 r2 r biθ W j let W2 j I2 For j I1 let W1 cid7 cid5 j j R2 biθ W j j I1 S2 Ci j jI2 j Let R1 aiθ b j W2 cid7 cid5 W2 W1 jI2 jI1 W j cid5 cid8 cid8 8 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 S Notice Scid3 1 R1 j I1 trabiθ S1 Scid3 aiθ bproper respectively Furthermore h 1 2 ˆSh obtained Scid3 clusters Also R2 j I2 traiθ bS2 Also S1 S2 biθ h deleting points W1 S cid3 j cid3 j 2 4 5 6 7 8 C j biθ C j aiθ b C j b R1 C j b R2 cid2 C j S1 cid2 C j S2 cid2 C j S1 cid2 C j S2 cid2 C j CS1 Note C j biθ cid2 C j CS2 C j aiθ b cid2 C j CS C j b R1 R2 δiθ S R1 R2 Then 47 wa b S w1 w 2 r1 r2 w 1 w 2 r1 r2 cid2 C j S cid2 C j b cid2 C j b C j CS cid2 C j b C j b cid2 C j CS1 cid2 w 1 C j S1 C j biθ cid2 C j CS2 cid2 C j biθ C j S2 C j aiθ b 2r1 2r2 C j biθ C j S1 cid2 w 2 C j CS1 cid2 C j aiθ b C j aiθ b δiθ S C j S2 C j CS2 Recall wa b S s Note cid2 cid2 C j biθ C j CS1 C j CS C j b R2 Using 4 obtain cid2 C j S1 C j biθ cid2 C j CS1 C j biθ wa b S s This means S1 biθ proper Similarly S2 aiθ bproper Therefore w 1 w 2 cid2 C j S1 cid2 C j S2 C j biθ C j aiθ b cid2 C j CS1 cid2 C j CS2 C j biθ wa biθ S1 C j aiθ b waiθ b S2 This allows extend 8 conclude wa b S wa biθ S1 waiθ b S2 This shows wa b S concludes proof ﬁrst inequality Now wa b S The inequality trivial Suppose case Then assumption assigning value s Suppose minimum achieved ﬁrst Then Ci b C j cid2 cid2 C j SCi C j CS C j b 9 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 cid5 C j SC Ci S We deﬁne W C j b We family obtained trabS deletion points W explainable cid9clustering deleted points b C j S excepts Ci This implies ωa b S W obtain wa b S W Ci b Ci b Ci b C j C j b cid2 cid2 cid2 cid2 Ci S Ci CS C j SCi C j CS exactly ωa b S Assume minimum achieved second Suppose 1 d θ coordiX ai θ bi θ sfeasible partition S1 S2 S θproper chosen way achieves minimum value wa biθ S1 waiθ b S2 δiθ S cid7 cid5 cid7 cid5 C j S2 C j Then obtain Let R1 aiθ b C j S1 C j R2 biθ cid2 C j b R1 C j S1 cid2 C j b R2 C j biθ C j aiθ b cid2 C j S1 cid2 C j S2 cid2 C j S2 cid2 C j biθ C j CS1 C j CS2 δiθ S R1 R2 C j aiθ b cid2 C j CS C j b R1 R2 9 10 11 12 Let w 1 ωa biθ S1 w 2 ωaiθ b S2 Then collection W1 biθ X collection sets obtained collections trabiθ S1 deletions points W1 explainable S1clustering Sim ilarly collection W2 aiθ b X family collections obtained collections traiθ bS2 deletions points W2 explainable S2clustering Consider W W1 W2 R1 R2 The crucial obser vation family obtained collections trabS deletions points W explainable cid9clustering ﬁrst cut Cutiθ Using 912 obtain wa biθ S1 waiθ b S2 δiθ S W ωa b S cid2 Ci S cid2 Ci S Ci b Ci b cid2 Ci CS cid2 Ci CS Ci b Ci b wa b S Since completes correctness proof recurrence 3 In ﬁnal stage proof evaluate running time We construct table values wa b S pairs b canonical vectors b The total number pairs n 22d constructed n2d dnO1 time We interested b C sfeasible respect b Clearly given b sfeasibility checked dnO1 time If C sfeasible respect b bproper subfamilies S C listed brute force follows Observe X Ci C Ci b constructed polynomial time subfamily bproper S Let Y Ci C Ci split b Since C sfeasible Y mins k We construct Y polynomial time generate 2minsk subfamilies Z Y total 2minsk dnO1 time Then bproper subfamilies S exactly families form Z X We obtain 2minsk bproper subfamilies generated time 2minsk dnO1 time Then conclude dynamic programming algorithm computes 2minsk n2d values wa b S The value wa b S S constructed dnO1 time cid9 1 If cid9 2 recurrence 3 Computing polynomial time To compute 1 d θ coordiX partitions S1 S S S1 S2 required θproper This implies computed 2minsk dnO1 time Summarizing total running time dynamic programming algorithm 22 minsk n2d dnO1 This concludes proof cid2 33 Data reduction In section prove Clustering Explanation admits polynomial kernel parameterized k d s Thus instance Clustering Explanation effectively compressed equivalent size polynomial k d s irrespective original number points instance 10 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Theorem 3 Clustering Explanation parameterized k d s admits kernel r 2s 1dk points 1 rd Proof Let C s instance Clustering Explanation C C1 Ck disjoint collections points Ci Rd Let X cid5 k i1 Ci Our ﬁrst aim reduce number points For use procedure marks essential points For 1 k j 1 d following Order points Ci increasing order jth coordinate ties broken arbitrarily Mark ﬁrst mins 1 Ci points mins 1 Ci points ordering The procedure marks 2s 1dk points Then delete remaining unmarked points Formally denote Y collection marked points set Si Ci Y 1 k Then consider instance S s Clustering Explanation S S1 Sk We following claim Claim 31 C s yesinstance Clustering Explanation S s yesinstance Proof Claim 31 Trivially C s yesinstance S s yesinstance deleted points construct S s We S s yesinstance C s yesinstance Because S s yesinstance collection s points W Y S1 W Sk W explainable kclustering In words explainable clustering Y W canonical threshold tree T k ϕ clusters S1 W Sk W correspond leaves threshold tree We claim use threshold tree X W C1 W Ck W correspond leaves The proof contradiction Assume collection points corresponding leaf distinct C1 W Ck W Then node v V T j 1 k C j W split cut Cutiθ θ ϕv A B Cutiθ X A C j W cid13 B C j W cid13 Observe A S j W B S j W We assume loss generality A S j W case symmetric This means unmarked point x C j W A marked points C j W B Because C j unmarked point C j 2s 1 1 Following marking procedure order points C j increase ith coordinate breaking ties exactly marking procedure Let L collection ﬁrst s 1 points marked Since W s y L W Because L W S j W B yi θ Then xi yi θ x B contradiction We conclude use T k ϕ cluster X W C1 W Ck W correspond leaves This proves C s yesinstance Clustering Explanation cid2 cid5 k We obtained instance S s Y i1 Si cid9 2s 1dk points equivalent original instance Now modify points ensure 1 cid9d For observe 1 d values ith coordinates changed maintain order Formally following For 1 d yi let coordiY θ 1 d Then Si containing y replace y z Denote Z constructed collection points let R R1 Rk family collections points constructed S1 Sk For y Y construct point z setting zi j θ j We R s yesinstance Clustering Explanation S s yesinstance Z 1 cid9d Then data reduction algorithm returns R s To complete proof remains observe marking procedure polynomial coordinates replacement polynomial time cid2 1 θ ri θ 1 θ ri 34 Hardness approximation In Theorem 1 proved optimization Clustering Explanation problem admits polynomial k 1 approximation Here hard approximate minimum number deleted points s factor depends s Speciﬁcally provide parameterpreserving reduction Hitting Set Clustering Explanation transfers known results hardness parameterized approximation Hitting Set problem Clustering Explanation Recall decision version Hitting Set problem input family sets A universe U parameter cid9 goal decide set H U size cid9 H nonempty intersection set A In parameterized approximation variant cid9 parameter goal ﬁnd hitting set size F cid9cid9 G hitting set size cid9 The starting point reduction following result Hitting Set Karthik Laekhanukit Manurangsi 44 Theorem 4 Theorem 14 44 Assuming ETH f cid9U Aocid9time algorithm approximate Hitting Set factor F cid9 computable functions f F cid9 Note Theorem 14 44 stated Dominating Set problem standard parameterpreserving reduction Dominating Set Hitting Set Theorem 1328 11 statement immediately follows 11 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 cid3 m j1 S j Now intuitively given instance U A cid9 Hitting Set A S1 Sm reduction constructs clusters C0 The clusters C1 Cm represent sets S1 Sm family A C0 special cluster Cm R needs separated C1 Cm clustering explainable The separation performed removing special points C0 corresponds element universe U Removing point allows separation C0 C j corresponding set S j contains corresponding universe element The clusters separated special coordinate special point blocks separation This crux reduction results following theorem Theorem 5 For computable functions f F F sapproximation algorithm optimization version Clus tering Explanation running time f sndos ETH fails Proof We reduction Hitting Set Consider instance Hitting Set universe U family sets A size target hitting set cid9 We construct following instance Clustering Explanation denote A m The target dimension d equal sum set sizes family A d SA S The number clusters constructed instance m 1 clarity denote C0 Cm The target parameter s number points remove cluster set exactly cid9 cid3 Now clusters composed Intuitively clusters C1 Cm represent sets family A C0 special cluster needs separated C1 Cm clustering explainable The separation performed removing special points C0 correspond element universe U Removing point allows separation C0 C j corresponding set S j contains corresponding universe element The clusters separated special coordinate special point blocks separation This crux reduction cid9cid3 Formally deﬁne point sets C0 Cm terms coordinates Rd The constructed instance binary values zero vectors Order arbitrarily sets family A A S 1 Sm j 1 m cluster C j corresponds set S j The d coordinates partitioned m sets 1 d That ﬁrst range following way For j 1 m denote I j jcid3 j coordinates I1 ﬁrst S1 coordinates I2 following S2 coordinates Now j 1 m set C j consists F cid9 cid9 1 identical points w j w ji 1 j I j w ji 0 j I j The set C0 consists parts C0 O V First F cid9 cid9 1 identical zero vectors C0 denote set O Second element u universe U point vu V The coordinates vu set j 1 m u S j exactly coordinate I j vu set coordinate unique u S j If j 1 m u S j vui 0 I j More speciﬁcally S j A order arbitrarily elements S j S j u1 uS j For S j vui S j 1 After performing j 1 m set remaining coordinates vector vu zero This concludes construction S jcid3 1 S jcid3 jcid3 j jcid3 j cid3 cid3 cid10 Now F sapproximate solution constructed Clustering Explanation instance imply F cid9approximate solution original Hitting Set instance Speciﬁcally prove following Claim 32 Whenever exists set W X size F s s C0 W Cm W explainable clustering exists set H U hitting set A H W On hand hitting set H U exists solution W Clustering Explanation instance W H Proof In forward direction consider set W X We assume W C j j 1 m W O sets consist F s s 1 identical points replacing W smaller set intersecting sets solution Thus W V Recall points V corresponds element universe U Hitting Set instance Denote H subset U corresponding W H u U vu W Clearly H W claim H solution Hitting Set instance Consider threshold tree provides explanation C0 W Cm W By clusters cid3 cid3 nonempty For j 0 m denote C j j 1 m We C j W Recall assume C j C j j 1 m set H nonempty intersection set S j Since tree represents clustering cid3 cid3 cid3 cid3 cid3 X j 1 m exists cut tree separates C j C 0 That exists cut Cutiθ X 1 X 2 cid3 cid3 cid3 cid3 X C X C tree X 2 wlog The dimension cut necessarily belongs 0 j cid3 j indistinguishable O For I j exists point vu V vui 1 I j coordinates C cid3 u S j Since points C j set coordinate points O zero vu W Thus construction u H set S j hit H In direction assume exists hitting set H A We set W corresponding H subset V W H For j 1 m exists element u H u S j Consider corresponding element vu W coordinate I j vui 1 By construction vu element C0 ith cid3 cid13 j elements C jcid3 zero coordinate Thus cutting dimension coordinate j cid3 cid3 X 1 C 0 cid3 C j cid3 X 12 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 threshold θ set zero separates C j clusters Finally threshold tree C0 W Cm W constructed separating C j corresponding cut cid2 The theorem follows easily Claim 32 Namely assume exists F sapproximate algorithm Clustering Explanation running time f sndos algorithm correctly decides input instance solution size F s s solution size s We construct F cid9approximate algorithm Hitting Set follows First construct Clustering Explanation instance reduction Second run Clustering Explanation algorithm instance output answer If returns solution W size F s s Claim 32 solution H original Hitting Set instance size F s s F cid9 cid9 If solution Clustering Explanation instance size s Claim 32 solution Hitting Set instance size s cid9 This shows constructed algorithm provides F cid9approximation Hitting Set problem Finally running time bounded f sndos f cid9F cid9U mocid9 contradicts Theorem 4 ETH fails cid2 4 Explainable clustering Explainable kmeansmedian clustering We consider Explainable kmeans resp Explainable kmedian problem given collection X Rd n k points task ﬁnd explainable kclustering C1 Ck X minimum kmeans resp kmedian cost 41 Exact algorithms Our ndkO1 time algorithm simple based branching technique At nonleaf node threshold tree like ﬁnd optimal cut As focus canonical threshold trees number distinct choices branching nd Also number nonleaf nodes threshold binary tree k 1 following theorem Theorem 6 Explainable kmeans Explainable kmedian solved ndkO1 time Our n2d dnO1 time algorithm based dynamic programming following For vectors x y Rd write x y x y respectively denote xi yi xi yi respectively 1 d We highlight write x y require strict inequality coordinate Theorem 7 Explainable kmeans Explainable kmedian solved n2d dnO1 time Proof The algorithms problems Hence demonstrate Explainable kmeans For simplicity ﬁnd minimum cost clustering algorithm easily modiﬁed produce optimal clustering standard arguments Let X k instance problem X x1 xn X Rd Following proof Theorem 2 vector z R d canonical zi coordiX 1 d For pair canonical vectors b b positive integer s k compute minimum means cost explainable sclustering Xab xi X xi b denote value ωa b s We assume ωa b s Xab admit explainable sclustering It convenient assume ωa b s Xab k interested clusters Notice minimum means cost explainable kclustering X ωa 1 d We compute table values ωa b s consecutively s 1 2 k b b If s 1 deﬁnition cid6 ωa b s cost2Xab Xab cid13 Xab value computed polynomial time Let s 2 assume tables constructed lesser values s Consider pair b canonical vectors R d b For 1 d θ coordiX ai θ bi deﬁne vectors aiθ biθ setting cid6 aiθ j θ j j j cid13 biθ j cid6 θ b j j j cid13 Then compute ωa b s following recurrence 13 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 ωa b s minωabiθ s1 ωaiθ b s2 1 d θ coordiX ai θ bi s1 s2 1 s1 s2 s 13 The correctness 13 follows deﬁnition explainable clustering It suﬃcient observe compute optimum means cost explainable sclustering Xab minimum sums optimum costs explainable s1clusterings s2clusterings X1 X2 respectively X1 X2 Cutiθ Xab 1 d θ coordiXab s1 s2 s exactly 13 To evaluate running time observe compute ωa b s 13 consider d values n values θ k n values s1 s2 dn2 choices Thus computing ωa b s s 2 ﬁxed b Odn2 time Since n 22d pairs canonical vectors b obtain time compute table values Xab pairs vectors n2d dnO1 Since table s 1 constructed n2d dnO1 iterate 13 k 1 n times total running time n2d dnO1 cid2 42 Hardness In section hardness results Explainable kmeans Explainable kmedian problems NPcomplete W2hard parameterized k solved f k nok time computable function f ETH fails Moreover hardness holds input points binary More precisely prove following theorem Theorem 8 Given collection n points X 0 1d positive integer k n nonnengative integer B W2hard decide X admits explainable kclustering mean median respectively cost B problem parameterized k Moreover problems NPcomplete solved f k nok time computable function f ETH fails Proof We theorem means costs brieﬂy explain proof modiﬁed medians The case median cost easier fact collection binary points median binary vector We reduce Hitting Set problem The task problem given family sets W W 1 W m universe U u1 positive integer k decide S U size k hitting set W S W cid13 1 m This problem wellknown W2complete parameterized k input restricted families sets size 13 Let W W 1 W m family sets U u1 W 1 W m r let k positive integer We construct following points Let s 8nm2 For 1 n construct vector ui 0 1n setting cid6 ui j 1 j 0 We deﬁne Ui collection s points identical ui For 1 m let wi characteristic vector W That cid6 wi j 1 u j W 0 We deﬁne W w1 wm For t 16ns2 1024n3m3 construct collection Z t zero points Finally deﬁne cid8 cid7 cid5 n i1 Ui cid3 k 1 X k B mr 1 sn k mr 1 8nm2n k W Z Clearly X k B constructed considered instance Hitting Set polynomial time We claim W hitting set S size k X explainable k clustering means cost B cid3 cid3 For forward direction assume S hitting set size k Without loss generality assume clustering C1 Ck1 follows We set C1 x X xi1 1 We deﬁne k cid3 S k Let S ui1 uik j 2 k 1 14 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 C j x X xi j 1 j1cid11 h1 Ch j1 h1 Ch Then X j C j Observe constructed clustering explainable To let X0 X set X j X Cuti j 0 X j1 j 1 k 1 Notice Ui j C j j 1 k Z Ck1 Uh Ck1 h 1 m i1 ik Moreover S hitting set w j W included cluster Ch cid5 h 1 k Ck1 Z R R For j 1 k deﬁne c j ui j set ck1 zero vector Clearly j 1 k 1 2 Let W j W C j j 1 k Note C j W j Ui j j 1 k Because xi j 1 r 1 x W j cost2C j x C j j 1 k x exactly r nonzero elements cid5x c jcid52 2 j 1 k If x R cid5x ck1cid52 2 1 We obtain cid5x c j2 h1mi1ik Uh xC j cid3 cid5 cost2C1 Ck1 k1cid2 cid2 j1 xC j cid5x c jcid52 2 kcid2 cid2 j1 xW j cid5x c jcid52 2 cid2 xR cid5x ck1cid52 2 mr 1 sn k B Thus C1 Ck1 explainable k cid3 clustering means cost B For opposite direction let C1 Ck1 explainable k clustering cost2C1 Ck1 B Let cid3 ϕ canonical threshold tree clustering Notice point X binary nonleaf cid3 T k v V T ϕv 0 1 d We following property T k cid3 ϕ nonleaf node v V T right child leaf Suppose case Denote x root T let P x1 xp rootleaf path x1 x xi left child xi1 2 p P constructed starting root following left children achieve cid3 1 k Denote Cq leftmost leaf Because T k cluster corresponding leaf xp T Notice Z Cq n p 1 collections Ui included Cq 1 n Let i1 inp1 1 n distinct indices Ui j Cq Denote c mean Cq Observe j 1 n multiset jth coordinates points Cq contains s m ones t zeros Then j 1 n leaves right child leaf p k cid3 c j m s Cq obtain m s t 2s t 1 8ns 1 2s cost2Cq np1cid2 cid2 j1 xUi j cid5x ccid52 2 s np1cid2 j1 cid5u ji ccid52 2 cid8 cid7 cid7 1 1 2 sn k 1 sn k 1 2s sn k s n sn k 2nm n mn 1 sn k B cid8 1 1 s contradicting cost2C1 Ck1 B Because nonleaf node v V T right child leaf clustering obtained consecutive cutting cluster set points Then assume loss generality ktuple distinct indices i1 ik 1 n C1 x X xi1 1 C j x X xi j 1 j1cid11 h1 Ch j 2 k 1 We claim S ui1 uik hitting set W The proof contradiction Suppose S hitting set For 1 k 1 let Wi Ci W Notice S hitting set Wk1 cid13 We analyze structure clusters upper bound means costs For denote c1 ck1 means C1 Ck1 Let j 1 k consider C j mean c j We C j Ui j W j Then c ji j 1 Let h 1 n distinct j If x Ui j xh 0 Then multiset hth coordinates points C j contains W j m ones s zeros c jh m C j m s 1 8mn 15 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Recall x W exactly r elements equal This implies x W j cid7 2 r 1 cid8 cid8 cid5x c jcid52 2 cid7 1 1 r 1 8mn 1 1 4mn cid7 cost2C j r 1 cid8 1 1 4mn W j r 1W j 1 4m W j 14 r n cid5 Now consider Ck1 corresponding mean ck1 Notice Ck1 Z Wk1 R R h1mi1ik Uh Let h 1 n We xh 0 x Z Hence multiset hth coordinates points Ck1 contains Wk1 s m s ones t zeros Therefore ck1h m s Ck1 2s t 1 8sn Since x W exactly r n elements equal x Wk1 1 1 8mn r 1 4m 1 1 4mn 1 1 8sn cid5x ck1cid52 2 2 r 2 r r cid8 cid7 cid8 cid7 cid8 cid7 For x R x contains unique nonzero element cid5x ck1cid52 2 cid8 cid7 1 1 8sn 2 1 1 4sn Note R contains exactly sn k points Then cid2 cid2 cost2Ck1 cid5x ck1cid52 2 cid5x ck1cid52 2 xWk1 xR cid7 r 1 4m cid8 cid7 Wk1 sn k cid8 1 1 4sn rWk1 sn k 1 4m Wk1 1 4 15 Recall W1 Wk1 m Then combining 14 15 obtain cost2C1 Ck1 k1cid2 j1 cost2C j mr 1 sn k Wk1 1 2 B 1 2 Wk1 cid13 However contradicts means cost C1 Ckcid3 B This means S hitting set W This concludes hardness proof Explainable kmeans For median cost use exactly reduction Hitting Set Then W hitting set S size k X explainable k clustering median cost B The proof forward direction identical proof means replacement 2 cid5 cid51 For opposite direction proof follows lines proof means cost2 cost1 cid5 cid52 gets simpliﬁed assume medians binary In particular multiset hth coordinates points cluster C j contains zeros ones c jh 0 median c j Notice crucial proof means obtaining upper bounds values c jh Now immediately assume c jh 0 considered cases upper bound c jh lower bounds costs proof straightforward To second theorem note reduction Hitting Set polynomial This immediately implies NPhardness considered problems measures For lower bound ETH use wellknown fact 11 Chapter 14 Hitting Set admit algorithm running time f k n mok computable function f assuming ETH Because reduction polynomial value parameter cid3 k 1 obtain algorithm running time f k n mok problems constructed instance k contradict ETH cid2 cid3 5 Approximate explainable clustering Approximate explainable kmeansmedian clustering In Approximate Explainable kmeans given collection n points X Rd positive integer k n positive real constant ε 1 Then task ﬁnd collection points Y X Y 1 εX explainable kclustering Y kmedian cost exceed optimum k median cost explainable kclustering original collection points X Note ask construction Y corresponding clustering decision variant trivial Observe optimum cost unknown priori Approximate Explainable kmedian differs clustering measure 16 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Fig 5 An example deﬁnition set Zu node u T The points X partitioned stripes containing n cid17 6 ﬁgure points The set Zu stripe containing optimal cut represented red Any cut inside stripe gives partition X Zu optimal This means removing n points algorithm essentially reduces numbers different cuts n O k cid3 cid16 cid3n k cid3 cid3 Theorem 9 Approximate Explainable kmeans Approximate Explainable kmedian solvable 4dkcid3 cid3 k n O1 time cid8 cid7 2k1 k1 Formally let n Proof As proofs problems identical algorithm Approximate Explainable kmeans Let X Rd instance Approximate Explainable kmeans X n let T k ϕ optimal canoni cal threshold tree explainable kmeans clustering C1 Ck clustering induced T k ϕ The goal algorithm guess approximation T k ϕ Recall total number binary trees k leaves k 1th Catalan number Ck1 1 4k Since T binary tree k leaves guessing T requires k 4k tries Guessing ϕ complicated d n 1 choices node u T d possibilities choose coordinate coordiX 1 possibilities select threshold value This gives potentially dn 1k1 possibilities k 1 nonleaf nodes T The idea guess nonleaf node u T second element ϕu precision O cid3n k gives Od k cid17 note ﬁrst n cid3 0 algorithm trying possible values T ϕ computing value obtained clustering runs time 4k dk cid3 k n Let U denote set nonleaf nodes T Let ϕcid3 U 1 d R function obtained ϕ fol lowing rounding Consider u U assume ϕu j θ Denote x1 xn points X assuming sorted jth coordinate x1 j xn j Recall T k ϕ canonical θ coord jX x1 j xn j We deﬁne ϕcid3u j θ cid3 θ cid3 xincid31 j largest nonnegative integer xincid31 j θ Consider clustering obtained threshold tree T k ϕcid3 At node u U ϕu j θ ϕcid3u j θ cid3 θ cid3 xincid31 points x X misplaced Cutϕcid3uX exactly points θ cid3 x j θ By choice θ xi1ncid3 This means Zu denotes set points xh n uU Zu T k ϕcid3 T k ϕ induce exact clustering X Z Because Zu n partitions Cutϕcid3uX Zu CutϕuX Zu identical Fig 5 Therefore Z cid3 choices nonleaf node cid3 This means n O1 ends proof From let assume n k 1 n k cid3 1 Z kn cid3 h 1n cid3 0 cid3n cid3 cid16 cid3n k cid3 cid3n cid5 cid3 cid3 h 1n Our algorithm based observations The algorithm tries possible choices T Given T set nonleaf nodes U tries possible choices ϕcid3u j θ cid3 u U follows For u ﬁrst pick coordinate j 1 d Then sort X jth coordinate points assume X x1 xn x1 j xn j We consider possible values θ cid3 xincid31 0 k cid3 Furthermore algorithm constructs set points Zu xh cid5 For choice T ϕ algorithm constructs Z uU Zu removes Z computes value clustering induced T k ϕcid3 Y X Z Finally outputs set Y threshold tree T k ϕcid3 minimizes value clustering We value clustering T k ϕcid3 Y exceed value clustering optimum threshold tree Y Therefore obtain explainable kclustering Y cost upper bounded minimum cost explainable kclustering X Because set choices T ϕcid3 cid3 cid3n algorithm outputs set Y Y 1 cid3X This concludes description algorithm correctness proof set Z size k n Since 4k possible choices T d k cid3 tree conclude total number possible choices T ϕcid3 running time algorithm 4dkcid3 cid7 d k cid3 O1 This concludes proof cid2 1 possible choices ϕcid3u nonleaf node cid8 k1 1 This implies 4k 4dkcid3 cid3 k n cid8 k cid7 cid3 cid3 cid3 6 Conclusion In paper initiated study computational complexity variants explainable clustering Con cluding discuss limitations approach outline research directions state number open problems 17 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 Explainable kmeans Explainable kmedian Our ﬁrst question concerns FPTapproximability Explainable kmeans Explainable kmedian Recall Theorem 8 problems W2hard parameterized k The reduction rules algorithms running time f k ndok problems widely believed assumption complexity theory Theorem 6 indicates lower bound tight naive brute force algorithm solves problems time ndkO1 However hardness reduction exclude possibility existence eﬃcient FPT approximation algorithms Such algorithms exist vanilla clustering Particularly kmeans 1 ε approximation 2kεO1 nd time demonstrated Kumar Sabharwal Sen 31 The techniques kmedian 31 Our question Is possible ﬁnd 1 εapproximation Explainable kmeans Explainable kmedian time 2 f kε ndO1 In Theorem 7 algorithm solving Explainable kmeans Explainable kmedian time n2d ndO1 The exponential dependence d makes algorithm running time unsuitable instances large dimension d number features Due lower bound exponential dependence d unavoidable However plausible problem FPT parameterized d In words question possible obtain f d dnO1 time algorithm solving Explainable kmeans Explainable kmedian function f The question FPT approximation open In particular existence 1 εapproximation Explainable kmeans Explainable kmedian time 2 f dε nkO1 remains open Observe standard dimension reduction tools inappli cable explainable clusterings speciﬁc dimension crucial separating clusters contribute signiﬁcantly distance unclear distinguish dimensions The question valid approximation stronger parameterization k d 1 ε O1 Our Theorem 9 shows approximation Explainable kmeans Explainable kmedian time 2 f dkε n O1 time allowed sacriﬁce εn input points order approximation 8dk struct explainable clustering fact obtain explainable kclustering increasing cost Simultaneously theorem indicates main technical diﬃculty approximation fact small fraction points drastically increase clustering cost bounds paper Moshkovitz et al 41 cost explanation optimal kmeans medians clustering cid3 k n In Theorem 1 Clustering Explanation admits polynomialtime approximation factor k 1 We know exist polynomial time algorithms providing better ratio The concrete question leave open research approximation ratio log k Clustering Explanation achievable polynomial time algorithm Clustering explanation Further paper obtained number results Clustering Explanation problem aim explain given kclustering cost deletion bounded number points Theorem 5 shows unlikely minimum number s deleted points approximated f s nos time F s factor computable functions f F However Theorem 1 shows Clustering Explanation admits polynomial time approximation factor k 1 The concrete question leave open research approximation ratio log k Clustering Explanation achievable polynomial time algorithm The complexity lower bound Theorem 5 leads questions parameterized complexity Clustering Explanation parameterizations k d s combinations In particular Clustering Explanation FPT parameterized k d combined parameterizations k d s d s k Note Clustering Explanation assumptions input clustering C1 Ck partition input points However practical setting assumed C1 Ck real clusters For example C1 Ck optimum kmeans kmedians clustering Would assumption Clustering Explanation tractable Nonunary conditions generalizations From highlevel perspective idea Moshkovitz et al 41 adapt work design eﬃcient procedure clustering explainable small decision tree quality close optimal kclustering In model conditions decision tree unary form x y b natural research direction investigate general linear conditions like x y An interesting question right balance complexity conditions decision tree human ability grasp explanations But certain situations plausible assume decision tree non unary conditions reasonable interpretation purposes clustering From mathematical algorithmic perspectives trees interesting objects It easy construct examples decision trees linear combination coordinates provide better price explainability trees unary conditions However providing quantitative bounds eﬃcient algorithms computing trees way challenging Another interesting research direction study decision trees allow k leaves In case cluster assigned different leaves How helpful introduce leaves Frost Moshkovitz cid3 k leaves Rashtchian 21 proposed heuristic algorithm constructs decision tree given number k analyzed performance empirically Makarychev Shan 38 took theoretical approach problem designed parameter δ 0 1 polynomialtime randomized algorithm constructing threshold decision tree 1 δk leaves The constructed tree induces explainable clustering cost 1δ polylogk cost optimal 18 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 kclustering However best knowledge parameterized complexity constructing types decision trees remains completely unexplored Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Data availability No data research described article Acknowledgements We thank anonymous referees helpful comments suggestions References 1 Charu C Aggarwal Chandan K Reddy Eds Data Clustering Algorithms Applications CRC Press 2013 2 Daniel Aloise Amit Deshpande Pierre Hansen Preyas Popat NPhardness Euclidean sumofsquares clustering Mach Learn 75 2 2009 245248 3 Dimitris Bertsimas Agni Orfanoudaki Holly M Wiberg Interpretable clustering optimization approach Mach Learn 110 1 2021 89138 https httpsdoi org 10 1007 s10994 009 5103 0 doi org 10 1007 s10994 020 05896 2 4 Leo Breiman Random forests Mach Learn 45 1 2001 532 5 Leonardo CañeteSifuentes Raúl Monroy Miguel Angel MedinaPérez A review experimental comparison multivariate decision trees IEEE Access 9 2021 110451110479 httpsdoi org 10 1109 ACCESS 20213102239 6 Diogo V Carvalho Eduardo M Pereira Jaime S Cardoso Machine learning interpretability survey methods metrics Electronics 8 8 2019 832 2013 7 Deeparnab Chakrabarty Prachi Goyal Ravishankar Krishnaswamy The nonuniform kcenter problem 43rd International Colloquium Automata Languages Programming ICALP 2016 July 1115 2016 Rome Italy 2016 67 8 Moses Charikar Lunjia Hu Nearoptimal explainable kmeans dimensions Proceedings 2022 ACMSIAM Symposium Discrete Algorithms SODA 2022 Virtual Conference Alexandria VA USA January 9 12 2022 SIAM 2022 pp 25802606 9 Moses Charikar Samir Khuller David M Mount Giri Narasimhan Algorithms facility location problems outliers Proceedings Twelfth Annual ACMSIAM Symposium Discrete Algorithms Society Industrial Applied Mathematics 2001 pp 642651 10 Chen Ke A constant factor approximation algorithm kmedian clustering outliers Proceedings Nineteenth Annual ACMSIAM Sym posium Discrete Algorithms 2008 pp 826835 Algorithms Springer ISBN 9783319212746 2015 11 Marek Cygan Fedor V Fomin Lukasz Kowalik Daniel Lokshtanov Dániel Marx Marcin Pilipczuk Michal Pilipczuk Saket Saurabh Parameterized 12 Sanjoy Dasgupta The Hardness kMeans Clustering Department Computer Science Engineering University California 2008 13 Rodney G Downey Michael R Fellows Fundamentals Parameterized Complexity Texts Computer Science Springer ISBN 9781447155584 14 Petros Drineas Alan M Frieze Ravi Kannan Santosh S Vempala V Vinay Clustering large graphs singular value decomposition Mach Learn 56 13 2004 933 httpsdoi org 10 1023 B MACH 0000033113 59016 96 15 Hossein Esfandiari Vahab S Mirrokni Shyam Narayanan Almost tight approximation algorithms explainable clustering Joseph Seﬃ Naor Niv Buchbinder Eds Proceedings 2022 ACMSIAM Symposium Discrete Algorithms SODA 2022 Virtual Conference Alexandria VA USA January 9 12 2022 SIAM 2022 pp 26412663 16 Jean Feng Noah Simon Sparseinput neural networks highdimensional nonparametric regression classiﬁcation arXiv preprint arXiv1711 07592 2017 2019 112 2019 125 17 Qilong Feng Zhen Zhang Ziyun Huang Jinhui Xu Jianxin Wang Improved algorithms clustering outliers 30th International Symposium Algorithms Computation ISAAC 2019 Schloss DagstuhlLeibnizZentrum fuer Informatik 2019 18 Fedor V Fomin Daniel Lokshtanov Saket Saurabh Meirav Zehavi Kernelization Theory Parameterized Preprocessing Cambridge University Press 19 Ricardo Fraiman Badih Ghattas Marcela Svarc Interpretable clustering unsupervised binary trees Adv Data Anal Classif 7 2 2013 125145 20 Zachary Friggstad Kamyar Khodamoradi Mohsen Rezapour Mohammad R Salavatipour Approximation schemes clustering outliers ACM 21 Nave Frost Michal Moshkovitz Cyrus Rashtchian ExKMC expanding explainable kmeans clustering CoRR arXiv2006 02399 abs 2020 https Trans Algorithms 15 2 February 2019 arxivorg abs 2006 02399 22 Buddhima Gamlath Xinrui Jia Adam Polak Ola Svensson Nearlytight oblivious algorithms explainable clustering Advances Neural Information Processing Systems 34 Annual Conference Neural Information Processing Systems 2021 NeurIPS 2021 December 614 2021 Virtual 2021 pp 2892928939 httpsproceedings neurips cc paper 2021 hash f24ad6f72d6cc4cb51464f2b29ab69d3 Abstract html 23 Pierre Geurts Nizar Touleimat Marie Dutreix Florence dAlchéBuc Inferring biological networks output kernel trees BMC Bioinform 8 2 2007 24 Badih Ghattas Pierre Michel Laurent Boyer Clustering nominal data unsupervised binary decision trees comparisons state art methods Pattern Recognit 67 2017 177185 25 David G Harris Thomas Pensyl Aravind Srinivasan Khoa Trinh A lottery model centertype problems outliers ACM Trans Algorithms 15 3 26 Trevor Hastie Robert Tibshirani Generalized additive models Stat Sci 1 3 1986 297318 27 Russell Impagliazzo Ramamohan Paturi Complexity kSAT Proceedings 14th Annual IEEE Conference Computational Complexity Atlanta Georgia USA May 46 1999 IEEE Computer Society 1999 pp 237240 28 Russell Impagliazzo Ramamohan Paturi Francis Zane Which problems strongly exponential complexity J Comput Syst Sci 63 4 2001 512530 19 S Bandyapadhyay FV Fomin PA Golovach et al Artiﬁcial Intelligence 322 2023 103948 29 Yacine Izza Alexey Ignatiev Joao MarquesSilva On tackling explanation redundancy decision trees J Artif Intell Res 75 2022 30 Ravishankar Krishnaswamy Shi Li Sai Sandeep Constant approximation kmedian kmeans outliers iterative rounding Proceedings 50th Annual ACM SIGACT Symposium Theory Computing 2018 pp 646659 31 Amit Kumar Yogish Sabharwal Sandeep Sen Lineartime approximation schemes clustering problems dimensions J ACM 57 2 2010 5 httpsdoi org 10 1145 1667053 1667054 32 Eduardo Sany Laber Lucas Murtinho On price explainability clustering problems Proceedings 38th International Conference Machine Learning ICML 2021 pp 59155925 httpproceedings mlrpress v139 33 Zachary C Lipton The mythos model interpretability machine learning concept interpretability important slippery Queue 16 3 2018 3157 doi org 10 1016 j tcs 2010 05 034 34 Yang Young Lu Yingying Fan Jinchi Lv William Stafford Noble DeepPINK reproducible feature selection deep neural networks NeurIPS 2018 35 Scott M Lundberg SuIn Lee A uniﬁed approach interpreting model predictions Adv Neural Inf Process Syst 30 2017 47654774 36 Meena Mahajan Prajakta Nimbhorkar Kasturi R Varadarajan The planar kmeans problem NPhard Theor Comput Sci 442 2012 1321 https 37 Konstantin Makarychev Liren Shan Nearoptimal algorithms explainable kmedians kmeans Proceedings 38th International Con ference Machine Learning ICML vol 139 PMLR 2021 pp 73587367 httpproceedings mlrpress v139 38 Konstantin Makarychev Liren Shan Explainable kmeans dont greedy plant bigger trees Stefano Leonardi Anupam Gupta Eds STOC 22 54th Annual ACM SIGACT Symposium Theory Computing Rome Italy June 20 24 2022 ACM 2022 pp 16291642 39 Riˇcards Marcinkeviˇcs Julia E Vogt Interpretability explainability machine learning zoo minitour arXiv preprint arXiv2012 01805 2020 40 Christoph Molnar Interpretable machine learning Lulu com 2020 41 Michal Moshkovitz Sanjoy Dasgupta Cyrus Rashtchian Nave Frost Explainable kmeans kmedians clustering Proceedings 37th Inter national Conference Machine Learning ICML vol 119 PMLR 2020 pp 70557065 httpproceedings mlrpress v119 moshkovitz20a html 42 W James Murdoch Chandan Singh Karl Kumbier Reza AbbasiAsl Bin Yu Interpretable machine learning deﬁnitions methods applications arXiv preprint arXiv190104592 2019 43 Marco Tulio Ribeiro Sameer Singh Carlos Guestrin Why I trust Explaining predictions classiﬁer Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining 2016 pp 11351144 44 CS Karthik Bundit Laekhanukit Pasin Manurangsi On parameterized complexity approximating dominating set J ACM ISSN 00045411 66 5 August 2019 httpsdoi org 10 1145 3325116 ence Machine Learning PMLR 2017 pp 31453153 45 Avanti Shrikumar Peyton Greenside Anshul Kundaje Learning important features propagating activation differences International Confer 46 Mukund Sundararajan Ankur Taly Qiqi Yan Axiomatic attribution deep networks International Conference Machine Learning PMLR 2017 pp 33193328 47 Berk Ustun Cynthia Rudin Supersparse linear integer models optimized medical scoring systems Mach Learn 102 3 2016 349391 48 Fulton Wang Cynthia Rudin Falling rule lists Artiﬁcial Intelligence Statistics PMLR 2015 pp 10131022 20