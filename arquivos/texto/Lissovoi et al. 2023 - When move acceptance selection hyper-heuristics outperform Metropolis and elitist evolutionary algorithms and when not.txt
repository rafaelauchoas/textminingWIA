Artiﬁcial Intelligence 314 2023 103804 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint When acceptance selection hyperheuristics outperform Metropolis elitist evolutionary algorithms Andrei Lissovoi Pietro S Oliveto John Alasdair Warwicker b Department Computer Science University Sheﬃeld UK b Institute Operations Research Karlsruhe Institute Technology Germany r t c l e n f o b s t r c t Article history Received 30 April 2021 Received revised form 6 September 2022 Accepted 3 October 2022 Available online 4 October 2022 Keywords Hyperheuristics Runtime analysis Nonelitism Metropolis Move acceptance operators Theory Selection hyperheuristics HHs automated algorithm selection methodologies choose different heuristics optimisation process Recently selection HHs choosing collection elitist randomised local search heuristics different neighbourhood sizes shown optimise standard unimodal benchmark functions evolutionary computation optimal expected runtime achievable available lowlevel heuristics In paper extend understanding performance HHs domain multimodal optimisation considering Move Acceptance HH MAHH literature switch elitist nonelitist heuristics run In essence MAHH nonelitist search heuristic differs search heuristics source nonelitism We ﬁrst identify range parameters allow MAHH hillclimb eﬃciently prove optimise standard hillclimbing benchmark function OneMax best expected asymptotic time achievable unbiased mutationbased randomised search heuristics Afterwards use standard multimodal benchmark functions highlight function characteristics MAHH outperforms elitist evolutionary algorithms wellknown Metropolis nonelitist algorithm quickly escaping local optima ones Since MAHH essentially nonelitist random local search heuristic paper independent researchers ﬁelds artiﬁcial intelligence randomised search heuristics 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 1 Introduction Selection hyperheuristics HHs automated algorithm selection methodologies designed choose set lowlevel heuristics apply steps optimisation process 15 Rather deciding advance heuristic related parameter settings apply problem manual trial error preliminary experiments automated algorithm conﬁgurators 3264 aim HHs automate process runtime Originally shown effectively optimise scheduling problems scheduling sales summit university timetabling 1516 successfully applied variety hard combinatorial optimisation problems 76 27405864 surveys results An extended abstract manuscript appeared 2019 Association Advancement Artiﬁcial Intelligence Conference AAAI 2019 46 Email address polivetosheﬃeldacuk PS Oliveto httpsdoiorg101016jartint2022103804 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Selection HHs consist separate components 1 heuristic selection method referred learning mechanism decide heuristic applied step optimisation process 2 acceptance operator decide newly produced search points accepted The majority heuristic selection methods literature apply machine learning techniques generate scores heuristic based past performance A commonly method purpose reinforcement learning 1550 5 Despite numerous successful applications limited rigorous theoretical understanding behaviour performance available 431 Recently proved reinforcement learning HH simple selection HH called Random Descent 1516 choosing elitist randomised local search RLS heuristics different neighbourhood sizes respectively optimise standard unimodal benchmark functions OneMax LeadingOnes best possible runtimes achievable lower order terms available lowlevel heuristics 2248 Since Random Descent HH store information past performance lowlevel heuristics necessary run selected lowlevel heuristics suﬃcient time called learning period allow HH accurately determine useful chosen heuristic current stage optimisation process However optimal duration learning period change optimisation process Doerr et al recently introduced selfadjusting mechanism rigorously proved allows HH track optimal learning period optimisation process LeadingOnes 25 The optimal asymptotic performance selfadjusting Random Descent HH recently shown OneMax Ridge unimodal benchmark functions 47 For survey available theoretical results performance HHs 51 In paper aim extend understanding behaviour performance HHs multimodal optimisation problems In order evaluate capability escaping local optima consider elitist nonelitist selection operators called acceptance operators HH literature Move acceptance operators referred selection operators classic evolutionary computation literature classiﬁed deterministic decision independent stage optimisation process nondeterministic different decisions solutions different stages 56 Cowling et al introduced variants deterministic acceptance operators elitist OnlyImproving OI op erator accepts moves improve current solution nonelitist AllMoves AM operator accepts new solution independent quality 1516 Another acceptance operator considered literature ImprovingandEqual IE operator addition accepting improving solutions accepts solutions equal quality 2455 In mentioned works acceptance operator remains ﬁxed run HH switching different mutation operators However desirable HHs allowed decide change acceptance operator selection pressure different stages optimisation process For instance use elitist acceptance exploitation phases search hillclimbing nonelitism exploration instance escape local optima Indeed Qian et al analysed HH switches acceptance operators context multiobjective optimisation 60 They considered HH selects elitist IE strict elitist OI acceptance operators presented function necessary mix acceptance operators Lehre Özcan presented available analysis HH chooses elitist nonelitist low level heuristics 43 In particular HH uses described OI strict elitist AM nonelitist acceptance operators The considered Move Acceptance HH MAHH uses 1bit ﬂips local mutations mutation operator selects AM acceptance operator probability p OI acceptance operator probability 1 p Essentially algorithm randomised local search RLS algorithm switches strict elitism accepting improvements extreme nonelitism accepting new solution For standard RoyalRoadk benchmark function evolutionary computation consists blocks k 2 bits set correctly observe ﬁtness improvement proved necessary mix acceptance operators MAHH eﬃcient accepting improvements p 0 runtime inﬁnite MAHH able cross plateaus equal ﬁtness accepting p 1 algorithm simply performs random search By choosing value parameter p appropriately provide upper bound expected runtime HH O n3 k2k3 versus O n logn 2kk expected time required evolutionary algorithms standard bit mutation bit ﬂipped probability 1n standard selection operator accepts new solutions ﬁtness good parent 26 In particular IE acceptance operator run leads better performance Hence advantages switching selection operators time evident In paper present systematic analysis HH considered Lehre Özcan 43 multimodal optimisation problems1 considerable advantages changing acceptance operator run highlighted In particular increase understanding behaviour performance MAHH providing 1 In literature term multimodal optimisation refer optimisation tasks involve identifying set local global optima global optimum 59 In paper use term simply refer fact optimisation functions unimodal local optima algorithms trapped Concerning term multimodal function use deﬁnition cited book Naturally objective functions possess optimizer These generally considered easier optimize called unimodal All called multimodal 2 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 features multimodal landscapes eﬃcient escaping local optima features Furthermore provide comparative analysis wellstudied Metropolis nonelitist algorithm 37 steadystate evolu tionary algorithms EAs highlights MAHH superior performance preferred We ﬁrst perform analysis standard unimodal OneMax benchmark function evolutionary computation identify range parameter values p allow MAHH hillclimb locate local optima eﬃciently In particular prove p O log log n1εn constant ε 0 MAHH asymptotically eﬃcient best unary unbiased mutationbased randomised search heuristic 44 rely elitism On hand larger p ω n log nn MAHH optimise function unique optimum polynomial time high probability Afterwards highlight power MAHH analysing performance standard benchmark function classes chosen allow isolate important properties multimodal optimisation landscapes Firstly consider Cliffd class functions consisting local optima escaped allow identiﬁcation new slope increasing ﬁtness For class functions rigorously prove MAHH eﬃciently escapes local optima hardest instances function class elitist search heuristics 12 achieves best possible expected runtime O n log n unary mutation unbiased randomised search heuristics problem polynomial number optima 3 Thus prove considerably outperforms established elitist nonelitist evolutionary algorithms including Metropolis algorithm We consider standard Jumpm multimodal instance class functions provide example ﬁtness landscape identify new slope increasing gradient optimum hard possible While MAHH runtime exponential size basin attraction eﬃcient instances moderate jump size constant considerably outperforms Metropolis requires exponential time problem size overwhelming probability independent basins size The superior performance MAHH Metropolis considered function classes mainly large ﬁtness value difference local optima neighbouring solutions To end design smoother function class called CliffJumpdrs sizes basins attraction local global optima steepness negative slope traversed escape local optima tuned Our aim identify basinofattraction characteristics MAHH outperforms Metropolis viceversa CliffJumpdrs combines aspects Cliffd Jumpm From local optima Jumpm slope returning local optima overcome identify Cliffd slope leading global optimum If negative slope leading away local optima decreases ﬁtness gently Metropolis conﬁgured outperform variant HH Conversely slope steep HH conﬁgured faster variant Metropolis Hence analysis shows MAHH preferable rugged landscapes steep changes ﬁtness Metropolis eﬃcient smoother behaved landscapes neighbouring points similar ﬁtness values However MAHH eﬃcient length negative slope large independent steepness slope Metropolis ineﬃcient slope small descending gradient gentle As corollary function class allows point kinds basin attraction overcome eﬃciently nonelitist hyperheuristic compared elitist evolutionary algorithms standard bit mutation In particular small basins attraction located superconstant distance optimum nonelitism difference small polynomial superpolynomial runtimes including exponential ones We use insight complete picture presenting ﬁtness landscape characteristics Metropolis eﬃcient MAHH The benchmark function consider gentle slope smooth ﬁtnessdecreasing gradient surrounded points considerably lower ﬁtness While Metropolis eﬃcient function follow gentle path rejecting points lower ﬁtness function deceptive MAHH likely accept worsening solutions path resulting exponential optimisation time overwhelming probability Since MAHH essentially nonelitist random local search heuristic differs search heuristics source non elitism paper general outside parameter control 21 hyperheuristics community particular researchers randomised search heuristics artiﬁcial intelligence general 361018 analyses emphasising effectiveness nonelitism The rest paper structured follows In section formally introduce moveacceptance hyper heuristics MAHHOI MAHHIE problem classes mathematical analysis tools rest paper In Section 3 analyse MAHHOI unimodal benchmark function OneMax In Sections 4 5 analyse MAHHOI multimodal Cliffd Jumpm functions comparing expected runtime known Metropolis algorithm Section 6 analyses ability MAHHOI Metropolis escape basins attraction tuneable gradients size In Section 7 analyse example function Metropolis provably outperforms MAHHOI We ﬁnish paper conclusions MAHH performs doesnt offering promising avenues future research Compared conference version 46 manuscript considerably extended We include proofs omitted space constraints theorems strengthened Furthermore sections 6 7 completely new additions 3 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Algorithm 1 Move Acceptance HyperHeuristic MAHHOI 43 1 Choose x 0 1n uniformly random 2 termination criteria satisﬁed 3 4 5 6 7 cid5 FlipRandomBitx x Choose r 0 1 uniformly random cid5 r p x x cid5 f x cid4 f f x cid5 cid4 f 0 x x AM OI Algorithm 2 Metropolis algorithm 49 1 Choose x 0 1n uniformly random set αn 1 2 termination criteria satisﬁed 3 4 5 6 7 cid5 FlipRandomBitx x cid4 f f x cid5 cid4 f 0 x x choose r 0 1 uniformly random cid5 r αncid4 f x x cid5 f x Algorithm 3 1 1 Evolutionary algorithm 1 Choose x 0 1n uniformly random 2 termination criteria satisﬁed 3 4 5 cid5 ﬂip bit x independently probability 1n x cid4 f f x cid5 cid4 f 0 x x cid5 f x 2 Preliminaries In section formally introduce hyperheuristic algorithms problem classes analyse paper brieﬂy state widelyknown mathematical tools runtime analysis randomised search heuristics use paper 21 Algorithms We analyse Move Acceptance hyperheuristic previously considered Lehre Özcan 43 In iteration bit chosen uniformly random ﬂip probability p AllMoves AM acceptance operator probability 1 p OnlyImproving OI acceptance operator Algorithm 1 shows pseudocode We consider MAHHIE variant Algorithm 1 ImprovingandEqual IE acceptance operator accepts moves increase maintain current ﬁtness instead OI operator Thus MAHHIE AM acceptance operator probability p IE acceptance operator probability 1 p However problem classes consider changing number 1bits bitstring 1 FlipRandomBit mutation operator Algorithm 1 change ﬁtness value plateaus constant ﬁtness Therefore point given conditions statements paper MAHHOI hyper heuristic hold MAHHIE hyperheuristic Thus rest paper focus analysis MAHHOI hyperheuristic Throughout paper compare performance MAHHOI hyperheuristic wellknown Metropo lis algorithm 49 allows global exploration nonelitism standard elitist evolutionary algorithm 1 1 EA uses global mutations escape local optima 39 While local mutations ﬁnd improving solutions accepted Metropolis accepts worsening moves nonzero probability Suppose parent solution x ﬁtness f x offspring solution y ﬁtness f y f y f x cid4 f 0 Metropolis accepts worsening probability αncid4 f function αn 1 The pseudocode Metropolis shown Algorithm 2 The 1 1 EA accepts solutions nondecreasing ﬁtness elitism ﬂips bit ﬁxed probability 1n reach area search space single step probability reaching new point decreases increased Hamming distance current point The pseudocode 1 1 EA shown Algorithm 3 22 Benchmark function classes We start considering standard benchmark function classes deﬁned bit strings length n commonly theory randomised search heuristics evaluate performance These problem classes artiﬁcially constructed purpose reﬂecting isolating common diﬃculty proﬁles known appear classical combinatorial optimisation problems expected appear realworld optimisation The OneMax problem class class unimodal functions provide consistent ﬁtness gradient leading global optimum The class displays typical function optimisation feature improving solutions harder identify 4 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Cliff5 b Jump5 20 15 10 5 e u l v s s e n t F Global Optimum d 5 30 20 10 0 0 5 10 x1 c CliffJump831 15 20 0 0 5 20 10 Global Optimum d 8 d r 5 20 10 Global Optimum m 5 10 x1 d CliffJump832 15 20 Global Optimum d 8 0 0 5 10 x1 15 20 0 0 5 d r 5 15 20 10 x1 Fig 1 Examples Cliffd Jumpm CliffJumpdrs benchmark functions n 20 optimum approached It generally evaluate validate hillclimbing performance randomised search heuristics The function deﬁned follows OneMaxx ncid2 i1 xi In deﬁnition global optimum placed 1n bitstring convenience analysis ﬁtness increases number 1bits The results derive hold instance function class optimum bit string ﬁtness function returns number matching bits This class functions known easiest functions unique global optimum unary unbiased black box algorithms mutationbased EAs 44 We consider Cliffd Jumpm multimodal problem classes optimisation algorithm need escape local optima order reach global optimum The classes differ ﬁtness function guides search away global optimum algorithm escaped local optimum The Cliffd class functions originally proposed example nonelitist evolutionary algorithms outperform elitist ones 36 Functions class generally lead optimisation process local optimum ﬁtness decreasing mutation taken ﬁnd ﬁtnessimproving slope leading global optimum An example instance shown Fig 1a We deﬁne Cliffd class functions 1 d n2 follows cid3 Cliffdx OneMaxx OneMaxx d 12 x1 n d As OneMax global optimum placed 1n bit string simplify notation The parameter d controls ﬁtness decrease local optimum lowest point slope leading global optimum length second slope This class problems captures realworld problems local optima narrow basins attraction able escape local optimum search heuristic good chances identifying new basin attraction The Jumpm class functions similar Cliffd ﬁtness gradients leading local optima ﬁnd global optimum search algorithms ﬁtnessdecreasing mutation escape local optimum disregard ﬁtness gradient iterations An example instance shown Fig 1b We deﬁne Jumpm class functions 1 m n2 follows 5 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Jump mx n m m OneMaxx n OneMaxx x1 n x1 n m As OneMax global optimum placed 1n bit string simplify notation Compared Cliffd class functions features local optimum broader basin attraction making diﬃcult search heuristics escape basin locate global optimum Additionally deﬁne function class combines features Cliffd Jumpm allowing local global optima basins attraction region search space optima Thus optimise function CliffJumpdrs search algorithm employing local mutations need multiple steps away local optimum slope leading global optimum Example instances shown Figs 1c 1d s 1 s 2 We deﬁne class functions 1 r d n2 s 0 follows CliffJump drsx n OneMaxx OneMaxx r rs n d s OneMaxx d n x1 n x1 n d x1 n d r The global optimum placed 1n string d r parameters control lengths positive negative slopes following local optimum reaching local optimum r mutations decreasing ﬁtness s taken local search heuristic ﬁtnessincreasing slope length d r accessible We consider following function rename GentleNegativeSlope originally introduced Jansen Wegener example ﬁtness landscape Metropolis considerably outperforms 1 1 EA 38 GentleNegativeSlopex 2n n 1 2n n n 2n OneMaxx x 0n x 1i0ni 1 n GentleNegativeSlope GNS deceptive function lead algorithms away global optimum 0n There steep OneMax style path 1n bitstring followed ridge gently decreasing ﬁtness global optimum 23 Mathematical analysis tools We present known drift analysis theorems bound expected runtime randomised search heuristics Drift analysis general mathematical technique allows statements long term performance runtime randomised search heuristic analysing expected progress algorithm makes step All required apply drift analysis potential function called distance function estimate distance current solution target area search space global optimum bound expected onestep change referred drift algorithm makes respect potential function conditional current distance target Hence given time step t distance target Xt analysing drift requires providing bounds quantity E Xt Xt1 Xt quantify expected onestep progress algorithm For gentle introduction drift analysis refer 42 extensive overview refer reader Lenglers recent book chapter 45 We apply following theorems paper analyses Hamming distance measure distance optimum set target solutions write cid4i refer drift conditioned parent solution containing 1bits The following Additive Drift Theorem provides upper lower bounds expected runtime given respectively lower upper bounds drift hold process Theorem 1 Additive Drift Theorem 34 Let Xtt0 sequence random variables ﬁnite set states S R 0 let T random variable denotes ﬁrst point time Xt 0 If exist δu δl 0 t 0 δu EXt Xt1 Xt δl expected optimisation time ET satisﬁes ET X0 X0 δl ET EX0 X0 δu EX0 δu δl 6 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 When expected drift negative nontrivial region search space exponential lower bounds runtime derived Negative Drift Theorem Theorem 2 Negative Drift Theorem 5253 Let Xt t 0 random variables describing Markov process state space let δti Xt Xt1 Xt S t 0 Suppose exist interval b state space constants δ cid7 0 t 0 following conditions hold 1 Eδti cid7 b 2 Prδt j 11 δ j j 0 Then constant c 0 T mint 0 Xt X0 b holds PrT 2c ba 2 cid8ba We use Negative Drift Theorem Scaling allows negative drift ε subconstant magnitude Theorem 3 Negative Drift Theorem Scaling 54 Let Xt t 0 realvalued random variables describing stochastic process state space Suppose exist interval b R possibly depending cid9 b drift bound ε εcid9 0 scaling factor r rcid9 t 0 following conditions hold 1 E Xt1 Xt X0 Xt Xt b ε 2 Pr Xt1 Xt jr X0 Xt Xt e 3 1 r2 εcid9132 logrε j j N0 Then ﬁrst hitting time T mint 0 Xt X0 b holds PrT eεcid9132r2 O e εcid9132r2 3 Unimodal optimisation hillclimbing We begin study MAHHOI analysing performance unimodal functions Since accepting worsening moves required setting hyperheuristic outperform elitist algorithms Nevertheless Theorem 7 shows MAHHOI eﬃcient optimising OneMax p 0 We ﬁrst introduce Theorem 4 Theorem 5 parameter p large Theorem 6 subsequently states expected runtime MAHHOI OneMax lower bound expected runtime functions unique global optimum For OneMax larger value p greater probability accepting moves away global optimum optimisation process greater expected runtime hyperheuristic For constant values p standard Negative Drift Theorem Theorem 2 applied directly exponential number iterations required exponentially high probability Theorem 4 The runtime MAHHOI OneMax p cid101 2cid8n probability 1 2 cid8n Proof Let denote number 1bits current bitstring consider expected change Hamming distance drift optimum OneMax equivalent expected change solution ﬁtness iteration MAHHOI cid4i n pi n 1 p n n n improving mutations require ﬂipping n remaining 0bits accepted acceptance operators worsening mutations ﬂipping 1bits accepted AM operator chosen probability p Let p 0 constant We 1C 1p n cid4i C constant C p C 0 Hence region size n 1 1C n cid8n drift negative C Since MAHHOI local search algorithm 1p region negative drift escaped large jump By Negative Drift Theorem Theorem 2 runtime case 2cid8n overwhelming probability 1 2 cid8n cid2 For smaller values p large apply negative drift theorem scaling Theorem 3 Theorem 5 The runtime MAHHOI OneMax p ω n log nn nω1 probability 1 n ω1 Proof To prove result apply Simpliﬁed Drift Theorem Scaling Theorem 3 handles regions subconstant negative drift 7 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Recall Equation 1 proof Theorem 4 drift iteration MAHHOI OneMax current solution contains 1bits cid4i n p n n pi n n We consider drift region n c n n length cid9 c n c 0 constant Let p ω1 As cid4i decreases negative drift weakest n cid9 c n log nn c cid4n cid9 n cid9 n cid9p n cid9 cid8 n np n c n n cid10 p c cid11 np cid9p cid9 n cid10 p op O n cid11 12 cid8p ε ω n log n n satisﬁes drift condition Simpliﬁed Drift Theorem Scaling Theorem 3 The second condition forbidding large jumps satisﬁed choosing r 2 verifying j N0 proba j 1 bility magnitude ﬁtness change jr e j 1 probability distance optimum changes jr 2 0 mutation operator MAHHOI ﬂips bit iteration j trivially true j 0 e Finally need verify following condition 1 r2 εcid9132 logrε To end note cid8 cid9 n log n n n ωlog n c εcid9 ω 132 logrε 132 log cid8 n 2 cid9 ωlog n 132 log2 n 66 logn O 1 Thus εcid9132 logrε ω1 greater r2 4 suﬃciently large n Having veriﬁed conditions Simpliﬁed Drift Theorem Scaling satisﬁed apply theorem concluding probability optimum eεcid9132r2 nω1 iterations O eεcid9132r2 ω1 cid2 n We remark ω n log nn lower bound probability applying AM selection operator lowest bound Negative Drift Theorem scaling yields superpolynomially high probability requiring superpolynomial number iterations optimise OneMax We note εcid9 ωlog n required Theorem 3 yield superpolynomial runtime bound reducing p require reductions drift bound ε length negative drift region cid9 Closing gap upper bound probability positive result presented Theorem 7 require proof techniques applied The following theorem proves MAHHOI setting solve function unique global optimum polynomial time Thus provide pretty general lower bound value parameter p eﬃcient optimisation We follow proof idea 23 Theorem 9 showed expected runtime 1 1 EA OneMax lower bound expected runtime function unique global optimum 65 Theorem 10 proved result arbitrary mutationbased EAs mutation probability 1n Theorem 6 The expected runtime MAHHOI function unique global optimum large expected runtime MAHHOI OneMax setting p Proof Let f denote function unique global optimum loss generality set unique global optimum f bitstring 1n We prove expected runtime MAHHOI f large runtime MAHHOI OneMax Formally aim E f MAHHOI E OneMax MAHHOI 2 Let E f MAHHOI denote minimum expected time required MAHHOI ﬁnd global optimum f given MAHHOI sampled solutions x x1 far MAHHOI samples Hamming neighbours sample solution x1 j j reaching global optimum By deﬁnition E f MAHHOI n E f MAHHOI n 1 E f MAHHOI 0 We deﬁne cid12E sampled By deﬁnition cid12E OneMax MAHHOI deﬁned E OneMax OneMax MAHHOI MAHHOI E OneMax MAHHOI added constraint solution x1 8 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 We verify claim Equation 2 proving E MAHHOI n cid12E MAHHOI cid12E MAHHOI n 0 assume E tion Firstly note E OneMax f f OneMax MAHHOI holds We use proof induc MAHHOI j cid12E OneMax f MAHHOI j j Since MAHHOI uses local search neighbourhood size number 1bits solution decrease worsening accepted AM operator stay worsening rejected MAHHOI 1 OI operator increase If number 1bits increases expected optimisation time E Let Y denote number 1bits solution mutation accepted rejected induction hypothesis f f MAHHOI 1 PrY 1 E 1 PrY 1 cid12E f MAHHOI 1 PrY E MAHHOI 1 PrY E OneMax f MAHHOI f MAHHOI E Hence E f MAHHOI 1 PrY 1 cid12E OneMax MAHHOI 1 1 PrY Furthermore MAHHOI OneMax cid12E MAHHOI 1 PrY 1 cid12E OneMax OneMax MAHHOI 1 PrY cid12E OneMax MAHHOI OneMax cid12E MAHHOI 1 PrY 1 cid12E OneMax MAHHOI 1 1 PrY Therefore proven E initialised uniformly random regardless ﬁtness function distribution number 1bits initial solution Hence theorem holds cid2 OneMax MAHHOI Since MAHHOI MAHHOI E OneMax MAHHOI cid12E f Theorem 6 combined statements Theorem 4 Theorem 5 shows expected runtime MAHHOI function unique global optimum ncid8log n p n log2 nn 2cid8n p cid101 We provide upper bound p hyperheuristic eﬃcient hillclimbing OneMax Theorem 7 The expected runtime MAHHOI OneMax p O log log n1cid7 n constant cid7 0 O n log n With p O log n1cid7 n constant cid7 0 expected runtime MAHHOI OneMax on2 log n Proof Let Yt denote current solution iteration t hYt Hamming distance OneMax optimum g N0 1 inverse function g To prove result apply R0 injective function g0 0 g additive drift theorem process Xt ghYt bounding expected number steps Xt 0 ﬁrst time MAHHOI constructed optimal solution ﬁrst time Consider drift cid4i E Xt Xt1 g 1 Xt expected decrease Xt1 value current solution Hamming distance away optimum If MAHHOI mutation produces improvement accepted regardless selection operator decrease hYt 1 If MAHHOI mutation produces worsening accepted AM operator applied probability p case hYt increase 1 hYt Xt remain unchanged Combining yields cid4i n cid13 Let gx cid4i 1 n 1 n cid8 gi gi 1 n n p gi 1 gi x i1 1 cid8 si s N R0 monotonically decreasing function cid8 cid9cid9 cid9 cid8 1 si ip 1 1 si 1 np cid9 1 1 si 1 1 si np 1 np si 1 cid10 cid11 3 4 Suppose s1 0 cid4i 1 n When p 2 cn 0 c 2 cid4i c2n 0 1 additive drift theorem yields upper bound expected runtime MAHHOI gnc2n O nc log n 1 np 1i For p 2n need modify s ensure drift cid4i remains positive search space Observe equation 3 bound cid4i increased increasing si long expression 9 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 parentheses produces positive constant drift cid81n maintained First 2np adjustment si necessary achieve cid42np n 1 np 2np 1 1 2 cid4i increasing function cid4i 12n cid112npcid12 Let i0 minimum Hamming distance optimum 1 np i01 2 cid4i 12n i0 Per i0 cid112npcid12 If i0 1 direct application additive drift theorem si 0 described previously yields O n log n upper bound runtime Thus remains consider case 2 i0 cid112npcid12 Let j i0 j j 1 i0 1 Using si0 0 adjust si1 ensure cid4i1 suﬃciently large 1 We induction si j np j3i j suﬃcient ensure cid4i j 12n To begin consider drift i1 cid4i1 n 1 i1 si1 np i1 1 1 1 1 1 i01 j 1 cid4i1 12n establishing induction base case j 22 ji01 i0 j1 i01 i0 1 np i0 1 1 i1 si1 1 2 np 3 3 i0 2 j i0 1 Hence setting si1 np3i1 ensures i1 si1 np 3 For j 2 si j si j 1 si j1 positive Given induction hypothesis holds si j1 cid4i j n 1 j si j np j 1 np si j 1 1 np i0 1 np 3 np si j1 j si j 1 2 np 3 np j 3i j1 j si j 1 2 np j 3 j si j np 3 np2 6 np 2 j1 2 j 2 Hence setting si j np j3i j ensures cid4i j 12n Thus si j np j3i j npi0i j 3i j 1 j i0 cid112npcid12 si 0 i0 suﬃcient ensure cid4i 12n 1 Additionally holds s1 npi013 np2np3 p O log log n1cid7 n constant cid7 0 np2np olog n s1 olog n For p O log log n1cid7 n np 2 i1 s12i1 2s1 olog n gn O log n Apply ing additive drift theorem X0 O log n δ cid81n yields Xt 0 expectation X0δ O n log n steps Hence MAHHOI p O log log n1cid7 n constant cid7 0 optimises OneMax O n log n iterations expectation For p O log n1cid7 n np2np gn Applying additive drift theorem yields MAHHOI i1 si p O log n1cid7 n constant cid7 0 optimises OneMax on2 log n iterations expectation cid2 cid13 n cid13 n For simplicity remainder paper consider MAHHOI p 1 1εn constant cid7 0 gives desired O n log n runtime This value p allows MAHHOI maintain positive drift search space OneMax removes need complicating log n1εn terms subsequent calculations We parameter value hyperheuristics hillclimbing eﬃciently escape diﬃcult local optima effectively Regarding performance Metropolis OneMax present following Theorem Jansen Wegener 38 provides polynomial upper bound expected runtime Metropolis OneMax given function αn chosen suﬃciently large respect problem size Theorem 8 38 Theorems 47 The expected runtime Metropolis OneMax polynomially bounded αn cid8n log n Furthermore αn εn constant ε 0 expected runtime Metropolis OneMax O n log n 4 Multimodal optimisation easy basins attraction We analyse performance search heuristics multimodal Cliffd 1 d n2 class benchmark functions Recall Cliffd features local optima heuristics escape climbing ﬁtness gradient global optimum We refer local optimum n d cliff OneMax style hillclimbs ﬁrst slope second slope respectively Fig 1a use d denote length cliff To ﬁnd global optimum Cliffd function necessary escape local optimum dropping cliff accepting worse candidate solution climbing second slope making prohibitive jump global optimum cliff possible standard bit mutation requiring expected exponential time distance cliff optimum local mutations We consider performance MAHHOI Cliffd Clearly p 1 MAHHOI reduces random walk ﬁtness landscape Similarly p 0 probability 12 bitstring initialised n2 1bits 10 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 hillclimb cliff There improving step position global optimum reached By law total expectation expected runtime inﬁnite We MAHHOI p 1 1εn shown hillclimb eﬃciently Theorem 7 allow worsening moves suﬃciently high probability able cliff reach global optimum expected polynomial time Theorem 10 bounds expected runtime MAHHOI Cliffd We begin introducing helper Lemma proved Droste et al trajectory based algorithms change number 1bits bitstring 1 29 The lemma subsequently analyse performance 1 1 EA noisy OneMax small noise strength 28 Unlike noisy optimisation noise represents uncertainty respect true ﬁtness solutions hyperheuristics AM operator intended helpful optimisation process allowing algorithm escape local optima Lemma 9 29 Lemma 3 Let ET transition probabilities reach state respectively 1 1 1bits Then p expected time reach state 1 1bits given state 1bits p ET 1 p p p ET i1 Within context nonelitist local search algorithms MAHHOI transition probability p refers probability making local mutation increases respectively decreases number 1bits offspring solution accepting solution p Theorem 10 The expected runtime MAHHOI Cliffd p 1 cid10 n log n n3 1εn constant ε 0 O d2 cid11 Proof Let denote number 1bits bitstring time t 0 We wish bound expected runtime separately bounding stages optimisation process starting earlier stages ended ending solution certain number 1bits constructed ﬁrst time optimisation process HH backwards remain stage ﬁrst stage ends n d reached second n d 1 n d 2 fourth n optimum constructed We use T 1 T 4 denote number iterations algorithm spends stages note deﬁnition stages number iterations T optimum reached T T 1 T 2 T 3 T 4 linearity expectation ET ET 1 ET 2 ET 3 ET 4 5 In ﬁrst stage n d Cliffd resembles OneMax function use upper bound expected runtime MAHHOI OneMax p 1 1εn Theorem 7 Hence ET 1 O n log n The second stage begins n d ﬁrst time ends n d 1 ﬁrst time When n d improving moves If OI operator selected change candidate solution Hence 1εn A mutation step come use AM operator selected probability increase number 1bits bitstring probability dn decrease number 1bits probability n dn We use Lemma 9 bound ET 2 expected time jump cliff 1 ET 2 ET nd n21 ε d n d d ET nd1 6 We bound ET nd1 At n d 1 drift follows cid4n d 1 d 1 n 1 1 εn n d 1 n nd εd 1 d 1 n21 ε ﬂipping d 1 remaining 0bits increases ﬁtness 1 accepted operator ﬂipping n d 1 1bits decreases ﬁtness 1 accepted AllMoves operator chosen occurs probability p 11 εn Clearly 0 n d 1 drift bounded drift n d 1 distance required point n d 1 1 By Additive Drift Theorem Theorem 1 11 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 ET nd1 1cid4n d 1 n21 ε nd εd 1 d 1 O cid10 cid11 n d We return bounding ET 2 Equation 6 n d d n d d ET 2 n21 ε d n21 ε d nd1 cid11 ET cid10 n2 d O O cid9 cid8 n d 7 The stage begins n d 1 ends n d 2 When n d 1 moves improving moves moves accepted regardless choice OI AM operator With probability d 1n accepted decreases number 1bits n d hyperheuristic returns local optimum With probability n d 1n accepted instead increases number 1bits n d 2 We use Lemma 9 bound ET 3 expected time step second slope Noting ET n d 1 d 1 cid9 n2 d nd1 n d 1 n d 1 d 1 nd O n2d Equation 7 ET 3 ET ET cid8 nd cid9 n d 1 n3 d2 O O cid8 If d 2 Cliffd function optimised point However d 3 necessary climb second slope If d 3 point ET 4 ET nd2 applying Lemma 9 bound For d 3 know n d 2 n d 3 points second slope applying Lemma 9 bound ET nd2 n d 2 n d 2 n d 2 1 1 εn 1 1 εn cid9 cid8 n3 d3 O ET nd1 cid9 cid8 O n3 d2 n d 2 d 2 n d 2 d 2 ET nd3 n d 3 n d 3 n d 3 1 1 εn 1 1 εn cid9 cid8 n3 d4 O ET nd2 cid9 cid8 O n3 d3 n d 3 d 3 n d 3 d 3 For d 4 trend continue MAHHOI progresses closer global optimum particular k d ET n3 O dk d1 ndk The terms summation asymptotically dominated O n3d3 term k2 ET ndk n dk cid13 Hence ET 4 cid11 cid10 ET nd2 d sublinear giving ET 4 d d O n3d3 O n3d2 However d linear problem size ﬁrst terms dominate ET 4 d cid10n O 1 d1cid2 i2 n d O 1 d1cid2 i0 n d O 1 n dcid2 j1 1 j O n log n Combining bounds sublinear linear d conclude ET 4 O cid9 cid8 n log n n3 d2 12 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 We return overall runtime bound Equation 5 complete proof ET ET 1 ET 2 ET 3 ET 4 cid8 n log n n2 d O n3 d2 n log n n3 d2 cid9 O cid9 cid8 n log n n3 d2 cid2 cid15 cid14 n log n n3d2 Theorem 10 gives expected runtime bound MAHHOI Cliffd O This bound smallest d large suggesting algorithm fastest cliff hardest elitist algorithms linear cliff length d cid10n gives expected runtime O n log n This runtime asymptotically matches best case expected performance artiﬁcial immune systems escape local optimum ageing operator O n log n expectation 8 9111214 best runtime known hard Cliffd functions We suspect MAHHOI faster practice having smaller leading constants expected runtime leave proof future work Mutation based EAs expected runtime cid10nd d n2 57 d ω1 superpolynomial expected runtime length gap Steadystate Genetic Algorithms use crossover recently proven faster linear factor 17 require exponential expected runtimes large gaps The worstcase scenario MAHHOI constant gap length giving runtime O n3 This means 1 1 EA outperform MAHHOI d 3 slower 3 d n2 performance gap increases exponentially distance cliff optimum Concerning nonelitist search heuristics apart artiﬁcial immune systems upper bound O nη η 39767 expected runtime 1 λ EA recently proved 35 carefully chosen parameter λ n λ O log n improving considerably previously best known bound n25 36 The best case satisﬁes log e e1 expected runtime nonelitist strongselection weakmutation SSWM evolutionary regime rejects ﬁtness improving mutations nonzero probability ndecid8d 57 Hence MAHHOI faster SSWM 3 d n2 We compare performance MAHHOI Cliffd nonelitist Metropolis algorithm Algorithm 2 The orem 11 shows MAHHOI considerably outperform Metropolis Cliffd d 3 Theorem 11 The expected runtime Metropolis Cliffd min cid16 nd1 2d1 cid10 cn log n cid11 d32 cid17 nω1 constant c 0 Proof Let denote number 1bits bitstring time t 0 The Cliffd function OneMax slopes 0 n d n d 1 n 1 In order reach global optimum necessary Metropolis optimise slopes The expected runtime Metropolis OneMax polynomially bounded problem size αn cid8n log n Theorem 8 Hence αn cid8n log n necessary order optimise slopes polynomial time However jump cliff diﬃcult Metropolis The randomly initialised candidate solution n2 bits probability 12 Metropolis accepts jump n d n d 1 probability αnnd32nd αn32d Hence expected nd αnd32 From state necessary improving time event occur ET ﬁnd global optimum Given state n d 1 1bits moves accepted use Lemma 9 p p nd1 d1 n nd1 n nd1 n d 1 d 1 n d 1 d 1 αnd32 ET nd αnd32 cid9 ET nd1 n d 1 n d 1 cid8 n d 1 d 1 Given want minimise overall runtime αn cid8n log n hillclimb polynomial time However jump cliff ET nd1 n d 1 d 1 cid9 d32 cid8 cn log n constant c 0 We note d ω1 term ET nd1 exponential problem size setting α cid8n log n optimal In case lower bound exponential term nω1 time taken hillclimbs Overall order optimise function jump cliff probability 12 step second slope Hence 13 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 ET min cid3 n d 1 d 1 1 2 cid8 cn log n cid9 d32 cid18 nω1 cid2 We proven Metropolis optimise hard Cliffd variants expected polynomial time MAHHOI extremely eﬃcient hard cliffs expected runtime O n3 worst case 5 Multimodal optimisation hard basins attraction We consider Jumpm function class 1 m n2 example multimodal function MAHHOI harder time escaping local optima Unlike Cliffd ﬁtness decreases second slope making harder traverse Typically optimise Jumpm function Fig 1b search heuristic ﬁrst reaches local optimum slope Then jump global optimum requires exponential expected time length jump unbiased mutation operators jump slope decreasing ﬁtness traversed global optimum Theorem 12 The runtime MAHHOI Jumpm p 1 constant ε 0 probability 1 2 cid8m 1εn cid8n log n 2cm constant c 0 Proof Let represent number 1bits bitstring time t 0 First consider m O log n The initialised candidate solution n2 bits probability 12 The expected time optimise function bounded expected time reach local optimum slope If m O log n algorithm hillclimb point n k ln n constant k 0 The expected time ﬂip n2 k ln n bits correctly cid8n log n proved reusing arguments proof 30 Lemma 10 Similarly time optimise function bounded time traverse slope decreasing ﬁtness ﬁnd global optimum traverse region n m 1 n 1 Consider negative drift number 1bits bitstring region in1 ε 1 n n21 ε cid4i n n n 1 1 εn n2n1 ε 1 n n21 ε 1 2 n1 ε 1 2n1 ε 1 2n1 ε 04 We note number 1bits change 1 iteration chance escaping area negative drift large jumps We apply Negative Drift Theorem Theorem 2 region cid5 0 probability n m 1 n 1 length m 2 Hence exists constant c cid5m2 2cm steps 1 2 c c cid8m global optimum area negative drift 2c cid5m 2m 0 different constant If m ωlog n term dominate cid8m2 1 2 By considering possibilities state constant c 0 expected time MAHHOI optimise Jumpm cid8n log n 2cm probability 1 2 cid8m cid2 The following theorem provides upper bound expected runtime considering expected time MAHHOI accept m consecutive worsening moves local global optimum Theorem 13 The expected runtime MAHHOI Jumpm p 1 1εn constant ε 0 cid8 n log n 1 εm1n2m O m2m cid9 Proof Let denote number 1bits bitstring time t 0 The expected time reach bitstring n m 1bits O n log n Theorem 7 From position order global optimum necessary ﬂip remaining 1εn Starting search point n m 1bits m 0bits accept result probability p 1 0bits probability reaching bit string n 1 1bits m 1 steps given cid8 mcid19 i2 cid9 n 1 1 εn m 1 εm1n2m1 14 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 m Let p nm 1εm1n2m1 probability bitstring n 1 1bits reached m 1 consecutive steps nm probability mutation decreases ﬁtness n m 1 nm1 O nm Additive Drift Theorem Theorem 1 use Lemma 9 bound local optimum n m p accepted By noting ET expected time reach bit string local optimum cid11 cid10 ET opt nm 1 εm1n2m1 1 εm1n2m1 m2 m 1 O m cid8 1 εm1n2m1 m cid9 O n m Hence application Lemma 9 expected time reach local optimum traverse slope decreasing ﬁtness ﬁnd global optimum cid8 ET O n log n 1 εm1n2m m2 m cid9 cid2 Using global mutations jump allows smaller upper bounds expected runtime The expected runtime 1 1 EA mutation rate 1n Jumpm cid10nm 30 bound O nm1 Steady State μ 1 Genetic Algorithms crossover slightly higher mutation rate recently proved Dang et al 17 outperforming upper bound MAHHOI Recent work shown increasing mutation rate exponential speedups achieved static adaptive Artiﬁcial Immune System 91213 hypermutations 1 1 EAs heavytailed mutation operators following power law distribution 3124 stagnation detection mechanism identify best mutation rate overcome local optima 616362 expected runtime exponential m A compact GA superpolynomial speedups algorithms superconstant jump lengths m 33 In particular logarithmic jump lengths algorithm optimises Jumpm expected polynomial time We point steady state GAs diversity mechanisms enhance power crossover optimise Jumpm expected time O mn log n 4m jumps m n8 19 cid10n log n 4m unrealistically small crossover probability O mn 41 Concerning algorithms use nonelitism escape local optima μ λ EA improve expected runtime elitist μ λ EA 20 Metropolis worse performance MAHHOI Jumpm The large ﬁtness difference points n m n m 1 makes Metropolis unlikely accept n m 1 point αn set high enable eﬃcient hillclimbing ﬁrst slope Theorem 14 The runtime Metropolis Jumpm 2cid8n probability 1 2 cid8n Proof Let represent number 1bits bitstring time t 0 The initialised candidate solution n2 1bits probability 12 The jump local optimum accepted probability αn f nm1 f nm αnm1n expected time ET n p n bound expected time transition For n m use Lemma 9 p nm αnn1m αn1 ni neighbouring states n m 1 n m 1 αn n m 1 m 1 αn n m 1 m 1 ET nm αnn1m nm1 αn αn αnn2m cid8 ET ET nm2 αn n m 2 n m 2 αnn3m αn ET n m 2 m 2 αn n m 2 m 2 cid9 nm1 αnn2m We note general ET nmk αnnk1m ET ET n2 αnnm21m αnn1 We require lower bound αn If m n5 consider negative drift number 1bits region n m 1 n 1 Clearly αn 1 best choice region lead large area negative drift 15 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 cid4i n n n cid4 cid9 cid8 4n 5 3 5 Since Metropolis local search algorithm region negative drift escaped large jump Hence Negative Drift Theorem Theorem 2 time escape region 2cid8n overwhelming probability 1 2 cid8n This lower bound time optimise function If m n5 probability 12 Metropolis hillclimb 3n4 4n5 Consider drift ﬁtness region αn 2 cid4i 1 2 n n n cid4 cid9 cid8 3n 4 1 8 Clearly avoid large region negative drift αn 2 required giving ET 2n1 2cid8n cid2 Unlike Metropolis MAHHOI consider magnitude ﬁtness difference solutions decide accept worsenings This allows optimise Jumpm smaller jump sizes m cid101 expected polynomial time Metropolis requires exponential time expectation cases 6 Multimodal optimisation basins attraction tuneable gradient size The CliffJumpdrs function class Fig 1c combines elements Cliffd Jumpm function classes Upon reaching local optimum ﬁrst OneMax slope r ﬁtnessworsening mutations decreasing ﬁtness current solution s overcome slope leading global optimum reached For algorithms ﬂip bit uniformly random mutation step RLS setting r 1 makes CliffJumpdrs similar Cliffd setting r d makes similar Jumpm consider function class parameters 1 r d n2 To begin following theorem shows radius region attraction local optimum large superlogarithmic MAHHOI Metropolis require superpolynomial expected time optimise CliffJumpdrs like elitist heuristics standard bit mutation Theorem 15 On CliffJumpdrs r ωlog n d 05 cn constant 0 c 05 s 0 expected optimisation time MAHHOI Metropolis parameters superpolynomial Proof We note algorithms probability 12 initialised solution containing n2 1bits case need descend entire length negativesloped segment CliffJumpdrs We apply Negative Drift Theorem regardless choice algorithm parameters reaching solution n d r 1bits superpolynomial time expectation overall expected runtime algorithm CliffJumpdrs superpolynomial law total probability Let Xt denote number 1bits current solution tth accepted mutation changing number 1bits current solution Suppose n d Xt n d r A solution fewer 1bits generated probability Xt n 05 c denote lower bound probability solution 05 c accepted probability 1 let p generated accepted single iteration A solution 1bits generated probability n Xtn 05 c 05 c denote upper bound probability solution accepted probability 1 let p generated accepted single iteration Thus conditional Xt changing increases 1 probability 05 c Thus expected change p p number 1bits 05 c decreases 1 probability p p p p EXt1 Xt n d Xt n d r 1 05 c 1 05 c 2c cid101 As algorithms employ local mutations P Xt1 Xt 1 0 apply Negative Drift Theorem Theorem 2 lower bound number steps needed increase Xt n d n d r region length cid8r ωlog n 2cid8r 2ωlog n nω1 accepted mutations changing probability 1 2 number 1bits required This lower bound number iterations MAHHOI Metropolis required construct solution n d r 1bits starting solution n d 1bits Thus expected optimisation time MAHHOI Metropolis parameters CliffJumpdrs r ωlog n d 05 c constant 0 c 05 superpolynomial cid2 cid8r 1 2 The following theorem shows radius region attraction local optimum constant MAHHOI able optimise CliffJumpdrs expected polynomial time Theorem 16 The expected runtime MAHHOI p 1 s 0 1εn constant ε 0 CliffJumpdrs 1 r d 16 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 cid8 O d r 1 εrn2r2 r 12r 1 cid9 Proof As MAHHOI considers sign ﬁtness difference parent offspring solutions magnitude behaviour unaffected choice s parameter CliffJumpdrs For r 1 result proven exactly proof Theorem 10 For r 1 adapt proof Theo rem 10 cope larger basin attraction local optimum Let T u number iterations MAHHOI takes ﬁrst step slope leading global optimum ﬁrst time constructs solution OneMax x n d r 1 This diﬃcult ﬁnding global optimum Jumpm m r 1 ET u O 1 εrn2r2r 12r 1 Theorem 13 If d r 1 global optimum point need separately bound time required climb second slope taking account possibility MAHHOI returning local optimum In language proof Theorem 10 ET u ET 1 ET 2 ET 3 need bound ET 4 j number iterations solution j 0bits accepted time solution Let T j 1 0bits accepted We bound ET dr ET u 1 j d r ET j 1 jn 0 1 jn p ET j1 ET j 1 1 jn p ET 1 1 jn j1 n n j p ET j1 j O n ET u p O 1n Hence dr1cid2 T j dr1cid2 ET j d r 1 O n ET u ET 4 E j1 j1 Thus MAHHOI ﬁnds global optimum CliffJumpdrs O d rn log n 1 εrn2r2r 12r 1 O d r1 εrn2r2r 12r 1 iterations expectation cid2 We provide tighter upper bound MAHHOI CliffJumpdrs negative slope far away global optimum Theorem 17 The expected runtime MAHHOI p 1 s 0 O nr3 1εn constant ε 0 CliffJumpdrs d cid10n r Proof For d cid10n r probability mutating solution higher number 1bits negative slope section CliffJumpdrs n d rn cid101 A mutation increases number 1bits negative slope accepted probability p 1 1εn acceptance probability MAHHOI ﬁxed affected ﬁtness decrease s Hence local optimum probability accepting r consecutive decreasing moves reach local minimum step slope cid8 1 1 εcn cid9 r1 constant c 0 cid11 cid10 r1 nd Let p 1 1εcn probability solution n d r 1 1bits reached r 1 consecutive steps solution n d 1bits We bound time reach bitstring n d 1bits random initialisation O n Theorem 7 bound time reach solution n d r 1 1bits Lemma 9 O n O cid15 cid14 nr1 O cid11 cid10 nr2 Similarly reusing arguments proof Theorem 16 expected time required climb second positive slope length d r O n reach global optimum bounded O n log n O n O nr2 O nr3 Lemma 9 Hence overall expected runtime bounded O nr3 cid2 17 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Theorem 16 Theorem 17 respectively upper bounds MAHHOI CliffJumpdrs general case case local optimum far away global optimum The following corollary shows nonelitist MAHHOI outperform wellknown elitist μ λ EAs standard bit mutation CliffJumpdrs cases provided s suﬃciently large constant The result generalises μ λ populations provide advantage 1 1 EA Jumpm slope 20 Corollary 18 MAHHOI faster 1 1 EA expectation CliffJumpdrs s 1 3r If d cid10n MAHHOI faster 1 1 EA expectation s 3r Proof The 1 1 EA standard bit mutation elitist algorithm accept mutations increase ﬁtness solution From local optimum CliffJumpdrs solution n d 1bits nearest solution higher ﬁtness consists cid11n d r rscid12 1bits Hence expected time perform mutation local optimum cid8nrs1 overall lower bound expected runtime 1 1 EA Theorem 16 gives general upper bound MAHHOI CliffJumpdrs O d rr n2r2 O n2r3 Hence state MAHHOI outperforms 1 1 EA expectation 2r 3 rs 1 That s 1 3r When d cid10n Theorem 17 gives tighter upper bound MAHHOI CliffJumpdrs O nr3 Hence state MAHHOI outperforms 1 1 EA case expectation r 3 rs 1 That s 3r cid2 In particular minimal basins attraction r 1 MAHHOI faster 1 1 EA s 4 general case s 3 d cid10n Furthermore basin attraction size r 3 MAHHOI faster s 2 s 1 respectively We note CliffJumpdrs modiﬁed global optimum point ﬁtness value exceeding local optimum expected runtime 1 1 EA cid8nd If d cid10n expected runtime exponential cases MAHHOI polynomially bounded case r constant Thus function illustrates nonelitism allow eﬃciently escape wide range local optima elitist EAs ineﬃcient Regarding Metropolis ﬁrst able exactly match performance MAHHOI CliffJumpdrs s 1 Theorem 19 On CliffJumpdrs s 1 integers d r MAHHOI Metropolis conﬁgured behave identically That p αn1 algorithms maintain identical state probability distributions runs Proof With exception global optimum ﬁtness difference CliffJumpdr1 bitstrings Hamming distance 1 1 1 Thus offspring global optimum differs parent ﬁtness 1 1 If offspring better parent ﬁtness difference 1 offspring global optimum MAHHOI Metropolis accept offspring solution If offspring worse parent parent isnt global optimum MAHHOI accept offspring solution probability p Metropolis accept offspring solution probability αn1 Thus p αn1 ensures global optimum algorithms probability accepting offspring solution considered optimisation process As algorithms initialised solution chosen uniformly random use mutation operator generate offspring solutions consider exactly offspring iteration maintain identical probabilities having observed global optimum given iteration budget cid2 We extend argument ﬁtness differences adjacent points equal entire search space MAHHOI Metropolis conﬁgured identical performance This holds ﬁtness functions display property Finally Metropolis sensitive magnitude ﬁtness differences neighbouring solutions exist choices CliffJumpdrs parameter s allow Metropolis outperform MAHHOI choices allow MAHHOI outperform Metropolis Theorem 20 On CliffJumpdrs 0 s 1 αn chosen Metropolis outperforms MAHHOI choice 0 p 1 If s 1 p chosen MAHHOI outperforms Metropolis choice αn Proof On CliffJumpdrs types mutations reduce ﬁtness value solution reduce number 1bits solution positive slopes x1 n d x1 n d r reduce ﬁtness 1 increase number 1bits solution negative slope n d x1 n d r reduce ﬁtness s Recall Metropolis accepts mutations decrease ﬁtness cid4 f probability αncid4 f Thus s 1 easier Metropolis accept mutations increasing number 1bits 18 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 negative slope compared mutations decreasing number 1bits positive slope opposite holds s 1 Suppose s 1 consider parameters Metropolis chosen outperform MAHHOI ﬁxed parameter p Choosing αn 1p ensures Metropolis matches MAHHOIs probabilities making progress positive slopes accepts mutations increase number 1bits solution negative slope 1 fast positive slopes MAHHOI probability αns p 1s ensures p traversing negative slope faster MAHHOI p Choosing αn p Metropolis matches MAHHOIs probabilities making progress negative slope making Metropolis likely accept detrimental mutations positive slopes Choosing αn values result 1 reduced improvement MAHHOI slope types Combining results yields Metropolis p αn p 1s outperforms MAHHOI CliffJumpdrs parameter p 0 s 1 s p Thus Metropolis αn p Similarly αns p αn1 ensures MAHHOI outperforms Metropolis CliffJumpdrs s 1 Here choos ing p αns allows MAHHOI replicate Metropolis behaviour negative slope making MAHHOI likely accept worsenings positive slopes p αn1 allows MAHHOI replicate Metropolis behaviour pos itive slopes making MAHHOI likely accept mutations increasing number 1bits negative slope values boundaries providing mix behaviours cid2 Theorem 20 implies Metropolis preferable CliffJumpdrs ﬁtness differences adjacent points negative slope smaller magnitude adjacent points positive slopes MAHHOI preferable opposite true MAHHOI eﬃcient CliffJumpdrs radius local optimas basin constant s 0 able escape jump cliff sections polynomial time We prove Metropolis eﬃcient negative slope gentle short far away global optimum Theorem 21 The expected runtime Metropolis CliffJumpdrs r 1 superpolynomial s ω1 Furthermore d 05 cn constant 0 c 05 expected runtime polynomially bounded r O log n rs O 1 Proof Recall Theorem 8 expected runtime Metropolis polynomially bounded OneMax αn cid8n log n bound αn hold order guarantee polynomial expected runtime OneMax sections CliffJumpdrs Metropolis initialised solution containing n2 1 1bits probability 12 case required descend entirety negative slope CliffJumpdrs ﬁnding global optimum Metropolis accepts moves increase number 1bits negative slope probability αns αns expected time Since αn cid8n log n required polynomial expected runtime OneMax slopes bound s O 1 required polynomial expected runtime negative slope length r 1 Therefore s O 1 necessary order bound overall expected runtime polynomial setting s ω1 superpolynomial expected runtime We know Theorem 15 r ωlog n d 05 cn constant 0 c 05 expected runtime Metropolis CliffJumpdrs superpolynomial Therefore consider case r O log n Let ET LM denote expected time required reach local maximum expected time ﬁnd solution n d 1bits ﬁrst time initialisation By Lemma 9 expected time step negative slope constants c 0 c cid5 0 c αns c cid5αns1 ET LM cid10αns ET LM O n Furthermore reusing arguments proof Theorem 14 repeated application Lemma 9 expected time r ﬁtnessdecreasing steps negative slope cid10αnrs Therefore αn cid8n log n required polynomial expected runtime OneMax slopes rs O 1 r O log n necessary polynomial expected runtime Metropolis CliffJumpdrs settings rs ω1 superpolynomial expected runtime cid2 7 When Metropolis outperforms acceptance hyperheuristic In previous sections analysed performance MAHHOI Metropolis multimodal landscapes typical characteristics The analysis showed MAHHOI robust Metropolis escaping local optima sensitive ﬁtnessgradients basinsofattraction overcome On hand Metropolis exhibits better performance smooth basins attraction exhibiting slow ﬁtnessdecreasing slopes In section present analysis GentleNegativeSlope GNS function designed Metropolis advantage long slowly decreasing slope surrounded points considerably lower ﬁtness Since Metropolis unlikely accept moves away smooth ﬁtnessdecreasing gradient optimum MAHHOI high chance accept disadvantageous moves function allows 19 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 highlight landscape characteristics Metropolis considerably outperforms MAHHOI In particular MAHHOI penalised taking ﬁtnessdifferences account accepting worsening solutions characteristic crucial HH outperform preferable choice Metropolis ﬁtness landscapes previously considered paper The following theorem proves Metropolis optimise GNS expected polynomial time parameter setting allows eﬃciently hillclimb steep OneMax slope perform random walk ridge linking local optimum global optimum large ﬁtness difference ridge rest search space Theorem 22 38 Theorem 11 The expected runtime Metropolis αn n42n GNS O n3 The Metropolis algorithm αn n42n reach local optimum 1n O n log n expected iterations random search path leads algorithm global optimum 0n expected O n3 time 38 Jansen Wegener proved 1 1 EA exponential expected runtime GNS 2cid8n choice cid8n mutation rate 38 The initialised solution 1 1 EA n3 1bits probability 1 2 ﬁnding path point 1i 0ni n6 exponential time If path point 1i0ni sampled n6 large jump required ﬁnd global optimum taking exponential expected time The following theorem proves MAHHOI exhibits poor performance GNS regardless choice parameter p Essentially MAHHOI ﬁtness differences account choosing accept ﬁtnessdecreasing offspring able stay eﬃciently navigate gentle ridge local optimum global optimum GNS Theorem 23 The runtime MAHHOI GNS 0 p 1 nω1 probability 1 n ω1 Proof As GNS unique global optimum 0n Theorem 6 applies expected runtime MAHHOI GNS large expected runtime MAHHOI OneMax p For p ω applying Theorem 5 yields MAHHOI requires nω1 iterations probability 1 n cid14 ω1 optimise GNS n4 4 Suppose p n34n consider behaviour MAHHOI region OneMax value MAHHOIs current n2 MAHHOI decrease OneMax value current solution ﬂips solution 4 remaining 1bits applies AM selection operator occurs probability p OneMaxxn 12n Within region MAHHOI increase OneMax value current solution ﬂips ﬁrst 0bit solution occurs probability 1n Thus conditional OneMax value current solution changing iteration MAHHOI increases 1 MAHHOI applies RLS mutation operator probability 1n1n 12n 23 decreases 1 probability 12n1n 12n 13 n4 Xt b 4 Let Xt OneMax value MAHHOIs current solution tth change We focus region length cid9 4 n4 Xt 4 n2 Then n log nn cid15 EXt1 Xt 4 n4 Xt 4 n2 23 13 13 Since MAHHOI applies RLS mutation Pr Xt1 Xt 1 0 Thus conditions apply Negative Drift Theorem Theorem 2 satisﬁed ﬁrst hitting time Xt 2cid8n14 probability 1 2 cid8n14 It remains note MAHHOI initialised n4 1bits current solution probability cid8n Thus X0 b runtime MAHHOI GNS number iterations solution 1 e Xt ﬁrst constructed OneMax value solution changes holds runtime MAHHOI p n34n GNS 2cid8n14 nω1 probability 1 2 cid8n14 1 n ω1 Finally note n34n ω arguments applies Hence MAHHOI requires probability 1 n global optimum GNS cid2 n log nn Therefore regardless value p set ω1 nω1 iterations ﬁnd Hence completed picture providing ﬁtness landscape characteristics allow Metropolis ﬁnd global optimum eﬃciently MAHHOI requires exponential time parameter setting 8 Conclusions We presented analysis performance Move Acceptance HyperHeuristic MAHHOI multimodal optimisation The hyperheuristic chooses random OnlyImproving OI acceptance operator probability 1 p AllMoves AM acceptance operator probability p All statements given paper MAHHOI hold MAHHIE variant MAHHOI chooses ImprovingandEqual acceptance operator probability 1 p AllMoves acceptance operator probability p We summarise ﬁndings Table 1 We ﬁrst identiﬁed parameter values allow algorithm hillclimb local global optima We shown setting parameter value p O log log n1εn constant ε 0 allows hillclimb OneMax function 20 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 Table 1 Comparative performance MAHHOI Metropolis 1 1 EA functions considered paper For details relevant theorems Formal theorems presented proofs straightforward Function OneMax Cliffd Jumpm CliffJumpdrs 0 s 1 CliffJumpdrs 1 s GNS MAHHOI Metropolis O n log n cid11 cid10 n log n n3 cid10 d2 n log n 1εm1n2m m2m O O slower faster nω1 O n log n cid10 cid11 d12 cid11 n log n 2cid8n faster slower O n3 1 1 EA O n log n cid10nd cid10nm cid10nd cid10nd cid10nn expected O n log n runtime desired On hand large parameter values shown runtime exponential function unique optimum Afterwards considered multimodal function classes different characteristics encountered applications involving optimisation process Firstly showed local optima gradient increasing ﬁtness close MAHHOI eﬃcient For purpose Cliffd function class MAHHOI matches best possible asymptotic expected runtime unary unbiased randomised search heuristic problem polynomial number optima 3 On hand elitist search heuristics Metropolis require exponential runtime distance local global optima We considered ﬁtness landscapes harder escape local optima large basins attraction decreasing ﬁtness traversed global optimum identiﬁed For purpose considered standard Jumpm function class While Metropolis requires exponential runtime problem size overwhelming probability independent size basin attraction local optima MAHHOI polynomial expected runtime large basins runtime exponential size basin attraction overwhelming probability We pointed higher mutation rates shown beneﬁcial compared nonelitism settings Obviously advantages beneﬁt MAHHOI incorporated mutation operator allowing ﬂip bit iteration The reason better performance MAHHOI Metropolis considered landscapes particular sen sitivity algorithm magnitude ﬁtness difference parent offspring solutions To shed light aspect designed general function class called CliffJumpdrs steepness smoothness slopes local optimas basin attraction tuned The analysis function class reveals Metropolis tuned outperform MAHHOI smooth basins gently decreasing ﬁtness oppo site occurs ragged basins ﬁtness values change drastically In particular depending magnitude negative slope CliffJumpdrs parameters Metropolis MAHHOI set outperform algo rithm regardless choice parameters However basin large MAHHOI optimises function polynomial time independent steepness gradient Metropolis requires basin decreases gently eﬃcient We conclude paper capitalising sensitivity Metropolis present scenario Metropolis sig niﬁcantly outperforms MAHHOI The GNS function introduced originally Jansen Wegener 38 consists steep OneMax style ﬁtness gradient followed path slowly decreasing ﬁtness surrounded points drastically lower ﬁtness While Metropolis able follow gradient ignoring drastically worse solutions MAHHOI falls path overwhelming probability accepting points considerably lower ﬁtness Overall apart pathological scenario believe analysis presented paper provides considerable theoretical evidence advantages MAHHOI Metropolis multimodal optimisation Furthermore analysis CliffJumpdrs sheds light wide range basins attraction nonelitist MAHHOI outperforms elitist evolutionary algorithms polynomial versus exponential runtimes We believe allowing arbitrarily large radii attraction benchmark function provides convincing example importance nonelitism combinatorial optimisation commonly Cliffd function class Indeed results giving mildly general advice existing approaches cope local optima preferable situations recently deemed Doerr highly desirable 20 We believe contribution sheds light topic The analysis points aspects investigated build results On hand experimental theoretical comparisons nonelitist heuristics classical combinatorial optimisation problems undertaken validate insights presented shed light differences behaviour performance forms nonelitism On hand studies undertaken equipping MAHHOI sophisticated mutation operators hyperheuristic options escaping local optima Given MAHHOI Metropolis outperform different scenarios analysis HH selects algorithms applied different stages optimisation process promising route explored general effectiveness multimodal optimisation Concerning general ﬁelds automated algorithm selection hyperheuristics use sophisticated acceptance operators explored One interesting option allow MAHH automatically adapt 21 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 value parameter p optimisation process keeping static similarly adaptation learning period heuristic selection hyperheuristics 2547 Another natural direction investigate sophisticated moveacceptance approaches commonly hyperheuristics literature Monte Carlo approaches 2 lead improved performance Finally comprehensive hyperheuristics choose multiple pa rameter sets mutation selection population size analysed Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Data availability No data research described article Acknowledgements This work supported EPSRC Grant No EPM0042521 The authors like thank Dirk Sudholt suggesting improve upper bound Theorem 13 References 1 F Alanazi PK Lehre Runtime analysis selection hyperheuristics classical learning mechanisms Proceedings IEEE Congress Evolutionary Computation CEC 14 IEEE 2014 pp 25152523 2 M Ayob G Kendall A Monte Carlo hyperheuristic optimise component placement sequencing multi head placement machine Proceedings International Conference Intelligent Technologies InTech 03 Springer 2003 pp 132141 3 G Badkobeh PK Lehre D Sudholt Blackbox complexity parallel search distributed populations Proceedings Workshop Founda 4 B Bilgin E Özcan EE Korkmaz An experimental study hyperheuristics exam timetabling Practice Theory Automated Timetabling tions Genetic Algorithms FOGA 15 ACM 2015 pp 315 PATAT 07 Springer 2007 pp 394412 5 E Burke G Kendall E Soubeiga A tabusearch hyperheuristic timetabling rostering J Heuristics 9 6 2003 451470 6 EK Burke M Gendreau M Hyde G Kendall G Ochoa E Özcan R Qu Hyperheuristics survey state art J Oper Res Soc 2013 16951724 Springer 2010 pp 449468 7 EK Burke M Hyde G Kendall G Ochoa E Özcan JR Woodward A classiﬁcation hyperheuristic approaches Handbook Metaheuristics Artif Intell 274 2019 180196 Comput Sci 832 2020 166185 25 5 2021 956970 01 Springer 2001 pp 176190 8 D Corus PS Oliveto D Yazdani On runtime analysis optIA artiﬁcial immune Proceedings Genetic Evolutionary Computation Conference GECCO 17 ACM 2017 pp 8390 9 D Corus PS Oliveto D Yazdani Fast artiﬁcial immune systems Parallel Problem Solving Nature PPSN 18 Springer 2018 pp 6778 10 D Corus PS Oliveto D Yazdani Artiﬁcial immune systems ﬁnd arbitrarily good approximations NPhard number partitioning problem 11 D Corus PS Oliveto D Yazdani On inversely proportional hypermutations mutation potential Proceedings Genetic Evolutionary Computation Conference GECCO 19 ACM 2019 pp 215223 12 D Corus PS Oliveto D Yazdani When hypermutations ageing enable artiﬁcial immune systems outperform evolutionary algorithms Theor 13 D Corus PS Oliveto D Yazdani Automatic adaptation hypermutation rates multimodal optimisation Proceedings Workshop Foundations Genetic Algorithms FOGA 21 ACM 2021 pp 112 14 D Corus PS Oliveto D Yazdani Fast immune systeminspired hypermutation operators combinatorial optimization IEEE Trans Evol Comput 15 P Cowling G Kendall E Soubeiga A hyperheuristic approach scheduling sales summit Practice Theory Automated Timetabling PATAT 16 P Cowling G Kendall E Soubeiga Hyperheuristics tool rapid prototyping scheduling optimisation Proceedings Workshop Applications Evolutionary Computing EvoWorkshops 02 Springer 2002 pp 110 17 D Dang T Friedrich T Kötzing MS Krejca PK Lehre PS Oliveto D Sudholt AM Sutton Escaping local optima crossover emergent 18 DC Dang A Eremeev PK Lehre Escaping local optima nonelitist evolutionary algorithms Proceedings AAAI Conference Artiﬁcial diversity IEEE Trans Evol Comput 22 3 2018 484497 Intelligence vol 35 2021 pp 1227512283 19 DC Dang T Friedrich T Kötzing MS Krejca PK Lehre PS Oliveto D Sudholt AM Sutton Escaping local optima diversity mechanisms crossover Proceedings Genetic Evolutionary Computation Conference GECCO 16 ACM 2016 pp 645652 20 B Doerr Does comma selection help cope local optima Proceedings Genetic Evolutionary Computation Conference GECCO 20 ACM 2020 pp 13041313 21 B Doerr C Doerr Theory parameter control discrete blackbox optimization provable performance gains dynamic parameter choices B Doerr F Neumann Eds Theory Evolutionary Computation Recent Developments Discrete Optimization Springer 2020 pp 271321 22 B Doerr C Doerr J Yang kbit mutation selfadjusting k outperforms standard bit mutation Proceedings International Conference Parallel Problem Solving Nature PPSN 16 Springer 2016 pp 824834 23 B Doerr D Johannsen C Winzen Drift analysis linear functions revisited IEEE Congress Evolutionary Computation CEC 10 IEEE 2010 24 B Doerr HP Le R Makhmara TD Nguyen Fast genetic algorithms Proceedings Genetic Evolutionary Computation Conference GECCO pp 18 17 ACM 2017 pp 777784 25 B Doerr A Lissovoi PS Oliveto JA Warwicker On runtime analysis selection hyperheuristics adaptive learning periods Proceedings Genetic Evolutionary Computation Conference GECCO 18 ACM 2018 pp 10151022 22 A Lissovoi PS Oliveto JA Warwicker Artiﬁcial Intelligence 314 2023 103804 26 B Doerr D Sudholt C Witt When evolutionary algorithms optimize separable functions parallel Proceedings Workshop Founda tions Genetic Algorithms FOGA 13 ACM 2013 pp 5164 27 JH Drake A Kheiri E Özcan EK Burke Recent advances selection hyperheuristics Eur J Oper Res 285 2 2020 405428 28 S Droste Analysis 1 1 EA noisy OneMax Proceedings Genetic Evolutionary Computation Conference GECCO 04 Springer 2004 pp 10881099 Genetic Algorithms FOGA 01 ACM 2001 pp 275294 29 S Droste T Jansen I Wegener Dynamic parameter control simple evolutionary algorithms Proceedings Workshop Foundations 30 S Droste T Jansen I Wegener On analysis 1 1 evolutionary algorithm Theor Comput Sci 2002 5181 31 T Friedrich F Quinzan M Wagner Escaping large deceptive basins attraction heavytailed mutation operators Proceedings Genetic Evolutionary Computation Conference GECCO 18 ACM 2018 pp 293300 32 GT Hall PS Oliveto D Sudholt On impact performance metric eﬃcient algorithm conﬁguration Artif Intell 303 2022 103629 33 V Hasenöhrl AM Sutton On runtime dynamics compact genetic algorithm jump functions Proceedings Genetic Evolu tionary Computation Conference GECCO 18 ACM 2018 pp 967974 34 J He X Yao Drift analysis average time complexity evolutionary algorithms Artif Intell 127 1 2001 5785 35 MA Hevia Fajardo D Sudholt Selfadjusting offspring population sizes outperform ﬁxed parameters cliff function Proceedings Workshop Foundations Genetic Algorithms FOGA 21 ACM 2021 pp 115 36 J Jägersküpper T Storch When plus strategy outperforms comma strategy Proceedings IEEE Symposium Foundations Computational Intelligence FOCI 07 IEEE 2007 pp 2532 37 T Jansen Simulated annealing A Auger B Doerr Eds Theory Randomized Search Heuristics World Scientiﬁc 2011 pp 171195 38 T Jansen I Wegener A comparison simulated annealing simple evolutionary algorithm pseudoboolean functions unitation Theor Comput Sci 2007 7393 39 A Juels M Wattenberg Stochastic hillclimbing baseline method evaluating genetic algorithms Proceedings 8th International Con ference Neural Information Processing Systems NIPS 95 MIT Press 1995 pp 430436 40 L Kotthoff Algorithm selection combinatorial search problems survey AI Mag 35 3 2014 4860 41 T Kötzing D Sudholt M Theile How crossover helps pseudoboolean optimization Proceedings Genetic Evolutionary Computation 42 PK Lehre PS Oliveto Theoretical analysis stochastic search algorithms MGC Resende R Marti PM Pardalos Eds Handbook Heuristics 43 PK Lehre E Özcan A runtime analysis simple hyperheuristics mix mix operators Proceedings Workshop Foundations Conference GECCO 11 ACM 2011 pp 989996 Springer 2018 pp 136 Genetic Algorithms FOGA 13 ACM 2013 pp 97104 44 PK Lehre C Witt Blackbox search unbiased variation Algorithmica 2012 623642 45 J Lengler Drift analysis B Doerr F Neumann Eds Theory Evolutionary Computation Recent Developments Discrete Optimization Springer 46 A Lissovoi PS Oliveto JA Warwicker On time complexity algorithm selection hyperheuristics multimodal optimisation Proceedings AAAI Conference Artiﬁcial Intelligence AAAI 19 2019 pp 23222329 47 A Lissovoi PS Oliveto JA Warwicker How duration learning period affects performance random gradient selection hyperheuristics Proceedings AAAI Conference Artiﬁcial Intelligence AAAI 20 2020 pp 23762383 48 A Lissovoi PS Oliveto JA Warwicker Simple hyperheuristics control neighbourhood size randomised local search optimally LeadingOnes Evol Comput 28 3 2020 437461 49 N Metropolis AW Rosenbluth MN Rosenbluth AH Teller E Teller Equation state calculations fast computing machines J Chem Phys 1953 10871092 50 A Nareyek Choosing search heuristics nonstationary reinforcement learning Metaheuristics Computer DecisionMaking Springer 2004 51 PS Oliveto Rigorous performance analysis hyperheuristics N Pillay R Qu Eds Automated Design Machine Learning Search Algorithms 52 PS Oliveto C Witt Simpliﬁed drift analysis proving lower bounds evolutionary computation Algorithmica 2011 369386 53 PS Oliveto C Witt Erratum Simpliﬁed drift analysis proving lower bounds evolutionary computation CoRR arXiv12117184 abs 2012 54 PS Oliveto C Witt Improved time complexity analysis simple genetic algorithm Theor Comput Sci 605 2015 2141 55 E Özcan B Bilgin EE Korkmaz Hill climbers mutational heuristics hyperheuristics Parallel Problem Solving Nature PPSN 06 Springer pp 523544 Springer 2021 pp 4571 2006 pp 202211 56 E Özcan B Bilgin EE Korkmaz A comprehensive analysis hyperheuristics Intell Data Anal 12 1 2008 323 57 T Paixão J Pérez Heredia D Sudholt B Trubenová Towards runtime comparison natural artiﬁcial evolution Algorithmica 2017 681713 58 N Pillay R Qu HyperHeuristics Theory Applications Springer 2018 59 M Preuss Multimodal Optimization Means Evolutionary Algorithms Natural Computing Series Springer 2015 60 C Qian K Tang ZH Zhou Selection hyperheuristics provably helpful evolutionary multiobjective optimization Parallel Problem Solving 2020 pp 89131 61 A Rajabi C Witt Selfadjusting evolutionary algorithms multimodal optimization Proceedings 2020 Genetic Evolutionary Computa Nature PPSN 16 Springer 2016 pp 835846 tion Conference GECCO 20 ACM 2020 pp 13141322 Conference GECCO 21 ACM 2021 pp 11781186 mization EvoCOP 21 2021 pp 152168 2019 pp 541579 62 A Rajabi C Witt Selfadjusting evolutionary algorithms multimodal optimization Proceedings Genetic Evolutionary Computation 63 A Rajabi C Witt Stagnation detection randomized local search European Conference Evolutionary Computation Combinatorial Opti 64 T Stützle M LópezIbáñez Automated design metaheuristic algorithms M Gendreau JY Potvin Eds Handbook Metaheuristics Springer 65 D Sudholt A new method lower bounds running time evolutionary algorithms IEEE Trans Evol Comput 17 3 2013 418435 23