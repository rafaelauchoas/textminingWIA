ELSEVIER Artificial Intelligence 85 1996 5999 Artificial Intelligence An empirical symbolic approach processing natural language Roberto Basili Maria Teresa Pazienzaa3 Paola Velardib2 Department Computer Science Systems Production University Tor Vergata Via della Ricerca Scientifica 00133 Roma Italy bIstituto di Informatica University Ancona Ancona Italy Received October 1995 revised November 1995 Abstract conceptual systems work Empirical methods language processing field natural mechanisms human communication linguistic phenomena Probabilistic meritorious view intrinsically unable NLP usually based probabilistic model language These methods recently gained popularity claim provide better coverage language phenomena Though claim entirely proved empirical methods certainly outperform regard rationalist symbolic methods However empirical methods provide probabilistic explanation analyzed applications insight plain words word clusters attached probabilities Eventually sense data In past years explored combining linguistically language ARIOSTO_LEX methods acquisition selectional languages acquired NLP useful comparative ARIOSTO_LEX impact largescale applicability commonly NLP techniques lexical learning knowledgebased restrictions words sublanguages We present real provide represented human analyst possibility lexical data practical applications analysis sublanguages NLP Our objective scalable analysis acquisition theoretically results largescale Importantly problematic advantages empirical rationalist In paper evaluate uses combination shed light recurrent linguistic phenomena define methods lexical knowledge different probabilistic data obtained amenable experimental different appealing approaches output domains founded corpora Corresponding 1 Email pazienzainfoutovrmit Email velardianvaxlcinecait author Email basiIiinfoutovrmit 00043702961500 Copyright 0 1996 Elsevier Science BV All rights reserved SSDI 0004370295001166 60 H Bad et d Artificial lnrelligrncr 85 lYY6 SYYY 1 The empiricist resurgence natural language processing At beginning 1990s field natural language processing systems research concluded impass commercial commercial like robust admit overview The panorama systems discouraging In 1989 Financial Times willing based NLP 20 presented technology editorial spread sentences coverage modest background knowledge resolve linguistic sustain natural free ambiguities flowing conversation subject choice unlikely exist decades Though ambitious increase commercially formation text greatly actual task There variety interfaces partial use NLP everyday deep understanding scientist objective impact NLP systems industrial retrieval required field NLP seriously admittedly pursuing complex failed objective missed building based NLP acceptability computers databases technologies applications technology online innovative But translation situations important programs limited help applications NLP limited coverage real systems The manual aspects lexical knowledge unrealistic poor acquisition language processors codifica systematic practical On hand lexical knowledge fully demonstrated representation analysis real applications beginning view In panorama scientific new era empiricists resurgence events marked The major limitation exhibit applied tion basis proliferation merits applications deficiencies theories l In 1990 symposium titled Textbased 33 baptised held note Intelligent new Stanford Systems In introductory Jacobs systems combine artificial intelligence techniques robust shallower methods The meeting success idea improving Linguistics Watson Research Center proposed field speech processing proposed methodwas techniques Computational use stochastic methods translation paper published group researchers coverage year In NLP systems use shallow milestone paper The paperand IBM Thomas widely striking 15 l In machine success based methods The core idea statistically language available crude generally extensive online limited model translations methods The quantitative mathematical model sophisticated methods adopted analysis resources predictive NLP learn word patterns word The statistical methods statistical texts current word counts NLP roughly grouped according R Basili et al I Artificial Intelligence 85 1996 5999 61 stream analysis information independently analysis 17 cooccurring words Wl W2 compares concerned analysis word cooccur A research mutual rences One popular measure cooccurrence tscore lexical ussociution The information derivations mutual probability observation Wl W2 probability observing cooccurrence A second stream recast language modelling problem computing probability single word W given words preceded W sentence These statistical models based Shannons Noisy Channel Model An overview A group currently categorize approach summarized In 40 objectives methods 18 minority An overview statistical measures language phenomena learning methods use machine Probabilistic techniques applied encouraging language processing problems natural disambiguation translation difficult read summarized partofspeech 15 etc3 The literature sections tagging 36 word classification like syntactic results variety 47 28 automatic area recently grown point field 3 semantic published Our contribution NLP One major claims followers probabilistic approaches initial represented 46 think statistically based methods training optimal parameter scalability Though supporters largescale methods linguistics strict equivalence probability calculus scalability Many probabilistic models fact require setting Often manual work statistically reliable results obtained small fragment data case suspect problem handled easily hand One example sense scale disambiguation Many methods described require manual training4 statistical model ambigu ous word Perhaps approach based manually defined heuristic rules general studies literature Another word represented clusters attached probabilities A conceptual explanation results provided Eventually gain linguistic insight matter But manual analysis complex inspection raw tests thousands millions different observed word patterns For example word clustering methods 22 30381 create word groups similarity linguistic ground problem statistically based methods words word strings bilingual word correspondences human analyzer sense data types comparative literature output 3 We selected brevity representative papers application 4 That given learning set sentences manually assigned senses including ambiguous word occurrence word h7 R Busili et ul Artificial Intelligence 85 1996 X99 evaluated contrast inspection machine learning No conceptual conceptual In conclusion agree description clustering methods general empirical methods cluster provided rationalist methods outperform claim taken concerned applications inherently analysis The linguistic material thesis paper inadequate produced far coverage granted concerned Furthermore meritorious NLP believe empirical methods case poorly concerned theoretical recent work pure symbolic methods scale pure quantitative methods dig coverage deep An linguistic quantitative artificial qualitative balancing area NLP integration insight We believe consider field methods intelligence obtain domain necessary problem In sections ARIOSTO_LEX past years probabilistic combination extensively acquires undertaken selectional sense data word observations adopted selectional probabilistic restrictions model semantic model like represented relations The semantic model developed knowledge restrictions ARIOSTO_LEX area NLP It corpora The interpretation corpusbased bias The approach sublanguages general start based methods ARIOSTO_LEX words empirical empirical derive interpretation data studies highlevel lexical human founded naive categories semantic learning activity Its definition The acquisition unrestricted requires minimal relatively wellspecified use online thesaura reduced casebased ARIOSTO_LEX The choices methodologies implies analysis semantic adopted lexicon aspects representation cognitive modelling design balancing symbolic methods quantitative However evaluation paper results Though provide emphasis main processing data obtained steps aim critically corpora different different approach present combination numeric methods applications allows acquire NLP lexical data useful comparative sublanguages Furthermore experimental findings impact popular NLP techniques discuss domains objective challenging development knowledge probabilistic qualitative summary experimental languages We symbolic practical analysis applicability 2 An overview ARIOSTO_LEX ARIOSTO_LEX g acquires corpusbased lexical types linguistic knowledge learning ARIOSTO like syntactic disambiguation R Basili et al I Artificial Intelligence 85 1996 5999 63 7 conceptual clusters words 610 ARIOSTO applied criteria far fields information retrieval hypertextual navigation 3 The general objectives ARIOSTO project linguistic shed light interesting computational probabilistic knowledgebased demonstrate techniques recurrent sublanguages advantages combining largescale lexical acquisition language phenomena study sublanguages We ARIOSTO l legal domain LD taxation norms l commercial domain CD agricultural activities l collection remote sensing RSD abstracts We environmental crossanalysis categorization process analyzing medical domain English Italian In near future plan systematic types sublanguage English Italian Italian domain restrictions ARIOSTO_LEX application corpora The lexicon objective acquiring extensively lexicon word acknowl sense selectional MT edged major components NLP machine NLP systems based systems far based lexicon However handbuilt 78k word lexicons obvious problems size extension barrier Therefore expect sizable lexicons successful It broadly agreed automatically implementations translation industrial A fundamental property computational lexicons account relations structure conceptual argument instrument purpose words arguments Arguments identified position relation names partof predicate annotated selectional agent restrictions impose type constraints set content words arguments relation Selectional instantiate NLP provide semantic information necessary syntactic approaches basis majority computational semantic disambiguation Arguments restrictions food restrictions Unfortunately flavour practice hand writing selectional easy matter time consuming hard consistency data lexicon thousand words The major difficulty words relate different domaindependent ways lexicons filled neat examples The current vast literature computational language domains eatanimute selectional constraints words unintuitive It matter kill process car drinks violating gasoline Rather hard harder structure tune adopted precise semantic relationships present lexical knowledge sublanguage general dictionaries slot conceptual The key idea ARIOSTO LEX assign appropriate lexical representation imagine priori semantic expectations exist statistically expresses linguistic relations dictionaries standard relationships relevant 6i R Basili et al I Arttjicial Intelligencr X5 1996 5999 A short description algorithm presented summarize main steps analysis The subsequent sections provide details step 1 2 3 corpus restrictions statistically application semantic patterns ACTwithINSTRUMEN identify semantic patterns detected tagger Highlevel semantic relevant sublanguages The linguist replace syntactic INSTRU The step prevailing generalized shallow TALITY Generalized syntactic analyzer semantic tags assigned words manually online thesaurus available We detected semantic patterns unintuitive generalize sublanguages Then generalized patterns linguist identify selectional links appropriate MENT INSTRUMENTALITY identify conceptual power posited set relations evaluated posteriori case relations use conceptual acquisition step obviously Finally use restrictions acquisition domain relevant selectional sublanguage sublanguage Selectional strength expectation selectional automatic lexicon The algorithm extracts content words w restrictions weighted statistical measure semantic bias algorithm casebased descriptive In subsequent relation ACT We statistically add informative power semantic restrictions semiautomatically coarse sublanguage automatic way essential including w fragment sentences conceptual significant relations acquired Though details step needed advantages approach respect brief description highlights pure quantitative important methods patterns cooccurrence linguistic patterns generalized linguistic material let human sublanguage analysis probabilistic methods Digging deep Generalized amenable analyst sink ocean data Scaling Since detected method sensitive problem low counts Quantitative methods defined literature unreliable applied linguistic patterns drawback rare patterns rarely observed use learning majority To obtain relatively good coverage Instead general corpora million words rarely available ized patterns predictive power interpret word patterns observed learning corpus It possible necessary In Sections 21 22 illustrate method coarse occurrence discussion patterns algorithm extracted results comparing corpora sublanguages acquisition casebased provide detailed In Section 23 lexicon semantic Hereafter use conceptual graph 4l notation express selectional restrictions R Basili et al I Artificial Intelligence 85 1996 5999 65 Finally Section 24 provides linguistic analysis data A formal method performance Section 3 partial sake brevity presented evaluation 21 Acquisition syntactically semantically tagged word cooccurrences The input ARIOSTO_LEX ARIOSTO provided corpus preprocessing module There phases 1 First corpus analyzed grammarbased speech tagger surface syntactic analyzer described 59 The syntactic analyzer produces database productive word pairs triples identify like example NV subject relation surface syntactic relations prepositional relation VprepN direct object VN phrases We triples elementary following structure links ess An es1 syntactic NprepN eslw1 prep wl prep expresses VN relations syntactic relation In case prep nil Each detected es1 weighted measure called plausibility es1 syntactic sentence John flies Rome sentence For example colliding esls collision sets formally defined inversely proportional structures plane plausibility detected number mutually 4 To simplify excluding N_prep_NRome plane V_prep_N plane 2 The evidence given es1 type corpus computed sum plausibility values identical esls words syntactic relation Second word included domain appropriate STRUMENTALITY domain appropriate actual tagging automatic example WordNet tagged set IN selection like highlevel categories ABSTRACTION set categories best performed manually 12 English language es1 semantically like HUMANENTITY online thesaurus Though available There motivations highlevel categories briefly summarized First highlevel tags ambiguous assigning word categories relatively intuitive Hence simple task proposed automatically create flat set categories WordNet 6 In 29 method controlled upper lower bound category size However objective select 1215 domain appropriate categories easy reliable perform manually choice categories 66 R Basili et al 1 Artificial Intelligence 85 1996 5999 thesauri available automatic tagging Second highlevel tags support psychologically plausible model semantic bootstrapping 39 human language learning They represent semantic bias lexical learning Semantically tagged esls N_prep_NtemperuturePROPERTY waterlNATURAL_OBJECT input ARIOSTO_LEX One important semantically tagged advantage major significantly reduce problem low counts In fact evidence limitations corpusbased corpus increased pattern observation syntactically semantically similar patterns For example pattern similar following statistical methods esls N_prep_NemissivityPROPERTY airlNATURAL_OBJECT To evaluate syntactically marked cooccurrences respect problem low counts performed experiment semantically advantages Table 1 summarized numerically Table 1 shows data obtained extracting legal domain LD cooccurrences including word reddito income methods 1 Distancebased associations derived extracting pairs second word 5 words apart income excluding articles techniques conjunctions prepositions Such windowing popular corpusbased literature 2 Syntactic cooccurrences including word income In literature detect cooccurrences 3 Semantic cooccurrences esls extracted syntactic analyzer surface parsers obtained set syntactic cooccur rences ambiguous semantic tags assigned word income For example consider sentence fragment different methods detect cooccurrences Table 1 Statistics Method 1 Distancebased 2 Syntactic 3 Semantic cooccurrences cooccurrences In case online word ontologies acceptable thesauri In general Total 4044 5609 7048 Different Frequencies 3 preserved information 623 1454 311 3546 3272 6869 087 076 097 produced humans classification choices reflect relative difficulties manually highlevel categories defining R Basili et al Artificial Intelligence 85 1996 5999 67 Z redditi 1 di 2 marittimu8 o aereu9 in15 uno Stutol6 di 4 gestione 3 imprese 5 di 6 navigazione 7 10 sede 11 di 12 direzione 13 effettivu 14 ono imponibili 17 soZo 18 nello 19 Stuto20 The wordbyword translation of2 incomel means deriving The of4 compunies 5 6 muritime 8 ueriul 9 nuvigution 7 10 ofice 11 12 uctuul 14 munugement 13 means primary management 19 office Stute20 zk eligible tuxution 17 only18 munugement3 inU Stutel6 With method l following associations obtained including word income l2 13 14 l5 l61 method 2 l23 l45 l67 lloll l1213 11516 117 detects subjectverb Notice surface syntactic parser detects syntactic relations sentence context reddito di imprese reddito di semantically correct nuviguzione eligible missed distancebased methods surface parsers words far apart Most statistical methods use filtering techniques posteriori increase important initial information The filtering techniques preserve possible use described precision collected data recall Therefore reduce noise However income techniques filtering relation With method 3 associations detected method semantically similar For example grouped gestione reddito di nuviguzione similar navigation belong nuviguzione management category activity ACT This example nuviguzione correct navigation Since preserve pattern use patterns knowledge semantically income originated activity structure detected improve domain information 2 reddito di words gestione e semantic reddito di correct attachment sentence context locally correct interesting general In Table 1 column shows total number detected associations It higher seen number syntactic semantic associations triples distancebased legal corpus pairs This sentences Many related words located coordinations distance higher 5 example Notice total number semantic associations syntactic associations semantic ambiguity associations despite explained nested prepositional fact collect higher fact result 68 R Basili et al I Articial Intelligence 85 1996 5999 shows column total number second associations clustered associations The based order Syntactic words significantly clusters navigazione reddito di gestione low es1 type The number obtained tag previous clustered distinct order example semantic different cooccurring semantic words clustering words clusters syntactic like reddito di clusters Distance A commonly criterion correct error prone detected frequency obviously correct mean perfectly times However patterns detected common wisdom error reliable accumulating suggests consider higher statistically reliable 3 corpus The A language pattern extremely term associations reliable rare repeat wrong pattern analyzers evidence originated observations rare language statistical noise preserve A crude meaningful information following measure reliably acquired methods given preserved information PI associations freq 3 associations times estimate associations methods allow preserve perform frequent words similar patterns We appear It seen column 4 Table 1 semantic initial word information income tendency analysis 97 This legal domain extended Fig frequency allows semantic Though clusters filtered posteriori relation LD The range clustering preserve 1 plots groups words different frequencies PI preserved figure shows 50 detected information low frequencies associations observations include noisy data 22 Acquisition coarse selectional patterns identify concept statistical In order ARIOSTO_LEX given syntactic More probability C2 pattern Csynt_relC2 synt_rel syntactic syntactic es extracted PROPERTYinNATURALOBJECT typical concept patterns significance formally measure cooccurrence shallow analyzer pattern sublanguage pair occurring computed classes C relations For pattern CPC synt_rel C defined C synt_relC computed conditioned plausibility Recently counts 2228 socalled smoothing techniques adopted reduce problem low R Basili et al I Artificial Intelligence 85 1996 5999 69 O5 510 1 O20 2050 50200 200 Freq Range Fig 1 preserved information CPC synt_rel C wEC1w2ECZ ps4w vnt_reh wz c C pslw synt_rel wz wwzany The numerator sum plausibility values esls type conceptual categories C C sum plausibility values esls type like popular mutual information 17 studies present synt_rel relate word pairs belonging The denominator synt_rel The example matters detect statistically relevant phenomena linguist measures CP reason xi yj represents Clustered esls build tables syntactic structure element corpus general statistical significance ized pattern Csynt_relCj All statistically prevailing couplings classes submitted according hisher linguist replaces synt_rel appropriate intuition conceptual relation A linguistic analysis tables useful illustrate advantages table shows method In follows distribution tables discussed domainsP given corpus Csynt_reZCj In illustrating groups similar conceptual patterns detected relation subsumed subsumes beneficiary followed HUMANENTITY Each associations propose interpretation preposition recipient case relation type conceptual 9 The tables list semantic tags Appendix A 70 R Basili et al I Artificial Intelligence 85 1996 5999 Table A1 summarizes relations CperC commercial corpus brevity CD follows Some pertinent articofi lo sport items sport uttrezzi associations l ARTEFACT ACT giardinaggio tools gardening l ARTEFACT BUILDING biuncheria la case linens house mobili negozi furnitures BUILDING 0 MACHINE shops mucchinuri Zuborutori equipment All subsume intuitive relations beneficiary relations laboratories mucine mulini grindstones mills Notice purpose relevant culzuture uomo Zudy HUMANENTITYHUMANENTITY hairdresser use relation senses preposition frequent hold ARTEFACT corpus lady shoes tools gardening benejkiury statistically The HUMANENTITY man biuncheriu Signora linens purrucchiere Signora The proposition relatively conventional use legal corpus seen Table A3 Examples frequent l ACT ACT pugure prestuzione cause relation relations pay job interpreted 0 ACT ABSTRACTION category interpreted ussegnure cutegoriu manner relation assign disintinguere quota distinguish l ACT rates AMOUNT interpreted In RSD Table A3 ACT NITIVE_PROCESS remotesensing ACT NATURALOBJECT data Orgeon analysis concerning COGNITIVE_PROCESS study university INSTRUMENTALITY CESS We spent time manner following relation uses preposition ABSTRACTION fo COGNITIVE_PROCESS DISCIPLINE interpreted COGNITIVE_PROCESS method evaluation technique purpose relation LOCATION INSTRUMENT frequent ACT COG analysis atmosphere underlying calculation satellite reference relation referring atmosphere INDIVIDUAL ORGANIZATION interpreted recipient relation ARTEFACT ACT COGNITIVE_PRO spectrometer analysis subsumes support lexicons remarkable intuitive general use prepositions restrictions computational selectional literature tables illustrating difference use relation results usually dictionaries patterns way words sublanguages relate Many study appears There domains generalize indicates asterisk The compound I Many examples sport items discussed XI literal translation In cases English translation R Basili et al I Artificial Intelligence 85 1996 5999 71 conceptual semantic earlier 23 To define set relations interpretation C In examples provided task associating synt_relCi pattern As remarked relation automated The syntactic pattern appropriate problem large consensus use relational notwithstanding models lexicon greatest disagreement number type sublanguages relations posited relied possible previous work semantic networks example relations frequently mentioned 44 We commonly agreed like agent object computational theme patient location purpose necessary like sublanguages data Oregon jigurative_location Overall 30 conceptual set relations sublanguages For domain prepared list correspondences concept triples syntactic pattern interpretations semantic compounds prepositions means manner lo30 different CRC highest number Csynt_relCj patterns conceptrelation instrument plus small set relations English relations use exactly straightforward include program specific word patterns CRC On average reference type There interpretation interpret literature linguistic relations In case consider task preparing CRC tables particularly relation sound appropriate relevant purpose demonstrating conceptual matters personal view relational model possibility automatically detecting generalized word patterns frequent given sublanguage sublanguages hisslher intuition language domain thesis paper The choice reader These patterns highly variable It difficult linguist identify In sections automatically detected conceptual patterns semantic bias acquisition semantic lexicon 23 Acquisition casebased lexicon The CRC semantic bias ARIOSTO_LEX The process acquiring word sense selectional For word W corpus sense W restrictions summarized follows 1 Select esls include Was arguments W data radar Oregon code The esls collected global RSD N_prep_Ndata satellite N_prep_Ndata V_Nanalyze data N_Vdata demonstrate N_prep_Ndata N_prep_Ndata plausibility value corpus 2 For esl given semantic tags assigned W cooccurring appropriate word given semantic type syntactic reject esl Put rejected patterns interpretations relation 72 R Basili et al I Artificial lntelligencr 85 1996 5999 data available code inspection linguist list R accepted list A Both lists Example existing CRC interpretations RSD MENTALOBJECT data classified WordNet satellite INSTRUMENTALITY preposition INSTRU SOURCE MENTALOB N_p_Ndatu interpretation data SOURCE satellite 3 esls interpreted CRC generalize create restriction corpus radar W word sense INSTRUMENTALITY dataSOURCE selectional following datuMENTAL_OBJECT satellite restriction data 4 For selectional restriction W SRW compute MENTALITY JECT satellite receives If new selectional Eg If data SOURCE acquired SOURCE acquired statistical The figures semantic expectation freqSR W freq W esls frequency SR restriction including W corpus including W freqW SESR W freqSR W given plausibilityweighted interpreted SESR W selectional phrases absolute frequency High values semantic expectation suggest word W particular succeed The semantic verbs verbal nouns including parse sentence order important semantic later interpretation algorithms guide parsing However verbs shallow expectations parsing case role filled expectation particularly thematic structure The certainty factor CFSR W restriction selectional sentence structure program unambiguous example competente submit competent authority submit authorityHUMAN_ENTITY short unambiguous words increase reliability acquired relations ARIOSTO_LEX sentence sentences analyzes Boolean measure CF set SRW set learned 0 For sottomettere allautorita doubt In valid pattern corpus intended final 1 NL MT ARIOSTO_LEX provides A postediting nice environment linguist section One problem partially handled stage The overgeneral derived lexicon The lexicon suggested ambiguity strongly TION especially verbs Therefore lexicon The problem complex This discriminating restricted different semantic interpretations power sublanguages tags possible discriminate bankBUILDING good semantic banklLOCA concrete nouns subtle ambiguities single entry verbs especially matter different capture senses verbs collapse sense disambiguation systematic See lo investigation R Basili et al I Artificial Intelligence 85 1996 5999 13 24 Linguistic analysis lexicon ARIOSTO_LEX acquires selectional restrictions content words corpus frequently generalize patterns usage step 3 However advantage empirical methods rely semantic model step 2 allows interpret patterns corpus This important In section brief account linguistic material produced domains CD LD RSD The reported data learning corpora 500000 words belonging processing acquired domains proposes restrictions selectional ARIOSTO_LEX linguist Four windows left list accepted selectional RSD presented restrictions Fig 2 shows lexical entry verb obtain relatively high frequency acquire respectively window 1 upper example window 2 middle left called limbo list rejected esls underlying window 3 upper based shallow conceptual relation collect noisy relations An NLP statistical environment The linguist modify finally accept selectional restrictions windows 1 2 For rejected pattern provided A pattern rejected CRC interpret CRC word es1 MOR classified TAX pattern lexicon In case linguist decide right window 4 lower left ARIOSTO_LEX techniques window 3 explanation output optimally morphologic provided list accepted review esls tune Fig 2 ARIOSTO_LEX Screendump validation session entry to_obtain 74 R Basili et ul I Arterial intelligence 85 1996 5999 CRC morphologic update environment Heshe browse corpus database esl lexicon taxonomy Each acquired selectional restriction represented follows pre_sem_lexword conceptual_relation semantic tag direction es1 type preposition SYNE SE CF The arguments relation conceptual identify selectional restriction direction obtain PURPOSE COGNITIVE_PROCESS variance computed order obtain obtain INSTRUMENT INSTRUMENTALITY obtain image satellite SYNE syntactic expectation SE CF described earlier In form lexical entry sparse syntactic statistical central right window Fig 2 opened lexical data shown The selectional different syntactic patterns However demand provides compact semantic representation entry restriction generated conceptual graph Fig 2 good summary recurrent verb belong expected INDIVIDUAL ORGANIZATION findings research The verb technical use The category This pattern window 2 line limbo CO following analysis obtained results obtain RSD like frequent subject HUMANENTITY Commonly COGNITIVE_PROCESS INST concrete nouns ARTEFACT NO antenna obtains These patterns NATURALOBJECT use deduced standard dictionary definitions For example Websters dictionary gives following definition verb obtain 1 To gain possession acquire 2 To widely acceptable subjects words belonging abstract categories INSTRUMENTALITY appears ART observe Fig 2 SE values relatively It interesting low The highest expectation In fact sentences verb obtain RSD specify instrument obtain information artifact obtain image satellite As defined SE 1 given selectional restriction word including word But cases SE 1 plausibility 1 sentence instrument relation seen Fig 2 associated definition Its corpus preposition follows straightforwardly SE probability word conceptual relation expressed syntactic relation represented es1 type R Basili et al I Artificial Intelligence 85 1996 5999 75 different like associate rare Even verbs preposition semantic relate prepositional modifier belonging relate categories For example relate datuMENTAL_OBJECT SE values higher turbulencePROCESS We experimentally 05 indicate highly expected RSD values rare Strong semantic expectations conveyed restricted number verbs tendency appear similar contexts identical expressions Verbs strong expectations stereotypical commercial domain adopts telegraphic frequent style In contrast highest syntactic ambiguity sentences legal domain SE values lower plausibility values low style technical determined relations Another observation emerging analysis Fig 2 relational patterns words highly variable despite fact highlevel semantic categories inherent limit detection finetuned relations Table 2 shows general validity finding The table lists LD restric average number detected classes according frequency selectional Italian RSD English tions verbs Verbs grouped corpus Though restriction number detected selectional restrictions lower total number different detected fewer examples verb context selectional 80 frequency range domains This class lower frequency verbs average number different populated possible gain relations attached evidence patterns use sublanguage verb 30 It remarkable obviously types The data Table 2 provide experimental difficulty defining manually automatically identification common relational patterns verbs evidence justify wellknown verb taxonomy based Interesting matter linguistic analysis emerges comparison sublanguages Many verbs exhibit completely different patterns use For example domains occurs different contexts verb produrre produce relatively frequent In RSD example Table 2 Average number selectional restrictions verb sublanguages Verb frequency ranges Legal domain Remote sensing domain Average relations verb different detected relations Average relations verb different detected relations XC10 lOxlOO xlOO 376 98 27 81 85 85 64 179 352 76 80 80 R Basili et al I Artificial Intelligence 85 1996 5999 ORGANIZATION produce ABSTRACTION MENTALOBJECT INSTRUMENT produce ABSTRACTION MENTALOBJECT 76 NASAORGANIZATION produced imageMENTALOBJECT satelliteINSTRUMENTALITY high accuracy produced daMENTALOBJECT In CD ORGANIZATION INSTRUMENTALITY produce ARTEFACT INSTRUMENTALITY la ditta produce articoli pelle macchinari propri companyiORGANIZATION produces owned machineryINSTRUMENTALITY itemsARTEFACT leather LD ORGANIZATION produce DOCUMENT la societb deve produrre attestato companyORGANIZATION produce form DOCUMENT Once appears word patterns generalize sublanguages Often words narrower unintuitive sense suggested dictionaries common sense Finally data provide insight problem relating conceptual window 4 Fig 2 interpretation roles syntactic structures Notice example syntactic patterns different instrument semantic Another example In RSD typical conceptual pattern provided verb produce producel INSTRUMENT INSTRUMENTALITY 1 OBJ MENTALOBJECT MANNER PROPERTY satellite produces image high accuracy R Basili et al I Artificial Intelligence 85 1996 5999 77 CD typical pattern producel AGENT ORGANIZATION OBJ BYPRODUCT INSTRUMENT INSTRUMENTALITY la ditta produce vino macchinari propri company produces wine owned machinery linguistic styles relation relations syntactic subsume produce This example different These examples semantic ery syntactic relation machinery different semantic interpretations instrument satellite produces versus produces machin accuracy versus produce manner instrument generaliz ing word patterns similarity cause problems Syntactic similarity systematic marker semantic similarity This motivates choice conceptual relations price additional human In demonstrates section reported basis syntactic variety commented labour phenomena languages Our findings summarized follows recurrent different technical domains patterns use deduced 0 Word uses different sublanguages Often standard dictionary definitions reliable marker semantic similarity 0 Syntactic similarity l The case structure words especially verbs difficult identify strongly expected highly variegated poorly overlapping Furthermore patterns use verb statement limitations The inherent verbs effectiveness expectationdriven applications world example problematic One consequence possibility defining language independent ontology imposed outside limited levels Another consequence interpreters semantic lowest strong semantic expectations NL interfaces databases 3 Performance evaluation In previous section analyzed results ARIOSTO_LEX linguistic impact findings commonly NLP grounds discussed techniques This section provides quantitative evaluation ARIOSTO_LEX There issues related task 7x R Basili et 01 Artificial Intelligence 85 1996 59W 0 l The problem lexicon evaluation termined The second computational lexical learning linguistics fully adequate evaluation frameworks theoretically underde adopted area measure complexity attention evaluation Furthermore final output retaining As issue notice literature effective evaluation mechanisms lexicons Frequently paid problem making lexicon consistent provably correct agree Yorick Wilks lexicons inconsistent proved consistent impossible happen comparative lexical component NLP alternated alternative compare rest constant3 evaluation lexicon confused Finally believe evaluation international TREC different systems use given technology compared common linguistic components NLPbased However difficult example sentence misunderstanding originated lexicon deductive component grammar like MUC Message Understanding Conference task language understanding Text Retrieval Conference lexicon For example establish conferences uses section Therefore possible ways attack problem The selfstanding knowl limited task human pursue edge base lexical facts evaluate specific intuitive relatively consider ARIOSTO_LEX task We intuitive performances easily judge decision evaluate ARIOSTO_LEX experiment The second comparative time different application example information extraction In second case motivations possibility performing stated lexicon compare Rather propose dynamic adaptable tuning performance corpus This second experiment setup design effort hopefully able carry cooperation research sites larger project As far evaluation popular evaluation parameters systems statistically based syntactic analyzers classifiers accuracy Accuracy measures decisions global number automatic decisions inductive IR systems pattern efficiency recall precision number correct decisions number expected final definition concerned number correct decisions test cases Precision requires experimental NLP literature stages lexical performance referred compare framework Wilks personal We communication January 19Y5 debt Yorick Wilks intuitions problem R Basili et al I Artificial Intelligence 85 1996 5999 79 As observed different authors provide good measure quality different sensitive complexity decision tusk hand 34 accuracy precision inductive systems This criticism intuitively understood following example Say n classes The wish assign instances observed phenomenon blind probability assign correct class absence decision strategy ln Clearly inductive selects correct class n 80 precision fact better job exhibits 90 precision selecting n2 classes The absence notion blind probability called 34 priori prior evaluation classification methods presented differ In section shown information probability renders ent authors hard compare gain adequate precision In follows perform systematic evaluation ARIOSTO_LEX task syntactic disambiguation like recall takes account complexity learning task hand limited popular evaluation parameters The problem analyze numerically effects morphology tags provides good interpretative coarse classes create unacceptable semantic associations cumulative interaction polysemy Though obviously linguist refine CRC add new syntactic rule revise classification choices adopted problem overgenerality initial noise reduced lexical acquisition model use highlevel In fact clustering word syntactic ambiguity parsing errors noise inherent power This evaluation concerning aspect ARIOSTO_LEX necessary advantages First available syntactic analyzer15 Second disambiguation methodspresented tion data available developed computational Section 4 literature complete NLP results compared evalua literature lexicons developed 31 Measuring information gain inductive linguistic methodr In modeling section task favours definition principled uniform notion classification performance framework different disambiguation methods allows systematic study performances terms information gain PP attachment unified view This evaluation provides problem However linguistic nature classification problem hand renders complex prior probability formal definition prior standard classification given class evaluated number available training instances posterior probability tasks For example I5 This easy matter developed shallow grammar legal language produced complete parse trees 400 complex sentences LD x0 I Basilr et al I Artificial Intelligence 85 1996 59W class case lexical noisy unsupervised 34 Furthermore learning In section devise formal definition set instances instead test set relatively acquisition small assessed triggered In disambiguation syntactic semantic evaluation method task The definition applied disambiguation methods precisely expectation easily extended computed First syntactic disambiguation task Any sample parse tree forest ys set sentences ts A given grammar In general ys parse includes trees Furthermore G class correct s generated language deficiency user meaningful E 2 characterized r provides sentence ts correct parse ys partitioned express class wrong trees adopted grammatical model training information corpus information popular ARIOSTO_LEX lexical modelled learning gain PP 31 The evaluation lexical association tasks classification underlying s E 2 related ts E ys classes disjoint assigned structure meaningless More generally available sentences 2 sample C Us ys derived trees partitioned disjoint GtRl3s tst ERG parses space classes Given universe R ys syntactic reduced method universe classifying members disambiguation given positionnoun disambiguation space structures elementary syntactic substructures types syntactic J2 Simpler correct classification generated syntactic wrong sentence structure formalized relations analysis sentence s equivalent set E G The evaluation related evaluation syntactic classifier Note respect trees modelled subjectverb source sentences nounpre class correct wrong ones The notion correctness section corpus 34 classifiers linguistics produce community probability In words corpus driven disambiguation instances selecting methods class assign confidence Most disambiguation methods categorical ambiguous considered competing distribution instance factors correctness competing example structures related example candidates interpretations interpretations In 31 factors seen probabilities Decisions generally significantly confident sentences ambiguous undertaken like VN_prep_N watching girls binoculars readings competing 1 2 distribution girlswithbinoculars towatchwithbinocular computed confidence 1 Y 2 Y normalize factor Y CY respectively cy obtaining Given probabili R Basili et al I Artificial Intelligence 85 1996 5999 81 ty distribution confidence assigns following statements correct reading denote P 1 E G P2 E G 2 P2 P1 1 Pl P2 simply 1 2 In absence lexical distributional knowledge towatch girl uninformed classifier assign 1 binocular blind confidence 2 simply P 1 E G P2 E G 05 respectively given syntactic structure With notation P P represent posterior probability tion perform P P Pt E G expected correct structures t 2 example Vice versa Pt E E expected structures classification PE 1 PG Before defining PG acterize The prior blind correct A lexical acquisi increase t t 1 example lower wrong ones increase wrong let char binary PE 1 PG PG formally information gain lexical learning framework information gain inductive t task defined number bits necessary Our definition information information score follows closely general definition provided theory correct PP reduction average classificationdisambiguation disambiguation example problem 34 Definition 1 Information score Given sample space 0 classifier performs correct PG PG t E G PE PE t E E t information score I useful classification Z log PG log PG correct classification E Z log PE log PE If corresponding PE PE w h en t E E magnitude decision misleading PG PG t E G score I penalty information logl PG PG wrong classification E logl PE log1 PE information score Z log 1 PG log 1 PG Z log1 PE log1 PE The overall performance scores Z testing cases averaged cardinality 12 I index I test set 2 sum information 83 R Basili et ul I Artificial Intelligence 85 19 59W The important aspect definition assigns inductive step score positive classification correct complexity task high penalty strong classification incorrect complexity task low 32 Syntactic disambiguation classification task In order algorithm probability perspective probability confidence confidence syntactic evaluate information gain given lexical acquisition PE necessary previous provides PG linguistic syntactic syntactic disambiguation tree structure probability gained virtue model test set carefully model called prior specific prior blind source corpus task hand structure Correspondingly necessarily correctness posterior simply sections In information structure semantics constituents 321 Modelling prior probability space In order 1 define sample probabilities tion methods In corpus sublanguage model disambiguation following define suitable set syntactic notions phenomena events actually prior posterior probability observed observed 3 determine 2 establish prior events sample 4 interpret lexical disambigua 31 posterior linguistics _Y analyzing use 2 The majority common reference sample extrapolate space properties corpus We Ce embodies syntactic methods automatic section rely evaluation concern space probabilities practice hypothesis Hl corpus The observe u phenomenon rely assertion meaningful semantically plausible related language Y Vice versa rare observations markers inconsistency noise observation phase structures complete observed The literature syntactic simpler subjectverbobject note In order structures evaluate R observe If ESSs denotes phenomena majority word collocations augmented observed vary significantly parse trees For example SVO verbprepositional methods syntactic markers The proposed general type like VprepN We triples productive modifier elementary syntactic structures esss different methods rely set structures corpus This set global sample universe set elementary structures syntactic Though general confusing wc USC surface syntactic structures sense includes SVO esl defined literature term ess indicate surface structures R Basili et al I Artificial Intelligence 85 1996 5999 83 observed actually observed ARIOSTO sentences corpus n UV ESSs 1 different sentences generate ess However multiple Note 0 indexed source versions ess different fact local property ess depends sentence s Correctness ess classified context Therefore multiple occurrences separately Note 1 locates performance evaluation methods sample space produced underlying grammatical model The global ambiguity necessary different disambiguation methods compared applies uniformly function adopted grammar This restriction As consequence 1 overall set correct esss denote G G 0 union correct esss sentence s GU G SE 2 set wrong G set correct esss sentence s Clearly locally meaningless syntactic structures E simply given E U ESSs G 0 G SE 3 As example consider corpus restricted single sentence sl acquires data satellite It follows fi ESs toacquire ii toacquire data iii toacquire satellite iv data satellite G ii iii E iv Note classification local 1 In fact sentence program processes data satellite iv belong class G correct esss In order derive prior probability different esss simply count number correct esss cardinality sample universe 0 sentences real corpus This process replicated In fact simply know noisy unsupervised priori esss correct learning performed modelling prior probability simply solution average number correct esss sentence Each sentence reading sentence Trivial collision sets single iii elementary A better approximate produces collision sets belong nonambiguous In sl iv form nontrivial collision set For sentence s ESs groups structures syntactic structures partitioned structures 84 R Basili et al I Artificial Intelligence 85 1996 5999 elements set collision ESSsp sets denoted following collision sets ESSslp In example Cl acquires C2 ii acquires data C3 iii iv acquires f ram satellite data satellite Let model prior probability PG correct ess follows c PG PG Sifw PGT probability This value dependent s elementary collision 4 syntactic sets generated structures locally correct sentence Let point general collision set This hypothesis allows correct following definition ess E GY PG PG IESSslp s ESWj collision sets esss 5 In example PG 314 blind ess correct given s Correspondingly probability elementary structure PE 1 PG 6 The definition sentences disjoint PG clearly follows observation esss different ESs n ESSs 8 ks zs Thus PG 10 PG derived PG follows PG rzq PGVs 16 I j z PG 7 322 Modelling posterior probability The posterior probability probability class assigned trained driven methods corpus driven score methods express raw texts extensive In order automatic collections test instances esss sentence disambiguation assign patterns use information gain performance data syntactic appropriate form preference index scores G ES s All probability extracted p collision indicate equivalence sets fact elements 24 91 relation holds conflicting syntactic quotient set ESSsp More details structures relation The p R Basili et al I Artificial Intelligence 85 1996 5999 85 defined method The problem syntactic disambiguation operator essential evaluation information gain literature probabilistic flavour probability Yet Given disambiguation preference obey following condition function cr score posterior probability PL derived according probability correctness 20 P ess E G 1 toll 1 Vcoll E ESSslp calf denotes collision set sentence s 8 partition ESs In fact global probability correct given collision set co11 collision set colless syntactic set scores provide redistribution preference members sentence s given 1 Note ESSslp given ess belongs disambiguation For reason probability unique Within collision given ess correct Pbess E G c Pbess E G 1 collPbcoll 1 s colIEESSslp Pbess E G 1 coUessPbcolless s 9 ess belongs collision set colless Pbess E G I toll 0 Vcoll colless A definition Pbess E G I coUess applies disambiguation score following Pbess E G I coZless cess 2 cess essEcolless Pbcolless includes correct ess implying Is probability colZess correct 10 sentence Pbcolless I s 1 For example following scores example let assume hypothetical 1 puted disambiguation score v com 11 crto_ucquire satellite 05 adutu satellite 01 Using 9 lo ll posterior probabilities computed follows PXWew toacquire E G crsystem acquire PCl I s 1 v asystem toacquire Pbto_ucquire dutu E G cto acquire data PC2 I S 1 y 7 ato_ucquiri data X6 R Basili et al I Artificial Intelligence 85 1996 5999 Pto_acquire satellite E G ato_ucquire satellite oto_acquire satellite adutu satellite PC3 I s 05 os o 1 083 Pdutu satellite E G adutu satellite adutu satellite adutu satellite PC3 s oJol EO17 In example P Pess E G Cl C2 C31CII IC2 IC3 075 signs role information P easily esSs following seen Note implies scores gain measure improvement probability prior The distribution P Pto_ucqztire satellite 083 P data satehire 017 As result decisions C following values obtained useful PC PC correct classification I logPiii E G logPiii E G logO83017 23 I logPiv E E logPiv E E logO83017 23 notion 10 9 appropriate lexical association LA modelled Clearly 9 learning applicability 10 algorithms 11 allow model disumbiguution method given collision set defined In following evaluate Section 23 31 terms probability semantic expectation SE distributions sets training instances framework 11 easily extended limited methods corpusbased language 33 Posterior probability bused lexical association lexical association LA classical preference tscore measure The extension sentence frameworks like verb dir_obj prep noun introduced score 48 LA 31 ambiguous 12 correct select significantly positive noun negative referent PP constituent given opposite preference suggests verb attachment prep noun attachment If score verb prep dir_obj prep noun R Basili et al I Artificial Intelligence 85 1996 5999 87 To summarize recall definition models syntactic preference sentences like 12 The LA value defined 31 LAverb dir_obj prep log P prep verbPNULL dir_obj P prep 1 dir_obj 13 PNULL 1 dir_obj dir_obj extend like 1218 In order test set model sentence structures probability observing prepositional modifier frame sentences ambiguous structures In form 13 useful ambiguous coverage like word prep word prep word prep word 14 definition readings chains prepositional modifiers Since cases alternative referents use single LA value rise 13 express modified preference According referent follows This preference score slightly different LA fits experimental purposes denoted oLra When referent post modifier preference score follows philosophy denominator closest word prep word word3 13 qword prep Pprep 1 word score given numerator 13 awordi prep Pprep I wordPNULL I word 3 15 16 word preferred mation LA appropriate preference derived follows use 10 fully defined score referent 14 This definition close approxi evaluation purposes When associated posterior probability Definition 2 Posterior probability distributions based lexical association Given sentence posterior probability based LA PLessw essw p w2 E ESs p w2 E G given s syntactic structure PLAess E G PLess E G collessPcolZess s 17 colless actual collision set essw p wz _ stands word I8 The authors derive Pprep 1 verb PNULL source corpus manually validated dir_obj Pprep 1 dir_obj partial parses 88 R Basili et al I Artificial Intelligence g5 1996 5999 34 Posterior probability based semantic expectation The disambiguation method based semantic expectation lexicon acquired ARIOSTO_LEX casebased according semantic expectation typical lexical entry acquired corpus guided Syntactic relations validated lexicon In Section 24 described pre_sem_lexword conceptual_relation semantic_class direction esl_type prep SYNT SE CF 18 The semantic expectation SE probability word word belonging Section 24 In following denote semantic_class occurs conceptual_relation corpus SE factor 18 SEword prep conceptual_relation semantictag 19 Given sentence s collision set cofless derived ambiguous PP semantic defined 14 elementary score based cofless preference syntactic 12 structure referent expectation follows SEess word prep word IIK ki j SEword prep conc_rel C SEword prep cone_rel C 1 20 essword prep word E cofless C set supertypes semantic tags word Note algorithm ceptual relations ess according generalizations given ess SE 0 20 considers interpretations preposition prep possible interpretation word word Clearly semantic According correspondingly derived lo posterior probability associated SE score pssw prep Wj E G Pesswi prep w E G 1 collessPkcolless 1 s SEessw prep w zzz c SEess rcoes 21 coUess collision set essword prep word 35 Evaluation results We selected test set evaluated accuracy information score derived LA SE disambiguation phase R Basili et al I Artificial Intelligence 85 19 5999 89 VN experiments The test corpus evaluation nounprepositionnoun Italian The Italian grammar adopted trees sentence Sentences belonged including 103 rules In 7871 parse trees 4117 elementary 2 7871 parse trees derived legal test definite clause syntactic syntactic structures verb includes 232 complex sentences average 33 parse domain grammar structures detected The classes elementary following N_prep_N prepositionnoun Vnil verbNULL class E G defined collision sets sentence human judges The number correct according average 13 correct trees sentence number sentences 1540 The links repetitions number correct elementary prior probability ess correct 5 7 066 Fig 3 provides examples sentence brackets related indicate represents ambiguous PP Italic characters collision set Bold characters words compete The test set manually validated learning corpus according nounverb N nil nounNULL judges parses approximates fragment sentence verbnoun NV originates segment fragments sets The syntactic enclosed collision VprepN indicate Examples Simple Collision aet8 Minimal Attachment 1 11 su richiasta de1 ministm le finanze il setiio di tigiJatzs sulle aziendej di cradito service o catrol agencies credit controlla I esattezza delle attestazioni contenute nel cmtificato aI_N_pp_Zidadicreditol0945 maasN_prap_riianzadicreditol00001 measg_Ngrep_Nl6serviziodcrdifol000061 QgeCyOfCEdlL Woncrolofcredit serviceofcredit NonMinimal Attachment 12 sostituti d imposta davono presentare pagamenti fatti e agli utili distribuiti nail anno 1974 l ntm il 15_aprilc_l975 1 present tk declaratton comma 4th item 9 wJatimJy tk payment tk pxfit distributed tk year 1974 aprJJ 15 1974 la dichinrazione di_cui_a quart0 comms dell articolo 9 relatizwmmte ai measllgrep17articolosntrox_l5_aprilel975100001 measLg_Vrep_Nl7distrikireenCrox_l5_aprile_l97Sl10166l moaagdvqrep_Nllrelativamentetrox_l5_april_l975l100001 measllgrep_N119canmaentrox_lS_aprile_l97Sll0000l measg_Ngrep22dichiaaioneentrox_15_apri1e_1975110107 measIg__rep_N24presenLareentrox_l5_apr_710166 2 ComplexCollision Sat ic non singletons memben iCemwihCipril_l5ch ta_ditTibutewithin_April_l5th Oreativelywithinpril_l5th commawithinApril_l5th Odeclarationvithipi_h t_pT8etwithiRpril_l5th gJi organi de1 hibutario possono dicbiarare non dovute le pena pecuniarie quando la violazione obiettive cmdizimi di inurtezza aulia portata sull ambito justtpd scope ambit di applicazione delle disposizioni alle quali si riferisce giustftita objective omditions unurtsinty da measg_Nrep_N4codirionesuportatag_Nqrep_N7cdizionesuambito100121 measlg_V_prep_Nl7gustificaresuportatag_V_prep_NllOgiustificaresubitol0000 coditioonscope coditioonat to_justifyonscope to_3ustifyonambit ________________________________ _____ ________ Fig 3 Examples collision sets test set l9 In corpora slightly different values 060 domain ecological newspaper articles 057 RSD English 90 R Basili et ul I Artificial lntelligmcr 85 1996 5999 PP attachment provided reported A wordbyword translation segment For competing set semantic expectation English es1 collision Example 11 straightforward example esls different esls 2 5 collision types Notice 12 example Adi_prepN expectation distribute April 15th The distance words In case simple heuristic cooccurring words colliding collide esls NprepN set 12 select argument nearest es1 fact In type VprepN pair words Finally including interpretations example coordination represented pairs esls 2 shows collision sets created prepositional phrase scope ambit In case alternative To run experiment word senses We use limbo 18 These pre_sem_fex Furthermore Table 3 The test set row shows information acquired ARIOSTO_LEX lexicon 961 lexicon selectional data incorporated supervised human learning restrictions analyst See corpus score Ix obtained disambigua collision information complex sets Both methods gain To comparison perform shown 34 information classification tasks prior probability class higher 45 supervised learning algo tion methods good gain probable rithms groups sentences number shown 1 test set Though complexity values 90 90 Rather increasing linguistic dependencies domains PP disambiguation experimenting growing dimensions set We obtained corpus patterns learning learning It table Z values In fact observe sensitive trees sensitive Z changes 2 30 30 methods dimension Ix sensitive growing complexity performances 200000 500000 words This intuitive methods future perform objective gain evidence better observe closely fixed linguistic different possibly million words domains Table 3 Experimental results information score versus accuracy I csss II correct Accuracy I averaged ess ess sentences SE 0203 0748 hXhh 033 LA 0174 0673 614r 070 R Basili et al I Artificial Intelligence 85 1996 5999 91 Row 3 provides accuracy methods selecting correct ess low literature collision set according The reader notice simple maximum likelihood method accuracy values relatively result order anaphora figures reported phrases coordination analysis Then analyzed compared performance learning set 500000 words reason In experiment considered methods Hindle Rooths example report 781 accuracy paper The reason millions words score LA SE provides model Second preference adjectival adverbial PP referents ambiguity phenomena higher accuracy values Suitable models phenomena LA method tested V N Another important types ambiguity prep N sentences including multiple prepositional examples Fig 3 Our point wish test real cases V N prep N phrases small fragment possible ambiguities addition necessary extract hand phrases sentences corpus build test set Rather let DCG grammar run corpus retained sentences grammar successfully complete sentences hand rejected sentences judged grammar produce complete set parses marked semantically correct parse 232 sentences This task facilitated graphic interface When remaining results produced disambiguator set noticed analyzed gained colliding esls allow reliable choice Many eels close SE value like to_distribute april distribuire topresent upril second example Fig 3 In entro uprile example choice performed nearest best basis When values close use likelihood method Many methods use threshold maximum prevented ly observed choices Once think accuracy reliable Pesl E G probable preceding number useful adequate measure A approach retain ambiguous eels Table 3 sake uniformity taking unreliable decisions However experimental criterion significantly values cases confidence esl We accuracy crudely emo uprile PG presenture higher literature selecting reduce Given complexity task happy 687 accuracy tags lexicon 074 information gain Though induce combined effect probabilistic semantic filters produces acceptable performances Consider testing conditions particularly test set included learning set limbo lexicon consulted accept erroneous overgeneralized syntactic structure severe 2o Since methods forced choice accuracy precision 92 K Basili Ed ul Artificial Intelligence 85 1996 5999 This means necessary Finally acquired accept syntactic structure selectional fact manually restrictions verified cause errors performance evaluation filtering wrong parses tion accepted patterns But semantic methods information proposed lexicon literature consider ARIOSTO_LEX semantic interpretu quality testing complex matter provides remarked far 4 Related research In introduction provided general account corpusbased methods section summarize lexicon closely presented linguistics design For sake completeness computational paper recently groups unrestricted MRDs semantic corpora based MRDs engaged lexicon MRC computational literature related Several research dictionaries tive acquiring readable lexicon taxonomic payment acquisition Usually methods structural In MRD imposed persons groups governmental support online acquisition like example automatic words acquired features majority dictionary LDOCE acquisition patterns objec challenging NLP machine sources Among 14213741 tax u senses verbs syntactic Poznanski desirable templates word LDOCE generated Sanfilippo 8 verbs presents proposed advantage taking genus alternative assisted takes NP NP use restricted grammar The information reliably extracted genus semantic information For example word included definitions taxonomy However 42 remark cause verbs classes The paper genus 20 verb senses different fact belong method requires process correlate word MRDs constant definition grammar interaction linguist Methods words example LDOCE definitions 1131 simplified semantic senses dictionary lexical In 24 ULTRA language associated content word like MRD natural information semantic constraints semantic described processing heuristic extracting lexical entries techniques Basically list pragmatic entitybank41 class countable institution abstract object economics banking This extracted information entered manually Even largely human work automatically acquiring dictionary lexical entries relies 26 method R Basili et al I Artificial Intelligence 85 1996 5999 93 None aforementioned papers provide performance evaluation tools conceived primarily help lexicographers Very papers build semantic lexicon corpora source word In 45 verbal case frames acquired bilingual corpora sense information An example buy agent HUMAN object CONCRETE ABSTRACT HUMAN Semantic tags HUMAN syntactic relations selected online thesaurus case labels detected prepositions agent subject object In cases syntactic semantic ambiguities solved comparing reliable results languages Though promising limited coverage bilingual feature descriptions obtained 16 verbs approach produces One generally important problem MRDbased restrictions dictionaries definitions practice easytounderstand niques simple selectional formation semantic knowledge selectional patterns lexicons think point research MRD MRC integrated purpose automatic NLP Structural types information relevant include structural computational taxonomic tech useful type lexical acquisition deeper 5 Conclusions future work The thesis paper demonstrate combining empirical major advantages possible combine rationalist methods radically different approaches We presented ARIOSTO_LEX scaling digging deep twostep algorithm tool selectional analysis complex corpora ARIOSTO_LEX restrictions tool provides acquisition readable compact comparative analysis sublanguages domain appropriate utility selfstanding form linguistic data amenable systematic lexical categories conceived tool lexicographic studies primarily computational NLP strong commitment designer final application The information acquired ARIOSTO_LEX selected corpora For lexical entry ARIOSTO_LEX provides account situations given domain expressed clearly desirable selectional features semantic lexicon existing industrial research NLP projects demonstrate lexical data portability In particular ARIOSTO_LEX dynamic knowledge base lexical facts respect coverage languages domains word participate like verbs ARIOSTO_LEX restrictions This adequate different information type Yl R Basili rt ul Artificial Intelligence 85 1996 5999 lexical profitably tuning existing lexicon adding domain specific existing case relations tool needs stressed grand computational challenge systematic reliable linguistic knowledge acquisition large scale include lexical acquisition major intelligent applications lexicon driven natural language processing moment items The potential In paper performed In addition overwhelming produced ARIOSTO_LEX systematic study sublanguages gained experimental impact applicability popular approaches processing detailed analysis lexical entries linguistic material evidence language automatic l The relational structure verbs highly variable poorly overlapping invariants structures type hierarchy Finding common task inherent limitations l Most verbs impose weak expectations argument impact validity expectation structures This driven finding problematic semantic interpreters l The relational structure words varies significantly sublanguages lexicon design inappropriate General purpose approaches lexicon automatic language processors selectional hand ARIOSTO_LEX task limitation acquires merits limitations The merit extensively limited manual cost useful type semantic knowledge restrictions generalize sublan We demonstrated time guages acquiring consuming The patterns selectional power On hand refined conceptual lexical acquisition The performance beg question automatic section demonstrated general generality semantic lexicon good despite selectional adequate types evaluation discriminating power high case provide unintuitive conceptual generalize types level Another limitation overgeneral view senses verbs classified In fact discriminating nouns highlevel categories good ambiguous categories These categories sufficient discriminate subtle highly polysem ous senses verbs Hence different senses verb collapsed unique CIAULA classifies different algorithm expected problematic relational variable advantages refined verb classification clear We believe insight needed 61011 presented corpus driven conceptual clustering method word classification categories polysemous words The results verbs highly fully structure Because inherent difficulty lexical entry This issue explored complex matter ARIOSTO_LEX bushy Finally open issue extensive onfield evaluation ARIOSTO_LEX We proposed formal evaluation concerned syntactic tags verb polysemy R Basili et al I Artificial Intelligence 85 1996 5999 95 disambiguation appropriate pretative power acquired selectional application evaluation ARIOSTO_LEX consider inter restrictions NLPbased In Section 3 pointed difficulties substantial computational lexicons general evaluation Appendix A Cisynt_relCj tables Legenda21 CD Table A 1 INDIVIDUAL ORGANIZATION customer plough organize table PA PHYSICALACT MA MENTALACT ART ARTEFACT HE HUMANENTITY V VEGETABLE B BUILDING BP BYPRODUCT MT MATTER AN ANIMAL MC MACHINE P PLACE iron cow corn greenhouse milk INSTRUMENTALITY grindstone LOCATION beach Table A2 A ACT enclose RE REALESTATE G GOODS table greenhouse Table A1 CperC table commercial domain CD PA MA ART HE V B BP MT AN MC P PA MA ART HE V B BP MT AN MC P 0121 0041 0094 0016 0003 0061 0013 0027 0000 0032 0004 0075 0054 0068 0024 0001 0030 0004 0008 0001 0036 0004 0031 0023 0045 0002 0002 0012 0006 0011 0010 0000 0022 0018 0026 0024 0000 0010 0001 0001 0002 0000 0000 0000 0001 0000 0000 0000 0001 0039 0023 0057 0014 0000 0013 0004 0015 0000 0033 0001 0002 0001 0005 0000 0000 0001 0002 0001 0002 0000 0005 0001 0000 0001 0002 0001 0000 0001 0000 0001 0010 0005 0011 0001 0001 0001 0002 0003 0004 _ 0004 0003 0004 0001 0000 0001 0001 0001 0000 0001 0000 ARIOSTO implemented Italian domains automatic tagger later RSD domain This explains available WordNet WordNet labels When labels identify exactly Italian domains tag labels correspond label brackets corresponding WordNet category tags adopted 96 R Busili ct ul I Artificial Intrlligence X5 1996 5YYY Table A3 CerrC table legal domain LD A RE G AM D ABS TE HE P s A 0211 0004 0012 l1033 0033 I030 001 I 0036 0002 1048 RE 0020 0002 0003 0004 0001 0002 0010 0001 0003 G AM D ABS TE HE P S 0021 OOOI I002 000x 0001 000 000I 0005 OOOY 0071 0001 0007 0016 0017 I010 01lO3 0007 OOll 0018 0031 0001 0007 0004 OOOH 1100s 0 001 0006 0006 0117 0003 0001 0016 011s 0018 0005 0015 0001 0025 0055 0002 0003 0012 0007 0010 0002 0004 0020 0036 0001 OlOl 0006 0010 001 1 O002 0006 0001 0010 0007 0002 0001 0001 0001 0001 0019 _ 0001 0004 0005 _ 0001 0001 0006 income contract AM AMOUNT D DOCUMENT ABS ABSTRACTION TE TEMPORALENTITY HE HUMANENTITY P PLACE S STATUS LOCATION obligarion definitinn year INDIVIDUAL beach ORGANIZATION company RSD Table A3 l DS SCIENTIFIC DISCIPLINE l CO COGNITIVE l ART ARTIFACT l TE TEMPORAL l ORG ORGANIZATION l INST INSTRUMENTALITY l LOC LOCATION PROCESS image ENTITY Oregon geography evalztuziion day month SOCIAL GROUP satellite university Table A3 forC table remote sensing domain RSD DS ART I ORG INSI LO PR ND NO ABS ATrR Act MO MT PS 111111 0002 0002 0 on2 11002 1l015 u021 II042 UU3U 0092 u041 11nu4 1lU11 II uu4 0 01 I flrl lcil6 0 on2 UW 0052 u004 0m no01 ollU2 110I12 I lull KU06 11002 0 onz I009 UH5 lUO 0 rnll I002 0 IWH IJW 0 UO2 uuo7 0002 0012 ouu3 11111 I OU7 0 023 0 I5 IUlll UKlh ouo3 001 I UOO 0m 0001 unw unu2 1JOUl u011 1l013 uo15 0w U004 onot 0 0115 1l005 0 014 OlKl7 U016 UUII l1Ull2 lIUUI U010 11OU5 UuU7 0012 0 on5 0 uox 0013 0027 IH13 UXl 0 OnY 11103 uuu7 0n10 I006 Ulllh 1l0117 I01 OHJl 0002 OlM ns CO AR1 TE ORG INS7 LOC PR IND NO ABS AllR Ad MO MT PS R Basili et al I Artificial Intelligence 85 1996 5999 97 researcher chief l PR PROPERTY emissivity MT MATTER iron l IND INDIVIDUALS l NO NATURAL OBJECT mountain sea l ABS ABSTRACTION l AITR ATTRIBUTE l Act ACT HUMAN ACT manage l MO MENTAL OBJECT idea project l PS PROCESS NATURAL EVENT earthquake data model smooth raw References l H Alshawi Qualitative quantitative designs speech balancing act language Proceedings ACL Workshop Las translation combining symbolic statistical approaches Cruces NM 1994 2 R Basili MH Candito MT Pazienza P Velardi Evaluating probabilitybased PPdisambiguation methods Methods Language Processing Manchester information gain Proceedings Znternational Conference New 1994 3 R Basili F Grisoli MT Pazienza Might semantic lexicon support hypertextual authoring Proceedings Applied Natural Language Processing Stuttgart 1994 4 R Basili A Marziali MT Pazienza Modelling syntax uncertainty lexical acquisition texts J Quant Linguist 1 1 1994 S R Basili MT Pazienza P Velardi A shallow syntactic analyzer extract word associations corpora Literary Linguist Comput 7 1992 114124 6 R Basili MT Pazienza P Velardi Hierarchical clustering verbs ACLSIGLEX Workshop Lexical Acquisition Columbus OH 1993 Proceedings 7 R Basili MT Pazienza P Velardi Semiautomatic extraction linguistic information syntactic disambiguation Appl Artif Intell 4 1993 8 R Basili MT Pazienza PVelardi What learned raw text Mach Trunsl 8 1993 9 R Basili MT Pazienza P Velardi A notso shallow parser collocational analysis Proceedings COLING94 Kyoto 1994 lo R Basili MT Pazienza P Velardi A context driven conceptual clustering method verb B Boguraev J Pustejovsky eds Corpus Processing Lexical Acquisi classification tion MIT Press Cambridge MA 1996 II R Basili MT Pazienza P Velardi Integration probabilistic symbolic methods semantic categorization Proceedings AAAI Spring Symposium Stanford CA 1995 12 R Beckwith C Fellbaum D Gross G Miller WordNet lexical database organized U Zernik ed Lexical Acquisition Exploiting OnLine Resources psycholinguistic principles Build Lexicon Lawrence Erlbaum London 1991 13 B Boguraev Building lexicon contribution computers IBM Report TJ Watson Research Center 1991 14 B Boguraev T Briscoe eds Computational Lexicography Natural Language Processing Longman New York 1989 15 P BrownVJ Della Pietra J Cocke SA Della Pietra F Jelinek R Lafferty R Mercer P Roossin A statistical approach machine translation Comput Linguist 16 2 1990 16 P Brown VJ Della Pietra PV deSouza JC Lai R Mercer Classbased ngram models natural language Comput Linguist 18 4 1992 17 K Church W Gale P Hanks D Hindle Using statistics lexical analysis U Zernik ed Lexical Acquisition Exploiting OnLine Resources Build Lexicon Lawrence Erlbaum London 1991 YH R Basili et al I Artificial Intelligence XS 1996 5999 1X K Church 1 1993 RL Merccr eds Special Issue Using Large Corpora Comput Linguist 19 lY KW Church P Hanhs Word association norms mutual information lexicography comput Linguist 16 1 IYYO need 20 C Cookson Why computers 21 A Copestake lexicons large 27 I Dagan The ACQUILEX Proceeding ird ANLP 1992 L Lee Similaritybased abilities F Pereira Procredings ilf_ Workshop Las Cruces NM 1994 estimation word cooccurrences prob learn English Financial Times September semiautomatic LKB representation issues 20th 1989 acquisition 1231 M Evens 1988 1241 D Farewell ed Relational Models Lexicon Cambridge University Press Cambridge L Guthrir y Wilka Automatically creating lexical entries ULTRA multilingual MT MI Tram 8 lYY3 127135 1251 R Grishman patterns Compur Linguist 12 19X6 L Hirschman NT Nhan Discovery procedures sublanguage selectional PI 271 PC 1291 301 R Grishman Proceedings R Grishman Nantes 1992 C MacLeod COLING A Meyers Complex lY94 OJ Kyoto syntax building computational lexicon J Sterling Acquisition selcctional patterns Proceedings COLZNG YJ J Sterling Generalizing IYY4 94 Kyoto R Grishman automatically generated selectional patterns Proceedings COLlNG M Hearst Proceedings ACLSIGLEX Workshop Lexical Acquisition Text Columbus OH predicate D Hindle suite computational H Schuetzc classification Customizing structures lexicon better argument 1993 Proceedings ACL Noun task 311 Workshop 1990 D Hindle lYY3 M Rooths Structural ambiguity lexical relations Comput Linguist 19 1 37 L Hirschman R Grishman N Sager Gramaticallybased automatic word class formation Proceedings AAXI Spring Symposium Stanford evaluation criterion classifiers performance Inform Process Manage 11 1975 3Y57 P Jacobs Text based CA 1990 I Koronenko 1 Bratko Informationbased intelligent systems Improved Tagging English Much Learn 6 1991 6780 FM Lang L Hirschman patterns 1988 B Merialdo S Montemagni semantic F Pereira N Tishby L Lee Distributional Workshop Columbus OH S Pinker Cambridge MA 1989 Background Powers A Sanfilippo Word knowledge L Vanderwende dictionaries experiments machine information 1994 94 Kyoto acquisition 1994 I331 341 351 I361 371 381 391 401 411 421 Proceedings COLING A Sanfilippo readable 1992 dictionary Proceedings Second Conference Applied Computational Linguistics Austin TX parsing interactive acquisition selectional text probabilistic model Comput Linguist 20 2 1994 Structural vs Proceedings COLZNG clustering string YZ Nantes Proceedings ACL English verbs extracting patterns patterns 1992 Leurnahility Cognition The Acquisition Argument Structure MIT Press eds Proceedings First SHOE Workshop Tilburg 1992 lexicon construction dictionary compilation learning natural language W Daelemans D V Pozanski sources The acquisition combined machine Proceedings Applied Natural Language Processing Povo Trento lexical knowledge I431 441 JJ Carrol S Anadianou S Sekine Proceedings Third ANLP Trento JF Sowa Conceptual Structures Mind Machine AddisonWesley J Tsujii Automatic 1992 learning semantic collocation Reading MA 1984 R Basili et al I Artificial Intelligence 85 1996 5999 99 45 T Utsuro Y Matsumoto M Nagao Verbal case frame acquisition bilingual corpora Proceedings IJCAZ93 Chambery 1993 461 P Velardi Why human translators sleep peace Four engineering linguistic gaps NLP Proceeding COLING 90 Helsinki 1990 47 D Yarowsky Wordsense disambiguation statistical models Rogets categories trained large corpora Proceedings COLING 92 Nantes 1992 481 D Hindle M Rooth Structural ambiguity lexical relations Proceedings ACL Workshop Berkeley CA 1991