Artiﬁcial Intelligence 172 2008 483513 wwwelseviercomlocateartint Quantifying uncertainty belief net response Bayesian errorbars belief net inference Tim Van Allen Ajit Singh b Russell Greiner c Peter Hooper d Apollo Data Technologies 12729 NE 20th Suite 7 Bellevue WA 98005 USA b Center Automated Learning Discovery Carnegie Mellon University Pittsburgh PA 15213 USA c Department Computing Science University Alberta Edmonton Alberta T6G 2E8 Canada d Department Mathematical Statistical Sciences University Alberta Edmonton Alberta T6G 2G1 Canada Received 22 June 2006 received revised form 8 September 2007 accepted 10 September 2007 Available online 26 September 2007 Abstract A Bayesian belief network models joint distribution variables DAG represent variable dependencies net work parameters represent conditional probability variable given assignment immediate parents Existing algorithms assume network parameter ﬁxed From Bayesian perspective network parameters ran dom variables reﬂect uncertainty parameter estimates arising parameters learned data elicited uncertain experts Belief networks commonly compute responses queriesie return number PH h E e Parameter uncertainty induces uncertainty query responses random variables This paper investigates query response distribution shows accurately model distribution query network structure In particular prove query response asymptotically Gaussian provide mean value asymptotic variance Moreover present algorithm computing quantities worstcase complexity inference general straightline code query includes n variables We provide empirical evidence 1 approximation variance accurate 2 Beta distribution moments provides accurate model observed query response distribution We use produce accurate error bars responsesie determine response PH h E e x y conﬁdence 1 δ 2007 Elsevier BV All rights reserved Keywords Bayesian belief network Variance Bucket elimination Credible interval Error bars 1 Introduction Bayesian belief nets BNs provide succinct model joint probability distribution increasing range applications 13 They typically built ﬁrst ﬁnding appropriate structure interviewing expert selecting good model training data training sample estimate Corresponding author Email addresses timapollodatatechcom T Van Allen ajitcscmuedu A Singh greinercsualbertaca R Greiner hooperstatualbertaca P Hooper 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200709004 484 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 parameters 22 The resulting belief net answer querieseg compute conditional probability PCancertrue Smoketrue Gendermale These responses clearly depend training sample instantiate parameters different training samples produce different parameters lead different responses This paper investigates variability sample induces variance query response presents tech nique estimating posterior distribution query responses produced belief net Stated informally goal algorithm takes belief net structure assume correct I map true distribution D 34 prior distribution network parameters Θ data sample S generated D query form What qΘ PH h E e Θ returns expected value approximate variance query response qΘ based posterior distribution parameters given sample By moments appropriate distributional form approximate density qΘ produce explicit L U 0 1 bounds 1001 δ posterior density interval In words algorithm returns pointestimate answer error bars cid2 There ways use variance query response 1 In general good classiﬁer minimize Mean Squared Error expressed Bias2 Variance 36 The results paper provide way compute variance Bias2 Variance measure estimate quality different belief net structures seeking best classiﬁer Empirical evidence 18 suggests measure fact effective discriminative model selection criteria 2 The maximum likelihood approach combining responses independent belief net classiﬁers Pj involves weighting respective mean probabilities 1variance P hie j Pj hi evarj hie We 32 works practice 3 We use query variance detect outliers help differentiate sampling variation true outliers 33 4 In classiﬁcation high probability frequently taken proxy conﬁdence Error bars provide statistically rigorous measure conﬁdence For example imagine determined action1 apply treatmentX optimal Pc e 05 condition means action1 maximizes expected utility MEU 27 In traditional view comfortable taking action1 larger values Pc eeg conﬁdent given ecid5 Pc ecid5 07 versus ecid5cid5 Pc ecid5cid5 06 Now imagine know Pc ecid5 07 05 Pc ecid5cid5 06 0001 In case MEU response action1 expected utility depends expected value response characteristics posterior distribution However conﬁdent taking action second situation ecid5cid5 probability mass 05 decision boundary This useful making decisions safetycritical scenarios analysis help quantify chance bad outcome taking appropriate actiongood decision bad outcome 5 Finally expert available provide query response error bars validate given belief net structure For example expert claims PB 1 D 0 C 1 05 technique asserts response 026 034 99 conﬁdence question validity network However technique instead asserts response 017 058 99 conﬁdence need question network structure This paper provides way quantify variance response In particular approximate posterior distribution query response credible intervals Bayesian errorbars readily computed The overall process shown Fig 1 We begin network structure prior parameters The COMPUTEPOSTERIOR routine uses data sample perform Bayesian update prior yielding posterior distribution network parameters Next MEANVAR algorithm calculates mean approximate vari ance query response Finally COMPUTEERRORBAR routine uses moments produce model posterior distribution query response computes 1 δ credible interval The COMPUTEPOSTE RIOR COMPUTEERRORBAR subroutines understood main contribution deﬁning implementing MEANVAR Section 2 provides required background deﬁning belief nets describing Bayesian framework use This section describes COMPUTEPOSTERIOR COMPUTEERRORBARS steps Section 3 presents T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 485 Fig 1 Overview overall process computing error bar query response oretical justiﬁcation MEANVAR Given set explicitly speciﬁed assumptions prove query response distribution asymptotically univariate Gaussian provide closedform formula asymptotic variance1 This theorem basis approximation posterior distribution Section 4 presents algorithm MEANVAR computing variances In particular ﬁrst describes algorithm extension wellknown Bucket Elimination algorithm 14 computes BUCKELIM response derivative response wrt CPtable entries MEANVAR uses derivatives computing approximation variance This section provides theorems state algorithm correct asymptotic computational cost inference Appendix A analyzes special case query PH h E e complete E speciﬁes value variable H It provides straightforward lineartime algorithm case While variance approximation asymptotically accurate clear error bar algorithm work practice especially small sample approximation ﬁrstorder COMPUTEERROR BAR inherits assumption posterior query response Gaussian We perform experiments based Monte Carlo simulations range belief net structures queries The results described Section 5 suggest 1 approximation variance acceptable 2 resulting distribution wellmodeled Gaussian assumption This negative result surprising The response probability 0 1 interval Gaussian distribution signiﬁcant mass outside 0 1 We considered approximating query response Beta distribution produces accurate error bars Section 6 surveys relevant work place results context Appendix B provides proofs claims paper Given abundance notation provide list symbols Fig 3 Further details complete results inputs experimental parameters raw data available webpage 20 2 Preliminaries 21 Belief nets We assume ﬁxed underlying distribution n discrete random variables X1 Xn denote underlying distribution event distribution We encode event distribution belief net2 cid6V A Θcid7 consists directed acyclic graph DAG nodes V represent variables directed arcs A represent dependencies variables The cid6V Acid7 network structure encodes independency relationships variables underlying distributionie node independent nondescendants given assignment immediate parents Each node C V associated 1 Essentially asymptotic refers sufﬁciently large sample Theorem 2 2 Also known Bayesian network Bayesian belief network probability net 486 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Fig 2 Diamond network A simple example belief network CPtable distributions conditional probability table called CPtable speciﬁes Θcf PC c F f value c C assignment f set parents F 34 That belief network factors underlying distribution product conditional probabilities While typically view CPtable table useful Section 4 view function maps assignment variables C F associated probability value PC c F f 0 1 Fig 2 provides example belief network As function CPtable B denoted fB b row view ΘB1A0 0412 ΘB0A0 0588 As ΘBA0 distribution row entries sum cid2 b ΘBbA0 1 ΘB0A0 implicitly deﬁned ΘB1A0 We let Θ Θcf denote set CPtable rows In addition providing compact representation joint distribution belief nets effectively compute marginal conditional probabilities A query question form What PH h E e answer real number 0 1 known query response3 In paper assume network structure ﬁxed particular query answer depends network parameters Θ cf Eq 16 Section 51 associated Diamond network Fig 2 To emphasize relationship use qheΘ qΘ denote response query Ph e Θ function parameters Θ 22 COMPUTEPOSTERIOR Learning network parameters We view Θ random vector follow Bayesian view parameter learning placing prior Θ integrating data yield posterior distribution Θ Fig 4 We assume rows independent Deﬁnition 1 2 Section 3 row ΘCf independent row Hence prior Θ decomposed priors row Following standard practice 22 assume row Dirichlet distributed ΘCf DirmCc1f mCcr f mCci f 04 An alternative notation Dirichlet distribution ΘCf cid6ΘCc1f ΘCcr fcid7 DirmCFf ˆΘCc1f ˆΘCcr f mCf cid3 mCci f ˆΘCci f mCci f mCf 1 2 mCf called effective sample size distribution ˆΘCcf EΘCcf expected value ΘCcf 9 3 We allow conditioning event E e involve variables allow unconditional queries form PH h Also notation H h suggests approach works single query variable note holds dealing set query variablesie PH1 h1 Hr hr E e 4 A Dirichlet distribution parameters known Beta distribution denote Bea b T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 487 Top level algorithms BUCKELIM BUCKELIM Bucket elimination algorithm computing expected value query response Section 41 Variant bucket elimination algorithm computes expected value query response derivatives qcf ˆΘcf Section 42 COMPUTEPOSTERIOR Computes posterior distribution network parameters Section 22 COMPUTEERRORBARS Computes actual Bayesian error bars based distribution Section 23 MEANVAR Computes mean variance response Section 4 D n k C C c C f Θ ΘCfCf ΘCf ΘCFf Θcf ΘCcFf ˆΘ EΘ mCFf qh eΘ qΘ qcfΘ qΘ Θcf σ 2 σ 2 σ 2 σ 2 vheCf heΘ heΘ bXi b fij JoinF ElimX f maxIndexf y Belief nets Underlying distribution data drawn Number nodesvariables belief net Total number CPtable rows entire network Capital letters denote nodes belief net equivalently variables represent Bold denotes set variableseg F Lowercase letters denote assignment variable If assignment single binary variable c c denotes values Refers CPtable row variable C corresponding parental assignment F f Set network parameters entire network CPtable row ΘCf cid6Θc1f Θccid4fcid7 Single network parameter corresponding PC c F f Expected values network parameters Effective sample size associated CPtable row ΘCFf Eq 2 Query response function parameters ﬁxed implicit belief net structure ﬁxed implicit query PH h E e Θ Derivative query response respect formal parameter Θcf Eq 6 Variance query response Approximate variance query response Eq 7 Contribution variance query Ph e CPtable row ΘCFf Eq 8 BUCKELIM BUCKELIM Section 4 Buckets holding tables including CPtables Function belonging bucket bXi Join functions f F Marginalizes variable X table f Largest index variables Schemef Eq 11 Query response y qΘ PH h E e Θ Fig 3 Notation Consider density ΘCA1 Fig 2 Assume prior density Dir1 1 complete data setie instance set speciﬁes value variables Only instances A 1 contribute ΘCA1s posterior Moreover values C multinomially distributed selected sample If 34 instances A 1 8 C 1 26 C 0 posterior distribution ΘCA1 Dir1 8 1 26 Dir9 27 corresponds ﬁrst row Cs CPtable shown Fig 2 This Bayesian update Dirichlet prior multinomial data yields Dirichlet posteriorie Dirichlet distribution conjugate prior multinomial distributions 43 Bayesian parameter learning belief networks consists updating row fashion In absence prior knowledge uniform prior Dir1 1 row Note distribution ﬂat assignment equally likely Frequently posterior Dirichlet row distributions replaced expectations In example ΘCA1 Dir9 27 reduced ˆΘCA1 cid6025 075cid7 Replacing posterior row distributions expectations effectively ignores parameter uncertainty When network parameters ﬁxed real numbers query response real number However uncertain network parameters 488 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Fig 4 Example COMPUTEPOSTERIOR induce distribution query response Θ random variable qΘ random variable Furthermore Cooper Herskovits 9 shown assumptions Deﬁnition 1 cid4 E cid5 qΘ cid7 cid6 EΘ q q ˆΘ 3 That posterior expectation qΘ simply query response network mean parameters Recall ˆΘ cid6 ˆΘ1 ˆΘr cid7 encoded posterior row distributions DirmCf ˆΘ1 ˆΘr Eq 1 There number algorithms computing query response network parameters ﬁxed values Section 4 use Bucket Elimination Such algorithms calculate expected value response qhe ˆΘ provide variance σ 2 E cid4cid6 cid7 qheΘ qhe ˆΘ 2 cid5 Theorem 2 provides asymptotic form variance Section 4 describes algorithm MEANVAR effectively compute quantity general T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 489 23 COMPUTEERRORBARS Error bars query response COMPUTEERRORBARS uses mean variance query response estimate posterior distribution qΘ construct approximate Bayesian credible regions error bars This depends actual form distribution consider classes Normal Beta In case need ﬁnd Beta parameters α β correspond given mean μ variance σ 2 α μ μ1 μ σ 2 σ 2 β 1 μ μ1 μ σ 2 σ 2 4 If α β nonpositive associated density function welldeﬁned Also parameter 1 associated density modes 0 andor 1 For distribution D δ 0 1 1 δ credible region query response set ωD PqΘ ωD 1 δ While inﬁnitely credible intervals continuous distribution COMPUTEERRORBARS chooses contiguous intervals form L U mass lower bound L equals mass upper bound U Letting CDF1 qheΘ denote inverse cumulative density function model qheΘ cid8 cid8 cid9 cid9 L CDF 1 qheΘ U CDF 1 δ 2 1 δ 2 qheΘ Note framework precludes use criteria interval selection shortest contiguous 1 δ interval 3 The posterior distribution response The MEANVAR subroutine requires effective way compute variance response σ 2 heΘ Our main theorem provides closedform expression asymptotic approximation quantity given following assumptions σ 2 Deﬁnition 1 The independent Dirichlet property respect DAG cid6V Acid7 holds following condi tions 1 True Structure cid6V Acid7 Imap underlying joint distribution 34ie independence claim implied graph holds underlying distribution 2 Parameter Independence The distribution variable conditioned assignment parents indepen dent distribution conditioned assignment parents That f1 cid12 f2 ΘCf1 ΘCf2 independent local parameter independence Furthermore local conditional probability tables independent global parameter independence 39 3 Dirichlet Assumption The distribution variable given assignment parents Dirichlet distributed ΘCFf DirmCc1f mCcr f 4 Nondegenerate Posterior Means The expected posterior parameters ˆΘcf strictly 0 15 Note assumptions conform maximum posteriori learning belief network parame ters 22 As assuming network parameter ΘCcFf Θcf random variable query response function network parameters random variable We continue use ˆΘ ˆΘcf EΘ refer expected value parameters posterior distribution conditioned implicitly observed data andor expert opinion 5 We discuss constraint belowboth statement Theorem 2 following proof Appendix B 490 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 As mentioned denote query response qΘ emphasize simply function instantiation network parameters real number 0 1 For example query qabΘ Pa b Θ Fig 26 viewed qabΘ qabΘa Θa Θba Θba Θa Θba Θa Θba Θa 5 For network parameter Θcf Θ consider partial derivative query respect parame ter qcfθ qΘ Θcf cid10 cid10 cid10 cid10 θ 6 functional θ evaluated speciﬁc instantiation network parameters Using qabΘ function Eq 5 observe qbaΘ Θa ΘbaΘa ΘbaΘa ΘbaΘa ΘbaΘa ΘbaΘa2 We state main technical result Theorem 2 Given independent Dirichlet property wrt given DAG assumption posterior mean qh eΘ Eqh eΘ qh e ˆΘ Now consider asymptotic framework minmCf posterior means ˆΘcf remain ﬁxed values strictly 0 1 let 1 1 mCf vheCf cid3 σ 2 CFf cid8cid3 vheCf cid9 qcFf ˆΘ2 ˆΘcFf qcFf ˆΘ ˆΘcFf cid9 2 cid8cid3 cC cC Under framework standardized random variable qh eΘ qh e ˆΘ σhe converges distribution standard Normal distribution N 0 1 7 8 9 Given form Eq 9 σ 2 q ˆΘ EqΘ mean Eq 7 corresponds asymptotic variance Recall Eq 3 Each summation Eq 8 values particular variable Cso variable binary sum involve term c c The summation Eq 7 CPtable rows Diamond network Fig 2 involve 9 terms corresponding 9 different vheCf values vheA vheB vheB vheD b c Moreover vheCf 0 qcf ˆΘ 0 c C happens CPtable row involved computing qheΘ Ph e Θeg C dseparated query HC FE C barren node descendants instantiated query 31 Appendix A As extreme case imagine query corresponded single CPtable entry Fig 2 query qba Pb Here vbaB relevant value vbaCf 0 terms corresponding 8 CPtable rows C f As qba ba ˆΘ 0 query ba ˆΘ 1 qba 6 This corresponds Q2 Eq 16 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 491 vbaB cid9 qba ba ˆΘ2 ˆΘba cid8 cid3 bB cid8 cid3 bB cid9 2 qba ba ˆΘ ˆΘba 12 ˆΘba 02 ˆΘba 1 ˆΘba 0 ˆΘba2 ˆΘba ˆΘba2 ˆΘba ˆΘba means variance associated qba query σ 2 ba 1 1 mBa ˆΘba ˆΘba expected exactly variance associated single Dirichlet distributed variable DirmBa ˆΘba ˆΘbaie Eq 7 exact approximation Hooper 24 generalizes situations Eq 7 exact Appendix A provides class situations easy compute σ 2 Eqs 7 8 σ 2 adds inﬂuence Θcf query Ph e based ˆΘ weighted 1 mCf1 So mCf query variance different derivative qcf different queries derivatives differ While Appendix B provides proof note asymptotic normality derivation approximate variance Eq 7 employ ﬁrstorder Taylor expansion function qΘ posterior mean ˆΘ The conditions ˆΘcf mCf ensure ﬁrstorder approximation asymptotically valid Comment1 proof Appendix B discusses possible violations assumptions We use Eq 7 understand deal ﬁxed CPtable rowsie speciﬁc Θcf entries correspond deﬁnitions viewed constants variables Here corresponds vheCf 0 variance ing effectively inﬁnite mCf value means row contribute approximationie Eq 7s summation simply ignore CPtable rows Here course allow CPtable entry deterministicie 0 1 1 1mCf Section 4 presents algorithm compute mean variance query response On expw n number variables network w induced tree width 14 This worstcase complexity inference 31 The Beta approximation posterior Theorem 2 suggests Normal distribution approximate response posterior distribution response conﬁned 0 1 Normal distribution positive density entire real line When variance large Normal approximation deviate considerably true posterior Furthermore expect response distribution skewed mean near 0 1 It sense use Beta distribution instead conﬁned 0 1 model skewed distributions standardized converges Fig 5 MEANVAR Process including BUCKELIM 492 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 distribution standard Normal random variable 3 Eq 4 shows map computed mean μ variance quantities σ 2 Beta distributions α β parameters 4 Computing variance BUCKELIM MEANVAR Eqs 7 8 Theorem 2 approximate variance response This section sketches MEANVAR algorithm Fig 5 computing approximate variance In particular equations involve derivative algorithm appearing Section 42 computes response respect parameter The BUCKELIM values That section presents theorems showing correctness efﬁciency algorithm As BUCKELIM basically extension Bucket Elimination algorithm ﬁrst review algorithm Section 41 Finally Section 43 presents remaining issues particular explicitly describes results BUCKELIM algorithm compute variance There ways optimize overall MEANVAR First note MEANVAR uses BUCKELIM turn based BUCKELIM As BUCKELIM standard wellstudied algorithm number known effective optimizations including graph reduction heuristics ﬁnding good ordering MEANVAR Second ﬁnd translates directly improvement BUCKELIM insights squinting Eq 8 In particular Appendix A provides straightline code applies special case query complete evidence includes variables query variable 41 Bucket elimination algorithm BUCKELIM The bucket elimination algorithm BUCKELIM 14 elegant framework belief net inference general uses nonserial dynamic programming iteratively eliminate variables marginalization Notation We ﬁrst introduce notation table function mapping set named variables cid17 For example f A B 1 1 0 0 b 1 0 1 0 f A B b 030 070 091 009 Notice table corresponds ΘBA function shown Fig 2 A tables input variables called scheme Schemef A B We deﬁne operations tables Join Elim The join operation combines set tables f F Schemef union variables tables new table Letting h JoinF Schemeh hs values cid11 cid12 hx f xSchemef f F xS denotes projection assignment x X subset variables S X cid6A 1 B 0 C 0cid7AC cid6A 1 C 0cid7 Thus value h x product f F s evaluated projections x scheme f We simplify notation Joinf g abbreviation Joinf g Note Joinf f Join undeﬁned To Join concrete consider table gA C 1 1 0 0 c 1 0 1 0 gA C c 022 078 099 001 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 493 Then h Joinf g hA B C 1 1 1 1 0 0 0 0 b 1 1 0 0 1 1 0 0 c 1 0 1 0 1 0 1 0 hA B b C c 03 022 03 078 07 022 07 078 091 099 091 001 009 099 009 001 This operation similar relational join database theory 28 The elimination operation reduces table f marginalizing variables f s scheme producing function fX ElimX f variables Schemef X Assuming Schemef X Y1 Yk fXy1 yk cid3 f x y1 yk x Hence f deﬁned ElimA f fAB b 1 0 fAB b 03 091 07 009 We eliminate set variables repeating marginalization process iteratively variable write ElimXi Xj f Observe obtain result independent order eliminate variables Xi Xj Notice Elim f f BUCKELIM Algorithm The bucket elimination algorithm BUCKELIM takes input belief net encoded set associated CPtables F variables V partial assignment subset variables E e ordering variableseg π0 cid6A B C Dcid7 Below assume variables numbered order cid6X1 X2 Xncid7 BUCKELIM computes value PE e y f xSchemef 10 cid3 cid12 x xEe f F sum assignments V include partial assignment E e Of course compute y simply joining functions F eliminating variables V E However BUCKELIM achieves better efﬁciency brute force approach exploiting fact function depends variables scheme To BUCKELIM builds uses set buckets bXi associated variable Xi V additional bucket b Each bucket hold set functions The buckets ordered according π b beginning BUCKELIM initially loads input functions F belief nets CPtables buckets follows Recalling table f uses set variables Schemef Xii let cid13 cid14 maxIndexf argmax Xi Schemef 11 largest index f s variables based π order We assign f maxIndexf bucket If Schemef deﬁne maxIndexf Hence π0 assign ΘA ΘA bucket bA ΘBA bucket bB ΘCA bucket bC ΘDBC bucket bD Here happened onetoone mapping tween CPtable functions buckets true general7 7 For example ordering cid6D C B Acid7 bD bC buckets bB bucket include function ΘDBC bA bucket include functions ΘA ΘBA ΘCA The rest text consider π0 ordering 494 Step 0 1 2 3 b f1 gA T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 bA fA1a ΘAa bB fB1b ΘBbAa bC fC1c ΘCcAa fC2b c ΘD1BbCc bD fB1b ΘBbAa fB2a b gC b fA1a ΘAa fA1a gB fA2a gB Fig 6 Trace BUCKELIM algorithm PD 1 BUCKELIM uses buckets answer queries It traverses buckets reverse order means processing bD ﬁrst bC bB bA b To process bucket bXj If bucket BUCKELIM skip Otherwise BUCKELIM joins functions bucket eliminates variable Xj producing gj Elim cid7 cid6 Xj JoinbXj 12 We store gj bucket bXi maxIndexgj That Schemegj store gj b bucket Otherwise Schemegj variable highest index Xi Note j j largest index functions bXj eliminated BUCKELIM stores gj bXi bucket continues ordering process bXj 1 At end processes b bucketwhich involves Joining constant functions corresponds simple scalar multiplication BUCKELIM returns resulting scalar The algorithm ﬁnal complication The process suggested compute P 1 In general want compute expression form PE e To ﬁrst initialize relevant CPtable functions correspond instantiations use restricted functions Example To illustrate consider PD 1 query π0 ordering Observe ﬁrst need Ds CPtable entries fD1BCb c ΘD1bcb c As function B C Dthat maxIndexfD1BC Cwe store function bucket bC Hence BUCKELIM starts 5 buckets conﬁgured shown Step0 Fig 6 BUCKELIM processes buckets reverse order As bD BUCKELIMs ﬁrst nontrivial operation deals bC joins functions bucket eliminates variable C produces8 gCa b Elim cid3 cid7 cid6 C JoinfC1 fC2 cid3 c ΘCcAa ΘD1BbCc PD 1 C c B b A PD 1 B b A c As function scheme SchemegC A B stored bucket bB called fB2 Step1 entry Fig 6 BUCKELIM sums bB computing gB Elim cid7 cid6 B JoinfB1 fB2 cid3 b ΘBbAa PD 1 B b A PD 1 B b A PD 1 A cid3 b 8 In cases result computation easy interpretation PD 1 B b A shown This case T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 495 As function involves variable A stored bA called fA2 It sums bA produce constant gA Elim cid7 cid6 A JoinfA1 fA2 cid3 PA PD 1 A PD 1 stored b Step2 Step3 Fig 6 BUCKELIM returns f1 correct response query 42 Extension compute derivatives BUCKELIM BUCKELIM takes inputs BUCKELIM belief net represented set CPtable functions F variables V ordering variables π partial assignment variables E e It returns expected response y PE e EΘ P E eΘ shown Eq 10 CPtable fi Θi partial derivative y qiΘ qΘΘi For notation continue functions appearing fi bucket bXi fij j fi1 fi2 functions bA fA1 fA2 Notice function original CPtable subset table fC2 ΘD1BC function produced ﬁrst joining set previously produced functions common bucket eliminating variable resulting functioneg cid7 cid6 fX2 gY Elim Y JoinfY1 fY2 fY3 13 We continue let gi refer output bucket bXi BUCKELIM uses information compute partial derivative qΘfij function appearing bucket includes Θi CPtable function It chain rule Eq 13 qΘ fY1 qΘ fX2 fX2 fY1 Moreover given form Eq 13 fX2 corresponds roughly JoinfY2 fY3 marginalizing fY1 variables Eq 14 The remaining trick process buckets forward direction sure computed qΘ fX2 The overall algorithm asked compute qΘ fY1 Step A Run BUCKELIM inputs compute y The intermediate results functions gi created output bucket stored use Step B Step B Compute derivative function bucket function fij follows 1 Compute y g output b g Elim Joinb cid15 fi Eq 12 This trivial g constant y g y g 1 2 Iterate remaining buckets order reversed BUCKELIMie original order π starting b Let bXi current bucket output gi ElimXi JoinbXi Now recall maxIndexgi gi formed joining functions Xi bucket variables indices eliminating Xi As processing buckets order BUCKELIM reaching bXi computed y gi For fij bXi compute y fij cid8cid16 cid17 cid9 J Join y fij Elim h bXi h cid12 fij y gi cid6 SchemeJ Schemefij J cid7 14 The ﬁrst argument Elim SchemeJ Schemefij X SchemeJ X Schemefij That join y functions bucket fij eliminate variables appear Schemefij gi 496 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 scheme Scheme y produce function y fij fij y respect fij Note qCf y fC1 equals Schemefij values partial derivatives general Example We saw b y g 1 means y f1 cid8 cid9cid9 cid8 1 In bA f1 formed fAi s qAa y fA1 y f1 Elim Join f1 fA1 1 cid12 j cid121 y f1 f1 fA1 fAj fA2 PD 1 A y That fA1 ΘA ΘA PA fA1a PA Notice Scheme y fA1 PD 1 A This makes sense y PD 1 A SchemefA1 Similarly cid2 PD 1 A y fA2 fA1a PA continues working forward ordernext processing functions bB Here fB1 BUCKELIM produce fA2 qBAa b y fB1 cid8 Elim y fA2 fA2 fB1 cid9 PA cid12 fBj j cid121 PA fB2 PA PD 1 B b A Elim PA aPB b A PA B b Finally fC1 fC2 produce y fB2 fB2 qCAa c y fC1 cid3 cid8 Elim B y fB2 fB2 fC1 cid9 cid3 b PA B b cid12 j cid121 fCj PA B b fC2 b cid3 b PA B b PD 1 B b C c qD1BCb c y fC2 ElimA y fB2 fB2 fC1 cid2 PA B b PC c A Appendix B provides proofs following theorems respectively prove algorithm correct bound computational complexity Theorem 3 BUCKELIM yΘCF correctly computes partial derivatives response wrt CPtable qCFΘ Theorem 4 The worst case complexity BUCKELIM ﬁxed size largest domain variables w induced tree width given ordering On r w time n number nodes r 43 How MEANVAR uses BUCKELIM The basic BUCKELIM algorithm deals unconditional probabilities To compute conditional probabil inference algorithms PH hi E e ﬁnally ities requires summation division That PH h E e PHhEe ﬁrst compute PH hi E e sum values compute PE e returns PH h E e PH h E ePE e PEe cid2 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 497 Our MEANVAR ﬁrst BUCKELIM ter computing response BUCKELIM eΘi parameters Θi s evaluated ˆΘ MEANVAR uses quotient rule derivatives PH h E e PE e In case af compute derivatives PH h E eΘi resp PE PH h E e Θi 1 PE e PH h E e Θi qΘ PE e Θi cid18 cid19 compute actual derivatives need Given complexity results Theorem 4 trivial observe MEANVAR On exp w time 5 Experiments Theorem 2 provides approximation variance query response σ 2 asymptotically accurate When network parameters estimated small moderately sized data sets guarantee accuracy σ 2 However Section 52 shows approximation works practice Given true mean approximate variance query response considered models distribution qΘ Normal Beta based respectively asymptotic behavior observations issues like support Sec tion 53 compares ﬁt model samples qΘ An important application query response model estimation errorbars explored Section 54 Finally researchers suggested vari ance approximated simple binomial expression Section 55 presents empirical evidence demonstrate good approximation 51 Experimental setup To explore claims require ability control parameter uncertainty Θ ability sample qΘ Our experiments use common benchmark networks parameters usually given ﬁxed quantities ΘBa 03 07 We require parameter uncertainty form Dirichlet row distribution ΘBa Beta3 7 To resolve problem treat network ﬁxed parameters underlying distri bution produce sample D drawing m tuples network Using structure initial network learn new parameters D Assuming uniform Dirichlet prior parameters Input Belief Net B structure parameters Set queries qi Θ Phi ei Θ m effective size compute parameters r number trialseach involves speciﬁc set parameter values 1 Generate posterior parameters For k 1m dk RandomInstanceFromB Compute posterior ΘD initial distribution Θcf Dir1 1 1 data D dk 2 Compute replicate parameters query responses For j 1r Draw Θj ΘD posterior distribution For query Qj Qj 2 For query based Qj 3 Actual Analysis Qj 1 qi Θj j samples Compute compare ai ei b Perform relevant statistical testsNormal vs Beta Section 52 Produce QuantileQuantile plots Section 53 c Perform coverage experiments error bars d Compare binomial distribution Section 55 Section 54 Fig 7 Experimental setup 498 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Θi Dir1 1 data integrated produce posterior parameters ΘD discussed COM PUTEPOSTERIOR Section 22 Given belief net posterior distribution use Monte Carlo strategy sample qΘ We generate j 1 replicate formed replacing Dirichlet row distribution r replicates Θ denoted Θ j r sample That replicate instantiates belief net ﬁxed values parameters Θ To illustrate imagine started Diamond network parameters initially uniformie Θi Beta1 1then m 100 data samples produce posterior ΘD distribution shown Fig 2 ΘA Beta35 67 ΘBa Beta7 29 We drew samples ΘD form cid20 cid20 cid20 cid20 Θ 1 Θ 2 Θ 3 035 Θ 1 031 Θ 2 033 Θ 3 ba ba ba 018 021 017 cid21 cid21 cid21 cid21 15 Notice drawing replicates values domain variables A B For instantiation parameters Θ j ﬁxed query qi calculate Qj qiΘ j exact algorithm belief net inference Our experiments based r 1000 samples This allows empirically evaluate quality approx imations implicit Theorem 2 deals distribution parameters Θ Eq 15 basic tuples We performed tests Qj j values subsections Fig 7 summarizes steps We use network structures experiments Diamond Alarm Insurance Hailﬁnder Diamond variable network illustrated Fig 2 allows variety inferential patterns We considered following queries Q1 Pa Θa Q2 Pa b ΘbaΘa Θba ΘaΘba Θa Q3 Pa b c Q4 Pb c Θba Θca Θba Θca Θa Θba Θca ΘaΘba Θca Θa Q5 Pa d Q6 Pd cid2 cid2 bc ΘdBbCcΘBbaΘCca Θa abc ΘdBbCcΘBbAa ΘCcAa ΘAa bc ΘdBbCc ΘBba ΘCca cid2 16 The Alarm network 37 variable network described 21 designed medical experts monitoring intensive care patients Alarm standard evaluating belief net learning inference algorithms Here m 150 samples produce Dirichlet parameters We generated 100 queries choosing single query variable ﬁve evidence assignments 21 determine variables query variables Here value query evidence variable assigned uniformly random The mean response queries tends cover 0 1 interval including queries mean response near 0 1 The Insurance network 6 models car insurance risk 27 variables We generated 100 queries randomly sampling query variable zero evidence variables To ensure mean response queries covered 01 interval rejection sampling procedure We divided 01 5 ranges 002 0204 0810 generated queries equal number responses fell bin9 The Hailﬁnder network 1 56 variable network forecasting summer hailstorms Again generated 100 queries rejection sampling procedure More details networks queries parameters extensive experimental results available 20 9 This required naive approach uniformly sampling queries 1 query variable 02 evidence variables produced queries responses tightly clustered 0203 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 499 Diamond m 25 b Alarm m 150 c Insurance m 150 d Hailﬁnder m 150 Fig 8 Each cid6x ycid7 point represents query xvalue approximate variance query response computed MEANVAR ai yvalue sample variance response based r 1000 instances drawn true response distribution ei Eq 17 Subﬁgures queries Diamond b queries Alarm c queries Insurance d queries Hailﬁnder 52 Accuracy variance approximation The simplest measure accuracy σ 2 close true variance query response Even know true variance query response r 1000 samples query response distribution Qir i1 provides good estimate This especially true qΘ unidimensional unimodal That wide range queries qi want analytic approximation ai σ 2 Eq 7 Theorem 2 compares largesample estimate10 ei 1 r rcid3 cid6 Qj j 1 cid7 EQi 2 17 As know approximate variance asymptotically correct m interested seeing behavior relatively small values m Fig 8 shows results networks We m 150 larger networks m 25 Diamond Using m 150 reduce variance query responses nearly zero The fact essentially values extremely near y x diagonal line shows tight ﬁt queries We quantify While absolute scale variances appears 10 In experiments use nonparametric bootstrap improve largesample estimate 500 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Fig 9 Mean scaled percentage error vs effective sample size m small note variance usually 1 12 variance uniform distribution 01 Of course query distributions typically smaller variances Eg query mean 04 variance 001 usually fall standard deviations meanie 02 0611 While deviances ai ei relatively small larger deviance value associated queries largest variance values This surprising follows fact Delta method based ﬁrstorder Taylor expansion We elaborate Comment2 proof Theorem 2 Appendix B We use Mean Scaled Percentage Error cid3 MSPE 100 n ai ei ei average percentage ai deviates ei n queries evaluate quality approximation Fig 9 shows MEANVAR approximations accurate range sample sizes For example m 25 samples worst average structure 14 200 examples worst 7 The rate convergence σ 2 differs query Comment1 proof Theorem 2 Appendix B 53 Normal vs Beta distribution Some tasks ones mentioned 18 32 require approximation variance previous section showed analytic approximation variance ﬁts empirical evidence closely However tasks computing error bars need know form underlying distribution While parametric form unknown general 24 determine appropriate answers plausible form Section 31 argued ﬁtting mean approximate variance Normal Beta distribution Here compare samples true distribution Qir i1 model Fig 10 presents histograms Qj j values different queries Qi expected means near 05 005 respectively On superimposed pdf probability distribution function best ﬁt Normal best ﬁt Beta distributions The left ﬁgure shows distributions good ﬁts mean far 0 1 distribution fairly symmetric The right ﬁgure shows bestﬁtting Normal distribution problems distribution skewed 11 Each point Fig 8 based single Θ vector produced single data set We considered Θ parameter values estimating new parameters data sets generated underlying distributionie rerunning process Fig 7 starting Step 1 We quality variance approximation similar T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 501 Low skew b High skew Fig 10 Two query response distributions Alarm Each histogram Qi r models When skew low models tend similar skew high Beta model better ﬁt i1 instances overlayed ﬁtted Beta Normal Fig 11 Quantilequantile plots query instances Alarm network comparing query sample distribution approximation qΘ distribution calculated mean variance The lefthand assumes qΘ Gaussian righthand assumes qΘ Beta distributed The μ value expected query response A quantilequantile plot provides way visualize ﬁt plotting sample quantiles theoretical quantiles ﬁtted Normal Beta distributions 4 Here straight diagonal line indicates k 1r kth instance r appears distribution predicts kr data Fig 11 shows quantilequantile plots queries numbered 15 25 95 numbers based querys mean response12 We queries better approximated Beta Normal especially true distribution skewed ones mean response μ near 0 near 1 One way quantify difference models compare loglikelihood query resp qiΘ j computed based Normal resp Beta parameters j log PNormalμi σ 2 stances Qj cid2 Qj 12 These representative sample 600 quantilequantile plots Alarm All plots available 20 502 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Table 1 FailBeta fraction queries ﬁt Beta hypothesis 005 signiﬁcance Fail Normal fraction queries ﬁt Normal hypothesis 005 signiﬁcance Domain Diamond m 25 Alarm m 300 Insurance m 300 Hailﬁnder m 300 FailBeta 06 16100 13100 10100 FailNormal 16 50100 36100 31100 cid2 j log PBetaai bi Qj For Diamond constructed m 25 samples likelihood Beta model higher Normal model queries For Alarm m 300 likelihood Beta model higher Normal model 92 100 queries For Insurance m 300 Beta model better 89 100 queries For Hailﬁnder m 300 Beta model better 89 100 queries The fact Beta higher likelihood impressive given loglikelihood measure unnaturally favors Normal model especially response near 0 near 1 As Θ 0 Θ 1 loglikelihood Beta distribution usually tends occur Normal distribution This loglikelihood N μ σ 2 model constant plus sum terms 12 x μσ 2 log likelihood Bea b constant plus sum terms 1 logx b 1 log1 x blow large negative 1 x near 0 b 1 x near 1 Hence Beta distribution strongly affected values near boundary unit interval Normal log likelihood Another approach use goodnessofﬁt test formally null hypothesis samples Qj r j 1 drawn Normal model resp Beta model alternate negation null hypothesis Such tests allow state data drawn null model instead focus measure agreement data null hypothesis 16 Here asking close observed distribution bestﬁt Normal resp bestﬁt Beta distribution These goodnessofﬁt procedures suggest Normal model problematic Using AndersonDarling mality test 2 example 80 100 Alarm queries showed evidence nonnormality signiﬁcance level 005 Due lack critical values Beta distribution resort lower power KolmogorovSmirnov test compare model 11 At 005 signiﬁcance level results Table 1 In domains Beta distribution tends ﬁt queries better Normal distribution 54 Accuracy Bayesian error bars Another way compare Normal Beta models based respective performance task If model accurate functionals model credible regions posterior response accurate As mentioned 1 δ credible set region ω cid17 PqΘ ωD 1 δ If model posterior response distribution good expected fraction query samples Qj ω 1 δ Here query q ﬁrst MEANVAR compute mean variance response values compute 90 credible region Normal distribution LN UN Beta distribution LB UB We drew r 1000 instances qΘ procedure described Section 51 recorded fraction instances appeared LN UN resp LB UB If distributional assumption correct expect ratio close 09 The results presented Fig 12 While approximations problematic cases Beta distribution produces accurate intervals queries The problems Normal model conspicuous mean query response near 0 1 qΘ exhibits signiﬁcant skew Repeating experiment 80 credible interval produces similar results13 13 We considered intervals However larger intervals 95 99 contrary goal verifying Normal Beta distributions good models posterior distributions satisfy given 099 interval chance distributions satisfy 090 interval chance By contrast small interval like 50 10 reﬂect model quality immediately EqΘ T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 503 Alarm Normal b Insurance Normal c Hailﬁnder Normal d Alarm Beta e Insurance Beta f Hailﬁnder Beta Fig 12 Query sample coverage computed 90 Bayesian credible intervals ac assumes qΘ Normally distributed df assumes qΘ Beta distributed Each point represents query sorted order expected response The horizontal dotted line represents desired result 90 coverage Normal approximation σ 2 heBS b Beta approximation σ 2 heBS Fig 13 Query sample coverage computed 90 Bayesian credible intervals These plots generated queries samples Figs 12a 12d σ 2 heBS Eq 18 instead σ 2 Eq 7 55 Binomial variance Several researchers suggested personal communication variance response m total number tuples q ˆΘ 1q ˆΘ m simpler trivial binomial variance σ 2 heBS 504 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 σ 2 heBS q ˆΘ 1 q ˆΘ m k 1 18 Bayesian framework assume uniform priors query variable k values To investigate claim repeated coverage test Alarm σ 2 Eq 7 Fig 13a resp Fig 13b shows results assuming distribution Normal resp Beta Clearly approximation performs poorly instances This expected Eq 18 matches Eq 7 essentially query variable connected nodes heBS approximation place σ 2 Observation 5 If query variable H kary given uniform priors connected nodes evidence E e σ 2 q ˆΘ 1 q ˆΘ1 k m In general connections variance larger sum include positive quantities 56 Timing information Both BUCKELIM BUCKELIM On expw time complexity w induced treewidth depends ordering While BUCKELIM compute derivatives average size functions running BUCKELIM This BUCKELIM respect functions buckets BUCKELIM For example 100 Alarm queries average cost MEANVAR algorithm 33 times BUCKELIM obviously slower additional cost running BUCKELIM 6 Related work Our results provide way compute variance belief nets response query based posterior distribution Θ determined data sample This Delta method basically propagates variance parameter based partial derivatives Kleiter 26 similarly uses method stochastic simulation technique approximate mean variance query His methodology appears general allowing incomplete data computationally intensive His calculates approximate variance complete query iteration simulation expression derived Delta method While derivation somewhat similar given Appendix B obtain different expression approximate variance We unable verify expressions equivalent Kleiter presents exact result network nodes A B He shows given complete data appropriate prior distribution query PA B Beta distribution In recent work Hooper 24 generalizes result ideas related likelihood equivalence BDe metric 23 given BDe prior complete data query Beta distribution represented CPtable parameter equivalent DAG structure inducing dependence model In addition paper differs Kleiters providing effective way compute variance information BUCKELIM empirical evidence approach typically works effectively despite approximations Several researchers consider posterior distribution CPtables different purposes For example Cooper Herskovits 9 use compute expected response query Eq 3 We extend foundational result computing variance response Similarly BNlearning algorithms compute posterior distribution CPtables 22 seek single set parameters maximizes likelihood different task Others including 42 use response variance tasks including biasvariance analysis probabilistic classiﬁers similar 18 These earlier systems estimate quantity empirically based replicates Θ parameters based set training instances base domain D instantiate parameters compute response ﬁxed question We provide empirical evidence analytic approximations accurate faster compute Most empirical estimates require hundreds thousands replicates means require computation hundreds thousands times computing single response By contrast noted Section 56 approach require total computation small factor slower computing response For complete queries Appendix A additional cost needed compute variance negligible T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 505 Our main theoretical result Theorem 2 uses sensitivity query CPtable entry Many projects consider sensitivity analyses providing mechanisms propagating ranges CPtable values produce range response cf 810122930 Those papers consider variance typically assume parameters intervals propagated network While require user explicitly specify range local CPtable entry work uses data sample source intervals consider intervals based posterior distribution CPtable parameters Θcf turn based observed data This assumes CPtable parameters independent Dirichlet rows accord common models learning belief network parameters While propagate ranges systems propagate single range based associated derivative One exception Darwiche 12 simultaneously pro duce derivatives This compiles belief network polynomial representation symbolically differentiated There appears close similarity approach independently algorithm Section 42 polynomial viewed symbolic representation developed BUCKELIM variable elimination process differentiation proceeds leaves treerepresentation polyno traces time However Darwiche mial root tracing pattern space BUCKELIM consider errorbar application include additional optimizations incorporate Excluding 12 result projects provides efﬁcient way compute partial derivatively Also papers focus properties derivativeeg 0 speciﬁc CPtable entry Note information derived Eq 8 Finally results deal singly connected networks 10 analysis holds arbitrary structures Our analysis connects work abstractions involves determining inﬂuential CPtable entry respect query deciding include speciﬁc node arc 17 Their goal typically computational efﬁciency computing response exactly approximately By contrast focus computing errorbars response independent time required determine results Lastly experiments require instances drawn qΘ estimate approximate credible intervals There applications sampling methods problems belief nets possibly confused concerns For example stochastic sampling inference algorithms conﬁdence intervals posterior 7 refer distribution induced sampling However underlying network parameter uncertainty applications 7 Conclusion Further extensions Our current implemented data indicates reasonably accurate errorbars pro duced There possible extensions One class extensions involves discharging assumptions listed Deﬁnition 1 It possible deal situations single correct structure provided Instead given distribution structures forced learn structure parameters data sample However empirical evidence tasks use approximations 1832 suggest Eq 7 works fairly model structure wrong We assume network parameters Θ decomposed Dirichlet row distributions It useful apply similar analysis alternate CPtable encodings NoisyOR 34 CPTrees 5 The normative process depicted Fig 1 starts completely speciﬁed training instances It clear use incomplete training sample First posterior distribution Θ guaranteed Dirichlet Second clear compute quantities like effective sample size The naïve EM algorithm learning parameters 22 produce effective sample size large Note technique independent source Θ distributions apply given product Dirichlet distributions obtained While takes advantage optimizations available Bucket Elimination algorithms optimized variable orderings bucket data structures initialized query It desirable use multiple query algorithm inference calculation partial derivatives amortize cost initializing tables evidence reuse intermediate computations shared different queries 506 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 QueryDAGs 15 given similarity Bucket Elimination obvious candidate option Junction Trees 25 Appendix A provided simple algorithm computing variance approximation σ 2 special class complete data queries showed use quickly deal query Naïve Bayes structure provided query node root It useful develop specialized algorithms similarly bypass BUCKELIM speciﬁc network topologies Contributions Many realworld systems work reasoning probabilistically based given belief net model When belief net parameters based ﬁnite random sample parameters viewed random variables uncertainty induces uncertainty response given query This paper addresses challenge computing posterior distribution belief nets response query We deﬁne task prove given standard assumptions parameters response distribution asymptotically Normal provide mean asymptotic variance Theorem 2 estimate associated credible regions Bayesian error bars expected response We connect task wellunderstood problem learning belief networks parameters complete data provide algorithm MEANVAR computing asymptotic variance query belief net We prove MEANVAR correct asymptotic complexity belief net inference This procedure effect propagates uncertainty parameter based partial derivative The subroutine compute derivatives independent derivatives uses outside scope paper 15 BUCKELIM 38 We provide simpler straightline algorithm efﬁciently computing variance common situations complete data queries Naïve Bayes inference Our underlying theoretical claims distribution Normal variance σ 2 Eq 7 asymp totic properties distribution given ﬁnite sample We ran body empirical tests investigate performance Our results approximate variance σ 2 extremely close correct given small sample sizes However associated distribution close Normal Our experiments distribution wellapproximated Beta distribution particular associated errorbars typically fairly accurate In earlier works identiﬁed tasks use variance quantities discriminative model selection way combine responses different Bayesian classiﬁers These existing tasks enabled relatively efﬁcient methods approximating variance We eagerly anticipate emergence applications Acknowledgements We grateful comments suggestions received Adnan Darwiche anonymous reviewers All authors gratefully acknowledge generous support NSERC TvA AS acknowledge support iCORE AS acknowledges support National Science Foundation grant IIS 0325581 RG Alberta Ingenuity Center Machine Learning Most work TvA AS students University Alberta Appendix A Complete query case This appendix considers challenge computing variance response query PH h E e simple case query complete evidence E includes variable query H Here straightline code compute variance expected value response Using identity 1219 Θ Ph e qcf Θcf 1 Θcf cid4 cid5 PΘ c f h e PΘ h e PΘc f e T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 507 term PΘ refers value computed belief net parameters instantiated Θ obtain following derivativefree form Eq 8 vheCf cid22 cid2 cid5 cid4 Pc f h e Ph ePc f e 1 cC cf cid5 cid4 Pf h e Ph e Pf e 2 cid23 2 use P P ˆΘ ˆΘ posterior mean We trivial read vheCf Eq A1 0 HC FE The following theorem shows vheCf terms 0 remaining ones trivial compute Theorem 6 Given complete query PH h E e E includes nonH variable non0 vheCF f terms associated query variable H Assuming Hs parents M1 Mk evidence E e includes m m1 mk assignment variables vheH m Ph e2 cid18cid3 Ph e2 ˆΘhm 1 2Ph e ˆΘhm cid19 A2 hH For m cid12 m assignment parents vheH m 0 associated Hs children For child S H parent set H U1 Ur evidence E e includes uuu u1 ur S s vheS h uuu Ph e2 cid5 cid4 2 1 Ph e cid18 1 ˆΘshuuu cid19 1 hcid5 cid12 h vheShcid5 uuu Ph e2 Phcid5 e2 cid18 1 ˆΘshcid5uuu cid19 1 vheSh uuu 0 parental assignment h uuu uuu cid12 uuu A1 A3 A4 Connecting general graph Fig A1 need deal CPtable rows associated H children Si s Notice values inﬂuence variance correspond variables parents exactly Markov blanket H doublecircled triplecircled nodes Fig A1 This makes sense values inﬂuence response Note means Theorem 6 require evidence E include nonquery variable theorem apply need require E include variables Markov blanket H Fig A1 Example Belief Net structure computing PH h E eused illustrate Theorem 6 508 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Here H involves H r classes cid4 children compute variance adding 1 cid4 r quantities Moreover computations easy ˆΘ simply quick lookup Ph e terms trivial given evidence E includes Markov blanket H Moreover quantity probably computed needed compute responsenote basic inference algorithm typically computed Phi e hi H We close section simpliﬁcations First dealing binary classes H h h Eq A2 reduces vheH m Ph e2 Ph e2 cid18 1 ˆΘhm 1 ˆΘhm cid19 Eqs A3 A4 reduce vheSh uuu Ph e2 Ph e2 cid18 1 ˆΘshuuu cid19 1 Hence notation deﬁned total variance simply cid18 cid3 cid3 cid8 cid9cid19 1 1 mHf heΘ Ph e2 Ph e2 σ 2 1 ˆΘhf Second recall ignore barren nodesaka uninstantiated descendants For example imagine evidence E e include value query variables children include values childs descendantseg Fig A1 imagine evidence E include values S1 T1 T2 We ignore branch This follows observation values variables contribute response 31 associated parameters irrelevant variance 1 1 mShuuu 1 ˆΘshuuu 1 S h Note means quickly compute variance approximation Naïve Bayes structure require computing 1 cid4cid5 r quantities cid4cid5 cid2 cid4 number Hs children instantiated Appendix B Proofs Proof Theorem 2 The following detailed version proof 40 employing Delta method Consider ﬁrstorder Taylor expansion qΘ q ˆΘ L R linear term L Θ ˆΘT q cid5 ˆΘ remainder term Θ ˆΘT q R 1 2 cid5cid5 Θ Θ ˆΘ 1 mCf Θ ˆΘ aΘ ˆΘ 0 1 Here qcid5 ˆΘ vector ﬁrst partial derivatives evaluated ˆΘ composed subvectors qCf ˆΘ qcid5cid5 ˆΘ matrix second partial derivatives evaluated ˆΘ superscript T denotes transposition Under asymptotic framework variances components Θcf Θ 0 Θ converges probability ˆΘ Since components ˆΘcf ˆΘ strictly 0 1 order follows components matrix qcid5cid5Θ remain uniformly bounded Θ open neighborhood ˆΘ Consequently remainder term R converges zero rate faster linear term L R asymptotically negligible compared L We approximate qΘ q ˆΘ L deﬁne σ 2 variance L term Now qΘ q ˆΘ σhe L σhe asymptotic distribution We claim distribution standard Normal distribution First note L σhe mean 0 variance 1 Asymptotic normality follows fact CPtable rows Θ T Cf independent row suitable standardization asymptotically multivariate cid24 mCf 1 ΘCf ˆΘCf converges distribution multivariate Normal distribution Normal More precisely ˆΘ T Cf Diag ˆΘCf diagonal matrix mean vector 0 0 covariance matrix Diag ˆΘCf ˆΘCf T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 509 cid3 components ˆΘCf diagonal 3 We express L σhe linear combination independent asymptotically Normal terms In expression standardize independent random variables unit variance We write L σhe r ar Zr r indexes CPtable rows network Zr independent random variables mean 0 variance 1 Zr asymptotically normal The coefﬁcients ar necessarily remain ﬁxed asymptotic framework coefﬁcients satisfy constraint cid2 1 L σhe variance It follows L σhe asymptotically standard normal a2 r The proof completed deriving expression σ 2 deﬁned variance linear term L cid2 Since CPtable rows independent variance decomposed σ 2 ˆΘT CovΘCf qCf qCf ˆΘ B1 Cf summation ranges CPtable rows covariance matrix ΘCf CovΘCf 1 mCf 1 Diag ˆΘCf ˆΘCf ˆΘ T Cf It straightforward obtain Eqs 7 8 cid2 Comment1 Assumptions underlying Theorem 2 The asymptotic framework Theorem 2 involves posterior distribution derivation sample data The following argument shows assumptions supported largesample theory provided Dirichlet prior distribution appropriate Suppose CPtable parameters fact generated assumed prior It follows probability Θcf strictly zero Now consider behavior posterior distribution number complete training cases arbitrarily large In frequentist perspective conditioning parameters sample data effective sample sizes mCf posterior means ˆΘcf random variables mCf arbitrarily large probability ˆΘcf converges probability Θcf The asymptotic framework Theorem 2 similar largesample framework differing primarily assumption appropriate Bayesian context posterior means ﬁxed The text Section 3 argued deal deterministic links asymptotic framework Here basically ignore CPtable rows What happens instead Dirichlet prior incorrectly adopted Largesample theory shows ˆΘ converges boundary The variance approximation Eq 7 valid guaranteed higherorder terms expansion nonnegligible It possible Eq 7 valid asymptotic normality fails For simple example scenario suppose query single belief net parameter degenerate qΘ Θcf 0 Suppose prior distribution Θcf Bea b Use b 1 ﬂat prior There zero probability C F c f posterior distribution Bea b m m number samples F f The posterior variance given Eq 7 exact result approximation A simple calculation shows m posterior distribution qΘ σhe variance 1 If 1 Gamma distribution Gamma distribution shape parameter mean exponential distribution 1 σ 2 Comment2 Why σ 2 σ 2 large σ 2 large Consider queries q1Θ q2Θ corre sponding variances σ 2 2 The larger variance associated q2 arise combination factors First components Θ relevant q2 variable components relevant q1ie Θi terms q2Θ large large covariances The linear approximation describes local Θi havior function qi near mean increased variation produce larger remainder term Second ﬁrst derivatives q2 tend larger q1 In practice larger linear terms associated larger quadratic terms statistical variable selection methods polynomial models seldom delete linear term retaining corresponding quadratic term 35 Larger quadratic terms produce larger remainder term Both arguments support claim approximations variance produced ﬁrstorder Delta method accurate variance larger We note practice largest errors small 510 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 Proof Theorem 3 The proof induction bucket ordering The basis ﬁrst bucket b trivial compute derivative function output g constant 1 To prove inductive step assume computed derivatives functions ﬁrst J buckets know y 1 J 1 fik k Scheme y Schemefik want compute derivatives functions fik fJk bucket bJ Recall BUCKELIM values forming fI1 ElimXJ JoinfJ1 fJk appears earlier bI bucket cid22 cid22 Ordering Step A BUCKELIM Ordering Step B cid22 b bI fI1XI bJ fJ1XJ fJ2XJ fJkXJ cid22 To simplify notation simply write f x f xSchemef Now observe y fJcid4 cid8 cid8 Elim Z Join cid9cid9 y fI1 fI1 fJcid4 B2 B3 Z variables fJcid4 Eq B5 The Join trivial application chain rulealso known backpropagation 37 As I J inductive assumption assume computed ﬁrst term y SchemefI1 Moreover fI1 fI1x ElimXJ JoinfJ1 fJk cid7 arguments Scheme y fI1 k fJkx second term Join Eq B3 xJ cid12 cid2 cid15 x1I xJ Join cid6 fJk k cid12 cid4 fI1 fJcid4 fJkx1I xJ kcid12cid4 B4 product functions J th bucket 41 x1I corresponds variables fI1 involve variables fI1 XJ It joined y fI1 depend The result computation fI1 fJcid4 Eq B3 produce function arguments SchemefI1 XJ As SchemefJcid4 remaining step marginalize extra variables y fJcid4 Z SchemefI1 XJ SchemefJcid4 B5 second Eq 14 This fulﬁlls second inductive step insures Scheme y Jcid4 SchemefJcid4 cid2 needs consider 2n functions Step A BUCKELIM starts Proof Theorem 4 BUCKELIM function CPtable n variables adds new function time variable eliminated We need cost computing derivative functions Or w r largest domain variables w treewidth We consider general case notation Eq B2 fI1 ElimXJ JoinbXJ Combining Eqs B3 B4 obtain cid8 cid8 Elim Z Join y fJcid4 y fI1 fJk k cid12 cid4 cid9cid9 B6 Now consider variables involved join Scheme SchemefI1 cid8 cid9 y fI1 SchemefJk SchemefI1 XJ k T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 511 means entire computation involve 1 SchemefI1 variables 1 w SchemefI1 treewidth w Therefore joining required tables Eq B6 requires Or w1 Or w time space r constant Of course join computation algorithm marginalize variables Z Eq B5 produce function derivative w variables The cost marginalization cost join overall complexity computing single derivative Or w cid2 Proof Observation 5 As noted need consider CPtables nodes dseparated query node H means need consider single H node The variance σ 2 Ph 1 Ph1 mH Recall variance associated single Dirichletdistributed variable σ 2 H Ph q ˆΘ As H parents n training instances contributes Dirichlet parameters As H kary start uniform priors means mH k n Note derive value Eqs A2 A1 cid2 Proof Theorem 6 As sweep vheCf terms need consider cases depending query variable H H C ii H appears F iii Case iii If H cid12 C H F vheCF f 014 Here evidence variables E includes variable H includes C F To simplify notation write E c f indicate speciﬁc assignments f f1 fj refers Es assignment Cs parents Notice vheC f 0 f cid12 f cid6 c F f h F f Pc f h e P cid7 0 Similarly Pc f e 0 Pf h e 0 Pf e 0 We consider F f Here Pf h e Pf h f Ph e Pf e 1 use reduce line Eq A1 cid5 cid4 Pf h e Ph e Pf e 2 cid4 Ph e Ph e 1 cid5 2 0 Now consider Eq A1 observe need consider value c value C summation For c cid12 c Pc f h e Pc f h c 0 As Pc f h e Pc h e Pc f e Pc e Hence cid3 cC 1 ˆΘcf 1 ˆΘcf cid5 cid4 2 Pc f h e Ph e Pc f e cid5 cid4 Pc f h e Ph e Pc f e 2 1 ˆΘcf cid4 Ph e Ph e 1 cid5 2 0 Case If H C Hs parents M Mi vheH m Ph e2 cid18cid3 hH Ph e2 ˆΘhm 1 2Ph e ˆΘhm cid19 vheH m 0 m cid12 m As argued vheHm value 0 M m cid12 m value e When M m omit M m equations means second line Eq A1 0 Hence value query variable h c 14 Note follows observation condition means HC FE We include case provides useful analysis notation 512 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 vheH m cid5 cid4 Ph m h e Ph e Ph m e 2 cid3 hH 1 ˆΘhm cid5 cid4 1 Ph e Ph e Ph e ˆΘhm cid3 2 cid5 cid4 Ph h e Ph e Ph e 2 1 ˆΘhm cid5 cid4 Ph e1 Ph e hcid12h 1 ˆΘhm Ph e2 Ph e2 cid18 1 Ph e2 ˆΘhm Ph e2 ˆΘhm hH cid18cid3 cid3 hcid12h Ph e2 ˆΘhm cid19 1 2Ph e ˆΘhm hcid12h cid3 2 cid5 cid4 2 Ph e Ph e 1 ˆΘhm cid19 Case ii If H F write Cs parents F H UUU assume evidence E e contains UUU uuu C c Using arguments know vheCh uuu 0 uuu cid12 uuu We consider UUU uuu reduce Eq A1 cid3 vheCh uuu cC cid4 cid5 Pc h uuu h e Ph ePc h uuu e 1 ˆΘchuuu cid5 cid4 2 Ph uuu h e Ph e Ph uuu e 2 cid5 cid4 2 Ph h e Ph e Ph e 1 ˆΘchuuu cid5 cid4 2 Ph h e Ph e Ph e This uses fact term summation c cid12 c 0 If H h vheC h uuu cid4 cid5 Ph e Ph ePh e 1 ˆΘchuuu cid5 cid4 Ph e Ph e Ph e 2 Ph e1 Ph e2 hcid5 cid12 h vheChcid5 uuu cid5 cid4 Ph ePhcid5 e 2 cid18 1 ˆΘchcid5uuu cid19 1 References cid18 1 ˆΘchuuu 2 cid19 1 cid2 1 B Abramson J Brown W Edwards A Murphy RL Winkler Hailﬁnder A Bayesian forecasting severe weather Intl J Fore casting 12 1 March 1996 5771 2 TW Anderson DA Darling A test goodness ﬁt J Amer Statist Assoc 49 268 December 1954 765769 3 Y Akimoto A note uniform asymptotic normality Dirichlet distribution Math Japon 44 1 December 1996 2530 4 AC Atkinson Plots Transformations Regression Oxford 1985 5 C Boutilier N Friedman M Goldszmidt D Koller Contextspeciﬁc independence Bayesian networks Twelfth Annual Conference Uncertainty Artiﬁcial Intelligence UAI96 1996 6 J Binder D Koller SJ Russell K Kanazawa Adaptive probabilistic networks hidden variables Machine Learning 29 23 1997 213244 T Van Allen et al Artiﬁcial Intelligence 172 2008 483513 513 7 J Cheng MJ Druzdzel Conﬁdence inference Bayesian networks Seventeenth Conference Uncertainty Artiﬁcial Intelligence UAI2001 Morgan Kaufmann Publishers 2001 pp 7582 8 EF Castillo JM Gutiérrez AS Hadi Sensitivity analysis discrete Bayesian networks IEEE Trans Man Cybernet Syst 27 1997 412424 9 G Cooper E Herskovits A Bayesian method induction probabilistic networks data Machine Learning 9 1992 309347 10 P Che RE Neapolitan J Kenevan M Evens An implementation method computing uncertainty inferred probabilities belief networks 9th Annual Conference Uncertainty Artiﬁcial Intelligence UAI93 Morgan Kaufmann Publishers 1993 pp 292300 11 DA Darling The KolmogorovSmirnov Cramervon Mises tests Ann Math Stat 28 1957 823838 12 A Darwiche A differential approach inference Bayesian networks 16th Annual Conference Uncertainty Artiﬁcial Intelligence UAI93 2000 13 March 1995 Special issue Communications ACM Bayesian Networks 14 R Dechter Bucket elimination A unifying framework probabilistic inference Learning Inference Graphical Models 1998 15 A Darwiche GM Provan Query DAGs A practical paradigm implementing belief network inference Twelfth Conference Uncer tainty Artiﬁcial Intelligence UAI96 1996 16 RB DAgostino MA Stephens GoodnessofFit Techniques Statistics Textbooks Monographs vol 68 Marcel Dekker Inc 1986 17 R Greiner C Darken I Santoso Efﬁcient reasoning Computing Surveys 2001 18 Y Guo R Greiner Discriminative model selection belief net structures Twentieth National Conference Artiﬁcial Intelligence AAAI05 Pittsburgh July 2005 pp 770776 19 R Greiner A Grove D Schuurmans Learning Bayesian nets perform 13th Annual Conference Uncertainty Artiﬁcial Intelligence UAI97 1997 20 httpwwwcsualbertacagreinerRESEARCHBNvar 21 EH Herskovits CF Cooper Algorithms Bayesian beliefnetwork precomputation Methods Information Medicine 1991 pp 362370 22 DE Heckerman A tutorial learning Bayesian networks MI Jordan Ed Learning Graphical Models 1998 23 D Heckerman D Geiger DM Chickering Learning Bayesian networks The combination knowledge statistical data Machine Learning 20 1995 24 P Hooper Exact distribution theory belief net responses Technical report University Alberta 2007 httpwwwstatualberta cahooperresearchpaperstalksexactbetapdf 25 F Jensen F Jensen Optimal junction trees Tenth Annual Conference Uncertainty Artiﬁcial Intelligence UAI94 Morgan Kauf mann San Francisco CA 1994 26 G Kleiter Propagating imprecise probabilities Bayesian networks Artiﬁcial Intelligence 88 1996 27 RL Keeney H Raiffa Decision Multiple Objectives Preferences Value Tradeoffs John Wiley Sons New York 1976 28 H Korth A Silberschatz S Sudarshan Database System Concepts McGraw Hill 1998 29 U Kjaerulff LC Van der Gaag Making sensitivity analysis computationally efﬁcient 16th Conference Uncertainty Artiﬁcial Intelligence UAI00 2000 30 KB Laskey Sensitivity analysis probability assessments Bayesian networks IEEE Trans Man Cybernet Syst 25 6 1995 901909 31 Y Lin MJ Druzdzel Computational advantages relevance reasoning Bayesian beliefs networks Uncertainty Artiﬁcial Intelligence UAI97 1997 pp 342350 32 C Lee R Greiner S Wang Using variance estimates combine Bayesian classiﬁers International Conference Machine Learning ICML06 Pittsburgh June 2006 33 AW Moore Private communication May 2003 34 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann 1988 35 JL Peixoto Hierarchical variable selection polynomial regression models American Statistician 41 1987 36 B Ripley Pattern Recognition Neural Networks Cambridge University Press Cambridge UK 1996 37 DE Rumelhart JL McClelland PDP Research Group Eds Parallel Distributed Processing Explorations Microstructure Cognition vol 1 Foundations The MIT Press Cambridge 1986 38 AP Singh What dont data Issues small sample parameter learning Bayesian networks Masters thesis University Alberta 2004 39 DJ Spiegelhalter SL Lauritzen Sequential updating conditional probabilities directed graphical structures Networks 1990 579605 40 T Van Allen R Greiner P Hooper Bayesian errorbars belief net inference 17th Conference Uncertainty Artiﬁcial Intelligence UAI01 Aug 2001 41 T Van Allen Handling uncertainty youre handling uncertainty Model selection error bars belief networks Masters thesis Department Computing Science University Alberta 2000 42 GI Webb P Conilione Estimating bias variance data Technical report 2005 43 S Wilks Mathematical Statistics John Wiley Sons 1962