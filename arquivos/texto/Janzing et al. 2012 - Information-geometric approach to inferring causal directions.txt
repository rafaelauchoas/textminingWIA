Artiﬁcial Intelligence 182183 2012 131 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Informationgeometric approach inferring causal directions Dominik Janzing Povilas Daniušis e Bastian Steudel f Bernhard Schölkopf Joris Mooij b Kun Zhang Jan Lemeire cd Jakob Zscheischler Max Planck Institute Intelligent Systems Tübingen Germany b Radboud University Nijmegen Netherlands c Vrije Universiteit Brussel Brussels Belgium d Interdisciplinary Institute Broadband Technology Ghent Belgium e Vilnius University Lithuania f Max Planck Institute Mathematics Sciences Leipzig Germany r t c l e n f o b s t r c t Article history Received 21 December 2010 Received revised form 10 January 2012 Accepted 10 January 2012 Available online 12 January 2012 Keywords Deterministic causal relations Pythagorean triple Causeeffect pairs While conventional approaches causal inference mainly based conditional independences recent methods account shape conditional distributions The idea causal hypothesis X causes Y imposes marginal distribution P X conditional distribution P Y X represent independent mechanisms nature Recently postulated shortest description joint distribution P XY given separate descriptions P X P Y X Since description length sense Kolmogorov complexity uncomputable practical implementations rely notions independence Here deﬁne independence orthogonality information space This way explicitly kind dependence occurs P Y P XY making causal hypothesis Y causes X implausible Remarkably asymmetry cause effect particularly simple X Y deterministically related We present inference method works case We discuss theoretical results nondeterministic case clear employ general inference method 2012 Elsevier BV All rights reserved 1 Introduction The problem inferring X causes Y write X Y Y causes X observations x1 y1 xm ym iid drawn P XY particularly challenging task causal inference 1 Although restricted problem ignores important problems causal inference unobserved common causes bidirectional inﬂuence useful studying statistical asymmetries cause effect Conventional methods causal inference 23 focus conditional independences require observations variables Extending idea 45 postulates X Y acceptable causal hypothesis shortest description P XY given separate descriptions P Y X P X Here description length understood sense algorithmic information Kolmogorov complexity 68 Note postulate equivalent saying P Y X P X algorith mically independent sense knowing P X enable shorter description P Y X vice versa To helps distinguishing cause effect observed variables 5 constructed toy models causal mechanisms causal structure X Y yields algorithmic dependences P XY P Y Even Corresponding author Email address dominikjanzingtuebingenmpgde D Janzing 00043702 matter 2012 Elsevier BV All rights reserved doi101016jartint201201002 2 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Section Contents Section 3 Section 4 Postulating independence conditions h1h3 P cause P effectcause Justifying conditions Rephrasing h1h3 orthogonality Implications h3 deterministic causality Generalizing h3 exponential families Inference method deterministic case based generalized condition h3 Main reference Postulate 1 Deﬁnition 1 Lemmas 1 2 Theorem 1 Theorem 2 Postulate 2 Sections 43 44 Appendix A Outlook employing orthogonality inferring non deterministic relations toy examples negative results Lemmas 9 10 Fig 1 Structure main results algorithmic independence P cause P effectcause appealing formalization independence practical methods based computable criteria 910 described potential asymmetry cause effect independence meant terms statistical independence cause noise term occurs causal mechanism If Y function X additive noise term statistically independent X Y f X E E X 1 usually exceptions like bivariate Gaussian additive noise model Y X In words writing X X gY E function g render residual term E statistically independent Y 11 generalizes model class cid2 Y h f X E cid3 E X 2 postnonlinear PNL model exists direction special cases If P XY consistent 1 2 respectively direction infers direction causal implied corresponding model For model 1 shown 12 kind reasoning justiﬁed algorithmic independence principle Note inference methods assume causal relations form They decide causal directions direction admits model The idea following X Y correct model additive noise form unlikely generates joint distribution admits additive noise model opposite direction The reason require contrived adjustments P X marginal distribution hypothetical cause P Y X conditional distribution effect given cause 12 This article develops informationgeometric principle require restricted class additive noise postnonlinear models To end revisit additive noise models Section 2 entropies play key role describing kind dependences P XY P Y occur X causes Y This motivates informationgeometric perspective developed Section 3 results inference method deterministic causal relations Section 4 outlook nondeterministic case Appendix A The table Fig 1 shows main results structured Readers interested inference method focus Section 4 Sections 43 44 main parts The sections provide general background large class asymmetries cause effect helpful developing informationtheoretic methods future 2 Informationtheoretic view additive noise models We consider additive noise model 1 low noise regime Fig 2 relationship input distribution conditional different directions We use following notational conventions P Y x distribution Y given ﬁxed value x P Y X denotes entire conditional distribution The range random variable X denoted D X SP Y x denotes differential Shannon entropy P Y x ﬁxed x The function x cid5 SP Y x called conditional entropy function Throughout paper assume distributions densities respect ﬁxed reference measure Lebesgue measure realvalued variables counting measure discrete variables This measure appear explicitly confused reference probability distributions occur article By slightly overloading notation P X stand distribution density x cid5 P X x We write P x instead P X x causes confusion P x dx understood sums interpreting dx dμx μ For discrete variables X integrals form denotes counting measure Regarding 1 observe E X ensures conditional entropy function SP Y x constant x coincides SP Y xP x dx In studying P Y P XY y y large yvalues f conditional entropy SP Y X deﬁned average related ﬁrst assume P X uniform Then P y P X f 1 y f 1 1 cid4 cid4 cid7 cid7 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 3 Fig 2 Functional relation small noise The conditional entropy function SP X y high regions high slope f point 1 y small slope f large At time entropy SP X y large yvalues regions large f y Fig 2 Hence large entropy SP X y correlates high density P y assuming P x constant interval consideration If 1 y high We P X uniform distribution high values P y occur points f argue later peaks P x correlate slope f qualitative argument holds SP X y correlates P y This reasoning formalized Section 3 y P X f 1 cid7 cid7 1 The ﬁrst informationgeometric inference principle going state section longer assumes entropy SP Y x constant x X Y true causal direction Instead postulates regions large SP Y x correlate regions large density P x The example shows dependences P Y P XY occurring wrong causal direction appear level correlations informationtheoretic expressions like conditional entropy computed conditional P X y density P y We correlations type phrased orthogonality relation sense information geometry 3 A class testable independence relations The intention section postulate independence conditions P Y X P X tested empirically We options solve task 31 General structure independence relations The following postulate describes general structure postulates share Postulate 1 General structure independence Assume X causes Y Let x cid5 hx R function hx describes local properties conditional P Y X point X x1 Then structure function h P X likely satisfy hxP x dx hxU X x dx 3 U X reference density X necessarily uniform Note difference sides 3 rephrased covariance formally consider h P X U X cid5 cid5 cid5 cid5 functions random variable X distribution U X cid5 cid5 hxP x dx hxU x dx U x dx hxU x dx P x U x U x dx 4 cid5 P x hx U x cid6 CovU X h cid7 P X U X Therefore 3 formalizes uncorrelatedness functions h P X U X justiﬁed idea way P X differs U X independent h The postulate remains vague choose h U X We later discuss different reasonable choices cid7x instance hx SP Y x nondeterministic relations hx f cid7x deterministic ones hx log f 1 Note avoided concise formulation hx describes properties conditional P Y x following reason For deterministic relations Y f X function hx f cid7x expresses property P Y X local X x hx derived P Y x 4 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Fig 3 Visualization options intervalwise generation conditional P Y X dice throws r j Left P Y X corresponding Y X E distribution Ex uniform distribution interval 0 hx Right dice determines slope f Y f X f cid7x hx deterministic monotonically increasing relations We recommend choose noninformative distributions like uniform ones Gaussians U X If assume typical choices P X long choice independent h yield cid7 integral assume changing U X U X matter long chosen U cid7 X independently h This suggests robustness changing reference measure 32 Probabilistic models justiﬁcation Even specifying reference density U X map conditional P Y X structure function h mathematical justiﬁcation 3 given probabilistic model nature chooses P X chooses P Y X To consider random process generates functions h equivalently seen generating random conditionals P Y X Lemma 1 Intervalwise random generation P Y X Let X Y realvalued Let r j 0 j Z random numbers iid drawn distribution Q r standard deviation σr We deﬁne piecewise constant function h hx r j x j j 1 Fig 3 shows options h correspond conditional P Y X We c 0 cid9 cid10 cid10 cid10 cid10 cid11 cid8 cid8 cid8 cid8 cid2 cσr hxU x dx P x U x dx hxP x dx cid13 j1cid5 cid8 cid5 cid8 cid8 cid8 cid142 cid12 cid5 probability 1 1c2 higher Proof This cid5 cid2 hx cid3 P x U x dx j j cid12 r j j cid13 j1cid5 cid14 P x U x dx j sum independent random variables having variance cid13 j1cid5 cid142 σ 2 r P x U x dx j Then statement follows Chebyshevs inequality noting expected value ishes cid2 cid4 hxP x U x dx van The example instructive shows 3 likely hold regardless P X U X provided following conditions satisﬁed First distributions P X U X chosen independently independently P Y X P x U x dx2 small Roughly speaking Second distributions suﬃciently spread β P X U X width n β O 1n 3 holds error O 1 n Neglecting conditions easily construct counter examples First distributions P X U X U X constructed having seen r j U X constructed P X U X positive intervals j j 1 r j large cid4 j j1 j cid15 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 5 cid4 cid4 negative small r j This results distributions U X supported interval j j 1 depends single r j strongly deviate cid4 hxU x dx systematically greater hxP x dx cid4 hxU x dx Second hxU x dx r j right hand 3 One construct probabilistic model P Y X h ﬁxed instead P X generated randomly instance following procedure On interval j j 1 multiply U x random number r j Then renormalize obtained function obtain P x If U X spread intervals 3 holds high probability We skipped detailed analysis example technical The following model assumes P X chosen prior invariant group action Lemma 2 Symmetric prior Let G group acting domain X PP X probability density set distributions P X Ginvariant PP X PP X P X denotes average P X action G Then ﬁxed h cid5 cid5 EP hxP X x dx EP hxP X x dx EP denotes expectation prior P The result follows immediately linearity expectation It suggests choose P X reference measure believes Ginvariant prior appropriate The fact expectations sides 3 coincide hxP X x dx close high probability However suﬃciently necessarily guarantee large groups follows concentrationofmeasure results 13 14 similar statement rotations highdimensional spaces To elaborate general groups scope paper hxP X x dx cid4 cid4 We seen degree trust 3 heavily relies particular probabilistic models gener ating P X P Y X Therefore provide conﬁdence levels valid referring models After deciding instance example Lemma 1 good model generation P Y X need estimate size intervals correspond independent random experiments Then believe 3 interval sizes suﬃciently small compared width P X U X Example 2 Section 4 shows context deterministic relations violation 3 easily happen simple P Y X P X P X U X differ large regions We want mention Postulate 1 fail intelligent design P X P Y X This fundamental limitation approach wellknown postulates causal inference like causal faithfulness 2 33 Independence orthogonality information space Our structure functions relativeentropylike expressions turned helpful formalizing asymmetries cause effect We introduce terminology For densities P Q P absolutely continuous respect Q relative entropy KLdistance deﬁned cid5 DP cid10 Q log We deﬁne P w Q w P w dw cid3 0 Deﬁnition 1 Structure functions conditional Let U X U Y reference densities X Y respectively denote output distribution obtained feeding conditional reference input U X Similarly later use cid5 P Y cid5 P x P yxU x dx P x yU y dy cid5 P yx dy DP Y x cid10 U Y log h1x Then deﬁne following structure functions P yx U y P yx P y P y U y P yx dy DP Y x cid10 h3x h2x log log P yx dy h1x h2x P Y cid5 cid5 6 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 The reason list functions represented terms yield conditions interpretation terms information geometry relying following concept Three densities P R Q said form Pythagorean triple distributions DP cid10 Q DP cid10 R DR cid10 Q 5 This terminology motivated interpreting relative entropy squared distance triple satisﬁes Pythagorean theorem If condition 5 holds vector connecting P R orthogonal necting R Q orthogonal mind relation symmetric respect exchanging vectors respect reversing arrows We use following formulation Lemma 3 Orthogonality information space Orthogonality 5 equivalent cid5 log Rw Q w cid5 P w dw log Rw Q w Rw dw 6 The proof given straightforward computation In analogy interpretation 3 interpret 6 integral log term depend weighted P R We ﬁnd Theorem 1 Three orthogonality conditions The conditions CovU X hi P X U X 0 1 2 3 equivalent DP Y X cid10 U X U Y h1 DP Y X cid10 U X P Y X DU X P Y X cid10 U X U Y h2 DP Y X cid10 U X P Y X DU X P Y X cid10 U X P Y cid10 U Y P Y D P Y h3 DP Y cid10 P Y DP Y X cid10 U X DP Y cid10 U Y Proof Using Lemma 3 cases h1 h2 straightforward computations For case h3 note cid5 P y U y P y U y log log cid5 P yxP x dx dy log cid5 P yxU x dx dy log P y U y P y U y P y dy P y dy cid2 cid5 To geometrically justify orthogonality assumption h1 consider space V functions x y identify distribution Q XY point cid2 cid3 x y cid5 log Q x y V Then observe difference vector connecting points P Y X U X P Y X depends P X sense common term P Y X cancels taking difference points vector pointing U X P Y X U X U Y depends P Y X In highdimensional spaces likely vectors close orthogonal chosen independently according uniform prior Even know precise statement form respect informationgeometric orthogonality accept leading intuition interpretation uncorrelatedness given Theorem 1 Regarding h2 argue similar way The fact joint distributions occurring points U X P Y X U X P Y contain P X makes plausible vector orthogonal vector depends P X How geometrically interpret orthogonality given h3 clear essential Section 4 applicable deterministic case Condition h1 outlook Appendix A A simple example reasonable reference measure uniform distribution interval b It natural choice data points priori restricted b For example conditional relative entropy reduces conditional Shannon entropy Example 1 Uniform reference measure Let range X Y restricted interval 0 1 U X U Y uniform distributions 0 1 Then orthogonality condition h1 Theorem 1 equivalent cid5 SP Y xP x dx 1cid5 0 SP Y x dx 7 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 7 Fig 4 If structure density P X correlated slope f ﬂat regions f implausible causal mechanism f 1 appears adjusted input distribution P Y induce peaks P Y The causal hypothesis Y X cid2 cid3 SP Y x P x 0 CovU X 8 Hence 7 states regions high entropy SP Y x correlate regions high density P x If P Y X P X chosen independently assume independence assumption approximately hold For additive noise models satisﬁed 1 implies SP Y x constant x We given intuitive argument Fig 2 7 violated backward direction low noise regime We deﬁne group cyclic shifts Tt t01 Tt x x t mod 1 having uniform reference measure unique invariant measure Then covariance 8 vanishes average shifted copies P X cf Lemma 2 result saying holds shifted copies approximately To extent orthogonality relations approximately satisﬁed realworld causeeffect pairs answered extensive empirical studies An interesting theoretical question cases orthogonality direction imposes violation orthogonality converse direction The simplest model class conﬁrmed given deterministic invertible relations 15 A remarkable fact backward direction h3 positively correlated hypothetical input density fact output Appendix A discusses cases relation cause effect bijective deterministic direction There able violations orthogonality backward direction additional independence conditions P X P Y X orthogonality postulates turn necessary 4 Deterministic invertible relation The bijective case Y f X X f 1Y particularly challenging causal inference First absence noise makes additive noise model based inference impossible 9 second methods use noninvertibility functional relation fail 16 Surprisingly hopeless noiseless invertible case theory turns elegant violation orthogonality conditions backward direction follows easily orthogonality forward direction Moreover simulations suggest corresponding inference method robust respect adding noise empirical results noisy realworld data known ground truth positive This section largely follows conference paper 15 puts ideas broader context contains systematic experimental veriﬁcations 41 Motivation We start motivating example For realvalued variables X Y let Y f X invertible differentiable function f Let P X chosen independently f Then regions high density P Y correlate regions f small slope Fig 4 The following lemma phenomenon explicit Lemma 4 Correlations slope density Let Y f X f differentiable bijection 0 1 differentiable inverse f cid5 P X uncorrelated sense 1 If log f cid5 cid7 log f cid7 xP x dx log f cid7 x dx 9 log f cid5 1cid7 cid2 log f cid3cid7 1 P Y positively correlated cid5 xP y dy cid7 log f y dy f identity 8 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Fig 5 Left violation 9 global deviation P X uniform measure Right P X oscillating constant density ensures uncorrelatedness Note terminology uncorrelated justiﬁed interpret f P X random variables probability space 0 1 uniform measure interpretation 3 uncorrelatedness The lemma actually follows general results shown later proof elementary helpful cid7 1cid5 0 cid2 log f cid3cid7 1 yP y dy 1cid5 cid2 log f cid3cid7 1 y dy log f 1cid5 0 1cid5 log f cid7 cid7 0 1cid5 xP x dx log f cid7 x f cid7 x dx 0 1cid5 x dx log f cid7 x f cid7 x dx 0 0 cid3 cid7 f x 1 log f cid7 x dx cid3 0 1cid5 cid2 0 The ﬁrst equality uses standard substitution exploits fact cid2 log f cid3cid7cid2 1 cid3 f x log f cid7 x 10 The second equality uses assumption 9 inequality follows integral nonnegative Since vanish Z constant entire statement Lemma 4 follows Peaks P Y correlate regions large slope f X cause One observation easily generalized case f bijection sets higher dimension Assuming P X uncorrelated logarithm Jacobian determinant log f implies P Y positively correlated log f 1 f 1 small slope Before embedding insights informationgeometric framework example idea fails Example 2 Failure uncorrelatedness Let f piecewise linear f Then cid7x x x0 f cid7x b x cid3 x0 1cid5 0 log f cid7 xP x dx 1cid5 0 cid7 cid2 x dx log log b cid2 cid3 0 x0 P X cid3 x0 log f Therefore uncorrelatedness fail spectacularly P X 0 x0 x0 large meaning P X uniform measure differ larger scale Fig 5 left If P X oscillates locally 1 holds Fig 5 right The fact logarithm slope turned particularly convenient 10 intimately related P X straightforward generalizations determin 1 respectively If U X U Y uniform distributions informationgeometric framework We ﬁrst observe istic case images U X U Y f g f 0 1 given P Y P y g y P x f cid7 x We obtain 9 equivalent 1cid5 0 1cid5 cid7 log g yP y dy log g cid7 yg cid7 y dy 0 11 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 9 transformed P y U y log 1cid5 0 1cid5 P y dy log P y U y P y dy 0 equivalent orthogonality condition h3 One easily think mechanisms nature violate model choosing function f input distribu tion P X independently P X result intelligent design long adaption process like evolution biological systems If reward optimized controlling value y P X time shifted regions f P X f spectacularly violate 9 Such effects imply fundamental limitation large slope method 42 Identiﬁability results Here rephrase theory developed 15 elaborate asymmetries cause effect Orthogonality h3 Theorem 1 applicable deterministic case refers image uniform input density conditional P Y X exists deterministic case refer conditional density P yx exist correspond deltafunction Condition h3 rephrased different ways Theorem 2 Equivalent formulations orthogonality h3 For bijective relations following conditions equivalent I Orthogonality h3 Theorem 1 P Y D DP Y cid10 U Y DP Y cid10 P Y cid10 U Y II Uncorrelatedness input transformed density cid6 cid7 P X U X P X U X 0 CovU X log III Transformed orthogonality DP X cid10 P X DP X cid10 U X DU X cid10 P X IV Additivity irregularities DP Y cid10 U Y DP X cid10 U X D P Y cid10 U Y V Additivity approximation error DP X cid10 P X DP Y cid10 P Y D P Y cid10 U Y cid5 Proof Condition 13 equivalent P x U x P x U x P x dx log log cid5 U x dx 12 13 14 15 16 4 Due Lemma 3 equivalent 14 The equivalence 12 14 immediate applying f distributions 12 relative entropy conserved bijections Equivalence 15 12 1 term left obtained applying f f ﬁrst term right hand 15 transformed 16 cid2 1 ﬁrst term right hand 12 By applying f 1 Later section generalization condition 15 essential postulate For reason mention idea distance DP X cid10 U X measures irregularities input distribution D P Y cid10 U Y quantiﬁes irregularities function The irregularities output given sum terms This irregularities input function independent interfere constructively destructively P Y error approximating P Y Condition 16 admits interesting interpretation assuming U X U Y given smoothing P X P Y respectively DP Y cid10 P Y image smoothed input Then 16 implies output sensitive smoothing input vice versa imagine case peaks P Y stem P X f By smoothing peaks caused f generate additional peaks P X smoothing ones P X removes P Y peaks nonsmoothed P X For interpretations essential relative entropy nonnegative 10 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Fig 6 The orthogonality condition I inconsistent analog orthogonality backward direction f P X required gular angle information space P Y implies rectangular angle U X 1 preserves distances rectan Theorem 3 Violations backward direction Let f nontrivial sense image U X f coincide U Y If condition Theorem 2 holds corresponding conditions exchange role X Y violated deﬁnite sign CovU Y DP X cid10 cid6 P X D P X cid10 U X DP X cid10 U X P Y P Y U Y U Y log cid7 0 P Y DP Y cid10 P Y P X cid10 U X DP X cid10 U X P X cid10 U X DP Y cid10 P Y DP Y cid10 U Y DU Y cid10 DP Y cid10 U Y D P X D DP X cid10 17 18 19 20 21 Proof Reordering 14 yields DP X cid10 U X DP X cid10 P X cid10 U X P X D P X DU X cid10 1 terms 20 21 follow showing inequality 17 Inequalities 1921 follow applying f directly 15 16 respectively 18 follows left hand difference right 1 hand left hand 19 The fact 12 implies 17 seen Fig 6 Moreover fact f conserves shape triangle shows discrepancy sides 17 given symmetrized relative entropy P X DP X cid10 P X cid10 U X DU X cid10 D P X cid2 Generalization reference manifolds 22 The choice reference measure delicate method structure distribution P X represented vector connecting P X U X The uniform distribution certain interval reasonable choice range respective variable priori restricted interval If realvalued variable unbounded range ﬁnite variance Gaussian mean variance P X natural candidate U X likewise Y However U X depends P X mean variance A better way expressing given introducing families reference distributions having single reference distribution We measure irregularities distance P X exponential family Gaussians represent structure P X vector connects P X closest point manifold The family Gaussians example reasonable choice Even turn useful cases theory phrased terms general exponential manifolds Deﬁnition 2 Exponential manifolds Let Ω Rd assume ﬁnitedimensional vector space V functions f Ω R given Then V deﬁnes exponential manifold E set probability densities written as2 P ω e vω ω Ω 2 It common use slightly general deﬁnitions 17 exponent contains ﬁxed additional function V Our formulation ensures E contains constant density Ω ﬁnite measure D Janzing et al Artiﬁcial Intelligence 182183 2012 131 11 For density P DP cid10 E denotes inﬁmum DP cid10 Q Q E If Q DP cid10 E DP cid10 Q called projection P E Note projection unique exists 17 Given appropriate reference manifolds X Y formaliz ing set smoothest distributions inference method based following assumption Postulate 2 Orthogonality reference manifolds Let E X EY reasonable reference manifolds X Y respec tively If X causes Y conditions Theorem 2 hold approximately U X U Y projections P X P Y E X EY respectively For reference manifolds instead single reference distributions postulate requires slightly different justiﬁcation This explained Appendix B The choice reference manifold point prior knowledge respective domain enters method way choice single reference measure theory developed previously Choosing family Gaussians following interesting feature distance closest Gaussian deﬁnes scale locationinvariant measure irregularities P X Choosing manifold smaller set Gaussians information location scale choosing larger manifold remove scale locationinvariant information P X This Gaussians natural choice onedimensional case For multidimensional variables X Y later manifold Gaussians large removes information relative scaling different components variable X Y In case choose proper submanifold 43 Inference method general form Having derived long list asymmetries cause effect chose convenient inferring causal direction To end observe additivity irregularities 15 obviously implies DP X cid10 U X cid2 DP Y cid10 U Y X causes Y Generalizing reference manifolds Postulate 2 implies DP X cid10 E X cid2 DP Y cid10 EY 23 P Y cid10 U Y 0 function simple image U X U Y Therefore equality D inference method reads InformationGeometric Causal Inference IGCI Let E X EY manifolds smooth reference distributions X Y respectively Consider distances P X P Y E X EY respectively complexity distributions Deﬁne complexity loss P X P Y C XY DP X cid10 E X DP Y cid10 EY 24 Likewise loss P Y P X given exchanging roles X Y Then infer X causes Y C XY 0 Y causes X C XY 0 To rule applicable ﬁrst derive explicit forms C XY refer general reference manifolds Section 44 describes estimators empirical data refer particular reference manifolds Lemma 5 C XY difference Shannon entropies Let P X P Y densities Rd Assume U X U Y projections P X E X P Y EY respectively Then C XY cid2 cid2 cid3 SU X SU Y cid3 SU X SP X cid2 cid2 cid3 SP X SP Y cid3 SU Y SP Y Proof Since U X projection P X E X cid5 DP X cid10 E X DP X cid10 U X SP X P x log U x dx SP X SU X 25 26 27 To derive equation ﬁrst assume P X densities E X compact support Λ Rd Then E vector space deﬁning E clearly contains constant function x cid5 0 contains uniform distribution U 0 Because U X projection P X E X P X U X U X form Pythagorean triple 18 Using Lemma 3 obtain 0 X 12 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 cid4 P x log U x dx SU X For noncompact supports consider restrictions densities increasing se quence compact subsets Λn The statement follows limit n cid2 The entropy difference X Y rewritten follows Lemma 6 C XY mean log Jacobi determinant If f diffeomorphism submanifolds Rd cid8 cid8 cid8det f x cid8P x dx log C XY SU X SU Y cid5 notations Lemma 5 Proof The entropy Y f X reads cid8 cid8 cid8det f x cid8 dx SP Y SP X P X x log cid5 C XY cid2 cid3 SU X SP X cid5 SU X SU Y cid2 cid3 SU Y SP Y cid8 cid8 cid8det f x cid8P x dx log cid2 28 Note C XY invariant joint rescaling P X U X likewise P Y U Y SU X changes additive constant det f sign In subsection discuss important cases domains X Y possible choices reference manifolds empirically estimate ˆC XY 44 Inference method explicit form reference measures R Lemmas 5 6 reduce estimation C XY C Y X estimating entropies Jacobians respectively In paper mainly concerned onedimensional continuous variables We explicit form estimators case experiments For completeness discuss situations Section 45 propose corresponding reference measures Uniform reference measure intervals For motivating example Section 41 X Y attain values 0 1 Lemmas 5 6 imply following simple versions IGCI 1 Entropybased IGCI infer X Y SP X SP Y To implement practice entropy estimator 19 ˆSP X ψm ψ1 1 m 1 m1cid12 i1 log xi1 xi 29 xvalues ordered ascendingly xi cid2 xi1 ψ digamma function3 Note set log 0 0 points xi1 xi dont contribute sum The estimate C XY based 29 given ˆC XY ˆSP Y ˆSP X ˆC Y X 2 Slopebased IGCI infer X Y cid8 cid8 f cid7 cid8 cid8P x dx x log 1cid5 0 cid8 cid8g cid7 cid8 cid8P y dx y log 1cid5 0 We introduce following estimator ˆC XY 1 m 1 cid8 cid8 cid8 cid8 log m1cid12 i1 cid8 cid8 cid8 cid8 yi1 yi xi1 xi xi values ordered similar ˆC Y X 30 31 3 The digamma function logarithmic derivative gamma function ψx ddx log Γ x It behaves log x asymptotically x D Janzing et al Artiﬁcial Intelligence 182183 2012 131 13 With assumptions section 30 31 coincide exactly ψ terms cancel taking difference estimated entropies X Y ordering xvalues equivalent ordering yvalues In noisy case relation methods understood Section 46 31 diverges m difference yvalues remains ﬁnite difference xvalues gets closer zero Then compensate considering difference estimator analog reverse direction obtained swapping roles X Y Gaussian reference measure R Let discuss case d 1 ﬁrst Lemmas 5 6 imply C XY C Y X remain formally uniform reference measure rescale X Y variance note ensures SU X SU Y In contrast uniform measure required data points lie 0 1 The different scaling changes C XY log σ X log σY σ 2 Y denote variances X Y respectively according scaling uniform measure Consequently methods infer different directions σ 2 Y differ signiﬁcantly happen realworld data experiments X σ 2 X σ 2 45 Inference rule variable ranges reference manifolds Although experiments contained realvalued variables sketch use IGCI variables ranges Gaussian reference measure Rd Suppose X Y ddimensional real random vectors f diffeomorphism Rd Rd Let E X EY manifolds ddimensional Gaussian distributions The projection U X dvariate Gaussian mean vector covariance matrix X denoted Σ X U Y derived similarly The difference entropies U X U Y reads 1 2 logdet Σ X det ΣY Then easily compute C XY based 26 Because entropy difference SU X SP X measure nonGaussianity method considers variable closer Gaussian cause Isotropic Gaussians reference Rd We deterministic case method described 13 14 relies assumption implies Postulate 2 particular choice reference manifold Let P X P Y multivariate Gaussians Rd zero mean X Y related Y A X 32 A invertible d d matrix4 For arbitrary d d matrix B let τ B trBd denote renormalized trace Then 13 based assumption X Y implies approximately τ ΣY τ Σ X τ cid2 cid3 A A T 33 Σ X ΣY denote covariance matrices X Y respectively In 13 justiﬁed showing given A choosing Σ X randomly rotationinvariant prior ensures 33 approximately true high probability5 We implies Postulate 2 E X EY manifold isotropic Gaussians covariance matrices multiples identity U X U Y mean X Y covariance matrices read τ Σ X I τ ΣY I The relative entropy distance Gaussians equal mean covariance matrices Σ1 Σ0 given cid6 DP Σ1 cid10 P Σ0 1 2 log det Σ0 det Σ1 cid2 cid16 τ d Σ 1 0 Σ1 cid3 1 cid7 cid17 The distances manifold isotropic Gaussians read 13 DP X cid10 E X 1 2 DP Y cid10 EY 1 2 cid2 cid3 d log τ Σ X log detΣ X cid2 cid3 d log τ ΣY log detΣY P Y reads τ Σ X A A T Hence cid18 The covariance matrix cid6 D P Y cid10 U Y 1 2 log τ ΣY d τ Σ X d det A A T d τ Σ X τ A A T τ ΣY 34 35 cid19cid7 1 4 Ref 13 considers case Y A X E E independent noise term restrict attention deterministic 5 Ref 14 extends framework case number dimensions exceeds number samples 14 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Due detΣY detΣ X det A A T DP Y cid10 EY DP X cid10 E X D P Y cid10 U Y d 2 cid18 1 τ Σ X τ A A T τ ΣY cid19 Assumption 33 equivalent condition V Theorem 2 Postulate 2 gets additional justiﬁcation probabilistic scenario f ﬁxed P X chosen randomly prior satisﬁes certain symmetry condi tion For highdimensional relations close linear method appropriate uses set Gaussians opposed isotropic ones reference manifold Allowing Gaussians method makes use nonlinearities f removes information contained Σ X For relations close linear case looses essential information taking isotropic Gaussians reference ensures information describes joint overall scaling lost Nonuniform reference measure ﬁnite sets The intuitive explanation identiﬁability cause effect fact regions high density effect correlate regions high slope inverse function Remarkably method principle applicable P Y cid10 EY 0 case bijections ﬁnite probability spaces provided ensure D E X EY consist uniform distribution We omit details brief sketch special case Assume X Y values 1 k P X P Y probability mass functions P Y y P X g y Note discrete case g invertible monotonic Let E X EY twoparametric manifold distributions discrete Gaussians xμ2 2σ 2 U x μ σ e μ R σ R Then image discrete Gaussians usually discrete Gaussian inference principle nontrivial yielding preference direction The essential question conditions Postulate 2 reasonable The following explanations provide idea Assume k large P X distribution close discrete Gaussian small number xvalues Let f bijection preserves points 1 k permuting It likely permutation increases distance reference manifold decreasing This way reasoning certainly relies assumption k large distance P X reference manifold large For small k easily construct examples P X deviating strongly Gaussians signiﬁcant fraction permutations decrease distance reference manifold 46 Performance noisy regime The assumption having bijective deterministic relation actually necessary IGCI method Section 47 performance real data sets unexpectedly good obviously noisy We present explanations fact Although noisy case actually scope develop ment future methods inspired understanding reasonable performance regime On hand estimate small noise needs order spoil method Section 461 On hand conditions noise contribute inferring correct causal direction Section 462 First discuss case IGCI necessarily fails Let Y generated X linear model additive noise Y X E E X P Y obtained convolution P Y P X P E For Gaussians reference manifolds projections U X U Y P X P Y E X EY respectively given Gaussians mean variance If E Gaussian U Y U X P E additivity means variances convolution We DP X cid10 E X DP X cid10 U X DP X P E cid10 U X P E DP Y cid10 U Y DP Y cid10 EY convolution Gaussian decreases distance set Gaussians nonincreasing follows monotonicity relative entropy distance stochastic maps 20 Hence 23 violated renor malizing X Y unit variance entropy Y greater entropy X The entropybased estimator C XY converge positive number theory makes statement slopebased estimator note equivalence required deterministic models Similar arguments hold Y α X E restricted derivation α 1 technical convenience Hence entropybased IGCI Gaussians reference manifold fails nonlinearity f small compared width Gaussian noise The following subsection provides bound relevant small noise decision IGCI D Janzing et al Artiﬁcial Intelligence 182183 2012 131 15 461 Robustness entropybased inference adding small noise We restrict attention realvalued X Y recall entropy generated adding independent Gaussian noise related Fisher information J Y EP cid7 2 cid6 log P y y De Bruijns identity 20 t SP Y t Z 1 2 J Y t Z 36 Z Gaussian variance 1 Y Z The following Lemma provides lower bound nonGaussianity perturbed variable Lemma 7 NonGaussianity noisy output If EY denotes manifold Gaussians E Gaussian noise E Y decrease nonGaussianity bounded DP Y cid10 EY DP Y E cid10 EY cid2 1 2 cid6 log 1 cid16 J Y σ 2 Y 1 cid17 σ 2 E σ 2 E σ 2 Y cid7 σ 2 Y σ 2 E denote variance unperturbed output noise respectively Proof Set E σE Z standard Gaussian Z 36 implies SP Y E SP Y σ 2 Ecid5 0 t SP Y t Z dt 1 2 σ 2 Ecid5 0 J Y t Z dt cid2 1 2 σ 2 Ecid5 0 J Y J J Y J t Z t Z dt inequality Fisher information inequality 21 1 J Y W cid3 1 J Y 1 J W arbitrary independent random variables Y W Using J computation obtain t Z 1t checked straightforward SP Y E SP Y cid2 1 2 σ 2 Ecid5 0 1 t 1 J Y dt 1 2 cid18 cid6 log σ 2 E 1 J Y cid7 cid6 log cid7cid19 1 J Y cid2 log 1 2 J Y σ 2 E 1 cid3 Recalling 27 nonGaussianity given cid2 2π eσ 2 Y DP Y cid10 EY 1 2 SP Y log cid3 ﬁrst term entropy Gaussian variance σ 2 Y nonGaussianity changes according DP Y cid10 EY DP Y E cid10 EY cid2 1 2 1 2 cid6 log cid6 σ 2 Y σ 2 Y σ 2 E cid16 log 1 J Y σ 2 Y cid2 log J Y σ 2 E cid7 cid3 1 cid7 cid17 1 σ 2 Y σ 2 E σ 2 E cid2 Note Gaussians minimize Fisher information given variance J Y σ 2 Y 1 cid3 0 equality Gaus sians If Y Gaussian convolution Gaussian decrease nonGaussianity zero For nonGaussian Y decrease depends sensitivity term J Y σ 2 1 ratio Y variance noise total variance σ 2 Y σ 2 Lemma 7 assumes Gaussian noise We expect nonGaussian noise typically decrease nonGaussianity Gaussian noise rare cases particularly distributed noise We propose use bound general noise To decide noise reversed inferred causal arrow proceed follows For hypothetical cause X estimate density function f compute distribution effect Y noise After computing Fisher information estimate decrease non Gaussianity caused noise check smaller difference DP Y cid7 cid10 EY DP X cid10 E X Y cid7 Y E denotes noisy effect E noisy output 16 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 462 Performance slopebased inference noisy regime We closer look estimator 31 noisy case The arguments partly heuristic simulation studies Section 47 support claims Assume iid sample xi yi 1 m generated additive noise model 1 strictly monotonic differentiable f We assume xi f f xi ordered xi1 cid3 xi f i1 cid3 f We large m cid8 cid8 cid8 1 cid8 m1cid12 m1cid12 log log cid8 cid8 cid8 cid8 cid8 cid8 cid8 cid8 cid8 cid8 cid8 cid8 f i1 f ei1 ei xi1 xi yi1 yi xi1 xi m 1 1 m 1 i1 i1 m1cid12 i1 1 m 1 log ei1 ei 1 m 1 m1cid12 i1 log xi1 xi 37 The approximation based observation difference f i1 f gets negligible compared ei1 ei m term remains ﬁnite converges zero The second term 37 actually entropy estimator 30 term ψm ψ1 Without noise E estimators 30 31 coincide argued However noisy regime ﬁrst term tends dominate m increases diverges m Now write X X f Y E arbitrary function f To focus noise effect let assume X Y entropy information contained nonlinear functions help identifying causal direction estimator 30 value ˆC XY ˆC Y X To investigate behavior estimator 31 denote A XY ﬁrst term 37 A XY 1 m 1 let AY X 1 m 1 m1cid12 i1 m1cid12 i1 log ei1 ei log ei1 ei As second term 37 directions assumption 31 prefer direction X Y respec tively Y X A XY smaller respectively larger AY X The Jacobian matrix associated transformation X ET X Y T cid18 J cid19 1 0 cid7X 1 f J 1 J denotes absolute value determinant J We P XY P XE J P XE As X E independent SX Y SX SE On hand 38 SX Y SY E 39 Except special cases instance f linear X E Gaussian E Y dependent 911 SY S E SY E 0 Due 38 39 S X SE SY S E As assumed S X SY ﬁnally SE S E Furthermore E E approximately variance6 inequality implies E Gaussian E Let D E i1 E D E i1 E Under condition E iid P D convolution P E P E convolution P E P E Since E Gaussian E likely D Likewise P D Gaussian D We consider following possible cases 1 If E Gaussian D D Gaussian given heuristics A XY AY X Hence noise change decision 6 Note E E exactly variance directions ﬁtted linear functions f f linear X Y variance D Janzing et al Artiﬁcial Intelligence 182183 2012 131 17 2 Consider case E superGaussian roughly speaking means P E sharper peak longer tails Gaussian variables mean variance The Laplacian distribution example distributions Since E Gaussian E E superGaussian E Consequently D relatively values close zero D The function log D concave 0 symmetric wrt yaxis obtain large negative values D close zero A XY gets smaller AY X That superGaussian noise tends favor correct direction X Y 3 Suppose E subGaussian ﬂatter Gaussian variable mean variance An example uniform distribution As D Gaussian subGaussian D values close zero large D Hence AY X larger A XY In words subGaussian noise tends favor wrong direction Y X Fortunately superGaussian noise occurs practice Although analyze noise effect bear mind estimator 31 decision based joint effect properties nonlinear function noise distribution correspond second ﬁrst terms 37 respectively7 In analysis assume datagenerating process noisy case approximated additive noise model Analyzing noise effect general settings PNL causal model 11 complicated given However Section 47 simulation results data complex data generating process illustrate noise inﬂuences performance IGCI 47 Experiments In section experiments illustrate theory method detect true causal direction realworld data sets Complete source code experiments provided online httpwebdavtuebingenmpgdecausality httpparallelvubacbeigci The provides applet showing data results IGCI Simulation studies I Causeeffect pairs larger causal network We investigate performance IGCI deterministic noisy regime To end simulate causal relation n variables X1 Xn different pairs Y X Xi X j X j parents Xi All causal dependences given structural equations This ensures pairs effects causes outcomes structural equations reﬂecting fact causes real world effects variables The precise form data generating process follows We ﬁrst generate 20 independent variables X1 X20 Their distribution randomly chosen options equal probability uniform distribution 0 1 Gaussian mixture distribution GM following density GMx gcid12 i1 w iφxμi σi g 1 5 means μi 0 1 standard deviations σi 0 1g weights w 0 1 i1 w 1 Each param eter randomly chosen interval according uniform distribution Then 50 variables X21 X70 deﬁned according following structural equation cid15 g Xi f iX j X jk λi R E j k deﬁned later The function f randomly selected following families LIN Linear functions form f x j x jk kcid12 j0 c j x ji c j 1 1 k natural number randomly according probability 12k POL Polynomials form f x ncid12 i1 ici xi 7 Rigorously speaking noise forward direction changes bestﬁtting nonlinear function backward direction inﬂuence estimate C Y X As simple illustration consider case X E uniform Then bestﬁtting function f direction Y X longer linear shape depends noise level However skip details aspect 18 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 n 1 5 ci 1 1 The purpose factor ensure magnitude term similar x 0 1 MON Monomials form f x xn n 2 5 ROOT Root functions f x x1n n 2 5 MG Cumulative distribution functions mixtures Gaussians f x 5cid12 i1 αiΦxμi σi convex combination Gaussian cdfs Φxμi σi αi μi 0 1 σi 0 02 PROD Product functions form f x j x j1 x j x j1 QUOT Quotients f x j x j1 x jx j1 The variables X j X jk chosen randomly X1 Xi1 causally preceding variables Note k cid3 0 linear function k 1 product division function The results nonadditive noise study relation input variable output variable based marginalizing data second input variable All functions independent variable λi probability 05 zero chosen uniformly 0 02 R difference maximum minimum function f feeding values X j X jk In way noise proportional range function values The noise term E drawn Gaussian distribution mean 0 variance 1 Note deterministic relation obtained k 0 X j parent Xi noise parameter λ vanishes When deterministic relation monotonic decreasing increasing function replacing yvalue 1 y We repeat procedure generating variables X1 X70 100 times time generate 200 samples causal decision based m 200 iid data points For data set generated procedure apply inference method 50 pairs Y X Xi X j 21 70 randomly chosen j We compared entropybased slopebased method We compare different families reference measures uniform family amounts preprocessing compo nents data aﬃne transformation minimum 0 maximum 1 Gaussian family component data preprocessed aﬃne transformation zero mean standard deviation 1 Fig 7 shows typical examples input distributions relations input output corresponding output distribution Table 1 lists values ˆC XY ˆC Y X slopebased estimator 31 entropy estimator 30 corresponding decision Remarkably decision correct linear noisy case fourth case For possible explanations Section 462 By taking decisions ˆC XY ˆC Y X cid3 δ threshold δ trade accuracy percentage correct decisions versus decision rate percentage cases decision taken Fig 8 shows accuracy versus decision rate deterministic relations Fig 9 shows probabilistic relations These results method works best deterministic relations expected For deterministic relations increasing threshold increases accuracy method coming close accuracy 100 large threshold values For deterministic relations Gaussian reference measure performs somewhat better uniform reference measure For probabilistic relations picture different The uniform reference increasing accuracy starting 70 threshold reaching 85 large thresholds The Gaussian reference hand fails small thresholds accuracy close 50 random guessing Only large thresholds decision rates smaller 20 accuracy reaches 70 For deterministic probabilistic relations slopebased estimator 31 entropybased 30 yield similar results It instructive check extent procedure generates joint distributions P Y X satisfy orthogonality assumptions We compare cid3 cid2 cid2 cid3 CovU X log f P X CovU Y log f P Y cid7 1 cid7 covariance condition II Theorem 2 corresponding expression backward direction form use 11 uniform reference measure The ﬁrst expression given estimator cid2 cid20Cov P X log f cid3 cid7 1 m 1 cid6 m1cid12 i1 1 xi1 xi1 2 cid7 cid8 cid8 cid8 cid8 log cid8 cid8 cid8 cid8 yi1 yi xi1 xi 40 second exchanging roles X Y By simulation explained 1575 examples deterministic strictly monotonic relations generated The xaxis Fig 10 shows values 40 yaxis analog backward direction The ﬁgure conﬁrms postulate sense covariance forward direction usually closer zero Most values forward direction interval 1 1 backward values reach values 5 It clearly shows backward covariance biased away zero spread higher D Janzing et al Artiﬁcial Intelligence 182183 2012 131 19 Fig 7 Typical synthetic causeeffect pairs illustrating different cases probabilistic versus deterministic relations correct versus incorrect result indecision The corresponding quantitative description given Table 1 20 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Table 1 Quantitative description uniform reference measures typical examples depicted Fig 7 Function Type X46 f X12 X48 f X40 X25 f X20 X22 f X1 X50 f X13 POL POL MG LIN MG 31 ˆC XY 029 073 003 560 533 ˆC Y X 029 073 003 644 471 Dec OK 30 ˆSP X 253 381 284 264 314 ˆSP Y 283 301 28 293 292 Dec OK Fig 8 Results different implementations IGCI simulated deterministic causal relations 2000 different X Y pairs Fig 9 Results different implementations IGCI simulated probabilistic causal relations 3000 different X Y pairs D Janzing et al Artiﬁcial Intelligence 182183 2012 131 21 Fig 10 Empirical violations orthogonality condition h3 forward vs backward direction text 471 Simulation studies II Performance different shapes noise We investigate performance slopebased inference estimator 31 uniform reference measure changed shape noise We generate data according Y f X E We use distributions P E Gaussian Laplacian superGaussian uniform subGaussian distributions strongly subGaussian distribution represented mixture Gaussians 05N 2 1 05N 2 1 Similarly distributions P X Gaussian uniform distributions superGaussian distribution obtained passing Gaussian variable powernonlinearity exponent 15 keeping original sign subGaussian represented mixture Gaussians 05N 05 1 05N 05 1 f X X 13 f X X 3 f X X Note case f informative causal inference noisefree case f different forms For setting repeat simulations 500 times Figs 11 12 13 plot performance function noise standard deviation possible cases P E P X forms f respectively One second columns ﬁgures corresponding Laplacian noise performance increases noise variance In fourth columns corresponding uniform strongly subGaussian noise performance tends worse noise variance increases As seen Fig 12 function f X X 3 informative causal inference performance good regardless different choices P E P X When P E P X Gaussian f X X 13 f X X topleft panels Figs 11 13 decision high noise level like linear useful causal inference deterministic setting Fig 13 random guess Finally f certain combinations P E P X IGCI infers correctly noise effect We consider ﬁxed signaltonoise ratio change shape noise continuously To end randomly generate iid samples noise term E according zeromean generalized exponential distribution GED P e v 8Γ 1v cid21 cid8 cid8 cid8 cid8 exp cid22 v cid8 cid8 cid8 cid8 e 2σ 41 v mode Γ gamma function σ standard deviation SubGaussian Gaussian superGaussian noise obtained v 2 v 2 v 2 respectively In particular v 1 Laplacian distribution The uniform distribution obtained limit v We use ratioofuniform method 22 generate random numbers For cases P X f vary v 1 5 41 ratio standard deviation noise wrt f X ﬁxed 2 Fig 14 depicts performance function v In cases P X f consideration performance decreases remains v increases P E superGaussian sub Gaussian consistent claims Section 462 As general setting repeat simulations data generated Y X e E tanhE Y X e E E 3 5 Y X e E E respectively Y generated multiplicative block nonlinear linear effect noise E The performance IGCI function v given Fig 15 Again Fig 14 performance decreases remains v increases 22 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Fig 11 The performance percentage correct inferences different noise levels choices P E P X function f X X 13 Columns left right correspond Gaussian Laplacian uniform strongly subGaussian P E 05N 2 1 05N 2 1 noise respectively Rows correspond Gaussian uniform superGaussian subGaussian distributions cause X respectively Fig 12 The performance percentage correct inferences different noise levels function f X X 3 caption Fig 11 Realworld data Causeeffect pairs We evaluated IGCI method realworld data extended version Causeeffect pairs dataset described 1 This dataset consists observations 70 different pairs variables domains task pair ﬁnd variable cause variable effect For example pairs consists 349 measurements altitude temperature taken different weather stations Germany Obviously altitude cause temperature effect The complete dataset detailed description pair httpwebdavtuebingenmpgdecauseeffect Note pairs data set high noise levels necessarily expect method work In Fig 16 results 70 pairs following variants IGCI uniform distribution Gaus sians reference measures case combined slopebased entropybased estimator The absolute value ˆC XY heuristic conﬁdence criterion By taking decisions high absolute value trade accuracy versus decisions taken Fig 16 shows accuracy fraction correct decisions function decision rate fraction decisions taken total 70 possible decisions causeeffect pair If absolute value ˆC XY good measure conﬁdence expect D Janzing et al Artiﬁcial Intelligence 182183 2012 131 23 Fig 13 The performance percentage correct inferences different noise levels function f X X caption Fig 11 Fig 14 The performance percentage correct inferences different noise distributions indicated v 41 P X f The ratio noise standard deviation wrt f X ﬁxed 2 accuracy lowest decision rate 100 decisions taken regardless estimated conﬁdence increases monotonically decision rate decreases A complication data sets causeeffect pairs accuracy estimated decreases proportionally decision rate This means accuracies reported low decision rates higher uncertainty accuracies reported high decision rates For decision rate indicated 95 conﬁdence interval accuracy signiﬁcantly different 50 gray area The variants IGCI yield comparable results We conclude majority decisions agree causal ground truth agreement statistically signiﬁcant high decision rates However accuracy clearly increase decreasing decision rates This indicates heuristic conﬁdence estimate absolute value estimated ˆC XY functioning properly diﬃcult draw ﬁnal conclusions high uncertainty accuracy low decision rates Nevertheless considering noise present causeeffect pairs surprising method works takes decision IGCI variants accuracies 70 7 75 7 69 7 70 7 respectively 24 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Fig 15 The performance IGCI data generated complex transformations different noise distributions indicated v 41 P log X transformations Note yaxis labels correspond distribution log X The variance noise E ﬁxed 045 Fig 16 Results different implementations IGCI 70 causeeffect pairs Fig 17 provides comparative results IGCI variant based 31 uniform reference measure causal inference methods suitable inferring causal direction pairs variables LINGAM 23 Additive Noise AN 9 PostNonLinear PNL model 11 recent nonparametric method GPI 24 All methods GPI employ HSIC independence test 25 accepting rejecting ﬁtted models use maximum HSIC pvalues pvalue corresponds possible causal direction conﬁdence estimate The LINGAM method ﬁts functional relationships form Y α X E data preferring causal direction noise E independent hypothetical cause X The additive noise based method recall remarks 1 9 implemented Gaussian Process regression code GPML toolbox 26 ﬁnd D Janzing et al Artiﬁcial Intelligence 182183 2012 131 25 Fig 17 Results causal inference methods 70 causeeffect pairs likely function f For postnonlinear model based inference 2 employed neural networks model functions f h8 Finally nonparametric GPI method assume particular class functional relationships uses general model Y f X E exploits smoothness function f criteria deciding causal direction For method conﬁdence value taken approximated Bayes factor models corresponding possible causal directions In contrast experiments reported Fig 16 500 data points causeeffect pair methods need signiﬁcantly computation time IGCI large sample sizes Note performance IGCI case comparable performance reported Fig 16 data points This explained pairs measured values discretized effective number data points IGCI usually lower number available data points We repeated experiments times different subsamples plotted average curves Fig 17 We observe high decision rates methods LINGAM draw causal conclusions signiﬁcantly correlated ground truth IGCI PNL GPI yield comparable performances overall The performance additive noise method deviate methods accuracy somewhat lower forced decision hand conﬁdence estimate appears accurate methods accuracy increases quickly 100 decision rate decreases Again diﬃcult draw deﬁnite conclusions relative performances methods based 70 causeeffect pairs Realworld data Water levels Rhine The data consists water levels Rhine9 measured 22 different cities Germany 15 minute intervals 1990 2008 It natural expect causal relationship water levels different locations upstream levels inﬂuence downstream levels We tested method 231 pairs cities Since measurements actually time series causal inﬂuence needs time propagate performed experiments shifted time series pair time series series shifted relatively maximize correlation Fig 18 shows pair decision correct It shows representative plots data One clearly sees noise nearby cities relatively low large distant cities Nevertheless method performed situations overall accuracy uniform reference measure 87 201 correct decisions The results Gaussian reference measure similar 202 correct decisions 8 The large discrepancy results PNL reported reported 15 fact 15 applied handtuned preprocessing method pair treated pairs equally preprocessing method pair 9 We grateful German oﬃce Wasser und Schiffahrtsverwaltung des Bundes provides data request 26 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 Fig 18 Results German Rhine data All pairs total 22 cities tested White means correct decision black wrong decision gray ignored On right typical data illustrated measurement stations near measurement stations farther apart shows noise increases signiﬁcantly distance 48 Discussion The assumption P effectcause P cause satisfy nongeneric relation helpful paradigm ﬁnding novel causal inference rules Hence main important challenges consists describing kind nongeneric dependences typically occur backward direction A general answer question given shown option deﬁning dependences empirically testable way given orthogonality conditions sense information geometry We presented method able infer deterministic causal relations variables domains The accuracy proposed method shown competitive existing methods In terms computation time method orders magnitude faster particular linear number data points In addition handle deterministic case existing methods work presence noise It desirable reliable conﬁdence criterion inference method Moreover like point large noise regime present method completely fail For Gaussian reference measure dimension instance entropybased version necessarily shows wrong direction effect given linear function cause plus independent Gaussian noise This effect Gaussian cause A generalization informationgeometric inference method case relation cause ef fect close bijective map straightforward In Appendix A discuss toy examples showing asymmetries cause effect phrased terms information geometry Acknowledgements Part work supported Deutsche Forschungsgemeinschaft SPP 1395 Appendix A Outlook Special cases nonbijective relations The following subsections provide list toy models explore conditions violation orthogonality conditions shown backward direction The models suggest straightforward extension IGCI method nonbijective case orthogonality help identifying causal direction A1 One way deterministic Let ranges D X D Y X Y respectively ﬁnite P X Y distribution Y deterministi cally determined X Y f X surjective necessarily injective function f surjectivity backward model deﬁned Fig A19 We orthogonality conditions Theorem 2 simple case U X U Y uniform distributions D X D Y respectively First consider orthogonalities expect X causes Y Lemma 8 Orthogonalities surjective functions X Y Assume Y deterministically given X CovU X hi P X U X 0 holds trivially 1 For 2 3 equivalent D Janzing et al Artiﬁcial Intelligence 182183 2012 131 27 Fig A19 Left causal relation given deterministic noninjective map cause effect Right splitting model cause deterministically inferred effect CovU X log m f P X U X 0 m y cid8 cid8 f cid8 cid8 1 y denotes number preimages y A1 Proof Condition h1 trivial function x cid5 DP Y x cid10 U Y constant P yx δ y f x P Y x point measure To rephrase condition h2 ﬁrst compute P y m y D X obtain h2x cid12 y log P yx P y P yx cid12 y log δ y f x m y δ y f x c log m cid2 cid3 f x c c log D X The constant term c clearly irrelevant covariance Since h1 constant function uncorrelat edness h3 h1 h2 P X U X obviously equivalent uncorrelatedness h2 P X U X cid2 The following lemma describes relations expect condition Y f X Y causes X splitting model Fig A19 right10 The causal relation given mechanism splits yvalue different xvalues set A y mapping x y deterministic Lemma 9 Orthogonalities splitting model Y X Assume Y deterministically given X assume Y cause For functions y cid5 hi y equation CovU Y hi P Y U Y 0 trivial 2 1 3 equivalent cid6 CovU Y SP X y cid7 P y U y 0 By slightly abusing notation SP X y P yU y denote functions y cid5 SP X y y cid5 P yU y respectively Proof We ﬁrst compute P x y δ y f x P X x P Y f x To rephrase condition h2 compute P x 1 D Y cid12 y P x y P X x D Y P Y f x We obtain h2 y cid12 x log P x y P x P x y log D Y Therefore condition h2 trivial 10 Note case paper Y cause The reason want compare properties P XY expect possible causal directions 28 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 To reformulate condition h1 observe h1 y DP X y cid10 U X sign additive constant given SP X y Since h2 constant condition h3 equivalent h1 cid2 These results obtain reasonable conditions directions X cause uncorrelatedness input logarithm number preimages On hand Y cause postulate zero correlation input conditional entropy Unfortunately orthogonality direction imply violation orthogonality direction Moreover violation orthogonality backward direction positive negative sign This shown following example Example 3 No deﬁnite violation backward direction Let f number m preimages A1 constant Then nontrivial conditions Lemma 8 satisﬁed hypothesis X Y y cid5 SP X y positively negatively correlated uncorrelated P Y U Y To set D Y 1 2 D X 1 2 3 4 P x y 1 12 x 1 2 On hand P x y 2 1 x 3 Then SP X y1 log 2 SP X y2 0 Depending P y 1 greater smaller P y 2 induce positive negative correlations Conversely assume SP X y uncorrelated P yU y orthogonality conditions consistent hypothesis Y X To log m f x negatively positively correlated P xU x consider following example Set D X 1 2 3 4 5 D Y 1 2 Let P X y1 uniform distribution set 1 2 let P X y2 distribution 3 4 5 entropy 1 bit Then SP X y constant y uncorrelated P yU y design P y like To check log m f x positively negatively correlated P xU x observe cid2 cid3cid2 log m f x cid3 P x U x log 2 cid12 x cid6 P Y 1 2 5 cid7 cid6 log 3 P Y 2 3 5 cid7 cid6 log 2 log 3 cid7 P Y 1 2 5 positive P Y 1 25 negative P Y 1 25 This result bit disappointing ﬁrst glance questions informationgeometric method nonbijective case violations orthogonality backward direction occur possible signs decision rules simple preferring direction violation orthogonality smaller respect absolute value natural inference rules work absolute value It notions independence orthogonality conditions needed To support conjecture mention designing P Y P XY example fact adjusted way captured orthogonality conditions There following nice result Lemma 10 Number preimages input probability For Y f X let m y number preimages y If log m uncorrelated P Y U Y log m f negatively correlated P X U X On hand log m f uncorrelated P X U X log m positively correlated P Y U Y cid7 cid6 cid6 cid7 CovU X log m f P X U X P Y U Y cid7 CovU Y cid6 D log m cid23 cid23 cid23 cid23 U Y m D X cid6 D m D X cid23 cid23 cid23 cid23 U Y cid7 Proof cid6 CovU X log m f cid7 P X U X cid12 x cid12 y cid12 cid2 cid3cid2 log m f x cid3 P x U x cid7 P y m y D X cid6 log m y cid6 log m y P y U y U y m y D X cid7 y cid6 CovU Y cid6 D log m cid23 cid23 cid23 cid23 U Y m D X cid7 P Y U Y cid7 cid6 D m D X cid23 cid23 cid23 cid23 U Y cid7 cid2 A2 A3 A4 A5 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 29 The term A2 measures extent m nonconstant Since m yD X coincides P y known expression 22 Note correlations m P Y U Y positive Y effect correlation m f P X U X negative X effect Here different sign correlation disturbing However following special case turns natural Example 4 All preimages equally likely For causal directions X Y Y X assume P x y δ y f x m y A6 If Y X unlikely occur means divide probability uniformly preimages x given y For X Y instance occur P X uniform11 We obtain h3x cid12 log h3 y y cid12 x P y U y P yx log m f xD Y D X P x U x log P x y log D X m yD Y Therefore hsx h3 y irrelevant constants given m f x m y respectively Hence Lemma 10 implies h3 y negatively correlated P yU y h3x uncorrelated P xU x vice versa nicely ﬁts informationgeometric framework Note log m y coincides SP X y constant DP X ycid10U X sign constant Therefore uncorrelatedness log m P Y U Y equivalent orthogonality condition h1 Theorem 1 A2 Functional relation small independent noise In subsection revisit motivating remarks Section 2 precise way ﬁt informationgeometric framework Consider socalled additive noise model Y f X E E X Let f bijection 0 1 E compact support 0 cid18 Let P X support 0 1 support Y given 0 1 cid18 By adapting arguments Example 1 uniform distribution 0 1 cid18 instead 0 1 checks easily orthogonality condition h1 equivalent uncorrelatedness SP Y x P x holds SP Y x attains constant value SE We assume cid18 small compared curvature f scale ﬂuctuations P x conditional distribution P X y approximately given distribution yE shifted ydependent value We assume 1 f cid7 SP X y SE log f A7 y close boundaries interval 0 1 For backward direction condition covariance h1 reads cid7 1 y cid6 CovU Y SP X y cid7 1cid18cid5 P y U y cid2 SP X y cid3 P y U y dy 0 1cid5 log f 0 cid7 1 cid2 y cid3 P y U y dy A8 A9 approximation A7 neglected fact S X y actually integrated 0 1 cid18 0 1 errors order cid18 Expression A9 positive small cid18 deterministic limit cid18 0 A9 transformed 1cid5 log f 0 cid7 cid2 x P x f cid7 cid3 x dx D cid2 P X cid10 f cid3 cid7 cid2 D cid7 cid10 P X f cid3 cid3 0 A10 11 If P x attains different values unlikely attains value A y 30 D Janzing et al Artiﬁcial Intelligence 182183 2012 131 cid7 cid7x probability density f 1 1 f 0 0 Note deterministic invertible case interpret f P x A10 symmetrized relative entropy term This result shows additive noise models f low noise regime induce backward models noise depends input way leads violation orthogonality condition h1 It remarkable violation described term similar occurred bijective case A1 cases refer orthogonality h3 This suggests common principle observations Appendix B Justiﬁcation Postulate 2 For single reference densities instead manifolds justiﬁed conditions h1 h3 argument structure functions h correlate P X depend conditional P Y X function f case This justiﬁcation completely convincing generalize setting manifolds functions h1 h3 contain reference density U Y deﬁned projecting output probability P Y EY U Y depends P X f P Y image P X f We justify Postulate 2 slightly different way Our justiﬁcation remains informal provide precise version arguments scope paper We start following statement Observation 1 Projection random moves Let P set probability distributions large inﬁnite prob ability space Let E P lowdimensional exponential manifold P P arbitrary Generate new point R moving random direction P chosen independently E Denoting projections P R E P E RE respectively obtain typical P E RE The approximate equality means error replacing point relative entropy expres sion small compared DP cid10 R DR cid10 P Apart approximate equality signs statement informal specifying typical means This require probability distribution set possible moves Assume pair P X f generated follows Let U X E X given obtain P X modifying U X independently E X EY We assume U X projection according random Generate f P X E X seriously restricting random moves assumption approximates typical case This seen applying Observation 1 special case P E Let U Y usual projection P Y EY W P Y random projection This justiﬁed map U X P X f Since f chosen independently manifold EY Observation 1 states P Y EY We apply Observation 1 consider P Y obtained W U Y Applying f 1 sides yields W f P X B1 B2 W f denotes image W f point EY closest image U X f In typical case expect point f 1 Note W f 1EY closest U X W DP X cid10 W f DP X cid10 U X DU X cid10 W f vector connecting U X W f depend P X depends f manifolds The vector pointing U X P X typically close orthogonal pointing U X W f Together B2 obtain DP X cid10 P X DP X cid10 U X DU X cid10 P X equivalent conditions Theorem 2 In Section 44 mentioned special case linear relations 32 highdimensional Gaussian variables isotropic Gaussians reference manifold Postulate 2 justiﬁed concentration measure results It instructive verify B1 holds case To recall U Y covariance matrix τ ΣY I τ cid2 AΣ X A T cid3 I D Janzing et al Artiﬁcial Intelligence 182183 2012 131 31 approximately equal cid2 cid3 τ Σ X τ A A T I Section 44 13 One checks easily covariance matrix W isotropic Gaussian closest P Y Using notations shows B1 References 1 J Mooij D Janzing Distinguishing cause effect Journal Machine Learning Research WCP 6 2010 147156 2 P Spirtes C Glymour R Scheines Causation Prediction Search Lecture Notes Statistics SpringerVerlag New York NY 1993 3 J Pearl Causality Cambridge University Press 2000 4 J Lemeire E Dirkx Causal models minimal descriptions multivariate systems httpparallelvubacbejan 2006 5 D Janzing B Schölkopf Causal inference algorithmic Markov condition IEEE Transactions Information Theory 56 10 2010 51685194 6 R Solomonoff A formal theory inductive inference Information Control Part II 7 2 1964 224254 7 G Chaitin A theory program size formally identical information theory Journal Association Computing Machinery 22 1975 329340 8 A Kolmogorov Three approaches quantitative deﬁnition information Problems Information Transmission 1 1 1965 17 9 P Hoyer D Janzing J Mooij J Peters B Schölkopf Nonlinear causal discovery additive noise models Proceedings Conference Neural Information Processing Systems NIPS 2008 Vancouver Canada 2009 MIT Press 2009 10 J Peters D Janzing B Schölkopf Causal inference discrete data additive noise models IEEE Transactions Pattern Analysis Machine Intelligence 33 12 2011 24362450 11 K Zhang A Hyvärinen On identiﬁability postnonlinear causal model Proceedings 25th Conference Uncertainty Artiﬁcial Intelligence Montreal Canada 2009 12 D Janzing B Steudel Justifying additivenoisebased causal discovery algorithmic information theory Open Systems Information Dynam ics 17 2 2010 189212 13 D Janzing P Hoyer B Schölkopf Telling cause effect based highdimensional observations Proceedings 27th International Confer ence Machine Learning ICML 2010 Haifa Israel June 2010 pp 479486 14 J Zscheischler D Janzing K Zhang Testing linear equations causal A free probability approach Proceedings 27th International Conference Uncertainty Artiﬁcial Intelligence UAI 2011 Barcelona Spain 2011 pp 839847 15 P Daniušis D Janzing J Mooij J Zscheischler B Steudel K Zhang B Schölkopf Inferring deterministic causal relations Proceedings 26th Conference Uncertainty Artiﬁcial Intelligence UAI July 2010 pp 18 16 N Friedman I Nachman Gaussian process networks Proceedings 16th Conference Uncertainty Artiﬁcial Intelligence UAI Stanford CA USA 2000 Morgan Kaufmann 2000 pp 211219 17 S Amari H Nagaoka Methods Information Geometry Oxford University Press 1993 18 S Amari Information geometry hierarchy probability distributions IEEE Transactions Information Theory 47 5 2001 17011711 19 A Kraskov H Stoegbauer P Grassberger Estimating mutual information httparxivorgabscondmat0305641v1 2003 20 T Cover J Thomas Elements Information Theory Wileys Series Telecommunications Wiley New York 1991 21 A Dembo T Cover J Thomas Information theoretic inequalities IEEE Transactions Information Theory 37 1991 15011518 22 AJ Kinderman JF Monahan Computer generation random variables ratio uniform deviates ACM Transactions Mathematical Soft ware 3 3 1977 257260 23 S Shimizu PO Hoyer A Hyvärinen AJ Kerminen A linear nonGaussian acyclic model causal discovery Journal Machine Learning Research 7 2006 20032030 24 Joris M Mooij Oliver Stegle Dominik Janzing Kun Zhang Bernhard Schölkopf Probabilistic latent variable models distinguishing cause effect Advances Neural Information Processing Systems 23 NIPS 2010 Vancouver Canada Curran Associates 2011 pp 16871695 25 A Gretton R Herbrich A Smola O Bousquet B Schölkopf Kernel methods measuring independence Journal Machine Learning Research 6 2005 20752129 26 CE Rasmussen H Nickisch Gaussian Processes Machine Learning GPML Toolbox Journal Machine Learning Research 11 2010 press