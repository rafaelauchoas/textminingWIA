Artiﬁcial Intelligence 172 2008 17311751 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint The combination multiple classiﬁers evidential reasoning approach Yaxin Bi Jiwen Guan b David Bell b School Computing Mathematics University Ulster Jordanstown Co Antrim BT37 0QB UK b School Computer Science The Queens University Belfast Belfast BT7 1NN UK r t c l e n f o b s t r c t Article history Received 4 June 2007 Received revised form 12 June 2008 Accepted 12 June 2008 Available online 19 June 2008 Keywords Ensemble methods Dempsters rule combination Evidential reasoning Evidential structures Combination functions 1 Introduction In domains competing classiﬁers available want synthesize accurate classiﬁer combination function In paper propose classindifferent method combining classiﬁer decisions represented evidential structures called triplet quartet Dempsters rule combination This method unique distinguishes important elements trivial ones representing classiﬁer decisions makes use information calculating support class labels provides practical way apply theoretically appealing DempsterShafer theory evidence problem ensemble learning We present formalism modelling classiﬁer decisions triplet mass functions establish range formulae combining mass functions order arrive consensus decision In addition carry comparative simplet dichotomous structure compare study alternatives combination methods Dempsters rule majority voting UCI benchmark data demonstrate advantage approach offers 2008 Elsevier BV All rights reserved The idea characterizing ensemble learning learn retain multiple classiﬁers combine decisions way order classify new instances 36 The attraction approach supervised machine learning based premise combination classiﬁers accurate individual classiﬁer A theoretical explanation success different classiﬁers offer complementary information instances classiﬁed harnessed improve performance individual classiﬁers 27 Generally speaking successful ensemble method depends components set appropriate classiﬁers combination method function scheme 30 Classiﬁers assign single classes sets classes new instance respective numeric values decisions combination function merges decisions way determine ﬁnal decisionusually voting decisions Ensemble classiﬁers generated different ways A typical approach use single learning algorithm operate different subsets attributes instances training data bagging 9 boosting 1121 derivatives random forests 10 random subspace method constructing decision tree forests 26 Another approach use different learning algorithms operate single data set 4832 Among set individual classiﬁers accurate given task accurate However dominant Corresponding author Email addresses ybiulsteracuk Y Bi jguanqubacuk J Guan dabellqubacuk D Bell 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200806002 1732 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 complete data distribution By taking account strengths classiﬁers combination functions performance best individual classiﬁer improved 12 Kuncheva 28 roughly characterizes combination methods based forms classiﬁer outputs categories In ﬁrst category combination decisions performed single class labels including majority voting Bayesian probability extensively examined ensemble literature 18274049 The second category cerned utilization continuous values corresponding class labels One set methods called classaligned methods based class labels different classiﬁers calculating support class labels regard support classes This method includes linear sum order statistics considerable effort devoted 2527474851 Another method called stacked generalization metalearning use continu ous values class labels set features learn combination function addition set classiﬁers 194654 An alternative group methods called classindifferent methods use information possible obtained single classes sets classes calculating support class 28 Classaligned methods classindifferent methods based continuous values class labels calculating support class labels A distinction takes impacts different classes account determining support class permits presence uncertainty informationas happens instance classiﬁed different classes different classiﬁers Several related studies presented literature classindifferent methods utilize single classes sets classes 163949 Classindifferent methods combining decisions form list ordered decisions intensively studied poorly understood In particular little known value evidential reasoning methods combining truncated lists ordered decisions 58 In study consider classindifferent approach combining classiﬁers Dempsters rule combination Our focus generating classiﬁers different learning methods manipulate single data set combination classiﬁers modeled process reasoning uncertainty We model output given classiﬁers new instances list contender decisions reduce subsets 2 3 decisions respectively repre sented evidential structures triplet quartet 58 We ﬁrst establish formalism combining triplets quartets Dempsters rule combination constrain ﬁnal decision empirically analytically ex amine effect different sizes decision lists combination classiﬁers Furthermore justify assumption modelling classiﬁer results independent bodies evidence sensible The advantages approach summarized follows The ﬁrst advantage method makes use wide range evidence items classiﬁcation ﬁnal decision The idea inspired observation best single class labels selected basis corresponding values valuable information contained discarded labels lost Arguably potential loss support classes avoided utilizing support information decision making process The evidence structures triplet able distinguish important classes trivial ones incorporate bestsupported class second bestsupported class rest classes treated terms ignorance process decision making The second advantage evidence structures provide eﬃcient way combining pieces evidence break large list contender decisions smaller tractable subsets Like simplet dichotomous structures 244 method deals longstanding criticism saying evidence theory translate easily practical applications computational complexity combining multiple pieces evidence To validate method illustrate power carried numerous comparative experiments UCI data sets 3 We experimentally compare triplet quartet alternatives simplet dichotomous structure list decisions We comparison Dempsters rule majority voting combining classiﬁers During course classiﬁer combination important issue extent agreement reached classiﬁcation decisions classiﬁers assessed means κ statistics associative property combining triplets experimentally examined Finally explain empirical ﬁndings present investigation calculation process Dempsters rule provides insight reason superiority method The rest paper organized follows Section 2 presents representation classiﬁer outputs idea classindependent methods Section 3 reviews DempsterShafer theory evidence Section 4 presents review related studies focus alternative structures simplet dichotomous structure previously combining classiﬁers The rationale evidential structure triplet associated formulae presented Section 5 The combination functions Dempsters rule different evidential structures majority voting combining classiﬁers evaluated experimental settings results detailed Section 6 Section 7 presents discussion advantage triplet quartet alternatives Section 8 gives justiﬁcation independence evidence derived classiﬁer outputs The concluding summary given Section 9 2 Representation classiﬁer outputs combination methods In supervised machine learning learning algorithm provided training instances form cid3d1 c1cid4 cid3dD cqcid4 di D ci C 1 cid2 q cid2 C inducing unknown function f f d c D space tribute vectors vector di form w i1 w components symbolic numeric values C set categorical classes class ci form class label Given set training data learning algorithm Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1733 Fig 1 A decision proﬁle instance d generated ϕ1d ϕ2d ϕM d aimed learning function ϕa classiﬁer training data The classiﬁer ϕ approximation unknown function f Given new instance d classiﬁcation task decide ϕ instance d belongs class ci In multiclass assignment denote process mapping ϕ D C 0 1 1 C 0 1 ci si ci C 0 cid2 si cid2 1 si numeric value different forms similarity score classconditional probability prior posterior probability measure depending types learning algorithms It represents degree support conﬁdence proposition instance d assigned class ci The greater value class ci greater conﬁdence instance belongs class Without loss generality denote classiﬁer output ϕd s1 sCa general representation classiﬁer outputs From representation ϕd alternative forms outputs derived For example rank class labels according continuous values descending order By choosing single label ranked listthe single label maximal value classiﬁer output ϕd assign unique label label subset instance d classiﬁcation decision Alternatively rearrange process assigning unique label instance d mapping pair cid3d cicid4 Boolean value true T false F terms oracle output 29 If value T assigned cid3d cicid4 means decision proposition instance d belonging class ci true value F indicates proposition false These alternatives seen output information ﬁnal stage classiﬁcation Given ensemble classiﬁers ϕ1 ϕ2 ϕM classiﬁer outputs single class label instance d results classiﬁers combined majority voting weighted 31 Naive Bayes 49 example More generally classiﬁer produces multiple classes output instance da numeric score vector list represented ϕid vectors organized matrix called decision proﬁle DP depicted Fig 1 28 There different ways combination classiﬁer outputs carried ϕid si j cid2 cid3 cid3 1 cid2 j cid2 C cid4 1 cid2 cid2 M 2 One commonly combination methods calculate support class c j DPs jth column s1 j s2 j sM j regardless support classes We method classaligned method Some examples linear sum 51 order statistics minimum maximum median 48 probabilistic product sum rules 27 Alternatively combination classiﬁer outputs performed entire decision proﬁle selected information order constrain class decision We refer alternative group methods classindifferent methods There exist related contributions subject including combination neural network classiﬁers Dempsters rule 41 combination neural network classiﬁers derived different feature sets Dempsters rule 1 In broad sense studies similar approach calculating support classes That clas siﬁer class mean vector called reference vector generated organized matrix called decision template denoted DT For M classiﬁers C classes C decision templates M C dimensions formed Given decision proﬁle DPd closeness DPd DT 1 cid2 cid2 C computed different proximity measures Euclidean distance cosine function class largest support assigned instance d In work concept classindifferent methods slightly different aforementioned We generate decision templates use entire decision proﬁle compute degrees support class Instead select 2 3 classes ϕd according numeric values restructure new list composed subsets classes C respectively represented evidential structures triplet quartet With triplet example ﬁrst subset contains class largest value conﬁdence second contains second largest valued class set classes C In way decision proﬁle illustrated Fig 1 restructured triplet quartet decision proﬁle column longer corresponds class The degree support class computed combining triplets quartets decision proﬁle We method later sections 1734 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 3 DempsterShafer DS theory evidence Fig 2 Representation belief interval The DS theory evidence recognized effective method coping uncertainty imprecision embedded evidence reasoning process It viewed generalization Bayesian probability theory providing coherent representation ignorance lack evidence discarding insuﬃcient reasoning principle The DS theory suited range decision making activities It formulates reasoning process pieces evidence hypotheses subjects strict formal process order infer conclusions given uncertain evidence avoiding human subjective intervention extent In DS theory referred evidence theory evidence represented terms evidential functions ignorance These functions include mass functions belief functions plausibility functions 44 Any conveys information Deﬁnition 1 Let Θ ﬁnite nonempty set called frame discernment Let 0 1 interval numeric values A mapping function m 2Θ 0 1 called mass function satisﬁes 1 m 0 2 cid5 XΘ m X 1 A mass function basic probability assignment bpa subsets X Θ A subset A frame Θ called focal element focus mass function m Θ m A 0 A called singleton oneelement subset Deﬁnition 2 A function bel 2Θ 0 1 called belief function satisﬁes 1 bel 0 2 belΘ 1 collection A1 A2 An n cid3 1 subsets Θ cid6 cid7 cid5 bel A1 A2 An cid3 1 I1bel cid8 Ai I12nIcid10 iI One attractive features DS theory belief function contrast conventional probability inequality replaced equality With function plausibility function deﬁned pls A 1 bel A Notice belief function gathers support subset A gets mass functions subsets plausibility function difference 1 support As complement subsets The difference pls A bel A represents residual ignorance denoted ignorance A ign A This gives important feature DSthe representation precisely known remains unknown Fig 2 presents intuitive representation supporting subset A belief function plausibility function ignorance Deﬁnition 3 Let m1 m2 mass functions frame discernment Θ subset A Θ orthogonal sum mass functions A deﬁned m A 1N cid5 m1 Xm2Y XY Θ XY A 3 cid9 N 1 XY m1 Xm2Y K 1N called normalization constant orthogonal sum m1 m2 The orthogonal sum fundamental operation evidential reasoning called Dempsters rule combination Dempsters rule short There conditions governing combine mass functions The ﬁrst condition N cid10 0 N 0 orthogonal sum exist The second mass functions independent represent independent opinions evidence sources relative frame discernment Notice Dempsters rule simply involves mass functions tell mass function independent able rule dependence mass functions Therefore context applying Dempsters rule effect combining mass functions depend evidence sources treated modelled mass functions accounting relation 2443 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1735 31 Simple support function In situations structure evidence taken account mass functions signiﬁcantly simpliﬁed The simplest form mass function called simple support function limited providing degree support single proposition A Θ referred simplet structure simplet This structure provides support propositions discerned frame discernment Θ 44 Formally mass function said simple nonempty subset A Θ m A s mΘ 1 s mΘ A 0 0 s cid2 1 subset A focus m s called degree support mass function form called simple support function focused A A simple support function belief function bel simple mass function m Thus cid5 bel A m X m A X A A Θ Let m1 m2 mn simple mass functions common focus A respective degrees support s1 s2 sn Then m1 m2 mn simple mass function focus A support degrees A follows m1 m2 mn A 1 cid10 1 si m1 m2 mnΘ i12n cid10 1 si i12n m1 m2 mnΘ A 0 Separable mass functions include simple mass functions orthogonal sums simple mass functions They based homogeneous focuses heterogeneous evidence different subsets frame discernment referenced different evidence sources 32 Dichotomous function A mass function m said dichotomous function possible focal elements m A Θ A Θ A Θ A special case occurs A singleton x Θ In situation dichotomous mass function m focuses x Θ x Θ x referred dichotomous structure 2 Let Θ x1 x2 xΘ Suppose 1 2 Θ dichotomous mass function mi mixi miΘ xi miΘ mixi miΘ xi miΘ 1 We view quantities follows mixi degree support xi miΘ xi degree support refutation xi miΘ degree support assigned proposition xi Barnett proposed technique based dichotomous mass functions instead general mass functions It means instead potentially exponential time complexity function deriving evidence combination computation dichotomous mass functions involves 3 particular subsets x Θ x Θ x Θ general mass functions enumerate 2 subsets Θ Barnetts approach consider entire orthogonal sum evidence bodies structure m1 m2 mΘ These precisely evidence spaces separable exactly Θ dichotomous mass functions m1 m2 mΘ time complexity combining mass functions linear Guan Bell 2223 generalized consider general orthogonal sum explained Θ Let Θ x1 xΘ Suppose xi Θ li dichotomous mass functions repeated focus Θ 1 1 2 Θ j 1 2 ls s l1 lk xi m m 1 cid2 k cid2 Θ 0 cid2 l1 lk The task calculate quantities associated Θ xi m Θ xi m xi m Θ m j j j j j j m m1 1 cid11 ml1 cid14 1 cid12cid13 m1 k cid11 lk m cid14 cid12cid13 k l1 items lk items 4 1736 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 l1 lk n n number masses summed greater Θ In Eq 4 calcu lation combining n dichotomous mass functions divided parts The ﬁrst combine mass functions repeated focal elements The second combine mass functions focuses A method combining mass functions studied 2223 shown computational complexity combining mass functions linear 4 Existing methods In section ﬁrst develop model unify tasks combining outputs multiple classiﬁers ceptual framework DempsterShafer theory look DSbased studies In supervised learning domain classiﬁers assign classes instance The key assumption underlying approach single class assignment instance belongs class This assumption suggests oneelement subsets 2C semantic represent propositions By DS terminology given frame discernment C c1 cC evidence derived classiﬁers concerns speciﬁc individual classes tending support singleton classes C subsets 2C particularly meaningful Therefore powerset 2C propositions reduced subsets contain individual classes making frame discernment C Formally let M number classiﬁers ϕ1 ϕM let C c1 cC set classes For instance d D classiﬁer produces output vector ϕid Classifying d means assigning class C deciding set C hypotheses d belongs ck k 1 C according ϕid In DS terms C referred frame discernment classifying process regarded decides true value proposition instance d belongs ck according knowledge ϕd ϕd regarded piece evidence represents degrees support belief proposition Instead 100 certainty expresses belief committed ck 2C rest belief directly derived ϕd negation proposition remains unknown indiscernible In DS formalism situation regarded ignorance belief mass functions provide effective way express This attractive features DSbased methods The merit assuming single class assignment signiﬁcantly eases application DS theory practical evidential functions general context reduced C The following sections problems computations 2 review DSbased methods representing evidence deﬁning mass belief functions based ϕd C 41 Xus method Xu et al 49 discussed approaches combining multiple classiﬁers including majority voting weighted Bayesian formalism proposed DS model combining results multiple classiﬁers Their method treated classiﬁer outputs single class labels deﬁned sources evidence propositions basis performance classiﬁers terms recognition substitution rejection rates The items evidence represented dichotomous mass functions Let D training data set suppose recognition substitution rates classiﬁers denoted s For new instance d piece evidence ϕid sϕiD rejection rates given 1 cid5i cid5i r cid5i rϕiD cid5i represented following mass function cid16 cid16 cid15 ck cid15 ck mi k mi k cid5i r cid5i s mi kC 1 mi cid16 cid15 ϕid cid15 1 cid2 cid2 M k cid16 ϕid cid15 ck 1 cid2 cid2 M k cid16 cid15 ck mi k cid16 k cid4 cid2 1 C cid2 1 C cid4 5 6 7 ck C ck With M pieces evidence existing represented M dichotomous mass functions degrees support classes calculated combining mass functions formula 3 A ﬁnal class decision given instance selecting class largest degree support Notice combination classindifferent method calculation support class ck based mass functions focus ck support impacted mass values second subset ck C ck set C ignorance In deﬁnition dichotomous mass functions directly deﬁned basis numeric values ϕid formula 2 Instead deﬁned based overall performance classiﬁers Therefore different instances basic probability assignments derived classiﬁer outputsdichotomous mass functions This method ignores fact normally classiﬁer performance different classes consequently degrade combined performance classiﬁers Nevertheless proposed method lays groundwork formalizing problem combining classiﬁer outputs dichotomous evidence structure achieved considerable performance improvement applied handwriting recognition 42 Rogovas method Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1737 Rogova 41 proposed model combining results neural network classiﬁers DS theory This work accounted relation classiﬁer outputs reference vectors pieces evidence developed general way measure relation proximity measures The pieces evidence represented simple support functions Formally let Dk subset training data set instances belong class ck C let ϕix x Dk set classiﬁer outputs A mean vector ϕix denoted E k called reference vector class k For instance d general proximity measure E k ϕid deﬁned follows cid2 M k cid2 C 8 μi k φ cid15 cid16 E k ϕid The measure φ different formscosine function Euclidean distance forth It provides evidence likely instance d belongs class ck If output ϕid far E k ϕid considered providing little information proposition d ck case φ small value On contrary ϕid close E k belong class Simple support functions given k inclined believe d E cid16 cid15 μi ck k cid16 cid15 ck 1 j jcid10k mi k mi mi kC 1 μi k cid10 cid16 cid15 1 μi r r1rcid10k mi j jcid10kC 1 mi j cid16 cid15 ck cid10 cid15 1 μi r cid16 r1rcid10k 9 10 The interpretation formulation formulae 9 10 quantify relation ϕid class reference vector E kpieces evidence However piece evidence represents belief committed ck point particular hypotheses Thus rest belief distributed C frame discernment Based formulation unseen instance decision proﬁle remodeled formula 11 kth 1 cid2 k cid2 C column DP corresponds M simple support functions focal elements viz ck set C DPd cid2 mi j cid3 cid3 1 cid2 j cid2 C cid4 1 cid2 cid2 M For decision proﬁle combination performed column k Dempsters rule resulting degree support class ck follows cid16 cid15 ck cid15 m1 k mM k cid16cid15 cid16 ck m 12 The ﬁnal class c argmaxmck 1 cid2 k cid2 C assigned instance d The combination performed way Dempsters rule seen classaligned method Rogovas work based original idea proposed Mandler Schurmann 35 extended work aspects regardless reference vectors generated The ﬁrst aspect introduce generic form proximity measure φ allowing different distance measures applied compute classconditional probabilitiesbasic probability assignments The second obtain support ck combining simple mass functions given formulae 9 10 However issue extension use opponent ck ck explicitly speciﬁed If ck C ck formula 10 sense combination mi j Rogova notation k mk mk questionable mi 43 AlAnis method A similar attempt apply DS theory evidence combining neural network outputs carried AlAni et al 1 This method treated distance reference vector classiﬁer output piece evidence But difference previous work way obtains reference vectors It ﬁrst initializes reference vectors class iteratively uses training instances optimize reference vectors minimizing mean square errors combined classiﬁer outputs target outputs ensuring optimized reference vectors achieved Finally distance optimized reference vectors classiﬁer outputs deﬁned piece evidence represented simple support function Let E k optimized reference vector For instance d classiﬁer produces output vector ϕid 1 cid2 cid2 M simple support function deﬁned mi k cid15 ck j cid16 μi kcid9C j1 μi gi j1 μi expcid14E k cid9C mi kC gi gi j ϕidcid142 gi coeﬃcient tuned The combination method Rogovas μi k static However way obtaining reference vectors minimizing overall mean square error makes 11 13 14 1738 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 process combining classiﬁers trainable lead better performance static combination scheme additional cost training additional training data 44 Denoeuxs method Denoeux 17 proposed evidence theoretic knearest neighbors kNN method classiﬁcation problems based DS theory Unlike methods designed combining classiﬁers ensemble learning method focuses single classiﬁer ϕ classifying new instances accounting distances neighbors determine class labels Formally let D training data set instance d D let Φ set knearest neighbors d according distance measures Euclidean distance Classifying d means assigning classes ck C based weights representative classes neighbors Thus distance d neighbor di Φ considered piece evidence support proposition class membership d The evidence represented simple support function follows cid16 cid15 ck φd di mi miC 1 φd di mi A 0 A 2C di Φ cid2 cid4 ck C 15 16 17 φ suggested expγ μi2 μi cid14d dicid14 This formulation appears similar Rogovas method considering speciﬁc distance measures However Denoeuxs method different Rogovas sense computes distances feature space works decision classiﬁer output space Therefore decision proﬁle method slightly different given Fig 1 For decision matrix neighbor treated row column corresponds classes combination Dempsters rule carried column In order improve classiﬁcation accuracy effective decision procedure proposed determine optimal nearoptimal parameter values data minimizing error function 50 Denoeuxs method shows advantage permitting clear distinction presence conﬂicting informationas happens instance close neighbors different classesand incomplete information instance far away instances neighborhood It proves competitive standard kNN methods A similar idea adapted neural network classiﬁer Denoeux 16 5 Triplet mass function In section development key evidence structurethe triplet formulation Starting analyzing computational complexity combining multiple pieces evidence consider eﬃcient method combining evidence established Given M pieces evidence represented formula 2 computational complexity combining pieces evidence Eq 3 dominated number elements C number classiﬁers M In worst case time complexity combining M pieces evidence O CM1 One way reducing computational complexity reduce number pieces evidence combined combination evidence carried partition frame discernment C fewer focal elements C including possible answers question The partition place C computations orthogonal sum carried 42 For example dichotomous structure partition frame discernment C subsets ϑ1 ϑ2 number mass functions represent evidence favor ϑ1 ϑ2 lack evidence ignorance It shown Dempsters rule implemented way number computations increases linearly number elements C mass functions combined focused subsets ϑ1 singleton ϑ2 complement ϑ1 O C 22 Another approach reducing computational complexity Dempsters rule approximate calculation belief functions coarsened frame discernment detailed 15 The partitioning technique enables large problem broken smaller tractable problems However fundamental issue applying technique select elements contain possibly correct answers propositions corresponding C An intuitive way select element highest degree conﬁdence Indeed classiﬁer outputs approximate class posteriori probabilities selecting maximum probability reduces selecting output certain decisions This justiﬁed perspectives First probability assignments given mula 2 quantitative representation judgments classiﬁers propositions greater values likely decisions correct Thus selecting maximum distinguishes trivial decisions important ones Second combination decisions lower degrees conﬁdence contribute performance crease combined classiﬁers combination classiﬁers decisions complicated 48 The drawback selecting maximum combined performance reduced single dominant classiﬁer repeatedly provides high conﬁdence values Contenders higher values chosen ﬁnal classiﬁcation decisions correct Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1739 To cope deﬁciency resulting maximal selection propose second maximum decision account combining classiﬁers Its inclusion provides valuable information contained discarded class labels maximal selection combining classiﬁers extent avoids deterioration combined performance caused errors resulting single dominant classiﬁer repeatedly produces high conﬁdence values We propose novel structurea tripletpartitioning list decisions ϕd subsets Deﬁnition 4 Let C frame discernment choice ci C proposition instance d classiﬁed category ci Let ϕd s1 s2 sC list scores localized mass function deﬁned mapping function m 2C 0 1 bpa ci C 1 cid2 cid2 C follows cid16 cid15 ci m sicid9C j1 s j 1 cid2 cid2 C 18 This mass function expresses degrees belief regard choices classes given instance belong With deﬁnition rewrite formula 2 ϕd mc1 mc2 mcC Deﬁnition 5 Let C frame discernment let u v focal singletons m respective mass function An expression form Y cid3u v Ccid4 deﬁned triplet satisﬁes cid16 cid15 u m cid16 cid15 v m mC 1 mass function m called triplet mass function To obtain triplet mass functions deﬁne outstanding rule Deﬁnition 6 Let C frame discernment ϕd mx1 mx2 mxn n cid3 2 outstanding rule focusing operator denoted mσ breaks ϕd makes cid16 m u argmax m cid16 m cid16cid4cid16 19 cid15cid2cid15 cid15cid2 cid15 x2 cid15 xn x1 cid3 cid4cid16 cid3 x x1 xn u x cid16 mσ C 1 v argmax m cid16 cid15 mσ v cid15 u mσ 20 21 Clearly mσ triplet mass function referred twopoint mass function Based notation mula 2 simply rewritten formula 22 cid16 mσ cid4 cid16 mσ C cid15 v cid15 u cid2 mσ ϕid 1 cid2 cid2 M 22 For simplicity write ϕid mu mv mC With deﬁnition triplet easy illustrate meets conditions given Deﬁnition 1 We mass mC represents ignorance Given triplet cid3u v Ccid4 mu mv mC 1 let A u A v bel A cid5 m X m cid15 u cid16 pls A Xu cid5 Xucid10 m X m cid16 cid15 u mC ignorance A pls A bel A m cid16 cid15 u mC m cid16 cid15 u mC From formulation seen u gives maximum quantitative judgments representing class highest degree conﬁdence support list decisions It implies decision strong possibility correct v represents class second highest degree conﬁdence decision list This decision likely correct u However support important combining decisions making maximal selection lose valuable information contained discarded class labels Moreover including v avoid deterioration combined performance caused single error classiﬁer Apart support u v certain conﬁdence remains unassigned assigned entire set classesit committed frame discernment C The triplet evidence structure intuitively interpreted follows If classiﬁer successfully assign instance correct class occasions likely classiﬁer classify instance correctly We use ignorance concept capture important information situation 1740 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 To develop formulae combining triplet mass functions need consider relation pairs singletons triplets Suppose given triplets cid3x1 y1 Ccid4 cid3x2 y2 Ccid4 xi yi C 1 2 associated triplet mass functions m1 m2 The enumerative relations pairs focal points x1 y1 x2 y2 illustrated 1 Two focal points equal 1 x1 x2 y1 y2 x1 y2 y1 x2 2 x1 y2 y1 x2 x1 x2 y1 y2 In case combination triplet functions involves different focal elements 2 One focal point equal 1 x1 x2 y1 cid10 y2 x1 y2 y1 x2 y1 y2 2 x1 cid10 x2 y1 y2 x1 y2 x2 y1 x1 x2 3 x1 y2 y1 cid10 x2 x1 x2 y1 x2 y1 y2 4 y1 x2 x1 cid10 y2 x1 x2 x1 y2 y1 y2 In case combination triplet functions involves different focal elements 3 Totally different focal points x1 cid10 x2 y1 cid10 y2 x1 cid10 y2 y1 cid10 x2 x1 x2 y1 y2 x1 y2 y1 x2 combination involves ﬁve different focal elements The different cases require different formulae combination In cases 2 3 combinations multiple triplet functions iteratively performed interested combining twopoint focuses However applying outstanding rule Deﬁnition 6 combined results triplets transformed triplet mass function In following sections seek general formulae combining triplet mass functions based different cases present combination algorithm 51 Two focal points equal Considering case focal singletons x1 y1 triplet equal x2 y2 triplet x1 x2 y1 y2 x1 cid10 y1 xi yi C 1 2 triplet mass function m1 m2 cid16 cid16 cid15 x cid15 x m1 m2 cid16 cid16 cid15 y cid15 y m1 m2 m1C 1 m2C 1 First need combination m1 m2 exist establish formulae compute combination To ensure combinability triplet mass functions need conditions triplet mass functions m1 1 cid10 0 By formula 3 combine m1 m2 m2 conﬂict normalization factor K 1 greater zero obtain K 0 1 m1xm2 y m1 ym2x m1xm2 y m1 ym2x 1 Under condition establish formulae computing combination triplet mass functions 1 1 m1xm2 y m1 ym2x K 1 cid10 0 true K cid16 x K cid15 cid16 m1 m2 m2 cid15 cid16 m1 m2 m2 m1 m2C K m1Cm2C cid15 x cid15 y cid17 m1 cid17 m1 K y cid16 cid16 cid15 x cid15 y cid15 x cid15 y cid16 m2C m1Cm2 cid16 m2C m1Cm2 cid15 x cid15 y cid16cid18 m1 cid16 m1 cid16cid18 1 1 K cid5 XY m1 Xm2Y 1 m1 cid15 x cid16 m2 cid15 y cid16 cid15 y cid16 m2 cid15 x cid16 m1 52 One focal point equal 23 24 25 26 We consider case given triplet mass functions m1 m2 pairs singletons x y x z y cid10 z x y z C focal element triplet equal triplet Following procedure given Section 51 m1 m2 combinable following condition held cid15 x cid15 y cid15 y cid15 x cid15 z cid15 z 1 cid16 cid16 cid16 cid16 m2 cid16 m2 cid16 m2 m1 m1 m1 Thus orthogonal sum operation general formulae computing combination triplet mass cid16 x functions given cid17 K m1 cid15 K m1 y K m1Cm2 cid15 cid15 m1 m2 x cid15 cid16 m1 m2 m2C cid15 cid15 z m1 m2 m1 m2C K m1Cm2C cid16 m2 cid15 x y cid16 cid16 z cid16 cid16 cid15 x cid16 m2C m1Cm2 cid15 x cid16cid18 m1 27 28 29 30 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1741 cid16 cid15 cid16 K m1 cid15 z cid15 x cid16 m2 cid16 m2 cid15 y 1 1 m1 cid16 m2 31 It noted new mass function m1 m2 longer triplet mass function It involves different focal elements x y z C For triplet functions combining process proceed iteratively interested twopoint focuses However applying outstanding rule combined result transformed new triplet mass function We computational steps By Deﬁnition 6 new function m1 m2σ follows cid15 y cid15 x m1 z cid16 m1 m2σ m1 m2σ m1 m2σ C 1 cid16 cid15 x cid16 cid16 cid15 y cid16 cid15 y cid16 f y m1 m2 cid16 cid15 z f x cid16 m1 m2 To obtain m1 m2σ assume cid15 x Then focal element x cid15 x f x m1 m2 cid16 cid16 f x cid16 argmax f x f y f z m1 m2 x cid16 cid16 For focal element y cid16 cid16 cid16 cid15 y cid16 m1 m2 f y cid16 argmax f t t x y z x cid16 y For focal element C cid16 m1 m2σ C 1 f x f y cid16 53 Completely different focal points 32 33 34 Finally let examine case focal element common triplets As indicated previously combi nation triplet mass functions involve ﬁve different focal elements Let m1 m2 triplet functions x y u v x cid10 y x cid10 u y cid10 v x y u v y C pairs focal elements following conditions cid15 x cid15 u m1C 1 m2C 1 cid15 y cid15 v m1 m2 m1 m2 cid16 cid16 cid16 cid16 Following patten previous sections shown m1 m2 combinable following constraint holds cid15 x cid16 m2 cid16 cid15 u cid15 x cid16 m2 cid15 v cid16 m1 cid15 y cid16 m2 cid15 u cid16 cid15 y cid16 m2 cid15 v cid16 m1 m1 1 m1 Given condition derive formulae computing focal element cid16 cid16 cid15 m1 m2 x cid15 y m1 m2 cid16 cid15 m1 m2 u cid15 v m1 m2 cid16 cid15 y cid15 y K m1 K m1 K m1Cm2 K m1Cm2 cid16 m2C f x cid16 m2C f y cid15 f u z cid15 f v z cid16 cid16 cid5 1 1 K m1 Xm2Y XY cid15 x 1 m1 cid16 m2 cid15 u cid16 cid15 cid16 m2 cid15 v cid16 x m1 cid15 y cid16 m2 cid15 u cid16 cid15 y cid16 m2 cid15 v cid16 m1 m1 35 36 37 38 39 However situation occurs Section 52 combination m1 m2 longer triplet mass function involves ﬁve focal elements x y u v C combinations triplet functions invalid context To obtain new triplet mass function need apply outstanding rule combined results More speciﬁcally Deﬁnition 6 obtain new function m1 m2σ follows m1 m2σ m1 m2σ m1 m2σ C 1 cid16 cid15 x cid16 cid16 cid15 y cid16 Then focal element x cid15 x m1 m2 cid16 f x cid16 cid16 cid16 argmax f x f y f u f v x cid16 40 1742 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 combine formulae 2326 cid16 T cid16 ct ct t focuses equal t ct endif focus equal t ct 1 set T set triplet mass functions 2 ct holds combined results triplet mass functions 3 set ct t 4 t T t 5 6 7 8 9 10 11 12 13 14 15 16 endfor 17 return ct ct ct t ct ctσ ct ct t ct ctσ endelseif endelse combine formulae 2731 transform new triplet formulae 3234 combine formulae 3539 transform combined results new triplet formulae 4042 Algorithm 1 Combine multiple triplet mass functions For focal element y cid16 cid16 cid16 cid15 y cid16 m1 m2 f y cid16 argmax f t t x y u v x cid16 y Finally focal element C f y cid16 m1 m2σ C 1 f x cid16 41 42 We shown triplet mass functions combinable conditions hold established formulae computing combination triplet mass functions These formulae provide way compute combinations triplet mass functions eﬃciently help develop general algorithm combining multiple triplet mass functions complex situation evidence sources triplets independent Suppose given M triplet mass functions m1 m2 mM algorithm combined order Dempsters rule commutative associative That means arrange triplet mass functions certain order based cases mentioned By repeatedly applying outstanding rule computational step combining triplet mass functions results transformed new triplet mass function Formula 43 pairwise orthogonal sum calculation combining number triplets performed algorithm Its time complexity approximately O 2 C ﬁnal decision class largest degree support classes m m1 m2 mM m1 m2 mM 43 cid17 cid17 cid18cid18 54 Focusing pointsquartet In previous sections presented formulation triplet functions developed formulae combining Similarly consider threepoint focuses fourpoint focuses focuses In general mass function focal singletons use outstanding rule σ focus 3 points terms quartet Let m mass function focal singletons x1 x2 xn n cid3 4 n cid2 C Then focusing operator σ obtain m follows cid15 mσ u mσ C 1 cid15 v cid15 z mσ mσ cid16 cid16 cid16 u argmax m cid15cid2cid15 cid15cid2 cid15cid2 cid16cid4cid16 cid16 m cid16 m cid15 cid15 x1 xn x2 cid4cid16 x x x1 xn u cid4cid16 x x x1 xn u v cid16 cid15 cid15 v u cid15 z mσ mσ cid16 cid16 v argmax m z argmax m mσ C 1 mσ 47 u v z threepoint focuses The structure quartet conceptually simple added advantage handle case classiﬁer results diverse For case theoretical analysis clearly complicated triplet structure In addition properties mass function ignorance corresponding analysis needs address case situation focuses ordered masses pieces evidence combined For example need consider cases focuses focuses identical focus shared focuses common More details extension triplet quartet 6 44 45 46 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1743 Table 1 General description datasets Dataset Instance Class anneal audiology balance car glass autos iris letter heart segment soybean wine Zoo 798 200 625 1728 214 205 150 20 000 303 1500 683 178 101 6 23 3 4 7 6 3 26 5 7 19 3 7 Attribute Categorical Numerical 32 69 4 6 0 10 0 0 8 0 35 0 15 6 0 0 0 9 15 4 16 5 19 0 13 2 Table 2 General description thirteen learning algorithms No Classiﬁer Description 0 1 2 3 4 5 6 7 8 9 10 11 12 AOD NaiveBayes SMO IBk IB1 KStar DecisionStump J48 RandomForest DecisionTable JRip NNge PART Perform classiﬁcation averaging small space alternative naiveBayeslike models weaker detrimental independence assumptions naive Bayes The Naive Bayes classiﬁer kernel density estimation multiple values continuous attributes instead assuming simple normal distribution Sequential minimal optimization algorithm training support vector classiﬁer polynomial RBF kernels A instancebased learning algorithm It uses simple distance measure ﬁnd training instance closest given test instance predicts class training instance The IBk instancebased learner K 1 nearest neighbors order offset KStar maximally local learner The K instancebased learner nearest neighbors entropybased distance Building decision stump conjunction boosting algorithm Decision tree induction Java implementation C45 Constructing random forests classiﬁcation A decision table learner A propositional rule learnera Java implementation Ripper It repeats incremental pruning produce error duction Nearest neighborlike algorithm nonnested generalized exemplars Generating PART decision list classiﬁcation 6 Experimental evaluation 61 Experimental settings In experiments thirteen data sets downloaded UCI machine learning repository 3 All selected data sets classes required evidential structures The details data sets Table 1 For generating individual base classiﬁers thirteen learning algorithms taken Waikato Environment Knowledge Analysis Weka version 34 Table 2 These algorithms simply chosen basis performance data sets randomly picked They ensembles classiﬁers Parameters algorithm set default settings Detailed description algorithms 55 To reﬂect ensemble performance faithfully avoid overﬁtting extent experiments performed partition scheme tenfold cross validation We divided data sets 10 mutually exclusive sets For fold excluding test set training set subdivided 70 new training set 30 validation set Apart evaluating performance individual classiﬁers validation set select best combination classiﬁers The performance combining selected classiﬁers DS MV majority voting combining schemes evaluated testing set Seven groups experiments reported carried individually combination thirteen data sets These evaluating performance 13 algorithms shown Table 2 experimenting combinations individual classiﬁers DS classiﬁer outputs repre sented triplet quartet functions respectively 1744 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 Fig 3 An example combination individual classiﬁers ϕ1 ϕ2 triplet structure fold tenfold cross validation T1 number instances fold training data examining performance combining 13 individual classiﬁers different orders decreasing corre sponding increasing increasing DS experimenting combinations individual classiﬁers represented dichotomous structure DS dichotomous mass functions deﬁned basis performance classiﬁers terms recognition substitution rejection rates 49 conducting experiments combining individual classiﬁers represented simplet structure DS The simplet mass functions deﬁned based Rogovas method Section 42 classiﬁers generated learning algorithms shown Table 2 neural networks experimenting combinations classiﬁers DS classiﬁer outputs represented list decisions contenders experimenting combinations individual classiﬁers MV individual outputs single class labels It noted ensemble construction involves 213 combinations 13 individual classiﬁers evidential struc ture data set fold The computational cost structures tenfold cross validation requires 213 13 5 10 combinations total Instead exhausting combinations classiﬁers ranked individual classiﬁers based performance combined decreasing order suggested 18 For example took best individual classiﬁer combined second best best forth achieved best accuracy combined classiﬁers During course combination hybrid order non consecutive dering involved combining classiﬁers order ﬁnd best combination classiﬁers Fig 3 presents example combining individual classiﬁers classiﬁer outputs represented triplets To compare classiﬁcation accuracies individual classiﬁers ensemble classiﬁers data sets employed ranking statistics terms win draw loss WDL record Webb 53 The windrawloss record presents values number data sets classiﬁer A obtained better equal worse classiﬁer B respect classiﬁcation accuracy All collected classiﬁcation accuracies measured averaged F measure 55 A paired ttest domains carried determine differences base classiﬁers combined classiﬁers ensembles classiﬁers statistically signiﬁcant 005 level 62 The basics Kappa κ statistics To examine reliability ensemble performance performed κ Kappa statistic analysis extent level agreement combined classiﬁers different evidential structures DS extent agreement majority voting Dempsters rule combining base classiﬁers The κ statistic widely pairwise method measure level agreement disagreement ratersclassiﬁers 29 It thought chancecorrected proportional agreement 20 Formally given classi ﬁers ϕ1 ϕ2 testing data set T construct global contingency table entry E j contains number instances x T ϕ1 ϕ2 j If ϕ1 ϕ2 identical data set nonzero counts appear diagonal table number counts diagonal Now deﬁne μ1 μ2 cid9 L i1 E ii T cid19 Lcid5 Lcid5 i1 j1 cid20 E j T Lcid5 j1 E ji T Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1745 Fig 4 Comparison running time simplet triplet quartet dichotomous structures μ1 probability classiﬁers agree μ2 correction term μ1 estimating probability classiﬁers agree simply chance Then κ statistic deﬁned follows κ μ1 μ2 1 μ2 κ 0 agreement classiﬁers equals expected chance κ 1 classiﬁers agree testing instances negative values κ mean agreement expected chance 63 Special cases triplets quartets In experiments ensure Dempsters rule properly applied combine triplet mass functions quartet mass functions making sure N cid10 0 required Deﬁnition 3 adjustments following special cases The ﬁrst given mass functions m1 m2 pairs singletons x1 y1 x2 y2 set C x1 cid10 x2 y1 cid10 y2 xi cid10 y j j 1 2 m1x1 1 m1 y1 1 m2x2 1 m2 y2 1 intersection m1 m2 committed set consequently condition N cid10 0 hold Thus necessary redistribute masses triplets uncertainties lie In situation discount masses xi y j j 1 2 miC 1 2 small value α defaulted 0001 experiments The addition α represents uncertainty involved classiﬁcation process justiﬁed ways 1 generating classiﬁers approximation process class conditional probabilities estimated classiﬁers 100 certainty 2 α play role improving combined performance deteriorating combined effect The second case usual m mass function derived classiﬁer output x y pair singletons C set classes If classiﬁer output instance nil means classiﬁer able assign class label instance mx m y 0 For situation reallocate 1 mC Making mC 1 regarded representation uncertainty classifying instance classiﬁer fact ensures N cid10 0 m combined mass function m affect value m m The case given resulting triplet mass function m singletons x y z mx m y mz To approximate m new triplet mass function determine best focus criterion identifying best choice All pick focuses random constructing triplet randomly ﬁnal decision cid16 cid16 Similar treatments administered special cases quartet mass functions 64 Experimental results 641 Run time The ﬁrst experimental result comparison running time required combining different evidential structures DS As expected combining simplets eﬃcient computation involved The empirical results time required combining triplet functions 35 longer combining simplets The time required combining dichotomous functions 1196 longer simplets average time combining quartets 1691 longer simplets average The comparative results illustrated Fig 4 From results seen triplet dichotomous structures focal ele ments computational time required combining triplet functions signiﬁcantly required combining 1746 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 Table 3 The classiﬁcation accuracy best base classiﬁer best combined classiﬁers based structures simplet triplet quartet dichotomous structure fullist DS MV 13 data sets Datasets Anneal Audiology Balance Car Glass Autos Iris Letter Heart Segment Soybean Wine Zoo Average WDL Sig win Individual Simplet 8023 4867 6567 8962 6536 7759 9533 9205 3548 9669 9589 9890 9062 7939 8157 5367 6317 9540 6491 7846 9667 9241 3602 9748 9692 10000 9361 8079 1102 7 Triplet 8157 5744 6317 9429 6681 7917 9667 9291 3709 9728 9669 10000 9361 8130 1201 7 Quartet Dichotomous 7977 5744 6444 9696 6491 7808 9667 9254 3703 9668 9688 10000 9361 8115 1003 5 8068 5197 6567 9192 6626 7878 9667 9341 3537 9774 9685 9890 9361 8060 1021 5 MV 8114 5430 6272 9175 6669 7794 9667 9277 3437 9655 9617 9897 9361 8028 1003 4 Fullist 6988 4932 2168 9003 6275 6680 6071 6838 3426 8840 9111 9670 6439 6649 Table 4 The extent agreement best combined classiﬁers structures agree ment combination methods DS MV Triplet DS MV Simplet 09325 09427 Quartet 09396 09273 Dichotomous 09349 09432 Triplet 09437 Average 09357 09392 dichotomous functions This mainly second element dichotomous structure subset set C contains class set classes C Examining calculation process Dempsters rule diﬃcult ﬁnd optimized case combining dichotomous mass functions requires C 22 computations combining triplet mass functions DS 642 Performance summary The seven groups experimental results summarized Table 3 The ﬁrst column lists data sets second column gives classiﬁcation accuracies best individual classiﬁers rest columns represent accuracies best combined classiﬁers DS MV data sets If difference best combined classiﬁer best individual base classiﬁer data set statistically signiﬁcant larger shown bold The table provides summary statistics comparing performance best base classiﬁers best combined classiﬁers data sets From summary observed accuracy combined classiﬁers based triplet structure DS better ﬁve average It wins loses simplet quartet dichotomous structure best combined classiﬁers MV compared best individual classiﬁers This conspicuous superiority supported number statistically signiﬁcant winsthe triplet quartet dichotomous structure MV 643 κ statistics For examining reliability ensemble performance selected data sets Anneal Audio Car Iris Wine Zoo ensemble performance statistically signiﬁcant better best individuals carried pairwise analysis level agreement best combined classiﬁers level agreement DS MV testing data Table 4 shows statistical results averaged data sets Within table ﬁrst row consists best combined classiﬁers denoted Simplet Quartet Dichotomous structure Triplet corresponds data sets The ﬁrst column names best combined classiﬁers DS MV simply denoted Triplet DS MV respectively They associated data sets Each remaining cell table averaged κ value represents level agreement pair classiﬁers classifying instances data sets For example cell Triplet DS Simplet 09325 averaged κ value best combined classiﬁers Triplet Simplet agree classifying instances data sets The κ values presented Table 4 indicate agreement classifying instances pairs classiﬁers substantial according rough guide provided 20 This establishes performance combined classiﬁers reliable Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1747 Fig 5 Performance different numbers classiﬁers combination different orders decreasing corresponding increasing increasing 644 Order combining classiﬁers triplet As explained Sections 52 53 combining triplet mass functions general result triplet mass function The combined results need approximated new triplet focusing operator However approximation combination triplets longer meet associative property Dempsters rule Thus different orders combination classiﬁers lead different combined performance To better understand chosen order classiﬁers affects combined performance ensemble construction conducted experiment combining classiﬁers different orders decreasing corresponding increasing cincreasing increasing The combination process follows We ranked 13 best individual classiﬁers decreasing increasing orders consecutively combined Dempsters rule With respect cincreasing order reverse process corresponding decreasing combinations For instance ﬁrst decreasing combination combine best classiﬁer second best second combine best classiﬁer second best best process repeated decreasing combinations completed The reverse process decreasing combination correspondingly combine second best classiﬁer ﬁrst combine best classiﬁer second best forth Fig 5 illustrates effect comparison orders combining classiﬁers triplet It observed combined classiﬁers cincreasing order outperforms performance increasing combination worst orders combination This suggests combining better classiﬁers perform combining worse classiﬁers Consider cincreasing order It noted performance better decreasing combination mainly occurs combination 9 classiﬁers com bination 3 8 classiﬁers decreasing performance tends cincreasing This experimental result leads conclusion order classiﬁers combination impact performance combined classiﬁers dramatic For decreasing cincreasing orders smaller number classiﬁers combined impact combination order 645 Ensemble size Fig 6 presents sizes best combined classiﬁers data sets With different structures construction ensembles involve 27 classiﬁers ensembles composed classiﬁers This result consistent previous studies 1819 different 3638 experiments showed ensemble accuracy increased ensemble size performance levels ensemble sizes 1025 The sizes ensembles classiﬁers generated different learning algorithms operate single data set necessarily combined classiﬁers constructed single learning method manipulate different portions features instances 646 Contribution ensembles Fig 7 presents contribution ensemble construction individual classiﬁer structures simplet triplet quartet dichotomous structure domains There aggregated columns different colors ﬁgure Each column represents number classiﬁer contributes construction thirteen best combined classiﬁers corresponding thirteen data sets color presents evidential structure For example ﬁrst column indicates AOD occurs times thirteen ensembles thirteen data sets simplet triplet structures seven times thirteen ensembles quartet dichotomous structure It observed classiﬁers contribute ensembles structures AOD SMO IBk KStar Of AOD best performing individually plays important role constructing effective ensembles 1748 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 Fig 6 Ensemble size number individual classiﬁers involved best combined classiﬁers data sets evidential structures simplet triplet quartet dichotomous structure Fig 7 Number individual classiﬁers 0 AOD 1 NaiveBayes Table 2 occurring thirteen combined classiﬁers thirteen data sets different structures simplet triplet quartet dichotomous structure 7 Discussion The structure simplet consists elements The ﬁrst decision list decisions simplet mass represents degree support decision derived decision proﬁle cosine function The second list decisions C mass representing uncertainty singleton element With structure list decisions associated list simplet mass functions simplet functions corresponds decisions lists share order ranking It easy illustrate combined effect simplet functions highly affected larger simplet masses Consider combination simplet mass functions m1 m2 singletons x y case x y equal By formula 3 combine m1 m2 x better supported decision m1x m2 y y better m1Θ m2Θ affect combination effect Thus ﬁnd x y better supported decision depends merely mass values x y This effect generalized combining simplet classiﬁers ϕ1 ϕ2 performance ϕ1 ϕ2 dominated single simplet classiﬁer ϕ1 ϕ2 repeatedly provides larger mass functions m1 m2 Although way deriving mass values simplets different triplets quartets broadest sense triplet quartet regarded extensions simplet structure The key difference triplet quartet simplet makes use wide range information contained second best decisions combining classiﬁers The performance combining triplet classiﬁers example determined ﬁrst second elements ignorance We conjecture use information plays important role overcoming problem identiﬁed earlierthat single error produced single classiﬁer repeatedly provides high conﬁdence values classes occur combining simplets Now look theoretical justiﬁcation claim We state formally conditions ﬁrst second decision triplets better supported decision Assume triplet functions m1 m2 fall Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1749 category pair singletons x1 y1 equal pair x2 y2 x1 x2 y1 y2 Section 51 By formulae 2326 following inequality x better choice cid15 x cid16 m2 cid15 x cid16 m1 cid15 x cid16 m2C m1Cm2 cid15 x cid16 m1 cid15 y cid16 m2 cid15 y cid16 cid15 y cid16 m2C m1Cm2 cid15 y cid16 m1 m1 Substituting C formula 48 rearranging condition better support x cid16 cid15 x m1 1 1 m1 y1 m2 y 1 m2x Likewise derive condition y better supported decision cid16 cid15 y m2 1 1 m1x1 m2x 1 m1 y 48 49 50 We obtain conditions cases triplets similar manner Formulae 49 50 present conditions determining x y better choice The ﬁrst condition indicates example x second position list decisions ranked ﬁrst decision triplets combined long condition met This effect provides insight process combining triplet classiﬁers single classiﬁer dominate combined performance ignorance derived plays important role deciding best supported decisions This explanation superior performance triplet simplet A similar analysis case combining quartets carried way obtaining possible account quartet outperforms simplet With respect dichotomous structure drawback way measuring evidence ignores fact classiﬁers normally performance different classes cause deterioration performance combined classiﬁers Section 41 It surprise performance combining classiﬁers form triplets quartets better combining classiﬁers list decisions The reason different individual classiﬁers produce distributions classconditional probabilities classes The classes predicted values range zero When lists combined orthogonal sum operation ϕ1 ϕ2 produce lists decisions similar distribution nonzero values appear diagonal intersection table orthogonal sum calculation Otherwise large number values diagonal In situation combination lists committed conﬂicting class labels Examining calculation orthogonal sums closely ﬁnd conﬂict incurred combining classiﬁers form list larger combining triplet classiﬁers Such conﬂict result poor combined performance classiﬁers list This ﬁnding consistent result illustrated Fig 5 previous studies combination decisions lower degrees conﬁdence contribute increase combined performance classiﬁers combination decisions complicated 48 8 Independence assumption Making independence assumption set variables set attribute values common practice learning tasks Bayesian belief networks naive Bayes classiﬁer 37 Such assumption dramatically reduces complexity learning classiﬁers makes computational process tractable This true applications instance present context modelling classiﬁer outputs independent bodies evidence The independence assumption practical value little information available substantially deteriorated effectiveness classiﬁers applications As mentioned Deﬁnition 3 condition Dempsters rule pieces evidence combined independent However precise meaning independence practice diﬃcult specify In effort clarify Dempster 13 explained opinions different people based overlapping experiences regarded inde pendent sources This subsequently complemented statement Denoeux 14 said nonoverlapping random samples population clearly distinct items evidence In present context possible argument independence pieces evidence derived classiﬁer outputs fact classiﬁer outputs arise sample instance However time argued classiﬁer outputs distinct clear classiﬁers overlapping experiences determining class labels instance Somewhat philosophically argument favor independence assumption classiﬁers involved combination generated distinct learning algorithms shown Table 2 These algorithms built different theories use mechanisms search characterization data So share experiences generating classiﬁers correlative dependence internal structures classiﬁers The inherent processes producing outputs classiﬁers entirely distinct This distinctness formally interpreted follows Let ϕ1 ϕ2 distinct classiﬁers instance d ϕ1d logically imply ϕ2d vice versa mutually unrelated Consequently natural case denoting e1 proposition corresponding ϕ1d e2 1750 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 ϕ2d Section 4 e1 independent e2 probability P e1 e2 P e1 P e2 52 Therefore assumption modelling classiﬁer outputs independent pieces evidence sensible There great deal debate conditions validly applying Dempsters rule precise meaning distinct bodies evidence literature 1314334352 far conclusive study issues Shafer 43 pointed task sorting evidence independent items easy theory directs task grappling real problems assessment evidence Over past decades progress applications suﬃcient independent condition explicitly speciﬁed applications independence assumption Dempsters rule demonstrates effectiveness 16173949 It emphasized study focused developing eﬃcient effective computational method convincing practical applications based DS theory investigating alternative combination rules dependent evidence sources conﬂict management More detailed discussions aspects recent studies 1434 9 Conclusion future work We described effective framework built DempsterShafer theory evidence combining multiple classiﬁers expert opinions classiﬁcation decision making systems We developed formalism triplets quartets formulae combining classiﬁers represented modelling classiﬁer outputs terms form triplets extended quartet The distinguishing aspect classindifferent method classaligned methods selecting prioritized class decisions combined making use ignorance representing unknown uncertain class decisions A range experiments carried thirteen UCI benchmark data sets Our results performance best combined classiﬁers better best individuals data sets corresponding ensemble sizes 27 ensemble sizes 2 3 615 thirteen best ensembles The comparative analysis structures simplet triplet quartet dichotomous structure identiﬁes triplet best comparison DS MV shows DS better MV combining individual classiﬁers We κ statistic examine extent agreement different combination methods deciding classes testing instances The statistics reveal classiﬁcation performance achieved DS combination method evidential structures reliable However work touch issue classiﬁer diversity Although successful ensemble methods encourage diversity extent recent study diversity measures raised question role diversity plays constructing effective ensembles classiﬁers measured 29 To knowledge conclusive study showing measure diversity best use evaluating ensembles We working discuss ﬁndings future paper Moreover like carry comparative study alternative combination rules future These include new combination rule combining non distinct items evidence recently introduced 14 unnormalized combination rule introduced 45 Acknowledgements The authors like thank anonymous reviewers detailed constructive comments helped improve paper considerably References 1 A AlAni M Deriche A new technique combining multiple classiﬁers DempsterShafer theory evidence J Artif Intell Res 17 2002 333361 2 JA Barnett Computational methods mathematical theory evidence Proc 17th Joint Conference Artiﬁcial Intelligence 1981 pp 868 875 3 CL Blake CJE Keogh UCI repository machine learning databases httpwwwicsuciedumlearnMLRepositoryhtml 4 Y Bi D Bell JW Guan Combining evidence classiﬁers text categorization Proc KES04 2004 pp 521528 5 Y Bi Combining multiple classiﬁers text categorization DempsterShafer theory evidence PhD thesis University Ulster UK 2004 6 D Bell JW Guan Y Bi On combining classiﬁers mass functions text categorization IEEE Trans Knowledge Data Engrg 17 10 2005 13071319 7 Y Bi JW Guan An eﬃcient tripletbased algorithm evidential reasoning Proc 22nd Conference Uncertainty Artiﬁcial Intelligence 2006 pp 3138 8 Y Bi SI McClean T Anderson On combining multiple classiﬁers evidential approach Proc TwentyFirst National Conference Artiﬁcial Intelligence AAAI06 2006 pp 324329 9 L Breiman Bagging predictors Machine Learning 24 2 1996 123140 10 L Breiman Random forests Machine Learning 45 1 2001 532 T Dietterich Machine learning research Four current directions AI Magazine 18 4 1997 97136 11 T Dietterich An experimental comparison methods constructing ensembles decision trees Bagging boosting randomization Machine Learning 1 22 1998 12 T Dietterich Ensemble methods machine learning Proc 2nd Int Workshop Multiple Classiﬁer Systems MCS2000 LNCS vol 1857 2000 pp 115 13 AP Dempster Upper lower probabilities induced multivalued mapping Ann Math Stat 38 1967 325339 Y Bi et al Artiﬁcial Intelligence 172 2008 17311751 1751 14 T Denoeux Conjunctive disjunctive combination belief functions induced nondistinct bodies evidence Artif Intell 172 23 2008 234264 15 T Denoeux A Ben Yaghlane Approximating combination belief functions fast Moebius transform coarsened frame Internat J Approx Reason 31 12 2002 77101 16 T Denoeux A neural network classiﬁer based DempsterShafer theory IEEE Trans Systems Man Cybernet A 30 2 2000 131150 17 T Denoeux A knearest neighbor classiﬁcation rule based DempsterShafer theory IEEE Trans Systems Man Cybern 25 5 1995 804813 18 RPW Duin DMJ Tax Experiments classiﬁer combining rules J Kittler F Roli Eds Multiple Classiﬁer Systems 2000 pp 1629 19 S Dzeroski B Zenko Is combining classiﬁers stacking better selecting best Machine Learning 54 3 2004 255273 20 JL Fleiss J Cuzick The reliability dichotomous judgments unequal numbers judgments subject Appl Psycholog Meas 3 1979 537542 21 Y Freund R Schapire Experiments new boosting algorithm Machine Learning Proceedings Thirteenth International Conference 1996 pp 148156 22 JW Guan DA Bell Evidence Theory Its Applications vols 12 Studies Computer Science Artiﬁcial Intelligence vols 78 Elsevier North Holland 19911992 23 JW Guan DA Bell Eﬃcient algorithms automated reasoning expert systems The 3rd IASTED International Conference Robotics Manufacturing 1995 pp 336339 24 R Haenni Are alternatives Dempsters rule combination real alternatives Comments belief function combination conﬂict management problem Lefèvre et al Information Fusion 3 3 2002 237239 25 LK Hansen P Salamon Neural network ensembles IEEE Trans Pattern Anal Machine Intell 12 10 1990 9931001 26 TK Ho The random subspace method constructing decision forests IEEE Trans Pattern Anal Machine Intell 20 8 1998 832844 27 J Kittler M Hatef RPW Duin J Matas On combining classiﬁers IEEE Trans Pattern Anal Machine Intell 20 3 1998 226239 28 L Kuncheva Combining classiﬁers Soft computing solutions SK Pal A Pal Eds Pattern Recognition From Classical Modern Approaches 2001 pp 427451 29 L Kuncheva CJ Whitaker Measures diversity classiﬁer ensembles Machine Learning 51 2003 181207 30 AK Jain RPW Duin J Mao Statistical pattern recognition A review IEEE Trans Pattern Anal Machine Intell 22 1 2000 437 31 L Lam CY Suen Application majority voting pattern recognition An analysis behavior performance IEEE Trans Systems Man Cyber net 27 5 1997 553568 32 LS Larkey WB Croft Combining classiﬁers text categorization Proceedings SIGIR96 19th ACM International Conference Research Development Information Retrieval 1996 pp 289297 33 W Liu J Hong Reinvestigating Dempsters idea evidence combination Knowledge Inform Syst 2 2 2000 223241 34 W Liu Analyzing degree conﬂict belief functions Artif Intell 170 11 2006 909924 35 EJ Mandler J Schurmann Combining classiﬁcation results independent classiﬁers based DempsterShafer theory evidence Pattern Recogn Artif Intell X 1988 381393 36 P Melville RJ Mooney Constructing diverse classiﬁer ensembles artiﬁcial training examples Proc IJCAI03 2003 pp 505510 37 T Mitchell Machine Learning McGraw Hill 1997 38 D Opitz Feature selection ensembles Proc AAAI99 AAAI Press 1999 pp 379384 39 B Quost T Denoeux MH Masson Pairwise classiﬁer combination belief functions Pattern Recogn Lett 28 5 2007 644653 40 F Sebastiani Machine learning automated text categorization ACM Comput Surv 34 1 2002 147 41 G Rogova Combining results neural network classiﬁers Neural Networks 7 5 1994 777781 42 G Shafer R Logan Implementing Dempsters rule hierarchical evidence Artif Intell 33 3 1987 271298 43 G Shafer Belief functions possibility measures JC Bezdek Ed The Analysis Fuzzy Information vol 1 Mathematics Logic CRC Press 1987 pp 5184 44 G Shafer A Mathematical Theory Evidence Princeton University Press Princeton NJ 1976 45 Ph Smets The combination evidence Transferable Belief Model IEEE Trans Pattern Anal Machine Intell 12 5 447458 46 KM Ting IH Witten Issues stacked generalization J Artif Intell Res JAIR 10 1999 271289 47 DMJ Tax M van Breukelen RPW Duin J Kittler Combining multiple classiﬁers averaging multiplying Pattern Recognition 33 9 2000 14751485 48 K Tumer GJ Robust Combining disparate classiﬁers order statistics Pattern Anal Appl 6 1 2002 4146 49 L Xu A Krzyzak CY Suen Several methods combining multiple classiﬁers applications handwritten character recognition IEEE Trans System Man Cybernet 2 3 1992 418435 50 LM Zouhal T Denoeux An evidencetheoretic kNN rule parameter optimization IEEE Trans Systems Man Cybernet C 28 2 1998 263271 51 Y Yang T Ault T Pierce Combining multiple learning strategies effective cross validation Proc ICML00 2000 pp 11671182 52 F Voorbraak On justiﬁcation Dempsters rule combination Artif Intell 48 2 1991 171197 53 GI Webb MultiBoosting A technique combining boosting wagging Machine Learning 40 2 2000 159196 54 D Wolpert Stacked generalization Neural Networks 5 2 1992 241259 55 IH Witten E Frank Data Mining Practical Machine Learning Tools Techniques second ed Morgan Kaufmann San Francisco 2005