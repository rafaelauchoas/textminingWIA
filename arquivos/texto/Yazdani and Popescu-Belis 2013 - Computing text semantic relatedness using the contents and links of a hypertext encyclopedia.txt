Artiﬁcial Intelligence 194 2013 176202 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Computing text semantic relatedness contents links hypertext encyclopedia Majid Yazdani ab Andrei PopescuBelis Idiap Research Institute 1920 Martigny Switzerland b EPFL École Polytechnique Fédérale Lausanne 1015 Lausanne Switzerland r t c l e n f o b s t r c t Article history Available online 19 June 2012 Keywords Text semantic relatedness Distance metric learning Learning rank Random walk Text classiﬁcation Text similarity Document clustering Information retrieval Word similarity 1 Introduction We propose method computing semantic relatedness words texts knowledge hypertext encyclopedias Wikipedia A network concepts built ﬁltering encyclopedias articles concept corresponding article Two types weighted links concepts considered based hyperlinks texts articles based lexical similarity We propose implement eﬃcient random walk algorithm computes distance nodes sets nodes visiting probability set nodes Moreover algorithm tractable propose validate empirically truncation methods use embedding space learn approximation visiting probability To evaluate proposed distance apply method important tasks natural language processing word similarity document similarity document clustering classiﬁcation ranking information retrieval The performance method stateoftheart close task demonstrating generality knowledge resource Moreover hyperlinks lexical similarity links improves scores respect method hyperlinks bring additional realworld knowledge captured lexical similarity 2012 Elsevier BV All rights reserved Estimating semantic relatedness text fragments words sentences entire documents important natural language processing information retrieval applications For instance semantic relatedness spelling correction 1 word sense disambiguation 23 coreference resolution 4 It shown help inducing information extraction patterns 5 performing semantic indexing information retrieval 6 assessing topic coherence 7 Existing measures semantic relatedness based lexical overlap widely little help text similarity based identical words Moreover assume words independent generally case Other measures PLSA LDA attempt model probabilistic way relations words topics occur texts use structured knowledge available large scale word distribution properties Therefore computing text semantic relatedness based concepts relations linguistic extralinguistic dimensions remains challenge especially general domain andor noisy texts Corresponding author Idiap Research Institute 1920 Martigny Switzerland Email addresses majidyazdaniepﬂch majidyazdaniidiapch M Yazdani andreipopescubelisidiapch A PopescuBelis 00043702 matter 2012 Elsevier BV All rights reserved httpdxdoiorg101016jartint201206004 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 177 In paper propose compute semantic relatedness sets words knowledge enclosed large hypertext encyclopedia speciﬁc reference English version Wikipedia experimental We propose method exploit knowledge estimating conceptual relatedness deﬁned Section 2 following statistical unsupervised approach improves past attempts reviewed Section 3 making use large scale weakly structured knowledge embodied links concepts The method starts building network concepts assumption encyclopedia article corresponds concept node network Two types links nodes constructed original hyperlinks articles lexical similarity articles content Section 4 This resource estimating semantic relatedness text fragments sets words follows Each fragment ﬁrst projected concept network Section 5 Then resulting weighted sets concepts compared graphbased distance computed based distance concepts This estimated visiting probability VP random walk network concept following types links Section 6 Visiting probability integrates density connectivity length path factors computing relevant measure conceptual relatedness network Several approximations based truncation paths proposed Section 7 justiﬁed Section 8 order computations tractable large network 12 million concepts 35 million links Moreover method learn approximation visiting probability embedding space Section 9 shown solution tractability problem To demonstrate practical relevance proposed resources network distance approxi mations apply natural language processing problems beneﬁt accurate semantic distance word similarity Section 10 document similarity Section 11 document clustering classiﬁcation Sections 12 13 respectively information retrieval Section 14 including learning rank Section 15 The results method competitive tasks demonstrate method provides uniﬁed robust answer measuring semantic relatedness 2 Semantic relatedness Deﬁnitions issues Two samples language said semantically related things associated world bearing inﬂuence evoked speech thought things Semantic relatedness multifaceted notion depends scale language samples words vs texts exactly counts relation In case adjective semantic indicates concerned relation senses denotations surface forms etymology 21 Nature semantic relations words texts Semantic relations words senses studied categorized linguistics freedom liberty antonymy opposition They include classical relations synonymy identity senses senses increase vs decrease hypernymy hyponymy vehicle car meronymy holonymy partwhole relation wheel car From point view semantic similarity speciﬁc semantic relatedness For instance antonyms related similar Or following Resnik 8 car bicycle similar hyponyms vehicle car gasoline pair related world Classical semantic relations listed handcrafted lexical ontological resources WordNet 9 Cyc 10 implicitly Rogets Thesaurus Jarmasz 11 inferred distributional data discussed Additional types lexical relations described nonclassical Morris Hirst 12 instance based membership similar classes positive qualities association location stereotypes relations qualify similarity ones generally listed lexical resources Budanitsky Hirst 1 point semantic distance seen contrary similarity relatedness In paper use distance refer measure semantic relatedness deﬁned At sentence level semantic relatedness subsume notions paraphrase logical relations entailment contradiction More generally sentences related similarity topic notion applies multi sentence texts notion topic diﬃcult deﬁne Topicality expressed terms continuity themes referents entities predicated ensures coherence texts Linguists analyzed coherence maintained cohesive devices 1314 include identityofreference lexical cohesion similarity chains based classical lexical relations 1516 A key relation semantic relatedness words occurrence texts long exploited researchers natural language processing NLP form distributional measures 17 despite certain limitations pointed Budanitsky Hirst 1 Section 62 The assumption sentences texts form coherent units makes possible infer word meanings lexical relations distributional similarity 18 vectorbased models Latent Semantic Analysis 19 possibly enhanced syntactic information 20 In return hidden topical parameters govern occurrences words modeled probabilistically PLSA 21 LDA 22 providing measures text similarity 178 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 22 Use encyclopedic knowledge semantic relatedness Semantic relatedness mainly considered perspective repertoires semantic relations hand crafted perspective relations inferred distributional properties words collections texts However computation use relations founded realworld knowledge considerably explored possible recently emergence largescale hypertext encyclopedias Wikipedia We believe use encyclopedic knowledge signiﬁcantly complement semantic relatedness measures based word distributions remainder section brieﬂy frame encyclopedic knowledge outline proposal discuss taskbased validation Encyclopedias lists general concepts named entities accompanied descriptions natural language They differ dictionaries concepts entities deﬁne words provide signiﬁcant factual knowledge grounding real world linguistic information While printed encyclopedias al ready include certain references entry linking mechanism extensively hypertext encyclopedias Wikipedia As result hypertext encyclopedias adept capturing semantic lations concepts range culturespeciﬁc universal ones including classical nonclassical relations mentioned To measure extent text fragments semantically related according encyclopedia main operations necessary First concepts related texts identiﬁed These concepts directly mentioned texts related sense speciﬁed Section 5 Second relatedness proximity sets concepts identiﬁed measured This presupposes capacity measure relatedness concepts ﬁrst place taking advantage contents corresponding articles hyperlinks Section 6 Empirical evidence support deﬁnition relatedness demonstrate relevance NLP applications cited beginning paper The measure proposed judged compared based performance variety tasks following empirical stance similar expressed introductions articles Budanitsky Hirst 1 Padó Lapata 20 cite examples Our proposal applied word similarity document similarity document clustering classiﬁcation information retrieval Sections 10 15 3 Related work This paper puts forward new method computing semantic relatedness makes use graph structure Wikipedia solve NLP problems Therefore related work spans large number domains approaches divided mainly 1 previous methods computing semantic relatedness including uses Wikipedia networked resources tasks common paper 2 previous algorithms computing distances graphs 3 stateoftheart methods scores targeted tasks In fact combinations tasks methods resources share elements proposal This section focus ﬁrst categories previous work performance comparisons stateoftheart methods application sections A synthetic view previous work appears Table 1 end section resources algorithms tasks data sets testing 31 Word semantic relatedness WordNet Wikipedia Many attempts past deﬁne word text similarity distances based word overlap applications language technology One approach construct manually semiautomatically taxonomy word senses concepts types relations map text fragments compared taxonomy For instance WordNet 9 Cyc 10 wellknown knowledge bases respectively word senses concepts overcoming strong limitations pure lexical matching A thesaurus Rogets similar purposes 1123 This approach makes use explicit senses concepts humans understand reason granularity knowledge representation limited taxonomy Building maintaining knowledge bases requires lot time effort experts Moreover cover fraction vocabulary language usually include proper names conversational words technical terms Several methods computing lexical semantic relatedness exploit paths semantic networks WordNet surveyed Budanitsky Hirst 1 Section 2 Distance network obvious criteria similarity modulated type links 24 local context applied word sense identiﬁcation 25 Resnik 826 improved distancebased similarity deﬁning information content concept measure speciﬁcity applied measure word sense disambiguation short phrases An informationtheoretic deﬁnition similarity applicable entities framed probabilistic model proposed Lin 27 applied word concept similarity This work share similar concern quest generic similarity relatedness measure albeit different conceptual frameworks probabilistic vs hypertext encyclopedia Other approaches use unsupervised methods construct semantic representation words documents analyzing mainly cooccurrence relationships words corpus Chappelier 28 review Latent M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 179 Semantic Analysis 19 offers vectorspace representation words grounded statistically applied doc ument representation terms topics Probabilistic LSA 21 Latent Dirichlet Allocation 22 These unsupervised methods construct lowdimensional feature representation concept space words longer supposed independent The methods offer large vocabulary coverage resulting concepts diﬃcult humans interpret 29 Mihalcea et al 30 compared knowledgebased corpusbased methods including instance 25 word similarity word speciﬁcity deﬁne general measure text semantic similarity Results methods combinations reported paper Because computes word similarity values word pairs proposed measure appears suitable mainly computing similarity short fragments computation quickly intractable One ﬁrst methods use graphbased approach compute word relatedness proposed Hughes Ramage 31 Personalized PageRank PPR 32 graph built WordNet 400 000 nodes 5 million links Their goal exploit possible links words graph shortest path They illustrated merits approach frequentlyused data sets word pairs paper Section 10 standard correlation metrics original Their method reaches limit human interannotator agreement strongest measures semantic relatedness uses WordNet In recent years Wikipedia appeared promising conceptual network relative noise incomplete ness collaborative origin compensated large size certain redundancy availability alignment languages Several large semantic resources derived relational knowledge base DBpedia 33 concept networks BabelNet 34 WikiNet 35 ontology derived Wikipedia WordNet Yago 36 WikiRelate 37 method computing semantic relatedness words Wikipedia Each word mapped corresponding Wikipedia article titles To compute relatedness methods proposed paths Wikipedia category structure contents articles Our method comparison uses knowledge embedded hyperlinks articles entire contents articles Recently category structure exploited WikiRelate applied computing semantic similarity words 38 Overall WikiRelate measures relatedness words applicable similarity longer frag ments unlike method Another method compute word similarity proposed Milne Witten 39 similarity hyperlinks Wikipedia pages 32 Text semantic relatedness Several studies measured relatedness sentences entire texts In study Syed et al 40 Wikipedia ontology different ways associate keywords topic names input documents 1 cosine similarity retrieval Wikipedia pages 2 spreading activation Wikipedia categories pages 3 spreading activation pages hyperlinked The evaluation ﬁrst performed articles related Wikipedia pages validated hand 100 Wikipedia pages task restore links categories similarly 41 The use private test set makes comparisons work uneasy In text labeling task Coursey et al 42 entire English Wikipedia graph 58 million nodes 65 million edges version Personalized PageRank 32 initialized Wikipedia pages related input text Wikify43 The method tested random selection 150 Wikipedia pages goal retrieving automatically manuallyassigned categories Ramage et al 44 Personalized PageRank WordNetbased graph detect paraphrases textual entailment They formulated theoretical assumption similar stationary distribution graph random walk forms semantic signature compared distribution relatedness score texts Our proposal includes novel method comparing distributions applied different tasks tested paraphrases previous paper 45 Explicit Semantic Analysis ESA proposed Gabrilovich Markovitch 4647 instead mapping text node small group nodes taxonomy maps text entire collection available concepts computing degree aﬃnity concept input text ESA uses Wikipedia articles collection concepts maps texts collection concepts termdocument aﬃnity matrix Similarity measured new concept space Unlike method ESA use link structure structured knowledge Wikipedia Our method walking content similarity graph beneﬁts addition nonlinear distance measure according word cooccurrences ESA semantic representation modiﬁcations studies word similarity crosslingual experiment Wikipedias Hassan Mihalcea 48 evaluated translated versions English data sets Section 10 In study Zesch et al 49 concept vectors akin ESA path length evaluated WordNet Wikipedia Wiktionary showing Wiktionary improved previous methods ESA provided semantic representations higherend application crosslingual question answering 50 Yech et al 51 turn 180 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Table 1 Comparison present proposal line previous work cited section terms resources algorithms NLP tasks data sets The abbreviations data sets rightmost column explained Section 10 page 190 The methods abbreviated follows ESA Ex plicit Semantic Analysis 4647 LSA Latent Semantic Analysis 19 IC Information Content 826 PMIIR pointwise mutual information data collected information retrieval 5330 PPR Personalized PageRank algorithm 3254 Article Jarmasz 11 Jarmasz Szpakowicz 23 Mihalcea et al 30 corpusbased Mihalcea et al 30 knowledgebased Resource Roget Algorithm Shortest path Task Word sim Data set MC RG Synonyms WebBNC PMIIRLSA Paraphrase Microsoft WordNet Shortest path IC Hughes Ramage 31 WordNet PPR Word sim MC RG WS353 Gabrilovich Markovitch 46 Agirre Soroa 52 Zesch et al 49 Wikipedia ESA TFIDF Cosine sim Word sim Doc sim WS353 Lee WordNet WordNet Wikipedia Wiktionary PPR WSD Path length concept vectors Word sim Senseval2 3 MC RG WS353 German Strube Ponzetto 37 Wikipedia Shortest path categories text overlap Word sim coreference resolution MC RG WS353 Milne Witten 39 Wikipedia Similarity hyperlinks Word sim MC RG WS353 Hassan Mihalcea 48 Wikipedia Modiﬁed ESA Crosslingual word sim Translated MC WS353 Syed et al 40 Wikipedia Coursey et al 42 Ramage et al 44 Gabrilovich Markovitch 47 Wikipedia WordNet Wikipedia Cosine sim spreading activation PPR PPR Doc classif 3100 handpicked docs Doc classif 150 WP articles Paraphrase entailment Microsoft RTE ESA TFIDF Cosine sim Doc clustering Reuters 20NG OHSUMED short docs Yeh et al 51 Wikipedia PPR Word sim Doc sim MC WS353 Lee Present proposal Wikipedia Visiting Probability VP Word sim Doc sim clustering IR See Sections 1014 Probably closest antecedent study WikiWalk approach 51 A graph documents hyperlinks constructed Wikipedia Personalized PageRank PPR 32 computed text fragment teleport vector resulting ESA A dictionarybased initialization PPR algorithm studied To compute semantic similarity texts Yeh et al simply compared PPR vectors Their scores word similarity slightly higher obtained ESA 47 scores document similarity Lee data set Section 11 state art initializing random walk words document characterize documents By comparison method consider addition hyperlinks effect word cooccurrence article contents use different random walk initialization methods In particular previously shown 45 visiting probability improves PPR likely captures different properties network Mihalcea Csomai 43 Milne Witten 41 discussed enriching document Wikipedia articles Their methods add explanatory links news stories educational documents generally enrich unstructured text fragment bagofwords structured knowledge Wikipedia Both perform disambiguation ngrams requires timeconsuming computation relatedness senses context articles The ﬁrst method detects linkable phrases associates relevant article probabilistic approach The second learns associations uses results search linkable phrases 33 Distances nodes graphs We turn abstract methods measuring distances vertices graph Many graphbased methods applied NLP problems instance proceedings TextGraphs workshops recently surveyed Navigli Lapata 55 application word sense disambiguation A similar attempt Ion Stefanescu 56 Navigli 57 deﬁned method truncating graph WordNet senses built input text Navigli Lapata 55 focused measures connectivity centrality graph built purpose sentences disambiguate close spirit ones analyze large Wikipediabased network Section 43 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 181 Two measures node distance similar goal visiting probability VP proposed paper hitting time standard notion graph theory Personalized PageRank PPR 32 surveyed Berkhin 54 Hitting time vertex si s j number steps random walker takes average visit s j ﬁrst time starts si The difference hitting time visiting probability discussed end Section 62 proposal properly introduced Hitting time studies distance measure graphs dimensionality reduction 58 collaborative ﬁltering recommender 59 Hitting time link prediction social networks graphbased distances 60 semantic query suggestion queryURL bipartite graph 61 A branch bound approximation algorithm proposed compute node neighborhood hitting time large graphs 62 PageRank word sense disambiguation graph derived candidate text Navigli Lapata 55 As PPR measure word sense disambiguation Agirre Soroa 52 graph derived WordNet 120 000 nodes 650 000 edges PPR measuring lexical relatedness words graph built WordNet Hughes Ramage 31 mentioned 4 Wikipedia network concepts 41 Concepts nodes vertices We built concept network Wikipedia Freebase Wikipedia Extraction WEX dataset 63 version dated 20090616 Not Wikipedia articles considered appropriate include network concepts reasons related nature reliability tractability overall method given large number pages English Wikipedia Therefore removed Wikipedia articles belonged following spaces Talk File Image Template Category Portal List articles concepts contain auxiliary media information belong concept network Also disambiguation pages removed point different meanings title variants As noted Yeh et al 51 short articles appropriate candidates include concept network reasons speciﬁc concepts little chances occur texts correspond incomplete articles stubs contain unreliable selection hyperlinks number considerably slows computation network In previous work Yeh et al 51 set size limit 2000 nonstop words entries pruned limit decreased considerably size network As goal minimize risk removing potentially useful concepts respect possible original contents Wikipedia set cutoff limit 100 nonstop words pruning minor articles This value lower value Yeh et al 51 similar Gabrilovich Markovitch 47 Out initial set 4 327 482 articles WEX ﬁltering removed 70 articles based namespaces length cutoff yielding resulting set 1 264 611 concepts Each concept main Wikipedia title main page describing concept However cases words phrases refer concept One type words determined examining Wikipedia redirects articles content point user proper article alternative title The titles redirect pages added secondary titles titles articles redirect In addition hyperlink article extracted corresponding anchor text considered possible secondary title linked article capturing signiﬁcant terminological variation concept names noise variability linking practice Therefore concept types titles summary data structure Table 2 original anchor texts hyperlinks targeting variants provided redirect pages speciﬁc title listed 42 Relations links edges Relations concepts determined ways In previous study 45 considered types links concepts hyperlinks links computed similarity content category template While type links captures form relatedness focus present study ﬁrst types complementary However proposed computational framework general accommodate types links particular optimal combination learned training data The use hyperlinks Wikipedia articles embodies somewhat evident observation hyperlink content article indicates certain relation articles These encyclopedic pragmatic relations concepts world subsume semantic relatedness In words article A contains hyperlink article B B helps understand A B considered related A Such links represent substantial human knowledge embodied Wikipedia structure It noted links essentially asymmetric decided list given page outgoing links incoming ones Indeed observations showed target page link helps understanding source contrary true relation speciﬁc For article XML text WEX 182 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Table 2 Logical structure node network resulting English Wikipedia Concept ID Names concept Integer Name article encyclopedia Alternative names redirecting article Anchor texts incoming hyperlinks Description concept Text Relations concepts outgoing links Hyperlinks description concepts weights Lexical similarity links closest concepts weights cosine similarity parsed extract hyperlinks resulting total 35 214 537 hyperlinks timeconsuming operation required ability handle instances illformed XML input The second type links based similarity lexical content articles Wikipedia computed word occurrence If articles words common topicsimilarity relation holds To capture content similarity computed lexical similarity articles cosine similarity vectors derived articles texts stopword removal stemming Snowball1 We linked article k similar articles weight according normalized lexical similarity score nonzero weights In experiments described k set 10 node outgoing links nodes based lexical similarity2 The value k 10 chosen ensure computational tractability slightly lower average number hyperlinks concept 28 As Wikipedia articles scattered space words tuning k bring crucial changes If k small neighborhood contains little information large k makes computation timeconsuming 43 Properties resulting network The processing English Wikipedia resulted large network concepts having logical structure represented Table 2 The network 12 million nodes vertices average 28 outgoing hyperlinks node 10 outgoing content links node A natural question arising point structure network characterized apart putting work It possible visualize entire network size displaying small instance Coursey et al 42 Fig 1 representative entire network3 A number quantitative parameters proposed graph theory social network analysis instance analyze WordNet enhanced version Navigli Lapata 55 We compute wellknown parameters network add new informative characterization A ﬁrst characteristic graphs degree distribution distribution number direct neighbors node For original Wikipedia hyperlinks representation 64 suggests distribution follows power law A relevant property network clustering coeﬃcient average clustering coeﬃcients node deﬁned size immediate neighborhood node divided maximum number links connect pairs neighbors 65 For hyperlink graph value coeﬃcient 016 content link graph 0264 This shows hyperlink graph clustered content link distribution nodes links homogeneous overall graphs low clustering5 We propose adhoc measure offering better illustration networks topology aimed ﬁnding graph clustered communities nodes based neighborhoods preferred size uniformly distributed We consider sample 1000 nodes For node graph Personalized PageRank algorithm 32 initialized node run resulting proximity coeﬃcient node graph initial node This ﬁrst hyperlinks content links The community size node computed sorting nodes respect proximity counting nodes contribute 99 mass The results shown Fig 1 distribution ﬂat uniformly decreasing peak provides indication average size clusters This size 150400 nodes hyperlink graph sharp maximum showing clustering content links average 714 nodes The value partly related 10link limit set content links entirely limit concerns 1 From Apache Lucene indexing available httpluceneapacheorg 2 Therefore outdegree nodes 10 indegree vary number incoming links graph strictly speaking 10regular graph 3 An example neighborhood according relatedness measure shown Section 63 4 For content links coeﬃcient computed regardless weights A recent proposal computing weighted graphs applied 66 5 The observed values power law degree distribution suggest graph scalefree network characterized presence hub nodes smallworld network 65 However clear impact properties deﬁned mainly social networks M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 183 Fig 1 Distribution community sizes sample 1000 nodes text deﬁnition community For community size xaxis graphs number nodes yaxis having community size graphs built Wikipedia Both graphs tendency clustering nonuniform distribution links average cluster size 150400 hyperlinks 714 content links outgoing links The use hyperlinks avoids local clusters extends considerably connectivity network comparison content similarity ones 5 Mapping text fragments concepts network In order use concept network similarity judgments text fragments operations nec essary explained end Section 22 The ﬁrst mapping text fragment set vertices network explained section Sections 6 7 deﬁne distance weighted sets vertices For mapping cases considered according text matches exactly title Wikipedia page Exact matching likely occur individual words short phrases entire sentences longer texts If text fragment consists single word phrase matches exactly title Wikipedia page simply mapped concept In case words phrases refer concepts Wikipedia simply assign page assigned Wikipedia contributors salient preferred sense denotation For instance mouse directs page animal contains indication mouse_computing page describes pointing device senses listed mouse_disambiguation page So simply map mouse animal concept However words sense denotation preferred Wikipedia contributors word plate In cases disambiguation page associated word phrase We chose include pages network correspond individual concepts So order select referent page words simply use lexical similarity approach described paragraph When fragment word phrase sentence text match exactly Wikipedia title vertex network mapped network computing lexical similarity text content vertices network cosine distance stemmed words stopwords removed Concept names principal secondary ones given twice weight words description concept The text fragment mapped k similar articles according similarity score resulting set k weighted concepts The weights normalized summing text representation network probability distribution k concepts Finding k closest articles according lexical similarity eﬃciently Apache Lucene search engine footnote 1 For example consider following text fragment mapped network Facebook privacy security problems When fragment mapped k 10 similar Wikipedia articles result ing probability distribution following Facebook 0180 Facebook history 0116 Criticism Facebook 0116 Facebook features 0116 Privacy 0084 Privacy International 0080 Internet privacy 0080 Privacy policy 0054 Privacy Lost 0054 Facebook Beacon 0119 This mapping algorithm important role performance ﬁnal combination network distance described It noted effects wrong mappings stage countered later For instance large sets concepts related text fragments compared individual mistakes likely alter overall relatedness scores Alternatively comparing individual words wrong mappings 184 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 likely occur test sets word similarity described Section 10 page 190 consider implicitly salient sense word described Wikipedia 6 Semantic relatedness visiting probability In section framework computing relatedness texts represented sets concepts network Although applied Wikipedia model general applied networks The goal estimate distance concepts sets concepts captures relatedness taking account global connectivity network biased local properties Indeed use individual links paths estimating conceptual relatedness length shortest path account relative importance respect overall properties network number length possible paths nodes Moreover length shortest path sensitive spurious links frequent Wikipedia Therefore number aggregated proximity measures proposed including PageRank hitting time reviewed Section 3 Our proposal uses random walk approach deﬁnes relatedness concept sets visiting probability long run random walker going set nodes The walker use type directed links concepts described Section 42 hyperlinks lexical similarity ones This algorithm independent mapping algorithm introduced previous section 61 Notations Let S si 1 cid2 cid2 n set n concepts vertices network Any concepts si s j connected directed weighted links L different types L 2 case hyperlinks lexical similarity links The structure links type l 1 cid2 l cid2 L fully described matrix Al size n n Ali j weight link type l si s j possible weights 0 1 hyperlink matrix actual lexical similarity scores 0 content similarity matrix The transition matrix Cl gives probability direct step transition concepts si s j links type l This matrix built Al matrix follows Cli j Ali j cid2 n k1 Ali k In random walk process link types 1 cid2 l cid2 L let weight wl denote importance link type l Then overall transition matrix C gives transition probability Ci j concepts si s j C L l1 wlCl Finally let cid4r ndimensional vector resulting mapping text fragment concepts network Sec tion 5 indicates probabilities concepts network given text relevance concepts texts words The sum elements 1 cid2 62 Deﬁnition visiting probability Given probability distribution cid4r concepts weighted set concepts concept s j network ﬁrst compute probability visiting s j ﬁrst time random walker starts cid4r network concepts cid4r nonzero probability6 To compute visiting probability following procedure provides model state St random walker concept random walker positioned Executing procedure termination gives visiting probability VP Step 0 Step t Choose initial state walker probability P S 0 si cid4r ri In words position random walker concepts cid4r probability stated cid4r Assuming St1 determined St1 s j return success ﬁnish procedure Otherwise proba bility α choose value concept St according transition matrix C probability 1 α return fail The possibility failing absorption probability introduces penalty long paths makes probable cid5 We introduce C equal transition matrix C row j C cid5 j k 0 k This indicates fact random walker visits s j ﬁrst time exit probability mass drops zero step This modiﬁed transition matrix deﬁned account deﬁnition visiting probability probability ﬁrst visit s j similar idea proposed Sarkar Moore 62 6 A simpler derivation ﬁrst given visiting probability s j starting concept si avoid duplication equations provide directly formula weighted set concepts M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 185 To compute probability success process introduce probability success step t ptsuccess cid5 t power t Then probability success process probability visiting s j starting cid4r sum cid5 t j jth element vector cid4rC cid5 t j In formula cid4rC ptsuccess αtcid4rC matrix C probabilities success different lengths cid5 t C cid5 psuccess cid3 t0 ptsuccess cid4 cid4rC cid5 cid5 t αt j cid3 t0 The visiting probability s j starting distribution cid4r computed different probability signed s j running Personalized PageRank PPR teleport vector equal cid4r In computation VP loops starting s j ending s j effect ﬁnal score unlike computation PPR loops boost probability s j If pages type loops typically popular pages PPR high probability close teleport vector cid4r cid2 The visiting probability s j different hitting time s j deﬁned average number steps random walker visit s j ﬁrst time graph If use notations hitting time cid4r cid5 t j Hitting time sensitive long paths comparison VP t comparison αt s j Hcid4r s j formula introduce noise VP reduces effect long paths sooner walk We compared performance algorithms previous paper 45 concluded VP outperformed PPR hitting time t0 tcid4rC 63 Visiting probability weighted sets concepts texts To compute VP weighted set concepts set distribution cid4r1 distribution cid4r2 construct virtual node representing cid4r2 network noted sR cid4r2 We connect concepts si sR cid4r2 according weights cid4r2 We create transition matrix C adding new row numbered nR transition matrix C elements zero indicate sR cid4r2 adding new column weights sR cid4r2 updating rows C follows Ci j element C cid5 cid4 cid5 C C C cid5 j cid5 inR cid5 nR j 1 cid4r2i Ci j cid4r2i 0 j j cid7 nR These modiﬁcations graph local run time possibility undo subsequent text fragments compared To compute relatedness texts average visiting probability cid4r1 given cid4r2 noted VPcid4r2 cid4r1 visiting probability cid4r2 given cid4r1 noted VPcid4r1 cid4r2 A larger value measure indicates closer relatedness It worth mentioning 2 VPcid4r1 cid4r2 VPcid4r2 cid4r1 metric distance satisﬁes properties nonnegativity identity indiscernible symmetry triangle inequality shown deﬁnition VP 64 Illustration visiting probability vs text To illustrate difference VP text fragment consider following example Though text fragment took deﬁnition jaguar animal corresponding Wikipedia article Although word jaguar polysemous topic particular text fragment ambiguous human reader The text ﬁrst mapped Wikipedia described Then VP equal weights hyperlinks content links closest concepts Wikipedia pages terms VP text respectively following ones Original text The jaguar big cat feline Panthera genus Panthera species Americas The jaguar thirdlargest feline tiger lion largest Western Hemisphere Ten closest concepts according VP text John Varty European Jaguar Congolese Spotted Lion Lionhead rabbit Panthera leo fossilis Tigon Panthera hybrid Parc des Félins Marozi Craig Busch Ten closest concepts according VP text Felidae Kodkod North Africa Jaguar Panthera Algeria Tiger Lion Panthera hybrid Djémila The closest concepts according VP text tend general closest concepts according VP text For instance second list includes genus family jaguar species Panthera Felidae ﬁrst includes hybrid extinct species related jaguar Concepts close text bring detailed information related topics text concepts close text popular general Wikipedia articles Note closest concepts related Jaguar car brand examining page 186 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 including lesserknown ones persons cited wildlife ﬁlm makers park founders However concepts related obvious way Algeria likely retrieved Western Hemisphere The behavior illustrated example expected given deﬁnition VP A concept close text paths network text respect concepts network means concept speciﬁc relation topics text example Conversely concept close text terms VP text typically related paths text concept comparison concepts This generally means likely article general popular article topics text If consider hierarchy concepts general speciﬁc ones concepts close text generally lower hierarchy respect text concepts close text generally higher 7 Approximations T truncated εtruncated visiting probability The deﬁnition random walk procedure direct consequence computation iteratively truncated T steps needed especially maintain computation time acceptable limits Truncation makes sense higher order terms longer paths smaller larger values t αt factor Moreover making computation tractable truncation reduces effect longer reliable paths computed value psuccess Indeed VP popular vertices links point high close starting distribution cid4r high number long paths truncation conveniently reduces importance vertices We propose section methods truncating VP justify empirically level approximation chosen tasks T truncated visiting probability The simplest method called T truncated VP truncates computation paths longer T To compute upper bound error truncation start considering possible state random walk time T success failure particular node The sum probabilities states equals 1 value probability returning success failure ﬁrst t steps cid5 ti fact probability mass time t nodes s j targeted node If computed p T success denotes probability success considering paths length T εT error truncating step T replacing p T success value computed noting psuccess p T failure cid2 1 obtain following upper bound εT icid7 j αtcid4rC cid2 n εT psuccess p T success cid2 cid4 cid4rC cid5 cid5 T α T ncid3 icid7 j So p T success approximation psuccess upper bound approximation error εT cid5 T decreasing right term inequality This term decreases time α T time εT decreases T increases cid2 n icid7 jrC εtruncated visiting probability A second approach referred εtruncated VP truncates paths lower probabilities earlier steps lets paths higher probabilities continue steps Given probability si time cid5 t Setting cid5 t neglected set zero error caused approximation αtrC step t αtrC term zero means exactly paths si time step t longer followed So εtruncation cid5 t cid2 ε This approach faster compute paths probability ε followed αtrC previous upper bound error established We use εtruncated VP experiments leading competitive results acceptable computation time T truncated visiting probability vertices vertex Given dataset N documents tasks document clustering Section 12 require compute average VP N documents Similarly information retrieval Section 14 necessary sort documents repository according relatedness given query It tractable compute exact VP values concepts provide way compute T truncated VP fromto distribution cid4r concepts network faster repeating computation concept As Section 63 consider virtual concept sR cid4r representing cid4r network noted simply sR It possible compute VP concepts sR time following recursive procedure compute T truncated visiting probability VPT This procedure follows recursive deﬁnition VP given Section 62 states VPsi sR α cid5si skVPsk sR Therefore cid2 k C VPT si sR α cid5 si skVPT 1sk sR C cid7 nR cid3 k VPT si sR 1 nR VP0si sR 0 Using dynamic programming possible compute T truncated VP concepts sR O E T steps E number edges network M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 187 Fig 2 VPsumT average VP content links hyperlinks b Wikipedia graph depending T α varying 04 09 01 The shape curves indicates values T leading acceptable approximation exact nontruncated value α 08 T 10 chosen subsequent experiments T truncated visiting probability vertex vertices Conversely compute VPT cid4r concepts network total computation time O N E T N number concepts E number edges VPT computed concept With current network time consuming propose following sampling method approximate T truncated VP The sampling involves running M independent T length random walks cid4r To approximate VPT concept s j cid4r s j visited ﬁrst time tk1 tkm time steps cid2 cid6 VPT cid4r s j l αtkl M M samples T truncated VP s j approximated following average According proposed method possible approximate truncated VP cid4r concepts network O M T time M number samples It remains ﬁnd samples obtain desired approximation level question answered following theorem For vertex estimated truncated VP approximates exact truncated VPT vertex ε probability larger 1 δ number samples M larger The proof result given Appendix A α2 ln2nδ 2ε2 8 Empirical analyses VP approximations We analyze convergence approximation methods proposed variation margin error T ε In case T truncated approximation T increases inﬁnity T truncated approximated VP converges exact value computation time increases linearly T In case εtruncation ε tends zero approximated value converges real computation time increases Therefore need ﬁnd proper values T ε compromise estimated error computing time 81 Convergence T truncated VP Wikipedia The value T required certain level convergence upper bound approximation error depends transition matrix graph It possible ﬁnd convenient theoretical upper bound analyzed relation T approximation error empirically sampling method We chose randomly set S 1000 nodes graph computed T truncated VP nodes graph nodes S Then computed average values T truncated VP nodes nodes S VPsumT icid7 j VPT jS Given S large random sample consider evolution VPsumT T representative evolution average VPT T Fig 2a shows values VPsumT depending T Wikipedia graph lexical similarity content links values α Fig 2b shows curves Wikipedia graph hyperlinks Both analyses expected larger values α correspond slower convergence terms T larger α requires random walker explore longer paths level approximation The exact value VPsumT converges important The ﬁgures indication given α extent paths explored acceptable approximation nontruncated VP value In experiments chose α 08 T 10 compromise computation time accuracy approximation error 10 Figs 2a 2b cid2 cid2 jS 82 Convergence εtruncated VP Wikipedia To analyze error induced εtruncation computing VP Wikipedia graph proceeded similar way We chose 5000 random pairs nodes computed sum εtruncated VP pair Fig 3a shows 188 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Fig 3 Sum εtruncated VP depending 1ε Wikipedia graph content links hyperlinks b α 06 09 5 α 08 chosen The shape curves indicates values ε leading acceptable approximation nontruncated value ε 10 subsequent experiments Fig 4 Average proportion identical nodes K closest neighbors cosine similarity vs average T truncated VP varying sizes K neighborhood values α little inﬂuence result The proportion decreases larger neighborhoods metrics different results smaller relatedness values values sum depending 1ε Wikipedia graph content links Fig 3b Wikipedia graph hyperlinks Again appears curves larger α smaller values ε needed reach level approximation error larger α longer paths explored reach approximation level The 5 value α 08 εtruncation word similarity document similarity value ε 10 83 Differences random walk model content links direct lexical similarity articles Performing random walk Wikipedia graph leads relatedness measure different direct lexical similarity cosine similarity space words empirically following experiment We randomly chose 1000 nodes sorted nodes Wikipedia articles based cosine similarity randomly selected nodes measured TFIDF vectors In parallel randomly selected nodes nodes sorted based average T truncated VP T 10 The comparison resulting sorted lists node shows difference measures To perform comparison node look intersection heads sorted lists neighborhoods node varying size subsets Fig 4 shows percentage nodes common depending neighborhood sizes different values α changes little results It appears closest neighbors particular closest lexical similarity VP content links return nodes However expanding size neighborhood size intersection decreases rapidly For example looking closest nodes M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 189 50 nodes average lists This empirical argument showing walking content links leads different relatedness measure simply lexical similarity articles 9 Learning embeddings visiting probabilities In approach computing semantic relatedness text mapped ﬁxed number concepts distance computed VP runtime To computation tractable introduced number approxi mations Section 7 In section forward principled approach learns VP values similarity measure text vectors applies lower cost run time This approach require ﬁxed number nodes project document Moreover integrated prior knowledge learning algorithms NLP applied large scale problems At training time given series samples pairs texts VP values ﬁrst text second goal learn transformation space words latent space similarity latent representation texts close possible VP similarity In words goal approximate VP cid5 texts j matrix product xi A B j xi x j TFIDF vectors texts constructed x words ﬁxed dictionary The size matrices A B n m n size dictionary number words m size latent space akin number topics topic models Two different matrices A B needed VP values symmetric j cid5 In principle pairs Wikipedia articles texts corresponding nodes network training set extremely large ca 14 1012 Therefore formulate following constraints training 1 training focus neighboring articles articles high VP values 2 exact values VP replaced ranking pairs articles decreasing VP We constraints valuable embeddings learned Let VPtoki set k closest articles article according VP similarity We deﬁne hinge loss function L follows similarity k closest articles larger similarity articles ﬁxed margin M cid3 cid3 cid3 cid4 L iWP jVPtoki z VPtoki max 0 M xi A B cid5 cid5 x j xi A B cid5 cid5 x z cid5 We optimize L stochastic gradient descent iteration randomly choose article randomly choose k closest articles noted j article rest documents noted z In experiments set k 10 M 02 We computed VPtoki values approximations Section 7 We built dictionary Snowball tokenizer Lucene removing highest lowest frequency words keeping 60 000 words We set number topics m 50 larger m offers higher learning capability increases linearly number parameters learn Given size training set chose small number m training possible reasonable time satisfactory prediction error Moreover perform regularization matrices A B optimizing L impose constraint A B orthonormal In order apply constraint project 1000 iterations A B nearest orthogonal matrix SVD decomposition The rationale constraint following assume latent dimension corresponds possible topic theme orthogonal possible Figs 5a 5b average training error 1000 iterations regularization respectively hyperlink graph content link The training error computed number text triples j z cid5 cid5 x test hinge loss function false xi A B j To test predictive power embeddings replacement computation VP runtime excluded 50 000 articles training set built 50 000 triples articles choosing randomly excluded articles article k closest ones randomly rest articles The test set error number triples set respect order given VP similarities shown Table 3 cid5 z M x xi A B cid5 The main ﬁndings following First VP hyperlinks graph harder learn fact hyperlinks deﬁned users manner totally predictable Second regularization decreases prediction ability However regularization traded prediction power generality words reduced overﬁtting problem distance general constitute useful operation This checked experiments Sections 12 13 15 Moreover experiments embeddings plugged stateoftheart learning algorithms prior knowledge improving performance 10 Word similarity In following sections assess effectiveness visiting probability applying language processing tasks In particular task examine type link separately compare results obtained combinations links attempt single optimal combinations The word similarity task heavily researched variety methods resources WordNet Rogets Thesaurus English Wikipedia Wiktionary starting instance Resniks seminal paper 8 190 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Fig 5 Average training error learning embeddings measured 1000 iterations regularization Table 3 Accuracy embeddings learned different training sets The er ror percentage computed 50 000 triples articles separate test set number triples respect ordering VP similarities Training set embedding Hyperlinks Hyperlinks Regularization Content Links Content Links Regularization Error 98 135 54 59 earlier We reviewed Section 3 recent work word similarity focusing mainly graphbased methods Wikipedia WordNet Recent studies including references previous scores baselines Bollegala et al 67 Gabrilovich Markovitch 46 47 Tables 2 3 Zesch et al 49 Agirre et al 68 Ramage et al 44 Three test sets English word similarity task extensively past They consist pairs words accompanied average similarity scores assigned human subjects pair Depending instructions given subjects notion similarity interpreted relatedness discussed instance Hughes Ramage 31 Section 5 The sets reproduced Jarmaszs thesis 11 instance designed respectively Rubenstein Goodenough 69 henceforth RG 65 pairs 51 judges Miller Charles 70 MC 30 pairs Finkelstein et al 71 WordSimilarity353 353 pairs We estimate relatedness words mapping concepts computing εtruncated VP distance 5 We set value α 08 explained Section 82 The correlation human ε 10 judgments relatedness measured Spearman rank correlation coeﬃcient ρ Pearson correlation coeﬃcient r VP values pair human judgments The values Spearman rank correlation coeﬃcient ρ WordSimilarity353 data set varying relative weights content links vs hyperlinks random walk shown Fig 6 The best scores reach ρ 070 combination hyperlinks content links weighted 0307 0208 As shown ﬁgure results improve combining links comparison single type Results improve rapidly hyperlinks added content ones right end curve small weight shows adding encyclopedic knowledge represented hyperlinks word cooccurrence information improve signiﬁcantly performance Conversely adding content links hyperlinks improves results likely effect spurious hyperlinks reduced adding content links To ﬁnd types links encode similar relations examined extent results VP hyperlinks correlated results content links The Spearman rank correlation coeﬃcient scores ρ 071 shows independence scores We tested method RG MC data sets The values Pearson correlation coeﬃcient r previous algorithms lexical resources given Jarmasz 11 Section 432 Gabrilovich Markovitch 47 Table 3 Agirre et al 68 Table 7 showing word similarity lexical resources successful reaching correlation 070085 human judgments For algorithm correlation r human judgments RG MC respectively 038042 046050 depending combination links Lexicallybased M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 191 Fig 6 Spearman rank correlation ρ automatic human judgments word similarity WordSimilarity353 data set depending weight content links random walk weight hyperlinks complement 1 The best scores ρ 070 reached content links weight hyperlinks 0708 vs 0302 The result LSA ρ 056 quoted 46 outperformed scores literature Table 4 A comparison word similarity scores terms Spearman rank correlation ρ Pearson correlation r data sets WordSimilarity 353 71 MC 70 Our scores VP appear line WordSimilarity353 upper range stateoftheart ones WordSimilarity353 Study Finkelstein et al 71 Jarmasz 11 Strube Ponzetto 37 Hughes Ramage 31 Gabrilovich Markovitch 46 Agirre et al 68 ρ 056 055 048 055 075 078 MC data set Study Wu Palmer 72 Resnik 8 Leacock Chodorow 25 Lin 27 Jarmasz 11 Patwardhan Pedersen 73 Bollegala et al 67 Alvarez Lim 74 Hughes Ramage 31 Agirre et al 68 VP 070 VP ρ 078 081 079 082 087 NA 082 NA 090 092 069 r 078 080 082 083 087 091 083 091 NA 093 050 techniques successful data sets lexical similarity relation important method considers linguistic extralinguistic relations eﬃcient However examine Spearman rank correlation ρ RG MC data sets considering ranking pairs words exact values relatedness scores method reaches 067069 Our scores lower best lexicallybased techniques ρ 074081 069086 RG MC difference smaller Our method suitable capturing ranking pairs instead exact scores observation Gabrilovich Markovitch 47 Gabrilovich Markovitch 46 Table 4 47 Table 3 Agirre et al 68 Table 9 provide values Spearman rank correlation coeﬃcient ρ previous methods WordSimilarity353 data set The best results obtained Explicit Semantic Analysis ρ 075 combination distributional WordNetbased methods Agirre et al 68 ρ 078 Apart methods best reported scores data ones reached LSA ρ 056 oly Our method outperforms score margin similar ESA combination 68 method reach scores Some authors attempted reproduce ESA scores Zesch et al 49 reached ρ 046 Ramage et al 44 Hassan Mihalcea 48 reported results close original ones 4647 A key factor ensures high performance cleaning procedure applied concept vectors 47 323 Overall facilitate comparison scores important scores literature Table 4 provides synthetic view To improve understanding types links Fig 7 shows average frequency path lengths traveled computing εtruncated VP word pairs WordSimilarity353 different combinations links The results hyperlinks shortens length average path computation VP Conversely hyperlinks number paths words increasing dramatically comparison content links 192 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Fig 7 Average frequency path lengths contributing VP WordSimilarity353 data set conﬁgurations left right integer value hyperlinks equal weights content links Fig 8 Pearson correlation coeﬃcient r VP results human judgments document similarity depending weight hyperlinks combination weight content links complement 1 The best score r 0676 reached hyperlinks weight content links The result LSA r 060 quoted Lee et al 75 fact explains adding hyperlinks small weight content links improves results rapidly Fig 6 11 Document similarity The estimation document similarity task proposal assessed The document similarity data set experiment gathered Lee et al 75 contains average human similarity scores pairs set 50 documents As experiment word similarity parameters α 08 ε 10 5 tested method combinations weights types links results shown Fig 8 Following Gabrilovich M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 193 Fig 9 Frequencies path lengths contributing VP averaged document similarity data set conﬁgurations left right integer value hyperlinks content links equal weights types Markovitch 46 averaged human judgments cover small range possible values use Pearson correlation coeﬃcient r evaluate close method approaches human judgments For experiments document set mapped 1000 closest concepts network Otherwise random walk parameters word similarity task Our ﬁndings behavior best results similar previous experiment word similarity Adding hyperlinks content links improves correlation sharply adding content links hyperlinks improves results initial decrease best performance r 0676 reached combination links appears similar word similarity experiment The authors data set Lee et al 75 report results methods document similarity task best reaching r 060 LSA However case mentioned Gabrilovich Markovitch 46 LSA trained small document corpus 314 news articles When trained larger corpus performance LSA increases r 069 value reported Hassan Mihalcea 76 training LSA entire Wikipedia corpus ESA 46 reaches score r 072 task outperforms methods case word similarity including small margin Indeed method best observed combination reached r 0676 Therefore method best results literature reaches high scores general semantic relatedness measure As word similarity Fig 9 frequency path lengths traveled computing VP averaged document similarity data set different combination links The ﬁgure shows hyperlinks shortens average path length computation content links lengthens path lengths 12 Document clustering This section describes experimental setting results applying text relatedness measure deﬁned problem document clustering 20 Newsgroups dataset7 The dataset contains 20 000 postings 7 The dataset distributed httpwwwcscmueduafscscmueduprojecttheo20wwwdatanews20html 77 Chapter 6 194 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Table 5 Rand Index RI precision recall Fscore different clustering methods VPComb stands VP combination content links weighted 06 hyperlinks 04 Scores bold signiﬁcantly higher LSA italics signiﬁcantly lower ttest p 0001 Distance metric Precision Cosine similarity TFIDF vectors LSA VP content links VP hyperlinks VPComb 06 content links Embedding content links ECL Embedding hyperlinks EHL ECL regularization EHL regularization 929 1910 2413 1836 2426 2130 2263 2274 2408 Recall 1904 2064 3715 3922 3691 2613 2840 2767 2964 Fscore 1249 1984 2913 2472 2923 2347 2517 2495 2656 RI 8667 9167 9094 8778 9103 9148 9156 9168 9180 20 news groups 20 document classes 1000 documents class We aim ﬁnding classes automatically testing entire data set training set The knowledge comes entirely Wikipedia network techniques described Sections 67 computing distances texts projected network We experimented embeddings trained VP similarities described Section 9 121 Setup experiment 20 Newsgroups We ﬁrst compute similarity matrix entire 20 Newsgroups data set relatedness score documents VPT For tractability ﬁxed T 10 value gives suﬃcient precision studied empirically Section 8 Similarly empirically set absorption probability random walk 1 α 02 Based α T results Section 7 allowed compute error bound truncation So choice α T guided fact smaller α fewer steps T needed achieve approximation precision penalty set longer paths In application instead computing VPT possible pairs separately ﬁlled row matrix time approximations proposed To use embeddings simply transform documents embeddings matrices run clustering algorithm transformed documents projection space We trained matrices A B Wikipedia graph The qualitative behavior results A B similar Here report results transformation A help readability results Clustering performed kmeans algorithm similarity matrices feature representations The similarity metric representation vectors cosine similarity Given randomized initialization algorithm ﬁnal clusterings different run algorithm The quality clustering measured Rand Index RI counts proportion pairs documents similarly grouped different clusters reference vs candidate clusterings RI tends penalize smaller clusters example algorithm clusters classes splits class receive high penalty RI This happens 20 Newsgroups data set given classes similar classes general potentially suitable splitting As consequence RI values vary largely run run making signiﬁcance testing diﬃcult To gain information quality clustering consider precision recall Fscore Table 5 shows average results runs clustering algorithm relatedness measures The random walk model signiﬁcantly outperforms baseline cosine similarity TFIDF vectors document clustering outperforms LSA terms Fscore relatedness measures The RI scores allow conclusions variation allow signiﬁcance judgments The comparison previous work uneasy systematic comparison clustering methods 20 Newsgroups datasets known In recent paper Hu et al 78 Fscore 148 baseline word vector method improved 196 use Wikipedia agglomerative clus tering However partitional clustering Kmeans scores increased twice 382 baseline 418 best method result conﬁrmed independently For approach combination hyperlinks content links improves results Using embeddings content links reduces performance comparison computation VP values graph On contrary embeddings hyperlinks improves results comparison VP values hyperlinks graph The embedding learned VP similarities hyperlinks appears provide general similarity measure overﬁt Hyperlinks graph Wikipedia The high recall obtains related larger extension paths computed hyperlinks connect documents attach class high precision obtained content links tendency cluster smaller neighborhoods Although regularization imposed embeddings reduced predictive power VP similarities improves performance task Computation time embeddings expected greatly reduced computations performed low dimensional latent space Moreover unsupervised clustering algorithms applied documents transformed embeddings stateoftheart clustering algorithm proposed Bordogna Pasi 79 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 195 Fig 10 Values kpurity vertical axis averaged documents neighborhoods different sizes k The horizontal axis indicates weight w visiting probability vs cosine lexical similarity formula w VPComb 1 w LS 122 Comparison VP cosine similarity To ﬁnd cases proposed method improves simple cosine similarity measure considered linear combination cosine similarity VPComb VP content links weighed 06 hyperlinks weighed 04 w VPComb 1 w LS varied weight w 0 1 Considering knearest neighbors document according combined similarity deﬁne kpurity number documents correct label total number documents k computed neighborhood The variation kpurity w sample values k shown Fig 10 The best purity appears obtained combination methods values k tested This shows VPComb brings valuable additional information document relatedness LS Furthermore size examined neighborhood k increases lower curves Fig 10 effect VPComb important weight optimal combination increases For small neighborhoods LS suﬃcient ensure optimal purity larger ones k 10 15 VPComb w 1 outperforms LS w 0 Their optimal combination leads scores higher obtained separately noted weight VPComb optimal combination increases larger neighborhoods These results explained follows For small neighborhoods cosine lexical similarity score nearest 15 documents high words common LS good measure text relatedness However looking larger neighborhoods relatedness based identical words VPComb comes effective LS performs poorly Therefore predict VPComb relevant looking larger neighborhoods order increase recall VPComb relevant low diversity document words instance documents short 13 Text classiﬁcation We showed previous section VP similarities Wikipedia hyperlinks content links graphs improved text clustering Although text clustering important application studies text clas siﬁcation labeled examples available learning In section investigate problem embeddings learned VP similarities Section 9 Embeddings easily integrated dis tance learning algorithm prior knowledge initial state We designed distance learning algorithm purpose compared embeddings SVM text classiﬁer outperforming score training examples available 131 Distance learning classiﬁer We built distance learning classiﬁer learns given training set similarity measure data point training set similarity data points label class higher data points different labels This classiﬁer essentially similar large margin nearest neighbor classiﬁer 80 changes applicable large scale text classiﬁcation problems large number features words 196 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Table 6 Classiﬁcation accuracy different sizes training set 20 Newsgroups data set The accuracy average 10 times run randomly divided data set CL embedding learned content links graph HL embedding learned hyperlinks REG stands regularization matrix The numbers italics signiﬁcantly better accuracy SVM ttest p 0001 DL distance learning classiﬁer Method Size training set DL CL embedding DL CL emb REG DL HL embedding DL HL emb REG SVM 40 2113 2254 2140 2250 790 100 3293 3414 3431 3529 1593 200 4279 4432 4409 4617 2848 500 5657 5712 5718 5864 5240 800 6254 6382 6309 6408 6176 1000 6590 6643 6631 6684 6567 1500 7057 7110 7065 7114 7083 We deﬁne similarity documents j represented TFIDF vectors xi x j xi A A A matrix n m n size feature dictionary m size latent space If Ci denotes class label document deﬁne following loss function L training set cid5 cid5 x j cid3 cid3 cid3 L cid4 max 0 M xi A A cid5 cid5 x j xi A A cid5 cid5 x z cid5 C jCi Czcid7Ci M margin set 02 experiments We performed optimization L stochastic gradient descent At testing time proceeded similarly knearest neighbors algorithm chose k closest documents training set according learned distance returned class label resulting majority vote Our goal starting prior knowledge obtained VP similarities Wikipedia graphs forms embeddings improve performance classiﬁcation especially small number training samples available 132 20 Newsgroups classiﬁcation We applied method classify texts 20 Newsgroups data set We compared results distance learning algorithm initial points linear SVM method LIBSVM implementation 81 stateof theart text classiﬁer The classiﬁcation accuracy given Table 6 sizes training set We trained matrices VP similarities A B VP similarity symmetric We experimented initialization A B gives similar results results matrix A The distance learning classiﬁer random initialization parameters performed poorly reported The ﬁrst important observation training set small distance learning classiﬁer initialized embeddings VP similarities Wikipedia graphs outperforms baseline SVM classiﬁer signiﬁcantly By adding labeled data importance prior knowledge appears decrease presumably distance learning algorithm infer reliable decisions based training data A similar effect shown method based deep learning Ranzato Szummer 82 method TFIDFSVM method reaching 65 accuracy 50 samples class corresponding 1000 total samples training data The second observation orthonormality regularization imposed embeddings improved performance The generalization ability improved price decreasing slightly precision approximation VP values A observation accuracy hyperlinks slightly higher content links 14 Information retrieval In section apply proposed distance information retrieval data TREC7 TREC8 83 The appli cation method large scale information retrieval task requires computation VP query representation documents repository By approximations proposed Section 7 compute T truncated VP query documents acceptable time Firstly map documents data set Wikipedia graph query time query mapped graph ﬁnally query VPT computed documents data set proposed approximations In section use α 08 T 10 combination hyperlinks content links VP weighted 0208 following observations previous sections The timeconsuming operation ﬁrst viz mapping large collection documents Wikipedia graph query time We TREC7 TREC8 Adhoc Test Collections8 The data set includes repository 530 000 documents sets 50 queries For query data set provides relevance judgments subset documents considered related query retrieved pool search methods method intended maximize 8 These available httptrecnistgov speciﬁcally test queries topics numbers 351400 401450 associated relevance judgments The documents available Linguistic Data Consortium M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 197 recall To study effect VP comparison lexical similarity TFIDF scoring single retrieval operation compute document query linear combination VPT weight w lexical similarity weight 1 w The weight w sets relative importance conceptual relatedness vs lexical similarity serve illustrate respective contributions We refer Combined similarity We measure IR performance average precision ﬁrst 10 returned documents 10 typical size ﬁrst result page Web search engines We computed scores w 0 w 1 increments 01 highest score reached w 01 TREC7 data set This optimized value tested TREC8 query set 50 topics leading average precision Combined 0515 improves 154 precision lexical similarity w 0 0446 Egozi et al 84 reported precision 10 TREC8 data bagofwords information retrieval systems Xapian Okapi LMKLDIR improved results 14 obtained new retrieval algorithm integrates ESA 47 semantic similarity previous systems9 Here reported mean average precision 10 lower bound precision 10 informative Direct comparison scores scores reported 84 possible different base retrieval systems We examined precision score query separately In particular counted proportion queries Combined similarity returned relevant documents lexical similarity Combined similarity outperformed lexical 14 queries reverse true 5 queries scores identical remaining 31 queries The average score difference Combined lexical similarity 0018 maximum 1 This value signiﬁcantly 0 usual levels p 001 normal distribution probability true difference zero given observations p 011 While ensure Combined similarity signiﬁcantly better lexical provide encouraging evidence utility VP TREC8 test set The precision scores methods vary considerably queries We examined separately diﬃcult queries deﬁned queries lexical similarity score 03 meaning returned 0 3 relevant documents There 21 queries 50 TREC8 test set Over queries Combined similarity outperforms lexical 7 reverse true 13 queries tie Of course unsurprising difference queries diﬃcult lexical similarity deﬁnition How examining 20 queries diﬃcult Combined similarity measure outperforms lexical 5 reverse true 13 tie These results VP provides complemen tary information improve results lexical similarity IR especially queries lexical similarity performs 15 Learning rank VP similarities We seen previous section linear combination VP lexical similarities improved slightly retrieval results VP provides additional information This motivated integrate VP similarity learning rank 85 instead linear combination lexical similarity We chosen approach similar discriminative projection learning algorithm introduced Yih et al 86 exhibits good performance We reimplemented algorithm experiments section ﬁrst brieﬂy discuss results Assume query q document dr relevant dnr relevant The algorithm learns projection A TFIDF vectors articles latent space similarity projections q dr higher similarity q dr Given training set consisting qi dri dnri algorithm minimizes following loss function L training set cid4 max L cid3 0 M qi A A qi A A d d cid5 cid5 cid5 cid5 ri cid5 nri We embeddings learned VP starting point minimizing L help improve ranking performance To build training test sets 50 queries TREC8 considered query documents labeled relevant 4728 documents unlabeled documents considered irrelevant We divided evenly randomly pairs queries relevant documents training test set The possible number triples training set large large number irrelevant documents We perform stochastic gradient descent training set choosing iteration query relevant document irrelevant document chosen randomly rest documents We stop training error lower ﬁxed threshold To test performance report average precision 10 This computed number relevant documents training Therefore larger training set fewer documents left testing precision 10 scores necessarily decrease comparing scores different methods training set 9 Precision 10 improved follows Xapian 0472 0478 13 Okapi 0488 0522 70 LMKLDIR 0442 0506 144 198 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 Table 7 Precision 10 different ranking algorithms training size varies As Table 6 RL ranking learner CL embedding learned content links graph HL embedding learned hyperlinks REG stands regularization matrix Method Size training set RL CL embeddings RL CL emb REG RL HL embeddings RL HL emb REG RL random start Cosine TFIDF vectors 500 1362 1748 1574 1944 163 2116 1000 1440 1784 1598 1860 1516 1474 1500 1262 1402 1310 1434 1356 962 2000 1124 1208 1128 1246 1248 798 3000 554 556 536 570 534 270 Table 8 Spearman rank correlation coeﬃcient ρ automatic human word similarity twostage random walk VP In parentheses respective weights hyperlinks content links The best result ρ 0714 hyperlinks content links ﬁrst stages weights 07 vs 03 content links stages Hyperlink weight Content link weight 10 00 3 steps 00 10 00 10 3 steps 10 00 07 03 3 steps 00 10 ρ 0684 0652 0714 Table 7 shows precision 10 algorithms different initial states different number training samples The ﬁrst observation training size small 10 documents average query cosine similarity outperforms learning rank algorithms Otherwise performance learning rank algorithms better cosine similarity The second result number training examples small learning algorithm initialized regularized embeddings outperforms random initialization Gradually adding training examples useful leverage prior knowledge learning algorithm solve problem better simply looking training samples Similarly classiﬁcation experiments embeddings learned hyperlinks useful ones learned content links 16 Perspectives In addition experiments examined twostage random walks ﬁrst stage network built set weights links second stage different set The hypothesis links useful explored ﬁrst useful explored later discussed CollinsThompson Callan 87 For word similarity task focused εtruncated twostage walks combination links ﬁrst steps random walker different combination fourth steps The choice steps empirical looking average length singlestage εtruncated walks We report ρ scores signiﬁcant combinations weights scenario Table 8 including best reached ρ 0714 higher onestage walks Section 10 As optimization took place test data competitive score intends twostage walks promising approach particular exploring hyperlinks ﬁrst content links A similar analysis shown Fig 7 explains scores improve twostage random walk Table 8 travels hyperlinks ﬁrst expanding possible paths content links following precise neighborhoods In case exploring hyperlinks ﬁrst content links second longer paths comparison hyperlinks In case exploring hyperlinks second stage long paths comparison scenarios The results VP following twostage random walks meaningful combinations links given Table 9 document similarity data set The scores document similarity slightly improved r 0680 hyperlinks explored ﬁrst steps content links followed congruent ﬁnding stage random walk word similarities 17 Conclusion We proposed general framework text semantic similarity based knowledge extracted Wikipedia We constructed graph including Wikipedia articles different link structures Our hypothesis M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 199 Table 9 Pearson correlation coeﬃcient r twostage VP results human judgments document similarity The best result 0680 ﬁrst stages hyperlinks 08 content links 02 content links stages Hyperlink weight Content link weight 10 00 3 steps 00 10 00 10 3 steps 10 00 08 02 3 steps 00 10 r 0667 0635 0680 word cooccurrence information userdeﬁned hyperlinks articles improve result ing textual distance application variety tasks word similarity document similarity clustering classiﬁcation information retrieval learning rank We tested approach different benchmark tasks results competitive compared stateoftheart results obtained taskspeciﬁc methods Our experimental results supported hypothesis types links useful improvement performance higher separately We introduced visiting probability VP measure proximity weighted sets concepts proposed approximation algorithms compute eﬃciently large graphs One advantage approach training phase building network concepts computationally demanding computation query time Therefore update data set related network imply additional cost recomputation At run time shown computation possible knearestneighbors graph random walk framework large resource English Wikipedia The truncation VP speed computation improved accuracy results reducing importance popular vertices To speed computation run time showed possible train embeddings learn proposed similarity measures In addition gain speed able integrate proposed distance prior knowledge form embeddings learning algorithms document clustering document classiﬁcation learning rank Acknowledgements This work supported Swiss National Science Foundation National Center Competence Research NCCR Interactive Multimodal Information Management IM2 httpwwwim2ch We indebted AI Journal reviewers special issue editors numerous insightful suggestions previous versions paper We like thank Michael D Lee access textual similarity dataset Ronan Collobert JeanCédric Chappelier helpful discussions issues related paper Appendix A Estimate truncated VP vertex In Section 7 announced formal probabilistic bound differences estimated truncated VP cid6 VPT exact truncated VPT vertex The complete proof result inspired proof 88 noted given Let note estimation variable X ˆX suppose concept s j visited ﬁrst time time steps M samples We deﬁne random variable Xl αtkl M tkl indicates time step tk1 tkM s j visited ﬁrst time lth sampling If s j visited Xl 0 convention The l random l αtkl M variables Xl k1 cid2 l cid2 kM independent bounded 0 1 0 cid2 Xl cid2 1 We E cid6 VPT cid4r s j VPT cid4r s j So applying Hoeffdings inequality cid6 VPT cid4r s j l Xl cid2 cid2 cid4cid7 cid7cid6 VPT cid4r s j E cid4cid6 VPT cid4r s j cid5cid7 cid7 cid3 ε cid5 P cid2 2 exp cid8 2Mε2 α2 cid9 If probability error δ setting right lower δ gives bound M stated theorem As consequence following lower bound M look εapproximation possible s j probability 1 δ We use union bound Hoeffdings inequality prove cid4 P j 1 n cid7 cid7cid6 VPT r s j E cid4cid6 VPT r s j cid5cid7 cid7 cid3 ε cid5 cid2 2n exp cid9 cid8 2Mε2 α2 gives desired lower bound M cid3 α2 ln2nδ 2ε2 200 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 References 1 A Budanitsky G Hirst Evaluating WordNetbased measures semantic distance Computational Linguistics 32 2006 1347 2 S Patwardhan S Banerjee T Pedersen Using measures semantic relatedness word sense disambiguation Proceedings CICLing 2003 4th International Conference Computational Linguistics Intelligent Text Processing LNCS vol 2588 Mexico City Mexico pp 241257 3 US Kohomban WS Lee Learning semantic classes word sense disambiguation Proceedings ACL 2005 43rd Annual Meeting Associa tion Computational Linguistics Ann Arbor MI pp 3441 4 SP Ponzetto M Strube Knowledge derived Wikipedia computing semantic relatedness Journal Artiﬁcial Intelligence Research 30 2007 181212 5 M Stevenson M Greenwood A semantic approach IE pattern induction Proceedings ACL 2005 43rd Annual Meeting Association Computational Linguistics Ann Arbor MI pp 379386 6 M Baziz M Boughanem N AussenacGilles C Chrisment Semantic cores representing documents IR Proceedings SAC 2005 ACM Symposium Applied Computing Santa Fe NM pp 10111017 7 D Newman JH Lau K Grieser T Baldwin Automatic evaluation topic coherence Proceedings HLTNAACL 2010 Annual Conference North American Chapter Association Computational Linguistics Los Angeles CA pp 100108 8 P Resnik Using information content evaluate semantic similarity taxonomy Proceedings IJCAI 1995 14th International Joint Conference Artiﬁcial Intelligence Montreal pp 448453 9 C Fellbaum Ed WordNet An Electronic Lexical Database The MIT Press Cambridge MA 1998 10 DB Lenat CYC A largescale investment knowledge infrastructure Communications ACM 38 1995 3338 11 M Jarmasz Rogets Thesaurus lexical resource natural language processing Masters thesis University Ottawa 2003 12 J Morris G Hirst Nonclassical lexical semantic relations Proceedings HLTNAACL 2006 Workshop Computational Lexical Semantics Boston MA pp 4651 13 M Halliday R Hasan Cohesion English Longman London 1976 14 J Hobbs Why discourse coherent F Neubauer Ed Coherence Natural Language Texts Buske Hamburg 1983 pp 2970 15 R Hasan Coherence cohesive harmony J Flood Ed Understanding Reading Comprehension Cognition Language Structure Prose International Reading Association Newark DE 1984 pp 181219 16 M Halliday R Hasan Language Context Text Aspects Language SocialSemiotic Perspective 2nd edition Oxford University Press London 1989 17 S Mohammad G Hirst Distributional measures proxies semantic distance A survey available online consulted April 15 2012 httpwwwcstorontoedupubghMohammadHirst2005pdf 2005 18 JE Weeds Measures applications lexical distributional similarity PhD thesis University Sussex 2003 19 S Deerwester ST Dumais GW Furnas TK Landauer R Harshman Indexing latent semantic analysis Journal American Society Infor mation Science 41 1990 391407 20 S Padó M Lapata Dependencybased construction semantic space models Computational Linguistics 33 2007 161199 21 T Hofmann Probabilistic latent semantic indexing Proceedings SIGIR 1999 22nd ACM SIGIR Conference Research Development Information Retrieval Berkeley CA pp 5057 22 DM Blei AY Ng MI Jordan Latent Dirichlet allocation Journal Machine Learning Research 3 2003 9931022 23 M Jarmasz S Szpakowicz Rogets thesaurus semantic similarity Proceedings RANLP 2003 Conference Recent Advances Natural Language Processing Borovetz Bulgaria pp 111120 24 R Rada H Mili E Bicknell M Blettner Development application metric semantic nets IEEE Transactions Systems Man Cybernet ics 19 1989 1730 25 C Leacock M Chodorow Combining local context WordNet similarity word sense identiﬁcation C Fellbaum Ed WordNet An Electronic Lexical Database The MIT Press Cambridge MA 1998 pp 265283 26 P Resnik Semantic similarity taxonomy An informationbased measure application problems ambiguity natural language Journal Artiﬁcial Intelligence Research 11 1999 95130 27 D Lin An informationtheoretic deﬁnition similarity Proceedings ICML 1998 15th International Conference Machine Learning Madison WI pp 296304 28 JC Chappelier Topicbased generative models text information access E Gaussier F Yvon Eds Textual Information Access Statistical Models ISTEWiley London UK 2012 pp 129178 29 J Chang J BoydGraber S Gerrish C Wang DM Blei Reading tea leaves How humans interpret topic models Y Bengio D Schuurmans J Lafferty CKI Williams A Culotta Eds Advances Neural Information Processing Systems vol 22 The MIT Press Cambridge MA 2009 pp 288296 30 R Mihalcea C Corley C Strapparava Corpusbased knowledgebased measures text semantic similarity Proceedings AAAI 2006 21st National Conference Artiﬁcial Intelligence Boston MA pp 775782 31 T Hughes D Ramage Lexical semantic relatedness random graph walks Proceedings EMNLPCoNLL 2007 Conference Empirical Methods Natural Language Processing Conference Computational Natural Language Learning Prague Czech Republic pp 581589 32 TH Haveliwala Topicsensitive PageRank A contextsensitive ranking algorithm web search IEEE Transactions Knowledge Data Engineer ing 15 2003 784796 33 C Bizer J Lehmann G Kobilarov S Auer C Becker R Cyganiak S Hellmann DBpedia A crystallization point web data Journal Web Semantics 7 2009 154165 34 R Navigli SP Ponzetto BabelNet Building large multilingual semantic network Proceedings ACL 2010 48th Annual Meeting Association Computational Linguistics Uppsala Sweden pp 216225 35 V Nastase M Strube B Boerschinger C Zirn A Elghafari WikiNet A large scale multilingual concept network Proceedings LREC 2010 7th International Conference Language Resources Evaluation Valletta Malta 36 F Suchanek G Kasneci G Weikum Yago A large ontology Wikipedia WordNet Journal Web Semantics 6 2008 203217 37 M Strube SP Ponzetto WikiRelate Computing semantic relatedness Wikipedia Proceedings AAAI 2006 21st National Conference Artiﬁcial Intelligence Boston MA pp 14191424 38 SP Ponzetto M Strube Taxonomy induction based collaboratively built knowledge repository Artiﬁcial Intelligence 175 2011 17371756 39 D Milne IH Witten An effective lowcost measure semantic relatedness obtained Wikipedia links Proceedings WIKIAI 2008 1st AAAI Workshop Wikipedia Artiﬁcial Intelligence Chicago IL pp 2530 40 ZS Syed T Finin A Joshi Wikipedia ontology describing documents Proceedings Second International Conference Weblogs Social Media Seattle WA pp 136144 41 D Milne IH Witten Learning link Wikipedia Proceedings CIKM 2008 17th ACM Conference Information Knowledge Manage ment Napa Valley CA pp 509518 42 K Coursey R Mihalcea W Moen Using encyclopedic knowledge automatic topic identiﬁcation Proceedings CoNLL 2009 13th Conference Computational Natural Language Learning Boulder CO pp 210218 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 201 43 R Mihalcea A Csomai Wikify Linking documents encyclopedic knowledge Proceedings ACM CIKM 2007 16th ACM Conference Infor mation Knowledge Management Lisbon Portugal pp 233242 44 D Ramage AN Rafferty CD Manning Random walks text semantic similarity Proceedings TextGraphs4 4th Workshop GraphBased Methods Natural Language Processing Singapore pp 2331 45 M Yazdani A PopescuBelis A random walk framework compute textual semantic similarity uniﬁed model benchmark tasks Proceedings IEEE ICSC 2010 4th IEEE International Conference Semantic Computing Pittsburgh PA pp 424429 46 E Gabrilovich S Markovitch Computing semantic relatedness Wikipediabased explicit semantic analysis Proceedings IJCAI 2007 20th International Joint Conference Artiﬁcial Intelligence Hyderabad India pp 612 47 E Gabrilovich S Markovitch Wikipediabased semantic interpretation natural language processing Journal Artiﬁcial Intelligence Research 34 2009 443498 48 S Hassan R Mihalcea Crosslingual semantic relatedness encyclopedic knowledge Proceedings EMNLP 2009 Conference Empirical Methods Natural Language Processing Singapore pp 11921201 49 T Zesch C Müller I Gurevych Using Wiktionary computing semantic relatedness Proceedings AAAI 2008 23rd National Conference Artiﬁcial Intelligence vol 2 Chicago IL pp 861866 50 P Cimiano A Schultz S Sizov P Sorg S Staab Explicit vs latent concept models crosslanguage information retrieval Proceedings IJCAI 2009 21st International Joint Conference Artiﬁcial Intelligence Pasadena CA pp 15131518 51 E Yeh D Ramage CD Manning E Agirre A Soroa WikiWalk random walks Wikipedia semantic relatedness Proceedings TextGraphs4 4th Workshop GraphBased Methods Natural Language Processing Singapore pp 4149 52 E Agirre A Soroa Personalizing PageRank word sense disambiguation Proceedings EACL 2009 12th Conference European Chapter Association Computational Linguistics Athens Greece pp 3341 53 P Turney Mining web synonyms PMIIR versus LSA TOEFL Proceedings ECML 2001 12th European Conference Machine Learning Freiburg Germany pp 491502 54 P Berkhin A survey PageRank computing Internet Mathematics 2 2005 73120 55 R Navigli M Lapata An experimental study graph connectivity unsupervised word sense disambiguation IEEE Transactions Pattern Analysis Machine Intelligence 32 2010 678692 56 R Ion D Stefanescu Unsupervised word sense disambiguation lexical chains graphbased context formalization Proceedings LTC 2009 4th Language Technology Conference LNAI vol 6562 Poznan Poland pp 435443 57 R Navigli A structural approach automatic adjudication word sense disagreements Natural Language Engineering 14 2008 547573 58 M Saerens F Fouss L Yen P Dupont The principal components analysis graph relationships spectral clustering Proceedings ECML 2004 15th European Conference Machine Learning Pisa Italy pp 371383 59 M Brand A random walks perspective maximizing satisfaction proﬁt Proceedings 2005 SIAM International Conference Data Mining Newport Beach CA pp 1219 60 D LibenNowell J Kleinberg The link prediction problem social networks Proceedings CIKM 2003 12th ACM International Conference Information Knowledge Management New Orleans LA pp 556559 61 Q Mei D Zhou K Church Query suggestion hitting time Proceeding CIKM 2008 17th ACM International Conference Information Knowledge Management Napa Valley CA pp 469478 62 P Sarkar A Moore A tractable approach ﬁnding closest truncatedcommutetime neighbors large graphs Proceedings UAI 2007 23rd Conference Uncertainty Artiﬁcial Intelligence Vancouver BC pp 335343 63 Metaweb Technologies Freebase Wikipedia Extraction WEX httpdownloadfreebasecomwex 2010 64 V Grishchenko Wikipedia anthill web page consulted April 15 2012 httpnogritzkoherelivejournalcom22900html 2008 65 DJ Watts SH Strogatz Collective dynamics smallworld networks Nature 393 1998 440442 66 T Opsahl P Panzarasa Clustering weighted networks Social Networks 31 2009 155163 67 D Bollegala Y Matsuo M Ishizuka Measuring semantic similarity words web search engines Proceedings WWW 2007 16th International Conference World Wide Web Banff Alberta pp 757766 68 E Agirre E Alfonseca K Hall J Kravalova M Pa sca A Soroa A study similarity relatedness distributional WordNetbased ap proaches Proceedings HLTNAACL 2009 Human Language Technologies The 2009 Annual Conference North American Chapter Association Computational Linguistics Boulder CO pp 1927 69 H Rubenstein JB Goodenough Contextual correlates synonymy Communications ACM 8 1965 627633 70 GA Miller WG Charles Contextual correlates semantic similarity Language Cognitive Processes 6 1991 128 71 L Finkelstein E Gabrilovich Y Matias E Rivlin Z Solan G Wolfman E Ruppin Placing search context The concept revisited ACM Transactions Information Systems TOIS 20 2002 116131 72 Z Wu M Palmer Verb semantics lexical selection Proceedings ACL 1994 32nd Annual Meeting Association Computational Linguis tics Las Cruces NM pp 133138 73 S Patwardhan T Pedersen Using WordNetbased context vectors estimate semantic relatedness concepts Proceedings EACL 2006 Workshop Making Sense Sense Trento Italy pp 18 74 MA Alvarez SJ Lim A graph modeling semantic similarity words Proceedings ICSC 2007 1st International Conference Semantic Computing Irvine CA pp 355362 75 MD Lee B Pincombe M Welsh An empirical evaluation models text document similarity Proceedings CogSci 2005 27th Annual Confer ence Cognitive Science Society Stresa Italy pp 12541259 76 S Hassan R Mihalcea Semantic relatedness salient semantic analysis Proceedings AAAI 2011 25th AAAI Conference Artiﬁcial Intelli gence San Francisco CA pp 884889 77 TM Mitchell Machine Learning McGrawHill New York NY 1997 78 X Hu X Zhang C Lu E Park X Zhou Exploiting Wikipedia external knowledge document clustering Proceedings KDD 2009 15th ACM SIGKDD International Conference Knowledge Discovery Data Mining Paris France pp 389396 79 G Bordogna G Pasi Hierarchicalhyperspherical divisive fuzzy Cmeans H2DFCM clustering information retrieval Proceedings WIIAT 2009 IEEEWICACM International Joint Conference Web Intelligence Intelligent Agent Technology vol 1 Milan Italy pp 614621 80 K Weinberger J Blitzer L Saul Distance metric learning large margin nearest neighbor classiﬁcation Y Weiss B Schölkopf J Platt Eds Advances Neural Information Processing Systems vol 18 The MIT Press Cambridge MA 2006 pp 14731480 81 CC Chang CJ Lin LIBSVM A library support vector machines ACM Transactions Intelligent Systems Technology 2 2011 article n 27 82 M Ranzato M Szummer Semisupervised learning compact document representations deep networks Proceedings ICML 2008 25th International Conference Machine Learning Helsinki Finland pp 792799 83 EM Voorhees D Harman Overview Eighth Text REtrieval Conference TREC8 Proceedings TREC8 Gaithersburg MD pp 124 84 O Egozi S Markovitch E Gabrilovich Conceptbased information retrieval explicit semantic analysis ACM Transactions Information Systems TOIS 29 2011 article n 8 202 M Yazdani A PopescuBelis Artiﬁcial Intelligence 194 2013 176202 85 H Li Learning Rank Information Retrieval Natural Language Processing Morgan Claypool San Rafael CA 2011 86 WT Yih K Toutanova JC Platt C Meek Learning discriminative projections text similarity measures Proceedings CoNLL 2011 15th Conference Computational Natural Language Learning Portland OR pp 247256 87 K CollinsThompson J Callan Query expansion random walk models Proceedings CIKM 2005 14th ACM Conference Information Knowledge Management Bremen Germany pp 704711 88 P Sarkar AW Moore A Prakash Fast incremental proximity search large graphs Proceedings ICML 2008 25th International Conference Machine Learning Helsinki Finland pp 896903