ELSEVIER Artificial Intelligence 88 1996 101142 Artificial Intelligence A statistical approach adaptive problem solving Jonathan Gratch Gerald DeJong bl Information Sciences Institute University Southern California 4676 Admiral Way Marina de1 Rey CA 90292 USA h Beckman Institute University Illinois 405 N Mathews Urbana IL 61801 USA Received May 1994 revised February 1995 Abstract Domain independent general purpose problem solving techniques desirable stand points software engineering human interaction They employ declarative modular knowledge representations present constant homogeneous interface user untainted peculiarities specific domain Unfortunately insulation domain details precludes effective problem solving behavior General approaches proven successful complex realworld situations tedious cycle manual experi mentation modification Machine learning offers prospect automating adaptation cycle reducing burden domain specific tuning reconciling conflicting needs generality efficacy A principal impediment adaptive techniques utility problem acquired information accurate helpful isolated cases degrade overall problem solving performance difficult predict circumstances We develop formal char acterization utility problem introduce COMPOSER statistically rigorous learning approach avoids utility problem COMPOSER successfully applied learning heuristics planning scheduling systems This article includes theoretical results extensive empirical evaluation The approach shown outperform significantly leading approaches utility problem 1 Introduction There wide gulf general approaches flective solving Practical success come custom 52661 application systems specific techniques techniques Corresponding Email dejongcsuiucedu author Email gratchisiedu approaches problem like expert systems reactive require extensive human 00043702961500 PII SOOO43702960001 l2 Copyright 1996 Elsevier Science BV All rights reserved complete AI researchers developed domain independent algo constraint satisfaction algorithms Unfortunately success systems derived general usually extensive domain specific technique bear investment rithms nonlinear planning general approaches adjustments The resulting resemblance Adaptive problem tradeoff performance solvers work performance irrelevant problem realworld unseen unlikely problems fact machine problem solving performance adaptive problem solving learning custom approaches solving repetitive problem problems hypothetical potential means solving actually circumventing generality situations We want problem solving Pragmatically systems overall performance enhanced encounter problems Worstcase behavior care largely sacrificing good behavior In enhance Nonetheless capacity techniques successfully demonstrated limited contexts 18465765 general sense far realized The principal impediment adaptive problem introduced machine hypothesized adaptation actually automatically formance Steve Minton refer difficulty insuring performance discussed search control heuristics progress issue understood solving performance 15404853 fail improve performance context improving called control certain circumstances problem results solving characterizing improved problem solving term utility problem average problem 531 Minton originally solving speed learning improvements rules While considerable proposed methods ad hoc poorly worse actually degrade problem Adaptivity effective method improving problem solving performance deduce priori blocksworld domain block represented On hand realworld problems constrained experience On hand domain specification implicitly embed constraints difficult atop distribution tasks embeds constraints For example greater induced application contain experience towers height ways obvious particular blocksworld explicitly constraint Exploiting regularities characterizations lead clear performance past problems Most distributions improvements Of course look future anticipate particular problems However problem solver exhibit generalize peculiarities intractable exploited detected Conversely worstcase algorithms perform certain distributions For example Goldberg suggests solved 0 n2 time 221 naturally occurring 5561 devising Recent work focused characterizing 3 techniques available Research optimization 45 exploit construction 51 pp 2522851 expected distribution easy distributions information exploit specific distribution satisfiability problems frequently fact problem better expected performance solver substantially tasks allow selforganizing dynamic learning systems In section provide formal characterization decision problem theoretic solving performance terms This introduces casts notion expected utility metric space learning utility problem search J Gratch G DeJongArcial Intelligence 88 1996 101142 103 piEEJcy Fig I Learning transforms initial problem solver new To accomplish learner choose set possible transformations techniques transformations COMPOSER algorithm performs probabilistic ensure problem solving 3 describes problem COMPOSER incorporates describes extensive evaluation approach domain compares average case analysis algorithms polynomial confidence favorably existing approaches independent problem number transformations probabilistic problem solver high expected utility Section utility approach space search efficiency process Section 4 context learning control rules COMPOSER avoiding transformation solver The experiments indicate complexity COMPOSERs utility problem Section 5 provides run time shown statistical considered required Finally discuss limitations present conclusions 2 A formal characterization utility problem algorithm learning Before rigorous terize learning formal characterization makes precise explicit provides structure precise statements constructed explicitly charac achieve In section introduce attempting general class learning problem solvers This formalism intuitive unstated notions learning formal analysis enabling definitive Abstractly learning algorithm operates initial problem solver transforming relative effect change criterion pattern tasks associated intended application solver assessed different problem evaluation This illustrated set potential problem transformations To characterize utility 2 common argued merits decision systems 4164677 1 machine Fig 1 set hypothesized transformations defines solvers The learning adopt good outcomes use decision outcome decision framework characterizing decide hypothesized solver theoretic notion expected decision problem Doyle new problem 131 seen increasing theory standard evaluating artificial acceptance artificial intelligence intelligence large learning particular 30354569 21 Expected utility Decision theory relies observation natural assumptions preferences different outcomes characterized realvalued utility function 104 J Gratch G DeJoqArtcial Intelligence RR 1996 101142 Outcome A preferred outcome B iff utility A greater utility B outcome Even In uncertain world decision produce The correct best decision policy time poorly decision policy uncertainty maximizes characterized set outcomes probability distribution set expected utility decision occurrence utility possible outcomes weighted probability expected utility decision For learning characteristics solvers problem transformed preferences utility choosing application transformation preferred With decision intended application determine outcomes theory represent cast decision problem learning expected utility However require function allowing increases obey following restrictions probability random selection tasks according Fixed distribution assumption The pattern tasks problem ment characterizable unknown probability occurrence associated task independent tasks seen probability common impose change time This solving environ fixed states domain discourse This restriction great applications However discuss Section 6 simplification applies distribution limitations set preferences expected utility hypothesis problem solver problem criterion expressed utility Expected utility assumption The evaluation function This function numeric value The function chosen problem solver A preferred problem solver B expected utility A greater expected utility B Decision theory posits function consistent requires efficient computable problem solve determine solver problem average problem solving If problem solver semidecidable proceed imposing resource bound assigning application CPU cost solve problem solved This computed actually attempting For example function time The expected utility problem time given fixed problem distribution suitable utility value conclusion learning utility exists utility 11 Ch 71 The utility I dont know measuring function solver 341 attributes In realworld It immediately clear represent preferences terms single situations comparisons actions utility measure In planning problems care basis performance fast plan created attributes execution cost translate robustness A large literature 631 problems multiattribute let PS denote problem utility PS function expected utility assumptions characterize value problem solver D denote probability decision single utility solver expected utility Formally problem x E D UPSx set possible problems With fixed distribution domain Prox example occurrence expressible devoted sciences J Crutch G DeJongArtifcial Intelligence 88 1996 101142 105 problem n The expected utility PS respect distribution D written ED U PS 1 defined UPSxPrDxdx D continuous upsxDx D discrete 22 Composite transformations Next formalize effects learning A learning algorithm maps initial problem solver PId new problem solver PS We introduce notion composite transformation denote structural changes performed problem solver course learning A composite transformation required transform Pd P built component structural changes For example PRODIGYEBL 531 builds composite transformation set learned control rules A given learning algorithm potential produce variety composite transformations depending initial problem solver distribution observed problems This corresponds notion hypothesis space classification learning characterize set possible composite transformations Alternatively set thought set problem solvers reachable learning algorithm Note set large possibly infinite nontrivial learning approach A composite transformation maps P1J PS The value composite transformation measured difference expected utility PS PId This provides metric assessing performance learning algorithm 23 Optimay versus improvement Expected utility defines total preference ordering set composite transforma tions associated learning approach The ideal learning algorithm choose composite transformation set highest expected utility Such algorithm considered optimal respect set entertained composite transforma tions One consider optimality requirement avoiding utility problemthat learning algorithm avoid lowering expected utility avoid suboptimal improvements Unfortunately optimality extremely expensive requirement For machine intractable identify optimal transfor learning algorithms computationally mation For example Greiner shows inherent difficulties transformations constructed macrooperators 361 We chosen insist optimality instead adopt weaker requirement In analysis learning algorithm adopts composite transformation increase expected utility need optimal choice In fact improving problem solver capabilities learning algorithm If composite transformations lead decrease expected utility 106 J Gratch G DeJmArtficiul Intelligence 88 1996 101142 learning algorithm solver unchanged ignore transformations leave initial problem 24 Unknown icformatiorl problem For given learning problem true unknown expected utility asso transformed problem solver utility transformed estimate solving randomly solvers typically unknown A learning training examples For example learning estimate ciated possible problem solver given problem probability distribution space possible problems formation selected problems original value transformation transformed improve estimate differ true expected utility sampling error composite transfor fact Nevertheless Unknown transformation mation characterizing learning improve performance Therefore adopt probabilistic An algorithm solves utility problem adopt composite negative expected utility long event occurs probability prespecified information means learning algorithm guarantee utility problem like explicitly quantify transformations increases expected utility There increase expected utility arbitrarily number examples close zero possibility estimated requirement chance Increasing 25 The utility problem 2 initial problem A learning algorithm exhibits utility problem lowers expected utility initial problem solver More precisely given 1 application described utility fixed probability function set problems access problems drawn according distribution 3 confidence parameter produces greater probability distribution Under definition composite learning algorithm fixed adaptive problem solver adopts improve performance We Pd b probability solver application Hold utility problem high probability exhibits solver PS lower expected utility Hold respect I 6 learning requirement transformation 6 PS satisfies transformed algorithm problem solves utility problem utility problem defined Admittedly solving weak requirement utility problem measure solve trivial way provide algorithm For example What requirement learning avoid learning confidence algorithm modifies problem solver time avoiding confidence mance problems drawn according minimal acceptable learn While general statement requirements scope article satisfied COMPOSER changes learning algorithm That learning utility problem modifications improve problem solving perfor Ideally restrictions outside requirements probability requirement additional subsequent like strengthen section discusses stronger distribution J Gratch G DeJongArtificial Intelligence 88 1996 101142 107 3 A solution utility problem The preceding framework defines goal learning technique sug embodies utility problem The effectiveness solution terms expected utility estimated gests natural judged level confidence statistical procedures Beyond solving COMPOSER performance high probability transformations basic solution realized tion outlines strategies addressing algorithm transformation arbitrary utility problem improve domain resolved efficient practical algorithm This sec difficulties ends presentation difficulties satisfied However basic solution shown requirements certain given 31 Incremental learning The significant composite difficulty arises transformations exploited In learning Frequently efficiently techniques investigate internal vast set structure composite trans later refer atomic For example SOAR constructs new problem builds learned control strategy transformations share transformations consists individual atomic simply transformations chunks individual 46 PRODIGYEBL control rules components 531 In systemscomposite possible transformations formation modifications solver individual individual given learning transformation transformations making local Instead making global decision possible composite builds composite transformation This analogous incremental decisions The composability problem problem identifying effective composite multiple atomic planning problem A planner transformations set complete plans Rather uses operators solve goal searching learning operators address solve variety goals particular combination planning described solving viewed perspective operators flexibly combined In COMPOSER view atomic constructed In fact learning generally Just individual combined terms transformations transformations inefficiencies problem incremental techniques individual progress planning atomic acceptable independent measures 311 Operationality The composability criteria problem constrains formation quality A measure ensure locally beneficial beneficial local measures measure ily combined Mitchell et als operationality hypothesis develop quality atomic composite One solution assess provide measure final composite 141 attempts transformations transformations appear criteria local measures atomic transformations trans combine independent measures These transformation transformation With eas independently 54 Etzionis nonrecursive Incidentally separately evaluated sampled problems 108 J Gratch G DeJongArtcial Intelligence 88 1996 101142 dependent problem distribution transformations obviating need statistical validation independent measures general possible Atomic interact difficult source difficulty transforma predict ways Such interactions planning Section 4 hypoth detrimental illustrated nonrecursive learning unfortunate independent measures consequences like operationality provide weak guarantees Unfortunately tions potentially long recognized looked Simple syntactic esis useful heuristics results consider illustration trustworthy As intuitive odor Another equally leaves Each rule problem selecting satisfactory improve chance rules particularly served typically This diminish evil dish large green blob dough Xanthin sequence things says Dont eat interact smells single friends warning way Each items sin common horrible Furthermore pay waiter allow sniff leaf social situations better dishes Dim Sum restaurant One seasoned patron advises Avoid exude faint hazelnutlike things prepared Xanthin enjoyable meal Suppose hazelnut offensive entree rule avoids single superficial overhead evaluating rules persuade dish comes explain avoid Xanthin Depending simply awkward forget rules risk taste green doughy blob Here rules avoid penalty As added benefit point fact idea Xanthin dish eliminates delicious leaves sensitivity broken Cantonese leaf sauce Indeed eliminating wish feature twice utility isolation A subtle sensitive problem rule context equivalent sum improvements interaction evaluated A second rule significantly change involves rule evaluation cost alters cost average evaluation solving procedure substantially To formal focus problem building composite transfor control transformations search control rules PRODIGYEBL mation atomic 531 Let utility negative time solve problem Control duce time solve problem eliminating overhead space portion search increases utility time saved minus evaluation However control rules sum improvements search introduce evaluation rules preconditions matched node search rule control cost Thus search evaluation criterion rules time candidate interact improvement rules isolation benefits search exceed tree pruned multiple control independent evaluation rules In isolation Let quantify Consider interaction control rules illustrated Fig 2 This shows hypothetical searched initial problem node 1 Suppose r s heuristics respectively exhaustively solution nodes sets R S trimmed r 1 SI similarly defined When search space nodes 1 RI number nodes conclude prune nodes solver J Crutch G DeJongArtcial Intelligence 88 1996 101142 109 RS nodes saved r SR nodes saved s RnS nodes saved rands M Average match cost r MS Average match cost s g Average cost expand node Fig 2 Example interacting heuristics r checked times isolation node 2 saving nodes 38 node 9 saving nodes 1012 Heuristic times evaluation node g twice s checked 15 ISI succeeds node 1 saving nodes 915 Let average cost r M average cost s MY average time expand 15 RI It successfully applies Let U Xp utility problem solver p The interaction additive rules problem set rules X problem utilities ResidueUrsp Urp Usp RSIMIRnSg This residue measures interaction value mations combine synergistically prune subtrees control When negative overlap utility isolation interactions benefit atomic avoid The key point preclude effective transformations potentially search harmful combine atomic rule tends expensive transformations positive For example control The transfor rule evaluate If residue maybe large rules positive yield strategy worse Such control determining independent criteria rule average match cost second decreases engage interaction For example 312 Incremental utility context sensitive measure While develop general independent measure atomic transformation quality develop context sensitive measure atomic view composite transformations transformation participating composite In particular problem solvers expected utility We adopt context sensitive improves expected sequence intermediate transformation transformation states given atomic accounts context PsldPSlPS2PSnew measure utility appended utility existing sequence transformations Let PS denote problem solver let PST Apply PS denote solver incremental results applying atomic utility r difference transformation problem expected utility PS r PS We define I IO J Grtrtch G IeJonArtQicicd Intelligence 88 I 996 101142 expected utility PS 2 We denote conditional given problem change expected utility provided transformation incremental utility AlJo71PS meaning 7 distribution D solver PS We state formally equivalently UPSr UPSxlPrDxdx D continuous lUPSx UPSxProx D discrete 2 The change expected utility provided composite transformation equivalent sum incremental utilities transformation Our definition incremental utility clarifies important properties transforma utility dependent distribution D tions First effect transformation Second effect conditional incremental distribution incremental posite utility values utility transformation vary unpredictably problem indicates transformation considering problem solver applied In general change nature identify globally maximal com explosive number conditional general potentially applied The conditional solver utility 32 COMPOSER COMPOSER learning element ite transformation utility Additionally adopt sequence improving tence sequence requires guaranteed COMPOSER element certain characteristics training problems drawn randomly COMPOSERs method searching define method achieves constraints statistical approach provides atomic transformations provably solves utility problem Given prespecified given sufficient COMPOSER confidence adopts compos improve expected examples COMPOSER exis given transformations high probability input transformable specified initial problem utility solver learning source problem distribution We transformations We set composite Finally transformations function proposing atomic statistical guarantee In learning algorithms incremental utility simply referred utility We feel additional terminology helps highlight difference utility problem solver user utility transformation learning algorithm J Gratch G DeJongArtcial Intelligence 88 1996 101142 111 32 I Overview Because compose linearly typically However want great improvement utility conjunction greedy hillclimbing set possible composite transformations solver incrementally incremental solver results utility Each new transformation previous applying intractable determine COMPOSER best possible proce assessed transforma training utility incremental problems This hillclimbing transformations adopts begins interactions One shortcoming uses statistical methods according estimate distribution difficulty negative explore uses incremental sequence transformations COMPOSER dure search original problem estimated respect tion COMPOSER examples drawn randomly approach successfully exploit positive possess positive problem avoids interactions Solutions local optima 322 Transformation generator COMPOSER requires source transformations step search This formalized set candidate abstractly TGPSxXq examples transformations map problem solver new possibly SOAR viewed transformation training problem single Etzionis STATIC original problem function transformation generator This function ok maps problem solver optional set training set improved problem solver For example takes current problem solver impasse generates set chunks takes solver training examples generates set control 141 seen transformation Each transformation produced generator generator rules 323 Statistical inference COMPOSER estimate incremental utility training stances reasoning data assess favored worstcase These useful function set predefined parameters unknown learning community techniques theoretical called nonparametric statements inefficient accuracy estimates There philosophical statistical issues The computational statistical models techniques provide practical alternative assumed Inference popular parametric prior knowledge We prefer frequentist need specification insuring absolutely probable performance estimating alternative prior reduces practical uses Parametric In distribution utility values values unknown values data Bayesian models worstcase models require alternative specification transformations prior avoid dependence socalled information statistical models efficiency Bayesian approaches information3 When balance efficiency error exceed bound achieving adopted reasonable s There controversy Bayesians frequentists approach appropriate cases shown Bayesian approaches noninformative approaches We stand Bayesian changing mapping COMPOSER O1 loss function issue One contributions map probability 6 bound expected frequentist replace frequentist model work The straightforward statement Bq 3 framework interpretation principal priors equivalent loss decision 2 p 631 Under Bayesian theoretical I12 J Gratch G DeJongArtcial Intelligence 88 1996 101142 err efficiency long good arguments exceeded necessary practice In Appendix A adjust error bounds tradeoff COMPOSER ensure overall error remains threshold S Formally To achieve 3 COMPOSER account identifies new transformation time sources error Each adopt compares set estimates considered step The source error associated COMPOSER transformation estimates utility somewhat step Third error steps accounted appears positive Second probability larger probability individual error overall decision adopt given final problem solver PSnew produced result steps errors combine transformation negative incremental We define function Bound 6 ITI specifies acceptable error utility estimate function overall error 6 size set transformations search 4 T given step hillclimbing Bound6 ITI avoids sense definition second error individual bound 6 Given incremental issues First determine Second decide accurate estimates We estimate incremental remains utility transformation utility problem bounding incremental utility estimate sense error way combine total acceptable overall error remains construct statistical procedure specified error bound This estimates involves estimates generated training problems needed training problems attain sufficiently utility randomly drawing problems according resulting averaging incremental consideration utility estimated solver PS set transformations distribution D transformation n utility values Call jPs incremental training problems Given current problem problem X COMPOSER determine current transformed problem solvers Vr E T U PS x U PS x Recall definition behavior problem solver problem Thus given set m transformations compute PS PST PF problem solving attempts The complexity processing problem involves m 1 tied PFm Processing training example measurable observing utility values solving utility problem solver problem utility necessary difference example incremental 4 This definition embodies compromise bounding statistical error example efficiency works practice The rational compromise discussed Appendix A illustrate possible definitions J Cratch G DeJongAtcial Intelligence 88 1996 101142 113 A 5 9 5 9 _ __m______ Estimate 0 I 1 I __ __ 5 __ I I __ I 10 I 15 Training Examples Fig 3 Sizing confidence interval We like examples confidence interval lies entirely axis complexity m 1 problem solvers We bruteforce processing default method COMPOSER We Appendix A applications utility values efficient methods incremental obtaining utility estimated sufficiently incremental We determine suitable sample size considering accuracy estimates number examples computation Specifically large sample size bound improves increase COMPOSER determine probability positive fact negative vice versa We prefer use examples possible COMPOSER determine examples sufficient relies sequential statistical inference fixed sample advance determined function observations Sequential procedures provide test called stopping rule determines sufficient examples taken An important advantage sequential significantly 23 Sequential procedures differ common average number examples number examples techniques technique required procedures required fixed sample sample size stopping rule stopping techniques rule proposed NGdas 60 The simple Given sample values We determine tends level cr construct confidence 1 In words lies outside confidence shrink 5 The stopping utility intuition confidence utility probability incremental interval needed incremental 1 Y transformation positive interval negative tends utility illustrated interval containing true incremental true probability interval With examples width rule determines examples zero shrink point entirely Fig 3 At point state probability estimate incremental negative utility positive After example processed rule evaluates Nhdas stopping true When transformation speed slow PS estimated negative confidence 1 Y Examples taken rule evaluated Sampling occurs state incremental utility following terminates given positive equation holds 5 The interval size actually random function data It tend shrink examples monotonically It grow temporarily outlying data point encountered 114 I Grutclt G DelonArtijiciul Intelligence 88 1996 101142 4 II number examples erage improvement ment LY acceptable initial sample size Qcu parametric true estimated incremental error S r PS observed variance utility taken far AU TIPS transformations estimate 110 small transformations integer finite av improve indicating discrepancy function models Q LY x Iy statistical assumption parametric The function Qa makes utility parametric model 38 p 1921 This true incremental normally distributed random variable true mean distribution This theorem demonstrates incremental sonableness Central Limit Theorem distribution distributed underlying mal purposes estimating strictly holds sample size tends infinity extensive experience problems demonstrated small sample sizes justified important states average values tend normally Central Limit Theorem realworld effectiveness normal approximation expected utility Although accurately 39 Section 531 nonnormal approximated distribution estimated utility The rea statistics regardless theorem theorem The choice minimum Appendix A By default use sample size worked size 110 relates definition Qa sample discussed empirical investigations 324 The COMPOSER algorithm COMPOSER illustrated statistics evaluates Fig 4 After problem solving attempt COMPOSER rule candidate transformation stopping If updates stopping utility COMPOSER invokes instead stopping eliminated affect candidates time transformation higher rules satisfied transformations positive incremental transformation highest adopts transformation generator obtain new set candidate rules satisfied transformations negative eliminating consideration note current problem unaffected This cycle repeats solver statistics associated adopted predecessor giving COMPOSER training expected utility resulting problem solver anytime behavior 71 transformations incremental incremental utility If utility candidate remaining set exhausted Each 4 Utility problem PRODIGY problem solver This section describes extensive evaluations performed search We application COMPOSER learning COMPOSER I Gratch G DeJongArtcial Intelligence 88 1996 101142 115 COMPOSERPSld TG s 8 examples 4 1 PS Pd T TGPS n 0 0 cy Bound6 ITI 2 While T 0 examples1 3 4 5 6 7 8 9 10 11 12 Hillclimb Repeat long data possible transformations Until data taken identify step n n 1 1 steptaken Jr E T Get AUi71PS FALSE Observe incremental utility values ith Problem signicunt Q E T n Collect transformations reached statistical significance S2lPSulPS12 nQ412 If 3 E signiznt E 1 PS 0 Then Adopt T increases expected utility PS Apply x E significant Vy E signrcant bUxlPS T TG PS n 0 LY Bound S ITI steptaken aclylPSPS TRUE Else T T r E signicant lPs transformations Discard 0 lower expected utility Until steptaken T 8 Iexamples j Return PS BoundS ITI SlTI Qa x s 1e05J2dy a2 Fig 4 The COMPOSER algorithm described 261 PRODIGY planning served basis learning 531 PRODIGY status benchmark learning investigations systems COMPOSER studied 1543 problem learning heuristic control strategies control strategies planning 531 attained successfully NASA scheduling domain A main goal evaluation applied contrast COMPOSERs approach including methods developed explicitly addressing utility problem general methods PRODIGY COMPOSER We like compare implementation minimizing context PRODIGY problem solving method use PRODIGYs learned reimplemented discussed source statistical theoretical basis alternatives details Therefore pains provide fair comparison systems Each method differences transformations constrained explanation approach approach learning similar based 41 The application This available problem implementation constructed PRODIGY 20 architecture Carnegie Mellon University PRODIGY solver based STRIPS planner general purpose meansends 181 enhancements Plans 116 J Gmrch G DeJonArtiJicicrl intelligence 88 1996 101142 identified depthfirst decisions search Search proceeds recursively applying control I choosing node expand current search space node contains conjunction goals achieved 2 choosing unachieved goal node 3 choosing operator possibly achieves 4 choosing binding list operator default control method implements PRODIGY methods modified introduction rules described rules Control Section 62 goal decisions called control heuristic knowledge 41 I Problem distributions We evaluate COMPOSERs domain reported different domain problem robot moving boxes ABWORLD domain designed BINWORLD PRODIGYEBL construction domain introduced highlight deficiencies domain ability identify effective modifications theories The STRIPS domain reported interconnected It rooms lockable doors The PRODIGY 53 161 It variant standard blocksworld Mintons PRODIGYEBL approach The 301 designed Etzionis STATIC approach This domain highlight deficiencies simple Problem distributions STRIPS domain ABWORLD generators 20 Following provided PRODIGY set problems biased filtering problems problem 53 difficult easy problems excluded I CPU second structed methodology judged control Problems deficiencies detailed description experiments PRODIGYEBL strategy required BINWORLD generated domain default PRODIGY 100 CPU seconds highlight Appendix B A designed STATIC This described appears 241 distribution 412 Expected utility We follow established PRODIGY methodology CPU seconds time cumulative interpretation theoretic decision solving performance 16531 Under In function based time required captured particular let utility problem solver problem negative CPU time required translates improving solve problem Maximizing time required solve set problems evaluation expected utility solve problem solve problem measuring utility average criterion problem 42 Transformation generator Minton introduced technique generating control strategy The approach uses explanation struct atomic search control theory problem control points The search control rules conditionaction way PRODIGY atomic modifications based learning rules based traces problem PRODIGY 12541 EBL solving behavior rules associated statements alter space possible plans solver Sets control explores J Crutch G DeJongArtifcial Inrelligence 88 1996 101142 117 RULEl IF currentnode n currentgoal n CLEAR x NOT HOLDING x true n THEN choose operator UNSTACK Fig 5 An example control rule PRODIGYEBL generates rejection selection control rules guaranteed sound prune valid solutions search tree 6 In particular acquire rejection control rule PRODIGY particular decision principle lead successful outcome PRODIGY performs tree search viable options If success node encountered exhaustive search PRODIGY reexpresses conditions failure way tested node Under conditions point conducting search doomed fail The rejection rule prunes decision future searches Likewise selection control rule acquired ways resolving decision lead failures If resources exhausted search completed rules learned portion tree This limitation approach fortunately recursive domains usually fully explored subtrees learn useful control rules An example operator selection rule shown Fig 5 Under conditions antecedent operators UNSTACK necessarily lead failures While control rules sound increase efficiency planning All control rules avoid search plan space introduce cost matching preconditions A rule harmful precondition evaluation cost exceeds savings Furthermore control rules interact subtle ways Without criterion choosing possible rule sets learning algorithm quickly degrades performance Minton introduced heuristic empirical procedure addressing utility problem context This procedure attempts account distributional nature incremental utility individual control rules Minton calls overall approach EBL learning heuristic utility analysis PRODIGYIEBL Unfortunately performs domains PRODIGYEBL shown undesirable properties Etzioni illustrated seemingly innocuous changes domain theory result degraded problem solving performance 151 We showed behavior utility procedures inability correctly estimate distribution information handle composability problem 291 COMPOSERs statistically sound utility estimation procedure corrects problems result effective learning algorithm The EBL unit PRODIGYEBL produce control rule types rejection selection preference rules Preference rules fact lead incorrect action selection Minton noted preference rules effective We verified PRODIGYEBL actually produces strategies higher utility prevented producing preference rules 29 We disabled learning preference rules implementation enabled efficient means gathering incremental utility data points Appendix A II8 J Gmtch G DeJonArtijiciul Intelligence 88 1996 101142 For evaluation EBL component PRODIGY serves transformation analyzes trace solution attempt conjectures search control rules serves atomic rules Each control generator The EBL component new control current actual use transformation usage lines 1 3 algorithm set T problem line 8 algorithm new transformation strategy To consistent use PRODIGY normal conjectured added adopted carried forward transformations statistics discarded previously conjectured set generator differs somewhat solving event Whenever Fig 4 Rather transformation COMPOSER transformation transformations transformations potentially forcing 43 COMPOSER implementation details construct adaptive problem solver applica We COMPOSER tions We resulting PRODIGY COMPOSERPRODIGY control rules acts initial problem solver Mintons EBL learning element acts solver transformation set control adding control takes problem PS generator Thus COMPOSERPRODIGY rules Pd produces rules positive incrementally implementation incremental utility control extracts modified procedure attempt To accomplish allowed tailor COMPOSER Several properties application adopted candidate control utility data points The implementation rules unobtrusive achieve rules efficiently greater statistical efficiency We exploited property control utility incremental gather incremental trace values candidate PRODIGY planner single solution rules distinguish utility added shown control strategy PRODIGY planner Candidate rules proposed EBL technique validated When solving problem candidate rules checked recorded search paths actually eliminated After problem eliminate search paths solved eliminated candidate control savings provided rule This savings avoidable paths indicates reported compared recorded precondition match cost difference incremental identify rules The time spent exploring cost search paths rule problem rules Adopted control trace analyzed utility control precondition positive annotated incremental 44 Evaluation We evaluated COMPOSERPRODIGYs utility problem addressing criteria performance proposed Appendix A describes applications Bouzd function modified improve efficiency utility analysis For implementation Eqs A2 A appendix define variant default Bound As stated Section A I I allows transformations added T low variability control rules minimum 3 time utility analysis Due relatively J Gratch G DeJongArtificial Intelligence 88 1996 101142 119 1 heuristic utility analysis PRODIGYEBL 2 3 hybrid PRODIGYEBL hypothesis STATIC 141 STATIC suggested Etzioni nonrecursive 531 141 overcome limitations systems 4 PALO 351 statistical approach similar COMPOSER based conservative statistical model experiments review differences problem solving techniques evaluations systems All systems implemented transformation use For framework Before discussing tried generator minimize PRODIGY 441 PRODIGYIEBLs utility analysis This technique developed Minton rules proposed example problem use PRODIGYEBL strategy The savings afforded rule estimated transfor adopts added value rule time applies Match cost measured directly problem current control strategy The issue gathered mations heuristic utility analysis As control current control single example credited traces averaged multiple savings cumulative interactions interactions rule removed transformations If cumulative cost exceeds addressedestimates rule learned training examples 442 STATICs nonrecursive STATIC utilizes control utility The criterion effective EBL 14 p 61 The hypothesis transformations generated predicate A weaker constructed negative performance addressed STATIC applies macrooperators For Etzioni subgoal reading nonrecursive incremental explanation hypothesis rule selection criterion based Etzionis structural grounded able nonrecursive curtail admits search nonrecursive The strongest interpretations explanations interpretation hypothesis This states theory positive incremental utility regardless problem distribution nonrecursive explanations planning behavior derived instantiation predicate composite elements strategy admitting utility set nonrecursive average The issue interactions criterion control 47681 improve expected utility transformations improve rules issue transformations transformations important control PICKUP operator chance leading proposition tree contains executing proof suppose blocksworld PICKUP operator pruned matches alizing PICKUP necessarily explanation includes requirement inferences CLEARB rule consideration recursive solution proposition predicate For example rule stating future situation constructed gener examples strong control CLEARA turn supported This explanation recursive According conditions The precise conditions problem PRODIGY acquire rejection fails example Now suppose 120 J Gmtcch G DeJongArtiJicial Intelligence 88 1996 101142 reading nonrecursive overall utility hypothesis associated control rule improve reason success 151 8 This claim nonrecursive cited principal domains The nonrecursive STATIC outperforms PRODIGYEBLs algorithms construct use different respective control pothesis evaluate ies employed effectiveness plicating NONREC algorithm PRODIGYEBL analysis syntactic criterion adopts nonrecursive acts filter allowing PRODIGYEBL satisfy hy difficult rule generators Different vocabular rules We wish focus com hypothesis empirical utility rules This rules All rules hypothesis rule generator To achieve reimplementation framework NONREC final problem solver remove STATICs nonrecursive replaces PRODIGYEBLs criterion incorporated goal constructed generate nonrecursive factor different nonrecursive control 443 A composite algorithm strengths STATIC PRODIGYEBL 141 He proposed hybrid algorithm embodies including layered utility criterion The nonrecursive combined hypothesis acts utility nonrecursive control rules subject Etzioni suggests single approach advancements initial filter remaining analysis later discarded NONRECUA We implemented rules proposed PRODIGYEBLs basis nonrecursive PRODIGY EBL algorithm learning module test hybrid criterion As control filtered rules undergo utility analysis hypothesis The remaining 444 PALOs Chernoff bounds Greiner Cohen proposed approach similar COMPOSERs 331 The approximately evaluates rule probably technique stopping terminates transformation based Chernoff bounds locally optimal transformations incorporates learning high probability space Our evaluation criterion PALO statistical method PALO differs approach adopts hillclimbing stop learning PALO identified nearlocal maximum rule focuses different stopping Chernoff bounds provide conservative model discrepancy sample mean true mean distribution As result PALO provides stronger user bounds statistical error cost examples This means specifies error level 6 true error level exceed 6 fact PALORI uses candidate learning begins A candidate approach Like COMPOSER set rules In case size set fixed lower9 Our PALORI algorithm evaluates adopted following rule holds stopping Etzioni Minton subsequently suggested success STATIC fact global analysis allows concise rules 17 1 In addition conservative stopping rule PALO adopts conservative definition Bound follows adopting Eqs A I A5 COMPOSER adopts Eqs A I A3 J Crutch G DeJongArtcial Intelligence 88 1996 101142 121 n KGTPS A7 2nln J Tm h 1 W 36 hU TIPS estimated size largest possible current transformation search transformation candidate sequence incremental utility transformation set h number transformations number steps taken 7 Tmax added hillclimbing size range incremental utility values given A7 max AVrlPS minA0rPS true value strongly performance minimum typically unavailable sample complexity A disadvantage technique utility values tightly possible underestimating depends maximum transformation effected advance improvement To use method able bound setting AT parameter learning possible given range incremental true range In context PRODIGY set upper bound noting problems restricted transformation purposes Rather bound A7 maximum solve problems training require knowledge provide 100 seconds This conservative time PRODIGY actually requires learning Using tighter bound help solvable resource bound 100 CPU secondsthe help hurt set learning detailed knowledge provided systems unfair transformations expected PALORI PALORI uses method COMPOSERPRODIGY obtain setting systems parameters incremental utility statistics We discuss section sampling test terminates One advantage PALO included PALORI incremental threshold data best transformation fall prespecified tional recognized pend considerable performance parameter corporates indifference includes detailed discussion tradeoffs indifleerence zone methods referred Section 5 Methods incorporate zone COMPOSERlike method incorporates addi utility transformations In contrast COMPOSER ex leads negligible improvement additional dont care 11 Our recent work 251 That paper imposed approaches 445 Experimental procedure We investigated STRIPS domain 531 ABWORLD domain produced harmful strategies BINWORLD 301 yielded detrimental PRODIGYIEBL learning The problem distributions constructed provided PRODIGY criteria Results summarized results STATICs PRODIGYEBLs Fig 6 In domain algorithms random problem generator rule set saved architecture The current control 141 domain 122 I Grtrtclr G DrJolSArtfirictI lntelligrnce 88 I 996 IO142 training execution experiments times distinct training examples The independent measure number training examples dependent measure time CPU seconds 100 test problems drawn distribution This process repeated problem generator All results reported average trials rounded nearest number Fig 6 shows comparison graphs number 100 rules 100 training examples number seconds required set test problems COMPOSER IO experimental optimal values difficult given information influenced parameters assess We tried assign values close optimal require error parameter 6 runs PALORIs behavior learned algorithm number seconds generate solutions test sets constructed PALORI process strongly available required evaluation proved examples wcrc required During tions reach quiescence training harmful statistics PALORI performed instead terminated PALORI examples whichever came PALORI adopt transforma 100 training examples We tried algorithm examples large To collect trials Further transformation adopted 10000 expensive The problem candidate rules discarded quickly COMPOSERPRODIGY twofoldfirst set grew secondly learning apparent The results illustrate exceeded interesting performance features The implementation approaches developed domain The rules COMPOSER learned fewer control beneficial control strategies mations domain Therefore differences results strategies yielded higher expected utility succinct STRIPS COMPOSERPRODIGY In ABWORLD In BINWORLD In fact appear It stressed control algorithms USC transformation containing identified algorithm adopt transfor rule improves performance generator utility problem represent differences approaches vocabulary transformations We expected COMPOSERPRODIGY higher learning times PRODIGY EBL NONREC rigorous Surprisingly COMPOSERPRODIGY quickly discarded control rule high match cost PRODIGYEBL NONREC times higher nonstatistical learned quickly BINWORLD assessment incremental utility approaches learning actually I PRODIGYEBLs utility analysis requires addttional settling phase training Each control strategy produced PRODIGYEBL NONRECfUA received settling phase 20 problems following methodology outlined 153 1 C fixed based size candidate list observed practice In best case rule save entire cost solving problem domain lambda rule set maximum problem solving cost observed practice ARWORLD C 30 A 15 STRIPS C 20 A 100 BINWORLD CS A 150 e m T n o t u l o S e g r e v A m e l b o r p r e p s d n o c e s U P C e T n o t u l o S e g r e v A e T n o t u l o S e g r e v A 124 I Garch G DeJongArtijicid Intelligence 88 1996 OId2 hypothesis difference completely account fact STATIC rules In experiments ABWORLD The nonrecursive remaining learning difference We attribute NONREC entertain constrained NONREC Etzionis experiments STATIC entertained This conjecture recently somewhat different sets control use rule vocabulary available PRODIGYEBL somewhat different space rules supported Minton Etzioni Finally PALORI improve performance 100 training exam approaches This converge selected Thus transforma fact initial large sample sizes required PALO allow utility estimates true values transformation likely outperform selected 171 utility rules allowed PALORI tend In exceed COMPOSERPRODIGYs given extended examples In contrast COMPOSER utility variance utility values determining reached significance This results COMPOSER transformations quickly Unfortunately high terms examples learning somewhat beneficial strategies recog cost PALOs time achieves higher incremental ples given sufficient examples closely tions selection better control performance sesses transformation nizing performance While COMPOSER identify faster convergence ABWORLD incremental low variance improvement 5 Complexity results We turn analysis COMPOSER algorithm Given emphasis aspects algorithm analysis consider worstcase behavior practical sample results Rather provide average case complexity complexity run time complexity algorithm We focus number examples work required total steps taken domain complexity depend number hillclimbing specific perform single step hillclimbing number examples statistical search Obviously algorithm inference required The number examples taken hillclimbing complexity factors related work step depends number candidate step specify transformations With knowledge transformation organize exact function specific relationship user COMPOSER generator meaningful ways For example step depends domain specific transformations work number assess best learning problem 51 Properties Ncidas stopping rule The properties COMPOSER inference Therefore consider Given transformation examples sufficient tive probability 7 error level stopping characteristics follow properties method statistical rule rule determines nega rule proven Nadas stopping utility 7 positive stopping incremental 1 LY The characteristics J Crutch G DeJongArtificial Intelligence 88 1996 101142 125 601 The proofs technical restate results Nadas intuitive explanation COMPOSER hold takes examples following inequality holds Eq 4 taken far bu transformations n number examples improvement acceptable sample size Q cu function estimated incremental utility observed variance Si error estimate small finite transformations models integer indicating discrepancy minimum true improvement average Y Qa x For given sequence training examples time somewhat misleading number examples called stopping term stopping temporal duration procedure We retain statistical complexity time E ST This average number examples results Nadas derive time literature The stopping rule characterized stopping following time sequential stopping rule satisfied The statistical refers number examples literature term consistent time ST random variable The sample expected value stopping decision From expected stopping relationship required transfotmation Theorem 1 Let ST stopping given distribution associated transformation utility Then time associated Nddas stopping rule Let requested error level o2 actual variance incremental t_ actual 1 For small 1 expected stopping time governed following relation ship EST a2 2 u2 2 For large 1 Y expected stopping time governed following relation ship EST z lnla This result states stopping time determined control user fixed unknown properties inference problem The average stopping particular bounded quadratic transformation constants error level parameter cr2 P time associated ILY log la large Ia inverse mean This makes difficult values zero difficult default strategy difficult bound linearly variance transformation greater intuitive sense mean greater variance incremental mean closer incremental bound transformation quadratically required confidence utility utility better worse form equation time derived Nadas Proof A nonclosed 60 stopping The theorem follows proof results 24 Appendix B Although complete proof lengthy result like Theorem 1 hold The Central Limit Theorem states normalized incremental difference utility easy compute approximately confidence statistics book shows suitable mapping notations utility sample observation mean given sample II observations Any introductory true incremental normally distributed Using include easy provide intuition interval words probability p lies The Nadas stopping interval rule designed interval AUQcv m I true incremental utility transformation II w h ere Si variance n samples examples size confidence twice size unknown mean p Formally Solving relationship tor II WC following relationship This shows II number examples produce confidence appropriate result Theorem I size Or stated differently II stopping time It remains Qcu grows function interval bounded linear shows Q LY m obtained Both derivations follows function x 1 For large 1 Qa asymptotic expansion 241 given 2 Mors inequality More precisely converges dm standard normal distribution ILY That Q cy This 52 Sample complexit The sample complexity inference takes number examples step hillclimbing way bound search As stated number steps COMPOSER function equivalent number examples largest stopping statistical perform required time step COMPOSER I Gratch G DeJongArtkd Intelligence 88 1996 101142 127 expected number examples taken step hillclimbing space associated application However transformation particular characterize search terms parameters Within particular hillclimbing LY allowable transformation negative total error bound The error Y related LY SjTI incremental utility step set transformations statistical error associated incremental utility estimate T The value LY bound probability transformation T Recall perceived positive vice versa Let value S 6 Bound function default consumes 2 As validation negative proceeds estimates computing 1 transformations step COMPOSER transformations shown transformation positive dynamically arise discarded adopted 3 examples second adopted sample complexity given sufficient data provided decision examples training T One cases utility utility In case expected number T time governs expected number examples time transformations theorem describes maximum set exhausted COMPOSER stopping relationship The following transformation equivalent equivalent identified training incremental incremental stopping Theorem 2 Let ST number examples consumed step COMPOSERs hillclimbing search default settings Eqs A 1 A3 S error bound T set transformations step Then 1 For small ITlS expected sample complexity bounded polynomial T 116 2 For large TS expected sample complexity governed c constant value depends expected incremental utility variance incremental utility values transformations T Proof This follows directly The constant c expected value gFuT transformation adoptedrejected 1 default definition LY SIT Theorem step Thus expected number examples required step grows quadratically IT grows quadratically increase increase polynomially number candidates Similarly require greater statistical confidence 1 S This means things equal required step sample complexity increase expected number examples 0 12X J Grurch G DeJonqArtiJicrol intelligence 88 1996 lOI 53 Run time complex The expected run time algorithm depends number examples bound single step algorithm 7 example cost algorithm cost process example possible advance Therefore provide results complexity performing Under utility statistics gathering actually processing tied complexity problem solver tries transformed problem solver bruteforce method incremental Theorem 3 Let R upper bound cost ojsolving problem Then expected run time complexity I For small jTlS 2 For large T6 rhe expected run rune complexity ORiTiln v Proof Where T6 small Theorem 2 number samples required step Using sample maximum large jTiS default means gathering jr 1 times Each solution incremental utility statistics attempt cost mosi R leading requires solving cost RT process example The analogous argument holds Therefore expected cost hillclimbing step grows linearly required confidence ically mations step JTI cid1441 l6 cubically R quadrat number transfor 54 Discussion Theorem I interesting consequence COMPOSERs performance fewest examples Theorem 1 states transformation exhausts utility transformations positive A step terminate COMPOSER cremental required fewest examples highest utility Thus COMPOSER This observed necessarily ratio set possible identifies transformation positive If utility COMPOSER adopt highest incremental utility transformations incremental requiring necessarily variance square incremental steepest ascent hill climbing perform A problem utility COMPOSER comparison PALORI arises transformations terminate T nearzero incremental accepted transformation J Gratch G DeJongArtQicial Intelligence 88 1996 101142 129 sample complexity utility transformations increases dramatically Although zero rejected As incremental transformation nearzero utility occurs algorithm able decision Rather algorithm simply exhaust training examples making improvements sense terminate search We discuss step early proceed possibility expected utility Under circumstances step hillclimbing Section 6 different unlikely tends 6 Limitations future work COMPOSER provides probabilistic solution COMPOSER In section discuss problem reported realize generality practicality implementations important strated aging restrict tensions successful utility problem utility transformations set commitments large space possible commitments approach We characterize limitations results We consider organizing search gathering 3 1 solving applications 26271 While utility problem demon described successes encour embodies design commitments limitations possible ex necessary aspects conditions extensions space modifications embodies statistics COMPOSER estimating particular aspects seen point 61 Applicability conditions We summarize basic conditions COMPOSER requires satisfactory results 611 A structured transformation space solver constructed composing body atomic mod space possible composite modifications large genera small sequence search steps relatively intractable COMPOSER requires transformation search space A modified problem In general ifications exhaustive tor structures factor branching Clearly COMPOSERs performance tied transformations effective COMPOSER strategy Because nature hill climbing exists COMPOSER exist good methods guarantee given If control points good strategy For experiments existing transformation problem solver set control adopts addition single entertaining rules time This problems hill climbing However COMPOSER allowed transformed rules One control transformations imagine work effectively rules act unit The cost actually performing transformation new problem solver small Transformations rule control single ameliorate probably control generate I30 J Grutch G kJmAtcd lnrelligence 88 1996 101142 spatial changing likely candidates reasoner COMPOSER approach surface representation volume representation 6 I 2 Availability representative training problems fixed problem distribution provided sufficiently COMPOSERs statistical approach assumes To estimate pattern tasks represented distribution algorithm large body training problems set different violate cycles subpatterns A task distribution fixed distribution COMPOSER assumption However difficulties With quickly shifting cyclic patterns approach Shifts shifts slowly time presents difficulty distribution partially overcoming extent suffice average cycle One draw problems systematic changes Training cycle randomize randomized distribution result problem solver advantage slow shifts nonetheless use steady shifts shifts 49 methods Tracking taking advantage predicted shifts important area future research distribution periodically improve average performance When problem solver distribution windows training problems approaches problems destroy retrain incremental 613 Low problem solving cost utility feasible strongest improve Extracting transformations This use COMPOSER solver One realworld domain lem scheduling antennas COMPOSERs modeling forced ciency Potential ways relaxing discussed 261 This application communications training statistics solving problems solved sufficiently limitation technique example average case exponential COMPOSER applied earth orbiting demonstrated necessity reducing It feasible examples low cost time problem prob satellites ground based learning cost complexity reasonable managing maintain effi solving reliance tractable initial problem assumptions additional helped dramatically innovations 62 Organizing search A key challenge successfully navigating posite modifications COMPOSERs search In ways restriction transformations In ways restriction mations considered step hillclimbing strong When restriction attempts vast space possible com ensure efficient interactions strong COMPOSER poor local maxima solution weak It says transfor It vital focus search promising follows generate test paradigm Performance tion intelligent intelligent generation lem solving COMPOSER improved making genera achieves measure EBL component This carefully analyzes prob possible PRODIGYs propose sound learning component trace search transformations inefficiencies alternatives J Gratch G DeJongArtcial Intelligence 88 1996 101142 131 address observed deficiencies Where applicable heuristics hypothesis help filter unpromising transformations like Etzionis nonrecursive Finally investigating apply notions help control improvement work bounded cost identifying rationality good reasoning cost transformations bias ensured imposing identifies improvements Currently COMPOSER utility efficiency search 13411 bandit problems 202 1421 adaptations These methods balance achieving high incremental explorationthe pled method relates incremental area 252832 characterizing utility function actual search behavior algorithm arises interaction like develop princi transformations directly utility cost achieve improvement For results biases We value investigating 43 Estimating incremental utility The search transformation transformations utility Typically utility alternative number examples estimated applying precise shape distribution incremental butional nature incremental general form unavailable statistical model While grows log required confidence tical concern Another chief limitation appropriateness initial sample size parameter adjusted Reliance parameters substantially investigation Chernoff bounds space relies ability accurately estimate This difficult distri training data form estimates cost significant prac tied includes particular domain statistical alternative like Chernoff bounds result area future work reducing accuracy estimates statistical model In case COMPOSER use weak method statistical models higher sample complexities An important obvious unfortunate alternative rules ultimately lie Nadas technique stopping required inefficiency entity identical independent COMPOSER treats relationship gathers statistics trans allow efficient use information As extreme algorithm sample complexity grows develop utility multiple One formation transformations example maintain log number transformations single statistical model derive transformations important incremental taken 4569 While possible transformations twice statistics necessary This area future research possible COMPOSER approach provided Sometimes strategy process For example Several statistical methods applied estimation resulting transformations better default quickly eliminated default control strategy We investigate 61 Similar strategies efficiency transformation significantly worse default control strategy However given COMPOSER compared promising inference appear 50581 extensions transformations improving transformation statistical eliminated currently improve 132 J Grcitch G DeJonArriJiciul lntellipvzce 88 1996 101142 distributional binary measures incremental Heuristics prior information like operutionality criteria Mitchell Keller KedarCabelli utility Unfortunately measures seen approximate measures difficulty capturing investigating example COMPOSER transformations based fewer examples application considered information However estimation replace augment statistical estimates Syntactic 541 syntactic nature incremental utility Instead use measures bias estimation process For confidence allowing estimates PRODIGY step steps Currently COMPOSER discards different control strategy step useful bias later control subsequent satisfy certain syntactic measures utility steps step conditional issue observed hillclimbing modified previous rules considered statistical In related hillclimbing information require gained 64 Gathering statistics The need efficient search estimation incremental incremental applications limitation utility values COMPOSER problem solving domains discussed utility statistics The default method limited gather obtain solve problems original intermediate expensive problems feasible dependence problem solving overcome mitigated For example article able advantage properties transformations examples efficiently implement resolved identifying important arises fact requires solving cases problem solvers While process vocabularies easier issue Perhaps means possibility utility estimates weaker cheaper obtain information For example Greiner Jurisica 351 propose method single solution transformations upper lower bounds utility novel search paths attempt maintaining Other authors suggested 53611 currently observing intractable problems learning estimate utility values basing transformation framework In general COMPOSER information possible hybrid statisticalanalytic simpler problems area future work gain useful evaluating incremental 125570 teacher An 7 Conclusion This article argued desirable possible automatically solving solving techniques Adaptive problem On hand general purpose application representation On demands individual tedious cycle manual experimentation satisfy means reconciling construct general problem adapt characteristics specific application needs techniques ease burden developing modular knowledge seemingly contradictory oft argued need declarative hand special purpose approaches best suited applications General approaches proven successful modification Adaptive techniques J Gratch C DeJongArtifcial Intelligence 88 19 101142 133 promise reduce burden modification process step reconciling conflicting needs generality efficiency In article developed formal characterization utility problem connects work adaptive problem solving rich field decision theory This fertile connection giving rise COMPOSER COMPOSER statistically rigorous algorithm built decision theoretic foundation It transforms general problem solving technique specialized application COMPOSER heuristic approach casting statistically sound framework able articulate assumptions underly technique predict consequences Most importantly assumptions stated explicitly subjected empirical investigations A larger theme learning algorithm strike balance maximiz ing performance efficiently COMPOSER embodies numerous commitments achieve efficient learning performance We argued effective learning com posed essentially basic roughly independent problems First problem searching space possible composite modifications Second issue obtaining estimates local properties transformations pattern task case estimating incremental utility atomic transformations Finally issue efficiently gathering information produce estimates By decomposing problem way possible consider approaches like COM POSER single algorithm collection methods tested individually It hope future research area proceed simply development large techniques like COMPOSER PRODIGY development smaller understood methods combined variety ways produce complete algorithm Acknowledgements Many thanks Steve Minton whos PRODIGY inspired work Russ Greiner contributed greatly technical discussions Adam Martinsek assisted numerous statistical issues Thanks Marsha Brofka Dan Oblinger Oren Etzioni anonymous reverers critical comments earlier versions work This research conducted primarily Beckman Institute University Illiois support National Science Foundation grants NSFIRI8719766 NSFIRI9209394 Appendix A Implementation tradeoffs Our motivation designing COMPOSER simply provide statistically sound learning technique provide practical tool A chief drawback COM POSERs statistical approach expensive This section discusses pragmatic issues techniques improving COMPOSERs performance tailoring specific characteristics application 134 J Grarch G DeJongArtificficrui Intellipxce RR I 9 101142 impediment The principal computational aging COMPOSERs consume particular application ways expense mitigated utility problem transformations like efficiently process example COMPOSERs expense identifying inferences When applying COMPOSER man To maximize performance approach good examples statistical perform 1 Tailoring Bound function 2 Tailoring discrepancy modeling 3 Tailoring methods describes This appendix gathering rationale function Q LY incremental utility statistics standard configuration alternative approaches This tailoring allows advantage domain specific knowledge improve POSER describes signer ciency COM application effi learning A1 Tailoring BowzdG TI 1 The Bound function defines error way overall probability 8 Here consider general definition Bound hillclimbing error error step hillclimbing cumulative step set 7 estimates search We look sources estimate acceptable error step 3 denote level incremental learning worse problem includes utility estimate solver current step sources search given choosing error steps hillclimbing individually We use LY denote error search Bound 6 TI To account A I 1 Error step On step investigating set T transformations Given error joint event cy expected equal 37 p 3631 That sum probabilities total error step 3 bounded LY step error greater situation member T negative estimates ITILY The upper bound follows Bonferronis probability event seen correct decision probability incremental greater I function size T The true relationship depends covariance distribution utility data points associated transformation inequality states cx clearly utility The incremental positive transformation However estimated larger number transformations cy given utility adopted Typically probability estimates error meaning transformation incorrectly generally unavailable adopt information incremental step precise choosing LY achieve step relationship It difficult impossible characterize size T p We considered error PiI methods Worst case cy PjT default best case cy pi A1 A2 J Gratch G DeJongArtcial Intelligence 88 1996 101142 135 In worst case error grows linearly size number transformations utility negatively incremental negative example transformation correlated This situation probably arise practice provide observed statistical error higher expected The strong guarantee size T bestcase model Eq A2 assumes grows We performed reasonable size T known generator advance allow transformation existing members small 30 The advantage assumption error grow appreciably empirical evaluations add new transformations T evaluating assumption T relatively showing Al2 Error steps Let pi denote final problem result decrease liberal policy COMPOSER solver significant worth step backwards chance adopting transformation negative ith step As number steps grows chance actually utility implement worstcase approach step 6 Ci pi 8 This guarantees requirement require examples different ways implement possible terms overall error parameter 8 incremental utility step expected utility However steps reduce improvement By default allows error 8 step rational steps forward The COMPOSER adopt incorrect COMPOSER error approach There number setting pi worstcase approach depending advance We considered limit probability steps known methods quickly satisfies Liberal bound pi S default worstcase boundlimited steps pi t worstcase boundunlimited steps pi 7 66 1279 A3 A4 A5 The default policy Eq A3 relies assumption magnitude utility incorrect steps comparable magnitude incremental steps reduce utility solver This assumption incremental utility correct steps tend improve initial problem simulation sacrificed simply divide steps unbounded final number steps total error sums 6 Bq AS requirement final result held useful efficiency advance number error step matter satisfies error evenly k steps Eq A4 When experiments rigor When number steps limited equations advance later The t2 Once model implementor application error steps chooses model unified error step function overall I2 This equation suggested Russell Greiner basis PALO algorithm 35 1 136 J Grutch G DeJonArticiul Intelligence 88 1996 101142 Pl d Fig A 1 Probability distribution normalized difference sample mean true mean value probability achieving distance greater original distribution Q n equal a2 Bound 6 ITI combines ith step hillclimbing choices error level estimate search A2 Tailoring Q cu difference incremental incremental expected discrepancy normalized true rzormalized difference d difference divided observed variance The function Qa models utility esti utility Here consider average finite sample n data utility observations This sample roughly representative actual fixed distribution However data points drawn randomly true mean distribution The Nadas sample mean true mated general definition Q n The estimate incremental distribution sample mean approximate stopping rule bounds mean The normalized sample mean The expected normalized difference modeled probability particular normalized difference distribution function shows likelihood Fig A1 The arises random sample Such distribution function difference The bell shaped curve shows function Q n called positive difference d probability cr2 equal It rarely possible situation Fortunately converges standard normal distribution This property statistics This fact implies weak conditions approximated motivation data points ensures accurate approximation We investigated methods improves sample size increases This large initial sample approximation called asserted Central Limit Theorem function Qn cu illustrated normalized th quantile distribution difference greater know precise distribution differences distribution given learning distribution differences normal distribution zero mean unit variance quantile standard normal distribution parameter Taking sufficiently 39 Section 531 The approximation likelihood observing defining Qn n increases observing satisfactorily r2 I3 The distribution positive variance finite mean J arch G DeJongArtijicial Intelligence 88 1996 101142 137 Standard normal Q n Y x 00 x ezdy z default J J n n 121 y2rjf2dy ercn2jc1 00 X 46 A7 T Qtn9aY x The COMPOSERs default It based standard normal distribution model The second based model called student t distribution This second model accurate high variance sample expensive compute For given learning situation function Q n cu chosen best model expected discrepancy given learning situation If exact model determined initial sample size unnecessary In general higher variance incremental utility values requires greater ensure approximation model close Smaller values 6 require precise modeling error better approximation Thus smaller requested error level greater ensure close approximation 6 If set small likely result higher requested statistical error If large excessive number examples required perform statistical inference The general experience statistical community normal approximation good initial samples We recommend value A3 Gathering statistics The largest cost COMPOSER tends cost obtain utility observations Given current problem solver PS set transformations T COMPOSER obtain utility observations transformation large sample problems There techniques significantly reduce cost depending characteristic application These techniques reduce cost ways reducing number utility values necessary observe second reducing cost obtaining utility value A sampling technique known blocking reduce number utility values nec essary statistical decisions 458 As shown theoretical analysis number examples needed statistical inferences grows variance incremental utility values Blocking works minimizing variance To understand blocking consider problem finding highest yielding variety wheat Wheat yield effected factors variety wheat factor We influences nuisance factors weather conditions year crop grown Often nuisance factors greatest influence washing contribution factor increasing variance data A standard solution called randomized block design combine data identical values nuisance factors single block consider differences observations block computing utility values For wheat example block corresponds plot land location Block design suggests having plots land different wheat plot One considers plot land averages differences locations planting variety yield varieties difference different plots problem easy hard dif In COMPOSER nuisance factors specific characteristics small changes problems influences incremental applications block dominant transformation compute overwhelm transformations transformations current problem inference These problem intended behavior possible task distributionsome utility transformation procedure lead significant utility values derived subtracting differences choice transformation block observe Incremental turn block When reduction drawn ferences likely The solution adopted problem problem default strategy problem number examples needed statistical dominate relatively transformed problem blocking different problem possible benefit problem We utility influences tend generally solver solver perform similarly similar problems The alternative utility utility value derived strategy limited effect transformations cost obtaining utility data The simple solve given problem candidate prob strategy recommend lem solvers The complexity bruteforce processing tied complexity ITI problem solvers In learning situations bruteforce processing prove expensive For example candidate problem solvers requires explicit exper intractable Furthermore imentation alternative problem solvers desirable solver As gathering learn passively statistics employ information reduce normal operations problem principal expense In situations repeatedly intrusiveit In learning solve identical problem Blocking There techniques bruteforce method problem differences necessary small relative COMPOSERs incremental important reducing perform situations cost The cost obtaining utility observations dramatically reduced solving detailed cost model mations actually simply draw random effect different possible different problem PRODIGY Sometimes Greiner Jurisica conflict COMPOSERs transformations problem transfor efficiently derive ramification proposed 3668 With model example use model Such models rarely available short determine training transformations extract necessary determine solver Section 4 describes unobtrusive solely observing possible 35 propose method statistics effectiveness normal operations current worked way implementation information partial information extract partial assumptions incorporated Appendix B BINWORLD domain The BINWORLD PRODIGYEBLs domain introduced utility analysis 30 highlight deficiencies Etzionis nonrecursive hypothesis The J Gratch G DeJangArtcial Intelligence 88 1996 101142 139 robot assembly domain set components All components bin free defects examined suitable achieving represented particular stored task goal construct composite true bin If components assembled Otherwise bin given bin state partassembled operator determines acceptability The INSPECTBIN assembly The ASSEMBLECOMPONENTS operator constructs BI Domain theory B2 Problem distribution Effects add defectfreecomponentsx according Fortynine A problem distribution problem problem distribution The experiment defined enumerating class A set problems set problem classes assign constructing created randomly ing probabilities based uniform distri problems class equal chance classes This means bution solving attempt The class contains problems participating bins components bins contain defective component One bin contains defects The bins ordered defectfree bin The second class defect contains problems bins components free The bin contains ordered defective component The rational variance PRODIGYEBL utility bimodal distribution example results learned control utility estimates single example Estimating single distribution high variance utility true incremental The bins ordered defectfree bin defective component The components construct distribution high problem distribution inaccurate One bin representation bases rule References ranking means normal populations RE Bechhofer A singlesample multiple decision procedure known variances Ann Mafh Stat 25 I 1954 JO Berger StatisticaL Decision Theory Bayesian Analysis Springer Berlin 1980 A Borgida DW Etherington Hierarchical knowledge bases efficient disjunctive Proceedings First International Conference Principles Knowledge Representation Reasoning Toronto Ont 1989 3343 H Bthinger H Martin KH Scriever Nonparametric Seyuenticrf Selection Procedures Birkhauser Boston 1980 P Cheeseman B Kanefsky IJCAI89 Sidney WM Taylor Where hard problems Proceedings reasoning 1633169 1989 140 J Gratch G DeJonArtifclal Intelligence 88 1996 101142 16 1 SA Chien JM Gratch MC Burl On efficient allocation resources machine learning statistical approach 171 T Dean M Boddy An analysis timedependent hypothesis evaluation IEEE Trans Pattern Anal Mach Intell 17 1995 652665 Proceedings AAAI88 St Paul planning MN 1988 4954 81 TL Dean MI Wellman Planning Control Morgan Kaufmann San Mateo CA 1991 19 R Dechter Constraint networks SC Shapiro ed Encyclopedia Artcial Intelligence Wiley New York 1992 10 1 R Dechter J Pearl Networkbased heuristics constraintsatisfaction problems Arf Intell 34 1987 l38 1 1 I I MH DeGroot Optimal Statistical Decisions McGrawHill New York 1970 I I2 I GE DeJong RJ Mooney Explanationbased learning alternative view Mach Learn 1 1986 145176 13 I J Doyle Rationality roles reasoning extended version Proceedings AAAI90 Boston MA 1990 1093I 100 I 14 1 0 Etzioni A structural theory search control PhD thesis Department Computer Science Carnegie Mellon University Pittsburgh PA 1990 I IS I 0 Etzioni Why ProdigyEBL works 16 1 0 Etzioni STATIC problemspace 1991 s33540 Proceedings AAAI90 Boston MA 1990 916922 compiler Proceedings AAAI9I Anaheim CA PRODIGY I 171 0 Etzioni S Minton Why EBL produces overlyspecific knowledge Proceedings Ninth International Conference Machine Learning Aberdeen critique PRODIGY 1992 approaches 137143 I8 RE Fikes NJ Nilsson STRIPS new approach application theorem proving problem solving Art Intell 2 197 I 189208 19 I M Fisher The Lagrangian relaxation method solving integer programming problems Manage Sci 27 1981 l18 1201 PWL Fong A quantitative study hypothesis selection Proceedinp Twelfth International Conference Machine Learning Tahoe City CA 1995 226234 2 1 1 JC Gittins MultiArmed Bandit Allocation Indices Wiley New York 1989 1221 A Goldberg CA Brown Average PW Purdom time analysis simplified DavisPutnam Inform Process Lett 15 1982 7279 procedures I 231 Z Govindarajulu 1241 JM Gratch COMPOSER UIUCDCSR931806 The Sequential Statistical Analysis American Sciences Press Columbus OH 198 1 solving Tech Rept adaptive problem decisiontheoretic approach Department Computer Science University Illinois Urbana IL 1993 1251 J Gratch On efficient approaches utility problem adaptive problem solving PhD thesis Department Computer Science University Illinois UrbanaCampaign Urbana IL 1995 261 J Gratch S Chien Learning scheduling Proceedings Tenth International Conference Machine Learning Amherst MA 1993 deep space network improve schedule quality search control knowledge search control knowledge I27 I J Gratch S Chien G DeJong Learning problem Proceedings IJCA193 Schedulirq Workshop 1993 I281 J Gratch S Chien G DeJong Improving learning performance rational resource allocation Proceedings AAAI94 Seattle WA 1994 I29 1 J Gntch G DeJong A hybrid approach guaranteed effective control strategies Proceedings Eighth International Workshop Machine Learning Evanston IL I99 I 1301 J Gratch G DeJong COMPOSER solution Proceedings AAAI92 San Jose CA 1992 235240 G DeJong A framework probabilistic learning 131 I J Gntch simplifications International Conference Artcial Intelligence Planning Systems College Park MD 1992 7887 learning action principled IL 801 Department Computer Science University Illinois Urbana G DeJong Rational Proceediqs balancing learning approach learning plan First 1321 J Gntch utility problem speedup Tech Rept UIUCDCSR93I 1993 133 I R Greiner WW Cohen Probabilistic hillclimbing Proceedings Computational Learning Theory Natural Learning Systems 1992 J Gratch G DeJongArtificial Intelligence 88 1996 101142 141 34 R Greiner C Elkan Measuring improving 199 1 AAAI91 Sidney effectiveness representations Proceedings 35 R Greiner I Jurisica A statistical approach solving EBL utility problem Proceedings AAAI92 San Jose CA 1992 241248 361 R Greiner J Likuski Incotporating redundant learned rules preliminary formal analysis EBL Proceedings IJCAI89 Detroit MI 1989 744749 1371 Y Hochberg 38 RV Hogg AT Craig Introduction 39 RV Hogg EA Tanis Probability 40 LB Holder Empirical AC Tamhane Multiple Comparison Procedures Statistics Mathematical Wiley New York 1987 Macmillan New York 1978 Inference analysis general utility problem Statistical Macmillan New York 1983 machine learning Proceedings AAAI92 San Jose CA 1992 249254 I41 1 EJ Horvitz GE Cooper DE Heckerman Reflection action scarce resources IJCAI89 Detroit MI 1989 1121l 127 study Embedded Systems MIT Press Cambridge MA 1993 principles empirical 1421 LP KaeIbIing Learning 43 C Knoblock Learning hierarchies abstraction Proceedings spaces Proceedings Sixth international Conference theoretical Machine Learning Ithaca NY 1989 241245 441 RE Korf Planning search quantitative 145 P Laird Dynamic optimization approach A Intell 33 1987 6588 Proceedings Ninth International Conference Machine Learning Aberdeen 1992 263272 461 JE Laird PS Rosenbloom A Newell llniversal Subgoaling Chunking The Automatic Generation Learning Goal Hierarchies Kluwer Academic Publishers Hingham MA 1986 I47 S Letovsky Operationality criteria recursive predicates Proceedings AAAI90 Boston MA 1990 936941 1481 NJ Lewins Practical solutioncaching PROLOG explanationbased learning approach PhD thesis Department Computer Science University Western Australia 1491 N Littlestone 212261 MK Warmuth The weighted majority algorithm 1993 Inform Compur 108 1994 SO 0 Maron AW Moore Hoeffding races accelerating model selection search classification function approximation Los Altos CA 1994 Advances Neural Irrmation Processing Systems 6 Morgan Kaufmann 5 I K Melhom Data Structures Algorithms 1521 DP Miller RS Desai E Gat R Ivlev J Loch Reactive navigation Proceedings AAAI92 San Jose CA 1992 823828 Learning Search Control Knowledge An ExplanationBasedApproach I Sorring Searching I 531 S Minton experimental results Springer Berlin 1984 rough terrain Kluwer Academic Publishers Norwell MA 1988 S4 TM Mitchell R Keller S KedarCabelli Explanationbased generalization unifying view Mach Learn 1 1986 4780 S51 TM Mitchell S Mahadevan Proceedings LI Steinberg LEAP IJCAI85 Los Angeles CA 1985 573580 learning apprentice VLSI design 561 D Mitchell B Selman H Levesque Hard easy distributions SAT problems Proceedings AAAI92 San Jose CA 1992 459465 1571 TM Mitchell RE Utgoff R Banerji Learning experimentation acquiring refining problem J Carbonell T Mitchell eds Machine Learning An Artificial solving heuristics R Michalski Intelligence Approach Morgan Kaufman San Mateo CA 1983 581 AW Moore MS Lee Efficient algorithms minimizing cross validation error Proceedings Eleventh International Conference Machine Learning New Brunswick NJ 1994 591 J Mostow Mechanical transformation Department Computer Science CMU Pittsburgh PA I98 I task heuristics operational procedures PhD thesis 1601 A Nadas An extension theorem Chow Robbins sequential confidence intervals mean Ann Math Stat 40 1969 667671 exercises 16 I BK Natarajan On learning Proceedings Second Annua Workshop Computational Learning Theory Santa Cruz CA 1989 7287 621 MA Perez 0 Etzioni DYNAMIC new rule training problems EBL Proceedings Ninth International Conference Machine Leurning Aberdeen 1992 367372 142 I Grclch DeJorArifrlttr Irrellerc XX I 996 101142 I63 1 B Roy Problems methods multiple objective I64 1 S Russell E Wefald Principles metareasoning cm Principles cf Knmvfrde Reprsertrfion md Keasonin functtons Motl Prordrrzin 1 2 Prrerliqs First Toronto Ont 1989 4004 Internntioncd I I_ 197 I Confhrence 165 1 AL Samuel Some studies 1661 M Schoppers Building machine learning game checkers fBM J 3 3 1959 plans monitor exploit openloop closedloop dynamics Prowedings Iirs nfenwrtiotlt mfrence m Articd Inteilipnce PkmninR Systems College Park MD 1992 204213 1671 UM Schwuttke L Ciasser Realtime metareasoning dynamic tradeoff evaluation Proceedinq AAAI92 San Jose CA C 1992 500506 1681 D Subramanian R Feldman The utility EBL recursive domain theories Proceedings AAAI90 Boston MA 1990 942949 I69 I 0 Subramanian S Hunter Measuring utility design provably good EBL algorithms PrweedinK7 Nirlth nrermrkmtrl Ccmfrerue OH Mtrchine Lecuxin Aberdeen 1992 426435 Learning Lecwninq Evanston 1 70 1 P Tadepalli Mtrchine 171 Ml Wellman Pncrrdis MD 1992 236242 Firrt inscrutable theories IL I99 I 544548 utility Procwdins Eith fnrernarioncd WorWmp CI representation decisiontheoretic planning J Doyle Modular nfernotiomrl Cimferrwu OH Arifirl htelliprwe Pkmnin S_wms College Park