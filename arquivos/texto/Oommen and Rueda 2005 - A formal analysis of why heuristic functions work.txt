Artiﬁcial Intelligence 164 2005 122 wwwelseviercomlocateartint A formal analysis heuristic functions work B John Oommen a1 Luis G Rueda b2 Senior Member IEEE School Computer Science Carleton University 1125 Colonel By Dr Ottawa ON K1S 5B6 Canada b School Computer Science University Windsor 401 Sunset Ave Windsor ON N9B 3P4 Canada Received 10 May 2001 Abstract Many optimization problems science proven NPhard likely polynomialtime algorithms solve problems exist P NP Alternatively solved heuristics algorithms provide suboptimal solution hopefully arbitrarily close optimal Such problems wide range applications cluding artiﬁcial intelligence game theory graph partitioning database query optimization Consider heuristic algorithm A Suppose A invoke possible heuristic func tions The question determining heuristic function superior typically demanded yesno answerone substantiated empirical evidence In paper Pat tern Classiﬁcation Techniques PCT propose formal rigorous theoretical model provides stochastic answer problem We prove given heuristic algorithm A utilize heuristic functions H1 H2 ﬁnd solution particular problem accuracy evaluating cost optimal solution H1 greater accuracy evaluating cost H2 H1 higher probability H2 leading optimal solu tion This unproven conjecture basis designing numerous algorithms A algorithm variants Apart formally proving result address correspond ing database query optimization problem open decades To validate proofs report empirical results database query optimization techniques involving wellknown histogram estimation methods 2005 Elsevier BV All rights reserved Corresponding author Email addresses oommenscscarletonca BJ Oommen lruedascscarletonca LG Rueda 1 Partially supported NSERC Natural Science Engineering Research Council Canada Fellow IEEE 2 This work partially supported Departamento Informática Universidad Nacional San Juan Argentina NSERC Member IEEE 00043702 matter 2005 Elsevier BV All rights reserved doi101016jartint200202001 2 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 Keywords A algorithms Heuristic algorithms Pattern recognition Optimization 1 Introduction 11 Overview The area science open unsolved problems In paper concerned problems heuristics solve optimization problems Any arbitrary optimization problem3 typically deﬁned terms instances drawn ﬁnite set X objective function feasibility functions The aim ﬁnd hopefully unique instance X leads maximum minimum value objective function subject feasibility constraints A formal deﬁnition optimization problem 10 But speciﬁc sider wellknown Traveling Salesman Problem TSP cities numbered 1 n salesman starts city 1 visits city returns city 1 An instance X permutation cities example 1 4 3 2 5 considering world consisting ﬁve cities The objective function instance f 1 4 3 2 5 obtained performing summation intercity distances 1 4 4 3 3 2 2 5 5 1 The optimal solution instance minimizes value f A heuristic algorithm algorithm attempts ﬁnd certain instance X maximizes f proﬁt iteratively invoking heuristic function The instance maximizes f optimal solution4 optimization problem A heuristic method performs modiﬁcations given solution instance order obtain different solution superior leads superior solution The heuristic turn invokes heuristic function estimates measures cost solution particular state search process This context use terms Many heuristic algorithms heuristic functions reported literature include alphabeta search 11 backtracking hillclimbing 10 sim ulated annealing 1 genetic algorithms 13 tabu search 7 learning automata 15 The issue heuristic functions heuristic algorithms searching game playing 1624 enormous ﬁeld study This question addressed To clarify issues let consider classical npuzzle problem 16 This problem sists square board containing n square tiles position called blank The aim rearrange tiles predeﬁned usually random initial conﬁgura tion predetermined goal conﬁguration sliding tile adjacent blank 3 Every optimization problem formulated decision problem 6 4 We use term solution refer element x X term proﬁt refer value f x In minimization problems f cost function BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 3 blank position A heuristic algorithm solves problem examining heuris tic function possible valid movements Viewed perspective underlying state graph possible states encountered level form children nodes current node search structure Other variants heuristic algorithms involve examination lower levels The breadthﬁrst search depthﬁrst search schemes examples heuristic algorithms useful problem solving strategy An example heuristic function measurement estimate number tiles place Another measure sum depth node number tiles place One betterknown solutions npuzzle problem A algorithm This algorithm graph search algorithm ﬁnd path minimum cost nodes start node goal node The A maintains tree stores paths explored Using paths measure f potential advantage choosing path calculated The value f cost traversing graph nodes calculated different heuristic functions A heuristic said admissible A converges correct result heuristic function upper bound true cost nodes goal node In general arbitrary problem question useful heuristic function determining cost traversing node known analytic solutionit traditionally empirically analyzed In paper present formal analysis provides stochastically positive answer question comparing relative advantages potential heuristic functions The A algorithm variants like A algorithm success fully applied problems object recognition deformable templates 162628 Various solutions optimization problems different heuristic functions 28 shall use paper 28 highlight difference heuristic algorithms effect algorithm potential heuristic functions The authors 28 address problem tracking roads satellite images twentyquestion search paradigm A algorithm cousin A algorithm Using algorithms roads represented terms straightline segments The paths expanded application ensemble heuristic functions One heuristic function based conditional entropy mea surements branches choose promising path While paper discusses heuristic functions question compare solutions obtained heuristic functions achieved comparing em pirical simulation results We hope formal analysis tool achieve rigorous comparison heuristic functions 28 similar scenarios5 The tools propose use drawn wellestablished theory Pattern Recognition PR 527a prominent ﬁeld machine intelligence Broadly speaking PR involves decisionmaking based priori learned knowledge classes ob jects recognized More speciﬁcally learns information features 5 The model presented limitations investigating quality solutions yielded Alike algorithm These limitations discussed later subsection 4 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 set classes Subsequently given object unknown identity informa tion attempts recognize unknown object belonging known classes arbitrary accuracy Necessarily overview PR brief There applications PR including face speech recognition ﬁngerprint identiﬁcation character recognition medical diagnosis In applications information classes structural statistical In deal ﬁeld structural syntactic pattern recognition ﬁeld statistical pattern recognition Furthermore statistical information features classes represented random vectors The procedure obtaining features consists mapping feature values sample vector Feature values example width height ﬁgure value pixel image Statistical pattern recognition subdivided welldeﬁned ap proaches parametric nonparametric In random vectors known probability distribution normal Gaussian exponential multinomial No model assumed nonparametric case Although aware use PR principles real life scenarios aware previous results PR principles solve theoretical unsolved problem completely different ﬁeld Our result crystallized follows Given heuristic functions question determining superior typically demanded yesno answer substantiated based empirical evidence We solved problem deciding superior heuristic function PR techniques It mentioned numerous wellknown techniques utilized context pattern classiﬁcation hypothesis testing bootstrap methods NeymanPearson methods A good reference methods 21 However results derived paper essentially use methods traditionally applied optimal Bayesian Classiﬁcation described statistical pattern recognition literature 4 Using principles prove following assertion Given heuristic functions H1 H2 heuristic algorithm ﬁnding solution particular problem accuracy obtaining optimal solution H1 greater H2 H1 higher probability leading optimal solution H2 To best knowledge open problem However unproven conjecture basis designing numerous algorithms A algorithm variants searching game playing numerous applications 16242528 Our strategy achieving analysis follows The ﬁrst task model cost solution Since optimal true cost unknown represent terms estimate estimated heuristic function Observe inaccurate cost represented terms random variable Note cost mean cost search process involved determining optimal solution cost optimal solution estimated heuristic function This difference crucial Now modelling heuristic function place question quantifying quality heuristic function considered Informally speaking paper concerns heuristicfunction quality assessment problem addressed turn viewing pattern recognition problem We solve pat BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 5 tern recognition problem considering independent random variables ﬁrst optimal solution second suboptimal pursued heuristic function H1 We use reasonable model accuracy heuristic func tion error H1 doublyexponential random variable6 This distribution shall presently approximate Gaussian distribution typ ically reliability failure models reasonable scenario In model accuracy heuristic function related variance random variable represent The analysis Gaussian distribution follows If consider heuristic function H2 variance greater H1 mean H1 model efﬁciency heuristic functions compared Indeed model theoretically proven H1 likely succeed obtaining optimal solution H2 For model proved uniqueness result conditions heuristic functions lead coincident probabilities success The doubly exponential distribution actually meant approximation Gaussian distribution typically model errors However algebraic analysis Gaussian distributions impossible closedform expression integrating probability density function Consequently extended analysis doubly exponential distribution formulate reasonable analysis Gaussian distribution numerical integration By means analysis corroborated validity hypothesis Gaussian distributions We provide empirical results histogramlike estimation methods database query optimization demonstrate validity theoretical analysis 12 Applications There heuristic algorithms solve wide variety NPhard problems Such problems wide range applications spanning spectrum artiﬁcial intelligence include game playing game theory graph ory database query optimization networking computational geometry number theoretic problems parallel processing The results presented paper applicable heuristic algorithm uses different heuristic functions solve particular problem In introductory section In area database query optimization tables joined intermediate join operations performed ultimately obtain ﬁnal relation As sult query performed means different intermediate join operations A simple sequence join operations leads ﬁnal result called query evaluation plan QEP Each QEP associated internal cost depends number operations performed intermediate joins The problem choosing best QEP combinatorially explosive optimization problem This problem currently 6 The reasoning paper assumes errors true value However believe distribution onesided similar arguments true long distribution heavilytailed We grateful anonymous referee brought attention 6 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 solved estimating query result sizes intermediate relations selecting efﬁcient QEP Since analysis selecting best QEP real time pos sible inspect real data phase Consequently query result sizes usually estimated statistical information structures data maintained database catalogue This information approximate distribution attribute values particular relation Hence problem selecting best QEP depends distribution approximated In 8 shown errors query result size estimates increase exponen tially number joins Since current databases associated queries increase complexity numerous efforts devise efﬁcient techniques solve query optimization problem Many techniques proposed estimate query result sizes including tograms sampling parametric techniques 9121422 Histograms com monly form statistical information They incorporated commer cial database systems Oracle Microsoft SQL Server Teradata DB2 mainly use Equidepth histogram The prominent models histograms known literature Equiwidth 29 Equidepth 1422 Rectangular Attribute Cardinal ity Map RACM 18 Trapezoidal Attribute Cardinality Map TACM 19 VOptimal Histograms 823 In scenario heuristic algorithm actual algorithm uses histogram heuristic function obtains optimal suboptimal QEP The heuristic function algorithm actual histogram approximates distribution attribute values relevant tables Thus model terminology Equiwidth Equidepth RACM TACM heuristic functions Other areas model answer open questions ﬁelds game theory game playing 25 In game playing widely structure analyze best possible strategy game tree root node represents initial status board All possible moves ﬁrst player edges root ﬁrst level edges child represent possible moves second player opponent Continuing fashion game played plans executed players wins The aim optimize moves ﬁrst player based searching branches tree leaves perform best based maximizing reward ﬁrst player minimizing second There techniques optimize moves ﬁrst player One minimax search algorithm searches ﬁxed number levels entire tree ﬁnds best moves node This exhaustive search procedure complexity grows exponentially number nodes tree A efﬁcient mechanism alphabeta search algorithm 11 heuristic signiﬁcantly reduces number nodes explored Both assume heuristic function use typically evaluates position board viewed perspective ﬁrst player advantageous determining superior strategy This question address paper The model presented paper important consequences choosing heuristic function Such heuristic function example cost path current state goal state unfortunately exactly known BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 7 estimated The search scheme alphabeta search minimax search algorithm uses heuristic function search hopefully optimal path game tree Another application result graph theory example solving uniform graph partitioning problem Given complete graph 2n vertices G V E cost function f E Z 0 aim ﬁnd partition sum costs individual subsets minimized This problem known NPhard applications especially VLSI design hydrology networks Many heuristic algorithms proposed solve problem including simulated annealing ge netic algorithms learning automata 1020 When considering particular heuristic algorithm incorporate different heuristic functions approximate sum costs individual subsets particular partitioning It intuitive accurate heuristic function likely succeed ﬁnding optimal solution However happens cases We provide stochastic answer question By means rigorous theoretical analysis prove particular heuristic function provides accurate approximations sum costs individual subsets likely obtain minimal cost partitioning accurate heuristic function 13 Problem statement In paper propose theoretical model solves fundamental open problem science relating heuristic functions solution optimality principles theory pattern classiﬁcation This problem knowledge open In particular corresponding database query optimization problem unsolved decades More speciﬁcally prove following Given heuristic algorithm A invokes heuristic functions H1 H2 decision problem accuracy approx imating optimal solution H1 greater H2 H1 higher probability leading optimal solution H2 The importance results paper answer accu racyoptimality question stochastically positive In words prove superior heuristic function yield better solution probability superior heuristic function yields optimal solution exceeds probability ferior heuristic function yields optimal solution This paper justiﬁes gives formal rigorous basis heuristic functions work We analytically prove wellacclaimed models inaccuracy better accuracy heuristic function greater probability choosing optimal solution We provided empirical results related ﬁeld database query optimization These results superiority RACM traditional histogram estimation methods Equiwidth Equidepth The empirical results obtained testing properties histogram methods random databases RACM signiﬁcantly superior Equiwidth Equidepth schemes 8 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 14 Restrictions model As mentioned paper addresses problem quantifying quality heuristic function achieves posing problem fairly general framework However results applicable particular application domain7 uses speciﬁc search strategy A algorithm logistics search process considered Informally speaking main result paper proves following Given heuristic functions evaluating cost search mechanism utilizing func tions converge higher probability superior solution utilizes function lesser variance However comparing performance heuristic func tions search process initiated A complicated issue The reason argued follows In iteration A computes values heuristic function f candidate nodes OPEN list represent promising A selects highest value f generates children computes values f inserts OPEN list For algorithm like A claim expedient use heuristic function better estimates cost estimates poorly The question nodes OPEN list lead solutions problemdependent question answer We intend study problem database query optimization domain mentioned later incorporating search strategy search set QEPs costs estimated histogram methods Note invalidate queryoptimization results presented paper simulations exhaustively search QEP space intelligent search strategy like A 2 Heuristic function accuracy vs optimality Consider heuristic algorithm A invokes heuristic functions H1 H2 The probability correctly estimating cost value particular solution H1 estimating cost value H2 represented independent random variables In model assume heuristic functions independent value obtained heuristic function affect value obtained second For analysis work models error function doubly exponential distribution normal distribution In probability ob taining value deviates mean true value falls exponentially function deviation The exponential distribution typical reliability analysis failure models particular domain question evaluating reli able quality solution estimate performance available More importantly approximation Gaussian distribution reasons clariﬁed momentarily The Gaussian model difﬁcult analyze 7 We grateful anonymous referee brought limitation attention BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 9 closedform algebraic expression integrating probability density func tion However formal computational proof included conﬁrms hypothesis 21 Analysis exponential distributions A random variable X said doubly exponentially distributed parameter λ density function given fXx 1 2 λe λxc x 1 If X doubly exponential random variable elementary integration straight forward algebraic steps shown EX c VarX 2 λ2 2 3 Without loss generality mean cost optimal solution c1 shifting origin c1 work assumption cost best solution 0 mean random variables The cost second best solution given random variables H1 H2 mean c2 0 variables An example help clarify Example 1 Suppose H1 leads optimal cost probability represented doubly exponential random variable Xopt mean 0 λ1 04 This heuristic function leads suboptimal cost according Xsubopt mean 8 λ1 04 1 1 H2 heuristic function optimal cost chosen prob parameters c1 0 λ2 02 It leads ability distribution given Xopt second suboptimal cost value probability density given Xsubopt parameters c2 8 λ2 02 The fact 2λ2 2 signiﬁes probability H1 lead sub optimal cost smaller probability H2 leading suboptimal cost This scenario depicted Fig 1 formalized presently 1 2λ2 2 2 The result depicted formalized following theorem ﬁrst pri mary result paper answers open question referred The theorem formulated terms probabilities heuristic functions lead wrong decision inherently related probability heuristic func tions lead convergence suboptimal solutions The formulation result proof utilize techniques typically foreign database theory game theory artiﬁcial intelligence matter science area approach applied They belong theory PR The second theorem extends results ﬁrst shows results geometrically interpreted 10 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 Fig 1 An example doubly exponential distributions random variables Xopt Xsubopt parameters λ1 04 λ2 02 1 2 Xopt 2 Xsubopt 1 Theorem 1 Suppose A heuristic algorithm potentially utilize heuristic functions H1 H2 Let X1 X2 doubly exponential random variables represent estimated Xcid6 2 doubly exponential random variables representing esti costs optimal solutions obtained H1 H2 respectively 1 Xcid6 mated costs nonoptimal solutions obtained H1 H2 respectively c 0 EX1 EX2 cid1 EXcid6 1 p1 p2 probabilities H1 H2 respectively lead wrong decision EXcid6 2 Then VarX1 VarX cid6 1 2 λ2 1 cid1 2 λ2 2 VarX2 VarX cid6 2 p1 cid1 p2 Proof Consider particular cost x The probability x leads wrong decision A uses H1 incorrectly classifying x obtained nonoptimal solution This error classiﬁcation area curve pdf function Xcid6 1 cumulative probability x pdf H1 refers suboptimal solution Because discontinuity doubly exponential function c area decomposed following integrals xcid1 I11 ccid1 I12 1 2 1 2 λ1eλ1uc du x cid1 c λ1eλ1uc du xcid1 c 1 2 λ1uc du x c λ1e 4 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 11 Solving integrals 4 results I11 1 2 I12 lim u eλ1xc lim u λ1uc 1 2 1 2 1 2 e e λ1uc 1 2 λ1xc e 1 2 e λ1xc 1 2 1 1 2 λ1xc 5 e The probability H1 leads wrong decision values x following function λ1 c 0cid1 p1 I λ1 c I11 1 2 λ1eλ1x dx ccid1 0 I11 1 2 λ1x dx λ1e cid1 c I12 1 2 λ1x dx λ1e 6 applying distributive law substituting values I11 I12 written 0cid1 λ1 4 e2λ1xλ1c dx ccid1 0 λ1 4 λ1c dx e cid2 cid1 c λ1 2 e λ1x λ1 4 e cid3 2λ1xλ1c dx 7 After solving integrals 7 transformed λ1c 1 λ1c 3 4 8 λ1c 1 2 Similarly analysis p2 function λ2 c λ1c 1 4 λ1c λ1ce λ1ce 1 8 e e e p2 I λ2 c 1 2 e λ2c 1 4 λ2ce λ2c We prove 8 9 e p1 1 2 λ1c 1 4 λ2c 1 4 Multiplying sides 2 substituting λ1c α1 λ2c α2 10 λ1c cid1 1 2 λ2c p2 λ2ce λ1ce 10 e written follows α1 1 2 e α2 1 2 Substituting α2 kα1 α1 cid2 0 0 k cid1 1 11 results α1 cid1 e α2 α2e α1e q1 e α1 1 2 α1e α1 cid1 e kα1 1 2 kα1e kα1 q2 11 12 We prove q1 q2 cid1 0 After applying natural logarithm sides 12 algebraic manipulations q1 q2 cid1 0 implies 1 1 2 F α1 k kα1 α1 ln 1 1 2 ln α1 cid5 cid4 cid4 cid5 kα1 cid1 0 13 To prove F α1 k cid1 0 use fact ln x cid1 x 1 Hence 12 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 F α1 k α1k 1 ln cid5 cid4 1 1 1 1 2 α1 2 kα1 1 1 1 1 1 2 α1 cid1 α1k 1 2 kα1 α1k 1 α1 kα1 2 kα1 α1 kα2 1 kα1 k2α2 1 2 kα1 α1k 1kα1 1 2 kα1 cid1 0 14 15 16 17 18 0 k cid1 1 α1 cid2 0 α1k 1 cid1 0 kα1 1 0 Hence α1k 1kα1 1 cid1 0 ii 0 k cid1 1 α1 cid2 0 0 kα1 cid1 α1 kα1 2 2 0 Hence theorem cid1 The theorem viewed sufﬁciency result In words shown q1 q2 cid1 0 p1 cid1 p2 We necessity result stated uniqueness result This result states function p1 cid1 p2 equality ONLY boundary condition distributions exactly identical To prove necessity result consider q2 q1 derived 12 written function α1 k Gα1 k e kα1 1 2 kα1e kα1 e α1 1 2 α1 α1e 19 By examining partial derivatives shall solutions equal ity Furthermore α1 cid2 0 0 k cid1 1 shall given k solution α1 0 k 0 k cid1 1 proving uniqueness Theorem 2 Suppose α1 cid2 0 0 k cid1 1 Let Gα1 k Gα1 k e kα1 1 2 kα1e kα1 e α1 1 2 α1 α1e 20 Then Gα1 k cid2 0 exactly solutions Gα1 k 0 α1 1 k 1 α1 0 k Proof We prove deﬁned theorem statement Gα1 k cid2 0 We shall prove satisﬁed determining local minima G α1 cid2 0 0 k cid1 1 We ﬁrst ﬁnd partial derivatives 19 respect α1 k G α1 1 2 ke kα1 1 2 k2α1e kα1 1 2 e α1 1 2 α1e α1 0 21 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 G k 1 2 α1e kα1 1 2 kα2 1e kα1 0 We solve 21 22 α1 k Eq 22 written follows 1 2 α1e kα1 1 2 kα2 1e kα1 13 22 23 canceling terms results kα2 1 α1 1 obtain k α1 0 Substituting α1 1 α1 0 Solving equation α1 k 21 canceling terms 1 2 e α1 1 2 α1e α1 0 24 results solution α1 1 consequently k 1 The second root α1 0 indicates minimum achieved value k We solutions 21 22 α1 0 k α1 1 k 1 Since α1 cid2 0 means α1 value 0 local minima α1 0 k Substituting values G Gα1 k 0 minimum Therefore Gα1 k cid2 0 α1 cid2 0 0 k cid1 1 Hence theorem cid1 To physical perspective results let analyze geometric relation function G heuristic functions G positive function region α1 cid2 0 0 k cid1 1 When α1 0 G 0 This means small values α1 G small Since α1 λ1c value α1 depends λ1 c When c small G close minimum 0 probabilities p1 p2 close This behavior noticed Fig 2 phenomenon observed heuristic functions comparable equally efﬁcient In terms histogram methods database query optimization c small optimal suboptimal QEP close Since histogram methods Equi Fig 2 Function Gα1 k plotted ranges 0 cid1 α1 cid1 1 0 cid1 k cid1 1 14 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 width Equidepth produce larger error RACM TACM likely ﬁnd optimal QEP Interpreted alternatively G small λ1 close 0 This means VarX1 large Since VarX1 cid1 VarX2 VarX2 large close Fig 1 observe ﬂat curves distributions Random variables histogram methods Equiwidth Equidepth yield similar error es timation distributions large similar variances Hence probabilities p1 p2 close consequently similar results expected estimation meth ods However heuristic functions yield widely different estimated costs case new histogram methods RACM TACM compared traditional methods effectively imply random variables smaller variances compared random variables larger variances In case value G highimplying yield superior solutions 22 Analysis considering normal distributions For analysis section consider given heuristic func tions H1 H2 probabilities choosing optimal suboptimal solutions represented normally distributed random variables X1 X2 means 1 σ 2 µ1 µ2 variances σ 2 Although model normal distributions realistic real life problems analysis impossible closedform algebraic expression integrating normal probability density function Alternatively numerical integration obtained representative values implication efﬁciency optimality corroborated 2 respectively Without loss generality mean cost optimal solution µ1 shifting origin µ1 assume cost best solution 0 mean random variables The cost second best solution given random variables heuristic function H1 heuristic function H2 mean µ2 0 variables We assume scaling distributions8 variance H1 leading optimal solution unity An example help clarify Example 2 Suppose H1 leads optimal cost probability represented normal random variable Xopt mean 0 standard deviation σ1 1 This heuristic function estimates suboptimal cost according Xsubopt mean 4 σ1 1 1 1 H2 heuristic function estimate optimal cost probabil ity given Xopt parameters µ1 0 σ2 14 The corresponding suboptimal cost given heuristic function H2 obtained probability given Xsubopt parameters µ2 4 σ2 14 2 2 2 8 This multiplying σ 2 2 σ 1 simultaneous diagonalization ddimensional normal random vectors d 1 5 1 µ1 µ2 σ 1 1 σ 2 This particular case BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 15 Fig 3 An example showing probability density function normal random variables parameters σ1 1 σ2 14 µ1 0 µ2 4 Observe σ1 σ2 expecting probability H1 leading wrong decision smaller H2 The probability density functions random variables depicted Fig 3 Note doubly exponential distribution given particular value x probability Xopt high area H1 leads wrong decision cumulative probability Xsubopt small Since quantities multiplied integrated ﬁnal 1 value smaller H2 σ2 greater σ1 1 This formally 1 Result 1 9 Suppose A heuristic algorithm potentially utilize heuristic functions H1 H2 Let X1 X2 normally distributed random variables represent costs Xcid6 2 normally distributed random variables represent costs optimal solutions obtained H1 H2 respectively 1 Xcid6 nonoptimal solutions obtained H1 H2 respectively EXcid6 2 0 EX1 EX2 cid1 EXcid6 µ 1 p1 p2 probabilities H1 H2 respectively lead wrong decision 9 We claim result theorem formal analytic proof impossible This closedform expression integrating Gaussian probability density function However computational proof present renders conjecture 16 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 Then VarX1 VarX cid6 1 σ 2 1 cid1 σ 2 2 VarX2 VarX cid6 2 p1 cid1 p2 Computational Proof To achieve proof proceed analysis doubly exponential distributions Theorem 1 If consider particular cost x probability x leads wrong decision H1 given xcid1 I1 1 2πσ1 uµ2 2σ 2 1 du e 25 The probability H1 leads wrong decision values x obtained integrating function resulting multiplying value I1 x respective probability density function Xopt results 1 cid1 p1 I1 1 2πσ1 x2 2σ 2 1 dx e Similarly p2 expressed follows cid1 p2 I2 1 2πσ2 x2 2σ 2 2 dx e 26 27 I2 obtained way 25 distribution variance σ 2 2 Since closedform algebraic expression integrating normal probability density function analytical solution proving p1 cid1 p2 formalized Alternatively invoked computational analysis calculating integral representative values σ1 σ2 trapezoidal rule The values G p2p1 cid2 1 1 cid1 σ1 cid1 10 1 cid1 σ2 cid1 10 σ1 cid1 σ2 depicted Table 1 form lowerdiagonal matrix All values upperdiagonal matrix shown unity Note making value σ1 1 analysis reduces ﬁrst second columns table For example σ1 1 σ2 2 p2p1 336276 For neighboring values σ1 σ2 σ1 9 σ2 10 σ1 1 σ2 12345 scaling p2p1 10318 close unity The ratio σ1 1 σ2 10 bigger times cid1 In order better perspective computational analysis study behavior function G p2p1 Using values G given Table 1 plotted function threedimensional space Gσ1 α1 α1 kσ1 1 cid1 k cid1 10 The plot depicted Fig 4 In order enhance visualization G approximated regres sion utilities symbolic mathematical software package Maple V 3 When k 1 surface lies z 0 plane form straight line x y labeled k 1 σ1 σ2 ﬁgure This place G reaches minimum heuristic functions identical variances When k larger k 10 function G BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 17 Table 1 Ratio probability making wrong decision normally distributed random variables standard deviations σ1 σ2 σ2 100 200 300 400 500 600 700 800 900 1000 100 200 300 400 500 600 700 800 900 1000 σ1 10000 336276 739210 1025081 1221988 1362472 1466138 1547078 1610448 1661716 10000 21982 30483 36339 40516 43599 46006 47891 49415 10000 13867 16531 18431 19834 20929 21786 22480 10000 11921 13291 14303 15092 15710 16211 10000 11150 11998 12660 13179 13598 10000 10761 11355 11820 12196 10000 10552 10984 11334 10000 10410 10741 10000 10318 10000 Fig 4 Function Gσ1 kσ1 plotted ranges 1 cid1 σ1 cid1 10 1 cid1 kσ1 cid1 10 σ2 kσ1 larger 1661716 Table 1 This clearly shows importance minimizing variance deciding heuristic function When concerns histograms database query optimization k small implies optimal suboptimal QEP close Therefore histogram methods like Equiwidth Equidepth likely ﬁnd optimal QEP produce larger errors histogram approximation methods RACM TACM The produce small errors comparing Equiwidth Equidepth larger value k This reﬂected empirical results presented section 18 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 3 Simulation results database heuristic functions 31 Empirical results In order provide practical evidence theoretical results presented above10 performed simulations database query optimization In experiments conducted independent runs In run 100 random databases generated Each database composed relations having attributes Each relation populated 100 tuples For database random query including relations arbitrary attributes performed The cost executing query estimates histograms obtained Equiwidth Equidepth RACM evaluated This cost calculated counting number tuples intermediate relations involved query processing tree More details simulations 17 The efﬁciency RACM compared Equiwidth Equi depth performing simulations 50 values attribute We set number bins Equiwidth Equidepth 22 In order impartial evaluation set number bins RACM approximately half Equiwidth Equidepth needs twice storage The simulation results obtained 400 independent runs compare efﬁ ciency RACM Equiwidth Equidepth given Table 2 The column labeled R W number times RACM obtains better solution Equiwidth The column labeled W R indicates number times Equiwidth leads better QEP determined RACM Similarly column labeled R D represents number times RACM yields better solution Equidepth column labeled D R Table 2 Simulation results RACM Equiwidth Equidepth optimizing queries 400 randomly generated databases The column labeled R W contains number times RACM obtained better solution Equiwidth 100 randomly generated databases The information contained columns similar interpretation R W D stand RACM Equiwidth Equidepth respectively The row contains sum values column Simulation R W W R R D D R 1 2 3 4 Total 26 24 35 29 114 12 15 11 15 53 35 42 46 46 169 12 13 8 8 41 10 The empirical results presented paper intended compare histogram methods Equi width Equidepth RACM TACM Voptimal The experimental results submitted merely included demonstrate theoretically proven results experimentally justiﬁed BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 19 number times Equidepth superior RACM The row total column gives evidence superiority RACM Equi width demonstrated twice The factor relating superiority RACM Equidepth 32 Geometric justiﬁcation rationale We present different perspective formulation QEP model earlier Indeed shall analyze suitability doubly exponential distribution query optimization problem To demonstrate suitability ex amined 200 randomly selected queries Since cost query different database computed difference actual cost executing query estimated cost For histogram methods Equiwidth Equi depth RACM obtained points11 Using points samples estimated parameters doubly exponential distribution λ histogram method Maximum Likelihood Estimate MLE method 4 Given N samples x1 xN obeying doubly exponential distribution easy purely algebraic maximum likelihood parameter ˆλ satisfying distribution obeys ˆλ cid6 N N i1 xi 28 Using estimate 28 computed parameters doubly exponential dis tribution Equiwidth Equidepth RACM resulted 06399 06120 07089 respectively We calculated variances 3 48834 53401 39791 Equiwidth Equidepth RACM respec tively As expected variance RACM smaller Equiwidth Equidepth This observed Fig 5 corresponding doubly ex ponential probability distribution functions plotted histograms This slight difference RACM Equiwidth Equidepth schemes reﬂects corresponding results leading superior QEPs shown Table 2 Clearly RACM variance smaller Equiwidth Equidepth superior heuristic function In order observe similarities doubly exponential distribution distribution actual cost executing query plotted expected values doubly exponential distribution actual costs obtained optimizing queries RACM histogram The plot depicted Fig 6 obtained grouping data bins width values ranges x1 x2 x2 x1 2 x2 2i 4 5 In ﬁgure RACM light gray represents actual cost values queries dexp dark gray represents expected population bin random variable doubly exponential value λ determined 11 Since histograms tend underestimate costs queries shifted points estimated mean samples zero In way work zeromean random variables 20 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 Fig 5 Estimated probability density function doubly exponential random variables represent error estimation Equiwidth Equidepth RACM Fig 6 Expected values doubly exponential random variable actual costs obtained optimizing queries 400 random databases RACM histogram 28 Observe similarity histograms We corroborate validity model database query optimization problem 4 Conclusions The theory PR developed applications In paper applied pattern classiﬁcation techniques solve fundamental open problem science relates heuristic function accuracy solution optimality More speciﬁcally paper discussed efﬁciency heuristic functions optimization problems resolved open problem knowledge open years The problem involves accuracy heuristic function relates quality corresponding solution obtained The efﬁciency quantiﬁed means probability heuristic function leading optimal solution We BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 21 shown analytically reasonable model accuracy doubly exponential distribution errors accuracy heuristic function increases probability leading superior solution increases Due constraints involved deriving closedform expression integrating normal probability density function presented computational analysis accuracyoptimality result Gaussian distribution Again analysis corroborates result heuristic functions producing smaller errors lead optimal lutions For ﬁeld database query optimization highlighted histogram methods produce errors similar variances Equiwidth Equidepth query processing results similar However shown RACM TACM produce errors smaller variances traditional methods yield better query optimization plans This result earlier shown oretically experimentally veriﬁed Thus empirical results database query optimization RACM provides superior solutions twice times Equiwidth times Equidepth More tailed empirical results including design random databases random queries random databases 17 We estimated parameters doubly exponential distributions repre senting Equiwidth Equidepth RACM shown graphically experiments relate theoretical model presented paper Acknowledgments The authors grateful anonymous referee hisher suggestions These suggestions allowed signiﬁcantly enhance quality paper In particular like thank himher hisher critical remarks enabled clearly elucidate motivation paper terms examples explanation difference heuristic algorithms heuristic functions utilize References 1 E Aarts J Korst Simulated Annealing Boltzmann Machines A Stochastic Approach Combinatorial Optimization Neural Computing Wiley New York 1989 2 S Christodoulakis Estimating selectivities data bases Technical Report CSRG136 Computer Science Department University Toronto 1981 3 E Deeba A Gunawardena Interactive Linear Algebra MAPLE V Springer Berlin 1997 4 R Duda P Hart D Stork Pattern Classiﬁcation second ed Wiley New York 2000 5 K Fukunaga Introduction Statistical Pattern Recognition Academic Press New York 1990 6 M Garey D Johnson Computers Intractability A Guide Theory NPCompleteness Freeman New York 1979 7 F Glover M Laguna Tabu Search Kluwer Academic Dordrecht 1997 8 Y Ioannidis S Christodoulakis On propagation errors size join results Proceedings ACMSIGMOD Conference 1991 pp 268277 9 RP Kooi The optimization queries relational databases PhD thesis Case Western Reserve University 1980 22 BJ Oommen LG Rueda Artiﬁcial Intelligence 164 2005 122 10 D Kreher D Stinson Combinatorial Algorithms Generation Enumeration Search CRC Press Boca Raton FL 1998 11 D Levy How Computers Play Chess Computer Science Press New York 1991 12 MV Mannino P Chu T Sager Statistical proﬁle estimation database systems ACM Computing Surveys vol 20 1988 pp 192221 13 M Michell An Introduction Genetic Algorithms MIT Press Cambridge MA 1998 14 M Muralikrishna D Dewitt Equidepth histograms estimating selectivity factors multidimensional queries Proceedings ACMSIGMOD Conference 1988 pp 2836 15 Kumpati Narendra MAL Thathachar Learning Automata An Introduction Prentice Hall Englewood Cliffs NJ 1989 16 N Nilsson Artiﬁcial Intelligence A New Synthesis Morgan Kaufmann San Mateo CA 1998 17 BJ Oommen L Rueda The efﬁciency modernday histogramlike techniques query optimization Comput J 45 5 2002 494510 18 BJ Oommen M Thiyagarajah The Rectangular Attribute Cardinality Map A New Histogramlike Tech nique Query Optimization Proceedings International Database Engineering Applications Symposium IDEAS99 Montreal Canada 1999 pp 315 19 BJ Oommen M Thiyagarajah On use trapezoidal attribute cardinality map query result size estimation Proceedings 2000 International Database Engineering Applications Symposium Yokohama Japan 2000 pp 236242 20 BJ Oommen T De St Croix Graph partitioning learning automata IEEE Trans Comput 45 2 1995 195208 21 F Peracchi Econometrics Wiley New York 2001 22 G PiatetskyShapiro C Connell Accurate estimation number tuples satisfying condition Proceedings ACMSIGMOD Conference 1984 pp 256276 23 W Poosala Histogram based estimation techniques databases PhD thesis University Wisconsin Madison 1997 24 E Rich K Knight Artiﬁcial Intelligence second ed McGraw Hill New York 1991 25 G Romp Game Theory Introduction Applications Oxford University Press Oxford 1997 26 S Russell P Norvig Artiﬁcial Intelligence A Modern Approach second edition PrenticeHall New York 2002 27 A Webb Statistical Pattern Recognition Oxford University Press New York 1999 28 A Yuille M Coughlan An A perspective deterministic optimization deformable templates Pattern Recognition 33 2000 603616