Artiﬁcial Intelligence 148 2003 335383 wwwelseviercomlocateartint Possibilistic instancebased learning Eyke Hüllermeier Department Mathematics Computer Science University Marburg Marburg 35032 Germany Received 16 July 2001 received revised form 9 August 2002 Abstract A method instancebased learning introduced makes use possibility theory fuzzy sets Particularly possibilistic version similarityguided extrapolation principle underlying instancebased learning paradigm proposed This version compared commonly probabilistic approach methodological point view Moreover aspects knowledge representation modeling uncertainty discussed Taking possibilistic extrapolation principle point departure instancebased learning procedure outlined includes handling incomplete information methods reducing storage requirements adaptation inﬂuence stored cases according typicality First theoretical experimental results showing efﬁciency possibilistic instancebased learning presented 2003 Elsevier BV All rights reserved Keywords Possibility theory Fuzzy set theory Machine learning Instancebased learning Nearest neighbor classiﬁcation Probability 1 Introduction A major theme machine learning concerns problem induction creation general knowledge particular examples observed data In respect uncertainty plays fundamental role To begin data presented learning algorithms imprecise incomplete noisy time problem badly mislead learning procedure But observations perfect generalization data afﬂicted uncertainty For example observed data generally explained candidate theory means Email address eykemathematikunimarburgde E Hüllermeier 00043702 matter 2003 Elsevier BV All rights reserved doi101016S0004370203000195 336 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 sure truth particular theory Consequently inductive reasoningby natureis inseparably connected uncertainty 13 In fact insight inductive inference produce ultimate truth traced far Francis Bacons epistemology In Novum Organum1 Bacon advocates gradualist conception inductive enquiry proposes set degrees certainty Thus experience form given data best conclude theory likely truenot true certainty In machine learning mathematical statistics uncertainty type generally handled means probabilistic methods In Bayesian approaches example inference result usually given form probability distribution space candidate models model theory assigned degree probability In paper concentrates possibility theory 29 alternative calculus modeling processing uncertainty generally partial belief By possibility theory handling uncertainty learning procedures inductive reasoning possibilistic sense certain generalizations declared plausible In paper shall employ possibility theory context instance based learning IBL special approach supervised machine learning IBL relies kind extrapolation principle2 expressing commonsense rule suggested David Hume3 In reality arguments experience founded similarity discover natural objects induced expect effects similar follow objects From causes appear similar expect similar effects This sum experimental conclusions Thus HUME suggests extrapolate properties object properties similar ones The idea possibilistic induction combined extrapolation principle leads following inference pattern The similar causes plausible effects Since possibility theory conjunction fuzzy set theory establishes close connection concepts similarity uncertainty provides excellent framework translating principle formal inference procedure This paper complements recent work use possibility theory fuzzy sets instancebased reasoning 2527 The concerned extending IBL means fuzzy setbased modeling techniques focus learning process More speciﬁcally introduce method possibilistic IBL referred POSSIBL implements abovementioned inference pattern Together frameworks yield powerful methodology instancebased reasoning possibility theory fuzzy setbased modeling respectively representing gradation uncertainty evidential support complementing datadriven inference procedure means domainspeciﬁc expert knowledge By way background Section 2 recalls important ideas possibility theory Section 3 gives brief review instancebased learning NEAREST NEIGHBOR principle based Besides aspect uncertainty IBL discussed 1 Published 1620 2 IBL actually realize induction proper discussed later 3 See 45 p 116 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 337 section In Section 4 possibilistic extrapolation principle introduced compared principles commonly instancebased learning Proceeding extrapolation principle method possibilistic instancebased learning developed Section 5 Finally Section 6 presents experimental studies The paper concludes summary Section 7 2 Background possibility theory In section recall basic concepts possibility theory far required current paper Possibility theory deals degrees possibility The term possibility employed graded notion way term probability At ﬁrst sight strike odd possibility usually considered twovalued concept natural language possible Before turning technical aspects let brief remarks semantics underlying notion possibility possibility theory Just concept probability notion possibility different semantic meanings To begin physical sense degree ease One instance possible Hans eggs breakfast eggs simply eating eggs easy feasible practicable eating eggs 82 However concerns use applications paper particular possibility theory considered means representing uncertain knowledge means characterizing epistemic state agent For instance given information Hans eaten eggs clearly uncertain precise number Still eggs appears somewhat plausible possible eggs compatible linguistic quantiﬁer It important note degree possibility opposed degree probability necessarily number In fact applications sufﬁcient suitable assume qualitative ordinal scale possibility degrees ranging hardly fairly completely 3352 Still possibility degrees measured cardinal scale 0 1 different semantic interpretations For example possibility theory related probability theory case possibility degree specify upper probability bound 31 For convenience possibility degrees coded numbers unit interval qualitative framework possibility theory As means representing uncertain knowledge possibility theory makes distinction concepts certainty plausibility event As opposed probability theory possibility theory claim conﬁdence event determined conﬁdence complement event consequently involves nonadditive measures uncertainty Taking existence opposite complementary types knowledge representation information processing account different versions possibility theory outlined following For closer discussion refer 34 24 338 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 21 Possibility distributions generalized constraints A key idea possibility theory originally introduced Zadeh 82 consider piece knowledge generalized constraint excludes world states extent Let Ω set worlds conceivable agent including true world ω0 With incomplete knowledge K true world associate possibility measure ΠK ΠKA measures compatibility K event set worlds A Ω proposition ω0 A Particularly ΠKA small K excludes world ω A large worlds ω A compatible K More speciﬁcally ﬁnding A incompatible K degree corresponds statement form ΠKA cid1 p p possibility degree taken underlying possibility scale P The basic informational principle underlying possibilistic approach knowledge representation reasoning stated principle minimal speciﬁcity4 In order avoid unjustiﬁed conclusions represent piece knowledge K largest possibility measure measures compatible K means inequality turned equality ΠKA p Particularly complete ignorance modeled measure Π 1 Knowledge K usually expressed terms possibility distribution πK mapping Ω P related associated measure ΠK ΠKA supωA πKω Thus πKω degree world ω compatible K Apart boundary conditions ΠKΩ 1 world fully possible ΠK 0 basic axiom underlying possibility theory Zadeh involves maximumoperator cid1 ΠKA B max ΠKA ΠKB cid2 1 In plain words possibility precisely upper possibilitybound union events A B maximum respective possibilities possibilitybounds individual events As constraints naturally combined conjunctive way possibility measures associated pieces knowledge K1 K2 combined minimum operator cid1 πK1K2A min πK1A πK2A cid2 A Ω Note πK1K2Ω 1 indicates K1 K2 fully compatible K1 K2 contradictory extent The distinction possibility certainty event reﬂected existence socalled necessity measure NK dual possibility measure ΠK More precisely relation measures given NKA 1 ΠKΩ A A Ω5 An event A necessary far complement logical negation possible 4 This principle plays role comparable maximum entropy principle probability theory 5 If possibility scale P unit interval 0 1 mapping 1 replaced order reversing mapping P E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 339 Worth mentioning close relationship possibility theory fuzzy sets In fact idea Zadeh 82 induce possibility distribution knowledge stated form vague linguistic information represented fuzzy set Formally postulated πKω µF ω µF membership function fuzzy set F To emphasize ω plays different roles sides equality written explicitly πKω F µF ω Given knowledge K ω element fuzzy set F possibility ω0 ω evaluated degree fuzzy concept modeled F satisﬁed ω To illustrate suppose world states integer numbers The uncertainty related vague statement ω0 small integer ω0 element fuzzy set F small integers translated possibility distribution lets ω0 1 appear fully plausible µF 1 1 5 regarded plausible µF 5 12 10 impossible µF 10 0 22 Possibility evidential support Possibility theory outlined provides basis generalized approach constraint propagation constraints expressed terms possibility distributions fuzzy sets ordinary sets correspond special case 0 1 valued possibility measures A constraint usually corresponds piece knowledge excludes certain alternatives impossible extent This knowledge driven view reasoning complemented datadriven view leads different type possibilistic calculus According view statement ω possible intended mean ω provisionally accepted sense excluded constraining piece information ω supported conﬁrmed observed facts form examples data To distinguish meanings possibility degree shall denote degree evidential support conﬁrmation ω δω6 πω denotes degree compatibility To illustrate suppose values variable V assume subset V 1 2 10 interested inferring values possible In agreement examplebased dataoriented view δv 1 soon instantiation V v observed δv 0 The knowledgedriven approach actually exploit examples observation V v exclude possibility V assume value vcid17 cid18 v As seen datadriven knowledgedriven approach intended respectively expressing positive negative evidence As examples express positive evidence change distribution π 1 This distribution changed knew information source V values v cid2 6 case πv 1 v cid2 6 πv 0 v cid1 5 The distinction modeling positive negative evidence especially clear comes expressing complete ignorance As mentioned 6 In 75 type distribution called σ distribution 340 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 situation adequately captured possibility distribution π 1 If known reason exclude worlds ω remains completely possible At time complete ignorance modeled distribution δ 0 The simply express worlds ω actually supported observed data Within context modeling evidential support possibilistic reasoning accompanies process data accumulation Each observed fact φ guarantees certain degree possibility world state ω expressed inequality form δφω cid2 d The basic informational principle principle maximal informativeness suggests adopting smallest distribution compatible given data turn inequality equality The accumulation observations φ1 φ2 realized deriving distribution pointwise deﬁned cid1 δφ1φ2 ω max cid2 δφ1ω δφ2ω As seen adding new information opposite effect connection types possibilistic reasoning In connection knowledgedriven constraintbased approach new constraint reduce possibility degrees means turning current distribution π smaller distribution π cid17 cid1 π In connection datadriven examplebased approach new data increase lower bounds degrees possibility Closely related view possibility evidential support setfunction introduced 30 called measure guaranteed possibility A degree worlds ω A possible event A possible sense usual measure potential possibility ΠA discussed ω A possible7 For measure characteristic property 1 cid1 A B min A B cid2 3 Instancebased learning In recent years variants instancebased approaches supervised machine learning devised memorybased learning 70 exemplarbased learning 64 casebased reasoning 50 Though emphasizing slightly different aspects approaches founded concept instance case basis knowledge representation reasoning A case observation example thought single experience pattern classiﬁcation pattern recognition problem solution casebased reasoning To highlight main characteristics IBL useful contrast modelbased learning8 Typically IBL methods learn simply storing observed examples They defer processing inputs prediction type query actually requested property qualiﬁes lazy learning methods 3 7 The semantics clearly line measuretheoretic approach underlying probability theory 8 Needless clear borderline approaches In fact learning techniques fall inbetween 22 combine concepts 62 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 341 Predictions derived combining information provided stored examples way After query answered prediction intermediate results discarded As opposed modelbased inductive approaches derive predictions indirect way First observed data order induce model decision tree regression function Predictions obtained basis model serve purposes explaining As opposed lazy learners inductive methods eager sense greedily compile inputs intensional description model discard inputs In general eager modelbased algorithms higher computational costs training phase lazy instancebased methods learning basically amounts storing selected examples On hand lazy methods greater storage requirements typically linear size data set higher computational costs comes deriving prediction Modelbased learning line parametric methods classical statistics instancebased approaches machine learning share important features nonparametric statistics kernel smoothing techniques 74 It deserves mentioning instancebased methods necessarily nonparametric 77 Besides lazy learning paradigm naturally related called transductive inference statistical learning theory 73 Transductive inference inference speciﬁc speciﬁc Thus stands problem estimating values function directly given set empirical data Instead transductive inference shall employ pompous term extrapolation denote process The known values function extrapolatedin locally restricted wayin order estimate unknown values This type inference represents alternative indirect modelbased approach estimates complete functional relationship ﬁrst step induction evaluates estimation points deduction 31 Nearest Neighbor classiﬁcation The wellknown NEAREST NEIGHBOR NN principle originated ﬁeld pattern recognition 16 constitutes core family IBL algorithms It provides simple means realize aforementioned extrapolation observed instances Consider following setting paper X denotes instance space instance corresponds description x object usually attributevalue form X endowed distance measure DX 9 L set labels cid19x λx cid20 called labeled instance case In classiﬁcation tasks focus IBL implementations L ﬁnite usually small set λ1 λm comprised m classes S denotes sample consists n labeled instances cid19xı λxı cid20 1 cid1 ı cid1 n Finally new instance x0 X given label λx0 estimated In connection sample S note X L corresponds set potential observations For label λ L let Cλ X denote set instances x X 9 X DX supposed metric space From practical point view usually assume reﬂexivity symmetry DX 342 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 cid19x λcid20 observed Cλ referred concept For example bicycle belongs concept twowheelers car Formally assume underlying population P entities element p P mapped labeled instance cid19xp λpcid20 unique way Thus x element Cλ cid19x λcid20 existing instance p P cid19x λcid20 cid19xp λpcid20 Observe mapping p cid22 xp assumed injective different elements P description means concepts overlap Cλ Cλcid17 cid18 λ cid18 λcid17 The NN principle prescribes estimate label unclassiﬁed point x0 label closest sample point minimizes distance x0 The kNEAREST NEIGHBOR kNN approach slight generalization takes k 1 nearest neighbors new sample point x0 account That estimation λest x0 λx0 derived set Nkx0 k nearest neighbors x0 means majority vote decision rule λest x0 arg max λL cid2 cid1 x Nkx0 λx λ card 2 Not NN principle classiﬁcation employable realizing locally weighted approximation continuousvalued target functions To end reasonably computes weighted mean k nearest neighbors new query point instead returning common value10 The inductive bias11 underlying NN principle corresponds representativeness closeness assumption suggesting similar closely located instances similar classiﬁcation This hypothesis gives rise similarity guided extrapolation principle discussed introduction clearly heuristic nature Still theoretical properties NN classiﬁcation investigated thoroughly statistical perspective 1412 In fact origin NN approach work nonparametric discriminatory analysis 3839 Besides conceptual modiﬁcations extensions distance weighting discussed considered Particularly editing methods selecting optimal training samples stored memory developed order improve classiﬁcation performance 78 reduce computational complexity 41 Other extensions aim supporting determination adequate metrics optimal size neighborhood Computational aspects addressed For example fast algorithms ﬁnding nearest neighbors devised order improve computational efﬁciency 404981 10 Shephards interpolation method 67 considered special type NN estimation 11 Roughly speaking inductive bias corresponds priori assumptions identity model learned Without biased angle view observed data actually meaningless generalization data impossible 56 12 Needless corresponding results derived certain statistical assumptions setting problem E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 343 32 Uncertainty NN classiﬁcation In statistical estimation theory estimated quantity endowed characterization reliability usually terms conﬁdence measure conﬁdence region Alternatively estimation given directly form probability distribution As opposed NN principle basic form merely provides pointestimation decision rule estimation statistical sense The neglecting uncertainty makes principle appear questionable situations 43 To illustrate Fig 1 shows classiﬁcation problems The new instance x0 represented cross dark light circles correspond instances different classes respectively In cases kNN rule k 5 suggests DARK label x0 As seen classiﬁcation reliable In setting proportion dark light examples balanced apart closest points light This situation ambiguity The setting illustrates problem ignorance It true neighbors dark closest actually distant A simple drastic step handle type problem apply reject option form distance frequency threshold That classiﬁcation answer query simply refused nearest neighbors actually close 153672 frequent label neighbors frequent 1242 A second possibility equal statistical methods especially Bayesian ones deriving probability distribution inference result In fact obvious idea NN techniques originally employed context nonparametric density estimation 3853 Thus single decision replaced estimation form probability vector cid3 cid4 px0λ1 px0λm 3 px0λı Prλı x0 probability λx0 λı conditional probability label λı given instance x0 Taking k nearest neighbors x0 point departure intuitively reasonable approach specify probability px0λı kık relative frequency label λı labels neighbors px0λı kı denotes number neighbors having label λı In fact approach justiﬁed theoretically shown following The NEAREST NEIGHBOR approach density estimation confused classiﬁcation closely related kernelbased density estimation An NN density estimator kernel estimator variable kernel width 68 The size Fig 1 Two situations uncertainty connection basic kNN rule caused existence frequent class label nearest neighbors absence close neighbor 344 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 neighborhood point x0 adapted include exactly k observations Thus consider sample n observations x1 xn Rl realizations l dimensional random vector X probability density φ Rl Rcid10 For x0 Rl let v volume smallest sphere V x0 x0 contains k observations The relation cid3 X V x0 cid4 Pr φx0 v holds true small spheres suggests following estimation φx0 density point x0 φestx0 k n v 4 Coming NN classiﬁcation consider sample S comprises n n1 nm observations nı denotes number tuples cid19x λx cid20 S λx λı Let x0 new observation Again choose small possible hypersphere x0 contains set Nkx0 k instances S k k1 kı cardx Nkx0 λx λı The conditional probability density x0 given label estimated φestx0 λı kı nı v 5 v denotes volume hypersphere x0 Moreover unconditional density x0 prior probability label λı estimated pestλı nı n φestx0 k n v 6 respectively For probabilities 3 obtains px0λı pestλı x0 φestx0 λı pestλı φestx0 kı k 7 Remark 1 Note NN estimation conditional probability density 5 actually given φestx0 λı kı nı vı vı volume smallest sphere x0 contains kı neighbors label λı Then probabilities px0λı kı v k vı 8 necessarily add 1 This problem related general difﬁculty NN density estimation Namely deriving 4 x X leads nonnormalized density function φest x requires different hypersphere13 13 Apart NN density estimation suffer heavy tails inﬁnite integral E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 345 Of course 7 considered formal justiﬁcation original kNN decision rule The label estimated majority vote kNN rule maximal posterior probability 18 Still cautious distribution 7 Particularly clear reliable estimated probabilities px0λı kık actually It possible construct corresponding conﬁdence intervals asymptotically valid 68 In fact k generally small 7 reliable14 Improving quality predictions simply increasing k obviously work entails enlarging hypersphere x015 33 Weighted NN rules A straightforward modiﬁcation kNN rule weight inﬂuence neighboring sample point distance This idea leads replace 2 λest x0 arg max λL cid5 xNkx0 λxλ ωx x0 S 9 ωx x0 S weight neighbor x There different possibilities deﬁne weights For example let neighbors Nkx0 x1 xk arranged dı DX xı x0 cid1 DX x x0 d ı cid1 In 37 weights determined as16 cid6 ωxı x0 S dk dıdk d1 1 dk cid18 d1 dk d1 10 The weighting neighbors appears reasonable intuitive point view For instance weighted kNN rule likely yield LIGHT DARK classiﬁcation Fig 1 More general evidence usefulness distance weighting provided 5458 practically relevant case ﬁnite samples In fact 5 shown asymptotic performance kNN rule improved distanceweighting Note original kNN rule corresponds weighted rule cid6 ωx x0 S 1 x Nkx0 0 x Nkx0 11 Thus NN rule expressed global principle involving complete sample S observations loss generality λest x0 arg max λL cid5 cid19xλxcid20S λx λ ωx x0 S 12 14 An estimated probability multiplicity 1k Particularly px0 λı 0 1 special case k 1 1NN rule 15 Good estimations obtained small hyperspheres containing points Besides asymptotic convergence generally assumes adaptation k function n 16 See 54 modiﬁcation performed better experimental studies types weight functions 79 346 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 Interestingly possible consider probabilistic NN prediction 7 context weighted NN approach Namely 7 written cid5 px0λ cid19xλxcid20S λx λ ωx x0 S weight function ω deﬁned ωx x0 S cid6 1k 0 x Nkx0 x Nkx0 13 14 Again 12 amounts choosing label maximal posterior probability Of course following situation hardly advocate uniform distribution suggesting labels DARK LIGHT probability This example reveals shortcoming weight function 14 disregard arrangement neighbors In fact derivation probabilistic NN estimation 7 disregards actual distances positions estimation probability densities17 This justiﬁed sphere containing k nearest neighbors small usually case practice Note label DARK assigned higher degree probability LIGHT according 8 cf Remark 1 In order account problem possible combine idea weighting probabilistic estimation The use uniform weights 14 corresponds use uniform Parzen window kernelbased density estimation 59 By making use general kernel function K Rl Rcid10 density function usually symmetric 0 NN density estimation 4 generalized follows φestx0 1 n ncid5 ı1 Kdk x0 xı 15 dk distance x0 kth nearest neighbor Kdk rescaling kernel function K Ku 0 u 1 Kd u cid22 1d l Kud The reasoning Section 32 suggests weighted counterpart 7 pestλ x0 cid5 cid19xλxcid20S λx λ Kdk x0 x 16 17 Taking positions account tricky instance spaces higher dimension 86 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 347 As seen 16 estimation derived weighted NN rule means normalization18 Thus proceeding weights 10 simply deﬁnes probability distribution px0 cid5 px0λ ωx x0 S 17 cid19xλxcid20S λx λ Related approach extensions NN classiﬁcation use fuzzy sets 684647 By weighting neighbors according distance methods compute fuzzy classiﬁcation cid3 λest x0 uλ1x0 uλmx0 cid4 18 new instance x0 That x0 assigned unique label unequivocal way Rather degree membership uλx0 speciﬁed label λ Consider example fuzzy kNN algorithm proposed 47 The degree x0 assigned label λı classiﬁed ıth class given uλı x0 cid7 1 uı x0 x 2m1 k cid7 x0 x 2m1 k 1 19 uı uλı x membership degree instance x ıth class The possibility assigning fuzzy membership degrees uı labeled instances x seen decisive feature Turning nonfuzzy label λx observed instance x fuzzy label allows adjust inﬂuence instance considered prototypical class The constant m 19 determines weighting distance x0 neighbors Clearly 19 probabilistic ﬂavor degrees membership add 119 However use fuzzy labels makes general 17 In fact fuzzy classiﬁcation 18 written uλ0x0 ncid5 ı1 uλ0xı ωxı x0 S Formally main difference probabilistic estimation fuzzy classiﬁcation use fuzzy labels approach In probabilistic case observed instance cid19x λx cid20 supports label λx Depending typicality instance concern boundary case labeling unequivocal support labels λ cid18 λx case fuzzy classiﬁcation 18 Note 16 actually considers k instances kth nearest neighbor unique See 58 alternative type distanceweighting kNN uniﬁes classiﬁcation density estimation 19 Formally 19 interpreted probability distribution It noted interpretation criticized derivation 19 assume underlying probabilistic model 348 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 34 IBL algorithms Proceeding basic NN approach family instancebased machine learning algorithms proposed 24 The simplest algorithm known IB1 mainly differs basic NN algorithm normalizes numeric attribute values instances characterized means attributevalue representation guarantee features equally weighted processes instances incrementally uses simple method tolerating missing attribute values IB2 extends IB1 editing strategy maintains memory case base selected cases called prototypes falsely classiﬁed points added references A extension IB3 aims reducing inﬂuence noisy observations20 To end classiﬁcation record maintained counts correct incorrect votes stored references By weighting attribute values computation distance measure IB4 IB5 2 relevance features account The weights adapted time new classiﬁcation To summarize IBL algorithms concept learning basically consist components 2 A similarity function computes numeric similarity instances A classiﬁcation function decides membership newly presented instance concept given similarities new instance stored examples labels classiﬁcation performance examples It yields complete concept description applied unclassiﬁed instances After classiﬁcation task concept description updater derives modiﬁed concept description maintaining memory cases The decision retain remove case based records previous classiﬁcation performance information provided new classiﬁcation task As basic NN rule efforts improve performance IBL algorithms Important points mentioned include conceptual aspects reduction storage requirements editing prototype selection 55 toleration noise 4 deﬁnition similarity functions 80 feature weighting selection 77 practical issues efﬁcient techniques indexing training examples 76 Apart classiﬁcation IBL techniques employed function approximation predict realvalued attributes 4886 4 Possibilistic extrapolation cases 41 The basic estimation principle The following type possibilistic prediction proposed 23 developed 2527 max 1cid2ıcid2n δx0λ0 cid1 σX x0 xı σLλ0 λı cid2 min 20 See 78 early work lines 20 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 349 λ0 L δx0λ0 denotes estimated possibility label λ0 λ0 Moreover σX σL 0 1valued similarity measures X possibility λx0 L respectively 411 The possibility distribution δx0 According 20 λx0 λ0 regarded possible instance cid19xı λxı cid20 xı close x0 λxı close λ0 Or deﬁne joint similarity labeled instance cid19xı λxı cid20 hypothetical case cid19x0 λ0cid20 minimum similarities σX x0 xı σLλ0 λxı expressed saying case cid19x0 λ0cid20 regarded possible existence similar case cid19xı λxı cid20 conﬁrmed observation In words similar case provides evidence existence cid19x0 λ0cid20 sense possibility qualiﬁcation21 Following notational convention Section 2 possibility degrees δx0λ0 denote degrees guaranteed possibility Thus actually considered degrees plausibility usual sense degrees conﬁrmation introduced Section 22 More speciﬁcally distribution δx0 L 0 1 thought lower upper bound Particularly δx0λ0 0 equated λ0 merely means evidence supporting label λ0 impossibility λx0 available far In fact δx0 provisional nature degree possibility assigned label λ0 increase gathering evidence observing new examples reﬂected application maximum operator 20 This completely accordance use possibility theory connection special approach fuzzy rulebased reasoning Indeed proceeding rule The closer x x0 possible λx close λx0 possibility distribution 20 originally derived inference result related approximate reasoning method 32 The concerns examplebased approach fuzzy rules single rule case considered piece data 84 This contrasts constraint based approach rule modeled implication rules combined conjunctively possibility distribution upper bound cf Section 21 It natural assume possibility distribution π Ω 0 1 normalized sense supωΩ πω 1 πω speciﬁes degree plausibility ω corresponds true world ω022 The remarks clear constraint sense δx0 In connection noticed necessarily unique actual world ω0 sense possible worlds semantics 9 Since x0 assumed unique label δx0 provides information set λ L x0 Cλ potential labels Thus state complete knowledge corresponds distribution δx0 δx0λ 1 x0 Cλ δx0λ 0 21 The idea possibility qualiﬁcation usually considered connection natural language proposi tions 6583 Here possibility qualiﬁcation casuistic linguistic 22 Though generally accepted constraint questioned authors For example subnormalized distribution allowed order express kind conﬂict 350 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 When applied x X 20 yields fuzzy concept descriptions possibilistic approximations concepts Cλ λ L x X x δxλ cid1cid3 cid2 cid4 Cest λ 21 δxλ degree membership x X fuzzy concept Cest λ Note fuzzy concepts overlap sense x positive degree λcid17 λ cid18 λcid1723 membership concepts Cest λ Cest 412 The similarity measures σX σL Let remarks similarity measures σX σL To begin notice thataccording 20the similarity cases direct correspondence possibility assigned label Roughly speaking principle expressed fuzzy rule underlying equation 20 gives rise turn similarity possibilistic support Consequently σX σL thought support measures similarity measures usual sense They actually serve purpose weight functions Section 33 Particularly σX x0 xı 0 means label λxı considered relevant piece information xı sufﬁciently similar x0 For computation irrelevant cases 20 clearly left account Thus consider cases certain region x0 As opposed kNN approach size region number neighboring cases ﬁxed We assume σX σL reﬂexive symmetric special kind transitivity required In fact application maximum operator 20 permit purely ordinal approach In case range similarity measures ﬁnite subset A 0 1 encodes ordinal scale completely different similar identical 22 Correspondingly degrees possibility interpreted qualitative way 3352 That δx0λ δx0λcid17 means label λ supported label λcid17 apart difference values meaning Needless scale 22 convenient instances complex objects points Euclidean space similarity distance objects assessed human experts common practice casebased reasoning Note ordinal structure sufﬁcient original kNN rule In connection distanceweighting structures involved measures important In case aware fact cardinal interpretation similarity raises crucial semantic questions corresponding measures deﬁned straightforward way In weighted kNN rule example patient died certain medical treatment compensates patients survived twice similar current patient But exactly twice similar mean context Looking 20 point view observed cases estimation principle deﬁnes possibilistic extrapolation sample cid19x λx cid20 In original NN approach 23 In practice fuzzy andor overlapping concepts rule exception 1 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 351 involve distance measure DL L case cid19xı λxı label λxı This corresponds special case σL 20 given cid20 S support cid6 σLλ λcid17 1 0 λ λcid17 λ cid18 λcid17 23 reasonable L nominal scale concept learning pattern recognition classiﬁcation L 2 By allowing graded distances labels possibilistic approach provides cid20 support similar labels This type extended extrapolation case cid19xı λxı reasonable L cardinal ordinal scale In fact observed 20 applies continuous scales way discrete scales uniﬁes performance tasks classiﬁcation function approximation For example knowing price label certain car 10500 plausible similar car exactly price plausible costs 10700 Interestingly principle employed kernelbased estimation probability density functions probabilistic support allocated kernel functions centered observations 5963 Indeed 20 considered possibilistic counterpart kernelbased density estimation Let ﬁnally mention consideration graded distances labels related idea classdependent misclassiﬁcation costs 6071 42 Generalized possibilistic estimation The possibility distribution δx0 speciﬁes fuzzy set wellsupported labels disjunctive combination individual support functions cid1 λ0 cid22 min σX x0 xı σLλ0 λxı cid2 δı x0 24 In fact maxoperator 20 socalled triangularconorm serves λ0 regarded possible cid19x0 λ0cid20 similar generalized logical oroperator λx0 cid19x1 λx1 cid20 cid19xn λxn cid20 cid19x2 λx2 cid20 Now fuzzy set theory offers tconorms max 20 generalized follows δx0λ0 δ1 x0 cid8 λ0 δn x0 λ0 δ2 x0 cid1 σX x0 xı σLλ0 λxı λ0 min cid2 1cid2ıcid2n cid9 1 cid1 1 σX x0 xı 1 σLλ0 λxı max cid2 1cid2ıcid2n λ0 L tnorm related tconorm respectively Recall tnorm binary operator 0 12 0 1 commutative associative monotone increasing arguments satisﬁes boundary conditions x 0 0 x 1 x An associated tconorm deﬁned mapping α β cid22 1 1 α 1 β The tnorm associated tconorm max minoperator Other important operators product P α β cid22 αβ related tconorm P α β cid22 352 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 α β αβ Lukasiewicz tnorm L α β cid22 max0 α β 1 related t conorm L α β cid22 min1 α β Observe minimum operator employed determination joint similarity cases considered logical operator fuzzy conjunction Two cases cid19x0 λx0 cid20 similar x0 similar x1 λx0 similar λx1 Consequently operator replaced tnorm By 24 20 cid20 cid19x1 λx1 δı x0 λ0 cid22 σX x0 xı σLλ0 λxı δx0λ0 cid8 1cid2ıcid2n σX x0 xı σLλ0 λxı 25 26 respectively Note fuzzy logicbased derivation joint similarity compulsory Particularly tnorm 26 need necessarily related tconorm For example thoroughly min P combine similarity degrees σX x0 xı σLλ0 λxı means operator tnorm In case logical interpretation 26 lost 421 Control compensation accumulation support By choosing appropriate tconorm 26 control accumulation individual degrees evidential support especially extent compensation To illustrate consider following situation σX x0 x1 34 σX x0 x2 σX x0 x3 12 σX x0 x4 14 Should prefer DARK LIGHT classiﬁcation new point The use maxoperator tconorm yields δx0DARK 34 δx0LIGHT 12 decision DARK The moderately similar instances label LIGHT compensate similar instance label DARK As opposed probabilistic sum α β cid22 α β αβ brings compensation effect entails δx0DARK 34 δx0LIGHT 1316 slightly larger possibility LIGHT More generally different tconorms model different accumulation modes typically entail kind saturation effect In case probabilistic sum P example additional βsimilar observation increases current support α β1 α Thus larger support granted smaller absolute increase new observation This appears reasonable intuitive point view If support label large surprised close instance having label A small support increment reﬂects low information content related new observation 44 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 353 422 Possibilistic support weighted NN estimation A tnorm called Archimedian following holds For x y 0 1 number n N nx y nx n1x x 1x x It shown continuous Archimedian tnorm iff continuous strictly decreasing function g 0 1 0 g1 0 27 0 cid1 α β cid1 1 pseudoinverse g1 deﬁned α β g1 gα gβ cid4 cid3 cid6 g1 x cid22 g1x 0 0 cid1 x cid1 g0 g0 x The function g called additive generator For example x cid22 1 x x cid22 lnx additive generators Lukasiewicz tnorm L product P respectively Based representation 27 establish interesting connection 26 weighted NN rule To end let g additive generator t norm24 related tconorm aggregation operator 26 With dı 1 σX x0 xı σLλ0 λxı ωı gdı write 26 δx0λ0 1 g1ω1 ω2 ωn 28 Since g decreasing considered weight function turns distance dı weight ωı associated ıth instance Then 28 tells possibility degree δx0λ0 monotone increasing transformation sum weights ωı In words 26 seen distanceweighted NN estimation weight neighbor determined function similarity new instance As opposed 9 weight case according 28 depend cases stored memory cf Section 431 Consider Lukasiewicz tconorm example obtain ωı 1 dı σX x0 xı σLλ0 λxı δx0λ0 min1 ω1 ω2 ωn 29 If σL given 23 δx0λ0 bounded sum λ0 Thus similarity degrees σX xı x0 x0 instances xı label λxı 29 basically equivalent global NN method weighted NN approach k n25 apart fact distinguish labels accumulated support exceeds 1 type saturation effect For probabilistic sum P mapping possibility degrees sum weights onetoone cid3 δx0λ0 1 exp ω1 ω2 ωn cid4 In connection generalized model 26 tconorm combining individual degrees support deﬁnes degree freedom model It 24 This tnorm 26 deﬁning joint similarity measure 25 The proper kNN rule emulated 11 weights ωı depend absolute distance Section 431 354 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 interesting mention existence parameterized families tconorms comprise commonly operators special cases For example Frankfamily deﬁned ρ α β cid22 maxα β α β αβ min1 α β cid3 1 ρ1α1ρ1β1 1 lnρ ρ1 cid4 ρ 0 ρ 1 ρ 30 Proceeding family tconorms degree freedom model reduces single parameter ρ adapted simple way means crossvalidation techniques 423 Upper lower possibility bounds The possibility degree 26 represents support conﬁrmation label λ0 gathered similar instances according basic NN principle suggesting similar cid20 instances similar labels Now sense principle observation cid19xı λxı conﬁrm disqualify label λ0 This happens xı close x0 λxı similar λ0 A possibility distribution expressing degrees exclusion degrees support complementing 26 natural way given πx0 λ0 cid22 cid9 cid3 1 σX x0 xı cid4 σLλ0 λxı 31 1cid2ıcid2n According 31 individual observation cid19xı λxı cid20 induces constraint label cid20 σX x0 xı large σLλ0 λxı x0 A label λ0 disqualiﬁed cid19xı λxı cid20 completely ignored σX x0 xı 0 small As opposed cid19xı λxı 1 expression case individual support righthand 31 1 πx0 complete ignorance upper possibility bounds 1 reason discredit label This approach obviously agreement constraintbased view possibilistic reasoning cf Section 21 Moreover distribution 31 related special type fuzzy rule 26 The possibility label λ0 characterized means extended estimation tuple cid14 δ x0 λ0 cid15 δx0λ0 πx0λ0 lower bound δx0λ0 expressing degree conﬁrmation upper bound πx0λ0 expressing degree plausibility The following cases comple mentary distribution πx0 greatly improve informational content possibilistic evaluation26 δ λ0 0 1 This expression complete ignorance Neither λ0 supported x0 partly excluded observation Thus λ0 fully plausible conﬁrmed 26 Recall positive negative evidence distinguished probability theory E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 355 λ0 0 0 Clear evidence λ0 accumulated form δ x0 instances similar x0 labels dissimilar λ0 λ0 1 1 The label λ0 strongly supported observation similar δ x0 instances Notice δx0λ0 πx0λ0 32 indicates kind conﬂict closely related problem ambiguity connection NN principle cf Section 32 In fact 32 occur x0 close neighbors xı x dissimilar labels λxı λx mathematically speaking x0 point discontinuity In case evaluation λ0 unsteady support δx0λ0 taken caution The inequality 32 trigger revision process aims removing conﬂict means model adaptation 424 Fuzzy logical evaluation The values δx0λ0 26 considered membership degrees fuzzy set fuzzy set wellsupported labels In fact possibility degree δx0λ0 seen truth degree cid19P λ0cid20 following fuzzy predicate P λ0 There instance close x0 label similar λ0 P λ0 deﬁnes property qualiﬁes λ0 wellsupported label Of course easily think alternative characterizations wellsupported labels Fuzzy setbased modeling techniques allow translating characterizations given linguistic form logical expressions By fuzzy logical connectives including tnorms fuzzy quantiﬁers fuzzy relations closely located specify sophisticated fuzzy decision principles simple NN rule Example There closely located instances instances label moderately close instances different label The logical expression P associated speciﬁcation place righthand 26 cid19P λ0cid20 33 δx0λ0 The decision rule related 26 favors label λest meets requirements speciﬁed x0 P best This generalization appears especially interesting allows adapt NN principle speciﬁc characteristics application account Observe 33 mimic original kNN rule Consider fuzzy proposition λ0 supported k nearest neighbors x0 let fuzzy quantiﬁer k modeled mapping ı cid22 ık Then δx0λ0 ık iff ı k nearest neighbors label λ0 In case possibility degrees derived fuzzy truth degrees formally coincide probability degrees 356 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 43 Comparison extrapolation principles So far discussed types NN approaches estimation decision making A probabilistic agreement original kNN rule possibilistic introduced section Both approaches considered step procedure The ﬁrst step derives distribution subsequently referred NN estimation This estimation deﬁnes degree support label λ L The second step NN decision chooses label basis NN estimation Usually decision given label maximal support ties broken coin ﬂipping Still case continuous ordinal scale L decision obtained kind averaging procedure In order facilitate comparison approaches write degrees evidential support general form νλ x0 S α νx λ x0 S cid19x λx cid20 S obtain maximal support decision λest x0 arg max λL νλ x0 S cid4 34 35 In 34 νx λ x0 S support hypothesis λx0 instance cid19x λxcid20 α aggregation function λ provided labeled To reveal original kNN rule probabilistic approach special cases 35 note probability distribution 7 obtained arithmetic sum aggregation function α deﬁning support function νp x λ x0 S 1k 0 x Nkx0 λ λx More generally support function deﬁned νp x λ x0 S Kdk x0 x 0 λ λx 36 37 cid3 cid6 cid6 K kernel function The index dk denotes distance x0 kth nearest neighbor It signiﬁes kernel function scaled exclude exactly instances xı DX x0 xı dk Proceeding 37 probability distribution px0 obtained normalizing supports νpλ x0 S yields cid5 cid19xλxcid20S νp x λ x0 S px0λ cid7 νpλ x0 S m 1 νpλ x0 S 38 λ L That aggregation α normalized simple arithmetic sum Of course normalization change mode distribution effect decision making omitted point view E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 357 The possibilistic approach 26 recovered α νδ x λ x0 S σX x0 x σLλ λx 39 As seen main difference probabilistic possibilistic approach concerns deﬁnition individual support function νx aggregation corresponding degrees support Apart direct comparison complicated similarity measure labels σL 39 37 One possibility handle problem consider 39 special case 23 cid6 νδ x λ x0 S σX x0 x 0 λ λx 40 Eq 40 reveals similarity measure σX plays role kernel function K 37 431 Absolute versus relative support An important difference 37 40 example cid19x λx cid20 S provides relative support label λ probabilistic approach absolute support x λ x0 S depends absolute similarity x0 possibilistic That νδ x λ x0 x independent observations In fact actually write νδ x λ x0 S S appear righthand 40 The support place νδ provided observed samples cid19x λx cid20 bounded nearby instances decreases gradually distance vanishes completely dissimilar examples As opposed support νp x λ x0 S relative depends relation distance x x0 distances observations x0 This reﬂected scaling kernel function 37 On hand means νp x λ x0 S large x distant x0 On hand extension sample S instance close x0 exclude similar observation x neighborhood Nkx0 The corresponding rescaling kernel function cancel support provided cid19x λx cid20 far The induced thresholding effect appears especially radical questioned grounds connection 36 νp x λ x0 S reduced 1k 0 support support The bounding evidential support realized possibilistic approach advisable Consider simple example Let X 0 1 λx I121x27 suppose instances chosen random according uniform distribution Moreover assume new instance x0 labeled given observation x1 Using 1NN rule probability correct decision obviously 12 Now suppose NN rule applied x0 x1 cid1 d decision determined ﬂipping coin exactly procedure results possibilistic approach deﬁning σX 20 σX x xcid17 1 x xcid17 cid1 d 0 A simple calculation shows probability correct decision 12 d1 d As seen dissimilar instances 27 IA indicator function IAx 1 x A 0 358 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 likely provide misleading information example disregard instances advantageous Loosely speaking better guess label random rely observations similar Of course concept absolute support actually reserved possibilistic approach realized probabilistic method To end simply replaces 37 cid6 νp x λ x0 S Kx0 x 0 λ λx 41 kernel function K ﬁxed That K longer scaled size neighborhood x0 This exactly estimation derives reasoning Section 32 generalized NN density estimation 15 replaced simple kernel estimator φestx0 1 n ncid5 ı1 Kx0 xı 42 Here problem occurs νpλ x0 S 0 λ L In situation complete ignorance probability distribution derived normalization Apart 41 preferred 37 reasons mentioned In fact realize major reasons NN density estimator 15 kernel estimator 42 guarantee continuity density function φest In context instancebased learning important interested estimating complete density function single value thereof To best knowledge 37 41 compared systematic way IBL far Note 41 actually called NEAR NEIGHBOR estimation involves near nearest neighbors The remark applies possibilistic approach course Above argued consideration graded degrees similarity labels advised example Section 45 It mentioned probabilistic approach extended direction To end joint probability density estimated based kernel function K deﬁned X L An estimation label λ derived conditioning x0 cid5 cid5 px0λ νp x λ x0 S Kx0 x λ λx cid19xλxcid20S cid19xλxcid20S This general form probabilistic estimation Still mind requires X L certain mathematical structure assumption satisﬁed applications refer example Let conclude section ﬁnal remark related work different context Interestingly distinction similar absolute relative support connection cluster analysis In fuzzy cluster analysis point positive degree membership classes Still classical approach 7 membership degrees add 1 interpreted relative numbers In 51 difﬁculties caused constraint discussed possibilistic clustering E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 359 advocated alternative In approach membership degree reﬂect absolute compatibility point prototype cluster 432 Similarity versus frequency The estimation principle underlying probabilistic approach combines concepts similarity distance frequency It applies closeness assumption typical similaritybased reasoning suggests focus similar observations weight observations distance From reduced set supposedly relevant instances probabilities estimated relative frequencies This contrasts basic maxmin possibilistic approach 20 relies similarity The application maximum operator produce compensation reinforcement effect Thus possibility depicts existence supporting evidence frequency28 The generalized possibilistic approach based 26 allows modes compensation combine aspects Especially operators mentioned produce kind saturation effect limited reinforcement effect The increase support observation similar instance decreasing function support available In connection important realize different nature concepts possibility probability Particularly emphasized interpreted terms latter29 For example consider standard probabilistic setting cases chosen randomly independently according ﬁxed probability measure X L The possibility degree δx0λ0 converge 1 increasing sample size cid19x0 λ0cid20 nonzero probability occurrence In fact possibilistic approach interested existence case probability Roughly speaking major concern approach approximation concepts Cλ probabilistic approach aims estimating conditional probability distributions Pr x0 Of course distinction relevant concepts overlapping px0 query x0 unique label Otherwise possibilistic probabilistic approach equivalent sense x0 Cλ Prλ x0 1 It question frequency observations usually provides valuable information Yet frequencybased approach heavily rely statistical assumptions concerning generation training test data Thus misleading assumptions violated Suppose probability observing positive example learning concept C1 X depends number positive examples observed far contradicts independence assumption probability label λx given instance x independent data In case probabilistic estimation clearly biased possibility distribution 20 affected Indeed information expressed δx0 remains valid negative examples xı C0 X C1 presented far δx01 0 simply means evidence x0 C1 gathered Moreover value δx00 reﬂects 28 To certain extent related distinction existential enumerative analogy factor models analogical induction 57 29 Though relationship established interpreting possibility upper probability 31 fuzzy sets coherent random sets 28 360 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 available support x0 C0 This support depends distance x0 observed negative examples Note δx00 0 possible In case evidence available x0 C1 See Section 63 simulation experiment concerns aspect robustness NN estimation violations standard statistical assumptions Apart statistical assumptions structure application important inﬂuence To illustrate consider classes form clusters known diameter clusters smaller distance DX x1 x2 DX x1 x3 λx1 cid18 λx3 The label instance λx2 determined certainty soon distance nearest neighbor known In words 1NN rule involve frequency information performs better kNN rule k 1 44 NN estimations NN decisions In addition extrapolation principles let compare induced distributions referred NN estimations knowledge representational point view especially background shortcomings NN rule illustrated Fig 1 A crucial difference possibility distribution δ probability function p obeys normalization constraint demands total probability mass 1 constraint exists possibility theory Consequently possibility distribution expressive situations Especially following points deserve mentioning Possibility reﬂects ignorance All possibility degrees δx0λ remain small 0 sufﬁciently similar instances available Particularly distribution δx0 expression complete ignorance reﬂects absence relevant observation σX x0 xı 0 xı A learning agent estimation knows 1m indicates doesnt know 70 As opposed distribution δx0 small evidence available m labels λı These situations distinguished probability theory induce distribution 1m suggested principle insufﬁcient reason complete ignorance px0 modeled uniform distribution Possibility reﬂects absolute frequency For example suppose σX x0 xı 1 d 0 λ1 n instances xı stored memory The probabilistic estimation 7 λxı yields onepoint distribution px0λ1 1 px0λ 0 λ cid18 λ1 Thus λ1 certain n small With compensating suggests λx0 tconorm probabilistic sum P extended estimation 26 yields δx0λ1 1 d n δx0λ 0 λ cid18 λ1 Thus possibilistic λ1 reﬂect distance actual number support hypothesis λx0 voting instances δx0λ1 increasing function n approaches 1 n As seen probabilistic estimation represent ambiguity possibilistic approach captures problems ambiguity ignorance Ambiguity Fig 1 present plausible labels similar degrees support E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 361 ignorance Fig 1 reﬂected fact supported label small degree possibility Thus 26 taken point departure decision making procedure goes guessing label For example possible line action proceeding 26 expressed following rules involving thresholds 0 dmax dmin 1 If δx0λ cid2 dmin supported label λ δx0λ cid1 dmax λ cid18 λ let λest x0 λ If δx0λ dmin gather information If δx0λ cid2 δx0λ cid2 dmin labels λ λ L refuse prediction The ECHOCARDIOGRAM DATABASE30 realworld example interesting respect One problem addressed machine learning researchers connection database predict attributes patient suffered heart attack survive year Since data sparse 132 instances 10 attributes possibilistic approach yields estimations low support alternatives surviving surviving year This clearly reasonable knowledge representational point view reveals advantage absolute relative degrees support For example telling patient 0 experience allow statement concerning prospect survival δx0 different telling chance 12 px0 12 Let mention generalization kNN rule closely related approach developed 19 In method motivated problems ambiguity ignorance original kNN rule estimation label λx0 given terms belief function 66 possibility distribution See 27 comparison approaches The discrepancy probabilistic possibilistic approach approach based belief functions disappears extent interested ﬁnal decision decision irrespective quality quantity information hand The method 19 example refers socalled transferable belief model 69 turns belief function credal level specifying unknown label probability function pignistic level making decision Thus support individual labels expressed terms probability NN estimation derived taking probable labels breaking ties random Observe consequence applying maximum operator possibilistic NN decision derived 20 coincides 1NN rule The generalized version 26 moderately similar examples compensate similar instance comes closer original kNN rule In fact certain special cases possibilistic approach equivalentfrom decision making point viewto probabilistic approach based support function 41 Eq 28 shows possibility degree δx0λ monotone transformation sum weights ωı relation onetoone 30 Available httpwwwicsuciedumlearn 362 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 pseudoinverse g1 actually inverse g1 The similarity function σX chosen δx0λı cid1 δx0λ px0λı cid1 px0λ That labels better supported possibilistic sense probable vice versa To illustrate consider case X Rl σLλ λcid17 1 λ λcid17 0 Let K kernel function deﬁne σX x y cid22 1 expKx y31 For tconorm P weights 28 given ωı Kx0 xı Therefore cid16 δx0λı 1 exp cid17 cid5 Kx0 x cid19xλxcid20S λxλı cid3 1 exp c px0λı cid4 px0λı probability degree derived 41 kernel function K c normalization factor cid5 c px0λ λL 45 An illustrative example Here present simple example possibilistic approach considered superior probabilistic The task shall predict students grade physics given information grades student Thus instance subject label given corresponding grade We assume grades taken scale L 0 1 10 10 best result Moreover consider scenarios S1 S2 Subject Chemistry French Philosophy Spanish Sports S1 5 S2 10 3 3 3 It clearly obvious deﬁne reasonable similarity measure set subjects In fact ordinal measuresufﬁcient possibilistic approach 20 appears simpler cardinal Nevertheless let assume following cardinal degrees similarity σX Chem French Phil Span Sports Physics 34 13 13 13 0 31 Formally set K0 ensure σX reﬂexive E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 363 Concerning set labels L graded degrees similarity clearly advised example Let deﬁne similarity grades b cid1 1 1 σLa b max 5 b 0 cid2 Needless application deﬁne statistical setup par excellence main reason probabilistic approach hardly appear suitable To begin scenario deﬁned considered independent sample information censored comes student mention small number observations Moreover relative frequency interpretation sense Finally set X endowed similarity measure σX partly speciﬁed likely lack mathematical metric structure enables deﬁne reasonable kernel function K X X L Consequently derivation kNN estimation Section 32 longer valid Clearly prevents applying formulae simply interpreting normalized degrees additive support degrees probability But mind approach actually lacks solid foundation The ﬁrst scenario typical example complete ignorance relevant piece information It true case base grade sports allow draw conclusion grade physics subjects dissimilar This adequately reﬂected possibilistic δphysics 0 A probabilistic estimation relative support estimation yields δx0 obviously appropriate example Since sports neighbor obtains probability distribution favors grade 5 physics Thus clearly advised use absolute relative support Then probability actually deﬁned denominator 38 zero One way uniform distribution 111 default estimation raises wellknown question px0 adequate expression complete ignorance deﬁnitely denied scholars Scenario S2 reveals problems weighting aggregation Undoubtedly weighted estimation preferred example Still example shows deﬁnition aggregation weights tricky What likely grade Particularly grade 3 physics likely grade 10 vice versa The weighted kNN rule favors grade 3 subjects moderately similar physics compensate chemistry similar Of course result judged critically Especially example reveals problem interdependence taken account means simple summation weights Namely subjects Spanish French similar Thus wonder grade 3 count twice In fact prefer consider grades French Spanish piece evidence suggesting student good languages instead pieces distinct information Formally problem probabilistic approach makes assumption conditional independence longer valid taking structural assumptions application account Here assumptions correspond NN inductive bias hypothesis similar instances similar classiﬁcations Given hypothesis instances stored 364 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 case base longer independent grade 3 French conjunction hypothesis makes grade 3 Spanish likely The problem interdependence taken account long estimation disregards similarity instances stored memory estimations presented far Still aggregation operator possibilistic approach provides means alleviating problem With max example frequency count obtains δx03 13 34 δx010 The probabilistic sum P brings reinforcement effect yields δx03 07 34 δx010 result appears reasonable A second problem related scenario S2 ambiguity Particularly probabilistic approach yields bimodal distribution px0 true aggregation operators possibilistic approach For example 26 P P yields δx03 δx07 δx010 This result intuitive hardly judge intermediate grade possible extreme grades To solve problem δx0 replaced convex hull cid1 max λcid17cid2λ δx0λcid17 max λcid17cid1λ λ cid22 min δx0λcid17 43 cid2 In example leads following distribution λ 0 1 2 3 4 5 6 7 8 9 10 δx0 λ 0 03 053 07 07 07 07 07 07 07 075 Of course prediction ambiguous sense supports grades means high degrees possibility This defect adequate representation ambiguity present situation associated scenario S2 The modiﬁcation 43 δx0 considered adhoc Rather convexity requirement thought possibilityqualifying rule complements similaritybased justiﬁcation possibility degrees The possible labels possible label inbetween This type background knowledge associated constraints met easily possibilistic approach probabilistic In fact incorporation background information hardly compatible nonparametric density estimation In summary example shown following advantages possibilistic approach First interpretation aggregated weights terms degrees evidential support critical interpretation terms degrees probability Second possibility distribution represent ignorance Third use aggregation operators arithmetic sum useful Fourth possibilistic approach ﬂexible allows incorporating constraints background knowledge 46 Complexity issues Even algorithmic aspects scope paper let rough look computational complexity possibilistic approach IBL A straightfor E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 365 ward implementation prediction 25 running time linear size S sample number L labels In respect completely comparable instancebased learning methods In order reduce computational complexity IBL approaches advantage fact prediction determined nearest neighbors query instance Thus consideration sample instance actually necessary efﬁciency gained means fast algorithms ﬁnding nearest neighbors 404981 Such algorithms employ efﬁcient similaritybased indexing techniques corresponding data structures order ﬁnd relevant instances quickly The idea applied connection possibilistic approach In fact possibility degree δx0λ completely determined neighborhood case cid19x0 λcid20 sample instances cid19x λx cid20 satisfying σX x x0 0 σLλx λ 0 As seen apart minor differences possibilistic method comparable IBL methods complexity point view One difference concerns relevant sample instances In kNN approach number relevant instances k degree relevance instance change modifying case base As opposed degree relevance neighboring instance ﬁxed possibilistic approach number relevant instances change Let ﬁnally mention efﬁciency gained complete possibility distribution δx0 needed In fact interested labels having high degree possibility For example interested ﬁxed number maximally supported labels labels support exceeds given possibility threshold In cases computation δx0λ omitted broken certain labels λ 5 Possibilistic instancebased learning Proceeding NN estimation 26 developed possibilistic method instancebased learning called POSSIBL This section presents extensions basic model turn POSSIBL powerful practically useful IBL algorithm 51 Dealing incomplete information The problem dealing incomplete information missing attribute values important issue machine learning 2061 For example suppose speciﬁcation new instance x0 incomplete let X0 X denote instances compatible description x0 Moreover recall lower supportbound semantics possibilistic approach IBL The following generalization 26 accordance semantics δx0λ inf xX0 δxλ inf xX0 cid8 1cid2ıcid2n σX x xı σLλ λxı 44 Indeed potential candidate x X0 gives rise lower bound according 26 additional knowledge guarantee smallest bounds 366 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 valid This agreement idea guaranteed possibility cf Section 22 The simplicity handling incomplete information coherent possibilistic way clearly strong point POSSIBL Notice computation lower bound 44 line handling missing attribute values IB1 values assumed maximally different comparative value Yet possibilistic solution appears appealing avoids default assumption Indeed inferring possible reasonable way dealing missing attribute values handling incomplete uncertain information coherent way More generally imprecise knowledge x0 modeled form possibility distribution π X πx corresponds degree plausibility x0 x A graded modeling kind useful attributes speciﬁed linguistic way It suggests following generalization 44 δx0λ cid3 inf xX πx cid3 δxλ cid4 45 cid3 generalized implication operator reasonably chosen Gödel implication 35 cid6 α cid3 β α cid1 β 1 β α β From logical point view 45 speciﬁes extent label λ supported plausible candidates x0 Notice distributions δx π 44 different semantics express degrees conﬁrmation plausibility respectively cf Section 2 Particularly π assumed normalized instance x πx 1 One obviously recovers 44 45 special case π 0 1 valued possibility distribution π IX0 corresponds crisp subset X0 X Similar generalizations realized coping incompletely speciﬁed examples Let ıth case memory characterized set Xı Lı X L Then 26 cid8 δx0λ inf cid19xλxcid20Xı Lı 1cid2ıcid2n σX x0 x σLλ λx accordance 44 Moreover obtain δx0λ cid8 1cid2ıcid2n cid1 max inf cid19xλxcid20X L σX x0 x σLλ λx 1 πıx λx cid2 ıth case characterized means possibility distribution πı X L crisp set Xı Lı Observe expression combined 45 order handle incomplete speciﬁcations sample cases new instance Moreover notice distribution δx0 generally remain unaffected example completely unspeciﬁed πı 1 clearly reasonable property See 27 thorough discussion handling incomplete information detailed derivation extensions E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 367 52 Discounting noisy atypical instances IBL sensitive noisy instances discarded 2 By noise generally means incorrect attribute value information concerning descriptive x case label λx However problem noise closely related typicality case A typical instance representative neighbors exceptional incorrect instance label different labels neighboring instances 85 Recall case cid19xı λxı cid20 S extrapolated placing support function possibilistic kernel 25 point cid19xı λxı cid20 X L like density kernel function centered observation kernelbased density estimation Of course representative noisy exceptional instance neighborhood smaller extent extrapolation A simple learning mechanism adapts extent extrapolation stored cases cid3 cid4 δı x0 λ cid22 mı σX x0 xı realized means slight generalization kernel function 25 σLλ λxı 46 Here mı 0 1 0 1 monotone increasing modiﬁer function mı1 1 This function allows discounting atypical cases Roughly speaking mı adapts similarity instance xı neighbors For example xı completely dissimilar instances letting mı0 1 0 Replacing σX modiﬁed measure mı σX closely related idea local distance measures NN algorithms Suppose new observation x0 label λx0 consider stored cid20 Should case discounted light new observation The fact case cid19xı λxı cid19xı λxı cid20 supports label different observed label λx0 need necessarily exclude x0 Cλ λ cid18 λ0 In ﬂaw In fact recall x0 Cλx0 words nonsupport observed support different label actually punished However punished disqualiﬁcation label λx0 expressed upper possibility model 31 Thus reasonable require degree disqualiﬁcation induced cid19xı λxı σLλx0 λxı cid2 β cid3 σX x0 xı cid20 bounded 1 mı 47 cid4 β 0 constant The constraint 47 suggests update scheme stored case cid19xı λxı cid20 cid20 Let F denote maybe discounted time new observation cid19x0 λx0 parameterized completely ordered class functions mı chosen An adaptation realized cid1 mı min cid1 mı sup cid3 cid4 cid2cid2 f F 1 f σX x0 xı σLλx0 λxı cid2 β 48 The discounting noisy atypical instances modifying possibilistic kernel functions appears natural somewhat simpler method IB3 2 Firstly possibilistic discounting gradual instance accepted rejected temporarily inbetween IB3 Secondly question discount instance extent answered naturally possibilistic approach support absolute graded In IB3 instance punished corresponding 368 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 Fig 2 Left The large circle corresponds support function possibilistic kernel centered xı marks extrapolation label λxı Right The support function updated observing new instance different label λx0 cid18 λxı supported decision based rule appears reasonable considered adhoc xı discounted DX xı x0 smaller equal distance x0 closest accepted neighbor32 The possibilistic adaptation scheme simple special case X Rl L 0 1 mı Iγı 1 0 cid1 γı 1 If σX strictly decreasing function Euclidean distance support function 25 corresponds ball xı λ 0 The δı x0 parameter γı chosen large possible support function cid18 λxı γı cid1 xı x holds true cover observed instance x λx x Fig 2 gives illustration l 2 λ 1 λ λx x0 located inside ball δı x0 This special case useful point departure investigating theoretical properties POSSIBL In 4 convergence properties IB1 shown special setup makes statistical assumptions generation training data geometrical assumptions concept C1 learned For POSSIBL prove similar properties assumptions More speciﬁcally let l 2 X 0 1 0 1 results generalized dimension l 2 bounded region X Rl consider concept C1 X For special case POSSIBL approximation C1 given cid18 Cest 1 Bρxı xı cid19xı 1cid20S Bd xı x X x xı d open dball xı cid1 ρxı min cid2 x xı cid19x λx Moreover approximation C0 X C1 given cid20 S λx cid18 λxı cid18 Cest 0 Bρxı xı cid19xı 0cid20S However Cest X necessarily Cest It readily veriﬁed Cest 0 1 0 0 instances x0 X hold true Thus δx0 classiﬁed random Consequently approximation concept C1 actually Cest 1 32 Auxiliary rules x0 accepted neighbor 49 50 51 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 369 1 divides instances x0 X groups represented tuple Cest Those supposedly belong C1 δx00 0 δx01 1 δx00 1 δx01 0 evidence available far δx0 0 Cest 0 0 Cest 1 Now ﬁrst desirable property convergence concept approximation convergence Cest C0 C1 respectively In context property convergence weakened exact convergence achieved fact NN classiﬁer guarantee avoidance wrong decisions boundary concept Moreover assumptions generation samples geometry concept C1 Here assumptions 4 Instances generated randomly independently according ﬁxed probability measure µ X Furthermore C1 concept having nice boundary union ﬁnite number closed hypercurves ﬁnite size We employ following notation The εneighborhood C1 set C 1 ε cid2 cid1 x X Bεx C1 cid18 εcore C1 deﬁned cid1 x X Bεx C1 C 1 ε cid2 cid3 A set A X called ε γ approximation C1 measurable set N X µN cid1 γ 1 ε N C Finally let Cest 49 51 S n n observations 0n denote respectively possibilistic concept approximations 1n Cest 1 ε N C A N cid4 cid4 cid3 Lemma 2 The equalities 1 ε X C C 0 ε hold true 0 ε 1 C 0 ε X C 1 ε Proof For x C x1 C1 Consequently x0 C0 x x0 ε x C Now suppose x X C means x x1 ε implies x1 C1 x C shown way 1 ε Bεx C1 means x x1 ε implies 0 ε 0 ε Thus x0 C0 x x0 ε 1 ε The second equality Theorem 3 Let C1 X 0 ε γ d 1 There integer n0 following holds true probability 1 d The possibilistic concept approximation Cest 1n 2ε γ approximation C1 Cest 0n 2ε γ approximation C0 n n0 Proof Let N denote set instances x X sample xı S exists x xı ε In 4 following lemma shown µN cid1 γ holds true probability 1 d n n0 2ε2γ 2 ln 2ε2d cid3 cid4 52 370 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 Subsequently ignore set N formally replace X X N C1 C1 N C0 C0 N Thus following holds true deﬁnition For x X instance xı S x xı ε Now consider instance x C 1n Let xı S instance x xı ε For instance xı Bεx C1 means xı belongs C1 Furthermore Bεxı B2εx C1 ρxı cid2 ε value 50 This implies x Bρxı xı x Cest 1n Thus shown C 1 2ε We x Cest 1 2ε Cest 1n Since arguments apply C0 property C 0n shown 0 2ε Cest analogous way Thus Lemma 2 Cest 1n X Cest 0n X C Likewise shows Cest 0n 1 2ε 0 2ε C 0 2ε C Roughly speaking Theorem 3 guarantees 2εcore C0 C1 classiﬁed correctly high probability sample S large In words classiﬁcation errors occur boundary region For able quantify probability error necessary restrictions size boundary region probability distribution µ Thus let C denote class concepts C1 X represented union ﬁnite set regions bounded closed curves total length L 4 Moreover let Pβ denote class probability distributions µ X µA cid1 µLA β Borelsubsets A X µL Lebesgue measure β 0 Theorem 4 The concept class C polynomially learnable respect Pβ means 0 Cest possibilistic concept approximation Cest 1 1 2ε C Proof If C1 C size region C 1 2ε bounded 4εL Consequently probability area α 4εLβ Since classiﬁcation error occur region set N deﬁned Theorem 3 probability N γ probability classiﬁcation error bounded α γ Now ﬁx parameters γ ε follows γ e2 ε e8Lβ By substituting parameters 52 ﬁnds required sample size n polynomial 1e 1d In summary following holds true 0 e d 1 C1 C µ Pβ If n1e 1d examples presented n polynomial function 1e 1d probability 1 d possibilistic concept approximation classiﬁcation error e This precisely claim theorem 53 From instances rules Selecting appropriate instances stored memory pruning training set important issues IBL strong inﬂuence performance Especially reducing size memory necessary order maintain efﬁciency The basic idea remove instances actually necessary achieve good E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 371 concept descriptions For example imagine concept having form circle twodimensional instance space To classify inner points correctly means kNN rule sufﬁcient store positive examples concept near boundary In connection POSSIBL support absolute relative deleting instances memory produce holes concept description An interesting alternative allows reduce size memory time ﬁll holes concept description interpolation based idea merging instances generalizing cases rules This idea appears particularly reasonable possibilistic estimation principle closely related fuzzy rulebased reasoning More precisely observation interpreted fuzzy rule instance fuzzy metarule suggesting similar instances similar labels δ2 x0 To illustrate onetoone correspondence rules cases POSSIBL let X R L 0 1 suppose instances x1 4 x2 6 label 0 observed The possibilistic kernels 25 induced cases shown Fig 3 The ﬁrst case equivalent fuzzy rule If x0 approximately 4 λ 0 fuzzy set approximately 4 modeled possibility distribution δ1 individual x0 support function 25 The rules associated cases merged rule If x0 5 λ 0 fuzzy set 5 modeled δ2 pointwise maximum δ1 x0 x0 Fig 3 right δ1 x0 The procedure closely related techniques proposed connection IBL Viewing cases maximally speciﬁc rules idea generalizing cases rules forward 2122 The method proposed 64 generalizes cases placing rectangles different size A new instance labeled nearest rectangle nearest case This similar approach rectangles replaced possibility distributions Relations exist idea merging nearest neighbors class generating new pseudosample prototypes 11 In example point 5 regarded pseudoinstance replacing 4 6 endowed modiﬁed support function In example Fig 3 summarizing rule exactly equivalent conjunction individual rules Of course merging procedure incorporate concepts approximation interpolation For example suppose x2 8 x2 6 The x I57 goes replacement δ1 x0 simple combination δ larger pointwise maximum δ1 δ2 x0 x0 6 05 1 δ6 This kind possibilistic induction rea δ1 x0 sonable allows incorporating background knowledge Particularly replacing convex hull δ x cid22 maxδ1 x0 6 δ2 x0 x δ2 x0 δ2 x0 Fig 3 Possibility distributions induced cases left middle distribution associated summarizing fuzzy rule right 372 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 possibilistic estimation δx0 convex hull advised multimodal distribu tion sense example Section 45 relation observable cases cf Section 31 known satisfy convexity constraint form x Cλ Cλcid17cid17 x Cλcid17 λ λcid17 λcid17cid17 As seen extensions discussed basically suggest maintains optimal rule base optimal case base including combination adaptation rules These extensions wellsuited discounting instances discussed Section 52 Indeed deriving rule instances rules accomplished replacing pseudoinstance deﬁning appropriate modiﬁer function m pseudoinstance Still extensions direction premature implemented POSSIBL 6 Experimental studies 61 Preliminaries This section presents experimental studies providing evidence POSSIBLs excellent performance practice We like emphasize meant exhaustive comparative study covering competing learning algorithmsand showing POSSIBL superior competitors Apart fact empirical studies clearly limited evidence33 realize primary motivation underlying POSSIBL εimprovement classiﬁcation accuracy enrichment IBL concepts possibilistic reasoning clearly exclude Besides following points mind Firstly POSSIBL developed statistical framework Thus type problems POSSIBL suitable example Section 45 represented best way standard public data sets commonly testing performance Secondly important aspect possibilistic approach knowledge representation But aspect neglected ifas experimental studiesonly correctness ﬁnal decision classiﬁcation accuracy counts estimated distribution Thirdly comparison IBL algorithms appear dubious POSSIBLin general formis extension IBL covers speciﬁc algorithms kNN special cases Due difﬁculties decided apply basic version POSSIBL data sets UCI repository employ kNN resp IB1 algorithm reference use kNN k 1 3 5 weighted 5NN rule weight function 10 Thus refrained tuning degrees freedom order optimize performance POSSIBL exception experimental study presented Section 64 Instead applied original maxmin version 20 extended 33 It known algorithm selective superiority 10 Thus ﬁnd data sets certain algorithm tuned appropriately performs better E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 373 learning scheme presented Section 52 The function mı 46 deﬁned t cid22 expγı1 t γı cid2 0 discounting rate ıth instance The constant β 47 taken 0834 In order avoid difﬁculties different handling nonnominal class labels deﬁnition similarity measures nonnumeric attributes restricted data sets predictive attributes numeric class label deﬁned nominal scale The similarity σX deﬁned 1 minus normalized Euclidean distance similarity σL given 23 62 Classiﬁcation accuracy The experiments section performed follows In single simulation run data set divided random training set case base test set discounting rates γı adapted training set A decision derived element test set extrapolating training set adapting discounting rates expanding case base percentage correct decisions determined Statistics obtained means repeated simulation runs Results summarized means statistics percentage correct classiﬁca tions mean standard deviation minimum maximum 01fractile 09fractile Ta bles 15 The experiments POSSIBL achieves comparatively good results best algorithms Thus said basic version POSSIBL performs Table 1 BALANCE SCALE DATABASE 625 observations 4 predictive attributes classes training set size 300 1000 simulation runs Algorithm mean std min max 01frac 09frac POSSIBL 1NN 3NN 5NN W5NN 08776 07837 08117 08492 07864 00148 00161 00165 00155 00164 08215 07323 07630 08030 07294 09230 08369 08707 08923 08428 08584 07630 07907 08307 07655 08984 08030 08338 08707 08067 Table 2 IRIS PLANT DATABASE 150 observations 4 predictive attributes classes training set size 75 10000 simulation runs Algorithm mean std min max 01frac 09frac POSSIBL 1NN 3NN 5NN W5NN 09574 09492 09554 09586 09561 00204 00196 00175 00181 00187 08400 08400 08666 08533 08400 10000 10000 10000 10000 10000 09333 09200 09333 09333 09333 09733 09733 09733 09866 09733 34 Variations parameter signiﬁcant inﬂuence 374 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 Table 3 GLASS IDENTIFICATION DATABASE 214 observations 9 predictive attributes seven classes training set size 100 10000 simulation runs Algorithm mean std min max 01frac 09frac POSSIBL 1NN 3NN 5NN W5NN 06841 06870 06441 06277 06777 00419 00410 00421 00412 00414 05300 05200 04800 04800 05000 08400 08200 08100 07800 08300 06300 06300 05900 05700 06200 07400 07400 07000 06800 07300 Table 4 PIMA INDIANS DIABETES DATABASE 768 observations 8 predictive attributes classes training set size 380 1000 simulation runs Algorithm mean std min max 01frac 09frac POSSIBL 1NN 3NN 5NN W5NN 07096 06707 06999 07190 06948 00190 00199 00183 00183 00188 06421 06132 06447 06553 06421 07711 07289 07500 07684 07474 06868 06447 06763 06947 06684 07316 06947 07237 07421 07184 Table 5 WINE RECOGNITION DATA 178 observations 13 predictive attributes classes training set size 89 1000 simulation runs Algorithm mean std min max 01frac 09frac POSSIBL 1NN 3NN 5NN W5NN 07148 07163 06884 06940 07031 00409 00408 00407 00392 00404 05506 05843 05506 05730 05730 08652 08652 08315 08090 08315 06629 06629 06404 06404 06517 07640 07640 07416 07416 07528 basic IBL NN algorithms In words possibilistic IBL way inferior standard IBL basis improvements sophisticated learning algorithms This exactly wanted Due special setting experimental studies especially choice max aggregation operator use 0 1valued similarity measure L wonder explain different performance POSSIBL NN classiﬁers In fact Section 44 argued possibilistic NN decision derived 20 actually equivalent 1NN rule applying maximum operator It recalled POSSIBL employed experiments involves adaptation absolute possibilistic support comes stored cases essence responsible differences A interesting ﬁnding following In examples classiﬁcation performance kNN algorithm generally increasing decreasing function k POSSIBL hand performs irrespective direction E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 375 tendency regardless smaller larger neighborhood called This taken indication robustness possibilistic approach 63 Statistical assumptions robustness Let elaborate little closely aspect robustness Above claimed possibilistic approach robust methods violations statistical assumptions independence end Section 432 This clearly true possibilistic estimation δx0 informational content remains meaningful data independent Here like provide experimental evidence supposition possibilistic approach advantageous estimation decision making point view sample fully representative population The experimental setup determined follows The instance space deﬁned X R set labels L 1 1 class probabilities 12 conditional probability density x given λx normal standard deviation 1 mean λx In single simulation run random sample size n 20 generated class probabilities 12 α 12 α respectively 0 α cid1 12 Based resulting training set fully representative sense 17 predictions derived 10 new instances These instances generated true class probabilities 12 For ﬁxed value α ﬁxed prediction method misclassiﬁcation rate rα derived averaging 10000 simulation runs Fig 4 shows misclassiﬁcation rates methods As expected r increasing function sample bias α The best results course obtained classprobabilities training set test set coincide α 0 The ﬁgure reveals sensitivity kNN classiﬁer increases k On hand true larger k leads better results α close 0 On hand performance decreases quickly smaller k k 1 preferred α close 12 This ﬁnding grasped intuitively The larger k kNN rule relies frequency information affected information misleading Fig 4 Misclassiﬁcation rates kNN methods left POSSIBL right comparison 1NN 376 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 Apart kNN methods tested POSSIBL P The similarity measure σX deﬁned triangle x y cid22 max0 1 x y08 Interestingly approach yields satisfactory results For α close 0 good kNN rules k 1 α close 12 equals 1NN rule Thus combination mode realized probabilistic sum α β cid22 α β αβ turns reasonable conditions experiment As explained Section 42 operator produces kind saturation effect It takes frequency information account limited extent larger current support smaller absolute increase new observation Thus inbetween 1NN rule kNN rules k 1 Intuitively explains ﬁndings experiment especially POSSIBL robust sample bias kNN rules k 1 64 Variation aggregation operator An interesting question concerns dependence POSSIBLs performance speciﬁcation aggregation operator 25 To ﬁrst idea dependence performed experiments described Section 62 Now tested POSSIBL different tconorms More precisely speciﬁed tconorm means parameter ρ 30 taken different aggregation operators Frankfamily tconorms POSSIBL applied data set different operators ρ The simulation results presented Figs 59 Each ﬁgure shows average classiﬁcation performance POSSIBL 100 experiments function parameter ρ Please note different scaling axes ﬁve data sets Conﬁrming previous considerations results general different t conorms optimal different applications Still POSSIBLs performance robust variation aggregation operator That classiﬁcation accuracy drop choosing suboptimal operator A interesting ﬁnding observation parameter ρ 0 maximum operator optimal simultaneously 1NN classiﬁer performs comparison kNN classiﬁers If case BALANCE Fig 5 Experimental results BALANCE SCALE data E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 377 Fig 6 Experimental results IRIS PLANT data Fig 7 Experimental results GLASS IDENTIFICATION data Fig 8 Experimental results PIMA INDIAN DIABETES data 378 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 Fig 9 Experimental results WINE RECOGNITION data SCALE PIMA INDIANS DIABETES data parameters ρ 0 achieve better results This ﬁnding astonishing grasped intuitively In fact mentioned POSSIBL 0 max closely related 1NN classiﬁer methods fully concentrate relevant information As opposed aggregation operators ρ ρ 0 combine information neighbors way kNN classiﬁers k 1 65 Representation uncertainty It mentioned important aspect POSSIBL concerns representa tion uncertainty The fact POSSIBL adequately represent ignorance related decision problem easily understood empirical validation To ﬁrst idea POSSIBLs ability represent ambiguity derived approxima tions characteristic quantities experimental setup described Section 61 Let D1 denote expected difference possibility degree predicted possibility degree second best label given prediction label λest x0 correct D1 δx0λx0 max λL λcid18λx0 δx0λ Moreover let D0 denote expected difference possibility degree predicted label λest possibility degree actually true label λx0 given x0 λx0 δx0 δx0λx0 cid18 λest x0 D0 cid4 cid3 λest x0 Ideally D0 small D1 large Wrong decisions accompanied large degree uncertainty reﬂected comparatively large support actually correct label As opposed correct decisions appear reliable reﬂected low possibility degrees assigned labels λ cid18 λx0 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 379 Table 6 Database D0 0094 BALANCE SCALE 0194 IRIS PLANT GLASS IDENTIFICATION 0181 PIMA INDIANS DIABETES 0211 0226 WINE RECOGNITION D1 0529 0693 0401 0492 0721 Table 6 shows approximations expected values D0 D1 averages 1000 experiments As seen reliability prediction reﬂected possibilistic estimations 7 Summary future work The idea underlying method presented paper extend instancebased learning concepts techniques possibility theory fuzzy sets Here idea realized form basic learning procedure called POSSIBL Apart discussing methodological aspects paper started investigation theoretical properties approach standard statistical assumptions validation POSSIBL means experimental studies The application possibility theory allows realizing graded version similaritybased extrapolation principle underlying IBL Not version appear natural intuitively appealing We presented detailed comparison possibilistic extrapolation principle commonly approach endowed probabilistic basis Even methods based different semantics POSSIBL formally seen extension probabilistic approach Indeed shown formerat general formcan mimic Apart possibilistic approach following advantages Knowledge representation A possibilistic instancebased prediction expres sive probabilistic Especially able represent absolute evidential support partial ignorance point major importance IBL Furthermore interpretation aggregated degrees individual support terms guaranteed possibility degrees conﬁrmation generally critical interpretation terms degrees probability Scope applications The possibilistic approach robust extends range applications Particularly makes statistical assumptions generation data mathematical assumptions structure underlying instance space In fact POSSIBL performs standard NN techniques typical realword data sets Beyond applied data violates certain statistical assumptions Finally maxmin version POSSIBL applied purely ordinal setting Support extensions The possibilistic method ﬂexible supports extensions IBL This includes adaptation aggregation modes 380 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 combination individual degrees support coherent handling incomplete information graded discounting atypical cases Moreover allows complement similaritybased extrapolation principle inference procedures In paper outlined extensions basic POSSIBL algorithm deserve investigation This concerns particularly ideas automatically adapt parameterized aggregation operator Section 422 complement lower possibility bounds means upper bounds Section 42 combination instance based rulebased inference Section 53 These extensions important topics ongoing research aims realizing efﬁcient framework plausible instance based learning basis possibility theory fuzzy sets In regard let mention idea supplementing IBL fuzzy setbased modeling techniques In fact methods 27 allow guiding extending instancebased learning means domain knowledge combining knowledge data ﬂexible way Parts possibilistic IBL framework realized connection PRETI project Platform Research Experimentation Treatment Information maintained INSTITUT DE RECHERCHE EN INFORMATIQUE DE TOULOUSE Acknowledgements The author wishes thank anonymous referees useful comments helped improve paper References 1 DW Aha Incremental instancebased learning independent graded concept descriptions Proc 6th Internat Workshop Machine Learning Ithaca NY Morgan Kaufmann San Mateo CA 1989 pp 387391 2 DW Aha Tolerating noisy irrelevant novel attributes instancebased learning algorithms Internat J ManMachine Stud 36 1992 267287 3 DW Aha Ed Lazy Learning Kluwer Academic Dordrecht 1997 4 DW Aha D Kibler MK Albert Instancebased learning algorithms Machine Learning 6 1 1991 37 66 5 T Bailey AK Jain A note distanceweighted knearest neighbor rules IEEE Trans Systems Man Cybernet 8 4 1978 311313 6 M Béreau B Dubuisson A fuzzy extended knearest neighbors rule Fuzzy Sets Systems 44 1991 1732 7 JC Bezdek Pattern Recognition Fuzzy Objective Function Algorithm Plenum Press New York 1981 8 JC Bezdek K Chuah D Leep Generalized knearest neighbor rules Fuzzy Sets Systems 18 1986 237256 9 R Bradley N Swartz Possible Worlds Basil Blackwell Oxford UK 1979 10 CE Brodley Addressing selective superiority problem Automatic algorithm model class selection Proc 10th Machine Learning Conference 1993 pp 1724 11 CL Chang Finding prototypes nearest neighbor classiﬁers IEEE Trans Comput 23 11 1974 1179 1184 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 381 12 CK Chow On optimum recognition error reject tradeoff IEEE Trans Inform Theory 16 1970 4146 13 LJ Cohen An Introduction Philosophy Induction Probability Clarendon Press Oxford 1989 14 TM Cover PE Hart Nearest neighbor pattern classiﬁcation IEEE Trans Inform Theory 13 1967 2127 15 BV Dasarathy Nosing neighborhood A new structure classiﬁcation rule recognition partially exposed environments IEEE Trans Pattern Analysis Machine Intelligence 2 1 1980 6771 16 BV Dasarathy Ed Nearest Neighbor NN Norms NN Pattern Classiﬁcation Techniques IEEE Computer Society Press Los Alamitos CA 1991 17 ER Davies Training sets priori probabilities nearest neighbor method pattern classiﬁcation Pattern Recognition Lett 8 1 1988 1113 18 R Lopez Mantaras E Armengol Machine learning examples Inductive lazy methods Data Knowledge Engrg 25 1998 99123 19 T Denoeux A knearest neighbor classiﬁcation rule based DempsterShafer Theory IEEE Trans Systems Man Cybernet 25 5 1995 804813 20 JK Dixon Pattern recognition partly missing data IEEE Trans Systems Man Cybernet 9 10 1979 617621 21 P Domingos Rule induction instancebased learning A uniﬁed approach CS Mellish Ed Proc IJCAI95 Montreal Quebec Morgan Kaufmann San Mateo CA 1995 pp 12261232 22 P Domingos Unifying instancebased rulebased induction Machine Learning 24 1996 141168 23 D Dubois F Esteva P Garcia L Godo R Lopez Mantaras H Prade Fuzzy set modelling casebased reasoning Internat J Intelligent Syst 13 1998 345373 24 D Dubois P Hajek H Prade Knowledge driven vs data driven logics J Logic Language Inform 9 2000 6589 25 D Dubois E Hüllermeier H Prade Flexible control casebased prediction framework possibility theory E Blanzieri L Portinale Eds Advances CaseBased Reasoning Proc EWCBR 2000 5th European Workshop CaseBased Reasoning Trento Italy Springer Berlin 2000 pp 6173 26 D Dubois E Hüllermeier H Prade Formalizing casebased inference fuzzy rules SK Pal DY So T Dillon Eds Soft Computing CaseBased Reasoning Springer Berlin 2000 pp 4772 27 D Dubois E Hüllermeier H Prade Fuzzy setbased methods instancebased reasoning IEEE Trans Fuzzy Systems 10 3 2002 322332 28 D Dubois H Prade Fuzzy sets statistical data European J Oper Res 25 1986 345356 29 D Dubois H Prade Possibility Theory Plenum Press New York 1988 30 D Dubois H Prade Possibility theory basis preference propagation automated reasoning Proc 1st IEEE Internat Conference Fuzzy Systems FUZZIEEE92 San Diego CA 1992 pp 821 832 31 D Dubois H Prade When upper probabilities possibility measures Fuzzy Sets Systems 49 1992 6574 32 D Dubois H Prade What fuzzy rules use Fuzzy Sets Systems 84 1996 169185 33 D Dubois H Prade Possibility theory Qualitative quantitative aspects DM Gabbay P Smets Eds Handbook Defeasible Reasoning Uncertainty Management Systems Vol 1 Kluwer Academic Dordrecht 1998 pp 169226 34 D Dubois H Prade P Smets Not impossible vs guaranteed possible fusion revision Proc ESCQARU2001 Toulouse France Lecture Notes Comput Sci Vol 2143 Springer Berlin 2001 pp 522531 35 D Dubois H Prade L Ughetto A new perspective reasoning fuzzy rules NR Pal M Sugeno Eds Advances Soft Computing Proc AFSS International Conference Fuzzy Systems Calcutta India Lecture Notes Artiﬁcial Intelligence Vol 2275 Springer Berlin 2002 pp 111 36 B Dubuisson M Masson A statistical decision rule incomplete knowledge classes Pattern Recognition 26 1 1993 155165 37 SA Dudani The distanceweighted knearestneighbor rule IEEE Trans Systems Man Cybernet 6 4 1976 325327 38 E Fix JL Hodges Discriminatory analysis nonparametric discrimination consistency principles BV Dasarathy Ed Nearest Neighbor NN Norms NN Pattern Classiﬁcation Techniques IEEE Computer Society Press Los Alamitos CA 1991 Reprint original work 1951 382 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 39 E Fix JL Hodges Discriminatory analysis Nonparametric discrimination Small sample performance BV Dasarathy Ed Nearest Neighbor NN Norms NN Pattern Classiﬁcation Techniques IEEE Computer Society Press Los Alamitos CA 1991 Reprint original work 1952 40 JH Friedman F Baskett LJ Shustek An algorithm ﬁnding nearest neighbors IEEE Trans Comput 24 1975 10001006 41 PE Hart The condensed nearest neighbor rule IEEE Trans Inform Theory 14 1968 515516 42 ME Hellman The nearest neighbor classiﬁcation rule reject option IEEE Trans Systems Man Cybernet 6 1970 179185 43 E Hüllermeier Toward probabilistic formalization casebased inference T Dean Ed Proc IJCAI 99 Stockholm Sweden Morgan Kaufmann San Mateo CA 1999 pp 248253 44 E Hüllermeier On representation combination evidence instancebased learning Proc ECAI2002 15th European Conference Artiﬁcial Intelligence Lyon France IOS Press Amsterdam 2002 pp 360364 45 D Hume An Enquiry concerning Human Understanding Oxford University Press New York 1999 46 A Józwik A learning scheme fuzzy kNN rule Pattern Recognition Lett 1 1983 287289 47 JM Keller MR Gray JA Givens A fuzzy knearest neighbor algorithm IEEE Trans Systems Man Cybernet 15 4 1985 580585 48 D Kibler DW Aha Instancebased prediction realvalued attributes Comput Intelligence 5 1989 5157 49 BS Kim SB Park A fast k nearest neighbor ﬁnding algorithm based ordered partition IEEE Trans Pattern Analysis Machine Intelligence 8 6 1985 761766 50 JL Kolodner Casebased Reasoning Morgan Kaufmann San Mateo CA 1993 51 R Krishnapuram JM Keller A possibilistic approach clustering IEEE Trans Fuzzy Systems 1 2 1993 98110 52 DK Lewis Counterfactuals comparative possibility J Philos Logic 2 1973 53 DO Loftsgaarden CP Quesenberry A nonparametric estimate multivariate density function Ann Math Stat 36 1965 10491051 54 J Macleod A Lik D Titterington A reexamination distanceweighted knearest neighbor classiﬁcation rule IEEE Trans Systems Man Cybernet 17 4 1987 689696 55 E McKenna B Smyth Competenceguided edition methods lazy learning Proc 14th European Conference Artiﬁcial Intelligence ECAI2000 Berlin 2000 pp 6064 56 TM Mitchell The need biases learning generalizations Technical Report TR CBMTR117 Rutgers University New Brunswick NJ 1980 57 I Niiniluoto Analogy similarity scientiﬁc reasoning DH Helman Ed Analogical Reasoning Kluwer Academic Dordrecht 1988 pp 271298 58 G Parthasarathy BN Chatterji A class new KNN methods low sample problems IEEE Trans Systems Man Cybernet 20 3 1990 715718 59 E Parzen On estimation probability density function mode Ann Math Statist 33 1962 1065 1076 60 EA Patrick FP Fischer A generalized knearest neighbor rule Inform Control 16 2 1970 128152 61 JR Quinlan Unknown attribute values induction Proc 6th International Workshop Machine Learning Morgan Kaufmann San Mateo CA 1989 pp 164168 62 R Quinlan Combining instancebased modelbased learning Proc 10th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1993 pp 236243 63 M Rosenblatt Remarks nonparametric estimates density function Ann Math Statist 27 1956 832837 64 S Salzberg A nearest hyperrectangle learning method Machine Learning 6 1991 251276 65 E Sanchez On possibility qualiﬁcation natural languages Inform Sci 15 1978 4576 66 G Shafer A Mathematical Theory Evidence Princeton University Press Princeton NJ 1976 67 D Shepard A twodimensional interpolation function irregularly spaced data Proc 23rd National Conference ACM 1968 pp 517523 68 BW Silverman Density Estimation Statistics Data Analysis Chapman Hall London 1986 69 P Smets R Kennes The transferable belief model Artiﬁcial Intelligence 66 1994 191234 70 C Stanﬁll D Waltz Toward memorybased reasoning Comm ACM 1986 12131228 E Hüllermeier Artiﬁcial Intelligence 148 2003 335383 383 71 M Tan Costsensitive learning classiﬁcation knowledge application robotics Machine Learning 13 7 1993 734 72 I Tomek A generalization kNN rule IEEE Trans Systems Man Cybernet 6 1976 121126 73 VN Vapnik Statistical Learning Theory Wiley New York 1998 74 MP Wand MC Jones Kernel Smoothing Chapman Hall London 1995 75 J Weisbrod A new approach fuzzy reasoning Soft Comput 2 1998 8999 76 S Wess KD Althoff G Derwand Using kd trees improve retrieval step casebased reasoning S Wess KD Althoff MM Richter Eds Topics CaseBased Reasoning Springer Berlin 1994 pp 167181 77 D Wettschereck DW Aha T Mohri A review empirical comparison feature weighting methods class lazy learning algorithms AI Rev 11 1997 273314 78 DL Wilson Asymptotic properties nearest neighbor rules edited data IEEE Trans Systems Man Cybernet 2 3 1972 408421 79 DR Wilson Advances instancebased learning algorithms PhD Thesis Department Computer Science Brigham Young University Provo UT 1997 80 DR Wilson TR Martinez Improved heterogeneous distance functions J Artiﬁcial Intelligence Res 6 1997 134 81 TP Yunck A technique identify nearest neighbors IEEE Trans Systems Man Cybernet 6 10 1976 678683 82 LA Zadeh Fuzzy sets basis theory possibility Fuzzy Sets Systems 1 1978 328 83 LA Zadeh PRUF A meaning representation language natural language Internat J ManMachine Stud 10 1978 395460 84 LA Zadeh Toward theory fuzzy information granulation centrality human reasoning fuzzy logic Fuzzy Sets Systems 90 2 1997 111127 85 J Zhang Selecting typical instances instancebased learning Proc 9th International Conference Machine Learning ICML92 Aberdeen Scotland 1992 pp 470479 86 J Zhang Y Yim J Yang Intelligent selection instances prediction lazy learning algorithms Artiﬁcial Intelligence Rev 11 1997 175191