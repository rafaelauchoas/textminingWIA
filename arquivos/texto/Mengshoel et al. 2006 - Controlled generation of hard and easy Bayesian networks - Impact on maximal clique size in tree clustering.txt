Artiﬁcial Intelligence 170 2006 11371174 wwwelseviercomlocateartint Controlled generation hard easy Bayesian networks Impact maximal clique size tree clustering Ole J Mengshoel David C Wilkins b Dan Roth c RIACS NASA Ames Research Center Mail Stop 2693 Moffett Field CA 94035 USA b Center Study Language Information Stanford University Stanford CA 94305 USA c Department Computer Science University Illinois UrbanaChampaign 201 N Goodwin Urbana IL 61801 USA Received 1 July 2005 received revised form 20 September 2006 accepted 24 September 2006 Available online 30 October 2006 Abstract This article presents analyzes algorithms systematically generate random Bayesian networks varying difﬁculty levels respect inference tree clustering The results relevant research efﬁcient Bayesian network inference computing probable explanation belief updating allow controlled experimentation determine impact improvements inference algorithms The results relevant research machine learning Bayesian networks support controlled generation large number data sets given difﬁculty level Our generation algorithms called BPART MPART support controlled random construction bipartite multipartite Bayesian networks The Bayesian network parameters vary total number nodes degree connectivity ratio number nonroot nodes number root nodes regularity underlying graph characteristics conditional probability tables The main dependent parameter size maximal clique generated tree clustering This article presents extensive empirical analysis HUGIN tree clustering approach theoretical analysis related random generation Bayesian networks BPART MPART 2006 Elsevier BV All rights reserved Keywords Probabilistic reasoning Bayesian networks Tree clustering inference Maximal clique size CV ratio Random generation Controlled experiments 1 Introduction Essentially inference problems studied Bayesian network BN formalism known computa tionally hard general case 146066 Given central role BNs wide range automated reasoning ap plications example medical diagnosis 34367 probabilistic risk analysis 945 language understanding 10 12 intelligent data analysis 405461 error correction coding 27284849 biological pedigree analysis 68 developing efﬁcient algorithms inference problems important research problem The performance exact Bayesian network inference algorithmsincluding tree clustering algorithms 23338394665 conditioning algorithms 16172232575864 elimination algorithms 194772depends treewidth optimal Corresponding author Email addresses omengshoelriacsedu OJ Mengshoel dwilkinsstanfordedu DC Wilkins danrcsuiucedu D Roth 00043702 matter 2006 Elsevier BV All rights reserved doi101016jartint200609003 1138 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 maximal clique size BNs induced clique tree 5172021 Treewidth initially theoretical concept related graph minors 59 recently established notion treewidth plays key role analysis algorithms 82044 A signiﬁcant component research inference BNs experimental rely use BN instances Similar experiments needed performed problems including satisﬁability prob lem SAT 11326556263 For SAT established empirically phase transition probability satisﬁability instance drawn certain distribution 55 This phase transition phenomenon closely related parameter describing constrainedness instances ratio tween number variables V number clauses C denoted CV ratio Interestingly algorithmic hardness varies CV ratio certain algorithms 55 As CV ratio var ied variation problem difﬁculty hardness measured mean median inference time certain algorithms sample problems Maximal hardness algorithms occurs phase transition region Experimental work Bayesian network inference performed randomly generated instances In article investigate following research questions How BNs experimentation randomly generated computational hardness understood analyzed controlled More speciﬁcally fruitful generalize CV ratio SAT BN setting If relationship CV ratio treewidth maximal clique size Answering research questions important reasons Generating problem instances randomly common practice BN community 717343541566970 result easy inference problems present challenge inference algorithms 41125 worst case complexity results exact approximate MPE computation NPhard 166 In article extend previous research randomly generating BN instances present experimental paradigm systematically generating increasingly hard random Bayesian network instances tree clustering We algorithms controlled generation BNs bipartite BPART multipartite MPART construction algorithms prove properties BNs construct For BPART case includes distribution root node outdegrees minimum outdegree small probability irregular BN regular For MPART networks 41 analyze relationship BPART BNs particular present formula probability MPART BN bipartite We characterize properties BN generation algorithms order better understand factors turn contribute hardness inference thorough benchmarking comparison algorithms performed The inference approach focus tree clustering implemented HUGIN algorithm introduced belief updating algorithm 46 later extended encompass belief revision 18 Thus tree clustering approach computing marginal distributions probable explanations MPEs closely related In particular depend total clique tree size maximal clique size BNs clique tree In BN let V number root nodes C number nonroot nodes We CV ratio key parameter BN inference hardness SAT 1155 Analytically provide conservative lower bound total clique tree size introduce new class BNs onlychild BNs sufﬁcient conditions Hamiltonicity longest cycle Formation cycles including Hamiltonian cycles important need ﬁllin edges order triangulated graph constructed cycles signiﬁcantly contribute clique tree size Even topology restricted BPART MPART types identify input parameters varied randomly generating BNs We empirically study parameters changing affects properties generated BNs increase computational hardness tree clustering For BPART MPART constructions generating random networks result easy instances careful selection parameters dimensions discuss keeping size networks ﬁxed gradually increases complexity inference results networks tree clustering algorithm handle A main empirical result CV ratio predict upper bound treewidth optimal maximal clique size induced clique trees samples BPART MPART BNs Our selection families hard networks extends research generating hard instances satisﬁability problem 4 112555 existing research BN community 344170 Increasing CV ratio causes certain values C V approximately linear increase upper bound treewidth number nodes largest clique In words obtain easyhardharder pattern tree clustering algorithms including HUGIN contrasts easyhardeasy pattern observed SAT formulas DavisPutnam algorithm 55 Experimenters use CV ratio directly instead complement maximal clique size treewidth OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1139 In addition CV ratio study regularity BNs underlying graph distributional nature conditional probability tables The rest article organized follows Section 2 introduces Bayesian network deﬁnitions notation MPE problem In Section 3 brieﬂy inference particular tree clustering HUGIN algorithm concepts maximal clique size treewidth Section 4 discusses use application BNs randomly generated BNs experimentation In particular Bayesian networks generated BPART algorithm MPART algorithm presented analyzed results relationship Section 5 discusses interaction properties randomly generated BNs hardness tree cluster ing algorithms HUGIN particular In Section 6 turn experimental article experimental results BPART MPART networks stateoftheart inference HUGIN study characteristics maximal clique sizes generated inference times Section 7 concludes discusses future work Earlier versions research reported previously 5152 In closely related work developed investigated stochastic local search approach computing MPEs compared tree clustering varying CV ratios 5153 2 Preliminaries A Bayesian network BN represents multivariate probability distribution directed acyclic graph DAG nodes represent random variables Deﬁnition 1 Directed acyclic graph DAG Let G X E directed acyclic graph DAG nodes X X1 Xn edges E E1 Em An ordered tuple Ei Y X 1 cid2 cid2 m X Y X represents directed edge Y X Here ΠX denotes parents X ΠX Y Y X E Similarly ΨX denotes children X ΨX Z X Z E The outdegree indegree node X oX ΨX iX ΠX respectively The minimal nonzero outdegree node G denoted δoG minimal nonzero indegree node G denoted δiG nG X number nodes G The following characterization graphs general BNs particular turns fruitful analyzing performance inference algorithms BNs Deﬁnition 2 Root node nonroot node leaf node Let G nonempty DAG let X node G If iX 0 X root node If iX 0 X nonroot node If iX 0 oX 0 X leaf node Any nonempty DAG G root node V cid3 1 CV ratio welldeﬁned non DAGs according Deﬁnition 2 Only nonempty graphs considered rest article In important special case bipartite DAGs formally introduce CV ratio ratio number leaf nodes number root nodes Deﬁnition 3 Bipartite DAG Let G X E DAG If X split partite sets V X X iX 0 root nodes C X X iX 0 leaf nodes V C E V V C C G bipartite DAG B set bipartite DAGs In tree clustering algorithms return Section 3 directed graph BN transformed undi rected graph clique tree inference performed Deﬁnition 4 Undirected graph Let G X E undirected graph nodes X X1 Xn edges E E1 Em An undirected edge Ei X Y set 1 cid2 cid2 m X Y X The set adjacent neighbor nodes node X denoted aX Y X Y E degree dX aX number neighbors X G Finally δG minimal degree nodes G nG X number nodes G 1140 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 In article usually distinguish BNs graph node corresponding random variable For purpose article focus exclusively labelled graphsgraphs distinguishable vertices directed undirected cases When parts graph BN studied following notion induced subgraph useful Deﬁnition 5 Induced subgraph Let X nodes directed graph The product X X deﬁned Xi Xj Xi Xj X Let Y nodes undirected graph The product Y Y deﬁned Yi Yj Yi Yj Y Let G Z E directed undirected graph The induced subgraph GW W EW graph nodes W Z edges EW W W E We extend graph notation deﬁnitions BNs understanding apply graph formal deﬁnition BNs follows First following deﬁnition Deﬁnition 6 BN node A discrete BN node X random variable discrete ﬁnite state space ΩX x1 xk While BN nodes continuous article restricted discrete case BN node mean discrete BN node following Deﬁnition 7 Bayesian network A Bayesian network tuple β X E P X E DAG associated set conditional probability distributions P PrX1 ΠX1 PrXn ΠXn Here PrXi ΠXi conditional probability distribution Xi X Further let πXi represent instantiation parents ΠXi Xi The independence assumptions encoded X E imply joint probability distribution Prx Prx1 xn PrX1 x1 Xn xn ncid2 i1 Prxi πXi 1 Bayesian networks known belief networks Bayesian belief networks probabilistic networks ditional probability distribution PrXi ΠXi known conditional probability table CPT Sometimes BN provided observations evidence setting clamping m nodes O1 Om known states o O1 o1 Om om o1 om These nodes called observation nodes need considered computing explanation deﬁned Deﬁnition 8 Explanation Consider BN β X E P X X1 Xn observations o o1 om m cid2 n An explanation x assigns states nonevidence nodes Xm1 Xn x xm1 xn Xm1 xm1 Xn xn When discussing explanation x BN β x explanation easily understood left implicit Among explanations u probable ones particular Deﬁnition 9 Most probable explanation MPE Let x range explanations BN β Finding probable explanation MPE β problem computing explanation x Prx cid3 Prx The u probable explanations X x Prx Prx cid3 Prx 1 cid2 cid2 u 1 Prx u Prx 1 x u Here u X simply number MPEs BN explanation higher probability x X Since u 1 MPEs probability MPE MPE As common compute MPE x multiple MPEs exist BN Following Pearl denote computing MPE belief revision computing marginal distribution BN node denoted belief updating 58 It shown exact MPE computation NPhard 66 The problem relative approximation MPE ﬁnd assignment probability close MPE small ratio This problem proven NPhard 1 Belief updating computationally hard 1460 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1141 3 Inference Bayesian networks In addition arguments referring mapping SAT claim way generate hard easy BNs inference task needs supported analytically experimentally considering BN inference algorithms BN inference algorithms classiﬁed exact approximate Exact BN inference algorithms main focus article include tree clustering algorithms 23338394665 conditioning algorithms 16172232575864 elimination algorithms 194772 hybrid exact methods 20 For purpose article study prominent inference approachesthe tree clus tering approach speciﬁcally HUGIN algorithm1 Tree clustering discussed Section 31 Section 32 brieﬂy discusses exact BN inference algorithms 31 Inference tree clustering The role maximal clique size Tree clustering currently major approaches inference multiply connected Bayesian networks 58 Like tree clustering algorithms HUGIN algorithm employs phases compilation clustering phase propagation runtime phase 2373946 During compilation Bayesian network transformed cliques organized clique tree During propagation evidence propagated clique tree leading belief updating belief revision computations appropriate A clique junction tree constructed Bayesian network following way HUGIN algorithm First initial moral graph βcid6 constructed making undirected copy β augmenting follows Let X systematically range nodes β For node X HUGIN adds βcid6 edge pair nodes ΠX edge exists βcid6 Second HUGIN triangulates moral graph βcid6 creating triangulated graph βcid6cid6 Triangulation amounts adding ﬁllin edges moral graph βcid6 chordless cycle length greater exists Third clique tree βcid6cid6cid6 created triangulated graph βcid6cid62 This clique treewhich consists cliques separatorsmust exhibit property clique nodes F H tree nodes contain F H In βcid6cid6cid6 cliques separators belief tables associated joint probability PrX product clique belief tables divided separator belief tables The following quantities important characterizing computation clique tree 46 Deﬁnition 10 Clique tree parameters Let Γ set cliques clique tree βcid6cid6cid6 created BN β tree clustering The state space size clique H βcid6cid6cid6 g deﬁned cid2 g ΩH ΩX XH X node β The maximal number nodes clique βcid6cid6cid6 h deﬁned h sup H Γ H The total clique tree size total state space size k βcid6cid6cid6 deﬁned cid3 k ΩH H Γ cid8 maximal clique size maximal state space size clique βcid6cid6cid6 cid8 sup H Γ ΩH 2 3 4 5 1 For sake simplicity generally distinguish HUGIN algorithm ii HUGIN software implementation HUGIN algorithm In general article discusses HUGIN experimental parts article HUGIN algorithm 2 Some tree clustering algorithms HUGIN employ intermediate step right clique tree construction This intermediate step creates junction graph cliques triangulated graph 1142 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 A functional notation example kβcid6cid6cid6 cid8βcid6cid6cid6 article order βcid6cid6cid6 explicit For optimal minimal values h k cid8 h k cid8 respectively When parameters considered random variables letters G H K L Some investigations restricted nodes states S 2 case state space size g clique 2 simpliﬁes g ΩH cid2 ΩX S H 2 H XH When BN nodes S states maximal number h nodes clique number nodes clique maximal size cid8 Consequently distinguish quantities article general need kept distinct It easy cid8 Sh 2h 6 When BN highly connected networks considered article cliques clique tree large making tree clustering inference slow A crucial step process creating clique tree Bayesian network triangulationthe construction triangulated moral graph βcid6cid6 Triangu lation determines g h k cid8 Optimal triangulation including computation h unfortunately known NPhard heuristic algorithms MINIMUMFILLINWEIGHT MINIMUMFILLINSIZE MINI MUMCLIQUEWEIGHT MINIMUMCLIQUESIZE compute upper bounds h k cid8 practice perform triangulation 333742 HUGIN introduced belief updating algorithm 46 later extended MPE computation belief revision 18 essentially clique tree βcid6cid6cid6 cases Thus HUGIN approach computing marginal distributions computing MPEs closely related There main algorithmic differences First computing MPE x X maximization performed belief updating summation performed For purposes step essentially performance cases Second HUGIN belief revision cases multiple probable explanations X 1 perform propagation times 50 On hand propagation sufﬁcient HUGIN belief updating Of differences signiﬁcant impact computational cost propagation discussed Section 56 32 Inference maximal clique size treewidth The complexity exact Bayesian network inference algorithmsincluding tree clustering algorithms ditioning algorithms elimination algorithmshas depend treewidth cid9 optimal maximal clique size h cid9 h 1 2046 Time space complexity tree clustering exponential treewidth clique tree Conditioning algorithms 16172232575864 transform multiply connected graph singly connected graphs introducing cycle cutsets perform computations singly nected graph The time complexity conditioning minimal cycle cutset size c bounded treewidth cid9 inequality cid9 cid2 c 1 8 The time complexity elimination closely related tree clustering depends treewidth cid9 521 Finally hybrid algorithmscombining tree clustering conditioning eliminationthat trade space timecomplexity dependency treewidth 1720 In hybrid algorithm possible On space On expcid9 log n time On expcid9 space On expcid9 time gradual fashion 17 Unfortunately computing treewidth cid9 graph computationally hard In particular problem determining treewidth given graph bounded integer k shown NPcomplete 6 However possible empirically establish lower bounds treewidth upper bounds computed heuristics treewidth polynomial time 44 Optimal triangulation closely related computing treewidth triangulation heuristics play key role tree clustering algorithms discussed investigated experiments Section 6 4 Bayesian networks experimentation benchmarking There ways experiment inference algorithms BNs In section discuss main classes BNs experimentation literature application BNs randomly generated BNs While OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1143 essential argue application BNs limitations It nontrivial application BNs understand different BN parameters interact contribute inference complexity Using randomly generated BNs start addressing problems need sure BNs generated inference hardness canat extentbe controlled predicted In Section 41 deﬁne subsets set Bayesian networks Section 42 brieﬂy discusses BNs applica tions In Section 43 discuss approaches randomly generating Bayesian networks BPART algorithm MPART algorithm 41 Classes Bayesian networks It turns regularity BNs underlying graph varies applications major impact maximal clique size BN inference times In order discuss effect graph regularity BN inference introduce following terminology applies directed graphs general Deﬁnition 11 Regularity Let G X E directed graph If nodes X Y X indegrees iX iY 0 number parents iX iY G parentregular G UPR If G parentregular parentnonregular G UPN UPR UPN If G parentregular parentnonregular parentirregular G UPI UPI UPR UPN If nodes X Y X outdegrees oX oY 0 number children oX oY G childregular G UCR If G childregular childnonregular UCN UCR UCN If G childregular childnonregular childirregular G UCI UCI UCR UCN We note deﬁnitions allow nontrivial graph G parent regular parentirregular childregular childirregular G UPI UPR G UCI UCR UPI UPR UCI UCR non This turns simplify construction algorithms analysis Section 434 We probability BN element UCI element UCR words size UCI UCR extremely small constructions parameter values considered Based regularity concepts introduced Deﬁnition 11 following classes directed graphs BNs identiﬁed Deﬁnition 12 Class A Class B Class C Class D directed graphs A directed graph G Class A regular graph childregular parentregular G UA UA UCR UPR If G childirregular parent regular Class B irregular graph G UB UB UCI UPR If G childregular parent irregular Class C graph G UC UC UCR UPI If G childirregular parentirregular Class D unconstrained graph G UD UD UCI UPI The sets Class A Class B Class C Table 1 An informal presentation directed graphs including Bayesian networks orthogonal dimensions childregularity parent regularity leading following classes Class A Parentregular childregular Class B Parentregular childirregular Class C Parentirregular childregular Class D Parentirregular childirregular Class D unconstrained Parentregular set UPR Nonroot nodes number parents Parentirregular set UPI Nonroot nodes typically different number parents Childregular set UCR Nonleaf nodes number children Childirregular set UCI Nonleaf nodes typi cally different number children Class A Parentregular Childregular Class B Parentregular Childirregular UA UPR UCR Classical Gallager codes 2849 Regular kCNF Readcid8 formulas Regular multipartite graphs UB UPR UCI Modern Gallager codes 4849 Irregular kCNF formulas 55 Irregular multipartite graphs Biological pedigrees 68 Class C Parentirregular Childregular Class D Parentirregular Childirregular UC UPI UCR Readcid8 formulas UD UPI UCI Many application BNs Mixed CNF formulas 1144 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 Fig 1 Examples Class A Class B Class C Class D bipartite Bayesian networks BNs The Class A BN parentregular childregular regular BN Class B BN parentregular childnonregular childirregular irregular BN Class C BN parentnonregular parentirregular childregular Class D BN parentnonregular parentirregular childnonregular childirregular unconstrained Class D bipartite graphs BNs respectively introduced follows BA UA B BB UB B BC UC B BD UD B Table 1 summarizes families BNs including BNs error correction coding 27284849 classiﬁed classes UA UB UC UD The following example Fig 1 illustrate classes networks presented Deﬁnition 12 Table 1 Example 13 Fig 1 contains examples Class A Class B Class C Class D BNs In propositional logic notion readcid8 means variable read cid8 times clauses formula This concept Table 1 Also development information theory classical Gallager codes modern Gallager codes ﬁts framework presented Table 1 Gallagers original codes 28 encoded Class A BNs according terminology Modern Gallager codes 48 hand correspond Class B BNs Table 1 includes biological pedigree BNs 68 typically Class B BNs In BN representing pedigree nonroot nodes typically parents number children nonleaf node vary Note regularity easily gradual framework presented Table 1 For example use variance indegree outdegree measure regularity With general measure high variance means irregular low variance means regular In article concerned extreme cases leave variations future work We investigate minor relaxation introduced Deﬁnition 16 effect regularity considering Class A BNs Class B BNs We denote regular irregular BNs chance confusion 42 Bayesian networks applications BN inference algorithms studied empirically evaluating performance BNs applications 415156 For example BNs taken Friedmans Bayesian Network Repository httpwwwcshujiacillabscompbioRepository Application BNs obviously important performing experimental studies However believe difﬁcult understand performance BN inference al gorithm studying application BNs First problem dimensionality application BNs vary considerably topological distributional parameters It unclear learn pooling BNs different applications Second inference times vary signiﬁcantly application BNs general clear correlation BN parameters inference times 51 Third OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1145 number BNs application smalltypically BN application Restricting oneself BNs application desirable It difﬁcult obtain good statistics small samples There fundamental limitation associated use application BNs gold standard performance Some application BNs ﬁnetuned adequate performance existing inference algorithms It valuable construct BNs biased way order thoroughly characterize existing algorithms lay groundwork novel algorithms challenging applications 43 Bayesian network generation algorithms A potential solution limitations associated application BNs empirical research randomly generate BN instances 7173441566970 Using randomly generated BNs create BNs needed provide signiﬁcant evaluation This approach reduces problem dimensionality vary BNs generated dimensions time Issues related randomly generating BNs addressed remainder section In Section 431 present parameters partly control process randomly generating Bayesian networks In Sections 432 433 434 present BPART algorithm generate certain class bipartite networks mapping satisﬁability problem SAT fairly direct In Section 435 discuss MPART construction related BPART construction result studied similar point view In Section 436 discuss MPART BPART constructions related work 431 Input parameters Bayesian network generation algorithms Many parameters varied randomly generating Bayesian networks The following parameters cover topological distributional issues impact inferential hardness tree clustering discussed Section 5 correspond input parameters BPART MPART algorithms presented later section Number root nodes V BN The range integer 1 default value 30 For special case SATlike BNs formally deﬁned Section 432 root nodes correspond variables conjunctive normal form CNF formulas Number nonroot nodes C BN The range integer 0 default value 90 For SATlike BNs Section 432 nonroot nodes leaf nodes correspond clauses CNF formulas Conditional probability table CPT type Q F BN root nonroot nodes respectively The choices deterministic xor uniform random default value root nodes uniform nonroot nodes Section 56 contains discussion CPTs For experimentation article nonroot CPTs uniform root CPTs employed Section 65 CPTs random Childregularity R BN The choices Class B childirregular R false Class A childregular R true The default value childirregular R false Four classes Class A Class B Class C Class D BNs varying regularity identiﬁed article consider parentregular BNs Section 54 analyzes childregular case R true Section 64 presents experimental results R true remaining experiments Section 6 focus childirregular case use R false The number states node X BN S ΩX The range integer 1 default value S 2 boolean nodes For boolean nodes states loss generality called 0 1 false true respectively Experiments Section 6 restricted S 2 The indegree number parents P nonroot node X BN Given focus parentregular BNs number parents P nonroot node X parameter P iX ΠX The range integer 1 cid2 P cid2 V default value P 3 Experiments Section 6 use P 3 Section 65 P 2 In following assume constraints V C Q F R S P presented satisﬁed As example default values V 30 C 90 Q uniform F R false S 2 P 3 valid set input parameters These default values speciﬁc signature BPART algorithm BPARTuniform 30 90 2 false 3 1146 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 The following quantities easily derived input parameters presented The total number BN nodes N C V From V C CV ratio obtained The range 0 default value CV 3 Since V cid3 1 CV ratio welldeﬁned See Section 52 discussions CV ratio experiments Section 6 use CV ratios varying CV 075 CV 34 Given C P total number BN edges E C P giving EV C P V Since V cid3 1 EV ratio welldeﬁned The EV ratio generalizes CV ratio generally ﬁx P use CV ratio article EV shows analytical results Section 542 To consistent existing research literature randomly generating problem instances areas satisﬁability constraint satisfaction use uppercase V denote number root nodes uppercase C denote number nonroot nodes BN Neither V C nodes random variables Bayesian network uppercase letters represent concepts 432 The BPART network generation algorithm A synthetic bipartite construction For NPhard problems simply generating random instances undiscriminating fashion resulted fairly easy problem instances 41125 This problem addressed context satisﬁability seminal work Mitchell Selman Levesque 55 shown generate hard instances 3SAT Here ideas generalized generate turns hard instances belief revision belief updating tree clustering In section present approach randomly generating bipartite BNs varying hardness Fig 2 presents BPART construction algorithm generates SATlike Bayesian networks special case When BPART invoked following signature creates SATlike BNs Deﬁnition 14 SATlike BN Let β BPARTuniform V C 2 false P The BN β SATlike The BPART algorithm construct general BNs reﬂected following deﬁnition BPARTQ F V C S R P Input Q conditional probability table CPT type root nodes F V C S R P CPT type nonroot nodes number variables root nodes number clauses leaf nonroot nodes number states node create regular BNtrue false number parents clauses nonroot nodes Output β Bayesian network begin β CREATEBN ADDLAYERβ V 0 S falseFirst add layer root nodesV variables ADDLAYERβ C P S RSecond add layer child nodesC clauses 1 V C node GETNODEβ ROOTNODEnode SETDISTRIBUTIONnode S Q Set CPT root node SETDISTRIBUTIONnode S F Set CPT nonroot node end end return β end Fig 2 The BPART algorithm constructing synthetic bipartite BNs The input parameters Q F V C S R P create dif ferent variants BPART networks The BPART algorithm creates classical irregular SATlike BNs invoked parameters Q uniform F S 2 R false P 2 words follows BPARTuniform V C 2 false 2 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1147 Table 2 Algorithms Bayesian network construction algorithms Possible values parameter F SETDISTRIBUTION details CHOOSEFEWESTCHILDREN discussed text Function ADDX X ADDPARENTX Y CHOOSEFEWESTCHILDRENY CREATEBN CREATENODEβ GETLEAFNODESβ GETNODEβ GETNUMBEROFCHILDRENX GETNUMBEROFNODESX RANDOMINTL H ROOTNODEX SETDISTRIBUTIONX S F SETNUMBEROFSTATESX S Description Adds node X set nodes X sets X X X If possible adds Y ΠX returns true returns false Randomly chooses returns appropriate node nodes Y Returns new BN Returns newly created node BN β Returns nodes children BN β Returns ith node BN β assuming node ordering Returns number children BN node X Returns number BN nodes set nodes X Returns uniformly random natural number interval L H Returns true X root node returns false Sets distribution BN node X S states CPT type F Creates S states input BN node X Deﬁnition 15 BPART BNs The set BNs generated BPART deﬁned UBPART β β BPARTQ F V C S R P The set regular irregular BPART BNs respectively denoted U r β β BPARTQ F V C S true P U β β BPARTQ F V C S false P BPART BPART cid4 m i1 Ci clauses Ci Xi1 Building existing construction 1460 basic idea generalize conjunctive normal form CNF formula generate Bayesian network MPE corresponds satisfying assignment formula For SATlike BN given 3CNF formula f Xi3 construct bipartite Bayesian network layer nodes root nodes X corresponds variables second layer leaf nodes C corresponds clauses A variable Xj X edge directed Ci C iff Xj occurs clause Ci Clause nodes clamped inference The conditional probability tables set Prf 1 0 iff assignment Xj s satisﬁes 3CNF formula It easy happens conditional probability table associated node Ci simulates gate inputs To generate BN corresponds nonmonotone CNF CPTs need generalized accordingly straight forward way It easy verify MPE xan assignment values Xj shas positive probability iff satisﬁes corresponding 3CNF formula 1460 There satisfying assignments probability making MPEs Xi2 There ways generate random BNs corresponding CNF formulas Our BPART approach presented Fig 2 based following policy work V variables C clauses generate clauses selecting variables uniformly clauses negate variable probability p 05 55 Subroutines BPART include ADDLAYER Fig 3 CHOOSEFEWESTCHILDREN SETDISTRIBUTION Table 2 Turning BPART Fig 2 CREATEBN ﬁrst creates new BN β ADDLAYERβ V 0 S false adds β layer V root nodes root node S states Next ADDLAYERβ C P S R adds β C nonroot leaf nodes Each leaf node P parents chosen V root nodes The nature leaf nodes parent selection process takes place ADDLAYER controlled regularity parameter R true false Fig 3 presents layer BPART construction added ADDLAYER procedure ADDLAYER returns BN β new layer M nodes added works differently regular R true irregular R false cases discussed Section 433 Section 434 respectively The CPTs nodes constructed level BPART Fig 2 Nonroot node CPTs determined input parameter F F xor uniform random Similarly root node CPTs determined Q xor uniform random In deterministic CPT CPTs xor entries 0 1 In uniform CPT node X ΩX S states probability mass node state given parent instantiation 1S In random CPT probabilities ﬁrst picked uniform random U 0 1 distribution normalized sure conditional probabilities given parent instantiation sum 1 These types cover CPTs required provide exact mapping SAT problems corresponding 1148 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 Input BN new layer added ADDLAYERβ M P S R β M number new nodes create new layer P number parent nodes assign new node S number states new node true regular BN layer created false R Bayesian network layer nodes added Output β begin parents GETLEAFNODESβ 1 M Xi CREATENODEβ SETNUMBEROFSTATESXi S c 0 c P The new node Xi given parents R The irregular case R false j RANDOMINT1 GETNUMBEROFNODESparents parent parentsj The regular case R true parent CHOOSEFEWESTCHILDRENparents Pick parent fewest children end success ADDPARENTXi parent false parent Xi s parents success c c 1 end end end return β end Fig 3 The function ADDLAYER adds layer consisting M nodes Bayesian network β ADDLAYER invoked BPART BNs idealizations CPTs occur applications In Section 56 discuss relationship CPTs hardness inference There related slightly different perspectives BPARTs signature The ﬁrst perspective regard parametrized probability distribution B BNsas B BPARTuniform 30 90 2 false 3 The second perspective regard signature assignment generates sample β probability distribution B The assignment β BPARTuniform 30 90 2 false 3 reasonable assumptions periodicity pseudorandom number generator create different BN β time BPART invoked In general perspective taken section Section 5 perspective taken experimental article Section 6 sample B The BPART MPART construction algorithms presented section construct irregular regular BNs The setting R true gives regular BN R false gives irregular BN We discuss cases separately 433 The BPART regular case R true In ADDLAYER procedure new layer nodes regular R true CHOOSEFEWESTCHIL DREN procedure invoked shown Fig 3 CHOOSEFEWESTCHILDREN selects node X input nodes Y input node fewer children By algorithm ensures parent layer child regular close childregular More formally ensured BN underlying relaxed Class A graph deﬁned We introduce concepts closely related Deﬁnitions 11 12 order notion regularity widely applicable losing essence regularity Deﬁnition 16 Relaxed Class A directed graph Consider directed graph G V nonleaf nodes E edges If nonleaf node Xi 1 V oXi cid13 E V V G relaxed childregular If directed graph G relaxed childregular parentregular G relaxed Class A directed graph The set relaxed Class A graphs UA set bipartite relaxed Class A graphs BA UA B cid16 children notation oXi E cid14 oXi cid15 E V OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1149 The parameter value R true controls number children outdegree BPART root nodes presented following theorem corollary Theorem 17 Suppose BPART algorithm called R true If F edges distributed BN construction process BPART oX F V root node X Proof Suppose case This means exists root node X ΨX cid13 F cid14 V ΨX cid15 F cid16 fewer V children Assume purpose contradiction root nodes excluding X cid13 F cid14 V cid3 F cid14 This means exists root node Y cid15 F V cid14 children Since cid13 F V cid16 Consider case ΨX cid13 F V V number children edges V 1 root nodes cid5 cid6 V 1 F V cid3 V 1 F V F F V cid14 cid3 F Since number children X ΨX total number edges BN F F ΨX V possible ΨX cid13 F V contradiction Consequently exists root node Y V cid15 F cid16 edges However fact contradicts CHOOSEFEWESTCHILDREN operates CHOOSEFEWESTCHILDREN V picks leaf node parent fewest children However fact X ΨX cid13 F cid14 implies V earlier stage X picked CHOOSEFEWESTCHILDREN having cid13 F cid14 children At time V node Y ΨY cid2 cid15 F cid16 CHOOSEFEWESTCHILDREN chosen Y This V contradiction The proof case ΨX cid13 F V cid14 similar cid2 Using Theorem 17 formally characterize output BPART algorithm Fig 2 Corollary 18 With input parameter R true BPART algorithm creates bipartite relaxed Class A BNs U r BA BPART Proof Use F CP Theorem 17 apply Deﬁnition 16 cid2 Unless noted distinguish relaxed Class A BNs Class A BNs following denote Class A BNs In particular shall BPART given input parameter R true creates Class A BNs true strictly speaking according Deﬁnition 11 special case cid13 CP cid16 Corollary 18 The notion regularity impact inference hardness discussed V Section 54 turn irregular case cid14 cid15 CP V 434 The BPART irregular case R false If R false parent nodes chosen uniformly random replacement ADDLAYER procedure Fig 3 It easy bipartite Class B BNs generated Theorem 19 If R false BPART algorithm creates bipartite Class B BNs U BPART BB In BN parents nonroot node distinct Consequently given BPART leaf node ADD LAYER selects V V root nodes replacement obtain following Theorem 20 Exact child distribution Consider BN root nodes V leaf nodes C created BPART input parameter R false For X V let number children N ΨX let b binomial distribution It case N bC P V Proof Using V root nodes form nodes Vi1 ViP Vij V 1 cid2 cid2 distinct supernodes supernode Wi contains P different root 1 cid2 j cid2 P Clearly BPART performs C Bernoulli trials supernodes One Bernoulli trial arbitrary node X V considered success X cid7 V P cid8 cid8 cid7 V P cid8 cid7 V P 1150 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 element chosen supernode There probability Bernoulli trial success X cid7 cid8 V 1 P 1 supernodes X element Consequently p cid8 cid7 V 1 cid8 P P 1 cid7 V V P C trials obtain binomial distribution bC P V cid2 In following theorem introduce simplifying assumption justiﬁed parent selection ADDLAYER replacement assumed differently Theorem 20 Theorem 21 Approximate child distribution Consider BN root nodes V leaf nodes C created BPART R false As approximation suppose Y C Y s P parents picked indepen dently uniformly random V For X V let number children N ΨX let b binomial distribution It case N bCP 1V Proof Consider BN β root nodes V created BPART algorithm input R false In ADDLAYER invoked BPART algorithm ith leaf node Yi C selects P parent nodes As approxima tion assume parent picked independently Now consider particular root node Xk V Call success selection j th parent ith leaf node Yi Xk gets picked parent node failure Let Iij indicator random variable trial By assumption selection root node independent Bernoulli trial probability success p 1V obtain sequence Bernoulli random vari ables I11 I1P Ii1 IiP IC1 ICP The number times Xk picked random variable cid9 C P N Iij Clearly N binomial distribution bn p n CP trials probability success i1 j 1 p 1V cid2 cid9 The approximating assumption independence Theorem 21 justiﬁed follows As V random parent selection process ADDLAYER approaches drawing independently replacement probability picking root node twice tends zero Having introduced analyzed BPART construction return discussion Deﬁnition 11 classes UCI UCR UCN As example suppose β U BPART child node parents Clearly β UCI typically β UCN We typically happen parents randomly picked graph ends childregular chance β UCR As example root node β end exactly children assuming twice leaf nodes root nodes However Theorem 22 Example 23 probability β UCR extremely small β U BPART β generated parameter values P V C order magnitude experimental article Theorem 22 Consider BN β UCI root nodes V leaf nodes C created BPART input parameter R false Suppose Y C Y s P parents picked independently uniformly random V Also assume k CP V integer It case Prβ UCR CP CP V V cid11 CP cid10 1 V 7 Proof The desired joint distribution root nodes V clearly multinomial giving Prβ UCR PrN1 k NV k n kV pn Using Theorem 21 p 1V n CP 7 follows cid2 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1151 Unfortunately easy happens CP 7 However putting n CP Stirlings formula approximate factorial function 7 Prβ UCR 2πnnnen 2πnV V nV nen cid12 cid11 n cid10 1 V V V 2πnV 1 8 easy limn Prβ UCR 0 In words childirregular BPART BN β UCI generated β childnonregular high probability Prβ UCN 1 ε ε Prβ UCR characterized 7 8 Here example Example 23 Consider BPART BN β UCI constructed parameters R false P 3 V 30 C 90 Using Theorem 22 obtain Prβ UCR 161 1025 small probability Using 8 gives approximation Prβ UCR 2130 1025 We turn related different question How minimum outdegrees root nodes irregular BPART BN β characterized This question answered Theorem 25 fact parents leaf node picked random BPART algorithm Before stating result formally deﬁne minimal root node outdegree random variable M Deﬁnition 24 Minimum outdegree Let X X1 Xn set n BN nodes randomly distributed chil dren let Ni oXi The minimum outdegree random variable M deﬁned M minN1 Nn abbreviated M minX In following Mi irregular case R false Mr regular case R true Theorem 25 Expected minimum outdegree BPART Consider BN β root nodes V leaf nodes C created BPART R false Suppose Cj C Cj s P parents picked independently uniformly random V Let Mi minV The expectation EMi EMi cid15 CP cid16cid3 V j 1 cid8 cid7 V 1 Bj 1 CP 1V 9 Bk n p cumulative binomial distribution function PrX cid2 k X bn p Proof The probability value Ni k greater PrNi cid3 k 1 PrNi k 1 Bk 1 n p 10 B cumulative binomial distribution Recall Theorem 21 uses binomial distribution R false Considering V root nodes β minimum k V nodes need k greater PrMi cid3 k PrN1 cid3 k N2 cid3 k NV cid3 k Since approximation root nodes assumed picked independently uniformly random independence N1 NV By introducing N N1 NV 10 obtain PrMi cid3 k Vcid2 i1 PrNi cid3 k cid8 cid7 V 1 PrN cid2 k 1 cid8 cid7 V 1 Bk 1 n p Given random variable Mi possible values 0 1 m tail sum formula expectation EMi mcid3 j 1 PrMi cid3 j combining 11 12 yields mcid3 cid8 cid7 V 1 Bj 1 n p EMi j 1 11 12 1152 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 By substituting m cid15 CP V cid16 n CP p 1 V obtain 9 desired cid2 The following example illustrates Theorem 25 parameter values experiments article Example 26 Expected minimum outdegree BPART Consider BPART network constructed input pa rameters C 60 V 30 P 3 R false Using Theorem 25 obtain EMi 172 If instead use C 102 parameters stay obtain EMi 439 While Eq 9 compute values EMi shown Example 26 unfortunately obvious EMi changes CV ratio changes However based Theorem 21 use binomial distribution bCP 1V ˆEMi μ aσ approximation EMi number standard deviations A explicit form obtained ˆEMi μ aσ CP V CP V 1 V 2 CP CP V 1 V cid13 13 The impact increasing CV ratio observed Eq 13 For ﬁxed number standard deviations CV ratio increases increase C clear ˆEMi Eq 13 increase Table 3 provides means example insight difference irregular regular cases The difference expectations minimal degrees EMr versus EMi dramatic Table 3 compares Eq 13 Eq 9 V 30 P 3 C varying 60 102 1 2 standard deviations Here EMi bounded follows ˆEMi 2 EMi ˆEMi 1 ˆEMi 1 provides conservative upper bound 435 The MPART network generation algorithm A synthetic multipartite construction Are patterns hard easy restricted BPART construction If constructionspeciﬁc general constructions In order explore questions investigated different lated algorithm generating random Bayesian networks MPART construction MPART closely related Table 3 These results relevant BPART BNs V 30 P 3 C varying 60 102 The table shows expectation minimum outdegrees EMr Class A regular BNs EMi Class B irregular BNs For irregular case approximation ˆEMi b μ aσ 1 2 standard deviations displayed CV Class A EMr Class B ˆEMi 1 Class B EMi Class B ˆEMi 2 20 6 359 172 118 22 6 407 207 155 24 7 456 244 192 26 7 505 281 231 28 8 555 320 270 30 9 605 359 310 32 9 655 399 351 34 10 706 439 392 Fig 4 Example Bayesian network generated MPART algorithm This BN root nodes X1 X4 seven nonroot nodes C1 C7 Among nonroot nodes C3 appeared BPART network remaining nonroot nodes MPARTspeciﬁc nonroot child parent For instance C1 nonroot C4 parent C5 child OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1153 Fig 5 Generation Bayesian networks BPART MPART constructions Edges nodes simplicity omitted In constructions nodes treated sequentially Let Xi BN node currently processed For Xi ﬁxed number P parent nodes randomly selected For BPART parent nodes Xi chosen root nodes XC1 XN chosen nodes Xi1 XN MPART The MPART construction generalization BPART construction When happens parent nodes MPART BN picked root nodes BN generated BPART approach Kask Dechter 41 The procedure viewed follows Choose parameters N number nodes network V number root nodes P number parents nonroot node Construct network follows Index nodes 1 N iterate Cth node XC At ith step process ith node Xi 1 cid2 cid2 C pick uniformly random P parents nodes indexed 1 N Repeat C nonroot nodes X1 XC assigned parents Example 27 An example MPART BN shown Fig 4 The MPART BPART constructions compared Fig 5 The essential difference MPART BNs allow nonroot nodes nonroot nodes parents allowed BPART networks Essentially MPART algorithm similar BPART algorithm slight variation BPARTs ADDLAYER algorithm ADDLAYER presented Fig 3 Instead BPARTs ADDLAYER statement success c c 1 MPART sequentially uses statements success c c 1 ADDnode parents The signature MPART mirrors BPARTs signature presented Fig 2 MPARTQ F V C S R P Deﬁnition 28 MPART BNs The set BNs generated MPART UMPART β β MPARTQ F V C S R P The sets regular irregular MPART BNs respectively deﬁned U r β β MPARTQ F V C S true P U β β MPARTQ F V C S false P MPART MPART This multipartite construction algorithm typically creates BNs treelike topology However number nonroot nodes C N V smaller V graph bipartite close bipartite To reﬂect different types MPART BNs introduce following terminology Deﬁnition 29 Let MPART BN β root nodes V nonroot nodes X X1 XC C C We deﬁne bipartite subset U V U B U V ΠXC β ΠX1 MPART Bi MPART MPART MPART Given terminology following theorems MPART construction generalization BPART construction case irregular MPART BNs 1154 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 Theorem 30 Irregular BPART BNs subset irregular MPART BNs U BPART U MPART BPART β U BPART β U MPART U Proof First β U MPART Assume purposes BPART contradiction β U MPART For happen exist node X β ΠX structed BPART ΠX constructed MPART However possible candidate set ΠX BPART subset MPART This follows structure MPART construction algorithm particular statement ADDnode parents adds node candidate set parents This statement MPART algorithm lacking BPART algorithm Second exhibit β U MPART β U cid2 U MPART Consider threenode chain β nodes X1 X2 X3 edges BPART U X1 X2 X2 X3 Clearly β U BPART β bipartite However β U MPART β constructed MPART construction algorithm signature MPARTQ F V C S R P V 1 C 2 cid2 U BPART Theorem 31 Irregular BPART BNs irregular bipartite MPART BNs U BPART Bi MPART Proof We need U U U BPART MPART gives follows BPART Bi MPART ii U BPART Bi MPART From proof Theorem 30 U BPART B U MPART B U BPART Bi MPART For ii consider BN β Bi MPART Since β bipartite MPART statement ADDnode parents effect structure β left In case construction BPART algorithm clearly β U BPART cid2 Finally provide probability randomly generated irregular MPART BN bipartite par ticular irregular BPART BN Theorem 32 Let β U MPART For P cid3 1 C cid3 1 probability event β Bi Ccid2 P 1cid2 cid11 cid10 MPART Prβ Bi MPART i1 j 0 V j C V j Proof Let C set C nonroot nodes let Xi C node β currently processed MPART al gorithm For Xi parents picked Xi1 XC XC1 XN nonroot nodes processed root nodes Clearly Xi s parents ΠXi picked root nodes V XC1 XN event β Bi MPART place For MPARTs ﬁrst pick parent Y Xi k C nonroot nodes Xi1 XC avoid For R false MPARTs selection distribution uniform giving probability success Xi PrY V V kV The nodes ΠXi need distinct picked independently multiplication principle applied giving cid10 cid11 cid10 cid10 cid11 cid11 cid11 cid10 PrΠXi V V k V V 1 k V 1 V P 1 k V P 1 P 1cid2 j 0 V j k V j 14 For β Bi MPART occur condition similar 14 needs hold C nonroot nodes Deﬁnition 29 Prβ Bi V Since selection parents Xi independent selection MPART PrΠX1 parents Xj cid22 j multiplication principle applies obtain nonroot nodes X1 XC β V ΠXC PrΠX1 V ΠXC V Ccid2 i1 PrΠXi V 15 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1155 Substituting 14 15 fact k C gives Prβ Bi MPART Ccid2 i1 PrΠXi V Ccid2 P 1cid2 cid10 i1 j 0 V j k V j Ccid2 P 1cid2 cid10 cid11 i1 j 0 V j C V j cid11 Since P cid3 1 C cid3 1 assumption 0 cid2 Prβ Bi MPART cid2 1 welldeﬁned probability cid2 A reader ask MPART algorithm relates work Kask Dechter 41 MPART based approach topologies BNs generated similar There minor differences including details algorithms CPTs generated clamping evidence Our main contribution MPART following The observation BPART special case MPART characterized Theorems 30 31 32 fact CV ratio key MPART ﬁnally fact MPART BNs generated CV cid2 075 turn relatively easy tree clustering investigated Section 65 The reason CV cid2 075 inequality holds BNs investigated Kask Dechter 41 436 Discussion related work The BPART construction mentioned generalization random generation problem instances satisﬁability problem SAT 55 Satisﬁability like limited problem consider decision problem optimization problem like MPE problem Deﬁnition 9 ii gives particular bipartite BN topology Before discussing possible concerns note BPART algorithm generate Bayesian networks SATlike parameter settings F random S 2 Concerning satisﬁability obviously decision problem decision problems special cases optimization problems For instance proven fruitful view SAT decision problem MAXSAT optimization problem 1530 MAXSAT optimization problem maximizes number satisﬁed clauses So satisﬁability decision problem strong connection optimization problem Concerning ii argue bipartite topology interesting right inference algorithms speciﬁcally designed bipartite BNs 31 Important classes application BNs including medical diagnosis BNs QMRDT BN 67 information theory BNs 2728 essentially bipartite3 Bipartite medical BNs typically model diseases symptomsdiseases root nodes symptoms leaf nodes 4367 These BNs compute MPE disease nodes given known symptoms Bipartite information theory BNs coding decoding presence noisy transmission 2728 Two classes information theory Bayesian networks Hamming code BNs low density parity code BNs close relationship BPART networks general SATlike BNs particular The fact traditional information theory BNs 28 ﬁxed number children root node corresponds BPART parameter value R true Fig 2 A parameter value R false gives modern Gallager codes 48 The BPART topology provides wellunderstood stepping stone topologies particular component multipartite BNs Furthermore MPART construction regard leaf nodes BPART BN corresponding nonroot nodes arbitrary nonbipartite BN MPART BN Finally note approach independent complementary related research relies Markov chain convergence order randomly sample BNs 3435 While use Markov chain different approach similarity use heuristic width 35 use heuristics clique tree optimization However important difference approaches While generate BNs randomly directly controlling maximal clique size total clique tree size Ide et al enforce constraint heuristic width random BN generation process 35 This makes approach general potentially slower difﬁcult analyze compared approach 3 The QMRDT BN computationally challenging unfortunately publicly available consequently experimentation 1156 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 5 Analysis hard easy synthetic networks This section synthesizes discussion Section 4 parameters randomly generated BNs presenta tion tree clustering Section 3 In particular discuss varying BN parameters affect performance tree clustering inference This discussion motivates experiments performed Section 6 justi ﬁes expectations effect varying different parameters Section 6 provides quantitative details clique sizes inference times increase CV ratio different triangulation heuristics The rest section organized follows In Section 51 relate focus CV ratio previous research The following sections discuss topological issues In Section 52 identify conservative lower bound clique tree size Qualitatively lower bound gives easyhardharder pattern increasing CV ratio BN inference tree clustering Section 53 introduces structural concept onlychild BN results following Section 54 focuses cycle formation moral graphs Regular Class A BNs The reason analysis cycles general Hamiltonian cycles particular cycles moral graph force tree clustering including HUGIN add ﬁllin edges moral graph signiﬁcantly impacts size clique tree inference time In Section 55 argue regular Class A BNs harder irregular Class B BNs based insights cycle formation The ﬁnal section Section 56 discusses nature CPTs impact tree clustering inference 51 Previous research CV ratio For problem solving SAT instances phase transition phenomenon observed probability satisﬁability 1155 This phase transition phenomenon studied controlling ratio CV number variables V number clauses C CNF formula generating problem instances ex perimentally observing resulting probability satisﬁability 55 Through extensive experimentation established large 3CNF formulas phase transition occurs CV 425 For smaller instances phase transition higher CV values For example V 20 transition CV 455 Interestingly computational cost ﬁnding satisfying instantiation correlated probability satisﬁabil ity 1155 For instance DavisPutnam algorithm search satisfying assignments easyhardeasy pattern SAT The hardest instances region phase transition hard region easyhardeasy pattern 13265571 The nature hardness pattern depends algorithmic approach investigated clearly recognized SAT 55 Similar results SAT need investigations synthetically generated BPART MPART networks BN inference algorithm HUGIN This section It noted aiming establish phase transition exactly like established SAT goal develop approach systematically generate BNs varying predictable hardness 52 A lower bound In BPART MPART constructions CV ratio controlled varying values input parame ters C V Via construction described Section 432 discussion Section 51 hypothesized im portance CV ratio carries BPART BNs Given way generated SATlike networks corresponding generation CNF formulas mapped V number root nodes BN C number nonroot nodes BN When limiting attention SATlike BNs problem exactly SAT problem ﬁrst thought expect similar result However previous work DavisPutnam algorithm recursive splitting approach 30 55 The DavisPutnam algorithm different tree clustering algorithms including HUGIN Consequently easyhardeasy pattern OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1157 observed SAT formulas DavisPutnam 55 different pattern tree clustering4 Our reasoning summarized Theorem 34 based following deﬁnitions Deﬁnition 33 Clique tree metrics Let βcid6cid6cid6 clique tree constructed BN β tree clustering The total size cliques containing root nodes β deﬁned kRβcid6cid6cid6 The total size cliques containing mixture root nodes nonroot nodes β deﬁned kM βcid6cid6cid6 Clearly cliques contain leaf nodes kβcid6cid6cid6 kM βcid6cid6cid6 kRβcid6cid6cid6 For BPART particular types cliques Cliques root nodes total size kR cliques leaf nodes root nodes total size kM The total size cliques root nodes kRβcid6cid6cid6 turns important experimental article ﬁrst focus kM βcid6cid6cid6 Theorem 34 Lower bound Consider BN β C nonroot nodes P cid3 0 parents Let S ΩX cid3 2 nodes X β Then βs clique tree βcid6cid6cid6 kM βcid6cid6cid6 cid3 CSP 1 Proof Suppose m mixed node cliques clique tree enumerated 1 m Let number root nodes nonroot nodes ith clique denoted V Ci respectively V cid3 P Ci cid3 1 Clearly SCiV cid3 SCiP cid3 CiSP 1 The inequality follows easy S cid3 2 P cid3 0 Ci cid3 1 SCiP cid3 CiSP 1 The total size kM β cid6cid6cid6 mcid3 i1 SCiV cid3 mcid3 i1 inequality follows mcid3 i1 Ci cid3 CSP 1 CiSP 1 SP 1 cid9 m i1 Ci cid3 C cid2 An informal explanation result follows By construction nonroot node Ci β P parents node β S states Recall Section 31 result moralization nonroot node Ci parents ΠCi end clique After moralization given nonroot node Ci belong clique size SP 1 As stated Theorem 34 lower bound argue true C nonroot nodes total size βcid6 moralization CSP 1 Since edges potentially added deleted subsequent steps tree clustering CSP 1 lower bound total size kβcid6cid6cid6 clique tree βcid6cid6cid6 Clearly space complexity lower bound implies time complexity lower bound CSP 1 Given Theorem 34 focus CV CV increase If hold V constant C needs increasing consequently CSP 1 increases Therefore HUGIN compilation propagation times typically increase An argument easyhardharder pattern based HUGINs moralization step considering nonroot nodes BNs including BPART MPART BNs In words considering moralization tree clustering expect easyhardharder pattern increasing CV ratio V kept constant C increased This expectation conﬁrmed experiments Section 6 53 Onlychild BNs For subclass Class A BNs onlychild BNs Deﬁnition 36 Hamiltonicity The orem 42 In preparation result introduce deﬁnitions apply BNs general Intuitively BN nodes X Y qsiblings q parents common Deﬁnition 35 qsiblings Let q nonnegative integer Consider nodes X Y directed acyclic graph BN If ΠX ΠY q X Y qsiblings 4 For DavisPutnam SAT formulas easier high CV ratio CV 425 overwhelming proportion unsatisﬁable instances DavisPutnam procedure encounters inconsistencies prune search space Suppose inconsistency BN employing DavisPutnam When tree clustering BN inconsistency detected propagation phase CPT values come play 1158 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 Based concept qsiblings notion common child introduced Informally node X onlychild node parents ΠX children common Deﬁnition 36 Onlychild Let X nodes directed graph BN A node X onlychild node Y X X X Y ksiblings k 2 k nonnegative integer A graph BN onlychild graph BN nodes onlychild nodes We hypothesize notion onlychild provides good approximation BNs constructed BPART MPART following sense When number root nodes V large BPART MPART construct node onlychild long number nonroot nodes C large compared V Stated differently high proportion children onlychildren given reasonable assumptions number nonroot nodes root nodes We believe ideas formalized leave future work The notion onlychild general valid application BNs built assumption BPART MPART algorithms However notion onlychild paves way formal analysis based ﬁltering clique trees introduce Deﬁnition 37 Filtering Let βcid6cid6cid6 undirected graph Xcid6cid6cid6 Ecid6cid6cid6 clique tree The ﬁltering clique nodes Xcid6cid6cid6 nodes V deﬁned Xcid6cid6cid6cid23V cid24 Xcid6cid6cid6 PV PV power set V Given ﬁltered set nodes construct induced subgraph Deﬁnition 5 clique tree βcid6cid6cid6 follows First consider tuple clique tree nodes edges Xcid6cid6cid6 Ecid6cid6cid6 βcid6cid6cid6 form Y cid6cid6cid6 Xcid6cid6cid6cid23V cid24 Second form induced subgraph Zcid6cid6cid6 βcid6cid6cid6Y cid6cid6cid6 clique tree nodes Y cid6cid6cid6 This process deﬁne βcid6cid6cid6V βcid6cid6cid6Xcid6cid6cid6cid23V cid24 illustrated For simplicity example V1V2C1 V1 V2 C1 represent clique tree node containing BN nodes V1 V2 C1 The difference Y cid6cid6cid6 βcid6cid6cid6Y cid6cid6cid6 Fig 6 An example compiling onlychild BPART BN Only subgraph induced BN root nodes V shown moral graph triangulated moral graph junction clique tree This onlychild BN leaf node Ci cid22 j common parent leaf node Cj For instance ΠC1 V1 V3 V2 V3 1 ΠC2 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1159 set nodes graph In Example 38 notions onlychild induced subgraph ﬁltering brought Example 38 Onlychild BN Fig 6 shows bipartite onlychild BN β induced subgraphs βcid6V βcid6cid6V βcid6cid6cid6V βcid6 βcid6cid6 βcid6cid6cid6 respectively This onlychild BN leaf node Ci parent common leaf node Cj cid22 j There loop V5 V7 V6 V4 V2 V3 V1 moral subgraph βcid6V causes ﬁllin edges V5 V6 V5 V4 V1 V4 V1 V2 triangulated subgraph βcid6cid6V cliques shown clique subtree βcid6cid6cid6V In Example 38 Hamiltonian cycle moral graph root nodes causes ﬁllin edges ﬁve cliques subgraph junction tree induced BN root nodes The notion onlychild useful analyze moralization affects nonleaf nodes BN In particular onlychild deﬁnition following theorem Theorem 39 Degree moralized onlychild BNs Let C nonroot nodes BPART onlychild BN β Deﬁnition 36 let V root nodes If P ΠC cid3 2 C C V V degree dV P 1ΨV subgraph γ cid6 βcid6V moralized graph βcid6 induced V Proof There cases ΨV 0 ΨV 1 ΨV cid3 2 For ﬁrst cases holds true V V dV 0 dV P 1 respectively formula correct We consider case ΨV cid3 2 Since C C P parents moralization ensures parent V ΠC P 1 neighbors moralized graph βcid6 C Consider arbitrary Ci Cj C Ci Cj ΨV cid22 j By assumption V V V Each C C onlychild Due assumed onlychild property ΠCi Ci Cj consequently P 1 neighbors V graph γ cid6 βcid6V induced V moralized graph βcid6 This holds Ci Cj ΨV giving total dV P 1oV P 1ΨV cid2 V ΠCj An example applying Theorem 39 follows Example 40 Degree moralized onlychild BNs Consider Fig 6 particular subgraph γ cid6 βcid6V induced BNs root nodes V Vi 1 cid2 cid2 7 The second graph Fig 6 γ cid6 For Vi V dVi P 1ΠC 2 predicted Theorem 39 54 Regular BNs In BPART MPART setting R true creates regular BNs In section focus regularity impacts cycles moralized graph BN Long undirected cycles loops moralized graph main factors causing large maximal clique sizes Consequently key question tree clustering impact moralization triangulation steps terms cycles We focus longest cycle extreme case cycles visiting nodes exactly Hamiltonian cycles 541 Regular Class A BNs The issue regularity underlying graph Bayesian network received attention information theory 272848 Gallagers original codes denoted classical Gallager codes Table 1 require root node number children leaf node number parents They Class A type 28 Recently compelling argument provided lifting Class A regularity constraints beneﬁcial computing MPE BNs decoding 48 In particular iterated belief propagation compute MPE given codeword transmitted noisy channel irregular BNs perform better regular ones 29 The intuition iterated belief propagation high degree root nodes tend quicker right setting given need satisfy constraints This leads wave effect helps lower degree root nodes ﬁnd right setting For belief propagation beneﬁcial mixture highdegree lowdegree low regularity root nodes information theory BNs These observations raise question BN 1160 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 regularity impact tree clustering In following provide insight question considering cycle formation related work exists 29 542 Regular onlychild BNs We turn cycle formation moralized graph onlychild BNs Example 38 Fig 6 illustrate importance Hamiltonian cycles The following Hamiltonicity result Jackson 36 Theorem 41 Hamiltonicity Jackson Let G undirected graph vertices degree δG dG k If dG cid3 nG3 G Hamiltonian In Theorem 42 apply Theorem 41 BNs moral graph βcid6 cid14 cid15 Theorem 42 Hamiltonicity BPART onlychild BN Consider onlychild BPART BN β constructed R true let βcid6 moralized graph β let γ cid6 βcid6V subgraph βcid6 induced root nodes V β If P 1 CP V cid3 V 3 16 γ cid6 Hamiltonian Proof The theorem follows Theorem 41 Corollary 18 Theorem 39 Consider subgraph γ cid6 induced root nodes V moralized graph βcid6 γ cid6 βcid6V Since β onlychild BN known Theorem 39 root node X V cid6 γ cid6 V cid6 Ecid6 dX P 1ΨX Due BPART BNs regularity Corollary 18 V Consequently root node X BN γ cid6 degree dX P 1cid15 CP applies ΨX CP cid16 dX P 1cid13 CP cid14 For purposes proof drop edges nodes P 1cid13 CP cid14 edges V V constructing γ cid6cid6 nodes degree dγ cid6cid6 P 1cid15 CP 1 cid16 Since γ cid6cid6 regular graph apply Theorem 41 nγ cid6 nγ cid6cid6 V obtain 16 cid2 V V We note constant P ratio CV plays prominent role Theorem 42 speciﬁcally expression cid16 And following example illustrates CV ratio need high Hamiltonian cid15 CP V cycle guaranteed Example 43 Hamiltonicity BPART onlychild BN Consider onlychild BPART network C 90 V 30 P 3 R true In particular consider undirected graph γ cid6 induced root nodes V β moralization βcid6 Using Theorem 42 obtain P 1 cid15CP V cid16 18 nγ cid63 10 Hamiltonian cycle guaranteed There cases able Hamiltonian cycle exist compute longest cycle cγ cid6 apply following theorem Dirac 23 Theorem 44 Longest cycle Dirac Let cG length longest cycle undirected graph G If G 2connected cG cid3 minnG 2δG The longest cycle determined following result underlying BN β certain type Corollary 45 Longest cycle BPART onlychild BN Consider onlychild BPART BN β constructed R true let V root nodes β Further let βcid6 moralized graph β consider γ cid6 βcid6V subgraph βcid6 induced V If γ cid6 2connected cid10 cid14 cid15cid11 cγ cid6 cid3 min V 2P 1 CP V 17 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1161 Proof Theorem 44 applies n V V δG cid3 P 1cid15 CP V cid16 result follows cid2 Again CV ratio lower bound 17 longest cycle cγ cid6 Example 46 Consider 2connected undirected graph γ cid6 βcid6V constructed tree clustering child BPART BN β created input parameters R true P 3 V 30 C 60 In case Corollary 45 applies longest cycle cγ cid6 min30 24 24 While longest cycle cγ cid6 mentioned corollary example clear cycles varying length moralized BN Due limited number root nodes V cycles likely interact increasing chances ﬁllin edges leading larger cliques larger maximal clique sizes 55 Irregular BNs So far role CV ratio regular Class A BNs established We turn irregular Class B BNs generated setting R false invoking BPART MPART How Class A regular R true BPART BNs compare Class B irregular R false BPART BNs speciﬁcally regard Hamiltonicity moralized graph induced root nodes BN The following theorem addresses question considering random variables Mi Mr representing minimal root node outdegree BPART irregular regular case respectively Section 434 Using expectations consider undirected graphs Gr Gi induced moralized graphs root nodes respective Class A Class B BPART networks Theorem 47 Let V r V root nodes onlychild BPART BNs α β generated R true R false respectively Mr minV r Mi minV Let Gr αcid6V r Gi βcid6V Further b 0 let dGr bEMr dGi bEMi respectively assume exist BPART input parameters EMr 0 cid2 EMr C There exist BPART input parameter values excluding R Gr Hamiltonian Gi necessarily Hamiltonian Proof For R true set input parameter values BPART EMr V 3P 1 according Theorem 39 gives dGr V 3 Using Theorem 41 Gr Hamiltonian For R false suppose BPART input parameters values R true case R Clearly EMi EMr consequently dGi dGr dGi V 3 result Gi necessarily Hamiltonian according Theorem 41 cid2 Theorem 47 says certain BPART input parameter values excluding Rs induced subgraph Gr structed regular BPART BN Hamiltonian guarantee Gi constructed irregular BPART BN The result analysis Section 434 suggest regular Class A BPART networks given values input parameters excluding R likely cycles need ﬁllin edges irregular Class B BPART networks This makes inference problem harder tree clustering Class A BPART case Further insight provided experiments discussed Section 64 consider effect regularity hardness tree clustering terms maximal clique sizes inference times 56 Hardness conditional probability tables To investigate effect different conditional probability tables CPTs consider described Sec tion 432 CPT types deterministic CPTs xor uniform CPTs random CPTs In BPART MPART algorithms CPTs controlled Q F input parameters The values CPTs notable exceptions discussed little signiﬁcance tree clusterings compilation phase phase primarily impacted BNs topology The exceptions include tech niques zerocompression approximation techniques numerical CPT values account 24 These techniques investigated article leave future research In remainder section focus tree clusterings propagation phase 1162 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 561 Random root nodes random nonroot nodes The nature BNs CPT impacts number MPEs formally deﬁne follows case randomly generated BNs Deﬁnition 48 Number MPEs Let β randomly generated BN MPEs X x MPEs β deﬁned random variable U X x 1 x u 1 x u The number For BPART MPART BNs random CPTs MPEs typically EU 1 The reason unlikely different explanations exactly probability conditional probabilities sampled continuous distribution Q random F random The existence MPE means propagation clique tree βcid6cid6cid6 required The time required clique tree propagation increases CV ratio average increased clique sizes argued earlier section 562 Uniform root nodes deterministic nonroot nodes We consider case uniform root nodes boolean leaf ornodes Q uniform F For SATlike BPART BNs lessons learned extensive research SAT constraint satisfaction problems search algorithms 11132671 Speciﬁcally approximately loglinear relationship CV ratio expected number solutions EU empirically established example ˆEU 1000 CV 3 ˆEU 115 CV 467 case V 24 variables satisﬁable problem instances 71 In words CV ratio increases number solutions decreases average While characteristics search space structure size variability global minima 26 decrease number local minima CV ratio 71 important clear dropoff EU function CV ratio prominent For SATlike BNs number MPEs depend CV ratio exactly SAT The propagation inference problem relatively harder HUGIN belief revision handle low CV ratios repeated propagations required arrive MPE x 50 As CV ratio increased question resulting decrease EUsuggesting decreased inference time fewer propagationswill interact increase inference time larger cliques clique tree The experimental article sheds light question 6 Experiments hard easy synthetic networks In section report experiments performed HUGIN BNs generated implemen tations BPART MPART algorithms Different parameters presented Section 431 control nature BNs generated BPART MPART varied experiments Which parameters systematically varied Our main concern answering question sure inference hardness terms tree clusterings maximal clique size varied interesting fashion Some parameters total number BN nodes N V C number states BN node S obviously tied complexity inference somewhat interesting Other structural distributional effects obvious main focus experiments Speciﬁcally study effecton maximal clique size inference timesof varying CV ratio graph regularity parameter R CPT value parameters F Q In following Section 61 outlines methodology In Section 62 present results computa tional experiments maximal clique size inference time vary CV ratio SATlike BNs generated BPART construction Section 63 follows Section 62 compares BPART instances MINIMUMCLIQUEWEIGHT MINIMUMFILLINWEIGHT triangulation heuristics Section 64 presents experiments BNs varying regularity BPART construction Section 65 pro vides experimental results MPART constructionunlike experiments random CPTs kept total number BN nodes constant OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1163 61 Methodology In experiments reported generated random samples BN instances according BPART MPART constructions ran HUGIN 24 samples recorded key clique tree characteristics introduced Deﬁnition 10 In particular focused closely related statistics maximal clique size cid8 5 maximal number nodes clique h 3 Given random generation instances characteristics considered random metavariables investigated maximal clique size L number nodes maximal clique H corresponding sample statistics maximal clique size sample mean xL maximal clique size sample standard deviation sL5 For tree clustering maximal clique size results obviously impact belief updating belief revision rely clique trees While focus primarily maximal clique size results present inference time results belief revision Speciﬁcally present inference time statistics sample median m sample mean x sample standard deviation s time seconds compute MPE x x function CV ratio These results believe interesting right upper bound inference time belief updating For timing experiments Dell 410 700 MHz Pentium III CPU 1 GB RAM 9 GB swap space 1 x The number parents nonroot node P 3 experiments reported Section 65 nonroot nodes parents P 2 Note state space sizes cliques including cid8 6 clique sizes short given numbers indicating memory requirements primary memory RAM required storage implementationdependent For instance implementation tree clustering use double data type requiring 8 bytes case RAM r needed store 24node clique consisting binary S 2 nodes r cid8 8 bytes Sh 8 bytes 224 8 bytes 128 MB u We report experiments large SATlike networks propositional formulas earlier ex perimental research SATsee example 55 repositories SATLIB httpwwwsatliborg HUGIN able process large networks nontrivial CV ratios For reason results approach region CV 425 phasetransition region SAT mulas 55 This line goal constructing BNs benchmarking understanding BN computational hardness focusing CV 425 phase transition We note HUGINs default settings generally experiments The MINIMUMFILLINWEIGHT triangulation heuristic Section 63 HUGINs default settings compression approximation As far evidence leaf nodes default clamp binary nodes 1 true 62 BPART Class B networks Hardness CV phenomenon What empirical impact maximal clique sizes inference times varying CV ratio gen erating irregular R false BPART BNs In order investigate question 800 SATlike BNs generated signature BPARTuniform 30 C 2 false 3 varying number leaf nodes C 60 C 102 In words CV ratio varied CV 20 CV 34 The leaf nodes clamped 1 HUGIN inference In Table 4 Fig 7 parts Table 5 maximal clique size inference time results experiments reported From results analysis provided Table 4 conclude BPART maximal clique size increases average CV ratio This correspondence theoretical results earlier article Fig 7 shows linear regression number nodes maximal clique varies CV ratio This linear regression result shows BPART given parameters sample mean number nodes maximal clique xH grows linearly CV ratio We obtain following empirical expressions maximal number nodes clique h maximal clique size cid8 h 306 CV 100 cid8 2h 2306CV 100 The regression statistically signiﬁcant R2 0716 F ratio 2013 pvalue 210 10220 The 95 conﬁdence interval slope regression line 293 320 Let discuss inference time results presented Fig 7 Table 5 The linear growth xH h CV translates exponential growth xL cid8 CV This exponential growth cid8 CV turn 5 The term metavariable distinguish random variables random variables nodes making BNs 1164 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 Table 4 Experimental results showing statistics BNs generated BPART algorithm R false irregular case V 30 C ranging 60 102 The number nodes sizes maximal cliques shown instances varying CV ratios The smallest maximal clique contained h 14 nodes 5 instances CV 20 largest maximal clique contained h 22 nodes 7 instances CV 34 Maximal clique size Nodes H State space L CV ratio BPART Class B irregular BNs 20 22 24 26 28 30 32 34 Total 14 15 16 17 18 19 20 21 22 16384 32768 65536 131072 262144 524288 1048576 2097152 4194304 5 26 48 19 2 1 7 28 37 23 4 10 35 41 13 1 1 23 50 20 6 10 33 41 14 2 2 10 46 41 1 Number instances Mean number nodes xH Mean size 1000s xL 100 1587 7094 100 1686 1506 100 1760 2386 100 1807 3297 100 1865 5033 100 1929 7209 6 33 87 126 167 170 143 60 8 800 1831 5820 7 30 42 20 1 100 1978 1077 1 16 39 37 7 100 2033 1565 Fig 7 These results Class B irregular BPART BNs V 30 root nodes CV ratio ranging CV 20 CV 34 Left The number nodes clique trees maximal clique plotted function CV ratio In scatter plot points representing BN instances linear regression results displayed Right In graph averages medians MPE computation time sample means xT sample medians mT shown explains approximately exponential growth inference times reported Fig 7 This result provides empirical answer question raised Section 56 On hand recall fewer propagations decrease EU cause decrease total inference time On hand increase expected maximal clique size EH cause increase inference time Clearly effect outweighs We note Table 5 standard deviations si T inference times substantial The high standard deviation typical area research observed SAT 556263 Both Table 4 Fig 7 provide evidence inference time standard deviation si T substantial Inference time clearly depends maximal clique size L exponential number nodes maximal clique H A change H exponential effect L approximately linear effect inference time In column CV 20 Table 4 H ranges 14 18 giving 286 increase smallest H 14 largest value H 18 This causes L range 16384 262144 1500 increase smallest L 16384 largest L 262144 value OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1165 Table 5 The effect regular irregular BPART BNs HUGIN maximal clique sizes computation times seconds shown The following sample statistics presented regular case median computation time mr T standard deviation computation time sr L Similar statistics presented superscripts irregular case T mean maximal clique size xr T mean computation time xr BN parameters V 30 30 30 30 30 30 30 30 C 60 66 72 78 84 90 96 102 CV 20 22 24 26 28 30 32 34 BPART Class A regular BNs xr mr T T sr T 1670 2900 4284 5841 10145 12105 15645 16830 194 331 456 658 1098 1310 1584 1920 115 164 198 318 597 663 654 974 xr L 3821 6554 9230 1376 2081 2642 3324 4708 BPART Class B irregular BNs mi T xi T si T 367 711 994 1460 2143 2941 4564 5874 310 821 1161 1838 2631 3532 5378 6736 210 501 675 1252 1798 2136 3390 4279 xi L 7094 1506 2386 3297 5033 7209 1077 1565 Ratios T xi xr T 313 404 393 358 417 371 295 285 L xi xr L 539 435 387 417 413 366 309 301 Table 6 Randomly generated BPART Class B instances V 30 C 60 showing seven instances smallest largest maximal clique cid6cid6cid6 BN instance βi number cliques contains different clique sizes From sample sizes A column contains clique tree β 100 BNs ﬁve columns β0 β12 β73 β89 β92 present BN instances smallest maximal clique size cid8 16 384 columns β6 β80 present BN instances largest maximal clique size cid8 262144 Of instances β6 largest total clique size k 39776 β92 smallest total clique size k 53344 Clique size Clique trees BPART Class B BNs CV 20 cid6cid6cid6 89 cid6cid6cid6 12 cid6cid6cid6 73 cid6cid6cid6 0 β β β β β cid6cid6cid6 92 β cid6cid6cid6 6 β cid6cid6cid6 80 Nodes h State space cid8 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 16 32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072 262144 60 2 2 2 1 2 1 2 2 62 1 1 4 2 2 1 4 61 3 2 1 2 1 1 1 1 4 60 3 2 2 1 1 3 4 60 3 1 4 3 1 2 1 2 60 1 2 2 2 2 1 1 Maximal clique size cid8 Total clique tree size k Maximal number nodes h Maximal clique cid8 k Top cliques 100 16384 61056 14 268 604 16384 75840 14 216 648 16384 81200 14 202 605 16384 99138 14 165 496 16384 53344 14 307 768 262144 439776 18 596 931 60 1 1 2 1 2 1 1 2 1 1 262144 284192 18 922 966 Table 6 contains details seven extreme instances listed column CV 20 Table 4 Table 6 displays ﬁve instances smallest maximal clique size cid8β0 cid8β12 cid8β73 cid8β89 cid8β92 16384 instances largest maximal clique size cid8β6 cid8β80 262144 The following example illustrates total clique tree size k distributed kM kR Deﬁnition 33 example BPART instances Example 49 Table 6 contains BPARTuniform 30 60 2 false 3 instance β12 The total state space size clique tree βcid6cid6cid6 12 kRβcid6cid6cid6 12 60 24 960 kRβcid6cid6cid6 12 2 24 1 25 1 213 4 214 73792 12 75840 kM βcid6cid6cid6 12 given kβcid6cid6cid6 12 kM βcid6cid6cid6 1166 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 In terms number cliques example cliques size Si 24 clearly dominant qi 60 cliques contributing kM βcid6cid6cid6 12 6024 960 However terms impact kβcid6cid6cid6 12 particular maximal cliques size Si 214 important contribution 4 214 65536 Several points seven extreme instances First cliques size ΩH 16 indicated lower bound Theorem 34 strong heavy tail effect present particular β6 total clique size k 439776 β80 k 284192 The largest cliques 931 966 total clique size BNs β6 β80 respectively In fact β80 extreme size cid8 largest clique 32 times second largest clique Clearly cliques size 217 218 present β6 β80 absent rest example dramatic impact total clique tree size variation total clique tree size different instances This pattern explain large standard deviations inference times Table 5 12 kRβcid6cid6cid6 In related experiments details omitted space restrictions randomly generated BNs consisted V 20 30 35 40 root nodes number leaf nodes C varied 40 140 CV ratio ranging 20 40 The results similar reported As CV ratio increased mean inference time increased approximately exponential rate Stated qualitatively experiments conﬁrm hypothesis high CV ratio implies large maximal clique size implies long inference time There strong causal relationship high CV ratio large maximal clique size Of course BPART topology certain ranges input parameters BPART particular tree clustering triangulation heuristic MINIMUMFILLINWEIGHT In section turn detailed study triangulation heuristics 63 BPART Class B networks Triangulation heuristics Here report experiments MINIMUMCLIQUEWEIGHT triangulation heuristic A ﬁrst question varying CV ratio similar impact MINIMUMCLIQUEWEIGHT heuristic observed MINIMUMFILLINWEIGHT heuristic Section 62 In order answer question additional experiments performed BPART Class B instances Section 62 time compiled tree clustering MINIMUMCLIQUEWEIGHT heuristic The maximal clique sizes resulting set experi ments analyzed linear regression The regression results MINIMUMCLIQUEWEIGHT statistically signiﬁcant R2 0695 standard error 0906 F ratio 1822 This regression gives following empirical results maximal number nodes clique h maximal clique size cid8 h 298CV 105 cid8 2h 2298CV 105 The 95 conﬁdence interval slope h 285 312 pvalue 3310208 The 95 conﬁdence interval intercept regression line 1013 1089 pvalue 29 10273 These results number nodes maximal clique h grows linearly CV ratio MINIMUMCLIQUEWEIGHT heuristic This gives exponential growth cid8 implications similar discussed Section 62 MINIMUMFILLINWEIGHT A second question MINIMUMCLIQUEWEIGHT MINIMUMFILLINWEIGHT produce different maximal clique sizes Comparing respective regression lines clear MINIMUMFILLINWEIGHT average slightly superior MINIMUMCLIQUEWEIGHT CV range considered One ask difference statistically signiﬁcant Out 800 BN samples 525 instances maximal clique sizes heuristics 231 instances MINIMUMFILLINWEIGHT better 44 instance MINIMUMCLIQUEWEIGHT better Doing paired comparison sign test difference heuristics 1 signiﬁcance level MINIMUMFILLINWEIGHT better according statistical test 64 BPART Class A networks Hardness graph regularity What impact maximal clique sizes inference times varying CV ratio generating regular R true BPART BNs To answer question complement analysis regularity presented Section 54 created BNs BPART construction varying CV ratio earlier generating Class A regular BNs Speciﬁcally signature BN construction BPARTuniform V C 2 true 3 A similar signature BPARTuniform V C 2 false 3 construct Class B irregular BNs OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1167 Fig 8 Tree clustering results BPART BNs V 30 root nodes CV ratios CV 20 CV 34 shown The plots reﬂect Class B irregular Class A regular BNs Left The number nodes maximal clique shown function CV ratio Data points representing Class A regular BPART instances regression results displayed The regression line Class B irregular BPART instances included Right The mean computation times seconds plotted function CV ratio Speciﬁcally sample mean xr T Class A regular BNs sample mean xi T Class B irregular BNs shown varying levels CV ratio Table 7 Experimental tree clustering results showing statistics BNs generated BPART input parameters R true Class A regular case V 30 C ranging 60 102 The number nodes size maximal clique shown varying CV ratios The smallest maximal clique contains h 16 nodes CV 20 largest maximal clique contains h 23 nodes CV 34 CV ratio BPART Class A regular BNs 20 22 24 26 28 30 32 34 Total Maximal clique size Nodes H State space L 16 17 18 19 20 21 22 23 65536 131072 262144 524288 1048576 2097152 4194304 8388608 3 14 44 31 8 4 16 44 36 10 31 48 11 1 14 48 36 1 Number BN instances Mean number nodes xH Mean size 1000s xL 100 1827 3821 100 1912 6554 100 1960 923 100 2022 1376 1 28 57 14 100 2084 2081 2 13 53 31 1 100 2116 2642 3 45 48 4 100 2153 3324 3 17 57 23 100 2200 4708 3 18 71 123 187 219 151 28 800 2034 2011 Section 62 The essential difference signatures relaxed Class A regular case root node essentially number children Class B irregular case number children exactly distributed bC P V Theorem 20 approximately distributed bCP 1V Theorem 21 Maximal clique size inference time results experiments leaf nodes clamped 1 HUGIN inference presented Table 5 Fig 8 Table 5s columns xr L summarize rows Tables 7 4 respectively Table 7 presents maximal clique size number nodes maximal clique vary CV ratio L xi Fig 8 summarizes linear regression results showing number nodes maximal clique xH varies linearly CV ratio The regression gives results maximal number nodes clique h maximal clique size cid8 h 259 CV 134 cid8 2h 2259CV 134 The regression based 800 observations statistically signiﬁcant R2 0703 F ratio 1893 pvalue 75 10213 The 95 conﬁdence interval slope regression line 247 270 For parameters result 1168 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 h shows BPART Class A construction number nodes maximal clique grows linearly CV ratio This linear growth h explains similar discussed Section 62 approximately exponential growth cid8 increasing inference times Can conclude maximal clique sizes BPART Class A Class B BNs signiﬁcantly different Comparing corresponding results Class B BNs Table 4 note regular Class A instances Table 7 skewed higher maximal clique sizes A statistical ttest population means assuming unknown population means variances performed maximal clique sizes Class A Class B samples With t 265 critical values t 165 onetail 196 twotail nullhypothesis population means equal μi L rejected This result Table 7 Fig 8 sheds additional light difference computation times Class A Class B BNs This result consistent theoretical analysis Section 54 L μr T From ratios xr The regular BPART BNs average required times time computation compared irregular T versus irreg T easily determine HUGIN consistently faster irregular L L L cid2 539 L xi BPART BNs More formally consider sample mean computation times regular case xr ular case xi T xi regular BNs 285 cid2 xr line results xr corresponding larger sample mean Class A regular BNs Class B irregular BNs 301 cid2 xr Table 5 T cid2 417 Table 5 The results maximal clique size sample means xr T xi T From ratios sample means xr L xi L xi T xi In summary shown regularity factor iterated belief propagation information theory BNs 2848 tree clustering closely related BNs Conﬁrming adding analytical results shown empirically regular Class A BNs harder average irregular Class B BNs larger maximal cliques 65 MPART Class B networks Hardness conditional probabilities In Sections 62 63 64 reported results BPART BNs What happens parameters topology changed different BNs generated Is CV ratio important BNs generated MPARTconstruction These questions investigated empirically section data set containing 800 MPART samples The signature generate BNs MPARTrandom random V C 2 false 2 C 110 156 V 256 C giving CV ratio ranging 075 156 Following Kask Dechter 41 kept N constant varied V determined C setting C N V The effect nonlinear change CV ratio function change C Also note P 2 parents nonroot node P 3 experiments differences random CPTs leaf nodes clamped inference Fig 9 plots results form individual data pointsnumber nodes maximal clique sample BN function CV ratioand displays linear regression results The following empirical results h cid8 obtained h 177 CV 587 cid8 2h 2177CV 587 The regression statistically signiﬁcant Table 8 Inference times seconds total 800 BNs generated MPART construction shown Median computation time mT mean compilation time xC standard deviation sC mean propagation time xP standard deviation sP mean computation time xT standard deviation sT mean size maximal clique xL presented BN parameters V 146 136 126 116 106 104 102 100 C 110 120 130 140 150 152 154 156 V C 256 256 256 256 256 256 256 256 CV 075 088 103 121 142 146 151 156 Statistics MPART BNs xC mT sC 0156 0203 0281 0688 4204 8039 12080 20420 0148 0195 0268 0566 3439 5565 10030 17811 0013 0047 0071 0359 3594 5121 10935 22477 xP 0010 0014 0047 0300 3174 5365 9852 17585 sP 0008 0009 0046 0352 3617 5163 11103 22568 xT 0158 0209 0315 0867 6614 10929 19882 35396 sT 0014 0050 0109 0710 7208 10281 22035 45044 xL 021 136 1340 9708 1186 1951 3509 7229 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1169 Fig 9 For BNs generated MPART construction plots display tree clustering results function CV ratio Here CV ratio ranges CV 075 CV 156 These results nonroot nodes C ranging C 110 C 156 root nodes V ranging V 146 V 100 Left The number nodes maximal clique plotted function CV ratio This scatter plot shows BN instances linear regression results Right The sample mean computation times xT sample median computation times mT displayed logplot Table 9 Experimental results tree clustering BNs generated MPART construction Here C V 256 CV ratios C ranging 110 156 V ranging 100 146 The number nodes sizes maximal cliques shown number instances varying CV ratios The smallest maximal clique contains h 4 nodes CV 075 largest maximal clique contains h 26 nodes CV 156 Maximal clique size Nodes H State space L CV ratio MPART BNs 075 088 103 121 142 146 151 156 1 9 25 32 17 12 3 1 3 17 23 29 18 7 3 3 7 18 18 31 7 8 7 1 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 16 32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072 262144 524288 1048576 2097152 4194304 8388608 16777216 33554432 67108864 4 3 10 26 22 23 10 1 1 1 4 11 22 18 25 10 5 4 1 4 10 16 33 21 9 4 2 1 3 9 10 21 21 20 9 5 1 1 3 7 17 22 20 18 5 4 3 Number BN instances Mean number nodes xH Mean size 1000s xL 100 708 021 100 975 136 100 1261 1340 100 1579 9708 100 1915 1186 100 2007 1951 100 2072 3509 100 2158 7229 1170 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 R2 0908 F ratio 7882 pvalue 0 The 95 conﬁdence interval slope regression line h 173 181 As regression results h increases linearly CV ratio giving approximately exponential growth cid8 CV ratio An approximately exponential growth maximal clique size cid8 explains large extent inference times reported Table 8 right Fig 9 Table 9 provides details statistics maximal clique size including xH xL increase CV ratio The results MPART respects similar results BPART However slightly greater variation MPART case seen comparing sample standard deviations comparing Tables 4 9 Also growth maximal clique size computation time function CV ratio fact stronger MPART BPART This clearly different CPTs following discussion Section 56 For MPART CPT values picked random distribution giving propagation BPART multiple propagations general needed We note MPART BNs similar multipartite BNs investigated Kask Dechter 41 They randomly generated BNs CV cid2 075 N V C 256 nodes For CV cid2 075 MPART samples relatively easy HUGIN solve average For example highest CV ratio Kask Dechter CV 075 MPART sample mean inference time xT 01579 seconds Table 8 row CV 075 maximal clique size sample mean xL 210 Table 9 column CV 075 For MPART largest maximal clique size CV 075 subsample cid8 2 048 When MPART BNs CV cid2 075 experiments algorithms mind BNs fact relatively easy HUGIN solve average On hand CV cid3 150 MPART BNs typically challenging 7 Conclusion future work The performance Bayesian network BN inference algorithms previous research empirically evalu ated BN instances applications To complement experiments randomly generated BNs 7173441566970 We believe certain care required randomly generating BNs The generation algorithms need provide knobs controlling difﬁculty generated instances The synthetic BNs need idealized support analysis time need somewhat realistic relevant applications We developed based previous research paradigm systematically generating increasingly hard random instances BN inference One classes BNs bipartite BPART networks generated BPART algorithm extends research generating hard instances satisﬁability problems 55 Here exploited relationship computing MPE ﬁnding satisfying assignment corresponding CNF formula construct hard instances MPE problem These BPART networks similar structure application BNs medicine 67 information theory 272849 results relevant areas research The class BNs MPART networks closely related approach Kask Dechter 41 For MPART networks analyzed relationship BPART BNs Different algorithms BN inference general MPE computation particular investigated experimental paradigm 51 In article focused HUGIN tree clustering algorithm 2373946 HUGIN best probabilistic inference algorithms available algorithm systematically varying structural distributional parameters synthetic Bayesian networks shown HUGIN inference impacted In particular randomly generated BNs BPART MPART algorithms varying parameters ratio CV number nonroot nodes C number root nodes V BN regularity structure BNs underlying graph conditional distribution tables CPTs nodes BN We carefully studied parameters impact inference hardness expressed terms maximal clique size inference time tree clustering We identiﬁed HUGIN easyhardharder pattern BPART MPART networks For classes generating random networks result easy instances On hand carefully varying parameters certain dimensions construct BNs existing tree clustering algorithms handle Speciﬁcally CV ratio impact maximal clique size OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1171 indication inferential hardness network suitable structural parametric assumptions As CV ratio grows ﬁxing number N C V nodes network inference problem harder average increasing maximal clique size We summarize results Four structurally distinct classes BNs identiﬁed articleClass A Class B Class C Class D BNsof closely investigated Class A regular Class B irregular BNs For Class B irregular BPART BNs analysis showed easyhardharder pattern increasing CV ratio Through regression analy sis linear relationship established CV ratio mean number nodes maximal clique giving approximately exponential growth maximal clique size reﬂected HUGIN tree clustering inference time Results similar tree clusterings MINIMUMFILLINWEIGHT MINIMUMCLIQUEWEIGHT triangulation heuristics slightly signiﬁcantly better terms optimizing maximal clique size A second structural parameter considered BPART construction regularity underlying graph BN Our analysis showed regular Class A BPART BNs harder irregular Class B BPART BNs expectation conﬁrmed experiments A regression analysis exhibited exponential growth maximal clique size function CV ratio similar irregular case We showed experimentally keeping parameters generated networks ﬁxed maximal clique size sample means 30 54 times greater Class A BNs compared Class B BNs These results shed new light computational beneﬁt irregularity information theory BNs 272848 Our studies MPART different input parameters BN sample generation Still regression results bear resemblance BPARTs regression results There turned approximately linear growth mean number nodes maximal clique function CV giving approximately exponential growth maximal state space size expect approximately exponential growth inference times In fact graph averages showed slightly stronger exponential growth It turned previous work similar class BNs CV cid2 075 41 correspond relatively easy region MPART distribution Since complexity exact Bayesian network inference algorithmsincluding tree clustering algorithms conditioning algorithms elimination algorithmsdepend treewidth optimal minimal maximal clique size 5172021 believe results researchers investigating exact Bayesian network inference algorithms Our empirical results limited fact employed suboptimal MINIMUMFILLINWEIGHT MINIMUMCLIQUEWEIGHT triangulation heuristics hardness BN inference problems 1 146066 widespread adoption similar heuristics believe results signiﬁcant In addition showing interesting aspects HUGIN tree clustering approach believe line research essential valid experimental evaluation algorithms performed For instance related work BPART MPARTnetworks benchmark stochastic local search strong dependence inference time CV ratio qualitatively similar results reported 5153 The approach constructing synthetic BNs guided developing stochastic local search approach point outperforms HUGIN certain synthetic instances certain application BNs 5153 Similar research help developing better understanding different algorithms varying conditions In particular results researchers focus work areas space Bayesian networks time spaceconsumption relatively high tree clustering This research extended directions It important develop better understanding dimensions CV regularity different CPT types intermediate cases inference algorithms While studied extreme cases dimensions essential perform similar studies dimen sions Other important areas include improved analytical models clique tree cluster formation growth loop formation interactions loops placement ﬁllin edges moral graphs induced BNs Improv ing understanding relationship synthetically generated networks networks applications natural extension research Acknowledgements The research reported largely conducted Ole J Mengshoel University Illinois Urbana Champaign Ole J Mengshoel David C Wilkins gratefully acknowledge support ONR Grant N00014 1172 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 9510749 ARL Grant DAAL019620003 NRL Grant N0001497C2061 Dan Roth gratefully acknowledges support NSF grants IIS9801638 SBR987345 Vadim Bulitko David Fried Song Han William Hsu Brent Spillner anonymous reviewers acknowledged comments related work David Fried Song Han Misha Voloshin acknowledged codevelopment software experiments References 1 AM Abdelbar SM Hedetnieme Approximating MAPs belief networks NPhard theorems Artiﬁcial Intelligence 102 1998 2138 2 SK Andersen KG Olesen FV Jensen F Jensen HUGINa shell building Bayesian belief universes expert systems Proceedings Eleventh International Joint Conference Artiﬁcial Intelligence vol 2 Detroit MI August 1989 pp 10801085 3 S Andreassen M Woldbye B Falck SK Andersen MUNINA causal probabilistic network interpretation electromyographic ﬁndings Proceedings Tenth International Joint Conference Artiﬁcial Intelligence Milan Italy August 1987 pp 366372 4 D Angluin LG Valiant Fast probabilistic algorithms Hamiltonian circuits matchings Journal Computer System Sci ences 18 2 1979 155193 5 S Arnborg Efﬁcient algorithms combinatorial problems graphs bounded decomposabilitya survey BIT 25 1985 223 6 S Arnborg DG Corneil A Proskurowski Complexity ﬁnding embeddings ktree SIAM Journal Algebraic Discrete Methods 8 1987 277284 7 A Becker D Geiger Approximation algorithms loop cutset problem Proceedings Tenth Annual Conference Uncertainty Artiﬁcial Intelligence UAI94 San Francisco CA 1994 pp 6068 8 U Bertele F Brioschi Nonserial Dynamic Programming Academic Press New York 1972 9 A Bobbio L Portinale M Minichino E Ciancamerla Improving analysis dependable systems mapping fault trees Bayesian networks Reliability Engineering System Safety 71 3 2001 249260 10 N Chater CD Manning Probabilistic models language processing acquisition Trends Cognitive Sciences 10 7 2006 335344 11 P Cheeseman B Kanefsky WM Taylor Where hard problems Proceedings Twelfth International Joint Conference Artiﬁcial Intelligence Sidney Australia 1991 pp 331337 12 M Ciaramita M Johnson Explaining away ambiguity Learning verb selectional preference Bayesian networks 18th International Conference Computational Linguistics COLING00 Saarbrücken Germany 2000 pp 187193 13 D Clark J Frank I Gent E MacIntyre N Tomov T Walsh Local search number solutions Proceedings Second International Conference Principles Practices Constraint Programming Lecture Notes Computer Science vol 1118 Springer Berlin 1996 pp 119133 14 FG Cooper The computational complexity probabilistic inference Bayesian belief networks Artiﬁcial Intelligence 42 1990 393 405 15 P Crescenzi V Kann A compendium NP optimization problems Technical Report SIRR9502 Dipartimento di Scienze dellInfor mazione Universita di Roma La Sapienza Roma Italy 1995 16 A Darwiche Conditioning methods exact approximate inference causal networks Proceedings Eleventh Annual Confer ence Uncertainty Artiﬁcial Intelligence UAI95 Montreal Canada 1995 pp 99107 17 A Darwiche Recursive conditioning Artiﬁcial Intelligence 126 12 2001 541 18 AP Dawid Applications general propagation algorithm probabilistic expert systems Statistics Computing 2 1992 2536 19 R Dechter Bucket elimination A unifying framework reasoning Artiﬁcial Intelligence 113 12 1999 4185 20 R Dechter Y El Fattah Topological parameters timespace tradeoff Artiﬁcial Intelligence 125 12 2001 93118 21 R Dechter J Pearl Networkbased heuristics constraint satisfaction problems Artiﬁcial Intelligence 34 1 1987 138 22 FJ Diez Local conditioning Bayesian networks Artiﬁcial Intelligence 87 12 1996 120 23 GA Dirac Some theorems abstract graphs Proc London Math Soc 2 1952 6981 24 Hugin Expert Hugin API Reference Manual Hugin Expert 2004 25 J Franco M Paull Probabilistic analysis Davis Putnam procedure solving satisﬁability problem Discrete Applied Mathematics 5 1983 7787 26 J Frank P Cheeseman J Stutz When gravity fails Local search topology Journal Artiﬁcial Intelligence Research 7 1997 249281 27 BJ Frey Graphical Models Machine Learning Digital Communication MIT Press Cambridge MA 1998 28 RG Gallager Low density parity check codes IRE Transactions Information Theory 8 1962 2128 29 X Ge D Eppstein P Smyth The distribution loop lengths graphical models turbo decoding IEEE Transactions Information Theory 47 6 2001 25492553 30 PW Gu J Purdom J Franco BW Wah Algorithms satisﬁability SAT problem A survey Satisﬁability Problem Theory Applications DIMACS Series Discrete Mathematics Theoretical Computer Science American Mathematical Society Providence RI 1997 pp 19152 31 M Henrion Searchbased methods bound diagnostic probabilities large belief networks Proceedings Seventh Annual Conference Uncertainty Artiﬁcial Intelligence UAI91 University California Los Angeles CA 1991 pp 142150 32 EJ Horvitz HJ Suermondt GF Cooper Bounded conditioning Flexible inference decisions scarce resources Proceedings Fifth Conference Uncertainty Artiﬁcial Intelligence UAI89 Windsor Ontario Morgan Kaufmann 1989 pp 182193 33 C Huang A Darwiche Inference belief networks A procedural guide International Journal Approximate Reasoning 15 1996 225 263 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 1173 34 JS Ide FG Cozman Generating random Bayesian networks Proceedings 16th Brazilian Symposium Artiﬁcial Intelligence Porto Galinhas Brazil November 2002 pp 366375 35 JS Ide FG Cozman FT Ramos Generating random Bayesian networks constraints induced width Proceedings 16th European Conference Artiﬁcial Intelligence 2004 pp 323327 36 W Jackson Hamilton cycles regular 2connected graphs Journal Combinatorial Theory Series B 29 1980 2746 37 FV Jensen An Introduction Bayesian Networks Springer New York 1996 38 FV Jensen SL Lauritzen KG Olesen Bayesian updating causal probabilistic networks local computations SIAM Journal Com puting 4 1990 269282 39 FV Jensen KG Olesen SK Andersen An algebra Bayesian belief universes knowledgebased systems Networks 20 5 1990 637659 40 P Jones C Hayes D Wilkins R Bargar J Sniezek P Asaro OJ Mengshoel D Kessler M Lucenti I Choi N Tu J Schlabach CoRAVEN Modeling design multimedia intelligent infrastructure collaborative intelligence analysis Proceedings International Conference Systems Man Cybernetics San Diego CA October 1998 pp 914919 41 K Kask R Dechter Stochastic local search Bayesian networks Proceedings Seventh International Workshop Artiﬁcial Intelligence Statistics Fort Lauderdale FL Morgan Kaufmann January 1999 42 U Kjaerulff Optimal decomposition probabilistic networks simulated annealing Statistics Computing 2 1992 717 43 I Kononenko Inductive Bayesian learning medical diagnosis Applied Artiﬁcial Intelligence 7 1993 317337 44 AMCA Koster HL Bodlaender SPM van Hoesel Treewidth Computational experiments H Broersma U Faigle J Hurink S Pickl Eds Electronic Notes Discrete Mathematics vol 8 Elsevier Science Publishers Amsterdam 2001 45 H Langseth L Portinale Bayesian networks reliability Reliability Engineering System Safety 92 1 2007 92108 46 S Lauritzen DJ Spiegelhalter Local computations probabilities graphical structures application expert systems discussion Journal Royal Statistical Society Series B 50 2 1988 157224 47 Z Li B DAmbrosio Efﬁcient inference Bayes nets combinatorial optimization problem International Journal Approximate Reasoning 11 1 1994 5581 48 MG Luby M Mitzenmacher MA Shokrollahi DA Spielman Improved lowdensity paritycheck codes irregular graphs belief propagation International Symposium Information Theory Cambridge MA August 1998 49 DJC MacKay Information Theory Inference Learning Algorithms Cambridge University Press Cambridge UK 2002 50 A Madsen Computing MPEs Hugin Personal communication April 2003 51 OJ Mengshoel Efﬁcient Bayesian network inference Genetic algorithms stochastic local search abstraction PhD thesis Department Computer Science University Illinois UrbanaChampaign Urbana IL April 1999 52 OJ Mengshoel D Roth DC Wilkins Hard easy Bayesian networks computing probable explanation Technical Report UIUCDCSR20002147 Department Computer Science University Illinois UrbanaChampaign Urbana IL January 2000 53 OJ Mengshoel D Roth DC Wilkins Stochastic greedy search Computing probable explanation Bayesian networks Technical Report UIUCDCSR20002150 Department Computer Science University Illinois UrbanaChampaign Urbana IL February 2000 54 OJ Mengshoel DC Wilkins Raven Bayesian networks humancomputer intelligent interaction MS Vassiliou TS Huang Eds Computer Science Handbook Displays Rockwell Scientiﬁc Company 2001 pp 209219 55 D Mitchell B Selman HJ Levesque Hard easy distributions SAT problems Proceedings Tenth National Conference Artiﬁcial Intelligence San Jose CA 1992 pp 459465 56 JD Park A Darwiche Approximating MAP local search Proceedings Seventeenth Conference Uncertainty Artiﬁcial Intelligence UAI01 Seattle WA 2001 pp 403410 57 J Pearl A constraintpropagation approach probabilistic reasoning LN Kanal JF Lemmer Eds Uncertainty Artiﬁcial Intelli gence Elsevier Amsterdam 1986 pp 357369 58 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann San Mateo CA 1988 59 N Robertson P Seymour Graph minors ii Algorithmic aspects treewidth Journal Algorithms 7 1986 309322 60 D Roth On hardness approximate reasoning Artiﬁcial Intelligence 82 1996 273302 61 CC Ruokangas OJ Mengshoel Information ﬁltering Bayesian networks effective user interfaces aviation weather data Pro ceedings 2003 International Conference Intelligent User Interfaces Miami FL 2003 pp 280283 62 B Selman HA Kautz B Cohen Noise strategies improving local search Proceedings Twelfth National Conference Artiﬁcial Intelligence Seattle WA 1994 pp 337343 63 B Selman H Levesque D Mitchell A new method solving hard satisﬁability problems Proceedings Tenth National Conference Artiﬁcial Intelligence San Jose CA July 1992 pp 440446 64 RD Shachter SK Andersen P Szolovits Global conditioning probabilistic inference belief networks Proceedings Tenth Annual Conference Uncertainty Artiﬁcial Intelligence UAI94 Seattle WA 1994 pp 514522 65 PP Shenoy A valuationbased language expert systems International Journal Approximate Reasoning 5 3 1989 383411 66 E Shimony Finding MAPs belief networks NPhard Artiﬁcial Intelligence 68 1994 399410 67 MA Shwe B Middleton DE Heckerman M Henrion EJ Horvitz HP Lehmann GF Cooper Probabilistic diagnosis formulation INTERNIST1QMR knowledge base I The probabilistic model inference algorithms Methods Information Medicine 30 4 1991 241255 68 C Skaaning Jensen A Kong Blocking Gibbs sampling linkage analysis large pedigrees loops Research Report R962048 Department Computer Science Aalborg University Denmark 1996 69 HJ Suermondt GF Cooper Probabilistic inference multiply connected belief networks loop cutsets International Journal Ap proximate Reasoning 4 1990 283306 1174 OJ Mengshoel et al Artiﬁcial Intelligence 170 2006 11371174 70 RL Welch Real time estimation Bayesian networks Proceedings Twelfth Annual Conference Uncertainty Artiﬁcial Intelligence UAI96 Portland OR 1996 pp 533544 71 M Yokoo Why adding constraints makes problem easier hillclimbing algorithms Analyzing landscapes CSPs Proceedings Third International Conference Principles Practice Constraint Programming Lecture Notes Computer Science vol 1330 Springer Berlin 1997 pp 357370 72 NL Zhang D Poole Exploiting causal independence Bayesian network inference Journal Artiﬁcial Intelligence Research 5 1996 301328