ELSEVIER Artificial Intelligence 82 1996 2144 Artificial Intelligence Probably approximately optimal satisficing strategies Russell Greiner Pekka Orponen b Siemens Corporate Research 755 College Road East Princeton NJ 08540 USA h Department Computer Science University Helsinki PO Box 26 FIN00014 Helsinki Finland Received September 1990 revised December 1992 Abstract A satiscirzg search problem consists set probabilistic successes experiments performed failures The expected cost order configuration individual experiments experiments strategy specifies minimizes compute optimal strategies order seeking satisfying search depends success probabilities performed search expected cost optimal Earlier work provided optimizing A strategy functions success certain classes search problems probabilities individual experiments We extend results providing general model optimal strategy strategies algorithm PA0 estimates probability values known The algorithm number trials undetermined uses estimates proper experiment identify strategy cost high probability close optimal optimizing PA0 We algorithm learn search search problem formulated identifies approximately statistics performing relevant probabilities andor necessary gather tree function Some work performed authors University Toronto supported operating grant Academy Finland The authors useful comments earlier versions paper Preliminary versions parts work appeared conference National Science Engineering Research Council Canada referees thank Dale Schuurmans Tom Hancock anonymous 191 IO respectively reports Corresponding author Email greinerscrsiemenscom 00043702961500 SSD10004370295000100 1996 Elsevier Science BV All rights reserved 22 R Greinec P OrponedArtificial Intelligence 82 1996 2144 1 Introduction Consider following situation reliable tests deciding blood obtain diagnosis Using strategy 01 blood individual hepatitis involves blood test liver biopsy Assuming doctor false negatives false positives follow perform If liver conclude diagnosis based patients performs result biopsy The doctors option strategy 02 liver tests orderfirst blood test blood necessary liver test strategies patient hepatitis test positive test conclude examine liver strategy Which strategy better Our goal measurement assume perform practice evaluate We define strategys expected cost perform To quantify doctor asked average cost required patients Assuming tests blood strategy 01 clearly better probability positive blood test pi liver test pi probability tests averaged distribution anticipated cost liver larger strategy 02 preferable distribution positive patients Earlier research decision making model produced number optimizing identify strategy optimal specific values relevant experiments probability 67182122 values practice testing situation given A limitation typically identify number trials experiment obtain estimates probability values good strategy high confidence It addresses complexities doctors situation defines strategies PA0 algorithm trials experiment approximately identify optimal The algorithm presumes class search structures andor notably trees PA0 optimal strategies general process general class uses existence considered When dealing learn strategy cost algorithm An extended version paper available statistics solving relevant performance tasks report basic algorithm presented technical 1 I discusses techniques functions success probability known priori This paper specifies required nearlyoptimal observing Section 2 trials generalizes structures decision arbitrary structures Section 3 specifies set observed high probability optimizing certain search structures gather necessary function variants applications 2 Framework 21 Decision structures The doctors task presented term Simon Kadane problem configuration events Section 1 simple example satisjcing search 2 1 goal single satisfactory test results Other combination case informative R Greiner l OrlmnenArrijicial Intelligence 82 1996 2144 23 Patient hepatitis f el f e2 l Fig I An andor tree representation decision structure GI examples product position treasure chests general constraints involve problems specimen include satisfactory 6 competing prizes quiz 211 performing inference 61 screening employment 6 mining simple expert systems performing sequence tests tasks involve searching general decision arbitrary number experiments constrained candidates decide In structures precedence gold buried 10221 arcs encode precedence trees More general versions diagnostic task represented trees G1 Fig 1 Here nodes A el correspond doctor serum el succeeds The experiment associated A node formally given line graph Fig 1 states relationshipseg patients blood reacts particular e2e5 arc e2e6 arc Hence succeed The set arcs descending draw blood patient indicated horizontal guaranteed el attempted e3 test experiment Andor decision andor decision experiments perform experiment performed degenerate node disjunctive connecting patient hepatitis holds The number near arc designates costs units incremental arcs traversed collapsed reduce el cost performing eo node e3 test I unit iff condition conjunctive simpler reduce twolevel el subgoal cost traversing 2 forth The sum costs additional This cost specification means trees blood serumA experiment archence draw blood el A e3 V e4 V e2 A e5 A e6 experiments trees trees The general class decision andor trees First andor structures shall consider trees encode simple Beyond andor strictly general mulae include experiment connections ands ors In general want express complicated ships experiments experiments ships complicated Third andor performing performed performed succeeded failed cost In general want cost depend e andor vari interrelation 3 5 specified relation simple precedence XOR m experiments relatively e depend experiments experiment experiments general want specify incremental form cost function trees use restricted boolean combination trees permit Second andor experiment 24 R Greiner I OrponedArtijicial Intelligence 82 1996 2144 ous prior experiments complicated ways computing experiment extended paper 111 successful There situations require particular cost performing incremental extensions define general class decision structure involve arbitrary constraints To accommodate tures A decision general precedence formed certain specified experiments success failure specified correspond hepatitis failures subset experiments experiments decision structures defined given arbitrary nondecreasing result The overall arbitrary boolean combination test result performed struc set experiments W eiyl patient successes sequence function This leads prevent experiment costs performing Notation Given sequences T al formed concatenating CT rie extended sequence The definition manner A sequence 1 II monotonically trivially subsequence sequence u T subsequence sequence CT r al Q 71 T let g 7 refer cnr 71 r obvious T r r Ci rhi function h The sequence r denoted increasing case u r single elements Definition 1 Decision structures A decision structure fourtuple G x F R c 0 W e F C WxI e set experiments x W precedence iments performed given Eg F el means e2 means successful following experiment conditions use The results previous relation specifies exper sequence experiments ex F el initially ei performed e2 performed et performed es performed unsuccessful abbreviated ei E W sequence satisfy les This ii E notion legal sequence form ett e E W appears labeled experiment sequence ek specified F relation sequence holds Furthermore precedence constraints F elkI lles ekk k The collection sequences enr_trt_te denoted LLISG eli l m l R CLSSG S FU th e result function renders sequence labeled experiment legal R maps lles S FU require R monotonic Ra F 3 R IJ T 3 c cr r lles lRt cost function nondecreasing sense overall l c LLS G cost It required lles specifies given test successful Success Failure Undecided We RU S RT 7 S maps lles nonnegative ca real T 2 cu g g 7 We let DS refer class decision structures To illustrate definitions diagnostic tree Fig 1 encoded structure Gi ei es FI RI cl es corresponds patients blood R Greiner P OrponedArtijicial Intelligence 82 1996 2144 2s mean es respectively reacts SerumA ical test The F relation FI 0 ez Fele3 dicate ceeded The RI function Rez RI el performing 1 2 es Notice reachable subsequence conditions FI el e2 When structure e5 corresponds encoding precedence patients et e2 performed respectively Fleed e4 performed includes Rleles relationships initially FIIe4 FI el liver positive cytolog includes FI el e3 et suc S 3 cost incremental S Rle1e4 cl encodes e6 ez 24 The cost function s forth Rlelez sequence experiments 1 4 crereseze6j standard graphlike decision structures example cr er e3 instance 1 22 e4I 1 6 stays performed Fae r r lles include experiment implies F7 e T e For instance condition FI 0 e2 experiment f o 11 ow condition unique minimal FI eez reachability treelike Formally define Definition treelike pathe 2 Treelike decision structures A decision identify experiment encodes necessary sufficient conditions e E W single minimal structure G W F R c lles denoted reaching e Ve E W 3pathe E CCSG Va E fXCESG FcTe u pathe C The class treelike decision structures denoted TDS When structure G represents andor decision tree lles pathe corresponds unique path leading experiment goes ea el pathe4 eat e tree path eq Gt et 22 Satisficing search strategies A search strategy satisficing associated decision structurein doctor perform tests determine A strategy represented binary tree example search problem order traversal sample application Gi Fig 1 tells specifies strategy 01 decision structure The strategy specifies tree labeled experiment given situation For example right Fig 2 represents possible Each internal node strategy node decision performed experiment labeled test succeeds Or advances Slabeled node signifying success Alternatively tree rooted definition process e4labeled node follows arc strategy subtree rooted e3 fails 01 follows et associated root ui If et succeeds 01 performs labeled sequence experiments performs eslabeled node performs e3 If 01 strategy follows 01 terminates arc descending e4 forth A general patient hepatitis tree shown structure Gi corresponds 26 R Greinec l OrponenArtijiciul Intelligence 82 1996 2144 In node strategy associated node indicated tree experiment ei node uk s 0 IWI t s rzo1 s _ r _ t _ I r1 _ 3 3 S e1 1111 1 dd 69 Fig 2 Decision structure GI associated strategy tree 01 binary Definition 3 Search strategies A strategy decision structure G q E R c tree 0 N A ZV ZJ N set nodes node arclabeled A C N x N set arcs connecting 1 maps internal node n E N tree experiment node S 3 The arclabeling internal node exactly e E W leaf 1 maps arc E A Each descendants The nodelabeling arcs labeled descending nodes A path T 0 alternating sequence nodes arcs leading root sequence form r 121 a12n2U23 tree leafie nk ni E N Uiit ninit E A Each path associated 1Nnk_zAUk__lk For labeled experiment 0 proper strategy G following conditions fulfilled path rnrar2nk sequence Er EnrZ4at2 aklk 1 lr E USG lrr legal labeled experiment sequence 2 3 R Greiner P OrponedArtijcial Intelligence 82 1996 2144 21 R lr INQ Lsuccess failure E S F label final node U al j k Zt _ jr RZt ir 1NnlAaijl IS t e sequence proper prefix path conclusive h j elements lNfi1iAai2ft Zn We let path let SSG SSDs structures refer SG set proper paths refer set strategies defined decision class strategies strategy 0 We structure G decision 1 G E Ds refer For illustration notions Fig 2 There 10 paths dicated letters S F 11aua1sa566 forwhichthecorrespondingllesislrrl6 figure For instance path strategy 01 corresponding tree 01 shown leaf node right 16 e 03 es 23 Optimal strategies We wish identify best strategy traversing given decision structure strategy expected cost minimal As depends success probabilities experiments individual To state precisely define different strategies optimal different distributions cost strategy Definition 4 Expected ture G WERc experiment distribution strategy weighted probability success probability The expected p denoted C 0 Let 0 strategy decision struc maps defined sum cost path function cost strategy 0 relative p W 0 l distribution CO c epath PZT x cZr Here probability path rr defined pZrr nrjEIrj Pan ei pe pe f equals 1 pe equals Definition 5 Optimizing structures D C VS function OSS maps decision structure G w F R c E D distribution minimal That p E 0 1 w strategy SSG cost class decision An optimizing functions function VG E D vp E O llW vo E SSG COSSGp CO For brevity denote O optimal given distribution p decision structure G understood strategy OSSGp provided optimizing context function 28 R Greinec l OrponenArtcial intelligence 82 1996 2144 While definitions assume experiments independent definitions plicated situations theorems extended handle com Since dealing finite decision structures optimal strategies impractical andor exhaustive dealing decision trees optimal strategies polynomialsize structures concise encodings search Of course exhaustive representations general search strategies Nevertheless allowed cf optimal special cases Garey constraints polynomial finding regular tree subgoals multiple predecessors 211 later extended determined 6 provided algorithm represented teresting strategy junctive Kadane algorithm cial case success intermediate global success currently known optimal strategies andor presents efficient algorithm case Smith general case andor dags problem probabilities time optimal search 221 Simon deal directed acyclic graphs spe In dags node implies global success 81 It time 181 In NPhard success results question exist instance Natarajan search strategies reaching specified goal node problem finding optimal serial strategies finding optimal depthfirst 22 provides algorithm trees Some partial polynomial 1 201 NPhard requires 3 The PA0 algorithm Each abovementioned algorithms assumes optimization precise success known course case real probabilities observing use estimates compute nearoptimal strategies computed estimates lead drastically different strategies Fortunately errors probability experiments probabilities life situations The best estimate set trials experiments strategy A potential pitfall algorithms small changes cost strategy obtained paper means choice actual strategy This realization estimates sensitive approach sensitive errors main contributions estimation strategy Below algorithm PA0 conjunction opti use estimates obtain nearoptimal outlines function OSS Section 31 formally defines task finding approximately strategies algorithm The following issues First Section 32 compute mizing optimal technical task samples experiment high level confidence optimal discuss sample complexity guarantee estimates close strategy based resulting needed sections Section 33 addresses second problem guaranteeing PAO algorithm obtain sufficient number samples experiment The main context arises Fig 1 sample complexity analysis suggest precedence For example constraints example able complication diagnostic R Greiner I OrpnenArcial Intelligence 82 1996 2144 29 Algorithm PAO G DS E R 6 0 l p GSGa I GS oracle 0 polynomial number times I OSSG Return 8 End PAO Fig 3 Outline PA0 algorithm doctor needs impossible e2 structure Gi succeeds general treelike decision general obtain 100 samples test CytologicalTest able perform biopsy patients In Section 33 provide solution structures observe task intractable liver This experiment problem 31 The PA0 task A PA0 problem instance consists decision structure G W E R c E DS required confidence S E 0 1 The samples drawn random produces bound allowed excess E E iR uses oracle 0 algorithm fixed unknown distribution PA0 high probability For algorithm instance expected O OSS G p optimal probability cost optimal strategy cost returns close 1 8 cost strategy O strategy true unknown distribution optimal Stated precisely strategy O E SS G let p Then E higher We split PA0 task subtasks statistics OSS uses statistics Fig 3 subroutine GS gathers produce appropriate relevant strategy The GS subroutine takes input decision structure G parameters E The subroutine oracle c3 specified specified number produces obtain j estimate estimates p fit ith experiment know priori 6 computes samples required makes calls vector probability success probability assuming experiments values directly bother estimation The PA0 algorithm function OSS estimated probabilities running instead unknown PA0 algorithm GS OSS functions rely ones provided earlier researchers know values simply use concludes true values We concentrate samplegathering ei E W To simplify description appropriate optimizing success probabilities If happen 30 R Greinec P OrponedArtifkial Intelligence 82 1996 2144 32 Sample complexity We analyze perform Here assume access sample complexity PA0 task simple case experiments success probabilities need estimate oracle 0 request produce sample population complete labeling CK t ei E W 0 The GS routine performs 1 passes experiment number M specified calls c3 oracle returns vector probability estimates 6 61 pi 1 M cy function cost uses values We prove strategy O OSSGp 1 6 Corollary A1 Appendix A E optimal reliability OSS optimizing M p21ngn C worstcase cost performing W In fact improve constant C somewhat Let denote De sequence experiments maximal cost concluding Formally sequence experiments beginning experiment e Definition define 6 Let G W F R c decision structure For experiment e E W De maxccu e p ca 1 ff ef p E fXESG The 3 indicates max range values We let C maxeEwD e maximum remainder cost starting values easy compute underlying structure e E W These De andor experiment decision sum costs trees arcs cpathe unique path andor For De4 10 2 8 instance tree leads root experiment tree Gt Fig 2 Ctt 10 cpathe4 tree De CtOt cpathe Crot e Definition 2 2 cost pathe result takes experiments The sample complexity bound 1 derived account general performing simple case prove M samples 1 6 confident probability directly definition cost strategy independent function cost obtained Appendix A Corollary A1 samples difficulty labeling proof correct value pi based optimizing precision probability To briefly outline estimates suffices guarantee estimate e2nC E optimal strategy R Greiner l OrponedArtificial Intelligence 82 1996 2144 31 33 Learning treelike structures The simple PA0 algorithm presented assumes oracle 0 produces e labeling sample Instead component returns complete vector CK learning statistics collect e values fZ K watching performance complete E 0 1 query In practical situations oracle typically needs available individual element perform large set samples In context diagnostic example task sufficiently recording learning module observe patients pass learner compute approximately We view learning use O strategy terminate learning phase protocol examining 161 overall performing useful work doctor examines patients optimal strategy ao instruct tests After gathering information doctor patients From assume oracle 0 queried provides unlabeled In order value label sample K GS subroutine actually labelings sample patient K Z K sample determine reach perform experiment e K problematic I values case decision Computing instance determine patients blood react draw blood patient Hence learning probability blood reacts PrDraw blood 0 event blood serumA intermediate structure Gi doctor experiments immediately serumA unable doctor able extract able estimate Fortunately reaching way let p ei probability This notion fol execution ef lowing small strategy unlikely smaller need fewer samples value success probability e In limit chance reaching ei pe 0 need samples OSS produce optimal strategy Ij_ pI 1 1 problem The critical observation experiment If pei obtain samples experiment However cost optimal strategy reach ei p e means value p ei sensitive formally defined Definition 7 Let G W F R c E 27s decision distribution strategy 0 N A IN IA E SS G experiment probability maps experiment 0 reach e function structure p W f 0 1 success probability For e E W let p e 0 Pe C plnn 7Ilnr We considering oneshot learning learning phase We considering ways modifying mentally better 9 1 Also issue differs ExplorationExploitation context Bandit problem cost learning performance cf 13 I7 I concerned minimizing infinite sequence samples systems learner sets strategy incre strategy gradually time tradeoff discussed cumulative 32 R Greiner II OrxmenArtijkial Intelligence 82 1996 2144 nodes n strategy 0 labeled e rn sum 0 leads Definition 4 Finally n probability let pe maxp e 0 path pZrn 0 E SG defined path The formula pe reduces articularly simple form decision structure In case pe fli pfi pathe ftztt G treelike unique path Fig 1 pathe4 eo et pe4 peo x pet e G For instance Gt decision leads fk structure Now let O OSS Gp actual optimal correct probability vector p pi PA0 algorithm produce based estimates j J lemma need ensure We wish bound cost difference C 0 C shows place obtaining product p ei x Ipi 1 small ei 2 precise estimates probabilities p 06 OSSG j3 strategy strategy based unknown GS subroutine obtained Or The following pi pn vector success probabilities Lemma 8 Let G w F R c decision structure 1 WI n experiments Let p strategy based estimated K j 81 p PI vector estimates Let optimal search strategy G respect OSS Gp probabilities let 0 OSSGp Then Cp C 2kDCeil X pei X IPi il il true distribution A complication arises fact pei values actually depend values p Fortunately approximate estimates pi In essence need aim ei time reach ei improve estimate pi Ipi PiI error bars time path ei blocked unknown obtaining certain number times reduce confidence value p e reduce The rest subsection shows estimate products p ei x lpi Ijil computing nearoptimal treelike decision strategies general structures structures discusses difficulties e22 ek pathe eiztr Dealing treelike decision structures Given experiment e treelike decision unique minimal structure G recall lles determines e performed We strategy 0 E SSG direct strategy e contains 0 labeled theexperiment fi equals experiment es node labeled ek labeled e We denote example sense root arc node labeled node labeled arc leads node labeled e Se As et 01 E SSet et tlabeled arc ift equals descends lles initial segment arc node descends class direct strategies strategy 0 shown Fig 2 goes directly arc labeled e2 labeled f labeled experiment 2 Appendix A contains proofs lemmata theorems corollaries presented text R Creiner P OrponedArtijicial Intelligence 82 1996 2144 33 Algorithm GS G IDS ForEach eEW E lR 6 0 1 Find 0 E SSe tote c 0 t 0 suee End ForEach While se 0 Get sample K oracle c Execute strategy 0 sample K After performing experiment ei totej totej 1 iZ ej Zej 1 If ej succeeds sucej tcejl If tjS result e reached success failure means 1 End While ForEach Bei e E W suc ei tot ej 1 z totei 0 End ForEach Return P eie End GS Fig 4 A GS algorithm treelike decision structures contains hand similarly 01 SSe2 The GS algorithm direct route ea initial segment 01 E SSe3 On strategy digresses consider es e4 01 SSe4 01 considers et e2 shown identifies Fig 4 deal treelike decision direct strategy 0 E SSe The algorithm general strategies performing common ternatives Nor consider observe andor tree structure After selecting experiment number times experiment number attempts GS updates counters tot ei suc ei m ei ei performed remain cost identifying outside different experiments structure G e E W There al strategies counters record number times ei succeeded instances time GS performs respectively totei trees constructed efficiently directly set strategies GS associates performed As processes incrementing initial path e paper consider choose 34 R Greiner I OrponedArtijicial Intelligence 82 1996 2144 e incrementing experiment menting m ei time GS attempted e strategy 0 failing sucei The remaining challenge reach ei identify use strategy time experiment reach experiment decre e succeeds ei performing trials different experiments On sample GS identifies general able observe strategy es 0 If samples terminate pass OSS algorithm Otherwise GS selects needy experiments executes Notice needy experiments obtained vector estimates p PA0 e associated GS decrements GS collected strategy 0 returning Clearly GS uses e mo e initial values counters counter sample viz aiming Hence requires currently terminate The algorithm course use far fewer samples mei values different experiments ej counters zero GS associated experiment c cEW mo e samples polynomial 0 strategies reduce GS changed unreachable behavior algorithm number samples decrease counters experiments process following O The following e deemed lemma characterizes Lemma 9 Let G W F R c treelike decision structure 1 WI n experi ments let p PI p vector success probabilities experiments let E 6 0 given constants let j 1 vector Furthermore estimates computed GS algorithm Fig 4 Then probability vei E W Pr Dei X pei X IpipIil 3 6 f While analysis uses p ei values notice GS algorithm actually following results Lemmas 8 9 obtain Combining computes theorem p vector success probabilities Theorem 10 Let G W F R c treelike decision structure 1 WI n experi experiments ments let p PI Furthermore let E S 0 given constants let BP PAO G E S strategy produced PA0 algorithm GS subroutine Fig 4 Then probability optimal strategy probability C 0 E 0 OSSGp 1 6 C O vector p treelike decision structures While specific GS algorithm presented related algorithms treelike decision Beyond applies learn strategies p e required fact distinct ways reaching experiment structure decision structures The main challenge product p ei x Ipi pi complicated genera1 decision estimating structures bound To address task recall Definition 7 pe e maximum maximum probability taken possible strategies We experiment reaching R Greinec P OrponedArtificial Intelligence 82 1996 2144 3s 1 SSG maximum values value estimating p e 0 possible strategy estimate e 0 E 1 value maxbe 0 1 approximate 0 taking p e 0 probability pe maxp e 0 1 0 E SS G probability 0 t SS G given decision number strategies 1 S Even structure G exponential structure G SS G limit number e 0 values need considered From point view GS algorithm based observation necessarily yield use GS algorithm strategy 0 This size G ways exploiting e In fact e structure G direct strategies 0 E SSe largest values p e 0 straightforward p e 0 pe identify experiment treelike structures treelike The extended paper experiment possible SSG uses dynamic programming layer certain result shows exponential computational number experiments techniques sequentially types decision structures Unfortunately complexity algorithm 1 l estimate includes algorithm probabilities following general likely time algorithms oneway error cf time algorithm consequently deterministic polynomial Theorem 11 Assume RP NP RP class problems solvable probabilistic 121 Then probabilistic polynomial time algorithm polynomial e E W distribution given decision structure G W E R c experiment function p W value pe F probability 1 6 l parameters E S 0 estimate 0 4 Conclusion The results presented required search strategies specifically user supply precise success probability 26182122 defined class decision paper motivated extend objective finding provably good search strategy Each satisficing structures ways First strategies search lines research The underlying comes work optimal earlier papers considered experiments Our work extends defined general encompasses general setting estimates Third provided efficient algorithm probability RP NP errors probability finding good estimates values case treelike decision structures proved body research structures models Second analyzed sensitivity optimal search strategies efficient algorithm task general structures generalizes decision framework values Our approach resembles 51415 work speedup chunking learning tionbased suggest way improving systems use single example works showing use set samples describing 131 uses previous solutions Most speedup learning suggest improvement extend speed performance furthermore learning including explana 36 R Greiner P OrponenArticial Intelligence 82 1996 2144 exact number samples purely heuristic considerations use mathematically new strategies close optimal provably high probability required Also speedup learning systems based guarantee sound techniques Finally work derives mathematical methods title 23 We hope enriched learning PAC framework outside traditional application approximately correct field probably field providing setting concept learning Appendix A Proofs This appendix contains proofs results mentioned body paper Lemma 8 Let G K E R c decision structure 1 WI n experiments Let p strategy based estimated p PlP vector estimates Let optimal search strategy G respect 0 OSS Gp probabilities vector success probabilities let 0 OSSG G p al Then C7p C ZeDei X pei X Pi PiI il vectors p 5 let p denote p Proof Given special cases pO p p 8 We shall prove strategy 0 G 0 n 1 vector pil CIO ClOI Dei X pei X PiiI A1 implies IC CII JCjc Cjcrlb0I CDei Xpei X IippiI il il Applying CD 0 0 yields bound desired result strategies O OF noting optimality Cb Ot C C Cp Cl Cfip Cppl C7 Cppl G kDei il X Pei X lpipi 2kDei X pei X IfiiPiI il 1 il 0 kDei Xpei X IiPiI 1 R Greiner P OrponenArtijicial Intelligence 82 1996 2144 ui Uj A U7 I 0 W rjf uj Tjt tuj Fig A I Illustration notation In proving inequality Al shall use following given node u strategy 0 let Tj denote path Fig Al root 0 briefly crrl final entry extend Ui For path Tj denote associated cost clTi Here extended I function Ti leaf node partial sequences ui tree We strategy cost function incomplete paths strategy tree defining notation leading cf CiiiI3il Uk OOliililOOlli ua root node strategy tree furthermore connected path tree uaaot ut uklkuk single nodes define c Ui c ui set leaf nodes Let uf descendant node Ui let tUi denote u For U E tui let Tjl denote path uj 1 LQ E t I let T denote subtree path node u7 ZQ Let COf ui denote expected cost Analogous definitions hold u7 t ui pr_l C 07 For given experiment ei E W let Nei iujjtk set nodes 0 labeled e Let 0 ei subtree 0 consisting paths root 0 node leaf let 0 Pi subtree consisting Nei 38 R Greinec P OrponenArtcial Intelligence 82 1996 2144 paths Notice paths O according strategy 0 experiment leaf node use representation influences eis success probability C 0 Cu 0 1 ei C 0 1 Fi We partition ui nodes passes e occur path root recall obtain simple formula expressing pi function C 0 1 ei CO pmcm 1 PTjPiPr cTj CLtj ci C PTil LIEtu k r CPtri Pi j I C uEtu PiPri CTj CUj Cri PT Crj CU7 Crj J l Pi C PTTCTj CUF CTi uEtu f I PT P Ccrj CUt C pTi pTCT uw jl I 1 l Pi crj Cu C pT uEtu R Greiner l OrponenArtificial Intelligence 82 1996 2144 39 k c jl Prj PiCrj Cp 1 PiCrj CpC I PtTj CCnj PiCpT 1 PiCp 1 Near end proof simplified elementary probabilities sums 1 case formulas fact complete c P c hj wwu ulEtu 1 An analogous formula derived PlPildPil ith value Using facts CP 0 1 pi Ce 0 1 gi CPT CD pn probability cost function Cb 0 ei vector differs JJ substrategies following bound 0 1 Ei 07 j involve experiment e obtain ICC Cfizl CI 1 ei CO j CiO ei CjjO I Zii ICp ei Cp e CpO I Fi CfiO I Zil k G CPbj x IPi PiI jl Pei X JPiPi x pox_ max C 07 C 0 The line calculation p e 0 pej ltlaXpei probability All remains strategy 0 reach ei E SG 0 value maxlujYCP OJ C I 0 uses facts CIJ 3 0 Cr prj bounded bounded D e TO consider Uj 1 uj ei Then cq max crj uEru 1 A21 14 40 R Greinec P OrponenArticial Intelligence 82 1996 2144 ctu max 97 uEru max CTJ UEfUj Dte Similarly C maxlEfLj c7rjl Dei I 0 Lemma 9 Let G w F R c treelike decision structure merits fet p PI E 6 0 given constants let 5 I vector probability estimates computed GS algorithm Fig 4 Then p vector success probabilities Furthermore lW n experi let Ye E W Pr De x pe x Ipi piI f1 8 Proof We use Hoeffdings let X set independent common mean samples Then inequality simple form Chernoff bounds identically distributed random Bernoulli variables p Let SC 1 h4 CE Xi sample mean taking M Pr SCM p A e2Mh2 Pr SCM p A eI A3 To prove inequality cases depending F e consequently experiment pei I A2 holds experiment performed ei E W consider initially If GS algorithm perform trials e As samples drawn random inequality m e samples A3 A4 fixed distribution use Pr pe x Ipi piI 2 E 2nD ei 1 Pr lliipil 3 2mi A j n Now consider experiment ei immediately F 0 e hold Here GS algorithm attempt path total M times reachable reach ei direct A5 R Creiner l OrponenArtcial intelligence 82 1996 2144 41 Of GS reaches performs ei number k times reach e value product pej x lb piI value k remaining M k times Denote jk assuming GS succeeds exactly k times It suffices 1 sn probability jk El2nWe Using fact j e kM estimated value p ei GS suc ceeded k times M inequality I 62n A3 know probability probability 1 62n If Pil G lnf Of course terms define tops 1 p ei 6 1 Ipi piI 1 We observe jk gk probability 1 S2n 3 1 sn We need bound largest possible value gk First note gk bounded 0 6 k ko whenkokM ln4n6 The expressions ko l2 As expression positive largest allowed value k viz ko Using second expression maximal k ko k M gk value k ko k largest second derivatives upwards concave interval k ko M value derivative respect Hence largest value g k bounded value second expression k ko k M larger values gkolnF J gM A By inspection values values gk 42 R Greiner P OrponenArtijicial Intelligence 82 1996 2144 As pei X Ipi pi jk gM need M sufficiently large gr e2nDei Solving gw e2nDei M yields M G ljln m ei value Eq AS gM value smaller To corresponding 1 y r2 holds y 0 particular taking mei sufficiently Notice samples confident small desired larger observe M 1 6 y e nD ei Hence product p ei x dm Ipi pii typical situation E small relative De m ei value obtained slightly larger value obtained Eq A4 q Theorem 10 Let G W F R c treelike decision structure WI experi PAO G E 6 strategy produced PA0 algorithm GS subroutine Fig 4 Then probability optimal strategy 1 CO 6 O probability vector p C OSSGp Proof Let pt vector probability GS subroutine By Lemma 9 products D ei x p e x bounded value e2n probability e2n 0 Lemma 8 1 sn Hence estimates produced pil upper 1 6 The theorems claim follows probability lpi Corollary Al G W E R c decision structure probability I 6 C O optimal strategy based correct probability Let pa PAO G E S result PA0 algorithm DS E 8 0 given constants Then C 0 E 0 OSS G p vector p Proof This result easy Lemma 9 observation value A4 Eq 1 larger value m ei Eq A4 q immediately follows C D ei guarantees proof Theorem 10 Theorem 11 Assume RP NP Then probabilistic gorithm distribution pe given decision function p W structure G W E R c experiment 0 1 parameters E 8 0 estimate E probability 1 6 polynomial time al e E W value Proof Assume e 6 0 F l3 8 We algorithm decide algorithm exists fixed values contrary R Greiner P OrponenArtijicial Intelligence 82 1996 2144 43 satisfiability NPcomplete boolean formulas SAT reliability follow standard arguments 1 6 3 As SAT problem RP NP Let D boolean decision formula variables XI x We construct structure G W F R cq formula 40 0 We g W 1 respectively variables assignment satisfying specific experiment satisfiability formula sp reliability 1 6 running hypothetical structure G experiment g checking estimate corresponding respectively value pg decide algorithm provides pg greater 1 E E That general permitting F ekl Fa ekl hold dence relation Fp permits exactly et Zr performed ez Z2 experiment The structure G 2n 1 experiments W et et e Z g The prece exactly initially exactly ek Ek kth iff cy form se ZZ Z identified false relation F finally 6 z2 quence Ptype experiments truth assignment specifies truth assignment Zi Zi Y The precedence F crg holds w h ere Ei ei Now complete x xi true respectively experiment satisfies d ei respectively variables xl Y Zt zk rp g performed formula trivial probability distribution recall pg Consider experiments g strategy Given constraints exists strategy 0 reaching g assignment pg 0 q clear truth qo strategy pg 0 1 Hence pg 1 exists satisfying maximum probability reaching experiment assigns success probability cp satisfiable specified precedence 1 References N Alon JH Spencer F Erdos The Probabilistic Method Wiley New York 1992 JA Barnett How control knowledge worth primitive example Arf Infell 22 1984 tests hypothesis 7789 DA Berry B Fristedt Bandit Problems Sequential Allocation Experiments Chapman Hall London 1985 efficiency H Chemoff A measure asymptotic observations Ann Math Stat 23 1952 493507 G DeJong R Mooney Explanationbased 145176 MR Garey Optimal II Geiger JA Barnett Optimal 1991 R Greiner Finding 1991 95116 R Greiner I JuriSica A statistical AAAI92 San Jose CA 1992 learning alternative view Mach Learn 1 1986 Proceedings AAAI91 Anaheim CA constraints Discrete Math 4 1973 task sequencing precedence redundant knowledge EBL utility problem optimal derivation Proceedings base Art Intell 50 sums tree searches solving based satisticing approach strategy III 121 131 141 151 161 171 181 191 We indebted Tom Hancock suggesting reduction 44 R Greiner t OrpnenArticiul Intelligence 82 1996 2144 I IO I R Greiner I Orponen Probably approximately optimal derivation strategies eds Proceedings KR91 Cambridge MA Morgan Kaufmann JA Allen R Fikes San Mateo CA E Sandewall 1991 I 1 I I R Greiner P Orponen Probably approximately optimal satisficing strategies Tech Rept Siemens Corporate Research 1993 I 12 I DS Johnson A catalog complexity J van Leeuwen Cornpter Science A Algorithms Complexity Elsevier Amsterdam classes ed Handbook 1990 67l 86 Theoretical 13 I JE Laird PS Rosenbloom A Newell Universal Subgoaling Chunking The Automatic Generation Learning Goal Hierarchies Kluwer Academic Publishers Hingham MA 1986 I 14 I S Minton J Carbonell CA Knoblock DR Kuokka 0 Etzioni Y Gil Explanationbased learning problem solving perspective Artif Intell 40 1989 63119 I IS I TM Mitchell RM Keller ST KedarCabelh Examplebased generalization unifying view Mncll Leurn 1 1986 4780 I 161 TM Mitchell S Mahadevan LI Steinberg LEAP learning apprentice VLSI design Proceedings IJCAI85 Los Angeles CA 1985 573580 I 17 I KS Narendra MAL Thathachar Learning Automata An Introduction PrenticeHall Englewood Cliffs NJ 1989 I I8 I KS Natarajan Optimizing depthfirst search ANDOR trees Research Report RCI 1842 IBM TJ Watson Research Center 1986 I I9 I P Orponen R Greiner On sample complexity finding good search strategies Proceedings COLT90 Rochester I20 1 S Sahni Computationally 121 I HA Simon JB Kadane Optimal problemsolving 1990 35258 related problems SIAM J Cornput 3 1974 262279 search allornone solutions Arf Intell 6 1975 235247 I2 1 DE Smith Controlling 23 I LC Valiant A theory learnable Commun ACM 27 1984 1134l 142 inference Artif Intell 39 1989 145208 backward