Artiﬁcial Intelligence 171 2007 440447 wwwelseviercomlocateartint A hierarchy prescriptive goals multiagent learning Martin Zinkevich Amy Greenwald b Michael L Littman c University Alberta Edmonton AB Canada b Brown University Providence RI USA c Rutgers University New Brunswick NJ USA Received 13 May 2006 received revised form 15 February 2007 accepted 15 February 2007 Available online 30 March 2007 Abstract A great deal theoretical effort multiagent learning involves embracing avoiding inherent symmetry problem solution Regret minimization approach prescriptive noncooperative goal explicitly breaks symmetry makes assumptions adversary achieves limited guarantees In paper consider hierarchy goals begins basics regret minimization moves utility guarantees achievable agents guarantee converging gametheoretic equilibrium 2007 Published Elsevier BV 1 Introduction The prescriptive noncooperative goal set forth Shoham et al design intelligent agents perform presence intelligent agents Much research analysis domain involves bypassing circularity objective regret minimization embracing equilibrium selfplay analyses In paper try unravel objective consciously breaking symmetry agent behavior prescribe environment players1 control Much regret minimization literature speaks perspective fact concepts like calibration origins predicting weather hardly multiagent problem Nonetheless making assumptions agents environment guarantees simply achieved convergence set Nash correlated equilibria This observation prompted past study selfplay environment assumed exhibit behavior agent We believe selfplay example restriction environment limits applicability results The fact agent performs selfplay says agent interact humans agents designed Instead believe useful focus characterizations environments agent independent way described Corresponding author Email addresses mazcsualbertaca M Zinkevich amycsbrownedu A Greenwald mlittmancsrutgersedu ML Littman 1 We use player refer agent environment 00043702 matter 2007 Published Elsevier BV doi101016jartint200702005 M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 441 Different learning agents considered trading breadth set environments work work set Or focus set environments apart agent relevant attributes environment class come mind Workability What type guarantees achieved class By deﬁnition agents care utility earning utility matter debate In paper concreteness focus particular form utility guarantees discuss environment sets possible achieve guarantees Breadth How broad class As MAL researchers wish develop agents achieve guarantees broad environment class possible Consider result nointernal regret2 agents 23 set ANIR converge empirical frequency joint actions set correlated equilibria playing environment nointernal regret set ENIR An interesting question arises environment nointernal regret Moreover cared getting utility stationary correlated equilibrium instead converging equilibrium For instance add ENIR ANIR environments play stationary Nash equilibria NIR agents high utility3 Oddly add altruistic environments play opponents utilities nointernal regret instance environment dominant strategy lead easy cooperation players identical utilities difﬁcult time making common choice Thus expand environment set stronger guarantee learn makes original result work ﬁrst place Saliency Are agents class perform class Are agents considered telligent Throughout description environment sets avoid circularity addressing saliency directly However hope ﬁnd environments ultimately derived principles workability breadth superset environments consider intelligent overlap signiﬁcantly Whether outcome prevails primary measure success proposed agentenvironment split Normally researchers formulate environment class consider set environments begin concept saliency selfplay4 Between extremes multiagent learning guarantees achieved environment guarantees achieved selfplaythere lies hierarchy Deﬁnition 1 A hierarchy prescriptive goals 1 A hierarchy environment sets S1 Sk contained predecessor 2 A hierarchy guarantees G1 Gk exists single agent satisﬁes Gi environment Si In large margin structural riskminimization literature hierarchy hypothesis spaces large margin hypotheses nearer hierarchy smaller margin hypotheses nearer Assume data agrees hypothesis given supervisedlearning problem If agrees small margin hypothesis large training data required good generalization performance This guarantee 2 They referred noregret agents Foster Vohra 2 We use terminology Greenwald Jafari 4 distinguish nointernal noexternal regret 3 In particular agents ANIR CEV work environments play stationary Nash equilibria Section 3 However empirical joint action frequency longer converge set correlated equilibria Moreover agents play stationary Nash equilibria work ENIR ENIR algorithms converge best response Nash equilibrium Nash equilibrium 4 Another popular choice set stationary environments The advantage set stationarity assumption classiﬁcation tasks supervisedlearning techniques carry However reason believe intelligent agents exhibit stationary behavior They learn 442 M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 near hierarchy weak On hand large margin hypothesis agrees data generalization performance excellent training examples A single algorithma supportvector machinecan achieve guarantees If algorithm exist hierarchy generalization guarantees hypotheses spaces viewed collection goals interpreted single overarching goal Similarly view proposed hierarchy prescriptive goals single overarching goal multiagent learning Our perspective similar objectives speciﬁed Bowling 1 suggests agents designed achieve different guarantees convergence Nash equilibrium best response different classes environments selfplay stationary Even closely related proposing results nointernal regret agents minimize internal regret arbitrary environment empirical joint action frequency converges set correlated equilibria In work focus implications utility guarantees analyze broader class environments handled level It theoretical elegance structural risk minimization excited machinelearning researchers impressive practical implications The advantage empiricist technique allows soft boundary expects happen Thus algorithm use data decide exactly accurate empiricists assumptions relax necessary deal realworld occurrences Whether regret methods hierarchies described soft assumptions true result performance guarantee false preclude algorithm performing reasonably well5 useful developing agents perform practice 2 A model interaction Repeated bimatrix games In paper restrict attention repeated bimatrix games sufﬁcient model fundamental issues multiagent learning6 Moreover assume game including utility functions known players Although assumption bias discussion believe good place begin We talk game G formally tuple A1 A2 u1 u2 A1 actions ﬁrst player A2 second u1 A1 A2 R u2 A1 A2 R utilities ﬁrst second players respectively We deﬁne SCEG set correlated equilibria singleshot variant G SNEG set Nash equilibria singleshot variant G As usual refer history game h A1 A2 ht joint action round t We refer ﬁrst player behavior agent second player behavior environ ment 3 A utility guarantee goal Before discuss possible behaviors agent environment ﬁx concept means perform In repeated bimatrix game goals stated terms utility best response regret minimization Because attempting develop objectives ﬁrst determine environments achieved later choose utility objective independent behavior environment agent produces behavior Deﬁnition 2 Given game G real number v agent σ G vworks environment ρ assuming history h outcome σ ρ playing G repeatedly probability 1 lim inf T 1 T Tcid2 t1 u1ht cid2 v We ρ G vworkable 1 5 An example soft assumption true hypothesis large margin 6 For instance bimatrix game formulation represent arbitrary extensiveform games notion repetition facilitates learning M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 There natural values agents proven achieve 1 The safe value v game G minimax value player 1 game safeG max a1A1 min s2ΔA2 u1a1 s2 443 2 2 The correlated equilibrium value CEV game G minimum value player 1 set correlated equilibria game CEVG min sSCEG u1s 3 3 The Nash equilibrium value NEV game G minimum value player 1 set Nash equilibria game NEVG min sSNEG u1s 4 Fact 1 The NEV game CEV CEV safe value Fact 2 There game NEV exceeds CEV game CEV exceeds safe value In words agent works certain level v average utility eventually approaches exceeds v Let relate concept game theory 1 If agent σ plays best response environment ρ game G terms immediate utility round σ G safeGworks ρ 2 If agent σ minimizes external regret 5 G environment ρ σ G safeGworks ρ 3 If agent σ minimizes internal regret game G environment ρ minimizes internal regret σ G CEVGworks ρ 4 If agent σ best response environment ρ game G terms immediate utility round ρ best response σ σ G NEVGworks ρ From ﬁrst point note best response guaranteed achieve safe value facing arbitrary environment From fourth point environment plays best response smaller possibly larger Nash equilibrium value achieved Thus natural concepts learnable like PAClearnable learnable statistical query model apply domains perfectly predicting environment guarantee good performance The level performance achieved contingent properties environment Moreover machinelearning research focuses learning independent identically distributed sequences stationary Markov process environments learn likely difﬁcult model7 Observe constructed hierarchy given game G given ρ agent σ G NEVG works environment ρ G CEVG works ρ agent σ G CEVG works ρ G safeG works ρ While hierarchy guarantees guarantees single environment class Therefore extend concept workable sets Deﬁnition 3 Given game G real number v agent σ G vworks class environments C ρ C σ G vworks ρ C said G vworkable To connect deﬁnition machinelearning literature easiest think environment classes hypoth esis classes Workable analogous concept learnable Given game G hierarchy goals consider certainly possible 7 We deny fact able predict environments behavior powerful tool It possible formalizing environment classes basis priors learning agents 444 M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 1 The agent nointernal regret8 2 The agent G CEVGworks 3 The agent G NEVGworks The ﬁrst levels guarantees nointernal regret algorithms utility guarantee form The represents utilitarian discussed objective converging stationary Nash equilibrium utility Throughout remainder paper focus hierarchy guarantees investigate hierarchy environment sets matches We goal hierarchy Hierarchy Equilibrium Utilities In section formalize environment set corresponds hierarchy We deﬁnition hierarchy end section 4 Developing hierarchy environment sets Let begin broadest possible classthe set environments S1 agents internal regret environment It natural consider environment class G CEVGworkable One likely candidate nointernal regret environments NIR Observe agent G CEVGwork NIR environments NIR environments The existence agent example achieving hierarchy prescriptive goals Now let forget origins consider NIR environments hypothesis space If consider principal property set G CEVGworkable expand achieve larger hypothesis spaceenvironment class This metaphor crucial machinelearning researchers realize broaden horizons virtually free hesitation Such expansion involve hypotheses unlikely downright nonsensical small price pay incorporating additional hypotheses In work work formally deﬁned environment classes sufﬁciently broad reasonable unreasonable environments Arguably guiding principle machine learning best bounds practical performance broadest hypothesis space Correspondingly critical theory maximal G CEVGworkable sets environment sets exists strict superset G CEVGworkable set Now problems concept technical philosophical 1 In general obvious maximal set exists add environment G CEVGworkable constructed example 2 By constructing maximal set far For instance environments work able agents ﬁrst actions left right right environments workable agents begin choosing right left left Thus trying design canonical maximal set ﬁnd plethora maximal sets environment class develop prescriptive agents Consider example point game utility functions equal player strictly dominant strategy If agent plays foolishly pay penalty It environment play dominant strategy sees certain sequence actions beginning left right right Otherwise play dominated action Such game contrived serves highlight issue exists subtle ordinary gamesan agent choose play dominant action arbitrary reasons Hence want general purpose class environments wish maximal possibly enabling agent focus peculiarity small subset environments maximal set larger Loosely 8 We include nointernal regret stronger G safeGworking applied environments M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 445 speaking productive consider environments common maximal setsthe intersec tion More formally Deﬁnition 4 An environment ρ canonical G vworkable set G vworkable set S S ρ G vworkable set Suppose agent σ G vworks set environments S If add element ρ canonical G vworkable set agent G vwork S ρ agent σ cid7 Therefore given workable set adding environment canonical set set unworkable This canonical concept avoids environments force particular line play environment requires agent left right canonical set One ﬁnal practical tweak building workable sets build hierarchy goals upwards For instance given existing hierarchy certain set agents A satisfy properties hierarchy When design higher levels hierarchy restrict agents consider included set A develop unachievable set goals Given game G design agents nointernal regret environments However G CEVGworkable set level It G CEVGworkable NIR agent agent able satisfy goals Thus consider environment sets G CEVGworkable work agent NIR Deﬁnition 5 Given game G tuple goals g1 gk tuple environment sets S1 Sk agent g1 gk S1 Sk viable achieves guarantee gi set Si Given new goal value vcid7 R relative canonical set Scid7 respect g1 gk S1 Sk set environments ρ set Scid7 Scid7 G vcid7workable viable agent σ exists viable agent σ cid7 G vcid7 works Scid7 ρ One way envision construction creating hierarchy level time Before add new environment highest level ﬁrst sure interfere existing structure Deﬁnition 6 The Hierarchy Equilibrium Utilities hierarchy guarantees 1 g1 agent nointernal regret 2 g2 agent G CEVG works 3 g3 agent G NEVG works hierarchy environments 1 S1 environments 2 S2 relative canonical G CEVG set respect g1 S1 3 S3 relative canonical G NEVG set respect g1 g2 S1 S2 5 Salience After hierarchy developed measure practicality noticing existing algorithms models intelligence ﬁt In particular comes hierarchy nointernal regret environments near provide evidence classes salient Another important aspect salience involves agents actually achieve hierarchy goals Are smallest environment classes If emergent deliberate result follows achieve highest guarantee hierarchy selfplay 446 M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 Theorem 7 In Hierarchy Equilibrium Utilities game G NIR environments S2 stationary Nash equilibrium environments S3 Proof Here prove ﬁrst result second result derived similarly The g1 S1 viable agents exactly NIR agents Therefore environment set Scid7 G CEVG workable viable agent σ σ viable NIR agent ρ NIR environment σ G CEVG works ρ σ G CEVG works Scid7 ρ This establishes environment set S2 salient cid2 One consider behaviors higher environment levels candidate agents These environments lifted artiﬁcial divide constructed agents environments performance agents analyzed 6 A continuum guarantees Although CEV NEV utility guarantees corresponding existing guarantees play interesting utility levels vary game game This point brings challenge design hierarchy environments hierarchy guarantees consisting utility guarantees game More mathematical difﬁculties arise ﬁniteguarantee case nonetheless hierarchy reasons First hierarchy goals construed deﬁning new concept rationality game Second agent achieved hierarchical goals ensuing behavior pair agents player yield new type equilibrium 7 Open problems What presented framework guide development multiagentlearning algorithms Nonetheless open mathematical problems derived examples For instance following prescriptive goals addressed Open Problem 8 Can agent constructed bimatrix game satisﬁes goals Hierarchy Equilibrium Utilities Open Problem 9 Where nointernal regret environments reside Hierarchy Equilibrium Utilities different games Are NEVworkable level hierarchy Could nointernal regret agents CEV work CEVworkable level hierarchy Open Problem 10 How Bayesian techniques ﬁctitious play ﬁt framework Where ﬁt environments As agents One ask questions technique applied bimatrix games past Also regards saliency descriptive goals Open Problem 11 In NEVworkable CEVworkable environments behaviors serve model humans Open Problem 12 Would agents work environment sets serve model humans From philosophical perspective questions posed hierarchies guarantees Hierarchy Equilibrium Utilities One consider higher level guarantees achieving particular utility quickly For instance develop uncountably inﬁnite set environment classes depending quickly CEVG approached M Zinkevich et al Artiﬁcial Intelligence 171 2007 440447 447 8 Discussion Some AI researchers ideas human learning like design learning algorithms guided ideas Since ideas human behavior like vague ideas car moves light travels design process easily astray If consider prescriptive goal design algorithms work humans natural solution problem develop descriptive model human behavior independent attempting solve prescriptive goals This tack requires ﬁrst understand learning like Since humans essence social animals model behave ﬁrst understand behaviors relevant social environments discover best represents humans However path returns essence prescriptive goal Thus end left problem address descriptive goals multiagent learning ﬁrst address prescriptive goals attack prescriptive goals ﬁrst deal descriptive ones In paper discussed bootstrap process We gave example hierarchy objectives pre scriptive agents These objectives meant indicative intelligence intelligence deal Thus lowest level process consider classes behaviors environments objective achieved Given beginning naturally intelligence hopefully better understanding human intelligence generally social intelligence The hypothesis paper important aspect multiagent learning development understanding classes agents intelligent handled intelligent agents We believe objective rests foundation multiagent learning 9 Conclusion Often multiagentlearning researchers consider set environments guided concerns salience workability select environment intelligent try build agent work tweaking agent environment hopefully useful property emerges What like highlight missing process desire breadth At extreme consider set environments While interesting guarantees regret minimization obtained assumption derive blanket guarantee utility achieve prescriptive goals environments safe value The main point paper working prescriptive noncooperative agenda multiagent learning fundamental difference way consider algorithms designing algorithms facing Whereas saliency desirable algorithms designs environments considers designed according workability breadth saliency In particular perspective considering agent perspective considering environment asymmetric Hence paper argued hierarchy prescriptive goals analogous structural risk minimiza tion framework binary classiﬁcation We believe developing hierarchy goals crucial grasping basic nature social intelligence ability interact effectively multiagent setting Acknowledgements We like thank Michael Bowling fundamental guidance direction paper This research supported NSF career grant IIS0133689 Alberta Ingenuity Alberta Ingenuity Center Machine Learning References 1 M Bowling Multiagent learning presence agents limitations PhD thesis Carnegie Mellon University 2003 2 D Foster R Vohra Calibrated learning correlated equilibrium Games Economic Behavior 21 1 1997 4055 3 D Foster R Vohra Regret online decision problem Games Economic Behavior 29 1 1999 735 4 A Greenwald A Jafari A general class noregret algorithms gametheoretic equilibria Proceedings 2003 Computational Learning Theory Conference August 2003 pp 111 5 J Hannan Approximation Bayes risk repeated play Contribution Theory Games 3 1957 97139