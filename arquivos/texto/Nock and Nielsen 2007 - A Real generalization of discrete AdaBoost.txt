Artiﬁcial Intelligence 171 2007 2541 wwwelseviercomlocateartint A Real generalization discrete AdaBoost Richard Nock Frank Nielsen b Université des AntillesGuyane UFR DSECeregmia Campus Schoelcher BP 7209 97275 Schoelcher Martinique France b SONY CS Labs FRL 31413 Higashi Gotanda ShinagawaKu Tokyo 1410022 Japan Received 1 June 2006 received revised form 16 October 2006 accepted 16 October 2006 Available online 21 November 2006 Abstract Scaling discrete AdaBoost handle realvalued weak hypotheses auspices convex optimization little generally known original boosting model standpoint We introduce novel generalization discrete AdaBoost departs mainstream algorithms From theoretical standpoint formally displays original boosting property brings fast improvements accuracy weak learner arbitrary high levels furthermore brings interesting computational numerical improvements signiﬁcantly easier handle Conceptually speaking provides new appealing scaling R known facts discrete adaboosting Perhaps popular iterative weight modiﬁcation mechanism according examples weights decreased iff receive right class current discrete weak hypothesis In generalization property hold anymore examples receive right class reweighted higher realvalued weak hypotheses From experimental standpoint generalization displays ability produce low error formulas particular cumulative margin distribution graphs provides nice handling noisy domains represent Achilles heel common Adaptive Boosting algorithms 2006 Elsevier BV All rights reserved Keywords AdaBoost Boosting Ensemble learning 1 Introduction In supervised learning hard exaggerate importance boosting algorithms Loosely speaking boosting algorithm repeatedly trains moderately accurate learner gets weak hypotheses combines ﬁnally output strong classiﬁer boosts accuracy arbitrary high levels 1415 Discrete Adaboost undoubtfully popular provable boosting algorithm 7 uses weak hypotheses outputs restricted discrete set classes combines leveraging coefﬁcients linear vote Strong theoretical issues motivated extension discrete AdaBoost 8 handle realvalued weak hypotheses 8172629 Even true generalizations discrete AdaBoost 1729 virtually share strong background convex Extends paper awarded Best Paper Award 17th European Conference Artiﬁcial Intelligence 2006 Corresponding author Fax 596 596 72 74 03 Email addresses RichardNockmartiniqueunivagfr R Nock Nielsencslsonycojp F Nielsen URLs httpwwwunivagfrrnock R Nock httpwwwcslsonycojppersonnielsen F Nielsen 00043702 matter 2006 Elsevier BV All rights reserved doi101016jartint200610014 26 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 optimization originally rooted key boosting AdaBoost strictly convex exponential loss integrated weight update rule examples loss upperbounds error approximates expected binomial loglikelihood However little known algorithms seminal boosting model standpoint 141527 model roughly requires convergence reduced true risk weak assumptions high probability In paper propose new real AdaBoost generalization discrete AdaBoost handles arbitrary real valued weak hypotheses With respect real AdaBoosts weight update fundamentally different integrate anymore convex exponential loss leveraging coefﬁcients weak hypotheses differ output ﬁnally leveraging coefﬁcients given closed form computation easily delayed end boosting case conventional real AdaBoosts 81729 The major theoretical key feature algorithm provable boosting algorithm original sense Another point saves computation time respect previous generalizations discrete AdaBoost need approximate solution convex minimization problem boosting iteration 1729 From experimental standpoint weight update rule require anymore approximation logarithms exponentials prone numerical errors Finally prevents reduces numerical instabilities previous generalizations 1729 face weak hypotheses reach perfect perfectly wrong classiﬁcation This explain experiments clearly display algorithm handles noise efﬁciently discrete real AdaBoosts Noise handling soon described AdaBoosts potential main problem 2 As matter fact interesting algorithm generalization discrete AdaBoost weak hypotheses outputs constrained set classes algorithms coincide From standpoint paper brings relevant conceptual contribution boosting Indeed complete generalization R popular discrete boosting properties clearly trivial For example discrete AdaBoost presented algorithm reweights lower examples received right class Scaled R true anymore Roughly speaking provided socalled Weak Learning Assumption holds states classiﬁer slightly different random lower reweighting occurs examples receive right class measure classiﬁers conﬁdence exceeds measure average conﬁdence examples known margin Only discrete prediction framework properties coincide Furthermore scaling property hold previous real AdaBoosts 8172629 Section 2 presents deﬁnitions followed section generalization discrete AdaBoost Section 4 presents discusses experimental results section concludes paper 2 Deﬁnitions related work Our framework rooted original weakstrong learning boosting frameworks Valiants PAC Prob ably Approximately Correct model learnability 71530 We access domain X observations 0 1n Rn Here n number description variables More precisely collect examples couples observation class written x y X 1 1 1 called positive class label 1 negative class In paper deal twoclasses case Well known transformations exist allow extension multiclass multilabel frameworks 29 In paper boldfaces x denote ndimensional vectors calligraphic faces X denote sets blackboard faces S denote subsets R set real numbers Unless explicitely stated sets enumerated following lowercase xi 1 2 vector sets xi 1 2 sets vector entries We assumption examples sampled inde pendently following unknown ﬁxed distribution D X 1 1 Our objective induce classiﬁer hypothesis H X R matches best possible examples drawn according D For objective deﬁne strong learner algorithm given parameters 0 ε δ 1 samples according D set S m examples returns classiﬁer hypothesis H X R probability cid2 1 δ true risk cid4DH bounded follows cid4 cid3 H x 1 Here signa 1 iff cid2 0 1 The time complexity algorithm required polynomial relevant parameters 1ε 1δ n To rigorous original models 1530 mention dependences concepts label examples Examples supposed labeled socalled target concept cid4DH cid3 ε PrxyD cid2 sign cid6 y cid5 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 27 Input sample S xi yi xi X yi 1 1 w1 u t 1 2 T Get ht X S WLS wt Find αt R Update 1 cid3 cid3 m wt1i wti expαt yi ht xi Zt 2 end Output HT x cid6 T t1 αt ht x Fig 1 An abstraction AdaBoost unknown ﬁxed Distribution D fact retrieve examples target concept time complexity algorithm required polynomial size Hereafter shall omit sake clarity notion target concept important purpose analysis ﬁt handle A weak learner WL basically constraints notable exceptions weak hypotheses delivers outputs restricted subset S R ii 1 required hold ε 12 γ γ 0 constant inverse polynomial relevant parameters veriﬁed regardless D Since predicting classes random unbiased coin yield PrxyDsignrandomx cid6 y 12 D comes weak learner required perform slightly better random prediction In original models assumed δ inverse polynomial relevant parameters makes constraints WL lightest possible statistical computational standpoints The discrete Weak Learning Assumption WLA assumes existence WL 1427 Simple simulation arguments WL 16 allow weakening δ superﬁcial fact weak learn arbitrary δ strong learning However conditions ε dramatically different question weak strong learning equivalent models tantalizing problem ﬁrst proof exists boosting algorithms strong learn sole access WL 27 proving models equivalent This ﬁrst boosting algorithm outputs large treeshaped classiﬁer majority votes nodes node built help WL The point easy implement yield classiﬁers correspond familiar concept representations AdaBoost 7 pioneered ﬁeld easily implementable boosting algorithms S 1 1 After 7 refer discrete AdaBoost Fig 1 abstraction algorithm Basically AdaBoost uses weak learner subprocedure initial distribution u S generally uniform repeatedly skewed hardest classify examples After T rounds boosting output HT linear combination weak hypotheses Below useful abstraction AdaBoost Zt normalization coefﬁcient elements S enumerated si xi yi successive weight vector noted wt t cid2 1 In discrete Adaboost 1 cid4wt ht cid4wt ht αt 1 2 3 ln cid4exp w1HT Exyw1 ln denotes natural logarithm The key steps AdaBoost choice αt weight date rule Fig 1 They strongly related follow naturally seek minimize following observed exponential loss 829 induction HT cid3 cid3 yHT x exp 4 E mathematical expectation Since I signHT x cid6 y cid3 expyHT x I indicator function cid4w1HT minimizing exponential loss amounts minimizing empirical risk 817 2628 turns brings boosting algorithm 728 There excellent reasons focus exponential loss instead empirical risk smooth differentiable approximates binomial loglikelihood 8 Its stagewise minimization brings weight update 2 following choice αt cid3 cid4exp cid4cid4 w1HT αt arg min αR Exywt cid3 cid3 αyht x exp cid4cid4 arg min αR Zt 5 28 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 One reason focus exponential loss brings stagewise maximization margins While empirical risk focuses binary classiﬁcation task class assigned good bad margins scale real classiﬁcation integrate binary classiﬁcation task sign real magnitude quantiﬁes conﬁdence label given Large margin classiﬁcation bring fast true risk minimization 28 Margins justify scale S 1 1 interval 1 1 829 case sign output gives class predicted Whenever enforce ht X 1 1 5 admits closedform solution naturally 3 boosting algorithm discrete AdaBoost 78 In domains realvalued classiﬁcation AdaBoost encompasses far concept One important challenging application ﬁeld ensemble classiﬁers vision 3233 domain easy obtain weak classiﬁers simple features The task requires fast accurate combinations simple In pioneering papers authors chosen use AdaBoost brings accuracy combination To satisfy condition fast processing authors consider simple features weak classiﬁers choose discrete version AdaBoost 7 makes necessary discretize real values features thresholding eventually loses useful information costs little time compute thresholds Relaxing S 1 1 handled algorithm AdaBoost Fig 1 5 5 closed form solution case 29 The algorithm obtained called real AdaBoost 81729 popular demonstrated applications language image processing 112135 Iterative techniques exist fast approximation 24 performed boosting iteration buys overall signiﬁcant load increase case solution lies outside boosting regime number approximation steps small Approximations exist 5 necessarily yield valid generalization discrete AdaBoost 1224 For purpose fast processing authors devised adhoc approximations real AdaBoost known formal boosting algorithms example Huang et al 9 use version real AdaBoost αt 1 update rule adapted discrete AdaBoost roughly 10 Friedman et al 8 Ridgeway 26 pick αt 1 Ridgeway 26 proposes leverage vote arbitrary values λ 0 1 dampen variation leveraging coefﬁ cients Other authors devised modiﬁcations discrete AdaBoost integrate classdependent misclassiﬁcation costs viewed lifting discrete AdaBoost handle real values 619 Finally papers modiﬁed AdaBoost optimize losses rooted maximization expected binomial loglikelihood 818 instead exponential loss 4 3 Our Real generalization AdaBoost We direct minimization 4 scale S 1 1 subset R This means weak classiﬁers authorized output values outside interval 1 1 Suppose replace weight update 2 Eq 5 follows cid7 cid8 1 μt yiht xi hcid8 t 1 μ2 t wt1i wti αt 1 2hcid8 t ln 1 μt 1 μt Here ﬁxed hcid8 t max1cid2icid2m ht xi R maximal value ht S μt 1 hcid8 t mcid9 i1 wtiyiht xi 1 1 t Inﬁnite values hcid8 normalized margin ht S For sake clarity suppose following subsection 1 cid3 t cid3 T hcid8 t handled ways biasthreshold output ht ﬁnite 29 code yields αt 0 μt iht xi wtisignyiht xi In cases wt1 valid distribution properties kept Let AdaBoostR real generalization discrete AdaBoost Notice 6 7 retrieved AdaBoost following gentle approximations approx imate leveraging coefﬁcient αt μt ﬁrst order approximation logarithm 7 ii approximate cid6 6 7 8 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 29 exponential update rule ﬁrstorder approximation exponential expz 1 z These approximations typically ones carried implementations real AdaBoost usually prefer ex ponential update rule AdaBoosts main trademark eventual approximations leveraging coefﬁcients approximate solution 5 logarithmic quantities 24 carry approximation 31 Basic properties We ﬁrst AdaBoostR generalization discrete AdaBoost Lemma 1 When S 1 1 AdaBoostR discrete Adaboost Proof In case hcid8 t cid4wt ht cid4wt ht like discrete AdaBoost Our update rule simpliﬁes 1 μt 1 2cid4wt ht brings Eq 7 αt 12 ln1 wt1i wti1 yiht xi 2yiht xicid4wt ht 2cid4wt ht 1 cid4wt ht cid10 wt1i wti21 cid4wt ht wti2cid4wt ht iff iff yiht xi 1 yiht xi 1 This expression weight update discrete AdaBoost cid2 Now AdaBoostR boosting algorithm arbitrary realvalued weak hypotheses In fact little bit objective deﬁne margin HT example x y cid4 cid3 x y νT expyHT x 1 expyHT x 1 1 1 9 This deﬁnition margin extends previous discrete AdaBoost 34 choice discussed Section 33 θ 1 1 deﬁne classiﬁers margin error proportion examples margin exceed θ 28 νuHT θ mcid9 i1 cid2 νT cid3 cid4 xi yi cid5 cid3 θ uiI 10 νuHT 0 νuHT θ Whenever example zero margin HT predicts label examples cid4uHT generalizes cid4uHT Ties extremely seldom occur νuHT 0 upperbound cid4uHT We let H denote realvalued prediction matches empirical Bayes rule computed S observation x S sign H x majority class examples S observation matches x We prove ﬁrst theorem AdaBoostR Theorem 1 S R θ 1 1 T cid2 1 iterations cid7 cid8cid11 cid13 cid12 cid10 νuHT θ cid3 cid4uH max 1 1 θ 1 θ 1 2 Tcid9 t1 exp μ2 t 11 Proof We need following simple lemma Lemma 2 1 1 b 1 1 1 ab cid2 1 a2 exp b 2 ln 1a 1a Proof The function righthand strictly convex b cid6 0 functions match b 1 0 Writing righthand 1 a1b21 a1b2 implies limits 1 zero cid2 30 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 Consider example xi yi S 1 cid3 t cid3 T The way use Lemma 2 simple ﬁx μt b yiht xi hcid8 t They satisfy assumptions lemma obtain cid3 μt yiht xi hcid8 t cid4 cid2 1 cid14 cid7 1 μ2 t exp yiht xi 2hcid8 t ln 1 μt 1 μt cid8 Unraveling weight update rule obtain wT 1i Tcid15 1 μ2 t ui t1 cid3 1 cid3 μt yiht xi hcid8 t cid4cid4 Tcid15 t1 Using T times 12 righthand 13 simplifying yields wT 1iui cid14 Tcid15 t1 1 μ2 t cid4 cid3 yiHT xi cid2 exp 12 13 14 We let u x resp u For xi yi S 1 I νT xi yi cid3 θ I νT xi yi cid2 θ I θ cid3 νT xi yi cid3 θ θ cid2 0 1 I νT xi yi cid3 θ I νT xi yi cid2 θ I θ νT xi yi θ θ 0 class resp chosen empirical Bayes rule We let y S S set contains observation x present S exactly example x y x y x denote total weight u examples S observation matches x x denote Bayes class Finally let x Suppose θ cid2 0 x S νT xi yi νT xi yi 1 cid3 cid3 m obtain mcid9 cid5 cid4 cid3 xi yi cid2 uiI xi xI νT cid4 cid3 x y x cid3 cid4 x y x cid3 cid2 x y νT 15 x cid3 θ cid3 I νT xi yi cid3 θ It easy inequality 15 holds θ 0 cid2 I νT cid2 I νT cid3 u I x I θ cid3 νT x y inequality looser case We obtain 15 cid3 θ cid2 I u νT x u u x x cid5 u cid3 θ x cid4 cid3 x y x cid3 1 I cid2 θ cid2 θ cid3 νT u x cid3 x y cid3 θ cid3 θ cid4 x u x cid4 x cid3 θ i1 cid5cid4 cid5 cid5 cid5 cid2 νT cid4 cid3 xi yi uiI cid3 θ cid5 mcid9 i1 mcid9 cid9 uiI xi xI cid2 νT cid3 cid4 xi yi cid3 θ i1 xy cid9 x S mcid9 uiI xi xI cid2 νT cid3 cid4 xi yi cid3 θ cid5 cid5 x S xy cid3 cid4uH i1 cid9 u x u xρxI cid2 νT cid3 x y cid4 x cid5 cid3 θ xu ρx u cid2 cid3 x y I νT x xy u cid4 x x S x Consider example x y 1 θ 1 θ cid8cid11 mcid9 cid3 max cid3 θ 1 cid7 cid10 cid5 i1 x S Then uiI xi x expyiHT xi u x u x 16 17 If I νT x y 1 expy mcid9 x cid3 θ 0 17 true righthand negative So suppose I νT x y xHT x cid2 1 θ 1 θ Fix short z y cid4 cid3 yiHT xi uiI xi x exp xHT x We x cid3 θ i1 x expz x expz u u cid4 cid3 expz expz x expz u u x u x R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 31 Fig 2 Graphical representation Theorem 1 cid4uH 0 bold curve upperbounds empirical margin distribution graph located inside dotted area As t increases provided μt small bold curve converges bold arrows step function I θ 1 bold dashed curve means examples receive right class inﬁnite conﬁdence text details If θ cid2 0 expz expz cid2 2 cid2 21 θ 1 θ immediately obtain u x expz cid2 u u x obtain u xexpz expz cid2 u x expz u x x There remains plug 17 16 use 14 obtain xexpz x 1 θ 1 θ righthand 17 cid2 1 Now θ 0 expz 1 u u x righthand 17 cid2 1 x expz u u mcid9 cid2 νT cid3 cid4 xi yi uiI cid3 θ cid5 i1 cid10 cid3 cid4uH max 1 cid7 1 θ 1 θ cid10 cid3 cid4uH max 1 cid7 1 θ 1 θ cid8cid11 cid9 xy cid8cid11 Tcid15 x S cid14 uiI xi x exp cid4 cid3 yiHT xi ρx mcid9 i1 1 μ2 t cid9 mcid9 ρx wT 1iI xi x x S The sums expectation ρ computed distribution wT 1 ρ cid3 1 expectation cid3 1 The statement theorem follows remarking 1 a2 cid3 expa22 1 1 cid2 xy t1 i1 Fig 2 gives visual interpretation Theorem 1 cid4uH 0 provided normalized margin small absolute value fast convergence empirical margin distribution graph right classi ﬁcation inﬁnite conﬁdence examples When empirical Bayes rule cid4uH 0 convergence established step function cid4uH I θ 11 cid4uH examples receive inﬁnite conﬁdence classiﬁcation receiving wrong label 32 AdaBoostR boosts labels conﬁdences Theorem 1 generalizes wellknown convergence theorem AdaBoosts empirical risk 29 θ 0 This gener alization important says virtually margin error subject convergence rate zero simply empirical risk Thus single point gives complete curve f θ upperbounding empirical margin distribution graph plots margin error 10 function θ 1 1 28 To prove AdaBoostR boosting algorithm need WLA realvalued WL satisfy Its formulation realvalued hypotheses follows discrete case 141529 basically amounts want ht perform signiﬁcantly different random case represented μt 0 A natural choice ﬁxing WLA t cid2 1 realWLAμt cid2 γ γ 0 discrete WLA 32 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 This addition provides generalization discrete WLA 1427 case μt cid2 12 γ 2 It previously remarked 1 2cid4wt ht This brings cid4wt ht second condition surprising ﬁrst glance empirical risk worse random fact equivalent ﬁrst boosting standpoint reverses polarity learning ht satisﬁes second constraint ht satisﬁes ﬁrst 8 cid3 12 γ 2 cid4wt ht Now proving AdaBoostR boosting algorithm amounts ﬁrst Theorem 1 conventional weakstrong learning frameworks ﬁx θ 0 cid4uH 0 obtain WLA T iterations cid3 expT γ 22 Thus run AdaBoostR T cid121γ 2 ln m iterations AdaBoostR cid4uHT HT consistent S Since T polynomial relevant parameters classical VCtype bounds deviation true risk linear separators 1631 immediately bring following theorem Theorem 2 S R provided WLA holds AdaBoostR boosting algorithm This theorem relies use Theorem 1 θ 0 account inﬂuence margins magnitude We integrate somewhat stronger boostingtype result says AdaBoostR ﬁtting classes WLA brings large conﬁdences right classiﬁcation This amounts integrate parameter θ strong learning model described 1 Suppose 1 θ cid14 1 given user ε δ replace strong learning condition 1 PrνDHθ cid14 cid3 ε cid2 1 δ 18 time complexity algorithm polynomial 11 θ cid14 This means high proba bility want limit probability example drawn according D small local margin From standpoint true margin distribution graph event νDHθ cid14 cid3 ε satisﬁed curve located step function ε I θ cid2 θ cid141 ε From Theorem 1 T cid121γ 2 lnm1 θ cid14 steps νuHT θ cid14 0 We remark condition νT xi yi θ cid14 equivalent stating yiHT xi yi ln1 θ cid141 θ cid14 0 T 1dimensional linear separator HT xi yi ln1 θ cid141 θ cid14 consistent S There ﬁnally remains use arguments Theorem 1 prove WLA strong learn model described 18 proving boostingtype result integrating labels conﬁdences 33 Discussion Perhaps important difference discrete AdaBoost offsprings 57829 lies fact early motivated built appealing intuition reweighting favors hard examples discrete AdaBoost precisely examples receiving right class reweighted lower This property appealing certainly participated spread use However scaling binary classiﬁcation problem S 1 1 R property fully integrate extended framework rely entirely margins classes conﬁdences instead classes This true AdaBoostR lower reweighting occurs examples current weak classiﬁers performance exceeds average margin μt 0 yiht xi hcid8 t cid2 μt 19 Thus examples receive right class ht weights increased When μt 0 polarity boosting reweighting reversed way cid4wt ht 12 discrete AdaBoost Finally properties true generalization discrete AdaBoosts coincide discrete case 331 Margins The normalized margin weak hypothesis ht 8 written μt Exywt νt x y cid4 cid3 x y νt y ht x hcid8 t 1 1 20 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 33 margin ht example x y Except output domain local margin bear similarity corresponding deﬁnition strong hypothesis HT 9 Moreover 20 evidently closer previous deﬁnition coined real AdaBoosts 82829 9 replaced cid4 cid3 x y νT y HT x cid6 T t1 αt 1 1 21 assumption αt cid2 0 1 cid3 t cid3 T In fact 20 21 exactly match weak hypotheses output 1 1 ii weak hypotheses empirical risk cid3 12 wt iii exists xi yi S cid6 T HT x t1 αt maximal possible value HT realized S Thus best lifting 20 HT 21 ﬁrst glance However outside appearances misleading The reason 9 turns convenient comes models approximated AdaBoost family It known 8 offsprings AdaBoost including discrete real versions viewed carrying direct approximate additive ﬁtting symmetric logistic transform HT x ln PrxyDy 1x PrxyDy 1x 22 probabilities PrxyDx estimated learning connection crisp 18 AdaBoostR viewed approximation algorithm family Plugging 22 9 yields cid4 cid3 y δ 1 1 x y x 2PrxyDy 1x 1 νT δ x 23 This time obtain local margin similar 20 This margin fundamental property 21 approximates poorly 22 turns theoretical local margin true Bayes rule opposed empirical Bayes rule Section 3 real prediction 1 1 computed δ PrxyDy 1x PrxyDy 1x x This real prediction leverages Bayes rule real values remarkable property best possible sense Bregman divergences More precisely Theorem 1 1 yields regardless properly deﬁned Bregman divergence B cid15 measuring proximity true label y real prediction p shall x X δ x arg min pR Exy Dx cid4 cid3 By cid15 p Here Dx distribution D support restricted examples x normalized total weight ex amples By means words δ PrxyDy 1x PrxyDy 1x value best summarizes x average different classes observation x 1 This quantity called gentle logistic approximation 8 reported stable logistic model There reasons prefer 9 21 The ﬁrst reason technical Theorem 1 shows νuHT θ vanishes WLA regardless value θ 1 1 margin deﬁnition 9 Upperbounds νuHT θ Eq 21 easy read require vanish θ smaller ﬂuctuating upperbounds cid16 1 2829 It boosting algorithm responsible case 21 yield vanishing νuHT θ θ cid2 maxt μt 2 situation identical previous analyses 72829 The second reason experimental consequence ﬁrst Deﬁnition 9 makes cumulative margin distributions easier read ﬂuctuating theoretical upperbound 1 boostable margins Following 9 fully characterize margin distribution graph true Bayes rule Indeed sum local margin distribution graphs built possible observation x X For observation x denote true Bayes class y x arg max b11 PrxyDy bx Fig 3 presents example local margin distribution graph To better catch picture consider domain zero Bayes error noisiﬁed η 0 12 class noise rate example gets class ﬂipped probability η In case δ x Fig 3 x located z η experiments examples 1 2η intermediate stair z PrxyDx y 34 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 Fig 3 Local margin distribution graph true Bayes rule single observation x X bold curve The dotted line equation z PrxyDxθ 12 depicts location inside negative stair Bayes rules prediction located function δ x circle The dotted line equation z PrxyDx positive stair square text details The reasons margin distribution graphs better errors good tools evaluate goodnessofﬁt HT come models built In Valiants seminal PAC model 30 use sign HT eventually leaving absolute value needed represent sort conﬁdence prediction 29 Thus labels counting errors The logistic framework 8 makes possible integrating sign conﬁdence estimator class conditional probabilities ˆPry 1x expHT x 1 expHT x 1 1 expHT x ˆPry 1x 24 25 Thus plotting margin distribution graph HT testing true Bayes rule goodness ofﬁt better curves come closer meaning labels tend local class conditional probabilities tend match Lifting usual properties classiﬁers classical labelserrors framework logistic framework immediate Consider example overﬁtting Such situation typically occurs classiﬁer complicated starts model better S expense domain meaning true risk increases empirical risk decrease This situation impact class conditional probabilities remains punctual example means greater intersection test margin distribution graph axis θ 0 In fact overﬁtting deﬁned means differences respect threshold 12 class conditional probabilities scarcely means estimation For example variation 001 ˆPry 1x ﬂip label predicted observation x passing 0495 0505 variation 049 ˆPry 1x ﬂip passing 0005 0495 In logistic framework overﬁtting comes differences estimations true values class conditional probabilities similarly differences corresponding margin distribution graphs Consider example situation x X ˆPrH y 1x 0 ﬁrst classiﬁer H PAC framework vocabulary predict class 1 inﬁnite conﬁdence ˆPrH cid14y 1x 040 045 classiﬁer H cid14 PrxyDy 1x 049 050 true Bayes rule Clearly H H cid14 achieve true risk corresponding examples However H predicts wrong label nearly half examples observation x inﬁnite conﬁdence H cid14 cautious wrong predictions conﬁdence approaching true Bayes rule approximately zero When comes situation H occurs andor visible starts overﬁtting Visually corresponding margin distribution graph sticked axis θ 1 It worthwhile remarking margin distribution graph true Bayes rule sticked axis regardless domain Fig 3 Thus overﬁtting means margin distribution graphs match Bayes rule The experimental section presents examples curves More generally comparing algorithms different Bayes rule best misclassiﬁes examples smallest possible R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 35 conﬁdences margin distribution graph values θ cid3 0 From standpoint classical true risk comparison Valiants PAC framework particular case logistic framework To ﬁnish margins let comment 8 When HT linear separator particularly relevant perform boosting domainpartitioning weak hypotheses 823 classiﬁers ﬁt local bucketwise computation conditional probabilities 22 The popular examples decision trees possible examples include decision lists symmetric functions 23 In cases margin classiﬁer depicted 8 admits nice expressions related Bregman divergences 17 Suppose short domain X partitioned ht ﬁnite number subsets general term Xcid14 Let wtcid14 twodimensional distribution vector entries written w tcid14 respectively denote normalized proportion examples respect wt 1 Let wt denote distribution induced negative positive classes S Xcid14 satisfying w cid6 partition wtcid14 wti There essentially ways deﬁne real values htcid14 output ht x Xcid14 The ﬁrst use logistic prediction deﬁne htcid14 12 ln w tcid14 829 The second use gentler alternative deﬁne htcid14 w tcid14 8 In cases straightforward classiﬁers margin simpliﬁes tcid14 w tcid14 w isi Xcid14 w w tcid14 tcid14 tcid14 μt 1 2hcid8 t Ecid14 wt cid3 cid4 B wtcid14 cid15 1 wtcid14 B cid15 Bregman divergence 17 KullbachLeibler divergence logistic prediction L2 2 divergence gentler alternative Since Bregman divergence quantiﬁes distortion non negative zero iff arguments equal helps classiﬁers margin WLA expectation local discrepancies positive negative class larger better classiﬁer Obviously picked discrete values htcid14 1 1 local majority class obtained μt 1 2cid4wt ht 332 Computations numerical stability A ﬁrst difference previous generalizations discrete AdaBoosts ﬁx ad hoc values αt 81729 computational Eq 5 closed form solution general case need approximate αt The problem convex single variable approximation simple needs performed iteration buys signiﬁcant additional computation time respect AdaBoostR αt exact Approximating drawback good current iteration lie outside boosting regime 81729 The extensions discrete AdaBoost 81729 face technical numerical difﬁculties compute αt ht ht reaches consistency cid4wt ht approaches extremal values 0 1 On extreme values ﬁnite solution Eq 5 theoretically weight update In case problem hold anymore multiplicative update Eq 6 zero inﬁnite adopt convention 00 1 Indeed numerator equals zero iff numerators equal zero iff denominators equal zero Thus zeroing numerator denominator amounts making perfect completely wrong classiﬁcation ht S brings weight change wt1 A second known technical difﬁculty extensions discrete AdaBoost 81729 occurs empirical risk approaches 0 1 regions αt extremely large regimes In case numerical approxi mations exponentials Eq 5 approximations αt computation weights instable Clearly large multiplicative coefﬁcients weight update possible AdaBoostR However instability pronounced split computation leveraging coefﬁcients weight update allowing computation αt delayed till end boosting 4 Experiments Experiments carried 25 domains come UCI repository 3 Domains classes indicated 2C transformed class problem grouping classes ﬁrst brought domains highly unbalanced classes complicating learning task On domain run discrete AdaBoost 7 AdaBoostR real AdaBoost 1729 WL set rule monomial learner ﬁxed maximal rule size r attribute number 2225 When output restricted 1 1 pick output ht triggered majority class according wt When output R 36 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 unrestricted use method decision trees real values leaves 29 Let wb t total weight wt examples ﬁre rule ht belong class b1 b Then examples trigger rule output ht local approximation logistic transform 22 29 ht 1 2 log w w t t The computations set examples trigger rule majority class logistic approx imation ht grown following procedure similar decision trees repetitive minimization index function Matsushitas error case 420 2229 details True risks estimated 10fold strati ﬁed cross validation procedure data Since rule ht possible outputs yht possible values analytic solution αt discrete AdaBoost apply real AdaBoost 1729 The true αt approximated 5 simple dichotomous search relative error exceed 106 results 24 faster With empirically execution time real AdaBoost 81729 average 100 times discrete AdaBoost AdaBoostR 41 General results We ﬁrst general comments results obtained early reasonable stages boosting T 10 T 50 steps boosting rule learner conﬁgured r 2 Fig 4 summarizes results obtained While AdaBoostR tends perform best interesting patterns emerge simulated domains know concept approximate Easy domains Monks12 3 Domain T 10 T 50 Balance 2C BreastWisc Bupa Echocardio Glass2 Hayes Roth 2C Heart HeartCleve HeartHungary Hepatitis Horse Labor 2C Lung cancer 2C LEDeven LEDeven17 Monks1 Monks2 Monks3 Parity Pima 2C Vehicle 2C Votes Votes wo XD6 Yeast 2C best second worst D 873 451 3457 3143 2294 1647 1851 2387 1933 1647 1710 1833 2750 976 2268 2518 3475 285 4593 2442 2600 478 978 2032 2893 7 9 9 U 873 465 3457 2643 1824 2411 1667 2129 1933 1529 1631 667 2750 1707 2146 2518 3393 357 4519 2455 2717 500 886 2164 2873 11 6 8 T 905 479 3285 3071 1941 1471 1889 1968 1633 1705 1816 1167 3000 1122 2560 1600 3228 214 4778 2532 2635 545 978 1951 2680 9 5 11 D 444 422 3081 3000 1765 1941 1963 1742 2133 1471 2000 833 2500 1024 2634 1339 3426 143 4630 2494 2541 386 1023 1574 2693 7 9 9 U 381 338 2771 2571 1765 1471 1667 1935 1633 1529 1631 500 2500 951 2536 1750 3770 179 4778 2403 2529 500 1023 1492 2733 15 4 6 T 492 451 2857 2786 1588 1588 1963 2097 1833 1823 3368 833 3000 1098 2610 150 1017 179 4630 2571 2588 568 1045 1377 2667 6 5 14 Fig 4 Estimated true risks 25 domains comparing discrete AdaBoost 7 D AdaBoostR U real AdaBoost 81729 T For domain emphasis best algorithms worst algorithms The 3 rows count number times algorithm counts respectively best second worst R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 37 real AdaBoost performs best converges fastest Increasing T makes real AdaBoost outstrips algorithms However domain gets complicated AdaBoostRbecomes algorithm beats T increases Consider following domain ordering easiest hardest Monks12 noise irrelevant attributes XD6 10 class noise irrelevant attribute LEDeven 10 attribute noise LEDeven17 LEDeven17 irrelevant attributes 322 Real AdaBoost beats algorithms XD6 experiment larger classiﬁers r 3 T 100 reveals AdaBoostRbecomes winner approaches Bayes risk 1115 error discrete real AdaBoost respectively achieve 1147 1246 error statistically worse case Winning occurs sooner LEDeven T 50 LEDeven17 T 10 One reason phenomenon fact reweighting scheme AdaBoostRis actually gentler especially noisy examples discrete real AdaBoost subject large weight update exponential update rule fact higher reweighting occur sole basis binary classiﬁcation result goodbad class classiﬁer minute conﬁdence label predicts This happen case classiﬁers margin negative positive examples receive right class reweighted higher counterbalancing higher reweighting eventual noisy examples Gentler updating thresholding soon proposed line research improve noise handling 5 In fact useful tackle overﬁtting 42 Noise handling overﬁtting In order shed light noise handling drilled results domains LEDeven XD6 3 plugging variable noise rates way margin errors degrade problems noisier harder LEDeven seven bits problem digits old pocket calculators Examples picked uniformly random possible classes grouped evenodd Each description variable gets ﬂipped η chances original domain η 10 XD6 n 10 problem describes noisy disjunctive normal form formula Name v1 v2 v10 Boolean variables Observations picked random labeled positive iff v1 v2 v3 v4 v5 v6 v7 v8 v9 true Thus v10 irrelevant strongest sense 13 Afterwards η chances class gets ﬂipped original domain η 10 LEDeven XD6 cover broad range difﬁculties study noise handling overﬁtting LEDeven variable attribute noise XD6 variable class noise irrelevant attribute unbalanced classes We computed margin distribution graphs training testing domains small large values parameters r T respectively 26 r 201000 T Each time S simulated m 300 examples On test margin distribution graphs computed exact curve Bayes rule order comparisons following Section 331 Among numerous curves obtained chosen summarize results obtained report important curves obtained largest rules r 6 noise rates η 10 20 30 40 Figs 5 6 Remark shape Bayes curves complex LEDeven noise affects attributes instead classes In domains training margins clearly display real AdaBoost 81729 fastest converge perfect classiﬁcation followed discrete AdaBoost 7 AdaBoostRus The phenomenon naturally prominent XD6 domain easier handle LEDeven prominent r increases allows faster ﬁtting data The empirical margin distribution graphs real AdaBoost exhibit twosteps plateaus rules complex r 6 domain XD6 plateau near 0 θ smaller 01 approximately plateau increases η larger values θ This plateau shape good accordance Theorem 1 proves sufﬁciently large number boosting rounds WLA empirical margin distribution graph bounded plateau located empirical Bayes risk The test margin distribution graphs display completely different pattern On domains majority curves r η T θ cid3 0 AdaBoostR beats algorithms The phenomenon visible r η T increase A glimpse curves r 6 η 40 domains Figs 5 6 phenomenon fact test curves real AdaBoost tend exhibit multi plateaus shape visible η increases This shape observed testing real AdaBoost cases observablethough visiblefor discrete AdaBoost observed AdaBoostR indicates conﬁdence classiﬁcation virtually inﬁnite examples receive right class receiving wrong class As discussed Section 331 think indicates tendency 38 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 Fig 5 Test margin distribution graphs domain LEDeven different attribute noise rates r 6 text details overﬁt trying model noisy data This tendency clearly pronounced AdaBoostR look margin distribution graphs θ cid3 0 fact AdaBoostRs curve systematically tends indicate AdaBoostR performs sensibly better discrete real AdaBoosts Section 331 It worthwhile remarking Theorem 1 provides rough primer appearance plateau R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 39 Fig 6 Test margin distribution graphs domain XD6 different class noise rates r 6 text details shapes testing m increases cid4uH converges cid4DH Theorem 1 combined results 28 brings statistical penalties Bayes plateau upperbounds test margin errors However results Theorem 1 apply algorithms discrete real AdaBoosts AdaBoostR The fact exponential rates convergence known empirical risk algorithms indicates 40 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 real AdaBoost converge faster practice However systematic worstcase results topdown induction decision trees 23 indicate exponential rate convergence best possible Tests AdaBoostR large values T typically tens hundreds thousands tend appear plateau shapes obtained iteration regime outside common datasets noisy hard It reasonable think gentler updates AdaBoostR keys beating AdaBoosts domains 5 Conclusion In paper proposed new generalization discrete AdaBoost handle weak hypotheses real values Our algorithm AdaBoostR departs usual generalizations rely explicitly exact minimization exponential loss loss upperbounds empirical risk While formally prove generalization boosting algorithm original sense provides interesting computational numerical fea tures respect real extensions discrete AdaBoost generalization wellknown facts discrete boosting Theoretical experimental results insights way algorithms compare respect clues helpful obtain boosting algorithms better handling hard noisy domains Acknowledgements We like thank reviewers insightful comments helped signiﬁcantly improve quality paper R Nock gratefully thanks Sony Computer Science Laboratories Inc Tokyo visiting grant work References 1 A Banerjee S Merugu I Dhillon J Ghosh Clustering Bregman divergences Proc SIAM International Conference Data Mining 2004 2 E Bauer R Kohavi An empirical comparison voting classiﬁcation algorithms Bagging boosting variants Machine Learning 36 1999 105139 3 CL Blake E Keogh C Merz UCI repository machine learning databases httpwwwicsuciedumlearnMLRepositoryhtml 1998 4 L Devroye L Györﬁ G Lugosi A Probabilistic Theory Pattern Recognition Springer 1996 5 C Domingo O Watanabe MadaBoost A modiﬁcation AdaBoost Proc 13th International Conference Computational Learn ing Theory 2000 6 W Fan SJ Stolfo J Zhang PK Chan AdaCost Misclassiﬁcation costsensitive boosting Proc 16th International Conference Machine Learning 1999 7 Y Freund RE Schapire A decisiontheoretic generalization online learning application boosting Journal Computer System Sciences 55 1997 119139 8 J Friedman T Hastie R Tibshirani Additive logistic regression A statistical view boosting Annals Statistics 28 2000 337374 9 C Huang H Ai Y Li S Lao Vector boosting rotation invariant multiview face detection Proc 11th IEEE International Conference Computer Vision 2005 10 C Huang B Wu H Ai S Lao Omnidirectional face detection based real AdaBoost Proc 10th IEEE International Conference Image Processing 2004 11 R Huang JHL Hansen Dialectaccent classiﬁcation boosted word modeling Proc 30th IEEE International Conference Acoustics Speech Signal Processing 2005 12 JC Janodet R Nock M Sebban HM Suchier Boosting grammatical inference conﬁdence oracles Proc 21st International Conference Machine Learning 2004 13 GH John R Kohavi K Pﬂeger Irrelevant features subset selection problem Proc 11th International Conference Machine Learning 1994 14 M Kearns Thoughts hypothesis boosting ML class project 1988 15 M Kearns L Valiant Cryptographic limitations learning boolean formulae ﬁnite automata Proc 21st ACM Symposium Theory Computing 1989 16 MJ Kearns UV Vazirani An Introduction Computational Learning Theory MIT Press 1994 17 J Kivinen M Warmuth Boosting entropy projection Proc 12th Int Conf Comp Learning Theory 1999 18 G Lebanon J Lafferty Boosting maximum likelihood exponential models Advances Neural Information Processing Systems 14 2001 19 J Leskovec J ShaweTaylor Linear programming boosting uneven datasets Proc 20th International Conference Machine Learning 2003 R Nock F Nielsen Artiﬁcial Intelligence 171 2007 2541 41 20 K Matsushita Decision rule based distance classiﬁcation problem Annals Institute Statistical Mathematics 8 1956 6777 21 R Nishii S Eguchi Supervised image classiﬁcation contextual AdaBoost based posteriors neighborhoods IEEE Transactions Geoscience Remote Sensing 43 2005 25472554 22 R Nock Inducing interpretable Voting classiﬁers trading accuracy simplicity Theoretical results approximation algorithms experiments Journal Artiﬁcial Intelligence Research 17 2002 137170 23 R Nock F Nielsen On domainpartitioning induction criteria Worstcase bounds worstcase based Theoretical Computer Science 321 2004 371382 24 R Nock F Nielsen On weighting clustering IEEE Transactions Pattern Analysis Machine Intelligence 28 2006 12231235 25 B Popescu JH Friedman Predictive learning rule ensembles Tech Report Stanford University 2005 26 G Ridgeway The state boosting Computing Science Statistics 31 1999 172181 27 RE Schapire The strength weak learnability Machine Learning 1990 197227 28 RE Schapire Y Freund P Bartlett WS Lee Boosting margin A new explanation effectiveness voting methods Annals Statistics 26 1998 16511686 29 RE Schapire Y Singer Improved boosting algorithms conﬁdencerated predictions Machine Learning 37 1999 297336 30 LG Valiant A theory learnable Communications ACM 27 1984 11341142 31 V Vapnik Statistical Learning Theory John Wiley 1998 32 P Viola MJ Jones Robust realtime face detection International Journal Computer Vision 57 2004 137154 33 P Viola MJ Jones D Snow Detecting pedestrians patterns motion appearance International Journal Computer Vision 63 2005 153161 34 I Witten E Frank Data Mining Practical Machine Learning Tools Techniques Java Implementation Morgan Kaufmann 1999 35 Z Yang M Li H Ai An experimental study automatic face gender classiﬁcation Proc 18th International Conference Pattern Recognition 2006