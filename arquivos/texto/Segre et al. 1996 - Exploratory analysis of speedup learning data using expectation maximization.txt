ELSEVIER Artificial Intelligence 85 1996 301319 Artificial Intelligence Exploratory analysis speedup expectation maximization learning data Albert0 Maria Segrea Geoffrey J Gordonb l Charles P Elkan Department Management Sciences University Iowa Iowa City IA 52242 USA bSchool Computer Science CarnegieMellon University Pittsburgh PA 15213 USA Department Computer Science Engineering University California San Diego La Jolla CA 15213 USA Received January 1995 revised September 1995 Abstract Experimental evaluations learning performance data analysis speedup determine learning methods data testing hypothesis estimate complementary obtain deeper insight parametric problem comparative approach parametric perimental performance methods propose accelerate leaving unchanged We apply expectation maximization fit kind multicomponent methodological censored learning linear model solution problems solver To model problem twocomponent values data parameters past non beneficial We learning methods In solvers approach ex statistical model learning use speedup captures learned knowledge relatively technique solution statistical EM model EM allows fit model difficulty experiments common presence involving speedup 1 Introduction Speedup learning methods subgoal caching 17 explanationbased intended 14 generally learning bounded problemsolving mean operating quickly fixed level competence Unfortunately determining performance resource usually defined extent performance Performance improvement allis difficult detecting improvementor improvement improve Corresponding Email ggordoncscmuedu Email elkancsucsdedu author Email segrecsuiowaedu 00043702961500 0 1996 Elsevier Science BV All rights reserved SSDI 0004370295001158 Since conclusive formal arguments performance improvement difficult collected learning method speedup realistic means detecting Data testing hypothesis performance methodological experimental ties nonparametric performance studies speedup presence methods learning choices compromise censored data construct experimental studies provide quantifying performance studies null hypothesis typically analyzed 15 In reliability conclusions improvement form difference 7 learning One methodological learning addressed subsequently hypothesis test drawn common difficul 6 improves Hypothesis testing provides It simply provides question problem population uninteresting learning certainty sampled differences little insight answer degree There misleading statistically practical learning times improves statistical performance significant standpoint qualitative behavior careful hiding intuitive statistical examination result important characteristics obtained data data substitute test checking 6 paper presents rigorous approach modeling performance This intended traditional This approach positing Exploratory method model expose The contributions kind hiding paper threefold hypothesis testing complementary data analysis exploratory model statistical deeper understanding analysis performing type advocated kind intuitive new mathematical The second contribution type exploratory analysis previous work simple support performance quantify 171 Here propose better captures knowledge affect problems benefits certain sophisticated speedup effects linear First augment approach exploratory parametric technique examination estimate operation quantitative nature parameter values achieved reproducible learning described We mentioned model speedup fitting parameter regression model problemsolving types speedup twocomponent learning particular learning linear model 1316 learned The contribution statistical technique estimate twocomponent model presence censored data This parameters technique A censored measurement value wait For example censored measurement hours nontrivial credible search problems empirical know Resource Thus censored test problemsolving data problem observe bound measured value hours know problem actual solver time solve problem solution bounds fundamental avoided addressed solving AM Segre et al I Artificial Intelligence 85 1996 301319 303 based expectation maximization likelihood estimation special case We use EM perform multiplecomponent regression maximum presence missing data censored data linear presence censored data4 general method EM learning reconstructed sample dataset In section introduce presence censored data discuss information literature This dataset serves example paper We review nonparametric method machine hypothesis remainder testing dataset revealed test In Section 3 introduce EM available applied speedup learning systems presence censored data In Section 4 EM fit simple dataset Section 2 example We introduce performance learning EM fit twocomponent model speedup new model presence censored data illustrate process learning performance censored data nonlearning sample dataset performance investigate linear model 2 An illustrative example The example speedup ll Reports experiments literature learning experiment times explore explanationbased combined standard backwardchaining performance paper revisits classic logic theorist LT domain appeared 101214 The primary question learning component provides problemsolving improvement The set LT problems domain taken Chapter 2 Principia Mathematics 19 original 92 problems Chapter 2 The 87 problems lo printed reformulated version domain theory problem set paper 141 set correspond use definiteclause theorem provers The backwardchaining problem solver definiteclause theorem prover Common Lisp This theorem prover implemented learning 14 The previous work subgoal caching 17 explanationbased theorem prover unitincrement depth iterative deepening The data described collected 32MB 90MHz Pentium running Gnu Common Lisp Linux operating perform resourcebounded configured A resource attempted annotation limit 5 x lo4 node explorations imposed problem In trial CPU times node exploration counts recorded indicating problem solved problem censored After submitting efficient EM algorithm problem 8 version paper discovered previously described iOJ AM Segre et trl 1 Artificial Intelligence 85 lYY6 301319 In trial bound In 34 solved theorem second trial prover trial 4 problems solved 34 87 problems randomly selected generate macrooperators prover tested remaining solved 36 83 algorithm 83 problems tested 14 The The learning theorem resource bound resource EBLDI remaining problems 21 Scatter plot inspection The simplest method analyzing data collected trials 1 2 scatter plot elapsed CPU time learning Fig 1 plot logarithmic axes clarity Each datapoint solution plotted solution learning An J x line correspond vertical problems Fig 1 plotted represents informal learning learning analysis versus transform horizontal single problem CPU axis CPU axis Datapoints solved nonlearning applied time time falling slower faster advantageous faster problem difficult unchanged factor The In addition learning solves 6 problems solved 30 problems learning 12 solved slowly time indicate learning nonlearning systems censored 17 solved solve problems Unfortunately 47 doubly kind informal analysis A comparison summary 1OQO Learning CPU time vs nonlearning kzming seconds Solved o Censored Doubly censored o Zy 100 IO CPU time 9 0001 E 0001 001 01 1 10 NonLearning seconds 100 1000 I Plot learning shown Fig datapoints systems The 47 square learning CPU time 30 problems correspond nonlearning solved CPU learning datapoints cross correspond problems solved datapoints nonlearning correspond exhausted doubly censored 5 x 10J node exploration problems time The diamond nonlearning learning problems resource bound AM Segre et al I Artificial Intelligence 85 1996 301319 305 statistics systems misleading 151 total CPU time problems solved potentially similarly confusing subjective 22 Hypothesis testing account solution tests accept cause nonlearning 6 relies null hypothesis Etzioni Etzioni The analysis advocated solution censored data These nonparametric methods onetailed paired sign test 2 onetailed paired Wilcoxon signedranks test 20 suitably extended tests non commonly parametric Student ttest They parametric analogues test statistically significant differences times Unfor times learning tunately maximally conservative way tests handle powerful dataset contains censored censored data 6 point sufficiently censored observations regardless datapoints strength evidence uncensored datapoints For example apply tests data Fig 1 half observations censored reach useful conclusions6 If censored data test null hypothesis test strongly fails reject reject small extension signedranks values p 1 p G 10e6 On hand test hypothesis 1 p 4 lo These results mean according ranks test reasonable indistinguishable datapoints allow reach extreme prospects conclusion differences easily despite differences described The situation extended signed learning If p values dim Thus despite revealed method test detect difference Section 4 extended signedranks faster fail strongly learning faster nonlearning extended sign test level performance similar nonlearning p values extreme significance reject inappropriate requires underlying solver normal This assumption distribution unwarranted measured solution censoring cluster assumed resource paper limit general form censorship censored variant In case doubly censored observed exactly arises naturally constant points values censored datapoint displays restrictive consumption form 6 The resource resource limit imposed directly parameter fall exactly y x line singly censored true coordinate Because observation support general treated extensions greater tests setting extend coordinate larger censoring lies null hypothesis We considered results test value tests 6 point m ways false support ttest problem The times cause datapoints The restrictive points uncensored natural way censored whichever extending conclusion provides 3Uh AM Segrr er ul j Artificial Intelligence 85 1996 30131Y 3 Modeling problemsolving performance combines Our approach learning performance evaluating speedup nature scatter plot inspection rigorous mathematical informative posit model foundation Briefly approach performance called expectation muxi mization EM estimate values parameters model Using EM technique allows estimate parameter values performance data censored We examine fitted model order identify trends directed directly raw data size censoring detected hypothesis use statistical technique testing studied names life testing reliability EM address problems statistics operations testing The research arises medical studies object estimate average lifetime group patients patients alive The second arises quality control experiments mean time failure sample parts parts failed Recently EM shown promise learning tasks 3 As expected discovering patterns tests 6 deal censored given heritage EM like nonparametric technique That data But unlike weaker methods EM parametric begins prespecified model prespecified adjusts parameters DNA protein sequences finite number parameters object fit data unsupervised estimate 31 Using EM model performance Assume given problemsolving wish evaluate performance respect set standard We gather data presenting n problems calibrated measuring Since generally afford input years centuries later elapsed CPU time problem let run completion finish cut finishes yielding censored data resource consumption difficulty observations x Y y S X Our difficulty ith Lx I problem x resource consumed ith problem yi 0 actually solves ith input 1 cut solving y 1 problem Note x lower bound resource consumption _v 8 measures comprise J nvectors 7 EM model assumptions hypothesis focus similar deviations test qualitative parametric required leaving hypothesis test linearity testing hypothesis normality However seriously computations required fit violations parametric affect significance fitted model unchanged level Thus qualitative appearance aspects analysis approximate EZ AM Segre et al I Artificial Intelligence 85 1996 301319 307 We assume observed vectors X Y obtained true data vector Z directly observe Each zi E Z resource consumed ith input ignored resource limit let run eventually halted We assume elements A Z known density parameters 0 parametric assumption We attempting parameters known distribution trying proceed information A Z whatsoever In principle A Z assume arbitrarily complicated distribution paper use linear models described later Our goal obtain good estimate 0 0 characterizes relationship Si zi identically distributed observations independent estimate Suppose instead censored observations X censoring flags Y true data Z Then relatively easy estimate 0 4 In fact maximum true data Z sujjicient statistics data statistics sufficient depends dis observe technique need describing tribution Si zi Let vector sufficient statistics denoted TZ likelihood estimation If knew TZ estimate 0 On hand knew 0 values statistics TZ expected sufficient 0 This apparent dilemma basis EM algorithm likelihood compute observed data statistics TAZ based ETZ 1 C X Y initial estimate We proceed follows We begin initial estimate 0 First use guess estimate 0 set r true sufficient improving estimate called expectation muximization algorithm computing estimate sufficient parameters Next use f0 update maximum statistics We repeat process alternately 0 TAZ This process alternates maximum likelihood estimate 5 It guaranteed certain general conditions maximum E step described likelihood estimate 0 based observables A X Y M step The EM algorithm estimate 0 assuming expectation converge f0 32 Measuring problem dijjiculty Our experiment explores relationship variables A true resource consumption Z Above informally explained Sj difficulty ith problem A need know measure In section LT experiment machinery EM experiment variable Beneath dependent variable A good predictor Z In words problem low Si consume fewer resources average A So basic requirement regression analysis The consumption Z true resource independent 8 This general works logarithm form EM algorithm probability density sufficient function purposes true data linear This form TZ 30x AM Segre et al I Artificial Intellipwce X5 1996 301319 problem high 6 It requirement ith problem A second called S difficulty easily accurately measurable In addition performance different problem solvers later requirement essential informally A want compare measurements planning likely paper adopt processor requirements These drawn problems problem solution alternative attractive provide benchmark usage control constitutes A To avoid censoring high resource multiple experiments For single prover disabled Cypress Of 87 problems control control learning slight bias control resource censored small effect regression version Data resource systems limit datapoints 18 collected resource experimental analyses far described resource A independent suggest problem solvers testing domain possibilities 3 For example steps consumption shortest Another number resource predict defining A use separate performance Here measurable required time necessary values CPU limit The high cost incurred control problem solver aspect resource problem solve run control amortized use problem approach set The control WAMbased parallel caching dedicated subgoal limit 5 X 10h node explorations firstorder intelligent logic theorem backtracking 128MB Sun Spare 670MP problem LT problem 41 problems Neither solved problems While limit We exclude set 46 problems solved solved introduce learning omission non results believe limit bias negligible orders systems regression line magnitude problems ignoring coefficients larger correspond points 4 Using EM analyze LT data LT domain augmented We ready analyze problemsolving EM Specifically wish compare problemsolving explanationbased performance compute component algorithm parameters nonlearning model performance fit complex model component We linear model learning censored behavior performance backwardchaining data posit linear model EM data Next posit provide learning censored presence experiment For different exploring quantity response low 8 predict hetween relationship given namr quantity ith patient 8 appropriate treatment drug In case high 6 predict administered effectiveness strong patient 8 For example weaker response AM Segre et al I Artificial Intelligence 85 1996 301319 309 data Finally compare qualitative conclusions based analyses performance systems order draw 41 A linear model Consider relationship nonlearning A Z data trial 1 Let assume zi at5 b normally linear distributed mean 0 This standard linear regression model censored data need EM In fact regular regression benefit EM allows censored points sample biasing line If threw points wasting potentially regression best losing statistical power worst valuable reaching information incorrect conclusions threw censored datapoints datapoint use EM fit linear model effect censored data intuitively 6 z seen attracting Before explaining mathematically In censored data useful understand regression ordinary line A point line pulls line upwards regression x point line pulls downwards A censored datapoint lower bound true z valueappearing pulls line upwards uncensored datapoint apparent position true position censored datapoint high apparent position In contrast censored datapoint line pull line downwards true datapoint actually lie line In fact censored datapoint line pushes line upwards slightly higher line makes likely datapoint line behaves similarly censored 6 xwhere We worry effects doubly censored datapoints performance directly try compare scatter plot comparison Fig 1 Instead example standard S assume Si available rely independent Thus problems solved appear doubly censored points direct comparison singly censored points learning plot nonlearning plot learning nonlearning indirect comparison transformed Without measurement variances subject achieve approximately I In order experiments line smaller A measurement I Of course deal solver experiment obtain A convenient specified control control control error test problem distribution transform variances end Also log A Z end regression assumes implicitly lack regression model error standard Just fiction seriously linear influence results corresponding disadvantage If use control fails solvers solve problem perform problem use problem information Fortunately learning nonlearning LT solves problem solved resource limit 310 z4M Sep et ui Arfificiul Intelligence 85 1996 Nl31 Y 411 Fitting u linear model censored data EM When EM fit linear model described M step involves likelihood estimates values model parameters finding maximum b u Formally model probability density function 1 We maximum result In f setting likelihood 0 This process gives estimates b T differentiating wellknown estimates 4 2 These estimates expressed terms sufficient statistics multiplying expression 6 obtain elements TZ For E step expected value TZ terms ci 6 Since Ez The trick 6 sufficient presence Ez data censored If y 0 z x Ez x Ez x However situation density zz difficult Assuming moment y 1 2 6 0 6 1 Let probability z z Gz I ct dt Conditioning fact I 3 x gives density z given z x 7 AM Segre et al I Artificial Intelligence 85 1996 301319 fCzi I xi 4z xi The required statistics m 4t t iI dt Ezzix XI _ Wi Xi J tt dt 4t C qz Izixi XI x 1 I Iy t 44 dt btjWf II3 j txi4Cxi ill I Shifting scaling handle arbitrary b 6 gives EZ I Zi xi Eli _ kr xj Pi WY pi L d 412 Application LT nonlearning data 311 9 W 11 12 13 14 15 16 17 Fig 2 plots nonlearning CPU time versus 8 46 problems 8 axes The line shown linear regression fit EM 34 solved 12 transform applied available logarithmic Fig 2 censored censored problems 311 AM Segre 61 al I Artijiciul Intelligence 85 1996 301319 Nonlearning loooo I CPU time vs control nodes searched J I I I NowLaming seconds Solved o 1 _ Censored I 10 IO0 lwo loo00 Control nodes seurched lOOWxI 10oW00 Fig 2 Plot nonlearning datapoints The correspond line 12 cross CPU 34 problems time versus control solved control nodes searched The diamond nonlearning datapoints currespond result EM perform tu censored censored problems linear regression solved control While regression obtained censored incorrect wards possible EM problems A regular obtain 34 problems exploits smaller slope additional substantially similar fit simple 8 Z available linear fit 12 available line 46 problems pull information datapoints censored regression 32 A multicomponent model In order learning learning introduce performance combination model complete analysis LT experiment data We perform analysis systems suggests different model appropriate model The premise mixture submodels behavior speedup analyze experience In section systems l learning learning behavioral modes We EM behavioral modes separately presence censored data 421 Justifying u twocomponent model speedup leurning generally operate perturbing The exact nature learned case subgoal previous caching learning problemsolving Typically learned case explanationbased learning algorithms problemsolving Speedup explored depends cache contents heuristics unaffected helped learned information learned previously Speedup problems learning performance space search perturbation experience certain rules search problems AM Segre et al I Artificial Intelligence 85 1996 301319 313 good candidate subset problems remaining problems More precisely changed learned knowledge largely unchanged assume twocomponent model performance performance augmented speedup learning displays distinct linear relationships difficulty mechanism A resource consumption 2 probability 1 A given problem unaffected learning point Si zi lies approximately line probability A learned knowledge contributes solution problem lower line The number A called point ai zi lies different trial 2 LT mixing parameter Given performance data like collected experiment subpopulations simultaneously characterize model relationships expectation maximization estimate A assign datapoint presence censored data The natural step twocomponent linear model model components The techniques described extended analyze multiplecomponent models need components LT data censored 422 Fitting twocomponent linear model censored data EM The twocomponent linear model described linear model Section 41 given natural extension simple subpopulations Here estimate parameters model presence censored data EM Others described algorithm fit model 8 The algorithm proposed based nested EM iterations efficient needs level iteration hypothesis In simple censored regression case unobserved vector true resource resource consumptions X censoring consumptions Z gives rise observed flags Y In complicated addition true resource consumptions Z introduce observed variables problem data V telling population IZ X 2 matrix unobserved belongs belongs population observed variables X Y EM The estimates V Z allow infer values mixing parameter slope intercept population j 0 otherwise13 As estimate V Z The element uij V defined twocomponent model 1 observation learned directly helped learning This utility problem knowledge adversely affect performance problem associated analysis 9 The method solver use advanced particular experiment explained techniques affects strongly speedup learning utility problem I2 The addition problems EBL algorithms clarify Section 43 I3 In situations outside knowledge prior distribution macrooperator reliable solution However backtracking failure caching intelligent learned ur For example employed knowledge necessary possible u We encode determine outside information knowledge inspection learned available generally leave trace sufficient solution generated condition speedup 314 AM Segre rr ul Arrrficial Intelligence X5 1996 301319 To derive unobserved M step EM algorithm need data For convenience logarithm density function density lnfZ V I ho k13 n ul A u Mfl u Ma 1 18 P b CL n6 h C constant To reduce priori lines There likelihood The resulting variances number ai remaining parameters ai parameters estimate assume populations know regression maximum slope A As result In f setting intercept zero exactly expect equal lines estimates mixing parameter differentiating estimate mixing parameter belong datapoints fraction line The estimates slope intercept line ci I c UA c uo c u c d c u cc ud 2 c u c UG c ud r c u 19 20 21 These estimates 3 include replaced C u points similar estimates term additional singleline case Eqs factor u belong line In particular 2 sums n C 1 estimate Trying variances models line latches variances variances If true variances line zero adds points unknown local maxima fits usually EM search exactly case recommend space These variance trying fit choice best fit determine sensitive AM Segre et al I Artificial Intelligence 85 1996 301319 315 The estimates slope intercept second line analogous estimates line We compute estimates sufficient statistics15 22 For These formulas constitute M step EM algorithm E step expected value sufficient statistics given current estimates b b A Since uiO uil values 0 1 compute expected values singleline case For example Eujozi EuioEzj 1 uio l calculate Ezi uio 1 Eq 16 pi pie The remaining calculation normal density function Euio derive Bayes rule WiO w yw 3 10 11 EUl w y IO 11 uncensored case 23 24 25 27 censored case In practice computing directly weighted regressions ho b ci perform 6 For regression use weights Euio treat censored points came line second use weights Euil treat censored points came second line slopes intercepts I5 We defined need statistic parameter statistics statistics order estimate parameters Many common distributions mixture distribution behaved We need log likelihood linear TZ V 316 AM Segre et ul I Artificul Intelligence 85 1996 I319 Learning CPU time vs control nodes searched Learning seconds 1 _ Censored I IO 100 1000 1oOtXl IOOOW 1ooWoO Control nodes searched Fig 3 Plot learning CPU time versus control nodes searched The 34 diamond 34 problems solved hy control learning datapoints correspond censored problems solved control The cross datapoints correspond solid lines result EM fit twocomponent linear model dotted line censored linear regression Fig 7 included comparison 423 Application Fig 3 shows tw LT lrurrhg 34 LT problems dutn solved resource The remaining limit learning mechanism As control node solid linear model lines shown fixed variances The dotted line censored learning censored problems solved 46 34 8 4 problems 46 nonlearning available exploration fitting Fig 3 Fig 2 result 10 05 regression linear count slope line represents relationship discovered consumption population slope corresponds faster dotted performance slower line line Fig 3 conclude problems corresponding nonlearning intuition original portion problems speedup upper entire applied largely unchanged underlying learning Since imported solid set problems learning line similar datapoints slopes Fig 2 This leaving fashion line conclude selectively In similar model twocomponent lower solid line line smaller problems slope corresponding dotted helped learning resource comparison The 5 x 10J node exploration control input problems considered twocomponent components included Recall difficulty smaller upper comparable subpopulation performance consistent Section 421 performance noticeably lower solid subpopulation larger solid AM Segre et al I Artificial Intelligence 85 1996 301319 317 43 The benefits EBLDI What useful performance datapoints solved noticeably overall analysis LT domain First macrooperators lying EBLDI produced EBLDI algorithm line correspond lower shown faster learning The estimated mixing problems problems helped parameter A indicates EBLDI lines EM source difference EBLDI 27 problems solutions finding spurious distinction examine contain learned macrooperator These problems contain 13 points EM assigns lower line showing use learned rule necessary sufficient condition 0324 3 8 problems To confirm helped learning16 approximately Second line imported increase slope upper solid line respect little evidence utility problem This problem manifest dotted Fig 2 This indicate subpopulation problems helped learning lower solid line subpopulation hindered learning Here lines issue These comparable results 14 observations EBL compared algorithm drawn machine slopes conclude utility problem consistent previously reported algorithm performance EBLDI literature learning 5 Discussion This paper shown model fitting valuable analysis speedup testing provide deeper provide information fittingwhen experimental like learning proposed results More specifically model multicomponent model speedup learning data Model fitting goes hypothesis understanding applied solves papercan problems faster mixing parameter A magnitude typical speedup ratio slope lower line learning analysis slope line problems slope information nonlearning ratio slope upper line learning analysis finergrained typical case learning testing We illustrated fit multicomponent model speedup analysis effect learning unhelped conclusion provided hypothesis learning experiments particular EBL algorithm real data obtained nonlearning nonlearning analysis This learning line 13 points estimated probability belonging These half Note 1342 h 0324 There contradiction estimate A formed raw thresholded probabilities lower line greater 318 AM Segrrr et al I Artificial Intelligence 85 IYY6 30131Y data literature performance prior evidence previous work 14 claimed In order fit model problem set taken machinelearning applied statistical technique expectation maximization EM handle censored data provide probabilistic partitioning datapoints submodels Whenever suspects experimental data arise distinct subpopulations valuable eliminate human bias identifying EM fit multicomponent model In case subpopulations problemsolving multiple subpopula tions exist relatively shallow based visual inspection data produced relatively deep based selective use macrooperators learning exploratory data analysis tool justified useful Ultimately practice Recall macro operators produced EBLDI effective likely cause produced given EBL utility problem These algorithm operating conclusions based relatively coarsegrained comparisons summary trials like ones described statistics collected credible censoring issue EBLDI learning solved problem solved control EBL allotted resource bound Had case able support claim access analysis technique like advocated More point remove Fig 3 lines EM simple visual examination data reveal little structure Only use linear model bimodal structure data EM twocomponent revealed macrooperators LT domain experimental nonlearning The message paper parametric analysis valuable insight In situations experimental data hypothesis test prefer nonparametric methods Wilcoxon signedranks able hypothesis testing gain useful understanding empirical data positing parametric model EM estimate values parameters hypothesis testing compared traditional modelfitting methods EM lets use complicated models model Modeling gives testing inconclusive information Acknowledgements comments draft version paper Support The authors wish thank Paul Cohen second anonymous reviewer constructive research provided Office Naval Research grants NOO1488 K0123 N0001490J1542 N0014941178 AMS Advanced Re search Project Agency Rome Laboratory Contract Number F3060293C 0018 Odyssey Research Associates AMS National Science Foundation graduate fellowship GJG National Science Founda Incorporated AM Segre et al I Artificial Intelligence 85 1996 301319 319 grant BES9402439 tion CPE GJG Hellman faculty fellowship References l M Aitkin DB Rubin Estimation hypothesis testing finite mixture models J Ro_v Stat Sot 47 1985 6775 2 J Arbuthnott An argument divine providence taken constant regularity observed births sexes Philos Trans 27 1710 186190 3 TL Bailey CP Elkan Unsupervised expectation maximization Mach Learn appear learning multiple motifs biopolymers 4 G Casella R Berger Statistical Inference BrooksCole Publishing Company Pacific Grove CA 1990 S AP Demptster NM Laird DB Rubin Maximum likelihood incomplete data EM algorithm J Roy Stat Sot B 39 1977 l37 6 0 Etzioni R Etzioni Technical note statistical methods analyzing speedup learning experiments Mach Learn 14 1994 333347 7 JN Hooker Needed empirical science algorithms Oper Res 42 1994 201212 8 K Jedidi V Ramaswamy W Desarbo A maximum likelihood method latent class regression involving censored dependent variable Psychometrika 58 1993 365394 9 S Minton Quantitative 42 1990 363392 results concerning utility explanationbased learning Artif Intell lo R Mooney The effect rule use utility explanationbased learning Proceedings ZJCAI89 Detroit MI 1989 725730 ll A Newell JC Shaw H Simon Empirical explorations logic theory machine J Feldman eds Computers Thought E Feigenbaum case study heuristics McGraw Hill New York 1963 12 P ORorke LT revisited explanationbased learning logic Principia Mathematics Mach Learn 4 1989 117160 13 AM Segre On combining multiple speedup Proceedings Ninth Internafional Conference Machine Learning Aberdeen 14 AM Segre C Elkan A highperformance explanationbased learning algorithm Artif techniques 1992 400405 Zntell 69 1994 l50 15 AM Segre C Elkan A Russell Technical note critical look experimental evaluations EBL Mach Learn 6 1991 183196 16 AM Segre CP Elkan D Scharstein GJ Gordon A Russell Adaptive eds Foundations Knowledge Acquisition Vol inference A 7 Kluwer Meyrowitz S Chapman Academic Publishers Boston MA 1993 4381 17 AM Segre D Scharstein Boundedoverhead caching definiteclause theorem proving J Autom Reasoning 11 1993 83113 18 AM Segre DB Sturgill Using hundreds workstations Proceedings AAAI94 Seattle WA 1994 187192 solve firstorder logic problems 191 AN Whitehead B Russell Principia Mathemafica Cambridge University Press Cam bridge 1913 20 F Wilcoxon Individual comparisons ranking methods Biometrics 1 1945 8083