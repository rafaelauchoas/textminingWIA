Artiﬁcial Intelligence 171 2007 429433 wwwelseviercomlocateartint The possible impossible multiagent learning H Peyton Young abc Johns Hopkins University USA b University Oxford UK c The Brookings Institution USA Received 16 May 2006 received revised form 20 October 2006 accepted 20 October 2006 Available online 7 February 2007 Abstract This paper surveys recent work learning games delineates boundary forms learning lead Nash equilibrium forms lead weaker notions equilibrium 2007 Elsevier BV All rights reserved Keywords Equilibrium Learning Dynamics Interactive learning inherently complex singleagent learning act learning changes thing learned If agent A trying learn agent B As behavior naturally depend learned far hopes learn But As behavior observed B Bs behavior change result As attempts learn The holds Bs attempts learn A This feedback loop central inescapable feature multiagent learning situations It suggests methods work singleagent learning problems fail multiagent settings It suggests learning fail general exist situations rules allow players learn anothers behavior completely satisfactory sense This turns case section I formulate uncertainty principle strategic interactions states ex ante uncertainty players payoffs potential behaviors way rational players learn predict anothers behavior inﬁnite number repetitions game 5 earlier results spirit 1 1314 Admittedly related impossibility theorems rest demanding assumptions agents rationality means learn opponents behavior Under restrictive conditions positive results attained shall Section 3 Thus purpose note claim multiagent learning impossibly difﬁcult try identify boundaryinsofar know itbetween possible impossible multiagent learning situations These issues discussed greater depth 18 1 Modelbased learning The accompanying perspectives paper Shoham Powers Grenager 17 referred SPG forms jumpingoff point They draw attention fact multiagent learning inherently complex Email address pyoungjhuedu 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200610015 430 HP Young Artiﬁcial Intelligence 171 2007 429433 singleagent learning They useful distinction modelbased modelfree forms learning I shall follow Using essentially language modelbased learning scheme following elements 1 Start model opponents strategy 2 Compute play best best response 3 Observe opponents play update model 4 Goto step 2 SPG leave concept model open I shall suggest general deﬁnition Namely modelbased learning method function maps history play prediction ones opponents period probability distribution opponents actions conditional history far This deﬁnition encompasses forms pattern recognition The key feature modelbased learning rule patterns able identify data uses patterns forecast opponents moves Many gametheoretic learning methods fall category Fictitious play simple example agent predicts opponent use distribution period cumulatively More gener ally Bayesian updating modelbased learning procedure agent updates beliefs repeatedgame strategy opponents conditional observed history leads prediction behavior period What exactly mean learning context A natural deﬁnition players learn eventually succeed predicting opponents behavior high degree accuracy 5 This idea given greater precision follows Suppose engaged twoplayer game Given history ht time t let pt prediction opponents nextperiod behavior conditional ht Let qt opponents actual intended behavior period conditional ht Notice pt qt probability distributions opponents action space assume ﬁnite Thus pt qt lie mdimensional simplex nonnegative integer m The predictive error period t cid2pt qt cid2 We learn predict cid2pt qt cid2 0 surely cid2ps qscid22 0 t A demanding deﬁnition mean square error goes zero 1t surely t We shall learning predict strong sense learning predict weak sense scid2t cid2 There wellknown condition statistics guarantees players learn predict strong sense Namely sufﬁces players forecast behavior conditional behavior exclude events positive probability actual joint behavior This absolute continuity condition 215 So far said determines agents behavior means learn In game theory standard assumption behavior rational point time given happened date players behavioral strategies optimal given forecasts going happen future dates If combine rationality absolute continuity condition guarantees good prediction convergence Nash equilibrium play path 15 Suppose player ignorant opponents payoff function If opponent rational strategy dependperhaps intricatelyon payoffs Hence ﬁrst player difﬁculty forecasting second players strategy gather information play path deduce optimizing The holds second player trying forecast behavior ﬁrst This turns impossible principle ex ante uncertainty payoffs Theorem 1 See 5 Consider nperson game ﬁnite joint action space A nA possible payoffs deﬁning G drawn iid continuous density f bounded away zero open interval G determined play begins Assume players forwardlooking rational discount factors unity know realized payoffs use forecasting rules depend opponents realized payoffs There positive probability players learn predict weak sense ii players periodbyperiod behaviors converge Nash equilibrium repeated game Furthermore support f sufﬁciently small interval conclusions ii hold probability HP Young Artiﬁcial Intelligence 171 2007 429433 431 A consequence result exist general modelbased procedures multiagent learning players perfectly rational sufﬁciently incomplete knowledge opponents payoff functions A crucial condition Theorem 1 hold unknown payoffs distributed interval If instead known lie ﬁnite set countable set result fail In case tailor forecasting rules account restricted set payoffs opponent satisfy absolute continuity The second crucial condition Theorem 1 rationality agents optimize exactly If instead agents optimize smoothed ﬁctitious play 8 result necessarily hold In view ﬁrst conditions lack knowledge important second perfect rationality For thing second condition merely ideal statement behavior little empirical support notion subjects optimize exactly By contrast ﬁrst condition realistic player hardly expected know von Neumann Morgenstern payoffs opponent precision surely hoped knows lie range I sketch modelbased multiagent learning method gets preceding impossibility result relaxing rationality bit maintaining assumption complete lack knowledge The method struc tured lines statistical hypothesis testing 6 Assume moment players 1 2 ﬁnite action spaces A1 A2 Let Δi simplex probability distributions Ai At time t agent 1s model agent 2 going play ﬁxed distribution p2t Δ2 future periods Given model agent 1 chooses smoothed best response q1t Δ1 Similarly agent 2s model time t p1t Δ1 smoothed best response q2t Δ2 Hypothesis testing takes following form player Let s large positive integer sample size let τ small positive real number tolerance level Hypothesis testing 1 Select model p opponents strategy uniformly random 2 Play smoothed best response q current model p 3 Start test phase probability 1s 4 Once test phase begins compute opponents empirical frequency distribution pcid6 s periods If cid2pcid6 pcid2 exceeds τ step 1 cid2pcid6 pcid2 exceed τ step 2 It shown given game G A1 A2 ε 0 s large τ small players behaviors constitute εequilibrium G 1 ε periods Further players gradually increase s decrease τ obtain following Theorem 2 See 6 Given nperson game G ﬁnite action space A hypothesis testing parameters annealed sufﬁciently slowly players periodbyperiod behaviors converge probability set Nash equilibria G 2 Modelfree learning The second class learning rules identiﬁed SPG rely prediction opponents behavior form heuristic adjustment previous experience Unfortunately setting clear meant learning Players obviously learning predict predicting SPG suggest analogy singleagent Markov decision problems MDPs agents learn adaptive rules lead high average payoffs In singleagent context rules said effective The difﬁculty high payoffs welldeﬁned singleagent MDPs usually deﬁned games In setting agents optimize given agents Of course optimizes conditional optimizing players form Nash equilibrium respect stage game repeated game This suggests possible deﬁnition learning modelfree environment agents average payoffs converge payoffs corresponding Nash equilibrium Alternatively learn behaviors come Nash equilibrium accident act predicting optimizing actually nonpredictive methods A 432 HP Young Artiﬁcial Intelligence 171 2007 429433 possibility Nash equilibrium appropriate solution concept setting We summarize possi bilities follows Learning criteria modelfree environments I Payoffs converge Nash equilibrium payoffs II Behaviors converge Nash equilibrium III Behaviors andor payoffs converge subset normative interpretation correspond Nash equilibrium Let consider turn The ﬁrst criterion easy satisfy Nash equilibrium payoffs mean payoffs repeatedgame Nash equilibrium The reason Folk Theorem virtually payoff com binations realized repeatedgame equilibrium provided player gets maximin payoff Many adaptive modelfree procedures achieve thing The second criterion difﬁcult achieve Indeed recently known exist modelfree learning rules cause behaviors converge Nash equilibrium special cases Here example modelfree rule called regret testing achieves criterion II ﬁnite twoperson game This simpliﬁed version rule treated 7 Let Δd set probability mixtures agents actions expressed d fewer decimal places Regret testing 1 Choose q Δd uniformly random 2 Play q s periods succession 3 For action compute regret ra having played action s periods 4 If maxa ra τ step 1 retain current q step 2 Given twoperson game G ε 0 players use regret testing sufﬁciently large s d sufﬁciently small τ behaviors constitute εequilibrium G 1 ε play periods 7 By annealing parameters sufﬁciently slowly obtain convergence probability set Nash equilibria ﬁnite twoperson game G With modiﬁcations almostsure convergence achievable 9 A crucial feature regret testing hypothesis testing random search occurs test fails What happens drop aspect learning process To general let G nperson game ﬁnite action space A Let s positive integer let state learning process s plays game Thus As states A learning rule fiz maps state z probability distribution actions period The rule fi uncoupled depend opponents payoffs 12 Theorem 3 See 12 Given ﬁnite action space A positive integer s exist uncoupled rules fiz state variable z s plays game G A periodbyperiod behaviors converge surely Nash equilibrium G εequilibrium G sufﬁciently small ε 0 Note regret testing uncoupled claimed earlier annealed version converges surely set Nash equilibria ﬁnite twoperson game G This contradict Theorem 3 annealed version value s grows state variable consist histories bounded length Furthermore nonannealed version s ﬁxed state variable consists s plays game includes realization random variable new choice q occurs someones regret exceeds tolerance level τ Hence satisfy conditions Theorem 3 Hypothesis testing satisfy state variable requirement reason This illustrates ﬁne line separates possible impossible multiagent learning theory We conclude example illustrates Nash equilibrium way evaluating agents learning criterion III Consider following simple adaptive procedure ﬁrst proposed Hart MasColell 1011 HP Young Artiﬁcial Intelligence 171 2007 429433 433 Unconditional regret matching 1 Choose action uniformly random 2 For action A compute regret ra having played previous periods 3 Among actions positive regrets choose probabilities proportional regrets step 2 actions step 1 When given player uses rule ﬁnite game G regrets ra nonpositive surely matter players 10 When players use rule empirical distribution joint behaviors converges surely convex set contains correlated equilibria G Nash equilibria Moreover average payoffs converge set expected payoffs generated distributions We shall coarse correlated equilibrium set 181 It simple equilibrium interpretation set joint probability distributions φ A players expected payoff φ high expected payoff deviate play arbitrary action players adhere outcome prescribed φ In correlated equilibrium contrast player wishes deviate prescribed action φ revealed restrictive condition The signiﬁcance solution concept describes average behavior wide variety adaptive rules including smoothed ﬁctitious play calibrated forecasting best responses 34 number variants regret matching 1011 Some rules actually converge set correlated equilibria It reason able conjecture coarse notion correlated equilibrium prove useful describing behavior experimental subjects knowledge point investigated systematically I conclude ﬁne line possible impossible multiagent learning situations It depends subtle differences assumptions information agents extent optimize desired form convergence target set In preceding I identiﬁed prominent landmarks dividing line Its precise course remains charted Acknowledgement I grateful Dean Foster Rakesh Vohra helpful comments earlier version References 1 K Binmore Modelling rational players I Economics Philosophy 3 1987 179214 2 D Blackwell L Dubins Merging opinions increasing information Annals Mathematical Statistics 38 1962 882886 3 DP Foster R Vohra Calibrated learning correlated equilibrium Games Economic Behavior 21 1997 4055 4 DP Foster R Vohra Regret online decision problem Games Economic Behavior 29 1999 735 5 DP Foster HP Young On impossibility predicting behavior rational agents Proceedings National Academy Sciences USA 98 222 2001 1284812853 6 DP Foster HP Young Learning hypothesis testing Nash equilibrium Games Economic Behavior 45 2003 7396 7 DP Foster HP Young Regret testing learning play Nash equilibrium knowing opponent Theoretical Economics 1 2006 341367 8 D Fudenberg D Levine The Theory Learning Games MIT Press Cambridge MA 1998 9 F Germano G Lugosi Global convergence Foster Youngs regret testing Games Economic Behavior 2007 press 10 S Hart A MasColell A simple adaptive procedure leading correlated equilibrium Econometrica 68 2001 11271150 11 S Hart A MasColell A general class adaptive strategies Journal Economic Theory 98 2001 2654 12 S Hart A MasColell Stochastic uncoupled dynamics Nash equilibrium Games Economic Behavior 57 2006 286303 13 JS Jordan Bayesian learning normal form games Games Economic Behavior 3 1991 6091 14 JS Jordan Three problems learning mixedstrategy equilibria Games Economic Behavior 5 1993 368386 15 E Kalai E Lehrer Rational learning leads Nash equilibrium Econometrica 61 1993 10191045 16 H Moulin JP Vial Strategically zerosum games class games completely mixed equilibria improved Interna tional Journal Game Theory 7 1978 201221 17 Y Shoham R Powers T Grenager If multiagent learning answer question Artiﬁcial Intelligence 171 7 2007 365377 issue 18 HP Young Strategic Learning Its Limits Oxford University Press Oxford 2004 1 It ﬁrst deﬁned named early paper Moulin Vial 16 Hart MasColell 11 Hannan set