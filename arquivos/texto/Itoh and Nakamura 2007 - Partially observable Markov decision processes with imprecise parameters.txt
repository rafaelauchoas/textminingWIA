Artiﬁcial Intelligence 171 2007 453490 wwwelseviercomlocateartint Partially observable Markov decision processes imprecise parameters Hideaki Itoh Kiyohiko Nakamura Department Computational Intelligence Systems Science Interdisciplinary Graduate School Science Engineering Tokyo Institute Technology 4259G346 Nagatsutacho Midoriku Yokohama Kanagawa 2268502 Japan Received 11 May 2005 received revised form 7 March 2007 accepted 16 March 2007 Available online 24 March 2007 Abstract This study extends framework partially observable Markov decision processes POMDPs allow parameters probability values state transition functions observation functions imprecisely speciﬁed It shown extension reduce computational costs associated solution problems First new framework POMDPs imprecise parameters POMDPIPs formulated We consider 1 interval case parameter imprecisely speciﬁed interval indicates possible values parameter 2 pointset case prob ability distribution imprecisely speciﬁed set possible distributions Second new optimality criterion POMDPIPs introduced As POMDPs criterion regard policy actionselection rule optimal maximizes expected total reward The expected total reward calculated precisely POMDPIPs parameter impre cision Instead estimate total reward adopting arbitrary secondorder beliefs beliefs imprecisely speciﬁed state transition functions observation functions Although possible choices secondorder beliefs regard policy optimal long choices policy maximizes total reward Thus multiple optimal policies POMDPIP We regard policies equally optimal aim obtaining By appropriately choosing secondorder beliefs use estimating total reward computational costs incurred obtaining optimal policy reduced signiﬁcantly We provide exact solution algorithm POMDPIPs efﬁciently Third performance optimal policy computational complexity algorithm analyzed theoretically Last empirical studies algorithm quickly obtains satisfactory policies POMDPIPs 2007 Elsevier BV All rights reserved PACS 0705Mh 0250Le Keywords POMDP Secondorder beliefs Parameter set Probability interval Corresponding author Email addresses hideakidistitechacjp H Itoh nakamuradistitechacjp K Nakamura 00043702 matter 2007 Elsevier BV All rights reserved doi101016jartint200703004 454 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 1 Introduction The theory partially observable Markov decision processes POMDPs normative sequential decision making uncertainty 2153551 It provides general framework designing intelligent agents 830 realworld applications reported 2938 Let consider toy example called tiger problem Imagine agent standing closed doors Behind doors tiger large reward If agent opens door tiger large penalty received presumably form bodily injury Instead opening doors agent listen order gain information location tiger Unfortunately listening free addition entirely accurate There chance agent hear tiger lefthand door tiger righthand door vice versa 30 What best actionselection rule agent By standard POMDP problem modeled follows The possible states environment sl meaning tiger lefthand door sr righthand door Assume agents initial belief states sl sr equally probable Prsl Prsr 05 The agent choose action LEFT open left door RIGHT open right door LISTEN listen tiger The action LEFT results 10 reward state sr 100 sl rewards reversed action RIGHT The action LISTEN costs 1 reward agent obtains noisy observation TL meaning tiger likely lefthand door TR tiger likely righthand door If state sl TL observed probability 085 TR observed probability 015 PrTLsl 085 PrTRsl 015 similarly PrTRsr 085 PrTLsr 015 An actionselection rule agent called policy A policy called optimal maximizes expected total reward In example optimal policy 1 choose action LISTEN times agents belief tiger right left sufﬁciently strong 2 choose action LEFT RIGHT accordingly POMDP theory tells optimal policy derived POMDPs parameters PrTLsl 085 speciﬁed precisely The parameters remain imprecise reasons 51332 including limited data insufﬁcient inference time dis agreement experts 3449 model abstraction 122023 Here mention examples First suppose PrTLsl estimated experiment examines frequency TL observed state sl Such estimate subject statistical error For case intervals 95 conﬁdence intervals express uncertainty Second suppose human expert consulted determine value PrTLsl The expert determine value subjective belief PrTLsl equal 085 However ﬁnd hard explain PrTLsl precisely 085 0849 example Thus situation intervals express experts belief faithfully Third suppose multiple experts consulted Even expert specify precise value parameter values differ In case set distributions adopted express disagreed uncertainty For example PrTLsl PrTRsl 084 016 085 015 expert speciﬁed dis tribution PrTLsl PrTRsl 084 016 expert speciﬁed 085 015 Last suppose tiger problem abstracted version complex problem For example probability distribution observing TL TR given sl actually depend temperature sonic sensor Suppose temperature high denoted thigh distribution PrTLsl thigh PrTRsl thigh 084 016 Similarly temperature low distribution PrTLsl tlow PrTRsl tlow 085 015 The agent want neglect small dependence temperature In case set distributions adopted express abstracted uncertainty PrTLsl PrTRsl 084 016 085 015 Motivated examples paper introduce POMDPs imprecise parameters POMDPIPs We consider cases One interval case parameter imprecisely speciﬁed interval PrTLsl 084 086 The pointset case distribution speciﬁed set distributions PrTLsl PrTRsl 084 016 085 015 In paper consider parameter imprecision state transition functions probability functions model state changed action H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 455 observation functions probability functions model observation obtained Other imprecisions imprecision reward function remain considered future Another motivation introduction POMDPIPs arises fact POMDPs computationally expensive solve 3642 By solving POMDP mean obtaining optimal policy Although algorithms solve POMDPs ﬁnite time 1124515758 developed given nonprohibitive time pe riod relatively smallsized POMDPs solved algorithms This high computational cost fact algorithms seek optimal policy strictly maximizes expected reward In problems strict optimization meaningless expected reward precisely evaluated parameter imprecision For problems sufﬁcient maximize expected reward roughly estimated imprecise parameters Such rough optimization require lower computational cost Thus motivated paper formulate optimality criterion POMDPIPs following manner We begin considering hypothetical situation strict optimization performed POMD PIPs To perform strict optimization need information POMDPIPs Let assume hypothetically agent specify correct secondorder beliefs 1941 beliefs models deﬁne model pair state transition function observation function For instance ex ample abstracted uncertainty probability distribution observing TL TR PrTLsl thigh PrTRsl thigh 084 016 PrTLsl tlow PrTRsl tlow 085 015 depending temperature sonic sensor Suppose simplicity probability distributions model state transition function observation function speciﬁed precisely Section 3 general case Thus models high temperature low temperature denote mhigh mlow respectively Suppose hypothetically agent performed detailed experiment temperature high low probability 05 Then secondorder belief mhigh mlow equally probable If additional beliefs given total reward deﬁned exactly strict optimization performed Then introduce relaxed optimality criterion Recall secondorder beliefs given POMD PIPs None secondorder beliefs considered reliable Thus adopt arbitrary secondorder beliefs estimate total reward We consider policy optimal long maximizes estimated total reward We refer policies satisfy optimality criterion quasioptimal policies By quasi mean policies optimized secondorder beliefs necessarily correct There possible approaches adopting arbitrarilyselected secondorder beliefs instead correct unknown beliefs When estimate total reward use secondorder belief multiple purposes estimate reward obtained immediately action estimate future rewards action subsequent observation One approach adopt single arbitrarilyselected secondorder belief instead correct belief use purposes The approach use multiple arbitrarilyselected second order beliefs instead correct belief use different beliefs different purposes Although research necessary detailed comparison argue Section 33 disadvantageous terms performance optimal policy obtained advantageous terms computational costs Thus choose approach study Next provide exact algorithm obtain quasioptimal policies The algorithm exploits fact allowed secondorder beliefs adopted arbitrarily To avoid confusion secondorder beliefs let refer beliefs states ﬁrstorder beliefs A ﬁrstorder belief said reachable agents ﬁrstorder belief actions observations In POMDPs usual large number ﬁrstorder beliefs reachable makes hard calculate optimal policy Our algorithm reduces number reachable ﬁrstorder beliefs picking secondorder beliefs ﬁrstorder beliefs reached repeatedly We secondorder beliefs picked efﬁciently solving linear programming problems We provide theoretical bound reward loss occur quasioptimal pol icy compared optimal policy hypothetical situation correct secondorder beliefs given strict optimization conducted Furthermore study empirically conditions quasioptimal policies satisfactory performance easy obtain algorithm 456 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Our optimality criterion closely related Eadmissibility 3334 If approach adopt single arbitrarilyselected belief instead correct unknown belief criterion exactly Eadmissibility quasioptimal policy optimal regard adopted single belief However select approach adopt multiple arbitrarilyselected beliefs quasioptimal policy optimal regard single belief Note argue criterion better Eadmissibility Detailed comparisons await future research In POMDP literature Cozman et al 14 considered Eadmissibility criterion parameter imprecision motivation reducing computational costs However study limited case agents actions affect state environment observation probability functions limited Gaussian distributions The present study considers general case There studies decisionmaking parameter imprecision deﬁnitions opti mality 21345054 For fully observable Markov decision processes MDPs parameter imprecision algorithms provided obtain maxmin policies 204856 maxmax policies 2048 Eadmissible poli cies 55 maximal policies 27 For inﬂuence diagrams IDs parameter imprecision admissible policies studied 101718 The ideas studies principle applied POMDPs However computational complexity solving corresponding problem parameters precise Thus approaches offers means reducing computational complexity applied POMDPs Other studies devoted solving largesized POMDPs One approach develop approximate algo rithms 28 gridbased algorithms 72859 αvectorbased algorithms 4446475253 policygradient methods 1 Another approach exploit structure POMDP problem factored state representation 925 hierarchies 26 Their combinations approximate algorithms factored state representation 16 37 studied However study explored possibility exploiting parameter imprecision reduce computational costs This paper organized follows Section 2 reviews POMDPs transformations historystate MDPs beliefstate MDPs background following sections Section 3 formulates POMDPIPs quasi optimal policies In Section 4 provide algorithm obtain quasioptimal policies The performances quasioptimal policies provided algorithm analyzed theoretically Section 5 empirically Section 6 Section 7 concludes study 2 POMDPs In section review POMDPs transformations 628 introductory explanations 21 Deﬁnition We assume discrete time steps We deﬁne POMDP tuple S A Θ Th h H Oh h H R p0 S ﬁnite set states A ﬁnite set actions Θ ﬁnite set observations Th h H set state transition functions Th S A S 0 1 state transition function given h H H set histories deﬁned follows Let st S A ot Θ denote state action observation time t 0 1 2 respectively Let ht a0 o1 a1 o2 at1 ot denote history time t1 We length history t We introduce Ht a0 o1 a1 o2 at1 ot a0 a1 at1 A o1 o2 ot Θ set possible tories length t Finally let H t0 Ht set histories length For h H s scid6 S A let Ths scid6 probability state scid6 reached state s action history h Note state transition function historydependent Th1 Th2 different scid6S Ths scid6 1 h H s S A functions h1 h2 different histories It holds cid2 cid3 1 For notational simplicity assume agent observe o0 time t 0 The modiﬁcation assumption include o0 straightforward H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 457 Fig 1 An inﬂuence diagram single time step POMDP Oh h H set observation functions Oh S A Θ 0 1 observation function given h H Note functions historydependent Ohscid6 o probability observation o oΘ Ohscid6 o 1 given agent state scid6 reached action history h It holds h H scid6 S A R S A S R reward function Rs scid6 denotes reward agent gains state s cid3 changes scid6 action s scid6 S A p0 S 0 1 prior probability function initial state s S It holds cid3 sS p0s 1 The process proceeds follows time t 0 process initial state s0 S probability p0s0 history h0 At time t 0 1 2 state history denoted st ht a0 o1 a1 o2 at1 ot Ht respectively The agent selects action A changes current state st subsequent state st1 probability Tht st st1 The agent observes ot1 Θ probability Oht st1 ot1 gains reward rt Rst st1 history changed ht1 a0 o1 a1 o2 at1 ot ot1 Ht1 The relations variables single time step shown Fig 1 For agent select action time t assume POMDP tuple S A Θ Th h H Oh h H R p0 past history ht a0 o1 a1 o2 at1 ot available The action selection rule agent mapping available information action called policy Solving POMDP ﬁnd optimal policy maximizes objective function We adopt inﬁnitehorizon discounted sum expected rewards criterion objective function deﬁned γ t1rt 1 cid6 cid4 cid5 E t1 0 cid2 γ 1 discount factor expectation E taken possible process paths2 22 Historystate MDP Solving POMDP problem equivalent solving fully observable Markov decision process MDP called historystate MDP In historystate MDPs alternative view POMDP process Fig 2 At time t 0 starts initial history history At time t 0 1 2 let h current history The agent selects action A observes o Θ gains reward The history h changed new history 2 The algorithm provide later modiﬁed ﬁnitehorizon cases objective function E termination time T cid3 T t1 rt given 458 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Fig 2 History tree POMDP actions a1 a2 observations o1 o2 Each open circle corresponds history treated state historystate MDP We associate history h corresponding belief bh The subscripts b indicate history belief associated b11 belief associated history h a1 o1 Taking view consider Bellman equation derive optimal policy We require functions purpose First let bh termed belief probability function bhs probability current state s given past history h It calculated recursively follow To begin let initial belief history h0 b p0 2 Note means b p0 identical functions bs p0s s S Next h H A o Θ let h cid8a ocid9 denote history o followed h The belief bhcid8aocid9 calculated bh Bayes rule Ohscid6 o scid6cid6S Ohscid6cid6 o sS Ths scid6bhs cid3 sS Ths scid6cid6bhs bhcid8aocid9s 3 cid3 cid3 cid6 scid6 S Second let P oh probability o observed given action taken history h It Last let ρh average reward agent gain taking action history h We calculated P oh cid5 scid6S cid6 Ohs o cid5 sS Ths s cid6 bhs ρh Rs s cid6 Ths s cid6 bhs cid5 cid5 sS scid6S cid7 Now principle optimality 4 write Bellman equation V h max aA ρh γ cid5 oΘ P oh aV cid10 cid9 cid8 h cid8a ocid9 h H V H R called optimal value function The optimal policy deﬁned mapping μ H A satisﬁes cid7 μ h arg max aA ρh γ h H P oh aV cid10 cid9 cid8 h cid8a ocid9 cid5 oΘ 4 5 6 7 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 459 23 Beliefstate MDP Solving historystate MDP previous section equivalent solving MDP problem called beliefstate MDP 2 restrict T s Os follows First restrict T s satisfy Th1 bh2 holds h1 h2 H Note Th2 bh1 symbols denote equality functions In words impose condition Th1 s scid6 Th2 s scid6 s scid6 S A beliefs h1 h2 bh1 s bh2 s bh2 treat Th1 Th2 identical Consequently redeﬁne set s S With restriction bh1 T s Tb b B instead Th h H B set possible beliefs B b b bh h H bh2 holds h1 h2 H Second similarly restrict Os satisfy Oh1 Consequently redeﬁne Os Oh h H Ob b B Oh2 bh1 With T s Os restricted belief b B sufﬁcient statistic That history belief b tuple S A Θ Tb b B Ob b B R p0 summarizes information available time agent predict happen probability future Thus Bellman equation Eq 6 rewritten cid7 V b max aA ρb γ cid5 oΘ P ob aV cid10 cid9 cid8 τ b o b B ρb P ob deﬁned redeﬁned Eqs 5 4 cid5 cid5 cid5 cid5 ρb Rs s cid6 Tbs s cid6 bs P ob cid6 Obs o Tbs s cid6 bs sS scid6S scid6S sS τ beliefupdate function deﬁned sS Tbs scid6bs cid3 cid3 cid3 cid6 τ b os Obscid6 o scid6cid6S Obscid6cid6 o sS Tbs scid6cid6bs The optimal policy rewritten mapping μ B A satisﬁes cid5 oΘ P ob aV cid10 cid9 cid8 τ b o scid6 S cid7 μ b arg max aA ρb γ b B 3 POMDPIPs 31 Deﬁnition 8 9 10 Here formulation POMDPs imprecise parameters POMDPIPs In POMDPIPs process proceeds exactly way POMDP Section 21 agent knows Th Oh history h H imprecisely A POMDPIP deﬁned tuple S A Θ T M OM R p0 S A Θ R p0 deﬁned POMDPs Furthermore let H set histories3 T M modelset function state transition functions Thh H We consider cases Fig 34 One interval case T M S A S I indicates range possible values Ths scid6 s scid6 S A h H I set intervals 0 1 For example suppose T M s scid6 08 09 certain s scid6 This means h H Ths scid6 08 09 s scid6 The case consider pointset 3 Here consider imprecision Th Oh In case p0 imprecise equivalent POMDPIP Th Oh imprecise constructed introducing auxiliary state Further research required handle imprecision R 4 For simplicity consider cases separately It straightforward consider combination cases 460 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Fig 3 Example Ths POMDPIPs The ﬁgure shows probability simplex ΔS S 3 Ths p1 p2 p3 certain h H s S A located A interval case For scid6 S Ths scid6 interval speciﬁed T M s scid6 Consequently Ths lie convex area striped ﬁgure B pointset case Ths probability functions T M s speciﬁes dots ﬁgure n cid3 i1 pi 1 Next let Δ case T M S A Δ S indicates probability functions Ths possibly identical Δ S deﬁned follows First n cid3 0 let Δn ndimensional probability simplex Δn p1 p2 pn pi cid3 0 n set ﬁnite sets different points Δn Thus s S A T M s ﬁnite set probability functions For example suppose states S s1 s2 s3 T M s 08 01 01 085 01 005 certain s This means h H Ths 08 01 01 Ths 085 01 005 relation Ths p1 p2 mean Ths s1 p1 Ths s2 p2 OM modelset function observation functions OM deﬁned way T M interval case OM S A Θ I indicates range possible values parameter Thus Ohscid6 o OM scid6 o scid6 S A o Θ h H For pointset case OM S A Δ Θ indicates possible probability functions Thus Ohscid6 OM scid6 scid6 S A h H 5 The information available agent selection action time t tuple S A Θ T M OM R p0 past history ht Let deﬁne basic notions later use We pair statetransition function Th observation function Oh h H model Further let M0 denote set possible models deﬁned regard modelset functions That deﬁne M0 cid7 M0 T O T S A S 0 1 cid5 T s s cid6 1 s S A O S A Θ 0 1 scid6S cid5 cid6 Os o 1 s cid10 cid6 S A oΘ Finally let M denote set possible models deﬁned regard modelset functions For interval case cid11 M T O T O M0 T s s cid6 T M s s cid6 s s cid6 Os o OM s cid6 o s cid6 S A o Θ cid6 S A cid12 5 Note Th Oh assumed historydependent This assumption natural problems values parameters Th Oh ﬂuctuate unknown neglected dynamics dynamics sonic sensors temperature Section 1 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 461 M set models parameter state transition function T observation function O interval speciﬁed corresponding modelset function T M OM For pointset case cid11 M T O T O M0 T s T M s s S A s cid12 cid6 S A OM s Os cid6 cid6 M set models probability distribution state transition function T observation function O identical possible distributions speciﬁed corresponding modelset function T M OM 32 A truly optimal policy Before formulate optimality criterion POMDPIPs section let consider hypothetical situation optimal policies deﬁned normative way POMDPs This provides basis formulate relaxed optimality criterion POMDPIPs section In POMDPIPs assumed agent knows h H model mh Th Oh member M One way deal uncertainty model use secondorder belief belief models Let suppose hypothetically agent information form specify secondorder belief probability density function bM h mh probability given history h model mh govern process immediately h h M0 0 1 let bM For instance let consider example abstracted uncertainty Section 1 Suppose model mh Th Oh depends temperature sonic sensor mhigh mlow high low tem peratures respectively If agent performs detailed experiment ﬁnds temperature high low h mh 12δmh mhigh δmh mlow probability agent sets secondorder belief bM δ Diracs delta function The secondorder belief bM h naturally assumed satisfy bM h mh 0 mh M cid13 mhM bM h mh dmh 1 11 12 h H 6 We Eqs 11 12 permissibility condition secondorder belief That function f M0 0 1 satisﬁes permissibility condition satisﬁes f m 0 m M cid14 mM f m dm 1 Now consider modiﬁed version POMDPs process proceeds exactly way original POMDPs Section 21 model mh h H determined stochastically probability bM h mh We refer modiﬁed POMDP given secondorder beliefs hypothetical POMDP In hypothetical POMDPs imprecise parameters Thus deﬁne optimal policy maximizes discounted sum expected rewards Eq 1 We optimal policies truly optimal policies For later use let derive Bellman equations truly optimal policy satisﬁes Let consider equivalent historystate MDP Section 22 Fig 2 First belief states bh S 0 1 h H calculated recursively follows To avoid confusion refer belief ﬁrstorder belief Let initial ﬁrstorder belief b p0 13 Next h H A o Θ belief bhcid8aocid9 calculated bh Bayes rule 6 We implicitly assume integral Eq 12 paper exist Then principle optimality 4 truly optimal policy deﬁned mapping μ H A 462 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 cid14 mhThOhM Ohscid6 o cid3 cid3 bhcid8aocid9s cid6 cid9 s scid6 S redeﬁned beliefupdate function τ Eq 9 τ scid6cid6S Ohscid6cid6 o cid6 cid14 mhThOhM cid8 bh o bM h sS Ths scid6bhsbM cid3 h mh dmh sS Ths scid6cid6bhsbM h mh dmh satisﬁes Bellman equations exactly Eqs 6 7 cid5 cid7 V h max aA ρh γ P oh aV cid10 cid9 cid8 h cid8a ocid9 μ h arg max aA ρh γ oΘ cid5 oΘ P oh aV cid10 cid9 cid8 h cid8a ocid9 cid7 cid13 ρh mhThOhM cid13 P oh mhThOhM cid5 cid5 sS scid6S Rs s cid6 Ths s cid6 bhsbM h mh dmh cid5 scid6S cid6 Ohs o cid5 sS Ths s cid6 bhsbM h mh dmh h H redeﬁne ρh P oh Eqs 5 4 respectively 14 15 16 17 18 Below notes secondorder beliefs introduced section First introduced beliefs models beliefs mhs types beliefs considered instead For example introduce beliefs conditional distributions beliefs Ths s s S A h H beliefs Ohscid6 s scid6 S A h H Another example beliefs sets models beliefs mhh H s Comparisons remain studied future following paragraph Second actually use secondorder beliefs conditional distributions provide solution algorithm Section 4 A belief conditional distributions equivalent belief models decomposable product beliefs conditional distributions Section 42 We use decomposable beliefs provide solution algorithm Section 4 Thus actually consider beliefs conditional distributions far solution algorithm concerned However began nondecomposed secondorder beliefs section wider class beliefs utilized wider range problems theoretical result Section 51 applicable beliefs Last parameter imprecision handled secondorder beliefs paper handled equivalently convex hulls possible probability measures credal sets 1333 Obviously secondorder belief bM h mh satisﬁes permissibility condition Eqs 11 12 referred member convex hull possible measures models convex hull fimh δmh mi h 1 2 fi M0 0 1 mi h member M δ Diracs delta function Similarly obvious Section 42 secondorder belief conditional distributions equivalent convex hull possible conditional distributions 33 Quasioptimal policies In previous section deﬁned truly optimal policies strict optimal criterion assuming agent specify secondorder beliefs In POMDPIPs agent precise idea specify secondorder beliefs In section formulate relaxed optimality criterion allowing agent employ arbitrary functions secondorder beliefs First recall deﬁne truly optimal policy secondorder belief bM h determined order Bayesupdate ﬁrstorder belief Eq 14 h H A o Θ Since bM h unknown POMDPIPs allow agent employ function satisﬁes permissibility condition Section 32 purpose Let ˆbM hao denote employed function H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 463 Fig 4 History tree POMDPIP actions observations The possiblycorrect ﬁrstorder beliefs calculated recursively possiblycorrect secondorder beliefs ˆb11 τ ˆb a1 o1 ˆbM a1o1 ˆb12 τ ˆb a1 o2 ˆbM a1o2 In following bM h Section 32 correct secondorder belief By correct stress bM h deﬁned belief agent employ sufﬁcient information available On hand ˆbM hao possiblycorrect secondorder belief By possibly correct mean possibly identical correct belief Let ˆbh denote ﬁrstorder beliefs calculated possiblycorrect secondorder beliefs That ˆbh h H calculated recursively Fig 4 ˆb p0 ˆbhcid8aocid9 τ cid8 ˆbh o ˆbM hao cid9 19 20 h H A o Θ We bh Section 32 correct ﬁrstorder beliefs We ˆbh possibly correct ﬁrstorder beliefs Second recall h H secondorder belief bM h estimate expected onestep reward ρh probability observation P oh Eqs 17 18 respectively Again allow agent employ function satisﬁes permissibility condition purposes Let ˆbM h denote employed function shall label possiblycorrect secondorder beliefs Last deﬁne quasioptimal policy solution Eqs 1318 bM h replaced arbitrary h That quasioptimal policy policy ˆμ H A satisﬁes possiblycorrect functions ˆbM cid7 ˆV h max aA ˆρh γ ˆP oh ˆV cid10 cid9 cid8 h cid8a ocid9 hao ˆbM cid5 oΘ 21 464 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 cid7 ˆμ h arg max aA ˆρh γ ˆP oh ˆV cid10 cid9 cid8 h cid8a ocid9 cid5 oΘ h H cid13 ˆρh mhThOhM cid13 ˆP oh mhThOhM cid5 cid5 sS scid6S Rs s cid6 Ths s cid6 ˆbhs ˆbM h mh dmh cid5 scid6S cid6 Ohs o cid5 sS Ths s cid6 ˆbhs ˆbM h mh dmh 22 23 24 ˆbh deﬁned Eqs 19 20 The hat mark ˆ V μ ρh P oh introduced order stress functions calculated correct unknown secondorder beliefs In summary quasioptimal policy policy satisﬁes Bellman equations hypothetical POMDPs secondorder beliefs replaced arbitrary possiblycorrect ones We regard quasi optimal policies solution POMDPIPs We proceed additional comments policies We ﬁrst note h H replaced correct secondorder belief bM hao A o Θ ˆbM h multiple possiblycorrect secondorder beliefs ˆbM h We instead single possibly correct secondorder belief Doing merit use single belief quasi optimal policy guaranteed truly optimal policy possible hypothetical POMDP termed Eadmissible 3334 However paper study use multiple beliefs following reasons First POMDPIPs multiple beliefs lead robust policy single belief risky rely single belief We provide simple example Appendix A Second multiple beliefs higher degree freedom quasioptimal policy easier obtain This useful property motivations paper solve problems low computational costs Note argue use multiple beliefs better use single belief Further research necessary detailed comparisons Section 61 empirical study Note algorithm section modiﬁed use single belief modiﬁcation increase computational costs We note deﬁnition possiblycorrect ﬁrstorder belief ˆbh history h H single probability function Another possibility use set probability functions 1318 This possibility worth investigating future The manner changing set probability functions given new action observation currently topic debate 322 multiple future research directions In paper use set probability functions We regard quasioptimal policy optimal policy broader sense agent allowed use single probability function expressing belief states Note introduced secondorder beliefs use inference models In future study interesting use secondorder beliefs inference uncertain models Bayesian inference identify true model actionobservation history Although strict inference impossible prohibitive computational costs able focus identifying true model extent remaining uncertainty signiﬁcantly affect total reward Making inference POMDPIPs interesting future research topic We ﬁnally note ˆbM hao ˆbM h arbitrary multiple quasioptimal policies single POMDPIP We regard policies solution POMDPIP In section provide efﬁcient algorithm obtain quasioptimal policies 4 Algorithm solving POMDPIPs 41 Determining ˆbM hao The ﬁrst step algorithm determine ˆbM condition Recall determine ˆbM hao h H A o Θ permissibility haos possiblycorrect ﬁrstorder beliefs ˆbh H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 465 h H given Eqs 19 20 Let ˆB set different ﬁrstorder beliefs derived ˆB ˆb ˆb ˆbh h H We try ˆB number different ﬁrstorder beliefs small possible order reduce computational costs This achieved following procedure term FINDASMALL BELIEFSET procedure We determine ˆbM Fig 4 example ﬁrst calculate ˆb set equal p0 determine ˆbM ˆbM a1o2 calculate ˆb12 determine ˆbM ˆbhcid8aocid9 τ ˆbh o ˆbM For example Fig 4 suppose determined ˆbM ˆb11 ˆb12 Now search ˆbM ˆb12 For search use ISFEASIBLE procedure Section 42 If ˆbM employ arbitrary ˆbM haos breadthﬁrst manner calculate ˆbhs possible a1o1 calculate ˆb11 determine a1o1a1o1 calculate ˆb1111 forth In determining ˆbM hao try hao identical ﬁrstorder beliefs calculated a1o2 calculated ˆb a1o1 ˆbM a2o1 identical ˆb ˆb11 a2o1 ˆb21 τ ˆb a2 o1 ˆbM a2o1 adopted a2o1 permissibility condition If possible ﬁrstorder belief identical descendant ﬁrst secondorder beliefs identical For example suppose ˆb21 identical ˆb11 Since determine a1o1a1o2 respectively ˆb2111 ˆb1111 a1o1a1o1 ˆbM a2o1a1o1 ˆbM ˆbM ˆb2112 ˆb1112 By following rule skip determination calculation beliefs following ˆb21 need consider beliefs following ˆb11 a2o1a1o2 ˆbM We continue determining calculating beliefs skipping possible Eventually haos ˆbhs determined skip remaining ones proof Section 52 This means ˆbM calculated respectively We proceed step described Section 43 42 ISFEASIBLE procedure The ISFEASIBLE procedure searches permissibility condition function ˆbM M0 0 1 τ ˆb o ˆbM ˆbcid6 holds given ﬁrstorder beliefs ˆb ˆbcid6 action observation o POMDPIP tuple S A Θ T M OM The search desired secondorder belief FINDASMALL BELIEFSET procedure Section 41 performed procedure times Each time ISFEASIBLE procedure let ˆbcid6 ﬁrstorder beliefs calculated let ˆb ˆbh For example Fig 4 suppose determined ˆbM a1o2 calculated ˆb ˆb11 ˆb12 We search ˆbM a2o1 identical ˆb ˆb11 ˆb12 This search performed ISFEASIBLE procedure times ˆbcid6 ˆb ˆb11 ˆb12 ˆb ˆb a1o1 ˆbM a2o1 τ ˆb a2 o1 ˆbM We begin replacing required task easier Recall task search permissi bility condition ˆbM cid6 cid6 ˆb s cid14 cid14 mT OM Oscid6 o cid3 cid3 scid6cid6S Oscid6cid6 o mT OM sS T s scid6 ˆbs ˆbM m dm cid3 sS T s scid6cid6 ˆbs ˆbM m dm s cid6 S holds ˆb ˆbcid6 o given We restrict search ˆbM decomposed according cid8 Os cid8 T s ˆbM m cid9 cid9 cid15 cid15 cid6 Gscid6 Fs sS aA scid6S aA m T O M0 Fs s S A Gscid6 scid6 S A probability density functions Here denote action distinguish action constant speciﬁed FINDASMALLBELIEFSET procedure procedure Note decomposable secondorder beliefs exist Note restriction search fail ﬁnd desired ˆbM exists Hence number resultant ﬁrstorder beliefs ˆB increase However restricted search performed quickly total computational time reduced Note ﬁnd quasioptimal policy despite restriction 25 26 466 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Fig 5 Example averaged model function ˆT s The striped areas indicate ˆT s located Compare ﬁgure Fig 3 letting A interval case ˆT s scid6 convex area constrained Ths scid6 Fig 3 B pointset case ˆT s convex combination possible probability functions Ths Fig 3 identical Let deﬁne averaged model functions cid13 ˆT s s cid6 T s s cid6 Fs T s aΔS cid9 cid8 T s dT s s scid6 S A cid13 cid6 ˆOs o Oscid6 aΔΘ cid6 Os oGscid6 cid8 Os cid6 cid9 dOs cid6 27 28 scid6 S A o Θ Again denote observation o distinguish speciﬁed observa tion o From Eqs 2628 rewrite Eq 25 cid6 cid6 ˆb s cid3 cid3 sS cid3 ˆOscid6 o ˆT s scid6 ˆbs ˆOscid6cid6 o scid6cid6S ˆT s scid6cid6 ˆbs sS s cid6 S 29 From permissibility condition Eqs 26 27 s S A ˆT s convex combination possible probability functions modelset function T M speciﬁes In interval case Fig 3A possible probability functions convex area constrained intervals parameter Thus convex combination ˆT s constrained intervals Fig 5A In pointset case Fig 3B possible probability functions directly speciﬁed modelset functions Thus ˆT s convex combination probability functions Fig 5B We conditions ˆT s s S A satisfy convexity conditions ˆT Similarly conditions ˆOscid6 scid6 S A convexity conditions ˆO In summary search ˆbM satisﬁes Eq 25 permissibility condition replaced search pair averaged model functions ˆT ˆO satisﬁes Eq 29 convexity conditions ˆT ˆO Whether pair averaged model functions exists easily determined feasibility test Fig 6 A proof validity test detailed Appendix B Henceforth shall label implementation test ISFEASIBLE procedure The averaged model functions ˆT ˆO exist derived solution test7 In procedure let T T O O denote lower upper bounds parameter interval case lower bounds deﬁned 7 Let ˆOscid6 o Zqscid6 scid6 S Employ arbitrary values convexity conditions undetermined parameters H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 467 A Procedure ISFEASIBLE interval case input ˆb o ˆbcid6 S A Θ T M OM output True False Test solution satisﬁes following constraints variables ˆT s scid6 R s scid6 S qscid6 R scid6 S Z R constraints T s scid6 cid2 ˆT s scid6 cid2 T s scid6 s scid6 S cid3 cid3 ˆT s scid6 1 s S ˆbs ˆT s scid6 ˆbcid6scid6qscid6 scid6 S cid2 qscid6 cid2 Z Oscid6ao scid6 S scid6S sS Z Oscid6ao Z cid3 0 Return True solution Return False end procedure B Procedure ISFEASIBLE pointset case input ˆb o ˆbcid6 S A Θ T M OM output True False Test solution satisﬁes following constraints 0 1 s S 1 T M s variables ˆT s scid6 R s scid6 S λi s qscid6 R scid6 S Z R constraints cid3 λi ˆT s scid6 cid3 cid3 s scid6 s scid6 S s T M 1 s S ˆbs ˆT s scid6 ˆbcid6scid6qscid6 scid6 S cid2 qscid6 cid2 Z Oscid6ao scid6 S λi s sS Z Oscid6ao Z cid3 0 Return True solution Return False end procedure Fig 6 Subroutine checks averaged model functions ˆT ˆO change ˆb ˆbcid6 A o Θ exist A interval case B pointset case In B deﬁne T M s number possible probability functions T M s speciﬁes Each probability function indexed T M s 1 2 T M s cid6 T s s cid6 Os min T M s s o min OM s cid6 o cid6 s scid6 S A o Θ In pointset case deﬁned T s s cid6 min T s aT M s T s s cid6 Os cid6 o min Oscid6 aOM scid6 Os cid6 o 30 31 32 33 s scid6 S A o Θ The upper bounds deﬁned substituting max min deﬁnitions The feasibility test Fig 6 conducted efﬁciently constraints linear interval case SS 1 1 variables 2SS 2 1 inequalities minimum comparison 468 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 number parameters bounds problem hand8 It efﬁcient pointset case The feasibility test solved subprocedures linear programming routines implemented programming languages The worstcase complexity polynomial order S 31 See Section 52 computational complexity overall algorithm For later use deﬁne notations Recall use procedure determine ˆbM hao h H A o Θ FINDASMALLBELIEFSET procedure We denote desired averaged model functions ˆThao ˆOhao If employ arbitrary ˆT ˆO satisfy convexity conditions denote ˆThao ˆOhao Thus ˆThao ˆOhao indicate implicitly ˆbM hao employed 43 Determining ˆbM h The second step algorithm determine h H ˆbM Although possible ways determine h satisﬁes permissibility condition ˆbM haomh mh M0 34 h mh 1 ˆbM Acid10Θ cid5 aAoΘ h H That average ˆbhaos Clearly ˆbM ˆbM h s haos satisfy permissibility condition Recall previous section replaced determination ˆbM ˆOhao determined ˆbM need ˆbM Eqs 23 24 righthand Eq 34 Eqs 2628 hao determination ˆThao h explicitly Eq 34 Recall h evaluation ˆρh ˆP oh Eqs 23 24 Substituting ˆbM hao implicitly Thus determine ˆbM h ˆρh 1 Acid10Θ ˆP oh 1 Acid10Θ cid5 cid5 cid5 Rs s cid6 ˆTh os s cid6 ˆbhs aA oΘ cid5 sS scid6S cid5 ˆOh os cid6 o cid5 ˆTh os s cid6 ˆbhs aA oΘ Thus ˆρh ˆP oh given terms ˆThaos determined sS scid6S ˆOhaos ˆbhs 44 Dynamic programming possiblycorrect ﬁrstorder beliefs The step algorithm solve Bellman equations Eqs 21 22 35 36 As Sec tion 23 solving equations equivalent solving beliefstate MDP cid7 ˆb max ˆρ ˆb γ aA ˆV cid7 ˆρ ˆb γ ˆb arg max aA ˆμ cid5 oΘ ˆρ ˆb 1 Acid10Θ ˆP o ˆb 1 Acid10Θ cid5 aA oΘ cid5 oΘ cid5 cid5 sS scid6S cid5 aA oΘ scid6S ˆP o ˆb ˆV cid8 τ cid8 ˆb o ˆbM ˆbao cid5 ˆP o ˆb ˆV cid8 cid8 τ ˆb o ˆbM ˆbao cid10 cid9cid9 cid10 cid9cid9 Rs s cid6 ˆT ˆb os s cid6 ˆbs ˆO ˆb os cid6 o cid5 sS ˆT ˆb os s cid6 ˆbs 35 36 37 38 39 40 8 To precise procedure Fig 6 correct condition ˆbcid6scid6 cid11 0 Oscid6 o Oscid6 o cid11 0 0 scid6 S For general case require little complicated procedure described Appendix B H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 469 ˆP ˆT ˆO redeﬁned ˆB instead H example substitute ˆρ ˆb ˆV ˆμ ˆρ ˆρh ˆbh To solving equations equivalent solving Eqs 21 22 35 36 note construction ˆTh1ao ˆTh2ao h1ao τ ˆbh2 o ˆbM ˆbh2 holds h1 h2 H A o Θ Finally ˆB ﬁnite quasioptimal policy obtained solving equations Eqs 3740 nu ˆOh1ao ˆOh2ao τ ˆbh1 o ˆbM h2ao ˆbh1 merically example value iteration methods 6 5 Theoretical analyses 51 A bound reward losses quasioptimal policies We provide bound reward loss occur quasioptimal policy instead specifying correct secondorder beliefs performing strict optimization Note bound applicable quasioptimal policies obtained algorithm Section 4 quasioptimal policies obtained method max cid10O In following let cid10T max denote maximum imprecision parameters Th Oh cid16 cid16T s s cid6 T s s max 41 cid16 cid16 cid6 cid10T max sscid6S aA cid10O max max scid6S aA oΘ cid16 cid16Os cid6 o Os cid6 cid16 cid16 o 42 Let V ˆμ ˆμ evaluated hypothetical POMDP Let V μ evaluated hypothetical POMDP Note deﬁnition V μ cid3 V ˆμ value inﬁnitehorizon discounted sum expected rewards given quasioptimal policy value truly optimal policy μ Theorem 1 The reward loss difference V ˆμ V μ 16γ dRmax 1 15γ γ d ˆV bounded max ˆV μ ˆμ max 1 γ 2 V μ V ˆμ cid2 1 γ Scid10T max d Scid10T Rmax max max Θcid10O max cid16 cid16Rs s 2Θcid10Scid10T cid16 cid16 cid6 maxcid10O max sscid6S aA ˆV μ max max hH aA oΘ cid16 cid16 ˆV μ cid8 h cid8a ocid9 cid9cid16 cid16 policy μ H A ˆV μ H R solution cid5 cid9 cid8 h cid8μh ocid9 cid9 cid8 oh μh cid9 cid8 h μh ˆV μh ˆρ ˆV μ γ ˆP 43 44 45 46 h H ˆρ ˆP functions Eqs 21 22 quasioptimal policy ˆμ satisﬁes oΘ Proof See Appendix C cid2 Here notes bound calculated upperbounded First ˆV easy calculate Second ˆV max ˆV μ ˆμ ˆμ max readily available algorithm Section 4 derive quasioptimal policy ˆμ Note ˆV μ Eq 46 extension ˆV Eq 21 sense ˆV μ value estimated max upperbounded Vmax Rmax γ Rmax γ 2Rmax Rmax1 γ 470 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 ˆμ max Eq 45 possiblycorrect beliefs policy ˆV value quasioptimal policy ˆμ Thus ˆV ˆμ identical ˆV Since algorithm calculates ˆV easy obtain ˆV max upperbounded larger value ˆV Last ˆV μ ˆμ max Otherwise ˆV μ upperbounded ˆV calculated exactly manner ˆV optimal policy ˆμ max tighter bound Vmax If Rs scid6 nonnegative s scid6 ˆV μ max ˆμ max ˆμ max reward function R replaced R quasi maximizes estimated total negative reward9 max max sufﬁciently small bound looser default bound 2Rmax1 γ applied cid10O policy10 Given presence 1 γ 2 factor bound limited practical use problems γ close 1 Tighter bounds remain derived future studies We study reward loss empirically Section 6 Thus theoretical bound easily calculated Unfortunately tight bound Unless γ cid10T optimized modiﬁed problem ˆμ ˆμ max ˆV ˆV ˆμ max 52 Computational complexity provided algorithm Here summarize computational costs algorithm Section 4 incurs solving quasioptimal policy First FINDASMALLBELIEFSET procedure Section 41 uses ISFEASIBLE procedure discussed Section 42 terminates ﬁnite ˆB moderate conditions Suppose example parameter nonzero imprecision parameter lower bound Eqs 3033 ﬁrstorder belief ˆb Bayesupdated Eq 29 equal upper bound Let consider set Δcid6 ˆb choosing set averaged model functions ˆT ˆO arbitrarily convexity conditions Note Δcid6 ˆb subset probability simplex ΔS Let bc ﬁrstorder belief added desired averaged model functions ISFEASIBLE procedure Section 42 Since parameter nonzero imprecision place bc inside Δcid6 positive distance boundary guarantee ˆb set Δcid6 ˆb includes nonempty ball cid11 b cid10b bccid10 cid2 cid10r b ΔS cid12 K cid6 Δ ˆb 47 cid10r 0 radius ball cid10 cid10 denotes L1 norm Thus FINDASMALLBELIEF SET procedure adds new ﬁrstorder belief belief distance cid10r ﬁrstorder beliefs Since ﬁrstorder belief simplex ΔS procedure continue adding new ﬁrstorder beliefs inﬁnite number times It proved procedure terminates ﬁnite number ﬁrstorder beliefs The condition parameter nonzero imprecision relaxed long nonempty ball guaranteed exist As evident discussion number ﬁrstorder beliefs ˆB worst case increase exponentially S As described ˆB important factor computational complexity provided algorithm However currently theoretical results concerning ˆB We instead provide empirical results section example ˆB increase exponentially S Next quantify computational complexity present algorithm We use notation O indicate order complexity First FINDASMALLBELIEFSET procedure need worst case O ˆB2Acid10Θ iterations ISFEASIBLE procedure Second noted Section 42 ISFEASIBLE proce dure conducted efﬁciently interval case complexity OpolyS polynomial order S In pointset case OpolySNT NT maxsSaA T M s Third construct belief state MDP Eqs 39 40 require O ˆBA2Θ2S2 time Last solve beliefstate MDP Eqs 37 38 value iteration method need O ˆBAΘ time iteration 9 The bound derived noting ˆV μ additionally ˆV μ deﬁne ˆV μ 10 This default bound obtained observing policy miss 2Rmax reward time step Calculating discounted sum inﬁnite horizon gives bound 2Rmax 2Rmaxγ 2Rmaxγ 2 2Rmax1 γ h cid8a ocid9 cid2 max ˆV μ μ manner ˆV μ replace reward function R R h cid8a ocid9 holds h o The bound derived noting h cid8a ocid9 hold h o h cid8a ocid9 ˆV μ h cid8a ocid9 ˆV μ h cid8a ocid9 cid2 ˆV ˆμ h cid8a ocid9 cid2 ˆV ˆμ H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 471 We small modiﬁcation algorithm reduces computational cost Combin ing complexity FINDASMALLBELIEFSET procedure ISFEASIBLE procedure time ISFEASIBLE procedure requires O ˆB2AΘpolyS interval case O ˆB2AΘpolySNT pointset case These changed O ˆBAΘpolyS ˆBS O ˆBAΘpolySNT ˆBS respectively following modiﬁcation Recall FINDA SMALLBELIEFSET procedure given ˆbh A o Θ search candidates ﬁrstorder beliefs calculated seeking τ ˆbh o ˆbM hao identical However cases searching candidates avoided search small number candidates likely include required belief To ﬁrst calculate tentatively Bayesupdated belief ˆbcid6cid6 τ ˆbh o ˆbM hao satisﬁes permissibility condition Then pick knearest neighbors ˆbcid6cid6 candidate beliefs search Note modiﬁcation ﬁnd quasioptimal policy Although modiﬁcation increase resultant size ˆB empirically signiﬁcantly reduce total computational time In section adopt technique k 5 L1 norm measuring distance ﬁrstorder beliefs Further note modiﬁcation FINDASMALLBELIEFSET procedure Section 41 terminates ﬁnite ˆB moderate conditions This proved exactly manner beginning section let ˆbcid6cid6 bc use ˆbcid6cid6 ﬁrstorder belief added search fails hao arbitrary ˆbM There modiﬁcation algorithm order reduce computational cost When number states S large takes long time solve linear programming problem ISFEASIBLE pro cedure Recall takes OpolyS time interval case OpolySNT time pointset case We modify procedure search values ˆO The transition function ˆT set arbitrarily convexity conditions With ˆT ﬁxed procedure Fig 6 requires OS2 time interval case pointset case This modiﬁcation reduced solution time S large We use modiﬁcation Sections 62 63 Note modiﬁcation restricts search space ISFEASIBLE procedure number required ﬁrstorder beliefs increase However modiﬁcation reduced signiﬁcantly time spent search consequently total solution time reduced Note modiﬁcation FINDASMALLBELIEFSET procedure Section 41 terminates ﬁnite ˆB moderate conditions For example parameter ˆO nonzero imprecision nonempty ball K Eq 47 guaranteed exist procedure terminates ﬁnite ˆB 6 Experiments We applied algorithm POMDPIPs We report results interval case Those pointset case expected similar All results obtained Matlab codes Pentium4 PC11 61 Smallsized POMDPIPs Here smallsized POMDPs optimal policies known 112457 First POMDP Table 1 parameter cid10 00125 0025 005 01 02 04 06 controls imprecision constructed POMDPIP follows We set T M T M s scid6 T s scid6 cid10 T s scid6 cid10 s scid6 S A T s scid6 parameter value original POMDP Similarly set OM scid6 o Oscid6 o cid10 Oscid6 o cid10 scid6 S A o Θ Oscid6 o original parameter value We bounds saturated 0 1 exceeded range 0 1 We calculated maximum imprecision cid10T max Eqs 41 42 let cid10max maximum max cid10O Second applied algorithm POMDPIPs obtained quasioptimal policies Since algorithm requires arbitrary averaged model functions ˆT ˆO calculated typical model functions necessary To typical model functions ﬁrst calculated middle points scid6 ˆT s scid6 1 upper lower bounds Since points break sumto1 constraints cid3 11 The codes freely available httpwwwbrndistitechacjphideakipomdpipsindexhtm 472 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Table 1 Smallsized POMDPs known optimal policies Test problem Tiger 1D maze 4 3 4 3 CO Cheese Part painting Network Shuttle Aircraft ID S 2 4 11 11 11 4 7 8 12 A 3 2 4 4 4 4 4 3 6 Θ 2 2 6 11 7 2 2 5 5 Fig 7 Smallsized POMDPIPs reward losses quasioptimal policies evaluated original POMDPs picked nearest points measured Euclidean norm violate constraints sumto1 constraints convexity conditions Then studied quasioptimal policies obtained nontrivial Fig 7 For POMDPIP problem assumed original POMDP true environment hypothetical POMDP Section 32 We value obtained quasioptimal policy original POMDP We evaluated V μ evaluated V ˆμ value truly optimal policy original POMDP The difference V μ V ˆμ indicates reward loss occurred quasioptimal policy instead identifying true environment performing strict optimization Fig 7 shows reward losses normalized V μ For problems reward losses small cid10max small This indicates quasioptimal policies nontrivial These policies admissible solutions POMDPIPs sense optimal possibly true environment Furthermore studied robustness obtained policies Fig 8 In study regard policy robust reward loss V μ V ˆμ small true environments For POMDPIP problem randomly generated hypothetical POMDPs For hypothetical POMDPs calculated reward loss way Fig 7 Then calculated largest reward loss environments The results Fig 8 suggest problems reward loss kept small long cid10max range However Aircraft ID Tiger relatively large reward losses observed In problems agent receives huge negative reward taking action state To avoid huge negative reward ﬁrstorder belief belief state needs inferred precisely For problems quasioptimal policies good solution need precise probabilities strict optimization need policies maximin We compared robustness quasioptimal policies Eadmissible policies Note quasioptimal policies robust Eadmissible policies Section 33 Appendix A For POMPDIP problem obtained kinds Eadmissible policies 1 optimal policy original POMDP problem 2 optimal policy POMDP consists typical model functions 3 optimal H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 473 Fig 8 Smallsized POMDPIPs largest reward losses quasioptimal policies evaluated randomlygenerated hypothetical POMDPs See Fig 7 legend Fig 9 Smallsized POMDPIPs solution time left ˆB right See Fig 7 legend policy randomlygenerated hypothetical POMDP For Eadmissible policies calculated largest reward loss Fig 8 Unfortunately results shown signiﬁcantly different quasioptimal policies Fig 8 Among kinds Eadmissible policies tested ﬁrst ones optimal policies original POMDPs robust largest reward loss smallest problems Thus compared Eadmissible policies quasioptimal policies Out 63 POMDPIP problems 9 original POMDP problems times 7 values cid10max Fig 8 Eadmissible policies robust quasioptimal policies 29 problems In 34 problems quasioptimal policies robust Thus quasioptimal policies tended robust However difference subtle research required detailed comparisons Fig 9 left shows cpu time required obtain quasioptimal policies Our algorithm terminated rea sonably short time cases For example Aircraft ID problem shortest solution time reported 27676 seconds 57 Although direct comparison impossible algorithm required 5152 seconds cid10max 005 467 seconds 01 degraded value For Network problem shortest solution time 140 seconds 57 algorithm required 35 seconds cid10max 005 11 seconds 01 In problems algorithm recorded longer solution times reported algorithms The number beliefs ˆB plotted Fig 9 right It noted plots ˆB appear similar cpu time This result agreement expectations algorithms complexity Section 52 We note ˆB increases cid10max smaller All plots ﬁgure appear approximated 474 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 straight line Since ﬁgure loglog scale plot conclude ˆB approximately proportional poly1cid10max The slopes plots Fig 9 clearly depend problem solved The slopes plots Aircraft ID Part painting problems large smaller Cheese 4 3 CO problems This indicates ˆB depend directly size problem S A Θ Rather appears slopes tended smaller problems Θ large compared S This result agreement expectations Θ large o Θ agent tends able obtain relatively large information state s ﬁrstorder beliefs ˆbh ˆB located rims probability simplex ˆbhs cid13 1 s S small number beliefs required FINDASMALLBELIEFSET procedure Section 41 62 POMDPIPs nearly observations Next study largersized problems As suggested previous section algorithm expected quickly solve POMDPIPs agents ﬁrstorder beliefs located rims probability simplex As example consider problems agent perform lownoise observations states First constructed POMDPs They mazetype environments n states actions agents state observed n kinds observations S Θ n A 4 constructed mazes n 320 640 1280 These POMDPs simple models navigation problem robot complete capability moving sensing capability close condition The agents action changes probability 09 current state agreement intention However probability 01 agent stays state moves unintended state selected uniformly randomly Each observation corresponds state onetoone mapping usually probability 09 agent observes current state correctly However probability 01 observes false state selected uniformly randomly The initial state distributed uniformly states There single goal state A reward 1 given reaching goal state reward We set γ 095 Having constructed POMDPs POMDPIPs POMDPs manner described Sec tion 61 Then obtained solutions We modiﬁed algorithm ISFEASIBLE procedure searches values ˆO Section 52 details We set transition function ˆT typical model function deﬁned Section 61 instead setting linear programming The results shown Figs 1012 The formats Figs 79 Figs 10 problem calculated approximately Perseus algorithm 11 value truly optimal policy V μ 5253 solve large problems exactly Fig 10 POMDPIPs nearly observations reward losses quasioptimal policies evaluated original POMDPs H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 475 Fig 11 POMDPIPs nearly observations largest reward losses quasioptimal policies evaluated randomlygenerated hypothetical POMDPs See Fig 10 legend Fig 12 POMDPIPs nearly observations solution time left ˆB right See Fig 10 legend The quasioptimal policies obtained successfully POMDPIPs cases S Θ 1240 signiﬁcant reduction performance The policies obtained suggested nontrivial Fig 10 robust Fig 11 parameter imprecision small As expected algorithm terminated reasonable time Fig 12 left small number ﬁrstorder beliefs Fig 12 right In problems number required ﬁrstorder beliefs 4 times larger S 63 Problems solved Although algorithm able solve largesized problems S 1240 Section 62 failed solve problems reasonable time We report problems section Further research required solve problems We tried solve TagAvoid problem 44 Cycle10 problem 3leg10 problem 4547 Table 2 For POMDP problems created POMDPIPs manner described Section 61 cid10 00125 01 04 06 08 In POMDPIP problem applied algorithm Section 62 searches values parameters ˆO The algorithm Section 61 searches values parameters ˆT ˆO applicable running memory For POMDPIP problem allowed algorithm run 48 hours For cid10 00125 01 04 06 problems solved time limit Many ﬁrstorder beliefs generated Table 2 beliefs generated For cid10 large 08 algorithm terminated generating tractable number 476 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Table 2 Problems algorithm terminate terminated failed ﬁnd satisfactory solution Test problem S A Θ beliefs 104 generated 48 h cid10 00125 01 TagAvoid Cycle10 3leg10 Shown time elapsed s normalized reward loss policy 870 1280 1280 32 29 29 30 2 2 5 21 21 31 28 28 04 20 28 28 06 14 14 16 beliefs 100 terminateda 08 439 759 21 33 23 041 127 76 042 beliefs Table 2 problem However policies unsatisfactory normalized reward loss calculated manner Fig 10 041 larger 7 Conclusion In paper formulated POMDPIPs quasioptimal policies provided efﬁcient algorithm obtain policies We provided theoretical bound reward losses quasioptimal policies computational complexity algorithm Empirical studies showed algorithm ﬁnd nontrivial policies reasonable time POMDPIPs There directions future research First characteristics quasioptimal policy studied deeply For example derivation tighter theoretical bounds investigated Also detailed comparisons quasioptimal policies E admissible policies remain performed Section 61 Second robustness policy pursued In paper attempt obtain robust policies possible For problems quasioptimal policies relatively robust Section 61 It desirable obtain robust policies maximin approach reasonable time Recently Nilim El Ghaoui proposed handling imprecision likelihoodbounded sets opposed intervals order efﬁciently obtain robust policies MDPs 3940 It interesting investigate POMDPs Third types solution algorithm considered Although algorithm solved POMDPIPs failed Section 63 solve problems recent POMDP algorithms able ﬁnd good approximate policies 434547 Our algorithm directly constructs gridbased belief state MDPs It suffer exponential growth number ﬁrstorder beliefs gridbased approaches 72859 Other ap proaches based αvectors 24444647515357 offer reduced solution times classes POMDPIPs algorithm slow Last note algorithm approximate planning method largesized POMDPs Even parameters given precisely choose introduce parameter imprecision balancing gains reduction solution time reduction accuracy The empirical results Section 6 encouraging suggest satisfactory policies obtained certain range parameter imprecision Note idea parameter imprecision exploited reduce computational complexity solving POMDPs orthogonal approximation methods reviewed Section 1 combination idea approximation methods pursued Acknowledgements We thank Minseok Kim Shigemi Sawa reviewers valuable comments We thank Anthony R Cassandra Joelle Pineau making codes POMDP problems available We thank Matthijs Spaan making code Perseus algorithm available This work supported GrantinAid Young Scientists B15700180 Ministry Education Culture Sports Science Technology Japan H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 477 Appendix A An example multiple secondorder beliefs leads robust policy single secondorder belief Let consider simple POMDPIP problem Suppose states S s1 s2 s3 s4 actions A a1 a2 observations Θ o1 o2 Suppose p0 1 0 0 0 process starts s1 Let consider pointset case Let T M T M s 0 0 0 1 s T M s1 a1 0 04 06 0 0 06 04 0 That state changes s4 matter action agent takes changes s2 s3 imprecise probability agent takes action a1 state s1 Let OM OM scid6 05 05 scid6 state completely unobservable Let R Rs scid6 0 s scid6 Rs scid6 1 s s2 a1 scid6 s4 Rs scid6 1 s s3 a2 scid6 s4 That reward gained agent takes action a1 state s2 agent takes action a2 state s3 For POMDPIP problem let consider history tree shown Fig 4 The possiblycorrect ﬁrstorder beliefs 1 ˆb p0 1 0 0 0 2 ˆb11 belief taking action a1 observing o1 equals convex combination 0 04 06 0 0 06 04 0 3 ˆb12 equals convex combination 0 04 06 0 0 06 04 0 4 0 0 0 1 Note ˆb11 ˆb12 calculated depending secondorder beliefs ˆbM a1o2 respectively Thus single secondorder belief allowed ˆb11 ˆb12 identical If multiple secondorder beliefs allowed ˆb11 ˆb12 different a1o1 ˆbM Now let consider action taken actionobservation history Since reward gained state s2 s3 initial action action history a1 For reason actions actions actions ˆb1111 ˆb1112 affect total reward Thus actions optimized immediately ˆb11 ˆb12 Let consider action ˆb11 Recall unit reward gained action a1 state s2 action a2 state s3 The estimated rewards actions a1 a2 depend value ˆb11 shown Fig A1 For example ˆb11s2 04 means ˆb11 0 04 06 0 estimated reward 04 action a1 06 action a2 action a2 taken The holds action ˆb12 Thus single secondorder belief allowed ˆb11 ˆb12 identical actions ˆb11 ˆb12 identical Let suppose example ˆb11s2 ˆb11s3 04 Then action a2 selected ˆb11 ˆb12 On hand multiple secondorder beliefs allowed ˆb11 ˆb12 different actions ˆb11 ˆb12 different Let suppose example agent adopts ˆb11s2 04 ˆb12s2 06 Then action a2 selected ˆb11 action a1 selected ˆb12 Since ˆb11 ˆb12 reached probability 05 agent selects action a1 a2 probability 05 Now suppose correct secondorder beliefs given Section 32 Depending values expected total reward varies 04γ 06γ agent selects a2 γ discount factor On hand expected reward 05γ agent selects a1 a2 probability 05 Fig A1 Reward expected taking action a1 a2 ˆb11 478 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 Thus agent adopts single secondorder belief gains reward agent multiple beliefs belief adopted correct worst case performance better agent multiple beliefs In sense use multiple beliefs lead robust policy Note parts paper use term robust slightly different meaning regard policy robust reward loss incurred policy instead optimal policy environment small environments Section 6 The conclusion Appendix A changed adopting meaning maximum reward loss 02γ agent selects a2 01γ agent selects a1 a2 probability 05 Note argue use multiple beliefs better Note algorithm try ﬁnd policy robust possible The example section suggests use randomized policies stochastic actionselection rules robustness Finding robust policies scope present paper The purpose introducing simple example advantageous use single secondorder belief compared use multiple secondorder beliefs Appendix B Derivation ISFEASIBLE procedure First consider interval case In ISFEASIBLE procedure query want answer true false formalized follows Q1 There exists ˆT s scid6 R s scid6 S ˆOscid6 o R scid6 S Z R st Z 0 cid5 Z sscid6S ˆbs ˆT s s cid6 ˆOs cid6 o cid6 cid2 ˆT s s cid6 cid2 T s s cid6 s s cid6 S T s s cid5 ˆT s s cid6 1 s S scid6S Os cid5 cid6 cid6 o cid2 ˆOs ˆbs ˆT s s cid6 o cid2 Os ˆOs oZ ˆb cid6 cid6 o s cid6 cid6 s s cid6 S cid6 S B1 B2 B3 B4 B5 B6 sS hold There nonlinear equations query In following transform query equivalent query consists linear equations To begin Q1 Eq B2 unnecessary Eqs B1 B6 cid3 scid6S ˆbcid6scid6 1 holds construction Consequently query Q1 equivalent Q2 There exists ˆT s scid6 R s scid6 S ˆOscid6 o R scid6 S Z R st Eqs B1 B3B6 hold Q1 equivalent Q2 Q1 true false Q2 true false respectively Next let divide S mutually exclusive sets S1 S2 S3 S4 cid6 o 0 s cid6 S cid12 cid11 s cid11 s cid11 s cid11 s cid6 cid6 cid6 cid6 cid6 ˆb cid6 ˆb cid6 ˆb cid6 ˆb s s s s cid6 cid6 cid6 cid6 S1 S2 S3 S4 cid11 0 Os cid6 cid11 0 s o 0 Os cid12 cid6 S S1 cid6 0 Os o 0 s 0 Os cid6 o cid11 0 s cid6 S cid6 S cid12 cid12 Note sets S1 S2 S3 S4 form partition S It straightforward Q2 equivalent Q3 There exists ˆT s scid6 R s scid6 S ˆOscid6 o R scid6 S Z R st Eqs B1 B3 B4 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 ˆOs cid5 cid6 o 0 s ˆbs ˆT s s ˆOs cid6 S1 oZ ˆb cid6 cid6 sS cid6 cid6 s cid11 0 s cid6 S1 Os cid5 cid6 cid6 o cid2 ˆOs ˆbs ˆT s s cid6 o cid2 Os ˆOs oZ ˆb cid6 cid6 o cid11 0 s cid6 cid6 s s cid6 S2 cid6 S2 sS 0 cid2 ˆOs cid5 cid6 o cid2 Os ˆOs ˆbs ˆT s s cid6 cid6 o s cid6 S3 cid6 oZ 0 s cid6 S3 sS sS cid6 0 Os cid5 o cid2 ˆOs ˆOs cid6 ˆbs ˆT s s cid6 o cid2 Os cid6 o cid6 oZ 0 s cid6 S4 s cid6 S4 479 B7 B8 B9 B10 B11 B12 B13 B14 hold Note Eq B9 Oscid6 o cid11 0 Oscid6 o 0 0 cid2 Oscid6 o cid2 Oscid6 o concluded Oscid6 o Oscid6 o 0 means scid6 S1 S2 Next prove Q3 equivalent Q4 S1 set exists ˆT s scid6 R s scid6 S ˆOscid6 o R scid6 S Z R st cid6 Eqs B1 B3 B4 o cid2 ˆOs ˆbs ˆT s s cid6 Os cid5 o cid2 Os ˆOs cid6 cid6 cid6 o cid11 0 s oZ ˆb cid6 cid6 s s cid6 S2 cid6 S2 sS ˆOs cid5 cid6 o 0 s ˆbs ˆT s s cid6 S2 0 s cid6 cid6 S4 sS hold To prove Q3 equivalent Q4 check Q4 true Q3 true Q3 true Q4 true To check note followings For scid6 S1 Eqs B7 B8 satisﬁed For scid6 S2 Eqs B9 ˆOscid6 o 0 holds For scid6 S4 Eqs B13 B14 satisﬁed B10 satisﬁed cid3 ˆbs ˆT s scid6 0 sS To check note followings For scid6 S3 Eqs B11 B12 satisﬁed setting cid6 ˆOs o 0 possible deﬁnition S3 For scid6 S4 Eq B13 satisﬁed setting cid6 ˆOs o arbitrarily st Os cid6 o cid2 ˆOs cid6 o cid2 Os cid6 o B15 B16 possible deﬁnition Next let deﬁne qscid6 Z ˆOscid6 o scid6 S2 Now Q4 equivalent Q5 S1 set exists ˆT s scid6 R s scid6 S qscid6 R scid6 S2 Z R st Eqs B1 B3 B4 Z Oscid6 o cid3 qs cid6 cid3 Z Oscid6 o s cid6 S2 B17 480 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 cid5 sS ˆbs ˆT s s cid6 ˆb cid6 s cid6 qs cid6 s cid6 S2 cid6 qs cid5 s ˆbs ˆT s s cid6 S2 0 s cid6 cid6 S4 sS B18 B19 B20 Z Oscid6ao Oscid6 o 0 Eq B17 Note Z Oscid6ao cid11 Oscid6 o cid11 0 hold deﬁne scid6 S2 In Q5 Eq B19 unnecessary Eq B18 lefthand ﬁnite ˆbcid6scid6 cid11 0 deﬁnition S2 Thus ﬁnally Q5 equivalent Q6 S1 set exists ˆT s scid6 R s scid6 S qscid6 R scid6 S2 Z R st Eqs B1 B3 B4 B17 B18 B20 hold Therefore S1 S2 S4 case obtain ISFEASIBLE procedure Fig 6 Otherwise need little negligible computational cost answer Q6 correctly need test S1 include Eq B20 constraint We need use Eqs B15 B16 set ˆOscid6 o scid6 S3 S4 respectively Still note easily answer Q6 constraints linear We consider pointset case similar manner The query want answer true false formalized follows Q7 There exists ˆT s scid6 R s scid6 S ˆOscid6 o R scid6 S λi s scid6 0 1 scid6 S 1 OM scid6 Z R st 1 T M s νi Z 0 cid5 Z ˆbs ˆT s s cid6 ˆOs cid6 o sscid6S T s s cid6 cid5 sT M λi s s cid6 s s cid6 S λi s 1 s S Os cid6 o cid5 scid6OM νi s cid6 o s cid6 S νi scid6 1 s cid6 S ˆbs ˆT s s cid6 ˆOs cid6 oZ ˆb cid6 cid6 s s cid6 S cid5 cid5 cid5 sS 0 1 s S B21 B22 hold The constraints Eqs B21 B22 νi scid6 0 1 equivalent Eq B5 note o ﬁxed After replacing Eq B5 obtain ﬁnal form Fig 6 way interval case described Appendix C Proof Theorem 1 Here provide proof Theorem 1 This proof based McAllester et al 37 The proof strategy follows Note Theorem 1 gives bound error optimal value occur possiblycorrect secondorder beliefs instead correct ones To prove Theorem 1 bound errors occur possiblycorrect secondorder beliefs instead correct ones After proving basic properties norm probability functions Lemmas 14 ﬁrst bound error ﬁrstorder belief action observation Lemma 5 Next Lemma 5 bound error ﬁrstorder belief H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 481 t steps Lemma 6 Subsequently Lemma 6 bound error value policy Lemma 7 Finally Lemma 7 bound error optimal value prove Theorem 1 We begin providing basic lemmas Let cid10 cid10 denote L1 norm probability function P s let cid10P scid10 P s For norm following lemmas cid3 s Lemma 1 Let P Q probability functions set We cid17 cid17 cid17 cid2 2 cid17P s Qs Proof cid17 cid17 cid17 cid17P s Qs cid5 cid16 cid16 cid16 cid2 cid16P s Qs cid5 P s s s cid5 s Qs 2 cid2 Lemma 2 Modiﬁed Lemma 15 37 Let X O sets Let P Q probability functions X O Let P o denote marginal probability function O P o x P x o similarly Qo Let P xo conditional probability function X P xo P x oP o P o cid11 0 Let P xo arbitrary probability function P o 0 Similarly Qxo We following cid17 cid17 cid17 cid17P x o Qx o cid17 cid17 cid17 cid2 cid17P xo Qxo cid17 cid17 cid17 cid17P o Qo cid3 EoP o Proof Let O set o P o cid11 0 holds First prove o O cid16 cid16 cid16 cid16P o Qo C1 P o cid5 xX Qx o P o cid16 cid16 cid16 cid16 Qxo When Qo cid11 0 Eq C1 holds P o cid5 xX Qx o P o cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 Qx o Qo cid16 cid16 cid16 cid16 cid5 Qx o P o cid16 cid16 cid16 cid16 P o Qxo xX cid16 cid16 1 cid16 P o cid16 Qo P o cid16 cid16 cid16 cid16P o Qo 1 cid16 cid16 cid16 cid16 cid5 xX cid16 cid16 cid16 Qx o P o cid16 1 P o 1 Qo cid16 cid16 cid16 cid16Qo When Qo 0 Eq C1 holds cid16 cid16 cid16 cid16 P o cid5 xX Qx o P o cid16 cid16 cid16 cid16 P o Qxo cid5 cid16 cid16 cid16 cid16Qxo Qx o 0 Qo 0 xX cid16 cid16 cid16 cid16P o Qo Qo 0 P o Therefore Eq C1 holds o O Next prove lemma follows cid5 cid17 cid17 cid17 cid17P xo Qxo EoP o P o cid5 oO cid5 xX cid5 cid16 cid16 cid16 cid16P x oP o Qxo cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid5 cid5 cid5 cid16 cid16 cid16 cid16 P o cid2 cid2 cid5 oO P o Qx o P x o P o P o xX cid16 cid16 cid16 cid16P x o Qx o oOxX cid17 cid17 cid17 cid17P x o Qx o oO xX cid16 cid16 cid16 cid16P o Qo oO cid17 cid17 cid17 cid17P o Qo cid2 cid16 cid16 cid16 Qxo cid16 Qx o P o Eq C1 Next let consider hypothetical POMDP correct secondorder beliefs speciﬁed Section 32 For later use let Prah μ denote probability taking action A given history h H policy μ Deﬁne 482 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 cid3 P scid6 hcid6h s t μ probability actionobservation sequence hcid6 generated ﬁnal state scid6 given time t elapsed history h ended state s policy μ For ﬁrstorder belief b deﬁne P scid6 hcid6h b t μ sS bsP scid6 hcid6h s t μ Note hcid6 actionobservation sequence length t denote hcid6 Ht little abuse notation cid3 Let deﬁne P hcid6h b t μ P hcid6h b t μ scid6S P scid6 hcid6h b t μ Also deﬁne P scid6hcid6 h b t μ P scid6hcid6 h b t μ P scid6 hcid6h b t μP hcid6h b t μ P hcid6h b t μ cid11 0 If P hcid6h b t μ 0 let P scid6hcid6 h b t μ arbitrary probability function S We denote P scid6hcid6 h b t μ P scid6hcid6 h b short t μ redundant given hcid6 h b For functions following bounds Lemma 3 Let b ˆb ﬁrstorder beliefs t elapsed time μ policy h history Then cid17 cid17P h cid17 cid17 cid2 cid10b ˆbcid10 cid6h ˆb t μ cid6h b t μ P h Proof cid17 cid17P h cid6h b t μ P h cid17 cid17 cid6h ˆb t μ cid5 cid5 cid16 cid16 cid16 cid16 P h cid6h s t μbs cid5 cid16 cid16 cid16 cid6h s t μ ˆbs cid16 P h hcid6Ht cid5 sS cid5 sS cid16 cid16 cid16 cid10b ˆbcid10 cid16bs ˆbs cid6h s t μ P h cid2 cid2 hcid6Ht sS Lemma 4 Let b ˆb ﬁrstorder beliefs t elapsed time μ policy h history Then cid5 cid17 cid17P s cid6h b t μ P h cid6 cid6h h b P s cid6 cid6h cid17 cid17 cid2 2cid10b ˆbcid10 h ˆb hcid6Ht Proof cid5 cid17 cid17P s cid6h b t μ P h cid6 cid6h h b P s cid6 cid6h cid17 cid17 h ˆb hcid6Ht cid2 cid6 cid17 cid17P s h cid17 cid17P h cid2 cid10b ˆbcid10 cid6 cid6h b t μ P s cid6h b t μ P h cid17 cid17 cid6h ˆb t μ h cid17 cid17 Lemma 2 cid6h ˆb t μ proof similar Lemma 3 cid10b ˆbcid10 2cid10b ˆbcid10 Lemma 3 cid2 Next prove bounds errors possiblycorrect ﬁrstorder beliefs Section 33 compared correct beliefs hypothetical POMDPs In following suppose possiblycorrect secondorder beliefs ˆbM hao ˆbM h determined h H A o Θ First provide bound error caused onestep update ﬁrstorder belief Lemma 5 Let b ﬁrstorder belief Let h history Let bcid6 ˆbcid6 correct possiblycorrect beliefs Bayesupdated b bcid6 τ b o bM hao respectively Let bcid6 arbitrary probability function S τ b o bM h calculated denominator zero Eq 14 Similarly ˆbcid6 Deﬁne d Eq 43 Then h ˆbcid6 τ b o ˆbM cid5 cid8 cid8a ocid9h b 1 μ cid9 cid10b P cid6 ˆb cid6cid10 cid2 4d aAoΘ H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 483 Proof First deﬁne Qscid6 oh b Qoh b cid5 cid13 cid6 oh b Qs Qoh b Then mhThOhM cid5 scid6S cid6 Qs oh b cid6 Ohs o Ths s sS cid6 bsbM h mh dmh cid6 b s Qscid6 oh b Qoh b s S Qoh b cid11 0 Similarly deﬁne ˆQscid6 oh b ˆQoh b cid13 cid6 ˆQs oh b mhThOhM cid6 Ohs o cid5 sS Ths s cid6 bs ˆbM haomh dmh ˆQoh b cid5 scid6S cid6 ˆQs oh b Then ˆb cid6 ˆQscid6 oh b ˆQoh b s S ˆQoh b cid11 0 Next deﬁne differences cid6 cid6 cid6 oh b ˆQs oh b cid10s oh b Qs C2 C3 cid10oh b Qoh b ˆQoh b Note cid5 aA oΘ cid8 cid8a ocid9h b 1 μ cid9 cid10b P cid6 ˆb cid6cid10 cid5 aA Prah μ cid5 oΘ Qoh b acid10b cid6 ˆb cid6cid10 C4 holds For h H ﬁrstorder belief b A o Θ satisﬁes Qoh b cid2 2cid10oh b Qoh b acid10b cid6 ˆb cid6cid10 cid2 4 cid2 4 cid16 cid16 cid16 cid16cid10oh b cid5 cid16 cid16cid10s cid6 cid16 cid16 oh b Lemma 1 assumption C5 scid6S If o Θ satisﬁes Qoh b 2cid10oh b Qoh b cid11 0 ˆQoh b cid11 0 hold proof contradiction Qoh b acid10b cid6 ˆb cid6cid10 Qoh b Qoh b cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid16 cid5 scid6S cid5 scid6S Qscid6 oh b Qoh b Qscid6 oh b Qoh b ˆQscid6 oh b ˆQoh b cid16 cid16 cid16 cid16 Qscid6 oh b cid10scid6 oh b Qoh b cid10oh b cid16 cid16 cid16 cid16 Eqs C2 C3 cid16 cid16 cid16 cid16 cid5 scid6S Qscid6 oh b acid10oh b Qoh b acid10scid6 oh b Qoh b cid10oh b cid16 cid16 cid16 cid16 484 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 cid5 scid6S cid5 2 2 cid16 cid16 cid16 cid16 Qscid6 oh b acid10oh b Qoh b acid10scid6 oh b Qoh b cid16 cid16 oh b cid10oh b cid10s s cid6 cid6 cid6 cid16 cid16b cid16 cid16 cid16 cid16 assumption scid6S cid16 cid16 cid16 2 cid16cid10oh b cid2 2 cid16 cid16cid10s cid6 cid16 cid16 cid2 4 oh b cid5 scid6S cid16 cid16cid10s cid6 cid16 cid16 oh b cid5 scid6S Thus Eq C5 cid6 ˆb Qoh b acid10b cid6cid10 cid2 4 cid16 cid16cid10s cid5 cid6 cid16 cid16 oh b h H ﬁrstorder belief b A o Θ scid6S Note cid5 cid5 cid16 cid16cid10s cid6 oΘ scid6S cid5 cid5 scid6S oΘ cid13 cid16 cid16 oh b cid16 cid16 cid16 cid16 mhThOhM cid13 cid5 cid5 cid16 cid16Qs oΘ scid6S cid6 Ohs o cid5 sS cid6 oh b ˆQs cid6 cid16 cid16 oh b Ths s cid6 bsbM h mh dmh cid6 Ohs o cid5 sS Ths s cid6 bs ˆbM haomh dmh cid16 cid16 cid16 cid16 mhThOhM cid5 cid5 cid5 bs max T 1O1T 2O2M oΘ cid5 scid6S cid5 sS cid5 cid16 cid16O1s cid6 oT 1s s cid6 O2s cid6 oT 2s s cid6 cid16 cid16 cid16 cid16 cid8 O0s cid6 o cid101 O s cid6 cid9cid8 o T 0s s bs cid8 max cid9 T 0cid102 cid9cid8 T 0cid101 T O0cid101 O scid6S oΘ cid8 O0s sS cid6 o cid102 O M cid102 let T 0 O0 arbitrary model M cid16 cid16O0s T 0s s T O0cid102 o O s bs cid5 cid5 cid5 T s s cid6 cid6 cid6 cid9cid16 cid16 scid6S oΘ sS T 0s s cid5 cid5 cid5 T 0cid101 cid6 cid6 cid8 cid101 O s cid8 O0s bs cid9 M max cid8 T O0cid101 T 0cid102 O o cid102 ocid10T O s T O0cid102 O cid9 cid101 O s o cid6 cid10O T 0s s cid6 cid6 cid6 max cid6 cid8 cid101 T s s o cid6 cid102 T s s cid6 cid9 cid6 ocid101 cid10T T s s maxcid10O max cid102 O s maxcid10O max cid10T max cid6 ocid102 cid9 T s s cid2 cid2 scid6S oΘ Scid10T sS Θcid10O maxcid10O max The desired result obtained combining result Eqs C4 C6 cid2 2Scid10Θcid10T d max max C6 cid6 cid101 T s s cid6 cid9 cid6 cid16 cid16 C7 Next provide following error bound ﬁrstorder beliefs time t cid3 0 Let Eμ expectation tlength histories h Ht occurs probability P h p0 t μ hHt Lemma 6 Let bh correct belief hypothetical POMDP Let ˆbh possiblycorrect belief let ˆbh arbitrary probability function S calculated denominator zero Eq 14 For t cid3 0 Eμ cid10bh ˆbhcid10 cid2 16dt hHt Proof First deﬁne generalized possiblycorrect ﬁrstorder belief ˆP scid6hcid6 h b calculated Bayes updating b repeatedly possiblycorrect secondorder beliefs assuming b belief history h That let ˆP scid6 h b bscid6 hold scid6 S h H ﬁrstorder belief b Also ˆbcid6 τ b o ˆbM hao ˆP scid6cid8a ocid9 hcid6cid6 h b ˆP scid6hcid6cid6 h cid8a ocid9 ˆbcid6 hold scid6 S h hcid6cid6 H A o Θ let H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 485 cid8a ocid9 hcid6cid6 actionobservation history hcid6cid6 follows o If calculate τ b o ˆbM zero denominator Eq 14 let ˆP scid6cid8a ocid9 hcid6cid6 h b arbitrary probability function S hao Next note Eμ hHt cid10bh ˆbhcid10 cid5 hHt cid17 cid17P s P h p0 t μ cid6h p0 ˆP s cid17 cid17 cid6h p0 holds Thus need prove righthand equation larger 16dt We prove general equation cid5 cid17 cid17P s cid6h b t μ P h cid6 cid6h h b ˆP s cid6 cid6h h b cid17 cid17 cid2 16dt C8 hcid6Ht h H ﬁrstorder belief b t cid3 0 policy μ The proof induction t Eq C8 holds t 0 P scid6 h b ˆP scid6 h b bscid6 Assume Eq C8 holds t For t 1 cid5 cid17 cid17P s cid6h b t 1 μ P h cid6 cid6h h b ˆP s cid6 cid6h cid17 cid17 h b hcid6Ht1 cid5 cid8 cid8a ocid9h b 1 μ P cid9 cid5 cid8 cid6cid6h cid8a ocid9 b h cid6 P t μ cid9 cid8 hcid6cid6Ht aA oΘ cid17 cid8 cid17P cid6cid6 cid6h cid6cid6 cid6h s Divide hcid6 hcid6 cid8a ocid9 hcid6cid6 Deﬁne bcid6 ˆbcid6 Lemma 5 h cid8a ocid9 ˆb h cid8a ocid9 b ˆP cid9cid17 cid17 cid9 s cid6 cid6 cid8 cid8a ocid9h b 1 μ P cid9 cid5 cid8 cid6cid6h cid8a ocid9 b h P cid6 t μ cid9 cid5 cid8 aA oΘ cid17 cid17P cid6cid6 cid6h s cid5 h cid8a ocid9 b P cid9 cid6 cid8 P cid8a ocid9h b 1 μ hcid6cid6Ht cid8 cid6cid6 cid6h s cid9 cid5 cid9cid17 cid17 cid6 h cid8a ocid9 ˆb cid8 P cid6cid6h cid8a ocid9 b h cid9 cid6 t μ aA oΘ cid17 cid17P cid6cid6 cid6h cid8 s cid5 cid9 h cid8a ocid9 ˆb cid8 hcid6cid6Ht ˆP cid6h cid6cid6 cid9 cid6 ˆb cid10b cid8a ocid9h b 1 μ cid8 s cid6 P cid2 2 h cid8a ocid9 ˆb cid9cid17 cid17 cid6 cid6cid10 Lemma 4 aA oΘ cid5 cid8 P cid8a ocid9h b 1 μ cid9 aA oΘ cid5 cid16 cid8 cid16P cid6cid6h cid8a ocid9 b h cid6 cid9 cid8 cid6cid6h cid8a ocid9 ˆb h cid6 P t μ t μ cid9cid16 cid16 hcid6cid6Ht cid17 cid8 cid17P s cid5 cid6cid6 cid6h P cid9 cid6 ˆP h cid8a ocid9 ˆb cid8 cid8a ocid9h b 1 μ cid8 cid6cid6 cid6h s cid9 cid5 cid9cid17 cid17 cid6 h cid8a ocid9 ˆb cid8 cid6cid6h cid8a ocid9 ˆb h P cid9 cid6 t μ aA oΘ cid17 cid8 cid17P cid6cid6 cid6h s cid5 cid9 h cid8a ocid9 ˆb cid8 hcid6cid6Ht cid8 ˆP cid6h cid6cid6 s cid9 cid6 ˆb cid10b cid8a ocid9h b 1 μ cid6 P cid6cid10 cid2 2 h cid8a ocid9 ˆb cid9cid17 cid17 cid6 aA oΘ cid5 2 cid8 cid8a ocid9h b 1 μ cid9 cid10b P cid6 ˆb cid6cid10 Lemmas 1 3 aA oΘ cid5 cid8 P cid8a ocid9h b 1 μ cid9 cid5 cid8 P cid6cid6h cid8a ocid9 ˆb h cid6 t μ cid9 cid8 aA oΘ cid17 cid17P cid6cid6 cid6h s cid2 8d 8d cid6 h cid8a ocid9 ˆb Lemma 5 cid9 cid8 s ˆP hcid6cid6Ht cid6cid6 cid6h h cid8a ocid9 ˆb cid9cid17 cid17 cid6 486 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 16dt 16dt 1 assumption Thus Eq C8 holds t 1 This completes proof cid2 Next provide error bound values given policy For policy μ let V μh value history h hypothetical POMDP deﬁned solution cid7 cid5 cid5 V μh Prah μ ρh γ P oh aV μ cid10 cid9 cid8 h cid8a ocid9 aA oΘ ρh P oh deﬁned Eqs 17 18 respectively Similarly deﬁne quasivalue history h denote ˆV μh solution cid5 cid5 cid7 ˆV μh Prah μ ˆρh γ ˆP oh ˆV μ cid10 cid9 cid8 h cid8a ocid9 aA oΘ ˆρh ˆP oh deﬁned Eqs 23 24 respectively Let V μ V μ value policy μ ˆV μ ˆV μ quasivalue policy μ Let Rmax ˆV μ max deﬁned Eq 44 Eq 45 respectively For policy μ H A deﬁne W μ W μ 1 γ Scid10T max 16γ dRmax 1 15γ γ d ˆV μ max 1 γ 2 Then following Lemma 7 For policy μ V μ ˆV μ cid2 W μ Proof First prove cid16 cid16 cid16 cid2 γ Eμ cid16V μh ˆV μh Eμ hHt hcid6Ht1 cid16 cid16V μh cid6 cid16 cid16 ˆV μh cid6 maxt Scid10T 16dRmax γ ˆV μ maxRmax γ d ˆV μ max C9 To prove note cid16 cid16 cid16 cid16V μh ˆV μh cid5 Eμ hHt Eμ hHt cid5 cid16 cid16 cid16 cid16ρh γ Prah μ oΘ cid16 cid16 cid16 cid16ρh ˆρh Prah μ P oh aV μ cid9 cid8 h cid8a ocid9 ˆρh γ ˆP oh ˆV μ cid16 cid16 cid9 cid8 cid16 h cid8a ocid9 cid16 cid5 oΘ cid2 Eμ hHt aA cid5 aA γ Eμ hHt γ Eμ hHt cid5 aA cid2 Eμ hHt cid5 aA cid5 cid16 cid16 cid16 Prah μ cid16 cid16 cid16 cid16 Prah μ cid16 cid5 oΘ cid5 P oh aV μ cid9 cid8 h cid8a ocid9 P oh ˆV μ cid9 cid8 h cid8a ocid9 aA cid16 cid16 cid16 cid16ρh ˆρh Prah μ oΘ P oh ˆV μ ˆP oh ˆV μ cid16 cid16 cid9 cid8 cid16 h cid8a ocid9 cid16 cid16 cid16 cid9 cid8 cid16 h cid8a ocid9 cid16 cid5 oΘ cid5 oΘ cid16 cid16V μ P oh cid9 cid8 h cid8a ocid9 cid8 h cid8a ocid9 cid9cid16 cid16 γ Eμ hHt γ Eμ hHt cid5 aA cid5 aA cid5 Prah μ oΘ cid16 cid5 cid16 cid16 Prah μ cid16 oΘ P oh cid5 oΘ ˆV μ cid16 cid16 cid16 ˆP oh cid16 ˆV μ max H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 487 Eμ cid5 hHt cid16 cid16 cid16 cid16ρh ˆρh Prah μ aA Eμ hHt cid5 aA cid13 cid13 cid16 cid16 cid16 Prah μ cid16 mhThOhM cid5 cid5 cid5 cid5 sS scid6S Rs s Ths s cid6 Rs s cid6 Ths s cid6 bhs ˆbM h mh dmh cid6 ˆbhs ˆbM h mh dmh cid16 cid16 cid16 cid16 cid13 cid16 cid16 cid16 cid16bhs Ths s cid6 bM h mh dmh cid13 cid16 cid16 cid16 cid16bhs Ths s cid6 bM h mh dmh mhThOhM cid5 cid2 RmaxEμ hHt aA cid13 sS scid6S Prah μ cid5 cid5 sS scid6S ˆbhs Ths s cid6 ˆbM mhThOhM cid5 cid2 RmaxEμ hHt Prah μ aA cid13 cid5 cid5 sS scid6S ˆbhs Ths s cid6 mhThOhM RmaxEμ hHt Prah μ cid5 aA cid13 cid5 cid5 sS scid6S mhThOhM cid16 cid16 cid16 cid16 h mh dmh mhThOhM cid16 cid16 cid16 cid16 bM h mh dmh ˆbhs Ths s cid6 ˆbM h mh dmh mhThOhM cid16 cid16 cid16 cid16 cid13 cid16 cid16 cid16 cid16 ˆbhs Ths s cid6 bM h mh dmh mhThOhM cid2 RmaxEμ hHt cid10bh ˆbhcid10 RmaxEμ hHt cid13 Ths s cid6 ˆbM h mh dmh cid5 aA cid16 cid16 cid16 cid16 Prah μ cid5 sS ˆbhs cid5 scid6S cid13 cid16 cid16 cid16 cid16 mhThOhM Ths s cid6 bM h mh dmh mhThOhM cid8 16dt Scid10T max cid9 Rmax cid2 γ Eμ hHt cid5 aA Prah μ cid5 oΘ Lemma 6 proof similar Eq C7 cid16 cid16V μ P oh cid9 cid8 h cid8a ocid9 ˆV μ cid8 h cid8a ocid9 cid9cid16 cid16 γ Eμ hcid6Ht1 cid16 cid16V μh cid6 ˆV μh cid6 cid16 cid16 Eμ hHt cid5 aA cid16 cid16 cid16 ˆP oh cid16 cid5 oΘ cid16 cid16 cid16 Prah μ cid16 cid5 P oh cid16 cid16 cid16 Prah μ cid16 cid5 oΘ cid13 cid5 aA cid13 oΘ cid5 mhThOhM cid5 sS cid6 Ohs o cid13 mhThOhM cid5 scid6S cid16 cid5 cid16 cid16 Prah μ cid16 aA oΘ mhThOhM Eμ hHt cid5 oΘ cid2 Eμ hHt Ths s cid6 bhsbM h mh dmh cid5 scid6S cid6 Ohs o cid5 sS Ths s cid6 ˆbhs ˆbM h mh dmh cid16 cid16 cid16 cid16 cid5 scid6S cid6 Ohs o cid5 sS Ths s cid6 bhsbM h mh dmh 488 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 cid5 oΘ Eμ hHt cid5 oΘ cid13 cid5 cid6 Ohs o Ths s cid6 ˆbhsbM h mh dmh cid16 cid16 cid16 cid16 cid5 sS mhThOhM cid5 scid6S cid16 cid16 cid16 Prah μ cid16 cid5 oΘ aA cid13 cid13 cid5 scid6S cid6 Ohs o cid5 sS mhThOhM cid5 cid5 scid6S cid6 Ohs o Ths s sS cid6 ˆbhs ˆbM h mh dmh cid16 cid16 cid16 cid16 Ths s cid6 ˆbhsbM h mh dmh mhThOhM cid10bh ˆbhcid10 cid5 Prah μ cid2 Eμ hHt Eμ hHt aA cid13 cid6 Ohs mhThOhM cid5 cid5 oΘ scid6S cid5 o sS cid5 cid6 Ohs o Ths s cid6 ˆbhsbM h mh dmh cid13 cid16 cid16 cid16 cid16 mhThOhM Ths s cid6 ˆbhs ˆbM h mh dmh sS cid16 cid16 cid16 cid16 cid2 16dt d Lemma 6 proof similar Eq C7 Taken obtain cid16 cid16 cid16 cid2 γ Eμ cid16V μh ˆV μh Eμ hHt hcid6Ht1 cid16 cid16V μh cid6 ˆV μh cid6 cid16 cid16 cid8 16dt Scid10T max cid9 Rmax γ 16dt d ˆV μ max Eq C9 Scid10T Next let rewrite equation deﬁning Yt Eμ maxRmax γ d ˆV μ max Then Y0 cid2 γ Y1 B cid2 γ γ Y2 A B B cid2 γ hHt cid2 Aγ 2γ 2 B1 γ γ 2 A Substituting A 16dRmax γ ˆV μ max B Scid10T note V μ ˆV μ equals Y0 cid2 V μh ˆV μh A 16dRmax γ ˆV μ max B cid9 cid18 cid8 γ γ Y3 2A B A B 1 1 1 γ 1 γ 2 maxRmax γ d ˆV μ B cid2 cid19 cid18 B cid19 1 1 γ max equation proves Lemma V μ V ˆμ cid2 V μ ˆV Now prove Theorem 1 ˆμ W cid2 V μ ˆV μ W cid2 V μ V μ W ˆμ W μ W Lemma 7 Deﬁnition ˆμ Lemma 7 ˆμ ˆμ ˆμ W μ cid2 References 1 D Aberdeen J Baxter Scaling internalstate policygradient methods POMDPs International Conference Machine Learning ICML02 Sydney Australia July 2002 pp 112 2 KJ Aström Optimal control Markov decision processes incomplete state estimation Journal Mathematical Analysis Applica tions 10 1965 174205 3 T Augustin On suboptimality generalized Bayes rule robust Bayesian procedures decision theoretic point viewa cautionary note updating imprecise priors Proceedings 3rd International Symposium Imprecise Probabilities Applications ISIPTA03 2003 4 R Bellman Dynamic Programming Princeton Univ Press Princeton NJ 1957 5 JM Bernard T Seidenfeld M Zaffalon Eds Proceedings Third International Symposium Imprecise Probabilities Appli cations Carleton Scientiﬁc 2003 6 DP Bertsekas Dynamic Programming Optimal Control vol 2 second ed Athena Scientiﬁc Belmont MA 2001 7 B Bonet An epsilonoptimal gridbased algorithm partially observable Markov decision processes Proc 19th International Conf Machine Learning ICML02 Morgan Kaufmann 2002 pp 5158 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 489 8 C Boutilier T Dean S Hanks Decisiontheoretic planning Structural assumptions computational leverage Journal Artiﬁcial Intelli gence Research 11 1999 194 9 C Boutilier D Poole Computing optimal policies partially observable decision processes compact representations Proceedings Thirteenth National Conference Artiﬁcial Intelligence AAAI96 Portland OR AAAI PressThe MIT Press 1996 pp 11681175 10 J Breese K Fertig Decision making interval inﬂuence diagrams Proceedings 6th Annual Conference Uncertainty Artiﬁcial Intelligence UAI91 New York Elsevier Science 1991 pp 467478 11 A Cassandra ML Littman NL Zhang Incremental Pruning A simple fast exact method partially observable Markov decision processes D Geiger PP Shenoy Eds Proceedings Thirteenth Annual Conference Uncertainty Artiﬁcial Intelligence UAI 97 San Francisco CA Morgan Kaufmann 1997 pp 5461 12 L Chrisman Independence lower upper probabilities Proceedings 12th Annual Conference Uncertainty Artiﬁcial Intelligence UAI96 San Francisco CA Morgan Kaufmann 1996 pp 169177 13 FG Cozman Credal networks Artiﬁcial Intelligence 120 2000 199233 14 FG Cozman E Krotkov QuasiBayesian strategies efﬁcient plan generation application planning observe problem E Horvitz FV Jensen Eds Proceedings Twelfth Conference Uncertainty Artiﬁcial Intelligence UAI96 San Francisco CA Morgan Kaufmann 1996 pp 186193 15 A Drake Observation Markov process noisy channel PhD thesis Massachusetts Institute Technology 1962 16 Z Feng EA Hansen Approximate planning factored POMDPs Proceedings 6th European Conference Planning ECP01 Toledo Spain September 2001 17 K Fertig J Breese Interval inﬂuence diagrams Proceedings 5th Annual Conference Uncertainty Artiﬁcial Intelligence UAI90 New York Elsevier Science 1990 pp 149161 18 KW Fertig JS Breese Probability intervals inﬂuence diagrams IEEE Transactions Pattern Analysis Machine Intelligence 15 3 1993 280286 19 H Gaifman A theory higher order probabilities Proceedings 1986 Conference Theoretical Aspects Reasoning Knowledge Morgan Kaufmann 1986 pp 275292 20 R Givan SM Leach T Dean Boundedparameter Markov decision processes Artiﬁcial Intelligence 122 12 2000 71109 21 IJ Good Good Thinking The Foundations Probability Applications University Minnesota Press Minneapolis 1983 22 AJ Grove JY Halpern Updating sets probabilities Proceedings 14th Annual Conference Uncertainty Artiﬁcial Intelligence UAI98 San Francisco CA Morgan Kaufmann 1998 pp 173182 23 V Ha P Haddawy Theoretical foundations abstractionbased probabilistic planning Proceedings 12th Annual Conference Uncertainty Artiﬁcial Intelligence UAI96 San Francisco CA Morgan Kaufmann 1996 pp 291298 24 EA Hansen Solving POMDPs searching policy space Proceedings Fourteenth Conference Uncertainty Artiﬁcial Intelligence UAI98 1998 pp 211219 25 EA Hansen Z Feng Dynamic programming POMDPs factored state representation Artiﬁcial Intelligence Planning Systems AIPS00 2000 pp 130139 26 EA Hansen R Zhou Synthesis hierarchical ﬁnitestate controllers POMDPs Thirteenth International Conference Automated Planning Scheduling ICAPS03 June 2003 27 D Harmanec Generalizing Markov decision processes imprecise probabilities Journal Statistical Planning Inference 105 2002 199213 28 M Hauskrecht Valuefunction approximations partially observable Markov decision processes Journal Artiﬁcial Intelligence Re search 13 2000 3394 29 M Hauskrecht H Fraser Planning treatment ischemic heart disease partially observable Markov decision processes Artiﬁcial Intel ligence Medicine 18 2000 221244 30 LP Kaelbling ML Littman AR Cassandra Planning acting partially observable stochastic domains Artiﬁcial Intelligence 101 1999 99134 31 N Karmarkar A new polynomialtime algorithm linear programming Combinatorica 4 1984 373395 32 PE Lehner KB Laskey D Dubois An introduction issues higher order uncertainty IEEE Transactions Systems Man Cyber netics Part A 26 3 1996 289293 33 I Levi On indeterminate probabilities Journal Philosophy 71 1974 391418 34 I Levi The Enterprise Knowledge MIT Press Cambridge MA 1980 35 WS Lovejoy A survey algorithmic methods partially observed Markov decision processes Annals Operations Research 28 1991 4766 36 C Lusena J Goldsmith M Mundhenk Nonapproximability results partially observable Markov decision processes Journal Artiﬁcial Intelligence Research 14 2001 83103 37 DA McAllester S Singh Approximate planning factored POMDPs belief state simpliﬁcation Proceedings Fifteenth Conference Uncertainty Artiﬁcial Intelligence UAI99 1999 pp 409416 38 M Montemerlo J Pineau N Roy S Thrun V Verma Experiences mobile robotic guide elderly Proceedings National Conference Artiﬁcial Intelligence AAAI02 Edmonton AB July 2002 pp 587592 39 A Nilim L ElGhaoui Robustness Markov decision problems uncertain transition matrices Advances Neutral Information Processing Systems 16 NIPS03 MIT Press Cambridge MA 2004 40 A Nilim L ElGhaoui Robust control Markov decision processes uncertain transition matrices Operations Research 53 2005 780798 41 G Paaß Second order probabilities uncertain conﬂicting evidence Proceedings 6th Annual Conference Uncertainty Artiﬁcial Intelligence UAI91 New York Elsevier Science 1991 pp 447456 490 H Itoh K Nakamura Artiﬁcial Intelligence 171 2007 453490 42 CH Papadimitriou JN Tsitsiklis The complexity Markov decision processes Mathematics Operations Research 12 3 1987 441 450 43 J Pineau Tractable planning uncertainty Exploiting structure PhD thesis Robotics Institute Carnegie Mellon University Pittsburgh PA 2004 44 J Pineau G Gordon S Thrun Pointbased value iteration An anytime algorithm POMDPs Proceedings Eighteenth Interna tional Joint Conference Artiﬁcial Intelligence IJCAI03 AAAI Press Menlo Park CA 2003 45 P Poupart Exploiting structure efﬁciently solve large scale partially observable Markov decision processes PhD thesis Department Computer Science University Toronto Toronto Ontario Canada 2005 46 P Poupart C Boutilier Bounded ﬁnite state controllers Advances Neural Information Processing Systems 16 NIPS03 MIT Press Cambridge MA 2004 47 P Poupart C Boutilier VDCBPI An approximate scalable algorithm large scale POMDPs Advances Neural Information Processing Systems 17 NIPS04 MIT Press Cambridge MA 2005 48 JK Satia RE Lave Markovian decision processes uncertain transition probabilities Operations Research 21 1973 728740 49 T Seidenfeld MJ Schervish Two perspectives consensus Bayesian inference decisions IEEE Transactions Systems Man Cybernetics 20 2 1990 318325 50 G Shafer A Mathematical Theory Evidence Princeton Univ Press Princeton NJ 1976 51 EJ Sondik The optimal control partially observable Markov processes PhD thesis Stanford University 1971 52 MTJ Spaan N Vlassis Perseus Randomized pointbased value iteration POMDPs Journal Artiﬁcial Intelligence Research 24 2005 195220 53 N Vlassis MTJ Spaan A fast pointbased algorithm POMDPs Benelearn 2004 Proceedings Annual Machine Learning Conference Belgium Netherlands Brussels Belgium 2004 pp 170176 54 P Walley Statistical Reasoning Imprecise Probabilities Chapman Hall London 1991 55 CC White HK Eldeib Parameter imprecision ﬁnite state ﬁnite action dynamic programs Operations Research 34 1986 120129 56 CC White HK Eldeib Markov decision processes imprecise transition probabilities Operations Research 43 1994 739749 57 NL Zhang W Zhang Speeding convergence value iteration partially observable Markov decision processes Journal Artiﬁcial Intelligence Research 14 2001 2951 58 W Zhang NL Zhang Restricted value iteration Theory algorithms Journal Artiﬁcial Intelligence Research 23 2005 123165 59 R Zhou EA Hansen An improved gridbased approximation algorithm POMDPs Proceedings 17th International Joint Con ference Artiﬁcial Intelligence IJCAI01 2001 pp 707716