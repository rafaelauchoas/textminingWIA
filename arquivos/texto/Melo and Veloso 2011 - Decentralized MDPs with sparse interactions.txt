Artiﬁcial Intelligence 175 2011 17571789 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Decentralized MDPs sparse interactions Francisco S Melo Manuela Veloso b GAIPS INESCID TagusPark Edifício IST 2780990 Porto Salvo Portugal b Computer Science Department Carnegie Mellon University Pittsburgh PA 15213 USA r t c l e n f o b s t r c t Article history Received 26 April 2010 Received revised form 29 April 2011 Accepted 7 May 2011 Available online 10 May 2011 Keywords Multiagent coordination Sparse interaction Decentralized Markov decision processes Creating coordinated multiagent policies environments uncertainty challenging problem greatly simpliﬁed coordination needs known limited speciﬁc parts state space In work explore local interactions simplify coordination multiagent systems We focus problems interaction agents sparse contribute new decisiontheoretic model decentralized sparseinteraction multiagent systems DecSIMDPs explicitly distinguishes situations agents team coordinate act independently We relate new model existing models MMDPs DecMDPs We propose solution method takes advantage particular structure DecSIMDPs provide theoretical error bounds quality obtained solution Finally reinforcement learning algorithm independent agents learn individual policies coordinate We illustrate application algorithms paper multiagent navigation scenarios 2011 Elsevier BV All rights reserved 1 Introduction Decisiontheoretic models DecMDPs DecPOMDPs provide rich framework tackle decentralized decisionmaking problems However models create coordinated multiagent policies environments uncertainty challenging problem decisionmakers tackle issues partial observability As solving ﬁnitehorizon DecPOMDPs NEXPcomplete problem computationally demanding solve simplest scenarios Recent years witnessed profusion work DecPOMDPrelated models aim capturing fundamental features class problems partial observability incurring associated computational cost In paper contribute area research introduce new model cooperative multiagent decision making presence partial observability Our model motivated observation realworld scenarios tasks different agents multiagent coupled decision step relatively infrequent situations We refer problems having sparse interactions Multirobot systems provide primary motivation interaction different robots naturally limited robots physical boundaries workspace communication range limited perception capabilities Therefore programming multirobot perform task natural approach subdivide task smaller tasks robot execute autonomously smaller group As example consider scenario Fig 1 In scenario robots navigate goal locations marked dashed lines While Robot 3 Corresponding author Email address fmeloinescidpt FS Melo 00043702 matter 2011 Elsevier BV All rights reserved doi101016jartint201105001 1758 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 Fig 1 Example simple navigation task navigate goal disregarding remaining robots Robots 1 2 need coordinate cross narrow doorway simultaneously However coordination needs occur doorway Other examples include problems sequential resource allocation groups agents interact extent need share common resource In context methods proposed leverage sparse interactions decomposing global problem smaller local problems solved eﬃciently combining obtained solutions 12 Such approaches particularly concerned partial observability issues Additional examples include problems task allocation different agents multiagent assigned different subtasks interactions agents performing subtasks localized small regions joint state space 3 Such problems include emergency response scenarios emergency teams assigned different subtasks example assisting different victims interact speciﬁc localized situations Several approaches exploited simpliﬁed models interaction multiagent settings For example learning tasks involving multiple agents partitioned statewise manner allowing different agents independently learn resulting smaller tasks 4 Similarly hierarchical learning algorithm considers interactions different agents higher control level allowing agents learn lower level tasks independently 5 Other works use coordination graphs compactly represent dependences actions different agents capturing local interaction 67 Local interactions exploited minimize communication policy execution 8 gametheoretic literature attain compact game representations Examples include graphical games 9 actiongraph games 10 In article consider DecMDPs sparse interactions henceforth DecSIMDPs DecSIMDPs leverage indepen dence agents decouple decision process signiﬁcant portions joint state space In situations agents interactthe interaction areasDecSIMDPs rely communication bring computational complexity joint decision process DecSIMDPs balance independence assumptions observability given state agents independent share state information communicating1 A related model recently proposed designation distributed POMDPs coordination locales 13 We postpone Section 6 detailed discussion related models The contributions article threefold We provide precise formalization DecSIMDP model discuss relation wellestablished decisiontheoretic models DecMDPs MMDPs MDPs We contribute new algorithms exhibit signiﬁcant computational savings compared existing algorithms DecSIMDPs illustrate application simple navigation tasks Finally investigate inﬂuence interaction areas performance multiagent contribute new learning algorithm allows agent individually learn interaction areas 2 Decisiontheoretic models multiagent systems We review standard decisiontheoretic models relevant work 1416 We start single agent models Markov decision processes MDPs partially observable counterparts POMDPs mov ing multiagent models multiagent MDPs MMDPs partially observable counterparts DecPOMDPs We establish notation use review fundamental concepts later relevance To fully specify different models section explicitly include initial state x0 distribution thereof However order avoid cluttering notation omit explicit reference initial state understanding state implicit 1 Both independence assumptions communication signiﬁcantly bring computational complexity DecPOMDP related models 1112 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1759 21 Markov decision processes A Markov decision process MDP describes sequential decision problem single agent choose action time step maximize rewardbased optimization criterion We use MDPs DecSIMDP model proposed Section 3 agent situations actions independent agentsie situations agent modeled individually Formally MDP tuple M X A P r γ X represents ﬁnite state space A represents ﬁnite action space Px y represents transition probability state x state y action taken rx represents expected reward taking action state x The scalar γ discount factor A Markov policy mapping π X A 0 1 x X cid2 aA π x 1 Solving MDP consists determining policy π maximize x X V π x Eπ Xt At cid3 cid4 γ tr cid2 t0 cid7 cid5 cid6 cid6 X0 x Xt denotes state time step t At denotes action taken time instant cid8 cid6 cid6 Ht h cid9 cid8 cid6 cid9 cid6 Xt x P P At π x Ht X0 A0 Xt 1 At 1 Xt random variable corresponding history MDP time t h denotes particular realization Ht Xt x We write At π denote dependence At policy π We deﬁne Q function associated policy π At cid3 Q π x Eπ cid4 γ tr cid2 t0 Xt At cid5 cid6 cid6 X0 x A0 cid7 At π t 0 For ﬁnite MDP optimal policy π V π x cid2 V π x policy π state x X The value function corresponding π veriﬁes Bellman optimality equation cid10 rx γ x max aA Px yV cid2 y cid11 V yX The associated Q function turn veriﬁes Q x rx γ cid2 yX Px y max uA Q y u denoted V short V π 1 2 The optimal policy recovered directly Q state x X arg maxuA Q corresponding optimal Q function Q assigning action A positive probability selected x u As solution given MDP obtained computing Given function q deﬁned X A Bellman operator H deﬁned Hqx rx γ cid2 yX Px y max uA q y u 3 The function Q estimate Q 0 dynamic programming DP method known value iteration 2 ﬁxedpoint H computed iteratively applying H initial 211 Qlearning It possible use operator H 3 use standard ﬁxedpoint iteration compute Q unknown Q consisting stateactionrewardnext state experienced environment deﬁned update rule estimated Q learning algorithm 17 Q learning allows Q If P andor r estimated transitions Q k1x 1 αkQ kx αk 4 cid12 rx γ max uA cid13 Q k y u 1760 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 x y sample distribution Px αk stepsize sequence Q kx kth estimate Q The sample y value rx obtained generative model actual requiring knowledge P r Under suitable conditions estimates Q k converge Q probability 1 18 We henceforth write QLUpdateQ x r y Q cid7 compactly denote general Q learning update operation 4 cid4 Q updtx QLUpdate Q x r y Q cid5 cid7 cid3 1 αkQ x αk cid12 r γ max uA cid13 y u cid7 Q Q Q cid7 general Q functions 22 Partially observable Markov decision processes Partially observable MDPs problems essentially similar MDPs agent choose action time step maximize rewardbased criterion However time step t agent POMDP directly observe state process Xt accesses indirect observation thereof Z t POMDPs useful planning DecSIMDPs Section 4 Taking advantage particular structure DecSIMDPs reduce global planning problem problem planning collection individual singleagent POMDPs Formally POMDP tuple X A Z P O r γ X ﬁnite state space A ﬁnite action space Z ﬁnite observation space As Px y represents transition probability state x state y action taken Ox z represents probability observing z Z given state x action taken Ox z P cid8 Z t 1 z cid6 cid6 Xt 1 x At cid9 Finally rx represents expected reward taking action state x γ discount factor A nonMarkov policy mapping π H 0 1 ﬁnite histories h H cid2 aA π h 1 h a0 z1 a1 1 zt ﬁnite history ﬁnite sequence actionobservation pairs As history observed time t random variable denoted Ht denote H set possible ﬁnite histories POMDP Solving POMDP consists determining policy π maximize X dimensional probability vectors b cid3 cid7 cid2 cid4 V π b Eπ γ tr Xt At cid5 cid6 cid6 X0 b t0 X0 b denotes fact X0 distributed according b Similarly deﬁne Q function associated policy π Q π b Eπ cid3 cid2 t0 cid4 γ tr Xt At cid5 cid6 cid6 X0 b A0 cid7 We refer probability vector b expressions initial belief state initial belief It translates belief agent time t 0 state time From initial belief given history time t Ht construct sequence bt probability vectors recursively cid4 cid5 bt At Z t 1 b yt 1 Bel cid5 y At Z t 1 x At y bxtP cid3 η cid2 O cid4 cid4 cid5 Bel belief update operator bxt denotes x component bt η normalization factor We refer vector bt belief time t It corresponds distribution unknown state time t x cid8 bxt P Xt x cid6 cid9 cid6 Ht Given POMDP model parameters P O ﬁnite history h H mapped belief b Moreover given policy histories leading belief yield future total expected discounted reward As beliefs compact representations histories deﬁne policies terms beliefs instead histories 19 In fact possible reinterpret POMDP inﬁnite MDP state space corresponds set possible beliefs Therefore ﬁnite POMDP optimal policy π V π b cid2 V π b FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1761 π initial belief b The corresponding value function denoted V optimality equation V b max aA cid2 bx x cid10 rx γ Px yO y zV cid4 cid5 Belb z cid2 z y cid11 veriﬁes Bellman x y values X z takes values Z Belb z corresponds updated belief taking action observing z The associated Q function turn veriﬁes Q b cid2 bx x cid10 rx γ cid2 z y Px yO y z max uA Q cid4 cid11 cid5 Belb z u In spite representative power POMDPs shown undecidable worst case inﬁnite horizon settings considered paper 20 As exact solutions computed speciﬁc instances approaches literature resort approximate heuristic methods 2122 We MDPbased heuristic solution method POMDPs use later paper This method known Q MDP makes use optimal Q function underlying MDP estimate optimal Q function POMDP 23 Since optimal solution underlying MDP eﬃciently computed method simple fast implement attains good performance practical situations 2123 Let M X A Z P O r γ POMDP ﬁnite state action observation spaces Associated POMDP M Q MDP uses estimate optimal Q function underlying MDP M X A P r γ Let Q optimal Q function POMDP given cid2 ˆQ b Q bx x x When Q MDP agent acts implicit assumption state uncertainty affects immediate decision decision agent act underlying MDP Since assumption seldom holds Q MDP exhibits poor performance works proposed improvements Q MDP address issue 2124 pursue discussion 23 Multiagent MDPs Multiagent Markov decision processes MMDPs generalize MDPs multiagent cooperative scenarios MMDPs sequential decision tasks multiple agents choose individual action timestep jointly maximize common rewardbased optimization criterion Formally MMDP tuple M N X Ak P r γ N number agents X represents ﬁnite state space Ak ﬁnite individual action space agent k As MDPs Px y represents transition probability state x state y joint action a1 aN taken rx represents expected reward received agents taking joint action state x In MMDP agents receive reward implies MMDPs represent fully cooperative multiagent tasks A joint action tuple a1 aN denote A N Ak set possible joint actionsthe joint k1 action space For k 1 N write Ak A1 Ak1 Ak1 AN denote set joint actions agents agent k We write ak denote general element Ak refer action reduced joint action simply reduced action We write ak ak denote fact joint action composed reduced action ak individual action ak agent k We assume state space X factored N agents X X0 X1 XN X0 denotes agentindependent component state As element x X tuple x x0 xN xk Xk k 0 N For x X refer pair x0 xk local state agent k generally denote xk An individual Markov policy agent k mapping πk X Ak 0 1 x X cid2 πkx ak 1 akAk Similarly joint policy mapping π X A 0 1 combination N individual policies π x Ncid14 k1 πkx ak a1 aN πk denotes reduced policy π πk πk denotes fact joint policy π composed reduced policy πk individual policy πk agent k 1762 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 In MMDP purpose agents determine joint policy π maximize x X cid3 cid2 cid4 γ tr Xt At cid7 cid5 cid6 cid6 X0 x V π x Eπ t0 Xt denotes state time t At denotes joint action taken time t The Q function associated joint policy π deﬁned V π singleagent counterpart For purposes planning computing optimal policy MMDP indistinguishable ordinary MDP In fact considering centralized controller MMDP reduces MDP It execution time MMDP differs MDP process decision making centralized However partially observable settings decentralized execution raises severe diﬃculties planning 24 Decentralized POMDPs Decentralized POMDPs DecPOMDPs partially observable generalizations MMDPs As MMDPs agents DecPOMDP choose individual action time step jointly maximize common rewardbased optimization criterion Unlike MMDPs agents access global state process means local indirect observations Formally DecPOMDP represented tuple cid4 M N Xk Ak Zk P Ok r γ cid5 N number agents X N Ak set joint actions Zk represents set possible local observations agent k Px y represents transition probabilities joint state x joint state y joint action taken Okx zk represents probability agent k making local observation zk joint state x action taken rx represents expected reward received agents taking joint action joint state x The scalar γ discount factor Xk joint state space A N In partially observable multiagent setting individual nonMarkov policy agent k mapping πk Hk 0 1 k1 k0 hk Hk cid2 π hk ak 1 akAk hk ak0 zk1 akt 1 zkt individual history agent k sequence individual action observation pairs Hk set possible ﬁnite histories agent k denote Hkt random variable represents history agent k time t Like POMDPs DecPOMDP agent partial perception global state Therefore agents perspective local observations nonMarkovianthe current local observation action suﬃcient uniquely determine observation In general multiagent setting compact representation histories plays role beliefs POMDPs implying passage MMDP partially observable counterpart fundamentally different passage singleagent scenarios2 However communication agents instantaneous free errorfree DecPOMDP reduces large POMDP partial observability longer issue Solving DecPOMDP consists determining joint policy π maximizes total sum discounted rewards In order write terms function consider distinguished initial state x0 X assumed common knowledge agents We want maximize cid3 cid2 V π Eπ cid4 γ tr Xt At cid5 cid6 cid6 X0 x0 cid7 t0 DecPOMDPs constitute representative models decisiontheoretic literature 26 In remainder section discuss specializations model relevant work 241 Decentralized MDPs state x X A decentralized MDP DecMDP particular case DecPOMDP joint observation z Z cid8 P Xt x cid6 cid6 Z t z cid9 1 2 This fact observed considering worstcase computational complexity different models In ﬁnite horizon settings POMDPs PSPACEcomplete versus Pcompleteness fully observable MDPs 25 In ﬁnitehorizon multiagent settings DecMDPs NEXPcomplete 16 2agent case versus Pcompleteness MMDPs FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1763 Intuitively agents DecMDP joint observability agents share observations recover state DecMDP unambiguously This contrast MMDPs agent individually observability In fact MMDPs seen particular subclass DecMDPs individual observations agent allow recover state DecMDPMMDP unambiguously Formally individual observation zk Zk state x X cid8 P Xt x cid6 cid6 Zkt zk cid9 1 5 Throughout work focus DecMDPs specializations thereof Moreover MMDPs assume state factored agents local observability meaning agent infer local observations corresponding local state unambiguously Formally local observability translated following condition local observation zk Zk local state xk X0 Xk cid8 P Xkt xk cid6 cid6 Zkt zk cid9 1 Although general DecMDP models possible 16 adhere simpliﬁed version suﬃcient purposes For future reference deﬁne set Xk X0 Xk1 Xk1 XN corresponding local state agents agent k denote xk general element Xk As actions write x xk xk denote fact kth component x takes value xk 242 Transition observation reward independence Transitionindependent DecMDPs constitute particular subclass DecMDPs x X A cid8 cid8 P P X0t 1 y0 Xkt 1 yk cid6 cid6 Xt x At cid6 cid9 cid6 Xt x At cid9 cid8 P cid8 P X0t 1 y0 Xkt 1 yk cid6 cid9 cid6 X0t x0 cid6 cid6 Xkt xk Akt ak cid9 6a 6b 7 The transition probabilities factorized Px y P0x0 y0 Ncid14 k1 Pkxk ak yk P0x0 y0 P Pkxk ak yk P cid8 cid8 X0t 1 y0 Xkt 1 yk cid6 cid6 X0t x0 cid9 cid6 cid6 Xkt xk Akt ak cid9 This particular class DecMDPs introduced 27 seeks exploit particular form independence bring computational complexity required solve models In class problems local state agent constitutes suﬃcient statistic history optimal policy agent computed terms individual state 12 This particular class DecMDPs shown NPcomplete ﬁnitehorizon settings versus NEXPcompleteness general ﬁnitehorizon DecMDPs 12 Similarly rewardindependent DecMDPs correspond subclass DecMDPs x rx f cid4 rkxk ak k 1 N cid5 8 global reward function r obtained local reward functions rk k 1 N To ensure consistency decision process require cid4 cid5 rkxk ak rkxk ak f cid4 cid5 rkxk ak rkxk uk cid2 f rkxk ak cid2 rkxk uk One typical example rx Ncid2 k1 rkxk ak 9 x x0 xN a1 aN Interestingly recently shown 2811 reward independent DecMDPs retain NEXPcomplete complexity However associated transition independence reward independence implies DecMDP decomposed N independent MDPs solved separately The complexity class problems reduces standard MDPs Pcomplete 1764 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 Fig 2 Scenario Example 1 Two robots coordinate narrow doorway 3 Decentralized sparseinteraction MDPs DecSIMDPs We depart transitionindependent DecMDP introduce new model multiagent decision problems time general speciﬁc transition independent DecMDPs Our goal exploit sparse interactions different agents DecMDP interested DecMDPs level transition reward dependency dependency limited speciﬁc regions state space To motivate proposed model start simple example section illustrate concepts introduced Example 1 Consider scenario depicted Fig 2 corresponding environment consisting rooms 10 cells connected narrow doorway D In scenario mobile robots acting team Robot 1 Robot 2 navigate depicted initial positions corresponding goal positions marked Fig 2 labels Goal 1 Goal 2 respectively Each robot available 4 actions N S E W adjacent cell corresponding direction probability 08 fail probability 02 In case failure robot remains cell Whenever robot reaches corresponding goal position team granted reward 1 Conversely robots stand simultaneously narrow doorway corresponding cell marked D robots bump team granted reward 20 Also situation robots way probability individual action failing 04 Each robot able perceive unambiguously position environment Also robots simultaneously stand shaded area Fig 2 able perceive joint state unambiguously This problem modeled DecPOMDP cid4 MH 2 Xk Ak Zk P Ok r γ cid5 Xk 1 20 D k 1 2 Ak N S E W k 1 2 Zk Xk X I k 1 2 X I 6 15 D 6 15 D transition observation functions derived description reward function r given rx x 20 9 x1 20 x2 9 2 1 20 x D D 0 The reward function example purposefully designed include region state space negative reward robots coordinate Outside interaction area shaded region Fig 2 robot pursue goal disregarding existence robot The DecSIMDP model introduce explicitly addresses situations interaction agents DecMDP localized Example 1 In order deﬁne model leverages local interactions ﬁrst formalize concepts agent dependence agent interaction These deﬁnitions reﬁne notions transition reward independence described Section 24 Let K k1 subset agents DecMDP We denote XK X0 Xk1 Xkm FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1765 joint state space agents K Extending notation introduced Section 2 write XK denote joint state space agents K We write xK denote general element XK xK denote general element XK We write x xK xK distinguish components x corresponding agents K corresponding agents K We assume reward function DecMDP decomposed rx Ncid2 k1 rkxk ak M Rcid2 i1 r I xK aK 10 rk corresponds individual component reward function depends agent k M R sets K 1 M R M R reward components r I interaction components depending agents K Similarly assume transition probabilities DecMDP decomposed Px y P0x0 y0 Ncid14 cid4 k1 1 IK kIX I Ki cid5 xK Pkxk ak yk P0x0 y0 M Pcid14 i1 PK xK aK y K IX I Ki xK 11 X I K factorized 7 We write IU x denote indicator function set U includes states XK transition probabilities agents k K Expressions 10 11 explicitly distinguish components reward transition functions factorized The decompositions 10 11 imply loss generality reward r trivially written form setting M R 1 rk 0 K1 1 N r I r occurs 1 P However general case transition probabilities setting M P 1 K1 1 N PI 1 subset X A quantities nonzeroare P0 supports small compared X A M P i1 PK M R i1 r I IX I Ki cid19 cid20 Deﬁnition 31 Agent independence Given DecMDP M agent k0 independent agent k1 state x X following conditions hold state x It possible decompose global reward function rx 10 way agent set K contains k0 k1 It possible decompose transition probabilities Px y 11 way agent set K contains k0 k1 When conditions hold agent k0 said depend agent k1 x Similarly agent k0 inde pendent set agents K k1 state x conditions hold k K state x dependent An agent k0 depends agent k1 particular state reward function transition probabilities decomposed decouple inﬂuence agents individual state action However dependence relation symmetrical agent k0 depends agent k1 reverse need hold In fact possible agents reward independent transition probabilities state xk0 depend individual stateaction agent k1 reverse holding We illustrate notion agent independence Example 1 Example 1 Cont 1 In DecMDP MH previously introduced possible write reward function r rx r1x1 a1 r2x2 a2 r I x r1x1 a1 I20x1 r2x2 a2 I9x2 r I x IDDx We write Ix denote indicator function set x In state D D Robot 1 Robot 2 depend seen transition probabilities In remaining joint states X Robot 1 Robot 2 independent Deﬁnition 32 Agent interaction In DecMDP M set agents K interact state x X following conditions hold For k0 K agent k0 depends agent k1 state x k1 K For k1 K agent k0 depends agent k1 state x k0 K There strict subset K cid7 K conditions hold K cid7 If agents set K interact state x refer xK interaction state agents K 1766 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 The concept interaction introduced captures local dependence set agents DecMDP If xK interaction state agents K mean agent k K depends agents state x Instead means agent K depends k k depends Furthermore deﬁnition agent interaction transitive agents k0 k1 interact state x agents k1 k2 interact state x agents k0 k2 interact x We resort Example 1 illustrate concept agent interaction Example 1 Cont Robot 1 Robot 2 interact state D D In fact seen Robot 1 depend Robot 2 converse holds Also strict subset 1 2 conditions Deﬁnition 32 hold Deﬁnition 33 Interaction area Consider general Nagent DecMDP M N Xk Ak Zk P Ok r γ We deﬁne interaction area X I verifying X I XK set agents K ii state x iii x X I aK AK y X I X I x cid8 P X K t 1 y cid6 cid6 X K t x A K t aK interaction state agents K cid9 P0x0 y0 cid14 kK Pxk ak yk 12 iv set X I connected3 An agent k involved interaction time t interaction area X I involving set agents K k K X K t X I Otherwise agent k involved interaction Condition states interaction area involves subset K agents DecMDP Condition ii ensures interaction area interaction state involving agents K minimize number agents involved interaction Condition iii deﬁnes interaction areas regions state space interaction state Finally condition iv states interaction areas sets adjacent states terms transition function Unlike interaction states interaction areas disjoint agent simultaneously involved interaction different interaction areas Example 1 Cont The joint state D D interaction state corresponding situation Robot 1 Robot 2 narrow doorway An interaction area include state D D states immediately adjacent In fact order robots avoid bumping narrow doorway coordinate actually reaching doorway inclusion neighboring states interaction area associated interaction state In particular scenario corresponds shaded area Fig 2 For illustration purposes consider X I 6 15 D 6 15 D larger sets possible As required X I X1 X2 Also required ii D D X I Condition iii requires transition proba bilities states X I states X I factored 7 In scenario factorization possible state D D possible transition state state outside X I This means condition iii holds Finally X I connected condition iv holds We exploit fact agent involved interaction unaffected partial joint state observability sense optimal action independently state agents The purpose deﬁning interaction areas DecMDP precisely single situations actions agent depend agents As interaction area agent use state information agents interaction area choose actions Therefore consider scenarios agents particular interaction area X I XK time t access state X K t We henceforth refer DecMDP having observable interactions Deﬁnition 34 Observable interactions A DecMDP observable interactions interaction area X I involving set K agents k K set local observations Z I k Zk x X I cid8 cid9 P Zkt Z I k cid6 cid6 X K t X I zk Z I k local state xK X I cid6 cid6 Zkt zk X K t xK 1 1 P cid9 cid8 3 A set U X connected pair states x y U sequence actions positive probability yields statetrajectory x0 xT xt U t 0 T x0 x xT y vice versa FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1767 Our focus DecMDPs observable interactions translates property observed realworld scenarios involved interaction agents able observe communicate information relevant coordination Interaction areas encapsulate need information sharing general multiagent decision problem Informally DecMDP M N Xk Ak Zk P Ok r γ sparse interactions agents XK set agents K X I M We refer DecMDP sparse observable interactions decentralized sparseinteraction MDP Dec dependent set M interaction areas X I X I SIMDP 1 X I cid11 XK Deﬁnition 35 DecSIMDP Let M N Xk Ak Zk P Ok r γ DecMDP sparse observable interactions encapsulated set M interaction areas X I described We represent decentralized sparse interaction MDP DecSIMDP tuple 1 X I M cid4 Γ Mk k 1 N cid21cid4 cid5 1 M cid22cid5 X I MI Mk MDP Mk X0 Xk Ak Pk rk γ individually models agent k absence agents rk component joint reward function associated agent k decomposition 9 MI MMDP captures local interaction K agents states X I given MI K XK Ak PI r I γ X I XK Each MMDP MI superset interaction area deﬁned describes interaction subset K N agents corresponding state space XK A DecSIMDP alternative way representing DecMDP observable interactions For agents outside teraction areas joint transition probabilities reward function DecSIMDP factorized 7 9 possible model agents individual MDPs In states interaction area DecSIMDP agents involved interaction able communicate freely In areas agents use communication overcome local state perception modeled local MMDP observing state deciding jointly action Outside areas agents local perception state choose actions independently agents A simple albeit inaccurate way thinking DecSIMDP DecMDP agent access time step stateinformation required predict local state reward In fact DecSIMDP include statesnamely interaction areasin agents able perceive joint state information strictly required predict local state reward Hence inaccuracy description For illustration purposes revisit Example 1 DecSIMDP model corresponding navigation problem Example 1 Cont The problem depicted Fig 2 represented DecSIMDP cid4 Γ M1 M2 cid4 X I MI cid5cid5 M1 MDP M1 X1 A1 P1 r1 γ describing task Robot 1 navigating Goal 1 absence Robot 2 X1 A1 deﬁnition original DecMDP page 1764 P1 r1 arise decomposition r P described 10 11 M2 MDP M2 X2 A2 P2 r2 γ describing task Robot 2 navigating Goal 2 absence Robot 1 X I interaction area deﬁned MI MMDP MI 2 X A P r I γ captures interaction Robot 1 Robot 2 X I In example unique MMDP MI single interaction area X I Also robotsagents environment MMDP MI fully observable version original DecMDP M simpliﬁed reward function In general scenarios MMDP corresponding interaction area includes agents involved interaction Finally explore relation DecSIMDP model MDP MMDP models As expected absence interaction areas DecSIMDP reduces set independent MDPs solved separately A DecSIMDP interaction areas captures situation agents completely independent In situations agents interact states assumed general DecMDP model state space interaction area assumption observable interactions renders model equivalent MMDP Nevertheless appeal DecSIMDP model practical situations fall extreme 1768 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 cases independent MDPs vs fully observable MMDP It situations DecSIMDP model bring advantage general potentially intractable models We ﬁrst assume interaction areas known advance provided model speciﬁcation present planning algorithm leverages particular structure DecSIMDPs Section 4 In Section 5 discuss areas determined algorithm allows agent learn interaction areas 4 Planning DecSIMDPs We address problem planning DecSIMDPs estimating optimal policy agent DecSIMDP model fully speciﬁed including interaction areas We start introducing general heuristic approach relies solution associated POMDP leading general algorithms MPSI LAPSI We introduce concept generalized αvectors DecSIMDPs instances MPSI LAPSI use generalized αvectors discussing main properties methods The complete proofs results section Appendix A 41 Heuristic planning DecSIMDPs We start considering DecSIMDP agents state observability We refer agent agent k suppose remaining agents state observability follow ﬁxed known policy πk Agent k modeled POMDP agents collectively regarded environment In particular situation POMDP solution method compute policy agent k Our heuristic departs simpliﬁed setting computes policy agent k agents observability followed ﬁxed known policy πk This hypothesized policy πk allows agent k approximately track agents choose actions accordingly The closer πk actual policy agents better agent k able track select individual actions In fact ability agent track agents necessarily depends stochasticity environment4 scenarios determinant factor agents ability maintain reliable albeit ﬂat belief state agents depends accurate transition model agents critically depends close πk actual policy Algorithm 1 summarize general algorithm Algorithm 1 Heuristic planning algorithm DecSIMDPs Require DecSIMDP model Γ 1 agents k 1 N 2 3 4 5 end Build hypothetical policy ˆπk Using Γ ˆπk build POMDP model agent k Use preferred POMDP solution technique compute πk The idea Algorithm 1 general DecPOMDPs However hypothesized policy πk seldom corresponds actual policy followed agents method allow agent k properly track agents decide accordingly environment close deterministic Hence approach inadequate general DecPOMDPs expect lead poor results The particular structure DecSIMDPs renders approach appealing outside interaction areas individual policy agent k ideally exhibits little dependence statepolicy agents As poor tracking areas little impact policy agent k ii inside interaction areas local observability allows agent k perfectly track agents involved interaction choose actions accordingly We present algorithms Myopic Planning Sparse Interactions MPSI LookAhead Planning Sparse Interac tions LAPSI particular instances Algorithm 1 consider different hypothetical policies agents In MPSI agent k considers agents completely selfcentered oblivious interactions Agent k j cid12 k acts according policy π j optimal policy corresponding MDP M j acts agent j DecSIMDP In environments interaction MPSI heuristic provides good approximation policy agents outside interaction areas In LAPSI agent k considers agents jointly adopt optimal policy underlying MMDP5 LAPSI counterpart MPSI provides good approximation policy agents scenarios interactions sparse 4 For example POMDPs stochastic transitions meaning transitions state lead states beliefs decisionmaking ﬂat states nonnegligible probability 5 The MMDP associated DecSIMDP MMDP obtained endowing agents global fullstate observability FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1769 Using corresponding hypothesized policies remaining agents MPSI LAPSI leverage POMDP solution methods obtain policy agent k We introduce concept generalized αvectors use construct particular instances MPSI LAPSI 42 Generalized αvectors DecSIMDPs MPSI LAPSI described terms general POMDP solvers compute individual policy agent DecSIMDP We propose particular instances MPSI LAPSI exploit structure DecSIMDP model We depart singleagent POMDP model constructed proposed approach Algorithm 1 introduce generalized αvectors simpliﬁcation αvectors POMDP solvers 29 Using generalized αvectors derive variation Q MDP heuristic 23 provide bounds performance method applied DecSIMDPs We focus 2agent scenarios avoid unnecessarily complicating presentation remarking development presented extends easily agents cost cumbersome expressions Using POMDP model obtained adopting introduced heuristic agent k computes individual policy πk maps beliefs actions However agent k local state observability implying kth component state unambiguously determined Furthermore given assumption observable interactions Deﬁnition 34 time step state components corresponding agents interacting agent k unobservable By deﬁnition statecomponents depend stateaction agent k depend πk Recovering POMDP model agent k Section 2 Q b cid2 bx x cid10 rx γ cid2 z y Px yO y z max u cid11 cid5 Belb z u cid4 Q Since agent k local observability belief b concerns state component agent Q xk bk cid2 xk bxk cid10 rx γ cid2 z y Px yO y z max u cid4 Q yk Belb z u cid5 cid11 The agent assumed follow ﬁxed policy depends current state eliminate explicit dependence action Q xk bk ak cid2 xk bxk cid10 rπk x ak γ cid2 z y Pπk x ak yO y z max uk Q cid4 yk Belb ak z uk cid5 cid11 rπk x ak cid2 πkx akr cid4 cid5 x ak ak ak Pπk x ak y cid2 ak cid4 πkx akP cid5 x ak ak y Every time step t agent k interaction area implying agent agent k unambiguously perceive joint state Zkt Xt In remaining time steps Zkt Xkt meaning agent observes local state Denoting XI set joint states interaction area XI cid23 M i1 X I Q xk bk ak cid2 cid10 rπk x ak γ bxk cid2 yXI Pπk x ak y max uk Q yk yk uk xk γ cid2 y XI Pπk x ak y max uk Q cid4 yk Belb ak yk uk cid11 cid5 13 We focus explicitly updated belief Bel optimal Q function states interaction areas The general beliefupdate expression Section 22 cid4 cid5 b ak Zkt 1 η Bel y cid2 x cid4 cid5 bxtP x At y O cid4 cid5 y At Zkt 1 In current setting belief concerns distribution states agent As Xkt xk cid4 cid5 b ak Zkt 1 η Bel yk cid4 cid5 bxk tPπk x Akt yk O cid4 cid5 yk Akt Zkt 1 cid2 xk 1770 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 x xk xk Pπk x ak yk cid2 ak πkx akPk cid4 xk ak yk cid5 Section 24 Pk denotes transition probabilities corresponding kth components state If agents interaction area transitions agent depend actions agent k cid4 cid5 b ak Zkt 1 η Bel yk bxk tPπk x ykO cid4 cid5 yk Akt Zkt 1 cid2 If Xt 1 interaction area agent k observe state agent xk Bel yk b ak y I Xkt1 yk Ix denotes indicator function singleton set x For general situation Xt 1 XI cid5 cid2 cid4 y 14 Bel yk b ak yk η bxk tPπk x Akt yk xk IX c I general set U X IU indicator function set U U c denotes complement U X We consider expression Q xk bk ak agents interaction area In case bk exk xk ex probability vector component y given Ix y This implies Q xk xk ak rπk x ak γ Pπk x ak y max uk Q yk yk uk cid2 yXI γ cid2 y XI Pπk x ak y max uk Q cid4 yk Belb ak yk uk cid5 15 Noting similarity righthand 15 term square brackets righthand 13 deﬁne generalized αvector agent k αk recursively follows αkx ak rπk x ak γ cid2 Pπk x ak y max uk cid2 αk y ukIXI y γ cid2 yk y Pxk ak yk max uk yk Pπk xk ykαk y ukIX c I y 16 Theorem 41 Given twoagent DecSIMDP Γ generalized αvectors associated agent k agent follows ﬁxed known policy πk deﬁned exist unique Proof sketch The complete proof theorem Appendix A We introduce dynamicprogramming operator Tk αk Tkαk operator contraction supnorm The statement theorem follows Banachs ﬁxedpoint theorem cid2 The operator Tk iteratively compute generalized αvectors way similar valueiteration algorithm compute optimal Q function MDP The result establishes generalized αvectors associated DecSIMDP computed eﬃciently Theorem 42 The generalized αvectors 2agent DecSIMDP Γ verifying conditions Theorem 41 computed polynomial time Proof sketch The result follows noting generalized αvectors computed solving associated MDP cid2 Given MDP particular case DecSIMDP single agent DecSIMDP generalized αvectors correspond exactly optimal Q values It follows Theorem 42 computing generalized αvectors DecSIMDP Pcomplete FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1771 All results extend scenarios agents For example deﬁnition 16 takes general form αkx ak rπk x ak γ cid2 cid2 Pπk xO ak y O max uk Pπk xO yO αk y uk yO y y O yO y X The components y O correspond observable components ythose belong agents involved interaction agent kand yO correspond remaining components y O 43 Generalized αvectors LAPSI MPSI We use generalized αvectors compute estimates ˆQ b ak optimal Q function agent k ˆQ xk bk ak cid2 bxk αkx ak xk 17 This approximation shares features Q MDP heuristic Section 22 compute generalized α vectors DecSIMDP solving associated MDP One expect methods suffer states great uncertainty like Q MDP In states action selection conservative assumed sparse interactions hopefully minimize effects situation Our experimental results indicate case We derive error bounds approximation 17 depend dispersion maximum values generalized αvectors outside interaction areas This result extended general POMDPs providing error bounds Q MDP heuristic depend optimal Q function underlying MDP Given generalized αvector αkx ak let xO denote observable components xthose corresponding agents interacting k xkand xO remaining components We deﬁne dispersion set αvectors αkx ak x X ak Ak cid6 cid6 cid6 cid6max uk cid3k max xO cid2 cid4 αk xO xO xO uk cid2 cid5 xO cid4 αk max uk cid5 xO xO uk cid6 cid6 cid6 cid6 18 The dispersion set αvectors measures maximum value αk taken actions differs corresponding average nonobservable components state The dispersion quantiﬁes lack knowledge agent k state agents impact action choice agent k terms value Theorem 43 Let M Mk k 1 N X I obtained approximation 17 policy πk agents ﬁxed known Then 1 M DecSIMDP let πk denote policy agent k MI cid24 cid24V V πk cid24 cid24 cid4 2γ 2 1 γ 2 cid3k cid3k represents dispersion αvectors associated πk Proof See Appendix A cid2 19 Theorem 43 translates wellknown bounds approximate policies particular setting As expected bound 19 proportional total dispersion generalized αvectors Also bound 19 zero The state space interaction area XI X In case recover MMDP version problem There interaction states XI In case generalized αvectors agent k depend Section 3 agents implying cid3k 0 Finally bounds assume policy hypothesized agents corresponds actual policy case MPSI LAPSI In particular deﬁning errors rπk Pπk arising imperfect knowledge πk εr cid14rπk r ˆπk cid14 εP cid14Pπk P ˆπk cid14 ﬁnally cid24 cid24V V πk cid24 cid24 cid4 2γ 2 1 γ 2 cid3k εr γ εP 1 γ 20 1772 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 ﬁrst term corresponds approximation error POMDP policy second term corresponds approximation error policy agents The error bounds 20 generally looseas known similar bounds MDPs loose However view interesting aspect proposed bounds arises deﬁnition identiﬁcation dispersion generalized αvectors bounds critically depend Using generalized αvectors LAPSI MPSI straightforward Both methods use estimate 17 choose action agent k The difference methods lies policy πk hypothesized agents needed track belief bk compute αvectors In MPSI πk taken reduced policy j cid12 k optimal policy corresponding MDP M j DecSIMDP In obtained individual policies π j LAPSI πk obtained optimal policy underlying MMDP ignoring component k For illustration purposes apply methods problems different dimension 17 estimate POMDP optimal Q function As discussed estimate 17 corresponds variation Q MDP heuristic Our results indicate suboptimal POMDP solver LAPSI able attain performance close optimal test scenarios incurring computational cost similar solving underlying MMDP MPSI computationally eﬃcient lead agents excessively avoid interactions We compare algorithms previous algorithm DecSIMDPs IDMG algorithm 30 Our results indicate LAPSI able attain similar performance IDMG providing signiﬁcant computational savings We IDMG algorithm design unable consider future interactions planning outside interaction areas leads poor performance Such limitation present LAPSI 44 Results To gain better understanding applicability general properties methods compared performance MPSI LAPSI individual agents plan disregarding agents environment optimal fully observable MMDP policy We tested performance IDMG algorithm previously introduced heuristic planning algorithm DecSIMDPs 30 It empirically shown algorithm attains nearoptimal performance scenarios use results obtained approach benchmark compare performance LAPSI MPSI We brieﬂy IDMG algorithm outline main differences approach presented article 441 The IDMG algorithm Let Γ Mk k 1 N X I MI 1 M denote N agent DecSIMDP Let Q function associated MDPs Mk Γ Similarly let Q I MMDP MI follows Γ IDMG proposes use sets functions Q k denote optimal Q denote joint optimal Q function associated 1 M k k 1 N Q I At time step t agent k involved interaction follows greedy policy respect Q πkxk arg max akAk Q k xk ak Otherwise let O denote set agents interacting k time step t For k cid7 O x X let k Q kcid7 x Q kcid7 xkcid7 akcid7 Mcid2 xK aK IXKi Q I xK i1 Q kcid7 x combines Q For x X state x Since functions different agent k constructs matrix game cid4 Γ Xt cid5 O Akcid7 qkcid7 kcid7 x joint Q functions associated interactions agent k cid7 cid7 O timestep t IDMG adopts gametheoretic approach involving agents O Each payoff function qkcid7 given qkcid7 O rkcid7 xkcid7 akcid7 Mcid2 i1 xK aK IXKi r I xK γ cid2 y O XO PxO O y O Nashkcid7 cid4 Q kcid7 y O k cid7 O cid5 Xt x Nashkcid7 maxlike operator denoting Nash value agent k respect game deﬁned matrices Q kcid7 state y O Finally time step t agent k involved interaction chooses action according Nash equilibrium game Γ Xt cid7 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1773 Table 1 Distinction different methods comparison terms planning prescribed policies Policy IDMG MPSI LAPSI Indiv Opt Planning Equilibrium MDP hyp MMDP hyp MMDP In int areas Equilibrium POMDP POMDP MDP MMDP Outside int areas MDP POMDP POMDP MDP MMDP There important distinctions IDMG algorithm LAPSIMPSI Outside interaction areas IDMG oblivious existence agents environment considers effect future interactions performance This soon apparent leads situations IDMG perform arbitrarily poor A second signiﬁcant difference concerns use gametheoretic solution resolve interactions requires computation multiple equilibria Computing equilibria known computationally demanding problem lead signiﬁcant computational advantage LAPSIMPSI apparent results Table 1 summarizes main distinctions different methods comparison For method second column indicates policy hypothesized agents planning The column corresponds policy prescribed method interaction areas column corresponds policy prescribed outside interaction areas During planning IDMG assumes agents adopt equilibrium solution interaction areas prescribes policy areas Outside interaction areas IDMG follows individual MDP policy disregards agents During planning LAPSI MPSI assume agents follow joint MMDP individual MDP policies respectively Both methods construct POMDP planning policy followed inside outside interaction areas Individual agents consider agents planning follow individual MDP policy The optimal agents assume agents follow joint MMDP policy planning adopt policy 442 Experimental setup Fig 3 depicts different scenarios test algorithm The reason navigation scenarios DecSIMDP model appears particularly appealing modeling multirobot problems Furthermore class problems results easily visualized interpreted In test scenarios robot set twofour robots reach speciﬁc state In smaller environments Fig 3a 3d goal state marked boxed number corresponding number robot The cells simple number correspond initial states robots In larger environments Fig 3e 3j goal robot marked cross robots depart robots goal state attempt increase possibility interaction Each robot 4 possible actions robot corresponding direction probability 08 fail probability 02 leaving state robot unchanged The shaded regions correspond interaction areas inside darker cells correspond interaction states When robots stand cells simultaneously penalty 20 Upon reaching corresponding goal agent receives reward 1 In experiments γ 095 Table 2 summarizes dimension joint state space corresponding DecMDP For comparison purposes Table 2 includes number interaction states number states individual MDPs idea number states corresponding DecSIMDP model 443 Description discussion results For different scenarios Fig 3 run algorithms Table 1 test obtained policies 1000 independent Monte Carlo trials Each trial lasts 250 time steps Except individual agents methods able completely avoid miscoordinations tested scenarios Table 3 summarizes average number miscoordinations individual agents The results Table 3 provide hint coordination needs different environments For example environments cit suny require coordination Map 4 Fig 3d requires signiﬁcant coordination Tables 4 5 present comparative results terms total discounted reward stepstogoal Performance results IDMG algorithm Map 4 Fig 3d available complexity particular instance prevented method providing solutions practical computational time The LAPSI algorithm performs close optimal MMDP policy environments spite signiﬁcant dif ference terms state information available methods Also scenarios LAPSI IDMG perform similarly terms total discounted reward terms stepstogoal The exceptions Map 2 LAPSI performs IDMG ISR IDMG outperforms LAPSI Interestingly difference terms timetogoal ISR environment signiﬁcant Our results agree previous ones showed IDMG attains closetooptimal performance scenarios considered 30 Interestingly ISR scenario individual agents actually able outperform DecSIMDP planning methods Observing results terms stepstogoal conclude particular scenario DecSIMDP plan ning methods longer reach goal indication attempting avoid miscoordinations cost 1774 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 Fig 3 Environments experiments Table 2 Dimension different test scenarios The number joint states scenario involving N agents environment described map M cells given M N Environment Joint states States inter areas Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny 441 1296 400 65536 4900 17689 1849 2401 2704 5476 9 36 18 1188 153 714 203 192 113 287 MDP states 21 2 36 2 20 2 16 4 70 2 133 2 43 2 49 2 52 2 74 2 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1775 Table 3 Number miscoordinations individual agents test scenarios The results averaged 1000 independent Monte Carlo tri als For environment bold entries correspond optimal values differences optimal statistically signiﬁcant Environment Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny Indiv 0409 0239 0280 1943 0000 0249 0004 0010 0120 0000 Table 4 Total discounted reward different algorithms test scenarios The results averaged 1000 independent Monte Carlo trials For environment bold entries correspond optimal values differences statistically signiﬁcant Italic entries correspond values differences statistically signiﬁcant Environment Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny IDMG 12035 10672 13722 11178 2839 14168 6663 16031 11161 MPSI 11130 10159 13249 15384 11105 2688 13937 6641 15162 11130 LAPSI 11992 10947 13701 15564 11126 2824 13997 6648 15976 11139 Indiv 5919 7423 9378 4741 11120 0982 14301 6628 14167 11144 Opt 12059 11108 13837 16447 11128 2840 14407 6705 16016 11149 Table 5 Stepstogoal different algorithms test scenarios The results averaged 1000 independent Monte Carlo trials For environment bold entries correspond optimal values differences statistically signiﬁcant Italic entries correspond values differences statistically signiﬁcant Environment Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny IDMG 11021 13368 8450 12422 39338 7993 22578 5348 12448 MPSI 12752 14433 9282 6088 12552 49341 8986 24507 19684 12500 LAPSI 11091 12828 8477 6071 12514 39444 8012 22618 5416 12487 Indiv 9964 12494 7529 6292 12526 38880 7500 22329 5008 12477 Opt 10977 12546 8267 5001 12512 39340 7440 22444 5365 12469 reaching goal later slightly impacting performance However scenarios performance individual agents signiﬁcantly worse methods The difference noticeable scenarios coordination critical Another interesting observation MPSI typically performs worse methods Since agent MPSI considers agents disregard consequences miscoordinations focused individual goal expected agent following MPSI cautious observed longer time goal To consider example environment Map 1 agents simultaneously cross narrow passage If agents ﬁnd opposite sides passage way In MPSI agent k assumes agent deviate agent k cautiously deviate This means case agents deviate Both agents eventually narrow passage cautious decisionprocess causes delays impacts total reward received seen Table 5 In LAPSI agents implicitly following coordinated policies delays observed MPSI occur In results difference performance LAPSI IDMG optimal MMDP policy occurs terms total discounted reward terms stepstogoal Given discount factor γ explains 1776 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 Fig 4 Computation time different algorithms function problem dimension corresponding number joint states reported second column Table 2 agents longer reach goal corresponding reward discounted The results indicate algorithms IDMG algorithm require time reach goal conﬁguration needed MMDP solution time spent avoiding penalties Also choice interaction areas greatly inﬂuences ability algorithms avoid penalties incurring delays reaching goal6 Since IDMG requires computation equilibria offline planning phase online running phase computational complexity IDMG algorithm quickly prohibitive scenarios large action 6 This reported 30 concerning IDMG algorithm FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1777 Fig 5 Example scenario avoiding interaction beneﬁcial Table 6 Estimated error bounds different environments Environment Map 1 Map 2 Map 3 cit cmu isr mit pentagon suny MPSI 7248 9826 13019 38153 98059 78229 54701 21586 54856 LAPSI 3683 5717 6436 20792 44586 44406 32704 21369 28723 spaces andor interaction areas We compare computational effort methods IDMG terms average offline computation time online computation time Fig 4 summarizes results Both MPSI LAPSI signiﬁcantly computationally eﬃcient IDMG algorithm according performance metrics It interesting note average computation times evolve dimension problem computation time MPSI LAPSI grows linearly dimension problem computation time IDMG depends number interaction areas irregular growth pattern observed Fig 4 Finally IDMG method construction unable consider future interactions planning action noninteraction area In sense IDMG algorithm myopic interactions handles reaches interaction area negative impact performance method Consider scenario depicted Fig 5 Once robots reach marked states avoiding simul taneously crossing narrow pathways We model problem DecSIMDP sets shaded cells represent interaction areas robots nonzero penalty standing simultaneously darker state In environment ignoring interaction Robot 1 reach goal narrow pathways trajectories length However Robot 2 use upper pathway signiﬁcantly faster lower pathway By IDMG algorithm Robot 2 goes upper pathway Robot 1 chooses randomly example upper pathway In case according IDMG algorithm robots reach interaction area simultaneously Robot 1 way Robot 2 Therefore Robot 1 takes 10 steps reach Goal 1 Robot 2 takes 8 steps reach Goal 2 average 9 steps reach goal If instead Robot 1 takes lower pathway robots reach goal states 8 steps The IDMG algorithm chooses possibilities randomlyor way differentiate Therefore IDMG agents average 85 steps reach goal We ran 1000 independent trials IDMG algorithm scenario obtained average 8485 stepstogoal standard deviation 05 For comparison purposes ran 1000 independent trials LAPSI algorithm scenario Out 1000 trials Robot 1 picked lower pathway group took average 8 stepstogoal variance 0 This difference arbitrarily large increasing narrow doorway arbitrary number states causing arbitrarily large delay We conclude section providing Table 6 value bounds Theorem 43 test environments Although bounds generally loose provide insights properties methods The error bounds MPSI general larger LAPSI policy estimate MPSI scenarios crude LAPSI Moreover bounds exhibit dependency dimension problemlarger scenarios tend larger boundsthis case For example Map 3 smaller Map 2 corresponding bounds actually larger Also ISR smaller CMU corresponding bounds approximately similar 1778 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 5 Learning interaction areas In previous sections introduced DecSIMDP model proposed MPSI LAPSI planning algorithms able leverage sparse interactions DecSIMDPs overcome computational burden associated general decisiontheoretic models The DecSIMDP model relies concept interaction areas capture local pendences groups agents These interaction areas include noninteraction states improve coordination performance agent group We introduce learning algorithm learns interaction areas We start considering simple case 2agent transitionindependent MMDP given M 2 X Ak P r γ r decomposable rx r1x1 a1 r2x2 a2 r I x rk corresponds individual component reward function associated agent k r I joint com ponent reward function If r I 0 agent use standard Q learning learn optimal individual policy π k policy π π 2 optimal M However r I x cid12 0 state x policy learned agent Q learning disregarding existence agent generally suboptimal 3132 The optimal policy case account agent 1 π When moving DecMDP setting situation r I 0 poses additional diﬃculties problem decomposed independent singleagent MDPs If interaction agents sparse expect agent coordinate states x r I x cid12 0the interaction areas This coordination require agent access state action information agent Therefore interaction areas comprise states information joint state brings improvements performance localstate information In general performance obtained joint state information time steps worse obtained information certain situations However interested determining interaction areas interested states information brings improvement performance To purpose add artiﬁcial penalty time agent uses joint state information ensure actually useful We augment individual action space agent pseudoaction Coordinate action incurs aforementioned penalty consists steps 1 An active perception step agent tries determine local state information agent 2 A coordinating step agent makes use local state information agent available choose primitive actions The active perception step Coordinate action succeed actually succeeds environment dependent Considering multirobot navigation scenario active perception step consist example use onboard camera localize robot In case robot able localize robot ﬁeldofview Another possibility consists use explicit wireless communication robot requires robot share location Going algorithm agent k uses standard Q learning estimate Q k xk ak local states xk ak Ak Coordinate The role coordinating step use local state information agent provided active perception step guide actual choice actions Ak To purpose agent k keeps estimate second Q function interaction Q function Q I k deﬁned terms immediate reward agent k k different stateaction pairs value agent ks policy Since policy deﬁned terms Q independent k values Q I k x rkx ak γ Q I cid2 ykXk kxk yk max Pa bkAk Q k yk bk The relation Q learninglike update estimate value Q I ak Ak joint state x1 x2 k associated individual action Algorithm 2 summarizes algorithm πcid8 learning policy policy ensures suﬃcient exploration εgreedy policy πgQ greedy policy respect function Q The ﬂag ActivePercept true cid7 denotes general Q learning active perception step successful general instruction QLUpdateQ x r y Q update deﬁned Section 21 The update Q I k uses estimates Q k Hence values Q I k Q I k determine step behavior Coordinate action direct dependency entries Q I k corresponding different statesactions Fig 6 Such independence particularly useful implies unlike algorithms learn directly joint stateaction space matrix Q I k sparseie number nonzero elements smalland requires similar sample complexity necessary learn Q k individual action step depends values Q k FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1779 k Q I k ˆAkt πg Q I Choose Akt πcid8 Akt COORDINATE ActivePercept TRUE Algorithm 2 Learning algorithm agent k Require Learning policy πcid8 1 Initialize Q 2 Set t 0 3 FOREVER 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 end end Sample Rkt Xkt 1 ActivePercept TRUE Xkt Akt Rkt Xkt 1 Q end QLUpdateQ t t 1 Sample Rkt Xkt 1 k Xt k Xkt QLUpdateQ I k ˆAkt πg Q end k k Xt ˆAkt Rkt Xkt 1 Q k Fig 6 Illustration dependence global Q function Q k local Q function Q I k Also Coordinate action use joint action information joint state information This fact aforementioned independence components Q I k similar generalized ﬁctitious play process 33 However scenarios action information agents necessary coordination algorithm exhibit poor performance k associated different xk makes learning Q I So far described apply learning algorithm 2agent MMDPs In MMDPs N 2 agents local state information agents arises active perception step agent perceive local state information concerning agent Therefore problems coordination requires state infor mation concerning agents performance algorithm expected decrease Another implication apply Algorithm 2 modiﬁcation disregarding identity agent merely consid ering local information obtained active perception concerns agent However consider general active perception processesie agent actually able perceive state agentsthe algorithm trivially modiﬁed address coordination number agents obvious tradeoff terms memory requirements algorithm 51 Results We apply learning algorithm test scenarios Fig 3 include individual penalty 01 scenarios time agent uses Coordinate action The excessive use Coordinate action impacts sparsity Q I k undesirable large problems We run learning algorithm test environments Table 7 summarizes number learning steps allowed environments During learning exploration ensured combining greedy policy optimistic initial values 34 We use initial learning rate α 01 Every 2 104 time steps rate updated α 01 α For comparison purposes run One instance singleagent Q learning robot absence robots environment The robots learn reach goal avoid miscoordination robots envi ronment learning stage learner actually experiences miscoordination penalty One instance singleagent Q learning robot presence robots environment The robots learn best reach goal avoiding interaction individual state information Due 1780 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 Table 7 Number timesteps learning test environments Environment Learning steps Map 1 Map 2 Map 3 cit cmu isr mit pentagon suny 104 105 104 5 105 2 105 5 105 2 105 3 105 2 105 Table 8 Distinction different learning approaches comparison The second columns indicate learning test conducted presence absence agents The column indicates state observability Approach Indiv Noncoop Coop Opt Learning Single Multiple Multiple Multiple Test Multiple Multiple Multiple Multiple State observability Local Local Local Active perc Global Table 9 Total discounted reward different algorithms test scenarios The results averaged 1000 independent Monte Carlo trials For environment bold entries correspond optimal values differences statistically signiﬁcant Environment Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny Indiv 2826 1607 5470 3180 9790 1319 10695 6554 14676 11167 Noncoop 3046 4487 7002 0000 11002 0910 14296 6666 12918 11104 Coop 6304 9957 7171 4229 11112 1630 14258 6502 16554 11135 Opt 12059 11108 13837 16447 11128 2840 14407 6705 16016 11149 general lack knowledge global state agents second comparison set correspond independent learners sense 31 Table 8 summarizes information The second columns learning testing conducted presence absence agents corresponding labels Multiple Single respectively The column indicates agents access local state information global state information case method local state information complemented state information active perception step Coordinate action We evaluate learned policies different environments performing 1000 independent Monte Carlo trials Each trial lasts 250timesteps learned policy Indiv Noncoop Coop evaluated original problem twofour robots moving simultaneously environment For comparison present results obtained optimal fully observable MMDP policy Section 4 However framework considered different considered Section 4 results optimal MMDP policy direct comparison reference better assess performance learning methods Table 9 presents results terms total discounted reward The results Table 9 algorithm generally outperforms learning methods matching optimal MMDP policy test scenarios However behavior different algorithms varies different scenarios For example individual agents experience miscoordination penalties learning Therefore act penalty exist impacts negatively total discounted reward receive seen example Map 2 The noncooperative agents experience miscoordination penalties learning better able avoid lack joint state information prevents attaining optimal performance In particular Map 4 environment adopt cautious policies prefer avoiding penalties attaining agents goals Such cautious policies explain total discounted reward 000 steps goal FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1781 Table 10 Stepstogoal different algorithms test scenarios The results averaged 1000 independent Monte Carlo trials For environment bold entries correspond optimal values differences statistically signiﬁcant Numbers parentheses indicate agents reached goal Environment Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny Indiv 10862 12900 8303 8244 13542 40100 8216 24100 5300 12441 Noncoop 10841 12800 8154 25000 13500 39800 8211 24600 5500 12558 Coop 10821 17500 8467 4999 13520 39815 8226 25300 4900 12492 Opt 10977 12546 8267 5001 12512 39340 7440 22444 5365 12539 Table 11 Number crashes different algorithms test scenarios The results averaged 1000 independent Monte Carlo trials For environment bold entries correspond optimal values differences statistically signiﬁcant Environment Map 1 Map 2 Map 3 Map 4 cit cmu isr mit pentagon suny Indiv 0408 0500 0303 1911 0000 0500 0012 0000 0100 0000 Noncoop 0397 0300 0261 0000 0000 0100 0005 0000 0200 0000 Coop 0000 0000 0000 0000 0000 0000 0006 0000 0000 0000 Opt 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 Tables 10 11 number stepstogoal number miscoordinations trial test environments Our algorithm exhibits nearly miscoordinations environments means robots able coordinate choice path However scenarios attained cost having agents reach goal corresponding values parenthesis Table 10 This indicates scenarios coordination require incurring excessive penalty arising Coordinate action agents opt sacriﬁcing reaching goals This interesting aspect learning approach agents tradeoff cost Coordinate action usefulness information provided This contrast individual agents reach goal minimum time cost severe miscoordination penalties experience learning poor performance terms total discounted reward Our results conﬁrm interaction larger environments sparser smaller environ ments In test scenarios cit environment initialgoal positions robots explicit coordination actions necessary avoid miscoordinations Therefore individualistic Q learners noncooperative Q learners able attain good performance In general seen Tables 9 11 environments methods attain similar performance indicating coordination actions necessary Individual Noncoordinated approaches able attain optimal performance The fact algorithm attains similar performance means learns Coordinate action necessary total discounted reward inferior methods We analyze increasing penalties miscoordination affect performance robots scenarios We run learning algorithms different values miscoordination penalty evaluated obtained policies As expected penalty miscoordination increases total discounted reward methods unable avoid miscoordinations Table 11 decreases accordingly accentuating differences observed Table 10 The reliability performance individualistic noncoordinated policiestranslated variance observed performanceis severely affected miscoordination penalty increases Finally noncooperative Q learning algorithm able surpass individualistic Q learning scenarios This particularly evident observing performance miscoordination penalty increases interpreted noncooperative agents having act increasing caution penalties experienced learning In conclusion proposed method able learn coordinate coordination necessary attain good performance In scenarios learning agents resort coordination actions states joint state infor mation leads improved performance states corresponds precisely interaction areas deﬁned Section 3 In scenarios agents able learn tradeoff beneﬁts coordination actions costs actions 1782 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 6 Related work A wide range models proposed formalize decisionmaking problems multiagent systems including stochastic games 35 multiagent MDPs 36 DecPOMDPs 3716 IPOMDPs 38 These models differ scope sumptions computational complexity 26 For example multiagent MDPs solved polynomial time ﬁnitehorizon DecMDP DecPOMDP models NEXPcomplete benign 2agent scenarios complexity results worse noncooperative settings Efforts handle worstcase complexity decentralized multiagent models led approximate methods trade optimality computability 3940 reduced models trade representability computability Several models assume interaction agents simpliﬁed different ways For example transition independent DecMDPs transition probabilities agent depend solely actions local states Transition independent DecMDPs NPcomplete solved Coverage Set algorithm 4127 Our DecSIMDP model viewed extension transitionin dependent DecMDP allows transition probabilities agent dependent actions agents interaction areas joint state observability free instantaneous communication assumed We expect worstcase complexity DecSIMDP transitionindependent DecMDPs general DecMDPs Local interactions exploited multiagent scenarios For example works propose use hierarchical approaches subdivide overall task hierarchy subtasks restricted states actions relevant particular subtask 5424 The subtasks conducted individually agent require local state different agents shared Going hierarchy corresponds moving low level local tasks higherlevel global tasks coordination necessary accounted explic itly Since execution highest level corresponds lowlevel time steps communication needs mini mized Coordination graphs 643 capture local dependences different agents MMDP allowing overall Q function decomposed local Q functions optimized individually Coordination graphs eﬃcient planning learning large multiagent MDPs 44 Although problems coordination graphs applied signiﬁcantly different study interactions agents captured coordination graphs related notion interaction areas DecSIMPD model interesting investigate use graphical structure compactly represent dependencies agents DecSIMDP The coordination graph structure learned experience based concept utile coordination 7 Although fundamentally different approach work relates learning interaction areas methods infer joint state information improve performance agents We differ fact agents explicitly learn tradeoff beneﬁts querying agents states environmentimposed limitations querying process associated cost Our method captures impact communication costs decision process process learning interagent dependencies Both hierarchical approaches coordination graphs discussed exploit local interactions agents able accommodate level partial state observability In gametheoretic literature local dependences players large games explored For example graphical games 9 represent nplayer matrix games undirected graphs players correspond nodes payoff node depends actions direct neighbors graph Actiongraph games generalize concept graphical games exploring sparser dependences players game 10 Multiple algorithms proposed compute Nash equilibria class games relying continuation methods 45 known solution simple game gradually perturbed solution desired game Continuation methods run time exponential indegree actiongraph number nodes Therefore games context speciﬁc independences yield sparsely connected actiongraph games leading exponential savings computational time In classes problems additional structure corresponding actiongraph games lead eﬃcient computation equilibria 4647 The main concept DecSIMDP model originally proposed designation interactiondriven Markov games 30 associated IDMG algorithm Our contributions paper extend original IDMG formulation aspects We formalize relation DecMDPs DecSIMDPs introducing concepts interaction ar eas interaction states We presented general heuristic planning algorithms corresponding convergence analysis error bounds overcome limitations original IDMG algorithm outside interaction areas LAPSI MPSI reason future interactions unlike IDMG considers interactions actually occur ii interaction areas LAPSI MPSI offer computational advantages IDMG requires computation equilibria Other closely related models DecSIMDP model explore eventdriven interactions 4850 dis tributed POMDPs coordination locales DCPLs 13 Particularly DCPLs agent assumed independent agents previously speciﬁed coordination locales As IDMG algorithm proposed TREMOR algo rithm DCPLs models agent k POMDP model solved yield policy πk agent Coordination locales handled modifying POMDP model agent taking policies agents account Instead approach assumes interactions fully observable coordination FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1783 More recently informationtheoretic measure interagent inﬂuence proposed designation inﬂuence gap 28 indicates actions agent determine actions agents optimal policy Such inﬂuence gap attempts quantifying level dependence different agents As expected larger inﬂuence gaps corresponding smaller interagent inﬂuence typically translate computational complexity While inﬂuence gap measure global interagent inﬂuence interaction areas capture local interac tions agents Larger numerous interaction areas typically lead harder problems As future work interesting relate number size interaction areas DecSIMDPs terms proposed inﬂuence gap assess potential usefulness DecSIMDP model particular problems 7 Conclusion future work In work analyzed local interactions multiagent simplify process decen tralized decision making We introduced DecSIMDPs new decisiontheoretic model decentralized multiagent systems sparse interaction explicitly distinguishes situations agents team coordinate act independently We formalized relation DecMDPs DecSIMDPs establishing DecSIMDPs particular instances DecMDPs We presented MPSI LAPSI general heuristic planning approaches reduce planning DecSIMDP sequence planning problems singleagent POMDPs We analyzed particular instances MPSI LAPSI derived bounds quality obtained policies Finally presented RL algo rithm learn interaction areas DecSIMDP We illustrated application algorithms paper multiagent navigation scenarios Both instances MPSI LAPSI experimental results rely having agent track agents environment belief vector choose actions The difference algorithms lies assumed policy agents MPSI assumes agents completely driven individual goals discarding interaction In cases interactions negative MPSI agents act cautiously In contrast LAPSI agent assumes agents teamplayers choose actions common goal group Hence LAPSI agent adopts policy closer actual optimal fullyobservable policy The LAPSI algorithm successfully leverages particular independences different agents attain eﬃcient nearoptimal performance In MPSI LAPSI modeling strategies abstract decision process agent singleagent decision processnamely POMDP Although illustrated methods Q MDPlike approach principle POMDP solver The differences MPSI LAPSI provide additional information deﬁning interaction areas While MPSI relies optimal policies individual MDPs DecSIMDP model LAPSI relies joint policy underlying MMDP Since outside interaction areas expect actions different agents approximately independent interactions areas estimated policies individual MDPs joint MMDP disagree This provides recipe choosing interaction states individual stateinformation suﬃcient determine best action The results learning algorithm open interesting questions One ﬁrst issue concerns dependence performance algorithm cost Coordinate action In fact observed agents able learn tradeoff beneﬁts arising good coordination cost coordination Such tradeoff similar problem exchanging reward information arising POMDPs 22 interesting analyze tradeoff inﬂuenced cost Coordinate action tradeoff extends situations partial state observability considered Another aspect open future investigation regards performance guarantees algorithm As remarked Section 5 parallel learning process taking place agent executes Coordinate action bears signiﬁcant semblances generalized ﬁctitious play It interesting analyze convergence guarantees ﬁctitious play translated particular learning algorithm The scenarios work learning algorithm exhibited interesting performance focused localized coordination based underlying assumption ad vantages model learning As coordination increases global level learning algorithm need adapted attain desired performance In view completely independent way agents learn remains investigate learning algorithm applicable different reward functions different agents scenarios agents completely independent terms dynamics exhibit weak couplingfor example states interaction occurs It interesting explore ideas general models DecPOMDPs alleviating requirement local state observability Acknowledgements The authors like acknowledge thorough comments anonymous reviewers signiﬁcantly tributed improve quality presentation helpful discussions Matthijs Spaan This research partially sponsored project CMUPTSIA00232009 Carnegie Mellon Portugal Program Informa tion Communications Technologies Institute Portuguese Fundação para Ciência e Tecnologia The views conclusions contained document authors 1784 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 Appendix A Proofs A1 Proof Theorem 41 We generalized αvectors computed convergent dynamicprogramminglike approach iterating recurrent expression 16 In DecSIMDP verifying conditions theorem generalized αvector αk actually X Ak matrix component x ak given αkx ak For general matrix X Ak matrix W let Tk operator deﬁned Tk W x ak rπk x ak γ Pπk x ak y max uk W y uk γ max uk cid2 yXI cid2 Pπk x ak yW y uk y XI Tk W x ak denotes element x ak matrix Tk W The operator Tk closely related Bellman operator H introduced 3 We establish assertion theorem showing Tk contraction supremum norm In fact cid14Tk W 1 Tk W 2cid14 max xak cid4 γ max xak cid6 cid6 cid6Tk W 1x ak Tk W 2x ak cid6 cid2 y Pπk x ak y max uk cid6 cid6 cid6W 1 y uk W 2 y uk cid6 inequality follows Jensens inequality implying cid6 cid6 cid6W 1x ak W 2x ak cid6 γ cid14W 1 W 2cid14 cid14Tk W 1 Tk W 2cid14 cid4 γ max xak We shown Tk contraction supremum norm implies Tk unique ﬁxedpoint corresponding generalized αvectors Tk compute generalized αvectors dynamicprogramminglike fashion update rule αn1 k x ak cid5 cid4 Tkαn k x ak αn k denotes nth estimate αkx ak cid2 A2 Proof Theorem 42 We problem computing generalized αvectors DecSIMDP verifying conditions theorem equivalent terms complexity solving MDP dimension depends polynomially dimension original DecSIMDP In particular computing generalized αvectors DecSIMDP equivalent computing optimal Q function MDP Since MDPs known Pcomplete 25 desired result follows We rewrite 16 αkx ak rπk x ak γ cid2 yXI Pπk x ak y max uk cid2 αk y uk γ cid2 y XI Pπk x ak y max uk ηxak z XI Pπk x ak zαkz uk A1 ηxak cid19 1 y XI Pπk x ak y We construct MDP ˆM ˆX Ak ˆP ˆr ˆX X X Ak ˆPˆx ak ˆy Pπk ˆx ak ˆy 1ηˆxak cid19 ηzuk ηzuk Pπk z uk yη yak 0 y XI Pπk z uk yPπk y ak ˆy ˆx X ˆy XI ˆx X ˆy ˆx ak ˆx z uk ˆy XI ˆx z uk ˆy y ak y XI These probabilities deﬁned ˆx X cid2 y XI cid26 ˆy ˆx z uk cid2 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1785 cid2 ˆPˆx ak ˆy cid2 ˆyXI Pπk ˆx ak ˆy 1ηˆxak cid2 ˆyXI Pπk ˆx ak ˆy Pπk ˆx ak y 1 ˆPˆx ak ˆy ηzuk ˆy cid2 y XI cid25 cid2 Pπk z uk y ˆyXI Pπk y ak ˆy 1η yak ηzuk cid2 y XI Pπk z uk y 1 The reward function cid27 ˆM rπk ˆx ak cid19 ηzuk y XI ˆrˆx ak Pπk z uk yrπk y ak ˆx X ˆx z uk The optimal Q function MDP veriﬁes recursive relation 2 Q ˆx ak ˆrˆx ak γ ˆPˆx ak ˆy max uk ˆy uk Q cid2 ˆy ˆX For ˆx X replacing deﬁnitions ˆP ˆr Q ˆx ak rπk ˆx ak γ cid2 ˆyXI Pπk ˆx ak ˆy max uk Q ˆy uk γ ηˆxak Q max uk cid4 ˆx ak uk cid5 Similarly ˆx z uk Q ˆx ak ηzuk cid2 y XI Pπk z uk y cid10 rπk y ak γ cid2 ˆyXI Pπk y ak ˆy max uk ˆy uk Q cid4 Q y ak uk cid11 cid5 γ η yak cid2 max uk ηzuk Pπk z uk yQ y ak A2 A3 y XI Replacing A2 A3 yields Q ˆx ak rπk ˆx ak γ cid2 ˆyXI Pπk ˆx ak ˆy max uk Q ˆy uk γ ηˆxak max uk ηˆxak cid2 y XI Pπk ˆx ak yQ y uk A1 As computing optimal Q function MDP ˆM compute generalized αvectors original DecSIMDP αkx ak Q x ak Since dimension new MDP grows linearly dimension generalized αvectors dimension corresponding DecSIMDP statement theorem follows cid2 A3 Proof Theorem 43 For general MDP M X A P r γ ˆπ greedy policy respect function ˆQ ˆπ x arg max aA ˆQ x x X cid24 cid24 ˆπ V cid24 cid24V cid4 2γ 1 γ 2 BE ˆQ A4 BE ˆQ Bellman error associated function ˆQ 7 cid2 cid6 cid6 cid6 cid6rx γ BE ˆQ sup xa cid6 cid6 cid6 ˆQ y u ˆQ x cid6 Px y max u y 7 This fact follows Proposition 61 18 1786 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 In DecSIMDP setting assuming policy πk ﬁxed known decision process agent ks perspective standard POMDP Since POMDP recast equivalent belief MDP follows relation A4 holds POMDPs Writing Bellman error general POMDP X A Z P O r γ yields BE ˆQ sup ba cid10 rx γ cid6 cid6 cid6 cid6 cid2 bx x cid2 z y Px yO y z max u cid5 cid4 ˆQ cid7 za u b cid11 cid6 cid6 cid6 ˆQ b ak cid6 For simplicity notation consider Bellman error b BE ˆQ b cid10 rx γ bx cid2 z y Px yO y z max u cid4 ˆQ cid7 za u b For POMDP perceived agent k DecSIMDP setting BE ˆQ b ak cid10 rπk x ak γ bxk cid2 z y Pπk x ak yO y z max u cid11 cid5 cid6 cid6 cid6 ˆQ b cid6 cid4 ˆQ b cid7 zak u cid11 cid5 cid6 cid6 cid6 ˆQ b ak cid6 replacing deﬁnitions ˆQ b ak generalized αvectors yields leads BE ˆQ b ak γ cid2 xO y O cid2 bxO Pπk xO ak y O max uk cid2 xO yO cid2 bxO bxO Pπk xO ak y O max uk yO x y O bxO Pπk xO yO αk y uk cid6 cid6 cid6 Pπk xO yO αk y uk cid6 cid6 cid2 cid6 cid6 cid6 x cid6 cid2 cid6 cid6 cid6 xk cid6 cid6 cid6 cid6 notation introduced Section 42 Letting ΛkxO y O uk cid2 Pπk xO yO αk y uk yO BE ˆQ b ak cid4 γ max y O cid6 cid6 cid6 cid6max uk cid2 xO bxO ΛxO y O uk cid2 xO bxO max uk cid6 cid6 cid6 cid6 ΛxO y O uk In order bound righthand expression need auxiliary results generalize bounds 51 case functions deﬁned Rn independent se Lemma 1 Let xk k 1 M set points Rn ﬁnite n βk k 1 M set corresponding weights verifying 0 cid4 βk cid4 1 k 1 M cid19 k βk 1 Let f Rn R convex function Then holds cid26cid11 cid10cid2 cid25cid2 cid25cid2 cid26 xkM A5 βkxk cid4 β f xk M f k k k cid2 k βk f xk f β max k βk Proof The proof essentially follows Lemma 1 51 Let k cid25cid2 cid25cid2 cid2 cid26 cid26 cid5 βk f xk f βkxk k cid2 β M f xkM k cid4 β kcid12k β βk Eq A5 rewritten β βk Mβ f xk 1 Mβ f cid25cid2 cid26 βkxk cid2 f cid25cid2 cid26 xkM k k equivalently cid2 Since kcid12k cid2 kcid12k β βk Mβ 1 Mβ 1 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1787 cid2 kcid12k β βk Mβ f xk 1 Mβ f cid26 βkxk cid2 f cid25cid2 k cid25 cid2 kcid12k β βk Mβ xk 1 Mβ cid2 k cid26 βkxk f cid25cid2 cid26 xkM k ﬁrst inequality follows Jensens inequality implying A5 cid2 Corollary 1 Let xi 1 N set points Rn ﬁnite n pi 1 N corresponding sequence weights verifying 0 cid4 pi cid4 1 pi 1 Let U denote closed convex set represented convex hull set points ak k 1 M Rn Let f U R convex function Then holds cid19 cid2 pi f xi f cid25cid2 cid26 pi xi cid4 cid2 k f ak M f cid25cid2 cid26 akM k Proof Each xi written A6 xi cid2 k λikak 0 cid4 λik cid4 1 cid19 k λik 1 1 n Then cid25cid2 cid25cid2 cid2 cid26 pi f λikak cid2 pi f xi f pi xi cid26 f cid25cid2 cid2 cid26 pi λikak cid2 cid4 f ak k cid2 cid25cid2 k cid2 cid26 ak piλik piλik f k k Letting βk cid19 piλik cid2 pi f xi f cid25cid2 cid26 pi xi cid4 Finally applying Lemma 1 cid25cid2 cid2 pi f xi f cid26 pi xi cid4 cid2 k cid2 βk f ak f cid25cid2 cid26 βkak k βk f ak f cid25cid2 cid26 βkak cid10cid2 k cid4 β k k cid25cid2 f ak M f cid26cid11 akM cid2 cid4 k f ak M f cid25cid2 k cid26 akM k cid2 The bounds differ A5 depends function f set points xk k 1 M A6 depends function f set U For given u k alphavectors αk y u XO y O Then Corollary 1 XO holds Λkx O y O O u k lies convex hull set Ak x O k y O y cid6 cid6 cid6 cid6max uk cid2 yO BE ˆQ b ak cid4 γ max y O cid4 αk y O yO uk cid2 cid5 yO cid4 αk max uk y O yO uk cid5 cid6 cid6 cid6 cid6 ﬁnally yielding cid24 cid24V V πk cid24 cid24 cid4 2γ 2 1 γ 2 max y O cid6 cid6 cid6 cid6max uk cid4 αk cid2 yO y O yO uk cid2 cid5 yO max uk cid4 y O yO uk cid5 αk cid6 cid6 cid6 cid6 cid2 1788 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 References 1 N Meuleau M Hauskrecht K Kim L Peshkin L Kaelbling T Dean C Boutilier Solving large weakly coupled Markov decision processes Proc 15th AAAI Conf Artiﬁcial Intelligence 1998 pp 165172 2 S Singh D Cohn How dynamically merge Markov decision processes Advances Neural Information Processing Systems 10 1998 10571063 3 R Nair M Tambe Hybrid BDIPOMDP framework multiagent teaming Journal Artiﬁcial Intelligence Research 23 2005 367420 4 P Stone M Veloso Teampartitioned opaquetransition reinforcement learning Proc RoboCup98 1998 pp 206212 5 M Ghavamzadeh S Mahadevan R Makar Hierarchical multiagent reinforcement learning Journal Autonomous Agents Multiagent Sys tems 13 2 2006 197229 6 C Guestrin D Koller R Parr Multiagent planning factored MDPs Advances Neural Information Processing Systems 14 2001 15231530 7 J Kok P Hoen B Bakker N Vlassis Utile coordination learning interdependencies cooperative agents IEEE Symp Computational Intelli gence Games 2005 pp 6168 8 M Roth R Simmons M Veloso Exploiting factored representations decentralized execution multiagent teams Proc 6th Int Conf Au tonomous Agents Multiagent Systems 2007 pp 469475 9 M Kearns M Littman S Singh Graphical models game theory Proc 17th Conf Uncertainty Artiﬁcial Intelligence 2001 pp 253260 10 A Xin Jiang K LeytonBrown N Bhat Actiongraph games Tech rep TR200813 Univ British Columbia 2008 11 M Allen S Zilberstein Complexity decentralized control special cases Advances Neural Information Processing Systems 22 2009 1927 12 C Goldman S Zilberstein Decentralized control cooperative systems categorization complexity analysis Journal Artiﬁcial Intelligence Re search 22 2004 143174 13 P Varakantham J Kwak M Taylor J Marecki P Scerri M Tambe Exploiting coordination locales distributed POMDPs social model shaping Proc 19th Int Conf Automated Planning Scheduling 2009 pp 313320 14 M Puterman Markov Decision Processes Discrete Stochastic Dynamic Programming John Wiley Sons Inc 1994 15 L Kaelbling M Littman A Cassandra Planning acting partially observable stochastic domains Artiﬁcial Intelligence 101 1998 99134 16 D Bernstein R Givan N Immerman S Zilberstein The complexity decentralized control Markov decision processes Mathematics Operations Research 27 4 2002 819840 17 C Watkins Learning delayed rewards PhD thesis Kings College Cambridge Univ 1989 18 D Bertsekas J Tsitsiklis NeuroDynamic Programming Athena Scientiﬁc 1996 19 R Smallwood E Sondik The optimal control partially observable Markov processes ﬁnite horizon Operations Research 21 5 1973 1071 1088 20 O Madani S Hanks A Condon On undecidability probabilistic planning inﬁnitehorizon partially observable Markov decision problems Proc 16th AAAI Conf Artiﬁcial Intelligence 1999 pp 541548 21 A Cassandra Exact approximate algorithms partially observable Markov decision processes PhD thesis Dept Computer Sciences Brown Univ 1998 22 D Aberdeen A revised survey approximate methods solving partially observable Markov decision processes Tech rep National ICT Australia 2003 23 M Littman A Cassandra L Kaelbling Learning policies partially observable environments scaling Proc 12th Int Conf Machine Learning 1995 pp 362370 24 F Melo M Ribeiro Transition entropy partially observable Markov decision processes Proc 9th Int Conf Intelligent Autonomous Systems 2006 pp 282289 25 C Papadimitriou J Tsitsiklis The complexity Markov decision processes Mathematics Operations Research 12 3 1987 441450 26 S Seuken S Zilberstein Formal models algorithms decentralized decisionmaking uncertainty Journal Autonomous Agents Multiagent Systems 17 2 2008 190250 27 R Becker S Zilberstein V Lesser C Goldman Solving transition independent decentralized Markov decision processes Journal Artiﬁcial Intelligence Research 22 2004 423455 28 M Allen Interactions decentralized environments PhD thesis Univ Massachusetts Amherst 2009 29 A Cassandra Optimal policies partially observable Markov decision processes Tech rep CS9414 Dept Computer Sciences Brown Univ 1994 30 M Spaan F Melo Interactiondriven Markov games decentralized multiagent planning uncertainty Proc 7th Int Conf Autonomous Agents Multiagent Systems 2008 pp 525532 31 C Claus C Boutilier The dynamics reinforcement learning cooperative multiagent systems Proc 15th AAAI Conf Artiﬁcial Intelligence 1998 pp 746752 32 M Tan Multiagent reinforcement learning independent vs cooperative agents Readings Agents Morgan Kaufman 1997 pp 487494 33 D Leslie E Collins Generalised weakened ﬁctitious play Games Economic Behavior 56 2006 285298 34 R Sutton A Barto Reinforcement Learning An Introduction MIT Press 1998 35 L Shapley Stochastic games Proceedings National Academy Sciences 39 1953 10951100 36 C Boutilier Planning learning coordination multiagent decision processes Theoretical Aspects Rationality Knowledge 1996 pp 195 210 37 D Pynadath M Tambe The communicative multiagent team decision problem analyzing teamworktheories models Journal Artiﬁcial Intelli gence Research 16 2002 389423 38 P Gmytrasiewicz P Doshi A framework sequential planning multiagent settings Journal Artiﬁcial Intelligence Research 24 2005 4979 39 R EmeryMontemerlo G Gordon J Schneider S Thrun Approximate solutions partially observable stochastic games common payoffs Proc 3rd Int Conf Autonomous Agents Multiagent Systems 2004 pp 136143 40 M Roth Executiontime communication decisions coordination multiagent teams PhD thesis Carnegie Mellon University August 2007 41 R Becker S Zilberstein V Lesser C Goldman Transitionindependent decentralized Markov decision processes Proc 2nd Int Conf Autonomous Agents Multiagent Systems 2003 pp 4148 42 R Makar S Mahadevan Hierarchical multiagent reinforcement learning Proc 5th Int Conf Autonomous Agents 2001 pp 246253 43 C Guestrin S Venkataraman D Koller Contextspeciﬁc multiagent coordination planning factored MDPs Proc 18th AAAI Conf Artiﬁcial Intelligence 2002 pp 253259 44 J Kok N Vlassis Sparse cooperative Q learning Proc 21st Int Conf Machine Learning 2004 pp 6168 45 N Bhat K LeytonBrown Computing Nash equilibria actiongraph games Proc 20th Conf Uncertainty Artiﬁcial Intelligence 2004 pp 3542 46 A Xin Jiang K LeytonBrown A polynomialtime algorithm actiongraph games Proc 21st AAAI Conf Artiﬁcial Intelligence 2006 pp 679684 47 A Xin Jiang K LeytonBrown Computing pure Nash equilibria symmetric actiongraph games Proc 22nd AAAI Conf Artiﬁcial Intelligence 2007 pp 7985 48 R Becker V Lesser S Zilberstein Decentralized Markov decision processes eventdriven interactions Proc 3rd Int Conf Autonomous Agents Multiagent Systems 2004 pp 302309 FS Melo M Veloso Artiﬁcial Intelligence 175 2011 17571789 1789 49 R Becker V Lesser S Zilberstein Analyzing myopic approaches multiagent communication Proc IEEE Int Conf Intelligent Agent Technology 2005 pp 550557 50 R Becker A Carlin V Lesser S Zilberstein Analyzing myopic approaches multiagent communications Computational Intelligence 25 1 2009 3150 51 S Simic On global upper bound Jensens inequality Journal Mathematical Analysis Applications 343 2008 414419