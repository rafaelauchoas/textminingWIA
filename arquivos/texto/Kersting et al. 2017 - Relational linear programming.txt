Artiﬁcial Intelligence 244 2017 188216 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Relational linear programming Kristian Kersting CS Department TU Dortmund University Germany b LEARINRIA RhoneAlpes Montbonnot France Martin Mladenov Pavel Tokmakov b r t c l e n f o b s t r c t Article history Received revised form 25 June 2015 Accepted 28 June 2015 Available online 2 July 2015 Keywords Machine learning Optimization Relational logic Statistical relational learning Linear programming Symmetry Fractional automorphism Colorreﬁnement Lifted probabilistic inference Lifted linear programming Equitable partitions Orbit partitions 1 Introduction We propose relational linear programming simple framework combining linear programs LPs logic programs A relational linear program RLP declarative LP template deﬁning objective constraints logical concepts objects relations quantiﬁed variables This allows express LP objective constraints relationally varying number individuals relations enumerating Together logical knowledge base effectively logic program consisting logical facts rules induces ground LP This ground LP solved lifted linear programming That symmetries ground LP employed reduce dimensionality possible reduced program solved offtheshelf LP solver In contrast mainstream LP template languages AMPL features mixture declarative imperative programming styles RLPs relational nature allows intuitive representation optimization problems particular relational domains We illustrate empirically experiments approximate inference Markov logic networks LP relaxations solving Markov decision processes collective inference LP support vector machines 2015 Elsevier BV All rights reserved Modern social technological trends result enormous increase accessible data sig niﬁcant portion resources interrelated complex way having inherent uncertainty Such data refer relational data arises instance social network media mining natural language processing open information extraction web bioinformatics robotics Making data amenable computing machinery typically yields substantial social andor business value Therefore surprising probabilistic logical languages1 currently provoking new AI research tremendous theoretical practical implications By com bining aspects logic probabilitiesa dream AI dating late 1980s Nils Nilsson introduced term probabilistic logics 5they help effectively manage complex interactions uncertainty data However instead looking AI glasses probabilities possible worlds approach optimization That preference relation objective function possible worlds want best possible world according preference Consider example typical data analyst solving machine learning problem given dataset She selects model underlying phenomenon learned choosing learning bias formats Corresponding author Email addresses kristiankerstingcstudortmundde K Kersting martinmladenovcstudortmundde M Mladenov paveltokmakovinriafr P Tokmakov 1 We refer 14 references overviews httpdxdoiorg101016jartint201506009 00043702 2015 Elsevier BV All rights reserved K Kersting et al Artiﬁcial Intelligence 244 2017 188216 189 raw data according chosen model tunes model parameters minimizing objective function induced data model assumptions iterate step model selection validation This instance declarative Model Solver paradigm prevalent AI 6 natural language processing 7 machine learning 8 data mining 9 instead outlining solution computed specify problem terms highlevel modeling language solve general solvers Unfortunately todays solvers mathematical programs typically require program presented solverreadable form andor offer restricted modeling environment For example solver require set linear constraints presented linear inequalities Ax b This create severe diﬃculties user First process turning intuition deﬁnes model paper solverreadable form cumbersome Consider following example graph isomorphism 10 Given graphs G H LP formulation introduces variable possible partial function mapping k vertices G k vertices H It trivial task come convenient linear indexing variables let expressing resulting inequalities Ax b It requires user produce maintain complicated matrix generation code tedious errorprone Moreover reusability specialized code limited relatively minor modiﬁcations equations require large modiﬁcations code example user decides switch having variables sets vertices variables tuples vertices Ideally like separate rules generate problem problem instance Finally solverreadable forms inherently propositional By design model domains variable number individuals relations enumerating As mentioned AI tasks domains best modeled terms individuals relations Agents deal heterogeneous information types Even important build models know individuals domain know variables exist Hence modeling facilitate formulation abstract general knowledge To overcome downsides triggered success probabilistic logical languages optimization liftable relational level Speciﬁcally lift linear programsthe tractable best understood widely practice fragment mathematical programsby introducing relational linear programs RLPs They declarative LP templates deﬁned logical concepts individuals relations quantiﬁed variables allow user express LP objectives constraints varying number individuals enumerating For instance following code subject vertexXnot sourceXnot targetX outflowXinflowX0 encodes conservation ﬂows vertices graph source target Together logical knowledge base LogKB referring individuals relations effectively logical program consisting logical facts rules induces ground LP solved LP solver However RLPs consist templates instantiated times construct ground linear model likely induce ground models exhibit symmetries demonstrate detect exploit As main technical contribution introduce lifted linear programming LLP It detects symmetries linear program quasilinear time eliminates reparametrizing original linear program smaller sharing optimal solutions computed LP solver Both contributions result relational linear programming best summarized cid2 cid3 LP Logic Symmetry Solver The user describes relational problem highlevel relational LP modeling language andgiven logical knowledge base LogKB encoding individuals datathe automatically compiles symmetryreduced LP turn solved offtheshelf LP solver Our empirical illustrations AI tasks computing optimal valuefunctions Markov decision pro cesses 11 approximate inference Markov logic networks 12 LP relaxations collective classiﬁcation 13 demonstrate relational linear programming ease process turning modelers form form modeler understands problem actually class problems deal varying number individuals relations solverreadable form considerably reduce time spent compute solutions The present paper signiﬁcant extension previously published conference paper 14 It provides concise development LLP ﬁrst coherent view relational linear programming novel promising way scaling AI developing showcasing ﬁrst modeling language LPs based logic programming One advantages language closeness syntax mathematical notation LP problems supporting language elements logic programming individuals relations quantiﬁed variables This allows concise readable relational deﬁnition linear optimization problems general declarative fashion relational algebraic formulation model contain hints process We proceed follows We start reviewing linear programming existing LP template languages Section 2 Then Section 3 introduce relational linear programming syntax semantics Afterwards Section 4 190 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 shows detect exploit symmetries linear programs Section 5 illustrates relational linear programming em pirically examples machine learning AI Before concluding ﬁnally touch related work directions future work 2 Linear programming A linear program LP 15 optimization problem expressed following general form minimizexRn subject cid5c xcid6 Ax b Gx h A Rmn G Rpn matrices b c h real vectors dimension m n p respectively cid5 cid6 denotes scalar product vectors Note equality constraints reformulated inequality constraints yield LP form containing inequalities minimizexRn subject cid5c xcid6 Ax b represent triplet L A b c 1 LPs wide application ﬁelds operations research applied problems like multi commodity ﬂow optimal resource allocation combinatorial optimization provide basis approximation algorithms hard problems TSP They way machine learning AI For instance classiﬁcation 1618 structured prediction 19 subroutines collective classiﬁ cation approaches 2021 eﬃcient approximate MAP inference 22 ﬁnding optimal policy Markov decision problems 2324 inverse reinforcement learning 25 clustering dimensionality reduction 2627 While LP models look intuitive paper applying practice presents challenge The main issue following gap The solverreadable form LPthe Ltriplet representation shown aboveis form natural users Furthermore matrix A typically sparse having 0entries Consequently modeling real world prob lem solverfriendly Lform error prone time consuming This explain LPs typically speciﬁed declarative abstract way discuss 21 Declarative modeling languages linear programs Linear programs presented declarative abstract way separates general structure problem hand actual instance As example consider maximizing total ﬂow network 28 The problem given ﬁnite directed graph GV E edge u v E nonnegative capacity cu v vertices s t called source target maximize function f V V R called ﬂow In order valid ﬂow f subjected following constraints f u v cu v f u v 0 cid4 cid4 f u w wV st wV st f w u The constraint asserts incoming ﬂow equals outgoing ﬂow internal vertices goods created destroyed nodes source sink To build ﬂow LP given network ﬁrst introduce variables represent ﬂow edge network Then formulate capacity constraints conservation constraints depending network structure Finally objective function LP yields value total ﬂow network As general deﬁnition ﬂow LP separated speciﬁcation network hand turn applied different networks Such declarative abstract modeling realized algebraic modeling languages Section 6 references They simplify LP deﬁnitions allowing use algebraic notation instead matrices deﬁne objective constraints parameters domains deﬁned separate ﬁle enabling modelinstance separation Let look Lform Starting 1 algebraic modeling languages typically involved arithmetic expressions explicit minimizexRn subject cid4 jP cid4 jP c j x j ai j x j bi K 2 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 191 P K P K 1 set P 2 set K 3 param jin 4 param c jin 5 param b iin 6 var x j P 7 8 objective 9 minimize sum j P cj xj 10 constraints 11 subject sum j P ai jxj bi column dimension A row dimension A provided input provided input provided input determined solver Fig 1 AMPL declaration scheme linear program set form shown 2 vertices 1 set VERTEX 2 set EDGES VERTEX diff sink cross VERTEX diff source edges 3 4 param source symbolic VERTEX 5 param sink symbolic VERTEX source 6 param cap EDGES 0 7 8 var Flow ij EDGES 0 capij 9 10 maximize sum sourcej EDGES Flowsourcej 11 12 subject k VERTEX diff sourcesink 13 sum ik EDGES Flowik sum kj EDGES Flowkj flow capacities objective flows entrance graph exit graph conservation flow Fig 2 AMPL speciﬁcation general ﬂow linear program The vertices edges corresponding ﬂow capacities provided separate ﬁles The ﬂows edges declared determined LP solver sets P K corresponding nonzero entries vectors c b matrix A deﬁned separate ﬁle This simpliﬁed representation automatically translated matrix format fed solver user prefers To code LP set form mathematical programming modeling languages proposed According NEOS solver statistics2 AMPL popular We brieﬂy review basic AMPL concepts For details refer 2930 Based set form LP written AMPL shown Fig 1 In principle AMPL program consists objective line starting maximize minimize keyword number ground indexed constraints lines starting subject keyword If constraint indexed constraint example indexed set K ground constraint generated combination values indexing variables example index variable constraint ground constraint generated value K The keyword set declares set members provided separate ﬁle The keyword param declares parameter single scalar value collection values indexed set Subscripts algebraic notation written square brackets bi instead bi The values determined solver deﬁned var keyword The typical cid5 symbol replaced sum keyword The key element AMPL socalled indexing expressions j P In addition variableparameter declaration indexing expressions serve limits sums indices constraints Finally comments AMPL start symbol To illustrate AMPL Fig 2 shows ﬂow problems formulated AMPL The program starts deﬁnition sets parameters variables appear They deﬁne objective constraints network ﬂow problem particular The ﬁrst constraints incorporated variable deﬁnition As AMPL allows write problem description declarative way It frees user engineering instance speciﬁc LPs capturing general properties problem class hand Unfortunately AMPL provide logically parametrized deﬁnitions arithmetic expressions index sets It diﬃcult use logical formula φX Y sourceS edgeS Y edgeY X sourceX compactly code set nodes distance source node network Relational linear programs RLPs introduce feature exactly They allows AMPLs beneﬁts number choice optimization experts time enable logical constructions established effective tools dealing relational problems 2 http wwwneosserverorg neos report html accessed April 19 2014 192 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 anna bob edward frank gary helen iris age education smokes 27 22 25 30 45 35 50 uni college college uni college school school Fig 3 Example collective inference There 7 people social network Each person described terms attributes The class label cancer shown 3 Relational linear programs The main idea parameterize AMPLs arithmetic expressions logical variables replace AMPLs indexing expression queries logical knowledge base Before showing let brieﬂy review logic programming For details refer 23132 31 Logic programs A logic program set clauses constructed types symbols constants variables functors predicates Say want model smoking habits people In addition usual ﬂat data attributes people like age education smoking habits access social network people cf Fig 3 Formally speaking attribute2 friends2 predicates arity number arguments listed explicitly The symbols anna bob edward frank gary helen iris constants X Y variables All constants variables terms In addition structured terms functor sX arity 1 contains function symbol s term X Atoms predicate symbols followed necessary number terms friendsbob anna natsX attributeX passive Literals atoms natsX positive literal negations natsX negative literals We able deﬁne key concept clause They formulas form A B1 Bm A head Bj body literals variables understood universally quantiﬁed For instance clause c attributeX passive friendsX Y attributeY smokes read X attribute passive X Y friends Y attribute smokes Clauses body facts A logic program consists ﬁnite set clauses The set variables term atom conjunction clause E denoted VarE Varc X Y A term atom clause E ground variable occurring E VarE A substitution θ V 1t1 V ntn Yanna assignment terms ti variables V Applying substitution θ term atom clause e yields instantiated term atom clause eθ occurrences variables V simultaneously replaced ti cYanna attributeX passive friendsX anna attributeanna smokes The Herbrand base logic program P denoted hbP set ground atoms constructed predicate constant function symbols alphabet P A Herbrand interpretation logic program P subset hbP A Herbrand interpretation I model clause c substitutions θ bodycθ I holds holds headcθ I A clause c logic program P entails clause c denoted c cid15 c The Herbrand model LHP model c P model c constitutes semantics logic program P consists facts f hbP P logically entails f P cid15 f A query q form B1 B m Bj literals variables understood existentially quanti ﬁed Given logic program P correct answer query q substitution θ qθ entailed P That qθ true LHP The answer substitution θ computed SLDresolution Finally answer set q set correct answers q logic program P P cid15 P P cid14 cid14 cid14 cid14 cid14 cid14 Logic programming especially convenient representing relational data social network Fig 3 All needs binary predicate friends2 encode edges social graph predicates attributeX Attr code attributes age people social network 32 Parametrizing linear programs logic programs Since language seen logic programming variant AMPL introduce syntax contrast AMPL syntax A ﬁrst important thing notice AMPL mimics arithmetic notation syntax possible It operates sets intersections sets arithmetic expressions indexed sets Our language relational linear programming K Kersting et al Artiﬁcial Intelligence 244 2017 188216 193 outflowX sum Y edgeXY flowXY 1 declarations LP predicates 2 var flow2 outflow1 inflow1 3 objective 4 maximize sum X sourceX outflowX 5 auxiliar variables capturing outflow nodes 6 subject forallX vertexX 7 8 auxiliar variables capturing inflow nodes 9 subject forallY vertexY 10 11 conservation flow 12 subject forallX vertexX sourceX sinkX 13 14 capacity bound 15 subject forall XY edgeXY capXY flowXY 0 16 negative flows 17 subject forall XY edgeXY flowXY 0 inflowY sum X edgeXY flowXY outflowX inflowX 0 Fig 4 Relational linear program encoding maximal ﬂow problem For details refer main text effectively replaces constructs logical predicates clauses queries deﬁne main parts RLP objective template constraints template logical knowledge base To illustrate let reconsider network ﬂow problem running example An RLP ﬂow example shown Fig 4 It directly codes ﬂow constraints concisely captures essence ﬂow problems illustrates nicely linear programming viewed highly relational nature Let discuss program line line 321 Logically parametrized algebraic expressions LP predicates deﬁne logically parametrized sets variables parameters LP In ﬂow running example flow2 captures instance ﬂows nodes Sets explicitly deﬁning domains AMPL discarded parametervariable domains deﬁned implicitly In contrast logic ground LP atoms numeric value true false For instance ﬂow nodes captured flow2 speciﬁc ﬂow node f t value 37 flowf t 37 Logically parameterized LP variables parvars short values determined solver follow AMPLs notation lines 12 1 declarations LP predicates 2 var flow2 outflow1 inflow1 The rest program speciﬁes objective line 4 constraints lines 517 ﬂow problem For use logically parameterized algebraic expressions parexpressions short Parexpressions expressions ﬁnite length form sumX φ ψ1 op1 ψ2 op2 opn1 ψn cid9 cid6 cid7cid8 ψ Here ψi numeric constants atoms parexpressions op j arithmetic operators The term sumX φ optional essentially implements AMPL aggregation sum indexed variables X subset logical variables appearing logical query φ LP atoms appearing ψi s let atomsψ denote set That AMPL indexing expression j P sum turned indexing respect X For readers familiar Prolog essentially think calling Prolog metapredicate cid10 cid11 cid2 cid3 setof X φ atomsψ P treating atomsψ conjunction involved atoms This produce set P substitutions variables X duplicate removed query φ atomsψ satisﬁed In case interested multisets express counts use findall3 This expressed sumcid5φcid6 instead sumφ The sum aggregation involved parexpression evaluated resulting multidimensional index P If sum provided logical indexing evaluation parexpression ψ1 op1 ψ2 op2 opn1 ψn Now objective parexpression free logical variables In ﬂow running example objective3 reads 3 For sake simplicity assume exactly source sink vertex If wants enforce simply add logical constraint selection query resulting objective source sink notes In AMPL use additional check statements express restrictions expressed simple inequalities 194 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 3 objective 4 maximize sum X sourceX outflowX This says want maximize outﬂows source nodes Note assume free logical variables avoid producing multiple conﬂicting objectives4 With parexpressions hand introduce parequalities5 parinequalities specifying constraints They ﬁnitelength expression form φ1 op 0 φ1 parexpression op denotes operators Without loss generality assume right hand 0 left hand subtraction We assume parinequalities allquantiﬁed bounds free variables bound sum statement This indicated forallX φ ψ X denotes free variables query φ atomsψ For tuple indexed set evaluate parinequality ψ This way constraints lead ground instances In ﬂow running example constraints auxiliary LP predicate outflow2 5 auxiliar variables capturing outflow nodes 6 subject forallX vertexX 7 outflowX sum Y edgeXY flowXY Since logical variable Y bound summation outflow1 logical variable X allquantiﬁcation says equality expression constraint node summing ﬂows outgoing edges Likewise deﬁne auxiliary LP predicate inflow2 summing ingoing edges 8 auxiliar variables capturing inflow nodes 9 subject forallY vertexY 10 inflowY sum X edgeXY flowXY The remaining constraints deﬁned similar fashion outflowX inflowX 0 11 conservation flow 12 subject forallX vertexX sourceX sinkX 13 14 capacity bound 15 subject forall XY edgeXY capXY flowXY 0 16 negative flows 17 subject forall XY edgeXY flowXY 0 Already simple ﬂow example illustrates power RLPs Since indexing expressions logical queries naturally express things look cumbersome AMPL capabilities For instance concept internal edge line 2 Fig 2 explicitly represented lengthy expression sets intersections dif ferences AMPL compactly represented logical query vertexX sourceX targetX parequations involving inoutflow line 12 Fig 4 As reader noticed predicates vertex1 source1 deﬁned ﬂow RLP In cases assume predicate deﬁned logical knowledge base LogKB discuss 322 The logical knowledge base Every predicate deﬁned parvar assumed logically parameterized LP parameter parparam short deﬁned possibly external6 logical knowledge base LogKB We use Prolog assume query RLP produces ﬁnite set answers ground substitutions logical variables rendering true Fig 5 illustrates instance maximal ﬂow problem It expressed following LogKB capsa 4 capsb 2 capac 3 capbc 2 capbd 3 capcb 1 capbt 2 capdt 4 edgeXY capXY 4 Investigating free variables objective turn bound leading forall statement attractive avenue future work allows produce LPs shared constraints embed relational linear programs logic programming Doing goes scope present paper 5 We parequations 6 Depending use case predicates deﬁned LP deﬁned external database K Kersting et al Artiﬁcial Intelligence 244 2017 188216 195 Fig 5 A graph particular instance ﬂow problem The node s denotes source t target The numbers associated edges ﬂow capacities vertexX edgeX_ vertexX edge_X sources targett caps 4 shorthand notation capsa4 capX Y capXY_ use anonymized variable7 _ The predicates edge2 vertex1 logical predicates taking values 1 0 corre sponding true false RLP For instance edgeXY true XsYa XaYs XsYb XbYs LogKB value 1 RLP Finally querying vertex result multiset answer deﬁnition vertex1 tests vertex source target edge However recall logical indexing statement removes duplicates ﬁrst Putting relational mathematical program deﬁned follows Deﬁnition 1 Relational mathematical program A relational mathematical program consists 1 ﬁnite set parvar declarations predicates 2 parexpression deﬁne objective 3 ﬁnite set parinequalities deﬁne constraints Everything explicitly deﬁned assumed parparam deﬁned external LogKB Finally relational linear programs RLPs relational mathematical programs involved parexpressions i1 φi ψi φi s aﬃne Aﬃne parexpressions written potentially simplifying form known quantities LP parameters ψi s LP atoms cid5 n Let turn semantics relational linear programs 323 The semantics grounding Together logical knowledge base LogKB relational linear program RLP induces linear program LP long logical queries RLP ﬁnite answer sets Theorem 2 An RLP LogKB logical queries RLP ﬁnite answer sets induces linear program LP Proof Since objective ﬁnite set constraints suﬃcient induces ﬁnite set ground instances ﬁnite length We induction depth syntax tree Let consider parequality constraint cases follow principles forallX φ ψ1 op1 ψ2 op2 opn1 ψn 0 Assume n 1 There cases The statement trivially true ψ1 constant case sense LPs If ψ1 parameter atom logical variables bound logical query φ LogKB By assumption ﬁnitely groundings Finally ψ1 LP atom logical variables bound logical query φ Again answer set φ assumed ﬁnite In atomic cases generate ﬁnite set ground equality constraints answer free variables ψ1 uniﬁed φ From induction follows combination n 1 LP atoms constants arithmetic operators induces ﬁnite set ground instances What remains sum statements If prove ﬁnite theorem follow So assume parequation involves sum statements The ﬁrst step turning par equation kind prenex normal form In logic formula prenex normal form written string quantiﬁers followed quantiﬁerfree For parequation hand means write string single forall statement followed 7 They withing LP RLPs 196 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 Algorithm 1 Grounding relational linear program R Input RLP R LogKB K Output Ground LP L consisting ground AMPL statements 1 Set L LP 2 Flatten parinequalities objective prenex normal form inlining deﬁnitions Section 324 simplifying brackets 3 parinequality objective 4 5 Query LogKB order obtain grounding set groundings dealing constraint involves indexing expression sumaggreation separate atom 6 7 Concatenate results queries evaluation grounding form ground inequality resp objective Add ground inequality resp objective L 8 return L Algorithm 2 Relational linear programming 1 Specify relational linear program RLP R 2 Given logical knowledge base LogKB ground R LP L Algorithm 1 3 Solve L LP solver transforming solvers input form string zero sum statements followed sumfree correspondingly simpliﬁed respectively rewritten Actually block sum statements merged suitable renaming involved logical variables Hence loss generality assume single sumY φcid14ψ string parexpression ψ length n include sum statement Now induction base ψ result ﬁnite set ground arithmetic expressions long free variables bound This case deﬁnition free variables ψ bound LogKB query φcid14 sum statement logical query φ leading forall statement Moreover assumption answer set leading forall statement ﬁnite For answers connect resulting ground arithmetic expressions ψ ﬁnite length operators Doing captures semantics sum statement This produces ﬁnite set arithmetic expressions ﬁnite lengths proves theorem cid2 The proof Theorem 2 constructive turned general algorithm grounding RLP We turn parinequalities objective prenex normal form Viewing essentially logical formulas ground insideout respect forall sum statements This Prolog engine imple menting metainterpreter calling findall setof internal loop A simple version summarized Algorithm 1 In cases experiments eﬃcient use relational database management RDBMS grounding RLP forwardchaining way treating queries parexpressions SQL queries Niu Ré Doan Shavlik 33 demonstrated beneﬁcial inference Markov logic networks MLNs 12 We essentially follow strategy grounding RLPs PostgreSQL experiments allows perform arithmetic computations string concatenations inside SQL query able sums LP variables corresponding coeﬃcients directly query As result post processing needed concatenation strings Our grounding implementation takes comparable time Niu et als TUFFY MLNs comparable number ground predicates generated stateoftheart performance task moment 324 Relational linear programming simpliﬁcations To summarize far relational linear programming works summarized Algorithm 2 We encode general problem structure optimization problem hand relational linear program line 1 Then specify problem instance external logical knowledge base LogKB line 2 Finally ground relational linear program problem instance Algorithm 1 line 3 For illustration reconsider ﬂow instance depicted Fig 5 Together relational ﬂow LP Fig 4 induces following ground linear program AMPL notation sake readability groundings shown var flowsa flowsb inflowa outflowa maximize flowsa flowsb subject outflowa flowac subject outflowb flowbc flowbd subject inflowa flowsa subject inflowb flowsb flowcb subject outflowa inflowa 0 subject 4 flows 0 subject 2 flows c 0 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 197 flow edges determined solver 1 var flow2 2 3 outflowX sum edgeX Y flowX Y 4 inflowY sum edgeX Y flowX Y 5 6 maximize sum sourceX outflowX 7 8 subject vertexX sourceX targetXconservation flow 9 10 subject edgeX Y capX Y flowX Y 0 11 subject edgeX Y flowX Y 0 capacity bound negative flows outflowX inflowX 0 outflow node inflow node objective Fig 6 Simpliﬁed relational encoding relational ﬂow LP Compared basic variant Fig 4 forall statements implicit inline deﬁnitions lines 3 4 subject flows 0 This illustrates beneﬁt relational linear programming While formulating LPs canonical form input LP solvers requires explicit value enumeration A matrix b c vectors relational linear programming avoids explicit value enumeration exploiting existence domain objects relations objects ability express objectives constraints quantiﬁcation We change LogKB induce different ﬂow LP touch LP logic anymore However relational linear programming concise Although auxiliary LP predicates inflow1 inflow1 increase readability relational LPs unnecessarily blow induced LP Both deﬁnitions directly substituted conservation ﬂow constraint conservation flow subject forall X vertexX sourceX targetX sum edgeX Y flowX Y sum edgeX Y flowX Y 0 This avoids creating 2n auxiliary LP variables constraints n number nodes graph sacriﬁces readability RLP To balance readability size complexity induced LP use inline deﬁnitions Inline deﬁnitions appear objective involve forall statements There single atom left hand equality followed parexpression The idea tell compiler substitute right hand deﬁnition inline objective constraints performing inline expansion inserting right hand code pattern matching saving overhead auxiliary LP variables Furthermore drop X parts X consists variables query Finally drop brackets long scope logical variables clear allquantiﬁcation forall statements implicitly That drop forall statements ψ statement corresponds forall statement variables appearing query ψ indexing All simpliﬁcations illustrated ﬂow RLP shown Fig 6 However relational linear programming concise modeling One eﬃciently detect exploit symmetries induced LPs turn speed solving How shown 4 Exploiting symmetries dimension reduction LPs As mentioned introduction features relational models produce model instances lot symmetries These symmetries turn exploited perform inference lifted level level groups variables For probabilistic relational models lifted inference yield dramatic speedups reasons groups indistinguishable variables instead treating individually Triggered success linear programming liftable To end start highlevel discussion approach connecting lifted probabilistic inference followed technical results necessary support approach 41 Symmetry detection compression In developing compression method motivated conceptual paradigm successful inference algorithms lifted belief propagation BP 3436 messagepassing algorithm approximate inference Markov random ﬁelds MRFs From highlevel perspective summarize paradigm follows precise deﬁnitions given later section We start standard inference algorithm look condi tions terms speciﬁc model solved set variables behave identically That look features model apriori guarantee set variables maintain equal values solu tion process ﬁnal solution This identical behavior refer symmetry variables subjected referred indistinguishable To advantage discovered symmetry ﬁnally replace 198 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 Fig 7 Using symmetry speed linear programming feasible region LP objective vector pink b span fractional automorphism LP grey c lifted compressed LP obtained projecting feasible region span fractional automorphism Best viewed color sets indistinguishable variables representative called lifted super variable We reformulate model carefully general necessary modify inference algorithm solution original model read reformulated If reformulated model smaller size good chance solve problem hand faster Intuitively increase chance lower endtoend solution times method detecting symmetries considerably faster actual solver end spending time detecting symmetries gain compressing model In following present eﬃcient approach speciﬁcally quasi linear time approach lifting linear programs For technical discussion ﬁrst introduce notion symmetry equitable partitions variables constraints linear program algebraic representations fractional automorphisms To justify equitable partitions form symmetry sense prove LPs admitting nontrivial equitable partitions admit optimal solutions variables belonging equivalence class equal We reparametrize LP given equitable partition replacing set indistinguishable variables single supervariable representing sum Unlike lifted BP compressed model MRF anymore actually compressed LP solutions transfered solutions original LP The compressed LP solved offtheshelf LP solver Finally discuss symmetry detection computation equitable partitions LPs The steps outlined detecting symmetry reparametrizing LP solving reparametrized LP recovering original solution constitute Lifted Linear Programming While use combinatorial algebraic concepts way operations geometrical interpretation illustrated Fig 7 Essentially identify lowerdimensional subspace intersects feasible region way optimal solution contained intersection To arrive smaller LP project subspace transfer computing inverse image projection We dive technical discussion Lifted Linear Programming 42 Equitable partitions fractional automorphisms Let L A b c LP A Rmn m constraints n variables In following aim partition variables constraints mutuallyexclusive classes indistinguishable Thus deﬁne partition LP set P P 1 P p Q 1 Q q sets p i1 P 1 n P P j partition variables sets q i1 Q 1 m Q j Q j partition constraints LP equivalence classes Hence require P Q j appropriate j We partition P P 1 P p Q 1 Q q L A b c equitable following conditions hold For variables j class P ci c j For constraints j class Q bi b j ii For variables j class P constraint class Q real number c k Q Aki c l Q Alj c Analogously constraints j class Q constraint class P real number c k P Aik c l P A jl c In words ﬁx class constraints Q number constraints Q variable P participates coeﬃcient c equal j P The hold equivalent constraints class variables K Kersting et al Artiﬁcial Intelligence 244 2017 188216 199 It clear LP admits equitable partition discrete trivial constraint variable singleton class Whether LP admits coarser ones depends structure In general like partition LPs coarsely possible order gain highest potential speed ups How eﬃciently discussed later Section 44 Moreover equitable partitions LPs generalize equitable partitions graphs 37 To connection explicit consider equitable partition matrix M equitable partition LP M 0 0 equitable partition bipartite graph G equitable partition bipartite adjacency matrix Let illustrate equitable partitions following didactic LP var p1 maximize sumgadgetX pX subject sumwidgetX pX sumgadgetX pX 1 subject widgetX widgetX 0 subject sumwidgetX pX sumgadgetX pX 1 LogKB recall logical atoms assumed evaluate 0 1 RLP widgetx widgety gadgetz If ground RLP convert dual form 1 obtain following linear program L0 A b c minimizex yzT R3 0x 0 y 1z subject 1 1 1 0 0 1 1 1 0 0 1 1 cid19 cid18 x y z 1 0 0 1 brevity substituted px p y pz x y z respectively We claim partition P 0 1 2 3 12 34meaning x equivalent y z second constraint equiva lent ﬁrst fourthis equitable partition L0 Let verify First c1 c2 cid18 c3 b2 b3 cid18 b1 cid18 b4 condition We need verify condition ii For sake illustration class P 1 2 Consider Q 1 Since Q singleton condition ii reduces A11 A12 c happens case example c 1 The holds Q 4 For Q 2 3 need check condition c 0 c 1 We Q Ai1 0 3 2 j Q A j2 0 Q Ai1 1 2 3 j Q A j2 1 One easily complete counting argument pairs classes So far introduced equitable partitions linear programs In order LP variables grouped equitable partition assume equal values optimal solution LP introduce bit mathematical machinery makes interplay equitable partitions apparent notion fractional automorphism Ramana Scheinerman Ullmann 388 Let M m n real matrix A fractional automorphism M pair doubly stochastic meaning entries nonnegative row column sum matrices X P X Q M X P X Q M The following theorem establishes correspondence equitable partitions fractional automorphisms Theorem 3 See 3839 Let M rectangular real matrix Then P P 1 P p Q 1 Q q equitable partition M LP M 0 0 matrices X P X Q having entries 8 Note deﬁnition slightly modiﬁed original 38 interested rectangular matrices equivalent weighted bipartite graphs 38 200 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 X P j X Q j cid20 1P 0 cid20 1Q 0 vertices j P U vertices j Q U 3 fractional automorphism M ii conversely let X P X Q fractional automorphism matrix M Then partition P columns j belong P P X P j X P ji greater 0 respectively rows j belong class Q X Q j X Q ji greater 0 equitable partition M In particular require Theorem 3 encoding equitable partitions LPs fractional automorphisms provide insights geometrical aspects lifting essential tool proving soundness Before continue review mathematical background lifted linear programming additional observation Proposition 4 Let P equitable partition A b c X P X Q matrices according 3 Then c T X P c T X Q b b Proof The proof follows directly fact group elements LP corresponding c b entries equal cid2 Finally note matrixgraph partition equitable represented doubly stochastic matrix 3 However mind resulting matrix called partition matrix fractional auto morphism partition equitable In case partition matrices useful property later allow reduce number constraints variables linear program More precisely Proposition 5 See 40 Let X doubly stochastic matrix produced partition P according 3 Then X cid21Bcid21B cid20 cid21Bi P 1 P 0 element belongs P T 4 Before starting develop lifted linear programming let illustrate fractional automorphisms partition matrices Reconsider equitable partition P 0 1 2 3 12 34 LP L0 running example The fractional auto morphism L0 induced 3 X P cid19 cid18 5 5 0 5 5 0 0 1 0 1 2 1 2 0 cid22 0 0 1 1 2 0 1 2 0 cid23 0 1 cid21Bcid21BT X Q 0 0 1 0 0 5 5 0 0 5 5 0 0 1 0 0 1 0 0 0 0 1 2 1 2 0 0 0 0 1 cid21Ccid21CT 1 0 0 0 1 2 0 0 1 2 0 0 0 1 Moreover Proposition 4 holds cid6 1 1 1 1 0 0 0 1 0 1 1 1 cid7cid8 A cid18 cid6 cid9 5 5 0 5 5 0 0 1 0 cid7cid8 X P cid19 cid9 1 1 5 5 5 5 1 1 0 0 1 1 cid6 0 1 0 0 0 5 5 0 0 5 5 0 0 1 0 0 cid7cid8 X Q cid9 cid6 cid9 1 1 1 1 0 0 0 1 0 1 1 1 cid7cid8 A It easily veriﬁed c T X P c T X Q b b hold 43 Lifted linear programming We ready establish lifted linear programs We split argument parts ﬁrst Note X P doubly stochastic matrix average value entries equivalence class Hence claimed variables LP L admits optimal solution x entry X P x admits solution X P x K Kersting et al Artiﬁcial Intelligence 244 2017 188216 201 Algorithm 3 Lifted linear programming Input An inequalityconstrained LP L A b c cT x Output x argmin xAxb 1 Compute equitable partition L Section 44 2 Read characteristic matrix cid21BP 3 Compress L Acid21BP b cid21BT 4 Obtain compressed solution y cid21BP y 5 return x P c remove redundant constraints standard LP solver indistinguishable according equitable partition behave identically Now add constraint y Rn x X P y words x spanX P linear program cut away optimum The second claim instead adding y Rn x X P y achieve restriction projecting entire LP span X P Unless P discrete partition singleton classes X P rank resulting LP fewer variables One verify rank X P number P classes P So project lowdimensional space solve LP recover highdimensional solution simple matrix multiplication This idea exactly illustrated Fig 7 Let state main result lifted linear programming Theorem 6 Let L A b c linear program X P X Q fractional automorphism L Then holds x feasible L X P x feasible objective value As consequence x optimal optimal solution X P x Proof Let x feasible L A b c Ax b Observe left multiplication doubly stochastic matrix preserves direction inequalities More precisely Ax b S Ax S b doubly stochastic9 S Now leftmultiply X Q Ax b X Q Ax X Q b A X P x b X Q A A X P X Q b b This proves ﬁrst Theorem Finally observe c T X P x c T x c T X P c T This proves second theorem cid2 We shown add constraint x spanX P L ﬁnd solution quality original program How help reducing dimensionality LP To answer observe constraint x spanX P implemented implicitly reparametrization cid14 A X P b X T That instead adding L A b c explicitly LP L P c Now recall X P gen erated equitable partition factorized X P cid21B P P cid21B P normalized incidence matrix cid21B T P 1 P p U 4 Note span X P cid21BP P equivalent vector space isomorphism sense column space cid21BP That x Rn x spanX P expressed x cid21BP y y Rp conversely cid21BP y spanX P y Rp Hence replace L P c Since problem p n variables potentially reduced dimension speedup solving original LP cid14cid14 possible Finally y P c equivalent L cid21BP y optimum solution L optimal solution L cid14cid14 Acid21BP b cid21BT cid14 A X P b X T cid21BT Moreover observe compressing equivalent variables equivalent constraints Q classes partition redundant That rows Acid21BP indices grouped equitable partition original LP identical vectors Thus achieve additional compression retaining constraint constraint equivalence class Overall proves lifted linear programming approach summarized Algorithm 3 sound Given LP ﬁnd equitable partition line 1 Since number classes directly determines size lifted LP seek compute coarsest partition topic subsection Then read characteristic matrix 4 line 2 Finally solve lifted LP line 3 unlift lifted solution solution original LP line 4 Applying lifted linear programming LPs induced RLPs revise relational linear programming summarized Algorithm 4 Before showing compute coarsest equitable partition illustrate lifted LP looks like running example L0 First compute Acid21B P 9 To case consider real numbers a1 b1 a2 b2 For positive s1 s2 s1a1 s2a2 s1b1 s2b2 We generalize j S j b j S bi S positive Ax j b j assumption induction ﬁnite number variables apply S Axi j S j Ax j cid5 cid5 202 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 Algorithm 4 Relational linear programming revised lifted linear programming 1 Specify RLP R 2 Given LogKB ground R LP L Algorithm 1 3 Solve L lifted linear programming described Algorithm 3 Fig 8 Construction coeﬃcient graph G L running example L0 On lefthand coloring LP shown This turns colored coeﬃcient graph shown righthand Best viewed color Acid21B P 1 1 1 0 0 1 1 1 0 0 1 1 1 2 1 2 0 0 0 1 2 2 1 2 1 2 2 2 1 0 0 1 As noted equivalent constraints 2 3 redundant drop If recompute c b vector accordingly lifted LP minimizexzT R2 subject 0x 1z 1 1 2 1 1 0 1 cid22 cid23 cid18 x z cid19 1 0 1 having replaced x y supervariable x 44 Computing fractional automorphisms So far developed method speeding solving LPs given equitable partition variables constraints However left open question ﬁnd partitions Now That deal computational aspect lifting In graph theory equitable partitions wellstudied generalizations orbit partitions 37partitions induced action subgroup automorphisms graph That orbit partition equitable partition vice versa While computing orbit partition graph hard GIcomplete precise problem computing coarsest equitable partition CEP graph algorithm called color reﬁnement known naive vertex classiﬁcation It simple extremely useful algorithmic routine graph isomorphism testing It classiﬁes vertices iteratively reﬁning coloringthe colors encode equivalence classesof vertices follows Initially vertices color Then step iteration vertices currently color different colors color c different number neighbors color c The process stops reﬁnement achieved resulting stable coloring graph If carefully chooses order resulting classes reﬁned Berkholz Bonsma Grohe 41 shown color reﬁnement realized time OE V logV quasilinear size graph nodes V edges E Automorphism group computation tools Nauty Saucy use fact employ coarsest equitable partition heuristic orbit computation As result highly eﬃcient theoretically empirically implementations color reﬁnement To compute coarsest equitable partition LP use tools converting LP CEP problem colored graph CEP problem Hence need graphical representation L A b c coeﬃcient graph L Fig 8 G L To construct G L add vertex G L m constraints n variables L Then connect constraint vertex variable vertex j Ai j cid18 0 Furthermore assign colors edges j way colori j coloru v Ai j Auv Finally ensure c b preserved automorphism ﬁnd color vertices similar manner row vertices j colori color j bi b j coloru colorv cu cv column vertices We choose colors way pair row column vertices share color possible K Kersting et al Artiﬁcial Intelligence 244 2017 188216 203 Fig 9 The Frucht graph 12 nodes The colors indicate resulting node partitions color reﬁnement coarsest equitable partition left automorphisms orbit partition right Best viewed color One verify equitable partition G L yields equitable partition L From complexity color ﬁnement fact graphical construction grows linearly size LP following theorem holds Theorem 7 The coarsest yielding compression equitable partition LP computable quasilinear time terms number variables size constraints nonzero entries A Before illustrating relational linear programming empirically like provide additional remarks lifted linear programming 45 Discussion lifted linear programming First lifted linear programming applied LP generated RLP At ﬁrst sight consider orthogonal However case One think lifted linear programming assembly language relational linear programming RLPs grounded solved ordinary LPs Going relational speciﬁcation LP compute equitable partitions faster ideally grounding Such approach demonstrated beneﬁcial Apsel Kersting Mladenov 42 relational MAP inference Under certain restrictions language generalized RLPs Another example language induced symmetries renaming groups Bui Huynh Riedel 43 While approaches heavy use relational representations based basic fact underlying ground problem liftable This perspective choose adapt RLPs present work The interaction languages equitable partitions promising avenue future research Second recall compression method works equitable partition coarsest hap pens eﬃciently computable As mentioned equitable partition graphthe orbit partitioncan constructed automorphism group set possible ways rename vertices graph The orbit partition groups vertices exists automorphism maps Applying partitioning method coeﬃcient graphs linear programs corresponding fractional automor phism equivalent previous theoretical wellknown results solving linear programs symmetry 44 references Note major beneﬁts color reﬁnement partition instead orbit partition linear programs The color reﬁnement partition coarse orbit partition shown 37 To illustrate consider socalled Frucht graph shown Fig 9 Suppose turn graph linear program introducing constraint nodes edges coloring color The Frucht graph extreme properties respect equitable partitions 1 asymmetric meaning orbit partition trivial having vertex equivalence class 2 regular vertex degree 3 easily verify case coarsest equitable partition consists single class Due properties orbit partition yields compression Frucht graph coarsest equitable resp color reﬁnement partition produces LP single variable The color reﬁnement partition computed quasilinear time current tools orbit partition enumeration signiﬁcantly worse running times computing orbitpartitions practical number cases computing GIcomplete problem Thus color reﬁnement achieve strict gains compression eﬃciency compared orbits Finally color reﬁnement algorithm essentially lifting algorithm lifted BP formulated Kersting Ahmadi Natarajan 35 Yet remarkable differences lifted BP lifted LPs For example lifting lifted MRF longer MRF classical sense needs special data structure track counts variables factors Hence introduce modiﬁed messagepassing algorithm In contrast lifted LP linear program 204 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 value function determined LP solver 1 var value1 2 3 maximize sumrewardS_ valueS 4 5 encoding discounted Bellman optimality inequality 5 6 subject forall S T transProbST_ valueS rewardSA 7 gammasumtransProbSTA transProbSTAvalueT best values states Fig 10 An RLP computing value function value1 Markov decision process There ﬁnite set states actions agent receives reward rewardSA performing action A state S speciﬁed LogKB specialized solver necessary Moreover solution lifted BP identical BP need true solution inference task computing singlenode marginals BP approximates task Thus exactness lifted BP inference limited BP On hand lifted LP recovers exact optimal solution corresponding LP No approximation involved Let turn illustrating empirically relational linear programming AI tasks 5 Illustrations relational linear programming Our intention investigate empirically viability ideas concepts relational linear programming following questions Q1 Can important AI tasks encoded concise readable relational way RLPs Q2 Are RLPs solved eﬃciently lifting Q3 Does relational linear programming enable programming approach AI tasks facilitating construction sophisticated models simpler ones adding parconstraints Q4 If lifted linear programming beneﬁcial beneﬁts observed different LP solvers bound particular solver Q5 Is numerical accuracy solver unaffected lifting If question answered aﬃrmatively relational linear programming potential linear models faster write easier understand reduce development time cost encourage experimentation turn reduce level expertise necessary build AI applications Consequently primary focus achieve best performance advanced models Instead focus basic models We implemented prototype relational linear programming illustrate relational modeling AI tasks computing value function Markov decision processes performing MAP inference Markov logic networks LP relaxation performing collective transductive classiﬁcation LP support vector machines 51 Lifted linear programming solving Markov decision processes Our ﬁrst application illustrating relational linear programing computation value function Markov Decision Problems MDPs The LP formulation task follows 45 maximizev 1T v subject v ck γ cid4 jcid6S j v j cid6S k K pk 5 v value state set states cid6S ck reward agent receives carrying action k K pk j probability transferring state state j taking action k γ discounting factor The corresponding RLP given Fig 10 Since abstracts away states rewardsthey deﬁned LogKBit extracts essence computing value functions MDPs Given LogKB ground LP automatically created instead coding LP hand problem instance vanilla linear programming This answers question Q1 aﬃrmatively The MDP instance wellknown Gridworld 46 The gridworld problem consists agent navigating grid n n states Every state associated reward Rs Typically states high rewards considered goals states zero negative associated rewards We considered instance gridworld single goal state upperright corner reward 100 The reward states set 1 The scheme LogKB follows rewardstaten1nright100 rewardstatenn1up100 rewardstateXY_1 X0 Xn1 Y0 Yn1 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 205 Fig 11 Experimental results relational linear programming solving Markov decision processes MDP Number variables ground lifted LPs basic gridworld b Measured times basic gridworld MDP log scale c Number variables gridworld additional symmetry d Measured times gridworld additional symmetry log scale e Measured times gridworld additional symmetry sparse form log scale Lifted solving denotes endtoend time lifting indicates portion time spent lifting Best viewed color We solved resulting set LPs CVXOPT 32 GHz Core i7 32 GB RAM workstation running Ubuntu 12 As summarized Fig 11a MDPSLPs compiled half original size Furthermore Fig 11b shows compression leads improved running time In ﬁgures broke measured total time solving LP time spent lifting solving respectively We introduced additional symmetries putting goal corner grid As expected gave room compression improved eﬃciency reﬂected Figs 11c 11d These results aﬃrmatively answer question Q2 strongly support Q1 answered aﬃrmatively However examples considered far sparse structure Thus wonder demonstrated beneﬁt achieved solving sparse problem dense form To address converted MDP problem sparse representation experiments As Fig 11e lifting resulted improvement size running time Therefore conclude lifting LP beneﬁcial regardless problem sparse dense view symmetry dimension orthogonal sparsity Remarkably results follow closely achieved MDPspeciﬁc symmetry ﬁnding model minimization approaches 4749 52 Programming MAPLP inference Markov logic networks MLNs MLNs 12 prominent probabilistic relational model weighted logical rules 075 smokesX cancerX Given set constants MLN induces Markov random ﬁeld MRF node ground atom clique ground formula We focus MAP maximum posteriori inference want ﬁnd likely joint assignment random variables A common approach approximate MAP inference MRFs based LPs 22 Let brieﬂy review approach Suppose presented propositional MRF binary random variables X x1 xn factors F θ f x f f θ f function having 0values subset random variables x f X As LP variables introduce variable beliefs μi states random variable xi μixi 0 μixi 1 LP variables μ f xi 0 x j 1 xk 0 The essence joint beliefs μ f joint conﬁguration subset x f MAPLP approach constrain joint beliefs consistent variable beliefs marginalization μ f xi 0 x j 0 μ f xi 0 x j 1 μixi 0 More precisely MAPLP deﬁned follows cid4 maximizeμ0 subject f cid4 xi cid4 θ f x f μ f x f μixi 1 xi X μ f x f μixi x f xi x f 6 x f xi 206 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 solve LP integer LP variable beliefs exact MAP assignment However integrality constraints approximation Using RLPs compactly encode MAPLP MLNs To consider friendsandsmokers MLN 12 consisting rules The ﬁrst rule says smoking cause cancer 075 smokesX cancerX sec ond implies people friends likely smoking habits 075 friendsX Y smokesX smokesY Since MLNs RLPs based logic programming expect able deﬁne MAPLP ﬁrstorder fashion incorporating MLN semantics directly 6 In order understand works let discuss key features From MLN semantics know following 1 generate set random variables grounding smokesX cancerX friendsX Y person domain 2 We gen erate set factors grounding rules Every ground rule factor ground atoms involved In friendsandsmokers example pairwise ternary rules involving 2 resp 3 ground atoms Thus set MAPLP variables parametrized terms predicates clauses MLN directly set logically parameterized LP variables They atom beliefs msmokesX t msmokesX f pairwise factor beliefs msmokesX f cancerX f msmokesX t cancerX f msmokesX f cancerX t msmokesX t cancerX t induced rule smokesX cancerX ﬁnally ternary beliefs like mfriendsX Y f smokesX t smokesY f induced rule list joint conﬁgurations It easy marginalization constraints logically parametrized For example constraints cid4 cancerXtf msmokesX cancerX msmokesX Finally objective weight joint belief variable follows MLN semantics weight rule joint conﬁguration satisﬁes rule 0 Eg weight msmokesanna t canceranna f 0 truth assignment model smokesanna canceranna In contrast beliefs remaining conﬁgurations weight 075 Consequently encode objective weights predicate w given LogKB The resulting RLP shown Fig 12 If ground RLP exactly LP 6 applied ground MRF induced friendsandsmokers MLN Note abstracted away names predicates As result RLP ternary MLN Changes evidence constants MLN rules conﬁned entirely LogKB friendsandsmokers MLN looks follows personanna personbob valuef valuet encoding MLN clauses weights smoking causes cancer wsmokesX cancerX t f 0 personX wsmokesX cancerX V1 V2 075 people social network atom true false friends similar smoking habits wfriendsX Y smokesX smokesY t t t 075 personX valueV1 valueV2 personX personY wfriendsX Y smokesX smokesY t f f 075 personX personY wfriendsX Y smokesX smokesY V1 V2 V3 0 personX personY valueV1 valueV2 valueV3 evidence msmokesgary t msmokeshelen t mfriendsanna bob t mfriendsheleniris t known friendships known smokers Here w represents objective value assigned joint belief atoms ﬁrst resp arguments having values resp arguments Please shorthand notation mind wsmokesX cancerX t f 0 stands wsmokesX cancerX t f 0 With RLP Fig 12 seamlessly merged MLN MAPLP semantics compact RLP retained freedom change MLN rules domain constants evidence easily RLP This illustrates modeling power RLP relational domains presents strong argument Q1 answered aﬃrmatively Let turn investigating Q2 As shown previous works inference graphical models dramat ically spedup lifted inference Thus natural expect symmetries graphical models exploited standard lifted inference techniques reﬂected corresponding MAPRLP To verify case induced MRFs varying size friendsandsmokers MLN varying number people 50 300 The results experiments summarized Figs 13a b As Fig 13a shows number LP variables signiﬁcantly reduced Not linear program reduced fact lifting carried measure considerable decrease running time depicted Fig 13b Note time lifted experiment includes time needed compile LP This aﬃrmatively answers Q2 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 207 sumwP1 P2 V1 V2 wP1 P2 V1 V2 mP1 P2 V1 V2 sumwP1 P2 P3 V1 V2 V3 wP1 P2 P3 V1 V2 V3 mP1 P2 P3 V1 V2 V3 single node pairwise triplewise beliefs configurations determined solver 1 var m2 2 var m4 3 var m6 4 value MAP assignment 5 score sumwP V wP V mP V 6 7 8 9 10 marginalization pairwise beliefs 11 marginalizeP1 P2 V1 sumwP2 V2 mP1 P2 V1 V2 12 13 marginalization ternary beliefs 14 marginalizeP1 P2 P3 V1 sumwP3 V3 wP2 V2 15 16 17 maximize score 18 subject forall P wP _ 19 20 pairwise consistency constraints 21 subject forall P1 P2 V1 wP1 P2 V1 _ 22 23 24 ternary consistency constraints 25 subject forall P1 P2 P3 V1 wP1 P2 P3 V1 _ _ 26 27 marginalizeP1 P2 P3 V1 mP1 V1 marginalizeP1 P2 V1 mP1 V1 mP1 P2 P3 V1 V2 V3 sum wP V mP V 1 assignment largest value atom beliefs sum Fig 12 RLP encoding MAPLP friendsandsmokers MLNs shown 6 The constraints aggregates symmetric copies omitted redundancy necessary logic predicates symmetric Fig 13 Experimental results relational linear programming MAPLP inference Markov logic networks Number variables lifted ground LPs b Time solving ground LP vs time lifting solving Best viewed color 53 Programming collective classiﬁcation LPSVM Networks ubiquitous interested objects networks inﬂuence Consequently collective classiﬁcation received lot attention 50531213 It refers task jointly classifying set interrelated objects It exploits fact interrelated objects share lot similarities For example citation networks dependencies topics papers references social networks people close contact tend similar interests Using dependencies allows collective classiﬁcation methods outperform methods assume object independence 50 Despite successful research collective classiﬁcation far focused generative models column generation approach solving quadratic program formulations collective classiﬁcation task We illustrate relational linear programming provide ﬁrst step principled largemargin approach Speciﬁcally introduce transductive10 collective SVM based RLPs In essence classify unlabeled objects consider attributes relationship labeled ones model We start reviewing vanilla LP approach SVMs RLPs program transduc tive collective classiﬁer Support vector machines SVMs 54 widely model discriminative classiﬁcation moment The hypothesis space SVM space aﬃne models represented coeﬃcients weights vector orthogonal hyperplane intercept In softmargin version SVM training strives strike balance width margin examples separating hyperplane number examples fall wrong 10 Transductive inference mode direct reasoning observed unobserved instances inductive hypothesis ﬁnding problem 54 208 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 slacks slope hyperplane intercept hyperplane margin 1 var slack1 2 var weight1 3 var b0 4 var r0 5 6 slackssumlabelI slackI 7 innerProdIsumattribute_J weightJattributeIJ 8 9 largest margin Here C encodes tradeoff parameter 10 minimize r C slacks 11 12 examples correct hyperplane 13 subject forallI labelI 14 15 weights 1 1 16 subject forall J attribute_ J 1 weightJ 1 17 subject r 0 18 subject forall I labelI slackI 0 labelIinnerProdI b slackI r margin positive total slack hyperplane slacks positive Fig 14 A linear programming SVM encoded RLP Note convenience use minimize instead maximize statement margin misclassiﬁcations Maximization margin achieved penalizing squared Euclidean norm weight vector traditionally posed quadratic optimization problem QP Zhou et al 16 shown problem modeled LP replacing squared Euclidean norm inﬁnity norm incurring major loss generalization performance The LP suggested Zhou et al follows refer 16 details minimizeξi rR r C subject cid4l ξi i1 yiwxi b r ξi 1 wi 1 ξi 0 r 0 This LPSVM readily applied classify papers Cora dataset 55 The Cora dataset consists 2708 scientiﬁc publications classiﬁed seven classes The citation network consists 5429 links Each publication dataset described 01valued word vector indicating absencepresence corresponding word dictionary The dictionary consists 1433 unique words We turned problem binary classiﬁcation problem taking largest 7 classes positive class merging 6 negative class The RLP Fig 14 encodes vanilla LPSVM In noncollective setting ignored citation information classiﬁed documents based 01 word features following LogKB C 0021 attribute31336 119 attribute31336 126 label17798 1 label10531 1 regularization parameter Here arguments attribute2 indices document word present document respectively Words present document value zero original dataset speciﬁed Again answers Q1 aﬃrmatively RLP stays ﬁxed different datasets LogKB changes We transform vanilla RLPSVM model transductive collective TCRLPSVM making slight modiﬁcation RLP Indeed number ways exploring interesting avenue future work Since primary focus achieve best performance illustrate ease relational mathematical programming approach chose following basic approach We add constraints favor unlabeled instances label labeled neighbors To account contradicting examples introduce slack variables constraints add objective separate penalty parameter This results TCRLPSVM model shown Fig 15 Here new predicate pred2 denotes predicted label unlabeled instances The LogKB gets new predicates C1 00021 C2 00031 cite89547 1132385 cite89547 1152379 query1128959 query16008 The cite2 predicate encodes citation information query1 predicate marks unlabeled instances labels inferred We notice parameters objective play different role TCRLPSVM In vanilla case parameter carefully chosen training phase prediction learned weight vector In transductive setting linear model heart RLPSVM plays role medium K Kersting et al Artiﬁcial Intelligence 244 2017 188216 209 sumciteI1I2labelI2queryI1 slackI1I2 sumlabelI slackI slack neighboring instances predicted label unlabeled instances slacks slope hyperplane intercept hyperplane margin 1 var pred1 2 var slack1 3 var coslack2 4 var weight1 5 var b0 6 var r0 7 8 slack 9 coslack sumciteI1I2labelI1queryI2 slackI1I2 10 11 12 largest margin Here Cs encode tradeoff parameters 13 minimize r C1 slack C2 coslack 14 15 subject forall I queryI predI innerProdI b 16 related instances labels 17 subject forall I1 I2 citeI1 I2 labelI1 queryI2 18 19 symmetric case 20 subject forall I1 I2 citeI1 I2 labelI2 queryI1 21 22 23 examples correct hyperplane 24 subject forall I labelI 25 26 weights 1 1 27 subject forall J attribute_ J 1 weightJ 1 28 subject r 0 29 subject forall I labelI slackI 0 labelIinnerProdI b slackI r labelI1 predI2 slackI1 I2 r labelI2 predI1 slackI1 I2 r margin positive slacks positive Fig 15 An RLPSVM model collective inference transductive setting Fig 16 Experimental results linear programming support vector machines TCRLPSVM versus vanilla RLPSVM Cora dataset b Experimental results evaluation different collective classiﬁcation methods Cora dataset Best viewed color labeled unlabeled instances The weights tuned new problem instance recall point transductive inference learn intermediate predictive model inference directly In case objective parameters important The optimal value parameters depends data tuned transductive inference phase instance crossvalidation To investigate beneﬁt relational mathematical programming approach collective inference compared performance vanilla RLPSVM TCRLPSVMthe RLPSVM additional relational constraintson task paper topic classiﬁcation Cora dataset The experiment protocol follows We ﬁrst randomly split dataset training set A validation set C test set B proportion 701515 The validation set select parameters TCRLPSVM 5fold crossvalidation fashion That split validation set 5 subsets Ci equal size On sets selected parameter grid search Ci A C Ci labeled B Ci unlabeled examples computing prediction error Ci averaging Ci s We evaluated selected parameters test set B labels revealed training We repeated experiment 5 times Ci TCRLPSVM For consistency followed protocol RLPSVM set B Ci appear training RLPSVM use unlabeled examples That selected parameters training A C Ci evaluating Ci The selected parameters evaluated test set B The results summarized Fig 16a As vanilla RLPSVM achieved prediction error 16 1 The TCRLPSVM achieved 75 1 A paired ttest p 005 revealed difference mean signiﬁcant Although best performance goal performance encouraging Consequently conducted detailed study TCRLPSVM performance We applied stateoftheart collective classiﬁcation approaches Cora dataset evaluated exactly splits data end compared performance model 210 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 The ﬁrst methods NetKit 56a toolkit classiﬁcation networked data Among available options chose methods performed best original study 56 The simplest classiﬁer available toolkit weightvoted relational neighbor classiﬁer wvRN turned best combination relaxation labeling collective inference procedure In basic setting experiments follow simply predicts probability label instance average probability label neighbors instance network P xi cNi Z 1 cid4 P x j cN j v j Ni Ni set neighbors node v Z normalization constant The second model compared linkbased classiﬁer 57 nLB This classiﬁer creates feature vector node aggregating labels neighboring nodes uses logistic regression build discriminative model based feature vectors The learned model applied estimate P xi cNi Various aggregation methods setting including existence mode value counts The method normalized value counts shown best performance study It combined relaxation labeling collective inference For fair comparisons TCRLPSVM methods allowed use instancelevel information word vectors training logistic regression model observed instances predictions priors unobserved ones Finally simple networkonly MLN baseline It consists single rule categoryv0 c citesv0 v1 categoryv1 c additional restriction document category For probabilistic inference Tuffy 33 The experiment protocol follows 4 settings kept 20 40 60 80 labels training set For setting 10 random splits generated labeled instances selected uniformly random Each method evaluated random splits We report average errors setting method To illustrate beneﬁts relational information Vanilla LPSVM uses instancelevel information included evaluation The results summarized Fig 16b The performances NetKit methods correspond originally reported Macskassy Provost 56 The nLB classiﬁer outperform simpler wvRN 80 labeled examples The MLN classiﬁer ignores instancelevel information shows similar performance wvRN consistently little bit worse Note 40 labeled examples classiﬁers Vanilla LPSVM achieve accuracies 85 In fact highest performance achieved Vanilla LPSVM 83 accuracy signiﬁcantly lower This conﬁrms utilizing relational information signiﬁcantly boost classiﬁcation performance However Vanilla LPSVM signiﬁcantly improved adding additional constraints More precisely TCLPSVM outperforms Vanilla LPSVM cases catches quickly NetKit methods The slightly lower performance general particular lower percent observed labels likely fact basic model propagate label information citation network In case empirical results clearly collective inference tackled RLPs provide aﬃrmative answer Q1 That performance gain achieved simply reprogramming Vanilla LPSVM highlights power relational linear programming answers Q3 aﬃrmatively Keep mind goal obtain novel better collective model demonstrate possibilities opened relational linear programming 54 Lifted LP support vector machines To close loop ﬁnally investigated liftability RLPSVMs Although surprising theoretical perspectivewe shown LP contains symmetries liftablethis constitutes ﬁrst symmetryaware LPSVM solver encouraging sign lifting goes probabilistic inference lifted statistical machine learning general insurmountable To investigate symmetries relational SVM classiﬁcation solved LPs produced parameter selection phase TCSVM different observed label percentages lifted fashion More precisely following experimental protocol experiment started cases 20 40 60 80 observed labels For case limited evaluating 4 regularization parameter pairs C1 C2 line 13 RLP Fig 15 referred c1 c2 brevity These pairs consist optimal parameters 2 10 1000 optimal parameters The motivation choice represent different parameter scales grid search As previous experiment parameter evaluation 5fold crossvalidation Finally encompass wider variety training sets repeated process 10 random ABC splits Thus dataset consists 4 4 5 10 800 LPs total We ﬁrst computed compression ratio ratio size number constraints variables lifted LP size ground LP LPs Fig 17a breaks ratios percentage observed labels As constraints induced random subgraphs Cora corresponding random label deletions K Kersting et al Artiﬁcial Intelligence 244 2017 188216 211 Fig 17 Speedingup TCSVM lifted linear programming All experiments performed Gurobi Compression ratios ratios lifted model size number variables plus number constraints ground model size random CVsplits parameter selection ﬁxed percentage observed labels b net speedup compression solving lifted LP Gurobi vs solving ground LP Gurobi solving Gurobi random CVsplits 4 different c1 c2 parameter pairs c scatter plot ground vs lifted time LPs experiment Best viewed color nonnegligible symmetry present problem Moreover average compression rate increases labels introduced On ﬁrst sight counterintuitive adding information typically considered break symmetries However RLP employs information pairs observed unobserved instances lower number pairs symmetry actually That increase percent observed labels number pairs likely decrease Next solved LPs Gurobi version 60 popular LP solver 2012 according NEOS statistics11 default settings We machine MDP experiments Fig 17b summarizes net gain LP lifted solving including time symmetry detection compression opposed solving ground model different parameter pairs One Gurobis sophisticated presolving heuristics lifting yields considerable speedup LPs solving lifted model slower Across LPs lifting LPs gained total 129083 seconds compared ground solver Fig 17c shows scatterplot lifted vs ground solving time solving Gurobi ﬁtted regression model As seen particular set LPs correlation coeﬃcient lifted ground time order symmetry present problem These results provide aﬃrmative answer Q2 present strong evidence speedups observed different solvers stateoftheart commercial packages Hence Q4 answered aﬃrmatively Finally investigate Q5 recorded relative difference quality lifted ground solutions cid8 maxc T x Lifted c T x Ground Lifted c T x minc T x Ground 1 7 For MDP experiments observed cid8s 10 experiments involving lifting For experiments CVXOPT ran solver convergence threshold 8 10 14 Gurobi automatically set convergence threshold With Gurobi experiments cid8 range 10 8 While imagine existence pathological cases lifting inﬂuences numerical performance 10 solver observed experiments This provides aﬃrmative answer question Q5 9 For MAP cid8s range 10 6 10 13 10 7 10 Taking results experimental illustrations clearly ﬁve questions Q1Q5 answered aﬃrmatively 6 Related work Relational linear programming introduced related lines research rich languages mathe matical programming exploiting symmetries AI tasks 61 Languages mathematical programming Several expressive modeling languages mathematical programming proposed Examples popular ones AMPL 2930 GAMS 58 AIMMS 59 XpressMosel 60 general surveys 6163 These modeling languages mixtures declarative imperative programming styles sets objects index multidimensional parameters LP variables Employing essentially forin statements indexing deﬁne 11 http zverovich net 2013 01 01 neosstatisticsfor2012 html In 2013 far popular CPLEX nonlinear solvers focus http zverovich net 2014 01 02 neosstatisticsfor2013 html Both webpages queried Nov 29 2014 212 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 objectives constraints abstract model separates declaration model data generate speciﬁc model instance Although common relational linear programming fundamental differences Consider AMPL As stated Fourer Gay Kernighan 30 page 122 current version AMPL support fullﬂedged logical type parameter stand values true false Instead promotes use parameters type binary logical operators ifthenelse particular forall exists However requires encode false operators intended work parameters LP variables featured relational linear programming To AMPL introduce binary LP variables corresponding constraints encode exists forall operators This blows model need represent false In contrast uniﬁcation different queries relational linear programming connects different sets LP variables encoded atoms logic programming proof techniques As alternative index sets data tables closely related attributes relations relational database systems proposals feeding linear program directly relational database systems 6466 AMPLs interface relational databases deﬁne sets parameters However takes logic LP modeling language Moreover logic programmingwhich allows use complex terms deﬁne recursive predicates know union tableswas considered resulting approaches provide syntax close mathematical notation linear logic programs This holds Roth Yih 67 presented abstract ILP model global inference natural language processing NLP tasks The idea use objectoriented entityrelationship model background index sumstatements ILPs members object relation classes This closer spirit relational linear programming uniﬁcation negation supported deﬁning constraints Generally motivated mentioned Model Solver paradigm NLP witnesses growing need relational mathematical modeling lan guages 6872 Consider subsequent work global inference Clarke Lapata 69 They quantify constraints statements j k x j xk conjoined xi saying head words conjoined source sentence add algebraic constraints variables indexed j k Although akin relational speciﬁcation general relational modeling language ILPs presented The approaches adhoc systems ground speciﬁc constraints looping specialized code highlevel programming language CC For solving relational Markov Decision Processes Sanner Boutilier 24 approximate linear programming relational constraints represented case statements This approach scaled problems previously prohibitive size avoiding grounding close spirit relational programming However Sanner Boutilier introduce relational modeling language arbitrary LPs Thus following Klabjan Fourer Ma 73 unpublished argue need mathematical pro gramming solution builtin language constructs facilitates natural algebraic modeling provides integrated capabilities logic programming Klabjan et al basic functionalities requirements math ematical programming languages AMPL realized Dataloglike logic programming language This probably closest spirit relational linear programming However signiﬁcant differences First RLPs directly extend syntax mathematical programming languages logic programming concepts stay close mathematical notation LPs Moreover Datalog expressive capture entire AI spectrum problems like tackle RLPs For instance disallows complex terms arguments predicateseg p1 2 admissi ble p f 1 2making hard work complex data structures graphs imposing certain stratiﬁcation restrictions use negation recursion allowing range restricted variables variable head rule appear negated clause premise rule Consider dealing LP problems graphs Indeed Datalog encode predicates encoding nodes edges graph However ways encode graphs complex terms form best certainly depends LP application hand For instance number nodes important provided user graph generated program simple predicatebased representation require metapredicates compute num ber nodes In listbased representation compute length list Moreover complex terms allowas illustrated present paperfor concise speciﬁcation MAPLP inference Markov logic networks Hence advocate Prolog similar reasons Eisner Filardo AI language DYNA 74 Recently Mattingley Boyd 75 introduced CVXGEN software tool takes high level description vex optimization problem family automatically generates custom C code compiles reliable high speed solver problem family CVXGEN features indexed parameters variables logical way declaring objective constraints referring Generally tendency embed mathematical programming languages imperative highlevel programming language Python 7677 Java CC Matlab others12 In particular Diamond Chu Boyds CVXPY 77 enables objectoriented approach constructing optimization problems notes objectoriented approach simpler ﬂexible traditional method constructing problems embedding information matrices In contrast goal relational linear programming logic programming optimization The holds Rizzolo Roths Learning Bayes Java LBJ 78 grew mentioned 12 All major solver suites GUROBI CPLEX feature APIs We refer corresponding manuals K Kersting et al Artiﬁcial Intelligence 244 2017 188216 213 research global inference NLP LBJ combines ideas optimization ﬁrst order logic objectoriented program ming compiling complex models ILPs In particular LBJ provides convenient syntax specifying interactions Java functions arbitrary ﬁrstorderlogical formulas That aim putting logic programming optimization Moreover predicates constraints equality inequality arguments arbitrary Java expressions Finally Gordon Hong Dudík 7980 developed ﬁrstorder programming FOP combines strength mixedinteger LPs ﬁrstorder logic In contrast present paper focused ﬁrstorder logical reasoning specifying arbitrary linear programs relational way 62 Exploiting symmetries solving AI tasks Another distinguishing feature relational linear programming compared modeling approaches mentioned far considered detect exploit symmetries arbitrary LPs Indeed symmetry breaking approaches mixedinteger programming MIPs 81 featured commercial solvers CPLEX 82 GUROBI 83 The dominant paradigm add symmetry breaking inequalities similarly SAT CSP 84 Alternatively prune search space eliminate symmetric solutions 81 survey In contrast lifted linear programming reduces dimensionality LP hand LP compressed To takes advantage convexity projects LP ﬁxed space symmetry group 44 The projections investigated present paper similar spirit Until recently discussions concentrated case symmetry group LP consists permutations 85 In cases problem computing symmetry group LP reduced computing colored automorphisms coeﬃcient graph connected linear program 8681 Moreover reduction LP case essentially consists mapping variables orbits Our approach subsumes method replace orbits coarser equivalence relation contrast orbits computable quasilinear time Going permutations Bödi Herr 44 extend scope symmetry showing invertible linear map preserves feasible region objective LP speedup solving While setting offers compression symmetry detection problem diﬃcult Finally lifted linear programming introduced proven beneﬁcial Lifted ILPMAP inference approaches relational graphical models based relaxed graph automorphisms variants explored ways 43878942 scope present paper 7 Future work The indent paper introduce explore basic idea concepts relational linear programming Consequently signiﬁcant additional work More work needed extended language presented concepts modules spaces allowing build libraries relational programs combining Mattingley Boyds 75 CVXGEN automatically generate custom C code compiles reliable high speed solver problem family hand The framework extended mathematical programs mixed integer LPs quadratic programs semideﬁnite programs Column generation cutting plane approaches lifted linear programming explored Very recently Lu Boutilier 90 presented valuedirected compression technique propositional assignment LP They dynamically segment individuals blocks form column generation constructing groups individuals provably treated identically optimal assignment solution Together declarative nature resulting relational mathematical programming approach AI investigate program analysis approaches automate problem decomposition lifted level If symmetries detected exploited eﬃciently mathematical programs general symmetryaware machine learning AI reach In general started explore interaction lifting solving linear programs mathematical programs general Since lifting changes geometry linear program impact heuristics employed modern solvers eﬃcient solving linear programs This interaction explored investigated cancel beneﬁts lifting Ground lifted linear programs require different convergence thresholds order maintain quality solutions While observe surprising behavior experiments exploring issues important interesting avenue future work The attractive immediate avenue explore relational linear programming AI machine learning tasks First novel collective classiﬁcation approach rigorously evaluated compared approaches number benchmark datasets Other attractive avenues exploration symmetryaware SVMs outlined present paper learning setting relational dimensionality reduction LPSVMs 91 novel relational boosting approaches linear programs 92 developing relational lifted solvers computing optimal Stackelberg strategies twoplayer normalform game 93 Lifting explored recent probabilistic CSP proposals For instance Sraswat et al 94 recently sketched probabilistic constraint pro gramming language C10 It based concurrent constraint programming framework implemented X10 language scaleout computation Lifting scale C10 Along similar lines explore lifting consensus optimization 95 214 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 One push programming view relational machine learning tasks As example consider kernels classifying graphs Graph classiﬁcation important task bioinformatics 96 natural language processing 97 ﬁelds Kernelized SVM method choice The inner products SVM replaced functions kernels effectively represent inner products higher dimensional space This inner product view hand makes SVM nonlinear classiﬁer allows deal structured objects graphs ing convolution kernels 98 Convolution kernels introduced idea kernels built work discrete data structures interactively kernels smaller composite parts RLPs suggests view programming task Within LogKB deﬁne parts generalized sum productsa generalized convolutionis realized RLP Similarly graph kernels known realized Walks 99 cyclic patterns 100 shortestpaths 101 examples substructures considered far All concepts naturally representable logic programs Prolog Eg shortestPathA B G Paths computes Path shortest path nodes A B graph G One program shortest path use deﬁne convolution kernel RLP SVM follows kG1 G2 sumvertexG1V1 vertexG1V2 shortestPathV1V2G1L1 vertexG2V3 vertexG2V4 shortestPathV3V4G2L2 simple_kV1V2L1V3V4L2 simple_kV1V2L1V3V4L2 simple_k kernel vertices lengths In way relational mathematical programming suggests novel programming view graph kernels integrates kernel programming mathematical program declarative model 8 Conclusion We introduced relational linear programming simple framework combining linear logic programming Its main building blocks relational linear programs RLPs They compact LP templates deﬁning objective constraints logical concepts individuals relations quantiﬁed variables This contrasts mainstream LP template languages AMPL mixes imperative linear programming allows intuitive repre sentation optimization problems relational domains reason varying number objects relations enumerating Inference RLPs performed lifted linear programming That symmetries ground linear program employed reduce dimensionality possible reduced program solved offtheshelf linear program solver This signiﬁcantly extends scope lifted inference paves way lifted LP solvers linear assignment allocation AI task solved LPs Empirical results approximate inference Markov logic networks LP relaxations solving Markov decision processes collective inference relational LP support vector machines illustrated promise relational linear programming Acknowledgements The authors like thank anonymous reviewers valuable feedback KK MM like thank Babak Ahmadi fruitful collaboration earlier version lifted linear programming Martin Grohe Aziz Erkal Selman fruitful collaboration colorreﬁnement PT CS Department University Bonn Germany working relational linear programming KK MM Fraunhofer IAIS working earlier version lifted linear programming The research presented partly supported Fraunhofer ATTRACT fellowship STREAM EC contract number FP248258FirstMM GermanIsraeli Foundation Scientiﬁc Research Development 118021862011 German Science Foundation DFG KE 168621 Coordination Project SPP 1527 References 1 L Getoor B Taskar Eds Introduction Statistical Relational Learning MIT Press Cambridge MA 2007 2 L De Raedt Logical Relational Learning Springer 2008 3 L De Raedt P Frasconi K Kersting SH Muggleton Eds Probabilistic Inductive Logic Programming Springer 2008 4 LD Raedt K Kersting Statistical relational learning GWC Sammut Ed Encyclopedia Machine Learning Springer Heidelberg 2010 pp 916924 2012 305362 5 N Nilsson Probabilistic logic Artif Intell 28 1 1986 7187 6 H Geffner Artiﬁcial intelligence programs solvers AI Commun 27 1 2014 4551 7 A Rush M Collins A tutorial dual decomposition Lagrangian relaxation inference natural language processing J Artif Intell Res 45 8 S Sra S Nowozin S Wright Eds Optimization Machine Learning MIT Press 2011 9 T Guns S Nijssen L De Raedt Itemset mining constraint programming perspective Artif Intell 175 1213 2011 19511983 10 A Atserias E Maneva SheraliAdams relaxations indistinguishability counting logics SIAM J Comput 42 1 2013 112137 11 M Littman T Dean L Pack Kaelbling On complexity solving Markov decision problems Proceedings 11th Annual Conference Uncertainty Artiﬁcial Intelligence UAI95 1995 pp 394402 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 215 2013 pp 642650 12 M Richardson P Domingos Markov logic networks Mach Learn 62 12 2006 107136 13 P Sen G Namata M Bilgic L Getoor B Gallagher T EliassiRad Collective classiﬁcation network data AI Mag 29 3 2008 93106 14 M Mladenov B Ahmadi K Kersting Lifted linear programming Proceedings 15th International Conference Artiﬁcial Intelligence Statistics AISTATS J Mach Learn Res Workshop Conf Proc vol 22 2012 pp 788797 15 G Dantzig M Thapa Linear Programming 2 Theory Extensions Springer 2003 16 W Zhou L Zhang L Jiao Linear programming support vector machines Pattern Recognit 35 12 2002 29272936 17 A Demiriz KP Bennett J ShaweTaylor Linear programming boosting column generation Mach Learn 46 13 2002 225254 18 K Ataman W Street Y Zhang Learning rank maximizing auc linear programming Proceedings International Joint Conference 19 Z Wang J ShaweTaylor Largemargin structured prediction linear programming Proceedings 12th International Conference Artiﬁcial Neural Networks IJCNN 2006 pp 123129 Intelligence Statistics AISTATS 2009 pp 599606 20 T Klein U Brefeld T Scheffer Exact approximate inference annotating graphs structural SVMs Proceedings European Confer ence Machine Learning Principles Practice Knowledge Discovery Databases Part 1 ECML PKDD08 2008 pp 611623 21 M Torkamani D Lowd Convex adversarial collective classiﬁcation Proceedings 30th International Conference Machine Learning ICML 22 MJ Wainwright MI Jordan Graphical models exponential families variational inference Found Trends Mach Learn 1 12 2008 1305 23 U Syed M Bowling RE Schapire Apprenticeship learning linear programming Proceedings 25th International Conference Ma chine Learning ICML ACM 2008 pp 10321039 24 S Sanner C Boutilier Practical solution techniques ﬁrstorder MDPs Artif Intell 173 56 2009 748788 25 AY Ng SJ Russell Algorithms inverse reinforcement learning Proceedings 17th International Conference Machine Learning ICML 26 N Komodakis N Paragios G Tziritas Clustering LPbased stabilities Proceedings 21st Conference Neural Information Processing 2000 pp 663670 Systems NIPS 2008 pp 865872 27 M Sandler On use linear programming unsupervised text classiﬁcation Proceedings Eleventh ACM SIGKDD International Confer ence Knowledge Discovery Data Mining ACM 2005 pp 256264 28 RK Ahuja TL Magnanti JB Orlin Network Flows Theory Algorithms Applications Prentice Hall 1993 29 R Fourer DM Gay BW Kernighan AMPL A Mathematical Programming Language The Scientiﬁc Press San Francisco CA 1993 30 R Fourer D Gay B Kernighan AMPL A Modeling Language Mathematical Programming second edition Duxbury PressBrooksCole Publishing Company 2002 httpamplcomresourcestheamplbook 31 J Lloyd Foundations Logic Programming SpringerVerlag Berlin 1987 32 P Flach Simply Logical Intelligent Reasoning Example Wiley Professional Computing Wiley 1994 33 F Niu C Ré A Doan J Shavlik Tuffy scaling statistical inference Markov logic networks RDBMS Proc VLDB Endow 4 6 2011 373384 USA 2008 pp 10941099 2009 2013 91132 ESA14 2014 pp 505516 34 P Singla P Domingos Lifted ﬁrstorder belief propagation Proceedings 23rd AAAI Conference Artiﬁcial Intelligence AAAI Chicago IL 35 K Kersting B Ahmadi S Natarajan Counting belief propagation Proceedings 25th Conference Uncertainty Artiﬁcial Intelligence UAI 36 B Ahmadi K Kersting M Mladenov S Natarajan Exploiting symmetries scaling loopy belief propagation relational training Mach Learn 92 37 C Godsil G Royle Algebraic Graph Theory Springer 2001 38 M Ramana E Scheinerman D Ullman Fractional isomorphism graphs Discrete Math 132 1994 247265 39 M Grohe K Kersting M Mladenov E Selman Dimension reduction colour reﬁnement Proceedings 22th Annual European Symposium 40 C Godsil Compact graphs equitable partitions Linear Algebra Appl 255 1997 259266 41 C Berkholz P Bonsma M Grohe Tight lower upper bounds complexity canonical colour reﬁnement Proceedings 21st Annual European Symposium Algorithms ESA 2013 pp 145156 42 U Apsel K Kersting M Mladenov Lifting relational MAPLPs cluster signatures Proceedings 28th AAAI Conference Artiﬁcial Intelligence AAAI 2014 Uncertainty Artiﬁcial Intelligence UAI 2013 43 H Bui T Huynh S Riedel Automorphism groups graphical models lifted variational inference Proceedings 29th Conference 44 R Bödi K Herr M Joswig Algorithms highly symmetric linear integer programs Math Program Ser A 137 12 2013 6590 45 M Littman T Dean LP Kaelbling On complexity solving Markov decision problems Proceedings 11th International Conference Uncertainty Artiﬁcial Intelligence UAI 1995 pp 394402 46 R Sutton A Barto Reinforcement Learning An Introduction The MIT Press 1998 47 S Narayanamurthy B Ravindran On hardness ﬁnding symmetries Markov decision processes Proceedings 25th International Conference Machine Learning ICML 2008 pp 688695 48 B Ravindran A Barto Symmetries model minimization Markov decision processes Tech Rep 0143 University Massachusetts Amherst 49 T Dean R Givan Model minimization Markov decision processes Proceedings 14th National Conference Artiﬁcial Intelligence AAAI MA USA 2001 1997 pp 106111 50 S Chakrabarti B Dom P Indyk Enhanced hypertext categorization hyperlinks Proceedings ACM SIGMOD International Conference 51 J Neville D Jensen Iterative classiﬁcation relational data Proceedings AAAI2000 Workshop Learning Statistical Models Rela 52 J Neville D Jensen Collective classiﬁcation relational dependency networks Proceedings 2nd International Workshop Multi Management Data SIGMOD 1998 pp 307318 tional Data 2000 pp 1320 Relational Data Mining 2003 pp 7791 53 J Neville D Jensen Relational dependency networks J Mach Learn Res 8 2007 653692 54 VN Vapnik Statistical Learning Theory Adaptive Learning Systems Signal Processing Communications Control Series John Wiley Sons New York 1998 A WileyInterscience Publication 55 P Sen GM Namata M Bilgic L Getoor B Gallagher T EliassiRad Collective classiﬁcation network data AI Mag 29 3 2008 93106 56 SA Macskassy F Provost Classiﬁcation networked data toolkit univariate case study J Mach Learn Res 8 2007 935983 57 Q Lu L Getoor Linkbased classiﬁcation Proceedings 20th International Conference Machine Learning ICML 2003 pp 496503 58 A Brooke D Kendrick A Meeraus GAMS A Users Guide The Scientiﬁc Press Redwood City CA 1992 59 J Bisschop P Lindberg AIMMS Modeling System Paragon Decision Technology 1993 216 K Kersting et al Artiﬁcial Intelligence 244 2017 188216 60 T Ciriani Y Colombani S Heipcke Embedding optimisation algorithms mosel 4OR 1 2 2003 155167 61 C Kuip Algebraic languages mathematical programming Eur J Oper Res 67 1993 2551 62 E Fragniere J Gondzio Optimization modeling languages P Pardalos M Resende Eds Handbook Applied Optimization Oxford University 63 S Wallace W Ziemba Eds Applications Stochastic Programming SIAM Philadelphia 2005 64 G Mitra C Luca S Moody B Kristjanssonl Sets indices linear programming modelling integration relational data models 65 A Atamtürk E Johnson J Linderoth M Savelsbergh A relational modeling linear integer programming Oper Res 48 6 2000 Press New York 2002 pp 9931007 Comput Optim Appl 4 1995 263283 846857 66 R Farrell T Maness A relational database approach linear programmingbased decision support production planning secondary wood product manufacturing Decis Support Syst 40 2 2005 183196 67 W Yih D Roth Global inference entity relation identiﬁcation linear programming formulation L Getoor B Taskar Eds An 68 S Riedel J Clarke Incremental integer linear programming nonprojective dependency parsing Proceedings Conference Empirical Introduction Statistical Relational Learning MIT Press 2007 Methods Natural Language Processing EMNLP 2006 pp 129137 69 J Clarke M Lapata Global inference sentence compression integer linear programming approach J Artif Intell Res 31 2008 399429 70 A Martins N Smith E Xing Concise integer linear programming formulations dependency parsing Proceedings 47th Annual Meeting Association Computational Linguistics ACL 2009 pp 342350 71 S Riedel D Smith A McCallum Parse price cutdelayed column row generation graph based parsers Proceedings Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning EMNLPCoNLL 2012 pp 732743 72 X Cheng D Roth Relational inference wikiﬁcation Proceedings Conference Empirical Methods Natural Language Processing EMNLP 2013 pp 17871796 73 D Kabjan R Fourer J Ma Algebraic modeling deductive database language httpdynresmanagementcomuploads33293329212datalog_ modelingpdf 2009 presented 11th INFORMS Computing Society ICS Conference 2009 published 74 J Eisner N Filardo Dyna extending datalog modern AI O Moor G Gottlob T Furche A Sellers Eds Datalog Reloaded 1st International Workshop Datalog 2010 Oxford UK March 1619 2010 Lect Notes Comput Sci vol 6702 Springer 2011 pp 181220 Revised Selected Papers 75 J Mattingley S Boyd CVXGEN code generator embedded convex optimization Optim Eng 12 1 2012 127 76 W Hart JP Watson D Woodruff Pyomo modeling solving mathematical programs Python Math Program Comput 3 2011 219260 77 S Diamond E Chu S Boyd CVXPY Pythonembedded modeling language convex optimization version 02 httpcvxpyorg May 2014 78 N Rizzolo D Roth Modeling discriminative global inference Proceedings IEEE International Conference Semantic Computing ICSC 2007 pp 597604 Intelligence UAI 2009 pp 213222 79 G Gordon S Hong M Dudík Firstorder mixed integer linear programming Proceedings 25th Conference Uncertainty Artiﬁcial 80 E Zawadzki G Gordon A Platzer An instantiationbased theorem prover ﬁrstorder programming Proceedings 14th International Conference Artiﬁcial Intelligence Statistics AISTATS vol 15 2011 pp 855863 81 F Margot Symmetry integer linear programming M Jünger T Liebling D Naddef G Nemhauser W Pulleyblank G Reinelt G Rinaldi L Wolsey Eds 50 Years Integer Programming 19582008 From Early Years StateoftheArt Springer 2010 pp 140 82 T Achterberg R Wunderling Mixed integer programming analysing 12 years progress M Jünger G Reinelt Eds Facets Combinatorial Optimization Festschrift Martin Grötschel Springer 2002 pp 449481 83 Gurobi Optimization Inc Gurobi optimizer reference manual http wwwgurobi com 2014 84 M Sellmann P Van Hentenryck Structural symmetry breaking Proceedings 19th International Joint Conference Artiﬁcial Intelligence IJCAI 85 R Bödi T Grundhöfer K Herr Symmetries linear programs Note Mat 30 1 2010 129132 86 T Berthold M Pfetsch Detecting orbitopal symmetries 2009 87 J Noessner M Niepert H Stuckenschmidt Rockit Exploiting parallelism symmetry map inference statistical relational models Proceed ings 27th AAAI Conference Artiﬁcial Intelligence AAAI 2013 88 M Mladenov A Globerson K Kersting Eﬃcient lifting MAP LP relaxations klocality Proceedings 17th Int Conf Artiﬁcial Intelligence Statistics AISTATS J Mach Learn Res Workshop Conf Proc vol 33 2014 89 M Mladenov A Globerson K Kersting Lifted message passing reparametrization graphical models Proceedings 30th Int Conf 2005 pp 298303 90 T Lu C Boutilier Valuedirected compression largescale assignment problems Proceedings 29th AAAI Conference Artiﬁcial Intelli Uncertainty Artiﬁcial Intelligence UAI 2014 gence AAAI 2015 91 J Bi K Bennett M Embrechts C Breneman M Song Dimensionality reduction sparse support vector machines J Mach Learn Res 3 2003 12291243 92 A Demiriz K Bennett J ShaweTaylor Linear programming boosting column generation Mach Learn 46 13 2002 225254 93 V Conitzer T Sandholm Computing optimal strategy commit Proceedings 7th ACM Conference Electronic Commerce EC 2006 94 V Saraswat V Gupta R Jagadeesan P Panangaden D Precup F Rossi P Sen Probabilistic constraint programming Working Notes 2014 pp 8290 NIPS Workshop Probabilistic Programming 2014 95 S Bach M Broecheler L Getoor D Oleary Scaling MPE inference constrained continuous Markov random ﬁelds consensus optimization Proceedings International Conference Neural Information Processing Systems NIPS 2012 pp 26542662 96 A Airola S Pyysalo J Björne T Pahikkala F Ginter T Salakoski Allpaths graph kernel proteinprotein interaction extraction evaluation crosscorpus learning BMC Bioinform 9 Suppl 11 2008 S2 97 J Suzuki T Hirao Y Sasaki E Maeda Hierarchical directed acyclic graph kernel methods structured natural language data Proceedings 41st Annual Meeting Association Computational Linguistics ACL 2003 pp 3239 98 D Haussler Convolution kernels discrete structures Tech rep UCSCCRL9910 UC Santa Cruz 1999 99 T Gärtner Exponential geometric kernels graphs Working Notes NIPS Workshop Unreal Data Principles Modeling Nonvectorial Data 2002 pp 4958 100 T Horváth T Gärtner S Wrobel Cyclic pattern kernels predictive graph mining Proceedings 10th ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD 2004 pp 158167 101 KM Borgwardt HP Kriegel Shortestpath kernels graphs Proceedings 5th IEEE International Conference Data Mining ICDM 2005