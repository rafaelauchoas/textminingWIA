Artiﬁcial Intelligence 305 2022 103661 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint LMMS reloaded Transformerbased sense embeddings disambiguation Daniel Loureiro LIAAD INESC TEC Dept Computer Science FCUP University Porto Portugal b School Computer Science Informatics Cardiff University United Kingdom Alípio Mário Jorge Jose CamachoCollados b r t c l e n f o b s t r c t Article history Received 20 May 2021 Received revised form 20 December 2021 Accepted 3 January 2022 Available online 5 January 2022 Keywords Semantic representations Neural language models Distributional semantics based neural approaches cornerstone Natural Language Processing surprising connections human meaning representation Recent Transformerbased Language Models proven capable producing contextual word representations reliably convey sensespeciﬁc information simply product self supervision Prior work shown contextual representations accurately represent large sense inventories sense embeddings extent distancebased solution Word Sense Disambiguation WSD tasks outperforms models trained speciﬁcally task Still remains understand use Neural Language Models NLMs produce sense embeddings better harness NLMs meaning representation abilities In work introduce principled approach leverage information layers NLMs informed probing analysis 14 NLM variants We emphasize versatility sense embeddings contrast taskspeciﬁc models applying senserelated tasks WSD demonstrating improved performance proposed approach prior work focused sense embeddings Finally discuss unexpected ﬁndings layer model performance variations potential applications downstream tasks 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 1 Introduction Lexical ambiguity prevalent different languages plays important role improving communication ef ﬁciency 87 Word Sense Disambiguation WSD longstanding challenge ﬁeld Natural Language Processing NLP Artiﬁcial Intelligence generally extended history research computational linguistics 75 Interestingly computational psychological accounts meaning representation converged high dimensional vectors semantic spaces From computational perspective rich line work learning word embeddings based statistical regularities unlabeled corpora following wellestablished Distributional Hypothesis 4135 DH The ﬁrst type distributional word representations relied countbased methods initially popularized LSA 27 later reﬁned GloVe 82 Before GloVe word embeddings learned neural networks ﬁrst introduced Bengio et al 7 gained wide adoption word2vec 72 culminated fastText 12 The development improvement word embeddings major contributor progress NLP decade 38 Corresponding author Email addresses danielbloureiroinesctecpt D Loureiro amjorgefcuppt A Mário Jorge camachocolladosjcardiffacuk J CamachoCollados httpsdoiorg101016jartint2022103661 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 From psychological perspective ample behavioral evidence support distributional representations word meaning Similarly word embeddings representations related according degree shared features semantic spaces translates proximity vectorspace 9748 Understandably nature features making psychological account semantic space aspects learning method clear ﬁnd computational account Nevertheless contextual cooccurrence informative factors meaning representation 673291 There use cases neurobiology motivating research accurate dis tributional representations word meaning In Pereira et al 83 word embeddings proven useful decoding words sentences brain activity learning mapping corpusbased embeddings GloVe word2vec fMRI activation The current understanding humans perform disambiguation attributes major relevance sentential context linguistic paralinguistic cues speaker accent lesser extent 9714 However previously mentioned computational approaches designed senselevel representation Meaning Conﬂation Deﬁciency 15 converge different senses wordlevel representation Some works explored variations word2vec method senselevel embeddings 99459065 dynamic wordlevel interactions composing sentential context targeted works The works Melamud et al 68 Yuan et al 126 Peters et al 84 ﬁrst propose Neural Language Models NLMs featuring dynamic word embeddings conditioned sentential context contextual embeddings These works showed NLMs trained exclusively language modeling objectives produce contextual embeddings word forms sensitive words usage particular sentences Furthermore works addressed WSD tasks simple nearest neighbors solution kNN based proximity contextual embeddings Their results rivaled systems trained speciﬁcally WSD additional modeling objectives highlighting accuracy contextual embeddings However development Transformerbased NLMs BERT 29 contextual embeddings NLMs showed clearly better performance WSD tasks previous systems trained speciﬁcally WSD LMMS Loureiro Jorge 61 In earlier work explored advantage representational power NLMs propa gation strategies encoding sense deﬁnitions Besides pushing stateoftheart WSD Loureiro Jorge 61 created sense embeddings entry Princeton WordNet v30 200k word senses 34 semantic space represented granular expansive encompass general knowledge domains parts ofspeech English language With fully populated semantic space disposal suggested strategies uncovering biases world knowledge represented NLMs Since work LMMS shown additional performance gains WSD ﬁnetuning classiﬁcation approaches better usage sense deﬁnitions 4411 semantic relations external resources 1049 alto gether different approaches WSD 5 However questions standing leverage NLMs creating accurate versatile sense embeddings optimizing WSD benchmarks Given semantic spaces distributional represen tations word meanings feature prominently conventional computational psychological accounts word disambiguation questions warrant exploration Contributions In extension LMMS broaden scope recent Transformerbased models addition BERT 1245952 14 model variants total verify exhibit similar proﬁciency sense representation explore performance variation attributed particular differences models Striving principled approach sense representation NLMs introduce new layer pooling method inspired recent ﬁndings layer specialization 95 crucial effectively use new NLMs sense representation Most importantly article provide general framework learning sense embeddings Transformers perform extensive evaluation sense embeddings different NLMs senserelated tasks emphasizing versatility representations Outline This work organized follows We ﬁrst provide background information main topics search Vector Semantics 21 Neural Language Modeling 22 Sense Inventories 23 Next related work Sense Embeddings 31 WSD 32 Probing NLMs 33 The method produce works sense embeddings described Section 4 covering aspects method introduced Loureiro Jorge 61 41 43 new layer pooling method Section 44 In Section 5 experimental setting providing relevant details choice NLMs 51 anno tated corpora learn sense embeddings 52 The layer pooling methodology described Section 44 requires validating performance distinct modes ap plication Consequently Section 6 report performance variation layer NLMs 61 highlight differences disambiguation matching proﬁles 62 present rationale choosing particular proﬁles task 63 In Section 7 tackle senserelated tasks proposed sense embeddings compare results stateoftheart WSD 71 Uninformed Sense Matching 72 WordinContext 73 Graded Word Similarity Context 74 Paired Sense Similarity 75 2 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 In order better understand contributions work Section 8 reports ablation analyses targeting following choice Sense Proﬁles 81 impact unambiguous word annotations 82 merging gloss representations 83 indirect representation synsets 84 We discuss ﬁndings Section 9 representations intermediate layers NLMs 91 irregularities models variants 92 potential downstream applications sense embeddings focusing knowledge integration 93 Finally Section 10 present concluding remarks provide details release sense embeddings code 2 Preliminaries This work exploits interaction vectorbased semantic representations 21 recent developments NLMs 22 curated sense inventories 23 In section provide background topics 21 Vector semantics Nearly century ago Firth 36 postulated meaning word contextual study meaning apart context taken seriously Indeed working formal theories word meaning deﬁnition Wittgen stein 121 conceded meaning word use language This view meaning representation known Distributional Hypothesis DH 41 proposes words occur contexts tend similar meanings During period Osgood et al 78 proposed representing meaning words points multi dimensional space similar words having similar representations placed closely space Still decades computing advancements appreciate implications DH Early VSMs After early works introducing vector space models VSMs information retrieval 102103 Deerwester 2827 ﬁrst use dense vectors represent word meaning initially method called Latent Semantic In dexing LSI later Latent Semantic Analysis LSA LSA based worddocument weighted frequency matrix ﬁrst 300dimensions resulting SingularValue Decomposition SVD correspond word embed dings Lund Burgess 64 introduced inﬂuential method similar LSA called Hyperspace Analogue Language HAL differed LSA considering wordword frequencies instead introducing notion ﬁxedsized win dow context words left right instead documents standard representation context Following developments Landauer Dumais 53 evaluated performance LSA embed dings learned large corpora simple semantic task synonymy tests embeddings performed comparably schoolaged children measuring similarity word pairs cosine similarity corresponding embeddings inspired applications information retrieval Already early period Schutze 107 Yarowsky 125 realized potential WSD applications based similarity unsupervised word embed dings Blei et al 10 later introduce Latent Dirichlet Allocation LDA uses generative probabilistic approach generalize improve approach LSA widely adopted topic modeling applications semantic analysis Neural models Having established corpusbased word embeddings able capture semantic knowledge additional progress followed swiftly A milestone evolution word embeddings discovery Neural Language Mod els NLMs implicitly develop word embeddings training task word prediction 8 Shortly Collobert 2325 demonstrated word embeddings incorporated neural architectures NLP tasks With word2vec Mikolov et al 73 distilled components NLMs responsible learning word embeddings lightweight scalable solution allowing neuralbased solution employed corpora unprecedented size 100B tokens Nevertheless countbased solutions remain important particularly GloVe 82 methods sig niﬁcantly improved The major improvement introduction fastText 12 able represent words absent training data leveraging subword information reﬁning aspects word2vecs training method Sense embeddings In spite success word2vec GloVe fastText conﬂated different senses word form representation shortcoming known Meaning Conﬂation Deﬁciency 15 While number exten sions proposed creation sensespeciﬁc representations AutoExtend 99 NASARI 18 DeConf 90 Probabilistic FastText 4 issue require development new generation NLMs order effectively addressed 22 Neural language modeling The ﬁrst major step contextual embeddings NLMs development context2vec 68 singlelayer bidirectional LSTM trained objective maximizing similarity hidden states target word embeddings similarly word2vec Peters et al 84 built context2vec ELMo deeper bidirectional LSTM trained lan guage modeling objectives produce transferrable representations Both context2vec ELMo emphasized WSD 3 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 applications providing convincing accounts sense embeddings effectively represented centroids contextual embeddings showing 1NN solutions WSD tasks rivaled performance taskspeciﬁc models With introduction highlyscalable Transformer architectures 113 kinds deep NLMs emerged causal lefttoright models epitomized Generative Pretrained Transformer 13 GPT3 objective predict word given past sequence words masked models objective predict masked hidden word given surrounding words prominent example Bidirectional Encoder Representations Transformers BERT 29 The difference training objectives results varieties NLMs specializing different tasks causal models excelling language generation masked models language understanding1 BERT proved highly successfully NLP tasks 98 motivated development numerous derivative models explore work In spite progress Transformerbased NLMs strong reliance surface features 66 social biases hard correct 127 There known theoretical limits language understanding expected models trained language modeling objectives 669 clear far current models limits 23 Sense inventories The currently popular English word sense inventory Princeton WordNet 34 henceforth WordNet large semantic network comprised general domain concepts curated experts2 The core unit WordNet synset represents cognitive concept Each lemma word multiword expres sion WordNet belongs synsets word senses combination word forms synsets referred sensekeys As result set words belong synset described synonyms words ambiguous belonging additional synsets speciﬁc synset The predominant seman tic relation WordNet relates synset pairs hypernymy IsA Each synset features gloss dictionary deﬁnition partofspeech noun verb adjective adverb lexname3 syntactic category logical group ing Synsets formally represented numerical codes Following related works represent readable format lemma P O S lemma corresponds synsets representative lemma As example lemma mouse polysemous belonging mouse1 n mouse synsets The frequent sense mouse mouse10500 sensekey belongs synset mouse1 n 02330245n hypernymy relation rodent1 n lexname nounanimal gloss numerous small rodents typically n rodent mouse4 Following Loureiro Jorge 61 use WordNet version 30 contains 117659 synsets 206949 senses 147306 lemmas 45 lexnames 3 Related work In section cover related work wellresearched topics work intersects Sense Embeddings 31 WSD 32 Probing NLMs 33 31 Sense embeddings Sense embeddings emerged NLP socalled meaning conﬂation deﬁciency word embeddings 15 By merging meanings single representation single vector proved insuﬃcient certain settings 123 contradicted common laws distance metrics triangle inequality 77 In order solve issue ﬁeld sense vector representation mainly split categories 1 unsupervised senses learned directly text corpora 9643118 2 knowledgebased senses linked predeﬁned sense inventory exploiting underlying knowledge resource 99906520 In article focus type representation particularly leveraging powerful Transformerbased language models trained unlabeled text corpora As ﬁnal representation mainly constructed based knowledge learned language models knowledge resources WordNet serve guide annotation process The goal paper construct taskagnostic sense representation leveraged semantic textual applications This differs traditional static sense embeddings notable exceptions 563789 mainly leveraged intrinsic sensebased tasks As paper generalpurpose sense representa tions learned power Transformers guided underlying lexical resource WordNet prove robust range textbased semantic tasks intrinsic sensebased benchmarks 1 Although recent models like BART 55 progress 2 Babelnet 76 Wiktionary 70 HowNet 31 popular alternatives covering languages 3 Lexnames known supersenses 3789 4 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 32 Word sense disambiguation As earliest Artiﬁcial Intelligence tasks WSD long history research In work coverage related work WSD focused recent systems Transformerbased architectures reasons ex periments focused Transformerbased systems current stateoftheart WSD converged systems Additionally distinguish solutions addressing WSD nearest neighbors paradigm precomputed sense embeddings taskspeciﬁc solutions ﬁnetuning Transformer models training classiﬁers internal representations 321 Nearest neighbors Our prior LMMS work described paper ﬁrst demonstrate nearest neighbors solution based sense embeddings pooled internal representations BERT feature extraction clearly outperform stateoftheart time adopted Transformerbased models SensEmBERT 104 followed similar approach LMMS leveraged BabelNet reduce dependency annotated corpora producing sense embeddings performed better WSD limited nouns With ARES Scarlini et al 105 introduce method produce large number semisupervised annotations dramat ically increase coverage sense inventory demonstrated sense embeddings learned annotations perform substantially better WSD LMMS SensEmBERT ARES use layer pooling method gloss embeddings LMMS employed BERTL multilingual variant showing strong performance languages English In addition WSD knowledge task works applied sense embeddings Wordin Context WiC 88 address work 322 Trained classiﬁers When comes Transformers train classiﬁers speciﬁc WSD task encounter diverse set solutions comparison feature extraction approaches One earliest straightforward supervised classiﬁers WSD BERT Sense Vocabulary Com pression SVC Vial et al 115 added layers BERT topped softmax classiﬁer trained targeting strategically reduced set admissible candidate senses Following outstanding results range text classiﬁcation tasks model ﬁnetuning GlossBERT 44 ﬁnetuned BERT glosses WSD framed text classiﬁcation task pairing glosses words context KnowBERT 86 employs sophisticated ﬁnetuning approach designed exploit knowledge bases WordNet Wikipedia glosses Straying prototypical classiﬁers Blevins Zettlemoyer 11 BEM propose biencoder method learns represent senses based glosses performing optimization jointly underlying BERT model Tak ing advantage ensemble sense embeddings LMMS SensEmBERT additional resources EWISER 9 trains multifaceted high performance WSD classiﬁer Finally current stateoftheart WSD ConSeC 5 obtains impressively strong results framing WSD extractive task similar extractive question answering trained ﬁnetuning BART 55 sequencetosequence Transformer outperforms BERT reading comprehension tasks comparable size In Loureiro et al 63 extensively compared ﬁnetuning feature extraction approaches WSD task Con sistent prior work ﬁnetuning overall outperforms feature extraction However comparable circumstances performance gap narrow feature extraction shows improved fewshot performance fre quency bias 33 Probing neural language models As NLMs popular investigating properties internal states intermediate representations important line research referred model probing Probing operates assumption rela tively simple classiﬁer based exclusively representations NLMs perform task required information encoded representations For clarity deﬁne probes functions learned heuristic designed reveal intrinsic property NLMs In section cover probing works focused lexical semantics layerspeciﬁc variation inspired probing analysis We distinguish works use probes trained representations learned probes directly comparing analyzing unaltered representations heuristics nearest neighbors 331 Learned probes Among inﬂuential ﬁndings line research discovery Hewitt Manning 42 syntac tically valid parse trees uncovered linear transformations word representations obtained pretrained ELMo BERT models Motivated discovery Reif et al 95 performed additional experiments focused sense representation including showing nearest neighbors based BERT representations outperform reported 5 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 WSD stateoftheart particularly following Hewitt Manning 42s methodology learn probe tailored sense representation To increase sensitivity sensespeciﬁc information Reif et al 95 loss considered difference average cosine similarity embeddings words senses embeddings words differ ent senses Both Hewitt Manning 42 Reif et al 95 approaches designed probing representations obtained single layers With ELMo Peters et al 84 introduced contextualized word representations obtained linear combination representations layers model This linear combination uses taskspeciﬁc weights learned optimization process referred literature scalar mixing produced better results downstream tasks compared representations obtained individual layers On closer inspection Peters et al 85 concluded layers effective semantic tasks possibly specialization language modeling tasks optimized pretraining Tenney et al 112 proposed edge probing methodology scalar mixing allowed evaluating different syntactic semantic properties common classiﬁer architecture probing models trained predict graph edges independently In Tenney et al 111 edge probing employed reveal BERT implicitly performed differ ent steps traditional NLP pipeline expected order information ﬂows model lower layers processing local syntax PartofSpeech higher layers processing complex semantics arbitrary distance Se mantic Roles Raising concerns remaining faithful information encoded representations Kuznetsov Gurevych 51 proposes reducing expressive power learned probes improving edge probing Liu et al 58 ran probing experiments simpler probes linear classiﬁers investigating differences tween NLM architectures ELMo GPT BERT ﬁnding competitive performance stateoftheart taskspeciﬁc models They conﬁrm LSTMbased models ELMo present taskspeciﬁc transferable lay ers Transformersbased models BERT predictable exhibit monotonic increase taskspeciﬁcity line ﬁndings GPT signiﬁcantly underperform ELMo BERT Liu et al 58 attributes fact GPT trained unidirectionally lefttoright ELMo BERT trained bidirectionally 332 Representational similarity Without recourse learned probes Ethayarajh 33 investigated differences ELMo GPT2 BERT relying experiments based cosine similarity learn contextspeciﬁcity representations Ethayarajh 33 layers highest degree contextspeciﬁcity layers models produced highly anisotropic representations directions vector space conﬁned narrow cone concluding property inherent sequence contextualization process The anisotropy observed contextualized NLMs supports hypothesis Reif et al 95 senselevel information encoded lowdimensional subspace contextualization crucial sense disambiguation Vuli c et al 119 reached similar conclusions detrimental contribution layers lexical tasks lexical semantic similarity ﬁnding improved results averaging different layers particularly taskspeciﬁc layer subsets prompting research layer weighting metaembedding approaches motivating present work Through direct comparison cosine similarities Chronis Erk 19 reached similar conclusions Vuli c et al 119 role layers lexical similarity tasks adding layers appear better approximate relatedness similarity Voita et al 116 probed Transformerbased NLMs InformationBottleneck perspective learn differences information ﬂow network according language modeling pretraining objectives particularly lefttoright MLM translation They ﬁnd MLM objective induces representation token identity lower layers fol lowed generalized token representation intermediate layers token identity information gets recreated layers Mickus et al 71 speciﬁcally veriﬁed BERT representations comprise coherent semantic space These exper iments explicitly detached learned probes Mickus et al 71 explains methodology interferes direct assessment coherence semantic space produced NLMs Using cluster analyses ﬁnd BERT appears represent coherent semantic space based representations ﬁnal layer Next Sentence Prediction NSP modeling objective leads encoding semantically irrelevant information sentence position corrupting similarity relationships complicating comparisons NLMs 4 Method We propose principled approach sense representation based contextual NLMs trained exclusively self supervision This approach extension Loureiro Jorge 61 addressing relevant issues largely unresolved particularly inﬂuence embeddings different layers composing NLMs introduction novel layer probing methodology Moreover work reinforce distinction sense disambiguation sense match ing introducing methodological differences speciﬁc application scenario This section starts explaining methods Loureiro Jorge 61 learning 41 extending 42 applying sense embeddings 43 Afterwards introduce proposed layer probing methodology 44 including resulting analysis informs grounded pooling operation combining embeddings layers NLM 6 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Fig 1 Overview learning sense embeddings annotated corpora Showing sense ψ mouse determined set sentences annotated sense Sψ padded special tokens expected NLM cid3 After pooling contextual embeddings Cψ layers L sense embedding cid3ψ computed centroid Cψ 41 Learning sense embeddings The initial process learn sense embeddings based senseannotated sentences contextualized embeddings annotated words phrases context An overview process seen Fig 1 Formally order generate sense embeddings learned context natural language require pretrained contextual NLM cid3 frozen parameters corpus senseannotated sentences S Every sense ψ represented set contextual embeddings cid3cl Cψ obtained employing cid3 set sentences Sψ annotated sense considering contextual embeddings speciﬁc tokens annotated sense ψ representations layer l L cid3ψ 1 Cψ cid3cl Cψ cid3Sψ cid2 cid2 1 lL cid3cCψ The L set layers typically sense representation 1 2 3 4 reversed layer indices discussed Section 31 Contextual NLMs typically operate subwordlevel tokenlevel embeddings cid3c produced cid3 correspond average tokens subword contextual embeddings depending NLM BPE WordPiece embeddings Similarly senseannotations cover span tokens use average corre 7 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 sponding tokenlevel embeddings contextual embedding Contextual NLMs pretrained special tokens speciﬁc locations include tokens expected positions CLS start SEP end BERT As described Section 23 WordNet represent senses ways sensekeys synsets Sense annotated corpora use sensekey annotations cases sensekey embeddings require interme diate mapping Synset embeddings derived sensekey annotations ways differentiate direct indirect In direct approach sensekey annotation converted mapped corresponding synset synset representations learned annotation instance In indirect approach ﬁrst learn sensekeylevel embeddings converting annotations compute synset embeddings average corresponding sensekey embeddings The approach explored earlier works sense embeddings 99 In work explore approaches 42 Extending coverage additional resources Given major issues supervised WSD lack sense annotations 80 quantity terms coverage sense inventory require solutions address method On matter follow methods ﬁrst proposed Loureiro Jorge 61 later optimized introduction UWA corpus Loureiro CamachoCollados 60 The methods ontological propagation gloss representation designed reach coverage sense inventory complementary exploiting different resources semantic relations senses glosses combined lemmas 421 Ontological propagation In Section 23 introduced WordNet different elements relations composing semantic network The ontological propagation method presented Loureiro Jorge 61 exploits relations senses WordNet order infer embeddings senses occur annotated corpora It possible infer accurate sense embeddings relations ﬁnegranularity WordNet widespread synonymy hypernymy relations extent Loureiro CamachoCollados 60 showed annotations unambiguous words signiﬁcantly improve propagation process Since available corpora provide fullcoverage annotations sense inventory following pro cess described Section 41 left represented senses cid4 set unrepresented senses cid4cid5 The propagation process involves steps increasingly abstract relations WordNet sets synonyms synsets hypernymy relations lexical categories lexnames supersenses In case targeting synsetlevel representations ﬁrst steplevel skipped Considering provided mappings sensekeys synsets hypernyms lexnames infer cid4cid5 iteratively following Algorithm 1 After sequential steps inferred cid3ψ added set represented senses cid4 This propagation method ensures fullcoverage provided initial sense embeddings cid4 suﬃciently diverse falling propagating lexnames supersenses possible Algorithm 1 Propagation method infer unrepresented senses cid4cid5 tations relations R sense embeddings cid4 learned anno Propagate cid4 cid4cid5 R foreach unrepresented sense ψ cid5 cid4cid5 Rψ cid5 represented cid3ψ cid4 ψ ψ cid5 R Rψ cid5 0 cid3ψ cid5 average sense embeddings Rψ cid5 Insert cid3ψ cid5 cid4 Removeψ cid5 cid4cid5 add represented remove unrepresented return cid4 cid4cid5 cid4 cid4cid5 Propagate cid4 cid4cid5 cid4 cid4cid5 Propagate cid4 cid4cid5 cid4 cid4cid5 Propagate cid4 cid4cid5 ψ ψ cid5 Synsetψ Synsetψ cid5 ψ ψ cid5 Hypernymψ Hypernymψ cid5 ψ ψ cid5 Lexnameψ Lexnameψ cid5 Since method designed achieve fullrepresentation sense inventory based subset senses observed context inferred representations similar contextual nature However initial set sense embeddings nearly complete particularly diversiﬁed propagation method produces number identical representations distinct senses undesirable disambiguation applications lesser extent sense matching applications 8 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 422 Leveraging glosses lemmas In Loureiro Jorge 61 introduced method representing sense embeddings based glosses lemmas This method inspired typical baseline approach works pertaining sentence embeddings amounts simply averaging contextual embeddings tokens present sentence In case use glosses sentences introduce lemmas gloss context By combining glosses lemmas augment information available represent senses able generate sense embeddings lemmaspeciﬁc sensekeylevel instead conceptspeciﬁc synsetlevel glosses As sense embeddings generated method address redundancy issue arising previously described propagation method simultaneously introducing representational information complementary contextual embeddings extracted senseannotated sentences The method proceeds follows For lemmasense pair sensekey sense inventory build template 2 lemmas race lemma sense lemmas sense gloss For instance based WordNet synset racev run provided following sensekeyspeciﬁc ﬁllouts template race23300 race run race compete race run23301 run run race compete race The initial lemma component template omitted target representation level synsets serves purpose reinforcing lemma speciﬁc sensekey The templated string processed cid3 similarly sentences S Section 41 use resulting set contextual embeddings token Ccid5 Considering complete set sense embedding cid4 based sense annotations propagation scribed Sections 41 421 augment cid3ψ cid4 gloss lemma information follows cid3ψ 1 2 cid3ψ2 1 Ccid5 cid2 cid2 lL cid3cCcid5 cid3cl2 Ccid5 cid3Templateψ 3 In contrast Loureiro Jorge 61 proposed concatenation merge new set sense embeddings based glosses lemmas previously mentioned set work propose merging averaging instead This departure motivated fact Loureiro Jorge 61 concatenation outperformed averaging WSD difference performance modest work interested additional tasks work cover Merging representations concatenation doubles dimensionality sense embeddings increasing computational requirements complicating comparison contextual embeddings potential applications On hand merging representations averaging allows adding components retaining similar vector equal dimensionality contextual embeddings represented vector space 43 Applying sense embeddings In section address sense embeddings employed solving tasks grouped paradigms disambiguation matching Disambiguation assigns word context sentence particular sense subset candidate senses restricted words lemma partofspeech Matching assigns speciﬁc senses words imposes restrictions admitting entry sense inventory assignment The different conditions disambiguation matching require sense representations different degrees lexical information semantic coherence Whereas disambiguation lexical information absent sense represen tations subset restrictions matching lexical information essential distinguish word forms carrying identical similar semantics Similarly disambiguation setting issues sense representations dis playing inconsistencies eat similar sleep drink belong disjoint subsets order coherence similarities relevant sense matching applications This distinction leads specialize sense embeddings accordingly Section 44 To disambiguate word w context start creating set cid4w candidate senses based lemma ofspeech information provided sense inventory Afterwards compute cosine similarities denoted cos words contextual embedding cid3c w precomputed embeddings sense subset cid4w layer pooling Finally assign sense similarity highest nearest neighbor cid4w cid3ψ cid4 lemma partofspeech ψ match w Disambiguationw arg max coscid3c w cid3ψ cid3ψcid4w 4 To match word w context restrictions follow approach disambiguation simply consider sense inventory cid4 instead cid4w Matchingw arg max cid3ψcid4 coscid3c w cid3ψ 9 5 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 44 Grounding layer pooling Up point described method closely following prior work Loureiro Jorge 61 As covered earlier sections NLMs substantial importantly unexpected variation task performance layers Considering works focus principled grounded sense representation NLMs methodology covers important aspect In section present methods targeting layers composing NLMs The ﬁrst method probes layers adept ness sense representation Consequently second method section designed capitalize knowledge sense representations better capture NLMs ability represent senses current paradigm As alluded Section 43 sense representation viewed light intended applications representations In particular work differentiate representations disambiguating words matching comparing senses This distinction motivated fact disambiguation prevalent senserelated task NLP requires sense representations adequately differentiated restricted set senses share lemmas partsofspeech However exist potential applications sense representations matched constraints sense inventory require senses coherently represented semantic space 441 Sense probing In order assess contribution individual layers pretrained NLM sense representation directly eval uate performance representations layers tasks related previously described disambiguation matching scenarios These tasks solved nearest neighbors approaches described Section 43 comparing precomputed sense embeddings contextual embeddings obtained layer For probing experiment follow method learning sense representations described Section 41 create multiple sets senses cid4l layer l NLM To maintain focus assessing performance representations learned directly speciﬁc layers ensure test instances senses represented senseannotated corpora precompute cid4l l L Thus probing experiments use techniques infer enrich sense representations described Section 43 act confounders The resulting performance scores layer l L composing speciﬁc cid3 corresponding cid4l reveal layers perform best inform layer pooling method described 442 Sense proﬁling We use probing results described earlier basis pooling operation better grounded current paradigm sum layers better performing later work We designate set modelspeciﬁc layer weights sense proﬁle consider distinct sense proﬁles disambiguation matching depending choice disambiguation matching tasks layer probing These proposed sense proﬁles immediate version Scalar Mixing Tenney et al 111 based heuristicallyderived sets layer weights instead learning task optimization Considering understand sense proﬁles closer extraction conﬁgurations Vuli c et al 119 Granted performance scores sl l L speciﬁc cid3 obtain layer speciﬁc weights wl l L applying softmax function wl cid3 expslt lcid5L expslcid5 t 4 We use temperature scaling parameter t skew weight distribution highest performing layers While simple temperature scaling surprisingly effective calibrating neural network predictions 39 This pa rameter determined empirically speciﬁc application settings models In Table 1 demonstrate interaction performance scores layer weights conditioned temperature parameter In table work use reverse layer indices consistently refer ﬁnal layer model 1 index regardless number layers NLM Consequently employ sense proﬁles comprised weights wl l L retrieve contextual embeddings cid3 generate sense embeddings accordingly updating formula 1 cid3ψ 1 Cψ cid2 cid2 lL cid3cCψ wl cid3cl Cψ cid3Sψ 5 This set sense embeddings learned annotations sense proﬁles undergoes extensions aug mentations described earlier 42 To clear process probing layer performance determining sense proﬁles pool contextual embeddings layers including learning sense embeddings annotations carried disambiguation matching settings independently As result produce sets sense embeddings NLM based sense proﬁles distinguish LMMS sense embeddings introduced Loureiro Jorge 61 LMMSSP Sense 10 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 1 Shows interaction F1 scores rounded 1NN WSD layer different NLMs respective weight distributions matching colors decreasing temperature parameters Lower temperatures induce higher skewness layers perform best probing validation set Distributions based t1000 uniform t0002 place mass single best layer For interpretation colors ﬁgurestables reader referred web version article Proﬁles The SPWSD Word Sense Disambiguation SPUSM Uninformed Sense Matching abbreviations refer sense embeddings based disambiguation matching sense proﬁles respectively 5 Experimental setting In section provide details experimental setting including description models 51 datasets 52 learning sense representations 51 Transformerbased language models In work experiment Transformerbased Language Models including original English BERT models released Devlin et al 29 BERTinspired alternatives XLNet 124 RoBERTa 59 ALBERT 52 This section brieﬂy describes relevant features models use case We summarize differences variant models Table 2 BERT The model released Devlin et al 29 ﬁrst prominent Transformerbased NLM designed language understand ing It pretrained unsupervised modeling objectives Masked Language Modeling MLM Next Sentence Prediction NSP English Wikipedia BookCorpus 128 It uses WordPiece tokenization splitting words differ ent components characterlevel subwords BERT available models differing parameter size tokenization casing The wholeword models released publication showing slightly improved benchmark performance trained words masked instead subwords resulting WordPiece tok enization 11 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 2 Feature comparison NLMs work Conﬁguration names shortened readability B Base L Large XL Extra Large XXL Extra Extra Large UNC Uncased WHL WholeWord Model Conﬁguration Params Layers Heads Dims Tokenization Tasks Corpus BERT XLNet RoBERTa ALBERT B BUNC L LUNC LWHL LUNCWHL B L B L B L XL XXL 110M 110M 340M 340M 340M 340M 110M 340M 125M 355M 11M 17M 58M 223M 12 12 24 24 24 24 12 24 12 24 12 24 24 12 12 12 16 16 16 16 12 16 12 16 12 16 16 64 768 768 1024 1024 1024 1024 768 1024 768 1024 768 1024 2048 4096 WordPiece WordPiece Unc WordPiece WordPiece Unc WordPiece WordPiece Unc MLM NSP MLM NSP MLM NSP MLM NSP MLM NSP MLM NSP SentencePiece SentencePiece Bytelevel BPE Bytelevel BPE SentencePiece SentencePiece SentencePiece SentencePiece PLM PLM MLM MLM MLM SOP MLM SOP MLM SOP MLM SOP 16 GB 16 GB 16 GB 16 GB 16 GB 16 GB 158 GB 158 GB 160 GB 160 GB 160 GB 160 GB 160 GB 160 GB XLNet Based TransfomerXL 26 architecture Yang et al 124 release XLNet featuring Permutation Language Modeling PLM pretraining objective The motivation PLM rely masked tokens makes pretraining closer ﬁnetuning downstream tasks It trained larger corpora BERT adding large volume web text sources corpora BERT Instead WordPiece tokenization XLNet uses SentencePiece 50 similar opensource version WordPiece RoBERTa The model proposed Liu et al 59 explicitly designed optimized version BERT RoBERTa use NSP pretraining objective ﬁnding deteriorates performance reported experimental setting performing MLM pretraining It trained different choices hyperparameters larger batch sizes improve performance downstream tasks The models released RoBERTa trained larger corpora composed web text similarly XLNet As tokenization RoBERTa opts bytelevel BPE following Radford et al 92 makes retrieving embeddings speciﬁc tokens challenging spacing explicitly encoded ALBERT Aiming lighter architecture Lan et al 52 propose ALBERT parametereﬃcient version BERT In spite changes introduced improve eﬃciency crosslayer parameter sharing ALBERT based similar architecture BERT Besides improving eﬃciency ALBERT improves performance downstream tasks replacing NSP challenging Sentence Order Prediction SOP objective ALBERT uses SentencePiece tokenization XLNet trained similar corpora It released conﬁgurations showing benchmark performance comparable BERT fewer parameters The set 14 model variants detailed Table 2 layerspeciﬁc validation performance WSD USM tasks For task evaluation analyses proceed single best performing model conﬁguration model family according results validation experiments We use Transformers package 122 v302 experiments BERT XLNet ALBERT fairseq package 79 v090 experiments RoBERTa4 52 Corpora training validation We learn initial set sense representations described Section 41 senseannotated corpora SemCor 74 Unambiguous Word Annotations corpus UWA 60 SemCor senseannotated version Brown Corpus remains largest corpus manual sense annotations despite age It includes 226695 annotations 33362 sensekeys 25942 synsets reaching coverage 161 WordNets sense inventory We use version released Raganato et al 94 includes mappings updated WordNet version 30 UWA recently introduced corpus composed exclusively annotations unambiguous words Wikipedia sentences Since WordNet composed unambiguous words UWA allows representing majority WordNet senses 567 combined SemCor direct annotations leads improved sense repre sentation senses learned propagation described Section 421 network effects UWA released versions different sizes work use version 10 examples sense denoted UWA10 includes 867252 annotations 98494 sensekeys 67860 synsets 4 Initial experiments RoBERTa showed slightly better results fairseq 12 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 In order avoid interference standard test sets perform layer analysis probing custom validation set based MASC5 corpus 46 following Loureiro et al 63 Considering layer experiments focused intrinsic properties NLMs custom version MASC corpus restricted include annotations senses occur SemCor Any sentence annotated senses occurring SemCor discarded leaving total 14645 annotations As layer experiments use sense embeddings learned SemCor validated restricted version MASC requiring strategies inferring senses ontological propagation fallbacks Most Frequent Sense 6 Probing analysis In section present outcome probing methodology described Section 44 applied models detailed Section 51 We report probing results section presented discussed evaluation analysis sections report downstream task results layer pooling informed probing analysis This section starts covering initial ﬁndings layer performance variation patterns observed models 61 Second present validation results proposed sense proﬁles disambiguation matching sce narios 62 Finally present rationale choosing sense proﬁle according type task 63 61 Variation layer performance NLMs As discussed Section 33 wellunderstood task performance varies considerably depending layers retrieve embeddings While works analyzed task performance layer speciﬁcally task WSD 9563 lacking indepth crossmodel comparison In Table 3 report WSD USM performance individual layers 14 models belonging 4 different Transformerbased model families These results obtained methodology described Section 441 We observe ﬁnal layer 1 optimal WSD USM performance More interestingly ﬁnd secondtolast layer 2 performs best WSD BERT models exception BERTLUNCWHL pattern hold models tested For models XLNetL ALBERTL ﬁnd best performing layers closer initialization layer INIT ﬁnal layer Another apparent pattern best performing layers USM consistently lower WSD This explained fact USM beneﬁts lexical information encoded lower layers initialization layer performs worst WSD demonstrating suﬃcient These empirical results suggest layer pooling strategy based ﬁxed set layers sum layers 14 accurately capture available sense information encoded pretrained NLMs 62 Sense proﬁles disambiguation matching In Section 442 described method uncovering modelspeciﬁc set layer weights informs weighted layer pooling results improved sense representations We applied method models dis ambiguation matching scenarios order verify proposed method reliably improves performance WSD USM tasks compared conventional pooling approaches Additionally compare different values temperature t parameter In order understand recommended t values actually result im proved testtime performance tasks run limited evaluation ALL test set Raganato et al 94 compare NLMs method described Section 421 glosses trained solely SemCor annotations Later Sections 71 72 report WSD USM results ﬁnal solution comparison current stateoftheart The conventional layer choices considered following lastﬁnal L1 secondtolast L2 sum 4 L1 L2 L3 L4 integer weighted sum 4 L1 2 L2 3 L3 4 L4 fractional weighted sum L3 L4 We tested temperature values t 0002 0005 001 01 106 Below discuss 4 1 4 ﬁndings impact sense proﬁles speciﬁc task L2 1 2 L1 1 3 The WSD validation results Table 4 reveal single best layer varies depending model Table 3 consistently outperforms sum 4 layers We ﬁnd WSD sense proﬁles t 0002 t 0005 perform comparably single best layer t 0002 slightly closer average Given close performance opt recommending t 0005 higher values likely overﬁt validation set biasvariance tradeoff In limited evaluation results Table 5 compare performance conventional layer choices WSD sense proﬁles recommended temperature value We observe 11 14 models WSD sense proﬁles recommended temperature reliably outperform conventional choices stands reliable 5 We use version MASC corpus released Vial et al 114 6 t 0001 results large exponents cause overﬂow errors 13 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 3 Performance variation development set models conﬁgurations sidered work Green represents best performing layers best marked cid5 red represents worst performing layers grey stands layers missing shallower variants crossmodel choice Moreover Table 5 WSD sense proﬁles recommended temperature generally match outperform single layers performed best validation set WSD sense proﬁles temperature value showed best performance validation set Our ﬁndings performance USM sense proﬁles largely follow previously mentioned ﬁndings WSD sense proﬁles In case USM validation results Table 6 clearly t 0100 performs better t 1000 performs As limited evaluation results Table 7 shows conventional layer choices signiﬁcantly underperform alternatives introduced work USM sense proﬁle recommended temperature t 0100 showing overall best performance 63 Choosing sense proﬁles different tasks Having established proposed sense proﬁles improve WSD USM performance conventional layer choices question remains choose WSD USM sense proﬁles represent sense embeddings In work propose choosing sense proﬁles based probing task shares similar constraints downstream task More speciﬁcally tasks requiring comparison different senses word ﬁt disambiguation pro ﬁle classical WSD 75 WiC 88 beneﬁt information lower layers On hand tasks lexical constraints USM synset similarity 20 semantic change 40 better suited 14 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 4 WSD validation results F1 Reports best single layer weighted sums speciﬁc sense proﬁles different t values model conﬁguration Sense representations experiment learned SemCor propagation required Model BERT XLNet RoBERTa ALBERT B BUNC L LUNC LWHL LUNCWHL B L B L B L XL XXL Sum LST4 716 719 738 727 720 715 666 665 725 741 683 694 682 724 Layer Best 725 2 730 2 747 2 729 2 730 2 727 8 709 3 727 17 729 3 749 10 689 5 705 15 714 13 738 6 t0002 t0005 t0010 t0100 t1000 Weighted Sum WS 724 729 747 729 731 726 710 730 728 749 683 702 711 735 722 728 745 727 726 721 712 734 726 747 682 700 711 734 718 723 743 727 718 720 712 733 722 744 681 699 710 733 696 703 722 715 704 715 703 724 716 736 676 693 705 728 686 691 709 708 691 707 699 718 712 731 673 693 704 724 Table 5 WSD test results F1 ALL Reports conventional layer choices alternatives sense proﬁles Recommended t USM 0005 Sense representa tions experiment learned SemCor propagation Model BERT XLNet RoBERTa ALBERT Layer 1 Layer 2 B BUNC L LUNC LWHL LUNCWHL B L B L B L XL XXL 721 735 733 734 720 720 691 662 719 712 706 701 643 694 729 735 739 736 734 730 674 704 733 740 696 705 690 737 Standard Sum LST4 726 730 740 739 732 729 648 657 733 741 701 705 688 739 WS I LST4 WS F LST4 725 733 740 740 731 728 627 648 734 740 701 706 678 731 725 733 740 740 730 728 637 661 733 739 703 704 672 725 Layer Best Dev 729 2 735 2 739 2 736 2 734 2 654 8 554 3 575 17 735 3 663 10 673 5 677 15 666 12 748 6 Proposed WS Rec t 728 734 742 740 735 731 723 738 736 747 697 711 730 751 WS Best t 729 002 735 002 740 002 738 002 734 002 731 002 723 005 738 005 737 002 747 002 697 002 707 002 730 002 751 002 matching proﬁle uses information layers In Section 7 evaluate sense embeddings learned sense proﬁles according tasks constraints Section 81 analyze performance gap alternate sense proﬁles 7 Evaluation In work address senserelated tasks selected investigate versatility proposed sense em beddings covering disambiguation 71 WSD matching 72 USM meaning change detection 73 74 WiC GWCS sense similarity 75 SID For task report new results LMMSSP comparison stateoftheart original LMMS 61 sense embeddings For brevity consider variant model family showed best results probing analysis 6 In comparisons omit LMMS2348 sense embeddings concatenated fastText 12 word embeddings exclusively based representations particular NLMs focused work All tasks solved essentially cosine similarity contextual embeddings LMMSSP precomputed sense embeddings represented NLM Each tasks subsection provides details similarities produce taskspeciﬁc predictions No additional taskspeciﬁc training validation datasets asides referred Section 52 NLMs employed exact fashion simply retrieving contextualized representations layer following 41 15 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 6 USM validation results F1 Reports best single layer weighted sums speciﬁc sense proﬁles different t values model conﬁguration Sense representations experiment learned SemCor propagation required Model BERT XLNet RoBERTa ALBERT B BUNC L LUNC LWHL LUNCWHL B L B L B L XL XXL Sum LST4 592 586 577 568 597 591 347 280 616 641 600 600 549 658 Layer Best 612 6 631 8 638 14 643 14 621 14 648 14 612 9 636 18 642 9 653 17 618 8 641 16 645 18 658 9 t0002 t0005 t0010 t0100 t1000 Weighted Sum WS 615 631 639 644 621 647 613 635 640 652 619 635 642 657 615 632 640 646 620 645 614 636 639 654 618 635 641 661 618 635 639 646 621 645 614 637 640 658 617 632 642 661 622 637 645 654 627 647 619 644 640 661 616 633 645 662 620 637 645 657 625 648 604 641 639 662 616 632 646 663 Table 7 USM test results F1 ALL Reports conventional layer choices alternatives sense proﬁles Recommended t USM 01 Sense representations experiment learned SemCor propagation Model BERT XLNet RoBERTa ALBERT Layer 1 Layer 2 B BUNC L LUNC LWHL LUNCWHL B L B L B L XL XXL 531 504 539 483 533 537 381 279 537 563 538 557 412 551 512 504 503 467 543 524 369 413 532 567 536 555 485 606 Standard Sum LST4 537 530 525 490 545 533 314 287 551 579 547 560 498 617 WS I LST4 WS F LST4 530 518 528 488 542 533 278 282 553 581 542 561 480 610 530 523 534 488 545 533 289 296 555 580 548 561 474 605 Layer Best Dev 570 6 579 8 589 14 585 14 576 14 583 14 573 9 590 18 583 9 606 17 558 8 574 16 599 18 606 9 Proposed WS Rec t 577 588 600 603 584 594 573 604 592 612 562 584 601 623 WS Best t 577 100 588 100 600 100 604 100 584 100 596 100 573 100 604 100 592 100 612 100 563 002 573 005 598 100 623 100 Table 8 Example WSD instance Raganato et al 94 Sentence lemma partofspeech POS provided The goal predict correct sensekey sense WordNet Sentence Eyes clear bright strange intensity sort cold ﬁre burning Lemma ﬁre POS NOUN Gold Sensekey ﬁre11200 As LMMSSP performance tasks indicative NLMs intrinsic ability approximate mean ing representations learned pretraining language modeling objectives 71 Word sense disambiguation WSD WSD popular obvious task evaluating sense embeddings This task researched early days Artiﬁcial Intelligence constitutes AIcomplete task 75 It usually formulated choosing correct sense word context list possible senses given words lemma partofspeech tag Table 8 Several test sets proposed years compilation Raganato et al 94 emerged facto evaluation framework English WSD use Naturally task suits sense proﬁles WSD follow method described Section 43 16 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 9 F1 scores test set WSD Evaluation Framework 94 Top rows results models sense annotations exclusively SemCor SC Bottom rows results models augmenting SC annotations additional sources Results marked correspond development sets ALL For group results underline best LMMS Model 1NN Defs Rels SE2 n2282 SE3 n1850 SE07 n445 SE13 n1644 SE15 n1022 ALL n7253 C S r o C m e S MFS context2vec 68 ELMo 84 BERTL 61 SVC 115 GlossBERT 44 EWISER 9 BEM 11 ConSeC 5 LMMS1024 61 LMMS2048 61 LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL s SVC 115 r e h t O C S KnowBERT 86 EWISER 9 ARES 105 A LMMSSPBERTL W LMMSSPXLNetL U LMMSSPRoBERTaL C S LMMSSPALBERTXXL 711 Results cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 656 718 716 763 766 777 789 794 823 754 763 761 760 772 774 794 764 808 780 767 761 774 777 660 691 696 732 769 752 784 774 799 740 756 740 731 735 748 781 760 790 771 741 731 735 750 545 613 622 662 690 725 710 745 774 664 681 670 664 679 710 714 714 752 710 664 659 677 705 638 656 662 717 738 761 789 797 832 727 751 752 742 755 747 778 731 807 773 752 742 753 747 671 719 713 741 754 804 793 817 852 753 770 774 749 764 748 814 754 818 832 776 750 767 749 648 690 690 735 754 770 783 790 820 738 754 750 741 752 754 785 751 801 779 752 741 752 755 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 cid2 On Table 9 report performance standard test sets WSD Evaluation Framework 94 Given breadth recent WSD solutions results comparable separating solutions SemCor annotations solutions augmenting SemCor senseannotated datasets In case LMMSSP combine SemCor unambiguous annotations UWA easily retrieved unlabeled corpora We report solutions use glosses relations sense annotations solutions based 1NN ﬁrst nearest neighbor precomputed sense embeddings represented space NLMs When considering SemCor source annotations LMMS LMMSSP remain best solutions based 1NN NLMspace Most notably LMMSSPALBERTXXL able match performance LMMS2048 combination test sets ALL concatenating gloss embeddings As expected taskspeciﬁc classiﬁers best results particularly BEM 11 ConSeC 5 Generally solutions ﬁnetuning NLMs combining classiﬁers trained WSD task improved performance 1NN Despite Loureiro et al 63 shown 1NN solutions offer advantages better sample eﬃciency frequency biases versatility advocated current work Allowing additional annotations ﬁnd LMMSSP results improve slightly BERTL ALBERT XXL ARES 105 uses semisupervised annotations increase coverage sense inventory sense embeddings represented space BERTL Results ARES dataset leads improved WSD performance comparison LMMSSP reported test sets particularly SE13 SE157 72 Uninformed sense matching USM We introduced USM task Loureiro Jorge 61 variation WSD accurately represent extent NLMs associate words phrases senses WordNet inventory The crucial difference relation WSD USM task use supplemental information restrict candidates sense inventory compare examples Table 8 Table 10 Conveniently allows USM use test sets WSD As expected address USM sense proﬁle follow method described Section 43 In work evaluate USM sensekey synset perspective provide clearer account impact lexical information task performance 7 We expect annotations ARES produce performance gains LMMSSP use resource large size 13x annotations SemCorUWA10 17 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 10 Example USM instance adapted WSD Evaluation Framework 94 The correct sensekey synset predicted separate evaluations Sentence Eyes clear bright strange intensity sort cold ﬁre burning Gold Sensekey ﬁre11200 Gold Synset 06711159n ﬁre9 n Table 11 USM results ALL test set WSD Evaluation Framework 94 sense synsetlevel Results marked obtained synset embeddings converted sensekey embeddings Model ARES LMMS1024 61 LMMS2048 61 LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL F1 614 522 348 608 601 622 629 Sensekeys P5 847 669 603 867 873 869 876 MRR 718 590 463 722 719 731 737 F1 607 294 325 510 517 502 527 Synsets P5 865 539 589 817 827 801 819 MRR 718 407 445 643 651 633 655 Table 12 Examples WiC training set Showing independent instances Sentence Pairs You carry camping gear Sound carries water He wore jock strap metal cup Bees ﬁlled waxen cups honey Lemma carry POS VERB Boolean False cup NOUN True 721 Results Following Loureiro CamachoCollados 60 evaluate performance considering additional metrics F1 Precision 5 P5 Mean Reciprocal Rank MRR To knowledge ARES 105 publicly available set fullcoverage sense embeddings represented space Transformerbased NLM compare LMMSSP sense embeddings Since prior LMMS sense embeddings ARES sense embeddings released ing sensekey representations USM synset evaluation requires converting sensekey embeddings synset embeddings We perform conversion simply averaging sensekey embeddings belong synset In Section 82 analyze impact conversion task performance On Table 11 observed LMMSSP dramatically improves performance LMMS metrics sidered The poor performance LMMS2048 comparison LMMS1024 suggests concatenating gloss embeddings detrimental USM performance particularly F1 metric In comparison consider LMMS2348 sense embeddings concatenated fastText static embeddings resulting 300 dimensions having ex act distribution sense embeddings corresponding identical lemmas This property LMMS2348 makes comparison inequitable diverts works focus intrinsic capabilities Transformer NLMs Interestingly ﬁnd targeting sensekeys LMMSSPALBERTXXL shows best performance metrics ARES based BERTL outperforms LMMSSPBERTL F1 metric However targeting synsets additional contexts ARES prove advantageous observe similar performance gap sensekeys synset LMMSSP expected considering additional contexts ARES targeted synsetlevel 73 Wordincontext WiC The WordinContext 88 WiC task designed assess context impacts word representations produced textual NLMs It binary classiﬁcation task simply requires determining particular word meaning pair sentences given lemma POS provided WSD tasks Table 12 examples The dataset balanced performance measured accuracy 731 Solution In work tackle WiC task proposed sense embeddings following unsupervised approach Loureiro Jorge 62 essentially applies 1NN method disambiguating target word sentences checks equal Even explored supervised approach Loureiro Jorge 62 based Logistic Regression work focus unsupervised approach performance revealing inherent representational abilities NLMs Given close relation disambiguation use WSD sense proﬁles WiC 18 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 13 Results Accuracy test set WiC task comparing unsupervised approach stateofart Best results approach reported bold Our results obtained Codalab online platform Results marked SuperGLUE version WiC test set minor preprocessing differences Method FineTuning Logistic Reg FineTuning FineTuning FineTuning FineTuning FineTuning 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD 1NN WSD d e s v r e p u S d l o h s e r h T d e s v r e p u s n U Language Model Sense Embeddings BERTL 120 BERTL RoBERTaL 59 KnowBERT 86 SenseBERT 54 T5 93 BERTL context2vec 68 ELMo 84 BERTL BERTL BERTL XLNetL RoBERTaL ALBERTXXL LMMS2048 62 ARES 105 JBT 81 DeConf 90 SW2V 65 LessLex 21 ARES 2020 LMMS2048 61 LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL Acc 696 681 699 709 721 769 722 536 587 593 581 577 592 676 663 674 661 678 679 732 Results WiC benchmark NLU task SuperGLUE 120 stateoftheart NLMs reported results task The initial baseline methods proposed WiC based cosine similarity thresholds learned validation set Most recent solutions involve ﬁnetuning NLM performed sentence classiﬁcation tasks SuperGLUE training validation sets provided WiC One notable exception Scarlini et al 105 proposed method leverages ARES sense embeddings improve ﬁnetuning process As Table 13 compare results solutions unsupervised LMMS LMMSSP unsupervised result based 1NN approach ARES embeddings Starting unsupervised results conﬁrm LMMSSPBERTL surpasses performance LMMS2048 based BERTL LMMSSPALBERTXXL displays best performance Nevertheless supervised solutions NLMs ﬁnetuned task best performance overall particularly T5 93 currently largest NLM reported results task 11B parameters KnowBERT 86 SenseBERT 54 NLMs based BERT augmented sense information WordNet SemCor resources showing improved performance comparison ﬁnetuning original BERTL The method Scarlini et al 105 employ sense embeddings ﬁnetuning BERTL WiC resulted table improvement similar SenseBERT In unsupervised setting ARES embeddings outperform LMMSSPBERTL underperform LMMSSPRoBERTaL LMMSSPALBERTXXL We expect following method assist supervised ﬁnetuning LMMSSP sense embeddings produce improved results consider experiment scope work As solutions threshold method reported models substantially underperform unsupervised results Transformerbased NLM 74 Graded word similarity context GWCS For evaluating graded contextual similarity contrast binary contextual similarity assignments WiC address SemEval 2020 Task 3 Graded Word Similarity Context GWCS 2 This task based CoSimLex resource 3 targets word pairs evaluating distributional semantic models necessarily polysemous words contexts spanning multiple sentences The task divided subtasks derived humanannotated similarity ratings 1 predict change similarity different contexts word pair 2 predict similarity ratings Table 14 shows single example GWCS featuring contexts occurrences pair words context speciﬁc similarity ratings associated similarity change 741 Solution While subtasks independently evaluated employ essentially method based straightforward approach WiC task covered Section 73 minor adjustments quantify observed change similarity Given contexts A B disambiguate target words instances word pair corresponding texts compute sense similarities sim A wsd cosine similarity embeddings predicted wsd simB 19 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 14 Example practice set GWCS single instance Contexts A B corresponding similarity ratings keepprotect word pair Contexts Sim Change A B Tim Drake keeps memorial cave hideout underneath Titans Tower San Francisco It later revealed Dr Leslie Thompkins faked death gang war effort protect Shisa wards believed protect evils When pairs shisa left traditionally The open mouth ward evil spirits closed mouth good spirits 444 392 052 Table 15 Results subtasks SemEval 2020 Task 3 032021 considering evaluation postevaluation submissions Users 1 Ferryman 2Alexa ARES results obtained method LMMS replacing corresponding sense embeddings Model Leaderboard Best ARES 105 LMMS1024 61 LMMS2048 61 LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL Subtask1 7741 769 741 757 762 787 757 752 Subtask2 7462 745 742 745 744 766 749 718 senses Considering disambiguation predict senses resulting sim A wsd instances compute contextual similarities sim A ctx cosine similarity contextual embeddings target words Thus determine similarity scores speciﬁc context A sim A 1 2 sim A ctx similarity scores speciﬁc context B simB 1 ctx These contextspeciﬁc similarities constitute solutions subtask 2 We determine semantic change scores subtask 1 trivially simB sim A Considering solution closely follows solution WiC word pairs contained dataset tend closely related use WSD sense proﬁle GWCS ctx simB 2 simB sim A simB wsd wsd wsd simB 742 Results Performance subtask 1 measured Pearson Uncentered Correlation systems scores aver age human annotations performance subtask 2 measured harmonic mean Spearman Pearson correlations systems scores average human annotations On Table 15 report results sense embeddings LMMS LMMSSP ARES sense embeddings scoring method BERTLarge pooling sum 4 layers best reported results task participants including postevaluation 032021 Similarly WiC scores test sets hidden participants evaluation ended 032020 postevaluation periods extends indeﬁnitely reported results obtained online platform SemEval submitting systems predictions We observe straightforward method combining similarity sense contextual embeddings able outperform solutions task participants Leaderboard Best relied Transformerbased NLMs 2 Interestingly GWCS shows wide variation performance choice NLM LMMSSPXLNetL standing clearly best results subtasks8 75 Sense similarity All tasks considered far WSD USM WiC GWCS evaluated sense embeddings utility accurately matching distinguishing word senses particular contexts In task address intrinsic evaluation sense embeddings directly comparing distributional similarity sense pairs human similarity ratings We perform evaluation Sense Identiﬁcation Dataset 22 SID based word pairs nouns human similarity ratings SemEval2017 Task 2 16 addition mapping word pairs particular senses BabelNet sense inventory examples Table 16 8 Complete leaderboard results available Appendix A Additionally Appendix B reports performance Stanford Contextual Word Similarities 43 task inspired GWCS WiC tasks similar conclusions 20 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 16 Two examples paired synsets human similarity ratings SID dataset Showing synset identiﬁers conversion WordNet readable format parenthesis Synset 1 08570634n hayﬁeld1 n 03169390n decoration1 n Synset 2 08598301n grassland1 n 03291741n envelope2 n Similarity 358 008 Table 17 Performance Pearson Correlation adapted SID dataset All reported embeddings feature 300 dimensions Embeddings marked converted sensekeys LessLex NASARI embeddings converted BabelNet WordNet mapping applied SID adaptation Synset Embeddings fastText 12 NASARIUMBC 17 DeConf 90 LessLex 21 SensEmBERT 104 ARES LMMS2048 61 LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL c t t S l u t x e t n o C WN Full Coverage cid2 cid2 cid2 cid2 cid2 cid2 cid2 All n377 Overlapping All n354 Polarized n182 Observed n297 644 751 825 669 706 712 778 795 741 774 635 716 749 823 668 704 722 778 796 742 772 693 791 806 855 746 805 762 804 812 790 805 654 744 769 851 695 733 763 831 845 809 814 751 Task adaptation We convert BabelNet sense identiﬁers synsets WordNet 30 mapping provided Navigli Ponzetto 76 However instances mapped missing entries WordNet rare cases mapping results senses pair equal leading reduction 492 instances 377 mapped Word Net We split SID different groups additional insights We ﬁrst separate 354 pairs senses represented related works compare overlapping considering complete sets WordNet sense embeddings Next breakdown overlapping pairs set polarized word pairs similarity ratings 1 3 set containing pairs senses annotated SemCorUWA10 observed 752 Solution We use cosine similarity synset embeddings correlate human similarity ratings Since directly comparing embeddings different dimensionality apply truncated SVD normalize 300 dimensions9 including related work The senses compared range completely unrelated polyhedron1 n highly related similar actor1 n use USM sense proﬁles SID n actress1 n actor1 753 Results Performance SID measured Pearson correlation For completeness report performance synset embed dings based contextual NLMs including new results based fastText embeddings trained Common Crawl10 Results related works based sense embeddings provided authors converting sensekeys synsets averaging corresponding embeddings required Section 72 The interannotator agreement set n377 reaches 879 measured averaged pairwise Pearson correlation original SemEval2017 human simi larity scores Results WordNetsubset SID shown Table 17 As observed LMMSSP substantially outperforms LMMS related works As GWCS LMMSSPXLNetL stands clearly best results While LMMSSP performs noncontextual embeddings underperforms LessLex 21 embeddings based ensembles learned BabelNet We note LMMSSP performs particularly Observed set corresponding senses learned annotated corpora The performance gap ARES LMMSSPBERTL suggests additional semisupervised annotations senses suﬃce Finally Polarized set consistently easier set indicating challenging pairs moderate similarity ratings 9 We veriﬁed SVDreduced embeddings outperform original embeddings 10 fastText embeddings given synset computed averaging word embeddings lemma belongs input synset WordNet 21 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 18 Impact pooling operation task performance Underline highlights pooling operation performed best NLM task Bold highlights NLM pooling operation formed best task cid5 denotes default choice LMMSSP Task Metric Pooling BERTL XLNetL RoBERTaL ALBERTXXL WSD F1 ALL USM P5 ALL WiC ACC Val GWCS COR ST2 SID COR ALL SumLST4 SPWSD cid5 SPUSM SumLST4 SPWSD SPUSM cid5 SumLST4 SPWSD cid5 SPUSM SumLST4 SPWSD cid5 SPUSM SumLST4 SPWSD SPUSM cid5 752 752 729 746 736 867 718 718 677 744 763 734 774 763 778 564 741 734 659 816 873 610 679 650 549 787 754 411 777 795 748 752 742 746 831 869 721 688 685 733 757 772 738 735 741 738 755 745 743 857 876 668 687 679 715 752 759 728 753 774 8 Analysis In section perform ablation studies better understand impact individual contributions introduced works extension LMMS These experiments target NLMs tasks11 addressed previous evaluation section Our ablation analyses cover impact sense proﬁles 81 UWA annotations 82 merging gloss representations 83 indirect representation synsets 84 Considering partofspeech important factor disambiguation sense representation report performance partofspeech LMMS LMMSSP sense embeddings WSD USM tasks 85 81 Choice sense proﬁles On Table 18 report performance according sense proﬁle weighted pooling contextual embeddings NLMs described Section 62 sum 4 layers SumLST4 commonly related work original LMMS 61 Our ﬁrst conclusion SumLST4 pooling appropriate particular models tasks WSD WiC wBERTL WiC wRoBERTaL detrimental specially task wXLNetL model USM However recommended choice sense proﬁle appears beneﬁcial WSD USM tasks models expected sense proﬁles based tasks WiC GWCS SID In fact 20 modeltask combinations ﬁnd 3 exceptions RoBERTaL WiC GWCS ALBERTXXL GWCS lesser extent Moreover conﬁrm tasks sensitive choice SPWSD SPUSM validate taskspeciﬁc recommendations 82 Unambiguous word annotations In work learnt initial set sense embeddings described Sections 41 421 SemCor source sense annotations LMMS 61 combination UWA 60 set sense annotations exclusively targeting unambiguous words On Table 19 present results showing impact UWA task performance As noted Loureiro Camacho Collados 60 increase WordNet coverage UWA allows disentangling dense clusters coarsen semantic space relying SemCor network propagation Consequently expect UWA beneﬁt sense matching tasks conﬁrmed results showing substantial improvements USM SID tasks SPUSM We ﬁnd UWA hinder performance remaining tasks modeltask combinations improves cases exceptions GWCS RoBERTaL WiC ALBERTXXL exception observed sense proﬁle ablation 81 As future work explore WordNetindependent procedures discover monosemous words method introduced Soler Apidianaki 108 lead improvements 11 Due leaderboard submission limits ablations WiC use validation set 22 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 19 Impact sense annotations task performance Underline highlights pooling operation performed best NLM task Bold highlights NLM pooling operation performed best task cid5 denotes default choice LMMSSP Task Metric Annotations BERTL XLNetL RoBERTaL ALBERTXXL WSD F1 ALL USM P5 ALL WiC ACC Val GWCS COR ST2 SID COR ALL SemCor SemCorUWA cid5 SemCor SemCorUWA cid5 SemCor SemCorUWA cid5 SemCor SemCorUWA cid5 SemCor SemCorUWA cid5 750 752 763 867 712 718 761 763 721 778 741 741 765 873 671 679 787 787 752 795 752 752 761 869 685 688 763 757 653 741 754 755 774 876 691 687 727 752 735 774 Table 20 Impact merging gloss representations task performance Underline highlights pooling operation formed best NLM task Bold highlights NLM pooling operation performed best task cid5 denotes default choice LMMSSP Task Metric Glosses BERTL XLNetL RoBERTaL ALBERTXXL WSD F1 ALL USM P5 ALL WiC ACC Val GWCS COR ST2 SID COR ALL Without Exclusively Averaged cid5 Concatenated Without Exclusively Averaged cid5 Concatenated Without Exclusively Averaged cid5 Concatenated Without Exclusively Averaged cid5 Concatenated Without Exclusively Averaged cid5 Concatenated 746 571 752 755 835 464 867 850 668 663 718 693 756 751 763 757 695 683 778 767 743 552 741 743 839 446 873 857 646 622 679 665 773 729 787 772 725 700 795 779 753 551 752 753 837 405 869 858 687 668 688 685 750 750 757 758 620 651 741 699 755 543 755 748 842 434 876 861 671 641 687 677 744 686 752 748 683 659 774 738 83 Merging gloss representations Another aspect LMMSSP differs LMMS 61 merging gloss embeddings averaging sense embed dings instead concatenation described Section 422 Results Table 20 reveal concatenation beneﬁts WSD minor improvements averaging Alternatively averaging shows clear improvements tasks exception GWCS wRoBERTaL We report performance exclusively gloss representations sense embeddings gloss information Surprisingly sets results close WiC GWCS SID showing unsupervised representations learned glosses competitive particular tasks 84 Learning synsets directly The SID task synset version USM require synsetlevel embeddings In Section 41 explain LMMSSP synset embeddings learned directly sensekey annotations converted synsets However evaluation compare LMMSSP works available sensekey embeddings converted representations synset embeddings learned average corresponding sensekey embeddings learned indirectly 23 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 21 Impact learning synset representations directly annotations indirectly average corresponding sensekey embeddings Underline highlights pooling operation performed best NLM task Bold highlights NLM pooling operation performed best task cid5 denotes default choice LMMSSP Task Metric Synset Repr BERTL XLNetL RoBERTaL ALBERTXXL USM P5 ALL SID COR ALL Indirect Direct cid5 Indirect Direct cid5 745 817 773 778 761 827 786 795 773 801 730 741 763 819 755 774 Table 22 Performance combined set Raganato et al 94 grouped partofspeech Reporting F1 WSD P5 USM MFS applicable USM Model MFS LMMS1024 2019 LMMS2048 2019 LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL Nouns Verbs Adjectives Adverbs WSD 676 756 780 780 768 782 778 USM NA 482 543 872 875 869 873 WSD 496 636 640 630 633 631 656 USM NA 653 646 842 866 868 866 WSD 783 798 807 803 769 792 791 USM NA 756 740 853 868 861 867 WSD 805 850 835 838 850 847 841 USM NA 786 772 965 965 962 971 On Table 21 compare LMMSSP embeddings learned directly indirectly showing learning represen tations directly leads average improvement models 73 USM 15 SID The fact indirect representation synsets reduced impact SID performance comparison USM suggests indirect represen tation leads intermingled synset embeddings harder rank nearly globally coherent learned direct representation 85 Partofspeech performance In Loureiro Jorge 61 presented error analysis targeting partofspeech mismatch predicted groundtruth senses showed verbs particularly challenging In work complement results reporting performance partofspeech comparing LMMS 61 LMMSSP Our results Table 22 conﬁrm verbs remain challenging partofspeech disambiguate correctly al ALBERTXXL shows appreciably better verb results NLMs work Considering ranked USM matches ﬁnd narrower gap verbs partsofspeech LMMSSP verbs performing comparably adjectives nouns BERTL showing differences larger 1 It interesting note XLNetL outperforms equals ALBERTXXL USM partsofspeech exception adverbs providing better insight overall performance differences reported USM evaluation 72 ALBERTXXL outperforms XLNetL 9 Discussion In section discuss main ﬁndings work More speciﬁcally discuss sense representation speciﬁc layers NLMs 91 differences observed models variants 92 ﬁnally sense embeddings beneﬁt downstream tasks 93 91 Layer distribution Throughout article provided empirical evidence supporting substantial nonmonotonic variation adeptness speciﬁc layers Transformerbased NLMs sense representation This evidence available probing analysis improvements senserelated tasks obtained proposed sense proﬁles based nonmonotonic pooling layers clearly shown Table 3 The cause variation remains elusive calling controlled experiments different NLMs tested comparable circumstances particularly regards training data modeling objectives experimen tal setup costprohibitive models scale Nevertheless seeking better understand variation conducted qualitative experiments targeting representations sentences different layers In ﬁrst qualitative experiment compared sense similarity different layers word context We evidence potentially support hypothesis advanced Voita et al 117 distribution ﬁnal 24 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Fig 2 Cosine similarity speciﬁc layers word square context 3 senses annotated SemCor represented ALBERTXXL The 3 senses square correspond shape public square correct green bar chart mathematical operation order Initial layer similarities inﬂuenced word forms sense SemCor Table 23 Mean silhouette scores 20 words 10shot training instances balanced CoarseWSD20 63 Top rows report scores speciﬁc layers rows report scores pooling sum 4 layers proposed pooling strategies Pooling First Layer Middle Layer Final Layer Sum Last 4 SPWSD SPUSM BERTL XLNetL RoBERTaL ALBERTXXL 0156 0384 0369 0376 0377 0388 0064 0167 0049 0088 0250 0255 0010 0180 0210 0218 0203 0196 0137 0328 0273 0347 0387 0390 layers resembling distribution ﬁrst layers moreso distribution middlemost layers difference correct incorrect senses marked example Fig 2 We extended previous experiment clusterlevel comparison embedding space For experiment focus words present CoarseWSD20 dataset 63 aggregate measuring correct sense clustering targeting spring distinct senses visualization Considering silhouette scores12 100 PCA visualizations embedding space Table 23 Fig 3 arrived similar conclusions ﬁnal layers tend produce accurate representations layers closer middle ﬁrst layer lowest scores Our proposed layer pooling methods generally improved clustering comparison sum layers In addition experiment conﬁrms unexpected ﬁnding different pattern semantic representation layers XLNetL representations ﬁnal layer showing atypical dispersion We leave thorough largescale analysis phenomenon future work alongside appropriately account measuring granularity different senses word confounding factors 92 NLM idiosyncrasies Besides unexpected results performance particular layers NLMs ﬁnd intriguing differences patterns layer performance observed models variants model Looking results Table 3 ﬁnd intriguing examples variation For WSD task striking examples differences BERTLUNCWHL BERT model bimodal distribution XLNet For example XLNetB exhibited bestperforming layer near model bestperforming layer XLNetL bottomhalf model While results USM consistent 12 We use mean silhouette coeﬃcients embeddings particular word measure model pooling strategy assign embeddings correct sense cluster Silhouette coeﬃcients based intra nearestcluster cosine similarities Low values represent overlapping clusters 25 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Fig 3 Visualization embedding spaces different pooling strategies The rows correspond proposed pooling strategies 44 6 Each point corresponds embedding word spring context provided 10shot set CoarseWSD20 63 Using PCA dimensionality reduction Silhouette scores s computed reduction ﬁnd peculiarities XLNet models showing worstperforming layers ALBERTXL showing biased distribution ALBERT variants The reasons differences patterns models variants straightforward specially considering models trained similar data architectures Still technical differences highlight differences modeling objectives covered Section 51 Out 4 models considered work formance summary Table 24 XLNet fact simultaneously model appears distinctive particularly strong performance graded similarity tasks objectives different model MLM Another interesting ﬁnding obtain best results WSD USM WiC ALBERTXXL half layers models larger embedding dimensionality model details available Table 2 As 26 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Table 24 Summary comparison different NLMs LMMSSP approach Model LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL WSD F1 752 741 752 755 USM P5 867 873 869 876 WiC ACC 674 661 678 679 GWCS COR 763 787 757 752 SID COR 778 795 741 774 Fig 4 Example sentence token matched LMMSSPALBERTXXL sense embeddings presenting synsets 5 nearest neighbors SPUSM sense proﬁle Shows direct hypernymy relations IsA included WordNet WN matched synsets hypernymy relations shared matched unmatched synset deducible generalizations 5 matches Finally VerbNet VN semantic frame matched sentence highlighting LMMSSP enables generalization argument spans differences variants model objectives consider possibility trivial runtime parameters impact variation akin unexpected inﬂuence random seeds ﬁnetuning BERT models 30 93 Knowledge integration The ability matching WordNet synsets fragment text allows downstream applications easily leverage manually curated relations available WordNet At time sense embeddings serve entry point knowledge bases linked WordNet multilingual knowledge graph BabelNet 76 commonsense triples ConceptNet 109 WebChild 110 semantic frames VerbNet 106 images ImageNet 101 Visual Genome 49 Several recent works symbolic relations expressed knowledge bases improve neural solutions Natural Language Inference 47 Commonsense Reasoning 57 Story Generation 1 As example LMMSSP bridge natural language symbolic knowledge beneﬁcial Fig 4 demonstrate sense embeddings allow generalization argument spans predicted semantic parser exploiting WordNet relations matched synsets The matches shown Fig 4 illustrate sense embeddings probing world knowledge encoded pretrained NLMs suggested Loureiro Jorge 61 27 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 10 Conclusion Leveraging neural language models combination senseannotated corpora complementary resources glosses relations work shown possible produce sense embeddings applicable mere disambiguation relevant implications longstanding challenges Artiﬁcial Intelligence symbol grounding This extension Loureiro Jorge 61 proposes principled approach learning distributional representations word senses pretrained NLMs focusing stateoftheart Transformerbased models From extensive evaluation senserelated tasks demonstrated LMMSSP approach effective prior work approximating precise word sense representations vector space NLMs The broad probing analysis variants popular NLMs endeavored work provides new evidence supporting research interplay pretraining objectives layer specialization model size The clusions probing analysis expected applicable tasks outside WSD learning representations sense embeddings leave future work Effectively known limitations meaning representation based language modeling objectives 669 Nonetheless believe work shows understand best leverage NLMs meaning representation addition thoroughly testing effectiveness current approaches centered selfsupervision Release This work accompanied release following resources sensekey synset embeddings coverage WordNet based BERTL XLNetL RoBERTaL ALBERTXXL scripts generate embeddings following method NLMs supported Transformers package scripts run task evaluations These resources released GNU General Public License v3 available public repository httpsgithub com danlou lmms Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Acknowledgements We thank reviewers thoughtful comments suggestions Daniel Loureiro supported EU Fundac ão para Ciência e Tecnologia contract DFABD90282020 Programa Operacional Regional Norte Jose CamachoCollados supported UKRI Future Leaders Fellowship MRT0420011 Appendix A Full results SemEval 2020 Task 3 On Table 25 report complete leaderboard results subtasks 1 2 SemEval 2020 Task 3 including languages English evaluation period Table 25 Results leaderboard subtasks 1 2 SemEval 2020 Task 3 Predicting Graded Effect Context Word Similarity Rank reported team names At time evaluation use sense proﬁles proposed paper reported results table based senses embeddings pooled 4 layers speciﬁed models following Loureiro Jorge 61 English Team 1 Ferryman 2 will_go 3 MULTISEM 4 LMMSRoBERTaL 5 InfoMiner Finnish Team 1 will_go 2 Ferryman 3 NS 4 RTM 11 LMMSXLMRL Sub1 0774 0768 0760 0754 0754 Sub1 0772 0745 0726 0671 0360 Team 1 MineriaUNAM 2 LMMSRoBERTaL 3 somaia 4 MULTISEM 5 InfoMiner Team 1 InfoMiner 2 NS 3 MineriaUNAM 4 MULTISEM 7 LMMSXLMRL Sub2 0723 0720 0719 0718 0715 Sub2 0645 0611 0597 0492 0354 Hungarian Team 1 NS 2 Hitachi 3 InfoMiner 4 Ferryman 5 LMMSXLMRL Slovenian Team 1 Hitachi 2 InfoMiner 3 NS 4 CitiusNLP 8 LMMSXLMRL Sub1 0740 0681 0754 0774 0754 Sub1 0654 0648 0646 0624 0560 Team 1 NS 2 Hitachi 3 MineriaUNAM 4 LMMSXLMRL 5 InfoMiner Team 1 NS 2 InfoMiner 3 CitiusNLP 4 tthhanh 9 LMMSXLMRL Sub2 0658 0616 0613 0565 0545 Sub2 0579 0573 0538 0516 0483 28 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 Appendix B Stanford contextual word similarities SCWS On Table 26 report results Stanford Contextual Word Similarities SCWS 43 task We address task similarly GWCS Section 74 Given words context independent sentence disambiguate occurrences score pair average similarities corresponding sense contextual embeddings Results SCWS follow performance GWCS XLNetL outperforming NLMs results related works Analyzing performance PartofSpeech POS ﬁnd nouns appear challenging task particularly compared nouns Table 26 Results SCWS Spearman correlation scores ρ 100 considering entire set pairs ALL results subsets pairing particular PartsofSpeech n denoting number instances subset similarly Colla et al 20 System Huang et al 43 SensEmbed 45 NASARI 18 DeConf 90 LessLex 20 ARES 105 BERTL SPWSD XLNetL SPWSD RoBERTaL SPWSD ALBERTXXL SPWSD LMMSSPBERTL LMMSSPXLNetL LMMSSPRoBERTaL LMMSSPALBERTXXL ALL n2003 NN n1328 NV n140 NA n30 VV n399 VA n9 AA n97 657 624 715 695 679 593 739 638 659 641 759 674 699 471 692 666 568 716 591 637 623 737 634 688 696 686 674 756 713 694 671 758 739 724 820 879 784 813 666 749 826 815 708 764 641 672 594 758 687 663 635 780 711 699 736 667 600 783 733 750 517 750 750 666 638 694 611 760 667 695 683 794 681 709 References 1 P Ammanabrolu E Tien W Cheung Z Luo W Ma LJ Martin MO Riedl Story realization expanding plot events sentences Proceedings AAAI Conference Artiﬁcial Intelligence vol 34 2020 pp 73757382 httpsojs aaai org index php AAAI article view 6232 2 CS Armendariz M Purver S Pollak N Ljubeši c M Ulˇcar I Vuli c MT Pilehvar SemEval2020 task 3 graded word similarity context Proceed ings Fourteenth Workshop Semantic Evaluation International Committee Computational Linguistics Barcelona 2020 pp 3649 https wwwaclweb org anthology 2020 semeval 13 3 CS Armendariz M Purver M Ulˇcar S Pollak N Ljubeši c M GranrothWilding CoSimLex resource evaluating graded word similarity context Proceedings 12th Language Resources Evaluation Conference Marseille France European Language Resources Association 2020 pp 58785886 httpswwwaclweb org anthology 2020 lrec 1720 4 B Athiwaratkun A Wilson A Anandkumar Probabilistic FastText multisense word embeddings Proceedings 56th Annual Meeting Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Melbourne Australia 2018 pp 111 httpswwwaclweb org anthology P18 1001 5 E Barba L Procopio R Navigli ConSeC word sense disambiguation continuous sense comprehension Proceedings 2021 Conference Empirical Methods Natural Language Processing Association Computational Linguistics Online Punta Cana Dominican Republic 2021 pp 14921503 httpsaclanthologyorg 2021emnlp main 112 6 EM Bender A Koller Climbing NLU meaning form understanding age data Proceedings 58th Annual Meeting Association Computational Linguistics 2020 pp 51855198 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 acl main 463 7 Y Bengio R Ducharme P Vincent C Janvin A neural probabilistic language model J Mach Learn Res 3 2003 11371155 8 Y Bengio R Ducharme P Vincent C Jauvin A neural probabilistic language model J Mach Learn Res 3 2003 11371155 9 M Bevilacqua R Navigli Breaking 80 glass ceiling raising state art word sense disambiguation incorporating knowl edge graph information Proceedings 58th Annual Meeting Association Computational Linguistics 2020 pp 28542864 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 acl main 255 10 DM Blei AY Ng MI Jordan Latent Dirichlet allocation J Mach Learn Res 3 2003 9931022 11 T Blevins L Zettlemoyer Moving long tail word sense disambiguation gloss informed biencoders Proceedings 58th Annual Meeting Association Computational Linguistics 2020 pp 10061017 Online Association Computational Linguistics https wwwaclweb org anthology 2020 acl main 95 12 P Bojanowski E Grave A Joulin T Mikolov Enriching word vectors subword information Trans Assoc Comput Linguist 5 2017 135146 httpsdoi org 10 1162 tacl _a _00051 httpswwwaclweb org anthology Q17 1010 13 TB Brown B Mann N Ryder M Subbiah J Kaplan P Dhariwal A Neelakantan P Shyam G Sastry A Askell S Agarwal A HerbertVoss G Krueger T Henighan R Child A Ramesh DM Ziegler J Wu C Winter C Hesse M Chen E Sigler M Litwin S Gray B Chess J Clark C Berner S McCandlish A Radford I Sutskever D Amodei Language models fewshot learners CoRR arXiv2005 14165 abs arXiv2005 14165 2020 14 ZG Cai RA Gilbert MH Davis MG Gaskell L Farrar S Adler JM Rodd Accent modulates access word meaning evidence speakermodel account spoken word recognition Cogn Psychol 98 2017 73101 httpsdoi org 10 1016 j cogpsych 201708 003 httpwwwsciencedirect com science article pii S0010028517300762 15 J CamachoCollados MT Pilehvar From word sense embeddings survey vector representations meaning J Artif Intell Res 63 2018 743788 httpsdoi org 10 1613 jair111259 29 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 2011 24932537 16 J CamachoCollados MT Pilehvar N Collier R Navigli SemEval2017 task 2 multilingual crosslingual semantic word similarity Proceed ings 11th International Workshop Semantic Evaluation SemEval2017 Association Computational Linguistics Vancouver Canada 2017 pp 1526 httpswwwaclweb org anthology S17 2002 17 J CamachoCollados MT Pilehvar R Navigli NASARI novel approach semanticallyaware representation items Proceedings 2015 Conference North American Chapter Association Computational Linguistics Human Language Technologies Association Computational Linguistics Denver Colorado 2015 pp 567577 httpswwwaclweb org anthology N15 1059 18 J CamachoCollados MT Pilehvar R Navigli Nasari integrating explicit knowledge corpus statistics multilingual representation concepts entities Artif Intell 240 2016 3664 httpsdoi org 10 1016 j artint 2016 07005 httpwwwsciencedirect com science article pii S0004370216300820 19 G Chronis K Erk When bishop like rook When like rabbi Multiprototype BERT embeddings estimating semantic relationships Proceedings 24th Conference Computational Natural Language Learning 2020 pp 227244 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 conll 117 20 D Colla E Mensa DP Radicioni LessLex linking multilingual embeddings SenSe representations LEXical items Comput Linguist 46 2020 289333 httpsdoi org 10 1162 coli _a _00375 httpswwwaclweb org anthology 2020 cl 2 3 21 D Colla E Mensa DP Radicioni Novel metrics computing semantic similarity sense embeddings KnowlBased Syst 206 2020 106346 httpsdoi org 10 1016 j knosys 2020 106346 httpswwwsciencedirect com science article pii S0950705120305025 22 D Colla E Mensa DP Radicioni Sense identiﬁcation data dataset lexical semantics Data Brief 32 2020 106267 httpsdoi org 10 1016 j dib 2020 106267 httpswwwsciencedirect com science article pii S2352340920311616 23 R Collobert J Weston Fast semantic extraction novel neural network architecture Proceedings 45th Annual Meeting Association Computational Linguistics Association Computational Linguistics Prague Czech Republic 2007 pp 560567 httpswwwaclweb org anthology P07 1071 24 R Collobert J Weston A uniﬁed architecture natural language processing deep neural networks multitask learning Proceedings 25th International Conference Machine Learning 2008 pp 160167 25 R Collobert J Weston L Bottou M Karlen K Kavukcuoglu P Kuksa Natural language processing scratch J Mach Learn Res 12 26 Z Dai Z Yang Y Yang J Carbonell Q Le R Salakhutdinov TransformerXL attentive language models ﬁxedlength context Proceed ings 57th Annual Meeting Association Computational Linguistics Association Computational Linguistics Florence Italy 2019 pp 29782988 httpswwwaclweb org anthology P19 1285 27 S Deerwester ST Dumais GW Furnas TK Landauer R Harshman Indexing latent semantic analysis J Am Soc Inf Sci 41 1990 391407 28 SC Deerwester ST Dumais GW Furnas RA Harshman TK Landauer KE Lochbaum LA Streeter Computer information retrieval latent semantic structure 1989 US Patent 4839853 29 J Devlin MW Chang K Lee K Toutanova BERT pretraining deep bidirectional transformers language understanding Proceedings 2019 Conference North American Chapter Association Computational Linguistics Human Language Technologies Long Short Papers vol 1 Association Computational Linguistics Minneapolis Minnesota 2019 pp 41714186 httpswwwaclweb org anthology N19 1423 30 J Dodge G Ilharco R Schwartz A Farhadi H Hajishirzi N Smith Finetuning pretrained language models weight initializations data orders early stopping arXiv2002 06305 2020 31 Z Dong Q Dong C Hao Hownet computation meaning 2006 32 K Erk What know alligator know company keeps Semant Pragmat 9 2016 163 httpsdoi org 10 3765 sp 9 17 33 K Ethayarajh How contextual contextualized word representations Comparing geometry BERT ELMo GPT2 embeddings Proceed ings 2019 Conference Empirical Methods Natural Language Processing 9th International Joint Conference Natural Language Processing EMNLPIJCNLP Association Computational Linguistics Hong Kong China 2019 pp 5565 httpswwwaclweb org anthology D19 1006 34 C Fellbaum Wordnet An Electronic Lexical Database MIT Press 1998 35 J Firth A synopsis linguistic theory 19301955 Studies Linguistic Analysis Philological Society Oxford 1957 Reprinted Palmer F ed 1968 Selected Papers JR Firth Longman Harlow 36 JR Firth The technique semantics Trans Philol Soc 34 1935 3673 37 L Flekova I Gurevych Supersense embeddings uniﬁed model supersense interpretation prediction utilization Proceedings 54th Annual Meeting Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Berlin Germany 2016 pp 20292041 httpswwwaclweb org anthology P16 1191 38 Y Goldberg Neural network methods natural language processing Synth Lect Hum Lang Technol 10 2017 1309 39 C Guo G Pleiss Y Sun KQ Weinberger On calibration modern neural networks D Precup YW Teh Eds Proceedings 34th Inter national Conference Machine Learning PMLR Proceedings Machine Learning Research vol 70 2017 pp 13211330 httpproceedings mlr press v70 guo17a html 40 WL Hamilton J Leskovec D Jurafsky Diachronic word embeddings reveal statistical laws semantic change Proceedings 54th Annual Meeting Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Berlin Germany 2016 pp 14891501 httpswwwaclweb org anthology P16 1141 41 ZS Harris Distributional structure Word 10 1954 146162 42 J Hewitt CD Manning A structural probe ﬁnding syntax word representations Proceedings 2019 Conference North American Chapter Association Computational Linguistics Human Language Technologies Long Short Papers vol 1 Association Computa tional Linguistics Minneapolis Minnesota 2019 pp 41294138 httpswwwaclweb org anthology N19 1419 43 E Huang R Socher C Manning A Ng Improving word representations global context multiple word prototypes Proceedings 50th Annual Meeting Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Jeju Island Korea 2012 pp 873882 httpswwwaclweb org anthology P12 1092 44 L Huang C Sun X Qiu X Huang GlossBERT BERT word sense disambiguation gloss knowledge Proceedings 2019 Conference Empirical Methods Natural Language Processing 9th International Joint Conference Natural Language Processing EMNLPIJCNLP Association Computational Linguistics Hong Kong China 2019 pp 35093514 httpswwwaclweb org anthology D19 1355 45 I Iacobacci MT Pilehvar R Navigli SensEmbed learning sense embeddings word relational similarity Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing Long Papers vol 1 Association Computational Linguistics Beijing China 2015 pp 95105 httpswwwaclweb org anthology P15 1010 46 N Ide CF Baker C Fellbaum RJ Passonneau The manually annotated subcorpus community resource people Proceedings 48th Annual Meeting Association Computational Linguistics Short Papers Uppsala Sweden 2010 pp 6873 47 P Kapanipathi V Thost S Sankalp Patel S Whitehead I Abdelaziz A Balakrishnan M Chang K Fadnis C Gunasekara B Makni N Mattei K Talamadupula A Fokoue Infusing knowledge textual entailment task graph convolutional networks Proceedings AAAI Conference Artiﬁcial Intelligence vol 34 2020 pp 80748081 httpsojs aaai org index php AAAI article view 6318 30 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 48 DE Klein GL Murphy The representation polysemous words J Mem Lang 45 2001 259282 httpsdoi org 10 1006 jmla 20012779 http wwwsciencedirect com science article pii S0749596X01927792 49 R Krishna Y Zhu O Groth J Johnson K Hata J Kravitz S Chen Y Kalantidis LJ Li DA Shamma M Bernstein L FeiFei Visual Genome Connecting Language Vision Using Crowdsourced Dense Image Annotations 2016 httpsarxivorg abs 1602 07332 50 T Kudo J Richardson SentencePiece simple language independent subword tokenizer detokenizer neural text processing Proceed ings 2018 Conference Empirical Methods Natural Language Processing System Demonstrations Association Computational Linguistics Brussels Belgium 2018 pp 6671 httpswwwaclweb org anthology D18 2012 51 I Kuznetsov I Gurevych A matter framing impact linguistic formalism probing results Proceedings 2020 Conference Empirical Methods Natural Language Processing EMNLP 2020 pp 171182 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 emnlp main 13 52 Z Lan M Chen S Goodman K Gimpel P Sharma R Soricut Albert lite bert selfsupervised learning language representations Interna tional Conference Learning Representations 2020 httpsopenreviewnet forum id H1eA7AEtvS 53 TK Landauer ST Dumais A solution platos problem latent semantic analysis theory acquisition induction representation knowl edge Psychol Rev 104 1997 211 54 Y Levine B Lenz O Dagan O Ram D Padnos O Sharir S ShalevShwartz A Shashua Y Shoham SenseBERT driving sense BERT Proceedings 58th Annual Meeting Association Computational Linguistics 2020 pp 46564667 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 acl main 423 55 M Lewis Y Liu N Goyal M Ghazvininejad A Mohamed O Levy V Stoyanov L Zettlemoyer BART denoising sequencetosequence pretraining natural language generation translation comprehension Proceedings 58th Annual Meeting Association Computational Linguistics 2020 pp 78717880 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 acl main 703 56 J Li D Jurafsky Do multisense embeddings improve natural language understanding Proceedings 2015 Conference Empirical Methods Natural Language Processing Association Computational Linguistics Lisbon Portugal 2015 pp 17221732 httpswwwaclweb org anthology D15 1200 57 BY Lin X Chen J Chen X Ren KagNet knowledgeaware graph networks commonsense reasoning Proceedings 2019 Conference Empirical Methods Natural Language Processing 9th International Joint Conference Natural Language Processing EMNLPIJCNLP Association Computational Linguistics Hong Kong China 2019 pp 28292839 httpswwwaclweb org anthology D19 1282 58 NF Liu M Gardner Y Belinkov ME Peters NA Smith Linguistic knowledge transferability contextual representations Proceedings 2019 Conference North American Chapter Association Computational Linguistics Human Language Technologies Long Short Papers vol 1 Association Computational Linguistics Minneapolis Minnesota 2019 pp 10731094 httpswwwaclweb org anthology N19 1112 59 Y Liu M Ott N Goyal J Du M Joshi D Chen O Levy M Lewis L Zettlemoyer V Stoyanov Roberta robustly optimized BERT pretraining approach CoRR arXiv190711692 abs 2019 60 D Loureiro J CamachoCollados Dont neglect obvious role unambiguous words word sense disambiguation Proceedings 2020 Conference Empirical Methods Natural Language Processing EMNLP 2020 pp 35143520 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 emnlp main 283 61 D Loureiro A Jorge Language modelling makes sense propagating representations WordNet fullcoverage word sense disambiguation Proceedings 57th Annual Meeting Association Computational Linguistics Association Computational Linguistics Florence Italy 2019 pp 56825691 httpswwwaclweb org anthology P19 1569 62 D Loureiro A Jorge LIAAD SemDeep5 challenge WordinContext WiC Proceedings 5th Workshop Semantic Deep Learning SemDeep5 Association Computational Linguistics Macau China 2019 pp 15 httpswwwaclweb org anthology W19 5801 63 D Loureiro K Rezaee MT Pilehvar J CamachoCollados Analysis evaluation language models word sense disambiguation Comput Linguist 2021 155 httpsdoi org 10 1162 coli _a _00405 64 K Lund C Burgess Producing highdimensional semantic spaces lexical cooccurrence Behav Res Methods Instrum Comput 28 1996 203208 65 M Mancini J CamachoCollados I Iacobacci R Navigli Embedding words senses joint knowledgeenhanced training Proceedings 21st Conference Computational Natural Language Learning CoNLL 2017 Association Computational Linguistics Vancouver Canada 2017 pp 100111 httpswwwaclweb org anthology K17 1012 66 T McCoy E Pavlick T Linzen Right wrong reasons diagnosing syntactic heuristics natural language inference Proceedings 57th Annual Meeting Association Computational Linguistics Association Computational Linguistics Florence Italy 2019 pp 34283448 httpswwwaclweb org anthology P19 1334 67 S Mcdonald M Ramscar Testing distributional hypothesis inﬂuence context judgements semantic similarity Proceedings 23rd Annual Conference Cognitive Science Society 2001 pp 611616 68 O Melamud J Goldberger I Dagan context2vec learning generic context embedding bidirectional LSTM Proceedings 20th SIGNLL Conference Computational Natural Language Learning Association Computational Linguistics Berlin Germany 2016 pp 5161 httpswww aclweb org anthology K16 1006 69 W Merrill Y Goldberg R Schwartz NA Smith Provable limitations acquiring meaning ungrounded form future language models understand arXiv2104 10809 2021 70 CM Meyer I Gurevych Wiktionary new rival expertbuilt lexicons Exploring possibilities collaborative lexicography 2012 71 T Mickus D Paperno M Constant K van Deemter What mean bert Assessing bert distributional semantics model Proceedings Society Computation Linguistics vol 3 2020 72 T Mikolov I Sutskever K Chen G Corrado J Dean Distributed representations words phrases compositionality Proceedings 26th International Conference Neural Information Processing Systems Volume 2 NIPS13 Curran Associates Inc Red Hook NY USA 2013 pp 31113119 73 T Mikolov I Sutskever K Chen GS Corrado J Dean Distributed representations words phrases compositionality Advances Neural Information Processing Systems 2013 pp 31113119 74 GA Miller M Chodorow S Landes C Leacock RG Thomas Using semantic concordance sense identiﬁcation Human Language Technology Proceedings Workshop Held Plainsboro New Jersey March 811 1994 1994 httpswwwaclweb org anthology H94 1046 75 R Navigli Word sense disambiguation survey ACM Comput Surv 41 2009 1011069 httpsdoi org 10 1145 1459352 1459355 httpdoi acm org 10 1145 1459352 1459355 76 R Navigli SP Ponzetto BabelNet Building Very Large Multilingual Semantic Network 2010 pp 216225 77 A Neelakantan J Shankar A Passos A McCallum Eﬃcient nonparametric estimation multiple embeddings word vector space Proceed ings 2014 Conference Empirical Methods Natural Language Processing EMNLP Association Computational Linguistics Doha Qatar 2014 pp 10591069 httpswwwaclweb org anthology D14 1113 78 CE Osgood GJ Suci PH Tannenbaum The Measurement Meaning vol 47 University Illinois Press 1957 31 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 79 M Ott S Edunov A Baevski A Fan S Gross N Ng D Grangier M Auli fairseq fast extensible toolkit sequence modeling Proceedings 2019 Conference North American Chapter Association Computational Linguistics Demonstrations Association Computational Linguistics Minneapolis Minnesota 2019 pp 4853 httpswwwaclweb org anthology N19 4009 80 T Pasini The knowledge acquisition bottleneck problem multilingual word sense disambiguation Proceedings TwentyEighth Interna tional Joint Conference Artiﬁcial Intelligence IJCAI20 Yokohama Japan 2020 81 M Pelevina N Areﬁev C Biemann A Panchenko Making sense word embeddings Proceedings 1st Workshop Representation Learning NLP Association Computational Linguistics Berlin Germany 2016 pp 174183 httpswwwaclweb org anthology W16 1620 82 J Pennington R Socher C Manning GloVe global vectors word representation Proceedings 2014 Conference Empirical Methods Natural Language Processing EMNLP Association Computational Linguistics Doha Qatar 2014 pp 15321543 httpswwwaclweb org anthology D14 1162 83 F Pereira B Lou B Pritchett S Ritter SJ Gershman N Kanwisher M Botvinick E Fedorenko Toward universal decoder linguistic meaning brain activation Nat Commun 9 2018 113 84 M Peters M Neumann M Iyyer M Gardner C Clark K Lee L Zettlemoyer Deep contextualized word representations Proceedings 2018 Conference North American Chapter Association Computational Linguistics Human Language Technologies Long Papers vol 1 Association Computational Linguistics New Orleans Louisiana 2018 pp 22272237 httpswwwaclweb org anthology N18 1202 85 M Peters M Neumann L Zettlemoyer Wt Yih Dissecting contextual word embeddings architecture representation Proceedings 2018 Conference Empirical Methods Natural Language Processing Association Computational Linguistics Brussels Belgium 2018 pp 14991509 httpswwwaclweb org anthology D18 1179 86 ME Peters M Neumann R Logan R Schwartz V Joshi S Singh NA Smith Knowledge enhanced contextual word representations Proceedings 2019 Conference Empirical Methods Natural Language Processing 9th International Joint Conference Natural Language Pro cessing EMNLPIJCNLP Association Computational Linguistics Hong Kong China 2019 pp 4354 httpswwwaclweb org anthology D19 1005 87 ST Piantadosi H Tily E Gibson The communicative function ambiguity language Cognition 122 2012 280291 httpsdoi org 10 1016 j cognition 201110 004 httpwwwsciencedirect com science article pii S0010027711002496 88 MT Pilehvar J CamachoCollados WiC wordincontext dataset evaluating contextsensitive meaning representations Proceedings 2019 Conference North American Chapter Association Computational Linguistics Human Language Technologies Long Short Papers vol 1 Association Computational Linguistics Minneapolis Minnesota 2019 pp 12671273 httpswwwaclweb org anthology N19 1128 89 MT Pilehvar J CamachoCollados R Navigli N Collier Towards seamless integration word senses downstream NLP applications Proceed ings 55th Annual Meeting Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Vancouver Canada 2017 pp 18571869 httpswwwaclweb org anthology P17 1170 90 MT Pilehvar N Collier Deconﬂated semantic representations Proceedings 2016 Conference Empirical Methods Natural Language Processing Association Computational Linguistics Austin Texas 2016 pp 16801690 httpswwwaclweb org anthology D16 1174 91 R Radach H Deubel C Vorstius M Hofmann Eds Abstracts 19th European Conference Eye Movements 2017 J Eye Mov Res 10 2017 httpsdoi org 10 16910 jemr10 6 1 httpsbop unibe ch JEMR article view JEMR 10 6 1 92 A Radford J Wu R Child D Luan D Amodei I Sutskever Language models unsupervised multitask learners 2019 93 C Raffel N Shazeer A Roberts K Lee S Narang M Matena Y Zhou W Li PJ Liu Exploring limits transfer learning uniﬁed texttotext transformer J Mach Learn Res 21 2020 167 httpjmlrorg papers v21 20 074 html 94 A Raganato J CamachoCollados R Navigli Word sense disambiguation uniﬁed evaluation framework empirical comparison Proceedings 15th Conference European Chapter Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Valencia Spain 2017 pp 99110 httpswwwaclweb org anthology E17 1010 95 E Reif A Yuan M Wattenberg FB Viegas A Coenen A Pearce B Kim Visualizing measuring geometry bert H Wallach H Larochelle A Beygelzimer F dAlche Buc E Fox R Garnett Eds Advances Neural Information Processing Systems vol 32 Curran Associates Inc 2019 pp 85948603 httpsproceedings neurips cc paper 2019 ﬁle 159c1ffe5b61b41b3c4d8f4c2150f6c4 Paperpdf 96 J Reisinger RJ Mooney Multiprototype vectorspace models word meaning Human Language Technologies The 2010 Annual Conference North American Chapter Association Computational Linguistics Association Computational Linguistics Los Angeles California 2010 pp 109117 httpswwwaclweb org anthology N10 1013 97 JM Rodd Settling semantic space ambiguityfocused account wordmeaning access Perspectives Psychol Sci 15 2020 411427 https doi org 10 1177 1745691619885860 PMID 31961780 98 A Rogers O Kovaleva A Rumshisky A primer BERTology know BERT works Trans Assoc Comput Linguist 8 2020 842866 httpsdoi org 10 1162 tacl _a _00349 httpswwwaclweb org anthology 2020 tacl 154 99 S Rothe H Schütze AutoExtend extending word embeddings embeddings synsets lexemes Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing Long Papers vol 1 Association Computational Linguistics Beijing China 2015 pp 17931803 httpswwwaclweb org anthology P15 1173 100 PJ Rousseeuw Silhouettes graphical aid interpretation validation cluster analysis J Comput Appl Math 20 1987 5365 https doi org 10 1016 0377 042787 90125 7 101 O Russakovsky J Deng H Su J Krause S Satheesh S Ma Z Huang A Karpathy A Khosla M Bernstein AC Berg L FeiFei ImageNet large scale visual recognition challenge Int J Comput Vis 115 2015 211252 httpsdoi org 10 1007 s11263 015 0816 y 102 G Salton The smart Retrieval Results Future Plans 1971 103 G Salton A Wong CS Yang A vector space model automatic indexing Commun ACM 18 1975 613620 104 B Scarlini T Pasini R Navigli SensEmBERT contextenhanced sense embeddings multilingual word sense disambiguation Proceedings ThirtyFourth Conference Artiﬁcial Intelligence Association Advancement Artiﬁcial Intelligence 2020 pp 87588765 105 B Scarlini T Pasini R Navigli With contexts comes better performance contextualized sense embeddings allround word sense disam biguation Proceedings 2020 Conference Empirical Methods Natural Language Processing EMNLP 2020 pp 35283539 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 emnlp main 285 106 KK Schuler VerbNet broadcoverage comprehensive verb lexicon PhD thesis University Pennsylvania 2006 httpverbs colorado edu kipper Papers dissertation pdf pp 787796 107 H Schutze Dimensions meaning Supercomputing92 Proceedings 1992 ACMIEEE Conference Supercomputing IEEE 1992 108 AG Soler M Apidianaki Lets play monopoly bert reveal words polysemy level partitionability senses Transactions Associ 109 R Speer J Chin C Havasi Conceptnet 55 open multilingual graph general knowledge Proceedings ThirtyFirst AAAI Conference ation Computational Linguistics TACL 2021 Artiﬁcial Intelligence AAAI17 AAAI Press 2017 pp 44444451 110 N Tandon G Melo G Weikum WebChild 20 ﬁnegrained commonsense knowledge distillation Proceedings ACL 2017 System Demonstra tions Association Computational Linguistics Vancouver Canada 2017 pp 115120 httpswwwaclweb org anthology P17 4020 111 I Tenney D Das E Pavlick BERT rediscovers classical NLP pipeline Proceedings 57th Annual Meeting Association Computa tional Linguistics Association Computational Linguistics Florence Italy 2019 pp 45934601 httpswwwaclweb org anthology P19 1452 32 D Loureiro A Mário Jorge J CamachoCollados Artiﬁcial Intelligence 305 2022 103661 112 I Tenney P Xia B Chen A Wang A Poliak RT McCoy N Kim BV Durme SR Bowman D Das E Pavlick What learn context Probing sentence structure contextualized word representations International Conference Learning Representations 2019 httpsopenreviewnet forum id SJzSgnRcKX 113 A Vaswani N Shazeer N Parmar J Uszkoreit L Jones AN Gomez Ł Kaiser I Polosukhin Attention need Advances Neural Information Processing Systems 2017 pp 59986008 114 L Vial B Lecouteux D Schwab UFSAC uniﬁcation sense annotated corpora tools Proceedings Eleventh International Conference Language Resources Evaluation LREC 2018 European Language Resources Association ELRA Miyazaki Japan 2018 httpswwwaclweb org anthology L18 1166 115 L Vial B Lecouteux D Schwab Sense vocabulary compression semantic knowledge WordNet neural word sense disambiguation Proceedings 10th Global Wordnet Conference Global Wordnet Association Wroclaw Poland 2019 pp 108117 httpswwwaclweb org anthology 2019 gwc 114 116 E Voita R Sennrich I Titov The bottomup evolution representations transformer study machine translation language modeling objectives Proceedings 2019 Conference Empirical Methods Natural Language Processing 9th International Joint Conference Natural Language Processing EMNLPIJCNLP Association Computational Linguistics Hong Kong China 2019 pp 43964406 httpswww aclweb org anthology D19 1448 117 E Voita D Talbot F Moiseev R Sennrich I Titov Analyzing multihead selfattention specialized heads heavy lifting rest pruned Proceedings 57th Annual Meeting Association Computational Linguistics Association Computational Linguistics Florence Italy 2019 pp 57975808 httpswwwaclweb org anthology P19 1580 118 T Vu DS Parker kEmbeddings learning conceptual embeddings words context Proceedings 2016 Conference North American Chapter Association Computational Linguistics Human Language Technologies Association Computational Linguistics San Diego California 2016 pp 12621267 httpswwwaclweb org anthology N16 1151 119 I Vuli c EM Ponti R Litschko G Glavaš A Korhonen Probing pretrained language models lexical semantics Proceedings 2020 Confer ence Empirical Methods Natural Language Processing EMNLP 2020 pp 72227240 Online Association Computational Linguistics https wwwaclweb org anthology 2020 emnlp main 586 120 A Wang Y Pruksachatkun N Nangia A Singh J Michael F Hill O Levy S Bowman Superglue stickier benchmark generalpurpose language understanding systems H Wallach H Larochelle A Beygelzimer F dAlche Buc E Fox R Garnett Eds Advances Neural Information Processing Systems vol 32 Curran Associates Inc 2019 httpsproceedings neurips cc paper 2019 ﬁle 4496bf24afe7fab6f046bf4923da8de6 Paperpdf 121 L Wittgenstein Philosophical investigations trans GEM Anscombe 261 1953 49 122 T Wolf L Debut V Sanh J Chaumond C Delangue A Moi P Cistac T Rault R Louf M Funtowicz J Davison S Shleifer P von Platen C Ma Y Jernite J Plu C Xu T Le Scao S Gugger M Drame Q Lhoest A Rush Transformers stateoftheart natural language processing Proceedings 2020 Conference Empirical Methods Natural Language Processing System Demonstrations 2020 pp 3845 Online Association Computational Linguistics httpswwwaclweb org anthology 2020 emnlp demos 6 123 Y Yaghoobzadeh H Schütze Intrinsic subspace evaluation word embedding representations Proceedings 54th Annual Meeting Association Computational Linguistics Long Papers vol 1 Association Computational Linguistics Berlin Germany 2016 pp 236246 httpswwwaclweb org anthology P16 1023 124 Z Yang Z Dai Y Yang J Carbonell RR Salakhutdinov QV Le Xlnet generalized autoregressive pretraining language understanding Advances Neural Information Processing Systems 2019 pp 57535763 125 D Yarowsky Unsupervised word sense disambiguation rivaling supervised methods 33rd Annual Meeting Association Computational Linguistics Association Computational Linguistics Cambridge Massachusetts USA 1995 pp 189196 httpswwwaclweb org anthology P95 1026 126 D Yuan J Richardson R Doherty C Evans E Altendorf Semisupervised word sense disambiguation neural models Proceedings COLING 2016 26th International Conference Computational Linguistics Technical Papers The COLING 2016 Organizing Committee Osaka Japan 2016 pp 13741385 httpswwwaclweb org anthology C16 1130 127 X Zhou M Sap S Swayamdipta Y Choi N Smith Challenges automated debiasing toxic language detection Proceedings 16th Conference European Chapter Association Computational Linguistics Main Volume 2021 pp 31433155 Online Association Computational Linguistics httpswwwaclweb org anthology 2021eacl main 274 128 Y Zhu R Kiros R Zemel R Salakhutdinov R Urtasun A Torralba S Fidler Aligning books movies storylike visual explanations watching movies reading books Proceedings IEEE International Conference Computer Vision 2015 pp 1927 33