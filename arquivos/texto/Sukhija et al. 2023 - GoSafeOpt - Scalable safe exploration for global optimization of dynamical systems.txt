Artiﬁcial Intelligence 320 2023 103922 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint GoSafeOpt Scalable safe exploration global optimization dynamical systems Bhavya Sukhija Sebastian Trimpe b Dominik Baumann cd Department Computer Science ETH Zürich Switzerland b Institute Data Science Mechanical Engineering RWTH Aachen University Germany c Department Electrical Engineering Automation Aalto University Espoo Finland d Department Information Technology Uppsala University Sweden Matteo Turchetta David Lindner Andreas Krause r t c l e n f o b s t r c t Article history Received 31 March 2022 Received revised form 12 April 2023 Accepted 14 April 2023 Available online 20 April 2023 Keywords Modelfree learning Bayesian optimization Safe learning Learning optimal control policies directly physical systems challenging Even single failure lead costly hardware damage Most existing modelfree learning methods guarantee safety failures exploration limited local optima This work proposes GoSafeOpt ﬁrst provably safe optimal algorithm safely discover globally optimal policies systems highdimensional state space We demonstrate superiority GoSafeOpt competing modelfree safe learning methods simulation hardware experiments robot arm 2023 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommons org licenses 4 0 1 Introduction The increasing complexity modern dynamical systems makes deriving mathematical models traditional modelbased control approaches forbiddingly involved timeconsuming Modelfree reinforcement learning RL meth ods 1 promising alternative learn control policies directly data To succeed need explore environment Without model risky unsafe Since modern hardware robots ex pensive repairs timeconsuming safe exploration crucial apply modelfree RL realworld problems This paper proposes GoSafeOpt modelfree learning algorithm search globally optimal policies guaranteeing safe exploration high probability 11 Related work Advances machine learning motivated usage modelfree RL algorithms obtaining control policies 2 6 However directly applying methods policy optimization presents major challenges Machine learning algorithms require large amounts data In learning control data gathered conducting experiments This paper Special Issue Riskaware Autonomous Systems Theory Practice Corresponding author krauseaethzch A Krause trimpedsmerwthaachende S Trimpe dominikbaumannaaltoﬁ D Baumann Email addresses bhavyasukhijainfethzch B Sukhija matteoturchettainfethzch M Turchetta davidlindnerinfethzch D Lindner httpsdoiorg101016jartint2023103922 00043702 2023 The Authors Published Elsevier BV This open access article CC BY license http creativecommons org licenses 4 0 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig 1 Illustrative example disjoint safe regions policy space The blue line depicts objective orange line constraint function There safe regions marked green SafeOpt explore global optimum initialized left region For interpretation colors ﬁgures reader referred web version article physical systems timeconsuming wears hardware ii Learning requires exploration lead unwarranted unsafe behaviors Challenges ii addressed jointly Bayesian optimization BO constraints BO 7 class blackbox global optimization algorithms variety works 811 optimize controllers sampleeﬃcient manner In constrained BO main classes methods On hand approaches like 1215 ﬁnd safe solutions allow unsafe evaluations training Herein focus approaches guar antee safety times exploration crucial dealing expensive hardware SafeOpt 16 safe learning methods emerged 1719 guarantee safe exploration high probability exploiting properties constraint functions regularity Unfortunately methods limited exploring safe set nected known initial safe policy Therefore miss global optimum presence disjoint safe regions policy space Fig 1 Disjoint safe regions appear learning impedance controller robot arm experiments applications 82021 To address limitation 21 proposes GoSafe provably safely discover safe global optimum presence disjoint safe regions mild conditions To achieve learns safe backup policies different states uses preserve safety evaluating policies outside safe set Speciﬁcally switches actively exploring local safe regions state policy space safe global exploration However active exploration state policy space requires coarse discretization space infeasible simplest systems lowdimensional state spaces 22 argues dimension d 3 challenging As result GoSafe handle realworld dynami cal systems restricted impractical systems lowdimensional state spaces The concept switching exploration stages pursued stagewise safe optimization algorithm proposed 23 However 23 restricted optimum connected safe initialization Lastly general idea learning backup policies related safety ﬁlters control barrier functions 2426 Nevertheless methods require availability learning dynamics model learning policy modelbased In work focus modelfree approach 12 Contributions This work presents GoSafeOpt ﬁrst modelfree algorithm globally search optimal policies safetycritical realworld dynamical systems systems highdimensional state spaces GoSafeOpt discretize actively explores state space Therefore overcomes main shortcomings restrictions GoSafe performing safe global exploration This makes GoSafeOpt ﬁrst modelfree safe global exploration algorithm realworld dynamical systems Crucially GoSafeOpt leverages Markov property systems state learn backup policies uses guarantee safety evaluating policies outside safe set This novel mechanism learning backup policies depend dimension state space We provide highprobability safety guarantees GoSafeOpt prove recovers safe globally optimal policy assumptions hold practical cases Finally validate simulated real safetycritical path following experiments robotic arm Fig 2 prohibitive GoSafe competing modelfree global safe search method Further GoSafeOpt achieves considerably better performance SafeOpt stateoftheart method local modelfree safe policy search highdimensional variants Table 1 compares GoSafeOpt SafeOpt GoSafe terms safety guarantees scalability global exploration sample eﬃciency It shows GoSafeOpt method perform sampleeﬃcient global exploration highdimensional systems providing safety guarantees 2 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig 2 Franka Emika Panda seven degrees freedom robot arm evaluations Table 1 Comparison GoSafeOpt prior work safe exploration based safety guarantees scalability global exploration sample eﬃciency Safe exploration SAFEOPT 18 GOSAFE 21 GOSAFEOPT State space dimension d 3 Global exploration Sample eﬃcient 2 Problem setting We consider Lipschitzcontinuous dxt zxt ut dt 1 z represents unknown dynamics xt X Rs state ut U Rp input apply steer state follow desired trajectory xdest X t 0 We assume starts known initial state x0 x0 The control input ut apply given state xt speciﬁed policy π X A U ut π xt π axt The policy parameterized A Rd A ﬁnite parameter space1 We encode goal following desired trajectory xdest objective function f A R Note trajectory deterministic 1 fully determined initial state x0 control policy Therefore objective independent state space X We seek controller parametrization A optimizes f constant initial condition x0 Since dynamics Eq 1 unknown objective f Nonetheless assume obtain noisy measurement f A running experiment We aim optimizing f measurements sampleeﬃcient way Additionally avoid deployment harmful policies formulate safety set unknown constraints trajectories satisﬁed times Similar f constraints depend parameter form gi A R constraint function gi 1 q Ig q N The resulting constrained optimization problem unknown objective constraints f subject gia 0 Ig max aA 2 We represent objective constraints scalarvalued function higher dimensional domain proposed 18 cid2 ha f gia 0 Ig 3 Ig 1 q I 0 1 q I This representation later help learning unknown function In summary goal ﬁnd optimal safe policy parameter starting nominal initial condition Note ﬁnding optimal policy ﬁxed initial x0 We refer solution Eq 2 safe global optimum condition x0 common task episodic RL 1 Solving problem dynamics model incurring failures generic systems objectives straints hopeless The following section introduces assumptions problem tractable 1 Inﬁnite parameter spaces handled discretization random subsampling 3 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 21 Assumptions To solve problem Eq 2 safely assume initial safe policy start data collection violating constraints This initial policy derived available simulators ﬁrst principles models performing controlled experiments hardware directly This policy conservative suboptimal For instance mobile robots policy barely moves robot initial safe policy Assumption 21 A set S0 A safe parameters known That parameters S0 gia 0 Ig In practice similar policies lead similar outcomes In words objective constraints exhibit regularity properties We capture assuming function h Eq 3 lives reproducing kernel Hilbert space RKHS 27 bounded norm space Assumption 22 The function h lies RKHS associated kernel k bounded norm RKHS cid8hcid8k B Furthermore objective f constraints gi Lipschitz continuous known constants Without Assumption 22 constraint reward functions discontinuous making impossible infer safety policy evaluating provide safety guarantees In practical applications behavior undesirable rare For discussion practicality assumption refer reader 28 Next formalize assumptions measurement model Assumption 23 We obtain noisy measurements h measurement noise independent identically distributed iid σ subGaussian That measurement yi h yi ha cid4i cid4i σ subGaussian I Assumptions 21 22 23 common safe BO literature 1618 However approaches treat evalu ation policy black box In contrast monitor rollout policy intervene bring safety necessary This achieved Markovian 29 like consider Eq 1 Proposition A3 appendix To monitor rollouts assume receive state measurement cid5t seconds discrete time steps arbitrarily jump movement typically small time intervals bounded Note robotic systems assumption valid Especially choose sampling time cid5t However estimating bound challenging A conservative value bound estimated performing controlled experiments safe initial policy Assumption 21 directly hardware Simulators ﬁrst principle models available leveraged Assumption 24 The state xt measured cid5t seconds Furthermore x t ρ 0 1 distance xt ρcid5t induced action bounded known constant cid7 cid8xt ρcid5t xtcid8 cid7 Remark Implicitly assume noisefree measurements state simplicity Our method works noisy case Appendix A11 typical real world Triggering backup policy Markovian suﬃcient guarantee safety trajectory generic constraint Consider case safety expressed constraint cost accumulated trajectory Even individually safe triggering backup policy unsafe overall Therefore limit types constraints consider Assumption 25 We assume 1 q gi deﬁned minimum statedependent function gi trajectory starting x0 controller π Formally gia min cid10 gix ξ0x0a x0 xcid10ξ0x0a cid3 0 zxτ π axτ dτ trajectory xt policy parameter starting x0 time 0 t 4 An example constraint minimum distance obstacle We provide formal deﬁnition safe experiment Deﬁnition 26 An experiment safe t 0 1 q gixt 0 5 4 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 This general way deﬁning safety optimization problem Eq 2 In particular Eq 2 considers trajectories associated ﬁxed policy parameter Deﬁnition 26 covers case different portions trajectory induced different controllers 3 Preliminaries This section reviews Gaussian processes GPs use construct frequentist conﬁdence intervals relevant prior work safe exploration SafeOpt 31 Gaussian processes We model unknown objective constraint functions Gaussian process regression GPR 30 In GPR cid5 cid4 prior belief captured GP fully determined prior mean function2 covariance function k Importantly observations corrupted iid Gaussian noise variance σ 2 yi f ai v v N 0 σ 2 posterior f GP mean variance computed closed form Let denote Yn Rn array containing n noisy observations f posterior f f N cid4 μn σ 2 cid10 cid5 n μn kn Kn Inσ 2 n k kn Kn Inσ 2 σ 2 1Yn 1kT n 6a 6b cid4 The entry j 1 n 1 n covariance matrix Kn Rnn k ai j data In n n identity matrix tures covariance x cid5 kn ka a1 ka cap Eq 6 considers case f function Eq 3 32 Frequentist conﬁdence intervals scalar function To model objective f constraints gi use selector To avoid failures determine safety given policy evaluating To end reason plausible worstcase values constraint gi new policy We use posterior distribution objective constraints given Eq 6 build frequentist conﬁdence intervals hold high probability 1 δ form μn1a ha β 12 n σn1a I 7 For functions fulﬁlling Assumption 22 23 3132 derive appropriate value βn This value depends δ n maximum information gain γn cf 333 33 SafeOpt modelfree safe exploration SafeOpt leverages conﬁdence intervals presented Section 32 solve blackbox constrained optimization problems guaranteeing safety iterates high probability It ensures safety limiting evaluations set provably safe inputs In particular SafeOpt deﬁnes lower bound conﬁdence interval ln lna max 12 n σn1a l0a 0 S0 Ig upper bound ln1a μn1a β 12 n σn1a u0a A I Given set safe parameters una minun1a μn1a β Sn1 infers safety nearby parameters combining conﬁdence intervals Lipschitz continuity constraints cid6 cid7 Sn A lna La cid10 cid8 cid8a cid10 cid8 cid8 0 8 iIg acid10Sn1 La joint Lipschitz constant f gia This leads local expansion safe set Thus case disconnected safe regions optimum discovered SafeOpt local Fig 1 2 Assumed zero loss generality wlog 3 The maximum information gain γn max observations y A information y A contains f 33 AD An I y A f A I y A f A mutual information f A evaluated points A 5 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Algorithm 1 Local Safe Exploration LSE Input Safe set S set backups B dataset D 1 Recommend parameter Eq 9 2 Collect R 3 B B R D D han εn 4 Update sets S G M Return S B D xk han εn kN cid9 Eq 8 Appendix D Deﬁnitions D1 D2 4 GOSAFEOPT In section present algorithm GoSafeOpt combines sample eﬃcient local exploration SafeOpt global exploration safely discover globally optimal policies dynamical systems To best knowledge GoSafeOpt ﬁrst modelfree algorithm globally search optimal policies guarantee safety exploration applicable complex hardware systems 41 The algorithm GoSafeOpt consists alternating stages local safe exploration LSE global exploration GE In LSE explore safe portion parameter space connected current estimate safe set Crucially exploit Markov property learn backup policies state visit LSE experiments During GE evaluate potentially unsafe policies hope identifying new disconnected safe regions The safety step guaranteed triggering backup policies learned LSE necessary If new disconnected safe region identiﬁed switch LSE step Otherwise GoSafeOpt terminates recommends optimum arg max lna 0 In following explain LSE GE stages provide pseudocode Algorithms 1 2 respectively Algorithm 4 presents pseudocode GoSafeOpt algorithm 411 Local safe exploration Similar SafeOpt LSE restrict evaluations provably safe policies policies safe set initialized safe seed Assumption 21 updated recursively according Eq 8 line 4 Algorithm 1 We focus evaluations relevant subsets safe set introduced 16 maximizers Mn plausibly optimal parameters expanders Gn parameters evaluated optimistically enlarge safe set For formal deﬁnitions 16 Appendix D During LSE evaluate uncertain parameter parameter widest conﬁdence interval expanders maximizers aSn 9 arg max aGnMn wna una lna max iI wna As byproduct experiments GoSafeOpt learns backup policies states visited rollouts leveraging Markov property Intuitively state xt visited deploying safe policy starting x0 know subtrajectory xτ τ t safe Assumption 25 Moreover subtrajectory safe regardless reach xt state Markovian Thus valid backup policy xt This means learn backup policies multiple states single LSE experiment To available GE introduce set backups Bn A X After running experiment policy collect discrete state measurements rollout R xk add set backups Bn1 Bn R Algorithm 1 line 3 cid9 We perform LSE connected safe set fully explored optimum safe set discovered Intuitively happens learned constraint objective functions high precision uncertainty expanders maximizers cid4 safe set expand kN max aGn1Mn1 max iI wn1a cid4 Sn1 Sn 10 Note GoSafeOpt like SafeOpt explores connected safe set parameter space learns backup policies Markov property 412 Global exploration GE aims discovering new disconnected safe regions In particular GE step evaluate uncertain parameter highest value maxiIg wna outside safe set A Sn As parameter safe set guaranteed safe Therefore monitor state experiment trigger backup policy learned LSE guarantee staying safe region state space continuing current choice policy parameters cf Fig 3 6 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 xt miniIg gi x 0 Trigger backup xt Fig 3 Illustration boundary condition The backup policy triggered xt guarantee high probability states ball xt safe Assumption 24 Section 412 Algorithm 2 Global Exploration GE Input Safe set S conﬁdence intervals C set backups B dataset D fail sets E XFail 1 Recommend global parameter Eq 11 2 xFail Boundary False 3 Experiment ﬁnished kTcid3 z xt π xt dt s Boundary Conditionxt B s xFail xk cid9 E E XFail XFail xFail xk han εn 4 xk x0 Not Boundary t0 Boundary Boundary 5 6 7 8 9 10 Collect R 11 Not Boundary 12 13 Return S C B D E XFail kN B B R D D han εn S S Ca Ca 0 Ig Rollout policy Not boundary Trigger backup policy update fail sets Successful global search Algorithm 3 Boundary Condition Input x Bn 1 xs Bn Ig lnas Lx cid8x xscid8 cid7 0 2 3 4 Boundary True Calculate Boundary False s Eq 12 s return Boundary s If backup policy triggered evaluating parameter mark experiment failed To avoid repeating experiment store state xFail intervened sets E A XFail X respectively line 9 Algorithm 2 Thus GE employ following acquisition function arg max aASnE max iIg wna 11 This picks uncertain parameter parameter widest conﬁdence interval provably safe shown trigger backup policy If experiment run triggering backup know safe Therefore add observed values gi f dataset rollout R collected experiment set backups Bn Bn1 Bn R Furthermore add parameter safe set update lower bound lna 0 Ig lines 12 13 Algorithm 2 Then switch LSE explore newly discovered 12 safe area Note lower bound updated LSE step ln1a maxlna μna β n1σna Ig Algorithm 4 line 6 If A Sn E safe areas discover GE converged 413 Boundary condition Throughout GE experiment monitor state evolution state measurement received evaluate online boundary condition determine backup policy triggered Ideally guarantee safety ii fast evaluate highdimensional dynamical systems iii incorporate discretetime measurements state To fulﬁll requirement boundary condition leverages Lipschitz continuity constraint 7 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 x XFail Algorithm 4 GoSafeOpt Input Domain A k S0 C0 D0 κ η 1 Initialize GP ha E XFail B 0 x0 S0 2 Sn expanding A Sn E cid18 3 4 5 6 7 8 9 10 Update Cna lna una A Ig LSE converged Eq 10 Not Boundary Conditionx Bn Sn1 Bn1 Dn1 LSESn Bn Dn E E XFail XFail x Sn1 Cn1 Bn1 Dn1 E XFail GESn Cn Bn Dn E XFail reevaluate fail sets Algorithm 3 Update fail sets Section 33 Perform LSE Algorithm 1 Perform GE Algorithm 2 return arg max aSn lna 0 In particular x t check point xs set backups Bn xs suﬃciently close x t guarantee steer safety state reach time step Boundary Condition During iteration n trigger backup policy x point set backups xs Bn lnas Lx cid8x xscid8 cid7 Ig In case use backup parameter s highest safety margin s max asAxsX asxsBn min iIg lnas Lx cid8x xscid8 12 Since calculate lnas Ig Sn oﬄine update safe set Eq 8 need evaluate cid8x xscid8 online computationally tractable realworld systems Os 2norm s dimension X Thereby satisﬁes requirement ii enables application algorithm complex systems high sampling frequencies The boundary condition summarized Algorithm 3 Updating Fail Sets Parameters boundary condition triggered parameters evaluated unsuccessfully GE added fail set E However LSE repeated discovering new region GE learn new backup policies makes boundary condition restrictive Hence happen parameter backup policy triggered previous GE step E trigger backup policy LSE step converged new safe region Thus learning new backup policies LSE reevaluate boundary condition line 3 update E XFail accordingly These states revisited GE steps In summary GoSafeOpt involves alternating stages LSE GE LSE steps similar SafeOpt nonetheless additionally leverage Markov property learn backup policies These backup policies GE global exploration The modelfree safe exploration method explores globally GoSafe However evaluates completely different expensive boundary condition relies safe set representation parameter state space This safe set actively explored Because active exploration expensive boundary condition GoSafe restricted systems lowdimensional state spaces Remark GoSafeOpt devised episodic RL setting initial state x0 ﬁxed known In appli cations initial state known apriori instead sampled iid state distribution ρ Our formulation extended setting treating initial state context variable cf 34 Moreover guarantee safety setting Assumption 21 modiﬁed parameters initial safe seed S 0 safe initial cid10 cid10 suppρ Then given contextinitial state x states support ρ x 0 acquisition function LSE GE 0 optimized context This similar contextual SafeOpt algorithm 18 The boundary condition extended incorporate context Finally continuous state space suppρ discretized similarly GoSafe 42 Theoretical results This section provides safety Section 421 optimality Section 422 guarantees GoSafeOpt 421 Safety guarantees The main safety result algorithm GoSafeOpt guarantees safety experiments Theorem 41 Under Assumptions 21 25 βn deﬁned 18 GoSafeOpt guarantees n 0 δ 0 1 experiments safe Deﬁnition 26 probability 1 δ The proof theorem provided Appendix A1 Intuitively analyze safety LSE GE separately For LSE leverage results 18 studies extensively Therefore novel analysis safety GE 8 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 We running experiments GE guarantee boundary condition triggers backup safe backup triggered experiment safe discovered new safe parameter 422 Optimality guarantees Next analyze GoSafeOpt ﬁnd safe global optimum solution Eq 2 During LSE explore connected safe region For safe region explore leverage results 18 prove local optimality Furthermore GE discover disconnected safe regions repeat LSE explore To end deﬁne parameter discovered GoSafeOpt LSE GE Deﬁnition 42 The parameter A discoverable GoSafeOpt iteration n exists set A Sn Rc cid4 A largest safe set safely reach A Eq A13 Appendix A2 1821 cid4 A Here Rc Next safe global optimum solution Eq 2 discoverable Deﬁnition 42 approximate cid4precision Theorem 43 Let ﬁnite integer n 0 ﬁnite integer n n probability 1 δ safe global optimum Further let Assumptions 21 25 hold βn deﬁned 18 Assume exists discoverable iteration n Deﬁnition 42 Then cid4 0 δ 0 1 exists f ˆan f ˆan arg maxaSn lna 0 cid4 n n 13 In practice GoSafeOpt tends ﬁnd better controllers SafeOpt converges LSE This formalized following proposition Proposition 44 For SafeOpt discoverable iteration n 0 discoverable iteration n 0 Proposition 44 states parameter lie largest safe set reachable S0 SafeOpt ﬁnd GoSafeOpt suffer restriction global exploration In Appendix A22 Lemma A18 provide additional conditions GoSafeOpt ﬁnd safe global optimum The performance beneﬁts GoSafeOpt empirically shown Section 5 Remark The safety threshold δ pick designers appetite unsafe evaluations For large value δ pa rameters available sampling iteration Accordingly method converges faster allowing unsafe evaluations 18 43 Practical modiﬁcations In practice improve sample computational eﬃciency introducing minor modiﬁcations While guarantee optimality yield good results evaluation Section 5 Furthermore proposed modiﬁcations affect safety guarantees method safely applied practice 431 Fixing iterations stage In Algorithm 4 perform global search GE convergence LSE Nonetheless beneﬁcial run LSE ﬁxed steps switch GE LSEs convergence This heuristic allows early discovery disconnected safe regions improve sample eﬃciency Moreover allows jumping different safe regions domain connected ran current LSE convergence currently disconnected To end apply following heuristic scheme run LSE nLSE steps ii run GE nGE steps discovered new safe parameters iii GE discovers new region return Else return GE completion reduced nLSE Note proposed scheme retains optimality restrict total number iterations However practice additionally impose upper bound interactions nLSE nGE inﬂuence budget global local exploration affects optimality cf Appendix C 432 Updated boundary condition If required boundary condition modiﬁed reduce computation time considering subset states collected experiments The updated boundary condition reduces online computation time expense conservative boundary condition Due conservatism lose optimality guarantees In practice achieve good results Section 5 9 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig 4 Setup evaluation Section 5 We consider safetycritical path following problem deviations desired path blue cause robot hit wall red box incur damage Deﬁnition 45 Consider ηl R ηu R ηl ηu The interior set cid16In marginal set cid16Mn deﬁned cid16In xs X xs Bn Ig ln ηu cid16Mn xs X xs Bn Ig ηl ln ηu The interior set contains points set backups Bn safe high tolerance ηu points marginal set safe smaller tolerance ηl We use sets updated boundary condition Updated Boundary Condition Consider dl R du R dl du We trigger backup policy x cid8 cid8 dl In case use cid10 point xs cid16In cid8x xscid8 du point x s backup parameter cid16Mn cid8 cid8x x cid10 s s s max asAasxsBn lnas xs min xcid10cid16Incid16Mn cid8 cid8x x cid10 cid8 cid8 14 Intuitively deﬁne distance tolerances du dl points Bn based safety tolerances ηu ηl As Theo rem 41 derive appropriate values ηu du respectively ηl dl guarantee safety 5 Evaluation We evaluate GoSafeOpt simulated real experiments Franka Emika Panda seven degree freedom DOF robot arm4 Fig 2 4 The objective experiments demonstrate GoSafeOpt applied systems high dimensional state spaces ii successful safely tuning control parameters common robotic tasks path following manipulators iii superior existing stateoftheart method SafeOpt safe control parameter tuning realworld robotic systems Accordingly results GoSafeOpt scale high dimensional systems jump disconnected safe regions guaranteeing safety directly applicable hardware tasks high sampling frequencies In work consider high dimensional parameter spaces challenging tackle methods SafeOpt Methods 35 22 alleviate challenge integrated algorithm easily Thus concentrate novelty method globally safe parameter exploration unlike SafeOpt scalability highdimensional state spaces compared GoSafe Speciﬁcally state space systems consider section large GoSafe applied problems Details objective constraint functions provided Appendix B The hyperparameters experiments listed Appendix E In experiments robot arm solely control position velocity endeffector To end consider operational space impedance controller 36 impedance gain K Appendix B The state space problem sixdimensional This prohibitively large GoSafe struggles state space greater 22 Therefore compare method SafeOpt Impedance controllers manipulators usually tuned manually This tedious timeconsuming process Accordingly results GoSafeOpt automate tuning safely 51 Simulation results We ﬁrst evaluate GoSafeOpt simulation environment based Mujoco physics engine 375 For consider distinct tasks reaching desired position ii path following We determine impedance gain 4 A video hardware experiments link code available httpssukhijab github io GoSafeOpt main _project html 5 The URDFs meshes taken httpsgithub com StanfordASL PandaRobot jl 10 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig 5 Comparison safe set simulation task SafeOpt GoSafeOpt 200 iterations The yellow regions represent safe sets In ﬁgure optimum represented blue triangle approximate model perform feedback linearization 36 For resulting linear design linearquadratic regulator LQR 38 quadratic costs parameterized matrices Q Rnn R Rpp Since model inaccurate feedback linearization cancel nonlinearities LQR optimal Thus goal tune cost matrices Q R compensate model mismatch This approach similar 9 We evaluate methods independent runs 200 iterations 511 Task 1 reaching desired position We select target xdes R3 robot For task parameterize matrices Q R parameters qc r 2 6 3 3 tradeoff accurate tracking large qc small inputs large r We choose objective function encourage reaching target fast possible penalizing large endeffector velocities control actions Appendix B1 details Thus total eightdimensional task sixdimensional state space twodimensional parameter space For analysis purposes run simple grid search run outside simulation estimate safe set global optimum Fig 5 depicts cid4precise cid4 01 safe set observed grid search From ﬁgure observe disconnected safe region Evaluation Fig 5 depicts safe sets SafeOpt GoSafeOpt 200 learning iterations We SafeOpt discover disconnected safe region stuck local optimum On hand GoSafeOpt discovers disconnected regions jump connected safe sets The learning curve methods depicted Fig 7 Our method performs considerably better SafeOpt The optimum method 0007 cid4 01 close optimum grid search SafeOpt signiﬁcantly improve initial policy This initial safe seed S0 contains nearoptimal policy connected region SafeOpt explores maxaS0 f maxa Rc cid4 S0 f Lastly method achieves comparable safety SafeOpt average 999 compared 100 We encounter failures LSE corresponds SafeOpt expect similar behavior SafeOpt initialized upper region Remark We increase βn encourage conservatism avoid unsafe evaluations However inﬂuences algorithms convergence rate Hence practice based task appetite unsafe evaluations βn selected 512 Task 2 path following task For experiment deﬁne parameterized path robot arm follow xdρt Here deﬁne ρt state indicate progress trajectory xd0 x0 xd1 xdes The evolution ρt 0 1 controlled param eter aρ 0 1 ρt mintaρ 1100 1500 1500 1 The objective ﬁnd optimal control parameters Q R cid8 cid8x xd aρ progress xd fast possible ensuring ζ In example model Q R parameters qc r κd κd 0 1 weigh velocity cost respect positional cost state Q matrix cf Appendix B1 Together aρ parameter task elevendimensional seven states including ρ parameters This problem incorporates challenging tradeoff fast trajectories hightracking performance We compare SafeOptSwarm 35 scalable version SafeOpt larger parameter spaces use adaptive discretization The results presented Fig 8 Our results GoSafeOpt performs siderably better SafeOpt speciﬁcally SafeOptSwarm Furthermore SafeOpt GoSafeOpt 100 safety 20 runs We compare method expected improvement constraints EIC 12 Fig 9 EIC discourages potentially unsafe regions allows unsafe evaluations Our results EIC GoSafeOpt attain similar formance However EIC considerably unsafe evaluations average greater ﬁfteen GoSafeOpt cid4 ρt cid5cid8 cid8 2 11 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig 6 Illustration triggering backup policy GE During global search policy directs robot wall b A backup policy automatically triggered boundary condition robot gets close wall The backup policy directs robot away wall green arrow c Fig 7 Mean normalized objective standard error SafeOpt GoSafeOpt eightdimensional simulation task 20 runs 52 Hardware results While simulation results showcased general applicability GoSafeOpt high dimensional systems ability discover disconnected safe regions demonstrate safely optimize policies realworld systems Control Task We consider path following task experimental setup Fig 4 model impedance gain K cid10 cid11 cid11 cid11 cid12 K diag K x K y K z 2 K x 2 K y 2 K z K x αx Krx Krx 0 reference value Frankas impedance controller αx 0 12 parameter like tune y z Accordingly αx yz 1 corresponds impedance controller provided manufacturer The parameter space consider task 0 123 We require controller follow known desired path avoiding wall depicted Fig 4 Optimization Problem We choose objective function encourage tracking desired path accurately possible impose constraint endeffectors distance wall Appendix B2 details We receive measurement state 250 Hz evaluate boundary condition GE 100 Hz Evaluation The parameter space task threedimensional Therefore compare method SafeOptSwarm 35 run 50 iterations algorithm independent runs We choose a0 06 06 06 initial policy During experiments GoSafeOpt SafeOptSwarm provide 100 safety runs For GoSafeOpt safety GE preserved triggering backup policy required One instance shown Fig 6 We Fig 10 GoSafeOpt performs considerably better SafeOptSwarm In particular prove existence disconnected safe regions task GoSafeOpt ﬁnds better policy GE Interestingly optimal value suggested GoSafeOpt αx α y 12 Therefore direction path GoSafeOpt suggests aggressive controls reduce tracking error Moreover controller suggested GoSafeOpt aggressive manufacturers reference controller αx 10 α y 10 tracks trajectory better 521 Choosing hyperparameters GoSafeOpt like safe exploration BO algorithms SafeOpt GoSafe makes assumptions prior knowl edge Section 21 These assumptions crucial theoretical guarantees In practice 12 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig 8 Mean normalized objective standard error SafeOpt GoSafeOpt elevendimensional simulation task 20 runs Fig 9 Comparison normalized rewards standard error number unsafe evaluations numbers bars SafeOptSwarm EIC GoSafeOpt elevendimensional simulation task 20 runs Fig 10 Mean normalized objective standard error SafeOptSwarm GoSafeOpt hardware task 3 runs The approximate location jump GE visible indicated cyan cross hard verify Yet safe exploration BO methods successfully safely applied large breath appli cations 233942 In case leverage available simulator obtain range hyperparameters kernel parameters βn distance metric boundary condition Lastly βn ﬁxed ﬁnetune remaining param eters performing controlled safe experiments hardware Even approach gives good results recent work 43 investigates hyperparameter selection problem safe BO systematically In general works investigate gap theory practice 2844 Nonetheless given potential algorithms reliable safe artiﬁcial intelligence AI acknowledge future research bridging gap needed 13 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 6 Conclusion This work proposes GoSafeOpt novel modelfree learning algorithm global safe optimization policies complex dynamical systems highdimensional state spaces We provide GoSafeOpt high probability safety guarantees provably performs better SafeOpt stateoftheart modelfree safe exploration algorithm We demon strate superiority algorithm SafeOpt empirically experiments GoSafeOpt handle complex realistic dynamical systems compared existing modelfree learning methods safe global exploration GoSafe This combination eﬃcient passive discovery backup policies leverages Markov property novel eﬃcient boundary condition detect trigger backup policy Future ex tensions design hybrid algorithms leverage Markov property actively explore state space Moreover GoSafeOpt designed eﬃcient safe controller tuning We believe applied dynamical systems legged robotics controller parameter tuning crucial component 45 Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Data availability No data research described article Acknowledgements We like thank Kyrylo Sovailo helping hardware experiments Franka Emika Panda arm Alonso Marco insightful discussions providing EIC code Furthermore like thank Christian Fiedler PierreFranc ois Massiani Steve Heim feedback work This project received funding Federal Ministry Education Research BMBF Ministry Culture Science German State North RhineWestphalia MKW grant number StUpPD_40721 Excellence Strategy Federal Government Länder European Research Council ERC European Unions Horizon 2020 research innovation program grant agreement No 815943 Swiss National Science Foundation NCCR Automation grant agreement 51NF40 180545 Microsoft Swiss Joint Research Center grant number 2019037 Appendix A Proofs theoretical results In section provide proof theoretical results stated main body paper In following denote k discrete time indices t continuous ones This difference important obtain state measurements discrete times need preserve safety times Moreover similarly notation GoSafe 21 cid10 t states trajectory induced policy starting denote ξtxta xt xt time t cid3 t zxτ π axτ dτ t t cid10 A1 Safety guarantees In following prove Theorem 41 gives safety guarantees GoSafeOpt Since GoSafeOpt stages LSE GE study safety separately For LSE 18 provides safety guarantees Therefore focus safety guarantees GE combining guarantee safety overall algorithm To end ﬁrst hypothesis safe set Sn conﬁdence bounds lna una Hypothesis A1 Let Sn cid18 The following properties hold Ig n 0 probability 1 δ Sn gia x0 0 A lna gia x0 una A1 A2 We leverage hypothesis prove safe GE satisﬁed GoSafeOpt Particularly LSE 18 proves hypothesis fulﬁlled Hence GE safe set conﬁdence intervals satisfy In following updates safe sets conﬁdence intervals implemented GE satisfy hypothesis suﬃcient conclude hypothesis satisﬁed n 0 concrete Lemma A9 14 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 During GE receive measurements state discrete times evaluate boundary condition trigger backup policy necessary Therefore ﬁrst discretetime measurements guarantee safety continuous time Lemma A2 Let Assumptions 24 25 hold let k k 0 arbitrary integers If integers k k k exists A gias xk Lxcid7 Ig gixt 0 t kcid5t k 1cid5t Ig Proof By choice sampling scheme state xk measured discrete time corresponds state xkcid5t continuous time Hence gias xk gias xkcid5t Consider k 0 A gias xkcid5t Lxcid7 For t kcid5t k 1cid5t gias xkcid5t gias xt Lx cid8xkcid5t xtcid8 Lipschitz continuity Assumption 22 Lxcid7 Now gias xkcid5t Lxcid7 t kcid5t k 1cid5t Ig gias xt gias xkcid5t Lxcid7 0 Assumption 24 A3 For choice constraints Assumption 25 implies gixt 0 Ig t kcid5t k 1cid5t Finally holds integers k k k k holds t kcid5t k 1cid5t cid2 Now established condition guarantees given time interval gi xt 0 Ig We collect parameter state combinations rollouts set backups Bn The intuition Markovian states visited safe experiment safe This important allows GoSafeOpt learn backup policies multiple states actively exploring state space We formalize following proposition Proposition A3 Let Assumption 25 hold If x0 safe minxcid10ξ0x0a safe minxcid10ξt1xt1a cid10 0 Ig gix gix cid10 0 Ig t1 0 xt1 Proof The Eq 1 Markovian xt1 ξ0x0a xt2 ξ0x0a t2 t1 0 xt2 x0 t1cid13 0 t2cid13 zxt π axtdt zxt π axtdt t1 t2cid13 xt1 zxt π axtdt t1 Therefore trajectory starting xt1 result state evolution independent arrived xt1 Combining Assumption 25 gia xt1 min xcid10ξt1xt1a cid10 gix cid10 gix min xcid10ξ0x0a gi x0 0 cid2 Assumption 25 Markov Property Assumption 25 In following gias x0 lower bound points xs Bn gias xs gias x0 This play crucial role showing preserve safety trigger backup policy Corollary A4 Let Assumption 25 hold For points xs Bn gias xs gias x0 Ig Proof Each point xs Bn collected safe experiments Algorithm 1 line 3 Algorithm 2 line 12 Therefore xs ξ0x0as The result follows Proposition A3 cid2 15 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Corollary A4 shows lnas conservative lower bound gias xs Crucially observe rollouts constraint values gi xs model GP obtain potentially conservative lower bound However work assume measure gias x0 Assumption 23 Proposition A3 Corollary A4 formalize collect backup policies leverage boundary condition In following prove experiments trigger backup policy safe First boundary condition triggered time step k cid5t time trigger safe k Lemma A5 Let assumptions Theorem 41 Hypothesis A1 hold If GE boundary condition Algorithm 3 triggers backup policy time step k cid5t Ig gixt 0 probability 1 δ 0 t k Proof Consider k k Since boundary condition Algorithm 3 trigger backup policy k xs Bn lnas Lx cid8xk xscid8 cid7 Ig By Lipschitz continuity g gias xs gias xk Lx cid8xk xscid8 implies gias xk gias xs Lx cid8xk xscid8 gias x0 Lx cid8xk xscid8 lnas Lx cid8xk xscid8 Lxcid7 A4 A5 Corollary A4 Hypothesis A1 A4 1 cid2 Ig k k Therefore use Lemma A2 prove claim choosing k 0 k k Lemma A5 shows time trigger boundary condition safe tolerance Lxcid7 fulﬁll constraints guarantee safety In following trigger safe backup policy k times triggering Lemma A6 Let assumptions Theorem 41 Hypothesis A1 hold If GE experiment parameter aGE time step k 0 boundary condition triggers backup policy s deﬁned Eq 12 cid8 cid8 cid8 cid8x xk lna Lx s arg max aAxX axBn min iIg A6 Then t k cid5t gixt 0 Ig probability 1 δ 0 follows deﬁnition Proof We want Eq A6 ﬁnds parameter Bn consists safe rollouts Algorithm 1 line 3 Algorithm 2 line 12 parameters Bn Ig gias x0 0 s gia 0 For k s xk Let consider integer k 0 Let xs Bn arbitrary Following Lipschitz continuitybased arguments Lemma A5 Ig gias xk lnas Lx lnas Lx lnas Lx 0 cid8 cid8xs xk cid4 cid8 cid8xk cid4cid8 cid8xk cid8 cid8 1 xs 1 xs xk cid8 cid8 1 cid5 cid8 cid8xk cid5 cid8 cid8 cid8 cid8 cid7 Lemma A5 Triangle inequality Assumption 24 A7 1 inequality follows fact boundary condition triggered time step k Section 412 Furthermore Eq A7 conclude exists A xs X xs Bn lnas Lx cid8xs xk cid8 0 Ig Therefore s recommended Eq A6 max aAxX axBn min iIg lna Lx cid8 cid8x xk cid8 cid8 0 Hence gia s xk 0 Ig probability 1 δ proves claim cid2 16 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Lemmas A5 A6 trigger backup policy GE guarantee safety experiment switching backup policy respectively Next prove backup policy triggered GE parameter aGE aGE safe high probability Lemma A7 Let assumptions Theorem 41 Hypothesis A1 hold If GE parameter aGE backup policy triggered boundary condition aGE safe probability 1 δ giaGE x0 0 Ig Proof Assume experiment safe exists t 0 Ig gixt 0 Consider time step k 0 t kcid5t k 1cid5t Since boundary condition triggered experiment triggered time step k This implies Section 412 exists point xs Bn lnas Lx cid8xs xkcid8 cid7 0 A8 Ig Therefore gias xk Lxcid7 Hypothesis A1 Hence Lemma A2 gixt 0 Ig This contradicts assumption t 0 Ig gixt 0 cid2 The following Corollary summarizes safety GE Corollary A8 Under assumptions Theorem 41 Hypothesis A1 GoSafeOpt safe GE t 0 gixt 0 Ig 0 ii experiment Proof Two scenarios occur GE backup policy triggered time step k completed triggering backup policy For ﬁrst case Lemma A6 guarantees safe triggering backup policy Lemma A5 guarantees safe trigger backup For second scenario Lemma A7 guarantees safety cid2 We shown assumptions Theorem 41 combined Hypothesis A1 guarantee safe GE irrespective trigger backup policy We leverage result Hypothesis A1 satisﬁed GoSafeOpt Lemma A9 Let assumptions Theorem 41 hold βn deﬁned 18 Then Hypothesis A1 satisﬁed GoSafeOpt probability 1 δ Ig n 0 Sn gia x0 0 A lna gia x0 una Proof We use induction n A9 A10 Base case n 0 By Assumption 21 S0 gia x0 0 Ig Moreover initialization conﬁdence intervals presented Section 33 follows l0a 0 S0 u0a A Thus follows l0a gia x0 u0a A Inductive step Our induction hypothesis ln1a gia x0 un1a gia x0 0 Sn1 Ig Based prove relations hold iteration n We start showing lna gia x0 una A To end distinguish different updates stages GoSafeOpt LSE GE During LSE deﬁne lna una lna maxln1a μna βnσna una minun1a μna βnσna We know gia x0 ln1 induction hypothesis gia x0 μna βnσna probability 1 δ 18 This implies gia x0 ln A similar argument holds upper bound During GE update lna parameter evaluate induces trajectory trigger backup pol icy Algorithm 2 line 13 For parameter induction hypothesis allows use Lemma A7 conclude gia x0 0 Therefore update conﬁdence intervals GE satisﬁes Eq A10 iteration n completing induction step conﬁdence intervals As conﬁdence intervals distinguish different updates safe set implemented LSE GE In case GE update safe set adding evaluated policy parameter trigger backup Sn Sn1 Following argument conclude gi x0 0 Ig This induction hypothesis means gia x0 0 Ig Sn case GE update 17 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Now focus LSE We showed Eq A10 holds n Moreover know induction hypothesis gia x0 0 Sn1 Ig high probability The update equation safe set Eq 8 gives cid10 Sn Sn1 exists Sn1 Ig cid8 cid8 0 cid8 cid8a lna La cid10 A11 cid10 x0 0 Due Lipschitz continuity We guarantee high probability gia constraint functions cid10 gia x0 gia x0 La cid8 cid8a lna La cid10 x0 0 Ig Sn probability 1 δ case LSE step cid2 Therefore gia cid8 cid8 cid8a cid8 cid10 cid8 cid8 0 cid10 Eq A11 Lemma A9 ensures Hypothesis A1 holds GoSafeOpt We know assumption The orem 41 Hypothesis A1 safe GE Corollary A8 Hence guarantee safety GE Finally prove Theorem 41 guarantees safety GoSafeOpt Theorem 41 Under Assumptions 21 25 βn deﬁned 18 GoSafeOpt guarantees n 0 δ 0 1 experiments safe Deﬁnition 26 probability 1 δ Proof We perform GoSafeOpt stages LSE GE In Lemma A9 proved parameters Sn gia x0 0 Ig probability 1 δ During LSE query parameters Sn Eq 9 Therefore experiments safe During GE Corollary A8 proves assumptions Theorem 41 Hypothesis A1 hold safe GE choice βn Furthermore Lemma A9 proved Hypothesis A1 satisﬁed GoSafeOpt Hence conclude assumptions Theorem 41 hold safe GE times cid2 A11 Proof boundary condition noisy measurements Lemma A10 Assume time step k receive noisy measurement state evaluate boundary condition y x ε ε iid Speciﬁcally assume P cid8εcid8 d2 1 δ2 If ys Bn ys xs εs lnas Lxcid8 y yscid8 cid7 d 0 probability 1 δ2 lnas Lxcid8x xscid8 cid7 0 Proof We like lnas Lxcid8 y yscid8 cid7 d lnas Lxcid8x xscid8 cid7 This implies d cid8x xscid8 cid8 y yscid8 Accordingly cid8x xscid8 cid8 y yscid8 cid8x xs y yscid8 reverse triangle inequality cid8εs εcid8 cid8εcid8 cid8εscid8 d cid2 probability 1 δ2 Following lemma come conservative boundary condition step jump bound cid7cid10 cid7 d guarantees safety However price pay measuring state perfectly additional probability term 1 δ2 Lastly look inﬂuence noisy state measurements boundary condition Nevertheless policy π uses form feedback noise enters dynamics In work assume inﬂuence captured observation model Assumption 23 A2 Optimality guarantees In section prove Theorem 43 guarantees safe global optimum cid4precision discoverable iteration n 0 Deﬁnition 42 Then Lemma A18 practical applications discoverability condition satisﬁed 18 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 A21 Proof Theorem 43 We ﬁrst deﬁne largest region LSE safely explore given safe initialization S cid4 S ﬁnd optimum cid4precision region To end deﬁne reachability operator Rc fully connected safe region Rc cid4S S A Rc cid4 S adapted 1821 cid10 S gia x0 cid4 La cid10 cid8 cid8a cid10 cid8 cid8 0 Rc cid4S lim n cid4 Rc cid4 cid5 n S Ig A12 A13 cid4 S contains parameters safely explore know constraint function cid4 S cid4 S closure Next derive property reachability operator leverage provide optimality The reachability operator Rc cid4precision safe set parameters S Further Rc Rc guarantees cid4 nS denotes repeated composition Rc Lemma A11 Let A S Rc cid4 A S cid18 Rc cid4S S cid18 Proof This lemma straightforward generalization 18 Lem 74 Assume Rc implies Rc limit Rc leads Rc cid4 A S By deﬁnition Rc cid4 S S Furthermore A S Rc cid4 S S Rc cid4 A Rc cid4 S S Iteratively applying Rc cid4 S S want cid4 sides cid4 A Rc cid4 S S cid4 S 18 Lem 71 Thus obtain Rc cid4 A S cid2 In following prove LSE convergence criterion Eq 10 guarantees safe initialization S explore Rc cid4 S LSE ﬁnite time Theorem A12 Consider cid4 0 δ 0 Let Assumptions 22 23 hold βn deﬁned 18 S A initial safe seed parameters ga x0 0 S Assume information gain γn grows sublinearly n kernel k Further let n smallest integer cdf convergence criterion LSE Eq 10 max 1 M n 1 max iI aG n wn1a cid4 Sn1 Sn Then n ﬁnite running LSE following holds probability 1 δ n n Rc cid4S Sn f ˆan max cid4 S Rc f cid4 ˆan arg max aSn lna 0 Proof We ﬁrst leverage result 18 Thm 41 provides following worstcase bound n n βnγIn C1 cid4 cid5 Rc 0S cid42 1 A14 A15 A16 A17 smallest integer satisﬁes Eq A17 Hence n C1 8 log1 σ 2 n ﬁnite The sublinear growth γn n satisﬁed practical kernels like ones consider work 31 Next prove Eq A15 For sake contradiction assume Rc cid4 Sn Sn cid18 Lemma A11 Therefore exists A Sn cid8 cid8a cid8 cid8 cid10 cid4 S Sn cid18 This implies Rc cid10 Sn Sn1 Eq A14 Ig x0 cid4 La cid8 cid8a La un1a 0 gia cid8 cid8 cid10 cid10 cid10 Therefore cid10 Gn1 18 Appendix D Deﬁnition D1 accordingly wn1a wn1a 0 gia cid10 cid4 Ig cid8 cid8 ln1a This means Sn Eq 8 contradiction Thus conclude Rc cid4 S Sn n n Proposition A13 Rc x0 cid4 La cid8 cid8a cid8 cid8a La cid8 cid8 cid10 cid10 cid10 cid10 cid4 S Sn Sn Sn A18 Lemma A9 cid10 cid4 Next 19 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Now prove Eq A16 Consider n n Note wn1a Algorithm 2 line 13 For simplicity denote solution arg maxa Rc cid10 cid4 Algorithm 1 line 4 cid10 cid4 implies wna S We cid4 S f una S 0 f S f ˆan lnˆan 0 max aSn lna 0 deﬁnition Lemma A9 S Lemma A9 deﬁnition ˆan Mn Appendix D Deﬁnition D2 uncertainty cid4 S cid4 For sake contradiction assume S maximizer Therefore S cid4 Now f ˆan f wna f ˆan f S cid4 S Then obtain lna S 0 lnˆan 0 f ˆan S cid4 f S 0 cid4 una S 0 wna una lna S 0 0 contradiction Therefore f ˆan f S cid4 cid2 A19 deﬁnition ˆan Lemma A9 Eq A19 wna deﬁnition wna Lemma A9 S 0 cid4 S 0 fully connected safe region Rc Theorem A12 states given safe seed S convergence LSE Eq 10 implies discovered cid4 S recovered optimum region cid4precision Based previous results safe global optimum discoverable iteration n 0 Deﬁnition 42 ﬁnd approximately optimal safe solution However prove optimality require Sn Sn1 Proposition A13 Let assumptions Theorem 41 hold For n 0 following property satisﬁed Sn Sn Sn1 A20 Proof The safe set provably increases LSE 18 Lem 71 During GE safe set updated new safe parameter The proposed update nondecreasing property Algorithm 2 line 13 Hence conclude Sn Sn1 cid2 Proposition A13 shows safe global optimum A added safe set Sn explore largest reachable safe set Rc cid4 A Sn Sn1 Next prove new safe region Lemma A14 Consider integer n 0 Let Sn safe set parameters explored n iterations GoSafeOpt let βn deﬁned 18 Consider A Sn1 Sn If A cid18 exists ﬁnite integer n n Rcid4 c Sn S n probability 1 δ c A Rcid4 c A S n Rcid4 c Sn S n Rcid4 c A S n cid18 We know A Sn1 S n Proposition A13 This implies Rc Proof First Rcid4 c A S n Assume Rcid4 cid4 S n S n cid18 Lemma A11 Since A cid18 safe set expanding For GoSafeOpt happen LSE GE new parameter successfully evaluated boundary condition triggered In case perform LSE till convergence Let n n smallest integer converge LSE c Sn S n We Rcid4 c A Rcid4 max aGn1Mn1 max iI w n1a cid4 S n1 S n A21 holds From Theorem A12 know n ﬁnite Consider Rc cid8 cid8a 0 gia u n1a La cid8 cid8 implies cid8 cid8 Eq A12 Furthermore S n1 S n means cid10 Gn1 Appendix D Deﬁnition D1 w n1a cid10 S n cid10 S n1 Hence 0 cid10 cid4 This implies cid4 S n S n Then exists cid10 x0 cid4 La cid8 cid8a cid10 cid10 20 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 cid10 La 0 l n1a proceed similarly Rcid4 c A Rcid4 Rcid4 c Sn S n cid2 cid10 cid8 cid8a cid8 cid8 Therefore according Eq 8 S n contradiction Hence Rcid4 c Sn S n Since Rcid4 c A S n Rcid4 c A S n We c Sn S n conclude In Lemma A14 shown set A add safe set explore fully connected safe region ﬁnite time This crucial allows guarantee discover new region GE explore till convergence Finally prove Theorem 43 Theorem 43 Let ﬁnite integer n 0 ﬁnite integer n n probability 1 δ safe global optimum Further let Assumptions 21 25 hold βn deﬁned 18 Assume exists discoverable iteration n Deﬁnition 42 Then cid4 0 δ 0 1 exists f ˆan f ˆan arg maxaSn lna 0 cid4 n n 13 Proof Since Rc Rc cid4 A region cid4 precision ﬁnite time n cid4 S n Lemma A14 Furthermore cid4 S n Theorem A12 shows ﬁnd optimum safe S n n Hence exists ﬁnite integer n discoverable iteration n exists set A Rc cid4 A Rc f ˆan f cid4 n n ˆan arg maxaSn lna 0 cid2 A22 A22 Requirements discovering safe sets GE In previous section showed safe global optimum discoverable iteration n ﬁnd cid4precision In section parameter aGE A Sn backup policies states trajectory aGE eventually added safe set parameters Finally conclude section showing practical cases fulﬁlls discoverability condition Now derive conditions allow explore new regionsparameters GE To end start deﬁning n states boundary condition trigger backup policy set safe states X s Deﬁnition A15 The set safe states X s n deﬁned X s n cid14 cid7 acid10xcid10Bn x X cid15 cid15 cid15 cid8 cid8x cid8 cid8 1 cid10 x Lx min iIg cid16 cid10 lna cid7 A23 Intuitively trajectory induced parameter evaluated GE lies X s triggered parameter Now prove set safe states X s property tells GoSafeOpt continues learn backup policies states n boundary condition n nondecreasing This important Lemma A16 Let assumptions Theorem 41 hold For n 0 following property satisﬁed X s n X s n X s n1 A24 Proof The lower bounds lna nondecreasing I deﬁnition Algorithm 1 line 4 Algorithm 2 line 13 Additionally continue add new rollouts set backups Bn Bn1 Algorithm 1 n exists xs Bn lnas Lx cid8x xscid8 cid7 0 line 3 Algorithm 2 line 12 For x X s Ig Because Bn Bn1 ln1as lnas x X s n1 cid2 Next state conditions parameter aGE A Sn discovered GE backup policy triggered GE ﬁnite time Lemma A17 Consider n 0 Let Sn safe set parameters explored n iterations GoSafeOpt aGE parameter A Sn Further let assumptions Theorem 43 hold βn deﬁned 18 If k 0 xaGE k X s n xaGE k represents state time step k starting x0 policy π aGE exists ﬁnite integer n n aGE S n 21 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 cid10 Proof Assume exists ﬁnite integer n n aGE S n This imply aGE A S n n n Thus aGE safe set evaluated LSE However Theorem A12 know LSE converge ﬁnite number iterations perform GE Since aGE safe set evaluated GE parameters outside safe regions queried The parameter space A ﬁnite parameter evaluated unsuccessfully boundary condition triggered added E evaluated Eq 11 Algorithm 2 line 9 This implies aGE evaluated n ncid10 Lemma A16 n n k 0 xaGE k X s ncid10 If aGE evaluated experiment unsuccessful trigger backup policy imply k cid8 cid8xaGE k lncid10 Lx cid10 X s Thus xaGE k contradiction Consequently aGE S n ﬁnite number iterations n cid2 xs ncid10 contradicts assumption Therefore aGE Sncid101 S n Proposition A13 cid10 n Sncid10 S n Proposition A13 Furthermore X s Ig xs Bncid10 cid8 cid8 cid7 X s n cid10 cid10 Lemma A17 guarantees trajectory parameter aGE lies X s set LSE GE We utilize result provide condition safe global optimum discoverable iteration n GoSafeOpt n eventually added safe Lemma A18 Consider n 0 Let Assumptions 21 24 hold βn deﬁned 18 let If exists aGE A Sn A safe global optimum k 0 n 0 xaGE k X s n xaGE k state visited starting x0 policy π aGE time step k Rc cid4 aGE ii discoverable iteration n n GoSafeOpt probability 1 δ Proof Since k 0 xaGE k X s cid4 aGE conclude Rc n exists ﬁnite integer n n aGE S n Lemma A17 Furthermore discoverable iteration n Deﬁnition 42 cid2 The condition interesting empirically practical cases fulﬁlled Crucially optimal near optimal parameters tend visit similar states safe policies We add rollouts safe policies set backups Bn backup policies trajectories trajectories lie close Therefore trajectories nearoptimal parameters lie X s n case safe global optimum fulﬁlls discoverability condition Theorem 43 Appendix B Additional information experiments For experiments Section 5 consider controller operational space The operational space dynamics endeffector given 36 uxt cid19qs cid20q qs ηq B1 s represents endeffector position q joint angles cid19q cid20q q ηq nonlinearities representing mass Coriolis gravity terms respectively The state consider xt sT t sT tT We apply impedance controller u xt K x xdesk cid20q qs ηq B2 K feedback gain The torque τ applied joints calculated τ J T uxt J Jacobian For experiments directly measure ga xk k denotes discrete time step Therefore instead lnas boundary condition Section 432 lower bound tuples set backups Bn lnas xs potentially reduce conservatism boundary condition Therefore deﬁne GP parameter state space contains points Bn The set Bn consists rollouts individual experiments typically add 50 100 data points experiment Bn As data points GP increase inference prohibitively costly To end use subset selection scheme select small subset points x Bn random probability proportional exp miniIg l2 na x Crucially want retain points small lower bound low uncertainty points We perform subset selection GP acquired nmax data points Then select subset m nmax points Lastly 22 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 described Section 521 boundary condition Section 432 deﬁne distances du dl covariances κu κl respectively Particularly pick du kdu κu stationary isotropic kernel k use model GP dl This makes choice du intuitive directly relates covariance function GP B1 Simulation For simulation task determine impedance K inﬁnite horizon LQR parameterized cid17 cid18 Q r 0 Q 0 κd Q r R 10r2 I3 A Q r 10qc I3 cid18 cid17 cid17 0 I3 0 0 B cid18 0 I3 The matrices A B obtained assuming use feedback linearization controller 36 However instead use impedance controller nonlinearities imprecisions model The parameters qc r κd tuning parameters like optimize We deﬁne desired path xdesk xdesk p T 0 k Ttraj xdesk xdesρk pdes s0T 01x3T Here ρ parameterize cubic spline x0 xtarget The constraint cid8st sdescid8 2 cid8s0 sdescid8 2 g xt cid4 cid5 xt g ζ cid8 cid8xt xd cid8s0 sdescid8 cid5cid8 2 cid4 cid8 2 ρt α α 008 The stage rewards rewards received time step 1 2 cid8s0 sdescid82 2 R xt cid8st sdescid82 1 25 cid8tanh stcid82 2 cid4 cid5 xt R νρ ρt 12 νx 1 25 cid8 cid8xt xd cid8tanh u xtcid82 2 cid5cid8 cid4 cid8 2 ρt νu cid8 cid8 cid8 cid8 u umax cid8 cid8 cid8 cid8 2 8D task 11D task 8D task 11D task 8D task 11D task Additionally encourage fast behavior eightdimensional task sample parameters eigen values A B K ﬁxed threshold eig A B K 10 Although constraint independent state evaluated experiment parameters rejected criterion fulﬁlled The value κd heuristically set 01 ﬁrst experiment 8D task For elevendimensional task κd tuned In eightdimensional task observe underlying functions f gi exhibit nonsmooth behavior Therefore use Matérn kernel 30 parameter ν 32 GP For remaining tasks use squared exponential SE kernel B2 Hardware For hardware task deﬁne subsequent objective constraint functions R xt cid8st sdestcid8 g st cid8st sw cid8 2 P ψ sw represents center wall Fig 4 cid8s sw cid8 wall ψ dw Appendix C Sensitivity LSE GE steps P dw deﬁnes rectangular shaped outline We analyze sensitivity practical version GoSafeOpt respect nLSE nGE simple dimensional toy example The toy example consists onedimensional following dynamics stage reward constraint cid11 cid11 sk 02 ayk vk yk sk wk sk 1 101 Rs s2 gs s2 081 23 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 Fig C11 Toy Example results Objective function safe set b Performance GoSafeOpt different nLSE nGE 4 6 5 control parameter like optimize We consider regulation vk wk N 0 10 problem start x0 0 like remain close x0 We run GoSafeOpt iterations seeds different nLSE nGE values Fig C11 depicts safe set performance GoSafeOpt 6 global optimum To different nLSE nGE In example disconnected safe sets advantage global exploration initialize GoSafeOpt safe region contain For experiments obtain 100 safety Furthermore results nLSE 5 faster convergence nLSE 10 This explore region global optimum earlier However perform LSE steps global exploration fails stuck bad optimum instance nLSE 1 nGE 10 This simple example highlights tradeoff local exploration global exploration steps Appendix D Additional deﬁnitions In section present deﬁnitions SafeOpt completeness Deﬁnition D1 The expanders Gn deﬁned Gn Sn ena 0 ena La cid8 cid8 0 cid8 cid8a cid10 cid10 A Sn Ig una Deﬁnition D2 The maximizers Mn deﬁned Mn Sn una 0 maxacid10Sn lna cid10 0 Appendix E Hyperparameters The hyperparameters simulated realworld experiments provided Table E2 Table E2 Table hyperparameters 8D task Simulation 11D task Simulation Hardware task SafeOpt GoSafeOpt SafeOptSwarm GoSafeOpt SafeOptSwarm GoSafeOpt Iterations 12 β n lengthscale κ f g σ f g x lengthscale cid4 max LSE steps max GE steps κl κu ηl ηu nmax m 200 4 012012 1 1 0101 200 4 012012 1 1 0101 030303 252525 01 30 10 090 094 04 06 1000 500 200 200 3 00670201302 1 1 0101 050505 060606 10 01 100 10 090 094 03 075 1000 500 3 00670201302 1 1 0101 24 50 3 0101 01 1 1 00503 50 3 0101 01 1 1 00503 030303 050502 001 20 5 090 094 09 11 1000 500 B Sukhija M Turchetta D Lindner et al Artiﬁcial Intelligence 320 2023 103922 References 1 RS Sutton AG Barto Reinforcement Learning An Introduction 2nd edition The MIT Press 2018 2 S Levine C Finn T Darrell P Abbeel Endtoend training deep visuomotor policies J Mach Learn Res 17 1 2016 13341373 3 J Peters S Schaal Reinforcement learning motor skills policy gradients Neural Netw 21 4 2008 682697 4 TP Lillicrap JJ Hunt A Pritzel N Heess T Erez Y Tassa D Silver D Wierstra Continuous control deep reinforcement learning arXiv preprint arXiv1509 02971 2015 5 J Kober JA Bagnell J Peters Reinforcement learning robotics survey Int J Robot Res 32 11 2013 12381274 6 S Schaal CG Atkeson Learning control robotics IEEE Robot Autom Mag 17 2 2010 2029 7 J Mockus V Tiesis A Zilinskas The application Bayesian methods seeking extremum Towards Global Optim 2 117129 1978 2 8 R Calandra A Seyfarth J Peters M Deisenroth Bayesian optimization learning gaits uncertainty Ann Math Artif Intell 76 2016 523 9 A Marco P Hennig J Bohg S Schaal S Trimpe Automatic LQR tuning based Gaussian process global optimization IEEE International Conference Robotics Automation 2016 pp 270277 10 R Antonova A Rai CG Atkeson Deep kernels optimizing locomotion controllers Conference Robot Learning 2017 pp 4756 11 M Turchetta A Krause S Trimpe Robust modelfree reinforcement learning multiobjective Bayesian optimization IEEE International Con ference Robotics Automation 2020 pp 1070210708 12 M Gelbart J Snoek R Adams Bayesian optimization unknown constraints Conference Uncertainty Artiﬁcial Intelligence 2014 13 JM HernándezLobato MA Gelbart RP Adams MW Hoffman Z Ghahramani A general framework constrained Bayesian optimization informationbased search J Mach Learn Res 17 1 2016 55495601 14 A Marco D Baumann M Khadiv P Hennig L Righetti S Trimpe Robot learning crash constraints IEEE Robot Autom Lett 6 2 2021 pp 250259 14391446 15 S Heim A Rohr S Trimpe A BadriSpröwitz A learnable safety measure Conference Robot Learning 2020 pp 627639 16 Y Sui A Gotovos J Burdick A Krause Safe exploration optimization Gaussian processes International Conference Machine Learning 2015 pp 9971005 Robotics Automation 2016 pp 491496 2021 arXiv210107825 2021 17 F Berkenkamp AP Schoellig A Krause Safe controller optimization quadrotors Gaussian processes IEEE International Conference 18 F Berkenkamp A Krause AP Schoellig Bayesian optimization safety constraints safe automatic parameter tuning robotics Mach Learn 19 C König M Turchetta J Lygeros A Rupenyan A Krause Safe eﬃcient modelfree adaptive control Bayesian optimization arXiv preprint 20 EN Gryazina BT Polyak Stability regions parameter space Ddecomposition revisited Automatica 42 1 2006 1326 21 D Baumann A Marco M Turchetta S Trimpe GoSafe globally optimal safe robot learning IEEE International Conference Robotics Au tomation 2021 pp 44524458 Proofs extended online version arXiv2105 13281 22 J Kirschner M Mutny N Hiller R Ischebeck A Krause Adaptive safe Bayesian optimization high dimensions onedimensional subspaces International Conference Machine Learning 2019 pp 34293438 23 Y Sui V Zhuang J Burdick Y Yue Stagewise safe Bayesian optimization Gaussian processes International Conference Machine Learning 24 KP Wabersich MN Zeilinger A predictive safety ﬁlter learningbased control constrained nonlinear dynamical systems Automatica 129 2021 2018 pp 47814789 109597 25 P Wieland F Allgöwer Constructive safety control barrier functions IFAC Proc Vol 40 12 2007 462467 26 R Cheng G Orosz RM Murray JW Burdick Endtoend safe reinforcement learning barrier functions safetycritical continuous control tasks AAAI Conference Artiﬁcial Intelligence 2019 pp 33873395 27 B Schölkopf AJ Smola Learning Kernels Support Vector Machines Regularization Optimization Beyond MIT Press Cambridge MA USA 2001 28 C Fiedler CW Scherer S Trimpe Practical rigorous uncertainty bounds Gaussian process regression AAAI Conf Artif Intell 35 8 2021 74397447 29 ML Puterman Markov Decision Processes Discrete Stochastic Dynamic Programming John Wiley Sons 2014 30 CE Rasmussen CKI Williams Gaussian Processes Machine Learning Adaptive Computation Machine Learning The MIT Press 2005 31 N Srinivas A Krause SM Kakade MW Seeger Informationtheoretic regret bounds Gaussian process optimization bandit setting IEEE Trans Inf Theory 58 5 2012 32503265 32 SR Chowdhury A Gopalan On kernelized multiarmed bandits International Conference Machine Learning 2017 pp 844853 33 TM Cover JA Thomas Elements Information Theory Wiley Series Telecommunications Signal Processing WileyInterscience USA 2006 34 A Krause C Ong Contextual Gaussian process bandit optimization Advances Neural Information Processing Systems 2011 35 RR Duivenvoorden F Berkenkamp N Carion A Krause AP Schoellig Constrained Bayesian optimization particle swarms safe adaptive controller tuning 20th IFAC World Congress IFACPapersOnLine 50 1 2017 1180011807 36 B Siciliano O Khatib Springer Handbook Robotics 2nd edition Springer Publishing Company Incorporated 2016 37 E Todorov T Erez Y Tassa Mujoco physics engine modelbased control IEEERSJ International Conference Intelligent Robots Systems 38 DP Bertsekas Dynamic Programming Optimal Control 2nd edition Athena Scientiﬁc 2000 39 A Wischnewski J Betz B Lohmann A modelfree algorithm safely approach handling limit autonomous racecar IEEE International Conference Connected Vehicles Expo 2019 pp 16 40 M Fiducioso S Curi B Schumacher M Gwerder A Krause Safe contextual Bayesian optimization sustainable room temperature PID control tuning International Joint Conference Artiﬁcial Intelligence 2019 pp 58505856 41 C König M Turchetta J Lygeros A Rupenyan A Krause Safe eﬃcient modelfree adaptive control Bayesian optimization IEEE International Conference Robotics Automation 2021 pp 97829788 42 SE Cooper TI Netoff Multidimensional Bayesian estimation deep brain stimulation safeopt algorithm medRxiv 2022 43 J Rothfuss C Koenig A Rupenyan A Krause Metalearning priors safe Bayesian optimization arXiv preprint arXiv2210 00762 2022 44 F Berkenkamp AP Schoellig A Krause Noregret Bayesian optimization unknown hyperparameters J Mach Learn Res 2019 124 45 A Schperberg SD Cairano M Menner Autotuning controller online trajectory planner legged robots IEEE Robot Autom Lett 2022 2012 pp 50265033 25