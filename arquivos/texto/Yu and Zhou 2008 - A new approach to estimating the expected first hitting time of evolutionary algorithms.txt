Artiﬁcial Intelligence 172 2008 18091832 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A new approach estimating expected ﬁrst hitting time evolutionary algorithms Yang Yu ZhiHua Zhou National Key Laboratory Novel Software Technology Nanjing University Nanjing 210093 China r t c l e n f o b s t r c t Article history Received 12 December 2007 Received revised form 8 July 2008 Accepted 9 July 2008 Available online 12 July 2008 Keywords Evolutionary algorithms Expected ﬁrst hitting time Convergence rate Computational complexity 1 Introduction Evolutionary algorithms EA shown effective solving practical problems important theoretical issues clear The expected ﬁrst hitting time important theoretical issues evolutionary algorithms implies average computational time complexity In paper establish bridge expected ﬁrst hitting time important theoretical issue convergence rate Through bridge propose new general approach estimating expected ﬁrst hitting time Using approach analyze EAs different conﬁgurations including mutation operators withwithout population recombination operator time variant mutation operator hard problem The results proposed approach helpful analyzing broad range evolutionary algorithms Moreover explanation makes problem hard EAs based recognition prove hardness general problem 2008 Published Elsevier BV Evolutionary algorithms EAs kind optimization technique inspired natural evolution process Despite different implementations 1 genetic algorithm genetic programming evolutionary strategies traditional evolu tionary algorithms summarized steps 1 Generate initial population random solutions 2 Reproduce new solutions based current population 3 Remove relatively poor solutions population 4 Repeat Step 2 stop criterion satisﬁed In evolutionary process population randomly initialized solutions maintained evolved Mutation combination popular operators reproduction Step 2 A ﬁtness function employed guide Step 3 The evolutionary repetition stops optimal solution time runs EAs solve problems straightforward ways require example continuous differentiable functions inversable matrices So EAs applied bioinformatics 17 circuit design 3 data mining 9 information retrieval 4 Despite remarkable success achieved EAs practice problems EAs criticized lack solid theoretical foundation Actually theoretical foundation desired order gain deep understanding strength weakness current EAs develop better EAs Corresponding author Email address zhouzhnjueducn ZH Zhou 00043702 matter 2008 Published Elsevier BV doi101016jartint200807001 1810 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 The ﬁrst hitting time EAs time run EAs ﬁnd optimal solution ﬁrst time expected ﬁrst hitting time EFHT average time EAs require ﬁnd optimal solution implies average computational time complexity EAs It evident EFHT important theoretical issues EAs Many papers devoted analysis simple EAs The 1 1EA EA population studied long path problem 22 OneMax problem 23 unimodel functions 57 linear functions 67 Another EA population studied OneMax problem 10 More details Beyer et als survey 2 Owing efforts theoretical properties EAs clear In works ad hoc approaches analyze simple EAs simple problems general approach analyze wider kinds EAs gain deeper insights desired Recently works 1315 devoted developing general analysis approaches summarized latest survey 19 He Yao 1315 developed general approach analyzing wide class EAs based drift analysis 11 signiﬁcant advance Intuitively know length path optimum length drift EA step estimate EFHT dividing path length step drift However practical measure quantities known He Yao 14 developed framework based analytical solution EFHT analyze compare EAs Under framework hard problem classes problems solved exponential time wide gap problem class long path problem class identiﬁed Since analytical framework derived homogeneous Markov chain models EAs static reproduction operators analyzed EAs time variant operators adaptive operators popular powerful 8 The convergence rate important theoretical issue EAs implies close current state optimal area step The convergence issue studied years 1216212325 He Yu 16 thorough study based minorization method 20 In paper present ﬁrst study relationship EFHT convergence rate establish bridge Through bridge propose new general approach estimating expected ﬁrst hitting time In contrast previous researches easy problems problems solved polynomial time 6 1523 studied use proposed approach analyze EAs hard problem The analyzed EAs involve conﬁgurations including mutation operators withwithout population recombination operator time variant mutation operator The results proposed approach helpful analyzing broad range EAs Moreover explanation makes problem hard EA based recognition prove hardness general problem The rest paper organized follows In Section 2 brieﬂy review related work introduce model EAs Markov chains In Section 3 introduce new approach estimating EFHT main result paper In Section 4 analyze EAs hard problem proposed approach followed discussions Section 5 Finally Section 6 conclude paper 2 Modeling EAs Markov chain EAs evolve solutions generation generation Each generation stochastically depends previous initial generation randomly generated This conditional independence modeled natually Markov chains 1214182425 Combinatorial optimization problems common problems practice solutions repre sented sequence symbols In paper use EAs tackle To model kind EAs construct Markov chains discrete state space The key construct Markov chain bijectively map populations EA states Markov chain A popular mapping 121625 enables state Markov chain correspond possible population EA Suppose EA encodes solution vector length L component vector drawn alphabet set B population contains M solutions Let S denote solution space There S BL number different solutions Let X denote population space There X number different possible populations 25 A Markov chain models EA constructed taking X state space chain ξt t0 built ξt X A population called optimal population contains optimal solution Let X X denote set initial population Thus process EA seeks X MBL 1 M cid2 cid3 optimal populations The goal EAs reach X analyzed studying corresponding Markov chain 1216 In rest section introduce notations deﬁnitions Given Markov chain ξt t0 ξt X target subspace X cid4 μt X let μt t 0 1 denote probability ξt X P ξt x x X Deﬁnition 1 Convergence Given Markov chain ξt X t0 ξt X target subspace X lim t μt 1 1 X ξt t0 said converge 2 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1811 In 16 convergence rate measured 1 μt step t equivalent 25 Therefore use 1 μt measure convergence rate paper Deﬁnition 2 Convergence rate Given Markov chain ξt X time t 1 μt t0 ξt X target subspace X X convergence rate Deﬁnition 3 Absorbing Markov chain Given Markov chain ξt absorbing chain t0 ξt X target subspace X t 0 1 P cid2 ξt1 X cid3 ξt X 0 X ξt t0 said 3 We use absorbing Markov chains model EAs studied paper absorbing Markov chains good theoretical properties practically achieved An EA modeled absorbing Markov chain loses optimal solution Actually EAs real problems satisfy condition optimal solution identiﬁed EA stop ﬁnds optimal solutions identiﬁed commonly strategy keeping bestsofar solution generation condition satisﬁed Moreover EAs modeled absorbing Markov chains converge optimal solutions certain operators 16 desirable property practice Deﬁnition 4 Expected ﬁrst hitting time EFHT Given Markov chain ξt random variable τ denote events t0 ξt X target subspace X X let τ 0 ξ0 X τ 1 ξ1 X τ 2 ξ2 X ξi X ξi X τ t ξt X ξi X 0 cid2 0 1 cid3 cid2 0 1 t 1 cid3 The mathematical expectation τ Eτ called expected ﬁrst hitting time EFHT Markov chain This deﬁnition EFHT equivalent 1314 The EFHT EA average time ﬁnds optimal solution average computational time complexity Markov chains model essential corresponding EA processes convergence convergence rate EFHT EAs obtained analyzing corresponding Markov chains So rest paper distinguish convergence convergence rate EFHT EAs corresponding Markov chains 3 Deriving expected ﬁrst hitting time convergence rate The convergence rate studied years 1225 recently general bound developed 16 minorization condition method 20 Since focus EAs solve combinatorial optimization problems paper discrete space version Theorem 4 16 proven Lemma 1 Given absorbing Markov chain ξt satisfy t0 ξt X target subspace X X sequences αt t0 βt t0 cid5 t0 1 αt 0 βt cid2 cid4 x X cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid2 αt chain converges X convergence rate bounded 1 μ0 t1cid5 i0 1 αi cid2 1 μt cid2 1 μ0 t1cid5 i0 1 βi 4 5 6 1812 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 Proof From Eqs 1 3 follows cid4 cid3 ξt1 x μt μt1 cid2 ξt X P P ξt1 x x X applying Eq 5 1 μt1αt1 cid3 μt μt1 cid3 1 μt1βt1 1 μt11 αt1 cid2 1 μt cid2 1 μt11 βt1 applying inequality recursively 1 μ0 t1cid5 i0 1 αi cid2 1 μt cid2 1 μ0 t1cid5 i0 1 βi cid2 Lemma 1 implies far probability EA jumping set optimal solutions estimated step bounds convergence rate derived The requirement EA modeled absorbing Markov chain EA satisﬁes Eq 3 As mentioned EAs real problems meet requirement In Deﬁnition 4 EFHT mathematical expectation random variable τ Meanwhile probability distribu tion τ probability optimal solution step t t 0 1 Thus long EA modeled absorbing Markov chain holds μt1 μt cid4 x X P ξt1 x cid4 x X P ξt x P τ t 1 This implies probability distribution τ equal μt minus convergence rate So conver gence rate EFHT sides coin Meanwhile bounds probability distribution bounds expectation random variable relationship shown Lemma 2 Lemma 2 Let u v denote discrete random variables nonnegative integers limited expectation D u D v denote distribution functions respectively D ut P u cid3 t D v t P v cid3 t tcid4 i0 tcid4 i0 P u P v If D ut cid2 D v t t 0 1 expectations random variables satisfy Eu cid3 Ev Eu cid6 t01 t P u t Ev cid6 t01 t P v t 7 Proof Since D u distribution u Eu 0 D u0 cid2 cid3 D ut D ut 1 t cid4 t1 cid4 cid4 cid2 cid3 D ut D ut 1 i1 cid4 ti cid7 lim t i0 cid8 D ut D ui cid4 cid3 cid2 1 D ui i0 v Thus Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1813 Eu Ev cid4 cid2 cid3 1 D ui cid4 cid2 cid3 1 D v i0 cid4 cid2 i0 cid3 D v D ui i0 cid3 0 cid2 Since minus convergence rate probability distribution τ EFHT expectation τ Lemma 2 reveals lowerupper bounds EFHT derived upperlower bounds convergence rate Thus based Lemmas 1 2 pair general bounds EFHT Theorem 1 obtained Theorem 1 Given absorbing Markov chain ξt satisfy t0 ξt X target subspace X X sequences αt t0 βt t0 cid5 t0 1 αt 0 βt cid2 cid4 x X cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid2 αt chain converges starting nonoptimal solutions EFHT bounded Eτ cid3 α0 Eτ cid2 β0 cid4 t2 cid4 t2 tαt1 t2cid5 i0 1 αi tβt1 t2cid5 i0 1 βi Proof Applying Lemma 1 Eq 9 1 μt cid3 1 μ0 t1cid5 i0 1 αi 8 9 10 11 Considering μt expresses distribution τ μt Dτ t lower bound Dτ t cid9 Dτ t cid2 μ0 1 1 μ0 cid10 t1 i01 αi t 0 t 1 2 Imagine virtual random variable η distribution equals lower bound Dτ The expectation η cid13 t 1 μ0 cid4 cid12 t2 t2cid5 1 αi 1 μ0 t1cid5 i0 i0 cid14 1 αi Eη 0 μ0 1 cid11 1 1 α01 μ0 μ0 1 αi 1 μ0 cid13 α0 cid4 t2 t2cid5 tαt1 i0 cid13 cid4 t2 t2cid5 tαt1 i0 cid14 cid14 Eτ cid3 α0 1 αi 1 μ0 Since Dτ t cid2 Dηt according Lemma 2 Eτ cid3 Eη Thus upper bound EFHT Note EA assumed start nonoptimal solutions μ0 0 The lower bound EFHT derived similarly cid2 Two points Theorem 1 remain clariﬁed Firstly starting nonoptimal solutions theoretical assump tion result easy read Practically problems EAs applied probability randomly generated solution optimal exponentially small In case assumption affect result asymptotic analysis Secondly Theorem 1 written compact form lower upper bounds 1814 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 EFHT βt αt Eq 9 Actually applicable We lower bound EFHT βt upper bound EFHT αt The bounds EFHT Eqs 10 11 intuitive explanation The αt1 t2 i01 αi replacing α β indicates probability event EA ﬁnds optimal solution tth step ﬁnd earlier step cid10 Theorem 1 shows bounds EFHT bounds formula cid2 ξt1 X P ξt x cid3 P ξt x 1 μt 12 cid4 x X ξt x probability EA jumping optimal population The ﬁrst formula P ξt1 X success probability The second P ξt x normalized distribution nonoptimal states As long 1μt parts estimated bounds EFHT derived The accurate estimated probability tighter derived bounds 4 Case study hard problem In section prove Trap problem hard solved exponential time EAs proposed approach The Trap problem deﬁned Deﬁnition 5 Trap problem Given set n positive values W w i1 capacity value c ﬁnd x arg max x x01n ncid4 i1 w xi st ncid4 i1 w xi cid3 c w 1 w 2 wn1 1 wn cid6 n1 i1 w 1 c wn Trap problem optimal solution x 000 01 A solution feasible solution satisﬁes constraint infeasible solution We try tackle Trap problem EAs conﬁgured commonly The Reproduction implemented concrete operators later Encoding Each solution encoded string n binary bits ith bit 1 w included 0 Initial Randomly generate population M solutions encoded binary strings Reproduction Generate M new solutions current population Selection Select best M solutions current population reproduced solutions called plusselection form population generation The selected M solutions best ﬁtness value according deﬁnition Fitness The ﬁtness solution x x1x2 xn deﬁned Fitnessx θ ncid4 i1 w xi c 13 θ 1 x feasible solution maximized larger ﬁtness better solution Here maximum ﬁtness value zero i1 w xi cid3 c θ 0 The ﬁtness function Stop criterion If largest ﬁtness value population zero stop output solution maximum ﬁtness cid6 n To implement Reproduction operator use popular operators listed Mutation1 bitwise mutation constant probability Independently ﬂip bit solution constant probability pm 0 05 Mutation2 bitwise mutation probability 1n Independently ﬂip bit solution probability pm 1 n This commonly mutation operator Mutation3 onebit mutation Randomly ﬂip bit solution Mutation4 timevariant mutation Independently ﬂip bit solution probability 05 d 0 05 t 0 1 t d Recombination onepoint crossover Exchange leading σ bits randomly selected solutions σ drawn randomly 1 2 n 1 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1815 Mutation1 like special case Mutation2 However signiﬁcant difference mutation prob ability Mutation2 adapted problem size Mutation1 constant So asymptotic behaviors n different To focus order asymptotic complexity EFHT EAs use Ω representation For functions f g write f n Ωgn represent gn asymptotic lower bound f n lim n f n gn 0 write gn O f n 41 Static mutation population First static mutation operators Mutations 1 2 3 perform Trap problem population size 1 1 1EA Since population equal solution population state space equal solution state space X S Proposition 1 Solving Trap problem EA Reproduction implemented Mutation1 bitwise mutation constant probability population size 1 1 1EA starting nonoptimal populations EFHT bounded Eτ Ωθ n θ 1 pm1 1 2 constant n problem size 14 To prove proposition need ﬁnd upper bound formula 12 applying Theorem 1 We ﬁrst investigate ξt x Assuming solution k bits different optimal success probability formula 12 P ξt1 X m1 pmnk Mutation1 So solution probability solution mutated optimal solution pk maximum probability solution mutated optimal solution pm1 pmn1 means ξt x cid3 pm1 pmn1 Then applying Theorem 1 bit difference Therefore P ξt1 X upper bound proposition ξt x cid3 pm1 pmn1 Proof Since P ξt1 X cid2 ξt1 X cid4 P x X cid3 cid4 x X ξt x cid3 P ξt x 1 μt pm1 pmn1 P ξt x 1 μt cid6 x X P ξt x 1 μt pm1 pmn1 pm1 pmn1 Eq 1 Let βt pm1 pmn1 Theorem 1 cid4 t2cid5 Eτ cid2 β0 tβt1 t2 i0 1 βi 1 pm cid16 n1 cid15 1 1 pm 1 pm θ n1 Considering pm constant Eτ Ωθ n cid2 Proposition 2 Solving Trap problem EA Reproduction implemented Mutation2 bitwise mutation proba bility 1n population size 1 1 1EA starting nonoptimal populations EFHT bounded Eτ Ω 2n cid2 cid3 n problem size If follow idea proof Proposition 1 obtain upper bound success probability P ξt1 X ξt x cid3 1en obtain Ωn lower bound EFHT loose To tighter bound steps First formula 12 O 12n beginning t 0 Second formula 12 decreases t increases So O 12n upper bound formula 12 By applying Theorem 1 Proposition 1 There trick calculating formula 12 We divide state space subspaces states share i0 common properties treat subspace We ﬁrst divide state space n 1 subspaces Xin 15 1816 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 Xi contains solutions exactly identical bits optimal solution solutions subspace probability mutated optimal solution By division success probability t 0 calculated We divide state space optimal space X feasible space X F infeasible space X I according solutions satisfy constraint combine division previous By division ﬁnd formula 12 decreases t increases optimal solution means solutions Xi bits identical Proof Let cid17 Xi x X cid9x x cid9 cid9H Hamming distance x i0 Xi Xi optimal solution X cid9H n cid19 n cid3 cid2 n cid18 Xn X Then applying Mutation2 calculate success probability x Xi cid2 ξt1 X cid3 ξt x P cid15 1 n cid16 ni cid15 1 1 n cid16 At t 0 cid4 cid2 ξ1 X P ξ0 x cid3 P ξ0 x 1 μ0 cid3 ξ0 x x X cid4 cid2 ξ1 X P P ξ0 x assumption μ0 0 x X n1cid4 cid4 cid2 P ξ1 X cid3 ξ0 xP ξ0 x X ncid20 i0 Xi cid16 ni cid16 cid15 1 1 n 1 2n 1 n x P ξ0 x 1 2n i0 n1cid4 x Xi cid15 n cid16cid15 i0 cid15 cid15 1 n 1 n cid16 n cid16 1 2n 1 2n e 1 e X F X I X Let X X contains optimal solutions X F contains nonoptimal feasible solutions bit 0 X I contains infeasible solutions bit 1 Denote X F Xi X F According ﬁtness function x0 X F 1 xn1 X F 0 x1 X F f x0 f x1 f xn1 f xI n1 xI X I f x selection behavior solutions largest ﬁtness selected P j q n 1 cid2 j q cid2 0 cid3 cid2 ξt1 X F j P ξt1 X F j P ξt1 X F q ξt X F 0 q ξt X I ξt X I j q P ξt1 X I ξt X F 0 P ξ0 X F j P ξ0 X F q equation infeasible solution lowest ﬁtness selection pressure leading n 1 bits solution infeasible leading n 1 bits probability 05 zero cid19 k i0 X F X Bk cid19 n1 ik1 X F time t 0 k 0 1 n 1 For k 0 1 n 1 denoting X Ak exists η Akt ηBkt ηF t ηIt cid2 ξt1 X cid3 ξt x P cid2 ξt1 X cid3 ξt x P P ξt x η Akt P ξt X Ak P ξt x ηBkt P ξt X Bk cid4 x X Ak cid4 x X Bk Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1817 cid2 ξt1 X cid3 ξt x P P ξt x ηF t P ξt X F cid2 ξt1 X cid3 ξt x P P ξt x ηIt P ξt X I cid4 x X F cid4 x X I On relationship η Akt ηBkt holds η Akt ηBkt P ξt1 X ξt X Ak cid3 P ξt1 X ξt X F k cid3 P ξt1 X ξt X Bk On relationship η Akt ηIt holds t 0 η Ak0 ηI0 ﬁrst η Ak0 ηF 0 ηF 0 η Ak0 x1 x2 X P ξ0 x1 P ξ0 x2 cid6 x X F ξ0 x P ξ1 X P ξ0 X F cid6 x X I ξ0 x P ξ1 X P ξ0 X I P ξ0 X A P ξ0 X F ηBk0 P ξ0 X B P ξ0 X F η Ak0 ηBk0 second ηF 0 ηI0 And t 0 η Akt ηI0 P ξt1 X Ak P ξt X Ak P ξt X Ak cid4 X I x X Bk cid4 P ξt1 X Ak ξt xP ξt x cid2 ξt1 X Ak P cid3 ξt x P ξt x cid2 ξt1 X P X Bk cid3 X I ξt x P ξt x cid2 ξt1 X cid3 ξt x P P ξt x cid4 x X Ak cid4 x X Ak x X Bk X I cid2 ξt1 X Bk cid3 P cid2 ξt1 X cid2 1 P X I ξt X Ak 0 P ξt X Ak cid3cid3 ξt X F k cid4 P P cid2 ξt1 X ξt X A cid3 cid2 P x X I cid2 ξt1 X cid2 ξt1 X Ak cid3 ξt X F k cid3 tcid5 cid2 1 P ξi1 X cid3 ξi X F k P cid2 ξ0 X Ak cid21 tcid4 cid4 i0 cid2 ξi1 X Ak P cid3 ξi x cid22cid21 P ξi x tcid5 cid2 1 P ξ j1 X cid22 cid3 ξ j X F k cid3 ξt x P ξt x i0 x X I ji P ξ0 X Ak tcid5 cid2 1 P ξi1 X i0 cid3 ξi X F k cid21 tcid4 i0 cid2 ξi1 X Ak P cid3 ξi X I P ξi X I tcid5 cid2 ji 1 P ξ j1 X cid3 ξ j X F k cid22 cid3 ξi X F k cid21 P ξi X I tcid5 ji similarly P ξt1 X Bk P cid3 tcid5 cid2 cid2 ξ0 X Bk i0 1 P ξi1 X tcid4 i0 cid2 ξi1 X Bk P cid3 ξi X I P ξt1 X F j P ξt1 X F q ξt X I ξt X I P ξ0 X F j P ξ0 X F q k P ξt1 X Ak P ξt1 X Bk P ξ0 X Ak P ξ0 X Bk cid2 1 P ξ j1 X cid22 cid3 ξ j X F k 1818 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 P ξt1 X cid6 ξt X Ak P ξt1 X P ξt1 X ξt xP ξt x ξt X Bk enumerating k ξt xP ξ0 x P ξ1 X cid6 x X Ak x X Ak P ξ0 X Ak P ξt X Ak η Akt η Ak0 η Ak0 ηI0 holds η Akt ηI0 Then P ξt1 X Ak 1 μt1 cid6 P ξt X Ak 1 μt cid6 x X Ak x X Ak X I X Bk ξt xP ξt x P ξt1 X P ξt1 X ξt xP ξt x 1 η AktP ξt X Ak 1 μt η Akt P ξt X Ak ηBkt P ξt X Bk ηIt P ξt X I 1 η AktP ξt X Ak 1 μt η Akt P ξt X Ak η Akt P ξt X Bk η Akt P ξt X I η Akt ηBkt η Akt ηI0 ηIt ηI0 1 η AktP ξt X Ak 1 η Akt1 μt P ξt X Ak 1 μt cid19 k n 1 cid2 k cid2 0 P ξt1 i0 X F 1μt1 cid2 P ξt cid19 k i0 X F 1μt writing X Ak cid19 k i0 X F So cid4 x X cid3 cid4 x X F cid4 x X F cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid2 ξt1 X P ξt x cid2 ξ1 X P ξ0 x cid3 P ξt x 1 μt cid3 P ξ0 x 1 μ0 cid4 cid2 ξt1 X P ξt x x X I cid4 cid2 ξ1 X P ξ0 x x X I P ξt1 cid19 k i0 X F 1 μt1 n 1 cid2 k cid2 0 cid21 P ξt1 X ξt kcid20 cid22 cid21 X F cid3 P ξt1 X ξt P ξt cid2 cid22 kcid20 i0 X F cid3 P ξt x 1 μt cid3 P ξ0 x 1 μ0 cid19 k i0 X F 1 μt ξ0 x i0 cid3 P ξ0 x 1 μ0 cid4 cid2 ξ1 X P x X e 1 e 1 2n Let βt e1 e 1 2n Theorem 1 Eτ cid2 e e 1 2n Eτ Ω2n cid2 Proposition 3 Solving Trap problem EA Reproduction implemented Mutation3 onebit mutation population size 1 1 1EA starting nonoptimal populations EFHT bounded Eτ Ω2n n problem size 16 We follow idea proof Proposition 2 First t 0 formula 12 calculated O 12n Then ﬁnd formula 12 reduces t increases leads upper bound O 12n formula 12 By Theorem 1 EFHT lower bound Ω12n The difference proof Proposition 2 solution space divided subspaces different way according characteristic Mutation3 To arrive proof divide state space subspaces subspace solutions Hamming distance optimal solution By division Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1819 ﬁnd solutions bit different optimal solution nonzero probability mutated optimal solution Thus formula 12 calculated Proof Let Xi cid17 x X cid9x x cid9H n cid18 cid9 cid9H Hamming distance x Then applying Mutation3 success probability n 1 cid3 ξt x cid2 ξt1 X x Xi 1 n 0 cid9 P optimal solution X cid19 n i0 Xi Xi cid3 cid2 n Xn X Noticing Xn1 contains feasible solution lowest ﬁtness feasible solutions n 1 infeasible solutions lowest ﬁtness solutions ξt Xn1 cid2 ξt1 Xn1 ξt X Xn1 X cid2 ξt1 X Xn1 X P cid3 P cid3 At t 0 cid4 P x X cid2 ξ1 X ξ0 x cid3 P ξ0 x 1 μ0 cid4 cid2 ξ1 X P ξ0 x x Xn1 cid4 x Xn1 n 2n 1 n cid3 P ξ0 x 1 μ0 cid2 ξt1 X P subspace dividing ξt Xn1 cid3 1 n 1 n P ξ0 x 1 μ0 1 2n P ξ0 Xn1 Xn1 2n 1 2n At time t 1 relationship μt μt1 μt1 μt P μt P cid3 cid2 ξt1 X cid2 ξt1 X cid2 ξt1 X ξt X Xn1 X ξt Xn1 cid3 ξt X Xn1 X P cid3 0 cid2 ξt1 X ξt Xn1 cid3 P On relationship P ξt Xn1 P ξt1 Xn1 P ξt1 Xn1 P ξt Xn1 P cid2 ξt1 Xn1 ξt X Xn1 X cid3 cid3 cid2 ξt1 X P ξt Xn1 cid3 P ξt Xn1 P ξt1 X cid2 ξt1 Xn1 ξt X Xn1 X P ξt Xn1 P cid2 ξt1 X Xn1 X cid3 ξt Xn1 cid3 cid2 ξt1 X Xn1 X P ξt Xn1 cid3 Considering relationships P ξt1 Xn1 1 μt1 P ξt Xn1 P ξt1 X ξt Xn1 1 μt P ξt1 X ξt Xn1 cid3 P ξt Xn1 1 μt t 1 μt cid2 P ξt Xn1 cid2 0 Therefore cid4 cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid3 P ξ0 x 1 μ0 ξ0 x x X cid3 cid4 x X cid2 ξ1 X P P ξt1 Xi 1 μt1 cid3 P ξt Xi 1 μt 1 2n So let βt 1 2n Theorem 1 EFHT lower bounded Eτ cid2 2n Eτ Ω2n cid2 1820 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 42 Static mutation population Now study static mutation operators perform population size larger 1 Speciﬁcally let consider case population size equal problem size n n nEA practical strategy In case population state space consists solution state spaces means population x X contains n solutions solution space S We consider population set solutions order Denoting solution popula tion set populations consist solutions different orders equal 001 011 110 cid2 2nn1 110 011 001 By consideration X number population states 25 But generate n population generating bit solution independently uniform distribution differ ent probabilities choose different states P 001 001 001 059 P 001 011 110 6 059 Meanwhile equivalently consider population ordered set solutions Denoting popu lation order populations consist solutions different orders unequal 001 011 110 cid13 110 011 001 By consideration X 2nn number different population states probability randomly generating population exactly 1 X We use second consideration follows calculation simple cid3 Proposition 4 Solving Trap problem EA Reproduction implemented Mutation1 bitwise mutation constant probability population size equals problem size n nEA starting nonoptimal populations EFHT bounded Eτ Ω cid15 cid16 θ n n 17 θ 1 pm1 1 2 constant n problem size The proof proposition Proposition 1 state level upgraded population states We know maximum probability solution mutated optimal solution Mutation1 pm1 pmn1 leads maximum probability population mutated optimal population 1 1 pm1 pmn1n Therefore upper bound formula 12 By Theorem 1 proposition ξt x cid3 1 1 pm1 pmn1n Proof Since P ξt1 X cid2 ξt1 X cid4 P x X cid3 cid4 ξt x cid3 P ξt x 1 μt pm1 pmn1 P ξt x 1 μt cid6 cid3 cid3 n cid2 1 pm1 pmn1 cid3 n x X cid2 1 cid2 1 pm1 pmn1 x X P ξt x 1 μt 1 Eq 1 npm1 pmn1 asymptotically equal Let βt npm1 pmn1 Theorem 1 cid4 t2cid5 Eτ cid2 β0 tβt1 t2 i0 1 βi 1 n cid16 n1 cid15 1 pm 1 1 pm θ n npm Considering pm constant Eτ Ω θ n n cid2 Proposition 5 Solving Trap problem EA Reproduction implemented Mutation2 bitwise mutation proba bility 1n population size equals problem size n nEA starting nonoptimal populations EFHT bounded 18 Eτ Ω cid15 cid16 2n n2 n problem size Following proof Proposition 2 divide population state space X 0 1nn n 1 subspaces Xin i0 Xn contains optimal populations Xi 0 n 1 contains nonoptimal populations solution Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1821 worst ﬁtness Xi identical bits optimal solution By state subspaces hold properties proof Proposition 2 leads calculation formula 12 The difference proof Proposition 2 upper bound formula 12 calculated population level solution level results O n2 2n Therefore lower bound Ω 2n n2 obtained Theorem 1 Proof Let cid23 Xi x X min sx cid9s s cid9H n cid24 cid9 cid9H Hamming distance x denotes population s denotes solution s optimal solution Denote Xi cid17 x Xi s x cid9s s cid9H n cid18 By applying Mutation2 success probability x Xi cid2 ξt1 X cid3 ξt x P cid2 ξt1 X cid3 P ξt Xi cid3 considering worst population subspace Xi cid16 n ni cid15 cid15 cid15 cid16 cid16 1 1 n 1 1 cid16 ni 1 cid15 n 1 n cid16 n cid15 1 1 n asymptotically equal Because solutions population subspace Xi bits identical optimal solution solution holding exact bits probability initialization cid22n1 cid21 cid15 cid16 cid16 cid16 cid16 cid16 cid15cid15 n n 1 1 2n icid4 cid15 n j j0 1 2n cid15 n cid3 n 1 2n P ξ0 Xi At t 0 cid4 cid2 ξ1 X P ξ0 x cid3 P ξ0 x 1 μ0 cid3 ξ0 x x X cid4 cid2 ξ1 X P P ξ0 x assumption μ0 0 x X n1cid4 cid4 cid2 cid2 ξ1 X cid3 ξ0 x cid3 P ξ0 x P X ncid20 i0 Xi cid3 i0 n1cid4 x Xi cid15 n2 i0 cid15 n cid15 n2 1 e 1 e n2 2n cid16 ni 1 cid15 1 1 n cid16 cid16 cid15 n 1 2n n 1 n cid16 n cid16 1 2n Let solution space S divided subspaces S S contains optimal solution S F contains nonoptimal feasible solutions bit 0 S I contains infeasible solutions bit 1 Denote S F S I S X F x X s x s S F X I X X F X denote X F Xi X F X F Xi X F According selection behavior solutions largest ﬁtness selected j qn 1 cid2 j cid2 q cid2 0 j q P ξt1 X F j P ξt1 X F q P ξt X I ξt X I cid2 ξt1 X F j P ξ0 X F j P ξ0 X F q ξt X F q cid3 0 cid2 ξt1 X I ξt X F cid3 P 0 1822 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 equation infeasible solution lowest ﬁtness selection pressure leading n 1 bits solution current population infeasible leading n 1 bits probability 05 zero cid19 k i0 X F X Bk cid19 n1 ik1 X F time 0 k 0 1 n 1 For k 0 1 n 1 denoting X Ak exists η Akt ηBkt ηF t ηIt cid4 cid2 ξt1 X cid3 ξt x P P ξt x η Akt P ξt X Ak x X Ak cid4 cid2 ξt1 X cid3 ξt x P P ξt x ηBkt P ξt X Bk x X Bk cid4 cid2 ξt1 X cid3 ξt x P P ξt x ηF t P ξt X F x X F cid4 x X I cid2 ξt1 X cid3 ξt x P P ξt x ηIt P ξt X I On relationship η Akt ηBkt holds η Akt ηBkt cid3 P ξt1 X ξt X Ak cid3 P ξt1 X cid2 ξt1 X cid2 ξt1 X P ξt X Ak cid3 P according deﬁnition X F k cid2 ξt1 X ξt X F cid3 P cid3 cid2 ξt1 X P n k ξt X Bk cid3 ξt X F k cid3 ξt X Bk On relationship η Akt ηIt holds t 0 η Ak0 ηI0 ﬁrst η Ak0 ηF 0 ηF 0 η Ak0 x1 x2 X P ξ0 x1 P ξ0 x2 P ξ0 X A P ξ0 X F ηBk0 P ξ0 X B P ξ0 X F η Ak0 ηBk0 second ηF 0 ηI0 cid6 x X F ξ0 x P ξ1 X P ξ0 X F cid6 x X I ξ0 x P ξ1 X P ξ0 X I And t 0 η Akt ηI0 P ξt1 X Ak P ξt X Ak P ξt X Ak cid4 X I x X Bk cid4 P ξt1 X Ak ξt xP ξt x P ξt1 X Ak ξt xP ξt x cid2 ξt1 X P X Bk cid3 X I ξt x P ξt x cid2 ξt1 X cid3 ξt x P P ξt x cid4 x X Ak cid4 x X Ak x X Bk X I P ξt1 X Bk cid2 cid2 ξt1 X 1 P ξt X F X I ξt X Ak 0 P ξt X Ak cid3cid3 k cid4 cid3 cid2 P x X I cid2 ξt1 X P ξt1 X Ak cid3 ξt X F k ξt xP ξt x ξt X A cid2 ξt1 X tcid5 cid2 1 P ξi1 X cid3 ξi X F k i0 cid2 ξi1 X Ak P cid3 ξi x cid22cid21 P ξi x cid2 1 P ξ j1 X cid22 cid3 ξ j X F k tcid5 ji P P ξ0 X Ak cid21 tcid4 cid4 i0 x X I P ξ0 X Ak tcid5 cid2 1 P ξi1 X i0 cid3 ξi X F k cid21 tcid4 i0 cid2 ξi1 X Ak P cid3 ξi X I tcid5 ji P ξi X I 1 P ξ j1 X cid22 ξ j X F k Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1823 similarly P ξt1 X Bk P ξ0 X Bk cid2 1 P ξi1 X tcid5 i0 cid3 ξi X F k cid21 tcid4 i0 cid2 ξi1 X Bk P cid3 ξi X I P ξi X I tcid5 cid2 1 P ξ j1 X cid22 cid3 ξ j X F k ji P ξ0 X Ak P ξ0 X Bk P ξt1 X ξt X Ak P ξt1 X ξt P ξt1 X F j P ξt1 X F q ξt X I ξt X I X Bk enumerating k P ξ0 X F j P ξ0 X F q cid6 k P ξt1 X Ak P ξt1 X Bk P ξt1 X ξt xP ξt x x X Ak P ξt X Ak cid6 x X Ak P ξ1 X ξt xP ξ0 x P ξ0 X Ak η Akt η Ak0 η Ak0 ηI0 holds η Akt ηI0 Then P ξt1 X Ak 1 μt1 cid6 P ξt X Ak 1 μt cid6 x X Ak x X Ak X I X Bk ξt xP ξt x P ξt1 X P ξt1 X ξt xP ξt x 1 η AktP ξt X Ak 1 μt η Akt P ξt X Ak ηBkt P ξt X Bk ηIt P ξt X I 1 η AktP ξt X Ak 1 μt η Akt P ξt X Ak η Akt P ξt X Bk η Akt P ξt X I η Akt ηBkt η Akt ηI0 ηIt ηI0 1 η AktP ξt X Ak 1 η Akt1 μt n 1 cid2 k cid2 0 P ξt1 cid2 P ξt cid19 k i0 X F 1μt writing X Ak cid19 k i0 X F So cid2 ξt1 X P ξt x cid4 cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid3 P ξ0 x 1 μ0 cid2 ξ1 X P ξ0 x x X I cid4 cid2 ξ1 X P ξ0 x x X I P ξt1 cid19 k i0 X F cid2 1 μt1 cid21 cid3 P ξt1 X ξt cid3 P ξt x 1 μt cid3 P ξ0 x 1 μ0 cid19 k i0 X F P ξt 1 μt cid22 n1cid20 ik1 X F P ξt X Ak 1 μt cid19 k i0 X F 1μt1 cid3 P ξt x 1 μt ξt x cid2 ξt1 X P cid4 x X cid3 cid4 x X F cid4 x X F n 1 cid2 k cid2 0 cid21 P ξt1 X ξt cid22 kcid20 i0 X F ξ0 x cid3 P ξ0 x 1 μ0 cid4 cid2 ξ1 X P x X e 1 e n2 2n Let βt e1 e n2 2n Theorem 1 2n n2 n2 cid2 Eτ cid2 e e 1 Eτ Ω 2n 1824 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 Proposition 6 Solving Trap problem EA Reproduction implemented Mutation3 onebit mutation population size equals problem size n nEA starting nonoptimal populations EFHT bounded 19 Eτ Ω cid15 cid16 2n n2 n problem size As proof Proposition 5 divide population state space X 0 1nn n 1 subspaces Xin i0 Xn contains optimal populations Xi 0 n 1 contains nonoptimal populations solution worst ﬁtness population n bits different optimal solution By state subspaces hold properties proof Proposition 2 leads calculation formula 12 The difference proof Proposition 2 upper bound formula 12 calculated population level solution level results O n2 2n Therefore lower bound Ω 2n n2 obtained Theorem 1 Proof Let cid23 Xi x X min sx cid9s s cid9H n cid24 cid9 cid9H Hamming distance x denotes population s denotes solution s optimal solution By applying onebit mutation success probability cid2 ξt1 X cid3 ξt x P cid9 cid3 1 1 1 0 n n x Xn1 x X Xn1 X considering populations Xn1 chance mutate optimal best case solutions Xn1 1 bit different optimal solution Since n solutions bit different optimal solution probability Xn1 initialization P ξ0 Xn1 1 cid15 1 n 2n cid16 n Noticing Xn1 consists populations contain feasible solution lowest ﬁtness feasible solutions n 1 infeasible solutions lowest ﬁtness solutions cid2 ξt1 Xn1 ξt X Xn1 X cid3 P cid2 ξt1 X Xn1 X P ξt Xn1 cid3 At t 0 cid2 ξ1 X cid4 P ξ0 x cid3 P ξ0 x 1 μ0 cid3 ξ0 x cid4 cid2 ξ1 X P P ξ0 x assumption μ0 0 x X x X cid4 x Xn1 cid3 1 n2 2n cid2 ξ1 X cid3 ξ0 x P P ξ0 x P cid15 cid15 cid2 ξt1 X cid16cid15 cid16 n 1 1 n cid3 0 cid15 ξt X Xn1 X 1 n 2n 1 cid16 n cid16 At time t 1 relationship μt μt1 μt1 μt cid4 cid2 ξt1 X cid3 ξt x P P ξt x x X Xn1 X cid4 cid2 ξt1 X cid3 ξt x P ξt x P μt cid2 ξt1 X cid3 ξt x P P ξt x cid4 x Xn1 x Xn1 cid2 ξt1 X P ξt X Xn1 X cid3 0 On relationship P ξt Xn1 P ξt1 Xn1 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1825 P ξt1 Xn1 P ξt Xn1 cid4 P ξt1 Xn1 ξt xP ξt x cid4 x X Xn1 X cid3 ξt x cid2 ξt1 X P P ξt cid4 cid2 ξt1 X Xn1 X cid3 ξt x P P ξt x x Xn1 P ξt Xn1 cid4 cid2 ξt1 X P x Xn1 cid3 ξt x P ξt x Xn1 cid2 ξt1 Xn1 ξt X Xn1 X P cid3 cid2 ξt1 X Xn1 X P ξt Xn1 cid3 Considering relationships cid6 x Xn1 cid6 P ξt Xn1 1 μt cid3 P ξt Xn1 1 μt P ξt1 X ξt xP ξt x Xn1 P ξt1 X ξt xP ξt x t 1 μt cid2 P ξt Xn1 cid2 0 ξt x x X cid3 cid4 cid2 ξ1 X P ξ0 x cid3 P ξt x 1 μt cid3 P ξ0 x 1 μ0 P ξt1 Xn1 1 μt1 cid3 P ξt Xn1 1 μt P ξt1 Xn1 1 μt1 Therefore cid4 cid2 ξt1 X P x X cid3 n2 2n So let βt n2 2n Theorem 1 EFHT lower bounded Eτ cid2 2n n2 Eτ Ω 2n n2 cid2 43 Mutation recombination population We implement Reproduction mutation operator recombination operator follows ξ M t1 ξ C t1 deﬁned sets solutions produced mutation recombination ξt respectively 1 population ξt contains n solutions 2 apply mutation ξt produce n solutions ξ M t1 3 apply recombination ξt produce n solutions ξ C 4 choose best n solutions ξt ξ M t1 ξ C t1 t1 Considering ﬁtness function Trap problem bit solution 1 optimal solution solution worst ﬁtness We divide solution space S SF SI bit solutions SF 0 bit solutions SI 1 This separation helpful analysis recombination We ﬁrst lemma based lower bounds EAs derived Lemma 3 Solving Trap problem EA population size equals problem size n nEA Let Φ X Z function cuts bit solutions population X Z population space Z 0 1nn1 population contains n solutions n 1 bits long Then long solution current population ξt SI Z Z cid2 Φξt Z cid3 P Z Z This lemma tells leading n 1 bits solutions population distribute given solution bit 1 To prove lemma ﬁrst ﬁnd initialization leading n 1 bits solution probability 05 zero Second notice applying mutation recombination operators change distribution Finally selection change distribution n 1 bits far solution SI remains population selection operation Thus lemma proved ξ M By applying arbitrary symmetric operator holds P ξt1 y ξt x P ξt1 x ξt y t1 denote populations ξt mutation recombination operators respectively 1826 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 Proof At t 0 Z Z cid2 Φξ0 Z cid3 P Z Z random initialization At time t 1 mutation operators recombination operator symmetric cid25 P ξ M t1 P ξ R t1 t1 ξ R y ξt x P ξ M t1 y ξt x P ξ R t1 x ξt y x ξt y arbitrary x X y X y P ξ1 y P ξ0 y cid4 P ξ1 y ξ0 xP ξ0 x cid4 x X y P ξ1 x ξ0 yP ξ0 y x X y cid4 P ξ1 y ξ0 xP ξ0 x P ξ0 y x X y P ξ1 y ξ0 x P ξ1 x ξ0 y cid4 cid15 cid16 P ξ1 y ξ0 x 1 X 1 X x X y 0 Therefore X X cid2 ξ M 1 P cid3 X X X cid2 ξ C 1 P cid3 X X X mutation operators recombination operator respectively This leads cid2 Z Z Z Z noticing universal quantiﬁer cid2 Φ Z ξ M 1 P cid3 cid3 cid3 cid2 cid2 Φ P ξ C 1 cid3 Z Z Z solution SI survives ξt1 cid2 s SF P s ξt1 s ξt ξ M t1 ξ R t1 cid3 1 The Selection operation generates ξt1 choosing best n solutions ξt ξ M t1 ξ R t1 Since considering solution SF better ﬁtness value nonoptimal solutions SI Since nonoptimal solutions SI ﬁtness values probability survive ξt1 P ξ R ξ R s1 ξt1 s1 ξt ξ M t1 s2 ξt1 s2 ξt ξ M t1 s1 s2 SI cid3 t1 t1 P cid3 cid2 cid2 Therefore Z Z cid2 Φξt1 Z cid3 P Z Z cid2 Proposition 7 Solving Trap problem EA population size equals problem size n nEA starting nonoptimal populations Reproduction implemented Mutation1 bitwise mutation constant probability Recombination EFHT bounded Eτ Ω cid15 cid16 2n n3 20 b Reproduction implemented Mutation2 bitwise mutation probability 1n Recombination EFHT bounded Eτ Ω cid15 cid16 2n n3 21 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 c Reproduction implemented Mutation3 onebit mutation Recombination EFHT bounded Eτ Ω cid15 cid16 2n n3 1827 22 n problem size To prove proposition ﬁrst notice recombination operator generate optimal solution population contain solutions SI derives upper bound recombination nonzero success probability Then ﬁnd probability populations contain solutions SI decreasing Thus upper bound formula 12 leads proposition Proof Considering mutation recombination applied independently Selection operation generate new solutions cid4 x X cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid4 x X cid2 ξ M t1 P X ξt x cid3 P ξt x 1 μt cid4 x X cid2 ξ R t1 P X ξt x cid3 P ξt x 1 μt ξ M Let t1 population reproduced ξt1 mutation ξ R t1 recombination X F x X s x s SF X I X X F X cid2 x X I P ξ R X P t1 I X cid17 cid3 ξt x cid18 0 Then t t P ξt1 X I ξt X F 0 cid2 0 ξ R t1 ξt X F X P cid3 considering behavior selection recombination When ξt X F exactly results mutation operators recombination useful When ξt X I mutation operators cid4 x X cid2 ξ M t1 P X ξt x cid3 P ξt x 1 μt cid3 npm1 pmn1 e1 e n2 2n n2 2n Mutation1 Mutation2 Mutation3 For recombination operator cid3 P ξt x 1 μt ξt x cid2 ξ R t1 X cid4 P x X I cid4 x X P I cid4 x X P I cid2 ξ R t1 P X ξt x cid3 P ξt x 1 μt deﬁnition X P I cid2 ξ R t1 P X ξt x cid3 P Φξt Φx1 μt 1 μt optimistic assumption bits solutions x 1 cid4 cid3 cid2 Φξt Φx P P cid2 ξ R t1 cid3 ξt x X cid3 1 cid3 cid3 x X P I cid2 Φξt Φx x X P I Φxx X P I Z P cid3 Lemma 3 Φ Z deﬁned Lemma 3 1828 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 By deﬁnition X P I contain solution leading d bits optimal solution solution tailing n 1 d bits optimal solution Thus I population X P Φxx X P I Z n2cid4 cid15 n cid16cid15cid15 1 d1 1 2 cid16 d1 cid15 cid15 cid15 1 1 1 2 cid16 nd cid16 n cid16cid16 cid3 n cid15cid15 n2cid4 d1 cid16 d1 cid15 n 1 2 1 2 cid16 nd cid16 n2n 2 2n1 Then cid4 x X I cid2 ξ R t1 P X ξt x cid3 P ξt x 1 μt cid3 n2n 2 2n1 Therefore Mutation1 cid4 x X cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid3 npm1 pmn1 n2n 2 2n1 Let β npm1 pmn1 n2n2 2n1 cid15 npm1 pmn1 n2n 2 Eτ 2n1 Ω cid16 2n n3 EFHT obtained cid161 cid15 For Mutation2 cid4 P x X Let β e1 e cid2 ξ C t1 X ξt x cid3 P ξt x 1 μt cid3 e 1 e n2 2n n2n 2 2n1 n2 2n n2n2 2n1 cid15 e 1 e n2 2n EFHT obtained cid15 cid161 n2n 2 2n1 Ω cid16 2n n3 Eτ ξt x cid3 P ξt x 1 μt cid3 n2 2n n2n 2 2n1 EFHT obtained cid161 cid15 n2 2n n2n 2 2n1 Ω cid16 cid2 2n n3 For Mutation3 cid4 cid2 ξ C t1 P X Let β n2 x X 2n n2n2 2n1 cid15 Eτ 44 Time variant mutation cid2 cid3 Eτ Ω θ n Proposition 8 Solving Trap problem EA Reproduction implemented Mutation4 population size 1 1 1EA starting nonoptimal population EFHT bounded 23 θ 1 2 constant n problem size Since model evolution process nonhomogeneous Markov chain easily model timevariant mu tation simply reduce homogeneous Markov chain prove proposition following proof Proposition 1 Proof Applying Mutation4 x cid9x x cid2 cid3 ξt x 05 t d 0 05 t d cid3 k P cid2 ξt1 X Since 05 cid2 ξt1 X P ξt X X cid3 cid2 05 cid3 t d cid3cid2 1 05 t d cid3 n1 cid9H k t d cid2 1 05 cid3 nk Thus Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1829 cid4 x X cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid2 05 cid3 t d cid3cid2 1 05 cid3 n1 cid4 t d cid3cid2 cid2 05 t d 1 05 cid3 051 dn1 relax cid3 n1 t d P ξt x 1 μt x X Eq 1 Therefore βt 051 dn1 Theorem 1 EFHT lower bounded 1 βi 21 d n1 2θ n1 Eτ cid2 β0 cid4 t2 tβt1 t2cid5 i0 Eτ Ωθ n cid2 5 Discussion In previous section proved needs exponential time obtain optimal solution Trap problem variations EAs To arrive proof Trap problem hard solved EA Mutation1 need bound success probability formula 12 In way prove problem exponential size solution space easy problems OneMax problem hard EA Mutation1 This suggests nonadaptive mutation rate suitable problem exponential size solution space cases adaptive mutation rate preferred From proofs EAs Mutations 2 3 Recombination ﬁnd common trick At ﬁrst success probability initial step exponentially small EA goes wrong direction makes success probability lower To disclose trick reinvestigate formula 12 cid4 x X cid2 ξt1 X P ξt x cid3 P ξt x 1 μt cid2 P cid4 x X ξ M t1 s ξ R t1 ξt x cid3 P ξt x 1 μt Selection generate solutions ξ M optimal solution t1 ξ R t1 populations reproduced mutation recombination operators ξt respectively s The formula consists parts The success probability cid2 P ξ M t1 s ξ R t1 cid3 ξt x determined Reproduction operators Once operators EA ﬁxed success probability determined In words algorithm The normalized distribution P ξt x calculated recursive 1μt equation P ξt1 x P ξt x P ξt1 x ξt yP ξt y P ξt1 y ξt xP ξt x cid4 cid4 y X y X Reproduction operators ﬁxed nonrecursive terms P ξt1 x ξt y P ξt1 y ξt x determined solutions favored dominated ﬁtness Considering apply existing EA tackle problem Reproduction operator ﬁxed problem ﬁtness fully depends problem So normalized distribution problem So explanation question makes problem hard EA problem EA run direction causes probability good areas having large success probability decrease bad areas having small success probability increase In words algorithm mismatches problem Motivated recognition raise question large problem class EA contains problem instance solved EA polynomial time The answer question implies general bound effectiveness EAs At ﬁrst glance question related No Free Lunch theorem 26 indicates possible problems considered problem instance equal chance encountered arguable algorithms equal average performance But note No Free Lunch theorem considers algorithm relatively better algorithm algorithm performs poor problem mean algorithms perform poor problem While want know problem class hard EAs EA problem class contains hard problem instance We ﬁnd EAs exist problem instance hard exponential size general problem 1830 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 Deﬁnition 6 General problem A general problem solution space S solution space size S ﬁnite set problem instances function bijective function cluster F S 1 2 S corresponds ﬁtness function problem instance In words general problem contains S number problem instances permutation solutions Thus point solution space optimal solution instance general problem We denote expn exponential order n polyn polynomial order n omitting exact components order Theorem 2 Given general problem solution space size smaller expn b EA population size larger polyn c solution equal probability appear initial population exists problem instance EFHT EA smaller expn polyn n problem size Proof Denote Reprodξt solution set generated Reproduction operators size polyn costs expn time Considering S expn population size polyn population state space X expnpolyn We cid2 cid3 s Reprodξt ξt x P cid3 cid30 cid30 cid30 cid3 polyn cid30Reprodξt cid4 sS cid4 cid4 sS x X s S cid2 cid3 s Reprodξt ξt x P cid3 polyn expnpolyn cid4 x X cid2 cid3 s Reprodξt ξt x P cid3 polyn expnpolyn 1 expn sum S exceed polyn expnpolyn Let s denote optimal solution s At time t 0 cid4 x X cid2 ξ1 X P ξ0 x cid3 P ξ0 x 1 μ0 cid2 P s cid4 x X Reprodξ0 ξ0 x cid3 P ξ0 x 1 μ0 selection generate new solutions cid4 cid3 Reprodξ0 ξ0 x P cid2 s assumption μ0 0 1 expnpolyn x X cid3 polyn expnpolyn 1 expn 1 expnpolyn polyn expn At time t 1 sort nonoptimal population states sequence xi xi X cid2 ξt1 X P cid3 ξt xi cid3 P cid2 ξt1 X ξt xi1 cid3 Afterwards cid4 x X cid2 ξt1 X cid3 ξt x P cid4 x X cid2 cid3 Reprodξt ξt x s P cid3 polyn expnpolyn expn ˆx X cid2 ξt1 X cid3 ξt ˆx P cid3 polyn expn sum X exceed polyn expnpolyn expn Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 1831 Therefore cid2 ξ R t P x ξt x0 cid3 cid2 ξ R t cid3 P x ξt ˆx cid3 cid3 polyn expn considering x0 lowest success probability Now choose ﬁtness function f f x f x0 f xi f xi1 Since solutions larger ﬁtness values higher probability survive selection k P ξt1 x0 xk P ξt1 X x0 xk x P ξt1 x0 xk 1 μt1 k cid2 P ξt x0 xk 1 μt P ξt x0 xk P ξt X x0 xk x Then cid4 x X cid3 cid2 ξt1 X P ξt x cid2 ξ1 X P cid4 x X ξ0 x cid3 P ξt x 1 μt cid3 P ξ0 x 1 μt cid3 polyn expn makes Eτ cid2 expn polyn Theorem 1 cid2 6 Conclusion This paper extends preliminary research 27 We establish bridge important theoretical issues evolutionary algorithms EAs expected ﬁrst hitting time EFHT convergence rate With bridge propose new approach analyzing EFHT EAs The proposed approach bases nonhomogeneous Markov chains suitable analyzing broad range EAs Using proposed approach proved problem hard solved exponential time EAs settings including static mutation operators withwithout population recombination operator timevariant mutation operator It noteworthy timevariant operator hard analyze proposed approach naturally useful situation We gave explanation question makes problem hard EA algorithm problem mismatch EAs usually considered general optimization approaches words problem independent Thus parameters EA ﬁxed EA run wrong direction problems makes problems hard EA Based recognition proved general problem hard exponentially large state space In future intend extend approach optimization problems realvalued functions Acknowledgements We want thank anonymous reviewers helpful comments suggestions want thank Tianshi Chen XiangNan Kong Chao Qian DeChuan Zhan proofreading paper We want thank editor Raymond Perrault generous help polishing ﬁnal version paper This research supported National Science Foundation China 60635030 60721002 References 1 T Bäck Evolutionary Algorithms Theory Practice Evolution Strategies Evolutionary Programming Genetic Algorithms Oxford University Press Oxford UK 1996 2 HG Beyer HP Schwefel I Wegener How analyse evolutionary algorithms Theoretical Computer Science 287 1 2002 101130 3 SJ Chang HS Hou YK Su Automated passive ﬁlter synthesis novel tree representation genetic programming IEEE Transactions Evolutionary Computation 10 1 2006 93100 4 YC Chang SM Chen A new query reweighting method document retrieval based genetic algorithms IEEE Transactions Evolutionary Computation 10 5 2006 617622 5 S Droste T Jansen I Wegener On optimization unimodal functions 1 1 evolutionary algorithm Proceedings 5th International Conference Parallel Problem Solving Nature PPSN V Amsterdam Netherlands 1998 6 S Droste T Jansen I Wegener A rigorous complexity analysis 1 1 evolutionary algorithm linear functions boolean inputs Evolu tionary Computation 6 2 1998 185196 7 S Droste T Jansen I Wegener On analysis 1 1 evolutionary algorithm Theoretical Computer Science 276 12 2002 5181 1832 Y Yu ZH Zhou Artiﬁcial Intelligence 172 2008 18091832 8 AE Eiben R Hinterding Z Michalewicz Parameter control evolutionary algorithms IEEE Transactions Evolutionary Computation 3 2 1999 124141 9 AA Freitas A survey evolutionary algorithms data mining knowledge discovery A Ghosh S Tsutsui Eds Advances Evolutionary Computing Theory Applications SpringerVerlag New York NY 2003 pp 819845 10 J Garnier L Kallel M Schoenauer Rigorous hitting times binary mutations Evolutionary Computation 7 2 1999 173203 11 B Hajek Hitting time occupation time bounds implied drift analysis applications Advances Applied Probability 14 1982 502525 12 J He L Kang On convergence rates genetic algorithms Theoretical Computer Science 229 12 1999 2339 13 J He X Yao Drift analysis average time complexity evolutionary algorithms Artiﬁcial Intelligence 127 1 2001 5785 14 J He X Yao Towards analytic framework analysing computation time evolutionary algorithms Artiﬁcial Intelligence 145 12 2003 5997 15 J He X Yao A study drift analysis estimating computation time evolutionary algorithms Natural Computing 3 1 2004 2135 16 J He X Yu Conditions convergence evolutionary algorithms Journal Systems Architecture 47 7 2001 601612 17 PCH Ma KCC Chan X Yao DKY Chiu An evolutionary clustering algorithm gene expression microarray data analysis IEEE Transactions Evolutionary Computation 10 3 2006 296314 18 AE Nix MD Vose Modeling genetic algorithms Markov chains Annals Mathematics Artiﬁcial Intelligence 5 1 1992 7788 19 PS Oliveto J He X Yao Time complexity evolutionary algorithms combinatorial optimization A decade results International Journal Automation Computing 4 3 2007 281293 20 JS Rosenthal Minorization conditions convergence rates Markov chain Monte Carlo Journal American Statistical Association 90 430 1995 558566 21 G Rudolph Convergence analysis canonical genetic algorithms IEEE Transactions Neural Networks 5 1 1994 96101 22 G Rudolph How mutation selection solve long path problems polynomial expected time Evolutionary Computation 4 2 1996 195205 23 G Rudolph Convergence Properties Evolutionary Algorithms Verlag Dr Kovaˇc Hamburg Germany 1997 24 G Rudolph Finite Markov chain results evolutionary computation A tour dhorizon Fundamenta Informaticae 35 14 1998 6789 25 J Suzuki A Markov chain analysis simple genetic algorithms IEEE Transactions Systems Man Cybernetics 25 4 1995 655659 26 D Wolpert WG Macready No free lunch theorems optimization IEEE Transactions Evolutionary Computation 1 1 1997 6782 27 Y Yu ZH Zhou A new approach estimating expected ﬁrst hitting time evolutionary algorithms Proceeding 21st National Confer ence Artiﬁcial Intelligence Boston WA 2006 pp 555560