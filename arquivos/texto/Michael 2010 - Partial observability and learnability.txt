Artiﬁcial Intelligence 174 2010 639669 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Partial observability learnability Loizos Michael Open University Cyprus Cyprus r t c l e n f o b s t r c t Article history Received 11 November 2007 Received revised form 30 March 2010 Accepted 30 March 2010 Available online 1 April 2010 Keywords Partial observability Appearance Reality Masking process Sensing Missing information Autodidactic learning Probably approximately correct Information recovery Reduction When sensing environment agent receives information partially describes current state affairs The agent attempts predict sensed pieces information available sensors Machine learning techniques naturally aid task providing agent rules making predictions For happen learning algorithms need developed deal missing information learning examples principled manner need external supervision We investigate problem We Probably Approximately Correct semantics extended deal missing information learning evaluation phase Learning examples drawn underlying probability distribution parts hidden passed learner The goal learn rules accurately recover information hidden learning examples We ﬁrst dispense requirement rules deﬁnite predictions dont know necessitated On hand abstentions freely suﬃcient information present deﬁnite predictions Under premise accurately recover missing information suﬃces learn rules highly consistent rules simply contradict agents sensory inputs It established high consistency implies somewhat discounted accuracy discount deﬁned sense unavoidable depends adversarially information hidden learning examples Within proposed learning model prove PAC learnable class monotone readonce formulas learnable incomplete learning examples By contrast prove parities monotoneterm 1decision lists properly PAC learnable properly learnable new learning model In process establishing positive negative results rederive basic PAC learnability machinery Occams Razor reductions learning tasks We ﬁnally consider special case learning partial learning examples prior bias exists manner information hidden provides uniﬁed view previous learning models deal missing information proposed learning model goes simple extension We suggest supervised learning case incomplete learning examples The principled general treatment missing information learning argue allows agent employ learning entirely autonomously relying presence external teacher case supervised learning We learning model autodidactic emphasize explicit disassociation model form external supervision 2010 Elsevier BV All rights reserved A preliminary version work appeared Loizos Michael Learning partial observations Manuela M Veloso Ed Proceedings Twentieth International Joint Conference Artiﬁcial Intelligence IJCAI07 January 2007 pp 968974 This work completed author School Engineering Applied Sciences Harvard University Cambridge MA 02138 USA supported grant NSFCCF0427129 Email address loizosoucaccy 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201003004 640 L Michael Artiﬁcial Intelligence 174 2010 639669 1 Introduction It argued central aspect fully autonomous agent ability learn rules govern envi ronment form external supervision An autonomous agent senses environment obtains information incomplete serves input learning process Such settings necessitate use learning algorithms deal incomplete learning examples In work propose framework learning incomplete learning examples formally studied For concreteness framework viewed extension Probably Approximately Correct semantics 28 Our goal possible learn rules accurately predict information missing agents sensory readings rules obtained eﬃciently accompanied formal PAClike guarantees irrespectively information hidden learning examples available learning evaluation phases We note point work problem learning incomplete learning examples goes learning classiﬁcation rules original PAC model We view results work ﬁrst step ambitious goal devising learning algorithms identify general rules Our exposition starts Section 2 problem learning incomplete information context problem underlying process scientiﬁc discovery identifying structure underlying reality given partial appearances reality We continue PAC semantics extended effect As PAC model learning examples drawn independently random underlying probability distribu tion Unlike PAC model examples directly accessible agent Instead arbitrary stochastic process hides parts examples giving rise partial observations These observations given agent learning phase means facilitate learning evaluation phase input learned rules applied predictions predictions tested Due lack complete information evaluation phase allow learned rules dont know pre dictions rules unambiguously evaluated given observation Under provision deﬁne rule consistent observation rules prediction directly contradict stated observation In particular observation offer information target attribute pre diction consistent Learning successful highly consistent rules obtained eﬃciently relevant learning parameters We consider stronger notion learnability deriving rules predictions manner consistent observation accurate underlying example Thus observation offer information target attribute prediction accurate depending hidden underlying value target attribute We stringent notion learnability informationtheoretically unattainable information hidden adversarially observations We introduce metric called concealment capture extent adversity consistency accuracy concealment tied natural manner consistency implies accuracy discounted factor determined conceal ment This allows focus conceptually simpler notion consistent learnability remaining work Section 3 discusses choices learning model contrasts existing work Statistical Analysis Learning Theory Three main aspects discussed dont know predictions allowed mean predict dont know ii extent autonomy possible learning iii regularity assumed way information missing learning examples This discussion shows particular unlike previous work learning framework assumption external teacher We learning framework autodidactic recognition property The subsequent sections provide positive negative learnability results autodidactic learnability Section 4 establishes certain machinery available PAC model applies form context autodidactic learnability In particular Occams Razor 4 applies unchanged PAC learnability reductions learning tasks 23 formalized way accommodates stringent requirements need met auto didactic learnability Using reductions establish PAC learnable concept class contains monotone readonce formulas learned autodidactically Hence broad set domains lack complete infor mation render learnability harder By contrast Section 5 establishes incomplete information cases diminish learnability We properly PAC learnable concept classes parities monotoneterm 1decision lists properly learnable autodidactic model RP NP The case information hidden completely arbitrarily learning examples examined Section 6 We argue demonstrate depending structured information hiding semantics missing information signiﬁcantly altered extent missing information learnability easier case complete information Related learning models presented assumptions structure We conclude Section 7 list open problems pointers future work L Michael Artiﬁcial Intelligence 174 2010 639669 641 2 Autodidactic learnability The dichotomy appearance reality inherent process scientiﬁc discovery Appearances partial depictions reality governs world appearances scientists attempt derive model hypothesis structure present underlying reality The hypothesis applied appearances predictions unobserved properties world In physics instance predictions concern spatially temporally distant properties world readings obtained sensors Central process certain premises Structure exists underlying reality environment necessarily way sensors hide informa tion reality rise appearances ii This underlying structure learned remains perpetually inaccessible sensing iii Any attempt discover structure rely solely partial information structure provided sensors external supervision learning phase iv Developed hypotheses aim model structure underlying reality necessarily way sensors hide information reality v A hypothesis underlying structure applied predict missing information present appearances given partial information available appearances Machine learning research largely ignored premises In words McCarthy 18 Our senses partial information objects present vicinity In particular vision gives 2dimensional view 3dimensional objects Our visual systems brains use sense information learn 3dimensional objects Also humans dogs represent objects presently visible The evidence dogs thrown ball goes sight dog look Humans infer existence objects sight human learning experience involves learning hidden reality phenomenon This science usually occurs common sense reasoning Machine learning research knowledge far involved classifying appearances involved inferring reality experience Classifying experience inadequate model human learning inadequate robotic applications Another way looking use observations recognize patterns world patterns observations We propose learning model makes explicit dichotomy appearance reality respects premises set forth We argue model goes simply able cope missing information learning examples Instead shows despite use supervised learning techniques possible learning carried entirely autonomously case scientiﬁc discovery supervision external teacher We new learning model autodidactic recognition fact We discuss conceptual merits autodidactic learning work 21 Learning partial observations In PAC learning model 28 agent given access learning examples randomly drawn arbitrary ﬁxed probability distribution D binary vectors exm 0 1A ﬁxed set binary attributes A The examples structured sense value designated target attribute xt determined unknown ﬁxed function ϕ C remaining attributes A xt function ϕ known target concept class C possible target concepts known concept class During initial learning phase agent expected eﬃciently produce hypothesis function h H highly accurate respect D sense predicts high probability value target attribute xt evaluation examples drawn D given access values remaining attributes A xt class H possible hypotheses known hypothesis class Implicit deﬁnition PAC learning model premise agent access complete information values attributes Each example contains suﬃcient information determine value target attribute xt primary challenge learning task forming hypothesis determine value target attribute given values remaining attributes In realistic domains agent burdened additional challenge information necessary determine value target attribute missing examples The agent access partial depictions learning examples case learning evaluation phase These partial depictions learning examples shall observations Although general case observations noisy respect learning examples shall consider scenario work henceforth assume observations incomplete Observations ternary vectors obs 0 1 A value indicating corresponding attribute observed dont know The mapping examples observations happens masking process stochastic process mask 0 1A 0 1 A aimed model agents sensors The masking process mask induces probability distribution maskexm observations depend example exm write obs maskexm denote 642 L Michael Artiﬁcial Intelligence 174 2010 639669 observation obs drawn probability distribution nonzero probability The noiseless nature sensing amounts insisting obs maskexm attribute xi A obsi exmi obsi exmi correspond respectively value ith attribute according obs exm An observation obs image example exm masking process said mask exm Each attribute xi A observation obs obsi said masked obs Masking processes general manytomany mappings examples observations Given instance examples exm1 0010110 exm2 0100100 observations obs1 0 10 0 obs2 10 00 obs3 0 01 masking process mask following input exm1 returns obs1 probability 03 obs3 probability 07 input exm2 returns obs2 probability 06 obs3 probability 04 The oneto nature masking processes intended capture stochastic nature sensing An agent attempting sense reality twice exm1 end different appearances obs1 obs3 On hand manytoone nature intended capture loss information agents limited sensing abilities Two distinct realities exm1 exm2 appear obs3 agent indication given obtained appearance reality actually sensed The loss information masking happens learning evaluation phase Thus agent directly observes learning examples access observations mask learning examples Yet PAC model agent expected produce hypothesis predicting value target attribute We emphasize hypothesis function boolean attributes case PAC model In sense agent trying encode hypothesis knowledge structure underlying examples knowledge structure observations way masking process hides information Indeed central premise work information observations hidden arbitrary manner The central question structure underlying examples learned PAC sense given arbitrarily selected partial information The PAC model viewed special case model observations contain values To formalize way structure present examples requirement value target attribute determined function remaining attributes way predictions hypothesis function follow PAC model employ boolean formulas syntactic objects set attributes A associated typical semantics evaluating given complete assignment values attributes Given formula ϕ example exm write valϕ exm denote value ϕ exm Unlike PAC semantics necessary deﬁne value valϕ obs formula ϕ observation obs general case agent predictions evaluating learned hypothesis partial observation Note possible valϕ obs value obs offer 0 1 values attributes ϕ On hand valϕ obs remains undetermined missing information deﬁne valϕ obs equal indicate dont know value ϕ obs Note evaluating formula observation necessarily eﬃciently formula eﬃciently evaluatable example Indeed evaluating 3CNF formulas observation attributes masked hard deciding 3CNF formulas satisfying assignment NPcomplete problem 8 Most deﬁnitions results later state rely actually evaluating formulas eﬃciently conditioned formulas involved eﬃciently evaluatable When formulas need eﬃciently evaluatable result hold stated explicitly conditions result It remains open formulas eﬃciently evaluatable observations examples learned sense deﬁned later We proceed deﬁne particular type structure encoded examples Deﬁnition 21 Supported concept classes A target attribute xt A expressed formula ϕ A xt wrt proba bility distribution D cid2 Pr valϕ exm exmt exm D cid3 1 A probability distribution D supports concept class C formulas A xt target attribute xt A exists formula c C xt expressed c wrt D c target concept xt D We view values attributes drawn probability distribution D We regard approach corresponding closely conceptually happens certain domains approach typically taken supervised learning models The attributes encode state affairs priori distinguished target nontarget attributes equivalent nature captured probability distribution D assigns value attributes An agents sensors mask attributes distinguishing The distinction target attribute assumption attribute correlated rest attributes serve premises particular type learning task correlation imposed appropriately restricting D Indeed view possible easily generalize Deﬁnition 21 encode types correlation attributes like instance number attributes A assigned value 1 example exm D divisible 3 probability 095 Although ﬁnd types correlation intriguing meriting investigation focus work type correlation stated Deﬁnition 21 L Michael Artiﬁcial Intelligence 174 2010 639669 643 To complete description learning model need deﬁne prediction formula consistent respect observation In words wish determine sources information available agent sensor readings conclusions draws learned rules agree We deﬁne formula ϕ consistency conﬂict target attribute xt wrt observation obs valϕ obs obst 0 1 value formula observed value target attribute 0 1 differ In cases suggested 0 1 values agree suggested values dont know We extend notion consistency apply probability distribution observations This probability distribution denote maskD indicate induced probability distribution D examples drawn masking process mask examples mapped observations Deﬁnition 22 Degree consistency A formula ϕ A xt 1 εconsistent target attribute xt A probability distribution D masking process mask cid2cid4 cid5 valϕ obs obst cid3 0 1 exm D obs maskexm Pr cid2 ε We state formally learning requirements autodidactic learning model consider In follows denote learning task A triple cid6xt C Hcid7 xt target attribute A C concept class formulas A xt H hypothesis class formulas A xt Deﬁnition 23 Consistent learnability An algorithm L consistent learner learning task cid6xt C Hcid7 A probability distribution D supporting C xt masking process mask real number δ 0 1 real number ε 0 1 algorithm L following property given access A cid6xt C Hcid7 δ ε oracle returning observations drawn maskD algorithm L runs time polynomial 1δ 1ε A size target concept xt D returns probability 1 δ hypothesis h H 1 εconsistent xt D mask The concept class C A xt consistently learnable target attribute xt A hypothesis class H A xt exists consistent learner cid6xt C Hcid7 A The deﬁnition consistent learnability follows closely PAC semantics added requirement learnability succeeds arbitrary masking process mask mask identity mapping case PAC semantics Although added requirement ﬁrst arduous recall exactly situations learnability harder missing information formulas dont know predictions freely avoiding consistency conﬂicts We emphasize dont know predictions abused learner produce hypothesis arbitrarily chooses abstain making predictions It masking process gives formula ability dont know predictions control learner We later contrast approach models learning hypotheses actively choose abstain making predictions In models required introduce second metric measuring success hypothesis degree completeness probability 0 1 prediction 22 Are accurate predictions possible With complete proposal learning model revisit original motivation developing model learning partial observations recover missing information incomplete sensory inputs agent Does deﬁnition consistent learnability address goal Recall highly consistent formula guaranteed predictions consistent randomly drawn observations In particular cases problem missing information interesting target attribute masked observation consistency guarantee offer essentially prediction consistent observation What need notion predictive correctness respect observations respect underlying examples agent wishes able match unobserved reality appearances environment We deﬁne formula ϕ accuracy conﬂict target attribute xt wrt observation obs obtained example exm valϕ obs exmt 0 1 value formula actual value target attribute 0 1 differ In cases suggested 0 1 values agree value formula dont know As case consistency extend notion accuracy apply probability distribution observations Deﬁnition 24 Degree accuracy A formula ϕ A xt 1 εaccurate wrt target attribute xt A probability distribution D masking process mask cid2cid4 cid5 valϕ obs exmt cid3 0 1 exm D obs maskexm Pr cid2 ε It natural ask deﬁnition learnability revised highly accurate instead highly consistent hypotheses returned A naive revision lead vacuous deﬁnition learnability trivially 644 L Michael Artiﬁcial Intelligence 174 2010 639669 unattainable Indeed masking process target attribute target attribute masked observations clearly learning algorithm access information value target attribute Yet formula remaining attributes forced 0 1 prediction It impossible determine formulas higher degree accuracy formulas 0 1 predictions feedback provided formulas makes correct prediction This compromises learnability domains concept hypothesis classes contain formulas Theorem 21 Statistical indistinguishability adversarial settings Consider target attribute xt A class F formulas A xt formulas ϕ1 ϕ2 F ϕ1 ϕ2 ϕ2 For real number ε 0 1 exist probability distributions D1 D2 masking process mask0 ϕ1 1accurate ϕ2 1 εaccurate wrt xt D1 mask0 ii ϕ1 1 εaccurate ϕ2 1accurate wrt xt D2 mask0 iii mask0D1 mask0D2 attribute A xt masked drawn observation Proof Let S set truthassignments attributes A xt ϕ1 ϕ2 assigned different truthvalues Fix probability distribution D truthassignments attributes A xt assigns probability ε set S probability 1 ε complement S ϕ1 ϕ2 ϕ2 S complement nonempty For 1 2 extend D probability distribution Di examples 0 1A completing truth assignments attributes A xt assign induced truthvalue ϕi target attribute xt Choose mask0 masking process maps example exm observation obs xt masked valϕ1 exm cid8 valϕ2 exm attribute A xt masked By construction mask0 example exm observation obs mask0exm holds xt masked obs valϕ1 obs cid8 valϕ2 obs By construction D1 D2 mask0 conditions claim follow cid2 It evident masking processes arbitrarily hide information completely ignoring extent information hidden prevent attaining meaningful useful notion learnability Given moments thought natural conclusion Structure exists examples learner attempts learn structure given access observations Thus learnability possible masking process allows structure underlying examples carry observations In words expect observations occasionally provide feedback according structure underlying examples candidate hypothesis highly accurate The extent feedback provided depends masking process quantiﬁed Feedback necessary candidate hypothesis errs accuracy conﬂict target attribute Recall agent necessarily aware accuracy conﬂict By way illustration target attribute x3 masked observation 10 1010 ϕ accuracy conﬂict observation depending examples 1001010 1011010 observation obtained agent oblivious choice example existence accuracy conﬂict In case accuracy conﬂicts expect probability value target attribute known agent conﬂict detected That expect particular reality indeﬁnitely sensed agent agent realizing making wrong prediction Deﬁnition 25 Degree concealment A masking process mask 1 ηconcealing target attribute xt A wrt class F formulas A xt η 0 1 minimum value cid5 valϕ obs exmt obst cid8 obs maskexm 0 1 Pr cid4 cid3 cid2 choices example exm 0 1A formula ϕ F 1 The concealment degree Deﬁnition 25 worstcase bound possible examples possible formulas making predictions Each pair formula ϕ example exm imposes constraint η implies lower bound concealment degree 1 η masking process mask Note probability distribution D assign zero probability examples bring adversarial nature masking process making look adversarial worst case Note concealment degree masking process vary arbitrarily target attributes As illustration consider particular domain target attribute x6 formulas ϕ x2 x4 x7 denotes exclusive binary operator examples exm 0110011 The masking process 1 A conditional probability undeﬁned event condition occurs probability 0 In cases deﬁne conditional probability equal 1 In context Deﬁnition 25 choice implies cases formula accuracy conﬂict target attribute wrt observations obtained particular example safely ignored cases constrain concealment degree masking process way L Michael Artiﬁcial Intelligence 174 2010 639669 645 Table 1 Observations obtained example exm 0110011 applying particular masking process mask corresponding predictions formula ϕ x2 x4 x7 target attribute x6 obs maskexm Prediction x6 observation 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 probability valϕ obs accuracy conﬂict 027 015 033 004 021 0 0 0 yes yes yes mask exm mapped observations shown Table 1 According mask observations rise accuracy conﬂicts drawn total probability 075 Among observations target attribute x6 masked drawn total probability 042 By law conditional probabilities follows cid2 Pr obst cid8 obs maskexm cid4 cid5 valϕ obs exmt 0 1 cid3 042 075 056 Deﬁnition 25 implies η cid2 056 masking process mask 044concealing Additional formulas examples impose extra bounds η turn increase concealment degree mask If extra bounds η smaller 056 deﬁnition η imply masking process mask exactly 044concealing particular target attribute x6 23 Going consistency accuracy Given crisp metric degree feedback masking process provides agent easy negative learnability result Theorem 21 holds precisely appeals 1concealing masking process As learning models parameterized resource renders learnability impossible parameter riches relevant limit case random classiﬁcation noise 1 learnability impossible noise rate 12 extend deﬁnition learnability allow resources grow inversely distance parameter limit In case concealment degree limit 1 η deﬁnes distance concealment degree 1 ηconcealing masking process mask limit Hence revise deﬁnition learnability highly accurate instead highly consistent hypotheses returned time allow additional resources grow 1η account adversarial nature mask hide information A learner expected return 1 εaccurate hypothesis exploit additional resources order obtain 1 η εconsistent hypothesis appeal following result establish hypothesis fact 1 εaccurate The proof result builds natural realization prediction consistent accurate target attribute masked Informally settheoretic terms holds consistency accuracy concealment Theorem 22 The relation consistency accuracy Consider target attribute xt A class F formulas A xt For real number η 0 1 masking process mask 1 ηconcealing xt wrt F following conditions hold probability distribution D formula ϕ F holds ϕ 1 εaccurate wrt xt D mask real number ε 0 1 ϕ 1 η εconsistent xt D mask η cid8 0 ii exists probability distribution D0 formula ϕ0 F ϕ0 1 εaccurate wrt xt D0 mask real number ε 0 1 ϕ0 1 η εconsistent xt D0 mask Proof For formula ϕ F example exm 0 1A masks exm probability distribution D denote Edrexm obs D event exm D obs maskexm Eccϕ obs event ϕ consistency conﬂict xt wrt obs Eacϕ exm obs event ϕ accuracy conﬂict xt wrt obs obtained exm Enmobs event xt masked obs Clearly event Eccϕ obs holds exactly events Eacϕ exm obs Enmobs hold simultaneously In particular true exm obs restricted event Edrexm obs D true Thus cid2 cid3 Eacϕ exm obs Enmobs Edrexm obs D cid3 Eccϕ obs Edrexm obs D observation obs 0 1 A Pr Pr cid2 From law conditional probabilities right hand equation equals cid2 cid3 Enmobs Edrexm obs D Eacϕ exm obs Pr cid2 cid3 Eacϕ exm obs Edrexm obs D Pr We proceed derive bounds ﬁrst term product In ﬁrst direction Deﬁnition 25 implies example exm 0 1A formula ϕ F holds 646 cid2 Pr obst cid8 obs maskexm cid4 cid5 valϕ obs exmt 0 1 cid3 cid3 η L Michael Artiﬁcial Intelligence 174 2010 639669 Now exm drawn given probability distribution D overall probability obst cid8 given valϕ obs exmt 0 1 remains lower bounded η Thus cid2 cid3 Enmobs Edrexm obs D Eacϕ exm obs Pr cid3 η In direction Deﬁnition 25 implies exists example exm0 0 1A cid4 cid3 cid2 cid5 valϕ0 obs exm0t 0 1 cid2 η Pr obst cid8 obs maskexm0 formula ϕ0 F 1 Now exm0 replaced example exm drawn probability distribution D0 deﬁned assigns probability 1 exm0 drawn probability obst cid8 given valϕ0 obs exmt 0 1 remains upper bounded η Thus cid2 cid3 Enmobs Edrexm obs D0 Eacϕ0 exm obs Pr cid2 η 2 Finally proceed establish conditions claim hold For Condition ﬁx arbitrary probability distribution D arbitrary formula ϕ F assume ϕ 1 η εconsistent xt D mask real number ε 0 1 η cid8 0 Then PrEccϕ obs Edrexm obs D cid2 η ε equivalently cid2 cid3 Enmobs Edrexm obs D Eacϕ exm obs Pr cid2 cid3 Eacϕ exm obs Edrexm obs D Pr cid2 η ε Since η cid8 0 Inequality 1 immediately implies PrEacϕ exm obs Edrexm obs D cid2 ε Therefore ϕ 1 ε accurate wrt xt D mask For Condition ii consider probability distribution D0 formula ϕ0 F deﬁned context Inequality 2 assume ϕ0 1 εaccurate wrt xt D0 mask real number ε 0 1 Then PrEacϕ0 exm obs Edrexm obs D0 cid2 ε Inequality 2 immediately implies cid2 cid3 Enmobs Edrexm obs D0 Eacϕ0 exm obs Pr cid2 cid3 Eacϕ0 exm obs Edrexm obs D0 Pr cid2 η ε equivalently PrEccϕ0 obs Edrexm obs D0 cid2 η ε Therefore ϕ0 1 η εconsistent xt D0 mask The claim follows cid2 Condition Theorem 22 provides formal implication highly consistent hypotheses highly accurate ones There caveat implication The degree accuracy predictions necessarily high degree consistency Given moments thought makes perfect sense The requirement making accurate predictions stronger compared making consistent predictions In cases target attribute masked accuracy conﬂicts consistency conﬂicts equivalent cases target attribute masked consistency conﬂicts occur accuracy conﬂicts possible What intriguing fact extent degree accuracy diminishes respect degree consistency depends degree concealment masking process Assuming sensors physical world adversarially hide information interpret result corroborating approach humans follow consistent theories recovering missing information appearances rational strategy The dependence accuracy concealment degree explains possible certain cases implica tion consistency accuracy violated This happens exactly cases concealment degree high η close zero Condition ii Theorem 22 establishes exist domains certain formulas high degree consistency low degree accuracy At time Condition ii suggests use consistent hypotheses worst case optimal strategy recovering missing information certain domains bound degree accuracy Condition guarantees highly consistent formula tight Furthermore optimality guaranteed knowledge concealment degree later discuss Section 32 hard impossible determine simple masking processes Obtaining highly accurate hypotheses highly consistent hypotheses cf Deﬁnition 23 valid op timal worst case approach Still direct approach learning highly accurate hypotheses instead The answer simple Consistency natural notion work avoids complications arising having deal degree concealment masking process More importantly formulas degree sistency reliably empirically estimated following simple result shows degree accuracy cf Theorem 21 require access value target attribute target attribute masked observations Deﬁnition 26 Degree consistency sample version A formula ϕ A xt 1 εconsistent target attribute xt A given sample O observations ϕ consistency conﬂict xt wrt ε fraction observa tions O L Michael Artiﬁcial Intelligence 174 2010 639669 647 Theorem 23 Empirical estimability consistency degree Consider target attribute xt A formula ϕ A xt probability distribution D masking process mask sample O observations drawn independently maskD For pair real numbers ε γ 0 1 ϕ 1 εconsistent xt given O probability 1 e holds ϕ 1 ε γ consistent xt D mask 2Oγ 2 cid6O cid6O Proof For observation obsi O let random variable Xi indicator variable event Eccϕ obsi ϕ consistency conﬂict xt wrt obsi construction O random variables independent Deﬁne random variable X cid4 O1 i1 Xi mean random variables By linearity expectations E X mean O1 i1 E Xi expectations random variables By standard Hoeffding concentration bounds 12 Clearly X fraction observations O wrt ϕ probability X E X γ e consistency conﬂict xt X cid2 ε By deﬁnition random variables E X E X1 PrEccϕ obs1 2Oγ 2 ϕ 1 E Xconsistent xt D mask Since X cid2 ε follows probability 1 e E X cid2 ε γ needed cid2 2Oγ 2 Whether formulas degree consistency reliably empirically estimated eﬃcient manner orthogonal issue depends formula evaluated eﬃciently partial observations 3 Discussion related work The problem missing information learning settings recognized early literature Valiant 28 paper introduced PAC learning fact considered form learning partial observations Various frameworks developed offered solutions related problems Within Learning Theory commu nity extensions PAC model proposed deal varying degrees problem missing information Within broader Machine Learning community problem dealing missing information received signiﬁcant attention especially devising practical solutions realworld settings Other communities area Artiﬁcial Intelligence Computer Science large offered solutions problems related manipulation incomplete data Fields outside Computer Science dealt problem especially area Statistical Analysis It scope work survey problems examined solutions offered In section focus discussing work closely related extensions PAC model deal missing information In sequel defend certain modelling choices framework We contrast degree concealment measure degree missing information standard metrics Statistical Analysis literature We ﬁnally consider related PAC learning frameworks discuss relate autodidactic learning We identify dimensions frameworks compared contrasted autodidactic learning semantics dont know predictions ii degree supervision learning iii regularity information hidden observations 31 Are dont know predictions justiﬁed Our choices dont know predictions considered justiﬁed predictions accounted measuring formulas degree accuracy cf Deﬁnition 24 raise certain objections We discuss classes objections identiﬁed A ﬁrst objection relates choice dont know predictions allowed Recall formula ϕ predicts observation obs0 valϕ obs0 undetermined It argued dont know prediction justiﬁed instance ϕ evaluates 1 vast majority valϕ obs0 undeﬁned examples masked obs0 reasonable deﬁne value ϕ obs0 1 case Our answer Just majority possible underlying examples exm observation obs0 valϕ exm 1 follow examples drawn high nonzero probability underlying probability distribution D That way exclude eventuality agents environment supply agent examples exm valϕ exm 0 completely undermine reason choosing deﬁne value ϕ obs0 1 Thus determining value formula obs0 simply counting number examples masked obs0 exhibit certain property meaningful Following argument reﬁned version objection raised Consider probability distribution D masking process mask cid2 Pr valϕ exm 1 exm D obs maskexm obs obs0 cid3 cid3 0999 cases particular observation obs0 drawn formula ϕ evaluates 1 underlying example overwhelming probability This situation illustrated toy domain observing birds observing birds penguins In domain underlying probability distribution D taken 648 L Michael Artiﬁcial Intelligence 174 2010 639669 observed birds penguins ability ﬂy The question reasonable deﬁne value ϕ formula employ predictions ability observed birds ﬂy observation obs0 bird Tweety 1 Our answer remains There exists theoretical justiﬁcation probability 0999 probability matter high prediction 1 dont know prediction It easy devise scenarios preferable agent predict dont know aware lack certainty predicting 0 1 value risking wrong prediction knowing happening In scenarios preferable dont know prediction lack certainty recorded dont know prediction replaced 0 1 value agents deliberation mechanism case value believed likely true Suppose subscribe view practical purposes probability 075 appropriate ϕ predict 1 probability 025 appropriate ϕ predict 0 ϕ predict dont know remaining cases That suppose domains preferable risk making wrong prediction small probability making dont know prediction This setting meaningful An agent access underlying probability distribution D examples drawn problem determining probability question given obs0 generally impossible That given valϕ obs0 possible estimate risk making wrong prediction argument favor choosing 0 1 prediction dont know prediction suggested valϕ obs0 A second objection relates choice measure degree accuracy Recall degree accuracy formula ϕ probability predicts correct underlying value target attribute predicts dont know It argued degree accuracy computed respect 0 1 predictions formula ignoring dont know predictions degree accuracy deﬁned percentage correct predictions 0 1 predictions After natural account percentage dont know predictions introducing second metric degree completeness formula captures probability 0 1 predictions We agree alternative approach appeal However opted follow main reasons The degree completeness 1 ω formula degree accuracy 1 ε proposed deﬁnition alternative deﬁnition discussed trivially derived degree accuracy 1 εcid13 εcid13 ε1 ω ii While alternative degree accuracy meaningful associated degree complete ness proposed degree accuracy encompasses metrics building existence natural degree completeness formula follows fact formulas choose abstain making 0 1 predictions predict suﬃcient information missing dont know prediction justiﬁed cf ﬁrst objection A objection relates choice requirements learnability Recall require returned hypothesis highly accurate It argued formulas achieve degree accuracy prefer formulas higher degree completeness cf second objection punishing formulas predict dont know For instance consider case target concept parity ϕ depends strict subset attributes A assume masking process exactly attribute masked observation Clearly parity ϕ0 attributes A predict dont know 1accurate Note parity ϕ 1accurate predict dont know formula ϕ depend attribute masked observations Should prefer learning algorithm returns ϕ instead ϕ0 Yes However argue way imposing additional requirement learner restricting hypothesis class capture prior knowledge target concept depend attributes A This exclude parity ϕ0 considered possible hypothesis Looking argument different angle target concept priori excluded removed hypothesis class possible learning phase case hypothesis hypothesis makes dont know predictions actual target concept learner looking It nonetheless argued learning algorithm need necessarily identify actual target concept hypothesis highly accurate Among satisfy requirement meaningful insist returned hypothesis complete possible We agree desirable obtain hypothesis achieves highest possible degree completeness At time note insisting case general reasonable Unlike feedback observations provide accuracy hypothesis compares optimal accuracy achieved target concept observations provide indication completeness hypothesis compares optimal completeness achieved hypothesis differ target concept In general case expect learning algorithm provide type guarantees completeness accuracy The characterization domains certain completeness guarantees learned hypothesis meaningfully insisted remains open L Michael Artiﬁcial Intelligence 174 2010 639669 649 32 Qualitative characteristics masking In addition concealment degree masking processes characterized based qualitative criteria We brieﬂy discuss representative types masking processes dimensions traditionally considered Statistical Analysis literature 1626 In ﬁrst dimension pattern masked attributes considered univariate pattern single attribute masked generally attributes ﬁxed set masked nonmasked arbitrary pattern attributes masked constraints2 In second dimension nature dependence masked attributes underlying examples considered attributes masked completely random MCAR masking attributes independent underlying example attributes masked random MAR masking attributes depend values nonmasked attributes ﬁnally attributes masked random MNAR masking attributes depends values masked attributes underlying example In simplest scenario Type 1 univariate pattern MCAR masking process mask maps examples observations mask target attribute xt happens randomly ﬁxed probability p independently example mapped Since accuracy conﬂict observed probability 1 p accuracy conﬂicts observed probability 1 p follows mask pconcealing target attribute xt wrt class F formulas The preceding scenario captures situations component agents sensors randomly fails provide reading In natural variation Type 2 univariate pattern MAR possibly MCAR component provide reading manner possibly dependent readings components independent reading The degree concealment masking processes dependence properties value range 0 1 In different scenario Type 3 arbitrary pattern MCAR independent masking attribute xi masked observations randomly ﬁxed probability pi differ attributes The attributes masked independently underlying example For given target attribute xt formulas predict dont know observations remaining attributes masked This fact seemingly makes calculation concealment degree Type 3 masking process mask xt involved Type 1 masking process Yet independence masking attributes imposed product distribution implies mask fact pt concealing particular target attribute xt wrt class F formulas It possible attributes masked independently underlying example Type 4 arbitrary pattern MCAR correlated masking For instance masking process map examples observations probability 023 ﬁrst half attributes masked probability 036 attributes indexed prime number masked probability 041 attribute x4397 masked Due correlation evaluation concealment degree masking process far straightforward depends target attribute xt class F formulas In general case Type 5 arbitrary pattern MNAR attribute xi masked observations manner depends value underlying example The human eyes exhibit behavior masking readings bright closing eyelids A survey viewed sensor behaves lack response question correlated answer Masking processes type hide information adversarially high degree concealment Although exhaustive list types aforementioned discussion illustrates qualitative characteristics masking process largely orthogonal degree concealment ease degree concealment calculated To emphasize point consider deterministic masking process mask maps examples observations target attribute xt masked depending Goldbach Conjecture true false standard axioms mathematics It easy attributes observations masked completely random univariate pattern concealment degree mask xt wrt class F formulas 0 1 In fact far know truth Goldbach Conjecture independent standard axioms mathematics case actual concealment degree mask meaning impossible mathematically prove concealment degree mask 33 Semantics dont know predictions Recall autodidactic learning formula makes dont know prediction insuﬃcient information exists observation formula evaluated defended choice earlier section Thus learner return hypothesis abstains making predictions certain observations abstention control learner hypothesis depends masking process observations gives rise 2 A possibility exists monotone pattern This pattern appears certain statistical studies attribute masked observation remains subsequent observations Monotone patterns meaningful setting consider observations assumed drawn independently 650 L Michael Artiﬁcial Intelligence 174 2010 639669 An immediate alternative consider setting learner returns program hypothesis The program receives input observation decides predict dont know 0 1 value In setting program decide predict dont know observations offer complete information attributes It evident simply measuring accurate predictions program suﬃcient program simply choose abstain making predictions A second metric completeness needed measures dont know predictions In PAClike model expect degree accuracy program suﬃciently high true degree completeness program Overall expect learner produce program makes dont know predictions inaccurate 0 1 predictions Put differently simply consider dont know predictions inaccurate predictions wrong predictions ask program produced learner makes wrong predictions Rivest Sloan 24 consider treatment dont know predictions similar discussed context complete information In second alternative learner expected produce hypothesis program predicts dont know Thus 0 1 prediction observations offer incomplete information value target attribute uniquely determined The PAClike guarantees expect setting achieve probability making accurate prediction suﬃciently close informationtheoretically optimal probability achieved Since value target attribute follow deterministically available information informationtheoretically optimal probability general 1 Such treatment followed instance Kearns Schapire 13 A alternative exists goal learner produce hypothesis given observation predicts target attribute masked observation Thus hypothesis longer attempts accurately recover missing value target attribute The learning goal longer identifying structure exists underlying examples Instead goal identifying structure exists observations In sense setting assumes way information sensed agent including information parts missing structured For instance target attribute masked certain attributes masked Note setting resembles case learning complete information original PAC model difference instead learning boolean formula boolean attributes learns ternary formula ternary attributes This approach taken instance work Valiant 29 Robust Logics work Goldman et al 9 34 Degree supervision learning The focus autodidactic learning model provide framework studying learned truly autonomous manner teacher present This particular implies value target attribute available learning phase Varying degree availability value target attribute gives rise settings relate certain learning models literature The orthogonal issue nontarget attributes masked dealt later On extreme consider setting target attribute masked observations available learner According autodidactic learning model nontrivial setting degree concealment masking process gives rise observations 1 It impossible learn predict missing value target attribute accurately cf Theorem 21 Such scenario contrasted unsupervised learning model case target attribute masked Despite lack information unsupervised learning expected group available obser vations meaningful clusters maximize metric The fundamental difference goal unsupervised learning autodidactic learning attempt uncover hidden deﬁnite reality value target attribute It simply partitions observations clusters associating cluster value target attribute That clusters identiﬁed perfectly observations masked target attribute hidden value 0 grouped impossible know given cluster corresponds target attribute having 0 value 1 value Overall focus unsupervised learning different autodidactic learning predictive guarantees unsupervised learning offers unrelated required task information recovery examined work On extreme consider setting target attribute masked observations available learner According autodidactic learning model degree concealment masking process gives rise observations 0 Thus learning predict missing value target attribute accurately compromised fact accuracy degenerates consistency The assumption value target attribute available learning phase followed super vised learning models Similar autodidactic learning supervised learning seeks identify value target attribute determined values rest attributes The fundamental difference goal super vised learning autodidactic learning lies availability value target attribute evaluation phase In autodidactic learning observations evaluation phase come source observa tions learning phase sensors agent Thus assuming target attribute masked L Michael Artiﬁcial Intelligence 174 2010 639669 651 observations learning phase follows true evaluation phase This implication makes goal learning predict value target attribute superﬂuous value target attribute observed need predicted In supervised learning hand valid assume value target attribute available evaluation phase goal learning predict value target value meaningful The natural way interpret discrepancy learning evaluation phase supervised learning approach learners sensors provide agent value target attribute However learning phase external teacher supervises learner provides values target attribute Overall premises supervised learning contrast goal developing framework learning completely autonomous manner Yet restrict context autodidactic learning techniques algorithms developed supervised learning models 35 Regularity information hidden In truly autonomous learning setting teacher available offer agent suﬃcient information aid learning task assumptions information hidden observations agent senses environment In autodidactic learning reﬂected making assumptions masking process looks like respect follow treatment PAC model assumption distribution examples nature chooses states environment sensed agent Nonetheless certain domains notably involving teacher regularity assumed sensors agent precisely regularity result combined workings agents sensors teacher completes information missing agents sensory inputs cf discussion supervised learning earlier section In teacherbased settings regularity information hidden differs learning evaluation phase We examine assumptions learning models place regularity information hidden learning phase On extreme learning models complete observations assumed The original PAC model taken fall category variants Learning Theory literature 24 Despite Valiants seminal paper introduced PAC model 28 discusses learnability partial observations certain benign type missing information observations considered It assumed information hidden nontarget attributes long nonmasked attributes provide information target concept evaluate 1 value target attribute 1 observation So examples value target attribute 1 masked observations manner essential information lost examples value target attribute 0 masked arbitrary observations We aware learning models autodidactic learning lie end spectrum essentially assumptions information missing Among approaches aware concep tually closer autodidactic learning learning models assume independence way attributes masked Such case work Decatur Gennaro 6 attribute masked observations ran domly independently underlying example attributes ﬁxed probability p constant attributes Other learning models assumptions information hidden observations nontarget attributes restrict target attribute way depends masked nontarget attributes This case model learning examples unspeciﬁed attribute values 9 target attribute masked value target concept determined values available rest attributes This asymmetry best explained presence teacher ensures constraint met The Robust Logics framework 29 priori impose similar restriction masking particular target attribute implicitly assumes case learnability possible In cases learner attempts learn predict value target attribute dont know learning structure observations allowing hypotheses condition predictions values certain nontarget attributes dont know Contrast autodidactic learning model goal learn structure underlying examples dont know values attributes explicitly taken account hypotheses Schuurmans Greiner 27 consider model target attribute masked examine cases remaining attributes masked arbitrarily according product distribution cf Type 3 masking Section 32 masked BenDavid Dichterman 2 consider different setting k attributes masked observation choice attributes masked change observations actively chosen learner A number models surface consider complete observations effectively viewed dealing special class partial observations masked attributes ﬁxed unknown subset If attempts compute value target attribute function values nonmasked attributes interpret lack existence deterministic function classiﬁcation noise 1 existence probabilistic function 13 existence deterministic function occasionally switched 3 Fig 1 shows missing information manifest ways 652 L Michael Artiﬁcial Intelligence 174 2010 639669 Manifestations Missing Information Assume set attributes required fully environ cid13 cid13 cid13 ment partitioned set hidden attributes Acid13 x Acid13 assigned dont know value 2 x 1 x observations set known attributes A x1 x2 xA assigned 0 1 value observa tions Let obs 0 1A unique corresponding partial observation exmcid13 0 1Acid13A randomly drawn underlying probability distribution Then complete observation obscid13 0 1 Acid13A example masked obscid13 When target attribute xt A expressed target concept ϕcid13 ϕ ϕcid13 formula Acid13 ϕ formula A partial observations Acid13 A manifest complete observations A xt noisily expressed manifested target concept ϕ The hidden value ϕcid13 exmcid13 determines xt obtains value manifested target concept ϕ noisy value manifested complete observation obs ii When target attribute xt A expressed target concept ϕcid13 partial observations Acid13 A manifest complete observations A xt ex pressed manifested probabilistic target concept evaluates 1 obs probability pϕcid13 obs The probability hidden value ϕcid13 1 exmcid13 determines probability pϕcid13 obs xt obtains value 1 manifested complete observation obs formula Acid13 ϕcid13 iii When target attribute xt A expressed target concept set formulas Acid13 exactly evaluates 1 given truthassignment attributes Acid13 ϕ1 ϕ2 ϕk set formulas A partial observations Acid13 A manifest complete observations A xt switching expressed manifested target concepts ϕ1 ϕ2 ϕk The unique formula ϕcid13 determines manifested target concept ϕ j xt obtains value manifested complete observation obs j hidden value 1 exmcid13 ϕ j ϕcid13 2 ϕcid13 j1ϕcid13 1 ϕcid13 k j cid7 k Fig 1 Various ways missing information manifest 4 Learnability results tools We continue section establish learnability results autodidactic learning model In spirit PAC model autodidactic learning model extends goal establish certain concept classes learnable despite arbitrary manner information hidden observations Since ultimate goal predicting accurately value target attribute expect learned hypotheses highly accurate By Theorem 22 suﬃces learn highly consistent hypotheses Our learnability results derived existing PAC learning algorithms techniques extend case partial observability As motivating example consider following algorithm properly learning monotone conjunctions PAC semantics Consider observations assign value 1 target attribute xt Out attributes Axt remove assigned value 0 observation learning phase Return hypothesis comprised conjunction remaining attributes This algorithm proposed proved correct PAC semantics Valiants seminal paper 28 The idea algorithm essentially following identify hypothesis agrees given learning examples appeal Occams Razor type argument Consider application algorithm partial observations Clearly observations obs obst ignored Since case observations obs obst 0 safely replace values target observations value 0 affecting outcome algorithm By entirely analogous attribute xt argument replacing values attributes A xt observations value 1 affect outcome algorithm attributes ignored algorithm Overall exist certain default values assigned masked attributes algorithm face complete observations affecting algorithms behavior This suggests possible obtain consistent learners reducing learning problem complete observations It suggests principle known Occams Razor apply case partial observations Consequently certain positive results certain learnability techniques PAC semantics lifted case consistent learnability partial observations 41 Learnability Occams Razor Intuitively ideas Occams Razor 4 rely learning observations com plete For completeness presentation reproduce version Occams Razor case consistent learnability Although actually employ technique derive positive learnability results later results derived Occams Razor argument It remains interesting prospect derive novel autodidactic learnability results established techniques consider section L Michael Artiﬁcial Intelligence 174 2010 639669 653 Deﬁnition 41 Compressibility An algorithm L compressor learning task cid6xt C Hcid7 A exists real number β 0 1 sample O observations given formula c C 1consistent xt real number δ 0 1 real number ε 0 1 algorithm L following property given access A cid6xt C Hcid7 δ ε O algorithm L runs time polynomial 1δ 1ε A sizec O returns probability 1 δ hypothesis h H 1 εconsistent xt given O size linear Oβ polynomial 1δ 1ε A sizec The concept class C A xt compressible target attribute xt A hypothesis class H A xt exists compressor cid6xt C Hcid7 A Unlike consistent learner cf Deﬁnition 23 compressor given oracle access observations instead sample O observations mention underlying reality observations obtained The consistency guarantees returned hypothesis expected respect given sample assumed perfectly consistent formula concept class exists The compressor allowed expend resources increase size O The compression requirement accounted insisting size returned hypothesis grows linearly Oβ nonnegative constant β 1 We establish compressor learning task essentially consistent learner learning task Theorem 41 Learning compression Consider learning task cid6xt C Hcid7 A The concept class C consistently learnable xt H concept class C compressible xt H Proof Consider algorithm Lcid13 compressor learning task cid6xt C Hcid7 A We construct algorithm L follows Fix probability distribution D supporting C xt c target concept xt D masking process mask real number δ 0 1 real number ε 0 1 Then algorithm L given access A cid6xt C Hcid7 δ ε oracle returning observations drawn maskD proceeds follows Algorithm L draws sample O number observations determined later oracle simulates algorithm Lcid13 returns hypothesis h algorithm L returns hypothesis h terminates input A cid6xt C Hcid7 δcid13 δ2 εcid13 ε2 O When algorithm Lcid13 εcid13 Consider set Hcid13 We prove algorithm L consistent learner learning task cid6xt C Hcid7 A To prove probability 1 δ returned hypothesis h 1 εconsistent xt D mask algorithm L runs time polynomial 1δ 1ε A sizec By construction algorithm L simulated algorithm Lcid13 given access sample O Clearly c C 1consistent xt given O By choice δcid13 Deﬁnition 41 exists constant β 0 1 algorithm Lcid13 runs time polynomial 2δ 2ε A sizec O returns probability 1 δ2 hypothesis h H 1 ε2consistent xt given O size linear Oβ polynomial 2δ 2ε A sizec Oβ polynomial 2δ 2ε A sizec By Theorem 23 setting γ ε2 formula ϕ Hcid13 probability e probability Hcid13e return probability δ2 Hcid13e formulas H 1 ε2consistent xt given O size linear Oε22 ϕ 1 εconsistent xt D mask By union bound Oε22 formula ϕ Hcid13 1 εconsistent xt D mask Overall algorithm L Oε22 hypothesis h H 1 εconsistent xt D mask contains formulas size linear Oβ polynomial 2δ 2ε A sizec follows log Hcid13 cid2 mOβ poly2δ 2ε A sizec constant m Thus probability algorithm L return hypothesis 1 εconsistent xt D mask Since Hcid13 δ2 2mOβ poly2δ2εAsizece Oε22 cid2 δ2 2 Oβ O1β ε22mpoly2δ2εAsizec Fixing O positive integer exceeds quantity cid8 cid8 2 ε2 log 2 δ cid9 m poly cid10 2δ 2ε A sizec cid11cid11 1 1β trivially implies Oβ cid3 1 ensures O1βε22 m poly1δ 1ε A sizec cid3 log 2 δ making prob ability algorithm L return hypothesis 1 εconsistent xt D mask δ required The running time algorithm L comprises time required draw sample O observations time Both tasks carried time polynomial 2δ 2ε A sizec O required simulate algorithm Lcid13 O polynomial 1δ 1ε A sizec claim follows cid2 42 Learnability reductions Eﬃciently reducing problem natural approach establish solving ﬁrst problem harder solving second Thinking input problem corresponding instance problem term 654 L Michael Artiﬁcial Intelligence 174 2010 639669 mapping inputs instance mapping Thinking output problem corresponding solution instance problem term mapping outputs solution mapping Such reductions widely employed area computational complexity context PAC learning 23 oracle available ﬁrst learner transformed oracle available second learner returned hypotheses second learner transformed hypotheses returned ﬁrst learner Other inputs required learning δ ε transformed instance mapping In case reductions learning problems solution mapping referred hypothesis mapping The transformation oracles reductions context PAC learning relatively straightforward Each complete observation drawn original oracle mapped complete observation thought drawn transformed oracle In case autodidactic learning oracle returns observations drawn probability distribution mask1D1 mask1 necessarily identity mapping Transforming oracle principle simply mapping drawn observation observation This suﬃce The induced probability distribution resulting observations needs express ible form mask2D2 resulting observations thought masking examples drawn underlying distribution D2 The following deﬁnition captures requirement For generality deﬁne onetomany reduc tions learning problem transformed This generality invoked later obtain positive learnability results In fact unaware learnability results obtained onetomany reduction Whether onetomany reductions powerful onetoone reductions context learnability remains interesting open problem t C j H jcid7 Deﬁnition 42 Reductions learning tasks A learning task cid6xt C Hcid7 A reducible set cid6x j1 learning tasks exists hypothesis mapping g H1 Hr H j 1 cid2 j cid2 r A jr exists instance mapping f j 0 1 A 0 1 A j probability distribution D supporting C xt c target concept xt D masking process mask exists probability distribution D j supporting C j x t D j masking process mask j j t c j target concept x j j tuple cid6h1 hrcid7 H1 Hr observation obs 0 1 A holds gcid6h1 hrcid7 j consistency conﬂict xt wrt obs exists j 1 cid2 j cid2 r h j consistency conﬂict x t wrt f jobs ii j 1 cid2 j cid2 r induced probability distribution f jmaskD equal mask jD j iii instance hypothesis mappings computable time polynomial A sizec size input r sizec j j 1 cid2 j cid2 r polynomial A sizec Roughly speaking conditions Deﬁnition 42 correspond respectively following requirements Condi tion ensures transformations inputs outputs involved learning problems highly consistent hypotheses resulting problems correspond highly consistent hypothesis original problem At time Condition ii ensures instance mappings respect requirement resulting observations mask examples drawn appropriate probability distribution Finally Condition iii ensures entire reduction carried eﬃciently One note Deﬁnition 42 dictate parameters δ ε inputs learning problem transformed Indeed given aforementioned conditions hold δ ε transformed appropriately proof following result illustrates Theorem 42 Learning reductions Consider learning task cid6xt C Hcid7 A reducible set learning tasks j1 The concept class C consistently learnable xt H j 1 cid2 j cid2 r concept class C j cid6x j t C j H jcid7 A jr consistently learnable x j t H j t H j let algorithm L j Proof Assume j 1 cid2 j cid2 r concept class C j consistently learnable x t C j H jcid7 A j Let g H1 Hr H hypothesis mapping learner learning task cid6x j 1 cid2 j cid2 r let f j 0 1 A 0 1 A j instance mapping existence guaranteed Deﬁnition 42 We construct algorithm L follows Fix probability distribution D supporting C xt c target concept xt D masking process mask real number δ 0 1 real number ε 0 1 Then algorithm L given access A cid6xt C Hcid7 δ ε oracle returning observations drawn maskD proceeds follows j j For j 1 cid2 j cid2 r algorithm L simulates algorithm L j input A j cid6x t C j H jcid7 δ j δr ε j εr oracle returning observations Whenever algorithm L j accesses oracle requests observation algorithm L draws observation obs oracle passes f jobs algorithm L j When simulated algorithm L j returns hypothesis h j algorithm L computes returns hypothesis gcid6h1 hrcid7 terminates j L Michael Artiﬁcial Intelligence 174 2010 639669 655 j cid6 r We prove algorithm L consistent learner learning task cid6xt C Hcid7 A To prove probability 1 δ returned hypothesis gcid6h1 hrcid7 1 εconsistent xt D mask algorithm L runs time polynomial 1δ 1ε A sizec j t wrt j t c j target concept x t D j mask j By union bound follows probability 1 By construction algorithm L simulated algorithm L j given access oracle f jmaskD By Condition ii j Deﬁnition 42 exists probability distribution D j supporting C j x t D j masking process mask j f jmaskD mask jD j By Deﬁnition 23 algorithm L j runs time poly nomial 1δ j 1ε j A j sizec j returns probability 1 δ j hypothesis h j H j 1 ε jconsistent j1 δ j 1 δ algorithm L j x j return hypothesis h j H j 1 ε jconsistent x t D j mask j Assume gcid6h1 hrcid7 1 εconsistent xt D mask Thus probability gcid6h1 hrcid7 consistency conﬂict xt wrt observation obs maskD ε By Condition Deﬁnition 42 follows probability t wrt f jmaskD mask jD j exists j 1 cid2 j cid2 r h j consistency conﬂict x ε By pigeonhole principle follows exists j 1 cid2 j cid2 r probability h j consistency j conﬂict x t D j mask j This event happens probability δ Therefore probability 1 δ returned hypothesis gcid6h1 hrcid7 1 εconsistent xt D mask establishes ﬁrst claim f jmaskD mask jD j εr ε j h j 1 ε jconsistent x The running time algorithm L comprises running time r simulated algorithms time required simulate oracle calls algorithms application instance mappings time required obtain hypothesis gcid6h1 hrcid7 application hypothesis mapping By Condition iii Deﬁnition 42 instance mappings computable time polynomial A sizec size A j set attributes resulting learning tasks polynomial A sizec By condition size sizec j target concept resulting learning tasks polynomial A sizec Since condition implies r polynomial A sizec follows 1δ j rδ 1ε j rε polynomial 1δ 1ε A sizec Therefore input simulated algorithm L j polynomial 1δ 1ε A sizec running time algorithm L j polynomial input polynomial 1δ 1ε A sizec This turn implies algorithm L j accesses oracle number times polynomial 1δ 1ε A sizec applications instance mapping f j computable time polynomial 1δ 1ε A sizec Furthermore running time simulated algorithms implies size cid6h1 hrcid7 polynomial 1δ 1ε A sizec Condition iii Deﬁnition 42 hypothesis mapping computable time polynomial 1δ 1ε A sizec In conclusion algorithm L runs time polynomial 1δ 1ε A sizec This concludes proof cid2 j Our motivating example algorithm learning monotone conjunctions suggests special case reductions observations resulting learning tasks complete In terms Deﬁnition 42 corresponds having resulting learning task instance mapping codomain complete observations Deﬁnition 43 Total reductions learning tasks As special case Deﬁnition 42 reduction learning task cid6xt C Hcid7 A set learning tasks cid6x j1 total j 1 cid2 j cid2 r instance mapping form f j 0 1 A 0 1A j t C j H jcid7 A jr j Establishing total reductions particular reasons philosophical point view total ductions establish links partial complete observability allowing identify conditions lack complete information affect learnability ii pragmatic point view established links relate autodidactic learnability wellstudied PAC semantics allowing carry positive results model 43 Monotonicity preserves learnability By reductions establish general result showing monotonicity concept class compensates missing information observations sense concept class monotone formulas learnable standard PAC semantics remains autodidactic learning semantics A formula ϕ set attributes A monotone pair examples exm1 exm2 0 1A xi A exm1i 1 xi A exm2i 1 holds valϕ exm1 cid16 valϕ exm2 cid16 imposes natural ordering values 0 1 In words changing input monotone formula attributes assigned value 1 result formulas value remaining changing 0 1 Consider value monotone formula ϕ observation obs If valϕ obs 0 1 clearly mapping attributes masked obs 0 1 value affect value formula Furthermore valϕ obs mapping attributes masked obs 0 1 result formula obtaining respective value These simple properties suggest partial observations replaced complete observations value monotone formula affected predictable manner This predictability facilitates existence total reductions 656 L Michael Artiﬁcial Intelligence 174 2010 639669 Theorem 43 Total selfreduction monotone formulas A learning task cid6xt C Hcid7 A total reducible learning task cid13 xt Ccid13 C Hcid13 H hypothesis mapping restricted identity mapping cid6x concept class C hypothesis class H classes monotone formulas C contain tautology formula cid17 cid13 Acid13 A x t t Ccid13 Hcid13cid7 Acid13 cid13 attribute x Proof We ﬁrst deﬁne constructs existence required Deﬁnition 42 Deﬁne hypothesis mapping g Hcid13 H identity mapping Deﬁne instance mapping f 0 1 A 0 1Acid13 observation obs 0 1 A holds f obsi 0 obst f obsi obst obsi obst 0 1 f obsi obsi obsi 0 1 obst 0 1 For probability distribution D supporting C xt c target concept xt D masking process mask deﬁne probability distribution Dcid13 identity mapping equal induced probability distribution f maskD deﬁne masking process maskcid13 Acid13 We proceed prove properties monotone formulas respect instance mapping f For observa tion obs 0 1 A cid13 attribute x Acid13 deﬁnition f directly implies cid13 valxi obs 0 1 obst 0 1 valx cid13 valxi obs obst 0 1 valx f obs valxi obs f obs obst Thus observation obs 0 1 A cid13 formula ϕcid13 x t Ccid13 Hcid13 holds valϕ obs 0 1 obst 0 1 valϕcid13 f obs valϕ obs valϕ obs obst 0 1 valϕcid13 f obs obst For Condition Deﬁnition 42 consider hypothesis h consistency conﬂict xt wrt obs Then valgh follows valh f obs 0 1 h cid13 f obs valgh cid13 cid13 obs valx t cid13 observation obs 0 1 A cid13 cid13 Hcid13 gh cid13 obs obst 0 1 By properties discussed f obs obst Hence valh cid13 cid13 f obs valx t cid13 t wrt f obs needed consistency conﬂict x Condition ii Deﬁnition 42 follows trivially deﬁnition probability distribution Dcid13 masking process cid13 cid13 target concept x t formula c x t Consider ﬁxed observation obs drawn maskD Clearly c consistency conﬂict xt wrt cid13 obs obst valc obs obst cid8 0 1 We proceed case analysis remaining To establish Dcid13 supports Ccid13 cid13 c Ccid13 maskcid13 Dcid13 obs valc possibilities In ﬁrst case assume obst By deﬁnition instance mapping f follows valx f obs 0 Thus monotone formula ϕ tautology valϕ f obs 0 In particular Acid13 cid13 cid13 x valc cid13 cid13 f obs 0 valx t f obs In second case assume obst 0 1 valc obs obst By properties discussed earlier follows cid13 f obs f obs obst The assumption implies valc cid13 cid13 f obs valc obs valx t valc cid13 valx t f obs In case assume obst 0 1 valc obs By properties discussed earlier follows f obs obst Therefore valc cid13 cid13 f obs obst valx t cid13 cid13 f obs valx t f obs valc cid13 cid13 f obs valx t cid13 cid13 exmcid13 valx t c f obs observation f obs drawn In case established valc f maskD Since observations drawn f maskD complete Dcid13 f maskD obtain valc expressed cid13 c wrt Dcid13 With regards Condition iii Deﬁnition 42 instance mapping f hypothesis mapping g clearly cid13 equal computable time linear size inputs number r resulting learning tasks 1 sizec sizec At point reduction established exmcid13 example exmcid13 t Dcid13 cid13 Deﬁnition 21 implies x t cid13 target concept x drawn Dcid13 cid13 The totality established reduction follows Deﬁnition 43 deﬁnition instance mapping f This concludes proof cid2 Theorem 43 establishes monotonicity formulas concept hypothesis classes suﬃcient condition lack complete information affect learnability Interestingly consistent learnability partial observations reduces consistent learnability concept class complete observations Equally intrigu ing fact hypothesis learned complete observations resulting learning task applies unmodiﬁed making predictions partial observations original learning task Since hypothesis appropriate information recovery cf Theorem 22 follows concrete strategy accurately recover missing information simply assign appropriate default truthvalues masked attributes learning phase consistently learn resulting complete observations employ learned hypothesis predictions Two technical points worth discussing The ﬁrst relates requirement tautology formula concept class This restriction loss generality An agent attempting learn structure L Michael Artiﬁcial Intelligence 174 2010 639669 657 environment employ sampling determine high probability target concept given target attribute tautology employ Theorem 43 case The second technical point relates encoding value target attribute certain attributes resulting learning task Although agnostic fact agent learning resulting learning task utilizes value target attribute involved manner typical use means test predictions hypotheses What makes established result nontrivial fact returned hypothesis depend target attribute context original learning task hypothesis eventually employed making predictions A useful suﬃcient condition consistent learnability follows immediately Theorems 42 43 Corollary 44 Suﬃcient condition consistent learnability Consider learning task cid6xt C Hcid7 A The concept class C consistently learnable xt H concept class C learnable H Probably Approximately Correct semantics C H classes monotone formulas Building known learnability results PAC semantics 51428 Corollary 44 implies learnability results certain concept classes consistent learnability semantics Distributionspeciﬁc PAC learnability results learning expected succeed particular uniform probability distribution fall auspices Corollary 44 reduction transforms learning task context consistent learnability PAC semantics distorts probability distribution examples Corollary 45 Proper consistent learnability certain concept classes Each concept classes conjunctions disjunctions kCNF kDNF linear thresholds monotone formulas A xt properly consistently learnable target attribute xt A 44 Shallowness preserves learnability One tools employed humans modelling environment abstraction The tool employed learning Structure captured complex formula abstracted monotone disjunction disjunct representing complex situation In cases ability learn type formulas cf Corollary 45 imply ability learn abstraction applied certain moderation We develop notions necessary model process abstraction learning determine degree moderation preserves learnability In terms given formula ϕ set attributes A abstraction thought process substituting new attributes subformulas ϕ manner prescribed set M substitutions Recall think formulas syntactic objects equivalently think formula corresponding particular circuit computes formula Abstraction process replacing parts representation formula certain subcircuits cid13 associated inputs new attributes Each substitution M form x iψψ indicates attribute cid13 substituted subformula ψ We require M induces bijection subformulas ψ x iψ cid13 substitution process invertible property critical abstraction attributes x iψ sense In general case substitutions applied nondeterministically formula ϕ possible resulting formula produced The unique maximal subset resulting formulas obeys following constraints known basis ϕ given M denoted basisϕ M Acid13 Acid13 cid13 formula ϕcid13 basisϕ M pair ψ1 ψ2 subformulas ϕ belong set ψ x iψψ cid13 M x iψ appears ϕcid13 attribute xi A shared ψ1 ψ2 cid13 iψψ M x cid13 ii formula ϕcid13 basisϕ M new set attributes Acid13 x iψ Roughly speaking Condition asks abstracted components independent restriction imposed ensure learnability preserved Condition ii asks attributes ϕ replaced substitution process restriction imposed notational convenience Note Condition trivially satisﬁed readonce formula ϕ attribute appears A number valid invalid sets substitutions illustrated Table 2 An intuitive graphical illustration substitution process constraints deﬁned respect depicted Fig 2 Deﬁnition 44 Shallowness classes formulas A class F formulas set attributes A shallow class F cid13 formulas set attributes Acid13 ϕF basisϕ M formula ϕ F exists formula ϕcid13 basisϕ M F cid13 wrt set M substitutions F cid13 subset cid12 A class F formulas shallow class F cid13 formulas wrt set M substitutions contains formulas exhibit structure fundamentally different determined M structure exhibited formulas F cid13 cf Table 2 Thus given class readonce formulas shallow class monotone formulas wrt set 658 L Michael Artiﬁcial Intelligence 174 2010 639669 Table 2 The bases formula x80 x5 x7 x2 x9 x56 given sets substitutions Whenever set substitutions valid formulas underlying disjunctive nature preserved basis Set M substitutions cid13 cid13 cid13 x 3x9 x56 2x7 x2 x 1x80 x5 x cid13 cid13 cid13 2x9 x56 2x7 x2 x 1x80 x5 x x cid13 cid13 cid13 cid13 x 4x80 x5 3x9 x56 x 2x7 x2 x 1x80 x5 x cid13 cid13 2x7 x2 1x80 x5 x9 x56 x x cid13 cid13 cid13 cid13 cid13 x 5x56 1x80 x5 x 4x9 x 3x2 x 2x7 x cid13 cid13 cid13 3x9 2x7 x2 x 1x80 x5 x x Basis formula given M cid13 cid13 cid13 x x x 1 2 3 cid13 invalid M x 2 invertible cid13 cid13 cid13 cid13 cid13 x x x x 3 x 2 1 2 4 cid13 cid13 x x 2 1 cid13 cid13 cid13 cid13 cid13 5 3 x x x x x 4 2 1 x56 replaceable cid13 x 3 cid13 iψψ M Fig 2 Graphical illustration substitution process operating circuit implements formula ϕ A Each substitution x cid13 corresponds gadget implements formula ψ x iψ standing gadget For notational convenience assume cid13 cid13 attribute xi A substitution x M The substitution process amounts xi belongs M circuit input xi implemented gadget x employing gadgets M replace shaded parts circuit Once circuit input replaced gadget unavailable gadgets refer input longer employed Nonetheless gadgets internally refer input multiple times When circuit inputs replaced gadgets left new circuit new inputs correspond names gadgets M The resulting circuit vary depending choice gadgets employed Every new circuit truncated version abstraction original circuit formula ϕcid13 implements element basis ϕ given M substitutions replace possible literal new attribute Similarly class readonce formulas disjunctive normal form shallow class disjunctions wrt set substitutions replace possible term conjunction literals new attribute In context learning F F cid13 viewed representing possible structures different envi ronments structure ﬁrst environment corresponds formula F structure second environment corresponds formula F cid13 wrt M implies structure second environment essentially abstraction structure ﬁrst As pointed extent abstraction moderate possible establish learnability environment abstract structure carries environment reﬁned structure The follow ing deﬁnition describes conditions possible terms M ultimately determines relation F F cid13 Establishing F shallow F cid13 Deﬁnition 45 Moderately shallow learning tasks A learning task cid6xt C Hcid7 A moderately shallow learning task cid13 cid6x exists set M substitutions t Ccid13 Hcid13cid7 Acid13 cid13 basisxt M x t ii M enumerable time polynomial A cid13 iii x C shallow Ccid13 iψψ M observation obs 0 1 A wrt M H shallow Hcid13 wrt M holds valψ obs computable time polynomial A Theorem 46 Reduction moderately shallow learning tasks A learning task cid6xt C Hcid7 A reducible learning task cid13 cid6x moderately shallow t Ccid13 Hcid13cid7 Acid13 Proof Assume learning task cid6xt C Hcid7 A moderately shallow learning task cid6x let M set substitutions existence guaranteed Deﬁnition 45 cid13 t Ccid13 Hcid13cid7 Acid13 L Michael Artiﬁcial Intelligence 174 2010 639669 659 exists hypothesis h H h We ﬁrst deﬁne constructs existence required Deﬁnition 42 By Deﬁnition 44 hypothesis cid13 Hcid13 cid13 basish M deﬁnition set M substitutions h h unique deﬁne hypothesis mapping g Hcid13 H map formula h unique formula h Deﬁne instance mapping f 0 1 A 0 1 Acid13 cid13 attribute x iψ holds f obsiψ valψ obs example exm masked obs deﬁnition formula evaluation implies valψ obs valψ exm deﬁnition instance mapping f implies f obsiψ f exmiψ f obs masks f exm For probability distribution D supporting C xt c target concept xt D masking process mask deﬁne probability distribution Dcid13 equal induced probability distribution f D deﬁne masking process maskcid13 example exm 0 1A welldeﬁned maps f exm f obs obs maskexm note f obs masks f exm maskcid13 observation obs 0 1 A maskcid13 Acid13 cid13 We proceed prove properties formulas respect instance mapping f For observation obs 0 1 A cid13 iψψ M deﬁnition f directly implies x cid13 valx iψ f obs valψ obs Now ﬁx formula ϕ xt C H formula ϕcid13 basisϕ M observation obs 0 1 A We continue example exmcid13 0 1Acid13 masked f obs construct example exm 0 1A masked obs valϕcid13 exmcid13 valϕ exm The sought example exm obtained starting observation obs proceeding follows cid13 cid13 iψψ M attribute x For x cid13 obs appear ψ ψ evaluate valx iψ arbitrary 0 1 value obtain exm iψ appears ϕcid13 valψ obs ﬁx masked attributes exmcid13 Fix remaining masked attributes obs By requirement subformulas ϕ replaced obtain ϕcid13 share attributes follows construction example exm welldeﬁned It clear exm masked obs Also subformula ψ cid13 cid13 exmcid13 valψ exm Indeed valψ obs ϕ replaced attribute x iψ holds valx iψ case construction exm guarantees claimed condition holds valψ obs 0 1 case claimed condition follows valψ exm valψ obs obs masks exm valx f obs valψ cid13 obs deﬁnition instance mapping f valx Thus iψ example exmcid13 0 1Acid13 obs valϕcid13 exmcid13 valϕ exm This implies cid13 iψ f obs f obs masks exmcid13 masked f obs exists example exm 0 1A cid13 exmcid13 valx iψ masked formula ϕcid13 basisϕ M valϕcid13 f obs valϕ obs valϕ obs 0 1 cid13 Hcid13 For Condition Deﬁnition 42 consider hypothesis h cid13 consistency conﬂict xt wrt obs Then valgh cid13 H h cid13 basisgh cid13 M By properties discussed observation obs 0 1 A cid13 obs obst 0 1 By deﬁnition hy follows f obs 0 1 cid13 cid13 f obs valx t f obs obst Hence valh cid13 t wrt f obs needed consistency conﬂict x cid13 cid13 obs valx t gh pothesis mapping gh valh h cid13 f obs valgh cid13 Condition ii Deﬁnition 42 follows immediately deﬁnition probability distribution Dcid13 process maskcid13 Dcid13 f D maskcid13 f D f maskD To establish Dcid13 cid13 basisc M Ccid13 formula c target concept xt Dcid13 cid13 t Dcid13 target concept x Deﬁnition 21 implies xt expressed c wrt D supports Ccid13 masking cid13 t x Deﬁnition 44 formula exists Since c cid2 Pr valc exm exmt exm D cid3 1 By properties discussed follows valc example exm 0 1A Hence cid13 cid13 f exm valc exm valx t f exm exmt cid2 cid9 val c cid10 cid13 f exm cid9 val Pr cid10 f exm cid13 x t exm D cid3 1 Since holds Dcid13 f D conclude cid2 cid9 val c Pr cid13 exmcid13 cid10 cid9 val cid13 cid10 cid3 cid13 x t exmcid13 exmcid13 Dcid13 cid13 target concept x 1 t Dcid13 Deﬁnition 21 implies c With regards Condition iii Deﬁnition 42 instance mapping f computable time required traverse set M substitutions evaluate associated subformulas observation Conditions ii iii Deﬁnition 45 operations carried time polynomial A The hypothesis mapping g computable 660 L Michael Artiﬁcial Intelligence 174 2010 639669 time required read input traverse set M substitutions identify subformula substituted attribute input formula Condition ii Deﬁnition 45 subformula identiﬁed time polynomial cid13 equal sizec At point reduction A The number r resulting learning tasks 1 sizec established proof complete cid2 A generalized version suﬃcient condition consistent learnability established Corollary 44 follows Corollary 44 taken conjunction Theorems 42 46 Corollary 47 Generalized suﬃcient condition consistent learnability Consider learning task cid6xt C Hcid7 A The concept class t Ccid13 Hcid13cid7 C consistently learnable xt H learning task cid6xt C Hcid7 A moderately shallow learning task cid6x Acid13 classes monotone formulas Probably Approximately Correct semantics Ccid13 learnable Hcid13 Hcid13 Ccid13 cid13 This generalized suﬃcient condition implies consistent learnability additional concept classes For classes conjunctions disjunctions linear thresholds following result generalizes Corollary 45 retracting mono tonicity assumption For classes kCNF kDNF formulas following result provides new consistently learnable subclasses incomparable subclasses consistent learnability established Corollary 45 sub stituting readonce property monotonicity property kCNF kDNF formulas properties As Corollary 44 distributionspeciﬁc PAC learnability results fall auspices Corollary 47 Corollary 48 Proper consistent learnability additional concept classes Each concept classes conjunctions disjunctions readonce kCNF readonce kDNF linear thresholds formulas literals A xt properly consistently learnable target attribute xt A In preliminary version work incorrectly reported general classes kCNF kDNF formulas properly consistently learnable We ﬁnd informative discuss subtle critical point prevents results generalizing classes Consider formula ϕ1 ϕ2 When formula evaluated example exm deﬁnition holds valϕ1 ϕ2 exm valϕ1 exm valϕ2 exm similar properties hold logical connectives Observe local evaluation formula carried partial observations Indeed valϕ1 obs valϕ2 obs valϕ1 ϕ2 obs general uniquely determined If instance ϕ1 semantically negation ϕ2 valϕ1 ϕ2 obs 1 ϕ1 ϕ2 semantically equivalent valϕ1 ϕ2 obs Note locality evaluation restored formulas ϕ1 ϕ2 assumed share attributes Although unaware relevant formal result learning literature natural conjecture locality formula evaluation essential reductions learning settings This turn explains reductions establish learnability general kCNF kDNF formulas PAC semantics able establish learnability readonce counterparts autodidactic learning semantics 5 Negative learnability results We pointed learnability PAC semantics special case autodidactic learnability So far excluded possibility learning models equivalent terms concept classes learnable On contrary general positive learnability results indicate models equivalent broad set concept classes tempting conjecture lack information learning render learnability harder shown true instance concept classes monotone formulas In section progress disproving conjecture We particular concept classes properly learnable PAC semantics properly learnable autodidactic learning semantics insists concept hypothesis classes coincide Such representationspeciﬁc nonlearnability results studied context PAC learnability 22 preclude possibility concept classes nonproperly learnable The nonproper learnability autodidactic learning semantics concept classes discussed section remains open The negative results prove respect consistent learnability Note accuracy implies consistency irrespectively concealment degree masking process Thus results imply learning accurately possible certain cases It worth emphasizing established negative results require use masking pro cesses high degree concealment Indeed observations masked target attribute constrain learner way hypothesis makes consistent predictions observations masking process masks target attribute observation offer advantage All results employ 0concealing masking processes We note results rely formulas eﬃciently evaluated observations We start general result later use obtain speciﬁc negative autodidactic learnability results L Michael Artiﬁcial Intelligence 174 2010 639669 661 Theorem 51 Suﬃcient condition hard learning tasks Fix arbitrary positive integer n N Consider set attributes A size polynomial n learning task cid6xt C Hcid7 A log C size polynomial n Assume exists algorithm input 3CNF formula χ size n runs time polynomial n outputs set observations Oχ 0 1 A formula H evaluatable time polynomial n observation Oχ ii χ satisﬁable exists formula H 1consistent xt given Oχ iii χ satisﬁable exists probability distribution D 0 1A C xt maskD uniform distribution Oχ masking process mask D supports Then concept class C consistently learnable target attribute xt hypothesis class H RP NP Proof Assume concept class C consistently learnable target attribute xt hypothesis class H Let L consistent learner learning task cid6xt C Hcid7 A let q associated polynomial determines running time algorithm L given input parameters Consider algorithm Lsat deﬁned follows On input 3CNF formula χ size n algorithm Lsat constructs set observations Oχ It proceeds simulate algorithm L input A cid6xt C Hcid7 δ 13 ε 12Oχ oracle returning observations The sim ulation interrupted q1δ 1ε A log C timesteps During simulation algorithm L accesses oracle requests observation algorithm Lsat draws observation obs uniformly random Oχ passes obs algorithm L If simulated algorithm L returns hypothesis h H algorithm Lsat checks returns h consistency conﬂict xt wrt observation Oχ If simulation algorithm L interrupted algorithm Lsat returns false In case algorithm Lsat terminates returning truthvalue We prove algorithm Lsat runs time polynomial n determines given arbitrary 3CNF formula χ size n satisﬁable χ unsatisﬁable algorithm Lsat return false probability 1 χ satisﬁable algorithm Lsat return true probability 23 Assume ﬁrst χ unsatisﬁable By Condition ii formula H consistency conﬂict xt wrt observation Oχ Therefore algorithm Lsat return false expected irrespectively algorithm L turns hypothesis h H simulation interrupted Assume χ satisﬁable By Condition iii exists probability distribution D masking process mask D supports C xt oracle algorithm L draws observations maskD let c target concept xt D By Deﬁnition 23 algorithm L run time q1δ 1ε A sizec return probability 1 δ hypothesis h H 1 εconsistent xt D mask Since sizec cid2 log C simulation algorithm L interrupted algorithm Lsat obtain h Since ε strictly probability particular observation Oχ drawn follows probability 1 δ returned hypothesis h H consistency conﬂict xt wrt observation Oχ algorithm Lsat verify return true expected Since δ 13 probability algorithm Lsat correctly report χ satisﬁable 23 Since 1δ 3 1ε 2Oχ polyn A polyn log C polyn follows q1δ 1ε A log C polynomial n The set observations Oχ constructible time polynomial n observations uniformly samplable Oχ time polynomial n Condition returned hypotheses testable consistency conﬂicts xt wrt observations Oχ time polynomial n Hence algorithm Lsat runs time polynomial n In conclusion established existence algorithm algorithm Lsat solves NPcomplete problem resource constraints allowed problems RP This implies RP NP concludes proof cid2 Under standard computational complexity assumption RP cid8 NP present intractability results proper consistent learnability certain explicit concept classes known properly PAC learnable Our results hold attributes masked observations target attribute suggesting property agents sensing process compromises consistent learnability frequency information missing obtained appearances context happens3 This realization corrobo rated viewed conjunction contrast certain results literature establish learnability severely impaired information observations missing independently random attribute despite giving rise observations possibly simultaneously masked attributes 6 3 Recall Theorem 21 similar phenomenon occurs learned hypotheses eventually employed agent making predictions The predictive accuracy hypotheses highly consistent ones compromised manner depends frequency sensors hide information mainly context happens Intriguing fact obtaining impossibility learning highly accurate hypotheses suﬃces target attribute masked obtaining intractability learning highly consistent hypotheses suﬃces nontarget attributes masked Hence masked features sense associated fundamental reason unlearnability compared masked labels learning instances 662 L Michael Artiﬁcial Intelligence 174 2010 639669 51 Nonlearnability parities A parity formula set attributes A formula form xi1 xir denotes exclusive binary operator A parity formula evaluates 1 example exm exactly odd number formulas attributes assigned value 1 exm The concept class parity formulas associated numerous open problems learning literature Nonethe concept class known properly learnable PAC semantics 711 albeit techniques rely critically availability explicit set complete observations In particular concept class parity formulas known unconditionally nonlearnable representationindependent sense Statistical Query model 15 nonevolvable 31 indicating singularity concept class A justiﬁcation singularity appeal ex treme sensitivity parity formulas attributes independently values remaining attributes change attributes value affects value parity formula It property following result exploits Interest ingly parity formulas highly nonmonotone accordance consistent learnability concept classes monotone formulas Theorem 52 Intractability proper consistent learnability parities The concept class C parities A xt properly consistently learnable target attribute xt A RP NP Proof Fix arbitrary positive integer n N Let V v 1 v 2 vn set variables instances 3SAT size n deﬁned Construct learning task cid6xt C Hcid7 A follows Deﬁne A cid4 x v V xt C x set parities A xt H cid4 C av cid4 x For variable v V deﬁne av cid4 x construction mapping set literals V set attributes A xt bijective For 3CNF formula mcid13 χ l j1 l j2 l j3 j1 l jk literal V denote Oχ set observations contains exactly following 1 cid2 cid2 n observation obsvari assigns value 1 attributes x x value 0 attributes A x xt x ii j 1 cid2 j cid2 m observation obscls j assigns value attributes al j1 al j2 al j3 value 1 attribute xt value 0 attributes A al j1 al j2 al j3 xt value 1 attribute xt cid14 We continue establish formula χ satisﬁable exists parity formula ϕ H consistency conﬂict target attribute xt wrt observation Oχ Consider set literals τ al l τ H corresponding parity formula bijective property mapping V let ϕτ cid4 set literalsets V set H hypotheses bijective We prove certain properties mapping The following derivation establishes correspondence truthassignments induced τ χ lack consistency conﬂicts ϕτ xt wrt observations Oχ type τ induces truthassignment χ 1 cid2 cid2 n exactly v v belongs τ 1 cid2 cid2 n exactly x belongs ϕτ x 1 cid2 cid2 n ϕτ consistency conﬂict xt wrt obsvari A second derivation establishes correspondence sets literals determined τ satisfy χ lack consistency conﬂicts ϕτ xt wrt observations Oχ type ii τ contains literal clause χ j 1 cid2 j cid2 m l j1 l j2 l j3 belongs τ L Michael Artiﬁcial Intelligence 174 2010 639669 663 j 1 cid2 j cid2 m al j1 al j2 al j3 belongs ϕτ j 1 cid2 j cid2 m valϕτ obscls j j 1 cid2 j cid2 m ϕτ consistency conﬂict xt wrt obscls j Together derivations imply τ induces satisfying truthassignment χ ϕτ consistency conﬂict xt wrt observation Oχ This conclusion bijection property mapping leads following derivation establishes claim χ satisﬁable exists set literals τ V induces satisfying truthassignment χ exists τ ϕτ consistency conﬂict xt wrt observation Oχ exists ϕ H consistency conﬂict xt wrt observation Oχ To conclude proof suﬃces conditions Theorem 51 satisﬁed Clearly A polynomial n log C 2 formulas C Also set observations Oχ constructible time polynomial n For Condition Theorem 51 note parity ϕ H evaluated observation obs Oχ time polynomial n Indeed attribute ϕ masked obs case valϕ obs attributes ϕ masked obs case number attributes assigned value 1 obs determines valϕ obs A Condition ii Theorem 51 follows directly derivation For Condition iii Theorem 51 assume χ satisﬁable let τ set literals V induces satisfying truthassignment χ Consider set examples contains exactly following 1 cid2 cid2 n example exmvari assigns value 1 attributes x x value 0 attributes A x xt x ii j 1 cid2 j cid2 m example exmcls j assigns value 1 ﬁxed ordering attributes A attribute al jk set al j1 al j2 al j3 al l τ value 1 attribute xt value 0 attributes A al jk xt value 1 attribute xt Deﬁne probability distribution D returns examples equal probability Deﬁne masking process mask maps example observation Oχ subscript probability 1 Clearly maskD uniform Oχ target attribute xt expressed parity formula ϕτ C wrt probability distribu tion D D supports C xt ϕτ target concept xt D Condition iii Theorem 51 follows proof complete cid2 When compared related results literature 22 existence partial observations complicates required reduction NPcomplete problem lies heart proof Theorem 52 Yet partial obser vations allow ﬂexible manipulation learning algorithm proof Theorem 51 invoked reduction explains ability establish particular intractability result More precisely reduction relies constructing observations order explained consistently require learned hypothesis depend nonempty subset masked attributes observations specifying subset chosen Such constraints allude combinatorial problem precisely learning algorithm expected solve It case complete observations force learned hypothesis depend certain subsets attributes possible dependencies attributes necessarily restricted observations subtle critical manner 52 Nonlearnability decision lists A kdecision list set attributes A ordered sequence cid6c1 v 1cid7 cid6cr vrcid7cid6cid17 vr1cid7 pairs comprising dition ci term k N literals A associated decision v 0 1 value A decision list evaluates example exm value v associated leastindexed condition ci evaluates 1 exm tautology formula appears condition ensures evaluation process welldeﬁned 664 L Michael Artiﬁcial Intelligence 174 2010 639669 The concept class kdecision lists constant k N known properly learnable PAC semantics 25 The proper learnability retained monotoneterm kdecision lists considered Unlike parity formulas monotoneterm kdecision lists learnable Statistical Query semantics 15 presence random classiﬁcation noise 1 Rivest 25 introduced concept class established PAC learnability asked learnability preserved instead complete observations considers partial observations In notation deﬁned agreement4 formula ϕ observation obs mean valϕ obs obst assuming target attribute xt masked drawn observations As posed question admits trivial negative answer observation obs generally masks set examples value ϕ varies implying valϕ obs making ϕ disagree obs We recast notion agreement believe appropriate possibly intended form formula ϕ agrees observation obs ϕ consistency conﬂict target attribute xt obs This notion agreement weaker requires valϕ obs obst valϕ obs 0 1 obst 0 1 We partially answer new question negative showing concept class monotoneterm 1decision lists properly consistently learnable partial observations RP NP The negative answer carries Rivests original question stronger notion agreement Theorem 53 Intractability proper consistent learnability monotoneterm 1decision lists The concept class C monotoneterm 1decision lists A xt properly consistently learnable target attribute xt A RP NP Proof Fix arbitrary positive integer n N Let V v 1 v 2 vn set variables instances 3SAT size n deﬁned Construct learning task cid6xt C Hcid7 A follows Deﬁne A cid4 x v V xt C x set monotoneterm 1decision lists A xt H cid4 C av cid4 x For variable v V deﬁne av cid4 x construction mapping set literals V set attributes A xt bijective For 3CNF formula mcid13 χ l j1 l j2 l j3 j1 l jk literal V denote Oχ set observations contains exactly following observation obszero assigns value 0 attributes A ii 1 cid2 cid2 n observation obsvari0 assigns value 1 attributes x x xt value 0 attributes A x xt x iii 1 cid2 cid2 n observation obsvari1 assigns value attributes x x xt value 0 attributes A x xt x value 0 attribute value 1 attribute iv j 1 cid2 j cid2 m observation obscls j assigns value attributes al j1 al j2 al j3 value 1 attribute xt value 0 attributes A al j1 al j2 al j3 xt Without loss generality remainder proof consider readonce monotoneterm 1decision lists attribute appears conditions monotoneterm 1decision list violates assump tion simply drop ﬁrst condition contain particular attribute obtain new monotoneterm 1decision list respects assumption equivalent ﬁrst We continue establish formula χ satisﬁable exists monotoneterm 1decision list ϕ H consistency conﬂict target attribute xt wrt observation Oχ Consider monotoneterm 1decision list ϕ H consistency conﬂict xt wrt observation Oχ let τϕ cid4 l cid6al 1cid7 ϕ set literals V We continue verify τϕ induces satisfying assignment χ For observation obs Oχ identify conclusions follow given ϕ H consistency conﬂict xt wrt obs We proceed case analysis types observations Oχ ϕ evaluate 1 obsvari0 observation obszero follows value corresponding tautology condition ϕ 0 ii 1 cid2 cid2 n observation obsvari0 follows ϕ contain pairs cid6x 1cid7 1cid7 cid6x 1cid7 1cid7 cid6x tautology condition ϕ satisﬁed Conclusion ϕ evaluate 0 obsvari1 iii 1 cid2 cid2 n observation obsvari1 follows ϕ contains pairs cid6x 4 Rivest 25 actually term consistency work term agreement employed We avoid use term consistency context term different meaning framework L Michael Artiﬁcial Intelligence 174 2010 639669 665 iv j 1 cid2 j cid2 m observation obscls j follows ϕ contains pairs cid6al j1 1cid7 cid6al j2 1cid7 cid6al j3 1cid7 tautology condition ϕ satisﬁed Conclusion ϕ evaluate 0 obscls j Conclusions ii iii imply τϕ induces truthassignment χ Conclusion iv implies induced truthassignment satisfying χ χ satisﬁable needed To conclude proof suﬃces conditions Theorem 51 satisﬁed Clearly A polynomial AA formulas C Also set observations Oχ constructible n log C 2 3 time polynomial n For Condition Theorem 51 note monotoneterm 1decision list ϕ H evaluated observation obs Oχ time polynomial n Indeed attribute ϕ assigned value 1 obs prune suﬃx ϕ follows tuple cid6ci v icid7 ϕ condition ci attribute replace condition said tuple tautology formula cid17 affecting value ϕ obs Similarly attribute ϕ assigned value 0 obs drop tuple cid6ci v icid7 ϕ condition ci attribute affecting value ϕ obs After monotoneterm 1decision list ϕ processed masked obs By construction valϕ obs valϕcid13 obs clearly obtain ϕcid13 remaining attributes ϕcid13 value 1 0 decisions ϕcid13 valϕcid13 obs 1 decisions ϕcid13 value 0 cid15 Condition ii Theorem 51 follows directly conclusions For Condition iii Theorem 51 assume χ satisﬁable consider set literals τ V induces satisfying truthassignment χ Let ϕτ cid4 cid6al 0cid7cid6al 1cid7 l τ cid6cid17 0cid7 multiplication tuples taken correspond noncommu tative concatenation l taken traverse τ ﬁxed order literals V By construction v V conditions x appear ϕτ condition appearing second corresponding value 1 associated truthvalue v determined τ By inspection valϕτ exm exmt example exm set examples contains exactly following x example exmzero assigns value 0 attributes A ii 1 cid2 cid2 n example exmvari0 assigns value 1 attributes x x value 0 attributes A x xt x iii 1 cid2 cid2 n example exmvari1 assigns value 1 single attribute x al l τ value 1 attribute xt value 0 attributes A x xt assigns value 1 attributes Acls j example exmcls j iv j 1 cid2 j cid2 m set x x set value 0 attribute xt al j1 al j2 al j3 al l τ value 1 attribute xt value 0 attributes A Acls j xt Deﬁne probability distribution D returns examples equal probability Deﬁne masking process mask maps example observation Oχ subscript probability 1 Clearly maskD uniform Oχ target attribute xt expressed monotoneterm 1decision list ϕτ C wrt proba bility distribution D D supports C xt ϕτ target concept xt D Condition iii Theorem 51 follows proof complete cid2 The intractability result Theorem 53 provides indication learnability partial observations harder learnability complete observations This indication remains true learnability complete observations restricted use statistical queries 15 use complete observations random classiﬁcation noise 1 6 Sensorrestricted learnability We taken approach domains agent priori assumptions nature information loss results imperfect sensors This premise reﬂected deﬁnition learnability introduced asks learning possible masking process In section turn attention domains bias exists way information hidden agent senses environment Such bias exists instance way human eye provides information surroundings spatiallydependent manner hiding values properties environment lie outside range sight A cryptanalyst attempting break decrypting device use probes gain insight internal workings device obtaining readings independently random probes attached device A piece text viewed appearance underlying reality presumably hides information asymmetrically instance properties underlying reality false hidden true Bias agents sensors captured letting masking process models member class S masking processes known sensor class class contains possible masking processes obtain observations Consistent learnability redeﬁned learning expected successful employed masking process member S sensor class S available learner way concept hypothesis classes 666 L Michael Artiﬁcial Intelligence 174 2010 639669 Deﬁnition 61 Consistent learnability restricted sensor class An algorithm L consistent learner learning task cid6xt C Hcid7 A sensors S probability distribution D supporting C xt masking process mask S real number δ 0 1 real number ε 0 1 algorithm L following property given access A cid6xt C Hcid7 S δ ε oracle returning observations drawn maskD algorithm L runs time polynomial 1δ 1ε A size target concept xt D returns probability 1 δ hypothesis h H 1 εconsistent xt D mask The concept class C A xt consistently learnable target attribute xt A hypothesis class H A xt sensors sensor class S exists consistent learner cid6xt C Hcid7 A sensors S The existence restricted sensor class S critically affect learnable information recovered partial observations First restrictions obeyed S guarantee agents sensors hide information entirely arbitrary manner exclusion certain sensors S This imply agent obtains information case S unrestricted Second restricted sensor class S allow agent employ learning algorithm tailored S learning algorithm known exist general case Through learned rules agent able recover information 61 Parameterized sensor classes It conceivable agent bias characteristics sensors depends structure envi ronment Consider instance student introductory Artiﬁcial Intelligence course lecture discusses agent The teacher acting students sensors presents positive negative instances agents properties entity depicted instance In trying learn constitutes agent student bias sensing process appearances obtained For sensing process hides information property states entity agent bias readily representable terms restricted sensor class Deﬁnition 61 The student additional subtle type bias sensing process hides properties important deﬁning agent teacher ensures case The bias type students sensors depends deﬁnition agent bias different deﬁnition different cid12 Formalizing structuredependent sensor class straightforward We simply update Deﬁnition 61 S union cC Sc subclasses possible structure c C environment ask learning succeeds masking process mask Sc c target concept xt D The learning algorithm given access sensor class S particular subclass Sc actual structure agents environment remains unknown One fact generalize deﬁnition allowing distributiondependent sensor classes bias characteristics agents sensors depends structure agents environment precise probability distribution reality obtained note probability distribution determines structure environment We update Deﬁnition 61 S union D SD subclasses possible probability distribution D ask learning succeeds masking process mask SD Similar case structuredependent sensor classes learning algorithm given access sensor class S particular subclass SD cid12 Appearances provided teachers hide information underlying reality convey addi tional information structure environment normally available A teacher presenting entity positive negative instance agent presents subset entitys properties conveys message hidden information irrelevant This additional piece information available student complete information entity presented In teacherassisted learning context dont know interpretation hidden properties longer characterize nature value Instead depending setting value better interpreted new distinct informationbaring value indicating instance value irrelevant probable nondeducible costly obtain hidden cases value provides implicit information fact play critical role facilitating learnability As proof concept consider concept class C formulas disjunctive normal form The learnability C hypothesis class PAC semantics remains longstanding open questions Computational Learning Theory proper learnability C DNF formulas restricted contain k N terms known intractable constant k cid3 2 RP NP 22 Yet teacher power determine parts randomly drawn example visible student assist learning process hiding information irrelevant example More precisely given learning task cid6xt C Hcid7 A C H classes kterm DNF formulas consider teacher modelled following structuredependent sensor class S cC Sc masking process mask Sc observation obs range mask target attribute xt masked obs obst 1 maximal subset attributes valc obs 1 masked obs obst 0 attribute masked obs The result shows power structuredependent sensor classes relevant cid12 L Michael Artiﬁcial Intelligence 174 2010 639669 667 Theorem 61 Proper consistent learnability kterm DNF formulas sensors Srelevant The concept class C formulas A xt disjunctive normal form k N terms properly consistently learnable target attribute xt A sensors Srelevant Proof We construct algorithm L follows Fix probability distribution D supporting C xt c target concept xt D masking process mask Sc Srelevant real number δ 0 1 real number ε 0 1 Then algorithm L given access A cid6xt C Ccid7 Srelevant δ ε oracle returning observations drawn maskD proceeds follows Algorithm L draws sample O number observations determined later oracle constructs xi Axt obsi1 xi returns hypothesis h xi Axt obsi0 xi terminates obsOobst1 cid16 cid7 cid7 We prove algorithm L consistent learner learning task cid6xt C Ccid7 A sensors Srelevant We returned hypothesis h C h probability 1 δ 1 εconsistent xt D mask algorithm L runs time polynomial 1δ 1ε A sizec Note ﬁrst observation obs maskD obst 1 encodes terms DNF formula c Indeed construction mask obst 1 holds valc obs 1 term c evaluates 1 obs Thus attributes term masked obs By construction mask rest attributes masked It follows xi Axt obsi0 xi precisely term c h DNF formula subset terms c particular h C Therefore c evaluates 0 observation h xi Axt obsi1 xi cid7 cid7 Consider observation obs maskD wrt h consistency conﬂict xt valh obs obst 0 1 By construction mask holds obst valc obs valh obs valc obs 0 1 By preceding discussion holds valc obs 1 valh obs 0 Thus obs encodes term c appear h implies term encoded observations O Consequently probability drawing observation ε probability observation O 1 εO cid2 e When O cid201ε ln1δcid21 probability δ needed εO The running time algorithm L clearly linear 1δ 1ε A This concludes proof cid2 Obtaining consistent learnability results sensors suﬃciently restricted sensor class trivial One need ensure attributes masked drawn observation target attribute masked returned hypothesis predicts dont know We emphasize proof Theorem 61 rely technique All masking processes Srelevant 0concealing target attribute xt wrt class formulas xt masked highly consistent hypotheses equally highly accurate Theorem 22 One verify returned hypotheses simultaneously highly consistent highly complete consistency conﬂict predict dont know total probability input parameter ε Thus returned hypotheses forced 0 1 predictions evaluation phase changing masking process identity mapping assigning arbitrary values masked attributes accuracy returned hypotheses suffer Overall Theorem 61 implies learning algorithm obtaining hypotheses highly accurate tested complete observations PAC semantics Theorem 61 exempliﬁes attempts examine learnability domains teachers choose parts underlying reality students observe situation shown naturally modelled parameterization sensor classes lead distortion meaning missing information Such domains essentially turn value addition 0 1 distinguished value employed transmit information agent This information allow agent learn environments learnability provably impossible complete observations illustrated Such treatment missing information lies outside scope autodidactic learnability demonstrated choice permit use parameterized sensor classes Deﬁnition 61 This problem dealt work Greiner et al 10 62 Special classes sensors The assumption sensor class S restricted underlies previous attempts literature model learnability partial observations Most previous work implicitly assumes existence teacher process learning corresponding learning settings naturally modelled relying parameterized sensor class The learning models differ primarily type restrictions impose sensor class S type parameterization consider These restrictions turn explain stronger learnability results obtained learning models compared obtained autodidactic learning semantics We revisit learning models discussed Section 3 summarize Table 3 approach partial observability learning phase viewed unifying prism restricted parameterized sensor class 668 L Michael Artiﬁcial Intelligence 174 2010 639669 Table 3 A uniﬁed view models deal problem learning partial observations Each model associated restrictions parameterization imposes sensor class Learning model This work 28 9 29 6 27 2 1313 Imposed restrictions parameterization sensor class S cid12 cC Sc Sc restricted None S mask Sc obs range mask obst 0 1 obst 1 valc obs 1 S mask Sc obs range mask obst valc obs cC Sc Sc restricted cid12 None priori makes implicit assumption 9 learnability possible For given probability p S maskbernoullip example exm xi A Probsi obs maskbernoullipexm p S restricted mask S obs range mask obst 0 1 For given model parameter k S restricted mask S obs range mask xi xi A obsi cid8 k Effectively set attributes A extended Acid13 A S maskhide maskhide unique masking process maps example unique observation obs masks exm xi xi Acid13 A obsi cid8 A 7 Outlook future directions We presented autodidactic learning model offers principled treatment partial information PAC like learning setting Although allows use supervised learning techniques autodidactic learning model assume presence external teacher supervision label target attribute provided extent agents sensors Within learning model shown principle known Occams Razor technique reductions learning problems applicable Through reductions shown monotone readonce formulas PAC learnable remain learnable learning examples arbitrarily incomplete On hand parities monotoneterm 1decision lists properly PAC learnable properly learnable incomplete learning examples values attributes hidden Numerous questions remain open To extent shallowness establish learnability results Are onetomany reductions beneﬁcial onetoone reduction context learnability What general techniques establish positive negative learnability results Can PAC learnable concept classes formulas eﬃciently general 3CNF locally general 2CNF evaluatable partial observations learned general result proven excludes possibility learning formulas Can learnability improved reasonable assumptions masking process sacriﬁcing autonomy learning Does sense attempt learn structure masking process addition structure underlying examples Is possible establish representationindependent nonlearnability PAClearnable concept class Under conditions certain degree completeness guaranteed learned hypotheses Endowing learning algorithms certain properties signiﬁcantly improve practical applicability One prop erty achieve running time independent observation size dependent number nonmasked attributes It easy exercise Winnow algorithm 17 learning linear thresholds modiﬁed obtain property Another property ability exploit information observations target attribute masked existing techniques semisupervised unsupervised learning complete information suggest direction fruitful Noise dealt Due equivalent treatment attributes autodidactic learning model harder justify consideration certain forms noise considered literature classiﬁcation noise 1 On hand random noise attributes meaningfully sidered 6 The extent reductions preserve noiseresilience investigated Conceivably obtained algorithms extent noiseresilient build existing noiseresilient PAC algorithms Noiseresilience alternatively established formulating corresponding Statistical Query model case PAC learning 15 Finally interesting examine learning possible examples attributes obey general types correlation considered work The role learned rules setting change explanatory explains value target attribute given values remaining attributes descriptive simply describes holds examples L Michael Artiﬁcial Intelligence 174 2010 639669 669 We believe treatment partial observability introduced provide basis addressing certain broader issues theoretical understanding actual implementation systems sense environment use imperfect sensors existing solutions problematic artiﬁcial Learning rules parallel multiple distinct target attributes expressed typical PAClike supervised learning models target attribute priori distinguished treated differently case autodidactic learning The use learned rules reasoning conclusions chained meaningfully supported learning models assume complete information Autodidactic learning hand naturally accommodates reasoning process missing information completed Finally domains machine learning typically employed autonomous acquisition unaxiomatized commonsense knowledge large corpora text 1930 understood conceptually cleaner manner autodidactic learning Text naturally viewed partial depiction underlying directly accessible reality commonsense knowledge acquisition amounts learning infer holds reality 21 Some considerations investigated 20 Acknowledgements The author grateful Leslie Valiant advice valuable suggestions remarks research Useful feedback received anonymous IJCAI AIJ reviewers References 1 Dana Angluin Philip D Laird Learning noisy examples Machine Learning 2 4 April 1988 343370 2 Shai BenDavid Eli Dichterman Learning restricted focus attention Journal Computer System Sciences 56 3 April 1998 277298 3 Avrim Blum Prasad Chalasani Learning switching concepts Proceedings Fifth Annual Workshop Computational Learning Theory COLT92 July 1992 pp 231242 4 Anselm Blumer Andrzej Ehrenfeucht David Haussler Manfred K Warmuth Occams Razor Information Processing Letters 24 6 April 1987 377380 5 Anselm Blumer Andrzej Ehrenfeucht David Haussler Manfred K Warmuth Learnability VapnikChervonenkis dimension Journal ACM 36 4 October 1989 929965 6 Scott E Decatur Rosario Gennaro On learning noisy incomplete examples Proceedings Eighth Annual Conference Computational Learning Theory COLT95 July 1995 pp 353360 7 Paul Fischer HansUlrich Simon On learning ringsum expansions SIAM Journal Computing 21 1 February 1992 181192 8 Michael R Garey David S Johnson Computers Intractability A Guide Theory NPCompleteness WH Freeman Co New York USA 1979 9 Sally A Goldman Stephen S Kwek Stephen D Scott Learning examples unspeciﬁed attribute values Information Computation 180 2 January 2003 82100 10 Russell Greiner Adam J Grove Alexander Kogan Knowing doesnt matter Exploiting omission irrelevant data Artiﬁcial Intelligence 97 1 2 December 1997 345380 11 David P Helmbold Robert H Sloan Manfred K Warmuth Learning integer lattices SIAM Journal Computing 21 2 April 1992 240266 12 Wassily Hoeffding Probability inequalities sums bounded random variables Journal American Statistical Association 58 301 March 1963 1330 13 Michael J Kearns Robert E Schapire Eﬃcient distributionfree learning probabilistic concepts Journal Computer System Sciences 48 3 June 1994 464497 14 Michael J Kearns Umesh V Vazirani An Introduction Computational Learning Theory The MIT Press Cambridge Massachusetts USA 1994 15 Michael J Kearns Eﬃcient noisetolerant learning statistical queries Journal ACM 45 6 November 1998 9831006 16 Roderick JA Little Donald B Rubin Statistical Analysis Missing Data 2nd ed John Wiley Sons Inc New York USA 2002 17 Nick Littlestone Learning quickly irrelevant attributes abound A new linearthreshold algorithm Machine Learning 2 4 April 1988 285318 18 John McCarthy Appearance reality John McCarthys home page httpwwwformalstanfordedujmcappearancehtml 30 August 2006 19 Loizos Michael Leslie G Valiant A ﬁrst experimental demonstration massive knowledge infusion Proceedings Eleventh International Conference Principles Knowledge Representation Reasoning KR08 September 2008 pp 378388 20 Loizos Michael Autodidactic learning reasoning PhD thesis School Engineering Applied Sciences Harvard University USA May 2008 21 Loizos Michael Reading lines Proceedings TwentyFirst International Joint Conference Artiﬁcial Intelligence IJCAI09 July 2009 pp 15251530 22 Leonard Pitt Leslie G Valiant Computational limitations learning examples Journal ACM 35 4 October 1988 965984 23 Leonard Pitt Manfred K Warmuth Predictionpreserving reducibility Journal Computer System Sciences 41 3 December 1990 430467 24 Ronald L Rivest Robert Sloan A formal model hierarchical concept learning Information Computation 114 1 1994 88114 25 Ronald L Rivest Learning decision lists Machine Learning 2 3 November 1987 229246 26 Joseph L Schafer John W Graham Missing data Our view state art Psychological Methods 7 2 June 2002 147177 27 Dale Schuurmans Russell Greiner Learning default concepts Proceedings Tenth Canadian Conference Artiﬁcial Intelligence AI94 May 1994 pp 99106 28 Leslie G Valiant A theory learnable Communications ACM 27 11 November 1984 11341142 29 Leslie G Valiant Robust logics Artiﬁcial Intelligence 117 2 March 2000 231253 30 Leslie G Valiant Knowledge infusion Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI06 July 2006 pp 1546 1551 31 Leslie G Valiant Evolvability Journal ACM 56 1 January 2009 31321