Artiﬁcial Intelligence 172 2008 955990 wwwelseviercomlocateartint Understanding role noise stochastic local search Analysis experiments Ole J Mengshoel RIACS NASA Ames Research Center Mail Stop 2693 Moffett Field CA 94035 USA Received 3 November 2006 received revised form 17 September 2007 accepted 17 September 2007 Available online 1 February 2008 Abstract Stochastic local search SLS algorithms recently proven best approaches solving computationally hard problems SLS algorithms typically number parameters optimized empirically characterize determine performance In article focus noise parameter The theoretical foundation SLS including understanding optimal noise varies problem difﬁculty lagging compared strong empirical results obtained algorithms A purely empirical approach understanding optimizing SLS noise problem instances vary computationally intensive To complement existing experimental results formulate analyze Markov chain models SLS article In particular compute expected hitting times rational functions individual problem instances mixtures Expected hitting time curves analytical counterparts noise response curves reported experimental literature Hitting time analysis polynomials convex functions discussed In addition present examples experimental results illustrating impact varying noise probability SLS run time In experiments probable explanations Bayesian networks computed use synthetic problem instances problem instances applications We believe results provide improved theoretical understanding role noise stochastic local search providing foundation progress area 2008 Elsevier BV All rights reserved Keywords Stochastic local search Noise Markov chain models Expected hitting times Rational functions Noise response curves Probabilistic reasoning Bayesian networks Most probable explanation Systematic experiments Polynomial approximation Convexity 1 Introduction The stochastic local search SLS approach proven highly competitive solving range hard compu tational problems including satisﬁability propositional logic formulas 11184546 computing probable explanation 222730 maximum posteriori hypothesis 3637 Bayesian networks While details different SLS algorithms vary 18 deﬁnition use stochasticity noise In article focus noise local search noisy initialization Empirically turns noise dramatic impact run time SLS algorithms 1417264445 Intu itively fundamental tradeoff high low levels noise SLS Let 0 cid2 p cid2 1 represent probability taking noise step The argument low noise p enables SLS algorithm greedily Email address omengshoelriacsedu 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200709010 956 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 climb hills taking unnecessary downhill noise steps The argument high noise p provides SLS algorithm powerful mechanism escape local nonglobal optima 17 Empiricallyand pending problem instance SLS algorithm input parameters including noise levelan approximately optimal noise level ˆp The difﬁculty approximating optimal noise p 26 led devel opment adaptive noise p static varies SLS algorithm runs 71426 However noise adaptation active area research believe present work provides key insights beneﬁt progress Our main contributions article follows Based seminal WALKSAT architecture 4445 troduce simple general SLS algorithm called SIMPLESLS SIMPLESLS performs noisy steps probability p greedy steps probability 1 p We expected hitting times SIMPLESLS rational functions P pQp P p Qp polynomials This explicitly provides functional form corresponding noise response curves previously established empirically SLS literature 78141626 We consider use polynomials convex functions Convexity important local optimality implies global opti mality dramatic simpliﬁcation compared unrestricted optimization Rational functions special case polynomials analytically determine optimal noise levels experiments article Using Markov chain analysis particular analyzing expected hitting times trap Markov chains clearly impact different settings noise parameter p difﬁculty problem instance varies key concern stochastic local search Trap Markov chains idealized model SLS gets trapped local nonglobal optima search space noise p impacts capability SLS escape traps Further optimal noise probability p varies dramatically depending problem instance In particular optimal noise parameter varies p 0 easy problem instances p close 1 hard problem instances We analysis empirical results Bayesian networks BNs synthetic applica tions BNs play central role wide range uncertainty reasoning applications including diagnosis probabilistic risk analysis language understanding intelligent data analysis error correction coding biological analysis Many interesting computational BN problems including MPE computation NPcomplete harder 374047 heuristic methods SLS In work study problem computing probable ex planation MPE Bayesian networks We use SLS algorithm known stochastic greedy search SGS search MPEs BNs SGS simulate SIMPLESLS ﬂexible operatorbased SLS approach different initialization search operators easily investigated 2730 Our approach generating synthetic BNs includes satisﬁability SAT special case 4047 reduc tion Let V number variables propositional logic number root nodes BNs Further let C number clauses propositional logic number nonroot nodes BNs For V 0 ratio CV deﬁned turned useful predicting inference difﬁculty randomly generated problem instances 3334 An easyhardeasy pattern inference difﬁculty function CV ratio observed SAT 34 For BNs easyhardharder pattern established inference difﬁculty measured terms upper bounds minimal maximal clique size treewidth 2933 Upper bounds optimal clique tree size optimal maximal clique size computed tree clustering 24 In article use CV ratio directly characterize difﬁculty synthetic BNs SLS There clear relationship Markov chain approach observed SLS run times We illustrate idealized trap Markov chain models relevant experiments real problem instances For small problem instances complete search spaces derive corresponding Markov chains With hand compare bounding hitting time results derived idealized trap Markov chain models ii analytical hitting time results derived Markov chain models created real problem instances behavior SIM PLESLS iii real observed SGS run times problem instances iv polynomial regression results SGS run times A key point relating Markov chain models classes real problem instances suggested following Increasing problem instance hardness controlled CV ratio corresponds roughly speaking increasing size trap trap Markov chain Consequently mixtures problem instances easy average small CV small traps solved greedier noisy SLS algorithms mixtures problem instances hard average large CV large traps In experiments synthetic problem stances observe patterns CV varied To complement experiments synthetic problems OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 957 investigate BNs applications advanced initialization noise algorithms Here noise minor impact SLS performance cases impact dramatic Generally empirical results support Markov chainbased analysis We believe work signiﬁcant following reasons First Markov chain hitting time analysis introducing explicit noise parameter p research provides improved understanding role noise plays stochastic local search Such theoretical understanding traditionally limited 141644 exists research based Markov chains explores role traps local maxima SLS 15 Second experimental results SLS impressive empirical basis means algorithms computationally intense optimize 26 We believe work paves way improved approaches optimize noise level stochastic local search optimization place principled fashion better understanding role noise established Third believe Markov chain analysis particular expected hitting times generally stochastic process theory underutilized researching SLS algorithms Hopefully impact parameters problem instances parameters control SLS search processes analyzed similar way utilizing techniques stochastic process theory The rest article organized follows Preliminary concepts introduced Section 2 In Section 3 present simple easytoanalyze SLS algorithm called SIMPLESLS Section 4 presents Markov chain models SIMPLESLS exact naive trap Markov chain models Section 5 provides indepth numerical analysis discussion examples trap Markov chains expected hitting times In Section 6 present general expected hitting time run time results Section 7 provides empirical results synthetic application problem instances speciﬁcally Bayesian networks conclude Section 8 2 Preliminaries We assume reader familiar fundamental deﬁnitions results areas graph theory probability theory statistics particular Markov chains 23 Bayesian networks 39 Some pertinent concepts brieﬂy reviewed section A direct natural way analyze SLS algorithms operation problem instance discrete time Markov chain discrete state space deﬁned follows Deﬁnition 1 Markov chain A discrete time discrete state space Markov chain Xn n 0 1 2 deﬁned 3tuple M S V P S s1 sk deﬁnes set k states V π1 πk kdimensional vector deﬁnes initial probability distribution The conditional state transition probabilities P characterized means k k matrix Only timehomogeneous Markov chains considered Many Markov chain models discussed including trap Markov chains introduced Section 43 random walks socalled boundary states s1 sk internal states s2 sk1 Further noise level p acts parameter Markov chain models discuss following In M states O S particular represent optimal states introduce following deﬁnition Deﬁnition 2 SLS model Let M S V P Markov chain Further assume objective function f S R optimal objective function value f R deﬁnes optimal states O s s S f s f An SLS model deﬁned 2tuple M O The objective function f optimal states O independent SLS algorithm parameters Note M O general explicitly speciﬁed Rather induced objective function problem instance SLS algorithm SLS algorithms parameter settings In fact ﬁnding s O purpose computation given implicitly means objective function f optimal objective function value f R Without loss generality main focus maximization problems article Consider SLS model M O A hitting time analysis Markov chain M gives expected number steps needed reach s O Expected hitting times introduced Deﬁnition 5 based ﬁrst passage times 958 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 expectations introduce In following deﬁnition Xj arbitrary random variable random variables Xn n 0 1 2 Markov chain Deﬁnition 3 First passage time Consider SLS model M O let si S S Ms states The ﬁrst passage time T s O O 1 given T minj cid3 0 Xj s The expected value T given initial state X0 si deﬁned mi ET X0 si In article mi function noise p case mip Note Deﬁnition 3 easily generalized cover passage time multiple optimal states O 1 simplify exposition generally focus onestate case We consider problem instances represented bitstrings length n b 0 1n case s b 0 1n We interested maximization ﬁnding b O f b cid3 f b b 0 1n The following deﬁnition formally introduces useful concept unitation summing bitstring Deﬁnition 4 Unitation Let b b1b2 bn 0 1n bitstring length n The number ones unitation b b1 bn deﬁned ub cid2 n i1 bi Counting number ones easy transformation search space equivalent counting number correct bits The number correct bits number bits equal b current state SLS search c More formally let b b c d 0 1n In order normalize search space use transformations b b xor c d b xor denotes exclusive b denotes bitwise inversion b Now ud transformed search space order obtain number correct bits clearly d 1 1 To simplify notation gloss transformations b 1 1 loss generality 6 See Fig 7 concrete examples Using conditional expectations obtains Deﬁnition 3 following wellknown result Theorem 5 Expected hitting time Let M Markov chain state space S s1 sk ﬁrst passage time T sk The expected hitting time h kcid3 kcid3 h ET X0 PrX0 i1 miπi i1 1 Expected hitting time analyze expected time reach optimal state closely related observed run time SLS algorithm In context SLS hitting time h respect state O depends algorithms input parameters including problem instance Our main article expected hitting time function noise p Typically hp instead h 1 reserve short form expected hitting time hp By studying hp varying p xaxis graphs obtain expected hitting time curves counterparts experimental noise response curves Clearly like ﬁnd SLS parameters minimal expected hitting time h obtained Often search minimal expected hitting time h empirical component notation ˆh In experimental work focus SLS approach computing probable explanation 222730 Bayesian networks BNs This problem interesting right generalizes satisﬁability 4047 BN nodes continuous restrict discrete BN nodes Deﬁnition 6 Bayesian network A Bayesian network tuple β X E P X E DAG associated set conditional probability distributions P PrX1 ΠX1 PrXn ΠXn Here PrXi ΠXi conditional probability distribution Xi X Let πXi represent instantiation parents ΠXi Xi The independence assumptions encoded X E imply joint probability distribution Prx Prx1 xn PrX1 x1 Xn xn ncid4 i1 Prxi πXi 2 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 959 A BN provided observations evidence setting clamping m nodes O1 Om X known states o O1 o1 Om om o1 om These nodes called observation nodes need considered explanations deﬁned follows Deﬁnition 7 Explanation Consider BN β X E P X X1 Xn observations o o1 om m n An explanation x assigns states nonevidence nodes Xm1 Xn x xm1 xn Xm1 xm1 Xn xn Among explanations BN u probable ones particular Deﬁnition 8 Most probable explanation MPE Let x range explanations BN β Finding probable explanation MPE β problem computing explanation x Prx cid3 Prx The u probable explanations X x Prx Prx cid3 Prx 1 cid2 cid2 u 1 Prx u Prx 1 x u As common compute MPE x X multiple MPEs exist BN Computation M probable explanations M cid3 1 explanations probability generalization investigated 50 Following Pearl denote computing MPE belief revision computing marginal distribution BN node denoted belief updating 39 Many computational BN problems hard Exact MPE computation NPhard 47 problem relative approximation MPE shown NPhard 1 Belief updating computationally hard 440 3 Stochastic local search We discuss simpliﬁed variant SLS SIMPLESLS based seminal WALKSAT architecture 164445 SIMPLESLS intended competitive stateoftheart SLS algorithms tailored domain problem instances hand 18 Rather SIMPLESLS intended enable analysis capture common SLS algorithms based WALKSAT particular respect noisy search components SIMPLESLS maintains current estimate b The SIMPLESLS algorithm takes input parameters nbitstring length pnoise probability f objective function evaluate f b b b1b2 bn 0 1n bit string length n f optimum value f MAXFLIPSthe number ﬂips restart ﬁnally MAXTRIESthe number restarts termination SIMPLESLS iteratively takes search steps greedy noisy discussed ˆb current state b To initialize b SIM PLESLS puts bi 0 Pr12 bi 1 Pr12 1 cid2 cid2 n Such initialization uniformly random common SLS algorithms 18 SIMPLESLS initializes ˆb manner Then onebit ﬂips repeatedly b success restart described takes place A onebit ﬂip means bs ith bit 1 cid2 cid2 n bi picked ﬂipped Suppose current bitstring b b1b2 bi bn1bn Then ﬂip set bcid6 ˆb formed bcid6 b1b2 bi bn1bn setting b bcid6 new bitstring bcid6 b Further f ˆb f SIMPLESLS terminates successfully If f b cid3 f ˆb The way ith bit b picked depends search operator algorithm A random variable O introduced representing random selection operator apply To analysis simple assume exactly local search operators operator types oG greedy oN noisy PrO oG PrO oN 11 The SIMPLESLS algorithm repeatedly instantiates random variable O randomly picking operators oG oN follows Greedy operator O oG With probability PrO oG 1 p greedy step b bcid6 objective function increase f b f bcid6 If tie k candidate bitstrings bcid6 algorithm picks uniformly random If f bcid6 cid3 f b puts bcid6 b maximizing 1 bcid6 k 1 In SGS experimentation Section 7 oN implemented NU operator oG implemented BM operator GM operator 960 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Noisy operator O oN With probability PrO oN p algorithm makes noise step follows First integer 1 cid2 cid2 n picked uniformly random This ith bit ﬂipped forming bcid6 The SIMPLESLS algorithm iteratively ﬂips bits f reached MAXFLIPS ﬂips performed Once MAXFLIPS ﬂips new try started process continues MAXTRIES reached The approach closely related WALKSAT 4445 particular random noise variant b In analysis MAXTRIES MAXFLIPS vary MAXFLIPS ex periments We assume Las Vegas algorithm guaranteed eventually terminate ˆb This assumption simpliﬁes analysis exposition need concerned randomly varying run time T One course ﬁx run time let approximated output ˆb random variable analysis different shall follow route article Our approach closely related previous stochastic local search SLS algorithms satisﬁability 111618434546 Bayesian network problems 2227303637 It somewhat related previous research stochastic simulation guided local search Stochastic simulation compute MPEs 39 essentially Gibbs sampling Bayesian networks Even Gibbs sampler respects general different SLS approaches typically operates cycles 25 A cyclic Gibbs sampler iterates systematically nonevidence nodes BN SLS algorithms hand generally opportunistic operate ﬁxed schedules Stochastic simula tion shown outperformed SLS approach combining greedy noisy search 22 investigate stochastic simulation article There class local search algorithms called guided local search 2035 emphasizes changing cost function employing noise Guided local search algorithms GLS 35 GLS 20 clearly interesting focus article stochastic local search algorithms analysis 4 Markov chain models stochastic local search Important aspects behavior stochastic local search SLS algorithms represented means discretetime Markov chains In Section 41 discuss exact Markov chain analysis SLS pros cons We provide approximate models results In Section 42 simple 3state Markov chain model discussed Section 43 general model trap Markov chains developed Readers ﬁnd naive trap Markov chain models restrictive want skim Sections 42 43 5 instead consider general analysis Sections 41 6 experimental results Section 7 41 Exact Markov chain analysis Clearly key aspects operation SIMPLESLS similar SLS algorithms speciﬁc problem instance formalized simulation Markov chain The structure underlying exact Markov chain hypercube hypercube state b represents bitstring A state b 0 1n Markov chain n neighbors bitstrings ﬂip away Stated formally bcid6 obtained ﬂipping bs bits neighbor b bcid6 Deﬁnition 9 Neighborhood Let b bitstring length n bs neighborhood nb strict neighborhood ncid6b nonneighborhood nb deﬁned follows cid7 cid5 nb c 0 1n bi ci cid2 1 ncid3 cid6 cid6 cid6 i1 cid6 b nb b n nb 0 1n nb The following Markov chain model introduced order reﬂect performance SIMPLESLS stated formally Theorem 11 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 961 Deﬁnition 10 Exact Markov chain model The exact Markov chain model M SIMPLESLS states S s0 s1 b b 0 1n initial probability distribution V PrX0 si 12n 1 cid2 cid2 2n The transition probability matrix P stochastic matrix given PrXj 1 bj 1 Xj bj 0 bj 1 nbj PrXj 1 bj 1 Xj bj cid3 0 bj 1 nbj 3 4 Theorem 11 SIMPLESLS simulates exact Markov chain model MAXFLIPS ﬂips Proof Obviously S V stated consider P Clearly SIMPLESLS regarded stochastic process Xi 0 1 2 discrete state space S b b 0 1n Further fewer MAXFLIPS ﬂips state bj 1 independent past given current state bj PrXj 1 bj 1 X0 b0 Xj bj PrXj 1 bj 1 Xj bj deﬁnes Markov chain For transition probabilities P construction SIMPLESLS operators consider oG oN Since number ﬂips MAXFLIPS obtain PrXj 1 bj 1 Xj bj PrXj 1 bj 1 Xj bj Oj oG PrXj 1 bj 1 Xj bj Oj oN In cases conditions 3 4 obeyed theorem follows cid2 oG picked oN picked cid8 Here example Deﬁnition 4 formal introduction unitation ub Example 12 An example hypercube representing exact Markov chain model 5bit SLS search space shown Fig 1 We consider state local minimum In case neighboring states ncid6b higher objective function value loss generality assumed SIMPLESLS performs maximization following result obtained Lemma 13 If current state bj SIMPLESLS local minimum state bj 1 state strict neighborhood bj 1 ncid6bj Fig 1 The hypercube 5bit bitstrings b 0 15 Neighboring bitstrings bitstrings bit differs edge The unitation ub bitstrings level shown left 962 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Proof Suppose arbitrary candidate state bj 1 denoted bcid6 bcid6 nbj SIMPLESLS applies operators O oG greedy O oN noisy If O oN bit ﬂipped bj 1 cid8 bj If O oG bj 1 bj f bcid6 cid3 f bj bcid6 ncid6b exists However clearly case bj assumption local minimum f bcid6 cid3 f bj bcid6 ncid6bj cid2 Since SIMPLESLS searches state space deﬁned ndimensional hypercube tempted perform analysis space However exactness comes steep price Since size P 0 1n 0 1n 2n1 size V 0 1n 2n speciﬁcation M impossibly large moderately sized problem instances For small n exact Markov chain analysis feasible For large n exact Markov chain analysis infeasible In following provide Markov chain analysis results similar approximate operate smaller state spaces compared exact space traversed SLS algorithms SIMPLESLS 42 Naive Markov chain analysis Here interested abstract Markov chain model SLS algorithms introduce following simple random walk model constitutes stepping stone trap Markov chains Section 43 Deﬁnition 14 Naive Markov chain model The naive Markov chain model M O Markov chain M states S s0 s1 s2 0 1 2 initial probability distribution V π0 π1 π2 transition probability matrix 5 cid9 P 1 1 b c 0 O s0 0 cid10 0 b 0 c 1 The naive Markov chain model example illustrated Fig 2 The values b c values initial probabilities π0 π1 π2 depend problem instance SLS algorithm hand In 5 s0 0 represents optimum O s1 1 represents search space close s0 0 s2 2 represents search space distant s0 0 Distance measured Hamming distance db1 b2 bitstrings b1 b2 0 1n There threshold 0 cid2 z n Formally s0 represents b s1 represents b b 0 1n 0 db b cid2 z s2 represents b b 0 1n db b z The state s2 2 represent search space traps 1115 topic discuss Section 43 The states s0 0 s1 1 represent search space strong SLS initialization algorithm reliably start search leading low number ﬂips reaching O s2 2 gives high number ﬂips Suppose PrXi1 1 Xi 1 b PrXi1 2 Xi 2 1 Here probability b staying s1 words close s0 transitioning optimum state s0 away s2 smaller probability 1 staying s2 distant optimum In words Fig 2 A naive Markov chain model parameters b c example parameter settings This topdown model state represents multiple states underlying exact Markov chain model SLS search process OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 963 SIMPLESLS close optimum likely ﬁnd optimum hand SIMPLESLS far optimum likely stay far away These concepts reﬂected numerical example Fig 2 Expected times ﬁrst entering optimum state s0 0 given following result Lemma 15 In naive Markov chain model suppose ab c 1 cid8 0 b cid8 1 Then ﬁrst passage times m0 m1 m2 m0 0 c m1 ab c 1 m2 b 1 ab c 1 Proof The fact m0 0 obvious A hitting time analysis Markov chain 5 gives simultaneous equations m1 1 b m1 c m2 m2 1 m1 1 m2 assumption ab c 1 cid8 0 solutions cid2 Here illustration potentially dramatic difference SLS starting state s1 1 giving ﬁrst passage time m1 versus state s2 2 giving ﬁrst passage time m2 Example 16 Let 5 11000 b 710 c 1100 Using Lemma 15 obtain c m1 ab c 1 m2 b 1 ab c 1 3793 1038 Suppose able increase probability good jumps state s2 2 distant optimum state s1 1 close optimum 1100 increasing probability bad jumps state s1 1 state s2 2 c 150 These changes c example implemented increasing SIMPLESLS noise p giving following result Example 17 Let 5 1100 b 710 c 150 Using Lemma 15 gives c m1 ab c 1 m2 b 1 ab c 1 1071 1107 In case improvements m1 m2 compared Example 16 relatively speaking 10fold reduction passage time m2 signiﬁcant Beneﬁts naive Markov chain model include simplicity ease speciﬁcation This leads improved understanding SLS calculation passage times extending previous research 15 However naive model simple capture important facets SLS algorithms particular direct representation noise probability p We turn realistic Markov chain models trap Markov chain models p explicitly represented attempt strike balance state space sizes exact naive Markov chain models 964 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 43 Trap Markov chain analysis In following continue use Markov chains introduce extensions noise parameter p variable substring length cid5 cid2 n approach vary problem difﬁculty higher abstraction level naive Markov chain model To size M tractable large problem instances focus objective functions f reasonable count number correct bits current bitstring b compared optimum b We introduce approach class Markov chains inspired deceptive trap functions 6 Trap functions related search space traps portions search space attractive SLS contain optima 1115 In deceptive trap functions number 1s unitation positions determine objective function values In deceptive trap functions bitstrings length cid5 n local deceptive optimum 0 representing b 0 0 global optimum cid5 representing b 1 1 slopechange location z Deﬁnition 18 Trap function A trap function gx cid5 z abbreviated gx length cid5 slopechange location z 0 cid5 1 function domain 0 cid5 N gx gx 1 0 cid2 x cid2 z 1 gx gx 1 z cid2 x cid2 cid5 1 There unique global optimum g gcid5 gz 1 gz 1 z 0 Intuitively trap function x z SLS algorithm SIMPLESLS greedily oG local optimum x 0 representing trapping search space dominated local maxima For x cid3 z SIMPLESLS greedily oG global optimum x cid5 representing search search space dominated global optimum b The size domain placement slopechange parameter z determine difﬁculty trap function SLS Here example trap function related SAT Note example simpliﬁcation compared actual SAT problem instances return Section 7 Example 19 Easy SAT instance Consider conjunctive normal form CNF formula V 5 variables C 20 clauses Further assume formula exactly satisfying assignment number satisﬁed clauses g varies 15 20 follows g0 16 g1 15 g2 17 g3 18 g4 19 g5 20 This trap function slopechange location z 1 g0 g1 g5 g4 g3 g2 g1 g2 g0 g g5 Example 19 illustrated Fig 3 examples varying difﬁculty A key point moving slopechange location z left right corresponds increasing problem instance hardness Intuitively easy problem instances slopechange location close b 0 0 solved greedier algorithm hard problem instance slopechange location close b 11 1 Two complete search spaces shown Fig 4 We assume SLS searches bitstrings integers Deﬁnition 18 introduce following deﬁnition Deﬁnition 20 Binary trap function Let b 0 1cid5 bitstring let gx cid5 z trap function introduced Deﬁnition 18 Then f b cid5 z gub cid5 z abbreviated f b deﬁnes binary trap function parameters cid5 z The concept trap functions deﬁned bitstrings somewhat abstract represent critical aspects SLS algorithms trapped applied NPhard problems SAT MPE For SAT bitstring b 0 1cid5 represents truth assignment cid5 n variables CNF formula MPE represents explanation cid5 n binary nodes Deﬁnition 21 Trap Let c 0 1cid5 bitstring let b 0 1cid5 optimal bitstring A greedy neighbor g nc neighbor reachable greedy SIMPLESLS step Then c trap state bitstring greedy steps c increase distance optima dg b dc b g b The search spaces trap deﬁned T c trap state c 0 1cid5 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 965 Fig 3 Trap functions gx cid5 z gx 5 z based satisﬁability problem 3SAT V 5 variables C 20 clauses assuming satisfying assignment We consider ﬁve hypothetical problem instances ranging easy slopechange location z 0 hard z 4 For assignment V 5 variables number correct assignments ranges x 0 x 5 shown xaxis The number satisﬁed clauses shown yaxis The complete search spaces Very Easy Hard shown Fig 4 Our concept trap related search space traps 1115 search space reachability analysis states solution reachable determined 51 It easy trap size binary trap function follows Lemma 22 Let f b cid5 z binary trap function The trap size T f given cid8 T 0 cid2 z1 i0 cid12 cid11 cid5 z o z cid3 1 6 The following result holds general Tmax largest trap possible 0 1n Tmin smallest trap possible Lemma 23 The maximal trap size Tmax 0 1n given Tmax 2n n 1 minimal trap size Tmin 0 Proof In worst case optimum b O Clearly nb trap nb cid8 Tmax nb n 1 obtain desired result Tmin 0 obvious cid2 Given result f b cid5 0 f b cid5 cid5 1 play distinguished roles bounding binary trap functions f b cid5 0 easiest trap function 0 1cid5 fact achieves Tmin 0 f b cid5 cid5 1 hardest trap function 0 1cid5 achieving Tmax 2cid5 cid5 1 follows 6 A trap Markov chain induced problem instance ones Fig 3 SLS algorithm specif ically SIMPLESLS input parameters including noise parameter p Given trap functions gx cid5 z construct trap Markov chain S V P S cid5 1 When constructing P account underlying binary trap function f b cid5 z map slopechange location z slopechange state The following deﬁnition trap Markov chain TMC describes performance SIMPLESLS trap function stated formally Theorem 25 966 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 4 The complete search spaces trap functions f b 5 0 f b 5 4 representing hypothetical SAT problem instances easiest hardest ones according trap function deﬁnition The number satisﬁed clauses shown point search spaces laid shown Fig 1 Deﬁnition 24 Trap Markov chain An cid5bit trap Markov chain change state z noise parameter p abbreviated TMCp cid5 z deﬁned follows It cid5 1 states S 0 cid5 initial distribution V deﬁned x S PrX1 x cid5 x cid12 cid11 2cid5 7 The state transition probabilities P Xi cid5 deﬁned PrXi1 cid5 Xi cid5 1 p PrXi1 cid5 1 Xi cid5 p Xi 0 z 0 deﬁned PrXi1 0 Xi 0 1 p PrXi1 1 Xi 0 p PrXi1 1 Xi 0 1 z 0 For internal states Xi cid8 0 Xi cid8 cid5 PrXi1 x 1 Xi x cid8 PrXi1 x 1 Xi x cid8 cid5x cid5 p 1 x 1 cid5x x cid5 p 1 cid2 x cid2 z 1 cid5 p z cid2 x cid2 cid5 1 cid5 p 1 cid2 x cid2 z 1 z cid2 x cid2 cid5 1 PrXi1 y Xi x 0 x y listed We emphasize trap Markov chains TMCs idealized models Quantitatively purpose TMCs provide interesting range expected hitting times hp including bounding cases examples lower bound Very Easy h50p upper bound Hard h54p Section 51 Qualitatively purpose TMCs display different shapes expected hitting time curves ranging ones monotonically increasing p p 0 optimal noise level ones decreasingincreasing pattern optimal noise levels increase value slopechange state z One expect ﬁnd realworld models nontrivial size exactly match TMCs intention ﬁt real SLS behavior TMCs For results showing idealized models aid understanding experiments real problem instances refer Section 71 Examples trap Markov chains presented Section 5 The reason terminology trap Markov chain Deﬁnition 24 clear following result formally state trap functions processed SIMPLESLS OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 967 Theorem 25 Trap Markov chain Let f b cid5 z cid5bit binary trap function slopechange location z If f given input SIMPLESLS noise probability p trap Markov chain TMCp cid5 z simulated MAXFLIPS ﬂips Proof Results boundary states cid5 0 follow easily exception z 0 For z 0 Lemma 13 applies bj 0 0 PrXi1 1 Xi 0 1 We turn internal states 0 x cid5 First consider PrXi1 x 1 Xi x Conditioning SIMPLESLS operators oN oG law total probability gives PrXi1 x 1 Xi x PrXi1 x 1 Xi x Oi oN PrOi oN PrXi1 x 1 Xi x Oi oGPrOi oG There cases consider Case 0 x z Case ii z cid2 x cid5 Case Suppose 0 x z In case gx 1 gx 1 means SIMPLESLS moves x x 1 case noise operation oN words PrXi1 x 1 Xi x Oi oG 0 PrXi1 x 1 Xi x Oi oN 0 Thus obtain PrXi1 x 1 Xi x PrXi1 x 1 Xi x Oi oN PrOi oN Since PrOi oN p PrXi1 x 1 Xi x Oi oN cid5 xcid5 PrXi1 x 1 Xi x cid5 x cid5 p law total probability fact random walk internal selfloops gives PrXi1 x 1 Xi x 1 cid5 x cid5 p Case ii Next suppose z cid2 x cid5 In case gx 1 gx 1 The derivation similar resulting PrXi1 x 1 Xi x x p cid5 PrXi1 x 1 Xi x 1 x cid5 concluding proof cid2 p Trap Markov chains related naive Markov chain model Section 42 socalled simple branched Markov chain models introduced Hoos 15 The simple branched Markov chain models capture similar phenomena trap Markov chains novel important features First trap Markov chain TMCp cid5 z parametrized noise parameter p essential analyzing impact noise SLS Second based empirical considerations trap Markov chain analytically derived model earlier work 6 Markov chains analysis genetic evolutionary algorithms 391248 Generally analysis emphasizes populationlevel effects different analysis article In particular know analysis genetic evolutionary algorithms includes noise level p explicit parameter Our approach allows example derivation closed form solutions expected hitting times deﬁned follows Deﬁnition 26 TMC hitting time The notation hkzp expected hitting time trap Markov chain TMCp k z Expected hitting times sense important underlying Markov chains explicitly Markov chains There reasons emphasis expected hitting times First empirical counterpart noise response curves common way reporting results SLS literature 781416 26 Second optimal noise levels p derived analytically expected hitting time formulas Section 5 Third displaying Markov chain transition matrices practice impossible nontrivial search spaces article expected hitting time expressions compact We discuss Markov chains smaller problem instances Section 71 968 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 5 Trap Markov chain models TMCp 5 z bitstrings length cid5 5 slopechange state z colored varies z 0 z 4 Each trap Markov chain models represents combined effect problem instance Fig 3 SIMPLESLS These topdown models state represents multiple states underlying exact Markov chain model SLS process 5 Trap Markov chain examples In section illustrate trap Markov chain model presented Section 43 means examples 51 Analysis trap Markov chains Perhaps easiest way illustrate utility trap Markov chain models discuss concrete problem stances Example 27 Trap Markov chains See Fig 5 graph representations transition probabilities TMCp 5 0 TMCp 5 1 TMCp 5 2 TMCp 5 3 TMCp 5 4 Markov chains In preparation formal result provide intuition Let consider TMCp 5 0 easy case suppose ﬁrst p 0 It easily seen Fig 5 regardless initial state 0 cid2 X0 cid2 5 SIMPLESLS search proceeds directly state 5 making transitions Xi k Xi1 k 1 k 1 0 Once p 0 chance SIMPLESLS makes transitions search longer The following result illustrated Fig 6 gives expected hitting times SIMPLESLS formalized Exam ple 27 This shows noise probability p impacts expected hitting time hkzp problem instances varying difﬁculty Difﬁculty increases z reﬂecting Lemma 22 Lemma 28 Assuming SIMPLESLS initialization uniformly random expected hitting times TMCp 5 z 0 cid2 z 5 SIMPLESLS follows h50p 24p4 1870p3 2875p2 625p 7500 8p 52p 520p 3p2 25 h51p 48 875p 39 175p2 17 865p3 194p4 625 32p4p 53p 5p 52p 5 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 969 Fig 6 The expected hitting time hp function noise probability p ﬁve 5bit trap Markov chain models The curves starting h50p dashed brown h51p solid blue h52p dashed green h53p solid red h54p dashed purple For interpretation references color ﬁgure legend reader referred web version article h52p 750p 18 625p2 9670p3 4112p4 1875 h53p 33p4 1465p3 1550p2 750p 1250 64p24p 53p 52p 5 48p34p 53p 5 h54p 912p4 4530p3 3875p2 3250p 8125 384p44p 5 Proof The Markov chains initial probability distribution V π0 π1 π2 π3 π4 π5 according 7 cid13 cid11 cid12 cid11 cid12 5 5 0 2 25 25 cid11 5 1 25 V cid12 cid12 cid11 5 3 25 cid12 cid11 5 4 25 cid12 cid11 5 5 25 cid14 8 We focus h53p In order obtain h53p TMCp 5 3 model form following simultaneous equations expected ﬁrst passage times mi 0 cid2 cid2 5 p p cid13 cid13 m2 1 m1 1 m0 1 1 pm0 pm1 cid14 m0 4 1 4 5 5 cid14 m1 3 1 3 5 5 cid13 1 3 5 cid13 1 4 5 m3 1 3 5 m4 1 4 5 m5 0 pm2 pm3 p p pm2 pm3 cid14 m4 cid14 m5 solved gives ﬁrst passage times m0 m5 follows m0 775p2 375p 205p3 204p4 625 m1 475p2 375p 215p3 60p4 625 300p3 420p4 144p5 300p3 420p4 144p5 970 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 300p3 420p4 144p5 m2 925p2 750p 140p3 24p4 625 m3 285p2 50p 60p3 125 100p2 140p3 48p4 m4 15p 22p2 25 25p 35p2 12p3 m5 0 Introducing 8 compute ET X0 PrX0 miπi 0 cid2 cid2 5 follows m0π0 775p2 375p 205p3 204p4 625 300p3 420p4 144p5 m1π1 475p2 375p 215p3 60p4 625 300p3 420p4 144p5 m2π2 925p2 750p 140p3 24p4 625 m3π3 285p2 50p 60p3 125 100p2 140p3 48p4 300p3 420p4 144p5 cid12 cid11 5 3 25 cid12 cid11 5 0 25 cid12 cid11 5 1 25 cid12 cid11 5 2 25 m4π4 15p 22p2 25 25p 35p2 12p3 cid12 cid11 5 4 25 cid2 5 i0 miπi desired The remaining h5j p j 0 2 j 4 Adding obtain h53p developed similar manner save space proof cid2 Illustrating Lemma 28 graphs Fig 6 impact varying noise p example TMCs The differ ence shapes curves easiest case h50p compared hardest case h54p dramatic One extreme h50p monotonically increasing The extreme h54p ﬁrst monotonically decreasing monotonically increasing Clearly impact optimal noise level discuss 52 Optimal noise level trap Markov chains Fig 6 clearly shows difference optimal noise levels p For h50p intuitively clear use low noise p speciﬁcally p 0 enables SIMPLESLS greedily hillclimb b 11111 50 taking unnecessary downhill noise steps For h54p hand intuitively use high noise p order let SIMPLESLS easily escape trap b 00000 containing local nonglobal optimum Given expected hitting times hp rational functions example derived Lemma 28 optimal noise probabilities derived analytically The following example deriving optimal noise p illustrates use quotient rule Example 29 Consider h53p Theorem 28 16 gives cid6 h 53p 198p6 17 580p5 1850p4 72 250p3 96 250p2 106 250p 46 875 3456p8 20 160p7 43 800p6 42 000p5 15 000p4 solving p hcid6 p 53 06687 53p 0 checking boundaries p 0 p 1 obtain optimal noise probability 53 Discussion trap Markov chains Several points respect Lemma 28 Fig 6 First note convex rational functions form hp P pQp P p Qp polynomials Further short moment disallow use restarts examples illustrate p 0 lead unbounded hitting times hp Fig 6 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 971 The reason p 0 certain unfortunate initializations unbounded search trap TMCs Fig 6 clearly shows unboundedness h5zp z cid3 1 p 0 Similar trapping effects respect local minima place real problem instances illustrating need p 0 MAXFLIPS SLS2 Second trap function setting restricted similar patterns appear experiments literature 1426 Section 7 For instance different SLS algorithms tested 400 hard random 3SAT problem instances variable noise relatively clear noise level performed optimally 26 Note dependent variable fraction problem instances solved mean run time curves concave maximum value optimum convex 26 Along similar lines empirical noise response curves Novelty SLS algorithm convex shapes similar h53p 14 In cases performance improves increasing noise hits optimum performance deteriorates increasing noise The pattern words similar curves h52p h53p h54p The beneﬁt small noise levels illustrated h50p h51p knowledge received attention literature However SLS results areas planning scheduling small noise levels empirically shown optimal 78 Third notice curves h5zp 0 cid2 z cid2 4 closer p increases particular lim p1 h50p lim p1 h54p 887 24 There greater performance difference problem instances small p compared large p Speciﬁcally ﬁx p1 cid6p p1 07 cid6p 01 Now form p p1 cid6p 08 Clearly difference performance problem instances h50p h54p greater p 1 1 This suggest general setting noise level low p cid6p detrimental p setting high p cid6p operating conditions uncertainty p problem instance distribution Similar recommendations fact based experimental observations 17 p1 cid6p 06 p 1 1 We believe results shed additional light signiﬁcant impact varying noise observed experiments 172645 When know problem instance distribution ahead time results argue favor use adaptive noise 1426 6 Hitting time analysis Experimentally observed curves mean SLS run times 1416 fraction solved instances SLS 26 plotted functions noise p shapes suggesting underlying convex functions rational functions hitting times We note convexity hitting times observed Section 51 experiments Section 7 We provide general SLS hitting time results focusing rational functions convexity polyno mials The justiﬁcations bit different cases The condition rationality supported analysis Section 61 For convexity justiﬁcation empirical results literature article For polynomials justiﬁcation partly Weierstrass theorem partly empirical results Section 7 The analysis respect speciﬁc SLS model M O M S V P k S states Further T random variable representing hitting time The results section apply hitting times general depend approximate Markov chain models developed Section 4 Section 5 In words readers approximate Markov chain models restrictive want consider general analysis provided section 61 Single problem instances We consider single problem instances assume noise probability p PrO oN independent parameter Let P p Qp polynomials Based results Section 51 hypothesize 2 An alternative way escaping traps use restarts SLS practitioner likely use nonzero noise noninﬁnite MAXFLIPS While topic restart crucially important SLS main focus article effect restart effect noise SLS detailed discussion joint effect restarts noise scope article 972 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 expected hitting time problem instance form rational function hp P pQp supported following analysis From linear algebra know n n equations solution exactly solution inﬁnitely solutions While theory exist conditions hitting time equations SLS inﬁnitely solutions solution case clearly greatest practical We article assume existence exactly solution Gaussian elimination discussed ﬁnite number steps In proof following theorem key idea perform Gaussian elimination symbolic fashion noise parameter p preserved derivation Theorem 30 Consider SLS model M O Markov Chain M S V P deﬁned bitstring length n noise parameter p Let κ 2n assume optimum states sλ sκ O λ cid2 κ form equations Ms expected ﬁrst passage times mi 1 cid2 cid2 κ There exists equivalent upper triangular U m b m m1 mλ1T coefﬁcients U b rational functions p Proof We form based M equations expected ﬁrst passage times O m1 1 f11pm1 f12pm2 f1κ1pmκ1 f1κ pmκ m2 1 f21pm1 f22pm2 f2κ1pmκ1 f2κ pmκ mλ1 1 fλ11pm1 fλ12pm2 fλ1κ1pmκ1 fλ1κ pmκ mλ 0 cid2 κ i1 fjip 1 1 cid2 j cid2 κ mκ 0 fjip αji βjipγji constants αji βji N γji N Clearly written cid11 cid12 1 f11p m1 f12pm2 f1κ1pmκ1 f1κ pmκ 1 f21pm1 m2 f2κ1pmκ1 f2κ pmκ 1 cid12 1 f22p cid11 cid12 cid11 mλ1 fλ1κ pmκ 1 1 fλ1κ1p fλ11pm1 fλ11pm1 mλ 0 mκ 0 We proceed means induction number elementary operations iteratively creating equations St1from St t cid3 1 Here S1 following created dropping triv ially rational mλ mκ 0 substituting mλ mκ 0 equations performing slight renaming 12pm1 a1 22pm2 a1 1λ1pmλ1 b1 2λ1pmλ1 b1 1 p 2 p a1 11pm1 a1 a1 21pm1 a1 λ11pm1 a1 a1 For base case t 1 a1 tth step t cid3 2 λ12pm2 a1 ij p b1 λ1λ1pmλ1 b1 κ1p p clearly rational Following Gaussian elimination OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 ik ik p at1 r t ij p at1 p bt1 bt ij pat1 kk p r t ik p r t ik p at1 kj bt1 k k 1 cid2 cid2 λ 1 t cid2 k cid2 λ 1 p k 1 cid2 j cid2 λ 1 p k 1 cid2 j cid2 λ 1 t cid2 k cid2 λ 1 973 9 10 11 p at1 Under inductive hypothesis at1 p rational ij follows left hand sides 9 10 11 rational rationality closed addition multiplication applied Rational functions abstract algebra known ﬁelds closed multiplication addition Consequently ﬁnite number Gaussian elimination steps upper triangular U m b obtained coefﬁcients U b rational functions p cid2 p bt1 p at1 kj p bt1 p at1 kk ik k The theorem creates upper triangular systems turn impact Gaussian elimination systems Theorem 31 Suppose equations upper triangular form U m b expected ﬁrst passage times mip 1 cid2 cid2 λ 1 κ Further suppose entries U b nonzero rational functions p Then exists rational function PipQip mip PipQip Proof We perform substitution U m b For arbitrary mip consequently bip mip cid2 λ1 ki1 aikpmkp aiip λ 1 λ 2 1 From Theorem 30 know bip b aikp aiip U rational mip rational closure addition multiplication cid2 The result shows noise probability p impacts expected ﬁrst passages times mi 1 cid2 mi cid2 2n In particular rational function mip PipQip Pip 0 λ cid2 cid2 κ We expected hitting time ET rational function hp assume ﬁrst passage times rational functions Theorem 32 Let ﬁrst passage time mip ET X0 rational function p 1 cid2 cid2 k suppose PrX0 constant Then expected hitting time ET rational function p hp Proof For random variables T C law conditional expectation says ET given cid3 ET ET C iPrC 12 icid11C Here cid11C 1 k ET C ET X0 mip Since rational functions closed multiplication constant PrX0 addition obtain desired result cid2 The following result follows easily results far section gives expected hitting times SIMPLESLS general Corollary 33 Rationality SIMPLESLS hitting time Consider SLS model M O M S V P exact Markov Chain deﬁned bitstring length n noise parameter p The expected hitting time M rational function p hp P pQp P p Qp polynomials Proof From Theorem 31 follows ﬁrst passage times mip PipQip 1 cid2 cid2 2n Pip Qip polynomials Applying Theorem 32 follows expected hitting time hp M rational function hp cid2 974 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 The result generalizes Theorem 28 shows rational functions great analysis SLS In particular analytical counterparts noise response curves experimental literature SLS 78141626 For couple reasons employ polynomial regression rational function regression The ﬁrst reason interval b case b 0 1 0 cid2 p cid2 1 Weierstrass celebrated theorem tells polynomials P x arbitrary good approximations exist Theorem 34 Weierstrass Suppose f x continuous b let ε 0 Then exists polynomial P x cid15 cid15 cid15 ε cid15f x P x cid9 cid9 denotes uniform norm interval b The second reason use polynomial regression better understood widespread rational function regression For reasons use polynomial regression rational function regression experiments Section 7 We provide sufﬁcient condition expected hitting time ET convex function hp Theorem 35 Let ﬁrst passage time mi ET X0 convex function p 1 cid2 cid2 k suppose PrX0 constant Then expected hitting time ET convex function p hp Proof Similar proof Theorem 32 use conditional expectation 12 observe convexity pre served nonnegative multiplication addition obtain desired result cid2 Experimental results giving noise response curves individual problem instances reported Section 71 synthetic instances Section 72 BNs applications 62 Mixtures problem instances We investigate multiple problem instances classes instances ﬁnite mixture distributions Again consider noise probability p PrO oN independent parameter We assume problem instances come probability distribution follows Deﬁnition 36 SLS mixture model Suppose ξ SLS models c1 cξ ci Mi Oi 1 cid2 cid2 ξ ξ i1 PrC ci 1 c1 PrC c1 If SLS model observed probability PrC ci cξ PrC cξ deﬁnes SLS mixture model cid2 Deﬁnition 37 First passage time mixture Consider SLS mixture model c1 PrC c1 cξ PrC cξ Let T conditional ﬁrst passage time random variable PrT t C ci 1 cid2 cid2 ξ The ﬁrst passage time mixture deﬁned PrT t ξcid3 i1 PrT t C ciPrC ci This ﬁnite mixture distribution ξ mixture components There number reasons mixtures interesting stochastic local search Algorithms NPhard problem typically developed class mixture problem instances mind Problem instances dynamically generated generation process induces SLS mixture During early design modeled completely known mixture BNs represent possibilities 28 Finally given CV ratio deﬁnes problem instance mixture 3334 Section 72 empirically investigate mixtures In mixture distinct Markov chain problem instance Informally ﬁrst pick ith Markov chain probability PrC ci use Markov chain PrT t C ci chains passage time Deﬁnition 3 instance hitting time Theorem 5 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 975 Deﬁnition 38 SLS mixture hitting time Consider SLS mixture model ξ components expected hitting times hj p 1 cid2 j cid2 ξ The SLS mixture hitting time H p deﬁned H p ξcid3 j 1 hj pPrC cj Having formally introduced H p function actually To notation simple assume state space size Markov chain Mi SLS mixture model c1 cξ M1 O1 Mξ Oξ Theorem 39 Let SLS mixture model ξ mixture components ﬁrst passage time random variable T The expected hitting time ET mixture hitting time H p ET H p Proof The expected value T mixture given initial state X0 si component C cj SLS mixture model deﬁned similar Deﬁnition 3 ET X0 si C cj Using law conditional expectation 12 obtain ET C cj κcid3 i1 ET X0 si C cj PrX0 si C cj Using 13 law conditional expectation 12 obtain ET ξcid3 j 1 ξcid3 j 1 ET C cj PrC cj hj pPrC cj 13 14 ET C cj hj p Corollary 33 By Deﬁnition 38 14 H p desired result cid2 We turn rational functions motivated results Section 61 Theorem 40 Consider SLS mixture model c1 PrC c1 cξ PrC cξ Suppose individual component hitting times hip 1 cid2 cid2 ξ rational functions Then SLS mixture hitting time H p rational function Proof Since hip assumption rational function written hip PipQip Pip Qip polynomials Due closure multiplication PrC cihip PrC ciPipQip rational function There closure addition H p ξcid3 i1 PrC cihip deﬁnition H p rational function cid2 From results expected hitting time SIMPLESLS mixture problem instances derived Corollary 41 Rationality SIMPLESLS mixture Consider SLS mixture model c1 PrC c1 cξ PrC cξ Suppose ci Mi Oi 1 cid2 ci cid2 ξ exact SIMPLESLS model noise parameter p The expected hitting time mixture rational function p H p P pQp P p Qp polynomials 976 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Proof From Corollary 33 follows SIMPLESLS hitting time hip 1 cid2 cid2 ξ rational function Applying Theorem 40 conclude mixture H p rational function cid2 The trap Markov chain results presented Sections 43 5 originally intended models individual problem instances However hitting time problem instance rational function followsas stated abovethat hitting time mixture problem instances class example deﬁned partic ular CV ratio rational function Hence use trap Markov chain results model hitting time mixture problem instances This mathematical consequence functional form hitting times artifact analysis We turn attention convex functions Theorem 42 Suppose problem instance hitting times hip 1 cid2 ci cid2 ξ convex functions Then mixture hitting time H p convex function Proof Since hip convex PrC cihip convex fact convexity preserved nonnegative multiplication Further H p ξcid3 i1 PrC cihip convex convexity preserved addition cid2 Convexity important local optimality means global optimality convex functions simplifying optimization algorithms Polynomials helpful noise optimization Section 72 contains experiments mixtures problem instances polynomial regression results 63 Optimal noise levels What optimal value p SLS noise parameter p This question discussed light analysis earlier section Deﬁnition 43 Hitting time minimization Let SLS noise p let hp expected hitting time The optimal noise probability minimizing hp deﬁned p h arg min 0cid2pcid21 hp minimal expected hitting time h hp h 15 We note hp 15 expected hitting time problem instance hp hip problem h respect problem instance instance distribution hp H p Optimal noise level p mixture problem instances If certain assumptions form hp progress We consider differen tiable hp certainly rational function polynomial We derivative hcid6p order ﬁnd optimal SLS noise level p h solve equation hcid6p 0 Forming hcid6p polynomial hp trivial mathematical fact general case polynomials complex roots In words solving hcid6p 0 r cid6p 0 rp expected run time Section 64 complex solutions Certain strict subsets polynomials instance polynomials real coefﬁcients odd degree real root Given setting options hp rp odd degree hcid6p 0 r cid6p degree root Disregard hp rp degree priori real root hcid6p 0 r cid6p 0 guaranteed ii Not disregard hp rp degree priori guarantee real roots cases example cases Table 3 containing r cid6p real roots provide useful insight In noise optimization algorithm OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 977 based computing hcid6p 0 prefer follow approach In article purpose analysis insight optimization algorithm development prefer ii When hp rational function hp P pQp wellknown quotient rule form cid6 h p QpP cid6p P pQcid6p Qp2 16 In addition better understanding noise phenomenon results servein future h For example rational function polynomial h estimated empirically notation ˆp h researchas basis improved algorithms computing p gression play role improved algorithms When p terminology optimized noise level estimated optimal noise level 64 Initialization restart Using advanced initialization algorithms initialization uniformly random proven powerful way improve SLS performance 2230313637 SIMPLESLS clearly support different ways initializing b SGS fact takes initialization algorithm portfolio I input In Markov chain M initial distribu tion V needs reﬂect particular initialization algorithm portfolio In general initialization algorithm operator distinct initialization distribution V Theorem 28 adapted correspondingly reﬂect Another technique restarts MAXFLIPS shown beneﬁcial SLS 38 systematic search 10 In cases restarts play central role SLS close optimal MAXFLIPS value essential strong performance 3843 TMCs observed Section 53 In recent research approach dynamically optimizing SLS restart parameter MAXFLIPS developed 4142 based learned Bayesian networks predict inference run times 19 In general consequently important distinguish expected run time expected hitting time We formally introduce expected run time Deﬁnition 44 Expected run time Let X random variable number ﬂips performed SIMPLESLS noise probability p The expected run time number ﬂips deﬁned rp EX Note concepts run time expected run time necessarily use Markov chains Our hitting time results hand rely use Markov chains Unfortunately SLS restarts place MAXFLIPS cid8 SIMPLESLS violate Markov property3 Expected run times welldeﬁned restarts occur interested minimization Deﬁnition 45 Run time minimization Let rp expected run time number ﬂips SIMPLESLS The optimal noise probability minimizes rp deﬁned p r arg min 0cid2pcid21 rp minimal expected run time r rp r p If MAXFLIPS obviously p r h general clear p h In theoretical article discussing optimal noise probability respect expected hitting time p h experimental article addition discussing optimal noise probability respect expected run time p h referred simply optimal noise probability p In Section 7 empirically investigate estimates expected hitting times ˆhp expected run times ˆrp similar techniques It turns noise response curves expected run times similar expected hitting times r As clear context p r p cid8 p r 3 In making claim assume Markov chain states reﬂect number ﬂips One expand exact Markov chain model reﬂect number ﬂips SIMPLESLS scope work 978 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 7 Experiments Our analysis certain assumptions simpliﬁcations raise questions following Is relationship trap Markov chain model real problem instances What SLS search behavior sets problem instances corresponding SLS mixtures What impact varying noise SLS run times realistic largescale problem instances applications To answer questions report real search behavior experiments Bayesian networks ing SGS SLS There strong evidence problem instance hard SLS algorithm hard SLS algorithms 17 investigate SLS depth Stochastic greedy search SGS simulate SIMPLESLS computes MPEs supports multiple search operators initialization operators 2730 SGS input parameters βa BN f MPE probability Prx Ia portfolio ini tialization algorithms Sa portfolio search algorithms MAXFLIPSthe number ﬂips restart MAXTRIESthe number tries termination For S focus greedy operators named BM GM noisy operators named NU BS GS NU implements uniform noise oN described Section 3 BS GS noisy biased improving current explanations probability BM GM corre spond oG pure hillclimbers maximize gain noise Without going covered 2730 sufﬁce BM BS operate gain probability based 2 GM GS probabilistic generalizations GSAT gain 4546 turned powerful BNs deterministic nodes 2730 In Section 71 based 100 small synthetic BNs investigate connection trap functions trap Markov chains real SGS search behavior In Section 72 investigate real SGS search behavior 800 syn thetic BNs emphasizing BNs varying CV ratio approximation polynomial regression In Section 73 experiment SGS application BNs investigate noise strategies uniform random noise advanced initialization algorithms initialization uniformly random 71 Experiments small synthetic Bayesian networks What relationship TMC models Sections 4 5 real SLS search behavior In order clearly link TMC analysis experimental parts article investigate 100 randomly generated 3SAT problem instances represented BNs search space size example trap Markov chains analysis In following ﬁrst discuss sample synthetic problem instances generated discuss noise response experiments satisﬁable problem instances sample A detailed analysis follows complete search spaces interesting problem instances analytically derive Markov chains compare analytical results empirical SGS search behavior 711 Methodology generating small synthetic Bayesian networks Let Bayesian network BN V number root nodes C number nonroot nodes It demonstrated ratio CV key parameter SAT BN inference hardness randomly generated problem instances 273334 For BNs CV ratio predict upper lower bounds optimal maximal clique size treewidth induced clique tree bipartite BNs randomly generated BPART algorithm 272933 The BPART algorithm input parameters QCPT type root nodes F CPT type nonroot nodes V number root nodes Cthe number leaf nodes Sthe number states node P number parents nonroot node Rregularity BNs underlying graph The input parameters BPART set follows generate 3SAT BNs experimentation The CPT type root nodes Q uniform CPT type nonroot nodes F number root nodes V 5 number leaf nodes C 20 number states node S 2 number parents leaf node P 3 irregular BNs created setting R false This gives CV 40 lies somewhat phase transitions SAT 34 meaning generated problems satisﬁable Using parameter settings 100 problem instances generated The existence satisfying assignments checked processing BNs clamped leaf nodes HUGIN tree clustering implements tree clustering algorithm 524 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 979 The SGS 273031 restarts uniform initialization I search portfolio Sp pUN 1 p GM employed varying noise probability Following standard methodology experiments SGS conducted instances satisfying assignments 712 Noise experiments small synthetic Bayesian networks Experimentation progressed phases First noise responses satisﬁable problem instances gen erated empirically varying noise level p 01 p 09 increments cid6p 005 The empirically observed optimal values noise parameter ranged ˆp 01 ˆp 07 Second detailed studies performed focusing expected hitting time results derived analytically real problem instances ii real SGS search behavior problem instances iii polynomial regression results based SGSs search behavior varying p The complete search spaces easiest hardest problem instances sample denoted β81 β8 respec tively illustrated Fig 7 Let b 0 15 range almostsatisfying assignments deﬁned assignments C 1 19 satisﬁed clauses Such almostsatisfying assignments form local optima trap SIMPLESLS search process From Fig 7 β81 β8 almostsatisfying assignments For easy problem instance β81 Hamming distance optimum b db b 1 db b 2 cases Clearly search space states db b 1 form local optima β81 For hard problem instance β8 hard db b 3 db b 4 case almostsatisfying assignments 19 states form local optimum More speciﬁcally 19 states β81 connected optimal 20 state neighbor Hence 19 states form plateau 20 reached relative ease In β8 19 states connected harder problem instance states form local optimum 20 neighbor Based inspection search spaces expect β81 easier β8 SIMPLESLS β81 trap search process degree More generally illustrates instances easier number relative location almostoptimal states generally local optima search space idealized form illustrated trap functions To provide detailed quantitative analysis created distinct Markov chains based prob lem instances taking account behavior SGS These Markov chain models denote MCp β81 MCp β8 derived inspecting complete search spaces β8 β81 respectively These approximate Markov chains shown Fig 8 In following table compare PrXi1 3 Xi 2 PrXi1 4 Xi 3 lead search optimum TMCp 5 0 TMCp 5 4 MCp β81 MCp β8 TMCp 5 0Very easy MCp β81Easiest sample MCp β8Hardest sample TMCp 5 4Hard PrXi1 3 Xi 2 10 04p 085 025p 023 037p 06p PrXi1 4 Xi 3 10 06p 078 038p 01 03p 04p Just considering signs coefﬁcients p table clearly similarity TMCp 5 0 MCp β81 hand TMCp 5 4 MCp β8 For TMCp 5 0 MCp β81 increasing noise parameter p decreases probabilities PrXi1 3 Xi 2 PrXi1 4 Xi 3 SIMPLESLS moves optimal state s 5 This reﬂects pure pure hillclimbing minimal noise p optimal underlying search spaces For TMCp 5 4 MCp β8 hand increasing noise parameter p increases probabilities PrXi1 3 Xi 2 PrXi1 4 Xi 3 SIMPLESLS moves s 5 This shows order counteract search trapped parts search space contain optima noise parameter p increased There similarity search traps observed idealized TMC models search traps reﬂected Markov chains real problem instances β8 β81 With Markov chains MCp β81 MCp β8 hand analytically derive expected hitting time curves compare empirical noise response curves In Fig 9 expected hitting times derived extreme trap Markov chains TMCs ii expected hitting time curves run times form rational 980 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 7 The complete search spaces randomly generated BNs represent instances satisﬁability problem 3SAT V 5 variables C 20 clauses We instances satisfying assignment easiest hardest problem instances random sample 100 BNs The number satisﬁed clauses shown state search space Fig 8 Markov chains created considering search spaces problem instances β81 β8 behavior SIMPLESLS Fig 7 shows underlying search spaces functions derived Markov chains Fig 8 iii data points reporting real SGS behavior problem instances There good correspondence analytical hitting time results ii observed SGS run times iii For ii iii provides lower bounding hitting time h50p upper bounding hitting time h54p To knowledge similar displays compare analytical experiments results reported literature earlier We learned closedform expressions expected hitting times β81 β8 Next determine experimentally polynomial regression approximations ˆP p SGS run times Our polynomial approx imations shown Table 1 Fig 10 present empirical results regression curves Regression results signiﬁcant according R2 values Fig 10 shows good correspondence empirical analytical results The polynomial regression lines close interesting endpoints domain 01 09 typically case 72 Experiments large synthetic Bayesian networks In section systematically simultaneously vary hardness problem instances SLS noise probability We report observed SGS search behavior ﬁtted polynomials Here real search results BNs corresponding SAT instances reported V 30 CV 20 CV 34 CV value 100 problem instances generated searched Hence real search behavior SGS 800 problem instances summarized analyzed section To knowledge similar noise response experiments reported literature earlier OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 981 Fig 9 The expected hitting time h function noise probability p extreme problem instances β8 β81 black line β81 red line β8 picked sample 100 actual SGS behavior instances black squares β81 red circles β8 Expected hitting times h50p dashed brown h54p dashed purple 5bit trap Markov chain models TMCp 5 0 TMCp 5 4 shown For interpretation references color ﬁgure legend reader referred web version article Table 1 Regression polynomials order k 4 k 5 k 6 created empirical SGS search data extreme problem instances β81 β8 Easiest Hardest sample respectively Instance Order β81 β81 β81 β8 β8 β8 4 5 6 4 5 6 Polynomial approximation ˆhp 11272p4 16074p3 89826p2 1369p 50822 36836p5 20941p4 2559p3 1331p2 22627p 57406 24555p6 76979p5 94283p4 57707p3 18409p2 29485p 13635 49522p4 11518p3 10212p2 41825p 94197 6825p5 22015p4 27244p3 16738p2 53631p 10133 11616p6 28024p5 18788p4 37205p3 98267p2 44052p 9657 R2 09973 09973 09983 09959 09970 09971 Fig 10 Comparison analytical hitting times derived Markov chains MCp β black dashed line ii empirical data points generated real SGS search behavior black circles iii polynomial regression curves estimated empirical data points The polynomial regressions lines order k k 4 green line k 5 red line k 6 blue line Left Results problem instance β81 Right Results problem instance β8 For interpretation references color ﬁgure legend reader referred web version article 721 Methodology generating large synthetic Bayesian networks For experiments reported SATlike BNs generated BPART approach discussed Section 71 Here BPARTs input parameters set follows generating larger problem instances compared Section 71 The CPT type root nodes Q uniform CPT type nonroot nodes F number root nodes V 30 number states node S 2 number 982 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 11 Experimental results synthetic BNs CV ratios ranging CV 20 CV 34 Sample means run times number ﬂips shown function SLS noise probability p Note piecewise linear functions convex parents leaf node P 3 irregular BNs created setting R false We varied number leaf nodes keeping V 30 constant giving CV ratios varying CV 20 CV 34 This satis ﬁable region SAT 34 solutions exist high probability The existence solutions checked processing BNs HUGIN tree clustering implements tree clustering algorithm 524 722 Noise experiments large synthetic Bayesian networks The purpose second set BN experiments investigate combined effect varying noise p varying hardness larger realistically sized problem instances For purpose experiments measured instance hardness means CV ratio The stochastic local search algorithm SGS 2730 employed restart parameter value MAXFLIPS optimized CV ratio p hand The search portfolio SN p p UN 1 p GM noise probability varying p 01 p 07 increments cid6p 01 Initialization uniformly random I Fig 11 summarizes experimental results form noise response curves Each BN searched 100 times SGS CV ratio 100 BNs generated data point Fig 11 represents 10000 suc cessful searches SGS There different CV ratios ranging CV 20 CV 34 The noise probability p varied reﬂected xaxis Fig 11 The yaxis measures mean number ﬂips optimum b For relatively easy CV 20 BNs sample mean monotonically increasing noise p experimentally determined global minimum run time ˆr minimal noise level investigated ˆp 01 For hardest problem instances CV 34 sample mean ﬁrst monotonically decreasing increasing function p Here experimentally determined optimal noise level ˆp 04 For CV 26 sample average close constant p 01 p 04 increases monotonically Results intermediate CV ratios 20 CV 26 26 CV 34 similar CV 20 case CV 34 case respectively As argued Section 62 similar analysis approaches mixtures sampled CV ratio individual problem instances The noise response curves Fig 11 similar reported TMCs Fig 6 smaller problem instances Fig 10 illustrating results carry individual problem instances mixtures problem instances predicted analysis Section 6 Polynomial approximation results shown Table 2 Fig 12 For CV ratio alternatives polynomials orders 4 5 6 Polynomials smaller order good results From CV 20 CV 28 polynomials similar results From CV 30 CV 34 polynomials order k 4 provide visibly poorer results seen Fig 12 R2 Table 2 Qualitatively results consistent analysis Sections 4 5 6 earlier experiments 16172645 A key point increasing average problem instance hardness controlled CV ratio OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 983 Table 2 Polynomial approximations SGS sample average run times ﬂips function noise level p varying CV ratios The polynomials determined experimental data nonlinear regression For CV ratio alternatives given polynomials order 4 5 6 CV Order 20 20 20 22 22 22 24 24 24 26 26 26 28 28 28 30 30 30 32 32 32 34 34 34 4 5 6 4 5 6 4 5 6 4 5 6 4 5 6 4 5 6 4 5 6 4 5 6 Polynomial approximation ˆrp y 5129p4 5302p3 2242p2 3088p 1543 y 1500p5 2486p4 1694p3 5256p2 8083p 9771 y 2798p6 5216p5 3861p4 1298p3 2059p2 5501p 1351 y 1255p4 1568p3 7409p2 1331p 2717 y 1112p5 9687p4 8131p3 1849p2 5021p 2297 y 1285p6 1973p5 1947p4 1293p3 5209p2 8987p 2469 y 1554p4 1766p3 7582p2 1272p 3384 y 2249p5 2944p4 1570p3 3664p2 4040p 2536 y 8333p6 2491p5 1054p4 6795p3 1486p2 1469p 2648 y 1998p4 2186p3 9199p2 1688p 5150 y 4797p5 7596p4 4930p3 1479p2 1886p 3341 y 1246p6 1807p5 4770p4 3597p3 1153p2 1501p 3508 y 3732p4 4380p3 1941p2 3926p 8594 y 8302p5 12872p4 7935p3 2210p2 2259p 5463 y 27081p6 73297p5 74297p4 36887p3 9289p2 1061p 1842 y 9418p4 12094p3 5708p2 1179p 1758 y 23729p5 38041p4 23104p3 6157p2 5889p 8635 y 22734p6 78291p5 89606p4 47409p3 12100p2 1290p 5595 y 22176p4 29756p3 14929p2 3402p 4388 y 46501p5 70825p4 39220p3 8321p2 6177p 2634 y 270595p6 602928p5 542934p4 250071p3 62412p2 8286p 6253 y 39431p4 51539p3 24947p2 5500p 6982 y 82600p5 125769p4 70985p3 16353p2 6536p 3867 y 165545p6 314708p5 249717p4 105998p3 26920p2 4454p 6081 R2 09998 10 10 09999 10 10 09999 10 10 09996 10 10 09994 10 10 09984 10 10 09981 09996 10 09985 10 10 corresponds moving slopechange state z trap Markov chain optimum state Fig 5 In words problem instances easy average corresponding TMCs slopechange states z close 00 0 solved greedier SLS algorithm problem instances hard average corresponding TMCs slopechange state z close 11 1 We note piecewise linear curves convex CV ratios What novel compared earlier experiments know different pattern easy CV 20 case versus hard CV 34 case extensive regression analysis polynomials clear display convexity wide range CV 723 Optimal noise level experiments large synthetic Bayesian networks From earlier analysis experiments know noise probability signiﬁcant impact number search steps needed reach optimum How optimal noise probability p change CV ratio changes This research question investigated section We wanted minimize number ﬂips function CV ratio We ﬁrst approximate optima ˆrp ˆr cid6p based regression polynomials reported Table 2 This taking derivatives d dp solving equation ˆr cid6p 0 respect p order compute optimized noise ˆp The results shown Table 3 For value CV empirical optimum regression polynomials different orders presented In cases ˆr cid6p 0 multiple candidate solutions obvious reasonable choose reﬂected table Further cases complex solutions case CV 24 polynomial order 6 solution zero set zero As discussed Section 63 degree polynomials guaranteed real roots Given fact opted include Table 3 cases r cid6p degree 4 Even guarantee real root cases speciﬁcally cases Table 3 containing r cid6p real root additional insight provided 984 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 12 Polynomial approximations SGS average run times ﬂips function noise level p BNs varying CV ratios For CV ratio polynomial regression lines varying order k presented k 4 line indicated green crosses k 5 line indicated red squares k 6 line indicated blue diamonds The means measured data connected straight lines black included For interpretation references color ﬁgure legend reader referred web version article Clearly good correspondence polynomials empirical results especially polynomials order k 6 higher CV values Considering Fig 12 excluding CV 20 CV 22 k 5 k 6 similar results ˆp 73 Experiments application Bayesian networks It great consider advanced initialization noise strategies problem instances applications Here report empirical SGS results application BNs taken Friedmans Bayesian Network Repository httpwwwcshujiacillabscompbioRepository The application BNs investigated OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 985 Table 3 Noise optimization based derivatives polynomial approximations SGS sample average run times number ﬂips function noise level p x varying CV ratios For CV ratio alternatives given polynomial order 4 5 6 In addition optimal noise levels experiments shown CV Order 20 20 20 20 22 22 22 22 24 24 24 24 26 26 26 26 28 28 28 28 30 30 30 30 32 32 32 32 34 34 34 34 4 5 6 NA 4 5 6 NA 4 5 6 NA 4 5 6 NA 4 5 6 NA 4 5 6 NA 4 5 6 NA 4 5 6 NA Derivative polynomial approximation ˆrcid6p 20516p3 1591p2 4484p 3088 7498p4 9945p3 5083p2 10512p 8083 7712p5 9864p4 7786p3 3878p2 1042p 8987 Empirical 5020p3 4704p2 1482p 1331 5560p4 3875p3 243 9p2 3698p 50 21 7712p5 9864p4 7786p3 3878p2 1042p 8987 Empirical 6215p3 5297p2 1516p 1272 11246p4 11 778p3 4711p2 7328p 4040 5000p5 1245p4 4217p3 2038p2 2971p 1469 Empirical 7994p3 6559p2 1840p 168 8 23986p4 30383p3 14789p2 2957p 1886 7476p5 9034p4 19079p3 10792p2 2306p 1501 Empirical 14929p3 13140p2 3882p 3926 41 512p4 51 488p3 23 805p2 4420p 2259 162 486p5 366 485p4 297 188p3 110 661p2 18 578p 1061 Empirical 37671p3 36 282p2 11416p 1179 118645p4 152 164p3 69 312p2 12314p 5889 136 404p5 391 455p4 358 424p3 142 227p2 24 200p 1290 Empirical 88 704p3 89 268p2 29 858p 3402 232 505p4 283 300p3 117 660p2 16643p 6177 1623 570p5 3014 640p4 2171 736p3 750 213p2 124 824p 8286 Empirical 157 724p3 154 617p2 49 894p 5500 413 000p4 503 076p3 212 955p2 32 706p 6536 993 270p5 1573 540p4 998 868p3 317 994p2 3840p 4454 Empirical Optimized noise ˆp 2 995 10 Complex 155 10 01 2 0149 0148 0147 01 02 0144 24 Complex 0 01 0196 0279 0280 03 0366 0327 0305 03 0412 0315 0307 03 0437 0355 0384 04 0448 0389 0394 04 Mildew Munin1 Pir3 Water The Mildew BN determining fungicides use counter act mildew attacks wheat The Munin1 network medical BN ﬁeld electromyography 2 The Pir3 BN information ﬁltering purpose battleﬁeld situation awareness 2132 The Water BN models biological processes water puriﬁcation The purpose experiments investigate effect varying p investigate SLS strategies uniform random noise initialization uniformly random More speciﬁcally compared following search algorithm portfolios SGp SU p Deﬁnition 46 Guided noise The guided noise portfolio SGp function noise probability p deﬁned cid13 cid14cid16 cid8cid13 cid14 cid14 cid13 cid13 cid14 SGp p 2 p 2 BS GS BM GM 1 p 2 1 p 2 Deﬁnition 47 Uniform noise The uniform noise portfolio SU p function noise probability p deﬁned cid8cid13 cid14 cid13 cid14 cid13 cid14cid16 SU p p NU BM GM 1 p 2 1 p 2 986 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 13 Empirical results Water BN different conditions 1 2 3 4 In cases noise probability p varying p 01 p 08 shown xaxis The mean run time measured ﬂips displayed logarithmic scale yaxis Each point represents sample mean 1000 runs For condition different values MAXFLIPS investigated shown In experiments SGp SU p search algorithm portfolio SGS We note experi ments SGp use uniform random noise advanced approaches noisy search In addition compared IU initialization uniformly random versus IG guided initialization varied MAXFLIPS 731 Varying noise initialization restart point One application BN The purpose ﬁrst set experiments establish effect different variants SGS particular BN Water Speciﬁcally varied I S We studied impact varying noise p p 01 p 08 conditions different noise initialization portfolios different values MAXFLIPS restart parameter The orthogonal dimensions investigated Initialization portfolio I Uniform initialization IU versus guided initialization IG Here uniform means initial ization uniformly random guided means use forward simulation 13 Search portfolio S Uniform noise SU p versus guided noise SGp The conditions investigated 1IU SU p 2IG SU p 3IU SGp 4IG SGp Fig 13 presents results different conditions form sample means piecewise linear approximations ˆrp In cases ˆrp convex close convex There different run time responses changes noise depending initialization operator I noise operator S value MAXFLIPS parameter Clearly condition 4 shortest run time overall average run time shortest impact varying p ˆrp small Condition 1 longest run times impact varying p large Overall greatest impact ˆrp uniform noise SU p row 1 2 guided noise SGp row 3 4 For uniform noise impact varying noise p dramatic ˆp cid2 04 given MAXFLIPS level cases For guided noise hand effect varying p ˆrp minimal Further surprisingly ˆp 3 4 large cases MAXFLIPS 50 3 Overall nontrivial interactions BN SLS algorithm parameters OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 987 noise p illustrated However piecewise linear approximation convex close convex cases supporting analytical results 732 Varying noise restart point All application BNs The purpose second set application BN experiments establish effect varying noise value MAXFLIPS different applications BNs Two variants SGS SGS uniform noise SU p SGS guided noise SGp Each SGS variant tested different BNs Mildew Munin1 Pir3 Watergiving total conditions shown Fig 14 Further BNspeciﬁc optimal initialization algorithmeither forward simulation 13 randomized variant Viterbi algorithm 31 49was I application BN Forward simulation Munin1 Water Mildew Pir3 respectively initialized forward backward variants randomized Viterbi algorithm For condition noise varied p 01 p 08 different values MAXFLIPS restart parameter Results form sample means piecewise linear approximations ˆrp reported Fig 14 Each data point represents mean 1000 runs In general guided noise SGp performed better uniform noise SU p effect estimated run time ˆrp increasing noise varied dramati cally problem instances For Pir3 initialization strong increasing noise hurt performance For Munin1 marked difference uniform noise guided noise Uniform noise hurt performance guided noise MAXFLIPS 10 minor impact run time Water Mildew somewhat similar performance Low levels uniform noise helpful values MAXFLIPS values MAXFLIPS increasing noise p 01 help For Mildew high levels guided noise op timal MAXFLIPS levels investigated This case Water MAXFLIPS 10 Except curves Pir3 piecewise linear curves clearly convex close convex supporting analytical results 8 Conclusion future work The use randomization form noisy initialization noisy local search steps empirically shown dramatic positive impact performance local search 781417182627304546 Consequently stochastic local search SLS algorithms currently strong performers areas automated reasoning including problems satisﬁability SAT propositional logic 11184546 computing MPE MAP Bayesian networks 2227303637 Previous research stochastic local search predominantly experimental need theoretical foundations 16 We article based discrete Markov chain models derived simple general SLS algorithm called SIMPLESLS developed theoretical foundation role noise stochastic local search Analytically hitting time analysis Markov chain models obtain results Curves expected hitting times analytical counterpart empirical noise response curves reported literature 78141626 Our analysis shows expected hitting times individual instances mixtures instances rational functions noise p independent variable We emphasize use polynomials convex functions Analytically impact noise contextdependent shape expected hitting time curve depends strongly problem instance hand In order improve understanding interaction noise presence local maxima simple class trap functions based deceptive functions 6 characteristics introduced global maximum ii local deceptive maximum maximal distance global maximum iii slope change location controlling size region greedy search sufﬁcient reach global maximum Obviously crucial issues involved SLS applied NPhard problems Our model derive trap Markov chains expected hitting times highlights problem local maxima trap search process careful application noise helps escaping traps Trap functions closely related search space trapsportions search space attractive SLS contain solutions 1115 Our results include experiments In area stochastic local search algorithm SGS stochas tic greedy search computing MPEs Bayesian networks 2730 Using SGS experimented synthetic Bayesian network varying difﬁculty measured terms CV ratio 33 In experiments illustrated Markov chain models relevant real problem instances For instance good correspondence 988 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 Fig 14 Empirical results BNs Pir3 Munin1 Water Mildew different experimental conditions In cases noise probability p varying p 01 p 08 shown xaxis The mean run time measured ﬂips displayed logarithmic scale yaxis Each point represents sample mean 1000 runs Two different noise mechanisms Uniform Guided ﬁve different values MAXFLIPS MAXFLIPS 10 MAXFLIPS 1000 In cases value noise probability signiﬁcant impact SLS run time expected hitting time results derived analytically real problem instances SGSs behavior problem instances polynomial regression results These noise response curves expected similar extreme bounding hitting times extreme trap Markov chains deﬁned search space We sampled mixtures problem instances deﬁned CV SGS generated noise sponse curves consistent hitting time analysis Here performed extensive polynomial approximation OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 989 experiments Finally experiments SGS application Bayesian networks performed Here input parameters SGSincluding restarts initialization algorithms noise algorithmswere varied addition level noise Results consistent analysis general We conclude outlining areas future work First natural extension research analyze optimize joint effect multiple SLS parameters taking account varying distributions problem instance inputs including Bayesian networks convex optimization response surface techniques Second given beneﬁt guided noise observed experiments fruitful study approaches analytically Third work provides framework improved analysis algorithms formalized means Markov chains instance genetic evolutionary algorithms Fourth optimal noise level vary different subproblems problem instance different problem instances problem instance distributions Following line reasoning pay adapt noise level problem instance try different tries While results adaptive noise exist 1426 hope analysis provided inspire research area Acknowledgements This material based work supported NASA award NCC21426 The anonymous reviewers acknowledged comments helped improve article References 1 AM Abdelbar SM Hedetnieme Approximating MAPs belief networks NPhard theorems Artiﬁcial Intelligence 102 1998 2138 2 S Andreassen M Woldbye B Falck SK Andersen MUNINA causal probabilistic network interpretation electromyographic ﬁndings In Proceedings Tenth International Joint Conference Artiﬁcial Intelligence Milan Italy August 1987 pp 366372 3 E CantuPaz Markov chain models parallel genetic algorithms IEEE Transactions Evolutionary Computation 4 3 2000 216226 4 FG Cooper The computational complexity probabilistic inference Bayesian belief networks Artiﬁcial Intelligence 42 1990 393 405 5 AP Dawid Applications general propagation algorithm probabilistic expert systems Statistics Computing 2 1992 2536 6 K Deb DE Goldberg Analyzing deception trap functions D Whitley Ed Foundations Genetic Algorithms II Morgan Kaufmann San Mateo CA 1993 pp 93108 7 A Fukunaga G Rabideau S Chien Robust local search spacecraft operations adaptive noise Proceedings 4th International Workshop Planning Scheduling Space IWPSS04 Darmstadt Germany 2004 8 A Gerevini A Saetti I Serina An empirical analysis heuristic features local search LPG Proceedings Fourteenth International Conference Automated Planning Scheduling ICAPS 2004 Whistler British Columbia Canada 2004 pp 171180 9 DE Goldberg P Segrest Finite Markov chain analysis genetic algorithms JJ Grefenstette Ed Genetic Algorithms Their Applications Proceedings Second International Conference Genetic Algorithms Erlbaum Hillsdale NJ 1987 pp 18 10 CP Gomes B Selman H Kautz Boosting combinatorial search randomization Proceedings Fifteenth National Conference Artiﬁcial Intelligence AAAI98 Madison WI 1998 pp 431437 11 PW Gu J Purdom J Franco BW Wah Algorithms satisﬁability SAT problem A survey Satisﬁability Problem Theory Applications DIMACS Series Discrete Mathematics Theoretical Computer Science American Mathematical Society 1997 pp 19 152 12 G Harik E CantuPaz DE Goldberg BL Miller The gamblers ruin problem genetic algorithms sizing populations Proceedings IEEE Conference Evolutionary Computation Indianapolis IN 1997 pp 712 13 M Henrion Propagating uncertainty Bayesian networks probabilistic logic sampling Uncertainty Artiﬁcial Intelligence 2 Elsevier Amsterdam 1988 pp 149163 14 HH Hoos An adaptive noise mechanism WalkSAT Proceedings Eighteenth National Conference Artiﬁcial Intelligence AAAI02 Edmonton Alberta Canada 2002 pp 655660 15 HH Hoos A mixturemodel behaviour SLS algorithms SAT Proceedings Eighteenth National Conference Artiﬁcial Intelligence AAAI02 Edmonton Alberta Canada 2002 pp 661667 16 HH Hoos T Stützle Towards characterisation behaviour stochastic local search algorithms SAT Artiﬁcial Intelligence 112 1 2 1999 213232 17 HH Hoos T Stützle Local search algorithms SAT An empirical evaluation Journal Automated Reasoning 24 4 2000 421481 18 HH Hoos T Stützle Stochastic Local Search Foundations Applications Morgan Kaufmann San Francisco CA 2005 19 E Horvitz Y Ruan C Gomes H Kautz B Selman D Chickering A Bayesian approach tackling hard computational problems Proceedings 17th Annual Conference Uncertainty Artiﬁcial Intelligence UAI01 Seattle WA 2001 pp 235244 20 F Hutter HH Hoos T Stützle Efﬁcient stochastic local search MPE solving Proceedings Nineteenth International Joint Conference Artiﬁcial Intelligence IJCAI05 Edinburgh Scotland 2005 pp 169174 990 OJ Mengshoel Artiﬁcial Intelligence 172 2008 955990 21 P Jones C Hayes D Wilkins R Bargar J Sniezek P Asaro OJ Mengshoel D Kessler M Lucenti I Choi N Tu J Schlabach CoRAVEN Modeling design multimedia intelligent infrastructure collaborative intelligence analysis Proceedings International Conference Systems Man Cybernetics San Diego CA October 1998 pp 914919 22 K Kask R Dechter Stochastic local search Bayesian networks Proceedings Seventh International Workshop Artiﬁcial Intelligence Statistics Fort Lauderdale FL January 1999 Morgan Kaufmann 1999 23 VG Kulkarni Modeling Analysis Design Control Stochastic Systems Springer New York 2005 24 S Lauritzen DJ Spiegelhalter Local computations probabilities graphical structures application expert systems discussion Journal Royal Statistical Society series B 50 2 1988 157224 25 DJC MacKay Information Theory Inference Learning Algorithms Cambridge University Press Cambridge UK 2002 26 D McAllester B Selman H Kautz Evidence invariants local search Proceedings 14th National Conference Artiﬁcial Intelligence AAAI97 Providence RI 1997 pp 321326 27 OJ Mengshoel Efﬁcient Bayesian network inference Genetic algorithms stochastic local search abstraction PhD thesis Department Computer Science University Illinois UrbanaChampaign Urbana IL April 1999 28 OJ Mengshoel Designing resourcebounded reasoners Bayesian networks System health monitoring diagnosis Proceedings 18th International Workshop Principles Diagnosis DX07 Nashville TN 2007 pp 330337 29 OJ Mengshoel Macroscopic models clique tree growth Bayesian networks Proceedings TwentySecond National Conference Artiﬁcial Intelligence AAAI07 Vancouver British Columbia 2007 pp 12561262 30 OJ Mengshoel D Roth DC Wilkins Stochastic greedy search Computing probable explanation Bayesian networks Technical Report UIUCDCSR20002150 Department Computer Science University Illinois UrbanaChampaign Urbana IL February 2000 31 OJ Mengshoel D Roth DC Wilkins Initialization restart stochastic local search Computing probable explanation Bayesian networks IEEE Transactions Knowledge Data Engineering 2007 submitted publication 32 OJ Mengshoel DC Wilkins Raven Bayesian networks humancomputer intelligent interaction MS Vassiliou TS Huang Eds Computer Science Handbook Displays Rockwell Scientiﬁc Company 2001 pp 209219 33 OJ Mengshoel DC Wilkins D Roth Controlled generation hard easy Bayesian networks Impact maximal clique tree tree clustering Artiﬁcial Intelligence 170 1617 2006 11371174 34 D Mitchell B Selman HJ Levesque Hard easy distributions SAT problems Proceedings Tenth National Conference Artiﬁcial Intelligence AAAI92 San Jose CA 1992 pp 459465 35 J Park Using weighted MAXSAT engines solve MPE Proceedings 18th National Conference Artiﬁcial Intelligence AAAI 04 Edmonton Alberta Canada 2004 pp 682687 36 JD Park A Darwiche Approximating MAP local search Proceedings Seventeenth Conference Uncertainty Artiﬁcial Intelligence UAI01 Seattle WA 2001 pp 403410 37 JD Park A Darwiche Complexity results approximation strategies MAP explanations Journal Artiﬁcial Intelligence Research JAIR 21 2004 101133 38 AJ Parkes JP Walser Tuning local search satisﬁability testing Proceedings Thirteenth National Conference Artiﬁcial Intelligence AAAI96 Portland OR 1996 pp 356362 39 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann San Mateo CA 1988 40 D Roth On hardness approximate reasoning Artiﬁcial Intelligence 82 1996 273302 41 Y Ruan E Horvitz H Kautz Restart policies dependence runs A dynamic programming approach Proceedings Eighth International Conference Principles Practice Constraint Programming Ithaca NY 2002 pp 573586 42 Y Ruan E Horvitz H Kautz Hardnessaware restart policies IJCAI03 Workshop Stochastic Search Algorithms Acapulco Mexico 2003 43 D Schuurmans F Southey Local search characteristics incomplete SAT procedures Artiﬁcial Intelligence 132 2 2001 121150 44 B Selman H Kautz Domainindependent extensions GSAT Solving large structured satisﬁability problems Proceedings Inter national Joint Conference Artiﬁcial Intelligence IJCAI93 Chambery France 1993 pp 290295 45 B Selman HA Kautz B Cohen Noise strategies improving local search Proceedings Twelfth National Conference Artiﬁcial Intelligence AAAI94 Seattle WA 1994 pp 337343 46 B Selman H Levesque D Mitchell A new method solving hard satisﬁability problems Proceedings Tenth National Conference Artiﬁcial Intelligence AAAI92 San Jose CA 1992 pp 440446 47 E Shimony Finding MAPs belief networks NPhard Artiﬁcial Intelligence 68 1994 399410 48 J Suzuki A Markov chain analysis genetic algorithm S Forrest Ed Proceedings Fifth International Conference Genetic Algorithms San Mateo CA 1993 pp 146153 49 AJ Viterbi Error bounds convolutional codes asymptotically optimal decoding algorithm IEEE Transactions Information Theory 13 1967 260269 50 C Yanover Y Weiss Finding m probable conﬁgurations arbitrary graphical models S Thrun L Saul B Schölkopf Eds Advances Neural Information Processing Systems 16 MIT Press Cambridge MA 2004 51 M Yokoo Why adding constraints makes problem easier hillclimbing algorithms Analyzing landscapes CSPs Proceedings Third International Conference Principles Practice Constraint Programming LNCS vol 1330 Springer Verlag 1997 pp 357370