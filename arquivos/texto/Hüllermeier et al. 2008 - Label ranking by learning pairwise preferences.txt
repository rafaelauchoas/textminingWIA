Artiﬁcial Intelligence 172 2008 18971916 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Label ranking learning pairwise preferences Eyke Hüllermeier Johannes Fürnkranz b Weiwei Cheng Klaus Brinker Department Mathematics Computer Science PhilippsUniversität Marburg Germany b Department Computer Science TU Darmstadt Germany r t c l e n f o b s t r c t Article history Received 21 January 2008 Received revised form 14 July 2008 Accepted 8 August 2008 Available online 15 August 2008 Keywords Preference learning Ranking Pairwise classiﬁcation Constraint classiﬁcation 1 Introduction Preference learning emerging topic appears different guises recent literature This work focuses particular learning scenario called label ranking problem learn mapping instances rankings ﬁnite number labels Our approach learning mapping called ranking pairwise comparison RPC ﬁrst induces binary preference relation suitable training data natural extension pairwise classiﬁcation A ranking derived preference relation obtained means ranking procedure different ranking methods minimizing different loss functions In particular simple weighted voting strategy minimizes risk respect wellknown Spearman rank correlation We compare RPC existing label ranking methods based scoring individual labels instead comparing pairs labels Both empirically theoretically shown RPC superior terms computational eﬃciency competitive terms accuracy 2008 Elsevier BV All rights reserved The topic preferences recently attracted considerable attention Artiﬁcial Intelligence AI research notably ﬁelds agents nonmonotonic reasoning constraint satisfaction planning qualitative decision theory 191 Pref erences provide means specifying desires declarative way point critical importance AI In fact consider AIs paradigm rationally acting decisiontheoretic agent The behavior agent driven underlying preference model agent recommending decisions acting behalf user clearly reﬂect users preferences It hardly surprising methods learning predicting preferences automatic way recent research topics disciplines machine learning knowledge discovery recommender systems Many approaches subsumed terms ranking preference learning different suﬃciently discriminated existing terminology We start paper clariﬁcation contribution Section 2 The learning scenario consider paper assumes collection training examples associated ﬁnite set decision alternatives Following common notation supervised learning shall refer labels However contrary standard classiﬁcation training example assigned single label set pairwise preferences labels complete entirely Corresponding author Email addresses eykeinformatikunimarburgde E Hüllermeier juﬃkeinformatiktudarmstadtde J Fürnkranz chenginformatikunimarburgde W Cheng brinkerinformatikunimarburgde K Brinker 1 The increasing activity area witnessed workshops devoted preference learning related topics NIPS02 KI03 SIGIR03 NIPS04 GfKl05 IJCAI05 ECAI2006 conferences second ﬁfth organized authors 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200808002 1898 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 Table 1 Four different approaches learning preference information representative references modeling utility functions modeling pairwise preferences object ranking label ranking comparison training 58 constraint classiﬁcation 28 learning order things 13 work 24 consistent expressing label preferred The goal learn predict total order ranking possible labels new training example The ranking pairwise comparison RPC algorithm introduce Section 3 paper modular struc ture works phases First pairwise preferences learned suitable training data natural extension socalled pairwise classiﬁcation Then ranking derived set preferences means ranking procedure In Section 4 analyze computational complexity RPC algorithm Then Section 5 shown suitable ranking procedures RPC minimize risk certain loss functions rankings Section 6 devoted experimental evaluation RPC comparison alternative approaches applicable learning problem The paper closes discussion related work Section 7 concluding remarks Section 8 Parts paper based 242533 2 Learning preferences In section motivate preference learning2 theoretically interesting practically relevant subﬁeld machine learning One distinguish types preference learning problems learning object preferences learning label preferences different approaches modeling preferences evaluating individual alternatives means utility function comparing pairs competing alternatives means preference relation Table 1 shows possible combinations obtained In section shall discuss options approach label ranking pairwise comparison missing literature novel contribution 21 Learning object preferences The frequently studied problem learning preferences induce ranking function r able order subset O underlying class X objects That r assumes input subset O x1 xn X objects returns output permutation τ 1 n The interpretation permutation object xi preferred x j τ τ j The objects websites typically characterized ﬁnite set features conventional attributevalue learning The training data consists set exemplary pairwise preferences This scenario summarized Fig 1 known learning order things 13 As example consider problem learning rank query results search engine 3552 The training information provided implicitly user clicks links query result This information turned binary preferences assuming selected pages preferred nearby pages clicked 36 22 Learning label preferences In learning scenario problem predict instance x person instance space X preference relation cid4x L L ﬁnite set L λ1 λm labels alternatives λi cid4x λ j means instance x prefers label λi label λ j More speciﬁcally especially interested case cid4x total strict order ranking L Note ranking cid4x identiﬁed permutation τx 1 m permutation τx τxi τx j λi cid4x λ j τ position λi ranking We shall denote class permutations 1 m Sm Moreover abuse notation shall employ terms ranking permutation synonymously The training information consists set instances partial knowledge associated preference relation available cf Fig 2 More precisely training instance x associated subset pairwise preferences Thus assume existence underlying true ranking expect training data provide information ranking Besides order increase practical usefulness approach allow inconsistencies pairwise preferences conﬂicting observation errors 2 We interpret term preference literally wide sense kind order relation Thus cid4 b mean alternative liked person b algorithm outperforms b certain problem event probable b student ﬁnishing studies b E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1899 Given potentially inﬁnite reference set objects X object typically represented feature vector ﬁnite set pairwise preferences xi cid4 x j xi x j X X Find ranking function r assumes input set objects O X returns permutation ranking set Fig 1 Learning object preferences Given set training instances xk k 1 n X instance typically represented feature vector set labels L λi 1 m training instance xk set pairwise preferences form λi cid4xk λ j Find ranking function maps x X ranking cid4x L permutation τx Sm Fig 2 Learning label preferences As case object ranking learning scenario large number practical applications In empirical investigate task predicting qualitative representation gene expression proﬁle measured microarray analysis phylogenetic proﬁle features 4 Another application scenario metalearning task rank learning algorithms according suitability new dataset based characteristics dataset 10 Finally preference statement wellknown CPnets approach 7 qualitative graphical representation reﬂects conditional dependence independence preferences ceteris paribus interpretation formally corresponds label ranking In addition observed authors 172428 conventional learning problems classi ﬁcation multilabel classiﬁcation formulated terms label preferences Classiﬁcation A single class label λi assigned example xk This implicitly deﬁnes set preferences λi cid4xk λ j 1 cid2 j cid6 cid2 m Multilabel classiﬁcation Each training example xk associated subset Lk L possible labels This implicitly deﬁnes set preferences λi cid4xk λ j λi Lk λ j L Lk f X Sm learned subset possible pairwise preferences In scenarios ranking model A suitable projection applied ranking model outputs permutations postprocessing step example projection toprank classiﬁcation learning label relevant 23 Learning utility functions As mentioned natural way represent preferences evaluate individual alternatives means real valued utility function In object preferences scenario function mapping f X R assigns utility degree f x object x induces complete order X In label preferences scenario utility function f X R needed labels λi 1 m Here f ix utility assigned alternative λi instance x To obtain ranking x alternatives ordered according utility scores λi cid8x λ j f ix cid3 f jx If training data offer utility scores directly preference learning reduce standard regression problem monotonic transformation utility values This information rarely assumed Instead usually constraints derived comparative preference information form This object label higher utility score object label given Thus challenge learner ﬁnd function possible agreement constraints For object ranking approaches idea ﬁrst formalized Tesauro 58 comparison training He proposed symmetric neuralnetwork architecture trained representations states training signal indicates states preferable The elegance approach comes property replace symmetric components network single network subsequently provide realvalued evaluation single states Later works learning utility function object preference data include 2731 3560 Subsequently outline approaches constraint classiﬁcation CC loglinear models label ranking LL direct alternatives method ranking pairwise comparison shall later compare 1900 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 231 Constraint classiﬁcation For case label ranking corresponding method learning functions f 1 m training data proposed framework constraint classiﬁcation 2829 Proceeding linear utility functions f ix ncid2 k1 αikxk 21 labelspeciﬁc coeﬃcients αik k 1 n preference λi cid4x λ j translates constraint f ix f jx 0 equiva lently f jx f ix 0 Both constraints positive negative expressed terms sign inner product cid10z αcid11 α α11 α1n α21 αmn concatenation labelspeciﬁc coeﬃcients Correspondingly vector z constructed mapping original cid5dimensional training example x x1 xcid5 m cid5dimensional space For positive constraint x copied components 1 cid5 1 cid5 negation x components j 1 cid5 1 j cid5 remaining entries ﬁlled 0 For negative constraint vector constructed elements reversed signs Both constraints considered training examples conventional binary classiﬁer m cid5dimensional space The ﬁrst vector positive second negative example The corresponding learner tries ﬁnd separating hyperplane space suitable vector α satisfying constraints For classifying new example e labels ordered according response resulting multiply ing e ith cid5element section hyperplane vector To work general types utility functions method obviously kernelized Alternatively HarPeled et al 2829 propose online version constraint classiﬁcation iterative algorithm maintains weight vectors α1 αm Rcid5 label individually In iteration algorithm checks straint λi cid4x λ j case associated inequality αi x f ix f jx α j x violated adapts weight vectors αi α j appropriately In particular perceptron training algorithm implemented terms multioutput perceptron way similar approach Grammer Singer 15 232 Loglinear models label ranking Socalled loglinear models label ranking proposed Dekel et al 17 Here utility functions expressed terms linear combinations set base ranking functions cid2 f ix α jh jx λi j base function h j maps instancelabel pairs real numbers Interestingly special case instances represented feature vectors x x1 xcid5 base functions form hkjx λ xk λ λ j λ cid6 λ j 0 1 cid2 k cid2 cid5 1 cid2 j cid2 m 22 cid3 approach essentially equivalent CC amounts learning classspeciﬁc utility functions 21 Algorithmically underlying optimization problem approached different way means boostingbased algorithm seeks minimize generalized ranking error iterative way 24 Learning preference relations The key idea approach model individual preferences directly instead translating utility function This natural approach noted utility scores diﬃcult elicit observed preferences usually relational type For example hard ensure consistent scale utility evaluations performed user The situation problematic utility scores elicited different users uniform scale scores 13 For learning preferences bring similar argument It typically easier learn separate theory individual preference compares objects labels determines better Of course learned utility function assigns score set labels L induces binary preference relation labels For object ranking problems pairwise approach pursued 13 The authors propose solve object ranking vice versa A ﬁnal problems learning binary preference predicate Q x x ordering second phase deriving ranking maximally consistent predictions cid12 cid12 predicts x preferred x For label ranking problems pairwise approach introduced Fürnkranz Hüllermeier 24 The key idea described Section 3 learn pair labels λi λ j binary predicate Mi jx predicts λi cid4x λ j λ j cid4x λi input x In order rank labels new object predictions pairwise label preferences obtained ranking maximally consistent preferences derived 3 Label ranking learning pairwise preferences The key idea ranking pairwise comparison RPC reduce problem label ranking binary classi ﬁcation problems Sections 31 32 The predictions ensemble binary classiﬁers combined E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1901 ranking separate ranking algorithm Section 33 We consider modularity RPC important advantage approach Firstly binary classiﬁcation problems comparably simple eﬃciently learnable Secondly clear remainder paper different ranking algorithms allow ensemble pairwise classiﬁers adapt different loss functions label rankings need retraining pairwise classiﬁers 31 Pairwise classiﬁcation The key idea pairwise learning wellknown context classiﬁcation 22 allows transform multiclass classiﬁcation problem problem involving m 2 classes L λ1 λm number binary problems To end separate model base learner Mi j trained pair labels λi λ j L 1 cid2 j cid2 m total number mm 12 models needed Mi j intended separate objects label λi having label λ j At classiﬁcation time query instance submitted models Mi j predictions combined overall prediction In simplest case prediction model Mi j interpreted vote λi λ j label highest number votes proposed ﬁnal prediction3 Pairwise classiﬁcation tried areas statistics 821 neural networks 40414451 support vector ma chines 30324254 Typically technique learns accurate theories commonly oneagainstall classiﬁcation method learns theory class examples class positive examples negative examples4 Surprisingly shown pairwise classiﬁcation computa tionally eﬃcient oneagainstall class binarization cf Section 4 32 Learning pairwise preference The procedure extended case preference learning natural way 24 Again preference order information form λa cid4x λb turned training example x y learner Mi j mina b j maxa b Moreover y 1 b y 0 Thus Mi j intended learn mapping outputs 1 λi cid4x λ j 0 λ j cid4x λi cid3 x cid13 1 λi cid4x λ j 0 λ j cid4x λi 31 The model trained examples xk λi cid4xk λ j λ j cid4xk λi known Examples known preference λi λ j ignored The mapping 31 realized binary classiﬁer Alternatively employ base classiﬁers map unit interval 0 1 instead 0 1 assign valued preference relation Rx query instance x X cid3 Rxλi λ j Mi jx 1 M jix j j 32 λi cid6 λ j L The output 0 1valued classiﬁer usually interpreted probability generally kind conﬁdence classiﬁcation closer output Mi j 1 stronger preference λi cid4x λ j supported Fig 3 illustrates entire process hypothetical dataset examples described binary attributes A1 A2 A3 preferences labels b c First original training set transformed twoclass training sets possible pair labels containing training examples relation labels known Then binary models Mab Mbc Mac trained In example result simple rules like following Mab b A2 1 Mbc b c A3 1 Mac c A1 1 A3 1 Given new example unknown preference structure shown left Fig 3 predictions models predict ranking example As section trivial example 3 Ties broken favor prevalent classes according class distribution classiﬁcation setting 4 Rifkin Klautau 53 argued case support vector machines oneagainstall effective provided binary base classiﬁers carefully tuned 1902 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 Fig 3 Schematic illustration learning pairwise comparison 33 Combining predicted preferences ranking Given predicted preference relation Rx instance x question derive associated ranking τx This question nontrivial relation Rx suggest unique ranking unequivocal way For exam ple learned preference relation need transitive cf Section 34 In fact problem inducing ranking valued preference relation received lot attention research ﬁelds fuzzy preference modeling multiattribute decision making 20 In context pairwise classiﬁcation preference learning studies empirically compared different ways combining predictions individual classiﬁers 2233462 A simple effective strategy generalization aforementioned voting strategy alternative λi eval uated sum weighted votes Sλi Rxλi λ j cid2 λ j cid6λi labels ordered according evaluations λi cid4x λ j cid4 cid5 Sλi cid3 Sλ j 33 34 Even ranking procedure appear adhoc ﬁrst sight shall theoretical justiﬁcation Section 5 shown ordering labels according 33 minimizes reasonable loss function rankings 34 Transitivity Our pairwise learning scheme outlined produces relation Rx learning preference degrees Rxλi λ j independently In regard wonder interdependencies grees taken account In particular transitivity pairwise preferences important properties preference modeling interesting question sort transitivity guaranteed Rx Obviously learned binary preference relation necessarily typical properties order relations For example transitivity general hold λi cid4x λ j λ j cid4x λk independently trained classiﬁer Mik predict λk cid4x λi 5 This problem subsequent ranking phase convert intransitive predictive preference relation total preference order However shown given formal assumptions setting following weak form transitivity satisﬁed j k 1 m Rxλi λ j cid3 Rxλi λk Rxλk λ j 1 35 5 In fact symmetry needs hold Mi j M ji different models case rule learning algorithms 22 This situation compared round robin sports tournament individual results necessarily conform ﬁnal ranking computed E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1903 As consequence property proved Appendix A predictions obtained ensemble pairwise learners Mi j actually satisfy 35 In words training learners independently fully legitimate Fortunately experience far shown probability violate 35 high Still forcing 35 hold potential point improvement ongoing work 4 Complexity analysis In section generalize previous results eﬃciency pairwise classiﬁcation preference learning In particular approach expected computationally eﬃcient alternative approaches like constraint classiﬁcation try model preference learning problem single binary classiﬁcation problem higherdimensional space cf Section 23 41 Ranking pairwise comparison First bound number training examples pairwise approach Let P k number prefer P k average number ences associated example xk Throughout section denote d 1n preferences examples cid6 k Lemma 1 The total number training examples constructed RPC n d bounded n mm 12 ncid2 k1 P k n d cid2 n mm 1 2 Proof Each n training examples added P k binary training sets correspond preferences P k n d This bounded size complete set Thus total number training examples preferences n mm 12 cid2 cid6 n k1 The special case classiﬁcation number training examples grow linearly number classes 22 obtained corollary theorem classiﬁcation class label expands d m 1 preferences As consequence follows immediately RPC base algorithm linear runtime complexity On total runtime Od n More interesting general case Theorem 1 For base learner complexity Ona complexity RPC Od na Proof Let ni j number training examples model Mi j Each example corresponds single preference cid2 1cid2i jcid2m ni j ncid2 k1 P k d n cid6 total learning complexity Ona j We obtain cid8 cid7cid7 cid8 cid6 Ona j Od na 1 d cid2 1 d cid2 Ona j Ona cid7 ni j n cid2 O 1 d cid8 cid2 O cid6 Oni j d On ni j n cid6 O ni j Od n Od n Od n O1 The inequality holds example preference involving pair labels λi λ j Thus ni j cid2 n cid2 Again obtain corollary complexity pairwise classiﬁcation linear number classes Om na incomplete proof previously given 22 42 Constraint classiﬁcation loglinear models For comparison CC converts example set examples positive negative preference This construction leads following complexity Theorem 2 For base learner complexity Ona total complexity constraint classiﬁcation Oda na 1904 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 P k 2dn examples means CC constructs Proof CC transforms original training data set 2 twice training examples RPC If problem solved base learner complexity Ona total complexity O2dna Oda na cid2 cid6 n k1 Moreover newly constructed examples projected space m times attributes original space A direct comparison obvious online version CC complexity strongly depends number iterations needed achieve convergence In single iteration algorithm checks constraints instance case constraint violated adapts weight vector correspondingly The complexity On d cid5 T cid5 number attributes instance dimension instance space T number iterations For reason diﬃcult compare RPC boostingbased algorithm proposed loglinear models Dekel et al 17 In iteration algorithm essentially updates weights associated instance preference constraint In label ranking setting considered complexity step Od n Moreover algorithm maintains weight coeﬃcients base ranking function If speciﬁed 22 number functions m cid5 Therefore total complexity LL Od n m cid5 T T number iterations 43 Discussion In summary overall complexity pairwise label ranking depends average number preferences given training example While quadratic number labels complete ranking given linear classiﬁcation setting In case expensive constraint classiﬁcation considerably cheaper complexity base learner superlinear 1 The comparison RPC LL obvious essentially depends na relates n T note implicitly T depends n larger data sets typically need iterations A possible disadvantage RPC concerns large number classiﬁers stored Assuming input space X dimensionality cid5 simple linear classiﬁers base learners pairwise approach store Ocid5 m2 parameters CC LL need store Ocid5 m parameters represent ranking model During training boostingbased optimization algorithm LL store typically higher number n d parameters preference constraint As model parameters deriving label ranking affect prediction time However classiﬁcation setting shown 48 eﬃcient algorithm yields predictions voting linear time Ocid5 m To extent algorithm generalized label ranking currently investigation As ranking basically sorting possible labels expect loglinear time Ocid5 m log m 5 Risk minimization Even approach pairwise ranking outlined Section 3 appears intuitively appealing argue lacks solid foundation remains adhoc extent For example easily think ranking proce dures 33 leading different predictions In case wonder rankings predicted basis 32 33 kind optimality property An aﬃrmative answer question given section 51 Preliminaries Recall setting label ranking associate instance x instance space X ranking ﬁnite set class labels L λ1 λm equivalently permutation τx Sm Sm denotes class permutations 1 m More speciﬁcally analogy setting conventional classiﬁcation instance associated probability distribution class rankings permutations Sm That instance x exists probability distribution P x τ Sm Pτ x probability observe ranking τ output given instance x input The quality model M induced learning algorithm commonly measured terms expected loss risk cid4 cid4 D E y Mx cid5cid5 51 D loss distance function Mx denotes prediction model instance x y true outcome The expectation E taken X Y Y output space6 case Y given Sm 6 The existence probability measure X Y course assumed E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1905 52 Spearmans rank correlation An important frequently applied similarity measure rankings Spearman rank correlation originally proposed Spearman 57 nonparametric rank statistic measure strength associations variables 43 It deﬁned follows 1 6Dτ τ cid12 mm2 1 linear transformation normalization sum squared rank distances Dτ cid12 τ df mcid2 cid4 τ cid12 cid5 τ 2 i1 52 53 interval 1 1 As shown RPC risk minimizer respect 53 Spearman rank correlation distance measure condition binary models Mi j provide correct probability estimates Rxλi λ j Mi jx Pλi cid4x λ j That 54 holds RPC yields risk minimizing prediction ˆτx arg min τ Sm cid2 τ cid12Sm Dτ τ cid12 Pτ cid12 x 54 55 D given 53 Admittedly 54 relatively strong assumption requires pairwise preference probabil ities perfectly learnable Yet result 55 sheds light aggregation properties technique ideal conditions provides valuable basis analysis In fact recalling RPC consists steps pair wise learning ranking clear order study properties assumptions result step And 54 best hold approximately practice natural assumption output ensemble pairwise learners Lemma 2 Let si 1 m real numbers 0 cid2 s1 cid2 s2 cid2 cid2 sm Then permutations τ Sm mcid2 si2 cid2 mcid2 sτ i2 i1 i1 56 Proof We mcid2 sτ i2 i1 mcid2 si si sτ i2 i1 mcid2 si2 2 i1 mcid2 i1 Expanding equation exploiting sisi sτ mcid2 si sτ i2 cid6 m i1 s2 cid6 i1 m i1 s2 τ yields mcid2 sτ i2 mcid2 si2 2 i1 i1 mcid2 i1 si 2 mcid2 i1 sτ cid6 i1 isτ depends τ This term maximal τ On righthand equation term si cid2 s j j maxi1m msi msm maxi1m1m 1si m 1sm1 Thus difference i1i si2 proves sums positive righthand larger equal lemma cid2 cid6 m m Lemma 3 Let P x probability distribution Sm Moreover let cid2 df m si Pλi cid4x λ j jcid6i Pλi cid4x λ j cid2 Pτ x Then si cid6 τ τ τ iτ j Pτ xτ 57 58 1906 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 Proof We cid2 si m Pλi cid4x λ j jcid6i cid2 cid5 cid4 1 Pλi cid4x λ j 1 1 1 1 1 jcid6i cid2 jcid6i cid2 jcid6i cid2 τ cid2 Pλ j cid4x λi cid2 Pτ x τ τ jτ cid2 Pτ x cid3 jcid6i cid4 Pτ x cid5 τ 1 1 τ τ j 0 τ τ j cid2 τ Pτ xτ cid2 τ Note si cid2 s j equivalent Sλi cid3 Sλ j deﬁned 33 assumption 54 Thus ranking alternatives according Sλi decreasing order equivalent ranking according si increasing order Theorem 3 The expected distance cid4 E Dτ cid12 cid5 τ x cid2 τ minimal choosing τ cid12 Proof We Pτ x Dτ cid12 τ cid2 Pτ x mcid2 cid4 τ cid12 cid5 2 τ i1 τ cid12i cid2 τ cid12 j si cid2 s j si given 57 τ cid4 E Dτ cid12 cid5 τ x cid2 Pτ x mcid2 cid4 τ cid12 cid5 2 τ τ mcid2 cid2 i1 mcid2 τ cid2 i1 mcid2 τ cid2 i1 cid4 τ cid12 Pτ x cid5 2 τ cid4 τ cid12 Pτ x cid5 si si τ 2 cid9cid4 Pτ x τ si cid4 cid5 2 2 τ si cid5cid4 si τ cid12 cid4 cid5 si τ cid12 cid10 2 cid5 i1 mcid2 τ cid11cid2 i1 τ cid4 Pτ x τ si cid5 cid4 2 2 si τ cid12 cid5cid2 cid4 τ si Pτ x cid5 τ cid4 Pτ x si τ cid12 cid5 2 cid12 cid2 τ In equation midterm righthand 0 according Lemma 3 Moreover term obviously simpliﬁes si τ cid12i2 ﬁrst term constant c Pτ xτ si2 depend τ cid12 Thus obtain EDτ cid12 τ x c i1si τ cid12i2 theorem follows Lemma 2 cid2 cid6 cid6 m τ 53 Kendalls tau The result shows approach label ranking form presented Section 3 particularly tailored 53 loss function We like point RPC restricted measure minimize loss functions As mentioned previously accomplished replacing ranking procedure second step RPC suitable way To illustrate consider wellknown Kendall tau measure 38 alternative loss function This measure essentially calculates number pairwise rank inversions labels measure ordinal correlation rankings formally Dτ cid12 cid13 τ df j cid14 cid14 j τ τ j τ cid12 τ cid12 cid15 j 59 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1907 denoting number discordant pairs items labels Kendall tau coeﬃcient given 1 4Dτ cid12 τ mm 1 linear scaling Dτ cid12 τ interval 1 1 Now ranking τ cid12 cid2 cid4 E Dτ cid12 cid5 τ x Pτ Dτ cid12 τ Sm cid2 τ Sm Pτ x τ cid2 jτ cid12iτ cid12 j cid2 cid2 Pτ x jτ cid12iτ cid12 j cid2 τ Sm Pλi cid4x λ j jτ cid12iτ cid12 j cid3 cid3 1 τ τ j 0 τ τ j 1 τ τ j 0 τ τ j 510 511 Thus knowing pairwise probabilities Pλi cid4x λ j derive expected loss ranking τ cid12 In words RPC predictions optimal 59 underlying loss function To end ranking procedure adapted pairwise probabilities predictions pairwise learners Finding ranking minimizes 510 formally equivalent solving graphtheoretical feedback arc set problem weighted tournaments known NP complete 3 Of course context label ranking result perspective set class labels typically small moderate size Nevertheless computational point view ranking procedure minimizes Kendalls tau deﬁnitely complex procedure minimizing Spearmans rank correlation 54 Connections voting theory It worth mentioning voting strategy RPC discussed Section 52 closely related socalled Bordacount voting rule wellknown social choice theory 9 Suppose preferences n voters expressed terms rankings τ1 τ2 τn m alternatives From ranking τi following scores derived alternatives The best alternative receives m 1 points second best m 2 points The overall score alternative sum points received voters representative ranking ˆτ aggregation single voters rankings obtained ordering alternatives according scores Now readily veriﬁed result obtained procedure corresponds exactly result RPC probability distribution class Sm rankings deﬁned corresponding relative frequencies In words ranking ˆτ obtained RPC minimizes sum distances ˆτ arg min τ Sm ncid2 i1 Dτ τi 512 A ranking kind called central ranking7 In connection social choice theory interesting note RPC satisfy socalled Condorcet criterion As pairwise preferences example thoroughly possible alternative case λ1 preferred pairwise comparisons Rλ1 λ2 5 Rλ1 λ3 5 overall winner election toplabel ranking Of course apparently paradoxical property relevant ranking classiﬁcation In context recognized Hastie Tibshirani 30 Another distance similarity measure rankings plays important role voting theory aforemen tioned Kendall tau When number discordant pairs 59 distance measure D 512 ˆτ called Kemenyoptimal ranking Kendalls tau intuitively appealing Kemenyoptimal rankings nice properties However noted earlier drawback Kendalls tau instead rank correlation distance mea sure 512 loss computational eﬃciency In fact computation Kemenyoptimal rankings known NPhard 5 6 Empirical evaluation The experimental evaluation presented section compares terms accuracy computational eﬃciency ranking pairwise comparison RPC weighted voting constraint classiﬁcation CC approach loglinear models label ranking LL outlined respectively Sections 231 232 CC particular natural counterpart 7 See Mardens book 45 contains results closely related results Section 52 1908 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 Table 2 Statistics semisynthetic real datasets dataset iris wine glass vowel vehicle spo heat dtt cold diau examples classes features 150 178 214 528 846 2465 2465 2465 2465 2465 3 3 6 11 4 11 6 4 4 7 4 13 9 10 18 24 24 24 24 24 compare approach orthogonal instead breaking label ranking problem set small pairwise learning problems CC embeds original problem single learning problem highdimensional feature space We implemented CC support vector machines linear kernel binary classiﬁer CCSVM8 Apart CC original version included onlinevariant CCP proposed 28 noisetolerant perceptron algorithm base learner 379 To guarantee fair comparison use LL 22 base ranking functions means based underlying model class CC Moreover implement RPC simple logistic regression base learner10 comes ﬁtting linear model logistic link function logitπ logπ 1 π derive 0 1valued scores type model output requested RPC Essentially approaches based linear models fact produce linear decision boundaries classes11 Nevertheless guarantee comparability RPC CC implemented logistic regression base learner CCLR 61 Datasets To provide comprehensive analysis varying conditions considered different scenarios roughly categorized realworld semisynthetic The realworld scenario originates bioinformatics ﬁelds ranking multilabeled data respectively frequently More precisely experiments considered types genetic data phylogenetic proﬁles DNA microarray expression data Yeast genome12 The genome consists 2465 genes gene represented associated phylogenetic proﬁle length 24 Using proﬁles input features investigated task predict ing qualitative representation expression proﬁle Actually expression proﬁle gene ordered sequence realvalued measurements 21 35 07 25 value represents expression level gene measured particular point time A qualitative representation obtained converting expression levels ranks ordering time points labels according associated expression values In example qualitative proﬁle given 2 1 3 4 means highest expression observed time point 2 secondhighest time point 1 The use qualitative proﬁles kind Spearman correlation similarity measure motivated 4 biologically data analysis point view We data ﬁve microarray experiments spo heat dtt cold diau giving rise ﬁve prediction problems input features different target rankings It worth mentioning experiments involve different numbers measurements ranging 4 11 Table 213 Since context measurement corresponds label obtain ranking problems different complexity Besides original measurements realvalued expression proﬁles containing ties broken randomly In order complement realworld scenario problems originating different domains following multiclass datasets UCI Repository machine learning databases 6 Statlog collection 46 included experimental evaluation iris wine glass vowel vehicle summary dataset properties given Table 2 These datasets recent experimental study multiclass support vector machines 32 8 We employed implementation offered Weka machine learning package 61 default setting To obtain ranking labels classiﬁcation scores transformed pseudoprobabilities logistic regression technique 50 9 This algorithm based alphabound trick We set corresponding parameter α 500 10 Again implementation offered Weka package 11 All linear models incorporate bias term 12 This data publicly available httpwww1cscolumbiaeducompbioexpphylo 13 We excluded additional subproblems measurements prohibitive computational demands constraint classiﬁcation approach E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1909 Table 3 Experimental results mean standard deviation terms Kendalls tau data iris wine glass vowel vehicle spo heat dtt cold diau RPC 885 068 921 053 882 042 647 019 854 025 140 023 125 024 174 034 221 028 332 019 CCP 836 089 933 043 846 045 623 019 855 022 138 022 126 023 180 037 220 029 330 019 CCLR 836 063 755 111 834 052 583 019 830 025 122 022 124 024 158 033 196 029 299 022 Table 4 Experimental results mean standard deviation terms Spearmans rank correlation data iris wine glass vowel vehicle spo heat dtt cold diau RPC 910 058 938 045 918 036 760 020 888 020 176 030 156 030 199 040 265 033 422 023 CCP 863 086 949 033 889 043 746 021 891 019 178 030 156 029 205 041 265 034 418 023 CCLR 874 052 800 102 879 048 712 020 873 022 156 029 154 029 183 038 234 035 377 026 CCSVM 812 071 932 057 820 064 594 020 817 025 121 020 117 023 154 045 193 040 297 019 CCSVM 856 057 942 052 860 062 724 021 864 023 156 026 148 027 178 054 235 050 377 022 LL 818 088 942 043 817 060 601 021 770 037 132 024 125 025 167 034 209 028 321 020 LL 843 089 956 034 859 060 732 022 820 036 167 030 155 031 193 038 251 033 406 025 For multiclass datasets corresponding ranking dataset generated following manner We trained naive Bayes classiﬁer14 respective dataset Then example labels present dataset ordered respect decreasing predicted class probabilities case ties labels lower indices ranked ﬁrst Thus substituting single labels contained original multiclass datasets complete rankings obtain label ranking datasets required experiments The fundamental underlying learning problem viewed learning qualitative replication probability estimates naive Bayes classiﬁer 62 Experimental results 621 Complete preference information In experiments actual true rankings test sets compared corresponding predicted rankings For approaches report average accuracy terms Spearmans rank correlation Kendalls tau This necessary showed Section 5 RPC weighted voting ranking procedure especially tailored maximizing Spearman rank correlation CC LL focused Kendall tau measure Minimization 01loss expanded set binary classiﬁcation examples yields implicit maximization empirical Kendall tau statistic label ranking function training set It true distance similarity measures rankings course closely related15 The results cross validation study 10fold 5 repeats shown Tables 3 4 clearly favor RPC CC online version These methods par outperform methods datasets wine LL yields highest accuracy These results corroborated standard classiﬁcation accuracy multiclass data probability place true class topmost rank reported Table 5 In terms training time RPC clear winner seen Table 616 In compliance theoretical results original version CC implemented CCSVM CCLR problematic point view extremely expensive data sets attributes labels For example trainings time CCSVM 5 hours vowel 7 days spo data abstained detailed analysis exposition results variants As expected RPC slightly eﬃcient LL CCP terms 14 We employed implementation naive Bayes classiﬁcation numerical datasets NaiveBayesSimple contained Weka machine learning package 61 15 For example recently shown 14 optimizing rank correlation yields 5approximation ranking optimal Kendall measure 16 Experiments conducted PC Intel Core2 6600 24 Ghz 2 GB RAM We stopped iteration LL soon sum absolute changes weights smaller 10 7 empirically largest value guaranteed stability model performance 1910 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 Table 5 Experimental results mean standard deviation terms standard classiﬁcation rate data iris wine glass vowel vehicle RPC 952 050 945 051 767 091 507 056 895 028 CCP 933 069 970 042 715 089 425 062 895 034 CCLR 907 075 927 043 706 092 445 063 868 035 CCSVM 911 076 948 057 696 099 433 064 865 033 Table 6 Time ms needed training left testing mean standard deviation data iris wine glass vowel vehicle spo heat dtt cold diau RPC 18 11 59 16 132 15 927 24 439 24 10 953 95 3069 39 1226 31 1209 32 4325 38 CCP 48 10 22 14 605 52 12 467 595 1810 177 343 506 27190 61 206 3648 19 592 1133 20 936 1358 83 967 9849 LL 833 587 575 376 1529 850 36 063 22 897 2177 1339 61 826 33 946 16 552 9415 2510 1340 3045 2001 27 441 12 686 RPC 06 32 06 31 16 48 137 51 16 48 905 58 265 73 102 74 106 74 347 66 CCP 00 00 03 23 00 00 03 21 00 00 09 38 06 32 03 21 00 00 12 43 LL 916 076 962 044 706 093 407 067 851 037 LL 00 00 03 23 03 23 06 31 00 00 103 81 37 67 28 60 34 65 41 70 testing time Table 6 times extremely small clearly negligible comparison training times 622 Incomplete preference information In Section 621 provided empirical study learning label ranking functions assuming complete ranking available example training set However practical settings access total order possible labels object Instead cases pairs preferences known object To model incomplete preferences modiﬁed training data follows A biased coin ﬂipped label ranking order decide delete label probability deletion p Thus ranking λ1 cid4 λ2 cid4 λ3 cid4 λ4 cid4 λ5 reduced λ1 cid4 λ3 cid4 λ4 pairwise preferences generated note pairwise preference survives probability 1 p2 average percentage preferences training data decreases faster p average number labels Of course rankings produced way varying size Fig 4 shows experimental results RPC LL CCP online variant CC More precisely ﬁgures accuracy terms Kendalls tau qualitatively similar Spearmans rank correlation function probability p As expected accuracy decreases increasing missing preference information methods deal missing preference information remarkably Still clear rank order LL sensitive method CC appears bit sensitive RPC Our explanation ﬁnding training quadratic instead linear number models RPC sense ﬂexible LL CC This ﬂexibility advantage training data available turn disadvantages case This explain superior performance LL wine data relatively instances Finally mention identical curves obtained sampling complete training examples suitable sampling rate Roughly speaking training instances complete preference information comparable training instances partial preference information provided expected total number pairwise preferences 7 Related work As noted Section 6 work constraint classiﬁcation 2829 appears natural counterpart algorithm In section discussed loglinear models label ranking proposed Dekel et al 17 As CC LL directly applicable label ranking problem studied paper compared RPC empirically approaches The subsequent review focus key works related label ranking pairwise decomposition techniques recently appeared literature somewhat exhaustive literature survey 12 We aware work method approaches label ranking problem learning pairwise preference predicates Rxλi λ j 1 cid2 j cid2 m reduces problem ranking basis preference relation Instead existing methods including CC LL essentially follow idea learning utility scoring functions f 1 fm inducing label ranking Given input x label λi evaluated terms score f ix labels ordered according scores E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1911 Fig 4 Results datasets Table 2 missing label scenario Accuracy terms Kendalls tau function expected percentage missing labels note different ﬁgures different scales 1912 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 In passing note important special case combine pairwise preferences RPC means simple voting strategy true eventually compute kind score label f ix Rxλi λ j cid2 1cid2 jcid6icid2m ﬁrst sight appear comparable utility functions cid2 f ix α jh jx λi j 71 72 LL However despite formal resemblance note 71 directly comparable 72 In par ticular base functions preference predicates L L 0 1 mappings instead scoring functions X L R mappings Moreover opposed 72 number functions predetermined number labels m relevance weighing coeﬃcients αi needed ShalevShwartz Singer 56 learn utility functions f basis different type training information real values gλi reﬂect relevance labels λi input x Binary preferences labels λi λ j weighted difference gλi gλ j value considered degree importance ordering λi ahead λ j This framework deviates purely qualitative setting preference information modeled form order relations Another interesting generalization utilitybased approach label ranking framework Aiolli 1 allows specify qualitative quantitative preference constraints utility functions In addition pairwise pref erence constraints use interprets constraints utility function Aiolli 1 allows straints type λi cid8x τ means value utility function f ix ti ti numerical threshold There previous work theoretical foundations label ranking We mentioned Dekel et al 17 introduced generalized ranking error assumes procedure decomposing preference graph subgraphs deﬁnes generalized error fraction subgraphs exactly agreement learned utility function Ha Haddawy 26 discuss variety different ranking loss functions introduce different extension Kendalls tau With respect predictive performance Usunier et al 59 analyze generalization properties binary classiﬁers trained interdependent data certain types structured learning problems bipartite ranking As mentioned Section 2 label ranking pairwise preference models viewed generalization learning tasks There considerable recent work tasks In particular pairwise classiﬁcation studied indepth area support vector machines 32 references We refer 22 Section 8 brief survey work pairwise classiﬁcation relation learning class binarization techniques Another special scenario application label ranking algorithms multilabel problems For example Crammer Singer 16 consider variety online learning algorithms problem ranking possible labels multilabel text categorization task They investigate set algorithms maintain prototype possible label order labels example according response signal returned prototypes 11 demonstrates general technique allows rank possible labels multilabel problem select appropriate threshold relevant irrelevant labels It wellknown pairwise classiﬁcation special case Error Correcting Output Codes ECOC 18 pre cisely generalization introduced 2 Even ECOC allows ﬂexible decomposition original problem simpler ones pairwise approach advantage provides ﬁxed domain independent nonstochastic decomposition good overall performance In experimental studies including 2 performed en par better competing decoding matrices While ﬁnding good encoding matrix open problem 49 said pairwise classiﬁcation eﬃcient decoding schemes Even train quadratic number classiﬁers training extent testing performed linear time discussed Section 4 ECOC matrices produce necessary redundancy deﬁning binary prediction problems labels expensive train What important pairwise case special advantages connection ranking preference learning problems In particular clearly deﬁned semantics terms pairwise comparison tween alternatives discussed Section 3 produces output binary preference relation established concept preference modeling decision theory As opposed semantics model compares classes subset positive subset negative ones possible ECOC unclear For example prediction λ3 cid4 λ2 obviously indicates λ3 ranked λ2 interpretations conceivable prediction λ3 λ5 cid4 λ1 λ2 Without going mention interpretations produce complications regard training models decoding step In case generalizing pairwise approach label ranking setting appears diﬃcult clas siﬁcation setting information class membership easily generalized single labels instance belongs λ3 set labels instance belongs λ3 λ5 The main reason label ranking single piece information concern class membership preference order information naturally relates pairs labels E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1913 8 Conclusions In paper introduced learning algorithm label ranking problem investigated properties theoretically empirically The merits method called ranking pairwise comparison RPC summarized follows Firstly ﬁnd RPC simple intuitively appealing elegant approach especially natural general ization pairwise classiﬁcation Besides RPC completely line preference modeling based binary preference relations established approach decision theory Secondly modular conception RPC allows combining different pairwise learning ranking methods convenient way For example different loss functions minimized simply changing ranking procedure need retrain binary models Section 5 Thirdly RPC superior alternative approaches regard eﬃciency computational complexity shown theoretically experimentally cf Sections 4 6 competitive terms predic tion quality Fourthly existing label ranking methods inherently restricted linear models RPC general choice base learner principle binary classiﬁer Finally note RPC appears attractive regard extension label ranking problem learning general preference relations label set L In fact practical applications reasonable relax assumption strictness allow indifference labels represent preferences terms partial instead total orders The learning pairwise preference predicates deﬁnitely suitable utility based methods utility function necessarily induces total order represent partial orders Extensions kind constitute important aspects ongoing work Acknowledgements We like thank anonymous reviewers insightful comments helped considerably improve paper This research supported German Research Foundation DFG Appendix A Transitivity properties pairwise preferences Our pairwise learning scheme introduced Section 3 produces preference relation Rx ﬁrst step inducing ranking τx As transitivity pairwise preferences important properties preference modeling interesting question sort transitivity guaranteed Rx Indeed pairwise preferences induced single ranking obviously transitive clear property preserved merging different rankings probabilistic way In fact recall instance x X associated probability distribution Sm cf Section 51 Such distribution induces unique probability distribution pairwise preferences pi j Pλi cid4 λ j cid2 Pτ τ Sm τ iτ j A1 An interesting ﬁnding pairwise preferences A1 satisfy form transitivity albeit relatively weak j k 1 m pik cid3 pi j p jk 1 More formally prove following theorem A2 Theorem 4 Consider probability distribution set rankings Sm The pairwise preferences induced distribution A1 satisfy A2 Proof Consider labels λi λ j λk Obviously need distinguish rankings labels order Thus partition Sm equivalence classes S jk S ikj Ski j S jk τ Sm τ τ j τ k classes deﬁned analogously Let qi jk df PS jk cid2 Pτ τ Sm τ iτ jτ k q qi jk qikj q jik q jki qki j qkjicid19 0 16 1914 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 Now consider probabilities p pi j p jk pikcid19 pairwise probabilities A1 Finding distribution rankings induces probabilities obviously comes solving linear equations form A q p A matrix dimension 3 6 01 entries qi jk qikj q jik q jki qki j qkji 1 The set solutions problem expressed pi j p jk 1 v 1 p jk u v pik pi j u 1 pik u v u v qi jk qikj q jik q jki qki j qkji u v 0 1 Additionally components q nonnegative If satisﬁed u v 0 pik cid3 pi j fourth entry A2 holds In case nonnegativity violated pi j p jk 1 pik pi j In second case u increased pi j pik obtains solution vector cid4 pi j p jk 1 1 pik pi j p jk 0 1 pi j pi j pik 0 cid5cid19 nonnegative pik cid3 pi j p jk 1 In ﬁrst case v increased 1 pi j p jk obtains solution vector cid4 0 pi j pik pi j pi j p jk pik 0 1 pi j p jk cid5cid19 nonnegative pik cid2 pi j p jk This inequality equivalent pkj cid3 pkj p ji 1 pkj 1 p jk transitivity property A2 holds reciprocal probabilities In similar way veriﬁes A2 hold case pi j p jk 1 pik pi j In summary probability distribution Sm induces probabilities pi j p jk pik exists probabilities satisfy A2 cid2 It interesting note A2 special type cid19transitivity A socalled tnorm generalized logical conjunc tion binary operator cid19 0 12 0 1 associative commutative monotone satisﬁes cid190 x 0 cid191 x x x Operators kind introduced context probabilistic metric spaces 55 studied intensively fuzzy set theory recent years 39 A binary relation R A A called cid19transitive satisﬁes Ra c cid3 cid19Ra b Rb c b c A Therefore condition A2 expresses cid19transitivity respect Lukasiewicz tnorm deﬁned cid19x y maxx y 1 0 An interesting idea guarantee condition hold replace original ensemble pairwise predictions cid19transitive closure 47 cid19 aforementioned Lukasiewicz tnorm References 1 Fabio Aiolli A preference model structured supervised learning tasks Proceedings Fifth IEEE International Conference Data Mining ICDM05 IEEE Computer Society 2005 pp 557560 2 Erin L Allwein Robert E Schapire Yoram Singer Reducing multiclass binary A unifying approach margin classiﬁers Journal Machine Learning Research 1 2000 113141 3 Noga Alon Ranking tournaments SIAM Journal Discrete Mathematics 20 1 pp 137142 4 Rajarajeswari Balasubramaniyan Eyke Hüllermeier Nils Weskamp Jörg Kämper Clustering gene expression data local shapebased similarity measure Bioinformatics 21 7 2005 10691077 5 John J Bartholdi III Craig A Tovey Michael A Trick Voting schemes diﬃcult tell won election Social Choice Welfare 6 2 1989 157165 6 Catherine L Blake Christopher J Merz UCI repository machine learning databases 1998 Data available httpwwwicsuciedumlearn MLRepositoryhtml 7 Craig Boutilier Ronen Brafman Carmel Domshlak Holger Hoos David Poole CPnets A tool representing reasoning conditional ceteris paribus preference statements Journal Artiﬁcial Intelligence Research 21 2004 135191 8 Ralph A Bradley Milton E Terry The rank analysis incomplete block designsI The method paired comparisons Biometrika 39 1952 324345 9 Steven J Brams Peter C Fishburn Voting procedures KJ Arrow AK Sen K Suzumura Eds Handbook Social Choice Welfare vol 1 Elsevier 2002 Chapter 4 10 Pavel B Brazdil Carlos Soares JP da Costa Ranking learning algorithms Using IBL metalearning accuracy time results Machine Learn ing 50 3 March 2003 251277 11 Klaus Brinker Johannes Fürnkranz Eyke Hüllermeier A uniﬁed model multilabel classiﬁcation ranking Proceedings 17th European Conference Artiﬁcial Intelligence ECAI06 2006 pp 489493 12 Klaus Brinker Johannes Fürnkranz Eyke Hüllermeier Label ranking learning pairwise preferences Technical Report TUDKE200701 Knowledge Engineering Group TU Darmstadt 2007 13 William W Cohen Robert E Schapire Yoram Singer Learning order things Journal Artiﬁcial Intelligence Research 10 1999 243270 14 Don Coppersmith Lisa Fleischer Atri Rudra Ordering weighted number wins gives good ranking weighted tournaments Proceedings ACMSIAM Symposium Discrete Algorithms SODA 2006 pp 776782 15 Koby Crammer Yoram Singer Ultraconservative online algorithms multiclass problems Journal Machine Learning Research 3 2003 951991 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 1915 16 Koby Crammer Yoram Singer A family additive online algorithms category ranking Journal Machine Learning Research 3 2003 10251058 17 Ofer Dekel Christopher D Manning Yoram Singer Loglinear models label ranking S Thrun LK Saul B Schölkopfand Eds Advances Neural Information Processing Systems 16 NIPS2003 MIT Press 2004 18 Thomas G Dietterich Ghulum Bakiri Solving multiclass learning problems errorcorrecting output codes Journal Artiﬁcial Intelligence Research 2 1995 263286 19 Jon Doyle Prospects preferences Computational Intelligence 20 2 2004 111136 20 János Fodor Marc Roubens Fuzzy Preference Modelling Multicriteria Decision Support Kluwer Academic Publishers 1994 21 Jerome H Friedman Another approach polychotomous classiﬁcation Technical report Department Statistics Stanford University Stanford CA 1996 22 Johannes Fürnkranz Round robin classiﬁcation Journal Machine Learning Research 2 2002 721747 23 Johannes Fürnkranz Round robin ensembles Intelligent Data Analysis 7 5 2003 385404 24 Johannes Fürnkranz Eyke Hüllermeier Pairwise preference learning ranking N Lavraˇc D Gamberger H Blockeel L Todorovski Eds Pro ceedings 14th European Conference Machine Learning ECML03 Cavtat Croatia Lecture Notes Artiﬁcial Intelligence vol 2837 SpringerVerlag 2003 25 Johannes Fürnkranz Eyke Hüllermeier Preference learning Künstliche Intelligenz 19 1 2005 6061 26 Vu Ha Peter Haddawy Similarity personal preferences Theoretical foundations empirical analysis Artiﬁcial Intelligence 146 2003 149173 27 Peter Haddawy Vu Ha Angelo Restiﬁcar Benjamin Geisler John Miyamoto Preference elicitation theory reﬁnement Journal Machine Learning Research 4 2003 317337 28 Sariel HarPeled Dan Roth Dav Zimak Constraint classiﬁcation A new approach multiclass classiﬁcation N CesaBianchi M Numao R Reischuk Eds Proceedings 13th International Conference Algorithmic Learning Theory ALT02 Lübeck Germany Springer 2002 pp 365379 29 Sariel HarPeled Dan Roth Dav Zimak Constraint classiﬁcation multiclass classiﬁcation ranking Suzanna Becker Sebastian Thrun Klaus Obermayer Eds Advances Neural Information Processing Systems 15 NIPS02 2003 pp 785792 30 Trevor Hastie Robert Tibshirani Classiﬁcation pairwise coupling MI Jordan MJ Kearns SA Solla Eds Advances Neural Information Processing Systems 10 NIPS97 MIT Press 1998 pp 507513 31 Ralf Herbrich Thore Graepel Peter BollmannSdorra Klaus Obermayer Supervised learning preference relations Proceedings des Fachgruppentr effens Maschinelles Lernen FGML98 1998 pp 4347 32 ChihWei Hsu ChihJen Lin A comparison methods multiclass support vector machines IEEE Transactions Neural Networks 13 2 March 2002 415425 33 Eyke Hüllermeier Johannes Fürnkranz Ranking pairwise comparison A note risk minimization Proceedings IEEE International Con ference Fuzzy Systems FUZZIEEE04 Budapest Hungary 2004 34 Eyke Hüllermeier Johannes Fürnkranz Comparison ranking procedures pairwise preference learning Proceedings 10th International Conference Information Processing Management Uncertainty KnowledgeBased Systems IPMU04 Perugia Italy 2004 35 Thorsten Joachims Optimizing search engines clickthrough data Proceedings 8th ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD02 ACM Press 2002 pp 133142 36 Thorsten Joachims Laura Granka Bing Pan Helene Hembrooke Geri Gay Accurately interpreting clickthrough data implicit feedback Proceedings 28th Annual International ACM Conference Research Development Information Retrieval SIGIR05 2005 37 Roni Khardon Gabriel Wachman Noise tolerant variants perceptron algorithm The Journal Machine Learning Research 8 2007 227248 38 Maurice G Kendall Rank Correlation Methods Charles Griﬃn London 1955 39 ErichPeter Klement Radko Mesiar Endre Pap Triangular Norms Kluwer Academic Publishers 2002 40 Stefan Knerr Léon Personnaz Gérard Dreyfus Singlelayer learning revisited A stepwise procedure building training neural network F Fogelman Soulié J Hérault Eds Neurocomputing Algorithms Architectures Applications NATO ASI Series vol F68 SpringerVerlag 1990 pp 4150 41 Stefan Knerr Léon Personnaz Gérard Dreyfus Handwritten digit recognition neural networks singlelayer training IEEE Transactions Neural Networks 3 6 1992 962968 42 Ulrich HG Kreßel Pairwise classiﬁcation support vector machines B Schölkopf CJC Burges AJ Smola Eds Advances Kernel Methods Support Vector Learning MIT Press Cambridge MA 1999 pp 255268 Chapter 15 43 Erich L Lehmann HJM DAbrera Nonparametrics Statistical Methods Based Ranks rev ed PrenticeHall Englewood Cliffs NJ 1998 44 BaoLiang Lu Masami Ito Task decomposition module combination based class relations A modular neural network pattern classiﬁcation IEEE Transactions Neural Networks 10 5 September 1999 12441256 45 John I Marden Analyzing Modeling Rank data Chapman Hall London 1995 46 Donald Michie David J Spiegelhalter CC Taylor Machine Learning Neural Statistical Classiﬁcation Ellis Horwood 1994 Data available ftp ftpnccupptpubstatlog 47 Helga Naessens Hans De Meyer Bernard De Baets Algorithms computation Ttransitive closures IEEE Trans Fuzzy Syst 10 2002 541551 48 SangHyeun Park Johannes Fürnkranz Eﬃcient Pairwise Classiﬁcation Proceedings 17th European Conference Machine Learning ECML 07 Warsaw Poland SpringerVerlag September 2007 pp 658665 49 Edgar Pimenta João Gama André Carvalho Pursuing best ECOC dimension multiclass problems Proceedings 20th International Florida Artiﬁcial Intelligence Research Society Conference FLAIRS07 2007 pp 622627 50 John Platt Probabilistic outputs support vector machines comparison regularized likelihood methods AJ Smola P Bartlett B Schoelkopf D Schuurmans Eds Advances Large Margin Classiﬁers Cambridge MA MIT Press 1999 pp 6174 51 David Price Stefan Knerr Léon Personnaz Gérard Dreyfus Pairwise neural network classiﬁers probabilistic outputs G Tesauro D Touretzky T Leen Eds Advances Neural Information Processing Systems 7 NIPS94 MIT Press 1995 pp 11091116 52 Filip Radlinski Thorsten Joachims Learning rank implicit feedback Proceedings ACM Conference Knowledge Discovery Data Mining KDD05 2005 53 Ryan Rifkin Aldebaro Klautau In defense onevsall classiﬁcation Journal Machine Learning Research 5 2004 101141 54 Michael S Schmidt Herbert Gish Speaker identiﬁcation support vector classiﬁers Proceedings 21st IEEE International Conference Acoustics Speech Signal Processing ICASSP96 Atlanta GA 1996 pp 105108 55 B Schweizer A Sklar Probabilistic Metric Spaces NorthHolland New York 1983 56 Shai ShalevShwartz Yoram Singer Eﬃcient learning label ranking soft projections polyhedra Journal Machine Learning Research 7 2006 15671599 57 Charles Spearman The proof measurement association things American Journal Psychology 15 1904 72101 58 Gerald Tesauro Connectionist learning expert preferences comparison training D Touretzky Ed Advances Neural Information Processing Systems 1 NIPS88 Morgan Kaufmann 1989 pp 99106 59 Nicolas Usunier MassihReza Amini Patrick Gallinari Generalization error bounds classiﬁers trained interdependent data Y Weiss B Schölkopf J Platt Eds Advances Neural Information Processing Systems 18 NIPS 2005 MIT Press 2006 pp 13691376 1916 E Hüllermeier et al Artiﬁcial Intelligence 172 2008 18971916 60 Jun Wang Artiﬁcial neural networks versus natural neural networks A connectionist paradigm preference assessment Decision Support Systems 11 1994 415429 61 Ian H Witten Eibe Frank Data Mining Practical Machine Learning Tools Java Implementations Morgan Kaufmann San Francisco 2000 62 TingFan Wu ChihJen Lin Ruby C Weng Probability estimates multiclass classiﬁcation pairwise coupling Journal Machine Learning Re search 5 Aug 2004 9751005