Artiﬁcial Intelligence 173 2009 240257 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A comparative runtime analysis heuristic algorithms satisﬁability problems Yuren Zhou ac Jun He b Qing Nie c School Computer Science Engineering South China University Technology Guangzhou 510640 China b Department Computer Science University Wales Aberystwyth Ceredigion SY23 3DB UK c Department Mathematics University California Irvine CA 926973875 USA r t c l e n f o b s t r c t Article history Received 18 March 2008 Received revised form 23 July 2008 Accepted 1 November 2008 Available online 7 November 2008 Keywords Boolean satisﬁability Heuristic algorithms Random walk 1 1 EA Hybrid algorithm Expected ﬁrst hitting time Runtime analysis 1 Introduction The satisﬁability problem basic core NPcomplete problem In recent years lot heuristic algorithms developed solve problem experiments evaluated compared performance different heuristic algorithms However rigorous theoretical analysis comparison rare This paper analyzes compares expected runtime basic heuristic algorithms RandomWalk 1 1 EA hybrid algorithm The runtime analysis heuristic algorithms 2SAT instances shows expected runtime heuristic algorithms exponential time polynomial time Furthermore heuristic algorithms advan tages disadvantages solving different SAT instances It demonstrates expected runtime upper bound RandomWalk arbitrary kSAT k cid2 3 O k 1n presents kSAT instance cid2k 1n expected runtime bound 2008 Elsevier BV All rights reserved The satisﬁability problem SAT propositional formula plays central role science artiﬁcial intelli gence It ﬁrst proposed NPcomplete problem 521 basic core NPcomplete problems 10 In addition theoretical importance SAT problem directly applied VLSI formal veriﬁcation software automation Researchers trying look effective algorithm SAT problem Since SAT problem NP complete problem nature polynomial algorithm currently available solve prove algorithm exist In fact basic conjecture modern science mathematics polynomial algorithm exists NPcomplete problems At present main methods solving SAT problems complete algorithms 3634 incomplete algorithms 7121315202527293132 There successful complete algorithms SATO 34 A complete algorithm explores search space termine given propositional formula satisﬁable time complexity usually exponential An incomplete algorithm carry complete search search space instead explores search space heuristic information limited time correct answer certainty Since 1990s use incomplete algorithm solving SAT problem grown quickly The basic incom plete heuristic methods RandomWalk algorithm 25 GSAT algorithm 1331 WalkSat algorithm 32 UnitWalk 15 Corresponding author School Computer Science Engineering South China University Technology Guangzhou 510640 China Email addresses yrzhouscuteducn Y Zhou junheieeeorg J He qniemathuciedu Q Nie 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200811002 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 241 populationsearchbased evolutionary algorithms 71220 In recent years powerful concepts tech niques statistical physics applied SAT problem One incomplete algorithms known survey propagation 422 based statistical physics methods shows good performance diﬃcult randomly generated SAT instances It known earliest applications statistical physics optimization prob lem simulated annealing algorithm 19 WalkSat 32 probability selection mechanism similar simulated annealing algorithm For heuristic algorithms SAT problem theoretical results computational complexities ob tained extent Papadimitiou 25 ﬁrst prove average time upper bound RandomWalk 2SAT O n2 Schöning 29 presented restarting localsearch algorithm satisﬁable kCNF formula n variables algorithm repeat O 21 1 k n times average ﬁnd satisfying assignment Specially k 3 average time O 1334n upper bound exhaustive search O 2n There improve ments upper bound hybrid algorithms based randomized algorithms Paturi et al 27 Schöning 29 O 1324n 18 O 1322n 28 Alekhnovich et al 2 proved clause density 163 average time complexity RandomWalk 3SAT linear Since incomplete heuristic algorithms SAT problems comparing understanding working principals heuristic algorithms useful The ﬁrst thing accept algorithm beats algorithms problems There numerical experiments compared heuristic algorithms SAT problems theoretical study rare This paper analyzes compares expected running time basic heuristic algorithms RandomWalk 1 1 EA hybrid algorithm We use absorbing Markov chains model search processes heuristic algorithms use explicit expressions ﬁrst hitting time Markov chain analyze estimate expected runtime Through runtime analysis SAT instances expected runtime heuristic algorithms exponential polynomial We ﬁnd heuristic algorithms comparative advantage different circumstances The rest paper organized follows Section 2 introduces concepts SAT problem heuristic algorithms SAT problem ﬁrst hitting time absorbing Markov chain Section 3 discusses worst case bound worstcase example RandomWalk Section 4 analyzes compares expected runtime bounds heuristic algorithms 2SAT instances Section 5 presents conclusions suggestions search 2 Heuristic algorithms satisﬁability ﬁrst hitting time Markov chain 21 The SAT problem We begin stating deﬁnitions notations paper In Boolean logic literal variable negation clause disjunction literals The formula f c1 c2 cm k conjunctive normal form kCNF conjunction clauses clause disjunction k literals We view CNF Boolean formula Boolean function set clauses Satisﬁability problem determining variables given Boolean formula assigned truth values way formula evaluate true SAT originally stated decision problem In paper consider general MaxSAT goal look assignment satisﬁes maximum number clauses Evolutionary algorithms EAs heuristic algorithms applied SAT NPcomplete problems EAs usually use ﬁtness value guide search process In MaxSAT formulation ﬁtness value deﬁned number satisﬁed clauses ﬁtx c1x c2x cmx 1 cix 1 cid2 cid2 m represents true value ith clause This ﬁtness function EAs SAT problems Throughout paper x x1 xn y y1 yn 0 1n denote Hx y Hamming distance xi yi We denote x x1 xn let S x x S 0 1n x cid2 n i1 points x y Hx y 0 1 n partition search space S 0 1n 22 Heuristic algorithms SAT problem RandomWalk ﬁrst introduced Papadimitiou 25 basic incomplete algorithms heuristics developed based improvement algorithm WalkSAT 32 combines RandomWalk greed bias assignments satisfy clauses RandomWalk algorithm ﬁrst randomly selects clause satisﬁed CNF randomly selects ﬂip clause Algorithm 1 242 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 Algorithm 1 The RandomWalk algorithm begin initialization Select initial bit string x random terminationcondition hold Select c unsatisﬁed clause chosen random Select xi variable c chosen random Flip value xi od end Evolutionary algorithms inspired modeling processes natural selection genetic evolution Here consider simple EA mutation selection approaches population size 1 denoted 1 1 EA 9 1 1 EA simple effective random hillclimbing EA Its general description Algorithm 2 1 1 EA begin initialization Choose randomly initial bit string x terminationcondition hold Mutation y mutatex Selection If ﬁtness y ﬁtnessx x y od end 1 1 EA generally uses kinds mutation called local mutation global mutation 1 Local mutation randomly chooses bit xi 1 cid2 cid2 n individual x x1 xn 0 1n ﬂips 2 Global mutation ﬂips bit individual x x1 xn 0 1n independently probability 1 n The expected number bit ﬂips global mutation 1 The hillclimbing algorithm usually trapped region local optimum needs restarted random new assignment Another widelyused mechanism escaping local optimum maximization problem permit search occasional downhill moves The following hybrid strategy combines 1 1 EA RandomWalk closely related WalkSat 32 allows possibility downhill moves Algorithm 3 The hybrid algorithm local 1 1 EA RandomWalk begin initialization Set parameters choose randomly initial bit string terminationcondition hold With probability p follow RandomWalk scheme With probability 1 p follow Local 1 1 EA scheme od end 23 The absorbing Markov chain Most heuristic algorithms memoryless sense processes selecting point search space depend current point This allows model search processes absorbing Markov chains absorbing set optimal solution s Such models widely heuristic algorithms Simulated Annealing 1 Genetic Algorithms 1424 Basic knowledge absorbing Markov chains literature random processes 17 Let Xt t 0 1 denote discrete homogeneous absorbing Markov chain ﬁnite state space S T transient state set H S T absorbing set Assume r absorbing states t transient states T t H r denotes cardinality set transition matrix written canonical form cid3 cid4 P I O R Q I rbyr identity matrix O rbyt zero matrix R nonzero tbyr matrix Q tbyt matrix For power P standard matrix argument shows region I remains I This corresponds fact Markov chain reaches absorbing state leave absorbing state Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 243 Deﬁnition 1 Let Xt t 0 1 absorbing Markov chain The ﬁrst hitting time status S absorbing status set H τi mint t cid3 0 Xt H X0 righthand involves set let τi We interested question Given chain starts state expected number steps chain absorbed Theorem 1 provides answer Theorem 1 Given absorbing Markov chain Xt starts transient state let mi expected number steps chain absorbed mi Eτi Denote m miiT Then m I T 11 1 represents column vector entries 1 Proof See Ref 17 cid2 Several corollaries derived directly Theorem 1 2 Corollary 1 Let Xt t 0 1 absorbing Markov chain ﬁnite state space S 0 1 n n 1 absorbing state set 0 n 1 transition probabilities deﬁned follows 1 For 0 n 1 cid5 pi j 1 j 0 2 For 1 cid2 cid2 n pi j ai bi 1 ai bi 0 j 1 j 1 j Then absorbing Markov chain mean ﬁrst hitting time absorbing state given mn1 0 cid2 1 a1 mn n1 h1 1 cid2 h j1 1 ah1 cid2 cid10 n n j1 j 1 1 bi ai cid10 h j bi ai mn1 mn1 bn cid10 mk1 mk mn n ik m0 0 cid2 nk1 j0 1 jk1 cid10 j i0 bki aki 1 ak k n 1 2 bi ai Corollary 2 Let Xt t 0 1 absorbing Markov chain ﬁnite state space S 0 1 n absorbing state set 0 transition probabilities deﬁned follows 1 For 0 n 1 cid5 pi j 1 j 0 2 For 1 cid2 n pi j ai bi 1 ai bi 0 j 1 j 1 j 244 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 3 For n pi j ai 1 ai 0 j 1 j Then absorbing Markov chain mean ﬁrst hitting time absorbing state given m0 0 mi mi1 1 ai mn mn1 1 cid2 ni1 j0 1 ji1 cid10 j h0 bih aih 1 n 1 The difference corollaries lies fact Markov chain Corollary 1 absorbing states Markov chain Corollary 2 He et al 14 Corollary 2 estimate expected running time evolutionary algorithms For time complexity discussion analysis Theorem 1 corollaries play key role These methods close Markov chain analysis stochastic local search algorithms Schöning 2930 The main difference We use linear 2 estimate absorbing time Markov chain Schöning calculated success probability Now introduce vector norms average vector norm maximum vector norm vector analysis For vector m miiS let μ0i P X0 initial distribution average vector norm cid6mcid61 maximum vector norm cid6mcid6 deﬁned cid11 μimi iS cid6mcid61 cid6mcid6 max iS mi Specially initial distribution uniform distribution S μ0i 1 S S cid6mcid61 1 S cid11 iS mi Norms cid6mcid61 cid6mcid6 present average case worst case performance measures respectively time complexity analysis 3 Bounds RandomWalk It known simple algorithm complete enumeration needs cid22n steps ﬁnd satisfying assign ment SAT problem n variables In following shall general upper bound average iteration number RandomWalk kSAT O k 1n We construct SAT instance bound tight expected runtime RandomWalk cid2k 1n Proposition 1 The expected runtime RandomWalk kSAT k cid3 3 instance O k 1n S Hx y denote distance point x S set S Proof Let S 0 1n search space S min yS Deﬁne D D x S dx 0 1 n Then search space S partitioned n 1 subspaces cid12 n i0 D satisfying assignment set given kCNF formula ω Let dx Suppose given string x D 1 cid2 cid2 n clause σ satisﬁed exists k bits ﬂipped decrease dx 1 Since RandomWalk picks variable unsatisﬁed clause ﬂips truth assignment probability RandomWalk transfers x string y D i1 1 k probability transfers x string y D i1 1 1 k Construct auxiliary homogeneous Markov chain deﬁned state space D0 D1 Dn transi tion matrix 1 1 k 0 0 0 0 0 1 k 0 0 0 k1 k 0 0 0 0 0 k1 k 0 0 0 0 0 0 0 0 1 k 0 0 1 0 0 0 k1 k 0 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 245 According Corollary 2 mean ﬁrst hitting time absorbing state D0 m0 0 mi mi1 k k mn mn1 1 cid2 ni1 j0 k 1 j1 1 n 1 By induction mn mn1 1 n 1k kk 1 k 2 n 1k kk 1 k 2 cid19 n1cid11 cid20 k 1nl1 n 1 cid3 l1 k 1n1 1 k 2 cid4 n 1 Hence cid6mcid6 mn O This completes proof cid2 cid21 k 1n cid22 We shown expected running time RandomWalk kSAT instance O k 1n mean RandomWalk needs average cid2k 1n steps ﬁnd satisfying assignment kSAT instance In following present kSAT instance ϕkx k 2 average case expected time complexity cid2k 1n Deﬁnition 2 The SAT instance ϕkx k n following clauses xi xi1 xi2 xik 1 cid2 cid2 n i1 ik ranges kelement subsets 1 n It evident SAT instance ϕkx unique satisfying assignment x Papadimitiou 26 ﬁrst proposed special cases k 3 ϕkx claimed diﬃcult instance Ran domWalk Here discuss general situation derive worst case average case bounds expected runtime RandomWalk 1 1 Proposition 2 Given integer k cid3 3 expected runtime RandomWalk SAT instance ϕkx 1 cid6mcid6 O k 1n 2 cid6mcid61 cid2k 1n Proof 1 It follows immediately Proposition 1 2 From consequence 1 suﬃcient cid6mcid61 cid7k 1n Let T x x S 0 1n x n 0 1 n partition search space S We denote Xt t 0 1 random string describing point RandomWalk iteration t Then Xt homogeneous Markov chain absorbing state set T 0 The transition probabilities subspaces described follows When 0 P Xt1 T 0 Xt T 0 1 When 1 cid2 cid2 n k 1 P Xt1 T i1 Xt T k 1 1 P Xt1 T i1 Xt T 1 k 1 k cid22 cid22 cid21 ni k1 cid21 ni k1 cid21 ni k1 cid21 ni k1 1 cid22 cid22 k When n k 1 cid2 n P Xt1 T i1 Xt T 1 246 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 Denote ai 1 k1 k ni k1 1ni k1 ni k1 1ni k1 bi k1 k ai 1 bi 0 1 cid2 cid2 n k 1 1 cid2 cid2 n k 1 n k 1 cid2 n n k 1 cid2 n Construct auxiliary homogeneous Markov chain Zt t 0 1 deﬁned state set S 0 1 n transition matrix 1 a1 0 0 0 0 1 a1 b1 a2 0 b1 1 a2 b2 0 0 0 0 0 0 b2 0 0 0 0 0 0 0 0 an1 1 an1 bn1 0 0 0 0 bn1 1 3 For absorbing Markov chain according Corollary 2 mean ﬁrst hitting time absorbing state 0 given m0 0 mi mi1 1 ai mi mi1 1 Note 1 cid2 cid2 n k 1 cid2 nik1 j0 1 ji1 cid10 j h0 bih aih 1 cid2 cid2 n k 1 n k 1 cid2 n 4 5 6 1 ai cid3 1 k bi ai k 1 1 cid21 ni 1 k k1 Here use inequality 1 1x From Eqs 4 5 6 cid22 cid3 k 1e kni k1 cid3 e x x cid3 0 m1 1 a1 bh1 ah1 nkcid11 jcid23 1 j2 h0 j0 nkcid11 cid21 k 1 j1 cid2 j1 h1 cid22 e k nh k1 cid3 1 k 1 k j0 Because j1cid11 h1 k cid21 nh k1 cid22 k cid2 k k j1cid11 h1 j1cid11 h1 j1cid11 h1 1 cid22 k 1 cid21 nh k1 1 n hn h 1 cid3 1 n h 1 1 n h k 2 cid4 cid2 k m1 cid3 1 k 1 k nkcid11 k 1 j1 k e j0 k k 1 k 2 cid22 e 1 1 k k cid21 cid7 k 1n cid22 cid21 k 1nk1 1 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 247 Finally monotonicity sequence mi obtain cid3cid3 cid4 m0 cid4 m1 cid3 n 1 cid4 cid4 mn cid3 n n cid21 cid7 k 1n cid22 cid21 k 1n cid7 cid22 n cid6mcid61 1 2n 0 cid3 2n 1 2n This completes proof cid2 From Proposition 2 mentioned exhaustive search upper bound 2n behaves better RandomWalk SAT instance ϕkx k 3 Droste et al 8 studied expected running time 1 1 EA special case ϕ3x demonstrated average time complexity exponential time Wei et al 33 presented class formulas involving socalled ternary chain similar ϕ3x They showed expected runtime RandomWalk ternary chain formula exponential proposed accelerating random walk problem For ϕkx according Eq 1 ﬁtness function evolutionary algorithms ﬁtϕk x cid11 xi cid11 cid11 cid11 cid21 1 1 xi1 xi2 xik cid22 1cid3icid3n 1cid3i1cid3n s k cid4 cid3 n k 1cid3i2cid3n i2cid8i1 1cid3ikcid3n ikcid8i1ikcid8ik1 cid4 cid3 n sk 1 s k 1 cid21 x s cid22 The ﬁtness function ﬁtϕk x induces MaxSAT problem ϕkx polynomial s number 1 x degree k We expect heuristic algorithms ﬁtness function diﬃculty ﬁnding allone string nonmonotone polynomial ﬁtness function misleading hints allone string 4 Behavior heuristic algorithms SAT instances In section order obtain theoretical understanding behavior different heuristic algorithms struct SAT instances analyze average time complexity RandomWalk 1 1 EA hybrid algorithm SAT instances Deﬁnition 3 For x x1 xn 0 1n SAT instance ψ1x deﬁned ψ1x x1 x2 x1 x3 x1 xn x1 x2 x1 x3 x1 xn The satisfying assignments ψ1x 0 0 1 1 We start 1 1 EA solving MaxSAT instance ψ1x According Eq 1 x k ﬁtness function ψ1x given cid5 ﬁtψ1 x 2n 1 k n 1 k 1 x 0 x 1 When x 0 x 1 ﬁtness function ﬁtψ1 x decreases increases monotonously increase number ones The ﬁtness function ﬁtψ1 x n 20 shown Fig 1 In following consider local 1 1 EA global 1 1 EA ψ1x respectively We shall ﬁnd satisfying assignment time cid2n ln n For simplicity Proposition 3 assume n We divide search space S 2n subspaces cid24 cid24 S0k S1k x x 0 S x k x x 1 S x k cid25 cid25 k 0 n 1 k 1 n 248 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 Fig 1 The ﬁtness function ψ1x n 20 Proposition 3 For SAT instance ψ1x x S uk u 0 1 k 0 n denote muk mean ﬁrst hitting time local 1 1 EA starting state x k m00 0 m0k n1 1 m1n 0 m1k n1 1 m0k 1 m1k 1 nk k1 n km0k1 m1k1 nk1 n n km1k1 m0k1 1 cid2 k cid2 n 2 n 2 cid2 k cid2 n 1 1 cid2 k cid2 n 1 n 2 1 cid2 k cid2 n 2 1 expected runtime local 1 1 EA cid6mcid6 cid2n ln n Proof Let Xt 0 1n t 0 1 random variable describing state local 1 1 EA solving SAT instance ψ1x time t transition probabilities described follows When k 0 P Xt1 S0k Xt S0k 1 When 1 cid2 k cid2 n 2 1 P Xt1 S0k1 Xt S0k k n P Xt1 S0k Xt S0k 1 k n When n 2 cid2 k cid2 n 1 P Xt1 S0k1 Xt S0k k n P Xt1 S1k1 Xt S0k 1 n P Xt1 S0k Xt S0k 1 k 1 n Similarly k n P Xt1 S1k Xt S1k 1 When n 2 1 cid2 k cid2 n 1 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 249 P Xt1 S1k1 Xt S1k 1 k n P Xt1 S1k Xt S1k k n When 1 cid2 k cid2 n 2 P Xt1 S1k1 Xt S1k 1 k n P Xt1 S0k1 Xt S1k 1 n P Xt1 S1k Xt S1k k 1 n Introduce auxiliary homogeneous Markov chain Zt z12 z1n transition probabilities deﬁned P Zt1 zvh Zt zuk P Xt1 S vh Xt S uk t 0 1 state space z00 z01 z0n1 z11 u v 0 1 h k 0 n Then Zt absorbing Markov chain absorbing state z00 z1n x S uk u 0 1 k 0 n mean ﬁrst hitting time mx equals mzuk According Theorem 1 mean ﬁrst hitting time stochastic process Zt given n m0k1 k n m0k1 k1 n m0k 1 n m0k 1 n m1k1 1 m00 0 k k m1n 0 nk n m1k1 nk n m1k1 nk1 The linear equations solved n m1k 1 n m1k 1 nk n m0k1 1 1 cid2 k cid2 n 2 1 1 cid2 k cid2 n 2 cid2 k cid2 n 1 n 2 1 cid2 k cid2 n 1 n 2 1 cid2 k cid2 n 2 1 k m00 0 m0k n1 1 m1n 0 m1k n1 1 m0k 1 m1k 1 nk k1 n km0k1 m1k1 nk1 n n km1k1 m0k1 1 cid2 k cid2 n 2 n 2 cid2 k cid2 n 1 1 cid2 k cid2 n 1 n 2 In following prove 1 1 k m0k cid2 n cid3 cid4 cid3 n 2 cid4 cid2 k cid2 n 1 induction When k n m0n2 cid3 1 n2 1 cid3 2 7 obtain cid3 n n 2 1 1 n2 cid2 n 1 cid4 cid4 cid3 n 1 1 n2 1 1 n n2 1 cid4cid4 Thus 8 holds k n 2 Assume true k cid3 n m0k1 1 k 2 cid21 n k 1m0k m1k2 2 m0k cid2 n1 1 cid22 k 7 cid4 cid3 n 1 1 k cid2 n k 2 cid3 cid2 n 1 1 k 1 cid4 Therefore 8 holds k 7 8 250 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 Similarly m1k cid2 n1 1 This proves claim cid2 nk 1 cid2 k cid2 n 2 Proposition 4 For SAT instance ψ1x expected runtime global 1 1 EA 1 cid6mcid6 O n ln n 2 cid6mcid61 cid2n ln n Proof 1 We decompose state space S 0 1n n subspaces Hamming distance string S satisfying assignment S Uk T 0k T 1k cid12 n1 k0 Uk k 0 1 n 1 cid24 cid24 T 0k T 1k x x 0 S H x x 1 S H cid21 cid21 cid22 x 0 0 cid22 x 1 1 cid25 cid25 k k In contrast notations S0k S1k proof Proposition 3 T 0k S0k T 1k cid8 S1k For x 1 S x S1k means Hamming distance x 0 0 k x T 1k means Hamming distance x 1 1 k For x Ukk 1 n 1 note ﬁtψ1 x 2n k Thus probability global 1 1 EA leads x y Uk1 U 0 greater k n n1 O n ln n Therefore cid6mcid6 cid2 1 1 2 According result 1 suﬃcient prove cid6mcid61 cid7n ln n The proof similar linear functions nonzero weights Droste et al 9 The main difference linear function optimum 0 0 SAT instance ψ1x satisfying assignments 0 0 1 1 By Chernoff bounds 23 0 cid9 1 2 probability initial string x satisﬁes 1 simplicity analysis assume 1 2 equivalent probability 1 e In order reach satisfying assignment strings needs ﬂip zeros ones Let X random variable deﬁned number generations required ﬂip zeros ones initialized string expectation cid7n randomly initialized string 1 2 cid9n integer exponentially close 1 e cid9n cid7n It cid9n ones cid9n zeros 1 2 cid9n cid2 x cid2 1 2 2 E X cid11 t1 t P X t cid11 t1 P X cid3 t Since probability bit ﬂip t 1 steps 1 1 1 2 Hence cid9n bits ﬂips t 1 steps 1 1 1 1 n t1 1 2 cid9n n t1 probability event cid4 t1 cid4 1 2 cid4 cid9n E X cid3 cid3 cid11 t1 cid3 cid3 1 1 1 1 n cid3 cid3 cid3 n 1 ln n 1 1 cid3 n 1 ln n cid21 1 e 1 2 cid9 cid7n ln n cid4n1 ln n cid4 1 2 cid4 cid9n cid3 1 1 n cid22 In use 1 1 Therefore cid6mcid61 cid3 cid21 1 e cid7n 1n 1 n n cid2 e cid22 cid7n ln n cid7n ln n This completes proof cid2 Papadimitiou 25 proved RandomWalk satisﬁable 2SAT reach satisfying assignment time O n2 theory random walks In following Proposition 5 shall demonstrate RandomWalk cid2n2 worst case average case expected runtime bound 2SAT instance ψ1x Now introduce lemma provides upper bound tail binomial distribution function It Proposition 5 estimating average case expected runtime RandomWalk Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 251 Lemma 1 Let X random variable following binomial distribution parameters n p Given integer 0 cid2 k cid2 n cumulative distribution expressed F k P X cid2 k kcid11 cid4 cid3 n i0 pi1 pni Then F k cid2 e 2npk2 n Proof It follows immediately Hoeffdings inequality 16 cid2 Proposition 5 For SAT instance ψ1x average case expected runtime RandomWalk cid6mcid61 cid2n2 Proof According discussion suﬃcient cid6mcid61 cid7n2 We denote Xt t 0 1 stochastic process RandomWalk SAT instance ψ1x Then Xt homogeneous Markov chain absorbing states 0 0 1 1 The transition probabilities subspaces described follows When 0 P Xt1 S Xt S 1 When 1 cid2 cid2 n 1 P Xt1 S i1 Xt S 12 P Xt1 S i1 Xt S 12 When n P Xt1 S Xt S 1 Construct auxiliary homogeneous Markov chain Zt t 0 1 deﬁned state space 0 1 n transition probabilities P Zt1 j Zt P Xt1 S j Xt S j 0 n According Corollary 1 mean ﬁrst hitting time absorbing chain Zt given mi 0 1 n Hence cid6mcid61 1 2n 1 2n m1 cid3cid3 n 0 ncid11 i1 cid4 m0 cid4 cid3 n 1 cid4 cid3 n cid4 cid4 mn cid3 n n cid3 3n2 16 1 2n 3n4cid11 cid4 cid3 n in4 cid22 n3 8 Lemma 1 cid21 cid3 3n2 16 cid21 cid7 n2 1 2e cid22 Finally reach conclusion cid2 For SAT instance ψ1x Propositions 35 cid2n ln n expected runtime bound 1 1 EA better cid2n2 expected runtime bound RandomWalk In following construct SAT instance ψ2x shall opposite situation conditions Deﬁnition 4 The SAT instance ψ2x following clauses xi 1 cid2 cid2 n xi x j j 1 n2 cid8 j 252 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 Fig 2 The ﬁtness function ψ2x n 21 The satisfying assignment SAT instance ψ2x True assignment 1 1 According Eq 1 x k ﬁtness function ψ2x given ﬁtψ2 x nn 1 kk n 1 polynomial k degree 2 The ﬁtness function ﬁtψ2 x n 21 shown Fig 2 We note ﬁtness function local minimum x 10 string global optimum If local hillclimbing 1 1 EA starts x x 10 reach global optimum Proposition 6 Let Tk x x S 0 1n x n k k 0 1 n partition search space 0 1n n odd number For SAT instance ψ2x Tk k 1 n expected runtime local 1 1 EA cid26 mi k n1 2 O n ln n k cid3 n1 2 Proof When k n1 initial string Tk reach satisfying assignment 1 1 2 ﬁtness function ﬁtψ2 x decrease monotonously x increases local 1 1 EA starting 2 similar OneMAX11 local 1 1 EA starting initial string Tk ﬁnd When k cid3 n1 satisfying assignment O n ln n average cid2 RandomWalk ψ2x worst case expected runtime bound ψ1x Proposition 7 For SAT instance ψ2x expected runtime RandomWalk cid6mcid6 cid2n2 Proof It suﬃcient cid6mcid6 cid7n2 The following proof similar Proposition 2 Let Xt t 0 1 random variable describing point RandomWalk iteration t Then Xt homogeneous Markov chain absorbing state set T 0 The transition probabilities subspace described follows When k 0 P Xt1 T 0 Xt T 0 1 When 1 cid2 k cid2 n 1 P Xt1 Tk1 Xt Tk 1 1 2 n k P Xt1 Tk1 Xt Tk 1 2 1 n k n k 1 n k Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 253 When k n P Xt1 Tk1 Xt Tk 1 Denote nk ak 1 1 2 nk bk 1 1nk 2 ak 1 bk 0 1nk 1 cid2 k cid2 n 1 1 cid2 k cid2 n 1 k n k n Construct auxiliary homogeneous Markov chain Zt t 0 1 deﬁned state set 0 1 n transition matrix 3 proof Proposition 2 For absorbing Markov chain according Corollary 2 mean ﬁrst hitting time absorbing state 0 given 9 m0 0 mk mk1 1 ak mn mn1 1 cid2 nk1 j0 cid2 1 jk1 n1 k1 1 ak1 1 a1 cid10 j i0 1 bik aik 1 cid2 k cid2 n 1 cid2 k j1 cid10 k j bi ai When n cid3 2 k cid2 n 2 1 n k n k 2 n kn k 1 n k 2n 1 k 1 cid3 1 1 4 bi ai k 1 1 kcid11 kcid23 j1 j It follows mn cid3 1 a1 n21cid11 kcid11 kcid23 cid19 1 1 ak1 bi ai cid20 cid22 j j1 cid4 cid21 cid7 n2 k1 cid3 n21cid11 cid3 1 2 k1 1 1 4 k 1 This completes proof cid2 Contrary prior result expected runtime 1 1 EA SAT instance ψ1x better RandomWalk demonstrate SAT instance ψ2x worst case expected runtime RandomWalk better local 1 1 EA In following analyze behavior hybrid algorithm 1 1 EA RandomWalk SAT instance ψ2x We ﬁrst set selection probability p 05 Proposition 8 If selection probability p 05 SAT instance ψ2x expected runtime hybrid algorithm cid4n2cid4 cid3 cid3 cid7 1 n2 4 e cid2 cid6mcid6 cid2 O cid3cid3 cid4n2cid4 4 e Proof For simplicity assume n odd The proof similar Proposition 7 notations speciﬁed proof Proposition 7 The transition probability stochastic process Xt t 0 1 introduced hybrid algorithm local 1 1 EA RandomWalk probability p 05 described follows When k 0 P Xt1 T 0 Xt T 0 1 When 1 cid2 k cid2 n 12 P Xt1 Tk1 Xt Tk p cid3 cid4 1 p k n 1 cid3 n k 2n k 1 cid4 1 k n P Xt1 Tk Xt Tk 1 p P Xt1 Tk1 Xt Tk p n k 2n k 1 254 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 When n 12 k cid2 n cid3 P Xt1 Tk1 Xt Tk p 1 n k 2n k 1 cid4 P Xt1 Tk Xt Tk 1 p P Xt1 Tk1 Xt Tk p k n n k 2n k 1 1 p n k n cid26 cid26 Denote ak bk p1 nk p1 nk nk 2nk1 nk 2nk1 p p 2nk1 1 p k 2nk1 n 1 cid2 k cid2 n 12 n 12 k cid2 n 1 p nk n 1 cid2 k cid2 n 12 n 12 k cid2 n 10 11 Construct auxiliary homogeneous Markov chain Zt t 0 1 deﬁned state 0 1 n space transition matrix 3 Its mean ﬁrst hitting time absorbing state 0 given Eqs 9 n2 4 Now lower bound cid7 1 By assumption p 05 according Eqs 9 e n2 cid19 1 cid20 bk ak icid11 icid23 j1 k j mn 1 a1 n1cid11 1 ai1 i1 n1cid23 bk ak cid3 1 2 kn121 n1cid23 kn121 cid3 n k n k 2 1 2n k 1 n cid4 cid21 10 11 cid22 16 n 1n 1 n32cid23 cid3 k1 1 2k 1 n cid4 Note cid19 n32cid11 cid3 I cid4cid202 1 2k 1 n cid4 2 cid3 k1 n 4 n 2n 1 n n 6 n cid3 n 3n 4 2n 1 nn3 1 2 n2 n 1n 2 2n nnn Using Stirling formula 2πn cid3 cid4 n n e n cid3 n 1 1 12n 1 cid4 cid3 cid4 n n e 2πn I cid3 1 2 1 2 n2 n 1n 2 n2 n 1n 2 cid3 cid4 2n 2n 1 nnn 2π 2n 2π 2n cid4 cid3 e 2n nn n 2 e 12 13 cid21 Stirling formula 12 cid22 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 255 n2 cid3 1 n 1n 2 2 cid21 cid22 cid7 4en 2π 2n cid4 2n cid3 2 e 1 2πn 12n 1 12n en cid21 Stirling formula 13 cid22 Therefore mn cid3 cid7 1 n2 4 e n2 That proves lower bound The upper bound proved follows Similar equations cid3 cid4 2 I n 4 n n 6 2n 1 n n cid2 n 4n 5 2n nn3 n3 n 1n 2n 3 2n nnn cid3 1 2π 2n cid3 cid4cid3 cid4 2n 2n 1 24n 1 cid4cid3 e cid4 2n 1 24n 1 2 e cid2 1 nnn cid2 2π 2n cid21 4en 1 cid22 O cid21 Stirling formula 13 cid22 1 2πn en cid21 Stirling formula 12 cid22 14 Note 1 ak cid2 4 1 cid2 k cid2 n bk ak cid2 1 1 cid2 k cid2 n 12 obtain mn 1 a1 n1cid11 1 ai1 i1 cid2 4n 4n 12 cid19 1 icid11 icid23 cid20 bk ak k j j1 n1cid23 bk ak kn121 cid3 n32cid23 cid4 1 2k 1 n 4n 32 cid21 4en O n 1 n 1 cid21 cid22 14 k1 cid22 This completes proof cid2 In Proposition 8 ﬁxed selection probability p 05 In fact p cid2 05 result upper bound holds Proposition 9 If selection probability p cid2 05 SAT instance ψ2x expected runtime hybrid algorithm cid7 1 n2 4 e n2 Proof The proof essentially Proposition 8 cid2 Proposition 10 If selection probability p cid3 n1 O n2 n SAT instance ψ2x expected runtime hybrid algorithm Proof The proof similar Proposition 8 2 according Eqs 10 11 notice p cid3 n1 n bk ak cid2 1 For 1 cid2 k cid2 n1 For n1 2 k n n k cid3 bk ak cid3 n k 2 cid2 n k n k 2 cid2 1 1 n 1 21 n k n 1 21 n k n cid4 cid4 1 p p 1 n 1 256 Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 Table 1 The expected runtime bounds RandomWalk 1 1 EA SAT instance ψ1x Global 1 1 EA Prop 4 Local 1 1 EA Prop 3 RandomWalk Prop 5 cid2n ln n cid2n ln n cid2n2 Table 2 The expected runtime bounds heuristic algorithms SAT instance ψ2x Local 1 1 EA Prop 6 x n1 2 O n ln n n1 cid3 x cid3 n 2 RandomWalk Prop 7 cid2n2 Hybrid algorithm Prop 9 10 e n2 p cid3 05 cid3 p cid3 1 cid7 1 n2 4 O n2 n1 n Then 1 cid2 j cid2 k n cid3 cid4 k j1 kcid23 j bi ai cid2 1 1 n cid4 n cid3 1 1 n cid2 e cid2 Note 1 ak cid2 2 p 1 cid2 k cid2 n cid19 mn 1 a1 cid19 n1cid11 1 ai1 i1 1 icid11 icid23 k j j1 cid20 cid2 2 p 1 n1cid11 i1 e 1 O cid20 bk ak cid21 n2 cid22 This completes proof cid2 Remark 1 For selecting probability p cid2 05 p cid3 n1 hybrid algorithm ψ2x Can better tight bounds n obtain lower upper bounds expected runtime Remark 2 For 05 p n1 However conjecture expected runtime changes exponential time polynomial time n time complexity analysis hybrid algorithm ψ2x complicated Tables 1 2 summarize expected runtime bounds heuristic algorithms solving SAT instances ψ1x ψ2x From Table 1 instance ψ1x 1 1 EA faster RandomWalk From Table 2 instance ψ2x starting initial string x satisfying x cid2 n 12 1 1 EA reach satisfying assignment RandomWalk ﬁnds satisfying assignment cid2n2 average But 1 1 EA faster RandomWalk initial string x satisﬁes x cid2 n 12 For instance ψ2x analysis demonstrates hybrid algorithm help local 1 1 EA escape local optimum It shows expected runtime hybrid algorithm changes exponential time bound polynomial time bound selection probability varies However remains unclear phase transition gradually happens worth investigating 5 Conclusion Incomplete heuristic algorithms prominent frequently applied techniques SAT problems Many experimental comparisons different heuristic algorithms reported theoretic comparisons rare This paper contributes theory heuristic algorithms SAT problems We derive expected runtime bounds RandomWalk kSAT problem We construct 2SAT instances provide analytic comparisons RandomWalk 1 1 EA hybrid algorithm instances It shown heuristic algorithms advantages disadvantages solving SAT instances expected runtime ranges polynomial time exponential time Our analysis provides insight runtime behavior heuristic algorithms Admittedly SAT instances ψ1x ψ2x considered paper relatively simple Future investigation extended broader class SAT problems heuristic algorithms UnitWalk 15 PPSZ 27 Theoretic runtime analysis comparison heuristic algorithms SAT problem lag far experimental comparisons Effort ﬁll gap theoretical studies SAT design application practical algorithms investigation challenging Y Zhou et al Artiﬁcial Intelligence 173 2009 240257 257 Acknowledgements The authors grateful anonymous reviewers valuable comments helpful suggestions improved papers quality greatly This work partially supported National Natural Science Foundation China Nos 60673062 60873078 Natural Science Foundation Guangdong Province China No 06025686 YZ NIH grants P50GM76516 R01GM75309 NSF grants DMS0511169 QN References 1 EHL Aarts JHM Korst Simulated Annealing Boltzmann Machines Wiley 1989 2 M Alekhnovich E BenSasson Linear upper bounds random walk small density random 3CNF SIAM Journal Computing 36 5 2006 12481263 3 P Beame H Kautz A Sabharwal Towards understanding harnessing potential clause learning Journal Artiﬁcial Intelligence Research 22 2004 319335 4 A Braunstein M Mezard R Zecchina Survey propagation An algorithm satisﬁability Random Structures Algorithms 27 2 2005 201226 5 S Cook The complexity theoremproving procedures Proc 3rd Ann ACM Symp Theory Computing Assoc Comput Mach New York 1971 p 151 6 M Davis H Putnam A procedure quantiﬁcation theory Journal ACM 7 3 1960 202215 7 KA DeJong WM Spears Using genetic algorithm solve NPcomplete problems Proceedings 3rd International Conference Genetic Algorithms Virginia USA 1989 pp 124132 8 S Droste T Jansen I Wegener A natural simple function hard evolutionary algorithms Proceedings Third AsiaPaciﬁc Conference Simulated Evolution Learning Nagaya Japan 2000 pp 27042709 9 S Droste T Jansen I Wegener On analysis 1 1evolutionary algorithm Theoretical Computer Science 276 12 2002 5181 10 MR Garey DS Johnson Computers IntractabilityA Guide Theory NPcompleteness Freeman New York 1979 11 J Garnier L Kallel M Schoenauer Rigorous hitting times binary mutations Evolutionary Computation 7 2 1999 167203 12 J Gottlieb E Marchiori C Rossi Evolutionary algorithms satisﬁability problem Evolutionary Computation 10 1 2002 3550 13 J Gu Eﬃcient local search largescale satisﬁability problems ACM SIGART Bulletin 3 1 1992 812 14 J He X Yao Towards analytic framework analyzing computation time evolutionary algorithms Artiﬁcial Intelligence 145 12 2003 5997 15 EA Hirsch A Kojevnikov UnitWalk A new SAT solver uses local search guided unit clause elimination Annals Mathematics Artiﬁcial Intelligence 43 2005 91111 16 W Hoeffding Probability inequalities sums bounded random variables Journal American Statistical Association 58 301 1963 1330 17 M Iosifescu Finite Markov Processes Their Applications John Wiley Sons Chichester 1980 18 K Iwama S Tamaki Improved bounds 3SAT Proceedings Fifteenth Annual ACMSIAM Symposium Discrete Algorithms 2004 pp 321322 19 S Kirkpatrick CD Gelatt MP Vecchi Optimization simulated annealing Science 220 1983 671680 20 F Lardeux F Saubion JK Hao GASAT A genetic local search algorithm satisﬁability problem Evolutionary Computation 14 2 2006 223253 21 LA Levin Universal search problems Problemy Peredachi Informatsii 9 1973 115116 Russian translation Problems Information Transmis sion 9 3 1975 265266 22 M Mezard G Parisi R Zecchina Analytic algorithmic solution random satisﬁability problems Science 297 2002 812815 23 R Motwani P Raghavan Randomized Algorithms Cambridge Univ Press Cambridge UK 1995 24 AE Nix MD Vose Modeling genetic algorithms Markov chain Annals Mathematics Artiﬁcial Intelligence 5 1 1992 7988 25 CH Papadimitiou On selecting satisfying truth assignment Proceedings 32nd Annual IEEE Symposium Foundations Computer Science 1991 pp 163169 26 CH Papadimitiou Computational Complexity Addison Wesley 1994 27 P Paturi P Pudlak ME Saks F Zane An improved exponentialtime algorithm kSAT Journal ACM 52 3 2005 337364 28 D Rolf Improved bound PPSZSchöningalgorithm 3SAT Electronic Colloquium Computational Complexity ECCC Report No 159 2005 29 U Schöning A probabilistic algorithm kSAT constraint satisfaction problems Proceedings 40th Annual Symposium Foundation Computer Science 1999 pp 410414 30 U Schöning Principles stochastic local search Proceedings Sixth Conference Unconventional Computation Kingston Canada 2007 Lecture Notes Computer Science vol 4618 2007 pp 178187 31 B Selman H Levesque D Mitchell A new method solving hard satisﬁability problems Proceedings Tenth National Conference Artiﬁcial Intelligence AAAI92 San Jose 1992 pp 440446 32 B Selman HA Kautz B Cohen Noise strategies improving local search Proceedings Twelfth National Conference Artiﬁcial Intelligence AAAI94 vol 1 1994 pp 337343 33 W Wei B Selman Accelerating random walks Proceedings 8th Intl Conference Principles Practice Constraint Programming CP2002 Lecture Notes Computer Science vol 2470 2002 pp 216232 34 H Zhang M Stickel Implementing Davis Putnam method Journal Automated Reasoning 24 12 2000 277296