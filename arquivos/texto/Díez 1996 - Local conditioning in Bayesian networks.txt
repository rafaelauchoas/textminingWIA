ELSEVIER Artificial Intelligence 87 1996 l20 Artificial Intelligence Local conditioning Bayesian networks FJ Diez Departamento Inteligencia Artificial UNED Senda de1 Rey 28040 Madrid Spain Received March 1993 revised February 1994 Abstract LC Local conditioning exact algorithm computing probability Bayesian networks networks A list developed extension Kim Pearls algorithm node guarantees nodes inside loop conditioned variables associated variable breaks The main advantage algorithm computes probability directly original network instead building cluster tree save time debugging model sparsity evidence allows pruning network The algorithm advantageous families network interact ANDOR gates A parallel implementation algorithm processor node possible case multiplyconnected networks singlyconnected 1 Introduction A Bayesian random variable network acyclic directed graph node represents probability distribution Px 9 JJWilPxi 1 xi represents possible value variable Xi pa xi instantiation prob conditional parents Xi graph For nodevariable ability property Bayesian Eq 1 The basic infer networks ence problem posteriori probability deduced conditional variable X given certain evidence parents independence priori probability The essential e 3 Xi XiXj consists computing called dseparation probabilities 2022 Pxe Email fjdiezdiaunedes WWW httpwwwdiaunedesYjdiez 00043702961500 SSDIOOO4370295001182 Copyright 1996 Elsevier Science BV All rights reserved El Diehrjiciul Intrlliqwcr X7 1996 J20 Fig I Evidence propagation message passing For singlyconnected x gorithm algorithms appendixwhile numerical values conditional 45 problem NPhard complexity I521 hut general case harder heavily depends structure networkthere networks exists elegant efficient exact al exact approximate methods depends mainly exact approximate methods time complexity example probabilities The bestknown exact methods clustering conditioning A version 14 191 standard algorithm inference cliquetree propagation Bayesian networks Conditioning hand real expert systems The purpose paper offer efficient version conditioning practical applications The key idea approach consists conditioning specific conditioning inside loop It called local node indicated list variables exclusively suitable remainder introduction follows organized 15211 algorithm singlyconnected The paper Kim Pearls conditioning methods Section 2 explains build associated links assigning derives technical details algorithm known methods finally variables evidence propagation Section 4 Section 5 compares conclusion offers suggestions list conditioning straightforward implementation discussed formulas node Section 3 try clearly explain The possible versions local conditioning future research summarizes tree removing networks reviews I 1 Algorithm polytree The goal algorithm probability proposition tind posteriori probability The value variable X x given observed evidence e network arbitrary node X divides Pxle In polytree singlyconnected connected link XY divides e evidence causes es connected evidence Similarly eir This partition evidence propagated network Fig I justifies following definition effects e link e messages EJ DiezArtQicial Intelligence 87 1996 I20 7rx 3 Pxexf Ax E Pelx rXui E PeX ACX PeIX 3 2 3 4 5 These definitions basically taken 221 Eqs 2 4 introduced Peot Shachter 23 simplify computation follow modification conditioning algorithms For singlyconnected network dseparation results subsidiary properties 221 l Two children x Yj node X independent given value parent PYilX PYilXfYj l A parent Ui child Yj node X independent given value X PUilX PUilXYj Nevertheless parents node X polytree priori independent general correlated instantiation X PUijX Z PUilXUj These independence properties lead recursive expressions computing mes sages Pxle arrxhx ll lb il jl qx 4x rpvXL kj AYX Yl AYj uup 3 I 1 PyjlXolurqLt c 6 7 8 9 10 VI V causes q X cr Pe finding constant computed Ax rx normalization 12 An overview conditioning methods Because dseparation precisely loopmore instantiation node node loop neighbors 4 b Fig 2 A network Its associated tree sense correlations disappear loop polytree Therefore loop parentsbreaks propagated tioning method break loops resulting cutsets C AD CDFEC requires structural process network tree A cutset network BE F finding curser set nodes numerical process propagates Fig 2a AE IS valid break loop evidence condi evidence possible RJ DiezArtijicial Intelligence 87 1996 I20 5 called global conditioning Pearls algorithm conditions 22 pp 2042101 node network cause initializing ery evidence node nodes GC exponential findings network variable weights computes 281 recursively network Consequently cutset size proportional GC cutset After ev transmitted complexity number Peot Shachter 23 introduced important improvements original al Unfortunately unconnected offer algorithm gorithm They define knot portion network removing edge knots graph Since knot cutset conditioning corresponding networks consisting knot improvement respect Section 1 l allows ings having process worstcase multiplied method called knot KC For example Fig 2a consists knots A B C D E F G set For efficiency definition VX x ui influence different evidence node Therefore cutset size algorithm network GC The second small change bounded maximum number knots cutset algorithm edges H finding complexity fusion knots applies conditioning tree list conditioning specific conditioning Local conditioning LC algorithm instead considering local means term displays associated nodes conditioned depend x Local conditioning Shachter exponential efficient includes path introduced paper goes step exclusively loop node Fig 2b node Observe cutset node X phantom node X flowing path Peot KC improvements exist structures introduced variables complexity LC requires linear time Section 51 variable 7r Amessage 2 Associated tree The process building finding cutsets list conditioning assigning algorithms Section 4 It consists depthfirst detect loops tasks network associated tree encompasses tasks finding cutset node Before reviewing previous new algorithm performs time integrated evidence propagation section introduces variables search undirected graph means 21 DFS depthrst search algorithm The following example shows transform Bayesian network associated tree Fig 2b The search begins arbitrary pivot nodeA Fig 2a We assume network connected case realworld models Otherwise procedure examine connected separately 6 EJ DiezArtijiciuI lntellrgence 87 1996 I20 travels ignoring network list called PATH A possible direction edges marking exampleand route goes A nodes including B G F H D C Backtracking node H untraversed edges removes node PATH list contains path pivot node node C node currently value ABGFDC forward C A fact marked denotes new presence loop As result phantom node A arises loop Every node A edge AC replaces A node PATH A end list adds A list conditioning moment examining investigated For instance Fig 2b breaking When going Fig Zb original link AC variables The search continues E F marked A new loop appeared loop A way node F reached sure chosen node break loop condition variable loop value PATH removed edge E case When detecting PATH F end list A B G E D C E Again node pivot add E list conditioning node search algorithm backtracks variables Then second Clearly associated tree obtained method depends choice pivot rule determine neighbor visited order achieve efficient neighbors node visited Selecting loops possible heuristic node breaks node order pivot difficult associated tree 22 Other algorithms forjnding cutsets The problem finding minimal cutset NPhard 261 However complexity state space product minimize global conditioning product number values variables attempting algorithm simply seeks cutset variables The heuristic algorithm Al proposed Suermondt depend directly size cutset proportional cutset Hence algorithm iteratively obtain small cutset tie number values candidate nodes examines For network n nodes algorithm visits node 3n times cutset obtained node neighbors cludes cutset results include unnecessary nodes belonging loop principle accurate worstcase complexity On Unfortunately Cooper trying 26 25 designed A2 modified version AI usually certain network n nodes nodes Al A2 yield cutsets size nJa pivot node chosen Furthermore Incidentally DFS finds cutset nodes DFS visits edge implies saving time A A2 sparse networks Other heuristic complexity algorithms For reason Stillman result finds smaller cutset He showed optimal cutset disastrous network depending Oe respect 1131 EJ DiezArtlcial Intelligence 87 1996 l20 7 Recently BarYehuda vertex feedback set problem During et al 2 offered new approach cutset problem reduction variable Xi taking Xi values gives way node undirected graph log Xi The vertex feedback set undirected graph coincides reducing Bayesian network weight cutset Bayesian network pegormance ratio vertex feedback total weight total weight complexity global ratio The performance Bayesian network For graph set cutset quotient optimal solution Hence product state size consequently grow exponentially cutset performance conditioning ratio algorithm worstcase performance nodes From example clear performance better The algorithms 2d2 general networks Subsequently Becker Geiger algorithm time complexity Oe nlogn theoretical Empirically Although average performance algorithms performance impossible ratio 122 excellent finding cutsets virtually R inj suggesting reasons 2 achieve performance ratio networks n ratio Al A ratios 4log n 3 designed 2 ratio equal result improve result Xt X real node maintaining loop X0X1 order obtain associated X0 breaking neighbors In conditioning list Alternatively Section 51 The extended algorithms resulting possible limit extend tree suitable local conditioning First node XX0 split phantom node connected links I loops originates 1 phantom nodes assign X0 link XiXii possible tree node Xz X0 add variable polynomial time complexity Fig 2 node breaking shown Nevertheless number variables local conditioning relevant local conditioning efficient desirable instead adapting problems conjecture arising NEhard design algorithms algorithms leading cutset product state size Even case cutsets Cl C C2 C2 Ct Therefore efficient computations developed global conditioning local conditioning From similarity graph optimal associated theory Bayesian networks tree local conditioning 3 Propagation evidence Local conditioning propagates algorithms As example network network In example e efc U e e e U e definitions es e e er loop ABDCA evidence associated let Fig 3 represent Since introduced tree generated portion tree fact singlyconnected Section 11 valid eiB e U e2 U e 31 Propagation phantom node The computation ra A The value Aa straightforward according definition assumed loop 8 EJ DiezArtijicid Intelligence 87 I 996 I20 Fig 3 A portion associated tree Aa PeJu PelaPeJu AaABu 11 In associated tree instantiation A breaks loop AB Pelu C Pe e bulu CPeebhi6iluPbeiuPeuoPu h1 Because dseparation factors inside summatory simplified Asa CPeBlhUPblcaPejuPu Let Abja eu A f Pejba QU s Pue PeluYc Then expression ABu equal Asa c Ablu Pblrluh h I CL 1 Hence need A blu The definition AD bu Abla feDlba leads Abla AW 12 13 14 15 16 EJ DiezArcial Inrelligence 87 1996 l20 The computation AD bla similar c PeIdPdlbyc fPeDlcuPcluPeDlfPf df PdlbCfDclUDf 1 2 rDClU z PCegDlU The value message DClUPe3lCPecClU J444 7clu Pcecfu c Pec wlu Pclwubrw W 17 18 19 20 21 The removal link AC original network means necessary pass message AC evidence CDBA associated node Thus messages flow network e3 propagated A tree exists exactly path piece evidence 32 Propagation phantom node Now messages flow opposite direction transmitting A probability Note messages sense computed effect independently A node compute collecting evidence previous section orthogonal For node B message Ablu given Q 16 So need rbu E Pbeiu 22 2 We define vDcla normalized depend disturb Shachters definitions TX PxUi c computation instead Pearls normalization Y cc rrDclal P ale This reason Peot constant Section 11 10 ffJ IXeArrijicid Intellipxce X7 1996 l20 order compute JDleCPbae cuCPhLle 1 As 7rfIu A a97a In way 77da c Pdlb fnOcrntrlLziTnf 1X f rrn6a s Pbuei ADTOU Clearly The computation Pcle requires A c u 23 24 25 26 27 28 29 30 Pcle nCcu7r L Message partition evidence hc happens harder define compute With suitable PCcle lPelPepecaPceuPue comparing expressions definition hc u turns hcu E PeceslcuPueB 31 PeceIcuPaec Note intuitive definition Peulc influence e C instantiation decomposed evidence link AB C modify path ABDC Message Aocu propagate A c P e uic correct value way A For reason ec e n eB probability sets evidence C A ee EJ DiezArtcial Intelligence 87 1996 I20 11 Then value A c Aca AeCADGU 32 33 c Pee efDedbflcuPue dfb CPeJdPcfP eFfPeDeluPuee dfb 34 33 Discussion The lists conditioning presence A LCVB messages conditioning variable variables LCV LCVC send receive LCV F B dependency LCV D prevents control dimension nodes In contrast message AD messages The summing bears extra eliminated summing Ad CPdlbcfDbuDclu bc 1 35 The equations indicate node conditioned real node A roughly phantom node A conditioned fusion message 7rn 6 proportional Pu speaking proportional A receives message message To clu Pu 35 Eqs 26 originated message instance conditioned In implementation algorithm node need care message implementations u conditioned Am proportional different formally equivalent Eq 10 polytree corresponding functions loop generally arrays instead vectors As consequence message handlers objectoriented programming Arx z AyxIz Eqs 14 difference It necessary 17 34 messages required rX AX MY Nevertheless AYW distinction proportional Pu conditioned u necessary methods describing mathematical point view Other presentations correct algorithms fail define conditioning r Amessages 3 Notation TX represent vector TX array xalh 12 properly Pble EJ DiezArtijicial Intelligence 87 1996 I20 In case example 231 correct formula instead expressing Eq 30 accurately read Pcle ru7TcaAcJN In turn o contains incorrect expressions Pse C7rXlUhXU 4 Two versions local conditioning Local conditioning structural described example possible phases time finds associated Fig 2 demonstrate extend tree exact algorithm consists processes numerical Sections 2 3 respectively We return independent instead having DFS algorithm propagates evidence Starting pivot node A Section 2 I crossing edges requests AB A A B ro F AHF immediately successively order integrated algorithm returns vector AH f71 D DC cA Node A marked visited Therefore edge AC removed consequent update conditioning In implementation crutch deriving lists nodes message vC create phantom nodes turns nclu necessary The following compute AEC visited node node new loop adds E list conditioning variables message AF E removal edge EF Subsequently messages AEc e pivot node4 As G nDceci goes belong 7roflu loop broken E G sum e propagation AF E Since F propagate backwards fela Asa rdela Aobla formulas step This pass generates associated tree During edge tree reverse direction information tively equivalent algorithms collect evidence distribute evidence compute necessary probability These second pass message passes way node receives passes respec Bayesian networks The main disadvantage depthfirst search hardly room applying heuristics evaluation Bayesian network use integrated version integrated version LC finds cutset instead Section 22 However network change efficient algorithms discussed Section 52it compensate time finding efficient specific query instead spending requiredbecause generated tree 4 Note e 1 play different roles equations evidence traveling real node E phantom node A 13 EJ DiezArtificial 87 I20 A C 1996 Intelligence E 22zc D B F Fig 4 The square ladder contains N squares 2N 2 nodes On hand distributed implementation local conditioning associated tree evidence propagation pre begin This allows parallel node possible conditioning distributing implementation polytrees local conditioning 921 The combina discussed evidence propagation viously generate end node simultaneously processor tion clustering 13241 5 Comparison methods 51 Conditioning algorithms KC 231 knot conditioning GC Peot Shachters Section 12 overview conditioning methods Pearls 221 global condi KC local conditioning knot For knot consisting adjacent accumulate In way LC efficient KC cutset LC conditions cutset loops LC different tioning LC For given network KC efficient GC general efficient given knot corresponding variables outside efficient loops form knot square In KC network 2N 2 number nodes consequence node belongs n In contrast given ladder LC achieves number squares ladder The difference holds diamond networks loops Recently Darwiche complexity grows exponentially independently loops structure loops cutset size On n linear complexity Fig A 1 Appendix Fig 4 single knot containing N adjacent new method based relevant ladder conditioned formed adjacent 6 proposed conditioning For instance local Ax nr X rx algorithms proposes finding relevant cutsets algorithm ruin performance conditioning results cutsets Rs R Rf XY Riy relevant cutsets Ay X respectively Unfortunately produce ineffective An example 6 applies cutset G actually necessary For binary variables message optimal cutset computation TX ui 25 times Local cutsets partial possible containing amounts indicates retrieve cache message rus TX ui suboptimal local cutset results variables requesting problem computing remedy I1 computing request W DiezArijicid lnrellipm X7 I 996 I20 r u times instead retrieving value obtained finding associated The problem tightest cutsets solved considering certain global cutset 22 showed assign list conditioning tree phantom nodes instead absorbing arcs Section variables link XY But LCVX fCV X iJ X message arriving node Y summed variables LCV Y U Y Therefore LC implicitly tightest relevant cutsets included Rsy R searched Darwiche node message originating list conditioning variables LCVXY local conditioning X conditioned suffices Rjzy R LCVXY LCVX c X n LCVY U Y 36 Clearly With distinction regard nx conditioned conditioning r leads to5 affecting Ri R necessary messages involved indicates Rz E q 7 ULCV Lix hand rx portion network X A similar computation needs reasoning In contrast LCVX ULCVUX UULCViXY 1 X 37 38 Since Ry C LCVXit cutset reduce In Fig 3 instance LCV D A R 8 Eqs 26 29 replaced differentiate variables number conditioning Rf Rthis nX AX necessary rd c PQJ rrh1 f Bohr Prlle cJlbnocjaTgf iw 40 require fewer computations Therefore worthy refinement local conditioning based Darwiches work consists R LCVXY rithms variables Section 2 Rx given Eq 37 instead list conditioning node assigned algo In summary performance explained _ main problem algorithm When 6 suboptimal cutsets drastically degraded local relevant cutsets information tightest essentially computes 5 Variable X belong Xi Rx relevant cutsets meant track extra conditioning rrX hX EJ DiezArtijcial Intelligence 87 1996 l20 15 conditioning The difference variables Darwiches instantiations relevant cutset relevant cutsets playing instead propagating messages algorithm requests message role lists conditioning conditioned variables times different 52 Clustering methods The widely algorithm multiplyconnected networks 14191 The structural phase called compilation cliquetree prop network agation consists triangulating crucial step determines NPhard optimal clique Shachter Andersen Szolovits tree 291 241 demonstrated graph forming tree cliques The triangulation efficiency numerical phase Unfortunately specific cluster probability computing triangulation phantom alent particular formed removing Ci containing Xi U paXi U LCV Xi An empirical rithms search Section 2 produce cluster 16181 simulated annealing In variable Xi parent variables equivalent local conditioning global conditioning equiv tree way equivalent tree node Xi cluster variables Ci algo trees efficient obtained heuristic 17291 determine conditioning comparison cluster nodes replacing In contrast triangulations equivalent use efficient restricted choice If network associated tree Therefore triangulation given efficient cluster tree usually pays propagating main advantage clustering local conditioning time spent evidence building Nevertheless integrated version local conditioning ity directly original network save time tions computes following probabil situa l When knowledge engineer debugging model wants measure impact modifications l When sparce evidence patient doctors usually provide In medicine example tests expert findings tests Thus pruning unobserved performed small potential nodes reduces size network eliminate l When network generated dynamically For instance probability ANDOR retrieval applications 111 information 11 loops Bayesian network P x2 V x3 A X6 22 pp 223ff Nat gates temporal lo planning usually generate specific interacting fields answer queries What introducing new nodes ural language processing reasoning network In addition ization directly conditioning case polytree explanation situation local conditioning expressions Instead clustering reasoning reducing able treat ANDOR gates general Q r A 79 change applying algorithms add dummy nodes 121 hindering efficiency evidence propagation 16 EJ DiezArtficiul Intelligence 87 1996 I20 6 Implementation DIAVAL 81 prototype expert echocardiography network tains 200 nodes loops The computation evidence integrated This version local conditioning satisfactory Com mon Lisp concern reduced algorithm neglects C time significantly leaf nodes observed evidence usually eliminates expected takes 5 seconds 48666Mz current version implemented achieve higher performance efficiency The computational number loops A future result considering implementation considerable 7 Conclusions future work We introduced local conditioning networks discussing algorithm realworld applications advantages new exact algorithm disadvantages inference versions assessment requires empirical comparison use prototype expert Bayesian Although suitability methods I The integrated version local conditioning builds tree propagates evidence Section 4 2 Local conditioning different heuristic algorithms building associated trees f 3 Clustering With regard conjecture future research Section 22 design new algorithms associated second method NPhard extend As mentioned algorithms different triangulation techniques optimal associated algorithms developed 16 18291 prove refute tree If confirmed global conditioning triangulation network opposite equivalent certain triangulation techniques local conditioning finding nearoptimal triangulations based heuristic algorithms second method local conditioning suitable tree equivalent associated specific cluster true Therefore tree optimal algorithms method outperform tree clustering question local conditioning evidence based depthfirst search method original network previous pro coincides Kim In presence tree On hand virtue propagating In singlyconnected cessing Pearls polytree algorithm loops efficient cluster probability Two determinant factors number length loops great available evidence number loops findings The use ANDOR gates model additional portion Bayesian network 15211 time spent searching tree compensate performing efficient associated numerical pruning network eliminate efficient solution use local conditioning computation reason EJ DiezArtificial Intelligence 87 1996 I20 17 Acknowledgements California Los Angeles CA research A version paper written Department Computer Science University facilities provided Pro fessor Pearl appeared Technical Report R18 1 Cognitive Systems Laboratory July 1992 The work directed Professor Jose Mira thesis advisor UCLA Investi supported grant gador Spanish Ministry Education thank Judea Pearl Mark Peot Ross Shachter MoisCs Goldzsmidt David Heckerman anonymous Personal Science The author wishes helpful discussions Plan National Formation referee Appendix A The complexity exact algorithms 51 algorithm Nevertheless topology diamond ladder A possible way Since probabilistic inference NPhard multiplyconnected expected polynomial time complexity particular network complexity Bayesian networks 4 general case exact algorithms depends number nodes The study particular structure Fig Al illustrate form associated point order Do BI Dl Cl B2 For ith diamond 1 N DFS produce network Q Ci conditioned accumulate additions multiplications binary variables Therefore computation number nodes n 3Nl P Doe Table A1 displays Di_1 A Di tree link Di1 Ci remove tree choosing DO pivot node covering ro Bi case 123 diamond The number mathematical required number operations simply vector conditioning If ladder N diamonds expression operations takes operations divisions 123N5 41n36 For example clustering methods order complexity Suermondt network cliquetree total number operations compilation 27 calculated needs 330N 111 110n 119 operations likely overestimation 241 needs 84N 5 28n 33 operations The clustering evidence propagation algorithm proposed deduced examining Cooper propagation structure Mijmessages In order complexity highlight exact algorithms depends addition link DO DN The number topology network consider nodes remains number edges varies 4N 4N 1 complexity LC double node Bi Di bear extra augmented graph lead presence conditioning entire network DO 241 DO clique different exact algorithm It plausible addition new arc raises produce significantly algorithm employed intrinsic better result Apparently independently DO A triangulation network different conditioning equivalent triangulation complexity In contrast addition new link barely increases complexity approxi mate method increase link DoDl link DoDN EJ DiezArtciul Intelligence 87 1996 l20 Fig A I Diamond ladder structure Table A1 Number mathematical operations required From D_ D_ I operations required rcil4I 1 D4 1 An hlcl I AR Ii I 0 6 IO 6 From D_l D_ operations required a4l r Cd 1 ADCidiI Probability Pbile P 4 I e Pcile Total 4 46 28 operations required 9 5 9 123 IV DiezArttjcial Intelligence 87 1996 I20 19 References 1 I M Baker TE Boult Pruning Bayesian networks PP Bonissone M Hemion LN Kanal JF Lemmer eds Uncertainty Artificial Intelligence 6 Elsevier Science Publishers Amsterdam efficient computation 1991 225232 2 1 R BarYehuda D Geiger J Naor R Roth Approximation satisfaction Bayesian alaorithms inference vertex feedback set Proceedings 5th Annual constraint problem applications ACMSIAM Symposium On Discrete Algorithms Arlington VA 1994 344354 A Becker D Geiger Approximation loop cutset problem Conference Uncertainty Artijicial Intelligence Seattle WA Morgan Kaufmann San Francisco CA 1994 6068 Proceedings Ih algorithms 13 complexity probabilistic inference Bayesian belief networks 14 GF Cooper The computational Artq Intell 42 1990 393405 5 1 F Dagum M Luby Approximating probabilistic inference Bayesian belief networks NPhard Arti Infell 60 1993 141153 161 A Darwiche Conditioning algorithms exact approximate inference causal Proceedings 11th Conference Uncertainty Artificial Intelligence Montreal Que Kaufmann San Francisco CA 1995 99107 networks Morgan 17 1 FJ Diez Parameter adjustement Proceedings 9th Conference Uncertainty Articial Intelligence Washington DC Morgan Kaufmann San Mateo CA 1993 99105 Bayes networks The generalized noisy ORgate para ecocardiografia PhD thesis Departamento lnformatica y Bayesian networks Cybern Sysr 25 1994 3961 information retrieval Commun ACM 38 181 FJ Diez Sistema expert0 Bayesiano 1994 Automatica UNED Madrid 19 I EJ Diez J Mira Distributed I01 R Fung B del Favero Applying Bayesian networks inference 1995 4248 57 Ill II2 I RP Goldman E Chamiak Probabilistic I D Heckerman Causal independence text understanding Statist Comput 2 1992 105I 14 Proceedings 9th Conference Uncertainty Arfiial Intelligence Washington DC Morgan Kaufmann San Mateo CA 1993 122127 knowledge acquisition inference 113 1 M Hede R Stigaard Distributed Huginan approach networks Masters 1994 thesis Department Mathematics distributed large Bayesian inference Computer Science Aalborg University 141 FV Jensen KG Olesen SK Andetsen An algebra Bayesian belief universes knowledge based systems Networks 20 1990 637660 I 151 JH Kim J Pearl A computational model combined causal diagnostic reasoning inference systems Proceedings IJCAI83 Karlsruhe 1983 190193 I I61 U Kjsrulff Graph University Aalbotg triangulationalgorithms giving small total state space Tech Rept R9009 1990 I7 1 U Kjaerulff Optimal decomposition probabilistic networks simulated annealing Sfatist Compur 2 1992 717 I8 1 A Kong Efficient methods computing linkage likelihoods recessive diseases inbred pedigrees Genetic Epidem 8 1991 81103 I91 SL Lauritzen DJ Spiegelhalter Local computations probabilities graphical structures application expert systems J Roy Statist Sot Ser B 50 1988 157224 201 RE Neapolitan Probabilistic Reasoning Expert Systems Theory Algorifhms Wileylnterscience New York 1990 21 I J Pearl Fusion propagation 22 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference belief networks A Intell 29 1986 241288 structuring Morgan Kaufmann San Mateo CA 1988 revised 2nd printing 1991 23 I MA Peot RD Shachter Fusion propagation multiple observations belief networks Art Intell 48 1991 299318 20 FIJ LiezArtwl Intellipwcr X7 1996 l20 24 1 KD Shachter SK Andersen P Szolovits Global conditioning Artijiciul Proceedings 10th Conference OH Lincertuirrty networks probabilistic inference belief Intelligence Seattle WA Morgan Kaufmann San Francisco CA 1994 Sl4522 125 I J Stillman On heuristics finding loop cutsets multiply connected belief networks PP Bonissone M Henrion LN Kanal JF Lemmer eds Uncertclinty Artificial fntelfigence 6 Elsevier Science Publishers Amsterdam 1991 233243 26 I HJ Suennondt GF Cooper Probabilistic inference multiply connected belief networks loop cutsets fnternat J Approx Reawzing 4 1990 283306 I27 I HJ Suermondt GF Cooper A combination exact algorithms inference Bayesian belief Internat networks 128 I HJ Suermondt networks Artif J Approx Reusoning 5 I99 I 521542 Initialization GF Cooper Intell 50 1991 8394 method conditioning Bayesian belief I29 1 WX Wen Optimal decomposition belief networks PP Bonissone M Henrion LN Kanal JF Lemmer eds Uncertcrinty UI Artificial I99 1 209224 fntelliqence 6 Elsevier Science Publishers Amsterdam