Artiﬁcial Intelligence 250 2017 105124 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Latent tree models hierarchical topic detection Peixian Chen Nevin L Zhang Zhourong Chen Farhan Khawar Department Computer Science Engineering The Hong Kong University Science Technology Hong Kong b Ant Financial Services Group Shanghai China c Department Mathematics Information Technology The Education University Hong Kong Hong Kong Tengfei Liu b Leonard KM Poon c r t c l e n f o b s t r c t Article history Received 2 May 2016 Received revised form 18 June 2017 Accepted 26 June 2017 Available online 29 June 2017 Keywords Probabilistic graphical models Text analysis Hierarchical latent tree analysis Hierarchical topic detection We present novel method hierarchical topic detection topics obtained clustering documents multiple ways Speciﬁcally model document collections class graphical models called hierarchical latent tree models HLTMs The variables level HLTM observed binary variables represent presenceabsence words document The variables levels binary latent variables represent word cooccurrence patterns cooccurrences patterns Each latent variable gives soft partition documents document clusters partitions interpreted topics Latent variables high levels hierarchy capture longrange word cooccurrence patterns thematically general topics low levels hierarchy capture shortrange word cooccurrence patterns thematically speciﬁc topics In comparison LDAbased methods key advantage new method represents cooccurrence patterns explicitly model structures Extensive empirical results new method signiﬁcantly outperforms LDAbased methods term model quality meaningfulness topics topic hierarchies 2017 Elsevier BV All rights reserved 1 Introduction The objective hierarchical topic detection HTD given corpus documents obtain tree topics general topics high levels tree speciﬁc topics low levels tree It wide range potential applications For example topic hierarchy posts online forum provide overview variety posts guide readers quickly posts A topic hierarchy reviews feedbacks businessproduct help company gauge customer sentiments identify areas improvements A topic hierarchy recent papers published conference journal readers global picture recent trends ﬁeld A topic hierarchy articles retrieved PubMed area medical research help researchers overview past studies area In applications mentioned problem search user know search Rather problem summarization thematic contents topicguided browsing Corresponding author Email address lzhangcseusthk NL Zhang httpdxdoiorg101016jartint201706004 00043702 2017 Elsevier BV All rights reserved 106 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Several HTD methods proposed previously including nested Chinese restaurant process nCRP 12 hierarchical Pachinko allocation model hPAM 34 nested hierarchical Dirichlet process nHDP 5 Those methods extensions latent Dirichlet allocation LDA 6 Hence refer collectively LDAbased methods In paper present novel HTD method called hierarchical latent tree analysis HLTA Like LDAbased methods HLTA probabilistic method involves latent variables However fundamental differences The ﬁrst differ ence lies types variables models In LDAbased methods observed variables token variables usually denoted W dn latent variables constructs hypothetical document generation process including list topics usually denoted β topic distribution vector document usually denoted θd topic signment token document usually denoted Zdn In contrast observed variable HLTA stands word It binary variable represents presenceabsence word document The latent variables HLTA considered unobserved attributes documents If compare words occur particular documents students subjects latent variables correspond latent traits analytical skill literacy skill general intelligence In LDAbased methods token variable stands location document possible values words vocabulary Here talk conditional independence words probabilities words sum 1 On hand output HLTA treestructured graphical model word variables leaves latent variables internal nodes Two word variables conditionally independent given latent variable path Words frequently cooccur documents tend located region tree This fact conducive discovery meaningful topics topic hierarchies A drawback binary word variables word counts discarded The second difference lies deﬁnition characterization topics Topics LDAbased methods prob abilistic distributions vocabulary When presented users topic characterized words highest probabilities In contrast topics HLTA clusters documents More speciﬁcally latent variables HTLA assumed binary Just concept analytical skill partitions student population soft clusters cluster consisting people high analytic skill consisting people low analytic skill latent variable HLTA partitions document collection soft clusters documents The document clusters interpreted topics For presentation users topic characterized words occur high probabilities topic occur low probabilities outside topic The consideration occurrence probabilities outside topic important word occurs high probability topic occur high probability outside topic When happens good choice characterization topic HLTA differs LDAbased methods ways Those differences technical nature explained Section 4 The rest paper organized follows We discuss related work Section 2 review basics latent tree models Section 3 In Section 4 introduce hierarchical latent tree models HLTMs explain hierarchical topic detection The HLTA algorithm learning HLTMs described Sections 57 In Section 8 present results HTLA obtains realworld dataset discuss practical issues In Section 9 empirically compare HLTA LDAbased methods Finally end paper Section 10 concluding remarks discussions future work 2 Related work Topic detection active research areas Machine Learning past decade The commonly method latent Dirichlet allocation LDA 6 LDA assumes documents generated follows First list β1 βK topics drawn Dirichlet distribution Then document d topic distribution θd drawn Dirichlet distribution Each word W dn document generated ﬁrst picking topic Zdn according topic distribution θd selecting word according word distribution β Zdn topic Given document collection generation process reverted statistical inference sampling variational inference determine topics topic compositions documents LDA extended ways additional modeling capabilities Topic correlations considered 7 3 topic evolution modeled 810 topic structures built 11314 information exploited 1213 supervised topic models proposed 1415 In following discuss details extensions closely related paper The hierarchical Pachinko allocation model hPAM 34 proposed method modeling correlations topics It introduces multiple levels supertopics basic topics Each supertopic distribution topics level Hence hPAM viewed HTD method hierarchical structure needs predetermined To pick topic token ﬁrst draws toplevel topic multinomial distribution turn drawn Dirichlet distribution draws topic level multinomial distribution associated toplevel topic The rest generation process LDA The nested Chinese Restaurant Process nCRP 2 nested Hierarchical Dirichlet Process nHDP 5 proposed HTD methods They assume true topic tree data A prior distribution placed possible trees nCRP nHDP respectively An assumption documents generated true topic P Chen et al Artiﬁcial Intelligence 250 2017 105124 107 Fig 1 The undirected latent tree model c represents equivalent class directed latent tree models includes b members tree data gives likelihood function possible trees In nCRP topics document assumed path tree nHDP topics document multiple paths subtree entire topic tree The true topic tree estimated combining prior likelihood posterior inference During inference theory deals tree inﬁnitely levels node having inﬁnitely children In practice tree truncated predetermined number levels In nHDP node predetermined number children nCRP uses hyperparameter control number As methods effect require user provide structure hierarchy input As mentioned introduction HLTA models document collections hierarchical latent tree models HLTMs latent variables models regarded unobserved attributes documents The concept latent tree models introduced 1617 referred hierarchical latent class models The term latent tree models ﬁrst appeared 1819 Latent tree models LTMs generalize classes models previous literature The ﬁrst class latent class models 2021 categorical data clustering social sciences medicine The second class probabilistic phylogenetic trees 22 tool determining evolution history set species The reader referred 23 survey previous works latent tree models The works conducted settings In ﬁrst setting data assumed generated unknown LTM task recover generative model 24 Here tries discover relationships latent structure observed marginals hold LTMs use relationships reconstruct true latent structure data One prove theoretical results consistency sample complexity In second setting assumption data generated task ﬁt LTM data 25 In setting sense talk theoretical guarantees consistency sample complexity Instead algorithms evaluated empirically heldout likelihood It shown realworld datasets better models obtained methods developed setting developed ﬁrst setting 26 The reason assumption ﬁrst setting reasonable data domains phylogeny reasonable types data text data survey data The setting similar second setting model ﬁt longer concern In addition needs consider useful resulting model users want example obtain hierarchy latent variables Liu et al 27 ﬁrst use latent tree models hierarchical topic detection They propose algorithm HLTA learning HLTMs text data method extracting topic hierarchies models A method scaling algorithm proposed Chen et al 28 This paper based 2728 There substantial extensions The novelty HLTA wrt LDAbased methods systematically discussed theory algorithm described details practical issues discussed new parameter estimation method large datasets empirical evaluations extensive Another method learn hierarchy latent variables data proposed Ver Steeg Galstyan 29 The method named correlation explanation CorEx Unlike HLTA CorEx modelfree method intend provide representation joint distribution observed variables HLTA produces hierarchy word variables multiple levels latent variables It lated hierarchical variable clustering However fundamental differences One difference HLTA deﬁnes distribution documents variable clustering In addition HLTA partitions document collections multiple ways There vast literature document clustering 30 In particular coclustering 31 identify document clusters cluster associated potentially different set words However document clustering topic detection generally considered different ﬁelds little overlap This paper bridges ﬁelds developing fullﬂedged HTD method partitions documents multiple ways 3 Latent tree models A latent tree model LTM treestructured Bayesian network 32 leaf nodes represent observed variables internal nodes represent latent variables An example shown Fig 1a In paper variables assumed binary The model parameters include marginal distribution root Z 1 conditional distribution nodes given parent The product distributions deﬁnes joint distribution variables 108 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Fig 2 Hierarchical latent tree model obtained toy text dataset The latent variables right word variables represent word cooccurrence patterns higher levels represent cooccurrence patterns level In general LTM n observed variables X X1 Xn m latent variables Z Z 1 Zm Denote parent variable Y paY let paY set Y root Then LTM deﬁnes joint distribution observed latent variables follows cid2 P X1 Xn Z 1 Zm P Y paY 1 Y XZ By changing root Z 1 Z 2 Fig 1a model shown b The models equivalent sense represent set distributions observed variables X1 X5 17 It possible distinguish equivalent models based data This implies root LTM orientations edges unidentiﬁable It makes sense talk undirected LTMs paper One example shown Fig 1c It represents equivalent class directed models A member class obtained picking latent node root directing edges away root For example b obtained c choosing Z 1 Z 2 root respectively In implementation undirected model represented arbitrary directed model equivalence class represents In literature variations LTMs internal nodes observed 24 andor variables continuous 3335 In paper focus basic LTMs deﬁned previous paragraphs We use Z denote number possible states variable Z An LTM regular latent node Z Z cid3 k i1 maxk i1 Z Z 2 Z 1 Zk neighbors Z inequality holds strictly k 2 17 When variables binary condition reduces latent node neighbors For irregular LTM regular model fewer parameters represents set distributions observed variables 17 Consequently focus regular models 4 Hierarchical latent tree models topic detection We later present algorithm called HLTA learning text data models shown Fig 2 There layer observed variables multiple layers latent variables The model called hierarchical latent tree model HLTM In section discuss interpret HLTMs extract topics topic hierarchies 41 HLTMs text data We use toy model Fig 2 running example It learned subset 20 Newsgroups data1 The vari ables level level 0 observed binary variables represent presenceabsence words document The latent variables level 1 introduced data analysis model word cooccurrence patterns For example Z 11 captures probabilistic cooccurrence words nasa space shuttle mission Z 12 captures probabilis tic cooccurrence words orbit earth solar satellite Z 13 captures probabilistic cooccurrence words lunar moon Latent variables level 2 introduced data analysis model cooccurrence patterns level 1 For example Z 21 represents probabilistic cooccurrence patterns Z 11 Z 12 Z 13 Because latent variables introduced layer layer latent variable introduced explain correla tions group variables level regard purpose model interpretation edges layers directed directed downwards The edges toplevel latent variables directed 1 http qwone com jason 20Newsgroups P Chen et al Artiﬁcial Intelligence 250 2017 105124 109 Table 1 Document partition given latent variable Z 21 Z 21 space nasa orbit earth shuttle moon mission s0 095 s1 005 004 003 001 001 001 002 001 058 043 033 033 024 026 021 This allows talk subtree rooted latent node For example subtree rooted Z 21 consists observed variables orbit earth mission 42 Topics HLTMs There totally 14 latent variables toy example Each latent variable states partitions document collection soft clusters To ﬁgure partition clusters need consider relationship latent variable observed variables subtree Take Z 21 example Denote document clusters gives Z 21 s0 Z 21 s1 The occurrence probabilities clusters words Z 21 subtree given Table 1 sizes clusters We cluster Z 21 s1 consists 5 documents In cluster words space nasa orbit occur relatively high probabilities It clearly meaningful interpreted topic One label topic NASA The cluster Z 21 s0 consists 95 documents In cluster words occur low probabilities We interpret background topic There subtle issues concerning Table 1 The ﬁrst issue word variables ordered To answer question need mutual information MI I X Y 36 discrete variables X Y deﬁned follows IX Y cid4 XY P X Y log P X Y P XP Y 3 In Table 1 word variables ordered according mutual information Z 21 The words placed table highest MI Z 21 They best ones characterize difference clusters occurrence probabilities clusters differ They occur high probabilities clusters Z 21 s1 low probabilities Z 21 s0 If choose 5 words characterize topic Z 21 s1 best words pick space nasa orbit earth shuttle The second issue background topic determined The answer document clusters given Z 21 words occur lower probabilities regarded background topic In general consider sum probabilities 3 words The cluster sum lower designated background topic labeled s0 considered genuine topic labeled s1 Finally creation Table 1 requires joint distribution Z 21 words variable subtrees P space Z 21 The distributions computed Belief Propagation 32 The computation takes linear time model treestructured 43 Topic hierarchies HLTMs If background topics ignored latent variable gives exactly topic As model Fig 2 gives 14 topics shown Table 2 Latent variables high levels hierarchy capture longrange word cooccurrence patterns thematically general topics low levels hierarchy capture shortrange word cooccurrence patterns thematically speciﬁc topics For example topic given Z 22 windows card graphics video dos consists mixture words aspects computers We topic computers The subtopics concerned aspect computers Z 14 card video driver Z 15 dos windows Z 16 graphics display image 44 More novelty In introduction discussed differences HLTA LDAbased methods regard types observed latent variables deﬁnition characterization topics There impor tant differences The difference lies relationship topics documents In LDAbased methods document mixture topics probabilities topics document sum 1 Because LDA 110 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Table 2 Topic hierarchy given model Fig 2 005 space nasa orbit earth shuttle 006 orbit earth solar satellite 005 space nasa shuttle mission 003 moon lunar 014 team games players season hockey 014 team season 011 players baseball league 009 games win won 008 hockey nhl 024 windows card graphics video dos 012 card video driver 015 windows dos 010 graphics display image 009 science models called mixedmembership models In HLTA topic soft cluster documents document belong multiple topics probability 1 In sense HLTMs said multimembership models The fourth difference HLTA LDAbased methods semantics hierarchies produce The LDAbased methods nCRP nHDP produce tree topics distribution vocabulary The topics higher levels appear lower levels necessarily related thematically Similarly hPAM yields collection topics organized directed acyclic graph The topics lowest level distributions words topics higher levels distributions topics level called supertopics On hand output HLTA tree latent variables Latent variables high levels tree capture longrange word cooccurrence patterns thematically general topics latent variables low levels tree capture shortrange word cooccurrence patterns thematically speciﬁc topics Note context document analysis common concept hierarchy rooted tree node repre sents cluster documents cluster documents node union document clusters children Neither HLTA LDAbased methods yield hierarchies Finally LDAbased methods require user provide structure hierarchy including number latent levels number nodes level The number latent levels usually set 3 eﬃciency considerations The contents nodes distributions vocabulary learned data In contrast HLTA learns model structures model parameters data The number latent levels limited 3 5 Model structure construction We present HLTA algorithm sections The inputs HLTA include collection documents algorithmic parameters The outputs include HLTM topic hierarchy extracted HLTM Topic hierarchy extraction explained Section 4 focus learn HLTM In section procedures constructing model structure In Section 6 discuss parameter estimation issues Section 7 discuss techniques employed accelerate algorithm 51 Toplevel control The toplevel control HLTA given Algorithm 1 subroutines given Algorithms 26 In subsection illustrate toplevel control toy dataset mentioned Section 4 involves 30 word variables There 5 steps The ﬁrst step line 3 yields model shown Fig 3a It said ﬂat LTM latent variable connected observed variable In hierarchical models shown Fig 2 hand latent variables lowest latent layer connected observed variables latent variables The learning ﬂat model key step HLTA We discuss details later We refer latent variables ﬂat model ﬁrst step level 1 latent variables The objective second step line 9 turn level 1 latent variables observed variables data completion To subroutine HardAssignment carries inference compute posterior distribution latent variable document The document assigned state highest posterior probability resulting dataset D1 level 1 latent variables In step line 3 executed learn ﬂat LTM level 1 latent variables resulting model shown Fig 3b In fourth step line 7 ﬂat model level 1 latent variables stacked ﬂat model observed variables resulting hierarchical model Fig 2 While subroutine StackModels cuts links level 1 latent variables The parameter values new model copied source models P Chen et al Artiﬁcial Intelligence 250 2017 105124 111 Algorithm 1 HLTAD τ μ δ κ Inputs D collection documents τ upper bound number toplevel topics μ upper bound island size δ threshold UDtest κ number EM steps ﬁnal model An HLTM topic hierarchy Outputs m1 LearnFlatModelD1 δ μ m null m m1 1 D1 D m null 2 repeat 3 4 5 6 7 8 end D1 HardAssignmentm D 9 10 number toplevel nodes m τ 11 Run EM m κ steps 12 return m topic hierarchy extracted m m StackModelsm1 m Fig 3 Illustration toplevel control HLTA A ﬂat model word variables ﬁrst learned b The latent variables converted observed variables data completion ﬂat model learned Finally ﬂat model b stacked ﬂat model c obtain hierarchical model Fig 2 In general ﬁrst steps repeated number toplevel latent variables falls userspeciﬁed upper bound τ lines 2 10 In running example set τ 5 The number nodes level current model 3 threshold τ Hence loop exited In ﬁfth step line 11 EM algorithm 37 run ﬁnal hierarchical model κ steps improve parameters κ user speciﬁed input parameter The ﬁve steps grouped phases conceptually The model construction phase consists ﬁrst steps The objective build hierarchical model structure The parameter estimation phase consists ﬁfth step The objective optimize parameters hierarchical structure ﬁrst phase 52 Learning ﬂat models The objective ﬂat model learning step ﬁnd ﬂat models highest BIC score The BIC score 38 model m given dataset D deﬁned follows BICm D log P D m θ d 2 log D 4 θ maximum likelihood estimate model parameters d number free model parameters D sample size Maximizing BIC score intuitively means ﬁnd model ﬁts data overly complex One way solve problem search The stateoftheart direction algorithm named EAST 25 It shown 26 ﬁnd better models alternative algorithms BIN 39 CLRG 24 However scale It capable handling data dozens observed variables suitable text analysis 112 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Algorithm 2 LearnFlatModelD δ μ 1 L BuildIslandsD δ μ 2 m BridgeIslandsL D1 3 return m Fig 4 The subroutine BuildIslands partitions word variables unidimensional clusters introduce latent variable cluster form island LCM In following present algorithm combined parameter estimation technique described section eﬃcient deal large text data The pseudo code given Algorithm 2 It calls subroutines The ﬁrst subroutine BuildIslands It partitions word variables clusters words cluster tend cooccur cooccurrences properly modeled single latent variable It introduces latent variable cluster model cooccurrence words inside Thus obtain LTM single latent variable word variable cluster called latent class model LCM In running example results shown Fig 4 We metaphorically refer LCMs islands The second subroutine BridgeIslands It links islands ﬁrst estimating mutual information pair latent variables ﬁnding maximum spanning tree 40 The result model Fig 3a We set subroutines details 521 Unidimensionality test Conceptually set variables said unidimensional correlations properly modeled single latent variable Operationally rely unidimensionality test UDtest determine set variables unidimensional To perform UDtest set S observed variables ﬁrst learn latent tree models m1 m2 S compare BIC scores The model m1 model highest BIC score LTMs single latent variable model m2 model highest BIC score LTMs latent variables Fig 5b shows models look like S consists word variables nasa space shuttle mission We conclude S unidimensional following inequality holds BICm2 D BICm1 D δ 5 δ userspeciﬁed threshold In words S considered unidimensional best twolatent variable model signiﬁcantly better best onelatent variable model Note UDtest related Bayes factor comparing models 41 K P Dm2 P Dm1 6 The strength evidence favor m2 depends value K The following guidelines suggested 41 If quantity 2 log K 0 2 evidence negligible If 2 6 positive evidence favor m2 If 6 10 strong evidence favor m2 And larger 10 strong evidence favor m2 Here log stands natural logarithm It known BIC score BICmD large sample approximation marginal loglikelihood log P Dm 38 Consequently difference BICm2D BICm1D large sample approximation logarithm Bayes factor log K According cutoff values Bayes factor conclude positive strong strong evidence favoring m2 difference larger 1 3 5 respectively In experiments set δ 3 522 Building islands The subroutine BuildIslands Algorithm 3 builds islands It builds ﬁrst island calling subrou tine OneIsland Algorithm 4 Then removes variables island dataset repeats process build islands It continues variables grouped islands The subroutine OneIsland Algorithm 4 requires measurement closely correlated pair variables In paper mutual information purpose The MI I X Y variables X Y given 3 We need MI variable X set variables S We estimate follows P Chen et al Artiﬁcial Intelligence 250 2017 105124 113 Algorithm 3 BuildIslandsD δ μ 1 V variables D L 2 V 0 3 4 5 6 end 7 return L m OneIslandD V δ μ L L m V variables D m L Algorithm 4 OneIslandD V δ μ 1 V 3 m LearnLCMD V return m 2 S variables V highest MI 3 X arg max AVS 4 S S X 5 V1 V S 6 D1 ProjectDataD S 7 m LearnLCMD1 S 8 loop 9 ˆI A S ˆI A S ˆI A X X arg max AV1 W arg max AS D1 ProjectDataD S X V1 V1 X m1 PemLcmm S X D1 V1 0 return m1 m2 PemLtm2lm S W W X D1 B I Cm2D1 B I Cm1D1 δ return m2 W X parent removed end S μ return m1 m m1 S S X 10 11 12 13 14 15 16 17 18 19 20 end loop Fig 5 Illustration unidimensionality test OneIsland subroutine For pairs models m1 m2 m1 picked BIC score m2 signiﬁcantly higher m1 ˆIX S max AS IX A 7 The subroutine OneIsland maintains working set S observed variables Initially S consists pair variables highest MI line 2 referred seed variables island Then variable highest MI variables added S variable lines 3 4 Then variables added S At step pick variable X highest MI current set S line 9 perform UDtest set S X lines 12 14 15 If UDtest passes X added S line 19 process continues If UDtest fails island created subroutine returns line 16 The subroutine returns size island reaches userspeciﬁed upperbound μ line 18 In experiments set μ 15 114 P Chen et al Artiﬁcial Intelligence 250 2017 105124 The UDtest requires models m1 m2 In principle best models latent variables respectively For sake computational eﬃciency construct heuristically paper For m1 choose LCM latent variable binary parameters optimized fast subroutine PemLcm described section Let W variable S highest MI variable X added island For m2 choose model latent variable connected variables S W second latent variable connected W X Both latent variables binary model parameters optimized fast subroutine PemLtm2l described section We illustrate OneIsland subroutine example Fig 5 The pair variables nasa space highest MI variables seed variables The variable shuttle highest MI pair variables chosen variable start island Fig 5a Among variables mission highest MI variables model To decide mission added group models m1 m2 Fig 5b created In m2 shuttle grouped new variable highest MI new variable variables Fig 5a It turns m1 higher BIC score m2 Hence UDtest passes variable mission added group The variable considered addition moon added group UDtest passes Fig 5c After variable lunar considered In case BIC score m2 signiﬁcantly higher m1 UDtest fails Fig 5d The subroutine OneIsland terminates It returns island model m2 contain variable lunar Fig 5e The island consists words nasa space shuttle mission Intuitively grouped tend cooccur dataset 523 Bridging islands After islands created step link obtain model word variables This carried BridgeIslands subroutine idea borrowed 42 The subroutine ﬁrst estimates MI pair latent variables islands constructs complete undirected graph MI values edge weights ﬁnally ﬁnds maximum spanning tree graph The parameters newly added edges estimated fast method described end Section 63 islands latent variables Y Y respectively The MI IY Y cid9 Y Y calculated Let m m cid9 cid9 cid9 Equation 3 following joint distribution P Y Y cid9 D m m cid9 C cid4 dD P Y m dP Y cid9 cid9 m d 8 P Y m d posterior distribution Y m given data case d P Y normalization constant cid9 m cid9 d Y cid9 cid9 m C In running example applying BridgeIslands islands Fig 4 results ﬂat model shown Fig 3a 6 Parameter estimation model construction In model construction phase large number intermediate models generated Whether HLTA scale depends parameters intermediate models ﬁnal model estimated eﬃciently In section present fast method called progressive EM estimating parameters intermediate models In section discuss estimate parameters ﬁnal model eﬃciently sample size large 61 The EM algorithm We start brieﬂy reviewing EM algorithm Let X H respectively sets observed latent variables LTM m let V X H Assume latent variable picked root edges directed away root For V V root parent paV V latent variable values 0 1 For technical convenience let paV dummy variable possible value V root Enumerate variables V 1 V 2 V n We denote parameters m θi jk P V kpaV j 1 n k value V j value paV Let θ vector parameters Given dataset D loglikelihood function θ given lθ D cid4 cid4 dD H log P d H θ The maximum likelihood estimate MLE θ value maximizes loglikelihood function 9 10 P Chen et al Artiﬁcial Intelligence 250 2017 105124 115 Fig 6 Progressive EM EM ﬁrst run submodel shaded estimate distributions P Y P AY P BY P DY EM run submodel shaded b P Y P BY P DY ﬁxed estimate distributions P Z Y P CZ P EZ Due presence latent variables closed form solution MLE An iterative method called ExpectationMaximization EM 37 algorithm usually practice EM starts initial guess θ 0 parameter values produces sequence estimates θ 1 θ 2 Given current estimate θ t estimate θ t1 obtained Estep Mstep For latent tree models steps follows The Estep n t jk cid4 dD The Mstep P V k paV jd m θ t θ t1 jk t n jkcid5 k n t jk 11 12 Note Estep requires calculation P V paV id m θ t data case d D variable V For given data case d calculate P V paV id m θ t variables V linear time message propagation 43 EM terminates improvement loglikelihood lθ t1D lθ tD falls predetermined threshold number iterations reaches predetermined limit To avoid local maxima multiple restarts usually 62 Progressive EM Being iterative algorithm EM trapped local maxima It timeconsuming scale Progressive EM proposed fast alternative EM model construction phase It estimates parameters multiple steps step considers small model runs EM submodel maximize local likelihood function The idea illustrated Fig 6 Assume Y selected root To estimate parameters model ﬁrst run EM model shaded Fig 6a estimate P Y P AY P BY P DY run EM model shaded Fig 6b P Y P BY P DY ﬁxed estimate P Z Y P C Z P E Z 63 Progressive EM HLTA We use progressive EM estimate parameters intermediate models generated HLTA speciﬁcally generated subroutine OneIsland Algorithm 4 It carried subroutines PemLcm PemLtm2l At lines 1 7 OneIsland needs estimate parameters LCM observed variables It EM Next enters loop At beginning LCM m set S variables The parameters LCM estimated earlier line 7 beginning line 12 previous pass loop At lines 9 10 OneIsland ﬁnds variable X outside S maximum MI S variable W inside S maximum MI X At line 12 OneIsland adds X m create new LCM m1 The parameters m1 estimated subroutine PemLcm Algorithm 5 application progressive EM Let explain PemLcm intermediate models shown Fig 5 Let m model shown left Fig 5c S nasa space shuttle mission moon The variable X added m lunar model m1 adding lunar m shown left Fig 5d The distribution estimated P lunarY distributions estimated PemLcm estimates distribution running EM model m1 Fig 7 left variables involved marked rectangles The variables nasa space included submodel instead observed variables seed variables picked line 2 Algorithm 4 At line 14 OneIsland adds X m create new LTM m2 latent variables The parameters m2 estimated subroutine PemLtm2l Algorithm 62 application progressive EM In running example let moon variable W highest MI lunar variables S Then model m2 2 When seed variables W use seed variable variable picked line 3 Algorithm 4 116 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Algorithm 5 PemLcmm S X D 1 Y latent variable m 2 S1 X seed variables S 3 While keeping parameters ﬁxed run EM m involves S1 Y estimate P XY 4 return m Algorithm 6 PemLtm2lm S W W X D 1 Y latent variable m 2 m2 model obtained m adding X new latent variable Z connecting Z Y connecting X Z reconnecting W connected Y Z 3 S1 W X seed variables S 4 While keeping parameters ﬁxed run EM m2 involves S1 Y Z estimate P W Z P XZ P Z Y 5 return m2 Fig 7 Parameter estimation island building To determine variable lunar added island m1 Fig 5c models created We need estimate P lunarY model left P Z Y P moonZ P lunarZ model right The estimation running EM parts models variables marked rectangles shown right hand Fig 5d The distributions estimated P Z Y P moon Z P lunar Z PemLtm2l estimates distributions running EM model m2 Fig 7 right variables involved marked rectangles The variables nasa space included submodel instead shuttle mission seed variables picked line 2 Algorithm 4 There parameter estimation problem inside subroutine BridgeIslands After linking islands parameters edges latent variables estimated We use progressive EM task Consider model Fig 3 To estimate P Z 11 Z 12 form submodel picking children Z 11 instance nasa space children Z 12 instance orbit earth Then estimate distribution P Z 11 Z 12 running EM submodel parameters ﬁxed 64 Complexity analysis Let n number observed variables N sample size HLTA requires computation empirical MI pair observed variables This takes O n2 N time When building islands observed variables HLTA generates roughly 2n intermediate models Progressive EM estimate parameters intermediate models It run submodels 3 4 observed variables The projection dataset 3 4 binary variables consists 8 16 distinct cases matter large original sample size Hence progressive EM takes constant time denote c1 submodel This key reason HLTA scale The data projection takes O N time submodel Hence total time island building O 2nN c1 To bridge islands HLTA needs estimate MI pair latent variables runs progressive EM estimate parameters edges islands A loose upper bound running time step n2 N nN c1 The total number variables observed latent resulting ﬂat model upper bounded 2n Inference model takes 2n propagation steps data case Let c2 time propagation step Then hard assignment step takes O 4nc2 N time So total time ﬁrst pass loop HLTA O 2n2 N 3nN c1 4nc2 N O 2n2 N 3nc1 4nc2 N term 3nN ignored dominated term 4nc2 N As level number observed variables decreased half Hence total time model construction phase upper bounded O 4n2 N 6nc1 8nc2 N The total number variables observed latent ﬁnal model upper bounded 2n Hence EM iteration takes O 4nc2 N time ﬁnal parameter optimization steps takes O 4nc2 Nκ times The total running time HLTA O 4n2 N 6nc1 8nc2 N O 4nc2 Nκ The terms times model construction phase parameter estimation phase respectively 7 Dealing large datasets We employ techniques accelerate HLTA handle large datasets millions documents The ﬁrst technique downsampling use reduce complexity model construction phase Speciﬁcally P Chen et al Artiﬁcial Intelligence 250 2017 105124 117 cid9 cid9 cid9 cid9 6nc1 8nc2 N randomly sampled data cases instead entire dataset reduce complexity use subset N cid9 When N large set N O 4n2 N small fraction N achieve substantial computational savings In meantime expect obtain good structure N small The reason model construction relies salient regularities data regularities preserved subset N small The second technique stepwise EM 4445 We use accelerate convergence parameter estimation process second phase task improve values parameters θ θi jk Equation 9 obtained model construction phase While standard EM aka batch EM updates parameter iteration stepwise EM updates parameters multiple times iteration Suppose data set D randomly divided equalsized minibatches D1 DB Stepwise EM updates param eters processing minibatch It maintains collection auxiliary variables ni jk initialized 0 experiments Suppose parameters updated u 1 times current values θ θi jk Let Db minibatch process Stepwise EM carries updating follows cid9 cid4 cid9 jk n P V k paV jd m θ dDb ni jk 1 ηuni jk ηun θi jk ni jkcid5 k ni jk cid9 jk 13 14 15 Note equation 13 similar 11 statistics calculated minibatch Db entire dataset D The parameter ηu known stepsize given ηu u 2α parameter α chosen range 05 α 1 46 In experiments set α 075 Stepwise EM similar stochastic gradient descent 47 updates parameters processing mini batch It shown yield estimates better quality batch EM converges faster 46 As run fewer iterations batch EM substantially reduce running time 8 Illustration results practical issues HLTA novel method hierarchical topic detection discussed introduction fundamentally different LDAbased methods We empirically compare HLTA LDAbased methods section In section present results HLTA obtains realworld dataset reader gain clear understanding offer We discuss practical issues 81 Results NYT dataset HLTA implemented Java The source code available online3 datasets paper details results obtained HLTA tested datasets One NYT dataset consists 300000 articles published New York Times 1987 20074 A vocabulary 10000 words selected average TFIDF 48 analysis The average TFIDF term t collection documents D deﬁned follows tfidft D cid5 dD tft d idft D D 16 stands cardinality set tft d term frequency t document d idft D logDd D t d inverse document frequency t document collection D A subset 10000 randomly sampled documents model construction phase Stepwise EM parameter estimation phase size minibatches set 1000 Other parameter settings given section The analysis took 420 minutes desktop machine The result HLTM 5 levels latent variables 21 latent variables level Fig 8 shows model structure Four toplevel latent variables included ﬁgure The level 4 level 2 latent variables subtrees rooted ﬁve toplevel latent variables included Each level 2 latent variable connected word variables subtrees Those word variables highest MI latent variable word variables subtree The structure interesting We words subtree rooted Z 501 economy stock market words subtree Z 502 companies industries words subtree rooted Z 503 movies music words subtree rooted Z 504 cooking 3 http wwwcse ust hk lzhang topic index htm 4 http archive ics uci edu ml datasets Bag Words 118 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Fig 8 Part hierarchical latent tree model obtained HLTA NYT dataset The nodes Z 501 Z 504 toplevel latent variables The subtrees rooted different toplevel latent variables coded different colors Edge width represents mutual information A bigger version ﬁgure available http wwwcse ust hk lzhang topic NYTimes NYTgraph pdf Table 3 A topic hierarchy obtained HLTA NYT dataset 1 020 economy stock economic market dollar 11 020 economy economic economist rising recession 12 020 currency minimum expand expansion wage 121 020 currency expand expansion expanding euro 122 023 labor union demand industries dependent 123 021 minimum wage employment retirement 13 020 stock market investor analyst investment 14 020 price prices merger shareholder acquisition 15 020 dollar billion level maintain million lower 16 020 profit revenue growth troubles share revenues 17 020 percent average shortage soaring reduce 2 022 companies company firm industry incentive 21 023 firm consulting distribution partner 22 023 companies company industry 23 014 insurance coverage insurer pay premium 24 025 store stores consumer product retailer 25 021 proposal proposed health welfare 26 020 drug pharmaceutical prescription 27 007 federal reserve fed nasdaq composite 28 009 enron internal accounting collapsed 29 009 gas drilling exploration oil natural Table 3 shows topic hierarchy extracted model shown Fig 8 The topics relationships meaningful For example topic 1 economy stock market It splits groups subtopics economy stock market Each subtopic splits subsubtopics For example subtopic 12 economy splits subtopics currency expansion labor union minimum wages The topic 2 companyﬁrmindustry Its subtopics include types companies insurance retail storesconsumer products natural gasoil drug P Chen et al Artiﬁcial Intelligence 250 2017 105124 119 Table 4 Topics detected AAAIIJCAI papers 200015 contain word network 005 neuralnetwork neural hiddenlayer layer activation 008 bayesiannetwork probabilisticinference variableelimination 004 dynamicbayesiannetwork dynamicbayesian slice timeslice 003 domingos markovlogicnetwork richardsondomingos 006 dechter constraintnetwork freuder consistencyalgorithm 009 socialnetwork twitter socialmedia social tweet 003 complexnetwork communitystructure communitydetection 001 semanticnetwork conceptnet partof 009 wireless sensornetwork remote radar radio beacon 008 traffic transportation road driver drive roadnetwork 82 Two practical issues Next discuss practical issues 821 Broadly vs narrowly deﬁned topics In HLTA latent variable introduced model pattern probabilistic word cooccurrence It gives topic soft cluster documents The size topic determined considering words pattern words vocabulary As conceptually includes types documents 1 documents contain probabilistic sense pattern 2 documents contain pattern similar Because inclusion second type documents topic said broadly deﬁned All topics reported broadly deﬁned The size broadly deﬁned topic appear unrealistically large ﬁrst glance For example topic detected NYT dataset consists words affair widely scandal viewed intern monica_lewinsky size 018 Although large actually reasonable Obviously fraction documents contain seven words topic smaller 18 However documents contain words clinton American politics Other documents contain words included topic surprising topic cover 18 corpus As matter fact topics American politics similar sizes One corruption campaign political democratic presidency In applications desirable identify narrowly deﬁned topics topics documents containing particular patterns Such topics obtained follows First pick list words characterize topic method described Section 4 form latent class model words observed variables ﬁnally use model partition documents clusters The cluster words occur relatively higher probabilities designated narrow topic The size narrowly deﬁned topic typically smaller widely deﬁned version For example sizes narrowly deﬁned version aforementioned topics 0008 0169 respectively 822 Use collocations observed variables In HLTMs observed variable connected latent variable If individual words observed variables word appear branch resulting topic hierarchy This reasonable Take word network example It appear different multiword expressions neural network Bayesian net work constraint network social network sensor network Clearly terms appear different branches good topic hierarchy Multiword expressions neural network known collocations literature In general collocation sequence consecutive words characteristics syntactic semantic unit 49 It shown 50 replacing collocations single tokens improves performance topic models We borrow idea mitigate problem mentioned Speciﬁcally use method described 51 The method ﬁrst treats individual words tokens ﬁnds n tokens highest average TFIDF Let PicktoptokensD n subroutine The method calculates TFIDF values 2grams includes n 1grams 2grams highest TFIDF values tokens After selected 2grams social network replaced single tokens socialnetwork documents subroutine Picktoptokens D n run pick new set n tokens The process repeated wants consider ngrams n 2 tokens The method applied analysis papers published AAAI IJCAI 2000 2015 Table 4 shows topics resulting topic hierarchy They contain word network different branches hierarchy 120 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Table 5 Information datasets empirical comparisons Vocabulary size Sample size NIPS1k 1000 1955 NIPS5k 5000 1955 NIPS10k 10000 1955 News1k 1000 19940 News5k 5000 19940 NYT 10000 300000 9 Empirical comparisons We present empirical results compare HLTA LDAbased methods hierarchical topic detection including nested Chinese restaurant process nCRP 2 nested hierarchical Dirichlet process nHDP 5 hierarchical Pachinko allocation model hPAM 4 Also included comparisons CorEx 29 CorEx produces hierarchy latent variables probability model variables For comparability convert results hierarchical latent tree model5 Implementations methods obtained online6 91 Datasets Three datasets experiments The ﬁrst NYT dataset mentioned The second 20 Newsgroups dataset ﬁrst mentioned Section 4 It consists 19940 newsgroup posts The NIPS dataset consists 1955 articles published NIPS conference 1988 19997 Symbols stop words words occur 3 times removed datasets Three versions NIPS dataset versions Newsgroup dataset created choosing vocabularies different sizes average TFIDF For NYT dataset version chosen So experiments performed datasets Information given Table 5 Each dataset versions binary version word frequencies discarded bagsofwords version word frequencies kept HLTA CorEx run binary version process binary data The methods nCRP nHDP hPAM run versions results denoted nCRPbin nHDPbin hPAMbin nCRPbow nHDPbow hPAMbow respectively Note LDAbased algorithms run binary datasets modiﬁcation binary data represented bagsofwords words appear 92 Settings cid9 HLTA run modes In ﬁrst mode denoted HLTAbatch entire dataset model struction phase batch EM parameter estimation phase In second mode denoted HLTAstep randomly sampled data cases model construction phase stepwise EM subset N parameter estimation phase Section 7 In experiments N set 10000 size minibatch 1000 parameter α stepwise EM 075 HLTAbatch run datasets HLTAstep run NYT Newsgroup datasets HLTAstep run NIPS datasets sample size small For HLTAbatch number κ iterations batch EM set 50 For HLTAstep stepwise EM terminated 100 updates cid9 The parameters HLTA Algorithm 1 set follows modes The threshold δ UDtests set 3 upper bound μ island size set 15 upper bound τ number toplevel topics set 30 NYT 20 datasets When extracting topics HLTM Section 4 ignored level 1 latent variables topics ﬁnegrained consist different forms word image images The LDAbased methods nCRP nHDP hPAM learn model structures A hierarchy needs supplied input In experiment height hierarchy set 3 usually literature The number nodes level set way nCRP nHDP hPAM yield roughly total number topics HLTA For example HLTA produced 170 topics NIPS1k dataset To ensure nHDP produce similar number topics 13 nodes level let node children leading total number 169 topics For hPAM 100 subtopics 70 supertopics root topic In nCRP number topics controlled hyperparameter η After tuning setting η 01 005 0025 respectively levels result approximately 170 topics For CorEx number nodes level set number nodes corresponding level HLTA 5 Let Y latent variable Z1 Zk children Let Z variable CorEx gives distribution P Y Z 0 Then expression obtain P Z Y P Y Y root 6 nCRP https github com bleilab hlda nHDP http wwwcolumbia edu jwp2128 code nHDPzip hPAM httpwwwarbylonnetprojectsknowceans ldacgenHpam2pGibbsSamplerjava CorEx https github com gregversteeg CorEx 7 http wwwcs nyu edu roweis data html value Z data case d It obtained hard assignment Z latent k 1dZ1 ZkD deﬁnes joint distribution Y Z1 Zk From joint distribution k data case d Let 1dZ1 Zk function takes value 1 Z Z dD P Y Z 1 Z 1 Z d d cid5 d d d d P Chen et al Artiﬁcial Intelligence 250 2017 105124 121 Table 6 Perdocument heldout loglikelihood binary data Best scores marked bold The sign indicates nontermination 72 hours NR stands run HLTAbatch HLTAstep nCRPbin hPAMbin nHDPbin CorEx Table 7 Running times Time min HLTAbatch HLTAstep nCRPbin nHDPbin hPAMbin nCRPbow nHDPbow hPAMbow CorEx NIPS1k 393 0 NR 671 16 1183 3 1188 1 445 2 NIPS1k 56 01 NR 782 39 152 1 261 17 853 150 379 14 850 27 53 02 NIPS5k 1121 1 NR 3034 135 3272 3 1243 2 NIPS10k 1428 1 NR 4001 11 1610 4 NIPS5k 861 37 NR 3608 163 288 16 3939 301 416 49 371 23 NIPS10k 318 12 NR 299 9 413 16 1190 9 News1k 114 0 114 0 183 2 183 1 149 1 News1k 661 29 96 01 162 13 328 9 250 18 604 19 779 34 News5k 242 0 243 0 407 2 323 4 News5k 432 39 133 5 263 9 332 16 4287 52 NYT 754 1 755 1 1530 5 NYT 787 42 421 17 430 42 564 59 All experiments conducted desktop Each experiment repeated 3 times variances estimated 93 Model quality running times For topic models standard way assess model quality measure loglikelihood heldout test set 25 In experiments dataset randomly partitioned training set 80 data test set 20 data Models learned training set perdocument loglikelihood calculated heldout test set The statistics shown Table 6 For comparability results binary data included We heldout likelihood values HLTA drastically higher alternative methods This implies models obtained HLTA predict unseen data better methods In addition variances signiﬁcantly smaller HLTA methods cases Table 7 shows running times We versions HLTA HLTAstep signiﬁcantly eﬃcient HLTAbatch large datasets virtually decrease model quality To compare HLTA methods need mind algorithms parameters control computational complexity Thus running time comparison meaningful reference model quality It clear Tables 6 7 HLTA achieved better model quality alternative algorithms comparable time 94 Quality topics It argued general better model ﬁt necessarily imply better topic quality 52 It meaningful compare alternative methods terms topic quality directly We measure topic quality metrics The ﬁrst topic coherence score proposed 53 Suppose topic t characterized list W t w M words The coherence score t given t 1 w t 2 w t M CoherenceW t Mcid4 i1cid4 i2 j1 log cid6 D w cid7 t w cid6 t j cid7 D w t j 1 17 Dw number documents containing word w Dw w j number documents containing w w j It clear score depends choices M generally decreases M In experiments set M 4 topics produced HLTA 4 words choice larger value M methods disadvantage With M ﬁxed higher coherence score indicates better quality topic The second metric use topic compactness score proposed 54 The compactness score topic t given compactnessW t 2 MM 1 Mcid4 i1cid4 i2 j1 Sw t w t j 18 122 P Chen et al Artiﬁcial Intelligence 250 2017 105124 Table 8 Average topic coherence scores HLTAbatch HLTAstep nCRPbow nHDPbow hPAMbow nCRPbin nHDPbin hPAMbin CorEx NIPS1k 595 004 NR 746 031 766 023 686 008 701 037 895 011 683 011 720 023 NIPS5k 774 007 NR 903 016 970 019 983 008 1159 012 976 048 NIPS10k 815 005 NR 1089 038 1234 011 1196 052 News1k 1200 009 1166 019 1351 008 1174 014 1345 005 1263 006 1349 148 Table 9 Average compactness scores HLTAbatch HLTAstep nCRPbow nHDPbow hPAMbow nCRPbin nHDPbin hPAMbin CorEx NIPS1k 0253 0003 NR 0163 0003 0164 0005 0215 0013 0176 0005 0119 0005 0145 0008 0243 0018 NIPS5k 0279 0008 NR 0153 0001 0147 0006 0137 0005 0107 0003 0162 0013 NIPS10k 0265 0001 NR 0138 0002 0102 0003 0167 0003 News1k 0239 0010 0250 0003 0150 0003 0210 0006 0138 0003 0169 0010 0185 0012 News5k 1267 015 1208 011 1393 021 1417 008 1471 045 News5k 0239 0006 0243 0002 0148 0004 0134 0003 0156 0009 NYT 1209 007 1197 005 1290 016 1455 007 NYT 0337 0003 0338 0002 0250 0003 0166 0001 Sw w j similarity words w w j determined word2vec model 5556 The word2vec model trained Google News dataset8 It contains 100 billion word tokens word mapped high dimensional vector The similarity words deﬁned cosine similarity corresponding vectors When calculating compactnessW t words occur word2vec model simply skipped Note coherence score calculated corpus analyzed In sense intrinsic metric The intuition words good topic tend cooccur documents On hand compactness score calculated general large corpus related corpus analyzed Hence extrinsic metric The intuition words good topic closely related semantically Tables 8 9 average topic coherence topic compactness scores topics produced meth ods For LDAbased methods reported scores datasets binary bagsofwords versions We scores topics produced HLTA signiﬁcantly higher obtained methods cases 95 Quality topic hierarchies There metric measuring quality topic hierarchies best knowledge diﬃcult come Hence resort manual comparisons The entire topic hierarchies produced HLTA nHDP NIPS NYT datasets URL men tioned beginning previous section Table 10 shows hierarchy nHDP corresponds hierarchy HLTA shown Table 3 In HLTA hierarchy topics nicely divided groups economy stock market companies In Table 10 clear division The topics mixed The hierarchy match semantic relationships topics Overall topics topic hierarchy obtained HLTA meaningful nHDP 10 Conclusions future directions In paper presented novel method called HLTA hierarchical topic detection The idea model patterns word cooccurrence cooccurrence patterns hierarchical latent tree model Each latent variable HLTM represents soft partition documents document clusters partitions interpreted topics Each topic characterized words occur high probability documents belonging topic occur low probability documents belonging topic Progressive EM accelerate parameter learning intermediate models created model construction stepwise EM accelerate parameter learning ﬁnal model 8 httpscodegooglecomarchivepword2vec P Chen et al Artiﬁcial Intelligence 250 2017 105124 123 Table 10 A topic hierarchy obtained nHDP NYT dataset 1 company business million companies money 11 economy economic percent government 12 percent stock market analyst quarter 121 stock fund market investor investment 122 economy rate rates fed economist 123 company quarter million sales analyst 124 travel ticket airline flight traveler 125 car ford sales vehicles_chrysler 13 technology software 14 company deal million billion stock 15 worker job union employees contract 16 project million plan official area HLTA differs fundamentally LDAbased methods hierarchical topic detection While approaches deﬁne probability distribution documents use different types observed latent variables HLTA involves structure learning parameter learning LDAbased methods involve parameter learning The notions topics topic hierarchies different Empirical results HLTA outperforms LDAbased methods terms overall model ﬁt quality topicstopic hierarchies takes time HLTA treats words binary variables word allowed appear branch hierarchy For future work interesting extend HLTA handle count data word allowed appear multiple branches hierarchy Another direction scale HLTA distributed computing means Acknowledgements We thank anonymous reviewers valuable comments suggestions John Paisley sharing nHDP im plements Jun Zhu useful discussions Research article supported Hong Kong Research Grants Council grants 16202515 16212516 The Education University Hong Kong project RG9020142015R References 1 DM Blei T Griﬃths M Jordan J Tenenbaum Hierarchical topic models nested Chinese restaurant process NIPS vol 16 2004 pp 106114 2 DM Blei T Griﬃths M Jordan The nested Chinese restaurant process Bayesian nonparametric inference topic hierarchies J ACM 57 2 2010 71730 3 W Li A McCallum Pachinko Allocation DAGstructured mixture models topic correlations ICML 2006 pp 577584 4 D Mimno W Li A McCallum Mixtures hierarchical topics pachinko allocation ICML 2007 pp 633640 5 J Paisley C Wang DM Blei M Jordan et al Nested hierarchical Dirichlet processes IEEE Trans Pattern Anal Mach Intell 37 2 2015 256270 6 DM Blei AY Ng MI Jordan Latent Dirichlet allocation J Mach Learn Res 3 2003 9931022 7 J Lafferty DM Blei Correlated topic models NIPS 2006 pp 147155 8 DM Blei JD Lafferty Dynamic topic models ICML 2006 9 X Wang A McCallum Topics time nonMarkov continuoustime model topical trends SIGKDD 2006 pp 424433 10 A Ahmed EP Xing Dynamic nonparametric mixture models recurrent Chinese restaurant process ICDM 2007 11 YW Teh MI Jordan MJ Beal DM Blei Hierarchical Dirichlet processes J Am Stat Assoc 101 2005 476 12 D Andrzejewski X Zhu M Craven Incorporating domain knowledge topic modeling Dirichlet forest priors ICML 2009 pp 2532 13 J Jagarlamudi H Daumé III R Udupa Incorporating lexical priors topic models Association Computational Linguistics 2012 pp 204213 14 JD Mcauliffe DM Blei Supervised topic models NIPS 2008 pp 121128 15 AJ Perotte F Wood N Elhadad N Bartlett Hierarchically supervised latent Dirichlet allocation NIPS 2011 pp 26092617 16 NL Zhang Hierarchical latent class models cluster analysis Proceedings 18th National Conference Artiﬁcial Intelligence 2002 17 NL Zhang Hierarchical latent class models cluster analysis J Mach Learn Res 5 2004 697723 18 NL Zhang S Yuan T Chen Y Wang Latent tree models diagnosis traditional Chinese medicine Artif Intell Med 42 3 2008 229245 19 Y Wang NL Zhang T Chen Latent tree models approximate inference Bayesian networks AAAI 2008 20 PF Lazarsfeld NW Henry Latent Structure Analysis Houghton Miﬄin Boston 1968 21 M Knott DJ Bartholomew Latent Variable Models Factor Analysis vol 7 Edward Arnold 1999 22 R Durbin SR Eddy A Krogh G Mitchison Biological Sequence Analysis Probabilistic Models Proteins Nucleic Acids Cambridge University 23 R Mourad C Sinoquet NL Zhang T Liu P Leray et al A survey latent tree models applications J Artif Intell Res 47 2013 157203 24 NJ Choi VYF Tan A Anandkumar AS Willsky Learning latent tree graphical models J Mach Learn Res 12 2011 17711812 25 T Chen NL Zhang T Liu KM Poon Y Wang Modelbased multidimensional clustering categorical data Artif Intell 176 2012 22462269 26 T Liu NL Zhang P Chen AH Liu KM Poon Y Wang Greedy learning latent tree models multidimensional clustering Mach Learn 98 12 Press 1998 2013 301330 27 T Liu NL Zhang P Chen Hierarchical latent tree analysis topic detection ECMLPKDD Springer 2014 pp 256272 28 P Chen NL Zhang LK Poon Z Chen Progressive EM latent tree models hierarchical topic detection AAAI 2016 29 G Ver Steeg A Galstyan Discovering structure highdimensional data correlation explanation NIPS vol 27 2014 pp 577585 30 M Steinbach G Karypis V Kumar et al A comparison document clustering techniques KDD Workshop Text Mining Boston vol 400 2000 pp 525526 31 IS Dhillon Coclustering documents words bipartite spectral graph partitioning SIGKDD ACM 2001 pp 269274 124 P Chen et al Artiﬁcial Intelligence 250 2017 105124 32 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann 1988 33 L Poon NL Zhang T Chen Y Wang Variable selection modelbased clustering facilitate ICML10 2010 pp 887894 34 S Kirshner Latent tree copulas European Workshop Probabilistic Graphical Models 2012 35 H Liu A Parikh E Xing Nonparametric latent tree graphical models inference estimation structure learning J Mach Learn Res 12 2017 663707 36 TM Cover JA Thomas Elements Information Theory 2nd edition Wiley 2006 37 A Dempster N Laird D Rubin Maximum likelihood incomplete data EM algorithm J R Stat Soc B 39 1 1977 138 38 G Schwarz Estimating dimension model Ann Stat 6 1978 461464 39 S Harmeling CKI Williams Greedy learning binary latent trees IEEE Trans Pattern Anal Mach Intell 33 6 2011 10871097 40 CK Chow CN Liu Approximating discrete probability distributions dependence trees IEEE Trans Inf Theory 14 3 1968 462467 41 AE Raftery Bayesian model selection social research Sociol Method 25 1995 111163 42 CK Chow CN Liu Approximating discrete probability distributions dependence trees IEEE Trans Inf Theory 14 3 1968 462467 43 KP Murphy Machine Learning A Probabilistic Perspective The MIT Press 2012 44 MA Sato S Ishii Online EM algorithm normalized Gaussian network Neural Comput 12 2 2000 407432 45 O Cappé E Moulines Online expectationmaximization algorithm latent data models J R Stat Soc Ser B Stat Methodol 71 3 2009 593613 46 P Liang D Klein Online EM unsupervised models Proceedings Human Language Technologies The 2009 Annual Conference North American Chapter Association Computational Linguistics 2009 pp 611619 47 O Bousquet L Bottou The tradeoffs large scale learning NIPS 2008 pp 161168 48 GG Chowdhury Introduction Modern Information Retrieval Facet Publishing 2010 49 Y Choueka Looking needles haystack locating interesting collocational expressions large textual databases RIAO 88 Recherche dInformation Assistée par Ordinateur Conference 1988 pp 609623 50 JH Lau T Baldwin D Newman On collocations topic models ACM Trans Speech Lang Process TSLP 10 3 2013 10 51 LK Poon NL Zhang Topic browsing research papers hierarchical latent tree analysis preprint arXiv160909188 52 J Chang S Gerrish C Wang JL Boydgraber DM Blei Reading tea leaves humans interpret topic models NIPS 2009 pp 288296 53 D Mimno HM Wallach E Talley M Leenders A McCallum Optimizing semantic coherence topic models Proceedings Conference Empirical Methods Natural Language Processing 2011 pp 262272 54 Z Chen N Zhang DY Yeung P Chen Sparse Boltzmann machines structure learning applied text analysis AAAI 2017 55 T Mikolov K Chen G Corrado J Dean Eﬃcient estimation word representations vector space ICLR 2013 56 T Mikolov I Sutskever K Chen GS Corrado J Dean Distributed representations words phrases compositionality NIPS 2013 pp 31113119