Artiﬁcial Intelligence 239 2016 7096 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Diffusion centrality A paradigm maximize spread social networks Chanhyun Kang Sarit Kraus b Cristian Molinaro c VS Subrahmanian Department Computer Science University Maryland USA b Department Computer Science BarIlan University Israel c DIMES Università della Calabria Italy d Department Computer Science Boise State University USA Francesca Spezzano d r t c l e n f o b s t r c t Article history Received 7 February 2015 Received revised form 21 May 2016 Accepted 30 June 2016 Available online 7 July 2016 Keywords Social networks Diffusion model Logic programming Quantitative logic We propose Diffusion Centrality DC semantic aspects social network characterize vertices inﬂuential diffusing property p In contrast classical centrality measures diffusion centrality vertices varies property p depends diffusion model describing p spreads We DC applies known diffusion models including tipping cascade homophilic models We present hypergraphbased algorithm HyperDC optimizations exactly compute DC However HyperDC scale huge social networks millions vertices tens millions edges For scaling develop methods coarsen network propose heuristic algorithm called Coarsened Back Forth CBAF compute topk vertices having highest diffusion centrality We report experiments comparing DC classical centrality measures terms runtime spread achieved k central vertices 7 realworld social networks 3 different diffusion models Our experiments DC produces higher quality results comparable centrality measures terms runtime 2016 Elsevier BV All rights reserved 1 Introduction An increasingly important problem social networks SNs assigning centrality value vertices reﬂect importance SN Wellknown measures degree centrality 2146 betweenness centrality 820 PageRank 9 closeness centrality 495 eigenvector centrality 7 structure network account differentiate vertices central wrt spreading topic meme sentiment vs spreading A vertex important spreading awareness mobile phone program unimportant spread ing information restaurant Likewise past work assumes information properties verticesedges edge weights modern social networks selfdeclared properties exist cases analysis tweets posts provide information These omissions cause different problems shown following toy example Corresponding author Email address cmolinarodimesunicalit C Molinaro httpdxdoiorg101016jartint201606008 00043702 2016 Elsevier BV All rights reserved C Kang et al Artiﬁcial Intelligence 239 2016 7096 71 Fig 1 A small HIV social network Shaded vertices denote people HIV Example 1 HIV Fig 1 shows people b c d b HIV Solid edges denote sexual relationships dashed edges denote friend relationships Both friend sexual partner relationships play role diffusion HIV friends unbeknownst sexual partners Edge weights denote intensity relationships The table shows centrality b c d wrt centrality measures Notice nature rela tionships friend sexual partner taken account centrality measures consider topological structure network Centrality measure Degree Betweenness PageRank Closeness Eigenvector 1 2 0367 033 0375 b 033 0 0141 02 0125 c 066 0 0246 025 025 d 066 0 0246 025 025 The person network capable spreading HIV b However b lowest centrality according ﬁve centrality measures mentioned Example 2 Consider network shown Fig 1 suppose vertices denoted users Twitter A solid edge u v denotes fact u v retweeted tweets dashed edge indicates friends u follows v vice versa Suppose b person tweeted positive opinion political candidate Then reasoning previous example given Fig 1 entire network infer user c d tweets positively candidate inﬂuenced b inﬂuenced exogenous process b clearly credit users positive tweet lowest centrality according classical centrality measures Past centrality measures account property wrt vertexs importance measured ii properties HIV example diffuse SN iii semantic aspect network properties vertices edges solely focusing topological structure Taking aspects account crucial determining central vertices We readily think networks Twitter person A highest centrality terms spread support Republicans person B central player terms spread support conserving gorillas The network cases Twitter central person depends diffusive property respect vertex considered inﬂuential central Taking account diffusion models person inﬂuences crucial aspect Different ways spreading property disease lead different central vertices Furthermore intrinsic properties vertices customers patients nature strength relationships edges important For instance 45 talks role different demographic factors inﬂuencing online purchases 14 product categories showing demographic factors relevant product types relevant This paper proposes novel notion diffusion centrality takes SN diffusive property p previously learned diffusion model cid2 p deﬁnes centrality vertices based input We provide algorithms automatically learn diffusion modelsinterested readers ﬁnd algorithm 10 The papers goal diffusion centrality achieve higher spread diffusive property p diffusion models p classical centrality measures We diffusion models seen literature Last methods shown scale social networks 2M vertices 20M edges Realworld diffusion models fall diverse categories In cascade models probability vertex spread diffusive property p neighbors 3436these include popular disease spread models SIR model disease spread 26 Tipping models use mathematical calculations costbeneﬁt analysis involving probabilities order decide vertex adopt certain behavior Tipping models introduced Nobel laureate Tom Schelling 50 model segregation neighborhoods Granovetter 24 In homophilic models similarities vertices considered order decide vertices adopt similar behavior 42123 72 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Homophilic models use types distance measures attributes vertex age occupation gender combine nonprobabilistic measures achieve degree similarity vertices Because diffusion models vary dramatically paradigm express capable expressing semantic properties vertices connections ii representing probabilistic inferences iii expressing inferences based nonprobabilistic quantitative reasoning The suite knowledge representation paradigms offers starting points Bayesian nets causal inference 47 offer obvious place start express cascade models However clear express generic quantitative information needed express real diffusion models tipping homophilic models In contrast language Generalized Annotated logic Programs GAPs 35 wellstudied knowledge representation rich capture wide variety different forms reasoning It represent structure social networks semantics weight annotations diverse types diffusion models Indeed shown 51 existing diffusion models aforementioned categories expressed GAPs including SusceptibleInfectiousRemoved SIR 2 SusceptibleInfectiousSusceptible SIS 26 models disease spread Big Seed marketing approach 57 strategy advertising large group individuals likely spread advertisement network effects models diffusion favorited pictures Flickr 13 tipping models like JacksonYariv model 28 We considered richer versions GAPs hybrid knowledge bases 40 decided far expressive needed reasoning diffusive processes The paper organized follows Related work discussed Section 2 Section 3 introduces basic notions rest paper We deﬁne Diffusion Centrality DC Section 4 Section 5 proposes general hypergraph ﬁxed point al gorithm eﬃciently compute likelihood arbitrary vertex certain properties according GAP diffusion model We deﬁne novel classes GAPs novel optimization methods develop HyperDC algorithm computing DC In Section 6 propose CBAF algorithm ﬁnding vertices topk highest diffusion cen tralities approximate way Section 7 describes extensive experiments comparing DC classical centrality measures terms runtime spread generated central vertices 2 Related work Several centrality measures proposed graph theory social network analysis degree centrality 2146 betweenness centrality 820 PageRank 9 closeness centrality 495 eigenvector centrality 7 Variants centrality measures proposed 18125 gametheoretic centrality measures proposed 2752 These centrality measures account network topology determining vertex centrality DC considers additional information semantics embedded network diffusive property respect vertex considered inﬂuential central model describing property propagates There extensive work reasoning diffusion social networks One wellknown problem inﬂuence maximization problem identifying subset vertices social network maximizes spread inﬂu ence The problem formulated optimization problem seminal paper 33 focuses propagation models independent cascade linear threshold models Subsequently approaches proposed eﬃciently solve problem 39152916235637 All aforementioned approaches consider restricted diffusion models vertexedge properties taken account In contrast approach deals general diffusion mod els takes social network semantic properties account Being able accurately model diffusion processes incorporate network semantics fundamental correct analysis realworld diffusion phenomena Another wellstudied related problem target set selection problem 1417 assumes deterministic tipping model seeks ﬁnd set vertices certain size optimizes ﬁnal number adopters These approaches focus speciﬁc diffusion models neglect networks semantics The notion diffusion centrality ﬁrst proposed 31 This paper extends 31 different respects In 31 diffu sion models expressed simple conditional probability rules paper use general language GAPs The algorithms introduced paper general eﬃcient exploit new optimizations troduced paper Moreover addressed new problem ﬁnding approximate set topk vertices highest diffusion centrality proposed eﬃcient algorithms solve Recently paper diffusion centrality 31 pieces work observed individual universal inﬂuencer inﬂuential members network tend inﬂuential speciﬁc domains knowledge 41153 This led extension classic independent cascade linear threshold models topicaware 411 considering propagation respect particular topic Still diffusion models considered works limited characteristics vertices edges Recently 48 addressed problem coarsening social network ﬁnding succinct representation grouping vertices preserving propagation characteristics possible It works independent cascade model considers structure network merging The idea coarsening network extensively community detection techniques 32 However use different metrics coarsening cutbased ﬂowbased heavyedge matchingbased conditions Methods compress weighted graphs smaller ones proposed 5455 nodes edges grouped supernodes superedges No semantic properties verticesedges diffusion models considered In CBAF algorithm coarsening networks C Kang et al Artiﬁcial Intelligence 239 2016 7096 73 explicitly consider given diffusion model merge vertices role process considering semantic aspects SN Another related problem graph sparsiﬁcation relies notion spanners 22 41 The main difference graph sparsiﬁcation removes edges nodes stay contract graph trying preserve behavior wrt given diffusion model 43 uses probabilistic soft logic develop graph summarization techniques grouping similar entities vertices relations edges considering semantic aspects networks The approach consider diffusion process explicitly 3 Preliminaries In section deﬁne social networks SNs illustrate generalized annotated programs GAPs 35 diffusion models expressed GAPs We refer reader 35 details GAPs We model SNs weighted directed graphs properties assigned vertices edges More speciﬁcally properties vertices edges taken disjoint sets VP EP respectively For instance property VP hiv meaning vertex HIV properties EP fr sp representing friend sexual relationships respectivelyeg edge assigned property fr endpoints friends A social network SN tuple V E VL ω 1 V ﬁnite set vertices 2 E V V EP ﬁnite set labeled edges 3 VL V 2VP assigns set properties vertex 4 ω E 0 1 assigns weight edge Thus SN directed graph VL assigns set properties vertex multiple labeled edges given pair vertices associated weight unique edge property An SN depicted Fig 1 property hiv assigned vertex b sp assigned solid edges fr assigned dashed edges Below brieﬂy recall GAPs We start example illustrates GAP modeling spread HIV SNs like Fig 1 Example 3 A GAP cid2hi v SN Example 1 cid6 Y hivV cid6 r1 hivV 09 X Y spV V r2 hivV 04 X Y Y r3 hivV 06 X Y Y cid6 frV V cid6 spV V The ﬁrst rule says conﬁdence vertex V HIV given partner V HIV conﬁdence X 09 X Y Y weight sexual relationship vertices The rules similarly read Y spV cid6 Y spV cid6 hivV cid6 hivV X cid6cid6 X Y cid6cid6 Y V cid6 V cid6cid6 cid6cid6 cid6 cid6 X cid6 We treat properties VP unary predicate symbols called vertex predicate symbols properties EP binary predicate symbols called edge predicate symbols Given vertex resp edge predicate symbol p pt resp pt1 t2 vertex atom resp edge atom t t1 t2 variables constants representing vertices For instance Example 3 hivV spV V cid6 vertex atom edge atom respectively An edge vertex annotated atom form A μ A edge vertex atom μ annotation term expression built functions variables distinct inside atoms real numbers 0 1 For instance Example 3 hivV 09 X Y annotated atom A GAPrule simply rule form A0 μ0 A1 μ1 An μn n 0 Ai μi annotated atom A0 μ0 head rule A1 μ1 An μn body rule A generalized annotated program GAP ﬁnite set rules We assume VP contains distinguished vertex predicate symbol vertex represents presence vertex SN Every SN S V E VL ω represented GAP denoted cid2S follows cid2S vertexv 1 v V pv 1 v V p VLv epv 1 v 2 ωcid11v 1 v 2 epcid12 cid11v 1 v 2 epcid12 E When augment cid2S rules describing certain properties diffuse social network GAP cid2 cid2S captures structure SN diffusion principles In paper consider restricted class GAPs rule nonempty body vertex annotated atom head 35 allows annotated atom head rule Thus edge atoms appear rule bodies rules body This restriction results set diffusion models consider paper edge weights edge labels change result diffusion However techniques developed paper directly applied easily generalized unrestricted GAPs 74 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Table 1 Iterations Tcid2 Iteration Tcid2 0 1 2 3 4 hiva 0 0 009 009 009 hivb 0 1 1 1 1 hivc 0 0 0006 00081 00081 hivd 0 0 0032 0032 0032 An annotated atom resp rule GAP ground iff contains variables inside atoms annotation terms We use A denote set ground atoms Moreover grdr denotes ground instances rule r set rules obtained r replacing occurrence variable annotation term real number 0 1 occurrence variable inside atom vertex Given GAP cid2 deﬁne grdcid2 We brieﬂy recall semantics GAPs An interpretation I mapping set ground atoms A 0 1 We I satisﬁes ground annotated atom A μ denoted I A μ iff I A μ 35 associates operator Tcid2 maps interpretations interpretations GAP cid2 Suppose I interpretation Then rcid2 grdr cid2 Tcid2I A maxI A μ A μ A A1 A An grdcid2 1 n I A Ai Roughly speaking semantics GAPs requires multiple ground instances GAPrules head ﬁres highest annotation ground rules assigned head atom 35 shows start interpretation assigns 0 ground atom iteratively apply Tcid2 reach ﬁxed point denoted lfpcid2 captures ground atomic logical consequences cid2 Example 4 Consider social network S Example 1 GAP cid2hi v Example 3 Let cid2 GAP modeling SN diffusion model cid2 cid2S cid2hi v Then Tcid2 reaches ﬁxed point iteration shown Table 1 Edge atoms reported table annotations weights change applying Tcid2 edge atoms appear rule heads Speciﬁcally 01 assigned spa b spb spa c spc 08 assigned fra d frd 07 assigned frc d frd c Initially 0 assigned ground atoms iteration 0 Next 1 assigned ground atom hivb b HIV original SN iteration 1 From rules cid2hi v assign higher values vertex ground atoms Speciﬁcally iteration 2 following rules ﬁre r r r cid6 1 cid6 2 cid6 3 hiva 09 1 01 spa b 01 hivb 1 hivd 04 1 08 01 frd 08 spa b 01 hivb 1 hivc 06 1 01 01 spc 01 spa b 01 hivb 1 assign 009 hiva 0032 hivd 0006 hivc At subsequent iteration iteration 3 following rule ﬁre assign higher value hivc 00081 r cid6cid6 1 hivc 09 009 01 spc 01 hiva 009 No higher values derived application Tcid2 ﬁxed point reached assigning 009 hiva 1 hivb 00081 hivc 0032 hivd Example 5 Suppose SN Fig 1 represents Cell phone users edges property fr representing friendship relations vertices properties like male female young old adopter telling user adopted cell phone plan The phone company wants identify important users Suppose d male female initially adopter A cell phone provider diffusion rule learned past promotions adopterV cid6 06 X Y adopterV X maleV Y frV V cid6 01 The vertex greatest inﬂuence given free mobile phone plan diffusion model clearly d vertex inﬂuence adopt plan However table Example 1 d relevant vertex wrt centrality measures 4 Diffusion centrality Diffusion centrality tries measure vertex v diffuse property p hiv property Given SN S V E VL ω vertex predicate symbol p vertex v V insertion pv S denoted S pv cid6v VLv p In words inserting pv SN V E VL exactly like VL VL cid6 cid6 ω VL C Kang et al Artiﬁcial Intelligence 239 2016 7096 75 social network merely says vertex v property p network stays cid6cid6 ω like S Likewise removal pv S denoted S cid16 pv social network V E VL VL cid6cid6v VLv p Deﬁnition 1 Diffusion centrality Let S V E VL ω SN cid2 GAP p property The diffusion centrality DC short vertex v V wrt cid2 p S denoted dccid2pS v deﬁned follows cid3 vcid6V v lfpcid2 cid2Spvpv cid6 cid3 vcid6cid6V v lfpcid2 cid2Scid16pvpv cid6cid6 Whenever cid2 p S clear context denote diffusion centrality vertex v simply dcv This deﬁnition says computing diffusion centrality vertex v involves steps First assume vertex v property p diffusion occurs This computing ﬁxed point diffusion model SN S pv original SN p assigned v Notice overall diffusion quantiﬁed cid6 cid17 v S Then summing values ﬁxed point assigns atoms form pv assume vertex v property p diffusion occurs This computing ﬁxed point diffusion model SN S cid16 pv original SN p assigned v The diffusion centrality v difference numbers captures impact occur terms diffusion property p vertex v property p1 cid6 vertices v Notice ﬁxed point cid2 S pv S cid16 pv takes account initial assignment p vertices S Thus DC depends initial assignment p S For instance consider SNs identical ﬁrst vertex property p second vertex property p In ﬁrst SN dcv 0 vertex v p given v impact p In contrast second SN dcv reﬂects overall spread p achieve SN assigning p v Example 6 Consider HIV SN Example 1 GAP Example 3 Recall vertex property hiv b It easily veriﬁed values positive negative summands Deﬁnition 1 vertices reported following table Positive summand Negative summand Diffusion centrality 1122 10401 00819 b 013 0 013 c 1122 1122 0 d 10981 10981 0 For instance consider vertex If assume hiv given original SN overall spread hiv 1122 positive summand Deﬁnition 1 If assume hiv given original SN overall spread hiv 10401 negative summand Deﬁnition 1 Then diffusion centrality difference values The argument applied remaining vertices It worth noting hiv assigned b original SN spread hiv occurs negative summand b 0 HIV SN Thus b highest centrality wrt hiv cid2hi v classical centrality measures Example 1 capture b central vertex purely topological perspective However b highest centrality HIV Vertices c d increase conﬁdence vertex HIV So diffusion centrality zero Example 7 If return cell phone case Example 5 DC d 12 vertices 0 DC Furthermore opposed classical centrality metrics c d centrality properties diffusion important different extent Diffusion Centrality Problem DCP Given SN S V E VL ω GAP cid2 property p diffusion centrality problem consists ﬁnding DC wrt cid2 p S vertex S Topk Diffusion Centrality Problem kDCP Given 0 k V topk diffusion centrality problem consists ﬁnding set T k vertices S having highest DC DC vertex T greater equal DC vertex V T 1 Considering ﬁrst summation Deﬁnition 1 wrong Suppose SN vertex v st ﬁrst summation dcv high number N N expected number vertices property p assuming v property p Suppose assume v property p value N determined second summation equals N Then intuitively v high diffusion centrality expected number vertices property p regardless v property p v play central role diffusing p In contrast considering ﬁrst summation v high centrality 76 C Kang et al Artiﬁcial Intelligence 239 2016 7096 5 The HyperDC algorithm exact diffusion centrality computation In section present HyperDC algorithm Section 53 solves DCP exactly HyperLFP algorithm Section 52 compute ﬁxed point GAP We start set optimizations speed HyperDC 51 Optimization steps We ﬁrst present optimizations applied arbitrary GAPs identify subclasses GAPs DC computed eﬃciently Before introduce notation terminology following The dependency graph GAP cid2 directed graph depcid2 vertices predicate symbols cid2 There edge predicate symbol q predicate symbol p iff rule cid2 p occurs head q occurs body We p depends q exists path q p depcid2 If p depends q vice versa p q mutually recursive We use Mcid2p denote set predicate symbols mutually recursive p We deﬁne Rcid2p set predicate symbols q p depends q ii q appears head rule cid2 Note Mcid2p Rcid2p Given GAP cid2 property p deﬁne cid2p r cid2 p head predicate symbol r p depends head predicate symbol r We ready introduce ﬁrst optimization Caching lfp When computing dcv note S S pv S cid16 pv differ vertex v property p One way leverage ﬁrst compute cache lfpcid2 cid2S independent v We need calculate summation order compute dcv Proposition 1 Consider social network S V E VL ω GAP cid2 property p Let φ lfpcid2 cid2S v vertex V Then dcv cid8 vcid6V v cid8 vcid6V v φpv cid6 cid8 vcid6cid6V v lfpcid2 cid2Spvpv cid6 lfpcid2 cid2Scid16pvpv cid8 φpv vcid6cid6V v cid6cid6 cid6cid6 p VLv p VLv Proof Consider vertex v V If v property p deﬁnition S pv φ lfpcid2 cid2Spv ﬁrst equation holds Likewise v property p deﬁnition S cid16 pv φ lfpcid2 cid2Scid16pv second equation follows cid2 Network ﬁltering We present optimization called network ﬁltering consists reducing given SN removing vertices relative incoming outgoing edges play role diffusion process vertices receive transmit diffusive property p fromto vertices SN rule considered diffusion model As shown following network ﬁltering sound optimization technique aim reduce size SN DC computation faster altering DC values Speciﬁcally removed vertices zero DC original SN DC remaining vertices reduced SN original cf Proposition 2 Example 8 Consider diffusion model cid2hi v Example 3 hivV 09 X W spV V hivV 04 X W W hivV 06 X W W cid6 frV V cid6 spV V cid6 X cid6 W spV cid6 W spV W hivV cid6 cid6 cid6cid6 V cid6 V cid6cid6 W cid6cid6 W cid6 hivV cid6 hivV X cid6cid6 X Suppose v vertex sp relations given SN Moreover suppose vs friends sp relations Then v unnecessary vertex purpose computing DC rule r cid2hi v vertex v receive property hiv transmit hiv vertices In fact easy rules vertex transmit hiv sp relations Moreover vertex hiv sp relations ﬁrst rules applied friends sp relations second rule applied Thus roughly speaking network ﬁltering matches diffusion rules vertices identify vertices partici pate activation rule regardless properties involved diffusion rules SN C Kang et al Artiﬁcial Intelligence 239 2016 7096 77 Deﬁnition 2 Rule activation Given GAP cid2 property p rule r cid2 deﬁne relbodyr A A A A annotated atom body r predicate symbol Rcid2p Vertex v SN S activates rule r cid2 iff exists ground rule r cid6 grdr 1 A A relbodyr cid6 2 A μ head r 3 v appears annotated atom body r cid6 cid2S A A μ 0 cid6 Deﬁnition 3 Necessary unnecessary vertices Let S SN cid2 GAP p property A vertex S necessary activates rule cid2p unnecessary Roughly speaking necessary vertex trigger rule ﬁxed point computation unnecessary vertices chance involved ﬁxed point computation Identifying necessary vertices similar spirt identiﬁcation relevant rules probabilistic logic programs aim ﬁnd ground rules relevant given query 19 In case interested values assigned ground atoms form pv sense query atomshere p diffusive property However getting rid ground rules rid vertices constants irrelevant This important applications SN millions vertices disregarding irrelevant ones yield signiﬁcantly better performances Of course disregarding vertices lead disregarding ground rules ground rules containing irrelevant vertex The ﬁltering social network eliminates unnecessary vertices incomingoutgoing edges Deﬁnition 4 Network ﬁltering Let S V E VL ω SN cid2 GAP p property U set unnecessary vertices The ﬁltering S wrt cid2 p SN S cid6 V cid6 ωcid6 cid6 VL cid6 E 1 V 2 E 3 VL 4 ωcid6e ωe e E cid6 V U cid6 E cid11u v qcid12 E u U v U cid6v VLv v V cid6 cid6 The ﬁltering Scid6 SN S useful unnecessary vertices DC zero S DC necessary vertices computed smaller SN S cid6 sound way DC S cid6 S Proposition 2 Let S SN cid2 GAP p property S cid6 dccid2pS v 0 v necessary dccid2pS v dccid2pScid6 v ﬁltering S For vertex v S v unnecessary Proof If vertex v unnecessary activate rule cid2 independent diffusion property p contribute diffusing p vertices item 2 Deﬁnition 2 It follows cid8 cid8 vcid6V v lfpcid2 cid2Spvpv vcid6V v lfpcid2 cid2S pv cid8 cid6 vcid6V v lfpcid2 cid2Scid16pvpv cid6 cid6 dccid2pS v 0 If vertex v necessary rules activates contain necessary vertices cid8 cid8 vcid6V v lfpcid2 cid2Spvpv vcid6V v lfpcid2 cid2Scid16pvpv cid6 cid6 cid8 cid8 vcid6V v lfpcid2 cid2Scid6pvpv vcid6V v lfpcid2 cid2Scid6cid16pvpv cid6 cid6 It follows dccid2pS v dccid2pScid6 v cid2 Subclasses GAPs We introduce pmonotonic GAPs class GAPs apply optimizations addition discussed far Deﬁnition 5 pmonotonic GAP We rule A0 μ0 A1 μ1 An μn monotonic iff μ0 monotonic function2 Given GAP cid2 property p cid2 pmonotonic iff rule r cid2 head predicate symbol Rcid2p r monotonic 2 If μ0 constant considered monotonic Moreover μ0 f μ0 monotonic f f x1 xn f y1 yn n number arguments f monotonic xi yi 1 n 78 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Example 9 Consider GAP cid2 Example 3 Rcid2hiv hiv To cid2 hivmonotonic need check rule cid2 having annotated atom form hivV μ head μ monotonic function Since case cid2 hivmonotonic The following GAP pV 05 X W frV V qV 1 X W frV V pV 09 X W frV V cid6 cid6 W pV W pV cid6 W qV cid6 X X X cid6 cid6 pmonotonic Rcid2p p q annotation head atom second rule nonmonotonic function Given GAP cid2 property p deﬁne pinterfered predicate set follows cid9 Icid2p Mcid2p Rcid2p cid2 pmonotonic cid2 pmonotonic Thus pinterfered predicate set contains predicate symbols mutually recursive p GAP cid2 pmonotonic cid2 pmonotonic set predicate symbols q p depends q q appears head rule cid2 For instance Icid2hiv hiv GAP Example 3 Icid2p p q GAP Example 9 Notice Icid2p Rcid2p Icid2p bigger Mcid2p Rcid2p Recall given GAP cid2 property p cid2p set rules r cid2 p depends head predicate symbol r p head predicate symbol We deﬁne cid2 p r cid2p predicate symbol q body r st q Icid2p p SN depend remaining rules cid2p Then cid2p cid2 Roughly speaking rules cid2p ones affect values assigned ground atoms form pv ﬁxed pointso ignore rules cid2 cid2p Moreover cid2p partitioned sets cid2 p cid2p cid2 p We ﬁrst evaluate cid2 p evaluated This reminds stratiﬁcation logic programs program partitioned different strata evaluated time according order dictated dependencies In case ﬁrst evaluate base stratum cid2 p use result starting point evaluation stratum consisting mutually recursive rules cid2p cid2 p Thus setting strata interested special predicate p modeling diffusive property The following proposition states precisely cid2p cid2 p exploited recall A denotes set ground atoms Proposition 3 Consider SN S property p GAP cid2 Let ψ interpretation lfpcid2S cid2 lfpcid2p cid2 p A ψ A A Apv vertex v S p Then lfpcid2 cid2S pv While previous proposition applied arbitrary GAPs effective pmonotonic ones p precomputed initial stage larger In general smaller Icid2p cause Icid2p smaller cid2 effective proposition Example 10 Consider following GAP cid2 r1 sV 05 X pV X r2 pV W X Y frV V r3 sV 06 X W frV V r4 qV 09 X W frV V cid6 cid6 Y W pV X sV cid6 W qV X cid6 cid6 W maleV cid6 X Suppose p diffusive property Clearly cid2 pmonotonic rule heads contain monotonic functions cid2p cid2 cid2 r3 r4 predicate symbols body r3 resp r4 mutually recursive p Proposition 3 says p SN S ﬁrst compute interpretation ψ deﬁned lfpcid2S cid2 p Then starting ψ GAP consisting r1 r2 evaluated Below present class GAPs called pdwindling As discussed section pdwindling GAPs HyperLFP algorithm faster convergence ﬁxed point Deﬁnition 6 pdwindling GAP Suppose p property A GAP cid2 pdwindling iff ground rule A0 μ0 A1 μ1 An μn grdcid2 st q predicate symbol A0 q Icid2p case μ0 μi 1 n st predicate symbol Ai Icid2p C Kang et al Artiﬁcial Intelligence 239 2016 7096 79 For Flickr JacksonYariv SIR models Appendix A consider experimental evaluation state following properties Proposition 4 The Flickr model pmonotonic pdwindling The JacksonYariv model pmonotonic pdwindling The SIR model pmonotonic pdwindling 52 The HyperLFP algorithm In section propose eﬃcient hypergraphbased algorithm HyperLFP compute ﬁxed point diffusion centrality computation A directed hypergraph pair cid11V Hcid12 V ﬁnite set vertices H ﬁnite set directed hyperedges A hyperedge pair cid11S tcid12 S set vertices called source set t vertex called target vertex Given hyperedge h H Sh denotes source set th denotes target vertex We deﬁne hypergraph captures property p diffuses SN S according GAP cid2 The hypergraph depend vertices property p original SN depends cid2 structure S terms edges vertex properties p Therefore given GAP cid2 SN S diffusion hypergraph computed different assignments property p vertices S This allows save time computing diffusion centrality requires computing ﬁxed point different initial assignments p However SN changes diffusion hypergraph needs recomputed In addition hypergraph allows eliminate diffusion rules useless computing ﬁxed point Deﬁnition 7 Enabled rule Consider SN S GAP cid2 property p Let ϕ lfpcid2S cid2 enabled iff ϕ A μ annotated atom A μ body r predicate symbol Icid2p p A rule r grdcid2 cid2 p Intuitively enabled rules ground rules affect diffusion p directly indirectly ﬁxed point computation Example 11 Consider GAP cid2hi v Example 3 SN Example 1 cf Fig 1 In case cid2 ϕ lfpcid2S following ground instance second rule belongs grdcid2 cid2 p p r hivd 04 08 01 1 frd 08 spa c 01 hivc 1 The rule enabled ϕfrd 08 ϕspa c 01 Notice atom hivc play role determining rule enabled hiv Icid2hiv recall Icid2hiv hiv Deﬁnition 8 Diffusion hypergraph Consider SN S V E VL ω GAP cid2 property p The hyperedge associ cid6cid6v ated ground rule r grdcid2 head annotated atom form p cid6vcid12 denoted hedger The diffusion hypergraph HS cid2 p triple μi body r p cid11N H W cid12 cid6v μ deﬁned cid11p cid6cid6 Icid2p p cid6cid6v p 1 cid11N Hcid12 directed hypergraph N qv q Icid2p v V H hedger r enabled ground rule cid2 head predicate symbol Icid2p 2 W function h H matrix M1Icid2p1V real values 0 1 W h M head annotation ground rule r satisfying following properties hedger h ii atom qv appearing Sh Mqv equal annotation qv r Example 12 Fig 2 shows diffusion hypergraph GAP cid2hiv Example 3 social network Example 1 Since Icid2hiv hiv V b c d nodes diffusion hypergraph N hiva hivb hivc hivd The hyperedges derived enabled ground rules Consider instance enabled ground rule r Example 11 This rule corresponds hyperedge hedger cid11hivc hivdcid12 hivc set atoms body r st predicate symbols belongs Icid2hiv hivd atom head rule The hyperedge labels represent function W label hyperedge function annotation head atom corresponding rule For instance rule r Example 11 label hyperedge hedger 04 08 01 hivc 04 constant function values 08 01 precomputed represents update value hivd value hivc changed The rough idea HyperLFP algorithm Algorithm 1 hyperedges propagate value greater zero kept maxheap propagating higher values visited ﬁrst maxheap updated propagation unfolds For q Icid2p v V Mqv initial value ground atom qv U qv keeps track hyperedges cid6qv initially set M cid6qv current assignment qv Speciﬁcally M having qv source set M 80 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Fig 2 A diffusion hypergraph Algorithm 1 HyperLFP Input For social network S V E VL ω GAP cid2 property p matrices U 1Icid2p1V M1Icid2p1V maxheap Heap diffusion hypergraph HS cid2 p cid11N H W cid12 cid6 Heap Heap cid17 cid11h wcid12 deleteMaxHeap Let qv th cid6qv w M cid6qv w M h Output lfpcid2 cid2S qv v V q Icid2p 1 C copy M cid6 copy M 2 M 3 Heap cid17 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 return M cid6 U qv cid6 cid6 cid6 cid6 Cq cid2 pmonotonic cid6 W h cid6v cid6 Cq cid6v cid6 M cid6 th cid6v cid6 w Heap Heap cid6 Add cid11h cid6 w Let q w cid6cid12 Heap cid6cid12 Heap Add cid11h cid6 w cid6 w cid6 iteratively updated algorithm C keeps track highest values propagated hyperedges added Heap At iteration loop lines 518 pair cid11h wcid12 maximum w retrieved Heap cid6qv cid6qv w set w hyperedge retrieved Heap If w assigned M If M hyperedges affected inspected loop lines 1018 Only hyperedges having th source set inspected For hyperedge added Heap propagated higher value line 13 The cid2 pmonotonic hyperedge added Heap value propagates added Heap reason cid2 pmonotonic retrieve hyperedges descending order weights values derived different iterations Tcid2 However cid2 pmonotonic iterations Tcid2 executed mixing values derived So cid2 pmonotonic hyperedges retrieved descending order weight iteration Tcid2 When Heap Heap returned heaps assigned Heap M cid6 cid6 cid6 If GAP cid2 pdwindling pmonotonic HyperLFP ensures value w assigned ground atom qv w ﬁnal value qv ﬁxed point hyperedge propagated w hyperedge having qv target atom longer needs considered order new higher value assigned qv Proposition 5 The worstcase time complexity Algorithm HyperLFP O N 1 α Umax maxvV qIcid2p cid6qv w M H log H Umax Smax log H h h H qv Sh Smax maxhH Sh α minimum value obtained line 9 C Kang et al Artiﬁcial Intelligence 239 2016 7096 81 Algorithm 3 Init Input A social network S V E VL ω pinterfered predicate set Icid2p diffusion hypergraph H cid11N H W cid12 Output M1Icid2p1V U 1Icid2p1V Heap 1 n Icid2p m V 2 M1n1m U 1n1m 3 Heap 4 v V q Icid2p Mqv 0 U qv 5 6 v V q VLv Icid2p 7 8 h H 9 10 11 12 return cid11M U Heapcid12 Add cid11h W h Mcid12 Heap qv Sh U qv U qv h Mqv 1 Algorithm 2 HyperDC Input An SN S V E VL ω GAP cid2 property p diffusion hypergraph D HS cid2 p cid11N H W cid12 vV F pv Minpv 1 cid12 InitS Icid2p D h U pv Heap copy Heapin p VLv Minpv 0 Output cid11v dcvcid12 v V 1 cid11Min U Heapin 2 Heap copy Heapin 3 F HyperLFPU Min Heap D cid8 4 sump 5 Re sult 6 v V 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 return Result remove cid11h wcid12 Heap cid6 th Let p W h Min Minp cid6 Add cid11h W h Mincid12 Heap M HyperLFPU Min Heap D cid6 sum p p VLv dcv sump F pv sum Minpv 1 dcv sum Minpv 0 Add cid11v dcvcid12 Result vcid6V vcid6cid17v Mpv cid6v cid6v cid6 cid8 cid6 p sump F pv cid6 p 53 The HyperDC algorithm The HyperDC algorithm Algorithm 2 initializes Min U Heapin calling Algorithm 3 line 1 uses compute lfpcid2 cid2S lines 24 After diffusion centrality vertex computed lines 645 Speciﬁcally value atom pv incorporated Min Heap updated accordingly lines 716 Then ﬁxed point computed updated Min Heap lines 1718 Finally diffusion centrality vertex v computed cid6 positive value pv Min restored lines 1925 In step Proposition 1 leveraged In fact p VLv summand deﬁnition DC computed lines 24 computed negative cid6 summand In case positive summand equal sump F pv negative summand equal sum p cid6 negative summand computed cid6 diffusion centrality vertex v sump F pv sum p If p VLv cid6 sump F pv lines 24 In case diffusion centrality vertex v sum p Proposition 6 The worstcase time complexity Algorithm HyperDC O V N 1 α Umax maxvV h h H v Sh Smax maxhH Sh α deﬁned Proposition 5 H log H Umax Smax log H A brief note order techniques section yield better scalability Compared past work 31 HyperDC approximately 100 times faster average speedup obtained experimental evaluation In particular U Heapinit structures HyperDC built This yields speedup approximately 5 In addition ﬁltering based Proposition 3 yields speedup 20 leading total speedup 100 6 CBAF algorithm approximating kDCP The goal section develop Coarsened Back Forth CBAF algorithm approximately compute topk diffusion centrality vertices huge social networks feasible compute DC vertices The basic idea coarsen original social network S smaller network S cid6 tries preserve diffusive behavior S The topk vertices computed S cid6 solution mapped subgraph S compute topk vertices Given social network S GAP cid2 property p CBAF performs following steps 1 Compute ﬁltering S cid6 merging vertices obtain new social network SC mapping vertices S cid6 vertices SC 3 Compute set T C topk vertices SC 4 Use T C compute approximate set topk vertices S The ﬁrst step described Section 51 steps detailed rest section S 2 Coarsen Scid6 82 C Kang et al Artiﬁcial Intelligence 239 2016 7096 61 Social network coarsening This section proposes new semantical coarsening technique reduces network size merging vertices similar trying preserve structural semantic properties network wrt property p The coarsening process involves following issues select similar vertices merged ii assign properties merged vertex edges merging iii compute edge weights merged vertices These issues addressed following Vertex similarity CBAF work function determine vertices similar Throughout paper assume given vertex v set SIMv V vertices similar Function SIM obviously deﬁned ways We provide way takes diffusion model p account social network structure Consider SN S GAP cid2 property p Given vertices u v S write u v iff u v activate set rules cid2 cf Deﬁnition 2 notion activation Using equivalence relation deﬁne SIMcid2v u V u v Obviously deﬁnitions possible experiments Vertex merging We deﬁne merge similar vertices When set vertices merged new vertex v specify vertex properties v associated edge propertiesweights Deﬁnition 9 Vertex properties merging Let S SN v 1 vn subset vertices S merged For p VP let g p 0 1n 0 1 associative commutative function Then deﬁne mergeVPv 1 vn VL p VP g pϑv 1 p ϑvn p 1 ϑv p 1 p VLv ϑv p 0 The deﬁnition assumes existence function g p takes values 0 1 pproperty vertices merged combines single 0 1 value denoting merged vertex property p Some examples function g p computing property intersection union taking minimum maximum value xi s We use majority function new vertex property p majority vertices merged property p We address problem assigning edges new merged vertex Let S V E VL ω SN V cid6 cid6 For edge v u ep E v V v 1 vn subset V merged new vertex v u V V edge v v cid6 u V cid6 ep u V new vertex v v V V cid6 cid6 cid6 cid6 cid6 incoming outgoing edge v cid6 u ep v V cid6 Edge weighting We deﬁne assign weight arbitrary set edges having label S V E VL ω Deﬁnition 10 Edge weighting Let S V E VL ω SN ep EP Moreover let e1 em V V ep arbitrary set edges vertices V labeled ep gep 0 1m 0 1 associative commutative function Then deﬁne weighte1 em gepω e1 ω ωe ωe e E ωe 0 em cid6 Given sets vertices V V possible edges having property ep exist vertices V cid6cid6 Finally v v new incoming edge weight computed weightpossEdgesv V outgoing edge weight computed weightpossEdgesV vertices V cid6 new vertex obtained merging set V cid6 ep e edge property ep EP deﬁne set possEdgesV possEdgesV vertices e cid6cid6 v cid6 v ep cid6cid6 ep v cid6cid6 V cid6 V cid6 v cid6 v cid6cid6 cid6 cid6 cid6cid6 ep set cid6cid6 ep cid6 V cid6 v v cid6 ep cid6 v ep new cid6 V cid6cid6 Social network coarsening We ready deﬁnition social network coarsening Deﬁnition 11 Social network coarsening Let S V E VL ω SN θ real number 0 1 called contrac tion factor A coarsening S SN S cid6 V cid6 ωcid6 mapping π V V cid6 VL cid6 E st cid6 cid6 θ V V cid6 cid11π v 1 π v 2 epcid12 cid11v 1 v 2 epcid12 E π v 1 cid17 π v 2 E cid6v VL ωcid6e cid6 mergeVPv V π v v cid6 cid6 weightpossEdgesπ 1v 1 π 1v 2 ep e cid6 VL v cid6 V cid6 cid11v 1 v 2 epcid12 E cid6 C Kang et al Artiﬁcial Intelligence 239 2016 7096 83 Algorithm 4 CoarsenSN Input A social network S V E VL ω GAP cid2 contraction factor θ 0 1 similarity function SIM neighbors threshold ρ 0 1 vertex properties merging function mergeVP edge weight function cid6 VL cid6 ωcid6 π V V cid6 S cid6 ωcid6 V E VL ω cid6v mergeVPv M VL cid6 cid6 π 1v cid6 M Randomly select vertex v R M getMergingSetv Scid6 cid2 SIM ρ R R M v M cid17 cid6 θ V cont cid6 cid6 E cid6 VL cont true Output A coarsening Scid6 V 1 Scid6 V cid6 E 2 π v v v V 3 R V 4 V 5 6 7 8 9 10 11 12 13 14 15 16 return cid11Scid6 π cid12 17 getMergingSetv Scid6 cid2 SIM ρ 18 19 20 21 cid6 V cid6 ωcid6cid12 UpdateEdgesE cid6 v v cid6 v v cid6 M R cont f alse VL π v π v V cid11E cid6 U SIMv U neighbors vertex v Scid6 U Randomly select set M U return M cid6 cid6 ωcid6 v M weight π size M ρ U cid6 22 updateEdgesE 23 24 25 26 cid6 ω v M weight π cid6cid6 cid11π v 1 π v 2 epcid12 cid11v 1 v 2 epcid12 E E e ωcid6e return cid11E cid6 cid11π v 1 π v 2 epcid12 E cid6 weightpossEdgesπ 1v 1 π 1v 2 ep cid6cid6 ωcid6cid12 cid6 π v 1 cid17 π v 2 cid6cid6 Coarsening SN S yields new SN S cid6 number vertices S cid6 u v S u merged new vertex u edge u function mergeVP iv weights edges S cid6 denote π 1v cid6 set v V π v v v cid6 cid6 cid6 mapping π vertices S vertices S cid6 smaller S factor θ ii edge vertices Scid6 cid6 cid17 u assigned iii vertex properties merged vertex S cid6 assigned function weight In following given v v merged new vertex v Scid6 Scid6 cid6 V cid6 cid6 cid6 Algorithm 4 general algorithm coarsening social network It starts initializing mapping function π identity function In iteration vertex v current SN randomly selected set vertices merged vs neighbors similar computed function getMergingSet This function computes set U v according similarity function SIM received input returns subset M U size percentage ρ cid6 If set vertices M vertex v merged M new vertex v randomly selected If U v merged M new vertex properties v computed mergeVP function mapping π updated set edges updated function UpdateEdges This function ﬁrst computes new set edges edge vertex M v vertex u edge v u new edge weight computed function weight received input The algorithms iterations stop cid6 θ contraction factor establishing number V desired size coarsened network possible merge vertex Algorithm 4 similarity function SIMcid2 cid6 vertices current SN equal θ V cid6 cid6 62 Adapting DC deﬁnition coarsened network We adapt diffusion centrality case coarsened networks This intermediate step later computation diffusion centrality vertices original network When set vertices M original network S represents network Svcid6 consisting vertices M merged single vertex v edges original network Thus vertices v diffusion centrality coarsened network actual diffusion property original network vertices belonging Svcid6 Svcid6cid6 different depends properties subnetworks Svcid6 Svcid6cid6 number vertices coarsened network representing edges To account assign weight vertex v importance v wrt original network coarsened v v cid6cid6 cid6 cid6 cid6 cid6 cid6 Deﬁnition 12 Diffusion centrality coarsened network Let S social network SC V C E C VLC ωC coarsening S vertex mapping π mv w function assigning weight vertex V C Then diffusion centrality vertex v coarsened SN deﬁned 84 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Algorithm 5 CBAF Approximate Topk Input An SN S V E VL ω GAP cid2 property p integer k set options opts described Table 2 cid6 E cid6 VL cid6 ωcid6 net workF ilteringS cid2 p Output An approximate set topk vertices 1 Scid6 V 2 cid11SC V C E C VLC ωC π cid12 CoarsenS NScid6 cid2 opts 3 T C computeT op K SC k mv w Use HyperDC 4 T C T C nbrsT C dC SC 5 V I π 1T C nbrsπ 1T C dI S 6 SI V I E I VLI ωI SN induced S vertices V I 7 return computeT op K SI k mv w 0 Table 2 Input options opts Algorithm 5 θ 0 1 SIM ρ 0 1 mergeVP weight contraction factor similarity function neighbors threshold vertex properties merging function edge weight function mv w dC dI merged vertex weight function neighbors distance extending set T C neighbors distance extending set π 1T C cid6 dc cid2pSC v cid12vcid6V C v cid6 cid10 mv wv cid10 mv wv lfpcid2 cid2SC pvpv cid6cid6 lfpcid2 cid2SC cid16pvpv cid6 cid11 cid11 cid6cid6 cid12vcid6cid6V C v Function mv w merged vertex weight function deﬁned ways Below provide alternative deﬁnitions Consider SN S V E VL ω let SC π coarsening S Given vertex v SC mv w 0v 1 In case original deﬁnition DC mv w 1v V v E v V v 1 V v π 1v E v cid11v 1 v 2 epcid12 v 1 v 2 V v cid11v 1 v 2 epcid12 E Thus weight v given number vertices original SN merged v multiplied density Sv The idea Sv high density vertices weight v high mv w 2v lnmv w 1v 1 Here consider fact diffusion property p rapidly decrease V v 2V v E v according distance vertices V v diffusion source vertex v 63 CBAF approximately solving kDCP problem In section approximately compute topk diffusion centrality vertices social network First introduce deﬁnitions Deﬁnition 13 Induced social network Given SN S V E VL ω SN induced S set vertices V I V SI V I E I VLI ωI E I cid11v 1 v 2 epcid12 cid11v 1 v 2 epcid12 E v 1 v 2 V I VLI v VLv v V I ωI e ωe e E I Given SN S V E VL ω set vertices T V positive integer d denote nbrsT d S set neighbors vertices T distance greater d If d 0 nbrsT d S line 1 Then Scid6 We ready present CBAF algorithm shown Algorithm 5 CBAF takes input social network S GAP cid2 property p integer k set options opts described Table 2 It returns approximate set topk diffusion centrality vertices S The ﬁrst step algorithm ﬁlters original SN S removing unnecessary vertices obtaining network S cid6 coarsened smaller social network SC line 2 exact set T C topk vertices SC computed line 3 running HyperDC ﬁnd diffusion centrality vertices Scid6 choosing topk At point set T C extended neighbors SC distance greater dC order limit bias vertices T C following computation line 4 Next set vertices mapped vertices original SN S extended neighbors S distance greater dI obtaining set vertices V I line 5with slight abuse notation use π 1T C denote vT C π 1v After social network SI induced vertices V I S computed line 6 algorithm returns approximate set topk vertices S exact set topk diffusion centrality vertices computed SI line 7 Note CBAF ﬁrst evaluates diffusion centrality vertices coarsened network SC typically small diffusion centrality vertices subgraph original network corresponding neighborhood vertices associated solution line 3 CBAF algorithm This typically small fraction vertices original social network S C Kang et al Artiﬁcial Intelligence 239 2016 7096 85 Table 3 Nongame social networks experiments Network BlogCatalog email Enron Douban wikiVote socEpinions emailEuAll Description Friendship email communications Friendship Wikipedia vote relationships Trust relationships email communications Type Undirected Undirected Undirected Directed Directed Directed Vertices 10312 36692 154907 7115 75879 265214 Edges 333983 183831 654188 103689 508837 420045 Table 4 STEAM networks experiments Social network GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 GAME420 GAME220 Vertices 4083 28872 52879 66571 82377 89942 122467 1307335 2030579 Edges Avg degree 21447 115254 164702 303240 437554 327258 441657 12431564 20596331 525 399 311 456 531 364 361 951 1014 Density 628E03 273E04 545E05 205E03 884E05 597E06 Avg degree 6478 1002 845 1457 671 158 Density 129E03 138E04 589E05 684E05 645E05 405E05 294E05 727E06 500E06 7 Experimental evaluation This section contains detailed report experiments 1 Exact Computation HyperDC We compared runtime spread generated diffusion centrality clas sical centrality measures Section 72 networks 265K vertices 440K edges 2 Approximate Computation CBAF We tested scalability runtime spread CBAF networks 2M vertices 20M edges Section 73 3 Comparing spread memes high DC vertices low DC vertices In order test diffusion centrality captured real spread ran test MemeTracker data knew initiated meme We tested hypothesis vertices high centrality inﬂuential low centrality according diffusion centrality classical centrality measures KolmogorovSmirnov tests validate ﬁndings high DC vertices diffuse memes better low DC vertices ii diffusion centrality better job explaining real meme diffusion classical centrality measures We implemented HyperLFP Algorithm 1 HyperDC Algorithm 2 CBAF Algorithm 5 Java To compute degree eigenvector PageRank closeness betweenness centrality Java Universal NetworkGraph Framework JUNG3 All experiments run Intel Xeon 240 GHz 24 GB RAM 71 Experimental setup Social networks Our experiments realworld social networks summarized Table 3 The networks taken Stanford Large Network Dataset Collection 38 We considered additional online game dataset called STEAM 6 contains friendship relations represented directed edges players vertices Each player vertex properties selected country groups games played total time played game We extracted 10 subnetworks STEAM dataset choosing different games selecting game players played edges players The features extracted networks reported Table 4 Diffusion models We ran experiments conditional probability model referred Flickr model 13 JacksonYariv tipping model 28 SIR model disease spread 2 We generalized diffusion models adding additional condition qu μ rule bodies When vertices property q original diffusion models The qcondition determines diffusion process happen More speciﬁcally Flickr model vertices satisfying property q spread diffusive property JacksonYariv SIR models vertices satisfying property q diffusive property In Flickr model q represent willingness person share herhis information inﬂuence people In JacksonYariv SIR models q represent precondition vertex adopt behavior disease Thus property q useful expressing fact vertices characteristics modeled q infect infected diffusive property Note edges originating nodes property q play role diffusion depending diffusion model In experiments compared DC classical centrality measures varying percentage vertices network property q The diffusion models reported Appendix A 3 jungsourceforgenet 86 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Fig 3 STEAM data Runtimes ms averaged vertex δq 1 2 3 4 5 10 15 20 25 30 72 Diffusion centrality vs classical centrality measures As described earlier Flickr JacksonYariv SIR models assume set vertices network property p vertices satisfying property q spread case Flickr model receive case JacksonYariv SIR models p For STEAM data able use known properties vertices q able datasets Before getting details experimental evaluation summarize high level conclusion com parative experiments experiments scalability CBAF algorithm subsection runtime HyperDC better betweenness closeness centrality comparable ii spread achieved diffusion centrality better achieved classical centrality measures 721 STEAM data We large STEAM data sets Table 4 comparative analysis classical centrality measures ﬁnish computation ﬁrst seven STEAM data sets Consistent data set percentage δp vertices initially property p 0 varied percentage δq vertices having property q real properties vertices STEAM data In STEAM data q taken playing time users game assigned follows sorted players descending order according playing time assigned q ﬁrst δq users We varied δq 1 2 3 4 5 10 15 20 25 30 The STEAM subnetworks substantially larger evaluation CBAF algorithm cf Section 73 Runtime We compared time compute DC wrt diffusion models time compute classical centrality measures Fig 3 shows runtimes vary wrt δq representative STEAM games Of 7 STEAM games tested experiment closeness centrality computation ﬁnished smallest case GAME8690 HyperDC faster betweenness closeness centrality STEAM networks values δq Even δq 30 runtime HyperDC low HyperDC faster PageRank networks GAME8690 GAME50510 smallest STEAM networks Flickr JacksonYariv models faster degree network GAME50510 δq low4 However number vertices increases STEAM datasets GAME8690 GAME50510 HyperDC slower PageRank degree diffusion models For instance GAME1500 crossover occurs δq 3 SIR model JacksonYariv model There main reasons size Icid2p larger SIR model recall smaller Icid2p effective optimization Proposition 3 ii SIR model pmonotonic pdwindling optimizations effective cf Section 51 Runtime HyperDC tends linear We note Proposition 6 worst case complexity HyperDC nonlinear In order assess actual runtime characteristics HyperDC ran experiment STEAM data number vertices increases δq varies We networks GAME 50510 6850 11450 17300 similar average degree 34 range Fig 4 shows number vertices network xaxis average runtime vertex yaxis Different curves varying values δq range 530 We irrespective diffusion model value δq HyperDC runs linear time practice 4 Though surprising HyperDC beats degree centrality reason δq low 3 HyperDC needs compute diffusion centrality small number vertices However δq larger case C Kang et al Artiﬁcial Intelligence 239 2016 7096 87 Fig 4 STEAM data Average runtime vertex HyperDC number vertices increased different δq Flickr JacksonYariv SIR models Table 5 Spread STEAM networks different δq values Model Network STEAM data average ratio spread generated diffusion centrality best spread generated classical centrality measures different δq 1 3 2 5 4 FKModel JYModel SIRModel GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 Average GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 Average GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 Average 194 181 4366 6776 1256 5120 2887 2969 28 110 81 88 54 109 90 80 12 101 49 57 20 67 43 50 24 225 6542 899 65 5855 3635 2464 17 50 58 67 33 77 45 50 08 26 36 33 14 44 21 26 20 238 116 483 61 8043 66 1290 14 51 49 46 28 62 24 39 08 30 28 22 15 27 10 20 18 265 104 499 63 89 67 158 12 43 45 37 26 51 22 34 08 22 26 24 15 17 11 18 19 195 107 466 57 97 68 144 13 40 36 36 22 42 22 30 08 22 25 21 14 18 12 17 10 19 50 81 488 49 69 54 15 16 36 64 500 31 55 51 20 15 30 56 562 23 51 44 25 14 27 41 555 19 49 37 30 13 24 32 31 17 37 28 116 108 112 106 12 27 25 23 16 29 16 21 08 14 18 17 11 14 07 13 11 19 21 20 14 22 12 17 09 11 17 12 08 11 06 11 11 15 18 19 13 19 12 15 09 10 14 11 08 11 06 10 11 11 17 17 13 17 12 14 08 09 13 10 08 10 06 09 26 12 10 16 09 13 16 12 13 08 09 12 08 08 09 06 09 Higher values δq lead higher running times vertices property q diffusion unfolds HyperDC takes time converge ﬁxed point In real scenarios information spreading limited individuals close proximity 13 Spread We compare diffusion property p choose topk central vertices according DC vs classical centrality measures STEAM data In case topk vertices called seeds We vary k 10 100 steps 10 The spread wrt given set seeds expected number vertices property p diffusion assuming seeds property p minus expected number vertices property p diffusion original social network This difference normalized By expected number vertices property p SN S diffusion vV lfpcid2 cid2S pv V set vertices S Our spread experiments presented cid2 mean ways In ﬁrst spread varies averaging number seeds ﬁxed δq values In second opposite Spread experiments averaged varying k values speciﬁc δq values We varied δq set 1 2 3 4 5 10 15 20 25 30 For selection δq considered value k set 10 20 100 Then ﬁxed δq k DiffusionModel triple computed ratio spread DC best spread achieved classical centrality measures Table 5 reports average ratios Thus ratio greater 1 shows DC achieves higher spread classical centrality measures cid8 For Flickr JacksonYariv models DC achieved better spread classical centrality measures For SIR model average DC achieves better spreads classical centrality measures long δq 15 20 2530 classical centrality measures achieve better spread 88 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Table 6 STEAM data Average ratio spread generated diffusion centrality best spread generated classical centrality measures different k values Model Network Average ratio DC spread best spread centrality measures different k values 90 50 10 60 70 80 40 20 30 FlickrModel JYModel SIRModel GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 Average GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 Average GAME8690 GAME50510 GAME6850 GAME1500 GAME24420 GAME11450 GAME17300 Average 165 481 414 3640 482 709 633 932 19 33 26 31 22 30 21 26 12 21 22 25 14 21 13 18 21 68 673 943 722 1164 1010 657 15 36 33 33 22 43 19 29 08 20 24 24 14 28 12 19 22 90 861 584 44 1551 1348 643 13 39 37 39 22 47 25 32 08 24 25 26 14 24 13 19 21 105 1012 678 52 1877 1640 769 12 37 37 39 23 45 28 32 08 24 24 22 13 23 14 18 21 77 1149 742 55 2168 1878 870 13 41 38 40 24 47 28 33 08 28 24 22 13 22 13 19 20 86 1279 810 57 2415 80 678 14 37 35 37 24 45 28 31 08 25 22 21 11 22 12 17 20 92 1385 883 55 2652 80 738 14 39 38 37 25 46 30 33 08 27 23 21 11 21 13 18 21 98 1496 935 57 2878 85 796 14 40 40 36 24 48 30 33 08 30 25 19 11 22 13 18 21 103 1578 999 60 3100 89 850 14 37 42 35 24 48 29 33 09 27 25 18 10 22 13 18 100 21 72 1662 1047 57 953 94 558 14 38 40 35 23 46 30 32 09 27 24 18 10 22 14 18 Not surprisingly spread ratio decreases δq increases δq large vertices infected regardless seeds chosen difference DC centrality measures decreases Interestingly spread ratios Flickr model large compared JacksonYariv SIR models This expected number vertices property p diffusion higher case Flickr model cases Spread experiments averaged varying δq values speciﬁc k values Here selected values k averaged different possible values δq drawn set 1 2 3 4 5 10 15 20 25 30 Table 6 summarizes results As previous case ratio exceeding 1 implies DC outperforms classical centrality measures On average values k diffusion models DC achieves better spread classical centrality measures exceptions SIR model Interestingly spread ratio GAME8690 consistently lowest according diffusion models setting Tables 5 6 This game 4083 vertices small dense Our choice q based playing time player strong correlation number friends As consequence having multiple seeds greatly increase diffusion property p overlaps vertices inﬂuenced seeds 722 Nongame social network data In section perform experiments similar reported networks Table 3 For networks data associated vertex properties We looked cases Case 1 We randomly selected δp 01 vertices synthetic property p 5 runs In run varied δq 1 2 3 4 5 10 15 20 25 30 selected δq vertices synthetic property q Case 2 We randomly selected δq 3 vertices varied δp 01 02 03 04 05 associating synthetic properties p q vertices Recall mentioned vertices qproperty play role spreading p depending diffusion model5 Runtime Case 1 Fig 5 shows runtime varies wrt δq Case 1 The networks sorted left right according number vertices directed networks ﬁrst row undirected networks second row As 5 For instance consider SN vertices v 1 v 2 v 3 v 4 edges v 1 v 2 v 2 v 3 v 3 v 4 v 4 property q Consider diffusion model saying X property p X connected Y Y connected Z Z connected W W property q gets property p If instance v 1 property p v 4 gets p Notice vertices connections play role So instance delete v 2 edge v 2 v 3 v 2 property q lose real behavior discussed C Kang et al Artiﬁcial Intelligence 239 2016 7096 89 Fig 5 Nongame SNs Runtime vertex varying δq 1 30 δp 01 case experiments STEAM data closeness betweenness centrality time consuming compared algorithms including HyperDC HyperDC Flickr model faster PageRank majority cases considered faster eigenvector centrality HyperDC SIR model faster PageRank directed networks δq 4 As case STEAM data described earlier network ﬁltering step eliminate vertices For Flickr SIR models runtimes increase slightly δq increases higher diffusion takes place However case STEAM data HyperDC JacksonYariv model takes time By creasing percentage vertices having property q computing diffusion centrality JacksonYariv model slower diffusion models diffusion process determined sum neighbors diffu sion probability 01 vertices initially property p Thus time vertexs probability updated neighbors q updated step Case 2 Fig 6 shows runtime varies δp varies The runtime closeness betweenness centrality high worse 1 3 orders magnitude report runtimes HyperDCs runtime Flickr SIR models vary δp HyperDC Flickr model faster PageRank networks settings Compared degree centrality HyperDC Flickr exhibits competitive runtimes faster half settings considered comparable slightly worse HyperDC SIR model faster PageRank data sets Douban data set As case STEAM data HyperDC JacksonYariv worst performer wrt runtime excluding betweenness closeness centrality eliminated earlier high running times computation diffusion centrality JacksonYariv model slower vertices diffusion property p Spread In Cases 1 2 average experimental results showed ratio spread generated HyperDC spread generated best classical measure exceeds 1 As case STEAM data best ratios Flickr model 73 CBAF algorithm performance experiments In section experiments performed compare CBAF HyperDC terms runtime spread Simply experiments CBAF achieves spread HyperDC runtime lower correct choice settings HyperDCmoreover takes half runtime HyperDC Input options CBAF There number input options CBAF inﬂuence performance cf Table 2 In order ﬁnd best possible input options ran extensive experiments considered 972 different candidate option sets determined candidate values reported Table 7 Each option set tested 5 times 90 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Fig 6 NonGame SNs Runtime vertex varying δp 01 05 δq 3 Table 7 Candidate best values input options CBAF Possible values Option dC dI weight mergeVP mv w ρ 0 1 2 0 1 2 min max average union intersect majority mv w 0 mv w 1 mv w 2 One vertex 10 50 100 Best input options O P 1 1 0 max majority mv w 1 100 O P 2 1 0 max intersect mv w 1 100 O P 3 1 0 average majority mv w 0 100 random component algorithm GAME17300 network Candidate option sets compared basis running time quality results measured terms spread recall Kendall Spearmans rank correlation coeﬃcients The option sets achieved good balance runtime spread reported Table 7these options experiments CBAF Speciﬁcally dC 1 dI 0 showed better running times values little difference terms quality weight min disregarded computing time slightly better max average quality worse running times mergeVP union mv w mv w 2 worse values qualities similar ρ 100 showed slightly higher running times better quality values In experiments vertex similarity function SIMcid2 deﬁned earlier The contraction factor θ chosen set 02 03 04 CBAF evaluation measures We measures evaluate CBAF The time ratio denoted ratiotime simply ratio time taken CBAF vs HyperDC The spread ratio denoted ratiospread ratio spread according CBAF assuming topk vertices property p vs according HyperDC We compared HyperDC CBAF largest STEAM networks GAME420 13M vertices 1243M edges GAME220 203M vertices 2059M edges 2 largest nongame networks EmailEu Douban Flickr JacksonYariv SIR models best option sets Table 7 In experiments set δp 0 δq 5 For EmailEu Douban networks set k 100 θ 04 03 02 For STEAM networks ran experiments θ 04 k 130 650 1300 GAME420 k 200 1000 2000 GAME220 Runtime Option O P 3 consistently yielded fastest runtimes CBAF results reported Table 8 present results network GAME220 largest results GAME420 similar ones GAME220 The time ratios networks diffusion models exception JacksonYariv model huge network GAME220 k 2000 In particular C Kang et al Artiﬁcial Intelligence 239 2016 7096 91 Table 8 Results Douban EmailEu GAME220 networks Option set O P 3 Douban network Model θ EmailEu network GAME220 network Flickr JY SIR 04 03 02 04 03 02 04 03 02 ratiotime 063 062 064 093 091 091 046 048 047 ratiospread 096 097 097 093 094 094 099 099 099 ratiotime 083 082 083 052 055 066 072 076 076 ratiospread 061 067 062 082 082 083 090 089 091 k 200 1000 2000 200 1000 2000 200 1000 2000 ratiotime 052 045 060 095 095 104 066 072 064 ratiospread 099 099 094 097 100 100 097 098 100 Douban network SIR model time ratio 50 delivering perfect spread ratio 099 On EmailEu network JacksonYariv model runs 50 time taken HyperDC On huge GAME220 network Flickr model runs 60 time taken HyperDC CBAF work JacksonYariv model huge networks high average degree networks Spread In cases options yield approximately spread In huge networks Douban network spread ratio close On EmailEu network spreads range 0809 JacksonYariv SIR models However Flickr model spread lower 0607 range The reason EmailEu network low average degree leads merged vertices coarsened networks representing small sets vertices original network induced network small representative compute approximate topk vertices The performance CBAF wrt runtime spread depends input options In general observed aggressive coarsening step faster algorithm quality spread gets worse In step computing induced SN add neighbors quality gets better running times higher 74 Testing quality DC MemeTracker data We tested quality diffusion centrality real context memes diffusion Web We MemeTracker data6 consisting set 172M news articles blog posts 1M online sources collected September 1 2008 till August 31 2009 For articlepost dataset contains timestamp phrases contained document hyperlinks In addition phrases clustered information available data We considered phrases cluster meme We built network raw phrase data vertices online websites edges hyperlinks More speciﬁcally selected vertices 10000 sites wrt number hyperlinks inserted direct edge u v sites u v webpage site u having hyperlink webpage site v Our aim diffusion centrality correlates spread generated vertices vertices high diffusion centrality spread vertices low diffusion centrality To restricted attention source vertices online websites ﬁrstly showed meme m computed actual spread number websites infected m We assumed site u infected site v meme m edge u v m appeared u time t1 v time t2 t1 t2 When webpages belonging website u infected meme m consider smaller webpage timestamp timestamp infection u Moreover analysis focused 5000 spread memes We assumed memes diffusion websites described cascade diffusion model estimated parameters actual spread memes identiﬁcation actual model memes diffusion scope paper By MemeTracker data diffusion model performed set experiments follows We use kfold cross validation temporal data considered time window t1 t2 determine trainingtesting data moved steps day size days assumed values set 30 60 90 For time window considered data timestamp ﬁrst 80 days training set estimate parameter diffusion model test set For meme m test set computed tuple m v c s v source m c centrality value computed diffusion centrality PageRank degree betweenness closeness centrality s actual spread m Fig 7 left shows distribution centrality values DC PageRank closeness betweenness degree centrality distribution actual spread memes dataset considering sliding time window size 30 days The plot Fig 7 right shows value distance measured KolmogorovSmirnov 6 http wwwmemetrackerorg data html 92 C Kang et al Artiﬁcial Intelligence 239 2016 7096 Fig 7 Left Actual spread centrality measures distributions Right KolmogorovSmirnov statistic actual spread distribution centrality measures distributions left plot statistic actual spread distribution distribution PageRank closeness diffusion betweenness degree centrality The ﬁgure shows distribution DC values closest actual spread distance smallest This result intuitively suggests initiator high value diffusion centrality likely reach high value spread low value diffusion centrality hold centrality measures 75 Summary experiments In section conducted experiments The ﬁrst compares HyperDC classical centrality measures The experiment shows The runtime HyperDC better betweenness closeness centralities comparable centrality measures The spread achieved diffusion centrality better achieved classical centrality measures The second experiment compares HyperDC CBAF We CBAF achieves spread HyperDC runtime lower correct choice settings HyperDC Recall experimental setup considered initial distribution property q models characteristic vertex infect infected diffusive property As discussed running times increase percentage δq vertices having property q increases This diffusion unfolds vertices having property q means time compute DC increases Nevertheless HyperDC showed good performances values δq 30 Lower percentages vertices having property q essentially mean propagation unfolds ﬁxpoint HyperDC reached sooner However worth mentioning case real scenarios For instance 13 analyzed propagation Flickr social network popular photos substantially limited popularity outside immediate network neighborhood uploader The runtime DC expected higher complex diffusion models A structural analysis corre sponding GAPs provide insights expect diffusion model terms running time evaluate In regard subclasses GAPs optimizations introduced Section 5 useful tools For instance GAP pmonotonic andor pdwindling lower running times apply optimizations HyperLFP converges quickly Another useful parameter analyze size Icid2p roughly speaking consists predicate symbols interfering p expect better performances Icid2p small These analyses useful practice tune diffusion model ﬁnd good balance accurateness model time evaluate For instance want simplify considered diffusion model pmonotonic sacriﬁcing accurateness describing diffusion process obtaining eﬃcient evaluation To strengthen claims advantage DC predicting spread initiated given vertices MemeTracker data diffusion occurred naturally intervention The parameters diffusion model estimated data making sure separate learning testing data The results experiment showed correlation DC values actual diffusion memes high diffusion centrality vertices diffuse memes better low diffusion centrality vertices It showed diffusion centrality better predictor real meme diffusion classical centrality measures 8 Conclusion Centrality importance social networks closely interlinked concepts Central vertices assumed impor tant vice versa However realworld online social networks people considered important authoritative topics considered unimportant For instance inﬂuential sports commentator likely inﬂuential movie critic Moreover importance individual measured inﬂuence related fac tor A person inﬂuence 1000 people movies considered important regard movies inﬂuence 500 Diffusion models mechanism capture spread concepts phenomena C Kang et al Artiﬁcial Intelligence 239 2016 7096 93 network Many diffusion models developed successfully predict extent spread memes concepts networks In paper propose novel concept Diffusion Centrality computed respect wide array diffusion models 51 shows framework generalized annotated programs GAPs express known diffusion models In paper increase breadth knowledge GAPs introducing novel speciﬁc classes GAPs present HyperDC algorithm exactly computes topk diffusion centrality vertices wrt GAPexpressible diffusion models In addition present novel approximate Coarsening Back Forth CBAF algorithm allows huge social network reduce manageable size eﬃciently solve approximate way problem ﬁnding topk vertices We conduct detailed experimental study realworld social networks A ﬁrst set experiments compares runtime spread generated HyperDC algorithm classical centrality measures Our results HyperDC eﬃcient produces better spread current centrality measures A second set experiments looks scalability CBAF showing lower runtime HyperDC achieving high spreads In particular CBAF tested networks 2M vertices 20M edges achieved acceptable runtime Using MemeTracker data diffusion centrality captures importance people truly responsible spread meme effectively past centrality measures This work opens dramatic new set possible diffusion models automatically learned data account rich semantics associated vertices edges modern social networks like Twit ter Facebook LinkedIn For instance case predicting election outcomes Twitter data 30 learn rules identify likelihood person P vote candidate C taking gender demographic fac tors tweets tendencies neighbors vote C Alternatively look diffusion banking crises worlds nations considering network ﬂows exposuresthe nodes network countries edges labeled exposure country 44 High exposure country A country B suggest systemic banking crisis country B lead default triggering systemic bank ing crisis country A Clearly factors considered For instance political factors represented semantic properties vertices capture likelihood country A taking intelligent steps forestall crisis Mutual trust property edge determine A B work address problem Such diffusion model use rich semantic opportunities offered research knowledge representation GAPs paradigms learn ﬁnegrained diffusion models relatively coarse grained diffusion models developed past We believe form rich line inquiry future Acknowledgements Some authors partly supported ARO grants W911NF11103 W911NF1410358 W911NF09102 Mafat Israel Science Foundation grant 148814 Appendix A Diffusion models Below provide details diffusion models experimental evaluation The Flickr model consists following rule 51 pv μvcid6v μp μq dp F ev cid6 v μvcid6v pv cid6 μp qv cid6 μq cid6 saying vertex v properties q p diffuse property p neighbor v The value dp F constant representing probability vertex v receive property p This model falls category cascade models The JacksonYariv model diffusion model stating vertex receive adopt property p according cumulative effect neighbors ratio beneﬁt cost vertex adopting p Suppose v agent having default behavior changed new behavior p v speciﬁc cost ci beneﬁt bi adopting behavior p Then JacksonYariv model expressed rule 51 pv bi ci cid8 r j E j cid8 j w jcid8 j E j wq dp J Y cid12 cid10 ev j v E j pv j w j cid11 qv wq v j v j v E cid8 1 r j E j function describing number neighbors v affects beneﬁts v adopting cid8 j w jcid8 fraction neighbors v having property p 3 dp J Y constant representing behavior p 2 j E j j E j logarithmic function probability vertex v adopt property p In experiments set r max network having values interval 01 2 normalized logarithm maximum indegree din 01 When annotation μ atom pv v V greater 1 r set μ 1 Moreover observe vertex v adopt property p property q For STEAM data j E j 19 ln j E j max lndin cid8 cid8 cid8 94 C Kang et al Artiﬁcial Intelligence 239 2016 7096 1 vertices NonGame networks randomly assigned bi ci vertices according normal set bi ci distribution 05 bi ci 15 The SIR model classic disease model labels vertex susceptible disease receive neighbors infectious caught disease t units time expired recovered vertex longer catch transmit disease The diffusion rules following 51 pv 1 R μe riv μr v r1v μp v μp vcid6v ri1v μr pv μp v vcid6 1 R v 2 t cid6 μq v dp S I R rt v R ev cid6 v μe vcid6v pv cid6 μp vcid6 rt v cid6 R cid6 qv μq v Here vertices having property q susceptible diffusion property p represents vertex infected Properties ri cid17 t express vertex infectious state time t 1 rt means vertex recovered In experiments set t 2 The constant dp S I R probability vertex v infected The SIR model falls category cascade models We conclude section showing wellknown linear threshold model expressed GAPs Recall linear threshold model vertex v inﬂuenced neighbor w according weight b wv bwv 1 Furthermore vertex v threshold θv 0 1 weighted fraction vs neighbors cid8 w neighbor v active order v active v active cid8 bwv θv The GAP w active neighbor v captures behavior linear threshold model For vertex v GAP rule following form pv cid3 w neighbor v B wv X w θv cid12 cid10 ew v B wv pw X w cid11 w neighbor v Notice SN assumed modeled facts form ew v bvw meaning w neighbor v bwv weight according w inﬂuences v If vertex v active pv 1 holds pv 0 holds Appendix B Proofs Proposition 3 Consider SN S property p GAP cid2 Let ψ interpretation lfpcid2S cid2 lfpcid2p cid2 p A ψ A A Apv vertex v S p Then lfpcid2 cid2S pv Proof All rules cid2 cid2p predicate head atom reach predicate p rules affect value lfpcid2 cid2S pv As interested computing diffusion centrality property p ignore rules computation Moreover rules cid2p partitioned sets rules p rules cid2p cid2 cid2 p atom having predicate symbol q Icid2p appear head rule cid2 p vice versa Thus rules cid2p cid2 p value ψ precomputed cid2 p contribute ﬁxed point cid2 p body rule cid2p cid2 p depend rules cid2 p deﬁnition cid2 p cid2p cid2 Proposition 4 The Flickr model pmonotonic pdwindling The JacksonYariv model pmonotonic pdwindling The SIR model pmonotonic pdwindling Proof The GAP describing Flickr model pmonotonic function μvcid6v μp μq dp F head unique rule clearly monotonic The GAP pdwindling value function μvcid6v μp μq dp F equal value μp assume annotations μvcid6v μq constant dp F assume values 0 1 The GAP describing JacksonYariv model pmonotonic In fact function head unique rule dp J Y constant However GAP describing JacksonYariv model r cid8 monotonic term bi ci pdwindling term j E j 1cid8 cid8 j E j j ω j head atom function The GAP describing SIR model pmonotonic terms 1 R 1 R cid6 head atom dp S I R Moreover GAP annotation function ﬁrst rule 1 R μe cid6 head pdwindling To suﬃcient note presence terms 1 R 1 R atom annotation function ﬁrst rule value assumed function equal R R remember Icid2S I R p p r1 r2 cid2 vcid6 1 R cid6 μq v μp vcid6v cid6 Proposition 5 The worstcase time complexity Algorithm HyperLFP O N 1 α Umax maxvV qIcid2p cid6qv w M H log H Umax Smax log H h h H qv Sh Smax maxhH Sh α minimum value obtained line 9 C Kang et al Artiﬁcial Intelligence 239 2016 7096 95 Proof The cost making copies matrix M Lines 12 O N The loop Line 5 executed H times iteration remove hyperedge Heap Line 6 Within loop predominant costs cost deleting maximum Heap Line 6 O log H cost loop Line 10 This loop executed cid6 U qv number Umax worstcase iteration cost computing hyperedge h cid6 cid6cid12 Heap Line 16 Heap function W Line 11 O Smax worstcase cost adding cid11h Line 18 O log H Thus cost executing Lines 519 O N H logH Umax Smax Finally loop Line 3 executed 1 α times worstcase 1 maximum growth annotation atom α minimum increment step cid2 cid6 w Proposition 6 The worstcase time complexity Algorithm HyperDC O V N 1 α Umax maxvV h h H v Sh Smax maxhH Sh α deﬁned Proposition 5 H log H Umax Smax log H Proof By leveraging Proposition 1 HyperDC algorithm computes ﬁx point vertex worstcase time complexity given number vertices V times worstcase time complexity HyperLFP algorithm O N 1 α H log H Umax Smax log H called line 17 cid2 References 1 S Adali X Lu M MagdonIsmail Local community global centrality methods analyzing networks Soc Netw Anal Min 4 1 2014 210 2 RM Anderson RM May Population biology infectious diseases I Nature 280 5721 1979 361367 3 S Aral L Muchnik A Sundararajan Distinguishing inﬂuencebased contagion homophilydriven diffusion dynamic networks Proc Natl Acad 4 N Barbieri F Bonchi G Manco Topicaware social inﬂuence propagation models Knowl Inf Syst 37 3 2013 555584 5 MA Beauchamp An improved index centrality Behav Sci 10 2 1965 161163 6 R Becker Y Chernihov Y Shavitt N Zilberman An analysis steam community network evolution Proc Convention Electrical Electronics Sci 106 51 2009 2154421549 Engineers Israel 2012 pp 15 7 P Bonacich Factoring weighting approaches status scores clique identiﬁcation J Math Sociol 2 1 1972 113120 8 U Brandes A graphtheoretic perspective centrality Soc Netw 30 2 2008 136145 9 S Brin L Page The anatomy largescale hypertextual web search engine Comput Netw 30 17 1998 107117 10 M Broecheler P Shakarian VS Subrahmanian A scalable framework modeling competitive diffusion social networks Proc International Conference Social Computing 2010 pp 295302 ing Database Technology 2014 pp 295306 11 Çigdem Aslay N Barbieri F Bonchi RA BaezaYates Online topicaware inﬂuence maximization queries Proc International Conference Extend 12 D Centola The spread behavior online social network experiment Science 329 5996 2010 11941197 13 M Cha A Mislove PK Gummadi A measurementdriven analysis information propagation Flickr social network Proc International World Wide Web Conference 2009 pp 721730 14 N Chen On approximability inﬂuence social networks SIAM J Discrete Math 23 3 2009 14001415 15 W Chen C Wang Y Wang Scalable inﬂuence maximization prevalent viral marketing largescale social networks Proc International Con ference Knowledge Discovery Data Mining 2010 pp 10291038 16 Y Chen W Peng S Lee Eﬃcient algorithms inﬂuence maximization social networks Knowl Inf Syst 33 3 2012 577601 17 CY Chiang LH Huang BJ Li J Wu HG Yeh Some results target set selection problem J Comb Optim 25 4 2013 702715 18 S Dolev Y Elovici R Puzis Routing betweenness centrality J ACM 57 4 2010 19 D Fierens GV den Broeck J Renkens DS Shterionov B Gutmann I Thon G Janssens LD Raedt Inference learning probabilistic logic programs weighted boolean formulas Theory Pract Log Program 15 3 2015 358401 20 LC Freeman A set measures centrality based betweenness Sociometry 40 1 1977 3541 21 LC Freeman Centrality social networks conceptual clariﬁcation Soc Netw 1 3 1979 215239 22 WS Fung R Hariharan NJA Harvey D Panigrahi A general framework graph sparsiﬁcation Proc ACM Symposium Theory Computing 23 A Goyal W Lu LVS Lakshmanan SIMPATH eﬃcient algorithm inﬂuence maximization linear threshold model Proc International 2011 pp 7180 Conference Data Mining 2011 pp 211220 24 M Granovetter Threshold models collective behavior Am J Sociol 83 6 1978 14201443 25 A Graves S Adali J Hendler A method rank nodes RDF graph Proc International Semantic Web Conference 2008 26 HW Hethcote Qualitative analyses communicable disease models Math Biosci 28 34 1976 335356 27 MT Irfan LE Ortiz On inﬂuence stable behavior inﬂuential individuals networks gametheoretic approach Artif Intell 215 2014 28 M Jackson L Yariv Diffusion social networks Econ Publ 16 1 2005 6982 29 Q Jiang G Song G Cong Y Wang W Si K Xie Simulated annealing based inﬂuence maximization social networks Proc AAAI Conference 30 V Kagan A Stevens VS Subrahmanian Using Twitter sentiment forecast 2013 Pakistani election 2014 Indian election IEEE Intell Syst 31 C Kang C Molinaro S Kraus Y Shavitt VS Subrahmanian Diffusion centrality social networks Proc International Conference Advances Social Networks Analysis Mining 2012 pp 558564 32 G Karypis METIS parmetis Encyclopedia Parallel Computing 2011 pp 11171124 33 D Kempe JM Kleinberg É Tardos Maximizing spread inﬂuence social network Proc International Conference Knowledge 79119 Artiﬁcial Intelligence 2011 30 1 2015 25 34 D Kempe JM Kleinberg É Tardos Inﬂuential nodes diffusion model social networks Proc International Colloquium Automata Lan 35 M Kifer VS Subrahmanian Theory generalized annotated logic programming applications J Log Program 12 3 4 1992 335367 36 M Kimura K Saito H Motoda Eﬃcient estimation inﬂuence functions SIS model social networks Proc International Joint Conference Discovery Data Mining 2003 pp 137146 guages Programming 2005 pp 11271138 Artiﬁcial Intelligence 2009 pp 20462051 96 C Kang et al Artiﬁcial Intelligence 239 2016 7096 37 M Kimura K Saito R Nakano Extracting inﬂuential nodes information diffusion social network Proc AAAI Conference Artiﬁcial Intelligence 2007 pp 13711376 38 J Leskovec The Stanford Large Network Dataset Collection httpsnapstanfordedudataindexhtml 2015 39 J Leskovec A Krause C Guestrin C Faloutsos JM VanBriesen NS Glance Costeffective outbreak detection networks Proc International Conference Knowledge Discovery Data Mining 2007 pp 420429 40 JJ Lu A Nerode VS Subrahmanian Hybrid knowledge bases IEEE Trans Knowl Data Eng 8 5 1996 773785 41 M Mathioudakis F Bonchi C Castillo A Gionis A Ukkonen Sparsiﬁcation inﬂuence networks Proc International Conference Knowledge Discovery Data Mining 2011 pp 529537 42 M McPherson L SmithLovin JM Cook Birds feather homophily social networks Annu Rev Sociol 27 2001 415444 43 A Memory A Kimmig SH Bach L Raschid L Getoor Graph summarization annotated data probabilistic soft logic Proc International Workshop Uncertainty Reasoning Semantic Web 2012 pp 7586 44 C Minoiu C Kang VS Subrahmanian A Berea Does ﬁnancial connectedness predict crises Quant Finance 15 4 2015 607624 45 MB Naseri G Elliott Role demographics social connectedness prior internet experience adoption online shopping applications direct marketing J Target Meas Anal Mark 19 2 2011 6984 46 J Nieminen On centrality graph Scand J Psychol 15 1 1974 332336 47 J Pearl S Russell Bayesian Networks Computer Science Department University California 1998 48 M Purohit BA Prakash C Kang Y Zhang VS Subrahmanian Fast inﬂuencebased coarsening large networks Proc International Conference Knowledge Discovery Data Mining 2014 49 G Sabidussi The centrality index graph Psychometrika 31 1966 581603 50 T Schelling Micromotives Macrobehavior WW Norton Co 1978 51 P Shakarian M Broecheler VS Subrahmanian C Molinaro Using generalized annotated programs solve social network diffusion optimization problems ACM Trans Comput Log 14 2 2013 10 52 PL Szczepanski TP Michalak M Wooldridge A centrality measure networks community structure based generalization Owen value Proc European Conference Artiﬁcial Intelligence 2014 pp 867872 53 J Tang J Sun C Wang Z Yang Social inﬂuence analysis largescale networks Proc International Conference Knowledge Discovery Data Mining 2009 pp 807816 Mining 2011 pp 965973 54 H Toivonen F Zhou A Hartikainen A Hinkka Compression weighted graphs Proc International Conference Knowledge Discovery Data 55 H Toivonen F Zhou A Hartikainen A Hinkka Network compression node edge mergers Bisociative Knowledge Discovery An Introduction Concept Algorithms Tools Applications 2012 pp 199217 56 C Wang W Chen Y Wang Scalable inﬂuence maximization independent cascade model largescale social networks Data Min Knowl Discov 25 3 2012 545576 57 D Watts J Peretti Viral marketing real world Harv Bus Rev 2007