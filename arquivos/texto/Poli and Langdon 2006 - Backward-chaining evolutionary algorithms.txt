Artiﬁcial Intelligence 170 2006 953982 wwwelseviercomlocateartint Backwardchaining evolutionary algorithms Riccardo Poli William B Langdon Department Computer Science University Essex UK Received 12 August 2005 received revised form 11 April 2006 accepted 24 April 2006 Available online 9 June 2006 Abstract Starting simple observations popular selection method Evolutionary Algorithms EAstournament selectionwe highlight previouslyunknown source inefﬁciency This leads rethink order operations performed EAs suggest algorithmthe EA efﬁcient macroselectionthat avoids inefﬁciencies associ ated tournament selection This algorithm expected behaviour standard EA yields considerable savings terms ﬁtness evaluations Since ﬁtness evaluation typically dominates resources needed solve nontrivial problem savings translate reduction time Noting connection algorithm rulebased systems modify order operations EA effectively turning evolutionary search inference process op erating backwardchaining mode The resulting backwardchaining EA creates evaluates individuals recursively backward generation ﬁrst depthﬁrst search backtracking It powerful EA efﬁcient macroselection shares beneﬁts provably ﬁnds ﬁtter solutions sooner faster algorithm These algorithms applied form population based search representation ﬁtness function crossover mutation provided use tournament selection We analyse behaviour beneﬁts theoretically Markov chain theory spacetime complexity analysis empirically performing variety experiments standard backward chaining versions genetic algorithms genetic programming 2006 Elsevier BV All rights reserved Keywords Evolutionary computation Genetic algorithm Genetic programming Efﬁcient search Backward chaining Tournament selection 1 Introduction Evolutionary Algorithms EAs Algorithm 1 simple today popular form search opti misation technique 126132122 Their invention dates decades 1114183440 10 EAs share ingredients mainstream AI search techniques For example EAs seen special kinds generateandtest algorithms parallel forms beam search 26 discussion similarities differences EAs search algorithms However development largely parallel independent AI search Despite simplicity EAs sound theoretical models EAs precise mathematical results scarce hard obtain emerging years proposal original algorithm 715161924252831 Corresponding author Email addresses rpoliessexacuk R Poli wlangdonessexacuk WB Langdon 00043702 matter 2006 Elsevier BV All rights reserved doi101016jartint200604003 954 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 Select subpopulation reproduction Recombine genes selected parents 1 Initialise population 2 Evaluate population 3 loop 4 5 6 Mutate offspring stochastically 7 8 9 end loop Evaluate ﬁtness new population If stopping criterion satisﬁed exit loop Algorithm 1 Generic evolutionary algorithm 3639424447 An important reason delay algorithm representation set genetic operators cases ﬁtness function requires different theoretical model In addition randomness nonlinearities immense number degrees freedom present typical EA life hard theoreticians One line theoretical research differences representations obstacle analysis selection algorithms step 4 Algorithm 1 This selection requires knowledge ﬁtness phenotype individuals population form selection applied irrespective representation individual genotype Different selection methods analysed mathematically depth decade The main empha sis previous research takeover time 12 time required selection ﬁll population copies best individual initial generation evaluation changes produced selection ﬁtness distribution population 4523 In second line research behaviour selection algorithms characterised loss diversity proportion individuals population selected These theoretical studies comprehensive appeared completely characterised selection funda mentally making largely understood process However starting simple observations sampling behaviour popular selection method tournament selection paper possible source inefﬁciency EAs This phenomenon analysed previous research deep implications analysis effectively leading completely new class EAs powerful closer spirit classical AI techniques traditional EAs The paper organised follows In Section 2 tournament selection brieﬂy review previous relevant theoretical results Section 3 sampling inefﬁciency form selection In order remove predicted sampling inefﬁciency tournament selection Section 4 rethink order operations performed EAs This reveals embedded EAs graphstructure induced tournament selection connects individual samples search space time See Fig 1 Section 4 We able suggest algorithm EA efﬁcient macroselection exploits graph remove inefﬁciencies associated tournament selection The algorithm expected behaviour standard EA providing considerable savings terms ﬁtness evaluations Furthermore totally general applied representation ﬁtness function crossover mutation In Section 5 note unexpected connection operations EA efﬁcient macroselection rulebased systems leads modify order operations EA effectively turning evolutionary search inference process operating backwardchaining mode The resulting algorithm backwardchaining EA creates evaluates individuals recursively It starts generation depthﬁrst search backtracking works backwards ﬁrst This algorithm powerful EA efﬁcient macroselection shares beneﬁts provably ﬁnds ﬁtter solutions sooner faster algorithm We analyse theoretically behaviour EA efﬁcient macroselection backward chaining EA algorithms Section 6 In particular Section 61 start analysing sampling behaviour tournament se lection focusing effects time step generation EA We noting exploiting similarity sampling coupon collection problem We extend onegeneration analysis runs Section 62 inventing modelling mathematically Markov chain theory complex version problemthe iterated coupon collection problemwhich exactly mimics tournament selection multiple gen erations This allows fully exactly evaluate effects sampling inefﬁciency tournament selection entire runs indicates extent savings achieved R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 955 We discuss details practical implementation backwardchaining EA Section 7 compare time space complexity implementation standard EA Section 8 In Section 9 provide experimental results Genetic Algorithm GA Genetic Programming GP implementation backward chaining EA We discuss ﬁndings Section 10 provide conclusions Section 11 2 Tournament selection Tournament selection popular forms selection EAs In simplest form group n individuals chosen randomly uniformly current population best ﬁtness selected 2 The parameter n called tournament size vary selection pressure exerted method higher n higher pressure select average quality individuals In population size M takeover time deﬁned number generations required selection operator present obtain population containing M 1 copies best individual initial generation 12 In 12 takeover time tournament selection estimated asymptotic expression t 1 ln n cid2 lnM ln cid4cid5 cid3 lnM approximation improves population size M The loss ﬁtness diversity proportion individuals population selected selection phase Assuming member population unique ﬁtness loss diversity pd tournament selection estimated 45 pd n 1 n1 n n n1 later calculated exactly 23 cid7 pd 1 M Mcid6 k1 1 kn k 1n M n cid8 M The quantities t pd idea intensity selection scheme acts population function tournament size n population size M The pieces research aware relevant context work 4145 In 41 particular version tournament selection guarantees individuals run sampled proposed shown cases improve problemsolving ability GA Similar results recently reported 45 following early version work 32 proposes different tournament strategy guarantees individuals sampled While lines work concentrate modifying tournament selection focus understanding exploiting sampling behaviour standard tournament selection 3 Sampling behaviour tournament selection Let denote S number selection steps required generation immediately apparent meant If assumed selection use selection form mating pool1 creation new generation require exactly S M selection steps These exactly conditions assumed 4145 However assumption Instead consider currently common case genetic operator directly invokes selection procedure provide sufﬁcient number parents application twice case crossover So situations M selection steps required form new generation Consider generational selectorecombinative algorithm crossover performed probability pc reproduction performed probability 1 pc Mutation included making random changes children created crossover andor reproduction In cases number 1 The mating pool intermediate population gets created selection operations reproduction crossing draw individuals uniformly random 956 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 selection steps unchanged The number S selection steps required form new generation stochastic variable mean ES M1 pc ρMpc M 1 ρ 1pc cid2 cid5 ρ 1 crossover operator returns offspring application ρ 2 offspring returned The twooffspring version crossover requires fewer tournaments ρ 1 number selection steps required form new generation operator stochastic simply S M For brevity following use deﬁnition α 1 ρ 1pc2 Because tournament need n individuals perform S αM selection steps tournament selection requires drawing nαM individuals uniformly random resampling current population An interesting effect particularly small tournaments individuals particular generation necessarily sampled nαM draws For example let imagine running EA starting random population containing individuals denote 1 2 3 4 Let assume creating generation tournament selection tournament size n 2 mutation Then creation ﬁrst individual require randomly picking individuals current population individuals 1 4 selecting best mutation We repeat processes create second fourth new individuals Note entirely possible individual 3 involved tournaments It absolutely crucial stage stress difference sampling selecting individual particular generation Not selecting refers individual involved tournaments win exactly previous work loss diversity concentrated Not sampling refers individual participate tournament simply sampled creation required S αM tournament sets It individuals focus paper Therefore results paper orthogonal appeared work mentioned Section 2 limited uniqueness assumptions Continuing argument general individuals expect S αM tournaments As shown Section 61 answer comes straight literature coupon collector problem However explain connection want reﬂect brieﬂy effect important In general individuals sampled selection process inﬂuence whatsoever future generations However individuals use resources memory importantly CPU time creation evaluation For instance individual 3 previous example randomly generated ﬁtness evaluated preparation selection ﬁtness genetic inﬂuence future generations So ask generate individual ﬁrst place And generations following ﬁrst It entirely possible individual generation got created evaluated neglected tournament selection effect whatsoever generations 3 4 Did need generate evaluate individual If parents individual need What sort saving obtain creating unnecessary individuals run In Section 6 provide theoretical answers questions In particular conditions savings 20 ﬁtness evaluations fact easily achievable Before want reconsider way EAs run ways exploit inefﬁciencies tournament selection Amazingly ﬁnd efﬁcient algorithms achieving altering way expected behaviour evolutionary algorithms 4 Running EAs efﬁciently Normally generation EA tournament selection iterate following phases Algorithm 2 2 In following ignore potential stochasticity S This justiﬁable reasons simpliﬁes analysis signiﬁcant loss terms accuracy results obtained empirically veriﬁed b ρ 1 twooffspring crossover mutation algorithm stochasticity analysis exact c ρ 2 oneoffspring crossover possible slightly modify EA way stochasticity R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 957 1 Randomly initialise individuals population pop calculate corresponding ﬁtness values store vector fit op choose genetic operator arg 1 arityop ind 1 M 2 gen 1 G 3 4 5 6 7 8 9 10 11 12 13 14 end pool choose n random individuals drawing pop warg select winner pool based ﬁtnesses fit end newpopind result running operator op arguments w1 newfitind ﬁtness newpopind end pop newpop fit newfit Algorithm 2 Standard generational EA tournament selection M population size n tournament size G maximum number generations choice genetic operator use create new individual step 4 Algorithm 2 b creation random pool individuals application tournament selection step 6 c identiﬁcation winner tournament parent based ﬁtness step 7 d execution chosen genetic operator step 9 e evaluation ﬁtness resulting offspring step 10 Naturally phases b c iterated times arity genetic operator chosen phase process needs repeated times individuals new population The genetic makeup individuals involved operations phase d need know parents order produce offspring phases c e know genetic makeup individuals order evaluate ﬁtness However phases b steps 4 6 Algorithm 2 require knowledge actual individuals involved creation new individual In implementations phases performed properly manipulating numbers drawn pseudorandom number generator So reason ﬁrst iterate phases b times needed create new generation course memorising decisions taken iterate phases ce This idea ﬁrst 46 purposed speeding GP ﬁtness evaluation3 In fact In practical applications EAs people ﬁx maximum number gener ations prepared run algorithm for4 Let number G So cost memory space shown Algorithm 3 iterate phases b generation run ﬁrst generation generation G steps 29 iterate phases ce steps 1018 required generation G stopping criterion satisﬁed We algorithm EA macroselection obvious reasons Because decisions operator adopt create new individual elements popu lation use tournament random statistically speaking version algorithm exactly original In fact seed random number generator algorithms indistin guishable However unlike standard EA EA macroselection easily modiﬁed avoid wasting computation involved generating evaluating individuals neglected tournament selection 3 The main idea 46 estimate ﬁtness individuals involved tournaments evaluating randomly chosen subset training set available On basis estimate tournaments possible determine small error probability individual win examples These tournaments decided quickly subset tournaments individuals ended evaluated training set This produced speed 4 This limit virtually present stopping criterion based ﬁtness present 958 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 1 Randomly initialise individuals population pop0 calculate corresponding ﬁtness values store vector fit0 opgenind choose genetic operator arg 1 arityopgenind poolgenindarg choose n random individuals drawing popgen1 ind 1 M end end 2 gen 1 G 3 4 5 6 7 8 9 end 10 gen 1 G 11 12 13 14 15 16 17 18 end end ind 1 M arg 1 arityopgenind warg select winner poolgenindarg based ﬁtnesses fitgen1 end popgenind result running operator opgenind arguments w1 fitgenind ﬁtness popgenind Algorithm 3 EA macroselection Fig 1 Example graph structure induced tournament selection Shaded nodes possible ancestors ﬁrst individual generation Note nodes directly indirectly connected nodes generation To possible note iteration phases b multiple generations steps 2 9 Algorithm 3 induces graph structure containing G 1M nodes Nodes represent individuals run precisely elements pop array Edges stored pool array connect indi vidual individuals involved tournaments necessary select parents individual We individuals possible ancestors individual note possible ancestors individual superset actual ancestors parents parents parents individual question Let consider example population M 6 individuals run G 3 generations binary tournaments n 2 crossover rate pc 13 crossover produces child Mutation reproduction performed rate 1 pc 23 The graph induced tournament selection look like Fig 1 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 959 ind 1 M opgenind choose genetic operator arg 1 arityopgenind 1 gen 1 G 2 3 4 5 6 7 8 end 9 Analyse connected components pool array calculate neglected array 10 Randomly initialise individuals population pop0 end end poolgenindarg choose n random individuals drawing popgen1 marked neglected0 calculate ﬁtness values store vector fit0 ind 1 M 11 gen 1 G 12 13 14 15 16 17 18 19 20 21 end end end notneglectedgenind arg 1 arityopgenind warg select winner poolgenind arg based ﬁtnesses fitgen1 end popgenind result running operator opgenind arguments w1 fitgenind ﬁtness popgenind Algorithm 4 EA efﬁcient macroselection We emphasise graphstructure connecting individuals population time induced tournament selection particularly evident revealed EA macroselection nonetheless present deeply embedded EA form selection So going help avoid generating evaluating individuals neglected tournament selec tion Simple After macroselection iteration phases b generation G completed analyse information graph structure induced tournament selection identify individuals unnecessary mark avoid calculating evaluating iterating phases ce Clearly want mark population members involved tournament generation However interested calculating evaluating individuals population generation G maximum efﬁciency achieved considering individuals directly indirectly connected M individuals generation Ga problem easily solve trivial connectedcomponent algorithm The modiﬁed algorithm shown Algorithm 4 We EA efﬁcient macroselection EAEMS Note maximise efﬁciency unusually initialisation ﬁrst phase EA effectively coming macroselection connectedcomponent detection phases Let brief look differences new EAEMS standard EA Irrespective problem solved parameter settings behaviours standard algorithm efﬁcient version proposed average identical So differences EAs Obviously standard algorithm requires ﬁtness evaluations creations individuals EAEMS requires bookkeeping use memory Also clearly particular run plots average ﬁtness maximum ﬁtness generation differ EAEMS individuals considered calculating statistics However averaged multiple runs average ﬁtness plots coincide A important difference comes fact practitioners track best individual seen far run EA designate result run In EAEMS return best individual generation G best individual seen run sampled tournament selection Because EAEMS algorithm create evaluate individuals sampled endofrun results differ EA EAEMS algorithms Of course best individual seen run actually member population generation So creates evaluates individuals generation G leads minor inefﬁciency EA efﬁcient macroselection time algorithms behave identically point view 960 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 The EAEMS offers way fully exploit sampling behaviour tournament selection This appear best However recursive nature connectedcomponent detection similarity mechanics EAs rulebased systems suggest way substantial improvements discussed section 5 Backwardchaining EAs rulebased systems At sufﬁciently high level abstraction surprising similarities EAs RuleBased Systems RBSs operating forwardchaining mode 3548 In RBSs start working memory containing premises apply set IFTHEN inference rules modify working memory adding removing facts iterate process certain condition satisﬁed fact consider conclusion asserted The working memory RBS similar rôle population EA facts working memory effectively like individuals population Because rules knowledge base RBS effectively manipulate facts working memory share similarity genetic operators EA create new members population Running EAs generation 0 generation 1 generation 2 norm clock ticks forward nature certainly decades ﬁeld evolutionary computation The loose analogy RBSs EAs mentioned terribly useful thing suggests possibility running EA backward chaining mode like RBS radically subverting natural order operations EA time generation number EA canon Broadly speaking RBS run backward chaining focuses particular conclusion attempts prove operates follows looks rules conclusion consequent term following THEN rule b analyses antecedent IF rule c antecedent fact words working memory original conclusion proven placed working memory Otherwise saves state inference recursively restarts process antecedent new conclusion prove If rule conclusion consequent recursion stopped way proving attempted If rule condition common attempts prove truth conditions time It assert conclusion rule conditions satisﬁed When backward chaining RBS considers rules contribute determining truth falsity target conclusion This lead major efﬁciency gains So run EA backwardchaining mode Let suppose interested knowing makeup population generation G let start focusing ﬁrst individual population Let r individual Effectively r plays rôle conclusion want prove In order generate r need know operator apply produce parents use In turn order know parents use need perform tournaments select them5 In tournaments need know makeup n tournament size individuals previous generation course stage know Let I s1 s2 set individuals need know generation G 1 order determine r Clearly s1 s2 like premises rule applied allow work r require evaluating ﬁtness element I deciding winners tournaments applying chosen genetic operator generate r Normally know makeup individuals However recursively consider si subgoal So determine operator compute s1 determine set individuals generation G 2 needed continue recursion When emerge repeat process s2 The recursion terminate ways reach generation 0 case directly instantiate individual question invoking initialisation procedure particular EA considering b individual need know genetic makeup constructed evaluated Clearly individuals generation 0 rôle similar initial contents working memory RBS Once ﬁnished r repeat process individuals population generation G The process summarised Algorithm 5 We EA running mode BackwardChaining EA BCEA 5 Decisions operator choice tournaments trivial spot drawing random numbers advance EAEMS R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 961 1 Let r individual population generation G 2 Choose operator apply generate r 3 Do tournaments select parents s1 s2 individuals generation G 1 involved tournaments 4 Do recursion unknown si subgoal Recursion terminates generation 0 individual known evaluated 5 Repeat individuals generation G Algorithm 5 Backwardchaining EA Clearly level BCEA recursive depthﬁrst traversal graph induced tournament selection Fig 1 Section 4 While traverse graph precisely reemerge recursion position know genetic makeup nodes encountered invoke ﬁtness evaluation procedure Thus label node genetic makeup ﬁtness individual represented node Recursion stops reach node incoming links generation0 individual gets immediately labelled randomly evaluated reach node previously labelled Statistically BCEA fully equivalent EAEMS presents level equivalence ordinary EA In particular seed random number generators decisions operators tournaments performed batch graph traversal Gth generations BCEA EA indistinguishable So differences bother BCEA instead simpler forwardchaining version algorithm One important difference modes operation order individuals population evaluated To illustrate let reconsider example Fig 1 let suppose ﬁrst instance interested knowing ﬁrst individual generation The possible ancestors individual shown shaded nodes Fig 1 Furthermore brevity let denote nodes row individual column g generation graph notation rig In forward chaining EA knew individuals unnecessary deﬁne target individual r13 individuals r50 r31 r61 r22 evaluate individuals column column left right Eg EAEMS evaluates r10 r20 r30 r40 r60 r11 r21 r41 r51 r12 r32 ﬁnally r13 That generation 0 individuals computed generation 1 individuals turn computed generation 2 individuals A BCEA instead evaluate nodes different order For example according sequence r10 r30 r40 r11 r20 r21 r12 r60 r41r51 r32 ﬁnally r13 So algorithm forth evaluating nodes different generations That time cid5 generation number BCEA Why important Typically EA average ﬁtness population maximum ﬁtness generation grow generation number grows In forward chaining EA ﬁrst 3 individuals evaluated expected average ﬁtness equal average ﬁtness individuals generation 0 true BCEA However unlike forwardchaining EA fourth individual created evaluated BCEA belongs generation 1 ﬁtness expected higher previous individuals Individuals 5 6 expected ﬁtness algorithms However seventh individual drawn BCEA generation 2 individual forward EA draws generation 1 individual So BCEA expected produce higher ﬁtness sample EA Of course process going continue indeﬁnitely point individuals evaluated BCEA start average inferior This unavoidable sets individuals sampled algorithms identical This behaviour general In virtually problems practical ﬁtness tends increase generation generation So BCEA ﬁnd ﬁtter individuals faster EAEMS ﬁrst run slower second So restricts oneself ﬁrst phase BCEA efﬁcient ordinary EA avoids evaluating individuals neglected tournament selection tends ﬁnd better solutions faster Ie BCEA effective search algorithm How sure work region BCEA superior corresponding EAEMS Simple like ordinary EA BCEA need continue evolution individuals generation G known evaluated stop algorithm best ﬁtness seen far reaches suitably high value In way avoid phase BCEA slower EAEMS 962 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 It worth noting faster convergence behaviour present BCEA irrespective value tournament size course beneﬁts BCEAs depend 6 Theory In section want model mathematically sampling behaviour tournament selection understand savings achieve EAEMS BCEA We start drawing analogy tournament selection coupon collection problem 61 Coupon collection tournament selection In coupon collector problem time collector buys certain product coupon given The coupon equally likely N types In order win prize collector coupon type The question products collector buy expect set coupons The answer 9 derived considering probability obtaining ﬁrst coupon trial 1 expected waiting time 1 trial probability obtaining second coupon distinct ﬁrst N 1 N 1 probability obtaining coupon distinct ﬁrst N 2 N 2 So expected number trials obtain set coupons N expected waiting time N N expected waiting time N EN 1 N N 1 N N 2 N N log N ON It known N log N limit sharp If X number purchases type coupon collected constant c PrX N log N cN 1 e ec lim N Eg c 3 limit types coupons probability takes N log N 3N trials purchase type 5 How process tournament selection related coupon collection problem We imagine M individuals current population N M distinct coupons tournament selection draw replacement nαM times pool coupons Because sharpness couponcollector limit mentioned nα log M c suitable positive constant c expect tournament selection sample individuals population time However sufﬁciently small tournament sizes sufﬁciently large populations probability individuals sampled selection signiﬁcant So different coupons individuals expect sampled end nαM trials In coupon collection problem expected number trials necessary obtain set x distinct coupons 9 Ex 1 N N 1 N N 2 N N x 1 N log N N x ON By setting Ex nαM N M ignoring terms order ON obtain estimate number distinct individuals sampled selection nα x M1 e 1 This indicates expected proportion individuals sampled current population varies approximately like negative exponential tournament size This approximation accurate However calculate expected number individuals neglected performing nαM trials directly We ﬁrst calculate probability individual involved trial 1 1M Then expected number individuals involved tournaments simply cid7 cid8nαM M1 1MnαM M M M 1 varies like negative exponential tournament size R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 963 Fig 2 Proportion individuals sampled generation tournament selection different tournament sizes n population sizes M assuming twooffspring crossover andor mutation As shown Fig 2 α 1 twooffspring crossover crossover typically n 2 13 population neglected n 3 drops 5 n 4 2 negligible bigger values n This simple analysis suggests saving computational resources avoiding creation evaluation dividuals sampled tournament selection process possible relatively low selection pressures However tournament sizes range 25 common practice particularly attacking hard multimodal problems require extensive exploration search space zooming search particular region Furthermore section look behaviour tournament selection multiple generations bigger savings suggested achieved 62 Iterated coupon collector problem Let consider new game iterated coupon collection problem coupon set changes regular intervals number coupons available N remains constant Initially collector given pos sibly incomplete set m0 old coupons Each old coupon allows collector draw n new coupons So perform total nm0 trials produce set m1 distinct coupons new set The coupon set changes player performs nm1 trials gather new distinct coupons possible And Interesting questions happens mt t grows Will reach limit Will oscillate In way values n m0 N inﬂuence behaviour Before answer questions let motivate analysis little How new problem related EAs tournament selection The connection simple assume α 1 offspring crossover mutation sake clarity Suppose interested computing evaluating m0 individuals particular generation G run These like initial set old coupons given player Clearly order create individuals need know parents This require running m0 tournaments select parents In tournament randomly pick n individuals generation G 1 distinct individual generation equivalent coupon new coupon set After nm0 trials position determine individuals generation G 1 contribute future generations count denote number 964 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 m16 So concentrate individuals They equivalent new set coupons collector gathered We perform nm1 trials determine individuals generation G 2 new coupon set contribute future generations count denote number m2 reach initial random generation There game stops The graph induced tournament selection stochastic variable Every time run EA instantiate variable So terms graph structure associated tournament selection process described corresponds instantiation structure mt corresponds number possible ancestors nodes m0 individuals G tth vertical layer graph So effectively iterated coupon collector problem model sampling behaviour tournament selection multiple generations generational EA Knowing sequence mt particular EA tell save creating evaluating individuals sampled selection Naturally oracle help choose G m0 For concentrate understanding iterated coupon collector problem think G number generations prepared run EA imagine m0 M population In classical coupon collection problem shopper typically perform trials necessary gather collection coupons As seen easy estimate distinct coupons expect end given ﬁxed number trials Because iterated coupon collection game starts known number trials calculate expected value m1 However directly apply theory previous section gather information m2 This m1 stochastic variable order estimate m2 need know probability distribution m1 expected value Exact probabilistic modelling obtained considering coupon collection game Markov chain state chain number distinct coupons collected The transition matrix chain easily constructed noticing chain state k collector k distinct coupons coupon purchased state k new coupon duplicate happens probability k1 N state k 1 coupon different currently held course happens probability N k1 So number distinct individuals previous generation sampled randomly picking individuals tournament selection described applying following Markov transition matrix number times N A 1 M 0 1 0 M 0 M 1 0 0 0 0 0 0 2 0 0 0 M 2 3 0 0 0 0 0 0 M The process e0 1 0 samples population given At e0 simply ﬁrst column matrix At state 0 This represented state probability vector 0 T7 So probability distribution states t coupon purchases random 0 starts 0 Suppose interested m0 individuals generation G The number tournaments construct generation nαm0 Therefore probability distribution number distinct individuals need know generation G 1 m1 given Anαm0 e0 Notice gives probability distribution number draws nαm1 need generation G 2 order fully determine m1 individuals want know generation G 1 6 Note work generation index0 generation G 1 index1 generation G 2 index2 Also stage interested knowing number individuals playing active rôle generation G 1 need determine winners tournaments We need know involved tournament So need evaluate ﬁtness need know genetic makeup individual 7 Each element state probability vector represents probability corresponding state Since state elements vector add 1 In coupon collection problem initially state 0 possible So ﬁrst element e0 nonzero R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 965 For example population size M 3 number states Markov chain M 1 4 tournament size n 2 use twooffspring version crossover α 1 interested m0 1 individuals nαm0 2 probability distribution m1 represented following probability vector A2e0 cid8 2 cid7 1 3 0 3 0 0 0 1 2 0 0 0 2 1 0 0 0 3 2 cid7 cid8 1 9 1 0 0 0 0 0 0 0 3 1 0 0 6 6 4 0 0 2 5 9 1 3 1 0 0 0 0 1 2 0 If interested m0 2 individuals generation G probability distribution number m1 unique individuals sampled A4e0 00 00370 05185 04444 T Finally interested population m0 3 distribution A6e0 0 00041 02551 07407 T reveals conditions building generation 1 4 chances sampling population previous generation Of course m0 0 probability vector m1 e0 m1 0 generally mt 0 0 t G Although example trivial reveals given m0 compute distribution m1 That deﬁne new Markov chain model iterated coupon collector problem In chain state exactly couponcollector chain number distinct coupons collected time step corresponds complete set draws new coupon set draw coupon Since number states unchanged transition matrix B new chain size A M 1 M 1 Column B probability distribution m0 Anαie0 That cid15 cid15AMnαe0 cid15 cid15A2nαe0 cid15 cid15Anαe0 cid15 cid15 cid3 e0 B cid4 For instance calculated case M 3 n 2 α 1 B 0 0 0 1 0 03333 00370 00041 0 06667 05185 02551 04444 07407 0 0 The important thing transition matrix deﬁned chain iterated compute proba bility distributions m2 m3 far necessary reach generation 0 In general B block diagonal form cid7 cid8 1 0T 0 C B 0 column vector containing M zeros C M M stochastic matrix Clearly B ergodic state 0 reach state 0 expect unique limit distribution mt However B block diagonal Bx cid7 cid8 1 0T 0 Cx So ensured probability chain initially state 0 0 Prm0 0 0 chain visit state future time Because property objectively state 0 totally uninteresting course know interested individual generation G need know individual previous generations declare state iterated couponcollection chain invalid reduce state set 1 2 M In situation C state transition matrix chain understand sampling behaviour tournament selection multiple generations need concentrate properties C The transition matrix C ergodic nα 1 easily seen following argument If nα 1 old coupon gives right draw new coupon iterated couponcollection problem So state chain k k M possible reach state k 1 stage game nonzero probability From course possible reach state k 2 M So lower state possible reach higher state repeated iterations game But course converse true irrespective value nα chance getting fewer coupons 966 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 iteration game resampling So higher state reach lower state fact unlike reverse achieve iteration game Since α cid2 1 n 1 practical applications condition nα 1 virtually satisﬁed C ergodic Then PerronFrobenius theorem guarantees probability states chain converges limit distribution independent initial conditions 78253336 applications result EAs This distribution given normalised eigenvector corresponding largest eigenvalue C λ1 1 speed chain converges distribution determined magnitude second largest eigenvalue λ2 relaxation time ergodic Markov chain 11 λ2 Naturally inﬁnitetime limit behaviour chain particularly important G sufﬁciently big mt settles limit distribution iterate generation 0 Otherwise transient behaviour needs focus Both provided theory Because transition matrices talking relatively small matrix C M M amenable numerical manipulation We example ﬁnd eigenvalues eigenvectors C respectable population sizes certainly range applications EAs determining limit distribution speed approached If pt probability vector representing probability distribution mt expected value mt Emt 1 2 M pt 1 2 M Ct p0 2 Typically m0 ﬁxed user probability distribution p0 delta function centred value chosen user Thus p0 em0 el base vector containing zeros element l 1 If p denotes limit distribution pt large G average number γ individuals generations 0 G 1 effect whatsoever designated set m0 individuals M p This average saving achieved generation G approximately γ M 1 2 creating evaluating unnecessary individuals EAEMS BCEA Notice ergodicity selection process means generations fraction individuals avoid creating depend m0 That large G m0 large M little difference saving So want know entire makeup generation G saving approximately γ G creations evaluations individuals 63 Approximate model transient behaviour The model presented previous section comprehensive accurate practical purposes approximate simpler model desirable We develop model section In particular focus modelling transient behaviour number individuals sampled tournament selection multiple generations mt When number individuals want know ﬁtness end EA run m0 sufﬁciently smaller population size M expect number individuals sampled selection mt grow exponentially look start run The reasons simple samples drawn population resampling unlikely ancestors individual tend form tree generations The branching factor tree nα So small t generation G t include nαt ancestors individual generation G Naturally exponential growth continues resampling starts signiﬁcant m0nαt comparable expected number individuals processed limit distribution p For small populations high selective pressures transient short However cases transient lasts generations For example population M 100 000 individuals α 1 case mutation andor twooffspring crossover transient lasts 20 generations exponential 16 17 This population size appear big generation numbers appear small However big populations short runs actually typical class EAs genetic programming 3 1819 populations millions individuals uncommon solving complex important problems 17 So worth evaluating impact transient total number ﬁtness evaluations R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 967 Let assume G te generation transient effectively exponential We obtain approximation te assuming G te generation exponential m0nαte hits population size limit M te logMm0 lognα 3 So example n 2 α 1 population size M m0 1 exponential transient te log2 M generations The number individuals evaluated te generations run sum geometric series F B m0 nαte1 1 nα 1 By substituting Eq 3 expression obtain cid8cid7 cid7 cid7 cid8 cid8 F B m0 1 nα 1 nα M m0 1 4 This leads simple upper bound number individuals required exponential transient F B nα nα1 M Eq 4 allows compute maximum efﬁciency gain obtainable Let assume run BCEA te generations interested computing m0 individuals generation te To compute individuals standard EA need construct evaluate order F F M te M logMm0 lognα individuals So speedup achievable exploiting transient speedup F F F B cid3 m0 M logMm0 lognα cid4cid3 1 nα1 nα M m0 cid4 logMm0 nα lognα nα1 cid3 cid4 1 Clearly maximum speedup obtained m0 1 nα 2 case speedup factor M This big large populations However practice appears hard approximately log2 achieve speedups 10 speedup factor logarithmic function population size 7 BCEA implementation Encouraged theoretical evidence provided previous section indicate substantial savings achieved following ideas Section 5 designed implemented backwardchaining EAs Java One simple GA refer BCGA GP implementation BCGP The objective evaluate BCEA approach brings signiﬁcant efﬁciency gains BCEAs compare equivalent standard forward versions terms ability solve problems Algorithm 6 provides pseudocode description key components implementation All com ponents exactly ordinary EA omitted brevity The main thing notice use lazyevaluation type approach We create graph structure induced tournament selection In stead statically create nodes graph store twodimensional arrays graph edges dynamically generated stored stack recursion This achieved choosing genetic operator invoking tournament selection procedure needed order construct individual beginning run individuals generations Also note implementation simplistic requires preallocation G 1 M arrays 968 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 procedure runGM 1 Create arrays Known Population Fitness 2 individuals I generation G 3 4 end 5 return I evolve_backIG return random_float crossover_rate Populationgenindiv random individual procedure evolve_backindivgen 1 Knownindivgen 2 3 end 4 gen 0 5 6 7 8 9 10 11 12 13 14 15 end 16 Fitnessgenindiv fit_funcPopulationgenindiv 17 Knowngenindiv true 18 return parent1 tournamentgen1 parent2 tournamentgen1 Populationgenindiv crossoverparent1parent2 parent tournamentgen1 Populationgenindiv mutationparent end procedure tournamentgen 1 fbest 0 2 best undefined 3 tournament_size times 4 5 6 7 8 end 9 10 end 11 return Populationgenbest candidate random integer 1 M evolve_back candidate gen Fitnessgencandidate fbest fbest Fitnessgencandidate best candidate Algorithm 6 Backwardchaining EA oneoffspring crossover Population array containing individuals population generation8 Fitness array single precision ﬂoating point numbers This store ﬁtness individuals Population Known array bits This initialised 0 A bit set 1 indicates corresponding individual Popu lation computed ﬁtness calculated Using arrays main data structures appropriate given scientiﬁc objectives implementation However wasteful BCEAs entries arrays corresponding individuals sampled tournament selection In production implementation use sophisticated efﬁcient data structures hash tables save memory 8 If representation ﬁxed size individuals directly stored Population With variablesize representations array pointers dynamicallyallocated data structures representing individuals R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 969 myrand crossover_rate2 sibling_poolgen return Populationgenindiv random individual myrand random_float myrand crossover_rate procedure evolve_backindivgen 1 Knownindivgen 2 3 end 4 gen 0 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 end 23 Fitnessgenindiv fit_funcPopulationgenindiv 24 Knowngenindiv true 25 return parent1 tournamentgen1 parent2 tournamentgen1 offsprings crossoverparent1parent2 Populationgenindiv offspring1 sibling_poolgenaddoffspring2 parent tournamentgen1 Populationgenindiv mutationparent end end Populationgenindiv sibling_poolgenremove_random_indiv Algorithm 7 Backwardchaining EA twooffspring crossover As mentioned Section 3 crossover operators return offspring require average half number selection steps crossovers returning offspring Therefore child crossover operators efﬁcient BCEA In order advantage twooffspring crossovers need modify algorithm slightly The ma jor change add expandable array sibling_pool temporarily stores second offspring generated crossover operation Other minor changes required evolve_back routine Algorithm 7 8 Space time complexity BCEA BCEAs based changing order operations EA This requires memorising choices individuals multiple generations Let evaluate space complexity BCEA compare space complexity standard EAs9 81 Fixedsize representations We consider EAs representation individual requires ﬁxed memory b bytes The space complexity forward generational EA CF 2 b 4 M assumed store current new generation ﬁtness values stored vector ﬂoats 4 byte So b cid8 1 CF 2bM In BCEA need store array individuals size b ﬂoats bit array size G 1 M So space complexity CB G 1 M cid7 cid8 b 4 1 8 9 Our calculations ignore small memory required stack recursion Also case BCEAs twooffspring crossover ignore memory required expandable array sibling_pool typically contains individuals 970 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 For b cid8 1 CB G 1bM So difference space complexity algorithms cid6C CB CF G 1 M b indicates conditions use BCEA carries signiﬁcant memory overhead However prevent use BCEAs For example representation individual requires b 100 bytes run population M 1 000 individuals 100 generations BCEA requires 10 MB memory run Let consider time complexity BCEA In ﬁxedsize representations time required ﬁtness evaluation approximately constant Since time spent ﬁtness evaluation invariably dominates required phases EA time complexity standard EA proportional number ﬁtness function calls F F G 1M Likewise BCEA time complexity proportional F B EB EB number individuals actually created evaluated run Naturally low selective pressure large populations F B F F BCEA runs faster corresponding EA 82 Variablesize representations We divide calculation parts C Cﬁxed Cvariable Cﬁxed represents memory bytes required store data structures necessary run EA excluding individuals Cvariable represents memory individuals For variablesize representations GP trees vary function random seed generation number parameters details run As far ﬁxed complexity concerned forward generational EA CF ﬁxed 2 M 4 4 16M As factor 2 arises generational approach store current new generation This requires 2 vectors pointers 4 byte population members vectors ﬁtness values ﬂoats 4 byte vectors size M In BCGP instead need cid7 4 4 1 8 G 1 M 8G 1M CB ﬁxed cid8 need store array pointers 4 bytes ﬂoats bit array size G 1 M Variable space complexity harder compute For forward variablesize representation EA CF 2 M SF max variable max maximum generations average size individuals generation run SF In BCEA CB variable EB SB avg SB avg average size individuals BCEA run individual size averaged individuals created run Remember EB number individuals actually created evaluated run So difference space complexity algorithms cid3 2 M SF 8G 1 16 cid6C CB CF M cid4 EB SB avg max indicates conditions BCEA carries signiﬁcant memory overhead However prevent use BCEA Consider example BCGP population 100 000 individuals run 50 generations average program size run 100 bytes In worst possible case R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 971 programs constructed evaluated EB G 1M need 500 MB memory readily available modern personal computers The memory overhead BCEA cid6C function average averageindividualsize SB avg maximum max We know statistically BCEA EA behave expect SF max max We cid6C general size individuals SB max averageindividualsize SF avg SF SB varies widely As ﬁxedsize representations time complexity dominated ﬁtness evaluation Naturally number ﬁtness function calls ﬁxedsize case Section 81 However variablesize representations execution time ﬁtness function varies size individual evaluated So precise need know size varies evaluation time depends size representation To illustrate analysis section consider case GP 821 Space time complexity BCGP The variability individual size particularly marked GP common phenomenon known bloat Bloat progressive growth program size accompanied corresponding improvement ﬁtness 20 This marked late phases run If bloat happens particular problem programs standard GP BCGP increase size However BCGP choose evaluate individuals generations run m0 cid9 M bloat typically marked generations SB avg That bloat programs created BCGP average smaller created forward GP So SB max avg avg smaller SF cid9 SF These effects partly mitigate memory overhead cid6C BCGP Also BCGP tends evaluates smaller programs GP bloat interesting impact run time To need assess computational complexity T required run GP BCGP T effectively dominated cost running ﬁtness function The cost ﬁtness evaluation depends factors typically approximately proportional number primitives program evaluated executed number examples training set ﬁtness cases GPs jargon K So express T number primitives executed standard GP T F F F K SF avg G 1 M K SF avg BCGP T B F B K SB avg EB K SB avg So saving provided BCGP cid6T T F T B K cid3 G 1 M SF avg EB SB avg cid4 That bloating population parsimony BCGP terms ﬁtness evaluations compounded parsimony terms program sizes produce impressive savings 83 Wallclock executiontime The number ﬁtness evaluations standard measure efﬁciency evolutionary algorithms This rea sonable mentioned nontrivial problem cost ﬁtness evaluation overwhelming component computation load EAs This particularly true GP When comparing different algorithms assumed ﬁtness evaluation require approximately computation algorithms tested However observed true evaluation time depends particular features size structures evaluated like GP In case GP example saw appropriate use number primitives executed measure efﬁciency However practice situations execution time effected factors total memory algorithm In particular algorithm uses memory exceeds physical memory andor programs competing memory running time paging disk access memory important factors determining performance algorithm Naturally factors difﬁcult include analysis However BCEAs use memory corresponding forward versions clear settings paging disk access slow 972 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 BCEAs This algorithms actually efﬁcient corresponding forward EAs theory faster Fortunately Section 94 demanding conditions observed happen despite fairly ordinary PC experiments 9 Experimental results We performed experiments BCGA BCGP oneoffspring twooffspring crossover comparing standard GA GP versions 91 Test problems In case BCGA run experiments counting ones problem This simple linear problem widely benchmarking purposes requiring binary representation ﬁtness individual number ones bit string representing individual In case BCGP considered larger variety problems complexity ranging simple hard In problems objective induce continuous target function examples Problems type known symbolic regression problems GP literature 318 GP asked ﬁnd function ﬁts certain datapoints ﬁnding coefﬁcients preﬁxed function standard regression task The target functions univariate quartic polynomial multivariate quadratic polynomial multivariate cubic polynomial The quartic polynomial f x x4 x2 x3 x For easy problem 20 ﬁtness cases form x f x obtained choosing x uniformly random interval 1 1 One multivariate polynomial Poly4 f x1 x2 x3 x4 x1x2 x3x4 x1x4 This harder problem previous solvable For problem 50 ﬁtness cases form x1 x2 x3 x4 f x1 x4 They generated randomly setting xi 1 1 The second multivariate polynomial Poly10 f x1 x10 x1x2 x3x4 x5x6 x1x7x9 x3x6x10 Also problem 50 ﬁtness cases form x1 x10 f x1 x10 obtained randomly setting xi 1 1 This problem extremely hard 92 GA vs BCGA Let start corroborating experimentally equivalence GA BCGA expected faster convergence behaviour BCEA To assess performed 100 independent runs forward GA BCGA applied 100bit counting ones problem In runs maximum number generations G set 99 100 generations The population size M 100 Only tournament selection mutation mutation rate pm 001 α 1 In BCGA computed individuals generation G That m0 M To comparison algorithms possible stated computed statistics M ﬁtness evaluations We treated interval generation ﬁtness evaluations spread generations Fig 3 shows ﬁtness vs generation plots GA BCGA tournament size n 2 It clear ﬁgure BCGA performs 20 fewer ﬁtness evaluations standard EA reaching average maximum ﬁtness values As predicted previous sections signiﬁcant computational saving comes altering substantial way behaviour EA Fig 4 shows ﬁtness vs generation plots algorithms tournament size n 3 With tournament size saving 6 deﬁnitely worth having clearly higher selection pressures disadvantages BCGA terms memory use bookkeeping quickly preponderant An important question BCEA expected number ﬁtness evaluations changes function M n m0 G In order assess impact transient limitdistribution behaviour BCEA performed series experiments task evaluate individual generation G m0 1 In experiments set G 49 performed exactly 50 generations 0 49 This big transients ﬁnished generation 0 revealing limitdistribution sampling behaviour In experiments populations size M 10 M 1 000 M 100 000 So forward runs R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 973 Fig 3 Comparison BCGA standard GA tournament size 2 Means 100 independent runs Note comparison possible case BCGA unit abscissa axis Generations corresponds M 100 ﬁtness evaluations Fig 4 Comparison BCGA standard GA tournament size 3 Means 100 independent runs required exactly F F 500 F F 50 000 F F 5 000 000 ﬁtness evaluations complete For setting 100 independent runs Fig 5 shows average proportion individuals evaluated BCEA generation mutation α 1 tournaments sizes n 2 n 3 function population size M Fig 6 shows average proportion individuals evaluated BCEA oneoffspring crossover pc 05 α 15 tournaments sizes 974 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 Fig 5 Average proportion individuals evaluated BCEA mutation α 1 tournaments sizes n 2 n 3 Means 100 independent runs Fig 6 Average proportion individuals evaluated BCEA oneoffspring crossover pc 05 α 15 tournaments sizes n 2 n 3 Means 100 independent runs From ﬁgures expected limitdistribution saving largely independent size population Eg n 2 transient 80 population possible ancestor individual generation G mutation goes 94 α 15 For EAs long runs percentages provide approximate estimation total proportion ﬁtness evaluations required backward chaining version algorithm wrt standard algorithm R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 975 Fig 7 Logarithmic plot average proportion individuals evaluated BCEA mutation α 1 tournaments sizes n 25 population size M 100 000 Table 1 Mean number ﬁtness evaluations recorded 50 generations experiments shown Figs 5 6 percentage ﬁtness evaluations required forward EA reported column reference M 10 1 000 100 000 BCEA mutation n 2 768 640 534 n 3 908 819 741 BCEAoneoffspring crossover n 3 n 2 904 818 738 952 892 832 Forward EA 500 50 000 5 000 000 Figs 5 6 transient number individuals sampled tournament selection mt grows quickly backward generation 49 As clearly shown Fig 7 growth exponential predicted Section 63 The rapid growth lasts te 18 generations n 2 te 12 generations n 3 te 9 generations n 4 te 8 generations n 5 conﬁrms accuracy approximation Eq 3 predicts te values approximately 17 10 8 7 respectively Even runs te generations effects exponential transient marked To illustrate Table 1 reports mean total number ﬁtness evaluations recorded experiments shown Figs 5 6 percentage standard EA ﬁtness evaluations F F G 1 M Taking example case n 2 mutation limit distribution effort 80 efforts low 534 required forward EA achieved10 93 GP vs BCGP The function set GP included functions protected division DIV DIVx y xy y 0001 case DIVx y x avoid runtime errors The terminal set included indepen dent variables problem x Quartic x1 x2 x3 x4 Poly4 x1 x2 x10 Poly10 Fitness calculated negation sum absolute errors output produced program desired 10 The similarity numbers fourth columns table mistake mutation tournament size n 3 oneoffspring crossover tournament size n 2 pc 05 require average number selection steps 3M 976 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 Fig 8 Quartic polynomial regression problem Normal GP contrasted chance success BCGP population size 100 average 1 000 runs output ﬁtness cases A problem considered solved program error 105 summed ﬁtness cases We binary tournaments n 2 parent selection The initial population created grow method 18 maximum depth 6 levels root node level 0 We 80 twooffspring subtree crossover uniform random selection crossover points 20 point mutation 2 chance mutation tree node The population size M 100 1 000 10 000 100 000 For purpose comparing problem solving ability GP BCGP gave algorithms number ﬁtness evaluations The maximum number ﬁtness evaluations 30M standard GP corre sponds 30 generations For different experiments depending statistical requirements performed 100 1 000 5 000 independent runs backward forward GP In BCGP computed 80 ﬁnal generation m0 08M approximately steady state value mt n 2 t Figs 8 9 compare success probabilities BCGP GP quartic polynomial population sizes 100 1 000 The error bars indicate standard error based binomial distribution As expected BCGP better difference statistically signiﬁcant ﬁnal generations With population 1 000 Fig 9 bigger data reported BCGP statistically better equal standard GP Naturally big populations forward backward GP solve quartic polynomial Nevertheless BCGP reaches 100 faster The fourvariate polynomial Poly4 harder Quartic requires large populations solvable runs Fig 10 shows fraction successful runs population 1 000 Fig 11 plots similar data population 10 000 The difference BCGP forward GP statistically signiﬁcant population sizes Poly10 hard We tried 1 000 runs populations 100 1 000 10 000 100 runs 100 000 individuals Neither standard GP BCGP solution runs As illustrated Fig 12 case M 10 000 BCGP average ﬁnds better programs number ﬁtness evaluations In Figs 812 compared forward GP BCGP algorithms given number ﬁtness evaluations Instead Table 2 comparison run number generations G 30 Like BCGA thanks savings obtained avoiding creation evaluation individuals sampled selection unnecessary ancestors end runs BCGP evolved solutions similar ﬁtness conﬁrms statistical equivalence EAs BCEAs took 20 fewer ﬁtness evaluations Similar savings obtained population sizes R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 977 Fig 9 Quartic polynomial regression problem As Fig 8 population 1 000 Fig 10 Fraction successful runs 5 000 runs Poly4 problem forward GP BCGP 30 generations populations 1 000 All tests reported section performed case tournament size n 3 We dont report brevity In cases BCGP superior GP naturally smaller margin 94 Wallclock executiontime comparison To evaluate paging disk access memory impact experiments considered demanding representationsvariablesize GP treesand ran series experiments measuring wallclock 978 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 Fig 11 Fraction successful runs 1 000 runs Poly4 problem forward GP BCGP populations 10 000 Fig 12 Error summed 50 test cases Poly10 regression problem means 1 000 runs populations 10 000 Table 2 Normal GP v Backward chaining Quartic Poly 4 Poly 10 Population 10 000 Generations 30 Means 1 000 runs Problem Quartic Poly4 Poly10 Forward Best Fit 000 012 1112 Evals 300 000 300 000 300 000 Succ Prob 1000 963 00 Backward Best Fit 000 016 1129 Evals 240 321 240 315 240 299 Succ Prob 1000 960 00 Saving 199 199 199 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 979 Table 3 Wallclock perprimitive executiontime comparison GP BCGP GP BCGP Population size 100 0782 µs 0819 µs 1 000 0596 µs 0598 µs 10 000 0592 µs 0591 µs 100 000 0599 µs 0595 µs execution times forward backward GP different population sizes applied Quartic polynomial problem Each run lasted 50 generations All GP parameters Section 93 The results shown Table 3 Results average execution time primitive microseconds Averages computed 10 independent runs dividing total execution time runs number primitives executed runs For fairer comparison report execution time primitive instead total execution time BCGP runs 20 fewer ﬁtness evaluations GP runs Runs performed 3 GHz Linux PC 2 GB memory As clearly Table 3 large populations signiﬁcant differences primitive execution times GP BCGP Also exception populations 100 individuals signiﬁcant differences execution times population size varied The higher execution time pop ulations 100 individuals artifact code collecting statistics requiring nonnegligible proportion computation time small population sizes 10 Discussion In paper focused source inefﬁciency sampling behaviour tournament selection creation evaluation individuals inﬂuence future generations We proposed general methods remove source inefﬁciency speed EAs based tournament selection One methods backward chaining EA provides additional beneﬁt converging faster standard forward algorithm constructing evaluating individuals belonging later generations sooner We analysed algorithms theoretically Section 6 experimentally Section 9 strongly corroborating feasibility approach The implementation backward chaining EA complex added book keeping required limited However doubt BCEAs require memory forward counterparts If prepared accept overhead adopt ideas BCEA computational savings big These achievable exploit transient behaviour algorithm limitdistribution behaviour illustrated Maximum savings achieved nα minimum The smallest value α 1 standard tournament selection minimum n 2 So know best saving 20 ﬁtness evaluations However form tournament selection exists 1222 modify obtain spectacular savings In form tournament selection picks individuals random chooses higher ﬁtness probability p probability 1 p For p 1 form selection equivalent standard tournament selection n 2 form random selection p 05 By acting p possible vary selection pressure method continuously extremes An alternative description method choose higher ﬁtness individual probability q randomly probability 1 q naturally p q 1 q2 1 q2 In case q varied interval 0 1 This second version algorithm modiﬁed purposes Instead ﬁrst choosing pair individuals deciding select best pick random equivalently ﬁrst decide selection strategy going use based randomly draw individuals population If decide best tournament draw individuals population However decide choose randomly members tournament draw random individual population instead drawing individuals randomly discarding With method expected number individuals drawn tournament n 2 q 1 1 q q 1 cid3 2 So clearly smaller q bigger saving expect BCEA Just feel 980 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 order magnitude savings let assume α 1 let use Eq 1 Section 61 estimate expected proportion individuals sampled This approximately e1q So low selection pressures saving 35 ﬁtness evaluations possible Naturally substantial savings obtained exploiting transient behaviour BCEAs In Section 63 showed running BCEA m0 1 EA run 10 times faster However reader probably wonder usefulness evaluating individual generation Normally want generation However need remember individual provided BCEA m0 1 generation G effectively random sample drawn population generation Although expect individual insufﬁcient important question need generation G order solve problem particularly considering EAs substantial loss genotypic diversity population late phases run In 27 experimented implementation BCGP oneoffspring crossover showing run m0 1 BCGP solve problems So cases need population To complete satisfactory answer future work BCEAs need include thorough investigation best way choose m0 G 11 Conclusions In paper analysed sampling behaviour tournament selection multiple generations analysis come demonstrate efﬁcient implementations evolutionary algorithms EAs rooted classical AI previous class EAs In particular proposed new way running EAs backward chainingEA BCEA offers combination fast convergence increased efﬁciency terms ﬁtness evaluations complete statistical equivalence standard EA broad applicability Because interesting properties think class BCEAs area worthy investigation To reiterate BCEA algorithm hard implement discussed Section 7 Also BCEA tends ﬁnd better individuals faster irrespective tournament sizes However wants use tournaments individuals compute large proportion ﬁnal generation computational saving pro vided BCEA limited worth implementation effort memory overhead In applications require computing small number individuals given generation large population BCEA fruitfully applied large tournament size For example BCEA tournament size 7 population million individualswhich unusual EAs GPone calculate 1 individual generation 7 7 individuals generation 6 49 individuals generation 5 cost inferior required initialise population forward EA The information gained way future generations prove important example deciding continue run This information certainly available traditional EA In future research intend test new algorithm realworld problems explore possible ways improving allocation trials decision making BCEAs example replacing current depthﬁrst search strategy informed search algorithm A Applying backward chaining approach forms local selection tournament selection promising area future research Acknowledgements We like thank Chris Stephens Darrell Whitley Kumara Sastry Bob McKay useful com ments The reviewers coeditorinchief charge manuscript warmly thanked help improving References 1 T Baeck DB Fogel Z Michalewicz Eds Oxford Univ Press Oxford 1997 2 T Bäck DB Fogel T Michalewicz Eds Evolutionary Computation 1 Basic Algorithms Operators Institute Physics Publishing 2000 3 W Banzhaf P Nordin RE Keller FD Francone Genetic ProgrammingAn Introduction On Automatic Evolution Computer Programs Applications Morgan Kaufmann January 1998 dpunktverlag R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 981 4 T Blickle L Thiele A mathematical analysis tournament selection LJ Eshelman Ed Proceedings Sixth International Con ference Genetic Algorithms ICGA95 San Francisco CA Morgan Kaufmann 1995 pp 916 5 T Blickle L Thiele A comparison selection schemes evolutionary algorithms Evolutionary Computation 4 4 1997 361394 6 L Davis Ed Handbook Genetic Algorithms Van Nostrand Reinhold New York 1991 7 TE Davis JC Principe A Markov chain framework simple genetic algorithm Evolutionary Computation 1 3 1993 269288 8 KA De Jong WM Spears DF Gordon Using Markov chains analyze GAFOs LD Whitley MD Vose Eds Proceedings Third Workshop Foundations Genetic Algorithms San Francisco CA July 31August 2 1995 Morgan Kaufmann 1995 pp 115138 9 W Feller An Introduction Probability Theory Its Applications vol 2 John Wiley 1971 10 DB Fogel Ed Evolutionary Computation The Fossil Record Selected Readings History Evolutionary Computation IEEE Press 1998 11 LJ Fogel AJ Owens MJ Walsh Artiﬁcial Intelligence Simulated Evolution Wiley New York 1966 12 DE Goldberg K Deb A comparative analysis selection schemes genetic algorithms GJE Rawlins Ed Foundations Genetic Algorithms Morgan Kaufmann 1991 13 DE Goldberg Genetic Algorithms Search Optimization Machine Learning AddisonWesley Reading MA 1989 14 J Holland Adaptation Natural Artiﬁcial Systems University Michigan Press Ann Arbor MI 1975 15 J He X Yao Drift analysis average time complexity evolutionary algorithms Artiﬁcial Intelligence 127 1 2001 5785 16 J He X Yao Towards analytic framework analysing computation time evolutionary algorithms Artiﬁcial Intelligence 145 12 2003 5997 17 JR Koza MA Keane MJ Streeter W Mydlowec J Yu G Lanza Genetic Programming IV Routine HumanCompetitive Machine Intelligence Kluwer Academic 2003 18 JR Koza A genetic approach truck backer upper problem intertwined spiral problem Proceedings IJCNN International Joint Conference Neural Networks vol IV IEEE Press 1992 pp 310318 19 WB Langdon R Poli Foundations Genetic Programming SpringerVerlag Berlin 2002 20 WB Langdon T Soule R Poli JA Foster The evolution size shape L Spector WB Langdon UM OReilly PJ Angeline Eds Advances Genetic Programming 3 MIT Press Cambridge MA June 1999 pp 163190 Chapter 8 21 Z Michalewicz Genetic Algorithms Data Structures Evolution Programs second ed SpringerVerlag Berlin 1994 22 M Mitchell An Introduction Genetic Algorithms MIT Press Cambridge MA 1996 23 T Motoki Calculating expected loss diversity selection schemes Evolutionary Computation 10 4 2002 397422 24 NF McPhee R Poli JE Rowe A schema theory analysis mutation size biases genetic programming linear representations Proceedings 2001 Congress Evolutionary Computation CEC2001 COEX World Trade Center 159 Samseongdong Gangnamgu Seoul Korea 2730 May 2001 IEEE Press 2001 pp 10781085 25 AE Nix MD Vose Modeling genetic algorithms Markov chains Annals Mathematics Artiﬁcial Intelligence 5 1992 7988 26 R Poli B Logan The evolutionary computation cookbook Recipes designing new algorithms Proceedings Second Online Workshop Evolutionary Computation Nagoya Japan March 1996 27 R Poli WB Langdon Backwardchaining genetic programming HG Beyer UM OReilly DV Arnold W Banzhaf C Blum EW Bonabeau E CantuPaz D Dasgupta K Deb JA Foster ED Jong H Lipson X Llora S Mancoridis M Pelikan GR Raidl T Soule AM Tyrrell JP Watson E Zitzler Eds GECCO 2005 Proceedings 2005 Conference Genetic Evolutionary Computation vol 2 Washington DC 2529 June 2005 ACM Press 2005 pp 17771778 28 R Poli NF McPhee General schema theory genetic programming subtreeswapping crossover Part I Evolutionary Computa tion 11 1 March 2003 5366 29 R Poli NF McPhee General schema theory genetic programming subtreeswapping crossover Part II Evolutionary Computa tion 11 2 June 2003 169206 30 R Poli NF McPhee JE Rowe Exact schema theory Markov chain models genetic programming variablelength genetic algorithms homologous crossover Genetic Programming Evolvable Machines 5 1 March 2004 3170 31 R Poli Exact schema theory genetic programming variablelength genetic algorithms onepoint crossover Genetic Programming Evolvable Machines 2 2 June 2001 123163 32 R Poli Tournament selection iterated couponcollection problem backwardchaining evolutionary algorithms Proceedings Foundations Genetic Algorithms Workshop FOGA 8 4th January 2005 33 R Poli JE Rowe NF McPhee Markov chain models GP variablelength GAs homologous crossover Proceedings Genetic Evolutionary Computation Conference GECCO2001 San Francisco CA 711 July 2001 Morgan Kaufmann 2001 34 I Rechenberg Evolutionsstrategie Optimierung technischer Systeme nach Prinzipien der biologischen Evolution FrommannHolzboog Stuttgart 1973 35 SJ Russell P Norvig Artiﬁcial Intelligence A Modern Approach second ed Prentice Hall Englewood Cliffs NJ 2003 36 G Rudolph Convergence analysis canonical genetic algorithm IEEE Transactions Neural Networks 5 1 1994 96101 37 G Rudolph Genetic algorithms T Baeck DB Fogel Z Michalewicz Eds Handbook Evolutionary Computation Oxford University Press Oxford 1997 pp B242027 38 G Rudolph Models stochastic convergence T Baeck DB Fogel Z Michalewicz Eds Handbook Evolutionary Computation Oxford University Press Oxford 1997 pp B2313 39 G Rudolph Stochastic processes T Baeck DB Fogel Z Michalewicz Eds Handbook Evolutionary Computation Oxford Univer sity Press Oxford 1997 pp B2218 40 HP Schwefel Numerical Optimization Computer Models Wiley Chichester 1981 41 K Sastry DE Goldberg Modeling tournament selection replacement apparent added noise Proceedings ANNIE 2001 vol 11 2001 pp 129134 982 R Poli WB Langdon Artiﬁcial Intelligence 170 2006 953982 42 CR Stephens Some exact results coarse grained formulation genetic dynamics L Spector ED Goodman A Wu WB Langdon HM Voigt M Gen S Sen M Dorigo S Pezeshk MH Garzon E Burke Eds Proceedings Genetic Evolutionary Computation Conference GECCO2001 San Francisco CA 711 July 2001 Morgan Kaufmann 2001 pp 631638 43 CR Stephens H Waelbroeck Effective degrees freedom genetic algorithms block hypothesis T Bäck Ed Proceedings Seventh International Conference Genetic Algorithms ICGA97 East Lansing Morgan Kaufmann 1997 pp 3440 44 CR Stephens H Waelbroeck Schemata evolution building blocks Evolutionary Computation 7 2 1999 109124 45 A Sokolov D Whitley Unbiased tournament selection HG Beyer UM OReilly DV Arnold W Banzhaf C Blum EW Bonabeau E CantuPaz D Dasgupta K Deb JA Foster ED Jong H Lipson X Llora S Mancoridis M Pelikan GR Raidl T Soule AM Tyrrell JP Watson E Zitzler Eds GECCO 2005 Proceedings 2005 Conference Genetic Evolutionary Computation vol 2 Washington DC 2529 June ACM Press 2005 pp 11311138 46 A Teller D Andre Automatically choosing number ﬁtness cases The rational allocation trials JR Koza K Deb M Dorigo DB Fogel M Garzon H Iba RL Riolo Eds Genetic Programming 1997 Proceedings Second Annual Conference Stanford University CA 1316 July 1997 Morgan Kaufmann 1997 pp 321328 47 MD Vose The Simple Genetic Algorithm Foundations Theory MIT Press Cambridge MA 1999 48 PH Winston Artiﬁcial Intelligence ed AddisonWesley Reading MA 1992