Artiﬁcial Intelligence 191192 2012 6195 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint GAMoN Discovering MofN latticebased Genetic Algorithm hypotheses text classiﬁcation Veronica L Policicchio Adriana Pietramala Pasquale Rullo Dept Mathematics University Calabria Italy r t c l e n f o b s t r c t Article history Received 16 November 2011 Received revised form 4 July 2012 Accepted 11 July 2012 Available online 20 July 2012 While long history rulebased text classiﬁers best knowledge MofNbased approach text categorization far proposed In paper argue MofN hypotheses particularly suitable model text classiﬁcation task socalled family resemblance metaphor members documents family category share small number features common feature Nevertheless resemble Starting conjecture provide sound extension MofN approach negation enables best ﬁt true structure data disjunction called MofN hypothesis space Based thorough theoretical study MofN partial orders form complete lattices GAMoN taskspeciﬁc Genetic Algorithm GA exploiting latticebased structure hypothesis space eﬃciently induces accurate MofN Benchmarking performed 13 realworld text data sets rule induction algorithms GAs BioHEL OlexGA nonevolutionary algorithms C45 Ripper Further included study linear SVM reported best methods text categorization Experimental results demonstrate GAMoN delivers stateoftheart classiﬁcation performance providing good balance accuracy model complexity Further GAMoN scale large realistic realworld domains better C45 Ripper hypotheses 2012 Elsevier BV All rights reserved 1 Introduction An MofN hypothesis called Boolean threshold function thought intuitively follows Given set N features example satisﬁes M features positive example negative That MofN hypothesis description involves counting properties There literature methods building MofN hypotheses For instance 1 algorithms extracting MofN hypotheses neural networks reported MofN concepts constructed tests induction decision trees 257 However best knowledge MofNbased approach text classiﬁcation far proposed Despite conjecture MofN hypotheses suited model text classiﬁcation task Text categorization TC aimed assigning natural language texts thematic categories basis contents It diﬃcult task essentially main factors hand TC complexity richness natural language allows concept expressed variety constructs words This aspect ampliﬁed presence category documents single narrow subject Corresponding author Email address policicchiomatunicalit VL Policicchio 00043702 matter 2012 Elsevier BV All rights reserved httpdxdoiorg101016jartint201207003 62 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 limited vocabulary On hand TC task deals highly dimensional data sets features Both factors concur unlikely existence set features single feature occur documents given category It happen documents belong category share content words However argued 6 relationship family resemblance holds That documents category share usually small set N features set present document Instead document contains M cid2 N features different documents share features That text classiﬁcation task deals kind data MofN hypotheses able explain A shortcoming MofN approach propositions handle positive information negative evidence deemed play crucial role text categorization This mainly natural languages intrinsically ambiguous negation helps disambiguate concepts word ball ambiguously refer concept sport dance conjunction ball ballroom likely refers sport To overcome drawback extend classical MofN hypotheses negation In addition best ﬁt true structure data allow disjunctions hypotheses That deﬁne new hypothesis language text classiﬁcation called generalizes classical MofN language negation disjunction preliminary description MofN proposed approach 8 In approach classiﬁer propositional formula form Hc H1 c piofPos niofNeg atom note atoms forming Hc share sets Pos Neg Here Pos set positive terms Neg set negative terms pi cid3 0 ni 0 integers called thresholds The meaning atom Hi c classify document d category c pi positive terms occur d strictly ni negative terms occur provides support explicitly modeling interactions positive negative features d That MofN Of course Hc classiﬁes document d c H1 c classiﬁes d c c Hi c Hr Hr c hypothesis atom thresholds p n 1 OlexGA 10 The special case MofN There natural ordering space MofN hypotheses determined kinds subsumption relation ships feature threshold relationships The feature relationship determined feature sets Pos Neg appearing classiﬁer Hc As example assume Hc atomic classiﬁer pofPos nofNeg p 2 n 1 Clearly larger Pos higher probability condition positive features occur docu ment satisﬁed Dually smaller Neg likely document contain negative feature Neg In summary larger Pos smaller Neg general Hc The threshold relationship turn determined thresh olds appearing Hc For instance classiﬁer replace p 2 p 1 new classiﬁer general previous intuitively instead positive features necessary classifying document These relationships deﬁne hierarchies hypotheses precisely complete lattices exploitable effective exploration hypothesis space To end provide suitable reﬁnement operators navigating hypothesis lattices As argued 7 evolutionary approach particularly suited MofN learning task global search style GAs opposed oneattributeatatime greedy approach makes capable catching hidden interactions attributes strongly characterize induction MofN hypotheses However purely nondeterministic nature conventional genetic operators enable search strategy beneﬁt structure hypothesis space To overcome drawback deﬁne taskspeciﬁc Genetic Algorithm GA called GAMoN relying specialized evolutionary operators representing stochastic implementation reﬁnement operators deﬁned subsumption lattices At glance following main characteristics GAMoN It relies variablelength individual representation individual encodes candidate classiﬁer Pittsburgh approach 9 It combines standard search strategy GAs ad hoc generalizingspecializing GS reproduction operators exploit structure hypothesis space It dynamically adapts probability selecting GS operators standard ones It maintains number competing subpopulations It uses F measure assess ﬁtness individual Unlike classical approach feature space simply subset terms vocabulary GAMoN builds hypotheses consists set positive set negative candidate features One main issue general arises inducing classiﬁer selecting appropriate dimensionality feature space features classiﬁer access learning process This important design choice quality selected features strongly determines quality learned classiﬁer especially text classiﬁcation data sets usually highly dimensional noisy ambiguous For systems size feature space managed tuning parameter learning process rerun feature spaces different dimensions best results eventually taken Unfortunately require long training times especially large data sets To inconvenience GAMoN provided techniques automatically detect convenient dimensionality feature space This way manual feature selection preliminarily needed VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 63 GAMoN designed binary classiﬁcation We use onevsall approach produce independent model class multiclass classiﬁcation task technique frequently multilabel classiﬁcation example label case text classiﬁcation We performed extensive empirical analysis aimed comparing proposed approach known learn ing algorithms The experimental results GAMoN provides appealing combination strengths First provides stateoftheart predictive accuracy wide class problem domains Second constructs simple compact models facilitating human comprehension learned Third scale large data sets better stateoftheart rulebased classiﬁers GAMoN implemented Java plugin Weka platform 11 This paper organized follows In Section 2 discuss different current learning techniques In Section 3 provide overview proposed hypothesis language In Section 4 develop thorough theoretical investigation prop erties In Section 5 deﬁne suitable reﬁnement operators exploiting structure hypothesis space In Section 6 state learning problem complexity In Section 7 GA paying particular attention taskspeciﬁc operators In Section 8 experimental framework applied empirical analysis In Section 9 report results comparative study GAMoN GAbased rule induction systems OlexGA 10 BioHEL 1213 nonGA systems Ripper 14 C4515 Platts Sequential Minimal Optimiza tion SMO method linear SVM training 16 In Section 10 provide discussion proposed method relate learning algorithms Finally Section 11 concludes paper 2 Background Various supervised machine learning techniques applied document classiﬁcation An excellent overview 17 SVMs class learning algorithms showed highly accurate data mining tasks In 196 Joachims investigated application text classiﬁcation The results empirical study showed SVMs effective learning algorithms Naive Bayes Rocchio C45 kNearest Neighbor Further linear SVM showed perform nonlinear kernels substantially eﬃciently Naive Bayes NB popular technique classify texts computational eﬃciency simplicity McCallum Nigam 20 investigated main document representations NB text classiﬁcation Bernoulli multinomial They concluded superior accuracy cases However problem Multinomial NB MNB class training examples selects poor weights decision bound ary One additional problem MNB model text To improve performance MNB Rennie et al 21 proposed Complement Naive Bayes CNB While learning conditional probability class CNB uses frequency information pertaining classes uses negative information In different view rule learning algorithms successful strategy classiﬁer induction Direct methods extract rules directly data indirect methods extract rules classiﬁcation models decision trees C45 15 Representative examples direct methods include Inductive Rule Learning IRL systems FOIL 22 Ripper 14 Associative Rule Learning ARL systems CMAR 23 CPAR24 TFPC 25 A subclass inductive rule learners GeneticsBased Machine Learning algorithms GBML 26 rely Evolutionary Algorithms search mechanisms Examples systems XCS 27 SIA 28 GAssist 29 BioHEL 1213 Many GBML systems explicit generalizationspecialization operators 3035 The wellknown rulebased classiﬁers learn texts notably Ripper C45 actually originate nontext data mining 141936 Among examples rulebased systems speciﬁcally designed classify texts mention associative classiﬁer NeW 37 IRL systems Olex 38 OlexGA 10 Olex induces rules consisting positive conjunction zero negative conjunctions It relies search technique greedily selects step conjunct positive negative maximizes F measure training set OlexGA atom thresholds p 1 n 11 GBML special case GAMoN classiﬁer MofN A peculiarity systems explicitly dealing negated features Even prior studies SVMs Complement Naive Bayes particularly effective text categorization rule based text classiﬁers preferred realworld applications provide interpretable models Readability desirable property classiﬁcation models allows human understand possibly modify based priori knowledge However drawback rulebased systems high computational cost especially high dimensional data sets In ARL systems time cost frequent pattern mining increase sharply size data set grows In addition high number rules generated usually requires additional pruning step redundant rules discarded Also IRL systems typically rely twostage process greedy heuristics constructs initial rule set 1 The Olex OlexGA suite downloadable httpwwwmatunicalitOlexGA 64 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 optimization phases improve compactness accuracy rule set similar approach decision tree All makes diﬃcult rule induction methods scale large realistic realworld data sets 3 Language overview The MofN classiﬁer category c propositional formula form Hc H1 c representation generalizes classical notion MofN concepts allowing negation disjunction An MofN piofPos niofNeg atom expressing following condition classify document d category c pi positive features Pos ni negative features Neg occur d Integers pi cid3 0 ni 0 called thresholds Of course Hc classiﬁes document d c H1 c Hi Since atoms forming Hc H1 c c share sets features Pos Neg convenient notation cid5Pos Neg T cid6 T p1 n1 pr nr set threshold pairs appearing atoms Hc Hc T called threshold set For example 1ofPos 2ofNeg 2ofPos 3ofNeg simpler represented cid5Pos Neg 1 2 2 3cid6 c classiﬁes d c c Hr Hr Hr c As concrete example consider classiﬁer constructed GAMoN category grain Reuters data set Hgrain cid2 Pos barley cereals corn grain maize rice sorghum wheat Neg acquisition bank earning pay proﬁt tax york T cid3 1 1 2 2 cid4cid5 This classiﬁer order 2 threshold set elements 1 1 2 2 8 positive features barley cereals 7 negative ones acquisition bank The meaning Hgrain following classify document d category grain following conditions holds 1 d contains exactly positive feature negative features 2 d contains positive feature negative ones That single positive feature effect predicting category grain negative feature occurs d single negative feature effect denying classiﬁcation d positive features occur d As example shows beneﬁcial aspect MofN representation readability This impor tant feature makes possible people visually inspecting understanding induced model The MofN hypothesis space structure determined kinds subsumption relationships feature threshold subsumptions Intuitively positive features indicative membership category contrary negative ones indicative nonmembership Thus elements Pos Neg general classiﬁer cid5Pos Neg T cid6 classiﬁes documents Feature subsumption encodes intuition As example cid5t0 t1 t3 t4 T cid6 subsumes cid5t0 t3 t4 T cid6 subsumed cid5t0 t1 t3 T cid6 The threshold subsumption relationship turn determined threshold sets appearing classiﬁers For stance hypothesis cid5Pos Neg 1 1cid6 subsumes cid5Pos Neg 2 1cid6 instead positive features necessary classify document Thus hierarchies capture intuitive notion generaltospeciﬁc ordering Hc subsumes Hcid7 hierarchy classiﬁed Hcid7 form complete lattices hypothesis space hypothesis reached search space c c classiﬁed Hc One interesting property relationships We advantage generaltospeciﬁc ordering order selectively search hypothesis space For stance classiﬁer cid5Pos Neg 2 1cid6 speciﬁc covers positive examples generalized threshold subsumption replacing threshold set 2 1 restrictive 1 1 ii feature subsumption adding term Pos removing term Neg Another way generalizing specializing hypothesis interaction To end exploit lattice structure hypothesis space That upper bound resp greatest lower bound hypotheses taken lattices order general resp speciﬁc As example given hypotheses sharing threshold sets cid5t0 t1 t3 2 1cid6 cid5t0 t4 t5 2 1cid6 specialize taking greatest lower bound feature subsumption lattice cid5t0 t3 t5 2 1cid6 classiﬁer sets positive negative features t0 t0 t1 t0 t4 t3 t5 t3 t5 respectively Likewise given hypotheses sharing feature sets cid5Pos Neg 1 1cid6 cid5Pos Neg 2 2cid6 specialize taking greatest lower bound threshold subsumption lattice cid5Pos Neg 2 1cid6 classiﬁer threshold set max1 2 min1 2 Fig 1 It easily veriﬁed greatest lower bounds speciﬁc respective parents As later paper concepts basis deﬁnition reﬁnement operators These abstract tools searching hypothesis space ﬁnd concrete application deﬁnition reproduction operators GAMoN 4 Language deﬁnition hypothesis space Now intuitive view basic ideas subsections provide formal deﬁnitions In particular start notion feature space set features provide lexicon VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 65 Fig 1 τ subsumption lattice threshold bounds P 2 N 3 language deﬁne feature cid4φ threshold cid4τ hypotheses built Then formalize MofN subsumption relationships showing number interesting properties In particular prove form complete lattices hypothesis space provide constructive deﬁnition meet join operators lattices Finally deep insight structure hypothesis space notion decision boundary MofN classiﬁers Note For proofs propositions reported section reader referred Appendix A 41 Feature space We given set T training documents called examples set C categories called concepts A document set features called terms feature sequence words word stems Each document T associated category C We denote T c T training set c set training documents associated category c We vocabulary set features occurring documents T Unlike classical deﬁnitions feature space simply subset vocabulary deﬁnition hypotheses feature space consists set positive set negative features This MofN explicitly models interaction positive negative features regarded ﬁrst class citizens Deﬁnition 41 Feature space We given vocabulary V nonnegative integer k scoring function σ assigns score feature V based correlation category c CHI Square 39 Deﬁne feature space Fck size k category c pair cid5Pos c k Neg c k V Neg c kcid6 Pos c k V follows c k set k highest scoring features V category c according σ t Pos Pos c k candidate positive feature c given Pos cid6 N t V t Pos Θ tPos k consider set N terms cooccurring positive candidate features negative examples c k Θ Θt T c cid12 Θt T set training documents containing feature t c k Θt T c training set c With feature t N assign score ηt follows ηt Θ Θt T c Θ T c Θt T c It easily seen 0 ηt cid2 1 In particular term t occurring negative examples positive containing positive feature score ηt 1 On hand ηt 0 t N deﬁnition t cooccurs c k set best k elements candidate positive feature negative example Then deﬁne Neg N according η t Neg c k candidate negative feature c The rationale deﬁnition intuitive candidate positive features supposed capture positive examples characterized high scoring values On contrary candidate negative features deﬁned terms cooccurring positive candidate terms negative examples supposed discard potentially false positive examples For instance feature ball candidate positive ballroom cooccurs 66 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 ball negative examples ballroom negative candidate feature Clearly higher scoring σ t resp ηt term t higher value candidate positive resp negative feature 42 Hypothesis space A hypothesis propositional formula examples given concept The hypothesis language propose section extension MofN language called MofN Deﬁnition 42 Hypothesis language We given feature space Fck cid5Pos N called threshold bounds An MofN follows c kcid6 integers P c k Neg hypothesis classiﬁer category c Fck inductively deﬁned Basis pofPos nofNeg atom 1order classiﬁer 0 cid2 p cid2 P 0 n cid2 N integers called positive negative thresholds respectively Pos Pos c k Neg Neg c k possibly sets features In particular Pos set positive features Neg set negative features This classiﬁer classiﬁes document d category c p positive features occur d n negative features occur d A convenient notation pofPos nofNeg cid5Pos Neg p ncid6 Induction let H1 c cid5Pos Neg T1cid6 rorder classiﬁer H2 c c share sets features Then Hc H1 c H2 cid5Pos Neg T2cid6 sorder classiﬁer note H1 c c classiﬁer order q cid2 r s Hc classiﬁes document c H2 c classiﬁes d c A convenient notation Hc cid5Pos Neg T cid6 T T1 T2 H2 d c H1 As noticed atoms forming rorder classiﬁer hold sets Pos Neg That denote c compact notation cid5Pos Neg T cid6 T p1 n1 pr nr set threshold Hc H1 c pairs appearing atoms Hc Clearly size T order classiﬁer Hr Example 41 The 2order classiﬁer 1ofPos 2ofNeg 2ofPos 3ofNeg represented cid5Pos Neg 1 2 2 3cid6 An atom p 0 n Neg acts acceptor p Pos understood rejector An atom cid5Pos Neg 1 1cid6 coincides OlexGA classiﬁer 10 In general nonacceptor nonrejector atom cid5Pos Neg p ncid6 logically equivalent following propositional formula c T 1 Tk Tk1 Tkm T 1 Tk possible conjunctions p positive terms Pos Tk1 Tkm possible conjunctions n negative terms Neg The rulebased semantics rorder classiﬁer Hc H1 c obvious c generalization base case Hc equivalent union rule sets Hi c 1 cid2 cid2 r Hr We ﬁnally provide deﬁnition hypothesis space Deﬁnition 43 The hypothesis space HFck P N set hypotheses constructible feature space Fck given thresholds bounds P N 43 Ordering hypothesis space There natural ordering hypothesis space determined kinds subsumption relationships feature threshold subsumption 431 Ordering feature dimension Let HT Fck HFck P N hypothesis subspace consisting hypotheses HFck P N having given threshold set T Hypotheses HT Fck said τ homogeneous On HT Fck exists binary relation featuresubsumption φsubsumption short Deﬁnition 44 Featuresubsumption We given classiﬁers HT Fck H1 c cid5Pos2 Neg2 T cid6 H1 H1 c φsubsumes H2 c called φgeneralization H2 c φsubsumed H1 c φspecialization H1 c c Pos2 Pos1 Neg1 c H2 c H2 cid5Pos1 Neg1 T cid6 H2 c cid4φ H2 Neg2 write H1 c c Example 42 According deﬁnition classiﬁer Hc cid5t0 t1 t3 t4 1 1cid6 φsubsumes Hcid7 cid5t0 t3 t4 1 1cid6 φsubsumed Hcid7cid7 cid5t0 t1 t4 1 1cid6 Intuitively subsumption holds classiﬁcation c condition positive features t0 t1 occur d clearly weaker t0 occur d ceteris paribus documents classiﬁer Hc Hcid7 c condition c Dually Hc φsubsumed Hcid7cid7 c VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 67 negative features expressed t4 occur d weaker expressed t3 t4 occur d Next H1 c cid4φ H2 c H1 c classiﬁes documents classiﬁed H2 DHc D set documents classiﬁed Hc document set D c In following denote Proposition 41 Let H1 c H2 c classiﬁers HT Fck Then H1 c cid4φ H2 c implies DH1 c DH2 c The following proposition shows HT Fck cid4φ complete lattice Proposition 42 HT Fck cid4φ complete lattice Indeed H1 c upper bound lubφH1 bound glbφH1 c follows c H2 c H2 c H2 c HT Fck greatest lower lubφH1 b glbφH1 c H2 c H2 c cid5Pos1 Pos2 Neg1 c cid5Pos1 Pos2 Neg1 Neg2 T cid6 Neg2 T cid6 It easy recognize element HT Fck cid5 Neg k T cid6 cid5Pos k T cid6 432 Ordering threshold dimension Let HΦ P N HFck P N hypothesis subspace consisting hypotheses having Φ cid5Pos Negcid6 Pos Neg Fck We classiﬁers HΦ P N φhomogeneous Next subsumption hierarchy exists HΦ P N We thresholdsubsumption τ subsumption short Notation Since φhomogeneous hypotheses share feature sets following ambiguity arises shall represent classiﬁer cid5Pos Neg T cid6 simply T Example 43 The 2order classiﬁer cid5Pos Neg 1 2 2 3cid6 represented simply 1 2 2 3 Deﬁnition 45 Thresholdsubsumption Let T1 p1 n1 T2 p2 n2 threshold sets size 1 Then T1 τ subsumes T2 denoted T1 cid4τ T2 p1 cid2 p2 n1 cid3 n2 hold More general given threshold sets size cid7 cid4τ T1 τ subsumes T2 element p n T2 exists element p p n cid7 T1 p cid7 n cid7 n The relation cid4τ HΦ P N H1 c H2 H2 induces relation HΦ P N follows Given H1 c c H2 c τ subsumes H2 c τ subsumed H1 c τ specialization H1 c c T1 cid4τ T2 write H1 c cid5Pos Neg T1cid6 H2 c cid5Pos Neg T2cid6 c called τ generalization cid4τ H2 c H1 Example 44 Given φhomogeneous atoms H1 c H1 1 2 2 1 H2 c c smaller H2 2 1 H1 c c vice versa holds negative thresholds As general case let H1 c holds positive threshold 1 2 H2 c cid4τ H2 c 1 1 φhomogeneous classiﬁers Since 1 2 cid4τ 1 1 holds H1 c cid4τ H2 c follows Next H1 c cid4τ H2 c document classiﬁed H2 c classiﬁed H1 c Proposition 43 Let H1 c H2 c classiﬁers HΦ P N Then H1 c cid4τ H2 c implies DH1 c DH2 c Unlike cid4φ binary relation cid4τ partial order Example 45 Classiﬁers H1 c 1 2 H2 c 1 2 2 2 H1 c cid4τ H2 c H2 c cid4τ H1 c hold cid4τ H2 Deﬁnition 46 Equivalence minimality Two φhomogeneous classiﬁers H1 H1 H2 c If classiﬁer Hc expressed H1 c redundant Otherwise Hc minimal If Hc cid5Pos Neg T cid6 minimal T minimal H1 H1 c H2 c H1 c equivalent denoted H1 cid4τ H2 c cid4τ H1 c H2 c strictly τ subsumes H2 c H2 cid4τ H1 c c c c c τ H2 c H1 c cid4τ H2 c H1 c H2 c H2 c c Hc c denoted Example 46 Classiﬁers H1 Hc H1 c H1 c It easily recognized Hc H1 H2 c c holds c H2 1 1 2 2 H2 c c Example 45 equivalent Classiﬁer Hc 1 1 3 1 2 2 redundant cid4τ H2 c minimal c holds On contrary H1 3 1 H1 c 68 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 The notion equivalence encodes intuition equivalent hypotheses provide classiﬁcation behavior In fact Proposition 43 immediately follows equivalent classiﬁers classify documents The lemma proposition vice versa holds classiﬁers classify documents equiva lent Lemma 41 Let Hc H1 c DH1 c D ˆH2 H2 c DH2 c ˆHc ˆH1 c D ˆH2 c c ˆH2 c given Then DHc D ˆHc DH1 c D ˆH1 c DH2 c D ˆH1 c Proposition 44 Let Hc Hcid7 c classiﬁers HΦ P N Then DHc DHcid7 c implies Hc cid4τ Hcid7 c From proposition Proposition 43 immediately follows following statement Corollary 41 Given classiﬁers H1 c H2 c H1 c H2 c iff DH1 c DH2 c Next number interesting properties classiﬁers Proposition 45 Let Hc cid5Pos Neg T cid6 given Then 1 Hc redundant iff exist pi ni p j n j T pi ni cid4τ p j n j 2 Hc minimal iff T p1 n1 pr nr pi p j ni n j vice versa j 1 r 3 If Hc H1 c c Hc H1 c c H1 cid4τ H2 H2 c Example 47 According Part 1 Proposition 45 classiﬁer Hc 1 1 2 1 2 2 redundant 1 1 cid4τ 2 1 H1 c satisﬁes condition p1 p2 n1 n2 c Part 2 Proposition 45 Since Hc H1 c follows Part 3 c Proposition 45 1 1 2 2 minimal It easily veriﬁed H1 H2 2 1 H1 c c H2 c Hc H1 cid4τ H2 c Another interesting property HΦ P N classiﬁers H1 ﬁer Hc τ specialization H1 H2 c Hc equivalent logical AND H1 constructive deﬁnition andH1 cid4τ forms complete lattice set minimal classiﬁers c H2 c exists classi c c Next provide c As shortly deﬁnition preliminary step showing c classiﬁes exactly documents classiﬁed H1 c We denote classiﬁer andH1 c H2 c H2 c H2 c H2 Deﬁnition 47 AND classiﬁers Given H1 c H2 c HΦ P N andH1 c H2 c classiﬁer inductively deﬁned follows Basis H1 c Minn1 n2 p1 n1 H2 c p2 n2 atoms andH1 c H2 c p n p Maxp1 p2 n Inductive step H1 c andH11 c H21 c H2 andH11 H11 c H12 c H22 c c H21 H2 c H3 andH12 c H22 c H21 c c andH1 H4 andH12 c H2 c H22 c c H1 H2 H3 H4 H1 Example 48 If H1 c inition Intuitively andH1 negative speciﬁc atom As example H3 c andH3 1 2 H2 c c H2 c speciﬁc H1 2 3 andH1 c H2 c H2 c 1 1 2 1 2 2 inductive step deﬁnition Notice andH3 c max1 2 min2 3 2 2 base step def c higher positive threshold lower 0 1 2 2 1 1 2 3 H4 c c minimal c H4 c H4 Proposition 46 Given Hc ˆHc HΦ P N classiﬁer andHc ˆHc 1 andHc ˆHc HΦ P N 2 DandHc ˆHc DHc D ˆHc 3 Hc cid4τ andHc ˆHc ˆHc cid4τ andHc ˆHc c H2 Example 49 In Example 48 seen andH1 andH1 recognized document satisfying condition classiﬁed H1 H1 andH1 2 3 Hence c classiﬁes document d d contains x cid3 2 positive features y 3 negative features immediately c On hand d classiﬁed c contains x cid3 max1 2 positive features y min2 3 negative features d classiﬁed c 2 2 H1 1 2 H2 c c H2 c H2 c H2 c H2 c c The result shows inclusion operator deﬁnition classiﬁer increase expressivity language redundant Now turn attention minimal classiﬁers The following proposition shows key result uniqueness minimal classiﬁer equivalence class VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 69 Functions cid18T1 T2 cid19T1 T2 1 function MinimizeT 2 4 return T drop T p n st p 5 function cid19T1 T2 6 return MinimizeT1 T2 cid7 n cid7 T st p cid7 n cid7 cid2τ p n T p n T1 8 function cid18T1 T2 9 10 11 12 13 return MinimizeT cid7 T2 cid7 n T T Maxp p p cid7 Minn n cid7 Fig 2 Computation cid19T1 T2 glbτ T1 T2 Proposition 47 Any equivalence class partitioned hypothesis subspace HΦ P N relation unique minimal classiﬁer We denote MinHc minimal classiﬁer equivalence class Hc From restrict attention set minimal classiﬁers MΦ P N HΦ P N It immediate recognize restriction binary relation cid4τ MΦ P N partial order More precisely complete lattice Proposition 48 The poset MΦ P N cid4τ MΦ P N set minimal classiﬁers HΦ P N complete lattice Indeed elements H1 c upper c H2 bound lubτ H1 c follows c MΦ P N greatest lower bound glbτ H1 c H2 c H2 lubτ H1 c H2 H2 H1 c c c H2 b glbτ H1 andH1 c H2 c c MinH1 c H2 c upper bound H1 c H2 c minimal classiﬁer equivalence class c MinandH1 c H2 c greatest lower bound H1 c H2 c minimal classiﬁer equivalence class Example 410 The τ subsumption lattice threshold bounds P 2 N 3 depicted Fig 1 How 19 classiﬁers general 0 3 speciﬁc 2 1 Further maximum order classiﬁer 3 order 0 1 1 2 2 3 We conclude section providing constructive deﬁnition lubτ glbτ Let H1 cid5Pos Neg T2cid6 Now Proposition 48 lubτ H1 H2 c Deﬁnition 42 lubτ H1 discarding p n exists p tion 45 Part 1 We denote MinT1 T2 cid19T1 T2 lubτ H1 tion 48 glbτ H1 c H2 Deﬁnition 47 minimized shown glbτ H1 cid5Pos Neg T1cid6 c Mincid5Pos Neg T1 T2cid6 c lubτ H1 c cid5Pos Neg MinT1 T2cid6 MinT1 T2 obtained T1 T2 simply cid7 cid4τ p n immediate Proposi cid7 n c cid5Pos Neg cid19T1 T2cid6 Likewise Proposi c We denote cid18T1 T2 threshold set constructed cid7 T1 T2 p c cid5Pos Neg cid18T1 T2cid6 c MinandH1 c MinH1 c H2 c H2 c H2 c H2 c H2 c H2 H2 cid7 n c c cid7 cid5Pos Neg T1cid6 H2 Proposition 49 Let H1 c c cid2 cid5 Pos Neg cid19T1 T2 cid2 cid5 Pos Neg cid18T1 T2 c H2 cid8 lub τ cid7 H1 H1 c H2 cid8 c c glb τ cid5Pos Neg T2cid6 minimal classiﬁers MΦ P N Then cid19T1 T2 cid18T1 T2 constructively deﬁned shown Fig 2 A proof correctness algorithms Fig 2 reported Appendix A 44 The minimal hypothesis space In previous subsection deﬁned notion minimal classiﬁer representative hypothesis equiva lence class Minimality desirable property classiﬁers guaranteeing uniqueness representation imposes ordered structure hypothesis space For reason restrict minimal classiﬁers 70 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 k Neg Deﬁnition 48 Let feature space Fck cid5Pos hypothesis space constructible Fck given P N values MFck P N cid4τ cid4φ kcid6 threshold bounds P N given The minimal cid7 M Fck P N cid8 cid9 MΦ P N st Φ cid3 cid5Pos Negcid6 cid10 cid10 Pos Pos k Neg Neg cid4 k Φ Thus minimal hypothesis space uniquely determined k P N Using previously deﬁned notational convention following denote Mτ Fck set minimal classiﬁers MFck P N threshold set T It immediate recognize given minimal threshold set T Mτ Fck Hτ Fck coincide consist minimal classiﬁers threshold set T constructible Fck It turns Mτ Fck cid4φ Hτ Fck cid4φ coincide Next discuss structure MFck P N cid4τ cid4φ determined subsumption relations Since φsubsumption τ subsumption lattices basic building blocks minimal hypothesis space start discussion preliminarily showing size lattices 441 The size types lattice A φsubsumption lattice MT Fck consists given T hypotheses built given feature space Fck hypothesis corresponding particular choice sets Pos Neg Fck It immediate recognize following fact Fact 41 The size MT Fck equal number sets Pos Neg constructible feature space Fck cid5Pos kcid6 HT Fck 22k k Neg A τ subsumption lattice MΦ P N consists given Φ cid5Pos Negcid6 hypotheses built given threshold bounds P N hypothesis corresponding particular threshold set satisfying P N The lemma proposition size MΦ P N maximum order classiﬁer Lemma 42 Given threshold bounds P N k cid2 MinP 1 N let T integers cid2 k 0 cid2 pi cid2 P 0 ni cid2 N Then exists unique subset S T threshold set k p1 pk T T n1 nk sets k having size k minimal k k Proposition 410 Given threshold bounds P N 1 maximum order classiﬁer MΦ P N MinP 1 N 2 number minimal threshold sets constructed given bounds cid13 cid13 cid12 cid12 N j 1 λP N MinP 1Ncid11 j1 P 1 j 442 The landscape τ subsumption perspective cid7 Neg Given threshold bounds P N let consider lattices MΦ P N cid4τ MΦcid7 P N cid4τ Φ cid5Pos Negcid6 cid7cid6 By Proposition 410 number λP N classiﬁers constructible Φcid7 cid5Pos given P N Hence correspondence g classiﬁers MΦ P N MΦcid7 P N related classiﬁers having threshold sets Since τ subsumption relation classiﬁers determined τ subsumption relation respective threshold sets Deﬁnition 45 clearly H1 cid4τ H2 c c holds MΦ P N cid4τ iff gH1 c holds MΦcid7 P N cid4τ That lattices isomorphic Further cid7cid6 classiﬁers MΦ P N share feature sets cid5Pos Negcid6 MΦcid7 P N share feature sets cid5Pos MΦ P N MΦcid7 P N disjoint Since number different Φs pairs sets Pos Neg constructible given feature space Fck 22k conclude MFck P N cid4τ structure 22k isomorphic disjoint lattices MΦ P N cid4τ size λP N For instance given P 2 N 3 MFck 2 3 cid4τ consists 22k lattices structure depicted Fig 1 c cid4τ gH2 cid7 Neg Fact 42 The partial order MFck P N cid4τ consists 22k isomorphic disjoint lattices MΦ P N cid4τ size λP N 443 The landscape φsubsumption perspective The φsubsumption perspective course dual τ subsumption Consider lattices MT Fck cid4φ MT cid7 Fck cid4φ T T cid7 As stated Fact 41 size 22k Since relationship cid4φ classiﬁers determined inclusion relationship respective sets features Deﬁnition 44 structure lattices depend T Hence MT Fck MT cid7 Fck isomorphic cid4φ Since hypothesis MT Fck threshold set T hypothesis MT cid7 Fck threshold set T cid7 MT Fck MT cid7 Fck disjoint Therefore hypothesis space MFck P N cid4φ exist λP N isomorphic disjoint lattices MT Fck cid4φ size 22k VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 71 Fig 3 Decision boundaries 1 2 left 1 2 2 3 right Fact 43 The partial order MFck P N cid4φ consists λP N isomorphic disjoint lattices Mτ Fck cid4τ size 22k 45 Decision boundaries There interesting graphical representation classiﬁer Hc cid5Pos Neg T cid6 2dimensional space N 2 Fig 3 Here point x y x y nonnegative integers labeled pair integers cid5π x y νx ycid6 π x y number positive examples documents νx y number negative ones containing exactly x features Pos y features Neg Intuitively think point x y identifying set positive negative examples x positive features y negative ones Hence region plane cid3 RHc x y cid10 cid10 x cid2 Pos y cid2 Neg pi ni T st x cid3 pi y ni cid14 cid4 points satisfy threshold conditions identiﬁes documents classiﬁed Hc RHc classiﬁcation region It turns number documents classiﬁed Hc π x y νx y The border region RHc decision boundary Hc As example classiﬁcation regions φhomogeneous classiﬁers Hc 1 2 Hcid7 1 2 2 3 x yRHc c depicted Fig 3 Here following noted 1 decision boundary atom Hc rectangle left Fig 3 2order classiﬁer Hcid7 c overlapping rectangles atom right Fig 3 2 classiﬁcation region Hc τ specialization Hcid7 c contained classiﬁcation region Hc c Hc Hcid7 The statements generalized In particular concerning point 2 easily veriﬁed classiﬁers Hcid7 RHc holds vice versa suﬃces use deﬁnition classiﬁcation region Deﬁnition 45 As point 1 state decision boundary classiﬁer H1 c stepwise nondecreasing polyline N 2 consisting alternately vertical horizontal segments To c suﬃces observe following cid4τ Hc condition RHcid7 Hr c c decision boundary single atom Hi c satisfy test conditions pi cid2 x cid2 Pos 0 cid2 y ni pi ni 1 cid2 cid2 r rectangle subtending points x y b r atoms H1 c Hr c pi1 pi ni1 ni 1 r Proposition 45 Part 2 Intuitively nondecreasingness decision boundaries implies documents likely belong category c documents positive features negative ones likely classiﬁed Hc For instance consider documents dx y d positive features respectively cid7 cid3 x containing number y negative features Intuitively dx y likely positive example x cid7 y holds positive features indicative membership number negative c d ones On hand boundary nondecreasing happens dx y likely fall RHc d cid7 y dx y likely classiﬁed Hc cid7 cid7 y having x x cid7x cid7x cid7x 46 Remarks proposed language The family resemblance metaphor In binary classiﬁcation task families classes positive P negative N Let assume atom pofPos nofNeg characterize members P 72 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 Here Pos set features members share threshold p states features member hold Symmetrically Neg set features shared members family N Actually members speciﬁcally similar members P recall Deﬁnition 41 features Neg characterize members N holding features family P Thus example exhibits p positive features member P provided holds n negative features To use analogy imagine members Brown family hold following features green eyes black hair tallness However White family members tall green eyes hold following features fair hair long nose high forehead Thus individual tall green eyes belongs Brown family provided possesses features play role negative features Brown family threshold n 2 On expressivity MofN MofN hypotheses regarded MofN atoms positive features atoms form pofPos Simple MofN hypotheses suﬃcient classiﬁcation new unseen data known cases need negative features avoided A simple example following document t0 t1 belongs c document t0 t1 t2 belongs c It easy recognize scenario modeled atoms Hc cid5t0 t2 1 1cid6 Hccid7 cid5t0 t2 2 1cid6 Hccid7cid7 cid5t1 t0 1 1cid6 negative features needed discriminate classes document t1 t2 belongs c cid7cid7 cid7 Although MofN atoms surpass classical MofN hypotheses expressive power data sets represented simply atoms As example assume documents d1 t0 d2 t0 t1 d3 t0 t1 t2 associated category c d4 t0 t2 Intuitively correctly classify data need hypothesis Hc stating following occurrence t0 t1 suﬃcient order document d classiﬁed c provided t2 appear d t2 appear d stronger condition needed t0 t1 occur d We easily recognize Hc 2order classiﬁer cid5t0 t1 t2 1 1 2 2cid6 atomic equivalent classiﬁer exists However proposed language improves expressive power MofN concepts MofN ac hypothesis capable explaining tually reach expressiveness DNF For instance MofN following data d1 t0 d2 t1 t2 d3 t0 t2 d1 d2 belonging class c d3 complement It easy recognize reason limitation atoms forming hypothesis share sets positive negative features As sections rationale choice drastically restricts search space That effectiveness tradedof eﬃciency Why subsumption relations important As seen relations cid4τ cid4φ codify intuitive notion c H2 c c example covered H 2 c H1 moregeneralthan hypotheses That given H1 covered H1 cid4τ H2 c c The idea ordering concept space moregeneralthan relation new Inductive Logic Programming 40 What actually original approach ordering dimensions feature threshold dimensions The ordering relations important provide learning algorithm means selectively search hypothesis space For instance search strategy general hypothesis positive examples covered current vice versa speciﬁc hypothesis negative examples covered The implementation selective search requires deﬁnition suitable operators exploiting subsump tion relations enable generalizationspecialization hypothesis This section 5 Reﬁnement operators Informally reﬁnement operator function enables navigate space minimal classiﬁers partial order relations We provide classes reﬁnement operators unary binary reﬁnement operators Notation For sake simplicity following deﬁnitions denote set minimal hypotheses MFck P N simply M 51 Unary reﬁnement operators A unary reﬁnement operator nondeterministic function returns neighbor Hc φsubsumption τ subsumption relationship It classiﬁer step upward downward hierarchies Deﬁnition 51 Unary reﬁnement operators A unary reﬁnement operator nondeterministic function M M In par ticular unary xgeneralization operator denoted x x φ τ cid2Hcid7 c c Hcid7 c x Hc Hc element x Hc Hcid7 c x Hc Theunary x specialization operator x deﬁned accordingly function x Hc Hc c x Hc cid2Hcid7cid7 M Hcid7 c Hcid7 c x Hcid7cid7 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 73 We ﬁrst provide constructive deﬁnition φ Hc φ Hc φsubsumption lattice Informally direct ancestor Hc φsubsumption hierarchy obtained Hc adding Pos candidate positive term removing term Neg direct descendant obtained dual way cid23 COMPUTATION φ Hc φ Hc Given Hc cid5Pos Neg T cid6 compute φ Hc Hc Pos Pos k Neg Hc element φsubsumption lattice cid15 φ Hc cid5Pos t Neg T cid6 t Pos cid5Pos Neg t T cid6 t Neg k φ Hc Hc Pos Neg Neg cid15 k φ Hc cid5Pos t Neg T cid6 t Pos k cid5Pos Neg t T cid6 t Neg cid2 A proof correctness computation reported Appendix A Example 51 Given Hc cid5t0 t1 t2 T cid6 let t Pos hypotheses neighbors Hc φsubsumption hierarchy cid5 cid5 k t cid4 cid5 φ Hc φ Hc cid2 t0 t1 cid2 t0 t1 T T cid3 t2 t cid7 cid2 t0 t1 t t2 T cid2 t1 t2 T cid5 φ Hc φ Hc cid7 Neg k candidate features Then following Let neighbor τ Hc τ Hc Hc τ subsumption lattice computed Clearly obtain τ Hc replace Hc threshold set T immediate ancestor T τ subsumption lattice So problem reduces computation T cid23 COMPUTATION τ Hc τ Hc Given Hc cid5Pos Neg T cid6 compute τ Hc τ Hc follows τ Hc cid5Pos Neg T cid6 τ Hc cid5Pos Neg T cid6 nondeterministic operator resp applied T returns immediate ancestor resp descendant T τ subsumption hierarchy T constructed T algorithm Fig 4 report dual algorithm T space reason cid2 For description algorithm Fig 4 reader referred Appendix B 52 Binary reﬁnement operators Binary reﬁnement operators aimed exploiting lattice structure φsubsumption τ subsumption hierarchies In particular given classiﬁers return classiﬁer lub glb classiﬁers subsumption lattices depending generalization specialization needed respectively Deﬁnition 52 Binary reﬁnement operators A binary reﬁnement operator function M M M Let classiﬁers cid5Pos2 Neg2 T2cid6 given There binary generalization operators τ generalization cid5Pos1 Neg1 T1cid6 H2 H1 cid16 cid16 c c τ φgeneralization φ deﬁned follows cid17 τ cid17 φ cid7 H1 cid7 c H2 c H1 c H2 c cid8 cid8 cid2 cid5 Pos1 Neg1 cid19T1 T2 cid5Pos1 Pos2 Neg1 cid18 Neg2 T1cid6 cid18 binary specialization operators cid8 τ cid2 cid5 Pos1 Neg1 cid18T1 T2 c H2 cid19 cid7 c φ deﬁned follows H1 cid7 τ cid19 φ cid8 H1 c H2 c cid5Pos1 Pos2 Neg1 Neg2 T1cid6 It noted operators commutative In fact xH1 ization H1 c yields specialization H1 c H2 c c H2 cid18 c H2 xH1 c x τ φ yields general c H2 c cid16 74 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 Nondeterministic function T Input threshold bounds P N minimal threshold set T τ1 τk τi pi ni 1 k pi pi1 ni ni1 1 k Proposition 45 Output direct ancestor T T px p p y nx n n y p n p y 1 nx 1 function NewElement X Y 1 px p y swap X px nx Y p y n y 2 δ p y px δ n y nx 3 δ 1 δ 1 compute speciﬁc threshold pair p n 4 5 compute speciﬁc threshold pair p n px cid3 p cid3 p y 6 7 8 9 10 return p n nx cid3 n cid3 n y δ 1 p n cid2τ Y set p n p y 1 n y δ 1 p n cid2τ X set p n px nx 1 p n cid2τ X p n cid2τ Y set p n px n y T 0 N T lattice return τ0 p0 n0 1 0 τk1 pk1 nk1 P 1 N 1 randomly select 1 k 1 pi 0 adj τi1 right adjacent k ni N adj τi1 left adjacent 11 begin 12 13 14 15 16 17 18 return T MinimizeT NewElementτi adj randomly select adj τi1 τi1 Fig 4 Pseudo code random selection direct ancestor threshold set τ subsumption lattice Fig 5 Given H1 c classiﬁer cid5Pos1 Neg1 T2cid6 cid5Pos1 Neg1 T1cid6 H2 cid16 c τ H1 c H2 c cid5Pos1 Neg1 cid19T1 T2cid6 cid5Pos2 Neg2 T2cid6 hypothesis cid16 τ H1 c H2 c upper bound H1 c φhomogeneous cid16 Intuitively c H2 c Fig 5 Dually τ H1 H2 feature sets H2 c upper bound H1 cid16 c upper bound H1 c specialization operators deﬁned accordingly φH1 c H2 c φhomogeneous classiﬁer having threshold set c τ homogeneous classiﬁer having Example 52 Consider H1 c Deﬁnition 52 cid5Pos1 Neg1 T1cid6 H2 c cid5Pos2 Neg2 T2cid6 T1 2 2 T2 1 1 According cid17 τ cid19 τ cid7 H1 cid7 H1 c H2 c c H2 c cid8 cid8 cid2 cid5 Pos1 Neg1 cid19T1 T2 cid2 cid5 Pos1 Neg1 cid18T1 T2 cid16 cid2 cid2 Pos1 Neg1 cid3 cid3 cid4cid5 1 1 2 2 cid4cid5 Pos1 Neg1 2 1 cid18 It easily veriﬁed τ H1 c H2 c generalization H1 c Now assume Pos1 t1 t2 Neg1 cid16 t3 t4 Pos2 t2 t5 Neg2 φ operator follows cid16 cid8 cid7 H1 c H2 c φ cid5Pos1 Pos2 Neg1 Neg2 T1cid6 cid2 t1 t2 t5 t3 T1 cid5 τ specialization H1 c c H2 t3 We generalize H1 c specialize H1 c H2 c VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 75 cid18 φ follows cid18 cid7 φ H1 c H2 c cid8 cid5Pos1 Pos2 Neg1 Neg2 T1cid6 cid2 t2 t3 t4 T1 cid18 cid5 It easy recognize cid16 φH1 c H2 c cid4φ H1 c H1 c cid4φ φH1 c H2 c hold Proposition 51 Given classiﬁers H1 c H2 c following hold cid16 H1 c xH1 cid4x c H2 cid18 c cid4x H1 c c H2 xH1 c x τ φ 6 Learning problem complexity Before providing effective algorithm learning classiﬁers section deﬁnition learning problem complexity The goal ﬁnd category c C minimal hypothesis Hc MFck P N best ﬁts training data To end assume categories C mutually independent learning task consists C independent binary subtasks category To assess Hc use F measure This measure trades precision Pr versus recall Re deﬁned harmonic mean Pr Re follows2 F 2 Pr Re Pr Re 2 Let denote F Hc T F measure obtained Hc applied documents training set T Now learning problem formulated following optimization problem Deﬁnition 61 Learning problem Let feature space Fck threshold bounds P N given The learning problem ﬁnd minimal classiﬁer Hc MFck P N maximizes F measure F Hc T Hc training set T The learning problem essentially instance Inductive Logic Programming ILP 41 deals general problem inducing logic programs examples presence background knowledge It known ILP problems computationally intractable Proposition 61 The decision version learning problem NPcomplete The reader referred Appendix A proof statement The theory PAClearnability ﬁrst proposed Valiant 42 provides model approximated polynomial learning polynomially bound resources number examples computational time tradedoff accuracy induced hypothesis However shown proposition algorithm produces consistent MofN hypotheses PAC hypothesis p examples time polynomial p MofN learnable surprising given MofN concepts PAClearnable Pitt Valiant 43 7 Learning classiﬁer GAbased approach So far seen structural properties MofN hypothesis space designed set reﬁnement operators search abstract tools Further deﬁned learning problem showed compu tationally diﬃcult In section provide effective algorithm learning classiﬁers MofN hypothesis space In particular propose heuristic approach based Genetic Algorithm GA A GA represents known powerful domainindependent search technique based natural evolutionary opera tors A standard GA regarded composed basic elements 1 A population set candidate solutions classiﬁers called individuals chromosomes evolve number iterations generations 2 ﬁtness 2 This known F 1measure recall precision evenly weighted 76 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 function assign score individual population 3 evolution mechanism based operators elitism selection crossover mutation A comprehensive description GAs 44 GAs showed suited learning classiﬁcation rules 292827 MofN hypotheses 7 perform thorough search hypothesis space limited greedy search bias However GAs disadvantages rule discovery For instance conventional genetic operators crossover mutation mally applied directly trying optimize quality new candidate solution exploiting structure hypothesis space A recent research trend aimed overcoming drawback combining standard search strategy GAs taskspeciﬁc genetic operators incorporate knowledge speciﬁc applica tion 3033 application mean task inducing classiﬁcation rules Next present GAMoN taskspeciﬁc GA designed induce MofN hypotheses As GAMoN relies search strategy ad hoc selective reproduction operators aimed exploiting structure hypothesis space combined standard ones Detecting best hypothesis space MFck P N explored GAMoN fundamental task strongly affects quality learning process In principle 1 manage model parameters k P N uniquely determines hypothesis space parameters manually tuned 2 embed evolutive dynamics GA letting adaptively evolve best values GAMoN incorporates approach To end evolution relies number competing subpopulations Sk1 P 1 N1 Skn Pn Nn Ski P Ni consists individuals encoding classiﬁers hypothesis space MiFcki P Ni 1 cid2 cid2 n A preliminary step creation subpopulations Sk1 P 1 N1 Skn Pn Nn detection suitable range kmin kmax feature space dimensionality ki subpopulation This subject subsection Afterward discuss individual encoding reproduction operators Then report detailed description genetic algorithm GAMoN ﬁnally provide remarks proposed GA 71 Detecting feature space dimensionality The feature space Fck provides basic symbols classiﬁers given hypothesis space MFck P N constructed Behind deﬁnition implicit assumption selected terms representative category learned rest redundant Thus predicting right value dimensionality k crucial step On hand reduced feature space desirable redundant noisy features deceive learning algorithm detrimental effect classiﬁcation results particularly true text classiﬁcation task data sets usually noisy ambiguous Further reducing number features makes learning process eﬃcient especially evolutionary approach large feature spaces entail large individuals match operations On hand aggressive feature selection discard features carry essential information Next provide criterion inspired proposed 36 detecting range dimensionality values based statistical characteristics data set hand Deﬁnition 71 Dimensionality range We given vocabulary V c scoring function σ We deﬁne dimensionality range kmin kmax category c follows cid10 cid4cid10 cid10 σ t c cid3 m s cid10 cid10 cid4cid10 cid10 σ t c cid3 m 3s cid10 cid10 cid3 cid10 t V c cid10 cid3 cid10 t V c kmin kmax σ t c score feature t V c wrt category c m s average standard deviation scoring values respectively c k candidate positive features We notice deﬁnition essentially aimed selecting good set Pos c k Deﬁnition 41 Indeed determine kmin c k consists terms cooccurring terms Pos recall Neg resp kmax compute scoring function σ features count number features score higher 1 resp 3 standard deviations average features high discriminating power 72 Individual encoding Given hypothesis space MFck P N candidate minimal classiﬁer Hc cid5Pos Neg T cid6 MFck P N encoded bit string I cid5I I I cid2τ cid6 1 positive component I ti Pos c k A 1 0 gene I encode Pos Pos c k It k bits associated candidate feature encode Neg Neg ti 1 cid2 cid2 k indicates ti Pos c k belongs Pos c k It k bits associated candidate ti 1 cid2 cid2 k indicates ith candidate feature ti c k A 1 0 gene I 2 The negative component I feature ti Neg belongs Neg VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 77 cid2τ 3 The threshold component I encode threshold set T The encoding T relies straightforward binary representation pairs p n T 0 cid2 p cid2 P 0 n cid2 N One additional bit element p n MinP 1 Ncid24logNP 1cid25 1 represent presenceabsence element Thus length I MinP 1 N maximum order classiﬁer threshold bounds P N Proposition 410 In following denote enc encoding function I cid2τ encT cid2τ It turns length LI I following function k P N cid8 cid7cid20 cid7 cid7 cid7 cid8 cid7 cid8 LI L I L I L cid2τ I 2k MinP 1 N log NP 1 cid8cid21 cid8 1 Clearly individuals encoding classiﬁers hypothesis space MFck P N equal length Example 71 Let hypothesis space MFck P N given k 50 P 2 N 3 According Proposi tion 410 maximum order classiﬁer MinP 1 N Min3 3 3 Example 410 Thus individual encoding classiﬁer cid5Pos Neg T cid6 MFck P N consists 2k 100 bits needed represent sets Pos Neg fur ther MinP 1 Ncid24logNP 1cid25 1 15 bits encode threshold set T 73 Fitness The performance measure evaluating ﬁtness individual objective function learning prob lem Deﬁnition 61 Deﬁnition 72 Fitness We given chromosome I encoding classiﬁer Hc training set T The ﬁtness I F Hc T 74 Taskspeciﬁc GA operators stochastic reﬁnement Next propose applicationspeciﬁc reproduction operators implementation reﬁnement operators deﬁned Section 5 Such operators provide concrete means learning algorithm selectively searches hypothesis space In particular deﬁne classes GeneralizingSpecializing GS operators GS Crossover GS Mutation 741 Generalizingspecializing crossover Crossover operation swapping genetical material individuals parents GS crossover GSX special kind crossover aimed making classiﬁer general speciﬁc The GSX operators deﬁning application binary reﬁnement operators given Deﬁnition 52 As seen combine classiﬁers hypothesis space provide new classiﬁer space Thus GSX operators combine parents belonging subpopulation encoding classiﬁers hypothesis space yields individual subpopulation Therefore operate individuals equal length iso morphic Notation With small abuse notation following denote encoding classiﬁers 1 cid2 cid2 2 Further write I1 cid4x I2 H1 c c H2 xH1 cid4x H2 c xI1 I2 individuals c respectively x τ φ I binary encoding Hi c xI1 I2 c c H2 xH1 cid18 cid16 cid16 cid18 Deﬁnition 73 GSX operators We given individuals I1 I2 encoding classiﬁers H1 The generalization crossover GXI1 I2 I1 I2 individual encoding binary φgeneralization c H2 binary τ generalization τ I1 I2 probability p 05 φI1 I2 c More precisely agreed notation GXI1 I2 c H1 cid15 cid16 cid16 τ H1 c H2 c H2 cid16 c MFck P N respectively c H2 c φH1 cid16 The specialization crossover operator SXI1 I2 deﬁned accordingly cid18 cid16 x place x x τ φ Based Deﬁnition 52 implementation operations OR AND I1 I2 follows cid16 φI1 I2 cid18 φI1 I2 achieved simple bitwise logical cid16 cid18 Here I φI1 I2 I st I φI1 I2 I st I ORI 1 I ORI ANDI 1 I 1 I 2 I 2 I ANDI ORI 1 I 1 I ti ORI 2 I 2 I 1 cid2τ I cid2τ I cid2τ 1 cid2τ 1 2 stands 1 k I ti I 2 ti AND deﬁned accordingly 78 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 Fig 6 Relevance ρt irrelevance ρt functions Terms t0 tn increasing order scoring value However problem implementation In fact performing blind OR AND individuals exchange 0s 1s regard relevance features represent This detrimental surplus lowquality features increase risk overﬁtting training data To overcome drawback introduce probabilistic OR pOR probabilistic AND pAND logical operators biased high quality features They rely notion relevance candidate feature Deﬁnition 74 Given feature space cid5Pos ative candidate terms respectively Deﬁnition 41 With t Pos follows c k Neg c kcid6 let σ η scoring functions positive neg k assign relevance measure ρt k Neg ρt f t Max f titi S f t σ t S Pos c k t candidate positive feature3 f t ηt S Neg deﬁne irrelevance ρt t ρt Min f titi S f t t Dually f S Clearly 0 ρt cid2 1 takes value 1 highest scoring term t 0 ρt cid2 1 takes value 1 lowest scoring term t Fig 6 Deﬁnition 75 Probabilistic logical operators Given individuals I1 I2 feature t deﬁne probabilistic OR follows cid7 pOR cid8 I1t I2t cid15 ORI1t I2t probability p ρt I1t alternatively ρt relevance t Deﬁnition 74 The pAND operator deﬁned accordingly ρt place ρt We note operators commutative An important property pOR pORI1t I2t cid2 ORI1t I2t pORI1t I2t 0 ORI1t I2t vice versa In particular pORI1t I2t holds ORI1t I2t I1t 1 I1t I2t 0 Otherwise I1t 0 I2t 1 pORI1t I2t ORI1t I2t 1 probability p ρt Clearly higher relevance ρt recall ρt 1 t highest scoring term higher probability 1 moved I2 I1 phenotypic level means classiﬁer encoded I1 acquires new feature t classiﬁer encoded I2 Thus overall effect pOR moving preferably relevant features I2 I1 The pAND operator works dual way That effect pAND discarding I1 moving zeroes I2 I1 preferably relevant features Now probabilistic logical operators place standard ones compute approximation cid16 cid18 φ φ follows cid16 cid18 cid23 COMPUTATION φI1 I2 follows φI1 I2 cid18 φI1 I2 Given individuals I1 I2 compute cid16 φI1 I2 cid16 cid18 φI1 I2 I st I φI1 I2 I st I pORI pANDI 1 I 1 I 2 I 2 I pANDI pORI 1 I 1 I 2 I 2 I cid2τ I cid2τ I cid2τ 1 cid2τ 1 cid2 cid18 It easily veriﬁed φI1 I2 generalization I1 φI1 I2 specialization I1 cid16 3 We assume σ t 0 t Pos t VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 79 cid16 cid18 Unlike implementation φsubsumption primitives genotype level implementation τ subsumption primitives To point preliminarily recall cid16 φ relies bitwise operations performed τ performed phenotype level c cid5Pos1 Neg1 cid19T1 T2cid6 Deﬁnition 52 Thus implement τ I1 I2 ﬁrst extract individuals I1 I2 threshold sets T1 T2 inverse en 1 cid2 cid2 2 Then compute cid19T1 T2 algorithm Fig 2 ﬁnally τ I1 I2 individual having positive negative coding function Ti enc apply encoding function enccid19T1 T2 Therefore components I1 threshold component enccid19T1 T2 τ H1 c H2 τ φ 1I cid2τ cid16 cid16 cid16 cid18 cid23 COMPUTATION cid16 cid18 τ I1 I2 τ I1 I2 Let I1 I2 individuals let Ti enc 1I cid2τ thresh cid16 cid18 old set classiﬁer encoded I 1 cid2 cid2 2 Then compute I I I I cid19T1 T2 cid18T1 T2 constructed algorithm Fig 2 cid2 τ I1 I2 I I τ I1 I2 I I cid2τ enccid19T1 T2 cid2τ enccid18T1 T2 1 I 1 I 1 I 1 I The correctness computation directly follows Deﬁnition 52 742 Generalizingspecializing GS mutation The GS mutation GSM operators implementation unary reﬁnement operators deﬁned Deﬁnition 51 Therefore GSM applied individual encoding Hc returns individual encoding neighbor generalization specialization Hc hierarchies Notation In following denote small abuse notation x I resp x I individual encoding classiﬁers x Hc resp x Hc x φ τ I encoding Hc Deﬁnition 51 Deﬁnition 76 GSM operators Let I individual encoding Hc MFck P N Thegeneralization mutation GM I I individual encoding direct ancestor Hc MFck P N τ φgeneralization hierarchy GMI τ I GMI φ I More precisely cid15 GMI φ I probability p 05 τ I The specialization mutation SMI deﬁned accordingly That GS mutation I yields individual encoding equal probability neighbor classiﬁer encoded I φ τ hierarchy exactly model small change hypothesis The computation nondeterministic primitives φ I φ I clearly transposition genotype level computation unary reﬁnement operators φ Hc φ Hc shown Section 51 Hence obtain binary encoding φ I classiﬁer φ Hc simply ﬂipping 0 1 I add positive feature Hc 1 remove negative feature Hc Dually binary encoding φ I classiﬁer φ Hc 0 I ﬂipping 1 0 I 0 1 I However like case GS crossover bias GM mutation highrelevance features To end introduce notions insertion probability ipt removal probability rpt candidate feature t follows ipt ρtcid14 i1k ρti rpt ρtcid14 i1k ρti ρt ρt relevance irrelevance measures t respectively Deﬁnition 74 Intuitively probability ipt represents chance It ﬂipped 0 1 chance candidate feature t selected term positive negative classiﬁer encoded individual I The meaning rpt dual We notice ρti cid2 1 condition i1k ρti cid2 k holds recall k number positive i1k ρti cid3 1k negative features Therefore highest scoring feature t ρt 1 ipt 1 maximum insertion probability smaller 1k deﬁned 45 lower bound optimal mutation rate Dually removal probability rpt maximum lowest scoring feature t relation rpt cid3 1k holds We ready provide computation φ I φ I cid14 cid14 cid23 COMPUTATION φ I φ I Let individual I given Compute φ I φ I follows φ I select randomly probability 05 options 1 probabilistically select bit I t 0 according insertion probability distribution ipt mutate 0 1 2 probabilistically select bit I t 1 according removal probability distribution rpt mutate 1 0 80 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 b φ I select randomly probability 05 options t 1 according removal probability distribution rpt mutate 1 t 0 according insertion probability distribution ipt mutate 0 1 probabilistically select bit I 0 2 probabilistically select bit I 1 cid2 cid18 cid16 τ Now let consider τ subsumption primitives τ τ Like case previously seen τ subsumption τ implementation τ τ performed phenotype level To end ﬁrst primitives cid2τ Then extract individual I threshold set T inverse encoding function T enc compute T algorithm Fig 4 ﬁnally apply encoding function enc T Therefore T individual having positive negative components I threshold component enc T 1I cid23 COMPUTATION τ I τ I Let individual I given let T enc cid2τ Now τ I implemented simply replacing encoding I encoded I direct ancestor T computed algorithm Fig 4 τ I implemented accordingly That cid7 cid2τ enc T τ I I cid7 cid2τ enc T cid2 τ I I I I cid7 I cid7 I cid7 I cid7 I I I I I cid7 cid7 1I cid2τ threshold set cid2τ T encoding enc T 75 The genetic algorithm First dimensionality range kmin kmax computed input vocabulary applying Deﬁnition 71 Then given userdeﬁned input values P max Nmax randomly generated triple k P N k kmin kmax 0 cid2 P cid2 P max 0 N cid2 Nmax random number 1 individuals length 2k MinP 1 Ncid24logNP 1cid25 1 created Each individuals encodes classiﬁer hypothesis space MFck P N The set individuals created triple k P N form subpopulation Sk P N Each individual I Sk P N initialized follows k bits cid2τ randomly set minimal threshold set p1 n1 pr nr I 0 cid2 pi cid2 P 0 ni cid2 N 1 r Deﬁnition 42 Afterwards evolution takes place iterating elitism selection crossover mutation predeﬁned number generations created Finally phenotype best generated chromosome returned set 1 probability 05 I I Next details selection crossover mutation Selection We want able preserve subpopulations pressure selection order guarantee certain degree population diversity niching methods purpose 4647 At time want avoid premature convergence subpopulations consist small number individuals At aims maintain set mating pools union subpopulations threshold bounds P N MP N cid6 Ski P N So individuals MP N different length belonging isomorphic τ subsumption lattices In particular lengths individuals MP N differ far feature components concerned threshold components equal length threshold bounds P N Section 72 Now selection performed follows mating pool randomly selected tournament selection applied individuals Crossover We given individuals I1 Sk1 P N I2 Sk2 P N belonging mating pool MP N The GAMoN crossover I1 I2 combines slightly modiﬁed version uniform crossover called MUX GS crossover operators deﬁned previous sections A sketch proposed method shown Fig 7 It basically relies steps Step 1 Decide probabilistically MUXI1 I2 takes place line 18 This decision positively userdeﬁned probability px MUX adaptation uniform crossover UX deal different lengths feature components mating individuals ii presence threshold sets Informally MUXI1 I2 regarded UXI1 I2 1 ﬁrst mink1 k2 bits positive negative components I1 I2 cid2 probabilistically exchanged lines 24 2 I 2 swapped single bits line 5 Note offspring J 1 J 2 J 1 Sk1 P N J 2 Sk2 P N belong subpopulations parents cid2 1 I Step 2 If decision MUX positively Step 1 perform GS crossover invoking function GSXI1 I2 line 20 This executed probability equal F measure classiﬁer Hc encoded I1 line 13 This way ﬁtter individuals higher chance generalize specialize allow reﬁnement discussion Section 77 Whether generalization specialization I1 performed depends Hc speciﬁc general line 14 However carry GSXI1 I2 individual I2 preliminarily promoted citizen subpopulation Sk1 P N I1 Deﬁnition 73 GS crossover applied members subpopulation To end I2 length I1 invoking function promote line 15 The following cases arise recall threshold set components I1 I2 equal length k1 cid2 k2 Only ﬁrst k1 bits I 2 I 2 picked lines 78 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 81 copy ﬁrst mink1 k2 positive negative features I2 J J length k1 J J J 2 cid2τ 2 2 2 cid2τ I J 1 I1 I J J cid2τ 2 probability 05 copy threshold component I2 J probability 05 probability 05 GAMoN XoverI1 I2 Input individuals I1 Sk1 P N I2 Sk2 P N MUX probability px Output offspring J 1 Sk1 P N J 2 Sk2 P N function MUXI1 I2 J 2 I2 1 2 1 mink1 k2 3 Swap J 1 Swap J 4 1 cid2τ J 5 Swap J 1 return J 1 J 2 function promoteI2 generalize 6 7 1 mink1 k2 I 8 2 9 k2 1 k1 generalize 10 0 11 12 return J function GSXI1 I2 13 probability equal FmeasureHc generalize precisionHc recallHc 14 15 cid22I2 promoteI2 generalize 16 17 return j begin 18 probability px set cid5 J 1 J 2cid6 MUXI1 I2 19 20 return J 1 generalize J GXI1cid22I2 J SXI1cid22I2 MUX performed J 1 GSXI1 I2 J 2 GSXI2 I1 k1 k2 add k1 k2 bits J I2 citizen Sk1 P N J 1 J J 0 1 pad J pad J J 2 J J k1 k2 0s 1s resp J k1 k2 1s 0s resp GX SX performed according Deﬁnition 73 Hc classiﬁer encoded I1 Fig 7 Pseudocode GAMoN Xover k1 k2 Both I 2 I 2 extended n k1 k2 bits In particular I 2 padded n 1s resp 0s 2 n 0s resp 1s generalization resp specialization performed lines 912 I At point GXI1 I2 SXI1 I2 computed according Deﬁnition 73 lines 1617 Once GSXI1 I2 carried GSXI2 I1 performed likewise line 20 Again offspring J 1 J 2 belong subpopulations Sk1 P N Sk2 P N respectively Mutation Mutation performed similar framework This combination modiﬁed version stan dard mutation denoted MSM takes account threshold sets GS mutation GSM operators previously deﬁned In particular MSMI works follows ﬁrst randomly decides probability 05 operate feature threshold component In case MSMI randomly ﬂips bits I prob ability 12k In case MSMI replaces I encoding randomly chosen neighbor threshold contrary GSM selectively chooses direct ancestor direct descendant depending set encoded I generalization specialization performed respectively Note cases MSMI causes small changes position subsumption lattices Now GAMoN mutation works follows offspring I cid2 cid2 Step 1 decide probabilistically GSMI takes place This decision positively probability F measureHc Hc phenotype I Step 2 If decision GSMI positively step 1 execute MSMI 76 GAMoN time complexity It immediate recognize cost taskspeciﬁc reproduction operators O k cost ﬁtness computation O k size feature space m number examples training set fact evaluation ﬁtness individual requires evaluation number candidate positive negative features occurring document training set Now number different features words occurring training set asymptotically independent m lexicon ﬁnite irrespective feature selection k asymptotically independent m Thus technically O O m asymptotic behavior GAMoN linear size training set Quite obviously relatively small values m like characterize reallife data sets practical complexity O 82 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 77 Remarks proposed GA Individual encoding There basic approaches according chromosome population represent single rule rule set 9 Within approach chromosome rule rule induction GAs like XCS 27 SIA 28 COGIN 48 In second approach chromosome set rules called Pittsburgh approach rule code entire classiﬁer GAssist 29 OlexGA 10 BioHEL 1213 fall category From chromosome rule approach makes individual encoding simpler ﬁtness geno type meaningful indicator quality phenotype 49 Further competitive style GA conﬂict individual collective interests rules forming classiﬁer 9 On chromosome set rules approach requires sophisticated encoding individuals ﬁtness provides reliable indicator 49 Moreover conﬂict interests happen case competition occurs classiﬁers single rules In approach individual encodes candidate classiﬁer falls class Pittsburgh methods Despite individual encoding simple compact 2k bits encoding Pos Neg tens bits altogether handful bits encode threshold set Thus GAMoN combines advantages mentioned approaches individual simplicity compactness chromosome rule approach effec tiveness reproductive competition ﬁtness function chromosome set rules approach Search strategy It known standard reproduction operators disruptive sense offspring different parents On hand advantage making unlikely GA getting stuck local optima hand high degree unpredictability generation new candidate classiﬁers GA converge slowly In contrast GS operators hypotheses position hierarchies controlled way depending state current hypothesis Such search bias forces search strategy quickly converge local optima As seen GAMoN combines space search standard GA based GS operators The rationale choice exploiting perform selective search compensate selectiveness search introducing certain degree diversity standard operators In particular GAMoN runs GS operators crossover mutation increasing probability deﬁned F measure achieved individual I training set This way generations pass algorithm approaches optimal solution controlled search space performed 8 Empirical investigation framework 81 Machine learning algorithms To evaluate GAMoN approach proposed paper focused comparisons rule learning algorithms To end selected rule induction GAs BioHEL OlexGA nonevolutionary algorithms C45 Ripper Further included study Platts Sequential Minimal Optimization SMO method linear SVM training 16 reported best methods text categorization Our OlexGA assessing extent GAMoN effective extension Section 2 BioHEL chosen best performing GAbased methods We aware experimental results BioHEL textual data sets Finally C45 Ripper selected standard decision treerule learners widely text classiﬁcation All selected learning algorithms implemented Java available platform In particular GAMoN runs Weka platform BioHEL available KEEL platform KEEL Knowledge Extraction based Evolutionary Learning suite machine learning software tools 50 C45 Ripper SMO OlexGA run platforms For purpose work Weka version 358 algorithms BioHEL 82 Data sets We carried empirical work 13 realworld data sets properties summarized Table 1 As span wide range sizes minimum 900 Oh15 maximum nearly 204000 market documents The rarest category 51 documents Oh0 frequent 85440 documents Market Most datasets widely large scale text classiﬁcation tasks publicly available Market data set 203926 documents extracted Reuters Corpus Volume I RCV1 63 R10 standard subset Reuters21578 Distribution 10 consists 12897 documents uses 10 frequent Top ics categories 51 Ohsumed Ohsumed233445 collection subset MEDLINE database 52 34389 cardiovascular diseases abstracts 50216 medical abstracts contained year 1991 The classiﬁcation scheme consists 23 cardiovascular diseases MeSH categories Ohscale Oh0 Oh5 Oh10 Oh15 subsets Ohsumed233445 53 Data sets SRAA 20NG 20newsgroups articles newsgroups In particular 20NG VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 83 Table 1 Data set description Name Source Oh15 Oh5 Oh0 Oh10 BlogsGender Ohscale R10 20NG Ohsumed Cade12 SRAA ODPS22 Market Ohsumed233445 Ohsumed233445 Ohsumed233445 Ohsumed233445 Blog author gender Ohsumed233445 Reuters21578 20 newsgroups Ohsumed233445 Gerindo Proj UseNet ODP Rcv1 Original format arff arff arff arff text arff text csv text csv text text text Doc Feat Cat Cat size 913 918 1003 1050 3232 11162 12897 18846 34389 40983 73218 107262 203926 3100 3012 3182 3238 15026 11465 21363 59903 34359 69470 63966 25068 68604 10 10 10 10 2 10 10 20 23 12 4 22 4 Min 53 59 51 52 1548 709 237 628 427 625 4796 88 26036 Max 157 149 194 165 1684 1621 3964 999 9611 8473 41351 28286 85440 collection 18846 newsgroup documents organized 20 different categories We version sorted date include newsgroupidentifying headers The SRAA 54 data set contains 73218 articles discus sion groups simulated auto racing simulated aviation real autos real aviation BlogsGender binary data set 3232 blogs author gender classiﬁcation 55 Cade12 subset CADE Web Directory consisting 40983 web pages classiﬁed 12 categories 56 ODPS22 subset ODP Open Directory Project 57 documents stored RDF ﬁles For experimentation subset 107262 documents classiﬁed categories TopScience subtree 25 ﬁrstlevel categories We ﬁrst collapsed 25 subtrees respective root obtaining ﬂat structure 25 categories Then grouped category Misc 4 small est categories Search Engines 7 documents Chartsandforums 16 documents Directories 27 documents Events 38 documents getting set 22 categories From document web page extracted title description discarding URL 83 Experimental setup We preliminarily preprocessed data sets downloaded textual format performing tokenization word unigrams stopword removal We bagofwords representation binary word weighting Each feature represented numerical attribute Experiments performed binary classiﬁcation setting To end binarized data sets performing multiclass twoclass conversion This way mclass learning problem decomposed m independent twoclass subproblems class ith classiﬁer separating class remaining ones Finally category feature scoring CHI square 39 performed training set Following major issues arose design experiments 1 Dimensionality feature space Unlike systems GAMoN automatically detects appropriate dimensionality feature space That manual feature selection preliminarily needed As later section feature spaces selected GAMoN usually consist tens features b Previous works systems like Ripper C45 SVM require relatively large vocabularies usually thousands features learn good prediction functions c The eﬃciency OlexGA like evolutionary methods strongly depends feature space dimen sionality features imply long individuals low eﬃciency d BioHEL represents exception evolutionary landscape designed eﬃciently deal high dimensional data sets However memory space limitations KEEL platform severely limits number attributes actually case large data sets The observations demonstrate diﬃculty applying single feature selection policy systems In fact unfair nonevolutionary methods feature dimensionalities detected GAMoN small characteristics points b On hand running OlexGA BioHEL number features nonevolutionary methods practically unfeasible points c d 2 Time eﬃciency As seen empirical study involves large data sets ODPS22 Market experimented systems perform ineﬃciently In particular Ripper C45 showed extremely slow data sets especially tried use large vocabularies instance vocabularies 10000 features stop C45 overly ineﬃcient This prevented performing optimization vocabularies 84 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 Table 2 Feature space dimensionality range kmin kmax size feature space optimal classiﬁer kopt F measure categories R10 kmin kmax kopt PRavg acq 105 172 132 corn 16 43 21 crud 58 106 100 earn grain 37 60 41 42 77 55 int 59 102 68 mon 86 148 99 ship 53 97 61 trad 78 141 86 wheat 18 41 21 8666 9020 8750 9518 9209 5605 6812 8041 6915 8866 Given premises following experimental design choices ﬁnally taken 1 GAMoN run 500 individuals 200 generations elitism rate 02 MUX probability 06 Section 75 The max imum threshold bounds P max Nmax 4 2 OlexGA BioHEL executed vocabularies 100 features The run default parameters shown httpwwwmatunicalitOlexGA provided KEEL 3 The remaining nonevolutionary systems executed vocabularies 2000 terms default settings provided Weka The SMO normalization option turned improve training time Due eﬃciency reasons performed 5fold cross validation 80 training 20 test small data sets Oh15 R10 holdout 70 training 30 test applied remaining data sets 84 Predictive performance measure statistical tests Performance measured common text classiﬁcation arithmetic mean Precision Recall denoted PRavg approximation PrecisionRecall BreakEven Point To obtain global estimates categories standard deﬁnitions microaveraged Precision Recall notably μPr μRe cid14 cid14 cid14 cid14 cC TPc cCTPc FPc cC TPc cCTPc FNc TPc set documents correctly assigned classiﬁer category c FPc set documents incorrectly assigned classiﬁer category c FNc set documents incorrectly assigned classiﬁer cate gory c We note microaveraging gives equal weight document called documentpivoted measure largely dependent common categories Each run evolutionary algorithms repeated 3 times average PRavg taken In order comparisons statistically signiﬁcant performed ImanDavenport test Holms posthoc test recommended comparison classiﬁers multiple data sets 58 9 Experimental results 91 A glimpse MofN hypotheses The experimental results GAMoN bias learning compact readable hypotheses The following examples classiﬁers induced categories corn wheat grain R10 cid2 cid3 cid4cid5 Hwheat cid2 Hcorn Hgrain wheat deﬁcit investment net treasury york cid4cid5 corn maize london money quarter cid2 barley cereals corn grain maize rice sorghum wheat acquisition bank earning pay proﬁt tax york 1 1 1 1 cid3 cid3 cid4cid5 1 1 2 2 As classiﬁers atoms 2order classiﬁer description Section 3 It emphasized high semantic correlation positive features respective categories 92 Automatic selection feature space dimensionality Table 2 shows categories R10 values kmin kmax kopt given execution GAMoN kmin kmax deﬁne dimensionality range feature space kopt kmin kopt kmax size feature VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 85 Fig 8 Distribution features CHI square categories R10 corn left acq right Only ﬁrst 200 features shown Fig 9 Decision boundary classiﬁer cid5Pos Neg 1 1 2 2 3 3cid6 category earn R10 Each label π x y νx y represents number π x y positive examples number νx y negative ones x positive features y negative ones Labels 0 0 omitted ﬁgure space optimal classiﬁer As seen learning categories generally relies small sets candidate features As example corn kmin 19 kmax 43 PRavg equal 9020 meaning positive terms best classiﬁer ﬁrst k higher scoring features 19 cid2 k cid2 43 This clearly indicative aggressive feature selection To let look Fig 8 left distribution features CHI square reported As corn features scoring high remaining ones rapidly approach nearzero values The sharply declining shape graph indicative easy category category high performance achieved discriminative words In contrast acq diﬃcult category As Fig 8 right lower initial CHI square values graph smooth decreasing trend That features highly discriminative power exist As consequence dimensionality range shifted rightwards xaxis kmin 105 kmax 172 indicative aggressive reduction feature space 93 Decision boundaries Fig 9 shows decision boundary DBHearn 3order classiﬁer Hearn cid5Pos Neg 1 1 2 2 3 3cid6 category earn R10 Here Pos 14 positive features Neg consists 16 negative features As DBHearn threestep polyline From data reported Fig 9 results subset documents classiﬁed Hearn consists 2954 positive examples 113 negative ones lying classiﬁcation region RHearn delimited DBHearn The generalization error PRavg value Hearn err b c b c d 0031 PRavg 2a2 ac 2a ba c 094 86 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 Table 3 Microaveraged PRavg values data set obtained GAMoN GAMoN version GAMoN GS reproduction operators Legend BG BlogsGender OhS Ohscale Ohsu Ohsumed Mkt Market GAMoN GAMoN Oh15 8046 7759 Oh5 8485 8255 Oh0 8427 8290 Oh10 7862 7554 BG 6882 6833 OhS 7503 7428 R10 8650 8456 20NG 7572 7521 Ohsu 6725 6709 cade 4842 4643 SRAA 8525 8427 ODP 7114 7020 Mkt 9242 9059 Table 4 Microaveraged PRavg results obtained 5fold crossvalidation Oh0 Oh5 Oh10 Oh15 BlogsGender Ohscale R10 8020 split holdout remaining data sets 7030 split Dataset Oh15 Oh5 Oh0 Oh10 BlogsGender Ohscale R10 20NG Ohsumed cade SRAA ODP Market avg microPRavg avg rank Ripper 7955 8374 8437 7882 6096 7296 8521 7266 6035 4431 8104 6673 9463 7426 396 SMO 7995 8429 8480 7470 6095 6952 8894 8364 6694 5406 9006 8065 9556 7800 208 C45 7675 8230 7924 7478 5833 7077 8467 7486 6325 4810 8685 7476 9537 7462 362 OlexGA BioHEL GAMoN 7433 8076 8129 7446 6675 7436 8407 7297 6558 4410 7960 6946 8895 7359 431 6603 7672 7320 6739 6282 6826 8359 7065 6379 4296 8134 7121 7475 6944 523 8046 8485 8427 7862 6882 7503 8650 7572 6725 4842 8525 7114 9242 7683 208 b c d computed follows Section 45 c cid11 x yRH cid11 hearn x y RH hearn π x y 2954 b π x y 205 d cid11 x yRH cid11 νx y 113 hearn νx y 6988 x y RH hearn 94 Effect GS operators To effect GS reproduction operators deﬁned paper compared accuracy results GAMoN data sets previously seen obtained running version GAMoN GS operators GS Xover GS mutation disabled The experimental results reported Table 3 GS operators improve accuracy single data set induce trivial improvements Ohsu BG gain remarkable Oh15 Oh10 R10 cade Mkt On average enhancement data sets 15 points As discussed earlier paper aim GS operators reﬁning ﬁtter individuals exploiting structure hypothesis space This explains signiﬁcant improvements obtained 95 Comparison systems Table 4 shows algorithm data set microaveraged PRavg The average values data sets average ranking algorithm included table The best results stressed bold face The ranking obtained assigning position algorithm depending performance data set The algorithm showing best accuracy given data set assigned rank 1 As SMO best performer PRavg 7800 followed GAMoN PRavg 7683 The algorithms average rank 208 We note GAMoN outperforms rule induction methods In particular behaves uniformly better OlexGA individual data sets compares favorably rule learners data sets In order establish differences performance statistically signiﬁcant ImanDavenports test applied This nonparametric statistical test recommended 58 comparing classiﬁers multiple data sets In brief 13 data sets 6 algorithms conﬁdence α 005 ImanDavenport statistics 954 greater critical value CV 237 Thus null hypothesis states algorithms equivalent rejected Hence apply Holms posthoc test 58 GAMoN control algorithm controlling familywise error multiple hypothesis testing The results test summarized Table 5 Based reject null VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 87 Table 5 Holms test GAMoN control algorithm The null hypothesis rejected pvalue αi 5 4 3 2 1 Method BioHEL OlexGA Ripper C45 SMO z R0Ri SE 42980 30400 22014 20966 00000 pvalue 00005 00024 00278 00366 10000 αi 001 00125 00167 0025 005 Table 6 Avg size rulebased classiﬁers R10 Algorithm GAMoN Ripper C45 BioHEL OlexGA Avg size classiﬁers Pos 20 Neg 10 order 18 Rules 16 Rules 78 Rules 14 literalsrule 19 Pos 16 Neg 15 Table 7 Learning times expressed hours Each run GAMoN OlexGA BioHEL repeated 3 times Overall learning time h runs Avg learning time category h Ripper 445 395 113 C45 488 395 123 SMO 71 395 018 OlexGA BioHEL Gamon 46 1185 004 185 1185 016 156 1185 012 hypothesis equivalence BioHEL OlexGA pvalue αi holds That conﬁdence 95 state GAMoN performs better algorithms statistically equivalent SMO Ripper C45 96 Size classiﬁers Apart SMO classiﬁers yield models sets rules Although unique formal deﬁnition size classiﬁer number rules number features Table 6 provide statistical data averaged ﬁve folds giving insight quantitative characteristics classiﬁers induced R10 As Ripper C45 BioHEL induce classiﬁers consisting average 16 rules 78 rules 14 rules respectively BioHEL rule having 19 literals average In turn GAMoN induces classiﬁers 20 positive features 10 negative ones average 16 positive features 15 negative ones OlexGA classiﬁers Going results given table nearly 44 classiﬁers induced GAMoN atoms 38 order 2 18 order 3 2 order 4 note P N 4 maximum order classiﬁer MinP 1 N 4 Proposition 410 97 Time eﬃciency The experiments previously described performed Intel Xeon 233 GHz machine 4 Gb RAM The learning times needed achieve accuracy results previously seen reported method Table 7 ﬁrst row As OlexGA 46 hours best performers followed SMO 71 GAMoN 156 BioHEL 185 Ripper 445 C45 488 recall run GAMoN OlexGA BioHEL repeated 3 times Table 7 reports average learning times category Again OlexGA fastest algorithm 004 hcategory followed GAMoN 012 BioHEL 016 SMO 018 Ripper C45 times slower GAMoN 113 123 respectively To effect training set size learning times Fig 10 plotted average learning times category data set data sets ordered increasing size The graph provides empirical picture progression learning times number training documents As GAMoN asymptotically behaves similarly OlexGA BioHEL SMO signiﬁcantly smoother trend Ripper C45 That GAMoN scales better nonevolutionary rule induction methods 10 Discussion related work The experimental study described previous sections shows GAMoN induces classiﬁers accurate compact Interestingly properties consistently observed 13 data sets GAMoN showed uniform behavior Given different application domains corpora refer clear proof robustness Further GAMoN showed perform eﬃciently large data sets MofN representation As discussed Section 46 family resemblance metaphor provides quali tative understanding basic reason MofN paradigm suited purpose text categorization 88 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 Fig 10 Comparison average learning time category data set data sets ordered increasing size extends MofN negation disjunction constructs enables express hypotheses capable MofN best ﬁtting true structure data discussion expressivity MofN reported Sec tion 46 Unlike existing classiﬁers focus features positively discriminate class approach negation ﬁrst class citizen allowing explicitly model interactions positive negative features given example In turn disjunction enables modulate interactions capturing positive cor relation simple atoms miss existing positive negative features Section 45 Negation takes precision control disjunction improves recall One advantage proposed language DNFtype representation conciseness Indeed Mof N hypothesis represented terms disjunctions conjunctions Section 42 DNFtype representation MofN concept prohibitively long number disjuncts exponential size Pos Neg We believe proposed language main contribution paper One interesting direction future work mathematical relationship linear function thresholds p n instead current threshold multiple pairs Feature space MofN hypotheses built set preselected candidate features While long history applying dimensionality reduction methods contribution paper represented original deﬁnition feature space consisting terms indicative membership terms indicative nonmembership category Unlike traditional feature selection approach positive terms selected deﬁnition enables learner focus negative information way positive similar approach distinguishing positive negative features proposed 59 A criterion automatic detection suitable dimension provided Deﬁnition 71 This criterion proved effective practice Experimental results showed tens wellselected features suﬃcient build accurate prediction functions irrespective data set GAMoN biases Apart language bias characterize GAMoN terms search bias overﬁtting avoidance bias 18 We extensively discussed refers way hypothesis space searched subsumption relationships means taskdependent genetic operators The proposed approach actually new inductive learning 3035 overcomes major problem use conventional GAs account structure search space The overﬁtting avoidance bias preference simpler classiﬁers GAMoN includes bias induction mechanisms suitable feature probability distributions Section 74 enable reproduction operators select highquality features In combination proposed feature selection technique provides GAMoN effective lexicon capable expressing essential patterns overﬁtting avoidance bias guarantees induction classiﬁers parsimonious handful wellselected features This makes effective unseen data highquality features drastically reduce risk overﬁtting training data VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 89 GAMoN C45 Ripper Unlike GAMoN rulebased nonevolutionary classiﬁers work notably C45 Ripper achieve expressive power DNF Despite conducted experimental study showed outperform GAMoN This clear proof effectiveness proposed algorithm In addition GAMoN performs signiﬁcantly eﬃciently C45 Ripper large data sets graphs Fig 10 This surprising time complexity Ripper O m log2 m C45 O m3 complexity GAMoN O m m number examples training set Section 76 That GAMoN scale large realistic realworld domains better rulebased classiﬁers We point improvements learning times obtained eﬃcient implementation task speciﬁc reproduction operators real application environment distributed approaches Current research likely improve eﬃciency GAMoN GAMoN OlexGA BioHEL GAMoN substantial extension OlexGA respects The ﬁrst language atom thresholds equal 1 cid5Pos Neg 1 1cid6 An OlexGA hypothesis special case MofN strictly expressive language OlexGA The second genetic algorithm Since MofN hypothesis space OlexGA provide structure OlexGA relies simple standard GA population ﬁxedlength individuals reproduction operators standard uniform crossover mutation That OlexGA special case GAMoN As shown Table 4 proposed extension results statistically signiﬁcant improvement OlexGA Needless price slower learning procedure BioHEL Bioinformaticsoriented Hierarchical Evolutionary Learning stateoftheart GA showed perform effectively nontextual data sets 60 To best knowledge ﬁrst study BioHEL tested text classiﬁcation problems BioHEL inherits features GAssist It relies Pittsburgh represen tation approach applies iterative rule learning approach 28 BioHEL explicitly designed handle largescale datasets To end rule instead coding domain attributes keeps subset avoiding hun dreds irrelevant computations Using approach BioHEL able handle problems hundreds attributes datasets large sets instances 13 tens thousands attributes instances In addition order reduce computational cost BioHEL uses windowing scheme called ILAS incremental learning alternating strata The experimental results paper conﬁrm BioHEL behaves eﬃciently learning time similar GAMoN Table 7 On contrary terms predictive accuracy showed statistically inferior GAMoN However feel better results obtained ﬁner tuning For instance recent publication 62 shows BioHEL parameter highly problem sensitive coverage breakpoint Also appropriate use ILAS windowing scheme usage C implementation4 place KEEL implementation improve eﬃciency Other systems learning negation As mentioned negative evidence deemed important text classiﬁcation task However apart OlexGA GAMoN experimented systems focuses exploitation negative information In general examples IRL Inductive Rule Learning approaches involve direct generation negation rare 6138 Outside realm rule learners Complement Naive Bayes CNB text classiﬁers leverage negative features 21 Its peculiarity learning weights class training data class CNB works multiclass setting needs 3 classes In 21 authors claim CNB approaches stateoftheart accuracy SVMs Unfortunately compare GAMoN CNB empirical study binary oneversusall technique deal multilabel classiﬁcation basically problem Weka provide support multilabel data set representation necessary order provide CNB input systems Other MofN approaches Several research works recently develop methods inducing MofN cepts best knowledge text categorization For instance 1 technique extracting MofN hypotheses neural networks reported However work ﬁeld carried construc tive induction ID2of3 2 MofN induction algorithm incorporates MofN tests decisiontree learning It based greedy hillclimbing approach best MofN hypotheses node decision tree XofN 4 greedy constructive induction algorithm learns X ofN nominal attributes Both ID2of3 XofN building decision tree construct new attribute decision node local training set More recently Ge netic Algorithm constructive induction proposed 7 It relies variable length individual representation encoding set N attributevalue pairs composing X ofN attribute The ﬁtness deﬁned information gain ratio constructed attribute The genetic operators standard uniform crossover mutation simple variant standard A conventional niching method foster population diversity 11 Conclusions In paper proposed new language called MofN constructing MofN hypotheses training data text classiﬁcation GAbased approach 4 Available httpicoscsnottacuksoftwarebiohelhtml 90 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 The MofN representation generalizes classical notion MofN concepts allowing negation disjunction We conjectured wellsuited express text classiﬁcation conditions complies socalled family resemblance metaphor We shown space MofN hypotheses structure determined kinds subsumption relationships feature threshold relationships form complete lattices Based suitable reﬁnement operators effective exploration hypothesis space designed To induce MofN hypotheses taskspeciﬁc genetic algorithm GAMoN proposed It based Pittsburgh approach individual encodes candidate classiﬁer ad hoc GS reproduction operators stochastic implementation reﬁnement operators GAMoN dynamically adapts probability selecting GS operators The population partitioned number competing subpopulations consisting individuals belonging hypothesis subspace To end statistical criterion automatically detecting dimensionality range feature space proposed This paper presented empirical results obtained extensive experiments 13 realworld test collections wide spectrum sizes hundreds hundreds thousands documents We GAMoN competitive large collection stateoftheart learning techniques belonging different classes provides hypotheses compact easily interpretable In particular small differences predictive accuracy GAMoN SMO bit performant GAMoN Ripper C45 bit performant systems showed statistically equivalent Whereas GAMoN proved superior evolutionary algorithms In particular showed statistically signiﬁcant improvements predecessor OlexGA conﬁrming effectiveness proposed extension Finally observed scale size data set GAMoN performs eﬃciently Ripper C45 Acknowledgement The authors wish thank Giuseppe Manco helpful comments earlier versions paper Appendix A Proofs c H2 c The proof proceeds induction Basis H1 c classiﬁers HT Fck Next H1 Proof Proposition 41 Let H1 c H2 DH2 cid5Pos2 Neg2 p ncid6 A document d classiﬁed H2 set features Section 41 It easily seen Pos2 Pos1 Neg1 esis d Pos1 cid3 p d Neg1 generic classiﬁers cid5Pos1 Neg1 T cid6 cid5Pos2 Neg2 T cid6 HT Fck H1 following form H1 c H21 cid5Pos2 Neg2 T1cid6 H22 c c document classiﬁed H21 H1 c c atoms HT Fck form cid5Pos1 Neg1 p ncid6 cid2 n recall document c iff d Pos2 cid3 p d Neg2 Neg2 hold hypoth c c Thus expressed cid5Pos1 Neg1 T2cid6 Neg2 hold It turns cid5Pos2 Neg2 T2cid6 By inductive hypothesis Pos2 Pos1 Neg1 document classiﬁed H22 classiﬁed H12 classiﬁed H11 c cid2 cid2 n veriﬁed d classiﬁed H1 c classiﬁes documents classiﬁed H2 cid5Pos1 Neg1 T1cid6 H12 c Inductive step H1 c implies DH1 c DH1 H11 c DH2 c H2 H2 c cid4φ H2 cid4φ H2 H21 H11 H12 H22 c c c c c c c c c c c c c H2 c cid4φ H c cid4φ H1 c H2 cid7 c H c H2 c lubφH1 c cid5Pos1 Pos2 Neg1 c cid4φ H2 cid7 c Proof Proposition 42 Next HT Fck cid4φ complete lattice To end ﬁrst prove Neg2 T cid6 From Deﬁnition 44 immediately follows statement lubφH1 cid5Pos Neg T cid6 c H2 lubφH1 cid7 c H2 lubφH1 c Pos Pos1 Pos2 c hold Pos1 Pos Pos2 Pos Deﬁnition 44 However conditions H true contradiction From statement follows Now prove statement b glbφH1 c cid5Pos1 Pos2 Neg1 cid4φ glbφ Now let assume ab cid7 surd existence Hcid7 cid7 c H2 cid4φ H c From c c cid7 cid4φ H H c hold Pos subset Pos1 Pos2 contradiction From statement b follows cid2 Neg2 T cid6 By Deﬁnition 44 H1 cid5Pos Neg T cid6 H c turns Pos1 Pos2 Pos But conditions H1 c hold Now let assume absurd existence Hcid7 c H1 c cid4φ H cid4φ glbφ H2 c H2 c From lubφH1 cid7 cid4φ H2 c H c cid4φ H cid7 c H2 cid7 cid4φ H2 c cid4φ H1 c H cid7 c cid4φ glbφH1 cid4φ glbφH1 c cid4φ H cid4φ H1 c H2 c H2 cid7 c cid7 c c c c c c c DH2 c classiﬁers HΦ P N Next H1 c H2 Proof Proposition 43 Let H1 c The proof proceeds induction Basis H1 DH1 cid5Pos Neg p2 n2cid6 By Deﬁnition 42 document d classiﬁed H2 Now H1 c ﬁed H1 atom H1 j classiﬁed H2i c c DH2 c H1 j inductive hypothesis follows H1 cid4τ H2 c DH1 appearing H1 c Inductive step H1 implies c atoms form cid5Pos Neg p1 n1cid6 c d Pos cid3 p2 d Neg cid2 n2 c classi c exists classiﬁes documents c cid2 c p1 cid2 p2 n1 cid3 n2 Deﬁnition 45 implies document d classiﬁed H2 cid4τ H2 c atom H2i appearing H2 immediate Deﬁnition 45 Since H1 j c classiﬁes documents classiﬁed H2 c H2 cid4τ H2i cid4τ H2 c c c c c c c c Proof Lemma 41 Next prove given Hc H1 c c DH1 c DH2 DH1 c D ˆH1 c D ˆH1 H2 c D ˆH2 c ˆHc ˆH1 c DH2 ˆH2 c D ˆH2 c following holds DHc D ˆHc c The proof proceeds induction c VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 91 p1 n1 H2 c p2 n2 ˆHc ˆp ˆn atoms By Deﬁnition 42 DHc d D st d Pos cid3 Basis H1 c p1 d Neg cid2 n1 d Pos cid3 p2 d Neg cid2 n2 D ˆHc d D st d Pos cid3 ˆp d Neg cid2 ˆn Thus DH D ˆHc ˆHc 1 ˆp cid3 p1 ˆn cid2 n1 2 ˆp cid3 p2 ˆn cid2 n2 By Deﬁnition 45 condition 1 entails H1 c c D ˆHc Inductive step DH D ˆHc condition 2 H2 c DH1 c inductive hypothesis DH1 ˆHc DH D ˆHc DH1 c D ˆH1 c DH2 c D ˆHc DH2 c D ˆH1 cid4τ c DH2 c DH1 c DH1 c D ˆH2 c D ˆH2 c DH2 c cid2 c DH1 c D ˆH2 c D ˆH1 c D ˆH1 c D ˆH2 c DH2 c DH2 cid4τ c The proof proceeds induction Basis Hc p n Hcid7 c c classiﬁers HΦ P N We DHc DHcid7 p cid7 n Proof Proposition 44 Let Hc Hcid7 Hc cid4τ Hcid7 d D st d Pos cid3 p d Neg cid2 n DHcid7 p cid2 p DH1 DH2 DHcid7 DHcid7 Hc cid4τ Hcid7 2 DH2 DH c cid2 Hc cid4τ Hcid7 1 DH n cid3 n cid7 cid7 c d D st d Pos cid3 p cid7 d Neg cid2 n 2 inductive hypothesis H1 cid4τ Hcid7 c Inductive step Let Hc H1 H2 Hcid7 2 Lemma 41 DH1 DHcid7 1 H2 cid4τ Hcid7 cid7 cid7 c implies cid7 atoms By Deﬁnition 42 DHc cid7 Clearly DHc DHcid7 c 2 Now DHc DHcid7 c 1 DH1 2 H2 cid4τ Hcid7 2 Hcid7 1 DH2 DHcid7 1 H1 cid4τ Hcid7 Hcid7 1 c Proof Corollary 41 We prove given classiﬁers H1 c iff DH1 iff H1 c DH2 c DH2 c DH2 c H2 cid4 H2 c cid2 cid4 H1 c c H2 c H1 c H2 c iff DH1 c DH2 c Indeed H1 c c DH1 c Proposition 43 Proposition 44 iff DH1 H2 c c Proof Proposition 46 Next given Hc ˆHc HΦ P N classiﬁer andHc ˆHc 1 andHc ˆHc HΦ P N 2 DandHc ˆHc DHc D ˆHc 3 Hc cid4τ andHc ˆHc ˆHc cid4τ andHc ˆHc The proof proceeds induction Basis Hc p n ˆHc ˆp ˆn atoms Statement 1 To andHc ˆHc HΦ P N suﬃces observe p Maxp ˆp cid2 P n Minn ˆn cid2 N hold Statement 2 A document d classiﬁed andHc ˆHc iff d contains x cid3 Maxp ˆp positive features y Minn1 ˆn negative features iff x cid3 p x cid3 ˆp y n y ˆn iff d classiﬁed Hc ˆHc DHc DHc D ˆHc Statement 3 Immediate H2 Statement 1 Proposition 44 Inductive step Let Hc H1 c classiﬁers Statement 1 From c Deﬁnition 47 andHc ˆHc H1 H2 H3 H4 H1 andH1 c H3 andH2 c c By inductive hypothesis H1 H2 H3 H4 HΦ P N Deﬁnition 42 andHc ˆHc H4 andH2 HΦ P N Statement 2 By inductive step Deﬁnition 47 inductive hypothesis State ment 2 DandHc ˆHc DH1 c DandHc ˆHc DH1 c immediately follows Statement 3 By inductive step Deﬁnition 47 ap cid4τ H2 plying inductive hypothesis Statement 3 H1 c ˆH2 cid4τ H4 c Thus Deﬁnition 45 follows Hc cid4τ andHc ˆHc ˆHc cid4τ andHc ˆHc hold H4 andH2 andHc ˆHc speciﬁc Hc ˆHc cid2 cid4τ H1 H1 andH1 c H2 c ˆH1 c H1 cid4τ H4 ˆH2 cid4τ H2 H2 andH1 cid4τ H3 H3 andH2 cid4τ H3 ˆH1 c ˆHc ˆH1 c H2 andH1 cid4τ H1 ˆH1 c D ˆH1 c D ˆH1 c D ˆH2 c D ˆH2 c DH2 c DH2 c DH1 c DH2 c H2 c ˆH2 c ˆH1 c ˆH2 c ˆH2 c ˆH2 c ˆH1 c ˆH1 ˆH2 c c c c c c c c Proof Proposition 45 Let Hc cid5Pos Neg T cid6 given We following properties hold 1 Hc redundant iff exist pi ni p j n j T pi ni cid4τ p j n j 2 Hc minimal iff T p1 n1 pr nr pi p j ni n j vice versa j 1 r 3 If Hc H1 c c Hc H1 c c H1 cid4τ H2 H2 c Hcid7 c H j1 Hr H1 c 1 Let Hc H1 c c Hcid7 cid4 H j c 2 From point 1 Hc c By Deﬁnition 45 pi ni cid4τ p j n j iff Hi Hi c iff pair pi ni p j n j T pi ni cid4τ p j n j p j n j cid4τ pi ni iff pi cid2 p j ni cid3 n j p j cid2 pi n j cid3 ni Deﬁnition 45 iff pi p j ni n j vice versa 3 H1 c c Proposition 43 DHc DH1 c iff Hc redundant Deﬁnition 46 H j1 c c minimal c j 1 r iff Hc Hcid7 c Hc H1 c DH1 c DH1 c DH2 c H2 Hr cid4τ H2 c cid4 H j H j c c c Proposition 44 cid2 Proof Proposition 47 Next equivalence class partitioned hypothesis subspace HΦ P N relation unique minimal classiﬁer To end prove Hc ˆHc minimal clas siﬁers Hc ˆHc Hc ˆHc From statement immediately follows The proof proceeds induction Basis Hc ˆHc atoms Trivial Inductive step Hc H1 c Note minimality Hc c follows By Corollary 41 Hc ˆHc iff DHc D ˆHc iff DHc D ˆHc c ˆH1 ˆHc minimality H1 c D ˆH2 DHc D ˆHc By Lemma 41 DHc D ˆHc DH1 c DH2 c DH2 c c Likewise DHc D ˆHc DH1 c ˆHc ˆH1 c DH2 c DH1 c D ˆH1 c D ˆH2 c D ˆH1 c D ˆH2 c DH1 c D ˆH1 c D ˆH1 c ˆH2 c H2 ˆH2 H2 c c 92 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 DH2 DH2 2 H1 c c D ˆH2 c D ˆH2 ˆH2 c It turns minimality H1 c D ˆH1 c DH2 c 2 DH1 c By inductive hypothesis Hi c D ˆH2 ˆH1 c H2 c c c H2 c ˆH1 c ˆH2 c 1 DH1 ˆH1 c D ˆH1 c H2 c c ˆH2 c c Corollary 41 1 H1 ˆH j ˆH j c Hc ˆHc cid2 c Hi c c Proof Proposition 48 The statement going prove poset MΦ P N cid4τ MΦ P N set minimal classiﬁers HΦ P N complete lattice To end let consider classiﬁers H1 c MΦ P N We ﬁrst lubτ H1 c H2 H2 c Hc cid4τ H1 c hold On hand c DH2 H2 DMinH1 c c statement lubτ H1 Now prove glbτ H1 cid4τ Hc H2 c c H2 c Let Hc MΦ P N τ generalization H1 c DHc DH2 c DHc hold On hand c DHc Therefore Proposi c DHc DMinH1 c H2 c follows c H2 c H2 c Let Hc τ specialization H1 c Therefore Proposition 44 Hc cid4τ MinH1 c Thus Proposition 43 DHc DH1 c Proposition 46 DMinandH1 c DHc DH2 c H2 cid4τ Hc Thus Proposition 43 DH 1 c MinH1 H2 c MinandH1 c Hc cid4τ H2 c DH1 c MinH1 c H2 c c DH2 c H2 c H2 H2 H2 c c c c c cid4τ Hc statement glbτ H1 c H2 c MinandH1 c H2 c follows cid2 H1 c DMinandH1 tion 44 MinandH1 c DH1 c H2 H2 Proof Proposition 49 We ﬁrst prove lubτ H1 c MinH1 It immediate recognize classiﬁer c cid5Pos Neg cid19T1 T2cid6 Fig 2 function cid19T1 T2 simply minimizes T1 T2 discarding thresholds p j n j exists pi ni pi ni cid4τ p j n j holds Proposition 45 Part 1 c cid5Pos Neg cid19T1 T2cid6 By Proposition 48 c Mincid5Pos Neg T1 T2cid6 c lubτ H1 lubτ H1 c H2 c H2 c H2 Now glbτ H1 c H2 c cid5Pos Neg cid18T1 T2cid6 By Proposition 48 glbτ MinandH1 c H2 c Next MinandH1 c cid5Pos Neg cid18T1 T2cid6 To end observe lines 912 Fig 2 iterative version inductive deﬁnition andH1 c atoms function computes cid18T1 T2 Maxp1 p2 Minn1 n2 coincides base step Deﬁnition 47 course clas siﬁer cid5Pos Neg Maxp1 p2 Minn1 n2cid6 minimal atom Now let consider general case It easy inductive step Deﬁnition 47 generates couple pairs p1 n1 T1 p2 n2 T2 pair Maxp1 p2 Minn1 n2 And exactly function cid18T1 T2 lines 912 Thus nested c H2 carried lines 1012 classiﬁer andH1 c generated However classiﬁer minimal Example 48 function Minimize invoked So ﬁnally cid5Pos Neg cid18T1 T2cid6 MinandH1 c Deﬁnition 47 Indeed H1 c H2 c H2 c H2 c H2 c cid2 k p1 pk T Proof Lemma 42 Given threshold bounds P N k cid2 MinP 1 N let T n1 nk sets integers cid2 k 0 cid2 pi cid2 P 0 ni cid2 N Next exists unique subset k having size k minimal threshold set Without loss generality assume pi p j T T S T k k ni n j T k j pi p j ni n j hold Uniqueness We subset S cid7 Existence The set S p1 n1 pi ni pk nk subset T k size k pair elements pi ni p j n j j pi p j ni n j hold Thus Proposition 45 Part 2 S minimal threshold set k minimal threshold set size lower k minimal threshold set Proposi ps pi nt n j pi ps n j nt Now immediate recognize k elements ps T k s j t greater nt k j It turns k Suppose pi n j S tion 45 Part 2 ps nt S assumption ordering elements T smaller pi 1 elements nt T 1 1 elements ps nt S j t That size S cid7 cid12 S T j case j likewise Since S s 2 k j elements ps nt S k j k cid2 k T T T k k cid7 cid7 cid7 cid7 cid7 k In fact given T Proof Proposition 410 Part 1 Every classiﬁer MΦ P N order r cid2 MinP 1 N p1 n1 pr nr Part 2 Proposition 45 pi cid12 p j ni cid12 n j j 1 r cid12 j That T appear r different positive thresholds r negative thresholds Since 0 cid2 pi cid2 P 0 ni cid2 N 1 r turns r cid2 P 1 r cid2 N r cid2 MinP 1 N Part 2 Given P N s MinP 1 N let consider sets T set possible values positive resp negative thresholds appearing classiﬁers MΦ P N Now T r 1 s exist binomP 1 r subsets T r Also Lemma 42 know pair sets T It turns binomP 1 r binomN r minimal threshold sets order r constructible T order r Therefore r cid2 MinP 1 N total number threshold sets MΦ P N given Eq 1 cid2 r unique minimal threshold set S T binomN r subsets T r 0 1 P T r elements T 1 N T r T resp T r T T T r Correctness computation φ Hc See Section 51 We restrict proof correctness computation φ Hc The proof concerning φ Hc follows similar framework Let start proving φ Hc cid5Pos cid7 Pos Neg cid7cid7 T cid6 φ Hc cid4τ H cid7 T cid6 Pos cid7 Neg First note cid7 Neg φ Hc cid4φ Hc holds Deﬁnition 44 Now assume absurd existence cid7cid7 Neg However cid7 Pos t t Pos cid7cid7 Pos Neg k Neg cid4τ Hc Thus Pos cid7 Neg cid7 Pos cid7 Neg Pos cid7cid7 Neg Hcid7cid7 cid5Pos c cid7cid7 c VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 93 Pos differ exactly term Pos cid7 Pos holds That Hcid7cid7 c Hc Hcid7cid7 cid7 Neg cid7cid7 cid7 Pos φHc contradiction cid7 Pos Neg c cid7 T cid6 Pos Pos Now let prove φ Hc cid5Pos cid7 Neg φ Hc cid4φ Hc holds Deﬁnition 44 Now absurd assume exists Hcid7cid7 cid7 Neg t t Neg Since Pos cid5Pos cid7cid7 Pos holds Further Neg Neg cid7 Neg cid7 Neg cid7cid7 Neg Neg φ Hc cid4τ H Neg Hcid7cid7 c cid7 Neg Hc Hcid7cid7 c cid7cid7 c cid7cid7 Neg Neg Neg φ Hc contradiction cid7 cid4τ Hc Since Pos cid7 Pos cid7 cid7cid7 Pos Pos Pos differ exactly feature Neg follows Pos cid7cid7 Neg Neg cid7 Pos cid7cid7 T cid6 cid7cid7 Neg cid7cid7 Pos Moreover cid7cid7 Neg That c cid7 cid18 Proof Proposition 51 Next given classiﬁers H1 H1 cid4x c cid4x H1 cid18 xH1 c x τ φ By Deﬁnition 52 c H2 c H2 c turns c H2 c follows c Dually c cid4x H1 c H2 xH1 xH1 xH1 cid16 cid18 c H2 cid16 c following holds c lubxH1 xH1 c H2 c H12 c glbxH1 H1 c c H12 c c xH1 c H2 Since lubxH1 c cid4x H1 c c H12 c H1 cid4x c cid4x glbxH1 c H12 c cid16 Proof Proposition 61 We prove decision version GAMoN learning problem NPcomplete The proof reduction Knapsack problem Given atom Hc cid5Pos Neg p ncid6 let S T set training documents classiﬁed Hc c S d D st d Pos cid3 p d Neg n Precision deﬁned probability document S training set T c c PrHc T S T c S Recall deﬁned probability document T c S ReHc T S T c T c A1 A2 By replacing Eqs A1 A2 Eq 2 algebra following formulation objective function F Hc T 2 b T c S T c b S T c Hence maximize F Hc T want large possible keeping b bound given value note T c constant Thus problem learning atomic classiﬁer recognition version formulated follows LEARNATOMDECISION LAD Given training set T feature space cid5Pos c kcid6 positive integers U c k Neg V exist hypothesis Hc cid5Pos Neg p ncid6 cid5Pos c kcid6 cid3 U b cid2 V That c k Neg exist hypothesis consistent positive examples consistent b negative examples Now KNAPSACK following NPcomplete problem Given 2n 2 positive integers w 1 wn v 1 vn W Z cid14 cid14 exist X 1 n X w cid2 W X v cid3 Z We claim KNAPSACK polynomially reduces LAD To suppose I w 1 wn v 1 vn W Z instance KNAPSACK Make following instance LAD U Z V W b cid5Pos c kcid6 cid5t1 tn cid6 c k Neg feature space consists n positive candidate features negative candidate feature c training set T c1 Θti Θt j ti t j t1 tn c2 v Θti T c w Θti T c 1 n cid14 Θti denotes set examples documents T term ti occurs From point c1 follows document contains positive candidate term Further points c1 c2 turns given Pos t1 tn following holds tPos w Thus LAD turns following problem exist Hc cid5Pos 1 cid6 tPos w cid2 C symbol stands immaterial cid14 X w cid2 C Clearly Neg Or equivalently exist X 1 n swer LAD yes iff I instance KNAPSACK proving claim To conclude proof suﬃces notice verifying YES instance LAD requires polynomial time Hence problem LAD problem deciding exists atom satisfying constraints cid3 U b cid2 V NPcomplete It immediate realize decision version learning problem Deﬁnition 61 NPcomplete tPos v cid3 V tPos v b X v cid3 V cid14 cid14 cid14 cid14 Appendix B The nondeterministic function T Next description algorithm Fig 4 It creates direct ancestor T T τ1 τk τi pi ni 1 cid2 cid2 k applying T local changes We preliminarily recall Proposition 45 minimality T 94 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 requires pi p j ni n j vice versa j 1 k In following discussion assume pi p j ni n j read τi smaller τ j j The algorithm starts checking condition T 0 N T element Clearly case direct ancestor exists algorithm returns set line 12 Since algorithm works distance elements T order exclude ﬁrst element τ1 τk ﬁctitious elements deﬁned τ0 1 0 τk1 P 1 N 1 line 2 Then element τi T adjacent left right randomly selected lines 1417 Of course 1 τi pi ni smallest element T pi 0 adjacent τi right τi1 line 15 Symmetrically k τk pk nk greatest element T ni N recall N negative threshold bound adjacent τi left τi1 line 16 Then function NewElement invoked passing τi selected adjacent adj line 18 This function works follows First orders element X px nx adjacent Y p y n y way X smallest line 1 Then distances δ X Y computed line 2 Now ways constructing immediate ancestor T 1 adding suitable element τ T 2 replacing element τi T speciﬁc τ generalizes τi Which alternatives applied depends distances δ δ In particular distances greater line 3 intuitively means room X Y accommodate new element T speciﬁc threshold pair p n px p p y nx n n y computed p p y 1 n nx 1 speciﬁc p n highest possible p value lowest possible n value Then T set MinimizeT p n line 18 Minimize function sketched Fig 2 δ As example let consider classiﬁer Hc cid5Pos Neg T cid6 T τ1 τ2 τ1 0 1 τ2 2 3 assume threshold bounds P 2 N 3 Note τ1 smaller τ2 Now suppose algorithm line 13 selects 2 τ2 2 3 Since τ2 greatest element T n2 N algorithm choses left adjacent τ1 line 15 Then function invoked line 18 distances δ p2 p1 2 δ n2 n1 2 computed line 3 Since δ 1 δ 1 holds line 3 function sets p n p2 1 n1 1 1 2 line 4 returns main The resulting threshold set T MinimizeT 1 2 0 1 1 2 2 3 immediate ancestor T Fig 1 If condition distances line 3 apply speciﬁc threshold pair p n generalizes In particular δ 1 X Y computed Again depending values distances δ algorithm generates speciﬁc element p n generalizes Y p n p y 1 n y On contrary δ 1 algorithm generates speciﬁc element p n generalizes X p n px nx 1 Finally conditions hold line 9 algorithm generates speciﬁc element p n generalizes X Y p n px n y As example assume T τ1 τ2 τ1 1 1 τ2 2 2 let 2 τ2 2 2 Suppose chosen adjacent left τ1 Since δ δ 1 conditions lines 3 7 8 applies Thus p n p1 n2 1 2 computed line 9 returned main This element added T line 18 minimization algorithm returns T 1 2 immediate ancestor T Fig 1 δ References 1 R Setiono Extracting MofN rules trained neural networks IEEE Trans Neural Netw 11 2001 512519 2 PM Murphy MJ Pazzani Id2of3 Constructive induction MofN concepts discriminators decision trees Proc Eighth Int Work shop Machine Learning Evanston IL 1991 pp 183187 3 GG Towell JW Shavlik Extracting reﬁned rules knowledgebased neural networks Mach Learn 13 1993 71101 4 Z Zheng Constructing xofn attributes decision tree learning Mach Learn 40 1 2000 3575 5 R Setiono S Pan M Hsieh A Azcarraga Automatic knowledge extraction survey data learning MofN constructs hybrid approach J Oper Res Soc 2005 314 6 T Joachims Learning Classify Text Using Support Vector Machines Kluwer 2002 7 O Larsen AA Freitas JC Nievola Constructing X ofN attributes genetic algorithm Proc Genetic Evolutionary Computation Conference Morgan Kaufmann 2002 p 1268 8 VL Policicchio A Pietramala P Rullo A GAbased learning algorithm inducing MofNlike text classiﬁers Proceedings 10th International Conference Machine Learning Applications Workshops ICMLA vol 1 2011 pp 269274 9 F Herrera Genetic fuzzy systems Status critical considerations future directions International Journal Computational Intelligence Research 1 2005 5967 10 A Pietramala V Policicchio P Rullo I Sidhu A genetic algorithm text classiﬁcation rule induction Proceedings European Conference Machine Learning Knowledge Discovery Databases Part II ECML PKDD08 SpringerVerlag Berlin Heidelberg 2008 pp 188203 11 IH Witten E Frank Data Mining Practical Machine Learning Tools Techniques 2nd edition The Morgan Kaufmann Series Data Management Systems Morgan Kaufmann Publishers San Francisco CA 2005 12 J Bacardit EK Burke N Krasnogor Improving scalability rulebased evolutionary learning Memetic Comput 1 1 2009 5567 13 M Franco N Krasnogor J Bacardit Speeding evaluation evolutionary learning systems GPGPUs Proceedings 12th Annual Conference Genetic Evolutionary Computation GECCO10 2010 pp 10391046 14 WW Cohen Y Singer Contextsensitive learning methods text categorization ACM Transactions Information Systems ACM Press 1996 pp 307315 15 JR Quinlan Generating production rules decision trees Proceedings 10th International Joint Conference Artiﬁcial Intelligence vol 1 Morgan Kaufmann Publishers Inc San Francisco CA USA 1987 pp 304307 16 J Platt Fast training support vector machines sequential minimal optimization B Scholkopf C Burges A Smola Eds Advances Kernel Methods Support Vector Learning MIT Press Cambridge MA 1998 VL Policicchio et al Artiﬁcial Intelligence 191192 2012 6195 95 17 F Sebastiani Machine learning automated text categorization ACM Comput Surv 34 2002 147 18 C Schaffer Overﬁtting avoidance bias Mach Learn 10 1993 153178 19 T Joachims Text categorization support vector machines learning relevant features Proceedings ECML98 10th European Conference Machine Learning No 1398 SpringerVerlag Heidelberg 1998 20 A McCallum K Nigam A comparison event models naive Bayes text classiﬁcation AAAI98 Workshop Learning Text Categorization AAAI Press 1998 pp 4148 21 JD Rennie L Shih J Teevan DR Karger Tackling poor assumptions naive Bayes text classiﬁers ICML 2003 pp 616623 22 JR Quinlan Learning logical deﬁnitions relations Mach Learn 5 1990 239266 23 W Li J Han J Pei CMAR Accurate eﬃcient classiﬁcation based multiple classassociation rules Proceedings IEEE International Conference Data Mining 2001 pp 369376 24 X Yin J Han CPAR Classiﬁcation based predictive association rules Proceedings SIAM International Conference Data Mining 2003 pp 331335 25 F Coenen P Leng The effect threshold values association rule based classiﬁcation accuracy Data Knowl Eng 60 2007 345360 26 A Fernández S García J Luengo E BernadóMansilla F Herrera Geneticsbased machine learning rule induction state art taxonomy comparative study Trans Evol Comput 14 2010 913941 27 SW Wilson Classiﬁer ﬁtness based accuracy Evol Comput 3 1995 149175 28 G Venturini SIA A supervised inductive algorithm genetic search learning attributes based concepts Mach Learn ECML93 1993 280296 29 J Bacardit DE Goldberg MV Butz Improving performance Pittsburgh learning classiﬁer default rule Proceedings 20032005 International Conference Learning Classiﬁer Systems IWLCS0305 SpringerVerlag Berlin Heidelberg 2007 pp 291307 30 JJ Liu JT Kwok An extended genetic rule induction algorithm Proceedings 2000 Congress Evolutionary Computation CEC00 2000 pp 458463 31 DR Carvalho AA Freitas A hybrid decision treegenetic algorithm method data mining Inform Sci 163 2004 1335 32 A Giordana L Saitta F Zini Learning disjunctive concept deﬁnitions genetic algorithm ECAI 1994 pp 483486 33 A Giordana C Anglano A Giordana GL Bello L Saitta A network genetic algorithm concept learning Proceedings Sixth International Conference Genetic Algorithms Morgan Kaufmann 1997 pp 436443 34 F Divina M Keijzer E Marchiori A method handling numerical attributes GAbased inductive concept learners GECCO 2003 pp 898908 35 J Bacardit N Krasnogor Performance eﬃciency memetic Pittsburgh learning classiﬁer systems Evol Comput 17 3 2009 307342 36 E Gabrilovich S Markovitch Text categorization redundant features Using aggressive feature selection SVMs competitive C45 ICMLí04 2004 pp 321328 37 E Baralis P Garza Associative text categorization exploiting negated words Proceedings 2006 ACM Symposium Applied Computing 2006 pp 530535 38 P Rullo L Policicchio C Cumbo S Iiritano Olex effective rule learning text categorization IEEE Trans Knowl Data Eng 21 2009 11181132 39 G Forman I Guyon A Elisseeff An extensive empirical study feature selection metrics text classiﬁcation J Mach Learn Res 3 2003 1289 1305 40 A TamaddoniNezhad S Muggleton A genetic algorithms approach ILP Proceedings 12th International Conference Inductive Logic Programming ILP02 SpringerVerlag Berlin Heidelberg 2003 pp 285300 41 SH NienhuysCheng Rd Wolf Foundations Inductive Logic Programming SpringerVerlag New York Secaucus NJ USA 1997 42 STOC84 Proceedings Sixteenth Annual ACM Symposium Theory Computing ACM New York NY USA 1984 508840 43 L Pitt LG Valiant Computational limitations learning examples J ACM 35 1988 965984 44 Ahn C Wook Advances Evolutionary Algorithms Theory Design Practice Studies Computational Intelligence SpringerVerlag New York Secaucus NJ USA 2006 45 T Baick Optimal mutation rates genetic search Proc Fifth International Conference Genetic Algorithms Morgan Kaufmann San Mateo CA 1993 pp 29 46 DE Goldberg J Richardson Genetic algorithms sharing multimodalfunction optimization ICGA 1987 pp 4149 47 J Bacardit Pittsburgh geneticsbased machine learning data mining era Representations generalization runtime PhD thesis Ramon Llull University Barcelona Spain 2004 48 DP Greene SF Smith Competitionbased induction decision models examples Mach Learn 13 1993 229257 49 AA Freitas Data Mining Knowledge Discovery Evolutionary Algorithms SpringerVerlag New York Secaucus NJ USA 2002 50 J AlcaláFdez L Sánchez S García MJ del Jesús S Ventura JM Garell J Otera C Romero J Bacardit VM Rivas JC Fernández F Herrera KEEL software tool assess evolutionary algorithms data mining problems Soft Comput 13 3 2009 307318 51 F Debole F Sebastiani An analysis relative diﬃculty Reuters21578 subsets Proceedings 4th International Conference Language Resources Evaluation LREC 2004 2004 pp 971974 52 W Hersh C Buckley T Leone D Hickman Ohsumed interactive retrieval evaluation new large text collection research WB Croft CJ Van Rijsbergen Eds Proceedings SIGIR94 17th ACM International Conference Research Development Information Retrieval Springer Verlag HeidelbergDublin 1994 pp 192201 53 E hong Han G Karypis Centroidbased document classiﬁcation Analysis experimental results Principles Data Mining Knowledge Discovery 2000 pp 424431 54 httpwwwcsumassedumccallumdatahtml 55 httpwwwcsuiceduliubFBSbloggenderdatasetrar 56 httpwebistutlptacardosodatasets 57 httpwwwdmozorgrdfhtml contentrdfu8gz 58 J Demšar Statistical comparison classiﬁers multiple data sets J Mach Learn Res 7 1 2006 130 59 Z Zheng R Srihari Optimally combining positive negative features text categorization Workshop Learning Imbalanced Datasets II Proceedings ICML 2003 60 J Bacardit M Stout JD Hirst K Sastry X Llorà N Krasnogor Automated alphabet reduction method evolutionary algorithms protein structure prediction GECCO07 Proceedings 9th Annual Conference Genetic Evolutionary Computation ACM Press 2007 pp 346 353 61 S Chua F Coenen G Malcolm Classiﬁcation inductive rule learning negated features Proceedings 6th International Conference Advanced Data Mining Applications Part I SpringerVerlag 2010 pp 125136 62 MA Franco N Krasnogor J Bacardit Analysing BioHEL challenging boolean functions Evol Intell 5 2 2012 87102 63 DD Lewis Y Yang T Rose F Li RCV1 A new benchmark collection text categorization research Journal Machine Learning Research 5 2004 361397