Artificial Intelligence 72 1995 81138 Artificial Intelligence Learning act realtime dynamic programming Andrew G Barto Steven J Bradtke Satinder I Singh2 Department Computer Science University Massachusetts Amherst MA 01003 USA Received September 199 1 revised February 1993 Abstract Learning methods based dynamic programming DP receiving increasing attention artificial intelligence Researchers argued DP provides appropriate basis compiling planning results reactive strategies realtime control learning strategies controlled incompletely known We introduce algorithm based DP RealTime DP RTDP embedded improve performance experience RTDP generalizes Korfs LearningRealTimeA algorithm problems involving uncertainty We invoke results theory asynchronous DP prove RTDP achieves optimal behavior different classes problems We use theory asynchronous DP illuminate aspects DPbased reinforcement learning methods Watkins QLearning algorithm A secondary aim article provide bridge AI research realtime planning learning relevant concepts algorithms control theory 1 Introduction The increasing environments lem solving embedded dynamic artificial demanding intelligence AI researchers realtime performance narrowing control engineering Similarly machine learning systems comparable systems A growing number researchers investigating methods systems embedded gulf prob adaptive control learning systems techniques suited author Email bartocsumassedu Corresponding Present address GTE Data Services One E Telcom Parkway Temple Terrace FL 33637 USA 2 Present address Department Cambridge MA 02139 USA Brain Cognitive Sciences Massachusetts Institute Technology 00043702950950 SSDIOOO437029400011O 1995 Elsevier Science BV All rights reserved 82 AG Burro et dArtificial Intelligence 72 1995 81138 DP algorithms DP provides appropriate basis compiling planning solving stochastic optimal control results realtime control learning strategies algorithms based DP efficiency conventional DP versions DP incremental reacting learning planning learning learn 83871 Watkins Suttons Dyna architecture short longterm performance based principles The key issue addressed DPbased agent computational known Learning 8 1 proposed incompletely improving arguing based dynamic programming problems reactive strategies controlled employ novel means algorithms Werbos learning algorithms 6970 tradeoff improve longterm mance DPbased autonomous explicit teachers 7 11 performance learning algorithms require examples reinforcement sacrificing agents improve skills environments shortterm perfor learning methods contain In article introduce performance experience types problems RTDP A LRTA prove result recognizing algorithm3 lo This novel observation ideas LRTA apply realtime problem solving longterm different 38 LearningRealTime learning algorithm based DP Real RTDP embedded problem solving results Time Dynamic Programming improve behavior Korfs form DP known asynchronous DP generalize involving uncertainty Bertsekas optimal involving suitability asynchronous DP parallel processing adapt theory performing DP concurrently problem extension RTDP called Adaptive RTDP applicable information problems permits tasks In particular apply theory asynchronous DP developed RTDP converges tasks types realtime problem theory asynchronous DP motivated case solving control We present lacking IO Bertsekas solutions applied Tsitsiklis uncertainty Whereas 121 solution addition closely structure solving related Recognizing theory asynchronous DP relevant algorithm provide new insight learning results comparing ventional DP algorithm simulated uncertainty performance Watkins QLearning learning permits 81821 algorithm DPbased explored AI researchers We present simulation tasks involving RTDP Adaptive RTDP QLearning realtime problem solving Another aim article discuss important issues arise learning algorithms particular attention devoted indicating DPbased aspects use formal attempt relevant concepts believe relevant capable performing control clarify justification links AI research realtime planning In theory We discuss selected concepts control efforts AI develop autonomous learning theory systems real time uncertainty 3 We use term realtime following usage Korf refers problems performed hard time constraints We address details scheduling arise algorithms components complex realtime systems actions issues AG Barto et alArtcial Intelligence 72 1995 81138 83 issues relevant DPbased AI wide variety specific problems easy specify advantage AI A formalism systems best abstract formalism learning But remain applicable discuss For example adopt abstract best apply problems potentially exactly subproblems complex methods learning developing In accord Dean Wellman component sophisticated technology embedded 231 regard DPbased issues agents address reinforcement important addresses Because reader unlikely proper concepts Section 2 followed familiar contributing necessary background relationship lines Section 3 AI control theoretical material occupies Sections 4 9 class stochastic optimal control problems occupying Section 4 conventional DP occupying Section 5 There major parts properties additional accurate model learning algorithms research provide discussion theory Development introduction introduction theoretical development The Sections 5 6 concerns problems available Here RTDP accurate models complexity problem outside issues In Section 10 use example problem conclude open problems In Section 9 discuss learning algorithms address illustrate RTDP algorithms We Section 11 appraisal significance approach discuss lacking Section 8 brief discussion DPbased scope article DPbased relationship present LRTA The second case incomplete Section 7 concerns convergence implementations theoretical practical information 2 Background A major influence research leading 6162 method Samuel checkers His method updated board evaluations current board position evaluation board position game modify heuristic evaluation comparing likely current DPbased algorithms function game evaluation arise later attempting look like calculated probably occur actual play Samuel current board position terminal board position chain moves score calculated 611 As result process backing board evaluations improve ability version algorithm Samuel features adjusted numerical evaluations current predicted board positions evaluate longterm function moves In represented function weighted sum weights based error derived comparing evaluation evaluation consequences Because compatibility connectionist learning algorithms refined extended Sutton 67681 heuristically approach number 84 AG Barto et alArhjicial Intelligence 72 1995 81138 solving problem theoretical 41 Sutton neuronlike Following methods 671 The algorithm implemented tasks Barto Sutton Anderson ideas context credit assignment problem TD methods obtained proposals Klopf models animal singleagent 11 Sutton element called Adaptive Critic Element Temporal Difference convergence developed similar systems Hampson animal behavior Christensen updating evaluation bucketbrigade Samuels method Tesauros recent TDGammon connectionist achieved 4 Anderson connectionist 681 later called algorithms 72741 discussed learning developed ideas related Korf 161 experimented Samuellike method 301 related 771 program TD method playing backgammon 36371 Sutton Barto learning Minsky function coefficients linear regression Hollands closely credit classifier systems improve performance 281 independently reinforcement results assigning remarkable algorithm success network 5354 Independently approaches inspired Samuels similar algorithms based theory optimal control control problems DP term optimal approximating stochastic problems solution methods As applied 91 consists methods successively rules deterministic checkers player search space compositional suggested decision form DP applies searchers important DP provides introduced Bellman evaluation functions In general objects object globally minimum Kanal applies problem recurrence ing state evaluations relations We discuss DP algorithms problems solving control relations objects state sequences optimization tasks DP solves instead explicitly searching basic step DP procedures Section 5 structure problems exploited costs search Kumar DP generated problems solving space state sequences Back recurrence solving cost performing 40 discuss DP level generality However restrict attention exhaustive optimization contrast explicitly designed Although DP algorithms avoid exhaustive search statesequence require repeated generation exhaustive AI standards possible states For reason DP played significant search algorithms way But DP algorithms relevant adjust problems heuristic evaluation shallow searches Although estimates costs typically update goal state h function 4 space expansion role AI Heuristic avoid exhaustive way heuristic search algorithms results repeated A 291 update As g function cost reach heuristic search algorithms state reach states heuristic evaluation evaluations states function incorporating learning update function estimating systematically initial effect Despite possible fact DP algorithms arrange computational exhaustive steps use control realtime sense described 4 We exceptions M6 5 I Gelperin 261 Although functions developed independently DP heuristic algorithms use DPlike backups search literature algorithms proposed update heuristic evaluation AG Barto et al Artijicial Intelligence 72 1995 81138 85 problem solving This basis RTDP algorithms article In cases convergence optimal evaluation function requires repeated generation expansion states performance improves incrementally necessarily monotonically accomplished It improvement ultimate convergence optimality central This perspective taken Werbos 851 proposed method similar Adaptive Critic Element framework DP He called approach Heuristic Dynamic Programming written extensively 8386 88 Related algorithms discussed Witten 9293 recently Watkins 811 extended Suttons TD algorithms developed explicitly uti lizing theory DP He term Incremental Dynamic Programming refer class algorithms discussed examples Williams Baird 91 theoretically analysed additional DPbased algorithms suitable ontime application We come work Jalali Ferguson 321 independently pro posed method similar Adaptive RTDP Sutton Barto Williams 751 discussed reinforcement learning perspective DP adaptive control White Jordan 89 Barto 2 provide additional background extensive references current research Although aspects approach apply problems involving continuous time andor state action spaces restrict attention discretetime problems finite sets states actions relative simplicity closer relationship nonnumeric problems usually studied AI This excludes differential approaches use optimization algorithms related connectionist errorbackpropagation algorithm Jacobson Mayne 3 11 Jordan Jacobs 331 Werbos 83841 White Jordan 891 The relevance DP planning learning AI articulated Suttons 69 Lym architecture The key idea Dyna perform computational steps DP algorithm information obtained state transitions actually taken controlled hypothetical state transitions simulated model To satisfy time constraints approach interleaves phases acting planning performed hypothetical state transitions The underlying DP algorithm compiles resulting information efficient form directing future course action Another aspect Dyr model refined learning process deriving training information state transitions observed control Even online model refinement executing DP algorithm concurrently generation actions implications planning AI discussed Sutton 701 In article introduce fact theory asynchronous DP applicable analysis DPbased reinforcement learning algorithms Asynchronous DP algo rithms differ conventional DP algorithms proceed systematic exhaustive sweeps problems state set Bertsekas lo Bertsekas Tsitsiklis 121 proved general theorems convergence asynchronous DP applied discretetime stochastic optimal control problems However motivated suitability asynchronous DP parallel processing relate results realtime variants DP article To best 86 AG Baro ef Artificial Intelligence 72 1995 81138 knowledge asynchronous DP realtime control work explicit use theory Jalali Ferguson 321 Korfs current state generating heuristic search algorithm improves repeated 381 LRTA algorithm caches state evalua trials Evaluations states tions search performance hash table Each cycle algorithm visited problem solver maintained successor immediate proceeds expanding exist hash states evaluating previously function Assuming table initially given heuristic evaluation objective cost edge current neighboring state The minimum current state stored hash table Finally lowestscoring way backs state evaluations neighboring Samuels algorithm DI In fact shall follows slight caveat LRTA specialization asynchronous DP applied online path goal state score computed minimumcost deterministic new evaluation stored evaluations evaluation state adding scores resulting state LRTA 3 Heuristic search control dynamic systems relatively Whereas AI focused problems having structure theorists studied restrictive classes problems developed theory example Dean stochastic optimal theory control correspondingly detailed Wellman control heuristic search realtime heuristic search selected concepts theories Some concepts methods control AI discussed introducing results cast discuss relationship control 231 In section prelude problems little mathematical framework relevant 31 Heuristic search control Heuristic optimizes possibly maps statespace search algorithms states initial sequence operators apply map states states set operators states The objective goal states solution path These components solving puzzle proving theorem planning literature heuristic deciding manipulating similarities refers process manipulating supplying process time Unlike models manipulated search problems defined set state set goal initial state measure cost merit robot path The term control process question Despite theory real time formal search In AI control specifies behavior physical search algorithms physical systems set meaning term control appropriate control model real problem input signals steers behavior physical model problem search problem solving means control constitute theory 5 In Korfs 1381 related RealTime A RTA algorithm score control DP RTA discuss RTA second smallest LRTA closely related stored Because AG Barto et al Artificial Intelligence 72 1995 81138 87 systems control search states suspend activity await controllers control problems called dynamic systems passage time In follows control arbitrary immediately decisions Models formalize explicit mean taking control dynamic account In applications symbolic sequence methods control engineers applied final objective heuristic operator Here result substantially inputs actions produced policy meaning systems actual behavior control feedback producing priate given design procedure normal circumstances precedes execution phase In terms control openloop generate time sequence actual representation sequence operators search algorithm The intent execute form control control method differs theory A sequence way heuristic search openloop control addressed control physical inputs underway theory heuristic control policy model initial state Further normal circumstances completed control planning phase problem information execution monitoring search control design procedure appro ofline solving process strictly policy completely accurate model physical Openloop determine 2 physical systems deterministic 4 problems problems Any uncertainty process modeling performance Control real controller control works fine following true 1 model control policy 3 physical hold AI true realistic control behavior physical closedloop control produce better closedloop action depends current observations internal initial state exactly determined unmodeled studied disturbances These conditions past observations information implies example Chapman It closely corresponds A closedloop control policy called closedloop rule specifying behavior controlled 641 discussed In control controlled ables distinction significance 141 Although closedloop counteracts deviations special case closedloop control rule law strategy action function current possibly past information universal plan 651 control policy usually specifies action function current values observable vari discussed Chapman closely associated negative feedback merely desired behavior negative feedback control systems current state universal planning 271 Schoppers theory closedloop 141 Ginsberg control control When control closedloop uncertainty policy initial control For deterministic principle compe given tent openloop produces closedloop openloop policy generated running exactly simulating perfect model control given closedloop policy But true stochastic case unmodeled distur bances outcome random unmodeled state exists openloop policy behavior disturbances events anticipated 88 AG Barto et alArtcial Intelligence 72 1995 81138 systems use closedloop kind disturbance A game player For exactly opponent reason actual previous moves determining openloop policy Note gameplaying designing control uses opponents reasons closedloop problems closedloop acceptable designing practical alternative expensive impossible controlled control control engineers control policy significantly control better openloop control uncertainty A corollary explains systems behavior sufficient closedloop faithful closedloop involving control universal use actual control monitor instead openloop policies Openloop model designing singleagent determining accurate model theory addresses Most control offline assumption available The offline design procedure method possible control problems redesign control objectives hand require policy redesign problem designing adequate closedloop policies controlled efficient state If policy offline necessary perform additional instances differing initial state Changing studied engineers problem action function observed typically yields computationally design complete replanning closedloop One design closedloop policies online repeated online design 4250 discarded finitehorizon designed current state playing searching action specified resulting policy repeated requiring online design AI corresponds openloop policies This approach called receding horizon control For current state openloop policy role initial state The design procedure terminate time constraints imposed online operation This designing openloop fixed depth current policy example model remainder state After applying state policy Despite projection policy control policy According fixed time retain projection planning phase complete design systems methods closedloop control objectives current state closedloop view closedloop policy involve explicit planning observed online planning receding horizon control react online observed states In contrast reactivity policies offline receding horizon control produces model design process reactive prediction changes 32 Optimal control trajectory These called The familiar control objective reference output tracks reference disturbances regulation optimal control problem hand function controlled terms reference output trajectory One typical optimal control problem controlling trajectory control output matches face closely possible In requires tracking problems control objective systems behavior function need defined state goal state minimumcost respectively extremize tracking problemswhere In contrast initial desired trajectory AG Barto et al Artificial Intelligence 72 1995 N138 89 trajectory solution optimal control problem specificationthe problem Therefore optimal control problems closely related problems heuristic search algorithms apply Specialized solution methods exist optimal control problems involving linear sys tems quadratic cost functions methods based calculus variations yield closedform solutions restricted classes problems Numerical methods applicable problems involving nonlinear systems andor nonquadratic costs include gradient methods DP Whereas gradient methods optimal control closely related gradient descent methods studied connection ists errorbackpropagation algorithm 438689 DP methods closely related heuristic search Like heuristic search algorithm DP offline procedure designing optimal control policy However unlike heuristic search algorithm DP produces optimal closedloop policy instead openloop policy given initial state 33 Realtime heuristic search Algorithms realtime heuristic search defined Korf 381 apply statespace search problems underlying model extended account passage time The model dynamic Realtime heuristic search algorithms apply statespace search problems additional properties 1 time unique current state controlled known searchercontroller 2 sequence time intervals constant bounded duration searchercontroller commit unique action choice operator 3 changes state end time interval manner depending current state searchercontrollers recent action These factors imply fixed upper bound time searchercontroller deciding action action based uptodate state information Thus traditional heuristic search algorithm design procedure openloop policy realtime heuristic search algorithm control procedure accommodate possibility closedloop control Korfs 38 LRTA algorithm kind receding horizon control online method designing closedloop policy However unlike receding horizon control studied control engineers LRTA accumulates results local design procedure effectiveness resulting closedloop policy tends improve time It stores information shallow searches forward current state updating evaluation function control decisions Because updates basic steps DP view LRTA result interleaving steps DP actual process control control policy design occurs concurrently control This approach advantageous control problem large unstructured complete control design feasible offline This case mathematically requires partial closedloop policy policy useful subregion problems state space Designing partial policy online allows actual experience influence subregion state space design effort concentrated Design 90 AG Bcwio et 01 Artificial htellience 72 1995 81138 effort expended actual control Although general subset states design procedure considers possible certain conditions LRTA parts state space likely visited optimal entire state set theorem required Korfs convergence design policy possible 34 Adaptive control available interact A distinction time controller theorists use term adaptive control Control cases accurate model controlled policy offline These designing called control problems incomplete information Adaptive control algorithms design policies online based information control problem accumulates adaptive control learning control takes advan tage repetitive control experiences useful types control long kinds problems problems think mean adaptive control algorithms consider 61 algorithms article assume learning adaptive control algorithms algorithms certainly existence accurate model problem solved Although ipso facto adaptive odd control algorithm forced learns restrictive definition adaptive control adopt control engineers In Section 7 properties learning adaptive control algorithms information distinction useful limited applied term Although utility like LRTA Samuels algorithm article According algorithms acquired 4 Markovian decision problems The basis theoretical framework stochastic versions problems class stochastic optimal control prob simplest class problems heuristic include stochastic fact time allowing borrow literature Frameworks applications present research important uncertainty lems called Murkovian decision problems This include general apply search algorithms developed control operations problems presence uncertainty openloop control tions inputs Although applications complex behaviors Many problems practical Markovian decision problems framework action highlevel extensive dynamic action command gives closedloop reactive control advantages In Markovian decision problem operators primitive theory important probabilistically determine form ac successor states executes repertoire understand importance formulated treatment theory application 1 l Ross books Bertsekas 1601 AG Barto et al Artcial Intelligence 72 1995 N138 91 state defined selected A Markovian action decision problem If observed terms discretetime instants real time states actions immediate controller executes action u E Ui stochastic dy namic finite state set S 1 n Time represented sequence In Section 6 introducing RTDP treat sequence time steps tOl best treat merely abstract specific systems current state selects sequence At time step controller observes control action simply crion executed applied input finite set Ui admissible systems actions When probability pij u We state time step j statetransition application action u state incurs immediate cost ci u 7 When assume costs time steps necessary refer occur s u cf denote respectively state action immediate cost time step t uI E U s ct cs Us We discuss significant extension complete important state scope article introduces A closedloop policy specifies action function observed state Such executes action pi E controller policyisdenotedln stationary policy Ui observes article use term policy change time Throughout fp called evaluation mean stationary policy For policy function costfunction corresponding state total controller uses policy p starting cost expected accumulate u state define fp expected value injnitehorizon discounted cost accrue time given u initial state controller uses policy studied extensively given state Here policy u function certainty Although formalism time complexities observe possibility controller u It assigns current practice policy This state 1 controller future immediate uses policy y 0 y 1 factor discount expectation assuming cost state policy u Thus policy p ci p cost state policy p expected discounted immediate Theorists function giving average cost time step consider formulations costs Eti u We refer fi simply immediate cost state sum state functions study Markovian decision problems types evaluation incurred future starting costs 6 In control Al theory simply called control We use term action term commonly 7 To general alternatively regard immediate depending states actions action u state theory discussed remains unchanged In case clu denotes expecfed costs bounded random numbers immediate cost application 92 AG Barro et alArtcial Intelligence 72 1995 8113R The objective type Markovian objective minimizes policy achieves unique denote corresponds optimal cost function denoted For state fi policy evaluation cost state optimal policy decision problem consider defined Eq 1 A policy depends y To optimal policy optimal evaluation function u lpn function f u optimal policy ffi f optimal cost state possible cost state This infinitehorizon discounted version Markovian decision problem sim costs states finite ensures discounting optimal policy factor y determines strongly expected plest mathematically policy The discount current control decisions When y 0 cost state immediate transition E COSO ci u In case optimal policy simply selects actions imize minimum significant computation stationary8 influence cost O I Eq 1 fpi min gives future costs require optimal actions solution methods generally cost state optimal evaluation costs As y increases future costs state This immediate determining immediate function case thinking states assumptions case undiscounted When y 1 undiscounted costs arc lengths set states cost state given Eq produce welldefined required problems closely In problems Bertsekas Tsitsiklis immediate absorbing 1 need decision heuristic search 121 stochastic shortest graph nodes set states left immediate cost associated applying action finite additional problems We consider set assumptions resulting decision problems related applied path problems correspond entered states absorbing evaluation set assigns finite taking costs state y 1 This true finite number immediate stationary The discounted shortest absorbing solved path problem heuristic optimal closedloop policy optimal path given initial state costs incurred policy time zero Additionally case optimal policy goal set However unlike set goal states deterministic imply absorbing set zero These assumptions search objective set states corresponds infinitehorizon tasks typically policy function AI researchers studying reinforcement immediate delivered lems reward stochastic ment 67 formalism controller shortest path problems learning focus shortest path prob costs zero goal state reached new trial begins These special kinds issue delayed reinforce negative costs case rewards address particularly stark form Rewards correspond In discounted R In finitehorizon problems optimal policies arc generally nonstationary different actions optimal given state depending actions remain horizon reached AG Barto et alArtiial Intelligence 72 1995 81138 93 A Starting line Finish line 1 Fig 1 Example race tracks Panel A details Stwting line Finish line small race track Panel B larger race track See Table 1 Section 10 shortest path problem optimal policy produces state Another magnitude example stochastic positive value instead zero In case optimal policy produces shortest path goal state optimal control undiscounted problems shortest path receiving examples minimumtime nonrewarding case Such problems rewarding costs identical immediate attention 41 An example race track problem To illustrate Markovian decision framework formalize Track described Martin Gardner game use Section 10 compare learning algorithms considering 25 simulates automobile game called Race racing We modify performance DPbased single car making probabilistic A race track shape drawn graph paper starting designated line consisting squares Each square location car Fig 1 shows example car placed starting finish boundary track possible At start sequence trials random position track finish u squares vertically previous v squares horizontally difference h h 1 0 1 difference v v 1 0 1 This means slow speed dimension square If car hits moves line Acceleration car moved h squares horizontally present h squares vertically deceleration speed dimension car maintain car attempts simulated follows line end tracks line If 94 AG Barto et nlArtiJciol Intelligence 72 1995 81138 zero h h U c considered track boundary 9 random position starting line reduce velocity trial The objective line moves possible Figs 2 4 examples optimal nearoptimal paths race tracks shown zero continue car crosses Fig 1 learn control finish In addition difficulty discovering easy car gather speed negotiate matters worse introduce random actual accelerations accelerations actions executed One unpredictably cars velocity Although decelerations decelerations Thus 1 p probability faster ways reach finish tracks curves To factor problem With probability p intended intended zero independently controllers line think simulating driving track slippery braking throttling effect requiring formulate learning difficult example problems way best suited sensory motor capabilities Race Track problem intention vehicle realistic task abstract stochastic define time step t 0 I lems autonomous representative formulate entire formulation s xt yr jr The integers horizontal cars location second vertical directions That x xt t similarly state set pairs u uJ uX uy set lO We let 14 u u denote action admissible suggestive robot motion navigation prob design Instead regard skills shortest path problem The step controlled The state quadruple integers vertical coordinates horizontal speed car time step actions 1 u assigns horizontal assume xt yt 0 The set admissible action time state action time step t integers speeds represented t A closedloop vt yt ytt dynamic policy PSt Ell01101000111 I lOl The following I p controllers equations define action state transitions With probability reliably executed state time step t 1 Xtl xritu Ytt yju I xtu jrl p UT 2 probability time step t 1 p ignores controllers action state For computational experiments described Section 10 means projected path car intersects track boundary place finish line AG Barto et alArttjicial Intelligence 72 1995 81138 95 3 Xlfl Yfl 1 Xt Xt yr P Pl jr point line joining randomly straight x y point x1 ytt This assumes lies entirely track intersects finish line If case car collided tracks boundary state t 1 x y 0 O car x y finish line treated valid assume stays resulting track Eqs 3 2 define statetransition states admissible takes car subsequently state new trial begins This method keeping chosen position starting car line A probabilities actions line states To complete finish line inside time step crossing states x y 0O set goal states immediate function defined set absorbing The immediate state The set start states consists zerovelocity x y coordinates formulation stochastic shortest path problem need define costs associated states line The set goal states consists states track According set start states action starting squares making starting reached statetransition nongoal nongoal transition guaranteed For policy L expected number moves car cross finish line state controlled cost cross finish line state policy car expected quickly possible smallest expected number moves action actions U The immediate zero If restrict attention finish cost ciU 1 cost associated policies use discounting states goal state car policy Jo An optimal policy minimizes state The optimal cost state fi cost fp state line need admissible u undiscounted finish line infinitehorizon independently starting taken The total number states depends configuration imposed limit cars speed set states reached considered state set stochastic shortest path problem potentially race track finite infinite However set start states policy 42 The optimay equation To set stage discussing DP provide relationship evaluation policies cost state policy best successor greedy policy respect states evaluated fp evaluation functions Although evaluation u p necessarily function select actions In words p necessarily function ffi gives lead To define greedy policy stochastic case use Watkins 8 1 Q notation plays role realvalued function states QLearning method described evaluation Section 73 Let f policy function 96 AG Rrrrtcl 1 iri Artifi Intrlliwzw 72 1995 RIl38 guess good evaluation search arbitrary function function function For state action II E Ori let heuristic evaluation Q c II c c II y c p c If I j heuristic 4 Qi M cost action 11 state immediate cost discounted states action u If systems state transitions simplifies It sum expected value costs possible successor Eq 4 evaluated f deterministic Qi u cu rfCj operator zc In deterministic successor state action II node j j edge corresponding think u summary result oneply edge corresponding generalization having different probability followed policy Q u gives cost generating policy Using child node case node operator u evaluated f The stochastic case requires operator following function action II state policy fi greedy respect f states u view edges correspond If 1 evaluation Qvalues lookahead action satisfying greedy policy respect f state let CL denote policy f Also note policy greedy respect set Qvalues Although action minimizes greedy respect different evaluation A key fact underlying functions evaluation DP methods functions policies optimal policies That optima1 evaluation greedy u f function evaluation function state p satisfies This means respect optimal policy p uf Qjipi min Qill lli policy defining possible greedy respect define optimal policy simply defining Furthermore f known Eq 5 values policy current state Deeper search necessary information f optima1 policy Thus satisfy f Due definition Q stochastic case fact optima1 policy bestfirst respect f determined oneply search search obtain greedy respect f summarizes generalizes Eq 4 Letting Q u Qf u simplify notation related key fact necessary f optimal evaluation condition function sufficient state true AG Barto et alArtcial Intelligence 72 1995 81138 fi min Qiu UEUi min UEUi jES 1 ciu rjufj 97 6 form Bellman Optima Equation solved E S DP algorithm It set n number states equations The form equations depends dynamic simultaneous costs underlying decision problem Once f optimal action state determined This fi nonlinear immediate follows actions u E Vi determined Eq 4 steps n number states knows search computational actions state zero usually Om lookahead Qvalues amounts state However oneply systems statetransition probabilities computation probabilities takes Omn The Qvalues Q u admissible In general m number admissible statetransition case deterministic deterministic case Computing state Using m 1 comparisons The computational method DP algorithm requires knowledge Qvalues optimal action determined takes complexity finding optimal action f complexity Fq 5 dominated complexity finding 5 Dynamic programming probabilities principleto possibleat actions u E Ui including called policy versions basic DP algorithm Given complete accurate model Markovian decision problem ii immediate form costs ci u knowledge statetransition states solve decision problem offline applying wellknown DP algorithms iteration There We DP algorithms algorithms scope article briefly discuss policy based iteration oth iteration erwise noted As approximation successive func Bellman Optimality tion f It successive Equation basic operation state costs There variations value organized We chronously backing estimates optimal depending procedure approximation method called value iteration learning Section 8 We treat DP referring optimal evaluation computations solving Markovian backup operations decision problems converges applies version solving interaction value iteration value syn 51 Synchronous dynamic programming Let fk denote estimate f available stage k DP computation At stage k fk estimated optimal cost state refer kOl 98 AG Barto et alArticial Intelligence 72 1995 RI138 simply stagek cost state similarly refer fk stagek evaluation function policy We use index k stages DP computation use t denote time step control problem solved terms fk follows defined In synchronous DP k 0 1 fkl actually evaluation state function fc given equation operation backedup cost saved future use Here saved updating initial estimate f We refer application update common backedup cost state variety search algorithms backing cost Although backing costs AI mean evaluation function 7 sequential implementation equation synchronous If The iteration defined Eq 7 righthand associated state applying Eq 7 states means backs cost state time supplied processors This process updates values fk simultaneously Alternatively locations The sequential ordering backups storage stage k 1 costs computed based stagek costs values fki appear imagines having separate processor processor old costs states result If n states m largest number admissible state iteration consists backing cost state exactly requires Omn2 operations deterministic desirable converges iteration value MIPS processor AI control problems let repeat process 102 states single case 1000 years 1000 try complete iteration f For example backgammon stochastic case Omn operations case For large state sets typical iteration irrelevant temporary iteration requires actions If y I repeated synchronous iterations produce sequence functions verges optimal evaluation cost state need closer error fk f states decrease function optimal cost iteration 111 f initial estimate fo Although maximum sequence functions Synchronous DP offline versions value iteration discuss f y 1 formed f form greedy f optimal policy But possible Instead executes value forms policy generates explicitly generate sequence policies To stagek evaluation corresponds Ideally wait policy corresponding practice value iteration converges asymptotically meets test approximate iteration greedy policy policies explicitly sequence converges converges convergence function AG Barto et al Artial Intelligence 72 1995 81138 99 resulting evaluation function lo It important note function sequence evaluation functions generated value iteration closely approximate f order corresponding greedy policy optimal policy Indeed policy corresponding stagek evaluation function k optimal long algorithm converges f But unaided computations value iteration detect happens This fact important reason online variants value iteration discuss article advantages offline variants Because controller uses policy defined current evaluation function perform optimally evaluation function converges optimal evaluation function Bertsekas 1 l Bertsekas Tsitsiklis 121 conditions ensuring conver gence synchronous DP stochastic shortest path problems undiscounted case y 1 Using terminology policy proper use implies nonzero proba bility eventually reaching goal set starting state Using proper policy implies goal set reached eventually state probability The existence proper policy generalization stochastic case existence path initial state goal set Synchronous DP converges f undiscounted stochastic shortest path problems following conditions 1 initial cost goal state zero proper policy 2 3 policies proper incur infinite cost state The condition ensures optimal policy proper rules possibility leastcost path exists reaches goal set One condition true immediate costs transitions nongoal states positive QU 0 nongoal states actions u E Ui I1 In deterministic case conditions 2 3 satisfied solution path state sum immediate costs loop positive 52 GaussSeidel dynamic programming GaussSeidel DP differs synchronous version costs backed state time sequential sweep states computation state recent costs states If assume states numbered order sweep proceeds order result iteration GaussSeidel DP written follows state k 0 1 Policy iteration contrast explicitly generates sequence policies converges optimal policy finite number iterations finite number states admissible actions assuming However policy iteration shortcomings discuss Section 8 The assumption positive immediate costs weakened nonnegativity ci 2 0 E S u E Ui exists optimal proper policy 121 IO0 AG Burro et trl Arrijicitrl Intelligence 72 1995 81138 jifki t min LIEUi cU YCPijUfj iE7 I 8 j f Unlike synchronous DP order states costs backed influences computation Nevertheless GaussSeidel DP converges ditions synchronous DP converges When y 1 repeated GaussSeidel stochas sweeps produce sequence functions tic shortest path problems synchronous DP ensure convergence GaussSeidel DP 121 Because cost converges backup uses latest costs states GaussSeidel DP generally clear state orderings faster synchronous DP Furthermore produce shortest path problems goal states backwards likely shortest paths usually converges described problem For example leads faster convergence forward direction ensure convergence depending f For undiscounted faster convergence sweeping conditions sweeping Although GaussSeidel DP algorithms article solve example problem described bridge synchronous DP asynchronous 53 Asynchronous dynamic programming direct Section 41 serves form discussed developed terms systematic lo 121 asynchronous DP suitable time delays common GaussSeidel DP state successive organized multiprocessor systems clock For state E S Asynchronous DP similar costs simultaneously However sweeps state set As proposed Bertsekas Bertsekas Tsitsiklis communication separate processor dedicated processor responsible processor backs cost state different cost state processor uses costs states available awakens utility discuss lies fact require state costs backed systematically organized backing cost state generally number states The times processor To obvious algorithms asynchronous DP speeding DP practical significance perform backup Multiprocessor 44 However Lemmon implementations fashion Although asynchronous model stages apply processor awaken continuum times use facilitates discussion RTDP notion iteration estimate f available stage section As forms DP let fk denote notion discrete computational stage AG Barto et d Artificial Intelligence 72 1995 81138 101 costs remain unchanged k computation At stage k costs subset states backed synchronously states The subset states costs backed changes stage stage choice subsets determines precise nature algorithm For k 0 1 Sk c S set states costs backed stage k fkl computed follows fkfl min Qjiu uWi fkfi E Sk 9 According algorithm fkl differ fk state Sk Further unlike GaussSeidel DP costs states possibly depending costs backed states backed GaussSeidel Asynchronous DP includes special cases synchronous DP results Sk S k GaussSeidel DP results Sk consists single state collection Sks defined implement Snl n successive sweeps entire state set Sa l St 2 synchronous times algorithms S I Snl 2 I Discounted asynchronous DP converges selection It follows means f provided strategy eliminate state contained backed infinitely provided subsets Sk k 0 1 In practice states cost backups future In undiscounted convergence asynchronous DP converges cost state backed infinitely conditions synchronous DP met 1 convergence 2 zero proper policy 3 policies infinite cost state incur realize cost state infinite number selecting ensure shortest path problems Section 51 given initial cost goal state proper case y 1 additional assumptions result Bertsekas state possible single backup states cost asynchronous DP estimate states optimal cost worse However cost state conditions optimal cost repeated backups Further GaussSeidel DP rate convergence undiscounted appropriate 12 p 4461 Tsitsiklis necessary stochastic improve states costs backed influence way This fact underlies algorithms supplying utility strategies teaching experience dictating selected orderings It important necessarily fact converges order problemdependent DPbased backups learning Lin 481 Utgoff Clouse 801 Whitehead 901 6 Dynamic Programming real time The DP algorithms described offline algorithms solving Markovian function cision problems Although sequence stages stages related time steps decision controller performs problem solved Here consider concurrently asynchronous DP concurrently actual process control optimal evaluation successively approximate algorithms 102 AG Barr0 et alArtcial Intelligence 72 1995 81138 2 1 control decisions process executing follows DP computation selection states costs stored The asynchronous flexibility interaction guide behavior DP computation relevant RealTime DP RTDP present Throughout controller automatically control revealed actions The concurrent DP control processes information based uptodate state sequences generated control interact applied estimated role influence version DP appropriate stages defined As consequence DP backup operation uses intermediate results DP computation focus regions state set systems behavior The algorithm results interaction specific characteristics relation available When complete accurate model section assume AI In 70 discusses case Sutton adaptive case complete accurate model execution DP control carried simulation mode decision computational focus relevant parts model decision problem novel offline DP computation actual underlying offline DP ability surrogate planning realtime computation regard concurrent simulation mode form learning This fact 61621 771 Learning occurred simulated games learning realtime use DP applies Although emphasize reader aware discussion programs Samuel gameplaying formulation To abstract discretetime simulation mode execution DP control think time steps tOl Markovian decision problem indices sequence instants real time controller execute t let k total control actions Let s state observed t Then fk latest number asynchronous DP stages completed estimate optimal evaluation controller select action U E Us When controller executes uI incurs immediate cost c u action ul sI By time yield fk selected additional We let Bt denote set states costs backed stages Note states B costs backed stages time time available stages asynchronous DP stages completed state changes systems function model decision problem Section 7 discuss decision problem concurrent problem The result advantages conventional state set Despite execution DP control learning accomplished Tesauro systems competed based algorithms use algorithms concurrent learning 61 Realtime DP RTDP refers cases concurrently executing DP control processes influence follows First greedy respect greedy action respect resolved recent estimate f This means fk Moreover ties selecting randomly way ensures continuing U actions selection controller follows policy AG Barto et alArtcial Intelligence 72 1995 81138 103 execution u urt cost s greedy actions Second backed st E B r In simplest case Bt st t cost s backed time step t generally B contain search states addition search s For example B consist states generated exhaustive forward fixed search depth consist states generated search st generated type lookahead bestfirst according fk RTDP converges f Because required We described associated continues t The conditions ruled having case condition Section 53 ensuring asynchronous DP computation greedy respect asynchronous DP visit state There approaches assume engineering f apply executed concurrently control Consequently controller takes actions converges current estimate f RTDP converges optimal control performance attained converges discounted state completely backs cost current state way achieve controller One approach Markov process resulting nonzero probability RTDP converges stochastic absorbing state goal state A second way ensure allow proper subsets states convergence RTDP cost backed Because RTDP sure use policy ergodic This means trials A trial consists time RTDP new trial begins state selected start states problems possible RTDP simulation mode use multiple infinitely interval nonzero bounded duration set new starting state set possible performed After interval l3 Obviously visiting state matter actions executed Discounted method impossible trivial stochastic shortest path problem shortest path problems assumption However state visited satisfied unsatisfactory assumption approach ensuring literature 62 Trialbased RTDP infinite series trials If initial states trials selected state selected state visited oftenif start trial randomly probability initiated infinite series trials Then infinitely infinitely start infinite number trials A simple way accomplish selected state state nonzero selected By TrialBased RTDP mean RTDP trials start state infinitely result noting state probability immediate obviously following theorem switch optimal policies select greedy actions This results nonstationary optimal policy When optimal policy controller continue RTDP continues different optimal actions taken state different occasions t3 RTDP interrupted cost starting influencing end trial cost state trial influenced caused trainer state trial This prevents state transitions evaluation function 104 AG Barto et al Artificial Intelligence 72 1995 81138 discounted DP computation probability case TrialBased RTDP gives rise convergent asynchronous method terminating trials Theorem 1 For discounted Markov decision problem initial evaluation function TrialBased RTDP converges defined Section 4 probability It natural use TrialBased RTDP undiscounted stochastic shortest path prob lems trials terminate goal state reached predetermined number time steps Because TrialBased RTDP gives rise chronous DP computation probability path problems result asyn shortest Section 53 following undiscounted convergent conditions enumerated stochastic stochastic shortest path problems TrialBased RTDP Theorem 2 In undiscounted verges probability following 1 initial cost goal state zero 2 3 policies proper policy proper conditions incur infinite cost state TrialBased RTDP interesting relax requirement stochastic shortest path problems yield function complete optimal policy Consider trial solving undiscounted subset start states trials start We state start state s optimal policy exist reached optimal states use optimal policy If knew save controller uses policy It suffices apply DP states irrelevant relevant states states possibly policy time space But clearly possible knowing complete optimal evaluation based approach designated relevant state s restricted occur states relevant considerable states relevant seeking requires knowledge optimal policies However certain conditions continuing function policy policy converges costs irrel f relevant equals optimal relevant states backed More trials incrementally estimated state costs allocated evant states TrialBased RTDP converges states controllers states The costs irrelevant memory exhaustive memory RTDP tends focus computation computation following requirement conventional DP avoided TrialBased restricts set relevant states eventually possible stated precisely set Conditions theorem proof given Appendix A Theorem 3 In undiscounted stochastic shortest path problems TrialBased RTDP initial state trial restricted set start states converges probability f set relevant states controllers policy converges AG Barto et alArtijicial Intelligence 72 1995 81138 105 optimal policy possibly nonstationary set relevant states following conditions 1 initial cost goal state zero 2 proper policy I4 3 immediate costs incurred transitions nongoal states positive 4 ciU 0 nongoal states actions u E Ui initial costs states nonoverestimating foi 6 fi states E S Condition 4 satisfied simply setting fo 0 The significance Theorem 3 gives conditions policy optimal relevant states achieved continuing devote computational effort backing costs irrelevant states Under conditions RTDP yield optimal policy state action sets large feasibly apply conventional DP algorithms computation saved clearly depend characteristics problem solved branching structure Moreover RTDP applied online instead simulation mode evaluation function changes greedy policy shows improvement controller automatically takes advantage improvement This occur evaluation function close f Although discounted undiscounted problems eventual convergence RTDP depend critically choice states costs backed execution actions cost current state backed judicious selection states accelerate convergence Sophisticated exploration strategies implemented selecting states based prior knowledge information contained current evaluation function For example trial based approach stochastic shortest path problem guided exploration reduce expected trial duration helping controller goal states It makes sense RTDP costs states current costs accurate estimates optimal costs successor states accurate current costs Techniques teaching DPbased learning systems suggesting certain ups 468090 rely fact order costs states backed influence rate convergence asynchronous DP applied online A promising approach recently developed Peng Williams 58 Moore Atkeson 571 authors prioritized sweeping directs application DP backups likely predecessors states costs objective facilitate finding change significantly Exploration thiswhose optimal policy complete model decision problemmust distinguished exploration designed facilitate learning model decision problem available We discuss objective exploration Section 7 I4 If trials allowed time goal state reached possible eliminate requirement proper policy exists Timing prevents getting stuck fruitless cycles timeout period extended systematically ensure long let optimal paths followed intermption 106 AG Barto et cd Artijiciul Intelligence 72 1995 El138 63 RTDP LRTA Korfs 381 convergence Theorem 3 generalization theorem LRTA RTDP extends LRTA ways generalizes LRTA stochastic problems includes option backing costs states intervals form LRTA operates execution actions Using notation backs cost s follows action uI E Lr s setting f s minimum values c u off_ 1 j actions u E U s js current cost I5 The costs j ss successor action u f_t j states remain action inputs The controller observes sti repeats minimizing controller simplest determine time process It differs B st t 0 1 This form LRTA special case RTDP applied problem following way Whereas RTDP executes action ff LRTA executes action inconsequential j st st successor LRTA saves computation minimization gives greedy action However states cost time interval f select action deterministic special case usually j requiring backup general case RTDP backs makes sense use latest estimate time step minimization greedy respect greedy respect differ ftr difference LRTA ftj ft_t This perform required roughly RTDP It applies setting An extended costs ftt evaluation backedup forward search minimum instead s yf depth determined form LRTA related In discussion Korf evaluation state augmented lookahead search j sls successor states LRTA forward procedure fr_i s 38 assumes This means perform offline resources available time computational frontier nodes backs costs sts immediate search These backedup described forward search saved Despite computed new evaluation However costs multiple backedup DP costs backedup costs states generated costs states ft differs old s makes sense store backedup controller experience LRTA RTDP save defined stages asynchronous trials different starting states In contrast costs fk executing appropriately fact backedup function Korfs minimin update limits space constraints costs successors cost state generated costs successor states states possible especially successors This function ft_i Specifically saving responds executing depth forward search immediate predecessors frontier states backedup number stages asynchronous DP equal costs produced Korfs minimin procedure cor backs costs current costs tree The stage synchronously frontier states second stage backs costs states immediate Is Note A s t LKTA k equals t AG Barto et alArtifcial Intelligence 72 1995 81138 107 computation stage asynchronous DP predecessors states Then additional fk Not procedure cost St completes apply stochastic case suggests stages asynchronous DP useful These stages costs states forward search tree costs states tree For example noting forward search generate graph cycles multiple backups costs states improve All possibilities instances RTDP converge basically different general information contained fk conditions repeated described function learning trials With optimal evaluation RTDP However control adaptive control problems directly apply theory applies controlled theorems accumulating information improves algorithms developing estimate control performance Consequently LRTA suggested chosen Korf term complete accurate model In section discuss RTDP adaptive control problems problems lacking 7 Adaptive control The versions value GaussSeidel iteration described realtimerequire abovesynchronous prior knowledge underlying asyn Marko probabil require knowledge states j actions u E Ui chronous vian decision problem That require knowledge statetransition ities piju immediate means deterministic costs admissible policy knowledge incomplete information solution methods adaptive control methods I6 know actions state Finding approximating states immediate optimal known Markovian decision problem problems examples actions u E Ui available If states successor costs ciU There major classes adaptive methods Markovian decision problems class possible rest assumption distribution accumulate information Bayesian methods distribution incomplete systems priori probability As observations expected cost set selected DP policy possible contrast attempt approaches arrive optimal policy asymptotically prespecified class systems Actions optimal basis prior assumptions accumulated experience limit large literature classes minimizes systems time NonBayesian policy approach optimal policy revised Bayes accumulates Kumar stochastic dynamic known rule Actions 391 surveys observations l6 Markovian decision problems incomplete state information step control These called partially applications relevance problems incomplete controller complete knowledge state time decision problems despite information observuble Markovian scope article 108 AG Bartr et al Artificial Intelligence 72 1995 81138 methods conveys existing practical theoretical large problems subtlety issues sophistication nonBayesian methods results Here restrict attention current identiution dynamic true model typically control decisions controlled They use For indirect direct methods central model assumption Two types nonBayesian methods distinguished model update parameters values determine control They current model equivulence principle explicit models They directly estimate policy information model evaluation Zndirect methods explicitly algorithms time theorists certainty control 111 Direct methods hand form policies ling exploring This indirect methods conflict conducting model convergence methods require exploration trol algorithms universally available reviewed Kumar studied Barto Singh Sutton policy determined issue conflict control better appears achieve optimal policy Direct issues Adaptive optimal problems mechanism results 391 variety heuristic approaches 631 31 Kaelbling order discover control idenution behavior conjict control exploration favored Some approaches objective eventually 781 Thrun Miiller require mechanisms rigorous 551 Schmidhuber called 691 Watkins 341 Moore resolving 811 Thrun involve theoretical following function 791 converge In following subsections nonBayesian methods solving methods optimal policies theory proved information Although Markovian decision problems incomplete form basis algorithms exploration mechanisms rigor developing direction We method algorithm updates model time step control conventional DP algorithm methods computational serves reference approaches described point comparative simplest modification takes advantage RTDP We method Adaptive RTDP The method method Watkins purposes Next generic executed time step based current model Although 811 We briefly hybrid directindirect methods limits utility representative indirect method A direct QLearning engineering indirect method indirect method identification generic complexity literature severely 71 The generic indirect method Indirect adaptive methods Markovian decision problems incomplete statetransition probabilities immediate unknown mation estimate history state transitions terms parameter 9 contained states corresponding interact The usual approach define statetransition parameter j E S action u E Ui p j U 6 probability parameter 0 E 0 functional dependence 8 known probabilities space 0 Thus pair statetransition immediate costs observed infor costs based controller AG Barto et al Articial Intelligence 72 1995 81138 109 form Further usually assumes t9 E 0 true parameter SO pij U p j u 8 The identification task estimate 8 experience A common approach takes estimate 8 time step parameter having highest probability generating observed history maximumlikelihood estimate f9 The simplest form approach identification assume unknown parameter list actual transition probabilities Then time step t model consists maximumlikelihood estimates denoted p u unknown statetransition probabilities pairs states j actions u E Ui Let n t observed number times time step f action u executed state transition state j Then ny t cjEs nr number times action u executed state The maximumlikelihood state transition probabilities time I 10 If immediate costs ci u unknown determined simply memorizing observed l7 If infinite number time steps action taken infinitely state model converges true As mentioned nontrivial ensure occurs controlled At time step t generic indirect method uses non realtime DP algorithm determine optimal evaluation function latest model Let f denote optimal evaluation function Of course model correct f equal f generally case A certainty equivalence optimal policy time step t policy greedy respect f Let denote policy Then time step t st ll certainty equivalence optimal action Any offline DP algorithms described including asynchronous DP Here makes sense determine f time step initialize DP algorithm final estimate f produced DP algorithm completed previous time step The small change model time step t t 1 means f fF probably differ significantly As pointed computation required perform DP iteration prohibitive problems large numbers states n What action controller execute time t The certainty equivalence optimal action st appears best based observations time f Consequently pursuing objective control controller execute action However current model necessarily correct controller pursue identification objective dictates select actions certainty equivalence optimal actions It easy generate examples I7 In problems likelihood action immediate cost random function current state action maximum estimate immediate cost observed average immediate cost state 110 AG Barto et al Artijicial Intelligence 72 1995 81138 following true optimal policy lack exploration current certainty equivalence optimal policy prevents convergence example Kumar 391 One simplest ways induce exploratory behavior controller probabilities policies actions chosen according use randomized depend current evaluation executed equivalence highest probability To facilitate comparison algorithms described Section 41 adopt actionselection method based Boltzmann distribution Watkins function Each action nonzero probability 8 1 I Lin 47 Sutton current certainty simulations action having optimal 691 This method assigns execution probability admissible action current determined rating actions utility We compute state probability rating ru action u E Us follows ru Qfsu We transform probability mass function admissible time step t probability ratings actions controller executes action u E Us Boltzmann distribution negative sum 11 Probu eruT Cs ercr sharply probabilities T decreases T positive parameter controlling certainty equivalence optimal action p So As T increases uniform computational At zero certainty equivalence optimal policy infinite control probabilities temperature controls actions approach zero T acts kind 351 T decreases simulated annealing control necessary identification tradeoff policy equals exploration attempt peak u st approaches randomized temperature probabilities probability executing temperature time Here Section 41 introduced described described In simulations method exploratory behavior randomized policies let T crease time preselected minimum value learning progressed Our choice method dictated simplicity generic possible Without doubt sophisticated exploratory behavior effects behavior algorithms beneficial illustrate algorithms desire generating 72 Adaptive realtime dynamic programming The generic presented indirect method algorithm convergence resulting RTDP described online 10 given Eq non realtime DP relies executing substitute RTDP time step It straightforward indirect method Adaptive RTDP This method exactly updated method stages Section 61 1 model identification method 2 maximumlikelihood model performing current AG Barto et alArtificial Intelligence 72 1995 81138 111 instead true model 3 RTDP determined randomized policy given Eq 1 1 method balances control objectives action identification time step iteration number algorithms Adaptive RTDP related Although Suttons Dyna architecture based policy RTDP discusses Adaptive RTDP algorithm problems discounted similar performance 70 1 Lin In engineering Section 8 encompasses 4647 literature methods Adaptive related 32 focus Markovian decision measured average cost time step instead algorithms discusses methods closely Adaptive RTDP cost discussed focuses QLearning Jalali Ferguson investigated 69 identification influence Adaptive RTDP selection states advantageous accuracy Performing RTDP concurrently direct statetransition let progress identification 691 suggested applied Sutton provides opportunity backup operation costs states good confidence estimated estimates reliable time controller low improve aids identification 551 Schmidhuber possibilities tends model conflict control Kaelbling algorithm information use confidence measure statetransition possible 631 Sutton 691 Thrun according probabilities One devise measures confidence states cost backups use confidence measure At selection actions confidence direct visit regions state space regions This strategy produces exploration 341 Lin 47 Moore 791 discuss 781 Thrun Mbller 73 Qlearning QLearning method proposed Watkins Unlike information direct method 811 solving Markovian decision indirect adaptive methods discussed use explicit model dynamic problems incomplete underlying pairs states admissible Recall Eq 6 cost generating Any policy selecting actions optimal policy Thus determined relatively decision problem It directly estimates actions admissible Q u optimal Qvalue action u state greedy respect optimal Qvalues available little computation pairs optimal Qvalues stateaction state action u E Ui optimal policy optimal policy following optimal Qvalues family Qlearning methods Qlearning 8 1 actually proposed tR Watkins article methods based simple observed surprising pairs formed estimating values simplest case called onestep QLearning He observed studied idea suggested previously problems intensively earlier Although QJearning far knew He thirty years studied idea assigning values stateaction basis Denardos 24 approach DP seen algorithms like QLearning predate Watkins 1989 dissertation 112 AG Bartn et alArtificial Intelligence 72 1995 81138 We depart somewhat presentation 691 Barto Singh Sutton online control To emphasize QLearnings present unique problem We basic QLearning requiring direct access statetransition view taken Watkins 8 I 31 QLearning method adaptive relationship asynchronous DP decision probabilities usual online view QLearning algorithm oXne asynchronous DP method 731 OflLine QLearning Instead maintaining admissible explicit estimate optimal evaluation methods described QLearning maintains estimates optimal Qvalues let Qki U estimate Qi u available stage k computation Recalling state Eq 6 think f minimum optimal Qvalues Qvalues stage k implicitly defining fk stagek estimate f given state pair For state action u E Ui stateaction function 12 Although Qvalues define evaluation evaluation mation Qvalues knowledge statetransition function way contain infor function For example actions ranked basis function requires ranking actions evaluation probabilities immediate costs Instead function random statetransition having direct access probabilities OffLine Q Learning access samples according probabilities Thus state action u E Ui input function returns state j probability successor accurate model j successor form statetransition access probabilities role successor As shall function pij U Let function played online QLearning u The successor generate QLearning probabilities amounts function updates stateaction stateaction Qvalues synchronously pairs leaves unchanged At stage k OffLine QLearning admissible admissible pairs The subset admissible changes stage stage choice subsets determines algorithm For k 0 1 let 2 u 1 E S u f Ui set admissible stateaction new Qvalue value Let cik u 0 cJk u 1 denote learning Qvalue u stage k Then I Qvalues subset pairs Qvalues updated precise nature denote pairs Qvalues updated stage k For determines determined old value backed updating u E Sf computed follows define learning rate parameter rate parameter necessary stateaction pair QI u 1 aki u QdL u qiuCiu Yfksuccessoriul 13 fk given Eq 12 The Qvalues remain admissible stateaction pairs AG Barto et alArtijTcial lnlelligence 72 1995 81138 113 admissible u By QLearning backup mean application Eq 13 single admissible stateaction pair u If Qvalue admissible stateaction pair u backed infinitely infinite number stages learning rate parameters ok u decrease stages k appropriate way sequence Qk u generated OffLine QLearning converges probability Q U k 00 admissible pairs u This essentially proved Watkins 811 Watkins 821 Appendix B describes method meeting Dayan present revised proof required learning rate conditions developed Darken Moody 191 We method obtaining results RealTime QLearning example problems presented Section 41 One gain insight OffLine QLearning relating asynchronous DP The stagek Qvalues admissible stateaction pairs define evaluation function fk given Eq 12 Thus view stage OffLine QLearning defined Eq 13 updating fk fkt State fkli IlitQkliu This evaluation function update correspond stage usual DP selected actions algorithms based samples successor determined stateaction pairs A conventional DP backup contrast uses true expected successor costs admissible actions given state I9 It accurate think OffLine QLearning asynchronous version asynchronous DP Asynchronous DP asynchronous level states backup operation state requires minimizing expected costs admissi ble actions state The computation required determine ex pected cost admissible action depends number possible successor states action large total number states stochas tic problems OffLine QLearning hand asynchronous level admissible stateaction pairs Although QLearning backup requires minimizing admissible actions state order calculate Eq 12 require computation proportional fkSUCCeSSoriu number possible successor states Thus stochastic case asynchronous DP backup require Omn computational steps QLearning backup Used 13 lg However stage k OffLine QLearning effect stage asynchronous DP Sk special case I problem deterministic 2 A set admissible stateaction pairs states Sk 3 ok u 1 admissible stateaction pairs 14 M This complete minimization avoided follows Whenever Qk u backed new value Qkl u smaller fk fkt set smaller value If it9 new value larger fi fki Qi U fk U u u fk iS explicitly minimizing current Qvalues state admissible actions This case u sole greedy action respect fk Otherwise fkl fk This procedure computes minimization lq 12 explicitly updating Qvalues stateaction pairs u u sole greedy action Qvalue increases I14 AC Barto et 1 Artificial Intelligence 72 1995 81138 offset increased fact QLearning requires Om This advantage account QLearning takes information comparable backup asynchronous DP asynchronous DP backup required Q QLearning backups Nevertheless required asynchronous DP backup Learning backup QLearning advantageous stages computed quickly despite large number possible successor states realtime applications discuss space complexity computation backup 732 RealTime QLearning successor result turned online identical substitute algorithm executing indirect adaptive method If current model provides approximate large However use term RealTime QLearning Adaptive RTDP stages asynchronous OffLine QLearning currently control function tion 72 stages OffLine QLearning DP This advantages Adaptive RTDP number admissible tions discussed Watkins decision problem real acts successor tive algorithm backs Qvalue control action actually executed Using RealTime QLearning optimal policy problem function This direct adap pair time step single stateaction pair consists observed current state compute decision forming explicit model underlying model underlying case originally stateaction Sec 81 ac Specifically assume time step estimated optimal Qvalues produced preceding t controller observes estimates Qt u admissible selects action u E Lr s information receives state sI stages state sI Then Qtl computed allows exploration After executing ut controller cost cs uy state changes available RealTime QLearning We denote action pairs u The controller manner immediate follows Qtlsu 1 asruQt attcvt rfrstl 14 frsl minuEs Qslu time step t current stateaction stateaction pairs remain QtlLu Qtiu LYsu learning rate parameter admissible pair The Qvalues admissible u As far convergence Line QLearning step stage RealTime QLearning required convergence action performed sf u This process repeats time step f concerned RealTime QLearning set stateaction special case Off pairs Qvalues backed t So ul Th sequence Qvalues generated converges true values given Q conditions admissible state infinitely infinite number control OffLine QLearning This means AG Barto et alArtijicial Intelligence 72 1995 N138 115 steps It noteworthy pointed Dayan 221 admissible action state RealTime QLearning reduces TD0 algorithm investigated Sutton 681 To define complete adaptive control algorithm making use RealTime QLearning necessary specify action selected based current Qvalues Convergence optimal policy requires kind exploration required indirect methods facilitate identification discussed Therefore given method selecting action current evaluation function randomized method described Eq 11 method leads convergence indirect method leads convergence corresponding direct method based Real Time QLearning 733 Other Qlearning methods In RealTime QLearning define real underlying decision problem plays function However possible role successor function real model successor For stateaction pairs actually experienced control real provides function stateaction pairs model provides approxi successor mate successor function Sutton 691 studied approach algorithm called DynaQ performs basic QLearning backup actual state transitions hypothetical state transitions simulated model Performing QLearning backup hypothetical state transitions amounts running multiple stages OffLine QLearning intervals times controller executes actions A step RealTime QLearning performed based actual state tran sition This obviously possible ways combine direct indirect adaptive methods emphasized Suttons discussion general Dyna learning architecture 691 It possible modify basic QLearning method variety ways order enhance efficiency For example Lin 47 studied method RealTime QLearning augmented modelbased OffLine QLearning action clearly stand preferable according current Q values In case OffLine QLearning carried backup Qvalues admissible actions promising according latest Qvalues current state Watkins 811 describes family QLearning methods Qvalues backed based information gained sequences state transi tions One way implement kind extension use eligibility trace Qvalues stateaction pairs experi idea 437676872 enced past magnitudes backups decreasing zero creasing time past Suttons 68 TD A algorithms illustrate idea At tempting present combinations variations QLearning methods described scope present article Barto Singh 3 Dayan 120211 Lin 4647 Moore 57 Sutton 69 present comparative empirical studies adaptive algorithms based QLearning I16 AG Barto et dArtificial Intelligence 72 1995 81138 8 Methods based explicit policy representations action time step policy defined algorithms representation All DPbased learning cases use explicit Qvalues admissible adaptive function giving computing stored There number realtime policies evaluation step control Unlike closely related discussed Policy Section 5 iteration Bertsekas policy methods addressed functions iteration DP algorithm stateaction described pairs These functions evaluation nonadaptive function explicitly learning control methods based DP stored updated time methods value iteration algorithms article function evaluation improvement phase current evaluation uation phase 2 policy respect cuting value iteration algorithms discussed admissible policy evaluated Alternatively Although policy evaluation actions require More feasible policy evaluation phase phase Realtime asynchronous executed algorithms based policy require computation modified policy iteration Examples methods form modified policy appear 111 alternates phases current policy current policy updated function One way evaluate policy 1 policy eval determined greedy exe Section 5 assumption action state action specified explicit matrix inversion methods admissible large state sets iteration repeated minimizing practical 591 policy completion policy improvement iteration effectively work executing iteration concurrently control polebalancing 1671 LIynuPI method Sutton 4671 methods policy Iteration Barto Sutton Watkins iteration learning algorithms based policy theory learning Anderson PI means Policy discuss understood iteration However Williams theory addressing DP algorithms asychronous DP QLearning These algorithms iteration modified policy iteration special cases Integrating presented iteration scope article asynchronous Barto Sutton 691 connection 56 discuss In article theory value grain finer iteration policy theory include value algorithms based asynchronous Baird 1911 valuable contribution 9 Storing evaluation functions issue great practical importance implementing article evaluation functions represented results described assume lookuptable An scribed theoretical functions whichat principleis possible algorithms stored21 The evaluation number states representation I All comments apply storing Qvalues admissible stateaction pairs AG Barto et al Artificial Intelligence 72 1995 81138 I17 cf boxes actions admissible tional DP problems discretize representation Barto Sutton Anderson number state variables dimensionality QLearning havior TrialBased RTDP states reduce trials finite assumed continuous involving ranges continuous states andor article In applying conven actions usual practice state variables use lookuptable representation 4 This leads situation prompting Bellman The methods described circumvent curse dimensionality stochastic storage requirement exponential space complexity Michie Chambers 52 91 coin phrase curse article based asynchronous DP focusing start shortest path problems designated memory incrementally allocated lookuptable A number methods exist making representation efficient store costs possible states Hash table methods LRTA permit efficient storage retrieval costs provide necessary assumed Korf 38 small kdtree data structure access state costs explored Moore efficient storage retrieval costs finite set states kdimensional state space The resolved methods states need stored Similarly integrity stored costs subset possible article extend results described hash collisions preserve theoretical assuming 5556 Samuels configurations Other approaches storing evaluation 61 checkers player functions use function approximation methods weighted sum values set features The basic backup operation performed current cost state backedup cost This approach based parameterized models For example function approximated evaluation describing checkerboard weights state costs The weights adjusted discrepancy inspired variety recent studies parameterized The discrepancy functions learning connectionist Parametric approximations important use 771 generalize states visited approximations approximates form supervised supplies based training learning training data supply cost estimates examples shown error errorcorrection functions useful example Anderson factor large state sets 11 Tesauro natural way set function samples This provides evaluation procedure reduce networks function adapted approximating In fact supervised learning information derived senting hypotheses symbolic methods training algorithms For example Chapman tree methods Mahadevan 94 discusses learning algorithms approximation function evaluation examples These methods generalize learning method associated manner repre functions This includes backup operations DPbased 151 Tan 76 adapt decision Kaelbling Connell 49 use statistical clustering method Yee perspective use DPbased Despite large number studies principles DP combined generalizing methods presented article automatically extend approaches Although approximating theoretical results evaluation functions 118 AG Barto et alArtcial Intelligence 72 1995 81138 811 illustrated simple example Bradtke optimal evaluation function asynchronous DP algorithm 131 helpful approximating underlying convergence scheme adequately generalization detrimental pointed Watkins Even function approximation function trained samples function representation result iterative DP algorithm scheme stage The issues solving differential function absence training examples drawn true solution objective solution Here interested Equation easier problem approximating equations The objective problems solve approximately differential solution differential solving solution approximately equation represent optimal evaluation follow adequate uses approximation arise numerically approximate given boundary conditions equation In words approximate Bellman Optimality available challenge To best knowledge There extensive approximation methods DP 71 Bellman Kalaba Kotkin literature function multigrid methods methods splines orthogonal polynomials Dreyfus 411 However literature literature algorithms complete model decision problem Adapting produce approximation methods Bellman 18 I Kushner Dupuis learning RTDP DPbased offline algorithms future research 81 Daniel devoted techniques cases independent complete set linearly 68 Dayan problem representing results address represented theoretical 22 concern TD methods use generalizing methods DPbased address Sutton linear combination tunately compactly problem learning Qvalues results restricted Yee 66 point discounted evaluation performance control Without drastically undermine concerns combining DPbased research methods effectively algorithms described directly results learning algorithms The results evaluate given policy basis vectors Unfor function 131 addresses state regulation problems However Singh function basis errors true raise learning function approximation Much approximate plausible controller result provide better understanding lead worst small decrements function giving Qvalues control performancea small evaluation case small errors linear quadratic quadratic condition function approximating continuous article table Bradtke approximation evaluation lookup needed evaluation functions function 10 Illustrations DPbased learning We race track problem described conventional DP RTDP Adaptive RTDP RealTime QLearning tracks shown goal states 9115 states reachable shown Section 41 illustrate compare race Panel A 4 start states 87 start states policy We line The Fig 1 The small race track shown squares land crossing car finish AG Barto et alArtijcial Intelligence 72 1995 H138 119 Table 1 Example race track problems The results obtained executing Gauss Seidel DP GSDP Small Track Larger Track Number teachable states Number goal states Estimated number relevant states Optimum expected path length Number GSDP sweeps convergence Number GSDP backups convergence Number GSDP sweeps optimal policy Number GSDP backups optimal policy 9115 87 599 1467 28 252784 15 136725 22576 590 2618 2410 38 835468 24 541824 larger race track shown Panel B 6 start states 590 goal states 22576 states reachable start states We set p 01 controllers intended actions executed probability 09 We applied conventional GaussSeidel DP race track problem mean GaussSeidel value iteration defined Section 52 y 1 initial evaluation function assigning zero cost state GaussSeidel DP converges conditions special case asynchronous DP converges conditions given Section 53 satisfied Specifically clear proper policy track possible car reach finish line reachable state hit wall restart improper policy incurs infinite cost state immediate costs nongoat states positive We selected state ordering applying GaussSeidel DP concern influence convergence rate selected ordering GaussSeidel DP converged approximately half number sweeps synchronous DP Table 1 summarizes small larger race track problems computational effort required solve GaussSeidel DP GaussSeidel DP considered converged optimal evaluation function maximum cost change states successive sweeps 10m4 We estimated number relevant states race track number states reachable start states optimal policy counting states visited executing optimal actions lo7 trials We estimated earliest point DP computation optimal evaluation function approximation good corresponding greedy policy optimal policy Recall optimal policy greedy policy respect evaluation functions We running lo7 test trials sweep policy greedy respect evaluation function produced sweep For sweep recorded average path length produced test trials After convergence GaussSeidel DP compared averages optimal expected path length obtained DP algorithm noting sweep average path length lo optimal The resulting numbers sweeps backups listed Table 1 rows labeled Number GSDP sweeps optimal policy Number GSDP backups optimal policy 120 AG Barto et ul Artificial Intelligence 72 1995 81138 functions assessing requirements realtime initial evaluation computational offline value iteration algorithms requires considerable computations earlier note estimation process Although optimal policies emerged considerably optimal evaluation important conventional additional computation Nevertheless useful allow controllers optimal evaluation step restricted attention line zero velocity square starting We applied RTDP Adaptive RTDP RealTime QLearning costs positive know resulting numbers backups algorithms follow optimal policies comparable numbers backups race track fi problems Because immediate states Thus setting initial costs al1 states zero produces nonnegative function required Theorem 3 We applied nonoverestimating trialbased manner starting trial car placed realtime algorithms starting line selected equal probability A trial ended car reached goal state Thus according Theorem 3 y I RTDP converge function trials Although RTDP Adaptive RTDP costs repeated states control cost current state time step This case simulation mode B s t Obviously random number seeds We executed 25 runs algorithm different zero initialized run sequence trials beginning evaluation To monitor line moves data divided run sequence trial run To record trials By epoch sequence 20 consecutive disjoint epochs epoch path average path lengths generated epoch ing given algorithm Adaptive RTDP RealTime QLearning applied algorithms induced exploratory conditions behavior randomized policies based Boltzmann distribution described Section 71 To control control decreased tradeoff parameter T Eq 11 reached preselected minimum value T initialized run Parameter values additional simulation algorithm kept track path lengths finish starting algorithms applied simplest case details provided length mean Appendix B beginning performance car took incomplete identification information going function line line graph shows Fig 2 shows results RTDP Panel A Adaptive RTDP Panel B RealTime length Panel C The central QLearning lines averaged 25 runs corresponding I standard deviation average sample 25 runs Although initial epochs algorithm large average epoch path lengths note average epoch path lengths graphs epoch RTDP Adaptive RTDP RealTime QLearning respectively 455 866 13403 moves That initial average path lengths large especially RealTime QLearning reflects primitive nature exploration algorithm The upper lower epoch path useful strategy z Policy iteration algorithms address problem explicitly generating sequence improving policies corresponding evaluation function generally time updating policy requires computing consuming computation AG Baro et al Artificial Intelligence 72 1995 81138 Epoch number Epoch number learning realtime algorithms 25 runs corresponding Fig 2 Performance Adaptive RTDP Panel C RealTime QLearning The central averaged deviation epoch path length sample 25 runs Exploration controlled RealTime QLearning The right panel shows paths car follow noiseless conditions effective convergence track Panel A RTDP Panel B small line graph shows epoch path length standard Adaptive RTDP decreasin n T reached preselected minimum value start state algorithm The upper corresponding lines fl algorithm lower 122 AG Burro et 11 Artijiciul Intelligence 72 1995 RI138 2ei43 3eio3 Epoch number RealTime QLearning small track 5000 epochs The initial Fig 3 Performance graph shows data plotted Panel C Fig 2 different horizontal scale faster learned learning numbers information graphs moves given problem RTDP Panel A incomplete terms number epochs Adaptive RTDP RealTime QLearning rate Table 2 versions Panels RTDP Adaptive RTDP similar despite identification It clear variance measured discussed This surprising given differences problem complete B C That performances differences procedure algorithm converged low level stochasticity Time QLearning reach similar Time QLearning Adaptive RTDP disadvantage QLearning epochs problem takes epochs level performance This reflects information rapidly relevant states p 01 These graphs Real RTDP Adaptive RTDP Real RTDP simplicity results 5000 fact backup backups backup Fig 3 shows RealTime QLearning somewhat offset relative computational maximumlikelihood takes account fact information reflects randomness converged We inspected result algorithms A convenient way policies way policies produced algorithm judged graphs average epoch path lengths essentially RTDP RealTime QLearning car follow start state sources randomness problems function turned At right panel Fig 2 paths smallest epoch numbers levels 200 reached Panel B 2000 Adaptive RTDP caution Panel C Treated appropriate paths turned random exploration statetransition generated effectively epochs epochs effective convergence The path shown noiseless conditions Panels B C hand generated optimal policy despite fact shorter sense produced stochastic problem The paths path Panel A The control decisions comparing Panel A Fig 2 optimal Panel A 300 epochs times useful asymptotic optimal policy algorithms AG Barto et al Artificial Intelligence 72 1995 81138 123 Table 2 Summary Time DP ARTDP GaussSeidel DP GSDP learning performance small RealTime Qlearning track RTQ The computation RealTime DP RTDP Adaptive Real required included comparative purposes Average time effective convergence Estimated path length effective convergence Average number backups Average number backups epoch states backed states backed states backed 0 times 100 times 10 times GSDP RTDP ARTDP 28 sweeps 1456 252784 200 epochs 1483 127538 638 9845 805 1 318 300 epochs 1510 2 18554 728 9647 6541 174 RTQ 2000 epochs 1544 296 1790 1481 5334 668 I56 end track suboptimal policies produce higher probability car collide track boundary stochastic conditions Although illustrate uncertainty p optimal policies generate paths conservative safer distances increasing sense keeping track boundary lower velocities maintaining problem increases purposes information includes column policy produced track For comparative effective convergence Table 2 provides additional performance realtime algo table path length effective convergence RTDP executing 500 test trials learning rithms small GaussSeidel DP We estimated Adaptive RTDP RealTime QLearning algorithm We turned turned random exploration algorithms The row Table 2 labeled Estimated path length effective convergence gives average path length test trials 23 RTDP directly comparable GaussSeidel DP After 200 epochs 4000 average point trial 127538 backups required GaussSeidel function This number 15 sweeps GaussSeidel DP backups took average 1483 moves RTDP performed reaching DP converge function defines optimal policy optimal evaluation level performance 136725 backups resulting evaluation half number control performance comparable trials RTDP Table 1 improved Another way compare GaussSeidel DP RTDP examine backups cost state backed DP RTDP focused backups fewer states For run RTDP backed costs 100 times 8051 states run perform distributed states Whereas sweep GaussSeidel example 9845 states 10 times Although collect focused states optimal paths costs 290 states backed average 200 epochs average RTDP 200 epochs statistics s These path length estimates somewhat graphs Fig 2 convergence graphs costs start states given computed optimal evaluation path length average epoch path lengths shown effective turned For GaussSeidel DP averaged estimated smaller produced exploration lengths produced random exploration turned listed Table 2 path obtain function 124 AG Barto et al Artciul Intelligence 72 1995 81138 solving incomplete Not surprisingly level performance problem conditions achieve took 2000 epochs average 2961790 backups information took 300 epochs average 218554 requires backups Adaptive RTDP trials averaging 151 moves effective convergence Realtime Q backups achieve somewhat Learning backups skillful distributed states shows Adaptive RTDP considerably focused RealTime QLearning In 300 epochs Adaptive RTDP backed 9647 states 100 times 6541 states 10 times On 5334 hand 2000 epochs RealTime QLearning backed Qvalues 10 states times24 Again primitive exploration 100 times 668 states inadequacy strategy algorithm RealTime QLearning Fig 3 Examining results reflect converged effectively Fig 4 shows results terms number epochs respectively That Adaptive RTDP effectively RTDP Adaptive RTDP RealTime QLearning results larger track 7500 epochs We judged information These results ob larger race track Table 3 provides additional conditions described small track Fig 5 shows tained RealTime QLearning converged 500 400 RTDP Adaptive RTDP RealTime QLearning 3000 epochs partially fact epochs RTDP tended moves backups epochs RTDP We achieve slightly suboptimal performance RTDP required 62 computation initial epoch algorithm large graphs 7198 8749 180358 moves respectively RTDP Adaptive RTDP RealTime Q Learning Again reflect primitive nature exploration panel Fig 4 generated effective convergence Fig 4 optimal optimal hand generated slightly suboptimal policies strategy The paths shown right noiseless conditions policies produced Panel A noiseless conditions policy stochastic problem The paths Panels B C conventional GaussSeidel DP The average epoch path lengths large numbers moves especially algorithms The path shown RealTime QLearning produced corresponding sense faster Although simulations narrow learning continued Because costs states realtime algorithms definitive comparisons realtime algorithms features Whereas GaussSeidel DP conventional DP illustrate focused continued control objectives This focus subsets states relevant Trial increasingly Based RTDP applies simulations RTDP know algorithm eventually focused relevant states states making optimal paths RTDP achieved nearly optimal control performance 50 computation GaussSeidel DP small track 62 computation GaussSeidel DP larger progressively problems states race track track Adaptive RTDP RealTime QLearning fewer states run generic indirect method comparison inefficient focused convergence apply theorem strongly 24 We considered Qvalue state backed Qi u updated u E Ui AG Elarto et al Artificial hielligence 72 1995 81138 Epoch number Epoch number C Epoch number Sara Mm Finilh lbm Fig 4 Performance realtime learning algorithms huger track Panel A RTDP Panel B Adaptive RTDI Panel C RealTime QLearning The central line graph shows epoch path length averaged 25 runs corresponding algorithm The upper lower lines I standard deviation epoch path length sample 25 runs Exploration controlled Adaptive RTJJP RealTime QLearning decreasing T reached pmselected minimum value The right panel shows paths car follow noiseless conditions statt state effective convergence corresponding algorithm 126 AG Barto et ctlArtificinl Intelligence 72 1995 81138 250 0 0 le03 2e03 3e03 4e03 5e03 6e03 7eO Epoch number RealTime QLearning larger track 7500 epochs The initial Fig 5 Performance graph shows data plotted Panel C Fig 4 different horizontal scale Table 3 Summary learning performance RealTime DP ARTDP required GaussSeidel DP GSDP larger track RealTime QLearning RealTime DP RTDP Adaptive RTQ The computation included comparative purposes Average time effective convergence Estimated path length effective convergence Average number backups Average number backups epoch states backed states backed 8 states backed 100 times 6 10 times 0 times GSDP RTDP ARTDP RTQ 38 sweeps 500 epochs 400 epochs 3000 epochs 24 IO 835468 2462 5 17356 1035 9717 7046 817 2472 653714 1634 9003 5990 353 2504 IO330994 3444 5243 828 270 problems contrast small limiting It perform complete sweep In sharp computation algorithms factor simulations2 required realtime strategy decreased The results described Adaptive RTDP RealTime QLearning pro duced exploration selecting actions decreasing T reached preselected minimum value Although described conducted values decreasing T algorithms altered worse changes Although systematic attempt performance controlled introduced instead moves Performance experiments different minimum effects exploration algorithms exploration highly sensitive randomness investigate clear strategies trials 25 However action n divisions general conditions incomplete implementing Adaptive RTDP race track problems took advantage knowledge state This allowed avoid performing Fq 10 This possible implementation possible successors straightfonvard information required AG Barto et alArtijicial Intelligence 72 1995 81138 127 How indication studying problem details extrapolate algorithms simulations Although adequate function difficult larger problems continue function approximation methods continued research theoretical realtime DP algorithms What time DP algorithms offline DP algorithms encouraging use lookup 77 algorithms scale scale larger problems adequately collection problems issue The variability algorithms performance addressed results small larger race track sets performance size state action problems Proceeding algorithms functions Tesauros TDGammon conjunction learning hampered large space requirements tables storing evaluation data point DPbased problems larger described complexity real computational necessary clear simulations computational advantages conventional address confer significant In concluding discussion learning think application DPbased misleading productive way apply realistic DPbased racing specific track This skill transfer track robot navigation Mahadevan formulation applied 491 race track problem point problem tasks For example refines skill tracks specificity DPbased learning algorithms robot navigation race track problem learning represented More realistic applications requires abstract states actions work Lin 45 Connell 11 Discussion Conventional DP algorithms selectively possible successively approximate AI explores problems state spaces problems limited utility problems large state spaces states storing cost state Heuristic state space However DP combinatorial fully expanding require contrast search algorithms learning data structure improves function heuristic states initial state typically update heuristic evaluation estimating permanent forward state This information results repeated searches evaluation algorithm proceeds ultimately determine optimal policies relative ease Although A update estimate cost reach function cost reach goal state state They effectively way heuristic optimal evaluation search algorithms relevant optimal converging functions search cache learning learning algorithms principles DP relevant Although applied problem ence accumulates However actual simulated enced ongoing control process Doing conventional DP algorithms operate offline They designed learning occurs experi attempts problem solving control execute offline DP algorithm concurrently influ requirements solving control simulated satisfy certain DP algorithm control influence actual possible 128 AG Butto et cd Artijciul Intelligence 72 1995 81138 algorithm RTDP special case essentially 381 algorithm This general approach follows previous DP principles problem solving learning coincides research results Korfs LRTA 616970818788 Our contribution bring bear DPbased article theory asynchronous DP presented Bertsekas Tsitsiklis suitability asynchronous DP implementation multiprocessor theory novel use results Applying results stochastic shortest path problems DPbased RTDP retains algorithms provides algorithms ultimately yielding optimal behavior learning 121 Although systems motivated results especially RTDP provides new theoretical basis asynchronous DP imply GaussSeidel DP framework exhaustive nature offline DP extension Korfs LRTA convergence RTDP avoids algorithms Convergence competence conventional synchronous conditions theorems theorem learning playing learning fact simulated We DPbased 771 illustrations instead actual control DPbased term simulation mode refer execution RTDP related algorithms control 61621 Tesauros simulation mode illustrated Samuels checkers playing race track backgammon problem Despite simulation mode actually offline algorithms treat learning algorithms instead incrementally solely like RTDP require accurate model decision problem simulation mode option obvious advantages large number trials required Applying RTDP actual control makes sense time compute satisfactory policy offline method actual control begin application abstract computational methods For algorithms RTDP executed simulated experience improve control performance algorithms learning theorem information selecting stochastic improving produce policy shortest path problems responsive applied optimal pathseventually states backup operation control performance The convergence parts state set control Whether applied actual control simulation mode RTDP sig conventional DP algorithms Because RTDP likely specifies conditions nificant advantages demands control focus computation important TrialBased RTDP applied der RTDP focuses states statesto continuing costs states Our illustrations RTDP obtain near optimal policies putation approach functions mentioned 1000 years 1000 MIPS processor This true despite large fraction states backgammon normal play related abandoning optimal relevant states costs states possibly backing race track problem com fact optimal evaluation conventional DP feasibly applied We single sweep conventional DP fact required conventional DP However compelling illustrated RTDP form useful approximations problems significantly Monte Carlo algorithms achieve computational backgammon irrelevant example problems closely RTDP effi AG Bario et al Artificial Intelligence 72 1995 81138 129 ciency automatically allocating computation example unimportant terms sum correspond rare events computational process 171 For reason computational efficiency Monte Carlo methods exceed methods classes problems However Monte Carlo methods generally competitive deterministic methods small problems highprecision answers required More research needed fully elucidate correspondences exploit refining DPbased learning methods understanding computational complexity For problems large states sets backgammon lookup table method storing evaluation functions restricted attention practical Much research DPbased learning methods use storage schemes For problems DPbased learning algorithms focus increas ingly small subsets states illustrated simulations race track problem data structures hash tables Mtrees allow algorithms perform despite dramatically reduced space requirements One adapt supervised leam ing procedures use backup operation DPbased learning method provide training information If methods generalize adequately training data provide efficient means storing evaluation functions Although success achieved methods generalize connectionist networks theory presented article automatically extend cases Generalization disrupt convergence asynchronous DP Additional research needed understand effectively combine function approximation methods asynchronous DP In addition case accurate model decision problem available devoted considerable attention Markovian decision problems incomplete problems accurate model available Adopting information terminology engineering literature problems require adaptive control methods We described indirect direct approaches problems The method called generic indirect method representative majority algorithms described engineering literature applicable Markovian decision problems incomplete information A identification algorithm adjusts model line control controller selects actions based current estimate optimal evaluation function computed conventional DP algorithm assumption current model accurately models The DP algorithm executed model updated Although approach theoretically convenient costly apply large problems Adaptive RTDP results substituting RTDP conventional DP generic indirect method This means RTDP executed recent model generated identification algorithm Adaptive RTDP tailored available computational resources adjusting number DP stages executes time step control Due additional uncertainty case learning necessarily slower nonadaptive case measured number backups required However computation required select control action roughly This means practical apply Adaptive RTDP problems larger practical apply methods 130 AG Burto et trl Artd Intellipwce 72 1995 81138 generic model indirect method updated reexecute conventional DP algorithm In addition 811 QLearning function algorithm approximates asynchronous DP algorithm forming estimates statetransition asynchronous DP algorithm described probabilities QLearning generated model observed indirect adaptive methods discussed direct adaptive methods Direct decision problem We optimal eval operates Section 53 Whereas asynchronous DP backing cost state requiring basic oper methods form explicit models underlying described Watkins uation instead uses sample state transitions actual control QLearning finer grain basic operation computation ation QLearning backing Qvalue stateaction depend number possible successor states The fine grain basic QLearning addition selected states The cost flexibility stateaction information focus selected actions behavior controlled store Qvalues backup gather complete DP backup operation backup allows RealTime QLearning pairs fact QLearning increased space required number possible way responsive pair computation proportional successor states likely incomplete exploration Sophisticated complete strategies important information useful gained Knowledgeable sophisticated time required strategy improve control performance solving Markovian decision prob lems conditions information With complete information exploration decreasing reach goal states case RTDP focusing DP stages states evaluation function ordering backups accelerate vergence asynchronous DP applied online When incomplete reasons In case exploration unknown case complete kind ex ploration conducted online We discussed exploration performed reason conflicts performance objective control shortterm basis controller appear best based current evaluation structure controlled Unlike exploration information conducted sophisticated strategies address execute actions simulation mode improving necessity information information exploration gather useful function Although use sophisticated exploration strategies simulations sophisticated article strategies For example attempt exploration analyse strategies play essential learning methods practical clear easy devise consistent issues pertinent role making larger problems From mention race track problem exploration DPbased strategy exploration regions low quality state space backup operation 3 probabilities visit states having successors costs close optimal costs backup operation 1 visit states regions state space information uses accurate estimates statetransition 2 visit states high quality researchers argued exploration learn information regions Each suggestions efficiently propagates cost information set desiderata AG Barto et al Artificial Intelligence 72 1995 81138 131 current fact requires It encouraging practice For example flow sensations Although observable controller Although control policies distinguish proper context clear design strategy convergence article compatible wide range exploration article assumed unambiguously theory operation algorithms discussed satisfy best results strategies states controlled assump state robots world list robots current sensations On positive possible states approaches studied DP problem control objectives The frame decision problem existence single definitive grain In actuality control objectives dictate passage sensations flow controllers needed makes sense incorporates presented Throughout completely tion critical difficult vastly different effective closedloop sensations However exploiting complex subject research guises based learning methods Any widely applicable perspective constitutes article dynamic underlies work adopted misleading suggesting lineate events mark important models different recognized components recognize ability problem state identification systems state purposes controlindeed independent multiple objectivedependent If caution constitutes article wide application extending approach variety disciplines algorithms described levels abstraction applicability sophisticated achieve remains embedded critical systems itselfis factor Acknowledgment clarify relationships subject numerous discussions The authors thank Rich Yee Vijay Gullapalli Brian Pinette Jonathan Bachrach heuristic search control We thank Rich helping Sutton Chris Watkins Paul Werbos Ron Williams insights making aware Korfs research thoughtful manuscript We grateful pointing independently thank Harry Klopf learning problems This research supported grants National Science Foundation Office Scientific Research Bolling AFB AFOSR890526 fundamental thank Rich Sutton comments earlier version article Finally class encouraged Air Force insight persistence Dimitri Bertsekas AG Barto Steven Sullivan ECS9214866 ECS8912623 error sharing Appendix A Proof trialbased RTDP theorem Here prove Theorem 3 extends Korfs 381 convergence theorem LRTA TrialBased RTDP applied undiscounted stochastic shortest path problems 132 AG Barre et alArtijicial Intelligence 72 1995 81138 state backed time interval Proof Theorem 3 We prove theorem special case B st k t cost current t 0 1 Section 6 We observe proof change sy Let G denote goal set let B allowed arbitrary set containing s uf f respectively denote function time step t arbitrary functions generated TrialBased RTDP starting infinite sequence states actions evaluation state action evaluation arbitrary start state First observe 6 fi t fi Z sI ftj evaluation functions remain nonoverestimating states This true induction time ft ft fj j E S t fs min UEWi cu Cpsjuftj 1 iES uj CU Cpufj iES fh 1 equality restates Bellman Optimality Equation Eq 6 Let I C S set states appear infinitely arbitrary sequence c Ui actions state zero probability state set finite Let Ai set actions u E Ui set I nonempty causing transition admissible pj u 0 state I Ai finite j E S I Because states S I appear finite number times time To states visited I Then probability action chosen infinite number times state occurs TO Ai probability transition I occur probability exist time T TO t 71 sI E I u E A We know time step t RTDP backs cost s s E B We write backup operation follows ft1 St 6 CutI psutftj C pjutftj 1 jfSI IEl A1 But t Tl know s E I psi ur 0 j E S I ur E A s Thus t Tl rightmost Eq A 1 zero This means costs states S I influence operation RTDP T Thus TI RTDP performs asynchronous DP Markovian decision problem state set I summation If goal states contained I immediate decision problem asynchronous DP cause contradicts discounting costs states I grow bound But optimal cost fact cost state overestimate positive Because costs Markovian shown AG Barto et al Artcial Intelligence 72 1995 81138 133 I contains goal finite existence proper policy Thus state probability r After Tsitsiklis asynchronous DP applied TrialBased RTDP performs shortest path problem state set I satisfies undiscounted theorem Bertsekas RTDP converges optimal problem We know optimal evaluation optimal evaluation time rt 12 Proposition evaluation costs states function 33 p 3181 Consequently function stochastic asynchronous DP stochastic convergence conditions stochastic shortest path problems TrialBased shortest path identical states I I problem restricted function original problem S I influence costs states I contains current evaluation set states reachable Furthermore probability start state optimal policy Clearly I contains start states start state begins infinite number trails TrialBased RTDP executes greedy action respect ties way continues execute greedy actions Because know number function policies restricted optimal actions greedy respect Thus probability start state optimal policy time controller RTDP execute optimal actions finite TrialBased RTDP converges continues optimal evaluation select actions I time optimal evaluation states reachable I contains breaks function function Finally trivial revision argument holds RTDP backs costs states current state time step B arbitrary subset s cl Appendix B Simulation details Except discount simulations factor y set sets B set sI t RTDP involve parameters sweeps We selected GaussSeidel DP requires rate Both ordering concern trials Adaptive RTDP RealTime QLearning implemented Section 41 decreased Eq 11 To generate parameter T successive moves follows specifying influence convergence require exploration data described state ordering training T0 Tax Tk 1 TMinPTR TinT B1 B2 k number TMin 05 cumulative trials p 0992 TM 75 Realtime QLearning additionally requires sequences learning LY U Eq 14 8 1821 We defined satisfy hypotheses QLearning convergence sequences follows Let Ye u denote rate parameters theorem rate learning 134 AC Barto et 1 Artificial Intelligence 72 1995 81138 pair U backed time parameter step t Let n u number backups performed Qvalue U time step t The learning rate CY U defined follows Qvalue stateaction cuiu ffO7 7t niu initial learning searchrnconverge 191 They argue aa implements Moody stochastic optimization QLearning convergence theorem rate We set 0 05 r 300 This equation cut u suggested Darken achieve good performance schedules schedule tasks It shown schedule satisfies hypotheses References 1 I 1 CW Anderson Strategy Proceedings Fourth S093 GTE Laboratories published 103I 14 A Barto Reinforcement Handbook New York 1992 46949 A Barto S Singh On computational JL Elman TJ Sejnowski Sumnzer School Intelligent 12 13 learning multilayer Incorporated Waltham MA 1987 connectionist representations Tech Report TR87 corrected version report Irvine CA 1987 International Conference Machine Learning learning adaptive critic methods Control Neuml I Fu77t ond Adaptive Approaches DA White DA Sofge eds Van Nostrand Reinhold DS Touretzky GE Hinton eds Connectionist Models Proceedings 1990 economics reinforcement learning 14 I AG Barto RS Sutton CW Anderson Neuronlike Morgan Kaufmann San Mateo CA 199 I 3544 elements solve difficult learning J A Anderson I51 161 Foundations Research MIT Press Cambridge MA 1988 control problems fEEE Trans Sysr Man Cybern 13 1983 835846 E Rosenfeld Neurowmpufing AG Barto RS Sutton C Watkins Sequential decision problems neural networks Touretzky ed Advunce m Neural CA 1990 686693 AG Barto RS Sutton CJCH Watkins Learning sequential decision making J Moore eds Learning Computational Neuroscience Foundations Adaptive Networks Press Cambridge MA 1990 539602 R Bellman SE Dreyfus Functional DS Information Processing Sysfems 2 Morgan Kaufmann San Mateo dynamic programming Math Gbles M Gabriel MIT approximations reprinted rmd allocation processes Math Camp 17 1973 155161 Princeton University Press Princeton NJ 1957 new computational technique lEEE Trans Autom Control 27 1982 6 I O6 16 PrenticeHall Englewood Stochastic Models trnd Disrributed Computation Numerical Methods Prentice CL Gil SJ Hanson lrzformation Processing 5 Morgan Kaufmann San Mateo regulation I71 I81 I91 1101 IIll 1121 1131 I141 1151 1161 Deterministic Computation approximationa 13 1959 24725 Other Aides I R Bellman R Kalaba B Kotkin Polynomial dynamic programming RE Bellman Dynamic Programming DP Bertsekas Distributed dynamic programming DP Bettsekas Dynamic Prqrumminq Cliffs NJ 1987 DP Bertsekas JN Tsitsiklis Pamfiel Hall Englewood Cliffs NJ 1989 SJ Bradtke Reinforcement JD Cowan eds Advances CA 1993 295302 D Chapman Penquins cake Al Meg 10 1989 4550 D Chapman performance comparisons J Christensen RX Korf A unified learning theory heuristic evaluation Proceedings AAAI86 Philadelphia PA 1986 148152 LP Kaelbling input generalization linear quadratic learning applied Proceedings Neural IJCAI91 Sydney NSW delayed reinforcement learning algorithm 199 1 functions application AG Barto et al Artcial Intelligence 72 1995 81138 135 I 17 J JH Curtis A theoretical computing method Meyer ed Symposium Monre Carlo Methods Wiley New York 1954 191233 comparison component efficiencies classical methods Monte Carlo HA solution set linear algebraic equations I8 I JW Daniel Splines efficiency 191 C Darken J Moody Note learning dynamic programming rate schedule JE Moody DS Touretzky eds Advances Neural Kaufmann San Mateo CA 1991 832838 stochastic optimization J Math Annl Appl 54 1976 402407 RP Lippmann Information Processing Systems 3 Morgan 201 P Dayan Navigating JE Moody DS Touretzky eds Advances Neural Information Processing Systems 3 Morgan Kaufmann San Mateo CA 1991 464470 temporal difference RP Lippmann connectionism 21 1 F Dayan Reinforcing Edinburgh Scotland 22 P Dayan The convergence 123 1 TL Dean MP Wellman Planning nnd Control Morgan Kaufmann San Mateo CA 199 1 241 EV Denardo Contraction mappings TD A general A Mach kurn 8 1992 341362 statistical way PhD Thesis University Edinburgh theory underlying dynamic programming learning 1991 SIAM Rev 9 1967 165177 universally bad idea AI Mug 10 1989 4044 1251 M Gardner Mathematical 26 D Gelperin On optimality A Artif Intell 8 1977 6976 27 ML Ginsberg Universal planning 28 1 SE Hampson games Sci Amer 228 1973 108 Connectionisr Problem Solving Computational Aspects Biological Learning Birkhauser Boston MA 1989 129 1 PE Hart NJ Nilsson B Raphael A formal basis heuristic determination minimum cost IEEE Trans Syst Sci Cybern 4 1968 100107 paths 30 JH Holland rulebased Arfljicial Inteitigence Approach Volume II Morgan Kaufmann San Mateo CA 1986 593623 JG Carbonell TM Mitchell eds Muchine Learning An Escaping brittleness learning algorithms RS Michalski generalpurpose possibility systems applied 3 1 DH Jacobson DQ Mayne Differenrial Dynamic Programming Elsevier New York 1970 321 A Jalali M Ferguson Computationally Markov chains efficient adaptive control algorithms Proceedings 28rh Conference Decision Control Tampa FL 1989 1283 1288 33 1 M1 Jordan RA Jacobs Learning DS Touretzky ed Advances Neurul Information Processing Systems 2 Morgan Kaufmann San Mateo CA 1990 control unstable forward modeling 34 I LP Kaelbling Learning Embedded Systems MIT Press Cambridge MA 1991 revised version Teleos Research TR9004 1990 1351 S Kirkpatrick CD Gelatt MI Vecchi Optimization simulated annealing Sci 220 1983 671680 1361 AH Klopf Brain function adaptive systemsa heterostatic theory Tech Report AFCRL720164 Air Force Cambridge Research Laboratories Bedford MA 1972 summary appears Proceedings International Conference Systems Man Cybernetics 1974 37 I AH Klopf The Hedonistic Neuron A Theory Memory Learning Intelligence Hemishere Washington DC 1982 381 RE Korf Realtime heuristic search A 39 PR Kumar A survey results Inrell 42 1990 189211 stochastic adaptive control SIAM J Control Optimizufion 23 1985 329380 I40 V Kumar LN Kanal The CDP unifying formulation LN Kanal V Kumar eds Search Artificial Inrelligence heuristic search dynamic programming Springer branchandbound Verlag Berlin 1988 l37 41 I HJ Kushner P Dupuis Numerical Methods Stochastic Control Problems Continuous 7ime SpringerVerlag New York 1992 1421 WH Kwon AE Pearson A modified quadratic cost problem feedback stabilization linear fEEE Trans Aurom Control 22 1977 838842 431 Y le Cun A theoretical eds Proceedings 1988 2128 framework D Touretzky G Hinton T Sejnowski I988 Connectionisr Models Summer School Morgan Kaufman San Mateo CA backpropagation 136 Ati Btrrto PI trl Artjificirzl Intellzence 72 1995 RI138 I44 I M Lemmon Realtime optimal path planning distributed computing paradigm Proceedings American Control Conferenre Boston MA 1991 I45 I LJ Lin Programming robots reinforcement Anaheim CA 1991 781786 learning teaching Proceedings AAAI91 I46 I LJ Lin Selfimprovement based reinforcement learning planning teaching LA Bimbaum GC Collins eds Muclzing Learning Proceedings Eighih Internationul Workshop Morgan Kaufmann San Mateo CA 1991 323327 I47 I LJ Lin Selfimproving reactive agents case studies reinforcement learning frameworks From Arzimczls fo Animus Proceedings Firyt International Corzference Simulation Adaptive Behavior Cambridge MA 1991 297305 I48 I LJ Lin Selfimproving Leclrn 8 1992 29332 reactive agents based reinforcement I learning planning teaching Much I49 1 S Mahadevan J Connell Automatic programming behaviorbased robots reinforcement learning Artif Intell 55 1992 31 I365 I SO I DQ Mayne H Michalska Receding horizon control nonlinear systems EXE Trans Aufom Control 35 1990 8 14824 IS1 I L Mdro A heuristic search algorithm modifiable estimate Artif I S2 I D Michie RA Chambers BOXES experiment adaptive control Intell 23 1984 1327 E Dale D Michie eds Muchine lrztelligetzce 2 Oliver Boyd Edinburgh 1968 137 152 153 1 ML Minsky Theory neuralanalog reinforcement systems application brainmodel problem PhD Thesis Princeton University Princeton NJ 1954 I54 I ML Minsky Steps artificial intelligence Proceedings Institute Radio Engineers 49 196 I 830 reprinted E A Feigenbaum J Feldman eds Conzpufers Thought McGrawHill New York 1963 406490 IS5 I AW Moore Efficient memorybased learning robot control PhD Thesis University Cambridge Cambridge England 1990 IS6 I AW Moore Variable resolution dynamic programming efficiently learning action maps multivariate realvalued statespaces LA Bimbaum GC Collins eds Maczing Leurning Proceedings Eighth nrernurionul Workshop Morgan Kaufmann San Mateo CA 1991 333337 157 AW Moore CG Atkeson Memorybased reinforcement learning efficient computation prioritized sweeping SJ Hanson JD Cowan CL Giles eds Advances Neural Informafion Processing 5 Morgan Kaufmann San Mateo CA 1993 1581 J Peng RJ Williams Efficient learning planning dyna framework Adaptive Behavior 2 1993 437454 IS9 j ML Puterman MC Shin Modified problems Munage Sci 24 1978 policy I I27 I 137 iteration algorithms discounted Markov decision Introduction I 60 I S Ross 161 I AL Samuel Some studies reprinted 1959 210229 McGrawHill New York 1963 Stochastic Dynttmic Proqnzmming Academic Press New York 1983 machine learning game checkers IBM J Rex Develop EA Feigenbaum J Feldman eds Computers Thought I62 I AL Samuel Some studies machine learning game checkers IIRecent progress IBM 1967 6016 17 Adaptive confidence adaptive curiosity Tech Report FKI14991 lnstitut ftir Technische Universitat Miinchen 800 Miinchen 2 Germany 199 I I64 I MJ Schoppers Universal plans reactive robots unpredictable environments Proceedings IJCAI87 Milan Italy 1987 IO39 1046 1651 MJ Schoppers In defense reaction plans caches A fag 10 1989 5160 661 SP Singh RC Yee An upper bound loss approximate optimal value functions technical note Mach Lenm 16 1994 227233 167 I RS Sutton Temporal Massachusetts Amherst MA 1984 credit assignment reinforcement learning PhD Thesis University 68 I RS Sutton Learning 1691 RS Sutton Integrated predict method temporal differences Mach Lerzm 3 1988 944 architectures learning planning reacting based approximating dynamic programming Prwerdings Seventh fnterruzfional Conference Machine Learning Morgan Kaufmann San Mateo CA 1990 2 I h224 J Res Dewlo I63 I J Schmidhuber Informatik AG Barto et al Artificial Intelligence 72 1995 81138 137 170 I RS Sutton Planning incremental dynamic programming LA Bimbaum GC Collins eds Maching Learning Proceedings Eighth International Workshop Morgan Kaufmann San Mateo CA 1991 353357 I71 RS Sutton ed A Special Issue c Machine Learning Reinforcement Learning Mach Learn 8 1992 published Reinforcement Learning Kluwer Academic Press Boston MA 1992 721 RS Sutton AG Barto Toward modem theory adaptive networks expectation prediction Psychol Rev 88 1981 135170 1731 RS Sutton AG Barto A temporaldifference model classical conditioning Proceedings Ninth Annual Conference Cognifive Science Society Seattle WA 1987 741 RS Sutton AG Barto Timederivative models Pavlovian M Gabriel J Moore eds Learning Computational Neuroscience Foundations Adaptive Networks MIT Press Cambridge MA 1990 497537 reinforcement 1751 RS Sutton AG Batto RJ Williams Reinforcement learning direct adaptive optimal control Proceedings American Control Conference Boston MA 1991 21432146 761 M Tan Learning costsensitive LA Bimbaum eds Maching Learning Proceedings Eighth International Workshop Morgan reinforcement representation learning internal GC Collins Kaufmann San Mateo CA 1991 358362 temporal difference learning Mach Learn 8 1992 257277 177 1 GJ Tesauro Practical 78 1 S Thrun The role exploration issues DA White DA Sofge eds Handbook Intelligent Control Neural Fuzzy Adaptive Approaches Van Nostrand Reinhold New York 1992 527559 learning control 1791 SB Thrun K Moller Active exploration JE Moody SJ Hanson eds Advances Neural infortnafion Processing Sysfems 4 Morgan Kaufmann dynamic environments RP Lippmann San Mateo CA 1992 801 PE Utgoff JA Clouse Two kinds training information evaluation function learning Proceedings AAAI91 Anaheim CA I99 1 596600 81 1 CJCH Watkins Learning delayed rewards PhD Thesis Cambridge University Cambridge England 1989 1821 CJCH Watkins P Dayan Qlearning Mach Learn 8 1992 279292 dynamic programming 83 P Werbos Approximate DA White DA Sofge eds Handbook Intelligent Control Neural Fuzzy Adaptive Approaches Van Nostrand Reinhold New York 1992 493525 realtime control neural modeling 841 PJ Werbos Beyond regression new tools prediction analysis behavioral sciences PhD Thesis Harvard University Cambridge MA 1974 1851 PJ Werbos Advanced forecasting methods global crisis warning models intelligence General Systems Yearbook 22 1977 2538 1861 PJ Werbos Applications sensitivity analysis eds System Modeling Optimization SpringerVerlag Berlin 1982 nonlinear advances RF Drenick F Kosin 87 1 PJ Werbos Building understanding adaptive systems statisticalnumerical approach factory automation brain research IEEE Trans Syst Man Cybern 1987 1881 PJ Werbos Generalization propagation applications recurrent gas market model Neural Networks 1 1988 339356 891 D White M Jordan Optimal control foundation DA White DA Sofge eds Handbook Intelligent Control Neural Fuzzy Adaptive Approaches Van Nostrand Reinhold New York 1992 185214 intelligent control 1901 SD Whitehead Complexity LA Bimbaum GC Collins eds Maching Learning Proceedings Eighth International Workshop Morgan Kaufmann San Mateo CA 199 1 363367 cooperation Qlearning 911 RJ Williams LC Baird III A mathematical optimal controls Adaptive Learning Systems New Haven CT 1990 96101 dynamic programming incremental analysis actorcritic architectures learning Proceedings Sixth YaZe Workshop 1921 IH Witten An adaptive optimal controller discretetime Markov environments Infor Control 34 1977 286295 138 AG Barfo er al Artijicial lnfelligence 72 1995 RI138 I93 1 1H Witten Exploring modelling controlling discrete sequential environments IN J ManMach Stud 9 1977 715735 1941 RC Yee Abstraction control learning Tech Report 9216 Department Computer Science University Massachusetts Amherst MA 1992