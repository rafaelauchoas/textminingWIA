Artiﬁcial Intelligence 171 2007 107143 wwwelseviercomlocateartint Learning action models plan examples weighted MAXSAT Qiang Yang Kangheng Wu ab Yunfei Jiang b Department Computer Science Engineering Hong Kong University Science Technology Clearwater Bay Kowloon Hong Kong China b Software Institute Zhongshan University Sun YatSen University Guangzhou China Received 21 October 2005 received revised form 14 November 2006 accepted 27 November 2006 Available online 25 January 2007 Abstract AI planning requires deﬁnition action models formal action plan description language stan dard Planning Domain Deﬁnition Language PDDL input However building action models scratch difﬁcult timeconsuming task experts In paper develop algorithm called ARMS actionrelation modelling automatically discovering action models set successful observed plans Unlike previous work actionmodel learning assume complete knowledge states middle observed plans In fact approach works partial intermediate states given These example plans obtained observation agent know logical encoding actions state information actions In real world application cost prohibitively high labelling training examples manually annotating state plan example snapshots environment To learn action models ARMS gathers knowledge statistical distribution frequent sets actions example plans It builds weighted propositional satisﬁability weighted MAXSAT problem solves MAXSAT solver We lay theoretical foundations learning problem evaluate effectiveness ARMS empirically 2006 Elsevier BV All rights reserved Keywords Learning action models Automated planning Statistical relational learning 1 Introduction AI planning systems require deﬁnition action models initial state goal In past action modelling languages developed Some examples STRIPS 13 ADL 12 PDDL 1417 With languages domain expert sits writes complete set domain action representation These representations planning systems input generate plans However building action models scratch task exceedingly difﬁcult timeconsuming domain experts Because difﬁculty approaches 451834374243 explored learn action Corresponding author Email address qyangcseusthk Q Yang URL httpwwwcseusthkqyang 00043702 matter 2006 Elsevier BV All rights reserved doi101016jartint200611005 108 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 models examples In knowledge acquisition planning state art systems acquiring action models based procedure interacts human expert illustrated Blythe et al 5 McCluskey et al 29 A common feature works require states action known Statistical logical inferences learn actions preconditions effects In paper step automatically acquiring action models observed plans practice The resultant algorithm called ARMS stands ActionRelation Modelling System We assume observed plan consists sequence action names objects action uses The intermediate states actions partially known adjacent pair actions truth literal totally unknown This means input form action names associated parameter list state information practical previous systems learn action models Suppose observed plans input From incomplete knowledge ARMS automatically guesses approximately correct concise action models explain observed plans This action model guaranteed completely correct serve provide important initial guidance human knowledge editors Consider example input output algorithm Depot problem domain AI Planning com petition 1417 As input given relations clear xsurface denote x clear x type surface relation xlocatable yplace denote locatable object x located place y We given set plan examples consisting action names parameter list drivextruck yplace zplace liftxhoist ycrate zsurface pplace We pair consisting action associated parameter list action signature example action signature drivextruck yplace zplace Our objective learn action model action signature relations preconditions postconditions fully speciﬁed A complete description example shown Table 1 lists actions learned Table 2 displays training examples From examples Table 2 wish learn preconditions add delete lists actions Once action given lists complete action model Our goal learn action model action problem domain order explain training examples successfully An example output learning algorithms loadx y z p action signature action loadxhoist ycrate ztruck pplace pre del add x p z p lifting x y lifting x y y p y z available x clear y Table 1 Input domain description Depot planning domain Domain types relations actions Depot place locatable object depot distributor place truck hoist surface locatable pallet crate surface xlocatable yplace xcrate ysurface xcrate ytruck lifting xhoist ycrate available xhoist clear xsurface drivextruck yplace zplace liftxhoist ycrate zsurface pplace dropxhoist ycrate zsurface pplace loadxhoist ycrate ztruck pplace unloadxhoist ycrate ztruck pplace Q Yang et al Artiﬁcial Intelligence 171 2007 107143 109 Table 2 Three plan traces training examples Initial Step1 State Step2 Step3 State Step4 State Step5 Step6 Step7 Step8 Step9 Goal Plan1 I1 lifth1 c0 p1 ds0 drivet0 dp0 ds0 loadh1 c0 t0 ds0 drivet0 ds0 dp0 available h1 unloadh0 c0 t0 dp0 lifting h0 c0 drop h0 c0 p0 dp0 c0 p0 Plan2 I2 lifth1 c1 c0 ds0 lifting h1 c1 loadh1 c1 t0 ds0 lifth1 c0 p1 ds0 loadh1 c0 t0 ds0 drivet0 ds0 dp0 unloadh0 c1 t0 dp0 droph0 c1 p0 dp0 unloadh0 c0 t0 dp0 droph0 c0 c1 dp0 c1 p0 c0 c1 Plan3 I3 lifth2 c1 c0 ds0 loadh2 c1 t1 ds0 lifth2 c0 p2 ds0 drivet1 ds0 dp1 unloadh1 c1 t1 dp1 loadh2 c0 t0 ds0 droph1 c1 p1 dp1 drivet0 ds0 dp0 unloadh0 c0 t0 dp0 droph0 c0 p0 dp0 c0 p0 c1 p1 I1 p0 dp0 clear p0 available h0 h0 dp0 t0 dp0 p1 ds0 clear c0 c0 p1 available h1 h1 ds0 I2 p0 dp0 clear p0 available h0 h0 dp0 t0 ds0 p1 ds0 clear c1 c1 c0 c0 p1 available h1 h1 ds0 I3 p0 dp0 clear p0 available h0 h0 dp0 p1 dp1 clear p1 available h1 h1 dp1 p2 ds0 clear c1 c1 c0 c0 p2 available h2 h2 ds0 t0 ds0 t1 ds0 We wish emphasize important feature problem deﬁnition relaxation observable state information requirements The learning problem easier knew states action However reality observed action partially known In paper address situation know little states surrounding actions Thus know sure exactly true loadh1 c0 t0 ds0 drivet0 ds0 dp0 unloadh0 c0 t0 dp0 droph0 c0 p0 dp0 complete state goal state ﬁrst plan Table 2 Part difﬁculty learning action models uncertainty assigning state relations actions In plan relation c0 p0 goal conditions established ﬁrst action second rest It uncertainty causes difﬁculties previous approaches depend knowing states precisely In methodology training plans obtained monitoring devices sensors cameras sequence recorded commands UNIX These action models revised interactive systems GIPO It intriguing ask approximate action model application domain given set recorded action signatures partial intermediate state information In paper ﬁrst step answering question presenting algorithm known ARMS ARMS proceeds phases In phase algorithm ARMS ﬁnds frequent action sets plans share common set parameters In addition ARMS ﬁnds frequent relationaction pairs help initial state goal state These relationaction pairs initial guess preconditions add lists delete lists actions subset These action subsets pairs obtain set constraints hold order plans correct We transform constraints extracted plans weighted MAXSAT representation 253049 solve produce action models solution SAT problem The process iterates actions modelled While action models learn paper deterministic nature future extend framework learning probabilistic action models handle uncertainty For complex planning domain represented hundreds relations actions corresponding weighted MAXSAT representation likely large solved efﬁciently number clauses reach tens thousands In response design heuristic method modelling actions approximately We 110 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 measure correctness model deﬁnition error rates redundancy rates For testing model cor rectness design crossvalidation method evaluating learned action models different experimental conditions different observations number plans Our previous work 46 reported feasibility learning action models action sequences planning domains In paper ARMS add additional constraints allow partial observations actions prove formal properties evaluate ARMS STRIPS planning domains recent AI Planning Competition The rest paper organized follows The section discusses related work Section 3 deﬁnes problem learning action models plan examples Section 4 presents ARMS algorithm Section 5 presents experimental results Section 6 concludes discussion future work 2 Related work 21 Learning state images The problem learning action descriptions important AI Planning As result researchers studied problem past In 43 34 18 partially known action model improved knowledge intermediate states pairs actions plan These intermediate states provide knowledge preconditions postconditions missing action deﬁnition In response revision action model conducted complete action models In 43 STRIPS model constructed computing speciﬁc condition consistent observed examples In 42 decision tree constructed examples order learn preconditions However works require incomplete action models input learning intermediate states observed Compared works work learn action model state information incomplete state information available 22 Inductive logic programming In 37 inductive logic programming ILP approach adopted learn action models Similarly 4 presented learn preimage precondition action TOP operator ILP The examples require positive negative examples propositions held states actions application This enables concept preimage action learned state action ILP learn positive negative examples states target actions given However problem sequence bare action names provided example initial goal conditions known To best knowledge work far apply ILP case Even use logical clauses enumerate different possibilities precondition number possibilities going huge Our SATbased solution provides elegant control strategy resolving ambiguities clauses 23 Knowledge acquisition planning Another related thrust adopts approach knowledge acquisition action models acquired teracting human expert 5 Our work seen addon component mixedinitiative knowledge editing systems provide advice human users GIPO 29 The DISTILL 44 method learn programlike plan templates example plans shares similar motivation work The aim automatically acquire plan templates example plans templates planning directly Compared work focused learning action models planning delegated planner new problems In 44 focus extract plan templates example plans substitute planner The plan templates represent structural relations actions Also related work work Garland Lesh 16 introduced idea evaluating plans partial success action models incomplete In 27 programming demonstration PBD problem solved versionspace learning algorithm goal acquire normal behavior terms repetitive tasks When user start repetitive task going sequence states use learned action sequence map initial goal states directly This objective relates goal observed user traces Q Yang et al Artiﬁcial Intelligence 171 2007 107143 111 learn sequences actions However difference PBD 27 aim learn action sequences action models Plan traces exploited learn situationaction rules 3 decisiontree learning algorithm applied learn ifthen rules represent control strategy automated agent From sequence sensor readings control actions taken human users represent complex manmachine operation ﬂying Boeing 747 jetliner situationaction rules learned map sensor readings actions taken The condition rules considered form preconditions learned guide application actions rules Actionmodel learning considered complex forms planning CaMeL 2123 candidateelimination based learning algorithm acquires methodpreconditions hierarchical task network HTN planning domain given set taskdecomposition schemata 1045 These schemata known methods 3233 The input CaMel consists collections plan traces derivational trees derive plan traces Their aim learn preconditions control decomposition method applicable In order overcome problem lack training data CeMel stronger form input considered compared inference process plan trace taken input It interesting future work consider incorporate additional form problemsolving knowledge learning process extend learning problem HTN schemata We discuss issues future work paper 24 Satisﬁability problems A propositional logic formula F transformed conjunctive normal form conjunction m clauses clause disjunction set literals A literal variable negation variable Given formula F deﬁne satisﬁability problem SAT Given formula F conjunctive normal form truth assignment logical variables makes F true The satisﬁability problem known NPComplete 15 There classes solutions SAT problem The ﬁrst class consists exact algorithms including logical approaches 8 integer programming approaches 24 The second class heuristics based GSAT heuristic 38 These algorithms search satisfying truth assignment SAT problem Recently extensions SAT problems studied researchers Related problem learning action models weighted MAXSAT problem stated Given collection C m clauses C1 Cm involving n logical variables clause weights wi ﬁnd truth assignment maximizes total weight satisﬁed clauses C The weighted MAXSAT problems shown NPhard Goemans Williamson 19 shown feasible approximate MAXSAT factor 0758 optimal solution polynomial time The weighted MAXSAT problems getting increasing attention theory practice As result efforts implement MAXSAT solvers In 2639 efﬁcient algorithm solving weighted MAXSAT problems presented software tool available 11 Richardson Domingos 3536 presented uniﬁed framework statistical relational learning Markov logic network detailed discussion Section 25 It suggested MaxWalkSat applied ﬁnding approximately satisfying assignments truth values ground predicates result logical inference In 7 Borchers Furman twophase algorithm solving MAXSAT weighted MAXSAT problems In ﬁrst phase use GSAT heuristic ﬁnd good solution solving problem This solution serves seed generating heuristics second phase In second phase use enumeration procedure based DavisPutnamLoveland algorithm 828 ﬁnd provably optimal solution The ﬁrst heuristic stage improves performance algorithm obtaining upper bound minimum number unsatisﬁed clauses pruning branches search tree use upper bound guide searches 112 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 When compared integer programming approach problem algorithm better integer programming methods classes problems better classes In paper method weighted MAXSAT solution The solution available implemented software httpinfohostnmteduborchersmaxsathtml In paper concern mainly problem convert actionmodel learning problem weighted MAXSAT problem Once conversion weighted MAXSAT problem solved proven algorithms Also discussion follows use term constraints clauses inter changeably 25 Markov logic networks Another related area Markov logic networks MLN integrates statistical learning Markov networks symbolic logic powerful uniﬁed learning inferencing framework 93536 In Markov network set variables correspond network nodes X Xi 1 n joint distribution deﬁned loglinear model Over network ﬁrst order logic framework represented including logic constructs constants predicates truth value assignments In addition knowledge base given includes set ﬁrst order logic formulae Two tasks deﬁned learning The ﬁrst task learn structure Markov network solved inductive logic programming approach ILP The second task learn parameters MLN important component weights associated features learned methods conjugate gradient iterative scaling Once learned MLN accomplish number inferencing tasks Once learned MLN perform logical inference Inference form asking query knowledge base computation ﬁnd truthvalue assignments satisfy knowledge base formulae One method accomplishing task exploit weighted MAXSAT solver MAXWalkSat maximizes sum weights satisﬁed clauses conversely minimizes sum weights unsatisﬁed clauses The MLN model learn actionmodellearning problem described subsequently There direct mapping components action models structure parameters MLN We return correspondence Section 52 26 Relation SLAF algorithms Amir et al 24041 presented tractable exact solution version actionmodel learning problem technique known Simultaneous Learning Filtering SLAF In version learning problem aim learn actions effects partially observable STRIPS domain The input consists ﬁnite set propositional ﬂuents set world states ﬁnite set actions transition relation maps states states actions The training examples consist sequences actions partial observations states given The objective build complete explanation observations models actions Conjunctive Normal Form CNF formula By tying possible states ﬂuents effect propositions action models complexity CNF encoding controlled permit exact solutions efﬁciently circumstances This work closely related similar objectives learning action models However problems solved SLAF techniques different In Amir et als work aim learn action models state observations state observations technique work In addition SLAF methods ﬁnd exact solutions In contrast case aim learn approximate models action sequences possibly unknown intermediate states training 27 PDDL background PDDL Planning Domain Deﬁnition Language designed formally specify actions plans uniﬁed expressive language inspired wellknown STRIPS formulation planning problems 1417 There main components PDDL The actions represented set schemata action consists action typed parameter list precondition list effect list The effects partitioned add list Q Yang et al Artiﬁcial Intelligence 171 2007 107143 113 Table 3 A domain description PDDL deﬁne requirements types predicates action parameters precondition effect domain vehicle strips typing vehicle location fuellevel vvehicle plocation fuel vvehicle ffuellevel accessible vvehicle p1location p2location f1fuellevel f2fuellevel drive vvehicle fromlocation tolocation fbeforefuellevel fafterfuellevel v accessible v fuel v fbefore fbefore fafter v v fuel v fbefore fuel v fafter delete list following STRIPS formalism When applied state semantics action speciﬁes condition action applied outcome state result The pre postconditions actions expressed logical propositions logical connectives postconditions split add delete lists An action instantiated ﬁrst applied state For example action schema specify loadx y speciﬁes action loading object x vehicle y When applied x replaced object Book1 y replaced Truck2 resulting action loadBook1 Truck2 In particular action applied state s actions preconditions members state s The state obtained deleting delete list relations adding addlist relations s One important extension PDDL STRIPS language ability specify typed parameters For example x vehicle Table 3 speciﬁes variable x bound object vehicle Table 3 illustrates PDDL model domain vehicle locations 14 relations preceded operator considered deletelist items There ﬁve levels language descriptions PDDL 21 allowing expressivity consumable resource requirements In paper focus STRIPS level PDDL 21 14 An example problem description vehicle domain shown Table 4 includes initial state goal state 3 Problem statement In paper learn PDDL level 1 STRIPS representation actions A planning task P speciﬁed terms set objects O initial state I goal formula G set operator schemata O I G O based collection predicate symbols States sets logical relations instantiated actual parameters Every action precondition list list formulas hold state action action applicable An action add list delete list sets atoms A plan partial order actions successively applied initial state consistent order yields state satisﬁes goal formula The input actionlearning algorithm ARMS set plan examples plan example consists 1 list propositions initial state 2 list propositions hold goal state 3 sequence action signatures consisting action names actual parameters instantiated parameters Each plan successful achieves goal initial state However action models actions incomplete preconditions add delete lists actions completely speciﬁed In input instances ARMS plan examples provide action names drivet0 ds0 dp0 t0 ds0 dp0 objects plan examples actions preconditions effects These learned algorithm In plan initial goal states replaced special actions models need learned The ﬁrst action plan denoted αinit replaces initial state This action preconditions add list initialstate relations Its delete list Similarly action plan αgoal replaces goal state The preconditions αgoal goal relations effects 114 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Table 4 A problem instance associated vehicle domain deﬁne domain objects init goal problem vehicleexample vehicle truck car vehicle half fuellevel Paris Berlin Rome Madrid location truck Rome car Paris fuel truck half fuel car half half accessible car Paris Berlin accessible car Berlin Rome accessible car Rome Madrid accessible truck Rome Paris accessible truck Rome Berlin accessible truck Berlin Paris truck Paris car Rome Our objective learn complete set action deﬁnitions precondition add delete lists assigned action schema This collection called action model In order ensure generality learned action models avoid overﬁtting models training examples action models required 100 correct Thus deﬁne notion error rate evaluated separate set plans known test set For action test set replace learned action model We deﬁne following metrics evaluate action model We formally deﬁne correctness plan following STRIPS model We consider plan sequence actions state set atoms Deﬁnition 1 A plan said correct respect action model according action model 1 actions preconditions hold state action 2 goal propositions hold action If precondition action satisﬁed preceding state action plan example error occurs use Ea Ea 1 count error P Deﬁnition 2 In plan P count error said occurred actions precondition p true state action P Let EP total count errors plan P Then error rate P cid2 EP cid2 aP aP Ea preconda For set plans ΣP deﬁne error rate action model respect plans ΣP similarly total count errors EΣP ΣP total number preconditions actions ΣP We conversely deﬁne correctness action model respect plan set plans Deﬁnition 3 An action model correct respect plan according action model error count zero plan Similarly action model said correct respect set plans error rate zero set plans Another aspect action model wish preserve usefulness actions training examples This important easily assign goals actions plan leaving preconditions Q Yang et al Artiﬁcial Intelligence 171 2007 107143 115 postconditions actions model clearly ideal resulting plans correct However actions plan rendered useless In learning framework ensure learned action model nontrivial ﬁrst require actions add list nonempty Furthermore apply basic assumption actions add list member example plan useful later actions plan Conversely deﬁne notion redundancy follows Deﬁnition 4 An addlist element r action plan P nonredundant P exists action b ordered plan r precondition b action b adds r Otherwise r said redundant The intuition actions addlist relation r redundant plan P relation useful achieving precondition later actions plan The number redundant add list elements example plans provides measure small action model In extreme case relations add list actions example plans trivially correct However case level redundancy action model maximum Thus tricky task balance number relations action model number errors incurred action model set training plans We measure redundancy rate actions set plan examples follows For action let Ra number add list relations achieve precondition later action plan Then set plans ΣP redundancy rate cid2 1 RΣP cid2 aΣP Ra addlista aΣP Finally deﬁne ideal target learning Deﬁnition 5 An action model concise correct lowest redundancy rate correct action models respect set example plans ΣP Intuitively consider action model good explains plan examples possible testing plan examples simplest correct models Note concise model necessarily smallest model correct action models possible reduce number addlist relations simultaneously removes precondition relations Such global op timization difﬁcult achieve computationally Thus settle notion concise model work Note problem statement assumed learning problem actions observed states If actions observable states observable necessary know pair adjacent observed states action occurred In case state values continuous ﬁrst problem timeseries segmentation continuous signal sequence time series sensor values The segmentation problem determines beginning end action Subsequently apply learning algorithm learn actions preconditions effects An initial attempt solving problem 4748 If states discretevalued problem predicting minimal set actions associated models number actions sizes precondition effect sets models error rate minimized according minimal description length principle 20 4 The ARMS algorithm 41 Overview Given set correct example plans ΣP wish uncover constraints use conﬁne space learned models To illustrate ideas consider simple example Let single plan P initial state contains relations p1 p2 goal relation g Let P contain actions a1 a2 a1 ordered a2 There additional relation p3 present initial state 116 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 To learn action models a1 a2 consider following possibilities 1 p1 p2 belong precondition list a1 a2 adds goal g a1s add delete lists a2s precondition list The problem action model a1 redundant action order a1 a2 remains unexplained 2 An alternative model assign p1 precondition a1 p2 precondition a2 a1s add list contains p3 precondition a2 Finally a2 adds goal g In model a1 a2s delete lists In second model actions nonredundant In addition explained plan correct Therefore preferred model In general given set plans ΣP large number potential action models To ﬁnd models low error redundancy rates uncover number constraints plans The ﬁrst type straints constrain preconditions add delete lists single actions These constraints universally true Therefore precondition action appear add list action delete list subset precondition list This constraint known action constraint The second type constraint restricts reason action a1 occurs action a2 plan P This a1 achieves precon dition p a2 a1 deletes relation r added a2 a1 a2 require common precondition r This type constraint applies actions plan called plan constraints Finally observe relation r true actions a1 a2 r higher priority assigning add list a1 precondition list a2 We wish limit size preconditions add delete lists actions predeﬁned number k We types preference constraints information constraints Because example plan set SigmaP large number constraints uncovered prefer use statistical information generate relatively small set constraints basis learning In particular apply frequent set mining ﬁnd frequent subsets actions apply plan information constraints We algorithms section 42 Algorithm constraints Our ARMS algorithm iterative iteration selected subset actions training plan examples learned An overview algorithm follows The ARMS Algorithm Input Variables Constants Relations Domain A set correct plan examples Output A set action models Θ Step 1 We ﬁrst convert action instances schema forms replacing constants corresponding variable types Let Λ set incomplete action schemata Initialize action models Θ set action schemata Step 2 For unexplained actions Λ build set information action constraints based individual actions Ω Apply frequentset mining algorithm 1 ﬁnd frequent sets connected actions relations subsection Here connected means actions relations share common parameters Let frequent set actionrelation pairs Σ Step 3 Build weighted maximum satisﬁability representation Γ based Ω Σ based constraints frequency information uncovered Step 2 Step 4 Solve Γ weighted MAXSAT solver In experiments httpwwwnmteduborchers maxsathtml MaxWalkSat solver 2639 Section 634 provides details application MaxWalkSat Step 5 Select set A learned action models Λ highest weights Update Λ Λ A Update Θ adding A If Λ Step 2 Step 7 Output action models Θ Q Yang et al Artiﬁcial Intelligence 171 2007 107143 117 The algorithm starts initializing plans replacing actual parameters actions variables types This ensures learn action models schemata individual instantiated actions Subsequently algorithm iteratively builds weighted MAXSAT representation solves In iteration actions explained removed incomplete action set Λ The learned action models middle program help reduce number clauses SAT problem ARMS terminates action schemata example plans learned Below explain major steps algorithm 43 Step 1 Initialize plans variables A plan example consists sequence action instances We convert plans substituting occurrences instantiated object action instance variables type If object multiple types generate clause represent possible type object For example object o types Block Table clause o Block o Table We extract example plans sets actions connected actions a1 a2 said connected parametertype list nonempty intersection The parameter mapping x1 x2 called connector 44 Step 2 Build action plan constraints As introduced Section 2 weighted MAXSAT problem consists set clauses representing conjunc tion clause associated weight value representing priority satisfying constraint Given weighted MAXSAT problem weighted MAXSAT solver ﬁnds solution maximizing sum weight values associated satisﬁed clauses In ARMS kinds constraints satisfy representing types clauses They action information plan relation constraints 441 Action constraints Action constraints imposed individual actions These constraints derived general axioms correct action representations A relation r said relevant action parameter type Let prei addi deli represent ai s precondition list addlist delete list The general action constraints translated following clauses building SAT 1 Constraint A1 The intersection precondition add lists actions prei addi φ 2 Constraint A2 In addition actions delete list includes relation relation actions precondition list Thus action require delete list subset precondition list deli prei To illustrate action constraints consider following example Example 1 Consider action signature loadxhoist ycrate ztruck pplace Depot domain Interna tional Planning Competition IPC Table 2 From domain description possible relations action signature loadx y z p x p available x lifting x y y p y z clear y z p The precondition list pregoal action consists ycrate ssurface Suppose primary effect action loadx y z p y z For action action constraints given follows The preconditions include possible relations joined disjunction The possible relations add delete list lifting x y y p y z clear y z p Constraint A1 encoded conjunction following clauses lifting x y addi lifting x y prei 118 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 y p addi y p prei y z addi y z prei clear y addi clear y prei z p addi z p prei lifting x y prei y p prei y z prei clear y prei z p prei y p addi y z addi clear y addi z p addi Constraint A2 encoded follows lifting x y addi lifting x y deli lifting x y prei y p deli y p prei y z deli y z prei clear y deli clear y prei z p deli z p prei 442 Information constraints The information constraints explain optionally observed intermediate states exist plan The constraints derived given high priority need guessed Suppose observe relation p true actions an1 p ai1 aik share parameter types We represent fact following clauses given ai1 aik appear order Constraint I1 The relation p generated action aik 0 cid2 ik cid2 n p selected addik means logical Constraint I2 The action aik delete relation p p selected delete addlist aik p addi1 addi2 list aik p delik Example 2 Consider intermediate state lifting h0 c0 Plan 1 Table 2 This generated preceding actions lifth1 c0 p1 ds0 loadh1 c0 t0 ds0 unloadh0 c0 t0 dp0 However deleted unloadh0 c0 t0 dp0 The SAT clauses follows I1 liftingxy addlift addload addunload I2 liftingxy delunload Finally consider soft type information constraint deﬁne pairs relations actions relation r frequently occurs action This case initial states plan contain relations occurs ﬁrst action plan In case predict facts preconditions action This constraint applies parts plan partially observable states middle plan Constraint I3 We deﬁne weight value relationaction pair p occurrence probability pair plan examples If probability relationaction pair higher probability threshold θ set corresponding relation constraint p PRECONDa receives weight value equal prior probability Example 3 Consider example Table 2 A relationaction pair clear c0 lifth1 c0 p1 ds0 sharing parameter c0 Plan1 relationaction pair clear c1 lifth1 c1 c0 ds0 sharing parameter c1 Plan2 generalized clear y prelift labelled y Thus occurrence count clear y liftx y z p parameter label y Later Section 443 deﬁne count support count frequentsequence analysis Table 5 shows examples I3 type constraints Q Yang et al Artiﬁcial Intelligence 171 2007 107143 119 Table 5 Examples soft information constraints Parameters y x p x z p y p y z x y y z Information constraints clear y prelift x p prelift available x prelift z p prelift y p prelift y z prelift x y predrive y z adddrop 443 Plan constraints The plan constraints represent relationship actions plan ensure plan correct actions add list relations redundant First action plan generate constraint requirement plan correct The ﬁrst plan constraints follows Constraint P1 Every precondition p action b add list preceding action deleted actions b Constraint P2 In addition relation r add list action useful achieving precondition later action That action add list relation r precondition later action b action b adds deletes r While constraints P1 P2 provide general guiding principle ensuring plans correctness practice instantiations constraints This given actions precondition preceding action serve establisher relation To ensure efﬁciency design heuristics consider reasons explain actions frequently coexist These heuristics allow restrict small subset frequent action pairs When explaining action adds relation precondition actions higher priority actions Our heuristic based notion associationrule mining data mining area 1 The following constraints P3P6 applied set action pairs cooccur frequently The rationale constraints pair actions occur frequently plan examples reason frequent coexistence Thus possible reasons cooccurrence enumerated uncover reasons Our ﬁrst task ﬁnd frequent action pairs occur plans We apply Apriori algorithm 1 ﬁnd frequent action pairs cid8ai aj cid9 0 cid2 j cid2 n 1 indexes actions To capture frequent pair occurs plan examples training set plan examples following deﬁnitions Deﬁnition 6 Let S action pair cid8ai aj cid9 0 cid2 j cid2 n 1 The support count pair S set plans ΣP number times pair occurs plans ΣP The support count integer value We equivalently talk support rate percentage occurring pairs Deﬁnition 7 Let S action pair cid8ai aj cid9 0 cid2 j cid2 n 1 n number actions plan The support rate pair S set plans ΣP percentage pair occurs ΣP support count cid8ai aj cid9 ZΣP cid4 cid3 cid8ai aj cid9 support rate ZΣP total number action pairs ΣP Frequent pairs relative term Here follow data mining literature deﬁne probability threshold θ action pairs support rate equal θ considered frequent We apply subsequent 120 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 constraints explain frequent action pairs Of course θ deﬁned differently domain free parameter wish tune later experimental Note unlike general association rules suffer problem generating redundant association rules data mining research apply Apriori algorithm ﬁnd frequent pairs These pairs explained learning later Let action pair cid8ai aj cid9 0 cid2 j cid2 n 1 Plan constraint P3 One relevant relations p chosen preconditions ai aj delete list ai p p prei prej p deli Constraint P4 The ﬁrst action ai adds relevant relation precondition list second action aj pair p p addi prej Constraint P5 A relevant relation p deleted ﬁrst action ai added aj The second clause designed event action reestablishes fact deleted previous action p p deli addj Let prei addi deli represent ai s precondition list addlist dellist respectively The plan straints combined constraint Φai aj ARMS Φai aj restated prej p deli p addi prej p deli addj p p prei As example consider Depot domain Table 2 Example 4 Suppose liftx1hoist y1crate z1surface p1place loadx2hoist y2crate z2truck p2place frequent pair We explain connection actions lift load selecting candidate relation The relevant parameters share type x1 x2 y1 y2 p1 p2 The relations refer parameters x p available x lifting x y y p clear y From action pair SAT clauses constructed follows use x represent parameter type hoist y represent crate p represent place At relation x p available x lifting x y y p clear y selected explain connection liftx y z p loadx y z p If f x f x x p available x lifting x y y p clear y selected explain connection liftx y z p loadx y z p following true f x precondition list action loadx y z p add list liftx y z p b f x delete list action liftx y z p add list loadx y z p c f x precondition list action liftx y z p del list action liftx y z p precondition list loadx y z p 45 Step 3 Build solve weighted MAXSAT problem In solving weighted MAXSAT problem Step 3 clause associated weight value zero The higher weight higher priority satisfying clause In assigning weights types constraints weighted MAXSAT problem apply following heuristics Action constraints Every action constraint receives constant weight WAa action The weight action constraints set higher weight information constraints Information constraints Every partial information constraint receives constant weight WI r relation r The constant assignment empirically determined generally highest constraints weights Q Yang et al Artiﬁcial Intelligence 171 2007 107143 121 Table 6 Examples highly supported information constraints Label y x p x z p y p y z x y y z Information constraints clear y prelift x p prelift available x prelift z p prelift y p prelift y z prelift x y predrive y z adddrop Support count Support rate 3 3 3 3 3 3 1 3 100 100 100 100 100 100 33 100 For information constraints I3 deﬁne weight value relationaction pair p occur rence probability pair plan examples If probability relationaction pair higher probability threshold θ set corresponding relation constraint p PRECONDa receives weight value equal prior probability If corresponding relation constraint receives constant weight lower value determined empirically This assignment corresponds new type heuristic constraints especially useful partial information constraints Example 5 Consider example Table 2 A relationaction pair clear c0 lifth1 c0 p1 ds0 sharing parameter c0 Plan1 relationaction pair clear c1 lifth1 c1 c0 ds0 sharing parameter c1 Plan2 generalized clear y prelift labelled y Thus support count clear y liftx y z p parameter label y Table 6 shows information constraints support rate values previous example Plan constraints The weight value plan constraint WP a1 a2 higher probability threshold minimal supportrate value θ This value set equal prior probability action pair a1 a2 In experiments vary value threshold θ verify effectiveness algorithm When considering subsequences actions example plans consider sequences supports θ Example 6 Consider example Table 2 An action sequence cid8lifth1 c0 p1 ds0 loadh1 c0 t0 ds0cid9 shares parameters h1 c0 ds0 Plan1 action sequence cid8lifth1 c1 c0 ds0 loadh1 c1 t0 ds0cid9 shares parameters h1 c1 ds0 Plan2 generalized liftx1 y1 z1 p1 loadx2 y2 z2 p2 labelled x1 x2 y1 y2 p1 p2 The connector x1 x2 y1 y2 p1 p2 indicates parameters x1 y1 p1 action liftx1 y1 z1 p1 parameters x2 y2 p2 action loadx2 Table 7 Examples action constraints Label x1 x2 y1 y2 p1 p2 z1 x2 p1 y2 x1 z2 z1 p2 y1 y2 p1 p2 x1 x2 p1 p2 x1 x2 p1 p2 x1 z2 z1 p2 y1 p2 p1 p2 y3 z1 x2 z3 z1 p2 p3 p1 p2 p3 y4 z1 p2 p3 p4 Plan constraints Support count Support rate Φlift load Φload drive Φdrive unload Φunload drop Φload lift Φdrop unload Φdrive load Φdrive load Φlift load drive Φload drive unload Φdrive unload drop Φload lift load drive Φdrive unload drop unload 5 4 4 5 2 1 1 1 4 4 4 2 1 100 80 80 100 40 20 20 20 80 80 80 40 20 122 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 y2 z2 p2 respectively Thus support liftx1 y1 z1 p1 loadx2 y2 z2 p2 parameter label x1 x2 y1 y2 p1 p2 Table 7 shows plan constraints support count rate values 46 Step 5 Update initial states plans ARMS starts set Λ incomplete action models As action schemata fully learned maximum size actions preconditions adddelete lists reach upper bound value U action considered learned All learned actions removed set Λ step If learned action appears beginning plan ARMS updates initial state executing actions current initial state produces new initial state 5 Properties ARMS algorithm 51 Formal properties evaluation metrics Given set plan examples ﬁnd large number action models Some models superior An advantage ARMS action plan constraints satisﬁed set plans plan correct Theorem 1 For given set training example plans ΣP action constraints A1 A2 plan constraint P1 satisﬁed actions plans P correct Proof This theorem follows deﬁnition plan correctness states plan correct precon ditions actions true state action goal included Consider single plan P plan set ΣP For action b plan constraint P1 satisﬁed bs preconditions r PRECONDb add list action b actions b delete r The action constraints A1 A2 deﬁned Section 441 ensure r deleted These conditions ensure r true b Thus plan correct cid2 The following theorem establishes ideal case relation add list action model redundant example plans Theorem 2 For given set training example plans ΣP plan constraint P2 satisﬁed actions learned action models zero redundancy rate respect ΣP concise Proof We wish example plan add list relation establish precondition later action plan This ensured constraint P2 states add list member action useful adding relation r member precondition later action b action b adds deletes r Thus redundancy rate zero action model training plans ΣP cid2 It important guarantee learned action model correct low redundancy rate In case consider learned model approximately concise Deﬁnition 5 To achieve objective ARMS uses greedy algorithm explaining action pairs sufﬁciently frequent concept frequency deﬁned probability lower bound value θ As result possible preconditions actions learned model explained preceding actions appear frequently We use error rates EP estimate degree correctness plan redundancy rate RP measure degree redundancy training plans We metrics function training plan parameters section Finally number clauses generated ARMS polynomial number relations domain plan sizes number training plans number actions Theorem 3 The number clauses generated ARMS set example plans polynomial number relations action schemata size example plans training Q Yang et al Artiﬁcial Intelligence 171 2007 107143 123 Proof Because process generating constraints follows ﬁnite sequence steps complexity ARMS mainly depends number clauses variables SAT problem Consider planning domain A actions plan N plans Let U upper bound number relations preconditions postconditions action Then number action straints OA U N The number plan constraints OA2 U N The number information constraints OU A N Thus total number clauses bounded OA2 N A U cid2 Note localsearch nature ARMS algorithm guarantee learned action model smallest theoretically However weighted MAXSAT algorithm generates solution SAT problem parsimoniously action model generated generally small learned model approximately simple In section empirically verify redundancy rates learned models 52 Relation Markov logic networks In related works section brieﬂy mentioned exists strong relation actionmodel learning Markov logic networks MLN MLN combines Markov networks ﬁrst order logic uniﬁed learning inference framework 93536 In Markov network set variables corresponds network nodes X Xi 1 n joint probability distribution deﬁned loglinear model Over network ﬁrst order logic framework represented including logic constructs consist constants predicates truth value assignments In addition knowledge base given includes set ﬁrst order logic formulae In formal deﬁnition MLN set binary valued nodes possible grounding predicate appearing MLN 93536 There set features corresponding formula Fi network value Fi binary Associated formula weight value The higher weight stronger features A strong feature MLN formalism ability perform learning tasks performed 36 The ﬁrst task learn structure Markov network solved inductive logic programming approach ILP The second task learn weight parameters MLN learned methods conjugate gradient iterative scaling Once learned MLN accomplish number interesting inference tasks For example collective classiﬁcation MLN applied predict classes related objects linked Web pages considering objects The ARMS algorithm considered special case MLN algorithm In particular invent predicates follows We invent new predicate InPrecond literal P action A InPrecond takes P A arguments form InPrecondP A denote fact literal P assigned precondition action A We invent new predicate InAdd literal E action A InAdd takes E A arguments form InAddE A denote fact literal E assigned effects action A Similarly deﬁne InDeleteE A delete list items Then convert constraints mentioned Section 42 new predicates For example action constraint A1 Section 441 represented knowledgebase KB formula P Literals A ActionsInPrecondP A InAddP A states intersection preconditions add list actions We represent actionmodel learning problem MLN learning problem follows As stated invent new predicates InPrecond InAdd InDelete apply types objects relations actions Relations constructed domain speciﬁc predicates variables constants xy Relations predicates include logical axioms encode speciﬁc constraints problem domain For example blocks world axiom states predicate clear x True preclude exists object y yx True These axioms constraints form relations nodes MLN The weights hard constraints example set highest The action information 124 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 plan constraints receive weights according procedure Section 44 Then use algorithm solving corresponding weighted MAXSAT problem obtaining approximate solution sum weights maximized limits imposed computational resources In experimental section implementations ARMS systems MaxSatSolver MaxWalkSat respectively We report performance weighted MAXSAT solvers similar terms approximate solutions CPU time spent 6 Experimental results In section assess effectiveness learned action model empirically An action model incom plete errorprone respect set example plans A previous evaluation method 16 applied planning incomplete action model assessed models degrees correctness In work available set example plans split training testing sets We adapt crossvalidation evaluation method making use available example plans We split given plan examples separate sets training set testing set The training set build model testing set deﬁned assess ing model We test plan test data set turn evaluate test plans correct nonredundant modelled learned action model 61 Experimental setup planning domains In order evaluate ARMS generated example plans existing planning tool planning domains International Planning Competition 20021 The example plans generated MIPS planner2 In domain ﬁrst generated 200 plan examples We applied ﬁvefold crossvalidation dividing examples ﬁve parts equal size We selected ﬁve parts consist 160 plan examples training set folds use remaining fold 40 separate plan examples test set This repeated ﬁve times different fold test data We plot mean values metrics associated error bars corresponding 95 conﬁdence intervals obtained ﬁvefold crossvalidation experiments We ran experiments personal 768 MB memory Pentium Mobile Processor 17 GHz CPU Linux operating Also experiments set upper bound U actions according maximum value actual domain description This parameter experimentally varying U small value large value checking resultant error redundancy rates action models test plans The best value determined way empirically The domains Depots Driverlog Zenotravel Satellite Rover Freecell planning domains Features domains summarized Table 8 In table summarize number action schemata domain actions number different predicates domain relations maximum number relations precondition list postcondition list domain Max PreEff maximum number relations Table 8 Features problem domains Domain features Domain names Depots Driverlog Zenotravel Satellite Rover Freecell actions relations Max PreEff Max Initial Max Goals Plans Avg Length 5 6 4 24 3 200 10 6 5 3 48 8 200 26 5 5 4 19 8 200 24 5 8 6 28 5 200 16 5 25 6 63 6 200 23 10 11 10 149 5 200 27 1 httpplanningcisstrathacukcompetition 2 httpwwwinformatikunifreiburgdemmips Q Yang et al Artiﬁcial Intelligence 171 2007 107143 125 Table 9 The learned action model Depot domain θ 10 ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL drivextruck yplace zplace x y x z x y liftxhoist ycrate zsurface pplace x pavailable xat y pon y z clear yat z p lifting x yclear z y pclear yavailable xon y z dropxhoist ycrate zsurface pplace x pat z pclear zlifting x y available xclear yon y z lifting x yclear z loadxhoist ycrate ztruck pplace x pat z plifting x y y zavailable xat y p clear y lifting x y unloadxhoist ycrate ztruck pplace x p z p available x y z clear y lifting x y y zavailable xclear y Table 10 The learned action model Driverlog domain θ 10 ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL ACTION PRE ADD DEL loadtruck objobj trucktruck loclocation truck loc obj loc obj truck obj loc unloadtruckobjobj trucktruck loclocation truck loc obj truck obj loc obj truck boardtruckdriverdriver trucktruck loclocation truck locat driver locempty truck driving driver truck truckat driver loc disembarktruckdriverdriver trucktruck loclocation truck locdriving driver truck driver locempty truck driving driver truck drivetrucktrucktruck locfromlocation loctolocation driverdriver truck locfromdriving driver truck path locfrom locto truck loctoempty truck truck locfrom walkdriverdriver locfromlocation loctolocation driver locfrom path locfrom locto driver locto driver locfrom initial state Max Initial goal state Max Goals training examples number example plans training testing ﬁnally average number actions training plans Avg Length This summary table serves provide sense complexity learning task domain As Freecell domain demanding learning task 10 action schemata 11 relations long plan lengths The Depot domain 126 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Table 11 Varying probability threshold planning domains Depots Probability threshold 0 1 2 3 4 5 6 7 8 9 10 30 50 70 Depots E 016 019 012 003 024 006 027 027 026 004 019 019 019 019 019 019 019 Table 12 Varying probability threshold planning domains Driverlog Probability threshold Driverlog 0 1 2 3 4 5 6 7 8 9 10 30 50 70 E 009 008 006 005 005 005 005 005 005 005 005 005 005 005 R 018 020 017 024 004 025 025 022 006 011 011 011 011 011 011 011 R 014 001 012 001 014 006 005 004 004 004 004 004 004 004 004 004 004 Table 13 Varying probability threshold planning domains Zenotravel Probability threshold Zenotravel 0 1 2 3 4 5 6 7 8 9 10 30 50 70 E 0 0 0 0 0 0 0 0 0 0 0 0 0 0 R 013 013 013 013 013 013 013 013 010 004 009 004 009 004 009 004 009 004 009 004 CPU sec 96 05 9 07 78 04 7 7 68 04 7 72 11 72 04 72 04 74 05 74 05 74 05 74 05 CPU sec 534 55 514 30 428 25 414 23 348 146 27 165 206 163 212 172 214 170 214 170 214 174 214 174 214 174 214 174 CPU sec 7 07 68 04 72 04 72 08 72 04 76 05 68 04 72 04 78 04 7 07 7 7 7 7 Clauses 6607 4 4292 3 2967 725 705 884 310 3 310 3 284 57 180 3 180 3 180 3 180 3 180 3 180 3 180 3 Clauses 349 3 312 4 138 23 70 19 61 61 61 59 4 41 17 41 17 41 17 41 17 41 17 41 17 Clauses 103 2 69 2 62 6 52 6 40 40 40 40 40 40 40 40 40 40 simplest contrast For domains generated 200 example plans subsequently split training testing sets In experiments following independent variables varied Q Yang et al Artiﬁcial Intelligence 171 2007 107143 127 Table 14 Varying probability threshold planning domains Satellite Probability threshold 0 1 2 3 4 5 6 7 8 9 10 50 70 Satellite E 021 008 023 004 026 001 026 001 025 003 021 003 020 020 020 023 007 026 009 026 009 026 009 Table 15 Varying probability threshold planning domains Rover Probability threshold 60 61 62 63 64 65 66 67 68 69 70 75 80 Rover E 068 066 004 067 002 067 002 066 001 066 001 066 001 067 067 001 067 001 067 001 067 001 067 001 Table 16 Varying probability threshold planning domains Freecell Probability threshold 60 61 62 63 64 65 66 67 68 69 70 75 80 Freecell E 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 R 007 005 005 002 003 003 004 002 007 002 007 007 007 007 007 001 007 001 007 001 R 007 002 006 001 006 002 007 002 008 008 008 009 001 008 008 008 008 008 R 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 047 001 CPU sec 6 58 04 58 04 56 05 54 05 54 05 54 05 58 04 52 04 48 16 4 19 4 19 4 19 CPU sec 224 17 217 11 210 21 192 15 181 1 176 12 167 11 166 9 172 8 169 2 168 4 168 4 168 4 CPU sec 388 6 387 6 390 5 390 5 390 6 390 5 390 5 390 6 390 5 390 5 389 5 389 5 389 5 Clauses 688 8 140 8 134 134 137 7 92 23 82 82 82 85 6 87 7 87 7 87 7 Clauses 62700 6774 51078 13355 51078 13355 43368 13355 35657 29734 10259 23811 10259 26762 15371 23811 10259 17888 17887 1 17887 1 17887 1 Clauses 91 1 91 1 91 1 91 1 91 1 91 1 91 1 91 1 91 1 91 1 91 1 91 1 91 1 128 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 1 Varying probability threshold planning domains Error Rate Fig 2 Varying probability threshold planning domains Redundancy Rate The probability threshold θ This control frequent action pairs enforcing plan constraints When θ increases zero 100 expect fewer clauses weighted MAXSAT formula tion The degree partial information This relational information observe ac tions training plans partial information represented percentage original states This variable deﬁned test ARMS algorithm beneﬁt partial information occasion ally observed middle plans Let p value ranges zero 100 We use p control partial information observed action terms total p N relations N total number relations plans actions summing preconditions add delete lists When p ranges zero 100 available state information increases The plan length This average number actions example plans These variables varied small large test performance ARMS Q Yang et al Artiﬁcial Intelligence 171 2007 107143 129 Fig 3 Varying probability threshold planning domains CPU Time Fig 4 Varying probability threshold planning domains Clause Number In experiments wish conﬁrm following hypotheses Hypothesis 1 ARMS learn accurate action models low error rates redundancy rates domains different experimental conditions Hypothesis 2 ARMS accomplish learning task reasonable computational time The ﬁrst hypothesis veriﬁed checking average error rates E accumulated learned models models applied testing examples associated redundancy rates R The second hypothesis veriﬁed average CPU time number clauses incurred 160 training plan examples reserve 40 testing We use denote range error bar error rate redundancy rate shown subsequent tables 130 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 5 Varying partial state information Error Rate Fig 6 Varying partial state information Redundancy Rate In subsections use weighted MAXSAT solver tool 6 implement ARMS In Section 634 replace MaxSatSolver MaxWalkSat report comparison results 62 Learning depot domain To readers intuitive feeling resultant model learned ARMS ﬁrst output gen erated ARMS Depot Driverlog domains In learned models possible visually compare learned action models groundtruth action models PDDL domain descriptions IPC 2002 In example action model use generate example plans called ideal action model Mi From plan examples learn model Ml In table relation appears models shown normal font If relation appears Mi Ml shown italic font Otherwise Q Yang et al Artiﬁcial Intelligence 171 2007 107143 131 Fig 7 Varying partial state information CPU Time Fig 8 Varying partial state information Clause Number relation appear Ml Mi shown bold font As seen Tables 9 10 parts learned action models correct respect groundtruth domain descriptions 63 Varying independent variables We conducted series systematic evaluations ARMS abovementioned PDDLSTRIPS domains IPC 2002 competition In experiments vary independent parameters measure error rate redundancy rate CPU time robustness crossvalidation In subsequent experiments E error rate R redundancy rate CPU time measured seconds values means ﬁvefold cross validation 132 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 9 Varying number plans Error Rate Fig 10 Varying number plans Redundancy Rate 631 Varying probability threshold The probability threshold θ varied 0 100 When θ high fewer frequent action pairs remain building clauses weighted SAT formula result computation efﬁcient cost potentially higher error redundancy rates In experiments degree partial information α set zero observation intermediate states middle plans The results varying θ range zero 100 given Tables 1116 excerpt data smaller range zero 01 table shown graphically Figs 14 ease understanding We ﬁrst focus domains consistent performance entire probability threshold range zero 100 As seen Figs 14 increase probability threshold 01 error rates domains Depot Satellite generally increase unstable behavior In contrast range error rates Zenotravel Driverlog domains stable low The redundancy rates domains generally decrease smaller range stays stable larger range Tables 1116 On CPU time Clause Number charts efﬁciency domains dramatically increases Q Yang et al Artiﬁcial Intelligence 171 2007 107143 133 Fig 11 Varying number plans CPU Time Fig 12 Varying number plans Clause Number Over 100 range Tables 1116 probability threshold θ increases number clauses decreases At time CPU time decreases This understood probability threshold increases fewer actions relations encoded clauses Thus computational work load decreases Two domains produce result θ 60 high quality action models θ 60 We θ increases action models slightly accurate seen increasing error rates This number plan constraints decreases probability threshold θ increases resulting ARMS producing simple action model number plan constraints small Thus MaxSatSolver underconstrained Overall error rate E redundancy rate R stay roughly level θ increases 100 domains This indicates learned model relatively stable efﬁciency improved greatly increase probability threshold 134 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 13 Learning Depot domain MaxWalkSat varying probability threshold θ Error Rate Fig 14 Learning Depot domain MaxWalkSat varying probability threshold θ Redundancy Rate For domains Rover Freecell learning computationally expensive run PC value θ 60 Thus learn action models computational resources θ 60 threshold This domains complex seen large number relations initial state relations domains Table 8 From Figs 14 Depot domain highest error rates This fact Depot domain actions paired frequent action pair As result clause numbers large resulting inaccurate solutions weighted MAXSAT problem This conﬁrmed clause number chart Figs 14 average number clauses Depot domain highest This tells ARMS perform number potential clauses small number frequent pairs small Q Yang et al Artiﬁcial Intelligence 171 2007 107143 135 Fig 15 Learning Depot domain MaxWalkSat varying probability threshold θ CPU Time Fig 16 Learning Depot domain MaxWalkSat varying probability threshold θ Clause Number 632 Varying degree partial information As mentioned beginning section degree partial information designed test ARMS algorithm beneﬁt partial information occasionally observed middle plans This variable denoted p value ranges zero 100 When p ranges zero 100 available state information increases The observed relations explain frequent pairs actions additional clauses These clauses involve additional observations given higher weights relations Thus increase p expect increase accuracy increase number clauses The experimental results shown Figs 58 In experiments ﬁxed probability threshold θ 10 As intermediate state information known plans number clauses larger CPU time increases However knowing information states makes lution better seen error redundancy rates As expected Freecell domain produces 136 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 17 Learning Depot domain MaxWalkSat varying degree partial information p Error Rate Fig 18 Learning Depot domain MaxWalkSat varying degree partial information p Redundancy Rate clauses ARMS handle large number relations enforce partial information constraints Thus results reported domain We conclude number relations small knowing state information helps accuracy conciseness learned action model cost demanding computation 633 Varying number plans In order test size training set affects quality learned model conducted experiments vary number plans training data set In Figs 912 vary number plans training set 32 160 record error redundancy rates quality model We record number clauses CPU time As training set increases size error rate E redundancy rate R stay level However CPU time number clauses increase Q Yang et al Artiﬁcial Intelligence 171 2007 107143 137 Fig 19 Learning Depot domain MaxWalkSat varying degree partial information p CPU Time Fig 20 Learning Depot domain MaxWalkSat varying degree partial information p Clause Number expected This shows stable learning action models training set sizes small 634 Using MaxWalkSat weighted MAXSAT solver MaxWalkSat powerful solver ﬁnding approximate solution weighted MAXSAT problems 1126 It aims minimize weights associated unsatisﬁed clauses local minima avoided randomly ﬂipping assignment variable unsatisﬁed clause 26 By applying different MAXSAT solver hope establish fact learning algorithm general independent actual tool select As local search algorithm MaxWalkSat terminates sum weights unsatisﬁed clauses falls predeﬁned target cost value We conducted series experiments replaced MaxSatSolver MaxWalkSat solving weighted MAXSAT problem We able similar learning performance MaxSatSolver experiments 138 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 21 Learning Depot domain MaxWalkSat varying Number Plans Error Rate Fig 22 Learning Depot domain MaxWalkSat varying Number Plans Redundancy Rate We illustrate results MaxWalkSat solver ARMS Figs 1316 plot error rate redundancy rate CPU time average number clauses SAT formulae Depot domain Figs 1720 error rate redundancy rate CPU time average number clauses function partial information known intermediate states plan Finally Figs 2124 metrics function number plans training data set Depot domain Comparing ﬁgures Figs 112 MaxWalkSat MaxSatSolver performs similarly terms quality solution efﬁciency Finally compared CPU times MaxWalkSat MaxSatSolver systems planning problems training testing Depot domain The results shown Figs 2527 In ﬁgure error bars obtained ﬁve fold cross validation experiments As seen ﬁgures systems similar performance trends solving Depot domain learning problems Q Yang et al Artiﬁcial Intelligence 171 2007 107143 139 Fig 23 Learning Depot domain MaxWalkSat varying Number Plans CPU Time Fig 24 Learning Depot domain MaxWalkSat varying Number Plans Clause Number 7 Conclusions future work In paper developed algorithm automatically learning action models set plan examples intermediate states unknown With domain description set successful plan traces learn action model approximates ideal models designed human experts We shown empirically feasible learn models reasonably efﬁcient way weighted MAXSAT solution intermediate states observed removes signiﬁcant burden accumulation training data Our learned action models edited experts applied practical planning While able learn reasonable action models plan traces limitations current work plan overcome future extensions 140 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 Fig 25 A comparison MaxWalkSat MaxSat Solver Probability Threshold Fig 26 A comparison MaxWalkSat MaxSat Solver Threshold Partial State Information 1 Our algorithm starts action model However real world case partially completed existing action model start It interesting consider extend ARMS allow possibly imperfect incomplete action model input We conjecture situations existing action models bias learning process simplifying weighted MAXSAT formulation making learning effective 2 We assumed ARMS given correct set plan traces training data However real world noise exists form incomplete observed action sequences incorrect observations actions Furthermore plans fail achieve goals Thus interesting consider extend ARMS handle noise training data 3 Although evaluated new error measures ﬁnd way learned models test plan generation A major difﬁculty ability evaluate quality Q Yang et al Artiﬁcial Intelligence 171 2007 107143 141 Fig 27 A comparison MaxWalkSat MaxSat Solver Number Plans generated plans learned models We believe accomplished interactively repairing models help human experts 4 The STRIPS language considered paper simple model real world situations We wish extend PDDL language including quantiﬁers time resources We believe quantiﬁcation learned considering compression learned action model SAT representation Another direction allow actions duration consume resources 5 We wish explore application ARMS iteratively sets actions collection plans order decreasing support measure following planning abstraction approach This allows frequent action sets explained ﬁrst causing fast convergence producing superior action models 6 As pointed Section 54 strong connection ARMS algorithm actionmodel learning problem framework Markov Logic Networks MLN We pointed model actionmodel learning problem MLN mapping components including constraints weights solution MLN corresponds learned action model In future wish explore connection especially issue allow expressive types action models learned 7 Another possibility involve humans loop learning process In mixinitiative planning frame work Myers et al 31 human users systems work completing planning task While identiﬁes possible candidates selected completing plan sketch human users choices options In future work consider involve human users learning process potential options exist explaining frequent sets actions training plans human experts biased making ﬁnal selection order obtain high quality action model 8 In related work section reviewed CaMeL 23 acquires method preconditions hierarchical task network HTN planning domain We mentioned 23 plan traces deriva tional trees traces taken input learning produces set preconditions HTN schemata It interesting consider possible directions future works First interesting include input ARMS additional forms domain theory derivational traces learning process With additional knowledge expect weighted MAXSAT solution effective additional bias learning A second direction extend ARMS learning HTN method preconditions HTN schemata Such extension comparison CaMel interesting future work 9 Finally future wish evaluate best determine upper bound U pre post conditions action models This parameter experimentally varying U small value large value 142 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 order search lowest error redundancy rates test plans The best value determined empirically We wish continue evaluate strategy future work Acknowledgement We wish thank Hong Kong RGC grant 621606 supporting research References 1 R Agrawal R Srikant Fast algorithms mining association rules Proceedings 20th International Conference Very Large Data Bases VLDB Santiago Chile Chile Morgan Kaufmann 1994 pp 487499 2 E Amir Learning partially observable deterministic action models Proceedings International Joint Conference Artiﬁcial Intelli gence IJCAI 2005 Edinburgh Scotland UK August 2005 pp 14331439 3 M Bain C Sammut A framework behavioural cloning Machine Intelligence 15 1996 4 S Benson Inductive learning reactive action models Proceedings International Conference Machine Learning ICML 1995 Stanford University Stanford CA 1995 pp 4754 5 J Blythe J Kim S Ramachandran Y Gil An integrated environment knowledge acquisition Proceedings 2001 International Conference Intelligent User Interfaces IUI2001 Santa Fe NM 2001 pp 1320 6 B Borchers J Furman httpinfohostnmteduborchersmaxsathtml 7 B Borchers J Furman A twophase exact algorithm MAXSAT weighted MAXSAT problems Journal Combinatorial Optimiza tion 2 4 1999 299306 8 M Davis H Putnam A computing procedure quantiﬁcation theory Journal The Association Computing Machinery 7 1960 201215 9 P Domingos S Kok H Poon M Richardson P Singla Unifying logical statistical AI Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 Boston MA July 2006 10 K Erol JA Hendler DS Nau Htn planning Complexity expressivity Proceedings National Conference Artiﬁcial Intelli gence AAAI 1994 Seattle WA 1994 pp 11231128 11 H Kautz et al httpwwwcswashingtoneduhomeskautzwalksat 12 WH Evans JC Ballegeer NH Duyet ADL An algorithmic design language integrated circuit synthesis Proceedings 21st Design Automation Conference Design Automation 1984 13 RE Fikes NJ Nilsson Strips A new approach application theorem proving problem solving Artiﬁcial Intelligence 2 1971 189208 14 M Fox D Long PDDL21 An extension pddl expressing temporal planning domains Journal Artiﬁcial Intelligence Research 20 2003 61124 15 MR Garey DS Johnson Computers Intractability A Guide Theory NPCompleteness WH Freeman Company 1979 16 A Garland N Lesh Plan evaluation incomplete action descriptions Proceedings Eighteenth National Conference AI AAAI 2002 Edmonton Alberta Canada 2002 pp 461467 17 M Ghallab A Howe C Knoblock D McDermott A Ram M Veloso D Weld D Wilkins PDDLthe planning domain deﬁnition language 1998 18 Y Gil Learning experimentation Incremental reﬁnement incomplete planning domains Eleventh Intl Conf Machine Learning 1994 pp 8795 19 MX Goemans DP Williamson Improved approximation algorithms maximum cut satisﬁability problems semideﬁnite pro gramming Journal ACM 42 1995 11151145 20 PD Grunwald IJ Myung MA Pitt Advances Minimum Description Length Theory Applications MIT Press Cambridge MA 2005 21 O Ilghami H MunozAvila DS Nau DW Aha Learning preconditions planning plan traces HTN structure Journal Artiﬁcial Intelligence Research 20 2003 379404 22 O Ilghami H MunozAvila DS Nau DW Aha Learning preconditions planning plan traces htn structure Proceedings International Conference Machine Learning ICML 2005 Bonn Germany 2005 23 O Ilghami DS Nau H MunozAvila Camel Learning method preconditions htn planning Proceedings Sixth International Conference AI Planning Scheduling AIPS02 Toulouse France 2002 pp 168178 24 RG Jeroslow J Wang Solving propositional satisﬁability problems Annals Mathematics AI 1 1990 167187 25 H Kautz B Selman Pushing envelope Planning propositional logic stochastic search Proceedings Thirteenth National Conference Artiﬁcial Intelligence AAAI 1996 Portland OR 1996 pp 11941201 26 H Kautz B Selman Y Jiang A general stochastic approach solving problems hard soft constraints The Satisﬁability Problem Theory Applications 1997 27 T Lau P Domingos DS Weld Version space algebra application programming demonstration Proc 17th International Conf Machine Learning Morgan Kaufmann San Francisco CA 2000 pp 527534 28 DW Loveland Automated Theorem Proving A Logical Basis NorthHolland New York 1978 29 T Leo McCluskey D Liu RM Simpson GIPO II HTN planning toolsupported knowledge engineering environment Proceedings International Conference Automated Planning Scheduling ICAPS 2003 Trento Italy 2003 pp 92101 Q Yang et al Artiﬁcial Intelligence 171 2007 107143 143 30 MW Moskewicz CF Madigan Y Zhao L Zhang S Malik Chaff Engineering efﬁcient sat solver Proceedings 38th Design Automation Conference DAC 2001 31 KL Myers P Jarvis WM Tyson M Wolverton Mixedinitiative framework robust plan sketching Thirteenth International Confer ence Automated Planning Scheduling ICAPS03 BC Canada AAAI Press 2003 pp 256266 32 DS Nau TC Au O Ilghami U Kuter JW Murdock D Wu F Yaman Shop2 An HTN planning Journal Artiﬁcial Intelligence Research 20 2003 379404 33 DS Nau TC Au O Ilghami U Kuter JW Murdock D Wu F Yaman Applications shop shop2 IEEE Intelligent Systems 20 2 2005 3441 34 T Oates PR Cohen Searching planning operators contextdependent probabilistic effects Proceedings Thirteenth National Conference AI AAAI 1996 Portland OR 1996 pp 865868 35 M Richardson P Domingos Markov logic networks Technical Report 2004 36 M Richardson P Domingos Markov logic networks Machine Learning 62 12 July 2006 107136 37 G Sablon D Boulanger Using event calculus integrate planning learning intelligent autonomous agent Current Trends AI Planning IOS Press 1994 pp 254265 38 B Selman H Kautz An empirical study greedy local search satisﬁability testing Proceedings Eleventh National Conference Artiﬁcial Intelligence AAAI93 Washington DC 1993 pp 4651 39 B Selman H Kautz B Cohen Local search strategies satisﬁability testing Cliques Coloring Satisﬁability Second DIMACS Implementation Challenge 26 October 1113 1993 40 D Shahaf E Amir Learning partially observable action schemas Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 Boston MA July 2006 41 D Shahaf A Chang E Amir Learning partially observable action models Efﬁcient algorithms Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 Boston MA July 2006 42 W Shen Autonomous Learning Environment Computer Science PressWH Freeman Company 1994 43 X Wang Learning observation practice An incremental approach planning operator acquisition Proceedings Twelfth International Conference Machine Learning ICML 1995 1995 pp 549557 44 E Winner M Veloso Analyzing plans conditional effects Proceedings Sixth International Conference AI Planning Scheduling AIPS 2002 Toulouse France 2002 45 Q Yang Formalizing planning knowledge hierarchical planning Computational Intelligence Journal 6 2 1990 1224 46 Q Yang K Wu Y Jiang Learning action models plan examples incomplete knowledge Proceedings Fifteenth Interna tional Conference Automated Planning Scheduling ICAPS 2005 Monterey CA 2005 pp 241250 47 J Yin D Shen Q Yang ZN Li Activity recognition goalbased segmentation Proceedings Twentieth National Confer ence Artiﬁcial Intelligence AAAI05 Pittsburgh PA 2005 pp 2834 48 J Yin Q Yang Integrating hidden Markov models spectral analysis sensory time series clustering Proceedings Fifth IEEE International Conference Data Mining Houston TX 2005 49 H Zhang SATO An efﬁcient propositional prover Proceedings 14th International Conference Automated Deduction CADE 1997 Townsville North Queensland Australia 1997 pp 272275