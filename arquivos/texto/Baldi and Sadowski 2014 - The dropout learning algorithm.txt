Artiﬁcial Intelligence 210 2014 78122 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint The dropout learning algorithm Pierre Baldi Peter Sadowski Department Computer Science University California Irvine CA 926973435 United States r t c l e n f o b s t r c t Article history Received 11 June 2013 Received revised form 18 February 2014 Accepted 18 February 2014 Available online 24 February 2014 Keywords Machine learning Neural networks Ensemble Regularization Stochastic neurons Stochastic gradient descent Backpropagation Geometric mean Variance minimization Sparse representations Dropout recently introduced algorithm training neural networks randomly dropping units training prevent coadaptation A mathematical analysis static dynamic properties dropout provided Bernoulli gating variables general accommodate dropout units connections variable rates The framework allows complete analysis ensemble averaging properties dropout linear networks useful understand nonlinear case The ensemble averaging properties dropout nonlinear logistic networks result fundamental equations 1 approximation expectations logistic functions normalized geometric means bounds estimates derived 2 algebraic equality normalized geometric means logistic functions logistic means mathematically characterizes logistic functions 3 linearity means respect sums products independent variables The results extended classes transfer functions including rectiﬁed linear functions Approximation errors tend cancel accumulate Dropout connected stochastic neurons predict ﬁring rates backpropagation viewing backward propagation ensemble averaging dropout linear network Moreover convergence properties dropout understood terms stochastic gradient descent Finally regularization properties dropout expectation dropout gradient gradient corresponding approximation ensemble regularized adaptive weight decay term propensity selfconsistent variance minimization sparse representations 2014 The Authors Published Elsevier BV Open access CC BYNCND license 1 Introduction Dropout recently introduced algorithm training neural networks 27 In simplest form presentation training example feature detector unit deleted randomly probability q 1 p 05 The remaining weights trained backpropagation 40 The procedure repeated example training epoch shar ing weights iteration Fig 11 After training phase completed predictions produced halving weights Fig 12 The dropout procedure applied input layer randomly deleting inputvector componentstypically input component deleted smaller probability q 02 The motivation intuition algorithm prevent overﬁtting associated coadaptation feature detectors By randomly dropping neurons procedure prevents neuron relying excessively output neuron forcing instead rely population behavior inputs It viewed extreme form Corresponding author Email address pfbaldiciuciedu P Baldi httpdxdoiorg101016jartint201402004 00043702 2014 The Authors Published Elsevier BV Open access CC BYNCND license P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 79 Fig 11 Dropout training simple network For training example feature detector units dropped probability 05 The weights trained backpropagation BP shared examples Fig 12 Dropout prediction simple network At prediction time weights feature detectors output units halved bagging 17 generalization naive Bayes 23 denoising autoencoders 42 Dropout reported yield remarkable improvements diﬃcult problems instance speech image recognition known benchmark datasets MNIST TIMIT CIFAR10 ImageNet 27 In 27 noted single unit dropout performs kind geometric ensemble averaging property conjectured extend deep multilayer neural networks Thus dropout intriguing new algorithm shallow deep learning effective comes little formal understanding raises interesting questions For instance 1 What kind model averaging dropout implementing exactly approximation applied multiple layers 2 How crucial parameters For instance q 05 necessary happens values What happens transfer functions 3 What effects different deletion randomization procedures different values q different layers What happens dropout applied connections units 4 What precisely regularization averaging properties dropout 5 What convergence properties dropout To answer questions useful distinguish static dynamic aspects dropout By static refer properties network ﬁxed set weights dynamic properties related temporal learning process We begin focusing static properties particular understanding kind model averaging implemented rules like halving weights To extent question asked set weights regardless learning stage procedure Furthermore useful ﬁrst study effects droupout simple networks particular linear networks As case 89 understanding dropout linear networks essential understanding dropout nonlinear networks Related work Here point connections dropout previous literature attempt exhaustive require review paper First dropout randomization algorithm connected vast literature science mathematics centuries old use randomness derive new algorithms improve existing ones prove interesting mathematical results 22333 80 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Second speciﬁcally idea injecting randomness neural network hardly new A simple Google search yields dozen references dating 1980s 2425303412637 In references noise typically injected input data synaptic weights increase robustness regularize network empirical way Injecting noise data precisely idea denoising autoencoders 42 closest predecessor dropout recent variations marginalizedcorruptedfeatures learning approach described 29 Finally posting 27 articles dropout title presented NIPS 2013 conference training method based overlaying dropout binary belief network neural network 7 analysis adaptive regularizing properties dropout shallow linear case suggesting possible improvements 43 subset averaging regularization properties dropout described primarily Sections 8 11 article 10 2 Dropout shallow linear networks In order compute expectations associate deﬁned random variables unit activities connection weights dropped Here consider unit activity connection set 0 unit connection dropped 21 Dropout single linear unit combinatorial approach We begin considering single linear unit computing weighted sum n inputs form S SI ncid2 i1 w Ii 1 I I1 In input vector If delete inputs uniform distribution possible subsets inputs equivalently probability q 05 deletion 2n possible networks including network For ﬁxed I average output networks written ES 1 2n cid2 N SN I 2 N index possible subnetworks possible edge deletions Note simple case deletion input units edges thing The sum expanded networks size 0 1 2 n form cid3 cid4 cid5 cid6 cid2 cid7 w Ii w j I j cid8 w Ii ES 1 2n 0 ncid2 i1 1cid2i jcid2n cid7 cid6 n 1 n 1 2n1 In expansion term w Ii occurs cid7 cid7 cid6 cid6 1 n 1 1 n 1 2 times So ﬁnally average output ES 2n1 2n cid5 w Ii ncid2 i1 ncid2 i1 w 2 Ii cid4 3 4 5 Thus case single linear unit ﬁxed input I output obtained halving weights equal arithmetic mean outputs produced possible subnetworks This combinatorial approach applied cases p cid3 05 easier work directly probabilistic approach 22 Dropout single linear unit probabilistic approach Here simply consider output random variable form S ncid2 i1 w iδi Ii 6 δi Bernoulli selector random variable deletes weight w equivalently input Ii probability P δi 0 qi The Bernoulli random variables assumed independent fact pairwise independence opposed global independence suﬃcient results presented Thus P δi 1 1 qi pi Using linearity expectation immediately P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 ES ncid2 i1 w EδiIi ncid2 i1 w pi Ii 81 7 This formula allows handle different pi connection values pi deviate 05 If connections associated independent identical Bernoulli selector random variables pi p ES ncid2 i1 w EδIi ncid2 i1 w p Ii 8 cid9 Thus note instance inputs deleted probability 02 expected output given 08 w Ii Thus weights multiplied 08 The key property Eq 8 linearity expectation respect sums multiplications scalar values generally follows linearity expectation respect product independent random variables Note approach applied estimating expectations input variables training examples training examples subnetworks This remains true distribution examples uniform If unit ﬁxed bias b aﬃne unit random output variable form S ncid2 i1 w iδi Ii bδb 9 The case bias present δb 1 special case And linearity expectation ES ncid2 i1 w pi Ii bpb 10 P δb 1 pb Under natural assumption Bernoulli random variables independent variance linear respect sum easily calculated previous cases For instance starting general case Eq 9 VarS ncid2 i1 w 2 VarδiI 2 b2 Varδb ncid2 i1 w 2 piqi I 2 b2 pbqb 11 qi 1 pi S viewed weighted sum independent Bernoulli random variables approximated Gaussian random variable reasonable assumptions 23 Dropout single layer linear units We consider single linear layer k output units S iI ncid2 j1 w j I j 1 k 12 In case dropout applied input units slightly different dropout applied connections Dropout applied input units leads random variables S iI ncid2 j1 w jδ j I j 1 k dropout applied connections leads random variables S iI ncid2 j1 δi j w j I j 1 k 13 14 In case expectations variances covariances easily computed linearity expectation independence assumption When dropout applied input units ES ncid2 j1 w j p j I j 1 k 15 82 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 VarS ncid2 j1 w 2 j p jq j I 2 j 1 k CovS Sl ncid2 j1 w j wlj p jq j I 2 j 1 cid2 l cid2 k When dropout applied connections ES ncid2 j1 w j pi j I j 1 k VarS ncid2 j1 w 2 j pi jqi j I 2 j 1 k CovS Sl 0 1 cid2 l cid2 k 16 17 18 19 20 Note difference covariance models When dropout applied connections S Sl entirely independent 3 Dropout deep linear networks In general feedforward linear network described underlying directed acyclic graph units organized layers shortest path input units unit consideration The activity unit layer h expressed I Sh cid2 cid2 lh j whl j Sl j S 0 j I j 21 Again general case dropout applied units slightly different dropout applied connections Dropout applied units leads random variables Sh cid2 cid2 lh j whl j δl j Sl j S 0 j I j dropout applied connections leads random variables Sh cid2 cid2 lh j j whl δhl j Sl j S 0 j I j 22 23 When dropout applied units assuming dropout process independent unit activities weights cid10 cid11 cid2 E Sh cid2 whl j pl j E cid11 cid10 Sl j h 0 24 lh j j I j input layer This formula applied recursively entire network starting ES 0 input layer Note recursion Eq 24 formally identical recursion backpropagation suggesting use dropout backward pass This point elaborated end Section 10 Note expectation ESh taken possible subnetworks original network Bernoulli gating variables previous layers l h matter Therefore coincides expectation taken induced subnetworks node comprising nodes ancestors node Remarkably expectations covariances computed recursively input layer output layer writing CovSh cid4 Sh icid4 ESh cid2 Sh cid2 cid4 icid4 ESh ESh cid13 icid4 computing cid2 cid2 cid2 cid2 cid12cid2 cid2 cid11 cid10 E cid4 Sh Sh icid4 E cid4 cid4 l wh icid4 jcid4 δl cid4 cid4 jcid4 Sl jcid4 whl j wh cid4 cid4 l icid4 jcid4 E cid10 cid4 jδl δl jcid4 cid11 cid10 E cid4 j Sl Sl jcid4 cid11 25 lh j lcid4hcid4 jcid4 lh lcid4hcid4 j jcid4 usual assumption δl independent l cid3 l usual independence assumptions ESh jcid4 independent Sl jδl case Eδl cid4 jcid4 Furthermore usual assumption δl jcid4 pl j pl icid4 computed recursively values ESl jcid4 furthermore Eδl jcid4 j Thus short j Sl jcid4 lower layers j δl j cid3 j j Sl jδl Sh j pl jδl cid4 cid4 cid4 cid4 cid4 cid4 whl j δl j Sl j cid4 cid4 cid4 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 83 boundary conditions EIi I j Ii I j ﬁxed input vector layer 0 The recursion proceeds layer layer input output layer When new layer reached covariances previously visited layers computed intralayer covariances When dropout applied connections similar independence assumptions cid2 cid2 cid11 cid10 E Sh j whl phl j E cid11 cid10 Sl j h 0 lh j 26 j I j ES 0 input layer This formula applied recursively entire network Note expectation ESh taken possible subnetworks original network Bernoulli gating variables previous layers l h matter Therefore expectation taken induced subnet works node corresponding ancestors node Furthermore expectations covariances computed recursively input layer output layer similar analysis given case dropout applied units general linear network In summary linear feedforward networks static properties dropout applied units connections Bernoulli gating variables independent weights activities necessarily identically distributed fully understood For input expectation outputs possible networks induced Bernoulli gating variables computed recurrence equations 24 26 simple feedforward propagation network weight multiplied appropriate probability associated corresponding Bernoulli gating variable The variances covariances computed recursively similar way 4 Dropout shallow neural networks We consider dropout nonlinear networks shallow fact single layer weights 41 Dropout single nonlinear unit logistic Here consider output single unit total linear input S given logistic sigmoidal function O σ S 1 1 ceλS 27 Here c cid3 0 There 2n possible subnetworks indexed N ﬁxed input I subnetwork produces linear value SN I ﬁnal output value O N σ N σ SN I Since I ﬁxed omit dependence I following calculations In uniform case geometric mean outputs given G cid14 N 12n N O Likewise geometric mean complementary outputs 1 O N given cid14 1 O N 12n cid4 G N The normalized geometric mean NGM deﬁned NGM G G Gcid4 The NGM outputs given cid10 cid11 O N NGM cid15 cid15 N σ SN 12n N σ SN 12n cid15 N 1 σ SN 12n cid15 1 1 1σ SN σ SN N 12n Now logistic function σ 1 σ x σ x ce λx Applying identity Eq 31 yields cid10 cid11 O N NGM cid15 1 1 N ceλSN 12n 1 ce λ 1 cid9 N SN 2n cid10 cid11 ES σ 28 29 30 31 32 33 35 36 37 38 39 84 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 cid9 ES cid11 σ S cid10 cid11 ES σ N SN 2n Or compact form cid10 NGM 34 Thus uniform distribution possible subnetworks N equivalent having iid input unit selector variables δ δi probability pi 05 NGM simply obtained keeping overall network dividing cid9 weights applying σ expectation ES n i1 It essential observe result remains true case nonuniform distribution subnetworks N distribution generated Bernoulli gating variables identically distributed p cid3 05 For consider general distribution P N This course general assuming P product n independent Bernoulli selector variables In case weighted geometric means deﬁned w 2 Ii cid14 G O P N N N cid14 1 O N P N cid4 G N similarly normalized weighted geometric mean NWGM NWGM G G Gcid4 Using calculation uniform case compute normalized weighted geometric mean NWGM form cid10 cid11 O N cid15 NWGM cid10 cid11 O N NWGM cid9 cid15 N σ SN P N cid15 N σ SN P N 1 N 1σ SN σ SN cid15 1 P N N 1 σ SN P N 1 ce λ 1 cid9 N P N SN cid10 cid11 ES σ N P N SN Thus summary distribution P N possible subnetworks N including ES case independent identically distributed input unit selector variables δi probability pi NWGM simply obtained applying logistic function expectation linear input S In case independent necessarily identically distributed selector variables δi probability pi equal expectation S computed simply keeping overall network multiplying weight w pi ES cid9 n i1 pi w Ii Note linear case property logistic units general That set S 1 Sm i1 P 1 associated outputs O 1 O m O σ S associated probability distribution P 1 P m NWGMO σ E σ P S Thus NVGM computed inputs inputs subnetworks distributions associated subnetworks distribution uniform For instance add Gaussian noise weights formula applied Likewise approximate average activity entire neuronal layer applying logistic function average input neurons layer long neurons layer use logistic function Note property true c λ analyses provided sections applicable units network different units different values c λ Finally property general sense calculation shows function f cid9 cid9 m cid10 cid10 σ cid11cid11 cid10 cid10 cid11cid11 f S σ E f S NWGM particular k cid10 cid10 cid11cid11 cid10 NWGM σ Sk σ cid11cid11 cid10 E Sk 42 Dropout single layer logistic units In case single output layer k logistic functions network computes k linear sums S 1 k k outputs form 40 41 cid9 n j1 w j I j O σiS 42 The dropout procedure produces subnetwork M N1 Nk Ni represents corresponding subnetwork associated ith output unit For 2n possible subnetworks unit 2kn possible subnetworks M In case Eq 39 holds unit individually If dropout uses independent Bernoulli selector variables δi j edges generally subnetworks N1 Nk selected independently covariance output units 0 If dropout applied input units covariance sigmoidal outputs small nonzero 43 Dropout set normalized exponential units P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 85 We consider case layer normalized exponential units In case think network j1 w j I j 1 k k outputs having k outputs obtained ﬁrst computing k linear sums form S form cid9 n O eλS cid9 k j1 eλS j cid9 1 1 jcid3i eλS j eλS 43 Thus O logistic output coeﬃcients logistic function depend values S j j cid3 The dropout procedure produces subnetwork M N1 Nk Ni represents corresponding subnetwork associated ith output unit For 2n possible subnetworks unit 2kn possible subnetworks M We assume ﬁrst distribution P M factorial P M P N1 P Nk equivalent assuming subnetworks associated individual units chosen independently This case independent Bernoulli selector applied connections The normalized weighted geometric average output unit given NWGMO cid15 M cid15 cid9 k l1 eλSi N cid9 k j1 e λS j N j P M M eλSl N l cid9 λS j N k j1 e j P M Simplifying numerator NWGMO P M Factoring collecting exponential terms gives 1 cid9 k l1lcid3i cid15 1 M eλSl N l eλSi N NWGMO cid9 1 e M λP MS iNi 1 cid9 k l1lcid3i e cid9 M λP MSlNl 44 45 46 1 eλES NWGMO 1 cid9 k l1lcid3i eλESl eλES cid9 k l1 eλESl Thus distribution P N possible subnetworks N including case independent identically distributed input unit selector variables δi probability pi NWGM normalized exponential unit obtained applying normalized exponential expectations underlying linear sums S In case independent necessarily identically distributed selector variables δi probability pi equal expectation S computed simply keeping overall network multiplying weight w pi ES cid9 n j1 p j w I j 47 5 Dropout deep neural networks Finally deal interesting case deep feedforward networks sigmoidal units1 described set equations form O h σ h cid11 cid10 Sh σ cid6cid2 cid2 lh j cid7 O 0 j I j whl j O l j Dropout units described O h σ h cid11 cid10 Sh σ cid6cid2 cid2 lh j cid7 O 0 j I j whl j δl j O l j selector variables δl j similarly dropout connections For sigmoidal unit NWGM cid10 cid11 O h cid15 cid15 P N N O h cid15 N 1 O h P N N O h P N 1 Given results previous sections network include linear units normalized exponential units 48 49 50 86 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 basic idea approximate expectations corresponding NWGMs allowing propagation expectation symbols outside sigmoid symbols inside cid10 cid16 σ E cid11cid17 SN I NWGM cid16 cid17 O N I cid10 cid16 E σ cid17cid11 SN I More precisely following recursion cid11 cid10 E O h NWGM cid11 cid10 E Sh NWGM cid11 cid10 σ h O h cid2 cid2 cid10 cid11 O h cid10 cid16 cid11cid17 Sh E j pl j E whl cid11 cid10 O l j 51 52 53 54 lh j Eqs 52 53 54 fundamental equations underlying recursive dropout ensemble approximation deep neural networks The direct approximation equations course Eq 52 discussed depth Sections identical possible subnetworks N However 8 9 This equation exact numbers O h numbers O h identical normalized weighted geometric mean provides good approximation If network contains linear units Eq 52 necessary units average computed exactly The fundamental assumption Eq 54 independence selector variables activity units value weights expectation product equal product expectations Under conditions analysis applied dropout gating variables applied connections instance Gaussian noise added unit activities Finally measure consistency CO h I neuron layer h input I variance VarO h I taken subnetworks N distribution input I ﬁxed The larger variance consistent neuron worse expect approximation Eq 52 Note random variable O 0 1 variance bound small exceed 14 This VarO EO 2 EO 2 cid2 EO EO 2 EO 1 EO cid2 14 The overall input consistency neuron deﬁned average CO h I taken training inputs I similar deﬁnitions generalization consistency averaging CO h I generalization set Before examining quality approximation Eq 52 study properties NWGM aver aging ensembles predictors classes transfer functions satisfying key dropout NWGM relation NWGM f x f Ex exactly approximately 6 Ensemble optimization properties The weights neural network typically trained gradient descent error function computed outputs corresponding targets The error functions typically squared error regression relative entropy classiﬁcation Considering single example single output O target t errors functions written ErrorO t 1 2 t O 2 ErrorO t t log O 1 t log1 O 55 Extension multiple outputs including classiﬁcation multiple classes normalized exponential transfer functions immediate These error terms summed examples predictors case ensemble Both error functions convex simple application Jensens theorem shows immediately error ensemble average average error ensemble components Thus case ensemble producing outputs O 1 O m convex error function cid2 cid6cid2 cid7 Error pi O t cid2 pi ErrorO t ErrorE cid2 EError Note true individual example true set examples identically distributed Eq 56 key equation ensembles averaging arithmetically In case dropout logistic output unit previous analyses NWGM approximation E basis reasonable way combining predictors ensemble possible subnetworks However following stronger result holds For convex error function weighted geometric mean WGM normalized version NWGM ensemble possess qualities expectation In words cid6cid14 cid7 Error cid6 Error cid15 O pi t cid2 cid15 O cid15 cid2 pi O pi i1 O ipi pi ErrorO t ErrorWGM cid2 EError cid7 t cid2 cid2 pi ErrorO t ErrorNWGM cid2 EError 56 57 58 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 87 In short convex error function error expectation weighted geometric mean normalized weighted geometric mean ensemble predictors expected error f convex g increasing composition f g convex This easily shown di Proof Recall rectly applying deﬁnition convexity 3916 additional background convexity Eq 57 obtained applying Jensens inequality convex function Errorg g increasing function gx ex points log O 1 log O m Eq58 obtained applying Jensens inequality convex function Errorg g creasing function gx ex1 ex points log O 1 log1 O 1 log O m log1 O m The cases O equal 0 1 handled directly irrelevant purposes logistic output exactly equal 0 1 Thus circumstances ﬁnal output equal weighted mean weighted geometric mean normal ized weighted geometric mean underlying ensemble Eqs 56 57 58 apply exactly This case instance linear networks nonlinear networks dropout applied output layer linear logistic normalizedexponential units Since dropout approximates expectations NWGMs concerned errors introduced ap proximations especially deep architecture dropout applied multiple layers It worth noting result shave layer approximations legitimizing use NWGMs combine models output layer instead expectation Similarly case regression problem output units linear expectations computed exactly level output layer results linear networks reducing number layers approximation expectations NWGMs carried Finally shown expectation WGM NWGM relatively close ﬂex ibility robustness predictors combined ensemble sense combining models approximations quantities outperform expectation error individual models Finally pointed prediction phase use expected values estimated computational cost Monte Carlo methods approximate values obtained forward propagation network modiﬁed weights 7 Dropout functional classes transfer functions 71 Dropout functional classes Dropout rely fundamental property logistic sigmoidal function NWGMσ σ E Thus natural wonder class functions f satisfying property Here class functions f deﬁned real line range 0 1 satisfying G G Gcid4 f f E 59 set points distribution consists exactly union constant functions f x K 0 cid2 K cid2 1 λx As reminder G denotes geometric mean G logistic functions f x 11 ce denotes geometric mean complements Note constant functions f x K 0 cid2 K cid2 1 viewed logistic functions taking λ 0 c 1 K K K 0 limiting case corresponding c cid4 Proof To prove result note ﬁrst 0 1 range required deﬁnitions G G impose f x 1 f x positive In addition function f x K 0 cid2 K cid2 1 class shown logistic functions satisfy property Thus need solutions By applying Eq 59 pairs arguments real numbers u v u cid2 v real number 0 cid2 p cid2 1 cid4 function class satisfy f f v1p f f v1p 1 f up1 f v1p cid10 f pu 1 pv cid11 60 Note f u f v function f constant entire interval u v Note f u 0 f v 0 f 0 u v As result impossible nonzero function class satisfy f u 0 f v 1 0 f v 2 0 Thus function f class constantly equal 0 f 0 Similarly symmetry function f class constantly equal 1 f 1 Consider function f Eq 60 shows interval u v f interval letting x pu 1 pv equivalently p v xv u function given class different constant 0 constant 1 function 0 f 1 completely deﬁned parameters f u f v On f x cid10 1 1 f u f u 1 cid11 vx vu cid10 cid11 xu vu 1 f v f v 61 88 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 f x 1 1 ceλx cid6 c 1 f u f u cid7 v vu cid6 1 f v f v cid7 u vu cid6 λ 1 v u log 1 f u f u f v 1 f v cid7 Note particular simple parameterization given terms f 0 1 1 c f x 1 2 x log c λ 62 63 64 65 As note elegant formula obtained Eq 60 f 0 taking u v p 05 Simple algebraic manipulations 1 f 0 f 0 cid6 1 f v f v cid6 cid7 1 2 cid7 1 2 1 f v f v 66 As result interval u v function f 1 continuous uniformly continuous 2 differentiable fact inﬁnitely differentiable 3 monotone increasing decreasing strictly f constant 4 f deﬁned limits It easy limits 0 1 For instance limit let u 0 v cid4 α v 0 α 1 v cid4 v Then cid11 cid4 cid10 f v cid10 1 1 f 0 f 0 1 1αcid10 cid11 cid11α 1 f v f v 67 As v cid4 limit independent α limit f v 0 1 Finally consider u1 u2 u3 By results quantities f u1 f u2 deﬁne unique logistic function u1 u2 similarly f u2 f u3 deﬁne unique logistic function u2 u3 It easy logistic functions identical analycity taking new points v 1 v 2 u1 v 1 u2 v 2 u3 Again f v 1 f v 2 deﬁne unique logistic function v 1 v 2 identical logistic functions v 1 u2 u2 v 2 respectively Thus logistic functions identical In short f u f v deﬁne unique logistic function inside u v unique continuation outside u v From result incorrectly infer dropout brittle overly sensitive use logistic nonlinear functions This conclusion erroneous reasons First logistic function important widely transfer functions neural networks Second alternative sigmoidal function tanhx translate upwards normalize range 0 1 interval reduces logistic function 2x This leads formula NWGM1 tanhx2 1 tanhEx2 Note 1 tanhx2 11 e NWGM approach applied directly tanh transfer function assumes negative values G NWGM deﬁned positive numbers Third use different sigmoidal function 1 x2 rescaled 0 1 deviations logistic function small lead ﬂuctu arctanx x ations range ﬂuctuations introduced approximation E NWGM Fourth importantly dropout shown work empirically transfer functions logistic including instance tanh rectiﬁed linear functions This point addressed section In case reasons overly concerned superﬁcially fragile algebraic association dropout NWGMs logistic functions 72 Dropout transfer functions In deep learning interested alternative transfer functions particular rectiﬁed linear functions alleviate problem vanishing gradients backpropagation As pointed transfer function possible compute ensemble average prediction time sampling However ensemble averaging property dropout preserved extent rectiﬁed linear transfer functions broader classes transfer functions P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 89 To ﬁrst note properties NWGM useful logistic transfer functions NWGM needed enable approximation ensemble average deterministic forward propagation For transfer function f needed relation cid10 cid11 f S E cid10 cid11 ES f 68 Any transfer function satisfying property dropout allow estimation ensemble prediction time forward propagation Obviously linear functions satisfy Eq 68 previous sections linear networks A rectiﬁed linear function RLS threshold t slope λ form cid18 RLS S cid2 t 0 λS λt 69 special case piecewise linear function Eq 68 satisﬁed linear portion satisﬁed threshold variance S small Everything equal smaller value λ help approx imation To formally assume loss generality t 0 It reasonable assume S approximately normal mean μS variance σ 2 S treatment assumption given Appendix A In case cid10 RL cid11 ES RLμS 70 cid18 μS cid2 0 0 λμS On hand cid10 cid11 RLS E cid19 λS 0 1 2πσS SμS 2 2σ 2 S e cid19 dS λ σS u μS μS σS 1 2π e u2 2 du cid10 cid11 RLS E λμS Φ cid7 cid6 μS σS λσ 2π μ2 S 2σ 2 S e Φ cumulative distribution standard normal distribution It known Φ satisﬁes 1 Φx 1 2π 1 x x2 2 e x large This allows estimate error cases If μS 0 cid20 cid20E cid10 cid11 RLS cid10 RL ES cid11cid20 cid20 λσ 2π 71 72 73 74 error approximation small directly proportional λ σ If μS 0 σS small μS σS large ΦμS σS 1 2π cid11cid20 cid11 cid20 0 RLS cid10 RL σS μS e S S 2σ 2 ES cid20 cid20E μ2 75 cid10 And similarly case μS 0 σS small μS σS large Thus cases Eq 68 holds As shall Section 11 dropout tends minimize variance σS assumption σ small reasonable Together results dropout ensemble approximation rectiﬁed linear transfer functions It possible model population RL neurons hierarchical model mean μS Gaussian random variable In case error ERLS RLES approximately Gaussian distributed 0 This point relevant Section 9 More generally line reasoning shows dropout ensemble approximation piecewise linear transfer functions long standard deviation S small relative length linear pieces Having small angles subsequent linear pieces helps strengthen quality approximation Furthermore continuous twicedifferentiable function small second derivative curvature robustly approximated linear function locally tend satisfy Eq 68 provided variance S small relative curvature In respect rectiﬁed linear transfer function closely approximated twicedifferentiable function integral logistic function For standard rectiﬁed linear transfer function Scid19 Scid19 RLS σ x dx 1 1 eλx dx 76 With approximation second derivative given σ cid4S λσ S1 σ S bounded λ4 90 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Finally general case line reasoning shows dropout ensemble approximation continuous piecewise twice differentiable transfer function provided following properties satisﬁed 1 curvature piece small 2 σS small relative curvature piece Having small angles left right tangents junction point helps strengthen quality approximation Note goal dropout training precisely σS small output unit robust independent details activities units roughly constant possible dropout subnetworks 8 Weighted arithmetic geometric normalized geometric means approximation properties m cid9 To understand dropout better understand properties relationships weighted arithmetic geometric normalized geometric means speciﬁcally NWGM sigmoidal unit approximates expectation Eσ NWGMSσ Thus consider m numbers O 1 O m corresponding probabilities i1 P 1 We typically assume m numbers satisfy 0 O 1 necessary P 1 P m results Cases O equal 0 1 trivial examined separately The case course m numbers outputs sigmoidal unit form O N σ SN given input I I1 In We let E expectation weighted arithmetic mean E m i1 P O G weighted geometric i1 P i1 O expectation complements mean G When 0 cid2 O cid2 1 let E cid15 cid4 1 E The normalized G cid4 We let V VarO We following properties weighted geometric mean given NWGM GG G i11 O iP weighted geometric mean complements Obviously E i1 O P cid4 cid4 cid9 cid9 cid15 m m m 1 The weighted geometric mean equal weighted arithmetic mean G cid2 E G cid4 cid2 E cid4 77 equality numbers O equal This true regardless number O bounded This results immediately Jensens inequality applied logarithmic function Although directly interesting bounds approximation E G involving variance 1 2 maxi O VarO cid2 E G cid2 1 2 mini O VarO 78 equality O equal This inequality originally proved Cartwright Field 20 Several reﬁnements maxi O G 2 maxi O VarO cid2 E G cid2 VarO 79 piO G2 cid2 E G cid2 piO G2 mini O G 2 mini O imini O E 1 cid2 2 mini O cid2 1 2 maxi O interesting bounds 45313212 2 Since G cid2 E G cid4 equality numbers O equal Thus weighted geometric mean equal normalized weighted geometric mean cid4 cid2 1 G cid2 GG G cid4 1 E G G cid4 cid2 E 3 If numbers O satisfy 0 O cid2 05 consistently low G cid2 G cid2 E G Gcid4 cid2 E Ecid4 G Gcid4 80 81 Note O 0 pi cid3 0 G 0 result true This easily proved Jensens inequality applying function ln x ln1 x x 0 05 It known Ky Fan inequality 113536 viewed special case Levinsons inequality 28 In short consistently low case normalized weighted geometric mean equal expectation provides better approximation expectation geometric mean We later section consistently low case particularly signiﬁcant dropout 4 If numbers O satisfy 05 cid2 O 1 consistently high cid4 G G cid4 cid2 E E G G Gcid4 cid3 E 82 cid4 0 result true In short normalized weighted Note O 1 pi cid3 0 G geometric mean greater equal expectation The proof similar previous case interchanging x 1 x 5 Note GG G cid4 underestimates E G cid4G G cid4 overestimates 1 E vice versa P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 91 Fig 81 The curve associated approximate bound E NWGM cid3 E1 E1 2E1 2E1 E Eq 87 6 This important set properties When numbers O satisfy 0 O 1 ﬁrst order approximation G E G G Gcid4 E E G cid20 cid20 cid20E G cid20 G Gcid4 cid20 cid20 cid20 cid20 83 Thus ﬁrst order approximation WGM NWGM equally good approximations expectation However results particular property 3 lead suspect NWGM better approximation bounds estimates ought derivable terms variance This seen taking second order approximation gives G E V G cid4 1 E V G G Gcid4 E V 1 2V cid4 G G Gcid4 1 E V 1 2V differences E G V 1 E G cid4 V E G G Gcid4 V 1 2E 1 2V V 1 2E 1 2V cid2 V 1 E G cid4 G Gcid4 V 2E 1 1 2V 84 85 86 The difference E NWGM small second order approximation entire range values E This E close 05 term 1 2E small E close 0 1 term V small Before provide speciﬁc bounds difference note E 05 second order approximation NWGM E vice versa E 05 Since V cid2 E1 E equality achieved 01 Bernoulli variables cid20 cid20 cid20E G cid20 G Gcid4 cid20 cid20 cid20 cid20 V 1 2E 1 2V cid2 E1 E1 2E 1 2V cid2 E1 E1 2E 1 2E1 E cid2 2E1 E1 2E 87 cid21 The inequalities optimal sense attained case Bernoulli variable expectation E The function E1 E1 2E1 2E1 E zero E 0 05 1 symmetric respect E 05 It convex maximum interval 0 05 achieved E 05 5 22 Fig 81 The function 2E1 E1 2E zero E 0 05 1 symmetric respect E 05 It convex maximum interval 0 05 achieved E 05 36 Fig 82 Note beginning learning small random weights initialization typically E close 05 Towards end learning E close 0 1 In cases bounds close 0 NWGM close E Note possible E NWGM numbers O identical For instance O 1 025 E NWGM 05 O 2 075 P 1 P 2 05 G G In short general NWGM better approximation expectation E geometric mean G The property true second order approximation Furthermore exact NWGM cid2 E G cid2 NWGM cid2 E Furthermore general NWGM better approximation mean random sample Using randomly chosen O estimate mean E leads error scales like standard deviation σ V NWGM leads error scales like V cid4 92 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 82 The curve associated approximate bound E NWGM cid3 2E1 E1 2E Eq 87 When NWGM E order cases G G Gcid4 E E G G G Gcid4 E cid3 E G 88 An example provided O 1 0622459 O 2 0731059 uniform distribution p1 p2 05 In case E 0676759 G 0674577 G cid4 0318648 NWGM 0679179 E G 0002182 NWGM E 0002420 cid4 0 In case NWGM 1 Extreme cases Note O 1 nonzero probability G j cid3 O j 0 nonzero probability Likewise O 0 nonzero probability G 0 In case NWGM 0 j cid3 O j 1 nonzero probability If O 1 O j 0 achieved nonzero probability NWGM 00 undeﬁned In principle sigmoidal neuron extreme output values 0 1 achieved simulations happen machine precision In extreme cases NWGM good approximation E depends exact distribution values For instance O 1 nonzero probability O j s close 1 NWGM 1 E On hand O 1 small nonzero probability O j s close 0 NWGM 1 good approximation E Higher order moments It useful able derive estimates variance V higher order moments numbers O especially O σ S While NWGM easily generalized higher order moments yield simple estimates mean Appendix C However higher order moments deep network trained dropout easily approximated linear case Section 9 Proof To prove results compute ﬁrst second order approximations Depending case numbers 0 O 1 expanded E G 05 0 1 consistently close boundaries Without assuming consistently low high expand 05 writing O 05 cid8i 0 cid2 cid8i cid2 05 Estimates obtained expanding E given Appendix B For distribution i05 P 1 P m m subnetworks EO 05 Ecid8 VarO Varcid8 As usual let G cid8iP 05 O P cid15 cid15 cid15 i1 2cid8iP To ﬁrst order approximation cid6 mcid2 mcid14 cid7 P mcid14 G 1 cid8i 2 i1 1 2 1 2cid8iP 1 2 i1 i1 P icid8i E The approximation obtained Taylor expansion fact 2cid8i 1 In similar way G GG G cid4 1 E cid4 E These approximations accurate cid8i 0 To second order approximation cid14 cid2 cid14 cid6 cid7 cid12 G 1 2 P n 2cid8in 1 2 1 P i2cid8i P iP 1 2 cid13 2cid8i2 R3cid8i n0 cid6 cid7 R3cid8i remainder order R3cid8i P 3 2cid8i3 1 ui3P cid11 cid10 cid82 o 89 90 91 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 ui cid2 2cid8i Expanding product gives cid6 cid7 cid12 G 1 2 cid14 cid2 n0 P n 2cid8in 1 2 1 cid2 P i2cid8i cid2 P iP 1 2 2cid8i2 cid2 j cid13 4P P jcid8icid8 j R3cid8 93 92 reduces G 1 2 cid2 cid6cid2 cid7 2 P icid8i cid2 P icid8i P icid82 o cid11 cid10 cid82 1 2 Ecid8 Varcid8 o cid11 cid10 cid82 EO VarO R3cid8 93 By symmetry cid14 cid4 G 1 O iP 1 EO VarO R3cid8 94 R3cid8 higher order remainder Neglecting remainder writing E EO V VarO G G Gcid4 E V 1 2V cid4 G G Gcid4 1 E V 1 2V 95 Thus differences mean hand geometric mean normalized geometric means satisfy E G V E G G Gcid4 V 1 2E 1 2V 1 E G cid4 V 1 E G cid4 G Gcid4 V 1 2E 1 2V 96 97 To know NWGM better approximation E WGM consider factor 1 2E1 2V equal There cases 1 E cid2 05 V cid2 05 E cid3 V 2 E cid2 05 V cid3 05 E V cid3 1 3 E cid3 05 V cid2 05 E V cid2 1 4 E cid3 05 V cid3 05 E cid2 V However 0 O 1 V cid2 E E 2 E1 E cid2 025 So cases 1 3 possible cases relationship trivially satisﬁed Thus cases second order approximation NWGM closer E WGM 9 Dropout distributions approximation properties Throughout rest article let W l σ U l denote deterministic variables dropout approximation ensemble network cid6cid2 cid2 wlh j ph j W h j W l σ cid7 98 hl j case dropout applied nodes The main question wish consider W l EO l input layer l unit good approximation 91 Dropout induction Dropout relies correctness approximation expectation activity unit dropout subnetworks corresponding deterministic variable form W l E cid11 cid10 O l 99 input layer l unit The correctness approximation seen induction For ﬁrst layer property obvious W 1 results Section 8 Now assume property true layer l Again results Section 8 NWGMO 1 EO 1 94 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 cid11 cid10 E O l1 NWGM cid10 O l1 cid11 cid10 cid10 E σ Sl1 cid11cid11 computed cid10 cid10 E σ Sl1 cid11cid11 σ cid6 cid2 cid2 wl1h j ph j E cid7 cid11 cid10 O h j σ cid6 cid2 cid2 cid7 wl1h j j W h ph j W l1 hl1 j hl1 j 100 101 The approximation Eq 101 uses course induction hypothesis This induction provide sense errors errors increase signiﬁcantly depth networks The error decomposed terms cid10 cid16 cid11cid17 cid10 cid10 cid11 cid11 cid10 cid11 cid16 cid17 cid8l E O l W l E O l NWGM O l NWGM O l W l αl βl 102 Thus follows study term 92 Sampling distributions In Section 8 shown general NWGMO provides good approximation EO To understand dropout approximation behavior deep networks look distribution difference α EO NWGMO Since E NWGM deterministic functions set O values distribution deﬁned look different samples O values taken general distribution These samples correspond dropout samples output given neuron Note number dropout subnetworks neuron exponentially large sample accessed simulations large networks However consider samples associated population neurons instance neurons given layer While expect neurons layer behave homogeneously given input general separated small number populations neurons low activity medium activity high activity analysis applied populations separately Letting O S denote sample m values O O m going simulations formal arguments general EO S NWGMO S mean close 0 small standard deviation cases approximately normally distributed For instance values O originate uniform distribution 0 1 easy E NWGM approximately normally distributed mean 05 small variance decreasing 1m 93 Mean standard deviation normalized weighted geometric mean More generally assume variables O iid mean μO variance σ 2 O Then variables S satisfying O σ S iid mean μS variance σ 2 S Densities S O Beta distribution O S Gaussian distribution derived Appendices AF These model nonuniform distributions distributions corresponding low high activity For m suﬃciently large central limit theorem2 means quantities approximately normal cid7 cid6 cid7 cid6 EO S N μO ESS N μS σ 2 O m σ 2 S m If standard deviations small case instance m large σ approx imated linear function slope t corresponding small range In case NWGMO S σ ESS approximately normal cid6 cid7 103 104 NWGMO S N σ μS t2σ 2 S m Note t cid2 λ4 σ cid4 λσ 1 σ Very σ μS μO This particularly true μO 05 Away 05 bias appearfor instance know O 05 thenNWGM Ebut bias relatively small This conﬁrmed simulations shown Fig 91 Gaussian uniform distributions generate values O Finally note variance EO S NWGMO S order behave like C1m C2m respectively m Furthermore σ 2 O C1 C2 σ 2 O small If necessary possible derive better general estimates EO assumption S Gaussian approximating logistic function cumulative distribution Gaussian described Appendix F 41 If sample neurons activities come distribution sample mean sample NWGM normally distributed roughly mean The difference approximately zero mean To difference approximately normal need E NWGM uncorrelated 2 Note weights P identical equal 1m However central limit theorem applied nonuniform case long weights deviate uniform distribution P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 95 Fig 91 Histogram NWGM values random sample 100 values O taken 1 uniform distribution 0 1 upper left 2 uniform distribution 0 05 lower left 3 normal distribution mean 05 standard deviation 01 upper right 4 normal distribution mean 025 standard deviation 005 lower right All probability weights equal 1100 Each sampling experiment repeated 5000 times build histogram 94 Correlation mean normalized weighted geometric mean We cid16 Var cid17 EO S NWGMO S Var cid16 cid17 EO S cid16 cid17 NWGMO S cid16 2 Cov cid17 EO S NWGMO S Var 105 Thus estimate variance difference estimate covariance EO S NWGMO S As shall covariance close null In section assume samples size m distribution O mean E μO variance V σ 2 O To simplify notation use ES V S NWGMS denote random variables corresponding mean variance normalized weighted geometric mean sample We seen Taylor expansion 05 NWGMS ES V S 1 2V S We ﬁrst consider case E NWGM 05 In case covariance NWGMS ES estimated CovNWGMS ES E cid12cid6 ES V S 1 2V S 1 2 cid7cid6 ES 1 2 cid7cid13 cid12 E cid13 E 1 2 2 1 2V S 106 We 05 cid2 1 2V S cid2 1 EES 1 2 2 VarES V m Thus short covariance order V m goes 0 sample size m goes inﬁnity For Pearson correlation denominator product similar standard deviations scales like V m Thus correlation roughly constant close 1 More generally mean E equal 05 approximations cid12 cid12cid6 cid7 cid13 ES V S 1 2V S E V 1 2V cid13 ES E E E ES 2 V V S ES E 1 2V S 1 2V CovNWGMS ES E 107 96 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 92 Behavior Pearson correlation coeﬃcient left covariance right empirical expectation E empirical NWGM function number samples sample distribution For number samples sampling procedure repeated 10000 times estimate Pearson correlation covariance The distributions uniform distribution 0 1 uniform distribution 0 05 normal distribution mean 05 standard deviation 01 normal distribution mean 025 standard deviation 005 And leading term order V m Similar results obtained expansions 0 1 given Appendices AF model populations neurons low high activity Thus covariance NWGM E goes 0 Pearson correlation constant close 1 These results conﬁrmed simulations Fig 92 Combining previous results VarES NWGMS VarES VarNWGMS C1 m C2 m 108 Thus general EO S NWGMO S random variables 1 similar identical means 2 variances covariance decrease 0 inversely sample size 3 approximately normal distributions Thus E NWGM approximately normally distributed zero The NWGM behaves like random variable small ﬂuctuations mean Of course contrived examples constructed instance small m small networks deviate general behavior 95 Dropout approximations cancellation effects To complete analysis dropout approximation EO l EO l general error term cid8l αl βl cid8l W l 0 Furthermore error cid8l W l induction layers small approximately normally distributed mean EO l First property true l 1 W 1 uncorrelated error αl NWGMO 1 results previous sections apply immediately l 1 NWGMO l case For induction step assume property true layer l At following layer cid6cid2 cid2 W l1 σ wl1h j j W h ph j σ hcid2l j hcid2l j cid7 cid6cid2 cid2 wl1h j ph j cid11 cid16 cid10 E O h j cid8h j cid17 cid7 Using ﬁrst order Taylor expansion W l1 NWGM compactly W l1 NWGM cid10 cid10 cid11 O l1 σ cid4 cid6cid2 cid2 wl1h j ph j E cid7cid12 cid11 cid10 O h j cid2 cid2 wl1h j j cid8h ph j cid13 hcid2l j O l1 cid11 cid10 cid10 E σ cid4 Sl1 cid12cid2 cid11cid11 cid2 hcid2l j hcid2l j cid13 wl1h j j cid8h ph j βl1 NWGM cid10 cid11 O l1 W l1 σ cid4 cid10 cid10 E Sl1 cid12cid2 cid11cid11 cid2 cid13 wl1h j j cid8h ph j hcid2l j 109 110 111 112 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 As sum linear small terms βl1 cid11 cid10 βl1 E 0 approximately normally distributed By linearity expectation By linearity variance respect sums independent random variables cid2 cid10 cid2 cid10 cid10 cid10 cid11 cid10 cid10 cid11 cid11 Var βl1 cid16 σ cid4 cid11cid11cid17 2 E Sl1 wl1h j 2 ph j Var cid8h j cid11 2 97 113 114 j hcid2l This variance small σ cid4ESl1 2 cid2 116 standard logistic function smaller 116 j small induction The weights wl1h j 2 cid2 1 Varcid8h end learning ph small beginning learning shall Section 11 dropout performs weight regularization automatically While observed simulations concern large layers sum large We leave detailed study issue future work Finally need αl1 uncorrelated Since terms approximately mean 0 compute mean product βl1 j cid12 cid10 βl1 αl1 cid11 E E cid11 cid10 cid10 E O l1 NWGM cid10 O l1 cid11cid11 cid10 cid10 E σ cid4 Sl1 cid11cid11cid2 cid2 hcid2l j wl1h j j cid8h ph j cid13 By linearity expectation cid10 βl1 αl1 σ cid4 E E cid11 cid10 cid10 Sl1 cid11cid11cid2 cid2 wl1h j ph j E cid16cid10 cid10 E cid11 O l1 NWGM cid10 cid11cid11 cid17 cid8h j O l1 0 115 116 EEO l1 NWGMO l1 hcid2l j EEO l1 cid8h j NGWMO l NWGMO l1 j 0 viewed good approximations EO l Ecid8h In summary general W l small deviations approximately Gaussians mean zero small standard deviations These deviations act like noise cancel extent preventing accumulation errors layers These results previous section conﬁrmed simulation results given Figs 93 94 95 96 97 The simulations based training deep neural network classiﬁer MNIST handwritten characters dataset layers size 784120012001200120010 replicating results described 27 p 08 input layer p 05 hidden layers The raster plots accumulate results obtained 10 randomly selected input vectors For ﬁxed weights ﬁxed input vector 10000 Monte Carlo simulations sample dropout subnetworks estimate distribution activities O neuron layer These simulations use weights obtained end learning cases beginning end learning compared Figs 96 97 In general results NWGMO l layer beginning end learning deviations roughly viewed small approximately Gaussian ﬂuctuations bounds derived Section 8 approximate true expectation EO l deterministic values W l 96 Dropout approximations estimation variances covariances We seen deterministic values W s provide simple effective estimates values EO s entire network dropout Perhaps surprisingly W s derive approximations variances covariances units follows First dropout variance neuron use cid10 cid10 cid11 cid10 cid11 O l O l W l equivalently Var O l W l 1 W l cid11 O l O l W l W l equivalently Var cid11 cid10 O l 0 E cid10 E cid11 117 118 These approximations viewed respectively rough upperbounds lower bounds variance For neurons activities close 0 1 general neurons end learning bounds similar This case beginning learning small weights standard logistic 0 Figs 98 99 At beginning end learning variances transfer function W l small 0 better approximation However learning variances expected larger closer approximate upper bound W 1 W Figs 910 911 05 VarO l For covariances different neurons use cid10 E cid11 cid10 cid11 cid10 E O l O h j cid11 E O l O h j W l W h j 119 This independence approximation accurate neurons truly independent pairs neurons ﬁrst layer However expected remain approximately true pairs neurons loosely coupled 98 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 93 Each row corresponds scatter plot neurons hidden layers deep classiﬁer trained MNIST dataset text learning Scatter plots derived cumulating results 10 random chosen inputs Dropout expectations estimated 10000 dropout samples The second order approximation left column blue dots correspond E NWGM V 1 2E1 2V Eq 87 Bound 1 variancedependent bound given E1 E1 2E1 2V Eq 87 Bound 2 varianceindependent bound given E1 E1 2E1 2E1 E Eq 87 In right column W represent neuron activations deterministic ensemble network weights scaled appropriately corresponding propagated NWGMs For interpretation references color ﬁgure legend reader referred web version article pairs neurons large neural networks times learning This conﬁrmed simulations Fig 912 conducted network trained MNIST dataset The approximation better simply 0 Fig 913 For neurons directly connected approximation holds try improve j feeding directly neuron output introducing slight correction Consider case neuron output O h O l j By isolating contribution O h h l weight wlh cid6cid2 cid2 O l σ w f l kcid3 j lf ikδ f k O f k wlh j δh j O h j σ cid7 cid6cid2 j cid7 lf ikδ f k O f k σ cid4 cid2 w f l kcid3 j cid6cid2 cid2 w f l kcid3 j cid7 f k lf ikδ f k O wlh j δh j O h j 120 ﬁrst order Taylor approximation accurate wlh satisﬁed beginning learning sparse coding In expansion ﬁrst term independent O h j small conditions particularly j j O h P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 99 Fig 94 Similar Fig 93 sharper potentially restricted second order approximation NWGM obtained Taylor expansion mean Appendix B Eq 202 expectation easily computed cid6 cid7cid7 cid6 cid6cid2 cid2 E σ w f l kcid3 j lf ikδ f k O f k σ E cid6cid2 cid2 w f l kcid3 j lf ikδ f k O cid6cid2 cid2 cid7cid7 f k σ f l kcid3 j cid7 w lf ik p f k W f k W lh j 121 Thus W lh simply deterministic activation neuron layer l ensemble network neuron j layer j h removed inputs Thus easily computed forward propagation deterministic network Using ﬁrstorder Taylor expansion estimated W lh j W l σ cid4 cid11 cid10 U l In case wlh j ph j W h j cid11 cid10 E O l O h j W lh j W h j cid6cid2 cid2 cid6 σ cid4 E w f l kcid3 j cid7cid7 f k wlh j ph j E cid10 lf ikδ f k O O h j O h j cid11 122 123 100 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 95 Similar Figs 93 94 Approximation 1 corresponds second order Taylor approximation 05 E NWGM V 1 2E1 2V 105V E1E Appendix B Eq 202 Eq 87 Approximation 2 sharper restrictive second order Taylor approximation E EV 2E Histograms approximations interleaved ﬁgure right column Towards end learning σ cid4 0 second term neglected A slightly precise estimate obtained writing σ cid4 λσ σ close 0 σ cid4 λ1 σ σ close 1 replacing corresponding expectation W lh j In case leading term approximation j 1 W lh cid11 cid10 E O l O h j W lh j W h j 124 The accuracy formula pairs connected neurons demonstrated Fig 914 beginning end learning compared approximation EO l j The correction provides small improvement end learning beginning This neglects term σ cid4 presumably close 0 end learning The improvement small purposes simpler approximation W l j cases connected unconnected j W l W h W h O h P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 101 Fig 96 Empirical distribution NWGM E approximately Gaussian layer training This performed Monte Carlo simulations dropout subnetworks 10000 samples 10 ﬁxed inputs After training distribution slightly asymmetric activation neurons asymmetric The distribution layer training particularly tight simply input network MNIST data relatively sparse 10 The duality spiking neurons backpropagation 101 Spiking neurons There longstanding debate importance spikes biological neurons artiﬁcial neural networks particular precise timing spikes carry information In biological systems examples instance visual motor systems information carried short term average ﬁring rate neurons exact timing spikes However experiments shown cases timing spikes highly reproducible known examples timing spikes crucial instance auditory location systems bats barn owls brain regions detect small interaural differences considerably smaller 1 ms 261918 However relatively rare specialized cases On engineering question course having spiking neurons helpful learning 102 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 97 Empirical distribution W E approximately Gaussian layer training This performed Monte Carlo simulations dropout subnetworks 10000 samples 10 ﬁxed inputs After training distribution slightly asymmetric activation neurons asymmetric The distribution layer training particularly tight simply input network MNIST data relatively sparse purposes precise timing spikes matters There connection dropout spiking neurons shed moment faint light questions A sigmoidal neuron output O σ S converted stochastic spiking neuron letting neuron ﬂip coin produce spike probability O Thus network spiking neurons neuron computes random variables input sum S spiking probability O stochastic output cid10 Fig 101 Two spiking mechanisms considered 1 global neuron spikes sends quantity r outgoing connections 2 local connectionspeciﬁc neuron spikes respect speciﬁc connection sends quantity r connection In case different coin ﬂipped connection Intuitively ﬁrst case corresponds dropout units second case droupout connections When spike produced corresponding unit dropped ﬁrst case corresponding connection dropped second case P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 103 Fig 98 Approximation EO l variance neurons MNIST classiﬁer network training Histograms obtained taking noninput neurons aggregating results 10 random input vectors corresponding respectively estimates W l W l W l W l O l 1 W l Fig 99 Histogram difference dropout variance O l MNIST classiﬁer network training Histograms obtained taking noninput neurons aggregating results 10 random input vectors Note beginning learning random small weights EO l approximate upperbound W l 05 VarO l W l 0 W l 1 W l 025 W l Fig 910 Temporal evolution dropout variance V O training averaged hidden units 104 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 911 Temporal evolution difference W 1 W V training averaged hidden units Fig 912 Approximation EO l j pairs noninput neurons directly connected MNIST classiﬁer network training Histograms obtained taking 100000 pairs unconnected neurons uniformly random aggregating results 10 random input vectors j W l W h O h Fig 913 Comparison EO l training As shown previous ﬁgure W l neurons uniformly random aggregating results 10 random input vectors j 0 pairs noninput neurons directly connected MNIST classiﬁer network j provides better approximation Histograms obtained taking 100000 pairs unconnected W h O h P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 105 Fig 914 Approximation EO l j pairs connected noninput neurons directed connection j MNIST classiﬁer network training Histograms obtained taking 100000 pairs connected neurons uniformly random aggregating results 10 random input vectors j W l j W l W lh W h O h Fig 915 Histogram difference Eσ cid4S σ cid4ES Eq 220 noninput neurons MNIST classiﬁer network training Histograms obtained taking noninput neurons aggregating results 10 random input vectors The nodes ﬁrst hidden layer 784 sparse inputs nodes upper hidden layers 1200 nonsparse inputs The distribution initial weights slightly different ﬁrst hidden layer The differences ﬁrst hidden layer hidden layers responsible initial bimodal distribution 106 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Fig 101 A spiking neuron formally operates 3 steps computing ﬁrst linear sum S probability O σ S stochastic output cid10 size r probability O 0 To precise multilayer network described following equations First spiking unit cid18 cid10h probability O h rh 0 global ﬁring case cid18 cid10h ji ji probability O h rh 0 connectionspeciﬁc case Here allow size spikes vary neurons connections spikes ﬁxedsize easy special case While spike sizes principle greater connection dropout requires spike sizes size The spiking probability computed usual form O h σ cid11 cid10 Sh sum term given cid2 cid2 Sh whl j cid10l j lh j global ﬁring case cid2 cid2 Sh whl j cid10l j 127 128 129 lh j connectionspeciﬁc case The equations applied layers including output layer input layer layers consist spiking neurons Obviously nonspiking neurons input output layers combined spiking neurons network In formalism issue exact timing spike addressed However information coin ﬂips given order deﬁne behavior network Two common models assume complete asynchrony assume synchrony layer As spikes propagate network average output Ecid10 spiking neuron spiking conﬁgurations equal r times size average ﬁring probability EO As seen average ﬁring probability approximated NWGM possible inputs S leading following recursive equations cid10 cid11 cid11 rh E O h global ﬁring case cid11 cid11 cid10 rh ji E O h cid10 cid10h E cid10 cid10h ji E connectionspeciﬁc case Then cid10 cid11 cid11 cid10 cid10 cid10 E O h NWGM O h σ E cid11cid11 Sh cid2 cid2 cid11 cid10 E Sh cid11 cid10 cid10l j whl j E cid2 cid2 j rl whl j E cid11 cid10 O l j lh j lh j 125 126 130 131 132 133 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 107 Fig 102 Three closely related networks The ﬁrst network operates stochastically consists spiking neurons neuron sends spike size r probability O The second network operates stochastically consists logistic dropout neurons neurons sends activation O dropout probability r The connection weights ﬁrst second networks identical The network operates deterministic way consists logistic neurons Its weights equal weights second network multiplied corresponding probability r global ﬁring case cid11 cid10 E Sh cid2 cid2 cid11 cid10 cid10l j whl j E cid2 cid2 j rl whl j E cid11 cid10 O l j 134 lh j lh j connectionspeciﬁc case In short expectation stochastic outputs stochastic neurons feedforward stochastic network approxi mated dropoutlike deterministic feedforward propagation proceeding input layer output layer multiplying weight whl j acts dropout probability parameterof corresponding pre synaptic neuron Operating neuron stochastic mode equivalent setting inputs 1 dropout connections different Bernoulli probabilities associated sigmoidal outputs previous layer j corresponding spike size rl j rl In particular shows given feedforward network spiking neurons spikes size 1 ap proximate average ﬁring rate neuron simply deterministic forward propagation corresponding identical network sigmoidal neurons The quality approximation determined quality approxima tions expectations NWGMs More generally consider feedforward networks Fig 102 identical topology identical weights The ﬁrst network stochastic weights whl j consists spiking neurons neuron activity O h 0 similar argument connectionspeciﬁc spikes size rh ji Thus network neuron layer h sends signal instantaneous mean variance given sends spike size rh probability O h E rh O h Var cid10 cid11 2 cid10 rh O h 1 O h cid11 ﬁxed O h shortterm mean variance given cid10 rh Var 1 E O h O h O h E cid11cid10 E cid10 cid10 cid11 cid11 cid10 2 cid11cid11 E rh 135 136 averaged spiking conﬁgurations ﬁxed input The second network stochastic identical weights ﬁrst network consists dropout sigmoidal probability rh 0 similar argument ji Thus neuron layer h sends signal instantaneous neurons neuron activity O h sends value O h connectionspeciﬁc dropout probability rh expectation variance given E rh O h Var cid10 cid11 2 cid10 rh O h 1 rh cid11 ﬁxed O h shortterm expectation variance given cid11 cid10 cid11 cid10 cid11 cid11 cid10 E rh E O h Var r Var O h E O h 2 rh cid10 1 rh 137 138 averaged dropout conﬁgurations ﬁxed input The network deterministic consists logistic units Its weights identical previous networks rescaled form whl j Then remarkably feedforward deterministic propagation j network approximate average output neurons ﬁrst network possible spiking conﬁgurations average output neurons second network possible dropout conﬁgurations In rl 108 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 particular shows stochastic neurons forward pass neural network sigmoidal units similar dropout Note ﬁrst second network different details In particular variances signals sent variance greater dropout cid3 05 variance greater O h neuron following layer equal O h network When rh typical case sparse encoding rh spiking network This corresponds Poisson regime relatively rare spikes Whenr h O h rh In summary simple deterministic feedforward propagation allows estimate average ﬁring rates stochastic asynchronous networks need knowing exact timing ﬁring events Stochastic neurons instead dropout learning Whether stochastic neurons preferable dropout instance differences variance described requires investigations There aspect connection dropout stochastic neurons backpropagation 102 Backpropagation backpercolation Another important observation backward propagation backpropagation algorithm viewed closely related dropout Starting errors output layer backpropagation uses orderly alternat ing sequence multiplications transpose forward weight matrices derivatives activation functions Thus backpropagation essentially form linear propagation reverse linear network combined multiplication derivatives activation functions node formally looks like recursion Eq 24 If derivatives 0 1 interpreted probabilities In case logistic activation functions σ cid4x λσ x1 σ x σ cid4x cid2 1 value x λ cid2 4 Thus backpropagation computing dropout ensemble average reverse linear network dropout probability p node given derivative corresponding activation This suggests possibility dropout stochastic spikes addition Gaussian noise backward pass dropout stochastic spikes addition Gaussian noise forward pass different amounts coordination forward backward pass dropout Using dropout backward pass faced problem vanishing gradients units activities close 0 1 derivatives close 0 lead rare sampling However imagine instance layers 1000 units fully connected derivatives equal 01 Standard backpropagation produces error signal 6 time ﬁrst layer reached Using dropout backpropagation instead selects contains factor 10 average 100 units layer propagates signal attenuation Thus strong error signal propagated narrow channel backpercolation Backpropagation thought special case backpercolation small learning rate backpercolation essentially identical backpropa gation backpropagation corresponds ensemble average backpercolation passes This approach course slow lot time spent sampling compute average signal provided pass backpropagation However shows exact gradients necessary backpropagation tolerate noise alleviating concerns biological plausibility backpropagation Furthermore aside speed issue noise backward pass help avoiding certain local minima Finally note sev eral variations ideas possible backpercolation ﬁxed value p p 05 backpropagation layers followed backpercolation lower layers vice versa Detailed investigation issues scope paper left future work 11 Dropout dynamics So far concentrated static properties dropout properties dropout ﬁxed set weights In section look dynamic properties dropout related training procedure evolution weights 111 Dropout convergence With properly decreasing learning rates dropout sure converge small neighborhood local minimum global minimum case strictly convex error function way similar stochastic gradient descent standard neural networks 381314 This viewed form online gradient descent respect error function Error E TENS cid2 cid2 I N P N f w cid10 cid11 O N tI P N f w cid10 cid11 O N tI cid2 IN 139 true ensemble tI target value input I f w elementary error function typically squared error regression relative entropy error classiﬁcation depends weights w In case dropout probability P N network N factorial associated product underlying Bernoulli selector variables P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 109 Thus dropout online respect input examples I networks N alternatively form new set training examples examples formed taking cartesian product set original examples set possible subnetworks In section dropout performing form stochastic gradient descent respect regularized ensemble error Finally write gradient error E TENS wlh j cid2 cid2 I N δh j 1 P N f w whl j cid2 cid2 I N δh j 1 P N f w Sl O h j N I 140 If backpropagated error vary mean network reasonable large network replace mean similarly activity O h j Thus gradient true ensemble approximated product expected backpropagated error postsynaptic terms expected presynaptic activity cid6 cid7 cid6 cid7 j W h ph j 141 E TENS wlh j E f w Sl cid10 cid11 ph j E O h j E f w Sl 112 Dropout gradient adaptive regularization single linear unit As static properties instructive ﬁrst consider simplest case single linear unit In case single linear unit trained dropout input I output O S target t error typically quadratic form Error 1 2 t O 2 Let consider error functions E ENS E D associated ensemble possible subnetworks network dropout In linear case ensemble network identical deterministic network obtained scaling connections dropout probabilities For single input I error functions deﬁned cid4 E ENS 1 2 t O ENS2 1 2 t cid52 ncid2 i1 pi w Ii cid4 E D 1 2 t O D 2 1 2 t cid52 ncid2 i1 δi w Ii 142 143 Here δi Bernoulli selector random variables P δi 1 pi E D random variable E ENS deterministic function We use single training input I notational simplicity errors training example combined additively The learning gradients form E w t O O w yielding E O O w E ENS w t O ENSpi Ii E D w t O D δi Ii tδi Ii w iδ2 I 2 cid2 jcid3i w jδiδ j Ii I j 144 145 The vector random vector variable expectation Assuming usual random variables δi s pairwise independent cid6 cid7 E E D w cid10 cid11 t EO D δi 1 pi Ii tpi Ii w pi I 2 cid2 jcid3i w pi p j Ii I j t O ENSpi Ii w I 2 pi1 pi yields cid6 E cid7 E ENS w w I 2 Var δi E ENS w w Varδi Ii E D w Thus general dropout gradient aligned ensemble gradient Remarkably expectation gradient dropout gradient regularized ensemble error E E ENS 1 2 ncid2 i1 w 2 I 2 Var δi 148 146 147 110 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 The regularization term usual weight decay Gaussian prior term based square weights ensuring weights large overﬁt data Dropout provides immediately magnitude regularization term adaptively scaled square input terms variance dropout variables Note pi 05 value provides highest level regularization regularization term depends inputs target outputs Furthermore expected dropout gradient online respect regularization term term training example Obviously result holds entire layer linear units The regularization effect dropout case generalized linear models discussed 43 derive regularizers 113 Dropout gradient adaptive regularization deep linear networks Similar calculations deep linear networks For instance previous calculation adapted imme diately layer linear network T layers cid10 ti O T cid11 j O l δl j E D w T jl cid7 cid6 E E D w T l j E ENS w T l j w T l j Var cid11 cid10 j O l δl j 149 150 corresponds adaptive quadratic regularization term w T l j coeﬃcient associated input j O l corresponding variance dropout presynaptic neuron Varδl j To study gradient weight w network let assume loss generality deep network single output unit Let denote activity S dropout network U deterministic ensemble network Since network linear given input output linear function w S α w β U ES Eαw Eβ 151 The output obtained summing contributions provided possible paths inputs output Here α β random variables α corresponds sum contributions associated paths input layer output layer contain edge associated w β corresponds sum contributions associated paths input layer output layer contain edge associated w Thus gradients given E D w t S S w α w β tα E ENS w t U cid10 U w Eαw Eβ t cid11 Eα The expectation dropout gradient given cid7 cid6 E E D w α w β tα E cid11 cid10 α2 w Eαβ t Eα This yields remarkable expression cid7 cid6 E E D w E ENS w w Varα Covα β 152 153 154 155 Thus expectation dropout gradient gradient ensemble plus adaptive regularization term components The component wVarα corresponds weight decay quadratic regularization term error function The adaptive coeﬃcient Varα measures dropout variance contribution ﬁnal output associated inputtooutput paths contain w The component Covα β measures dropout covariance contribution associated paths contain w contribution associated paths contain w In general covariance small equal zero single layer linear network Both α β depend training inputs target outputs P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 111 114 Dropout gradient adaptive regularization single sigmoidal unit For single sigmoidal unit similar identical holds With sigmoidal unit O σ S 11 λS typically uses relative entropy error ce cid11 cid10 t log O 1 t log1 O E 156 We consider error functions E ENS E D Note linear case E ENS exactly equal ensemble error nonlinear case use E ENS denote error deterministic network approximates ensemble network By chain rule E w E O O S S w E O t 1 O 1 t 1 1 O O S λO 1 O Thus ﬁnally grouping terms E w λt O S w 157 158 Thus overall form derivative similar linear case multiplication positive factor λ ﬁxed However outputs nonlinear complicates comparison derivatives We use O σ S dropout network W σ U deterministic ensemble approximation For ensemble network λt W pi Ii λ cid11 cid10 t σ U E ENS w pi Ii λ t σ cid6 cid6cid2 cid7cid7 For dropout network E D w λt O δi Ii λ t σ cid6 cid7cid7 w jδ j I j δi Ii cid6cid2 j Taking expectation gradient gives cid6 cid7 cid6 cid7cid13cid7 λ t E w jδ j I jδi 1 pi Ii w j p j I j pi Ii j 159 160 161 Using NWGM approximation expectation allows expectation inside sigmoidal function cid7cid7 cid6 cid6 cid7 λ t σ w j p j I j w pi Ii w Ii pi Ii λ U Ii w i1 pi pi Ii 162 cid10 cid10 t σ cid11cid11 The logistic function continuously differentiable ﬁrstorder Taylor expansion U cid6 cid7 λ cid10 t σ SENS σ cid4 cid11 SENSIi w i1 pi pi Ii σ cid4x σ x1 σ x denotes derivative σ So ﬁnally obtain result similar linear case E D w E ENS w λσ cid4 U w I 2 Varδi E ENS w λσ cid4 U w Varδi Ii The dropout gradient aligned ensemble approximation gradient Remarkably simple approximations expectation gradient dropout gradient regularized ensemble error E E ENS 1 2 λσ cid4 U ncid2 i1 w 2 I 2 Varδi 165 The regularization term usual weight decay Gaussian prior term based square weights ensuring weights large overﬁt data Dropout provides immediately magnitude regularization term adaptively scaled square input terms gain λ sigmoidal function variance dropout variables instantaneous derivative sigmoidal function This derivative bounded approaches zero SENS small large Thus regularization maximal beginning learning decreases learning progresses Note pi 05 value provides highest level regularization Furthermore expected dropout gradient online respect 163 164 E D w E D w E D w E E E E cid6 cid7 cid6cid2 cid12 σ j cid6cid2 j 112 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 regularization term term training example Note regularization term depends inputs target outputs A similar analysis identical results carried set normalized exponential units entire layer sigmoidal units A similar result derived similar way suitable transfer functions instance rectiﬁed linear functions expressing integrals logistic functions ensure differentiability 115 Dropout gradient adaptive regularization deep neural networks In deep neural networks logistic transfer functions nodes basic idea remains In fact ﬁxed set weights ﬁxed input linearize network weight w Eq 155 applies instantaneously To derive speciﬁc approximations consider deep dropout network described O h σ h cid11 cid10 Sh σ cid6cid2 cid2 cid7 lh j whl j δl j O l j O 0 j I j 166 layers ranging h 0 inputs h T output layer selector random variables δl corresponding approximation ensemble network described j The W h σ h cid11 cid10 U h σ cid6cid2 cid2 cid7 whl j pl j W l j W 0 j I j 167 lh j new set U W distinct variables avoid confusion In principle node use different logistic function different c λ parameters simplify notation assume logistic function neurons Then gradient ensemble network computed E ENS whl j E ENS U h U h whl j backpropagated error computed recursively E ENS U h cid2 cid2 lh k E ENS U l k wlh ki ph σ cid4 cid11 cid10 U h initial values network λ cid10 ti W T cid11 E ENS U T 168 169 170 Here ti ith component target vector example consideration In addition presynaptic term U h whl j pl j W l j Likewise dropout network E D whl j E D Sh Sh whl j E D Sh cid2 cid2 lh k E D Sl k wlh kiδh σ cid4 cid11 cid10 Sh initial values network λ cid11 cid10 ti O T E D S T presynaptic term Sh whl j δl j O l j 171 172 173 174 175 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 113 Consider unit output layer T receiving connection unit j layer l typically l T 1 weight w T l j The gradient error function dropout network given λ cid11 cid10 ti O T j O l δl j λ cid10 cid10 ti σ S T E D w T l j cid11cid11 j O l δl j λ cid10 cid10 ti σ S T l j w T l j δl j O l j cid11cid11 j O l δl j 176 notation Section 95 S T l j terms gives Sl w T l j δl j O l j Using ﬁrst order Taylor expansion separate independent λ cid10 cid10 ti σ S T l j cid11 σ cid4 cid11 cid10 S T l j E D w T l j w T l j δl j O l j cid11 j O l δl j We expectation gradient cid7 cid6 E E D w T l j λ cid10 ti E cid10 σ cid10 S T l j cid11cid11cid11 j W l pl j λE cid11cid11 cid10 cid10 σ cid4 S T l j cid10 w T l j pl j E cid11 O l j O l j j σ U T l j σ ES T l cid10 cid11 cid10 σ cid4 O l pl j λE S T l j j O l j E cid10 j j W T l cid10 cid11 j σ cid4 cid11 U T w T l W T σ cid4U T j pl j W l j w T l cid11 j W l pl j pl j W l j Now NWGM approximation Eσ S T l cid7 cid6 E E D w T l j λ cid10 ti W T cid11 j W l pl j form cid6 cid7 E E D w T l j E ENS w T l j w T l j A 177 178 179 180 A complex expression given Eq 179 Thus expectation dropout gradient layer approximately gradient ensemble network regularized quadratic weight decay adaptive coeﬃcient Towards end learning sigmoidal functions saturated derivatives close 0 A 0 Using dropout approximation EO l j W l j Eσ cid4S T l j σ cid4U T Fig 915 Eq 220 produces compact approximation cid7 cid6 E E D w T l j λ cid10 ti W T cid11 j W l pl j w T l j λσ cid4 cid10 U T cid11 cid10 Var j O l δl j cid11 181 similar single layercase showing dropout tends minimize variance Varδl j A approximated A σ cid4U T mation Section 95 EO l j W l write expected gradient product postsynaptic backpropagated error presynaptic expectation j Also approxi j In case j O l j1 pl j W l pl j W l j O l cid7 cid6 E E D w T l j cid10 λ cid10 ti W T cid11 λw T l j σ cid4 cid11cid10 cid10 U T 1 pl j W l j cid11cid11 j W l pl j 182 With approximations similar results appear true deeper layers To ﬁrst approximation j immediate pre postsynaptic assume backpropagated error independent product σ cid4Sh terms δl j P l cid6 E E D wh jl cid7 cid6cid2 cid2 E lh k cid7 E D Sl k wlh kiδh cid10 E σ cid4 δh cid11 cid10 Sh j O l δl j cid11 cid2 cid2 E lh k cid6 E D Sl k cid7 wlh ki ph E cid11 cid10 cid10 σ cid4 δh Sh j O l δl j cid11 183 cid10 This approximation reasonable increasingly accurate units closer input layer presence activity units bears vanishingly inﬂuence output error As case layer use ﬁrstorder Taylor approximation separate dependent terms Eq 183 Eδh j approximately equal cid10 δh Shl j We approximate Eσ cid4Shl σ cid4cid4U h j W l j whl cid10 σ cid4 σ cid4U h σ cid4cid4U h cid11 cid10 cid11cid11 cid10 j O l δl j δ j O l j σ cid4U hl j use similar Taylor expansion reverse Eσ cid4Shl j EO l j whl pl cid11cid16 cid10 σ cid4 j cid11 cid10 j σ cid4U h σ cid4Sh σ cid4cid4 σ cid4cid4 cid10 σ cid4 cid16 σ cid4 ph ph ph pl 184 185 δl j O l pl pl σ cid4cid4 j O l whl whl Shl j Shl j Shl j O l j j E O l j E j E cid11cid11 cid11cid11 cid11cid17 E E E cid10 cid10 cid10 cid11 cid10 cid11 cid11 cid10 cid10 cid10 cid11 cid11 cid11 cid10 cid17 j j j j U h j whl pl j E O l j pl j E O l j pl ph j E U h Shl j O l j 114 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 Collecting terms ﬁnally gives cid10 cid11 cid11 cid10 σ cid4 δh E Sh j O l δl j ph pl j cid10 cid16 σ cid4 U h cid11 cid10 E O l j cid11 σ cid4cid4 cid11 cid10 U h j whl pl j E cid10 cid11 cid11 cid10 E O l j O l j cid10 cid10 σ cid4cid4 E Shl j cid11cid11 whl j E cid10 O l j O l j extracting variance term cid10 cid11 cid11 cid10 cid10 σ cid4 δh E Sh j O l δl j ph pl j E O l j cid11 cid10 cid11 σ cid4 U h ph σ cid4cid4 cid11 cid10 U h whl j Var cid11 cid10 j O l δl j Combining result Eq 183 gives cid7 cid6 E E D whl j E ENS whl j whl j A cid11cid17 186 187 188 A adaptive coeﬃcient proportional σ cid4cid4U h Varδl j Note obvious A positive requirement form weight decayespecially σ cid4cid4x negative x 05 case standard sigmoid Further analyses simulations issues underlying approximations left future work j O l In conclusion approximations suggest gradient dropout approximation ensemble E ENS whl j expecta tion gradient E E D whl j dropout network similar The difference approximately weight decay term linear whl j complex adaptive coeﬃcient varies learning depends variance presynaptic unit input Thus dropout built regularization effect keeps weights small Furthermore regularization tends dropout variance unit small This form selfconsistency small variances ensure higher accuracy dropout ensemble approximations Furthermore dropout variance unit minimized inputs 0 dropout builtin propensity sparse representations 116 Dropin It instructive think apparently symmetric algorithm dropin units randomly inde pendently set 1 0 dropout Although superﬁcially symmetric dropout simulations dropin behaves differently fact work The reason understood terms previous analyses setting units 1 tends maximize variances minimizing 117 Learning phases sparse coding Finally light results expect roughly phases dropout learning 1 At beginning learning weights random small total input unit close 0 units consistency high output units remains roughly constant subnetworks equal 05 logistic coeﬃcient c 10 2 As learning progresses sizes weights increase activities tend 0 1 consistencies decreases given input dropout variance units subnetworks increases units 1 units 0 However overall regularization effect dropout keeps weights variances small To variances small sparse representations tend emerge 3 As learning converges consistency units stabilizes given input variance units subnetworks roughly constant small units converged 1 small units converged 0 This consequence convergence stochastic gradient σ Sh cid9 For simplicity let assume dropout carried layer h units output form O h Sh cid11 cid10 j constant dropout applied layer l Thus j For ﬁxed input O l cid11 j whl j δl cid10 cid11 lh cid2 cid10 j O l cid11 cid9 cid10 Var Sh whl j 2 O l j 2 pl j 1 pl j 189 lh usual assumption selector variables δl j independent A similar expression obtained dropout applied way connections Thus VarSh ultimately inﬂuences consistency unit layer h depends factors Everything equal reduced 1 Small weights goes regularizing effect dropout random initial condition 2 Small activities dropout symmetric respect small large activities failure dropin Overall dropout tends favor small activities sparse coding 3 Small close 0 large close 1 values dropout probabilities pl j The sparsity learning phases dropout demonstrated simulations Figs 111 112 113 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 115 Fig 111 Empirical distribution ﬁnal neuron activations layer trained MNIST classiﬁer demonstrating sparsity The empirical distribu tions combined 1000 different input examples Fig 112 The phases learning For particular input typical active neuron red starts low dropout variance experiences increase variance learning eventually settles steady constant consistency value A typical inactive neuron blue quickly learns stay silent Its dropout variance grows minimally low initial value Curves correspond mean activation 5 95 percentiles This single ﬁxed input 1000 dropout Monte Carlo simulations For interpretation references color ﬁgure legend reader referred web version article Fig 113 Consistency active neurons noticeably decline upper layers Active neurons deﬁned activation greater 01 end training There 100 active neurons layer For neurons 1000 dropout simulations performed time step 100 training epochs The plot represents dropout mean standard deviation 5 95 percentiles computed active neurons layer Note standard deviation increase higher layers 116 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 12 Conclusion We developed general framework enabled understanding aspects dropout good mathematical precision Dropout eﬃcient approximation training possible submodels given architecture taking average While theoretical questions static dynamic properties dropout require investigations instance generalization properties existing framework clariﬁes ensemble averaging properties dropout regularization properties In particular shows standard approaches regularizing large models avoiding overﬁtting 1 ensemble averaging 2 adding noise 3 adding regularization terms equivalent Bayesian priors error functions present dropout viewed uniﬁed manner Dropout wants produce robust units depend details activation individual units As result seeks produce units activities small dropout variance dropout subnetworks This partial variance minimization achieved keeping weights small sparse encoding turn increases accuracy dropout approximation degree selfconsistency Thus sense small weights sparse coding dropout leads large energy eﬃcient networks potentially biological relevance known carbonbased computing orders magnitude eﬃcient siliconbased computing It worth consider classes models linear nonlinear feedforward networks beneﬁt dropout Some form dropout ought work instance Boltzmann machines Hopﬁeld networks Further dropout successfully applied reallife problems remain tested Among problem predicting quantitative phenotypic traits height genetic data single nucleotide polymorphisms SNPs worth mentioning While genomic data growing rapidly complex traits illposed regime typically number loci genetic variation occurs exceeds number training examples Thus best current models typically highly L1 regularized linear models limited success With strong regularization properties dropout promising algorithm applied questions simple linear logistic regression models complex models potential capturing epistatic interactions Finally ﬁrst sight dropout like clever hack More careful analysis reveals underlying web elegant mathematical properties This mathematical structure unlikely result chance leads suspect dropout clever hack time important concept AI machine learning Acknowledgements Work supported grants NSF IIS0513376 NSFIIS1321053 NIH LM010235 NIH NLM T15 LM07443 We wish acknowledge hardware grant NVIDIA We thank Julian Yarkony feedback manuscript Appendix A Rectiﬁed linear transfer function Gaussian assumption Here consider rectiﬁed linear transfer function RE threshold 0 slope λ If assume S uniformly distributed interval similar considerations hold intervals symmetric μS 0 σS a3 We RLES 0 ERLS cid22 0 λx12a dx λa4 In case cid20 cid10 cid20RL cid11 ES E cid10 RLS cid11cid20 cid20 λa 4 190 This difference small standard deviation small small proportional λ Gaussian case Alternatively consider m input dropout values S 1 Sm probabilities P 1 P m We cid9 P S cid2 0 P S 0 cid18 cid10 RL cid11 ES cid10 cid11 RLS E λ cid9 P S cid9 0 λ cid2 P S iS i0 Thus cid20 cid10 cid20RL cid11 ES E cid10 cid11cid20 cid20 RLS cid23 cid9 cid9 λ λ iS i0 P S iS icid20 P iS In usual case P 1m yields cid10 cid9 cid23 cid20 cid10 cid20RL cid11 ES E RLS cid11cid20 cid20 λ 1 m λ 1 m cid9 iS 0 S iS cid20 S cid9 cid9 P S cid2 0 P S 0 cid9 cid9 S cid2 0 S 0 191 192 193 194 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 117 Again differences proportional λ easy small standard deviation small instance Tchebycheffs inequality Appendix B Expansion mean zero B1 Expansion mean Using notation Section 8 consider outputs O 1 O m sigmoidal neuron associated P 1 O σ S The difference expand mean write cid9 probabilities P 1 P m O Ecid8i As result cid14 cid14 G O P E cid6 cid7 P 1 cid8i E 195 196 197 198 199 200 201 cid4 G cid14 1 O iP 1 E cid7 P cid6 cid14 1 cid8i 1 E In order use Binomial expansion assume cid8i minE 1 E In case P iP 1 2 cid13 R3cid8i 1 P G E E P n cid8i E cid8i E cid8i E cid2 cid7 n cid7cid6 cid14 cid14 cid7 cid6 cid6 cid12 2 n0 R3cid8i remainder order Expanding collecting terms gives cid12 G E 1 cid2 P cid8i E cid2 P iP 1 2 cid7 2 cid6 cid8i E cid2 icid3 j P P j cid8i E cid8 j E cid13 R3cid8 cid9 Noting cid12 G E P icid8i 0 ﬁnally E V 1 V E 2 2E cid13 R3cid8 similarly symmetry cid4 1 E G V 21 E As result G G cid4 1 1 2 V E1 E V E1E bining results yields cid2 1 measure distribution deviates binomial case mean Com NWGM G G Gcid4 E V 2E V E1E 1 1 2 202 In general approximation slightly accurate approximation obtained Section 8 expanding 05 Eq 87 shown Figs 94 95 range validity slightly narrower B2 Expansion zero Consider expansion O 1 cid8i G requires cid8i 1 satisﬁed O We cid15 i1 cid8iP G cid4 cid15 icid8iP The binomial expansion G cid14 cid2 cid6 n0 cid7 P n 1ncid8n cid12 cid14 1 P icid8i P iP 1 2 cid13 R3cid8i cid82 R3cid8i remainder order Expanding collecting terms gives G E 1 2 V R3cid8 E 1 2 V 203 204 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 118 G cid4 1 E 1 2 V As result G G cid4 1 V Thus NWGM G G Gcid4 2E V 2 2V cid6 E NWGM cid7 V 1 V E 1 2 This yields approximate bounds E NWGM cid4 1 2 V 1 V cid2 1 2 E1 E 1 E1 E cid2 1 6 E NWGM cid4 cid20 cid20 cid20E 1 cid20 2 cid20 cid20 cid20 cid20 E1 E 1 E1 E cid2 1 2 E1 E 1 E1 E cid2 1 6 205 206 207 208 209 210 Over interval 0 1 function f E E1E 1E1E E 1 reaches maximum E 05 f 05 1 G yields cid4 positive concave It satisﬁes f E 0 E 0 3 Expansion 0 similar interchanging role G NWGM 1 E 05V 1 V similar bounds E NWGM derived Appendix C Higher order moments 211 It useful better estimates variance V potentially higher order moments We seen 0 cid2 V cid2 E1 E cid2 025 212 Since V EO 2 EO 2 EO 2 E 2 like estimate EO 2 generally EO k tempting use NWGM approach know general theory EO k NWGMO k This leads cid10 cid11 O k cid15 NWGM cid15 iO k P cid15 i1 O k P iO k P cid15 1 1 cid10 1O 1O O k1 O k cid11 P For k 2 gives cid10 cid11 E O 2 NWGM cid10 cid11 O 2 cid15 cid10 1 1O O 1 1O O cid11 P 1 ceλES 1 cid15 i2 ceλS P 213 214 However calculate exactly approximately term denominator More equiv alently use general fact NWGMσ f S σ E f S leads particular cid11cid11 cid10 cid10 σ Sk cid11cid11 cid10 cid10 E Sk σ NWGM By inverting sigmoidal function S 1 λ c O 1 O expanded E 05 log1 u letting O 05 cid8 gives 215 216 cid9 n11n1unn u 1 Expanding 05 S 1 λ log c 1 λ cid3 cid2 n0 2cid82n1 2n 1 2 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 cid8 1 λ log c 4 λ cid8 119 217 approximation obtained retaining second order terms expansion Thus approximation cid6 cid11 cid10 E S 2 E 1 λ log c 4 λ cid8 cid7 2 cid6 E 1 λ log c 4 λ cid7 2 O 05 218 We estimate E EO provided NWGMO Thus estimate ES 2 obtained directly NWGMσ S 2 inverting Eq 215 leads estimate O 2 Eq 218 estimate variance V And similarly higher order moments However cases additional costly information required order estimates V sharper Eq 212 directly sample values O Appendix D Derivatives logistic function expectations For σ x 11 ce λx2 λx ﬁrst order derivative given σ cid4x λσ x1 σ x λce second order derivative σ cid4cid4x λσ x1 σ x1 2σ x As expected λ 0 maximum σ cid4x reached σ x 05 equal λ4 As usual let O σ S 1 m corresponding probabilities P 1 P m To approximate Eσ cid4S λx1 ce apply deﬁnition derivative cid10 σ cid4 cid11 S E E cid6 lim h0 σ S h σ S h cid7 lim h0 Eσ S h Eσ S h lim h0 σ ES h σ ES h 219 NWGM approximation expectation Note NWGM approximation requires 0 cid2 σ cid4S cid2 1 satisﬁed λ cid2 4 Using ﬁrst order Taylor expansion ﬁnally cid10 σ cid4 cid11 S E lim h0 σ cid4ESh h σ cid4 cid10 cid11 ES To derive approximation Eσ cid4S cid10 σ cid4 cid11 S E NWGM cid10 cid11 S σ cid4 1 cid15 1 1σ cid4S P σ cid4S iP cid15 cid10 1 λc eλS 2 λ 1 1 1 c λ eλS cid11 P 220 221 As applications assume c λ 1 slightly simplify calculations odd terms Taylor expansion exponential functions denominator cancel In case cid10 σ cid4 cid11 S E NWGM cid10 cid11 S σ cid4 1 cid15 cid10 3 1 cid9 n1 cid11 P 1 3 cid15 cid10 1 1 cid9 n1 cid11 P 2λS i2n 32n 2λS i2n 2n 222 Now different approximations derived truncating denominator For instance retaining term cor responding n 1 sum 1 xα 1 αx x small ﬁnally approximation cid10 σ cid4 cid11 S E 1 4 λ2 ES 2 1 4 λ2VarS ES2 Appendix E Distributions 223 Here look distribution O S O σ S simple assumptions E1 Assuming S Gaussian distribution Under probabilistic assumptions natural assume incoming sum S neuron Gaussian distribution mean μ variance σ 2 density fS s 1 2π σ sμ2 2σ 2 e 224 120 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 In case distribution O given F O o P O cid2 o P S cid2 cid6 1 λ log 1 o cid7 λ log 1o 1 cid19 0 sμ2 e 2σ 2 ds 1 2π σ yields density f O o 1 2π σ λ log 1o 1 2σ 2 e μ2 1 λ 1 o1 o 225 226 In general density bellshaped similar identical beta density For instance μ 0 λ c 1 σ f O o 1 2π 1 o 1 1 2 log 1o o o 1 1 2 log 1o o 227 E2 The mean variance S Consider sum form S mean μO variance σ 2 approximately Gaussian central limit theorem cid9 n i1 w O Assume weights mean μw variance σ 2 w activities O weights activities independent Then n large S ES nμwμO VarS nVarw O n cid16 cid10 E cid11 cid11 cid10 E O 2 w 2 Ew i2 EO i2 cid16cid10 cid17 n σ 2 w μ2 w cid11cid10 σ 2 O μ2 O cid11 μ2 wmu2 O cid17 In typical case μw 0 variance reduces VarS n cid10 cid16 σ 2 w σ 2 O μ2 O cid11cid17 E3 Assuming O Beta distribution 228 229 230 The variable O 0 1 natural assume Beta distribution parameters cid3 0 b cid3 0 density f O o Ba boa11 ob1 normalizing constant Ba b Γ bΓ aΓ b In case distribution S given F S s P S cid2 s P cid10 cid11 O cid2 σ s σ scid19 Ba boa11 ob1 0 yields density f S s Ba bσ sa1 cid10 cid10 cid11 b1λσ s 1 σ s cid11 1 σ s λBa bσ sa cid10 cid11 b 1 σ s 231 232 233 In general density bellshaped similar identical Gaussian density For instance balanced case b f S s λBa bσ sa cid10 cid11 λBa b 1 σ s cid6 cid7 ce λs 1 ceλs2 234 Note instance density decays exponentially like e quadratic exact Gaussian case λas linear term exponent P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 121 Appendix F Alternative estimate expectation Here alternative way obtaining closed form estimate EO O σ S S Gaussian S reasonable assumption case dropout applied large distribution mean μS variance σ 2 networks It known logistic function approximated cumulative Gaussian distribution form 1 1 eS Φ01α S 235 cid22 1 x t22 dt suitable value α Depending optimization criterion different rea Φμσ 2 x sonably close values α literature α 0607 21 α 11702 0587 15 Just equating ﬁrst derivatives functions S 0 gives α 2π 4 0626 In follows use α 0607 In case general logistic case 2π σ e 1 1 ceλS Φ01 cid10 cid11 αλS log c As result general case cid19 EO Φ01 cid10 cid11 αλS log c SμS 2 2σ 2 s e dS 1 2πσS It easy check cid6 cid7 μ σ Φμσ 2 0 Φ01 Thus cid19 EO P Z 0S SμS 2 2σ 2 s e 1 2πσS dS P Z 0 236 237 238 239 Z S normally distributed mean λS log c variance 1α2 Thus Z normally distributed mean λμs log c variance σ 2 S α2 expectation estimated EO P Z 0 Φλμslog cσ 2 S α2 0 Φ01 cid7 cid6 λμS log c α2 σ 2 S Finally reverse logistic approximation cumulative Gaussian distribution cid6 EO Φ01 cid7 λμS log c α2 σ 2 S 1 e In usual case c λ 1 gives 1 1 α λμS cid24 σ 2 S log c α2 1 e 1 λμS cid24 log c S α2 1σ 2 EO 1 S α212μS 1σ 2 1 10368σ 2 S 12μS 1 e 1 e 240 241 242 α 0607 approximation In cases approximation EO accurate NWGMS approximation tradeoff This approximation requires normal assumption S knowing mean variance S NWGM approximation uses mean S form EO NWGMO σ ES For small values σ 2 S estimate Eq 242 converges 05 NWGM arbitrarily close 0 1 depending values ESμS In practice observed size weights remains limited dropout regularization effect variance S bounded S approximations similar For large values σ 2 Note nonGaussian distributions artiﬁcial cases constructed discrepancy E NWGM larger goes way 1 For example large discrepancy S 1cid8 probability 1 cid8 S 1cid83 probability cid8 cid8 close 0 In case EO 0 NWGM 1 122 P Baldi P Sadowski Artiﬁcial Intelligence 210 2014 78122 References 1 J Aldaz Self improvement inequality arithmetic geometric means J Math Inequal 3 2 2009 213216 2 J Aldaz Sharp bounds difference arithmetic geometric means arXiv preprint arXiv12034454 2012 3 N Alon JH Spencer The Probabilistic Method John Wiley Sons 2004 4 H Alzer A new reﬁnement arithmetic mean geometric mean inequality J Math 27 3 1997 5 H Alzer Some inequalities arithmetic geometric means Proc R Soc Edinb Sect A Math 129 02 1999 221228 6 G An The effects adding noise backpropagation training generalization performance Neural Comput 8 3 1996 643674 7 J Ba B Frey Adaptive dropout training deep neural networks C Burges L Bottou M Welling Z Ghahramani K Weinberger Eds Advances Neural Information Processing Systems vol 26 2013 pp 30843092 8 P Baldi K Hornik Neural networks principal component analysis Learning examples local minima Neural Netw 2 1 1988 5358 9 P Baldi K Hornik Learning linear networks survey IEEE Trans Neural Netw 6 4 1994 837858 10 P Baldi PJ Sadowski Understanding dropout Advances Neural Information Processing Systems vol 26 2013 pp 28142822 11 EF Beckenbach R Bellman Inequalities SpringerVerlag Berlin 1965 12 CM Bishop Training noise equivalent Tikhonov regularization Neural Comput 7 1 1995 108116 13 L Bottou Online algorithms stochastic approximations D Saad Ed Online Learning Neural Networks Cambridge University Press Cambridge UK 1998 14 L Bottou Stochastic learning O Bousquet U von Luxburg Eds Advanced Lectures Machine Learning Lecture Notes Artiﬁcial Intelligence vol 3176 Springer Verlag Berlin 2004 pp 146168 15 SR Bowling MT Khasawneh S Kaewkuekool BR Cho A logistic approximation cumulative normal distribution J Ind Eng Manag 2 1 2009 114127 16 S Boyd L Vandenberghe Convex Optimization Cambridge University Press 2004 17 L Breiman Bagging predictors Mach Learn 24 2 1996 123140 18 C Carr M Konishi A circuit detection interaural time differences brain stem barn owl J Neurosci 10 10 1990 32273246 19 CE Carr M Konishi Axonal delay lines time measurement owls brainstem Proc Natl Acad Sci 85 21 1988 83118315 20 D Cartwright M Field A reﬁnement arithmetic meangeometric mean inequality Proc Am Math Soc 1978 3638 21 DDR Cox The Analysis Binary Data vol 32 CRC Press 1989 22 P Diaconis Bayesian numerical analysis Statistical Decision Theory Related Topics IV vol 1 1988 pp 163175 23 RO Duda PE Hart DG Stork Pattern Classiﬁcation second ed Wiley New York NY 2000 24 D Gardner Noise modulation synaptic weights biological neural network Neural Netw 2 1 1989 6976 25 SJ Hanson A stochastic version delta rule Physica D 42 1 1990 265272 26 G Harnischfeger G Neuweiler P Schlegel Interaural time intensity coding superior olivary complex inferior colliculus echolocating bat molossus ater J Neurophysiol 53 1 1985 89109 27 G Hinton N Srivastava A Krizhevsky I Sutskever RR Salakhutdinov Improving neural networks preventing coadaptation feature detectors httparxivorgabs12070580 2012 28 N Levinson Generalization inequality Ky Fan J Math Anal Appl 8 1 1964 133134 29 L Maaten M Chen S Tyree KQ Weinberger Learning marginalized corrupted features Proceedings 30th International Conference Machine Learning ICML13 2013 pp 410418 30 K Matsuoka Noise injection inputs backpropagation learning IEEE Trans Syst Man Cybern 22 3 1992 436440 31 AM Mercer Improved upper lower bounds difference angn J Math 31 2 2001 32 PR Mercer Reﬁned arithmetic geometric harmonic mean inequalities J Math 33 4 2003 33 M Mitzenmacher E Upfal Probability Computing Randomized Algorithms Probabilistic Analysis Cambridge University Press 2005 34 AF Murray PJ Edwards Enhanced mlp performance fault tolerance resulting synaptic weight noise training IEEE Trans Neural Netw 5 5 1994 792802 35 E Neuman J Sándor On Ky Fan inequality related inequalities Math Inequal Appl 5 2002 4956 36 E Neuman J Sandor On Ky Fan inequality related inequalities ii Bull Aust Math Soc 72 1 2005 87108 37 Y Raviv N Intrator Bootstrapping noise An effective regularization technique Connect Sci 8 34 1996 355372 38 H Robbins D Siegmund A convergence theorem non negative supermartingales applications Optimizing Methods Statistics 1971 pp 233257 39 RT Rockafellar Convex Analysis vol 28 Princeton University Press 1997 40 D Rumelhart G Hintont R Williams Learning representations backpropagating errors Nature 323 6088 1986 533536 41 DJ Spiegelhalter SL Lauritzen Sequential updating conditional probabilities directed graphical structures Networks 20 5 1990 579605 42 P Vincent H Larochelle Y Bengio P Manzagol Extracting composing robust features denoising autoencoders Proceedings 25th International Conference Machine Learning ACM 2008 pp 10961103 43 S Wager S Wang P Liang Dropout training adaptive regularization C Burges L Bottou M Welling Z Ghahramani K Weinberger Eds Advances Neural Information Processing Systems vol 26 2013 pp 351359