Artiﬁcial Intelligence 170 2006 925952 wwwelseviercomlocateartint Generating evaluating evaluative arguments Giuseppe Carenini Johanna D Moore b Computer Science Department University British Columbia 2366 Main Mall Vancouver BC Canada V6T 1Z4 b Human Communication Research Centre University Edinburgh 2 Buccleuch Place Edinburgh United Kingdom EH8 9LW Received 21 June 2005 received revised form 15 May 2006 accepted 23 May 2006 Abstract Evaluative arguments pervasive natural human communication In countless situations people attempt advise persuade interlocutors desirable vs undesirable right vs wrong With proliferation online systems serving personal advisors assistants pressing need develop general testable computational models generating presenting evaluative arguments Previous research generating evaluative arguments characterized major limitations First researchers tended focus speciﬁc aspects generation process Second proposed approaches empirically tested The research presented paper addresses limitations We designed implemented complete computational model generating evaluative arguments For content selection organization devised argumentation strategy based guidelines argumentation theory For expressing content natural language extended integrated previous work computational linguistics generating evaluative arguments The key knowledge source tasks quantitative model user preferences To empirically test critical aspects generation model devised implemented evaluation framework effectiveness evaluative arguments measured real users Within framework performed experiment test basic hypotheses design computational model based proposal tailoring evaluative argument addressees preferences increases effectiveness differences conciseness signiﬁcantly inﬂuence argument effectiveness The second hypothesis conﬁrmed experiment In contrast ﬁrst hypothesis marginally conﬁrmed However independent testing researchers recently provided support hypothesis 2006 Elsevier BV All rights reserved Keywords Natural language generation User tailoring Preferences Empirical evaluation 1 Introduction Evaluative arguments pervasive natural human communication In countless situations people attempt advise persuade interlocutors desirable vs undesirable right vs wrong For instance doctors need advise patients treatment best A teacher need convince student certain course best choice student And salespeople need compare similar prod ucts explaining products current customers liking With Corresponding author Email address giuseppecareninigmailcom G Carenini 00043702 matter 2006 Elsevier BV All rights reserved doi101016jartint200605003 926 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 explosion information available online everincreasing availability wireless devices witnessing proliferation systems serving personal assistants advisors 962 aim support replace humans similar communicative settings The success systems crucially depend ability generate present effective evaluative arguments In 1990s considerable research devoted developing computational models automatically generating presenting evaluative arguments Several studies investigated process selecting structuring content argument 7313547 23 developed detailed model selected content realized natural language Despite abundance prior work topic previous research characterized major limitations First complexity generating natural language researchers tended focus speciﬁc aspects generation process Second lack systematic evaluation difﬁcult gauge effectiveness scalability robustness proposed approaches The research presented paper addresses limitations By following principles argumentation ory computational linguistics developed complete computational model generating evaluative arguments In model aspects generation process covered principled way selecting ganizing content argument expressing selected content natural language For content selection organization devised argumentation strategy based guidelines argumentation theory For expressing content natural language extended integrated previous work generating evaluative arguments The key knowledge source tasks quantitative model user preferences To empirically test critical aspects generation model devised implemented evaluation framework effectiveness evalu ative arguments measured real users The design evaluation framework based principles techniques research ﬁelds including computational linguistics social psychology decision theory human interaction Within framework performed experiment test basic hypotheses design computational model based tailoring evaluative argument model addressees preferences increases effectiveness differences conciseness signiﬁcantly inﬂuence ar gument effectiveness The ﬁrst hypothesis marginally conﬁrmed experiment 005 p 010 second conﬁrmed p 005 Moreover recent work 62 direct extension research provided independent empirical support ﬁrst hypothesis In section focus problem generating evaluative arguments tailored model users preferences design development Generator Evaluative Argument GEA In Sec tion 2 evaluation framework First justify design evaluation framework reviewing literature persuasion social psychology previous work evaluating natural language generation techniques Next introduce motivate user task core framework In particular illustrate context task effectiveness argument assessed measuring effects users behaviors beliefs attitudes Section 3 describes experiment ran evaluation framework Section 4 discuss related work generating evaluating evaluative arguments 2 Generating evaluative arguments The generation evaluative arguments extensively investigated past Yet computational models developed previous work cover subparts generation process For instance 35 provided sophisticated approach content selection 23 mainly limited content realization Furthermore earlier models informed argumentation theory 42 theory rooted rhetoric providing guidelines effective arguments generated In section present GEA ﬁrst computational model covers aspects generating evaluative arguments principled way effectively integrating general principles techniques argumentation theory computational linguistics GEA complex computational model In section design development topdown fashion First illustrate GEA specializes pipeline architecture typically adopted Natural Language Generation NLG systems introduce basic algorithms knowledge structures Then discuss set guidelines argumentation theory effective argumentation strategy based After introduce quantitative model GEA represent users preferences argumentation strategy tailors content structure evaluative argument model The G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 927 Fig 1 The GEA architecture specialization generic NLG pipeline architecture section concludes detailed description GEA realizes content selected argumentation strategy natural language 21 The architecture Generator Evaluative Arguments GEA Text generation involves fundamental tasks process selects organizes content text deep generation process expresses selected content natural language surface generation GEA like previous work NLG makes assumption deep generation strictly precede surface generation adopts resulting pipeline architecture 50 In architecture center Fig 1 text planner selects organizes content domain model applying communicative strategy achieve set communicative goals given input The output text planning text plan data structure spec iﬁes rhetorical structure text propositions text convey partial order propositions Then text MicroPlanner packages selected content sentences selects words syntactic structures effectively express content Finally Sentence Realizer runs output MicroPlanner computational grammar English produces English text Notice text planing microplan ning content structure phrasing text tailored model communicative context user model Fig 1 shows GEA specializes standard pipeline architecture generic NLG GEA speciﬁc features shown grey boxes ﬁgure italics following text The input GEA ab stract evaluative communicative goal expressing user attitude entity domain house realestate domain increased positive negative direction posi tivenegative meaning user likedislike entity Given abstract communicative goal Longbow text planner 65 selects arranges content argument applying set communicative strategies implement argumentation strategy based guidelines content selection organization argumentation theory 42 Two knowledge sources involved process goal action decomposition Fig 1 A domain model representing entities relationships speciﬁc domain ii An additive multiattribute value function AMVF decisiontheoretic model users preferences 141 1 Currently GEA component dialogue sensitive dialogue history 928 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Next text plan passed GEA microplanner performs aggregation lexicalization generates referring expressions Aggregation packaging semantic information sentences performed according standard techniques 50 For lexicalization selection lexical items express desired meaning GEA microplanner selects words express evaluations applying decision tree extends previous work realizing evaluative statements 22 Decisions cue phrases express discourse relationships text segments implemented decision tree based features suggested literature 36 The generation referring expressions GEA straightforward entity referred proper noun pronoun For pronominalization deciding use pronoun refer entity simple strategy based centering theory 27 applied Finally output text microplanning uniﬁed GEA sentence realizer FUF Systemic Uniﬁcation Realization Grammar English SURGE 24 We key challenges developing GEA design argumentation strategy development model users preferences design microplanner 22 An argumentation strategy based user preferences 221 Guidelines argumentation theory An argumentation strategy speciﬁes content included argument arranged This comprises decisions represents supporting opposing evidence main claim position main claim argument supporting opposing evidence include order order supporting opposing evidence respect Argumentation theory developed guidelines specifying decisions effectively 16424445 details 41 alternative discussion guidelines In section guidelines In Section 223 provide computational versions guidelines What represents supporting opposing evidence claim determine strength Guidelines decision vary depending argument type Limiting analysis evaluative arguments argumentation theory indicates supporting opposing evidence strength determined according model readers values preferences For instance risk involved game strong evidence claim reader like game reader likes risky situations lot b Positioning main claim Claims presented usually sake clarity Placing claim early helps readers follow line reasoning However delaying claim end argument effective particularly readers likely ﬁnd claim objectionable emotionally shattering c Selecting supporting opposing evidence Often argument mention available evidence usually sake brevity Only strong evidence presented weak evidence brieﬂy mentioned omitted entirely d Arrangingordering supporting evidence Typically strongest support presented ﬁrst order provisional agreement reader early If possible effective piece supporting evidence saved end argument order leave reader ﬁnal impression arguments strength This guideline proposed 42 compromise climax anticlimax approaches discussed 43 e Addressing ordering counterarguments opposing evidence There options decision mention counterarguments acknowledge directly refuting acknowledge directly refuting Weak counterarguments omitted Stronger counterarguments brieﬂy acknowledged shows awareness issues complexity reasonable broadminded attitude A counterargument acknowledged need refuted reader agrees sub stantially different position Finally counterarguments ordered minimize effectiveness strong ones placed middle weak ones upfront end f Ordering supporting opposing evidence A preferred ordering supporting opposing evidence appears depend reader aware opposing evidence If preferred ordering opposing supporting reverse G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 929 Fig 2 Sample AMVF preference model UserA left Information sample House233 right Although guidelines provide information types content include evaluative argument arrange content design computational argumentative strategy requires concepts mentioned guidelines formalized coherent computational framework In particular require model readers values preferences allow identify supporting opposing evidence guideline operationally deﬁne term objectionable claim guideline b measure discrepancy readers initial position arguments main claim2 measure strength supporting opposing evidence guidelines c d e represent reader aware certain facts guideline f We model section 222 Modeling user preferences MAUT AMVFs One model satisﬁes requirements noted additive multiattribute value function AMVF based multiattribute utility theory MAUT 14 MAUT widely decision theory originally developed common choice ﬁeld artiﬁcial intelligence intelligent user interfaces 23139 Models similar AMVF proven useful psychology particular study consumer behavior 56 A critical aspect AMVFs elicited people reliable efﬁcient way 121 As suggests multiattribute utility models based notion valued valued multiple reasons 34 An AMVF model individuals values preferences respect entities given class To build AMVF particular domain identify attributes contribute users overall assessment entities determine relative importance attribute particular users More formally AMVF consists value tree set component value functions A value tree decom position entitys value hierarchy aspects entity called objectives decision theory leaves value tree correspond primitive objectives For example Fig 2 left shows value tree real estate domain value house combination values location amenities quality The value amenities broken values primitive objectives gardensize porchsize decksize Location quality broken primitive objectives A component value function primitive objective expresses preferability value objective number interval 0 1 preferable value mapped 1 preferable value 03 For instance Fig 2 value modern 2 An operational deﬁnition emotionally shattering outside scope work 3 For illustration component value functions shown text boxes Fig 2 930 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 primitive objective architecturalstyle preferred UserA distancefrompark 1 mile preferability 1 132 1 069 The arcs value tree weighted represent valuable decision maker worst best level objective respect siblings For instance Fig 2 UserA consider moving deco house modern house slightly valuable moving house view houses house view river cf weight Architecturalstyle 015 weight Viewobject 012 Formally AMVF predicts value ve entity e follows ve vx1 xn ncid2 i1 wivixi x1 xn vector primitive objective values entity e primitive objective vi component value function wi weight 0 cid2 wi cid2 1 cid3 n i1 wi 1 wi equal product weights path root value tree primitive objective A function voe deﬁned objective When applied entity function returns value entity respect objective For instance assuming value tree shown Fig 2 vQualitye cid5 cid4 wViewQuality vViewQualitye cid5 cid4 wViewObject vViewObjecte cid5 cid4 wArchitecturalStyle vArchitecturalStylee cid5 cid4 wAppearanceQuality vAppearanceQualitye Thus given AMVF particular user possible compute valuable entity individual Furthermore possible compute valuable objective aspect entity person All values expressed number interval 0 1 In general uncertainty present users valuation entity represented linear combination preferences primitive objectives AMVF cases preferences satisfy condition additive independence That objective assumed independent However standard heuristic tests users shown additive models good approximation peoples preferences conditions certainty 21 Thus AMVFs safely situations uncertainty uncertain situations additive independence veriﬁed Edwards Barron 21 shown AMVFs elicited people reliable efﬁcient way They devised SMARTER simple procedure eliciting objective weights component value functions users Objective weights userspeciﬁc reﬂecting individual preferences tradeoffs entities domain To elicit weights SMARTER asks user perform series easy assessments N 1 N objectives userspeciﬁc ranking objectives generated From ranking weights computed according following formula speciﬁes weight kth objective wk 1K Kcid2 1i ik There considerable experimental evidence indicating simple attribute ranking efﬁcient nearly accurate traditional timeconsuming methods weights directly elicited users Traditional methods require k N 1 assessments N objectives k possibly large 14 Moreover resulting efﬁciency gain appear penalize accuracy Simulation studies shown SMARTER introduces 2 utility loss 1 The elicitation component value functions SMARTER simpliﬁed follows If function speciﬁes preferability continuous objective gardensize user needs choose essentially basic possibilities function increases linearly value range decreases linearly G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 931 value range increases linearly subinterval decreases linearly complement If function speciﬁes preferability discrete objective architecturalstyle acquired manner similar way weights elicited having user rank possible values SMARTER makes simplifying assumptions Nevertheless remarkably effective Ref 12 shows models developed SMARTER return evaluations correlate highly experts holistic judgements Pear sons coefﬁcient 068 p 0001 223 Operationalizing argumentation strategy Presenting evaluative argument attempt persuade reader value judgement applies entity The value judgement called argumentative intent positive favor subject negative subject4 The subject single entity Ulysses good book difference entities Vancouver somewhat better Seattle form comparison entities set Vancouver best city North America We use information AMVF operationalize guidelines presented Section 221 Guideline Given users AMVF straightforward establish represents supporting opposing evidence argument given argumentative intent given subject If argumentative intent positive objectives subject positive value supporting evidence objectives subject negative value opposing evidence opposite holds argumentative intent negative The value different subjects reasonably measured follows If subject single entity e value subject objective o voe positive greater 05 midpoint 0 1 negative In contrast subject comparison entities ve1 ve2 value subject objective o voe1 voe2 positive greater 0 negative Guideline b Since argumentative intent value judgement reasonably assume instead simply positive negative speciﬁed precisely number interval 0 1 speciﬁcation normalized value interval Then term objectionable claim operationally deﬁned If introduce measure discrepancy MD absolute value difference argumentative intent readers expected value subject argument presented based AMVF claim objectionable reader MD moves 0 1 Guidelines c d e The strength evidence support opposition main argument claim critical selecting organizing argument content To deﬁne measure strength support opposition adopt extend previous work explaining decision theoretic advice based AMVF Klein 35 presents explanation strategies based argumentation theory justify preference alternative pair In strategies compellingness objective measures objectives strength determining overall value difference alternatives things equal And objective notablycompelling worth mentioning outlier population objectives respect compellingness The formal deﬁnitions cid6 cid6 compellingnesso a1 a2 wo root cid7 voa1 voa2 cid8cid6 cid6 o objective a1 a2 alternatives wo root product weights links path o root objective value tree vo component value function leaf objectives attributes recursive evaluation childreno nonleaf objectives wo ocid4 weight link ocid4 o cid2 voa ocid4childreno cid4 wo o vocid4a notablycompellingo opop a1 a2 compellingnesso a1 a2 μx kδx 4 Arguments neutral However paper discuss arguments neutral argumentative intent 932 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Fig 3 Sample population objectives represented dots ordered compellingness o a1 a2 deﬁned previous deﬁnition opop objective population siblingso opop 2 X x compellingnessp a1 a2 p opop μx mean X δx standard deviation We adopted compellingness measure strength supporting opposing evidence We adopted notablycompelling decision criterion including piece evidence argument Notice deﬁnition notablycompelling relies constant k determines lower bound compellingness objective included argument So setting constant k different values possible control principled manner number objectives pieces evidence included argument controlling degree conciseness generated arguments As shown Fig 3 k 0 objectives compellingness greater average compellingness population included argument 4 sample population For higher positive values k fewer objectives included 2 k 1 opposite happens negative values 8 objectives included k 1 The concepts compellingness notablycompelling deﬁned support arguments entity valuable We deﬁned similar measures arguing value single entity termed scompellingness snotablycompelling An objective scompelling strength weakness contributing value alternative So m1 measures value objective contributes overall value difference alternative worst possible case5 m2 measures value objective contributes overall value difference alternative best possible case deﬁne scompellingness greatest quantities m1 m2 Following terminology introduced previous equations cid7 scompellingnesso wo root max voa cid7 1 voa cid8cid8 We snotablycompelling deﬁnition analogous notablycompelling snotablycompellingo opop scompellingnesso μx kδx In snotablycompelling constant k plays role notablycompelling setting constant k different values possible control degree conciseness generated arguments Guideline f An AMVF represent reader aware certain facts We assume information represented 224 The argumentation strategy We applied guidelines argumentation theory corresponding formal deﬁnitions described previous section develop argumentative strategy shown Fig 4 The steps strategy marked 5 aworst alternative o voaworst 0 abest alternative o voabest 1 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 933 Arguesubject Root ArgInt k A assignments If subject singleentity e SVoi voi e Else If subject e1 e2 SVoi Measureofstrength scompellingness Worthmention snotablycompelling voi e1 voi e2 Measureofstrength compellingness Worthmention notablycompelling B content selection Eliminate objectives oi Worthmention oi siblingsoi subject Root AllEvidence childrenRoot AllInFavor oo AllEvidence SVo ArgInt SecondBestObjInFavor second compelling objective oo AllInFavor RemainingObjectivesInFavor AllInFavor SecondBestObjInFavor ContrastingObjectives AllEvidence AllInFavor guidelinec guidelinea guidelinea C ordering constraints AddOrderingRoot AllEvidence assume MD 0 claim objectionable If AwareUser ContrastingObjectives guidelineb guidelinef AddOrderingContrastingObjectives AllInFavor Else AddOrderingContrastingObjectives cid10 AllInFavor AddOrderingRemainingObjectivesInFavor SecondBestObjInFavor SortRemainingObjectivesInFavor decreasing order according Measureofstrength SortContrastingObjectives strong ones middle weak ones upfront end D steps expressing argue content ExpressValuesubject Root ArgInt For o ContrastingObjectives ExpressValuesubject o SVo For o AllInFavor If leafo Arguesubject o SVo k Else ExpressValuesubject o SV0 Legend b preceeds b ν1 ν2 ν1 ν2 positive negative values Section guideline means different subjects SVoi function computing subject value objective oi Fig 4 The argumentation strategy guidelined guidelined guidelinee guidelinee guideline based The strategy designed generating arguments present evaluation subject equivalent reader expected hold given model preferences argumentative intent equal expected value MD 06 We examine strategy introducing necessary terminology The strategy called Argue takes arguments ﬁrst line Fig 4 The subject entity entities evaluated compared single entity pair entities domain Root objective value tree evaluation overall value house location amenities ArgInt argumentative intent argument number 0 1 0 meaning worst 1 best The constant k deﬁnitions notablycompelling snotablycompelling determines degree conciseness argument The ExpressValue function end strategy indicates objective applied subject realized natural language certain argumentative intent In ﬁrst strategy A ﬁgure depending nature subject appropriate measure evidence strength assigned appropriate predicate determines piece evidence worth mentioning After B evidence worth mentioning assigned supporting opposing evidence comparing value argument intent In C ordering constraints argumentation 6 An alternative strategy generating arguments argumentative intent greater lower expected value deﬁned framework This strategy boost evaluation supporting evidence include weak counterarguments omit entirely opposite target value lower expected value 934 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Fig 5 Text plan segmentation structure corresponding argument generated GEA House233 tailored UserA k 03 UserAs model information House233 shown Fig 2 theory applied Notice assume predicate Aware true user aware certain fact false Finally D argument claim expressed natural language calling ExpressValue function realizes objective Root applied subject argument intent ArgInt Then opposing evidence ContrastingSubObjectives considered expressed natural language In contrast supporting evidence presented recursively calling strategy supporting piece evidence corresponding objective leaf value tree The argumentation strategy implemented library communicative action decompositions Longbow discourse planner 66 As shown Fig 1 application argumentation strategy produces text plan evaluative argument tailored given user For instance argumentation strategy applied preference model entity introduced Fig 2 subject House233 Root HouseValue UserA ArgIntent 06 k 03 text plan shown Fig 5a generated The leaves text plan express propositions argument convey cid12Assert Quality House233 UserA value 082cid137 Note subset objectives original AMVF included plan These snotablycompelling objectives selected B strategy The nodes text plan communicative actions ordered action Assertopposingprops performed Assertpropsinfavor These ordering relations established C strategy Fi nally text plan speciﬁes rhetorical structure argument This structure expressed hierarchical decomposition text plan rhetorical relations evidence concession elements hier archy The plan hierarchical decomposition generated D strategy recursive calls rhetorical relations determined B guideline 7 As AMVF values expressed 01 interval 1 corresponding best possible 0 worst possible G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 935 23 GEA microplanner The output argumentation strategy text plan indicating propositions include argument overall structure argument This text plan passed microplanner Fig 1 performs aggregation lexicalization referring expression generation To illustrate GEAs microplanner sample text plan Fig 5a realized corresponding argument shown Fig 5b8 A key aspect text plan microplanning speciﬁcation discourse segments Note node plan participates rhetorical relation corresponds discourse segment In Fig 5a discourse segment marked cid12segcid13 label corresponding portions text Fig 5b enclosed angle brackets Segments internally structured consist element directly expresses discourse purpose segment element head relation arrows ﬁgure number constituents supporting purpose9 We illustrate microplanning task Lexicalization proper discourse cue selection Lexicalization task selecting words associated syntactic structures express semantic information The GEA microplanner performs simple form lexical choice For proposition text plan chooses appropriate protophrase express proposition This decision based objective proposition value current user For example sample text plan Fig 5a proposition Location House233 06 mapped protophrase pronomi nalization realized reasonable location proposition Distanceshopping House233 084 mapped protophrase realized offers easy access shops Mapping protophrases implemented decision trees Fig 6top shows portion decision tree mapping objectives proto phrases Because linguistic theory indicating realize numeric intervals natural language based choice adjectives reasonable excellent estimates In practice lexicalization proper GEA implemented Functional Uniﬁcation Framework FUF ex tending previous work realizing evaluative statements 22 The decision tree partially shown Fig 6top represented FUF grammar selection instantiation template express proposition formed unifying proposition grammar Aggregation Aggregation task packaging semantic information sentences Three basic types ag gregation identiﬁed 50 In simple conjunction informational elements combined single sentence connective For instance informational elements realized independently a1 House B11 far shopping area a2 House B11 far public transporta tion combined realized single sentence a1 a2 In conjunction shared participants informational elements share argument positions ﬁlled content combined produce surface form shared content realized For instance informational elements aggregated simple conjunction combined conjunction shared participants House B11 far shopping area public transportation Finally syntactic embedding informational element realized separate major clause instead realized constituent embedded alized element For instance informational elements realized independently House B11 offers nice view House B11 offers view river combined realized House B11 offers nice view river GEA performs aggregation shared participants syntactic embedding To ensure argument coher ence aggregation attempted objectives related claim rhetorical relation type evidence concession This consistent heuristic proposed 55 addresses question aggregation desirable Do express type rhetorical relation single sen tence In example aggregation Location Neighborhood objectives related main claim relation type evidence Note Neighborhood related main claim 8 The text plan include objective Crime included argument The reason current implementation argumentation strategy processes objectives depth 3 AMVF The objective Crime reintroduced argument subsequent processing ad hoc fashion 9 The main element supporting elements given different names different discourse theories In paper main component relation core supporting elements contributors 936 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Fig 6 Portions decision trees microplanner lexical choice discourse cue selection chain evidence relations plan Fig 5a The propositions aggregated syntactic embedding Cue phrase generation Although substantial research effort devoted development computa tional theory discourse cue usage 1929363761 comprehensive theory lacking open questions remain Nevertheless considerable consensus ﬁeld factors inﬂuence usage discourse cues These include features characterize relationship intentional informational syntactic core contributor features segment structure core contributor appear features related embedding outside segment Lacking comprehensive theory developers NLG systems typically follow methodology devising genre specialized algorithm relies carefully selected subset features 50 In GEA cue phrase selection placement implemented decision tree taking account following features suggested literature tentional relationship core contributor b overall structure segment core contributor appear including position core contributors segment c relationship core contributor segment involved For example applying portion decision tree shown Fig 6bottom text plan example reader verify Even mark concession sample argument Pronominalization Most pronominalization algorithms NLG rely notion focus center sentence 50 GEA decides use pronoun refer evaluated entity applying simple strategy inspired centering 27 Centering theory indicates entity providing link previous discourse locally coherent discourse discourse segment preferentially realized pronoun repeated deﬁnite description10 Since GEA entity providing link previous discourse entity evaluated argument straightforward application centering imply discourse segment successive references entity realized pronouns beginning new segment deﬁnite description mark segment boundary For arguments generated GEA noticed centeringbased pronominalization policy works references segment restrictive references segment boundary In particular appears deﬁnite description beginning new segment cumbersome unnecessary segment boundary explicitly marked discourse 10 This thesis empirically veriﬁed 25 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 937 cue pronoun refer entity previous sentence So GEA realizes entity pronoun segment situation described Clearly application strategy requires information argument segmented ﬁnal text As described previously text plan expresses text segmentation core contributor rhetorical relation corresponds segment In Fig 5a discourse segment marked cid12segcid13 label The corresponding text Fig 5b contains ﬁve segment boundaries b1 b5 For illustration pronominalization strategy applied segment boundary b1 follows b1 explicitly marked discourse cue fact pronoun sentence preceding b1 refer House223 Thus pronoun refer entity following sentence 24 GEA Summary portability GEA implemented complete modular NLG generating user tailored evaluative argu ments We seen GEA covers aspects process generating evaluative arguments selecting organizing content argument expressing selected content natural language For content se lection organization GEA applies argumentation strategy based guidelines argumentation theory To express content natural language GEA relies set techniques extend integrate previous work computational linguistics microplanning realizing evaluative arguments Finally quantitative model user preferences expressed AMVF key knowledge source GEA tailoring content organiza tion phrasing generated arguments users The GEA implementation largely domain independent To port new domain implementor needs specify value decomposition relevant entity AMVF weights component value functions speciﬁed ii decision tree lexicalization proper like partially shown Fig 6bottom iii indication objectives aggregated type aggregation allowed distance objectives aggregated conjunction shared participants 3 Evaluating evaluative arguments The goal work presented paper complete research cycle starts designing computa tional model ends empirically testing design model In previous sections described GEA computational model generating evaluative arguments tailored users preferences In section present evaluation framework effectiveness evaluative arguments generated GEA measured real users Our framework based principles techniques social psychology NLG decision theory human interaction We ﬁrst discuss literature social psychology persuasion argument ef fectiveness Next examine previous work evaluating NLG techniques Finally architecture evaluation framework rationale 31 Research psychology persuasion argument effectiveness In social psychology communication theory attitudes beliefs persuasion deﬁned follows 45 Attitudes evaluative tendencies feature environment typically phrased terms like dislike favor disfavor Beliefs assessments case Persuasion involves intentional communicative act attempts affect current future behavior addressees creating changing reinforcing addressees attitudes beliefs The focus work evaluative arguments attempt change reinforce users attitudes vs beliefs Thus beliefs discussed paper Since goal evaluative arguments affect behavior affecting attitudes follows effectiveness tested comparing measurements subjects attitudes behavior exposure argument For instance compare effectiveness arguments positively evaluating state ﬁt attempt change persons daily exercise behavior perform experiment 938 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 How judge house B11 The like house closer cross good choice bad choice _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ good choice b How sure selected best house ones available unsure _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ sure Fig 7 Sample selfreports compare daily exercise groups subjects participants exposed evaluative arguments In experimental situations measuring effects overt behavior problematic research persuasion based measurements attitudes declarations behavioral intentions 45 The common technique measuring attitudes semantic differential selfreport subjects presented scale endpoints bipolar terms good choice vs bad choice usually separated seven equal spaces participants use evaluate attitude belief statement Fig 7 examples Ref 45 suggests individuals naturally resistant persuasion individual differences taken account studying persuasion Features individuals important respect argumentativeness tendency argue intelligence selfesteem need cognition tendency engage enjoy effortful cognitive endeavours 430 Therefore crucial control variables attempting evaluate persuasiveness argument Finally argument evaluated addressee respect dimensions quality coher ence content organization writing style convincingness However evaluations based judgements dimensions clearly weaker evaluations measuring actual attitudinal behavioral change 48 32 Evaluation NLG models Three main methods proposed literature purpose evaluating approaches natural language generation human judges corpusbased evaluation task efﬁcacy All shortcomings important choose appropriate method test hypothesis interested We present critical overview methods clarify task efﬁcacy appropriate testing effectiveness evaluative arguments tailored model users preferences The human judges evaluation method requires panel judges score outputs number different generation models 56174059 To compare models judge panel given outputs generated models asked rate outputs dimensions text quality coherence content organization writing style correctness Note writers text humans natural language generation systems combination Indeed common pit NLG systems human writers Clearly guard biases judges unaware text generated model Having panel judges combats eliminate inherent subjectivity human judgement natural language The rationale multiple judges rarely reach consensus collective opinion provide persuasive evidence signiﬁcant differences different models The main limitation approach requires speciﬁcation texts evaluated simple easily articulated judges This case 40 judges told text meant general explanation given biological entity process aimed freshman biology students For applications input generation process include detailed characterization context interactive applications output tailored complex user model history previous interaction extremely difﬁcult judges ﬁctitiously place speciﬁc context order judge texts11 As seen previous section input GEA complex It consists possibly complex novel argument subject new house 11 See 6 illustration speciﬁcation context extremely complex human judges evaluate content selection strategies dialogue G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 939 large number features complex model users preferences Therefore human judges method deemed appropriate evaluate GEA The corpusbased evaluation method applied corpus inputoutput pairs available 53 The input consists relevant knowledge sources output textual graphical multimedia A portion corpus training set develop computational model capable generating output corresponding input The remainder corpus testing set evaluate model The model evaluated verifying pair testing set determine output produced model applied input matches corresponding output pair The main advantage corpusbased method require subjective human judgements quality generated text However requires large corpus inputoutput pairs In cases extant corpus complexity input speciﬁcation increases effort required create corpus prohibitive As discussed GEA makes use rich input representation includes argument subject features complex user model possible create corpus Indeed clear generator GEA Arguably natural language processing tools components larger systems designed assist users engaged tasks natural extremely informative way evaluate effectiveness experi menting users performing tasks 33 This evaluation method called task efﬁcacy As early example task efﬁcacy evaluation NLG consider 60 In study explanation generation model medical belief network proven effective showing explanations generated improved diagnostic accuracy increased user conﬁdence ﬁnal diagnosis In general task efﬁcacy evaluation method allows evalu ate generation model explicitly evaluating output measuring outputs effects users behaviors beliefs attitudes context task The requirement method speciﬁcation sensible task However task efﬁcacy achieve sufﬁcient statistical power typically requires involving large number users evaluation considered expensive difﬁculttoorganize method Nevertheless applicability methods limited task efﬁcacy common choice NLG research 15185164 forms basis evaluation framework developed 33 The evaluation framework 331 The user task Generally speaking suitable task evaluating evaluative arguments user required perform evaluations comparisons objects order complete task b presenting user evaluative arguments context task change user performs task measurable dimensions A basic frequent task satisfying requirements preferential choice selection task extensively studied decision analysis It consists having decisionmaker select subset preferred objects houses set possible alternatives considering tradeoffs multiple objectives house location house quality evaluating objects respect values set primitive attributes distance work size garden The task devised evaluation framework extension preferential choice comprises subtasks As shown Fig 8 start ﬁrst subtask user presented information set alternatives Next asked select subset n preferred alternatives order preference called Hot List In second subtask user presented evaluative argument new instance included initial set alternatives asked wants include Hot List If users answer afﬁrmative decide place new instance ordered Hot List Finally user ﬁlls questionnaire attitudes beliefs new instance decision task 332 The data exploration environment In evaluation framework user performs task interactive data exploration analysis IDEA 54 The IDEA environment facilitates users autonomous exploration set alternatives performance subtasks realestate domain We chose domain familiar presents challenging decision task The design IDEA inspired 940 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Fig 8 User task core evaluation framework HomeFinder prototype 63 details interface reﬁned iterative evaluation HCI experts pilot subjects The interface shown Fig 9 reader ignore argument NewHouse326 The user visually inspect aspects set houses map bar charts table 14 distinct attributes including 1 attribute street house located 2 attributes neighborhood Furthermore user explore information applying powerful interactive techniques including dynamic queries draganddrop painting 54 333 The evaluation framework Fig 10 illustrates architecture evaluation framework consists main subsystems IDEA User Model Reﬁner New Instance Generator GEA The framework assumes model users preferences AMVF previously acquired user assure reliable initial model The user assigned task selecting dataset preferred alternatives placing Hot List Fig 9 upper right corner ordered preference When user feels task accomplished ordered list preferred alternatives saved Preliminary Hot List Fig 102 Then User Model Reﬁner reﬁnes initial model making adjustments necessary model consistent preferences user expressed creating Hot List Fig 103 This reﬁnement process produces Reﬁned Model Users Preferences heuristically adjusting model weights Then New Instance NewI designed ﬂy New Instance Generator preferable user given reﬁned preference model Fig 104 More precisely new instance designed value user average values instances highest values HotList At point stage set argument generation Given Reﬁned Model Users Preferences Ar gument Generator produces evaluative argument NewI tailored model Fig 105 presented user IDEA Fig 106 Fig 9 example The argument goal persuade user NewI worth considered Notice information NewI presented graphically Once argument presented user decide immediately introduce NewI Hot List b decide explore dataset possibly making changes adding NewI Hot List c Fig 9 shows display end interaction user reading argument decided introduce NewI Hot List ﬁrst position Fig 9 right When user decides stop exploring assumed satisﬁed selections hot list measures related arguments effectiveness assessed Fig 107 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 941 Fig 9 The IDEA environment display end interaction Fig 10 Architecture evaluation framework 942 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 How judge houses Hot List 1st house bad choice _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _X_ _ _ _ good choice 2nd house New house bad choice _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _X_ _ _ _ _ _ _ good choice 3rd house bad choice _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _X_ _ _ _ _ _ _ good choice 4th house bad choice _ _ _ _ _ _ _ _ _ _ _ _ _X_ _ _ _ _ _ _ _ _ _ _ _ _ good choice Fig 11 Sample selfreport users satisfaction houses Hot List1 334 Measures argument effectiveness Measures argument effectiveness obtained record users interaction user selfreports ﬁnal questionnaire Fig 7 example selfreport include Measures behavioral intentions attitude change user adopts NewI b posi tion Hot List places c likes NewI objects Hot List Measures b obtained record user interaction measures c obtained user selfreports A measure users conﬁdence selected best set alternatives This measure ones obtained user selfreports A measure argument effectiveness derived explicitly questioning user end interaction rationale decision 48 This provide valuable information aspects argument inﬂuential users decision making Additional measures argument effectiveness derived explicitly asking user end inter action judge argument respect dimensions quality content organization writing style convincingness However evaluations based judgements dimensions clearly weaker evaluations measuring actual behavioral attitudinal changes 48 A closer analysis measures behavioral intentions attitude change indicates measures c simply precise version measures b In fact assess like b preference ranking new alternative objects Hot List offer additional critical advantages Selfreports allow subject express differences satisfaction precisely ranking For instance selfreport shown Fig 11 subject able specify ﬁrst house Hot List unit satisfaction better house following ranking house unit better house following ii Selfreports force subjects express total order houses For instance Fig 11 subject allowed express second houses Hot List equally good Furthermore measures satisfaction obtained selfreports combined single statistically sound measure concisely expresses subject liked new house respect houses Hot List This measure zscore subjects selfreported satisfaction new house respect selfreported satisfaction houses Hot List The zscore item xi indicates far direction item deviates mean population X measured units distributions standard deviation Formally xi X zxi cid5 cid4 xi μX σ X For instance satisfaction zscore new instance given sample selfreports shown Fig 11 7μ8 7 7 5σ 8 7 7 5 02 Note satisfaction zscore new instance ranges 15 15 interval It equal min value 15 new instance scored 0 instances scored 9 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 943 equal max value 15 new instance scored 9 instances scored 0 equal mid point 0 instances including new equally scored Z scores called standard scores especially useful comparing relative standings items distributions different means andor different standard deviations The satisfaction zscore precisely concisely integrates measures behavioral intentions attitude change We satisfaction zscores primary measure argument effectiveness To summarize evaluation framework supports users performing realistic task interacting IDEA In context task evaluative argument generated GEA measurements arguments effectiveness collected 4 The experiment In previous section proposed taskbased framework evaluating evaluative arguments In context task evaluative argument generated GEA measurements collected effectiveness In section report results experiment run framework The design development GEA based assumptions necessary order generate effective evaluative arguments 1 Supporting opposing evidence main evaluative claim identiﬁed arranged according model readers values preferences In GEA assumed model effectively represented AMVF quantitative model preferences originally developed decision theory 2 Evaluative arguments concise presenting pertinent cogent information 3 Supporting opposing evidence main evaluative claim carefully arranged according argumentative strategy presented Section 223 4 The microplanning tasks speciﬁc instantiation described Section 23 contribute argument effec tiveness Naturally assumptions questioned empirically tested With respect ﬁrst sumption question user speciﬁc AMVF effective model tailoring evaluative arguments As second assumption disputes argument sake brevity present pertinent cogent information However remains open question effective degree conciseness Concern ing assumption argumentative strategy presented Section 223 implements set guidelines argumentation theory However alternative strategies effective speciﬁc situations particular class users Finally respect fourth assumption generally accepted form microplanning needed produce effective text conceivable implementations microplanning tasks ones devised GEA effective The experiment performed focuses empirical questions related ﬁrst assumptions user speciﬁc AMVF effective model tailoring evaluative arguments effective degree conciseness evaluative arguments To test ﬁrst assumption compared effectiveness arguments tailored users AMVF effectiveness arguments tailored default AMVF aspects house equally important weights AMVF equal To test second assumption preliminary attempt determine optimal level conciseness evaluative arguments compared effectiveness arguments generated argument generator different levels conciseness values constant k controls argument conciseness GEA discussed Section 223 41 Experiment design procedure To address questions designed betweensubjects experiment experimental conditions NoArgument This baseline condition In condition subjects completed selection task simply informed new house came market No evaluative argument new house presented subject Information new house presented graphically 944 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Fig 12 Hypotheses experiment outcomes TailoredConcise In condition subjects completed selection task presented evaluative argument new house tailored preferences level conciseness hypothesize optimal Our assumption domain effective argument contain slightly half available evidence By running generator different values k user models pilot subjects corresponds k 03 In fact k 03 arguments contained average 10 pieces evidence 19 available AMVF contains 19 objectives NonTailoredConcise In condition subjects completed selection task presented evaluation new house instead tailored preferences tailored preferences default average user aspects house equally important weights AMVF A similar default preference model comparative purposes 57 The level conciseness hypothesize optimal k 03 TailoredVerbose In condition subjects completed selection task presented evaluation new house tailored preferences level conciseness hypothesize low We chose k 1 analysis pilot subjects corresponded average 16 pieces evidence possible 19 In conditions information new house presented graphically informa tion hidden user new house House326 Fig 9 example And new house introduced subjects free perform data exploration compares Hot List choices Our hypotheses outcomes experiment summarized Fig 12 We expect arguments generated TailoredConcise TC condition effective arguments generated NonTailored Concise NTC TailoredVerbose TV conditions We expect TC condition somewhat better NoArgument NA condition lesser extent subjects absence argument spend time exploring dataset reaching informed balanced decision Finally strong hypotheses comparisons argument effectiveness NoArgument NonTailoredConcise TailoredVerbose conditions The experimental procedure summarized Fig 13 It consists phases In ﬁrst phase subject ﬁlls online questionnaires One questionnaire implements SMARTER elicitation method decision theory Section 222 effectively acquire AMVF model subjects preferences 21 In experiment safely assume user preferences represented AMVF uncertain aspects user selection task The questionnaires assess subjects argumentativeness tendency argue 30 need cog nition tendency engage enjoy effortful cognitive endeavours 3 These key individual features research persuasion shown inﬂuence peoples reaction arguments 45 Any experiment persuasion G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 945 Fig 13 Experimental procedure control variables In second phase experimental procedure Fig 13 control possible confounding variables including intelligence selfesteem subject randomly assigned experimental conditions Then subject interacts evaluation framework end inter action subject ﬁlls ﬁnal questionnaire measures argument effectiveness collected Section 234 The experiment ﬁrst performed 8 pilot subjects reﬁne improve experimental procedure The instructions ﬁnal questionnaire script followed experimenter presenting IDEA checked clarity12 Once experimental procedure sufﬁciently stable ran formal experiment involving 40 subjects 10 experimental condition Each subject interactive section evaluation framework 42 Experiment results During analysis experimental outcomes data subjects eliminated One subject eliminated TC condition outlier selfreport measure indicating subjects conﬁdence decision process score 3 standard deviations average Another subject eliminated NTC condition outlier need cognition measure score 25 standard deviations average subjects selfreports satisfaction instances HotList inconsistent explicit ranking HotList Finally subjects eliminated satisfaction selfreport score instances HotList new instance maximum possible 9 19 scale We consider extremely anomalous situation reasons First importantly instances HotList scored 9 new instance chance obtain positive zscore facing kind ceiling effect Secondly fact subjects gave ﬁve instances score indicates nondiscriminatory preferences One subjects TC condition NA condition 421 Effectiveness comparisons As discussed Section 334 precisely concisely integrating measures behavioral intentions attitude change satisfaction zscore primary measure argument effectiveness available framework We focus measure ﬁrst Our statistical analysis based Dennett test This appropriate test 12 The initial questionnaires standard ones developed decision theory social psychology 946 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Table 1 Results satisfaction zscores TailoredConcise compared conditions directionally Cond1 NA 0273 NTC 0275 TV 0047 Cond2 TC 0994 TC 0994 TC 0994 Mean difference Cond1 Cond2 0720 0719 0947 Std error Signiﬁcance 0389 0389 0380 0086 0087 0023 Table 2 From Logs interaction average time spent subjects conditions exploring dataset new house presented The difference NoArgument TailoredConcise signiﬁcant Condition NA NTC TC TV Time 0 03 56 0 03 37 0 02 44 0 03 30 experiment like groups apriori goal compare TC 2013 As shown Table 1 satisfaction zscores obtained experiment provide support hypotheses Argu ments generated TC condition greater satisfaction zscores arguments generated TV NTC NA conditions The difference effectiveness arguments generated TC condition arguments gen erated TV condition statistically signiﬁcant p 005 difference comparisons TC vs NTC NA marginally signiﬁcant p 01 A possible reasonexplanation differences marginally signiﬁcant relatively small number subjects Another possibility difference uniform default model NTC user speciﬁc TC small As Section 52 possibilities corroborated recent study Remarkably TC appears better NA condition greater extent expected We believed absence argument NA subjects spent time exploring dataset reaching informed balanced decision satisfaction TC TVNTC However NA subjects spend signiﬁcantly time exploring dataset Table 2 compensate lack explicit argument With respect measures argument effectiveness considered decision conﬁdence decision rationale argument quality ﬁnd signiﬁcant results 422 Possible confounding variables The design experiment takes account fact effectiveness argument determined argument subjects traits argumentativeness Arg need cognition NFC selfesteem intelligence The reason subjects randomly assigned conditions precisely control possible confounding variables As extra check subjects Arg NFC assessed running experiment order verify subjects successfully randomized obtain conditions equivalent Arg NFC At ﬁrst glance data Table 3 reporting means Arg NFC condition indicate randomization failed TC condition lowest Arg highest NFC Table 3 However consider differences means noted minimal respect ranges Arg NFG vary 54 54 50 50 respectively More tellingly means Arg NFC ﬁrst half positive ranges moderately high Since results social psychology 13 In previous papers 8 reported results based applying ttest pairwise comparison However subsequently realized Dunnett test appropriate given experimental design Furthermore prior analysis included subjects excluded analysis reporting paper G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 947 Table 3 Means Argumentativeness Arg Need Cognition NFC experimental conditions Condition NA NTC TC TV Arg mean 47 109 38 107 NFC mean 187 15 249 15 inﬂuence Arg NFC persuasion considered differences individuals scored high vs low personality traits assume conﬁdence Arg NFC substantially inﬂuence outcome experiments Unfortunately limited number subjects possible consider relevant independent variables argumenttype Arg NFC single generalized linear model 5 Related work In previous sections discussed related work necessary background research In section complete analysis previous work focusing key aspects require extensive treatment First examine previous work content selection organization evaluative arguments Second review related work cooccurred followed research GEA In particular consider projects extended GEAs approach andor evaluated generators usertailored evaluative arguments 51 Previous work content selection organization evaluative arguments Although considerable research devoted content selection organization generating evaluative arguments approaches proposed limited type evaluative arguments generated extent comply guidelines argumentation literature Ref 47 describes uses measure evidence strength tailor evaluations hotel rooms users However adopts qualitative measure evidence strength ordinal scale appears range veryimportant notimportant This limits ability select arrange argument evidence qualitative measures support approximate comparisons notoriously difﬁcult combine somewhatimportant pieces evidence equivalent important piece evidence Refs 613 studied generation evaluative arguments context collaborative planning dialogues Although adopt qualitative measure evidence strength evaluation needed measure mapped numerical values preferences compared combined effectively However respect GEA work makes strong simplifying assumptions It considers decomposition preference entity preferences primitive attributes considering complex preferences frequently hierarchical structure Additionally assumes dialogue turn provide supporting opposing evidence The described 7 employs additive decision models recommending courses focus work dynamically acquiring model students preferences The systems recommendations limited recommending single option considered better users current choice In addition work addresses problem selecting positive attributes justify recommendation consider plan realize positive negative attributes multiple suggested options In 38 Kolln proposes framework generating evaluative arguments based quantitative mea sure evidence strength Evidence strength computed fuzzy hierarchical representation user preferences Although fuzzy representation represent viable alternative AMVF discussed paper Kollns proposal sketchy describing measure strength select arrange argument content Finally 35 previous work relevant proposal As described Section 223 work adapted measure evidence strength compellingness measure deﬁnes piece evidence worth mentioning notablycompelling However key differences Kleins 948 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 Table 4 Contributions proposed argumentation strategy context previous work 47 35 38 67 Our Strategy Quantitative measure importance yes yes yes yes Based argumenta tion theory yes Argument type Single entity Comparison yes yes yes yes yes yes work Klein developed strategies comparison strategies based argumentation theory Table 1 summarizes contributions proposal respect previous work content selection orga nization generating evaluative arguments The table considers dimensions proposed approach uses quantitative vs qualitative measure evidence importance proposed approach based guide lines argumentation theory approach covers arguments evaluating single entity comparing entities It clear strategy extends previous work ways covering arguments evaluating single entity arguments comparing entities implementing comprehensive set guidelines argumentation theory 52 Related recent work cooccurred followed research Several recent projects extended GEAs approach andor evaluated generators usertailored evaluative ar guments Overall evidence studies indicates tailoring evaluative argument userspeciﬁc AMVF increase effectiveness These studies indicate hypothesis marginally conﬁrmed experiment run sufﬁcient number subjects default model considered NTC condition sufﬁciently different userspeciﬁc STOP generator usertailored smoking cessation letters 51 tailoring based information col lected means 4page multiple choice questionnaire smokers habits health concerns forth The STOP especially relevant research section generated letter evaluative argument More speciﬁcally letter motivation paragraph mentions userspeciﬁc important likes dis likes smoking helps relax vs expensive The effectiveness STOP tested far extensive longest costliest taskbased evaluation NLG clinical trial involving 2553 smokers In study smokers randomly assigned groups respectively received tailored letter nontailored letter letter Effectiveness tailoring tested months later asking smokers quit smoking positive answers checked saliva samples In general sults STOP evaluation inconclusive Although tailored letters better nontailored ones smokers quitting especially difﬁcult difference effectiveness conditions overall statistically signiﬁcant With respect work study provide positive negative evidence hypotheses tested experiment Although STOP generates text partially evaluative argument tailored user follow approach arguments tailored userspeciﬁc AMVF The aspect STOPs evaluation relevant experiment detailed analysis evaluation failed prove tailoring effective Four possible reasons considered tailoring effect inducing smoking cessation receiving letter matters actual content ii tailoring based different morecomplex knowledge smokers iii knowledge users appropriate tailoring inappropriate vi STOP tailoring effect larger clinical trial In practice outcome STOP evaluation tell reasons playing role Interestingly said results experiment showed difference tailored nontailored arguments marginally signiﬁcant However shortly outcome recent extensive evaluation MATCH similar GEA indicates tailoring AMVF G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 949 signiﬁcant effect likely reason marginally signiﬁcant ﬁndings vi insufﬁcient number datapoints The MATCH 62 extends GEAs approach generating evaluative arguments MATCH multimodal speechenabled dialogue implemented PDA intended help people ﬁnd information restaurants subway routes New York City Empirical testing spoken dialogue systems shown presentation complex information resulting user request timeconsuming phases dialogue One key research goals MATCH project improve information presentation phase enabling select relevant information effectively present To achieve goal MATCH adopts GEAs decisiontheoretic framework user preferences modelled AMVFs Like GEA MATCH relies userspeciﬁc AMVF generate cogent concise text content organization tailored user The generate different types usertailored presentations recommendations comparisons summaries A recommendation evaluative argument best available alternative A comparison evaluative argument comparing ﬁve alternatives pointing reasons choosing summary simply provides overview set alternatives highlighting attributes dissimilar 62 evaluated effectiveness presentation types withinsubjects experiment participant overheard series dialogues selecting restaurant In session participant presented argument generated according strategy tailored model model randomly selected participant note tailoring argument randomly chosen user extreme choice uniform default model GEA At end session participant asked rate information quality argument 05 scale The experiment involved 16 participants Because experimental setting based overhearing conversations possible run large number sessions 64 actual experiment participant total 1024 sessions Since session participant asked express information quality judgment proposed argument 1024 information quality judgements collected experiment Note larger number 36 judgements considered evaluation GEA To obtain number judgments betweensubjects evaluation framework run 1024 participants The results study provide empirical evidence ﬁrst hypotheses tested experiments tailoring evaluative arguments user speciﬁc AMVF increases effectiveness A twoway ANOVA information quality strategy model showed signiﬁcant effect strategy F 1279 p 00001 summaries clearly effective summaries scored 233 comparisons 353 recommendations 408 However distance14 randomly selected user model participant user model considered paired ttest information quality user model strategies highly signiﬁcant In particular distance greater 02 left set 464 paired comparisons tailored presentations preferred df 463 t 261 p 0009 Because average distance user models study 057 result indicates users sensitive relatively small perturbations models The FLIGHTS 46 represents recent attempt generate usertailored evaluative arguments spoken dialogue Like MATCH FLIGHTS concisely compares complex options ﬂights pointing relevant information intended user However FLIGHTS demonstrates tailoring user preferences carried levels information presentation appropriate content selected presented appropriately current dialogue context intonation expresses contrasts intelligibly 49 FLIGHTS employs sophisticated content planning strategies capable generating plans richer discourse structure including distinctions themerheme givennew indeﬁnite This information structure support ﬁnergrain choices linguistic realization intelligent control prosody convey meaning following theory presented 58 At time writing evaluation FLIGHTS planned following experimental design successfully test MATCH 62 14 The distance AMVFs deﬁned sum attributes absolute values difference rankings attribute 950 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 6 Conclusions future work The research presented paper interdisciplinary We integrated extended principles tech niques form argumentation theory decision theory computational linguistics social psychology human interaction Our research makes key contributions First developed complete computational model gener ating evaluative arguments tailored users preferences Second devised implemented evaluation framework effectiveness evaluative arguments measured real users Third framework performed experiment test proposal tailoring evaluative argument users preferences increases effectiveness differences conciseness signiﬁcantly inﬂuence argument effectiveness While second hypothesis conﬁrmed experiment ﬁrst marginally ﬁrmed However independent testing researchers recently provided support hypothesis A key goal research described paper complete research cycle begins developing computational model devising techniques evaluate model applying techniques actually evaluate aspects model To achieve goal complexity issues involved limit investigation ways Clearly limitations open doors future research More complex arguments Many naturally occurring arguments consist mixture evaluative arguments basic types arguments factual causal recommendation Although focus work purely evaluative arguments long term goal research develop testable models generating argu ments combine causal arguments evaluative arguments recommendations We plan integrate work generating evaluative arguments AMVF previous work generating causal arguments Bayesian Networks 6067 previous work generating recommendations inﬂuence diagrams 32 Automatic acquisition linguistic knowledge Another limitation model generating evaluative arguments human developer needs encode linguistic knowledge sources include rhetorical strategies lexicon sentenceplanning strategies aggregation generation referring expres sions The problem humanintensive process extremely timeconsuming repeated new domain importantly resulting knowledge tends brittle performance abruptly decreases unexpected situations arise To address problem plan supplement intuition hu man developer probabilistic datadriven procedure automatic acquisition linguistic knowledge evaluative arguments text corpora 1152 Beyond AMVF Although AMVF reasonably model peoples preferences situations strong assumptions independence attributes So settings necessary use complex models preferences attribute interactions account Notice adopting different model users preferences require redeﬁning measure evidence importance argumentation strategy select order content argument Also additional difﬁculty simple quick methods decision theory similar SMARTER 21 acquire models However set novel techniques apply machine learning problem preference elicitation help respect 1028 Generating multimedia arguments As seen Section 33 evaluation framework argument new house presented context graphical display shows information new house information houses However integration argument display In current architecture GEA generates evaluative arguments considering information displayed graphically A direction future work study natural language evaluative arguments integrated graphics increasing level sophistication The information graphic given changed However natural language generator planning text access representation information displayed graphically So generated argument enhanced adding references graphics indicating instance visualization user ﬁnd information mentioned argument ii Once textual argument planned graphics enhanced argument effective For instance information mentioned argument highlighted graphics way indicates role supporting opposing argument claims G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 951 iii Graphics text planned achieve abstract communicative goal This sophisticated integration text graphics require major architectural changes GEA However recently progress area 26 References 1 FH Barron BE Barrett Decision quality ranked attribute weights Management Science 42 11 1996 15151523 2 J Blythe Visual exploration incremental utility elicitation Proceedings National Conference Artiﬁcial Intelligence 2002 pp 526532 3 JT Cacioppo RE Petty CF Kao The efﬁcient assessment need cognition Journal Personality Assessment 48 3 1984 306307 4 JT Cacioppo RE Petty KJ Morris Effects need cognition message evaluation recall persuasion Journal Personality Social Psychology 45 4 1983 805818 5 CB Callaway JC Lester Narrative prose generation Artiﬁcial Intelligence 139 2 2002 213252 6 S Carberry J ChuCarroll Collaborative response generation planning dialogues Computational Linguistics 22 2 1998 355400 7 S Carberry J ChuCarroll S Elzer Constructing utilizing model user preferences collaborative consultation dialogues Compu tational Intelligence Journal 15 3 1999 185217 8 G Carenini JD Moore An empirical study inﬂuence user tailoring evaluative argument effectiveness Proceedings 17th International Joint Conference Artiﬁcial Intelligence Seattle USA 2001 pp 13071314 9 J Chai V Horvath N Nicolov M Stys N Kambhatla W Zadrozny P Melville Natural language assistant A dialog online product recommendation AI Magazine 23 2 2002 6375 10 U Chajewska D Koller D Ormoneit Learning agents utility function observing behavior Proceedings Eighteenth Interna tional Conference Machine Learning 2001 pp 3542 11 J Chen S Bangalore O Rambow MA Walker Towards automatic generation natural language generation systems Proceedings 19th International Conference Computational Linguistics COLING Taipei Taiwan 2002 pp 17 12 CF Chien F Sainfort Evaluating desirability meals An illustrative multiattribute decision analysis procedure assess portfolios interdependent items MultiCriteria Decision Analysis 7 4 1998 230238 13 J ChuCarroll S Carberry A planbased model response generation collaborative taskoriented dialogues Proceedings National Conference Artiﬁcial Intelligence AAAI Press Menlo Park CA 1994 pp 799805 14 RT Clemen Making Hard Decisions An Introduction Decision Analysis second ed Duxbury Press Belmont CA 1996 15 N Colineau C Paris K Vander Linden An evaluation procedural instructional text Proceedings International Natural Language Generation Conference 2002 pp 128135 16 EPJ Corbett RJ Connors Classical Rhetoric Modern Student Oxford University Press Oxford 1999 17 V Demberg JD Moore Information presentation spoken dialogue systems Proceedings 11th Conference European Chapter Association Computational Linguistics 2006 pp 6572 18 B Di Eugenio M Glass MJ Trolio The DIAG experiments Natural language generation intelligent tutoring systems Proceedings International Natural Language Generation Conference 2002 pp 120127 19 B Di Eugenio JD Moore M Paolucci Learning features predict cue usage Proceedings 35rd Annual Meeting Associ ation Computational Linguistics 1997 pp 8087 20 CW Dunnett A multiple comparison procedure comparing treatments control Journal American Statistical Associ ation 50 1955 10961121 21 W Edwards FH Barron SMARTS SMARTER Improved simple methods multiattribute utility measurements Organizational Be havior Human Decision Processes 60 1994 306325 22 M Elhadad Using argumentation text generation Journal Pragmatics 24 1995 189220 23 M Elhadad KR McKeown J Robin Floating constraints lexical choice Computational Linguistics 23 2 1997 195239 24 M Elhadad J Robin An overview SURGE A reusable comprehensive syntactic realization component Technical Report 9603 Depart ment Mathematics Computer Science Ben Gurion University Beer Sheva Israel 1996 25 PC Gordon BJ Grosz LA Gilliom Prounouns names centering attention discourse Cognitive Science 17 3 1993 311348 26 NL Green G Carenini S Kerpedjiev J Mattis JD Moore SF Roth Autobrief An experimental automatic generation brieﬁngs integrated text information graphics International Journal HumanComputer Studies 61 1 2004 3270 27 BJ Grosz AK Joshi S Weinstein Centering A framework modeling local coherence discourse Computational Linguistics 21 2 1995 203226 28 V Ha P Haddawy Similarity personal preferences Theoretical foundations empirical analysis Artiﬁcial Intelligence 146 2 2003 149173 29 KJ Hee M Glass R Freedman MW Evens Learning use discourse markers tutorial dialogue intelligent tutoring Proceedings TwentySecond Annual Conference Cognitive Science Society Philadelphia USA 2000 pp 262267 30 DA Infante AS Rancer A conceptualization measure argumentativeness Journal Personality Assessment 46 1982 7280 31 A Jameson R Schafer J Simons T Weis Adaptive provision evaluationoriented information Tasks techniques Proceedings 14th International Joint Conference Artiﬁcial Intelligence Montreal 1995 pp 18861895 32 HB Jimison LM Fagan DR Shacter HE Shortliffe Patientspeciﬁc explanation models chronic disease Artiﬁcial Intelligence Medicine 4 1992 191205 952 G Carenini JD Moore Artiﬁcial Intelligence 170 2006 925952 33 K Sparck Jones Automatic language information processing Rethinking evaluation Natural Language Engineering 7 1 2001 2946 34 RL Keeney H Raiffa Decisions Multiple Objectives Preferences Value Tradeoffs John Wiley Sons New York 1976 35 DA Klein Decision Analytic Intelligent Systems Automated Explanation Knowledge Acquisition Lawrence Erlbaum Associates 1994 36 A Knott R Dale Using linguistic pheomena motivate set coherence relations Discourse Processes 18 1 1994 3562 37 A Knott C Mellish A featurebased account relations signalled sentence clause connectives Language Speech 39 1996 143183 38 ME Kolln Employing user attitudes text planning Proceedings 5th European Workshop Natural Language Generation Leiden The Netherlands 1995 pp 163179 39 D Kudenko M Bauer D Dengler Group decision making mediated discussions Proceedings User Modelling Conference Johnstown Pennsylvania USA August 2003 pp 238247 40 J Lester B Porter Developing empirically evaluating robust explanation generators The KNIGHT experiments Computational Linguis tics 23 1 1997 65101 41 D Marcu The conceptual linguistic facets persuasive arguments ECAI WorkshopGaps Bridges New Directions Planning Natural Language Generation 1996 pp 4346 42 KJ Mayberry RE Golden For Arguments Sake A Guide Writing Effective Arguments Harper Collins College Publisher 1996 43 WJ McGuire The nature attitudes attitudes change G Lindzey E Aronson Eds The Handbook Social Psychology vol 3 AddisonWesley Reading MA 1968 pp 136314 44 WJ McGuire The nature attitudes attitudes change G Lindzey E Aronson Eds Handbook Social Psychology vol 3 second ed AddisonWesley Reading MA 1969 pp 136314 45 MD Miller TR Levine Persuasion An Integrated Approach Communication Theory Research Lawrence Erlbaum Associates 1996 pp 261276 46 JD Moore ME Foster O Lemon M White Generating tailored comparative descriptions spoken dialogue Proceedings Seventeenth International Florida Artiﬁcial Intelligence Research Society Conference AAAI Press 2004 pp 917922 47 K Morik User models conversational settings Modeling users wants A Kobsa W Wahlster Eds User Models Dialog Systems Symbolic Computation Series SpringerVerlag New York 1989 pp 364385 48 JM Olso MP Zanna Attitudes beliefs Attitude change attitude behavior consistency RM Baron WG Graziano Eds Social Psychology Holt Rinehart Winston New York 1991 pp 192225 49 S Prevost M Steedman Specifying intonation context speech synthesis Speech Communication 15 1994 139153 50 E Reiter R Dale Building Natural Language Generation Systems Cambridge University Press Cambridge 2000 51 E Reiter R Robertson LM Osman Lessons failure Generating tailored smoking cessation letters Artiﬁcial Intelligence 144 12 2003 4158 52 E Riloff J Wiebe Learning extraction patterns subjective expressions Proc Conf Empirical Methods NL Processing Sapporo Japan 2003 pp 105112 53 J Robin K McKeown Empirically designing evaluating new revisionbased model summary generation Artiﬁcial Intelli gence 85 12 1996 135179 54 SF Roth MC Chuah S Kerpedjiev JA Kolojejchick P Lucas Towards information visualization workspace Combining multiple means expression HumanComputer Interaction Journal 12 12 1997 131185 55 D Scott C Sieckenius Souza Getting message RSTbased text generation R Dale C Mellish M Zock Eds Current Research Natural Language Generation Academic Press New York 1990 pp 4773 56 MR Solomon Consumer Behavior Buying Having Being PrenticeHall Englewood Cliffs NY 1998 57 J Srivastava T Connolly LR Beach Do ranks sufﬁce A comparison alternative weighting approaches value elicitation Organizational Behavior Human Decision Process 63 1 1995 112116 58 M Steedman Informationstructural semantics English intonation M Gordon D Büring C Lee Eds LSA Summer Institute Workshop Topic Focus Santa Barbara July 2001 Kluwer Academic Dordrecht 2004 pp 245264 59 A Stent A conversation acts model generating spoken dialogue contributions Computer Speech Language 16 3 2002 313352 60 H J Suermondt GF Cooper An evaluation explanations probabilistic inference Computers Biomedical Research 1993 242254 61 K Vander Linden JH Martin Expressing rhetorical relations instructional text A case study purpose relation Computational Linguistics 21 1 1995 2958 62 MA Walker SJ Whittaker A Stent P Maloor JD Moore M Johnston G Vasireddy Generation evaluation usertailored responses multimodal dialogue Cognitive Science 28 2004 811840 63 C Williamson B Shneiderman The dynamic homeﬁnder Evaluating dynamic queries realestate information exploration B Shneiderman Ed Sparks Innovation HumanComputer Interaction Ablex Publishing Corp ACM SIGIR 1993 pp 295308 64 RM Young Using Grices maxim quantity select content plan descriptions Artiﬁcial Intelligence 115 2 1999 215256 65 RM Young JD Moore DPOCL A principled approach discourse planning Proceedings 7th International Workshop Natural Language Generation Kennebunkport ME June 1721 1994 pp 1320 66 RM Young JD Moore ME Pollack Towards principled representation discourse plans Proceedings Sixteenth Annual Conference Cognitive Science Society Lawrence Erlbaum Associates Hillsdale NJ 1994 pp 946951 67 I Zukerman R McConachy KB Korb Bayesian reasoning abductive mechanism argument generation analysis Proc AAAI Conference 1998 pp 833838