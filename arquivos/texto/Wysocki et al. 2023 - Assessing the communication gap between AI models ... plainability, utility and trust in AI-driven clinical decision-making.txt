Artiﬁcial Intelligence 316 2023 103839 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint Assessing communication gap AI models healthcare professionals Explainability utility trust AIdriven clinical decisionmaking Oskar Wysocki ab Anne Caroline Armstrong c Dónal Landers b Rebecca Lee c André Freitas abd Department Computer Science The University Manchester United Kingdom Great Britain Northern Ireland b Digital Experimental Cancer Medicine Team Cancer Biomarker Centre CRUK Manchester Institute University Manchester United Kingdom Great Britain Northern Ireland c Faculty Biology Medicine Health The University Manchester United Kingdom Great Britain Northern Ireland d Idiap Research Institute Switzerland Jessica Katharine Davies c Markel Vigo r t c l e n f o b s t r c t Article history Received 20 April 2022 Received revised form 27 October 2022 Accepted 12 December 2022 Available online 20 December 2022 Keywords Explainable model Explainable AI ML healthcare User study Clinical decision support Automation bias Conﬁrmation bias Explanations impact 1 Introduction This paper contributes pragmatic evaluation framework explainable Machine Learning ML models clinical decision support The study revealed nuanced role ML explanation models pragmatically embedded clinical context Despite general positive attitude healthcare professionals HCPs explanations safety trust mechanism signiﬁcant set participants negative effects associated conﬁrmation bias accentuating model overreliance increased effort interact model Also contradicting main intended functions standard explanatory models showed limited ability support critical understanding limitations model However new signiﬁcant positive effects repositions role explanations clinical context include reduction automation bias addressing ambiguous clinical cases cases HCPs certain decision support experienced HCPs acquisition new domain knowledge 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 Clinical predictive models based machine learning ML bring promise integrating real world evidence clinical decisionmaking balancing individual clinical experience datadriven evidence The dialogue evidence systematically collected analysed clinical practice allows continuous evolution understand ing disease treatment response This particularly important context new diseases COVID19 new treatments understanding personalised responses multimorbidity diverse populations Effective communication healthcare professionals HCPs ML models depends ability faithful mental representation critical ability assess strengths limitations ML models Corresponding author Department Computer Science The University Manchester United Kingdom Great Britain Northern Ireland Email address oskarwysockimanchesteracuk O Wysocki httpsdoiorg101016jartint2022103839 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 1 Overview framework pragmatic evaluation models explanation It highlights key aspects users mental model affect perceived utility tool clinical setting Pink boxes negative effects For interpretation colours ﬁgures reader referred web version article Explainability 16 emerged research area aiming address interpretability bottlenecks ML models resulting fact effective predicting outcome explaining underlying reasoning limits practical application critical areas In recent work 7 Veriﬁcation Explainability Methods identiﬁed main frontier topics area medical AI Complex Networks Inference Graph Causal models Counterfactuals order satisfy need trustworthiness multiple levels medical workﬂow Whilst recent explainable ML methods 815 instrumenting ML models transparent perceived utility suitability explanation models systematically investigated clinical context We aimed address research gap conducting user study effectiveness current modalities ML explainability healthcare professionals datadriven decision support The study performed ex plainable ML clinical decision support tool designed help manage admissions patients cancer COVID19 CORONET COVID19 Risk ONcology Evaluation Tool 16 In contrast existing explainability studies performed simpliﬁed tasks 1720 experiment closely mimicked realworld clinical scenario 21 decision based multiple factors supported recommendation As contributions propose novel methodology assess pragmatic utility explanations ac counts systematic understanding interaction clinicians mental model explainable ML model Additionally introduce framework evaluating impact ML explanations perceived utility usability clinical context 22 We present results experiment conducted involving 23 HCPs presented 10 scenarios decide admitdischarge patient cancer COVID19 supported model ﬁrstly explanation colour bar score explanation contributing features 2 Methods 21 Framework pragmatic evaluation models explanation We introduce framework pragmatic evaluation models explanation outlining key components models output including explanatory components models usefulness clinical setting Fig 1 It allows investigating key aspects users attitude impact perceived utility model clinical decision making setting Our endtoend approach allows evaluation change perception overall systems clinical suitability evaluation systemlevel effect instead itemlevel effect single recommendation 21 In general explainability transparency traceability highlighting decisionrelevant parts representations algorithm dataset population particular observation individual patient Explanation interchangeably interpretation distinction ﬁrst product second Expla nation collection features interpretable domain contributes production abstract statement Interpretation process mapping statement domain human expert perceive comprehend derstand detailed deﬁnition refer 22 In framework ﬁrst ease interpretation tools output evaluated prerequisite understanding satisfaction trust 2324 Note clinical decisionmaking process cognitive load model output disjointed contextual factors require working memory user 25 2 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Then questions trust satisfaction model understanding systematically assessed Trust model leads high persuasive power reassurance cases user certain right decision 26 Understanding models reasoning process supports critical assessment model understanding tool provides incorrect recommendations Satisfaction measures provided output corresponds users ex pectations Trust enhanced providing explanation 72729 uncertainty models recommendation 2926 andor ensuring high performance model given task 30 The aspect characterised declared performance models developer testing validation subjective performance perceived user experiment 3121 However excessive trust negative effects conﬁrmation bias 32 automation bias 3334 These evaluated concordance users action recommendation correct action time spent decisions Finally aspects account perceived pragmatic utility model evaluated asking HCPs use model clinical practice The pragmatic utility aggregates models usability deﬁned effective eﬃcient satisfying model speciﬁed context 22 clinical performance deﬁned ability deliver results correlated speciﬁc clinical condition target population experiments performed medical experts 35 The output model consist main components recommendation R prediction prognosis classiﬁcation ii uncertainty models recommendation U iii explanation Exp presented user R RU RExp RUExp In empirical analysis CORONET model communicate uncertainty aside aspect performed R RExp withinsubject scenario group HCPs Although visual components explanation scatter plot described 22 makes user aware models imperfect performance predicted scores training cohort including incorrect recommendations evaluate aspect directly We provide explicitly performance metric HCPs Similarly perceived performance model measured Thus evaluate contribution models performance HCPs attitude model Individual characteristics user users background knowledge expectations impact attitude 36 evaluated Guided framework investigate overall pragmatic impact explanations clinical decision targeting following research questions RQ1 Does explanation improve understanding models reasoning mechanisms RQ2 Does explanation lead increase model satisfaction RQ3 Does explanation lead increase trust RQ4 Do explanations improve HCPs critical understanding model RQ5 Do explanations enable improved clinical understanding RQ6 How explanations impact automation bias RQ7 Does explanation increase conﬁrmation bias This study indicates major unaddressed communication gap explainable ML models health care professionals elicits priority areas investigation 22 Clinical setting supporting model CORONET model In study CORONET COVID19 Risk ONcology Evaluation Tool 16 developed help determine need admit patients hospital bases likelihood needing oxygen generally oxygen given hospital severity COVID19 indicated predictions required oxygen andor death 3739 As result key outcomes established arranged 03 point ordinal scale discharged admitted 24 h inpatient admittedO2 requirement including ventilator support admittedO2died death directly attributable COVID19 disease cancer These outcomes measures disease severity Contrary analysis binary outcomes need oxygen vs need oxygen strategy improved ability provide complete clinical picture important overall decisionmaking hospital admission During development model ﬁrst validated external cohort 16 later recent data Omicron variant 40 The CORONET tool available online1 providing interactive user interface crossdevice compatibility The user interface depicted Fig 2 details Fig S7 S8 Suppl Methods 5 Recommendation R The basic output model recommended action CORONET score deﬁnes action The score presented colour bar scaled 03 decision thresholds Fig 3a The score 10 recommends discharge 10 admission 23 suggests high risk severe COVID19 illness Critically group patients present hospital cancer treatment related problems COVID19 This adds complexity decisionmaking HCP However CORONET built aid decisions COVID19 1 httpscoronet manchesterac uk 3 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 2 Users interface CORONET available httpscoronet manchesterac uk The user inputs values manually warned case values expected range Supp Fig S7 Calculate NEWS2 button leads popup window calculator Supp Fig S8 Convert button refers http unitslab com After pressing Calculate output generated presented user window Patient Details ﬁeld Of note experiment participants directly interact interface Static images tools output provided severity requirement admission The CORONET model regression random forest trained ordinal 0123 scale dependent variable True outcomes y 0patient discharged 1patient admitted 2 admitted required supplemental oxygen 3admitted required O2 died Thus model predicts score range 03 recommended action delivered based pragmatically deﬁned threshold 164142 Model explanation Exp The explanation output consists visual components Firstly shows scatter plot Fig 3b predicted score x axis patients model derivation scores predicted LeaveOne Out Cross Validation 16 Each dot represents individual patient colour corresponds true outcome All patients sorted left right according predicted score The plot allows user locate patient question marked star cohort considering true outcomes models recommendations The point 4 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 3 Output CORONET tool provided individual patient 5 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 4 Experimental design The withinsubject design enabled examination change attitude model participant ﬁrst supported CORONET Score CS CORONET Score explanation CSExp distribution elicits models errors recommendations incorrect For death outcomes model predicted scores admission threshold Similarly discharged patients tool recommended admission This reveals performance model user Second explanation shows bar plot features contributing individual prediction later referred contribution plot Fig 3c The length bar represents magnitude obtained SHAP explanation 8 The colour shows direction contribution discharge admission Features ordered depend ing contribution The ﬁgure serves local explanation model bars change individual recommendation The contribution plot produced patient question feature contribution cases user needs input new values The contribution plots layout iteratively derived discussions user experience designers oncologists order produce intuitive visualisation Among considered layouts bar plot waterfall plot 8 force plot 8 Partial Dependence plots 43 Individual Conditional Expectation plots 44 Overall models output consists cognitive chunks 28 organized threestep hierarchy colour bar score ii scatter plot iii contribution plot The derivation model associated source code available httpsgithub com digital ECMT CORONET _tool 23 Study design The study follows withinsubject design participant uses tool conditions Firstly partic ipant provided information form recommended action score scale 03 later referred CS CORONET score told higher patients score severe predicted outcome Secondly participant provided recommendation CORONET score explanation tool arrived score patient later referred CSExp CORONET score plus explanation This approach minimises random noise introduced variability users clinical technological exposure previous experience The order presented cases participants The study design depicted Fig 4 The online questionnaire consisted stages introductory questions referring HCPs background expecta tions ii ﬁve artiﬁcial patient cases recommendation provided iii questions evaluating models usefulness iv ﬁve artiﬁcial cases time provided recommendation explanation v questions evaluating models usefulness iii vi overall impressions tool The questions designed speciﬁcally study Ten artiﬁcial patient case scenarios constructed reviewed approved senior oncology fellow sultant oncologist The decision admit discharge based clinical guidelines best practice The scenarios similarly structured comprised introduction patient including demographics presenting complaint relevant past medical history 10 cases detailed supplementary material patients parameters observations vital signs blood test results 6 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Table 1 Characteristics 23 healthcare professionals participated experiment Age group 2130 3140 4150 5160 Advanced Nurse Practitioner higher Consultant attending physician Doctor 24 years graduating specialty trainee ST fellow Doctor ﬁrst year graduating FY1 intern General practitioner GPST community doctor Pharmacist Registered Nurse Specialist Nurse Specialist registrar ST 3 senior residentsenior fellow n 6 10 3 4 n 2 5 2 1 3 1 1 1 7 How comfortable feel new technology Knowledge management patients cancer developed COVID19 Median minmax 6 37 5 27 Two patient cases Daniel CS Christine CSExp scenarios intentionally built way model wrongly discharge patient CORONET considers features COVID19 severity 37 oncological emergencies concomitant neutropenic sepsis In cases healthcare professional able apply clinical judgement recognise patient require admission oncological reasons subsequently override models decision In Daniels case admission renal dysfunction tumour lysis syndrome following chemotherapy received days prior diagnosis diffuse large B cell lymphoma Therefore scenario CORONET thought potentially wrongly discharge patient sideration oncological emergencies recommendation In Christines case admission recent chemotherapy course Again CORONET types infections treatment related presentations situation CORONET adequate making accurate decision These cases included assess overreliance model automation bias We analysed concordance clinicians decisions models recommendations correct actions approved team experts We tracked time spent questionnaire ensuring quickly skipped user lost engaged external tasks It allowed tracking time spent decision patient case 24 Selection participants This research project ethically approved research ethics committee REC reference 20WA0269 Participants required clinically active healthcare professionals determined having patient contact 10 working hours currently working patients cancer presenting COVID19 This included senior doctors junior doctors physician associates pharmacists advanced practitioner nurses staff nurses Any participants failed meet inclusion criteria excluded Participants recruited email invitations NHS Trusts clinician networks social media groups Although CORONET available online time experiment best knowledge participants tool prior experiment The familiarity tool andor preprint 41 required Knowledge related ML computational methods statistics required 3 Results 31 Participants 23 healthcare professionals participated experiment levels experience expertise Table 1 The median knowledge managing patients COVID19 ﬁve seven scale evaluated direct question Most HCPs felt comfortable new technologies 35 participants perform statistical analysis work majority participants use online calculators scoring systems similar CORONET 83 65 Supp Table S1 32 HCPs want know contributing features uncertainty Uncertainty considered important contributing features The responses expectations MLbased DSS depicted Fig 5 87 2023 HCPs inter ested knowing features contributing models recommendation HCPs neutral opinions 91 2123 7 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 5 Responses questions related HCPs expectations ML model Q3Q5 relate directly expectation model transparency explanation Q3 investigates need global explanation Q4 asks local explanation individual patient Fig 6 Responses related HCPs easiness interpretation visual output CORONET convinces user accept models recom mendation considered models explanation individual recommendation important Apart knowing model recommends action uncertainty recommendation essential HCPs 96 2223 slightly agreed 43 1023 strongly agreed highest strongly agree proportion study Knowing mathematical framework model signiﬁcantly important localglobal explana tion models uncertainty p 0001 KruskalWallis test Supp Table S5 Intriguingly 73 HCPs interested mathematics model like associated model explanation uncertainty This points largely unaddressed research questions dialogue explanations safety properties representation uncertainty proxies risk assessment 33 Visual explanations easy interpret Explanation visualisations easy interpret contributed convincing HCPs accept reject models recommendation Fig 6 Supp Table S2 This aspect measured direct questions visualisation easy interpret ii convincing accept reject recommendation For majority HCPs colour bar easy interpret 83 1923 Interestingly CS score explanation presented 03 colour scale convinces 48 1123 HCPs acceptreject recommendation Among 11 convinced CS average response questions expectations high 57 We signiﬁcant difference ease interpretation colour bar scatterplot contribution plot difference persuasive power p 005 Supp Table S3 We ﬁnd signiﬁcant correlation questions related visual output knowledge management patients cancer developed COVID19 expectations MLbased DSS Supp Table S4 Thus perception visual output affected expectations competence task 8 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 7 Paired answers questions 23 Healthcare Professionals Each dot individual answer Lines grey change green positive change Likert scale red negative change No statistically signiﬁcant change observed pairwise comparison 34 Explanations signiﬁcantly impact decisionmaking In pairwise comparison responses CS CSExp scenarios ﬁnd statistically signiﬁcant change HCPs attitude model ia satisfaction trust understanding reassurance The results summa rized Fig 7 Fig S1 Supp Table S6 The explanation improve satisfaction RQ2 Fig 7 question A trust RQ3 Fig 7 questions B C E standing model produced recommendation RQ1 Fig 7 questions D F G Of note observe slight signiﬁcant positive change p 0056 help cases I conﬁdent decision proceed The majority HCPs replied positively 5774 answers Slightly agree quest AC CS output We established following associations positive responses HCPs expertise expectations The lower expertise helpful tool appeared explanation provided r 0482 p 002 Fig S2 The higher need knowing contributing features helpful CS output likely making safe decisions r 0653 p 0001 Fig S3 cases HCPs conﬁdent r 0553 p 0006 Fig S3 Additionally convinced colour bar correlated satisfaction reassurance CS scenario r 0484 p 0019 r 0527 p 001 Fig S4 35 Model explanations adverse effect HCPs CSExp output model led positive negative changes satisfaction helpfulness model reassur ance understanding 34 HCPs satisﬁed CSExp CS 23 satisfaction decreased RQ2 9 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 8 Concordance decisions respondents correct action approved team Higher better A correct decision admit D discharge Daniel Christine orange bars cases CORONET recommends incorrect discharge Table 2 Time spent individual decisions cases 15 CS 610 CSEx p values Mann WhitneyU Test signiﬁcance ns non signiﬁcant p 005 p 005 p 001 p 0001 Time s median Q1Q3 Cases 15 output CS Cases 610 output CSExp p Signiﬁcance Action recommendation 48 3474 38 2264 0042 Action recommendation 61 3985 84 50116 0170 ns p Signiﬁcance 0097 0000 ns Higher satisfaction weakly correlated ease interpretation contribution plot r 0436 p 0038 correlated convinced p 024 HCPs convinced contribution plot CSExp scenario 65 1523 showed high levels satisfaction average 54 explanation average 56 This true scatter plot 26 HCPs said tool CSExp helpful making safe clinical decisions 26 stating opposite There positive correlation r 063 p 0001 padj 0135 change satisfaction change help making safe decisions 22 HCPs felt reassured CSExp scenario 35 felt opposite The provided explanations change level understanding model produces wrong recommendations 57 1323 HCPs RQ1 For 17 explanation led lower understanding This attributed time spent patients analyses ease interpreting diagrams p 005 The change understanding wrong recommendations associated change reassuring utility tool r 0762 p 0001 padj 0003 strongest correlation changes Supp Table S7 The highest percentage 52 positive changes observed help cases HCP conﬁdent Among HCPs rated knowledge managing patients COVID19 2 RQ5 However 22 CSExp helpful 36 Overreliance models recommendation leads wrong decisions There cases HCPs correct decision cases concordance correct decisions 80 Fig 8 Of note concordance depend patient admitted tool recommends correctly average 88 vs 87 p 073 However lowest concordance correct decision observed cases CORONET recommended wrong action RQ6 Fig 8 In Daniels case 65 1523 HCPs decided discharge CORONET recommended Christine accounted 48 1123 As level complexity diﬃculty decisions cases 1 10 similar argue lowest concordance Daniel Christine caused wrong recommendation provided model The results strongly suggest types output CS CSEx HCPs overrelied recommendation provided model 37 Quicker decisions overreliance explanations Decisions agreement model recommendation required time RQ6 Table 2 Fig S5 For cases 15 tool provides CS median time spent case 48 s user agreed tool 61 s For cases 610 CSExp provided observed statistically signiﬁcant difference deci 10 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 Fig 9 Positive feedback CORONET evaluated end experiment Fig 10 Evaluation clinical utility tool Same question asked twice CS CSEx No signiﬁcant change p 005 sions required 38 s decisions aligned 84 s median values decisions opposed tools recommendation p 0001 C L E S 0754 MWU test As described previous section respondents needed time deciding cases explanation delivered Further investigation agreement model reveals HCP agrees recom mendation RQ7 combined CSExp output leads signiﬁcantly quicker decisions CS output 38 s CSExp 48 s CS p 004 C L E S 059 Moreover explanation provided takes time decide model contradicts users decision difference CS signiﬁcant Median time CS output 61 s compared 84 s CSExp p 017 MWU test 38 Positive feedback attribution explanatory model General feedback CORONET model positive Fig 9 The model supporting interface considered easy use majority respondents recommend tool colleagues We impact explanatory component clinical utility model Fig 10 explanation improve attitude use CORONET clinical practice RQ5 p 005 Wilcoxon signedrank test 39 Respondents engagement Our experiment supported online questionnaire completed direct supervision time limits Although questionnaire estimated require 30 min actual average completion time 19 min median 16 min Fig 11 Of note HCPs completed 15 min spending min deciding 10 patients We identiﬁed anomalies time spent particular sections manually curated details supplementary material Most likely caused occasional interruptions inherent experiment conducted clinical setting respondents distracted urgent matters Overall ﬁnd signiﬁcant difference total time spent deciding patient cases CS CSExp p 0846 Wilcoxon signedrank test Fig S6 However 56 1323 HCPs spent time CSExp provided We ﬁnd correlation decrease aspects investigated previous paragraphs 11 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 4 Discussion pragmatic clinical embedding explainable ML models Fig 11 Total time spent 23 Healthcare Professionals CORONET explanation existing taxonomies The empirical analysis conducted study ﬁts domain expert experiment exact application task applicationgrounded evaluation real humans real tasks according taxonomy introduced 28 In designing CORONET output followed perceptive interpretability framework 45 emphasising immediate interpretation know We argue contribution plot follows assumption According taxonomies established 5 19 output tool investigated study delivers modularity interpretable element output cognitive chunk appears separate visual component In contribution plot bar delivers meaningful portion information interpreted independently low contribution albumin level recommendation admission relevance provides insight particular audience case clinicians seeking patients characteristics critical decision contribution plot patient question located cohort scatter plot Additionally colour bar score severe patients outcome expected However CORONETs output deliver simulatability model simulatable user reason simulate model produces output arbitrary input It possible assuming limited complexity model algorithms decision tree set rules However CORONET uses Random Forest regression model multiple 100 decision trees arrive prediction user precisely elicit formal decision process unambiguity CORONET provides local explanations focusing individualised patient care Based possible reveal model behaves parts feature space Such behaviour investigated model derivation 16 support dependency plots However presented user context study Analogously unambiguity user able evaluate descriptive accuracy 5 measures relationship learned model reﬂected explanation Such evaluation performed model development intentionally excluded cognitive chunks presented user We argue additional cognitive load hamper beneﬁt information 4624 Characterising HCPs attitude explanations More 87 participants stated knowing contribution features important overall model individual recommendations The important aspect highest number strongly agree answers know models uncertainty This suggest HCPs selfreportedly comfortable assessing risks uncertainty interpreting feature contribution output ML model Hence delivering models uncertainty contribute building trust comparison breakdown explanation features contributions However conclusion solely based proportion strongly Agree ﬁnd signiﬁcant differences tween expectations uncertainty feature contribution individual recommendation p005 KruskalWallis test Supp Table S5 An intuition mathematical principles model perceived signiﬁcantly important p005 Showing uncertainty increases trust likelihood following model prediction 26 low cognitive load setting 29 Limited cognitive investment model interpretation result reduced understanding 12 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 output While model study explicitly provide uncertainty prediction recommends binary action based numeric score threshold shown HCPs We argue serves user proxy models recommendation conﬁdence Unmet need build pragmatically grounded explanatory models Although majority HCPs 78 1823 explanation component accounting features contributions easy interpret 17 HCPs explanation model diﬃcult RQ2 This points opportunities design optimisation explanatory models point view enduser interaction Contemporary explanation methods SHAP LIME focus calculation faithful explanation model abstracting away pragmatic aspects explanation interaction domain experts Recent work binding representation design cognitive models 4748 provide formal avenue designing pragmatically eﬃcient explanatory models Explanations form feature importance act safety feature drawing users attention features omit ted initial judgement marked highly relevant model 49 According 50 featurebased explanatory models groups desired expected included model ambivalent user indifferent included prohibited features expected omitted following ethical ML principles Display ing contributing features allows user verify model utilises expected features instead inconsistent irrelevant features based supporting domain knowledge In study asked HCPs explicitly model included desired features Additionally prohibited features recognized model One downsides feature importance lead conﬁrmation bias model derivation phase 51 posthoc interpretation output Practically caused noticing features conﬁrm previous assumptions overlooking potentially incorrect features Ambiguity utility perceived value explanations A considerable number HCPs change worse attitude model explanation provided RQ1 57 reported level understanding 17 understanding model provides recommendations different Satisfaction remained 43 23 satisﬁed explanation A possible explanation information overload cognitive effort effect simply meeting expectations explanation model For 48 26 HCPs model explanation considered terms support making safe clinical decisions helping ambiguous cases respectively For 26 22 explanation decreased helpfulness As high 35 felt reassured model provided recommendation initial decision We hypothesise explanation introduce uncertainty clinical judgement differed clinical assessment particular cases This highlights previous adoption barriers MLbased decision support tools lack agreement model lack knowledge model HCPs attitude model 365253 Despite widespread notion need safe explainable models study step models remained unnoticed negatively perceived half participants RQ3 As suggested 53 initial scepticism MLbased models caused rejection This implies explainable ML researchers algorithmcentric perspective 45 pragmatic perspective prioritising pragmatic embedding experts decisionmaking process Feature contribution wellreceived data science community necessarily acknowledged HCPs Intensifying dialogue creating feedback loops explainable AI researchers domain experts essential development pragmatically relevant explainable ML models Although explainable ML advocated methodological silver bullet addressing transparency issues 1 user studies similarly paper evidence limited impact explanation performance task 54185557 This highlights direction research explanation expected depending context clinical setting Our study shows careful preconceived notions needs explained For HCPs explanations deliver critical understanding model Explanations improve critical understanding model recommends different action HCPs 57 participants RQ4 One possible reason misunderstanding difference local global explanations The barplot visualise feature importance shows magnitude contribution feature recommendation individual patient For analysed case bars reordered This lead extra complexity interpretation shows simplistic design standard explanation visualisation devices However promoting individualised patient care expect higher variation local explanations model Second tool oversimplifying complexity patient refer underlying deep biological processes explain biomarker relation recommended action The tool focused aspect patient scenario COVID19 ﬁnite set predictors This overlooked HCPs limitations model remained unidentiﬁed explanation provided Explainable models help HCPs address ambiguous cases The highest number positive changes CS CSExp observed support clinical cases HCPs conﬁdent decisions RQ5 Interestingly experienced HCPs tended trust model This conﬁrms 13 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 ﬁndings 58 59 experienced clinicians likely rely recommendation changing initial decision This suggests main function MLbased datadriven clinical decision support CORONET helpful user uncertain pragmatically positioning model clinical workﬂow 60 This aligned direct feedback received HCPs Supp 5 The recommended action explanation deliver new evidence justiﬁcations ambiguous cases Explainable models help HCPs acquire new domain knowledge When user lacks consolidated domain prior knowledge explanation act guideline source new evidencebased knowledge pointing second function explainable ML models RQ5 In 20 explanations impact HCPs felt insuﬃcient knowledge achieve given task signify knowledge gap addressed models output Quicker decisions explanatory models contrast blackbox models 57 1323 HCPs spent time decisions cases 610 cases explanations information delivered endusers RQ6 Potential reasons lost experiment ii gained ﬂuency interpreting provided information After ﬁrst ﬁve cases HCPs familiarized layout decryption able decide upcoming cases quicker iii provided explanations expedited decision overreliance tool The questionnaire designed verify points ii leaving point ambiguity Cases 610 reﬂect level diﬃculty cases 15 Models explanations increase conﬁrmation bias The results suggest recommendation agrees initial decision HCPs decide quicker compared model disagreement setting RQ6 Furthermore explanations provided decision comparatively quicker When tool contradicts clinician explanations lead longer reﬂection decision This highlights risk conﬁrmation bias possibly caused explanation increases reassurance decision RQ7 This aligns results 1 users tend use explanation support justiﬁcation prior decision The authors users reinforced preexisting beliefs explanation supported abandon opposite Of note model accuracy affects likelihood adjusting prior beliefs The risk conﬁrmation bias particularly high explanation closely matches HCPs expectations 53 In cases explanation appear reassuring explain model 61 Evidence explanations reducing automation bias On hand explanations drove users reﬂect longer decision disagreeing model outcome RQ6 More time indicate cognitive forcing reported reduce overreliance model 24 At time reduce automation bias explanation reduce cognitive load 4634 Working memory constraints increased cognitive load lead higher uncertainty decisionmaking processes increasing automation bias This particularly true clinical settings multiple contextual factors affect reasoning process HCPs 27 Overreliance models recommendation Some evidence literature suggests models use explainability techniques hamper users ability detect error 56 The explanation excessively increase users conﬁdence algorithmic decision communicating false impression correctness rigour resulting decreased vigilance auditing output 6263 In addition 33 64 reported higher error rates decisions relied heavily automation resulting bias wrong action despite evidence available resulted user overriding model Automation bias linked higher cognitive load consistently higher accuracy model 46 familiarity task 20 In study identiﬁed overreliance model cases CORONET intentionally wrong RQ6 For incorrect recommendations HCP overridden decision admitted patient Only 35 CS scenario 52 CSEx HCPs took critical perspective model 41 Assumptions limitations study General concordance clinical judgement In assessment AI models decision support healthcare known challenge deﬁnition ground truth 65 decisions vary HCPs Thus panel HCPs recommended small number annotators In study correct actions deﬁned panel experienced oncologists Lack controlled environment An ideal environment HCPs participate facetoface setting Due pandemic social distancing measures study developed online setup In context devices individual HCPs controlled impacted consistency quantity usability feedback type device Despite developers CORONET designed interface crossdevice compatibility phones tablets computers undertaking method design able capture feedback CORONETs usability based mixture device types Due remote setting lack control potential additional factors impacted results However experimental design assumes performing task 14 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 HCPs familiarized admit discharge patient based detailed clinical description Thus argue participants good understanding task Rush completing questionnaire We argue attempting understand model building trust tool require substantial time commitment HCPs Designing study supervised better controlled better incentivises HCPs longer time commitment improve current study design Use simulated patient cases To preserve conﬁdentiality patients artiﬁcial patient case scenarios instead reallife data The cases constructed clinically consistent manner domain experts However away fact real life HCP undertake clinical observations patient account visible signs distress illness skin changes unusual responses patient examination ﬁndings possible reproduce artiﬁcial setting 5 Conclusions In addition acceptability predictive model 3266 pragmatic evaluation usermodel interaction key successful deployment MLbased recommendation tools healthcare The deployment depends explanationinterfaces 22 designed software developers greater extent shown study clinical performance evaluated biomedical experts initial attitudes biases This paper contributes pragmatic evaluation framework explainable ML models clinical decision support The study withinsubject design involving 23 healthcare professionals compared explainable ML model blackbox model Such relatively small number participants heterogeneity backgrounds levels expertise hinder drawing statistically robust conclusions However argue study points direction nuanced role ML explanation models pragmatically embedded clinical context results settle solid base research HCPs acknowledged role explanations safety trust mechanism Communicating uncertainty model emerged stronger requirement compared explanations Despite general positive attitude explanations signiﬁcant set participants 1735 undesirable effect explanations observed possibly increase cognitive effort Moreover explanations improve critical understanding model 57 participants ability detect error model We explanations increase conﬁrmation bias possibly accentuating overreliance model On hand explanations drove HCPs reﬂect longer decision disagreeing model outcome evidencing explanations possible mechanism reducing automation bias There strong evidence explainable models better supported HCPs address ambiguous clinical cases cases HCPs certain decision Also explainable models helped experienced HCPs acquire new domain knowledge This work points open research questions area including need pragmatic evaluation explainable models complex clinical workﬂows codevelopment models explanations domain experts better understanding dialogue model safety mechanisms explanations Funding This project received funding European Unions Horizon 2020 research innovation programme grant agreement No 965397 Funding developing CORONET online tool provided The Christie Charitable Fund 1049751 Dr Rebecca Lee supported National Institute Health Research Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Data availability Data available request Acknowledgements We like express great gratitude healthcare professionals participated experiment We appreciate voluntary devoted time limited time pandemic complete experiment This paper research possible signiﬁcant feedback delivered Appendix A Supplementary material Supplementary material related article online httpsdoi org 10 1016 j artint 2022 103839 15 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 References 1 Kevin Bauer Moritz von Zahn Oliver Hinz ExplAiNed impact explainable Artiﬁcial Intelligence cognitive processes SSRN Electron J ISSN 15565068 2021 httpswwwssrn com abstract 3872711 2 Zachary C Lipton The mythos model interpretability arXiv1606 03490 Mar 6 2017 3 Leilani H Gilpin et al Explaining explanations overview interpretability machine learning 2018 IEEE 5th International Conference Data Science Advanced Analytics DSAA Oct 2018 pp 8089 4 Wojciech Samek KlausRobert Müller Towards explainable Artiﬁcial Intelligence Wojciech Samek et al Eds Explainable AI Interpreting Ex plaining Visualizing Deep Learning Lecture Notes Computer Science vol 11700 Springer International Publishing Cham 2019 pp 522 ISBNs 9783030289539 9783030289546 5 W James Murdoch et al Deﬁnitions methods applications interpretable machine learning Proc Natl Acad Sci 116 44 Oct 2019 2207122080 httpsdoi org 10 1073 pnas 1900654116 ISSNs 00278424 10916490 6 Mokanarangan Thayaparan Marco Valentino André Freitas A survey explainability machine reading comprehension arXiv preprint arXiv2010 00389 2020 10 31234 osf io d4r9t 7 Andreas Holzinger et al Information fusion integrative crosscutting enabler achieve robust explainable trustworthy medical Artiﬁcial Intelligence Inf Fusion ISSN 15662535 79 Mar 2022 263278 httpsdoi org 10 1016 j inffus 202110 007 8 Scott M Lundberg SuIn Lee A uniﬁed approach interpreting model predictions Advances Neural Information Processing Systems vol 30 Curran Associates Inc 2017 httpsproceedings neurips cc paper 2017 hash 8a20a8621978632d76c43dfd28b67767 Abstract html 9 Marco Tulio Ribeiro Sameer Singh Carlos Guestrin Anchors highprecision modelagnostic explanations Proceedings AAAI Conference Artiﬁcial Intelligence vol 1 vol 32 2018 10 Satoshi Hara Kohei Hayashi Making tree ensembles interpretable Bayesian model selection approach Proceedings TwentyFirst Interna tional Conference Artiﬁcial Intelligence Statistics PMLR Mar 2018 pp 7785 11 Arnaud Van Looveren Janis Klaise Interpretable counterfactual explanations guided prototypes arXiv190702584 Feb 2020 12 Amit Dhurandhar et al Explanations based missing contrastive explanations pertinent negatives arXiv1802 07623 Oct 2018 13 Kazuaki Hanawa et al Evaluation similaritybased explanations arXiv2006 04528 Mar 2021 14 Daniel W Apley Jingyu Zhu Visualizing effects predictor variables black box supervised learning models arXiv1612 08468 Aug 2019 15 Heinrich Jiang et al To trust trust classiﬁer Proceedings 32nd International Conference Neural Information Processing Systems NIPS18 Curran Associates Inc Red Hook NY USA Dec 2018 pp 55465557 16 Rebecca J Lee et al Establishment CORONET COVID19 risk oncology evaluation tool identify patients cancer low versus high risk severe complications COVID19 disease presentation hospital JCO Clin Cancer Inform 6 2022 e2100177 httpsdoi org 10 1200 CCI 2100177 PMID 35609228 17 Yasmeen Alufaisan et al Does explainable Artiﬁcial Intelligence improve human decisionmaking preprint PsyArXiv June 18 2020 httpsdoi org 18 Samuel Carton Qiaozhu Mei Paul Resnick Featurebased explanations dont help people detect misclassiﬁcations online toxicity Proceedings International AAAI Conference Web Social Media vol 14 May 2020 pp 95106 httpsojs aaai org index php ICWSM article view 7282 19 Himabindu Lakkaraju et al Faithful customizable explanations black box models Proceedings 2019 AAAIACM Conference AI Ethics Society AIES 19 AAAIACM Conference AI Ethics Society ACM Honolulu HI USA ISBN 9781450363242 Jan 2019 pp 131138 20 James Schaffer et al I better AI expertise explanations Proceedings 24th International Conference Intelligent User Interfaces IUI 19 Association Computing Machinery New York NY USA ISBN 9781450362726 Mar 2019 pp 240251 21 Eoin M Kenny et al Explaining blackbox classiﬁers posthoc explanationsbyexample effect explanations errorrates XAI user studies Artif Intell ISSN 00043702 294 2021 103459 httpsdoi org 10 1016 j artint 2021103459 22 Andreas Holzinger Heimo Müller Toward humanAI interfaces support explainability causability medical AI Computer ISSN 15580814 54 10 Oct 2021 7886 httpsdoi org 10 1109 MC 20213092610 23 Antoine Hudon et al Explainable Artiﬁcial Intelligence XAI visualization AI predictions affects user cognitive load conﬁdence Fred D Davis et al Eds Information Systems Neuroscience Lecture Notes Information Systems Organisation vol 52 Springer International Publishing Cham 2021 pp 237246 ISBNs 9783030888992 9783030889005 24 Zana Buc inca Maja Barbara Malaya Krzysztof Z Gajos To trust think cognitive forcing functions reduce overreliance AI AIassisted decisionmaking Proceedings ACM HumanComputer Interaction vol 5 CSCW1 2021 188 25 Mark A Musen Blackford Middleton Robert A Greenes Clinical decisionsupport systems Edward H Shortliffe James J Cimino Eds Biomed ical Informatics Computer Applications Health Care Biomedicine Springer International Publishing Cham ISBN 9783030587215 2021 pp 795840 26 Sean McGrath et al When uncertainty matter Understanding impact predictive uncertainty ML assisted decision making arXiv 201106167 Nov 13 2020 27 Divya Ramani et al Examining patterns uncertainty clinical reasoning tasks effects contextual factors clinical reasoning process Diagnosis 7 3 Aug 2020 299305 httpsdoi org 10 1515 dx 2020 0019 ISSNs 2194802X 21948011 28 Finale DoshiVelez Been Kim Considerations evaluation generalization interpretable machine learning Hugo Jair Escalante et al Eds Explainable Interpretable Models Computer Vision Machine Learning The Springer Series Challenges Machine Learning Springer International Publishing Cham 2018 pp 317 ISBNs 9783319981307 9783319981314 29 Jianlong Zhou et al Effects uncertainty cognitive load user trust predictive decision making Regina Bernhaupt et al Eds Human Computer Interaction INTERACT 2017 vol 10516 Springer International Publishing Cham 2017 pp 2339 ISBNs 9783319680583 9783319 680590 30 Onur Asan Alparslan Emrah Bayrak Avishek Choudhury Artiﬁcial Intelligence human trust healthcare focus clinicians J Med Internet Res ISSN 14388871 22 6 June 2020 e15154 httpwwwjmirorg 2020 6 e15154 31 Taehyun Ha et al Examining effects power status explainable Artiﬁcial Intelligence users perceptions Behav Inf Technol 41 5 2022 946958 httpsdoi org 10 1080 0144929X 2020 1846789 32 Marzyeh Ghassemi Luke OakdenRayner Andrew L Beam The false hope current approaches explainable Artiﬁcial Intelligence health care Lancet Digit Health ISSN 25897500 3 11 Nov 2021 e745e750 httpswwwthelancet com journals landig article PIIS2589 750021 00208 9 fulltext PMID 34711379 33 Linda J Skitka Kathleen L Mosier Mark Burdick Does automation bias decisionmaking Int J HumComput Stud ISSN 10715819 51 5 1999 9911006 httpswwwsciencedirect com science article pii S1071581999902525 34 Kate Goddard Abdul Roudsari Jeremy Wyatt Automation bias hidden issue clinical decision support use Studies Health Technol ogy Informatics vol 164 Jan 2011 pp 1722 35 Heimo Müller et al Explainability causability Artiﬁcial Intelligencesupported medical image analysis context European vitro diagnostic regulation New Biotechnol ISSN 18716784 70 Sept 2022 6772 httpsdoi org 10 1016 j nbt 2022 05 002 16 O Wysocki JK Davies M Vigo et al Artiﬁcial Intelligence 316 2023 103839 36 Srikant Devaraj et al Barriers facilitators clinical decision support systems adoption systematic review J Bus Admin Res ISSN 1927 9515 3 2 2014 36 httpsdoi org 10 5430 jbarv3n2p36 httpsweb archive org web 20220303231827 https wwwsciedu ca journal index php jbar article view 5217 37 RJ Lee et al Longitudinal characterisation haematological biochemical parameters cancer patients prior COVID19 reveals features associated outcome ESMO Open ISSN 20597029 6 1 2021 100005 httpsdoi org 10 1016 j esmoop 2020 100005 38 Hannah Burke et al Biomarker identiﬁcation dynamic time warping analysis longitudinal cohort study patients COVID19 UK tertiary hospital BMJ Open ISSN 20446055 12 2 Feb 2022 e050331 httpsdoi org 10 1136 bmjopen 2021 050331 39 Anna Freeman et al Wave comparisons clinical characteristics outcomes COVID19 admissions exploring impact treatment strain dynamics J Clin Virol ISSN 13866532 146 Jan 2022 105031 httpsdoi org 10 1016 j jcv2021105031 40 Oskar Wysocki et al An international comparison presentation outcomes CORONET predictive score performance patients cancer pre senting COVID19 different pandemic waves Cancers ISSN 20726694 14 16 Aug 2022 3931 httpsdoi org 10 3390 cancers14163931 41 RJ Lee et al Establishment CORONET COVID19 Risk Oncology Evaluation Tool identify cancer patients low versus high risk severe complications COVID19 infection presentation hospital medRxiv httpsdoi org 10 1101 2020 1130 20239095 2020 42 Rebecca Lee et al CORONET COVID19 Oncology evaluatiON Tool use machine learning inform management COVID19 patients cancer J Clin Oncol 39 15_suppl 2021 1505 httpsdoi org 10 1200 JCO 202139 15 _suppl 1505 43 Jerome H Friedman Greedy function approximation gradient boosting machine Ann Stat 2001 11891232 44 Alex Goldstein et al Peeking inside black box visualizing statistical learning plots individual conditional expectation J Comput Graph Stat 24 1 Jan 2015 4465 httpsdoi org 10 1080 10618600 2014 907095 ISSNs 10618600 15372715 45 Erico Tjoa Cuntai Guan A survey explainable Artiﬁcial Intelligence XAI medical XAI IEEE Trans Neural Netw Learn Syst ISSN 2162 2388 32 11 Nov 2021 47934813 httpsdoi org 10 1109 TNNLS 2020 3027314 46 David Lyell Enrico Coiera Automation bias veriﬁcation complexity systematic review J Am Med Inform Assoc ISSN 1527974X 24 2 Mar 47 Daniel Raggi et al Dissecting Representations Springer International Publishing ISBN 9783030542481 Aug 2020 httpswwwrepositorycam ac 2017 423431 PMID 27516495 uk handle 1810 310207 48 Peter CH Cheng et al Cognitive properties representations framework Amrita Basu et al Eds Diagrammatic Representation Inference Lecture Notes Computer Science Springer International Publishing Cham ISBN 9783030860622 2021 pp 415430 49 Hilde JP Weerts Werner van Ipenburg Mykola Pechenizkiy A humangrounded evaluation SHAP alert processing arXiv190703324 July 7 2019 50 Himabindu Lakkaraju Osbert Bastani How I fool manipulating user trust misleading black box explanations Proceedings AAAIACM Conference AI Ethics Society AIES 20 AAAIACM Conference AI Ethics Society ACM New York NY USA ISBN 9781 450371100 Feb 2020 pp 7985 51 I Elizabeth Kumar et al Problems Shapleyvaluebased explanations feature importance measures Proceedings 37th International Conference Machine Learning International Conference Machine Learning PMLR Nov 2020 pp 54915500 httpsproceedings mlrpress v119 kumar20e html 52 Reed T Sutton et al An overview clinical decision support systems beneﬁts risks strategies success npj Digit Med ISSN 23986352 53 Theodore Evans et al The explainability paradox challenges xAI digital pathology Future Gener Comput Syst ISSN 0167739X 133 Aug 3 1 Dec 2020 17 httpsdoi org 10 1038 s41746 020 0221 y 2022 281296 httpsdoi org 10 1016 j future 2022 03 009 54 Jasper van der Waa et al Evaluating XAI comparison rulebased examplebased explanations Artif Intell ISSN 00043702 291 2021 103404 httpswwwsciencedirect com science article pii S0004370220301533 55 Ben Green Yiling Chen The principles limits algorithmintheloop decision making Proc ACM HumComput Interact 3CSCW Nov 2019 56 Forough PoursabziSangdeh et al Manipulating measuring model interpretability arXiv1802 07810 Aug 15 2021 57 Yunfeng Zhang Q Vera Liao Rachel KE Bellamy Effect conﬁdence explanation accuracy trust calibration AIassisted decision making Proceedings 2020 Conference Fairness Accountability Transparency FAT 20 Association Computing Machinery New York NY USA ISBN 9781450369367 Jan 2020 pp 295305 58 Kate Goddard Abdul Roudsari Jeremy C Wyatt Automation bias empirical results assessing inﬂuencing factors Int J Med Inform ISSN 13865056 83 5 May 2014 368375 httpswwwsciencedirect com science article pii S1386505614000148 59 Dawn Dowding et al Nurses use computerised clinical decision support systems case site analysis J Clin Nurs ISSN 13652702 18 8 2009 11591167 httpsdoi org 10 1111 j 1365 2702 2008 02607x 60 Qian Yang Aaron Steinfeld John Zimmerman Unremarkable AI ﬁtting intelligent decision support critical clinical decisionmaking processes Proceedings 2019 CHI Conference Human Factors Computing Systems CHI 19 Association Computing Machinery New York NY USA ISBN 9781450359702 May 2019 pp 111 61 Julius Adebayo et al Sanity checks saliency maps arXiv1810 03292 Nov 6 2020 62 Marzyeh Ghassemi et al ClinicalVis supporting clinical taskfocused design evaluation arXiv1810 05798 Oct 13 2018 63 Malin Eiband et al The impact placebic explanations trust intelligent systems Extended Abstracts 2019 CHI Conference Human Factors Computing Systems CHI EA 19 Association Computing Machinery New York NY USA ISBN 9781450359719 May 2019 pp 16 64 Ericka Rovira Kathleen McGarry Raja Parasuraman Effects imperfect automation decision making simulated command control task Hum Factors ISSN 00187208 49 1 Feb 2007 7687 httpsdoi org 10 1518 001872007779598082 65 PoHsuan Cameron Chen Craig H Mermel Yun Liu Evaluation Artiﬁcial Intelligence reference standard based subjective interpretation Lancet Digit Health ISSN 25897500 3 11 Nov 2021 e693e695 httpslinkinghub elseviercom retrieve pii S2589750021002168 66 Alex John London Artiﬁcial intelligence blackbox medical decisions accuracy versus explainability Hastings Cent Rep ISSN 1552146X 49 1 2019 1521 httpsdoi org 10 1002 hast 973 17