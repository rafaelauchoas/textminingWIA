ELSEVIER Artificial Intelligence 88 1996 143161 Artificial Intelligence Propagating imprecise probabilities Bayesian networks Gernot D Kleiter Institut fiir Psychologie Universitdt Salzburg Hellbrunnerxtr 34 5020 Salzburg Austria Received March 1995 revised February 1996 Abstract networks exact probabilities Often experts incapable providing likewise networks based small preliminary probabilities probabilities probability uncertainty probabilities The problem propagate point probabilities network Bayesian networks use beta Dirichlet distributions samples In cases handled second order Bayesian propagate Dirichlet distributions problem imprecise The It convenient transformed distributions imprecision express order probabilities It shown propagation Dirichlet distributions Bayesian networks incomplete Dirichlet distributions Ap second order probability density functions obtained probability mixtures betabinomial data results proximate stochastic discussed use examples An important property increases pruning criterion small Thus imprecision Ockams razor Bayesian networks simulation A number properties propagation imprecise probabilities imprecision inferences rapidly new premises added argument The imprecision number variables inferential argument network involved 1 Introduction Bayesian belief networks represent process probabilistic knowledge Their rep resentational components belong domains qualitative quantitative Thanks Fonds zur Ferderung der wissenschaftlichen Forschung Vienna financial support Thanks hospitality Department Psychology Bowling Green State University Ohio especially Michael E Doherty Email gemotkleitersbgacat 00043702961500 Copyright 1996 Elsevier Science BV All rights reserved PII SOOO437029600021S 144 GD KlerterArtcrcd lntrlligencr RR I 996 143161 Fig I Bayestan network Cooper medical diagnosis example Table I Weight tables associated Coopers example I numbers choosen conditions ratios weights preserve probabilities original version Coopers example Rg fullfilled ii total sum elementary weights 120 A ytl t 96 24 B A h h 0 77 I9 ii 5 I9 C A 70 Y c I 5 I 19 5 E C 7r Y yc 44 66 2 8 probability dependence distributions conditional relationships like diseases organized conditional involved specific metastatic associated quantitative Fig I Table 1 73437 specifications Assume investigated nodes graph The tables visible example shown dependencies variables independence visual language graph theory The quantitative The basic qualitative variables expressed specifications tables attached ical representation Consider Fig 1 represents network represent clinical absentpresent I contains patients suspected suffering 24 actually developed metastatic metastatic 77 These remaining main purpose Bayesian belief network patient variables observed known certain probabilities ties propagate diagnosis prediction explanation Bayesian network Bayesian belief networks belong tic models intelligence graph The graphical model The nodes AE test results symptoms Table 120 form cancer It turns total serum calcium 5 Of patients total serum calcium Table 1 The If affects states graph Evidence updated probabili like medical tutorials related work uncertainty www auai org page references given class graphical probabilis artificial special cases propagating probabilities 6 1 1 153043 http network Various kinds probabilistic form 96 Of having perform probabilistic form 19 increased form observed 19 increased neighboring metastatic contained frequencies inference inference GD KleiterArtificial Intelligence 88 1996 143161 145 I 10 20 30 40 50 100 Probability Fig 2 The beta distribution Be4414056 example 99 highest density interval Coopers Usually probabilities Bayesian networks treated known instead Probability precisely In present paper analyse Bayesian networks known precisely Experts provide exact point probabilities intervals estimates derived small sample considered bounds 17212526333940 empirical data based Bayesian network proposals lower upper precise point values handle probabilities providing 83 1381 second order distributions In literature probabilities In cases dependency variances propagation provided imprecision structures 91242 A tutorial 161 sizes function statistics uncertain We treat probabilities known precisely attached The distributions distribution way treated 31 uncertain quantities second order probability If little known If distribution small The use second order Bayesian statistics distribution exciting The procedure actually goes Thomas Bayes He probability density Bayesian density known probability especially plotted continuous upsidedown function beta distribution tight variance standard procedure unit interval imprecision variance flat quantity express large The method proposed paper allows derivation following inferences If metastatic fall comas d suffers severe headaches probability imprecision associated estimate We 99 sure true lies interval 00134 0227 The standard deviation estimate e patient intermittently e appreciable probability 00436 The imprecision expressed Be 4414056 brackets shorthand notation probability density al probability given function parameter beta distribution 0098 However corresponding d cancer 146 GD KlrrterArtijicid intelliwcr 88 1996 lJ161 corresponds content 099 Further analysis example inference hascd total sample size 120 cases precision e While sample size 45 cases Fig 2 shows present interval interval beta distribution severe headaches probability d essential mean 0087 standard deviation 00387 The precision compared D E instantiated We come sight counterintuitive 99 highest density shows metastatic increases D instantiated leads distribution cancer Conditioning Be4504744 slightly shortest inferences property nld 2 Basic model nodes variables V set directed edges E variables defined V x V The vertices DAG With node X E V set U If X I If arcs contain cycles U associate weight table X 1t graph G l VE dependencies set vertices directed acyclic graph We consider arcs probabilistic edges represented graph parents paX U twodimensional weight table denote W Xi X2 More generally subset ZI marginal We follow marginal distributions 141 denote probability X Idimensional weight table denote Xl X X2 Z C X marginal X U X Xzznf pdf brackets Joint respec written X Y XjY Xl denoted X Y XY Y The Xlpa _u variable X We conceive table shape parameters Dirichlet distributions Dirichlet dis defined binary cases beta distributions second order pdfs function density special versions tables define conditional tively The product densities weight weights tributions follows Definition 1 Dirichlet distribution plex S reals vector given VI 0 y K 0 yt Let Yl I dCfy XI random vector sim 6 I v vg vector I If density random Y O d D PYIvd fVl tvf I TVll I 1 _I I _ I I I 1 U Y Yd Div Yd follows dvariate Dirichlet distribution We use shorthand v Definition 2 Beta distribution density given Let Y random variable unit interval If 2 GD KleiterArtcinl Intelligence 88 1996 143161 147 Y beta distributed shape parameters VI 2 We write Be vtv2 Its mean variance given short Y EY VI v2 varY VIP2 VI v22v v2 1 4 In beta distribution interpret available proposition extensively point probability VI vt 9 VI weight hypothesis discussed Keynes v2 weight 20 sum vt v2 total evidence favor event Weights evidence The beta Dirichlet distributions underlying parameters probability pdf marginal dimensions number possible values parents order mc x ml x x m distribution implement network If node n parents second order pdfs If node X parents table 1 table weight If number possible values node consideration weight ml m vector underlying continuous parameters represents The probability probabilities 1 Dl dimension random variables Of course represented hidden den variables variable D possible values attach parent probability dimensional cause add The second order probability vector weight propositional Because lationship hidden variables twin nodes unneccessarily network directly observable graph Bayesian network hid random nodes To discrete propositional l lost hidden nodes children discrete states child nodes equal values hidden variables P xirri r graph weighted Bayesian network Drawing Dirichlet distribution table node The relationship redundant variable twin hidden variable hidden variable propositional It specified numbers conditional probabilities propositional complicate graph drawn redundant distribution D variables contained variable The space defined hidden probability variables domain propositional respects Bayesian belief networks It subspace space network The Dirichlet distributions The qualitative graph probabilities propagation uncertain parameters Methods learn structures data described Heckerman Geiger Chickering taken uncertain quantities posterior densities taken sure The numerical independencedependence Bayesian subspace possible variables consideration independencies conditional treated Bayesian posterior distributions visually structure underlying specifications conditional represented known sure We investigate networks given structure prior knowledge 171 If weight tables obtained database missing cases ables conditional distributions frequency joint distribution counts big complete domain vari Dirichlet 441 The propagation probabilities 14X GD KleiterArtijicinl lntellipwce X8 1996 143161 performed usual methods propagating point probabilities weights Dirichlet distributions probabilities sources combined jective expertize We introduce concepts solution resulting total sample size taking expected frequencies Completeness practical applications A database contain objective data sub helpful obtained simply multiplying strong assumption approximate incomplete data It violated 3 Natural children natural neighbors In complete contingency table marginals statistics marginals sums corresponding hold weights tionship cells member distribution With additive weights beta Dirichlet distribution distributions What shall In inferential The treatment missing data related averages complete weighted space missing data This results purposes expectation maximization viding maximum variance In large networks multiple II We combine estimates approximated iterative algorithm complicated algorithms local maxima difficult local noniterative solutions likelihood estimates EM cell counts A similar weighted network The case perfectly dimensions equal relationship rela treatment additive allows easy mathematical member distribution stay family probability case occurs data missing cell counts marginals add 29 complete case calculating performed 261 For computational solutions averaging probability mixtures Incomplete data usually analyzed IO EM iterative procedure pro presence missing data The precision 291 EM disadvantages slow Furthermore global maximum large networks estimation Gibbs sampling The probabilities node network depend states neighbor nodes blanket The estimation data conditional cell weights marginals We introduce parents natural neighbors Markov conditional Markov blanket missing iteration We use 6 method estimate means variances relationships concepts natural child natural probabilities We definition additive probabilities Definition 3 Natural child Let Y XX E paY parents pa Xk 1r marginal weights Xk table XkpaXk paXk identical parents paY Xi let U Y natural child X I pa Y Ypa Y Xk table X paY ltZPaYXkJ XkpaXklPaXkI 5 If X parents Y natural child X xX1 X GD KleiterArtificial Intelligence 88 1996 143161 149 Fig 3 The plates drawn A C indicating C middle known A A natural child C left known simultaneously Visually represent parent 51 gives credit Spiegelhalter instantiated natural child plate A plate rectangle drawn set nodes repetition number N written left lower Plates corner Plates introduced Buntine indicate data set kind The set nodes repeatedly N observations A plate shows complete set N data Often N weight table In left panel Fig sample size total sum weights nodes A C A node 3 plate repetition number NAC drawn table Y nonnatural child parent containing parents table containing Y parents Y We case parents second case information information cases sample observations child This case example missing additional In middle panel Fig 3 extra cases available plate drawn node C indicate NC additional observed cases C More known C A In right panel extra plate drawn A More known A C marginal weights larger ii smaller natural It follows children If parent children taking parents children If parent natural children repetition numbers parent natural children single plate identical We parent natural respect accountare parents repetition numbers identical parent children single plate In left panel Fig 4 node E natural children A B C The children natural parents D F A B C D E F single plate The parents node children parents children called neighbors Markov blanket node The probability distribution states node depends state nodes Markov blanket The condition Markov blanket builds plate important natural respect It follows Definition 4 Natural neighbors A node natural neighbors respect children natural parents Markov blanket GD KleterArtficinl ntelliptwr X8 1996 143161 Fig 4 The plates drawn E natural children A I C represented single plate drawn A R C D E F 4 Member distribution To propagate second order pdf weighted graph use stochastic simulation theorem called generalized It turns stochastic simulation Bayes Bayes theorem Bayes parameter tained Bayes parameter extend corresponding generalized Bayes 34 plays central theorem 2124261 We use previous role We investigated second order pdfs results ob slightly general situation 41 Member parameter Consider disease A present absent symptom B Y marginal base rate probability A present present absent Assume conditional I present patient suffers disease A given Bayes symptom probability symptom B given disease conditional p2 If observe patient showing symptom B probability symptom given symptom probability disease present theorem 6 We U member probabilities LY pt p2 imprecise imprecision beta distributions It Bayes theorem parameter parametric form If expressed LY Beala2 PI Beh1012 PZ Beb211322 7 want infer distribution member parameter evidence al 2 611 012 b21 b2 We distribution The member distribution precise posterior probability second order pdf defined LL given B weights member distribution Bayes formula It tells GD KleiterArtial Intelligence 88 1996 143161 151 42 Beta mixed beta member distributions Let A B binary random variables We shown following theorem 2326 Theorem 5 Natural child 1 If CX pt LL probability parameters underlying propositional variables A B A B 17 A A b respectively second order pdfs parameters Y Be al a2 I N Be bl I b12 p2 N Beb21b22 parameters I p2 independent B natural child A bl b12 a2 b21 b22 2 3 pdf Bayes parameter t_ given u Be b 1 b21 That B natural child A member distribution parameters table A needed directly read Bs weight beta distribution table Note weight natural parents The theorem directly generalizes locate cells weight tables We finally denote corresponding notation cumbersom We assume nodes binary use upper case characters A B _A S7B99 denote node variables We use lower case characters like AB b P denote cell AiB weights A TA la Using notation Theorem 5 reads If A Be A TA Alb Be Ab TAb node value bl cell indexed variable node value A instantiated node values parents bl BIA Be TAB theorem instantiated nodes We use conjunctions AB In following Abl f 6 denotes BIA Be AB weight TAB children B underlying probability parameters independent Dirichlet If A natural neighbors Theorem 6 Natural neighbors Bl distributed Abbl BeAblfb AbfbAblfbl Abfbn 8 In case weights evidence additive member distribution marginals known conditional probabilities distribution Two cases distinguished 261 The mixing weights follow PolyaEggenberger mixture beta distributions probability known probabilities disease In case know al bll b2 The difference D al bl I b2 positive D conceived number missing data present know number cases know disease symptom al bll b12 In disease case case ii In second case know conditional example presence conditional symptom probabilities symptom probabilities conditional example marginals presence IS2 GD KlrerArtfkicd fnfeiience 88 1996 143161 marginal probabilities statistics random situation arise contingency CII cases fixed experimenter statistical 01 I 0 12 cases In inferential marginals blI b12 al cases For inferences probabilities missing data predicted probabilistically future sample given observed shown predictive distribution distribution distributions mixing weights PolyaEggenberger 191 The member distribution analysis For inferences In statistics called predictive distribution I 21 It cases PolyaEggenberger turns probability mixture beta probabilities 261 table sampling remaining D al cases symptom second case conditional probability distribution Theorem 7 Nonnatural neighbors I If PI U probability parameters underlying propositional variables A BJA BITA Ab respectively second order pdfs 32 N thefirst parameters N Benluz 31 N Bebllblz 2 I independent pdf Bayes parameter JL Beb21h given bl lxlX 11 1 mnx I 1 pc c djmindf d2mind PEDIbil s1dlb12 DI sldl x PE Dz b21 f s2d2bzz k D2 s2d2 xBebl sldb21 s2d2 9 Let Bi bl b Di lai B Then range dl d2 constrained mindi 0 maxOb D B il u 8 a1 B maxdi Di minblD f B Bj s 1 0 1 B B B The PolyaEggenberger distribution defined follows Definition 8 PolyuEggenberger distribution given distribution Let Y discrete random variable If gg Isg2s g Y 1s ghghIsghn 1s x hh Ish2shny Is 10 PolyaEggenberger distribution write Y PE n g h GD KleiterArtijcial Intelligence 88 1996 143161 153 For s 1 PolyaEggenberger distribution equivalent betabinomial distri 2 s 0 binomial bution distribution For details refer 261 In general structure obtained linear programming observations calculation Below employ approximation s 1 hypergeometric constaints In large network containing missing exact beta mixtures cumbersom based S method 5 Stochastic simulation The use stochastic simulation 181 shown stochastic simulation Bayesian networks proposed Pearl Bayesian network It extensively employed Bayesian networks 34 special 13 Hrycej case Gibbs sampling 363941 At start instantiated node set arbitrary value Then clamped constant value non steps following iteratively node means mp node alphabetical order node names Select nonclamped Compute values node given current values neighbor nodes In Bayesian network containing obtained generalized Bayes theorem variances var p member distributions local probabilities point probabilities instantiated performed 1 2 PXlrestx KPXlPanPYlfx 11 jl represents X fix factor pax means distributions parents yj restx second order pdfs use completely The upper case letters refer random nodes lower case letters instantiated nodes K normalizing children X For determine calculated 6 method Determine ability value Build value current node new value node selecting random number The prob parents X yi variables formula sum means variances member distribution mean member distribution node The variances described value represents analogous equal 3 4 The sums averaged finally beta distributions Bep q fitted means m u variances var u p q obtained N mpl mp VMF 1 pmxN qNp 12 It interesting We turn determination note central variances step 2 role Bayes theorem plays stochastic simulation 51 The 6 method Gibbs according incomplete employ possible distribution data Principally turn determine Gibbs sampling allows propagation order probabilities Bayesian networks random example The random child twolevel sampling process instead We directly employ variance estimates obtained In section obtain second order densities At hidden nodes generate probabilities probabilities nodes This lead computationally We use shortcut node calculate node given incomplete data law beta distribution state probabilities expensive precision second order distributions variances second order distribution method obtain natural nonnatural neighbors Nonnatural associated sampler correspond neighbors Eq 6 U introduced The member parameter nonlinear function g variables LY 3t 2 For parameters pdf known Can derive mean variance member parameter For linear functions g random true g linear In variable X EgX cases mean variance ofg X approximated S method 14321 q ElX 1 This Definition 9 6 rule Let XI V E variances El partially differentiated f X1 asymptotically X X bc independent v If f Xi fE random variables means X function variables E respect El E normal mean Ex I x fEl E variance I varfx x CC dfE c I I J5 E 2 13 14 The mean variable X Bayesian network conditioned variables approximated generalized Bayes Bayes theorem form expectations applying formula expectation theorem 1 I We need rewrite state 13 generalized EXlrestxl KEXlpaxl Elyix I 1 15 The expectations member distribution estimates underlying obtained parameters The variance Eq 14 sum variance components probability varXrestxl varXlpax 1 f Cvar_lfjCa 16 GD KleiterArtijkial Intelligence 88 1996 143161 155 sum variance components sums variance components instantiated obtained applying 6 rule variable values parents The variance component jointly instantiated parents parents children proper consideration varXlpax 1 VXlpax 1 fEXlpalElflElfl WXlpax 1 2 17 similarily variance components children obtained Vryjlfj4 1 fEXpaxlEylflxlEylfxl EilfiWl 2 18 Building partial derivatives collecting terms finally leads following expressions varXlrestx I 1 19 20 21 22 n number children m number possible values X AEXlpaxll EXlpaxl Bi EiIfixll EyjIfixl D EXlpaxl fiELYifill j I EXlp4nlfi EYjjfjXl j I The E I terms easily obtained dividing b12 The VI1 variances beta distributions biibii b122 bl1 b12 1 1 The square bracket notation symbols stay close possible E Xrestx probabilities nodes X stands member parameter avoid symbols literature bl1 b12 611 greek point u node X given cell weights marginals 52 Program The propagation program written graph supported Raima Database Manager oriented supports performed C The navigation 351 This database network In definition processing directed graphs pointers IS6 GD KleiterArtiJiiciul Intellipxce RX 1996 143161 140 9642 81 2 346 745 319 716 306 60 1842 25258 8 261655 292 682 Fig 5 Simple chain base rate weights ij 140 1 60 conditional weights bin 98 7hlcl 42 JYJ 18 nlU 42 different respect simulation performed 1000 iterations relational databases For numerical examples stochastic Stochastic simulation work network close zero closer beta mixtures Especially neigbhors obtained directly means variances mixtures PolyaEggenberger look promising 71 We intend replace estimated probabilities 6 method method mean variances node given calculation rterms mixtures But requires determination weights direct programming formulas 6 Examples 61 A simple chair1 Consider chain 0307 graph consisting Fig 5 Denote consisting respectively Without nodes instantiated nodes I 2 3 4 nodes Cl G2 Gj G4 Gs Denote subgraph 140 respectively Assume 200 cases observed natural sampling conditions probabilities A cases 60 A cases At node probability 0703 distributed flatter marginal distribution u B C D E alG Be 14060 When B b clamped obtain course additionally C D ajh Gs Be9818 E clamped obtain alb 1 remains grow distribution If let reference mean 071 Note distribution Markov property aGs Be7932 alb Gz Be9818 conditional identical inferences included There slow learning beginning adGj Be8131 acGsl Be 98 18 The situation topdown GS eGs Be6158 Be8430 obtain Be8927 inferences elnGs Be6455 equivalent ecGs Be6848 distribution eldGg directly provided Be7231 instantiated bottomup finally ale Gs albGsl different The marginal e elbGsl eldGsl Be6158 Of course forward The long distance order probability 6 I 61 58 05 1 value practically equal 05 The value equal base rate e The long distance backward inference 8 I 81 31 072 E A noninformative A E noninformative probability respect results inference GD KleiterArtciul Intelligence 88 1996 143161 157 practically backward diagnosis cases precision E worse lnode identical inference base rate Prediction worst results forward base rate probabilities However nodes A B C D inference 5node containing containing A E respectively We recognize Markov property Bayesian network The distributions parent childthe decendent node chain depends predecessors grandchild additional equal independence structure The weight information The distributions ab Gs ajb G4 chain tables chain natural children The local member distri butions S method exactly Theorem 5 6 natural sampling There practically differences methods process determined stochastic approximately al b G3 simulation grandparent provide alb Gz If change given b changes divide marginal weights 14060 Be9818 Be479 That 3515 present example distribution marginal weights 4 divide conditional ones 2 62 A simple triangle Consider network Fig 6 Denote nodes Gs The marginal distribution uGI Be 14060 subgraph consisting subgraph consisting A marginal A B Gz graph A different 13 alGs Be3716 uG2 Be77 distributions ujbGx Be499 A given B graphs G G3 The distributionof ulbcG3 note distributions determined Generally observe invariant respect resulting weights evidence The limiting condition independent remain In highly connected This happened structural extension Then chain large cliques structures extensions larger systems conditional weights Gt denote consisting structures Accordingly ulbG2 Be9816 Be69 13 It important reference larger occurs second order distributions triangle loss imprecision variables independent inferences smaller structure If reduce marginal weights A 14060 A G3 Be22lO distribution precisehas We stated al b c Gs 22 decreased appreciably imprecision The distribution 35 15 marginal distribution Be3156 ubGg Be 397 The precision inferences Aor LY circumstancesincreases wise comparable complex We The tradeoff complexity networks minimum tradeoff illustrated example Can data uncertain problem description probabilistic systems strive inferential accuracy recently studied inferencesunder embedded systems simple Bayesian length criterion 271 From viewpoint looks terribly counterintuitive inferences Consider following 666 294 294 126 126 5 4 292 126 Fig 6 Simple triangle base rare weights I 140 70 60 d conditional weights bla 98 hlv 18 YW 42 weights itr h 686 COO 294 c17cl J 294 cltr 126 7 Bad news Imagine doctor remote brought After careful disease A However definite diagnosis laboratory blood tests expensive island One residents suffering technical equipment suspect investigation patient Since arrived island investigated 40 similar cases Later 30 cases actually A IO laboratory checking careful patient suffering A I What probability The probability limited 2 As experience 40 cases estimate absolutely precise Give confidence estimate I 90 sure true value probability interval lies For time thought diagnostic diagnosis A You 30 cases suffering symptom 2 I Of IO cases suffering 7 You realize I What probability patient patient diagnostic showing sign B sign B relevant A 9 showed A 3 showed symptom B suffering A given patient shows B The probability 2 Give confidence _ interval estimate I 90 sure true value lies Of course second point probability estimates 075 precise wider Assuming Your second estimate second interval expressed beta distributions distribution Be93 mean course confidence second distribution In example nondiagnostic flatter Its variance observed data distribution interval 075 90 confidence nondiagnostic data things worsewhich Be30lO 064086 interval larger The probabilistic uncertainty The mean The posterior The 056094 conditioning counterintuitive We GD KleiterArtcial Intelligence 88 1996 143161 159 additional new information assume We need principle necessarily Inferences noisy neutral arguments protects considering bad quality inferences data imprecision argument irrelevant data Nondiagnostic It increases 71 The resolution paradox In cover story describes reference Gr consisting disease A counts al 30 alGIl node associated a2 10 If assume improper prior BeOO distributed frequencies alGIl a21 Beala Be3010 In second cover story introduces associated counts bl I 9 b12 21 b21 3 b22 7 The resulting G2 nodes Let investigate case shows symptom B information symptom B reference Y G2 prior second node distribution alG21 alala2bllblb21b221 The order marginal probability estimates B estimated compound probability PB PAPBIA PlAPBIA 03 value known exactly If probability member distribution moment Lets assume 03 observe B probability observe Be93 probability 03 x 001442 07 x 000647 000885 P B 07 observe mean 2128 mean 912 variance 001442 Similarily member distribution Be217 variance 000647 The expected variance If fit beta distribution obtain alG2 Be 1413606 G resulting known exactly expected posterior distribution This As PB distribution Note mean distribution marginal distribution 2019 cases This half 40 total number cases effective symptom B 1B analysis 07 mean 1413 606 actually variability G2 leads member distributions A Gt Its precision GI Observing preposterior corresponds albGz Be93 albG2 Be27 respectively These distributions having clamped symptom b lb interpreted reference posterior distributions G2 respectively The distribution alGil aGz belong prior posterior distributions What makes problem counterintuitive Intuitively discard additional formation soon realized change probability estimate I60 GD KleiterArtijicicd Intelligence 88 1996 143161 aiGlJ situation information reduces irrelevant problem tend Thus conditionalize When discuss compare This misleading The conditioning prior posterior belong mation associated improved monotone knowledge precision We compare distributions CI information Gt accuracy change course We information We perform elementary pruning process GI G2 We Cz based information The additional infor principle better shows additional data decrease state knowledge We believe uncertainty The example sense equivalent information information protect Intuitively inferential aGz systems impact variables low diagnosticity order probabilities low positive Such loss second inferences There negative impact improvement precision systems order probabilities variables considerable tradeoff order precision References 1 I I J Aitchison The Statistictrl Ancwrs Contlitiorftrl Dotct Chapman Hall London 1986 12 1 J Aitchison IR Dunsmore Statistical Prrdictim Cambridge University Press Cambridge Arwysis 1975 I 3 J JM Bernard0 AFM Smith Btryesim Theory 141 YM Bishop SE Fienberg PW Holland Discrete Multivurime Wiley Chichester 1994 Analwis Theory Pmctice MIT Press Cambridge MA 1975 5 I WL Buntine Operations learning graphical models I ArtlJ Mel Rex 2 1994 159225 161 W Buntine A guide learning probabilistic networks literature data IEEE Trans Knowledge Duta Eng appear I7 1 RM Chavez GE Cooper A randomized approximation algorithm probabilistic inference Bayesian belief networks Nenuorks 20 1990 66 I685 18 I P Che RE Neapolitan J Kenevan M Evens An implementation method computing D Heckerman A Mamdami eds belief networks inferred probabilities uncertainty Uncertainty Artificial intelligence Kaufmann San Mateo CA 1993 292300 19 I G Coletti A Gilio R Scozzafava Conditional events vague information expert systems B BouchonMeunier RR Yager eds Uncertairzt Knowledge Bases Springer Berlin 199 I 106114 I IO AP Dempster NM Laird DB Rubin Maximum likelihood estimation incomplete data EM algorithm discussion J Roy Stat SM Ser B 39 1977 I38 fntroduction Graphiccd ModellinK I I I I C Edwards I I2 I KW Fertig JS Breese Interval influence diagrams H Henrion RD Shachter LN Kanal 5 NorthHolland Amsterdam 1990 149161 Intelligence I I3 I A Gammerman Z Luo CGG Aitken MJ Brewer Exact approximate algorithms Reasoning A Gammerman ed Probabilistic JF Lemmer eds Uncertainty Springer New York 1995 mixed graphical models implementations Articial Bayesian Beliqf Networks Alfred Wailer HenleyonThames 1995 33S3 I 141 AE Gelfand AFM Smith Samplingbased approaches calculating marginal densities J Am Stuf Assoc 85 1990 398409 I IS I P Hajek T Havranek R JirouSek Uncertain frzformarion Processing Experr Sysrems CRC Press Boca Raton FL 1992 I 161 D Heckerman A tutorial learning Bayesian networks Microsoft Research Advanced Technology Division Microsoft Corporation Redmond WA 1995 heckermamicrosof t corn GD KleiterArtijicial Intelligence 88 1996 143161 161 1 I7 1 D Heckerman D Geiger D Chickering Learning Bayesian networks combination knowledge statistical data Much Learn 20 1995 197243 1 181 T Hrycej Gibbs sampling Bayesian networks Art Intell 46 1990 351363 191 NL Johnson S Katz Urn Models Application Wiley New York 1977 1201 JM Keynes A Treatise Probability MacMillan London 1921 1211 GD Kleiter Bayesian diagnosis artificial intelligence Artif Infell 54 1992 l32 221 GD Kleiter Properties probabilistic imprecision B BuchonMeunier L Valverde RR Yager eds Uncertainty lnfelligent Systems NorthHolland Amsterdam 1993 155170 1231 GD Kleiter Natural sampling rationality base rates GH Fischer D Laming eds Contributions Mathematical Psychology Psychometrics Methodology Springer New York 1994 375388 24 1 GD Kleiter The precision Bayesian classification multivariate normal case Inf J General Syst 22 1994 139157 251 GD Kleiter Expressing imprecision probabilistic knowledge I Italian Stat Sot 2 1994 213232 261 GD Kleiter M Kardinal A Bayesian approach imprecision belief nets V Mammitzsch H SchneeweiO eds Symposia Gaussiana Proceedings 2nd Gauss Symposium Conference B Statistical Sciences De Gruyter Berlin 1995 91105 271 W Lam F Bacchus Learning Bayesian belief networks approach based MDL principle Comput Intell 10 1994 269293 281 SL Lauritzen N Wermuth Graphical models associations variables qualitative quantitative Ann Stat 17 1989 3157 291 RJA Little DB Rubin Sfatistical Analysis Missing Data Wiley New York 1987 30 131 RE Neapolitan Probabilistic Reasoning Expert Systems Wiley New York 1990 RE Neapolitan JR Kenevan Investigations variances belief networks Uncertainty Artijkiul Intelligence NorthHolland Amsterdam 199 I 232240 GW Oehlert A note delta method Am Stat 46 1992 2729 G PaaS Second order probabilities uncertain conflicting evidence PP Bonissone M Hemion LN Kanal JF Lemmer eds Uncertainty Artificial Infeligence 6 NorthHolland Amsterdam I991 447456 132 133 1341 J Pearl Probabilistic Reasoning fnrelligent Systems Morgan Kaufman San Mateo CA 1988 351 Raima Database Manager 1605 NW Sammamish Rd Suite 200 Isaaquah WA 98027 USA 1994 361 A Runnalls A survey sampling methods inference directed graphs P Cheeseman RW Oldford eds Selecting Models Dafa Springer New York 1994 153162 1371 DJ Spiegelhalter Probabilistic reasoning predictive expert systems LN Kanal JF Lemmer eds Uncertainty Arftjicial Intelligence NorthHolland Amsterdam 1986 4768 38 1 DJ Spiegelhalter A unified approach imprecision sensitivity beliefs expert systems MCR Biostatistics Unit Cambridge 199 1 391 D Spiegelhalter A Dawid S Lauritzen R Cowell Bayesian analysis expert systems Star Sci 1993 219283 401 DJ Spiegelhalter RCG Franklin K Bull Assessment criticism improvement imprecise subjective probabilities medical expert M Hemion RD Shachter LN Kanal JF Lemmer eds Uncertainty Artificial Intelligence 5 NorthHolland Amsterdam 1990 285294 4l A Thomas D Spiegelhalter W Gilks BUGS program perform Bayesian inference Gibbs sampling J Bemardo J Berger A Dawid AIM Smith eds Buyesian Statistics 4 Oxford University Press Oxford 1992 837842 1421 P Walley Statistical Reasoning Imprecise Probabilities Chapman Hall London 1991 431 J Whittaker Graphical Models Applied Multivariate Statistics Wiley Chichester 1990 1441 SS Wilks Mathematical Statistics Wiley New York 1962