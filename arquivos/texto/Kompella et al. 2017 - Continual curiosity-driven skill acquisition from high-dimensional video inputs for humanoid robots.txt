Artiﬁcial Intelligence 247 2017 313335 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Continual curiositydriven skill acquisition highdimensional video inputs humanoid robots Varun Raj Kompella Marijn Stollenga Matthew Luciw Juergen Schmidhuber The Swiss AI Lab IDSIA USI SUPSI Galleria 2 6928 MannoLugano Switzerland r t c l e n f o b s t r c t Article history Received revised form 12 October 2014 Accepted 2 February 2015 Available online 12 February 2015 Keywords Reinforcement learning Artiﬁcial curiosity Skill acquisition Slow feature analysis Continual learning Incremental learning iCub In absence external guidance robot learn map raw pixels highdimensional visual inputs useful action sequences We propose Continual Curiosity driven Skill Acquisition CCSA CCSA makes robots intrinsically motivated acquire store reuse skills Previous curiositybased agents acquired skills associating intrinsic rewards world model improvements reinforcement learning learn intrinsic rewards CCSA unlike previous implementations world model set compact lowdimensional representations streams highdimensional visual information learned incremental slow feature analysis These representations augment robots state space new information environment We information higher level compared pixels useful interpretation example robot grasped cup ﬁeld view After learning representation large intrinsic rewards given robot performing actions greatly change feature output tendency change slowly time We empirically actions grasping cup useful skills An acquired skill includes learned actions learned slow feature representation Skills stored reused generate new observations enabling continual acquisition complex skills We present results experiments iCub humanoid robot uses CCSA incrementally acquire skills topple grasp pickplace cup driven intrinsic motivation raw pixel vision 2015 Elsevier BV All rights reserved 1 Introduction Over past decade growing trend humanoid robotics research robots large number joints degrees freedom notably ASIMO 1 PETMAN 2 iCub 3 These robots demonstrate high dexterity potentially capable carrying complex humanlike manipulation When interacting realworld robots faced challenges problem solve tasks processing abundance highdimensional sensory data In case structured environments robots carefully programmed experts solve particu lar task But realworld environments usually unstructured dynamic makes daunting task program robots manually This problem substantially alleviated reinforcement learning RL 45 Corresponding author Email address varunidsiach VR Kompella httpdxdoiorg101016jartint201502001 00043702 2015 Elsevier BV All rights reserved 314 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 1 A playroom scenario baby humanoidrobot lab environment placed table moving objects The robot limited ﬁeldofview encounters continuous streams images holds shifts gaze Figure shows perspectives oriented moving objects How robot learn solve tasks absence external guidance robot learns acquire desired taskspeciﬁc behaviors maximizing accumulation taskdependent external rewards simple trialanderror interactions environment Unfortunately humanoid robots equipped vision sensory joint state space large ex tremely diﬃcult procure rewards exist random exploration For example robot receives reward sorting objects extremely long time obtain reward ﬁrst time Therefore necessary build lowerdimensional representations statespace learning tractable b explore envi ronment eﬃciently But robots learn presence external rewards typically sparsely available Much human capacity explore solve problems driven selfsupervised learning 67 seek acquire behaviors creating novel situations learning As example consider simple playroom scenario baby humanoid shown Fig 1 Here robot placed table moving objects The robot limited ﬁeldofview encounters continuous streams images holds shifts gaze If robot learn compact representations predictable behaviors grasp interactions cup learned behaviors speed acquisition external rewards related teacherdeﬁned task placing cup particular location Continually acquiring reusing repertoire behaviors representations world learned selfsupervision robot adept solving external tasks But robot selfsupervise exploration b build representations highdimensional sensory inputs c continually acquire skills enable solve new tasks These problems individually researched machine learning robotics literature 829 However develop single addresses important sues challenging open problem artiﬁcial intelligence AI research We propose onlinelearning framework addresses open problem In order robot selfsupervised intrinsicallymotivated explore new environments use theory Artiﬁcial Curiosity AC 3031 AC mathematically describes curiosity creativity ACdriven agents interested learnable asyetunknown aspects environment disinterested learned inherently unlearnable noisy aspects Speciﬁcally agent receives intrinsic rewards action sequences rewards pro portional improvement agents internal model predictor environment Using RL selfgenerated intrinsic rewards derived AC 323625 agent motivated explore environment makes maximum learning progress Most RL algorithms tend work dimensionality state space small structure simple In order deal massive highdimensional streams raw sensory information obtained example vision essential reduce input dimensionality building lowdimensional informative abstractions environment 37 An abstraction maps highdimensional input lowdimensional output The highdimensional data sensed robot temporally correlated greatly compressed temporal coherence data exploited Slow Feature Analysis SFA 143839 unsupervised learning algorithm extracts temporal regularities rapidly changing raw sensory inputs SFA based Slowness Principle 4042 states underlying causes changing signals vary slowly primary sensory stimulus For example individual retinal receptor responses grayscale pixel values video change quickly compared latent abstract variables position moving object SFA achieved success problems scenarios extraction driving forces dynam ical 43 nonlinear blind source separation 44 preprocessor reinforcement learning 39 learning placecells headdirection cells gridcells spatial view cells highdimensional visual input 38 SFA techniques readily applicable openended online learning agents estimate covariance matrices data batch processing We instead use Incremental Slow Feature Analysis IncSFA 4546 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 315 need store input data computationally expensive covariance matrix estimates IncSFA makes feasible handle highdimensional image data openended manner IncSFA like online learning approaches gradually forgets previously learned representations statistics input change example robot shifts gaze perspective perspective Fig 1 To ad dress issue previous work proposed algorithm called CuriosityDriven Modular Incremental Slow Feature Analysis Curious Dr MISFA 4748 retains previously learned form expert modules 29 From set input video streams Curious Dr MISFA actively learns multiple expert modules comprising slow feature abstractions order increasing learning diﬃculty The algorithm continually estimates initially unknown learning diﬃculty intrinsic rewards generated exploring input streams Using Curious Dr MISFA robot Fig 1 ﬁnds interactions plastic cup interesting easier encode complex movements objects This results compact slow feature abstraction encodes teractions cup Eventually robot ﬁnds cupinteraction boring shifts encoding perspectives retaining learned abstraction Can robot simultaneously acquire reusable skills acquiring abstractions Each abstraction learned encodes previously unknown regularity input observations basis acquiring new skills Our contribution Continual Curiositydriven Skill Acquisition CCSA framework acquiring abstrac tions skills online continual manner In RL options framework 49 formalizes skills RL policies active subset state space terminate subgoals option takes When agent highdimensional input like vision option requires dimensionality reducing abstraction policy learning tractable CCSA taskindependent curiositydriven learning algorithm combines Curious Dr MISFA options framework Each slow feature abstraction learned Curious Dr MISFA augments robots default state space case set lowlevel kinematic joint poses learned Task Relevant Roadmaps 50 This augmented state space clustered create new distinct states A Markovian transition model learned exploring new state space The reward function learned exploration agent intrinsically rewarded making statetransitions produce large variation slowfeature outputs This specialized reward function build options policies drive robot states transitions occur Such transitions shown correspond bottleneck states doorways known good subgoals absence externally imposed goals 5152 Once transition reward functions learned options policy learned LeastSquares Policy Iteration 53 Skills acquired robot form options reused generate new input observations enabling acquisition complex skills continual openended manner 2954 Using CCSA experiments iCub humanoid robot addresses open problems discussed earlier acquiring repertoire skills topple grasp rawpixel vision driven purely intrinsic motivation The rest paper organized follows Section 2 discusses related research work carried prior paper Sections 3 4 present overview formulation learning problem associated CCSA framework Section 5 discusses details internal workings CCSA Section 6 contains experiments results conducted iCub humanoid robot Sections 78 present future work conclusions 2 Related work Existing intrinsicallymotivated skill acquisition techniques RL applied simple domains For example Bakker Schmidhuber 55 proposed hierarchical RL framework called HASSLE grid world environment highlevel policies discover subgoals clustering distancesensor outputs lowlevel policies specialize reaching subgoals Stout Barto 24 explore use competencebased intrinsic motivation developmental model skill acquisition simple artiﬁcial gridworld domains Pape et al 25 proposed method autonomous acquisition tactile skills biomimetic robot ﬁnger curiositydriven reinforcement learning There attempts ﬁnd skills featureabstractions domains humanoid robotics Hart 56 proposed intrinsically motivated hierarchical skill acquisition approach humanoid robot The com bines discrete event dynamical systems 57 control basis intrinsic reward function 26 learn set controllers However intrinsic reward function task speciﬁc requires teacher design developmental schedule robot Konidaris et al 5859 option assigned abstraction library senso rimotor abstractions acquire skills The abstractions typically handdesigned learning assisted humandemonstration In recent work 27 intrinsic motivation makes robot acquire skills task improve performance second task However robot augmented reality tags identify target objects access preexisting abstraction library CCSA autonomously learns library abstractions control policies simultaneously rawpixel streams generated exploration priorknowledge environment Mugan Kuiperss 60 Qualitative Learner Action Perception discretizes lowlevel sensorimotor expe rience deﬁning landmarks variables observing contingencies landmarks It builds predictive models lowlevel experience later uses generate plans actions It selects actions randomly early expects fast progress performance predictive models artiﬁcial curiosity The sensory channels preprocessed input variables example track positions objects scene 316 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 A major difference operate raw pixels directly instead assuming existence lowlevel sensory model track positions objects scene Baranes Oudeyer 61 proposed intrinsic motivation architecture called SAGGRIAC adaptive goalexploration The comprises learning parts selfgeneration subgoals taskspace ex ploration lowlevel actions reach subgoals selected The subgoals generated heuristic methods based local measure competence progress The authors results simulated quadruped robot reaching tasks The assumes lowdimensional taskspace provided CCSA taskindependent approach subgoals generated automatically slow feature abstractions encode spatiotemporal regularities raw highdimensional video inputs Ngo et al 6263 investigated autonomous learning utilizes progressbased curiosity drive ground given abstract action placing object The general framework formulated selective sampling problem agent samples action current situation soon sees effects action statistically unknown If available actions statistically unknown outcome agent generates plan actions reach new setting expects ﬁnd action Experiments conducted Katana robot arm ﬁxed head camera blockmanipulation task The authors proposed method generates sampleeﬃcient curious exploratory behavior continual skill acquisition However unlike CCSA sensorimotor abstractions handdesigned learned agent CCSA uses IncSFA ﬁnd lowdimensional manifolds raw pixel inputs providing basis coupled ceptual skill learning We emphasize special utility SFA task similar methods principal component analysis 64 predictiveprojections 65 based variance nearest neighbor learning Slow features IncSFA extract temporal invariance input streams represent doorway bottleneck pects chokepoints fully connected subareas similar LaplacianEigen Maps 6668 The hierarchical reinforcement learning literature 69714951556752 illustrates bottlenecks useful subgoals Finding bottlenecks visual input spaces relatively new concept exploit iCub experiments For exam ple moves arm cup scene bottleneck state topples cup invariant arm position The subareas case 1 cup upright stable arm moves 2 cup stable arm moves More studies types representations learned IncSFA algorithm 4746 An initial implementation Curious Dr MISFA learning slow feature abstractions 48 discussion neuro physiological correlates 47 prototypical construction skill slow feature abstraction 72 previous work The novel contribution paper present online learning algorithm CCSA uses Curious Dr MISFA learning slow feature abstractions enables robot acquire store reuse skills openended continual manner We formally address underlying learning problem taskindependent contin ual curiositydriven skill acquisition We demonstrate working algorithm iCub experiments advantages intrinsically motivated skill acquisition solving external task 3 Overview proposed framework In section brieﬂy summarize overall framework proposed algorithm Continual Curiosity driven Skill Acquisition CCSA Fig 2 illustrates overall framework The learning problem associated CCSA described follows From set predeﬁned previously acquired input exploratory behaviors generate potentially highdimensional timevarying observation streams objective agent acquire easily learnable unknown target behavior b reuse target behavior acquire complex target behaviors The target behaviors represent skills acquired agent A sample run CCSA framework acquire skill follows Fig 2 The agent starts set predeﬁned previously acquired exploratory behaviors We use options framework 49 formally represent exploratory behaviors exploratory options Section 4 formal deﬁnition terminology b The agent makes highdimensional observations sensorfunction camera actively executing exploratory options c Using previously proposed curiositydriven modular incremental slow feature analysis Curious Dr MISFA algo rithm agent learns slow feature abstraction encodes easiesttolearn unknown regularity observation streams Section 52 d The slow feature abstraction outputs clustered create feature states augmented agents abstractedstate space contains previously encoded featurestates Section 53 e A Markovian transition model learned exploring new abstractedstate space The reward function learned exploration agent intrinsically rewarded making statetransitions produce large variation high statistical variance slowfeature outputs This specialized reward function learn actionsequences policy drives agent states transitions occur Section 53 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 317 Fig 2 Highlevel control ﬂow Continual Curiositydriven Skill Acquisition CCSA framework The agent starts set predeﬁned previously acquired exploratory behaviors represented exploratory options b It makes highdimensional observations actively executing exploratory options c Using Curious Dr MISFA algorithm agent learns slow feature abstraction encodes easiesttolearn unknown regularity observation streams d The slow feature abstraction outputs clustered create feature states augmented agents abstractedstate space e A Markovian transition model new abstractedstate space intrinsic reward function learned exploration f A deterministic policy learned modelbased LeastSquares Policy Iteration ModelLSPI target option constructed The deterministic targetoptions policy modiﬁed stochastic policy agents new abstracted states added set exploratory options f Once transition reward functions learned deterministic policy learned modelbased LeastSquares Policy Iteration LSPI 53 The learned policy learned slow feature abstraction constitute target option represents acquired skill Section 53 fa The deterministic targetoptions policy modiﬁed stochastic policy agents new abstracted states added set exploratory options Section 54 This enables agent reuse skills acquire complex skills continual openended manner 2954 CCSA taskindependent algorithm require design modiﬁcations environment changed However CCSA makes following assumptions The agents default abstractedstate space contains lowlevel kinematic joint poses robot learned oﬄine Task Relevant Roadmaps 50 This limit iCubs exploration arm plane parallel table This assumption relaxed resulting larger space armexploration iCub skills developed different b CCSA requires input exploratory option To minimize human inputs experiments t 0 agent starts single input exploratory option randomwalk default abstractedstate space However environment domain speciﬁc information design input exploratory options order shape resulting skills For example randomwalk policies mapped different subregions robots joint space 4 Theoretical formulation learning problem In section present theoretical formulation learning problem associated proposed CCSA frame work We ﬁrst formalize curiositydriven skill acquisition problem later section present continual extension 41 Curiositydriven skill acquisition Given ﬁxed set input exploratory options generate potentially high dimensional observation streams maynot unique objective acquire previously unknown target option corresponding easilyencodable observation stream Fig 3 illustrates learning process The learning process iterates following steps Estimate easilyencodable unknown observation stream simultaneously learning compact encoding ab straction b Learn option maximizes statistical variance encoded abstraction output The problem formalized follows 411 Notation Environment An agent environment statespace S It action A transition new state according transitionmodel environment dynamics P S A S The agent observes environment state s highdimensional vector x RI I N 318 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 3 Curiositydriven Skill Acquisition Given ﬁxed set input exploratory options represented red dashed boxes generating n observation streams abstractions represented circles corresponding target options represented pink dotted boxes learned sequentially order increasing learning diﬃculty The learning process involves acquiring target options sequence acquired The ﬁgure shows example desired result ﬁrst target option learned The ﬁgure shows desired end result possible target options learned The curved arrow indicates temporal evolution learning process Abstraction Let cid2 denote online abstractionestimator updates featureabstraction φ cid2x φ returns updated abstraction input x The abstraction φ x cid5 y maps highdimensional input observation stream xt RI lowerdimensional output yt R J J cid6 I J N yt φ xt Abstractedstate space The agents abstractedstate space S cid4 contains space spanned outputs y ab stractions previously learned cid2 Input exploratory options The agent execute input set predeﬁned temporally extended action sequences called exploratory option set Oe O e Scid4 π e Scid4 0 1 option termination initiation set comprising abstracted states option available βe A 0 1 condition determine option terminates probability state π e predeﬁned stochastic policy random walk applicable state space Each exploratoryoptions policy generates observation stream sensorfunction U imagesensor like camera n 1 Each exploratory option deﬁned tuple cid8I e cid9 I e 1 O e βe I e n xit UPs π e scid4 P unknown transition model environment scid4 I e agents current abstracted state execut scid4 returns action Let ing ith exploratory option O e X x1 xn denote set n I dimensional observation streams generated n exploratoryoptions policies At time t learning algorithms input sample n observationstreams time t s S corresponding environment state π e Curiosity function Let cid7 X 0 1 denote function indicating speed learning abstraction abstraction estimator cid2 cid7 induces total ordering observation streams making comparable terms learning diﬃculty1 1 Refer previous work 4773 proof existence function analytical expression cid7 IncSFA VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 319 Target options Unlike predeﬁned input exploratoryoption set targetoption set OL process A target option O βL φi π L tuple cid8IL Scid4 Scid4 Scid4 φi φi termination condition π L outcome learning It deﬁned targetoptions initiation set deﬁned augmented statespace xj X βi options denotes space spanned abstraction φi s output yt φ contains learned abstraction φi learned deterministic policy π L A learned deterministic policy L OL cid9 IL Scid4 Scid4 φi cid3 xjt cid2 Scid4 Scid4 φi Encoded observation streams Let X OLt φ outputs X yi O L OLt denote ordered set induced time t preimages learned abstractions OLt X OLt represents set encoded observation streams time t Other notation indicates cardinality set cid13cid13 indicates Euclidean norm cid8cid9t indicates averaging time cid8cid9τ indi t cates windowedaverage ﬁxed window size τ time δ small scalar constant 0 Var represents statistical variance indicates f orall 412 Problem statement With notation curiositydriven skill acquisition problem formalized optimization problem number objective Given ﬁxed set input exploratory options Oe ﬁnd targetoption set OL target options learned time t maximized cid4 cid4 cid4 cid4 cid4OLt cid4 t 1 2 max OL constraints j 1 J O j cid6 cid8 y cid5 j cid9t 0 L O cid8 y 2cid9t 1 OLt j 1 n O OLt L kcid16i cid7xi cid7x j j xi x j X cid7 π L UPs πiscid4 Var φi cid2 arg sup πi OLt L cid8cid13cid2x j φi φicid13cid9τ δ t cid8cid13cid2x j φk φkcid13cid9τ t δ OLt cid3cid8 scid4 IL O L OLt 1 2 3 4 Constraint 1 requires abstractionoutput components zero mean unit variance This constraint enables abstractions nonzero avoids learning features constant observation streams Constraint 2 requires unique abstraction learned encodes input observation streams avoiding redundancy Constraint 3 imposes totalordering induced cid7 abstractions learned Easiertolearn observation streams encoded ﬁrst And ﬁnally Constraint 4 requires targetoptions policy maximizes sensitivity determined variance observed abstraction outputs 74 In rest paper interchangeably use word skill denote learned target option O skillset denote targetoption set OL L Optimal solution For objective minimized time t optimal solution learn target option corre sponding current easiest notyetlearned abstraction observation streams satisfy Constraints 13 policy maximizes variance encoded abstraction output satisfy Constraint 4 However cid7 Constraint 3 known priori needs estimated online actively exploring input exploratory options time One possible approach ﬁnd analytical expression2 cid7 particular abstractionestimator cid2 b observation stream selection technique estimate cid7 values obser vation stream This approach dependent abstractionestimator However proposed framework employs abstractionestimator independent approach making use reinforcement learning estimate cid7 values form curiosity rewards generated learning progress cid2 42 Continual curiositydriven skill acquisition In formulation agent ﬁxed set n 1 input exploratory options Therefore number learn able target options equal total number learnable abstractions equal number input exploratory options cid4 cid4 cid4 cid4 cid4OLt cid4 n 5 lim t 2 Refer previous work 47 analytical expression cid7 IncSFA 320 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 4 Exploratoryoption policy phases If estimation error learned abstraction modules incoming observations lower threshold δ exploratoryoptions policy learned Least Squares Policy Iteration LSPI If estimation error higher threshold policy random walk b An example thresholded estimation error c corresponding exploration policy To enable continual learning 29 number skills acquired agent necessarily bounded agent needs reuse previously acquired skills learn complex skills Therefore continual curiositydriven skill acquisition learning problem slightly modiﬁed version formulation target options learned form basis new input exploratory options Oe Oe FO L 6 F denotes functional variation deterministic target option stochastic exploratory Therefore number input exploratory options n increases new skill acquired agent Subtarget options Constraint 4 requires targetoptions policy maximizes variance observed J dimensional abstraction outputs However principle constraint rewritten subset J dimensions abstraction learn policy This results maximum number 2 J 1 learnable policies We denote set target options share abstraction cid8IL cid9 j 2 J 1 subtarget options To simple rest paper use J dimensions presented Constraint 4 learn targetoptions policy limiting 1 target option learned abstraction φi π L βL j 5 Continual curiositydriven skill acquisition CCSA framework Section 3 presented overview proposed framework Here discuss framework addresses learning problem formalized Section 4 51 Input exploratory options As discussed Section 4 deﬁned set input exploratory options agent execute interact environment Here present details construct options The simplest exploratoryoption policy random walk However present sophisticated variant uses form initial artiﬁcial curiosity based errorbased rewards 22 This exploratoryoptions policy π e determined predictability observations xt switch random walk environment unpre dictable This policy π e phases If estimation error learned abstraction modules incoming observations lower threshold δ exploratoryoptions policy learned LeastSquares Policy Iteration Tech nique LSPI 53 estimation transition model actively updated options statespace I e Scid4 estimated reward function rewards high estimation errors Such policy encourages agent explore seen world Fig 4a But estimation error learned abstraction modules higher threshold δ exploratoryoptions policy randomwalk options statespace Fig 4 illustrates error seeking exploratoryoptions policy We denote policy LSPIExploration policy When agent selects exploratory option execute follows options policy generating observation stream xi U Ps π e scid4 termination O e condition met To general nonspeciﬁc environment experiments exploratoryoptions termination condition option terminates ﬁxed τ timesteps execution VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 321 Fig 5 Architecture Curious Dr MISFA includes reinforcement learning agent generates observationstream selection policy based intrinsic rewards b adaptive Incremental SFA coupled Robust Online Clustering module updates abstraction based incoming observations c gating prevents encoding observations previously encoded Setting different input exploratoryoption set inﬂuence skills developed CCSA In experiments t 0 agent starts single exploratory option deﬁned The LSPIExploration policy speeds agents exploration acting deterministically predictable world randomly unseen world Since t 0 world unexplored LSPIExploration policy random walk agents abstracted states Environment domain speciﬁc information design input exploratoryoption set order shape resulting skills For example exploratory options randomwalk policies mapped different subregions robots joint space 52 Curiositydriven abstraction learning Curious Dr MISFA At core CCSA framework Curiosity Driven Modular Incremental Slow Feature Analysis Algorithm Curious Dr MISFA 47483 The order skills acquired CCSA framework direct consequence order abstractions learned Curious Dr MISFA algorithm The input Curious Dr MISFA algorithm set highdimensional observation streams X x1 xn xit RI I N generated input exploratoryoptions policies The result slow feature abstraction φi corresponding easiest unknown observation stream Apart learning abstraction learning process involves selecting observation stream easiest encode To end Curious Dr MISFA uses reinforcement learning learn optimal observationstream selection policy based intrinsic rewards proportional progress learning abstraction In section brieﬂy review architecture Curious Dr MISFA Fig 5 illustrates architecture Curious Dr MISFA includes reinforcement learning RL agent gen erates observationstream selection policy based intrinsic rewards b adaptive Incremental Slow Feature Analysis coupled Robust Online Clustering IncSFAROC module updates abstraction based incoming observa tions c gating prevents encoding observations previously encoded The RL agent internal environment set discrete states S int sint equal number observation streams 1 sint agent allowed actions Aint stay switch The action stay makes In state sint n 3 A Pythonbased implementation Curious Dr MISFA URL wwwidsiachkompellacodes 322 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 It maintains adaptive abstraction cid9φ RI J cid9φ cid4t agents state previous state switch randomly shifts agents state internal states The agent state sint receives ﬁxed τ time step sequence observations x corresponding stream xi updates based observations x IncSFAROC abstractionestimator The agent receives intrinsic rewards proportional learning progress IncSFAROC The observation stream selection policy π int S int Aint 0 1 learned intrinsic rewards select observation stream iteration yielding new samples x These new samples encodable previously learned abstractions update adaptive abstraction The updated abstraction cid9φ added abstraction set cid4t IncSFAROCs estimation error falls low threshold δ If added new adaptive abstrac tion cid9φ instantiated process continues The rest section discusses details different parts Curious Dr MISFA algorithm Abstractionestimator Curious Dr MISFAs abstraction estimator Incremental Slow Feature Analysis IncSFA 46 coupled Robust Online Clustering ROC 7576 algorithm IncSFA learn realvalued abstractions observations ROC learn discrete mapping abstraction outputs y agents abstracted state space Scid4 In particular abstracted state scid4 Scid4 associated ROC implementation node estimates multiple cluster centers slowfeature outputs IncSFA incremental version Slow feature analysis SFA 14 unsupervised learning technique extracts features observation stream objective maintaining informative slowlychanging feature response time SFA concerned following optimization problem Given I dimensional input signal xt x1t xI tT ﬁnd set J instantaneous realvalued functions gx g1x g J xT generate J dimensional output signal yt y1t y J tT y jt g jxt j 1 J cid10 j cid10 y j cid8 y2 j cid9 minimal constraints cid8 y jcid9 0 zero mean cid8 y2 j cid9 1 unit variance j cid8 yi y jcid9 0 decorrelation order 7 8 9 10 cid8cid9 y indicating temporal averaging derivative y respectively The goal ﬁnd instantaneous functions g j generating different output signals slowly varying possible The decorrelation constraint 10 ensures different functions g j code features The constraints 8 9 avoid trivial constant output solutions SFA operates covariance observation derivatives scales size observation vector instead number states SFA originally realized batch method requiring data collected processing The algorithmic complexity cubic input dimension I By contrast Incremental SFA IncSFA linear update complexity 46 adapt features new observations achieving slow feature objective robustly openended learning environments ROC clustering algorithm similar incremental Kmeans algorithm 77 set cluster centers maintained new input similar cluster center winner adapted like input Unlike Kmeans input follows adaptation step merging similar cluster centers creating new cluster center latest input In way ROC quickly adjust nonstationary input distributions directly adding new cluster newest input sample mark beginning new input process Estimation error curiosity reward Each ROCEstimator node j associated error ξ j These errors initialized 0 updated node activated ξ jt min cid13yt vw cid13 yt slowfeature output w vector vw estimate wth cluster activated node cid13cid13 represents L2 norm The total estimation error calculated sum stored errors nodes ξt ξ jt The agent receives rewards proportional derivative total estimation error motivates continue executing option yielding meaningful learnable abstraction The agents reward function computed iteration curiosity rewards ξ follows j1 Rintsint sint aint 1 η Rintsint sint aint η tτcid10 t ξ t 0 η 1 discount factor τ duration current option termination sint sint aint stay switch S int pcid10 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 323 Observationstream selection policy The transitionprobability model P int internal environment similar complete graph given cid11 P int jstay 1 0 j cid16 j P int jswitch 0 1 N1 j cid16 j 11 cid11 j 1 N Using current updated model reward function R int internalstate transitionprobability model P int use modelbased Least Squares Policy Iteration 53 generate agents internalpolicy π int S int stay switch iteration The agent uses decaying cid13greedy strategy 5 internal policy carry internalaction stay switch iteration Module freezing new module creation Once adaptive training modules cid9φ estimation error gets lower thresh old δ agent freezes saves IncSFAROC module resets cid13greedy value starts training new module Gating abstraction assignment The trained frozen modules represent learned library abstrac tions cid4t If trained modules estimation error option threshold δ option assigned modules abstraction adaptive training module cid9φ prevented learning gating signal Fig 5 There intrinsic reward case Hence training module cid9φ encode data observation streams encoded earlier Input badly encoded trained modules serve train adaptive module 53 Learning target option From set observations streams generated input exploratory options Curious Dr MISFA learns slow feature abstraction φi corresponding estimated easiestyetunlearned exploratory option stream xj The abstrac tions output stream yi φixj zeromean unitvariance time 46 lowerdimensional representation input x j satisﬁes Constraint 1 Section 412 The output values yit discretized set abstraction states Scid4 represent newly discovered abstracted states agent A deterministic target option φi structed follows The initiation set simply product statespace IL Initiation set IL larger abstractedstate space includes newly discovered abstraction states I e j Scid4 φi Therefore option deﬁned The target option policy π L Target option policy π L A way satisfy Constraint 4 To end use Modelbased LeastSquares Policy Iteration Technique LSPI 53 estimated transition reward models The targetoptions transition model P O samples generated exploratoryoptions policy π e j As estimate reward function agent uses rewards proportional difference subsequent abstraction activations continually estimated scid4 scid4 IL L 12 13 L t cid13yit yit 1cid13 r O L scid4 1 αR O R O cid12 L scid4 αr O cid13 L t cid12 cid13 j scid4 U Ps π e U Ps π e yit φi s s corresponding environment states P unknown transitionmodel environment 0 α 1 constant smoothing factor Once estimated transition reward models stabilize LSPI follows RL objective learns policy π L maximizes expected cumulative reward time yit 1 φi j scid4 cid14 cid10 cid4 cid4 L cid4π R O t L γ tr O cid15 π L arg sup π E t0 γ discount factor close 1 Therefore π L maximizing variance activations 78 approximately4 satisfying Constraint 4 maximizes average activation differences equivalent 14 Termination condition βL R O maximum reward max sa The option terminates agent reaches abstractedstate observes L Each target option learned added targetoption set OL learning process iterates learnable exploratory option streams encoded Since expected behavior Curious Dr MISFA ensures Constraints 4 The error true estimated targetoption policy depends transition reward models estimated based samples scid4 scid4 generated exploratoryoptions policy 324 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 6 Reuse learned target options For target option learned represented pink dotted box new exploratory options Biased Initialization Explore Policy Chunk Explore added input exploratoryoption represented red dashed boxes set Biased Initialization Explore option biases agent explore ﬁrst stateaction tuples previously received maximum intrinsic rewards Policy Chunk Explore option executes deterministic targetoptions policy exploration 13 satisﬁed 47 learned targetoptions policy satisﬁes Constraint 4 targetoption set OL time t satisﬁes required constraints In Section 4 discussed alternative Constraint 4 different dimensions learned abstraction learn multiple policies resulting set subtarget options To simple dimensions abstraction learn targetoptions policy However subtarget option set constructed following approach discussed Multiple reward functions simultaneously estimated scid4 scid4 samples generated exploratoryoptions policy set subtarget options constructed leastsquares policy iteration parallel 54 Reusing target options To skill acquisition openended acquire complex skills Section 42 learned target option L explore newly discovered abstractedstate space Section 53 However target option O reused straightaway deﬁnition differs exploratory option targetoptions policy deterministic exploratoryoptions policy stochastic Section 51 We construct new exploratory options instead based target option O learned Fig 6 IL The policy combines targetoptions policy π L In ﬁrst option called policy chunk explore initiationset learned target option I e terminates state variance subse n1 quent encoded observations highest LSPIExploration policy described Section 51 Every time policy initiated policychunk A policy chunk nonadaptive frozen policy π L executed followed LSPIExploration policy This beneﬁcial target option terminates bottleneck state agent enters new world experience LSPIExploration policy useful explore L In second option called biased initialization explore exploratoryoptions policy uses normalized value function target option initial reward function estimate This initialization biases agent explore ﬁrst stateaction tuples previously received maximum intrinsic rewards Otherwise standard initial errorseeking LSPIExploration policy For target option learned exploratory options added input exploratoryoption set In way agent continues process curiositybased skill acquisition exploring new exploratory option set cid9 learned consequence chaining multiple discover unknown regularities A complex skill O skills learned earlier k φk π L k βL cid8IL L k k 55 Pseudocode The entire learning process involves determining policies 1 π e Exploratoryoptions stochastic policy determined Section 51 generate highdimensional observations 2 π int An internal policy learned Section 52 determine exploratory option O e encode slow feature abstraction VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 325 Algorithm 1 IntPolicyUpdate x Curious Dr MISFA Internal Policy Update 1 AbstractionLearned False 2 φ GatingSystemx 3 ξt1 cid13cid2x φ φcid13 4 cid8ξt1cid9τ δ cid9φ cid2x cid9φ 5 cid8cid13cid2x cid9φ cid9φcid13cid9τ δ cid4t1 cid4t cid9φ AbstractionLearned True 6 7 8 9 end 10 end 11 Rint t1 12 π int t1 13 π int t1 14 return π int UpdateReward ξt1 ModelLSPI P int Rint cid13greedy π int t1 t1 AbstractionLearned t1 Algorithm 2 Continual Curiositydriven Skill Acquisition CCSA 1 cid40 π0 Random cid9φ 0 AbstractionLearned False 2 t 0 3 state sint sint current internal state aint action selected π int Take action aint observe internal state sint Execute exploratory option O e βe t t scid4 current abstractedstate action selected π e Take action observe abstractedstate scid4 AbstractionLearned sample x state scid4 Internal Policy Update t1 AbstractionLearned IntPolicyUpdate x π int t L Rprev R O Learn target option π int t1 L R O P O cid13R O π int scid4 1 αR O scid4 scid4 1 αP O L Rprevcid13 δ cid13P O R O L L L P prev P O L L scid4 αcid13yit yit 1cid13 scid4 scid4 α L P prevcid13 δ L L π L LSPIModel P O L cid8IL βL cid9φ π Lcid9 O OL OL O Construct new exploratory options L Oe Oe BiasedInitExplore O L Oe Oe PolicyChunkExplore O cid9φ 0 AbstractionLearned False Reset end end end 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 end Abstraction learned Get assigned abstraction Estimation Error Update adaptiveabstraction Update abstraction set Update int reward func Update int policy Explorationexploitation tradeoff Construct target option Add targetoption set Targetoptions deterministic policy learned Section 53 maximize variation slow feature 3 π L abstraction output The resultant target options skills stored reused discussed facilitate openended continual learning Algorithms 1 2 summarize entire learning process5 6 Experimental results We present experimental results focus continuallearning skills iCub humanoid platform More studies types representations learned IncSFA algorithm curiositybased abstraction learning Curi ous Dr MISFA 47484668 The results ﬁrst humanoid robot 5 Pythonbased code excerpts URL wwwidsiachkompellacodes 326 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 7 An iCub robot placed table object plastic cup reach right arm ﬁeldofview b Sample input images captured left right iCub cameraeyes input algorithm iCub learns repertoire skills rawpixel data online manner driven curiosity starting lowlevel joint kinematic maps6 Learning skillset largely depends environment robot For sake developing speciﬁc types skills toppling object grasping preselected safe environment iCub explore iCub unaware environment properties Environment Our iCub robot placed table object plastic cup reach right arm ﬁeldofview Fig 7a The cup topples contact resulting images toppling predictable There human experimenter present monitors robots safety replaces cup original position toppled The iCub know plasticcup experimenter exist It continually observes grayscale pixel values highdimensional images 75 100 captured left right camera eyes Fig 7b In addition experimenter cup recognize moving hand incoming image stream shown Fig 7b Taskrelevant roadmap We induce exploration level joint angles complexity robots joint space Instead robot map poses priori This compressed actuator jointspace representation called TaskRelevant Roadmap TRM 50 This map contains family iCub postures adhere relevant constraints The TRM grown oﬄine repeatedly optimizing costfunctions represent constraints Natural Evolution Strategies NES 79 algorithm taskspace covered This allows deal complex costfunctions 41 degreesoffreedom iCubs upper body The constraints iCubs hand positioned 2D plane parallel table keeping palm oriented horizontally b left hand kept certain region way c head pointed table The taskspace TRM comprises x y position hand forms initial discretized 10 5 abstractedstate space S cid4 Scid4 y The action space x contains 6 actions North East South West Handclose Handopen Scid4 Because body movements look dynamic consequence head moves looks table different directions making task bit diﬃcult Even IncSFA ﬁnds resulting regularities raw camera observation stream skill learner continues learn regularities external rewards Experiment parameters We use ﬁxed parameter setting entire experiment IncSFA algorithm IncSFA learning update rules 46 CandidCovariance free Incremental Principal Component Anal ysis CCIPCA 80 normalizing input Minor Component Analysis MCA 81 extracting slow features For CCIPCA use learning rates 1t amnesic parameter 04 MCA learning rate set 001 CCIPCA variable size dimension reduction calculating eigenvalues needed 99 input variance typically 510 7500 pixels effectively reduced 10 dimensions The output dimension set 1 use ﬁrst IncSFA feature abstraction However number features desired Robust online clustering ROC algorithm ROC algorithm maps slowfeature outputs abstracted states Section 52 Each clustering implementation maximum number clusters set N max 3 encode multiple slow feature values abstracted state Higher values high values lead spurious clusters The estimation error threshold current module saved new module created set low value 6 A video experiment URL http wwwyoutube com watch v OTqdXbTEZpE VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 327 δ 03 The amnesic parameter set β amn 001 Higher values ROC adapt faster new data cost stable Curious Dr MISFAs internal reinforcement learner To balance exploration exploitation cid13greedy strategy Section 52 The initial cid13greedy value set 10 10 pure exploration 00 pure exploitation 0995 decay multiplier The windowaveraging time constant set τ 20 20 sample images compute windowaveraged progress error ξ corresponding curiosityreward Section 52 Targetoptions reinforcement learner Slow features abstractions unitvariance typically range 15 15 46 Since experiments expecting steplike slow features simple abstraction output values discretized 1 1 S φi 2 abstracted states Experiment initialization The iCubs abstractedstate space S cid4 t 0 10 5 grid TRM To minimize human input input exploratoryoption set Oe exploratory option begin ﬁned Section 51 Oe O e randomwalk iCubs abstractedstate space However predeﬁne 1 multiple input exploratory options lead different result The exploratory option terminates τ 20 time steps execution The internal statespace t 0 S int sint corresponds exploratory 1 option O e 1 The plastic cup roughly placed 2 2 gridpoint table sint 1 61 iCub learns topple cup The iCub starts experiment learned modules exploratoryoptions policy π e 1 randomwalk abstracted state space S cid4 Section 54 It explores taking actions North East South West Handclose Handopen grabs highdimensional images cameraeyes The exploration causes outstretched hand eventually displace topple plasticcup placed table It continues explore arbitrary timesteps experimenter replaces cup original position After τ timesteps currently ex ecuting option terminates Since exploratory option iCub reexecutes option Fig 8a shows sample input image stream leftcamera7 Fig 8b shows developing IncSFA output algorithm execution time IncSFA abstraction created The outcome IncSFA abstraction learning steplike function discretized indicates pose cup toppled vs nontoppled Fig 8c shows ROC estimation error blue solid line Expected Moving Average EMA error green dashed line algorithm execution time As process continues error eventually drops threshold δ 03 abstraction module φ1 saved Fig 9a shows ROC cluster centers map feature outputs y 10 5 abstracted states There separated clusters representing state plasticcup L 1 learned followed corresponding targetoptions policy π L Immediately abstraction saved cluster centers discretized red yellow colors indicate dis cretized feature states S cid4 Fig 9a transition model represented blue lines Fig 9a reward model φ1 1 discussed Section 53 Fig 9b shows O learned policy π L 1 cup toppled The arrows indicate optimal action taken gridlocation iCubs hand They direct iCubs hand grid point 1 3 iCub topple cup placed 2 2 Fig 9c shows policy cup toppled The policy directs iCubs hand east This experiment experimenter happened replace cup iCubs L 1 given environment Topple skill hand far east We label learned target option O 62 iCub learns grasp cup The iCub continues learning process reusing learned topple skill construct additional exploratory options discussed Section 54 One topple policy Fig 9b executed prior LSPIExploration policy normalized value function Fig 10b initialize rewardfunction LSPIExplorer 3 denote exploratory options respectively Therefore including original exploratory option O e Let O e 1 total 3 exploratory options input CCSA 2 O e 1 O e Initially explores executing options termination τ time steps When selects O e 2 cup gets toppled process Fig 10aTop exists learned abstraction φ1 encodes toppling outcome receives internal reward executing options gating Section 52 This case beginning executing O e 2 LSPIExploration policy initially causes iCub topple cup yielding rewards The initialized values corresponding visited stateaction tuples soon vanish iCub explores neighboring state action pairs Eventually result biased exploration 7 We left right camera images input observation concatenating 328 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 8 A sample image stream iCubs lefteye camera showing topple event b Developing IncSFA abstraction output algorithm execution time created The result steplike function encoding topple event c ROC estimation error algorithm execution time The estimation error eventually drops threshold δ 03 abstraction saved 3 executed algorithm iterations iCub ends grasping cup Fig 10aBottom This gives rise high estimation error novelty event Fig 10c Figs 10di stateaction LSPIExploration reward function time steps The handclose action 2 2 generates novel event This results LSPIExploration policy increases number successful grasp trials 77 91 total attempts unsuccessful trials beginning exploratory option O e corresponding O e Now executing option O e 3 adaptive abstraction ˆφ begins progress encoding samples correspond ing observation stream x3 After algorithm iterations agent ﬁnds action stay internal state sint 3 rewarding progress IncSFA ROC estimator Fig 11a 3 Fig 11b shows normalized internal reward function Curious Dr MISFA algorithm iterations new adaptive module created The internal policy π int quickly converges select execute option O e 3 receive observations When estimation error drops threshold δ 03 saves module φ2 ˆφ Fig 11c shows IncSFA output time new module created Fig 11d shows learned cluster centers mapping slowfeature output abstractedstate space Note abstracted states corresponding learned ple abstraction Scid4 shown Fig 11d grasp abstraction outputs uncorrelated topple φ1 abstraction diﬃcult illustrate 4D plot The iCub begins learn target policy π L 2 learning targetoptions transition reward model Fig 12af targetoptions stateaction reward model developed 8000 observation samples module time 8000 And ﬁnally Fig 12g shows corresponding skill learned perform HandClose 2 2 anti clockwise circular arrow represents Hand Close action VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 329 Fig 9 The resultant ROC cluster centers map abstraction outputs abstractedstate space case X Y grid locations Blue lines connecting cluster centers illustrate learned transition iCubs hand Red yellow colors indicate discretized feature states Scid4 φ1 model new abstractedstate space b Part learned targetoptions policy cup toppled The arrows indicate optimal action taken gridlocation scid4 y iCubs hand They direct iCubs hand grid point 1 3 iCub topple cup placed 2 2 c Part learned targetoptions policy cup toppled They direct iCubs hand right This result experimenter replacing cup iCub moved hand away 2 2 grid location For interpretation references color ﬁgure reader referred web version article x scid4 This experiment demonstrated iCub reused knowledge gained topple skill learn subsequent skill labeled Grasp The grasp skill includes abstraction represent cup successfully graspedornot policy directs iCubs hand 2 2 close hand 63 iCub learns pick place cup desired location We present experiment demonstrate utility intrinsic motivation solving subsequent external ob jective Fig 13 The iCub similar environment discussed However given external reward picks plastic cup places drops desired location following grid locations scid4 y 6 2 6 3 6 1 5 2 7 2 The agent intrinsic motivation ﬁnds reward inaccessible random 5 However cu exploration abstractedstate space S cid4 probability successful trial low8 10 riosity driven iCub greatly improves learning pickgrasp cup reusing skill access reward x scid4 Starting 10 5 abstractedstate space TRM iCub learns topple grasp discussed previous sections The process continues adds exploratory options O e 5 corresponding grasp skill discussed Section 54 The biased initialization explore option O e 4 results iCub dropping cup close picked Since doesnt reward case initialized values visited stateactions tuples vanish explores neighboring stateaction tuples This option long time execute desired stateaction tuple drop cup The policy chunk explore option O e 5 ﬁrst executes grasp policy randomly explores receives novelty curiosity reward When drops cup desired states exploring gets external reward results LSPIExploration policy executes rewarding behavior Curious Dr MISFA eventually ﬁnds internal action stay internalstate sint corresponding option 5 O e 5 rewarding As soon experimenter replaces cup iCub repeats pick place behavior external reward removed 4 O e 8 The probability successful pick 1300 probability drop given successful pick 1300 160 330 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 10 Sample iCubs lefteye camera images corresponding input exploratory options x1 x2 correspond original policy chunk explore exploratory option respectively x3 corresponds biased init explore exploratory option b Normalized value function previously learned target option topple It rewardinitialization biased init explore exploratory option c Estimation error learned topple abstraction module φ1 observationstreams di LSPIExploration reward function estimated novelty curiosity signal The HandClose action 2 2 maximum reward value novel grasp event This experiment demonstrated CCSA enabled iCub reuse grasp skill previously learned intrinsic motivation learning pick place cup desired location Note experiments human experimenter unknown robot acted environment speed learning process Without experimenter robot acquired set skills instead learned push object refer previous work 48 experiment simulated iCub learns slow feature abstraction encodes push 7 Discussion While research humanoid robot learning based human demonstrations humangiven taskdescriptions preprocessed inputs CCSA makes important step combining aspects needed develop online continual curiositydriven humanoid robotic agent In following brieﬂy discuss aspects current limitations framework insights future work Raw highdimensional information processing CCSA uses linear IncSFA algorithm updated online directly rawpixels encode abstractions lead acquiring skills To learn complex skills CCSA beneﬁt ex VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 331 Fig 11 ROC estimation error current adaptivemodule encoding new regularities b Normalized internalreward function Curious 3 St rewarding learning progress Dr MISFA The action stay state corresponding exploratory option 3 shown sint IncSFAROC module graspevent c IncSFA output execution time created d Resultant ROC cluster centers mapping IncSFA output wrt abstractedstate space Note abstracted states corresponding learned topple abstraction Scid4 shown φ1 grasp abstraction outputs uncorrelated topple abstraction diﬃcult illustrate 4D plot Red yellow colors indicate discretized states Scid4 blue lines illustrate learned transition model For interpretation references color ﬁgure reader φ2 referred web version article Fig 12 af Estimated rewardfunction new abstractedstate space learn targetoptions policy The handclose action 2 2 receives maximum reward produces maximum variation slowfeature output 15 15 g Learned targetoptions policy representing grasp skill The arrows indicate optimal actions taken gridlocation scid4 y The circular arrow represents handclose action The policy directs iCubs hand 2 2 close hand result successful grasp x scid4 tracting nonlinearities video inputs Hierarchical extensions IncSFA HIncSFA expanded input quadratic space 82 remedy We plan combine nonlinear hierarchical structures improve quality abstractions learned 332 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Fig 13 CCSA 5 exploratory options input Among 5 options policy chunk explore corresponding grasp skill makes easier iCub access externalreward present placing cup desired grid locations This results policy place cup desired location clockwise circular arrow represents HandOpen action b Birds eye view iCub demonstrating pick place skill b Figure shows increasing dimensions agents abstractedstate space new abstraction learned This experiment demonstrates CCSA enables iCub reuse grasp skill previously learned intrinsic motivation learning pick place cup desired location Invariant skills The skill labeled grasp experiments actually represents grasp cylindrical cup partic ular location given environment invariant experimenters actions iCubs headbody movements The invariance picked skills acquired largely depend invariance learned IncSFA observations sensed exploring iCub Refer previous work 4746 details invariance extracted IncSFA In experiments human experimenter replaced cup different locations cup toppled dropped expect IncSFA learn abstraction encodes cup graspedor invariant cups position events uncorrelated This result grasp skill invariant cups position Continual learning CCSA uses previously acquired knowledge form biased explorations policychunks learn complex skills This facilitates continual learning skills A previously acquired skill reﬁned adapted suit changing environments For example experiments cups position changed acquiring grasp skill biased init explore exploratoryoption corresponding grasp skill speed learning new skill grasp cup new position However old skills retained reused cups position changed original position The complex skills acquired CCSA form chainlike hierarchy Fig 14a exists single chainlink connecting higherorder lowerorder skills This target option CCSA learned observations exploratory options Section 4 Each node chainlike hierarchy single input act input nodes Fig 14b Whereas node general compositional hierarchy Fig 14c multiple inputs One way achieve compositional hierarchy CCSA add learned target options primitive action set A North East South West Handclose Handopen VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 333 Fig 14 The higherorder complex skills acquired CCSA form chainlike hierarchy There exists single chainlink shown unique color connecting higherorder lowerorder skills This target option CCSA learned observations exploratory options b An illustration node chain hierarchy Each node single input act input nodes c Whereas node compositional hierarchy multiple inputs Environment openness CCSA beneﬁt larger set predeﬁned input exploratory options However minimize human inputs experiments iCub starts single exploratory option randomwalk autonomously adds exploratory options derived learned target options Since CCSA acts directly rawpixels prior calibration robot cameras required Algorithm parameters intuitive tune Refer previous work 47484668 detailed description tuning IncSFA Curious Dr MISFA algorithms Therefore CCSA different environments different humanoid robots making design changes learning algorithm On motor end kinematic map transforms 41 degreesoffreedom iCub joint conﬁgurations 2D positions hand parallel table For complex manipulations required handling complicated objects higher dimensional kinematicmaps 50 As future work plan different approaches tackle easier safer manipulation iCub Quality skills acquired We presented formally underlying learning problem constrained optimization problem The objective function metric tune different parameters method However metric suﬃciently evaluate quality skills acquired One major factor type abstractionestimator For example method uses simpler abstraction learning algorithm acquire large number skills functionally equivalent acquiring single skill discriminative abstraction estimator Therefore evaluating different taskunrelated intrinsicallymotivated IM approaches providing external goal illposed problem As future work plan build realistic taskindependent skillacquisition benchmarks hidden external tasks evaluate multiple IM approaches Scalability For target option acquired CCSA number input exploratory options increases value See Section 54 Observations previously encoded exploratory options automatically ﬁltered gat ing Curious Dr MISFA Therefore target option acquired number unknown exploratory options increases value Hence space input exploratory options scales linearly respect number skills acquired Sensor fusion And ﬁnally CCSA uses visual inputs onboard cameras joint angles iCub A humanoid robots actions improved different sensory modalities tactile audio addition visual inputs This straightforward addition CCSA IncSFA agnostic modality sensory informa tion The raw inputs different modalities concatenated single input fed IncSFA algorithm causing computational overhead IncSFA linear update complexity 46 Related work combining sensory modalities SFA methods shown achieve good results 83 8 Conclusion We proposed onlinelearning algorithm enables humanoid robotic agent iCub incrementally acquire skills order increasing learning diﬃculty onboard highdimensional camera inputs lowlevel kine matic joint maps driven purely intrinsic motivation The method combines recently introduced active modular Slow Feature learning algorithm called Curious Dr MISFA options framework We formally deﬁned underlying learning problem provided experimental results conducted iCub humanoid robot topple grasp pick place cup To knowledge ﬁrst method demonstrates continual curiositybased skill acquisition highdimensional video inputs humanoid robots 334 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 Acknowledgements We thank Sohrob Kazerounian Alan Locketts assistance revising paper This work funded SNF grant 138219 Theory Practice Reinforcement Learning II 7th framework program EU grant 270247 NeuralDynamics project References 1 Honda Asimo robot httpworldhondacomASIMO 2 BostonDynamics Petman protection ensemble test mannequin humanoid military robot httpwwwbostondynamicscomrobot_petmanhtml 3 G Metta G Sandini D Vernon L Natale F Nori The iCub humanoid robot open platform research embodied cognition Proceedings 8th Workshop Performance Metrics Intelligent Systems PerMIS08 ACM New York NY USA 2008 pp 5056 4 LP Kaelbling ML Littman AW Moore Reinforcement learning survey J Artif Intell Res 4 1996 237285 5 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press Cambridge MA 1998 6 RW White Motivation reconsidered concept competence Psychol Rev 66 5 1959 297 7 GR Norman HG Schmidt The psychological basis problembased learning review evidence Acad Med 67 9 1992 557565 8 H Abut Ed Vector Quantization IEEE Press Piscataway NJ 1990 9 IT Jolliffe Principal Component Analysis SpringerVerlag New York 1986 10 P Comon Independent component analysis new concept Signal Process 36 1994 287314 11 DD Lee HS Seung Learning parts objects nonnegative matrix factorization Nature 401 6755 1999 788791 12 T Kohonen SelfOrganizing Maps 3rd edition SpringerVerlag Berlin 2001 13 GE Hinton Training products experts minimizing contrastive divergence Neural Comput 14 8 August 2002 17711800 14 L Wiskott T Sejnowski Slow feature analysis unsupervised learning invariances Neural Comput 14 4 2002 715770 15 J Schmidhuber Learning complex extended sequences principle history compression Neural Comput 4 2 1992 234242 16 J Schmidhuber Learning unambiguous reduced sequence descriptions JE Moody SJ Hanson RP Lippman Eds Advances Neural Information Processing Systems 4 NIPS 4 Morgan Kaufmann 1992 pp 291298 17 J Schmidhuber Learning factorial codes predictability minimization Neural Comput 4 6 1992 863879 18 S Lindstädt Comparison unsupervised neural network models redundancy reduction MC Mozer P Smolensky DS Touretzky JL Elman AS Weigend Eds Proc 1993 Connectionist Models Summer School Erlbaum Associates Hillsdale NJ 1993 pp 308315 19 M KlapperRybicka NN Schraudolph J Schmidhuber Unsupervised learning LSTM recurrent neural networks Proc Intl Conf Artiﬁcial Neural Networks ICANN2001 Lecture Notes Comp Sci vol 2130 Springer Berlin Heidelberg 2001 pp 684691 20 OC Jenkins MJ Matari c A spatiotemporal extension Isomap nonlinear dimension reduction Proceedings TwentyFirst International 21 H Lee Y Largman P Pham AY Ng Unsupervised feature learning audio classiﬁcation convolutional deep belief networks Advances Conference Machine Learning ACM 2004 p 56 Neural Information Processing Systems 2009 pp 10961104 22 S Singh AG Barto N Chentanez Intrinsically motivated reinforcement learning Advances Neural Information Processing Systems NIPS 2004 23 Y Girdhar D Whitney G Dudek Curiosity based exploration learning terrain models IEEE International Conference Robotics Automation 24 A Stout AG Barto Competence progress intrinsic motivation IEEE 9th International Conference Development Learning ICDL IEEE 2010 25 L Pape CM Oddo M Controzzi C Cipriani A Förster MC Carrozza J Schmidhuber Learning tactile skills curious exploration Front 26 S Hart S Sen RA Grupen Intrinsically motivated hierarchical manipulation Proceedings 2008 IEEE Conference Robots Automation 27 G Konidaris S Kuindersma R Grupen AG Barto Autonomous skill acquisition mobile manipulator Proceedings TwentyFifth AAAI Conference Artiﬁcial Intelligence 2011 pp 14681473 28 L Gisslén M Luciw V Graziano J Schmidhuber Sequential constant size compressors reinforcement learning Artiﬁcial General Intelligence pp 12811288 ICRA 2014 pp 257262 Neurorobot 6 2012 ICRA 2008 pp 38143819 Springer 2011 pp 3140 29 MB Ring Continual learning reinforcement environments PhD thesis University Texas Austin 1994 30 J Schmidhuber Developmental robotics optimal artiﬁcial curiosity creativity music ﬁne arts Connect Sci 18 2 2006 173187 31 J Schmidhuber Formal theory creativity fun intrinsic motivation 19902010 IEEE Trans Auton Ment Dev 2 3 2010 230247 32 J Schmidhuber Curious modelbuilding control systems Proceedings International Joint Conference Neural Networks vol 2 Singapore IEEE Press 1991 pp 14581463 33 J Storck S Hochreiter J Schmidhuber Reinforcement driven information acquisition nondeterministic environments Proceedings Inter national Conference Artiﬁcial Neural Networks vol 2 Paris EC2 Cie 1995 pp 159164 34 J Schmidhuber Artiﬁcial curiosity based discovering novel algorithmic predictability coevolution Congress Evolutionary Computa tion CEC IEEE Press 1999 pp 16121618 35 J Schmidhuber Developmental robotics optimal artiﬁcial curiosity creativity music ﬁne arts Connect Sci 18 2 2006 173187 36 J Schmidhuber Formal theory creativity fun intrinsic motivation 19902010 IEEE Trans Auton Ment Dev 2 3 2010 230247 37 S Lange M Riedmiller Deep learning visual control policies European Symposium Artiﬁcial Neural Networks Computational Intelligence 38 M Franzius H Sprekeler L Wiskott Slowness sparseness lead place headdirection spatialview cells PLoS Comput Biol 3 8 2007 Machine Learning ESANN 2010 pp 265270 e166 39 R Legenstein N Wilbert L Wiskott Reinforcement learning slow features highdimensional input streams PLoS Comput Biol 6 8 2010 40 P Földiák MP Young Sparse coding primate cortex The Handbook Brain Theory Neural Networks vol 1 1995 pp 895898 41 G Mitchison Removing time variation antiHebbian differential synapse Neural Comput 3 3 1991 312320 42 G Wallis ET Rolls Invariant face object recognition visual Prog Neurobiol 51 2 1997 167194 43 L Wiskott Estimating driving forces nonstationary time series slow feature analysis arXiv preprint arXivcondmat0312317 2003 44 H Sprekeler T Zito L Wiskott An extension slow feature analysis nonlinear blind source separation J Mach Learn Res 15 2014 921947 45 VR Kompella M Luciw J Schmidhuber Incremental slow feature analysis Proc 20th International Joint Conference Artiﬁcial Intelligence IJCAI 2011 pp 13541359 VR Kompella et al Artiﬁcial Intelligence 247 2017 313335 335 46 VR Kompella M Luciw J Schmidhuber Incremental slow feature analysis adaptive lowcomplexity slow feature updating highdimensional 47 M Luciw VR Kompella S Kazerounian J Schmidhuber An intrinsic value developing multiple invariant representations incremental input streams Neural Comput 24 11 2012 29943024 slowness learning Front Neurorobot 7 2013 48 VR Kompella M Luciw M Stollenga L Pape J Schmidhuber Autonomous learning abstractions curiositydriven modular incremental slow feature analysis Proc Joint Conference Development Learning Epigenetic Robotics ICDLEPIROB IEEE San Diego 2012 pp 18 49 RS Sutton D Precup S Singh Between MDPs semiMDPs framework temporal abstraction reinforcement learning Artif Intell 112 1 1999 181211 2002 pp 295306 50 M Stollenga L Pape M Frank J Leitner A Förster J Schmidhuber Taskrelevant roadmaps framework humanoid motion planning IEEERSJ International Conference Intelligent Robots Systems IROS IEEE 2013 pp 57725778 51 I Menache S Mannor N Shimkin Qcut dynamic discovery subgoals reinforcement learning Machine Learning ECML 2002 Springer 52 O Sim sek AG Barto Skill characterization based betweenness NIPS08 2008 pp 14971504 53 MG Lagoudakis R Parr Leastsquares policy iteration J Mach Learn Res 4 2003 11071149 54 MB Ring Child A ﬁrst step continual learning Mach Learn 28 1 1997 77104 55 B Bakker J Schmidhuber Hierarchical reinforcement learning based subgoal discovery subpolicy specialization F Groen et al Eds Proc 8th Conference Intelligent Autonomous Systems IAS8 IOS Press Amsterdam NL 2004 pp 438445 56 SW Hart The development hierarchical knowledge robot systems PhD thesis University Massachusetts Amherst 2009 57 M Huber RA Grupen A hybrid discrete event dynamic systems approach robot control Tech Rep Univ Mass Dept Comput Sci Amherst MA 58 G Konidaris AG Barto Skill discovery continuous reinforcement learning domains skill chaining Advances Neural Information Process 1996 pp 96143 ing Systems 2009 pp 10151023 59 G Konidaris S Kuindersma AG Barto R Grupen Constructing skill trees reinforcement learning agents demonstration trajectories Advances Neural Information Processing Systems 2010 pp 11621170 60 J Mugan B Kuipers Autonomous learning highlevel states actions continuous environments IEEE Trans Auton Ment Dev 4 1 2012 7086 61 A Baranes P Oudeyer Active learning inverse models intrinsically motivated goal exploration robots Robot Auton Syst 61 1 2013 4973 Psychol 4 2013 62 H Ngo M Luciw A Förster J Schmidhuber Learning skills play artiﬁcial curiosity Katana robot arm Proceedings International Joint Conference Neural Networks IJCNN June 2012 pp 18 63 H Ngo M Luciw A Förster J Schmidhuber Conﬁdencebased progressdriven selfgenerated goals skill acquisition developmental robots Front 64 I Jolliffe Principal Component Analysis Wiley Online Library 2005 65 N Sprague Predictive projections Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 2009 pp 12231229 66 H Sprekeler On relation slow feature analysis Laplacian eigenmaps Neural Comput 23 12 2011 32873302 67 S Mahadevan M Maggioni Protovalue functions Laplacian framework learning representation control Markov decision processes J Mach Learn Res 8 21692231 2007 16 68 M Luciw J Schmidhuber Low complexity protovalue function learning sensory observations incremental slow feature analysis Proc 22nd International Conference Artiﬁcial Neural Networks ICANN Springer Lausanne 2012 pp 279287 69 J Schmidhuber Learning generate subgoals action sequences T Kohonen K Mäkisara O Simula J Kangas Eds Artiﬁcial Neural Networks Elsevier Science Publishers BV NorthHolland 1991 pp 967972 70 J Schmidhuber R Wahnsiedler Planning simple trajectories neural subgoal generators JA Meyer HL Roitblat SW Wilson Eds Proc 2nd International Conference Simulation Adaptive Behavior MIT Press 1992 pp 196202 71 M Wiering J Schmidhuber HQlearning Adapt Behav 6 2 1998 219246 72 VR Kompella MF Stollenga M Luciw J Schmidhuber Explore learn perceive actions free skillability International Joint Conference Neural Networks IJCNN IEEE 2014 pp 27052712 73 VR Kompella Slowness learning curiositydriven agents PhD thesis Informatics Department Università della Svizzera Italiana 2014 74 A Saltelli K Chan EM Scott et al Sensitivity Analysis vol 134 Wiley New York 2000 75 ID Guedalia M London M Werman An online agglomerative clustering method nonstationary data Neural Comput 11 2 1999 521540 76 D Zhang D Zhang S Chen K Tan K Tan Improving robustness online agglomerative clustering method based kernelinduce distance measures Neural Process Lett 21 1 2005 4551 77 EW Forgy Cluster analysis multivariate data eﬃciency versus interpretability classiﬁcations Biometrics 21 1965 768769 78 Y Zhang H Wu L Cheng Some new deformation formulas variance covariance Proceedings International Conference Modelling Identiﬁcation Control ICMIC IEEE 2012 pp 987992 79 D Wierstra T Schaul J Peters J Schmidhuber Natural evolution strategies IEEE World Congress Computational Intelligence IEEE 2008 pp 33813387 80 J Weng Y Zhang W Hwang Candid covariancefree incremental principal component analysis IEEE Trans Pattern Anal Mach Intell 25 8 2003 10341040 81 D Peng Z Yi W Luo Convergence analysis simple minor component analysis algorithm Neural Netw 20 7 2007 842850 82 M Luciw VR Kompella J Schmidhuber Hierarchical incremental slow feature analysis Workshop Deep Hierarchies Vision Vienna 2012 83 S Höfer M Spranger M Hild Posture recognition based slow feature analysis Language Grounding Robots Springer 2012 pp 111130