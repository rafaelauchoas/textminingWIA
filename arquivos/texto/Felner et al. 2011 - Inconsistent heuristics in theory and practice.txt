Artiﬁcial Intelligence 175 2011 15701603 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Inconsistent heuristics theory practice Ariel Felner Zhifu Zhang c Uzi Zahavi b Robert Holte c Jonathan Schaeffer c Nathan Sturtevant c Department Information Systems Engineering BenGurion University Negev BeerSheva 85104 Israel b Department Computer Science BarIlan University RamatGan 52900 Israel c Department Computing Science University Alberta Edmonton Alberta T6G2E8 Canada r t c l e n f o b s t r c t Article history Received 26 July 2010 Received revised form 10 February 2011 Accepted 10 February 2011 Available online 18 February 2011 Keywords Heuristic search Admissible heuristics Inconsistent heuristics A IDA 1 Introduction overview In ﬁeld heuristic search usually assumed admissible heuristics consistent implying consistency desirable attribute The term inconsistent heuristic times portrayed negatively avoided Part historical early research discovered inconsistency lead poor performance nodes reexpanded times However issue fully A investigated reconsidered invention IDA This paper shows preconceived notions inconsistent heuristics outdated The worstcase exponential time inconsistent heuristics shown occur contrived graphs edge weights exponential size graph Furthermore paper shows avoided inconsistent heuristics add diversity heuristic values search lead reduction number node expansions Inconsistent heuristics easy create contrary common perception AI literature To demonstrate number methods achieving effective inconsistent heuristics presented Pathmax way propagating inconsistent heuristic values search parent children This technique generalized bidirectional pathmax BPMX propagates values parent child node vice versa BPMX integrated When inconsistent heuristics BPMX experimental results IDA Positive results large reduction search effort required IDA presented A searches A 2011 Elsevier BV All rights reserved Heuristic search algorithms A 22 guided cost function f n gn hn gn cost current path start node node n hn heuristic function estimating cost n goal node If hn admissible lower bound algorithms guaranteed ﬁnd optimal paths 15 IDA The A algorithm guaranteed return optimal solution admissible heuristic There quirement heuristic consistent1 It usually assumed admissible heuristics consistent In popular AI textbook Artiﬁcial Intelligence A Modern Approach Russell Norvig write work hard concoct Corresponding author Email addresses felnerbguacil A Felner zahaviubiuacil U Zahavi holtecsualbertaca R Holte jonathancsualbertaca J Schaeffer nathanstcsualbertaca N Sturtevant zhangcsualbertaca Z Zhang 1 A heuristic consistent states x y hx cid2 cx y h y cx y cost shortest path x y Derivations deﬁnitions consistent inconsistent heuristics provided Section 3 00043702 matter 2011 Elsevier BV All rights reserved doi101016jartint201102001 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1571 heuristics admissible consistent 38 Many researchers work assumption admis sible heuristics consistent 25 Some algorithms require heuristic consistent Frontier A 30 searches closed list2 The term inconsistent heuristic times portrayed negatively avoided Part historical early research discovered inconsistency lead poor performance A However issue inconsistent heuristics fully investigated reconsidered invention IDA This paper argues perceptions inconsistent heuristics wrong We inconsistent heuris tics beneﬁts Further practice search domains We observe recently developed heuristics inconsistent A known problem inconsistent heuristics cause algorithms like A ﬁnd shorter paths nodes previously expanded inserted closed list If happens nodes moved open list chosen expansion This phenomenon known node reexpansion A inconsistent heuristic perform exponential number node reexpansions 32 We present insights phe nomenon showing exponential time behavior appears contrived graphs edge weights heuristic values grow exponentially graph size For IDA important note node reexpansion inevitable algorithms depthﬁrst search The use inconsistent heuristic exacerbate Because history pre heuristic consistent vious searches maintained separate path node examined IDA Inconsistent heuristics add diversity heuristic values search We values escape heuristic depressions regions search space low heuristic values lead large reduction search effort Part achieved generalization pathmax bidirectional pathmax The idea pathmax introduced Mero 34 method propagating inconsistent values search parent node children Pathmax causes f values nodes monotonic nondecreasing path search tree The pathmax idea undirected state spaces generalized bidirectional pathmax BPMX BPMX propagates values similar manner pathmax directions parent child child parent BPMX turns effective pathmax practice It easily integrated IDA Using BPMX propagation inconsistent values allows search escape heuristic depressions quickly slightly effort A Trivially create inconsistent heuristics taking consistent heuristic degrading values The resulting heuristic informed Contrary perception literature informed inconsistent heuristics easy create General guidelines number simple methods creating effective inconsistent heuristics provided The characteristics inconsistent heuristics analyzed provide insights effectively use reduce search effort Finally experimental results inconsistent heuristics BPMX yields signiﬁcant reduction based search applications The application domains slidingtile search effort required IDA puzzle Pancake problem Rubiks cube TopSpin pathﬁnding maps A The paper organized follows In Section 2 provide background material Section 3 deﬁnes consistent consistent heuristics Section 4 presents study behavior A inconsistent heuristics BPMX introduced Section 5 attributes inconsistent heuristics studied Methods creating inconsistent heuristics discussed Section 6 Extensive experimental results IDA provided Sections 7 8 respectively Finally provide conclusions Section 9 A Portions work previously published 14214447 This paper summarizes line work ties results In addition new experimental results provided 2 Terminology background This section presents terminology background material research 21 Terminology Throughout paper following terminology A state space graph vertices called states The execution search algorithm A initial state creates search graph A search tree spans graph according progress search algorithm The term node paper refer nodes search tree Each node search tree corresponds state state space The search tree contain nodes correspond state different paths These called duplicates IDA The fundamental operation search algorithm expand node compute generate nodes successors search tree We assume node expansion takes time This allows measure time complexity algorithms terms total number node expansions performed algorithm solving 2 The breadthﬁrst heuristic search algorithm 49 competitor Frontier A requirement works inconsistent heuristics 1572 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 1 3 3 3 Rubiks cube given problem3 The space complexity search algorithm measured terms number nodes need stored simultaneously A second measure number unique states expanded search The phrase number distinct expanded states refers measure denoted N The term cx y denote cost shortest path x y In addition hx denotes admissible x denotes cost shortest path x goal cx goal heuristic x goal h 22 Search algorithms The A A algorithm bestﬁrst search algorithm 15 It keeps open list nodes denoted OPEN usually implemented priority queue initialized start state node At expansion step algorithm node minimal cost extracted OPEN children generated added OPEN The expanded node inserted closed list denoted CLOSED The algorithm halts goal node chosen expansion A employs duplicate detection mechanism stores node given state Before node added OPEN ﬁrst matched OPEN CLOSED If duplicate node node state OPEN node smaller gvalue kept OPEN If duplicate node CLOSED smaller equal gvalue newly generated node ignored If node CLOSED larger gvalue copy CLOSED removed copy smaller gvalue added OPEN iterativedeepening version A requires memory linear number distinct states expanded uses cost function f n gn hn gn cost reaching node n start node best known path hn estimate remaining distance n goal If hn admissible estimate guaranteed return shortest path solution exists 6 lower bound actual distance A proven admissible complete optimally effective 6 With Furthermore consistent heuristic A inconsistent heuristic A optimal respect number distinct states expanded N reexpand nodes times A IDA 22 It performs series depthﬁrst searches increasing solutioncost threshold T T initially set hs s start node If goal threshold proceeds iteration increasing T minimum f value search ends successfully Otherwise IDA given heuristic consistent exceeded T previous iteration The worstcase time complexity IDA O N 2 trees O 22N directed acyclic graphs 31 ΩN cyclic undirected graphs The space complexity O bd b maximum branching factor d maximum depth search number edges IDA effectively solve traversed root goal Despite worstcase time bounds practice IDA combinatorial problems especially ones state spaces small cycles Due modest space complexity IDA exhausts available memory arriving solution solve problems A 23 Applications We provide overview application domains paper 231 Rubiks cube Rubiks cube invented 1974 Ern o Rubik Hungary The standard version consists 3 3 3 cube Fig 1 different colored stickers exposed squares subcubes cubies There 20 movable cubies stable cubies center face The movable cubies divided corner cubies faces edge cubies faces Corner cubies corner positions edge cubies edge positions There 4 1019 different reachable states In goal state squares cube color Pruning redundant moves results search tree asymptotic branching factor 1334847 244 Pattern databases PDBs effective commonlyused heuristic domain 3 In experiments IDA common report number generated nodes instead number node expansions We follow practice IDA experiments 4 We adopt setting ﬁrst Korf 24 90degree 180degree rotation face count legal A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1573 Fig 2 20 4TopSpin puzzle Fig 3 The 8 15 24puzzle goal states 232 TopSpin puzzle The n rTopSpin puzzle n tokens arranged ring Fig 2 The ring tokens shifted cyclically clockwise counterclockwise The tokens pass reverse circle ﬁxed ring At given time r tokens located inside reverse circle These tokens reversed rotated 180 degrees The task rearrange puzzle tokens sorted increasing order The 20 4 version puzzle shown Fig 2 goal position tokens 19 20 1 2 reverse circle reversed We classic encoding puzzle N operators clockwise circular shift length 0 N 1 entire ring followed reversalrotation tokens reverse circle 4 Each operator cost Note n different ways permute tokens However puzzle cyclic relative location different tokens matters n 1 unique states PDBs effective heuristic puzzle 233 The slidingtile puzzles One classic examples singleagent pathﬁnding problem AI literature slidingtile puzzle Three common versions puzzle 3 3 8puzzle 4 4 15puzzle 5 5 24puzzle They consist square frame containing set numbered square tiles position called blank The legal operators slide tile horizontally vertically adjacent blank blanks position The objective rearrange tiles random initial solvable conﬁguration particular desired goal conﬁguration The state space grows exponentially size number tiles increases shown ﬁnding optimal solutions sliding tile puzzle NPcomplete 37 The 8puzzle contains 92 181 440 reachable states 15puzzle contains 1013 reachable states 24puzzle contains 1025 states The goal states puzzles shown Fig 3 The classic admissible heuristic function slidingtile puzzles called Manhattan Distance It computed counting number grid units tile displaced goal position summing values tiles excluding blank PDBs provide best existing admissible heuristics problem 234 The pancake puzzle The pancake puzzle inspired waiter navigating busy restaurant stack n pancakes 8 The waiter wants sort pancakes ordered size deliver pancakes pleasing visual presentation Having free hand available operation lift portion stack reverse In domain state permutation values 0 n 1 A state n 1 successors kth successor formed reversing order ﬁrst k 1 elements permutation 0 k cid2 n 1 For example n 5 successors goal state cid30 1 2 3 4cid4 cid31 0 2 3 4cid4 cid32 1 0 3 4cid4 cid33 2 1 0 4cid4 cid34 3 2 1 0cid4 shown Fig 4 From state possible reach 1574 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 4 The 5pancake puzzle permutation size state space n In domain operator applicable state Hence constant branching factor n 1 PDBs wellinformed commonlyused heuristic domain5 Fig 5 Sample game map 235 Pathﬁnding A map m n grid passable areas obstacles There possible movements positionfour cardinal moves diagonal movessubject obstacles boundary conditions Cardinal moves cost 1 diagonal 2 Fig 5 shows maps experiments 512 512 grid The goal instance moves cost point A point B fewest number moves traversing light area In general application best heuristic depends properties domain 24 Pattern database heuristics The eﬃciency singleagent search algorithm largely dictated quality heuristic An effective commonlyused heuristic application domains paper memory tablebased heuristics The largest body work heuristics pattern databases 5 PDBs PDBs experimental studies purpose section background details However important note 5 The best heuristic known puzzle called gap heuristic 16 uses domaindependent attributes A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1575 Fig 6 States state space mapped patterns abstract space Fig 7 Example regular PDB lookups papers key ideas inconsistency BPMX depend heuristic PDB ideas apply heuristics forms PDB heuristics allow achieve stateoftheart performance application domains PDBs built follows6 The state space permutation problem represents different ways placing given set objects given set locations possible states A subproblem abstraction original problem deﬁned considering objects treating irrelevant dont care A pattern abstract state speciﬁc assignment locations objects subproblem The pattern space abstract space set different reachable patterns given abstract problem Each state original state space abstracted state pattern space considering pattern objects ignoring The goal pattern abstraction goal state As illustrated Fig 6 edge different patterns p1 p2 pattern space exist states s1 s2 original problem p1 abstraction s1 p2 abstraction s2 operator original problem space connects s1 s2 A pattern database PDB lookup table stores distance pattern goal pattern pattern space A PDB built running breadthﬁrst search7 backwards goal pattern entire pattern space spanned A state s original space mapped pattern p ignoring details state description preserved pattern The value stored PDB p lower bound serves admissible heuristic distance s goal state original space pattern space abstraction original space Pattern databases proven powerful technique ﬁnding effective lower bounds numerous combinatorial puzzle domains 245261011 Furthermore proved useful search problems multiple sequence alignment 3348 planning 9 241 Pattern database example PDBs built slidingtile puzzles illustrated Fig 7 Assume subproblem deﬁned include tiles 2 3 6 7 tiles ignored 2 3 6 7 The resulting 2367PDB entry pattern containing distance pattern goal pattern shown Fig 7d Fig 7bd depicts PDB lookup estimating distance given state S Fig 7a goal Fig 7c State S mapped 2367 pattern 6 We deﬁnition PDBs speciﬁc permutation state spaces paper However PDBs built wider set state spaces abstractions planning domains 9 combinatorial problems 1033 7 This description assumes operators cost Uniform cost search cases operators different costs 1576 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 8 Partitionings reﬂections tile puzzles ignoring tiles 2 3 6 7 Fig 7b Then patterns distance goal pattern Fig 7d looked PDB To speciﬁc PDB represented 4dimensional array PDB array indexes locations tiles 2 3 6 7 respectively lookup state S PDB8121314 tile 2 location 8 tile 3 location 12 As example consider cubies yellow face Rubiks cube A yellow face PDB store distances conﬁgurations yellow cubies goal location These distances admissible heuristics complete set cubies 242 Additive PDBs The best existing method optimally solving slidingtile puzzles uses disjoint additive pattern databases 1026 The tiles partitioned disjoint sets PDB built set An x yz partitioning partition tiles disjoint sets cardinalities x y z We build PDB set stores cost moving tiles pattern set given arrangement goal positions For PDB moves tiles sets counted The important attribute puzzle changes location tile Since set pattern tiles count moves pattern tiles moves tile values different disjoint PDBs added results admissible Fig 8 presents 78 partitionings 15puzzle 6666 partitionings 24puzzle ﬁrst context additive PDBs 1026 3 Consistent inconsistent heuristics Admissibility desirable property heuristic guarantees solution returned A optimal Another attribute heuristic consistent An admissible heuristic h consistent states x y path x y IDA hx cid2 cx y h y 1 cx y cost leastcost path x y 15 This kind triangle inequality estimated distance goal x reduced moving different state y adding estimate distance goal y cost reaching y x Pearl 36 showed restricting y neighbor x produces equivalent deﬁnition intuitive interpretation moving state neighbor h decrease cost edge connects This means cost function f n gn hn nondecreasing given path search graph We monotonicity8 cost function f guaranteed h consistent Note consistency property heuristic h monotonicity property cost function f n gn hn In Section 5 different methods enforcing monotonicity consistency If graph undirected cost going x y y x Since heuristic consistent h y cid2 c y x hx Merging Eqs 1 2 yields alternative deﬁnition consistent heuristics undirected state spaces cid2 cid2 cid2hx h y cid2 cid2 cx y 2 3 This inequality means moving parent child search tree heuristic h increase decrease change g 31 Inconsistent heuristics An admissible heuristic h inconsistent pair states x y hx cx y h y 4 8 Pearl 36 term monotonicity different sense A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1577 Fig 9 Inconsistent heuristic Fig 10 Reexpanding nodes A If y successor x f value decrease moving x y The cost function f case referred nonmonotonic cost function Similar reasoning undirected graphs heuristic inconsistent pair states x y cid2 cid2 cid2hx h y cid2 cx y 5 This means difference heuristic values x y larger actual cost going x y According deﬁnition types inconsistencies undirected search graphs shown Fig 9 As ﬁgures paper number inside node admissible hvalue An edge generally labeled cost Type 1 h decreases parent child The parent node p f p gp hp 5 5 10 Since heuristic admissible path start node goal node passes p cost 10 Since edge p c1 cost 1 f c1 gc1 hc1 6 2 8 This lower bound total cost reaching goal c1 This weaker lower bound parent valid children Thus information provided evaluating c1 inconsistent sense agree information parent p In case f nonmonotonic moving p c1 Type 2 h increases parent child Node c2 presents possible case inconsistency case inconsistent graph undirected Here heuristic increased 5 8 cost edge monotonic increasing p f 10 c2 f 14 However increase 1 The cost function f hvalue larger increase gvalue Note graph undirected edge c2 p Hence logically p children c2 In second occurrence p f value decrease 14 12 nonmonotonic Thus historical claim Pearl 36 consistency equivalent monotonicity technically correct9 The difference types inconsistency important later pathmax prop agation deals Type 1 corrects heuristics monotonic new bidirectional pathmax BPMX described deals Type 2 cause heuristic fully consistent Note good behavior consistent heuristics reexpand nodes usually comes cost function f monotonic 32 Inconsistent heuristics A IDA Assume state reached start state multiple paths possibly different cost Whenever ﬁrst matched OPEN CLOSED duplicate copy larger node generated A gvalue ignored If consistent heuristic f monotonic ancestors node n f values f n K optimal gvalue equal f n Therefore ﬁrst time node n expanded A possible paths start n Otherwise ancestors n optimal path n OPEN f value smaller K expanded prior n As consequence node expanded moved CLOSED chosen expansion By contrast inconsistent heuristics f function nonmonotonic A reexpand nodes reached lower cost path A simple example shown Fig 10 Nodes b c generated start node expanded f b 1 6 7 f c 3 1 4 Next node c expanded goal discovered f value 8 Since b lower f value expanded resulting lower cost path c This operation referred reopening nodes 9 In practical applications common practice known parent pruning list parent node children In cases heuristic inconsistent according Eq 5 corresponding f function monotonic In practice search tree inconsistencies second case cost function f monotonic probably rare Therefore reminder paper generally assume inconsistent heuristics produce cost function f nonmonotonic 1578 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 11 G 5 Martellis family c case nodes CLOSED reopened moved OPEN Now c reexpanded lower gcost lower cost path length 7 goal So A use inconsistent heuristics comes real risk node expansions consistent heuristic As section shows risk nearly great previously thought In later section experiments inconsistent heuristics actually speed A IDA depthﬁrst search DFS algorithm perform duplicate detection10 Using IDA state space Fig 10 node c expanded twice paths heuristic consistent Thus inconsistent heuristics result additional problem reexpanding nodes exists IDA performance degradation search 4 Worstcase behavior A inconsistent heuristics This section presents analysis worstcase time complexity A If heuristic admissible consistent A However explained heuristic admissible consistent nodes reopened A O 2N node expansions N number distinct expanded states This proven Martelli 32 optimal terms number node expansions 36 p 85 inconsistent heuristics 41 The G family state spaces Martelli deﬁned family state spaces G cid3 3 G contains 1 states requires A O 2i node expansions ﬁnd solution 32 G 5 Martellis family shown Fig 11 number inside state heuristic value number edge cost There inconsistencies graph For example cn4 n3 1 hn4 hn3 6 The unique optimal path start n5 goal n0 states decreasing order index n5 n4 n0 n4 large heuristic value f n4 14 expanded possible paths goal f 14 involving states fully explored Thus n4 A expanded nodes n3 n2 n1 reopened expanded The sequence node expansions reaching goal f values shown inside parentheses follows n523 n111 n212 n110 n313 n19 n210 n18 n414 n17 n28 n16 n39 n15 n26 n14 Note n4 expanded entire sequence expansions occurred prior expansion n4 repeated time nodes examined paths n4 Thus existence n4 G 5 essentially doubles search effort required G 4 This property holds ni total work O 2i As worstcase behavior hinges state space having properties edge weights heuristic values grow exponentially number states clearly seen deﬁnition Martellis state spaces 42 Variants A called B improves A Martelli devised variant A s worstcase time complexity maintaining admis sibility 32 algorithm B maintains global variable F keeps track maximum f value nodes expanded far search When choosing node expand fm minimum f value OPEN satisﬁes fm cid3 F node minimum gvalue f F chosen Because value fm chosen A F change increase node expanded ﬁrst time node expanded 10 In advanced implementation IDA DFS detect current node appeared ancestors current branch tree However small portion possible duplicates detected method compared algorithms OPEN CLOSED lists A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1579 Fig 12 First explored path given value F worstcase time complexity algorithm B O N 2 node expansions However improvement worstcase scenario poor reinforcing impression inconsistency undesirable Bagchi Mahanti proposed algorithm C variant B changing condition special case fm F fm cid2 F altering tiebreaking rule prefer smaller gvalues 1 Cs worstcase time complexity Bs O N 2 43 New analysis Although Martelli proved number node expansions A performs exponential number distinct expanded states behavior reported realworld applications A His family worstcase state spaces solution costs heuristic values grow exponentially number states We present new result exponential growth solution costs heuristic values necessary conditions A s worstcase behavior occur We assume edge weights nonnegative integers edge weights zero permitted The key quantity analysis cid3 deﬁned greatest common divisor nonzero edge weights The cost path start node node n multiple cid3 difference costs paths start node n Therefore search reopen n new path smaller cost current gn value know gn reduced cid3 Theorem 1 If A performs M N node expansions node heuristic value LB cid3 cid6M NNcid7 M node expansions N distinct expanded states number reexpansions Proof If A M N By pigeonhole principle node K cid6M NNcid7 reexpansions Each reexpansion decrease gK cid3 process gvalue K reduced LB cid3 cid6M NNcid7 cid2 In Fig 12 S start node K node reexpanded cid6M NNcid7 times seen node exist L path resulted ﬁrst expansion K upper path K B path resulted expansion K Denote f gvalues path L f L gL f gvalues upper path f glast respectively Node B node upper path excluding S maximum f value maximum f value node upper path Nodes distinct S K exist path direct edge S K K opened soon S expanded gvalue smaller gLK Hence K expanded L leading contradiction Node B intermediate nodesit S deﬁnition K f lastK largest f value entire upper path expanded K expanded L contradiction Hence B intermediate node S K hB large f lastB cid3 f LK K ﬁrst expanded L We use following facts hB LB f lastB glastB hB f lastB cid3 f LK f LK gLK hK glastB cid2 glastK LB cid2 gLK glastK So hB f lastB glastB Fact 6 cid3 f LK glastB Fact 7 gLK hK glastB Fact 8 6 7 8 9 10 1580 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 cid3 gLK glastK hK Fact 9 cid3 gLK glastK cid3 LB Fact 10 hK cid3 0 From Theorem 1 follows A cid3 cid62N NNcid7 A 2N node expansions node heuristic value N 2 node expansions node heuristic value cid3 N 1 Corollary 2 Let h start denote optimal solution cost If A performs N node expansions h start cid3 LB Proof Since proof Theorem 1 A LB cid2 expanded node B goal h start f B Corollary 3 If h N N λcid3 start cid2 λ M number node expansions A ﬁnd path goal equal Proof Using Corollary 2 cid4 cid3 M NN cid3 implies LB cid2 h start cid2 λ M cid2 N N λcid3 cid2 Corollary 4 Let m ﬁxed constant G graph arbitrary size depending m edge weights equal m Then M number node expansions A search G N N m N 1cid3 Proof Because nongoal nodes solution path expanded N 1 edges solution path h start λ m N 1 By Corollary 3 M cid2 N N λcid3 cid2 N N m N 1cid3 cid2 This example conditions A s worsttime complexity nearly bad Martellis bound suggests The key observation arising analysis section intimate relationship number node expansions magnitude heuristic values cost optimal path goal The number node expansions grow exponentially factors 5 Pathmax bidirectional pathmax It known f values path search tree forced monotonic nondecreasing This simply propagating f value parent child larger This technique usually called pathmax In section idea pathmax introduced undirected state spaces generalized new method called bidirectional pathmax provides better heuristic propagation 51 Pathmax cid8 Mero introduced algorithm B variant B dynamically updates heuristic values search main taining admissibility 34 This achieved adding rules known pathmax rules propagating heuristic values cid8 worstcase time complexity O N 2 node children Like algorithm B described Section 4 B The rules propagate While pathmax rules introduced context algorithm B applicable A heuristic values search parent node p child node ci edge connecting costs cp ci follows Pathmax Rule 1 hci maxhci hp cp ci Pathmax Rule 2 hp maxhp minci Successorsphci cp ci11 For Rule 1 know hp cid2 h h admissible We know h p h p cid2 cp ci h x denotes optimal cost goal node hci cid2 h ci ci possible path p goal goes ci By 11 This version pathmax Rule 2 The version original paper 34 clearly correct probably printing error A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1581 Fig 13 Pathmax Rule 1 Fig 14 Pathmax Rule 2 Fig 15 Example closed node reopened pathmax ci cid3 hp cp ci Fig 13 shows parent node p updates combining facts inferred h heuristic values child nodes c1 c2 according Rule 1 A consequence rule child node inherits f value parent node larger Pathmax Rule 1 written f ci max f p f ci This causes f value monotonic nondecreasing path However child node heuristic value larger parent change g heuristic inconsistent graph undirected inconsistency Type 2 presented end Section 31 Our bidirectional pathmax method BPMX described deals cases corrects type inconsistency The explanation Rule 2 introduced Mero follows In directed state spaces optimal path p p large minci Successorsphci cp ci goal contain ps successors p goal h Rule 2 corrects hp reﬂect Fig 14 shows child nodes c1 c2 update heuristic value parent node p according Rule 2 c1 minimal f value value propagated parent While idea Rule 2 correct practical value limited First state spaces undirected state spaces edge state p parent shortest path p goal pass state In cases Rule 2 relevant actually listed child p search graph This possible parent pruning optimization We discuss limitations Rule 2 Section 54 introduce generalization Rule 1 bidirectional pathmax 52 Pathmax f function monotonic It thought pathmax Rule 1 actually converts nonmonotonic cost function f monotonic cost function consequence node reexpansion prevented12 This correct It true applying pathmax f values decrease path traversed However f values non monotonic paths traversed To recall consistent heuristic cost function node removed OPEN ﬁrst time monotonic closed nodes reopened A guaranteed leastcost path This key advantage consistent heuristic inconsistent heuristic nonmonotonic cost function closed nodes reopened Pathmax correct deﬁciency inconsistent heuristics This noted Nilsson 35 p 153 Zhou Hansen 50 Consider example Fig 15 heuristic admissible inconsistent ha 99 hb 1 f nonmonotonic f b f The optimal path example startabgoal cost 100 A expand start c f 30 point b OPEN f 99 b pathmax f 30 instead f 21 b expanded closed leastcost path b 12 We use term consistent understanding cost function monotonic heuristic inconsistent Type 2 presented Section 31 undirected graphs 1582 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 16 Example BPMX The arrows direction propagation heuristic values Propagation occurs bold edges A revealing better path b requiring b nodes T expanded second time expand entire set nodes subtree T expands At point expanded 53 Bidirectional pathmaxBPMX Meros Rule 1 deﬁned propagate values parent child search tree However pathmax rule applied given node x node y direction necessarily parent children search tree long path x y This beneﬁcial application domains search graph undirected operators invertible costs symmetric Assume admissible heuristic h hx y cid3 h y cx y Now h x cx y cid3 hx cx y h admissible apply following general rule h y possible path x goal passes y Therefore h x cid2 cx y h h y max cid5 cid6 h y hx cx y 11 Pathmax Rule 1 general rule parent node children In search tree edge child c parent p achieved introducing new pathmax rule childrentoparent value propagation follows Pathmax Rule 3 hp maxhp hc cc p Fig 16a shows Rule 3 The heuristic child c1 propagated parent p ps heuristic increased 3 8 Our new method bidirectional pathmax BPMX uses Rules 1 3 propagate inconsistent heuristic values direction described generally Eq 11 Large heuristic values propagated edges preserve admissibility subtract weight edges way Therefore updating nodes value cascading effect neighbors propagation started child c1 continues parent children shown Fig 16b The BPMX process stops arrive node original heuristic value smaller propagated value The bold edges Fig 16b correspond cases BPMX propagates new heuristic value p children c2 c3 By contrast child c4 exploit BPMX original heuristic value 8 BPMX propagate value 6 Note Rule 1 deals inconsistencies Type 1 described Section 31 causes cost function monotonic edge BPMX extends inconsistencies Type 2 causes heuristic fully consistent 531 BPMX IDA Before discussing BPMX IDA ﬁrst highlight following observation Observation What important IDA exact f value node f value causes cutoff Explanation IDA T Thus cutoff reduces work performed expands node f value equal current threshold T backtracks larger It immediately obvious Rule 1 IDA beneﬁt13 This propagating heuristic parent p Rule 1 child c cause f c f p It increase f value threshold T f value p equal T result additional pruning Using Rule 3 IDA fully expanded For example suppose current IDA great potential prune nodes generated threshold T Fig 17 2 Without propagation h 13 We discuss Rule 2 context IDA Section 54 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1583 Algorithm 1 IDA BPMX adds element list Fig 17 BPMX In IDA right branch generated bpmxinitial_node s threshold hs repeat function IDA 01 02 03 04 05 06 07 08 end function GoalFound return Path GoalFound DFSbpmxs NULL 0 Path threshold hs threshold next_threshold cid11 Returns optimal solution h p hp h p g threshold return false p goal_node return true legal_move mi 1 continue mi pm generate child ci applying mi p DFSbpmxci mi g cp ci Path threshold hci true 09 boolean function DFSbpmxnode p previous_move pm depth g List Path integer threshold heuristic_value h p 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 end function h p maxh p hci h p g threshold return false Path mi Path return true cid11 Rule 3 cid11 Backtrack ASAP end return false cid11 Parent pruning cci p end left child parent node f p gp hp 0 2 2 right child f c1 gc1 hc1 1 1 2 expanded When BPMX propagation following occur The left child f c2 1 5 6 T 2 IDA backtrack However BPMX update parents hvalue hp 4 overall cost f p 0 4 4 This results cutoff search backtrack root node generating right child heuristic value modiﬁed 3 A discussed An eﬃcient implementation BPMX IDA provided Algorithm 1 In implementation Rule 3 applied free backtracking child First heuristic parent p updated Rule 3 line 20 Then f value parent larger threshold subtree immediately pruned line 21 control passed parent p In case children p generated An alternative exhaustive implementation stop line 21 continue generate children p calculate heuristics This result propagating higher heuristic values Rule 3 parent p increase chance pruning ancestors p The drawback implementation parent p fully expanded14 We experimented variant domains studied paper However gains provided lazy approach stopping soon cutoff occurred consistently outperformed exhaustive variant Therefore report experimental results lazy variant A reminiscent idea BPMX propagating heuristic values nodes introduced context learn ing heuristics DFS searches 3 The difference unlike BPMX learning algorithm work requires transposition table 532 BPMX A Due depthﬁrst nature BPMX propagation easily implemented IDA values propagated naturally BPMX updates diﬃcult nodes updated children parents By contrast A need retrieved OPEN CLOSED BPMX parameterized maximum depth heuristic value propagated BPMX1 extreme propagating h updates node children BPMX extreme propagating h updates far possible 14 This process extended perform klookahead search ﬁnd large heuristic values 1584 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Algorithm 2 A BPMX1 assumes symmetric edge costs BestH maxBestH lookupHneighbor ccurrent neighbor cid11 Stores parent hcost pathmax cid11 Cache lookups later use current pop best node queue current goal return extractPathstart goal neighbors generateSuccessorscurrent BestH 0 neighbor 1i neighbors end storeHcurrent maxlookupHcurrent BestH neighbor 1i neighbors EdgeCost ccurrent neighbor switch getLocationneighbor lookupHneighbor BestH EdgeCost storeHneighbor BestH EdgeCost end lookupGcurrent EdgeCost lookupGneighbor setParentneighbor current storeGneighbor lookupGcurrent EdgeCost reopenneighbor lookupGcurrent EdgeCost LookupGneighbor setParentneighbor current storeGneighbor lookupGcurrent EdgeCost updateKeyneighbor end BestH EdgeCost lookupHneighbor storeHneighbor BestH EdgeCost updateKeyneighbor addOpenNodeneighbor lookupGcurrent EdgeCost maxhneighbor goal BestH EdgeCost cid11 BPMX PMX update cid11 Found shorter path cid11 Found shorter path cid11 Resort OPEN cid11 BPMX PMX update cid11 applies BPMX PMX update pushstart case ClosedList bpmx1start goal 01 function A 02 03 queue 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 end 38 end 39 40 return nil 41 end function end case OpenList case NotFound end switch end BPMX1 implemented eﬃciently BPMX computation happens children node gen erated checked duplicates OPEN CLOSED addedmoved OPEN andor CLOSED Assume node p expanded k children c1 c2 ck generated References nodes saved faster manipulation following steps Let cmax node maximum heuristic value children let hmax hcmax In addition assume edge unit cost undirected hmax propagated parent node decreasing Rule 3 children decreasing Rule 1 Thus children ci heuristic cid5 hci hp 1 hmax 2 hBPMXci max cid6 After nodes value updated parent node inserted CLOSED new f value children inserted changed OPEN new f values Pseudocode eﬃcient implementation A BPMX1 shown Algorithm 2 There single data structure OPEN CLOSED implicit calls looking nodes In actual implementation lookups cached reduce overhead BPMXd d 1 starts new node generated continues propagate hvalues generated neighborhood nodes OPEN CLOSED long hvalues nodes increased There number possible implementations require ﬁnding retrieving nodes OPEN CLOSED Obviously incur following possible overheads associated BPMXd d 1 context A performing lookups OPEN andor CLOSED looking neighbors b ordering OPEN nodes based new f value values change c computational overhead comparing heuristic values assigning new value based propagations A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1585 Fig 18 Worstcase example BPMX Fig 19 Bestcase example BPMX These costs costs incurred performing A node expansions In BPMXd propagating heuristic values result equivalent multiple expansions reopenings The propagation reopenings follow children node depth d parameter satisﬁed As regard BPMX d 1 independent search process small optimization main search Our experimental results node expansions15 Therefore node expansions occur BPMX process cost A BPMXd 1 expansions When d 1 overhead remainder paper distinguish A included count node expansions time measurements A natural question determine value parameter d best It turns ﬁxed d optimal number node expansions graphs While particular d produce large reduction number node expansions given state space different state space result O N 2 increase number node expansions Fig 18 gives example worstcase behavior BPMX The heuristic values gradually increase nodes d When node b reached heuristic propagated node increasing heuristic value 1 When node c reached heuristic update propagated nodes b In general ith node chain generated BPMX update propagated previously expanded nodes Overall result 1 2 3 N 1 O N 2 propagation steps savings node expansions This provides general worstcase bound At entire set previously expanded nodes reexpanded BPMX propagations happens By contrast Fig 19 gives example BPMX propagation effective Assume node start node It expanded children b c goal generated f values f b 4 f c 3 f goal 50 Next c expanded d generated If BPMX activated left nodes subtree b f 50 expanded goal expanded search terminates Now consider case BPMX activated right While generating node d heuristic value propagated BPMX c b raising f value b 50 Note infer entire subtree b f cid3 50 In case f b f goal 50 assuming ties broken favor low hvalues goal expanded search halts expanding nodes 54 Pathmax Rule 2 We seen usefulness pathmax Rules 1 3 Mero created Rule 2 childrentoparent value propagation 34 Rule 2 hp maxhp minci Sucessorsphci cp ci We discuss properties Rule 2 541 Rule 2 IDA Similar Rule 1 beneﬁt Rule 2 IDA undirected state spaces pruning caused Assume node p children c1 c2 ck parent p shown Fig 20 Assume 15 This true PDB heuristics inexpensive compute However true cases heuristic calculation requires large time 1586 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 20 Example Rule 2 cm produced minimal applied p Rules 1 3 f value children We p beneﬁt Rule 2 p beneﬁt Rule 2 Assume p causing cutoff search In case search proceeds children Now minimum child cm causes cutoff children cause cutoff When Rule 2 children generated order ﬁnd minimum cost Either way Rule 2 children generated Rule 2 added value p beneﬁt Rule 2 Assume Rule 2 activated set f newp f cm Now activation Rule 1 ordinary pathmax f value monotonically increasing path search tree Thus f newp cid3 f p If f newp f p change course search applying Rule 2 Now consider case f newp f p Recall Rule 2 work list children p There cases The ﬁrst case cm produced minimal f value children Now apply Rule 3 hnewa hp ca p hcm cp cm ca p hcm cp ca p ha Thus change hvalue The second case cm cid13 child chosen minimum meaning ha ca p cid3 hcm ccm p Now apply Rule 3 hnewa hp ca p hcm cp cm ca p cid2 ha ca p ca p ha Here applying Rule 3 decrease hvalue unchanged Thus beneﬁt applying Rule 2 Rules 1 3 suﬃcient obtain potential beneﬁts 542 Rule 2 A Assume running A node p expanded Its children added OPEN p goes CLOSED If applying Rule 2 f value increases CLOSED higher f value new hvalue larger original hvalue This affect duplicate pruning future node p reached different path Furthermore Rule 2 special case k 1 klookahead search values frontier backed 23 repeated search root subtree In fact similar propagation heuristic learning LRTA trials place This applicable strict consistent heuristics Based implement Rule 2 experiments focus Rules 1 3 core aspects BPMX value propagation inconsistent heuristics 6 Creating inconsistent heuristics As illustrated quote Artiﬁcial Intelligence A Modern Approach 38 given earlier perception inconsistent admissible heuristics hard create However turns true The following examples use PDBbased heuristics applications paper create inconsistent heuristics However similar ideas applied heuristics We examples inconsistent heuristics pathﬁnding explicit graphs Section 8 It important note trivially heuristic inconsistent For example tablebased heuristic PDB randomly set table entries 0 Of course introducing inconsistency results strictly informed heuristic In section examples inconsistent heuristics provide informed values beneﬁt search 61 Random selection heuristics Many domains number heuristics available When heuristic search enter region bad low estimation values heuristic depression With single ﬁxed heuristic search forced traverse possibly large portion region able escape A wellknown solution problem consult number heuristics maximum value 5101819 2426 When search region low values heuristic region high values There tradeoff heuristic calculation increases time takes compute hn Additional heuristic consultations provide diminishing returns terms reduction number node expansions recommended use A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1587 Fig 21 Inconsistency compressed pattern database Given number heuristics select heuristic use randomly Only single heuristic consulted node additional time overhead needed ﬁxed heuristic Random selection heuristics introduces diversity values obtained search single ﬁxed selection The random selection heuristic produce inconsistent values little correlation heuristics Furthermore random selection heuristics produce inconsistent hvalues heuristics consistent When PDBs multiple heuristics arise exploiting domain speciﬁc geometric symmetries In particular additional PDB lookups performed given single PDB For example consider Rubiks cube suppose yellow face PDB described previously Section 241 Reﬂecting rotating puzzle enable similar lookups face different color green red faces symmetrical Different admissible heuristic values obtained lookups PDB As example consider main diagonal slidingtile puzzle Any conﬁguration tiles reﬂected main diagonal reﬂected conﬁgura tion shares attributes original Such reﬂections usually PDBs slidingtile puzzle 5101126 looked PDB In recent work learning algorithm decide switch heuristics 7 A classiﬁer map state heuristic considering likely quality heuristic estimate time needed compute value The resulting search inconsistencies heuristic values 62 Compressed pattern databases There tradeoff size tablebased heuristic PDB search performance Larger tables presumably contain detailed information enabling accurate heuristic values produced Researchers explored building large PDBs possibly disk compressing smaller PDBs 111227392 A common compression idea replace multiple PDB entries single entry exploiting locality property values entries highly correlated reducing size PDB To preserve admis sibility compressed entry store minimum value entries replacing This called lossy compression state lookups end effective heuristic value It shown values PDBs locally correlated heuristic accuracy preserved 11 Thus large PDBs built compressed smaller size little loss performance Such compressed PDBs informed uncompressed PDBs use memory 11 The compression process introduce inconsistency heuristic guarantee heuristic value adjacent states search space lose information compression For example consider PDB Fig 21 assume consistent Assume b c connected edge cost 1 During compression b mapped x abstract space c y To preserve admissibility x y contain minimum value states mapping locations Now states b c inconsistent abstract space difference heuristics 2 bigger actual distance 1 63 Dual heuristic The concept duality dual heuristics permutation state spaces introduced Zahavi et al 144445 Such heuristics produce inconsistent heuristic values The papers provide detailed discussion concepts Here provide suﬃcient details purposes In permutation state spaces states different permutations objects Similarly given operator sequence permutation transfers permutation permutation For state s dual state sd computed The basic deﬁnition follows Let π permutation transforms state s goal The dual state s labeled sd deﬁned state constructed applying π goal Alternatively O set operators transfer s goal applying O goal reach sd This dual state sd important property distance goal s The reason sequence operators maps s goal maps goal sd Since operators reversible permutation state spaces sequence inverted map sd goal Eﬃcient methods suggested deriving dual state sd given description state s 45 Since distance goal states identical admissible heuristic applied sd admissible s heuristic For state s term dual lookup 1588 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 22 Dual states parent children looking sd PDB When moving parent state child state performing dual lookup produce inconsistent value heuristic regular form consistent The explanation follows In standard search parent state p children ci neighbors deﬁnition Thus consistent heuristic return consistent values applied p ci However heuristic values obtained sd cd consistent sd cd necessarily neighbors illustrated Fig 2216 In general easy ways generate inconsistency given domain 1 use multiple different heuristics 2 heuristic values missing degraded We provided examples section This list exhaustive examples means ways creating inconsistent heuristics 64 Inconsistent versus consistent heuristics Besides potential hvalue propagation inconsistent heuristics attributes reduce num ber node expansions search compared consistent heuristics This section addresses attributes Most previous work admissible heuristics mainly concentrated improving quality heuristic sessment A heuristic h1 considered informed better quality h2 typically returns higher value arbitrary state 38 A facto standard usually researchers compare average values given heuristic entire domain space large sample states domain 10112426 Korf Reid Edelkamp denoted KRE introduced notion overall distribution heuristic values 2829 Deﬁne pv proba bility random state state space heuristic value v Likewise deﬁne P v probability random state heuristic value equal v KRE suggested distribution values heuristic function measure informedness function Doing admissible heuristics typically heuristic informed distribution values higher average value We Section 721 inconsistent heuristics distribution attributes consider single iteration KRE introduced formula predict number node expansions IDA consistent admissible heuristic 2829 Nb d P dcid7 i0 bi P d b bruteforce branching factor d depth search IDA threshold P heuristic distribution KRE showed P x deﬁned particular way equilibrium distribution number nodes n f n cid2 d equal Nb d P limit large d We nodes nodes potential expanded potential nodes short KRE proved consistent heuristics potential nodes eventually expanded IDA Assume n potential node Since heuristic consistent ancestor n f cid2 d potential node Then induction showed entire branch root n expanded nodes branch potential nodes For inconsistent heuristics behavior different For potential node n exist ancestor f d Once IDA visits node entire subtree pruned n generated A potential node actually expanded ancestors potential nodes This guaranteed consistent heuristics 16 This phenomenon explained original papers 144445 An example provided Appendix A paper A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1589 Fig 23 Consistent versus inconsistent heuristics Nodes marked hvalue inconsistent heuristics Thus inconsistent heuristic number potential nodes approximated Nb d P upper bound number node expansions17 Assume given PDB compare example dual random lookup PDB regular consistent lookup PDB Since exactly PDB heuristics perform single lookup overall distribution values number potential nodes However experimental results fewer nodes expanded practice The explanation shown Fig 23 Observe cases hvalue distribution level tree In particular depth nodes hvalue 3 single nodes hvalues 4 6 respectively In case consistent heuristic left ﬁgure threshold 5 nodes depth hvalue 3 f g h 2 3 5 current IDA expanded They potential nodes heuristic consistent ancestors potential nodes expanded The right subtree root pruned f value level 1 f g h 1 5 6 5 In case inconsistent heuristic right ﬁgure node depth expanded leftmost node The node hvalue 6 generated expanded f value 8 exceeds threshold Due BPMX value propagated parent Rule 3 parents hvalue changed 5 The f value parent changed 6 search backtrack generating rightmost child potential node h 3 7 Experiments IDA This section presents results different domains illustrate beneﬁts inconsistent heuristics BPMX IDA All experiments performed Intel P4 34 GHz 1 GB RAM 71 TopSpin We experimented 17 4TopSpin puzzle 17 356 1014 states A PDB leftmost 9 tokens built representing pattern space 17 16 9 882 10918 Given PDB 17 different symmetric geometrical lookups derived For example PDB 9 consecutive tokens 1 9 PDB 2 10 3 11 appropriate mapping tokens Since values PDB smaller 16 entry encoded 4 bits Hence PDB requires 247 MB space Some pairs operators commutative leading state When search IDA duplicate nodes avoided forcing commutative operators applied successively order For example operator reverses locations 1 2 3 4 related operator reverses locations 11 12 13 14 By forcing ﬁrst tried second eliminates unnecessary duplication search tree This operator ordering decreases number generated nodes order magnitude applied experiments Table 1 presents average number generated nodes average time seconds needed IDA solve 1000 random instances different PDB lookup strategies The ﬁrst column Lookups shows number PDB lookups n cid3 1 performed maximized The following PDBbased heuristics Regular A ﬁxed set n PDB lookups chosen node search Since nodes maximize lookups heuristic consistent Random Of 17 possible lookups n randomly chosen maximized node search Since secutive nodes h value computed differently possibly n different lookups resulting heuristic inconsistent Random BPMX The heuristics obtained combining inconsistent random lookups updated BPMX When multiple heuristics exist IDA following implementation enhancement save consider able number potential heuristic lookups For node n heuristic lookup determine f n cid2 T 17 Zahavi et al developed alternative formula predict number node expansions 4243 One beneﬁts provides accurate predictions inconsistent heuristics opposed upper bound 18 Since puzzle cyclic data stored linear array assume token number 1 leftmost position Thus implementation numbers divided 17 1590 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Table 1 Consistent inconsistent heuristics TopSpin 174 IDA Lookups 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Regular Consistent Nodes 40 019 429 6 981 027 1 787 456 651 080 332 642 208 062 148 003 116 208 95 863 81 749 71 451 64 227 58 455 53 926 50 376 47 784 45 849 Time 53129 10686 3213 1394 0835 0601 0484 0422 0382 0354 0335 0322 0312 0307 0303 0303 0304 Random Inconsistent Nodes 1 567 769 404 779 224 404 157 710 123 882 103 377 89 698 79 911 72 504 66 690 62 020 58 119 54 906 52 145 49 760 47 688 45 848 Time 2857 0865 0555 0443 0388 0356 0337 0324 0317 0311 0306 0304 0302 0301 0302 0301 0303 Random BPMX Inconsistent Nodes 564 469 279 880 187 797 143 051 116 779 99 653 87 596 78 609 71 709 66 184 61 682 57 947 54 773 52 079 49 736 47 663 45 849 Time 1032 0622 0480 0411 0373 0349 0332 0321 0315 0310 0306 0304 0303 0302 0302 0303 0304 exceed threshold meaning n expanded f n T meaning n pruned When max imizing multiple heuristics instead evaluating heuristics exhaustive evaluation computation stop heuristics exceeds T lazy evaluation lookups needed19 Lazy evaluation relevant A nodes expanded heuristics looked maximum needs calculated stored In addition IDA When BPMX beneﬁt exhaustive evaluations similar variant described Sec tion 531 Exhaustive evaluations yield higher values lazy evaluations leading additional BPMX cutoffs higher values propagated Experiments performance lazy exhaustive evaluations domains research In general nodes lazy evaluation occurs time node drop signiﬁcantly factor experiments By contrast exhaustive evaluations reduced number generated nodes reduction small 20 All results reported paper lazy evaluations The ﬁrst row Table 1 corresponds benchmark case lookup allowed The number generated nodes 40 019 429 Randomly selecting single lookup reduces number factor 255 1 567 769 nodes Adding BPMX reduces number generated nodes 564 469an improvement factor 709 bench mark This improvement achieved single PDB lookup The regular ﬁxed selection method needs different lookups PDB produce heuristic similar quality random selection BPMX row 4 This achieved potentially additional lookups increasing computational cost node Adding lookups provides diminishing returns Using lookups provide diversity heuristic values improvement factor additional lookup regular random decreases All selection methods converge case 17 lookups The random selection lookups converges faster For ﬁxed number potential lookups random selection strategy outperforms ﬁxed strategy When lookups possible relative advantage random selection decreases ﬁxed selection diversity values When interested time speedup variants regular consulting random lookups provide best time results nearly 03 seconds In practice course 17 symmetric lookups possible reason use 72 Rubiks cube Rubiks cube 20 movable cubes cubies 8 corners 12 edges The heuristic Rubiks cube usually obtained taking maximum PDBs corners covering edges 24 The 8corner PDB inconsistent manner corners examined symmetries dual lookup identical regular lookup This section reports results 7edge PDB There 24 lines geometrical symmetries arise different ways rotating reﬂecting cube For 7edge PDB symmetries considers different set edges resulting different PDB lookup Similar tendencies observed experiments based PDBs built mix edge corner cubies 19 One try order heuristics increase chance getting cutoff earlier 18 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1591 Table 2 Consistent inconsistent heuristics Rubiks cube IDA Row Lookups Heuristic One PDB lookup 1 2 3 4 5 1 1 1 1 1 Maxing multiple PDB lookups 6 7 8 9 10 2 2 2 4 4 Regular Dual Dual BPMX Random Random BPMX 2 Regular Regular Dual BPMX 2 Random BPMX 4 Regular 4 Random BPMX Nodes 90 930 662 19 653 386 8 315 116 9 652 138 3 829 138 13 380 154 2 997 539 1 902 730 1 053 522 1 042 451 Time 2818 738 324 330 125 491 134 083 064 064 Table 2 shows average number generated nodes average running time seconds set 100 Rubiks cube instances goal distance 14 Felner et al 14 The Lookups column gives number PDB lookups compute heuristic value state Lazy evaluation possible The following PDBbased heuristics Regular The regular PDB lookup This heuristic consistent set cubies PDB lookup parent child nodes Dual For node dual state calculated looked PDB This produce inconsistent heuristic values dual lookup parent consult different cubies dual lookup child Random Randomly select different 24 possible symmetric PDB lookups given node This inconsis tent set cubies parent necessarily child The table shows single random dual lookups perform better single regular lookup In addition BPMX improves results The dual lookup diverse regular lookup correlation successive lookups 14 Therefore search stuck region low heuristic values frequently happens regular lookups A random lookup BPMX faster regular lookup factor 24 dual lookup BPMX factor 25 Rows 610 results maximizing 2 4 regular random lookups It interesting random lookup BPMX outperforms regular lookups factor 35 number generated nodes factor 39 time Two random lookups better regular dual lookup better diversity values When lookups allowed values obtained regular lookups diverse advantage taking random lookups 721 Dynamic distribution heuristic values We claimed reason success inconsistent heuristic diversity values introduced search This section attempts greater understanding claim It easy analyze domain produce graph showing distribution values produced heuristic However obvious question ask static precomputed distribution reﬂects values actually seen search Of dynamic distribution values generated search Distinguishing static dynamic distributions heuristic values new previously explain maximum weak heuristics outperform strong heuristic 18 Fig 24 shows dynamic distribution heuristic values seen searches reported Table 2 static distribution values PDB The following observations results First dramatic difference static dynamic distribution values regular consistent heuristic As seen dynamic distribution regular lookup greatly shifted smaller heuristic values compared static distribution PDB This phenomenon discussed explained Holte et al 18 The main reason generated nodes deep search tree values necessarily small generated Second easy recognize heuristic best performance superior shifted right dynamic distribution heuristic values Note versions exactly PDB represented overall static distribution values Third regular heuristic poor dynamic distribution consistent heuristic value state low children state low values Inconsistent heuristics problem node receive value meaning distribution values seen closer static distribution 1592 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 24 Rubiks cube heuristic distributions Table 3 Rubiks cube random heuristic BPMX Choice Lookups 1 2 3 4 5 8 12 16 20 24 Dual 1 1 1 1 1 1 1 1 1 1 1 DBF 13355 9389 9388 9382 7152 7043 7036 6867 6852 6834 7681 Nodes 90 930 662 17 098 875 14 938 502 14 455 980 5 132 396 4 466 428 3 781 716 3 822 422 3 819 699 3 829 139 8 315 117 BPMX cuts 0 717 151 623 554 598 681 457 253 402 560 337 114 356 327 357 436 360 067 796 849 PDB Finally inconsistency effect improving dynamic distribution static distribution The greater degree inconsistency closer dynamic distribution approaches static distribution PDB 722 Dynamic branching factor BPMX The effectiveness BPMX characterized effect branching factor search The dynamic branching factor DBF deﬁned average number children generated node expanded search When heuristic function inconsistent BPMX employed dynamic branching factor smaller normal branching factor Table 3 presents DBF results Rubiks cube obtained 7edge PDB An experiment performed number possible PDB lookups varied single lookup randomly selecting set The ﬁrst column gives number available heuristics randomly select The columns results averaged set instances Table 2 In ﬁrst row PDB lookup Since PDB lookup performed nodes benchmark case single consistent regular heuristic The dynamic branching factor equal actual branching factor 13355 redundant operators removed consistent results Korf 24 As number possible heuristic lookups increases DBF decreases This results signiﬁcant reduction number generated nodes Note phenomena results First range heuristic values Rubiks cube small seen Fig 24 Thus potential large difference parents heuristic value childrens small Even domain inconsistency caused dramatic performance improvement Second extra overhead needed heuristics single PDB lookup performed node Thus reduction number generated nodes fully reﬂected running times 73 The 15puzzle Another source inconsistency data compression Section 62 Previous research compressed PDBs 15puzzle 11 771 additive partitioning shown Fig 25 These experiments repeated time BPMX The results averaged set 1000 random instances ﬁrst Korf Felner 26 reported Table 4 The ﬁrst line corresponds regular PDB 771 partitioning 536 MB memory PDBs represented sparse mapping 11 In second line PDB compressed roughly half size 268 MB Due resulting loss information number nodes generated increased 100 903 464 978 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1593 Fig 25 BPMX 771 partitioning disjoint sets 15puzzle Table 4 Results 771 partitioning 15puzzle Compress BPMX Nodes 464 978 565 881 526 681 Table 5 Results 24puzzle Row Lookups Heuristic One PDB lookup 1 2 3 4 5 Two PDB lookups 6 7 Three PDB lookups 8 9 Four PDB lookups 10 1 1 1 1 1 2 2 3 3 4 Regular Dual Dual BPMX Random Random BPMX Regular Regular 2 Randoms BPMX Regular Regular 3 Randoms BPMX Dual BPMX Time 0058 0069 0064 Av h 4359 4302 4302 Nodes 26 630 050 115 24 155 327 789 18 188 651 278 3 386 033 015 1 938 538 739 1 631 931 544 908 186 066 852 810 804 818 601 469 Memory 536 871 268 435 268 435 Time 15 095 20 105 10 761 3040 1529 1483 1065 1142 1022 1331 Regular Regular Dual Dual BPMX 751 181 974 565 881 agreeing previous results 11 Compressing PDBs produce inconsistency born BPMX results line decrease number nodes generated 526 681 At ﬁrst glance like modest reduction 10 However different way viewing BPMX reduced loss information introduced compression 40 100 903 61 703 This additional cost memory time 74 The 24puzzle We present results 24puzzle 6666 additive PDBs 26 Similar tendencies observed 15puzzle 78 additive PDBs 26 The results Table 5 averaged 10 instances smallest solution length standard 50 random states 26 Four heuristics available based 6666 additive PDBs 26 table dual lookup Fig 8 regular lookup regular lookup reﬂected main diagonal indicated refection dual lookup The random heuristic randomly chooses heuristic set heuristics A single dual random lookup outperforms regular lookup We showed Section 71 diminishing return adding lookups regular random case In 24puzzle adding lookups maximum beneﬁcial While smallest number nodes achieved lookups best time obtained inconsistent heuristic lookups 75 The pancake puzzle Table 6 shows results IDA optimally solving 10 random instances 17pancake puzzle PDB 7 pancakes There geometrical symmetric PDB lookups domain way achieve inconsistency dual lookup Rows 13 single PDB lookup The dual heuristic reduces number nodes generated factor 12 This improvement consequence larger diversity inconsistent heuristic values encountered 1594 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Table 6 17pancake results Row Lookup Nodes Normal operator order 1 2 3 4 Regular Dual Dual BPMX Regular Dual BPMX Operators ordered average heuristic difference 5 6 7 8 Regular Dual Dual BPMX Regular Dual BPMX 342 308 368 717 27 641 066 268 14 387 002 121 2 478 269 076 113 681 386 064 13 389 133 741 85 086 120 39 563 288 Time 284 054 19 556 12 485 3086 95 665 9572 74 49 DBF 1500 1500 1011 1045 1500 1500 418 593 Table 7 Average Heuristic Difference AHD operators 17pancake puzzle Operator 210 11 12 13 14 15 16 17 Regular 03700397 0396 0397 0400 0401 0402 0411 0216 Dual 0 0613 0958 1165 1291 1358 1376 1321 search When BPMX dual heuristic number nodes generated reduced factor result dynamic branching factor falling 15 1011 The best results row 4 achieved performing lookups regular dual BPMX propagate inconsistencies This combination produces 138fold reduction nodes generated regular lookup 751 Operator ordering increase BPMX cutoffs Consider following insight enhance performance domains If node child cause BPMX cutoff generated early set children possible This allow cutoff subtrees children searched If different operators tend create inconsistency different rates search sped ordering operators accordingly The operators Rubiks cube TopSpin symmetric diﬃcult ﬁnd useful way order This case pancake puzzle operator differs number pancakes moved We introduce new term average heuristic difference AHD The AHDoph given operator op heuristic h average states s op applied hs hops To estimate AHD operator random state chosen s1 relevant operator applied state yielding state s2 The difference heuristic value s1 s2 measured This repeated 100 million different states Table 7 shows AHD results operators 17pancake puzzle The Regular column presents AHD operator regular lookup performed Dual column presents AHD dual PDB lookup The regular PDB lookup consistent AHD greater 1 For dual PDB lookups results interesting Operators 210 AHD values exactly 0 artifact particular PDB experiments The PDB based locations 1117 moves affect locations operators 210 cause change dual heuristic 1445 However larger operators 1317 AHD dual lookup 1 Note operator 16 larger AHD operator 17 changes smaller number locations In Table 6 results rows 14 obtained operators order tokens moved For rows 58 operators ordered decreasing order AHD dual lookup measured Table 7 Even BPMX compare rows 5 6 rows 1 2 signiﬁcant improvements seen When BPMX AHD ordering roughly halves DBF dramatically reduces number generated nodes compare rows 7 8 rows 3 4 The best result regular dual lookup enhanced BPMX AHD orderingrow 8 reduces number generated nodes orders magnitude compared usual single regular lookup20 20 We use simple PDBs pancake puzzle demonstrate beneﬁts inconsistent heuristics However enhanced PDB methods 4117 domain speciﬁc gap heuristic 16 developed problem Applying techniques heuristic likely similar A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1595 8 Pathﬁnding experiments A Fig 26 Interleaved differential heuristics A different properties IDA setting need application preferred domain A pathﬁnding explicit state spaces In section demonstrate inconsistent heuristics incur signiﬁcant BPMX We demonstrate number cases inconsistent approaches outperform overhead A consistent approaches algorithm choice combinatorial puzzles A To properly assess inconsistent heuristics A suited Whereas IDA The application domain set 75 grid maps commercial games scaled grids size 512 512 Each location map blocked unblocked On map problems broken 128 buckets according optimal path length path lengths varying 1 512 We randomized 1280 problem instances map different startend locations The agent horizontally vertically diagonally possible directions All experiments section conducted 24 GHz Intel Core 2 Duo 4 GB RAM Most reported results BPMX1 Section 532 section term BPMX parameter refers BPMX1 All running times measured seconds 81 Pathﬁnding heuristics Octile distance common heuristic domain If distances x y coordinates 2 mindx dy dx dy This optimal distance points dx dy octile distance points 1 restrictions obstacles boundaries 2 allowed neighbors possible directions including diagonals The octile heuristic consistent require memory 811 True distance heuristics Truedistance heuristics TDHs memorybased heuristics recently developed pathﬁnding applica tions 4013 An example TDH differential heuristic 40 DH built follows choose K canonical states domain compute store shortest path distance K canonical states reachable states For canonical state S memory required S number states state space For ith canonical state ki admissible heuristic points b obtained hia b maxca ki cb ki octilea b cx y shortest path x y stored database Because ca b cb ki cid3 ca ki b ki follows ca b cid3 ca ki cb ki Hence ca ki cb ki admissible consistent heuristic distance b ca ki cb ki produce heuristic value higher octile heuristic For example happen b optimal path ki exact distance b larger octile distance However DH produce values smaller octile distance Taking maximum DH octile heuristic guarantees new heuristic dominates octile heuristic For given state takes maximum available differential heuristics resulting value consistent heuristic However random subset available heuristics considered resulting value inconsistent 812 Interleaved differential heuristics We introduce interleaved differential heuristic IDH convenient way beneﬁts multiple DHs multiple canonical states storage similar 5 Consider having ﬁve DHs 0 4 Instead storing distances ﬁve canonical states states store single distance state Consider Fig 26 grid Each cell labeled canonical state distance stored performance gains In fact advantage BPMX random heuristics application demonstrated recent PDBs 17 1596 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 state In setup S memory store portions ﬁve heuristics search beneﬁt follows A heuristic value states available distance canonical state stored state Thus current node expanded state D grid goal state G dotted border DH directly lookup heuristic value states use canonical state 3 However state A expanded differential heuristic A G directly computed different canonical states However neighboring G state F heuristic computed Thus h A G c A k0 cF k0 cF G canonical state k0 There lookups performed neighborhood current search node neighborhood goal More lookups improve heuristic estimate time In work lookups current state neighbors goal performed The ﬁnal heuristic maximum computed IDH heuristic octile heuristic Because different heuristics state overall heuristic inconsistent For eﬃciency improved performance use IDHs perform small breadthﬁrst search starting goal possible state possible DH lookup We cache distances associated errors Then given node search perform single lookup cache lookup heuristic value This cache approach eﬃcient identity neighbor state unimportant distance additional error needed Therefore cache size number interleaved heuristics number neighbors goal 82 Random heuristic The ﬁrst set experiments illustrates effect BPMX A inconsistent heuristic Three heuristics compared octile distance consistent baseline b DHs 10 canonical states built given state randomly chosen use inconsistent called random This BPMX c maximum DHs consistent best possible heuristic The memory needs 10 DHs random maximum 10S As expected A Fig 27 presents experimental results number nodes CPU time The problem instances partitioned buckets based solution length The xaxis presents different solution lengths point average solution length bucket The yaxis number node expansions logarithmic scale random max possible heuristics expands fewest number nodes A heuristic BPMX expands The random heuristic produce worse heuristic values octile heuristic However random BPMX performs order magnitude node expansions octile heuristic node reexpansions From slope lines appears random BPMX adds slight polynomial overhead A octile heuristic performance close consistent max heuristic This shows BPMX effective overcoming node expansion problem It impressive BPMX enhances random single lookup achieve nearly performance max lookups When BPMX added random performs better A Timing results ﬁgure similar trends Random BPMX faster lookup times correct maximum value needed Hence lazy evaluation max heuristic fewer heuristics consulted short paths better time performance Unlike IDA algorithm needs know cutoff occurs A possible A 821 Fixed number lookups Fig 28 presents comparison k heuristics These k lookups ﬁxed heuristic nodes consistent randomized random selection given node inconsistent 10 available heuristics BPMX inconsistent heuristics In experiment problems longest solutions map considered Each point represents average approximately 500 instances plotting time node expansions logarithmic scales The curve shows performance given k ﬁxed lookups curve shows performance k random lookups This experiment shows number lookups ﬁxed random strategy better ﬁxed strategy low values k diversity added resulting heuristic When k increases signiﬁcance effect decreases lookups implicitly adds diversity values ﬁxed lookups In domain consistent maxof10 heuristic achieved best time results The results explained number key differences maps combinatorial puzzles ap plication domains These differences cause diﬃculties achieve speedup search inconsistent heuristics pathﬁnding domains puzzle domains A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1597 Fig 27 Nodes expanded pathﬁnding 10S memory Fig 28 Comparing ﬁxed random lookups Memory Unlike permutation puzzles k lookups random ﬁxed need memory k ﬁxed lookups need kS memory k random lookups 10 need 10S memory k ﬁxed lookups uses memory better memory cache performance This explains k cid3 7 ﬁxed lookups lower search time random lookups Indexing time In puzzle domains cost determining ﬁnd heuristic lookup relatively expensive Typically nontrivial indexing function needed possibly application symmetries andor permutations In map domain indexing easier compute leading slightly faster lookup times Node reexpansion In IDA A maps domain problem node reexpansions cause problems BPMX puzzle domains inconsistent heuristic affect performance In Thus maximum 10 heuristic best choice pathﬁnding domain nodes time puzzles inconsistent heuristics fewer lookups yield faster times 24puzzle results Given new domain important factors determine inconsistent approaches successful In Section 83 interleaved heuristic allows multiple lookups improves performance differential heuristics inconsistency BPMX 1598 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 29 Time versus nodes tradeoff heuristics available 822 Varying number lookups performed This section examines performance random lookups available memory number heuristics increases 100S Three approaches performing lookups considered maximum available heuristics node b maximum 10 available heuristics random use BPMX c maximum 20 available heuristics random use BPMX For example 10 heuristics available 10S memory 1 2 lookups performed state 10 20 heuristics respectively Similarly 50 heuristics available 5 10 lookups performed 10 20 heuristics respectively Fig 29 shows curves approach Between 10 100 differential heuristics built increasing intervals 10 The nodes expanded time elapsed solve hardest problems length 508512 map computed compared approaches The points correspond having 10 50 100 differential heuristics available labeled curve This loglog plot making differences easier Consider consistent heuristic takes maximum available heuristics With 10 differential heuristics average 7 milliseconds needed complete search 1884 node expansions As differential heuristics number node expansions monotonically decreases However execution time decreases 30 differential heuristics point cost performing additional lookups overtakes reduction nodes expanded Randomly 10 available heuristics inconsistent This curve begins worst performance terms nodes time However 10 random heuristics 100 able match best time performance consistent heuristic 30 lookups faster consistent heuristic 40 lookups Randomly 20 available heuristics matches time performance 30 consistent lookups performing random lookups use 30S memory The fastest performance 60S memory available 12 lookups performed signiﬁcantly better best ﬁxed lookup result The error bars curve correspond 95 conﬁdence intervals showing result statistically signiﬁcant albeit small margin 83 Interleaved differential heuristics In section experimental results comparing number approaches use 1S memory reported The octile heuristic baseline results shown previously A single consistent lookup compared interleaved differential heuristic IDH deﬁned Section 812 built 10 differential heuristics 1S memory Fig 30 presents node expansions CPU time approaches The results plotted function solution length Unlike previous ﬁgures axes linear scales The nodes expanded timing results reinforce earlier discussion BPMX critical achieving good performance Again heuristic values worse octile heuristic performance interleaved inconsistent heuristic BPMX poor roughly factor 10 hardest problems Both versions interleaved heuristic BPMX outperform consistent DH heuristic BPMX1 better BPMX When 1S A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1599 Fig 30 Nodes expanded 1S memory memory available domain inconsistent heuristic outperforms consistent heuristic produces best results The curves timing results maintain orderings relative performance supporting BPMX node expansions d 1 count A node expansions Fig 31 shows results 80 DHs interleaved 10S memory This compared 10S memory 10 ﬁxed lookups The consistent heuristic 10S relatively informed slight reduction achieved inconsistent heuristic 84 Different degrees BPMX The ﬁnal experiment examines effect increasing BPMX propagation depth Fig 32 shows effect different BPMX propagation depths The set problems Fig 27 plotting number nodes expanded function solution length The heuristic random selection 10 available DHs Node expansions refers time neighbors node generated looked OPEN CLOSED As explained process exactly BPMXk k 1 regular A expansion Hence BPMX expansions counted A expansions Time results omitted trend For domain heuristic BPMX1 best Larger values k help average suﬃciently large heuristics step In sense fortunate BPMX1 easy implement produces best results 9 Discussion conclusions Historically inconsistent heuristics generally avoided searching optimal solutions belief inconsistent heuristics hard concoct This paper cost reexpanding closed nodes A demonstrated effective inconsistent heuristics easy create integrated IDA beneﬁts substantially reduce search effort This represents important change conventional wisdom heuristic search A behavior We showed A reexpands nodes heuristic consistent inconsistent heuristics hurt s worstcase exponential behavior valid unrealistic graph settings Furthermore IDA 1600 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 Fig 31 Average nodes expanded time 10S memory Fig 32 Average nodes expanded different degrees BPMX propagation generalized known pathmax propagation rules bidirectional pathmax BPMX showed BPMX lead performance gains Indeed experimental results showed major performance gains inconsistent heuristics IDA For puzzle domains large reduction order magnitude domains IDA cases number generated nodes CPU time obtained single inconsistent heuristic instead single regular consistent heuristic This consequence introducing diversity heuristic values encountered search A reduction number generated nodes obtained BPMX implemented inconsistent heuristic large heuristic value inﬂuence entire neighborhood states A In domains studied paper single heuristic available internal symmetries PDB puzzles manually creating heuristics pathﬁnding domain When multiple heuristics exist clearly taking maximum heuristics provides best heuristic value states generates fewest nodes This comes increase runtime overhead node cost additional lookups More heuristics considered increases diversity heuristic values reducing number node expansions There fore multiple heuristics available lookups performed performance advantage inconsistent consistent heuristics decreases The results presented paper vary number heuristics available experimental domains When A A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1601 Fig 33 9pancake states For TopSpin relative advantage inconsistent heuristics regular heuristics remains valid large range number multiple lookups performed In practice use 17 lookups perform equally variants random lookups For Rubiks cube lookups possible advantage inconsistent heuristics disappears For 24puzzle maximum number lookups Two random lookups shown outperform time maximum For pancake puzzle single lookup exists use inconsistent heuristics produced spectacular gains pathﬁnding domain problem node reexpansion arises This issue cause single random inconsistent heuristic generate nodes consistent octile heuristic inconsistent heuristic returns superior heuristic value states When BPMX added inconsistent heuristic random outper forms single consistent heuristic good maximum heuristics node expansions time However max 10 heuristics best choice It diﬃcult obtain speed multiple heuristics inconsistent manner pathﬁnding domain puzzle domains number reasons First number states pathﬁnding domain grows quadratic depth search puzzle domains grows exponentiallythere room improve ment Second symmetries possible pathﬁnding domain potential lookups need memory Third lookup time smaller PDB lookup performing multiple lookups costly Finally issue node reexpansions However despite complication domain use inconsistent heuris tic provides best results An inconsistent heuristic based interleaving number heuristics shown outperform consistent heuristic given memory The major result paper demonstration inconsistent heuristics increase diversity values encountered search leading improved performance Based results expectation use consistent heuristics accepted powerful tool development highperformance search algorithms A number directions remain future research Identifying ways creating inconsistent heuristics help usage common beneﬁcial As research needed different variations BPMX In partic ular different levels lookahead searches ﬁnding large heuristic values result better overall performance Acknowledgements This research supported Israel Science Foundation ISF grants number 72806 30509 Ariel Felner The research funding Albertas Informatics Circle Research Excellence iCORE Canadas Natural Sciences Engineering Research Council NSERC greatly appreciated Appendix A Example dual state heuristic In section provide example shows dual heuristic provide inconsistent values Consider 9pancake puzzle states shown Fig 33 State G goal state puzzle State S 1 neighbor G obtained reversing tokens locations 13 shown bold frame state S 2 obtained reversing tokens locations 16 States Gd Sd 2 dual states G S1 S2 respectively 1 Sd Note particular examples S1 Sd 1 identical In domain applying single operator twice row reach state state S1 single away goal It easy applying sequence 1602 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 operators reverse locations 13 reverse locations 16 Sd 2 produce goal state Observe states S1 2 duals neighbors Reversing consecutive k ﬁrst tokens state Sd S2 neighboring states Sd 1 arrive node Sd 2 differ 1 Using values S1 S2 inconsistent neighbors This shown following PDB example Suppose patterns 9pancake puzzle deﬁned considering tokens 46 ignoring rest tokens The resulting PDB provides distances goal pattern reachable patterns The right column Fig 33 shows corresponding pattern state obtained symbol represent dont care 2 Therefore consistent heuristic return values Sd 1 Sd 1 Sd Regular PDB lookups produce consistent heuristic values search 20 Indeed states S 1 S2 neighbors PDB heuristic values differ 1 In state S1 tokens 46 goal locations hS1 0 In state S2 tokens 46 goal locations need apply operator reach goal pattern hS2 1 Dual PDB lookups admissible necessarily consistent The dual PDB lookup state S 1 PDB lookup state Sd 1 However pattern projected state Sd 2 moves away goal pattern Thus performing dual lookup states S 1 S2 PDB lookups states Sd 2 produce heuristics inconsistent 0 2 When moving S 1 S2 vice versa g changed 1 h changed 2 1 returns 0 tokens 46 goal location state Sd 1 Sd References 1 A Bagchi A Mahanti Search algorithms different kinds heuristicsA comparative study Journal ACM 30 1 1983 121 2 M Ball RC Holte The compression power symbolic pattern databases International Conference Automated Planning Scheduling ICAPS 08 2008 pp 211 3 B Bonet H Geffner Learning depthﬁrst search A uniﬁed approach heuristic search deterministic nondeterministic settings appli cation MDPs International Conference Automated Planning Scheduling ICAPS06 2006 pp 142151 4 T Chen S Skiena Sorting ﬁxedlength reversals Discrete Applied Mathematics 71 13 1996 269295 5 JC Culberson J Schaeffer Pattern databases Computational Intelligence 14 3 1998 318334 6 R Dechter J Pearl Generalized bestﬁrst search strategies optimality A 7 C Domshlak E Karpas S Markovitch To max max Online learning speeding optimal planning AAAI Conference Artiﬁcial Journal ACM 32 3 1985 505536 Intelligence AAAI10 2010 pp 17011706 8 H Dweighter Problem e2569 American Mathematical Monthly 82 1975 1010 9 S Edelkamp Planning pattern databases European Conference Planning ECP01 2001 pp 1324 10 A Felner RE Korf S Hanan Additive pattern database heuristics Journal Artiﬁcial Intelligence Research 22 2004 279318 11 A Felner RE Korf R Meshulam RC Holte Compressed pattern databases Journal Artiﬁcial Intelligence Research 30 2007 213247 12 A Felner R Meshulam RC Holte RE Korf Compressing pattern databases National Conference Artiﬁcial Intelligence AAAI04 2004 pp 638 643 13 A Felner N Sturtevant J Schaeffer Abstractionbased heuristics true distance computations Symposium Abstraction Reformulation Approximation SARA09 2009 14 A Felner U Zahavi J Schaeffer RC Holte Dual lookups pattern databases International Joint Conference Artiﬁcial Intelligence IJCAI05 2005 pp 103108 15 PE Hart NJ Nilsson B Raphael A formal basis heuristic determination minimum cost paths IEEE Transactions Systems Science Cybernetics SCC4 2 1968 100107 16 M Helmert Landmark heuristics pancake problem Third Annual Symposium Combinatorial Search SOCS10 2010 pp 109110 17 Malte Helmert Gabriele Röger Relativeorder abstractions pancake problem ECAI 2010 pp 745750 18 RC Holte A Felner J Newton R Meshulam D Furcy Maximizing multiple pattern databases speeds heuristic search Artiﬁcial Intelli gence 170 2006 11231136 19 RC Holte J Newton A Felner R Meshulam D Furcy Multiple pattern databases International Conference Automated Planning Scheduling ICAPS04 2004 pp 122131 20 RC Holte MB Perez RM Zimmer AJ MacDonald Hierarchical A Intelligence AAAI96 1996 pp 530535 Searching abstraction hierarchies eﬃciently National Conference Artiﬁcial 21 A Junghanns J Schaeffer Domaindependent singleagent search enhancements International Joint Conference Artiﬁcial Intelligence IJCAI99 1999 pp 570575 22 RE Korf Depthﬁrst iterativedeepening An optimal admissible tree search Artiﬁcial Intelligence 27 1 1985 97109 23 RE Korf Realtime heuristic search Artiﬁcial Intelligence 42 3 1990 189211 24 RE Korf Finding optimal solutions Rubiks Cube pattern databases National Conference Artiﬁcial Intelligence AAAI97 1997 pp 700 705 25 RE Korf Recent progress design analysis admissible heuristic functions National Conference Artiﬁcial Intelligence AAAI00 2000 pp 11651170 26 RE Korf A Felner Disjoint pattern database heuristics Artiﬁcial Intelligence 134 12 2002 922 27 RE Korf A Felner Recent progress heuristic search A case study fourpeg Towers Hanoi problem International Joint Conference Artiﬁcial Intelligence IJCAI07 2007 pp 23242329 28 RE Korf M Reid Complexity analysis admissible heuristic search National Conference Artiﬁcial Intelligence AAAI98 1998 pp 305310 29 RE Korf M Reid S Edelkamp Time complexity iterativedeepeningA 30 RE Korf W Zhang I Thayer H Hohwald Frontier search Journal ACM 52 5 September 2005 715748 31 A Mahanti S Ghosh D Nau A Pal L Kanal On asymptotic performance IDA Annals Mathematics Artiﬁcial Intelligence 20 14 1997 Artiﬁcial Intelligence 129 12 2001 199218 161193 32 A Martelli On complexity admissible search algorithms Artiﬁcial Intelligence 8 1 1977 113 33 M McNaughton P Lu J Schaeffer D Szafron Memory eﬃcient A heuristics multiple sequence alignment National Conference Artiﬁcial Intelligence AAAI02 2002 pp 737743 34 L Mero A heuristic search algorithm modiﬁable estimate Artiﬁcial Intelligence 23 1984 1327 35 N Nilsson Artiﬁcial Intelligence A New Synthesis Morgan Kaufmann 1998 36 J Pearl Heuristics Intelligent Search Strategies Computer Problem Solving AddisonWesley 1984 A Felner et al Artiﬁcial Intelligence 175 2011 15701603 1603 37 D Ratner MK Warmuth Finding shortest solution N N extension 15puzzle intractable National Conference Artiﬁcial Intelligence AAAI86 1986 pp 168172 38 S Russell P Norvig Artiﬁcial Intelligence A Modern Approach Third edition PrenticeHall 2010 39 M Samadi M Siabani A Felner RC Holte Compressing pattern databases learning European Conference Artiﬁcial Intelligence ECAI08 2008 pp 495499 40 N Sturtevant A Felner M Barer J Schaeffer N Burch Memorybased heuristics explicit state spaces International Joint Conference Artiﬁcial Intelligence IJCAI09 2009 pp 609614 41 F Yang J Culberson RC Holte U Zahavi A Felner A general theory additive state space abstractions Journal Artiﬁcial Intelligence Research 32 2008 631662 42 U Zahavi A Felner N Burch RC Holte Predicting performance IDA AAAI08 2008 pp 381386 43 U Zahavi A Felner N Burch RC Holte Predicting performance IDA Research 37 2010 4183 conditional distributions AAAI Conference Artiﬁcial Intelligence BPMX conditional distributions Journal Artiﬁcial Intelligence 44 U Zahavi A Felner RC Holte J Schaeffer Dual search permutation state spaces National Conference Artiﬁcial Intelligence AAAI06 2006 pp 10761081 45 U Zahavi A Felner RC Holte J Schaeffer Duality permutation state spaces dual search algorithm Artiﬁcial Intelligence 172 45 2008 514540 46 U Zahavi A Felner J Schaeffer NR Sturtevant Inconsistent heuristics National Conference Artiﬁcial Intelligence AAAI07 2007 pp 1211 1216 47 Z Zhang N Sturtevant J Schaeffer RC Holte A Felner A gence IJCAI09 2009 pp 634639 search inconsistent heuristics International Joint Conference Artiﬁcial Intelli 48 R Zhou E Hansen Spaceeﬃcient memorybased heuristics National Conference Artiﬁcial Intelligence AAAI04 2004 pp 677682 49 R Zhou E Hansen Breadthﬁrst heuristic search Artiﬁcial Intelligence 170 45 2006 385408 50 R Zhou EA Hansen Memorybounded A graph search Florida Artiﬁcial Intelligence Research Society FLAIRS02 2002 pp 203209