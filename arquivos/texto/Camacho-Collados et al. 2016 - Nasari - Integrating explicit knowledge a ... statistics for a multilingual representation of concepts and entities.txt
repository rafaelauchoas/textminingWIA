Artiﬁcial Intelligence 240 2016 3664 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Nasari Integrating explicit knowledge corpus statistics multilingual representation concepts entities José CamachoCollados Department Computer Science Sapienza University Rome Italy b Language Technology Lab Department Theoretical Applied Linguistics University Cambridge United Kingdom Mohammad Taher Pilehvar b1 Roberto Navigli r t c l e n f o b s t r c t Article history Received 23 December 2015 Received revised form 14 July 2016 Accepted 25 July 2016 Available online 16 August 2016 Keywords Semantic representation Lexical semantics Word Sense Disambiguation Semantic similarity Sense clustering Domain labeling Owing need deep understanding linguistic items semantic representation considered fundamental components applications Natural Language Processing Artiﬁcial Intelligence As result semantic representation prominent research areas lexical semantics past decades However mainly lack large senseannotated corpora existing representation techniques limited lexical level effectively applied individual word senses In paper forward novel multilingual vector representation called Nasari enables accurate representation word senses different languages provides main advantages existing approaches 1 high coverage including concepts named entities 2 comparability languages linguistic levels words senses concepts thanks representation linguistic items single uniﬁed semantic space joint embedded space respectively Moreover representations ﬂexible applied multiple applications freely available http lcl uniroma1it nasari As evaluation benchmark opted different tasks word similarity sense clustering domain labeling Word Sense Disambiguation report stateoftheart performance standard datasets different languages 2016 Elsevier BV All rights reserved 1 Introduction Semantic representation modeling semantics linguistic item2 mathematical machine interpretable form fundamental problem Natural Language Processing NLP Artiﬁcial Intelligence AI Because represent lowest linguistic level word senses play vital role natural language understanding Effective representations word senses directly useful Word Sense Disambiguation 93 semantic similarity 13129106 coarsening sense inven tories 92124 alignment lexical resources 10198108 lexical substitution 75 semantic priming 100 Moreover senselevel representation directly extended applications requiring word representations added bene ﬁt provides extra semantic information Turney Pantel 129 provide review applications Corresponding author Email address colladosdiuniroma1it J CamachoCollados 1 Work mainly Sapienza University Rome 2 Throughout article linguistic item mean kind linguistic unit bear meaning word sense word phrase sentence larger piece text httpdxdoiorg101016jartint201607005 00043702 2016 Elsevier BV All rights reserved J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 37 word representation including automatic thesaurus generation 2122 word similarity 25128113 clustering 103 query expansion 140 information extraction 61 semantic role labeling 29104 spelling correction 53 Word Sense Disambiguation 93 The Vector Space Model VSM prominent approach semantic representation The model represents linguistic item vector point ndimensional semantic space mathematical space n dimen sions axes space denotes single linguistic entity word The popularity VSM representation main reasons Firstly straightforward view vectors sets features directly apply ma chine learning techniques Secondly model enjoys support ﬁeld Cognitive Science studies empirically theoretically suggested aspects human cognition accord VSMs 3664 However VSMbased techniques conventional cooccurrence based form 11912963 newer predictive branch 20818 usually base computation distributional statistics derived text corpora Hence order able represent individual meanings words word senses techniques require large amounts disambiguated text prior modeling Additionally Word Sense Induction techniques 103115827 require senseannotated data induced sense clusters mapped existing sense inventory However providing senseannotated data large scale timeconsuming process carried separately word sense repeated new language socalled knowledge acquisition bottleneck Importantly largest manual effort providing widecoverage senseannotated dataset dates 1993 case SemCor corpus 85 In fact cheap fast annotations obtained means Amazon Mechanical Turk 123 55 games purpose 13313156 voluntary collaborative editing Wikipedia 77 producing annotated resources manually onerous task On hand performance Word Sense Disambiguation WSD techniques far ideal 93 turn prevents reliable automatic senseannotation large text corpora modeling individual word senses This hinders functionality group vector space models tasks WSD require representation individual word senses There efforts adapt apply distributional approaches representation word senses 103 121144768 However techniques provide representations linked standard sense inventory consequently mapping carried manually help senseannotated data 48 Recently attempts address issue obtain vectors individual word senses exploiting WordNet semantic network 74106108116 glosses 19 These approaches restricted representation concepts deﬁned WordNet English language designed speciﬁc tasks In recent work 16 proposed method exploits structural knowledge derived semantic networks distributional statistics text corpora produce effective representations individual word senses concepts Our approach provides main advantages comparison previous VSM techniques Firstly multilingual directly applied representation concepts dozens languages Secondly vector represents concept irrespective language uniﬁed semantic space having concepts dimensions permitting direct comparison different representations languages enabling crosslingual applications In article improve approach referred Nasari Novel Approach SemanticallyAware Representation Items henceforth extend application wider range tasks lexical semantics Speciﬁcally novel contributions follow 1 We propose new formulation fast computation lexical speciﬁcity Section 311 2 We propose new ﬂexible way continuous embedded vector representations added beneﬁt obtaining semantic space shared BabelNet synsets words texts Section 33 3 We forward technique improved computation weights uniﬁed vectors improve accuracy eﬃciency representations Section 34 4 We compute assign weights individual edges semantic network Section 41 means different experiments advantage gain new weighted graph Section 10 5 We release lexical uniﬁed vector representations ﬁve different languages English French German Italian Spanish embedded vector representations English language http lcl uniroma1it nasari In addition contributions devised robust frameworks enable direct application representa tions different tasks Semantic Similarity Section 6 Sense Clustering Section 7 Domain Labeling Section 8 Word Sense Disambiguation Section 9 For tasks carried comprehensive set evaluations datasets order verify reliability ﬂexibility Nasari different datasets tasks We provide summary experiments Section 5 The rest article structured follows We ﬁrst provide introduction widely knowl edge resources lexical semantics Section 2 After Section 3 methodology convert text lexical embedded uniﬁed vectors The process obtain vector representations synset vectors leveraging knowledge resources described Section 2 methodology obtain vectors text described Section 3 presented Section 4 We present summary experiments performance Nasari tasks Section 5 Then applications vectors respective frameworks experiments Sections 6 Seman tic Similarity 7 Sense Clustering 8 Domain Labeling 9 Word Sense Disambiguation We analyze performance 38 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 different components model Section 10 Finally discuss related work Section 11 provide concluding remarks Section 12 2 Knowledge resources Knowledge resources divided general categories expert collaboratively constructed Each type advantages limits Manuallyannotated resources feature highlyaccurate encoding concepts semantic relationships exceptions usually limited lexical coverage typically focused speciﬁc language A good example WORDNET 83 semantic network basic units synsets A synset represents concept expressed nouns verbs adjectives adverbs composed different lexicalizations synonyms express For example synset middle day concept comprises lexicalizations noon noon high noon midday noonday noontide Synsets seen nodes semantic network These nodes connected means lexical semantic relations hypernymy meronymy These relations seen edges WordNet semantic network Despite largest complete manuallymade lexical resources WordNet lacks coverage lemmas senses domain speciﬁc lexicons law medicine named entities creative slang usages technology came existence recently On hand collaborativelyconstructed resources WIKIPEDIA provide features multilinguality wide coverage uptodateness As September 2015 Wikipedia provides 100K articles ﬁfty lan guages This coverage steadily increasing For instance English Wikipedia receives 750 new articles day Each articles provides corresponding concept great deal information form textual informa tion tables infoboxes relations redirections disambiguations categories These features persuaded researchers past years exploit huge amounts semistructured knowledge available collaborative resources different NLP applications 46125 The types knowledge available expertbased collaborativelyconstructed resources complemen tary This motivated researchers combine lexical resources categories 101108 A prominent example BABELNET 98 provides mapping WordNet number collaborativelyconstructed resources including Wikipedia The structure BabelNet3 similar WordNet Synsets main linguistic units connected semantically related synsets lexicalizations multilingual case For instance synset corresponding United States represented set multilingual lexicalizations including United_StatesEN United_States_of_AmericaEN AmericaEN USEN USAEN English Estados_UnidosES Estados_Unidos_de_AméricaES EEUUES EEUUES EE UUES Spanish Stati_Uniti_dAmericaIT Stati_UnitiIT AmericaIT USAIT Italian The relations tween synsets ones coming WordNet hypernyms hyponyms plus new relations coming resources Wikipedia hyperlinks WikiData4 relations Madrid capital Spain BabelNet largest mul tilingual semantic network available containing 13789332 synsets 6418418 concepts 7370914 named entities 354538633 relations 271 languages5 For English language BabelNet contains 4403148 synsets Wikipedia page associated 117653 synsets WordNet synset associated 99705 synsets com posed Wikipedia page WordNet synset The gist approach lies combination different types knowledge complementary resources Specif ically representation approach utilizes following sources knowledge lexicosemantic relations WordNet BabelNets mapping WordNet synsets Wikipedia articles texts Wikipedia articles interarticle links Wikipedia In experiments WordNet 30 covers 117K unique nouns 80K synsets Wikipedia dump December 2014 BabelNet 30 covers 271 languages contains 13 million synsets 3 Representing texts vectors One contributions article framework proposing transforming texts different kinds vector lexical embedded uniﬁed Our lexical vectors follow conventional approach representing linguistic item semantic space words dimensions 103 multiword expressions considered The weights vectors usually computed basis raw term frequencies tf normalized frequencies tfidf 52 Instead use lexical speciﬁcity computation weights lexical vectors Having solid statistical basis lexical speciﬁcity provides advantages previously mentioned measures 17 Section 10 comparison lexical speciﬁcity tfidf In follows section ﬁrst explain lexical speciﬁcity propose eﬃcient way fast computation Section 31 We provide details types vector lexical Section 32 embedded Section 33 uniﬁed Section 34 3 http babelnet org 4 https wwwwikidata org 5 The statistics taken BabelNet 30 release version experiments More statistics http babelnet org stats J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 39 31 Lexical speciﬁcity Lexical speciﬁcity 62 statistical measure based hypergeometric distribution6 The measure widely different NLP applications including term extraction 28 textual data analysis 66 domainbased term disam biguation 1410 rarely measure weights vector space model Lexical speciﬁcity essentially computes set representative words given text based hypergeometric distribution In setting interested representing given text referred subcorpus SC vector comprising weighted set relevant words concepts In order compute lexical speciﬁcity need reference corpus RC superset SC Lexical speciﬁcity computes weights word contrasting frequencies word SC RC Following notation 16 let T t respective total number content words RC SC F f denote frequency given word w RC SC respectively Our goal compute weight quantifying association strength w text SC We compute probability word w having frequency equal subcorpus SC hypergeometric distribution takes parameters frequency w higher f reference corpus RC F sizes RC SC T t respectively A word w high probability high occurrence chance arbitrary subsets RC size t Hence representative words given subcorpus low probabilities speciﬁc words suitable ones distinguishing subcorpus reference corpus As result computed probability inversely proportional relevance word w SC In order relation directly proportional making weights interpretable apply log10 operation computed probabilities customary literature 2842 This logarithmic operation speeds calculations details following section Moreover log10 instead instance natural logarithm added beneﬁt leading easy calculation prior probability For example 5 000005 item lexical speciﬁcity 50 means probability observing item SC 10 Therefore lexical speciﬁcity w SC given following expression specT t F f log10 P X f 1 X represents random variable following hypergeometric distribution parameters F t T P X f deﬁned follows P X f Fcid2 f P X 2 P X represents probability given word appear exactly times subcorpus SC according hypergeometric distribution parameters F t T We propose eﬃcient implementation Equation 2 following section 311 Eﬃcient implementation lexical speciﬁcity According Equation 2 computation hypergeometric distribution involves summing F f 1 addends calculated follows7 cid3 F cid4cid3 cid4 P X cid3 T F ti cid4 T t F T F tT t T iF iT F t 3 Given summation range Equation 2 generally directly proportional size corpus com putation lexical speciﬁcity expensive large corpora value F tends high Lafon 62 proposed method reduce computation cost Equation 2 According method ﬁrst calculate P X smallest f calculate rest probabilities P X f 1 P X F following property hypergeometric distribution P X 1 P X iF 1T F t 1 4 Lafon 62 suggested wellknown Stirling formula computation factorial components Equation 3 According Stirling formula logarithm factorial approximated follows log n n log n n 1 2 log 2πn 5 6 The hypergeometric distribution discrete probability distribution describes probability k successes n draws replacement ﬁnite population size N contains exactly K successes draw success failure In statistics hypergeometric test uses hypergeometric distribution calculate statistical signiﬁcance having drawn speciﬁc k successes n total draws aforementioned population https en wikipedia org wiki Hypergeometric _distribution 7 In cases t occur F t probability P X equal 0 40 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Fig 1 Hypergeometric distribution word mathematics arbitrary subcorpus SC size 100000 Wikipedia Thanks application Stirling formula transform Equation 3 summation Despite im provements calculation lexical speciﬁcity remain issues computation applied large reference corpus One main problems multiplication potentially small quantities Speciﬁcally 64bit binary ﬂoatingpoint number typically current computers approximate range 308 During computation lexical speciﬁcity large corpora lower bound reached 10 times Our solution solve problem optimizes calculations obtained equations Firstly rewrite Equation 4 extracting common factor P X f 308 10 P X f Fcid2 f P X P X f Fcid2 f ai 6 f 1 ai ai1 F iti i1T F ti1 f 1 F Now need apply logarithm sides equation order transform previous multiplication addition avoid small values In way avoid unnecessary exponentials calculations P X f log10 P X f log10 P X f log10 cid5 Fcid2 cid6 ai f 7 Therefore according Equation 1 applying change logarithm base compute lexical speciﬁcity given parameters T t F f follows specT t F f k loge P X f log10 cid5 Fcid2 cid6 ai f 8 k natural logarithm 10 loge 10 cid7 For computational feasibility F f ai sum usually computed F Instead stopping criterion introduced loop Since probability mass tail hypergeometric distribution cases mathematically insigniﬁcant respect ﬁnal cumulative probability distribution stopping criterion usually satisﬁed reaching ﬁnal F value considerably reduces computation time As example Fig 1 estimated probability distribution word mathematics arbitrary sub corpus SC 100000 content words Wikipedia If word mathematics occurs times SC word considered speciﬁc given subcorpus Fig 1 probability mass hypergeometric distribution concentrated left distribution range The distribution range extends 70029 number occurrences word mathematics Wikipedia However 20 rapidly gets smaller This illustrates point probability P X 45 small 10 right tail probability mass generally insigniﬁcant values close expected value adding stopping condition calculations faster having noticeable effect ﬁnal speciﬁcity score The sections provide details types vector leverage lexical speciﬁcity construction J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 41 32 Lexical vector representation So far explained lexical speciﬁcity determine relevance words given text In section explain leverage lexical speciﬁcity order construct lexical vector given text SC Throughout article texts considered come Wikipedia use Wikipedia reference corpus RC Our lexical vectors individual words dimensions lexical semantic space text represented basis association set lexical items words By contrasting term frequencies SC RC compute lexical speciﬁcity term given subcorpus Speciﬁcally order compute lexical vector cid5vlexSC simply iterate content words sub corpus SC words total frequency greater equal ﬁve Wikipedia considered compute lexical speciﬁcity We prune resulting vectors keeping words rele vant target text conﬁdence 99 according hypergeometric distribution P X f 001 performed earlier works 1017 Words weights aforementioned threshold considered zero di mensions The vector truncation step helps reduce noise Additionally truncation helps speeding computation vectors sparse computationally easier work In setting consider multiword expressions appear lexicalizations piped links8 Note apply lexical speciﬁcity content words nouns verbs adjectives tokenization lemmatization notational simplicity term word refer 33 Embedded vector representation In recent years semantic representation experienced resurgence use neural networkbased learning trend usually referred word embeddings In addition fast processing massive amounts text word embeddings proved reliable techniques modeling semantics words basis contexts However application wordbased techniques representation word senses trivial bound availability large amounts senseannotated data There efforts aimed learning sensespeciﬁc em beddings needing resort senseannotated data clustering contexts word appears 1384799 However resulting representations usually aligned existing sense inventories We forward approach allows plug arbitrary word embedding representation lexical vector representations providing main advantages 1 beneﬁting wordbased knowledge derived result learning massive corpora senselevel representation 2 reducing dimensionality lexical space ﬁxedsize continuous space 3 providing shared semantic space words synsets details Section 4 enabling direct comparison words synsets Our approach exploits compositionality word embeddings According property compositional phrase rep resentation obtained combining usually averaging constituents representations 82 For instance vector representation obtained averaging vectors words Vietnam capital close vector representation word Hanoi semantic space word embeddings Our approach builds property plugs trained word embeddingbased representation lexical vectors Speciﬁcally given input text T space word embeddings E ﬁrst calculate lexical vector T cid5vlexT explained Section 32 map lexical vector semantic space E follows cid7 ET wcid5vlexT cid7 cid3 cid4 rankwcid5vlexT Ew 1 wcid5vlexT 1 rankwcid5vlexT 9 Ew embeddingbased representation word w E rankw cid5vlexT rank dimension corresponding word w lexical vector cid5vlexT giving importance higher weighted dimensions In Section 10 compare harmonic average giving importance higher weighted words simple average One main advantages representation combination technique ﬂexibility word embedding space given input As experiments Sections 61 71 combination enables beneﬁt wordspeciﬁc knowledge improve integrating sensespeciﬁc representations 34 Uniﬁed vector representation We propose representation uniﬁed contrast lexical vector representation potentially ambiguous words individual dimensions BabelNet synsets individual dimensions Algorithm 1 8 A piped link hyperlink Wikipedia article redirects user Wikipedia page For example piped link dockside_craneCrane_machine hyperlink appears dockside_crane text links Wikipedia page titled Crane_machine The Wikipedia article represented suitable lexicalization preserves grammatical syntactic structure contextual coherency ﬂow sentence 42 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 hypernym h l BabelNet l1 l2 SC l1 l2 hyponyms h l1 cid12 l2 Algorithm 1 Uniﬁed vector construction Input A reference corpus RC subcorpus SC Output uniﬁed vector cid5us cid5ush dimension corresponding synset h 1 T sizeRC 2 t sizeSC 3 H 4 lemma l SC 5 H H h 6 7 cid5u vector 8 h H 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 return cid5u F 0 f 0 hyper pass F alse lexicalization lex h F F freqlex RC f f freqlex SC spech speciﬁcityT t freqlex RC freqlex SC spech specthres hyper pass T rue F F freqlex RC f f freqlex SC cid5uh speciﬁcityT t F f lexicalization lex hypo hyponym hypo h hyper pass shows construction process uniﬁed vector given subcorpus SC The algorithm ﬁrst clusters words SC sense sharing hypernym h algorithm according WordNet taxonomy integrated BabelNet lines 46 On hyponym clusters impose restriction lexicalization hypernym standard lexical speciﬁcity threshold 2 lines 1618 The reason include uniﬁed representation reduce noise detected applying old uniﬁed algorithm 16 Finally cluster passes threshold speciﬁcity computed set hyponyms h occur subcorpus SC lines 2024 As Section 31 F f denote frequencies reference corpus RC Wikipedia subcorpus SC respectively In case frequencies correspond aggregation frequencies h hyponyms Our clustering sibling words single cluster represented common hypernym transforms lexical space uniﬁed semantic space This space multilingual synsets dimensions enabling direct comparability languages We evaluated feature uniﬁed vectors task crosslingual word similarity Section 613 The clustering viewed implicit disambiguation potentially ambiguous words disambiguated intended sense represented hypernym resulting accurate semantic representation 35 Vector comparison As vector comparison method lexical uniﬁed vectors use squarerooted Absolute Weighted Over lap 1716 based Weighted Overlap measure 106 For notational brevity refer squarerooted Absolute Weighted Overlap Weighted Overlap WO WO compares vectors basis overlapping dimen sions harmonically weighted absolute rankings For measure vectors viewed semantic sets ranked lists 135 weights sort elements vector actual values calculation Formally Weighted Overlap vectors cid5v 1 cid5v 2 deﬁned follows cid8 cid9 cid9 cid10 W O cid5v 1 cid5v 2 cid7 cid3 dO rankd cid5v 1 rankd cid5v 2 i12i1 cid7O cid41 10 O set overlapping dimensions concepts words vectors rankd cid5v rank dimension d vector cid5v Absolute WO differs original WO takes account relative ranks dimensions respect overlapping dimensions instead considering dimensions vector Owing use absolute ranks measure gives lower scores comparison original WO This reason use squareroot operator smooths distribution values 01 scale This metric shown suit speciﬁcitybased vectors conventional cosine distance 17 In contrast use cosine comparing embedded vector representations The dimensions embedded repre sentations interpretable dimension values represent weights rankbased WO applicable setting Cosine usual measure literature measure similarity embedding space 811968 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 43 Fig 2 Our procedure getting contextual information sample BabelNet synset represented main sense reptile1 n 4 From synset vector representations In Section 3 proposed vector representations arbitrary text subcorpus SC belonging larger collec tion We leverage representations obtain semantic vector representation concepts named entities As knowledge base use BabelNet9 multilingual encyclopedic dictionary merges WordNet lexical encyclopedic resources Wikipedia Wiktionary thanks use automatic mapping algorithm 9798 We chose BabelNet large coverage named entities concepts hundreds languages Moreover concepts named entities organized fullﬂedged taxonomy integrates WordNet taxonomy experiments latest versions Wikipedia Bitaxonomy 34 WikiData isa relations coming open information extraction techniques 26 Our approach makes use power BabelNet exploits complementary information distributional statistics Wikipedia articles tied taxonomi cal relations BabelNet In experiments version 30 BabelNet released December 2014 covers 65M concepts 7M named entities 271 different languages The rest section divided parts We ﬁrst collect contextual information given synset Section 41 explain contextual information processed order obtain vector representations Section 42 41 Getting contextual information given synset The goal ﬁrst step create subcorpus SCs given BabelNet synset s Let Ws set containing Wikipedia page corresponding concept s wps henceforth related Wikipedia pages outgoing link page Note stage Ws Wikipedia page corresponding BabelNet synset s We enrich Ws adding corresponding Wikipedia pages hypernyms hyponyms s taxonomy BabelNet Fig 2 illustrates procedure obtaining contextual information Let SC s set content words occurring Wikipedia pages Ws tokenization lemmatization The frequency content word w SC s calculated follows ncid2 f w λi f iw i1 11 n number Wikipedia pages Ws f iw frequency w Wikipedia page pi Ws 1 n λi weight assigned page pi denote importance In following subsection explain calculate weight λi given page pi 411 Weighting semantic relations In section explain weight BabelNet semantic relations λi Equation 11 target synset s ith page Ws In previous versions Nasari 1716 making assumption Wikipedia pages Ws equally important λi 1 n In article set meaningful weights pages basis source type semantic connection target synset s A Wikipedia page Ws come different sources Section 41 1 Wikipedia page corresponding s wps 2 related Wikipedia pages outgoing link page wps 3 Wikipedia pages connected s taxonomic relations BabelNet We compute assign weight 0 1 range pages type follows 9 See Section 2 information BabelNet 44 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 1 Topweighted dimensions lexical vectors ﬁnancial geographical senses bank Bank ﬁnancial institution Bank geography English bank banking deposit credit money loan commercial_bank central_bank French banque bancaire crédit ﬁnancier postal client dépôt billet Spanish banco bancario banca ﬁnanciero préstamo entidad déposito crédito English river stream bank riparian creek ﬂow water watershed French eau castor berge canal barrage zone perchlorate humide Spanish banco limnología ecología barrera estuarios isla interés laguna Table 2 Topweighted dimensions uniﬁed vectors ﬁnancial geographical senses bank We represent synset word senses Word senses marked symbol languages correspond BabelNet synset Bank ﬁnancial institution Bank geography English bank2 n reserve2 n cid4ﬁnancial_institution1 n cid13deposit8 n banking2 n ﬁnance1 n French banque1 n fonds2 n cid13dépôt9 n emprunt2 n paiement1 n argent2 n Spanish English French Spanish banco1 n cid4Institución_ﬁnanciera1 n cid13depósito15 n Finanzas1 n dinero2 n préstamo2 n cid4stream1 n river1 n body_of_water1 n ﬂow1 n course2 n bank1 n eau1 n eau15 n excrément1 n castor1 n étendue_deau1 n fourrure1 n inclinación9 n lago1 n cuerpo_de_agua1 n cid4arroyo1 n tierra11 n costa1 n 1 The Wikipedia page corresponding BabelNet synset s wps assigned highest possible weight 1 2 The weights related Wikipedia pages outgoing link wps computed follows We ﬁrst com pute lexical vectors Wikipedia pages wps We apply Weighted Overlap Section 35 calculate similarity lexical vectors pages wps These similarity scores denote weight related Wikipedia page In order reduce high number ingoing links cases improve quality links prune ingoing links include 100 links basis similarity scores similarity score higher 025 3 Given possibility particular synset Wikipedia page associated Wikipedia pages coming taxonomic relations calculated previous case In case Wikipedia pages coming taxonomic relations given ﬁxed score 085 calculated follows We picked set 100 random taxonomic relations calculated average similarity score 100 pairs previous Nasari 42 Transforming contextual information vector representations Once gathered corpus SC s given BabelNet synset s computed associated frequencies f w word w SC s proceed calculate lexical embedded uniﬁed vectors s explained Sections 32 33 34 respectively In experiments Wikipedia corpus reference corpus RC Wikipedia dump December 201410 We computed Nasari lexical uniﬁed vectors English German French Italian Spanish The number synset vectors languages respectively 442M 151M 148M 110M 107M On average English language contextual information synset composed subcorpus SC s 1561 words total coming 17 Wikipedia pages For embedded vectors took word embeddings pretrained word phrase vectors Word2Vec11 These vectors trained 100billion English corpus Google News 300 dimensions Lexical uniﬁed synset vectors example We Tables 1 2 respectively topweighted dimensions lexical uniﬁed vector representations ﬁnancial geographical senses noun bank different languages English French Spanish As seen senses bank clearly identiﬁed distinguished according dimensions vectors irrespective language type Additionally note uniﬁed vectors comparable languages We mark Table 2 different languages word senses12 correspond BabelNet synset It seen Table uniﬁed vectors different languages share elements 10 Each language uses Wikipedia corpus respective language reference corpus 11 The pretrained Word2Vec word embeddings downloaded https code google com p word2vec 12 We use sense notation 93 word n nth sense word speech p p J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 45 Table 3 Closest embedded vectors BabelNet synsets corresponding ﬁnancial geographical senses bank word bank Bank ﬁnancial institution Bank geography bank Closest senses Cosine Closest senses Cosine Closest senses Cosine Deposit account Universal bank British banking German banking Commercial bank Banking Israel Financial institution Community bank 099 099 098 098 098 098 098 097 Stream bed Current stream River engineering Braided river Fluvial terrace Bar river morphology River Perennial stream 098 097 097 097 097 097 097 096 Bank ﬁnancial institution Universal bank British banking German banking Branch banking McFadden Act Four Northern Banks State bank 086 086 086 085 085 085 084 084 Word synset embeddings example The dimensions interpretable embedded vectors Therefore better way distinguish different senses closest elements space cosine vector similarity measure Table 3 shows closest senses word bank closest speciﬁc senses word ﬁnancial geographical senses recall embedded vector representation words synsets share space In case senses bank clearly distinguished closest BabelNet synsets space Looking closest senses word bank ﬁnancial meaning bank lower cosine values This shows predominant sense word bank Google News corpus word embeddings trained clearly ﬁnancial sense We note embedded vector representation easily compute predominance senses word directly comparing representation word individual senses Our shared space provides suitable framework studying ambiguity words 5 Summary experiments In order assess reliability ﬂexibility technique different datasets tasks carried comprehensive set evaluations Speciﬁcally considered different tasks Semantic Similarity Section 6 Sense Clustering Section 7 Domain Labeling Section 8 Word Sense Disambiguation Section 9 A brief overview evaluation benchmarks results tasks follows 1 Semantic similarity Nasari proved highly reliable task semantic similarity measurement provides stateoftheart performance datasets different evaluation benchmarks Monolingual word similarity standard word similarity datasets MC30 84 WSSim 33 SimLex999 43 RG65 117 In addition English word similarity datasets assessed multilinguality approach RG65 dataset languages Crosslingual word similarity different crosslingual datasets basis RG65 15 In addition word similarity benchmarks assessed capability approach provide comparable semantic representations different types linguistic items Speciﬁcally opted SemEval2014 task CrossLevel Semantic Similarity 57 Despite tuned task approach achieved near stateof theart performance word sense similarity measurement dataset 2 Sense clustering We constructed highly competitive unsupervised basis Nasari representations outperforming stateoftheart supervised systems manuallyannotated Wikipedia sense clustering datasets 23 3 Domain labeling We annotating synsets large lexical semantic resource BabelNet bench marked automatic baselines gold standard datasets dataset domainlabeled WordNet synsets coming WordNet 30 new manuallyconstructed dataset domainlabeled BabelNet synsets Nasari outperformed automatic baselines demonstrating approach reliable ﬂexible different tasks 4 Word Sense Disambiguation We proposed simple framework knowledgerich unsupervised disambiguation Our obtained stateoftheart results multilingual AllWords Word Sense Disambiguation Wikipedia sense inventory evaluated SemEval2013 dataset 95 English AllWords Word Sense Dis ambiguation WordNet sense inventory evaluated SemEval2007 111 SemEval2013 95 datasets Additionally performed experiment measure reliability semantic representations named entities obtaining best results unsupervised systems near stateoftheart performance SemEval2015 WSD dataset 88 6 Semantic similarity Semantic similarity popular benchmark evaluation different semantic representation techniques The task measure semantic closeness linguistic items The similarity items directly com puted comparing corresponding vector representations As mentioned Section 35 opted Weighted 46 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Overlap vector comparison method lexical uniﬁed representations cosine embedded repre sentations Note approach obtain representations individual BabelNet synsets Moreover BabelNet merges different resources representations calculate semantic similarity semantic units different resources instance Wikipedia pages WordNet synsets Wikipedia page WordNet synset 61 Evaluation We benchmark semantic similarity procedure word similarity task Word similarity speciﬁc task se mantic similarity measure semantically close words In order able compute similarity words ﬁrst need map words corresponding synsets However mapping straight forward process thanks multilingual sense inventory BabelNet As frequently task measure similarity words w w similarity closest senses 1151310617 cid15 simw w cid15 max cid5v1Lw cid5v2L cid15 w VC cid5v 1 cid5v 2 12 Lw represents set synsets contain w lexicalizations As vector comparison VC use WO Section 35 compare lexical uniﬁed representations cosine embedded representations Note thanks uniﬁed representation w w belong different languages Throughout section tasks based semantic similarity NASARIlexical NASARIuniﬁed represent systems based lexical uniﬁed vectors respectively We refer combination lexical uniﬁed vectors NASARI This combination based average similarity scores given lexical uniﬁed vectors sense pair We report results NASARIembed vector representations use pretrained Word2Vec vectors input We performed experiments monolingual word similarity English languages presented Sections 611 612 respectively crosslingual similarity presented Section 613 Additionally evaluate embedded representations crosslevel semantic similarity task Section 614 cid15 611 Monolingual word similarity English Datasets The majority benchmarks word similarity available English language We compare approach stateoftheart word similarity systems standard English word similarity datasets We chose standard MC30 84 WordSim353 33 SimLex999 43 evaluation benchmarks MC30 consists subset RG65 117 reannotated following new similarity guidelines WordSim353 consists 353 word pairs includ ing concepts named entities In original WordSim353 similarity conﬂated relatedness dataset In order avoid conﬂation 1 cleverly divided dataset subsets ﬁrst concerned relatedness second subset focused similarity experiments We refer sim ilarity subset 203 word pairs WSSim henceforth Finally took noun pairs SimLex999 dataset evaluation benchmark The complete SimLex999 dataset composed 999 word pairs 666 noun pairs Comparison systems We selected stateoftheart approaches available online comparison systems These systems split categories knowledgebased corpusbased As knowledgebased selected ap proaches based WordNet semantic graph 106 ADW13 69 Lin14 Another knowledgebased approach 35 ESA15 represents word semantic space Wikipedia articles We compared systems corpusbased approaches16 Firstly took pretrained word embeddings Word2Vec 8117 Nasariembed Section 42 Then took best predictive countbased models semantic similarity released 818 The best predictive model based Word2Vec BestWord2Vec henceforth best count based models PMISVD traditional cooccurrence vectors based Pointwise Mutual Information PMI combined Singular Value Decomposition SVD dimensionality reduction Finally benchmarked embeddingbased sense representation approaches The ﬁrst approach Chen henceforth 19 leverages word embeddings WordNet glosses WSD creating sense embeddings19 The second called SensEmbed 48 uses Babel 13 ADW implementation available https github com pilehvar ADW 14 Results Lin obtained WS4J implementation available https code google com p ws4j 15 ESA implementation available DKProSimilarity package 7 16 All corpusbased approaches mentioned paper use cosine comparison measure 17 The pretrained models available https code google com p word2vec They trained Google News corpus 100 billion words 18 Both models trained 28 billiontoken corpus including English Wikipedia They available cliccimecunitnitcomposessemantic vectorshtml 19 The sense representations downloaded http pan baidu com s 1eQcPK8i J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 47 Table 4 Pearson r Spearman ρ correlations different similarity measures human judgements RG65 MC30 WSSim SimLex999 noun instances datasets We best performance obtained 8 48 conﬁgurations different datasets including WSSim RG65 high lighted We SensEmbed conﬁguration tuned SimLex999 dataset highlighted The interannotator agreement WordSim353 highlighted cid13 reported 061 interannotator agreement reported WSSim subset MC30 WSSim SimLex999 nouns Average Nasari Nasarilexical Nasariuniﬁed Nasariembed ESA Lin ADW Chen Word2Vec BestWord2Vec BestPMISVD SensEmbed IAA r 089 088 088 091 059 076 079 082 080 083 076 089 ρ 078 081 078 083 065 072 083 082 080 083 071 088 r 074 074 072 068 045 066 063 063 076 076 068 065 ρ 072 073 070 068 053 062 067 064 077 078 066 075 cid13 061 r 050 051 049 048 016 058 044 048 046 048 040 046 ρ 049 049 048 046 023 058 045 044 045 049 040 047 061 r 071 071 070 069 040 067 062 064 067 069 061 067 ρ 067 068 065 066 047 064 065 063 067 070 059 070 Net main knowledge source relies predisambiguated text WSD We report results methods closest senses strategy systems Results Table 4 shows Pearson Spearman correlation performance systems comparison systems considered datasets20 Both lexical uniﬁed vectors especially lexical ones prove robust datasets The combination lexical uniﬁed vectors noticeable improvement lexical vectors singlehanded Our gets highest average Pearson correlation systems outperforming embeddingbased approaches use dataset SensEmbed datasets BestWord2Vec order tune hyperparameters21 In terms Spearman correlation based lexical vectors achieves highest average performance systems use datasets tuning single point advantage Word2Vec Nasariembed proves competitive outperforming Word2Vec approaches terms Pearson correlation obtaining best overall result MC30 Lin perform particularly MC30 WSSim surprisingly obtains best overall performance SimLex999 dataset largest considered dataset consisting 666 noun pairs Our gets second best overall performance dataset A closer look output similarity scores given compared gold standard shows noticeable errors measuring similarity antonym pairs heavily represented dataset These antonym pairs given consistently low values dataset irrespective target words argue similarity scores ought vary according particular semantics antonym pairs For instance pair daynight gets score 19 010 scale gets higher 80 score22 A similar phenomenon sunsetsunrise pair Nevertheless cases words pair belong coordinate synsets WordNet In fact recent works 12110591 shown signiﬁcant performance improvements obtained dataset simply tweaking usual word embedding approaches handle antonymy This differs scores given WordSim353 dataset antonym pairs considered similar 43 It outside scope work change feature order resolve judgment differences respect human annotation antonym pairs SimLex999 dataset 612 Multilingual word similarity Datasets We took RG65 dataset evaluation benchmark The language dataset originally English 117 It later translated French 54 German 39 Spanish 15 We versions dataset experiments Comparison systems We benchmark multilingual word similarity approaches Wikiwup 110 LSAWiki 38 systems use Wikipedia main knowledge resource We provide results cooccurrencebased methods PMI SOCPMI 54 newer word embeddings 31 For word embed 20 Interannotator agreement IAA reported datasets information available IAA reported terms average pairwise Spearman correlation 21 67 showed ﬁne tuning Word2Vec achieve 079 Spearman correlation performance WSSim higher 077 Spearman correlation reported 8 dataset 22 All scores converted 010 scale example 48 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 5 Pearson r Spearman ρ correlation performance different systems English French German Spanish RG65 datasets The interannotator English RG65 highlighted cid13 calculated subset ﬁfteen annotators English Nasari Nasarilexical Nasariuniﬁed Nasariembed SOCPMI PMI LSAWiki Wikiwup Word2Vec Retroﬁtting Nasaripolyembed Polyglotembed r 081 080 080 082 061 041 065 059 074 051 ρ 078 078 076 080 069 073 077 077 055 French Nasari Nasarilexical Nasariuniﬁed SOCPMI PMI LSAWiki Word2Vec Retroﬁtting Nasaripolyembed Polyglotembed r 082 080 082 019 034 057 060 038 ρ 073 070 076 052 047 061 069 035 German Nasari Nasarilexical Nasariuniﬁed SOCPMI PMI Wikiwup Word2Vec Retroﬁtting Nasaripolyembed Polyglotembed r 069 069 071 027 040 065 046 018 ρ 065 067 068 053 060 052 015 Spanish Nasari Nasarilexical Nasariuniﬁed Nasariembed BestWord2Vec Nasaripolyembed Polyglotembed r 085 085 082 079 080 068 051 ρ 079 079 077 077 080 074 056 IAA cid13 085 IAA IAA 081 IAA 083 dings report results Word2Vec model23 approach retroﬁtting Word2Vec vectors WordNet Retroﬁtting 31 For Spanish language result reported 31 Word2Vec trained Word2Vec hyperparameters BestWord2Vec 8 Spanish Billion Words Corpus24 18 We Spanish word embeddings input Nasariembed language Additionally report results pretrained embeddings languages 5 Polyglotembed25 These vectors sixtyfour dimensions trained Wikipedia corpus We compare embedded representations synsets polyglot word embeddings input continuous representations Section 33 We refer method Nasaripolyembed Results Table 5 shows Pearson Spearman correlation performance systems comparison systems RG65 word similarity datasets English French German Spanish26 Our outperforms multilingual comparison systems English French German terms Pearson Spearman correlation For Span ish language surprisingly slightly outperforms human interannotator agreement calculated terms average pairwise Pearson correlation demonstrating competitiveness approach language The Polyglotembed multilingual representations particular potential task The reason results apart inherent ambiguity words low dimensionality 64 small vocabulary 100K words However embedded representation word embeddings Nasaripolyembed hugely improves original vectors obtaining average twentythree Pearson twentyeight Spearman correlation points improvement Nasaripolyembed despite achieving lower results representations achieves competitive results respect comparison systems added beneﬁt applicable languages pretrained polyglot embeddings available languages 613 Crosslingual word similarity Datasets We chosen RG65 crosslingual datasets released 15 English French German Spanish These datasets27 automatically constructed taking manuallycurated multilingual RG65 datasets previous Section input In total evaluated datasets consisting possible language pair combinations languages Comparison systems As crosslingual comparison systems included best results provided CLMSR20 60 This applies PMI EnglishFrench parallel corpus obtained WordNet Additionally provide results best performing systems English word similarity English pivot language28 Baseline pivot systems include WordNetbased ADW 106 pretrained Word2Vec word embeddings 81 performing Word2Vec model similarity obtained 8 BestWord2Vec best countbased model obtained 8 PMISVD See Section 611 details comparison systems We report results 23 For English pretrained models Word2Vec trained Google News corpus 100 billion words considered evaluation For French German corpus 1 billion tokens Wikipedia training 24 Downloaded http crscardellino SBWCE 25 The pretrained polyglot word representations downloaded https sites google com site rmyeid projects polyglot 26 Interannotator agreement IAA reported languages information available IAA reported terms average pairwise Pearson correlation 27 The crosslingual datasets available http lcl uniroma1it similaritydatasets 28 NonEnglish words translated Google Translate J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 49 Table 6 Pearson r Spearman ρ correlation performances different similarity measures crosslingual RG65 datasets Notation English EN French FR German DE Spanish ES Measure ENFR ENDE ENES FRDE FRES DEES Average Nasariuniﬁed CLMSR20 Nasaripi vot ADW pi vot Word2Vecpi vot BestWord2Vecpi vot BestPMISVDpi vot r 084 030 079 080 077 075 076 ρ 079 069 082 082 084 076 r 079 078 073 070 069 072 ρ 079 076 082 073 076 074 r 084 080 078 076 075 077 ρ 082 074 084 080 082 077 r 075 079 072 065 077 065 ρ 070 070 077 070 073 069 r 086 080 081 075 074 076 ρ 078 067 081 076 079 074 r 081 072 068 064 064 062 ρ 080 068 072 063 064 061 r 082 078 075 071 072 071 ρ 078 071 080 074 076 072 combination lexical uniﬁed English Nasari vectors We refer systems English pivot language pivot Results Table 6 shows crosslingual word similarity results according Pearson Spearman correlation performance In section report results uniﬁed vector representations dimensions BabelNet synsets multilingual direct crosslingual comparison Our uniﬁed vector representations outperform comparison systems types terms Pearson correlation performance FrenchGerman pair pivot obtains best result It interesting note English monolingual similarity proves robust language pairs pivot systems according Pearson correlation measure demonstrating reliability purely monolingual scheme Pivot systems prove competitive outperforming crosslingual baseline use pivot language In fact despite obtaining relatively modest Pearson results ADW obtains best results according Spearman correlation measure uniﬁed vector representations obtain second best result overall In terms harmonic mean Pearson Spearman oﬃcial measure previous semantic similarity SemEval task 57 previous works 41 outperforms ADW second overall points 080 077 demonstrating effectiveness direct crosslingual word comparison respect use English pivot language 614 Crosslevel semantic similarity Finally evaluated embedded representations word sense semantic similarity task Recall Sec tion 42 embedded vector representations share space word embeddings Therefore order calculate similarity word sense compute cosine similarity respec tive vector representations In experiment BabelNet sense representation word sense modeled Nasari Otherwise order increase coverage simply word embedding lemma word sense representation Dataset As benchmark opted Word Sense word2sense similarity subtask SemEval2014 Cross Level Semantic Similarity CLSS task 57 The subtask provides 500 wordsense pairs test dataset Each pair associated score denoting semantic overlap items From dataset took subset senses nominal29 277 pairs This dataset includes words usually integrated knowl edge source slang words Our embedded representation model particularly suitable task provides single semantic space words BabelNet senses expanding coverage far vocabulary BabelNet Comparison systems Thirtyeight systems participated word2sense subtask We compare performance embedded representations best performing participating systems subtask Meerkat Maﬁa 59 relies Latent Semantic Analysis LSA uses external dictionaries handle OOV words SemantiKLUE 112 combines set different unsupervised supervised techniques measure semantic similarity The similar SimCompass 80 relies deep learning word embeddings uses WordNet knowledge source Results Table 7 shows Pearson Spearman correlation performance Nasari embedded represen tations comparison systems Meerkat Maﬁa obtains best overall performance dataset Our second best outperforming remaining 37 participating systems SemEval task Interest ingly Nasariembed provides considerable improvement SimCompass 009 007 terms Pearson Spearman correlations respectively based word embeddings uses WordNet lexical resource 29 Note embedded representations measure similarity words Part Of Speech tag 50 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 7 Pearson Spearman correlation performance different systems word2sense test set SemEval2014 task CrossLevel Semantic Simi larity Nasariembed Meerkat Maﬁa SemantiKLUE SimCompass r 040 044 039 031 ρ 040 044 039 033 7 Sense clustering Our second application focuses sense clustering Some sense inventories suffer high granularity sense inventory This high granularity possibly affect performance applications based sense inventories 102 clustering senses beneﬁcial Given setup seamlessly perform sense clustering BabelNet WordNet Wikipedia We follow procedure semantic similarity sense clustering Following 23 view sense clustering binary classiﬁcation task given pair senses task decide merged In usual setting clustering senses semantically related clustered rely similarity scale simply cluster pair items synsets senses pages provided similarity exceeds middle point similarity scale 05 scale 0 1 minimum overlap vectors ﬁve dimensions In speciﬁc sense clustering settings middlepoint threshold changed value determined tuning dataset 71 Evaluation Wikipedia sense clustering Given high granularity Wikipedia sense inventory clustering related senses improve systems Wikipedia knowledge source 46 Wikipediabased Word Sense Disambiguation 7724 example appli cation beneﬁt sense inventory clustering 711 Datasets Wikipedia considered sense inventory different meanings word denoted articles listed disambiguation page 78 Starting Wikipedia disambiguation pages help human annotation 23 created Wikipedia sense clustering datasets In datasets clustering viewed binary clas siﬁcation task possible pairings senses word annotated clustered The ﬁrst dataset refer 500pair dataset contains 500 pairs 357 set belong cluster clustered remaining 143 clustered The second dataset referred SemEval dataset based set highly ambiguous words taken SemEval evaluations 77 consists 925 pairs 162 positively labeled clustered Parameter_computer_programmingParameter FatiguemedicalFatiguesafety sample pairs Wikipedia pages merged As explained based Semantic Similarity Section 6 sense clustering tasks Two senses case Wikipedia pages set clustered similarity greater equal middle point similarity scale 05 712 Results Our experiments carried 500pair SemEval datasets We set naive baselines considering pairs positive clustered Baselinecluster opposite clustering test pairs Baselinenocluster We compare systems proposed 23 Both systems exploit structure content Wikipedia pages multifeature Support Vector Machine classiﬁer trained automaticallylabeled dataset This ﬁrst totally monolingual makes use English Wikipedia pages second exploits Wikipedia multilinguality30 We refer ﬁrst SVMmonolingual second SVMmultilingual Table 8 shows results obtained Wikipedia sense clustering task 500pair SemEval datasets The results shown terms accuracy number correctly labeled pairs divided total number instance pairs FMeasure harmonic mean precision recall As Table unsupervised setting achieves high accuracy outperforming systems 23 SemEval dataset SVMmonolingual 500pair dataset Only supervised 23 information Wikipedia pages different languages outper forms main combined Nasari terms accuracy FMeasure results reported narrow margin Our 30 For second report results conﬁguration exploits Wikipedia pages different languages English German Spanish Italian J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 51 Table 8 Accuracy Acc FMeasure F1 percentages different systems manuallyannotated English Wikipedia sense clustering datasets Measure System type 500pair SemEval Nasari Nasarilexical Nasariuniﬁed Nasariembed SVMmonolingual SVMmultilingual Baselinenocluster Baselinecluster unsupervised unsupervised unsupervised unsupervised supervised supervised Acc 838 816 826 812 774 844 714 286 F1 705 654 695 659 00 445 Acc 874 857 872 863 835 855 825 175 F1 631 574 631 455 00 298 variants comfortably outperforms naive baselines terms accuracy FMeasure When comparing systems combination lexical uniﬁed vectors outperforms singlehanded components However lexical uniﬁed based systems embeddingbased prove highly competitive singlehanded outperforming baselines SemEval dataset including multilingual approach 23 8 Domain labeling Taking BabelNet synset Wikipedia page WordNet synset input task domain labeling consists automatically tagging synset page domains given set The domain labeling task proven useful integrated given lexical resource 12770 direct applications Word Sense Disambiguation 71330 Text Categorization 94 WordNet version 30 domains synsets However number domains large 357 domains uniformly balanced For instance domain named Ethiopia containing single synset domains referring different countries There domains single synsets Molecular Biology Cytology domains annotated relatively high number instances Law 534 annotated instances Moreover coverage domains poor 4098 synsets annotated domain In section present Nasaribased approach automatically tag larger lexical resource BabelNet different set domains achieving signiﬁcantly higher coverage Our creation domain labels BabelNet synsets relies lexical vectors31 The ﬁrst step consists creating lexical vector domain To end follow procedure explained Section 32 learn lexical vector given domain We sets seed Wikipedia pages characterize given domain As context learning vectors given domain use concatenations texts corresponding Wikipedia pages seeds Then order ﬁnd domain synset computed Weighted Overlap corresponding English Nasari lexical vector lexical vector domain For given BabelNet synset s pick domain maximal similarity ˆds arg max dD W O cid5Nasarilexs cid5vlexd 13 cid5Nasarilexs Nasari lexical vector synset s cid5vlexd lexical vector domain d Similarly sense clustering task tagged synsets domain provided minimum overlap respective lexical vectors exceeded ﬁve dimensions For notational brevity refer domain synset s score highest domains domain Wikipedia domains seeds To select set domains Wikipedia featured articles32 set 33 domains Animals Meteorology Music provided For domain Wikipedians selected Wikipedia pages best represent domain We refer Wikipedia pages tagged domain seeds Each domain different number available pretagged Wikipedia pages ranging 9 Mathematics domain 189 Media domain totalling 4230 Wikipedia pages overall From set domains decided remove Companies domain retaining general Business economics ﬁnance domain thought Companies conﬂate domains For instance Nestlé Toyota tagged Companies domain Food drink Transport travel respectively We modiﬁed domain names order general taking account given seeds For example domain Law changed Law crime The ﬁnal set labels includes 32 different domains Table 9 shows domains alphabetical order 31 We lexical vectors shown perform better English Sections 6 7 32 httpsenwikipediaorgwikiWikipediaFeatured_articles 52 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 9 Our set thirtytwo domains Animals Art architecture archaeology Biology Business economics ﬁnance Chemistry mineralogy Computing Culture society Education Engineering technology Food drink Games video games Geography places Geology geophysics Health medicine Heraldry honors vexillology History Language linguistics Law Crime Literature theatre Mathematics Media Meteorology Music Numismatics currencies Philosophy psychology Physics astronomy Politics government Religion mysticism mythology Royalty nobility Sport recreation Transport travel Warfare defense By applying pipeline Wikipedia seeds 39 M BabelNet synsets total 44M English Nasari lexical vectors tagged domain Over 90 500K synsets annotated domain label isolated Wikipedia pages pages linked Wikipedia page composed sentences 81 Experiments In section report experiments domain labeling task First Section 811 explain construction gold standard domainlabeled datasets Then baseline systems Section 812 compare newly created gold standard datasets Section 813 811 Gold standard dataset construction In order evaluate performance domain labeling approach constructed gold standard domain labeled datasets WordNet domainlabeled dataset For construction dataset took WordNet 30 synsets man ually tagged domains The domain set WordNet differs set domains Table 9 ﬁnal domain set Therefore performed manual mapping WordNet domains domain set order comparable Domains WordNet mapped domains provided surface form WordNet domain matched surface form domain labels For instance WordNet synset domain Business Economics Finance mapped domain Business economics ﬁnance There WordNet synsets tagged domain WordNet considered single domain WordNet gold standard construction As result obtained gold standard dataset 1540 WordNet synsets tagged domain set33 BabelNet domainlabeled dataset In order realistic distribution BabelNet synsets comprising synsets belong WordNet sense inventory created second goldstandard dataset based BabelNet For randomly sampled 200 BabelNet synsets English lexicalization set 65M possible BabelNet synsets Of 65 integrated Wikipedia 15 belonged WordNet remaining synsets integrated WikiData Two annotators manually labeled 200 synsets They instructed mark synset single domain Any disagreements adjudicated ﬁnal phase annotators The interannotator agreement computed 86 viewed upperbound performance automatic systems 812 Comparison systems As benchmark developed different baselines baselines based Wikipedia propagating domains lexical resource taxonomy Similarly approach ﬁrst baselines construct lexical vector domain As seeds Wikipediabased baselines set Wikipedia pages Lexical vectors computed Wikipedia page similarity Wikipedia page domain calculated Finally Wikipedia page domain similarity score selected Vectors constructed following classic vector space model scheme resulting vectors individual content words similarity vectors calculated standard cosine similarity measure The difference baselines lays calculation weights dimension WikipediaTF calculates weights basis term frequencies TF WikipediaTFidf combines term frequency conventional inverse document frequency weighting scheme 52 tfidf For Wikipediabased systems relied mapping provided BabelNet 30 WordNet synsets Wikipedia pages The baseline TaxoProp henceforth uses taxonomybased domain propagation The takes seeds respective domainlabeled gold standard datasets Section 811 Algorithm 2 shows process obtaining 33 There overlap 1540 WordNet synsets Wikipedia seeds taken J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 53 Algorithm 2 Taxonomybased Domain Propagation TaxoProp Input nontagged synset s set domaintagged synsets D function T axs associates synset s reference sense inventory set hyponyms hypernyms taxonomy S prev S Synset s cid15 S prev Neighbour synset n T axs Output domain tag input synset s 1 Set S s 2 Frequency domain dictionary F 3 tie T rue 4 S prev 5 tie S prev S 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ˆds arg maxdF F d F 1 maxdF F d max Domain dn Dn dn cid12 F F dn 1 n S S S n n D F dn F dn 1 F 0 cid15 tie F alse 20 21 ti e 22 23 24 return ˆds return null dF ˆds F d domain label nontagged synset s The based taxonomy works iteratively First goes neighbors s taxonomy checks tagged domain lines 716 Algorithm In case particular domain ˆd encountered domain neighbors domains s tagged ˆd line 24 Algorithm Otherwise repeat process taxonomy checking domain tags neighbors neighbors We repeat domain appearing frequently domain lines 520 Algorithm In order test algorithm datasets WordNet TaxoProp WN BabelNet TaxoProp BN taxonomies carried 10fold cross validation test dataset34 Finally compared WNDomains32 709 latest released version WordNet Domains35 The essence similar TaxoProp described sense takes seeds domain manually selected synsets located high taxonomy input spreads WordNet taxonomy This involves undetermined manual intervention selection seeds small number high level synsets manually annotated pertinent Subject Code Fields36 manual curation main problems detected manual annotations corrected 70 WNDomains32 released WordNet 20 For testing WordNetbased dataset mapping versions 20 30 WordNet37 813 Results Results shown Table 10 terms standard precision recall FMeasure When comparing Wikipedia based systems tfidf proves reliable term frequency performance signiﬁcantly Nasaribased It interesting note robust datasets Wikipediabased approaches experience drastically reduced performance BabelNet dataset This fact Wikipedia pages associated WordNet synsets general richer longer average Wikipedia page BabelNet dataset synsets extracted randomly In contrast TaxoProp achieves competitive results obtaining lower pre cision highest overall recall WordNet dataset However lead wrong conclusions Given coverage actually considerably larger synsets covered WordNet recall approach fact larger relying WordNet knowledge resource 117K synsets WordNet Additionally WordNetbased approach advantage annotating exactly number domains occur gold standard dataset Our 4230 Wikipedia pages 32 different domains seeds contrast 1386 domainlabeled WordNet synsets 27 different domains comprising gold 34 In order results reliable sensitive dataset order repeated experiment times The gold standard dataset shuﬄed time ﬁnal score obtained averaging results different runs 35 WordNet Domains available http wndomains fbkeu 36 Subject Code Fields corresponds domain labels notation 37 https wordnet princeton edu wordnet download currentversion 54 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 10 Precision recall Fmeasure percentages different systems gold standard WordNet BabelNet domainlabeled datasets WordNet dataset BabelNet dataset Precision Recall FMeasure Precision Recall FMeasure Nasarilexical WikipediaTF WikipediaTFidf TaxoProp WN TaxoProp BN 779 254 459 713 735 WNDomains32 936 701 164 297 707 735 644 738 199 361 710 735 763 623 34 88 483 405 25 65 372 491 29 75 420 standard dataset As measure supervision case calculated seed density percentage synsets average domain seeds Formally calculated ratio average number seeds domain total number synsets given resource In fact seed density signiﬁcantly higher WordNetbased 0044 vs 0001 WNDomains32 outperforms terms FMeasure 25 absolute percentage points WordNet dataset Interestingly despite benchmark subset WordNet obtains higher recall WN Domains32 Additionally remarked annotates signiﬁcantly higher number instances including named entities specialized concepts covered WordNet 39M domainlabeled synsets annotated opposed 74K synsets annotated WNDomains32 In terms precision WNDomains 32 involves undetermined manual curation outperforms default However simply adding conﬁdence threshold considerably increase precision For instance tagging synsets domain score higher middle point similarity scale 05 obtain comparable results terms precision percentage 925 WNDomains32 obtaining considerably higher coverage Note WNDomains32 range domains considered original systems larger number domains gold standard increases error margin For instance original setting considered 32 domains Table 9 27 present gold standard dataset By analyzing errors given realized synsets tagged domain If domain tags account precision increases 918 831 recall 827 54 WordNet BabelNet datasets respectively For example tags WordNet synset corresponding concept angular_velocity1 n Mathematics domain narrow margin case tagged Physics astronomy domain second domain right answer according gold dataset As second source error realized arguable false positives given fact entirely wrong Indeed cases judgement considered justiﬁable equally correct tagging gold dataset For instance synset represented data processing sense operation tagged Mathematics domain gold domain Computing In case clear synset tagged domains Another example WordNet synset aesthetics1 n deﬁned WordNet The branch philosophy dealing beauty taste emphasizing evaluative criteria applied art tagged Philosophy psychology domain instead Art architecture archeology domain label gold dataset 9 Word sense disambiguation Word Sense Disambiguation WSD core task natural language understanding Given target word context task consists associating entry given sense repository 93 WSD eventually applied Natural Language Processing task enabling understanding sentences machine usually achieved mainstream statistical approaches beneﬁt applications Machine Translation 134 Information Retrieval 120 In Section 91 present different resources knowledge repositories WSD A uniﬁed framework WSD based Nasari presented Section 92 Experiments presented Section 93 91 Sense inventories One main knowledge sense repositories task manually constructed WordNet 95111 usually leads ﬁnegrained type disambiguation given nature senses WordNet Another resource recently task Wikipedia 782495 wide coverage named entities multilinguality A newer resource knowledge repository gaining popularity thanks multilinguality large coverage Babel Net 9588136 main resource Given nature vectors contrast WSD systems J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 55 seamlessly disambiguate resources BabelNet integrates resources WordNet Wikipedia In following section propose uniﬁed framework disambiguating words context irrespective resource 92 Framework word sense disambiguation In 16 presented WSD framework lexical vectors calculated overlap target word vector context harmonically weighting ranks overlapping words target word vector This method considers word context equally important weight disambiguation process In section present suitable approach keeps spirit previous lexical semantics applications gives word weight context Given set target words text T build lexical vector context explained Section 32 Then target word w text T retrieve set possible BabelNet synsets target word lexicalizations set refer Lw Finally simply compute Weighted Overlap Section 35 cid5vlexT lexical vector text T Nasari vectors corresponding BabelNet synsets contain senses w In setting BabelNet synset terms WO score ˆs selected best sense given target word ˆs arg max sLw W O cid5vlexT cid5 Nasarilexs 93 Experiments 14 We perform Word Sense Disambiguation experiments sense inventories Wikipedia WordNet Recall Section 91 main knowledge sense inventory BabelNet seamlessly disambiguate instances knowledge sources The setting cases difference use BabelNet synsets38 mapped Wikipedia page WordNet synset disambiguating resources respectively As literature 13214289 use backoff strategy Most Frequent Sense MFS baseline cases provide conﬁdent answer Hence WSD framework tagged instances similarity score Section 92 details WSD higher given threshold θ In order compute θ use English Wikipedia trial dataset provided SemEval2013 WSD task 95 The performing value θ 020 value WSD experiments39 Section 931 presents multilingual WSD experiments Wikipedia main sense inventory task strongly related Wikiﬁcation task 78 Section 932 presents experiments Named Entity Disambiguation task BabelNet sense inventory ﬁnally Section 933 presents WSD results English WordNet sense inventory 931 Multilingual word sense disambiguation Wikipedia We SemEval2013 allwords WSD dataset 95 benchmark multilingual evaluations4041 This dataset includes texts ﬁve different languages English French German Italian Spanish average 1303 disam biguated instances language including multiword expressions named entities Comparison systems As comparison include Babelfy 8942 stateoftheart graphbased multi lingual joint WSD Entity Linking Babelfy relies random walks BabelNet semantic network combined graphbased heuristics We report results best run language SemEval2013 sys tem 40 UMCCDLSI As baseline diﬃcult beat WSD tasks 93 include Most Frequent Sense MFS43 heuristic Finally report results MUFFIN 16 previous WSD based Nasari vectors contrast WSD framework words context considered equally important 38 In order avoid disambiguating synsets rarely practise isolated BabelNet graph experiments considered BabelNet synsets thirty edges BabelNet graph 39 We considered values θ 0 1 step size 005 40 In experiments Wikipedia dump December 2014 opposed original SemEval2013 dataset A Wikipedia page titles updated creation dataset update titles gold standard 41 We release updated gold standard dataset website Note Wikipedia page titles unique identiﬁers Wikipedia page change Wikipedia page title automatically modiﬁes unique identiﬁer For instance English Wikipedia page titled Sevenday week SemEval 2013 dataset updated Wikipedia currently titled simply Week 42 http babelfyorg 43 MFS provided baseline task organizers However MFS score French ﬁxed respect 16 showed lower MFS FMeasure score The scorer provided organizers casesensitive Wikipedia page titles gold standard ﬁle match casing baseline ﬁle lowercased This led misalignments gold standard baseline ﬁle 56 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 11 FMeasure percentage performance SemEval2013 Multilingual WSD datasets Wikipedia sense inventory System Nasarilexical Muﬃn Babelfy UMCCDLSI MFS English French Italian German Spanish Average 863 845 874 548 802 762 714 716 605 749 837 819 843 583 822 832 831 816 610 830 829 851 838 581 821 825 812 817 585 793 Table 12 FMeasure percentage performance English Named En tity Disambiguation dataset Multilingual AllWords Sense Disambiguation Entity Linking SemEval2015 task Ba belNet sense inventory System Nasarilexical DFKI SUDOKU el92 MFS Type unsupervised supervised unsupervised systems mix FMeasure 871 889 870 861 857 Results Table 11 shows FMeasure percentage results comparison systems SemEval2013 dataset As table achieves stateoftheart results French German achieve best average performance languages demonstrating robustness languages outper forming current stateoftheart results Babelfy Our outperforms previous WSD approach Muﬃn point average highlighting improvements particular WSD task proposed new framework Section 92 932 English Named Entity Disambiguation BabelNet In order evaluate quality named entity representations performed experiments Named Entity Disambiguation task Given Nasari provides semantic representations concepts named entities task analogous Word Sense Disambiguation Section 92 difference task considered entity synsets candidates To end English named entity dataset SemEval2015 Task AllWords Sense Disambiguation Entity Linking 88 This dataset consists 85 named entities disambiguate Comparison systems We benchmarked disambiguation best performing systems task ones outperforming MFS baseline DFKI 137 SUDOKU 72 el92 118 DFKI multiobjective based global unsupervised local supervised objectives SUDOKU uses Personalized PageRank algorithm disambiguating monosemous instances text Finally el92 based weighted voting disambiguation systems Wikipedia Miner 87 TagME 32 DBpedia Spotlight 76 Babelfy 89 Results Table 12 shows FMeasure percentage results Named Entity portion SemEval2015 WSD dataset44 Our obtains second overall position 17 systems participated SemEval2015 Named Entity Disambiguation task The combination global unsupervised local supervised objectives DFKI obtains best overall results As Section 933 discuss Section 94 based solely global semantic features generally improves including local supervised features 933 English Word Sense Disambiguation WordNet For task English WSD WordNet main sense inventory recent SemEval WSD datasets ﬁne grained allwords SemEval2007 111 allwords SemEval2013 95 We performed experiments 162 noun instances SemEval2007 dataset SemEval2013s dataset contains 1644 instances Comparison systems We include stateoftheart IMS 143 supervised As unsupervised systems report performance graphbased approaches based random walks respective semantic networks BabelNet 89 Babelfy WordNet 4 UKB Another approach uses BabelNet reference knowledge base MultiObjective 136 views WSD multiobjective optimization problem We report results 44 We inaccuracy instance gold standard dataset The unambiguous instance KAlgebra disambiguated KAlgebra concept Catalan language belongs separate synset general KAlgebra concept languages This instance repeated times dataset By ﬁxing issue achieves FMeasure results 90 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 57 Table 13 FMeasure percentage performance SemEval2013 SemEval2007 noun instances English allwords WSD datatets WordNet sense inventory ﬁne grained System Nasarilexical Nasarilexical IMS Muﬃn Babelfy UKB UMCCDLSI MultiObjective IMS MFS SemEval2013 SemEval2007 667 670 660 659 613 647 728 653 632 667 685 660 627 560 660 673 658 best conﬁguration topperforming SemEval2013 dataset UMCCDLSI 40 As Section 931 include earlier WSD MUFFIN comparison Finally include called NASARIIMS based WSD framework difference backoff IMS instead MFS45 Results Table 13 shows FMeasure percentage performance systems SemEval2007 SemEval2013 WSD datasets Similarly WSD results Wikipedia main sense inventory Section 931 Nasari performs previous Muﬃn Nasari default setting backingoff MFS surpassed MultiObjective SemEval2013 IMS SemEval2017 outperforming remaining systems datasets Our backingoff IMS NasariIMS improves default Nasari datasets obtaining best performance systems SemEval2007 dataset We remark Nasari unsupervised based global contexts IMS supervised based local contexts This combination local global contexts shown beneﬁcial WSD tasks 45107136 94 Discussion global local contexts Our method functions analyzing context given target word disambiguated Hence fail capture correct intended meaning word case local context plays key role determining speciﬁc meaning target word especially ﬁnegrained disambiguation setting For instance following example SemEval2013 Word Sense Disambiguation test set shows case committed mistake ﬁnegrained sense deﬁnition target word behaviour However mistake solved analyzing local context typical setting supervised systems 1 The expulsion presumably forged players Real Madrid Xabi Alonso Sergio Ramos game played 23rd November Ajax European Champions League caused rivers ink written behaviour unsportmanlike players sanctioned UEFA Our able conﬁdent selection sense behaviour3 n The aggregate responses reactions movements organism situation behaviour4 n Manner acting controlling picking narrow margin In case leverage exploits local contexts IMS accurate disambiguation On hand cases possible fully distinguish intended meaning looking local context target word The following sentence SemEval 2013 dataset example case 2 This way Real Madrid ﬁnish leader group players fulﬁll prescribed sanction game league In case IMS picks sanction1 n Formal explicit approval frequent sense noun sanction In fact misled considering local context ignoring general picture given global context In case Nasari correctly captures semantics text chooses sanction2 n A mechanism social control enforcing societys standards In cases combination Nasari IMS provides correct answer In general combination methods shows consistent improvement single components In fact results combination knowledgebased globalcontext disambiguation Nasari stateoftheart supervised localcontext 45 The MFS baseline obtained SemCor senseannotated corpus 85 58 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 Table 14 Ablation test results different settings We report Pearson r Spearman ρ correlations RG65 MC30 WSSim SimLex999 noun subset word similarity datasets columns 27 For Word Sense Disambiguation FMeasure percentage performance shown SemEval2007 SemEval2013 datasets use WordNet sense inventory columns 89 Word similarity Word Sense Disambiguation MC30 WSSim SL999 nouns SemEval2007 SemEval2013 Nasarilexical NasariTFidf NasariTFidf3000d Nasariunifweight Nasariembed Nasariavembed r 088 084 085 086 091 081 ρ 081 077 079 079 083 075 r 074 071 072 073 068 058 ρ 073 071 072 072 068 063 r 051 046 048 049 048 040 ρ 049 046 047 048 046 041 FMeasure FMeasure 667 660 660 660 667 661 659 664 approach IMS proves robust datasets outperforming strong baselines Table 13 10 Analysis In order gain better insight role key components systems pipeline play overall performance carried ablation test In particular interested evaluating impact importance following components 1 Lexical speciﬁcity To check lexical speciﬁcity Section 31 fares standard tfidf measure 52 generated Nasari lexical vectors weights calculated conventional tfidf Given word w calculate T F idf w follows T F idf w f w log D p D w p 15 f w frequency w subcorpus SC s representing contextual information synset s Section 41 D set pages Wikipedia We computed sets tfidf based lexical vectors The ﬁrst version called NasariTFidf keeps dimensions vector For second version NasariTFidf3000d follow 37 prune vector 3000 nonzero dimensions This pruning similar performed au tomatically lexical speciﬁcity reduces number nonzero dimensions retaining interpretability vector dimensions 2 Weighted semantic relations To assess advantage gain introducing weights semantic relations Section 411 computed version lexical vectors semantic relations uniformly weighted λi 1 1 n Equation 11 case earlier work 16 We refer version Nasariunifweight 3 Combination strategy embeddings Finally carried analysis compare harmonic combination word embeddings Section 33 uniform combination averaging For purpose computed embedding vector given synset centroid embeddings words present corresponding lexical vector We refer variant Nasariavembed tables We evaluated ﬁrst components intrinsic task word similarity downstream application Word Sense Disambiguation For component compared default Nasariembed embedding representations obtained uniform weighting word similarity task We performed evaluations datasets Section 611 word similarity Section 933 Word Sense Disambiguation WordNet sense inventory The pipeline tasks left unchanged variants components mentioned Table 14 shows results ablation test different word similarity Word Sense Disambiguation datasets Our default Nasarilexical consistently outperforms baselines datasets tasks demonstrating reliability proposed lexical speciﬁcity preweighting semantic relations This result especially meaningful taking account default fewest nonzero dimensions average evaluated approaches In fact average number nonzero dimensions Nasarilexical vectors 162 lower 280 nonzero dimensions Nasariunifweight 1033 NasariTFidf3000d46 1561 NasariTFidf This low average number nonzero dimensions enables fast processing vectors computationally faster work 46 In NasariTFidf3000d maximum number nonzero dimensions set 3000 cases vector actually lower number nonzero dimensions J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 59 As far Nasariembed vectors concerned default consistently obtained signiﬁcantly better results compared baseline Nasariavembed In general Nasariavembed produces consistently high similarity values nonsimilar pairs This fact words relevant input synset relatively low lexical speciﬁcity values given weight words clearly relevant high lexical speciﬁcity values This turn weighted average word embeddings lexical vector leads accurate results simple average 11 Related work In addition semantic representation word senses main topic article brieﬂy review recent literature popular applications evaluated representations semantic similarity Word Sense Disambiguation 111 Representation word senses Most research studies semantic representation far concentrated representation words seen numerous available word similarity datasets benchmarks relatively studies focused representation word senses concepts This partly socalled knowledge acquisition bottleneck arises application distributional word modeling techniques prominent representation approach sense level require availability highcoverage senseannotated data However word representations known suffer issues dampen suitability tasks require accurate representations meaning The important drawback word representations lies inability model polysemy homonymy conﬂate different meanings word single representation 130114 For instance word representation word bank distinguish ﬁnancial institution river bank meanings word noun bank senses according WordNet 30 The approach 31 leverages semantic lexicons improve word representations suffers drawback Because represent lowest linguistic level word senses concepts play crucial role natural language understanding Since level individual meanings word identiﬁed separately modeled resulting rep resentations ideal accurate semantic representation In addition ﬁnegrained representation word senses directly extended higher linguistic levels 13 words makes interesting These features recently attracted attention different research studies Most techniques view sense representation speciﬁc type word representation try adapt existing distributional word modeling techniques sense level usually clustering contexts word appears 1384799 The fundamental assumption intended meaning word mainly depends context obtain sensespeciﬁc contexts given word sense clustering contexts word appears given text corpus Various clusteringbased techniques usually differ clustering procedure combined representation technique How models limited representing senses covered underlying corpus Moreover sense representations obtained methods usually linked sense inventory linking carried manually help senseannotated data representations direct applications Word Sense Disambiguation Most sense modeling techniques based representation knowledge derived resources WordNet Earlier techniques exploit information provided WordNet synonymous words synset representation word senses 792 More recent approaches usually adapt distributional models sense level basis lexicosemantic knowledge derived lexical resources Wikipedia 3577 WordNet 1950116 languagespeciﬁc semantic networks 51 WordNet viewed semantic network individual synsets represented basis graphbased algorithms 106 Word Sense Disambiguation large amounts textual data explored means obtaining highcoverage annotated data learning sense representations based neural networks representation referred sense embeddings 48 19 uses WordNet main knowledge source relies WSD obtaining sense representations However approaches hampered inherently imperfect WSD systems Additionally techniques limited reduced coverage WordNet English language In contrast method provides multilingual representation word senses basis complementary knowledge different resources enabling signiﬁcantly higher coverage speciﬁc domains named entities Our representations multilingual compared languages uniﬁed representations 112 Semantic similarity Semantic similarity word senses usually computed basis structural properties lexical databases WordNet 613 thesauri Rogets 9049 These measures represent lexical resource semantic network exploit networks computation semantic similarity pair word senses The conventional WordNetbased similarity techniques source information structural 60 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 properties WordNet semantic network graph distance lowest common superordinate word senses 4465139 combine structural information statistics obtained text corpora 11569 Collaboratively constructed resources Wikipedia Wiktionary underlying lexical resources different semantic similarity techniques 4112686 More recent sense similarity methods ﬁrst perform random walks seman tic networks 106141109 order model individual word senses use representations computation sense similarity All techniques limited knowledge provided underlying semantic source In contrast approach combines expertbased encyclopedic knowledge different types resource providing advantages 1 effective measurement similarity based rich semantic representations 2 possibility measuring crossresource semantic similarity Wikipedia pages WordNet synsets 3 possibility comparing semantics word senses different languages 113 Word Sense Disambiguation Word Sense Disambiguation task beneﬁt signiﬁcantly representation word senses mainly senselevel application Based type resources use WSD techniques main categories knowledgebased supervised 93 Supervised systems receive senseannotated data source information set contexts speciﬁc sense word appears These systems analyze provided data capture context speciﬁc word sense likely appear It Makes Sense 143 IMS example supervised despite small set conventional features simple linear classiﬁer best performers different WSD benchmarks However performance supervised systems depends availability senseannotated data target word sense 107 Hence applicability systems limited words languages data available practically restricting small subset word senses mainly English language Knowledgebased approaches hand suffer lack senseannotated data provide relatively higher coverage These systems usually exploit structural lexicalsemantic information lexical resources disambiguation 122964 However similarly supervised counterparts knowledgebased techniques limited English language Recent years seen growing multilingual WSD 95 Multilinguality usually offered methods exploit structural information largescale multilingual lexical resources Wikipedia 407346 Babelfy 89 WSD performs random walks BabelNet multilingual semantic network 98 makes use densest subgraph heuristics However approach limited WSD Entity Linking tasks In contrast approach global different NLP tasks including WSD Entity Linking 12 Conclusions In article presented Nasari novel technique representation concepts named entities arbitrary languages Our approach combines structural knowledge semantic networks statistical information derived text corpora effective representation millions BabelNet synsets including WordNet nominal synsets Wikipedia pages We evaluated representations wide range NLP tasks applications semantic similarity sense clustering Word Sense Disambiguation domain labeling We reported stateoftheart performance datasets tasks different languages Three type sense representation forward explicit vector representations uniﬁed lexical vector dimensions interpretable latent embeddingbased representation Each representation advan tages limitations In general combination lexical uniﬁed vectors led reliable results semantic similarity sense clustering experiments Sections 6 7 Among representations lexical representation Nasarilexical obtained best performance monolingual settings However lexical vectors sparse computationally faster process dimensionality high equal size vocabulary In trast embedded representation Nasariembed ﬁxed low number latent dimensions Additionally embedded synset vectors share space word embeddings input As regards uniﬁed representation Nasariuniﬁed provide effective way representing word senses different languages thanks uniﬁed semantic space enables direct comparison different representations languages In addition multilingual Nasari improves existing techniques providing high coverage millions concepts named entities deﬁned BabelNet sense inventory Release We releasing complete set representations obtained technique ﬁve different languages English Spanish French German Italian http lcl uniroma1it nasari plan generate representations languages near future We provide Python script computation lexical speciﬁcity Finally main labels included BabelNet 35 release version47 gold standard domainlabeled datasets experiments Section 811 provided website 47 BabelNet domain labels based Nasari extended set taxonomybased heuristics BabelNet 35 includes 165M synsets annotated domain label J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 61 Future work As future work plan pursue main directions Firstly aim compute global representation concept exploiting statistical information obtained multiple languages Secondly plan develop framework meaningful combination representations supervised improved joint WSD Entity Linking Thirdly plan integrate multilingual semantic representations different enduser applications Machine Translation Acknowledgements The authors gratefully acknowledge support ERC Starting Grant MultiJEDI No 259234 References 1 E Agirre E Alfonseca K Hall J Kravalova M Pa sca A Soroa A study similarity relatedness distributional WordNetbased ap 2 E Agirre OL Lacalle Publicly available topic signatures WordNet nominal senses Proceedings LREC Lisbon Portugal 2004 proaches Proceedings NAACL 2009 pp 1927 pp 11231126 3 E Agirre OL Lacalle A Soroa Knowledgebased WSD speciﬁc domains performing better generic supervised WSD Proceedings 21st International Joint Conference Artiﬁcial Intelligence IJCAI Pasadena California 2009 pp 15011506 4 E Agirre A Soroa Personalizing PageRank Word Sense Disambiguation Proceedings EACL 2009 pp 3341 5 R AlRfou B Perozzi S Skiena Polyglot distributed word representations multilingual nlp Proceedings Seventeenth Conference Computational Natural Language Learning Soﬁa Bulgaria 2013 pp 183192 6 S Banerjee T Pedersen An adapted Lesk algorithm Word Sense Disambiguation WordNet Proceedings Third International Con ference Computational Linguistics Intelligent Text Processing CICLing02 Mexico City Mexico 2002 pp 136145 7 D Bär T Zesch I Gurevych DKPro similarity open source framework text similarity Proceedings 51st Annual Meeting Association Computational Linguistics System Demonstrations Soﬁa Bulgaria August 2013 pp 121126 8 M Baroni G Dinu G Kruszewski Dont count predict A systematic comparison contextcounting vs contextpredicting semantic vectors Proceedings ACL 2014 pp 238247 9 L Bentivogli P Forner B Magnini E Pianta Revising wordnet domains hierarchy semantics coverage balancing Proceedings Workshop Multilingual Linguistic Resources Association Computational Linguistics 2004 pp 101108 10 MB Billami J CamachoCollados E Jacquey L Kister Annotation sémantique et validation terminologique en texte intégral en SHS Proceedings TALN 2014 pp 363376 11 S Bordag Word sense induction tripletbased clustering automatic evaluation Proceedings 11th Conference European Chapter Association Computational Linguistics EACL Trento Italy 2006 pp 137144 12 S Brody M Lapata Bayesian Word Sense Induction Proceedings EACL 2009 pp 103111 13 A Budanitsky G Hirst Evaluating WordNetbased measures lexical semantic relatedness Comput Linguist 32 1 2006 1347 14 J CamachoCollados M Billami E Jacquey L Kister Approche statistique pour le ﬁltrage terminologique des occurrences candidats termes en texte intégral Proceedings JADT 2014 pp 121133 15 J CamachoCollados MT Pilehvar R Navigli A framework construction monolingual crosslingual word similarity datasets Pro ceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing Short Papers Beijing China 2015 pp 17 16 J CamachoCollados MT Pilehvar R Navigli A uniﬁed multilingual semantic representation concepts Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing Beijing China 2015 pp 741751 17 J CamachoCollados MT Pilehvar R Navigli NASARI novel approach semanticallyaware representation items Proceedings NAACL 18 C Cardellino Spanish billion words corpus embeddings httpcrscardellinomeSBWCE March 2016 19 X Chen Z Liu M Sun A uniﬁed model word sense representation disambiguation Proceedings EMNLP Doha Qatar 2014 20 R Collobert J Weston A uniﬁed architecture natural language processing deep neural networks multitask learning Proceedings ICML 21 CJ Crouch A clusterbased approach thesaurus construction Proceedings 11th Annual International ACM SIGIR Conference Research Development Information Retrieval SIGIR 88 1988 pp 309320 22 JR Curran M Moens Improvements automatic thesaurus extraction Proceedings ACL02 Workshop Unsupervised Lexical Acquisition 2015 pp 567577 pp 10251035 2008 pp 160167 vol 9 ULA 02 2002 pp 5966 ing Hissar Bulgaria 2013 pp 164171 23 B Dandala C Hokamp R Mihalcea RC Bunescu Sense clustering Wikipedia Proceedings Recent Advances Natural Language Process 24 B Dandala R Mihalcea R Bunescu Word sense disambiguation Wikipedia The Peoples Web Meets NLP Springer 2013 pp 241262 25 SC Deerwester ST Dumais TK Landauer GW Furnas RA Harshman Indexing latent semantic analysis J Am Soc Inf Sci 41 6 1990 26 C Delli Bovi L Telesca R Navigli Largescale information extraction textual deﬁnitions deep syntactic semantic analysis Trans Assoc Comput Linguist 3 2015 529543 27 A Di Marco R Navigli Clustering diversifying web search results graphbased word sense induction Comput Linguist 39 3 2013 28 P Drouin Term extraction nontechnical corpora point leverage Terminology 9 1 2003 99115 29 K Erk A simple similaritybased model selectional preferences Proceedings 45th Annual Meeting Association Computational Linguistics Prague Czech Republic 2007 pp 216223 30 S Faralli R Navigli A new minimallysupervised framework domain word sense disambiguation Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning Jeju Korea 2012 pp 14111422 31 M Faruqui J Dodge SK Jauhar C Dyer E Hovy NA Smith Retroﬁtting word vectors semantic lexicons Proceedings NAACL 2015 391407 709754 pp 16061615 62 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 32 P Ferragina U Scaiella Tagme ontheﬂy annotation short text fragments Wikipedia entities Proceedings 19th ACM International Conference Information Knowledge Management ACM 2010 pp 16251628 33 L Finkelstein G Evgenly M Yossi R Ehud S Zach W Gadi R Eytan Placing search context concept revisited ACM Trans Inf Syst 20 1 2002 116131 pp 16061611 34 T Flati D Vannella T Pasini R Navigli Two bigger better Wikipedia bitaxonomy project Proceedings 52nd Annual Meeting Association Computational Linguistics Baltimore USA 2014 pp 945955 35 E Gabrilovich S Markovitch Computing semantic relatedness Wikipediabased explicit semantic analysis Proceedings IJCAI 2007 36 P Gärdenfors Conceptual Spaces The Geometry Thought The MIT Press 2004 37 Y Gong L Wang M Hodosh J Hockenmaier S Lazebnik Improving imagesentence embeddings large weakly annotated photo collections 38 R Granada C Trojahn R Vieira Comparing semantic relatedness word pairs Portuguese Wikipedia Computational Processing Computer VisionECCV 2014 Springer 2014 pp 529545 Portuguese Language 2014 pp 170175 39 I Gurevych Using structure conceptual network computing semantic relatedness Proceedings IJCNLP 2005 pp 767778 40 Y Gutiérrez Y Castañeda A González R Estrada DD Piug IJ Abreu R Pérez A Fernández Orquín A Montoyo R Muñoz F Camara UMCC_DLSI reinforcing ranking algorithm sense frequencies multidimensional semantic resources solve multilingual word sense disambiguation Proceedings SemEval 2013 2013 pp 241249 41 S Hassan R Mihalcea Semantic relatedness salient semantic analysis Proceedings AAAI 2011 pp 884889 42 S Heiden JP Magué B Pincemin et al Txm une plateforme logicielle opensource pour la textométrieconception et développement Statistical Analysis Textual DataProceedings 10th International Conference Journées dAnalyse Statistique des Données Textuelles vol 2 Rome Italy 2010 pp 10211032 43 F Hill R Reichart A Korhonen Simlex999 evaluating semantic models genuine similarity estimation arXiv14083456 2014 44 G Hirst D StOnge Lexical chains representations context detection correction malapropisms C Fellbaum Ed WordNet An Electronic Lexical Database MIT Press 1998 pp 305332 45 J Hoffart MA Yosef I Bordino H Fürstenau M Pinkal M Spaniol B Taneva S Thater G Weikum Robust disambiguation named entities text Proceedings Conference Empirical Methods Natural Language Processing Association Computational Linguistics 2011 pp 782792 46 EH Hovy R Navigli SP Ponzetto Collaboratively built semistructured content artiﬁcial intelligence story far Artif Intell 194 2013 47 EH Huang R Socher CD Manning AY Ng Improving word representations global context multiple word prototypes Proceedings 227 ACL Jeju Island South Korea 2012 pp 873882 48 I Iacobacci MT Pilehvar R Navigli Sensembed learning sense embeddings word relational similarity Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing volume 1 Long Papers Association Computational Linguistics Beijing China 2015 pp 95105 49 M Jarmasz S Szpakowicz Rogets thesaurus semantic similarity Proceedings RANLP 2003 pp 212219 50 SK Jauhar C Dyer E Hovy Ontologically grounded multisense representation learning semantic vector space models Proceedings NAACL 51 R Johansson LN Pina Embedding semantic network word space Proceedings NAACL 2015 pp 14281433 52 KS Jones A statistical interpretation term speciﬁcity application retrieval J Doc 28 1972 1121 53 MP Jones JH Martin Contextual spelling correction latent semantic analysis Proceedings Fifth Conference Applied Natural 54 C Joubarne D Inkpen Comparison semantic similarity different languages Google ngram corpus secondorder cooccurrence Language Processing ANLC 97 1997 pp 166173 measures Advances Artiﬁcial Intelligence 2011 pp 216221 55 D Jurgens Embracing ambiguity comparison annotation methodologies crowdsourcing word sense labels HLTNAACL 2013 pp 556562 56 D Jurgens R Navigli Its fun games annotates video games purpose linguistic annotation Trans Assoc Comput Linguist 2 2014 449464 57 D Jurgens MT Pilehvar R Navigli Semeval2014 task 3 crosslevel semantic similarity SemEval 2014 2014 pp 1726 58 D Jurgens K Stevens Measuring impact sense similarity Word Sense Induction Proceedings First Workshop Unsupervised Learning NLP EMNLP 11 Edinburgh Scotland 2011 pp 113123 59 A Kashyap L Han R Yus J Sleeman T Satyapanich S Gandhi T Finin Meerkat maﬁa multilingual crosslevel semantic textual similarity systems Proceedings 8th International Workshop Semantic Evaluation Association Computational Linguistics 2014 pp 416423 60 A Kennedy G Hirst Measuring semantic relatedness languages Proceedings xLiTe CrossLingual Technologies Workshop Neural Information Processing Systems Conference 2012 61 AHF Laender BA RibeiroNeto AS da Silva JS Teixeira A brief survey web data extraction tools SIGMOD Rec 31 2 2002 8493 62 P Lafon Sur la variabilité la fréquence des formes dans corpus Mots 1 1980 127165 63 T Landauer S Dooley Latent semantic analysis theory method application Proceedings CSCL 2002 pp 742743 64 TK Landauer ST Dumais A solution Platos problem latent semantic analysis theory acquisition induction representation knowl edge Psychol Rev 104 2 1997 211240 Lexical Database MIT Press 1998 pp 265283 65 C Leacock M Chodorow Combining local context WordNet similarity word sense identiﬁcation C Fellbaum Ed WordNet An Electronic 66 L Lebart A Salem L Berry Exploring Textual Data Kluwer Academic Publishers 1998 67 O Levy Y Goldberg I Dagan Improving distributional similarity lessons learned word embeddings Trans Assoc Comput Linguist 3 2015 pp 683693 68 J Li D Jurafsky Do multisense embeddings improve natural language understanding Proceedings 2015 Conference Empirical Methods Natural Language Processing EMNLP Lisbon Portugal 2015 pp 17221732 69 D Lin An informationtheoretic deﬁnition similarity Proceedings Fifteenth International Conference Machine Learning San Francisco 2015 211225 CA 1998 pp 296304 70 B Magnini G Cavaglia Integrating subject ﬁeld codes WordNet LREC 2000 pp 14131418 71 B Magnini C Strapparava G Pezzulo A Gliozzo The role domain information word sense disambiguation Nat Lang Eng 8 04 2002 359373 72 SL Manion Sudoku treating word sense disambiguation entity linking deterministic problemvia unsupervised iterative approach 9th International Workshop Semantic Evaluation SemEval 2015 2015 p 365 73 SL Manion R Sainudiin Daebak peripheral diversity multilingual word sense disambiguation Proceedings SemEval 2013 2013 pp 250254 74 M Matuschek I Gurevych DijkstraWSA graphbased approach word sense alignment Trans Assoc Comput Linguist 1 2013 151164 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 63 75 D McCarthy R Navigli The English lexical substitution task Lang Resour Eval 43 2 2009 139159 76 PN Mendes M Jakob A GarcíaSilva C Bizer Dbpedia spotlight shedding light web documents Proceedings 7th International Conference Semantic Systems ACM 2011 pp 18 77 R Mihalcea Using Wikipedia automatic Word Sense Disambiguation Proceedings NAACLHLT07 Rochester NY 2007 pp 196203 78 R Mihalcea A Csomai Wikify Linking documents encyclopedic knowledge Proceedings Sixteenth ACM Conference Information Knowledge Management Lisbon Portugal 2007 pp 233242 79 R Mihalcea D Moldovan An automatic method generating sense tagged corpora Proceedings AAAI 99 Orlando Florida USA 1999 80 R Mihalcea J Wiebe Simcompass deep learning word embeddings assess crosslevel similarity SemEval 2014 2014 p 560 81 T Mikolov K Chen G Corrado J Dean Eﬃcient estimation word representations vector space CoRR arXiv13013781 http pp 461466 arxivorgabs13013781 2013 82 T Mikolov I Sutskever K Chen GS Corrado J Dean Distributed representations words phrases compositionality Advances Neural Information Processing Systems 2013 pp 31113119 83 GA Miller R Beckwith CD Fellbaum D Gross K Miller WordNet online lexical database Int J Lexicogr 3 4 1990 235244 84 GA Miller WG Charles Contextual correlates semantic similarity Lang Cogn Processes 6 1 1991 128 85 GA Miller C Leacock R Tengi R Bunker A semantic concordance Proceedings 3rd DARPA Workshop Human Language Technology Plainsboro NJ 1993 pp 303308 86 D Milne IH Witten An effective lowcost measure semantic relatedness obtained Wikipedia links Proceedings Workshop Wikipedia Artiﬁcial Intelligence An Evolving Synergy AAAI08 Chicago IL 2008 pp 2530 87 D Milne IH Witten Learning link Wikipedia Proc CIKM08 2008 pp 509518 88 A Moro R Navigli Semeval2015 task 13 multilingual allwords sense disambiguation entity linking Proceedings SemEval2015 2015 pp 288297 89 A Moro A Raganato R Navigli Entity linking meets word sense disambiguation uniﬁed approach Trans Assoc Comput Linguist 2 2014 231244 90 J Morris G Hirst Lexical cohesion computed thesaural relations indicator structure text Comput Linguist 17 1 1991 2143 91 N Mrkši c DÓ Séaghdha B Thomson M Gaši c L RojasBarahona PH Su D Vandyke TH Wen S Young Counterﬁtting word vectors linguistic constraints arXiv preprint arXiv160300892 2016 92 R Navigli Meaningful clustering senses helps boost word sense disambiguation performance Proceedings 21st International Conference Computational Linguistics 44th Annual Meeting Association Computational Linguistics 2006 pp 105112 93 R Navigli Word Sense Disambiguation survey ACM Comput Surv 41 2 2009 169 94 R Navigli S Faralli A Soroa O Lacalle E Agirre Two birds stone learning semantic models text categorization Word Sense Disambiguation Proceedings 20th ACM Conference Information Knowledge Management CIKM Glasgow UK 2011 pp 23172320 95 R Navigli D Jurgens D Vannella SemEval2013 task 12 multilingual word sense disambiguation Proceedings SemEval 2013 2013 pp 222231 96 R Navigli M Lapata Graph connectivity measures unsupervised Word Sense Disambiguation Proceedings IJCAI 2007 pp 16831688 97 R Navigli SP Ponzetto BabelNet building large multilingual semantic network Proceedings 48th Annual Meeting Association Computational Linguistics ACL Uppsala Sweden 2010 pp 216225 98 R Navigli SP Ponzetto BabelNet automatic construction evaluation application widecoverage multilingual semantic network Artif 99 A Neelakantan J Shankar A Passos A McCallum Eﬃcient nonparametric estimation multiple embeddings word vector space Pro Intell 193 2012 217250 ceedings EMNLP Doha Qatar 2014 pp 10591069 2007 137163 100 JH Neely DE Keefe KL Ross Semantic priming lexical decision task roles prospective primegenerated expectancies retrospective semantic matching J Exper Psychol Learn Mem Cogn 15 1989 10031019 101 E Niemann I Gurevych The peoples web meets linguistic knowledge automatic sense alignment Wikipedia WordNet Proceedings Ninth International Conference Computational Semantics 2011 pp 205214 102 M Palmer H Dang C Fellbaum Making ﬁnegrained coarsegrained sense distinctions manually automatically Nat Lang Eng 13 2 103 P Pantel D Lin Discovering word senses text Proceedings KDD 2002 pp 613619 104 M Pennacchiotti D De Cao R Basili D Croce M Roth Automatic induction FrameNet lexical units Proceedings Conference Empirical Methods Natural Language Processing EMNLP 08 2008 pp 457465 105 NT Pham A Lazaridou M Baroni A multitask objective inject lexical contrast distributional semantics Proceedings ACL 2015 pp 2126 106 MT Pilehvar D Jurgens R Navigli Align disambiguate walk uniﬁed approach measuring semantic similarity Proceedings ACL 2013 pp 13411351 40 4 2014 837881 pp 8792 2014 2014 pp 532540 107 MT Pilehvar R Navigli A largescale pseudowordbased evaluation framework stateoftheart word sense disambiguation Comput Linguist 108 MT Pilehvar R Navigli A robust approach aligning heterogeneous lexical resources Proceedings ACL 2014 pp 468478 109 MT Pilehvar R Navigli From senses texts allinone graphbased approach measuring semantic similarity Artif Intell 228 2015 95128 110 SP Ponzetto M Strube Knowledge derived Wikipedia computing semantic relatedness J Artif Intell Res 30 2007 181212 111 S Pradhan E Loper D Dligach M Palmer SemEval2007 task17 English lexical sample SRL words Proceedings SemEval 2007 112 T Proisl S Evert P Greiner B Kabashi Semantiklue robust semantic similarity multiple levels maximum weight matching SemEval 113 K Radinsky E Agichtein E Gabrilovich S Markovitch A word time computing word relatedness temporal semantic analysis Proceed ings 20th International Conference World Wide Web WWW 11 2011 pp 337346 114 J Reisinger RJ Mooney Multiprototype vectorspace models word meaning Proceedings ACL 2010 pp 109117 115 P Resnik Using information content evaluate semantic similarity taxonomy Proceedings IJCAI 1995 pp 448453 116 S Rothe H Schütze Autoextend extending word embeddings embeddings synsets lexemes Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing volume 1 Long Papers Association Computational Linguistics Beijing China July 2015 pp 17931803 117 H Rubenstein JB Goodenough Contextual correlates synonymy Commun ACM 8 10 1965 627633 118 P Ruiz T Poibeau El92 entity linking combining open source annotators weighted voting 9th International Workshop Semantic Evaluation SemEval 2015 2015 pp 355359 119 G Salton A Wong CS Yang A vector space model automatic indexing Commun ACM 18 11 1975 613620 64 J CamachoCollados et al Artiﬁcial Intelligence 240 2016 3664 120 H Schütze J Pedersen Information retrieval based word senses Proceedings SDAIR95 Las Vegas Nevada 1995 pp 161175 121 R Schwartz R Reichart A Rappoport Symmetric pattern based word embeddings improved word similarity prediction CoNLL 2015 2015 122 R Sinha R Mihalcea Unsupervised graphbased Word Sense Disambiguation measures word semantic similarity Proceedings ICSC 123 R Snow B OConnor D Jurafsky A Ng Cheap fast good Evaluating nonexpert annotations natural language tasks Proc pp 258267 2007 pp 363369 EMNLP08 2008 pp 254263 124 R Snow S Prakash D Jurafsky AY Ng Learning merge word senses Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning EMNLPCoNLL Prague Czech Republic 2007 pp 10051014 125 A Søgaard Ž Agi c HM Alonso B Plank B Bohnet A Johannsen Inverted indexing crosslingual NLP The 53rd Annual Meeting Asso ciation Computational Linguistics 7th International Joint Conference Asian Federation Natural Language Processing ACLIJCNLP 2015 2015 pp 17131722 126 M Strube SP Ponzetto WikiRelate Computing semantic relatedness Wikipedia Proceedings 21st National Conference Artiﬁcial Intelligence vol 2 AAAI06 Boston Massachusetts 2006 pp 14191424 127 D Tuﬁ s R Ion L Bozianu A Ceau su D Stefanescu Romanian wordnet current state new applications prospects Proceedings 4th Global WordNet Conference GWC 2008 pp 441452 128 PD Turney ML Littman J Bigham V Shnayder Combining independent modules solve multiplechoice synonym analogy problems Proceedings Recent Advances Natural Language Processing Borovets Bulgaria 2003 pp 482489 129 PD Turney P Pantel From frequency meaning vector space models semantics J Artif Intell Res 37 2010 141188 130 A Tversky I Gati Similarity separability triangle inequality Psychol Rev 89 2 1982 123154 131 D Vannella D Jurgens D Scarﬁni D Toscani R Navigli Validating extending semantic knowledge bases video games purpose Proceedings 52nd Annual Meeting Association Computational Linguistics Baltimore USA 2014 pp 12941304 132 F Vasilescu P Langlais G Lapalme Evaluating variants lesk approach disambiguating words LREC 2004 133 JN Venhuizen V Basile K Evang J Bos Gamiﬁcation word sense labeling Proceedings 10th International Conference Computational semantics IWCS 2013 Short Papers 2013 pp 397403 134 D Vickrey L Biewald M Teyssier D Koller Word sense disambiguation machine translation Proceedings Conference Empirical Methods Natural Language Processing Vancouver Canada 2005 pp 771778 135 W Webber A Moffat J Zobel A similarity measure indeﬁnite rankings ACM Trans Inf Syst 28 4 2010 138 136 D Weissenborn L Hennig F Xu H Uszkoreit Multiobjective optimization joint disambiguation nouns named entities Proceed ings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing volume 1 Long Papers Beijing China 2015 pp 596605 137 D Weissenborn F Xu H Uszkoreit Dfki multiobjective optimization joint disambiguation entities nouns deep verb sense disam biguation 9th International Workshop Semantic Evaluation SemEval 2015 2015 pp 335339 138 J Weston A Bordes O Yakhnenko N Usunier Connecting language knowledge bases embedding models relation extraction Pro 139 Z Wu M Palmer Verbs semantics lexical selection Proceedings 32nd Annual Meeting Association Computational Linguistics ceedings EMNLP Seattle Washington USA 2013 pp 13661371 ACL 94 Las Cruces New Mexico 1994 pp 133138 140 J Xu WB Croft Query expansion local global document analysis Proceedings 19th Annual International ACM SIGIR Conference Research Development Information Retrieval SIGIR 96 1996 pp 411 141 E Yeh D Ramage CD Manning E Agirre A Soroa WikiWalk random walks Wikipedia semantic relatedness Proceedings Workshop GraphBased Methods Natural Language Processing 2009 pp 4149 142 Z Zhong HT Ng It makes sense widecoverage Word Sense Disambiguation free text Proceedings 48th Annual Meeting Association Computational Linguistics ACL Uppsala Sweden 2010 pp 7883 143 Z Zhong HT Ng It makes sense widecoverage word sense disambiguation free text Proceedings ACL System Demonstrations 2010 pp 7883