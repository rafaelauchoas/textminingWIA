Artiﬁcial Intelligence 235 2016 6394 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Belief truth hypothesised behaviours Stefano V Albrecht The University Texas Austin United States b Masdar Institute Science Technology United Arab Emirates c The University Edinburgh United Kingdom Jacob W Crandall b Subramanian Ramamoorthy c r t c l e n f o b s t r c t Article history Received 27 July 2015 Received revised form 29 January 2016 Accepted 29 February 2016 Available online 4 March 2016 Keywords Autonomous agents Multiagent systems Game theory Typebased method There long history game theory topic Bayesian rational learning player maintains beliefs set alternative behaviours types players This idea gained increasing artiﬁcial intelligence AI community method control single agent composed multiple agents unknown behaviours The idea hypothesise set types specifying possible behaviour agents plan actions respect types believe likely given observed actions agents The game theory literature studies idea primarily context equilibrium attainment In contrast AI applications focus task completion payoff maximisation With perspective mind identify address spectrum questions pertaining belief truth hypothesised types We formulate basic ways incorporate evidence posterior beliefs resulting beliefs correct fail correct Moreover demonstrate prior beliefs signiﬁcant impact ability maximise payoffs longterm computed automatically consistent performance effects Furthermore analyse conditions able complete task optimally despite inaccuracies hypothesised types Finally correctness hypothesised types ascertained interaction automated statistical analysis 2016 Elsevier BV All rights reserved 1 Introduction There long history game theory topic Bayesian rational learning 73356563 Therein players maintain beliefs behaviours types players form probability distribution set alternative types These beliefs updated based observed actions player chooses action expected maximise payoffs received player given current beliefs player The principal questions studied context degree players learn correct predictions interaction process converges solutions Nash equilibrium 74 This general idea refer typebased method received increasing artiﬁcial intelligence AI community method control single agent composed multiple agents 4115225 This motivated applications require eﬃcient ﬂexible interaction Corresponding author Email address svalbcsutexasedu SV Albrecht httpdxdoiorg101016jartint201602004 00043702 2016 Elsevier BV All rights reserved 64 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 agents behaviours initially unknown Example applications include adaptive user interfaces robotic elderly care automated trading agents Learning interact scratch settings notoriously diﬃcult essentially unconstrained nature agents fact behaviours priori unknown The typebased method seen way reduce complexity problems focusing relatively small set points inﬁnite space possible behaviours More concretely idea hypothesise guess set types speciﬁes possible behaviour agents A type structural form simply view blackbox programme takes input interaction history chooses actions step interaction Such types speciﬁed manually domain expert generated automatically corpus historical data problem description By compar ing predictions types observed actions agents form posterior beliefs relative likelihood types The beliefs types turn utilised planning procedure ﬁnd action maximises expected payoffs respect beliefs A useful feature method fact hypothesise types behaviours gives ﬂexibility interact variety agents Moreover type speciﬁes com plete behaviour plan actions entire interaction space including situations encountered Nonetheless questions concerns associated method pertaining evolution impact beliefs implications detection incorrect hypothesised types Speciﬁcally evidence observed actions incorporated beliefs conditions beliefs correct What impact prior beliefs ability maximise payoffs longterm Furthermore conditions able complete task hypothesised types incorrect And ﬁnally ascertain correctness hypothesised types interaction The AI literature typebased method focused experimental evaluations exploration mechanisms com putational issues arising recursive beliefs partially questions outlined We defer detailed discussion related works Section 2 On hand game theory literature addresses questions primarily context equilibrium attainment repeated games cf Section 2 However reasons renders game theory literature limited applicability domains ones mentioned earlier First equilibrium concepts Nash equilibrium based normative assumptions including perfect rationality spect ones payoffs However normative assumptions diﬃcult justify situations assume prior knowledge behaviour agents For example evidence humans satisfy strict assumptions 64 Second equilibrium solution prescribes behaviours involved agents control single agent assume control choice behaviour agents Finally existence multi ple equilibria possibly differing payoff proﬁles means equilibrium attainment synonymous payoff maximisation controlled agent The purpose present article improve understanding typebased method providing insight questions outlined Our analysis based stochastic Bayesian games extension Bayesian games 56 include stochastic state transitions HarsanyiBellman Ad Hoc Coordination HBA viewed general algorithmic description typebased method 4 After discussing related work Section 2 technical preliminaries Section 3 article makes following contributions Section 4 considers basic methods incorporate observations posterior beliefs analyses conditions converge true distribution types including processes type assignments randomised correlated We discuss examples beliefs fail converge correct distribution Section 5 investigates impact prior beliefs payoff maximisation comprehensive empirical study We prior beliefs signiﬁcant impact longterm performance HBA magnitude impact depends depth planning horizon far look future Moreover automatic methods compute prior beliefs consistent performance effects Section 6 analyses relation hypothesised types true types order HBA able complete task despite inaccuracies hypothesised types We formulate hierarchy increasingly desirable termination guarantees analyse conditions met In particular novel characteri sation optimality based concept probabilistic bisimulation 69 Section 7 shows truth hypothesised types contemplated interaction form automated statistical analysis The presented algorithm incorporate multiple statistical features test statis tic learns distribution interaction process asymptotic correctness guarantees We comprehensive set experiments algorithm achieves high accuracy scalability low computational costs Finally Section 8 concludes work discusses directions future work Elements work appeared 2765 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 65 2 Related work This section discusses related work situates work literature We distinguish research typebased method areas game theory artiﬁcial intelligence 21 Typebased method game theory Perhaps earliest formulation typebased method form Bayesian games 5658 Bayesian games introduced address incomplete information games certain aspects game known players unknown Harsanyi proposed model private information types player number types govern players behaviour1 assignment types governed distribution types By assuming type spaces distribution common knowledge reduces incomplete information game complete imperfect information game admitting solution form Bayesian Nash equilibrium While idea controversial time2 Bayesian games ﬁrm game theory The model work builds Bayesian games includes stochastic state transitions making naturally applicable problems artiﬁcial intelligence This allows deﬁne concisely means complete task drive game initial state terminal state Moreover contrast Bayesian games explicitly consider cases type spaces distribution unknown agent assume agents necessarily use typebased reasoning common prior beliefs Much work game theory focused equilibrium attainment result learning repeated interaction games players maintain Bayesian beliefs strategies players In seminal work Kalai Lehrer 65 authors certain assumption players beliefs called absolute continuity essentially event true positive probability assigned positive probability players belief players prediction future play arbitrarily close true future play A related result shown Jordan 63 myopic players consider immediate payoffs In Section 4 convergence result Kalai Lehrer 65 carries model provide convergence results different formulations posterior beliefs recognise randomised correlated type assignments In addition posterior beliefs shown prior beliefs intimately connected equilibrium solution emerges result learning For example Nyarko 76 use similar weaker condition absolute continuity resulting subjective equilibrium Nash equilibrium players different prior beliefs Similarly Dekel et al 35 certain conditions learning common prior beliefs converge selfconﬁrming equilibrium 48 Nash equilibrium While important context equilibrium attainment results applicable focus individual payoff maximisation task completion cf Section 1 In Section 5 prior beliefs signiﬁcant impact ability maximise payoffs longterm Moreover results indicate prior beliefs computed automatically consistent performance effects The possibility discrepancies predicted true behaviour recognised works 734672 Essentially works certain games conditions players maintaining beliefs behaviours si multaneously correct predictions play optimally respect beliefs In Section 6 consider impact incorrect hypothesised types ability complete tasks certain form optimality preserved der bisimulation relation veriﬁed practice Furthermore Section 7 automatic statistical analysis allow agent contemplate correctness behavioural hypotheses 22 Typebased method artiﬁcial intelligence There substantial body work AI literature coordination 668653 learning 28612070 multiagent systems However noted 83 methods depend form prior coordination agents The typebased method studied alternative method interaction agents behaviours initially unknown Barrett et al 11 implement variant typebased method pursuit gridworld domain demonstrate practical potential Albrecht Ramamoorthy 4 introduce general algorithm called HarsanyiBellman Ad Hoc Coordi nation HBA cf Section 3 evaluate levelbased foraging gridworld domain matrix games played humans Both works propose implementations typebased method including tree expansion dynamic programming reinforcement learning stochastic sampling Carmel Markovitch 25 deﬁne types deterministic ﬁnite state machines study optimal exploration repeated games Similarly Chalkiadakis Boutilier 26 use types context multiagent reinforcement learning develop 1 The interpretation types behaviours consistent original deﬁnition Harsanyi deﬁnes types parameters payoff strategy functions cf Section 7 56 See Dekel et al 35 2 The controversy centred assumption players priori know true distribution types From personal conversation Reinhard Selten 66 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 exploration methods based concept value information 60 Their work essentially extension Dearden et al 34 study related idea maintaining Bayesian beliefs set environment models reinforcement learning Southey et al 82 apply typebased method variants poker game The poker domain differs works state interaction process player hands partially observable The authors beliefs maintained setting compare methods compute optimal responses respect beliefs In interactive partially observable Markov decision processes IPOMDPs 52 agents decisions presence uncertainty state environment types agents action choices Several solution methods developed IPOMDPs 413839 attempts apply IPOMDPs practice 4075 An interesting parallel work convergence result Kalai Lehrer 65 extended IPOMDPs 37 Bowling McCracken 19 use play books control single agent team agents Plays similar types specify behaviours complete team include additional structure applicability termination conditions roles agent Similarly plan libraries infer agents goals 2427 Plans resemble types include intricate structure temporal causal orderings grammars 8450 The works investigate aspects typebased method partially address questions outlined Section 1 Speciﬁcally works use posterior formulation likelihood deﬁned product action probabilities In Section 4 conditions formulation produce correct incorrect beliefs investigate alternative posterior formulations Moreover Chalkiadakis Boutilier 26 consider effects prior beliefs comparing uninformed uniform informed uniform narrowed support prior beliefs provide detailed analysis In Section 5 investigate prior beliefs affect ability maximise payoffs longterm computed automatically Finally works consider implications detection incorrect hypothesised types 3 Model algorithm This section introduces general model algorithm work elaborates connections related works 31 Stochastic Bayesian game We model interaction process stochastic Bayesian game SBG 4 viewed combination Bayesian game 56 stochastic game 81 This combination useful allows study type based method interaction established framework Bayesian games providing means specify environment states task completed The structural deﬁnition SBGs follows Deﬁnition 1 A stochastic Bayesian game SBG consists ﬁnite state space S initial state s0 S terminal states S S players N 1 n N ﬁnite set actions Ai A A1 An inﬁnite type space cid2i cid2 cid21 cid2n payoff function ui S A cid2i R strategy function πi H Ai cid2i 0 1 state transition function T S A S 0 1 type distribution ϒ cid2 0 1 cid2 ﬁnite subset cid2 H denotes set histories H t cid6s0 a0 s1 a1 stcid7 t 0 s0 st S a0 at1 A A SBG deﬁnes interaction process follows Deﬁnition 2 A SBG starts time t 0 state s0 1 In state st types θ t 1 θ t type θ t n sampled cid2 probability ϒθ t 1 θ t n player informed SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 67 2 Based history Ht player chooses action action 1 n Ai probability πiHt θ t resulting joint 3 Each player receives individual payoff given uist θ t game transitions successor state st1 S probability T st st1 This process repeated terminal state st S reached game stops Throughout work use contextual notation H τ denote τ preﬁx Ht H τ initial segment Ht state sτ τ t Similarly use sτ aτ denote respective τ elements Ht The set S specify environment players interact state s S speciﬁc conﬁguration environment For instance environment maze twodimensional grid states specify positions players walls The task SBG drive interaction process initial state terminal state Once terminal state reached task completed The type space cid2i contains possible behaviours player Each type θi cid2i corresponds complete behaviour player specifying preferences ui way chooses actions πi Footnote 1 We place restrictions behaviours players exhibit particular player decisions based entire history Ht This includes behaviours learn change time In practice useful view type blackbox programme πi takes input current interaction history returns probabilities action available player Sections 4 5 provide examples types 4 examples types complex SBGs The types assigned game type distribution ϒ In work consider classes types distributions Deﬁnition 3 A type distribution ϒ called pure θ cid2 ϒθ 1 A type distribution pure called mixed Pure type distributions specify ﬁxed type player game This normally expect means player single coherent behaviour However cases sense assume mixed type distribution For example Albrecht Ramamoorthy 4 mixed type distribution humanmachine experiments allow possibility human subjects change simple types opposed deﬁning complex type includes simple types Note type space cid2i uncountable strategy πi assigns probabilities actions interval 0 1 uncountable Therefore order ϒ welldeﬁned probability distribution deﬁne ﬁnite countable subset cid2 cid2 Otherwise ϒ need deﬁned density To differentiate spaces true types player For convenience abuse refer cid2i type space cid2 notation allow ϒθ θ cid2 ϒθ 0 θ cid2 32 HarsanyiBellman Ad Hoc Coordination As outlined Section 1 consider single agent employs typebased method interact agents unknown behaviours Throughout work use HarsanyiBellman Ad Hoc Coordination HBA 4 general algorithmic description typebased method Algorithm 1 provides formal deﬁnition HBA Given SBG cid7 use denote player j denote players Ai jcid11i A j The θ HBA ui πi compactness The behaviour players j behaviour player completely speciﬁed HBA In words single ﬁxed type cid2 deﬁned Algorithm 1 Thus omit θ HBA governed ϒ priori unknown Formally assume elements cid7 known cid2 ϒ latent elements In HBA latent elements essentially substituted hypothesised type space cid2 j posterior j ﬁnite countable subset type space cid2 j The posterior belief probability Ht quantiﬁes relative likelihood players j cid11 types θ n given lief Pr respectively Like cid2 Prθ tory H t If assume independence types deﬁne Pr i1 θ 1 θ i1 θ j cid2 θ θ HBA cid2 cid3 Prθ Ht cid4 jcid11i Pr jθ j Ht Pr jθ j Ht cid5 LHtθ j P jθ j j P j ˆθ j LHt ˆθ ˆθ j cid2 j 1 2 68 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Algorithm 1 HarsanyiBellman Ad Hoc Coordination HBA Input current history Ht cid6s0 a0 s1 a1 st cid7 Output action probabilities πi Ht ai player Parameters hypothesised type spaces cid2 j players j cid11 discount factor γ 0 1 1 For ai Ai compute expected payoff Eai st Ht cid4 ai ai s ˆH s ˆH Eai cid6 θ cid2 Prθ ˆH cid6 Q ai Ai s ˆH Q cid6 scid12S cid7 T s s cid12 ui s γ max ai Ai Eai scid12 π j ˆH j θ j jcid11i cid8 cid6 ˆH s cid9cid10 cid12cid7 3 4 2 Distribute πi Ht uniformly arg maxai Ai Eai st Ht j prior belief probability player j type θ j actions observed LH t θ P jθ j It convenient deﬁne Pr jθ nonnegative likelihood history H t assuming player j type θ j Note likelihood L 2 unspeciﬁed point consider variants L Section 4 j j H 0 P jθ The independence assumption types prevalent works discussed Section 2 In game theory literature cf Section 21 justiﬁed fact Nash equilibrium assumes players choose actions independently This opposed concepts correlated equilibrium 8 action choices correlated From practical perspective justiﬁcation fact types assumed independent behaviours encode depend behaviour players This player decisions based entire interaction history includes observed actions players Nonetheless Section 4 discusses possibility correlated types Where hypothesised types θ j j come In work assume user means generate hypotheses One way speciﬁed manually domain experts based experience problem 4 Another method generate types automatically problem description For example Sections 5 7 use different methods automatically generate sets types given matrix game Finally use machine learning methods extract types corpus historical data 1249 cid2 HBA performs planning procedure deﬁned 34 ﬁnd action maximises expected longterm payoff respect current beliefs hypothesised types Formally 3 corresponds player component Bayesian Nash equilibrium 57 4 corresponds Bellman optimality equation 15 Intuitively 34 expand tree possible future trajectories interaction process weight trajectory based posterior beliefs predicted action probabilities hypothesised types Note H t denotes current history ˆH construct future trajectories histories notation cid6 ˆH s cid12cid7 4 denotes concatenation ˆH s cid12 In practice HBA implemented limiting recursion 34 ﬁxed depth Section 5 However easy procedure time complexity exponential factors number players actions states game This costly operation usually requires sophisticated approximate methods applied complex domains In regard promising approach given stochastic sampling methods 411 In work stated assume 34 implemented given st Ht exploration implicit calculation Eai It worth noting HBA require explicit exploration methods deliberately choosing actions maximise Eai st Ht predicts impact ai HBAs beliefs future interaction This allows HBA reason beneﬁt choosing particular action sense information action potentially reveal HBA 60 Of course assumes true types players included hypothesised types Nonetheless predictive ability HBA limited ﬁxed recursion depth cf Section 5 use opponent modelling learn new types interaction 411 worthwhile use explicit exploration methods discussed 25 approximations 26 st Ht Speciﬁcally action ai Eai 33 Relation interactive decision models Section 2 provided overview related works models Here elaborate connections differences models models As pointed earlier SBG model viewed combination Bayesian games stochastic games If remove states deﬁnition SBGs equivalently assume single state terminal states reduces standard Bayesian game In case cid2 j corresponds type spaces Bayesian games ϒ corresponds basic probability distribution 56 However note contrast Bayesian games assume knowledge cid2 j ϒ On hand remove types deﬁnition SBGs model reduces standard stochastic SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 69 game However note Shapley 81 considers Markovian stationary strategies allow strategies depend entire interaction history This deﬁnition strategies consistent model Kalai Lehrer 65 strategies mappings histories probability distributions actions A central assumption SBGs states chosen actions fully observable players This contrast IPOMDPs cf Section 2 states actions directly observed Instead players receive noisy possibly incomplete signals depend state based players infer beliefs states This makes IPOMDPs general model increases computational complexity signiﬁcantly Another difference SBGs allow mixed type distributions IPOMDPs generally assume ﬁxed types These differences mean results work directly carry IPOMDPs Other models interactive decision making exist decentralised POMDP 18 partially observable stochastic game 42 Both models allow partial observability process states described While models explicitly encode types possible emulate types factored states composed individual elements3 Essentially deﬁne factored state space ˆS S cid2 n Selement 1 observed players controlled joint actions cid2 elements privately observed players controlled type distribution An interesting question extent solving model produce similar better solution HBA However discuss leads crucial difference work works cid2 Once model fully speciﬁed usual goal solve procedure In context game theory solution proﬁle strategies satisfy equilibrium property 4329 In context artiﬁcial intelligence solution control policy agents satisﬁes certain guarantees payoff maximisation 364154 This contrast work attempt solve SBGs sense Instead prescribe speciﬁc normative solution single agent form HBA This similar spirit works 65 consider single agent uses HBA The advantage approach HBA applied instantly need solve model This means HBA applied problems complex solved conventional sense Of course disadvantage exactly know HBA perform purpose present work precisely provide answers question 4 Correctness posterior beliefs A central aspect typebased method beliefs types Beginning initial beliefs relative likelihood types compare predictions types observed actions update beliefs reﬂect given evidence Associated process key questions evidence incorporated beliefs conditions beliefs correct As seen Algorithm 1 important questions accuracy expected payoffs 3 depends accuracy posterior belief Pr In section consider classes type distributions cover broad spectrum scenarios pure distribu tions agents ﬁxed type mixed distributions types randomly reallocated correlated distributions type assignments correlated Corresponding classes consider formulations posterior beliefs prescribe different ways incorporate evidence beliefs We provide theoretical conditions formulations produce correct beliefs provide examples fail Our deﬁnition correctness respect type distribution ϒ beliefs said correct assign probabilities true types ϒ This requires beliefs point types support type distribution Therefore results section pertain situation user knows true type space cid2 j subset hypothesised type space cid2 j Formally assume Assumption 1 j cid11 cid2 j cid2 j The case beliefs correct deﬁned incomplete incorrect hypothesised types examined Sections 6 7 Finally recall Section 32 posterior beliefs form 1 assume independence player types That assume type distribution ϒ represented product n independent factors ϒ j player ϒθ j ϒ jθ j Hence following states assume ϒ satisﬁes independence property Section 43 considers case correlated type distributions cid11 41 Product posterior We begin analysis product posterior 3 We thank anonymous reviewer suggesting line thought 70 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Deﬁnition 4 The product posterior deﬁned 1 LHtθ j t1cid4 τ 0 π jH τ aτ j θ j 5 This standard posterior formulation Bayesian games works discussed Section 2 It j respectively true probability based ϒ probability assigned HBA based Pr H τ shown pure type distribution HBA priori rule types cid2 learn correct future predictions Let H P PrH τ H continue prescribed H inﬁnite history preﬁx H τ denote P ϒ H τ H Theorem 1 Let cid7 SBG pure type distribution ϒ If HBA uses product posterior prior beliefs P j positive θ j 0 j cid9 0 time t τ t P jθ cid2 j P PrH τ H 1 cid9 P ϒ H τ H 0 H P ϒ H τ H P PrH τ H 1 cid9 6 Proof The proof diﬃcult tedious defer Appendix A Proof sketch Kalai Lehrer 65 studied model equivalently described singlestate SBG S 1 pure type distribution ϒ proved Theorem 1 model Their convergence result extended multistate SBGs translating multistate SBG cid7 singlestate SBG ˆcid7 equivalent cid7 sense players behave identically Essentially trick remove states cid7 introducing new player action choices correspond state transitions cid7 cid2 Theorem 1 states HBA eventually correct future predictions product posterior pure type distribution assuming prior beliefs positive However subtle important asymmetry making correct future predictions knowing true type distribution implies reverse generally true The following example4 illustrates Example 1 Consider SBG players actions C D Player 1 controlled HBA product θλ01 θλ05 assigned pure type distribution The posterior player 2 types cid2 types choose action C player 1 chose C previous round Otherwise probability λ forever play action D In case HBA know correct type absolute certainty Even HBA chooses D player 2 responds playing D indeﬁnitely certainty λ 0 types 2 Therefore HBA guaranteed correct future predictions time guaranteed learn type distribution game Finally note Theorem 1 pertains pure type distributions The following example shows product posterior fail SBGs mixed type distributions Example 2 Consider SBG players Player 1 controlled HBA product posterior player 2 θ A θB assigned mixed type distribution ϒ ϒθ A ϒθB 05 The type θ A types cid2 2 chooses action A θB chooses action B In case time t types assigned actions A B played player 2 This means time t subsequent times τ t Pr2θ AH τ Pr2θB H τ 0 Pr2 undeﬁned HBA fail correct future predictions 42 Sum posterior We continue analysis sum posterior Deﬁnition 5 The sum posterior deﬁned 1 LHtθ j t1cid6 τ 0 π jH τ aτ j θ j 4 All examples section assume cid2 j cid2 j uniform prior beliefs P j θ j cid2 j 7 1 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 71 The sum posterior allows HBA recognise changing types In words purpose sum posterior learn mixed type distributions It easy sum posterior learn mixed type distribution Example 2 However example additional requirements sum posterior necessarily learn pure mixed type distribution Example 3 Consider SBG players Player 1 controlled HBA sum posterior player 2 θ A θAB assigned pure type distribution ϒ ϒθ A 1 The type θ A chooses types cid2 2 action A θAB chooses actions A B equal probability While product posterior converges correct probabilities ϒ sum posterior converges probabilities cid6 2 cid7 incorrect 3 1 3 Note example readily modiﬁed use mixed type distribution similar results Therefore conclude assumptions sum posterior necessarily learn type distribution Under condition sum posterior guaranteed learn true type distribution game Consider following quantities computed given history H t Deﬁnition 6 The average overlap player j H t deﬁned AO jHt 1 t cid14 t1cid6 cid12 τ 0 cid11τ j 2 cid13 cid6 1 θ j cid2 j cid15 cid11τ j θ j cid2 j π jH τ aτ j θ j 0 π jH τ aτ j θ j cid2 j 1 b1 1 b true 0 Deﬁnition 7 The average stochasticity player j H t deﬁned AS jHt 1 t t1cid6 τ 0 cid2 j cid6 1 θ j cid2 j 1 π jH τ ˆaτ 1 A j1 j θ j ˆaτ j arg maxa j π jH τ j θ j 8 9 10 Both quantities bounded 0 1 The average overlap describes similarity types AO jHt 0 means player js types average chose action history H t AO jHt 1 means behaved identically The average stochasticity describes uncertainty types AS jHt 0 means player js types average fully deterministic action choices history H t AS jHt 1 means chose actions uniformly randomly Example 4 Consider SBG Example 3 Here player 2 chooses action A type θ A Therefore history Ht AO2Ht 075 indicates substantial overlap θ A θAB Further AS2Ht 05 indicates certain degree randomisation In fact θ A fully deterministic θAB uniformly random average stochasticity centre spectrum 0 1 It shown average overlap stochasticity player j converge zero t sum posterior guaranteed converge pure mixed type distribution Theorem 2 Let cid7 SBG pure mixed type distribution ϒ If HBA uses sum posterior t If AO jHt 0 AS jHt 0 players j cid11 PrθiHt ϒθi θi cid2 Proof Throughout proof let t The sum posterior deﬁned 1 L deﬁned 7 Given deﬁnition L numerator denominator 2 inﬁnite We invoke LHôpitals rule states cid12t cases quotient ut vcid12t respective derivatives respect t The derivative L respect t average growth time step general depend history H t states actions The average growth L cid6 vt equal quotient u 11 cid12 Htθ j F jHt π jHt j θ j j A j L 72 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Fig 1 Example run random SBG 2 players 10 actions 100 states Player j reinforcement learning types cid9greedy action selection Pr j θ j Ht ϒθ j Pr j sum decreasing linearly cid9 07 t 1000 cid9 0 t 2000 The error time t computed posterior θ j cid2 cid5 j F jHt cid6 θ j cid2 j ϒθ j π jHt j θ j 12 probability action j history Ht ϒθ j marginal probability player j assigned type θ j As shortly asymptotic growth prediction irrespective H t Given AO jHt 0 infer π jHt j θ j ˆθ j 0 action j type θ j 0 types ˆθ j π jHt j ˆθ cid2 cid11 θ j j j Therefore write 11 cid12 cid6 L Htθ j ϒθ j π jHt j θ j2 13 j A j Next given AS jHt 0 know exists action j 13 π jHt j θ j 1 conclude L irrelevant asymptotic growth rate L Finally cid5 ϒθ j 1 know denominator 2 1 conclude Pr jθ jHt ϒθ j cid2 cid12Htθ j ϒθ j This shows history H t θ j cid2 j Theorem 2 explains sum posterior converges correct type distribution Example 2 Since types θ A θB choose different actions completely deterministic average overlap stochasticity zero sum posterior guaranteed converge type distribution On hand Example 3 types θ A θAB produce overlap action A chosen θAB completely random Therefore average overlap stochasticity positive incorrect type distribution learned The assumptions Theorem 2 average overlap stochasticity converge zero require practi cal justiﬁcation First important note required converge zero average t This means beginning arbitrary overlap stochasticity long zero game proceeds In fact respect stochasticity precisely explorationexploitation dilemma 85 solved practice In early stages agent randomises deliberately actions order obtain information environment exploration game proceeds agent gradually deterministic action choices maximise payoffs exploitation Typical mechanisms implement cid9greedy SoftmaxBoltzmann exploration 85 Fig 1 demonstrates SBG player j reinforcement learning types The payoffs types average overlap eventually zero Regarding average overlap converging zero believe property guaranteed design following reason If hypothesised type space cid2 j constantlyhigh average overlap means types cid2 j effect similar However types similar likely produce similar trajectories planning step HBA cf ˆH 34 constitute redundancy time space Thus believe advisable use type spaces low average overlap 43 Correlated posterior As noted earlier implicit assumption 1 type distribution ϒ represented product n cid11 independent factors player ϒθ j ϒ jθ j Therefore sum posterior form 1 fact guaranteed learn independent type distributions This opposed correlated type distributions represented product n independent factors Correlated type distributions specify constraints type combinations player j type θ j player k type θk The following example shows sum posterior fail converge correlated type distribution Example 5 Consider SBG 3 players Player 1 controlled HBA sum posterior Players 2 3 θ A θB deﬁned Example 2 The type distribution ϒ chooses types probabilities types cid2 2 cid2 3 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 73 ϒθ A θB ϒθB θ A 05 ϒθ A θ A ϒθB θB 0 In words player 2 type player 3 From perspective HBA type action chosen equal probability players Thus despite fact zero overlap stochasticity sum posterior eventually assign probability 025 constellations types incorrect This means HBA fails recognise players choose action We propose posterior formulation learn correlated type distribution Deﬁnition 8 The correlated posterior deﬁned Prθ Ht η P θ t1cid6 cid4 τ 0 θ j θ π jH τ aτ j θ j P speciﬁes prior beliefs cid2 analogous P j η normaliser 14 The correlated posterior closely related sum posterior In fact converges correct type distribution conditions sum posterior Theorem 3 Let cid7 SBG correlated type distribution ϒ If HBA uses correlated posterior t If AO jHt 0 AS jHt 0 players j cid11 PrθiHt ϒθi θi cid2 Proof The proof analogous proof Theorem 2 cid2 It easy correlated posterior learn correct type distribution Example 5 Note guaranteed learn correlated type distribution guaranteed learn independent type distribu tion Therefore correlated posterior learn correct type distribution Example 2 This means correlated posterior complete sense covers entire spectrum puremixed independentcorrelated type distributions However completeness comes higher computational complexity While sum posterior O n max j cid2 n time space In practice time j j θ complexity reduced substantially computing probabilities π j H τ aτ j sum posterior reusing subsequent computations time space correlated posterior O max j cid2 j θ cid2 j j j 5 Practical impact prior beliefs The previous section concerned evolution posterior beliefs observe evidence However observe evidence based form posterior beliefs initial judgement relative likelihood types This initial judgement called prior belief Given lack evidence tempting use uniform prior beliefs types equal probability Indeed fact beliefs change rapidly observations suggests prior beliefs negligible effect On hand substantial body work game theory literature arguing importance prior beliefs cf Section 2 However works consider impact prior beliefs equilibrium attainment players use typebased reasoning In contrast practical impact prior beliefs payoff maximisation single agent typebased method In addition work Bernardo 17 Jaynes 62 uninformed priors The purpose priors express state complete uncertainty whilst possibly incorporating subjective prior information What means possible subject long debate De Finetti 33 However differs impact prior beliefs payoff maximisation Thus left following questions Do prior beliefs impact ability maximise payoffs longterm If And crucially automatically compute prior beliefs improve longterm performance To ﬁnd answers questions conducted comprehensive empirical study compared 10 methods au tomatically compute prior beliefs given set types The results prior beliefs signiﬁcant impact longterm performance depth planning horizon far look future plays central role Finally intriguingly automatic methods compute prior beliefs consistent performance effects variety scenarios An implication prior beliefs eliminated manual parameter instead computed automatically The following subsections experimental setup study The results discussed Section 57 74 51 Games SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 We comprehensive set benchmark games introduced Rapoport Guyer 78 consists 78 repeated 2 2 matrix games The games strictly ordinal meaning player ranks possible outcomes 1 preferred 4 preferred outcomes rank Furthermore games distinct game obtained transformation game includes interchanging rows columns players combination thereof payoff matrix game The games grouped 21 noconﬂict games 57 conﬂict games In noconﬂict game players preferred outcome relatively easy arrive solution best players In conﬂict game players disagree best outcome ﬁnd form compromise We note games benchmark correspond SBGs single states The simplicity games facilitates thorough inspection interaction process explanation observations It allows specify complete benchmark set sense contains games satisfy description turn allows draw general conclusions Finally fact use singlestate games limit inherent complexity interaction multistate SBGs emulated singlestate SBGs additional nature player cf Appendix A Therefore expect principal observations hold multistate SBGs 52 Performance criteria Each play game partitioned time slices consist equal number consecutive time steps For time slice measured following performance criteria Convergence An agent converged time slice action probabilities time slice deviate 005 initial action probabilities time slice Returns 1 true 0 false agent Average payoff Average payoffs agent received time slice Returns value 1 4 agent Welfare fairness Average sum product respectively joint payoffs received time slice Returns values 2 8 1 16 respectively Game solutions librium Pareto optimum Welfare optimum Fairness optimum Returns 1 true 0 false game solution Tests averaged action probabilities time slice formed approximate stagegame Nash equi Precise formal deﬁnitions performance criteria 3 53 Algorithm We HBA control player 1 ﬁxed type play control player 2 included set hypothesised types cid2 2 provided HBA discussed Section 56 Therefore product posterior formulation cf Section 41 update HBAs beliefs The planning step HBA implemented expanding ﬁnite tree future trajectories Formally HBA chooses action ai maximises expected payoff Eai h Ht deﬁned h ˆH Eai cid6 θ j cid2 j Pr jθ ˆH j cid6 j A j π j ˆH j θ j Q ai j h1 ˆH Q ai j h ˆH uiai j 0 h 0 cid12 h maxa cid8 E cid12 cid6 ˆH ai jcid7 cid9 15 16 h speciﬁes depth planning horizon HBA predicts h actions player j Note 15 16 correspond closely 3 4 respectively The difference 1516 use h specify planning depth 34 use discount factor γ Hence deeper planning horizon h translates greater discount factor γ All results reported section hold variants 54 Types We different methods automatically generate parameterised sets types cid2 j given game The generated types cover broad spectrum adaptive behaviours including deterministic CDT randomised CNN hybrid LFT policies Algorithmic details parameter settings Appendix B 1 LeaderFollowerTrigger Agents LFT Crandall 31 described method generate sets leader follower agents seek play speciﬁc sequences joint actions called target solutions A leader agent plays target solution long player If player deviates leader agent punishes player playing minimax strategy The follower agent similar punish Rather player deviates SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 75 follower agent randomly resets position target solution continues play usual We augmented set trigger agent similar leader follower agents plays maximin strategy indeﬁnitely player deviates CoEvolved Decision Trees CDT We genetic programming 67 automatically breed sets decision trees A decision tree takes input past n actions player case n 3 deterministically returns action played response The breeding process coevolutional meaning pools trees bred concurrently player In evolution random selection trees player 1 evaluated random selection trees player 2 The ﬁtness criterion includes payoffs generated tree dissimilarity trees pool This encourage diverse breeding trees trees tend similar identical CoEvolved Neural Networks CNN We stringbased genetic algorithm 59 breed sets artiﬁcial neural net works The process basically decision trees However difference artiﬁcial neural networks learn play stochastic strategies decision trees play deterministic strategies Our networks sist input layer 4 nodes previous actions players hidden layer 5 nodes output layer 1 node The node output layer speciﬁes probability choosing action 1 play 2 2 games action 2 All nodes use sigmoidal threshold function fully connected nodes layer 55 Prior beliefs We speciﬁed total 10 different methods automatically compute prior beliefs P j given set types cid2 j 1 θ j j This baseline prior The uniform prior sets P jθ j cid2 cid2 j Uniform prior priors compared The random prior speciﬁes P jθ Random prior j The remaining probability mass uniformly spread half The random prior check performance differences priors purely fact concentrate probability mass fewer types j 00001 random half types cid2 kθ Let U t Value priors player type θ j η ψθ P jθ types θ j expected cumulative payoff player k start time t player j j player HBA plays optimally Each value prior general form j b η normalisation constant b booster exponent magnify differences j Based general form deﬁne different value priors Utility prior ψU θ j U t Stackelberg prior ψS θ Welfare prior ψW θ Fairness prior ψF θ θ j j U t θ θ j U t j U t jθ j j U t j U t jθ j jθ j Our choice value priors motivated variety metrics cover As result priors produce substantially different probabilities set types In study set t 5 b 10 LPpriors based idea optimal priors formulated solution mathematical opti LPpriors misation problem case linear program Each LPprior generates quadratic matrix A element A j jcid12 jcid12 true type player j θ contains loss HBA incur planned actions type θ j kθ Formally let U t j We deﬁne j different LPpriors j HBA believes player j type θ jcid12 instead θ jcid12 like U t kθ θ j LPUtility A j jcid12 ψU θ j U t LPStackelberg A j jcid12 ψS θ LPWelfare A j jcid12 ψW θ LPFairness A j jcid12 ψF θ θ j U t θ θ j U t j U t θ jcid12 jθ j θ θ j j θ jcid12 jcid12 U t jcid12 U t jθ jθ j θ jcid12 jcid12 j θ The matrix A fed linear program form minc c Tx st z Ax 0 n cid2 c 1 0nT z 1nT ﬁnd vector x l p1 pn l minimised expected loss HBA probabilities j 76 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Fig 2 Prior beliefs signiﬁcant impact longterm performance Plots average payoffs player 1 HBA XhYZ format HBA X types horizon h player 2 controlled Y results averaged Z games p1 pn type prior belief P j In order avoid premature elimination types furthermore require p v 0 1 v n As set t 5 b 10 While mathematically rigorous formulation important note simpliﬁcation HBA ally works HBA incorporates beliefs recursion planning procedure LP formulation implicitly assumes HBA uses prior beliefs randomly sample types plans optimally Nonethe reasonable approximation 56 Experimental procedure We performed identical experiments type generation method described Section 54 Each 78 games played 10 times different random seeds play repeated opponents 30 plays total RT A randomly generated type control player 2 play lasted 100 rounds FP A ﬁctitious player 23 control player 2 play lasted 10 000 rounds CFP A conditioned ﬁctitious player learns action distributions conditioned previous joint action control player 2 play lasted 10 000 rounds In play randomly generated 9 unique types provided HBA true type player 2 cid2 10 That play pure type distribution cf Section 31 Thus true type player 2 2 included set hypothesised types cid2 2 To avoid endgame effects players unaware number rounds We included FP CFP try learn behaviour HBA While generated types adaptive create models HBAs behaviour To facilitate learning allowed 10 000 rounds Finally FP CFP choose dominating actions exist case interaction ﬁltered games FP CFP plays dominating action player 2 leaving 15 noconﬂict 33 conﬂict games CFP plays 57 Results We report main observations Observation 1 Prior beliefs signiﬁcant impact longterm performance HBA This observed classes types classes opponents classes games study Fig 2 provides representative examples range scenarios Many relative differences prior beliefs statistically signiﬁcant based paired twosided ttests 5 signiﬁcance level Our data explain follows Different prior beliefs cause HBA different actions beginning game These actions shape beliefs player models adapts HBAs actions turn affect HBAs actions Thus different prior beliefs lead different initial actions lead different play trajectories different payoffs Given time HBA know true type player 2 provided HBA surprising process lead differences longterm In fact experiments HBA learned true type 3 5 rounds cases 20 rounds After point planning horizon HBA suﬃciently deep realise initial actions suboptimal manipulate play trajectory achieve higher payoffs longterm diminishing impact prior beliefs However deep planning horizons problematic practice time complexity HBA exponential depth planning horizon Therefore planning horizon constitutes tradeoff decision quality computational tractability Interestingly data increase depth stay suﬃcient depth suﬃcient described amplify impact prior beliefs SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 77 Fig 3 Deeper planning horizons diminish impact prior beliefs Results shown HBA LFT types player 2 controlled FP averaged noconﬂict games h depth planning horizon predicting h actions player 2 Fig 4 Deeper planning horizons amplify impact prior beliefs Results shown HBA CNN types player 2 controlled RT averaged conﬂict games h depth planning horizon predicting h actions player 2 Observation 2 Deeper planning horizons diminish amplify impact prior beliefs Again observed tested scenarios Figs 3 4 examples deeper planning horizons diminish amplify impact prior beliefs respectively How deeper planning horizons amplify impact prior beliefs Our data different prior beliefs cause HBA different initial actions depends prior beliefs types depth planning horizon In cases differences types action choices visible near future visible distant future In cases HBA agent myopic planning horizon choose similar initial actions despite different prior beliefs differences types visible planning horizon On hand HBA agent deeper planning horizon differences types decide choose different initial actions based prior beliefs We turn comparison different prior beliefs Here data reveal intriguing property Observation 3 Automatic methods compute prior beliefs consistent performance effects Fig 5 shows prior beliefs consistent performance effects wide variety scenarios For example Utility prior produced consistently higher payoffs player 1 HBA Stackelberg prior produced consistently higher payoffs player 2 higher welfare fairness The Welfare Fairness priors similar Stackelberg prior consistent Similar results observed LP variants priors despite fact LP formulation simpliﬁcation HBA works cf Section 55 We note prior beliefs including Uniform prior produced high rates game solutions Nash equilibrium Pareto optimality This measured stagegame solutions notion time These hard attain repeated games especially player actively seek speciﬁc solution case study Observation 3 intriguing indicates prior beliefs eliminated manual parameter instead computed automatically methods ones speciﬁed Section 55 The fact methods produced consistent results means prior beliefs constructed optimise speciﬁc performance criteria Note result particularly interesting prior beliefs inﬂuence whatsoever true type player 2 78 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Fig 5 Automatic prior beliefs consistent performance effects Rows prior beliefs columns performance criteria Each element r c matrix corresponds percentage time slices prior belief r produced signiﬁcantly higher values criterion c Uniform prior averaged plays tested games All signiﬁcance statements based paired rightsided ttests 5 signiﬁcance level See Figure 2 XhYZ format This observation supported fact Random prior produce consistently different values criterion Uniform prior This means differences prior beliefs merely fact concentrate probability mass fewer types prior beliefs reﬂect intrinsic metrics based computed player 1 payoffs Utility prior player 2 payoffs Stackelberg prior How phenomenon explained We believe interesting analogy optimism uncertainty principle 22 The optimism lies fact HBA commits speciﬁc class types high prior belief truth evidence reason believe type priori likely Each class types characterised intrinsic metric prior belief For instance Utility prior assigns high probability types yield high payoffs HBA played optimally types By committing characterisation HBA effectively utilise Observation 1 choosing initial actions shape interaction maximise intrinsic metric If true type player 2 class types interaction proceed planned HBA intrinsic metric optimised However true type class HBA quickly learn correct type adjust play accordingly albeit necessarily maximising intrinsic metric This contrast Uniform Random priors intrinsic metric Under priors HBA plan actions respect types characterised common theme types Uniform prior random half Random prior Therefore HBA effectively utilise Observation 1 6 Optimal type spaces A potential concern typebased method fact hypothesised types incorrect This range slight deviations predicted action probabilities predicting entirely different actions observed The following example illustrates Example 6 Consider SBG players actions L R Player 1 controlled HBA player 2 single type θLR chooses LRLR HBA provided hypothesised types cid2 R chooses R θ R θ θ LRR cid2 cid3 j SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 79 θ 50 game LRR chooses LRRLRR Both hypothesised types incorrect sense predict player 2s actions Such inaccuracies signiﬁcant impact choice actions hypothesised types incorrect predictions future interactions incorrect turn lead suboptimal action choices Therefore important question relation hypothesised types true types order HBA able complete task In particular mean hypothesised types optimal Given complexity behaviours agents exhibit extremely diﬃcult question In addition generally suﬃcient consider types actions planned respect types beliefs types Rather consider stochastic process actions depend correctness types evolution beliefs In spirit formal methodology compare interactive processes true types known knowledge approximated beliefs hypothesised types Based processes use probabilistic temporal logic deﬁne hierarchy desirable termination guarantees analyse theoretical conditions met The main result analysis novel characterisation optimality based concept probabilistic bisimulation 69 In addition concisely deﬁning constitutes optimality hypothesised types allows user apply eﬃcient model checking algorithms verify optimality practice 61 Task completion We interested task completion formally capture following assumption Assumption 2 Let player controlled HBA Then uis 1 iff s S 0 Assumption 2 speciﬁes interested reaching terminal state way obtain nonezero payoff In analysis consider discount factors γ cf Algorithm 1 γ 1 γ 1 While results hold cases important distinction If γ 1 expected payoffs 3 correspond actual probability following state lead terminal state success rate necessarily case γ 1 This γ 1 tends prefer shorter paths means actions lower success rates preferred lead faster termination Therefore γ 1 HBA solely interested termination γ 1 interested fast termination lower γ prefers faster termination 62 Methodology analysis Given SBG cid7 deﬁne ideal process X process induced cid7 player controlled HBA HBA knows current future types players Then given posterior formulation Pr hypothesised type spaces cid2 j j cid11 deﬁne user process Y process induced cid7 player controlled HBA X HBA uses Pr cid2 j usual way Thus difference X Y X predict player types Y approximates knowledge Pr cid2 j We write Eai st HtC denote expected payoff deﬁned 3 action ai state st history Ht process C X Y The idea X constitutes ideal solution sense Eai means HBA chooses truly bestpossible actions X This opposed Eai estimated expected payoff based Pr cid2 analysis specify relation Y X satisfy certain guarantees termination st Ht X corresponds actual expected payoff st Ht Y merely j HBA choose suboptimal actions Y The methodology We specify guarantees PCTL 55 probabilistic modal logic allows speciﬁcation time constraints PCTL expressions interpreted inﬁnite histories labelled transition systems atomic propositions Kripke structures In order interpret PCTL expressions X Y following modiﬁcations loss generality Firstly terminal state s S absorbing state meaning process s state s probability 1 players receive zero payoff Secondly introduce atomic proposition term label terminal state term true s s S We use following PCTL expressions cid17pterm F t cid17p term F 17 t N p 0 1 cid17 F t cid17pterm speciﬁes given state s probability cid17p state s cid17p term similar s s We write s C φ state s satisﬁes PCTL expression φ process C X Y satisﬁes term The semantics F cid12 cid12 cid12 reached s t time steps reached arbitrary ﬁnite time 80 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 63 Critical type spaces In analysis assume hypothesised type spaces cid2 j uncritical Deﬁnition 9 The hypothesised type spaces cid2 j critical set S c S S satisﬁes following 1 For Ht H st S c ai Ai Eai st Ht Y 0 Eai st Ht X 0 2 There positive probability Y eventually state sc S c s0 3 If Y state S c probability 1 state S c We cid2 j uncritical critical Intuitively critical type spaces potential lead HBA state space believes chooses right actions complete task actions actually required complete task The effect actions induce inﬁnite cycle critical inconsistency hypothesised true type spaces The following example demonstrates Example 7 Recall Example 6 let task choose action player j Then cid2 complete task t 1 regardless posterior beliefs despite fact cid2 j RL chooses actions RLRL Then cid2 assume cid2 j action player j thinking complete task different action actually complete j uncritical HBA inaccurate Now j critical HBA choose opposite θ θ RL cid3 cid2 A practical way ensure type spaces cid2 j eventually uncritical include methods opponent mod elling cid2 j guaranteed uncritical In Example 7 standard modelling method eventually learn true strat egy player j θLR As model accurate posterior beliefs gradually shift eventually allow HBA right action j If opponent models guaranteed learn correct behaviours type spaces cid2 64 Termination guarantees Our ﬁrst termination guarantee states X positive probability solving task Y Property 1 s0 X F 0 term s0 Y F 0 term We Property 1 holds hypothesised type spaces cid2 j uncritical Y chooses actions Let AHtC denote set actions process C choose state st history Ht AHtC player positive expected payoff X arg maxai Eai st Ht C cf step 3 Algorithm 1 Theorem 4 Property 1 holds cid2 Ht H ai AHtY Eai j uncritical st Ht X 0 18 0 term Then know X chooses actions ai lead state s Proof Assume s0 X F cid12 X F Now given 18 tempting infer result Y 0 term holds states s Y chooses actions ai positive expected payoff X truly lead terminal state 0 term special case Y chooses actions ai However 18 suﬃcient infer s st Ht X 0 reaching terminal state This require hypothesised type spaces cid2 Eai 0 term Property 1 holds cid2 uncritical prevents special case Thus infer s cid12 Y F cid12 Y F s cid12 cid12 j The second guarantee states X completes task Y Property 2 s0 X F 1 term s0 Y F 1 term We Property 2 holds type spaces cid2 j uncritical Y chooses actions player lead states X Let μHt sC probability process C transitions state s state st history Ht SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 μHt sC 1 A cid6 cid6 T st cid6ai aicid7 s cid4 π jHt j θ t j ai Ai A AHtC let μHt S ai A cid12C cid5 jcid11i sScid12 μHt sC S cid12 S Theorem 5 Property 2 holds cid2 j uncritical Ht H s S μHt sY 0 μHt s X 0 81 19 20 Proof The fact s0 X F 1 term means process X transitions states s s X F 1 term As tempting infer result Y based 20 transitions states maximum success rate X However 20 suﬃcient Y choose actions 20 holds true Y reach terminal state Nevertheless hypothesised type spaces cid2 j uncritical know special case occur Property 2 holds cid2 We note Properties 1 2 reverse direction holds true regardless Theorems 4 5 Furthermore combine requirements Theorems 4 5 ensure properties hold The guarantee subsumes previous guarantees stating X Y minimum probability solving task Property 3 s0 X F p term s0 Y F p term We Property 3 holds hypothesised type spaces cid2 j uncritical Y chooses actions player X chosen Let Rai HtC success rate action ai formally Rai HtC Eai st HtC γ 1 corresponds actual probability ai lead termination future Deﬁne Xmin Xmax processes Ht choose actions ai AHt X respectively minimal maximal success rate Rai Ht X Theorem 6 If cid2 j uncritical Ht H AHtY AHt X γ 1 Proposition 3 holds directions p term s0 Y F ii γ 1 s0 X F pmin q pmax q p p s0 Xmax F pmax term cid2 cid3 cid12 pcid12 term pmin pmax highest probabilities s0 Xmin F pmin 21 term Proof Since γ 1 actions ai AHt X success rate given H t given 21 know Y s actions success rate X s actions Provided type spaces cid2 j uncritical conclude Property 3 hold reasons reverse direction hold ii Since γ 1 actions ai AHt X different success rates The lowest highest chances X j uncritical cid2 completes task precisely modelled Xmin Xmax given 21 fact cid2 holds Y Therefore infer common bound pmin pmax deﬁned Theorem 6 cid2 p p cid3 cid12 Properties 1 3 indeﬁnite sense restrictions time requirements Our fourth ﬁnal guarantee subsumes previous guarantees states probability p X terminates t time steps Y p t Property 4 s0 X F t pterm s0 Y F t pterm We believe Property 4 adequate criterion optimality hypothesised type spaces cid2 j holds cid2 j way allows HBA plan accurately terms solving approximate true type spaces cid2 task ideal HBA X knows true types j What relation Y X order satisfy Property 4 The fact Y X processes state transition systems means draw methods model checking literature answer question Speciﬁcally use concept probabilistic bisimulation 69 deﬁne context work 82 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Deﬁnition 10 A probabilistic bisimulation X Y denoted X Y equivalence relation B S S s0 s0 B ii s X X term sY Y term s X sY B Y ˆSY histories Ht iii μHt Y B equivalence classes ˆS B X ˆS X μHt Y st X Ht X st Intuitively probabilistic bisimulation states X Y average match transitions Our deﬁnition probabilistic bisimulation general require transitions matched action related states satisfy atomic propositions termination However note deﬁnitions exist additional requirements results hold reﬁnements The main contribution section optimality criterion expressed Property 4 holds directions exists probabilistic bisimulation X Y Thus offer alternative formal characterisation optimality hypothesised type spaces cid2 j Theorem 7 Property 4 holds directions exists probabilistic bisimulation X Y Proof First note strictly speaking standard deﬁnitions bisimulation 1069 assume Markov property means state process depends current state In contrast consider general case state depend history H t previous states joint actions player strategies π j depend Ht However enforce Markov property design augmenting state space S account relevant factors past In fact postulate histories constitute states S H Therefore simplify exposition assume Markov property write μs ˆSC denote cumulative probability C transitions state s state ˆS Given Markov property fact B equivalence relation μs X ˆS X μsY ˆSY s X sY B represent dynamics X Y common graph following s0 ˆS0 μ01 ˆS1 μ13 μ35 ˆS3 ˆS5 μ02 μ14 μ41 μ36 ˆS2 μ24 ˆS4 μ46 ˆS6 S The nodes correspond equivalence classes B A directed edge ˆSa ˆSb speciﬁes positive probability μAB μs X ˆSb X μsY ˆSbY X Y transition states s X sY ˆSa states s ˆSb respec cid12 X s cid12 cid12 Y B There X s tively Note s X sY s node ˆS0 contains initial state s0 node ˆS6 contains terminal states S states This X Y reach terminal state stay μs S X μs SY 1 s S states satisfy term Thus graph starts ˆS0 terminates ˆS6 cid12 Y need equal merely equivalent s X sY B s cid12 X s cid12 Y Since graph represents dynamics X Y easy Property 4 hold directions In particular probabilities X Y node ˆS time t identical One simply needs add probabilities directed paths length t end ˆS provided paths exist probability path product μAB path Therefore X Y terminate equal probability average number time steps cid2 Some remarks clarify usefulness result First contrast Theorems 4 6 Theorem 7 explicitly require cid2 j uncritical In fact implicit deﬁnition probabilistic bisimulation Moreover theorems relate Y X identical histories H t Theorem 7 relates Y X related histories H t Y Ht X making generally applicable Finally Theorem 7 important practical implication tells use eﬃcient methods model checking 1069 verify optimality cid2 j In fact shown Property 4 hold albeit direction suﬃces Y probabilistic simulation 10 X coarser preorder probabilistic bisimulation However algorithms checking probabilistic simulation 10 computationally expensive fewer probabilistic bisimulation practical use currently limited 7 Behavioural hypothesis testing In previous section considered possibility incorrect hypothesised types analysed conditions HBA able complete task While analysis rigorous complete performed SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 83 interaction respect true types agents How decide interaction knowledge true types hypothesised types correct There ways answer question For example persistently reject hypothesised types hypothesise alternative set types resort default plan action maximin strategy Unfortunately posterior beliefs provide answer question quantify relative likelihood types relative set alternative types measure truth That beliefs point type tell observed agent type Instead tells types discarded current interaction history To illustrate source diﬃculty consider interaction process agents choose actions The table shows ﬁrst 5 time steps interaction The columns respectively current time t interaction actions chosen agents time t agent 1s hypothesised probabilities agent 2 choose actions time t based prior interaction history t 1 2 3 4 5 1 2 1 2 3 1 2 3 2 3 1 2 θ 2 cid63 1 6cid7 cid62 3 5cid7 cid67 1 2cid7 cid60 4 6cid7 cid64 2 4cid7 Assuming process continues fashion restrictions behaviour agent 2 agent 1 decide reject hypothesis behaviour agent 2 Note agent 1 outright reject hypothesis observed actions agent 2 supported agent 1s hypothesis positive probability There exists large body literature referred model criticism 14717921 Model criticism attempts answer analogous question given data set generated given model However contrast work model criticism usually assumes data independent identically distributed case interactive setting consider A related problem referred identity testing test given sequence data generated given stochastic process 8013 Instead independent identical distributions line work assumes properties stationarity ergodicity Unfortunately assumptions unlikely interaction processes proposed solutions costly A natural way address question compute kind score information given table compare score manually chosen rejecting threshold A prominent example score empirical frequency distribution 2847 However simplicity method appealing signiﬁcant problems far trivial devise scoring scheme reliably quantiﬁes correctness hypotheses instance empirical frequency distribution taken past actions insuﬃcient example hypothesised action distributions changing b unclear choose threshold parameter given scoring scheme In section particular form model criticism frequentist hypothesis testing com bined concept scores decide reject behavioural hypothesis Our proposed algorithm addresses allowing multiple scoring criteria construction test statistic intent obtaining overall reliable scoring scheme The distribution test statistic learned interaction process learning asymptotically correct Analogous standard frequentist testing hypothesis rejected given point time resulting pvalue signiﬁcance level This eliminates b providing uniform semantics rejection invariant employed scoring scheme We present results comprehensive set experiments demonstrating algorithm achieves high accuracy scalability low computational costs Of course longstanding debate role statistical hypothesis tests quantities pvalues 511630 The usual consensus pvalues combined forms evidence reach ﬁnal conclusion 45 view adopt In sense method larger machinery decide truth hypothesis 71 Individual hypotheses beliefs As noted Section 6 generally suﬃce consider correctness individual types plan actions respect types beliefs relative likelihood types cf 3 In regard note combination beliefs Pr types cid2 j described single type ˆθ j form π jHt j ˆθ j cid6 θ j cid2 j Prθ j Ht π jHt j θ j 22 84 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Algorithm 2 Automatic behavioural hypothesis testing Input history Ht including observed action at1 Output pvalue reject θ Parameters hypothesis θ Expand action vectors cid6at1 Set cid7 p threshold α j score functions z1 zK N 0 at1 j j j j j π j Ht1 θ j set ˆat j cid6ˆat1 j ˆat1 j cid7 j Sample ˆat1 n 1 N Sample at1 j π j Ht1 θ Fit skewnormal distribution f update parameters j set atn j cid6at1n j at1 j cid7 cid15 Compute D j ˆat j n 1 N cid14 Tatn Fit ξ ω β D 35 Find mode μ ξ ω β Compute pvalue Compute q Tat return p f q ξ ω β f μ ξ ω β j 2528 j ˆat This combination equivalent sampling single type θ j j probabilities Prθ cid2 j 68 Analogously combine true types cid2 j j choose actions j A j π jHt j θ type distribution ϒ single type ˆθ j π jHt j ˆθ j cid6 θ j cid2 j ϒt θ j π jHt j θ j Ht θ j cid2 j player j 23 Therefore simplify notation section generally assume single hypothesised type θ j cid2 j cid2 j Note means method applied combination beliefs hypothe j Furthermore write π jHt θ j denote probability distribution single true type θ sised types individual types cid2 actions A j probabilities individual actions j 72 A method behavioural hypothesis testing Let denote agent let j denote agent Moreover let θ j cid2 j denote js true behaviour The central question ask θ let θ j Unfortunately know θ j j directly answer question However time t know js j ˆat1 j generate vector ˆat ˆaτ j j generated ˆa0 j ˆat If use θ θ j j j j j formulate related twosample problem generated θ cid2 j denote hypothesis js behaviour a0 j at1 past actions j sampled π jH τ θ behaviour θ j j In section propose general eﬃcient algorithm decide problem At core algorithm computes frequentist pvalue cid8 cid9 p P Tat j Tat j ˆat j j ˆat cid8 δt θ cid9 j π jHt1 θ π jH 0 θ j j observe test statistic extreme Tat signiﬁcance level α j The value p corresponds probability expect j ˆat p j nullhypothesis θ Thus reject θ j θ j j 24 In following subsections test statistic T asymptotic properties algorithm learns distribution Tat j ˆat j A summary algorithm given Algorithm 2 721 Test statistic We follow general approach outlined earlier compute score vector actions hypoth esised distributions Formally deﬁne score function z A jt cid20 A jt R cid20 A j set probability SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 85 distributions A j Thus zat abbreviate zat j δt θ j θ j score observed actions j We use Z denote space score functions j hypothesised distributions δt θ j Given score function z deﬁne test statistic T tcid6 Tat j ˆat j 1 Tτ aτ j ˆaτ j t τ 1 j zˆaτ j zaτ j θ j denote τ preﬁxes j θ j Tτ aτ j ˆaτ j ˆaτ aτ j ˆat j respectively 25 26 In work assume z provided user While formally unnecessary sense analysis require ﬁnd useful design guideline interpret score kind likelihood higher scores suggest higher likelihood θ j correct Under interpretation minimum requirement z consistent t 0 θ j θ j cid21z arg max cid2 j θ cid12 j E cid12 j δt θ j cid12 cid2 j cid13 cid12 j cid12 j θ za 27 θ j true score zat j θ j Eη denotes expectation η This ensures nullhypothesis θ maximised expectation j j θ δt θ j Ideally like score function z perfect consistent cid21z 1 This means θ j maximise zat Unfortunately unclear score function exists general case look Even restrict behaviours agents exhibit diﬃcult ﬁnd perfect score function On hand relatively simple task specify small set score functions z1 zK consistent imperfect Examples given Section 73 Given score functions consistent know cardinality k cid21zk monotonically decrease Therefore reasonable approach combine multiple imperfect score functions attempt approximate perfect score function j θ θ j j j Given score functions z1 zK Z bounded interval b R redeﬁne Tτ Tτ aτ j ˆaτ j cid8 wk zkaτ j θ Kcid6 k1 j zkˆaτ j θ cid9 j 28 wk R weight score function zk In work set wk 1 K We experiment alternative weighting schemes Section 73 However believe wk serve interface useful modiﬁcations algorithm For example Yue et al 87 compute weights increase power hypothesis tests j j j j ˆat1 cid6at1 cid6ˆat1 The vectors 722 Asymptotic properties j ˆat generated π jHt1 θ set ˆat j j constructed iteratively That time t observe agent js past action at1 j set cid7 At time sample action ˆat1 θ j converge process Unfortunately T converge This surprising ﬁrst glance given at1 at1 cid7 Assuming nullhypothesis θ j distribution π jHt1 θ important difference at1 distributions This scores depend entire preﬁx vectors at1 distributions different at1 learns distribution T interaction process discuss Section 723 j Ex yψ x y 0 distribution ψ However subtle j zkˆat j arbitrarily different respectively means Fortunately algorithm require T converge j π jHt1 θ ˆat1 distribution zkat j π jHt1 θ j θ ˆat1 ˆat1 Tat cid11 ˆat1 j θ j ˆat j j j j j j j j j j j j Interestingly T converge shown ﬂuctuation T eventually normally distributed j denote ﬁ j ˆaτ j generated j denote cumulative variance Then stan set score functions z1 zK bound b Formally let ETτ aτ j ˆaτ nite expectation variance Tτ aτ iteratively prescribed Furthermore let σ 2 t dardised stochastic sum tcid6 j irrelevant aτ cid5 j ˆaτ t j sampled directly δτ θ j VarTτ aτ τ 1VarTτ aτ j ˆaτ j ˆaτ 29 1 σt τ 1 Tτ aτ j ˆaτ j ETτ aτ j ˆaτ j converge distribution standard normal distribution t Thus T normally distributed To ﬁrst recall standard central limit theorem requires random variables Tτ independent identically distributed In case Tτ independent random outcome Tτ effect outcome 86 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Tτ cid12 However Tτ Tτ cid12 depend different action sequences different distributions Hence additional property commonly known Lyapunovs condition 44 states exists positive integer d t ˆσ 2d σ 2d tcid6 t lim t ˆσ 2d t 0 cid7cid19 cid19 cid19Tτ aτ E j ˆaτ j ETτ aτ j ˆaτ cid19 cid19 j cid19 cid10 2d 30 31 τ 1 Since zk bounded know Tτ bounded Hence summands 31 uniformly bounded U brevity Setting d 1 obtain lim t ˆσ 3 t σ 3 t U ˆσ 2 t σ 3 t U σt 32 The goes zero σt Lyapunovs condition holds If hand σt converges means variance Tτ zero point onward appropriate convergence zero From point θ j 1 statistical analysis longer necessary j prescribes fully deterministic action choices agent j j π jH τ j θ 723 Learning test distribution Given T eventually normal reasonable compute 24 normal distribution parameters ﬁtted interaction However fails recognise distribution T shaped gradually extended time period ﬂuctuation T heavily skewed direction convergence normal distribution emerges Thus normal distribution poor ﬁt shaping period What needed distribution represent normal distribution ﬂexible faithfully represent gradual shaping One distribution properties skewnormal distribution 977 Given PDF φ CDF cid23 standard normal distribution skewnormal PDF deﬁned cid20 x ξ f x ξ ω β 2 ω ω ξ R location parameter ω R scale parameter β R shape parameter Note reduces normal PDF β 0 case ξ ω correspond mean standard deviation respectively Hence normal distribution subclass skewnormal distribution x ξ ω 33 cid21cid21 cid23 cid20 cid21 cid20 φ β Our algorithm learns shifting parameters f interaction process simple effective sampling iteratively generate N additional action vectors at1 exact way j atN j procedure Essentially use θ j ˆat mapped data points j The vectors atn cid14 Tatn D j ˆat j cid15 j n 1 N estimate parameters ξ ω β minimising negative loglikelihood cid20 cid21cid21 cid21 cid20 cid20 N logω cid6 log φ xD x ξ ω log cid23 β x ξ ω whilst ensuring ω positive An alternative methodofmoments estimator obtain initial values 35 Note usually unnecessary estimate parameters point time reasonable update parameters frequently evidence observed actions grows Given asymmetry skewnormal distribution semantics extreme 24 longer obvious respect mean mode In addition usual tailarea calculation pvalue requires CDF closed form skewnormal CDF approximating cumbersome To circumvent issues approximate pvalue p f Tat j ˆat j ξ ω β f μ ξ ω β 36 μ mode ﬁtted skewnormal distribution This avoids asymmetry issue easier compute 34 35 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 87 Fig 6 Average accuracy random behaviours N 50 A j 2 10 20 Results averaged 500 processes 10 000 time steps θ θ j Xaxis shows score functions zk test statistic cid11 θ j j θ j Fig 7 Average pvalues random behaviours N 50 θ score functions zk test statistic j cid11 θ j hypothesis wrong Results averaged 500 processes Legend shows 73 Experiments We conducted comprehensive set experiments investigate accuracy correct incorrect rejection scala bility number actions sampling complexity algorithm The following score functions combinations z1at j θ z2at j θ j 1 t j 1 t t1cid6 τ 0 t1cid6 τ 0 π jH τ aτ j θ j maxa j A j π jH τ j θ j 1 Ea j π j Hτ θ j cid19 cid19 cid19π jH τ aτ j θ j π jH τ j θ cid19 cid19 cid19 j z3at j θ j cid6 j A j min cid22 1 t t1cid6 aτ j j1 τ 0 1 t t1cid6 τ 0 cid23 π jH τ j θ j 37 38 39 b1 1 b true 0 Note z1 z3 generally consistent cf Section 721 z2 consistent A j 2 necessarily A j 2 Furthermore z1 z2 z3 imperfect The score function z3 based empirical frequency distribution The parameters test distribution cf Section 723 estimated frequently t increased The ﬁrst esti mation performed time t 1 observing action After estimating parameters time t waited cid25 cid24 1 time steps parameters reﬁtted Throughout experiments signiﬁcance level t α 001 reject θ j pvalue 001 731 Random behaviours In ﬁrst set experiments behaviour type spaces cid2i cid2 j restricted random behaviours Each random behaviour deﬁned sequence random probability distributions A j The distributions created drawing uniform random numbers 0 1 action j A j subsequent normalisation values sum 1 Random behaviours good baseline experiments usually hard distinguish This fact entire set A j support behaviours react past actions These properties mean little structure interaction distinguish behaviours We simulated 1000 interaction processes lasting 10 000 time steps In process randomly sampled cid2 j control agents j respectively In half processes correct hypothesis haviours θi cid2i θ j 88 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Fig 8 Average accuracy random behaviours N 50 A j 2 10 20 Weights wk computed truemax weighting Xaxis shows score functions zk test statistic Fig 9 Average accuracy random behaviours N 50 A j 2 10 20 Weights wk computed truemin weighting Xaxis shows score functions zk test statistic j θ θ In half sampled random hypothesis θ j j A j 2 10 20 Ai A j N 10 50 100 cf Section 723 cid2 j θ j cid11 θ j We repeated set simulations Fig 6 shows average accuracy algorithm N 50 mean average percentage time j The xaxis shows steps algorithm correct decisions reject θ j combination score functions compute test statistic 1 2 means combined z1 z2 j reject θ θ cid11 θ j The results algorithm achieved excellent accuracy bordering 100 mark They algorithm scaled number actions degradation accuracy However exceptions observations z3 resulted poor accuracy θ combination z2 z3 scaled badly θ j cid11 θ The reason exceptions z3 good scoring scheme random behaviours The function z3 quantiﬁes similarity empirical frequency distribution averaged hypothesised distributions For random behaviours deﬁned work distributions converge uniform distribution Thus z3 random behaviours eventually explains low accuracy θ cid11 θ cid11 θ j j j j j As seen Fig 6 inadequacy z3 solved adding score functions z1 z2 These functions add discriminative information test statistic technically means cardinality cid21z 27 reduced However case z2 z3 converge substantially slower higher A j meaning evidence needed θ j rejected Fig 7 shows higher number actions affects average convergence rate pvalues computed z2 z3 zkaτ In addition score functions zk central aspect convergence pvalues corresponding weights wk cf 28 As mentioned Section 721 use uniform weights wk 1 K However weighting trivial j θ matter repeated experiments alternative weighting schemes Let zτ j denote k summands 28 The weighting schemes truemax truemin assign wk 1 ﬁrst k maximises minimises zτ 0 Similarly weighting schemes max min assign wk 1 ﬁrst k maximises minimises k zτ k 0 Figs 8 9 results truemax truemin As seen ﬁgures truemax similar uniform weights truemin improves convergence z2 z3 compromises The results max min similar truemin truemax respectively omit Finally recomputed accuracies lenient signiﬁcance level α 005 As expected marginally decreased increased percentage points accuracy θ respectively j This primarily observed early stages interaction Overall results similar obtained α 001 Recall N speciﬁes number sampled action vectors atn j learn distribution test statistic cf Section 723 In previous section reported results N 50 In section investigate differences accuracy N 10 50 100 j θ j zkˆaτ j θ Figs 10 11 differences A j 2 20 respectively The ﬁgure A j 10 virtually A j 20 minor improvements accuracy z2 z3 cluster Hence omit As seen improvements 10 N 10 N 50 marginal improvements N 50 θ cid11 θ j j SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 89 Fig 10 Average accuracy random behaviours A j 2 N 10 50 100 Results averaged 500 processes 10 000 time steps θ j j Xaxis shows score functions zk test statistic j θ cid11 θ θ j Fig 11 Average accuracy random behaviours A j 20 N 10 50 100 Results averaged 500 processes 10 000 time steps θ j j Xaxis shows score functions zk test statistic j θ θ cid11 θ j N 100 This observed A j 2 10 20 constellations score functions The fact N 50 suﬃcient A j 20 remarkable random behaviours 20t possible action vectors sample time t We compared learned skewnormal distributions ﬁtted data Figs 12 13 histograms ﬁtted skewnormal distributions example processes 1000 time steps In Fig 13 deliberately chose example learned distribution maximally skewed N 10 sign N small Nonetheless majority processes learned distribution moderately skewed algorithm achieved average accuracy 90 N 10 Moreover wants avoid maximally skewed distributions simply restrict parameter space ﬁtting skewnormal speciﬁcally shape parameter β cf Section 723 The ﬂexibility skewnormal distribution particularly useful early stages interaction test statistic typically follow normal distribution Fig 14 shows test distribution example process 10 time steps z2 test statistic N 100 histogram created N 10 000 The learned skewnormal approximated true test distribution closely Note examples normal Student distributions produce good ﬁts Our implementation algorithm performed calculations iterative updates skewnormal ﬁtting Hence little ﬁxed memory low computation times For example score functions A j 20 N 100 cycle algorithm cf Algorithm 2 took average 1 millisecond ﬁtting skewnormal parameters 10 milliseconds ﬁtting skewnormal parameters offtheshelf Simplexoptimiser default parameters The times measured Matlab R2014a Unix machine 26 GHz Intel Core i5 processor 732 Adaptive behaviours We complemented structurefree interaction random behaviours conducting analogous experiments additional classes behaviours Speciﬁcally benchmark framework speciﬁed Section 5 consists 78 distinct 2 2 matrix games methods automatically generate sets behaviours given game The behaviour classes LeaderFollowerTrigger Agents LFT CoEvolved Decision Trees CDT CoEvolved Neural Networks CNN These classes cover broad spectrum possible behaviours including fully deterministic CDT fully stochastic CNN hybrid LFT behaviours Furthermore generated behaviours adaptive varying degrees adapt action choices based players choices Detailed descriptions games behaviour classes appendix 1 The following experiments performed behaviour class identical randomisation For 78 games simulated 10 interaction processes lasting 10 000 time steps For process randomly sampled cid2 j control agents j respectively cid2i cid2 j restricted behaviour class behaviours θi cid2i θ In half processes correct hypothesis θ half sampled random hypothesis j θ As repeated simulation N 10 50 100 constellations score functions j cid2 j θ θ cid11 θ j j j j 90 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Fig 12 Example histograms ﬁtted skewnormal distributions shown red curve 1000 time steps random behaviours A j 10 N 10 50 100 Using score function z1 test statistic Fig 13 Example histograms ﬁtted skewnormal distributions shown red curve 1000 time steps random behaviours A j 10 N 10 50 100 Using score functions z1 z2 z3 test statistic virtually differences Hence following report results N 50 z1 z2 z3 cluster j j generally good accuracy θ Fig 15a shows average accuracy achieved algorithm behaviour classes While accuracy θ θ j mixed Note merely fact j score functions imperfect cf Section 721 obtained results combinations Rather reveals inherent limitation approach actively probe aspects hypothesis θ j In words algorithm performs statistical hypothesis tests based evidence generated θi cid11 θ To illustrate useful consider tree structure behaviours CDT class Each node tree θ corresponds past action taken θi Depending θi chooses actions subset entire tree deﬁnes θ way algorithm differentiate Hence asymmetry accuracy θ θ Note j problem occur random behaviours aspects eventually visible j unseen aspects θ j θ However hypothesis θ j differs θ j cid11 θ j j j j Following observation repeated experiments restricted cid2i random behaviours cf Section 731 goal exploring θ j thoroughly As shown Fig 15b led signiﬁcant improvements accuracy espe cially CDT class Nonetheless choosing actions purely randomly suﬃcient probing strategy accuracy CNN relatively low For CNN complicated fact neural networks θ j θ cid12 formally different θ j cid11 θ cid12 j essentially action probabilities extremely small differences Hence cases require evidence distinguish behaviours j 8 Conclusion Much work artiﬁcial intelligence focused innovative applications adaptive user interfaces robotic elderly care automated trading agents A key technological challenge applications design intelligent agent quickly learn interact effectively agents behaviours initially unknown Learning scratch problems viable solution time crucial factor exploration trialanderror feasible desirable Instead likely solution problem draw heavily prior experience intuition form hypothesised behaviours Indeed strong intuition behaviour agents based past experience structural constraints task completed intuition utilised interaction This motivation typebased method studied work The idea typebased method hypothesise set possible behaviours types agents plan actions respect types believe likely given observed actions SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 91 Fig 14 Example true test distribution z2 learned skewnormal distribution shown red curve 10 time steps A j 10 N 100 Fig 15 Average accuracy behaviour classes LFT CDT CNN N 50 Results averaged 500 processes 10 000 time steps θ θ j j Bars shown z1 z2 z3 test statistic cid11 θ j θ j agents In regard identiﬁed addressed spectrum important questions pertaining properties beliefs types possibility incorrect types Speciﬁcally formulated alternative methods incorporate observations beliefs studied conditions resulting beliefs correct incorrect We investigated impact prior beliefs payoff maximisation methods automatically compute prior beliefs For case hypothesised types incorrect analysed conditions able complete task despite incorrectness types Finally described automatic statistical analysis ascertain correctness hypothesised types interaction In addition theoretical insights results presented article number practical implications First analysis Section 4 shows standard posterior formulation likelihood deﬁned product action probabilities appropriate choice Rather consider alternative formulations posterior beliefs sum correlated posteriors Furthermore empirical analysis Section 5 shows prior beliefs crucial ability maximise payoffs longterm Indeed better servative uniform prior belief automatic methods ones work Another important practical implication use eﬃcient model checking methods verify optimality hypothesised types Speciﬁcally Section 6 useful connection probabilistic bisimulation checking Moreover case prior analysis based bisimulation possible correctness types contemplated interaction Our algorithm Section 7 simple implement highly eﬃcient achieves high accuracy scalability There potential directions future work Further formulations posterior beliefs developed interesting know asymptotic correctness analysis Section 4 complemented useful ﬁnitetime error bounds Our empirical analysis prior beliefs Section 5 reﬁned theoretical analysis important question prior beliefs computed useful error bounds LPpriors step direction Furthermore optimality analysis Section 6 focused task completion extended analysis focusing payoff maximisation Finally unclear concept perfect scores Section 7 generally feasible necessary impact score weights convergence decision quality Two aspects address crucial successful deployment typebased method complexity planning step size hypothesised type spaces Regarding seen Algorithm 1 speciﬁcally 34 time complexity planning exponential factors number agents actions states making costly operation complex systems A promising solution stochastic sampling procedures 411 Regarding problem number types wish hypothesise grow dramatically size interaction problem states actions agents This problematic predictions type computed point time desirable minimise number hypothesised types One way develop methods produce small sets reasonable types good coverage behaviours spirit works 32 Another method introduce learnable structure types parameters type covers spectrum behaviours However require ability infer parameters interaction history 92 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 Acknowledgements The authors acknowledge support German National Academic Foundation Masdar InstituteMIT collabora tive agreement Flagship Project 13CAMA1 UK Engineering Physical Sciences Research Council EPH0123381 European Commission TOMSY Grant 270436 FP7ICT200921 Call 6 Royal Academy Engineering Ingenious Grant MEI01ING14 RAM01ING12 European Commission SmartSociety Grant agreement 600854 FOCAS ICT2011910 The authors wish thank anonymous reviewers comments suggestions Appendix A Proof Theorem 1 Proof Kalai Lehrer 65 studied model equivalently described singlestate SBG S 1 pure type distribution product posterior They showed players assessment future play absolutely continuous respect true probabilities future play event true positive probability assigned positive probability player 6 hold In case absolute continuity holds Assumption 1 fact prior probabilities P j positive fact type distribution pure infer true types positive posterior probability In proof seek extend convergence result Kalai Lehrer 65 henceforth KL multistate SBGs pure type distributions Our strategy translate SBG cid7 modiﬁed SBG ˆcid7 equivalent cid7 sense players behave identically compatible model KL sense informational assumptions ignore differences We achieve introducing new player nature denoted ξ emulates transitions cid7 ˆcid7 Given SBG cid7 S s0 S N Ai cid2i ui πi T ϒ deﬁne modiﬁed SBG ˆcid7 follows Firstly ˆcid7 state arbitrary effect The players ˆcid7 ˆN N ξ N actions types cid7 Ai cid2i deﬁne actions types ξ Aξ cid2ξ S natures actions types correspond states cid7 The payoffs ξ zero strategy ξ time t deﬁned ξ H τ aξ θξ π t 0 1 T aτ 1 ξ aτ 1 iN aξ τ t aξ cid11 θξ τ t aξ θξ τ t H τ history length τ t H τ allows players N use π t This necessary establish equivalence ˆcid7 cid7 ξ future predictions ξ s actions The purpose ξ emulate state transitions cid7 Therefore modiﬁed strategies ˆπi payoffs ˆui N deﬁned respect actions types current type ξ determines action ξ Formally ˆπiHt ai θi πi Ht ai θi Ht θ 0 ξ a0 iN θ 1 ξ a1 iN θ t ξ ˆuis θ t uiθ t ξ j jN θ t s state ˆcid7 ˆN Ai at1 Finally ˆcid7 uses type distributions ϒ ϒξ ϒ type distribution cid7 ϒξ deﬁned ϒξ Ht θξ iN θξ If s0 initial state cid7 ϒξ H 0 θξ 1 θξ s0 T at1 ξ The modiﬁed SBG ˆcid7 proceeds original SBG cid7 following changes ϒ sample types N usual ϒξ sample types ξ b player informed type type ξ This completes deﬁnition ˆcid7 The modiﬁed SBG ˆcid7 equivalent original SBG cid7 sense players N identical behaviour SBGs Since players know type ξ know action ξ corresponds knowing current state game Furthermore note strategy ξ uses time indices t τ allow distinguish current time τ t future time τ t This means π t ξ compute expected payoffs ˆcid7 way T compute expected payoffs cid7 In words formulas 2 3 modiﬁed straightforward manner replacing original components cid7 modiﬁed components ˆcid7 yielding results Finally ˆcid7 uses type distribution cid7 sample types N differences payoffs strategies To complete proof note b procedural differences modiﬁed SBG model KL However specify players know type ξ need learn type distribution ϒξ b effect KL The important point KL assume model players interact players environment Since eliminated environment replacing player ξ precisely happens modiﬁed SBG Therefore convergence result KL carries multistate SBGs pure type distributions cid2 References SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 93 1 S Albrecht Utilising policy types effective ad hoc coordination multiagent systems PhD thesis The University Edinburgh 2015 2 S Albrecht J Crandall S Ramamoorthy An empirical study practical impact prior beliefs policy types Proceedings 29th AAAI Conference Artiﬁcial Intelligence 2015 pp 19881994 3 S Albrecht S Ramamoorthy Comparative evaluation MAL algorithms diverse set ad hoc team problems Proceedings 11th Inter national Conference Autonomous Agents Multiagent Systems 2012 pp 349356 4 S Albrecht S Ramamoorthy A gametheoretic model bestresponse learning method ad hoc coordination multiagent systems Tech rep School Informatics The University Edinburgh 2013 arXiv150601170 5 S Albrecht S Ramamoorthy A gametheoretic model bestresponse learning method ad hoc coordination multiagent systems extended abstract Proceedings 12th International Conference Autonomous Agents Multiagent Systems 2013 pp 11551156 6 S Albrecht S Ramamoorthy On convergence optimality bestresponse learning policy types multiagent systems Proceedings 30th Conference Uncertainty Artiﬁcial Intelligence 2014 pp 1221 7 S Albrecht S Ramamoorthy Are I think Criticising uncertain agent models Proceedings 31st Conference Uncertainty Artiﬁcial Intelligence 2015 pp 5261 8 R Aumann Subjectivity correlation randomized strategies J Math Econ 1 1974 6796 9 A Azzalini A class distributions includes normal ones Scand J Stat 12 1985 171178 10 C Baier Polynomial time algorithms testing probabilistic bisimulation simulation Proceedings 8th International Conference Computer Aided Veriﬁcation Lecture Notes Computer Science vol 1102 Springer 1996 pp 3849 11 S Barrett P Stone S Kraus Empirical evaluation ad hoc teamwork pursuit domain Proceedings 10th International Conference Autonomous Agents Multiagent Systems 2011 pp 567574 12 S Barrett P Stone S Kraus A Rosenfeld Teamwork limited knowledge teammates Proceedings 27th AAAI Conference Artiﬁcial Intelligence 2013 pp 102108 13 I Basawa D Scott Eﬃcient tests stochastic processes Sankhya Ser A 1977 2131 14 M Bayarri J Berger P values composite null models J Am Stat Assoc 95 452 2000 11271142 15 R Bellman Dynamic Programming Princeton University Press 1957 16 J Berger T Sellke Testing point null hypothesis irreconcilability P values evidence discussion J Am Stat Assoc 82 1987 17 J Bernardo Reference posterior distributions Bayesian inference J R Stat Soc B 41 2 1979 113147 18 D Bernstein S Zilberstein N Immerman The complexity decentralized control Markov decision processes Proceedings 16th Conference Uncertainty Artiﬁcial Intelligence 2000 pp 3237 19 M Bowling P McCracken Coordination adaptation impromptu teams Proceedings 20th National Conference Artiﬁcial Intelligence 112122 2005 pp 5358 20 M Bowling M Veloso Multiagent learning variable learning rate Artif Intell 136 2 2002 215250 21 G Box Sampling Bayes inference scientiﬁc modelling robustness J R Stat Soc A 1980 383430 22 R Brafman M Tennenholtz Rmaxa general polynomial time algorithm nearoptimal reinforcement learning J Mach Learn Res 3 2003 213231 23 G Brown Iterative solution games ﬁctitious play Proceedings Conference Activity Analysis Production Allocation 1951 pp 374376 Syst 2 2 1999 141172 24 S Carberry Techniques plan recognition User Model UserAdapt Interact 11 12 2001 3148 25 D Carmel S Markovitch Exploration strategies modelbased learning multiagent systems exploration strategies Auton Agents MultiAgent 26 G Chalkiadakis C Boutilier Coordination multiagent reinforcement learning Bayesian approach Proceedings 2nd International Confer ence Autonomous Agents Multiagent Systems 2003 pp 709716 27 E Charniak R Goldman A Bayesian model plan recognition Artif Intell 64 1 1993 5379 28 V Conitzer T Sandholm AWESOME general multiagent learning algorithm converges selfplay learns best response stationary opponents Mach Learn 67 12 2007 2343 29 V Conitzer T Sandholm New complexity results Nash equilibria Games Econ Behav 63 2 2008 621641 30 D Cox The role signiﬁcance tests discussion Scand J Stat 4 1977 4970 31 J Crandall Towards minimizing disappointment repeated games J Artif Intell Res 49 2014 111142 32 J Crandall Robust learning repeated stochastic games metagaming Proceedings 24th International Joint Conference Artiﬁcial Intelligence 2015 pp 34163422 1999 pp 150159 33 B De Finetti Philosophical Lectures Probability Collected Edited Annotated Alberto Mura Springer 2008 34 R Dearden N Friedman D Andre Model based Bayesian exploration Proceedings 15th Conference Uncertainty Artiﬁcial Intelligence 2008 pp 6368 35 E Dekel D Fudenberg D Levine Learning play Bayesian games Games Econ Behav 46 2 2004 282303 36 J Dibangoye C Amato A Doniec F Charpillet Producing eﬃcient errorbounded solutions transition independent decentralized MDPs Pro ceedings 12th International Conference Autonomous Agents Multiagent Systems 2013 pp 539546 37 P Doshi P Gmytrasiewicz On diﬃculty achieving equilibrium interactive POMDPs Proceedings 21st National Conference Artiﬁcial Intelligence 2006 pp 11311136 38 P Doshi P Gmytrasiewicz Monte Carlo sampling methods approximating interactive POMDPs J Artif Intell Res 2009 297337 39 P Doshi D Perez Generalized point based value iteration interactive POMDPs Proceedings 23rd AAAI Conference Artiﬁcial Intelligence 40 P Doshi X Qu A Goodie D Young Modeling recursive reasoning humans empirically informed interactive POMDPs Proceedings 9th International Conference Autonomous Agents Multiagent Systems 2010 pp 12231230 41 P Doshi Y Zeng Q Chen Graphical models interactive POMDPs representations solutions Auton Agents MultiAgent Syst 18 3 2009 376416 42 R EmeryMontemerlo G Gordon J Schneider S Thrun Approximate solutions partially observable stochastic games common payoffs Proceedings 3rd International Conference Autonomous Agents Multiagent Systems 2004 pp 136143 43 K Etessami M Yannakakis On complexity Nash equilibria ﬁxed points SIAM J Comput 39 6 2010 25312597 44 H Fischer A History Central Limit Theorem From Classical Modern Probability Theory Springer Science Business Media 2010 45 R Fisher The Design Experiments Oliver Boyd 1935 46 D Foster H Young On impossibility predicting behavior rational agents Proc Natl Acad Sci 98 22 2001 1284812853 47 D Foster H Young Learning hypothesis testing Nash equilibrium Games Econ Behav 45 1 2003 7396 94 SV Albrecht et al Artiﬁcial Intelligence 235 2016 6394 2004 pp 226231 48 D Fudenberg D Levine Selfconﬁrming equilibrium Econometrica 1993 523545 49 Y Gal A Pfeffer F Marzo B Grosz Learning social preferences games Proceedings 19th National Conference Artiﬁcial Intelligence 50 C Geib R Goldman A probabilistic plan recognition algorithm based plan tree grammars Artif Intell 173 11 2009 11011132 51 A Gelman C Shalizi Philosophy practice Bayesian statistics Br J Math Stat Psychol 66 1 2013 838 52 P Gmytrasiewicz P Doshi A framework sequential planning multiagent settings J Artif Intell Res 24 1 2005 4979 53 B Grosz S Kraus Collaborative plans complex group action Artif Intell 86 2 1996 269357 54 E Hansen D Bernstein S Zilberstein Dynamic programming partially observable stochastic games Proceedings 19th National Conference Artiﬁcial Intelligence 2004 pp 709715 55 H Hansson B Jonsson A logic reasoning time reliability Form Asp Comput 6 5 1994 512535 56 J Harsanyi Games incomplete information played Bayesian players Part I The basic model Manag Sci 14 3 1967 159182 57 J Harsanyi Games incomplete information played Bayesian players Part II Bayesian equilibrium points Manag Sci 14 5 1968 320334 58 J Harsanyi Games incomplete information played Bayesian players Part III The basic probability distribution game Manag Sci 14 7 1968 486502 MIT Press 1975 59 J Holland Adaptation Natural Artiﬁcial Systems An Introductory Analysis Applications Biology Control Artiﬁcial Intelligence The 60 R Howard Information value theory IEEE Trans Syst Sci Cybern 2 1 1966 2226 61 J Hu M Wellman Nash qlearning generalsum stochastic games J Mach Learn Res 4 2003 10391069 62 E Jaynes Prior probabilities IEEE Trans Syst Sci Cybern 4 3 1968 227241 63 J Jordan Bayesian learning normal form games Games Econ Behav 3 1 1991 6081 64 D Kahneman A Tversky Prospect theory analysis decision risk Econometrica 47 1979 263292 65 E Kalai E Lehrer Rational learning leads Nash equilibrium Econometrica 61 5 1993 10191045 66 G Kaminka I Frenkel Integration coordination mechanisms BITE multirobot architecture Proceedings International Conference Robotics Automation 2007 pp 28592866 67 J Koza Genetic Programming On Programming Computers Means Natural Selection The MIT Press 1992 68 H Kuhn Extensive games problem information Contributions Theory Games vol 2 Annals Mathematics Studies vol 28 69 K Larsen A Skou Bisimulation probabilistic testing Inf Comput 94 1 1991 128 70 M Littman Markov games framework multiagent reinforcement learning Proceedings 11th International Conference Machine 1953 pp 193216 Learning vol 157 1994 pp 157163 71 XL Meng Posterior predictive pvalues Ann Stat 1994 11421160 72 J Nachbar Prediction optimization learning repeated games Econometrica 65 2 1997 275309 73 J Nachbar Beliefs repeated games Econometrica 73 2 2005 459480 74 J Nash Equilibrium points nperson games Proc Natl Acad Sci 36 1 1950 4849 75 B Ng C Meyers K Boakye J Nitao Towards applying interactive POMDPs realworld adversary modeling Proceedings 22nd Innovative Applications Artiﬁcial Intelligence Conference 2010 pp 18141820 76 Y Nyarko Bayesian learning convergence Nash equilibria common priors Econ Theory 11 3 1998 643655 77 A OHagan T Leonard Bayes estimation subject uncertainty parameter constraints Biometrika 63 1 1976 201203 78 A Rapoport M Guyer A taxonomy 2 2 games General Systems Yearbook Society General Systems Research vol 11 1966 pp 203214 79 D Rubin Bayesianly justiﬁable relevant frequency calculations applied statistician Ann Stat 12 4 1984 11511172 80 D Ryabko B Ryabko On hypotheses testing ergodic processes Proceedings IEEE Information Theory Workshop 2008 pp 281283 81 L Shapley Stochastic games Proc Natl Acad Sci USA 39 10 1953 1095 82 F Southey M Bowling B Larson C Piccione N Burch D Billings C Rayner Bayes bluff opponent modelling poker Proceedings 21st 83 P Stone G Kaminka S Kraus J Rosenschein Ad hoc autonomous agent teams collaboration precoordination Proceedings 24th Conference Uncertainty Artiﬁcial Intelligence 2005 pp 550558 AAAI Conference Artiﬁcial Intelligence 2010 pp 15041509 84 G Sukthankar R Goldman C Geib D Pynadath H Bui Plan Activity Intent Recognition Theory Practice Morgan Kaufmann 2014 85 R Sutton A Barto Reinforcement Learning An Introduction The MIT Press 1998 86 M Tambe Towards ﬂexible teamwork J Artif Intell Res 7 1997 83124 87 Y Yue Y Gao O Chapelle Y Zhang T Joachims Learning powerful test statistics clickbased retrieval evaluation Proceedings 33rd International ACM SIGIR Conference Research Development Information Retrieval 2010 pp 507514