Artiﬁcial Intelligence 174 2010 12541276 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Practical performance models algorithms evolutionary program induction domains Mario Graff Riccardo Poli School Computer Science Electronic Engineering University Essex Colchester CO4 3SQ UK r t c l e n f o b s t r c t Article history Received 22 October 2008 Received revised form 18 July 2010 Accepted 19 July 2010 Available online 23 July 2010 Keywords Evolution algorithms Program induction Performance prediction Algorithm taxonomies Algorithm selection problem Evolutionary computation techniques seen considerable popularity problem solving optimisation tools recent years Theoreticians developed variety exact approximate models evolutionary program induction algorithms However models criticised applicable simplistic problems algorithms unrealistic parameters In paper start rectifying situation relation matters practitioners users program induction systems performance That introduce simple practical model performance programinduction algorithms To test approach consider important classes problems symbolic regression Boolean function induction model different versions genetic programming gene expression programming stochastic iterated hill climbing program space We illustrate generality technique accurately modelling performance training algorithm artiﬁcial neural networks heuristics offline bin packing problem We models performing accurate predictions help analysis comparison different algorithms andor algorithms different parameters setting We illustrate automatic construction taxonomy stochastic programinduction algorithms considered study The taxonomy reveals important features algorithms performance point view detected ordinary experimentation 2010 Elsevier BV All rights reserved 1 Introduction Evolutionary Algorithms EAs popular forms search optimisation 16 Their invention dates decades 7 So imagine theoretical understanding opera tions rich set theoreticallysound guidelines parametrisation customisation However case Despite simplicity EAs sound theoretical models EAs precise mathematical results scarce hard obtain emerging years proposal original algorithm 818 A key reason algorithm representation set genetic operators ﬁtness function requires different theoretical model In addition randomness nonlinearities immense number degrees freedom present typical EA life hard theoreticians This applies techniques automatic evolution programs Evolutionary Programinduction Algorithms EPAs including Genetic Programming GP 61519 Cartesian GP CGP 20 Grammatical Evolution GE Corresponding author Email addresses mgraffessexacuk M Graff rpoliessexacuk R Poli 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201007005 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1255 21 Gene Expression Programming GEP 22 Our theoretical understanding EPAs slower develop EAs chieﬂy objective diﬃculty modelling stochastic searchers inﬁnite spaces programs unbounded size search operators dynamically change dimension structure solutions explored case EPAs So despite recent successes developing solid theory GP related EPAs 151723 review 19 growing gap EPA theory practice Often theoretical studies models EAs criticised easily applicable realistic situations 24 One reason producing comprehensive theory complex adaptive systems EAs objectively hard slow mentioned earlier Another reason theoreticians focus approaches problems distant practice So despite proven effectiveness EAs EPAs example 19 urgent need theory clarify applicability different types algorithms particular problems provide design guidelines avoid current timeconsuming practice handtuning algorithms parameters operators This paper attempts rectify situation proposing practical model EPAs The model design capture characteristics algorithm models exactly extremely diﬃcult Instead focuses matters practitioners performance EAs realistic problems accepting fact practice modelling performance exactly This model allow answers questions How likely particular algorithm solve particular problem What ﬁtness expect ﬁnd end run Whats best algorithm solve problem class problems Since alternative model EPA performance available present alternative seek answers questions direct empirical experimentation Although approach initially aimed modelling EPAs easily extended program induction stochastic search capture characteristics forms search problem solving To illustrate model performance heuristics offline bin packing problem learning algorithm feedforward neural networks Our models related techniques solve algorithm selection problem 25 problem deciding tool choose solve problem set available tools particular modelling techniques algorithm portfolios 2635 collections algorithms run parallel sequence solve problem The methodology presented complementary competitive approaches illustrate creation effective portfolios program induction algorithms area algorithm selection technique tested Our models pure prediction performance For example enable analysis similarities differences algorithms relation performance To illustrate collection models different algorithms algorithms different parameters obtain meaningful informative taxonomy evolutionary stochastic programinduction algorithms completely automatic process The paper organised follows In Section 2 review related theoretical work ﬁeld EAs In Section 3 performance model arrived methodology instantiate Section 4 presents problems test approach Section 5 describes systems parameter settings experimen tation Experimental results corroborate validity models predictions presented Section 6 Section 7 looks algorithm selection problem surveying relevant literature applying models creation algo rithm portfolios programinduction problems The similarities differences approaches discussed Applications models comparison categorisation algorithms discussed Section 8 Some conclusions possible directions future work given Section 9 2 Related work Our work related problem understanding makes problem easy hard EAs Problemdiﬃculty studies EAs focused initially buildingblock hypothesis Genetic Algorithms GAs 2 related notion deception 36 The approach consisted constructing artiﬁcial ﬁtness functions based certain priori assumptions easy hard GAs This produced useful results puzzling counter examples 37 The notion ﬁtness landscape originally proposed 38 underlies recent approaches problem diﬃculty It clear example smooth landscape single optimum relatively easy search algorithms rugged landscape local optima problematic 3940 However graphical visuali sation ﬁtness landscapes rarely possible given size typical search spaces So needs condense useful information ﬁtness landscapes numeric descriptors In 41 Jones introduced descriptor problem diﬃculty GAs ﬁtness distance correlation fdc The study fdc extended GP 4245 These studies fdc reliable indicator problem hardness However big ﬂaw requires optimal solutions known This prevents use fdc estimate problem diﬃculty practical applications A measure suffer problem negative slope coeﬃcient nsc recently proposed 46 This based concept ﬁtness cloud scatter plot parentoffspring ﬁtness pairs The nsc uses idea ﬁrst dividing cloud certain number bins parentﬁtness axis computing mean offspring ﬁtness bin ﬁnally analysing changes slope adjacent bins resulting histogram The nsc shown reliable measure number different 1256 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 benchmark problems GP 4648 A slightly modiﬁed version nsc called ﬁtness proportional nsc given good results GAs 49 While fdc nsc measures problem diﬃculty provide reasonable indications problem hard easy direct estimation hard easy problem relation particular performance measure success rate EA expected endofrun ﬁtness To best knowledge approach proposed achieves EPAs paper attempts ﬁll precisely important gap knowledge However small number approaches including source inspiration work presented achieved good degree success predicting actual performance areas EAs So brieﬂy review Precise bounds expected run time variety EAs obtained computational complexity tech niques 5056 Typically speciﬁc classes functions exceptions runtime bounds derived general combinatorial optimisation problems 5762 At opposite end generality spectrum No Free Lunch NFL theorem 63 shows averaged possible algorithms problems diﬃculty averaged possible problems algorithms eﬃciency The implications applicability NFL clariﬁed 6467 While NLFtype results important limit achieved search algorithm performancewise provide indications performance algorithm class problems closed permutation 66 However know function program induction artiﬁcial problem classes closed permutation 6870 The situation better continuous optimisation reasonably clear understanding haviour performance essentially Evolutionary Strategies 71 applied particularly simple functions spheres ridges Markov chain models continuous state spaces deﬁned general results ob tained 72 However complexity calculations involved makes impractical analysis performance continuous optimisers These studied discrete Markov chain models approximate arbitrary continuous problems precision 73 While promising clear extend work 7273 EPAs given explore space discrete structures hybrid discrete space contin uous subspaces corresponding realvalued numerical constants embedded In principle possible apply Markov chains predict performance EPAs realvalued constants recently developed models 18 models immense making application practical problems effectively impossible The simple approach 74 performance GA modelled surprisingly reliable results partic ularly inspired work presented paper The idea selection based comparing ﬁtness different solutions case performance EA depends relative ﬁtness values The ﬁtness function rerepresented comparison matrix The matrix represents outcomes possible comparisons pairs solutions selection mechanism need perform So value element j matrix sign f f j f ﬁtness function Because matrix skewsymmetric relevant information stored upper triangle By reading elements upper triangle obtain vector v v 1 v 2 called information landscape represents information EA need f perform search Thus performance EA particular problem function v Clearly function expected nonlinear complex practically impossible derive ﬁrst principles algorithm complexity However 74 obtained good results modelling performance GA simple function P v a0 cid2 ai v 1 ai coeﬃcients These applying squares method training set containing suﬃciently large set v P v tuples obtained running GA variety problems recording associated performance In principle comparisonbased algorithm modelled way Also potentially users free choose performance measure However technique scale size n search space number elements v nn1 Thus uniquely identify coeﬃcients ai needs training set containing nn1 1 problemperformance pairs Also stochasticity GAs performance evaluation problem 2 typically requires gathering statistics multiple runs Thus construction training set suitable application method practically impossible small search spaces1 2 1 As reviewers observed n small enumerating n objects feasible optimisation problem interesting solved trivial way This implies small problems modelling performance algorithms simplest random search enumeration essentially academic exercise complex algorithms unlikely trivial cases However search space large search spaces consider paper performance models important M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1257 3 Modelling EPA performance In section performance model arrived We start considering applicability information landscapes program spaces 31 Information landscapes search program spaces Inspired informationlandscape idea 74 summarised Section 2 tried model performance GP v vector rerepresentation ﬁtness function Eq 1 Unfortunately detailed study problems limit applicability original information landscape modelling technique EPAs The ﬁrst issue scalability informationlandscape approach programinduction particularly prob lematic relatively slow ﬁtness evaluations associated typical induction regression problems In fact information landscape technique applied program induction principle size program search spaces inﬁnite So required v vector inﬁnitely dimensional inﬁnitely large training set needed determine associated coeﬃcients ai Of course practice upper bound size programs interested exploring Nonetheless problem remains typical primitive sets number distinct programs certain size grows exponentially size num ber coeﬃcients need identifying grows like square number This makes diﬃcult use information landscape approach smallest program spaces expressions p1 The size training set problem Another problem primitive sets symmetries imply syntactically different programs fact present functionality For example search space generated x y y x functionally indistinguish able If ﬁtness computed usual based behaviour programs syntactically distinct programs identical behaviours identical ﬁtness irrespective problem This translates constraints elements v vector rerepresentation ﬁtness For example p3 x v j represents comparison ﬁtness programs p1 p3 vk represents comparison programs p2 p3 case v j vk As result choice coeﬃcients j ak j ak constant produces model identical quality This makes problem identifying coeﬃcients ordinary linear regression generally illposed 75 sense unique solution More generally coeﬃcients Eq 1 associated elements v vector remain identical problems univocally determined2 x y p2 All suggests information landscape approach originally formulated unsuitable model EPAs Some modiﬁcation approach necessary overcome limitations Multiple alternatives possible In sections focus direction took work based different rerepresentation ﬁtness function We brieﬂy discuss possibility information landscapes viable EPAs 32 A sparse representation ﬁtness Let Ω program search space f ﬁtness function Ω We assume Ω ordered One represent f corresponding ordered set F Ω f p p Ω Clearly F Ω exact representation ﬁtness function So borrowing ideas information landscapes imagine estimating performance EPA linear model P f a0 cid2 pΩ ap f p 2 Note model far fewer parameters Eq 1 Nonetheless identiﬁcation ordinary linear regression problematic size typical program spaces indeterminacy resulting semantic equivalence distinct programs However Eq 2 transformed workable model accept partially represent ﬁtness function That instead F Ω representation f use F S f p p S S Ω This leads following model P f a0 cid2 pS ap f p 3 The advantage Eq 3 cardinality S control Therefore easily ensure coeﬃcients identify regression making problem wellposed keeping size training set control For reasons adopted Eq 3 stepping stone performance model 2 The problem simply caused Eq 1 linear d terms For example highly nonlinear model P v jakv j implies model j ak constant quality making choice j av v j vk ak nonunique v j vk j k cid3 1258 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 While principle approach applied domain objective model EPAs Eq 3 specialised revealing deeper structure exploit modelling purposes In particular common practice EPAs use ﬁtness function f p evaluates similar functionality program p Ω target functionality t Typically t represented ﬁnite set training inputoutput pairs cid4 cid3 called ﬁtness cases GP So ﬁtness generally computed f p i1 gpxi txi xi txi set ﬁtness cases cardinality cid3 xi set inputs txi corresponding desired output g function evaluates degree behaviour p outputs matches t ﬁtness case3 Typically set ﬁtnesscase inputs xi ﬁxed corresponding outputs txi pxi numbers ga b bk k 1 k 2 Under conditions 68 think t p cid3dimensional vectors t t1 tcid3 p p1 pcid3 respectively ti txi pi pxi We represent f p dp t d similarity measure vectors So different t vectors induce different ﬁtness measures program search space Also Eq 3 transforms P t a0 cid2 cid5 pp t cid6 ap d 4 pS P t instead P f ﬁtness determined t pp stands output produced program p tested ﬁtness cases associated problem Eq 4 specialised model performance program induction algorithms In following section generalise possible model performance EPAs searchers domains Before note F S sparse representation f Eqs 3 4 necessarily accurate Eq 2 Of course estimate makes sense ask careful choice S produce reasonable results4 We note suggested reviewers alternative approach making information landscapes viable EPAs sample use subset components v Eq 15 33 Performance models problem landscapes cid2 In program induction principle search space contain program exactly target functionality t This acts second term comparison factors dpp t Eq 4 It stands reason allowing ﬁrst term comparison pp functionality program actually Ω generalise Eq 4 obtaining P t a0 ap dp t 5 pS S subset set possible program behaviours independently program Ω implementing In particular S Rcid3 continuous regression problems S 0 1cid3 Boolean induction problems Note derived Eq 5 case p t cid3dimensional vectors reason generalise objects type structure long deﬁne similarity measure dp t With generalisation effectively Eq 5 applied problems program induction In particular interpret S subset set possible problems subset set possible programs precisely program behaviours For example S subset possible Boolean functions 4 inputs easily subset bin packing problems In case terms dp t Eq 5 effectively deﬁne problem landscape represent degree similarity problems6 This interpretation Eq 5 model explore rest paper The idea general successfully applied variety programinduction algorithms machine learners problem solvers However important question needs answering proceed Eq 5 Clearly expect different choices S produce models different accuracy So choose elements S The process adopted work described section 3 This means free associate ﬁtness program ﬁtness computed indirectly form similarity measure This constrains number type ﬁtness functions create EPAs 4 Naturally selecting S delicate task inappropriately introduce undesired biases As discussed Section 34 successfully simple procedure involves sampling Ω followed selection step Such procedures generalised domains However conceivable cases identiﬁcation suitable S diﬃcult 5 We tested idea experiments reported space limitations class 3input Booleaninduction problems different EPAs considered paper The models produced cases inferior obtained sparse ﬁtness landscapes Nonetheless results indicated approach viable worthy exploration development future research 6 In case program induction algorithms problems programs represented structures vectors t p respectively space problems described terms desired outputs speciﬁc inputs So great deal difference Eqs 4 5 interpreting S set program behaviours set problems However performance model applied general problem solving situations important remember distinction M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1259 34 Model identiﬁcation To instantiate Eq 5 EPAs algorithms need training set problems T validation set V closeness measure d set problems Σ draw elements S The sets T S identify coeﬃ cients ap obtain good ﬁt The validation set V test generality model T V composed pairs t P t t problem EPAs target vector P t corresponding performance algorithm study stochastic algorithms estimated averaging performance multiple independent runs An important issue choice Σ For discrete domains reasonably small size option use possible elements class problems Alternatively cardinality class problems big inﬁnite construct Σ drawing representative set samples class In work decided use approach Since typically training set T needs produced sampling decided Σ T Section 4 describes simple procedures obtain T different benchmark problems We Least Angle Regression LARS algorithm 76 method decide problems T include S LARS works follows It starts setting coeﬃcients ap zero ﬁnds predictor correlated response performance measure Then takes largest possible step direction predictor predictor correlation residual At point LARS proceeds direction equiangular predictors variable correlation residual The process continues predictor incorporated model7 In version LARS stop algorithm m steps m desired size set S pick m problems T chosen algorithm far elements S In way certain retain S elements T having high correlation performance increasing accuracy model alternative ways choosing S To automate choice parameter m wrapped procedure described 5fold crossvalidation loop That T split ﬁve sets equal size sets produce model remaining set assess generalisation The process repeated 5 times time leaving different ﬁfth T resulting prediction performance problems T This procedure iterated m 1 2 T aim identifying value m provided best generalisation The overall generalisation error measured Relative iP P 2 ranges T P average iP P i2 Squared Error RSE 77 deﬁned RSE performance recorded problem P performance predicted model P average performance problems cid4 cid4 Having identiﬁed optimal set S procedure described ﬁnally ordinary squares obtain models coeﬃcients ap 4 Test problems performance measures To illustrate scope effectiveness approach considered different classes problems different algorithms performance measures Firstly tested approach problems related EPAs continuous sym bolic regression rational functions Boolean inductions problems different versions GP GEP Stochastic Iterated Hill Climber SIHC Secondly modelled artiﬁcial neural network ANN training algorithm applied Boolean induction problems Finally tested approach onedimensional offline binpacking problem modelling wellknown First Fit Decreasing FFD Best Fit Decreasing BFD heuristics We present problems rest section algorithms solve Section 5 41 EPAs problems For program induction considered radically different classes problems continuous symbolic regression Boolean function induction problems typical performance measures Best Run Fitness BRF normalised version BRF success rate Continuous symbolic regression requires ﬁnding program seen function transforms numerical inputs output ﬁts set data points In Boolean function induction problems algorithm asked ﬁnd program implements given truth table In cases sum absolute errors ﬁtness measure A benchmark set created continuous symbolic regression generating 1100 different rational functions form tx W xQ x W x Q x polynomials following procedure W x Q x built randomly choosing degrees range 2 8 choosing random real coeﬃcients interval 10 10 powers x chosen degrees Each rational functions set sampled 21 7 We decided use LARS greedy model selection algorithms like forward selection allpossiblesubsets regression Moreover simple modiﬁcations implement Lasso forward stepwise linear regression algorithm like test future reﬁnements research 1260 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 points uniformly distributed interval 1 1 resulting target vector t R21 number ﬁtness cases range chosen based typical values GP literature For symbolic regression problems t vector performed 100 independent runs recording BRFs The corre sponding P t value obtained averaging BRFs We computed normalised version BRF NBRF short involved ﬁrst normalising target vector behaviour best individual run summing absolute differences components normalised vectors8 We created benchmark set 1100 random problems class 4input Boolean induction problems Each problem represented vector length 16 truth table So case t 0 116 As performance measure algorithm took success rate fraction successful runs 100 independent runs Each benchmark set 1100 random problems divided sets training set T composed 500 problems validation set V comprising remaining 600 problems 42 Artiﬁcial neural network problems We Boolean induction problems test approach modelling learning algorithm ANNs The job ANN learn functions 3 inputs 2 outputs These represented 8 2 truth tables We selected 1100 tables randomly replacement 500 went training set T remaining 600 formed validation set V For problem benchmark set ANN iteratively trained 8 possible different inputs patterns Training continued mean square error outputs 005 The training process repeated 500 times starting random sets weights biases The average number epochs required train network taken performance learning algorithm problem The ANN domain differs program induction important ways typically ANNs multiple outputs simplicity considered case programs output b functionality ANNs determined topology weights biases activation functions sequence instructions However point view modelling performance domains similar Like programs behaviour ANNs represented collection outputs produce response set input patters Thus linearise 8 2 table representing ANNtraining problem reading elements column column row row obtaining vector t 0 116 directly use Eq 5 model performance ANN training algorithms 43 Onedimension offline bin packing The objective bin packing pack certain number items smallest possible number bins bins content overﬂows The main difference domains described bin packing problems algorithm outputsbehaviours live space In bin packing solution problem assignment items bins problem list detailing size item packed Also typically predeﬁned target behaviour form assignment measure similarity output produced algorithm This suggests modelling bin packers Eq 5 best interpret terms p t problems The natural representation binpacking problems list item sizes However offline version problem considered order items list unimportant solver freely choose order bin For reason represented problems histograms indicating items size needed packing The number slots histogram item sizes upperbounded size largest possible item packed Since experiments item sizes integers 1 100 problems represented histograms including 100 slots Such histograms represented 100dimensional vectors We created random binpacking problems procedure presented 78 All problems required packing 1000 items We considered ﬁve problem classes following ranges item sizes 1100 1090 2080 3070 4060 For range 800 different histograms created 400 training set remaining 400 included validation set If B min Bmax minimum maximum size items problem class respectively histograms followed multinomial distribution success probabilities given pi 1 BmaxBmin1 Bmin Bmax pi 0 In total training set consisted 2000 different problems validation set included 2000 problems The performance binpacking algorithm number bins pack items Since considered deterministic algorithms performed run problem 8 If μ σ mean standard deviation elements ti t respectively elements normalised target vector set ti μ σ The normalised programbehaviour vectors similarly obtained shifting scaling components p M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1261 Table 1 Parameters primitives GP experiments Only combinations pxo pm pxo pm cid2 100 crossover mutation mutually exclusive operators GP protected avoid divisions 0 AND OR NAND NOR x R x1 x2 x3 x4 100 90 50 0 100 50 0 4 Tournament size 2 roulettewheel 1000 50 100 Function set rational problems Function set Boolean problems Terminal set rational problems Terminal set Boolean problems Crossover rate pxo Mutation rate pm Maximum tree depth mutation Selection mechanism Population size Number generations Number independent runs Table 2 Parameters GEP experiments Function set rational problems Function set Boolean problems Terminal set rational problems Terminal set Boolean problems Head length Number genes Mutation rate pm 1 point recombination rate 2 point recombination rate Gene recombination rate IStransposition rate IS elements length RIStransposition rate RIS elements length Gene transposition rate Selection mechanism Population size Number generations Number independent runs protected AND OR NAND NOR x R x1 x2 x3 x4 63 3 5 20 50 10 10 1 2 3 10 1 2 3 10 Tournament size 2 roulettewheel 1000 50 100 5 Systems parameter settings 51 GP systems We different implementations GP treebased subtree crossover subtree mutation One essentially identical Koza 6 signiﬁcant difference selected crossover mutation points uniformly random 6 nonuniform distribution The TinyGP 79 modiﬁcations presented 19 allow evolution constants The main difference Kozas generational selected individuals population reproduce parallel create generation TinyGP uses steadystate strategy offspring immediately inserted population waiting generation completed Table 1 shows parameters primitives With generational GP traditional roulette wheel selection gives parents probability chosen proportional ﬁtness tournament selection sorts random set individuals based ﬁtness picks best So total tested variants GP generational tournament selection generational roulettewheel selection steadystate tournament selection 52 GEP systems The performance GEP preliminary runs worse GP We mainly standard GEP initialisation method Thus replaced technique equivalent rampedhalfandhalf method 6 obtaining considerable performance improvement We different versions GEP generational tournament selection generational roulettewheel selection steadystate tournament selection The parameters primitives GEP runs shown Table 2 There important differences GP GEP Firstly GP programs represented trees vary size shape GEP uses ﬁxedsize linear representation components head contain functions terminals tail contain terminals Secondly treebased GP operators act trees modifying nodes exchanging subtrees GEP genetic operators similar GAs having 1262 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 Table 3 Parameters SIHC experiments Mutation Subtree Uniform Maximum number mutations Maximum number ﬁtness evaluations 50 500 1000 25000 25000 50000 50000 comply constraints primitives allowed head tail Finally GP transformation required ﬁtness program tree computed GEP ﬁxedlength representation transformed tree structure ﬁrst This picking primitives ﬁxedlength string inserting tree following breadthﬁrst order These differences GP GEP translate radically different behaviours performance classes EPAs 53 SIHC systems We Stochastic Iterated Hill Climber similar presented 80 different mutation operators subtree mutation type mutation GP runs mutation operator uniform mutation node tree mutated certain probability A node mutated replaced children b random node arity c randomly generated tree includes tree originally rooted mutated node subtree SIHC starts creating random program tree procedure primitives GP GEP ex periments SIHC mutates program ﬁtter This replaces initial program mutation process resumed When maximum number allowed mutations reached individual set aside new random individual created process repeated SIHC terminates solution maximum number ﬁtness evaluations reached Table 3 shows parameters SIHC experiments 54 ANN bin packing heuristics The ANN exercise training algorithm fully connected feedforward network 3 layers 7 hidden neurons The activation function symmetric sigmoid The algorithm train iRPROP 81 The initial weights biases network randomly uniformly chosen range 01 01 For case bin packing wellknown humandesigned heuristics First Fit Decreasing FFD Best Fit Decreasing BFD Both work ﬁrst sorting items nonincreasing size Then FFD places item ﬁrst bin ﬁt BFD places items ﬁt tightly We different bin sizes 100 1509 6 Results 61 Not similarity measures equal EPA performance modelling Eq 5 derived considering GP problems ﬁtness function f effectively distance However having extended interpretation model Section 33 clear free choose similarity measure d coincide f It makes sense compare quality models obtained different d measures cid4 Table 4 shows 5fold crossvalidation RSE models varies GP systems different d functions For symbolic regression RSEs computed BRF NBRF performance measures success rate Boolean induction The quality models depends d For case rational problems BRF ln1 ti pi ti pi best overall performance evaluated NBRF closeness measure ti pi close second For case Boolean induction problems p t2 provided lowest RSE values best Similar results obtained program induction systems described Section 5 So rest paper use optimal similarity measure identiﬁed class problems performance measure algorithms cid4 cid4 62 Performance models EPAs Table 5 presents accuracy comparison terms RSE performance models GP GEP SIHC systems presented Section 5 The lowest RSE values associated generational systems roulettewheel selection BRF measure However virtually cases RSEs small training validation sets 1 worst models able predict performance better mean 9 When size bins 150 heuristics gave performance experiments present results FFD M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1263 Table 4 Quality models GP crossover rate 90 mutation different closeness measures BRF best run ﬁtness NBRF normalised BRF Conﬁguration Type Selection Generational Roulette d cid7 1 cid4 n Generational Tournament Steady state Tournament Rational functions Boolean functions BRF S 26 55 5 1 1 399 12 340 6 7 3 67 25 110 2 5 1 100 RSE 00292 00215 01197 10862 10012 05621 03290 01577 05269 08931 10020 03570 05168 03148 09076 12406 10029 02949 Normalised BRF S RSE Success rate S 251 131 48 1 390 126 227 153 45 1 380 125 310 113 52 22 384 133 05261 04980 06251 10010 05780 04895 04679 03658 06078 10027 05408 03593 04567 04554 05640 09988 05954 04524 399 1 126 168 399 1 394 1 118 150 399 1 399 1 104 128 399 1 RSE 04629 10102 03007 03736 08045 10102 05788 10087 04158 04876 08993 10087 07303 10100 06117 06822 09245 10100 cid4 ti pi 2 ti pi t p2 t p3 exp cid9t pcid92 cid4 ln1 ti pi cid4 ti pi 2 cid7 1 cid4 n ti pi t p2 t p3 exp cid9t pcid92 cid4 ln1 ti pi cid4 ti pi 2 cid7 1 cid4 n ti pi t p2 t p3 exp cid9t pcid92 cid4 ln1 ti pi Table 5 Quality model RSE different GP GEP SIHC systems parameter settings The models optimal closeness measures identiﬁed Section 61 Conﬁguration Type Selection pxo pm Rational functions Best run ﬁtness S T set Generational Roulette Generational Tournament Steady state Tournament Sys SIHC Mut Subtree Unif 100 090 050 000 100 090 050 000 100 090 050 000 GEP GEP 000 000 050 100 000 000 050 100 000 000 050 100 GEP Max Mut 50 500 1000 25000 25000 47 55 43 58 180 122 340 18 112 99 106 110 100 112 14 S 170 150 220 193 93 00123 00062 00193 00179 00005 00065 00003 01503 00107 00048 00193 00182 00181 00138 01787 T set 00054 00046 00026 00041 00192 V set 00209 00267 00243 00359 00101 02683 05857 08291 04009 01270 06224 04300 04168 03549 05608 V set 01674 04079 08974 02579 03437 Normalised BRF S T set 127 126 143 128 118 137 125 160 167 129 131 133 130 132 126 S 114 113 93 84 130 02304 02251 01832 01986 02375 01612 01794 01382 01352 01877 02067 02092 02286 02331 01845 T set 02091 01934 02361 02510 02044 V set 05246 05375 04999 04907 05212 04082 04257 04130 04291 04477 05778 05634 05967 06367 04243 V set 04349 04540 04378 04587 04890 Boolean functions Success rate S T set 128 126 127 125 129 118 117 116 121 124 116 104 114 109 88 S 118 121 122 125 118 01640 01510 01505 01712 01969 02216 02530 02413 02760 02424 03501 03531 03735 04159 03966 T set 02294 02109 02066 01786 02174 V set 02877 02962 02833 03058 03745 04065 03941 04010 04686 04501 05401 05820 06379 06336 05512 V set 04045 03989 03787 03120 03641 RSE ﬁgures provide objective indication model quality However diﬃcult appreciate accuracy models ﬁgures To clearer illustration Fig 1 shows scatter plots actual performance GP vs performance estimates obtained model training validation sets symbolic regression Boolean induction The solid diagonal line plot represents behaviour perfect model The fact data form tight clouds line clear qualitative indication accuracy models Other systems parameter settings provided similar results 1264 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 Fig 1 Scatter plots performance measured vs performance obtained model continuous regression problems BRF b normalised BRF c d performance measures Boolean induction problems e f training set validation set The data refer GP 90 crossover rate mutation roulettewheel selection 63 Performance models ANN training Table 6 shows quality models ANN training resulting use different similarities measures The table reports RSE obtained cross validation column 2 RSE obtained training validation M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1265 Table 6 Quality models ANN training different closeness measures Closeness measure cid7 cid4 ti pi 2 1 cid4 n ti pi t p2 t p3 exp cid9t pcid92 cid4 ln1 ti pi S 399 7 134 189 399 7 RSE 05166 09954 03711 04798 08212 09954 T RSE 00011 09411 01814 01220 00016 09411 Table 7 Quality model different heuristics offline bin packing problem Name FFD FFD BFD Size bins 150 100 100 S 1315 828 855 Crossval RSE 00031 01405 01342 T RSE 00002 00414 00377 V RSE 04846 10004 03567 04502 07953 10004 V RSE 00028 01413 01361 Fig 2 Scatter plots actual performance epochs vs performance estimated model ANN training problems sets columns 3 4 respectively The d function best crossvalidation RSE t p2 corresponds lowest RSE validation set RSE values suggest model produced good predictions Fig 2 reports scatter plot actual performance vs performance estimated model d t p2 This shows model able accurately predict actual performance problems Only problems training took longer 15 epochs model signiﬁcantly underestimated performance The reason problems requiring high number learning epochs benchmark set10 64 Performance models bin packing algorithms We tested quality models obtained different closeness measures varied bin packing We sum absolute differences produced models lowest RSE value Thus adopted experiments reported Table 7 presents RSE values FFD BFD heuristics bin sizes In cases models predicted accurately performance binpackers generalised The size S considerably bigger models systems likely larger number degrees freedom 100 associated bin packing problems We scatter plots actual performance vs estimated performance validation set FFD Fig 3 Again data closely follow perfectmodel line 10 If accurate prediction performance rare cases long runs important user correspondingly bias training set ensure overrepresented accurately modelled 1266 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 Fig 3 Scatter plots actual performance vs estimated performance validation set FFD offline bin packing algorithm bin sizes 7 Performance models algorithm selection problem The process solving problem starts looking different procedures available job deciding apply In automated problem solving known algorithm selection problem 25 Here consider applicability models context algorithm selection problem We interested techniques solve problem rely form prediction algorithm performance bear similarities approach However highlight important differences 71 Algorithm selection problem Most approaches solving algorithm selection problem require set ingredients 82 1 large col lection problem instances variable complexity 2 diverse set algorithms having best performance instances 3 measure evaluate performance algorithms 4 set features properties problems collection These elements necessary approaches use machine learning techniques predict algorithm collection best performance starting description problem solved Of course work features 4 algorithms similar performance problems similar features The methods solve algorithm selection problem divided groups depending decision strategy use solve problem In dynamic selection decision run algorithm static selection decision taken search starts One algorithm uses prediction model guide search process dynamically STAGE 83 STAGE effectively hill climber intelligent restarts After hill climber search STAGE uses points sampled produce update prediction model linear regression heuristic The heuristic suggests promising point restart hill climber STAGE successfully tested SAT problems Note models constructed run STAGE predict performance problem instance trying solve Following similar idea 84 prediction model linear regression decide path follow search tree The approach tested Knapsack problems set partitioning problems In 85 algorithm selection problem SAT modelled Markov decision process MDP The DavisPutnam LogemannLoveland algorithm enhanced model decide node search tree branching rule use Similarly 86 Bayesian model created predict run time algorithm dynamically use prediction decide search restarted Also suggested 87 different recursive algorithms available solve problem modify way dynamically recursive Using MDP decide function gave good results sorting algorithms order statistic selection problem Staticselection methodologies instead alter course algorithm ﬁrst decide procedure use wait chosen algorithm terminates A substantial body work falls category attempts predict runtime algorithms matrix multiplication 8889 sorting 908889 solving partial differential equations 90 signal processing 91 The objective choosing algorithms eﬃcient implementation based factors computers architecture workload instance size Methodologies predicting runtime SAT solvers based linear regression 29 combination linear regression mixtureofexperts 3335 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1267 proposed Techniques based linear regression 2627323130 solve auction winner determination problem casebased reasoning constraint programming 9294 Algorithm portfolios 2635 particularly important class static algorithm selection techniques An algorithm portfolio collection algorithms run parallel sequence solve particular problem Given new problem instance performance models rank algorithms The ranking algorithms executed stopping soon solution Because way algorithms good matches problem instance average performance portfolio set instances better average performance algorithm portfolio Performance models identify sets particularly hard problems algorithm 2627 32 When attacking algorithm selection problem normally features problem performance model obtained expert careful analysis literature particular class problems problemspeciﬁc In 95 features obtained automatically preliminary runs algorithms collection problem instance Solutions different times runs algorithms collected form feature set Following similar approach 96 preliminary runs measure expected time algorithm spend processing node search tree Using Knuths method estimating size search tree possible predict runtime different algorithms 72 Similarities differences performance model The algorithm selection problem approached form machine learning technique predict perfor mance collection algorithms match problems algorithms similar performance models However methodology presents signiﬁcant differences respect prior work algorithm selection Firstly focus primarily program induction speciﬁcally EPAs seen method extends naturally domains This area neglected prior work algorithm selection Secondly charac terise problems features radically different prior work This mainly focused use problemspeciﬁc features experts problem domain consider useful measure hardness problem These features work selection require considerable domain knowledge effectively relies previous attempts characterise hardness Also idea computing features based preliminary runs algo rithms modelled works However method produces models lack generality having derived speciﬁc problem instance Instead Eq 5 features simply measure similar problem set reference problems automatically identiﬁed LARS crossvalidation Therefore features generic problemspeciﬁc models algorithms obtain applicable classes problems single instances Thirdly use performance models predict performance elicit important knowledge similarities differences algorithms models Section 8 73 Programinduction portfolios Despite good attempts Section 2 far researchers relatively little success practical charac terisation diﬃculty program induction This effectively prevented extension work portfolios domain The good results obtained models suggest allow extension We decided attempt develop algorithm portfolios symbolic regression Boolean induction As suggested 26 algorithms forming portfolio behave differently different problems Also algorithm beat algorithms portfolio problems So decided form portfolios subset 20 program induction algorithms considered Section 6 To determine algorithms consider insertion portfolio looked performance problems training set considered algorithms best performance problem This resulted exclusion 13 algorithms symbolic regression 2 algorithms Boolean induction To decide remaining algorithms include portfolio crossvalidation technique equivalent Section 34 We started creating portfolio having algorithm resulted best biggest number problems training set Then added algorithm overall second training set We predictions crossvalidation decide algorithms problems training set simulated running predicted best algorithm problem averaged resulting performance values estimate performance portfolio training set11 We added fourth best algorithm portfolio repeating phases algorithms consideration included This procedure repeated closeness measures d described Table 4 plus SokalSneath similarity measure steadystate GEP gave considerably better results function This allowed select portfolios d functions lowest RSEs 11 Since run algorithms problems create training set phase amounted simple look operation 1268 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 Fig 4 Performance algorithm portfolio training set different criteria select algorithms The performance model uses crossvalidation training set choose algorithms The perfect model decides based performance measured actual runs best model algorithm having best measured average performance problems training set Fig 4 illustrates performance algorithm portfolios constructed methodology The performance model curves portfolios performance crossvalidation conjunction model decide algorithm portfolio use problem training set The perfect model curves portfolios performance obtained perfect model performance available The best algorithm curves present performance portfolio select algorithm best average performance training set For reference provide mean portfolio curves obtained averaging performance algorithms composing portfolio indicate performance expect choice algorithm portfolio random Since ordered algorithms based performance training set surprising add algorithms portfolios average portfolios performance represented mean portfolio curves decreases Looking performance model curves cases portfolios based model better performance best algorithm As increase number algorithms portfolios performance rapidly peaks remains stable Boolean induction slightly decreases symbolic regression12 This slightly surprising expected algorithms available portfolio higher chances ﬁnding good match problem better performance The reason performance model curves peak portfolio size simply gains provided larger portfolio terms increased ability match algorithms problem offset increasing risk making incorrect decisions chances picking suboptimal algorithm So needs compare performance model mean portfolio curves Such comparison reveals improvement selection ability provided models signiﬁcant irrespective size portfolio fact increases selection problem harder We tested portfolios identiﬁed crossvalidation validation set generalised As shown Fig 5 portfolio performance obtained choosing algorithms models second achievable perfect model course expect match However symbolic regression Boolean induction problems pairwise differences performance selecting algorithms performance models best algorithm portfolio statistically signiﬁcant onesided twosample KolmogorovSmirnov test reporting p values 001 8 Eliciting knowledge performance models 81 Comparing algorithms parameter settings When considering different algorithms solve problem important understand similarities differences behaviour algorithms It reasonable attempt infer similarities differences comparison performance models 12 The best performance symbolic regression rational functions obtained portfolio composed algorithms TinyGP 100 mutation SIHC 25000 maximum mutations subtree mutation Boolean induction best portfolio included algorithms TinyGP systems pxo pm 100 steady state GEP M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1269 Fig 5 Mean performance algorithm portfolios validation set Error bars represent standard error mean cid10cid10 p1 cid10 p1 We start noting Eq 5 represents hyperplane clariﬁed rewriting normal form 1 ap1 apS P t dp1 t dpS t x0 0 scalar product p1 pS elements S x0 a0 0 0 point hyperplane depends term a0 Then measure sim cid10 pS represented vector ilarity algorithm characterised vector n cid10cid10 1 n cid10cid10 pS simply computing angle α arccos n cid10 1 Unfortunately sets S independently chosen different models principle ith coeﬃcient models hyperplane associated problem ith coeﬃcient models hyperplane associated different problem To circumvent problem following slightly sophisticated procedure Let Scid10 sets reference vectors associated performance models want compare The models cid10cid10 include corresponding sets coeﬃcients pScid10cid10 respectively We construct new sets cid10 pScid10cid10 obtained rerunning linear regression training set associated subscripts range 1 Scid10cid10 second cid10 cid10 pScid10 coeﬃcients b ﬁrst model reference vectors Scid10cid10 cid10cid10 b p1 1 b cid10 cid10cid10 pScid10 obtained symmetrically We deﬁne vectors b b cid10cid10 1 b Thus deﬁne angle dissimilarity algorithms α 1 2 cid10 p1 comparable b compare cid9acid10cid9cid9bcid10cid10cid9 arccos cid10 pScid10cid10 b b cid10cid10 1 b cid10 cid10cid10 pScid10 While b cid10 p1 cid10 pScid10 cid10 1 cid10cid10 pScid10cid10 b cid10n cid9ncid10cid9cid9ncid10cid10cid9 Scid10cid10 b b cid10cid10 p1 cid10cid10 cid10b cid10 p1 cid10cid10 p1 cid10 p1 cid10cid10 p1 cid10cid10 cid10cid10 cid10cid10 cid10cid10 cid10cid10 cid10 cid10 cid10 cid10cid10b cid9acid10cid10cid9cid9bcid10cid9 arccos If angle algorithms small algorithms reasonably expected produce similar performance problems If algorithm succeeds problem likely succeed vice versa So decide favour faster algorithm If angle algorithms big expect problems performance algorithms differs Upon failure solve problem algorithm hope solve 82 Toward automated taxonomies In presence algorithms build matrix collecting angles pairs algo rithms consideration infer useful information mutual relationships However considering algorithms comparison matrix large manually ﬁnding interesting patterns diﬃcult Here propose simple automated procedure aid complement manual analysis To exemplify approach focus 20 GP GEP SIHC systems presented Section 5 We start feeding pairwise comparison matrix clustering algorithm group systems based similarity performance More speciﬁcally adapted hierarchical clustering algorithm 97 create clusters The al jY Mi j gorithm works follows Firstly chose following similarity measure pairs clusters sX Y M matrix containing average similarity pairs algorithms study denotes num ber elements cluster X Y clusters More speciﬁcally M 1 3 MBRF MNBRF MB matrices MBRF MNBRF MB obtained performing pairwise comparisons 20 program induction systems different performance measures best d functions associated Then performed X Y iX cid4 cid4 1270 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 following steps 1 assigned separate cluster 2 new cluster created merging closest clusters based sX Y reducing number clusters 3 repeated step 2 cluster left This resulted cluster hierarchy 20 levels To simplify interpretation decided focus 8 topmost clusters To visualise clusters treated nodes fullyconnected graph graphdrawing package neato GraphViz library obtain draw graph layout pairs nodes corre sponding clusters high similarity placed closer clusters corresponding systems dissimilar performancewise The strategy neato position nodes graph interpret physical bodies connected springs edges graph The user set springs stiffness rest length To produce layout virtual physical initialised suboptimal conﬁguration iteratively modiﬁed relaxes state minimal energy In work associated edge length proportional sX Y nodes pushed apart producing graphs layout proportionally dissimilarity We set stiffness springs formula 1001 sX Y Fig 6 shows output produced neato The edges clusters ﬁgure represented dashed lines edges connecting systems mother cluster drawn solid lines We think diagram taxonomy systems study To distinguish different forms selection reproduction type mutation different symbols indicated left ﬁgure From ﬁgure steady state GP systems grouped cluster left ﬁgure The generational GP systems tournament selection arranged cluster left At ﬁnd cluster containing SIHCs subtree mutation The generational GP systems roulettewheel selection grouped according crossover mutation dominant operator More speciﬁcally ﬁgure ﬁnd cluster containing generational GP systems mutation Just cluster generational GP systems 50 100 mutation Finally surprisingly GEP systems placed separate cluster middle right ﬁgure indicating systems different performancewise Overall taxonomy suggests type selection reproduction bigger impact performance crossover mutation rates This evident clusters formed steady state GP systems generational GP systems tournament selection generational GP systems roulettewheel selection The taxonomy suggests reproduction strategy important determining behaviour GEP systems For SIHCs taxonomy indicates type mutation important maximum number mutations Surprisingly taxonomy suggests SIHC uniform mutation similar generational GP systems tournament selection hardly guess looking structural similarities algorithms13 Given systems grouped based performance similarity reasonable expect particular consistently fails solve problem eﬃcient try alternative systems different cluster ﬁnely optimise parameters ﬁrst This improve performance satisfactory For reasons algorithm portfolios pick best n algorithms look independent performance algorithms portfolios good coverage performance space expected generalise better 83 What knowledge extract measuring performance empirically In section compare learnt analysing performance models users program induction systems able learn traditional approaches These typically consist computing formance statistics sets test problems systems parameter settings comparison number independent runs We followed approach construct Table 8 reports performance GP GEP SIHC systems parameter settings considered paper rationalfunction Booleanfunction testbeds Statistics collected running 1100 different problems training validation sets problem class Performance estimated averaging results 100 independent runs This required total 4400000 runs As seen table SIHC subtree mutation maximum 25000 mutations restarts best performance rational functions problems irrespective BRF NBRF measures TinyGP 100 crossover best performance Boolean induction problems Also large performance differences roulettewheel selection tournament selection generational steady state systems These statistically signiﬁcant For generational GP rational problems mean standard devia tion performance measure decrease mutation rate increases suggesting differences behaviour highmutation highcrossover search modes However differences performance observed crossover mutation rates varied statistically signiﬁcant 13 Whether similarities differences present classes problems considered origin needs explored future research possibly alternative means M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1271 Fig 6 Taxonomy GP GEP SIHC systems different parameter settings based performance models In words like analysis based performance models data suggest systems problems studied changing selection mechanism bigger effect varying crossover mutation rates However variety phenomena able capture Section 82 simple performance statistics reveal This mean course information provided models corroborated empirically It require targeted empirical analyses To illustrate Table 9 report average Pearsons correlation coeﬃcients obtained comparing performance results pairs programinduction algorithms problems performance measures Table 8 Careful inspection table conﬁrms relationships highlighted taxonomy This includes unexpected ﬁnding generational systems tournament selection similar steady state systems tournament selection generational systems roulettewheel selection SIHC uniform mutation similar generational GP systems tournament selection different SIHC subtree mutation Why inferred Table 8 Simple subset problems subset opposite means standard deviations performance able tell systems apart 1272 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 Table 8 Standard experimental results GP GEP SIHC systems study Conﬁguration Rational functions Boolean functions Type Selection pxo pm Best run ﬁtness Normalised BRF Success rate Generational Roulette Generational Tournament Steady state Tournament Sys SIHC Mut Subtree Unif 9 Conclusions 100 090 050 000 100 090 050 000 100 090 050 000 GEP GEP 000 000 050 100 000 000 050 100 000 000 050 100 GEP Max Mut 50 500 1000 25000 25000 Mean 65635 66153 47502 42718 64979 25828 25341 23552 22864 41644 08576 08720 08682 08856 21178 Mean 12440 11943 10773 06816 14295 Std Dev 274912 280177 176580 153276 205852 58401 57355 54806 53267 107753 17970 17886 18666 19348 45666 Std Dev 25780 24902 24326 12105 32838 Mean 04658 04652 04516 04462 04816 04003 03977 03916 03878 04515 02535 02535 02511 02494 03580 Mean 02430 02419 02328 02021 02978 Std Dev 01146 01150 01176 01189 01109 01139 01141 01140 01126 01128 00860 00857 00817 00800 01101 Std Dev 00789 00785 00748 00686 00970 Mean 06554 06575 06693 06891 04869 08136 08094 08192 08327 04983 08518 08416 08455 08437 07894 Mean 04000 04143 04298 05326 07370 Std Dev 02374 02386 02326 02237 03573 01671 01720 01618 01504 03735 01333 01375 01329 01320 02445 Std Dev 02185 02265 02305 02460 02317 We presented set techniques build eﬃcient accurate models performance problem solvers We modelled versions GP multiple parameter settings versions GEP versions SIHC multiple parameters settings ANN learning algorithm binpacking heuristics These algorithms applied following problems symbolic regression rational functions Boolean induction offline bin packing Many applications possible models They determine best algorithm problem shown Section 7 obtained algorithm portfolios EPAs signiﬁcantly better average performance best overall algorithm portfolio As showed Section 8 reveal similarities differences algorithms problem classes build highlyinformative algorithm taxonomies Our taxonomy EPAs example provided numerous new ﬁndings including EPAs studied little sensitive choice genetic operator rates reproduction selection strategies inﬂuence performance signiﬁcantly Only information readily provided standard empirical analyses interesting relationships identiﬁed use models taxonomies important empirical results perform adhoc runs corroborate relationships Table 9 A difference models approaches modelling EAs models simply accurately predict performance algorithms unseen problems class training set drawn Other techniques ﬁtness distance correlation fdc negative slope coeﬃcient nsc predict particular problem hard easy precisely hard easy Section 2 Also approach allows user choose performance measure want model fdc nsc dont The execution models involves extremely low computational load Of course instantiation requires run ning possibly multiple times suitably large training set problems However cost similar required compare performance different algorithms empirically The difference reliable models constructed test performance new problems empirical testing requires effectively rerunning systems new problem behave The main difference models approaches solve algorithm selection problem models require manual selection sets features problems approaches features typically deﬁned expert problemspeciﬁc Instead models use concept closeness problem performance estimated reference problems previously automatically selected set S This makes easy apply approach different classes problems An important similarity models algorithm selection techniques linear functions features One wonder simple models work Firstly types models features related hardness problem In case models d functions measure similarity square dot product dissimilarity sum absolute differences input problem Table 9 Pearsons correlation coeﬃcient performance results obtained 1100 symbolic regression problems 1100 Boolean induction problems averaged performance measures BRF NBRF success rate Conﬁguration Generational Roulette Generational Tournament Steady state Tournament SIHC Type Selection Generational Roulette Type Selection Generational Tournament Type Selection Steady state Tournament pxo 100 090 050 000 GEP pxo 100 090 050 000 GEP pxo 100 090 050 000 GEP Sys SIHC Mut Subtree Unif Max Mut 50 500 1000 25000 25000 pxo 100 100 099 098 098 086 086 087 086 086 083 076 077 077 078 074 077 078 081 070 083 090 050 000 099 100 098 098 086 086 087 086 086 083 076 077 077 078 074 077 078 081 070 083 098 098 100 099 087 089 089 089 089 085 080 080 080 081 077 080 081 084 073 087 098 098 099 100 086 089 089 089 089 084 080 081 081 082 077 080 081 084 073 087 GEP 086 086 087 086 100 078 078 077 077 097 069 069 069 070 086 071 072 074 064 075 pxo 100 086 086 089 089 078 100 098 097 097 082 092 092 091 091 083 081 081 080 080 094 090 050 000 087 087 089 089 078 098 100 097 097 082 092 092 092 091 084 081 082 080 080 094 086 086 089 089 077 097 097 100 098 082 093 093 093 093 084 081 082 080 080 095 086 086 089 089 077 097 097 098 100 082 093 093 094 093 085 081 081 080 079 095 GEP 083 083 085 084 097 082 082 082 082 100 074 074 074 074 090 073 075 075 068 079 pxo 100 076 076 080 080 069 092 092 093 093 074 100 097 097 096 080 080 080 078 081 094 090 050 000 50 500 1000 25000 25000 GEP Subtree Mut Unif 077 077 080 081 069 092 092 093 093 074 097 100 097 096 079 080 080 079 081 094 077 077 080 081 069 091 092 093 094 074 097 097 100 097 080 078 078 077 078 093 078 078 081 082 070 091 091 093 093 074 096 096 097 100 080 077 077 076 076 092 074 074 077 077 086 083 084 084 085 090 080 079 080 080 100 070 071 070 068 082 077 077 080 080 071 081 081 081 081 073 080 080 078 077 070 100 096 096 095 084 078 078 081 081 072 081 082 082 081 075 080 080 078 077 071 096 100 096 093 084 081 081 084 084 074 080 080 080 080 075 078 079 077 076 070 096 096 100 092 084 070 070 073 073 064 080 080 080 079 068 081 081 078 076 068 095 093 092 100 083 083 083 087 087 075 094 094 095 095 079 094 094 093 092 082 084 084 084 083 100 M G r f f R P o l A r t ﬁ c l I n t e l l g e n c e 1 7 4 2 0 1 0 1 2 5 4 1 2 7 6 1 2 7 3 1274 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 set reference problems So properly setting magnitude sign coeﬃcients Eq 5 linear regression create scheme similarity diﬃcult reference problem leads reducing performance estimate vice versa Secondly models linear features features typically nonlinear functions degrees freedom problems representation The architecture types multilayer perceptrons radialbasis neural networks powerful function approximators So entirely surprising models ﬁt performance functions Finally like brieﬂy discuss possible future research avenues Our approach generally able accurate predictions unseen problems class training set drawn However seen case ANN learning presence rare problems requiring long training times new problems suﬃciently similar problems training set model predictions signiﬁcantly deviate actual performance In paper studied problem outliers depth By nature outliers rare obtaining statistically meaningful results model outliers require enormous computational effort We expect outliers generated mechanisms problem different assessed similarity measure d build model problems reference set S andor problem falls region performance function presents rapid changes discontinuities phase transition modelled simple kernel provided d function In future work investigate possibility detecting cases inform user model incorrect predictions andor counter measures Sizing population major step populationbased algorithms So future research intend apply models look population sizes inﬂuence performance Also shown Section 61 closeness measures produce better models It possible exist better measures ones settled paper In future research want use GP obtain predictive closeness measures Furthermore paper angles measure similarity models different algorithms In future want explore different ways measuring similarity distances hope reveal ﬁner details Also introducing regularisation terms standard technique transform illposed problems wellposed p minimised ones In future research explore beneﬁts adding terms sum squares Finally want explore performance measures problem classes approach particularly suitable unsuitable p a2 cid4 Acknowledgements We like thank editor associate editor reviewers fair useful comments ideas The paper considerably strengthened thanks feedback The ﬁrst author acknowledges support National Council Science Technology CONACyT Mexico pursue graduate studies University Essex References 1 JH Holland Adaptation Natural Artiﬁcial Systems University Michigan Press Ann Arbor USA 1975 2 DE Goldberg Genetic Algorithms Search Optimization Machine Learning AddisonWesley 1989 3 Z Michalewicz Genetic Algorithms Data Structures Evolution Programs 2nd edition Springer Berlin 1994 4 M Mitchell An Introduction Genetic Algorithms MIT Press Cambridge MA 1996 5 T Bäck DB Fogel Z Michalewicz Eds Evolutionary Computation 1 Basic Algorithms Operators Institute Physics Publishing 2000 6 JR Koza Genetic Programming On Programming Computers Natural Selection MIT Press Cambridge MA USA 1992 7 DB Fogel Ed Evolutionary Computation The Fossil Record Selected Readings History Evolutionary Computation IEEE Press 1998 8 AE Nix MD Vose Modeling genetic algorithms Markov chains Annals Mathematics Artiﬁcial Intelligence 5 1992 7988 9 MD Vose The Simple Genetic Algorithm Foundations Theory MIT Press Cambridge MA 1999 10 TE Davis JC Principe A Markov chain framework simple genetic algorithm Evolutionary Computation 1 3 1993 269288 11 G Rudolph Convergence analysis canonical genetic algorithm IEEE Transactions Neural Networks 5 1 1994 96101 12 CR Stephens H Waelbroeck Schemata evolution building blocks Evolutionary Computation 7 2 1999 109124 13 J He X Yao Drift analysis average time complexity evolutionary algorithms Artiﬁcial Intelligence 127 1 2001 5785 14 R Poli Exact schema theory genetic programming variablelength genetic algorithms onepoint crossover Genetic Programming Evolvable Machines 2 2 2001 123163 15 WB Langdon R Poli Foundations Genetic Programming Springer 2002 16 J He X Yao Towards analytic framework analysing computation time evolutionary algorithms Artiﬁcial Intelligence 145 12 2003 5997 17 R Poli NF McPhee General schema theory genetic programming subtreeswapping crossover Part II Evolutionary Computation 11 2 2003 169206 18 R Poli NF McPhee JE Rowe Exact schema theory Markov chain models genetic programming variablelength genetic algorithms homologous crossover Genetic Programming Evolvable Machines 5 1 2004 3170 19 R Poli WB Langdon NF McPhee A ﬁeld guide genetic programming published httplulucom freely available httpwwwgpﬁeld guideorguk 2008 contributions JR Koza 20 JF Miller P Thomson Cartesian genetic programming R Poli W Banzhaf WB Langdon JF Miller P Nordin TC Fogarty Eds Proceedings Third European Conference Genetic Programming EuroGP2000 LNCS vol 1802 SpringerVerlag Edinburgh 2000 pp 121132 21 M ONeill C Ryan Grammatical evolution IEEE Transactions Evolutionary Computation 5 4 2001 349358 22 C Ferreira Gene expression programming A new adaptive algorithm solving problems Complex Systems 13 2 2001 87129 23 R Poli CR Stephens The building block basis genetic programming variablelength genetic algorithms International Journal Computational Intelligence Research 1 2 2005 183197 invited paper M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 1275 24 R Poli Y Borenstein T Jansen Editorial special issue Evolutionary algorithms bridging theory practice Evolutionary Computa tion 15 4 2007 iiiv 25 JR Rice The algorithm selection problem Advances Computers 15 1976 65118 26 K LeytonBrown E Nudelman Y Shoham Empirical hardness models Methodology case study combinatorial auctions J ACM 56 4 2009 152 27 K LeytonBrown E Nudelman Y Shoham Empirical hardness models combinatorial auctions P Cramton Y Shoham R Steinberg Eds Combinatorial Auctions MIT Press 2006 pp 479504 Ch 19 28 F Hutter Y Hamadi HH Hoos K LeytonBrown Performance prediction automated tuning randomized parametric algorithms F Ben hamou Ed CP Lecture Notes Computer Science vol 4204 Springer 2006 pp 213228 29 E Nudelman K LeytonBrown HH Hoos A Devkar Y Shoham Understanding random SAT Beyond clausestovariables ratio Wallace 100 pp 438452 30 K LeytonBrown E Nudelman G Andrew J McFadden Y Shoham Boosting metaphor algorithm design F Rossi Ed CP Lecture Notes Computer Science vol 2833 Springer 2003 pp 899903 31 K LeytonBrown E Nudelman G Andrew J McFadden Y Shoham A portfolio approach algorithm selection G Gottlob T Walsh Eds IJCAI Morgan Kaufmann 2003 pp 15421543 32 K LeytonBrown E Nudelman Y Shoham Learning empirical hardness optimization problems The case combinatorial auctions PV Hentenryck Ed CP Lecture Notes Computer Science vol 2470 Springer 2002 pp 556572 33 L Xu F Hutter HH Hoos K LeytonBrown SATzilla portfoliobased algorithm selection SAT Journal Artiﬁcial Intelligence Research 32 2008 565606 34 L Xu F Hutter HH Hoos K LeytonBrown SATzilla07 The design analysis algorithm portfolio SAT Bessiere 101 pp 712727 35 L Xu HH Hoos K LeytonBrown Hierarchical hardness models SAT Bessiere 101 pp 696711 36 K Deb DE Goldberg Analyzing deception trap functions LD Whitley Ed Foundations Genetic Algorithms Workshop vol 2 Morgan Kaufmann 1993 pp 93108 37 M Mitchell S Forrest JH Holland The royal road genetic algorithms ﬁtness landscapes ga performance FJ Varela P Bourgine Eds Toward Practice Autonomous Systems Proceedings First European Conference Artiﬁcial Life The MIT Press 1992 pp 245254 38 SJ Wright The roles mutation inbreeding crossbreeding selection evolution DF Jones Ed Proceedings Sixth International Congress Genetics vol 1 1932 pp 356366 39 J Horn DE Goldberg Genetic algorithm diﬃculty modality ﬁtness landscapes LD Whitley MD Vose Eds Foundations Genetic Algorithms Workshop vol 3 Morgan Kaufmann 1995 pp 243269 40 SA Kauffman S Johnsen Coevolution edge chaos Coupled ﬁtness landscapes poised states coevolutionary avalanches Journal Theoretical Biology 149 4 1991 467505 41 T Jones S Forrest Fitness distance correlation measure problem diﬃculty genetic algorithms LJ Eshelman Ed ICGA Morgan Kaufmann 1995 pp 184192 42 M Clergue P Collard M Tomassini L Vanneschi Fitness distance correlation problem diﬃculty genetic programming WB Langdon E CantúPaz KE Mathias R Roy D Davis R Poli K Balakrishnan V Honavar G Rudolph J Wegener L Bull MA Potter AC Schultz JF Miller EK Burke N Jonoska Eds Proceedings Genetic Evolutionary Computation Conference GECCO2002 Morgan Kaufmann New York USA July 2002 pp 724732 43 L Vanneschi M Tomassini P Collard M Clergue Fitness distance correlation structural mutation genetic programming C Ryan T Soule M Keijzer EPK Tsang R Poli E Costa Eds EuroGP Lecture Notes Computer Science vol 2610 Springer 2003 pp 455464 44 L Vanneschi M Tomassini M Clergue P Collard Diﬃculty unimodal multimodal landscapes genetic programming E CantúPaz JA Foster K Deb L Davis R Roy UM OReilly HG Beyer RK Standish G Kendall SW Wilson M Harman J Wegener D Dasgupta MA Potter AC Schultz KA Dowsland N Jonoska JF Miller Eds Proceedings Genetic Evolutionary Computation Conference GECCO2003 Lecture Notes Computer Science vol 2723 Springer Chicago IL USA July 2003 pp 17881799 45 M Tomassini L Vanneschi P Collard M Clergue A study ﬁtness distance correlation diﬃculty measure genetic programming Evolutionary Computation 13 2 2005 213239 46 L Vanneschi M Clergue P Collard M Tomassini S Vérel Fitness clouds problem hardness genetic programming K Deb R Poli W Banzhaf HG Beyer EK Burke PJ Darwen D Dasgupta D Floreano JA Foster M Harman O Holland PL Lanzi L Spector A Tettamanzi D Thierens AM Tyrrell Eds Proceedings Genetic Evolutionary Computation Conference GECCO2004 Lecture Notes Computer Science vol 3103 Springer Seattle WA USA June 2004 pp 690701 47 L Vanneschi M Tomassini P Collard M Clergue A survey problem diﬃculty genetic programming S Bandini S Manzoni Eds Advances Artiﬁcial Intelligence Proceedings Ninth Congress Italian Association Artiﬁcial Intelligence AIIA2005 Lecture Notes Computer Science vol 3673 Springer Milan Italy September 2005 pp 6677 48 L Vanneschi M Tomassini P Collard S Vérel Negative slope coeﬃcient A measure characterize genetic programming ﬁtness landscapes P Collet M Tomassini M Ebner S Gustafson A Ekárt Eds EuroGP Lecture Notes Computer Science vol 3905 Springer 2006 pp 178189 49 R Poli L Vanneschi Fitnessproportional negative slope coeﬃcient hardness measure genetic algorithms Lipson 98 pp 13351342 50 S Droste T Jansen I Wegener A rigorous complexity analysis 1 1 evolutionary algorithm separable functions Boolean inputs Evolutionary Computation 6 2 1998 185196 51 S Droste T Jansen I Wegener On analysis 1 1 evolutionary algorithm Theoretical Computer Science 276 12 2002 5181 52 I Wegener On expected runtime success probability evolutionary algorithms U Brandes D Wagner Eds WG Lecture Notes Computer Science vol 1928 Springer 2000 pp 110 53 T Jansen KAD Jong I Wegener On choice offspring population size evolutionary algorithms Evolutionary Computation 13 4 2005 413440 54 C Witt Runtime analysis mu 1 ea simple pseudoboolean functions Evolutionary Computation 14 1 2006 6586 55 T Jansen I Wegener The analysis evolutionary algorithms proof crossover help Algorithmica 34 1 2002 4766 56 T Jansen I Wegener Real royal road functions crossover provably essential Discrete Applied Mathematics 149 13 2005 111125 57 O Giel I Wegener Evolutionary algorithms maximum matching problem H Alt M Habib Eds STACS Lecture Notes Computer Science vol 2607 Springer 2003 pp 415426 58 F Neumann I Wegener Randomized local search evolutionary algorithms minimum spanning tree problem Theoretical Computer Sci ence 378 1 2007 3240 59 J Scharnow K Tinnefeld I Wegener The analysis evolutionary algorithms sorting shortest paths problems Journal Mathematical Modelling Algorithms 3 4 2004 349366 60 S Baswana S Biswas B Doerr T Friedrich PP Kurur F Neumann Computing single source shortest paths singleobjective ﬁtness Founda tions Genetic Algorithms Workshop vol 10 ACM New York NY USA 2009 pp 5966 61 T Storch How randomized search heuristics ﬁnd maximum cliques planar graphs M Cattolico Ed Proceedings Genetic Evolution ary Computation Conference GECCO2006 ACM New York NY USA 2006 pp 567574 1276 M Graff R Poli Artiﬁcial Intelligence 174 2010 12541276 62 J Reichel M Skutella Evolutionary algorithms matroid optimization problems Lipson 98 pp 947954 63 DH Wolpert WG Macready No free lunch theorems optimization IEEE Transactions Evolutionary Computation 1 1 1997 6782 64 TM English Optimization easy learning hard typical function A Zalzala C Fonseca JH Kim A Smith Eds Proceedings Congress Evolutionary Computation CEC2000 IEEE Press July 2000 pp 924931 65 TM English Practical implications new results conservation optimizer performance M Schoenauer K Deb G Rudolph X Yao E Lutton JJM Guervós HP Schwefel Eds Proceedings Sixth International Conference Parallel Problem Solving Nature PPSN VI Lecture Notes Computer Science vol 1917 Springer Paris France September 2000 pp 6978 66 C Schumacher MD Vose LD Whitley The free lunch problem description length L Spector ED Goodman A Wu WB Langdon HM Voigt M Gen S Sen M Dorigo S Pezeshk MH Garzon E Burke Eds Proceedings Genetic Evolutionary Computation Conference GECCO2001 Morgan Kaufmann San Francisco CA USA 2001 pp 565570 67 C Igel M Toussaint On classes functions free lunch results hold Information Processing Letters 86 6 2003 317321 68 R Poli M Graff NF McPhee Free lunches function program induction Foundations Genetic Algorithms Workshop vol 10 ACM New York NY USA 2009 pp 183194 69 R Poli M Graff There free lunch hyperheuristics genetic programming scientists L Vanneschi S Gustafson A Moraglio ID Falco M Ebner Eds EUROGP2009 Lecture Notes Computer Science vol 5481 Springer 2009 pp 195207 70 R Poli M Graff Free lunches neural network search F Rothlauf Ed GECCO ACM 2009 pp 12911298 71 T Bäck Evolutionary Algorithms Theory Practice Evolution Strategies Evolutionary Programming Genetic Algorithms Oxford University Press New York 1996 72 G Rudolph Convergence evolutionary algorithms general search spaces Proceedings IEEE Conference International Conference Evolutionary Computation ICEC1996 IEEE Press Nagoya Japan May 1996 pp 5054 73 R Poli WB Langdon M Clerc CR Stephens Continuous optimisation theory easy Finiteelement models evolutionary strategies ge netic algorithms particle swarm optimizers CR Stephens M Toussaint LD Whitley PF Stadler Eds Foundations Genetic Algorithms Workshop vol 9 Lecture Notes Computer Science vol 4436 Springer Mexico City Mexico 2007 pp 165193 74 Y Borenstein R Poli Information landscapes HG Beyer UM OReilly Eds Proceedings Genetic Evolutionary Computation Confer ence GECCO2005 ACM Washington DC USA June 2005 pp 15151522 75 J Hadamard Sur les problèmes aux dérivées partielles et leur signiﬁcation physique Princeton University Bulletin 1902 4952 76 B Efron T Hastie I Johnstone R Tibshirani Least angle regression Annals Statistics 32 2 2004 407499 77 E Alpaydin Introduction Machine Learning MIT Press Cambridge MA USA 2004 78 R Poli J Woodward EK Burke A histogrammatching approach evolution binpacking strategies IEEE Congress Evolutionary Com putation IEEE 2007 pp 35003507 79 R Poli TinyGP Genetic Evolutionary Computation Conference GECCO2004 competition httpcswwwessexacukstaffsmlgeccoTinyGP html June 2004 80 UM OReilly F Oppacher Program search hierarchical variable length representation Genetic programming simulated annealing hill climbing Y Davidor HP Schwefel R Manner Eds Proceedings Third International Conference Parallel Problem Solving Nature PPSN VI Lecture Notes Computer Science vol 866 SpringerVerlag Jerusalem 1994 pp 397406 81 C Igel M Hüsken Empirical evaluation improved Rprop learning algorithms Neurocomputing 50 2003 105123 82 KA SmithMiles Crossdisciplinary perspectives metalearning algorithm selection ACM Comput Surv 41 1 2008 125 83 JA Boyan AW Moore Learning evaluation functions global optimization boolean satisﬁability Proceeding Fifteenth National Con ference Artiﬁcial Intelligence Tenth Innovative Applications Artiﬁcial Intelligence Conference AAAI1998 IAAI1998 AAAI Press Madison WI USA 1998 pp 310 84 O Telelis P Stamatopoulos Combinatorial optimization statistical instancebased learning Proceedings 13th IEEE International Conference Tools Artiﬁcial Intelligence ICTAI 2001 IEEE Computer Society Dallas TX USA 2001 p 203 85 MG Lagoudakis ML Littman Learning select branching rules dpll procedure satisﬁability Electronic Notes Discrete Mathematics 9 2001 344359 86 E Horvitz Y Ruan CP Gomes HA Kautz B Selman DM Chickering A Bayesian approach tackling hard computational problems JS Breese D Koller Eds UAI Morgan Kaufmann 2001 pp 235244 87 MG Lagoudakis ML Littman Algorithm selection reinforcement learning Langley 99 pp 511518 88 R Vuduc J Demmel J Bilmes Statistical models automatic performance tuning VN Alexandrov J Dongarra BA Juliano RS Renner CJK Tan Eds International Conference Computational Science vol 1 Lecture Notes Computer Science vol 2073 Springer 2001 pp 117126 89 N Thomas G Tanase O Tkachyshyn J Perdue NM Amato L Rauchwerger A framework adaptive algorithm selection STAPL K Pingali KA Yelick AS Grimshaw Eds PPOPP ACM 2005 pp 277288 90 EA Brewer Highlevel optimization automated statistical modeling Proceedings Fifth ACM SIGPLAN Symposium Principles Practice Parallel Programming PPOPP1995 ACM Press Santa Barbara CA USA 1995 pp 8091 91 B Singer MM Veloso Learning predict performance formula modeling training data Langley 99 pp 887894 92 C Gebruers B Hnich DG Bridge EC Freuder Using CBR select solution strategies constraint programming H MuñozAvila F Ricci Eds ICCBR Lecture Notes Computer Science vol 3620 Springer 2005 pp 222236 93 C Gebruers A Guerri B Hnich M Milano Making choices structure instance level case based reasoning framework JC Régin M Rueher Eds CPAIOR Lecture Notes Computer Science vol 3011 Springer 2004 pp 380386 94 C Gebruers A Guerri Machine learning portfolio selection structure instance level Wallace 100 p 794 95 T Carchrae JC Beck Applying machine learning lowknowledge control optimization algorithms Computational Intelligence 21 4 2005 372387 96 L Lobjois M Lemaître Branch bound algorithm selection performance prediction Proceeding Fifteenth National Conference Artiﬁcial Intelligence Tenth Innovative Applications Artiﬁcial Intelligence Conference AAAI1998 IAAI1998 AAAI Press Madison WI USA 1998 pp 353358 97 S Johnson Hierarchical clustering schemes Psychometrika 32 3 1967 241254 98 H Lipson Ed Proceedings Genetic Evolutionary Computation Conference GECCO2007 ACM London England UK July 2007 99 P Langley Ed Proceedings Seventeenth International Conference Machine Learning ICML 2000 Stanford University Stanford CA USA June 29July 2 2000 Morgan Kaufmann 2000 100 M Wallace Ed Principles Practice Constraint Programming CP 2004 10th International Conference CP 2004 Toronto Canada September 27October 1 2004 Proceedings Lecture Notes Computer Science vol 3258 Springer 2004 101 C Bessiere Ed Principles Practice Constraint Programming CP 2007 13th International Conference CP 2007 Providence RI USA September 2327 2007 Proceedings Lecture Notes Computer Science vol 4741 Springer 2007