ELSEVIER Artificial Intelligence 79 1995 241292 Artificial Intelligence Performance linearspace search algorithms Weixiong Zhang Richard E Korf Computer Science Department Universiry California Los Angeles Los Angeles CA 90024 USA Received December 1992 revised June 1994 Abstract space linear Search use algorithms DFBnB iterativedeepening tree T b d mean branching search depth widely employed planning scheduling search algorithms sum costs edges root nodes We prove practice In paper study solve difficult problems optimally including depthfirst branchand averagecase performance linearspace ID recursive bestfirst search RBFS To facilitate bound factor b depth d node analyses use random costs expected number nodes expanded DFBnB random tree bd times expected number nodes expanded bestfirst search BFS tree usually depth d We DFBnB asymptotically optimal requires space exponential time ID RBFS asymptotically optimal edge BFS runs exponential If bpo expected number children node costs costs T bd integers parent expected number nodes expanded exponential bpo 1 0 d4 bpo 1 linearspace algorithms factor T b d quadratic bpo 1 In addition study heuristic branching factor BFS DFBnB ID RBFS T b d Furthermore use effective branching analytic performance predict Asymmetric Traveling Salesman Problem explain surprising anomaly existence complexity algorithms transition results 1 Introduction overview Search fundamental problemsolving technique In paper study search algorithms widely practice problem solving In particular This research supported NSF Grant IRI9 119825 grant Rockwell second author partially GTE Graduate Fellowship Year Fellowship Corresponding 1001 Marina de1 Rey CA 90292 USA Telephone author Current address USCInformation author 3108221511 199394 Sciences Email zhangisiedu 199293 UCLA Chancellors Dissertation International Institute 4676 Admiralty Way Suite 0004370295X950 1995 Elsevier Science BV All rights reserved SSDlOOO4370294000476 242 initial state goal state pIFlJ initial goal states b Searching goal state Pig I An Eight Puzzle example algorithms linearspace interested algorithms problems andbound paper understand employed II The known iterativedeepening choice algorithms Linearspace algorithms use space linear search depth important NPhard include depthfirst branch recursive bestfirst search The primary goal algorithms large difficult problems search algorithms time complexity linearspace linearspace averagecase solve difficult problems optimally 11 Search problems We start discussing search problems slidingtile puzzles intelligence extensively example problems operations Traveling Salesman Problem problems model use search algorithms analyze artificial ubiquitous benchmarks experiments Asymmetric research These analytic introduce We present problems solved Operators decompose original solved directly The lowerbound cost functions effective operators briefly cost functions problem lowerbound efficient commonly problems subproblems guide search algorithm 11 I Slidingtile puules A square slidingtile puzzle consists k x k frame holding k2 1 distinct movable tiles blank space Fig 1 Any tiles horizontally vertically adjacent blank legal blank position An operator Given initial goal state slidingtile puzzle asked minimum number moves transform initial state goal state NPcomplete 431 arbitrarysize A commonly cost function heuristic evaluation puzzles e n number moves initial state state rr hn fn gn h n Manhattan W Zhang RE KmfArtifcial Intelligence 79 1995 241292 243 number moves grid away computed counting n goal state The Manhattan distance distance tile goal position values tiles excluding goal location summing underestimate distance number moves minimum tile Manhattan distance problem tile time blank Manhattan required goal location solve adjacent initial A given slidingtile state The cost function puzzle solved follows Starting state expanded individually moving tile horizontally state blank Each possible produces new state child new states A state selected current state current vertically current generated expanded stateselection equal cost best states unexpanded goal node far How new state selected depends search algorithms employed process continues unexpanded states costs greater stateexpansion Section 13 applied discussed 112 The Asymmetric Traveling Salesman Problem Given n cities 123 n matrix Traveling Salesman Problem tour visits city exactly returns cij intercity costs defines TSP starting city problems formulated TSPs wiring 291 When cost matrix equal cost pair cities cost minimumcost Many NPhard combinatorial vehicle routing workshop scheduling asymmetric city j city problem optimization cost city city j necessarily asymmetric TSP ATSP lowerbound cost function The effective 1361 The assignment problem ATSP assign solution city total cost assignments relaxation ATSP assignments tour allowing collections disjoint subtours provides city assignment problem city j cij cost assignment minimized The assignment problem need form single lower bound cost ATSP tour assignment successor tour solution time 36 tour If assignment problem solution happens single complete On3 ATSP The assignment problem solvable given cities Assume assignment illustrated Fig 2 introduce operators 1 I We contains problem problem subtours We use example assignment solution solve problem assignment If subtour 2 3 2 chosen exclude edge 23 edge 32 additional subproblems single complete solution contains constraint tour shown subtours try eliminate root node tree Since time choices We leads subproblem excluded edge We solve assignment problems assignment problem solution eliminated decompose subproblem In order total number subproblems avoid generating duplicate subproblems generated small possible This realized including I 23 t 6 t t 2 Fig 2 An exarnplc OF solving ATSP excluded suppose edges generate current subproblem example The second subproblem B excludes edge 32 fore subproblems B edge disjoint generated A edge guaranteeing previous subproblems edge In 23 edge 23 There subproblems subproblems mutually includes subproblem A excluding 23 23 In general let E denote set excluded edges I set included edges tour We choose problem x t children kth having subproblem assignment problem solution subtour solution 1 We decompose excluded arc set Ek included arc set II assignment t edges subtour xt x2 single complete minimum eliminate Assume number edges problem Ek EuXk fIIUx I XL kl __ t 1 Since _KL excluded edge kth subproblem edge k I st subproblem XL E 1 1 subproblems kth subproblem I st subproblem generated state space tree unique nodes Briefly given ATSP solved taking contain edge _i subproblems rk Therefore duplicate include edge xk E Ek included k subproblems generated obtained problem repeating assignment following First solve subproblem current subproblem If assignment select subtour generate child subproblems subtour Next select current subproblem new subproblem expanded This process continues unexpanded unexpanded complete Section 13 tour far How select current subproblem costs greater equal subproblems solution original problem single complete edges eliminating problem root tour generated subproblems cost best described W Zhang RE KmfArtcial Intelligence 79 1995 241292 245 E ECBEDCE UN Fig 3 A simple graph depthfirst search tree 12 Statespace tree model 121 A state space As described problem previous examples solving problem formulated search state space A state space consists set states collection operators The states configurations map state In general state space graph nodes represent states edges represent operators state transitions systematic specified properties path initial state goal state In paper focus statespace tree statespace graph cycles leaf nodes goal nodes varying cost The number children node referred space order goal nodes branching factor node solved The operators actions trees A statespace state space exploration Search We adopt tree model practice As discussed disjoint subproblems reasons The tree realistic model ATSP example problem infrom entity partition entities applied includingexcluding siblings decomposed excluded subproblems This general principle optimization If problem problems subproblem problem solving decomposed solutions combinatorial included state space reason The second tree duplicate nodes tree appropriate model linearspace general model state space algorithms linearspace While graph cycles algorithms restriction graph For example bruteforce depthfirst node A shown Fig 3b explore tree cost generating duplicate nodes Due space linearspace search graph Fig 3 starting general detect duplicate algorithms nodes duplicate nodes appear tree associated costs search explore The estimated cost node fn node estimate total cost best goal node cost solving The nodes statespace decide node algorithm estimate actual cost node fn problem tree underneath A cost function fn given node lowerbound includes fn obtained relaxing nodes n state space A lowerbound 391 One example original problem actual cost node cost function assignment overestimates 246 W Zhm KE KofArijiciul lnrellience 79 1995 241292 property monotonic cost nodes path function implies monotonic cost child node rr f n equal cost parent node n fn fn The represents parent costs The monotonic fact child node typically problem ATSP A cost function problem greater monotonic property comes constrained property vise versa Given lowerbound constructed root node guaranteeing additive cost function 161 gn slighter stronger intelligence taking lowerbound function cost node maximum fn fn n parent 11 fn gn hn A particular widely estimated cost heuristic estimate sum cost path initial state artificial n current node II hn 391 goal node example cost child node II parent II hrr krtrz 3 hn knfz 231 This edge n 11 Monotonicity simply shown follows Given f n 3 fn g n h n 3 gn h n gn gn knn gn knn I hn kttn hn AH f consistency h equivalent Manhattan distance I called consistent hn gn A statespace tree node costs treated edge costs instead cost sum edge The cost edge connects child node costs path root Edge costs nonnegative nondecreasing depths An edge cost viewed cost operator parent The cost node node costs monotonically difference maps parent child node nodes 122 A random An additional tree model reason tractable To facilitate averagecase use statespace tree model analyses introduce tree analytically following model random incremental 11 An tree random Definition random branching identically depth d root cost 0 factors mean b Edge costs finite nonnegative drawn common probability distribution The cost node sum edge costs cost depth d root node An optimal distributed independently leaf node minimum tree independent tree T b d goal node Fig 4 shows example incremental random tree numbers edges nodes edge costs resulting node costs respectively Two important features model worth mentioning The multiple optimal goal optimal goal nodes exist allowed costs nodes independent costs nodes correlated share common edges paths root degree dependence based number edges common unlike 123941 123941 The second conventional contrast assumption models In random cally distributed called branching edge costs assumed independent iid assumption The iid assumption factors different nodes These assumptions unfortunately tree model identi applies rarely hold W Bang R E KotfArtijicial Intelligence 79 1995 241292 247 optimh goal Fig 4 A simple incremental random tree t optimal goal Fig 5 A binary tree depth edge costs 01234 practice Nevertheless averagecase assumptions Examples analyses characterize presented Section 4 usually introduced facilitate analyses In cases analytic results obtained realworld problems assumptions especially iid hold Since branching factor node nonnegative random length Thus possible tree finite nodes nonzero probability factor greater mention random results asymptotic rule finite depth trees tree T b d assume tree finite meaning In order random random variable zero path root tree infinite number necessary sufficient mean branching b 1 151 In rest paper b 1 The reason assumption results tree depth goes infinity 13 Search algorithms random A search algorithm strategy decide node explore We use Fig 5 uniform tree depth edge costs chosen 0 1234 optimal goal simple binary node cost 4 The numbers edges edge costs numbers nodes resulting node costs tree illustrate algorithms As shown 0 expanded 0generated Fig 6 The order node expanwns bestfirst search 131 Befirst search statespace expansion To maintain The algorithm consider bestfirst search BFS BFS maintains tree cycle expands node minimum partially cost expanded nodes generated expanded optimal goal node chosen requires space exponential applications Appendix A Fig 6 illustrates BFS works Pseudocode tree Fig 5 numbers nodes order nodes expanded search depth making partially expanded tree BFS typically algorithm impractical A algorithm A special case BFS cost function f n gn Iz II qn sum cost path initial state estimated cost node II goal node An current node II hn important minimum number nodes algorithms guaranteed optimal goal tiebreaking feature A given consistent heuristic estimate h expands nodes cost equal optima1 goal cost 71 1 161 uses 132 Depthjirst branchandhound DFBnB uses space Depthfirst branchandbound search depth Starting root node global upper bound u cost optimal goal DFBnB selects recently generated node deepest node new leaf node reached cost u u expand Whenever revised expansion cost greater path root descendents great ancestors node selected U pruned node costs nondecreasing node costs cost new leaf Whenever equal linear In order optimal goal node quickly increasing order costs This child nodes called nodeordering paper refer DFBnB mean DFBnB node ordering Appendix A Fig 7 shows searched Throughout A recursive version algorithm DFBnB works tree Fig 5 The numbers expanded nodes order newly generated included DFBnB The penalty explored BFS nodes costs greater optima1 goal cost 4 Fig 7 expanded BFS For example run linear space expands nodes node sequence number W Zhang RE KorfArticial Intelligence 79 1995 241292 249 0 expanded 0generated Fig 7 The order node expansions depthfirst branchandbound cost 5 Fig 6 DFBnB work graph cycles expand cycle In case DFBnB guarantee In addition nodes requires finite tree cutoff depth order optimal goal cost 4 termination greater 133 Iterativedeepening In order use space linear search depth expand node cost greater optimal goal cost turn iterativedeepening ID 221 initially Using global variable called cutoff threshold performs series depthfirst search iterations terminates iterativedeepening expands nodes costs equal threshold expansion minimum new iteration iterativedeepening order thresholds 0 1 2 3 4 cost nodes generated expanded iteration begun The algorithm listed Appendix A Fig 8 shows works tree Fig 5 numbers nodes iterations cost expanded successfully Otherwise In example threshold In iteration set cost root If goal node chosen increased successive uses depth cost expands nodes given depth expanding general Iterativedeepening search algorithm includes cial cases depending cost function For example depthfirst DFlD nodes greater depth Uniformcost Dijkstras path root node node cost IterativedeepeningA A cost function fn gn hn iterativedeepening singlesource shortestpath algorithm iterative version 8 uses sum costs IDA employs number spe iterativedeepening 0 expanded 0 generated Fig 8 The order node expansions iterativedeepening 0 expanded 0 generated Fig 9 The order node expansions recursive bestfirst search Although iterativedeepening cost optimal goal node expand nodes shown Fig 8 expand node cost greater 133 Kecursive bestjrst Another problem srurch iterativedeepening It somewhat expand new nodes monotonic bestfirst order node costs nonmonotonic monotonic cost function Recursive bestfirst search RBFS unexplored nodes bestfirst order regardless cost function iterativedeepening cost function runs key difference maintains local cutoff formal description algorithm 241 iterativedeepening single global cutoff threshold RBFS tree RBFS computes separate subtree current search path For simplicity behavior algorithm tree Fig 5 Fig 9 leave threshold Appendix A treatment algorithm inefficient 241 expands efficient linear space The iterativedeepening return memory After expanding upper bound 2 causing RBFS children left child root node First called recursively left child root Fig 9 RBFS upper bound 2 value right child The reason best frontier node left subtree long value equal 2 left child childrens best frontier node right subtree After expanding root values 3 5 exceed release child value 3 stores new value left child backs minimum called recursively right child root After returning upper bound 3 value best frontier node left subtree After expanding upper bound 3 RBFS backs minimum values 4 stores new value root It calls recursively left child right child returns upper bound 4 value best frontier node right child After expanding left child left child frontier nodes left child greater equal 5 5 stored new value left child root RBFS called right child upper bound 5 It proceeds right subtree right child childrens values 4 6 exceed goal node cost 4 terminates root RBFS chooses At point immediate search path siblings nodes current path cost current memory expand time RBFS maintains bi Zhang RE KorfArtcial Intelligence 79 1995 241292 251 best frontier nodes siblings Thus space complexity search depth An important generates fewer nodes expands expands RBFS suffers node reexpansion seven nodes tree Fig 5 Fig 9 iterativedeepening iterativedeepening linear monotonic node costs tiebreaking For example RBFS nodes tree Fig 8 Similarly iterativedeepening feature RBFS overhead 14 A surprising anomaly In spite fact search algorithms performance explained existing original motivations fully understood For example results Understanding work widely following anomaly cause anomaly practice independently scale The straight dotted lines DFBnB trees different uniform branching Fig 10 shows performance factors b edge costs uniformly The horizontal axis tree depth d vertical axis number nodes generated left numbers nodes logarithmic trees lines tree depth right factor search minimumcost depth branching average number nodes generated DFBnB leaf node averaged 1000 trials branching 1 Obd growing exponentially b increasing factor The curved chosen 0 1234 increasing represent solid lb counterintuitive following trees larger branching anomaly For fixed search depth Fig 10 displays faster For example factors DFBnB search fixed 50 DFBnB generates 1846801 nodes average search depth factor random factor 1276 nodes tree 9076 nodes tree branching branching given computation total number node generations DFBnB search deeper trees greater branching factors 110894 nodes tree branching factor Alternatively tree branching factor chosen 01234 total txenodes DFBnB 0 1 I 50 100 search depth d I 150 I I 200 Fig 10 Anomaly depthfirst branchandbound edge costs uniformly chosen 01234 LL 0 I__ _i 50 100 search depth d 150 _ii 200 I II 0 edge costs uniformly chosen 0124 __ 50 A____ 100 search depth d A 150 200 Anomaly ID b Anomaly RBFS Fig I I Anomaly iterativedeepening recursive bestfirst search total computation For example reach depth 37 binary tree b 6 depth 200 tree b IO Furthermore RBFS illustrated Fig 11 anomaly exists iterativedeepening DFBnB tree depth 50 tree b 4 depth 160 tixed 100000 node generations random All algorithms mentioned worst case As worst case usually pathological averagecase search algorithm search algorithms unknown time practice examine node state space realistic measure fact linearspace complexity performance Despite averagecase In paper analyze averagecase complexity linearspace search algo tree T b d mean branching random time We iterativedeepening rithms To facilitate analyses use random factor 17 depth d node cost computed sum edge costs path tree T b d DFBnB expands root We analytically expected number nodes ex O tl N 0 d nodes average N b d optima1 BFS runs panded BFS We prove DFBnB RBFS asymptotically exponential tree integer edge costs If pu probability zerocost optimal random expected number children node costs edge T 0 d bpo Overall prove average number nodes parent expanded linearspace exponential bpo 1 Od4 bpo 1 quadratic bpo 1 In addition study heuristic branching factor BFS linearspace algorithms These results presented factor T b d effective branching asymptotically Section 2 algorithms The results Section 2 indicate exponential children bpo increases polynomial abrupt complexity search depth average number samecost equal We examine greater transition S Zhang RE KorjArtijicial Intelligence 79 1995 241292 253 transition ATSP We consider select search algorithm linearspace transition Section 3 In Section 4 apply complexity explain previously observed anomaly DFBnB slidingtile complexity given problem algorithms space running Section 7 Previous Section 5 Our experimental algorithms research presented time We discuss usually choice 57581 results results large problems terms analytic puzzles predict results related work Section 6 Finally conclude 2 Complexity linearspace search In section analyze trees The difficulty rithms effective branching factor random linearspace carried characterize bestfirst search relationship algorithms expected complexity factors random search algo linearspace trees heuristic branching mathematical algorithms To circumvent directly obtain tools bestfirst expected complexity search difficulty approach expected complexity linearspace algorithms 21 Problem complexity optimal goal cost To analyze expected complexity algorithms consider problem terms total number node complexity expansions finding optimal goal node tree monotonic node costs total number Lemma 21 On statespace nodes costs strictly optimal goal cost lower bound complexity finding optimal goal node total number nodes costs equal optimal goal cost upper bound complexity finding optimal goal node Proof See Appendix B Cl This lemma indicates problem complexity directly related goal cost The optimal goal cost properties random studied tools area mathematics particular 211 151 firstpercolation branching processes agedependent called branching processes optimal tree 151 14 processes Let po probability edge cost zero Since b mean branching leaving node expected factor bpo expected number zerocost branches cost parent We number children node nodes samecost children It turns expected number samecost children expected cost optimal goal nodes Intuitively bpo 1 node determines child optimal goal node expect samecost cost increase depth On hand boo samecost child causes depth I nodes optimal goal cost increase Lemma 22 32331 tree Tbd b 1 As d X Let C expected cost optimal goal nodes random I Cld cy surely bpo I constant independent IIJ tl 2 C log log d 1 surely rvhen bpo I 3 C surely remains bounded bpo I Lemma 22 means bpo I cd dominant term optimal goal cost C Similarly bpo I log log d dominant term C We following monotonic property optimal goal cost Lemma 23 On random tree T b d bpo I d t 00 expected cost optimal goal nodes decreases bpo increases given 6 given po 0 In particular zero po 1 given b b f x expected goal cost approaches given po 0 Proof See Appendix B U 22 Bestfirst search To characterke expected complexity linearspace algorithms sider expected complexity BFS Lemma 24 On LI statespace optimal algorithms use node costs guaranteed optimal goal node tiebreaking nodes costs equal optimal goal cost tree monotonic node costs bestfirst search Proof See Appendix B U Lemma 24 implies BFS optimal tiebreaking random tree monotonic node costs guaranteed It shown 32331 algorithm d bbo 1 expected number nodes expanded BFS Tb d quadratic algorithm expected number nodes expanded d bpo 1 Since BFS optimal d bpo I linear problem tiebreaking optimal goal node 7 b d following exponential A sequence X random variables said converge ulmost surely probability X P lim X X I 1441 W Zhang RE KorfArtificial Intelligence 79 1995 241292 255 Lemma 25 The expected number nodes expanded bestfirst search fornding optimal goal node random tree T bd I 6pd 2 bpo 1 3 constant 1 p b 2 0d2 bpo I 3 Bd bpo I d 00 po probability zerocost edge BFS expands nodes costs equal cost C average number nodes costs optimal goal node Intuitively C exponential bpo 1 C grows linearly depth bpo 1 The extreme case edges nodes cost po 0 bpo 0 On hand bpo 1 large number nodes cost optimal goal nodes The extreme case edges nodes cost zero po 1 leaf tree optimal goal node verify optimal easy 23 DepthJirst branchandbound runs linear space To reiterate search depth The difference major differences DFBnB BFS One space search depth BFS usually DFBnB DFBnR expand nodes exponential optimal goal cost BFS The second costs greater difference makes tool branching processes BFS carried DFBnB directly To circum complexity vent difficulty approach DFBnB BFS More specifically try answer question nodes DFBnB generate analysis DFBnB difficult mathematical relationship determine BFS requires Theorem 26 Let NB b d expected number nodes expanded bestjrst branch search ND 6 d andbound random expected number nodes expanded depthjrst tree Tb d As d f dl NDbd b lxNsbi d il Proof See Appendix B 0 Theorem 26 important analytic results paper It shows expected complexity DFBnB BFS Most relationship following results DFBnB based theorem 2 0xx denotes set functions wn x x1 In words 8xx exist positive constants CI c2 ro functions represents clxx asymptotic order xx wx czxx 256 W Zhunq KE KorfArticicd Intelligence 79 1995 241292 27 On random l Corollary Na 0 d ND 0 d expected numbers nodes expanded bestjirst search und depthfirst branchandbound Od Nsbd respectivel tree Tbd Nhd Proof See Appendix B E Theorem 26 combined Lemma 25 leads following Theorem nding optimal goal node cl random tree T b d d CQ 28 The expected number nodes expanded depthrst branchandbound I 1 O p bpo 1 meaning depthfirst branchandbound asymptoti cally optimul p constant Lemma 25 2 Od bpo 1 3 0d2 bpo I p probability qfa zerocost edge Proof See Appendix B C Lemma 25 showing Theorem 28 analogous p Theorem 28 p Lemma 25 This means expected number nodes ex panded DFBnB different values expected number samecost children Note bpo I d t CC DFBnB expands asymptotically number nodes BFS Since BFS case optimal Lemma 24 DFBnB asymptotically optimal We interested samecost children grows behavior DFBnB expected number 29 On random Corollary number nodes expanded depthfirst branchandbound po 1 given b b x u given po 0 tree Tl d bpo 1 d 3c expected approaches Od Proof See Appendix B KY This corollary means bpo I bpo increases given po given b easier optimal goal node verify optimal 24 Iterativedeepening Iterativedeepening ID nodes costs equal current cost bound depthfirst order costbounded depthfirst iteration In search iterativedeepening node cost cost optimal expands expands For behavior iterativedeepening iteration Thus goal node generate different critically depends distribution edge costs The following edgecost distribution plays important lattice distribution role A distribution takes values W Zhang RE KorfArtiial Intelligence 79 1995 241292 257 nonnegative finite set sod al A aA constant A chosen integers Q relatively prime Any distribution distribution A r integers aa positive 151 lattice finite set integers finite set rational numbers A irrational number example Furthermore tree T b d edge costs chosen continuous Theorem 210 On random distribution expands O NB b d expected number nodes iterativedeepening NB b d expected number nodes expanded bestfirst search On ran dom tree T 6 d edge costs chosen lattice distribution expands 0 Ns b d optimal expected number nodes d f 00 iterativedeepening asymptotically Proof See Appendix B 0 This indicates overhead significant noderegeneration continuous edge costs chosen theorem iterativedeepening case node costs unique probability node expanded upper bound expected complexity iterativedeepening chosen zero probability implies nodes bpo 1 bpo 1 bpo 1 respectively p defined Lemma 25 In iteration expands iteration Theorem 210 provides edge costs nonzero values impulse zerocost edge zero In case theorem 210 expected number hybrid distribution continuous expands O32d 0d4 iterativedeepening previous distribution Od On lead constant multiplicative hand edge costs chosen node edge costs search depth edge proof Theorem 210 number distinct node costs optimal goal cost regenerations nodes paths root different combinations cost The fact total number iterations linear shown costs chosen means linear lattice distribution lattice distribution overhead The reason search depth Continuous lattice distributions distribution Consider discrete edge costs rationally extreme cases edge costs Unfortu optimal edge costs chosen inde exists iterativedeepening asymptotically nately nonlattice pendent Two different numbers x y rationally rational number When tions ci 1 c2 TT instance total cost different longer asymptotically combinations r x r y For example 1 7 rationally independent edge costs rationally In case iterativedeepening different independent independent combina edgecost optimal 25 Recursive bestjirst search With monotonic cost function nodes iterativedeepening tiebreaking recursive bestfirst search fewer 24 Thus state space generates RBFS 758 Table I Expected complexity BFS DFBnB Algorithm BFS DFBnB Table 2 hp c I H pd optimal Hd2 optimal Hd optimal p asym optimal OtP Ot12 Expected complexity ID RBFS hp c I Lattice edge costs Hp asym optimal HrP asym optimal Hd asym optimal Nonlattice edge costs opd Od4 OG monotonic complexity iterativedeepening RBFS Thus RBFS lattice distribution node costs complexity recursive bestlirst random asymptotically iterativedeepening search Consequently upper bound expected complexity tree upper bound expected complexity optimal edge costs chosen 26 Summary tree search complexit Tables I 2 summarize results section expanded bestfirst search BFS depthfirst branchandbound deepening asymptotically recursive bestfirst d X RBFS search ID average number nodes iterative DFBnB tree T b d random 27 Bratiching factors Before close section discuss average bruteforce branching children node tree b captures depth different branching factors The factor b tree average number rate growth number nodes The second asymptotic heuristic branching factor heuristic branching factor B tree Sl B ratio average number nodes given cost X average number nodes largest cost X limit x c iterations capture BFS Iterativedeepening The heuristic branching factor B introduced asymptotically increases average number node expansions deepening cause sive In case panded iterativedeepening BC B I d x arc unique leads constant d K B exist large node costs When edge costs chosen average value B greater overhead iterative optimal B 1 exponentially succes overhead total average number nodes ex constant expanded BFS multiplicative variables node costs defined limit bpo 1 B I When bpo I optimal goal cost remains lattice distribution iterativedeepening 221 When edge costs continuous asymptotically W Zhang RE KorfArtijicial Intelligence 79 1995 241292 optimal case Theorem 210 As Theorem 211 B approaches constant 259 Theorem 211 Let B asymptotic heuristic branching T b d b 1 B 1 edge costs chosen continuous distribution When bpo 1 edge costs chosen lattice distribution al A a2A chosen nonnegative approaches constant d 00 solution greater a0 0 aA probabilities PO p1 p2 p respectively A 0 integers al a2 relatively prime B equation factor random tree Proof See Appendix B q asymptotic The branching factor factor search algorithm branching average number nodes expanded d 03 In words p measures relative level j Lemma 25 Theorem 28 example BFS DFBnB factor effective 40 p search depth d dth root search depth extra factor average complexity extending effective branching effective branching increase It evident factor p search algorithm state factor b tree However factor B tree greater b effective branching space tree greater bruteforce branching heuristic branching greater p B optimal goal cost C edge costs chosen distribution j algorithm We following relationship lattice tree Tb d b 1 bpo 1 edge costs Theorem 212 On random chosen lattice distribution iterativedeepening factor p B d 00 po probability edge cost zero B asymptotic heuristic branching factor T b d LY limit Cd d f m C optimal goal cost recursive bestfirst search efSective branching bestfirst search depthfirst branchandbound Proof See Appendix B q 3 Complexity transition tree search 3 I Averagecase complexity transition The results Section 2 averagecase complexity tree search algo rithm random This phenomenon trees experiences dramatic transition exponential polynomial similar phase transition dramatic change 260 W Zkrn KE Kor_fArtijbcd Intelligence 79 I 995 241292 I I baa 1 polynomial region transition boundary g 10 5 3 Oi3 6 g 06 0 g 3 E L II 04 02 ooo 5 10 mean branching factor b 15 20 Fig IL llitlicult easy regms tree search problem property order parameter changes critical point The simplest solid phase liquid phase example phase transition temperature lhat water changes rises freezing point complexity determines The order parameter edge takes cost zero factor b bpo expected number samecost algorithms expand number nodes average On hand bpo 3 1 BFS average expected number greater equal Fig 12 expected number samecost children If probability 0 mean branching children node When bpo I BFS linearspace exponential linearspace case complexity samecost children illustrates regions transition boundary average polynomial algorithms changes complexity exponential time Therefore polynomial increases run point transition tree search The condition bpo I Dpo I bo 1 estimated random sampling nodes edge costs independent results provide means nately independence independence Scction 4 state space counting average numbers samecost children random sampling estimate edge costs hold performance If analytic search algorithm Unfortu practice Nevertheless assumption reasonable cases discussed 32 Finding optimal goal rwdrs exists transition difficult The complexity solutions different Consequently goal depth Therefore number optimal goal nodes expected number nodes However desired To optimal goal node regions bpo 2 1 bpo 1 reasons When bpo I node samecost child average increases exponentially expected exponential On hand bpo 1 small optimal goal minimum expected number nodes costs optimal goal nodes difficult expected number optimal goal nodes cost particular depth cc Zhang RE KmfArtcial Intelligence 79 1995 241292 261 1 I I q 10 It6 by5 edge costs 012 po l5 261 L 6 b 0 I 20 I search pth I I 60 fi 20 search depth I 40 optimal goals b nodes generated DFBnB Fig 13 Finding solutions difficult cost increases exponentially tree depth Hence difficult number nodes lower costs optimal goal nodes exist optimal goal nodes exponential remaining Fig 13 shows example finding optimal goal nodes random factor b pc l5 DFBnB To emphasize nonzero edge costs uniformly trees impact uniform branching 216 1 0 choose The results Fig 13 averaged 1000 trials Fig 13a shows average increases exponentially bpo 1 Fig 13b gives number optimal goal nodes corresponding exponential average number nodes generated DFBnB values bpo 12 indicating 33 Meaning po ties POPIPP transition results indicate The previous number edges costs zero What example complexity zerocost edges Assume edges costs S6 16 2 6 m probabili respectively 6 positive number Does complexity closely transition related exist case Consider example random 12345 chosen uniformly Section 1 shifting performance Fig 14 experimental complexity algorithm edge costs At glance This different tree uniform branching factor b edge costs Fig 5 affect value added edge tree results DFBnB averaged 1000 runs Fig 14 shows transition edge costs example value zero When edge costs tree change node costs change accordingly However costs nodes located different depths adjusted cost node sum edge costs value added edge path root node Thus larger value added node deeper depth node shallower depth costs nodes depth d 262 W Zlung KE KoArfijiul Irzrellenre 79 I 995 241292 3 IO6 F 22 e IO 2 B 104 M s IO3 2 102 2 10 0 5 edge costs uniformly chosen 12345 I I 10 20 15 search depth d I 25 I 30 Fig 14 There search anomaly zerocost edges goal node located In consequence increase maximum optimal nodes new costs new optimal goal cost searching optimal goal node harder Furthermore zerocost edge node costs strictly cost node depth d 6d lower bound optimal goal cost costs equal 6d tree larger There nodes factor This factor tree smaller branching branching search anomaly edge costs nonzero shown Fig 14 increase depth If minimum edge cost 6 cost node 11 d goal depth II current upper bound edge cost increase search efficiency node n depth j The generating 6 II We use information minimum Consider scenario DFBnB node II subtree need explored fn This In summary cost leaf node II f n d j 6 intrinsic meaning po probability cost cost known priori In words known minimum zero edge cost equivalent Given edgecost distribution known minimum value convert minimum edges minimum edge cost zero edge costs subtracting cost edge distribution fn clj The obstacle overcome edges statespace generally unknown This learned minimum search algorithm obtain practice tree realworld problem Although learned estimated sampling edge cost subtracted efficiency improve minimum minimum cost edge cost tree statespace edge encountered po considered rough measure accuracy cost function The accuracy cost function given problem measured difference optimal goal cost cost initial node state space If zero cost function difference tree represented optimal goal cost root cost zero In random difference tree larger po gives rise samecost children reduces optimal goal larger po closer estimated cost cost according exact root node In random Lemma 22 Therefore W Bang RE KqfArtcial Intelligence 79 1995 241292 263 cost function However node actual cost accurate edge costs state space real problem generally dependent It clear dependence edge costs impact results 4 Applications Section 2 explain search anomaly presented pe zerocost edge constant edge costs trees uniformly Our analyses Section 14 Since probability 5 bpo 1 averagecase search depth d 0d4 b 5 bpo 1 quadratic b 5 bpo 1 Thus branching complexity l5 When complexity search algorithm factor b increases branching decreases chosen 0 1234 factor b exponential Figs 10 11 averagecase d This section presents applications analytic results benchmark problems slidingtile puzzles Asymmetric Traveling Salesman Problem 41 Anomaly DFBnB slidingtile puzzles Given initial goal state slidingtile map initial state goal state In realtime setting sequence moves limited computation One approach problem called fixeddepth lookahead search 231 search current state fixed depth child current state contains frontier node subtree child current state puzzle asked minimumcost initial Fig 15 shows experimental state node n hn results lookahead searches slidingtile puzzles total number DFBnB The cost function fn gn hn gn distance moves total numbers n goal state The straight dotted lines left represent puzzles nodes search tree Eight Fifteen Twentyfour total numbers nodes expanded The curved solid 231 For DFBnB The results reported given search depth DFBnB search trees larger puzzles faster Alternatively given computation DFBnB search deeper trees larger puzzles For example million node expansions DFBnB reach depth 35 Eight Puzzle depth 42 Fifteen Puzzle depth 49 Twentyfour Puzzle depth 79 Ninetynine right represent following fix total computation anomaly originally Manhattan Ninetynine respectively Puzzle lines An explanation anomaly Manhattan distance h decreases value cost function Thus edge costs zero dependence h value initially independent cost parent problem following Moving tile increases g f g h increases stays tree Since increases problem approximately modeled random edge costs increases decreases moving size Thus probability approximately half ignored The probability tile roughly half child node p x 05 In addition 1 _I LL 60 I 20 lib lookahead depth d 80 Rg 15 Anomaly lookahead hearch slidingtile puzzles average blanching arc approximately puzlc siLc Thus ha increases puzzle size lookahead easier larger puzzles factors 0 Eight Fifteen Twentyfour 1732 2130 2368 2790 respectively Section 2 analyses Ninetynine following b grows puzzles search This explanation imply solving larger puzzle length larger puzzle larger puzzle difficult unique board configuration easier longer solving smaller First solution solve Secondly smaller making particular problem specified instance goal stale Although multiple nodes board configuration search number copies goal state relatively small encountered compared edge costs valid A important sequence moves decrease initial states distance likely Specifically Eight Puzzle po 0501 initially steadily decreases search depth making problem difficult search depth increases number nodes minimal cost fixed search depth Finally Manhattan distance makes increases experiments 1000 random assumption independent puzzle reason 42 Complexity trunsitiotr Asymmetric TSP Section R 0 12 112 intercity number distinct Consider node costs statespace assigmnent problems r positive costs corresponding chosen uniformly relationship statespace K total sum smaller II edges total cost decreases r increases When Y small compared probability tree equal tree ATSP solution costs integer r Let examine sets tz values sets number cities search tree cost zero The probability problem cost child subproblem intercity costs r edges problem cost parent large In words r larger Thus subproblems assignment assignment assignment probability solutions problem N ifhang RE KorfArtcial Intelligence 79 1995 241292 265 uniform intercity cost distribution 6 4 2 0 1 10 102 lo6 number distinct edge costs r lo4 lo3 16 10 1 lo2 10 number clistiqcct edge costs r lo3 lo4 lo6 lo5 IO average samecost children b average node generations Fig 16 Complexity transition loocity Asymmetric TSP uniform distribution larger probability po zerocost edges search tree larger Y smaller b children bpo greater problem easy solve assignment problem cost child probability average number samecost significantly decrease Y small Conversely equal parent po zerocost edges search tree Consequently solve r large The argument r increases Therefore probability smaller r relatively complexity tested prediction experiments ATSP r changes shown ATSPs In experiment randomly generated 1000 problem assignment DFBnB tree ATSP average number nodes generated DFBnB Fig 16 shows intercity costs axes number results horizontal scale logarithmic problem averaged average number samecost BnB search loocity instances initial problems relationship instances We examined exist complexity solutions single complete problem difficult Fig 12 We experimentally transition randomtree children bpo node search tours solved r distinct results following transition suggests Fig 16a shows bpo 1 problem point A Following indicated Fig 12 means difficult r 130 Fig 16b DFBnB different values transition When r 6 20 problem complexity ATSPs transition complexity easy transition solve r 130 bpo 1 randomtree r 130 search r 130 average numbers nodes expanded costs showing complexity easy difficult r 1000 A similar random 200 300 400 500city illustrates r distinct intercity However transition easy problem instances expected This likely following difficult ones dramatic factors First bpo decreases 266 N Zhuny RE KorjArtijiciul Intelligence 79 1995 241292 gradually Secondly random importantly branching random trees violated r increases search trees relatively tree search asymptotic complexity Y 1000 making slowly transitions infinity Finally results tree depth approaches edge costs search tree independent factors different nodes iid assumptions small depths complexity increase As suggested Section 2 particular value r bps 1 determine transition point transition point point A Fig 16 instance problem size number cities It remains open problem analytically random ATSP In short increases value cost range r transition occurs primarily determined ATSP space ATSP Our results problem difficult ATSP instances easily generated large number test averagecase average number samecost children showed distinct search algorithm intercity costs useful need difficult ATSP instances complexity 5 Algorithm selection The complexity measure search algorithms Section 2 total number time practical optimal solution space algorithm nodes expanded generated However space running important measures given problem We continue investigation running selection choosing search algorithm time complexity The purpose provide guidelines taking account First consider space complexity search depth Given In order select node minimum cost generated expanded active nodes usually speed matter minutes millisecond nodes BFS maintain active nodes memory space required exponential processing current computers BFS typically exhausts halting algorithm For example megabyte memory minutes Therefore BFS applicable algorithms use space linear size algorithms choice large problems large problems Since linearspace problem search depth linear new active node generated BFS memory available memory ratio memory Assuming memory constraint given clear algorithm BFS DFRnB use given problem Although BFS expands algorithms results ditions The remaining algorithms random Problem Section 2 RBFS practice It clear analytic runs time trees slidingtile puzzles Asymmetric Traveling Salesman different running compares analytic iterativedeepening linearspace section experimentally run faster algorithm fewer nodes faster practice results W Zhang RE KorfArtial Intelligence 79 1995 241292 267 5 I Comparison analytic model We compare average number nodes expanded BFS DFBnB iterative time algorithms random deepening trees RBFS consider running 51 I Node expansions We use random trees uniform branching factors different edgecost In case edge costs uniformly distributed 0 1234 po l5 nonzero edge costs uniformly discrete edge costs In second case edge costs chosen hybrid edgecost distribution impact po complexity representative 216 1 The purpose choosing distributions We choose set zero probability 123 twofold We set po l5 search algorithms We nonzero costs large range distribution Note shallow distribution We choose complexity polynomial case bpo 1 b 5 transition case bpo 1 complexity approximates large distribution trees When d relatively different branching simulation study simulate continuous continuous treated discrete factors b 2 exponential distribution case bpo 1 b 10 The algorithms run different depths 1000 trials The average Fig 17 These results consistent analytic superior results BFS iterative optimal bpo 1 Figs 17a 17d asymptotically RBFS fewest nodes algorithms RBFS asymptotically optimal edge costs dis 17c However bpo 1 edge costs Fig 17d slope iterativedeepening result expands O N nodes average edge costs chosen distribution N expected number nodes expanded consistent analytic curve slope BFS curve This Figs 17a sults shown expands deepening DFBnB iterativedeepening 17b crete large range chosen nearly twice iterativedeepening continuous BFS Fig 17 provides additional information provided analytic results When RBFS asymptotically 17f iterativedeepening Figs 17b In cases regardless number distinct nonzero edge costs Moreover bpo 3 number RBFS When bpo 1 edge costs bpo 1 Figs 17c optimal 1 edge costs discrete iterativedeepening RBFS nodes expanded costs greater reexpansion discrete RBFS Fig 17d shows large set values RBFS unfavorable deepening overheads iterativedeepening Fig 17a DFBnB outperforms overhead DFBnB optimal goal cost larger bpo 1 edge costs chosen asymptotic complexity iterative iterativedeepening 17c DFBnB worse In summary large problems formulated depth require exponential easy problems ed computation bpo 1 unbounded bpo l DFBnB tree bounded depth RBFS adopt 268 W Zlung KE KorfArtxr hrelhpwce 79 1995 241292 edgecosts01234 uniformly PO t5 edgecostsOlZ 2161 pO 1S 0 10 20 30 search dcprh b2 bp 4 _ I _ r 40 50 0 10 40 20 30 search depth d b2 bpOl 50 60 7 rr 7 Lm_ 0 50 L_ m__mmm MU 200 150 100 search depth LL 0 50 100 search depth 150 200 e bS bp l search depth c blO bpo l loi 50 search depth 150 200 f blO bp l Fig 17 Average number nodes expanded random trees We consider BFS compares main performance measure assuming optimal faster linearspace terms number node expansions algorithms linearspace algorithms running time space constraint Although BFS runs mean W Ztang RE KorjArtlcial Intelligence 79 1995 241292 269 branching factor 2 edge costs chosen OA2 216l 20 30 40 search depth 50 running time 60 20 30 40 search depth b time node expansion 50 60 Fig 18 Running time time node expansion binary tree To expand nodes bestfirst order BFS maintain priority queue store nodes generated expanded The time access priority queue selecting node queue inserting new nodes queue depends total number nodes stored increases nodes added If heap optimal total number nodes heap For difficult problem exponential expand node difficult problems All linearspace implemented time takes constant constant p 1 heap access time search depth BFS takes time linear algorithms hand executed stack search depth Opd Od This means priority queue access time logarithmic requires node generations stack operation implementation logpd Fig 18 shows example binary random tree running time BFS faster DFBnB The zero edge costs chosen probability increases po l5 nonzero edge costs uniformly Fig 18b DFBnB case confirms analysis corresponding illustrates average chosen 123 time node expansion 216 1 BFS Overall BFS runs faster linearspace problem complexity time expanding size determine node generating children algorithm depends number nodes generated 52 Comparison actual problems 521 Lookahead search slidingtile pues Consider lookahead fixeddepth current state fixed depth returns node given depth cost minimum RBFS lookahead gn hn gn nodes depth We compared DFBnB search In experiments cost function fn total number moves puzzle searches search slidingtile iterativedeepening state initial 270 W Zhang RX KorfArtijicial Intelligence 79 199s 241292 I 0 20 40 search depth 60 80 Fig 19 Lookahead search slidingtile puzzles distance expands slightly nodes Manhattan iterativedeepening node n goal state Our RBFS node n hn experiments lookahead search expected Fig 19 compares DFBnB RBFS The horizontal axis lookahead depth vertical axis number nodes expanded The labeled 8 15 24 99 results averaged 200 initial states The curves results Eight Fifteen Twentyfour respectively The results DFBnB performs RBFS small puzzles RBFS Ninetynine slightly better superior puzzles puzzles In addition Unfortunately shortest complete solution path graph cycles advance Thus DFBnB applied nodes cycle terminate Furthermore insignificantly running algorithm choice finding optimal solution path problem fewer nodes iterativedeepening tree graph expand RBFS generates RBFS slightly higher overhead time iterativedeepening setting iterativedeepening Consequently unknown DFBnB large ones state space slidingtile 522 The Asymmetric TSP intercity RBFS ATSP 0 123 We ran DFBnB iterativedeepening axes number distinct costs r r integer Figs 20a 20b 300city ATSPs averaged 500 trials The intercity costs r vertical axes number distinct costs r small large compete RBFS DFBnB DFBnB easy overhead RBFS larger DFBnB RBFS DFBnB difficult ATSPs case node costs uniformly chosen results loocity horizontal numbers nodes generated When relative discussion especially ATSPs indicating poorly relative search tree unique causes significant node regeneration overhead Section 42 Iterativedeepening difficult ATSPs r large RBFS number cities n ATSP easy difficult respectively following worse W Bang RE KorfArtificial Intelligence 79 I 995 241292 271 I I I I I I intercity costs unifoImly chosen 1012 I rl number distinct intercity costs r loocity ATSP 0 ld Id 10 number distinct intercity costs r lo 10 10 ld b JOOcity ATSP Fig 20 Performance Iinearspace algorithms ATSI 53 Summary Even linearspace generated BFS regenerating ing nodes linearspace problem tion algorithms linearspace methods algorithms choice search algorithms expand nodes BFS explor In addi relatively space BFS For large problems algorithms run faster BFS This typically times case small large time node expansion space linearspace node multiple Among linearspace algorithms DFBnB inapplicable spaces graphs cycles cutoff bounds known preferable exponential boundeddepth computation RBFS applied problems problems search spaces boundeddepth solved polynomial trees problems problems state advance DFBnB trees require represented time 6 Related work 61 Analytic models Most existing models tree models different factor solutions satisfaction problems formulated model analyzing search algorithms trees There literature The model assumes uniform branching lie exactly depth d 132749 Constraint 42 The second model uniform 612183941 node cost function Node costs assumed random variables The important complexity exponential A algorithm cases 18391 This cost function distinguishes tree unique goal node given depth distributed expected 161 bestfirst search fn gn hn result model independent model identically analytic 272 W Zhuq RE KorfArtlficiul htellience 79 1995 241292 The model random tree independent edge costs cost node sum edge costs path root optimal goal node minimum cost leaf node fixed depth 203233535758 variant depths 48 The idea assigning costs edges traced work Fuller et al lo game tree search Two variations model literature uniform 323348 branching factor 581 This model Traveling Salesman Problem We adopt model random branching incremental random branching optimization suitable function factor problems minimized combinatorial 291 objective factor fixed goal depth 205357 typically random tree 62 Analyses brunchandbound BFS DFBnB considered special cases branchandbound problem 19 solving Kumar et al 261 showed BnB formulated intelligence BnB artificial 25301 general graphsearch For example Smith technique algorithms developed A algorithm averagecase 48 performed random equations obtained direct conclusion complexity BnB analysis BnB The model tree leaf nodes different depths goal nodes derived equations drawn averagecase complexity No closedform averagecase complexity solutions 161 BFS algorithm analysis conducted Wah Yu 53 random factor The process BFS modeled behavior wall minimum cost currently current upper bound A stochastic process Another averagecase tree uniform branching walls moving generated nodes employed Empirical edge costs gamma distribution compared DFBnB BFS arguing lowerbound derive approximate results verified formulas averagecase average number nodes expanded complexity BFS exponential binomial distribution They comparable cost function accurate inaccurate Karp Pearl 201 delineated polynomial exponential averagecase complexities tools branching processes tree edge costs 1 0 probabilities p I p BFS random binary 151 They elegantly showed The basic mathematical remain bounded p 05 cost optimal goal certain d likely p 05 They proved linear p 05 quadratic p 05 exponential p 05 These results suggested earlier Hammersley near loglogd p 05 surely proportional average number nodes expanded BFS 141 32331 McDiarmid significantly Provan factors They trees arbitrary edgecost distributions depth d bpo 1 goal cost surely grows loglogd bpo 1 remains bounded bpo 1 They showed average time bpo 1 runs linear average time bpo 1 random proved approaches BFS finds optimal goal node exponential time bpo I finishes quadratic average extended Karp Pearls work random branching linearly W Zhang RE KorflArtifcial Intelligence 79 1995 241292 273 The results Karp Pearl McDiarmid form basis In fact properties optimal goal cost Lemma 22 average Provans BFS Lemma 25 paper McDiarmid Provan research case complexities results 63 Analyses iterativedeepening showed worstcase Patrick et al 38 A cost function iterative gn hn occurs nodes N N 2 nodes N number worstcase IDA trees graphs Vempaty et al 511 compared DFBnB high preferable solution density fn deepening search tree unique costs expanding nodes performance IDA They argued DFBnB IDA heuristic branching surely expanded A Mahanti et al 3 1 discussed IDA factor high averagecase Patrick 37 considered tree uniform branching expected number nodes particular dependence nodes Chapter 6 page 631 In words dependent model treated independent complexity IDA random cost k Patrick account 37 share common edges paths root factor integer edge costs However computing 64 Algorithms limited space BFS linearspace algorithms exponentialspace BFS machine extreme cases algorithms A general method use end linearspace stand opposite ends space spectrum end In use space available algorithm Pearl presented requirement 31 combinations run slower iterativedeepening overhead generating linearspace meet limited memory algorithm algorithms maintenance algorithm iterative expansion monotonic close RBFS BFS iterativedeepening combinations linear space possible BFSDFBnB 391 Both MREC algorithm combine BFS 471 MA Fifteen Puzzle memory 241 One 451 Unlike RBFS bestfirst order node costs Unfortunately fewer nodes iterativedeepening iterative expansion expand new nodes Several algorithms iterativedeepening try reduce thresholds In order guarantee revert DFBnB developed reduce including DFS 51 IDACR node regeneration 46 MIDA number iterations iterativedeepening values larger minimum value exceeded finding optimal goal node goal complete final iteration overhead 521 They setting successive threshold previous algorithms On problems state spaces trees problem al gorithms use exponential space detect prevent generating 274 W Zhung RE KorfArtificiul Intelligence 79 1995 241292 duplicate nodes Taylor Korf 501 proposed scheme duplicate node pruning limited memory The idea learn structure state space given problem small exploratory structure finite automata search avoid duplicate node expansions search encode problemsolving 65 Complexity transitions The earliest evidence complexity BFS 201 Huberman systems Cheeseman complexity intelligent transitions satisfaction problems graph coloring Symmetric Traveling Salesman Problem The complexity attention researchers transitions Karp Pearls averagecase transitions complexity circuit constraint exist NPhard problems 171 discussed complexity problems attracted constraintsatisfaction including Hamiltonian et al 4 empirically 5283454 Hogg transitions showed Most recently Zhang Pemberton 565960 proposed method exploiting transitions approximate problems Their method allows BFS DFBnB solutions quickly obtain better solutions op combinatorial complexity high quality timization truncated DF sooner approximate BnB effective problem state space large number distinct edge costs On Asymmetric Traveling Salesman Problem DPBnB method runs faster finds better solutions local search method optimal solutions 551 This method 7 Conclusions We studied search algorithms DFBnB depthfirst branchandbound search RBFS Due linearspace nodes search depth Using random bestfirst search BFS use space linear search depth including ID recursive best algorithms expand typically uses space exponential iterativedeepening requirement proved exponential asymptotically optimal random iterativedeepening time We proved tree model analytically DPBnB expands 0 d N nodes average d goal depth N expected number optimal nodes expanded BFS We showed DFBnB RBFS BFS runs average asymptotically tree number nodes expanded linearspace depth Od4 Our analytic transition search performance algorithms Asymmetric Traveling Salesman Problem heuristic branching factor BFS factor random DFBnB tree integer edge costs Overall exponential algorithms average number samecost successfully average number samecost children tree effective branching existence complexity DFBnB predict In addition studied RBFS tree explain previously iterativedeepening equal greater observed anomaly children results WI Zhang RE KorfArtQkial Intelligence 79 1995 241292 275 algorithms large problems It run slower algorithm linearspace algorithms choice optimally formulated tree bounded depth algorithms DFBnB linearspace linearspace BFS compete linearspace space requirement algorithms usually large difficult problems Among exponential Therefore solving inapplicable statespace graph known cutoff depth DFBnB requires exponential boundeddepth boundeddepth tree problem computation problems best problem state space represented tree RBFS adopted problem represented solved polynomial time Acknowledgements like We thank Mark Cassorla Peter Cheeseman Sheila Greibach Lars Hagen Tad Hogg Judea Pearl Joe Pemberton Curt Powley Greg Provan Roberto Schonmann greatly research We want improved reviewers previous papers 57581 paper based SekWah Tan Colin Williams thank related reviewers comments clarity paper anonymous helpful discussions anonymous Appendix A Search algorithms This appendix contains pseudocode descriptions bestfirst search BFS iterativedeepening recursive bestfirst DFBnB depthfirst search ID branchandbound RBFS In following descriptions root represents root node initial state state space tree costn cost node n A1 Bestrst search The BFS algorithm tree search given Fig Al list open maintain current frontier nodes The algorithm starts BFS root BFShot open 0 n t WHILE n root goal EXPAND n generating INSERT DELETE n open n t minimumcost children node evaluating open children node open Fig Al Bestfirst search algorithm 276 W Zhung RE KorfArftjicial Intelligence 79 19951 241292 DFBnBn GENERATE k children n rz1rnk EVALUATE SORT increasing order cost FOR I k IF costn 1 IF ni goal node u ELSE DFBnBni COStni ELSE RETURN RETURN Fig A2 Depthfirst branchandbound algorithm IDroot threshold cost root nextthreshold CK REPEAT DFS root threshold nextthreshold nex_threshold oc DFSrl FOR child II n IF n goal node costni threshold EXIT optimal goal node IZ IF cost n threshold DFS q ELSE IF costn nextthreshold nextthreshold costni RETURN Fig A3 Iterativedeepening algorithm A2 Depthjirst branchandbound Fig A2 recursive version DFBnB node ordering The toplevel root node DFBnB root initial upper bound u A3 Iterativedeepening The iterativedeepening ID algorithm repeatedly calls depthfirst search procedure iteration increasing thresholds The global variables olds current iterations use node ordering implemented It cost threshold nextthreshold node cutoff thresh search DFS n Fig A3 starts ID root respectively The depthfirst recursively N Hang RE KorfArcial Inrelligence 79 1995 241292 217 u RETURN costn RBFSnFnlu costn n goal n children child costn Fn IF IF IF FOR IF ELSE Fi Costni ni n Fi EXIT optimal RETURN 00 MAXFncostni goal node n SORT ni Fi IF WHILE Fl child U Fl increasing F2 order Fi RHFSnlFl Fl INSERT nl Fl MINuF21 order sorted RETURN Fl Fig A4 Recursive bestfirst search algorithm A4 Recursive bestfirst search The RBFS algorithm given Fig A4 Fn stored value node n u local upper bound The initial RBFS root cost root oo Appendix B Proofs This appendix contains proofs lemmas theorems Section 2 tree monotonic node costs total number Lemma 21 On statespace optimal goal cost lower bound nodes costs strictly complexity ofjinding optimal goal node total number nodes costs equal optimal goal cost upper bound complexity fmding optimal goal node guaranteed algorithm based 71 Assume algorithm A guaranteed Proof We prove lower bound showing optimal goal node expand nodes costs optimal goal cost This contradiction converse optimal goal node tree monotonic node costs skips node cost A optimal goal cost Let T tree monotonic node costs Suppose strictly applied optimal goal cost We create tree T T node n T child optimal goal node T cost fn Note node costs monotonic applied Consequently optimality applied optimal goal node T contradicting T Since algorithm A skips node n new tree T T expand node n cost fn algorithm A assumption T A skip node n algorithm A 278 W Zlzung RE KotfArtijiciul Intelligence 79 1995 241292 c d Fig B 1 Structure random tree proving Lemma 23 algorithm We prove upper bound showing finds optimal goal node expands nodes costs equal node optimal goal cost Bestfirst search costs bestfirst nodeselection strategy uses BFS expands node cost x node cost greater x chosen expansion Thus BFS expand node cost greater optimal goal cost Cl algorithm By monotonicity Lemma 23 On random tree T b d bpo 1 d expected cost oj optimal goal nodes decreases bpo increases given b given po 0 In zero po I given 6 particular b 32 given p0 0 expected goal cost approaches Proof Let cd expected optimal goal cost random Fig B 1 We prove lemma induction depth d Consider random depth Fig Bl c 1 expected cost minimum costs k random variable mean b tree T b d shown tree k edge c 1 Eminet e2 ek I B1 c 1 decreases It evident independent increase Notice approaches zero ei nonzero probability zero bpo 1 implies po 0 Thus b increases identically distributed random variables k increases minimum taken Increasing b causes k infinity c 1 As inductive step assume cd 1 decreases approaches zero b increases fixed po 0 Let c d 1 optimal goal cost subtree rooted ith child node root node random tree T b d shown Fig computed B1 b Then cd cd Eminet cld lec2d lekqd I B2 random variable When b increases inductive assumption Thus b increases b cc 12 cd decreases random variables decreases b increases Similarly variable e cid 1 nonzero probability k following minimum e c d 1 decreases reaches e W Zhang RE KorfArtcial Intelligence 79 1995 241292 279 Tbdc Fig B2 Structure random tree proving Theorem 26 zero b oc po 0 ci d 1 0 case Consequently cd 0 b 00 concludes claim case b t 00 The fact optimal goal cost decreases reaches zero po 1 given b proved similarly cid144i tree monotonic node costs bestfirst search optimal Lemma 24 On statespace optimal algorithms goal node tiebreaking nodes costs equal optimal goal cost use node costs ana guaranteed Proof This optimal goal cost Because bestfirst nodeselection expands node cost greater corollary Lemma 21 BFS expand nodes costs strategy BFS optimal goal cost q Theorem 26 Let NB b d expected number nodes expanded bestfirst expected number nodes expanded depthrst branch search ND bd andbound random tree T b d As d W dl Nbd b lxNBbi d il discussion Proof For convenience Tb d c By notation For reason DFBnB T b d c initial upper bound U denote random tree root node cost c tree Tb d root cost zero Tb d 0 let ND b d c u expected number nodes expanded random As shown Fig B2 root Tb d c k children nr n2 Ilk k random variable mean b Let ei edge cost root T b d c The ithchild children root generated sorted nondecreasing order costs Thus er e2 6 ek arranged left right Fig B2 We following root ith subtree zbd observations 12k lcei First subtracting root cost nodes upper bound affect number nodes expanded DFBnB Tb d c search Therefore initial upper bound u equal expanded T b d 0 initial upper bound u c That 280 W Zlzar RE KorfArtijiciul Intelligence 79 1995 241292 NDbdCU NDbdOuc Nbdcx NDbdOx I B3 Secondly expanded Tbdc upper bound u U That larger number nodes expanded DFBnB smaller upper bound initial upper bound u number expanded initial initial upper bound causes nodes NOdcu 6 NDbdcu U U B4 It searches subtree Tlbd Now consider DFBnB T bdO 1 el Fig B2 expanding No b d 1 el cx expected number nodes Let p goal cost Tl b d I 0 Then minimum goal cost Tl b d 1 el minimum upper bound searching Tt b d 1 el After subtree p ei TI 6 d I el searched subtree Tz b d 1 e2 explored root cost e2 current upper bound p el expected number nodes expanded Nobdlezpet T b d 1 ei 34 number nodes expanded upper bound decrease searching T2 6 d I e2 edge cost ei increase increases cause fewer nodes expanded Since root T b d 0 b expected children write isalsoanupperboundontheexpected independently NDbdlezpel generated k This NtbdOoc Nobdlc1xblNDbdle2pell B5 I expansion root T b d 0 By B3 BS NobdOcx NNobdlOcbINbdlOpeeI B6 Since p e I e2 p el e2 B4 rewrite B6 NDbd 10x 0 lNDbd lOp f 1 B7 Now consider ND 0 d 1 0 p expected number nodes expanded DFBnB T 0 d 1O initial upper bound p If T b d 1O searched BFS optimal goal node expected cost p expand Nn b d 1 return searched DFBnB upper bound p nodes average When T b d 1O nodes costs strictly p expanded nodes expanded BFS We Nobd lOp Nebd 1 038 Substituting B8 B7 write W Zhang RE KmfArtcial Intelligence 79 1995 241292 281 NDbdOm NDbdlOcablNgbdI1 NDbd20mblNgbdlNgbd22 hboom b dl lxbi il d B9 This proves lemma ND bOO 0 0 Corollary 27 On random tree Tbd Nbd 1 NB b d ND b d expected numbers nodes expanded bestrst search depthjGst branchandbound respectively Od Nbd Proof It directly follows Theorem 26 fact dl c Nsbi d lNBbd I il NBbi NBbd 1 d 1 0 BlO Theorem 28 The expected number nodes expanded depthfirst branchandbound forfinding optimal goal node random tree T b d d 00 1 6pd bpo 1 meaning depthrst branchandbound asymptoti cally optimal p constant Lemma 25 2 Od3 bpo 1 3 Od bpo 1 po probability zerocost edge Proof It evident totic expected complexity Na b NB Ld2 This allows use asymp BFS d 00 By Lemma 25 Theorem 26 bpo 1 NDbdcblxNBbid dl dl 2b 1 c NBbid Ld2J dl 2b 1 c Opd id2 epd The cases directly follow Lemma 25 Theorem 26 0 282 W Zhang RE KorfArfijiciul Intelligence 79 I 995 241292 Corollary 29 On random number nodes expanded depthfirst branchandbound po 1 given b b cx given po 0 tree T b d bpo 1 d f W expected approaches Od given edgecost distribution I given Proof When b x b optimal goal cost C approaches zero according upper bound searching TI b d 1 el approaches ei Fig B2 Since root subtree T b d 1 e cost e current upper bound ei willbeexpandedfori23kThenNobdle2el nodesinTbdlei 0 B7 Lemma 23 Thus po NDbdOm NDbd lOcx 1 B11 1 expansion root T b d 0 leads NDbdOocNDbd20oc2 NDbd 30cc 3 d 0 Theorem 210 On random iterativedeepening distribution tree T b d edge costs chosen continuous expands 0 Ns b d expected number nodes NB b d expected number nodes expanded bestfirst search On ran dom tree T 6 d edge costs chosen lattice distribution expands 0 Ns b d expected number nodes d asymptotically iterativedeepening optimal Proof When edge costs chosen unique probability ith iteration expands iterations deepening Na b d Hence Thus successive continuous node costs iteration expands new node expected number expected number nodes expanded iterative nodes average Consequently distribution NBA Nmbd c iONgbd I Now consider lattice edge costs chosen set sod al A a2A aA a0 al a2 il 0 constant relatively prime integers To include situation edges zero cost set a0 0 It clear node costs multiples A difference distinct node costs This means constant independent increases A cost threshold nonnegative d iteration The iteration cost threshold equal optimal goal cost C T 6 d expands nodes costs C nodes costs H Zhang RE KmfArtijicial Intelligence 79 1995 241292 283 equal C In words expected number nodes expanded iteration order expected complexity bestfirst search In addition iteration fewer nodes iteration expands maximum total number iterations When bpc 1 optimal goal cost surely remains bounded constant constant cost threshold A constant num total num Lemma 22 Therefore increases constant ber iterations contributes ber nodes expanded Hence case d constant multiplicative iterativedeepening constant iteration overhead asymptotically cost threshold exceed optimal The number iterations constant bpo 1 optimal goal cost case bps 1 The optimal goal cost Tb d grows linearly d expected number nodes costs expected number d case Intuitively cost following increases depth d First consider Cd Cd nodes costs particular cost exponential optimality follow This suggests proof iterativedeepening exponential increases execute iterativedeepening 1 amA aA optimal goal cost Cd Cd d This means constant number iterations The iteration Zd iterations Tb d obtaining optimal goal 1 shallower C d 1 equal cost threshold d It evident largest edge cost constant Zd Consider cost Cd tree T b d 1 T b d For simplicity set iterations Zd explore new nodes costs greater Cd Each iteration Zd Cd independent nodes d 00 p 1 constant iteration expands nodes iterations Z d Therefore expected number nodes expanded iterations Zd iterativedeepening Notice number nodes expanded iterations asymptotic expected number nodes expanded BFS d cm By definition 0 p K p K 0 large Thus total expected number nodes expanded 0 12 d Zi Z d2J This allows US use expected number total number nodes expanded iterations executes sequence sets iterations 2i optimal goal node T b d following Lemma 25 In addition c9pd Furthermore expands Opd NIDbd 2 2 0p 2K k p LdSJ iLd2J dl _ 1 P1 2K2KP iO 2Kp_ 1 Ppd Opd Hence iterativedeepening asymptotically optimal bpo 1 The proof executes deepening fact total number iterations search depth d bpe 1 This directly algorithm execute linear indicates total number iterations iterative follows Cd obtained Cd I constant When bpo 1 bestfirst search expands H d2 expected number nodes random tree T 0 d Lemma 25 asymptotic order expected number The optimal goal cost nodes expanded iteration iterativedeepening Cd equivalent 1 Lemma 22 Cdloglogd T h d satisfies liml Cd loglogd loglogd B12 xc function q5 log logd d 2 log log d 4 log log d log log d d m Therefore iterations iterativedeepening constant A minimum searching T 0 d increment cost threshold t o loglogd Notice Cd total number I C dA Ologlogd iteration Consider shallower tree T b irrdl ol T 0 d constant 0 g I For reason optimal goal cost C ad1 T b ludl satisfies C vdl loglogcTdl logloglrrdl B13 Tb function 4 B12 d 3 Function 4 B13 In following T 0 d problems structurally chosen time nodes costs interval penultimate expansion result Lemma 25 iteration iteration reach depth cTdl Following iteration iteration expected number nodes For iteration reason expected number nodes By definition 0 d K d K 0 d f x 11 x 1 Thus total expected number nodes expanded I expands Od I 1 expands expected number nodes iteration implies iteration 1 2 expands C cTdl Cd 0 adj 19 udl penultimate ad Od c constant O y denotes set WC xr x xt equivalently functions WI linlr_r positive constants L 10 oxx 0 W Zhang RE KorfIArtificial Intelligence 79 1995 241292 285 We iteration nodes costs interval prove time d 00 It sufficient 0 By B12 B13 C4C41 limd Cd expanded C ad Cd c cdl loglogd loglogodl loglogd loglogcd B14 d cc We prove cdl 2 crd log x increasing Therefore loglogd loglogad function X Thus log logcdl 3 log log gd 0 d Notice lim doo loglogd loglograd ima loglogd loglog log lim dco log 1 0 log d log d log B15 In calculation 21 On hand continuous B15 mean lim log Yn loglim Yx log log d log logrcd 0 c 1 This logy lim d03 loglogd loglogrrrdl 0 B16 We prove loglogd function loglogd Consider depths d d 1 T b d Following log introduced logrod B12 0 d We nondecreasing d d 0 B12 write Cd Cd 1 loglogd loglogd 1 loglogd 4loglogd 1 B17 Using similar calculation B15 simply prove lim dm loglogd loglogd 1 0 Because Cd Cd 1 0 B17 write t4tloglogd tloglogtd 1 2 0 meaning log log d decrease d d Therefore limmloglogd loglogcdl 0 B18 c 1 If loglogd increase d 286 W Zhung RE KmjArtijicial Intelligence 79 1995 241292 diinl_ loglogd 4loglogadl o B19 If 4 log log d function increases d hand loglogd loglogd growth rate greater Thus B16 E ologlogd iim_ loglogd 44oglogbdl fim loglogd loglogcdl 0 B20 Inequations B18 B20 lead B19 Thus jimw Cd C rod 0 combining optimal case bpo 1 0 B 14 B 16 B 19 Therefore iterativedeepening asymptotically Theorem 211 Let B asymptotic heuristic branching T 6 d b 1 B 1 edge costs chosen continuous distribution When bpo 1 edge costs chosen lattice distribution a0 0 al A a2A aA probabilities p respectively A 0 pop 2 relatively prime B integers al a2 chosen nonnegative approaches constant d 4 x solution greater equation factor random tree I c Pi jg iO 1 jj B21 factor lattice Proof When edge costs chosen continuous distribution node costs unique probability ratio number nodes given cost number nodes cost heuristic branching Consequently The heuristic branching factor Tb d edge costs chosen lifetime random 151 In agedependent tree T b d mapped length At end life object distribution a0 0 al A a2A anrA corollary results agedependent branching process object born time branching processes replaced 0 finite random number similar objects age 0 independently parent This process generating new objects continues long objects branching process present A random process follows The root node tree corresponds lifetime object represented node An edge cost corresponds end edge The cost tree node corresponds children object born One minor corresponding lifetime discrepancy original object zero However analysis nonzero number number root node cost zero defined cost root node defined added internal nodes tree cause problem object dies original object agedependent absolute time behave W Zhang RE KorfArticial Intelligence 79 1995 241292 287 Edge costs multiples loss generality divide edge node costs A treat edge node costs integers A node cost K linear combination A node costs Without nonzero edge costs Kklal kzazka B22 nonnegative requires integer B22 solution costs include cost k exponential dependent branching processes integers kl k2 k solutions nonnegative B22 351 In fact integer K 2a_l kl k2 91 indicating integers integer K K 00 Indeed amml linear Diophantine equation node expected number nodes age results K prove Let DK expected number nodes cost K corresponds expected number objects process Furthermore duced exactly time K AK time K branching process Thus write let PK expected number objects expected number objects die exactly time K corresponding branching born pro alive AK AK 1 PK DK B23 total expected number currently alive objects AK means total expected number objects alive previous expected number new borns time K PK minus objects died time K DK equal A K 1 plus expected number time point It shown 151 AK grows exponentially K K CQ AK qepK CJI eK B24 cl 0 constant 1 epK E oepK root equation U constant positive l x5 iO Eq B25 unique positive root b 1 151 B25 Further notice PK 0 increases K B24 This gives DK 0 child objects born parents die Therefore B23 expected number alive objects B24 PK AKAKlDK AK AK 1 cte PK _ ctePLKl ft ePK ft eficK cil eePKtePK iePCK B26 It evident PK AK This combining B24 B26 gives 28X II Zhcm KE KorfArtiJiciul Intellqrnce 79 1995 241292 PK czePePLh B27 I I ep c cl 1 e FK 41efiK Therefore hy B23 B24 B27 e lepK DK PK AK AK I cy2 PK _ CePK CePLKl 1 42eF q5I epK 1 epLKp qe qb3epK B28 wherecccIIcCfi Using B28 definition finally obtain heuristic branching factor 0and63eLK2eyKlelKleK B DK lim KwDK Because p positive solution Eq B21 q Eq B25 B e solution greater 212 On u rundom tree T b d h I bpo I edge costs Theorem chosen lattice distribution bestfirst search depthjirst branchandbound recursive hestjirst search effective branching iterativedeepening factor p B d 4 W p probability edge cost zero B asymptotic heuristic branching factor T b cl Q limit Cd d cx arrd C optimal goal cost superior RBFS asymptotically asymptotic effective branching Proof When bpo I edge costs chosen iterativedeepening Theorems 28 210 fact RBFS Therefore effective branching lattice distribution DFBnB optima1 d according 24 factor asymptotic factor p BFS We need 3 B d f cc notations proof Theorem 2 I 1 Let lattice edge costs chosen set CIQ OalAaA aA Again divide edge node costs A loss generality By Lemmas 21 24 BFS expands treat integers nodes costs optima1 goal cost C plus nodes cost C nodes cost greater We use results iterativedeepening agedependent C That adopt processes branching ZC 1 bd ZC B30 ZK corresponds expected number nodes costs equal K ZK died time K The number expected number objects W Bang RE KwfArtcial Intelligence 79 1995 241292 289 dead objects time K greater died time K equal time points K living object alive That equal number objects total number living objects time counted point DK ZK CAi iO B31 DK time K Following d 4 Thus AK expected number died objects living objects epK 2clepK c5t ekK cte B24 AK clekKl CAi iO 2cl eepi iO ePKl _ 1 2ct efi 1 ep 2ct e ep1 PK By B28 proof Theorem 211 DK 1 csePLK ePCK cseepK efiCK From B30 B31 B32 epc qe pcc1 e bd kl B32 B33 This means c ad od finally epc dominant Lemma 22 definition effective branching function Nn b d d Therefore factor Pdi QXZGG im J epadofd im Eva ePodld ep B34 Because B efi Theorem 210 B34 equivalent p B d CXI 0 References 1 I 1 E Balas P Toth Branch bound methods EL Lawler et al ed The Traveling Salesman Problem John Wiley Sons Essex 1985 361401 12 1 KG Binmore Mafhematical Analysis Cambridge University Press New York 1982 290 W Zhung RE KorfArciul Intelligence 79 1995 241292 131 141 151 161 171 181 I91 1101 IIll 112 I13 I14 I IS II6 197221 Proceedings restricted memory Arrif satisfiability problems hard problems results crossover point search Tech Report UCLAENG8139 Applied Science University California Los S Ghose A Acharya SC Sarkar Heuristic search WM Taylor Where 1991 331337 LD Auton Experimental PP Chakrabarti Intel 41 1989 P Cheeseman B Kanefsky IJCAI91 Sydney Australia JM Crawford Proceedings AAAI93 Washington DC 1993 2127 A Dechter A probabilistic analysis branchandbound Cognitive Systems Lab School Engineering Angeles CA 198 I R Dechter J Pearl Generalized 1985 505536 EW Dijkstra A note problems P ErdBs RL Graham On linear diophantine problem frobenius Actu Arithmarica 399408 SH Fuller JG Gaschnig JJ Gillogly Analysis alphabeta pruning algorithm Tech Report CarnegieMellon University Computer Science Department Pittsburgh PA 1973 MR Garey DS Johnson Compurers und Intractability A Guide Theory ofNPCompleteness Freeman New York 1979 JG Gaschnig Performance measurement Report CMUCS79124 1979 ML Ginsberg WD Harvey JM Hammersley Postulates analysis certain search algorithms PhD Thesis Tech Computer Science Department CarnegieMellon University Pittsburgh PA lterative broadening Artif Infell 55 1992 367383 subadditive processes Ann Probability 2 1974 652680 connexion graphs Numer Math 1 1971 269271 search strategies optimality A J ACM 32 21 1972 bestfirst I T Harris The Theory Branching Processes Springer Berlin 1963 T Hart NJ Nilsson B Raphael A formal basis heuristic determination paths lEEE Trans Syst Sci Cybern 4 1968 minimum cost lOO 107 artificial intelligence systems Art fntell 33 1987 I I7 I BA Huberman T Hogg Phase transitions 15171 I I8 I N Huyn R Dechter J Pearl Probabilistic analysis complexity A Artif Infell 15 1980 241254 I I9 I T lbaraki Enumerafive Approaches Combinatorial Opfimizalionfart I Annals Operations Research 10 Scientific Basel Switzerland 1987 1201 RM Karp J Pearl Searching optimal path tree random costs Artif Intell 21 1983 99l 17 12 I I JFC Kingman The birth problem agedependent branching process Ann Probability 3 1975 790801 I22 I RE Korf Depthfirst 97109 iterativedeepening An optimal admissible tree search Artif Intell 27 1985 I23 I RE Korf Realtime heuristic search Artif Intell 42 1990 1892 I I bestfirst search Artif Intel 62 1993 4178 1241 RE Korf Linearspace 25 I V Kumar Search branchandbound SC Shapiro ed Encyclopedia ofArtificial Intelligence Wiley lnterscience New York 2nd ed 1992 14681472 I26 I V Kumar DS Nau L Kanal A general branchandbound formulation andor graph game tree search York 1988 91130 L Kanal V Kumar eds Seurch Artijicial Inrelligence SpringerVerlag New I27 I P Langley Systematic nonsystematic search strategies Proceedingsfirst Internafional Conference AI Plunning Sysrems College Park MD 1992 145152 I28 I T Larrabee Y Tsuji Evidence satisfiability threshold random 3CNF formulas Working Nores AAAI I993 Spring Symposium AI und NPHard Problems Stanford CA 1993 112l 18 1291 EL Lawler JK Lenstra AHGR Kan DB Shmoys The Traveling Salesman Problem John Wiley Sons Essex 1985 30 I EL Lawler DE Wood Branchandbound methods survey Operations Research 14 1966 699719 W Bang RE KorfArtifcial Intelligence 79 1995 241292 291 13 11 A Mahanti S Ghosh DS Nau AK Pal LN Kanal Performance IDA trees graphs Proceedings AAAI92 San Jose CA 1992 539544 1321 CJH McDiarmid Probabilistic analysis tree search GR Gummett DJA Welsh eds Disorder Physical Systems Oxford Science 1990 249260 I33 I CJH McDiarmid GMA Provan An expectedcost analysis backtracking nonbacktracking algorithms Proceedings IJCAI91 Sydney Australia 199 1 172177 341 D Mitchell B Selman H Levesque Hard easy distributions SAT problems Proceedings AAAI92 San Jose CA 1992 459465 1351 I Niven HS Zuckerman An Introduction Theory Numbers John Wiley Sons New York 4th ed 1980 36 1 CH Papadimitriou K Steiglitz Combinatorial Optimization Algorithms Complexity Prentice Hall Englewood Cliffs NJ 1982 I 371 BG Patrick An analysis iterativedeepeningA PhD Thesis Computer Science Department McGill University Montreal Que 1991 138 I BG Patrick M Almulla MM Newborn An upper bound complexity iterativedeepening A Ann Math Art Intell 5 1992 265278 391 J Pearl Heuristics AddisonWesley Reading MA 1984 I40 I J Pearl Branching factor SC Shapiro ed Encyclopedia Artificial Intelligence Wiley Interscience New York 2nd ed 1992 127128 141 1 I Pohl Practical theoretical considerations heuristic search algorithms EW Elcock D Michie eds Machine Intelligence 8 Wiley New York 1977 5572 I42 I PW Purdom Search rearrangement backtracking polynomial average time Art Intell 21 1983 117133 I43 1 D Ratner M Warmuth Finding shortest solution n x n extension 15puzzle intractable Proceedings AAAI86 Philadelphia PA 1986 168172 44 1 A Rtnyi Probability Theory NorthHolland Amsterdam 1970 45 S Russell Efficient memorybounded search methods Proceedings ECAI92 Vienna Austria 1992 1461 UK Sarkar PP Chakrabarti S Ghose SC DeSarkar Reducing teexpansions iterativedeepening search controlling cutoff bounds Artt Intell 50 1991 207221 47 I AK Sen A Bagchi Fast recursive formulations bestfirst search allow controlled use memory Proceedings IJCAI89 Detroit MI 1989 297302 481 DR Smith Random trees analysis branch bound procedures J ACM 31 1984 163188 1491 HS Stone P Sipala The average complexity depthfirst search backtracking cutoff IBM J Research Development 30 1986 242258 SOI LA Taylor RE Korf Pruning duplicate nodes depthfirst search Proceedings AAAI93 Washington DC 1993 756761 15 1 1 NR Vempaty V Kumar RE Korf Depthfirst vs bestfirst search Proceedings AAAI91 Anaheim CA 1991 434440 1521 BW Wah MIDA IDA search dynamic control Tech Report UILUENG912216 CRHC 919 Center Reliable HighPerformance Computing Coordinated Science Lab College Engineering University Illinois Urbana ChampaignUrbana IL 1991 53 I BW Wah CF Yu Stochastic modeling branchandbound algorithms bestfirst search IEEE Trans Sofrware Engineering 11 1985 922934 54 CP Williams T Hogg Extending deep structure Proceedings AAAI93 Washington DC 1993 152157 551 W Zhang Truncated branchandbound case study asymmetric TSP Working Notes AAAI I993 Spring Symposium AI NPHard Problems Stanford CA 1993 160166 1561 W Zhang Analyses linearspace search algorithms applications PhD Thesis Computer Science Department University California Los Angeles Los Angeles CA 1994 57 1 W Zhang RE Korf An averagecase analysis branchandbound applications summary results Proceedings AAAI92 San Jose CA 1992 545550 581 W Zhang RE Korf Depthfirst vs bestfirst search New results Proceedings AAAI93 Washington DC 1993 769775 192 W Zhanq KE KofArrcial Inrelliencr 79 1995 241292 1591 W Zhang JC Pemberton Epsilontransformation Exploiting combinatorial University California Los Angeles CA 1994 160 I W Zhang JC Pemberton Epsilontransformation optimization problems Tech Report UCLACSD940003 phase solve transitions Computer Science Department exploiting phase transitions solve combinatorial optimization problemsinitial results Pmceedings AAAI94 Seattle WA 1994