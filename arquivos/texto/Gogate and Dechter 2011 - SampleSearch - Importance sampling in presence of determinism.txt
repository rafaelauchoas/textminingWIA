Artiﬁcial Intelligence 175 2011 694729 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint SampleSearch Importance sampling presence determinism Vibhav Gogate a1 Rina Dechter b Computer Science Engineering University Washington Seattle WA 98195 USA b Donald Bren School Information Computer Sciences University California Irvine Irvine CA 92697 USA r t c l e n f o b s t r c t The paper focuses developing effective importance sampling algorithms mixed probabilistic deterministic graphical models The use importance sampling graphical models problematic generates useless zero weight samples rejected yielding ineﬃcient sampling process To address rejection problem propose SampleSearch scheme augments sampling systematic constraintbased backtracking search We characterize bias introduced combination search sampling derive weighting scheme yields unbiased estimate desired statistics probability evidence When computing weights exactly complex propose approximation weaker guarantee asymptotic unbiasedness We present results extensive empirical evaluation demonstrating SampleSearch outperforms schemes presence signiﬁcant determinism 2010 Elsevier BV All rights reserved Article history Received 31 July 2009 Received revised form 1 October 2010 Accepted 21 October 2010 Available online 5 November 2010 Keywords Probabilistic inference Approximate inference Importance sampling Markov chain Monte Carlo Bayesian networks Markov networks Satisﬁability Model counting Constraint satisfaction 1 Introduction The paper investigates importance sampling algorithms answering weighted counting marginal queries mixed probabilistic deterministic networks 14 The mixed networks framework treats probabilistic graphical models Bayesian Markov networks 5 deterministic graphical models constraint networks 6 single graphical model Weighted counts express probability evidence Bayesian network partition function Markov network number solutions constraint network Marginals seek marginal distribution variable called belief updating posterior estimation Bayesian Markov network It straightforward design importance sampling algorithms 79 approximately answering counting marginal queries variants summation problems importance sampling designed Weighted counts sum function domain marginal ratio sums The main idea transform summation expectation special distribution called proposal importance distribution easy sample Importance sampling generates samples proposal distribution approximates expectation called true average true mean weighted average samples called sample average sample mean The sample mean shown unbiased estimate original summation importance sampling yields unbiased estimate weighted counts For marginals importance sampling compute ratio unbiased estimates yielding asymptotically unbiased estimate Corresponding author Email addresses vgogatecswashingtonedu V Gogate dechtericsuciedu R Dechter 1 This work author graduate student University California Irvine 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201010009 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 695 In presence hard constraints zero probabilities importance sampling suffer rejection prob lem The rejection problem occurs proposal distribution faithfully capture constraints mixed network Consequently samples generated proposal distribution zero weight tribute sample mean In extreme cases probability generating rejected sample arbitrarily close yielding completely wrong estimates In paper propose sampling scheme called SampleSearch remedy rejection problem SampleSearch com bines systematic backtracking search Monte Carlo sampling In scheme sample supposed rejected algorithm continues instead randomized backtracking search sample nonzero weight This problem generating nonzero weight sample equivalent problem ﬁnding solution satisﬁability SAT constraint satisfaction problem CSP SAT CSPs NPcomplete problems idea generating sample solving NPcomplete problem ineﬃcient However recently SATCSP solvers achieved unprecedented success able solve large industrial problems having million variables seconds2 Therefore solving constant number NPcomplete problems approximate Pcomplete problem weighted counting longer unreasonable We SampleSearch generates samples modiﬁcation proposal distribution backtrackfree The backtrackfree distribution obtained removing partial assignments lead zero weight sample In particular backtrackfree distribution zero target distribution wish sample zero We propose schemes compute backtrackfree probability generated samples required com puting sample weights The ﬁrst computationally intensive method involves invoking CSP SAT solver O n d times n number variables d maximum domain size The second scheme approximates backtrackfree probability consulting information gathered SampleSearchs operation This scheme desirable properties runs linear time ii yields asymptotically unbiased estimate iii provide upper lower bounds exact backtrackfree probability Finally present empirical evaluation demonstrating power SampleSearch We implemented SampleSearch IJGPwcIS 10 powerful importance sampling technique uses generalized belief propagation algorithm 11 called Iterative Join Graph Propagation IJGP 1213 construct proposal distribution wcutset RaoBlackwellised sampling 14 reduce variance The search implemented minisat SAT solver 15 We conducted ex periments tasks counting models SAT formula b computing probability evidence Bayesian network partition function Markov network c computing posterior marginals Bayesian Markov networks For model counting compared approximate algorithms ApproxCount 16 SampleCount 17 Rel sat 18 IJGPwcIS vanilla importance sampling scheme classes benchmark instances Our experiments instances given timebound SampleSearch yields solution counts closer true counts orders magnitude compared schemes It clearly better IJGPwcIS failed benchmark SAT instances unable generate single nonzero weight sample hours CPU time For problem computing probability evidence Bayesian network compared SampleSearch Variable Elimination Conditioning VEC 19 advanced generalized belief propagation scheme called Edge Deletion Belief Propagation EDBP 20 IJGPwcIS linkage analysis 21 relational 22 benchmarks Our experiments instances estimates output SampleSearch accurate output EDBP IJGPwcIS VEC solved instances exactly remaining instances substantially inferior For posterior marginal task experimented linkage analysis benchmarks partially deterministic grid benchmarks relational benchmarks logistics planning benchmarks Here compared accuracy SampleSearch schemes generalized belief propagation schemes Iterative Join Graph Prop agation 1213 Edge Deletion Belief Propagation 20 adaptive importance sampling scheme called Evidence Prepropagated Importance Sampling EPIS 23 Again grid instances SampleSearch consis tently yields estimates having smaller error schemes Based large scale experimental evaluation conclude SampleSearch consistently yields good approxi mations In particular large instances substantial determinism SampleSearch yields order magnitude improvement stateoftheart schemes The paper based earlier conference papers 2425 The present article contains detailed general analysis proofs new bounding approximations described Section 421 new experimental results The rest paper organized follows In Section 2 present notation preliminaries graphical models importance sampling In Section 3 present rejection problem overcome backtrackfree distribution Section 4 describes SampleSearch scheme improvements In Section 5 present experimental results conclude Section 6 2 See results SAT competitions available httpwwwsatcompetitionorg 696 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 2 Preliminaries background We denote variables upper case letters X Y values variables lower case letters x y Sets variables denoted bold upper case letters X X1 Xn sets values denoted bold lower case letters x x1 xn X x denotes assignment value variable X x denotes assignment values variables set We denote Di set possible values Xi called domain Xi We denote projection assignment x set S X xS cid2 xX denotes sum possible values variables X value EQ X random variable X respect distribution Q deﬁned EQ X x X x EQ X2 V Q X X deﬁned V Q X cid2 cid2 x1 X1 cid2 x2 X2 cid2 cid2 xn Xn The expected x X xQ x The variance We denote functions upper case letters F C scope set arguments function F scopeF Frequently given assignment y superset Y scopeF abuse notation write F yscopeF F y 21 Markov Bayesian constraint networks Deﬁnition 1 Graphical models Markov networks A discrete graphical model denoted G Markov network noted T 3tuple cid5X D Fcid6 X X1 Xn ﬁnite set variables D D1 Dn ﬁnite set domains Di domain variable Xi F F 1 Fm ﬁnite set discretevalued functions poten tials Each function F deﬁned subset Si X variables A graphical model G represents joint distribution P G variables X given P Gx 1 Z mcid3 i1 F ix Z cid4 mcid3 xX i1 F ix Z normalization constant referred partition function The primary queries Markov networks computing posterior distribution marginals variables Xi X ﬁnding partition function Each graphical model associated primal graph captures dependencies present model Deﬁnition 2 Primal graph The primal graph graphical model G cid5X D Fcid6 undirected graph GX E variables G vertices edge variables appear scope function F F Deﬁnition 3 Bayesian belief networks A Bayesian network graphical model B cid5X D G Pcid6 G X E directed acyclic graph set variables X The functions P P 1 Pn conditional probability tables P scopeP Xi set parents Xi G The primal graph Bayesian network P Xipai pai called moral graph When entries CPTs 0 1 called deterministic functional CPTs An evidence E e instantiated subset variables A Bayesian network represents joint probability distribution given P BX i1 P Xipai answer query deﬁned joint distribution In paper consider queries computing probability evidence P E e b computing posterior marginal distribution P XiE e variable Xi X E cid5 n Deﬁnition 4 Constraint networks A constraint network graphical model R cid5X D Ccid6 C C1 Cm set constraints Each constraint Ci 01 function deﬁned subset variables Si called scope Given assignment Si si constraint satisﬁed Cisi 1 A constraint expressed pair cid5R Sicid6 R relation deﬁned variables Si contains tuples Si si Cisi 1 The primal graph constraint network called constraint graph The primary query constraint network decide solution ﬁnd assignment X x variables constraints satisﬁed prove assignment exists Another important query counting number solutions constraint network A constraint network represents uniform distribution solutions Propositional satisﬁability A special case constraint network propositional satisﬁability problem SAT A propositional formula F expression deﬁned variables having binary domains False True 0 1 Every Boolean formula converted equivalent formula conjunctive normal form CNF A CNF formula F conjunction denoted clauses Cl1 Clt denoted set Cl1 Clt clause disjunction denoted literals literals variables V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 697 negations For example Cl P Q R clause variables P Q R P Q R literals A clause satisﬁed literals assigned value True 1 A solution model formula F assignment values variables clauses satisﬁed Common queries SAT satisﬁability ﬁnding model proving exists model counting counting number models solutions 22 Mixed networks Throughout paper use framework mixed networks deﬁned 34 Mixed networks represent deterministic information explicitly form constraints facilitating use constraint processing techniques developed past decades eﬃcient probabilistic inference This framework includes Bayesian Markov constraint networks special case Therefore inference tasks equivalent consider mixed net work view allowing unifying treatment problems single framework For example problems computing probability evidence Bayesian network partition function Markov network counting solutions constraint network expressed weighted counting mixed networks Deﬁnition 5 Mixed network A mixed network fourtuple M cid5X D F Ccid6 X X1 Xn set random variables D D1 Dn set domains Di domain Xi F F 1 Fm set nonnegative real deﬁned subset variables Si X scope C C1 C p set valued functions F constraints 01 functions A mixed network represents joint distribution X given PMx cid6 cid5 1 Z 0 m i1 F ix x solC solC set solutions C Z m i1 F ix normalizing constant The primal graph mixed network variables vertices edge variables appear scope function F F constraint C C xsolC cid2 cid5 We deﬁne queries mixed network In paper focus following queries Deﬁnition 6 The weighted counting task Given mixed network M cid5X D F Ccid6 weighted counting task compute normalization constant given Z cid4 mcid3 xSolC i1 F ix Equivalently represent constraints C 01 functions rewrite Z Z cid4 mcid3 pcid3 F ix C jx xX i1 j1 We refer Z weighted counts 1 2 Deﬁnition 7 Marginal task Given mixed network M cid5X D F Ccid6 marginal task compute marginal distri bution variable Namely variable Xi xi Di compute P xi δxi xPMx δxi x 1 Xi assigned value xi x 0 cid7 cid4 xX To able use constraint portion mixed network effectively remainder paper require zero probabilities mixed network represented constraints It easy deﬁne network Deﬁnition 8 Modiﬁed mixed network Given mixed network M cid5X D F Ccid6 modiﬁed mixed network fourtuple Mcid9 cid5X D F C i1 cid9cid6 C cid7 cid9 C H im 0 F isi 0 1 H iSi si 3 H expressed relation The set constraints C distribution PM cid9 called ﬂat constraint network probability 698 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Clearly modiﬁed mixed network M original mixed network M equivalent PMcid9 PM It easy weighted counts mixed network specialize probability evidence Bayesian network b partition function Markov network c number solutions constraint network The marginal task expresses task computing posterior marginals Bayesian Markov network cid9 23 Importance sampling approximating weighted counts marginals Importance sampling 79 general Monte Carlo simulation technique estimating statistics given target distribution Since hard sample target distribution main idea generate samples easytosimulate distribution Q called proposal trial importance distribution estimate statistics target distribution weighted sum samples The weight sample ratio probability generating sample target distribution probability based proposal distribution In subsection weighted counts posterior marginals approximated importance sampling For details theoretical results presented subsection refer reader 826 We assume paper proposal distribution speciﬁed product form variable ordering o X1 Xn Q X ncid3 i1 Q iXi X1 Xi1 Q speciﬁed Bayesian network CPTs Q Q 1 Q n ordering o We generate sample product form speciﬁcation follows For 1 n sample Xi xi conditional distribution Q Xi X1 x1 Xi1 xi1 set Xi xi This referred ordered Monte Carlo sampler logic sam pling 5 Thus Q easy sample assume Q expressed product form speciﬁed polynomial space ncid3 Q X Q iXi X1 Xi1 ncid3 Q iXiYi 4 i1 Yi X1 Xi1 The size set Yi assumed bounded constant i1 Throughout paper use notion biased unbiased estimators deﬁne Deﬁnition 9 Unbiased asymptotically unbiased estimator Given probability distribution Q statistics θ Q N samples drawn Q function cid8θN deﬁned samples unbiased estimator θ EQ cid8θN θ Similarly function cid9θN asymptotically unbiased estimator θ limN EQ cid9θN θ Clearly unbiased estimators asymptotically unbiased Note denote unbiased estimator statistics θ cid8θ asymptotically unbiased estimator cid9θ arbitrary estimator θ The notion unbiasedness asymptotic unbiasedness important helps characterize performance estimator explain brieﬂy The meansquared error estimator θ given MSEθ EQ EQ cid10 EQ cid11 cid10 θ θ2 cid11 cid10 θ 2 cid10 θ 2 2EQ θ θ θ 2 cid10 cid11 EQ θ 2 cid11 EQ θ2 2EQ θ θ θ 2 cid11 The bias θ given B Q θ EQ θ θ The variance θ given V Q θ EQ EQ θ 2 cid11 cid10 θ 2 From deﬁnitions bias variance meansquared error cid11 2 B Q θ MSEθ V Q θ cid10 5 6 7 8 In words mean squared error estimator equal bias squared plus variance For unbiased estimator bias zero reduce mean squared error reducing variance In case asymptotically unbiased estimator bias goes zero number samples tend inﬁnity However ﬁnite sample size nonzero bias 699 9 10 11 12 13 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 231 Estimating weighted counts Consider expression weighted counts Deﬁnition 6 Z cid4 mcid3 pcid3 F ix C jx xX i1 j1 If proposal distribution Q X cid5 m i1 F ix cid5 p j1 C jx Q x Z cid4 xX Q x EQ cid5 p cid5 m i1 F ix cid12 cid5 m i1 F ix cid13 p j1 C jx Q x j1 C jx 0 Q x 0 rewrite Eq 9 follows cid5 Given independent identically distributed iid samples x1 xN generated Q estimate Z cid8Z N 1 N Ncid4 k1 cid5 cid5 m i1 F ixk p j1 C jxk Q xk 1 N Ncid4 k1 cid15 cid14 xk w cid15 cid14 xk w cid5 m i1 F ixk cid5 p j1 C jxk Q xk weight sample xk By deﬁnition variance weights given cid10 cid11 wx V Q cid4 cid14 wx Z cid15 2 Q x xX We estimate variance cid8Z N cid16V Q cid8Z N 1 NN 1 Ncid4 cid14 cid14 w cid15 xk cid8Z N cid15 2 k1 shown cid16V Q cid8Z N unbiased estimator V Q cid8Z N cid10 cid11 cid16V Q cid8Z N EQ V Q cid8Z N We 1 EQ cid8Z N Z cid8Z N unbiased 2 limN cid8Z N Z probability 1 follows central limit theorem 3 EQ cid16V Q cid8Z N V Q cid8Z N V Q wxN Therefore V Q cid8Z N reduced increasing number samples N reducing variance j1 C jx sample x wx Z yielding weights It easy Q x p optimal zero variance estimator However making Q x j1 C jx NPhard order small MSE practice recommended Q close possible function tries approximate case m i1 F ix m i1 F ix cid5 cid5 cid5 cid5 cid5 cid5 p m i1 F ix p j1 C jx 232 Estimating marginals The marginal problem deﬁned cid4 P xi δxi xPMx xX PM deﬁned PMx 1 Z mcid3 i1 pcid3 F ix C jx j1 Given proposal distribution Q x satisfying PMx 0 Q x 0 rewrite Eq 14 follows P xi cid4 xX δxi xPMx Q x Q x EQ cid12 cid13 δxi xPMx Q x 14 15 16 700 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Given independent identically distributed iid samples x1 xN generated Q estimate P xi m i1 F ixk Z Q xk δxi xkPMxk Q xk cid17P N xi 1 N j1 C jxk 1 N δxi xk Ncid4 Ncid4 cid5 cid5 p 17 k1 k1 Unfortunately Eq 17 unbiased estimator P xi evaluated Z known We sacriﬁce unbiasedness estimate P xi notion properly weighted samples Deﬁnition 10 A properly weighted sample See 26 A set weighted samples xk wxkN said properly weighted respect distribution P discrete function H k1 drawn distribution G cid10 cid14 cid15 cid14 H xk w xk cid15cid11 EG cEP cid10 cid11 Hx c normalization constant common samples Given set N weighted samples drawn P estimate EP Hx cid2 cid10 cid11 Hx cid9EP N k1 Hxkwxk cid2 N k1 wxk Substituting Eq 15 Eq 16 cid5 cid12 cid5 P xi 1 Z EQ δxi x m i1 F ix Q x cid13 p j1 C jx 18 It easy prove Proposition 1 Given wx δxi x PM cid5 m i1 F x Q x cid5 p j1 C j x set weighted samples xk wxkN k1 properly weighted respect Therefore estimate P xi cid2 cid18P N xi N k1 wxkδxi xk cid2 N k1 wxk 19 It easy prove limN Ecid18P N xi P xi asymptotically unbiased Therefore weak law large numbers sample average cid18P N xi converges surely P xi N Namely cid18P N xi P xi probability 1 weak law large numbers lim N In order small estimation error proposal distribution Q close possible target distribution PM 3 Eliminating rejection problem backtrackfree distribution In section rejection problem problem mitigated modifying proposal distribution Given mixed network M cid5X D F Ccid6 proposal distribution Q deﬁned X suffers rejection problem probability generating sample Q violates constraints PM expressed C relatively high When sample x violates constraints C weight wx zero effectively rejected sample average In extreme case probability generating rejected sample arbitrarily close generating large number samples estimate weighted counts given Eq 11 zero estimate marginals given Eq 19 illdeﬁned Clearly Q properly encodes zeros M rejection Deﬁnition 11 Zero equivalence A distribution P zero equivalent distribution P Deﬁnition 8 equivalent Namely set consistent solutions cid9 iff ﬂat constraint networks Clearly given mixed network M cid5X D F Ccid6 representing PM given proposal distribution Q Q 1 Q n zero equivalent PM sample x generated Q satisﬁes PMx 0 sample generated Q rejected Because Q expressed product form Q X i1 Q Xi X1 Xi1 o X1 Xn Q zero equivalent PM modifying components Q Xi X1 Xi1 o To accomplish set Q Q 1 Q n backtrackfree o relative constraints C The following deﬁnitions formalize notion cid5 n V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 701 Algorithm 1 Sampling backtrackfree distribution Input A mixed network M cid5X D F Ccid6 proposal distribution Q ordering o oracle Output A sample x1 xn backtrackfree distribution Q F Q x 1 n Xi x Q Xi x Q F value xi Di y x xi oracle says y globally consistent wrt C Q F xi x 0 Normalize Q F x x xi Xi x generate sample Xi xi 1 2 3 4 5 6 7 8 9 10 return x Deﬁnition 12 Consistent globally consistent partial sample Given set constraints C deﬁned X X1 Xn partial sample x1 xi consistent violate constraint C A partial sample x1 xi globally consistent extended solution C extended assignment n variables satisﬁes constraints C Note consistent partial sample globally consistent Deﬁnition 13 Backtrackfree distribution Q wrt C Given mixed network M cid5X D F Ccid6 proposal distribution i1 Q Xi X1 Xi1 ordering o backtrackfree distribution Q F Q Q 1 Q n representing Q X Q F cid5 n 1 Q F n Q o wrt C Q F X α Q ixix1 xi1 0 cid7 Q F xix1 xi1 α normalization constant cid5 Xi X1 Xi1 deﬁned n i1 Q F x1 xi globally consistent wrt C Let xi1 x1 xi1 deﬁne set B xi1 cid9 x cid9 globally consistent wrt C Then α Dix1 xi1 x expressed α cid2 1 1 cid9 Q ix cid9 x B xi1 x1 xi1 We borrow term backtrackfree constraint satisfaction literature 627 An order o said backtrackfree wrt set constraints C guarantees inconsistent partial assignment generated o sample generated rejected By deﬁnition proposal distribution Q Q 1 Q n backtrackfree o wrt ﬂat constraint network Deﬁnition 8 The proposal distribution presented Deﬁnition 13 takes proposal distribution backtrackfree relative modiﬁes components yield distribution backtrackfree relative PM Given mixed network M cid5X D F Ccid6 proposal distribution Q Q 1 Q n o Q wrt C Algorithm 1 assumes generate samples backtrackfree distribution Q F Q F oracle takes partial assignment x1 xi constraint satisfaction problem cid5X D Ccid6 input answers yes assignment globally consistent Given partial assignment x1 xi1 algorithm Xix1 xi1 initialized Q Xix1 xi1 constructs Q F Then assignment x1 xi1 xi extending Xi xi checks x1 xi1 xi globally consistent xix1 xi1 generates relative C oracle If sets Q F sample Repeating process order X1 Xn yields single sample Q F Note sample oracle invoked maximum O n d times n number variables d maximum domain size Xix1 xi1 samples value Xi follows Q F xix1 xi1 zero normalizes Q F 1 Q F n Given samples x1 xN generated Q F estimate Z deﬁned Eq 2 replacing Q Q F Eq 11 We cid8Z N 1 N Ncid4 k1 cid5 m i1 F ixk cid5 p j1 C jxk Q F xk 1 N Ncid4 k1 cid15 cid14 xk w F w F x cid5 m i1 F ix cid5 p j1 C jx Q F x backtrackfree weight sample 20 21 702 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 cid5 n i1 Q Xi X1 Xi1 ordering o X1 Xn Algorithm 2 SampleSearch Input A mixed network M cid5X D F Ccid6 proposal distribution Q X Output A consistent sample x x1 xn D copy domains Q SET 1 D 1 cid2 cid2 n cid9 1 X1 Q 1 X1 copy distribution x cid9 Forward phase cid9 D Sample Xi xi Q x1 xi violates constraint C cid9 remove D cid9 cid9 Xi xi x1 xi1 0 normalize Q cid9 SET Q Goto step 3 x x xi 1 D cid9 D Q cid9 Xi x1 xi1 Q Xi x1 xi1 Backward phase x xxi1 cid9 i1 Xi1 xi1x1 xi2 0 normalize Q SET Q SET 1 cid9 i1 Xi1x1 xi2 0 return inconsistent return x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Similarly estimate posterior marginals replacing weight wx Eq 19 backtrackfree weight w F x cid18P N xi cid2 N k1 w F xkδxi xk cid2 N k1 w F xk 22 Clearly cid8Z N deﬁned Eq 20 unbiased estimate Z cid18P N xi deﬁned Eq 22 asymptotically unbiased estimate posterior marginals P xi In practice use constraint solver substitute oracle Algorithm 1 However generating samples exact solver ineﬃcient cases Next present SampleSearch scheme integrates backtracking search sampling In essence integrate naturally sampling speciﬁc oracle based systematic backtracking search hopefully generating eﬃcient scheme 4 The SampleSearch scheme In nutshell SampleSearch incorporates systematic backtracking search ordered Monte Carlo sampler samples solutions constraint portion mixed network insist backtrackfreeness search process We sketch ideas basic form systematic search chronological backtracking emphasizing scheme work advanced systematic search scheme Indeed empirical work use advanced search scheme based minisat solver 15 Given mixed network M cid5X D F Ccid6 proposal distribution Q X traditional ordered Monte Carlo sam pler samples variables order o X1 Xn Q rejects partial sample x1 xi violates constraints C Upon rejecting sample sampler starts sampling anew ﬁrst variable X1 ordering Instead deadend x1 xi1 xi SampleSearch modiﬁes conditional probability Q Xi xix1 xi1 0 reﬂect x1 xi consistent normalizes distribution Q Xix1 xi1 resamples Xi normalized distribution The newly sampled value consistent case algorithm proceeds variable Xi1 inconsistent case algorithm modify Q Xix1 xi1 If repeat process reach point Q Xix1 xi1 0 values Xi In case x1 xi1 inconsistent algorithm revises distribution Xi1 setting Q i1 Xi1 xi1x1 xi2 0 normalizes Q i1 resamples new value Xi1 SampleSearch repeats process consistent sample satisﬁes constraints C generated By construction process yields consistent sample The pseudocode SampleSearch given Algorithm 2 It viewed depth ﬁrst backtracking search DFS state space consistent partial assignments searching solution constraint satisfaction problem cid5X D Ccid6 value ordering stochastically guided Q The updated distribution guides search Q In ward phase variables sampled sequence current partial sample assignment extended sampling cid9 xix1 xi1 0 value xi variable Xi current distribution Q cid9 SampleSearch backtracks previous variable Xi1 backward phase updates distribution Q i1 setting cid9 i1 continues Q cid9 i1xi1x1 xi2 0 normalizing Q cid9 If values xi Di Q cid9 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 703 Fig 1 A OR search tree given set constraints proposal distribution 41 The sampling distribution SampleSearch cid5 n Let I i1 I Xi X1 Xi1 sampling distribution SampleSearch ordering o X1 Xn We Theorem 1 Main result Given mixed network M cid5X D F Ccid6 proposal distribution Q sampling distribution I SampleSearch equals backtrackfree probability distribution Q F Q wrt C Q F I To prove theorem need following proposition Proposition 2 Given mixed network M cid5X D F Ccid6 proposal distribution Q Q 1 Q n partial assignment x1 xi1 globally consistent wrt C SampleSearch samples values replacement domain Di Xi globally consistent extension x1 xi1 xi generated cid9 cid9 Xix1 xi1 Proof Consider globally inconsistent extension x1 xi1 x x1 xi1 Let Q cid9 sampled Sample x1 x recently updated proposal distribution Because SampleSearch systematic Search eventually detect inconsistency able extend solution At point set cid9 cid9 x1 xi1 0 step 6 step 11 normalize Q Q sampled yielding sam In words x cid9 Xix1 xi1 On hand systematic nature globally pling replacement Q consistent extension x1 xi sampled SampleSearch extend sample consistent cid2 cid9 cid9 x We use Proposition 2 derive I ixix1 xi1 probability sampling globally consistent extension x1 xi1 xi globally consistent assignment x1 xi1 Q Xix1 xi1 illustrated ex ample Example 1 Example 1 Consider complete search tree corresponding proposal distribution constraints given Fig 1 The inconsistent partial assignments grounded ﬁgure Each arc labeled probability generating child node Q given assignment root node parent Consider assignment A 0 B 2 C 0 Based Proposition 2 ﬁve different ways assignment generated SampleSearch called DFStraces shown Fig 2 In following compute probability I B B 2 A 0 probability sampling B 2 given A 0 Given A 0 events lead sampling B 2 shown Fig 2 cid5B 2cid6 A 0 b cid5B 0 B 2cid6 A 0 c cid5B 3 B 0cid6 A 0 d cid5B 0 B 3 B 2cid6 A 0 e cid5B 3 B 0 B 2cid6 A 0 The notation cid5B 3 B 0 B 2cid6 A 0 means given A 0 states sampled order 704 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Fig 2 Five possible traces SampleSearch lead sample A 0 B 2 C 0 The children node speciﬁed left right order generated left right B 3 B 0 B 2 Clearly probability I B B 2 A 0 equals sum probability events Let compute probability event cid5B 3 B 0 B 2cid6 A 0 The probability sampling B 3 A 0 Q B A 0 03 04 02 01 01 The assignment A 0 B 3 inconsistent distribution cid9B A 0 0309 0409 0209 0 39 49 29 0 Subsequently Q B A 0 changed SampleSearch Q 39 However assignment A 0 B 0 globally inconsistent probability sampling B 0 Q cid9cid9B A 0 0 49 29 0 0 23 13 0 Next probability sampling distribution changed Q 13 Therefore probability event cid5B 3 B 0 B 2cid6 A 0 01 39 13 190 By B 2 Q calculating probabilities remaining events approach described taking sum verify probability sampling B 2 given A 0 I B B 2 A 0 13 cid9cid9 cid9 We Proposition 3 Given mixed network M cid5X D F Ccid6 initial proposal distribution Q Q 1 Q n partial assignment x1 xi1 xi globally consistent wrt C probability I ixix1 xi1 sampling xi given x1 xi1 SampleSearch proportional Q ixix1 xi1 I ixix1 xi1 Q ixix1 xi1 xi1 xi1 Proof The proof obtained deriving general expression I ixix1 xi1 summing probabilities events lead desired partial sample Consider globally consistent partial assignment xi1 x1 xi1 Let xi1 assume domain variable Xi given xi1 denoted D xi1 R x1 xi1 xi globally consistent B partitioned D xi1 xi1 xiq Let j 1 2q index sequence subsets B xi1 j denote sequence permutations B xi1 j πkB xi1 j xixi1 probability generating xi πkB xi1 B j denoting jth element xi1 j denoting kth element xi1 j given xi1 SampleSearch given xi1 obtained summing events generate Xi xi sequence Let π B sequence Finally let PrπkB The probability sampling xi R xi D xi1 xi1 xi1 xi1 xi1 xi1 D Let B R B R xi1 given xi1 I ixixi1 2qcid4 π B xi1 cid4 j j1 k1 cid14 cid14 πk Pr B xi1 j cid15 cid15 cid19 cid19xi1 xi PrπkB xi1 j xixi1 given cid15 cid15 cid14 cid14 cid14 cid14 Pr πk B xi1 j xi cid19 cid19xi1 Pr πk B xi1 j cid15cid19 cid19xi1 cid15 Pr cid14 xi cid14 cid19 cid19πk cid15 B xi1 j xi1 cid15 Substituting Eq 24 Eq 23 I ixixi1 2qcid4 π B xi1 cid4 j j1 k1 cid14 cid14 πk Pr B xi1 j cid15 cid15cid19 cid19xi1 cid14 Pr xi cid14 cid19 cid19πk B xi1 j cid15 xi1 cid15 23 24 25 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 705 PrxiπkB inconsistent Because sample replacement Proposition 2 Q probability given xi1 j xi1 probability value xi sampled given πkB xi1 j xi1 proved cid14 Pr xi cid14 cid19 cid19πk cid15 B xi1 j xi1 cid15 cid2 1 Q ixixi1 cid9 Q ix xi1 j B cid9 x xi1 From Eqs 25 26 I ixixi1 2qcid4 π B xi1 cid4 j j1 k1 cid2 1 Q ixixi1 cid9 Q ix xi1 j B cid9 x cid14 cid14 Pr πk B xi1 j cid15 cid15cid19 cid19xi1 xi1 Q ixixi1 depend indices j k Eq 27 rewrite Eq 27 cid20 I ixixi1 Q ixixi1 π B xi1 cid4 j 2qcid4 j1 k1 cid21 PrπkB cid2 cid9 x B xi1 j xi1 j xi1 cid9 Q ix xi1 1 26 27 28 The term enclosed brackets Eq 28 depend xi follows x1 xi1 xi globally consistent I ixixi1 Q ixixi1 cid2 29 We necessary components prove Theorem 1 Proof Theorem 1 From Proposition 2 I ixixi1 equals zero iff xi globally consistent Proposition 3 xi1 Consequently values I ixixi1 Q ixixi1 Therefore normalization constant equals 1 cid2 cid9 x B xi1 cid9 Q ix I ixixi1 cid2 1 Q ixixi1 cid9 Q ix xi1 B cid9 x xi1 30 The righthand Eq 30 deﬁnition equal Q F xixi1 Deﬁnition 13 cid2 42 Computing Q F x xixi1 From Deﬁnition 13 compute components Q F Once sample need compute weights estimating marginals weighted counts xixi1 turn requires computing Q F cid9 sample x x1 xn determine values x Di extended solution One way accomplish described Algorithm 1 use oracle The oracle invoked maximum n d 1 times n number variables d maximum domain size Methods adaptive consistency 6 exact CSP solver oracles But gained SampleSearch ultimately need use oracle number times sampling method presented Algorithm 1 Next approximate backtrackfree probabilities ﬂy maintaining desirable guarantees 421 Approximating Q F x During process generating sample x SampleSearch discovered values set B build approximation Q F Xi proved inconsistent given xi1 generating sample x We use set A approximation T F xixi1 follows Let A set values domain xi1 compute B xixi1 Q F xixi1 follows xi1 xi1 xi1 xixi1 T F cid2 1 xi1 31 Finally compute T F x Q F x computing weight w F x Eq 21 xixi1 However T F x guarantee asymptotic unbiasedness replacing cid9 To remedy situation store sample x1 xn partial assignments x1 xi1 x proved inconsistent trace independent execution SampleSearch called DFStraces example Fig 2 shows ﬁve DFStraces generate sample A 0 B 2 C 0 After executing SampleSearch N times generating N samples use stored DFStraces compute approximation Q F x illustrated following example Q ixixi1 cid9 Q ix xi1 cid9 A x cid5 n i1 T F 706 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Fig 3 Three DFStraces b Combined information DFStraces given c Two possible approximations IB A 1 Example 2 Consider traces given Fig 3a We combine information traces shown Fig 3b Consider assignment A 1 B 2 The backtrackfree probability generating B 2 given A 1 requires knowledge values B inconsistent Based combined traces know B 0 B 1 inconsistent given A 1 know B 3 consistent explored indicated Fig 3b Setting unexplored nodes inconsistent consistent gives different approximations shown Fig 3c Generalizing Example 2 consider bounding approximations denoted U F N respectively based setting unexplored node combined N traces consistent inconsistent respectively As approximations bound sample mean cid8Z N below3 N L F N Given mixed network M cid5X D F Ccid6 initial Deﬁnition 14 Upper lower approximations Q F U F proposal distribution Q Q 1 Q n combined sample tree generated N independent runs SampleSearch partial sample xi1 x1 xi1 generated N independent runs deﬁne sets N L F A xi1 xi1 Ni B SampleSearch xi1 D Ni xi1 C xi D xi1 xi D xi1 x1 xi1 xi proved inconsistent N independent runs x1 xi1 xi explored N independent runs SampleSearch We set nodes C xi1 Ni nodes explored consistent inconsistent yielding U F N x ncid3 i1 U F Nixixi1 U F Nixixi1 cid2 1 Q ixixi1 cid9 Q ix xi1 Ni A cid9 x xi1 32 3 Note easy envision approximations designate unexplored nodes consistent inconsistent based domain knowledge Monte Carlo estimate We consider extreme options usually work practice bound sample mean V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 N x L F ncid3 i1 Nixixi1 L F L F Nixixi1 1 cid2 cid9 x Q ixixi1 xi1 Ni xi1 Ni C A cid9 Q ix xi1 707 33 It clear N grows sample tree grows inconsistencies discovered N xi1 Ni B xi1 C xi1 Ni φ Clearly 1 xN n generated Sample Search inconsistencies discovered making respective sets approach A Proposition 4 limN U F N x limN L F N x Q F x As given set iid samples x1 x1 1 x1 estimate weighted counts Z statistics U F cid9Z U N 1 N Ncid4 k1 cid5 cid5 m i1 F ixk U F N xk p j1 C jxk 1 N Ncid4 k1 w U N xk n xN xN N x L F cid14 N x cid15 cid15 cid14 xk w U N cid5 cid5 p j1 C jxk m i1 F ixk U F N xk weight sample based combined sample tree upper approximation U F N cid9Z L N 1 N Ncid4 k1 cid5 cid5 p m i1 F ixk L F N xk j1 C jxk 1 N Ncid4 k1 cid15 cid14 xk w L N cid15 cid14 xk w L N cid5 cid5 p m i1 F ixk L F N xk j1 C jxk weight sample based combined sample tree lower approximation L F N cid2 Similarly marginals develop statistics N xkδxi xk N xk N k1 w U cid2 N k1 w U cid9P U N xi cid9P L N xi cid2 N k1 w L cid2 N k1 w L N xkδxi xk N xk 34 35 36 37 In following theorems state interesting properties cid9Z L Appendix A N cid9Z U N cid9P L N xi cid9P U N xi The proofs provided Theorem 2 cid9Z L N cid2 cid8Z N cid2 cid9Z U N Theorem 3 The estimates cid9Z U N xi cid9P L cid9P U N cid9Z L N xi P xi given Eqs 36 37 respectively asymptotically unbiased N Z given Eqs 34 35 respectively asymptotically unbiased Similarly estimates Theorem 4 Given N samples output SampleSearch mixed network M cid5X D F Ccid6 space time complexity computing cid9Z L N xi given Eqs 35 34 37 36 O N d n N xi cid9P U N cid9Z U N cid9P L In summary presented approximations backtrackfree probability Q F bound sample mean cid8Z N We proved approximations yield asymptotically unbiased estimate weighted counts marginals They enable trading bias variance discuss 422 Biasvariance tradeoff As pointed Section 2 mean squared error estimator reduced controling bias increasing number samples The estimators cid9Z U N bias zero requires invoking exact CSP solver O n d times However given ﬁxed timebound expect estimators cid9Z U N cid9Z L N absolute distance cid9Z U small expect cid9Z U N bias unbiased estimator cid8Z F N cid9Z L estimate bias If cid9Z U N N based larger sample size cid9Z L N perform better cid8Z F N allow larger sample size cid8Z F N bound cid8Z F cid9Z L N Moreover cid9Z U N cid9Z L N cid9Z L N N N 708 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 43 Incorporating advanced search techniques SampleSearch Theorem 1 applicable search procedure systematic search procedure encounters assign ment x1 xi prove assignment inconsistent return consistent sample extending x1 xi Therefore use advanced systematic search technique 6 instead naive backtracking easily Proposition 5 Given mixed network M cid5X D F Ccid6 initial proposal distribution Q Q 1 Q n SampleSearch aug mented systematic advanced search technique generates independent identically distributed samples backtrack free probability distribution Q F Q wrt C While advanced search techniques change sampling distribution SampleSearch practice signiﬁcant impact time complexity quality upper lower approximations In particular SAT solvers developed decade eﬃcient represent constraints mixed network CNF formula4 use minisat 15 SAT solver However minisat stateoftheart SAT solver RSAT 29 systematic following changes changes implemented minimal effort Turn random restarts far backtracks The use restarts far backtracks makes SAT solver nonsystematic Change variable value ordering We change variable ordering respect structure input proposal i1 Q Xi X1 Xi1 order variables o X1 Xn Also distribution Q given Q X decision point variable Xi assigned value xi sampling Q Xix1 xi1 cid5 n 5 Empirical evaluation We conducted empirical evaluation tasks counting models SAT formula b computing probability evidence partition function Bayesian Markov networks respectively c computing posterior marginals Bayesian Markov networks The results organized follows In subsection present implementation details SampleSearch Section 52 describes techniques compared In Section 53 results weighted counting task Section 54 focus posterior marginals task 51 SampleSearch Iterative Join Graph Propagation wcutset sampling IJGPwcSS In experiments SampleSearch operates advanced importance sampling algorithm IJGP wcIS presented 10 We resulting scheme IJGPwcSS IJGPwcIS uses generalized belief propagation scheme Iterative Join Graph Propagation IJGP construct proposal distribution wcutset sampling framework 14 reduce variance Below outline details IJGPwcIS followed IJGPwcSS The proposal distribution The performance importance sampling highly dependent close proposal distribution posterior distribution 830 In principle use prior distribution proposal distribution Likelihood weighting 3132 However evidence unlikely prior bad approximation posterior 530 In case variance sample weights large samples large weights dominate mean yielding ineﬃcient sampling scheme Several schemes proposed address problem brieﬂy review different com plementary approaches In ﬁrst approach referred adaptive importance sampling 3033 sampling algorithm periodically updates proposal distribution generated samples As samples drawn hope updated proposal distribution closer closer posterior distribution yielding low variance sampling scheme In second approach idea use stateoftheart approximation algorithm Belief propagation 5 construct proposal distribution 342310 In IJGPwcIS use approach In particular IJGPwcIS uses Q Q 1 Q n obtained output Iterative Join Graph Propagation IJGP 12 13 shown yield good performance earlier studies 2310 IJGP generalized belief propagation 11 technique approximating posterior distribution graphical models It uses message passing scheme join tree propagation 35 applies clusters join graph join tree iteratively A join graph decomposition functions mixed network graph clusters satisﬁes properties required valid join tree decomposition tree requirement The time space complexity IJGP controlled ibound parameter bounds cluster size IJGP exponential ibound accuracy 4 It easy convert relational constraint network CNF formula In implementation use direct encoding described 28 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 709 Algorithm 3 Implementation details IJGPwcSS SampleSearch IJGP based proposal wcutset sampling Input A mixed network M cid5X D F Ccid6 integers N w Output A set N samples globally consistent wrt C Create minﬁll ordering o X1 Xn Create join graph JG ibound o join graph structuring algorithm given 12 run IJGP JG Create wcutset K X greedy scheme described 1438 Let K K1 Kt Create proposal distribution Q K First select cluster A JG mentions K largest number variables common previous variables K1 K i1 Then construct Q K K1 K i1 marginalizing variables mentioned K1 K marginal variables A 1 N i1 Q K K1 K i1 messages functions JG following heuristic scheme 10 cid5 t Apply minisat based SampleSearch M proposal distribution Q K sample ki Store DFStrace sample ki combined sample tree Output required statistics marginals weighted counts based combined sample tree 1 2 3 4 5 6 7 8 generally increases ibound In experiments instance select maximum ibound accommodated 512 MB space follows The space required message function product domain sizes variables scope Given ibound create join graph cluster size bounded described 12 compute advance space required IJGP summing space required individual messages5 We iterate 1 space bound 512 MB surpassed This ensures IJGP terminates reasonable time requires bounded space wcutset sampling As mentioned Section 23 mean squared error importance sampling reduced reducing variance weights To reduce variance weights combine importance sampling wcutset sampling 14 The idea partition variables X sets K R treewidth mixed network restricted R bounded constant w The set K called wcutset Because eﬃciently compute marginals weighted counts mixed network restricted R conditioned K k exact inference techniques bucket elimination 19 need sample variables K From Rao Blackwell theorem 2636 easy sampling subspace K reduces variance Formally given mixed network M cid5X D F Ccid6 wcutset K sample k generated proposal distribution Q K wcutset sampling weight k given cid2 cid5 w wck rR m j1 F jr K k Q k cid5 p a1 Car K k 38 R X K Given wcutset K compute sum numerator Eq 38 polynomial time exponential constant w bucket elimination 19 It demonstrated higher wbound 14 lower sampling variance Here select maximum w resulting bucket elimination algorithm uses 512 MB space We choose appropriate w similar iterative scheme described choosing ibound Variable ordering heuristics We experimented different variable ordering heuristics constructing join graph IJGP minﬁll ordering mindegree ordering hmetis ordering6 We performed sampling reverse order join graph constructed Intuitively makes sense IJGP akin variable elimination sampling akin search known best ordering elimination reverse ordering search vice versa In case Bayesian networks experimented topological ordering sampling We observed minﬁll ordering gives best performance brevity compare performance minﬁll based IJGPwcIS IJGPwcSS solvers We evaluate ordering heuristics Section 55 The details IJGPwcSS given Algorithm 3 The algorithm takes input mixed network integer w N specify ibound IJGP w creating wcutset number samples N respectively7 In steps 12 algorithm creates join graph minﬁll ordering runs IJGP Then step 3 computes wcutset K i1 Q iK iK1 K i1 mixed network Then algorithm creates proposal distribution wcutset K Q K output IJGP heuristic scheme outlined step 4 Finally steps 58 algorithm executes minisat based SampleSearch mixed network generate required N samples outputs required statistics cid5 t Henceforth refer estimates IJGPwcSS generated upper lower approximations backtrackfree probability given Eqs 34 35 IJGPwcSSUB IJGPwcSSLB respectively Note IJGPwc 5 Note constructing messages explicitly 6 This ordering heuristic 37 based hypergraph partitioning To create partitioning use hmetis software available httpwww userscsumnedukarypismetishmetis 7 This determine ibound w wcutset 710 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 SSUB IJGPwcSSLB bound sample mean cid8Z N respectively true mean exact weighted counts Z 52 Alternative schemes In addition IJGPwcSS IJGPwcIS experimented following schemes 1 Iterative Join Graph Propagation IJGP In experiments anytime version IJGP 1213 start ibound 1 run IJGP convergence 10 iterations whichever earlier Then increase ibound reconstruct join graph We following conditions met equals treewidth case IJGP yields exact marginals b 2 GB space limit reached c prescribed timebound reached 2 ApproxCount SampleCount Wei Selman 39 introduced approximate solution counting scheme called Ap proxCount ApproxCount based formal result 40 sample uniformly close set solutions SAT formula F exactly count approximate good estimate number solutions F Consider SAT formula F S solutions If able sample solutions uniformly exactly com pute fraction number solutions denoted γ variable X set True 1 similarly False 0 If γ greater zero set X particular value simplify F F The estimate number solutions equal product 1 γ number solutions F Then recursively repeat process leading series multipliers variables assigned value conditioned formula easy exact model counters like Cachet 41 To reduce variance Wei Selman 39 suggest set selected variable value occurs given set sampled solutions In scheme fraction variable branching selected solution sampling method called SampleSat 16 extension wellknown local search SAT solver Walksat 42 We experimented anytime version ApproxCount report cumulative average accumulated runs cid9 cid9 SampleCount 17 differs ApproxCount following ways SampleCount heuristically reduces vari ance branching variables balanced variables having multipliers 1γ close 2 b branch point SampleCount assigns value variable sampling probability 05 yielding unbiased estimate solution counts We experimented anytime version SampleCount report unbiased cumulative averages runs8 In experiments implementation ApproxCount SampleCount available respective authors 1617 Following recommendations 17 use following parameters ApproxCount SampleCount Number samples SampleSat 20 b number variables remaining assigned value running Cachet 100 c local search cutoff α 100K 3 Evidence Prepropagated Importance Sampling EPIS importance sampling algorithm computing marginals Bayesian networks 23 The algorithm uses loopy belief propagation 543 construct proposal distribution In experiments anytime implementation EPIS submitted UAI 2008 evaluation 44 4 Edge Deletion Belief Propagation EDBP 20 approximation algorithm computing posterior marginals computing probability evidence EDBP solves exactly simpliﬁed version original problem obtained deleting edges primal graph Deleted edges selected based criteria quality approximation complexity computation treewidth reduction parameterized integer k called kbound Subsequently information loss lost dependencies compensated heuristic techniques The implementation scheme available 20 5 Variable Elimination Conditioning VEC When problem having high treewidth encountered variable bucket elimination unsuitable primarily extensive memory demand To alleviate space complexity use wcutset conditioning scheme 19 Namely condition instantiate variables wcutset remaining problem removing instantiated variables solved exactly bucket elimination 19 In experiments select wcutset way bucket elimination require 15 GB space Again ensure bucket elimination terminates reasonable time uses bounded space Exact weighted counts computed summing exact solution output bucket elimination possible instantiations wcutset When VEC terminated completion outputs partial sum yielding lower bound weighted counts As preprocessing algorithm performs SATbased variable domain pruning yields signiﬁcant performance gains practice Here ﬁrst convert zero probabilities constraints problem CNF formula F Then cid9 variablevalue pair construct new CNF formula F adding unit clause corresponding pair F cid9 check minisat 15 F inconsistent delete value domain variable The implementation scheme available publicly software website 45 consistent If F cid9 8 In original paper SampleCount 17 investigated lower bounding solution counts Here evaluate unbiased solution counts computed algorithm V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 711 Fig 4 Chart showing scope experimental study Table 1 Query types handled solvers Problem type Bayesian networks P e Markov networks Z Bayesian networks Mar Markov networks Mar Model counting IJGPwcSS IJGPwcIS IJGP EDBP EPISBN VEC SampleCount ApproxCount Relsat Z partition function P e probability evidence Mar posterior marginals 6 Relsat9 18 exact algorithm counting solutions satisﬁability problem When Relsat stopped com pletion yields lower bound number solutions 7 ACE10 package exact inference Bayesian Markov networks currently stateoftheart It ﬁrst compiles Bayesian Markov network Arithmetic Circuit AC 46 uses AC answer queries network ACE uses c2d compiler 47 compile network dDNNF 48 extracts AC dDNNF Note unlike exact schemes described ACE anytime scheme We report time required ACE solve instance use times baseline comparison The benchmarks solvers different task types shown Fig 4 Table 1 summarizes different query indicates algorithm able approximately estimate types handled solvers A query lack indicates 53 Results weighted counts Notation tables The ﬁrst column table Table 2 example gives instance The second column provides statistical information instance number variables n average domain size k number clauses constraints c number evidence variables e treewidth instance w computed minﬁll heuristic incorporating evidence removing irrelevant variables The fourth column provides exact answer problem instance available remaining columns display results solvers terminated speciﬁed timebound The solvers giving best results highlighted output solver indicates solved problem instance exactly timebound row A expired followed number seconds took solve instance enclosed brackets An X indicates solution output solver 9 Available httpwwwbayardoorgresourceshtml 10 Available httpreasoningcsuclaeduace 712 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Table 2 Table showing solution counts Z number consistent samples M sampling based solvers output IJGPwcSS IJGPwcIS ApproxCount SampleCount Relsat 10 hours CPU time 4 Latin Square instances exact solution counts known Problem cid5n k c wcid6 ls8norm cid5512 2 5584 255cid6 ls9norm cid5729 2 9009 363cid6 ls10norm cid51000 2 13820 676cid6 ls11norm cid51331 2 20350 956cid6 Exact 540E11 380E17 760E24 540E33 Sample Count 515E11 16514 449E17 7762 728E24 3854 208E34 2002 Approx Count 352E11 17740 126E17 8475 117E24 4313 491E31 2289 Z M Z M Z M Z M Relsat 244E08 178E08 136E08 109E08 IJGPwc SSLB 591E11 236510 344E17 138572 674E24 95567 387E33 66795 IJGPwc SSUB 591E11 236510 344E17 138572 674E24 95567 387E33 66795 IJGP wcIS X 0 X 0 X 0 X 0 Fig 5 Time versus solution counts sample Latin square instances IJGPwcIS plotted ﬁgures fails instances 531 Satisﬁability instances For task counting solutions models satisﬁability formula evaluate algorithms formulas domains normalized Latin square problems b Langford problems c FPGArouting instances We ran algo rithm 10 hours instance Results instances exact solution counts known Our ﬁrst set benchmark instances come normalized Latin squares domain A Latin square order s s s table ﬁlled s numbers 1 s way number occurs exactly row column In normalized Latin square ﬁrst row column ﬁxed The task count number normalized Latin squares given order The Latin squares modeled SAT formulas extended encoding given 49 The exact counts formulas known order 11 50 Table 2 shows results Latin square instances order 11 exact solution counts known Approx Count Relsat underestimate counts orders magnitude On hand IJGPwcSSUB IJGPwcSSLB SampleCount yield good estimates close true counts The counts output IJGPwcSSUB IJGPwcSSLB instances indicating sample mean accurately estimated upper lower approxima tions backtrackfree distribution discussion bias versus variance Section 422 IJGPwcIS fails instances unable generate single consistent sample hours IJGPwcSS generates far solution samples SampleCount ApproxCount In Fig 5a b estimates output solvers change time largest instances Here clearly superior convergence IJGPwcSSLB IJGPwcSSUB SampleCount approaches Our second set benchmark instances come Langfords problem domain The problem parameterized integer size denoted s Given set s numbers 1 2 s problem produce sequence length 2s 1 2 s appears twice sequence occurrences exactly apart This problem satisﬁable n 0 3 modulo 4 We encoded Langford problem SAT formula channeling SAT encoding described 51 Table 3 presents results ApproxCount Relsat severely estimate true counts instance size 12 lang12 Table 3 Relsat solves exactly 5 minutes SampleCount inferior IJGPwcSSUB IJGPwcSSLB orders magnitude IJGPwcSSUB slightly better IJGPwcSSLB Unlike Latin square instances solution counts output IJGPwcSSLB IJGPwcSSUB different large problems difference V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 713 Table 3 Table showing solution counts Z number consistent samples M sampling based solvers output IJGPwcSS IJGPwcIS output solver indicates ApproxCount SampleCount Relsat 10 hours CPU time Langfords problem instances A solved problem exactly timebound 10 hours expired followed number seconds took solve instance exactly Problem cid5n k c wcid6 lang12 cid5576 2 13584 383cid6 lang16 cid51024 2 32320 639cid6 lang19 cid51444 2 54226 927cid6 lang20 cid51600 2 63280 1023cid6 lang23 cid52116 2 96370 1407cid6 lang24 cid52304 2 109536 1535cid6 Exact 216E5 653E08 513E11 527E12 760E15 937E16 Sample Count 193E05 2720 597E08 328 973E10 146 113E11 120 753E14 38 117E13 25 Approx Count 295E04 4668 822E06 641 687E08 232 399E09 180 370E12 54 415E11 33 Z M Z M Z M Z M Z M Z M Relsat 216E05 297 s 628E06 852E05 855E04 X X IJGPwc SSLB 216E05 999991 651E08 14971 638E11 3431 283E12 2961 417E15 1111 874E15 271 IJGPwc SSUB 216E05 999991 699E08 14971 731E11 3431 345E12 2961 419E15 1111 140E16 271 IJGP wcIS X 0 X 0 X 0 X 0 X 0 X 0 Fig 6 Time versus solution counts sample Langford instances IJGPwcIS Relsat plotted ﬁgures fail given instances small IJGPwcIS fails instances perform search Again IJGPwcSS generates far consistent samples compared SampleCount ApproxCount In Fig 6a b estimates vary time largest instances We clearly superior anytime performance IJGPwcSSLB IJGP wcSSUB11 Results instances exact solution counts known When exact results available evaluating ca pability SampleSearch approximation algorithm problematic quality approximation close approximation exact assessed To allow comparison hard stances evaluate power sampling schemes generating good lowerbound approximations quality compared higher better Speciﬁcally compare lower bounds obtained combining IJGPwcSSLB IJGPwcIS SampleCount Markov inequality based martingale average schemes described previous work 52 These lower bounding schemes 1752 input set unbiased sample weights lower bound unbiased sample weights b real number 0 α 1 output lower bound weighted counts Z solution counts case SAT formula correct probability greater α In experiments set α 099 means lower bounds correct probability greater 099 11 An anonymous reviewer pointed number solutions Langford problem estimated specialized sampling scheme heshe provided python implementation This scheme suffers rejection problem rejection rate small The scheme yields sample means closer true mean sample means output SampleSearch SampleCount ApproxCount 714 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Table 4 Table showing lower bounds solution counts Z number consistent samples M samplingbased solvers output IJGPwc SSLB IJGPwcIS SampleCount Relsat 10 hours CPU time 5 Latin Square instances exact solution counts known The entries IJGPwcIS SampleCount IJGPwcSSLB contain lower bounds computed combining respective sample weights Markov inequality based Average Martingale schemes given 52 Problem cid5n k c wcid6 Exact ls12norm cid51728 2 28968 1044cid6 ls13norm cid52197 2 40079 1558cid6 ls14norm cid52744 2 54124 1971cid6 ls15norm cid53375 2 71580 2523cid6 ls16norm cid54096 2 92960 2758cid6 Z M Z M Z M Z M Z M Sample Count 223E43 1064 320E54 566 508E65 299 312E79 144 768E95 58 Relsat 126E08 932E07 71E07 206E07 X IJGPwc SSLB 125E43 13275 115E55 6723 124E70 3464 203E83 1935 208E98 1530 IJGP wcIS X 0 X 0 X 0 X 0 X 0 We samples derived SampleSearch IJGPwcSSLB rise superior lower bounds compared samplingbased schemes Comparing lowerbounds facilitates comparative evaluation instances exact weighted counts available12 IJGPwcSSUB lower bound Z outputs upper bounds unbiased sample weights Likewise ApproxCount lower bound Z unbiased Finally note Relsat yields lower bound solution counts probability First compare lower bounding ability IJGPwcIS IJGPwcSSLB SampleCount Relsat Latin square stances size 12 15 exact counts known Table 4 contains results IJGPwcSSLB yields far better higher lower bounds SampleCount problem size increases Relsat underestimates counts sev eral orders magnitude compared IJGPwcSSLB SampleCount As expected IJGPwcIS fails instances Again lower bounds obtained IJGPwcSSLB based larger sample size compared SampleCount Our ﬁnal domain FPGA routing instances These instances constructed reducing FPGA Field Pro grammable Gate Array detailed routing problems satisﬁability formula The instances generated GiJoon Nam SAT 2002 competition 53 Table 5 presents results instances IJGPwcSSLB yields higher lower bounds SampleCount Relsat ﬁfteen instances On remaining ﬁve instances Sam pleCount yields higher lower bounds IJGPwcSSLB Relsat inferior IJGPwcSSLB IJGPwcIS fails instances SampleCount fails yield single consistent sample 6 15 instances On remaining instances number consistent samples generated SampleCount far smaller IJGPwcSS Next present results Bayesian Markov networks benchmarks For rest paper note slight change content table In second column report time required ACE compute weighted counts marginals instance The timebound ACE set 3 hrs 532 Linkage networks The Linkage networks generated converting biological linkage analysis data Bayesian Markov network Linkage analysis statistical method mapping genes chromosome 54 This useful practice identifying disease genes The input ordered list loci L1 Lk1 allele frequencies locus pedi gree individuals typed loci The goal linkage analysis evaluate likelihood candidate vector θ1 θk recombination fractions input pedigree locus order The component θi candidate recombination fraction loci Li Li1 The pedigree data represented Bayesian network types random variables genetic loci variables represent genotypes individuals pedigree genetic loci variables individual locus paternal allele maternal allele phenotype variables selector variables auxiliary variables represent gene ﬂow pedigree Fig 7 represents fragment network describes parents child interactions simple 2loci analysis The genetic loci variables individual locus j denoted Li jp Li jm Variables Xi j S jp S jm denote phenotype variable paternal selector variable maternal selector variable individual locus j respectively The conditional probability tables correspond selector variables 12 We evaluate quality marginals exact solution known Markov inequality based schemes 1752 lower bound marginal probabilities V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 715 Table 5 Table showing lower bounds solution counts Z number consistent samples M samplingbased solvers output IJGPwc SSLB IJGPwcIS SampleCount Relsat 10 hours CPU time FPGA routing instances The entries IJGPwcIS SampleCount IJGPwcSSLB contain lower bounds computed combining respective sample weights Markov inequality based Average Martingale schemes given 52 Problem 9symml_gr_2pin_w6 cid5n k c wcid6 cid52604 2 36994 413cid6 9symml_gr_rcs_w6 cid51554 2 29119 613cid6 alu2_gr_rcs_w8 cid54080 2 83902 1470cid6 apex7_gr_2pin_w5 cid51983 2 15358 188cid6 apex7_gr_rcs_w5 cid51500 2 11695 290cid6 c499_gr_2pin_w6 cid52070 2 22470 263cid6 c499_gr_rcs_w6 cid51872 2 18870 462cid6 c880_gr_rcs_w7 cid54592 2 61745 1024cid6 example2_gr_2pin_w6 cid53603 2 41023 350cid6 example2_gr_rcs_w6 cid52664 2 27684 476cid6 term1_gr_2pin_w4 cid5746 2 3964 31cid6 term1_gr_rcs_w4 cid5808 2 3290 57cid6 too_large_gr_rcs_w7 cid53633 2 50373 1069cid6 too_large_gr_rcs_w8 cid54152 2 57495 1330cid6 vda_gr_rcs_w9 cid56498 2 130997 2402cid6 Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Exact SampleCount 336E51 3 849E84 374 121E206 8 583E93 54 217E139 1028 X 0 241E87 40 150E278 5 393E160 1 417E265 167 X 0 X 0 X 0 X 0 X 0 Relsat 341E32 336E72 188E56 483E49 369E46 278E47 761E54 142E43 735E38 113E73 213E35 117E49 146E73 102E64 223E92 IJGPwcSSLB 306E53 6241 280E82 16911 169E235 841 233E94 25161 964E133 48331 218E55 4491 129E84 14151 716E255 831 733E160 1971 685E250 6211 690E39 326771 744E55 341951 105E182 1561 566E246 1171 508E300 221 IJGPwcIS X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 Fig 7 A fragment Bayesian network genetic linkage analysis 716 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Table 6 Probability evidence Z computed VEC EDBP IJGPwcIS IJGPwcSS 3 hours CPU time Linkage instances UAI 2006 evaluation For IJGPwcSS IJGPwcIS report number consistent samples M generated 3 hours Problem BN_69 BN_70 BN_71 BN_72 BN_73 BN_74 BN_75 BN_76 cid5n k c e wcid6 ACE time cid5777 7 228 78 39cid6 ACE timeout cid52315 5 484 159 35cid6 ACE 233 s cid51740 6 663 202 53cid6 ACE timeout cid52155 6 752 252 65cid6 ACE timeout cid52140 5 651 216 67cid6 ACE timeout cid5749 6 223 66 35cid6 ACE timeout cid51820 5 477 155 37cid6 ACE timeout cid52155 7 605 169 53cid6 ACE timeout Z M Z M Z M Z M Z M Z M Z M Z M Exact 528E054 200E71 512E111 421E150 226E113 375E45 588E91 493E110 IJGPwc SSLB 300E55 684E5 121E73 192E5 128E111 746E4 473E150 153E5 200E115 775E4 213E46 280E5 219E91 772E4 195E111 252E4 IJGPwc SSUB 300E55 684E5 121E73 192E5 128E111 746E4 473E150 153E5 200E115 775E4 213E46 280E5 219E91 772E4 195E111 252E4 VEC EDBP 193E61 239E57 799E82 600E79 705E115 101E114 132E153 921E155 600E127 224E118 330E48 584E48 583E97 310E96 100E126 386E114 IJGPwc IS X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 Fig 8 Convergence probability evidence function time sample Linkage instances IJGPwcIS plotted ﬁgures fails instances parameterized recombination ratio θ The remaining tables contain deterministic information It shown given pedigree data computing likelihood recombination fractions equivalent computing probability evidence Bayesian network model problem details consult 21 We ﬁrst evaluate solvers Linkage Bayesian networks UAI 2006 evaluation 55 Table 6 contains results The exact results instances available UAI 2006 evaluation website We IJGPwcSSUB IJGPwcSSLB accurate usually yielding orders magnitude improvement VEC EDBP Because estimates output IJGPwcSSUB IJGPwcSSLB instances yield exact value sample mean Fig 8 shows probability evidence changes function time sample instances We superior anytime performance IJGPwcSS schemes compared VEC EDBP IJGPwcIS fails output single consistent sample 3 hours CPU time instances In Table 7 present results 18 linkage instances UAI 2008 evaluation 44 exact value probability evidence known13 We VEC anytime scheme exactly solves 10 instances indicated Table 7 On 7 remaining 8 instances IJGPwcSSLB IJGPwcSSUB better VEC EDBP exactly solves 5 instances On remaining 13 instances IJGPwcSSLB IJGPwcSSUB better 13 The exact value probability evidence instances ACE VEC unable solve obtained running Bucket elimination BE external memory BEEM algorithm 56 BEEM uses external memory disk storage increase memory available BE signiﬁcantly improving BEs scalability V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 717 Table 7 Probability evidence Z computed VEC EDBP IJGPwcIS IJGPwcSS 3 hours CPU time Linkage instances UAI 2008 evaluation output For IJGPwcSS IJGPwcIS cell table reports number consistent samples M generated 3 hours A solver indicates solved problem exactly timebound expired followed number seconds took solve instance exactly Problem pedigree18 pedigree1 pedigree20 pedigree23 pedigree25 pedigree30 pedigree37 pedigree38 pedigree39 pedigree42 pedigree31 pedigree34 pedigree13 pedigree9 pedigree19 pedigree7 pedigree51 pedigree44 cid5n k c e wcid6 ACE time cid51184 2 386 0 26cid6 ACE 10 s cid5334 2 121 0 20cid6 ACE 2 s cid5437 2 147 0 25cid6 ACE timeout cid5402 2 130 0 26cid6 ACE 8 s cid51289 2 396 0 38cid6 ACE timeout cid51289 2 413 0 27cid6 ACE 8 s cid51032 2 333 0 25cid6 ACE 52 s cid5724 2 263 0 18cid6 ACE 340 s cid51272 2 354 0 29cid6 ACE timeout cid5448 2 156 0 23cid6 ACE timeout cid51183 2 0 45cid6 ACE timeout cid51160 1 0 59cid6 ACE timeout cid51077 1 0 51cid6 ACE timeout cid51118 2 0 41cid6 ACE timeout cid5793 2 0 23cid6 ACE timeout cid51068 1 0 56cid6 ACE timeout cid51152 1 0 51cid6 ACE timeout cid5811 1 0 29cid6 ACE timeout Exact 718E79 781E15 234E30 278E39 169E116 184E84 263E117 564E55 632E103 173E31 198E70 59E65 544E32 343E79 94E60 149E65 134E74 336E64 IJGPwc SSLB 739E79 130E5 781E15 326E5 231E30 231E5 276E39 328E5 169E116 129E5 190E84 114E5 118E117 426E5 380E55 163E5 629E103 125E5 173E31 326E5 208E70 67E4 384E65 12E5 703E32 81E4 293E79 80E4 676E60 95E4 133E65 83E4 247E74 10E5 339E64 17E5 IJGPwc SSUB 739E79 130E5 781E15 326E5 231E30 231E5 276E39 328E5 169E116 129E5 190E84 114E5 118E117 426E5 380E55 163E5 629E103 125E5 173E31 326E5 208E70 67E4 384E65 12E5 703E32 81E4 293E79 80E4 676E60 95E4 133E65 83E4 247E74 10E5 339E64 17E5 Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M VEC EDBP IJGPwc IS 718E79 64 s 718E79 772 s 781E15 12 s 781E15 14 s 234E30 1216 s 619E31 278E39 913 s 152E39 169E116 318 s 169E116 2562 s 185E84 808 s 185E84 179 s 263E117 2521 s 569E124 565E55 735 s 841E56 632E103 136 s 632E103 694 s 173E31 3188 s 891E32 167E76 258E76 217E37 800E82 797E60 166E72 556E85 223E64 134E70 430E65 653E34 313E79 335E60 293E66 616E76 769E66 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 VEC Overall IJGPwcSSLB IJGPwcSSUB deviate slightly exact value probability evidence Again IJGPwcIS fails instances 533 Relational instances The relational instances generated grounding relational Bayesian networks primula tool 22 We experimented Friends Smoker networks mastermind networks domain 262 76212 variables Table 8 summarizes results VEC solves 2 friends smokers networks exactly remaining instances fails output answer EDBP solves instance exactly remaining instances fails inferior IJGPwcSS IJGPwcIS better IJGPwcSS 3 instances remaining instances fails generate single consistent sample especially instances larger The estimates computed IJGPwcSSLB IJGPwcSSUB hand close exact probability evidence VEC solves exactly mastermind instances remaining instances VEC worse IJGPwcSSUB IJGPwcSSLB EDBP solves instances exactly remaining instances worse IJGPwcSSLB IJGPwcSSUB Again estimates output IJGPwcSSLB IJGPwcSSUB relational instances indicating lower upper approximations zero bias 718 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Table 8 Probability evidence computed VEC EDBP IJGPwcIS IJGPwcSS 3 hours CPU time relational instances For IJGPwcSS IJGPwcIS output solver indicates solved cell table reports number consistent samples generated 10 hours A problem exactly timebound expired followed number seconds took solve instance exactly cid5n k c e wcid6 ACE time Problem Friends smokers Exact IJGPwc SSLB IJGPwc SSUB VEC EDBP fs04 fs07 fs10 fs13 fs16 fs19 fs22 fs25 fs28 fs29 cid5262 2 74 226 12cid6 ACE 4 s cid51225 2 371 1120 35cid6 ACE 4 s cid53385 2 1055 3175 71cid6 ACE 9 s cid57228 2 2288 6877 117cid6 ACE 9 s cid513240 2 4232 12712 171cid6 ACE 14 s cid521907 2 7049 21166 243cid6 ACE 22 s cid533715 2 10901 32725 315cid6 ACE 49 s cid549150 2 15950 47875 431cid6 ACE 74 s cid568698 2 22358 67102 527cid6 ACE 148 s cid576212 2 24824 74501 559cid6 ACE 167 s Mastermind mm_03_08_03 mm_03_08_04 mm_03_08_05 mm_04_08_03 mm_04_08_04 mm_05_08_03 mm_06_08_03 mm_10_08_03 cid51220 2 1193 48 20cid6 ACE 7 s cid52288 2 2252 64 30cid6 ACE 12 s cid53692 2 3647 80 42cid6 ACE 35 s cid51418 2 1391 48 22cid6 ACE 9 s cid52616 2 2580 64 33cid6 ACE 19 s cid51616 2 1589 48 28cid6 ACE 12 s cid51814 2 1787 48 31cid6 ACE 13 s cid52606 2 2579 48 56cid6 ACE 27 s Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M Z M 153E05 178E15 788E31 133E51 863E78 212E109 200E146 718E189 982E237 681E254 979E8 877E09 889E11 839E08 220E08 529E07 179E08 192E07 811E06 100E6 223E16 100E6 249E32 851E5 326E55 541E5 604E79 179E5 162E114 190E5 488E147 118E5 267E189 923E4 453E237 935E4 944E255 262E4 987E08 564101 819E09 35101 727E11 10401 837E08 379501 184E08 12901 478E07 60201 112E08 113301 501E07 10801 811E06 100E6 223E16 100E6 249E32 851E5 326E55 541E5 604E79 179E5 162E114 190E5 488E147 118E5 267E189 923E4 453E237 935E4 944E255 262E4 987E08 564101 819E09 35101 727E11 10401 837E08 379501 184E08 12901 478E07 60201 112E08 113301 501E07 10801 153E05 1 s 153E05 2 s 178E15 708 s 111E16 X X X X X X X X 770E34 163E55 132E82 X X X X X 979E08 3 s 979E08 11 s 877E09 1231 s 890E11 1503 s 839E08 7 s 121E08 X X X X 530E07 229 s 53E07 6194 s 180E08 2082 s 585E09 779E08 239E10 IJGPwc IS 152E05 217E8 X 0 X 0 133E51 467E7 863E78 137E7 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 X 0 54 Results posterior marginal tasks 541 Setup evaluation criteria We experimented following benchmark domains The linkage instances b relational instances c grid instances d logistics planning instances We measure accuracy solvers average Hellinger distance 57 Given mixed network n variables let P Xi A Xi denote exact approximate marginals variable Xi average Hellinger distance denoted cid8 deﬁned cid2 cid2 n i1 1 2 xi Di cid8 P xi n Axi 2 39 Hellinger distance lies 0 1 lower bounds KullbackLeibler divergence 58 A Hellinger distance 0 solver indicates solver output exact marginal distribution variable Hellinger distance 1 indicates solver failed output solution As pointed 57 Hellinger distance superior choices KullbackLeibler KL divergence mean squared error relative error zero inﬁnitesimally small probabilities present We use KL divergence lies 0 practice exact marginals 0 close ﬂoatingpoint V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 719 Table 9 Table showing Hellinger distance cid8 exact approximate marginals IJGPwcSS IJGPwcIS IJGP EPIS EDBP Linkage instances UAI 2006 evaluation 3 hours CPU time For IJGPwcIS IJGPwcSS report number consistent samples M generated 3 hours Problem BN_69 BN_70 BN_71 BN_72 BN_73 BN_74 BN_75 BN_76 cid5n k c e wcid6 ACE time cid5777 7 228 78 39cid6 ACE timeout cid52315 5 484 159 35cid6 ACE 233 s cid51740 6 663 202 53cid6 ACE timeout cid52155 6 752 252 65cid6 ACE timeout cid52140 5 651 216 67cid6 ACE timeout cid5749 6 223 66 35cid6 ACE timeout cid51820 5 477 155 37cid6 ACE timeout cid52155 7 605 169 53cid6 ACE timeout IJGPwcSS IJGP EPIS EDBP IJGPwcIS cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M 94E04 684E5 26E03 192E5 56E03 746E4 36E03 153E5 21E02 775E4 69E04 280E5 80E03 772E4 18E02 252E4 32E02 33E02 19E02 72E03 28E02 43E06 62E02 26E02 1 1 1 1 1 1 1 1 80E02 96E02 25E02 13E02 61E02 43E02 93E02 27E02 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 precision errors exact approximate solver yield false zero correct marginal nonzero vice versa yielding inﬁnite KL divergence14 We compute error commonly distance measures mean squared error relative error absolute error All error measures similar trends Hellinger distance discriminative Finally marginal task IJGPwcSSLB IJGPwcSSUB output marginals benchmarks experimented distinguish This implies lower upper approxima tions backtrack free probability strong negligible zero bias Therefore rest subsection refer IJGPwcSSLB IJGPwcSSUB IJGPwcSS 542 Linkage instances In Table 9 report average Hellinger distance exact approximate marginals linkage instances UAI 2006 evaluation 55 We report pedigree instances UAI 2008 evaluation 44 exact marginals known We IJGPwcSS accurate IJGP turn accurate EDBP 7 8 instances We clearly relationship treewidth performance propagation based sampling based techniques When treewidth relatively small BN_74 propagation based scheme like IJGP accurate IJGPwcSS treewidth increases orders magnitude difference Hellinger distance EPIS IJGPwcIS generate single consistent sample 3 hours CPU time average Hellinger distance 115 In Fig 9 demonstrate superior anytime performance IJGPwcSS compared solvers 543 Relational instances We experimented 10 Friends Smoker networks 6 mastermind networks relational Bayesian networks domain 22 Table 10 shows Hellinger distance exact approximate marginals 3 hours CPU time solver On small friends smoker networks fs04 fs13 IJGP performs better IJGPwcSS However large networks 13240 76212 variables treewidth 12 559 IJGPwcSS performs better IJGP EDBP slightly worse IJGP runs memory large instances indicated Hellinger distance 1 EPIS able generate single consistent sample 3 hours CPU time indicated Hellinger distance 1 instances IJGPwcIS fails instances On instances IJGPwcIS smaller error IJGPwcSS generates consistent samples IJGPwcSS factor 10200 Discussion The small sample size IJGPwcSS compared pure sampling counterpart IJGPwcIS overhead solving satisﬁability formula backtracking search generate sample IJGPwcIS hand uses 14 Also example results recent UAI evaluation 44 Dechter Mateescu 59 proved IJGP EDBP yield marginals having inﬁnite KL distance However cases solvers inﬁnite KL distance precision errors 15 The EPIS program output number consistent samples computing marginals It outputs invalid marginal distribution variables example output marginal distribution 0 0 0 variable having 3 values domain generates consistent samples stipulated timebound 720 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Fig 9 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Linkage instances Table 10 Table showing Hellinger distance cid8 exact approximate marginals IJGPwcSS IJGPwcIS IJGP EPIS EDBP relational instances 3 hours CPU time For IJGPwcIS IJGPwcSS report number consistent samples M generated 3 hours Problem Friends smokers cid5n k c e wcid6 ACE time IJGPwcSS IJGP EPIS EDBP IJGPwcIS fs04 fs07 fs10 fs13 fs16 fs19 fs22 fs25 fs28 fs29 Mastermind mm_03_08_03 mm_03_08_04 mm_03_08_05 mm_04_08_04 mm_05_08_03 mm_06_08_03 mm_10_08_03 cid5262 2 74 226 12cid6 ACE 4 s cid51225 2 371 1120 35cid6 ACE 4 s cid53385 2 1055 3175 71cid6 ACE 9 s cid57228 2 2288 6877 117cid6 ACE 10 s cid513240 2 4232 12712 171cid6 ACE 14 s cid521907 2 7049 21166 243cid6 ACE 23 s cid533715 2 10901 32725 315cid6 ACE 49 s cid549150 2 15950 47875 431cid6 ACE 74 s cid568698 2 22358 67102 527cid6 ACE 149 s cid576212 2 24824 74501 559cid6 ACE 168 s cid51220 2 1193 48 20cid6 ACE 7 s cid52288 2 2252 64 30cid6 ACE 12 s cid53692 2 3647 80 42cid6 ACE 35 s cid52616 2 1391 64 33cid6 ACE 19 s cid51616 2 2580 48 28cid6 ACE 12 s cid51814 2 1787 48 31cid6 ACE 13 s cid52606 2 2579 48 56cid6 ACE 27 s cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M 54E05 100E6 14E02 100E6 12E02 851E5 20E02 541E5 12E03 179E5 31E03 190E5 25E03 118E5 25E03 923E4 13E03 935E4 19E03 262E4 11E03 564E5 11E02 351E4 40E02 104E4 31E02 129E4 10E02 602E4 47E03 113E5 39E02 108E4 46E08 16E02 63E03 65E03 68E03 88E03 86E03 84E03 74E03 70E03 38E02 44E02 32E02 35E02 36E02 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 64E02 30E02 27E02 23E02 17E02 1 1 1 1 1 38E01 1 1 1 40E02 33E02 56E01 32E01 53E02 1 83E02 36E06 217E8 1 0 1 0 14E04 467E7 21E05 137E7 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 721 Fig 10 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Friends Smokers networks Fig 11 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Mastermind networks relational consistency 659 power IJGP reduce rejection preprocessing step 10 This highlights times constraintbased inference determine inconsistencies sampling costeffective combining search sampling Such constraint based inference schemes scalable fail yield single consistent sample larger instances fs19 fs29 Thus advantage larger sample size use simple strategy run conventional sampling minutes resort SampleSearch pure sampling produce consistent samples On mastermind networks IJGPwcSS superior scheme followed IJGP EPIS fails output single consistent sample 3 hours 6 7 instances IJGPwcIS fails instances EDBP slightly worse IJGP 5 6 instances Figs 10 11 anytime performance solvers demonstrating clear superiority IJGPwcSS 544 Grid networks The grid Bayesian networks available authors Cachet 41 A grid Bayesian network s s grid directed edges node neighbors right The upperleft node source right node sink The sink node evidence node The deterministic ratio p parameter specifying fraction nodes deterministic functional case values function values parents The grid instances designated ps For example instance 5018 indicates grid size 18 50 nodes deterministic functional Table 11 shows Hellinger distance 3 hours CPU time solver Time versus approximation error plots shown sample instances Figs 12 14 On grids deterministic ratio 50 EPIS best performing scheme instances On instances IJGPwcIS yields marginals having smaller error IJGPwcSS On instances sampling schemes 722 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Table 11 Table showing Hellinger distance cid8 exact approximate marginals IJGPwcSS IJGPwcIS IJGP EPIS EDBP Grid networks 3 hours CPU time For IJGPwcIS IJGPwcSS report number consistent samples M generated 3 hours Problem Deterministic ratio 50 50125 50145 50155 50175 50185 50195 Deterministic ratio 75 75165 75175 75185 75195 75205 75215 75225 75235 75245 75265 Deterministic ratio 90 90205 90225 90235 90245 90255 90265 90345 90385 cid5n k c e wcid6 ACE time cid5144 2 62 1 16cid6 ACE 3 s cid5196 2 93 1 20cid6 ACE 3 s cid5225 2 111 1 23cid6 ACE 6 s cid5289 2 138 1 25cid6 ACE 211 s cid5324 2 153 1 27cid6 ACE timeout cid5361 2 172 1 28cid6 ACE timeout cid5256 2 193 1 24cid6 ACE 7 s cid5289 2 217 1 25cid6 ACE 9 s cid5324 2 245 1 27cid6 ACE 11 s cid5361 2 266 1 28cid6 ACE 14 s cid5400 2 299 1 30cid6 ACE 11 s cid5441 2 331 1 32cid6 ACE 60 s cid5484 2 361 1 35cid6 ACE 78 s cid5529 2 406 1 35cid6 ACE 420 s cid5576 2 442 1 38cid6 ACE 228 s cid5676 2 506 1 44cid6 ACE timeout cid5400 2 356 1 30cid6 ACE 8 s cid5484 2 430 1 35cid6 ACE 7 s cid5529 2 468 1 35cid6 ACE 9 s cid5576 2 528 1 38cid6 ACE 6 s cid5625 2 553 1 39cid6 ACE 7 s cid5676 2 597 1 44cid6 ACE 10 s cid51156 2 1048 1 65cid6 ACE 25 s cid51444 2 1300 1 69cid6 ACE 136 s IJGPwcSS IJGP EPIS EDBP IJGPwcIS cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M cid8 M 43E04 190E6 49E04 937E5 49E04 524E5 80E04 434E5 93E04 346E5 11E03 287E5 65E04 974E5 14E03 715E5 12E03 447E5 90E03 407E5 62E04 410E5 19E03 313E5 32E03 267E5 20E03 175E5 84E03 129E5 24E02 125E5 16E03 832E5 46E04 442E5 28E04 670E5 50E04 701E5 27E07 704E5 10E03 413E5 86E04 280E5 16E02 115E5 32E07 26E04 25E02 18E02 12E04 40E02 10E02 23E04 61E02 21E02 20E04 36E03 19E02 30E04 21E03 34E02 40E04 34E04 25E07 17E04 78E02 26E07 27E04 12E03 39E02 20E04 50E03 43E02 25E04 67E05 31E07 19E04 17E02 29E07 28E04 15E02 23E02 26E04 20E02 48E02 23E04 24E02 43E02 26E04 35E02 51E02 35E04 51E02 27E07 25E04 37E02 28E07 15E04 51E02 32E07 39E04 19E02 39E07 35E04 28E02 27E07 34E04 46E02 19E06 23E04 39E02 18E07 39E04 41E02 43E07 17E03 16E01 15E04 123E8 21E04 890E7 65E04 768E7 10E03 582E7 76E04 515E7 15E03 280E7 14E04 711E7 16E04 541E7 19E04 523E7 19E04 393E7 28E04 264E7 62E04 233E7 54E04 212E7 71E04 177E7 89E04 261E7 14E03 220E7 65E05 477E7 10E04 397E7 70E05 400E7 92E05 229E7 27E07 257E7 19E04 290E7 63E04 137E7 10E03 708E6 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 723 Fig 12 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Grid instances deterministic ratio 50 Fig 13 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Grid instances deterministic ratio 75 Fig 14 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Grid instances deterministic ratio 90 724 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 Table 12 Table showing Hellinger distance cid8 exact approximate marginals IJGPwcSS IJGPwcIS IJGP EPIS EDBP Logistics planning instances 3 hours CPU time For IJGPwcIS IJGPwcSS report number consistent samples M generated 3 hours Problem log1 log2 log3 log4 log5 cid5n k c e wcid6 ACE time cid54724 2 3785 3785 22cid6 ACE 1 s cid526114 2 24777 24777 51cid6 ACE 58 s cid530900 2 29487 29487 56cid6 ACE 23 s cid523266 2 20963 20963 52cid6 ACE 68 s cid532235 2 29534 29534 51cid6 ACE 727 s IJGPwcSS IJGP EPIS EDBP IJGPwcIS cid8 M cid8 M cid8 M cid8 M cid8 M 22E05 135E8 86E04 149E6 12E04 105E5 23E02 103E5 86E03 973E3 2 s 0 98E03 75E03 18E01 12E02 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 yield smaller error EDBP IJGP There orders magnitude difference IJGPwcSS EDBPIJGP order magnitude difference EPIS IJGPwcIS IJGPwcSS On grids deterministic ratio 75 IJGP best smaller grids size 21 EPIS dominates larger grids size 2226 IJGPwcIS worse IJGP smaller grids size 21 dominates IJGP larger grids IJGPwcIS consistently worse EPIS overhead exact inference step wcutset sampling minﬁll ordering IJGPwcIS We topological ordering EPIS performs better minﬁll ordering Speciﬁcally set w 0 use topological ordering performance IJGPwcIS EPIS results shown On grids deterministic ratio 90 IJGP superior scheme IJGPwcIS slightly better EPIS turn slightly better IJGPwcSS EDBP accurate scheme Again orders magnitude difference sample size IJGPwcIS IJGPwcSS The poor performance IJGPwcSS compared EPIS IJGPwcIS search overhead On grid networks rejection issue IJGPwcIS EPIS solvers deterministic portion easy infer ence Although surface EPIS IJGPwcIS reason determinism case It known Loopy Belief propagation run convergence makes constraint portion mixed network relationallyarcconsistent 59 Therefore constraint network small treewidth Loopy BP IJGP yield proposal distribution backtrackfree backtrackfree Note enforcing relational consis tency reduce completely eliminate rejection problem Typically guarantee backtrackfree distribution obtained use consistency enforcement scheme time space complexity bounded treewidth constraint portion mixed network 60 Chapter 2 details Overall treewidth constraint portion large SampleSearch practical alternative available date 545 Logistics planning instances Our domain logistics planning Given prior probabilities actions facts task compute marginal distribution variable Goals initial conditions observed true Bayesian networks generated plan graphs additional nodes observed false added represent mutex actioneffect preconditions actions These benchmarks available authors Cachet 41 Table 12 summarizes results IJGPwcIS EPIS EDBP fail instances IJGP solves log1 instance exactly Table 12 remaining instances IJGPwcSS accurate IJGP In Fig 15 indicated demonstrate superior anytime performance IJGPwcSS compared schemes 55 Impact ordering heuristics Table 13 shows impact different variable ordering heuristics performance IJGPwcSS measured terms Hellinger distance exact approximate marginals For brevity results sample instances domain We clearly Grid instances average minﬁll ordering performs better schemes The topological ordering scheme performs best grid instances hmetis mindegree ordering worst performing schemes 56 Summary experimental evaluation We implemented SampleSearch advanced importance sampling technique IJGPwcIS presented 10 yielding scheme called IJGPwcSS The search implemented minisat 15 For model counting compared IJGPwcSS approximate solution counters available literature ApproxCount 16 SampleCount 17 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 725 Fig 15 Time versus Hellinger distance cid8 exact approximate marginals IJGPwcIS IJGPwcSS IJGP EPIS EDBP sample Logistics planning instances Table 13 Table showing effect ordering heuristics minﬁll mindegree hmetis topological Hellinger distance cid8 exact approximate marginals computed IJGPwcSS The timebound 3 hours The best performing scheme highlighted row For ordering heuristic report treewidth w Problem Linkage BN_69 BN_70 BN_75 BN_76 Grids 50185 50195 75245 75265 90345 90385 Relational fs28 fs29 mm_06_08_030015 mm_10_08_030015 cid5n k ecid6 cid5777 7 78cid6 cid52315 5 159cid6 cid51820 5 155cid6 cid52155 7 169cid6 cid5324 2 1cid6 cid5361 2 1cid6 cid5576 2 1cid6 cid5676 2 1cid6 cid51156 2 1cid6 cid51444 2 1cid6 cid568698 2 67102cid6 cid576212 2 74501cid6 cid51814 2 48cid6 cid52606 2 48cid6 IJGPwcSS minﬁll mindegree topological hmetis cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w cid8 w 94E04 39 26E03 35 80E03 37 18E02 37 93E04 27 11E03 28 43E02 38 24E02 44 86E04 65 16E02 69 11E03 527 19E03 559 47E03 31 39E02 56 20E03 38 14E02 56 22E03 41 15E01 40 71E03 27 13E03 27 38E02 40 80E02 48 16E03 65 16E02 69 13E03 527 21E03 559 61E03 31 65E02 56 47E03 122 75E03 144 21E02 178 32E02 333 33E04 21 35E04 21 19E03 37 8E04 38 14E03 51 30E03 56 64E04 632 68E03 803 19E02 173 88E02 185 22E03 39 83E03 51 55E03 46 18E02 40 23E03 30 18E03 28 22E02 38 45E02 46 94E04 61 45E02 69 27E03 719 34E03 799 85E03 35 56E02 48 Relsat 18 IJGPwcIS benchmarks Latin Square instances b Langford instances c FPGA routing instances We instances IJGPwcSS yields solution counts closer true counts orders magnitude output SampleCount orders magnitude output 726 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 ApproxCount Relsat IJGPwcIS fails generate single consistent sample SAT instances 10 hours CPU time clearly demonstrating usefulness IJGPwcSS deriving meaningful approximations presence signiﬁcant determinism For computing probability evidence Bayesian network partition function Markov network compared IJGPwcSS Variable Elimination Conditioning VEC 19 advanced generalized belief propagation scheme called Edge Deletion Belief Propagation EDBP 20 benchmark domains linkage analysis b rela tional Bayesian networks We instances estimates output IJGPwcSS closer exact answer output EDBP VEC solved instances exactly remaining instances substan tially inferior IJGPwcIS superior IJGPwcSS able generate consistent samples However majority instances simply failed yield consistent samples For posterior marginal task experimented linkage analysis benchmarks partially deterministic grid bench marks relational benchmarks logistics planning benchmarks We compared accuracy IJGPwcSS Hellinger distance schemes generalized belief propagation schemes Iterative Join Graph Prop agation 12 Edge Deletion Belief Propagation 20 adaptive importance sampling scheme called Evidence Pre propagated Importance Sampling EPIS 23 IJGPwcIS We grid instances IJGPwcSS consistently yielded estimates having smaller error EDBP IJGP Whenever IJGPwcIS EPIS fail generated consistent samples performed better IJGPwcSS On remaining instances IJGPwcSS clearly superior 6 Conclusion The paper presented SampleSearch scheme improving performance importance sampling mixed proba bilistic deterministic graphical models It known graphical models importance sampling performs poorly rejection problem SampleSearch overcomes rejection problem interleaving random sam pling systematic backtracking Speciﬁcally sampling variables logic sampling 5 instead rejecting sample inconsistency detected SampleSearch backtracks previous variable modiﬁes pro posal distribution reﬂect inconsistency continues process consistent sample We showed SampleSearch viewed systematic search technique value selection stochastically guided sampling distribution This view enables integration systematic SATCSP solver Sample Search minor modiﬁcations Indeed experiments advanced SAT solver called minisat 15 Thus advances systematic search community primary focus solving yesno type NPcomplete problems leveraged SampleSearch approximating harder Pcomplete problems Bayesian inference We characterized sampling distribution SampleSearch backtrackfree distribution modiﬁcation proposal distribution inconsistent partial assignments speciﬁed order removed When backtrackfree probability given sampled assignment complex compute proposed approxima tions bound backtrackfree probability yield asymptotically unbiased estimates weighted counts marginals We performed extensive empirical evaluation benchmark graphical models results clearly demon strate lower upper approximations accurate benchmarks Overall SampleSearch consistently superior stateoftheart schemes domains having substantial determinism Speciﬁcally probabilistic graphical models showed stateoftheart importance sampling techniques EPIS 23 IJGPwcIS 10 reason determinism limited way unable generate single consistent sample hard linkage analysis relational benchmarks In cases SampleSearch alternative importance sampling technique date SampleSearch superior generalized belief propagation schemes like Iterative Join Graph Propagation IJGP 12 Edge Deletion Belief Propagation EDBP 20 In theory propagation techniques anytime approximation quality improved increasing ibound However time space complexity exponential practice memory requirement major bottleneck certain ibound typically 22 Consequently benchmarks observed IJGP EDBP quickly converge estimate unable improve time On hand demonstrated SampleSearch improves time yields superior anytime performance IJGP EDBP Finally problem counting solutions SATCSP showed SampleSearch slightly better recently proposed SampleCount 17 technique substantially better ApproxCount 16 Relsat 18 SampleSearch leaves plenty room future improvements likely cost effective practice For instance generate samples solve SATCSP problem multiple times Therefore goods goods knowledge problem space learnt generating sample speedup search solution generating sample How achieve principled structured way important theoretical practical question Some initial related research solving similar SAT problems appeared bounded model checking community 61 applied improve SampleSearchs performance A second line improvement eﬃcient algorithm compactly storing combining DFS traces deriving lower upper approximations Currently store DFS traces OR tree Instead easily use ANDOR V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 727 search space 62 Borrowing ideas literature ordered binary decision diagrams OBDDs 63 merge isomorphic traces eliminate redundancy compact representation A line future research use adaptive importance sampling 303364 In adaptive importance sampling updates proposal distribution based generated samples update proposal gets closer closer desired posterior distribution Because store DFS traces generated samples SampleSearch use dynamically update learn proposal distribution Acknowledgements This work supported NSF award number IIS0713118 NIH grant R01HG00417502 Appendix A Proofs Proof Theorem 2 Because B xi1 A xi1 Ni cid4 cid14 cid9 x cid15 cid19 cid19xi1 Q cid2 xi1 Ni C cid4 cid14 cid15 cid19 cid19xi1 cid9 x Q cid9 x xi1 B cid4 cid14 cid9 x cid15 cid19 cid19xi1 Q cid9 x A xi1 Ni cid4 1 C xi1 Ni cid4 1 cid14 cid9 x cid15 cid19 cid19xi1 Q xi1 cid9 B x Q ixixi1 cid9 Q ix xi1 B cid9 x cid2 1 xi1 cid2 1 xi1 Ni cid9 x A cid2 cid9 x A xi1 Ni C Q ixixi1 xi1 Ni xi1 Ni C cid9 Q ix xi1 Q F ncid3 xixi1 cid2 L F Nixixi1 ncid3 Q F xixi1 cid2 Nixixi1 L F i1 cid5 m i1 F ix cid5 Q F x i1 Q F x cid2 L F N x cid5 p m i1 F ix j1 C jx L F N x cid5 cid4 p j1 C jx 1 N Ncid4 k1 w F x cid4 w F w F cid14 cid15 xk cid4 1 N cid8Z N cid4 cid9Z L N L x Ncid4 k1 cid15 cid14 xk w F L Similarly A xi1 Ni B xi1 easy prove cid8Z F N cid2 cid9Z U N cid2 40 41 42 43 44 45 46 47 48 49 N L F N limit inﬁnite samples coincide Proof Theorem 3 From Proposition 4 follows U F backtrackfree distribution Q F Therefore cid5 cid5 lim N w L N x lim N cid5 m i1 F ix p j1 C jx m i1 F ix L F N x p j1 C jx cid5 Q F x w F x Therefore EQ lim N cid22 1 N Ncid4 k1 cid23 w Lx lim N 1 N cid4 xX w L N xQ x Ncid4 1 k1 1 N N lim N cid4 xX w L N xQ x 50 51 52 53 54 728 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 w F xQ x cid14 cid15 Eq 52 cid4 xX Z 55 56 w U Similarly prove estimator based U F N x Eqs 5356 Finally estimates cid9P U N xi cid9P L asymptotically unbiased estimators deﬁnition asymptotically unbiased cid2 N xi P xi given Eqs 36 37 respectively ratios N Eq 34 asymptotically unbiased replacing w L N x cid9 Proof Theorem 4 Because store solutions x1 xn partial assignments x1 xi1 x proved inconsistent N executions SampleSearch require additional O N n d space store combined sample tree estimate Z marginals Similarly compute sum ratios visiting nodes combined sample tree time complexity O N d n cid2 References 1 R Dechter D Larkin Hybrid processing beliefs constraints Proceedings 17th Conference Uncertainty Artiﬁcial Intelligence UAI 2001 pp 112119 2 D Larkin R Dechter Bayesian inference presence determinism Tenth International Workshop Artiﬁcial Intelligence Statistics AISTATS 2003 3 R Dechter R Mateescu Mixtures deterministicprobabilistic networks ANDOR search space Proceedings 20th Annual Confer ence Uncertainty Artiﬁcial Intelligence UAI 2004 pp 120129 4 R Mateescu R Dechter Mixed deterministic probabilistic networks Annals Mathematics Artiﬁcial Intelligence AMAI Special Issue Prob abilistic Relational Learning 54 13 2008 351 5 J Pearl Probabilistic Reasoning Intelligent Systems Morgan Kaufmann 1988 6 R Dechter Constraint Processing Morgan Kaufmann 2003 7 AW Marshall The use multistage sampling schemes Monte Carlo computations Symposium Monte Carlo Methods 1956 pp 123140 8 RY Rubinstein Simulation Monte Carlo Method John Wiley Sons Inc 1981 9 J Geweke Bayesian inference econometric models Monte Carlo integration Econometrica 57 6 1989 13171339 10 V Gogate R Dechter Approximate inference algorithms hybrid Bayesian networks discrete constraints Proceedings 21st Annual Conference Uncertainty Artiﬁcial Intelligence UAI 2005 pp 209216 11 JS Yedidia WT Freeman Y Weiss Constructing free energy approximations generalized belief propagation algorithms IEEE Transactions Information Theory 51 2004 22822312 12 R Dechter K Kask R Mateescu Iterative join graph propagation Proceedings 18th Conference Uncertainty Artiﬁcial Intelligence UAI Morgan Kaufmann 2002 pp 128136 13 R Mateescu K Kask V Gogate R Dechter Joingraph propagation algorithms Journal Artiﬁcial Intelligence Research 37 2009 279328 14 B Bidyuk R Dechter Cutset sampling Bayesian networks Journal Artiﬁcial Intelligence Research JAIR 28 2007 148 15 N Sorensson N Een Minisat v113A SAT solver conﬂictclause minimization SAT 2005 Competition 2005 16 W Wei J Erenrich B Selman Towards eﬃcient sampling exploiting random walk strategies Proceedings Nineteenth National Conference Artiﬁcial Intelligence 2004 pp 670676 17 CP Gomes J Hoffmann A Sabharwal B Selman From sampling model counting Proceedings 20th International Joint Conference Artiﬁcial Intelligence IJCAI 2007 pp 22932299 18 J Roberto J Bayardo JD Pehoushek Counting models connected components Proceedings 17th National Conference Artiﬁcial Intelli gence AAAI 2000 pp 157162 19 R Dechter Bucket elimination A unifying framework reasoning Artiﬁcial Intelligence 113 1999 4185 20 A Choi A Darwiche An edge deletion semantics belief propagation practical impact approximation quality Proceedings TwentyFirst National Conference Artiﬁcial Intelligence AAAI 2006 pp 11071114 21 M Fishelson D Geiger Optimizing exact genetic linkage computations Proceedings Seventh Annual International Conference Research Computational Molecular Biology RECOMB 2003 pp 114121 22 MD Chavira A Darwiche M Jaeger Compiling relational Bayesian networks exact inference International Journal Approximate Reason ing 42 12 2006 420 23 C Yuan MJ Druzdzel Importance sampling algorithms Bayesian networks Principles performance Mathematical Computer Modelling ISSN 08957177 43 9010 2006 11891207 24 V Gogate R Dechter SampleSearch A scheme searches consistent samples Proceedings 11th Conference Artiﬁcial Intelligence Statistics AISTATS 2007 pp 147154 25 V Gogate R Dechter Approximate counting sampling backtrackfree search space Proceedings 22nd Conference Artiﬁcial Intelligence AAAI 2007 pp 198203 26 J Liu MonteCarlo Strategies Scientiﬁc Computing SpringerVerlag New York 2001 27 EC Freuder A suﬃcient condition backtrackfree search Journal ACM 29 1 1982 2432 28 T Walsh SAT v CSP Proceedings 6th International Conference Principles Practice Constraint Programming SpringerVerlag London UK ISBN 3540410538 2000 pp 441456 29 K Pipatsrisawat A Darwiche RSAT 20 SAT solver description Tech Rep D153 Automated Reasoning Group Computer Science Department UCLA 2007 30 J Cheng MJ Druzdzel AISBN An adaptive importance sampling algorithm evidential reasoning large Bayesian networks Journal Artiﬁcial Intelligence Research JAIR 13 2000 155188 31 RD Shachter MA Peot Simulation approaches general probabilistic inference belief networks Proceedings Fifth Annual Conference Uncertainty Artiﬁcial Intelligence UAI 1990 pp 221234 32 RM Fung KC Chang Weighing integrating evidence stochastic simulation Bayesian networks Proceedings Fifth Annual Confer ence Uncertainty Artiﬁcial Intelligence UAI 1990 pp 209220 33 L Ortiz L Kaelbling Adaptive importance sampling estimation structured domains Proceedings 16th Annual Conference Uncertainty Artiﬁcial Intelligence UAI 2000 pp 446454 V Gogate R Dechter Artiﬁcial Intelligence 175 2011 694729 729 34 R Fung B del Favero Backward simulation Bayesian networks Proceedings Tenth Annual Conference Uncertainty Artiﬁcial Intelli gence UAI 1994 pp 227234 35 K Kask R Dechter J Larrosa A Dechter Unifying tree decompositions reasoning graphical models Artiﬁcial Intelligence 166 12 2005 165193 36 G Casella CP Robert Raoblackwellisation sampling schemes Biometrika 83 1 1996 8194 doi101093biomet83181 37 A Darwiche M Hopkins Using recursive decomposition construct elimination orders jointrees dtrees Trends Artiﬁcial Intelligence Lecture Notes AI SpringerVerlag 2001 pp 180191 38 B Bidyuk R Dechter On ﬁnding minimal wcutset problem Proceedings 20th Conference Uncertainty Artiﬁcial Intelligence UAI 2004 pp 4350 39 W Wei B Selman A new approach model counting Proceedings Eighth International Conference Theory Applications Satisﬁability Testing SAT 2005 pp 324339 40 LG Valiant The complexity enumeration reliability problems SIAM Journal Computation 8 3 1987 105117 41 T Sang P Beame H Kautz Heuristics fast exact model counting Eighth International Conference Theory Applications Satisﬁability Testing SAT 2005 pp 226240 42 B Selman H Kautz B Cohen Noise strategies local search Proceedings Eleventh National Conference Artiﬁcial Intelligence 1994 pp 337343 43 KP Murphy Y Weiss MI Jordan Loopy belief propagation approximate inference empirical study Proceedings Fifteenth Conference Uncertainty Artiﬁcial Intelligence UAI 1999 pp 467475 44 A Darwiche R Dechter A Choi V Gogate L Otten Results probabilistic inference evaluation UAI08 available online httpgraphmod icsucieduuai08EvaluationReport 2008 45 R Dechter V Gogate L Otten R Marinescu R Mateescu Graphical model algorithms UC Irvine website httpgraphmodicsuciedugroup Software 2009 46 A Darwiche A differential approach inference Bayesian networks Journal ACM 50 3 2003 280305 47 A Darwiche New advances compiling CNF decomposable negation normal form Proceedings 16th European Conference Artiﬁcial Intelligence ECAI 2004 pp 328332 48 A Darwiche P Marquis A knowledge compilation map Journal Artiﬁcial Intelligence Research JAIR 17 2002 229264 49 C Gomes D Shmoys Completing quasigroups Latin squares structured graph coloring problem Proceedings Computational Symposium Graph Coloring Extensions 2002 50 T Ritter Latin squares A literature survey available online httpwwwciphersbyrittercomRESLATSQHTM 51 T Walsh Permutation problems channelling constraints Proceedings 8th International Conference Logic Programming Automated Reasoning LPAR 2001 pp 377391 52 V Gogate B Bidyuk R Dechter Studies lower bounding probability evidence Markov inequality Proceedings 23rd Conference Uncertainty Artiﬁcial Intelligence UAI 2007 pp 141148 53 L Simon DL Berre E Hirsch The SAT 2002 competition Annals Mathematics Artiﬁcial Intelligence AMAI 43 2005 307342 54 J Ott Analysis Human Genetic Linkage The Johns Hopkins University Press Baltimore Maryland 1999 55 J Bilmes R Dechter Evaluation probabilistic inference systems UAI06 available online httpsslieewashingtonedubilmesuai06Inference Evaluation 2006 56 K Kask R Dechter A Gelfand BEEM bucket elimination external memory 26th Conference Uncertainty Artiﬁcial Intelligence UAI 2010 pp 268276 57 G Kokolakis P Nanopoulos Bayesian multivariate microaggregation Hellinger distance criterion Research Oﬃcial Statistics 4 2001 117125 58 S Kullback RA Leibler On information suﬃciency The Annals Mathematical Statistics 22 1 1951 7986 59 R Dechter R Mateescu A simple insight iterative belief propagations success Proceedings 19th Conference Uncertainty Artiﬁcial Intelligence UAI 2003 p 175183 60 V Gogate Sampling algorithms probabilistic deterministic graphical models PhD thesis University California Irvine 2009 61 N Eén N Sörensson Temporal induction incremental SAT solving Electronic Notes Theoretical Computer Science ISSN 15710661 89 4 2003 543560 62 R Dechter R Mateescu ANDOR search spaces graphical models Artiﬁcial Intelligence 171 23 2007 73106 63 RE Bryant Graphbased algorithms Boolean function manipulation IEEE Transactions Computers 35 8 1986 677691 64 S Moral A Salmerón Dynamic importance sampling Bayesian networks based probability trees International Journal Approximate Reason ing 38 3 2005 245261