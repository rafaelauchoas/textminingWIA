ELSEVIER Artificial Intelligence 73 1995 271306 Artificial Intelligence Reinforcement learning nonMarkov decision processes Steven DWhitehead LongJi Lin b GTE Laboratories Incorporated 40 Sylvan Road Waltham MA 02254 USA h School Computer Science Carnegie Mellon University Pittsburgh PA 15213 USA Received September 1992 revised April 1993 Abstract Techniques based reinforcement learning RL build systems learn perform nontrivial sequential decision tasks To date work focused learning tasks described Markov decision processes While formalism useful modeling wide range control problems important tasks inherently non Markov We refer hidden state tasks arise information relevant agents immediate identifying sensation Two important types control problems resist Markov modeling 1 high degree control information collected sensors active vision 2 limited set sensors provide adequate information current state environment Existing RL algorithms perform unreliably hidden state tasks state environment hidden missing This article examines general approaches extending reinforcement learning hidden state tasks The Consistent Representation CR Method unifies recent approaches Lion algorithm Galgorithm CSQL The method useful learning tasks require agent control sensory inputs However assumes appropriate control perception external states identified point time immediate sensory inputs A second general set algorithms agent maintains internal state time considered These storedstate algorithms different share common feature derives internal representation combining immediate sensory inputs internal state maintained time The relative merits methods considered conditions useful application discussed Correponding Currently Siemens Corporate Research author Fax 617 8909320 Email swhiteheadgtecom 755 College Road East Princeton NJ 08540 USA Fax 609 7346565 Email Incorporated Ijllearningscrsiemenscom 00043702950950 SSDI 00043702 1995 Elsevier Science BV All rights reserved 9400012P 272 SD Whitehead LJ LinArtijiciul lnfelligence 73 1995 271306 1 Introduction necessary robots achieved theories agentenvironment interaction need include account develop maintain intelligent agents Sophisticated complex idiosyncratic rigid meticulous programming uncertain inexpressive The real know ahead time programming agents bear burden skill acquisition world stand maintain high agents learn new skills adapt old ones changes level Computational learning Learning realworld world programming feasible Intelligent Also performance environment Though languages kinds learning things end learning boils learning control The value measured terms ability terms effect agents environment interaction environment This article control focuses reinforcement suited learning environments desired end paradigm 7461 agent learn learned modeled controller In reinforcement learning finite state unknown At time step controller change state generate payoff maximizes control policy coupled control highly interactive interaction transition probabilities learning agentenvironment machine performs action causes The agents objective measure total payoff received time reinforcement environment Some features 1 RL weak method learn statedependent b trialanderror learning occurs ment feedback learning teacher offers correct answer learning RL appealing experimentation environ takes form scalar payoffno explicit required c sequential decision making tasks payoffs sparse considerably delayed little prior knowledge d required 2 RL incremental 3 RL learn direct sensorymotor mappings appropriate unexpected tasks agent respond quickly online highly reactive events environment 4 RL valid nondeterministic 5 When conjunction temporal difference environments TD methods proven checkers effective difficult sequential decision making 381 pole balancing 8301 backgammon 491 6 RL architectures incorporate supervised extensible Recently RL systems extended aspects planning 152456 learning 25455X hierarchical intelligent exploration Traditionally research RL focused Markov decision processes 19441 RL tasks 21235 11 control 16264 1621 intuitively agent directly observes MDPs corresponds Described control formally Section 2 Markov decision process task point time SD Whitehead LJ LinArtijcial Intelligence 73 1995 271306 273 b predicting internal time encodes information In applications effects actions reasons First agent observe relevant It important effects actions depend sense given label names action state state repre known important focusing Markov decision processes stochastic processes learning methods 441 rely Markov property credit assignment apply classical mathematicals 7445455 state directly agent point state environment current environment This assumption sentation Murkov assumption theoretical development allowed dynamic programming use TDmethods perform badly important Markov decision processes These nonMarkov hidden information situation occur missing relevant piece current 18593 Nevertheless easily violated naturally tusks hidden formulated tasks commonly control problems Second existing RL assumption possible representation reinforcement agents researchers referred learning state learning distinguish guessing bins according guess classification context autonomous Hidden state tasks arise naturally color Binl red blue robots The agents sensors task hand Suppose robot charged task sorting red Bin2 blue If robots given block If equal number blocks robot achieve 100 performance The case missing simplest example hidden state task occurs inadequate blocks sensors unable better color detect color corresponds agents available In general readings uniquely needed information task decision problem available sensor sensors provide internal circumstances identify nonMarkov The case Markov color sense state environment respect information robots chance On hand defined immediate achieve optimal performance better decision problem nonMarkov relevant representation easily representation information needed learn state resources perception integrating 601 In active perception tasks natural consequence Hidden tiveselective allocation sensory sense visual processing modules efficient properly maintained data generated sensors fail code relevant piece information resultant environment periods time decision learning ac agent degree control controlling visual attention selecting current state sensors representation ambiguous respect agent learn control It follows 356 This control control representation inadequate Therefore taskspecific way However environment internal internal task nonMarkov applying Techniques reinforcement learning nonMarkov central technique focus article We generalized sistent Representation CR Method learn control CRmethod active perception 571 The principal idea underlying decision processes called Con systems split 214 SD Whitehead LJ LinArtificial Intelligence 73 I 995 271306 phase overt phase During sensor configuration actions perceptual order gen current external environment select overt action representation changes Markov overt stage representation performs phases perceptual sensing trol phase erate adequate During action representation method perceptual environment Although Lion algorithm proper control sensors It appropriate state information state agent CRmethod learn actions needed 60 CSQL identify state external environment Systems consistent overt actions needed perform task construct adequate taskspecific representation 48 Galgorithm current unifies recent algorithms 131 restricted state environment tasks tasks agent store order infer current time remember previous events speech sensory immediate To overcome considered restriction achieve crude general approaches augments form short recognition retain state information time We refer storedstate methods The simplest approaches inputs delay 25 This approach line successful certain method predictive distinctions learns predictive model sensory drive action selection A approach internal uses uses recurrent neural network learning methods methods considered Following environmental 28 Each term memory tasks 4 142539 inputs combination existing useful application 53 Another alternative conditions control policy directly learn recurrent state model statedependent approach reinforcement described observables called reinforcement The remainder article organized follows Section 2 provides basic review learning Section 3 discusses nonMarkov decision pro cause learning control Section 4 presents control perception storedstate Section 6 discusses drawn conclusions concepts cesses considers Consistent Representation Method technique reviews examples technique Section 5 describes compares methods specifies preference methods conditions broader context scalability Section 7 difficulties learning 2 Review reinforcement learning In section basic concepts reinforcement learning reviewed We begin interaction Next review simple model agentenvironment describing principles Markov decision processes Qlearning learning algorithm A thorough learning focus primarily Qlearning processes Other algorithms review Markov decision processes Qlearning IO 541 For review reinforcement 8194463 learning suffer similar reinforcement review Markov decision processes reinforcement 541 popular scope article Throughout difficulties caused nonMarkov article shall decision fate For complete reader wish consult general 71 SD Whitehead LJ LinArtcial Intelligence 73 1995 271306 275 Environment States S Payoffs Actions A Agent Fig 1 A simple model agentenvironment interaction 21 Modeling agentenvironment interaction Fig 1 illustrates model agentenvironment interaction reinforcement synchronized At time point learning In model agent environment finite state automatons interacting discrete following sequence events occurs widely represented time cyclical process 1 The agent senses state environment 2 Based current state agent chooses action perform 3 Based current state action selected agent environment new state generates payoff makes transition 4 The payoff passed agent 211 The environment The environment modeled described tuple Markov decision process Formally decision process states A set possible actions T state transition reward function At time environment accepts action A S A usually assumed transitions new states typically modeled transition T S x A S The transition Markov S A T R S set possible R function occupies exactly state S discrete finite State pairs terms set transition probabilities Px function T maps stateaction general probabilistic specified function Pxa ProbTxa y 1 Payoffs generated environment maps stateaction probabilistic pairs scalarvalued determined reward function R rewards R S x A II The reward function Notice Markov decision process reward generated state immediate models type said memoryless fundamental Markov property knowledge current state sufficient reward received time actionselection MDP depend current state Process Markov property The implies maximize devise strategies decisions depend additional model environment lo Thus effects actions possible optimal control satisfy information 216 SD Whiteheud LJ LinArtcial Intelligence 73 1995 271306 trace history depend knowledge current state strategies possibly outperform best decision strategies 212 The agent The agent responsible generating current state selects action observes Rewards feedback learning One way specify agents behavior control actions At time step senses result new state reward state action perform Formally scribes states actions f S A fx terms control policy pre state X denotes action performed function policy f learning In reinforcement agents objective learn control policy maxi mizes measure total reward accumulated time In principle number based reward measures discounted sum reward received time This sum called return defined time t prevalent measure returnt yrrl y called temporal discount factor reward received time t n Because objective policy maximizes return given agents objective That f process begins policy V x mfgyfx Vx t S 2 constant 0 1 r agents process stochastic expected return state x follows policy f uniformly f expected The states f best possible For fixed policy f define VJ x value function policy 3 An important property MDPs f defined guaranteed Optima Theorem dynamic programming time discrete state Markov decision process 91 guarantees exists deterministic exist In optimal Furthermore policy f optimal satisfies particular discrete policy following relationship Qrx f x 1 Qxa bx E S 4 actionvalue expected state x applies action follows policy defined function Intuitively Eq 4 states policy optimal Qf x agent starts 9 lo policy specifies action maximizes local actionvalue That return given f state fx Qfxa sgaQfxb dx E S VPX mgyQfxa bx E S 5 6 SD Whitehead LJ LinArtijicial Intelligence 73 1995 271306 277 function uniformly zero Q set initial values actionvalue Repeat forever 1 2 usually consistent fx x current state Select action execute occasionally alternate Execute action let y state r reward received Update Qxa QL t 1 aQxa dryUyl 3 4 UY QY fy Here x E S fx Q X maxA Q x 6 Fig 2 A simple version onestep Qlearning algorithm For given MDP set actionvalues values said define optimal actionvalue Fq 4 holds unique These function Q MDP If MDP completely including optimal policy computed directly transition known probabilities reward cases structure dynamics agent compute learn effective environment techniques circumstances distributions dynamic programming environment optimal policy directly explore control policy trialanderror known Under 91036 However 22 Qlearning Qlearning 54 incremental decision problems Also reinforcement reinforcement learning method It good repre simple mathematically founded difficulties learning 44 TDmethods illustrating useful techniques useful understanding weaknesses reinforcement learning sentative widely For purposes Qlearning caused nonMarkov algorithms understanding algorithms In Qlearning use similar credit assignment difficulties Qlearning Fq directly function initialize 5 A simple Qlearning agent estimates uses algorithm agents actionvalue optimal actionvalue derive control policy shown Qfunction mandated step algorithm agents estimate optimal actionvalue available values arbitrary main controllearning agent selects action Q execute Most time specified policy agent occasionally called greedy strategy Fig 2 The function Q Q task initial action action defined Eq 5 purposes exploration choose action appears suboptimal For example uniformly loop The step sense agent enters state X Next information encoded initial values zero After initialization If prior knowledge current function fx 278 SD Whitehead LJ LinArtijicial Intelligence 73 1995 271306 follow J probability choose The agent executes Finally estimate estimate p choose random action 2 reward r state y result pair x updated In particular actionvalue Q X reward r utility immediate state Iy maxA Qy b The sum stateaction obtained combining selected action notes estimate y YUY called onestep corrected estimator Q definition unbiased estimator Qxn Q 7 Qxu ElRxa yVTxul 8 VX maxA Qx The onestep estimate Q X weighted sum estimate combined old Qxa 1 aQra alvyUvl 9 rate Qlearning learning finite Markov decision process infinitely learning guaranteed limit stateaction converge optimal policy pair tried 551 rate decreases according proper schedule 3 NonMarkov tasks To nonMarkov decision actual decision problem need distinction abstract task external internal decision problems agentindependent agent agentspecific exists outside tasks arise agentenvironment agentindependent faced learning agent We shall refer interactions specification endogenous respectively 31 An example consider To illustrate distinction task inspecting packaging apples ripe Ripe plant Suppose apples sorted according supermarket green apples crushed juice If apples apples shipped Markov decision process model come conveyor time follows We define state variable called ripeness task formulated state variable characterize induce state space size 2 The model actions accept reject apple A reward function defined unit reward ripe apple accepted green apple rejected unit penalty A transition current apple ripe greenthis function defined model dynamics Occasionally choosing ronment Exploration examples sophisticated necessary action random guarantee particularly agent eventually simple mechanism exploring envi learn optimal policy For exploration strategies I 2 I 45511 SD Whitehead LJ LinArtifcial Intelligence 73 1995 271306 279 01 09 Fig 3 The apple sorting example task facing robot sensors b reject 3 6 ept An abstract external model task b The actual internal nonuniform hopper transition probabilities case apples loaded apple sequence dependent different orchards dependence For example assume type probability 09 different probability If types apples coming conveyor temporally crates apple conveyor 01 specification task exist It mathematical mind modeler intended task requirements perform abstraction The states actions capture rewards transitions essence environment external decision problem Fig 3a corresponds model physical agent It makes explicit This process model illustrated reference task It defines consider Conversely decision problem agent situated consider environment robots head For simplicity uses represent affect action In totally basic structure internal decision problem actions A 1 0 A 1 1 The transition determined physics sensorymotor detect lever select apples match On hand sensor detects bruises external color apple conveyor task assuming external sensors affecters determine facing control embedded single binary robot single binary actuator Al inside sensor Sl uses states Sl 0 Sl 1 reward dynamics internal process internal decision process closely red apples ripe green apples green able robots sensor red green affector agents sensor affector closely matched internal problem bear little resemblance dynamics external environment interface For example 32 Accounting perceptualmotor processes The distinction external internal decision model agentenvironment mappings performed agents perceptualmotor shown account processes The new model Fig 4 On sensory agents perceptual processes map states interaction augmenting Fig 1 explicitly tasks incorporated 280 SD Whiieheud LJ LinArtifcml Intelligence 73 1995 271306 States S Actions A I t Percaphrd Pr _i Internal Representation s I I Motor PmmsssS Embedded Controller d Motor Commands A Agent Fig 4 A model agentenvironment processes Perceptual processes map states external world model internal representations Motor processes map internal motor commands actions agents perceptualmotor external world model explicitly accounts interaction involve internal actions considerable motor mapping immediate precepts attentional mechanisms representation On motor external task complexity A perceptual stored history involve complex motor sequences parallel external world states agents agents motor processes map internal motor commands model s In general mappings mapping information actions For purposes present discussion shall restrict consideration simple mappings remainder sensory mappings motor commands embedded generate reward external model controller article concentrate onetoone mapping reward correctly passed embedded actions We assume situation external controller That passed directly world assume Indeed reward arises 33 NonMarv internal tasks Formally decision knowledge current state better predict dynamics process improve control 4 task nonMarkov information agents In general internal states represent multiple external states We phenomenon ceptual aliasing occurs external states perceptual internal decision problem nonMarkov Notice mappings conceptual physical run abstract task model For example state agents internal physical universe modeling process mapped world representation states external represent certain situations task model external 4 Strictly information prediction additional speaking definition history process slightly overstated Traditionally process addition current control However dealing systems sensing chosen information adopt definition said nonMarkov improve collect state potential slightly general SD Whitehead LJ LinArtijicial Intelligence 73 1995 271306 281 mapping superimposed single internal state Intuitively perceptual aliasing occurs agent uncertain state external world For example situated agent occurs agents sensors fail code relevant piece information Perceptual aliasing results nonMarkov decision tasks defi nition states internal representation code information needed characterize future dynamics task Returning apple sorting robot suppose robots color sensor temporarily disconnected read zero In case robot unable distinguish apples color internal representation collapses single state Fig 3b Clearly decision problem nonMarkov 1 knowledge current apples color improve systems performancewithout sensor robot reduced guessing 2 knowledge recent history process improve performance For example knowledge action resulted positive reward exploited yield control policy performs better chance This follows according external model 90 percent time current apple type previous 331 The ubiquity nonMarkov tasks Markov decision tasks ideal NonMarkov tasks norm They ubiquitous uncertainty An agent uncertain state external task necessarily faces internal decision problem nonMarkov And sources uncertainty abound Sensors physical limitations Rarely perfectly matched task Sensor data typically noisy unreliable spurious information Sensors limited range relevant objects occluded Also information hidden time The spoken word transient lost actively processed stored Short term memory limited subject deterioration Lin 281 provides good example Consider packing task involves 4 steps open box gift close seal An agent driven current visual precepts accomplish task facing closed box agent know gift box decide seal open box In case occlusion gift lid prevents immediate perception vital piece information Such hidden state tasks arise temporal features velocity acceleration important optimal control included systems primitive sensor set Even perfect sensors available control problems ambiguous illposed specify state space advance Indeed agents task discover useful state space problem For example integrating learning active perception invariably leads nonMarkov decision tasks 571 Active perception refers idea intelligent agent actively control sensors order sense represent information relevant immediate ongoing activity If agent task learn control sensors 13561252 282 SD Whitehead LJ LinArtificial lnrelligence 73 I 995 271306 periods time agent improperly control internal control problem necessarily learning fail attend state external relevant piece information task nonMarkov This follows sensors identify fail unambiguously 34 Eflects control The level performance inferior assumes problem nonMarkov robot color sensor obtained agent generally decision problem usually problem nonMarkov mistakenly robot degree performance 95 apples ripe blind policy 5 time On hand perceptual decision memory degradation suboptimal Markov This seen apple sorting intact achieve perfect classification internal decision Markov counterpart That agent task perform better chance Notice depends problem For example accepted apple fail tasks best fixed policies arbitrarily bad5 aliasing lead nonMarkov sensor control leads 35 Dijjiculties reinforcement learning learning methods non control cases inability agents underlying states decision agents states external world The straightforward application traditional reinforcement cases yields suboptimal stem represent These difficulties In particular perceptual representation Markov decision problems severely degraded performance obtain accurate estimates utility actionvalues process internal model The utility actionvalue internal represent Because averaging actionvalue result agent inaccurately optimal action agent incorrectly degrade selection suboptimal estimates erroneous perceive suboptimal inevitable aliasing states estimates tend reflect mix average values distinct learned agent nonMarkov states external agents internal utility situations These errors turn states actions action higher value true nonMarkov Unfortunately difficulties compounded 44 cause errors propagate nonaliased employ utility actionvalue rewards utility estimates estimators subsequent states In particular reinforcement state space value optimal action use temporal difference meth infecting learning recently onestep Qlearning combine values statesin estimates obtained adding reward utility es state cf Eq 7 Thus given state agent constructs states estimators likely use utility estimates nonMarkov immediate ods action selection algorithms observed actionvalue timate estimators 5 This significant reinforcement learning algorithms aim learn fixed policies Opening door probabilistic policies improve performance circumstances SD Whitehead LJ LinArtijicial Intelligence 73 1995 271306 283 error propagated state Once infected state erroneous propagate error states similar manner 4 Consistent representation methods In years RL algorithms developed 60 learns CSQL algorithm control visual attention 481 learns efficient extract taskrelevant Galgorithm input vector 6 In section review Consistent Represenration Method computational 131 learns algorithms perception The Lion algorithm deictic sensorymotor sensing operations large present different algorithms 621 deal selective primitive taskspecific bits turn We unifies framework 41 The Lion algorithm The Lion algorithm address adaptive perception reinforcement 59 learning algorithm specifically learn simple It designed feature task manipulation agent focus partial access visual attention relevant objects select appropriate motor commands The details task follows task modified Blocks World The distinguishing equipped controllable To learn environment provides learn task agent sensory task 41 I The blockstacking task The learning task organized sequence trails On trial agent arbitrary stacks Blocks distinguishable pick green block quickly possible ranging presented pile blocks A pile consists random number blocks 1 50 arranged color red green blue Each pile contains single green block The agents goal simply goal reward receives reward The dynamics environment block agents hand Thus cases uncovered grasped goal In task effects block reach unstack blocks necessary manipulating actions completely deterministic receives fixed positive If robot achieves time limit expires trials agents sensorymotor What differentiates tasks task Block World problems ment learning equipped deictic sensorymotor ability time provides complete objective description object provides implemented reinforce Instead assuming sensory scene controller scene flexibly access limited information selective perception 1 _ In deictic sensorymotor h Note methods differ supervised feature selection methods 351 rely presentation preclassified embedded samples The algorithms presented operate explicit supervision context reinforcement learning task 284 SD Whiiehead LJ LinArtcial Intelligence 73 1995 271306 Sensory Inputs Internal Motor Commands Peripher iaaturas redinscene greeninscene blueinscene objectinhand actionframecolor 00 red 01 green actionframeshapeO block 1 table actionframestackheight 00OOll Local features _ attnframer 00 red 01 green attnframestackhafght 00 0Ol 1 attnframetabbbaiow Action Frame Commands moveactbnframetogreen moveaotiinframeto4ue moveactbnframetoattop eaoeonframetostWkottom eMrametotable m mweattnframetorad moveattnframetogreen mweattntrametoUue mweattnframetostaoktcq moventiSxttom mweattnframetetat4e Relational properties d attnframeinhand framesverticallyaligned frameshotizontallyaligned Fig 5 A specification deictic sensorymotor Meliora11 Whitehead 60 The markers actionframe marker attentionframe marker The 20bit input vector 8 overt actions 6 perceptual actions The values registered input vector effects internal action commands depend bindings markers sensorymotor objects environment information internal perception focus attention input vector point In practice action On environment representation targets overt manipulation A specification marker corresponds frames reference view select agent 1521 Conceptually markers markers establish sensory placing marker object object marker placement sensorymotor markers sensory generates 20bit bits represent local markerspecific markers bound object Other bits detect relational properties horizontal presence absence red scene By moving markers agent multiplex brings On motor Fig 5 This employs given actionframe marker attentionframe marker respectively On time Most color shape vertical object small input register supported attentionframe marker Both index objects topofstack sensorymotor related groups contain commands primitive The actionframe marker additional commands The graspobjectatactionframe object marked actionframe marker Similarly controlling marker placement These actions alignment detect spatially nonspecific blocks grasp possible Fig 5 internal motor commands actionframe marker related color spatial relationship manipulating properties object wide range information These commands placeobjectataction Listed righthand relatively groups partitioned information command features causes SD Whitehead LJ LinArtijcial Intelligence 73 1995 271306 285 World State 1 World State 2 Internal Rep 11100000000100000000 Fig 6 An example perceptual abasing blockstacking domain In case world states different utilities optimal actions generate internal representation The indicates location attentionframe marker location actionframe frame command causes place held object location marked actionframe The decision problem facing agents embedded controller nonMarkov improper placement systems markers fails multiplex relevant information agents internal representation This point illustrated Fig 6 shows different external world states corresponding different state Markov model task improper placement markers generate internal representation 412 Control To tackle nonMarkov decision problem Lion algorithm adopts approach attempts select overt manipulative actions based actionvalues internal states unaliased Markov To accomplish Lion algorithm breaks control stages At beginning control cycle perceptual stage performed During perceptual stage sequence commands moving attentionframe marker executed These socalled perceptual actions cause sequence input vectors appear input register These values temporarily buffered short term memory Since perceptual actions change state external environment buffered input corresponds representation current external state If perceptual actions selected care internal states Markov encode information relevant determining optimal action Once perceptual stage completed overt stage begins During overt stage action changing state external environment selected These called overt actions correspond commands actionframe marker To guide Notice moving actionframe marker object changes state external environment changes set objects effected grasp place commands 286 SD Whitehead LJ LinArtial Intelligence 73 I 995 271306 l World states 7 Internal states Perceptual actions Overt action Consistent internal states large nodes correspond Fig 7 A graphical depiction Lion algorithm The large super graph depicts large node depict perceptual current world state arcs corresponding world states arcs correspond cycles nodes corresponding perceptual actions internal overt control cycle overt actions The subgraphs embedded representations Lion algorithm maintains special actionvalue overtaction pairs This overt actionvalue perceptually aliased states suppressed equal zero actionvalues actionvalue defined internalstate actionvalues selection overt action special ideally nominal values Given buffered state choosing Since aliased states tend suppressed correspond optimal action twostage control cycle graphically internal unaliased internal function function unaliased states allowed Lion algorithm actionvalues actionvalue function selected action state Fig 7 illustrates tends action maximum actionvalues overt stage selects overt action simply examining 413 Learning A special learning function The learning rule learn overt actionvalue procedure operates follows First internal state maximal actionvalue identified Lion The actionvalue rule onestep Qlearning Lion state update This changes tested zero standard rule remaining buffered states state learned aliased states cease Finally buffered state state updated according Eq 9 Next error term updating actionvalues accurate actionvalue aliased actionvalues unaliased actionvalue tests positive If state reset sign error A simple procedure identify potentially term onestep Qlearning examines difference constructed task deterministic overestimate hand positive error Therefore aliased states detected monitoring aliased states The rule simply sign estimate aliased states tend regularly states sign states current actionvalue If actionvalues actionvalues tend monotonically actionvalue initially negative error Unaliased rewards nonnegative onestep delay optimal actionvalue set zero rule approach SD Whitehead LJ LinArtcial Intelligence 73 1995 271306 287 estimation error The learning rule perceptual stage simpler For perceptual control function estimated internalstate perceptualaction maximizes actionvalue perceptual perceptual pairs During action The perceptual standard onestep Qlearning accounted actions perceptual See actionvalue stage actions selected choosing input bit vector perceptual current updated actionvalue function internal stage perceptual state state Since aliased lead unaliased rule states overt utility internal tend suppressed overt actionvalues internal states tend higher actionvalues 573 details 414 Discussion The Lion algorithm able learn learns perceptual block learns overt control policy unstack covering blocks Detailed experimental control strategy focuses moves block manipulation task described It attentionframe marker green actionframe marker needed results 571 deal nonMarkov exploits The Lion algorithm decision problems These assumptions 1 The effects actions deterministic 2 Only nonnegative 3 For external state exist configuration rewards allowed limiting assumptions order sensory generates internal state unaliased 42 CSQL cost 2351 Tan recognized cost sensing learning work machine fails account explicitly account algorithms There information necessary sensitive CSQL stands CostSensitive QLearning costsensitive agent efficient procedure learning reinforcement overt actions needed classification learning learns identifying CSQL Lion algorithm focuses tasks CSID3 predictive power efficient cost respectively ideas learning In CSQL perform task learns In 481 develops CSIBL resulted combined 47 learning decomposed twostage process sensing QL control action different model specific piece information perceptual control policy Lion algorithm CSQL constructs classification sensing model CSQL sensorymotor agent set primitive external environment Also instead learning tree basic control cycle That CS control considerably CSQL adopts sensing overt control However Instead deictic test provides tests Each perceptual sensing current state environment share 8 Subtle interactions suppression actionvalues This tend bounce suppressions leads eventually stabilize For detailed discussion technique detecting aliased states 571 1481 cause unaliased states However states states unaliased overestimate 288 SD Whitehead LJ LinArtijicial Intelligence 73 1995 271306 internal node corresponds test result leaf corresponds example tree leaf tree unaliased state Markov model task Fig 8 In CSQL sensing operation state agents internal branch corresponds representation agent learned adequate classification leaf represents unique incrementally tree learned The classification tree consists single root node In words entire external state space collapsed single internal internal nodes state As aliased introduce new attaching distinctions achieved The tree expanded Markov representation The new leaf nodes result representations leaves detected expanded sensing operations converted Initially attach inexpensive remain When expanding tests tends explore efficient accounts cost discriminatory algorithm efficient classification CSQL uses technique limitations node CSQL simply selects expensive sensing operation lowcost target leaf This heuristic generate test G trees result To detect aliased leafs employed Lion algorithm Thus CSQL shares power sensing trees By incorporating Lion algorithm sophisticated sensing procedures selection method favoring learn deduce simulated CSQL successfully demonstrated position robot sensors One simple selective classification robots sensing operations nearby cells maze The cost sensing cell proportional robot By accumulating features efficiently allow detect properties tree learned CSQL position robot navigation information task gathered task shown Fig 8 In example barrier cup distance nearby cells robot successfully instance type navigation maze identify limited 43 The Galgorithm Instead 131 tried minimize The Galgorithm apply Qlearning Galgorithm developed context general videogame domain technique developed Lion algorithm CSQL address kind adaptive development cost sensing need mitigate In particular Chap ception task However unlike specifically motivated desire control active sensory problems caused availability information man Kaelbling subtask formation generated sensors The subtask agent target orienting agents sensory generated 100 bits input Using information internal relevant introducing bits specifically time resulted 2O states Most bits ir interfere learning representation On hand known ahead time called Amazon sheer volume involved aligning firing weapon At point learn simple alignandshoot overwhelmed studying relevant necessarily specific subtask state space containing studying internal unnecessary distinctions learning SD Whitehead LJ LinArticial Intelligence 73 1995 271306 289 b CUP 4T upstate t1 0 3 Cl q obstacle 4 1 7 2 6 Fig 8 A simple example CSQL 3 x 3 grid world b learned mapping state descrip tions states c learned optimal decision policy d learned costsensitive classification tree Reproduced permission stages game bits irrelevant vitally important The Galgorithm developed learn control policy generalize irrelevant information input The Galgorithm works identifying bits input vector important control It similar CSQL grow classification trees incrementally That start single root node assuming information relevant construct treestructured classification circuit recursively splitting nodes based values sensory inputs In CSQL information split nodes tree corresponds results sensing acts tests Galgorithm nodes split based values bits input As CSQL leaves Galgorithms tree define agents internal state space Unlike CSQL Galgorithm associate cost sensingreading bit What sets Galgorithm apart CSQL Lion algorithm internal states CSQL Lion method uses detect nonMarkov aliased algorithm monitor sign estimation error detect nonMarkov states method limited deterministic tasks The Galgorithm uses general statistical test In general leaf classification tree nonMarkov shown bits input vector tested traversing tree roottoleaf statistically relevant predicting future rewards To detect leaf nonMarkov Galgorithm uses Students Ttest 1421 statistically significant bits That time agent experiences 290 SD Whiteheud LJ LinArtijiciul Intelligence 73 1995 271306 estimate leaf separated threshold distinct distributions relevant leaf split bins One bin corresponds insight provided consequently states reward data collected stored leaf targetbit sets variety external pair Data given situations target bit bit Given data Students Ttest determine probable gave rise If sufficient sampling deemed The relevance Ttest assumes clearly case general reward distributions problem mitigated comparing distributions Galgorithm A bits relevance apparent required limitations domains test bit limited reward distributions compared Gaussian This arbitrary However rewards Also higher order pairings isolation Finally additional memory sensing stochastic gather statistics Galgorithm detect nonMarkov states The specific algorithm use statistical methods detect bits relevant pay method minor price testing Nevertheless guaranteed difficulties cumulative relevance extends bit The Galgorithm successfully demonstrated orientandshoot task It significantly neural network See 131 details discussion difficulties encounter outperform alternative approach error backpropagation 44 The Consistent Representation Method While algorithms described vary considerably basic approach We refer common tation share framework Consistent Represen stages perceptual stage 1 2 3 4 5 representation partitioned time step control overt stage stage aims generate internal CR Method 9 The key features CRmethod At followed action The perceptual The action stage aims generate optimal overt actions Learning occurs control stages For action stage traditional ment learning internal perceptual resentation modified It assumed sensory stage perceptual nonMarkov external state identified state space This constraint states When These techniques stage constantly monitors eliminate techniques inputs reinforce impose Markov constraint turn drives adaptation internal perceptual process rep Markov immediate Markov The term Consistent Representation derived internal state space Fig 8 different external predicting states classified sub optimal policy This slightly weaker concept partially Markov Markov respect lo reward associated term consistent See 57 1 futther discussion distinction absolutely necessary state Markov respect state state 2 This misclassification future states For instance result necessarily sufficient fact In particular rewards future SD Whitehead LJ LinArtijicial Intelligence 73 1995 271306 291 Motor System f The Agent Fig 9 The basic architecture Consistent Representation Method Control accomplished stages perceptual stage followed overt stage The goal perceptual stage generate Markov taskdependent internal state space The goal overt controJ maximize future discounted reward Both control stages adaptive Standard reinforcement Learning algorithms overt learning perceptual learning driven feedback generated representation analysis module monitors internal state space nonMarkov states Fig 9 illustrates architectural ponents include overt control trol selective The line perceptual ceived tion monitor detects nonMarkov trol sensory overt control overt controllers overt controller selective motor representation monitor The sensory embodiment line perceptual CRmethod The major com control acts perceptuai selection represents overt acts Both control environment represents perceptual motor adaptive Reward representation monitor The representa perceptual feedback states provides The correspondence components architecture pre follows The Lion algorithm assumes deictic sensorymotor includes assumes assumes commands sensorymotor vious algorithms CSQL ing acts Galgorithm bits selected relevant The identification tual control takes form binary classification internal representation input bit vectors classification learning algorithm form perceptual tree The Lion CSQL use overestimation CSQL generated moving perceptual interface consists set discrete attentional markers sens individual percep procedure implemented policy Lion algorithm binary input vector tree CSQL Galgorithm The taskspecific Lion algorithms Galgorithm Galgorithm subset corresponds defined leaves alI use form Q Lion CSQL relies technique Galgorithm overt control For representation monitoring 292 SD Whitehead LJ LinArtiial Intelligence 73 1995 271306 general statistical method Relating Lion CSQL Galgorithm common statistical methods Galgorithm limitations yield algorithms Lion CSQL useful reasons First promotes crossfertilization structure provided CRmethod highlights extensions method specific algorithms For instance incorporated domains Second tions fundamental identified point techniques track information These general hidden state tasks storedstate subject section suggests assumption algorithms time immediate interesting reason perceptually approaches function overcome inappropriate sensor external states inputs This assumption makes tasks require memory framework CR ideas stochastic shared assump In particular inaccessible 5 Storedstate methods One obvious approach dealing inadequate perception nonMarkov decision allow agent memory past This memory help problems agent identify hidden states use differences based immediate perception situations huge volume information remember encode problem sliding window history predictive model environmental approaches control policy directly approach observables discussed reinforcement literature memory appear identical The problem traces distinguish given available past agent decide agent keeps use There approaches In approach agent builds statedependent section describes approach learns historysensitive 414283950 In addition 51 Three storedstate architectures Fig 10 depicts Markov domains trained temporal difference methods Qfunction storedstate In experiments neural network reinforcement architectures learning incrementally learn actionvalue non Qnet function In windowQ architecture instead relying immediate define sensations sensations tions represent access information size The windowQ current internal representation N recent state In words inputs sensory sensa agent uses immediate time steps N recent actions allows direct window use windowQ architecture past sliding window N called simple straightforward However architecture I0 A version Lion algorithm developed feedback detect nonMarkov 57 I learning states This external supervision dramatically external supervisor improves perceptual overt SD Whitehead LI LinArrificial Intelligence 73 1995 271306 293 actionvalue aXlValue current sensation recent N sensations recent N actions sensation action contextual features Fig 10 Three storedstate b recurrentQ c recurrentmodel architectures reinforcement leaming nonMarkov domains windowQ architecture choose window size difficult advance On hand selected window size small internal representation sufficient define state space Markov On hand input problem arise window size chosen large generalization window necessarily large capture relevant information sparsely distributed time Under circumstances excessive amounts training required neural network accurately learn actionvalue function generalize irrelevant inputs In spite problems windowQ architec ture worthy study 1 kind timedelay neural network useful speech recognition tasks 53 2 architecture establish baseline comparing methods The windowQ architecture sort bruteforce approach memory An alternative distill small set contextual feutures large volume information past This historical context agents current sensory inputs define internal representation If contextual features constructed correctly resultant internal state space Markov standard RL methods learn optimal control policy The recurrent Q recurrentmodel architectures illustrated Fig 10 based basic idea However differ way construct contextual features Unlike windowQ architecture architectures principle discover utilize historical information depends sensations arbitrarily deep past practice difficult achieve Recurrent neural networks Elman networks 171 provide approach constructing relevant contextual features As illustrated Fig 11 input units Elman network divided groups immediate sensory input units context units The context units encode compressed representation relevant information past Since units function kind memory 294 SD Whiteheud LJ LinArtcia Intelligence 73 1995 271306 Fig I I An Elman network encode aggregate previous network states output network depends past current The recurrentQ uses recurrent network inputs architecture estimate Fig 10b To predict actionvalues recurrent Qnet learn contextual correctly features enable different external states generate immediate actionvalue recurrent network sensory network function directly called distinguish inputs architecture learning Fig 10c responsible action Because The recurrentmodel consists concurrent predict immediate sensory performing code state external environment features learning onestep prediction module action model Qlearning module inputs immediate model effects action accurate predictive model learned Markov state space features This completely components The prediction module payoffs result inputs completely learn use contextual environment models contextual generated Qlearning conjunction follows given determined new state representation recurrentmodel leastmeansquare method features extracted component defining inputs internal input contextual action taken architectures state environment error backpropagation agents immediate accurately predict Both recurrentQ learn contextual If assume state space agents sensory time important way In learning predictive model goal gradient descent differ errors actual predicted environment time long environment minimize Eq In 9 environment value function Since information weak noisy inconsistent recurrentQ errors architecture case uncertain provides needed training information sensory inputs immediate change For recurrent Qlearning temporally successive predictions actionvalues error signals computed based partly information partly agents current estimate optimal action little useful term changes time carries error signals general time Because practical viability early stages learning features minimize rewards In case consistent goal Having architectures possible For example combine introduced approaches recurrent Qnet include current sensory architectures input recent architectures For instance approach share context units inputs recent actions We combine combinations worthwhile inputs note SD Whitehead LJ LinArtcial Intelligence 73 1995 271306 295 2 possible initial states W c 3 aXions walk left walk right pick 4 blnaty inputs left cup right cup left collision right collision reward 1 cup picked 0 Fig 12 Task 1 twocup collection task The cup locations denoted filled circles model network Qnetwork based prediction article needed versions concerned errors networks Although contextual features learned basic architectures Further possibilities investigation basic combinations result better performance 52 Network training The nonrecurrent Qnets windowQ recurrentmodel temporal difference methods architectures 441 37 1 This combination successfully trained straightforward connectionist backpropagation solve nontrivial applied combination algorithm Training model recurrentmodel architecture reinforcement learning problems 242549 slightly complicated Recurrent networks trained recurrent version backpropagation algo rithm called backpropagation time BFTT unfolding time 371 BFTI recurrent network spanning T steps converted based observation network T times Once equivalent current network directly applied The Qnet recurrentQ For detailed network structures implementation trained BFTT temporal difference feedforward network duplicating unfolded backpropagation architecture 281 53 Simulation results This subsection presents simulation results study historybased architectures applied gained insight relative merits conditions descriptions series nonMarkov decision behavior architectures simulation results 281 study tasks Through better understanding Detailed useful application 53 I Task 1 twocup collection We begin simple twocup collection pick cups located learning agent walking pickup action current cell The agents sensory right cell walking pick cup input includes task requires task Fig 12 This 1D space The agent actions cup located agents binary bits bits indicating agent executes left cell pickup When 296 SD Whitehead LJ LinArtificial Intelligence 73 1995 271306 cup immediate action results agent space cause collision restricted The cup collection collision left right cell bits indicating left right An action attempting previous problem possible initial states Fig 12 In trial agent starts initial states Because trial location agent cups beginning cup learned previous cups optimally agent use contextual decide way picking cup Note reason restricting possible contextual This initial states avoid perceptual aliasing onset trial initial state starts information task nontrivial 1 The agent trials To collect available information reasons sense current cell 2 operates cup sight especially picking cup architectures tested cup collection gets reward cups picked 3 The memory iment repeated 5 times optimal control policy 500 trials observation following The recurrentmodel perfect model 500 trials For instance steps model normally imperfect model prevent Qlearning able predict The main results experiment time architecture The window size N 5 One interesting architecture learned successfully task The exper learned agent seen cup 10 appearance cup But learning optimal policy cup l To demonstrate architectures simple hidden state task particular architectures able develop use memorybased capable learning demonstrate recurrent perform l To notice recurrentmodel architecture partially correct This good news perfect model provide sufficient contextual difficult obtain features contextual model features optimal control 532 Task 2 Task I random features Task 2 simply Task 1 random bits agents sensory difficulttopredict irrelevant features accessible features difficult input The agent In real world random bits simulate learning predict fortunately pick cups inside The ability important learning relevant going rain outside difficult matter task solved For example predicting task features handle difficulttopredict irrelevant practical The simulation results summarized windowQ architecture recurrentQ follows The random features little architecture impact perfotmance noticeable negative The formance optimal policy policies impact recurrentmodel architecture recurrentmodel architecture course 300 trials However streaks optimal stabilize optimal policy suboptimal exhibited apparently oscillated It observed model tried vain reduce prediction errors SD Whitehead LJ LinArtijcial Intelligence 73 1995 271306 297 fail learn contextual poorer performance random bits There possible explanations compared obtained random sensation bits First model task features needed effort wasted trying learn predict model network Qnet activations context units shared features model simply change destabilize times To test second ruled explanation fixed model point learning allowed changes Qnet solve random bits Second change significant The explanation welltrained Qnet representation optimal policy agent optimal policy stuck lessons In setup revealed contextual architecture economic try learn contextual recurrentmodel feature architecture This experiment l The recurrentQ sense appear relevant predicting actionvalues l A potential problem recurrentmodel contextual features model cause instability changes architecture representation Qnet 533 Task 3 Task 1 control errors prevail Noise uncertainty real world To study handle noise added 15 control errors agents actuators architectures 15 time executed action effect environment The random bits removed capability In runs windowQ architecture successfully policy recurrentQ architecture runs suboptimal optimal policy little instability The recurrentmodel learned architecture optimal In contrast policies optimal suboptimal optimal policy 500 trials ones Task 2 If features happened contextual rate 0 end able model example gradually decreasing obtain stable optimal policy oscillated representation changing way stablize learning policy Two lessons learned l All architectures l Among errors architectures experiment handle small control errors degree recurrentQ scale best presence control 534 Task 4 pole balancing In pole balancing problem cart order systems objective movable Fig 13 This problem studied widely reinforcement resemblance It practical problems missile guidance biped balance locomotion robotics difficult credit assignment base cart hinge literature It theoretical problem arises sparse learning aerospace balance pole apply forces attached Fig 13 The pole balancing problem inputs completely signals In particular task systems sensory In traditional pole balancing formulations problem characterizes In experiments reinforcement receives nonzero simulations reinforcement pole falls For instance receives penalty 1 pole tilt exceeds 12 degrees vertical position velocity cart angular position velocity pole 431 This state yields control problem information Markov pole angle given decision problem order learn adequate control This yields nonMarkov cart policy pole In experiment pole balanced test trials pole f2 3 degrees In training phase pole angles starts angle 0 fl randomly The initial cart velocity pole velocity cart positions generated set 0 N 1 5000 steps seven construct policy considered satisfactory features resembling cart position contextual velocities include The input representation straightforward pole angles cart positions The following trials taken architecture satisfactory policy learned These numbers average results A satisfactory policy 1000 trials best runs realvalued table shows input unit number method window0 recurrentQ recurrentmodel trials 20h 552 247 While collection task recurrentQ tasks outperformed architectures architecture suitable architecture cup pole balancing 54 Comparisons The experiments provide insight performance memory architectures This subsection determining architecture preferred considers problem characteristics useful SD Whitehead LJ LinArtiJcial Intelligence 73 1995 271306 299 541 Problem parameters Some features parameters architectures task affect applicability l Payodelay l Memory depth One important problem parameter length time represen task important In cases payoff order generate internal inputs memory depth pole balancing agent remember previous tation Markov For example 1 evidenced fact windowQ agent able obtain satisfactory optimal policy control based window size 1 Note policy For require larger memory depth needed stance Task 1 cup collection optimal policy based window size small 2 occasionally learn learn optimal control window size 5 reliably zero goal state define leading overall difficulty learning accurate Qfunction payoff delay problem goal This parameter Qlearning As payoff delay increases increasingly length optimal action sequence windowQ agent able represent learning represent difficult increasing difficulty credit assignment learned In general predicting actionvalues features perceptual aliasing agent discover agent faces contextual requires difficult task Qnet turn features contextual representing optimal policies Consider Task requires contextual 1 example Only optimal actions cup second cup righthand requires features lefthand But perfect Qfunction cups picked far far second cup A perfect model features perfect Q function But perfect model Task 2 requires features current state random number generator perfect Qfunction Task 2 requires extra features binary contextual task requires In general predicting features required model determine influences sensations features l Number contextualfeatures note It important order actions relative values correct order Similarly need perfect Qfunction needs perfect assign obtain optimal policy A Qfunction model values model needs provide sufficient features constructing good Qfunction 542 Architecture characteristics problem parameters like understand key types problems Here consider particular architecture problem parameters Given architectures advantages influence l Recurrentmodel best suited disadvantages architecture The key difference driven learn recurrentQ ing action model agent obtain better training data action model Qfunction One strength approach learning contextual architecture architecture features importance characteristics 300 SD Whiteheud LJ LinArtijicial Intelligence 73 1995 271306 learning reliable effective Qfunction making examples action model ples directly observable step agent In contrast training examples Qfunction triples directly observable values based changing approximation sensation sensation agent estimate action nextsensation takes In particular training payoff quadru environment action actionvalue true actionvalue training action function features dependent action reward functions goals predict rewards sensations As result agent different The second strength approach learned independent reward function environment model trained features reused learn achieve suffers training examples relative disadvantage offsetting relevant features represent optimal action optimal Qfunction This l RecurrentQ architecture While architecture learn contextual features needed represent need learn indirectly observable advantage control problem The contextual model superset needed easily seen noticing action model features predict completely learned recurrentQ needed recurrentmodel architecture architecture necessary look ahead search Thus optimal control action principle computed cases needed number state number contextual smaller predicting actionvalues features recursively architectures l WindowQ architecture The primary advantage architecture networks networks This advantage learn state representation network nonrecurrent history fixed window captures bounded history In contrast network approaches sensations represent contextual arbitrarily deep agents history longer typically offset disadvantage features directly observable recurrent train recurrent depend use limited principle information Recurrent features Given competing advantages preferred architecture l One expect advantage windowQ tasks memory depths smallest task l One expect recurrentmodel different architectures imagine types problems architecture example greatest pole balancing important example pole balancing architectures advantage directly available tasks payoff delay It situations task training Qvalues problematic recurrentQ l One expect advantage recurrentQ relevant controlto pronounced relevant irrelevant contextual task random features features Although architecturethat need tasks lowest example recurrentmodel acquire optimal policy long relevant features training examples longest indirect estimation architecture learn features ratio cup collection architecture SD Whitehead LJ LitArtificial Intelligence 73 1995 271306 301 learned drive learning irrelevant features cause problems First representing irrelevant features use limited context units sacrifice learning good relevant features Secondly seen experiments recurrentmodel architecture subject instability change improves changing representation contextual featuresa model likely deteriorate Qfunction needs relearned The tappeddelay line scheme windowQ architecture uses widely applied speech recognition 53 turned useful technique However expect work control tasks speech recognition important difference tasks A major task speech recognition temporal patterns exist given sequence speech phonemes Whereas reinforcement learning agent look temporal patterns generated actions If actions generated randomly case early learning unlikely sensible temporal patterns action sequence improve action selection policy On hand improve performance windowQ architecture The TDNN sophisticated timedelay neural networks TDNN primitive fixed number delays input layer We delays hidden layer 20531 Bodenhausen Waibel 111 TDNN adaptive time delays Using TDNN windowQ able determine proper window size automatically 6 Discussion This paper described learning techniques developed handle tasks involve selective perception state hidden time However tasks fact involve selective perception memory solutions tasks require integration Consistent Representation Methods storedstate methods One simple approach extending CRmethod extend agents selective sensory include remembered sensorymotor events That instead selecting bits information current sensory input select bits memory trace previous inputs actions This approach similar windowQ architecture memory trace maintained differs relatively small information selected point time Moreover scheme possible devise referencebased rules way preserve relevant memories updating historytrace dropping irrelevant ones Other architectures combine features CRmethod historybased architectures useful For example problem CRmethod currently stands uses information previous state environment trying identify current state In sense reidentifies state environment starting scratch action Knowledge state recent action considerably reduce effort required 302 SD Whiteheutl IJ LirzArfjficd Inrellipnce 73 1995 271306 current state environments identify local predictable Thus instead rediscovering agent merely verify limited number possibilities transitions states tend state action outcome current state worst case identify In addition exploring variations architectures assess scalability extend desire problems involve selective perception andor successful However compared scale problems A issues future work algorithms These algorithms derived time To extent simple remain painfully intelligent behavior learning Markov decision problems state hidden tasks explored required addressed truly autonomous achieve scalability reinforcement include l Learning bias Reinforcement proceed quickly learning viewed kind search If search biased appropriate learning One introducing bias learning agent allow interact tasks Other agents serve role models general strongly bias space possible control policies direction approach intelligent advice givers agents context reinforcement time learning learning Simple versions methods demonstrated 152561 Nevertheless work needed agents performing instructors learning produced significant critics supervisors improvements similar given reward l FastefJicient credit assignment Credit assignment fundamental environment actions learning algorithms solve problem changed payoff learning generating reinforcement responsible improve performance Most reinforcement making knowledge model developed For example 253 I 33344564 incremental long period time If additional example action available efficient credit assignment methods causal structure environment problem changes l Generalization A reinforcement impractical In particular ences optimal control experience similar tions Qfunction lookup tors promote useful generalization 1327293247491 situations state space learning exhaustive agent generalize large experi search agent guess new situations based func tables develop function approxima Instead representing actionvalue states actions For example l Hierarchical reinforcement learning To date work small compared learning facing real robotic systems robot require precise focused problems For example walking dozens sensors need control dozens effecters The combinatorics associated problems quickly overwhelm source complexity learning complex fectively Then control policies integrated tractable ones First tasks solved ef tasks elementary simplest RL methods An arises agents pursue multiple goals Hierarchical problems multiple elementary learned approach task decomposed task Once agent learned solve original complex continuous solving information turning intractable SD Whitehead LJ LinArtcial Intelligence 73 1995 271306 303 solve complex elementary task solving new task easier share subtasks For work direction stand current 2226274062 technology issues intelligent autonomous Of course development However play important role autonomy afforded reinforcement learning methods makes agents reinforcement learning panacea likely 7 Conclusions control systems deal information inadequate Intelligent sensors When agent actively control decision problem difficult information sensors faces necessarily limitations available order select relevant nonMarkov Learning imposed agents sensors internal tasks features control In article presented approaches dealing nonMarkov control current state environment dealing tasks involve controlselection partitioned decision problems The Consistent Representation approach In CRmethod aims identify reward Three aims 601 Galgorithm 131 CSQL uses presented The major assumption CRmethod environment sensory relevant state information point prevents temporarily hidden view CR Method proposed active sensory control phase overt control phase Lion algorithm examples state controlling instances method 47 described time appropriately phases perceptual This assumption applied tasks identified maximize Storedstate methods appropriate tasks information hidden predictive model external environment internal state architectures described windowQ line uses tappeddelay events The recurrentmodel architecture The windowQ time Three storedstate recurrentQ fixed length history recent sensorymotor constructs conjunction sensory uses recurrent neural network task directly Because steps internal input These architectures demonstrated conditions inputs learn useful application discussed drive control The recurrentQ actionvalue recurrent network encode state information state resolve ambiguities function time sensory series hidden state tasks caused inadequate recurrentmodel maintain architecture architecture nonMarkov The methods described relatively complicated demonstrated compared nificant advance traditional nonMarkov stones lems hidden state sophisticated tasks Perhaps article preliminary simple domains Nevertheless reinforcement tasks extensively tested represent sig learning algorithms address modest algorithms serve stepping dealing ubiquitous prob capable methods algorithms 304 SD Whiteheud LJ LmArtifciul Intelligence 73 1995 271306 Acknowledgements acknowledge We gratefully article Dana Ballard contributions Tom Mitchell Rich Sutton Lonnie Chrisman Ming Tan Sebastian Thrun Rich Caruana Thank sharing thoughts ideas comments criticisms time energy References I 1 PE Agrc The dynamic structure everyday life PhD Thesis Tech Report No 1085 MIT Artificial intelligence Lab Cambridge MA 1988 12 1 DW Aha D Kibler Noisetolerant instancebased learning algorithms Proceedings IJCAI89 Detroit MI 1989 794799 13 1 J Aloimonons 333356 1 Weiss A Bandyopadhyay Active vision fnt I Cornput Vision 1 4 1988 14 1 JR Bachrach Connectionist modeling control finite state environments PhD Thesis University Massachusetts Department Computer Information Sciences Amherst MA 1992 I 5 1 R Bajcsy P Allen Sensing strategies Proceedings USFrance Robotics Workshop Philadelphia PA 1984 161 DH Ballard Animate vision Technical Report 329 Department Computer Science University Rockester Rochester NY 1990 17 1 AB Barto SJ Bradtke SP Singh Realtime learning control asynchronous dynamic programming Technical Report 9 l57 University Massachusetts Amherst MA 1991 18 1 AG Barto RS Sutton CW Anderson Neuronlike elements solve difficult learning control problems IEEE Truns Syst Man Cybern 13 5 1983 834846 I 9 I RE Bellman Dynamic Programming I IO I DP Bertsekas Dynamic Progrummin Deterministic Princeton University Press Princeton NJ 1957 Stochastic Models PrenticeHall Englewood Cliffs NJ 1987 I I I I U Bodenhausen A Waibel The Tempo 2 algorithm adjusting DS Touretzky ed Advances Mateo CA 1991 Neural Inrmafion timedelays supervised learning Processing Systems 3 Morgan Kaufmann San I I2 1 D Chapman Vision I I3 I D Chapman Proceedings LP Kaelbling Learning IJCAI91 Sydney Australia Instntction Action I 14 I L Chrisman Reinforcement learning perceptual MIT Press Cambridge MA 1993 delayed reinforcement 199 I Teleos Technical Report TR90l predictive distinctions aliasing complex domain I 1990 approach Proceedings AAAI92 San Jose CA 1992 183l 88 I IS I J Clouse PE Utgoff A teaching method reinforcement Conference Machine Learning Aberdeen Scotland International learning Proceedings Ninth 1992 I6 I P Dayan G Hinton Feudal reinforcement JE Moody SJ Hanson RP Lippmann NeuruL fnfirmation Processing Sysrems 5 Morgan Kaufmann San Mateo CA 1993 learning eds Advances I I7 1 JL Elman Finding structure 1 I8 I JJ Grefenstette Credit assignment 3 1988 225245 1 19 1 JH Holland Escaping brittleness rulebased parallel Kaufmann San Mateo CA 1986 systems time Cogn Sci 14 1990 17921 I rule discovery systems based genetic algorithms Much Learn possibilities generalpurpose learning algortihms applied Muchine Lenrnin An Artificial Intelligence Approach II Morgan I20 I AN Jain A connectionist Report CMUCS9 12 I I LP Kaelbling Learning I22 I LP Kaelbling Hierarchical learning architecture parsing spoken language PhD Thesis Technical I208 Carnegie Mellon University School Computer Science 199 1 embedded learning systems PhD Thesis Stanford University Stanford CA 1990 Proceedings Tenth stochastic domains preliminary results bternutional Corzference Muchine Lxcwning 1993 SD Whitehead LJ LinArtcial Intelligence 73 1995 271306 305 I231 S Koenig R Simmons Complexity analysis realtime reinforcement learning applied finding shortest paths deterministic domains Technical Report CMUCS93106 School Computer Science Carnegie Mellon University Pittsburgh PA 1992 241 LongJi Lin Programming robots reinforcement learning teaching Proceedings AAAI91 Anaheim CA 1991 781786 25 LongJi Lin Selfimproving reactive agents based reinforcement learning planning teaching Mach Learn 8 1992 29332 I 261 LongJi Lin Hierarchical learning robot skills reinforcement Proceedings 1993 IEEE International Conference Neural Networks 1993 271 LongJi Lin Reinforcement learning robots neural networks PhD Thesis Technical Report CMUCS93103 Carnegie Mellon University School Computer Science Pittsburgh PA 1993 28 LongJi Lin TM Mitchell Reinforcement learning hidden states Proceedings Second International Conference Simulation Adaptive Behavior From Animals Animats MIT Press Cambridge MA 1993 I 29 1 S Mahadevan JH Connell Scaling reinforcement learning robotics exploiting subsumption architecture Proceedings Eighth International Workshop Machine Learning Evanston IL 1991 301 D Michie RA Chambers Boxes model patternformation CH Waddington ed Toward Theoretical Biology 1 Prolegomena Edinburgh University Press Edinburgh 1968 206215 31 TM Mitchell SB Thrun Explanationbased neural network learning robot control JE Moody SJ Hanson RF Lippmann eds Advances Neural Information Processing Systems 5 Morgan Kaufmann San Mateo CA 1993 132 A Moore Variable resolution dynamic programming efficiently learning action maps multivariate Proceedings Eighth International Conference Machine Learning realvalues state spaces Evanston IL 1991 333337 1331 AW Moore CG Atkeson Prioritized sweeping reinforcement learning data real time Mach Learn 13 1 1993 103130 1341 Jing Peng RJ Williams Efficient learning planning Dyna framework Proceedings Second International Conference Simulation Adaptive Behavior From Animals Animats MIT Press Cambridge MA 1993 351 JR Quinlan Induction decision trees Mach Learn 1 1986 81106 361 S Ross Introduction Stochastic Dynamic Programming Academic Press New York 1983 371 DE Rumelhart GE Hinton RJ Williams Learning internal representations error propagation DE Rumelhart JL McClelland PDP Research Group eds Parallel Distributed Processing Explorations Microstructure Cognition 1 MIT Press Cambridge MA 1986 Chapter 8 38 1 AL Samuel Some studies machine learning game checkers EA Feigenbaum J Feldman eds Computers Thought Krieger Malabar FL 1963 71105 39 J Schmidhuber Reinforcement DS Touretzky ed Advances Neural Information Processing Systems 3 Morgan Kaufmann San Mateo CA 1991 500506 learning Markovian nonMarkovian environments 40 SP Singh Transfer learning compositions sequential tasks Proceedings Eighth International Workshop Machine Learning Evanston IL 1991 348352 1411 SP Singh Transfer learning composing solutions elemental sequential tasks Mach Learn 8 1992 323339 1421 GW Snedecor WG Cochran Statistical Methods Iowa State University Press Ames Iowa 1989 learning PhD Thesis University 1431 RS Sutton Temporal credit assignment reinforcement Massachusetts Amherst 1984 COINS Tech Report 8402 1441 RS Sutton Learning predict method temporal differences Mach Learn 3 1 1988 944 1451 RS Sutton Integrating architectures learning planning reacting based approximating dynamic programming Proceedings Seventh International Conference Machine Learning Austin TX 1990 1461 RS Sutton ed Reinforcement Learning Kluwer Boston MA 1992 I49 1 G Tesauro Practical I50 I SThrun 306 SD Whitrhrtrd LJ la Artirrl Itltulligence 73 1995 271306 147 1 Ming Tan Cost sensitive ICAY Sydney Australia reinforcement 199 I learning adaptive classification control Proceedings 148 1 Ming Tan Cost sensitive robot learning PhD Thesis Carnegie Mellon University Pittsburgh PA 1991 issues temporal differcncc learning Mtrch Ileum 8 1992 2577277 K Moller Planning adaptive world model DS Touretzky ed Adwnces 1n Neural Imtation 5 I 1 S Thrun Efficient exploration Processing wm 3 Morgan Kaufmann San Mateo CA I99 I reinforcement learning Technical Report CMUCS92102 School Computer Science Carnegie Mellon University Pittsburgh PA 1992 1 52 1 S Ullman Visual routines Cqynition 18 1984 97 159 S Pinker ed visuctl Cognition MIT Press Cambridge MA 198s IS3 I A Waibel Modular construction timedelay neural networks speech recognition Neunll Cornput 1 1989 3946 I S4 I CJCH Watkins Learning 1989 delayed rewards PhD Thesis University Cambridge England IS5 I CJCH Watkins P Dayan Technical note Qlearning Mu 1 Sh 1 SD Whitehead Complexity learning cooperation Anaheim CA I99 I similar version appears Proceedings Eighth Mtrchrne LetrmiuK Evanston IS7 1 SD Whitehead Reinforcement IL I99 I learning reinforcement Lam 82 1992 3946 ProcedinRs AAAI91 International Workshop adaptive control perception action PhD Thesis Department Computer Science University Rochester Rochester NY 1991 IS8 I SD Whitehead DH Ballard A role anticipation reactive systems learn Proceedings Sixth Internatiorurl Workshop OH Machirw Lecmin Ithaca NY 1989 IS9 I SD Whitehead DH Ballard Active perception reinforcement learning Neurul Conqf 2 4 1990 Proceedings Semth Interncrtionul Cmzference Machine Leurning Austin TX 1990 1601 SD Whitehead DH Ballard Learning perceive act trial error Mach Lmrrr 7 I I99 I Technical Report 33 I Department Computer Science University Rochester NY 1990 Rochester 16 I I SD Whitehead DH Ballard A study cooperative mechanisms learning Technical Report 365 Computer Science Department University Rochester Rochester NY I99 I faster reinforcement I62 I SD Whitehead J Karlsson J Tenenberg Learning multiple goal behavior task decomposition dynamic policy merging JH Connell S Mahadevan eds Robot Learning MIT Press Cambridge MA 1993 If3 I RJ Williams Reinforcement learning connectionist networks Technical Report ICS 8605 Institute Cognitive Science University California San Diego 1986 1641 RC Yee S Saxena PE Utgoff AG Barto Explaining temporaldifferences create useful concepts evaluating states Prowedings AAAI90 Boston MA 1990