Artiﬁcial Intelligence 140 2002 71106 wwwelseviercomlocateartint Nagging A scalable faulttolerant paradigm distributed search Alberto Maria Segre Sean Forman b Giovanni Resta c Andrew Wildenberg d Department Management Sciences The University Iowa Iowa City IA 52242 USA b Mathematics Computer Science Department Saint Josephs University Philadelphia PA 19131 USA c Istituto di Informatica e Telematica Consiglio Nazionale delle Ricerche I56124 Pisa Italy d Computer Science Department SUNY Stony Brook Stony Brook NY 11794 USA Received 5 April 2001 received revised form 23 January 2002 Abstract This paper describes nagging technique parallelizing search heterogeneous distributed computing environment Nagging exploits speedup anomaly observed parallelizing problems playing multiple reformulations problem portions problem Nagging fault tolerant robust long message latencies In paper nagging parallelize different algorithms drawn artiﬁcial intelligence literature nagging combined partitioning traditional search parallelization strategy We present theoretical analysis advantage nagging respect partitioning empirical results obtained cluster 64 processors demonstrate naggings effectiveness scalability applied A search αβ minimax game tree search DavisPutnam algorithm 2002 Published Elsevier Science BV Keywords Paralleldistributed search algorithms Search pruning Game tree search Branch bound Boolean satisﬁability 1 Introduction Many artiﬁcial intelligence problems practical posed terms search Not surprisingly development robust network infrastructure coupled Corresponding author Email addresses segrecsuiowaedu AM Segre sformansjuedu S Forman restaiitcnrit G Resta awildencssunysbedu A Wildenberg 0004370202 matter 2002 Published Elsevier Science BV PII S 0 0 0 4 3 7 0 2 0 2 0 0 2 2 8 X 72 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 advent reasonablypriced computing equipment helped focus attention techniques use multiple processors operating parallel improve search performance Some work centered developing applicationlevel toolkits access distributed computing environment resourcemanagement tools enable application exploit computing resources span organizational boundaries grid computing 2111522 Aside differences enabling technology attempts parallelize search similar involving sort partitioning 1314 The general idea processing element takes responsibility portion search space individual solutions subproblems compared composed obtain solution original problem Different partitioning schemes usually differ target architecture SIMD vs MIMD shared memory multiprocessors vs networks workstations address number technical problems load balancing best assign work order exploit available processors time fault tolerance notice recover momentary inaccessibility outright loss processing elements Nevertheless partitioning basis diverse set projects including SETIHome search radio signal evidence extraterrestrial life GIMPS search Mersenne prime numbers code breaking efforts distributednet Unfortunately problems partition One reason information acquired early serial search process reduce search performed overall example consider α β values prune game tree αβ minimax search If search partitioned information acquired searching subspace come late help reduce search subspace explored simultaneously resulting search actually slower multiple processors single processor1 This problem instance general speedup anomaly problem ﬁrst studied branchandbound style algorithms wellknown NPhard problems 19 A speedup anomaly occurs solution obtained slowly processors fewer processors alternatively solution obtained superlinearly faster multiple processors Later superlinear speedups routinely studied particularly parallel logic programming theorem proving communities 7 This paper describes distributed search paradigm called nagging exploits speedup anomaly observed parallelizing problems playing multiple reformulations problem portions problem Nagging advantages partitioning techniques intrinsically fault tolerant naturally 1 Even information available timely fashion sharing information multiple processors entail communication overhead In general cost communication tends increase number processors increases Depending underlying architecture sharing information involve interprocessor communication use shared memory For sharedmemory multiprocessors practical design limits number processors incorporate single machine For looselycoupled processors interprocessor communication directed broadcast form requires overhead generating servicing messages furthermore number processors increases higher message latencies associated larger networks generally entail larger communication overheads AM Segre et al Artiﬁcial Intelligence 140 2002 71106 73 loadbalancing requires relatively brief infrequent interprocessor communication robust presence reasonably large message latencies These properties help nagging suitable use geographicallydistributed networks processing elements Originally developed course work distributed automated deduction 3539 40 nagging generalized applied broad range search algorithms artiﬁcial intelligence literature We develop analytical performance model comparing nagging partitioning use model predictions respective performance Finally performance claims justiﬁed empirical evaluation nagging partitioning wellknown signiﬁcantly different search algorithms A search 20 αβ minimax game tree search 27 DavisPutnam search algorithm 6 2 Nagging search Nagging asynchronous parallel search pruning technique single master processor master performing standard search procedure advised nagging processors naggers performing identical search procedures portions masters search space need explored Let consider search procedure designed ﬁnd globallyoptimal solution ﬁnite implicitlydeﬁned search space case example trying determine best game tree search ﬁxed horizon similar arguments hold situations legal solution sufﬁces theorem proving satisﬁability problems At initialization nagger obtains problem speciﬁcation master processor It engages series nagging episodes initiated nagger idle master completed search A nagging episode begins nagger requests snapshot masters current state concisely described communicating sequence choices predeﬁned search space Fig 1 The master selects nagpoint node current path nagger begin exploration space according predeﬁned nagpoint selection criteria communicates nagpoint value describing current best solution nagger The master nagger race exhaust respective search spaces search spaces semantically equivalent searched differently leading different expected solution times If searching optimal solution possible outcomes consider 1 Abort If master backtracks nagpoint nagger completes search master signals nagger abort nagging episode causes nagger idle initiate new nagging episode 2 Prune If nagger completes search ﬁnds better solutions known master nagger interrupts master forces backtrack nagpoint resulting reduction masters search space 74 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 Fig 1 Nagging episode The square node indicates current position master process executing depthﬁrst search A nagpoint selected masters search path described nagging process communicating series choices root search tree nagpoint The nagger reconstructs masters search space nagpoint commences exploring transformed version space rooted nagpoint 3 Solve Finally nagger ﬁnd better solution communicated master abort current search report new value master master use new value reduce search space Of course nagging greatest possible positive effect masters search efﬁciency prefer second cases occur high probability hope ﬁrst case occurs rarely Should second cases occur expect improvement masters expected time solution masters search surely efﬁcient smallbut measurableadditional overhead servicing naggers Two techniques applied improve odds First nagger nagged recursively processor order help exhaust search space quickly Second importantly nagger apply problem transformation function reduce practice size search space retaining information content implicit note effective problem transformation functions typically dependent search algorithm use problem domain We discuss techniques later paper The main characteristics nagging clear simpliﬁed context First nagging require explicit load balancing idle machines initiate nagging episodes busy As long master processor AM Segre et al Artiﬁcial Intelligence 140 2002 71106 75 searching new nagpoints provided Second nagging intrinsically fault tolerant masters search unaffected nagger fail inaccessible losing nagger compromise correctness quality solution produced master master stop wait missing nagger Finally communication brief deep search states described concisely sequence choices results reported nagger reduce single bit prunedont prune bits new value best solution date 3 NICE A Network Infrastructure Combinatorial Exploration To support work nagging developed NICE Network Infrastructure Combinatorial Exploration NICE speciﬁcally designed support nagging partitioning search algorithms allows application programmers parallelize search procedures making appropriate function calls code The NICE distribution includes niced Unix resource management daemon niceq daemon status query program niceapi applications programmers interface library The code written ANSI C BSD sockets TCPIP known run multiple variants Unix operating including Linux Sun OS HPUX The NICE daemon niced running participating machine master nagger Typically started automatically boot process runs long host CPU running daemon single threaded extremely lightweight having noticeable impact performance NICE daemons arranged hierarchically daemon reporting single parent daemon answering zero child daemons The NICE daemon fulﬁlls primary functions 1 The daemon maintains contact NICE hierarchy occasionally exchanging host load availability information parent child daemons managing failure recovery parent andor child daemons unreachable unresponsive Each daemon initiate local reorganization NICE hierarchy greedy attempt enhance overall search performance according hosts current actual load 2 The daemon provides interface qualifying application request additional processors It responsible security certifying applications hosts allowed request support processors executables run local host ﬁles accessed locally 3 The daemon manages local hosts resources according prespeciﬁed hostspeciﬁc constraints example hosts available speciﬁed times nighttime hours Thus daemon decide processor respond requests new processes manage previously spawned running processes putting sleep later waking host available 76 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 A NICEenabled application communicates NICE infrastructure set callable functions contained NICE applications programmers interface This link able library contains functions invoked request new copies application spawned machines It contains functions support communication applications application NICE daemon Note library actually contain code search algorithms handful functions needed parallelizevia nagging partitioningappropriately designed serial search algorithms Both NICE daemon NICE API represent fairly mature software efforts versions NICE daemon running continuously systems years noticeable impact performance The code robust NICE daemons running reliably unobtrusively periods months restarts But point NICE infrastructure represents necessary enabling technology directly supports research distributed search algorithms additional features provided general toolkits PVM 2 MPI 15 interesting parallelization issues algorithmic ones What remarkable NICE simple lightweight infrastructure naturally supports parallelization broad array search algorithms 4 Applications nagging While seen Section 2 main idea underlies nagging simple number important details design appropriate problem transformation functions direct effect nagging works These issues best discussed context speciﬁc search algorithms wellknown search algorithms drawn artiﬁcial intelligence literaturethe Davis Putnam algorithm A search algorithm αβ minimax search algorithmcan parallelized nagging 41 The DavisPutnam algorithm First proposed Davis Putnam later reﬁned Loveland DavisPutnam algorithm fastest known solution technique Boolean satisﬁability problems sound complete2 We complete solution DavisPutnam algorithm guarantees solution eventually Other fast solution methods satisﬁability problems GSAT WSAT simulated annealing local search procedures sound complete 36 Note completeness 2 Recall Boolean variable variable true false A Boolean formula consists variables related usual logical connectors formula conjunctive normal form CNF conjunction clauses clause disjunction literals literal Boolean variable negation By deﬁnition SATCNF problem determining given CNF Boolean formula satisﬁable NPcomplete possible certify solution correct polynomial time commonly believed actually ﬁnding solution requires exponential time 5 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 77 necessarily imply exhaustive search given problem satisﬁable solution equally correct terminate search soon solution The search space searched exhaustively need guarantee unsatisﬁable problem solution This kind searchuntilﬁrstsolution behavior arises automated deduction theorem proving environments precisely contexts nagging ﬁrst proposed The DavisPutnam algorithm solves particular type Boolean satisﬁability problem usually called SAT3CNF simply 3SAT deals Boolean formulas having literals clause note Boolean formula expressed CNF expressed 3CNF direct manipulation addition number new Boolean variables 16 The general idea simple total N variables systematically examine 2N possible combinations truth assignments order determine truth assignments satisﬁes formula Two observations serve reduce number variable combinations Davis Putnam algorithm need look practice First partial solution having m variable bindings inconsistent fails satisfy clauses 2Nm completions partial solution inconsistent safely pruned exist solution portion search space Second partial solution contains negations k 1 literals given kliteral clause hope ﬁnding satisﬁcing solution lone remaining literal clause satisﬁed binding variable appropriately Of course bound newly bound variable force variable bindings effectively reducing search space process called unit propagation Thus DavisPutnam algorithm operates systematically examining combinations truth assignments periods unit propagation occurring possible Fig 2 What expected solution time algorithm In principal larger formulae deﬁne larger search space entail longer solution times In practice exactly time required depends subtle characteristics speciﬁc problem instance ratio number variables number clauses distribution variables clauses Put simply likesized 3SAT problems equally hard 2643 Some problem instances easy imagine example Boolean formula CNF clause shares single literal exactly size result exponentialtime performance In practice order variable settings tried critical effect time solution Numerous splitting rules heuristics good orderings exist universal rule work problem instances existence universal splitting rule providing polynomial time performance 3SAT problem instances imply P NP Parallelizing algorithm Fig 2 partitioning relatively straightforward explore recursive calls search separate processors long additional processors available Of course know priori large individual subspace branches quickly lead inconsistent partial solution This means hard ensure work fairly distributed available processing elements Furthermore notion subspace size difﬁcult determine processor assigned subspace actually failed gone ofﬂine simply taking 78 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 3satF formula boolean returnsearchF extractVarsF searchF formula S variables boolean V variable headS C clause F returntrue elseif C F sizeC 0 returnfalse elseif C F V C V C returnsearchF SV elseif searchpropagatesubstituteF V SV returntrue elseif searchpropagatesubstituteF V SV returntrue returnfalse substituteF formula V variable formula G formula C clause C F ifV C G G CV returnG propagate F formula formula C clause whileC F sizeC 1 F substituteF headC returnF Fig 2 DavisPutnam algorithm 3SAT The problem instance Boolean formula F 3CNF represented list clauses clause list variables variable literal negated literal The search function operates recursively removing satisﬁed clauses F clauses left F clauses F shown unsatisﬁable The splitting rule encoded explicitly ordering pattern negations list S initially contains literals F negated Variables values set early unit propagation skipped clause large conditional statement The substitute function constructs returns G new copy F satisﬁed clauses ﬁltered references negated sense variable V removed remaining clauses The functions head size functions return ﬁrst element cardinality argument respectively function extractVars returns list variables contained given formula long time search turned unexpectedly large space Addressing load balancing fault tolerance issues add overhead costs associated partitioning 32 Compare partitioning strategy nagging Fig 3 At initialization nagger provided original Boolean formula speciﬁes 3SAT problem wish solve A nagging episode begins nagger requests nagpoint master brieﬂy interrupts search select random nagpoint Each nagpoint corresponds partiallyinstantiated Boolean formulae considered master course recursive calls search Upon receiving nagpoint master resumes search nagger ﬁrst applies problem transformation function nagpoint reordering list asyetunbound variables randomly AM Segre et al Artiﬁcial Intelligence 140 2002 71106 79 status true false abort nag3satF formula boolean N formula result status niceInit ifniceRoot returnsearchExplicitF extractVarsF whiletrue N niceIdle result searchExplicitN transformextractVarsN result true niceSolveN result elseif result false nicePruneN Fig 3 Sketch nagging implementation algorithm Fig 2 The niceInit connects NICE infrastructure ensures copies process spawned participating processors ensures spawned processes provided copies original problem The root process function niceRoot returns true root processor performs normal DavisPutnam search nonroot processes engage series nagging episodes initiated niceIdle requests new nagpoint denoted N The function searchExplicit logically equivalent search function Fig 2 explicit stack manipulation interrupt handling capabilities processing messages tofrom parentchild processes convenient primitives support functionality provided NICE API For nagging processors transform function randomly reconﬁgures splitting rule described text Depending outcome naggers search nagger pass solution parent function niceSolve force parent backtrack function nicePrune Note naggers search interrupted parent parent exhausts space rooted naggers nagpoint nagger case searchExplicit immediately return abort nagger simply request new nagpoint inverting logical sense switching order V V explored begins search Should nagger ﬁnd assignment makes formula true interrupts master provides solution master turn provide solution problem Should nagger instead exhaust space ﬁnding solution interrupt master force master backtrack past nagpoint Should master backtrack assigned nagpoint nagger completes search master abort nagger free seek new nagpoint master Of course simply racing nagger master produce useful speedups needed good problem transformation function increase naggers chances beating master subspace deﬁned nagpoint reducing masters search space The insight serial search procedure necessarily commit searching single incarnation current problems search space alternate versions search space entail differing effort search Since expected time solution given problem instance critically dependent splitting rule changing splitting rule lead nagger complete search quickly master To gauge strategy succeed ﬁrst approximation empirically examine random permutation applied variable selection descendent ordering random splitting rule affects overall solution time We randomly 80 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 Fig 4 DavisPutnam algorithm applied 100 randomlygenerated difﬁcult 3SAT problems sizes Each problem solved twice default search ordering tstd second time permutation transformation applied solving trnd Results shown loglog scale clarity Had datapoints tightly clustered diagonal trnd tstd line probability nagging processor beating master low case permutation appears good candidate problem transformation function particular domain generate 100 difﬁcult 3SAT problems varying sizes 37 For demonstration time solution measured recorded twice 3SAT default search ordering tstd second time random permutation transformation applied solving trnd The results shown Fig 4 datapoint graph represents CPU time required solve permuted problem ordinate plotted CPU time default problem abscissa Datapoints appearing diagonal trnd tstd line represent problems solved quickly permutation function applied falling diagonal solved quickly default ordering We observe random splitting rule beats default splitting rule half time signiﬁcant margin note plot uses loglog scale clarity unfortunately somewhat obscures magnitudes differences The datapoints lie tightly clustered diagonal line farther line greater advantage expect nagging problem transformation function3 This demonstration illustrates speedups possible single nagging episode allowed problem selected nagpoint root node search Indeed nagging episode gives 3 Of course example effective problem transformation functions use alternative splitting heuristics elect throw away subset clauses order decrease size search space In case solutions reduced space longer correspond solutions original Boolean formula failure ﬁnd solution smaller space implies possible solution exists original space master forced backtrack AM Segre et al Artiﬁcial Intelligence 140 2002 71106 81 chance beating default splitting rule multiple nagging episodes broad array properly selected nagpoints better chance improving systems overall search performance effect observe experiments Section 6 42 A search Consider wellknown traveling salesperson problem TSP The problem simple state hard solve Given collection N points ﬁnd shortest tour visits point returns original starting point TSP problems lie hidden surprisingly large number problems operations research engineering TSP known NPhard research focused appropriate algorithms use heuristics ﬁnd good lessthanoptimal solutions quickly 33 Yet devising algorithm guaranteed provide globally optimal solution simple willing accept poor worstcase performance Here examine optimal algorithm Euclidean TSP points lie Euclidean space based A search heuristicallyguided branchandbound search strategy4 The obvious solution technique enumerate possible tours return shortest Starting arbitrary 3point tour symmetry reduce space easy N 12 different possible tours We little better showing H points deﬁning convex hull point set appear ﬁxed order optimal tour starting convex hull opposed random 3point tour reduce size search space slightly N 1H 1 In case exhaustive search algorithm operating search space require Sterlings formula ON N time The basic insight required turn simple enumeration algorithm branchand bound algorithm information garnered search reduce combinations examined If cost current partial solution greater shortest solution far exclude completions partial solution searchsince Euclidean space adding points tour longerwithout sacriﬁcing optimality We better incorporating heuristic estimate cost complete partial tour pruning subtrees rooted partial tours partial tour cost plus estimate additional cost required complete tour exceeds cost current best solution If heuristic estimate underestimates true additional cost partial solution heuristic admissible shown resulting A search algorithm 4 Note recommending solution technique TSP problems encountered practice TSP intuitively accessible example parallelization A search Most real applications better served efﬁcient heuristic algorithms TSP yield good optimal solutions advanced cut techniques combined partitioning strategies ﬁnd optimal solutions problems large 15000 points 1 82 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 tspS points tour H tour convexHullS returnsearchH SpointsH searchT tour S points B tour tour P point ifS B costT costB returnT elseifS returnB elseifcostT estimateS cid1 costB returnB P headS 0 T 1 B searchinsertP T SP B returnB algorithm Euclidean TSP The tsp function takes set points S input returns Fig 5 A lowestcost tour The heart code recursive function search takes partial tour T set S points visited B lowestcost tour far recursively explores space rooted partial tour Note solution better B T need search The cost function returns cost tour argument 0 argument null tour estimate returns lower bound additional cost adding points argument existing partial tour As function head returns ﬁrst element argument points returns point set tour argument insert inserts new point P ith position partial tour T Parallelization algorithm nagging follows manner similar sketch given Fig 3 return optimal solution exploring nodes generally far fewer nodes branchandbound search Fig 5 28 As DavisPutnam algorithm applying partitioning serial algorithm Fig 5 relatively simple idea explore different recursive calls search separate processors 42331 Of course load balancing fault tolerance issues present DavisPutnam algorithm But additional complications fact A search satisﬁcing search search ﬁrst solution optimizing search search best solution The difference node search space searched safely pruned search procedure certify better solution exists compare Davis Putnam algorithm unsatisﬁable formulae entail exhaustive search ﬁnding solution satisﬁable formula terminates search immediately The net effect subspaces interdependent consider happens new presumably favorable best solution ﬁrst subspace explored serial search process Clearly new solutions lower cost lead signiﬁcantly search subsequent subspaces When subspaces searched parallel new best solution discovered work subspaces performed And new best solution soon impact concurrent subspace searches cost communicated processors entailing additional communication overhead AM Segre et al Artiﬁcial Intelligence 140 2002 71106 83 We consider parallelizing serial algorithm Fig 5 nagging general idea identical Fig 3 DavisPutnam algorithm Idle naggers request nagpoint corresponding partial tour passed recursive calls search function master processors calling stack search space parallel Unlike partitioning case load balancing fault tolerance problems nagpoints generated solicited lost unresponsive naggers affect masters search Of course DavisPutnam algorithm key effective naggingthat nagging actually provides speedupis effective problem transformation function One reasonable approach randomly perturb order points inserted growing partial tour reordering list asyetunvisited points perturb order descendent nodes generated changing insertion order loop searchExplicit function Alternatively rely problemspeciﬁc knowledge choose mixture TSPspeciﬁc heuristic ordering strategies operations research literature Or adopt abstraction transformation nagger simply throws away certain number points point set This transformation provide speedup optimal tour abstracted search space longer masters current best solution Euclidean space cost optimal tour reduced problem lower bound cost optimal tour original points Well look nagging compares partitioning Euclidean TSP algorithm described Section 6 43 SPAM αβ minimax search Historically research artiﬁcial intelligence community focused problem playing zerosum twoplayer games chess checkers Most research involves derivatives variants αβ minimax search straightforward reﬁnement original notion minimax search Within ﬁeld forms parallelism received lot attention 38917 Here αβ minimax search algorithm parallelized nagging producing algorithm SPAM Scalable Parallel AlphaBeta Minimax The idea underlying minimax procedure generate tree legal moves ﬁxed depth given ply argument evaluate quality resulting board positions static board evaluation function SBE5 The SBE looks board returns value representing loss representing win speciﬁed player By selecting maximumminimum values alternating levels leaf SBE values propagated root tree deﬁnition maximizing level select branch leading best attainable outcome current player Deeper searches generally lead informed choices unfortunately 5 There exist numerous alternative formulations minimax search procedure This particular formulation selected simplicity Of course formulation minimax algorithm assumes good ones opponent symmetrically bad player opponent rational making decisions based identical SBE Also note formulation returns best SBE value tree given ply rooted given board In practice procedure return corresponding 84 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 alphabetaB board P player D integer score returnsearchB P D searchB board P player D integer α score β score score M ifwonB opponentP returnα elseifD 0 returnSBEB P M legalMovesB P α maxα searchapplyMoveB M opponentP D 1 β α ifα β returnα returnα Fig 6 Negamax formulation αβ minimax search algorithm Return best possible score player P hope attain search space D depth rooted board B Pruning occurs α exceeds β loop possible legal moves board position causing function return immediately We assume won legalMoves SBE applyMove functions appropriate gamespeciﬁc deﬁnitions opponent returns player max usual semantics size game tree examined increases exponentially depth exploration Thus meaningfully large games chess exponential growth makes shallow searches impractical The intuition underlying αβ pruning exploit information generated exploration portion search space justify skipping pruning parts space Judicious pruning extend computational horizon search operates allowing deeper searches computation time Of course beat exponential nature search practical maximum depth search The idea extend maximum possible reducing number branches examined Extending minimax algorithm perform αβ pruning relatively straightforward basic idea pass additional parameters called α β serve bounds interesting values given node Fig 6 Paths leaf values guaranteed fall interval α β initially chosen minimax procedure safely ignored As game tree searched α values increase β values decrease constraining search It clear construction αβ minimax search produces identical choices minimax search cases Furthermore A search search reduction produced αβ pruning depends order paths searched pathological game trees αβ minimax standard minimax search identical game trees6 From analytic perspective number calls SBE vary roughly 2bd2 best case bd worst case game tree depth d uniform branching 6 Indeed realworld implementations αβ minimax search try improve performance generating internal choice points ordered fashion usually guided cheap fast secondary SBE function applied board positions represented internal nodes Of course board positions look bad locally actually turn good ply deeper internal choice point reordering greedy optimization technique lead tobut guaranteemore efﬁcient search AM Segre et al Artiﬁcial Intelligence 140 2002 71106 85 factor b 18 Thus exponential factor present signiﬁcantly reduced allowing deeper informed searches time While implementing αβ minimax search requires minor extensions basic minimax procedure modiﬁcations attempts parallelize search signiﬁcantly complicated This minimax search easily decomposed node game trees rooted child node considered independently separate processing elements Once child SBE values computed parallel simple matter compare estimates select best alternative In contrast αβ minimax search partitioning parallel exploration game tree require time corresponding serial search This savings realized αβ pruning standard minimax search result exploiting information obtained searching game tree exploring like A search unlike DavisPutnam algorithm Even instantly share new α β values processing elements zero communication cost new values come late matter In short parallel algorithm search greater number nodes serial algorithm The SPAM algorithm exploits search constraints embodied α β values keeping communication processing elements infrequent brief The nagging algorithm Fig 7 essentially identical described A search nagpoint B board P player D integer α score β score nagalphabetaB board P player D integer score βcid26 score N nagpoint result αcid26 niceInit niceRoot return searchExplicitB P D whiletrue cid26 β cid26 narrowNα Nβ N niceIdle α result searchExplicitNB NP ND αcid26 βcid26 result αcid26 result βcid26 elseif result αcid26 elseif result βcid26 niceSolveN result niceRestrictN Nα result niceRestrictN result Nβ Fig 7 Sketch nagging implementation algorithm Fig 6 As Fig 3 function searchExplicit identical search function Fig 6 explicit stack manipulation interrupt handling capabilities The root process performs normal αβ minimax search nonroot processes engage series nagging episodes initiated niceIdle requests new nagpoint Nagpoints denoted N consist board position player search depth limit α β values For nagging processors narrow function randomly reduces search range described text serves problem transformation function combination permutations provided modifying naggers copy legalMoves function Fig 6 randomly perturb sequence legal moves generated Depending outcome naggers search nagger pass solution parent function niceSolve force parent reﬁne α β parameters function niceRestrict Note naggers search interrupted parent parent exhausts space rooted naggers nagpoint nagger case searchExplicit immediately return abort nagger request new nagpoint 86 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 DavisPutnam algorithm exploit problem transformation function based permutation window narrowing The idea nagger artiﬁcially restrict masters αβ window narrow interval αcid26 βcid26 α αcid26 βcid26 β ensures naggers search procedure prune aggressively exploring relatively small number nodes search space increasing odds beat master course permute node exploration order well7 In exchange reduction search value computed naggers transformed search directly substituted true value computed master subspace More precisely nagger returns value greater αcid26 βcid26 corresponds true value computed master force master backtrack continue search point But nagger returns value αcid26 alternatively greater βcid26 means true value lies α αcid26 alternatively βcid26 β This information reduce masters search space setting masters β αcid26 alternatively α βcid26 lead immediate improvement search efﬁciency masters current search In Section 6 empirically examine behavior SPAM problem transformation function 5 Analysis We introduce simple analytical models nagging partitioning help explain nagging expected provide performance advantage traditional partitioning methods Our analysis relies techniques developed reliability data analysis speciﬁcally understanding problem transformation function deﬁnes probability distribution solution times speciﬁc problem 212425 Let assume random variable x drawn distribution X represents solution time speciﬁed problem given problem transformation function Of course exact nature X depends search algorithm problem transformation function applied examine different choices X later The behavior x described density function f x corresponding cumulative density function F t measures probability solution time x speciﬁed time value t tcid1 F t Prx cid2 t f x dx x0 51 Note physical interpretation x imposes certain constraints allowable values f x F t precisely f x cid1 0 f x 0 x cid2 0 F 0 0 F 1 follow fact real problems solved zero time probabilistic semantics F t We use statistical model study behavior coarsest possible form nagging n nagging processors operates identical copy entire 7 In limit αcid26 ε βcid26 reduces zerowindow search 29 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 87 problem instance In essence n naggers racing ﬁnd solution operating different solutionequivalent transformation original space8 Once processor completes search solution ﬁnds fails ﬁnd applies modiﬁcation original problem instance In practice multiple naggers usually search different randomly selected nodes masters current search path service master search process furthermore effective transformation functions use solution equivalent abstraction transformation Sections 41 42 window narrowing transformation Section 43 Let random variable vn represent time elapsed n independent processors ﬁnds problem solution Clearly coarse nagging model vn minx1 x2 xn xi independent random variable drawn original distribution X Let Gnt corresponding cumulative density function vn Since vn minx1 x2 xn xi independent Gnt represents probability xi values t 1 minus probability xi values exceed t Thus Eq 51 Gnt Prvn cid2 t 1 1 F t cid4 cid3 1 1 F t cid4 n ncid2 cid3 i1 52 Taking derivative Gnt respect t evaluated vn yields density function gnvn gnvn nf vn cid3 1 F vn cid4 n1 53 A similar argument construct coarse model partitioning Technically argument somewhat problematic problem partitioned distributed different processors processor solving different problem solution time distributions vary signiﬁcantly original However case nagging reasonable assumptions support crude informativecomparisons models First assume original problem solution time described random variable x drawn distribution X partitioned n subproblems having identicallysized search spaces essentially claiming perfect priori solution load balancing problem Second assume fault tolerance issue processors actually terminate search return partial solutions Third assume run times scale linearly run time subproblem size 1n governed xn random variable x refers solution time original problem Finally assume cost merging subproblem solutions form solution original problem negligible The assumption problematic NPhard problems cost likely high merger feasible However intent compare model 8 A solutionequivalent transformation transforms original space losing existing solutions adding spurious solutions permutation transformation good example solutionequivalent transformation abstraction transformation Section 42 window narrowing transformation Section 43 54 55 88 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 partitioning nagging afford generous assumptions behalf partitioning compromising essence comparisons With assumptions place time required solve partitioned problem coarse partitioning model described random variable wn deﬁned wn maxx1n x2n xnn necessary solve subproblems merging solutions together9 Proceeding fashion nagging obtain density function hnwn cumulative density function Hnt follows Since Hnt represents probability wn t xn t cid6 cid5 Hnt Prwn cid2 t t F ntn ncid2 Pr i1 xi n obtain hnwn n2f nwn cid3 cid4 n1 F nwn Note g1v1 h1w1 f x G1t H1t F t expect given single processor trivial case coarse nagging coarse partitioning models Once appropriate distribution X ﬁxed relatively easy performance comparisons serial execution coarse nagging coarse partitioning comparing expected solution times 51 The uniform distribution The ﬁrst sample distribution look simplistic case X uniform distribution values x ranging constants tlo thi10 For uniform distribution density function f x 1 thi tlo tlo cid2 x cid2 thi f x 0 It easy tcid1 F t xtlo 1 thi tlo dx t tlo thi tlo 56 57 9 Unfortunately model complicated satisﬁcingas opposed optimizingsearch Consider DavisPutnam algorithm Boolean formula true algorithm terminate ﬁrst subproblem ﬁnds solution terminates wn minx1n x2n xnn On hand Boolean formula false search exhaust entire search space guarantee solution overlooked required time wn maxx1n x2n xnn given text Thus mixture model blends cases provide appropriate model satisﬁcing search 10 From practical perspective uniform distribution great relatively little reason believe solutions times real problems ﬁt Nonetheless serve useful point comparison distributions considered later paper AM Segre et al Artiﬁcial Intelligence 140 2002 71106 89 tlo t thi F t 0 t cid2 tlo F t 1 t cid1 thi Applying Eq 53 obtain density function nprocessor coarse nagging gnvn nthi vnn1 thi tlon 58 In similar fashion Eq 55 nprocessor coarse partitioning model obtain hnwn n2 nwn tlon1 thi tlon 52 The exponential distribution 59 Using approach consider realistic probability distributions Here look exponential distribution ﬁxed minimum time tlo decay parameter λ The exponential distribution long model equipment failure reliability studies follows uniform random failure pattern modeled Poisson process 24 This distributions density function given f x λ eλx eλtlo λeλtlox cumulative density function F t eλtlot Appropriate substitution Eq 53 yields gnvn nλeλntlovn coarse nagging case Eq 55 produces hnwn n2λeλtlonwn coarse partitioning case cid3 1 eλtlonwn cid4 n1 510 511 512 513 53 The lognormal distribution Recently characterized observed behavior backtracking search satisﬁability problems distributions ParetoLévy form Such distributions differ exponential distribution Section 53 heavy tailed complementary cumulative density function 1 F t decays slower exponentially Heavy tailed distributions justify random restart strategy sort single processor version coarse nagging satisﬁability problems 12 Many different heavytailed distributions reliability analysis commonly Weibull lognormal distributions include Gumbel extreme value distribution BirnbaumSaunders distribution The key question remains choose distribution best models observed search behaviornot satisﬁability problems search problems studied Fortunately exploratory data analysis techniques testing distributional adequacy 90 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 KolmogorovSmirnov somewhat sensitive AndersonDarling goodnessofﬁt tests known 38 support use lognormal distribution analysis11 Consider lognormal distribution ﬁxed minimum time tlo scale parameter µ shape parameter σ The distributions density function f x 1 σ x tlo 2π logxtloµ2 2σ 2 e 514 The cumulative density function expressed terms Φ cumulative density function standard normal distribution erf error function cid6 cid6 cid5 cid5 F t Φ logt tlo µ σ 1 2 1 2 erf logt tlo µ 2σ 515 Appropriate substitutions Eqs 53 54 obtain gnvn hnwn resulting expressions terribly informative expressions containing erf notoriously difﬁcult simplify 54 Comparing nagging partitioning We ready direct comparisons performance estimates coarse nagging coarse partitioning range 1 n processing elements One simple comparison look performance ratios deﬁned ratio serial expected solution time Ex parallel expected solution time Evn nagging Ewn partitioning expected solution times Ex Evn Ewn simply average elapsed times A meaningful statistic expected speedup deﬁned expected value xvn nagging alternatively xwn partitioning We metric meaningful represents expected speedup observed experiment serial performance compared directly parallel performance individual problem We deﬁne new random variable φn xvn alternatively ψn xwn compute expected value Eφn alternatively Eψn Since φn ψn ratios consider geometric means E φn eElogφn 516 11 We generated sets 100 datapoints solving single problem A TSP Davis Putnam3SATunsatisﬁable DavisPutnam3SATsatisﬁable SPAMOthello 100 times strictly solutionequivalent problemtransformation function permutation case We applied AndersonDarling test set tested distributions normal lognormal exponential Weibull Gumbel logistic consistent observed data In cases AndersonDarling test rejected p 005 tested distributions lognormal distribution exceptions DavisPutnam3SATsatisﬁable data AndersonDarling test rejected lognormal Weibull distributions SPAMOthello data AndersonDarling test rejected lognormal Gumbel distributions While tests entirely conclusive based randomlygenerated problems suggest lognormal suited modeling range search behaviors studied analysis AM Segre et al Artiﬁcial Intelligence 140 2002 71106 91 Eψn eElogψn 517 Eφn Eψn directly geometric mean representative expected speedup trials Exploiting additive properties expected values fact x vn alternatively x wn independent obtain cid3 E logφn cid4 cid4 cid3 logx cid3 logvn cid4 E E thicid1 thicid1 logxf x dx logvngnvn dvn 518 xtlo similarly cid3 E logψn thicid1 cid4 vntlo thicid1 logxf x dx logwnhnwn dwn 519 xtlo wntlon Unfortunately formulae Eφn Eψn complex gen eral case However values Eφn Eψn easily tabulated spe ciﬁc values n lend graphical comparison Fig 8 Qual itatively speaking Fig 8 makes clear noticeable scaling advantage nagging partitioning analytical model especially exponential log normal distributions correspond closely distributions observed prac tice Another interesting metric suggested closer examination performance ratio coarse nagging exponential distribution case Ex Evn tlo 1 λ tlo 1 nλ 520 We note λ tlo small performance advantage obtained coarse nagging canon averageapproach n linear speedup This implies coarse nagging expected provide superlinear speedups respect serial search half time exponential distribution case For heavytailed distributions advantage coarse nagging decisive providing theoretical justiﬁcation observed effectiveness random restart strategies serial processors More formally interesting compute compare probability coarse nagging partitioning exhibit superlinear speedup respect average sequential case easily expressed cid6 cid6 cid5 cid5 vn cid2 Ex n Pr Gn Ex n nagging cid5 wn cid2 Ex n Pr cid6 cid5 Hn cid6 Ex n 521 522 92 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 Fig 8 Expected speedups Eφn Eψn vs number processing elements uniform tlo 001 thi 10 exponential tlo 001 λ 2 lognormal distributions tlo 001 µ 05 σ 15 This statistic illustrates scaling advantage nagging partitioning simple analytic model distributions studied advantage nagging grows distribution heavy tailed partitioning As resulting expressions difﬁcult simplify easy tabulate values n 2 10 Table 1 It clear distribution models studied coarse model nagging retains potential producing superlinear speedup number processors AM Segre et al Artiﬁcial Intelligence 140 2002 71106 93 Table 1 Technique Distribution n 2 n 3 n 4 n 5 n 6 n 7 n 8 n 9 n 10 Nagging Uniform Partitioning Uniform Nagging Partitioning Nagging Partitioning Exponential Exponential Lognormal Lognormal 043 025 062 040 085 060 041 013 062 025 088 046 039 006 061 016 089 036 038 003 060 010 090 028 037 002 059 006 091 021 036 001 059 004 091 017 036 000 058 003 091 013 035 000 057 002 091 010 034 000 056 001 091 008 increased limited potential partitioning rapidly vanishes processors added Of course analytic models presented relatively simple correspond exactly nagging partitioning actually applied practice Were interested observations coarse models hold realistic situations example nag partition recursively use problem distribution functions strictly solution equivalent window narrowing abstraction elect nag partition multiple times problem internal nodes search tree root node We turn practical questions section experimentallyobtained quantitative data support claims naggings performance principled manner 6 Empirical evaluation Empirical studies carefully realistic picture systems behavior Here focus performance issues experimental data contrast relative performance nagging partitioning support claims scalability nagging 61 Experiment 1 The ﬁrst experiment compares implementations nagging partitioning purposefully designed evoke coarse analytic models previous section The experimental procedure straightforward First tested algorithms DavisPutnam3SAT ATSP SPAMOthello 100 randomlygenerated problems solved serially ﬁxed search order 450 MHz Celeron machine running Linux operating system12 Next second 450 MHz Celeron machine added NICE hierarchy problem solved twice second machine 12 The random problem sets generated provide good crosssection solution times ranging 001 seconds resolution Linux clock roughly 20 minutes single processor DavisPutnam3SAT problems ranged 120 140 variables 514 604 clauses mentioned Section 41 intended difﬁcult problems randomlygenerated A TSP problems ranged 29 33 cities The SPAMOthello problems consisted random legal midgame Othello boards having 18 22 pieces placed searched 8 9 ply horizon 94 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 nagging processor partitioning processor instead For problem solution elapsed processor time master processor returned ANSI C clock function recorded Any search completed prespeciﬁed resource limit marked censored best solution far returned comparison Like coarse analytic models previous section problem transformation function permutation transformation algorithms examine transformation functions Section 62 Unlike coarse analytic models optimizing search algorithms SPAMOthello ATSP nagging event occur course experiment While nagging takes place root node nagger ﬁnds better solution allowed immediately report new bound master begin new nagging event applying newly obtained bound new permutation transformation root node A similar change partitioning case address loadbalancing issues normally associated partitioning use asynchronous partitioning protocol similar nagging As nagging idle slave processor initiates process requesting additional work master Instead nagpoint master provides slave available sibling node highest available ancestor node search path The slave searches partition identical search ordering master When slave completes search reports result master marks assigned sibling node solved The master free assign partition slave Note masters search enters subspace assigned slave slave effect nagger albeit beneﬁt problem transformation function head start search space All 3SAT problems solved conﬁguration prespeciﬁed resource limit For Othello problems 100 random problems censored solved resource limit serial search easily solved nagging partitioning systems The situation complicated TSP problems total 14 problems censored serial Of 14 problems ﬁve solved optimality nagging partitioning additional problem solved optimality nagging It important note censoring occurred serial parallel conﬁgurations qualitatively better solutions produced parallel systems problems nagging shortest tour partitioning shortest tour remaining cases So terms number problems solved optimality quality solution problems solved optimally performance edge appears belong nagging Differences solution quality aside interested quantifying changes performance Here compare computed speedup values recall speedup deﬁned ratio serial solution time parallel solution time speedup value 10 implies difference serial parallel systems larger AM Segre et al Artiﬁcial Intelligence 140 2002 71106 95 Table 2 TSP A SPAMOthello DavisPutnam3SAT satisﬁable unsatisﬁable N 92 100 100 49 51 min µ Nagging speedup µ 151 155 200 250 162 243 328 754 1293 236 086 098 045 045 098 max 6973 14832 22100 22100 1419 N 91 100 100 49 51 µ min Partitioning speedup µ 168 175 187 225 157 180 214 739 1343 159 055 101 055 055 107 max 523 2255 48117 48117 199 speedups imply parallel faster13 Table 2 presents minimum arithmetic mean µ geometric mean µ maximum speedups computed excluding doubly censored datapoints tested systems Given methodological difﬁculties noted interpretation order Consider example ATSP SPAMOthello values shown table For algorithms mean speedup µ reported nagging larger partitioning geometric mean µ partitioning This consequence nondeterministic nature nagging speedup vary dramatically trial solving problem In contrast partitioning nature conservative likely provide uniform amounts speedup subsequent trials This behavior partially evident maximum speedup values shown naggings best performance test suite represents order magnitude improvement partitioning Note expect minimum values hover large 10 Values 10 represent performance penalty incurred parallel systems regard serial This partly initial setup costs connecting NICE infrastructure partly communication overhead shall soon smallest values observed usually associated problems solved quickly single processor precluding amortization startup costs longer solution times The results DavisPutnam3SAT problems notably different tested systems entire set problems naggings measured speedups exceed partitioning measured µ µ If restricted satisﬁable formulae partitionings measured performance similar nagging Recall 13 Methodologically direct comparison sets speedup values somewhat difﬁcult number reasons First noted earlier reporting arithmetic means ratios like speedup problematic reporting geometric means better choice consistent general practice parallel processing community arithmetic means norm Furthermore important mind distribution observed speedup values skewed surprisingly given deﬁnition bounded 0 simply reporting summary statistics evoke normal distributions minds readers misrepresentative Finally caution exercised comparing censored datapoints 34 Since identical resource limits imposed parallel serial trials doublycensored datapoints unit speedup values Singlycensored datapoints harder fortunately experiments singlycensored datapoints censored serial parallel computed speedup values represent underestimates true speedup Note direct comparisons computed speedup values uncensored singlycensored datapoints singlycensored doublycensored datapoints care 96 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 DavisPutnam3SAT algorithm searches encounters satisﬁcing solution exhausting search space given formula satisﬁable This early termination behavior implies nagging partitioning lucky quickly encounter solution yielding large observed speedup values consistent maximum observed speedup values reported table In contrast search unsatisﬁable formulae unfolds manner consistent search algorithms exhaust entire search space labeling formula unsatisﬁable On problems naggings performance clearly dominates partitioning reported measures Of course summary statistics speedup values shown obscure relation speedup values problem difﬁculty observing 200fold performance improvement problem takes hours solve processor meaningful observing similar speedup problem solved milliseconds To provide gestalt view speedup respect problem difﬁculty turn graphical representation data Figs 911 plots parallel solution time serial solution time14 Interpretation kind plot relatively straightforward datapoints falling upper diagonal line faster parallel speedup values larger 10 datapoints falling upper diagonal line faster processor The lower line represents speedup value equal number processors use datapoint falling line correspond superlinear speedups Fig 9 shows results ATSP Since identical resource limits imposed serial parallel trials doublycensored datapoints fall diagonal line Singlycensored datapoints best understood datapoints artiﬁcially shifted left true position plotted serial solution times ordinate represent lower bounds true serial solution times As clear plot nagging generally provides speedup occasionally provides exceptional speedups partitioning constrained region diagonal lines As expected datapoints slower nagging partitioning datapoints corresponding speedup values 10 reported earlier relegated left hand plot represent small problems startup costs parallel execution effectively amortized longer solution times A similar trend observed difﬁcult larger serial solution times problems superlinear speedups likely occur Aside amortization argument second factor work naturally expect concomitantly greater payoff ﬁnding good solution early larger search space While mechanism nagging attains superlinear speedup clear somewhat clear partitioning achieve kind performance To 14 Note clarity data plotted loglog space transformation tend obscure relative performance differences large small problems 1 unit vertical alternatively horizontal difference half alternatively right plot represents larger time interval identical 1 unit vertical alternatively horizontal difference half alternatively left plot AM Segre et al Artiﬁcial Intelligence 140 2002 71106 97 Fig 9 Twoprocessor A TSP performance plot loglog space Datapoints falling upper diagonal line faster parallel datapoints falling lower diagonal line superlinearly faster parallel Doublycensored datapoints fall diagonal line singlycensored datapoints appear artiﬁcially displaced left true positions understand happen recall partitioning implementation tested differs coarse model previous section respect asynchronous load balancing policy adopted consequence multiple partitioning events occur course solving single problem We attribute occasions partitioning attains superlinear speedups situations exceptionally good bounds early partitions subsequent partitioning events enjoy beneﬁts terms additional pruning A similar explanation accounts occasional superlinear speedups reported SPAMOthello partitioning Fig 10 Note ATSP SPAMOthello partitioning likely attain superlinear speedups especially larger problems nagging 98 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 Fig 10 Twoprocessor SPAMOthello performance plot loglog space Datapoints falling upper diagonal line faster parallel datapoints falling lower diagonal line superlinearly faster parallel nagging occasion delivers large speedups Particularly noticeable lone censored problem serial solution time plotted lower bound true serial solution time speedup attained nagging 14832 compared lower bound speedup 2254 partitioning The results shown Fig 11 DavisPutnam3SAT solver similar pattern Recall nagging partitioning expected result large speedups satisﬁable formulae algorithms earlytermination behavior Thus unsatisﬁable problems datapoints lie superlinear speedup zone plot Yet nagging likely result superlinear speedups unsatisﬁable problems meaningful size 62 Experiment 2 Our analysis previous section relied strictly solutionequivalent transformation functions In experiment explore performance window narrowing non solutionequivalent problem transformation function SPAM Recall main idea nagger artiﬁcially restrict αβ window order gain execution speed expense information window narrowing prune aggressively informed solutionequivalent transformations Our protocol somewhat arbitrarily randomly narrows nagpoint window forcing processors corresponding leaf nodes nagging hierarchy use unit window essentially performing zerowindow search leaf processors The experimental procedure identical Experiment 1 100 random Othello problems Note processors nagging processor leaf processor operating unit AM Segre et al Artiﬁcial Intelligence 140 2002 71106 99 Fig 11 Twoprocessor DavisPutnam3SAT performance plot loglog space Datapoints falling upper diagonal line faster parallel datapoints falling lower diagonal line superlinearly faster parallel Fig 12 Twoprocessor SPAMOthello performance plot loglog space use window narrowing transformation Datapoints falling upper diagonal line faster parallel datapoints falling lower diagonal line superlinearly faster parallel window size The results plotted serial solution times shown Fig 12 note nonnarrowing data identical shown Fig 10 In addition graphical comparisons test statistically null hypothesis permutation window narrowing faster permutation If reject null hypothesis conclude window narrowing beneﬁcial To test 100 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 hypothesis making distributional assumptions use nonparametric statistic paired Wilcoxon signedranks test 42 nonparametric statistics sacriﬁce terms power counterbalanced broad applicability Using 100 paired samples input paired sign test easily rejects null hypothesis traditional critical value statistical signiﬁcance p 005 Of course statistics notwithstanding Fig 12 clearly shows transformation functions job results shown underline critical importance nature transformation function 63 Experiment 3 While nagging partitioning capable delivering effectiveand case nagging superlinearspeedups exceptional speedups necessarily rule problem In section shall examine problem domain nagging partitioning deliver speedup approach manages produce exceptional results Consider following team assignment problem TAP drawn sports economics literature Given collection N players T teams ith player associated quality value Qi ﬁnd assignment players teams equal number players team differences relative team qualities computed function constituent player qualities minimized There variants problem depending team quality metric complex variants involve higher order effects individual player quality function teammate qualities 41 For experiment choose simple linear metric effect minimizing sum set teams absolute value difference team quality sum player qualities hypothetical average team quality computed product team size average player quality Our goal ﬁnd bestmatched team assignments terms team quality particular metric perfect solution produces T teams exactly average quality solution exists Our solution applies A search algorithm described Section 42 team assignment problem Formulating admissible heuristic function properly bounds solution value partial assignment overly difﬁcult function estimates potential deviation target team quality average player quality times team size 30 The experiment follows protocol Experiment 1 100 randomly generated matching problems results shown Fig 1315 The striking feature plot Fig 13 extent partitioning tracks linear speedup line smaller problems solution times order 15 Random problems generated serial solution times ranged 001 seconds resolution Linux clock 5 minutes The resulting problem set assigned 15 32 players 2 3 4 teams AM Segre et al Artiﬁcial Intelligence 140 2002 71106 101 Fig 13 Twoprocessor A parallel datapoints falling lower diagonal line superlinearly faster parallel TAP performance plot Datapoints falling upper diagonal line faster clock resolution signiﬁcant deviation line On hand performance nagging decidedly worse partitioning rarely attaining linear speedup It tempting attribute poor performance nagging use inadequate problem transformation function permutation search space clearly provide additional pruning TSP Yet likely problem lies heuristic estimate informative close true additional cost partial solution A search expected explore fewer nodes simple branchandbound One compensate poor heuristic estimate better problem transformation function Alternatively difﬁculty lie problem If cost landscape tends populated local minima values near global minima highly informative heuristic lead pruning As mentioned Section 41 acknowledge NPhard problems harder Unlike 3SAT problems possible TAP problems simply uniformly difﬁcult 64 Experiment 4 In experiment provide empirical support scalability nagging showing additional processors beneﬁcial effect performance provide direct comparison behavior partitioning The experimental procedure like Experiment 1 use 8 16 32 64 essentially identical 102 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 Table 3 CPUs 2 64 N 92 100 µ Nagging speedup µ 151 495 243 1229 min 086 096 max 6973 25622 Partitioning speedup N 91 95 min 055 038 µ 180 449 µ 168 390 max 523 1021 processors arranged hierarchy branching factor equal three16 In section focus results obtained 64 processors17 We turn ﬁrst simple descriptive statistics Recall Experiment 1 processors 8 problems left unsolved nagging 9 problems left unsolved partitioning correspond doublycensored datapoints Fig 9 When 64 processors applied nagging solves 100 problem optimally partitioning leaves ﬁve unsolved problems So qualitatively performance nagging exceeds partitioning We use observed speedup values help quantify trend Table 3 doublycensored datapoints excluded Direct comparison µ µ conﬁrms advantage nagging experiment maximum observed speedup nagging exceeds maximum observed speedup partitioning factor Moreover values given table fully consistent argument ﬁrst advanced Section 53 probability obtaining superlinear speedup higher nagging partitioning nagging produces superlinear speedups 100 problemsand possibly given censoring observedwhile partitioning fails produce superlinear speedup One troubling fact observed mean speedups µ µ signiﬁcantly N number processors employed In analytical model predicted µ valueswhile N signiﬁcantly line N We attribute discrepancy differences First model Section 5 coarse model processors engage single nagging episode root node search experimental model allows repeated nagging episodes applied internal nodes search process Second point analytical model N 1 naggers reporting directly single master search process experiment allowed processors nag master directly remaining N 4 processors recursively nag naggers That NICE hierarchy limits daemon descendents arbitrary increasing branching factor hierarchy raises communication overhead incurred master balance increased overhead performance beneﬁts obtained Note tradeoff complicated optimal conﬁguration differ depending search algorithm problem instance In case adaptive conﬁguration NICE hierarchy area actively exploring 16 The actual hierarchy depends NICE resource management daemon constantly changing response local load availability 17 For record tests nagging partitioning scale smoothly 2 64 processors AM Segre et al Artiﬁcial Intelligence 140 2002 71106 103 Fig 14 Sixtyfour processor A processor A faster parallel datapoints falling lower diagonal line superlinearly faster parallel TSP performance plots nagging partitioning compare TSP performance plots given Fig 9 Datapoints falling upper diagonal line Fig 14 presents graphical view data compared directly plots Fig 9 conﬁrm descriptive statistics outlined nagging appears effective partitioning applying additional processors reduce solution time Not surprisingly observation appears striking larger problems cost initializing additional processors readily amortized longer solution times We conjecture trend extends larger problems fact nagging 104 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 solves problems optimally partitioning leaves ﬁve unsolved problems prespeciﬁed resource bound consistent conjecture 7 Conclusion Nagging paradigm parallel search heterogeneous distributed computing environment It applicable broad range different artiﬁcial intelligence search algorithms scales easily large number processors We shown analytical performance model nagging superior approach traditional partitioning strategy commonly employed parallelize search We presented empirical results conﬁrm predictions analytical model support claims performance scalability nagging different domains search algorithms We currently working number reﬁnements nagging First inspired empirical analytical results reported experimented randomly mixing nagging partitioning search The idea nagging partitioning appear complementary actively manage mixture approaches order effectively guide use computational resources We currently focusing decide new event nagging event partitioning event Based analytic model Section 5 possible use statistical evidence obtained course problem solving episode decide best use idle processor particular problem This decision change course search example early events primarily nagging events later events primarily partitioning events The basic idea let information particular problem solving process guide best application processor power course search process We working extensions NICE infrastructure We experi menting better hierarchyformation restructuring algorithms order better apply available computational resources given problem We looking cryptographic certiﬁcation techniques distributing new NICEenabled applications processors NICE hierarchy Much work performed context extraordinarily challenging computational biology application Over years working HOPS ab initio distributed hybrid optimization protein structure prediction engine 10 HOPS large interdisciplinary project involving faculty students biochemistry science operations research applied mathematics It combines distributed search nagging partitioning discrete space protein conformations traditional continuous optimization techniques nonlinearly constraint nonlinear programming interior point methods ﬁnd energetically favorable conformation speciﬁed protein according energy model design Given sheer size search problems HOPS perfect example kind application naggings distinguishing featureseffectiveness scalability fault toleranceshould shine AM Segre et al Artiﬁcial Intelligence 140 2002 71106 105 Acknowledgements The authors like thank Karl Arndt Bruno Codenotti Mauro Leoncini Harry Paarsch Marco Pellegrini Tianbing Qian David Sturgill advice comments suggestions Ted Herman Hantao Zhang graciously provided access University Iowa Computer Science Department Linux cluster collect experimental results reported We like acknowledge contributions Ragothaman Balakumar Satyanarayanan Jayar Shantan Kethireddy Sumantra Kundu late Jinghou Li Vivek Narayanamurthy Neela Patel Zhaxian Que Sunita Raju Kevin Sagon Sumana Vijayagopal portions NICE distributed search infrastructure The authors wish thank anonymous reviewers helpful comments resulted precise better written paper Support research provided Ofﬁce Naval Research grant N00149411178 National Science Foundation grant NSFBIO9730053 Italian National Research Council visiting professorship University Iowa Faculty Scholar award equipment support provided NSFCDA9529518 NSFIRIS9729807 University Iowa Ofﬁce Sponsored Programs References 1 D Applegate R Bixby W Cook V Chvátal On solution traveling salesman problems Technical Report 98744 Center Research Parallel Computation Rice University Houston TX July 1998 2 A Beguelin J Dongarra W Jiang R Manchek V Sunderam PVM Users Guide Reference Manual Oak Ridge National Laboratory Oak Ridge TN 1994 3 M Brockington J Schaeffer APHID Asynchronous parallel gametree search J Parallel Distributed Comput 60 2 2000 247273 4 D Cook RC Varnell Adaptive parallel iterative deepening search J Artiﬁcial Intelligence Res 9 1998 139166 5 S Cook The complexity theoremproving procedures Proc 3rd Annual ACM Symposium Theory Computing 1971 pp 151158 6 M Davis G Logemann D Loveland A machine program theoremproving Comm ACM 5 7 1962 394397 7 W Ertel Performance analysis competitive orparallel theorem proving Technische Universität München FKI16291 1992 8 R Feldmann P Mysliwietz B Monien Game tree search massively parallel H van den Herik I Herschberg J Uiterwijk Eds Advances Computer Chess VII University Limburg Maastricht Netherlands 1994 pp 203218 9 C Ferguson RE Korf Distributed tree search application alphabeta pruning Proc AAAI88 St Paul MN 1988 pp 128132 10 SL Forman Torsion angle selection emergent nonlocal secondary structure protein structure prediction PhD Thesis Department Mathematics The University Iowa Iowa City IA August 2001 11 I Foster C Kesselman Globus A metacomputing infrastructure toolkit Internat J Supercomput Appl High Performance Comput 11 2 1997 115128 12 C Gomes B Selman N Crato H Kautz Heavytailed phenomena satisﬁability constraint satisfaction problems J Automat Reasoning 24 12 2000 67100 13 A Grama V Kumar Parallel search algorithms discrete optimization problems ORSA J Comput 7 4 1995 365385 14 A Grama V Kumar State art parallel search techniques discrete optimization problems IEEE Trans Knowledge Data Engrg 11 1 1999 2835 106 AM Segre et al Artiﬁcial Intelligence 140 2002 71106 15 W Gropp E Lusk N Doss A Skjellum A highperformance portable implementation MPI messagepassing interface standard Parallel Comput 22 6 1996 789828 16 J Gu PW Purdom J Franco BW Wah Algorithms satisﬁability SAT problem A survey Satisﬁability Problem Theory Applications DIMACS Series Discrete Mathematics Theoretical Computer Science American Mathematical Society Providence RI 1997 pp 19151 17 C Joerg B Kuszmaul Massively parallel chess Proc Third DIMACS Parallel Implementation Challenge Rutgers University Rutgers NJ 1994 18 DE Knuth RW Moore An analysis alphabeta pruning Artiﬁcial Intelligence 6 4 1975 293326 19 TH Lai S Sahni Anomalies parallel branchandbound algorithms Comm ACM 27 4 1984 594 602 20 E Lawler D Wood Branch bound methods A survey Oper Res 14 4 1966 699719 21 E Lee Statistical Methods Survival Data Analysis Second Edition Wiley New York 1992 22 M Litzkow M Livny M Mutka Condor A hunter idle workstations Proc Eighth Conference Distributed Computing Systems San Jose CA 1988 23 A Mahanti CJ Daniels A SIMD approach parallel heuristic search Artiﬁcial Intelligence 60 2 1993 243282 24 N Mann R Schafer N Singpurwalla Methods Statistical Analysis Reliability Life Data Wiley New York 1974 25 W Meeker L Escobar Statistical Methods Reliability Data Wiley New York 1998 26 D Mitchell B Selman H Levesque Hard easy distributions SAT problems Proc AAAI92 San Jose CA 1992 pp 459465 27 A Newell J Shaw H Simon Chess playing programs problem complexity IBM J Res Development 2 1958 320335 28 N Nilsson ProblemSolving Methods Artiﬁcial Intelligence McGraw Hill New York 1971 29 P Norvig Paradigms Artiﬁcial Intelligence Programming Case Studies Common Lisp Morgan Kaufmann San Mateo CA 1992 30 HJ Paarsch AM Segre Extending computational horizon Effective distributed resourcebounded computation intractable problems Proc Fifth International Conference Society Computa tional Economics 1999 31 C Powley C Ferguson RE Korf Depthﬁrst heuristic search SIMD machine Artiﬁcial Intelli gence 60 2 1993 199242 32 A Reinefeld V Schnecke Workload balancing highly parallel depthﬁrst search Proc 1994 Scalable HighPerformance Computing Conference 1994 pp 773780 33 G Reinelt The Traveling Salesman Computational Solutions TSP Applications Springer Berlin 1994 look experimental evaluations EBL Machine 34 AM Segre CP Elkan A Russell A critical Learning 6 2 1991 183196 35 AM Segre DB Sturgill Using hundreds workstations solve ﬁrstorder logic problems Proc AAAI94 Seattle WA 1994 pp 187192 36 B Selman H Levesque D Mitchell A new method solving hard satisﬁability problems Proc AAAI92 San Jose CA 1992 pp 440446 37 B Selman D Mitchell H Levesque Generating hard satisﬁability problems Artiﬁcial Intelligence 81 1996 1729 38 MA Stephens EDF statistics goodness ﬁt comparisons J Amer Statist Soc 69 1974 720727 39 DB Sturgill AM Segre Nagging A distributed adversarial searchpruning technique applied ﬁrstorder logic J Automat Reasoning 19 3 1997 347376 40 DB Sturgill AM Segre A novel asynchronous parallelization scheme ﬁrstorder logic Proc Twelfth Conference Automated Deduction 1994 pp 484498 41 C Whittinghill Social choice resource allocation professional sports league PhD Thesis Department Economics The University Iowa Iowa City IA 1999 42 F Wilcoxon Individual comparisons ranking methods Biometrics 1 1945 8083 43 W Zhang StateSpace Search Algorithms Complexity Extensions Applications Springer Berlin 1999