Artificial Intelligence 82 1996 157179 Artificial Intelligence Noise modelling evaluating learning examples Ray J Hickey Faculty Informatics University Ulster Coleraine Ulster Co Londonderry BT52 lSA N Ireland UK Received July 1993 revised July 1994 Abstract The means evaluating artificial data algorithms ID3 learn method artificial universes examples enhanced referred treated dependent variable description attributes concepts The central notions class model associated representations class attribute independent irrelevant attribute small universe altered data generated noise detrimental discussed modelled The notion construction increase noise Learning curves ID3 trials These increasing universes estimated effect learning variables The nature noise model considered The ideas illustrated majorisation informationtheoretic ideas especially functioning 1 Introduction Supervised learning concepts major Amongst induction decision S instancebased Niblett artificial neural networks Goldberg Holland Iba Thompson approaches classified examples remains problem taken task rules Clark 9 Booker 6 Langley Aha 2 Cost Salzberg 26 genetic classifiers 24 highlevel trees Quinlan learning Rumelhart 4 B y esian classifiers Cheeseman 17 examples presented Typically algorithm representative sense set possible examples constitute small subset Each language example consists description appropriate simple attributevalue assigned class concept offered teacher The learning task induction general concept description particular cases provided representation formalism Telephone 440126544141 Email rjhickeyulstacuk 00043702961500 SSDZ 000437029400094S 0 1996 Elsevier Science BV All rights reserved 158 RJ Hickey I Artificial Intelligence 82 1996 157179 In realworld settings task complicated presence noise forms errors attribute values classification teacher This makes work learning algorithm difficult complicates Performance algorithm comparison algorithms evaluation performance recording usually assessed following means 1 Empirical analysis real data From database examples standard Murphy learning Further classification performance Aha sets kept Machine Learning Repository 21 subsets drawn random subsets drawn evaluation learned description 2 Empirical analysis artificial data In order effects simulate given prescription aspects noise data generated according example description class altered mechanism involving known probabilities Irrelevant attributes introduced 3 Averagecase analysis Examples known probabilistic prescription The expected value behaviour learning algorithm derived generated according 4 PAC analysis The probably correct approximately basis assessing performance A algorithm 31 provides theoretical said PAClearnable Valiant probability concept 1 6 learned concept description probability 1 E classifying analysis probabilistic correctly subsequent underlying distribution assumed worst case possible distributions trials Although examples PAC theory 2 provides greater opportunity 16 argue method method wellknown artificial data sets containing In methods learning curve showing performance typically classification accuracy number training examples derived For methods 1 2 curve estimated data trials Kibler systematic Langley investigation 1 particularly regard controlled adminis tration noise method 1 naturally occurring noise quantified satisfactorily Amongst element noise LED domain Breiman et al 5 components LED display digits inverted small probability Aha l introduced variation empirical approach taking database altering random called case retains In method essential characteristics original 3 theoretical underlying derived Pazzani Sarrett 22 Langley et al 17 Unfortunately feasible simple algorithms Analysis sophisticated ID3 appear learning curve mathematics permitting approach algorithms 26 setup noise introduced produce variant experimentation 2425 backpropagation Rumelhart Quinlan fashion difficult l3 Unlike methods performance algorithm PAC approach offers little insight overly pessimistic typical circumstances RJ Hickey I Artijicial Intelligence 82 1996 157l 79 159 capabilities Pazzani Sarrett 22 learning curves derived average case PAC approaches For noise modelling PAC approach 27 Valiant 32 Angluin Laird 3 Sakakibara 11 Modelling noise extending use artificial data method artificial universes Although indicated introduction noise systematic way modelling 2 3 4 practice methods undertaken ad hoc lacking underlying theory The resulting artificial domains sufficiently realistic regard complexity noise Also clear noise overall introduced The purpose work 2 introducing measure modify ease noise artificial Amongst benefits approach enhance theory noise makes capabilities method possible unified 1 simple means producing data sets possess required tasks wide range algo noise provide challenging rithms 2 greater clarity concerning relative importance different sources noise 3 greater insight nature noise information explicated The approach involves specifying complete probabilistic model attributes example description class This referred artificial universe The class model universe declare descriptions class distributionsas The class model represented table comparatively relationships distinct individual classes forms exhaustive small set general rules An artificial universe generate examples use types learning algorithm mentioned learning taken place provide true indication performance learned concept description Since approximated series trials involving generated examples random variable different value behaviour expected Noise modelled class distributions noise manipulated principally majorisation concerned relative degree inequality elements case vector probabilities constituting distribution real vector class model The relation Although 2 merely extension elaboration proposed referred method method artificial universes Specifying complete probability model generation data hardly new idea Many artificial domains literature LED domain 5 generators offering limited noise modelling models Generalpurpose example Lounis Bisson 18 In capability developed simple general means essence contribution provide 160 RJ Hickey I Artificial Intelligence 82 1996 157179 prescribing particular noise way largely independent physical source The ideas paper outlined Hickey method illustrated sequent experimentation trates underlying 14 performed larger number trials construction ID3 generated theory similar set experiments 14 use small universe sub examples The paper concen 12 Plan paper The definition artificial universe class model given Section 2 class model Section 3 Two running example introduced The representation discussed The modelling different types irrelevant attribute role majorisation pure noise redundant explicating noise relationship types noise addressed defined The Section 4 This applied produce ideas reviewed informationtheoretic information statistics universe In Section 5 means assessing performance deterministic classifier results obtained discussed Experimental learning universes varying degrees noise reported acquired ID3 data generated 2 Artificial universes referred set An object situation classified described attributes attributes labelled b c description condition usually finite continuous The description These attributes discrete attribute description schema A vector values b value called description vector More generally attribute partially called condition complex The instantiated object situation labelled classl classes assigned class2 regarded values attribute called class class attribute attributes description vector schema values called An artificial universe notion class model representation defined follows Definition 21 An artificial universe consists description attribute joint distribution class description attributes schema class Definition 22 The function maps description vector distribution class attribute class model universe conditional vector called RJ Hickey I Artificial Intelligence 82 1996 157179 161 Definition 23 Any statement specifying class model said representu tion class model The class model analogous statistical model regression schema independent variables Likewise class distributions play role note correct This section physical sources analysis class treated dependent variable description providing error distributions play statistical model It important observed schema vectors class probabilistic modelled The artificial universe makes statement example point elaborated noise discussed A representation class associated particular description usually set rules form relationship complex class distribution complexes model completely define universe joint distribution form partition set description vectors The class description sufficient attributes To illustrate ideas small universe called universe 1 built This variation universe defined 14 description schema number classes joint distribution description attributes increased The schema involving description attributes b c d e b c d e al a2 b b b b Cl c27 C3 d d d e1 e2 class 1234567 g The rule set specifying appear The joint distribution description attributes Table 2 Here dependent obtained class model shown Table 1 Note d e d c The probability b c e defined description occurring mutually independent provided Pa vi b u2 c u3 d u4 e u5 PuubuPcuPduaucuPeu 1 righthand The description vector 1 example generating expression The rule set Table 1 offers fairly compressed general complexes Clearly discrete possible attributes description tabulate mapping representation involving schema finite individual description 162 RJ Hickey I Artificial Intelligence 82 1996 157179 Table 1 A representation class model universe I Rule number 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Description Class distribution bb andcc cc bb aa a2 b b c c1 a2 b b c c2 aaz b b c c b b c cz bb b b a2 b b c c az b b c c2 b b c c3 b b c c aa b b c c2 b b c c b b c c ccz bb 0505000000 055000450000 0006000004 OO OO OO 0406 OO 00300070 OO oo 00550450 OO 04006000 0307000000 OO 00600400 OO 06000400 OO OOO lO 0 OO 0035006500 OO oo oso 005 0400060000 0060000400 OO oo 005050 vectors class distributions representation referred enumerated One motivations complications dependent artificial data modelling mutual work simulate artificial setting attributes way aspect In approaches real world Making description independence assumed joint distribution description attributes universe 1 b c e Table 2 Probabilities mutually independent d dependent c b b 005 015 C2 005 a2 C Cl 06 Conditional values ofaandc c c c1 a2 c a2 c2 c b 002 003 C3 035 d 04 02 01 025 03 b 04 025 e e 03 02 05 02 07 03 b 0 01 e2 07 04 03 07 005 04 RJ Hickey I Artificial Intelligence 82 1996 157179 163 Once universe specified examples generated follows 1 Generate description vector joint distribution description attributes generating expression 2 Look generated vector class model class distribution class class distribution 3 Generate 4 The description vector class obtained constitute It appear example use rule set define class model learning algorithms induce highlevel rules This It convenient way produce specification universe representation biased case Moreover choice representation use general rules versus enumerated generation examples example consequence The marginal unconditional distribution class referred default universe specification For universe 1 distribution obtained 0121501510004000211500445026750127000370 2 The majority class probable default class Here default distribution class 6 2675 chance occurring called 21 Subuniverses removed attributes example description There universes universes These called subuniverses subuniverse reduced description marginal subuniverse wrt conditional attributes subinverse instantiation partially values restricted subuniverses uninstantiated conditional attributes If schema This referred remaining description attributes There given complex having complex Such complex involve partial In case schema subuniverse specified instantiation attributes included b b b relative These attributes formulate results instantiated Class models generating expressions constructed subuniverses specification universe 3 Noise class model In learning relationship examples noise obscures description class There major physical sources noise 1 insufficiency description 2 corruption attribute values example description 3 erroneous classification training examples schema 164 RJ Hickey I Artificial Intelligence 82 1996 157179 distributions appearing class model If nondegenerate presence noise sources The user attributed origin noise artificial universe method adopt attitudes investigate Firstly ignore noise affects learning varying degree noise manner described Secondly declare noise originates specify came sources It difficult source building elaborate model For origin noise affect analysis learning approach sources separately required To consider catered class model The class distributions class model association observed description vector class assigned insufficiency description schema statisticians residual error uncertainty model taken These distributions modelor eliminate separate need forattribute distinction actual attributes observed Usually example domain actual attribute true attribute according distribution description vector noise To explicitly allow corruption attribute values LED takes values set corresponding called corruption true given probability distribution general case observed true attributes conditional The definition universe augmented joint distribution class attribute From true actual attributes obtained true actual class model class distributions conditional true actual attributes respectively provide If assumed It argued tested fresh unclassified examples mechanism corrupts example descriptions learning work examples obtained learned concept description true class model role play actual class model generate examples assess learned concept purpose introducing learning overall effect noise resulting It follow paradoxical true actual allows affect attribute noise determined Schaffer LED domain probability corruption difficult In general particular set corrupting corrupting true class model distributions actual class model attributes 28 example investigates state distributions attribute distributions necessarily define universe nondegenerate distributions degenerate class uncertain single class In fact possible consequences increases noise increases separate example classification require elaborate model Errors ling Here sources classes present examples actually wrong class After learning acquired concept description teacher missclassification class distributions reflect RJ Hickey I Artificial Intelligence 82 1996 157179 165 present classifications Thus classification error mechanism Evaluation learning use true class distributionsnot required actual universe generate examples Two universes It appears literature example generation true universe little awareness aspect classification noise data testing usually generated according prescription learning 31 Irrelevant attributes In addition noise schema contribute cases distinguish irrelevant little classification There redundancy attributes present pure noise Definition 31 An attribute having property class model attributes appear appear representation relative said redundant Definition 32 An attribute satisfies Pclass c S A Pclass c 1 S c S A S subset vector description space attributes excluding A subset values taken description called pure noise attribute A pure noise attribute class attributes said informative A universe attribute uninformative said uninformative said said informative informative redundancy attribute relative subset example functionally dependent subset omission attribute The attributes Pure noise implies redundancy vice versa representation seen universe guarantee pure noise particular In universe 1 d redundant instantiated given representation distribution class model condition b b c c1 rules pure noise The class 01130263003750025000 indicating class distribution changes class 4 likely If d d instantiated 02120494001760011800 makes class 2 likely Some authors mean redundancy useful Only pure noise attribute circumstances information defined Since redundant example Pazzani Sarrett 22 use term irrelevant attributes contain unfortunate class use term irrelevant irrelevant sense truly knowledge value influence classification 166 RJ Hickey I Artificial Intelligence 82 1996 157179 The definition dence class attributesabbreviated Pearl definition independence attribute assertion conditional essentially conditional The implementation pure noise appears similar conditional pure noise attribute discussion indepen given description In conditioning description vector fully instantiated case pure noise Thus pure noise implies conditional 23 general discussion conditional conditional independence independence attribute appear universe 1 equivalent conversely The specification class model independence independence pure noise attribute achieved In universe 1 e description requirement addition vector description attributes independent pure noise Additional pure noise attributes added easily universe defined simply specifying marginal distribution adding corresponding vector term generating expression concept description redundancy omission By taking S set Definition 32 follows class pure noise attribute independent Clearly universe uninformative class distributions class model identical In case equal default distribution class In representation subinverses The notion informa rules define uninformative noise universe class tiveness attribute occasions distributions knowledge value use classification regardless noise universe course extent limits informative class model conditional attribute separate If attribute complexes informative appear It possible universe noise class distributions pure noise attributes The assessment attributes universe explicated informationtheoretic pursued informative concepts section noise individual 4 Assessing degree noise class distributions The question remains manipulate class distribution required noise In statistical modelling residual error employ normal distributions variance parameter practice achieve common indicating Let denote vector description attributes excluding The assertion independence independence class given implies pure noise conditional follows easily elementary probability pure noise readily seen equivalent independence clama theory This sufficient condition RJ Hickey I Artificial Intelligence 82 1996 157179 167 degree noise extent noise For class distributions small number means discrete nominal classes appropriate What needed explicating class universe 1 class distribution classes For example rule class model Table 1 05 05 0 0 0 0 0 distribution rule 0 Is noise distribution 055 0 0 045 0 0 0 O Such means comparison provided majorisation relation applied probability vectors Majorisation underpins theory measurement probabilities uncertainty information 41 Majorisation noise Measures uncertainty Shannons entropy function entropyP 2 pi In pi il Pp possible classes provide assessment noise based probabilities p C pi 1 probability distribution n Entropy possible measures uncertainty The unique wellknown additivity information property entropy renders positive multiples communication Gini index diversity Shannon Weaver 30 meaning relevant Another popular measure provided theory giniPlC n il p Information information identified uncertainty uncertainty lesser uncertainty usually regarded dual treated Noise greater Uncertainty measures usually defined strictly Schurconcave Hickey ll 121 respect preordering afforded majorisation relation probability distributions developed mathematical Majorisation theory inequalities provides total vectors real numbers having necessarily having number elements interpreted vector equal vector It partial ordering lacks antisymmetry vectors elements majorise preordering elements elements preordering differ permutation identical necessarily Applied discrete probability strict majorisation distributions distribution probabilities informative Here probability distributions For general background distributions notion noisy relative explicates equal noisy equivalently frequency majorisation outlined theory majorisation 19 168 R I Hickey I Artificial Intelligence 82 1996 157179 Majorisation redistributing larger probability defined notion equalising transfer pi ith jth involves having smaller probability probabilities j pi pj probabilities unchanged replaced pi pi excess p pi transferred created zero way render involved equal Formally new distribution P event P pl p p cpi 1 cp p 1 cp cpj 3 c 0 CC c 1 This leads definition majorisation P finite number equalising Q Definition 41 If discrete distributions P Q transformed Q said majorise P P demajorises Q written P Q If P Q Q permutation P Q said strictly majorise P P strictly demajorises Q written P Q transfers It follows majorisation swapping ith jth probabilities Thus Q permutation P P S Q 3 reformulated Q S P The expression transitive The case c 0 corresponds PQSy 4 S doubly stochastic matrix kk 1 ki j sii c sji 1 c sij 1 c sjj c elements zero It shown 19 P S Q 4 holds complex doubly stochastic matrix S If P Q P regarded uncertain noisier informative noise low noise synonymous interpretation Q With majorisation high concentration noise corresponds probability small number events high spread probability large number events The class distributions universe 1 related majorisation 0505000000 055000450000 single transfer 005 055 045 produces permutation lefthand A useful reformulation definition majorisation terms partial sums sorted probabilities Pl pm denote P 4 Q holds decreasing rearrangement vector given 19 Let P P 3 3pPrn1 ql qk I Pk 5 k 1 s k s n This suitable form computation RJ Hickey I Artificial Intelligence 82 1996 157179 169 The following useful properties majorisation P Q immediate conse quences 5 1 maxP maxQ max maximum function 2 The size support P number nonzero probabilities great Q 3 The smallest nonzero probability P large Q distributions It seen 5 majorisation 0307 defined modulo zero equally noisy probabilities application noise usually modelled fixed number Since uniform point classes distribution _ 1 n noisy majorised distribution n classes At end scale degenerate distribution noisy sense majorises distribution little consequence Amongst 0307000 IZ classes ln If classes universe combined class original combining probabilities amounts reverse resulting class distribution inequality transfer A concrete example use majorisation transfer provided commonly inversion classes known device introducing noise probability This mechanism classification noise process Angluin Laird 131 In twoclass problem suppose Q ql q2 class distribution Suppose Q inverted probability CY This results new class distribution P pl p2 class generated according model majorises equality Pl I hi aq2 P2 y41 I ahI2 3 6 3 Thus P Q More generally inversion probability LY split equally remaining IZ 1 classes 1 Y This generalizes diagonal elements aln 1 P Q 4 S diagonal elements nclass problem 6 42 Increasing noise universe 1 To illustrate majority use majorisation universe 1 noisier creating universe 2 demajorising class distribution This achieved leaving remaining probability evenly classes instead spreading single class case universe 1 This concentrated strategy description joint distribution attributes altered distribution employed class probability 14 The unchanged distribution redistribute The new class distributions shown Table 3 alongside universe 1 residual Notice probability majority class probability spread fairly evenly classes rule 11 unchangedthere In cases complement 170 RJ Hickey I Artificial Intelligence 82 1996 157179 Table 3 Class distributions universes 1 2 Rule number Universe 1 Universe 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0505000000 055000450000 0006000004 0000000406 0000300070 000000550450 0004006000 0307000000 0000600400 0006000400 OOOOO lOO 000035006500 0000050005 0400060000 0060000400 0000005050 05010100010101 0550050101005001005 0101060005005005005 0101005005000500506 005005005005000507005 000500500500505501501 0100500501060005005 010700500500050050 0100500506001 0050050600501005005005 OOOOOLOO 005005000500650101 01010050050501005005 0100500506005005005005 0050600500500500500501 01010101000505005 005005 Universe 2 different default class distribution It 0136401699005940137800531021240153400778 7 universe 1 given 2 0121501510004000211500445026750127000370 The default class class 6 probability dropped 5 It generally increasing noise reduce probability default class In section universe 2 altered slightly produce increase default class probability universe 1 case 43 Measures information uncertainty As step defining measures function monotone wrt majorisation needed Definition 42 A continuous discrete probability distributions Q If W4 strictly Schurconvex rconvex h w enever P Q 4 realvalued function 4 strictly Swconcave 4 space finite 4P Z 4Q P S strictly Schurconvex If 4 Schurconvex symmetry arbitrary Schurconvex formation measures guarantees familiar entropy function functions necessarily symmetric Ordinary convexity implies Schurconvexity 19 Although essential convexity important meaning fact equivalent property expected conditional information RJ Hickey I Artificial Intelligence 82 1996 157179 171 great unconditional variables X Y property distribution Expressed terms random 4X I Y 3 4Jm 8 If 4 8 strictly convex independent Accordingly equality occurs following definition X Y Definition 43 A realvalued probability distributions strictly convex concave weak continuous measure information If function function 4 space discrete symmetric said lacks strictness measure uncertainty Entropy Gini index symmetric strictly concave weak error weak measure uncer measures uncertainty The maximum information measure missclassification tainty function maxP strict The commonlyused function errorP 1 maxP convexity The development given extended carries idea majorisation strict convexity desirable observation Breiman et al 5 context selection measures CART continuous distributions natural way 1131 The measure information learning algorithm fact majorisation terms property All information measures render ordering informativeness 19 On hand distributions distributions related majorisation defined related majorisation dis measures order tributions assessed realvalued measure The inequality majorisation holding artefact measure indicative material difference generally better information increasing entropy noise majorisation differently Thus majorisation information stronger condition possible It reason greater manipulate information 44 Information universe Measures information useful providing overall summaries information content universe information class Definition 44 The rule information measure universe The defauZt information measure The information gain expectation class distributions class model default distribution respect universe respect information 172 RJ Hickey I Artificial Intelligence 82 1996 157179 1 rule information default information 1 A dual definition holds uncertainty measures By virtue strict Definition 43 information gain zero universe convexity uninformative The rule information assesses contribution class The benefit attributes provided information universes universe class distributions permutations distribution P identical The uninformative universe class distributions equal P rule information zero information gain terms noise content rule information Suppose gain Care attributes identifying relative default information taken comparing When information measure max function default rule informations interpretations classification rates Definition 45 When informations UCR respectively called information measure default rule max default universe classification rates DCR Increasing noise class distribution rule increase formation measure weak information gain increase decrease information In particular representation decrease universe UCR decrease The substantial Table 4 shows information statistics entropy max universes 1 identification class 2 It clear description attributes class provided attribute For universes b max No single attribute classify satisfactorily Because universe 2 information default Also shown information judged entropy informative facilitate attribute Table 4 Information shown brackets statistics relating entropy max universes 1 2 ranks attribute Information Universe 1 Default Rules Gain Attributes b c d e Entropy 18742 05976 12766 16728 13936 16420 18526 18742 3 1 2 4 5 Max 02675 06540 03865 03622 04250 03395 02675 02675 2 1 3 4 5 Universe 2 Entropy 19847 11849 07998 18853 16761 18921 19728 19847 2 1 3 4 5 Max 02124 06540 04416 02922 03649 02316 02151 02124 2 1 3 4 5 RJ Hickey I Artificial Intelligence 82 1996 157179 173 constructed distribution unaltered universe 1 leaving majority probability class UCR 654 universe 1 15 observed Even small universe provide substantial algorithms Holte Machine Learning Repository informative contributing universes attributes needed effective classification task learning real data sets 21 appear possess single attribute little With method artificial simple build small manageable universe comparatively 5 Evaluating learning The result concept learning examples usually mechanism means deciding given description vector classification class Some learning algorithms produce nondeterministic corresponding offer choice possible classes indication classification For example trained neural network degree uncertainty tree relative strengths output units decision observed classes leaves Some rule induction algorithms frequencies CN2 Clark Boswell 7 d m uce overlapping rules The user decide rule selection particular class creating deterministic classifier It assumed classifiers produce definite class occasion attached Definition 51 A determintiitic classifier mapping set description vectors set classes A statement mapping called representation classifier The universe provides best classifier optimal probability correct classification Definition 52 A best classifier associated universe maps description vector majority class associated class distribution class model distribution A default classifier maximum probability universe class default dis tribution assigns description vector majority Because ties class distribution best default classifiers unique Concept learning characterised examples With forms learning examples networks explicit To obtain description vectors supplied For small universes discrete attributes estimation best classifier neural implicit classifier possible resulting classifications noted present problem For learning extensional classifier obtained instancebased form 174 RJ Hickey I Artificial Intelligence 82 1996 157179 continuous large cases present attributes difficulties One possibility estimate classifier second stage learning A large sample description vectors supplied classifier noiseless example set passed learning classification The resulting algorithm generates true classifier rules The rule set obtained approximation involved Evaluating extensionally provides set deterministic available classifier straightforward The classifier rules form complex class calculated universe specification classification rate Definition 53 The probability classifies randomly selected condition value actual classification probability rate ACR called actual classification probability rule The expected classifier actual clussiJication universe satisfies induced classification rule correctly example The ACR classifier exceed universe rate default classifier The UCR classification best classifier UCR Definition 45 classifier sufficiently bad DCR rate It An ACR associated classifier learned data randomly generated relationship random variable The expected ACR example sets universe particular size useful indicator effectiveness learning algorithm set The learning curve The expected ACR example set size difficult number trials compute learning algorithms ID3 estimated expected ACR size example 51 Experiments 103 universes 1 2 To examine effect increased noise learning ID3 number trials performed universes 1 2 Each trial consisted generating example majority class leaf deterministic tree universe set particular classifier obtained adopting size inducing tree ID3 producing mild form pruning ACR classifier probabilities computing To estimate expected ACRs learning curve number trials carried range example set sizes 5 5000 Because high small sizes large number replications variability carried 4000 size 5 number replications decreasing size increased 25 size 5000 ACR obtained The results experiments shown Table 5 learning curves RJ Hickey I Artificial Intelligence 82 1996 157l 79 175 Table 5 Estimates expected ACR expressed percentage sets generated universes 1 2 estimated standard errors given percentages rules sets induced ID3 example Size No trials Universe 1 Universe 2 Est expected ACR Est standard error DCR 2675 UCR 654 Est expected ACR Est standard error DCR 2124 UCR 654 5 8 10 12 15 20 30 50 75 100 200 300 500 750 1000 1250 1500 2000 3000 5000 4000 4000 2000 2000 2000 1000 1000 500 500 200 100 100 100 100 100 100 50 50 50 25 292 348 375 398 421 451 482 510 524 536 560 573 591 607 616 622 624 631 639 645 012 013 018 017 016 020 016 016 012 016 019 020 013 009 010 009 011 009 008 006 235 278 300 317 336 357 391 425 444 466 512 538 517 604 616 625 631 639 646 649 011 013 018 018 018 024 021 024 018 025 029 023 014 011 008 007 007 006 004 004 size 100 displayed Fig 1 The expected ACR universe 1 consistently increased noise universe 2 presents considerable difficulties ID3 For larger example set sizes universe 2 appears slightly better differences statistically significant sizes 1500 The explanation universe 2 size 750 showing 70 60 50 40 g Q g g 30 j 20 IO 0 0 20 40 60 80 Ex set size Universe 1 Uniwrse 2 UCR 645 100 Fig 1 ID3 learning curves example sets size 100 universes 1 2 176 RJ Hickey I Artificial Intelligence 82 1996 157179 Table 6 Class distributions Rule number universe 3 Class distribution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0501005000200501 055005010050050150005 OOSOOS 060005015005005 005005005005001500506 005005005005000507005 000500500500505501501 005005005006015005005 005070050050010050 005005005060015005005 0050050600500501005005 OO oo 0 lO 0 005005000500650101 0100500500505015005005 0050050050600501005005 0050600500500501005005 0050100500500205005 single minority class universe 1 emerging occasions universe 2 unlikely majority small residual probabilities datasomething happen It argued better expected ACRs obtained universe 1 smaller sample sizes superior default classification rate DCR Table 4 DCR universe 1 2675 universe 2 2124 universe 1 considerable head start universe 2 To investigate universe 2 modified slightly produce DCR close universe 1 This achieved transferring class class 6 class distribution rules description statistics universe 3 shown Table 7 The default distribution attributes define universe 3 shown Table 6 The information universe The new distributions small probability joint distributions majority Table 7 Information shown brackets statistics universe 3 ranks attribute Information Default Rules Gain Attributes b e Entropy 19453 11692 07761 18391 16217 18412 19326 19453 2 1 3 4 5 Max 02705 06540 03835 03293 03825 02736 02705 02705 2 I 3 4 5 RJ Hickey I Artiial Intelligence 82 1996 157179 177 Table 8 Estimates expected ACR expressed percentage sets generated universes 1 3 estimated standard errors given percentages rules sets induced ID3 example Size No trials Universe 1 Universe 3 Est expected ACR Est standard error DCR 2675 UCR 654 Est expected ACR Est standard error DCR 2705 UCR 654 4000 4000 2000 2000 2000 1000 1000 500 500 200 100 100 100 100 100 100 50 50 50 25 292 348 375 398 421 451 482 510 524 536 560 573 591 607 616 622 624 631 639 645 012 013 018 017 016 020 016 016 012 016 019 020 013 009 010 009 011 009 008 006 5 8 10 12 15 20 30 50 75 100 200 300 500 750 1000 1250 1500 2000 3000 5000 3 248 292 313 330 350 367 403 435 457 474 518 553 580 605 620 627 633 640 646 650 012 013 018 018 017 023 019 022 019 022 024 016 014 010 008 007 008 007 003 002 0112901666005510124500525027050141900760 universe 1 comparison DCR 2705 slightly universe 1 2675 Trials similar described universes 1 2 carried ID3 learning curve universe 3 The results shown Table 8 estimate It seen pattern alongside universe 3 follows universe 2 ACRs universe 1 sample sizes 750 universe 3 creeps ahead small statistically significant Thus results universe 3 comparable universe 2 similar regards DCR caused The lower ACRs universes 2 3 demajorising class distributions appear 6 Conclusion The method artificial universes class model central notion By focusing class distributions model means describing noise 178 RJ Hickey I Artificial Intelligence 82 1996 157179 tasks varying degrees difficulty inclusion redundant irrelevant attributes setting learning Smallish universes 5 10 attributes provide challenging remain comprehensible relation affords simple practical way manipulating noise The majorisation uncertainty measures gives levels link information indicators unified account seemingly different ID3 results facilitated tasks experimenter experimental process increasing greatly class distributions leads poorer learning curves It appear noise demajorising References l DW Aha Generalising Proceedings Ninth International Conference Mateo CA 1992 l10 case studies case study D Sleeman P Edwards eds Morgan Kaufmann San Machine Learning 2 DW Aha D Kibler M Albert Instancebased learning algorithms Mach Learning 6 1991 3766 3 D Angluin P Laird Learning noisy examples Mach Learning 2 1988 343370 4 LB Booker DA Goldberg JH Holland Classifier systems genetic algorithms Artif Intell 40 1989 235282 5 L Breiman J Friedman R Olshen C Stone Classification Regression Trees Wadsworth Belmont CA 1984 6 P Cheeseman classification Morgan Kaufmann San Mateo CA 1988 5464 J Kelly M Self J Stutz W Taylor D Freeman AUTOCLASS Bayesian Machine Learning International Conference Proceedings Fifth 7 P Clark R Boswell Rule induction CN2 recent improvements Y Kodratoff ed EWSL91 SpringerVerlag Berlin 1991 151163 8 P Clark T Niblett The CN2 induction algorithm Mach Learning 3 1989 261283 9 S Cost S Salzberg A weighted nearest neighbour algorithm learning symbolic features Mach Learning 10 1993 5778 lo D Haussler Quantifying inductive bias AI learning algorithms Valiants learning frame work Artif Intell 36 1986 177221 ll RJ Hickey A note measurement randomness 12 RJ Hickey Majorisation randomness discrete distributions J Appl Prob 19 1982 229232 J Appl Prob 20 1983 897902 13 RJ Hickey Continuous majorisation randomness 14 RJ Hickey Artificial universes learn examples Conference Machine Learning evaluating algorithms D Sleeman P Edwards eds Proceedings Ninth International systematic approach Morgan Kaufman San Mateo CA 1992 196205 J Appl Prob 21 1984 924929 15 RC HolteVery simple classification rules perform commonly datasets Mach Learning 11 1993 6391 16 D Kibler P Langley Machine Proceedings Third European Working Session Learning 8192 learning experimental science D Sleeman ed Pitman Glasgow Scotland 1988 17 P Langley W Iba K Thompson An analysis Bayesian classifiers Proceedings AAAI92 San Jose CA MIT Press Cambridge MA 1992 223228 18 H Lounis GM Bisson Evaluation learning systems artificial databased approach Y Kodratoff ed EWSL91 SpringerVerlag Berlin 1991 463481 19 AW Marshall I Olkin Inequalities The Theory Majorisation Applications Academic Press New York 1979 RJ Hickey I Artificial Intelligence 82 1996 157179 179 20 J Mingers An empirical comparison pruning methods decision tree induction Mach Learning 4 1989 227243 21 PM Murphy DW Aha UC1 repository machine learning databases Maintained Department Information Computer Science University California Irvine CA 1992 learning 22 MJ Pazzini W Sarrett A framework average case analysis conjunctive algorithms Mach Learning 9 1992 349372 23 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Znference Morgan Kaufmann San Mateo CA 1988 24 JR Quinlan Induction decision trees Mach Learning 1 1986 81106 25 JR Quinlan Simplifying decision trees Znt J ManMach Stud 27 1987 221234 internal representations 26 DE Rumelhart GE Hinton RJ Williams Learning error DE Rumelhart JL McClelland PDP Research Group eds Parallel propagation Distributed Processing 1 MIT Press Cambridge MA 1986 318362 27 Y Sakakibara Noisetolerant occam algorithms applications learning decision trees Mach Learning 11 1993 3762 28 C Schaffer Sparse data effect overfitting avoidance decision tree induction Proceedings AAAZ92 San Jose CA MIT Press Cambridge MA 1992 147152 29 C Schaffer Overfitting avoidance bias Mach Learning 10 1993 153178 30 CE Shannon W Weaver Mathematics Communication Theory University Illinois Press Urbana IL 1964 31 LG Valiant A theory learnable Commun ACM 27 1984 11341142 32 LG Valiant Learning conjunctions disjunctions Proceedings ZJCAZ85 Los Angeles CA Morgan Kaufmann San Mateo CA 1985 560566