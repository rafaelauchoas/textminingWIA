Journal Preproof Entropy Estimation Uniformization Ziqiao Ao Jinglai Li PII DOI S0004370223001005 httpsdoiorg101016jartint2023103954 Reference ARTINT 103954 To appear Artiﬁcial Intelligence Received date 4 June 2022 Revised date 1 June 2023 Accepted date 3 June 2023 Please cite article Z Ao J Li Entropy Estimation Uniformization Artiﬁcial Intelligence 103954 doi httpsdoiorg101016jartint2023103954 This PDF ﬁle article undergone enhancements acceptance addition cover page metadata formatting readability deﬁnitive version record This version undergo additional copyediting typesetting review published ﬁnal form providing version early visibility article Please note production process errors discovered affect content legal disclaimers apply journal pertain 2023 Published Elsevier Entropy Estimation Uniformization School Mathematics University Birmingham Birmingham B15 2TT UK Ziqiao Ao Jinglai Li Abstract Entropy estimation practical importance information theory statis tical science Many existing entropy estimators suﬀer fast growing esti mation bias respect dimensionality rendering unsuitable high dimensional problems In work propose transformbased method highdimensional entropy estimation consists following main ingredients Firstly provide modiﬁed knearest neighbors kNN entropy estimator reduce estimation bias samples closely resembling uni form distribution Second design normalizing ﬂow based mapping pushes samples uniform distribution relation entropy original samples transformed ones derived As result entropy given set samples estimated ﬁrst transforming uniform distribution applying proposed estima tor transformed samples The performance proposed method compared existing entropy estimators mathematical examples realworld applications Keywords entropy estimation k nearest neighbor estimator normalizing ﬂow uniformization 2010 MSC 0001 9900 1 Introduction 5 Entropy fundamental concept information theory applica tions ﬁelds physics statistics signal processing machine learning For example statistics data science contexts ap plications rely critically estimation entropy including goodnessofﬁt testing 1 2 sensitivity analysis 3 parameter estimation 4 5 Bayesian experimental design 6 7 In work focus continuous version entropy takes form cid2 HX logpxxpxxdx 1 Email addresses zxa029bhamacuk Ziqiao Ao jli10bhamacuk Jinglai Li Preprint submitted Journal LATEX Templates June 7 2023 10 15 20 25 30 35 40 45 50 pxx probability density function PDF random variable X Despite simple deﬁnition entropy admits analytical expres sion limited family distributions needs evaluated numerically general When distribution analytically available princi ple entropy estimated numerical integration schemes Monte Carlo method However realworld applications distribu tion analytically available estimate entropy realizations drawn target distribution makes diﬃcult impossible directly compute entropy numerical integration Entropy estimation attracted considerable attention com munities decades numerous methods developed directly estimate entropy realizations In work consider nonparametric approaches assume parametric model target distribution methods broadly classiﬁed cate gories The ﬁrst class methods known plugin estimators ﬁrst estimate underlying probability density compute integral Eq 1 numerical integration Monte Carlo 8 detailed description Some examples density estimation approaches studied plugin methods kernel density estimator 9 10 11 12 togram estimator 13 10 ﬁeldtheoretic approach 14 A major limitation type methods rely eﬀective density estimation diﬃcult problem right especially dimensionality problem high A diﬀerent strategy directly estimate entropy independent samples random variable Popular methods falling category include samplespacing 15 knearest neighbors kNN 16 17 based estimators The particularly appealing exist ing estimation methods thanks theoretical computational advantages widely practical problems Eﬀorts constantly voted extending improving kNN methods recent variants extensions methods 18 19 20 It worth mentioning types direct entropy estimators available For example Ariel Louzoun 21 decoupled target entropy sum entropy marginals estimated onedimensional methods entropy copula estimated recursively splitting data statistically dependent dimensions Kandasamy et al 22 suggested leaveoneout tech nique von Mises expansion based estimator 23 We note certain applications main purpose minimize maximize quantity entropy case entropy gradient estimation strategies 24 25 explored avoid direct entropy estimation It known entropy estimation increasingly diﬃ cult dimensionality grows diﬃculty mainly es timation bias decays slowly respect sample size high dimensional problems For example popular approaches including kNN method 16 estimation bias decays rate ON γd N sample size d dimensionality γ positive constant 26 22 27 28 As result existing entropy 2 55 60 65 70 75 80 85 90 estimation methods eﬀectively handle highdimensional problems making strong assumptions smoothness underlying distribu tion 22 Indeed wellknown minimax bias results 29 30 indicate strong smoothness assumption 22 curse dimensional ity unavoidable However eﬀorts reduce diﬀerence actual estimation bias theoretical bound The main goal work provide eﬀective entropy estimation approach achieve faster bias decaying rate mild smoothness assumption eﬀectively deal highdimensional problems The method presented consists main ingredients First propose trun cated kNN estimators based 16 17 respectively provide bounds estimation bias estimators Interestingly theoretical results suggest estimators achieve zero bias uniform dis tributions result existing kNN based estimators according bias analysis available date 27 31 32 This property oﬀers possibility signiﬁcantly improve performance entropy estimation mapping data points uniform distribution procedure refer uniformization Therefore second main ingredient method conduct uniformization data points normalizing ﬂow NF technique 33 34 Simply speaking NF constructs sequence invertible diﬀerentiable mappings transform simple base distribution standard Gaussian complicated distribution density function available Speciﬁcally use Masked Autoregressive Flow 35 NF algorithm originally developed density estimation combined probability integral transform push original data points uni form distribution We estimate entropy resulting nearuniform data points proposed truncated kNN estimators derive original ones accordingly adding entropic correction term transformation Therefore combining truncated kNN estimators normalizing ﬂow model able decode complex highdimensional distribution represented realizations obtain accurate estimation entropy The rest paper organized follows In Section 2 traditional kNN based methods entropy estimation convergence properties In Section 3 introduce truncated kNN estimators dis tributions compact support combine new estimators NFbased uniformization procedure estimate entropy general distributions Numerical examples applications presented Sections 4 Section 5 respectively demonstrate eﬀectiveness proposed methods Finally Section 6 summarize ﬁndings discuss future research directions 95 2 kNN Based Entropy Estimation We provide brief introduction commonly kNN based entropy estimators section We start original kNN entropy estimator 3 100 proposed 16 kth nearest neighbor contained smallest possible closed ball Next introduce popular variant kNN estimator proposed 17 method uses smallest possible hyperrectangle cover k points We ﬁnally discuss theoretical analysis estimation errors estimators 21 KozachenkoLeonenko Estimator Recall deﬁnition entropy Eq 1 Given density estimator cid3pxx i1 drawn pxx pxx set N iid samples S xiN entropy random variable X estimated follows cid3HX N 1 Ncid4 i1 log cid3pxxi 2 The KozachenkoLeonenko KL estimator depends local uniformity sumption obtain estimate cid3pxx For xi ﬁrst identiﬁes knearest neighbors terms pnorm distance deﬁnes smallest closed ball covering k neighbors Bxi cid3i2 x Rd cid5 cid5 cid3x xicid3p cid3i2 cid3i twice distance xi kth nearest neighbor set S We shall refer closed ball Bxi cid3i2 cell centered xi let qi mass cell Bxi cid3i2 cid2 qicid3i xBxicid3i2 pxxdx It derived expectation value log qi cid3i given Elog qi ψk ψN 3 ψx Γcid2x assumes density constant Bxi cid3i gives Γx Γx Gamma function 17 KL estimator qicid3i cdcid3d pxxi 4 d dimension X cd Γ1 1 p dΓ1 d p volume ddimensional unit ball respect pnorm Combining 3 4 estimate logdensity sample point log cid3pxxi ψk ψN log cd d log cid3i 5 Plugging estimates 1 N 2 yields KL estimator cid3HKLX ψk ψ N log cd d N Ncid4 i1 log cid3i 6 4 22 KSG Estimator As mentioned earlier KraskovStogbauerGrassberger KSG estima tor important variant ˆHKL Unlike KL estimator based closed balls KSG estimator uses hyperrectangles form cells data point Namely chooses norm distance metric p result cell Bxi cid3i2 hypercube length cid3i Next allow hypercube hyperrectangle cells admit dif ferent lengths diﬀerent dimensions Speciﬁcally j 1 d deﬁne cid3ij twice distance xi kth nearest neighbor dimension j cell centered xi covering knearest neighbors Bxi cid3i1d2 x x1 xd xj xi j cid3 ij2 j 1 d 7 cid3i1d cid3i1 cid3id This change leads diﬀerent formula comput ing mass cell Bxi cid3i1d2 Elog qi ψk d 1 k ψN 8 It worth noting equality Eq 3 replaced approximate equality Eq 8 uniform density rectangle assumed obtain Eq 8 Lemma 2 Appendix A2 details Using similar local assumption Eq 4 KSG estimator derived cid3HKSGX ψk ψ N d 1 k 1 N Ncid4 dcid4 i1 j1 log cid3ij 9 105 110 115 120 We note KSG method actually developed context esti mating mutual information 17 reported outperform KL estimator wide range problems 27 As shown straightforward extend entropy estimation numerical experi ments suggest competitive performance entropy estimator demonstrated Section 4 23 Convergence Analysis Another important issue analyze estimation errors entropy estimators especially behave sample size increases In kNN based estimators including mentioned variance generally controlled decaying rate ON 1 N sample size main issue lies estimation bias In fact bias estima tor cid3HKL studied cid3HKSG receives little attention Previous results related listed follows The original 16 paper established asymptotic unbiasedness k 1 36 obtained result general k For distributions unbounded support 37 proved 5 Figure 1 The schematic illustration truncated estimator The shaded area removed kNN cell ford 1 bias bound decays rate O 1 N higher dimensions obtaining bias bound ON 1 d polylogarithmic factors For distributions compactly supported usually densities satisfying βHolder condition considered 32 gave quickanddirty upper bound bias ON β simple class univariate densities supported 0 1 d β 0 2 bounded away zero general d additional conditions boundary support We reinstate works obtained variance bound ON 1 31 proved bias ON β 27 generalized It noted bias bounds given previous studies typically depend properties target densities smoothness parameter Hessian matrix providing insights estimators perform certain distributions This motivates idea transform given data points desired distribution accurate entropy estimation detailed section 125 130 135 3 Uniformizing Mapping Based Entropy Estimation In section present proposed approach As mentioned earlier consists main ingredients truncated version kNN entropy estimators transformation map data points uniform distribution 140 31 Truncated KLKSG Estimators For compactly supported distributions signiﬁcant source bias comes boundary support kNN cells constructed cluding areas outside support distribution density 31 Intuitively 6 speaking incorrectly including areas results underestimate den sities leading bias estimator We propose method reduce estimation bias excluding areas outside distribution support remarkably resulting estimator enjoy certain convergence properties enable design NF based estimation approach The additional requirement estimators bound support density speciﬁed Without loss generality suppose target density supported unit cube Q 0 1d Rd The procedure method follows ﬁrst determine cells KL KSG ex amine kNN cell covers area distribution support truncate cell boundary exclude area Fig 1 schematic illustration Mathematically truncated KL tKL estimator norm given cid3HtKLX ψk ψN 1 N Ncid4 dcid4 i1 j1 log ξij 10 j cid3i2 1 maxxi truncated KSG tKSG esitmator given ξij minxi j cid3i2 0 cid3HtKSGX ψk ψ N d 1k 1 N Ncid4 dcid4 i1 j1 log ζij 11 ζij minxi j cid3ij2 1 maxxi j cid3ij2 0 Next shall theoretically analyze bias truncated estimators Our analysis relies assumptions density function px summarized Assumption 1 The distribution px satisﬁes px continuous supported Q b px bounded away 0 C1 inf xQ c The gradient px uniformly bounded Qo C2 sup xQo pxx 0 cid2pxx1 First consider bias estimator cid3HtKL following theorem states bias cid3HtKL bounded vanishes rate ON 1 d 145 150 Theorem 1 Under Assumption 1 ﬁnite k d bias truncated KL estimator bounded cid5 cid5E cid3HtKLX HX cid5 cid5 C2 C 11d 1 cid7 1 d cid6 k N 7 The variance truncated KL estimator bounded Var cid3HtKLX C 1 N C 0 Proof We provide skeleton proof complete proof including notations detailed Appendix A3 Appendix A4 Proof bias bound truncated KL estimator proceeds follows 1 Show E cid3HtKLX E cid8 log P Bx cid3k2 μBx cid3k2 cid9 2 Bound following diﬀerence cid5 cid5 cid5 cid5 log px log P Bx cid3k2 μBx cid3k2 cid5 cid5 cid5 C2 cid5 2C1 cid3k 12 13 3 Note HX Elog px Eq 12 Eq 13 upper bound Ecid3k obtained Lemma 4 derive bias E cid3HtKLX bounded cid5 cid5E cid3HtKLX HX cid7 1 d cid5 cid5 14 C2 C 11d 1 cid6 k N 155 Proof variance bound truncated KL estimator proceeds fol lows 1 Let αi cid10 d j1 log ξij let α 2 N estimators sample x1 removed Then EfronStein inequality 38 Var cid3HtKLX Var cid11 1 N Ncid4 i1 cid12 cid11cid13 αi 2N E 1 N Ncid4 i1 αi 1 N Ncid4 i2 α cid142cid12 2 Let 1Ei indicator function event Ei cid3kx1 cid7 cid3 kx1 twice kNN distance x1 α 15 kx1 Then αi cid142 1 N Ncid4 i2 α cid13 1 Ckd α2 1 2 cid14 1Eiα2 α2 Ncid4 i2 16 Ckd constant 3 Since αi α upper bounds following expectations Eα2 N 1E1E2α2 2 identically distributed need derive 1 N 1E1E2α2 2 160 8 cid3 cid13 Ncid4 N 2 1 N i1 4 Finally obtain bound variance cid3HtKLX Var cid3HtKLX C 1 N 17 C 0 Note C2 0 px uniform Q following corollary follows directly 165 Corollary 1 Under assumption Theorem 1 X uniformly distributed Q truncated KL estimator unbiased 170 175 This corollary theoretical foundation proposed method suggests transform data points uniform distribution tKL method yield unbiased estimate In reality usually im possible map data point exactly uniform distribution achieve unbiased estimate To end Theorem 1 suggests long transformed samples close uniform distribution sense C2 small transformation signiﬁcantly reduce bias Since main contribution meansquare estimation error comes bias variance decays rate ON 1 reducing bias leads accurate estimation entropy We consider bias tKSG estimator The second theorem shows expectation cid3HtKSG limiting behavior polylogarithmic factor N Theorem 2 Under Assumption 1 ﬁnite k d bias truncated KSG estimator bounded cid5 cid5E cid3HtKSGX HX cid5 cid5 C log N k2 C k1 1 N 1 d C 0 The variance truncated KSG estimator bounded Var cid3HtKSGX C cid5 log N k2 N 180 C cid5 0 Proof Again provide skeleton proof complete details given Appendix A5 Appendix A6 Proof bias bound truncated KSG estimator proceeds follows 1 Suppose cid15P cid15p cid15qcid3 x1 k cid3 xd k x deﬁned Lemma 2 d j1 log ζij identically cid10 d Lemma 2 fact l px 1 distributed E cid3HtKSGX E xp cid8 E P log ζ x1 k ζ xd k cid9 E xp E cid2P cid6 cid8 log pxcid3x1 k cid3 xd k cid7cid9 18 9 2 We separate ddimensional unit cube Q subsets Q Q1 Q2 cid6 cid7 1 d andQ 2 Q Q1 185 Q1 aN 2 1 aN 2 d aN 2k log N C1N 3 Note HX Elog px decompose bias terms according separation unit cube cid5 cid5E cid3HtKSGX HX cid5 cid5 cid7cid9 cid6 cid5 cid5 E xp ζ xd k ζ x1 k log cid8 E P I1 I2 I3 cid5 cid5 cid6 cid8 log cid3x1 k cid3 xd k cid7cid9 cid5 cid5 cid5 cid5 E xp E cid2P 19 I1 I2 I3 cid5 cid5 cid5 cid5 E xQ2 cid5 cid5 cid5 cid5 E xQ1 cid5 cid5 cid5 cid5 E xQ cid8 log E P cid3kaN cid6 cid6 ζ x1 k ζ xd k ζ x1 k ζ xd k cid7cid9 cid7cid9 cid8 log cid6 log ζ x1 k ζ xd k cid7cid9 E P cid3kaN cid8 E P cid3kaN cid5 cid5 cid5 cid5 cid5 cid5 cid5 cid5 E xQ2 E cid2P cid3kaN cid8 E xQ1 cid5 cid5 cid5 cid5 E xQ cid5 cid5 cid5 cid5 E cid2P cid3kaN cid8 E cid2P cid3kaN cid6 cid8 log cid3x1 k cid3 xd k cid7cid9 cid5 cid5 cid5 cid5 cid6 log cid3x1 k cid3xd k cid7cid9 cid6 log cid3x1 k cid3 xd k cid7cid9 cid5 cid5 cid5 cid5 cid5 cid5 cid5 cid5 20 means taking expectation probability measure P cid3xj E P cid3kaN k aN j 1 d 4 Finally bounding terms separately obtain cid5 cid5E cid3HtKSGX HX cid5 cid5 C log N k2 C k1 1 N 1 d 21 C 0 190 1 Let βi cid10 Proof variance bound truncated KSG estimator proceeds ﬂlows d j1 log ζij deﬁne β sample x1 removed Next N 1E1E2β2 1E1E2β2 Eβ2 cid8 2 Separate E 2 order Eβ2 1 O log N k2 Steps 2 3 cid9 2 N estimators 2 N 1 As need prove β2 1 parts cid8 cid9 E β2 1 E xQ E P cid3kaN cid9 cid8 β2 1 E xQ E P cid3kaN cid8 cid9 β2 1 22 aN cid6 cid7 1 d 2k log N C1N 3 By bounding parts separately obtain bound expec tation β2 1 195 C9 0 Eβ2 1 C9log N k2 23 10 4 With bound obtain bound variance cid3HtKSGX Var cid3HtKSGX C cid5 log N k2 24 N C cid5 0 200 205 210 215 As Theorem 2 uniform distribution leads zero bias cid3HtKL obtain result cid3HtKSG means theoretical justiﬁcation mapping data points uniform distri bution estimator That said tKSG estimator Theorem 2 useful reason twofold First mentioned earlier existing result bound bias available KSG estimator best knowledge end analysis tKSG ﬁrst known bias bound type estimators provide useful information understanding convergence property More importantly numer ical experiments demonstrate mapping data points uniform distribution signiﬁcantly improve performance tKSG In fact tKSG achieve slightly better results tKL transformed samples test cases 32 Estimating Entropy Transformation As mentioned earlier based interesting convergence properties truncated estimators particularly tKL want estimate entropy given set samples mapping uniform distribution To implement idea essential question ask entropy transformed samples relates original ones Proposition 1 provides answer question Proposition 1 39 Let f mapping Rd Rd X random variable deﬁned Rd following distribution px Z f X If f bijective diﬀerentiable cid2 HX H Z pzz log cid5 cid5 cid5 cid5 det cid5 cid5 cid5 cid5dz f 1z z 25 pzz distribution Z Therefore given data set S xiN i1 mapping Z f X Eq 25 construct entropy estimator X cid3HX cid3HZ cid5 cid5 cid5 cid5 det log f 1zi z cid5 cid5 cid5 cid5 1 n ncid4 i1 26 cid3HZ entropy estimator ofZ tKL tKSG based transformed samples SZ zi f xin i1 220 11 We refer mapping f uniformizing mapping UM resulting methods UM based entropy estimators main procedure outlined Algorithm 1 A central question implementation Algo rithm 1 obviously construct UM push samples uniform distribution discussed section The bias UM based estimators rely property UM equivalently NF following assumption 225 Assumption 2 Let S xiN struct UM pS sup zQo positive integer M positive real number C 1 i1 set iid samples 2 0 2 There exist z resulting density Z Eq 26 Denote C N z z1 assume C N 2 satisﬁes 1 C N 2 P N cid2pS N M C N 2 C Based Theorem 1 Theorem 2 obtain bias bounds MSE bounds UM based estimators Corollary 2 Suppose density function original distribution diﬀerentiable UM satisﬁes Assumption 2 The bias UMtKL estimator bounded cid5 cid5E cid3HUMtKLX HX cid5 cid5 C N UMtKL cid7 1 d cid6 k N lim N C N UMtKL 0 The MSE UMtKL estimator bounded E cid3HUMtKLX HX2 C1 1 N DN UM tKL cid7 2 d cid6 k N 27 28 230 C1 positive constant lim N DN UM tKL 0 Proof See Appendix B Corollary 3 Suppose density function original distribution diﬀerentiable UM satisﬁes Assumption 2 The bias UMtKSG esti mator bounded cid5 cid5E cid3HUMtKSGX HX cid7 1 Cd1 1 C cid6 CUM tKSG C UMtKSG estimator bounded 1 Ck1 cid5 cid5 CUM tKSG log N k2 N 1 d 29 C positive constant The MSE E cid3HUMtKSGX HX2 C2 log N k2 N C2 positive constant DN UM tKSG cid16 C 12 DN UM tKSG log N 2k2 N cid7 1 Cd1 cid172 2 d 30 1 Ck1 cid6 1 C Proof See Appendix C Algorithm 1 UM based entropy estimator Input set iid samples SX xi Output entropy estimate cid3HX compute uniformizing map f let SZ zi f xi 1 n estimate cid3HZ SZ Eq 10 Eq 11 compute cid3HX Eq 26 33 Constructing UM Normalizing Flow We discuss section construct UM NF method First image f 0 1d assume f form f Φ g g Rd Rd learned Φ Rd 0 1d prescribed Recall pz distribution Z f X withX following px want function g minimize KullbackLeibler divergence KLD pz uniform distribution pu cid2 Dpzpu pzz log min gΩ cid11 pzz puz cid12 dz 31 235 z Φ gx Ω suitable function space Solving Eq 31 directly poses computational diﬃculty calculation involves function Φ choice aﬀect computational eﬃciency To simplify computation recall following proposition Proposition 2 34 Let T Y Z bijective diﬀerentiable transfor mation pzz distribution obtained passing pyy T πzz distribution obtained passing πyy T Then equality Dπyypyy Dπzzpzz 32 holds We construct mapping Φ cumulative distribution func tion standard normal distribution technique known probability integral transform yielding given y Rd Φy φ1y1 φdyd φiyi 1 2 1 erf y 2 erf error function It clear y follows standard normal distribution z Φy follows uniform distribution 0 1d vice 13 versa Now applying Proposition 2 Eq 31 equivalent Dpyyqy min gΩ 33 y gx follows distribution py andq standard normal distri bution Now assume g invertible let inverse h g1 We assume g h diﬀerentiable Applying Proposition 2 Eq 33 T h ﬁnd Eq 33 equivalent min hΩ1 Dpxxqhx 34 Ω1 g1g Ω qh distribution obtained passing q mapping h qhx q cid6 h1x cid13 cid7 cid5 cid5 cid5 cid5 det cid14 cid5 cid5 cid5 cid5 h1 x 35 Eq 34 essentially says want push standard normal distribution q target distribution px solving Eq 34 falls naturally framework NF Speciﬁcally NF aims build mapping h composing multiple simple mappings h h1 hK Each hk needs diﬀeomorphism invertible inverse diﬀerentiable ensures composition h diﬀeomorphism Next plugging data rewrite Eq 34 maximum likelihood problem max hh1hK Epxlog qhx 1 N Ncid4 i1 log qhxi 36 As mentioned earlier intermediate mapping hi usually taken simple parametrized form gradient inverse easy compute Once h1 hK computed function g obtained g h1 hK 1 h1 K h1 1 37 recall Eq 13 main paper need detJacobian mapping g1 h calculated det g1y y det h1y1 y1 det hKyK yK 38 240 yK y y0 x yk1 hkyk k 1 K The NF methods depend critically component layers choice balanced computational eﬃciency representing ﬂexibility In paper use special version NF Masked Autoregres sive Flow MAF 35 originally designed density estimation Since purpose MAF estimate density px speciﬁcally designed eﬃciently evaluate inverse mappings particularly useful 245 14 250 255 260 265 application We note method rely speciﬁc implementation NF Once mapping h equivalently g1 obtained inserted directly Algorithm 1 estimate sought entropy In practice sam ples split sets construct UM estimate entropy 4 Numerical Experiments Before diving applications conduct numerical compar isons proposed estimators mathematical examples The code producing examples httpsgithubcomziqaoNFEE 41 An Illustrating Example Truncated Estimators Here use toy example demonstrate improvement truncated estimators naıve version Speciﬁcally test example indepen dent multivariate Beta distributions Bb b dimensionality d shape parameter b In numerical experiments dimensionality varied 1 40 parameter b takes values 1 15 2 In setup generate 1000 samples distribution use KL KSG tKL tKSG estimate entropy All experiments repeated 100 times Root meansquareerror RMSE estimates computed In Fig 2 plot RMSE logarithmic scale dimensionality d From ﬁgure truncated methods blue lines signiﬁcantly outperform naıve ones red lines cases indicating truncation technique improve performance KLKSG estimators compactly supported distributions E S M R 103 102 101 100 101 102 tKL b1 KL b1 tKL b15 KL b15 tKL b2 KL b2 tKSG b1 KSG b1 tKSG b15 KSG b15 tKSG b2 KSG b2 5 10 15 20 d 25 30 35 40 Figure 2 truncated estimators vs nontruncated estimators multidimensional Beta distri butions shape parameters b 270 15 UMtKL UMtKSG KL KSG 101 100 E S M R 101 10 20 d 30 40 UMtKL UMtKSG KL KSG 101 100 101 E S M R 102 05 1 15 2 N 25 3 104 Figure 3 Left RMSE plotted dimensionality d Right RMSE logarithmic scale plotted sample size N 42 Multivariate Normal Distribution To validate idea UM based entropy estimator natural question ask works perfect NF transformation yields exactly normally distributed samples To answer question ﬁrst conduct nu merical tests standard multivariate normal distribution corresponding situation perfect NF case function g Section 33 chosen identity map Speciﬁcally test methods KL KSG UMtKL UMtKSG conduct sets tests ﬁrst ﬁx sample size 1000 vary dimensionality second ﬁx dimensionality 40 vary sample size All tests repeated 100 times RMSE estimates calculated In Fig 3 left plot RMSE logarithmic scale function dimensionality One ﬁgure dimensionality increases estimation error KL KSG grows signiﬁcantly faster UM based ones error KL particularly large Next Fig 3 right plot RMSE sample size N note plot loglog scale d 40 shows highdimensional case UM based estimators yield lower fasterdecaying RMSE estimators original samples Overall results support theoretical ﬁndings Section 31 estimation error signiﬁcantly reduced mapping target samples uniform distribution 43 Multivariate Rosenbrock Distribution In example shall proposed method performs NF included Speciﬁcally example Rosenbrock type distributions standard Rosenbrock distribution 2D widely testing exam ple statistical methods Here consider highdimensional extensions 2D Rosenbrock 40 hybrid Rosenbrock HR Rosenbrock ER distributions The details distributions cluding density functions provided Appendix D2 The Rosenbrock 275 280 285 290 295 300 16 distribution strongly nonGaussian demonstrated Fig 4 left shows samples drawn 2D Rosenbrock As comparison Fig 4 right shows samples transformed uniform distribution entropy estimation Original samples UMtransformed samples 1 05 0 0 0 5 05 1 20 10 0 5 305 310 315 320 325 Figure 4 Left original samples drawn 2D Rosenbrock distribution Right UMtransformed samples entropy estimation In example compare performance seven estimators addition previous example include estimator NF details SI stateoftheart entropy estimators CADEE 21 vonMises based estimator 22 First test estimators scale respect dimensionality sample size taken N 500d With method experiment repeated 20 times RMSE calculated The RMSE dimensionality d test distributions plotted Figs 5 b One observe cases UM based methods especially UMtKSG oﬀer best performance An exception CADEE performs better low dimensional cases ER RMSE grows higher UM methods highdimensional regime d 15 Our second experiment ﬁx dimensionality d 10 vary sample size RMSE plotted sample size HR ER Figs 5 c d The ﬁgures clearly RMSE UM based estimators decays faster methods examples exception CADEE small sample 104 regime ER It worth noting justiﬁed theoretically UMtKSG perform slightly better UMtKL cases 44 Multivariate Rosenbrock Distribution Discontinuous Density Recall Corollaries 2 3 assume diﬀerentiability original density functions satisﬁed practice Thus inter est examine performance proposed methods distributions discontinuous densities To end modify multivariate Rosenbrock distributions studied Section 43 densities discontinuous boundaries supports Appendix D2 details repeat 17 E S M R 102 101 100 101 101 E S M R b E S M R 100 NF CADEE vonMises 20 UMtKL UMtKSG KL KSG 10 d c 5 10 15 20 d d 101 100 E S M R 101 103 104 N 100 103 104 N Figure 5 Top RMSE vs dimensionality HR ER b Bottom RMSE vs sample size HR c ER d 330 335 comparisons conducted Section 43 The results shown Figs 6 For modiﬁed HR Fig 6 c vonMises estimator achieves smaller RMSE UM based ones lowdimensional regime d10 UM based estimators perform best highdimensional regime For modiﬁed ER Fig 6 b d UM based estimators inferior CADEE outperform methods cases 5 Application Examples In section consider applications involving entropy estimation methods compared existing ones 51 Application Entropy Rate Estimation Our ﬁrst application example estimate diﬀerential entropy rate continuousvalued time series Shannon entropy rate 41 measures uncertainty stochastic process X XiiN For stationary process deﬁned HX lim t HXt Xt1 X1 39 340 H conditional entropy random variables In example consider stochastic processes satisfy following assumptions 18 101 E S M R 100 101 6 5 4 3 2 1 E S M R 103 b NF CADEE vonMises 20 UMtKL UMtKSG KL KSG 10 d c 101 E S M R 100 101 E S M R 100 20 10 d d 104 N 103 104 N Figure 6 Top RMSE vs dimensionality modiﬁed HR ER b Bottom RMSE vs sample size modiﬁed HR c ER d First X conditionally stationary process order p exists ﬁxed positive integer p integer t p conditional density function Xt given Xt1 xt1 Xtp xtp satisﬁes pXt xt Xt1 xt1 Xtp xtp f xt xt1 xtp 40 f ﬁxed conditional density function independent t Second X Markov process order p exists positive integer p integer t p pXt xt Xt1 xt1 X1 x1 pXt xt Xt1 xt1 Xtp xtp 41 Under assumptions entropy rate X calculated H HXt Xt1tp HXttp HXt1tp 42 Xttp Xt Xt1 Xtp Note t taken integer p simplicity t p 1 result Eq 42 simpliﬁed H HXt Xt1tp H Xp11 HXp1 19 Suppose T step T p observation X xtT compute entropy rate follows 42 t1 ˆH cid3HXp11 cid3HXp1 345 cid3HXp11 cid3HXp1 estimated desired estimator observation xtT t1 In example consider autoregressive models orders 3 7 15 respectively given AR3 Xt 135 05Xt1 04X 2 AR7 Xt 135 05Xt1 03X 2 03Xt3 cid3t 03Xt7 cid3t AR15 Xt 135 05Xt1 005Xt5 Xt6 Xt72 0005Xt11 Xt12 Xt132 01Xt15 cid3t t5 t2 43a 43b 43c cid3t N 0 0032 white noise Fig 7 shows simulated snapshots models We implemented procedure described estimate entropy rate models entropy estimated seven estimators Section 4 On hand conditional density functions analytically available example entropy rate directly estimated standard Monte Carlo integration ground truth We apply aforementioned entropy estimators compute entropy rate simulated sequence 10 000 steps With method 20 repeated trials conducted RMSE calculated The results reported Table 1 following observations The performance vonMises estimator appears best AR3 model estimators yield small Root Mean Squared Error RMSE suggesting problem particularly challenging For AR7 model UMbased methods smaller RMSE AR15 model UMbased methods KSG perform better Overall UMKSG results smallest RMSE AR7 AR15 350 355 360 Method UMtKL UMtKSG AR3 0029 AR7 067 AR15 115 0051 043 068 KL 0027 123 151 KSG 0032 090 098 NF 012 095 161 CADEE 031 240 414 vonMises 0016 070 142 Table 1 RMSE entropy rate estimations based entropy estimators autoregressive model The smallest best RMSE value shown bold 52 Application Optimal Experimental Design In section apply entropy estimation optimal experimental design OED problem Simply goal OED determine optimal experimental conditions locations sensors maximize certain utility 20 t X t X 0 1 2 4 2 0 2 0 50 0 50 AR3 100 t AR7 100 t AR15 150 200 150 200 t X 4 2 0 2 0 50 100 t 150 200 Figure 7 Snapshots simulated time series function associated experiments Mathematically let λ D design parameters representing experimental conditions θ parameter Y observed data An utility function entropy data Y resulting socalled maximum entropy sampling method MES 6 U λ H Y λ max λD 44 365 370 375 evaluating U λ entropy estimation problem This utility function equivalent mutual entropy criterion certain ditions 43 This formulation particularly useful problems expensive intractable likelihoods likelihoods needed utility function computed entropy estimation A common application OED deter observation times stochastic processes accurately estimate model parameters provide example arising ﬁeld population dynamics Speciﬁcally consider LotkaVolterra LV predatorprey model 44 45 Let x y populations prey predator respectively LV model given x ax xy y bxy y b respectively growth rates prey predator In practice parameters b known need esti mated population data In Bayesian framework assign prior distribution b infer measurements 21 Figure 8 Top sample data paths x y Bottom optimal observation times obtained methods 380 385 population x y Here assume prior b uniform distribution U 05 4 In particular assume pair x cid3x y cid3y cid3x cid3y N 0 001 independent observation noises measured d 5 time points located interval 0 10 goal deter observation times experiments As mentioned earlier shall determine observation times MES method Namely design parameter example λ t1 td data Y pair xcid3x y cid3y measured t1 td want ﬁnd λ maximizes entropy HY λ Method UMtKL UMtKSG CADEE NMC SE RMSE 145 00073 048 073 086 Equidistant 273 00074 KL 165 00072 360 KSG 156 00076 105 NF 148 00072 088 vonMises 181 00049 131 Table 2 The reference entropy values observation time placements obtained methods The smallest best entropy value shown bold 390 395 A common practice problems optimize observation times directly instead parametrize percentiles prescribed dis tribution reduce optimization dimensionality 46 Here use Beta distribution resulting distribution parameters optimized 46 Appendix D4 details We solve resulting optimization problem grid search entropy evaluated seven afore mentioned estimators 10000 samples We plot Fig 8 optimal observation time placements computed seven aforementioned estima tors equidistant placement comparison purpose Also shown ﬁgure sample paths population x y population samples generally subject larger variations near ends relative smaller ones middle Regarding optimization 22 400 405 410 415 420 425 results optimal time placements obtained UM based estimators CADEE diﬀerent results methods To validate optimization results compute reference entropy value optimal placement obtained method Nested Monte Carlo NMC 47 Appendix D5 details large sample size 105 105 results Table 2 Note NMC produce accurate entropy estimate expensive use directly OED problem Using reference values ground truth fur ther compute RMSE estimates 20 repetitions reported Table 2 From table observes placement obser vation times computed UM methods CADEE yields largest entropy values indicates methods clearly outperform estimators OED problem Moreover RMSE results UM based methods especially UMtKSG yield smaller RMSE CADEE suggesting statistically reliable CADEE 6 Conclusion In summary presented uniformization based entropy estimator provided theoretical analysis We believe proposed entropy estimator useful wide range realworld applications Some improvements extensions method possible First theoretical results provide justiﬁcation method analysis needed establish convergence rate understand estimation bias Additionally method extended estimate density functionals Renyi entropy KullbackLeibler divergence Finally work proposed method demonstrated synthetic data sensible examine method realworld data sets We explore research problems future studies 7 Acknowledgments 430 This work partially supported China Scholarship Council CSC The authors like thank Dr Alexander Kraskov discussion KSG estimator References 1 O Vasicek A test normality based sample entropy Journal Royal Statistical Society Series B Methodological 38 1 1976 5459 435 2 M N Goria N N Leonenko V V Mergel P L Novi Inverardi A new class random vector entropy estimators applications testing statistical hypotheses Journal Nonparametric Statistics 17 3 2005 277297 23 440 3 S Azzi B Sudret J Wiart Sensitivity analysis stochastic simulators diﬀerential entropy International Journal Uncertainty Quantiﬁ cation 10 1 4 B Ranneby The maximum spacing method estimation method related maximum likelihood method Scandinavian Journal Statistics 1984 93112 445 5 E Wolsztynski E Thierry L Pronzato Minimumentropy estimation semiparametric models Signal Processing 85 5 2005 937949 6 P Sebastiani H P Wynn Maximum entropy sampling optimal bayesian experimental design Journal Royal Statistical Society Se ries B Statistical Methodology 62 1 2000 145157 450 455 7 Z Ao J Li An approximate KLD based experimental design mod els intractable likelihoods International Conference Artiﬁcial Intelligence Statistics PMLR 2020 pp 32413251 8 J Beirlant E J Dudewicz L Gyorﬁ E C Van der Meulen Nonparamet ric entropy estimation An overview International Journal Mathematical Statistical Sciences 6 1 1997 1739 9 H Joe Estimation entropy functionals multivariate den sity Annals Institute Statistical Mathematics 41 4 1989 683 697 10 P Hall S C Morton On estimation entropy Annals Institute 460 Statistical Mathematics 45 1 1993 6988 11 K R Moon K Sricharan K Greenewald A O Hero III Ensemble esti mation information divergence Entropy 20 8 2018 560 12 G Pichler P J A Colombo M Boudiaf G Koliander P Piantanida A diﬀerential entropy estimator training neural networks International Conference Machine Learning PMLR 2022 pp 1769117715 465 13 L Gyorﬁ E C Van der Meulen Densityfree convergence properties estimators entropy Computational Statistics Data Analysis 5 4 1987 425436 14 WC Chen A Tareen J B Kinney Density estimation small data 470 sets Physical review letters 121 16 2018 160605 15 E G Miller A new class entropy estimators multidimensional den sities 2003 IEEE International Conference Acoustics Speech Signal Processing 2003 ProceedingsICASSP03 Vol 3 IEEE 2003 pp III297 475 16 L Kozachenko N N Leonenko Sample estimate entropy ran dom vector Problemy Peredachi Informatsii 23 2 1987 916 24 480 485 490 495 500 505 17 A Kraskov H Stogbauer P Grassberger Estimating mutual information Physical review E 69 6 2004 066138 18 S Gao G Ver Steeg A Galstyan Eﬃcient estimation mutual informa tion strongly dependent variables Artiﬁcial intelligence statis tics 2015 pp 277286 19 W M Lord J Sun E M Bollt Geometric knearest neighbor estimation entropy mutual information Chaos An Interdisciplinary Journal Nonlinear Science 28 3 2018 033114 20 T B Berrett R J Samworth M Yuan et al Eﬃcient multivariate en tropy estimation knearest neighbour distances Annals Statistics 47 1 2019 288318 21 G Ariel Y Louzoun Estimating diﬀerential entropy recursive copula splitting Entropy 22 2 2020 236 22 K Kandasamy A Krishnamurthy B Poczos L A Wasserman J M Robins Nonparametric von mises estimators entropies divergences mutual informations NIPS Vol 15 2015 pp 397405 23 L T Fernholz Von Mises calculus statistical functionals Vol 19 Springer Science Business Media 2012 24 L Wen H Bai L He Y Zhou M Zhou Z Xu Gradient estimation information measures deep learning KnowledgeBased Systems 224 2021 107046 25 J H Lim A Courville C Pal CW Huang Ardae unbiased neural entropy gradient estimation International Conference Ma chine Learning PMLR 2020 pp 60616071 26 A Krishnamurthy K Kandasamy B Poczos L Wasserman Nonparamet ric estimation renyi divergence friends International Conference Machine Learning PMLR 2014 pp 919927 27 W Gao S Oh P Viswanath Demystifying ﬁxed knearest neighbor infor mation estimators IEEE Transactions Information Theory 64 8 2018 56295661 28 K Sricharan D Wei A O Hero Ensemble estimators multivariate entropy estimation IEEE transactions information theory 59 7 2013 43744388 510 29 Y Han J Jiao T Weissman Y Wu Optimal rates entropy estimation lipschitz balls The Annals Statistics 48 6 2020 32283250 30 L Birge P Massart Estimation integral functionals density The Annals Statistics 1995 1129 25 515 520 525 530 31 S Singh B Poczos Finitesample analysis ﬁxedk nearest neighbor den sity functional estimators Advances neural information processing systems 2016 pp 12171225 32 G Biau L Devroye Lectures nearest neighbor method Vol 246 Springer 2015 33 D Rezende S Mohamed Variational inference normalizing ﬂows International Conference Machine Learning PMLR 2015 pp 1530 1538 34 G Papamakarios E Nalisnick D J Rezende S Mohamed B Laksh minarayanan Normalizing ﬂows probabilistic modeling inference Journal Machine Learning Research 22 57 2021 164 35 G Papamakarios T Pavlakou I Murray Masked autoregressive ﬂow density estimation Advances Neural Information Processing Systems 2017 pp 23382347 36 H Singh N Misra V Hnizdo A Fedorowicz E Demchuk Nearest neigh bor estimates entropy American journal mathematical manage ment sciences 23 34 2003 301321 37 A B Tsybakov E Van der Meulen Rootn consistent estimators en tropy densities unbounded support Scandinavian Journal Statis tics 1996 7583 38 B Efron C Stein The jackknife estimate variance The Annals Statis 535 tics 1981 586596 39 S Ihara Information theory continuous systems Vol 2 World Scientiﬁc 1993 40 F Pagani M Wiegand S Nadarajah An ndimensional rosenbrock distri bution mcmc testing arXiv preprint arXiv190309556 540 41 C E Shannon A mathematical theory communication The Bell technical journal 27 3 1948 379423 42 D Darmon Speciﬁc diﬀerential entropy rate estimation continuous valued time series Entropy 18 5 2016 190 43 M C Shewry H P Wynn Maximum entropy sampling Journal applied 545 statistics 14 2 1987 165170 44 A J Lotka Elements physical biology Williams Wilkins 1925 45 V Volterra Variazioni e ﬂuttuazioni del numero dindividui specie ani mali conviventi C Ferrari 1927 26 550 555 560 565 570 575 580 585 46 E G Ryan C C Drovandi M H Thompson A N Pettitt Towards bayesian experimental design nonlinear models require large num ber sampling times Computational Statistics Data Analysis 70 2014 4560 47 K J Ryan Estimating expected information gains experimental designs application random fatiguelimit model Journal Computa tional Graphical Statistics 12 3 2003 585603 48 M Hardy Combinatorics partial derivatives arXiv preprint math0601149 49 L Dinh J SohlDickstein S Bengio Density estimation real NVP 5th International Conference Learning Representations ICLR 2017 Toulon France April 2426 2017 Conference Track Proceedings 2017 50 M Germain K Gregor I Murray H Larochelle Made Masked autoen coder distribution estimation International Conference Machine Learning PMLR 2015 pp 881889 51 G LoaizaGanem Y Gao J P Cunningham Maximum entropy ﬂow net works 5th International Conference Learning Representations ICLR 2017 Toulon France April 2426 2017 Conference Track Proceedings OpenReviewnet 2017 52 T Rainforth R Cornish H Yang A Warrington F Wood On nesting monte carlo estimators International Conference Machine Learning PMLR 2018 pp 42674276 Appendix A Proofs Theorem 1 Theorem 2 Here provide proofs Theorems 12 We follow closely framework 31 27 ﬁnitesample analysis ﬁxed k nearest neighbor entropy estimators They gave bias bound roughly O γ posi tive constant variance bound roughly O 1 N entropy estimator cid3HKL mild assumptions Similarly prove proposed cid3HtKL cid3HtKSG bias variance bounds More interestingly analysis relates bias bound cid3HtKL gradient density function cid7γd 1 N cid6 Appendix A1 Deﬁnitions assumptions In section introduce notations assumptions proofs rely As mentioned main paper consider distributions densities supported unit cube Rd Let Q 0 1d denote unit cube ddimensional Euclidean space Rd P denote unknown μabsolutely continuous Borel probability measure μ Lebesgue measure Let p Q 0 density P 27 590 595 600 605 610 615 Deﬁnition 1 Twice kNN distance cubes Suppose xiN 1 set i1 N 1 iid samples P We deﬁne twice maximumnorm kNN distance cubes cid3kx 2x x x knearest element xiN 1 i1 x respect norm Deﬁnition 2 Twice kNN distance rectangles Suppose x1cid2 xkcid2 set k nearest elements xiN 1 x respect norm i1 We deﬁne twice kNN distance marginal direction xj cid3xj k x xj knearest element x1cid2 xkcid2 2xj x marginal direction xj x It noted cid3kx max 1jd cid3xj k x j j Deﬁnition 3 Truncated twice kNN distance Since consider densities supported unit cube deﬁne socalled truncated distance convenience In cubic case deﬁne truncated twice kNN distance marginal direction xj ξxj k x minxj cid3kx2 1maxxj cid3kx2 0 In rectangular case distance marginal direction xj deﬁned k x minxj cid3xj ζ xj Deﬁnition 4 rcell We deﬁne rcell centered x Bx r x cid5 Rd xcid5 x r cubic case Bx r1d k x2 1 max xj cid3xj k x2 0 xcid5 Rd xcid5 j xj dcid18 rj rectangular case j1 Deﬁnition 5 Truncated rcell We deﬁne truncated rball centered x Bx r Q B x r cubic case Bx r1d Q B x r1d rectangular case Deﬁnition 6 Mass function We deﬁne mass cell Bx r2 function respect r given prx P Bx r2 deﬁne mass cell Bx r1d2 function respect r1 rd given qr1rd x P Bx r1d2 Assumption 3 We following assumptions p continuous supported Q b p bounded away 0 C1 inf xQ c The gradient p uniformly bounded Qo C2 sup xQo px 0 cid2px1 Appendix A2 Preliminary lemmas Here present lemmas support proofs main results Lemma 1 17 The expectation log pcid3k x satisﬁes Elog pcid3k x ψk ψN 28 Lemma 2 Let cid15P probability measure uniform distribution supported ld x S ddimensional hypercubic area S Bx l2 cid15px 1 density function Deﬁne cid15qr1rd x cid15P Bx r12 rd2 cid15prx cid15P Bx r2 Then Elog cid15qcid3 x1 k cid3 xd k x ψk d 1 k ψN cid3xj k j 1 d deﬁned Deﬁnition 2 replacing P cid15P Proof The probability density function cid3x1 k cid3xd k given fNkr1 rd N 1 kN k 1 dcid15qk r1rd r1 r d 1 cid15prm N k1 A1 rj 17 Then cid15pr cid15P Bx r2 rm max 1jd cid14 cid13 cid2 cid2 Elog cid15qcid3 cid2 l x1 k cid3 cid2 xd k cid13 l 0 0 cid14 cid13 N 1 k N 1 k cid13 cid14 x N 1 k cid2 l cid14 0 kd 1 ld cid2 cid2 0 1 l cid6 d l cid7 dcid15qk r1rd r1 rd 1 ld rd 1 ld rd N 1 k 0 ld r1 rdk 1 r1 r d 1 ld r1 r dk11 u1 u dk11 ud 1 l 0 cid2 1 0 kd mN k1 logu1 u ddu1 du d A2 equality comes change variables ui 1 l ri 1 d Note integrand symmetric permutation labels 1 d 0 1 cid15prm N k1 log cid15qr1rd dr1 dr d mN k1 log mN k1 log 1 ld r1 r ddr1 dr d 1 ld r1 rddr1 dr d Elog cid15qcid3 cid13 x1 k cid3 N 1 k dkd x xd k cid14 cid2 1 cid13 0 dud uk1 d 1 ud dN k1 0 0 cid2 ud cid2 ud u1 u d1k1 logu1 u ddu1 du d1 cid14 A3 Computing integral u1 ud1 symmetry obtain cid2 ud 0 cid2 cid2 d 1 I1 I2 ud u1 u d1k1 logu1 u ddu1 du d1 cid2 ud 0 ud u1 ud1k1 log u1du1 du d1 log um 0 0 29 cid2 ud cid2 ud 0 0 u1 u d1k1du1 du d1 A4 I1 ﬁrst term I2 second term By basic calculus cid13 cid2 ud cid14d2 u2k1du2 log u1du1 I1 d 1 d 1 cid2 ud 0 cid6 1 k uk d uk1 1 cid7d1 log ud 1 k 0 yield I1 I2 1 change variables t ud k uk d Plug Eq A3 cid7d1 uk d I2 log ud cid7d1cid6 1 k d log ud d1 k d ﬁnally cid7 A5 A6 Elog cid15qcid3 cid13 x1 k cid3 xd k cid14 cid2 x dk cid13 k N 1 k N 1 k cid14 cid2 0 1 ψk ψN 0 d 1 k 1 ukd1 d 1 ud dN k1 cid6 cid6 d log ud d 1 k cid7 d 1 k dt cid7 dud A7 tk11 tN k1 log t Lemma 3 Lemma 3 31 Suppose p satisﬁes Assumption b Then x Q r cid71d cid6 k C1N Pcid3kx r eC1rdN cid6 eC1rdN k cid7k Lemma 4 Lemma 4 31 Suppose p satisﬁes Assumption b Then x Q α 0 Ecid3α k x 1 α d cid6 k C1N cid7 α d Lemma 5 Suppose p satisﬁes Assumption 3 x Q array r1 rd satisfy cid19 xj rj 2 xj rj 2 1 xj 1 2 0 xj 1 2 j 1 d cid5 cid5 cid5 cid5 dqr1rd x r1 r d 1 cid3d j1 1j 2 cid5 cid5 cid5 cid5 px 1 cid3d 2 j1 1j 1 cid5 cid5 cid5 cid5 uqr1rd x r1 ru 1 cid3u j1 1j 2 cid6 Bxu1d pxμ ru1 2 rd 2 cid7 cid5 cid5 cid5 cid5 30 C2rm 1 cid3u 2 j1 1j 1 cid6 C2rmμ Bxu1d ru1 2 cid7 rd 2 620 2 xj rj rj 1j indicator function admitting value u d rm max 1jd 1 xj rj 2 intersects 0 1 0 otherwisely Proof For sake convenience discuss case x 0 1 2 d 1j 1 j 1 n u The proof cases obtained permuting labels 1 d By deﬁnition qr1rd x qr1rd x cid2 cid2 x1r12 x1r12 x1r12 0 cid2 cid2 xdrd2 xdrd2 xnrn2 pxcid5 1 xcid5 ddxcid5 d dx cid5 1 cid2 xn1 rn1 2 cid2 xdrd2 xdrd2 pxcid5 1 xcid5 ddxcid5 d dxcid5 1 0 xn1 rn1 2 A8 partial derivative respect ﬁrst n variables given nqr1rd x r1 r n xn1 1 2n xn1 cid2 rn1 2 rn1 2 cid2 xdrd2 xdrd2 px1 r1 2 xn rn 2 xcid5 n1 xcid5 ddxcid5 d dxcid5 n1 A9 Next obtain partial derivative qr1rd x respect ﬁrst u variables xu1ru12 cid2 uqr1rd x r1 r u 1 2u 1 2u Bxu1d cid2 xu1ru12 ru1 2 cid2 xdrd2 xdrd2 px1 r1 2 xn rn 2 xn1 rn1 2 xu ru 2 xcid5 u1 xcid5 ddxcid5 u1 dx cid5 d px1 r1 2 xn rn 2 xn1 rn1 2 xu ru 2 rd 2 xcid5 u1 xcid5 ddxcid5 u1 dx cid5 d A10 notation p x r 2 p x r 2 p x r 2 Finally uqr1rd x r1 r u cid2 Bxu1d ru1 2 rd 2 pxμ 1 cid3u j1 1j 2 cid5 cid5 cid5 cid5px1 cid6 Bxu1d r1 2 xn ru1 2 rn 2 cid7 cid5 cid5 cid5 cid5 rd 2 xn1 rn1 2 xu ru 2 xcid5 u1 xcid5 d 2unpx cid5 cid5 cid5 cid5dxcid5 u1 dx cid5 d cid2 2un 2u 1 2n1 C2rmμ cid6 Bxu1d ru1 2 Bxu1d C2 rd 2 ru1 2 rm 2 dxcid5 u1 dxcid5 d cid7 rd 2 31 A11 cid5 cid5 cid5 cid5 1 2u completes proof u d Particularly cid5 cid5 cid5 cid5 dqr1rd x r1 r d cid5 cid5 cid5 cid5 px 1 cid3d j1 1j 2 1 cid3d 2 j1 1j 1 C2rm A12 Lemma 6 Suppose p satisﬁes Assumption 3 x Q r satisfy cid19 xj r 2 xj r 2 1 x 1 2 0 x 1 2 j 1 d cid5 cid5 cid5 cid5prx pxμ cid6 Bx cid7 cid5 cid5 cid5 cid5 C2 r 2 r 2 Bx r 2 cid5 cid5 cid5 cid5 dprx dr dcid4 j1 1 21j cid6 pxμ Bxˆj cid7 cid5 cid5 cid5 cid5 r 2 dcid4 j1 cid6 1 21j 1 C2rμ Bxˆj cid7 r 2 625 m d 1j indicator function admitting value 1 xj 2 xj r r Proof By deﬁnition prx 2 intersects 0 1 0 otherwiesly cid2 prx Bx r 2 It follows pxcid5 1 xcid5 ddxcid5 d dx cid5 1 A13 cid5 cid5 cid5 cid5prx pxμ cid2 cid5 cid5pxcid5 Bx r 2 cid2 cid6 Bx cid7 cid5 cid5 cid5 cid5 r 2 1 xcid5 d px C2 r 2 dxcid5 d dx cid5 1 2 Bx r r 2 Bx r 2 C2 cid5 cid5dxcid5 d dxcid5 1 A14 completes proof ﬁrst inequality For second inequality easily prx qrrx A15 32 Now Lemma 5 obtain cid5 cid5 cid5 cid5 dprx dr cid5 cid5 cid5 cid5 dcid4 j1 dcid4 j1 dcid4 j1 1 21j cid6 pxμ Bxˆj cid7 cid5 cid5 cid5 cid5 r 2 qr1rd x rj cid5 cid5 cid5 1 21j cid6 pxμ Bxˆj r1dr cid7 cid5 cid5 cid5 cid5 r 2 A16 cid6 1 21j 1 C2rμ Bxˆj cid7 r 2 Appendix A3 Proof bias bound truncated KL estimator Proof Note d j1 log ξij identically distributed cid10 E cid3HtKLX ψk ψN ψk ψN E Ncid4 cid8 dcid4 E cid9 log ξij 1 N i1 cid8 dcid4 j1 cid9 log ξxj k x j1 Elog pcid3k x Elog μBx ξx1 cid9 cid8 k 2 ξxd k 2 E log E cid8 log P Bx cid3k2 k 2 ξxd cid9 μBx ξx1 P Bx cid3k2 μBx cid3k2 k 2 A17 equality Lemma 1 ﬁfth equality fact p supported Q Note C1 P Bx cid3k2 μBx cid3k2 sup xQ px A18 cid5 cid5 cid5 cid5 log px log P Bx cid3k2 μBx cid3k2 cid5 cid5 cid5 cid5 cid5 cid5 cid5 cid5px 1 C1 P Bx cid3k2 μBx cid3k2 cid2 cid5 cid5 cid5 cid5 1 C1μBx cid3k2 1 C1μBx cid3k2 px pxcid5dxcid5 A19 Bxcid3k2 cid2 Bxcid3k2 C2x xcid5dxcid5 C2 2C1 cid3k 33 Finally Lemma 4 bias bound E cid3HtKLX obtained cid5 cid5E cid3HtKLX HX cid5 cid5 cid8 cid5 cid5 cid5 cid5 log px log E E xp C2 2C1 C2 E xp Ecid3k cid7 1 cid6 k d N C 11d 1 P Bx cid3k2 μBx cid3k2 cid9 cid5 cid5 cid5 cid5 A20 completes proof 630 Appendix A4 Proof variance bound truncated KL estimator d j1 log ξij We deﬁne Proof For sake convenience deﬁne αi αcid5 1 N estimators x1 resampled α 2 N estimators x1 removed Then EfronStein inequality 38 cid10 cid11 1 N cid11cid13 Var cid3HtKLX Var E N 2 1 N cid12 αi Ncid4 i1 Ncid4 αi 1 N αi 1 N cid142cid12 Ncid4 αcid5 i1 Ncid4 α i2 Ncid4 cid142 cid13 cid142cid12 α 1 N αi 1 N i1 i2 cid11cid13 1 N cid11cid13 i1 Ncid4 i1 Ncid4 N E 2N E 1 N Ncid4 i1 αcid5 1 N Ncid4 i2 α cid142cid12 Let 1Ei indicator function event Ei cid3kx1 cid7 cid3 kx1 twice thek NN distance x1 α cid3 Ncid4 Then Ncid4 Ncid4 cid14 cid13 N 1 N αi 1 N i1 i2 α α1 1Ei αi α A22 A21 kx1 By CauchySchwarz inequality cid13 N 2 1 N Ncid4 i1 αi 1 N Ncid4 i2 α cid142 cid13 1 Ncid4 i2 1Ei cid13 i2 cid14cid13 α2 1 Ncid4 i2 cid14 1Ei αi α 2 Ncid4 i2 1Ei αi α 2 cid14 cid14 Ncid4 i2 1Eiα2 α2 A23 1 Ckd α2 1 cid13 1 Ckd α2 1 2 34 Ckd constant x1 knearest neighbors Ckd samples Note αi α identically distributed need bound Eα2 1 N 1E1E2α2 2 N 1E1E2α2 2 Bound A24a We separate A24a parts cid8 cid9 cid8 E α2 1 E xQ E P cid3kaN α2 1 cid9 E xQ E P cid3kaN cid8 cid9 α2 1 A24a A24b A24c A25 aN cid6 cid7 1 d 2k log N C1N First consider bound ﬁrst term Eq A25 For x Q cid6 cid8 log ξx1 k ξ xd k cid7cid92 dr A26 cid9 cid8 α2 1 E P cid3kaN cid2 aN fNkr cid14 cid13 0 N 1 k fNkr k dpr dr pk1 r 1 prN k1 17 Note suﬃciently large N cid2 aN cid6 log 0 cid2 aN cid8 log cid7cid92 dr ξx1 k cid6 r 2 ξ xd k cid7cid92 r 2 0 C3 log N 3 N 1d dr A27 C3 0 focus bounding fNkr By basic calculus cid13 cid14 k N 1 k pk1 r 1 prN k1 C4N A28 C4 0 andp r 0 1 Also Lemma 6 dpr C5 dr C5 0 r aN Therefore pdf term bounded log N N fNkr C4C5 log N Combining Eq A27 Eq A29 bound Eq A26 cid9 cid8 α2 1 E P cid3kaN C3C4C5 log N 4 N 1d C6 A29 A30 35 C6 0 Thus ﬁrst term Eq A25 bounded E xQ E P cid3kaN cid8 cid9 α2 1 C6 A31 Now consider second term Eq A25 For cid3k aN suﬃciently large N cid6 cid8 log ξx1 k ξ xd k cid7cid92 cid8 cid6 d2 log cid8 cid3k2 cid3k2 cid7cid92 cid6 aN 2 C7log N 2 log cid7cid92 A32 C7 0 Using Lemma 3 Eq A32 second term Eq A25 bounded E xQ E P cid3kaN cid9 cid8 α2 1 E xQ E P cid3kaN cid6 cid8 log ξx1 k ξ xd k cid7cid92 cid11 cid12 C7log N 2 P cid3k aN A33 C8 log N k2 N 2k C8 0 Combining Eq A31 Eq A33 expectation α2 1 bounded Eα2 1 C9 A34 C9 0 635 Bound A24b Since event E2 equivalent event x1 kNN N 1 Additionally E2 x2 E1E2 Px1 Bx2 cid3kx2 k independent cid3kx2 A24b bounded N 1E1E2α2 2 N 1E1E2Eα2 2 kC9 A35 second inequality Eq A34 Bound A24c Using independence E2 cid3 x2 x1 removed bound A24c 2 N 1E1E2Eα2 N 1E1E2α2 kx2 twice thek NN distance 2 kC10 A36 C10 0 second inequality obtained Eq A34 sample size reduced N 1 Finally obtain bound variance cid3HtKLX Var cid3HtKLX C11 1 N A37 640 C11 0 36 Appendix A5 Proof bias bound truncated KSG estimator Proof We separate ddimensional unit cube Q subsets Q Q1 cid7 1 cid6 Q2 Q1 aN d Q2 Q Q1 Suppose cid15P cid15p cid15qcid3 d Lemma 2 fact x deﬁned Lemma 2 l px 1 d j1 log ζij identically distributed 2 d aN 2 1 aN 2k log N C1N x1 k cid3 cid10 xd k E cid3HtKSGX ψk ψ N d 1k cid8 cid8 cid9 E xp E xp E P E P cid8 log ζ x1 k ζ xd k log ζ x1 k ζ xd k cid9 E xp E xp E cid2P E cid2P cid8 log cid15qcid3 cid6 log Ncid4 cid8 dcid4 E cid9 log ζij 1 N i1 j1 cid9 x1 k cid3 xd k pxcid3x1 k cid3 xd k A38 cid7cid9 We decompose bias terms bound separately cid5 cid5E cid3HtKSGX HX cid5 cid5 cid7cid9 cid6 cid5 cid5 E xp ζ xd k ζ x1 k log cid8 E P I1 I2 I3 cid5 cid5 cid6 cid8 log cid3x1 k cid3 xd k cid7cid9 cid5 cid5 cid5 cid5 E xp E cid2P A39 I1 I2 I3 cid5 cid5 cid5 cid5 E xQ2 cid5 cid5 cid5 cid5 E xQ1 cid5 cid5 cid5 cid5 E xQ cid6 cid8 log ζ x1 k ζ xd k E P cid3kaN E P cid3kaN cid8 E P cid3kaN cid8 cid6 log ζ x1 k ζ xd k cid6 log ζ x1 k ζ xd k cid7cid9 cid5 cid5 cid5 cid5 cid5 cid5 cid5 cid5 E xQ2 cid7cid9 cid7cid9 E cid2P cid3kaN cid8 E xQ1 cid5 cid5 cid5 cid5 E xQ cid5 cid5 cid5 cid5 E cid2P cid3kaN cid8 E cid2P cid3kaN cid6 cid8 log cid3x1 k cid3 xd k cid7cid9 cid5 cid5 cid5 cid5 cid6 log cid3x1 k cid3 xd k cid7cid9 cid6 log cid3x1 k cid3 xd k cid7cid9 cid5 cid5 cid5 cid5 cid5 cid5 cid5 cid5 A40 means taking expectation probability measure P E P cid3kaN cid3xj k aN j 1 d Bound I1 For x Q2 cid8 cid6 ζ x1 k ζ xd k cid7cid9 E P cid3kaN cid2 aN log cid2 aN 0 0 fNkr1 rd log cid6 ζ x1 k ζ xd k cid7 dr1 dr d A41 fNkr1 rd cid14 cid13 N 1 k dqk r1rd r1rd 1 prm N k1 rm 37 max 1jd cid2 aN 0 cid2 aN 0 cid2 aN 0 rj 17 Note suﬃciently large N cid2 aN 0 cid2 aN 0 cid2 aN 0 cid2 cid5 cid5 log cid5 cid5 log cid5 cid5 log aN cid6 ζ x1 k ζ xd k cid7cid5 cid5dr1 dr d cid6 r1 2 cid6 cid7cid5 cid5dr1 dr d rd 2 cid7cid5 cid5dr1 dr d cid13 cid2 aN r1 r d cid2 aN 0 cid2 aN 0 cid14d d log 2dr1 drd daN d1 cid72 cid6 C3 log N C1N log rdr d log 2 dr 0 0 A44 A45 A42 C3 0 We focus bounding fNkr1 rd We omit subscripts qr1rd simplicity By multivariate version Faa di Brunos formula 48 obtains dqk r1 r d cid4 πΠ dπqk dqπ cid20 Bπ Bqcid21 jB rj A43 π runs set Π partitions set 1 d By Lemma 5 Bqcid21 jB rj pxrdB m C2rdB1 m implies cid20 Bπ Bqcid21 jB rj M rπ1d m M pd 1 p sup xQ px Therefore π k rm aN 38 bound fNkr1 rd cid13 cid4 fNkr1 rd cid14 N 1 k dπqk dqπ cid20 Bπ Bqcid21 jB rj 1 prm N k1 N 1 k πN k 1 qkπ1 prm N k1M rπ1d m M N kpkπ rm 1 prm N k1rπ1d m CM N πrπ1d m πΠ cid4 πΠ cid4 πΠ cid4 πΠ cid4 πΠ CM cid13 cid13 cid14π1 N 2k log N C1 cid14k1 N 2k log N C1 ΠCM A46 inequality fact pkπ1pN k1 CN kπ p 0 1 Combining Eq A46 Eq A42 bound expecta tion Eq A41 cid5 cid5 cid5 cid5 E P cid3kaN cid8 cid6 log ζ x1 k ζ xd k cid6 cid7cid9 cid5 cid5 cid5 cid5 C4 cid7k1 log N C k 1 A47 C4 0 It follows ﬁrst term I1 bounded cid5 cid5 cid5 cid5 E xQ2 E P cid3kaN cid6 cid8 log ζ x1 k ζ xd k cid7cid9 cid5 cid5 cid5 cid5 C4 cid6 cid6 C4 E xQ2 1 pμx Q2 cid7k1 cid7k1 log N C k 1 log N C k 1 cid6 cid7k1 log N C k 1 cid6 pC4 d 1pC4 d 1aN cid7k1 log N C k 1 cid6 2k log N C1N cid7 1 d A48 Since cid15P sepcial case P second term I1 bounded order Thus I1 bounded cid6 I1 C 5 cid7k2 log N C k1 1 N 1 d A49 645 C5 0 Bound I2 39 For x Q1 cid3xj k aN j 1 d easy ζ xj k cid3xj k Thus I2 bounded rewritten I2 E xQ1 E xQ1 cid8 cid5 cid5 cid5 cid5 E P cid3kaN cid5 cid2 cid5 cid5 cid5 aN 0 0 cid6 log cid2 aN cid6 cid7cid9 cid8 cid6 ζ x1 k ζ xd k E log cid3x1 k cid3 xd k cid2P cid3kaN fNkr1 rd cid15fNkr1 rd cid7cid9 cid5 cid5 cid5 cid5 cid6 cid7 log r1 r d cid7 dr1 dr d cid5 cid5 cid5 cid5 cid15fNkr1 rd subscripts cid15qr1rd following analysis Since r1rd r1rd dcid2qk cid13 cid14 N 1 k A50 1 cid15prm N k1 Again omit cid2 aN cid2 aN 0 cid6 C3 0 cid72 log N C1N cid6 cid5 cid5 log r1 rd cid7cid5 cid5dr1 dr d A51 A42 focus bounding fNkr1 rd cid15fNkr1 rd Recall Faa di Brunos formula Eq A43 Bqcid21 jB rj 1 prm N k1 pxr1 r d Or1 rdrm cid20 cid20 cid7cid6 px rj Orm rj 1 pxrd m cid7kπ cid7N k1 Ord1 m Bπ cid14 j cid4B cid6 j cid4B cid7kπcid6 1 Orm cid7kπ cid20 cid6 cid20 cid7 rj px pxr1 r d cid7cid6 cid7N k1cid6 1 Orm cid6 pxr1 r d 1 pxrd m cid20 cid7kπ cid6 px 1 Ord1 m cid20 cid7 rj Bπ j cid4B cid7N k1 fNkr1 rd cid14 cid4 cid13 πqk qπ cid20 Bπ cid6 k k π cid20 cid6 N 1 k cid14 N 1 k πΠ cid4 cid13 πΠ cid13 cid4 πΠ N 1 k cid13 cid4 πΠ cid14 N 1 k cid13 cid4 πΠ cid14 N 1 k k k π cid6 k k π πcid15qk cid15qπ cid6 cid6 1 pxrd m cid7N k1 cid6 Bπ 1 Orm j cid4B cid7kcid6 cid20 Bπ Bcid15qcid21 jB rj cid7kcid6 1 cid15prm N k1 cid7N k1 1 Ord1 m cid6 1 Orm cid7N k1 cid7kcid6 1 Ord1 m cid7N k1 cid15fNkr1 rd 1 O rm 1 Ord1 m A52 second equality Lemma 5 Lemma 6 ﬁfth equality fact cid15q pxr1 r d cid15prm pxrd m x Q1 rm aN 40 By Eq A52 obtain bound diﬀerence fNkr1 rd cid15fNkr1 rd fNkr1 rd cid15fNkr1 rd cid5 cid5 cid6 cid5 cid5 1 Ord1 m 1 Orm cid7kcid6 cid7N k1 1 C6rm cid13 C6 cid15fNkr1 rd 2k log N C1N cid14 1 d ΠCM cid13 2k log N C1 cid14k1 N cid5 cid5 cid5 cid5 cid15fNkr1 rd A53 C6 0 inequality Eq A46 fact cid15P special case P Combining Eq A53 Eq A51 obtain bound I2 I2 C3C6 C7 cid13 2k log N C1N log N k2 C k1 1 N 1 d cid14 1 d ΠCM cid13 2k log N C1 cid14k1 cid6 cid72 log N C1 E xQ1 1 A54 C7 0 E xQ1 1 1 650 Bound I3 To bound ﬁrst term I3 need bound cid6 cid8cid5 cid5 log ζ x1 k ζ xd k cid9 cid7cid5 cid5 E P cid3kaN ﬁrst Note event cid3k aN equivalent j 1 d cid3xj aN symmetry equation k expectation set rewritten cid6 cid8cid5 cid5 log ζ x1 k ζ xd k E P cid3kaN cid9 cid7cid5 cid5 dcid4 i1 C d P Consider term Eq A55 cid19 E cid3k1i aN cid3kid aN cid6 cid8cid5 cid5 log ζ x1 k ζ xd k cid9 cid7cid5 cid5 A55 cid19 E cid3k1i aN cid3kid aN cid19 E cid3k1i aN cid3kid aN P P cid6 cid8cid5 cid5 log ζ x1 k ζ xd k cid9 cid7cid5 cid5 cid6 cid8cid5 cid5 log ζ x1 k ζ xi k cid9 cid7cid5 cid5 P cid6 cid8cid5 cid5 log ζ xi1 k ζ xd k cid9 cid7cid5 cid5 cid19 E cid3k1i aN cid3kid aN For cid3xj k aN j 1 suﬃciently large N cid6 cid5 cid5 log ζ x1 k ζ xi k cid7cid5 cid5 k 2 cid6 cid5 cid5 log cid5 cid5 log k 2 cid3xi cid3x1 cid5 cid6 aN cid5 2 C8 log N cid7i cid7cid5 cid5 A56 A57 41 C8 0 Using Lemma 3 Eq A57 ﬁrst term Eq A56 bounded cid8cid5 cid5 log C8 log N Pcid3k1i aN cid3kid aN cid7cid5 cid5 cid19 E cid6 cid9 ζ x1 k ζ xi k P cid3k1i aN cid3kid aN C8 log N P cid3k aN C9 log N k1 N 2k A58 For C9 0 Now consider second term Eq A56 Like Eq A42 integration respect Lebesgue measure bounded cid2 1 cid2 1 cid13 cid2 aN aN aN d iaN di1 0 cid2 aN cid2 0 aN cid6 cid5 cid5 log ζ xi1 k ζ xd k log rdr d log 2 cid14 drd dr cid7cid5 cid5dri1 dr d cid2 aN drdi C10 log N 0 0 A59 C10 0 Again multivariate version Faa di Brunos formula bound fNkr1 rd π k rm aN cid13 cid4 fNkr1 rd cid14 N 1 k dπqk dqπ cid20 Bπ Bqcid21 jB rj 1 prm N k1 N 1 k πN k 1 N 1 k πN k 1 qkπ1 prm N k1M rπ1d m 1 C1ad N N k1M πΠ cid4 πΠ cid4 πΠ C11 1 N k A60 C11 0 Therefore combining Eq A59 Eq A60 leads bound second term Eq A56 cid6 cid8cid5 cid5 log ζ xi1 k ζ xd k cid9 cid7cid5 cid5 C10C11 log N N k A61 cid19 E cid3k1i aN cid3kid aN P larger bound Eq A58 As result bound Eq A56 cid19 E cid3k1i aN cid3kid aN P cid6 cid8cid5 cid5 log ζ x1 k ζ xd k cid9 cid7cid5 cid5 C10C11 log N N k A62 42 Given Eq A62 able estimate Eq A55 ﬁrst term I3 bound constant Similarly bound second term I3 O Thus I3 bounded cid6 cid7 log N N k I3 C12 log N N k A63 C12 0 Finally combining upper bounds I1 I2 I3 obtain bias bounded cid5 cid5E cid3HtKSGX HX cid5 cid5 C13 log N k2 C k1 1 N 1 d A64 C13 0 d cid10 Appendix A6 Proof variance bound truncated KSG estimator j1 log ζij deﬁne βcid5 Proof We let βi 1 N estimators x1 resampled β 2 N estimators x1 removed It noted proof completed following roadmap Appendix A4 issue needs validated Eβ2 1 O log N k2 Again separate E cid9 cid8 cid8 cid9 β2 1 parts E β2 1 E xQ cid9 cid8 β2 1 E P cid3kaN E xQ E P cid3kaN cid8 cid9 β2 1 A65 aN deﬁned Appendix A5 First consider bound ﬁrst term Eq A65 For x Q E P cid3kaN cid2 aN cid8 β2 1 cid2 cid9 aN fNkr1 rd cid6 cid8 log ζ x1 k ζ xd k cid7cid92 dr1 drd A66 0 0 cid14 cid13 N 1 k dqk r1rd r1rd 1prmN k1 rm max 1jd rj 655 660 Note suﬃciently large N aN cid8 cid6 log ζ x1 k ζ xd k cid7cid92 dr1 dr d fNkr1 rd 17 cid2 aN cid2 0 cid2 aN 0 cid2 aN cid8 log 0 cid2 aN 0 cid2 aN cid8 log d C3 0 log N 3 N 0 cid6 r1 2 cid6 r1 2 cid7cid92 dr1 dr d rd 2 cid7cid92 dr1 dr d dd 1 43 cid2 aN cid2 aN 0 0 log cid7 cid6 r1 2 log cid7 cid6 r2 2 dr1 drd A67 C3 0 Recall Eq A46 bound Eq A66 cid8 cid9 E P cid3kaN β2 1 C4log N k2 C4 0 Thus ﬁrst term Eq A65 bounded E xQ E P cid3kaN cid9 cid8 β2 1 C4log N k2 Now consider second term Eq A65 Like bound analysis I3 Appendix A5 rewrite cid9 cid8 β2 1 E P cid3kaN dcid4 i1 C d P cid8 cid9 β2 1 cid19 E cid3k1i aN cid3kid aN A68 A69 cid9 cid8 β2 1 E P cid3kaN A70 Consider term Eq A55 cid8 cid9 β2 1 cid19 E cid3k1i aN cid3kid aN P cid13 2 P For cid3xj k cid19 E cid3k1i aN cid3kid aN cid6 cid8cid5 cid5 log ζ x1 k ζ xi k cid7cid5 cid52cid9 P cid19 E cid3k1i aN cid3kid aN cid6 cid8cid5 cid5 log ζ xi1 k ζ xd k cid14 cid7cid5 cid52cid9 aN j 1 suﬃciently large N cid7cid5 cid52 cid6 cid6 cid5 cid5 log ζ x1 k ζ xi k k 2 cid5 cid5 log cid5 cid5 log k 2 cid3xi cid3x1 cid5 cid6 aN cid52 2 cid7i cid7cid5 cid52 A71 A72 C5log N 2 C5 0 Using Lemma 3 Eq A72 ﬁrst term Eq A71 bounded cid6 cid8cid5 cid5 log ζ x1 k ζ xi k cid7cid5 cid52cid9 C5log N 2 Pcid3k1i aN cid3kid aN cid19 P E cid3k1i aN cid3kid aN C5log N 2 P cid3k aN C6 A73 C6 0 Now consider second term Eq A71 Like Eq A67 integration 44 respect Lebesgue measure bounded cid2 1 cid2 1 cid13 cid2 aN cid2 aN cid6 cid5 cid5 log aN C7 aN 0 0 ζ xi1 k ζ xd k cid7cid5 cid52 cid14 dri1 drd drd dr A74 C7 0 Therefore combining Eq A74 PDF bound Eq A60 leads bound second term Eq A71 cid6 cid8cid5 cid5 log ζ xi1 k ζ xd k cid7cid5 cid52cid9 C8 A75 cid19 E cid3k1i aN cid3kid aN P C8 0 As result bound Eq A71 cid6 cid8cid5 cid5 log ζ x1 k ζ xd k cid9 cid7cid5 cid5 C6 C8 A76 cid19 E cid3k1i aN cid3kid aN P 665 Given Eq A76 able estimate Eq A70 second term Eq A65 bound constant Finally expectation β2 1 bounded Eβ2 1 C9log N k2 A77 C9 0 Following procedure Appendix A4 obtain bound variance cid3HtKSGX Var cid3HtKSGX C10 log N k2 N A78 C10 0 Appendix B Proof Corollary 2 Proof Given UM f density original distribution satisﬁes change variable formula pxx pzf xgx B1 cid5 cid5 cid5det f x x cid5 cid5 cid5 diﬀerentiable positive x Rd 35 49 gx Recall px diﬀerentiable follows pzz pxf 1z gf 1z B2 diﬀerentiable z Qo Thus supreme C N 2 random variable 670 deﬁned 45 Since pS z diﬀerentiable density function deﬁned Q exists z Q pS z z 1 By mean value theorem z z z ξ z z z ξ1 z z 1 pS cid2pS cid2pS C N 2 B3 ξ vector Q Thus 1 C N 2 pN x x 1 C N 2 B4 Now deﬁne C N 1 inf zQ pS z z For N M bias bounded cid5 cid5E cid3HUMtKLX HX cid5 cid5 EUM cid8 E C N cid5 cid5 cid5 cid5EX cid3HUMtKLX HX cid7 1 C N 2 1 11d cid6 k N cid9cid6 k N cid7 1 d d C N UM tKL B5 C C N UM tKL 1 1 C11d EC N 2 Note C N 2 EC N 2 0 lim N UM tKL 0 P N 0 andC N 2 C N N M lim N The MSE bounded E cid3HUMtKLX HX2 2E cid3HUMtKLX EX cid3HUMtKLX2 2EEX cid3HUMtKLX HX2 2EUMEX cid3HUMtKLX EX cid3HUMtKLX2 2EUMEX cid3HUMtKLX HX2 Note N M C N bound ﬁrst term Eq B6 1 C N B6 2 satisfy Assumption 3 Then Theorem 1 2EUMEX cid3HUMtKLX EX cid3HUMtKLX2 C1 1 N B7 C1 0 The second term Eq B6 bounded 2EUMEX cid3HUMtKLX HX2 cid9cid6 k N C N cid7 2 d cid8 2E 2 2 C N 1 211d cid7 2 cid6 k d N DN UM tKL 46 B8 DN N M Thus MSE bounded 1 C211d EC N UM tKL 2 2 2 Again lim N DN UM tKL 0 E cid3HUMtKLX HX2 C1 1 N DN UM tKL cid7 2 d cid6 k N B9 Appendix C Proof Corollary 3 Proof For N M bias bounded cid6 cid5 cid5E cid3HUMtKSGX HX pS z d 1 C k1 1 cid9 log N k2 N cid8 pS z cid5 cid5 cid7 1 d CE CUM tKSG log N k2 N 1 d C1 cid6 cid7 1 Cd1 1 C C positive constant pS 1 Ck1 Similarly proof Corollary 2 Theorem 2 bound MSE pS z z CUM tKSG C z sup zQ E cid3HUMtKSGX HX2 C2 log N k2 N C2 positive constant DN UM tKSG cid16 C DN UM tKSG log N 2k2 N cid7 1 Cd1 cid172 2 d C2 1 Ck1 cid6 1 C Appendix D Further details numerical examples 675 Appendix D1 Implementation details estimators 680 The setup MAF We use MAF built 10 autoregressive layers 50 Hybrid Rosenbrock distribution built 5 autoregressive layers Even Rosenbrock distribution application experimental design Each layer hidden layers 50 units tanh nonlinearities In experiment half samples train MAF model half estimate entropy The implementation CADEE nonMises estimator The estimators implemented code provided 21 22 default parameters 47 685 Appendix D2 The multivariate Rosenbrock distributions Hybrid Rosenbrock Distribution The density hybrid Rosenbrock distribution given πx exp ax1 μ2 n2cid4 n1cid4 j1 i2 bjixji x2 ji12 D1 dimensionality x d n1 1n2 1 The variable xj1 x1 j 1 n2 The normalization constant Eq D1 cid21 cid28 bji n1n2 i2j1 πd2 D2 In experiment set μ 10 10 bji 01 j n1 4 n2 ranging 1 7 This setting forms class distributions dimensions ranging 4 22 Even Rosenbrock Distribution The density Rosenbrock distribution given πx exp d2cid4 cid29 i1 x2i1 μ2i12 ci cid6 x2i x2 2i1 cid30 cid72 D3 690 695 dimensionality d number The normalization stant Eq D3 d2 i1 πd2 In experiment set μ2i1 0 ci 125 fori 1 d2 withd ranging 2 22 This setting forms class distributions dimensions ranging 2 22 D4 ci cid21 Hybrid Rosenbrock Distribution Discontinuous Density The density hybrid Rosenbrock distribution discontinuous density given πx unifpdfx1 μ cid31 1 8a n2cid20 n1cid20 j1 i2 unifpdfxji x2 ji1 cid31 1 8b D5 unifpdfx α β pdf continuous uniform distribution interval αβ αβ evaluated values x dimensionality x d n1 1n2 1 The variable xj1 x1 j 1 n2 In experiment set μ 10 10 bji 01 j n1 4 n2 ranging 1 7 This setting forms class distributions dimensions ranging 4 22 Even Rosenbrock Distribution Discontinuous Density The density Rosenbrock distribution discontinuous density given 48 πx d2cid20 cid8 i1 unifpdfx2i1 μ2i1 05 unifpdfx2i x2 2i1 ci cid9 D6 dimensionality d number 700 In experiment set μ2i1 0 ci 0025 1 d2 withd ranging 2 22 This setting forms class distributions dimensions ranging 2 22 Appendix D3 Entropy estimator NF In section simpliﬁed version proposed method estimate entropy NF truncated entropy estimators To start recall Eq 12 main paper cid2 HX H Z pzz log cid5 cid5 cid5 cid5 det cid5 cid5 cid5 cid5dz f 1z z D7 The main idea simpliﬁed method assume transformed random variable Z exactly follows uniform distribution result HZ 0 Therefore entropy X estimated ˆHN F X cid5 cid5 cid5 cid5 det log f 1zi z cid5 cid5 cid5 cid5 1 n ncid4 i1 D8 705 zi f xi A limitation method obvious trans formed random variable Z usually uniformly distributed simply taking entropy zero undoubtedly introduce bias demonstrated numerical examples main paper It noted context entropy estimation NF based approach maximum entropy modeling 51 Appendix D4 The Beta scheme parametrizing observation times In optimal experimental design OED example use lower dimen sional parameterization scheme reduce dimensionality optimiza tion problem 46 In particular use Beta scheme 46 allocate placements observation times Speciﬁcally let Q α β quantile function beta distribution shape parameters α β d observation times λ t1 td time interval 0 T allocated ti T Q d 1 α β 1 d D9 710 As ddimensional variable λ parametrized α 0 β 0 49 Appendix D5 Nested Monte Carlo Here Nested Monte Carlo NMC approach estimate entropy experimental design example Recall entropy HY simplicity omit design parameter λ cid2 HY log pypydy D10 estimated Monte Carlo MC HY 1 M Mcid4 i1 log pyi D11 yi drawn py A diﬃculty explicit expression py Note example likelihood pyθ prior pθ available write cid2 py pyθpθdθ D12 It follows py estimated MC pyi 1 N Ncid4 j1 pyiθj D13 θj drawn pθ Combining Eq D13 Eq D11 obtain estimator HY referred NMC method 47 In particular Eq D13 usually referred inner MC Eq D11 referred outer Since theoretical results 47 52 mean squared error NMC estimator decays rate O 1 N obtain accurate evaluation HY suﬃciently large number samples numerical example use M N 1 105 We emphasize large number samples computationally feasible use experimental design procedure example resort entropy estimation methods M 1 715 720 50 cid24cid286cid272cid367cid258cid396cid258cid410cid349cid381cid374cid3cid381cid296cid3cid349cid374cid410cid286cid396cid286cid400cid410cid400cid3 cid3cid3 cid1409cid3cid100cid346cid286cid3cid258cid437cid410cid346cid381cid396cid400cid3cid282cid286cid272cid367cid258cid396cid286cid3cid410cid346cid258cid410cid3cid410cid346cid286cid455cid3cid346cid258cid448cid286cid3cid374cid381cid3cid364cid374cid381cid449cid374cid3cid272cid381cid373cid393cid286cid410cid349cid374cid336cid3cid296cid349cid374cid258cid374cid272cid349cid258cid367cid3cid349cid374cid410cid286cid396cid286cid400cid410cid400cid3cid381cid396cid3cid393cid286cid396cid400cid381cid374cid258cid367cid3cid396cid286cid367cid258cid410cid349cid381cid374cid400cid346cid349cid393cid400cid3 cid410cid346cid258cid410cid3cid272cid381cid437cid367cid282cid3cid346cid258cid448cid286cid3cid258cid393cid393cid286cid258cid396cid286cid282cid3cid410cid381cid3cid349cid374cid296cid367cid437cid286cid374cid272cid286cid3cid410cid346cid286cid3cid449cid381cid396cid364cid3cid396cid286cid393cid381cid396cid410cid286cid282cid3cid349cid374cid3cid410cid346cid349cid400cid3cid393cid258cid393cid286cid396cid856cid3 cid3cid3 cid1407cid3cid100cid346cid286cid3cid258cid437cid410cid346cid381cid396cid400cid3cid282cid286cid272cid367cid258cid396cid286cid3cid410cid346cid286cid3cid296cid381cid367cid367cid381cid449cid349cid374cid336cid3cid296cid349cid374cid258cid374cid272cid349cid258cid367cid3cid349cid374cid410cid286cid396cid286cid400cid410cid400cid876cid393cid286cid396cid400cid381cid374cid258cid367cid3cid396cid286cid367cid258cid410cid349cid381cid374cid400cid346cid349cid393cid400cid3cid449cid346cid349cid272cid346cid3cid373cid258cid455cid3cid271cid286cid3cid272cid381cid374cid400cid349cid282cid286cid396cid286cid282cid3 cid258cid400cid3cid393cid381cid410cid286cid374cid410cid349cid258cid367cid3cid272cid381cid373cid393cid286cid410cid349cid374cid336cid3cid349cid374cid410cid286cid396cid286cid400cid410cid400cid855cid3 cid3 cid3 cid3cid3 cid3cid3 cid3cid3 cid3