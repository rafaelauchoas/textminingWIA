Artiﬁcial Intelligence 189 2012 1947 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Discovering hidden structure factored MDPs Andrey Kolobov Mausam Daniel S Weld Dept Computer Science Engineering University Washington Seattle WA 98195 United States r t c l e n f o b s t r c t Article history Received 1 August 2010 Received revised form 8 April 2012 Accepted 9 May 2012 Available online 15 May 2012 Keywords Markov Decision Process MDP Planning uncertainty Generalization Abstraction Basis function Nogood Heuristic Dead end 1 Introduction Markov Decision Processes MDPs wide variety planning scenarios ranging military operations planning controlling Mars rover However todays solution techniques scale poorly limiting MDPs practical applicability In work propose algorithms automatically discover exploit hidden structure factored MDPs Doing helps solve MDPs faster memory stateoftheart techniques Our algorithms discover complementary state abstractions basis functions nogoods A basis function conjunction literals conjunction holds true state guarantees existence trajectory goal Conversely nogood conjunction presence implies nonexistence trajectory meaning state dead end We compute basis functions regressing goal descriptions determinized version MDP Nogoods constructed novel machine learning algorithm uses basis functions training data Our state abstractions leveraged ways We diverse approaches GOTH heuristic function use heuristic search algorithms RTDP ReTrASE MDP solver performs modiﬁed Bellman backups basis functions instead states SixthSense method quickly detect deadend states In essence work integrates ideas deterministic planning basis functionbased approximation leading methods outperform existing approaches wide margin 2012 Elsevier BV All rights reserved Markov Decision Processes MDPs popular framework modeling problems involving sequential decisionmaking uncertainty Examples range militaryoperations planning userinterface adaptation control mobile robots 136 Unfortunately existing techniques solving MDPs deciding actions execute situations scale poorly dramatically limits MDPs practical utility Humans perform surprisingly planning uncertainty largely able recognize reuse abstractions generalizing conclusions different plans For example realizing walls particular Mars crater steep rover escape human planner abandon attempts collect rock samples crater traditional MDP solver rediscover navigational problem considered collecting sample turn This article presents new algorithms automatically discovering exploiting hidden structure MDPs Specif ically generate kinds abstraction basis functions nogoods describes sets states share similar relationship planning goal Both basis functions nogoods represented logical conjunctions state variable values encode diametrically opposite information When basis function holds state guarantees Corresponding author Email addresses akolobovcswashingtonedu A Kolobov mausamcswashingtonedu Mausam weldcswashingtonedu DS Weld 00043702 matter 2012 Elsevier BV All rights reserved httpdxdoiorg101016jartint201205002 20 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 certain trajectory action outcomes positive probability reaching goal Our algorithms associate weights basis function encoding relative quality different trajectories In contrast nogood holds state signiﬁes state deadend trajectory reach goal state Continuing Mars rover example conjunction described presence steepwalled crater nogood Our notions basis function nogood similar rules learned logical theories explanationbased learn ing constraint satisfaction 2714 work applies probabilistic context learns weights basis functions provides new mechanisms discovery Previous MDP algorithms basis functions 21 39 perform generalization different problems domain course solving single problem Other researchers handgenerated basis functions manner similar 222320 present methods automatic generation 11 Discovering nogoods basis functions We generate basis functions regressing goal descriptions action outcome trajectory determinized version probabilistic domain theory Thus trajectory potentially executable states satisfying basis function This justiﬁes performing Bellman backups basis functions states generalizing experience similar states Since basis functions typically hold given state value state complex function applicable basis functions We discover nogoods novel machine learning algorithm operates phases First generates candidate nogoods probabilistic sampling procedure basis functions previously discovered dead ends training data It tests candidates planning graph 6 ensure trajectories goal exist states containing nogood 12 Exploiting nogoods basis functions We present algorithms leverage basis function nogood abstractions speed MDP solution reduce memory usage GOTH uses classical planner generate heuristic function MDP solver use initial estimate state values While classical planners known provide informative approximation state value probabilistic problems expensive newly visited state GOTH amortizes cost multiple states associating weights basis functions generalizing heuristic computation Empirical evaluation shows GOTH informative heuristic saves MDP solvers considerable time memory ReTrASE selfcontained MDP solver based informationsharing insight GOTH However unlike GOTH sets weight basis function provide starting guess states values ReTrASE learns basis functions weights evaluating functions usefulness decisiontheoretic way By aggregating weights ReTrASE constructs state value function approximation empirically produces better policies participants International Probabilistic Planning Competition IPPC domains little memory SixthSense method quickly reliably identifying dead ends states possible trajectory goal MDPs In general problem intractable prove determining given state trajectory goal PSPACEcomplete 19 unsurprising modern MDP solvers waste considerable resources exploring doomed states SixthSense acts submodule MDP solver helping detect avoid dead ends SixthSense employs machine learning basis functions training data guaranteed generate false positives The resource savings provided SixthSense determined fraction dead ends MDPs state space reach 90 IPPC benchmark problems In rest paper present algorithms discuss theoretical properties evaluate empirically Section 2 reviews background material introduces relevant deﬁnitions illustrating running example Sections 3 4 5 present descriptions empirical results GOTH ReTrASE SixthSense respectively Section 6 discusses potential extensions presented algorithms Finally Section 7 describes related work Section 8 concludes paper 2 Preliminaries 21 Example Throughout paper illustrating concepts following scenario called GremlinWorld Con sider gremlin wants sabotage airplane stay alive process To achieve task gremlin pick tools The gremlin tweak airplane screwdriver wrench smack hammer A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 21 define domain GremlinWorld types tool predicates t tool gremlinalive planebroken constants Wrench tool Screwdriver tool Hammer tool action pickup parameters t tool precondition t effect t action tweak parameters precondition Screwdriver effect planebroken Wrench action smack parameters precondition Hammer effect planebroken probabilistic 09 gremlinalive define problem GremlinProb domain GremlinWorld init gremlinalive goal gremlinalive planebroken Fig 1 A PPDDLstyle description example MDP GremlinWorld split domain problem parts However smacking high probability lead accidental detonation airplanes fuel destroys air plane kills gremlin Fig 1 describes setting Probabilistic Planning Domain Description Language PPDDL As introduce relevant terminology subsequent subsections formally deﬁne corresponding MDP 22 Background Markov Decision Processes MDPs In paper focus probabilistic planning scenarios modeled discrete factored stochasticshortestpath SSP MDPs initial state In general MDPs deﬁned tuples form cid3S A T Ccid4 S set states A set actions T transition function S A S 0 1 giving probability moving si s j executing action C map S A R specifying action costs The MDPs consider paper speciﬁc kind deﬁned tuple cid3X A T C G s0cid4 A T C X set state variables st conjunction literals variables X state MDP Therefore slight abuse notation set S 2 X general MDP deﬁnition G set absorbing goal states s0 start state All action costs positive C map S A R 1 We assume state space 2 X action space A ﬁnite Another assumption action MDP precondition conjunction literals describing states action executed 1 This requirement actually stricter easier state original SSP MDPs deﬁnition 5 The original statement allows costs completely arbitrary long policy reach goal incurs inﬁnite cost However algorithms paper apply MDPs falling deﬁnition 22 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 action pickup0 parameters t tool precondition t effect t action tweak0 parameters precondition Screwdriver effect planebroken Wrench action smack0 parameters precondition Hammer effect planebroken action smack1 parameters precondition Hammer effect planebroken gremlinalive Fig 2 Alloutcomes determinization GremlinWorld domain Our example GremlinWorld formulated MDP ﬁve state variables gremlinalive planebroken hasHammer hasWrench hasScrewdriver abbreviated G P H W S respectively Therefore X G P H W S The problem involves ﬁve actions A pickupScrewdriver pickupWrench pickupHammer tweak smack Each action precondition smack actions precondition singleliteral conjunction Hammer smack states gremlin hammer Actions preconditions effects compactly specify transition function T For simplicity C assign cost 1 actions conforms restriction C imposed SSP MDP deﬁnition G set states gremlin alive airplane broken Finally assume gremlin starts alive tools airplane originally intact s0 G P H W S Solving MDP means ﬁnding good costminimizing policy π S A speciﬁes actions agent eventually reach goal The optimal expected cost reaching goal state s termed optimal value function V s satisﬁes following conditions called Bellman equations V V s 0 s G cid4 cid3 cid2 s min aA Cs T s s scid7S cid5 cid7 cid4 cid7 s V cid6 cid5 Given V s optimal policy computed follows cid3 cid6 Cs cid4 T cid5 cid7 cid4 cid5 cid7 s s s V cid2 s arg min aA π scid7S Solution methods The equations suggest dynamic programmingbased way ﬁnding optimal policy called value iteration VI 3 VI iteratively updates state values Bellman equations Bellman backup values converge VI given rise improvements Trialbased methods RTDP 2 try reach goal multiple times multiple trials update value function states trial path successively improving policy Bellman backup A popular variant LRTDP adds termination condition RTDP labeling states values converged solved 7 Compared VI trialbased methods save space considering fewer irrelevant states LRTDP serves testbed experiments approach present searchbased MDP solvers LAO 24 Determinization Successes number planners starting FFReplan 42 demonstrated promise deter minizing domain set actions given MDP disregarding probabilities transition function working state transition graph Our techniques use alloutcomes determinization 42 Dd domain D hand Namely note example Fig 1 action precondition c outcomes o1 respective probabilities p1 pn For example thesmack action outcomes o1 P G p1 09 o2 P p1 01 The alloutcomes determinization Dd example GremlinWorld domain shown Fig 2 tains action original domain set deterministic actions a1 precondition c effect oi Dd coupled description state space initial state goal viewed deterministic MDP plan given state goal exists corresponding trajectory positive probability original probabilistic domain D Importantly state art classical planning makes solving determin istic problem faster solving probabilistic problem comparable size Our abstraction framework exploits A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 23 facts eﬃciently extract structure given MDP ﬁnding plans Dd processing shown Section 23 Heuristic functions We deﬁne heuristic function termed simply heuristic value function initializes state values MDP algorithm Heuristic values tend derived automatically structure problem hand The properties heuristic determine quickly planning algorithm converges resulting policy optimal Algorithms like VI update value state iteration converge optimal policy faster closer heuristic V In trialbased algorithms like LRTDP heuristics help avoid visiting irrelevant states To guarantee convergence optimal policy trialbased MDP solvers typically require heuristic admissible overestimate V importantly admissibility requirement convergence policy How inadmissible heuristics tend informative practice approximating V better average Informativeness translates smaller number explored states associated memory savings reasonable sacriﬁces optimality In paper adopt number states visited planner guidance heuristic measure heuristics informativeness basis functions let derive highly informative heuristic GOTH cost admissibility A successful class MDP heuristics based alloutcomes determinization probabilistic domain D hand 8 To obtain value state s D determinization heuristics try approximate cost plan s goal Dd ﬁnding plan relaxed version MDP generally NPhard For instance FF heuristic 26 denoted hFF ignores negative literals delete effects outcomes actions Dd attempts ﬁnd cost cheapest solution new relaxed problem As hFF experience informative general MDP heuristic use baseline evaluate performance GOTH Planning graph Our work makes use planning graph data structure 6 directed graph alternating propo sition action levels The 0th level contains vertex literal present initial state s Odd levels contain vertices actions including special noop action preconditions present pairwise nonmutex previous level Subsequent levels contain literals effects previous action level Two literals level mutex actions achieving pairwise mutex previous level Two actions level mutex effects inconsistent ones precondition inconsistent effect preconditions mutex previous level As levels increase additional actions literals appear mutexes disappear ﬁxed point reached Graphplan 6 uses graph polynomialtime reachability test goal use procedure discover nogoods Section 5 23 Deﬁnitions essentials Let execution trace e s a1 s1 sn sequence s traces starting state a1 probabilis tic action applied s yielded state s1 An example execution trace GremlinWorld cid7 G P H W S pickupHammer G P H W S smack G P H W S e We deﬁne trajectory execution trace e sequence te s outa1 1 e outan n e s es starting state outak k e conjunction literals representing particular outcome action ak cid7 G P H W S H P trajectory example sampled kth step es execution Eg te execution trace e cid7 cid7 shown goal trajectory A suﬃx We te goal trajectory state sn e goal state te te sequence tie outai e outan n e 1 cid2 cid2 n Suppose given MDP goal trajectory te execution trace MDP Let preca denote precondition literal conjunction litc stand set literals forming conjunction c Imagine t generate following sequence literal conjunctions b0 G bi cid7cid8cid8 litbi1 lit cid4 outani1 n 1 e cid5cid9 cid4 lit cid5cid9 precani1 1 cid2 cid2 n This simple multistep procedure We start b0 G MDPs goal conjunction Afterwards step cid3 1 ﬁrst remove bi1 literals action ani1s outcome n 1th step e Then conjoin result literals ani1s precondition obtaining conjunction bi We procedure regression goal trajectory te orregression short 24 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 As example consider regressing trajectory te cid7 GremlinWorld In case b0 G G P First remove b0 literal P outcome action smack e The result G Then add precondition smack literal H producing G H Thus b1 G H Similarly remove b1 outcome pickupHammer add precondition action result obtaining b2 G At point regression terminates cid7 A basis function deﬁned literal conjunction b produced step regressing goal tra jectory Whenever literals basis function conjunction literals general present state s conjunction holds represents s For instance b1 G H example holds state G P H W S An alternative view basis function b mathematical function fb S 0 1 having value 1 states conjunction b holds 0 Basis functions central concept algorithms paper important understand intuition Any goal trajectory potentially causally important sequence actions Regressing gives preconditions trajectorys suﬃxes Basis functions exactly trajectory suﬃx preconditions Thus regression trajec tories thought unearthing relevant causal structure necessary planning task hand Moreover basis functions causal structure There trajectories preconditions consistent subconjunction given basis function We basis function b enables set goal trajectories T goal reached state represented b following trajectories T assuming Nature chooses right outcome action trajectory Since basis function essentially precondition trajectory typically holds states MDP hand Therefore obtaining goal trajectory te state lets generalize qualitative reachability informa tion states basis functions yielded regressing goal trajectory Moreover te interesting numeric characterizations cost probability successful execution To generalize quantitative descriptions states associate weight basis function The semantics basis function weight depends algorithm general reﬂects quality set trajectories enabled basis function Now consider value MDPs state As preconditions basis functions tell goal trajectories possible state Basis function weights tell good trajectories Since quality set goal tra jectories possible state strong indicator states value knowing basis functions weights allows approximating state value function As showed problems causal structure eﬃciently derived goal trajectories regression Thus relatively cheap source trajectories way readily extract structure problem Fortunately methods exist The ﬁrst based insight trial MDP solver reaches goal trajectory free byproduct solvers usual computation The caveat technique primary strategy getting trajectories time takes MDP solvers trials start attaining goal Indeed majority trials beginning planning terminate states path goal stage knowing problems structure helpful improving situation Therefore algorithms rely different trajectory generation approach Note trajectory MDP plan alloutcomes determinization Dd MDP vice versa Since classical planners fast use quickly ﬁnd goal trajectories Dd states choice By deﬁnition basis functions represent states reaching goal possible However MDPs contain type states dead ends fall outside basis function framework presented far Such states turn classiﬁed kinds explicit dead ends actions applicable implicit ones applicable actions sequence leads goal positive probability In GremlinWorld explicit dead ends state literal G implicit dead end To extend information generalization dead ends consider kind literal conjunctions nogoods Nogoods deﬁning property state nogood holds dead end Notice duality nogoods basis functions exactly form opposite guarantees state Whereas state represented basis function provably dead end state represented nogood certainly Despite representational similarity identifying nogoods signiﬁcantly involved discovering basis functions Fortunately duality allows derive collect corresponding beneﬁts algorithms present SixthSense demonstrates 3 GOTH heuristic 31 Motivation Our presentation abstraction framework begins example use heuristic function As mentioned heuristics reduce trialbased MDP solvers resource consumption helping avoid visiting states memoizing corresponding statevalue pairs ﬁnal policy The informative MDP heuristics hFF based alloutcomes determinization domain However eﬃciently computable heuristics add extra level relaxation original MDP determinizing For instance hFF liable highly A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 25 underestimate states true cost addition discarding domains probabilities ignores actions delete effects negative literals G actions outcomes determinized version On hand lot promise shown recently probabilistic planners solve non relaxed determinizations FFReplan HMDPP 29 It natural wonder improved heuristic estimates classical planner nonrelaxed determinized domains provide gains compensate potentially increased cost heuristic computation As section answer No Yes We propose new heuristic called GOTH Generalization Of Trajectories Heuristic 32 eﬃciently produces heuristic state values deterministic planning The straight forward implementation method classical planner called time state visited ﬁrst time produce better heuristic estimates reduces search cost calls classical planner vastly outweighs beneﬁts The crucial observation basis functions provide way amortize expensive planner calls generalizing resulting heuristic values guidance similar states By performing generaliza tion careful manner dramatically reduce classical planning needed providing informative heuristic values heuristics levels relaxation 32 GOTH description Given problem P probabilistic domain D MDP solver GOTH starts GOTHs initialization During initialization GOTH determinizes D classic counterpart Dd operation needs Our im plementation performs alloutcomes determinization likely better value estimates singleoutcome 42 However involved ﬂavors determinization described Related Work section yield better estimation accuracy Calling deterministic planner Once Dd computed probabilistic planner starts exploring state space For state s requires heuristic initialization GOTH ﬁrst checks explicit dead end This check place eﬃciency GOTH try use expensive methods analysis states For state s explicit dead end GOTH constructs problem P s original problems goal s initial state feeding P s Dd classical planner denoted DetPlan pseudocode Algorithm 1 setting timeout If s implicit dead end DetPlan proves unsuccessfully searches plan timeout In case returns plan point s presumed dead end assigned high value taken If s dead end DetPlan usually returns plan s goal The cost plan taken heuristic value s Sometimes DetPlan fail ﬁnd plan timeout leading MDP solver falsely assume s dead end In practice seen hurt GOTHs performance Regressionbased generalization By fullﬂedged classical planner GOTH produces informative state estimates hFF evidenced experiments However invoking classical planner newly encountered state costly stands GOTH prohibitively slow To ensure speed modify procedure based insight basis functions properties shown pseudocode Algorithm 1 Whenever GOTH computes deterministic plan ﬁrst regresses described Section 2 Then memoizes resulting basis functions associated weights set costs regressed plan suﬃxes When GOTH encounters new state s minimizes weights basis functions stored far hold s In GOTH sets heuristic value s cost cheapest currently known trajectory originates s Thus weight basis function generalized heuristic value states This way computing states value fast GOTH employs invoking classical planner However ss heuristic value needed GOTH basis function holds s In case GOTH uses classical planner described computing value s augmenting basis function set Evaluating state ﬁrst generalization generalization fails classical planning greatly amortizes cost classical solver invocation drastically reduces computation time compared deterministic planner Weight updates Different invocations deterministic planner occasionally yield basis function time potentially new weight Which weights use The different weights caused variety factors nondeterministic choices classical planner2 Thus basis function weight given invocation unrepresentative cost plans basis function precondition For reason generally beneﬁcial assign basis function average weights computed classical planner invocations far This approach line 27 Algorithm 1 Note compute average need number times function rediscovered 2 For instance LPG 18 relies stochastic local search strategy action selection produce distinct paths goal invoked twice state concomitant differences basis functions andor weights 26 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 M holds s return large penalty 1 000 000 cid7 return minbasis functions f subsume sM f declare problem P s cid3init state s goal Gcid4 declare plan pl DetPlanDd P s T pl Algorithm 1 GOTH heuristic 1 Input probabilistic domain D problem P cid3init state s0 goal Gcid4 determinization routine Det classical planner Det Plan timeout T state s 2 Output heuristic value s 3 4 compute global determinization Dd DetD 5 declare global map M basis functions weights 6 7 function computeGOTHstate s timeout T 8 action D applicable s return large penalty 1000 000 9 10 nogood holds s 11 12 member f 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 end declare action pli weight weight Costs f f preconda effecta f declare basis function f goal G declare weight 0 lengthpl 1 update M f incorporating weight M f s running average end SchedulerSaysYes learn nogoods discovered dead ends return large penalty 1 000 000 insert cid3 f weightcid4 M end return weight M end end Dealing implicit dead ends The discussion far ignored important When classical planner called implicit dead end deﬁnition trajectory discovered basis functions Thus invocation seemingly wasted point view generalization contribute reducing average cost heuristic computation described far As turns fact amortize cost discovery implicit dead ends way similar reducing average time states evaluation To use known dead ends stored basis functions derive latters duals informationsharing framework nogoods We remind reader nogoods generalize dead ends precisely way basis functions nondead ends help recognize dead ends resorting classical planning The precise nogood learning mechanism called SixthSense described Section 5 It needs invoked points GOTHs running time prescribed scheduler described section For abstract away operation SixthSense lines 3234 Algorithm 1 With nogoods available positively deciding state dead end simple checking known nogoods subsumes lines 89 Algorithm 1 Deterministic planning necessary answer question Speed memory performance To facilitate empirical analysis GOTH helpful look extra speed memory cost MDP solver incurs Concerning GOTHs memory utilization emphasize similar hFF heuristics GOTH store states given heuristic evaluation It merely returns heuristic values states MDP solver choose store resulting statevalue pairs discard However compute values GOTH needs memoize basis functions nogoods extracted far As experiments demonstrate set basis functions nogoods discovered GOTH MDP solvers running time small compensated reduction explored fraction state space GOTHs informativeness compared hFF Timewise GOTHs performance largely dictated speed employed deterministic planners number times invoked Another component signiﬁcant determining cheapest basis function holds state line 11 Algorithm 1 requires iterating average constant fraction known basis functions Although faster solutions possible patternmatching problem aware 16 pay increase speed degraded memory performance A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 27 Fig 3 GOTH outperforms hFF Machine Shop Triangle Tireworld Blocksworld speed large margin Theoretical properties Two especially important theoretical properties GOTH informativeness estimates inadmissibility The ensures compared hFF GOTH causes MDP solvers explore fewer states At time like hFF GOTH inadmissible One source inadmissibility comes general lack optimality deterministic planners Even optimal employing timeouts terminate classical planner occasionally causes GOTH falsely assume states dead ends Finally basis function generalization mechanism contributes inadmissibility The set discovered basis functions complete smallest basis function weight known far overestimate states true value exist cheaper goal trajectory state GOTH unaware In spite theoretical inadmissibility practice GOTH usually yields good policies quality better guidance hFF 33 Experimental results Our experiments compare performance probabilistic planner GOTH planner guidance hFF wide range domains In experience hFF included miniGPT 8 outperforms wellknown MDP heuristics IPPC domains minmin atommin heuristics supplied package Our implementation GOTH uses portfolio classical planners FF LPG To evaluate state launches planners line 12 Algorithm 1 parallel takes heuristic value returns sooner The timeout deterministic planner ﬁnding plan given state goal 25 seconds We tested GOTH hFF LRTDP planner available miniGPT package Our benchmarks probabilistic domains ﬁve come recent IPPCs goaloriented problems Machine Shop 37 Triangle Tireworld IPPC08 Exploding Blocks World IPPC08 version Blocks World IPPC06 version Elevators IPPC06 Drive IPPC06 All remaining domains IPPC06 IPPC08 easier versions Tireworld IPPC06 features supported implementation LRTDP rewards universal quantiﬁcation able test Additionally perform brief comparison LRTDP GOTH FFReplan shares insights GOTH In experiments measuring effect generalization planners 24hour limit solve problem All experiments GOTH forReTrASE SixthSense described Sections 43 53 respectively performed dualcore 28 GHz Intel Xeon processor 2 GB RAM Comparison hFF In subsection use domains illustrate aspects modes GOTHs behavior compare behavior hFF As shown ﬁve test domains LRTDP GOTH substantially outperforms LRTDP hFF We start comparison looking domain structure especially inconvenient hFF Machine Shop Problems set involve machines number objects equal ordinal corresponding problem Each object needs series manipulations machine able subset The effects manipulations cancel effects shaping object destroys paint sprayed Thus order actions plan critical This domain illuminates drawbacks hFF ignores delete effects distinguish good bad action sequences result Machine Shop dead ends Figs 3 4 speed memory performance LRTDP equipped heuristics problems MachineShop domains planners solve running memory As implied preceding discussion GOTHs space requirements memory consumption LRTDP GOTH measured number states basis functions nogoods values need maintained GOTH caches basis functions LRTDP caches states In case LRTDP hFF memory LRTDPs state caching hFF memoize On Machine Shop edge LRTDP GOTH clearly vast reaching orders magnitude In fact LRTDP hFF runs memory hardest problems LRTDP GOTH far Concerning policy quality use GOTH yield optimal nearoptimal policies Machine Shop This contrasts hFF policies average 30 costly optimal ones 28 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 Fig 4 GOTHs advantage hFF Machine Shop Triangle Tireworld Blocksworld memory large Fig 5 The big picture GOTH provides signiﬁcant advantage large problems Note axes Log scale The Triangle Tireworld domain unlike Machine Shop structure particularly adversarial hFF However LRTDP GOTH noticeably outperforms LRTDP hFF Figs 3 4 indicate Nonetheless heuristic saves memory let LRTDP solve past problem 8 In terms solution quality planners ﬁnd optimal policies problems solve The results Exploding Blocks World EBW Fig 5 similar Triangle Tireworld LRTDP GOTHs economical memory consumption eventually translates speed advantage Importantly EBW problems LRTDP GOTH superior LRTDP hFF illustrative way manages solve problems LRTDP hFF runs space The policy quality planners similar The Drive domain small GOTH provide signiﬁcant beneﬁt On Drive problems planners spend time decisiontheoretic computation explore 2000 states LRTDP guidance GOTH hFF explores roughly number states explored generalization play big role GOTH incurs additional overhead maintaining basis functions getting signiﬁcant beneﬁt Perhaps surprisingly GOTH leads LRTDP ﬁnd policies higher success rates coverage causing ﬁnd worse policies hFF The difference policy quality reaches 50 Drive domains largest problems Reasons topic future investigation On remaining test domains Elevators Blocksworld LRTDP GOTH dominates LRTDP hFF speed memory providing policies equal better quality Figs 3 4 performance Blocksworld example Classical planners portfolio cope determinized versions domains quickly abstraction ensures obtained heuristic values spread states Similar situation EBW effectiveness GOTH LRTDP GOTH solve ﬁve hardest problems Blocksworld LRTDP hFF Fig 5 provides big picture comparison For problem tried contains point coordinates logarithms timememory LRTDP GOTH LRTDP hFF took solve problem Thus points lie Y X line correspond problems LRTDP GOTH better according respective criterion The axes time plot Fig 5 extend log286 400 logarithm time cutoff 86 400 s 24 hours Similarly axes memory plot reach log210 000 000 number memoized statesbasis functions hash tables stored ineﬃcient allow problem solved 86 400 s A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 29 Table 1 Average ratio number states memoized LRTDP guidance hFF number GOTH test domain The bigger numbers memory GOTH saves MDP solver compared hFF EBW 207 EL 418 TTW 171 DR 100 MS 1440 BW 772 Fig 6 GOTH faster generalization time limit Thus points lie extreme right plots denote problems solved guidance heuristics Overall time plot shows GOTH ties slightly beaten hFF Drive smaller problems domains enjoys comfortable advantage large problems In terms memory advantage extends mediumsized small problems translates qualitative difference allowing GOTH handle problems hFF Why GOTHs hFF s comparative performance differ domain domain For insight refer Table 1 It displays ratio number states explored LRTDP hFF number explored LRTDP GOTH averaged problems solved planners domain Thus numbers reﬂect relative infor mativeness heuristics Note important difference data chart memory usage presented graphs information table disregards memory consumption heuristics separating description heuristics informativeness characterization eﬃciency Associating data table relative speeds LRTDP hFF LRTDP GOTH test domains reveals clear trend size LRTDP GOTHs speed advantage strongly correlated memory advantage advantage informative ness In particular GOTHs superiority informativeness suﬃcient compensate computation cost Indeed 171 average reduction compared hFF number explored states Triangle Tireworld barely good time spent deterministic planning generalization In contrast domains like Blocksworld GOTH causes LRTDP visit times fewer states hFF LRTDP GOTH consistently solves problems faster Beneﬁt generalization Our main hypothesis GOTH generalization vital making GOTH computationally feasible To test measure importance basis functions nogoods GOTHs operation ran version GOTH generalization turned domains classical planner invoked state passed GOTH evaluation As aside note akin strategy FFReplan fundamental difference GOTHs state values eventually overridden decisiontheoretic training process LRTDP We explore relationship FFReplan GOTH subsection As expected GOTH generalization proved vastly slower GOTH For instance Machine Shop LRTDP GOTH generalization turned approximately 3040 times slower Fig 6 problem 10 gap growing alarming rate implying generalization technique speedup hFF possible On domains implicit dead ends Exploding Blocks World difference dramatic reaching orders magnitude Furthermore relatively small problems managed run LRTDP GOTH generaliza tion quality policies measured average plan length yielded generalized GOTH typically better generalization This result somewhat unexpected generalization additional layer ap proximation determinizing domain We attribute phenomenon averaging weight update strategy As pointed earlier weight basis function length plan case nongeneralized GOTH single classical planner invocation reﬂective basis functions quality nongeneralized GOTH suffer noise regular GOTH In event GOTH generalization yielded better policies slowness use unjustiﬁable practice One wonder generalization beneﬁt hFF way helped GOTH While conducted experiments verify believe answer Unlike deterministic plan construction ﬁnding relaxed plan sought hFF easier faster Considering generalization mechanism involves iterating 30 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 available basis functions evaluate state savings result avoiding hFF s relaxed plan computation negated iteration Computational proﬁle An interesting aspect GOTHs modus operandi fraction computational resources MDP solver uses GOTH Eg Machine Shop domain LRTDP GOTH spends 7590 time heuristic computation LRTDP hFF 817 Thus GOTH computationally heavier causes LRTDP spend drastically time exploring state space Comparison FFReplan One ﬁnd similarities techniques GOTH FFReplan Indeed employ deterministic planners FFReplan action selection directly GOTH state evaluation One key dif ference lies fact GOTH complete planner lets dedicated MDP solver correct judgment As consequence GOTH se ignores probabilistic information domain probabilities nonetheless taken account solvers search policy FFReplan hand ignores entirely Due discrepancy performance FFReplan planner guided GOTH typically vastly distinct For instance FFReplan faster decisiontheoretic planners On hand FFReplan diﬃculty dealing proba bilistic subtleties It known come low success rate policies probabilistically interesting problems problems Triangle Tireworld06 35 LRTDP GOTH handle domains better Eg stated produces optimal 100 successrate policies ﬁrst problems harder version Triangle Tireworld appeared IPPC08 34 Summary GOTH heuristic function provides MDP solver informative state value estimates costs plans deterministic version given MDP Computing plans expensive To amortize time spent computation GOTH employs basis functions generalize cost plan states As experiments strategy informativeness state value estimates GOTH effective heuristic state art hFF 4 RETRASE 41 Motivation In GOTH role information transfer basis functions nogoods primarily reuse computation form classical planner invocations save time In section present MDP solver called ReTrASE Regressing Trajectories Approximate State Evaluation initially described 31 employs basis functions similar way time chieﬂy purpose drastically reducing memory footprint Many dynamic programmingbased MDP algorithms VI LRTDP suffer critical drawback represent state value function extensionally table requiring memory time exponential number MDP variables Since extensional representation grows rapidly approaches scale handle real world problems Indeed VI RTDP typically exhaust memory applied large problems IPPC Two broad approaches proposed avoiding creation statevalue table One method consists computing policy online help domain determinization alloutcomes In online settings policy needs decided ondemand current state time step This makes maintaining statevalue table unnecessary potentially useful Running classical planner domain determinization helps choose action current state resorting table Determinizationbased planners FFHop 43 slow invoking classical planner times case FFReplan disregard probabilistic nature actions trouble probabilistically interesting 35 domains short plans low probability mass The method dimensionality reduction maps MDP state space parameter space lower dimension Typically mapping constructing small set basis functions learning weights combining weighted basis function values values states Researchers successfully applied dimensionality reduction manually deﬁning domainspeciﬁc basis function set basis functions captured human intuition domain hand It relatively easy ﬁnd mapping domains ordinal numeric state variables especially numeric features correlate strongly value state gridworlds SysAdmin FreeCraft 222320 In contrast dimensionality reduction diﬃcult use nominal discrete logical domains IPPC Besides having metric quantities valid distance function states distance states usually asymmetric violates triangle equality It extremely hard human devise basis functions reduction mapping nominal domains The focus section automatic procedure To knowledge little work mating decision theory determinization dimensionality reduction With ReTrASE algorithm bridging gap proposing fusion ideas removes drawbacks ReTrASE learns compact value function approximation successful range nominal domains Like GOTH A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 31 cid7 cid7 s end cid7 arg minaExpActCosta s declare state s s0 declare numSteps 0 numSteps L declare action ModiﬁedBellmanBackupa s execute action s numSteps numSteps 1 Algorithm 2 ReTrASE 1 Input probabilistic domain D problem P cid3init state s0 goal Gcid4 trial length L determinization routine Det classical planner DetPlan timeout T 2 declare global map M basis functions weights 3 declare global set DE dead ends 4 compute global determinization Dd D 5 6 Do modiﬁed RTDP basis functions 7 1 8 9 10 11 12 13 14 15 16 end 17 18 function ExpActCostaction state s 19 declare array So successors s 20 declare array P o probs successors s 21 return costa 22 23 function Valuestate s 24 s DE 25 26 member f 27 28 29 30 31 end 32 33 function ModiﬁedBellmanBackupaction state s 34 basis functions f 35 36 end cid7 return minbasis functions f hold sM f GetBasisFuncsForSs return Values return large penalty 1 000 000 M f ExpActCosta s P oiValueSoi s enable M holds s cid10 ﬁrst obtaining set basis functions automatically planning determinized version domain hand However probabilistic planner unlike GOTH learns weights basis functions decisiontheoretic means aggregates compute state values dimensionalityreduction methods Thus opposed GOTH ReTrASE tries incorporate probabilistic information lost determinization stage solution The set basis functions normally smaller set reachable states giving planner large reduction memory requirements number parameters learned implicit reuse classical plans thanks basis functions makes fast We demonstrate practicality ReTrASE comparing IPPC04 06 08 performers state oftheart planners challenging problems competitions ReTrASE demonstrates orders magnitude better scalability best optimal planners frequently ﬁnds signiﬁcantly better policies stateoftheart approx imate solvers 42 ReTrASE description The main intuition underlying ReTrASE extracting basis functions MDP akin mapping MDP lowerdimensional parameter space In practice space smaller original state space relevant causal structure retained3 giving large reduction space requirements Solving new problem amounts learning weights quantitative measure basis functions quality There imaginable ways learn paper explore method modiﬁed version RTDP The weights reﬂect fact basis functions differ total expected cost goal trajectories enable total probability trajectories At point stress ReTrASE makes approximations way computing MDPs value function ﬁrst related semantics basis function weights importance Any given basis function enables subset T goal trajectories given state oblivious trajectories state The trajectories preferable ones T lead agent goal 100 probability Therefore importance trajectories corresponding basis functions depends state Our intuitive notion weights ignores subtlety weight 3 We approximate putting bound number basis functions willing handle step 32 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 Algorithm 3 Generating basis functions 1 Input probabilistic domain D problem P cid3init state s0 goal Gcid4 determinization routine Det classical planner DetPlan timeout T 2 declare global map M basis functions weights 3 declare global set DE dead ends 4 compute global determinization Dd D 5 6 function GetBasisFuncsForSstate s 7 declare problem P s cid3init state s goal Gcid4 8 declare plan pl DetPlanDd P s T 9 pl 10 insert s DE 11 12 13 14 15 16 17 18 19 20 end declare action pli cost cost costa f f preconda effecta insert cid3 f costcid4 M f declare basis function f goal G declare cost 0 lengthpl 1 M end basis function vary states basis function holds Thus weight effect reﬂection average importance basis function states represents The details notwithstanding differences basis function weights exist partly trajectory considers outcome actions The sequence outcomes given trajectory considers unlikely In fact getting action outcomes trajectory consider prevent agent getting goal Thus easier reach goal presence basis functions Now given state generally represented basis functions connection states value weights In general relationship complex optimal policy trajectories enabled basis functions possible causing trajectories factor weights basis functions simul taneously However determining subset basis functions enabling trajectories hard solving MDP exactly Instead approximate state value minimum weight basis function represent state This amounts saying better states best basis function better state second approximation ReTrASE makes Thus deriving useful basis functions weights gives approximation optimal value function Algorithms operation For stepbystep example operation ReTrASE pseudo code presented Algorithm 2 refer proof Theorem 1 ReTrASE starts computing determinization Dd domain As GOTH use Dd rapidly compute basis functions The algorithm explores state space running modiﬁed RTDP trials memoizing dead ends basis functions learns way Whenever state evaluation line 21 ReTrASE ﬁnds state known deadend basis function holds ReTrASE uses regression procedure GetBasisFuncsForS presented Algorithm 3 generate basis function Regression yields basis functions approximate cost reaching goal Dd state given basis function given plan We use value initialize corresponding basis functions weight As GOTH deterministic planner prove nonexistence plan simply ﬁnd plan timeout state question deemed dead end line 10 Algorithm 3 For state s visited modiﬁed RTDP ModiﬁedBellmanBackup routine updates weight basis line 33 The new weight basis function reﬂected basis functions executed basis functions enable hold expected cost function enables execution currently optimal action expected cost action executed state basis functions hold quality weights Analogously actions irrelevant determining weights The intuitive reason updating basis functions enabling cid7 cid7 cid7 cid7 cid7 cid7 Theoretical properties A natural question ReTrASE convergence To answer proved following negative result Theorem 1 There problems ReTrASE converge Proof By failing converge mean problems depending order basis functions discovered ReTrASE indeﬁnitely oscillate set policies different expected costs One MDP M presented Fig 7 shows Ms transition graph action set Solving M amounts ﬁnding policy minimum expected cost takes agent state s0 state g uses actions a1 a5 The optimal solution M linear plan s0 a1 s1 a4 s4 a5 g A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 33 Fig 7 An example MDP ReTrASE fails converge To ReTrASE fails converge M simulate ReTrASEs operation MDP Recall ReTrASE executes series trials originating s0 cid2 Trial 1 To choose action s0 ReTrASE needs evaluate states s1 s2 It basis functions uses procedure Algorithm 3 generate initial estimates weights Suppose procedure ﬁrst looks basis function s1 ﬁnds plan s1 a4 s4 a5 g Regressing yields following basis functionweight pairs W A B C D 0 W A B C 1 W A B 2 A B basis functions holds s1 far Therefore current estimate value s1 V s1 2 Accordingly current estimate value action a1 s0 Qvalues0 a1 Ca1 V s1 4 Next suppose state s2 GetBasisFuncsForS ﬁnds plan s2 a3 g Regressing yields basis functionweight pair addition discovered ones W A D 1 Function A D holds s2 V s2 1 Qvalues0 a2 2 Now ReTrASE choose action s0 Since moment Qvalues0 a1 Qvalues0 a2 picks a2 executes transitioning s2 In s2 ReTrASE needs evaluate actions a3 a4 Notice a4 leads s5 dead end GetBasisFuncsForS discovers fact failing produce basis functions Thus V s5 large deadend penalty 1 000 000 yielding Qvalues2 a4 1 000 001 However a3 lead dead end s3 P 05 Qvalues2 a3 500 001 Nonetheless a3 preferable action ReTrASE picks s2 At time ReTrASE performs modiﬁed Bellman backup s2 The known basis function holds s2 enables chosen action a3 A D Therefore ReTrASE sets W A D Qvalues2 a3 500 001 Executing a3 s2 completes trial transition goal g dead end s3 Trial 2 This time ReTrASE select action s0 resorting regression Currently V s1 2 A B W A B 2 minimumweight basis function s1 However V s2 500 001 backup performed trial 1 Therefore Qvalues0 a1 4 Qvalues0 a2 500 002 making a1 look attractive So ReTrASE chooses a1 causing transition s1 In s1 choice a3 a4 The values easily calculated known basis functions Q values1 a3 500 001 Qvalues1 a4 2 The natural choice a4 ReTrASE performs corresponding backup The basis functions enabling a4 s1 A B A D Their weights Qvalues1 a4 2 update The rest trial change weights irrelevant proof Trial n Crucially basis function A D weight changed previous trials holds state s1 state s2 Due update s2 trial 1 W A D large s1 look beneﬁcial On hand thanks update s1 trial 2 W A D small s2 look beneﬁcial It easy cycle continue subsequent trials As result ReTrASE switching policies suboptimal cid2 Overall classes problems ReTrASE diverge hard characterize generally Predicting ReTrASE diverge particular problem area future work We maintain lack theoretical 34 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 Fig 8 Memory usage logarithmic scale ReTrASE dramatically eﬃcient LRTDP OPT LRTDP hFF guarantees indicative planners practical performance Indeed IPPC winners including FFReplan weak theoretical proﬁle The experimental results ReTrASE performs planning communitys benchmark problems 43 Experimental results Our goal subsection demonstrate important properties ReTrASE 1 scalability 2 quality solutions complex probabilistically interesting domains We start showing ReTrASE easily scales problems stateoftheart optimal nondeterminizationbased approximate planners run memory Then illustrate ReTrASEs ability compute better policies hard problems stateoftheart approximate planners Implementation details ReTrASE implemented C uses miniGPT 8 base RTDP code Our implementation prototype stage fully support PPDDL language features IPPC problems universal quantiﬁcation disjunctive goals rewards Experiment setup We report results problem sets Triangle Tire World TTW IPPC06 08 Drive IPPC06 Exploding Blocks World EBW IPPC06 08 Elevators IPPC06 In addition ran ReTrASE problems IPPC04 Since implementation support PPDDL features universal quantiﬁcation unable test remaining domains competitions However emphasize domains evaluate probabilistically interesting hard Even performance best IPPC participants leaves lot room improvement attests informativeness testbeds planner To provide basis comparison domains present results best IPPC participants Namely results IPPC winner domain overall winner IPPC For memory consumption experiment run VIfamily planners LRTDP inadmissible hFF LRTDP hFF LRTDP OPT LRTDP AtomMin1ForwardMinMin heuristic 8 Both bestknown topperforming planners type We ran ReTrASE test problems restrictions resembling IPPC Namely problem ReTrASE maximum 40 minutes training planners results present ReTrASE 30 attempts solve problem In IPPC winner decided success rate percentage 30 trials particular planner managed solve given problem Accordingly relevant graphs present ReTrASEs success rate competitors While analyzing results important aware ReTrASE implementation optimized Consequently ReTrASEs eﬃciency likely better indicated experiments Comparing scalability We begin showcasing memory savings ReTrASE LRTDP OPT LRTDP hFF Triangle Tire World domain Fig 8 demonstrates savings ReTrASE increase dramatically problem size In fact LRTDP variant able solve past problem 8 run memory ReTrASE copes problems Scalability comparisons domains tested yield generally similar results Other popular approximate algorithms aside LRTDP hFF suffer scalability issues LRTDP Thus meaningful compare ReTrASE quality solutions produced As ReTrASEs scalability allows successfully compete IPPC problems participant Comparing solution quality Success rate Continuing Triangle Tire World domain compare success rates ReTrASE RFF41 overall winner IPPC08 HMDPP 29 winner particular domain Note Triangle Tire World famous probabilistically interesting domain designed largely confound A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 35 Fig 9 ReTrASE achieves perfect success rate Triangle Tire World08 Fig 10 ReTrASE par competitors Drive Fig 11 ReTrASE dominates Exploding Blocks World06 solvers rely domain determinization 35 FFReplan performance particularly important evaluating new planner Indeed Fig 9 shows domain ReTrASE ties HMDPP achieving maximum possible success rate 100 problems outperforms competition winner solve problem 10 achieves 83success rate problem 9 On IPPC06 Drive domain ReTrASE fares Fig 10 Its average success rate ahead unoﬃcial domain winner FFReplan IPPC06 winner FPG differences insigniﬁcant For Exploding Blocks World domain IPPC06 version Fig 11 ReTrASE dominates planner wide margin problem Its edge especially noticeable hardest problems 11 15 On recent EBW problem set IPPC08 Fig 12 ReTrASE performs Even advantage apparent IPPC06 nonetheless ahead competition terms average success rate The Elevators Triangle Tire World06 domains easier ones presented Surprisingly Elevators problems ReTrASE converge allocated 40 minutes outperformed planners We suspect bad luck ReTrASE basis functions domain However TTW06 ReTrASE winner problem Comparing solution quality Expected cost On problems ReTrASE achieves maximum success rate ing ask close expected trajectory cost policy yields optimal The way ﬁnd expected cost optimal policy problem running optimal planner Unfortunately optimal 36 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 Fig 12 ReTrASE outmatches competitors Exploding Blocks World08 narrow margin Table 2 Success rates IPPC04 problems Problem explodingblock gtireproblem FFHop 9333 60 ReTrASE 100 70 planner LRTDP OPT scales solve relatively small problems million states On problems ReTrASE produce trajectories expected cost 5 optimal Comparison FFHop FFReplan powerful planner winner IPPC However recent benchmarks defeat exploiting nearcomplete disregard probabilities computing policy Researchers proposed powerful improvement FFReplan FFHop 43 demonstrated capabilities problems IPPC04 Unfortunately current lack support PPDDL language features able run ReTrASE IPPC04 domains Table 2 compares success rates planners IPPC04 problems test Even ReTrASE performs better problems small size experimental base makes comparison ReTrASE FFHop inconclusive While test IPPC domains current experimental evaluation clearly demonstrate ReTrASEs scalability improvements VIfamily planners atpar better performance competition problems compared stateoftheart systems 44 Summary ReTrASE MDP solver based combination state abstraction dimensionality reduction It automatically extracts basis functions provide compact representation given MDP retaining causal structure Simultaneously discovering basis functions learns weights discovered ones modiﬁed Bellman backups These weights let ReTrASE evaluate states memoizing state values explicitly Such approach allows ReTrASE solve larger problems best performers recent IPPCs 5 SIXTHSENSE 51 Motivation Although basis functions eﬃciently generalize information states reaching goal possible dead ends As result algorithms use basis functions information transfer avoid caching dead ends rediscovering time run In fact issue quickly reliably recognizing dead ends plagues virtually modern MDP solvers For instance IPPC2008 9 domains complex deadend structure Exploding Blocks World proven challenging Surprisingly little research methods effective discovery avoidance dead ends MDPs Of types dead ends implicit ones confound planners executable actions However explicit dead ends resource drain verifying available actions applicable state costly number actions large Broadly speaking existing planners use approaches identifying dead ends When faced yetunvisited state planners LRTDP apply heuristic value function hFF hopefully assigns high cost dead end states This method fast invoke fails catch implicit dead ends problem relaxation inevitably heuristics Failure detect causes planner waste time exploring states reachable implicit dead ends states dead ends Other MDP solvers use state value estimation A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 37 approaches recognize dead ends reliably expensive example RFF HMDPP ReTrASE employ deterministic planners When problem contains dead ends MDP solvers spend lot time launch ing classical planners dead ends Indeed probabilistic planners run faster recognizing dead ends computationally expensive In section complete abstraction framework presenting novel mechanism SixthSense exactly quickly reliably identify deadend states MDPs Underlying SixthSense pioneered 33 key insight large sets deadend states usually characterized compact logical conjunction nogood explains solution exists For example Mars rover ﬂipped upside unable achieve goal regardless location orientation wheels Knowing explanation lets planner quickly recognize millions states dead ends Crucially dead ends MDPs described small number nogoods SixthSense learns nogoods generating candidates bottomup greedy search resembling rule duction 12 tests avoid false positives planning graphbased procedure A vital input learning algorithm basis functions derived shown previous sections SixthSense provably sound nogood output represents set true dead ends We empirically demonstrate SixthSense speeds different types MDP solvers IPPC domains implicit dead ends performance improvements SixthSense gives GOTH ReTrASE Overall SixthSense tends identify dead ends solvers encounter reducing mem ory consumption 90 Because SixthSense runs quickly gives 3050 speedup large problems With savings enables planners solve problems previously handle 52 SixthSense description An MDP exponential number dead end states explanations state goal trajectory A Mars rover ﬂipped upside deadend state irrespective values variables In Drive domain IPPC06 states alive literal dead ends Knowing explanations obviates deadend analysis state individually need store explained dead ends order identify later Our method SixthSense strives induce explanations factored MDP setting use help planner recognize dead ends quickly reliably Formally objective ﬁnd nogoods conjunctions literals property states conjunction holds dead ends After nogood discovered planner encounters new state SixthSense notiﬁes planner state represented known nogood dead end To discover nogoods devise machine learning generateandtest algorithm integral SixthSense The generate step proposes candidate conjunction dead ends planner far training data For testing stage develop novel planning graphbased algorithm tries prove candidate nogood Nogood discovery happens attempts called generalization rounds First outline generate andtest procedure single round scheduler decides generalization round invoked Algorithm 4 contains learning algorithms pseudocode Generation candidate nogoods There ways generate candidate conjecture number ex planationsnogoods given problem small naive hypotheses conjunctions literals picked uniformly random unlikely pass test stage Instead procedure makes educated guess employing basis functions according crucial observation Recall deﬁnition basis functions preconditions goal trajecto ries Therefore state represented dead end On hand state represented nogood nogoods deﬁnition dead end These facts combine following observation state generalized basis function nogood Of practical importance corollary conjunction conﬂicting pairs literals literal negation contains negation literal basis function defeats basis function nogood This fact provides guiding principle form candidate going basis function problem candidate defeat picking negation basis functions literals By end run candidate provably defeats basis functions problem The idea big drawback ﬁnding basis functions problem prohibitively expensive Fortunately turns making sure candidate defeats randomly selected basis functions 100200 largest problems encountered practice candidate nogood reasonably high probability certain motivating need veriﬁcation Therefore invoking learning algorithm ﬁrst time implementation acquires 100 basis functions running classical planner FF Candidate generation described lines 511 So far speciﬁed exactly defeating literals chosen Here better naive uniform sampling Intuitively frequency literals occurrence dead ends MDP solver encountered far correlates likelihood literals presence nogoods The algorithms sampleDefeatingLiteral subroutine samples literal defeating basis function b probability proportionate literals frequency dead ends represented constructed portion nogood candidate The methods strengths twofold 38 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 end return failure c c L end end c defeat b literals L c checkWithPlanningGraphsetL c L g success declare literal L sampleDefeatingLiteralsetDEs b c c c L Algorithm 4 SixthSense 1 Input training set known nongeneralized dead ends setDEs set basis functions setBFs set nogoods setNG goal g set domain literals setL 2 3 function learnNogoodsetDEs setBFs setNGs g 4 construct candidate 5 declare candidate conjunction c 6 b setBFs 7 8 9 10 11 end 12 check candidate planning graph prune 13 checkWithPlanningGraphsetL c g 14 15 16 17 18 19 20 21 end 22 got candidate valid nogood 23 setDEs 24 add c setNG 25 return success 26 27 function checkWithPlanningGraphsetL c g 28 literals G g c declare conjunction c 29 30 PlanningGraphc success 31 32 33 end 34 return success 35 36 function sampleDefeatingLiteralsetDEs b c 37 declare counters CL L b c 38 d setDEs 39 40 41 42 43 44 end 45 return literal L cid7 c setL c G L b st L d sampled according P L c generalizes d return failure CL end end end cid7 cid7 C Lcid7 account information solvers experience lets literals cooccurrence patterns direct creation candidate Nogood veriﬁcation If candidate generation procedure set basis functions exist given MDP verifying resulting candidate necessary The set states represented ba sis function exhaustive set exhaustive set nondeadend states Therefore generated candidate represent deadend states true nogood However general possible basis functions disposal Consequently need verify candidate created algorithm available basis functions nogood Let denote problem establishing given conjunction nogood NOGOODDECISION Theorem 2 NOGOODDECISION PSPACEcomplete Proof First NOGOODDECISION PSPACE To verify conjunction nogood verify state conjunction represents dead end For state veriﬁcation equivalent establishing plan existence alloutcomes determinization MDP This problem PSPACEcomplete 11 PSPACE Thus nogood veriﬁcation broken set problems PSPACE PSPACE To complete proof point mentioned problem establishing deterministic plan existence instance NOGOODDECISION providing trivial reduction NOGOODDECISION PSPACEcomplete problem cid2 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 39 In light Theorem 2 realistically expect eﬃcient algorithm NOGOODDECISION sound complete A sound algorithm conclude candidate nogood A complete pronounce candidate nogood candidate fact nogood A key contribution paper sound algorithm identifying nogoods It based observation perstate checks naive scheme replaced running time polynomial problem size Although sound operation incomplete reject candidates fact nogoods Nonetheless check effective identifying nogoods practice To verify candidate c eﬃciently group nongoal states represented c superstates c We deﬁne superstate candidate c set consisting cs literals negation goal literals present c literals variables domain As example suppose complete set literals problem A A B B C C D D E E goal A B E candidate A C Then superstates algorithm constructs candidate A B C D D E E A B B C D D E negation goal literal superstate highlighted bold The intuition deﬁnition superstates c follows Every nongoal state s represented c contained superstates c sense superstate c containing ss literals Moreover superstate trajectory goal trajectory exists state contained superstate states dead ends Combining observations goal trajectory exists superstate c states represented candidate dead ends By deﬁnition candidate nogood Accordingly ﬁnd candidate nogood procedure runs planning graph algorithm candidates superstates determinized actions Each instance returns success reach goal literals resolve mutexes The initial set mutexes feeds planning graph mutexes literal negation Theorem 3 The candidate conjunction nogood planning graph expansions superstates fails achieve goal literals b fails resolve mutexes goal literals Proof Since planning graph sound failure superstate expansions indicates candidate true nogood lines 2734 cid2 Our procedure incomplete reasons First superstate literals single state contains goal trajectory impossible execute state Second planning graph algorithm incomplete declare plan existence plan actually exists At cost incompleteness algorithm polynomial problem size To note plan ning graph expansion superstate polynomial number domain literals number superstates polynomial number goal literals If veriﬁcation test passed try prune away unnecessary literals lines 1318 included candidate sampling This analog Occams razor strives reduce candidate minimal nogood gives general conjunction original little extra veriﬁcation cost At conclusion pruning stage compression empties set dead ends served training data MDP solver ﬁll new ones The motivation step clear discuss scheduling compression invocations Scheduling Since know priori number nogoods problem need perform generaliza tion rounds Optimally deciding hard impossible designed adaptive scheduling mechanism works practice It tries estimate size training set likely suﬃcient learning ex tra nogood invokes learning data accumulated When generalization rounds start failing scheduler calls exponentially frequently Thus little computation time wasted nogoods reasonably discovered discovered There certain kinds nogoods discovery SixthSense possible highly improbable We elaborate point Discussion section Our algorithm inspired following tradeoff The sooner successful round happens earlier SixthSense start resulting nogood saving time memory On hand trying soon hardly training data available improbable succeed The exact balance diﬃcult locate approximately empirical trials indicate helpful trends 1 The learning algorithm capable operating successfully surprisingly little training data 10 dead ends The number basis functions play big role provided 100 2 If round fails statistics collected given number dead ends number usually needs increased drastically However learning probabilistic failure accidental justiﬁable return bad training data size occasionally 3 A typical successful generalization round saves planner time memory compensate failed ones These regularities suggest following algorithm Initially scheduler waits small batch basis functions setBFs Algorithm 4 small number dead ends setDEs accumulated invoking ﬁrst generalization round For reasons implementation initial settings setBFs 100 setDEs 10 problems 40 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 After ﬁrst round including round succeeds scheduler waits number dead ends unrec ognized known nogoods equal half previous batch size arrive invoking round Decreasing batch size usually worth risk according observations 2 3 round succeeded If round fails scheduler waits accumulation twice previous number unrecognized dead ends trying generalization Perhaps unexpectedly cases seen large training sets decrease probability learning good This phenomenon explained training sets large sizes containing subcollections dead ends caused different nogoods Consequently literal occurrence statistics induced mix hard generate reasonable candidates This ﬁnding led restrict training batch size setDEs Algorithm 4 10 000 If exponential backoff scheduler forced wait arrival n 10 000 dead ends skips ﬁrst n 10 000 retains latest 10 000 training For locality considerations training set emptied end round line 23 Theoretical properties Before presenting experimental results analyze SixthSenses properties The important procedure identifying dead ends states nogood holds sound It follows directly nogoods deﬁnition Importantly SixthSense puts bounds nogood length theoretically capable discovering nogood One ask nontrivial bounds training data SixthSense generate nogood given length given probability As following argument indicates bounds exist likely use practice For SixthSense generate given nogood training data contain dead ends caused nogood However depending structure problem dead ends unreachable initial state If planning algorithm uses SixthSense explores parts state space LRTDP practically collectable training data help SixthSense discover nogoods high probability At time prove important property SixthSense Theorem 4 Once nogood discovered memoized SixthSense SixthSense discover Proof This fact consequence dead ends recognized known nogoods construct training sets described Scheduling subsection erasing training data generalization attempt According Algorithm 4 nogood candidate built iteratively sampling literals distribution induced training dead ends represented constructed portion candidate Also know training dead end represented known nogood Therefore probability sampling known nogood lines 511 0 cid2 Regarding SixthSenses speed number frequently encountered nogoods given problem small makes identifying dead ends iterating nogoods quick procedure Moreover generalization round polynomial training data size training data size linear size problem length dead ends basis functions We point obtaining training data theoretically takes exponential time Nevertheless training dead ends identiﬁed usual planning procedure MDP solvers extra work SixthSense obtaining basis functions Their required number small nearly probabilistic problem quickly obtained invoking speedy deterministic planner states This explains practice SixthSense fast Last believe SixthSense incorporated nearly existing trialbased factored MDP solver explained training data SixthSense requires available solvers cheaply extracted obtained independently solvers operation invoking deterministic planner 53 Experimental results Our goal experiments explore beneﬁts SixthSense brings different types planners gauge effectiveness nogoods computational resources taken generate We IPPC domains benchmarks Exploding Blocks World08 EBW08 Exploding Blocks World06 EBW06 Drive06 IPPC06 08 contained domains dead ends structure similar domains chose In experiments restricted MDP solver use 2 GB memory Structure dead ends IPPC domains Among IPPC benchmarks domains types implicit dead ends In Drive domain exempliﬁes ﬁrst agents goal stay alive reach destination driving road network traﬃc lights The agent die trying domain formulation necessarily prevent car driving reaching destination Thus implicit dead ends domain generalized singleton conjunction alive A IPPC domains Schedule resemble Drive having exclusively singleliteral nogoods representing dead ends Such nogoods typically easy SixthSense derive A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 41 Fig 13 Time memory savings nogoods LRTDP hFF representing Fast Insensitive type planners 3 domains percentage resources needed solve problems SixthSense higher curves indicate bigger savings points zero require resources SixthSense The reduction large problems reach 90 enable problems solved data points marked Fig 14 Resource savings SixthSense LRTDP GOTHNO 6S representing Sensitive Slow type planners EBW06 08s dead ends complex structure unique IPPC domains In EBW domain objective rearrange number blocks conﬁguration block explode process For goal literal EBW multipleliteral nogoods explaining literal achieved For example block b4 needs block b8 goal conﬁguration state b4 b8 explodes picked manipulator dead end represented nogood nodestroyed b4 holding b4 b4 b8 nodestroyed b8 b4 b8 We nogoods immediate point EBW types nogoods described Discussion section The variety structural complexity EBW nogoods makes challenging learn Planners As pointed earlier MDP solvers divided groups according way handle dead ends Some identify dead ends fast unreliable means like heuristics miss lot dead ends causing planner waste time memory exploring useless parts state space We planners fast insensitive respect dead ends Most use accurate expensive deadend identiﬁcation means We term planners sensitive slow treatment dead ends The monikers types apply way solvers handle dead ends overall performance With mind demonstrate effects SixthSense type Beneﬁts fast insensitive This group planners represented experiments LRTDP hFF heuristic We combination LRTDP hFF LRTDP hFF equipped SixthSense LRTDP hFF 6S short Implementationwise SixthSense incorporated hFF When evaluating newly encountered state hFF ﬁrst consults available nogoods produced SixthSense Only state fails match nogood hFF resort traditional means estimating state value Without SixthSense hFF misses dead ends ignores actions delete effects Fig 13 shows time memory savings SixthSense domains percentage resources LRTDP hFF took solve corresponding problems higher curves bigger savings No data points problems indicate LRTDP hFF LRTDP hFF 6S solve 2 GB RAM There large problems solved LRTDP hFF 6S Their data points marked savings set 100 problem 14 EBW06 matter visualization know resources LRTDP hFF need solve Additionally point general trend problems grow 42 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 Fig 15 SixthSense speeds ReTrASE 60 problems dead ends The plot shows trend example problem 12 EBW06 complexity domain increasing ordinal However increase diﬃculty guaranteed adjacent problems especially domains rich structure causing jaggedness graphs EBW06 08 As graphs demonstrate memory savings average grow gradually reach staggering 90 largest problems In fact problems marked enable LRTDP hFF 6S LRTDP hFF The crucial qualitative distinction LRTDP hFF 6S LRTDP hFF explaining nogoods help recognize states dead ends explore memoize descendants Notably time savings lagging smallest mediumsized problems approximately 17 However takes seconds solve overhead SixthSense slightly noticeable On large problems SixthSense fully comes element saves 30 planning time Beneﬁts sensitive slow Planners type include IPPC performers RFF HMDPP ReTrASE Most use deterministic planner FF domain determinization ﬁnd plan given state goal use plans ways construct policy Whenever deterministic planner prove non existence path goal fails simply ﬁnd certain time MDP solvers consider state planner launched dead end Due properties classical planners method deadend identiﬁcation reliable expensive To model employed LRTDP GOTH heuristic GOTH evaluates states classical planners including excluding SixthSense GOTH allows simulating effects SixthSense algorithms As SixthSense standard GOTH implementation GOTH denoted GOTHNO 6S Fig 14 illustrates LRTDP GOTHs behavior Qualitatively results look similar LRTDP hFF 6S subtle critical difference time savings case grow faster This manifestation fundamental distinction SixthSense settings For Sensitive Slow SixthSense helps recognize implicit dead ends faster obviates memoizing For Fast Insensitive obviates exploring implicit dead ends descendants causing faster savings growth problem size Beneﬁts ReTrASE ReTrASE natural MDP solver augmented SixthSense It uses basis functions store information nondeadend states utilizing nogoods allow capitalize abstraction framework providing additional insights beneﬁts planners employ abstraction framework serve state space representation needs To measure effect SixthSense ReTrASE different perspective role SixthSense previous experiments ran ReTrASE ReTrASE 6S 12 hours 45 problems EBW 06 08 Drive sets noted policy quality reﬂected success rate ﬁxed time intervals For smaller problems measured policy quality seconds larger ones 510 minutes Qualitatively trends problems similar study example problem 12 EBW06 set hardest problems attempted For problem 12 hours CPU running time ReTrASE 6S extracted learned weights 62 267 basis functions discovered 79 623 dead ends states Out dead ends 18 392 identiﬁed ReTrASE 6S running deterministic planner starting having planner fail ﬁnd path goal The remainder 77 discovered 15 nogoods SixthSense derived Since deterministic planner nondeadend state typically yields basis functions SixthSense saved ReTrASE 79 623 18 39262 267 79 623 43 classical planner invocations accompanying time savings On hand ReTrASEs running time occupied solely basis function extraction signiﬁcant fraction consists basis function weight learning state space exploration Besides SixthSense fast instantaneous Therefore based model expected overall speedup caused SixthSense 40 likely 30 With mind refer Fig 15 showing plots policy quality yielded byReTrASE ReTrASE 6S versus time As expected intuitively use SixthSense change ReTrASEs pattern convergence shape plots roughly similar If allowed run long planners converge policies A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 43 quality plots However surprisingly time takes ReTrASE 6S arrive policy quality ReTrASE gets 12 hours execution turns 55 hours Thus speedup SixthSense yielded considerably larger predicted model roughly 60 versus expected 30 Additional code instrumentation revealed explanation discrepancy The model sketched implicitly assumes time cost successful deterministic planner yields basis functions proves state dead end This appears far reality average 4 times expensive With factor taken account model forecast 77 time savings classical planner calls employment SixthSense With adjustments described earlier need ﬁgure agrees actual data Regarding memory savings SixthSense helps ReTrASE picture clearer Indeed Re TrASE memoizes basis functions weights dead ends nogoods 43 reduction total number predicted model straightforwardly translates corresponding memory reduction experiments showed We point SixthSense ReTrASEs memory requirements low compared MDP solvers reducing signiﬁcant performance gain boost speed Last SixthSense takes 10 LRTDP hFF 6Ss LRTDP GOTHs running time For LRTDP hFF 6S fraction includes time spent deterministic planner invocations obtain basis functions LRTDP GOTH classical plans available SixthSense free In fact problem size grows SixthSense eventually gets occupy 05 total planning time As illustration SixthSenses operation ﬁnds single nogood Drive domain 10 dead ends training manages acquire statistically signiﬁcant immediate dead ends EBW In available EBW problems number dozens considering space savings bring attests nogoods high eﬃciency 54 Summary GOTH machine learning algorithm discovering counterpart basis functions nogoods The presence nogood state guarantees state dead end Thus nogoods help planner quickly identify dead ends memoizing helping save memory time GOTH serves submodule planner periodically attempts guess nogoods dead ends planner visited basis functions planner discovered training data It checks guess sound planning graphbased veriﬁcation procedure Depending type MDP solver GOTH vastly speeds reduces memory footprint MDPs deadend states 6 Discussion The experiments indicate proposed abstraction framework capable advancing state art planning uncertainty Nonetheless promising directions future improvement Making structure extraction faster Even employment basis functions GOTH renders GOTH faster relatively classical planner invocations expensive GOTHs advantage informativeness suﬃcient secure overall advantage speed MDP solver uses Incidentally noticed domains ReTrASE spends lot time discovering basis functions end having high weights important We ways handling frameworks occasional lack speed discovering useful problem structure The ﬁrst approach motivated noticing speed basis function extraction depends critically fast available deterministic planners deterministic version domain hand Therefore speed issue alleviated adding modern classical planners portfolio launching parallel hope able cope quickly given domain Of course method backﬁre number employed classical planners exceeds number cores machine MDP solver running planners start contending resources Nonetheless limit increasing portfolio size help In addition reasonablysized portfolio planners help reduce variance average time takes arrive deterministic plan The idea extensional approach accelerate domain structure extraction increases formance algorithm making computational resources available There intensional improves algorithm The ultimate reason frequent discovery useless basis functions deterministic plan ning fact basis functions importance largely determined probabilistic properties corresponding trajectory alloutcomes determinization completely discards An alternative classical plan ners domain determinization retains probabilistic structure Although seemingly paradoxical determinizations exist proposed authors HMDPP Its use improve quality obtained basis functions reduce deterministic planning time spent discovering subpar ones Different determiniza 44 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 action beevil parameters precondition gremlinalive effect Screwdriver Wrench planebroken Hammer planebroken probabilistic 09 gremlinalive Fig 16 Action conditional effects tion strategies ease task classical planners provided determinization avoids enumerating outcomes action signiﬁcant losses solution quality Lifting representation ﬁrstorder logic Another potentially fruitful research direction increasing power ab straction lifting representation basis functions nogoods ﬁrstorder logic Such representations beneﬁts apparent example EBW domain In EBW immediate nogoods form block b goal position exploded block stack Indeed b ﬁrst need remove blocks including exploded stack EBW exploded blocks relocated Expressed ﬁrstorder logic statement clearly capture dead ends In propositional logic translate disjunction ground conjunctions nogood Each ground nogood separately accounts small fraction dead ends MDP undetectable statistically preventing SixthSense discovering Handling conditional effects So far assumed actions precondition simple conjunction literals PPDDLs recent versions allow expressive way actions applicability conditional effects Fig 16 shows action feature In addition usual precondition action separate precondition controlling possible effects Depending state subset actions effects executed The presented algorithms currently handle problems construct reasons First regression deﬁned Section 22 work conditional effects However deﬁnition easily extended cases As starting step consider goal trajectory te suppose outcome outai e te result applying action ai state si1 e Denote precondition kth conditional effect ai cond_preckai When e sampled conjunction outai e generated following way For k checked cond_preckai holds si1 If dice rolled select outcome corresponding conditional effect Denote outcome cond_outkai e Furthermore let litcond_outkai e let cond_outkai e conjunction cond_preckai hold si By deﬁnition cond_preckai cases If cond_preckai hold si1 If cond_preckai holds si1 sampling cond_outkai e happened pick outcome modify si1 In light fact deﬁne cumulative precondition outai e cu_prec cid4 cid5 outai e precai cond_preckai cid12 cid12 lit cid4 cid5 cond_outkai e cid15 cid2cid7 cid11 cid6 cid13 k observe outai e cid7 cid11 cid13 cond_outkai e k Thus cu_precoutai e conjunction preconditions conditional effects ai contributed literal outai e In words minimum necessary precondition outai e Therefore extend regression actions conditional effects simply substitute cu_precoutai e precai formulas generating basis functions Section 22 obtain b0 G bi cid7cid8cid8 litbi1 lit cid4 outani1 n 1 e cid5cid9 cid4 lit cid4 cid5cid5cid9 cu_prec outai e 1 cid2 cid2 n Unfortunately second practical diﬃculty making GOTH ReTrASE SixthSense work presence conditional effects Recall primary way obtaining goal trajectories regression deterministic planning A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 45 Determinizing ordinary probabilistic action yields number deterministic actions equal number original actions outcomes In presence conditional effects statement needs qualiﬁcation Each conditional effect thought describing action action probabilistic outcomes These inside actions need mutually exclusive Therefore number outcomes action conditional effects generally exponential latters number As consequence determinizing actions lead blowup problem representation size Further research needed identify special cases determinization conditional effects eﬃciently Beyond stochastic shortest path MDPs So far probabilistic planning community predominantly concentrated stochastic shortest path SSP MDPs subclasses discounted cost MDPs However interesting problems SSP MDPs As example consider SysAdmin domain 9 goal network computers running long possible This type reward maximization problems received little attention till recent attempt tackle eﬃciently heuristic search 34 Extending abstraction framework rewardmaximization MDPs potentially impactful research direction How meets practical theoretical diﬃculty Recall natural deterministic analog SSP MDPs shortest path problems Researchers studied extensively developed wide range eﬃcient tools solving FF LPG LAMA As shown earlier techniques presented critically rely tools extracting basis functions estimating weights However closest classical counterpart probabilistic rewardmaximization scenarios longest path problems Known algorithms formulations setting best exponential state space size explaining lack fast solvers In absence vention alternative eﬃcient ways extracting important causal information ﬁrst step way extending abstraction SSP MDPs Abstraction framework existing planners Despite improvements possible abstraction framework useful current state evidenced experimental theoretical results Moreover property makes use practical framework complementary powerful ideas incorporated successful solvers recent years HMDPP RFF FFHop Thus abstraction greatly beneﬁt solvers sacriﬁces inspire new ones As example note FFReplan enhanced abstraction following way It extract basis functions deterministic plans producing trying reach goal store weight action regressed obtaining particular basis function Upon encountering state subsumed known basis functions generalized FFReplan select action corresponding basis function smallest weight Besides accompanying speed boost minor point case FFReplan fast FFReplans robustness greatly improved way action selection informed trajectories state goal opposed Employed analogously basis functions speed FFHop MDP solver great potential somewhat slow current form In fact believe virtually algorithm solving SSP MDPs convergence accelerated regresses trajectories policy search carries information explored parts state space weakly probed ones help basis functions nogoods We hope verify conjecture future At time solvers discountedreward MDPs unlikely gain kind abstraction proposed paper mathematically described techniques work MDP class Discounted reward MDPs viewed SSP MDPs action probability leading directly goal 4 As result sequence actions discountedreward MDP goal trajectory This leads overabundance basis functions potentially making number comparable number states problem A different approach having abstraction beneﬁt existing planners let ReTrASE produce value function estimate allow planner LRTDP complete solution problem starting estimate This idea motivated fact hard know ReTrASE converged given problem Therefore makes sense algorithm convergence guarantees ReTrASE certain point Empirical research needed determine switch ReTrASE solver happen 7 Related work In spirit concept extracting useful state information form basis functions related explanation based learning EBL 3027 In EBL planner try derive control rules action selection analyzing execution traces In practice EBL systems suffer accumulating information approaches presented The idea determinization followed regression obtain basis functions parallels research relational MDPs uses ﬁrstorder regression optimal plans small problem instances construct policy large problems given domain 2139 However function aggregation weight learning methods completely different theirs ReTrASE essence exploits basis functions perform dimensionality reduction basis functions known alternative serve purpose Other ﬂavors dimensionality reduction include algebraic binary decision diagram ADDBDD principle component analysis PCA based methods SPUDD Symbolic LAO Symbolic RTDP 46 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 optimal algorithms exploit ADDs BDDs compact representation eﬃcient backups MDP 2515 While signiﬁcant improvement eﬃciency nonsymbolic counterparts optimal algorithms scale large problems APRICODD approximation scheme developed SPUDD 40 showed promise clear competitive todays methods applied competition domains Some researchers applied nonlinear techniques like exponentialPCA NCA dimensionality reduction 3828 These methods assume original state space continuous applicable typical planning bench marks In fact basis functionbased dimensionality reduction techniques applied nominal domains A notable exception FPG 10 performs policy search represents policy compactly neural network Our exper iments demonstrate ReTrASE outperforms FPG consistently domains The use determinization solving MDPs general inspired advances classical planning notably FF solver 26 The practicality new technique demonstrated FFReplan 42 FF planner MDP determinization direct selection action execute given state More recent planners employ determinization contrast FFReplan successful dealing probabilistically interesting problems include RFFRGBG 41 At time kind algorithms typically invokes deterministic planner times techniques This forces avoid alloutcomes determinization invocations costly Other related planners include Temptastic 44 precautionary planning 17 FFHop 43 The employment determinization heuristic function computation famous FF heuristic hFF 26 originally classical planner LRTDP 7 HMDPP 29 adopted heuristic modiﬁ cations In particular HMDPP runs hFF selfloop determinization MDP forcing hFF s estimates account problems probabilistic information To knowledge previous attempts handle identiﬁcation dead ends MDPs The Sensitive Slow Fast Insensitive mechanisms actually designed speciﬁcally purpose identifying dead ends unsatisfactory ways One possible reason omission MDPs studied Artiﬁcial Intelligence Operations Research communities recently dead ends However MDPs dead ends receiving attention past years researchers realized probabilistic interestingness 35 Besides analogy EBL SixthSense viewed machine learning algorithm rule induction similar purpose example CN2 12 While analogy valid SixthSense operates different requirements algorithms demand SixthSensederived rules nogoods zero falsepositive rate Last term nogood shares closely mirrors concept areas truth maintenance systems TMSs 13 constraint satisfaction problems CSPs 14 However methodology ﬁnding nogoods little common algorithms literature 8 Conclusions A central issue limits practical applicability automated planning uncertainty scalability available techniques In article presented powerful approach tackle fundamental problem abstraction framework extracts problem structure exploits spread information gained exploring MDPs state space The components framework elements problem structure called basis functions nogoods The basis functions preconditions sequences actions trajectories agent state goal positive probability As applies MDPs states sharing associated reachability information Crucially basis functions easy come fast deterministic planning byproduct normal probabilistic planning process While basis functions MDP states reaching goal possible counterparts nogoods identify dead ends goal reached Crucially number basis functions nogoods needed characterize problem space typically vastly smaller problems state space Thus framework variety ways increase scalability state art methods solving MDPs We described approaches illustrating frameworks operation GOTH ReTrASE SixthSense The ex perimental results promise outlined abstraction idea Although ways enhance existing framework utilized qualitatively improve scalability virtually modern MDP solver inspire techniques tomorrow References 1 D Aberdeen S Thiébaux L Zhang Decisiontheoretic military operations planning ICAPS04 2004 2 A Barto S Bradtke S Singh Learning act realtime dynamic programming Artiﬁcial Intelligence 72 1995 81138 3 R Bellman Dynamic Programming Princeton University Press 1957 4 D Bertsekas J Tsitsiklis NeuroDynamic Programming Athena Scientiﬁc 1996 5 D Bertsekas Dynamic Programming Optimal Control Athena Scientiﬁc 1995 6 A Blum M Furst Fast planning planning graph analysis Artiﬁcial Intelligence 90 1997 281300 7 B Bonet H Geffner Labeled RTDP Improving convergence realtime dynamic programming ICAPS03 2003 pp 1221 A Kolobov et al Artiﬁcial Intelligence 189 2012 1947 47 8 B Bonet H Geffner mGPT A probabilistic planner based heuristic search Journal Artiﬁcial Intelligence Research 24 2005 933944 9 D Bryce O Buffet International planning competition uncertainty Benchmarks results httpippc2008loriafrwikiimages003Resultspdf 2008 10 O Buffet D Aberdeen The factored policygradient planner Artiﬁcial Intelligence Journal 173 2009 722747 11 Tom Bylander The computational complexity propositional STRIPS planning Artiﬁcial Intelligence 69 1994 165204 12 Peter Clark Tim Niblett The CN2 induction algorithm Machine Learning 1989 pp 261283 13 Johan Kleer An assumptionbased tms Artiﬁcial Intelligence 28 1986 127162 14 R Dechter Constraint Processing Morgan Kaufmann Publishers 2003 15 Z Feng E Hansen Symbolic heuristic search factored Markov decision processes AAAI02 2002 16 C Forgy Rete A fast algorithm patternmany object pattern match problem Artiﬁcial Intelligence 19 1982 1737 17 J Foss N Onder D Smith Preventing unrecoverable failures precautionary planning ICAPS07 Workshop Moving Planning Schedul ing Systems Real World 2007 18 A Gerevini A Saetti I Serina Planning stochastic local search temporal action graphs Journal Artiﬁcial Intelligence Research 20 2003 239290 19 J Goldsmith M Littman M Mundhenk The complexity plan existence evaluation probabilistic domains UAI97 1997 20 Geoff Gordon Stable function approximation dynamic programming ICML 1995 pp 261268 21 C Gretton S Thiébaux Exploiting ﬁrstorder regression inductive policy selection UAI04 2004 22 C Guestrin D Koller C Gearhart N Kanodia Generalizing plans new environments relational MDPs IJCAI03 Acapulco Mexico 2003 23 Carlos Guestrin Daphne Koller Ronald Parr Shobha Venkataraman Eﬃcient solution algorithms factored MDPs Journal Artiﬁcial Intelligence Research 19 2003 399468 24 E Hansen S Zilberstein LAO 25 J Hoey R StAubin A Hu C Boutilier SPUDD Stochastic planning decision diagrams UAI99 1999 pp 279288 26 J Hoffman B Nebel The FF planning Fast plan generation heuristic search Journal Artiﬁcial Intelligence Research 14 2001 253 A heuristic search algorithm ﬁnds solutions loops Artiﬁcial Intelligence 129 12 2001 3562 302 27 S Kambhampati S Katukam Q Yong Failure driven dynamic search control partial order planners An explanation based approach Artiﬁcial Intelligence 88 1996 253315 28 Philipp Keller Shie Mannor Doine Precup Automatic basis function construction approximate dynamic programming reinforcement learning ICML06 2006 pp 449456 29 E Keyder H Geffner The HMDPP planner planning probabilities Sixth International Planning Competition ICAPS08 2008 30 C Knoblock S Minton O Etzioni Integrating abstraction explanationbased learning PRODIGY Ninth National Conference Artiﬁcial Intelligence 1991 31 A Kolobov Mausam D Weld ReTrASE Integrating paradigms approximate probabilistic planning IJCAI09 2009 32 A Kolobov Mausam D Weld Classical planning MDP heuristics With little help generalization ICAPS10 2010 33 A Kolobov Mausam D Weld SixthSense Fast reliable recognition dead ends MDPs AAAI10 2010 34 A Kolobov Mausam D Weld Heuristic search generalized stochastic shortest path mdps ICAPS11 2011 35 Iain Little Sylvie Thiébaux Probabilistic planning vs replanning ICAPS Workshop IPC Past Present Future 2007 36 Mausam E Benazara R Brafman N Meuleau E Hansen Planning continuous resources stochastic domains IJCAI05 2005 p 1244 37 Mausam P Bertoli D Weld A hybridized planner stochastic domains IJCAI07 2007 38 Nicholas Roy Geoffrey Gordon Exponential family PCA belief compression POMDPs NIPS02 MIT Press 2003 pp 10431049 39 S Sanner C Boutilier Practical linear valueapproximation techniques ﬁrstorder MDPs UAI06 2006 40 R StAubin J Hoey C Boutilier APRICODD Approximate policy construction decision diagrams NIPS00 2000 41 F TeichteilKönigsbuch U Kuter G Infantes Incremental plan aggregation generating policies MDPs AAMAS10 2010 42 S Yoon A Fern R Givan FFReplan A baseline probabilistic planning ICAPS07 2007 pp 352359 43 S Yoon A Fern S Kambhampati R Givan Probabilistic planning determinization hindsight AAAI08 2008 44 HLS Younes RG Simmons Policy generation continuoustime stochastic domains concurrency ICAPS04 2004 p 325