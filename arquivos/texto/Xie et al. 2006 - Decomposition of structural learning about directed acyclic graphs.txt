Artiﬁcial Intelligence 170 2006 422439 wwwelseviercomlocateartint Decomposition structural learning directed acyclic graphs Xianchao Xie Zhi Geng Qiang Zhao ab School Mathematical Sciences LMAM Peking University Beijing 100871 China b Institute Population Research Peking University Beijing 100871 China Received 8 February 2005 received revised form 21 November 2005 accepted 16 December 2005 Available online 3 February 2006 Abstract In paper propose structural learning directed acyclic graph decomposed problems related decomposed subgraphs The decomposition structural learning requires conditional independencies require separators complete undirected subgraphs Domain prior knowledge conditional independencies utilized facilitate decomposition structural learning By decomposition search dseparators large network localized small subnetworks Thus efﬁciency structural learning power conditional independence tests improved 2005 Elsevier BV All rights reserved Keywords Bayesian network Conditional independence Decomposition Directed acyclic graph Junction tree Structural learning Undirected graph 1 Introduction Directed acyclic graphs DAGs widely represent independencies conditional independencies causal relationships variables 56815181924 Structure recovery DAGs discussed authors 512192427 Search dseparators vertex pairs key issue orientation directed edges recovering DAG structures causal relations variables To recover structure DAGs Verma Pearl 27 presented inductive causation IC algorithm searches dseparator S possible variable subsets variables u v independent conditional S A systematic way searching dseparators increasing order cardinality proposed 2324 The PC algorithm limits possible dseparators vertices adjacent u v 1924 A decomposition approach searching dseparators presented 11 To decompose graph subgraphs approach 11 needs moral graph requires conditions variable sets subgraphs independent conditional separator ii separator complete subgraph moral graph The conditions deﬁne decomposition undirected graph Deﬁnitions 21 22 15 In paper present decomposition approach recovering structures DAGs The ultimate use structed DAGs interpret association causal relationships variables Decomposition approach Corresponding author Email addresses xie1981waterpkueducn XC Xie zgengmathpkueducn Z Geng zhqmathpkueducn Q Zhao 00043702 matter 2005 Elsevier BV All rights reserved doi101016jartint200512004 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 423 needs undirected independence graph moral graph extra edges added moral graph requires condition conditional independencies require condi tion ii complete separators Thus decomposition weaker weak decomposition deﬁned 15 proposed 11 Deleting condition ii decomposition conditions important difﬁcult domain prior knowledge judge separator complete In practical applications dition conditional independencies judged domain prior knowledge incompletely observed data patterns Markov chain chain graphical models dynamic temporal models ﬁlematching large databases split questionnaire survey sampling 61620 Section 2 gives notation deﬁnitions In Section 3 condition decomposing structural learning DAGs Construction dseparation trees decomposition discussed Section 4 We propose main algorithm example Section 5 illustrate approach recovering global structure DAG Section 6 discusses complexity advantages proposed algorithms Conclusions given Section 7 The proofs main results algorithms given Appendix A 2 Notation deﬁnitions 21 Directed acyclic graphs undirected graphs Let cid2GV V cid2EV denote DAG V X1 Xn vertex set cid2EV set directed edges A directed edge vertex u vertex v denoted cid3u vcid4 We assume directed loop cid2GV We u parent v v child u directed edge cid3u vcid4 denote set parents vertex v pav We vertices u v adjacent cid2GV edge connecting A path l distinct vertices u v sequence distinct vertices ﬁrst vertex u v consecutive vertices connected edge l c0 u c1 cm1 cm v cid3ci1 cicid4 cid3ci ci1cid4 contained cid2EV 1 m m cid1 1 ci cid5 cj cid5 j We u ancestor v v descendant u path u v cid2GV edges path point direction v The set ancestors v denoted anv deﬁne Anv anv v A path l said dseparated set vertices Z 1 l contains chain u v w fork u v w middle vertex v Z 2 l contains collider u v w middle vertex v Z descendant v Z Two distinct sets X Y vertices dseparated set Z Z dseparates path vertex X vertex Y We Z dseparator X Y In DAG cid2GV collider u v w called vstructure u w nonadjacent cid2GV Let GV V EV denote undirected graph EV set undirected edges An undirected edge vertices u v denoted u v For subset A V let GA A EA subgraph induced A EA e EV e A A EV A A An undirected graph called complete pair vertices connected edge For undirected graph vertices u v separated set vertices Z path u v passes Z We distinct vertex sets X Y separated Z Z separates pair vertices u v u X v Y We undirected graph GV undirected independence graph DAG cid2GV fact set Z separates X Y GV implies Z dseparates X Y cid2GV We GV decomposed subgraphs GA GB 1 A B V 2 C A B separates A B B A GV The decomposition require separator C complete required weak decomposition deﬁned 15 decomposition search vstructures proposed 11 In section problem structural learning DAG decomposed problems decomposed subgraphs separator complete 424 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 b Fig 1 A directed graph moral graph triangulated graph The DAG cid2GV b The moral graph cid2Gm V c A triangulated graph cid2Gt V c Deﬁne moral graph cid2Gm A triangulated graph undirected graph cycle length cid1 4 possesses chord 15 For undirected graph GV triangulated add extra edges triangulated graph denoted Gt V V DAG cid2GV undirected graph GV V EV vertex set V edge set constructed marrying parents dropping directions EV u v cid3u vcid4 cid3v ucid4 cid2EV u v u w v forms vstructure 15 An undirected edge added marrying parents called moral edge The moral graph cid2Gm V undirected independence graph cid2GV 15 Example 1 Consider DAG cid2GV Fig 1a 2 4 3 4 7 6 vstructures A path l 2 1 5 dseparated vertex 1 path lcid11 2 4 7 6 5 dseparated set 4 7 6 collider Vertices 2 5 dseparated vertex 1 an4 1 2 3 An4 1 2 3 4 The moral graph cid2Gm V shown Fig 1b edges 2 3 4 6 moral edges Set 2 3 5 separates 1 4 6 7 cid2Gm V decomposed undirected subgraphs 1 2 3 5 2 7 An undirected independence graph cid2GV extra undirected edges added moral graph edges 1 4 1 6 added cid2Gm V dashed edges Fig 1c The graph Fig 1c triangulated graph cid2Gm V Given DAG cid2GV joint distribution density variables X1 Xn P x1 xn ncid1 i1 P xi pai P xi pai conditional probability density Xi given paXi pai The DAG cid2GV distribution P said compatible 19 P obeys global directed Markov property cid2GV 15 Let X Y denote independence X Y X Y Z conditional independence X Y given Z If sets X Y dseparated Z X independent Y conditional Z distribution compatible cid2GV 19 In paper assume distributions compatible cid2G We assume independencies probability distribution variables V checked dseparations cid2GV called faithfulness assumption 24 The faithfulness assumption means independencies conditional independencies variables represented cid2GV The global skeleton undirected graph obtained dropping direction DAG Thus absence edge u v implies variable subset S V u v independent conditional S vS S V u v Two DAGs variable set called Markov equivalent u induce conditional independence restrictions Two DAGs Markov equivalent global skeleton set vstructures 27 An equivalence class DAGs consists DAGs Markov equivalent represented partially directed graph directed edges represent arrows XC Xie et al Artiﬁcial Intelligence 170 2006 422439 425 Fig 2 A dseparation tree common DAG undirected edges represent proper orientation leads Markov equivalent DAG Therefore goal structural learning construct partially directed graph represent equivalence class A local skeleton subset A variables undirected subgraph A absence edge u v implies subset S A u vS 22 dseparation trees To depict separations conditional independencies multiple variable sets introduce notion dseparation trees subsection Let C collection variable sets C C1 CH cid2 H h1 Ch V Ci cid5 Cj cid5 j Let T tree node variable set C displayed triangle The term node tree distinguish term vertex graph An edge eh Ci Cj connects nodes Ci Cj T attached separator S displayed rectangle intersection adja cent nodes S Ci Cj We separator S connects nodes Ci Cj Removing separator S tree T splits T subtrees T1 T2 node sets C1 C2 respectively Let Vi C union nodes subtree Ti 1 2 CCi cid2 Deﬁnition 1 A tree T said dseparation tree DAG cid2G separator S T dseparates cid2G vertex sets V1 S V2 S subtrees T1 T2 obtained removing S The notion dseparation tree similar junction tree A dseparation tree deﬁned d separation need node clique junction tree dseparation tree Theorem 2 Example 1 Continued Let C 1 2 3 4 1 4 6 1 5 6 4 6 7 collection variable sets A dsepa ration tree C node set depicted Fig 2 Removing separator S 1 4 obtain subtrees T1 T2 node sets C1 1 2 3 4 C2 1 4 6 1 5 6 4 6 7 respectively separator S dseparates V1 S 2 3 V2 S 5 6 7 cid2GV 23 Hypergraph A collection variable sets C C1 CH said hypergraph V hyperedge Ch H h1 Ch V Chapter 17 4 A hypergraph reduced hypergraph nonempty subset variables Ci cid5 Cj cid5 j 3 In paper reduced hypergraphs simply called hypergraphs In Section 42 hyperedge represent domain knowledge association variables represent multiple databases overlapping cid2 Example 1 Continued Let C 1 2 3 4 1 3 5 6 4 6 7 hypergraph shown Fig 3 3 Decomposition structural learning In section vertices u v dseparated dseparator S DAG cid2GV u v contained node dseparation tree cid2GV exists node C contains u v Scid11 vScid11 vice versa Applying result structural learning split problem searching u 426 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 Fig 3 A hypergraph dseparators building skeleton DAG small problems node T This result useful designing observational studies recovering local causal relationships Theorem 1 Let T dseparation tree DAG cid2G Vertices u v dseparated S V cid2GV u v contained node C T ii exists node C contains u v subset Scid11 C dseparates u v According Theorem 1 problem searching dseparator S u v possible subsets V localized possible subsets nodes dseparation tree contain u v For given dseparation tree T node set C C1 CH recover skeleton vstructures DAG follows First construct local skeleton node Ch T constructed starting complete undirected subgraph removing undirected edge u v subset S Ch u v independent conditional S Next construct global skeleton combine local skeletons remove edges present local skeletons absent local skeletons Then determine vstructure nonadjacent vertices u v common neighbor global skeleton neighbor contained dseparator u v Finally orient undirected edges opposite creates directed cycle new vstructure 17 This process formally described following algorithm Algorithm 1 Construct equivalence class DAGs dseparation tree 1 Input dseparation tree T node set C C1 CH 2 Construct local skeleton Gh h separately Initialize Gh complete undirected graph If exists subset Suv Ch u v u dseparator list S 3 Construct global skeleton GV vSuv delete edge u v Gh save Suv Initialize edge set EV GV union edge sets Gh h 1 H For pair vertices u v contained local skeletons delete edge u v EV absent skeleton 4 For dseparator Suv list S determine vstructure u w v uwv appears global skeleton w Suv 5 Orient edges opposite creates directed cycle new vstructure 6 Output equivalence class DAGs According Theorem 1 prove global skeleton vstructures obtained applying decomposition algorithm correct obtained joint distribution V Appendix A proof Note separators dseparation tree complete moral graph Thus decomposition weaker decomposition usually deﬁned parameter estimation 51015 Example 1 Continued Consider dseparation tree Fig 2 input Algorithm 1 At step 2 separately build local skeleton node T shown Fig 4a At step 3 global skeleton obtained combining local skeletons Fig 4a Edge 1 6 subgraph node 1 4 6 spurious edge removed global graph edge 1 6 appear subgraph node 1 5 6 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 427 b Fig 4 Skeleton vstructures cid2GV Local skeletons node T b The global skeleton vstructures dseparator S16 5 dseparates 1 6 subgraph Similarly edge 4 6 removed absent local skeleton At step 4 vstructures determined Fig 4b For example vstructure 2 4 3 determined S23 1 S 243 appears global skeleton vertex 4 contained S23 Similarly vstructure 4 7 6 S46 1 S 476 appears global skeleton vertex 7 contained S46 But S16 5 S 156 appears vertex 5 contained S16 vstructure For DAG edges oriented step 5 The partially directed graph Fig 4b equivalence class DAG Fig 1a By Theorem 1 ensured global skeleton vstructures cid2GV recovered combining subgraphs nodes T In section discuss construct dseparation tree 4 Constructing dseparation tree In section discuss construct dseparation tree observed data domain prior knowl edge conditional independencies collection databases We ﬁrst propose approach undirected independence graph junction tree built observed data junction tree dseparation tree Next propose approach constructing dseparation tree domain knowledge collection databases different observed variable sets 41 Constructing dseparation tree observed data In algorithms structural learning ﬁrst step construct undirected independence graph vV u v To construct undirected graph start absence edge u v implies u complete undirected graph pair variables u v undirected edge u v removed u v independent conditional set variables For linear Gaussian models undirected graph efﬁciently constructed removing edge u v corresponding entry inverse covariance matrix zero For discrete data test conditional independence given large number discrete variables extremely low power To cope difﬁculty vertex u ﬁrst use information criterion ﬁnd variable subset contains Markov blanket u 18 test independence u variable conditionally variable subset 1426 This test efﬁcient Markov blanket u large For discrete data use algorithm proposed 9 reduces requirement testing high order conditional independencies A dseparation tree built constructing junction tree undirected independence graph 5 algorithm presented Section 42 Theorem 2 A junction tree constructed undirected independence graph cid2GV dseparation tree cid2GV 428 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 A dseparation tree T requires dseparation properties T hold cid2GV reverse required Thus need construct undirected independence graph fewer conditional independencies moral graph means undirected independence graph extra edges added moral graph Thus null hypothesis absence undirected edge tested statistically larger signiﬁcance level provided nodes dseparation tree contain variables Example 1 Continued To construct dseparation tree cid2GV Fig 1a ﬁrst undirected independence graph vV u v An undirected graph constructed starting complete graph removing edge u v u obtained way moral graph Fig 1b In fact need construct undirected independence graph extra edges added moral graph Next triangulate undirected graph ﬁnally obtain dseparation tree shown Fig 1c Fig 2 respectively 42 Constructing dseparation tree domain knowledge observed data patterns In subsection propose approach constructing dseparation tree domain knowledge observed data patterns conditional independence tests The domain knowledge experts prior knowl edge dependencies variables Markov chains chain graphical models dynamic temporal models In practical applications ﬁlematching split questionnaire survey sampling 1620 large databases different data patterns databases different attributes overlay Based domain knowledge dependencies data patterns databases designed recover entire structure variable set V correctly The problem reconstructing DAG multiple overlapping databases considered rules determining absence edges proposed 7 As author noticed rules exhaust possible rules existence remains open problem We theoretically prove approach perfect entire DAG reconstructed multiple databases reconstructed joint data variable set V multiple databases designed based domain knowledge dependencies We ﬁrst consider simple case variable sets A B Let C A B Suppose domain knowledge variable A C associates variable B C variables C implies B CC We depict hypergraph obtain dseparation tree shown Fig 5a A C b respectively The domain knowledge seen Markov property future state B C independent previous state A C conditional current state C For general case domain knowledge variable dependencies represented collection variable sets C C1 CH variables contained set associate directly variables contained different sets associate variables This means variables contained set independent conditionally variables We depict domain knowledge hypergraph Then equivalently domain knowledge legitimate edge moral graph Gm V underlying DAG contained hyperedge C A slightly stronger condition judging legitimacy domain knowledge variable u hyperedge Ch C contains u parent set On hand application study observed data collection different observed patterns C C1 CH Ch set observed variables hth group individuals For example observed data patterns C b c b c d mean groups individuals 1 variables b c observed variable d missing ﬁrst group 2 variables b c d observed variable missing second group Data having observed patterns uncommon ﬁlematching split questionnaire survey 1620 Given observed data patterns C C1 CH information association variables observed parameters relate association b Fig 5 A domain knowledge associations variables A hypergraph b A dseparation tree XC Xie et al Artiﬁcial Intelligence 170 2006 422439 429 inestimable assumptions The condition algorithms correct structural learning collection C C contain sufﬁcient data parameters underlying DAG estimable For DAG parameters estimable variable u observed data pattern Ch C contains u parent set Thus collection C observed patterns sufﬁcient data correct structural learning pattern Ch C u Ch contains u parent set underlying DAG Every pattern Ch seen hyperedge maximum complete undirected graph depict possible association variables Ch When variables categorical loglinear model generating class C C1 CH highest order interaction model latent variables We assume independencies inferred observed data patterns true underlying DAG Example 1 Continued Suppose hypergraph C 1 2 3 4 1 3 5 6 4 6 7 depicted Fig 3 It considered domain prior knowledge associations variables The hypergraph Fig 3 considered observed data patterns databases different attributes overlap In case variables categorical latent variable loglinear model generating class 1234 1356 467 includes interactions variables estimated observed data Since variable u hyperedge C contains u parent set hypergraph C legitimate domain knowledge legitimate collection databases underlying DAG Fig 1a However hypergraph C changed Ccid11 1 2 4 1 3 5 6 4 6 7 hyperedge 1 2 3 4 C replaced 1 2 4 hyperedge Ccid11 contains variable 4 parent set 2 3 hypergraph Ccid11 legitimate collection databases legitimate domain knowledge DAG Now discuss construct dseparation tree hypergraph represents domain knowledge dependencies observed data patterns First explain following example global skeleton DAG cid2GV constructed combining subgraphs obtained separately data pattern Then propose algorithm constructing correct global skeleton Example 2 Consider DAG cid2GV observed data patterns depicted Fig 6a From data patterns separately undirected subgraphs 12 13 23 Combining obtain combined undirected graph Fig 6b correct skeleton cid2GV edge 2 3 spurious edge Below propose algorithm constructing dseparation tree T domain knowledge observed data patterns correct skeleton constructed combining subgraphs nodes T Since hyper edge Ch represents possible associations variables Ch ﬁrst use complete subgraph Ch undirected independence graph variables Ch piece subgraphs entire undirected graph To reduce sizes tree nodes construct junction tree terms triangulating undirected graph In way following algorithm constructs dseparation tree T hypergraph C C1 CH domain knowledge observed data patterns Algorithm 2 Construct dseparation tree hypergraph 1 Input hypergraph C C1 CH hyperedge Ch variable set 2 For hyperedge Ch construct complete undirected graph Gh edge set Ch Ch Eh u v u v Ch Fig 6 An incorrect skeleton combined subgraphs A DAG observed data patterns b The combined undirected graph b 430 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 b Fig 7 Construct dseparation tree hypergraph The undirected graph obtained combining complete subgraphs b The dseparation tree 3 Construct entire undirected graph GV V E edge set E E1 EH 4 Construct junction tree T triangulating GV 5 Output T dseparation tree hypergraph C The correctness Algorithm 2 proven Appendix A Note need conditional independence test Algorithm 2 construct dseparation tree In algorithm use heuristic triangulation algorithm computational complexity construct junction tree 21321 Below examples illustrate Algorithm 2 Example 1 Continued Consider domain knowledge associations variables given Fig 3 At step 2 construct complete subgraph hyperedge step 3 combine depicted Fig 7a At step 4 construct dseparation tree Fig 7b Note edges added triangulation undirected graph Fig 7a triangulated Example 3 Suppose V 1 6 domain knowledge C C1 C2 C3 C4 1 2 3 1 2 4 1 3 5 4 5 6 depicted Fig 8a At step 2 construct complete subgraphs step 3 combine shown undirected graph solid lines Fig 8b At step 4 triangulate edge 3 4 dash line obtain dseparation tree Fig 8c The nodes dseparation tree T constructed domain knowledge observed data patterns large In case use conditional independence tests reduce node sizes We ﬁrst construct undirected independence subgraph node combine subgraphs global undirected independence graph ﬁnally construct reﬁned dseparation tree Section 5 For node Ch T h 1 H undirected independence subgraph Gh Ch Eh constructed starting complete subgraph removing undirected edge u v u v independent conditional Ch u v Theorem 3 Suppose Gh Ch Eh independence subgraph node Ch dseparation tree h 1 H The undirected graph GV edge set EV H h1 Eh undirected independence graph cid2GV cid2 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 431 b Fig 8 Construct dseparation tree Domain knowledge associations b The undirected graph triangulation c The dseparation tree c Fig 9 An undirected independence graph obtained combining subgraphs Undirected independence subgraphs node b Combining subgraphs b Note combination obtaining global undirected independence graph different obtaining global skeleton proposed Section 3 The pools edges subgraphs global graph deletes edge u v global graph present subgraph absent subgraph Example 1 Continued We suppose dseparation tree cid2GV node set C 1 2 3 5 2 3 4 5 6 7 intersection S 2 3 5 dseparates 1 4 6 7 Construct undirected independence subgraphs node separately shown Fig 9a construct global undirected independence graph edges subgraphs shown Fig 9b The undirected independence graph moral graph extra edges 3 5 2 5 added moral graph 5 Illustration structural learning decomposition In section ﬁrst formally complete algorithm structural learning DAGs decomposition Then illustrate algorithm ALARM network Fig 10 evaluate structural learning algorithms 11224 Main algorithm The decomposition approach structural learning DAGs domain knowledge 1 Input hypergraph C C1 CH domain knowledge databases observed data patterns C 2 Call Algorithm 2 construct dseparation tree T hypergraph C 3 If sizes nodes T large reﬁne T observed data Construct undirected independence subgraph node T Let edge set EV entire undirected independence graph GV union edge sets subgraphs Construct junction tree T cid11 triangulating GV dseparation tree 432 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 Fig 10 The ALARM network domain knowledge Fig 11 The dseparation tree T domain knowledge 4 Call Algorithm 1 construct equivalence class DAGs T cid11 T sizes nodes large 5 Output equivalence class DAGs The correctness main algorithm proven Appendix A To reduces requirement testing high order conditional independencies ﬁrst construct dseparation tree step 2 based prior knowledge necessary reduce size nodes dseparation tree step 3 Note undirected independence graph obtained step 3 extra edges tests conditional independencies performed large signiﬁcance level The conditional independence tests performed Algorithm 1 marginalized nodes dseparation tree The ALARM network Fig 10 describes causal relations 37 variables medical diagnostic patient monitoring Using network researchers generate continuous data normal distributions generate discrete data multinomial distributions 1224 Our approach applicable continuous discrete data Since correctness algorithms proved Appendix A complexity analysis accuracy independence tests discussed section main algorithm illustrated conditional independencies underlying DAG conditional independence tests simulated data Suppose domain knowledge associations variables depicted hypergraph Fig 10 The hypergraph seen databases overlap Since variable u ALARM hyperedge contains u parent set domain knowledge collection data base legitimate structural learning ALARM network However hyperedge 8 28 29 30 31 32 changed hyperedge 8 29 30 31 32 hyperedge contains variable 29 parent set 8 28 parameters ALARM network inestimable changed hypergraph domain knowledge collection databases legitimate At step 2 dseparation tree T constructed hypergraph shown Fig 11 Note step 2 need conditional independence test construct dseparation tree based domain knowledge Since nodes T obtained step 2 large ﬁrst construct undirected independence subgraphs nodes separately step 3 shown Fig 12 Next combining subgraphs XC Xie et al Artiﬁcial Intelligence 170 2006 422439 433 b Fig 12 Undirected independence graphs nodes T A subgraph left node T b A subgraph middle node T c A subgraph right node T c Fig 13 The global triangulated graph triangulating obtain triangulated independence graph Fig 13 From triangulated graph construct dseparation tree T cid11 Fig 14 separators omitted largest node 5 variables At step 4 construct subskeleton node T cid11 shown Fig 15 Combining subskeletons determining vstructures orienting edges possible obtain equivalence class Fig 16 directed edges oriented correctly undirected edges 5 27 10 33 21 34 35 37 oriented orientation leads Markov equivalent DAG Note edges Fig 16 oriented step 5 Algorithm 1 For example direction undirected edge 1 2 determined cid32 1cid4 cid34 2cid4 create new vstructure direction undirected edge 16 23 determined cid316 23cid4 cid316 20cid4 cid320 23cid4 create cycle The global skeleton vstructures depicted Fig 16 obtained joint distribution variables V Applying decomposition split graph 37 vertices 28 subgraphs largest contains 5 vertices In way problem highdimensional structural learning reduced small problems 434 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 Fig 14 The dseparation tree T cid11 Fig 15 Skeletons nodes T cid11 6 Complexity analysis advantages There obvious advantages decomposition approach structural learning proposed paper First dseparation tree constructed based prior domain knowledge conditional inde pendence tests By dseparation tree independence tests performed conditionally smaller sets contained node dseparation tree set variables Thus algorithm higher power statistical tests XC Xie et al Artiﬁcial Intelligence 170 2006 422439 435 Fig 16 The partially directed acyclic graph Second theoretical results proposed paper applied scheme design multiple databases Without loss information structural learning DAGs joint data set replaced group incomplete data sets based domain prior knowledge conditional independencies variables Finally computational complexity reduced This complexity analysis focuses number conditional independence tests constructing equivalence class Decomposition graphs computationally simple task compared conditional independence tests The triangulation undirected graph algorithms construct dseparation undirected independence graph Although problem optimally triangulating undirected graph NPhard suboptimal triangulation methods 21321 provided obtained tree contain large nodes test conditional independencies Two wellknown algorithms lexicographic search 22 maximum cardinality search 25 complexities One On e respectively Thus algorithms conditional independence tests dominate algorithmic complexity Let V denote set variables n number variables V In IC algorithm delete edge pair variables u v need test independencies u v conditionally possible subsets S V u v Thus complexity deleting edge global skeleton O2n complexity constructing global skeleton On22n In decomposition algorithm suppose set V variables decomposed H nodes C1 CH dseparation tree T H cid2 n Let m denote number variables largest node m maxh Ch Ch denotes number variables Ch The complexity constructing local skeleton Om22m local skeletons OH m22m Since m usually n algorithm computationally complex IC algorithm In PC algorithm delete edge pair variables u v need check possible subsets variables adjacent u v The decomposition large graph small subgraphs improve PC algorithm The sets variables adjacent u v split small subsets decomposition Thus delete edge u v need check subset variables adjacent u v contained subgraph u v Let Adju denote set variables adjacent u global skeleton Adjhu set variables adjacent u hth local skeleton Since Adjhu Adju complexity checking subsets Adjhu checking subsets Adju Thus decomposition reduce complexity PC algorithm 7 Conclusions We proposed decomposition approach structural learning DAGs In approach problem learning large DAG split problems learning small subgraphs Domain prior knowledge conditional independen cies utilized facilitate decomposition structural learning We theoretically proved correctness proposed algorithms Both complexity algorithms power conditional independence tests improved decomposing large graph small subgraphs The theoretical results scheme 436 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 design multiple databases Although assumed paper latent variables results applied case latent variables For example suppose domain knowledge Then apply approach local structural learning discussed uncompleted paper Acknowledgements We like thank referees valuable comments suggestions improved presentation paper This research supported NSFC MSAR NBRP 2003CB715900 Appendix A A1 Proofs theorems First deﬁnition lemmas proofs theorems Deﬁnition A1 Let T dseparation tree DAG cid2GV node set C C1 CH For vertices u v cid2GV distance u v tree T deﬁned du v min Ci cid14uCj cid14v dCi Cj dCi Cj distance nodes Ci Cj T We Ci Cj minimizers u v minimize distance dCi Cj Lemma A1 Let l path u v W set vertices l W contain u v Suppose path l dseparated S If W contained S path l dseparated W set containing W Proof Since dseparation path l depends vertices u v contained dseparator W contains vertices l l dseparated S W W l dseparated S Since colliders l active conditionally W adding vertices conditional set new collider active l This implies l remains dseparated set containing W cid1 Lemma A2 Let T dseparation tree DAG cid2GV K separator T separates T subtrees T1 T2 variable sets V1 V2 respectively Suppose l path u v cid2GV u V1 K v V2 K Let W denote set vertices l W contain u v Then path l dseparated W K set containing W K Proof Since u V1 K v V2 K series s u y v l u s t x y v s V1 K y V2 K vertices t x contained K Let lcid11 subpath l s y W cid11 vertex set t x W cid11 K Since s V1 K y V2 K deﬁnition dseparation tree K dseparates s y cid2GV K separates lcid11 By Lemma 1 lcid11 dseparated W cid11 K set containing W cid11 Since W cid11 W K lcid11 dseparated W K set containing W K Thus l lcid11 dseparated cid1 Lemma A3 Let u v nonadjacent vertices cid2GV let l path u v If l contained Anu Anv l dseparated subset S anu anv Proof Since l cid5 Anu Anv series s u y v l u s t x y v s y contained Anu Anv vertices t x Anu Anv Then edges st xy oriented s t x y t x belongs anu anv Thus exists collider s y l The middle vertex w collider closest s s y contained anu anv descendant w anu anv directed cycle So l dseparated collider active conditionally vertex S S anu anv cid1 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 437 Lemma A4 Let T dseparation tree DAG cid2GV C node T Let u v vertices C nonadjacent cid2GV If u v contained separator connecting C exists subset C dseparates u v cid2GV Proof Deﬁne S anuanvC We u v dseparated S path u v cid2GV dseparated S If l contained Anu Anv obtain Lemma A3 l dseparated S When l contained Anu Anv let x y vertices closest u l l u x y v The path l following possible cases 1 u x x C 2 u x y x C 3 u x y x y C 4 u x y x C y C 5 x C For cases 1 2 x contained C uxy collider path l dseparated x contained S Thus l dseparated S For case 3 x Anu Anv y v exists directed cycle cid2GV Since y middle vertex collider l y C path l dseparated y contained S Thus l dseparated S For case 4 let Ccid11 cid5 C node T contains y Since y Ccid11 v C subpath lcid11 y v passes separator K connecting C Ccid11 dseparates C K Ccid11 K From K C lcid11 Anu Anv K lcid11 C anu anv S Since y C vertices x y adjacent cid2GV x K Notice u x y collider know u K deﬁnition dseparation tree By assumption u v separator connecting C v C K Since y Ccid11 C Ccid11 K v C K Lemma A2 obtain subpath lcid11 dseparated K lcid11 y v S Thus l dseparated S For case 5 let Ccid11 cid5 C node T contains x Similar case 4 result cid1 Lemma A5 Let T dseparation tree DAG cid2GV For vertex u exists node T contains u pau Proof If pau trivial Otherwise let C denote node T contains u parents u Since set separate u parent node T contains u parent If u parent obtained lemma If u parents suppose reduction absurdity u parents v w contained single node contained different nodes T u v C u w Ccid11 respectively vertices V appear T On path C Ccid11 T separators contain u separate C Ccid11 However separator containing u separate v w Thus got contradiction cid1 Lemma A6 Let T dseparation tree DAG cid2GV C node T Let u v vertices C nonadjacent cid2GV If u v contained separator S connecting C T exists node Ccid11 T containing u v set Scid11 Scid11 dseparates u v cid2GV Proof Without loss generality suppose v descendant u cid2GV By Lemma A5 node C1 T contains u pau Let C2 contains u v distance dC1 C2 minimum If C1 C2 node T Scid11 deﬁned parents u separates u v If C1 C2 different nodes dC1 C2 cid1 1 parent p u contained C2 Thus separator K connecting C2 C1 T K dseparates p vertices C2 K Since u p adjacent cid2GV distance dC1 C2 cid1 1 minimum u K v K 438 XC Xie et al Artiﬁcial Intelligence 170 2006 422439 v K dC1 C2 minimum For parent pcid11 u contained C1 C2 way p K separates pcid11 vertices C2 K Deﬁne Scid11 Anu Anv C2 Similar proof Lemma A4 path u v cid2GV dseparated Scid11 u v dseparated Scid11 If path l u v contained Anu Anv obtain Lemma A3 l dseparated Scid11 When l contained Anu Anv let x adjacent u l l u x v Because l contained Anu Anv v descendant u edge u x oriented u x x parent u If x contained C2 l dseparated x contained Scid11 If parent x u contained C2 shown x v dseparated K By Lemma A2 obtain subpath lcid11 x v dseparated W K W denotes set vertices x v containing x v lcid11 Since Scid11 W K Lemma A2 lcid11 dseparated Scid11 Hence path l dseparated Scid11 cid1 Lemma A7 Let T dseparation tree DAG cid2GV If vertices u v distance du v 0 T Ci Cj minimizers u v T separator path Ci Cj T dseparates u v cid2GV Proof For separator S path Ci Cj T deﬁnition du v u v contained S Ci Cj minimizers u v Since Ci Cj belong different subtrees obtained removing edge separator S attached know u v dseparated S cid1 Proof Theorem 1 Suppose u v nonadjacent vertices cid2GV If du v 0 T Lemma A7 u v nonadjacent cid2GV If du v 0 Lemmas A4 A6 exists node C T contains u v subset S S dseparates u v cid2GV cid1 Proof Theorem 2 From 5 p 53 separator S junction tree T separates V1 S V2 S triangulated graph Gt V Vi denotes variable set subtree Ti induced removing edge separator S attached 1 2 Since edge set Gt V contains GV V1 S V2 S separated GV Further GV undirected independence graph cid2GV obtain immediately T dseparation tree cid2GV cid1 Proof Theorem 3 Since moral graph DAG cid2GV undirected independence graph deﬁnition undirected independence graph need G deﬁned Theorem 3 contains edges cid2Gm V It obvious E contains edges obtained dropping directions directed edges cid2GV set dseparate vertices adjacent cid2GV Now E contains moral edge connects vertices u v having common child w u v E We know Lemma A5 node Ch dseparation tree T contains u v w The undirected subgraph Gh Ch contain edges uw vw Gh undirected path uwv By deﬁnition undirected independence graph w contained separator separate u v undirected subgraph u v dseparated conditional set containing w collider u w v Thus Gh contains moral edge u v cid1 A2 Proofs algorithms correctness Proof Algorithm 1s correctness By sufﬁciency Theorem 11 initializations steps 2 3 creating edges guarantee edge created variables node dseparation tree By sufﬁciency Theorem 12 deleting edges steps 2 3 guarantees edge dseparated variables deleted local skeleton Thus global skeleton obtained step 3 correct According necessity Theorem 1 moral edge u v undirected independence XC Xie et al Artiﬁcial Intelligence 170 2006 422439 439 graph deleted subgraph node dseparation tree separator dseparation tree separate u v vstructure u w v Thus determine vstructures step 4 completes proof cid1 Proof Algorithm 2s correctness Each complete undirected subgraph Gh describes saturated model Ch entire graph GV obtained step 3 represents dependencies prior knowledge The triangulation step 4 bring new conditional independence Thus junction tree dseparation tree cid1 Proof Main Algorithms correctness The correctness Algorithms 1 2 proved Thus need prove step 3 correct obtaining dseparation tree According Theorem 3 entire undirected independence graph constructed correctly step 3 Then dseparation tree obtained correctly algorithm constructing junction tree cid1 References 1 I Beinlich H Suermondt R Chavez G Cooper The ALARM monitoring A case study probabilistic inference techniques belief networks Proceedings 2nd European Conference Artiﬁcial Intelligence Medicine SpringerVerlag Berlin 1989 pp 247256 2 A Becker D Geiger A sufﬁciently fast algorithm ﬁnding close optimal clique trees Artiﬁcial Intelligence 125 2001 317 3 C Beeri R Fagin D Maier M Yannakakis On desirability acyclic database schemes J ACM 30 1983 479513 4 C Berge Graphs Hypergraphs second ed NorthHolland Amsterdam 1976 5 RG Cowell AP David SL Lauritzen DJ Spiegelhalter Probabilistic Networks Expert Systems Springer Publications New York 1999 6 DR Cox N Wermuth Multivariate Dependencies Models Analysis Interpretation Chapman Hall London 1993 7 D Danks Learning causal structures overlapping variable sets Proceedings 5th International Conference Discovery Science SpringerVerlag Berlin 2002 pp 178191 8 D Edwards Introduction Graphical Modelling SpringerVerlag New York 1995 9 R Fung S Crawford CONSTRUCTORa induction probabilistic models Proceedings Eighth National Confer ence AI American Association Artiﬁcial Intelligence Boston 1990 pp 762779 10 Z Geng Decomposability collapsibility loglinear models Appl Stat 38 1 1989 189197 11 Z Geng C Wang Q Zhao Decomposition search vstructures DAGs J Multivariate Anal 2005 submitted publication 12 D Heckerman A tutorial learning Bayesian networks MI Jordan Ed Learning Graphical Models Kluwer Academic Dordrecht 1998 pp 301354 13 FV Jensen F Jensen Optimal junction trees Proceedings 10th Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann San Fransisco CA 1994 pp 360366 14 D Koller M Sahami Toward optimal feature selection Proceedings 13th International Conference Machine Learning Bari Italy 1996 pp 284292 15 SL Lauritzen Graphical Models Clarendon Press Oxford 1996 16 RJA Little DB Rubin Statistical Analysis Missing Data second ed Wiley New York 2002 17 C Meek Causal inference causal explanation background knowledge Proceedings 11th Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann San Francisco CA 1995 pp 403410 18 J Pearl Probabilistic Reasoning Intelligent Systems Morgan Kaufmann San Mateo CA 1988 19 J Pearl Causality Cambridge University Press Cambridge 2000 20 S Rassler Statistical Matching Lecture Notes Statistics vol 168 Springer New York 2002 21 N Roberston PD Seymour Graph minors XIII The disjoint paths problems J Combin Theory B 53 1995 65100 22 D Rose R Tarjan G Lueker Algorithmic aspects vertex elimination graphs SIAM J Comput 5 1976 266283 23 P Spirtes C Glymour An algorithm fast recovery sparse causal graphs Social Sci Comput Rev 9 1991 6272 24 P Spirtes C Glymour R Scheines Causation Prediction Search second ed MIT Press Cambridge MA 2000 25 RE Tarjan M Yannakakis Simple lineartime algorithm test chordality graphs test acyclicity hypergraphs selective reduce acyclic hypergraphs SIAM J Comput 13 1984 566579 26 I Tsamardinos C Aliferis A Statnikov Time sample efﬁcient discovery Markov blankets direct causal relations Proceedings 9th ACM SIGKDD International Conference Knowledge Discovery Data Mining Washington DC 2003 pp 673678 27 T Verma J Pearl Equivalence synthesis causal models Proceedings 6th Conference Uncertainty Artiﬁcial Intelligence Elsevier Amsterdam 1990 pp 255268