Artiﬁcial Intelligence 244 2017 143165 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Semanticbased regularization learning inference Michelangelo Diligenti Marco Gori Claudio Saccà Department Information Engineering Mathematics University Siena Via Roma 56 Siena Italy r t c l e n f o b s t r c t Article history Received revised form 13 August 2015 Accepted 26 August 2015 Available online 1 September 2015 Keywords Learning constraints Kernel machines FOL This paper proposes uniﬁed approach learning constraints integrates ability classical machine learning techniques learn continuous featurebased representations ability reasoning higherlevel semantic knowledge typical Statistical Relational Learning Learning tasks modeled general framework multiobjective optimization set constraints satisﬁed addition traditional smoothness regularization term The constraints translate First Order Logic formulas express learningfromexample supervisions general prior knowledge environment fuzzy logic By enforcing constraints test set paper presents natural extension framework perform collective classiﬁcation Interestingly theory holds case data represented feature vectors case data simply expressed pattern identiﬁers extending classic kernel machines graph regularization respectively This paper proposes probabilistic interpretation proposed learning scheme highlights intriguing connections probabilistic approaches like Markov Logic Networks Experimental results classic benchmarks provide clear evidence remarkable improvements obtained respect related approaches 2015 Elsevier BV All rights reserved 1 Introduction This paper presents Semantic Based Regularization SBR uniﬁed framework inference learning centered notion constraint parsimony principle Semantic Based Regularization bridges ability machine learning techniques learn continuous featurebased representations ability modeling arbitrary pattern relationships typically Statistical Relational Learning SRL model learn highlevel semantic knowledge In order provide uniﬁed context manipulating perceptual data prior knowledge propose use unifying concept constraint suﬃciently general represent different kinds sensorial data relations express abstract knowledge tasks We unify continuous discrete computational mechanisms accommodate framework different stimuli In paper focus kernel machine mathematical algorithmic apparatus learn featurebased pattern representations constraints resulting fuzzy translation First Order Logic FOL formulas expressing prior knowledge learning task hand More speciﬁcally SBR builds multilayer architecture having kernel machines input layer The output kernel machines fed higher layers implementing fuzzy generalization FOL knowledge Thanks Corresponding author Email addresses diligmicdiismunisiit M Diligenti marcodiismunisiit M Gori saccunisiit C Saccà httpdxdoiorg101016jartint201508011 00043702 2015 Elsevier BV All rights reserved 144 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 basic properties fuzzy FOL kernel machines resulting model continuous respect feature values Therefore highlevel semantic inference provided logic backpropagated kernel machines gradientbased schema This process iterated training convergence This extremely powerful technique advantage available unsupervised data inference process performed data logic knowledge correct output kernel machines We substantially extend earlier studies Diligenti et al 10 showing SBR enables new fundamental tasks learning inference rely joint informative evidence coming realvalued features simple pattern identiﬁers corresponding relations In particular paper gives following new main results fundamental importance gain overall view theory especially enable large set applications statistical relational learning domains variable dimension domains null inputs We extend SBR framework 10 truly hybrid domains real valued feature pattern representations integrated pure symbolic entities pattern identiﬁers Indeed complex relational classiﬁcation tasks case entities naturally representable pat tern spaces different dimensions including remarkable case void patterns relational information available collective classiﬁcation In paper propose novel collective classiﬁcation method enforce constraints test set exploiting expressiveness FOL like statistical relational learning SRL ap proaches Once distinctive feature solution proposed paper arises considering collective computational scheme naturally exploits realvalued feature pattern representations probabilistic links We extend studies probabilistic interpretation regularization networks 38 case learning constraints From highlights connections Markov Logic Networks MLNs 40 interpretation clearly shows natural integration realvalued features object identiﬁers SBR Furthermore paper presents plain SVM Transductive Laplacian SVMs derived special cases proposed SBR framework The paper introduces new heuristics connected ones employed constraint satisfaction programming improve quality solutions Finally present experimental results effectiveness generality approach The paper organized follows section previous work ﬁeld reviewed Section 3 introduces First Order Logic fuzzy extensions Section 4 discusses learning constraints kernel machines Section 5 presents SBR generalizes models commonly relational transductive learning Details training performed SBR framework presented Section 6 In Section 7 collective classiﬁcation approach SBR presented Section 8 presents connections SBR probabilistic models like Markov Logic Networks The experimental evaluation SBR presented Section 9 ﬁnally Section 10 draws conclusions 2 Previous work Statistical Relational Learning SRL combines robust parameter estimation presence noise learning complex relational structures Probabilistic Relational Models PRMs 13 early SRL approach learns statistical model relational database PRMs build probability distribution attributes objects instance schema A Bayesian network node attribute built parameters estimated data Relational Dependency Networks 34 learn local conditional probability distribution node given Markov blanket conditional learner like logistic regression decision trees Markov Logic Networks MLNs 40 received lot attention SRL community extensively applied ﬁelds like bioinformatics 28 vision 46 Markov Logic Networks generalize combine ﬁrstorder logic probabilistic graphical models Thanks ﬂexibility MLNs tackle SRL main tasks collective classiﬁcation link prediction linkbased clustering social network modeling object identiﬁcation Many papers studied learn structure Markov Logic Networks data requiring expert express structure terms prior knowledge 2322 Hybrid Markov Logic Networks HMLNs 49 extend MLNs deal continuous variables Probabilistic Soft Logic PSL 5 SRL approach relaxes MLNs continuous fuzzy values 0 1 interval restricts considered FOL formulas ones conjunctive body single literal head PSL weight training solved convex optimization problem face small subset tasks potentially solved MLN One disadvantage MLNs PSL realworld applications deal entities associated complex featurebased representations Lets example common scenario multiclass classiﬁcation task patterns represented large vectors numeric features In order perform learning inference domain classical SRL techniques different approaches possible M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 145 value feature correlated output class speciﬁc rule For example let x generic pattern domain f binary feature possible express rule HasTrueValue f x BelongsTox c category c The training process estimate weight modeling strength correlation MLNs capture logistic regression model approach 1112 The advantage solution employs coherent framework dealing pattern representations higherlevel semantic knowledge However requires deal large number weights groundings Furthermore simple correlations features classes captured complex models large tractable Another approach pretrain arbitrary classiﬁer working lowlevel feature representations let SRL layer use classiﬁer prior assignments The SRL layer assign proper weight prior training predictions base classiﬁer reﬁned prediction step This approach uses standard machine learning techniques learn low level data Complex feature dependencies eﬃciently captured example changing employed kernel kernel machine base classiﬁer The main disadvantage approach layers sequentially trained greedy fashion This means high level semantic inference stacked pretrained frozen layer processes pattern repre sentations This allow use output inference process performed higher level improve predictions base classiﬁer This huge limitation large number unsupervised patterns available decision semantic layer additional supervisions lower level SBR encodes multilayer architecture having kernel machines input layer provides input higher level layers implementing fuzzy generalization FOL knowledge Since fuzzy FOL generalization knowledge continuous respect values coming input layer information ﬂow directions ar chitecture This allows backpropagate information highlevel inference performed logic knowledge kernel machines simple gradientbased schema This allows SBR preserve compactness eﬃ ciency kernel machines deal feature space representations input level exploiting power FOL express higherlevel semantics As discussed later paper keeping separate communicating layers dedicated processing feature space performing higher conceptual reasoning allows devise eﬃcient training techniques breaking complexity learning SBR optimization tasks similar nested optimization problems studied bilevel programming 2 Unfortunately results obtained context bilevel programming easy reuse SBR training outer SBR optimization problem generally quadratic linear Other SRL approaches focused high level cognitive decision processes native integration learning presence continuous high dimensional data representations Integration logic learning traditionally hard achieve barriers erected different mathematical models classically handle logical reasoning learning examples real numbers Connections logic kernel machines studied context ﬁnding relationships symbolic subsymbolic models AI 21 particular emphasis hybrid models When restricting kernel machines rich analysis literature survey 26 broader coverage ﬁeld emphasis connections inductive logic programming DeRaedt al 39 Convolution kernels discrete structures 20 main sources inspiration exploring connections kernel machines logic Feature Description Logic FDL introduced support learning relational domains 89 The paradigm provides natural solution problem learning representing relational data extends uniﬁes lines works machine learning Muggleton et al 33 presents support vector machines based kernel inner product feature space spanned given set ﬁrstorder hypothesized formulas The inductive logic programming FOIL combined kernel methods Landwehr et al 24 The resulting model kFOIL implements dynamic propositionalization approach allowing perform classiﬁcation regression tasks Landwehr et al 25 presents general theoretical framework statistical logical learning based dynamic propositionalization structure learning corresponds inferring suitable kernel logical objects parameter learning corresponds function learning resulting reproducing kernel Hilbert space In context multitask learning functions coupled dependencies induced structure special multitask kernels 6 Quite different approach based imposing constraints perceptual space 141527 prior knowl edge incorporated form constraints support vector machine classiﬁer Most reviewed papers proposed different frameworks incorporating prior knowledge expressed logic formalisms kernel machines However integration shallow based asking kernel play additional role incorporating logic structures instead primarily measuring smoothness solution according Occams razor While interesting idea remarkable residual degree freedom way logic structures incorporated suggests partially addressing inherent limitation kernel methods In presented framework adoption Tnorms allows simply express classic logic formalisms straints realvalued functions Therefore general uniﬁed notion constraint employed paper natural straightforward extension classic statistical framework learning examples special instance constraint 146 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 3 First order fuzzy logic The term fuzzy logic ﬁrstly LA Zadeh 1965 51 Classical logic works variables assuming binary truth value Fuzzy Logic instead form manyvalues logic probabilistic logic It deals reasoning ap proximate exact 18 variables truth degree ranges 0 1 zero meaning variable false true certainty respectively A fuzzy generalization FOL ﬁrst proposed Novak 36 Tnorm residuum A tnorm function t 0 1 0 1 0 1 continuous commutative associative mono tone featuring neutral element 1 ta 1 A strict tnorm strictly monotone A tnorm fuzzy logic deﬁned tnorm ta1 a2 models logical AND Given variable continuous generalization 0 1 negation corresponds 1 Once tnorm functions corresponding logical AND NOT deﬁned composed convert arbitrary logic proposition continuous function A tnorm expression behaves classical logic variables assume crisp values 0 false 1 true Many different tnorm fuzzy logics proposed literature For example given Boolean values a1 a2 continuous generalizations a1 a2 0 1 range product tnorm deﬁned a1 a2 ta1 a2 a1 a2 The operator consequently deﬁned De Morgan rule a1 a2 a1 a2 ta1 a2 1 1 a1 1 a2 Another commonly tnorm minimum tnorm deﬁned a1 a2 ta1 a2 mina1 a2 follows a1 a2 ta1 a2 maxa1 a2 The equivalence a1 a2 a1 a2 classic logic represent implications modus ponens However equivalence appropriate perform deductions fuzzy variable values Any tnorm corresponding binary operator called residuum fuzzy logic generalize implications dealing continuous variables A tnorm residuum provides natural way express human fuzzy reasoning equivalent modus ponens fuzzy variable values approach extremes 0 1 range In particular residuum converting implication minimum tnorm deﬁned cid2 a1 a2 ta1 a2 a1 a2 1 a2 a1 a2 product tnorm residuum deﬁned cid2 a1 a2 ta1 a2 1 a2 a1 a1 a2 a1 a2 The residuum allows relax condition satisfaction implication satisﬁed soon tnorm expression head higher truth degree tnorm expression formula body Quantiﬁers With loss generality restrict attention FOL formulas Prenex Normal Form PNF form quantiﬁers associated quantiﬁed variables placed beginning expression For example Quantiﬁers cid3 cid4cid5 cid6 x1x2 Quantiﬁerfree expression cid6 cid4cid5 cid3 Ax1 Bx2 Cx1 1 Please note dealing variables ranging 0 1 quantiﬁerfree expression equiv alent assertion fuzzy propositional logic quantiﬁed variables grounded Hence tnorm fuzzy logic convert continuous function Lets consider FOL formula variables x1 x2 assuming values ﬁnite sets X1 X2 P p1 p2 X j1 X j2 In case unary predicate j indicate set possible groundings jth predicate PX indicate possible grounded vector predicates jth nary predicate grounded X j X j predicates PX p1X X j1 Let p jX Assuming atoms PX generalized assume real values 0 1 degree truth formula taining expression E universally quantiﬁed variable xi deﬁned minimum t E obtained tnorm generalization E grounding xi Xi 1 p2X 2 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 147 cid7 cid8 PX xi E cid2 cid7 cid8 PX cid7 cid8 PX t E min xi Xi For existential quantiﬁer truth degree instead deﬁned maximum tnorm expression domain quantiﬁed variable cid7 cid8 PX xi E cid2 cid7 cid8 PX cid7 cid8 PX t E max xi Xi The generalizations correspond conjunction disjunction minimum tnorm expressions propositional formula computed groundings respectively When multiple universally existentially quantiﬁed variables present conversion performed recursively outer inner variables Please note fuzzy FOL formula returns value 0 1 range The fuzzy formula expression continuous differentiable set points null measure respect fuzzy value predicate A small modiﬁcation classic Fuzzy FOL paper particular universal quantiﬁer generalized cid7 cid8 PX xi E cid2P cid7 cid8 X 1 Xi cid9 xi Xi cid7 cid8 PX t E min operator tnorm values replaced average set This deﬁnition allows faster convergence training model classical Fuzzy FOL formulation directly depends item set groundings argmin element average depends elements allows parallel optimization training Obviously Fuzzy FOL expression new formulation perfectly veriﬁed assumes value 1 iff atom assignments old formulation veriﬁed formulations consistent Example 31 The formula x1x2 Ax1 Bx2 product tnorm quantiﬁerfree conversion corresponds following continuous generalization cid2PX 1 X1 cid9 x1X1 max x2X2 Ax1 Bx2 Ax1 Bx2 fuzzy truth values predicates A B grounded x1 x2 As commonly description logic 1 different operator called nexistential quantiﬁer indicated n deﬁned express property hold true n objects The fuzzy generalization quantiﬁer deﬁned nxi E cid7 cid8 PX cid2n P cid7 cid8 X 1 n cid9 cid7 cid8 PX t E xi Mnt E PXi X Mnt E P Xi X set groundings xi Xi corresponding n maximum values t E The conversion quantiﬁer collapses universal quantiﬁer conversion n Xi existential quantiﬁer n 1 It interesting note translation computationally approachable SRL approaches MLNs example replace existential quantiﬁer disjunction groundings The n quantiﬁer expressed n nested existential quantiﬁers MLN number groundings formula scale Gn G number groundings considered variable In SBR n corresponds selecting n groundings best candidates satisfy formula This candidate selection complexity O G log n heap data structure 4 Semantic based regularization cid8 cid7 PX Consider multitask learning problem set T functions estimated query unknown functions functions evidence given functions known priori Let f f 1 f T f T 1 f T T cid14 indicate vector cid14 T functions We assume set H functional constraints form 1 cid2h f 0 0 cid2h f 1 h 1 H provided order query functions behave Functionals express property single function correlate multiple functions learning helped exploiting correlations Let Hk functional space kth function lives Following classical penalty approach constrained optimization constraint satisfaction enforced adding term penalizes violation The resulting cost function minimized 148 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 C f Tcid9 k1 fk2 Hk Hcid9 h1 λh 1 cid2h f ﬁrst term penalizes nonsmooth solutions λh weight hth constraint A higher value λh makes costly respecting constraint constraint hard λh The learning problem relaxed assuming constraints enforced ﬁnite sample input patterns Typically pattern represented vector realvalued features cases patterns simply represented unique identiﬁer In particular jth function associated set X j pattern representations x j It possible multiple functions share sample patterns X cid16 j Since functions express relations multiple patterns pattern representations associated function generally expressed combination patterns set ﬁnite domains X X j1 X j2 k indicate vector values obtained applying function fk set patterns X k f X 2 collects groundings functions Enforcing constraints samples data Let fkX X j j 1 f 2X f 1X yields following cost function Ce f X Tcid9 k1 fk2 Hk Hcid9 h1 λh 1 cid2h f X 2 Please note sake simplicity abuse notation symbol cid2 refer exact empirical approximation functionals The solution optimization task deﬁned objective function Equation 2 kernel expansion form k x βk f X cid9 k i1 wcid5 ki Kkxki x 3 βk bias Kk reproducing kernel associated space Hk The proof 10 straightforward extension Representer Theorem plain kernel machines 42 The bias βk added specify default value function The weights kth function indicated w k wk1 wkX Plugging Equation 3 2 expression minimized optimizing wk gradient descent k Tcid9 Ce f X w T k Gk wk Hcid9 cid10 1 cid2h cid7 λh cid8cid11 f X 4 k1 f X G 1 w 1 G T w T f T 1X T T cid14 G k Gram matrix kth function j element equal Kkxki xkj For sake simplicity explicitly added vector bias values input cid2h f X T 1 f T T cid14 X h1 41 Logic constraints Let assume knowledge base KB consisting set FOL formulas set groundings variables provided express domain knowledge Some predicates unknown estimated The jth unknown predicate approximated function σ f j f j function learned σ sigmoidal function mapping functions outputs 0 1 range tnorms deﬁned In remainder paper drop σ overload notation The variables KB input f j replaced featurebased representation object grounded variables We indicate xi representation object grounded xi ρxi xi function mapping pattern representation object identiﬁer The groundings Xi ith variable replaced set X indicating set featurebased representations groundings One constraint 1 cid2i 0 formula F knowledge base built taking fuzzy FOL generalization formula cid2i unknown predicates replaced learned functions variables input learned functions replaced duals iterating featurebased representations groundings Example 41 Lets suppose text categorization task documents assigned categories A B C D Let d generic document classiﬁed Ad Bd Cd Dd unknown indicator functions classes We indicate d feature representation d typically bagofword TF TFIDF representation The functions f Ad f B d f C d f D d estimated approximate respectively unknown Ad Bd Cd Dd We assume variable d grounded documents d1 d2 having bagofword representations D d1 d2 respectively We use simple learning task running example rest paper M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 149 The fuzzy FOL generalization knowledge base provides continuous differentiable surrogate initial logic formulas 0 cid2i 1 Therefore resulting constraints 1 cid2i 0 continuous differentiable allow train query functions gradientbased learning schema The form constraints depends tnorm generalize FOL rules Whereas true tnorms consistent classical logic variables assume crisp values 0 1 behavior tnorms differs intermediate values ultimately leading different constraints Similarly selection right kernel kernel machines choice tnorm matter context depends problem modeled 17 For example minimum tnorm better choice product tnorm performing inference presence long conjunctive chains value conjunction vanish exponentially number terms product tnorm The product tnorm correct case behaves consistently probabilisticindependence assumption training slow The tnorm selection strong impact applications provide signiﬁcant change experiments presented paper Example 42 Continuing text categorization example assume external knowledge categories d Ad Bd expressing document belong class A B The constraint resulting fuzzy FOL generalization formula substituting query predicates unknown functions document representations 1 cid2 f AD f B D 1 1 2 1 1 2 0 cid9 cid3 max t f A d f B d cid4cid5 cid7 cid6 cid8 f Ad f B d dD cid7 max cid7 cid8 f Ad1 f B d1 cid7 max cid8cid8 f Ad2 f B d2 t minimum tnorm representation propositional formula This constraint directly plugged Equation 4 optimization 42 Nodes feature vector representation It common query predicates input arguments groundings associated feature representation Most Statistical Relational Learning approaches assuming case In case inputs simply listed sequence unique identiﬁers Equation 4 generic expression cost function setting Gram matrix identity matrix predicates vectorial input G k I In case vector weights vector values function inputs fkX k Gk wk I wk wk These unknown values directly estimated generalizing input space null Obviously interesting case functions inputs represented vectors features dont The experimental section present experiments mixed setting 43 SBR multilayer architecture Given arbitrary KB procedure described previous section generally encoded multilayer network following structure input layer computation atoms performed computing query evidence function values possible groundings propositional layer value tnorm expression propositional formula computed compatible combination atoms quantiﬁer layers tnorm values computed previous layer aggregated average max operator universal existential quantiﬁers respectively Please note number quantiﬁer layers ﬁxed aggregation outputs recursively nested according number quantiﬁers FOL formula The output layer score 0 1 expressing strongly FOL formula respected output layer accumulation summation contributions coming single formulas Learning inference intractable large domains exponential growth number groundings respect number nested quantiﬁers However applications large portion ground clauses trivially satisﬁed satisﬁed known evidence atoms regardless unknown assignments These groundings discarded introducing approximation training inference process In context MLNs heuristics algorithms like FROG preprocessing 43 Tuffy 35 LazySAT 44 proposed detect discard noninformative groundings Since grounding process MLNs SBR essentially algorithms 150 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 directly reused SBR In particular FROG preprocessing presented experiments prune encoded network input propositional layers grounding evidence predicates makes formula holding true false independently values assumed query predicates In case contribution cost function grounding constant effect learning process Therefore useless instantiate grounding encoded network A common case pruning process applied dealing FOL formula following form Rx Ax Rx evidence predicate When holds Rp false given grounding x p rule veriﬁed regardless value query predicate Ap Therefore grounded predicates Rp Ap safely discarded encoding network training appearing FOL formula To summarize network encoded SBR given KB deﬁned following Deﬁnition 43 SBR network Assume FOL KB composed rules predicates grounded set constants Let x represent feature vector associated grounding x We indicate f function implemented Kernel Machine approximating ith unknown predicate pi f takes input feature vector representations constants grounding predicate SBR builds multilayer network N computing fuzzy FOL approximation KB value f ix replaces grounded unknown predicate pix Some examples KB relative network encodings shown remainder paper 5 Some special cases SBR This section SBR reproduce standard Transductive SVMs manifold graph regularization At time learning schemas mixed extended arbitrarily expressiveness FOL 51 Case 1 SVMs Let X k sets positive negative examples kth unknown predicate pk X k X k X k The following logic formula expresses fact pk constrained values assumed supervised data k X cid7 cid8 P kx pkx cid7 cid8 P kx pkx x x Xk predicate P kx evidence function holding true iff x positive example query predicate pk x X k Using minimum tnorm replacing pk approximation fk corresponds following constraint 1 cid2 cid7 P kX k fkX k cid8 1 1 X k cid9 cid10 max min cid7 cid8 P kρx fkx min cid7 1 P kρx 1 fkx cid8cid11 xX k cid10 cid9 xX k 1 1 X k cid10 cid9 1 X k max xX k cid7 cid8 1 fkx min cid9 cid7 1 1 fkx cid8cid11 min cid7 cid8 0 1 fkx xX k cid9 xX k cid7 cid8cid11 max 0 fkx 0 max0 1 fkx hinge loss regular SVM The hinge loss appear negative supervisions 1 value negative supervisions instead 0 Therefore plugging previous constraint Equation 4 shows classical SVMs special case framework expressing ﬁtting supervised data logic knowledge Please note constraints resulting supervised data ﬁtting special case constraint convex relatively easy optimize We discuss point details following section When unsupervised data available possible express supervisions evidence predicates In k The learned function constrained terms assumed values supervised data specifying following k available positive negative unsupervised examples task k X k X X k X k k X u k particular let X X u logic formulas F p Fn F p x P kx pkx Fn x Nkx pkx P kx Nkx evidence functions holding true iff x positive negative example query predicate pk respectively Also formulation corresponds learning task solved standard SVM training patterns M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 151 Fig 1 Network encoded knowledge base containing F formula ﬁgure implements SVM supervised training grounded constants patterns C d1 d2 positive negative supervised contribute cost function base step solving complicated learning tasks unsupervised data plays role remainder paper Example 51 In text categorization example let assume d1 belongs class A d2 This expressed stating d1 positive supervised example P Ad1 true P Ad2 false We express rule d P Ad Ad P Ad Ad incorporate labeled data learning task The following constraint results fuzzy FOL generalization formulas minimum tnorm substituting document identiﬁers feature representations query predicates corresponding functions estimate 1 cid2 cid7 cid8 f AD 1 2 cid7 cid8 max0 1 f Ad1 max0 f Ad2 0 Fig 1 shows network encoded performing simple learning task 52 Case 2 manifold regularization Manifold Regularization 3 assumes learned function regular input manifold repre sented graph edges connect input patterns The graph directly built input data distribution built external knowledge like html hyperlinks web page classiﬁcation problem Laplacian SVM LSVM 31 effective semisupervised approach train SVMs manifold regularization assumption Manifold Regularization special case SBR rules forcing ﬁtting supervised examples previously described additional rule expressing manifold assumption In particular let Rx y given evidence relation expressing x y connected manifold The manifold assumption logic setting predicate p expressed following FOL formula asserting connected points true false x y Rx y px p y px p y Example 52 Let assume previously considered text categorization task d1 links d2 hyperlink Rd1 d2 1 links available documents Manifold regularization domain ex pressed category A formula F R fuzzy FOL generalization cid2F R obtained substituting query predicate A unknown function f A minimum tnorm This yields following constraint 1 cid2F R cid7 cid8 f AD cid10 1 1 4 3 max cid7 cid8 min f Ad1 f Ad2 min1 f Ad1 1 f Ad2 0 152 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 Fig 2 Multilayer network encoded SBR KB F F R F R implements manifold regularization F supervised learning positive supervisions The KB grounded constants patterns C d1 d2 ﬁrst contribution comes 3 groundings R predicate false rule trivially veriﬁed This contribution affect training process dropped This constraint plugged Equation 4 cost function optimize Fig 2 shows encoded network FROGpreprocessing learning task added positive supervision expressed formula indicated F This rule form account unsupervised data previously described FROGpreprocessing keeps network groundings P A R true rules F F R veriﬁed based evidence predicates Similar rules added express manifold regularization classes B C D 53 Case 3 hierarchical classiﬁcation Complex classiﬁcation tasks involve large number classes organized hierarchy Typically hierarchy represented Directed Ordered Acyclic Graph DOAG node corresponds class A single root node provided starting point classiﬁcation process nodes reached The classiﬁcation process explores set paths graph path ends leaf node A levelhierarchical classiﬁcation n classes ﬁrst level expressed rules x p1x pnx x pix pc i1x pc ini x 1 n pi 1 n father classes ﬁrst level hierarchy ni number child classes class pi j j 1 ni child classes pi Class priors category expressed rules pc mi x pix mi j x pc jx 1 n 1 n j 1 ni mi mi j estimated supervised data This schema recursively generalized taxonomies arbitrary depth Example 53 For text categorization example assume C child class A taxonomy Therefore formula F T d Ad Cd expresses taxonomical information document belonging class A belongs class C The resulting constraint obtained fuzzy FOL generalization cid2F T formula set D available documents M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 153 1 cid2F T cid7 f AD f C D cid8 1 1 2 cid9 dD cid3 cid2 tT f A d f C d cid4cid5 cid6 f C d f Ad f C d f Ad 1 f C d 0 tT fuzzy generalization formula F T residuum minimum tnorm 54 Case 4 transductive SVMs Transductive SVMs TSVMs 48 extend SVMs ﬁnding hyperplane maximum separation margin labeled data labeling unsupervised data induced hyperplane Therefore Transductive SVMs tend place separating hyperplane low density regions input space TSVMs expressed SBR adding FOL formula forcing pattern classiﬁcations true false x pkx pkx pkx kth query predicate returns true iff pattern x belongs given class While formula trivially veriﬁed standard FOL apply fuzzy generalizations FOL classiﬁcation scores 0 1 range As Transductive SVMs trivial solutions avoided forcing balancing number unlabeled patterns positively negatively classiﬁed prior determined supervised data nk x pkx mk x pkx given sample Xk variable x nk mk nk mk Xk expected numbers patterns sample unknown predicate hold true false respectively 55 Case 5 graph regularization Graph regularization 52 transductive learning task generalization happens training All data patterns arranged nodes graph edges associated weights expressing degree similarity connected patterns Some graph nodes supervised associated target value The learning task consists assigning value nodes graph smooth similar connected nodes No feature representation available node generalization happens purely topological level In SBR supervisions Graph Regularization task expressed logic formulas described Section 51 Like manifold regularization let Rx y known relation expressing patterns nected following rule express smoothness graph connections x y Rx y pkx pk y pkx pk y kth query predicate pk The encoded network structure example presented Fig 2 void input representations ﬁrst layer functions implemented special kernel discussed Section 42 6 Training Training SBR means determine weights kernel machines input layer directly outputs functions null featurebased inputs The weights optimized gradient descent backpropagation schema output layer computes derivative respect constraint Ce In quantiﬁer layers derivative cid2k constraint respect predicate grounding computed cid2k At propositional level derivatives tcid2k respect single functions computed tcid2k f computed f K ix j w j At input level derivatives respect single parameters The overall derivative cost function respect jth weight ith function w j Ce w j cid9 k Ce cid2k cid2k w j cid9 k Ce cid2k cid9 tcid2k cid2k tcid2k tcid2k f f w j 5 When input feature representation provided weight function value w j f j propagation function weights needed Resilient gradient descent custom learning rate parameter empirically converge quickly presented experiments 154 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 61 Logic formulas complexity optimization Not FOL formulas translated constraint suited optimization Indeed section general intractability FOL inference directly translated SBR cost function plagued local minima Lets consider universally quantiﬁed FOL formulas DNF form x1 xn minterm 1 cid4cid5 cid3 cid7 n11 P 1x1 n1n P nxn cid6 minterm k cid6 cid4cid5 cid3 nk1 P 1x1 nkn P nxn cid8 ni j determines jth variable ith minterm negated Applying double negation DeMorgan rule yields following expression grounding cid10 cid8 cid7 n11 P 1x1 n1n P nxn cid7 nk1 P 1x1 nkn P nxn cid8cid11 Now converting propositional expression product tnorm assuming generalize atoms 0 1 range unknown function approximations constraint 1 cid2 cid7 cid8 f X 1cid16 n i1 X cid9 cid9 kcid17 cid10 x1 xn1 r1 1 cid17 A p r cid17 cid11 1 f jx j 0 f ixi j An r p r An p r cid16 cid16 A j An r f ixi A 1 set nonnegated negated atoms rth minterm rest paper omitted squashing function σ f keeps values 0 1 range An assignment 1 f jx j 1 Therefore null contribution result verifying rth minterm result assignment verifying minterm summing groundings Since minterms construction different polynomial equation continuous assuming values greater equal zero guaranteed tnorm resulting expression local minima number true conﬁgurations truth table grounded propositional formula number construction equal number minterms initial DNF Therefore perfect duality number possible assignments atoms verifying FOL formula given grounding variables number local minima introduced constraint resulting generalizing formula continuous domain Not surprisingly shows optimization continuous domain hard ﬁnding correct assignments original FOL formulation It clear formulas single minterm correspond convex constraint single minimum examples special case following paragraph While product tnorm paragraph simple study solutions resulting polynomial result extended strict tnorm strict tnorms isomorphic product tnorm 45 62 Stagebased learning As shown previous sections cost function plagued local minima dealing nontrivial knowledge base This section discuss heuristic experimentally proved allow successfully train models large scale datasets In constraint satisfaction programming 41 picking variable smallest number admissible values remaining domain commonly heuristics select variable assign search 16 As explained Haralick Elliot 19 heuristic minimizes depth search tree Following standard notation Rossi et al 41 refer heuristic dom In context SBR dom heuristic corresponds force earlier formulas lower number possible valid verifying formula assignments atoms learned evidence predicates grounded As explained previous section formulas introduce lower number local minima cost function For example lets consider following KB 1 x P 1x p1x P 1x p1x 2 x P 1x p1x p2x 3 x p1x p2x p3x predicates p1 p2 p3 learned The ﬁrst rule form presented Section 51 express ﬁtting supervised examples This rule minimum possible degree freedom possible assignment atom p1x evidence grounding P 1x Therefore rule ﬁrst training according dom heuristic Not surprisingly previous section showed rules single degree freedom convex translated continuous constraint tnorms Rule 2 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 155 possible assignments evidence grounded predicate P 1x holds true false respectively Rule 3 seven possible different assignments query predicates Therefore rule 2 added training process degrees freedom Since assignments verifying rule depend evidence predicate assignments usually use average possible evidence groundings rule example average 35 rule 2 We leave studies improve simple schema For example possible compute expected number possible assignments query predicates distribution observed evidence groundings training data The dom heuristic applied SBR connections research deep learning shifted attention teaching plans systematic way 4 generic level studies ﬁeld developmental psychology wellknown animals experiment stagebased learning 37 The experimental results beneﬁcial split optimization problem multiple stages constraints introduced order complexity dictated dom 7 Collective classiﬁcation Collective Classiﬁcation task performing inference set instances connected set relationships Collective classiﬁcation easier task independent pattern classiﬁcation relationships exploited enforce classiﬁcation consistency Collective classiﬁcation SBR assumes available FOL knowledge converted set straints previously described methodology Given test set composed set groundings collective classiﬁcation process force test set classiﬁcation assignments respect constraints In particular let fkX cid14 k indicate vector values obtained evaluating kernel machine function fk data points test set X cid14 T If kernel machine trained fk examples feature representations available training fkX cid14 k assumed ﬁlled default values equal 05 k The set vectors compactly referred f X cid14 f 1X cid14 1 f T X cid14 Collective classiﬁcation searches values f X cid14 f 1X cid14 1 fT X cid14 T respecting FOL formulas test data close prior values established kernel machines test data Ccc f X cid14 f X cid14 1 2 Tcid9 k1 fkX cid14 k fkX cid14 k2 cid9 cid10 cid7 cid8cid11 f X cid14 1 cid2h h Ccc f X cid14 f X cid14 fkX cid14 Optimization performed gradient descend computing derivative respect function values cid7 cid8 f X cid14 fkX cid14 As shown Equation 6 SBR collective classiﬁcation reuses schema compute gradients shown Equation 5 However gradient computed respect functions weights straints cid2h fk training phase ﬁrst terms chain rule taken account collective classiﬁcation fkX cid14 fkX cid14 cid2h cid9 6 cid18 cid19 h cid2h fk cid9 tcid2h cid2k tcid2h tcid2h fk Indeed input weights ﬁxed backpropagation derivative error weights needed This provides elegant solution collective classiﬁcation employs backpropagation routines training additional complexity implementation 8 SBR probabilistic model In section highlight probabilistic interpretations solutions SBR Given constraints cid2 cid21 cid2H computed data X probability distribution P V f X cid2 possible assignments functions f assumed follow exponential model P V f X cid2 1 Z 1 Z φpr f φ cid18 cid9 exp cid7 cid8 f X cid2 f k 2 cid9 cid7 cid8 f X λhcid2h cid19 cid7 k f X cid2 cid8 h Z normalization factor φ measures f respects constraints φpr f penalizes irregular solutions The prior expression classical Tikhonov regularization terms prior probabilities 50 assuming Gaussian prior P pr f φpr f exp 2 Therefore cid20 k f k 156 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 log P V f X cid2 log Z cid9 k fk2 cid9 cid7 cid8 f X λhcid2h h 7 This result provides probabilistic interpretation SBR formulation given Equation 2 change sign fact probability maximized Equation 2 cost minimized In special case constraint representing formula universal quantiﬁers holds P V f X cid2 1 Z exp cid9 k fk2 cid9 cid9 cid14 λ h cid7 cid8 f xih tcid2h h ih 8 ih iterates Gh possible groundings h formula Fh having tcid2h tnorm conversion xih set pattern representations ith grounding Fh f xih indicates set values returned functions computed xih λcid14 λh h Gh 81 SBR Markov logic networks A Markov network model joint distribution set variables V composed undirected graph expressing variable dependencies set potential functions The graph node variable model potential function clique graph A potential function nonnegative function state corresponding clique The joint distribution represented Markov network given P V v 1 Z cid17 k cid2vk vk state variables appear clique Z partition function Markov networks represented loglinear models clique potential replaced exponentiated weighted sum features state P V v 1 Z cid9 exp w j f jv j A Markov Logic Network MLN 40 set pairs Fh λh Fh formula ﬁrstorder logic λh real number Given ﬁnite set constants Ch deﬁning groundings variables appearing Fh Markov network MLN deﬁned 1 MLN contains node possible grounding predicate value node 1 iff atom true 2 MLN contains feature possible grounding formula Fh The value feature equal 1 iff ground formula true The weight feature λh The probability distribution possible assignments v speciﬁed ground Markov network MLN given P V v 1 Z exp cid19 λhnhv cid18 cid9 h nhv number true groundings Fh v Deﬁnition 81 A Markov Fuzzy Logic Network MFLN set pairs Fh λh Fh formula ﬁrstorder logic λh real number Together ﬁnite set constants Ch deﬁning groundings variables appearing Fh deﬁnes Markov network MFLN follows 1 MFLN contains node possible grounding predicate value node degree truth atom 2 MFLN contains feature possible grounding formula Fh The value feature equal degree truth formula computed fuzzy FOL generalization Fh The weight feature λh The resulting network contain clique grounded formula An MFLN extends MLN allowing nonbinary features continuous degree truth predicates While MLN counts number true groundings formula world MFLN computes sum degrees satisfaction formula computed tnorm Lets assume unknown predicates approximated output kernel machines f applied groundings Let xih set pattern representations associated ith grounding Fh f xih set M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 157 values returned functions computed xih The value feature associated grounding tnorm value tcid2h Therefore cid8 f xih cid7 P cid2V f 1 Z exp cid9 cid9 λh h ih cid7 cid8 f xih tcid2h 9 By comparing Equations 8 9 emerges SBR seen MLN FOL formulas node values replaced fuzzy generalization node values computed kernel machines 82 Discussion A fundamental difference MLNs SBR MLNs standard hybrid version 49 incorporate featurebased pattern representations associating rule weight feature example text categorization MLNs performed Domingos Summer 12 On hand SBR implements multilayer architecture pattern representations dealt kernel machines ﬁrst layer This advantages terms training ﬂexibility Indeed MLNs weights deﬁning input higher level inference level SBR employ appropriate eﬃcient learning schema input level perform different training phases like simple strategy employed described Section 62 Unlike MLNs SBR deal continuous highdimensional highly correlated feature vectors Furthermore SBR deﬁned cases input representation vectorial unknown kernel values inputs needed order perform training inference The connections enlighten paragraph MLNs SBR suggest possible reuse MLN training mechanism learn rule weights λh output layer SBR Equation 9 Like MLNs require compute partition function leads ineﬃciency approximations However experimental results learning input layer weights SBRs effective solving MLN learning task contexts 9 Experimental results The proposed framework subject large experimental analysis carried SBR software package1 91 Transductive learning text categorization CORA dataset The CORA research paper dataset relational dataset containing information papers associated authors 30 The CORA dataset collects papers classiﬁed topic hierarchy 80 classes 10 classes ﬁrst level2 In addition authors classiﬁed set 10 classes depending major topic research A random sample 2000 papers belonging 10 level classes extracted 3928 authors having publication selected sample papers Each paper associated vectorial representation containing title represented bagofwords TFIDF weights There proﬁle authors symbolic entities feature representation The learning task consists predicting category papers author research area This multilabel dataset authors associated multiple categories Papers split sets published year 1995 papers published year 1995 later papers The resulting sets contain 949 papers 2272 authors 316 papers 605 authors 735 papers 1051 authors respectively The sets form pools training validation test datasets sampled respectively This simulates real world scenario training process performed certain time testing expected involve new incoming papers In particular ﬁve folds generated randomly selecting n papers n authors n 10 20 30 40 50 supervisions kept ﬁrst second sets training validation data Experiments carried transductive context test data available unsupervised data training averaging 5 folds Knowledge base The available prior knowledge modeled terms FOL predicates Let B false true let denote paper author domains P A respectively Notice P Rd A simply set author identiﬁers Let deﬁne following predicates according relational representation Fig 3 1 The SBR package downloaded https sites google com site semanticbasedregularization home software 2 The data base downloaded httppeoplecsumassedumccallumdatahtml 158 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 Fig 3 Relational representation CORA Research Paper Dataset Ci P B Cix true x category 1 10 ACi A B ACix true x category 1 10 CoAuthor A2 B CoAuthorx y true x y coauthors AuthorOf A P AuthorOf x y true x author y Cite P 2 B2 Citex y true x cites y When considering categories 1 10 knowledge base KB use represent learning task composed 52 rules 0 x P ix Ckx x Nix Ckx x y Citex y Cix Ci y Cix Ci y ii x y CoAuthorx y ACix ACi y ACix ACi y iii x y AuthorOf x y ACix Ci y ACix Ci y iv x 10cid21 cid7 i1 cid8 icid161C1x icid162C2x icid1610C10x v x AC1x AC2x AC10x The 0 formulas express supervised data The 10 formulas state papers tend cite papers category ii ones state coauthors belong research area The iii formulas enforce coherence categories given papers authors iv formula Disjunctive Normal Form stating exclusive classiﬁcation assumption icid16 j adds negation predicate excluding jth paper assigned 10 classes Finally v states closeworld assumption forcing author assigned 10 classes Results discussion In ﬁrst experiment validated dom heuristic proposed Section 62 In particular SBR tested following conﬁgurations 0 ii iii iv v introducing constraints beginning training 0 ii iii iv v learning supervised data ﬁrst training data learned introducing constraints In particular second group constraints added gradient module small empirically observed happen 30 iterations gradient decent experiment 0 ii iii iv v splitting training multiple stages constraints sequentially introduced according dom heuristic looking degree freedom assignments formula A new set constraints added gradient module small The formulas sets ii iii degree freedom possible different assignments query predicates verifying formula introduced Table 1 reports classiﬁcation scores 3 different learning schemas obtained test set patterns paper research area domains Since pattern paper domain belongs single class singlelabel classiﬁcation task standard classiﬁcation accuracy metric task The F1 metric research area domain multilabel multiclass classiﬁcation task For conﬁgurations SBR minimum tnorm linear kernel metaparameters selected maximize classiﬁcation accuracy paper category validation set considered fold It clear results dom heuristic effectively breaks learning complexity allowing ﬁnd better solutions As shown smaller gains moving 0 ii iii iv v 0 ii iii iv v learning schema signiﬁcant portion gains comes introducing higher level semantic rules predicates approximated ﬁtting supervised data M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 159 Table 1 Results CORA dataset average 5 available folds different Semantic Based Regularization models trained variable number supervised patterns different heuristics controlling sequence incorporation FOL rules training process Papers accuracy SBR 0iiiiiiivv SBR 0 iiiiii ivv SBR 0 iiiiii iv v Research areas F1 SBR 0iiiiiiivv SBR 0 iiiiiiivv SBR 0 iiiiii iv v 10 0479 0492 0511 10 0464 0471 0482 20 0538 0554 0565 20 0511 0520 0526 30 0584 0599 0612 30 0551 0565 0561 40 0616 0630 0638 40 0577 0590 0595 50 0623 0647 0656 50 0585 0606 0610 A comparison Markov Logic Networks discrete MLN hybrid versions HMLN carried implementation provided Alchemy software package3 discriminative weight training optimized rescaled conjugate gradient provided best results task MLNs trained knowledge base previously employed SBR rules iv v added hard constraints Furthermore following formulas added care bagofwords representation page linking words document categories MLN rule deﬁnitions HasWordword x Cx class x variable spanning papers following Alchemy syntax means rule added word class pair This follows experimental set text categorization suggested Domingos et al 12 In case Hybrid Markov Logic Networks TFIDF score associate numeric feature ground clause previous rule In particular TFIDF score 1 gaussian decayed Alchemy soft equality penalty function In order compare Probabilistic Soft Logic PSL modiﬁcations KB required PSL process rules conjunctive bodies singleliteral head propositional formula obtained evaluation grounded predicates Horn Clause Therefore formula creating manifold structure authors papers split corresponding formulas PSL evaluation new formulas processed PSL logical AND equivalent original formulation For example ﬁrst manifold rule SBR replaced pair formulas x y Citex y Cix Ci y x y Citex y Cix Ci y The procedure performed formulas expressing manifold respect CoAuthor Author predicates The formulas implementing logic OR operation classes implemented PSL Unlike SBR PSL direct integration SVM processing input pattern representations However like Bröcheler et al 5 Wikipedia Category Prediction experiment PSL employ output previously trained feature based classiﬁer prior assignments This reusing SVMs previously trained adding following rules PSL learning task deﬁnition x SVMCategoryx Cix x SVMCategoryx Cix SVMCategoryx evidence predicate holding true value iff SVM assigns tag pattern x SVM trained tag provides output greater 0 processing input pattern x PSL formula weights learned validation set LazyMaxLikelihoodMPE algorithm Most Probable Explanation Max likelihood provided best results task employed training algorithm examples provided PSL software package SBR compared standard Structured Transductive SVMs paper classiﬁcation task Struc tured SVMs perform native multiclass classiﬁcation explained Tsochantaridis et al 47 Like SBR linear kernel metaparameters selected maximize accuracy F1 scores validation set SVM experiments The libSVM software package4 implementation plain SVMs SVMlight software package5 Transductive Structured SVM Since authors associated 3 http alchemycs washington edu 4 http wwwcsie ntu edu tw cjlin libsvm 5 http svmlight joachims org 160 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 Table 2 Accuracy F1 scores average 5 available folds CORA dataset patterns paper research area domains respectively Papers classiﬁed plain SVMs SVM Structured SVMs SSVM Transductive SVMs TSVM Semantic Based Regularization SBR plain MLN Hybrid HMLN Markov Logic Networks PSL SVM PSLSVM Transductive SVM PSLTSVM base classiﬁer providing classiﬁcation priors The classiﬁcation performed different numbers supervised patterns Research area classiﬁcation carried SRL methods SBR different MLN PSL versions process patterns associated vectorial feature representation Papers accuracy SVM SSVM TSVM MLN HMLN PSLSVM PSLTSVM SBR Research areas F1 MLN HMLN PSLSVM PSLTSVM SBR 10 0211 0352 0357 0272 0257 0258 0396 0511 10 0278 0275 0232 0365 0482 20 0274 0385 0396 0336 0328 0367 0425 0565 20 0335 0322 0304 0394 0526 30 0317 0410 0419 0382 0375 0404 0442 0612 30 0372 0375 0345 0419 0561 40 0348 0419 0436 0407 0407 0443 0460 0638 40 0386 0395 0385 0434 0595 50 0376 0430 0453 0435 0426 0460 0472 0656 50 0417 0404 0403 0447 0610 feature representation classiﬁed purely relational approach comparison SVMbased classiﬁcation schemas possible PSL tested SVM Transductive SVM base classiﬁer providing prior classiﬁcation These versions indicated PSLSVM PSLTSVM respectively Table 2 reports accuracy F1 scores test set patterns representing papers authors respectively Since methods allow express exclusive classiﬁcation explicit rule KB output class methods selected class associated highest value classiﬁcation outputs pattern Metrics bold represent statistically signiﬁcant gains 95 classiﬁers Since transductive learning task surprise Transductive SVMs best performers nonrelational classiﬁers Both PSLSVM PSLTSVM outperform corresponding base nonrelational classiﬁers PSLTSVM outperforms PSLSVM better performing base classiﬁer The SBR model outperforms nonrelational classiﬁers integrate richer prior knowledge The large improvement SBR tested SRL models important factors First SBR provides larger ﬂexibility designing KB rules dropped modiﬁed PSL deﬁnitions having conjunctive body single head Secondly SBR integrates processing input feature representations higher level logic knowledge This means SBR natively backpropagates output ference process performed learning KB underlying SVMs signiﬁcantly improving performances little supervised data available PSL uses frozen SVM prior classiﬁcations ability improve underlying classiﬁer unsupervised data This huge advantage SBR transductive context large portion unsupervised data 92 Collective classiﬁcation WebKB This experiment evaluates proposed collective classiﬁcation approach WebKB benchmark The WebKB dataset relational dataset consists labeled web pages science departments 4 universities We version dataset Craven Slattery 7 following papers 2932 features 4100 webpages 10 000 hyperlinks Each webpage associated vectorial representation content represented bagofwords link associated anchor text Each webpage belongs 5 categories person course department researchproject In addition anchors links belong 5 classes toPerson toCourse toDepartment toResearchProject toOther depending category pointed webpage The goal benchmark predict categories webpages links given data Each university represents independent world 4fold crossvalidation natural evaluation pro cedure evaluate performance classiﬁcation 4 folds generated keeping data university test set selecting ﬁrst remaining universities training set validation data Knowledge base Let assume W A denote set web pages anchor text identiﬁers respectively B rep resents Boolean value We consider following predicates following relational representation shown Fig 4 Ci W B Cix true x category 1 5 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 161 Fig 4 Relational representation WebKB Dataset LCi A B LCix true x category 1 5 Link W 2 B Linkx y true x linked y LinkTo A W B Linkx y true x linked y The following KB task 0 x P ix Ckx x Nix Ckx x y Linkx y Cix C j y Cix C j y ii x y LinkTox y LCix Ci y LCix Ci y iii x C1x C2x C3x C4x C5x iv x LC1x LC2x LC3x LC4x LC5x The 0 formulas express ﬁtting supervised data The formulas state linked pages tend class manifold regularization ii ones dictate classiﬁcation consistency predicted class pages anchors Formulas iii iv impose closeworld assumption forcing web page anchor belong 5 classes corresponding domain The overall knowledge base consists 42 rules The following formulas representing words correlated webpage link categories added MLN rule deﬁnitions HasWordword page Categorypage class HasWordword anchor AnchorCategoryanchor anchorclass deﬁne rule word class word anchorclass pairs respectively The rules iii iv added hard constraints MLN experiments veriﬁed For PSL experimental comparison procedure described previous experiment express manifolds built Link LinkTo predicates The input pattern representations embedded PSL classiﬁcation adding set rules expressing consistency PSL SVM class assignments SVM previously trained class PSL formula weights learned validation set LazyMaxLikelihoodMPE algorithm Results discussion Since training set single fold completely supervised SBR classiﬁer fold trained 0 rules converted minimum tnorm plain kernel machine linear kernel Classiﬁcation performance validation set select optimal regularization parameter fold The output learned functions initialize collective classiﬁcation step validation test sets In particular collective classiﬁcation performed separately validation set fold different λc parameters selecting λc value providing best results fold Finally collective classiﬁcation performed test set fold λc λc selected previous step The results obtained Semantic Based Regularization minimum tnorm collective classiﬁcation SBRCC compared plain SVM Markov Logic Network MLN We Alchemy software implementation MLNs discriminative weight training optimized rescaled conjugate gradient The libSVM software package implement plain SVMs linear kernel The SVM C parameter trade model complexity ﬁtting supervised data selected maximize classiﬁcation metrics validation set Table 3 shows F1 AUC scores obtained test set webpage anchor link categories average 4 folds SVM classiﬁcation metrics good webpages rich featurebased representations allow discriminate patterns SVM performances worse anchors anchor text represented feature vector small noisy representative MLNs perform slightly worse SVMs webpage classiﬁcation task MLNs advantage information available feature vectors On hand MLNs advantage available KB improve classiﬁcation anchors respect SVMs PSL 162 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 Table 3 F1 AUC scores test set webpage anchor link categories WebKB Dataset plain SVM SVM PSL Markov Logic Networks MLN Semantic Based Regularization collective classiﬁcation SBRCC average 4 WebKB folds SVM MLN PSL SBRCC F1 Webpage 0768 0730 0777 0810 Anchors 0319 0564 0510 0700 AUC Webpage 0861 0670 0763 0895 Anchors 0634 0763 0753 0775 performs webpage classiﬁcation task thanks good performance base classiﬁer However rich additional KB integrated PSL limits performance anchor patterns SBRCC best performer domains small margin best competitor separate domains fully advantage featurebased representations plus perform inference anchor domain entire KB 93 Arnetminer The Arnetminer Movie benchmark6 relational dataset containing information movies associated directors writers actors The Arnetminer dataset assigns set tags movie We selected movies tag containing 12 common keywords horror drama comedy television teen musical adventure western mystery thriller biographical Each tags assumed correspond underlying genre want predict The goal experiment predict movie genres looking movie title represented bagof words Please note multilabel dataset movies associated multiple genres The movies split sets set composed movies released year 1979 movies 1980 1997 ﬁnally later movies added set The resulting datasets contain 7567 4234 5653 movies respectively The ﬁrst second sets form pools training validation test data selected single experiments respectively This simulate real world scenario training performed point time available previous data trained model perform predictions newly received data A variable percentage supervised labels movies ﬁrst second sets kept training evaluate performance affected labeled data The remaining unlabeled patterns provided training set unsupervised data 5 different folds generated performing different samples percentage labels kept available training validation sets Together rules expressing ﬁtting supervised data following rules added prior knowledge tag Ci dataset cid7 x y SameDirectorx y x y SameProducerx y x y SameWriterx y cid8 Cix Ci y cid8 Cix Ci y cid8 Cix Ci y cid7 cid7 cid7 cid8 Cix Ci y cid8 Cix Ci y cid8 Cix Ci y cid7 cid7 express fact movies sharing director producer writer tend belong genres The following rules tag Ci express Transductive SVM assumption x Cix Cix nx Cix m x Cix n m N N overall number available movies considered dataset n m estimated looking distri bution Ci Ci training data fold respectively After training SBR models minimum tnorm linear kernel different λc metaparameters best model selected validation set Finally Semantic Based Regularization collective classiﬁcation SBRCC performed test set selected model The SBRCC results compared Semantic Based Regularization SBR collective classiﬁcation performed test set trained kernel machines directly perform predictions Furthermore SBRCC compared standard SVM supervised labels Transductive SVM TSVM Laplacian SVM LSVM director producer writer rules build manifold data Probabilistic Soft Logic SVM PSLSVM TSVM classiﬁers priors PSLTSVM One separate binary classiﬁer class built SVM LSVM TSVM In particular software simulator 6 It downloaded http arnetminerorg labdatasets soinf M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 163 Fig 5 F1 scores average 5 folds Arnetminer dataset obtained SVM TSVM LSVM PSL SVM PSLSVM Transductive SVM PSLTSVM base classiﬁer providing classiﬁcation priors SBR SBR collective classiﬁcation SBRCC varying number supervised patterns experiments Melacci Belkin 31 implementation Laplacian SVMs The software simulators previous experiments employed implementations SVMs TSVMs All SVMbased classiﬁers employed linear kernel models trained different metaparameters The best model foldbyfold selected cross validation validation set Similarly previous experiments formula creating manifold structure movies split corresponding formulas PSL evaluation For example ﬁrst manifold rule SBR replaced pair formulas PSL rule deﬁnitions x y SameDirectorx y Cix Ci y x y SameDirectorx y Cix Ci y The procedure performed formulas expressing manifold respect SameProducer SameWriter predicates The formulas implementing Transductive SVM assumption expressed PSL experimental comparison Like previous experiments rules expressing consistency tween PSL class assignments output SVMs trained class added PSL rule deﬁnitions PSL formula weights learned validation set LazyMaxLikelihoodMPE algorithm provided best results task Finally PSL collective classiﬁcation step rule weights learned training performed test set The output collective classiﬁcation step determine PSL classiﬁcation performances Fig 5 reports classiﬁcation results average 5 folds SVMs expected worst performers task tested model beneﬁt unsupervised data seen training PSL TSVM LSVM perform similarly outperformed SBR PSLTSVM takes advantage better performing base classiﬁer outperforms PSLSVM Both PSL versions perform worse SBR task PSL advantage unsupervised data improve underlying classiﬁers uses classiﬁcation priors Finally SBR collective classiﬁcation SBRCC slightly improves SBR tested conﬁgurations enforcing rules test set 10 Conclusions In paper proposed semanticbased regularization approach learning inference generalizes different kernel machine models processing realvalued features statistical relational learning approaches working symbolic identiﬁers domain knowledge expressed term FOL formulas The resulting inference mechanism involves novel collective classiﬁcation schema exploits realvalued features As shown experimental results useful feature representation relatively poor involvement constraints time test signiﬁcantly improves performance This paper signiﬁcantly extends theoretical results previously published presenting intriguing connections probabilistic models proposing novel heuristics allowing tackle complex learning tasks 164 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 Acknowledgements This research partially supported research grant 2009LNP494 PRIN2009 program Italian MURST References Res 7 2006 2434 gence UAI 2010 pp 7382 Springer 2003 pp 3247 2003 pp 107114 Learning 2004 pp 4954 1 F Baader The Description Logic Handbook Theory Implementation Applications Cambridge University Press 2003 2 JF Bard Practical Bilevel Optimization Algorithms Applications Nonconvex Optimization Its Applications vol 30 Springer 1998 3 M Belkin P Niyogi V Sindhwani Manifold regularization geometric framework learning labeled unlabeled examples J Mach Learn 4 Y Bengio Curriculum learning Proceedings 26th Annual International Conference Machine Learning ICML0 2009 pp 4148 5 M Broecheler L Mihalkova L Getoor Probabilistic similarity logic Proceedings TwentySixth Conference Uncertainty Artiﬁcial Intelli 6 A Caponnetto CA Micchelli M Pontil Y Ying Universal multitask kernels J Mach Learn Res 9 2008 16151646 7 M Craven S Slattery Relational learning statistical predicate invention better models hypertext Mach Learn 2001 97119 8 C Cumby D Roth Learning feature description logics Proceedings 12th International Conference Inductive Logic Programming 9 C Cumby D Roth On kernel methods relational learning Proceedings Twentieth International Conference Machine Learning ICML 10 M Diligenti M Gori M Maggini L Rigutini Bridging logic kernel machines Mach Learn 86 2012 5788 11 P Domingos M Richardson Markov logic unifying framework statistical relational learning ICML2004 Workshop Statistical Relational 12 P Domingos M Sumner The alchemy tutorial httpalchemycswashingtonedututorialtutorialpdf 2010 13 N Friedman L Getoor D Koller A Pfeffer Learning probabilistic relational models Proceedings International Joint Conference Artiﬁcial 14 GM Fung OL Mangasarian JW Shavlik Knowledgebased support vector machine classiﬁers Advances Neural Information Processing Systems 15 GM Fung OL Mangasarian JW Shavlik Knowledgebased nonlinear kernel classiﬁers Learning Theory Kernel Machines Springer 2003 Intelligence IJCAI 1999 pp 13001309 2002 pp 521528 pp 102113 16 SW Golomb LD Baumert Backtrack programming J ACM 12 1965 516524 17 M Gupta J Qi Theory tnorms fuzzy inference methods Fuzzy Sets Syst 40 1991 431450 18 P Hajek The Metamathematics Fuzzy Logic Kluwer 1998 19 RM Haralick GL Elliott Increasing tree search eﬃciency constraint satisfaction problems Artif Intell 14 1980 263313 20 D Haussler Convolution kernels discrete structures Technical report Department Computer Science University California Santa Cruz 1999 21 P Hitzler S Holldobler AK Sedab Logic programs connectionist networks J Appl Log 2 2004 245272 22 TN Huynh RJ Mooney Discriminative structure parameter learning Markov logic networks Proceedings 25th International Confer ence Machine Learning ICML ACM 2008 pp 416423 23 S Kok P Domingos Learning structure Markov logic networks Proceedings 22nd International Conference Machine Learning ICML ACM 2005 pp 441448 Intelligence 2006 pp 389394 24 N Landwehr A Passerini L De Raedt P Frasconi kfoil learning simple relational kernels Proceedings AAAI Conference Artiﬁcial 25 N Landwehr A Passerini L Raedt P Frasconi Fast learning relational kernels Mach Learn 2010 26 F Laurer G Bloch Incorporating prior knowledge support vector machines classiﬁcation review Neurocomputing 71 2009 15781594 27 QV Le AJ Smola T Gärtner Simpler knowledgebased support vector machines Proceedings 23rd International Conference Machine 28 M Lippi P Frasconi Prediction protein βresidue contacts Markov logic networks groundingspeciﬁc weights Bioinformatics 25 2009 Learning ICML ACM 2006 pp 521528 23262333 29 D Lowd P Domingos Eﬃcient weight learning Markov logic networks Proceedings Eleventh European Conference Principles Practice Knowledge Discovery Databases 2007 pp 200211 30 A McCallum K Nigam J Rennie K Seymore Automating construction Internet portals machine learning Inf Retr 3 2000 127163 31 S Melacci M Belkin Laplacian support vector machines trained primal J Mach Learn Res 12 2011 11491184 32 L Mihalkova RJ Mooney Bottomup learning Markov logic network structure Proceedings 24th International Conference Machine Learning ACM New York NY USA 2007 pp 625632 33 S Muggleton H Lodhi A Amini MJ Sternberg Support vector inductive logic programming Discovery Science Springer 2005 pp 163175 34 J Neville D Jensen Relational dependency networks J Mach Learn Res 8 2007 653692 35 F Niu C Ré A Doan J Shavlik Tuffy scaling statistical inference Markov logic networks RDBMS Proceedings VLDB 2011 pp 373384 36 V Novák Firstorder fuzzy logic Stud Log 46 1987 87109 37 J Piaget La psychologie lintelligence Armand Colin Paris 1961 38 T Poggio F Girosi A theory networks approximation learning Technical report MIT 1989 39 LD Raedt P Frasconi KSM Kersting Eds Probabilistic Inductive Logic Programming Lecture Notes Artiﬁcial Intelligence vol 4911 Springer 2008 40 M Richardson P Domingos Markov logic networks Mach Learn 62 2006 107136 41 F Rossi P Van Beek T Walsh Handbook Constraint Programming Elsevier 2006 42 B Scholkopf AJ Smola Learning Kernels MIT Press Cambridge MA USA 2001 43 JW Shavlik S Natarajan Speeding inference Markov logic networks preprocessing reduce size resulting grounded network Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 2009 pp 19511956 44 P Singla P Domingos Memoryeﬃcient inference relational domains Proceedings 21st AAAI Conference Artiﬁcial Intelligence AAAI 45 JT Starczewski Advanced Concepts Fuzzy Logic Systems Membership Uncertainty Springer 2012 46 SD Tran LS Davis Event modeling recognition Markov logic networks Proceedings European Conference Computer Vision Press 2006 pp 488493 ECCV Springer 2008 pp 610623 M Diligenti et al Artiﬁcial Intelligence 244 2017 143165 165 47 I Tsochantaridis T Joachims T Hofmann Y Altun Large margin methods structured interdependent output variables J Mach Learn Res 2005 14531484 48 V Vapnik The Nature Statistical Learning Theory 2nd edn Springer Verlag 2000 49 J Wang P Domingos Hybrid Markov logic networks Proceedings 23rd AAAI Conference Artiﬁcial Intelligence 2008 pp 11061111 50 PM Williams Bayesian regularization pruning Laplace prior Neural Comput 7 1995 117143 51 LA Zadeh Fuzzy sets Inf Control 8 1965 338353 52 D Zhou B Schölkopf Regularization discrete spaces Pattern Recognit 2005 361368