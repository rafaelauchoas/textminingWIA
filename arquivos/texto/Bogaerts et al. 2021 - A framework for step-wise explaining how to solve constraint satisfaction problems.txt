Artiﬁcial Intelligence 300 2021 103550 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A framework stepwise explaining solve constraint satisfaction problems Bart Bogaerts Emilio Gamba Tias Guns Vrije Universiteit Brussel Pleinlaan 2 1050 Brussel Belgium r t c l e n f o b s t r c t Article history Received 27 April 2020 Received revised form 20 June 2021 Accepted 22 June 2021 Available online 29 June 2021 Keywords Artiﬁcial intelligence Constraint solving Explanation We explore problem stepwise explaining solve constraint satisfaction problems use case logic grid puzzles More speciﬁcally study problem explaining inference steps propagation way easy interpret person Thereby aim constraint solver explainable agency help building trust solver able understand learn explanations The main challenge ﬁnding sequence simple explanations explanation aim cognitively easy possible human verify understand This contrasts arbitrary combination facts constraints solver use propagating We propose use cost function quantify simple individual explanation inference step identify explanationproduction problem ﬁnding best sequence explanations CSP Our approach agnostic underlying constraint propagation mechanisms provide explanations inference steps resulting combinations constraints In case multiple constraints involved develop mechanism allows break diﬃcult steps gives user ability zoom speciﬁc parts explanation Our proposed algorithm iteratively constructs explanation sequence optimistic estimate cost function guide search best explanation step Our experiments logic grid puzzles feasibility approach terms quality individual explanations resulting explanation sequences obtained 2021 The Authors Published Elsevier BV This open access article CC BYNCND license httpcreativecommonsorglicensesbyncnd40 1 Introduction In years AI systems employ advanced reasoning mechanisms computation power increasingly diﬃcult understand certain decisions Explainable AI XAI research aims fulﬁll need trustworthy AI systems understand decision verifying correctness control biased systematically unfair decisions Explainable AI studied context black box prediction systems neural networks goal provide insight parts input important learned model These insights local approximations thereof justify certain predictions In setting range techniques developed ranging This paper Special Issue Explainable AI Corresponding author Email addresses bartbogaertsvubbe B Bogaerts emiliogambavubbe E Gamba tiasgunsvubbe T Guns httpsdoiorg101016jartint2021103550 00043702 2021 The Authors Published Elsevier BV This open access article CC BYNCND license httpcreativecommonsorglicensesbyncnd40 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 local explanations predictions feature level 12 visual explanations saliency maps 3 Adadi et al 4 Guidotti et al 5 recently Arrieta et al 6 survey latest trends major research directions area In contrast Constraint Satisfaction Problems CSP 7 problem speciﬁcation explicit modelbased representation problem creating opportunity explain inference steps directly terms representation Explanations investigated constraint solving notably explaining overconstrained unsatisﬁable problems user 8 Our case general works satisﬁable problems At solving level lazy clause generation solvers explanations constraint studied form implication lowlevel Boolean literals encode result propagation step individual constraint 9 Also nogoods learned clauses conﬂictdriven clause learning SAT solvers seen explanations failure search 10 These meant humaninterpretable propagate effectively We aim explain process propagation constraint solver independent consistency level propaga tion augmenting propagators explanation capabilities For problems given strong propagation mechanism solved search problems logic grid puzzles unique solution means explaining entire problem solving process For problems involving search means explaining inference steps search node It deserves stressed interested computational cost performing expensive form propagation explaining consequences given assignment user way understandable possible More speciﬁcally aim develop explanationproducing complete interpretable By complete mean ﬁnds sequence small reasoning steps starting given problem speciﬁcation partial solution derives consequences Gilpin et al 11 deﬁne interpretable explanations descriptions simple person understand vocabulary meaningful user Our guiding principle simplicity smaller simpler explanations better An open issue presenting sequence explanations level abstraction Starting sequence assimpleaspossible explanations ways change level abstraction The ﬁrst direction group multiple single steps larger step expanded ﬂy The second direction provide detailed explanation harder steps broken simpler steps standard mechanism While ﬁrst direction useful explanations long theoretical perspective interesting The second direction hand interesting reasons First noticed generated steps complicated understood easily require explained detailed Secondly breaking smaller steps interesting challenge Indeed start idea steps simple possible possible break To break took inspiration methods people use solving puzzles mathematicians use proving theorems That explaining single step work reasoning contradiction starting negation consequence explain contradiction obtained In essence assume negation derived fact reuse principle explanation steps construct sequence leads inconsistency This novel approach allows provide mechanism zooming diﬃcult explanation step In practice choose present constraints natural language obvious choice logic grid puzzles given form natural language clues We represent previously newly derived facts visually seen grid Fig 1 In ﬁgure implicit Bijectivity axiom present logic grid puzzle derive following Arrabiata sauce eaten Farfalle eaten pasta types Our work speciﬁcally use case logic grid puzzles motivated Holy Grail Challenge1 objective provide automated processing logic grid puzzles ranging natural language processing solving explaining While integrated ZebraTutor capability solving logic grid puzzle starting natural language clues Section 8 focus paper explanationproducing The explanationgenerating techniques develop applied multitude use cases For instance explain entire sequence reasoning user better understand case unexpected outcome debug reasoning set constraints specify problem As approach starts arbitrary set facts virtual assistant user stuck solving problem The explain simplest possible step interactive setting explain complete partial solution user Such explanations useful context interactive conﬁguration 12 domain expert solves problem complicated scheduling rostering problem assisted intelligent The case typically knowledge required solve problem certain things hard formalize especially personal matters involved In case ﬁnd optimal solution automatically help expert instance propagating information follows knowledge base In case undesired propagated user need explanation follows methods plugged Finally measures simplicity reasoning steps estimate diﬃculty solving problem human gradual training experts Summarized main contributions following 1 httpsfreuderwordpress com pthg 19 workshop progress holygrail 2 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Fig 1 Demonstration explanation visualization For interpretation colors ﬁgures reader referred web version article We formalize problem stepwise explaining propagation constraint solver sequence small inference steps We propose algorithm agnostic propagators consistency level provide explanations inference steps involving arbitrary combinations constraints Given cost function quantifying human interpretability method uses optimistic estimate function guide search lowcost explanations making use Minimal Unsatisﬁable Subset extraction We introduce nested explanations provide additional explanations complex inference steps reasoning contradiction We experimentally demonstrate quality feasibility approach domain logic grid puzzles This paper structured follows In Section 2 discuss related work Section 3 explains rules logic grid puzzles presents background information Sections 4 5 formalize theory explanationproduction problem abstraction levels Section 6 describes algorithms needed solve explanationproduction problem In Section 7 motivate design decisions observations ZebraTutor use case Section 8 describes entire information pipeline ZebraTutor integrated In Section 9 experimentally study feasibility approach Finally conclude paper Section 10 Publication history This paper extension previous papers presented workshops conferences 1315 The current paper extends previous papers detailed examples additional experiments formalization nested explanation sequences 2 Related work This research ﬁts general topic Explainable Agency 16 order people trust autonomous agents able explain decisions reasoning produced choices Explanations Constraint Satisfaction Problems CSP studied context overconstrained prob lems The goal ﬁnd small conﬂicting subset The QuickXplain method 8 example uses dichotomic approach recursively partitions constraints ﬁnd minimal conﬂict set Many papers consider goal search explanations overconstrainedness 1718 A minimal set conﬂicting constraints called minimal unsatisﬁable subset MUS minimal unsatisﬁable core 19 Despite fact speciﬁcally aim explain overconstrained problems algorithms internally use MUS extraction methods 3 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Fig 2 Demonstration explanation visualization While explainability constraint optimization received little attention far related ﬁeld planning emerging subﬁeld eXplainable AI planning XAIP 20 concerned building planning systems explain behavior This includes answering queries certain decision best decision In contrast explainable machine learning research 5 explainable planning use explicit modelbased representation reasoning happens Likewise use constraint speciﬁcation available constraint solvers speciﬁcally typed ﬁrstorder logic 21 Our work inspired Holy Grail Challenge 22 2019 Constraint Programming conference CP turn roots earlier work E Freuder inferencebased explanations 23 In authors investigate logic grid puzzles develop number problemspeciﬁc inference rules allow solving puzzles search These inference rules equipped explanation templates propagation event inference rule templated explanation explanation solution process obtained We point complex inference rules NCC GNCC fact inference rules hardcoded combina tions inequality constraints In contrast proposed method works type constraint combination constraints automatically infers minimal set facts constraints explain inference step problemspeciﬁc knowledge This powerful combination constraints able automatically detect interesting consis tency patterns needed handcoded Freuders seminal work solutions submitted contestants challenge 24 Fig 2 shows example nontrivial explanation approach automatically generated combination socalled bijectivity transitivity constraints hardcoded specialpurpose path consistency pattern earlier logicgrid speciﬁc work 23 There rich literature automated interactive theorem proving recently focusing providing proofs understandable humans 25 teaching humans interaction theorem provers craft mathematical proofs 26 Our work ﬁts line research generated explanations seen proofs setting ﬁnitedomain constraint solving In context interactive explanations Caine Cohen 27 introduce mixedinitiative Intelligent Tutoring System MITS In framework maintains model knowledge user solving sudoku estimating certain strategies understood The tutor based model student decides beneﬁts outweigh costs interaction student proposing explaining step student lacks knowledge The student ability ask guidance words involved parties initiative engage interaction In comparison approach aim model user assume costfunction speciﬁed quantiﬁes diﬃculties derivations At point time suggest simplest derivation 4 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 3 Background While proposed method applicable constraint satisfaction problems general use logic grid puzzles example domain requires expert knowledge understand Our running example puzzle people having dinner restaurant ordering different types pasta hardest logic grid puzzle encountered sent got stuck solving wondered correct ﬁrst place The entire puzzle seen Fig 1 explanation sequence generated explored online httpbartbog github io zebra pasta In section ﬁrst present logic grid puzzles use case introduce typed ﬁrstorder logic use language express constraint programs Here important stress deﬁnitions algorithms work language modeltheoretic semantics including typical constraint programming languages 28 31 Logic grid puzzles A logic grid puzzle known Zebra puzzle Einstein puzzle consists natural language sentences hereon referred clues set entities occurring sentences For instance running example Fig 1 contains second clue The person chose arrabiata sauce Angie Elisa entities arrabiata sauce Angie Elisa The set entities left implicit derived clues given form grid Furthermore puzzle set entities partitioned equallysized groups corresponding types example person sauce types The goal puzzle ﬁnd relations types Each clue respected Each entity type matched exactly entity second type person chose exactly sauce sauce linked person type constraint referred bijectivity The relations logically linked Angie chose arrabiata sauce arrabiata sauce paired farfalle Angie eaten farfalle called transitivity 32 Typed ﬁrstorder logic Our constraint solving method based typed ﬁrstorder logic Part input logical vocabulary consisting set types typed constant symbols typed relation symbols associated type signature relation symbol typed T 1 Tn n types T 2 For running example constant symbol Angie type person linked relation chose signature person sauce constant symbol arrabiata type sauce A ﬁrstorder theory set sentences wellformed variablefree ﬁrstorder formulas 29 quantiﬁed variable associated type referred constraints In Section 8 explain obtain vocabulary theory automated way clues logic grid puzzle Natural Language Processing For assume given Since work ﬁxed ﬁnite domain vocabulary interpretation types domains constants ﬁxed This justiﬁes following deﬁnitions Deﬁnition 1 A partial interpretation ﬁnite set literals expressions form P d P d P relation symbol typed T 1 Tn d tuple domain elements di type T Deﬁnition 2 A partial interpretation consistent contain atom negation called interpretation contains P d P d welltyped atom P d For instance partial interpretation I1 choseAngie arrabiata choseElisa arrabiata known Angie arrabiata sauce Elisa This partial interpretation specify Elisa ate Farfalle The syntax semantics ﬁrstorder logic deﬁned usual 29 means satisfaction relation I T ﬁrstorder theories T interpretations I If I T I model solution T Deﬁnition 3 A partial interpretation I1 precise partial interpretation I2 notation I1 p I2 I1 I2 2 We omit function symbols paper 5 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Informally partial interpretation precise contains information For example partial interpretation I2 choseAngie arrabiata choseElisa arrabiata choseDamon arrabiata precise I1 I2 p I1 For practical purposes variablefree literals sentences freely use partial interpretation theory solver calls statements form I T J meaning J consequence I T stated differently J precise model M T satisfying M p I In context ﬁrstorder logic task ﬁnitedomain constraint solving better known model expansion 30 given logical theory T corresponding constraint speciﬁcation partial interpretation I ﬁnite domain corresponding initial domain variables ﬁnd model M precise I partial solution satisﬁes T If P logic grid puzzle use T P denote ﬁrstorder theory consisting One logical sentence clue P A logical representation possible bijection constraints A logical representation possible transitivity constraints For instance running example sentences T P choseClaudia puttanesca s sauce p pasta paireds p s sauce p1 pasta p2 pasta paireds p1 paireds p2 p1 p2 s sauce p person c price chosep s payedp c priceds c The ﬁrst sentences translation fourth clue running example Claudia choose puttanesca sauce The second express sauce paired exactly pasta bijectivity expresses transitivity constrained chose payed priced 4 Problem deﬁnition The overarching goal paper generate sequence small reasoning steps interpretable expla nation We ﬁrst introduce concept explanation single reasoning step introduce cost function proxy interpretability reasoning step cost sequence steps 41 Explanation reasoning steps We assume theory T initial partial interpretation I0 given ﬁxed Deﬁnition 4 We deﬁne maximal consequence theory T partial interpretation I denoted maxI T precise partial interpretation J I T J Phrased differently maxI T intersection models T precise I known set cautious consequences T I corresponds ensuring global consistency constraint solving Al gorithms computing cautious consequences explicitly enumerating models exist instance ones implemented clasp 31 IDP 32 task computing cautious consequences called optimalpropagate performs strongest propagation possible Weaker levels propagation consistency leading potentially smaller maximal consequence interpretation maxotherConsistencyI T The rest paper assumes want construct sequence starts I0 ends maxI0 T consistency algorithm explain computable consequences T I0 Deﬁnition 5 A sequence incremental partial interpretations theory T initial partial interpretation I0 sequence cid10I0 I1 In maxI0 T cid11 0 Ii1 p Ii sequence precisionincreasing The goal work obtain sequence incremental partial interpretations incremental step cid10Ii1 Iicid11 want explanation E S justiﬁes newly derived information Ni Ii Ii1 When visualized Fig 1 user precisely information constraints derive new piece information 6 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Deﬁnition 6 Let Ii1 Ii partial interpretations Ii1 T Ii We E S Ni explains derivation Ii Ii1 following hold Ni Ii Ii1 Ni consists newly deﬁned facts E Ii1 explaining facts subset previously derived S T subset constraints S E Ni newly derived information follows explanation The problem simply checking E S Ni explains derivation Ii Ii1 coNP problem performed verifying S Ni models precise E It instance negation ﬁrstorder model expansion problem 33 Part goal ﬁnding easy interpret explanations avoid redundancy That want nonredundant explanation E S Ni facts E constraints S removed explaining derivation Ii Ii1 word explanation subsetminimal Deﬁnition 7 We E S Ni nonredundant explanation derivation Ii Ii1 explains derivation E cid15 Ni explains derivation E E cid15 S E cid15 E S cid15 S S cid15 S cid15 Deﬁnition 8 A nonredundant explanation sequence sequence cid10I0 I1 E 1 S1 N1 In En Sn Nncid11 Iiin sequence incremental partial interpretations E S Ni explains derivation Ii Ii1 42 Interpretability reasoning steps While subsetminimality ensures explanation nonredundant quantify interpretable expla nation This quantiﬁcation problemspeciﬁc subjective In paper assume existence cost function f E S Ni quantiﬁes interpretability single explanation In line goal simple person understand Occams Razor 43 Interpretability sequence reasoning steps In general form like optimize understandability entire sequence explanations While quantifying interpretability single step hard sequence explanations harder For example related diﬃcult step average diﬃculty important ordering se quence As starting point consider total cost aggregation costs individual explanations average maximum cost Deﬁnition 9 Given theory T initial partial interpretation I0 explanationproduction problem consists ﬁnding nonredundant explanation sequence cid10I0 I1 E 1 S1 N1 In En Sn Nncid11 predeﬁned aggregate sequence f E S Niin minimized Example aggregation operators max average peculiarities max aggregation operator minimize cost complicated reasoning step capture step multiple Likewise average aggregation operator favor simple steps including splitting trivial steps small components constraint abstraction allows Even ﬁxed aggregation operator problem holistically optimizing sequence explanation steps harder optimizing cost single reasoning step exponentially sequences 5 Nested explanations If explanation sequence nonredundant means steps broken smaller sub steps Yet earlier work noticed explanations hard understand mainly clue combined implicit constraints couple previously derived facts All things implied consequence taken account Such steps turned complicated understood easily require explained 7 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 An example diﬃcult step depicted Fig 3 It uses disjunctive clue The person ordered Rotini person paid 8 Damon person paid 8 Damon combination previously derived facts derive Farfalle cost 8 This derivation nontrivial looking highlighted clue It turns derivation explained stepwise manner implicit constraints help reasoning contradiction If Farfalle cost 8 Damon eat Farfalle Damon pay 8 If Farfalle costs 8 cost 16 Since Farfalle cost 16 Capellini Tagliolini Rotini cost 16 However fact Rotini costs 16 Damon pay 8 contradiction clue question Hence Farfalle cost 8 We wish provide nested explanation diﬃcult reasoning steps We believe explanation contradiction good tool reasons people solving puzzles mathematicians proving theorems ii adding negation derived fact Farfalle cost 8 allows generate new sequence nonredundant explanations inconsistency reusing techniques previous section This novel approach allows provide mechanism zooming diﬃcult explanation steps explanation sequence 51 Nested explanation reasoning step We propose following principles constitutes meaningful simple nested explanation given nontrivial explanation E S N nested explanation starts explaining facts E augmented counterfactual assumption newly derived fact n N step uses clues S step easier understand strictly lower cost parent explanation cost f E S N counterfactual assumption contradiction derived Note explanation step derives multiple new facts N 1 compute nested explanation ni N More formally deﬁne concept nested explanation follows Deﬁnition 10 The nested explanation problem consists given nonredundant explanation E S N newly rived fact n N ﬁnding nonredundant explanation sequence cid10 I cid15 0 I cid15 1 E cid15 1 S cid15 1 N cid15 1 I cid15 n E cid15 n S cid15 n N cid15 n cid11 cid15 I 0 partial interpretation ni E cid15 S S cid15 cid15 cid15 f E f E S N N S cid15 I n inconsistent predeﬁned aggregate sequence cid2 f E cid15 S cid15 N cid3 cid15 minimized We augment explanation E S N set nested explanations exist We discuss algorithms computing explanations nested explanations 6 Explanationproducing search In section tackle goal searching nonredundant explanation sequence simple possible understand Ideally generate explanations fact maxI0 T search lowest scoring sequence explanations However number explanations fact quickly explodes number constraints feasible compute Instead iteratively construct sequence generating candidates given partial interpretation searching smallest 8 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Fig 3 A diﬃcult explanation step including nested explanation 9 B Bogaerts E Gamba T Guns 61 Sequence construction Artiﬁcial Intelligence 300 2021 103550 We aim minimize cost explanations sequence measured aggregate individual explana tion costs f E S Ni aggregate like max average The cost function f example weighted sum cardinalities E S Ni Section 7 discuss concrete cost function use case logic grid puzzles Instead globally optimizing aggregated sequence cost encode knowledge seeking sequence small explanations algorithm Namely greedily incrementally build sequence time searching lowest scoring explanation given current partial interpretation Such explanation exists end point explanation process maxI0 T contains consequences I0 T Algorithm 1 formalizes greedy construction sequence determines Iend maxI0 T propagation relies minexplanationI T function ﬁnd costminimal explanation Algorithm 1 Highlevel greedy sequencegenerating algorithm Input A initial interpretation I0 set constraints T Output An explanation sequence Seq 1 Iend propagateI0 T 2 Seq sequence 3 I I0 4 I cid18 Iend 5 6 7 E S N minexplanationI T append E S N Seq I I N 8 end 62 Candidate generation The main challenge ﬁnding lowest scoring explanation reasoning steps applied given partial interpretation I We ﬁrst study enumerate set candidate nonredundant explanations given set constraints For set constraints T ﬁrst use propagation set new facts derived given partial interpretation I constraints T For new fact n I wish ﬁnd nonredundant explanation E I S T n explains n possibly explains Recall Deﬁnition 8 means facts E constraints T removed result longer explanation We task equivalent ﬁnding Minimal Unsat Core Minimal Unsat Subset MUS To consider theory I T n This theory surely unsatisﬁable n consequence I T Furthermore assumption I T consistent left explain unsatisﬁable subset theory contains n We unsatisﬁable subset theory form E S n E S n necessarily redundant explanation derivation n I Vice versa explanation n corresponds unsatisﬁable subset Thus minimal unsatisﬁable subsets MUS theory onetoone correspondence non redundant explanations n allowing use existing MUS algorithms search nonredundant explanations We point MUS algorithms typically ﬁnd unsatisﬁable core subsetminimal cardinality minimal That unsat core reduced minimal unsat core size smaller That means size taken measure simplicity explanations guarantee ﬁnd optimal ones And deﬁnitely cost function kicks optimality guaranteed Algorithm 2 shows proposed algorithm The key algorithm line 4 ﬁnd explanation single new fact n searching MUS includes n We search subsetminimal unsat cores avoid redundancy explanations Furthermore good explanation E S N immediately explain implicants E S In words N subsetmaximal The reason assume derivable facts use theory previously derived knowledge probably require similar types reasoning better consider Thus choose generate candidate explanations implicants E S line 7 Note implicants N n simpler explanations later loop remove J We assume use standard MUS algorithm searches satisfying solution failure encountered resulting Unsat Core shrunk minimal 19 While computing MUS computationally demanding far demanding enumerating MUSs arbitrary size candidates 10 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Algorithm 2 candidateexplanationsI T Input A partial interpretation I set constraints T Output A set candidate explanations Candidates 1 Candidates 2 J propagateI T 3 n J I Minimal expl new fact X MU Sn I T E I X S T X N propagateE S add E S N Candidates 4 5 6 7 8 facts constraints implied facts 9 end 10 return Candidates 63 Cost functions costminimal explanations We use Algorithm 2 generate candidate explanations general goal ﬁnd costminimal explanations In following assume ﬁxed cost function f returns score possible explanation E S N To guide search costminimal MUSs use observation typically small 1 number straints suﬃcient explain reasoning A small number constraints preferred terms easy understand explanations lower cost For reason candidateexplanations set constraints T iteratively grow number constraints We assumption ensure search candidates possible subsets constraints The assumption optimistic estimate g maps subset S T real number E N S gS f E S N This example case f additive function f E S N f 1E f 2S f 3N gS f 2S assuming f 1 f 3 positive We search smallest explanation candidates searching increasingly worse scoring S shown Algorithm 3 This algorithm called iterative sequence generation Algorithm 1 Algorithm 3 minexplanationI T Input A partial interpretation I set constraints T Output A nonredundant explanation E S N E I S T E S N gS f best 1 N propagateI T 2 best I T N 3 S T ordered ascending gS 4 5 6 7 8 9 10 best cand break end end cand best explanation candidateexplanationsI S f cand f best 11 end 12 return best Every time minexplanationI T called updated partial interpretation I explanations regener ated The reason derivable facts n easier costeffective explanation fact There beneﬁt caching Candidates different iterations subsequent cost costeffective explanation applicable lower bound start Furthermore practice cache candidates recompute MUS fact n store costeffective best previously fact different iterations 64 Searching nested explanations We extend Algorithm 1 generate new candidate explanations support nested explanations introduced Section 5 Fundamentally generated candidate explanations decomposed nested explanation sequence provided sequence easier candidate explanation according deﬁned cost function f We assess possibility nested explanation newly derived fact candidate explanation Similar Algorithm 1 Algorithm 4 exploits minexplanation function generate candidate nested explanations The difference computing explanation step nestedexplanations Algorithm 5 generate nested sequence 11 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 The computation nested explanation described Algorithm 5 reuses minexplanation main dif ferences high level explanation generating algorithm come fact search space easiest explanationstep bounded input explanation use constraints facts original explana tion ii cost parent explanation upper bound acceptable costs nested level Given explanation step E S N newly derived fact n N want detailed explanation formed facts E negated new fact n Then Algorithm 5 ﬁrst constructs partial interpretation I cid15 long interpretation consistent gradually build sequence adding newly explanations E explanation easier explanation step E S N line Deﬁnition 10 serves avoid nested explanation simply singlestep explanation equally diﬃcult original step cid15 N cid15 S cid15 While Algorithm 5 tries ﬁnd nested explanation sequence derived fact ﬁnd fact ifcheck Line 8 This check present avoid nested explanation diﬃcult harder high level step aims clarify Deﬁnition 10 bullet point explicitly forbids nested justiﬁcations This check kick different reasons The ﬁrst reason explanation step main level simply simple broken pieces For instance explanation Fig 1 kind uses single bijectivity constraint single previously derived fact Breaking derivation strictly smaller parts helpful This phenomenon occur diﬃcult steps best nested explanation diﬃcult explanation step contains step diﬃcult harder highlevel step In case sign reasoning contradiction simplifying matters step methods explored explain Algorithm 4 greedyexplainI0 T Input A initial interpretation I0 set constraints T Output An explanation sequence Seq 1 Iend propagateI0 T 2 Seq sequence 3 I I0 4 I cid18 Iend 5 6 7 8 E S N minexplanationI T nested nestedexplanationsE S N append E S N nested Seq I I N 9 end Algorithm 5 nestedexplanationsE S N Input Explanation facts E constraints S newly derived facts N Output A collection nested explanation sequences nested_explanations 1 nested_explanations set 2 n N 3 4 5 6 7 8 9 10 store true nested_seq sequence cid15 E n I consistentI cid15 S E f E cid15 minexplanationI cid15 f E S N cid15 N store f alse break cid15 N cid15 S cid15 cid15 S 11 12 13 14 15 16 17 cid15 N cid15 nested_seq end append E cid15 N cid15 I I cid15 S cid15 end store steps simpler ESN append nested_seq nested_explanations end 18 end 19 return nested_explanations 7 Explanations logic grid puzzles We instantiated described algorithm context logic grid puzzles In setting T T P puzzle P described Section 3 There types constraints T transitivity constraints bijectivity 12 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 constraints clues ﬁrst follow structure puzzle clues obtained automatic way Section 8 Before deﬁning costfunction estimation g implementation provide observations drove design decision Observation 1 Propagations single implicit constraint easy understand Contrary clues implicit constraints transitivitybijectivity limited form propagations follow wellspeciﬁed patterns For instance case bijectivity typical pattern occurs X 1 X possible values given function derived possible propagated value true visualized instance Fig 1 Hence implementation ensure performed ﬁrst Stated differently g f designed way gS1 f E S2 N S2 consists implicit constraint S1 Observation 2 Clues rarely propagate We observed automatically obtained logic representation clues usually weak unit propagation strength isolation This property clues ﬁnal obtained translation As example consider following sentence The person ordered capellini Damon Claudia From human reasoner conclude Angie order capellini However automatically obtained logical representation p person orderedp capellini p Damon p Claudia This logic sentence entails Angie order capellini conjunction bijectivity constraint ordered In natural language sentence bijectivity implicit use article The entails unique person ordered capellini We observed rarely propagation sole clues implicit constraints active clue time Because observation implementation logic grid puzzles decided consider subsets implicit constraints combination clue candidate sets S Line 3 Algorithm 3 Instead combine clue entire set implicit constraints subsequently counting nonredundance explanation subsetminimality core eliminate implicit constraints Observation 3 Clues typically independently clues A observation puzzles en countered human reasoners needed combine clues order derive new information propagations possible hard explain split derivations containing single clues The course guaranteed artiﬁcially devise disjunctive clues allow propagation Our algorithms built handle case turned necessary practice puzzles tested encountered explanation step combined multiple clues Observation 4 Previously derived facts easier use clues implicit constraints Our ﬁnal observation drove design cost functions previously derived facts easier extra clue implicit constraint This fact previously derived facts simple nature implicit constraints contain quantiﬁcation harder grasp An additional reason perceived simplicity derived facts visualized grid A cost function With observations mind devised f g follows ncS denotes number clues implicit constraints S M N explanation parameters f E S N basecostS E N S gS basecostS S 1 ncS 0 S 1 ncS 0 0 M M ncS The parameter M taken larger reasonable explanation size E N S N relatively small comparison slightly larger facts E In experiments use combination M 100 N 5 provided good explanation sequences tested puzzles The effect generate subsets S Line 3 Algorithm 3 following order First S containing exactly implicit constraint Next S containing exactly implicit constraints optionally exactly clue Finally clue pairs triples practice reached Summarized instantiation logic grid puzzles differs generic methods developed previous section uses domainspeciﬁc optimization function f consider S Line 3 promising candidates based observations 13 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 For complete nonredundant explanation sequence tool produces running example scoring functions refer httpbartbog github io zebra pasta An example hardest derivation encountered cost 108 nested explanation depicted Fig 3 It uses bijectivity constraints uniqueness persons reasoning relation costs types pasta combination clue assumptions 8 Logic grid puzzles natural language clues typed ﬁrstorder logic The demo developed called ZebraTutor named Einsteins zebra puzzle integrated solu tion solving logic grid puzzles explaining humanunderstandable way solution obtained clues The input ZebraTutor plain English language representation clues names entities puzzle Angie arrabiata In typical logic grid puzzles entity names present grid supplied puzzle For puzzles entities named required know advance prototypical example Einsteins Zebra puzzle ends question Who owns zebra clues zebra entity puzzle solved knowledge fact zebra ﬁrst place The complete speciﬁcation undergoes following steps starting input A PartOfSpeech tagging A PartOfSpeech POS tag associated word outofthebox POS tagger 34 B Chunking lexicon building A problemspeciﬁc lexicon constructed C From chunked sentences logic Using custom grammar semantics logical representation clues constructed D From logic complete IDP speciﬁcation The logical representation translated IDP language augmented logicgridspeciﬁc information E Explanationproducing search IDP This main contribution paper explained Section 6 F Visualization The stepbystep explanation visualized The ﬁrst steps related Natural Language Processing NLP detailed section 81 Next explain section 82 IDP speciﬁcation formed step Step D generate explanations visualizations steps Step E Step F respectively An online demo httpbartbog github io zebra containing examples steps demo page 81 Natural Language Processing Step A POS tagging The standard procedure Natural Language Processing start tagging word estimated PartOf Speech tag POS tag We use standard English Penn treebank Pos tagset 34 NLTKs builtin perceptron tagger3 POS tagger It uses statistical inference mechanism trained standard training set Wall Street Journal Since POStagger mistakes manually verify correct assigned POStags sure puzzles entities tagged noun Step B Chunking lexicon building From natural language processing point view hardest step B automatically deriving lexicon building grammar The lexicon assigns role different sets words grammar set rules describing words combined sentences The goal second step group POStagged words clues chunks tagged lexical categories 3 puzzlespeciﬁc proper nouns individual entities central puzzle nouns groups entities like pasta sauce transitive verbs link entities Claudia choose puttanesca sauce The categories determiner number preposition auxiliary verb contain builtin list possible members We refer 35 details categories This process grouping words referred chunking We use NLTK custom set regular expressions chunking proper nouns different types transitive verbs The result lexicon word set words chunk assigned role based POS tags On roles deﬁned puzzleindependent grammar Blackburn Bos framework 3637 The grammar created based 10 example training puzzles tested 10 different puzzles ensure genericity 35 Step C From chunked sentences logic Next Step C lexicon partly problem agnostic partly puzzlespeciﬁc fed typeaware variant semantical framework Blackburn Bos 3637 translates clues Discourse Representation Theory 38 The 3 httpwwwnltkorg 14 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 typed extension allows discover case different verbs synonyms inherent relation types ateperson pasta orderedperson pasta In semiautomated method suggests lexicon lets user modify approve compensate possible creativity puzzle designers tend insert ambiguous words use implicit background knowledge morning time slot 1200 82 From logic visual explanations Once types built generate complete speciﬁcation reasoner IDP 32 solve problem Step D From logic complete IDP speciﬁcation From types built Step C construct IDP vocabulary containing types relation tran sitive verb preposition For instance clues contain sentence Claudia choose puttanesca sauce vocabulary contain binary relation chose ﬁrst argument type person second argument type sauce After vocabulary construct IDP theories translate clue IDP language add implicit constraints present logic grid puzzles The implicit constraints stemming translation clues translation generate multiple relations types For instance clues The person ate taglioni paid Angie The person ordered farfalle chose arrabiata sauce translation create relations ate ordered persons pasta However know relation types add theory containing synonymy axioms case concretely x person y pasta atex y orderedx y Similarly relations inverse signature represent inversion functions instance liked_by ordered clues Taglioni liked Elisa Damon ordered capellini In case add constraints form x person y pasta liked_by y x orderedx y Next refer end section 3 examples bijectivity transitivity axioms link different relations The underlying solver IDP 32 uses formal representation clues solve puzzle explain solution We chose IDP underlying solver natively offers different inference methods applied logic theories including model expansion searching solutions different types propagation optimalpropagate ﬁnd maxI T unsatcore extraction offers Lua 39 interface glue inference steps seamlessly 32 9 Experiments Using logic grid puzzles usecase validate feasibility ﬁnding nonredundant explanation sequences generating nested explanation sequences As data use puzzles Puzzle Barons Logic Puzzles Volume 3 40 The ﬁrst 10 puzzles construct grammar 10 test genericity grammar Our experiments test puzzles report results pasta puzzle sent manage solve As constraint solving engine use IDP 32 variety inference methods supports natively The algorithms written embedded LUA provides imperative environment inside declarative IDP The code optimized eﬃciency point interactive setting takes 15 minutes hours fully explain logic grid puzzle Experiments run IntelR XeonR CPU E31225 4 cores 32 Gb memory running linux 4150 IDP 371 The code generating explanations puzzles experiments available 41 91 Sequence composition We ﬁrst investigate properties puzzles composition resulting sequence explanations The results shown Table 1 The ﬁrst column puzzle identiﬁer puzzle identiﬁed p pasta puzzle running example The 3 columns properties puzzle t ype number types person sauce dom number entities type grid number cells grid number literals maximal consequence interpretation In max T Coincidentally puzzles 4 types domain size 5 150 cells pasta puzzle domain size 4 96 cells 15 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Table 1 Properties puzzles explanation sequences constraints explanations p 1 2 3 4 5 6 7 8 9 p types dom grid steps cost 1 bij 1 trans 1 clue 1 cluei mult mult c 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 4 150 150 150 150 150 150 150 150 150 96 112 119 110 115 122 115 110 118 114 83 2706 2777 2393 2454 2497 2258 2679 2681 2475 3445 3125 2353 3273 2783 2459 2696 3545 3390 2895 3373 5000 5714 5182 5565 5902 5826 4636 4746 5439 4096 089 168 0 261 082 261 091 339 351 120 1785 1764 1546 1392 1558 1218 1727 1525 1316 2169 0 0 0 0 0 0 0 0 0 241 0 0 0 0 0 0 0 0 0 0 Table 2 Statistics number previously derived facts E explanation steps p 1 2 3 4 5 6 7 8 9 p average nr facts explanations clue use facts 184 185 184 189 185 184 188 186 185 173 Bij 237 250 233 250 250 235 231 258 245 207 Trans Clue multi 0 facts 1 facts 2 facts 3 facts 3 facts 200 200 200 200 200 200 200 200 200 200 052 061 024 047 035 029 075 018 032 053 400 6667 4783 8235 6842 6500 7647 5500 8182 7895 6842 2857 4783 1176 1579 3500 1765 2500 1818 1579 2105 0 0 588 1579 0 588 1000 0 0 0 0 435 0 0 0 0 1000 0 526 1053 476 0 0 0 0 0 0 0 0 0 Columns 5 6 steps steps explanation sequences cost average cost explanation step puzzle The number inference steps 110120 pasta puzzle related grid size The rest columns investigate proportion inference steps explanation sequence trivial steps bijection constraint 1 bij transitivity constraint 1 trans clue 1 clue constraints complex inference steps clue implicit bijectivity transitivity constraint 1 cluei multiple implicit constraints mult We observe 7 30 steps simple bijectivity steps com pleting row column relation 50 transitivity steps pasta puzzle 3 use single clue 7 The majority steps involving clues use complex combination clue constraints We method combine multiple clues explanations generated require combining multiple clues inference step mult c 0 7 method ﬁnd simpler steps involving clue Also notably puzzles booklet require combining implicit constraints harder pasta puzzle In general 15th explanations actually need use clue combination clue implicit constraints Note puzzle 3 possible ﬁnd new facts based clue information combined 1 multiple constraints 92 Sequence progression The left Fig 4 shows visualization type explanation explanation steps hardest puzzles 12 p puzzles highest average step cost We typically beginning sequence individual clues 3rd line individual bijectivity 1st line transitivity 2nd line constraints trivial ones This followed series clues involve bijectivitytransitivity constraints large fraction table completed bijectivitytransitivity followed clueimplicit constraint combinations round completion The exception pasta puzzle We 20 steps clues twice combination implicit logigram constraints derive new fact table easily completed bijectivitytransitivity twice use clue 93 Explanation size Our costfunction constructed favor clues constraints explanations small number previously derived facts E Table 2 ﬁrst 5 columns shows average number facts type constraints explanation We observe average number facts low column The breakdown type constraint shows bijectivity typically uses facts uses negative facts 16 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Fig 4 Sidebyside comparison puzzle composition left puzzle complexity nested explanation steps highlighted right row infer positive fact Fig 1 uses positive fact infer negative facts Note intuitive constraint number facts matter Transitivity nature uses previously derived facts When explanation involves clue facts involved average The rest columns closer look number facts explanation involves clue We approach successfully ﬁnds small explanations clues trivial ones use facts use 1 fact occasionally 2 facts needed The puzzles 1 2 9 pasta require use 3 facts clue corresponding puzzles highest amounts clues implicit constraints Only puzzle 1 requires clues combined 3 facts Even puzzle 9 lower cost fact uses 3 facts combined clue linked complex clue formulation Notably facts explanations requiring clues equal 0 half time means new information derived independently new facts Part clues 1 fact linked clue involving constraints clues combined bijectivity derive 3 new facts 1 fact Altogether clues 0 1 facts form 80 explanations clues facts 17 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Table 3 Properties nested explanations p 1 2 3 4 5 6 7 8 9 p steps steps nested cluei mi average steps nested expl Composition nested explanation 1 bij 1 trans 1 clue 1 cluei mult 112 119 110 115 122 115 110 118 114 83 1429 1429 727 1304 1311 1043 1545 932 1053 1687 100 100 100 100 100 100 100 100 100 100 100 294 265 200 404 229 256 329 236 350 307 3699 4828 5000 4026 4783 5000 3750 4375 3125 4844 3288 1034 0 2987 0 800 3269 938 2812 2031 1233 2069 0 2078 870 2000 1827 2500 2031 781 1781 2069 5000 909 4348 2200 1154 2188 2031 1719 0 0 0 0 0 0 0 0 0 625 94 Nested explanations We investigate explanation cost different steps explanation sequence ones ﬁnd nested explanation Fig 4 right shows puzzles explanation cost explanation step step indicated nested explanation indicated dot ﬁgure The ﬁrst observation draw sidebyside ﬁgures peaks nested explanations right overcome peaks left Simply nontrivial explanations able ﬁnd nested explanation provide detailed explanation contradiction Secondly observe highest cost steps involve constraint clue implicit constraints combination implicit constraints pasta puzzle The harder pasta puzzle higher average cost reported Table 1 As explanation sequence progresses cost diﬃcult explanation steps increases meaning ﬁnd easy explanations beginning require complex reasoning ﬁnd new facts unlock simpler steps This especially visible trend line ﬁts puzzles nested explanation dots right Fig 4 We closer look composition nested explanations Table 3 When looking percentage reasoning steps nested explanation fraction steps nested explanation As ﬁgures indicated looking nontrivial steps method able generate nested sequence explains reasoning step contradiction When looking number steps nested sequence nested explanations small 2 4 steps The subsequent columns table composition nested explanations terms types constraints The majority simple steps involving single constraint single bijectivity transitivity clue Interestingly nested explanations typically explain 1 cluei step nested explanation step involving clue implicit constraint Detailed inspection showed typically ﬁnal step contradiction clue Fig 3 The composition nested sequence shows nested explanations simple decomposed second deeper level nested explanations This way cost function deﬁned promoting use simple constraints heavily penalizing combinations clues andor constraints We look cost different steps nested explanation use proxy diﬃculty steps Fig 5 displays violin plot ratio cost nested step divided cost original parent step contributing explain wider region indicates higher density steps having ratio Note Deﬁnition 10 step nested explanation costly costly parent step orange line comes close The ﬁgure shows steps cost fraction parent step near 010 original cost steps closer bit simpler parent step Due construction cost function Section 7 clue contributing value 100 fact value 1 means typically involves fewer previously derived facts easier understand 10 Discussion future work conclusions In paper formally deﬁned problem stepwise explanation generation satisfaction problems presenting generic algorithm solving problem We extended mechanism nested way explain explanation We developed algorithm context logic grid puzzles start natural language clues provide humanfriendly explanation form visualization When investigating nested explanation Fig 2 produced observe explanation entirely match explanation produced human reasoner couple reasons 18 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Fig 5 Distribution ratio cost nested explanation sequence step cost original explanation step puzzles In generation nested explanation highlevel sequence greedy algorithm section 6 While high level yields good results nested level results propagating facts The ﬁrst propagation nested explanation kind While easy ﬁx postprocessing generated explanation left example highlight difference nested nonnested explanation It happens ﬁnds minimal explanation X 1 negative facts instead corresponding single positive fact This seen step For human reasoners positive facts easier grasp A preference negative facts incidentally formulation clues incidentally happen way MUS computed subset minimality guaranteed In general observations kind taken account devising cost function A observation visible current ﬁgure generated nested explanations unnecessarily hard In cases encountered case explanation set implicit constraints contains lot redundant information small number suﬃcient imply Our cost function subsetminimality generated MUS entails explanation single step implicit constraints included follow included implicit constraints However generating nested explanations actually preferred redundant constraints allow breaking explanation simpler parts giving simple step single bijectivity complex step uses combination multiple implicit constraints These observations suggest research question constitutes understandable explanation humans needed respect appropriate interpretability metrics 4243 Additional directions produce easier tounderstand explanations optimize sequence individual steps ii learn cost function based user traces Additionally puzzles nested explanations simple reﬁnement needed For diﬃcult problems longer case possible avenue future work generic approach multi level nested explanations Another interesting direction research nesting related existing notions abstraction reﬁnement 4446 With respect eﬃciency main bottleneck current algorithm calls MUS hard problem For reason time submission acceptance paper investigated methods perform unsatisﬁable subset optimization 47 building existing algorithms searching cardinalityminimal MUSs 48 From systems point view direction future work approach near realtime essentially allowing ZebraTutor called user solving puzzle This especially important implement ideas critical domains interactive conﬁguration human search engine cooperate solve conﬁguration problem human interested understanding certain derivations 4950 Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper 19 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 Acknowledgements This research received funding Flemish Government Onderzoeksprogramma Artiﬁciële Intelligentie AI Vlaanderen programme FWO Flanders project G070521N We thank Jens Claes master thesis supervisor Marc Denecker implementation typed extension Blackburn Bos framework masters thesis Rocsildes Canoy help NLP aspect information pipeline References 1 MT Ribeiro S Singh C Guestrin Why I trust Explaining predictions classiﬁer Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining 2016 pp 11351144 2 SM Lundberg SI Lee A uniﬁed approach interpreting model predictions Advances Neural Information Processing Systems 2017 3 RR Selvaraju M Cogswell A Das R Vedantam D Parikh D Batra Gradcam visual explanations deep networks gradientbased localization 4 A Adadi M Berrada Peeking inside blackbox survey explainable artiﬁcial intelligence XAI IEEE Access 6 2018 5213852160 https 5 R Guidotti A Monreale S Ruggieri F Turini F Giannotti D Pedreschi A survey methods explaining black box models ACM Comput Surv pp 47654774 Proceedings ICCV 2017 pp 618626 doi org 10 1109 access 2018 2870052 51 5 2018 142 6 AB Arrieta N DíazRodríguez JD Ser A Bennetot S Tabik A Barbado S Garcia S GilLopez D Molina R Benjamins R Chatila F Herrera Explainable artiﬁcial intelligence XAI concepts taxonomies opportunities challenges responsible AI Inf Fusion 58 2020 82115 7 F Rossi P van Beek T Walsh Eds Handbook Constraint Programming Foundations Artiﬁcial Intelligence vol 2 Elsevier 2006 httpwww sciencedirect com science bookseries 15746526 2 8 U Junker Quickxplain conﬂict detection arbitrary constraint propagation algorithms IJCAI01 Workshop Modelling Solving Problems 9 T Feydy PJ Stuckey Lazy clause generation reengineered International Conference Principles Practice Constraint Programming Springer Constraints 2001 2009 pp 352366 10 J MarquesSilva I Lynce S Malik Conﬂictdriven clause learning sat solvers Handbook Satisﬁability Ios Press 2009 pp 131153 11 LH Gilpin D Bau BZ Yuan A Bajwa M Specter L Kagal Explaining explanations overview interpretability machine learning F Bonchi FJ Provost T EliassiRad W Wang C Cattuto R Ghani Eds 5th IEEE International Conference Data Science Advanced Analytics DSAA 2018 Turin Italy October 13 2018 IEEE 2018 pp 8089 12 A Felfernig L Hotz C Bagley J Tiihonen Knowledgebased conﬁguration research business cases 2014 13 J Claes B Bogaerts R Canoy T Guns Useroriented solving explaining natural language logic grid puzzles The Third Workshop Progress Towards Holy Grail 2019 2491 demo96 pdf 14 J Claes B Bogaerts R Canoy E Gamba T Guns Zebratutor explaining solve logic grid puzzles Beuls et al 51 httpceurws org Vol IEEE 2010 pp 914 15 B Bogaerts E Gamba J Claes T Guns Stepwise explanations constraint satisfaction problems 24th European Conference Artiﬁcial Intelli gence ECAI 2020 httpsdoi org 10 3233 FAIA200149 press 16 P Langley B Meadows M Sridharan D Choi Explainable agency intelligent autonomous systems TwentyNinth IAAI Conference 2017 17 K Leo G Tack Debugging unsatisﬁable constraint models International Conference AI OR Techniques Constraint Programming Combinatorial Optimization Problems Springer 2017 pp 7793 18 K Zeighami K Leo G Tack MG la Banda Towards semiautomatic learningbased model transformation Proceedings CP 2018 pp 403419 19 J MarquesSilva Minimal unsatisﬁability models algorithms applications 2010 40th IEEE International Symposium MultipleValued Logic 20 M Fox D Long D Magazzeni Explainable planning IJCAI17 workshop Explainable AI arXiv1709 10256 21 J Wittocx M Denecker M Bruynooghe Constraint propagation ﬁrstorder logic inductive deﬁnitions ACM Trans Comput Log 14 2013 22 EC Freuder Progress holy grail Constraints 23 2 2018 158171 23 MH Sqalli EC Freuder Inferencebased constraint satisfaction supports explanation AAAIIAAI vol 1 1996 pp 318325 24 G Escamocher B OSullivan Solving logic grid puzzles algorithm imitates human behavior arXiv preprint arXiv1910 06636 25 M Ganesalingam WT Gowers A fully automatic theorem prover humanstyle output J Autom Reason 58 2 2017 253291 httpsdoi org 10 1007 s10817 016 9377 1 26 K Yang J Deng Learning prove theorems interacting proof assistants K Chaudhuri R Salakhutdinov Eds Proceedings 36th International Conference Machine Learning ICML 2019 915 June 2019 Long Beach California USA Proceedings Machine Learning Research PMLR vol 97 2019 pp 69846994 httpproceedings mlrpress v97 yang19a html 27 A Caine R Cohen Mits mixedinitiative intelligent tutoring sudoku Conference Canadian Society Computational Studies Intelligence Springer 2006 pp 550561 28 F Rossi P Van Beek T Walsh Handbook Constraint Programming Elsevier 2006 29 H Enderton HB Enderton A Mathematical Introduction Logic Elsevier 2001 30 DG Mitchell E Ternovska F Hach R Mohebali Model expansion framework modelling solving search problems Tech Rep TR 200624 Simon Fraser University Canada 2006 31 M Gebser B Kaufmann T Schaub The conﬂictdriven answer set solver clasp progress report E Erdem F Lin T Schaub Eds Logic Programming Nonmonotonic Reasoning 10th International Conference LPNMR 2009 Potsdam Germany September 1418 2009 Proceedings Lecture Notes Computer Science vol 5753 Springer 2009 pp 509514 32 BD Cat B Bogaerts M Bruynooghe G Janssens M Denecker Predicate logic modeling language IDP M Kifer YA Liu Eds Declarative Logic Programming Theory Systems Applications ACM Morgan Claypool 2018 pp 279323 33 A Kolokolova Y Liu DG Mitchell E Ternovska On complexity model expansion CG Fermüller A Voronkov Eds Logic Programming Artiﬁcial Intelligence Reasoning 17th International Conference LPAR17 Yogyakarta Indonesia October 1015 2010 Proceedings Lecture Notes Computer Science Springer 2010 pp 447458 34 MP Marcus B Santorini MA Marcinkiewicz Building large annotated corpus English penn treebank Comput Linguist 19 2 1993 313330 35 J Claes Automatic translation logic grid puzzles typed logic Masters thesis KU Leuven Leuven Belgium June 2017 36 P Blackburn J Bos Representation Inference Natural Language A First Course Computational Semantics Volume 1 ISBN 9781575867731 2005 httpwwwcoli uni saarland publikationen softcopies Blackburn 1997RINpdf 20 B Bogaerts E Gamba T Guns Artiﬁcial Intelligence 300 2021 103550 37 P Blackburn J Bos Working discourse representation theory Advanced Course Computational Semantics 38 H Kamp Discourse representation theory ought A Blaser Ed Natural Language Computer Scientiﬁc Symposium Syntax Semantics Text Processing ManMachineCommunication Heidelberg FRG February 25 1988 Proceedings Lecture Notes Computer Science vol 320 Springer 1988 pp 84111 39 R Ierusalimschy LH De Figueiredo WC Filho Luaan extensible extension language Softw Pract Exp 26 6 1996 635652 40 S Ryder Puzzle Barons Logic Puzzles Alpha Books Indianapolis Indiana 2016 41 G Emilio B Bart G Tias C Jens A framework stepwise explaining solve constraint satisfaction problems Jun 2021 httpsdoi org 10 42 RR Hoffman ST Mueller G Klein J Litman Metrics explainable ai challenges prospects arXiv preprint arXiv1812 04608 43 A Rosenfeld Better metrics evaluating explainable artiﬁcial intelligence Proceedings 20th International Conference Autonomous Agents MultiAgent Systems 2021 pp 4550 44 M Leuschel M Butler Automatic reﬁnement checking b International Conference Formal Engineering Methods Springer 2005 pp 345359 45 ZG Saribatur P Schüller T Eiter Abstraction nonground answer set programs European Conference Logics Artiﬁcial Intelligence Springer 46 DG Mitchell E Ternovska Expressive power abstraction essence Constraints 13 3 2008 343384 47 G Emilio B Bart G Tias Eﬃciently explaining csps unsatisﬁable subset optimization Proceedings IJCAI 2021 48 A Ignatiev A Previti M Liﬃton J MarquesSilva Smallest mus extraction minimal hitting set dualization Proceedings CP Springer 2015 5281 zenodo 4982025 2019 pp 576592 pp 173182 49 PV Hertum I Dasseville G Janssens M Denecker The KB paradigm application interactive conﬁguration Theory Pract Log Program 17 1 2017 91117 httpsdoi org 10 1017 S1471068416000156 50 P Carbonnelle B Aerts M Deryck J Vennekens M Denecker An interactive consultant Beuls et al 51 httpceurws org Vol 2491 demo45 pdf 51 K Beuls B Bogaerts G Bontempi P Geurts N Harley B Lebichot T Lenaerts G Louppe PV Eecke Eds Proceedings 31st Benelux Conference Artiﬁcial Intelligence BNAIC 2019 28th Belgian Dutch Conference Machine Learning Benelearn 2019 Brussels Belgium November 68 2019 CEUR Workshop Proceedings vol 2491 2019 CEURWS org 2019 httpceurws org Vol 2491 21