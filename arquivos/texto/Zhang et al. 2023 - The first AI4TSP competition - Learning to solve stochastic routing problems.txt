Artiﬁcial Intelligence 319 2023 103918 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint The ﬁrst AI4TSP competition Learning solve stochastic routing problems Yingqian Zhang a1 Laurens Bliek a1 Paulo da Costa a1 Reza Refaei Afshar a1 Robbert Reijnen a1 Tom Catshoek b1 Daniël Vos b1 Sicco Verwer b1 Fynn SchmittUlms c2 André Hottung d2 Tapan Shah e2 Meinolf Sellmann f2 Kevin Tierney d2 Carl PerreaultLaﬂeur g2 Caroline Leboeuf g2 Federico Bobbio g2 Justine Pepin g2 Warley Almeida Silva g2 Ricardo Gama h2 Hugo L Fernandes i2 Martin Zaefferer l2 Manuel LópezIbáñez j2 Ekhine Irurozki k2 Eindhoven University Technology Netherlands b Delft University Technology Netherlands c McGill University Canada d Bielefeld University Germany e General Electric USA f InsideOpt USA g Université Montréal Canada h Polytechnic Institute Viseu Portugal Rockets Awesome New York City USA j University Manchester UK k Telecom Paris France l DHBW Ravensburg Germany r t c l e n f o b s t r c t Article history Received 25 January 2022 Received revised form 31 October 2022 Accepted 31 March 2023 Available online 3 April 2023 Keywords AI TSP competition Travelling salesman problem Routing problem Stochastic combinatorial optimization Surrogatebased optimization Deep reinforcement learning This paper reports ﬁrst international competition AI traveling salesman problem TSP International Joint Conference Artiﬁcial Intelligence 2021 IJCAI 21 The TSP classical combinatorial optimization problems variants inspired realworld applications This ﬁrst competition asked participants develop algorithms solve orienteering problem stochastic weights time windows OPSWTW It focused learning approaches surrogatebased optimization deep reinforcement learning In paper problem competition setup winning methods overview results The winning methods described work advanced stateoftheart AI stochastic routing problems Overall organizing competition introduced routing problems interesting problem setting AI researchers The simulator problem opensource researchers benchmark new This paper submitted Competition Section journal Corresponding author Email address yqzhangtuenl Y Zhang 1 The organization team 2 The winning teams httpsdoiorg101016jartint2023103918 00043702 2023 The Authors Published Elsevier BV This open access article CC BY license http creativecommons org licenses 4 0 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 learningbased methods The instances code competition available httpsgithub com paulorocosta ai fortsp competition 2023 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommons org licenses 4 0 1 Introduction Many realworld optimization problems combinatorial optimization problems COPs objective ﬁnd op timal solution ﬁnite set possible solutions COPs proven NPComplete solving optimality computationally expensive impractical large instances COPs studied extensively search communities including discrete mathematics theoretical science operations research An eﬃcient way ﬁnding acceptable solutions COPs heuristic approaches The time complexity heuristics mainly poly nomial provide solutions far optimal Besides approaches redesigned problem assumption settings changed Recent years seen rapidly growing machine learning ML dynamically learn heuristics ﬁnd closetooptimal solutions COPs 1 Among COPs routing problems traveling salesman problem TSP wellknown emerge reallife applications The TSP variants include uncertainty making problem challenging traditional exact heuristic algorithms TSP variants wellstudied COPs ML literature Previous works deep neural network approaches routing problems focused learning construct good tours 213 learning search good solu tions 1427 leveraging supervised deep reinforcement learning DRL Other approaches considered surrogatebased optimization SBO 2833 ML models guide search good tours In competition participants solve variant TSP ML methods The selected variant TSP contains stochastic weights cost traveling nodes stochastic Each node prize collecting prize depends arrival time agent These assumptions variant TSP similar reallife problems For example real life required time travel city depends road construction work traﬃc jams Moreover visiting location usually assigned time bounds respected To solve problem variant participants use ML methods SBO DRL Both methods shown considerable promise generating solutions routing problems previous works We emphasize primary goal competition bring new surrogatebased DRLbased approaches practice solving diﬃcult variant TSP This attracting ML researchers challenging solve diﬃcult routing problem The solutions built existing work adapted particular TSP variant Although previous work focused prize collecting orienteering problems stochastic weights researchers combination assumptions account This motivates establish platform provides opportunity AI researchers develop SBO DRL approaches solving wellknown routing problem As byproduct competition provides winning methods simulator generating problem instances researchers use benchmark MLbased approaches In summary objective organizing competition threefold 1 introduce routing problems interesting problem setting ML researchers 2 advance stateoftheart ML routing problems 3 provide challenging problem simulator researchers benchmark MLbased approaches We divide competition tracks requiring different knowledge subﬁelds AI Track 1 SBO Given instance previously tried tours total reward sum prizes collected tour tours goal learn model predicting reward new tour Then optimizer ﬁnds tour gives best reward according model tour evaluated giving new data point Then model updated iterative procedure continues ﬁxed number steps Over time model accurate giving better better tours This procedure SBO algorithms Bayesian optimization 34 Track 2 DRL We consider environment simulator generate set multiple instances I following generating distribution We expect output partial solutions containing order nodes visited The environment returns general instance features stochastic travel time traversing edge given solution The goal maximize prizes collected respecting timerelated constraints multiple samples selected test instances This procedure related neural combinatorial optimization 4 The ﬁrst competition named AI4TSP competition IJCAI21 International Joint Conference Artiﬁcial Intel ligence competition It ran May 27 July 12 2021 organized Delft University Technology Eindhoven University Technology By deadline ﬁnal test phase received submissions SBO track submissions DRL track The submissions tested 1 000 problem instances 200 nodes winners determined ranking total quality solutions The results competition oﬃcially announced Data Science Meets Optimization DSO workshop colocated IJCAI21 2 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 2 Problem description methodology Both tracks look orienteering problem stochastic weights time windows OPSWTW simpliﬁed version timedependent OPSWTW 35 This problem similar traveling salesman problem TSP nodes need visited respecting maximum tour time opening closing times nodes order maximize measure rewards We problem section 21 OPSWTW In TSP goal ﬁnd tour smallest cost visits locations customers network exactly However practical applications rarely knows travel costs locations precisely Moreover speciﬁc time windows customers need served certain customers valuable Lastly salesman constrained maximum capacity travel time representing limiting factor number nodes visited In competition consider realistic version classical TSP OPSWTW 35 In formulation stochastic travel times locations revealed salesman travels network The salesman starts depot return depot end tour Moreover node customer network assigned prize representing important visit given customer given tour Each node associated time windows We consider salesman arrive earlier node compromising prize salesman wait opening time serve customer Lastly tour violate total travel time budget collecting prizes network The goal collect prizes network respecting time windows total travel time tour allowed salesman Node locations More formally OPSWTW problem instance deﬁned set n nodes complete graph The x y coordinates nodes 2D space randomly generated integers limits For node limits input parameters having default values lx 0 200 l y 0 50 xi yi respectively Travel times The noisy travel times ti j R j 1 n node j obtained ﬁrst computing Euclidean distance di j rounded closest integer Later distance multiplied noise term η following discrete η uniform distribution U 1 100 normalized scaling factor β 100 τi j di j β ti j samples τi j Hence travel time j different different samples Unlike problem described 35 noise term timedependent Time windows Each node time window denoted lower bound li Nn i1 The time windows generated times begin service node second nearest neighbor TSP tour In let d2nn time visiting ith node second nearest neighbor tour assuming maximum travel times nodes The left time window randomly generated number d2nn w w In arbitrary integer Similarly right time window random number d2nn problem setting w 20 40 60 80 100 36 i1 upper bound hi Nn w d2nn d2nn Prizes Each node associated prize The prize pi Rn i1 node describes importance visiting node time window The prizes increasing distance node ﬁrst node tour depot In prize node determined according rounded L2 distances nodes depot That pi 100 d1i euclidean distance maximum travel time depot node This prize structure results challenging instances places nodes higher prizes away depot 37 d1i j1 d1 j 99 1 maxn cid4cid5 cid2 cid3 j1 d1 j T max max2 T min cid5 1 Constraints Each problem instance maximum tour length T determines maximum time allowed spent tour For instance sample max tour length T discrete uniform distribution U T min T max T min 2 maxn 2 dnncid6 dnn tour cost nearest neighbor TSP solution maximum travel times Note T min deﬁned possible depot farthest node T max deﬁned maximum twice T min time half nearest neighbor TSP tour cost ensure instances challenging feasible excessively large values T Moreover solutions respect time windows node li hi That tour arrives earlier opening time node wait opening time depart node A tour considered infeasible arrival time higher closing time node Note unlike case possible collect prizes node arriving earlier opening time time window 3 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Fig 1 An instance OPSWTW Penalties We treat violation constraints problem form penalties ei Rn i1 All solutions tours longer T penalized ei n incurred node violation ﬁrst occurred Moreover time window violation incurs penalty ei 1 current node violation occurred The stochastic travel times timedependent constraints prizes problem diﬃcult solve traditional solvers In implementation problem instance complete graph particular depot node1 The nodes ﬁxed 2D space travel times noisy vary different runs The goal problem ﬁnd tour total reward maximized Note prizes penalties node coordinates time window bounds maximum allowed tour time T known given instance To summarize main differences problem competition TSP Not nodes need visited allowed visit nodes Visiting node nodes opening time closing time gives prize Visiting node closing time gives penalty When visiting node opening time agent wait node opens The time takes travel node stochastic The travel times directly appear objective function The metric matters sum collected prizes penalties Fig 1 shows example node visitation decision policy visiting n 6 nodes In ﬁgure tour visited nodes 1 depot 6 travel time t16 revealed visiting node 6 At current decision moment need choose node visit The decision consider prizes node time windows total remaining travel time selecting node case node 3 selected Moreover salesman decides arrive node earlier earliest service time li travel time gets shifted beginning time window For example travel time depot node 1 node 6 lower l6 salesman wait l6 depart node This information available soon salesman arrives node 6 Lastly tour return depot travel time included maximum allowed tour time 22 Track 1 surrogatebased optimization The goal Track 1 solve optimization problem related instance OPSWTW problem ﬁnding tour maximizes total reward The total reward tour sum collected prizes penalties represented blackbox function f s I taking input instance I tour s The optimization problem denoted 4 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 arg max s E f s I s 1 given instance I We use expected value simulator stochastic different rewards tour evaluated multiple times The expected value tour s approximated evaluating f s I tour 10000 times calculating average total reward This computation takes multiple seconds standard hardware Therefore problem seen expensive optimization problem Surrogatebased optimization methods Bayesian optimization 34 approximate expensive objective online supervised learning known perform type problem The tour s indicates order visit nodes network It speciﬁc form s 1 s1 sn n number nodes s1 sn containing integers 1 n This means number 1 appear twice solution As number indicates starting node tour consists starting starting node visiting number nodes returning starting node point Any nodes appear tour returning starting node ignored SBO algorithms approximate blackbox function f iteration surrogate model g online learning problem optimize g instead optimization problem Both results learning optimization better simulator data available The problem typically split subproblems solved time tour given input simulator 1 Given tours tried corresponding rewards learn model predict promising new tour 2 Optimize model previous step suggest promising tour try Then tour given input simulator The ﬁrst step seen online learning problem new data comes iteration rewards need predicted It corresponds concept acquisition function Bayesian optimization In step 2 standard optimization methods gradient descent 221 Baseline As baseline provide implementation standard Bayesian optimization algorithm Gaussian pro cesses 34 For implementation use bayesianoptimization Python package 38 transforming input space approach 28 rounding solutions nearest integer The expectation baseline method perform problem combinatorial search space constraints possibility noisy lowcost approximations objective The competition participants requested develop new methods suitable problem existing baseline algorithms 23 Track 2 deep reinforcement learning In DRL track interested stochastic policy π mapping states action probabilities A policy OPSWTW selects node visited given sequence previously visited nodes To cope stochastic travel times policy adaptive Therefore policy needs consider instance information construct tours dynamically respect time windows nodes total tour time allowed instance Note unlike Track 1 interested general policies applicable instance OPSWTW training distribution More formally adopt standard Markov decision process MDP M cid7S A P rcid8 S state space A cid9s transition distribution taking action state s rs reward function Where action space Ps model state s partial complete tours action space remaining nodes visited rewards sum prizes penalties collected step Thus main objective ﬁnd policy π π arg max π EIPI cid6 cid7 Lπ I instance I sampled distribution PI Lπ I Eπ rsi ai cid10 cid8 n1cid9 i0 2 3 si state decision epoch example partial tour node si si s0 s1 si assuming eai tour start depot s0 1 Note reward taking action ai given rsi ai pai returned depot 0 231 Baseline We provide baseline RL track based neural combinatorial optimization 4 Note approach adaptive perform given task uses coordinates prizes decisions 5 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 complete tours Moreover tailored stochastic problems consider online travel times maximum tour budget information traversing tours Participants requested develop new methods better suited exploiting complete information instances including travel times revealed new location tour maximum travel budget While classical heuristic approaches use machine learning work simple problems expect new methods based DRL able outperform problem The reason DRL dynamically learn heuristics decide node visit based previously visited nodes classical approaches design decisions hand If data computation time learning available learning capabilities leveraged improve handtailored heuristics 3 Technical setup evaluation 31 Problem instances A set problem instances generated provided participants Each problem instance contains n nodes 2D space Each node xcoordinate ycoordinate lower upper bounds time window prizes maximum tour time The source code competition provided participants Thus access instances environments generated participants inspect components The implementation source code competition Python programming language In InstanceGenerator Algorithm 1 class generates features problem instances This class initialized number nodes limits x y coordinates w random seed The problem instances generated steps First coordinates nodes generated randomly intervals lx l y Note location nodes ﬁxed given input travel times nodes subject change according noisy values Algorithm 1 InstanceGenerator Require number nodes n limits lx l y time window size w 1 n xi U lx yi U l y end x x1 xn y y1 yn l h D TWGeneratorx y w p T PrizeGeneratorx y D I x y l h p T D return I Second time window node generated time visiting node second nearest neighbor tour TWGenerator class Algorithm 2 An instance class receives node coordinates OPSWTW instance value w computes L2 distances nodes returns time windows node based second nearest neighbor TSP tour Algorithm 2 TWGenerator Require node coordinates x x1 xn y y1 yn time window size w 1 n j 1 n di j L2xi yi x j y j end end D d11 dnn s2nn d2nn GetSecondNearestNeighborD t1 0 l1 0 h1 cid5maxn j1 d2nn j 2 n wcid6 j j s2nn li U max0 d2nn hi U max0 d2nn j j w d2nn d2nn j w j cid12 2nd NN tour time dist nodebynode end l l1 ln h h1 hn return l h D Third prize node determined according L2 distances nodes depot The prizes generated calling PrizeGenerator class Algorithm 3 This class takes input node coordinates 6 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Table 1 A sample problem instance 4 nodes CUSTNO XCOORD YCOORD TW_LOW TW_HIGH PRIZE MAX_T 1 2 3 4 47 38 53 116 24 15 49 23 0 102 9 30 285 198 52 137 00 019 038 10 256 256 256 256 L2 distance nodes maximum travel time outputs prizes based distance nodes depot That nodes farther depot larger prizes Moreover class generates maximum allowed tour budget T preventing tours obtaining big total prize Participants access L2 distance nodes corresponds maximum possible travel times nodes setup Algorithm 3 PrizeGenerator Require node coordinates x x1 xn y y1 yn maximum time matrix D dnn GetNearestNeighborCostD T min 2 maxn j1 d1 j T max max2T min cid5 1 T U T min T max 1 n cid2 2 dnncid6 cid4cid5 cid3 1 99 d1i j1 maxn 100 d1 j pi end p p1 pn return p T cid12 dnn total travel time NN tour As simple example assume 4 nodes instance time window prize maximum travel budget An illustration example shown Table 1 Each row table corresponds particular node columns deﬁned follows CUSTNO integer identiﬁer nodes XCOORD YCOORD xi yi coordinate node TW_LOW TW_HIGH li hi left sides right sides time window node PRIZE pi prize node Finally MAX_T T maximum travel budget A possible tour example 1 2 3 4 1 The total time tour 187 time visiting nodes 1 2 3 4 0 13 50 118 respectively At time 13 tour visits node 2 needs wait time 102 collect prize node If tour leaves node 2 collecting prize gets node 3 time window Therefore misses prize node 3 incurs penalty 1 Then node 4 time window collect prize Therefore total collected prize tour 019 32 Environments 321 Track 1 The environment Track 1 Env receives input instance information containing node coordinates time windows prizes maximum allowed travel time maximum travel times nodes The environment imple ments method check_solution takes input complete tour starting ending depot returning total reward tour time tour 322 Track 2 The environment Track 2 EnvRL serves similar purpose Track 1 Here participants interact environment nodebynode basis That method step expects single node input builds solution nodebynode In method returns total tour time travel time previous edge rewards penalties step feasibility indicator tour complete returned depot This allows participants consider dynamics problem considering sampled travel times constructing solution 33 Evaluation The participants evaluated generated instances Section 31 track competition The competition split phases validation test detailed In Track 1 teams given single instance containing n 55 nodes single problem containing Validation phase set nodes instance information Section 31 During phase participants allowed test methods instance cap number evaluations At end validation phase performance team evaluated 10000 Monte Carlo samples instance sampling different travel times new 7 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 sample Performance measured considering sum prizes penalties proposed tour sample averaged 10000 experiments Participants given random seed reproduce randomness experiments difference results performance algorithms For Track 2 participants given 1000 generated instances varying size In total 250 instances 20 50 100 200 nodes respectively The performance proposed algorithms evaluated 100 Monte Carlo samples instance averaged entire set instances samples 100000 simulations Similarly Track 1 performance measured sum prizes penalties evaluating single Monte Carlo sample averaged samples instances Note unlike Track 1 interested learning policy works varying number instances validation set Participants use validation set check performance policies utilizing instances speciﬁc random seed comparison approaches Test phase Only test phase determine winners competition This phase followed similar procedure validation phase In tracks participants week submit ﬁnal test scores following end validation phase In Track 1 new instance containing n 65 nodes generated evaluate ﬁnal performance Similarly Track 2 1000 instances generated fashion validation phase The procedure evaluating performance remained unchanged validation phase The instancegenerating distribution validation test phases cases Thus algorithm trained validation phase propose solutions instances test phase 331 Submissions In tracks participants asked submit output ﬁles containing result proposed method implementation code inspection organizing team In Track 1 submission ﬁle consisted single tour This tour compute average performance 10000 Monte Carlo samples In Track 2 participants supposed submit single ﬁle containing tours instance 1000 Monte Carlo sample 100 In total 100000 tours submitted evaluating performance This ﬁle compute overall performance submission In cases participants submit tours size n 1 visit depot determined end tour 332 Ranking submissions Ranking submissions proceeded follows First computed total prize penalties evaluated tour That given Monte Carlo sample j instance I tour score αs j I computed αs j I ncid9 cid6 1 i1 s j 0 s j 1 s j cid7 p e s j s j 4 n 65 Track 1 n 20 50 100 200 Track 2 1 indicator function takes value 1 predicate true 0 The makes sure stop calculating rewards tour returns starting depot After evaluating total number solutions average total performance obtained instances Monte Carlo samples track That ﬁnal score given score 1 mI cid9 mcid9 II j1 αs j I 5 I 1 m 10 000 Track 1 I 1 000 m 100 Track 2 Based scoring metric participating teams ranked descending order That teams higher performance scores ranked higher 4 Winning methods This section presents methods winning teams tracks competition Though organizers access code submitted participants methods presented section explained participants 41 Track 1 winners As threeway tie Track 1 Section 5 methods presented track Participants use surrogatebased optimization techniques optimize instance 65 nodes 411 The convexers The approach submitted signiﬁcant experimentation shaped observations 8 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 1 The expected performance true objective approximated samplingbased approximate model true probability space exact model approximation true probability space 2 A learning surrogate identify search regions search algorithm restarted 3 Within parallel restarted search approach slight differences true objective function approximation actually help diversifying parallel search efforts Based observations approach consists parallel portfolio 3940 restarted hyperparameterized 41 42 dialectic search 43 workers tuned genderbased genetic algorithm conﬁgurator GGA 4446 Our approach features research contributions including new application hyperparameterized dialectic search parallelization dialectic search surrogatebased restart method Parallel dialectic search Dialectic search iterative metaheuristic search procedure Given starting solution uses perturbation mechanism generate solution ﬁrst The space solutions searched path relinking In case OPSWTW perturbation operator select subset clients permute manner transition times zero time windows line form solution penalties Then synergize thesis antithesis path relink solutions Starting thesis permutation pick repositioned client place position prescribed antithesis swapping place client currently holds position We commit swap leads best solution value Then consider remaining clients currently placed accordance antithesis pick best swap antithesis reached Among permutations encountered pick best new thesis We shown 41 hyperparameterize dialectic search resulting search procedure learns oﬄine best solve given instance set Hyperparameterized dialectic search basically extends reactive search 47 proposing control hyperparameters search procedure learned models We use logistic regressions number parameters low input features 41 models The models learned algorithm conﬁgurator described Dialectic search highly parallelized worker performing independent dialectic search sharing information best solution Each parallel worker uses local selection 100 randomly sampled scenarios approximate true objective local search step When worker encounters solution improve best solution variable assignment evaluated exact model simplifying approximation true probability space Since workers use approximation provides shared ground accepting rejecting solution improving parallel workers Learning restart We use semisupervised learning approach forecasting quality achieved restarting search new starting point The unsupervised architecture consists autoencoder trained oﬄine encode reconstruct permutations respectively compressed latent space The supervised learned online aims forecast quality best solution restarting given permutation When worker decides restart determined hyperparameter quick local search latent space conducted ﬁnd promising starting point search This latent vector decoded resulting permutation restart search Training approach Our approach contains numerous parameters hyperparameters set advance applying method We ﬁrst train autoencoder parameters oﬄine data collected previous iterations dialectic search The loss function linear combination reconstruction loss KL divergence mean square error tween actual quality predicted quality predicted quality quadratic function encoder We use ADAM optimizer optimize loss function We separate training conﬁguration overall approach expensive couple parameters model hyperparameters dialectic search We note interesting avenue future work The dialectic search exposes hyperparameters described brieﬂy conﬁgured algorithm conﬁgurator We use GGA 46 conﬁgure hyperparameterized dialectic search subset instances provided ﬁrst phase competition Evaluated quality approach This approach solves test instance 90 seconds roughly 35 minutes raw compute time workers This eﬃciency allowed solve 1000 instances track 2 competition led test performance 100 scenarios instance prescribed competition 108106 expected value simpliﬁed probability space 1078 scenarios competition evaluation comparably friendly Note means best default tours sticking matter actual transit times evolve actually outperforms winning reinforcement solution Track 2 412 Margaridinhas The method proposed Margaridinhas Track 1 AI TSP competition main components 1 mixedinteger surrogate model 2 iterative approach 3 genetic algorithm The iterative approach improves surrogate model iterations according ﬁxed set parameter values outputs best route 9 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 end The genetic algorithm combines solutions output distinct calls iterative approach different sets parameter values ﬁnd better solutions 1 Surrogate model The surrogate model outputs route maximizes deterministic reward plus estimated penalty based previous simulations iterative approach Each node N nodes set N retrievable reward ri arc j j N estimated penalty pi j The decision variable xi j 0 1 equals 1 arc j route 0 For sake brevity let T set values xi ji jN describing route starts ends depot contain cycles subroutes Let MaxRouteSize parameter controlled externally iterative approach represents maximum route size The surrogate model MMaxRouteSize formulated follows MMaxRouteSize max xT subject r j pi j xi j xi j MaxRouteSize 6a 6b cid9 cid9 iN cid9 jN cid9 iN jN The objective function 6a maximizes deterministic reward route plus estimated penalty Constraint 6b bounds number arcs route maximum route size parameter MaxRouteSize Note set T enforce maximum duration T time windows nodes The surrogate model MMaxRouteSize learns arcs avoid building route better estimated penalties pi j objective function added cuts according previously visited solutions3 2 Iterative approach The iterative approach explores solution space problem according parameters ﬁnding distinct solutions accordingly The iterative approach parameters maximum number iterations K number simulations iteration M feasibility threshold FeasibilityThreshold gap threshold GapThreshold First iterative approach cuts nodes arcs surrogate model MMaxRouteSize visited feasible route nodes time window starting maximum duration T sets MaxRouteSize 2 At iteration approach solves surrogate model MMaxRouteSize obtain solution xk simulates associated route sk If route sk feasible FeasibilityThreshold percent M simulations approach stores cuts feasible solution xk feasible region Otherwise approach cuts infeasible structure given infeasible solution xk sequence arcs excluding return depot likely feasible If gap best solution far upper bound routes size limited MaxRouteSize GapThreshold approach increases MaxRouteSize 1 starts search larger routes The upper bound routes size limited MaxRouteSize calculated solving surrogate model MMaxRouteSize penalties pi j Next approach updates penalties pi j objective function according penalty route sk given M simulations The total penalty route sk equally divided arcs penalty pi j arc j simply average registered penalties The algorithm stops outputs best route best solution meets upper bound approach hits maximum number iterations K 3 Genetic algorithm A genetic algorithm GA local search policy improve set solutions distinct calls iterative approach We run GA generations 14 warmedup solutions output iterative approach scores ranging 983 1128 investigate neighborhood The iterative approach takes average 15 minutes ﬁnd warmedup solution Windows Intel Core i710510U CPU 180 GHz 8 processor 8 GB RAM memory The initialization step GA uses 14 warmedup solutions random solutions 25600 total Even surrogate solutions crucial GA output better solutions rapidly We ran 5 generations GA performing 100 evaluations new solution After evaluation generation reduce number solutions 200 parents picking winners tournament solutions past generations We use NonWrapping Ordered Crossover reproduction operator 48 preserves order nodes The GA managed successfully ﬁnd better solutions distinct calls iterative approach We number solutions scoring 1132 optimal solution Track 1 test instance running GA 2 hours previously described setup Fig 2 presents estimated probability node occurs certain position optimal solution These estimates based optimal solutions GA Note Fig 2 optimal solutions share exact set nodes order visit neighboring nodes swapped Further details method implemented GitHub4 3 In alternative surrogate model chosen random constructive initialization method The showed perform poorly competitively good compared surrogate model The advantages surrogate model lie fact overall pipeline surrogate model requires fewer iterations ﬁnd optimal solution performs better smaller populations robust stochasticity 4 httpsgithub com almeidawarley tsp competition 10 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Fig 2 Estimation node probability occurrence positions optimal solutions The ﬁrst visited node 32 The nodes visited 4th 7th places 47 5 41 49 413 ZLI The approach ZLI Team competition based assumption problem black box algorithm access number nodes n values returned objective function f By controlling number evaluations solution estimate expected objective value Eq 6a ﬁdelity estimation possible evaluate hundreds thousands solutions time limit competition Therefore point view classical Bayesian optimization problem considered expensive classical approaches based Gaussian Process regression GPR CEGO 49 feasible large number evaluation points However good estimation requires thousands evaluations important track estimated value level ﬁdelity progressively increase avoiding redundant evaluations Thus apply multiple techniques simultaneously address limitations 1 reducing dimensionality problem excluding infeasible nodes 2 caching expected objective value evaluated solutions ﬁdelity levels 3 training classiﬁer predict probability feasibility solution 4 applying selfadaptive blackbox evolutionary algorithm EA directly objective function increasing ﬁdelity 5 supporting EA surrogate model based GPR 6 enabling model deal evaluation points clustering approach As ﬁrst step reduce dimensionality problem evaluating possible tours visit exactly node 1 000 times If result infeasible 10 repeated evaluations respective node removed set available nodes In competition instance step reduced dimension n 65 n 37 Afterward stepwise increase ﬁdelity objective function increasing number repeated evaluations candidate tour order ﬁnd set reasonably good solutions For ﬁdelity level 1 10 100 run selfadaptive EA budget 10 000 evaluations The ﬁrst EA run starts random tours subsequent run starts population preceding lowerﬁdelity run The EA roughly follows Mixed Integer Evolution Strategy 50 Its selfadaption dynamically conﬁgures variation recombination mutation operator choices mutation rate The variation operators mutated randomly q expτ z probability p recombined choosing randomly parents The mutation rate mutated q learning rate τ z sample normal distribution zero mean unit variance The mutation rate recombined intermediate crossover The possible choices variation operators standard operators permutations recombination Cycle Order Position Alternating Position mutation Swap Insert We modiﬁed mutation operators avoid creating solutions visit nodes tested onenode tours dimensionality reduction avoid mutations occur inactive tour changing unvisited nodes The EA avoids reevaluating solutions level ﬁdelity maintaining cache memory expected values solution This cache memory shared EA runs The cache memory initial EA runs train Gradient Boosting Decision Tree Classiﬁer predict probability tour infeasible based rank representation tour The EA run fourth time ﬁdelity 100 budget 10 000 trained classiﬁer avoid evaluating solutions predicted probability feasible 04 11 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 A ﬁfth run EA ﬁdelity 100 budget 5 000 incorporates surrogate model Gaussian process regression model GPR aka Kriging At core GPR based kernel function captures similarity correlation To enable GPR model deal discrete sequence points use input points tours s s cid9 Levenshtein distance tours This distance cid9 dLx x exponential kernel kx x measure counts substitutions deletions insertions nodes required turn tour Only active tour considered distance calculation nodes actually visited cid9 cid9 expθdLx x Since surrogateassisted EA evaluate thousands solutions need able model large numbers points This usually challenge GPR models required computational effort increases signiﬁcantly To deal issue separate training data subsets clustering train GPR model cluster combine individual models ensemble weighted sum The ensemble prediction uses predicted uncertainty individual model weighting This approach clustered Kriging described Wang et al 51 To save computational effort model retrained new data 20 generations In EA generation surrogate model preﬁlter generated offspring removing worst 50 We run EA additional times The sixth run simply increases ﬁdelity 500 The seventh ﬁnal run EA uses probability predicted classiﬁer additional criterion prediction GPR model sort generated offspring instead simply discarding solutions predicted infeasible Since run EA starts population previous run seven EA runs seen different phases single overall run algorithm shared cache memory acting global archive best solutions retrain classiﬁer runs In step best 250 solutions cache memory contain best run EA evaluated ﬁdelity 10 000 evaluations best returned Experiments carried cluster 2 8core Intel Xeon E52650v2 260 GHz 64 GB RAM We launched multiple independent runs algorithm parallel assess robustness results run executed sequentially single core A complete run algorithm requires average 12 hours complete The code available Zenodo5 42 Track 2 winners In track participants use deep reinforcement learning methods ﬁnd optimal policies 1000 instances sizes ranging 20 200 nodes 421 RISE Our solution approach challenge consists components First use POMO reinforcement learning approach proposed Kwon et al 9 learn policy problem size Next use eﬃcient active search 52 ﬁnetune learned policies instance solved creating individualized policy instance test set Finally use MonteCarlo rollouts construct ﬁnal solutions In following components The code method trained models available online6 POMO We use POMO learn initial policy problem instance size The POMO approach endtoend deep reinforcement learning approach combinatorial optimization problems POMO based REINFORCE algorithm exploits symmetries combinatorial optimization problems encourage exploration learning The network architecture employed model based transformer architecture consists encoder decoder neural network The implementation POMO available authors POMO solve multiple instances batch perform multiple rollouts instance parallel We slightly adjust POMO implementation support OPSWTW We change input POMO expects node xi yi pi li hi All values scaled input POMOs deep neural network We scale xi yi coordinates based 2Dspace limits pi based maximum prize instance li hi based given maximum tour time T Furthermore change decoder context include current time Ls scaled T embedding current node embedding depot Finally adjust masking schema POMO forbid actions correspond traveling node li T hi Ls previously visited nodes We train separate policy model considered problem sizes The training set generated provided instance generator Each model trained days convergence single Tesla V100 GPU For larger instances n 100 n 200 use transfer learning start training policy model trained n 50 Eﬃcient active search Eﬃcient active search EAS 52 method uses reinforcement learning ﬁnetune policy single test instance In contrast original active search 4 adjusts small subset model parameters 5 doi 105281zenodo7015507 6 httpsgithub com FynnSu AI _for _TSP 12 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 This allows solve multiple instances parallel resulting signiﬁcantly reduced runtime For test instance create copy corresponding problemsize speciﬁc policy learned POMO ﬁnetune subset policy parameters speciﬁc instance During process generate 18 million solutions instance takes 30 minutes GPU larger instances 200 nodes The result set 1000 separate policies Note travel times nodes ﬁxed training process Instead travel times sampled solution construction process This enables EAS learn robust policy create highquality solutions wide range scenarios If use ﬁnetuned policies construct solutions greedily observe average reward 1067 test set In contrast sizespeciﬁc models solution construction results reward 1043 MonteCarlo rollouts We construct solutions test instances MonteCarlo rollouts instancespeciﬁc policies learned EAS We construct solution test instance follows At decision step ﬁrst use policy model generate probability distribution possible actions possible nodes visited For ﬁve actions highest associated probability values perform 600 MonteCarlo rollouts Each MonteCarlo rollout starts corresponding actions completes solution sampling following actions according learned policy model Once MonteCarlo rollouts ﬁnished action highest average reward selected Note ﬁnal action chosen actual travel times nodes revealed For MonteCarlo rollouts travel times sampled independently preceding step Using MonteCarlo rollouts increases average reward 1077 1067 obtained greedy solution EASbased policies 422 Ratel The approach team Ratel relies version Pointer Networks PN designed tackle problems dynamic timedependent constraints particular Orienteering Problem Time Windows 13 While model shares basic structure previous PNs 45 set encoding block encodes node sequence encoding block encodes constructed sequence far pointing mechanism block architecture differs previous PNs mainly introduces recurrence node encoding step This recurrence makes encoding decoding steps carried sequentially step solution construction process This aspect brings great advantages especially solving problems dynamic constraints allows use masked selfattention lookaheadinduced graph structure turn allows updated representation admissible node step 13 Input features Since model recursive step determines feature vector associated admissible node These feature vectors given input set encoder order compute stepdependent representation node This feature vector combination static features remain constant solution construc tion process dynamic features change step For problem model uses 11 static features 34 dynamic features The static features obtained directly instance data Concretely Euclidean coordinates node xi yi opening closing time li hi time window width hi li prize pi maximum time available max length T prize divided time window width prize divided distance depot difference maximum time available opening closing times T li T hi As dynamic features model uses 34 features functions current time current node Some dynamic features boolean indicate nodes feasibility conditions satisﬁed assuming fastest travel times possible worst travel times Some examples nonboolean dynamic features node time left opening time time left closing time fraction time elapsed tour start prize divided max time arrive node prize divided time closing time probability arriving closing time probability arriving maximum time available expected prize node Setup training In order speed training model development training sampling 804000 pre generated instances 4000 instances number nodes 10 210 nodes seeds ranging 1 4000 During training step model samples instance pregenerated set instances number nodes 10 210 seed 1 4 000 replacement Note seeds validation phase 12345 test phase 19120623 outside range considered training This avoidminimize leakage overﬁtting achieve realistic ﬁnal collected prizes The model trained 15 000 epochs method based REINFORCE algorithm 53 13 entropy regularization enhance exploration L2 regularization Adam optimizer In epoch 6 simulations instance performed composed 32 sample solutions equaling batch size 192 sampled solutions Further implementation details hyperparameter values GitHub7 7 httpsgithub com mustelideos td opswtwcompetition rl 13 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Inference During evaluation solutions constructed greedy strategy time step select node model gives highest probability All experiments performed 6 core CPU 17 GHz Intel Xeon CPU E52603 v4 12 GB RAM Nvidia GeForce GTX 1080 Ti GPU The ﬁnal model took 48 hours train 12 hours generate validationtest submission ﬁles 5 Results discussion This section presents results participating teams validation phase ﬁnal test phase In addition implemented heuristic benchmark method compare performance winning methods 51 Benchmark heuristic We implemented classical heuristic baseline tracks use machine learning Adaptive Large Neighborhood Search ALNS 54 ALNS shown good performance solving orienteering problems literature 5556 This heuristic based ruinandcreate principle Large Neighborhood Search LNS solutions gradually improved continuous destroy repair operators ALNS extends allowing multiple destroy repair operators search For destroy repair operators assigned weight determine operators selected iteration algorithmic search These weights dynamically adjusted based operator performances successful operators given higher weights likely selected iteration search We deﬁned destroy operators based random remove random edge remove In random remove n customers removed uniformly solution random edge remove sequence customers removed For destroy opera tors deﬁned alternatives modest severe variant removing 0 25 20 40 customers solution Three repair operators deﬁned reinsert customers destroyed solution random distance repair random price repair random ratio repair In random distance repair randomly select number customers insert solution expensive position terms distance Random prize repair random ratio repair work according principle insert nodes sequentially position total accumulated rewards maximized ratio reward additional distance traveled optimized Simulated Annealing selected acceptance criterion search starting temperature 1 linear decay step 001 cooling bounded 025 We use original scoring function ALNS update weights convex combinations current weights score obtained current candidate solution Search performed 100 iterations regardless instance size This baseline implemented competition available participants The code implemented ALNS available online For track 1 participants manage develop SBO approach tailored problem hand dealing constrained combinatorial search space expectation outperform classical heuristic approaches use machine learning ALNS The reason surrogate model deals aspects problem time 1 expensive objective 2 stochasticity By replacing objective deterministic cheap compute surrogate aspects problem circumvented Such method need evaluate expensive objective better suited low number evaluations traditional heuristic approaches 34 For track 2 expect new methods based DRL able outperform ALNS problem The reason works proven DRL ﬁnd better heuristics learning 1415 classical heuristics decisions based handcrafted rules If data computation time learning available learning capabilities leveraged improve handcrafted heuristics 52 Results Table 2 shows score deadline phase Only test phase relevant deciding winners competition As seen teams participated validation phase seven test phase disregarding benchmark heuristic ALNS The leaderboard results visible participants updated time participating team submitted solution During validation phase saw large improvement scores indicating teams actively improving methods It seen end test phase way tie participants Track 1 Although case beginning phase test phase participants managed ﬁnd globally optimal solution instance consideration The prize money ﬁrst second place Track 1 split teams ZLI Margaridinhas Convexers prize money ﬁrst second place Track 2 given teams RISEup Ratel respectively Comparison ALNS To analyze performance learningbased solutions compared perfor mances winning solutions tracks ALNS baseline solution For Track 1 seen Table 2 ALNS ﬁnd globally optimal solution come close results teams outperform nonwinning teams This indicates beneﬁcial apply learningbased solutions diﬃcult routing 14 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Table 2 Leaderboard validation phase ﬁnal test phase deadline The results ALNS added competition Track 1 SBO Validation phase Track 1 SBO Test phase ZLI Convexers Margaridinhas Topline VK 8404499999999793 78099999999988885 7533834999999982 3540000000000663 05300000000000102 Convexers Margaridinhas ZLI ALNS Topline 11320000000002786 11320000000002786 11320000000002786 691096999999927 4300000000000301 Track 2 DRL Validation phase Track 2 DRL Test phase Ratel ML TSP RISEup VK UniBw 1069104 982955 916567 669772 1314861 RISEup Ratel ML TSP ALNS 1077341 1058859 1039341 494231 Table 3 Validation comparison instance sizes solution method mean std min 25 50 75 max RISEup Ratel ALNS RISEup Ratel ALNS RISEup Ratel ALNS RISEup Ratel ALNS Instances 0250 20 nodes 541 532 475 028 079 126 351 119 148 544 548 430 Instances 251500 50 nodes 822 808 488 019 101 278 671 067 008 822 819 270 Instances 501750 100 nodes 1162 1139 479 010 116 333 1099 271 000 1160 1145 144 Instances 7511000 200 nodes 1787 1757 536 045 122 386 1430 737 000 1787 1754 126 546 550 525 824 823 589 1164 1155 574 1793 1772 627 546 551 553 826 825 688 1166 1164 730 1797 1784 830 547 553 574 827 831 831 1167 1171 982 1803 1807 1174 problems For Track 2 solved sets instances different sizes 250 instances 20 50 100 200 nodes respectively compared solutions RISEup Ratel The performances methods shown Table 3 comparing mean min max standard deviations quartiles 50 runs ALNS 100 iterations search solutions competition participants It observed learningbased approach performs better solving instances largest size best solutions ALNS algorithm perform better smallest instances This indicates beneﬁcial apply learningbased solutions diﬃcult routing problems classical heuristics suﬃcient easier problems To investigate claims selected diﬃcult solve solution largest set instances instance0853 explored ALNS baseline performs provided additional search budget For increased number iterations 100 250 The performances compared RISEup winning approach track 2 displayed Fig 3 These results indicate complicated routing problems competition learningbased approach positive impact larger search budget classical approach 53 Competition insights We obtained insights organizing competition First main goal advancing state oftheart ML methods routing problems achieved seven new surrogate reinforcement learning methods presented work winning participants methods having applied partic ipants Our goals attracting ML researchers solve challenging routing problem creating benchmark problem instances MLbased approaches achieved making successful competition eyes organizers What noticed having clear restrictions allowed competition stimulated creativity By limiting participants use SBO DRL approaches new interesting methods developed Another aspect competition beneﬁcial chat function confusion quickly cleared 15 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Fig 3 ALNS solution quality time instance0853 There points improvement future similar competitions It remains questionable providing simulator participants way ML competition Although samples random variables simulator known advance exact probability distributions retrieved looking code participants access This known organizers decided obfuscate information remained easy participants run provided simulator use ML Another point improvement diﬃculty problem especially Track 1 Because want slow solution evaluation process server noticed solutions validation phase improved decided instance size somewhat limited Over course test phase methods participants improved point methods managed ﬁnd globally optimal solution causing threeway tie Although shows power different methods purpose competition best organizers underestimate participants instead inclined diﬃcult instances instance proved diﬃcult classical heuristic This hopefully push creativity participants power methods Furthermore organizers provide working baseline methods recognized entry level competition high Therefore participants good knowledge optimization algorithms machine learning techniques To promote line research AI ML solving optimization problems attract participants beneﬁcial baseline methods provided beginning competition For future work consider points improvement addition winning methods generalized routing problems 6 Conclusion We reported ﬁrst international competition AI4TSP The participants asked solve orienteering prob lem stochastic weights time windows OPSWTW We described setup tracks focusing learning techniques AI surrogatebased optimization deep reinforcement learning We described approaches winning teams tracks gave overview results Furthermore explained developed simulation model generate training testing instances OPSWTW coupled implemented baseline algorithms tracks In addition simulator competition code winning approaches tracks competition publicly available The simulation model algo rithms serve benchmark researchers develop compare surrogatebased reinforcement learningbased approaches stochastic routing problems This paper written organizers winning teams competition The organizers pleased outcome purposes ﬁrst AI4TSP competition achieved The results diversity adopted methods pure machine learning approaches integrating learning traditional heuristics There great potential developing sophisticated algorithms especially leveraging machine learning expertcrafted heuristics solve stochastic routing problems Looking forward organization team continue AI4TSP considering improvements setting time computation budget implementing realistic distribution functions simulator In addition practical societally relevant optimization problems considered future editions competition 16 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper Data availability We shared link datacode paper Acknowledgements The organizers like thank Ortec Vanderlande sponsoring prize money References 1 Y Bengio A Lodi A Prouvost Machine learning combinatorial optimization methodological tour dhorizon Eur J Oper Res 290 2021 405421 2 O Vinyals M Fortunato N Jaitly Pointer networks Proceedings 29th Conference Neural Information Processing Systems NIPS 2015 pp 26922700 3 CK Joshi T Laurent X Bresson An eﬃcient graph convolutional network technique travelling salesman problem arXiv1906 01227 2019 4 I Bello H Pham QV Le M Norouzi S Bengio Neural combinatorial optimization reinforcement learning arXivabs 161109940 2017 5 W Kool H Van Hoof M Welling Attention learn solve routing problems arXiv preprint arXiv1803 08475 2018 6 Q Ma S Ge D He D Thaker I Drori Combinatorial optimization graph pointer networks hierarchical reinforcement learning arXiv preprint arXiv191104936 2019 7 M Nazari A Oroojlooy LV Snyder M Takác Reinforcement learning solving vehicle routing problem NeurIPS 2018 8 M Deudon P Cournut A Lacoste Y Adulyasak LM Rousseau Learning heuristics TSP policy gradient International Conference Integration Constraint Programming Artiﬁcial Intelligence Operations Research Springer 2018 pp 170181 9 YD Kwon J Choo B Kim I Yoon Y Gwon S Min POMO policy optimization multiple optima reinforcement learning H Larochelle M Ranzato R Hadsell MF Balcan H Lin Eds Advances Neural Information Processing Systems vol 33 Curran Associates Inc 2020 pp 2118821198 10 ZH Fu KB Qiu H Zha Generalize small pretrained model arbitrarily large tsp instances arXiv preprint arXiv2012 10658 2020 11 W Kool H van Hoof J Gromicho M Welling Deep policy dynamic programming vehicle routing problems arXiv preprint arXiv2102 11756 2021 12 A Delarue R Anderson C Tjandraatmadja Reinforcement learning combinatorial actions application vehicle routing Adv Neural Inf Process Syst 33 2020 13 R Gama HL Fernandes A reinforcement learning approach orienteering problem time windows Comput Oper Res 133 2021 105357 14 A Hottung K Tierney Neural large neighborhood search capacitated vehicle routing problem ECAI 2020 IOS Press 2020 pp 443450 15 PRdO da Costa J Rhuggenaath Y Zhang A Akcay Learning 2opt heuristics traveling salesman problem deep reinforcement learning SJ Pan M Sugiyama Eds Proceedings 12th Asian Conference Machine Learning Proceedings Machine Learning Research PMLR vol 129 2020 pp 465480 16 P da Costa J Rhuggenaath Y Zhang A Akcay U Kaymak Learning 2opt heuristics routing problems deep reinforcement learning SN Comput Sci 2 2021 116 tions 2020 2019 pp 62816292 17 Y Wu W Song Z Cao J Zhang A Lim Learning improvement heuristics solving routing problems arXiv preprint arXiv1912 05784 2019 18 H Lu X Zhang S Yang A learningbased iterative method solving vehicle routing problems International Conference Learning Representa 19 X Chen Y Tian Learning perform local rewriting combinatorial optimization Advances Neural Information Processing Systems vol 32 20 L Xin W Song Z Cao J Zhang NeuroLKH combining deep learning model LinKernighanHelsgaun heuristic solving traveling salesman problem Advances Neural Information Processing Systems 2021 21 S Li Z Yan C Wu Learning delegate largescale vehicle routing Advances Neural Information Processing Systems 2021 22 L Gao M Chen Q Chen G Luo N Zhu Z Liu Learn design heuristics vehicle routing problem arXiv preprint arXiv2002 08539 2020 23 R Zhang A Prokhorchuk J Dauwels Deep reinforcement learning traveling salesman problem time windows rejections 2020 Inter 24 Q Cappart T Moisan LM Rousseau I PrémontSchwarz AA Cire Combining reinforcement learning constraint programming combinatorial 25 J Sui S Ding R Liu L Xu D Bu Learning 3opt heuristics traveling salesman problem deep reinforcement learning Asian Conference national Joint Conference Neural Networks IJCNN 2020 pp 18 optimization Proc AAAI Conf Artif Intell 35 2021 36773687 Machine Learning PMLR 2021 pp 13011316 26 M Kim J Park J Kim Learning collaborative policies solve NPhard routing problems Advances Neural Information Processing Systems 2021 27 A Hottung B Bhandari K Tierney Learning latent search space routing problems variational autoencoders International Conference Learning Representations 2021 httpsopenreviewnet forum id 90JprVrJBO 28 L Bliek S Verwer M Weerdt Blackbox combinatorial optimization models integervalued minima Ann Math Artif Intell 2020 115 29 R Karlsson L Bliek S Verwer M Weerdt Continuous surrogatebased optimization algorithms wellsuited expensive discrete problems Proceedings Benelux Conference Artiﬁcial Intelligence 2020 pp 88102 30 M Namazi C Sanderson MAH Newton A Sattar Surrogate assisted optimisation travelling thief problems SOCS 2020 pp 111115 31 MY Fang J Li Surrogateassisted genetic algorithms travelling salesman problem vehicle routing problem 2020 IEEE Congress Evolutionary Computation CEC 2020 pp 17 32 A Bracher N Frohner GR Raidl Learning surrogate functions shorthorizon planning sameday delivery problems CPAIOR 2021 33 MA Ardeh Y Mei M Zhang A GPHH surrogateassisted knowledge transfer uncertain capacitated arc routing problem 2020 IEEE Symposium Series Computational Intelligence SSCI 2020 pp 27862793 34 B Shahriari K Swersky Z Wang R Adams ND Freitas Taking human loop review Bayesian optimization Proc IEEE 104 2016 35 C Verbeeck P Vansteenwegen EH Aghezzaf Solving stochastic timedependent orienteering problem time windows Eur J Oper Res 255 148175 2016 699718 17 Y Zhang L Bliek P da Costa et al Artiﬁcial Intelligence 319 2023 103918 36 Y Dumas J Desrosiers E Gelinas MM Solomon An optimal algorithm traveling salesman problem time windows Oper Res 43 1995 367371 37 M Fischetti JJS Gonzalez P Toth Solving orienteering problem branchandcut INFORMS J Comput 10 1998 133148 38 F Nogueira Bayesian optimization open source constrained global optimization tool Python 2014 URL httpsgithub com fmfn BayesianOptimization 39 S Kadioglu Y Malitsky M Sellmann K Tierney ISAC instancespeciﬁc algorithm conﬁguration H Coelho R Studer M Wooldridge Eds Proceedings 19th European Conference Artiﬁcial Intelligence ECAI10 Frontiers Intelligence Applications vol 215 2010 pp 751756 40 Y Malitsky A Sabharwal H Samulowitz M Sellmann Algorithm portfolios based costsensitive hierarchical clustering TwentyThird Interna tional Joint Conference Artiﬁcial Intelligence 2013 41 C Ansótegui J Pon M Sellmann K Tierney Reactive dialectic search portfolios MaxSAT AAAI 2017 pp 765772 42 C Ansótegui B Heymann J Pon M Sellmann K Tierney Hyperreactive tabu search MaxSAT International Conference Learning 43 S Kadioglu M Sellmann Dialectic search International Conference Principles Practice Constraint Programming Springer 2009 Intelligent Optimization Springer 2018 pp 309325 pp 486500 44 C Ansotegui M Sellmann K Tierney A genderbased genetic algorithm automatic conﬁguration algorithms I Gent Ed Principles Practice Constraint Programming CP09 LNCS vol 5732 Springer 2009 pp 142157 45 C Ansótegui Y Malitsky H Samulowitz M Sellmann K Tierney Modelbased genetic algorithms algorithm conﬁguration 24th International Joint Conference Artiﬁcial Intelligence 2015 pp 733739 46 C Ansótegui J Pon M Sellmann K Tierney PyDGGA distributed GGA automatic conﬁguration CM Li F Manyà Eds Theory Applica tions Satisﬁability Testing SAT 2021 Springer International Publishing Cham 2021 pp 1120 47 R Battiti M Brunato A Mariello Reactive search optimization learning optimizing Handbook Metaheuristics Springer 2019 pp 479511 48 V Cicirello Nonwrapping order crossover order preserving crossover operator respects absolute position 2 2006 11251132 httpsdoi org 49 M Zaefferer J Stork M Friese A Fischbach B Naujoks T BartzBeielstein Eﬃcient global optimization combinatorial problems C Igel DV Arnold Eds Proceedings Genetic Evolutionary Computation Conference GECCO 2014 ACM Press New York NY 2014 pp 871878 50 R Li MT Emmerich J Eggermont T Bäck M Schütz J Dijkstra J Reiber Mixed integer evolution strategies parameter optimization Evol Comput 10 1145 11439971144177 21 2013 2964 51 H Wang B van Stein M Emmerich T Bäck Time complexity reduction eﬃcient global optimization cluster Kriging Proceedings Genetic Evolutionary Computation Conference GECCO17 ACM Berlin Germany 2017 pp 889896 52 A Hottung YD Kwon K Tierney Eﬃcient active search combinatorial optimization problems arXiv preprint arXiv2106 05126 2021 53 RJ Williams Simple statistical gradientfollowing algorithms connectionist reinforcement learning Mach Learn 8 1992 229256 54 S Ropke D Pisinger An adaptive large neighborhood search heuristic pickup delivery problem time windows Transp Sci 40 2006 55 F Hammami M Rekik LC Coelho A hybrid adaptive large neighborhood search heuristic team orienteering problem Comput Oper Res 123 455472 2020 105034 56 AE Yahiaoui A Moukrim M Serairi The clustered team orienteering problem Comput Oper Res 111 2019 386399 18