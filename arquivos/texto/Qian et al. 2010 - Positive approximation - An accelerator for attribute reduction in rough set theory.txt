Artiﬁcial Intelligence 174 2010 597618 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Positive approximation An accelerator attribute reduction rough set theory Yuhua Qian ac Jiye Liang Witold Pedrycz b Chuangyin Dang c Key Laboratory Computational Intelligence Chinese Information Processing Ministry Education Taiyuan 030006 Shanxi China b Department Electrical Computer Engineering University Alberta Edmonton AB Canada c Department Manufacturing Engineering Engineering Management City University Hong Kong Hong Kong r t c l e n f o b s t r c t Article history Received 15 July 2009 Received revised form 6 April 2010 Accepted 7 April 2010 Available online 9 April 2010 Keywords Rough set theory Attribute reduction Decision table Positive approximation Granular computing 1 Introduction Feature selection challenging problem areas pattern recognition machine learning data mining Considering consistency measure introduced rough set theory problem feature selection called attribute reduction aims retain discriminatory power original features Many heuristic attribute reduction algorithms proposed methods computationally time consuming To overcome shortcoming introduce theoretic framework based rough set theory called positive approximation accelerate heuristic process attribute reduction Based proposed accelerator general attribute reduction algorithm designed Through use accelerator representative heuristic attribute reduction algorithms rough set theory enhanced Note modiﬁed algorithms choose attribute reduct original version possesses classiﬁcation accuracy Experiments modiﬁed algorithms outperform original counterparts It worth noting performance modiﬁed algorithms visible dealing larger data sets 2010 Elsevier BV All rights reserved Feature selection called attribute reduction common problem pattern recognition data mining machine learning In recent years encounter databases number objects larger dimen sionality number attributes gets larger Tens hundreds thousands attributes stored realworld application databases 61237 Attributes irrelevant recognition tasks deteriorate perfor mance learning algorithms 4445 In words storing processing attributes relevant irrelevant computationally expensive impractical To deal issue pointed 20 tributes omitted seriously impact resulting classiﬁcation recognition error cf 20 Therefore omission attributes tolerable desirable relatively costs involved cases 32 In feature selection encounter general strategies wrappers 16 ﬁlters The employs learning algorithm evaluate selected attribute subsets selects attributes guided sig niﬁcance measures information gain 2346 consistency 641 distance 15 dependency 30 These Corresponding author Tel 86 0351 7018176 fax 86 0351 7018176 Email addresses jinchengqyhsxueducn YH Qian ljysxueducn JY Liang pedryczeeualbertaca W Pedrycz mecdangcityueduhk CY Dang 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201004018 598 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 measures divided main categories distancebased measures consistencybased measures 20 Rough set theory Pawlak 3336 relatively new soft computing tool analysis vague description object popular mathematical framework pattern recognition image processing feature selection neuro computing data mining knowledge discovery large data sets 71131 Attribute reduction rough set theory offers systematic theoretic framework consistencybased feature selection attempt maximize class separability attempts retain discernible ability original features objects universe 131453 Generally speaking needs handle types data viz assume numerical values symbolic values For numerical values types approaches One relies fuzzy rough set theory concerned discretization numerical attributes In order deal numerical attributes hybrid attributes approaches developed literature Pedrycz Vukovich regarded features granular numerical 37 Shen Jenshen generalized dependency function classical rough set framework fuzzy case proposed fuzzyrough QUICKREDUCT algorithm 131448 Bhatt Gopal provided concept fuzzyrough sets formed compact computational domain utilized improve computational eﬃciency 34 Hu et al pre sented new entropy measure information quantity fuzzy sets 21 applied particular measure reduce hybrid data 22 Data discretization important approach deal numerical values usu ally discretize numerical values intervals associate intervals set symbolic values 528 In classical rough set theory attribute reduction method takes attributes assume symbolic values Through preprocessing original data use classical rough set theory select subset features suitable given recognition problem In years techniques attribute reduction developed rough set theory The concept βreduct proposed Ziarko provides suite reduction methods variable precision rough set model 60 An attribute reduction method proposed knowledge reduction random information systems 57 Five kinds attribute reducts relationships inconsistent systems investigated Kryszkiewicz 18 Li et al 24 Mi et al 29 respectively By eliminating rigorous conditions required distribution reduct maximum distri bution reduct introduced Mi et al 29 In order obtain attribute reducts given data set Skowron 49 proposed discernibility matrix method objects determine feature subset distinguish According discernibility matrix viewpoint Qian et al 4243 Shao et al 47 provided technique attribute reduction interval ordered information systems setvalued ordered information systems incomplete ordered infor mation systems respectively Kryszkiewicz Lasek 17 proposed approach discovery minimal sets attributes functionally determining decision attribute The attribute reduction methods usually computationally ex pensive intolerable dealing largescale data sets high dimensions To support eﬃcient attribute reduction heuristic attribute reduction methods developed rough set theory cf 19202225263952 5456 Each attribute reduction methods extract single reduct given decision table1 For convenience viewpoint heuristic functions classify attribute reduction methods categories positiveregion reduction Shannons entropy reduction Liangs entropy reduction combination entropy reduction Hence review representative heuristic attribute reduction methods 1 Positiveregion reduction The concept positive region proposed Pawlak 33 measure signiﬁcance condition attribute decision table While idea attribute reduction positive region originated JW Grzymala Busse 9 10 corresponding algorithm ignores additional computation required selecting signiﬁcant attributes Then Hu Cercone 19 proposed heuristic attribute reduction method called positiveregion reduction remains positive region target decision unchanged The literature 20 gave extension positive region reduction hybrid attribute reduction framework fuzzy rough set Owing consistency ideas strategies methods regard method 19 representative These reduction methods ﬁrst attempt heuristic attribute reduction algorithms rough set theory 2 Shannons entropy reduction The entropy reducts ﬁrst introduced 19931994 Skowron lectures Warsaw University Based idea Slezak introduced Shannons information entropy search reducts classical rough set model 5052 Wang et al 54 conditional entropy Shannons entropy calculate relative attribute reduction decision information In fact authors variants Shannons entropy mutual information measure uncertainty rough set theory construct heuristic algorithm attribute reduction rough set theory 225556 Here 1 The attribute reduct obtained preserves particular property given decision table However Prof Bazan said viewpoint stability attribute reduct selected reduct bad quality 12 To overcome problem Bazan developed method dynamic reducts stable attribute reduct decision table How accelerate method dynamic reducts interesting topic work YH Qian et al Artiﬁcial Intelligence 174 2010 597618 599 select attribute reduction algorithm literature 54 representative This reduction method remains conditional entropy target decision unchanged 3 Liangs entropy reduction Liang et al 25 deﬁned new information entropy measure uncertainty information applied entropy reduce redundant features 26 Unlike Shannons entropy information entropy measure uncertainty information fuzziness rough decision rough set theory This reduction method preserve conditional entropy given decision table In fact mutual information form Liangs entropy construct heuristic function attribute reduction algorithm For simplicity ignore discussion 4 Combination entropy reduction In general objects equivalence class distinguished objects different equivalence classes distinguished rough set theory Therefore broad sense knowledge content given attribute set characterized entire number pairs objects distinguished universe Based consideration Qian Liang 39 presented concept combination entropy measuring uncertainty information systems conditional entropy select feature subset This reduction method obtain attribute subset possesses number pairs elements distinguished original decision table This measure focuses completely different point view mainly based intuitionistic knowledge content nature information gain Each methods preserves particular property given information given decision table However methods computationally expensive intolerable dealing largescale data sets high dimensions In paper concerned discretize numerical attributes struct heuristic function attribute reduction Our objective focus improve time eﬃciency heuristic attribute reduction algorithm We propose new rough set framework called positive approximation The main advantage approach stems fact framework able characterize granulation struc ture rough set granulation order Based positive approximation develop common accelerator improving time eﬃciency heuristic attribute reduction provides vehicle making algorithms rough set based feature selection techniques faster By incorporating accelerator representative heuristic attribute reduction methods construct modiﬁed versions Numerical experiments modiﬁed methods choose attribute subset corresponding original method greatly reducing computing time We like stress improvement profoundly visible data sets discussion larger The study organized follows Some basic concepts rough set theory brieﬂy reviewed Section 2 In Section 3 establish positive approximation framework investigate main properties In Section 4 analyzing rank preservation representative signiﬁcance measures attributes develop general modiﬁed attribute reduction algorithm based positive approximation Experiments public data sets modiﬁed algorithms outperform original counterparts terms computational time Finally Section 5 concludes paper bringing remarks discussions 2 Preliminaries In section review basic concepts rough set theory Throughout paper suppose universe U ﬁnite nonempty set Let U ﬁnite nonempty set called universe R U U equivalence relation U Then K cid4U Rcid5 called approximation space 3336 The equivalence relation R partitions set U disjoint subsets This partition universe called quotient set induced R denoted U R It represents special type similarity elements universe If elements x y U x cid7 y belong equivalence class x y indistinguishable equivalence relation R equal R We denote equivalence class including x xR Each equivalence class xR viewed information granule consisting indistinguishable elements 59 The granulation structure induced equivalence relation partition universe Given approximation space K cid4U Rcid5 arbitrary subset X U construct rough set set universe elemental information granules following deﬁnition cid2 cid3 cid3 R X R X xR xR X xR xR X cid7 R X R X called Rlower approximation Rupper approximation respect R respectively The order pair cid4R X R Xcid5 called rough set X respect equivalence relation R Equivalently written cid2 R X x xR X R X x xR X cid7 600 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 There kinds attributes classiﬁcation problem characterized decision table S U C D C D element C called condition attribute C called condition attribute set element D called decision attribute D called decision attribute set Each nonempty subset B C determines equivalence relation following way cid4 R B x y U U ax y B cid5 ax y denote values objects x y condition attribute respectively This equivalence relation R B partitions U equivalence classes given cid4 cid5 U R B xB x U simplicity U R B replaced U B cid2 xB denotes equivalence class determined x respect B xB y U x y R B Assume objects partitioned r mutually exclusive crisp subsets Y 1 Y 2 Y r decision attributes D Given subset B C R B equivalence relation induced B deﬁne lower upper approx imations decision attributes D R B D R B Y 1 R B Y 2 R B Yr R B D R B Y 1 R B Y 2 R B Yr cid3 r i1 R B Y called positive region D respect condition attribute set B Denoted POSB D We deﬁne partial relation cid2 family B B C follows P cid2 Q Q cid3 P P U P exists Q j U Q P Q j U P P 1 P 2 Pm U Q Q 1 Q 2 Q n partitions induced P Q C respectively 40 In case Q coarser P P ﬁner Q If P cid2 Q U P cid7 U Q Q strictly coarser P P strictly ﬁner Q denoted P Q Q cid13 P It clear P Q X U P exists Y U Q X Y exists X0 U P Y 0 U Q X0 Y 0 3 Positive approximation properties A partition induced equivalence relation provides granulation world describing target concept 38 Thus sequence granulation worlds stretching coarse ﬁne granulation determined sequence attribute sets granulations coarse ﬁne power set attributes called positive granulation world If granulation worlds arranged ﬁne coarse sequence called converse granulation worlds 2740 In section introduce new setapproximation approach called positive approximation investigate important properties given set called target concept rough set theory approximated positive granulation world Given decision table S U C D U D Y 1 Y 2 Y r called target decision equivalence class Y cid4 r regarded target concept These concepts properties helpful understand notion granulation order set approximation granulation order Deﬁnition 1 Let S U C D decision table X U P R1 R2 Rn family attribute sets R1 cid3 R2 cid3 cid3 Rn R 2C Given P R1 R2 R deﬁne P lower approximation P X P upper approximation P X P positive approximation X cid6 cid3 k1 Rk Xk P iX P iX R X X1 X Xk X cid3 k1 j1 R j X j k 2 3 n 1 2 n Correspondingly boundary X given BN P X P iX P iX Theorem 1 Let S U C D decision table X U P R1 R2 Rn family attribute sets R1 cid3 R2 cid3 cid3 Rn R 2C Given P R1 R2 R P 1 2 n P iX X P iX P 1X P 2X P iX Fig 1 visualizes mechanism positive approximation In Fig 1 let P 1 R1 P 2 R1 R2 R1 cid3 R2 granulation orders R1 X1 lower approximation X1 obtained equivalence relation R1 R2 X2 lower approximation X2 obtained equivalence YH Qian et al Artiﬁcial Intelligence 174 2010 597618 601 Fig 1 Sketch map positive approximation relation R2 Hence P 2 X R1 X1 R2 X2 R2 X The mechanism characterizes structure rough set approximation gradually compute lower approximation target conceptdecision Theorem 2 Let S U C D decision table X U P R1 R2 Rn family attribute sets R1 cid3 R2 cid3 cid3 Rn R 2C Given P R1 R2 R P 1 2 n αP 1 X cid4 αP 2 X cid4 cid4 αP X αP X P X P X approximation measure X respect P In order illustrate essence positive approximation concentrated changes construction target concept X equivalence classes lower approximation X respect P redeﬁne P positive approximation X equivalence classes U The structures P lower approximation P X P upper approximation P X P positive approximation X represented follows cid2 SP iX xRk SP iX xR xRk xR Rk Xk k cid4 X1 X Xk1 Rk Xk X cid7 xR represents equivalence class including x partition U R Example 1 Let U e1 e2 e3 e4 e5 e6 e7 e8 X e1 e2 e3 e4 e7 e8 U R1 e1 e2 e3 e4 e5 e6 e7 e8 U R2 e1 e2 e3 e4 e5 e6 e7 e8 partitions U Obviously R1 cid3 R2 holds Thus construct granulation orders family equivalence relations P 1 R1 P 2 R1 R2 By computing positive approximation X easily obtain cid5 cid7 S cid8 P 1X cid8 P 1X cid8 P 2X cid8 P 2X cid7 cid7 cid7 S S S cid4 e1 e2 e3 e4 cid4 e1 e2 e3 e4 e5 e6 e7 e8 cid4 e1 e2 e3 e4 e7 e8 cid4 e1 e2 e3 e4 e7 e8 cid5 cid5 cid5 That target concept X described granulation orders P 1 R1 P 2 R1 R2 respectively Deﬁnition 2 Let S U C D decision table P R1 R2 R family attribute sets R1 cid3 R2 cid3 R R 2C U D Y 1 Y 2 Y r Lower approximation upper approximation D respect P deﬁned cid2 P D P iY 1 P iY 2 P iYr P D P iY 1 P iY 2 P iYr P D called positive region D respect granulation order P denoted POSU P D cid3 r k1 P Yk 602 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 Theorem 3 Recursive expression principle Let S U C D decision table X U P R 1 R2 Rn family attribute sets R1 cid3 R2 cid3 cid3 Rn R 2C Given P R1 R2 R POSU P i1 D POSU P D POS U i1 R i1 D U 1 U U i1 U POSU P D Example 2 Let S U C D decision table U e1 e2 e3 e4 e5 e6 e7 e8 C a1 a2 U D e1 e2 e3 e4 e7 e8 e5 e6 U a1 e1 e2 e3 e4 e5 e6 e7 e8 U C e1 e2 e3 e4 e5 e6 e7 e8 partitions U Obviously a1 cid3 C holds Thus construct granulation orders family equivalence relations P 1 a1 P 2 a1 a2 By computing positive approximation D easily obtain D e1 e2 e3 e4 U 1 U D POSU 1 P 1 D e5 e6 e7 e8 POSU P 1 U 2 U POSU P 1 POSU 2 P 2 D e5 e6 Hence POSU P 2 D e1 e2 e3 e4 e5 e6 POSU P 1 D POSU 2 P 2 D That target decision D positively approximated granulation orders P 1 P 2 gradually reduced universe respectively This mechanism implies idea accelerator proposed paper improving computing performance heuristic attribute reduction algorithm The dependency function level consistency 5 characterize dependency degree attribute subset respect given decision 833 Given decision table S U C D dependency function condition attributes C respect decision attribute D formally deﬁned γC D POSU C DU Using notation deﬁnition dependency function granulation order P respect D following Deﬁnition 3 A dependency function involving granulation order P D deﬁned γP D POSU P D U denotes cardinality set 0 cid4 γP D cid4 1 The dependency function reﬂects granulation order P s power dynamically approximate D When γ 1 says D completely depends granulation order P It means decision precisely described information granules generated granulation order P This dependency function measure signiﬁcance cate gorical attributes relative decision construct heuristic function designing attribute reduction algorithm 4 FSPA feature selection based positive approximation Each feature selection method preserves particular property given information based certain predetermined heuristic function In rough set theory attribute reduction ﬁnding attribute subsets minimal attributes retain particular properties For example dependency function keeps approxima tion power set condition attributes To design heuristic attribute reduction algorithm key problems considered signiﬁcance measures attributes search strategy stopping termination criterion As symbolic attributes numerical attributes realworld data needs proceed preprocessing Through attribute discretization easy induce equivalence partition However existing heuristic attribute reduction methods computationally intensive infeasible case largescale data As noted reconstruct signiﬁcance measures attributes design new stopping criteria improve search strategies existing algorithms exploiting proposed concept positive approximation 41 Forward attribute reduction algorithms In rough set theory support eﬃcient attribute reduction heuristic attribute reduction methods veloped forward greedy search strategy usually employed cf 19202225263952 In kind attribute reduction approaches important measures attributes heuristic functions inner importance YH Qian et al Artiﬁcial Intelligence 174 2010 597618 603 Fig 2 The process forward greedy attribute reduction algorithm measure outer importance measure The inner importance measure applicable determine signiﬁcance attribute outer importance measure forward feature selection It deserved point kind attribute reduction tries preserve particular property given decision table In forward greedy attribute reduction approach starting attribute maximal inner importance attribute maximal outer signiﬁcance attribute subset loop feature subset satisﬁes stopping criterion attribute reduct Formally forward greedy attribute reduction algorithm written follows Algorithm 1 A general forward greedy attribute reduction algorithm Input Decision table S U C D Output One reduct red Step 1 red red pool conserve selected attributes Step 2 Compute Siginnerak C D k cid4 C Siginnerak C D inner importance measure attribute ak Step 3 Put ak red Siginnerak C D U 0 Step 4 While EFred D cid7 EFC D Do This provides stopping criterion red red a0 Sigoutera0 red D maxSigouterak red D ak C red Sigouterak C D outer importance measure attribute ak Step 5 Return red end This algorithm obtain attribute reduct given decision table Fig 2 displays process attribute duction based forward greedy attribute reduction algorithm rough set theory helpful clearly understanding mechanism algorithm Remark Given deﬁnition attribute reduct heuristic function attribute reduction framework heuristically ﬁnd attribute reduct feature subset preserves particular property decision table If survey attribute reduct viewpoint rough classiﬁers attribute reduction algorithms lead overﬁtting approximation concepts weaken generalization ability rough classiﬁers induced attribute reducts obtained This problem caused cases One attribute subset induced attribute reduction algorithm forward greedy searching strategy redundant That redundant attributes attribute subset obtained deﬁnition given attribute reduct The deﬁnition attribute reductions account generalization ability rough classiﬁer induced attribute reduct obtained These situations yield overﬁtting problem decision tree tree long paths Hence desirable solve overﬁtting problem feature selection learning rough classiﬁer framework rough set theory This issue addressed future work 42 Four representative signiﬁcance measures attributes For eﬃcient attribute reduction heuristic attribute reduction methods developed rough set theory 192022252639525456 For convenience pointed introduction paper focus representative attribute reduction methods Given decision table S U C D obtain condition partition U C X1 X2 Xm decision partition U D Y 1 Y 2 Yn Through notations follows review types signiﬁcance measures attributes The idea attribute reduction positive region ﬁrst originated GrzymalaBusse Refs 9 10 corresponding algorithm ignores additional computation choice signiﬁcant attributes Hu Cercone proposed heuristic attribute reduction method called positiveregion reduction PR remains positive region target decision unchanged 19 In method signiﬁcance measures attributes deﬁned follows 604 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 Deﬁnition 4 Let S U C D decision table B C B The signiﬁcance measure B deﬁned Siginner 1 B D γB D γBaD γB D POSB D U Deﬁnition 5 Let S U C D decision table B C C B The signiﬁcance measure B deﬁned Sigouter 1 B D γBaD γB D As Shannons information entropy introduced search reducts classical rough set model 52 Wang et al conditional entropy calculate relative attribute reduction decision information 54 In fact authors variants Shannons entropy measure uncertainty rough set theory construct heuristic algorithm attribute reduction Here select attribute reduction algorithm literature 54 representative This reduction method remains conditional entropy target decision unchanged denoted SCE conditional entropy reads HDB mcid9 i1 pXi ncid9 j1 pY j Xi log cid7 cid8 pY j Xi p Xi Xi expressed following way U pY j Xi Xi Y j Xi Using conditional entropy deﬁnitions signiﬁcance measures Deﬁnition 6 Let S U C D decision table B C B The signiﬁcance measure B deﬁned Deﬁnition 7 Let S U C D decision table B C C B The signiﬁcance measure B deﬁned Siginner 2 B D H DB HDB Sigouter 2 B D HDB H DB cid8 As pointed 25 Shannons entropy fuzzy entropy measure fuzziness rough decision rough set theory Hence Liang et al deﬁned information entropy conditional entropy measure uncertainty information applied proposed entropy reduce redundant features 2526 This reduction method preserve conditional entropy given decision table denoted LCE The conditional entropy study deﬁned mcid9 ncid9 EDC i1 j1 Y j Xi U Y c j X c U The corresponding signiﬁcance measures listed follows Deﬁnition 8 Let S U C D decision table B C B The signiﬁcance measure B deﬁned cid7 cid7 cid8 cid7 cid8 cid7 Deﬁnition 9 Let S U C D decision table B C C B The signiﬁcance measure B deﬁned Siginner 3 B D E DB EDB Sigouter 3 B D EDB E DB cid8 Based intuitionistic knowledge content nature information gain Qian Liang 39 presented concept combination entropy measuring uncertainty information systems conditional entropy obtain feature subset This reduction method obtain attribute subset possesses number pairs elements distinguished original decision table denoted CCE The following deﬁnition conditional entropy considered CEDC cid10 mcid9 i1 Xi U C 2 Xi C 2 U ncid9 j1 Xi Y j U C 2 Xi Y j C 2 U cid11 Xi Xi Xi 1 C 2 2 equivalence class Xi denotes number pairs objects distinguishable The conditional entropy construct corresponding signiﬁcance measures attributes decision tables YH Qian et al Artiﬁcial Intelligence 174 2010 597618 605 Fig 3 The relationship core attribute reducts Deﬁnition 10 Let S U C D decision table B C B The signiﬁcance measure B deﬁned Siginner 4 B D CE DB CEDB cid7 Sigouter 4 B D CEDB CE DB cid8 cid8 cid7 Deﬁnition 11 Let S U C D decision table B C C B The signiﬁcance measure B deﬁned All deﬁnitions select attribute heuristic attribute reduction algorithm For given decision table intersection attribute reducts said indispensable called core Each attribute core attribute reduct decision table The core set The relationship core attribute reducts displayed Fig 3 The kinds signiﬁcance measures ﬁnd core attributes The following theorem regard Theorem 4 See 26333954 Let S U C D decision table C If Siginner core attribute S context type cid5 cid5 C D 0 cid5 1 2 3 4 From deﬁnition core attribute core attribute reduct cid5 C D 0 cid5 1 2 3 4 ﬁnd attribute cid5 C D 0 cid5 1 2 3 4 attribute indispensable attribute decision table It known Siginner reduct deleted If Siginner reducts Therefore attribute core attribute S context type cid5 In heuristic attribute reduction algorithm based theorem ﬁnd attribute reduct gradually adding selected attributes core attributes 43 Rank preservation signiﬁcance measures attributes As mentioned signiﬁcance measures attributes provides heuristics guide mechanism forward searching feature subset Unlike discernibility matrix computational time heuristic algorithms largely reduced attribute reduct needed Nevertheless algorithms time consuming To introduce improved strategy heuristic attribute reductions concentrate rank preservation signiﬁcance measures attributes based positive approximation encountered decision table Firstly investigate rank preservation signiﬁcance measures attributes based dependency measure For cid5 B D U cid5 1 2 3 4 clear representation denote signiﬁcance measure attribute Sigouter denotes value signiﬁcance measure universe U One prove following theorem rank preservation Theorem 5 Let S U C D decision table B C U cid16 Sigouter 1 b B D U Sigouter cid16 cid5 Sigouter b B D U B D U 1 1 cid16 U POSU B D For b C B Sigouter 1 B D U cid5 Proof From deﬁnition Sigouter function γB D POSB D Therefore Since U U 1 B D γBaD γB D know value depends dependency B D POSU B D B D know POSU BaD POSU BaD POSU cid16 U POSU cid16 cid16 Sigouter 1 Sigouter 1 B D U B D U cid16 BaD γ U γ U BaD γ U cid16 γ U cid16 B D B D 606 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 cid16 U U cid16 U U cid16 U U POSU POSU cid16 POSU POSU BaD POSU BaD POSU cid16 BaD POSU BaD POSU B D B D B D B D Because cid16 U U cid5 0 Sigouter 1 pletes proof cid2 B D U cid5 Sigouter 1 b B D U Sigouter 1 B D U cid16 cid5 Sigouter 1 b B D U cid16 This com Secondly research rank preservation signiﬁcance measures attributes based Shannons conditional entropy The following theorem elaborates rank preservation measure Theorem 6 Let S U C D decision table B C U cid16 Sigouter 2 b B D U Sigouter cid16 cid5 Sigouter b B D U B D U 2 2 cid16 U POSU B D For b C B Sigouter 2 B D U cid5 Proof Let U B X1 X2 X p X p1 Xm U D Y 1 Y 2 Yn X p1 X p2 Xm POSU B D Since equivalence class X POSB D exists decision class Y X Y X Let denote Shannons conditional entropy universe U H U DB Then follows H U DB mcid9 pXi pY j Xi log cid7 cid8 pY j Xi ncid9 j1 i1 cid10 pcid9 cid10 cid10 i1 pcid9 i1 pcid9 i1 pXi ncid9 j1 pY j Xi log cid7 cid8 pY j Xi mcid9 pXi ip1 ncid9 j1 pY j Xi log cid7 cid8 pY j Xi cid11 cid11 Xi U Xi U ncid9 j1 ncid9 j1 Xi Y j Xi log Xi Y j Xi Xi Y j Xi log Xi Y j Xi mcid9 ip1 mcid9 ip1 Xi U Xi U ncid9 j1 ncid9 j1 Xi Y j Xi log Xi Y j Xi cid11 Xi Xi log Xi Xi pcid9 i1 cid16 U U Xi U pcid9 i1 ncid9 j1 Xi U cid16 Xi Y j Xi log Xi Y j Xi ncid9 j1 Xi Y j Xi log Xi Y j Xi cid16 cid16 U U H U DB Therefore Sigouter 2 B D U cid16 cid5 Sigouter 2 aBDU aBDU cid16 Sigouter 2 Sigouter 2 b B D U cid16 This completes proof cid2 U cid16 U Thus Sigouter 2 B D U cid5 Sigouter 2 b B D U b C B Then obtain rank preservation signiﬁcance measures attributes based Liangs conditional entropy given following theorem Theorem 7 Let S U C D decision table B C U cid16 Sigouter 3 b B D U Sigouter cid16 cid5 Sigouter b B D U B D U 3 3 cid16 U POSU B D For b C B Sigouter 3 B D U cid5 Proof Suppose U B X1 X2 X p X p1 Xm U D Y 1 Y 2 Yn X p1 X p2 Xm POSU B D For equivalence class X POSB D exists decision class Y X Y X X Y We denote Liangs conditional entropy universe U E U DB Then YH Qian et al Artiﬁcial Intelligence 174 2010 597618 607 E U DB mcid9 ncid9 i1 mcid9 j1 ncid9 i1 pcid9 j1 ncid9 i1 pcid9 j1 ncid9 i1 pcid9 j1 ncid9 i1 j1 Y j Xi U Y c j X c U Y j Xi U Xi Y j U Y j Xi U Xi Y j U Y j Xi U Xi Y j U mcid9 ncid9 ip1 mcid9 j1 ncid9 ip1 j1 Y j Xi U Xi Y j U Y j Xi U U Y j Xi U Xi Y j U pcid9 ncid9 i1 j1 Y j Xi U cid16 Xi Y j U cid16 cid16 E U DB cid162 U U 2 cid162 U U 2 Hence Sigouter 3 B D U aBDU aBDU cid16 Sigouter 3 Sigouter 3 cid16 cid5 Sigouter 3 b B D U cid16 This completes proof cid2 U cid162 U 2 Therefore b C B Sigouter 3 B D U cid5 Sigouter 3 b B D U Finally similar theorems signiﬁcance measures attributes based condi tional combination entropy possesses property rank preservation shown follows Theorem 8 Let S U C D decision table B C U cid16 Sigouter 4 b B D U Sigouter cid16 cid5 Sigouter b B D U B D U 4 4 cid16 U POSU B D For b C B Sigouter 4 B D U cid5 Proof Suppose U B X1 X2 X p X p1 Xm U D Y 1 Y 2 Yn X p1 X p2 Xm POSU B D Hence equivalence class X POSB D exists decision class Y X Y X X Y Denote conditional combination entropy universe U CEU DB Then cid10 mcid9 CEU DB cid11 cid11 cid11 cid11 ncid9 j1 ncid9 j1 ncid9 j1 ncid9 j1 Xi Y j U Xi Y j U Xi Y j U Xi Y j U C 2 Xi Y j C 2 U C 2 Xi Y j C 2 U C 2 Xi Y j C 2 U C 2 Xi Y j C 2 U cid10 mcid9 ip1 mcid9 cid10 ip1 cid11 Xi U cid16 C 2 Xi C 2 U cid16 ncid9 j1 Xi Y j U cid16 C 2 Xi Y j C 2 U cid16 cid11 C 2 Xi Y j C 2 U cid11 Xi U Xi U C 2 Xi C 2 U C 2 Xi C 2 U ncid9 j1 ncid9 j1 Xi Y j U Xi U C 2 Xi C 2 U Xi U Xi U Xi U C 2 Xi C 2 U C 2 Xi C 2 U C 2 Xi C 2 U i1 pcid9 cid10 i1 pcid9 cid10 i1 pcid9 cid10 C 2 Xi C 2 U cid10 pcid9 i1 Xi U U cid16 i1 cid16C 2 U U C 2 U cid16C 2 U U C 2 U U cid16 cid16 CEU DB In sequel obtain Sigouter 4 B D U Sigouter 4 Sigouter 4 cid16 cid5 Sigouter aBDU aBDU cid16 b B D U 4 cid16 cid16C 2 U U U C 2 cid16 This completes proof cid2 U Therefore b C B Sigouter B D U cid5 Sigouter 4 b B D U 4 608 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 From Theorems 58 uniformly represented following theorem Theorem 9 Rank preservation Let S U C D decision table B C U Sigouter cid5 b B D U cid5 1 2 3 4 Sigouter cid5 B D U cid5 Sigouter cid5 B D U cid16 cid5 Sigouter cid16 U POSU cid5 b B D U cid16 B D For b C B From theorem rank attributes process attribute reduction remain unchanged af ter reducing lower approximation positive approximation This mechanism improve computational performance heuristic attribute reduction algorithm retaining selected feature subset Besides kind attribute reducts know different kind attribute reducts We discuss rank preservation existing reducts taking account compact paper In fact rank preservation attributes selected based monotonicity positive region target decision heuristic feature selection process In words rank attributes selected unchanged scale positive region target decision increases number selected attributes bigger 44 Attribute reduction algorithm based positive approximation The objective rough setbased feature selection ﬁnd subset attributes retains particular proper ties original data redundancy In fact multiple reducts given decision table It proven ﬁnding minimal reduct decision table NP hard problem When attribute reduct needed based signiﬁcance measures attributes heuristic algorithms proposed greedy forward search algorithms These search algorithms start nonempty set adding attributes high signiﬁcance pool time dependence increased From discussion previous subsection knows rank preservation attributes context positive approximation Hence construct improved forward search algorithm based positive approx imation formulated follows In general algorithm framework denote evaluation function stop criterion EFU B D EFU C D For example adopts Shannons conditional entropy evaluation function H U B D H U C D That EFU B D EFU C D B said attribute reduct Algorithm Q1 A general improved feature selection algorithm based positive approximation FSPA Input Decision table S U C D Output One reduct red Step 1 red red pool conserve selected attributes Step 2 Compute Siginnerak C D U k cid4 C Step 3 Put ak red Siginnerak C D U 0 These attributes form core given decision table Step 4 1 R1 red P 1 R1 U 1 U Step 5 While EFU red D cid7 EFU C D Do D Compute positive region positive approximation POSU P U U POSU P 1 red red a0 Sigoutera0 red D U maxSigouterak red D U ak C red R R a0 P R1 R2 R D Step 6 Return red end cid12C i1 U iC 1 Thus time complexity FSPA O U C Computing signiﬁcance measure attribute Siginnerak C D U key steps FSPA Xu et al 57 gave quick algorithm time complexity O U Hence time complexity computing core Step 2 O CU In Step 5 begin core add attribute maximal signiﬁcance set stage ﬁnding reduct This process called forward reduction algorithm time complexity U iC 1 However time com O U C 1 Obviously time complexity FSPA plexity classical heuristic algorithm O U C lower classical heuristic attribute reduction algorithms Hence draw conclusion gen eral feature selection algorithm based positive approximation FSPA signiﬁcantly reduce computational time attribute reduction decision tables To stress ﬁndings time complexity step original algorithms FSPA shown Table 1 cid12C i1 cid12C i1 To support substantial contribution general improved attribute reduction algorithm based positive approximation summarize factors speedup accelerator follows 1 One select attribute loop improved algorithm original This provides restriction keeping result attribute reduction algorithm YH Qian et al Artiﬁcial Intelligence 174 2010 597618 609 Step 2 O CU O CU Step 3 O C O C Step 5 cid12C i1 cid12C i1 O O U C 1 U C 1 Other steps Constant Constant Table 1 The complexities description Algorithms Each original algorithms FSPA Table 2 Data sets description 1 2 3 4 5 6 7 8 9 Data sets Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 Cases 5644 958 358 3196 683 376 58000 20000 5822 Features Classes 22 9 34 36 9 35 9 16 85 2 2 6 2 2 19 7 26 2 2 Computational time signiﬁcance measures attributes signiﬁcantly reduced considered gradually reduced universe It key factor accelerated algorithm 3 Time consumption computing stopping criterion signiﬁcantly reduced gradually decreasing size data set This important factor improved algorithm Based speedup factors draw conclusion general modiﬁed algorithm signif icantly reduce computational time existing attribute reduction algorithm producing attribute reducts classiﬁcation accuracies coming original ones It deserved point modiﬁed algorithms solve overﬁtting problem approxi mation concepts improve generalization ability rough classiﬁer induced obtained attribute reduct The general improved algorithm devotes largely reducing computational time original attribute reduction algorithms Algorithm 1 45 Time eﬃciency analysis algorithms Many heuristic attribute reduction methods developed symbolic data 192022252639525456 The heuristic algorithms mentioned Section 42 representative The objective following experiments time eﬃciencies proposed general framework selecting feature subset The data experiments outlined Table 2 downloaded UCI Repository machine learning databases In subsection order compare representative attribute reduction algorithms PR SCE LCE CCE modiﬁed ones employ UCI data sets Table 2 verify performance time reduction modiﬁed algorithms symbolic data Shuttle Ticdata2000 preprocessed discretization entropy In data sets Mushroom Breastcancerwisconsin data sets missing values For uniform treatment data sets remove objects missing values From rank preservation signiﬁcance measures attributes know modiﬁed attribute reduction algo rithm obtain attribute reduct original version Therefore following experiments consider attribute reducts obtained computational time compare classiﬁcation accuracies For heuristic attribute reduction algorithm rough set theory computation classiﬁcation ﬁrst key step To date quick classiﬁcation algorithms proposed improving eﬃciency attribute reduction However computation classiﬁcation pretreatment data heuristic attribute reduction algorithm Hence convenient comparison adopt classiﬁcation algorithm time complexity O CU 58 In follows apply original algorithms modiﬁed version searching attribute reducts To distinguish computational times divide data sets parts equal size The ﬁrst regarded 1st data set combination ﬁrst second viewed 2nd data set combination 2nd data set regarded 3rd data set combination parts viewed 20th data set These data sets calculate time original attribute reduction algorithms corresponding modiﬁcations visavis size universe These algorithms run personal Windows XP InterR CoreTM2 Quad CPU Q9400 266 GHz 337 GB memory The software Microsoft Visual Studio 2005 Visual C 610 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 Table 3 The time attribute reduction algorithms PR FSPAPR Data sets Original features PR algorithm Selected features Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 22 9 34 36 9 35 9 16 85 3 8 10 29 4 10 4 11 24 Time s 248750 03594 08438 280313 01250 06563 9060625 2826406 8864531 FSPAPR algorithm Selected features 3 8 10 29 4 10 4 11 24 Time s 204531 03125 04375 215781 00938 04219 7122500 1126250 2963750 Fig 4 Times PR FSPAPR versus size data 451 PR FSPAPR In sequence experiments compare PR FSPAPR real world data sets shown Table 2 The experimental results data sets shown Table 3 Fig 4 In subﬁgures xcoordinate pertains size data set 20 data sets starting smallest ycoordinate concerns computing time Table 2 shows comparisons selected features computational time original algorithm PR accelerated algorithm FSPAPR data sets While Fig 4 displays detailed change trend algorithms size data set increasing It easy note Table 3 Fig 4 computing time algorithms increases increase size data Nevertheless relationship strictly monotonic For example size data set varies 18th 19th subﬁgure f computing time dropped We observe effect subﬁgures c e One envision situation occurred different numbers features selected YH Qian et al Artiﬁcial Intelligence 174 2010 597618 611 As important advantages FSPA shown Table 3 Fig 4 modiﬁed algorithms faster original counterparts basis selecting feature subset Sometimes effect reduction reduce half computational time For example reduced time achieves 1700156 seconds data set Letterrecognition reduced time 5900781 seconds data set Ticdata200 Furthermore differences profoundly larger size data set increases Owing rank preservation signiﬁcance measures attributes feature subset obtained modiﬁed algorithm produced original algorithm 452 SCE FSPASCE It known attribute reduct induced Shannons information entropy keeps probabilistic distribution original data set based strict deﬁnition attribute reduct Hence attribute reduct obtained approach longer induced positiveregion reduction In follows compare SCE FSPASCE real world data sets shown Table 2 computational time selected feature subsets Table 4 presents comparisons selected features computational time original algorithm SCE accelerated algorithm FSPASCE data sets While Fig 5 gives detailed change trendline algorithms size data set increasing From Table 4 Fig 5 easy modiﬁed algorithms consistently faster original coun terparts Sometimes reduced time achieves seveneighths original computational time For example reduced time achieves 42754531 seconds data set Letterrecognition reduced time achieves 71097657 seconds data set Ticdata2000 In particular feature subset obtained modiﬁed algorithm produced original algorithm beneﬁts rank preservation signiﬁcance measures attributes based positive approximation Furthermore differences profoundly larger size data set increases Hence attribute reduction based accelerator good solution 453 LCE FSPALCE The attribute reduct induced Liangs information entropy keeps probabilistic distribution original data set based strict deﬁnition attribute reduct The attribute reduct obtained approach longer induced positiveregion reduction In following experiments compare LCE FSPALCE real world data sets shown Table 2 The comparisons selected features computational time original algorithm SCE accelerated algorithm FSPASCE data sets shown Table 5 detailed change trendline algorithms size data set increasing given Fig 6 As important advantages FSPA shown Table 5 Fig 6 modiﬁed algorithms faster original counterparts Furthermore differences profoundly larger size data set increases Sometimes computational time modiﬁed algorithm oneﬁfteenths computational time algorithm LCE On data set Ticdata2000 example FSPALCE needs 18055625 seconds LCE uses 279626250 seconds Like FSPAPR FSPASCE attribute reduct obtained modiﬁed algorithm FSPALCE proceed original algorithm LCE owing rank preservation signiﬁcance measures attributes 454 CCE FSPACCE Finally compare CCE FSPACCE real world data sets shown Table 2 In Table 6 shown comparisons selected features computational time original algorithm SCE accelerated algorithm FSPASCE data sets In Fig 7 display detailed change trendline algorithms size data set increasing Similarly subﬁgures ai xcoordinate pertains size data set 20 data sets starting smallest ycoordinate concerns computing time From Table 6 Fig 7 easy modiﬁed algorithm consistently faster original counterpart Sometimes reduced time seveneighths original computational time For example reduced time achieves 72134688 seconds data set Ticdata2000 reduced time achieves 45079062 seconds data set Letterrecognition In particular feature subset obtained modiﬁed algorithm produced original algorithm guaranteed rank preservation signiﬁcance measures attributes based positive approximation Furthermore differences profoundly larger size data set increases One attribute reduction based accelerator good solution As important advantages FSPA shown Table 6 Fig 7 modiﬁed algorithm faster original counterpart Furthermore differences profoundly larger size data set increases Owing rank preservation signiﬁcance measures attributes feature subset obtained modiﬁed algorithm produced original algorithm 46 Stability analysis algorithms The stability heuristic attribute reduction algorithm determines stability classiﬁcation accuracy The ob jective suite experiments compare stability computing time attribute reduction 612 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 Table 4 The time attribute reduction algorithms SCE FSPASCE Data sets Original features SCE algorithm FSPASCE algorithm Selected features Time s Selected features Time s Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 22 9 34 36 9 35 9 16 85 4 8 11 29 4 10 4 11 24 1626406 45000 53125 1496250 13438 43594 126653906 70157031 81536563 4 8 11 29 4 10 4 11 24 1595938 31094 19844 1059844 08438 17656 101531719 27402500 10438906 Fig 5 Times SCE FSPASCE versus size data modiﬁed algorithms obtained running original methods We use realworld data sets shown Table 2 In experiments order evaluate stability feature subset selected 10fold cross validation introduce deﬁnitions necessary notations Let X1 X2 X10 10 data sets coming given universe U We denote reduct induced universe U C0 The reducts induced data set Xi denoted Ci cid4 10 respectively To measure difference reducts Ci C j use following distance DCi C j 1 Ci C j Ci C j YH Qian et al Artiﬁcial Intelligence 174 2010 597618 613 Table 5 The time attribute reduction algorithms LCE FSPALCE Data sets Original features LCE algorithm FSPALCE algorithm Selected features Time s Selected features Time s Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 22 9 34 36 9 35 9 16 85 4 8 10 29 5 10 4 12 24 3002188 87344 104531 11561250 31250 98438 248836250 151767656 279626250 4 8 10 29 5 10 4 12 24 2940000 57813 37500 1911250 16719 32188 202283906 55587813 18055625 Fig 6 Times LCE FSPALCE versus size data Next calculate mean value 10 distances μ 1 10 cid13 10cid9 i1 1 Ci C0 Ci C0 cid14 C0 reduct induced universe U This standard deviation cid15 cid16 cid16 cid17 1 10 σ cid8 DCi C0 μ 2 10cid9 cid7 i1 614 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 Table 6 The time attribute reduction algorithms CCE FSPACCE Data sets Original features CCE algorithm FSPACCE algorithm Selected features Time s Selected features Time s Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 22 9 34 36 9 35 9 16 85 4 8 10 29 4 9 4 11 24 1669219 67656 58281 1497500 13594 45781 137188750 71182656 82620469 4 8 10 29 4 9 4 11 24 1596406 31406 22656 1057500 08906 19844 109489219 26103594 10485781 Fig 7 Times CCE FSPACCE versus size data characterize stability reduct result induced heuristic attribute reduction algorithm The lower value standard deviation higher stability algorithm Similarly use standard deviation evaluate stability computing time As heuristic attribute reduction algorithms modiﬁcations The results reported Tables 710 obtained 10fold cross validation Table 7 reveals FSPAPR comes far lower mean time standard deviation ones produced original PR The FSPAPRs stability reported PR In words accelerator attribute reduction positive approximation signiﬁcantly reduce time consumption algorithm PR The smaller standard deviation implies modiﬁed algorithm FSPAPR exhibits far better robustness original PR We note modiﬁed algorithm affected stability reducts induced original method obtained attribute reduct data set The mechanism interpreted YH Qian et al Artiﬁcial Intelligence 174 2010 597618 615 Table 7 The stabilities time attribute reduction algorithms PR FSPAPR Data sets PRs time Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 168359 02246 03234 00222 08234 00494 250781 43400 01156 00104 06344 00788 7786959 294587 2241219 73887 6981016 548386 FSPAPRs time 148438 02130 02391 00262 03922 00109 162438 02232 00813 00094 03891 00331 5516750 106770 905797 15252 2488391 65261 Table 8 The stabilities time attribute reduction algorithms SCE FSPASCE Data sets SCEs time FSPASCEs time Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 1306234 09870 38359 00614 40500 03197 1267734 157752 12156 00894 37234 03919 97491705 3088128 58915906 1810442 71073904 1057970 1261625 08873 25045 00617 16266 00422 832891 09501 07500 00677 14188 00655 81588490 2095685 22828141 730362 8612000 97081 Table 9 The stabilities time attribute reduction algorithms LCE FSPALCE Data sets LCEs time Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 2419891 13425 73328 00601 82875 06289 2289547 274934 25969 00493 79094 04949 177179594 3914628 123342729 806504 195826515 3852873 FSPALCEs time 2360313 16868 47531 01007 30938 00617 1544984 20417 14031 00554 27109 01746 143924496 992163 42525578 714054 14637391 145646 Table 10 The stabilities time attribute reduction algorithms CCE FSPACCE Data sets CCEs time FSPACCEs time Mushroom Tictactoe Dermatology Krvskp Breastcancerwisconsin Backuplargetest Shuttle Letterrecognition Ticdata2000 1339672 09331 38391 00297 46469 03029 1300047 175668 11969 00865 38016 03155 95648752 685368 59560833 437866 67264778 421287 1293531 12343 25172 00439 18016 00335 863641 09297 07406 00298 15875 01018 74403281 250001 21710000 365273 8594672 107790 PRs stability 00000 00000 00000 00000 02142 01692 00675 00652 01733 02736 04187 01830 00250 00750 02222 02020 02058 00862 SCEs stability 00000 00000 01111 01111 05312 01000 00675 00652 03562 03099 03599 02521 00250 00750 01689 01823 02485 00830 LCEs stability 00000 00000 01778 00889 01852 01783 00675 00652 02333 01528 01617 01630 00250 00750 01914 01436 01744 01192 CCEs stability 00000 00000 01778 00889 02735 01698 00733 00780 01200 01600 03426 01780 00250 00750 01370 01450 01742 00894 FSPAPRs stability 00000 00000 00000 00000 02142 01692 00675 00652 01733 02736 04187 01830 00250 00750 02222 02020 02058 00862 FSPASCEs stability 00000 00000 01111 01111 05312 01000 00675 00652 03562 03099 03599 02521 00250 00750 01689 01823 02485 00830 FSPALCEs stability 00000 00000 01778 00889 01852 01783 00675 00652 02333 01528 01617 01630 00250 00750 01914 01436 01744 01192 FSPACCEs stability 00000 00000 01778 00889 02735 01698 00733 00780 01200 01600 03426 01780 00250 00750 01370 01450 01742 00894 rank preservation signiﬁcance measures attributes algorithms PR FSPAPR Theorem 5 Theorem 6 From Tables 810 draw conclusions 47 Related discussion In subsection summarize advantages acceleratorpositive approximation attribute reduction offer explanatory comments Based experimental evidence aﬃrm Each accelerated algorithms preserves attribute reduct induced corresponding original 616 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 In Section 43 proved rank preservation signiﬁcance measures attributes implies selects attribute loop modiﬁed algorithms corresponding original Naturally obtain attribute reduct data set Hence accelerated algorithms affect attribute reduct induced corresponding method Each accelerated algorithms usually comes substantially reduced computing time compared time corresponding original algorithm Through acceleratorpositive approximation size data set reduced loop modiﬁed algorithms Therefore computational time determining partitions signiﬁcance measures attributes judging stopping criterion reduced data set smaller encountered entire data set Evidently modiﬁed algorithms outperform previous methods The performance modiﬁed algorithms getting better presence larger data sets larger data set profound computing savings The stopping criterion attribute reduction stricter data set larger number attributes reduct induced heuristic attribute reduction algorithm usually bigger In situation modiﬁed algorithms delete objects data set loops far time attribute reduction The greater size data set larger number attributes selected better performance modiﬁed algorithms comes computing time Hence accelerated algorithms particularly suitable dealing attribute reduction largescale data sets high dimensions 5 Conclusions To overcome limitations existing heuristic attribute reduction schemes study theoretic framework based rough set theory proposed called positive approximation accelerate algo rithms heuristic attribute reduction Based framework general heuristic feature selection algorithm FSPA presented Several representative heuristic attribute reduction algorithms encountered rough set theory revised modiﬁed Note modiﬁed algorithms choose feature subset original tribute reduction algorithm Experimental studies pertaining UCI data sets modiﬁed algorithms signiﬁcantly reduce computing time attribute reduction producing attribute reducts classiﬁcation accuracies coming original methods The results attribute reduction based positive approximation effective accelerator eﬃciently obtain attribute reduct Acknowledgements We like thank anonymous reviewers valuable comments suggestions The authors wish thank PhD candidate Feng Wang numeric experiments data statistics months Key Laboratory Computational Intelligence Chinese Information Processing Ministry Education usage 1000 CPU hours computers InterR CoreTM2 Quad CPU Q9400 266 GHz 337 GB memory empirical study This work supported National Natural Science Foundation China Nos 60773133 60903110 70971080 Na tional Key Basic Research Development Program China 973 No 2007CB311002 GRF CityU 113308 Govern ment Hong Kong SAR National High Technology Research Development Program China No 2007AA01Z165 Natural Science Foundation Shanxi Province China Nos 2008011038 20090210171 References 1 JG Bazan A comparison dynamic nondynamic rough set methods extracting laws decision tables L Polkowski A Skowron Eds Rough Sets Knowledge Discovery 1 Methodology Applications Studies Fuzziness Soft Computing PhysicaVerlag Heidelberg Germany 1998 pp 321365 2 JG Bazan HS Nguyen SH Nguyen P Synak J Wróblewski Rough set algorithms classiﬁcation problems L Polkowski S Tsumoto TY Lin Eds Rough Set Methods Applications New Developments Knowledge Discovery Information Systems SpringerVerlag Heidelberg Germany 2000 pp 4988 3 RB Bhatt M Gopal On fuzzyrough sets approach feature selection Pattern Recognition Letters 26 2005 965975 4 RB Bhatt M Gopal On compact computational domain fuzzyrough sets Pattern Recognition Letters 26 2005 16321640 5 MR Chmielewski JW Grzymala Busse Global discretization continuous attributes preprocessing machine learning International Journal Approximate Reasoning 15 4 1996 319331 6 M Dash H Liu Consistencybased search feature selection Artiﬁcial Intelligence 151 2003 155176 7 I Düntsch G Gediga Uncertainty measures rough set prediction Artiﬁcial Intelligence 106 1998 109137 8 G Gediga I Düntsch Rough approximation quality revisited Artiﬁcial Intelligence 132 2001 219234 9 JW GrzymalaBusse An algorithm computing single covering JW GrzymalaBusse Ed Managing Uncertainty Expert Systems Kluwer Academic Publishers 1991 p 66 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 617 10 JW GrzymalaBusse LERSa learning examples based rough sets R Slowinski Ed Intelligent Decision Support Handbook Applications Advances Rough Set Theory Kluwer Academic Publishers 1992 pp 318 11 JW Guan DA Bell Rough computational methods information systems Artiﬁcial Intelligence 105 1998 77103 12 I Guyon A Elisseeff An introduction variable feature selection Journal Machine Learning Research 3 2003 11571182 13 R Jensen Q Shen Semanticspreserving dimensionality reduction rough fuzzyroughbased approaches IEEE Transactions Knowledge Data Engineering 16 12 2004 14571471 14 R Jensen Q Shen Computational Intelligence Feature Selection Rough Fuzzy Approaches IEEE PressWiley Sons 2008 15 K Kira LA Rendell The feature selection problem traditional methods new algorithm Proc AAAI 92 1992 129134 16 R Kohavi GH John Wrappers feature subset selection Artiﬁcial Intelligence 97 12 1997 273324 17 M Kryszkiewicz P Lasek FUN fast discovery minimal sets attributes functionally determining decision attribute Transactions Rough Sets 9 2008 7695 18 M Kryszkiewicz Comparative study alternative type knowledge reduction inconsistent systems International Journal Intelligent Systems 16 2001 105120 19 XH Hu N Cercone Learning relational databases rough set approach International Journal Computational Intelligence 11 2 1995 323338 20 QH Hu ZX Xie DR Yu Hybrid attribute reduction based novel fuzzyrough model information granulation Pattern Recognition 40 2007 35093521 21 QH Hu DR Yu ZX Xie JF Liu Fuzzy probabilistic approximation spaces information measures IEEE Transactions Fuzzy Systems 14 2 2006 191201 22 QH Hu DR Yu ZX Xie Informationpreserving hybrid data reduction based fuzzyrough techniques Pattern Recognition Letters 27 5 2006 414423 23 CK Lee GG Lee Information gain divergencebased feature selection machine learningbased text categorization Information Processing Management 42 2006 155165 24 DY Li B Zhang Y Leung On knowledge reduction inconsistent decision information systems International Journal Uncertainty Fuzziness KnowledgeBased Systems 12 5 2004 651672 25 JY Liang KS Chin CY Dang CM Yam Richid A new method measuring uncertainty fuzziness rough set theory International Journal General Systems 31 4 2002 331342 26 JY Liang ZB Xu The algorithm knowledge reduction incomplete information systems International Journal Uncertainty Fuzziness KnowledgeBased Systems 10 1 2002 95103 27 JY Liang YH Qian CY Chu DY Li JH Wang Rough set approximation based dynamic granulation Lecture Notes Computer Science 3641 2005 701708 28 H Liu R Setiono Feature selection discretization IEEE Transactions Knowledge Data Engineering 9 4 1997 642645 29 JS Mi WZ Wu WX Zhang Comparative studies knowledge reductions inconsistent systems Fuzzy Systems Mathematics 17 3 2003 5460 30 M Modrzejewski Feature selection rough set theory Proceedings European Conference Machine Learning 1993 pp 213226 31 HS Nguyen Approximate Boolean reasoning Foundations applications data mining Lecture Notes Computer Science 3100 2006 334506 32 T Pavlenko On feature selection curseofdimensionality error probability discriminant analysis Journal Statistical Planning Infer ence 115 2003 565584 33 Z Pawlak Rough Sets Theoretical Aspects Reasoning Data Kluwer Academic Publishers Boston 1991 34 Z Pawlak A Skowron Rudiments rough sets Information Sciences 177 1 2007 327 35 Z Pawlak A Skowron Rough sets extensions Information Sciences 177 2007 2840 36 Z Pawlak A Skowron Rough sets boolean reasoning Information Sciences 177 1 2007 4173 37 W Pedrycz G Vukovich Feature analysis information granulation fuzzy sets Pattern Recognition 35 2002 825834 38 L Polkowski On convergence rough sets R Slowinski Ed Intelligent Decision Support Handbook Applications Advances Rough Set Theory vol 11 Kluwer Dordrecht 1992 pp 305311 39 YH Qian JY Liang Combination entropy combination granulation rough set theory International Journal Uncertainty Fuzziness KnowledgeBased Systems 16 2 2008 179193 40 YH Qian JY Liang CY Dang Converse approximation rule extraction decision tables rough set theory Computer Mathematics Applications 55 2008 17541765 41 YH Qian JY Liang CY Dang Consistency measure inclusion degree fuzzy measure decision tables Fuzzy Sets Systems 159 2008 23532377 42 YH Qian JY Liang CY Dang Interval ordered information systems Computer Mathematics Applications 56 2008 19942009 43 YH Qian JY Liang CY Dang DW Tang Setvalued ordered information systems Information Sciences 179 2009 28092832 44 YH Qian JY Liang DY Li HY Zhang CY Dang Measures evaluating decision performance decision table rough set theory Information Sciences 178 2008 181202 45 YH Qian JY Liang CY Dang Incomplete multigranulations rough set IEEE Transactions Systems Man Cybernetics Part A 40 2 2010 420431 46 R Quinlan Induction decision rules Machine Learning 1 1 1986 81106 47 MW Shao WX Zhang Dominance relation rules incomplete ordered information International Journal Intelligent Systems 20 2005 1327 48 Q Shen R Jensen Selecting informative features fuzzyrough sets application complex systems monitoring Pattern Recognition 37 2004 13511363 49 A Skowron Extracting laws decision tables rough set approach Computational Intelligence 11 1995 371388 50 D Slezak Approximate reducts decision tables Research report Institute Computer Science Warsaw University Technology 1995 51 D Slezak Foundations entropybased Bayesian networks theoretical results rough set based extraction data IPMU00 Proceedings 8th International Conference Information Processing Management Uncertainty KnowledgeBased Systems vol 1 Madrid Spain 2000 pp 248255 52 D Slezak Approximate entropy reducts Fundamenta Informaticae 53 34 2002 365390 53 RW Swiniarski A Skowron Rough set methods feature selection recognition Pattern Recognition Letters 24 2003 833849 54 GY Wang H Yu DC Yang Decision table reduction based conditional information entropy Chinese Journal Computer 25 7 2002 759766 55 GY Wang J Zhao JJ An A comparative study algebra viewpoint information viewpoint attribute reduction Fundamenta Informaticae 68 3 2005 289301 56 SX Wu MQ Li WT Huang SF Liu An improved heuristic algorithm attribute reduction rough set Journal System Sciences Informa tion 2 3 2004 557562 618 YH Qian et al Artiﬁcial Intelligence 174 2010 597618 57 WZ Wu M Zhang HZ Li JS Mi Knowledge reduction random information systems DempsterShafer theory evidence Information Sci ences 174 2005 143164 58 ZY Xu ZP Liu BR Yang W Song A quick attribute reduction algorithm complexity maxO CU O C2U C Chinese Journal Com puter 29 3 2006 391398 59 YY Yao Information granulation rough set approximation International Journal Intelligent Systems 16 1 2001 87104 60 W Ziarko Variable precision rough set model Journal Computer System Sciences 46 1993 3959