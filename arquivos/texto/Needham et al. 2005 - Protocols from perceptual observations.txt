Artiﬁcial Intelligence 167 2005 103136 wwwelseviercomlocateartint Protocols perceptual observations Chris J Needham Paulo E Santos 1 Derek R Magee Vincent Devin 2 David C Hogg Anthony G Cohn School Computing University Leeds Leeds LS2 9JT UK Received 28 July 2004 received revised form 14 February 2005 accepted 14 April 2005 Available online 27 July 2005 Abstract This paper presents cognitive vision capable autonomously learning protocols perceptual observations dynamic scenes The work motivated aim creating syn thetic agent observe scene containing interactions unknown objects agents learn models sufﬁcient act accordance implicit protocols present scene Discrete concepts utterances object properties temporal protocols involving concepts learned unsupervised manner continuous sensor input Crucial learning process methods spatiotemporal attention applied audio visual sensor data These identify subsets sensor data relating discrete concepts Clustering contin uous feature spaces learn object property utterance models processed sensor data forming symbolic description The PROGOL Inductive Logic Programming subsequently learn symbolic models temporal protocols presented presence noise representation symbolic data input The models learned drive synthetic agent interact world seminatural way The evaluated domain tabletop game playing shown successful learning protocol behaviours realworld audiovisual environments 2005 Elsevier BV All rights reserved Corresponding author Email address chrisncompleedsacuk CJ Needham 1 Paulo Santos Centro Universitario da FEI Sao Paulo Brazil 2 Vincent Devin France Telecom RD Meylan France 00043702 matter 2005 Elsevier BV All rights reserved doi101016jartint200504006 104 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 Keywords Cognitive vision Autonomous learning Unsupervised clustering Symbol grounding Inductive logic programming Spatiotemporal reasoning 1 Introduction This paper presents cognitive vision capable autonomously learning proto cols involving rudimentary language visual objects In models visual objects utterances obtained unsupervised statistical learning algorithms In der form symbolic data input inductive logic programming ILP continuous perceptual observations transformed models learned Perceptual observations taken sensory input acoustic visual inputs The concept qualitative time introduced key frames deemed importance The direct link perception action exploited change perceived brought action The ILP construct sets deﬁnite clauses express rules behaviour perceived actions sym bolic description scenes In particular protocols learned encode connection utterances visual objects occur scene This intrinsically solu tion anchoring problem 8 instance symbol grounding problem 15 The sets deﬁnite clauses obtained synthetic agent perform actions world Therefore work explore closing loop learn ing connection perception action bridging gap vision pattern recognition symbolic knowledge discovery The framework presented evaluated domain simple tabletop games In domain able learn complete protocols rules games different verbal utterances aspects dynamics involved playing game instance objects placed table The en tire learning process executed minimal human intervention assumes minimal domain speciﬁc knowledge scenes game concepts In earlier work 22 demonstrated approach capable learning simple mathematical concepts numerical ordering equality 11 The domain tabletop games We chosen work domain simple tabletop games involving interaction players small number visual objects incorporating spoken utterances The reason choosing scenarios games contain rich protocols complexity controlled adding excluding modifying rules actions andor objects domain Moreover argued realworld social interaction scenarios modelled games 14 suggests framework relevant development fully autonomous learn behave real world Experimental data collected standard PCs webcams microphone arrangement shown Fig 1 The games work described greater Section 4 Brieﬂy following games played CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 105 Fig 1 Example data collection phase A game played table players One camera points table captures face participant replaced synthetic agent The audio captured microphone worn participant 1 SNAP1 A generalised game snap cards played simultaneously fol lowed utterance dependent ﬁgures cards The utterances colour colours ﬁgures shape shapes ﬁgures shape colour match feature ﬁgures match The cards removed play uttered indicating play cards 2 SNAP2 A variation game cards placed resulting utterance dependent card visible card previous time step 3 Paperscissorsstone PSS Played sets cards depicting paper scissors stone Two players simultaneously select object cards place table When ﬁgures cards perceived utterances win lose draw spoken players simulated synthetic agent This player says win card beats shown playerpaper beats wraps stone scissors beats cuts paper stone beats blunts scissors A play uttered cards table 12 Overview An overview framework illustrated Fig 2 Firstly attention mechanisms necessary pick salient interesting sounds objects features audio visual input streams obtained setup similar shown Fig 1 There phases operation training execution In training learning phase synthetic agent observes world participating In phase class induction performed blobs sounds perceived class models formed classiﬁcation perceptual objects seen training These form symbolic 106 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 Fig 2 Overview learning framework synthetic agent data description input signals Protocol induction performed symbolic data stream set protocol models rules formed In execution play phase synthetic agent participates games learned protocol models An inference engine protocol instantiation fer synthetic agents vocal response current symbolic description world We assume knowledge type objects sounds presented sys tem Therefore unsupervised clustering audio visual feature vectors performed Models learned based clustering classify perceptual inputs classes This provides symbolic description input signals Protocol models represented ordered sets deﬁnite clauses expressed Prolog syntax This provides necessary ﬂexibility represent relations objects actions require Others previously demonstrated utility subsets ﬁrstorder predicate logic highlevel scene interpretation Neumann Weiss 30 Inductive Logic Programming ILP form PROGOL 28 protocol induction The reason choosing PROGOL resides capability construct theories possibly noisy positive examples This coincides aim learn proto col behaviour observation unsupervised way Moreover ILP enables concept generalisation extend rules learned observation certain objects unseen entities knowledge learned presented format allows assess complexity accuracy serving tools reasoningthe symbolic theories obtained work construct equivalence classes utterances order cope overclustering utterances audio signal Once learning complete synthetic agent process perceptual inputs infer appropriate action response state world realtime The interactive agent grounds learned perceptual models learned protocols descriptions objects real world plays suitable utterance The experiments reported paper evaluated new deﬁnitions sound ness completeness inspired homonyms assess semantics CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 107 inference systems Soundness accounts extent intended protocol tended semantics represented rules obtained In contrast completeness measures rules agree intended semantics The remainder paper structured follows Section 2 reviews previous work learning audiovisual input Section 3 details discussing applica tion ideas presented Section 4 describes methods evaluation presenting experiments results Discussion approach pre sented Section 5 alongside future research directions conclusions drawn Section 6 2 Background Most previous work modelling connection language world vision included capacity learning Typically aim sign sufﬁciently rich conceptual framework characterise activities target domain 42041 Nagel 29 example uses motion verbs conceptual framework mediate video sequences depicting trafﬁc scenes natural lan guage descriptions scenes More recently Siskind proposed use event logic map visual input single motion verbs pick stack 38 A key issue scalability conceptual frameworks designed hand prototype domain This motivated use learning procedures populate conceptual framework automatically generalisation extended observation target audiovisual language visual domain Cangelosi Parisi 6 adopt approach perceptionaction setting visual input language command determine action movement robotic arm By representing mapping feedforward neural network standard learning procedure set parameters network examples visual language inputs paired resulting actions For simpler domain involving visual language input Barnard et al 5 model relationship features image regions single words naming regions sky water joint probability distribution This distribution learnt EM algorithm subsequently purposes probabilistic inference example generating conditional distribution given gion Roy Pentland 35 demonstrate use mutual information audio video streams segment words corpus containing phoneme sequences associated simultaneous visual situations The corpus video sound recording care givers interacting infants playing variety objects The works raw speech utterances produces phoneme sequences trained recurrent neural network hidden Markov model Our work addresses general problem unsupervised learning game protocols corpus raw visual acoustic data depicting target scenarios A problem tackled earlier work likely generation acoustic visual 108 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 object classes unsupervised learning With exceptions Roy Pentland 35 ideal categorisation imposed supervised learning prior design early processing bypassed altogether replaced symbols denoting acoustic visual objects If unsupervised learning classes generated impossible learn structural concepts depend reﬁned categorisation If classes generated uncorrected redundant concepts generated identical structure involve distinguished semantically equivalent objectsthis impede learning training examples required separate instance A related problem misclassiﬁcation objects Within learning context earlier work integrating language vision addressed number audiovisual tasks involving different levels language complexity At sim plest task naming visual objects involving single words isolation 5 At end spectrum Roy 34 addresses task describing objects noun phrases optionally contain shorter constituent noun phrase pink square green rectangle peach rectangle From word sequences segmented acoustic signal joint probability distribution consecutive words bigrams esti mated relative frequencies turn generate legal utterances describing novel visual situations This bigram model equivalent stochastic regular grammar principle extended higher order language model instead sto chastic context free grammar learning grammar rules challenging Interestingly stochastic grammars model visual activities 217 25 characterised temporal sequence visual primitives However ideally suited visual modelling tasks involve essentially nonsequential data example spatial relationships single visual image One way forward learning paradigm adopt representational formalism sufﬁciently powerful model concepts target domain event logic Fern et al 11 attempt learn necessary background knowledge connecting language vision formalism In work adopted logic programs expressed Prolog syntax highlevel formalism inverse entailment embodied PROGOL learning procedure A limitation Prolog context absence mechanism representing uncertainty example spatial relationship objects action taken given situation Recent proposals combining predicate logic probabilistic inference overcome limitation assuming equivalent inductive inference procedures developed 33 For example Markov networks successfully learning reasoning spatial temporal uncertainties physical situations represented ﬁxed conjunction binary predicates 16 PROGOL infers sets Horn clauses statistical optimisation procedure deals occasional misclassiﬁed objects Like Roy 34 work directly raw acoustic visual data language domain contains single word ut terances A key challenge future automate learning richer conceptual descriptions currently handcrafted Nagel 29 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 109 Fig 3 Overview learning framework 3 Learning perceptual categories symbolic protocols In section present details implemented models visual objects utterances symbolic description protocol learned purely unsupervised manner Fig 3 illustrates overall learning scheme 31 From continuous symbolic data Video streams dynamic scenes contain huge quantities data irrelevant scene learning interpretation An attention mechanism required identify interesting parts stream terms spatial location temporal location For autonomous learning models heuristics required determine Such models based motion novelty high low degree spatial variation number factors It highly likely single factor provide generic attention mechanism learning interpretation scenarios It view attention multiple cues required fully generic learning This section details attention mechanisms segment continuous au dio video streams describes algorithms classify features extracted input signals order create symbolic data protocol learning Our aim develop methods sufﬁciently robust handle environments outside laboratory 311 Attention learning classiﬁcation visual objects For implementation gameplaying domain attention mechanism identify salient areas space time necessary We chosen motion straightforward work The spatial aspect attention mechanism based generic blob tracker 23 works principle multimodal Gaussian mixture background modelling foreground pixel grouping This identiﬁes cen troid location bounding box pixel segmentation separable moving object scene frame video sequence The temporal aspect attention mecha nism identiﬁes keyframes qualitatively zero motion number frames typically preceded number frames typically containing signiﬁcant motion Motion deﬁned change objects centroid bounding box threshold value standard deviations tracker positional noise typically ﬁve pixels experiments This method temporal attention based domain lim 110 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 Fig 4 Sequence keyframes identiﬁed spatiotemporal attention mechanism Intermediate frames objects motion subsequently remain stationary ﬁltered attention mechanism iting assumption objects remain motionless following change state process change important Fig 4 shows example sequence keyframes identiﬁed attention mechanism Features objects identiﬁed blob tracker keyframes identiﬁed attention mechanism extracted clustered Currently groups features extracted corresponding attributes texture colour position The texture feature classify global shape star addition conventional textures The remainder section discusses methods unsupervised clustering formation classiﬁers high dimensional feature vectors describing colour texture applied features wish use example global shape local shape local textures For experiments position divided classes pos0 objects left image pos1 objects right image This simplicity sufﬁces domains question demonstrates spatial position incorporated subsequent symbolic learning process Position learned similar way groups features Rotationally invariant texture features The texture feature set paper based Gabor wavelet 9 convolution based textural descriptions Fig 5 These applied object cen troid provided object tracker form 94dimensional feature vector To features rotationally invariant sets features multi ple orientations applied images statistics mean minimum maximum maximummean feature subsets feature description raw values The features Gabor wavelets scales equal covari ance Gaussians scales nonequal covariance Gaussians scales polar waveletlike features deﬁned convolution masks form Kr θ Gr σ sinK1θ K2 1 r distance pixel mask object centroid θ angle line object centroid pixel mask subtends horizontal axis Gr σ Gaussian distribution standard deviation σ centred object centroid K1 relates angular frequency K1 1 2 4 experiments K2 relates ﬁlter orientation typically values K2 chosen order form bank ﬁlters Convolution kernels form Eq 1 applied scales altering σ The scales chosen feature descriptor types selected approximately evenly distributed sensible limits general possible CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 111 Fig 5 Example wavelet convolution based textural feature descriptors Colour features The colour feature set paper consists bins 5 5 HueSaturation HS colour histogram The blob tracker attention mechanism provides binary pixel mask deﬁning pixels belonging object identiﬁed The RGB colour values pixel projected HueSaturationIntensity HSI colourspace 39 Hue Saturation Intensity independent dimensions space build 2D histogram object pixel colour distribution Each dimension quantised 5 levels compromise generality speciﬁcity representation histogram 25 bins 25dimensional feature description Clustering feature selection For attribute texturecolour wish form clusters features vectors deﬁne different classes denoted attribute labels Since knowledge class feature description belongs unsupervised clustering approach taken There number methods literature unsupervised clustering Kmeans 13 agglomerative methods 137 graph partitioning based methods 18 Such methods generally work associating similar data items ndimensional feature space All features clustering process feature selection usually seen supervised preprocessing step performed An alternative approach combine results multiple clusterings multiple methods single feature set single method multiple data sets 40 In highdimensional feature vectors texture responses variety convolution masks colour histogram bins subset features important task classifying visual objects More importantly different descriptors im portant different scenarios depending visual objects It reason variety textural descriptors form 94dimensional texture feature vector We use feature selection method based agreement features select important Firstly clustering performed independently dimension feature space 1D subspaces order induce Nc clusterings feature vectors These clusterings combined form weights edges graph data items nodes This graph partitioned form clusters data items Finally supervised learning classiﬁer performed cluster membership data items graph partitioning We use agglomerative clustering subspace based loosely presented 1 To begin data subspace normalised zero mean unit standard deviation This performed stopping criterion subspace clustering applied subspaces One cluster data item 112 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 initialised For cluster pairs Ci Cj containing Ni Nj members respectively calculate DCi Cj mean distance member Ci member Cj DCi Cj 1 NiNj cid1 cid1 sCi tCj s t 2 Iteratively closest clusters Ci Cj merged whilst minDCi Cj T T ﬁxed threshold The value T K d d dimensionality subspace 1 K constant 1 experiments This agglomerative clustering performed 1D subspaces results clusterings com bined novel weighted version clusterbased similarity partitioning algorithm Strehl Ghosh 40 In original algorithm nondirectional fully connected weighted graph formed vertices unweighted represent data items weighted edges represent similarity pairs data items Similarity mea sured number times pair data items occur cluster subspace clusterings This graph partitioned graph partitioning algorithm 183 Such algorithms attempt form set subgraphs edge cut required minimised This NP complete problem number suitable approximate methods disposal We extend method 40 form graph similarity redeﬁned sum weights relating subspace data items occur cluster Eq 3 In way relative importance individual features feature description weighted EdgeWeightVa Vb cid2 Nc c1 WcScVa Vb Nc c1 Wc cid2 3 Wc weight associated clustering c ScVa Vb 1 data items relating vertices Va Vb contained cluster clustering c 0 Nc number clusterings It noted denominator simply constant effect clustering produced However inclusion normalisation enables edge weights edge cuts compared different feature sets weightings Once initial clustering performed equal weights discrimina tive power individual subspaces build clustering evaluated respect clustering assuming clustering correct If subspace discriminate classes discrimination power good unusual lowdimensional subspace 1D experiments able discrimi nate classes We reweight edges graph partition data clusters reﬂect correct current case initial clustering This involves evaluating pair clusters discriminated subspace 3 Implementations graph partitioning algorithms freely available download httpwww userscsumnedukarypis METIS CLUTO packages CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 113 Fig 6 Accuracy stochastic classiﬁer class discrimination To consider accuracy class classiﬁer based particular sub space clustering C1 C2 If class labelled dataset available classiﬁer formed statistics associations classes clusters Eq 4 P class Ci X X class X Ci Y Y Ci 4 However class membership known case If clustering obtained graph partitioning method assumed correct class labelling classiﬁer formed Fig 6 illustrates theoretical accuracy classiﬁer Bayesian network In Fig 6 diagram left illustrates probability distribution data items twoclass discrimination problem From diagram easy joint proba bility P L Ci L label true class A B Ci subspace clusters given cid3 P L Ci P Ci LP L cid4 P A P B 5 On right heading P Correct alongside node probability P L Ci classiﬁcation correct stochastic classiﬁer P Correct L Ci The sum products P Correct L Ci P L Ci pairs clusters labels gives overall probability correct classiﬁcation classiﬁer based particular lowdimensional cluster set C C1 C2 Cn particular pair labellings A B P Correct A B C cid1 LAB P L P A P B cid1 Ci C cid5 cid6 P Ci LP L Ci 6 P A P B P Ci L P L Ci calculated cooccurrence frequency matrix clusters formed graph partitioning vs subspace clusters training data Thus deﬁne discriminative power DC maximum value P Correct A B C pairs classes Eq 7 DC max abABCab P Correct b C 7 DC lies 05 discriminative power 10 perfect discrimination pair classes This value weight graph partitioning based 114 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 clustering described Eq 3 better approach threshold value use binary weights Eq 8 We use fairly low threshold Tw 06 exclude subspaces poorly discriminative Essentially unsupervised feature selection approach cid7 Wc 0 DC Tw 1 DC cid1 Tw 8 In principle method applied iteratively repeatedly clustering output calculate new weights reclustering Our experiments suggest improved performance convergence occurs circumstances circum stances overﬁtting stability problems observed We use single application reweighting reason It noted approach relies initial clustering having onetoone mapping true class labelling 50 items cluster belong true class If case unlikely improvement obtained Once set examples partitioned partitions supervision conventional supervised statistical learning algorithm MultiLayer Perceptron Radial Basis Function Vector Quantisation based nearest neighbour classiﬁer implementation This allows construction models encapsulate information clustering way easily efﬁciently applied novel data These models generate training data suitable symbolic learning For object identiﬁed attention mechanism symbolic property associated semantic group learned classiﬁers 312 Attention learning classiﬁcation spoken utterances A symbolic description audio events obtained input audio signal This task relatively straightforward dealing small number isolated sounds generally single words Speech recognition production software perform task HTK 42 normally requiring supervised learning 71232 unsupervised approach task favoured ﬁt philosophy learning models rules autonomous synthetic agent Such approach advantage participants nonword utterances animal noises sounds objects instruments The methods presented section crude adequate purposes For complex applications sophisticated techniques likely required The participant game wish replace synthetic agent speaks microphone The audio sampled 8172 Hz attention mechanism based energy signal segment sounds Nonoverlapping windows formed containing 512 samples power 2 needed Fourier transform giving windows similar duration frame video The energy window calculated sum absolute values samples Utterances contiguous windows energy threshold maximal duration Fig 7 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 115 Fig 7 The attention mechanism segments audio signal Fig 8 Spectral analysis Spectral analysis performed detected window The dimensionality spectrum reduced 512 174 histogramming absolute values Reducing spectrum dimensionality makes clustering robust small variations pitch voice 19 Each utterance detected represented temporal sequence L reduceddimensionality windows L chosen equal length shortest utterance windows training set This achieved linearly resampling temporal sequence spectral histograms windows span utterance The utterances identical length Kmeans clustering performed set utterances times different numbers clusters Fig 8 shows set clusters formed run process visualised spectrograms plots frequency vs normalised time relating cluster centres The number clusters automatically chosen ratio mean distance utterance centre closest cluster mean distance cluster centres minimised Each utterance training set classiﬁed nearest cluster centre create symbolic data stream shown Fig 9 utterance 4 This depends rate audio sampled 8172 Hz fundamental pitch frequency human voice average 275 Hz size window 512 samples equivalent windows 16 Hz 27516 approximately 17 116 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 represented symbolically set labels utt1utt2 This method automatically choosing number clusters tends overcluster data clusters created approach problem build equivalence classes utterances discussed Section 332 32 Symbolic data representation To drive synthetic agent capable interacting environment wish learn protocols agents actions In work actions vocal utterances principle applicable possible action types movement robotic arm Our aim encapsulate protocol activity way actions ﬁred following observation perceptual inputs Thus utterances par ticipant described form actionutterance time The playing area described list objects described attributes texture colour position form statetexture1 colour1 position1texture2 colour2 position2time represents state containing objects In addition time timetime temporal successor successorprevious time current time denoted Exam ple audiovisual input SNAP1 corresponding symbolic data stream shown Fig 9 When table description perceptual inputs list objects tracked statet518 Further details discussed Section 331 Our synthetic agent interacts environment actions utterances ceives environment visual input objects playing area Thus action agent ﬁred immediately interpretation perceptual input states Therefore symbolic data presented protocol learning wish actions occur qualitative time states quantitatively occur succes sively This creates direct link perception action Our prototype works real time live input data As frame rate tracker variable depending number objects tracking motion scene features extracting images clustering Thus use frame numbers sufﬁcient link times input video stream utterances obtained processing separate audio stream To form direct link states actions beginning training phase clock started number seconds start recorded state time successor triple object tracker The number seconds start recorded action utterance audio stream The timings sym bolic data stream audio synchronised timings video data stream Each utterance backdated timing previous state description timing coincident This simple method works application A complex approach undertaken provide richer description tem poral successor relationships events descriptions useful extended temporal events future tracking movement objects particularly applied learning ro botic actions temporal representation Allens interval calculus 331 useful CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 117 Fig 9 Example audiovisual inputs corresponding symbolic data representation game SNAP1 The symbols correspond perceptual categories learned unsupervised clustering In case symbols utt10 utt9 utt1 utt6 symbols utterances representing respectively words colour play shape texX relate textureshape objects colX relate colour objects posX objects position The textual description italics provided readers viewing greyscale copy For colours web version article 33 Learning symbolic protocols perceptual data 331 Inductive logic programming Inductive logic programming ILP 2126 given ﬁeld AI studies inference methods given background knowledge B examples E 118 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 modeh1actionutterancetime modeb100stateshapecolourposition shapecolourpositiontime modeb100statetime modeb100successortimetime Fig 10 Mode declarations game SNAP1 simplest consistent hypothesis H B H E B H E logic programs informally B H true E true In terms seek induce simple logic program H combined state successor atoms training data B entails action atoms E No generic domain knowl edge included background B5 Our work uses CProgol44 28 implementation deﬁnite modes lan guage 27 PROGOL searches statistically plausible hypothesis H inverse entailmenta depthﬁrst search constructed hypotheses PROGOL works generalising set examples given list types accounting categories objects domain consideration set syntactic biases These syntactic biases user deﬁned mode declarations restrict possible form proposed generalisations restricting possible search space Mode declarations pred icates argument types head body hypothesised generalisations These declarations state variables argument predicates head body formulae formulae sought In words variables declared modes input output grounded constant An example mode declarations input data described Section 32 shown Fig 10 modeh modeb represent respectively predicates head body generalising formulae The symbols rep resent respectively input output grounded variables terms shape colour position time term types The number ﬁrst argument modeh modeb bounds number alternative solutions instantiating predicate stated second argument The mode declarations Fig 10 state generalising formula predicate action2 head grounded utterance input variable time arguments predicates state2 successor2 body In principle limit number occurrences predicate mode declarations different combinations input output grounded variables Examples PROGOL type declarations game SNAP1 shown Fig 11 The constant symbol names chosen example clear read practice semantic meaning object property utterance labels unknown labelling come lower level systems devoid semantics provided lower level grounding Although PROGOL capable generalising positive negative examples positive examples available 5 Implicit domain knowledge provided mode declarations CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 119 utteranceplay shapering utterancecolour shapestar utteranceshape utteranceboth utterancenothing colourred positionpos0 colourgreen positionpos1 shapeflash colourblue Fig 11 Type declarations game SNAP1 stateA 1 actionplayA stateBCDECFA 2 actioncolourA 3 actionshapeA stateBCDBEFA 4 actionnothingA stateBCDEFGA stateBCDBCEA 5 actionbothA Fig 12 Unordered PROGOL output game SNAP1 Brieﬂy PROGOLs search described follows For positive example PRO GOL initially generates speciﬁc Horn clause according mode declarations This clause contrasted remaining examples search general formula capable subsuming examples data set An example PRO GOLs output SNAP1 game given mode type declarations described presented Fig 12 Formula 1 Fig 12 shows PROGOL built connection utterance play state The variable A head actionplayA body stateA indicates generalisation time A represents ungrounded time state time Thus formula 1 interpreted table time A respond time A action uttering play Formulae 25 rules learned nonempty states expressed occurring variables arguments state2 Eg formula 2 repeated variable C second position objects feature list encodes condition objects colour appropriate action utter colour Note rules unordered constitute logic program correctly speciﬁes intended protocol In Section 34 discuss inference engine orders rules The key motivation use inductive logic programming generated rules applied situations seen For example complete rules SNAP1 learned example training data utterance colour red objects scene generalise examples training data colour uttered green blue objects scene Moreover learning object classiﬁers texture colour allowed run time play mode rules learned applicable objects features having seen training 332 Building equivalence classes utterances As mentioned Section 312 method utterance clustering prone cluster true number clusters However able use generalisation rules PROGOL construct equivalence classes utterances The procedure 120 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 generating equivalence classes based hypothesis rules similar bodies related equivalent utterances rule heads There possible ways deﬁning similarity logic programs 10 investigation outside scope paper As shall Section 42 work use uniﬁcation identity bodies similarity measures clause bodies To construct equivalence classes ﬁrstly pair input rules heads form actionutterancetime checked bodies sim ilar Clauses bodies discarded provide evidence equivalence Assume transitive reﬂexive symmetric binary relation equiv2 ﬁned utterances Thus pair clause heads bodies similar statement equiv2 created asserted stating hypothesis equivalence tween utterances arguments For instance let actionutt_it_x actionutt_jt_y clause heads similar bodies hypothesis equivalence utterances utt_i utt_j created asserted statement equivutt_iutt_j For notational convenience atomic formula equivutt_iutt_j equivalence pairs Two utterances utt_i utt_j hypothesised equivalent equivalence pair equivutt_iutt_j ap pears training examples6 Finally equivalence classes created taking transitive closure relation equiv2 The process building equivalence classes exempliﬁed game SNAP1 assume audio clustering algorithm distinct representations utterances For instance suppose algorithm clustered word play 4 distinct classes p_1 p_2 p_3 p_4 PROGOL following rules involving symbols actionp_1A stateA actionp_2A stateA actionp_3A stateA actionp_4A stateA Therefore method constructing equivalence classes suggests p_1 p_2 p_3 p_4 forms class pairwise disjoint respect classes combining remainder utterances In case bodies trivially similar identical need case In paper obtaining equivalence classes utterances leaving aside problem clustering learning models visual objects In fact problem clustering models perceptual objects setting evident audio visual domains This visual representation invariant variation orientation lighting little invariance audio representation variation pitch dynamics participant excited winning unexpected happens 6 The number times equivalence pair hypothesised equivalent training examples measure conﬁdence equivalence hypotheses providing thresholding value inclusion objects equivalence classes This issue matter future research CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 121 actionbothA stateBCDBCEA actionshapeA stateBCDBEFA stateBCDECFA actioncolourA actionnothingA stateBCDEFGA actionplayA stateA Fig 13 Sorted PROGOL output game SNAP1 corresponding unordered output Fig 12 34 Inference engine agent behaviour generation The symbolic protocols learned PROGOL Prolog program set rules drive interactive synthetic agent participate environment This set rules prior input agent automatically ﬁltered ordered according following criteria Ground atomic formulae kept beginning rule set initially given PROGOL These formulae selected agent temporal variable instantiated speciﬁc time point Nonground atomic formulae moved end data set general rules action expressed heads constrained particular perceptual input Nonatomic rules bodies unify ordered speciﬁc general body This ensures speciﬁc rules ﬁred ﬁrst subsumed general rules This ensures rule 4 Fig 12 satisﬁed perceptual input objects ordered rules 2 3 It ensures rule 5 speciﬁc ﬁrst rule Fig 13 shows result ordering ruleset Fig 12 Further examples given Section 4 The remaining rules kept order output PROGOL It worth pointing PROGOL ranks output formulae according predictive power compression respect data set rules rule set predict fewer examples handled rules The usually represent idiosyncrasies data positions rule set selected Prolog In addition cut added conjunct nonground formulas body This guarantees single action ﬁre time step The synthetic agent loaded learned rules ordered receives input perceptual outputs inferred utterance sound synthesis module This module replays audio clip appropriate response closest cluster centre automatically extracted training session For greater effect visual waiting head Fig 14 displayed screen time replay audio clip corresponding facial movement talking head The video captured audio participant replaced synthetic agent The details videorealistic waiting head focus work adds realism demonstration system7 7 Movies demonstrating work paper learning execution phases online httpwwwcompleedsacukvisioncogvis 122 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 Fig 14 Synthetic agent During training audio video stream participant captured microphone webcam objects tracked separate webcam pointing table b In execution phase participant longer says follows instructions interactive talking head displayed screen broadcast speakers c Closeup waiting head displayed screen b d The head talks replaying audiovideo stream associated inferred action As robotic arm human participant required follow instructions uttered synthetic agent Currently agent executes single action generation time step This restriction relaxed need change central ideas presented 4 Experiments results In section present experimental results applying framework learning protocol behaviour game playing scenarios Section 42 The results analysed evaluation method Section 41 based verifying soundness completeness learned rule sets Section 33 Note ordering constraints Section 34 taken account learned rule setsthey taken sets deﬁnite clauses As noted experimental results completeness cases better ordering constraints taken account Informally method provides extent given model represented learned rule set soundnessand extent predictions learned rule set agree CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 123 predictions model perceptual inputscompleteness Therefore method utilises logicbased concepts provide principled way evaluating reasons choosing symbolic learning tool framework 41 Evaluation method In order evaluate protocol rules learned noisy vision data contrast rules learned dataset handcoded set formulae hand coded rules understood underlying semantics game perfectly capture protocol rules learned If perfect learning takes place logically equivalent set rules learned vision data In practice worried exact logical equivalence actions performed identical circumstances learned formulae possibly entail statements involv ing performance actions necessarily hand coded set Therefore want evaluate 1 actions learned set agrees hand coded set 2 actions hand coded set agrees learned set Making analogy model theory inference systems ﬁrst condition soundness checks learned actions agree semantics learned rule set correctly predicts actions given gold standard hand coded set The second condition called completeness checks ac tions predicted semantics particular conditions similarly indicated learned rule set Formally characterise ideas follows Deﬁnition Agreement rule sets If X Y1 Z implies X Y2 Z Y1 agrees Y2 Z situation X Deﬁnition Total soundness completeness learned rule set respect hand crafted rule set Let A set formulae representing possible actions typically char acterised syntactically atomic formulae formed distinguished predicate Let H L sets formulae representing handcrafted learned rule sets respectively Let X represent symbolic description world given time set ground atomic formulaestate time successor actionas illustrated Figs 9 16 18 If L agrees H action θ A X L sound respect H If H agrees L action θ A X L complete respect H However practice data noisy incomplete causing rules missed overgeneralised misrepresented Therefore deﬁne partial soundness complete ness follows 124 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 Deﬁnition Partial soundness completeness learned rule set respect hand crafted rule set Let A H L X If L agrees H maximal subset actions Θ A X L sound respect H complete respect H If H agrees L maximal subset actions Θ A X L Θ A Θ A A set games performance cognitive vision The section describes game presenting typical outputs discussing experimental results based evaluation method 42 Experimental evaluation 421 Experiment 1 SNAP1 This game demonstrates systems ability generalise rules perceptual cate gories grounding instance SNAP1 generalised snap game cards played followed utterance dependent state cards colour shape The cards removed play uttered indicating play cards Fig 9 illustrates example sequence game For SNAP1 learning process completed times time dataset containing 100 objects 100 utterances equivalent 50 rounds game Fig 15 presents example typical rule set output This set shows distinct versions utterance play represented symbolic labels utt2 utt3 utt8 utt9 correctly learned rules colour shape utt6 utt10 utt1 utt4 respectively properly built suboptimal rule terms compactness required explain utt7 placed rule data set according criteria discussed Section 34 Table 1 presents evaluation runs SNAP1 according evaluation method introduced Section 41 Runs 1 2 totally sound scored 100 sound ness meaning prediction intended semantic obtained learned set perceptual input Both runs scored 100 com pleteness 714 run 1 888 run 2 idiosyncratic rules play These rules represented action play valid time point t previous time point objects table Although true fact domain question rule represent correct protocol producing utterance play allows pronounced table Run 1 ﬁnd rules colour scoring 60 respect soundness However 80 rules mapped intended semantics The misbehaviour experiment respect soundness explained erro neous clustering colour feature current implementation unreliable Analysis run 3 reveals sensitivity PROGOL misclassiﬁed data We learn ing small amounts data locally sparse classiﬁcation error signiﬁcant data form generalisation This frag ile behaviour expect SNAP1 run 3 contains examples CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 125 actionutt4t65 actionutt5t198 actionutt1t237 actionutt6A stateBCDBCEA actionutt10A stateBCDECFA actionutt1A stateBCDBEFA actionutt4A stateBCDEFGA actionutt9A stateA actionutt8A stateA actionutt2A stateA actionutt3A stateA actionutt7A successorBA stateCDEFGHB Fig 15 Sorted learned rule set SNAP1 run 1 Key symbolic utterance labels utt6 colour utt10 shape utt1 utt4 play utt2 utt3 utt5 utt7 utt8 utt9 Table 1 Results evaluation protocol rules learned ceptual observation runs game SNAP1 Game SNAP1 run 1 SNAP1 run 2 SNAP1 run 3 Soundness Completeness 1000 1000 600 714 888 800 objects having colour texture shape different colours followed time utterance PROGOL generalises rule actionsameA stateBCDBEFA If single ac tion following state colour misclassiﬁed removed training set SNAP1 run 3 contain examples objects having colour texture followed time utterance PROGOL generalises correct rule actionsameA stateBCDBCEA The sound clustering run 1 produced classes play class utterances The method constructing equivalence classes proposed Section 332 identity bodies similarity measure experiment With able cluster play utterances single class leaving remainder classes containing symbol unary classes In run 2 correct number equivalence classes class play containing ﬁve different instances utterance unary classes words In run 3 unary equivalence classes obtained expected result class utterances unary ones shape colour play 422 Experiment 2 SNAP2 This game demonstrates systems ability generalise rules perceptual cate gories introduces temporal dependency previous state SNAP2 variation SNAP1 cards placed resulting utterance dependent card visible card previous time step An exam 126 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 Fig 16 Example audiovisual inputs corresponding symbolic data representation game SNAP2 Table 2 Results evaluation protocol rules learned ceptual observation runs game SNAP2 Game SNAP2 run 1 SNAP2 run 2 SNAP2 run 3 Soundness Completeness 1000 1000 750 1000 714 1000 ple sequence game shown Fig 16 For SNAP2 learning process completed times time dataset containing 50 objects 50 utterances equivalent 50 rounds game Table 2 shows evaluation runs game according method described Section 41 The results run 1 SNAP2 learn totally sound complete data set game Fig 17 rule set Run 2 shown 100 sound 714 complete rule CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 127 actionutt11t48 actionutt12t101 actionutt8t131 actionutt8t160 actionutt8t184 actionutt11t218 actionutt6t256 actionutt3A stateBCDA successorEA stateBCDE actionutt1A stateBCDA successorEA stateFCDE actionutt5A stateBCDA successorEA stateFCGE actionutt7A stateBCDA successorEA stateBFDE actionutt9A stateBCDA successorEA stateBFDE actionutt10A successorBA stateCDEB successorFB stateGDEF actionutt4A successorBA stateCDEB successorFB stateGHEF actionutt2A Fig 17 Sorted learned rule set SNAP2 run 1 Key symbolic utterance labels utt3 utt6 utt9 colour utt1 utt5 shape utt2 utt7 utt11 utt4 utt8 utt10 utt12 misleading These rules probably result systematic misclassiﬁcation colour feature training period However sorting method described Section 34 placed misleading rules end rule set Authentic rules words higher ranked Therefore erroneous rules case jeopardise behaviour agent Run 3 scored 75 soundness learned set ﬁnd rule representing word This lack examples utterance data set considered The equivalence criteria SNAP1 identity bodies assumed SNAP2 In run 1 SNAP2 utterances clustered audio clustering method representing shape colour equivalence method constructed correct equivalence class colour class symbols remainder The reason missing elements equivalence classes result sound com plete fact useful rules answer set symbols representing utterances For example actionutt6t256 Fig 17 rule containing utterance utt6 body ﬁre This usually occurs data set contains restricted number examples utterances Therefore equivalence method information build appropriate classes This method suffered problem runs 2 3 SNAP2 128 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 423 Experiment 3 PSSpaper scissors stone This game demonstrates systems ability ground utterance actions sets speciﬁc perceptual inputs PSS played players simultaneously selecting object cards placed table The game protocol follows Ut terances play win lose draw spoken player replaced synthetic agent The action win occurs object paper beats wraps object stone scissors beats cuts paper stone beats blunts scissors In game posi tion object cards texture crucially important position cards swapped resulting action altered win lose viceversa Fig 18 illustrates example sequence game utterance said player places card right playing area typical output learning process shown Fig 19 For PSS learning process completed times time dataset containing 200 objects 200 utterances equivalent 100 rounds game This larger games greater number rules learned case It worth noting fewer examples available process fail completely partial description protocol learned Table 3 presents results runs PSS Runs 1 2 totally sound approximately 88 complete The reason totally complete occasional misclassiﬁcations visual objects causing misleading formulae generalised data sets Poor results ob tained soundness run 3 rules representing lose constructed This fact class utterance lose obtained audio clustering algorithm happened occur head idiosyncratic rule learned set In PSS use term uniﬁcation equivalence criteria In run 1 14 clusters representing utterances game method hypothesising equivalences ﬁve distinct classes The different representations utterance win lose accurately combined distinct classes Six symbols representing word play clustered class The single symbol representing draw occupied unary class Two symbols play missing relative equivalence class occurred head idiosyncratic rules The exact number equivalence classes built run 2 distinct versions play win joined distinct classes The single lose draw assigned unary classes This method presented accuracy applied run 3 In summary experiments reported section conclude learn correct protocols noisy visual data expressed 100 soundness results shown Tables 1 2 3 Even situations rules items protocol able construct partial description behaviour allows agent behave suitably given appropriate perceptual input Most partial results completeness jeopardise actuation synthetic agent idiosyncratic rules obtained general occupying end rule set dictated sorting procedure described Section 34 The ordering procedure palliative effect cause idiosyncratic rules resides occasional CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 129 Fig 18 Example audiovisual inputs corresponding symbolic data representation game PSS erroneous clustering colour feature generalisation irrelevant facts data sets A facto solution issues related integration robust camera apparatus setting development methods ILP choosing relevant formulae set generalising rules The proposed method building equivalence classes produced exact number classes game PSS behave accuracy SNAP1 SNAP2 experiments The reason difference accuracy results PSS 130 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 actionutt13t294 actionutt13t458 actionutt11t462 actionutt9A statetex2Bpos0tex0Bpos1A actionutt13A statetex1Bpos0tex0Bpos1A actionutt9A statetex1Bpos0tex2Cpos1A actionutt1A statetex1Bpos0tex1Cpos1A actionutt3A statetex0Bpos0tex1Cpos1A actionutt11A statetex2Bpos0tex1Cpos1A actionutt1A statetex2Bpos0tex2Cpos1A actionutt1A statetex0Bpos0tex0Cpos1A actionutt11A statetex0Bpos0tex2Cpos1A actionutt3A statetex2Bpos0tex0Cpos1A actionutt7A statetex2Bpos0tex0Cpos1A actionutt7A statetex1Bpos0tex2Cpos1A actionutt11A statetex1Bpos0tex0Cpos1A actionutt9A statetex0Bpos0tex1Cpos1A actionutt7A statetex0Bpos0tex1Cpos1A actionutt13A statetex0Bpos0tex2Cpos1A actionutt13A statetex2Bpos0tex1Cpos1A actionutt8A stateA actionutt2A stateA actionutt14A stateA actionutt5A stateA actionutt6A stateA actionutt12A stateA actionutt10A successorBA statetex2Cpos0 tex0Cpos1B actionutt5A successorBA statetex0Cpos0 tex2Dpos1B actionutt12A successorBA statetex1Cpos0 tex2Dpos1B actionutt4A Fig 19 Sorted learned rule set PSS run 1 Key symbolic labels win utt3 utt7 utt9 lose utt11 utt13 draw utt1 play utt2 utt4 utt5 utt6 utt8 utt10 utt12 utt14 paper tex0 scissors tex1 stone tex2 Table 3 Results evaluation protocol rules learned ceptual observation runs game PSS Game PSS run 1 PSS run 2 PSS run 3 Soundness Completeness 1000 1000 750 885 888 769 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 131 SNAP games resides fact protocols obtained SNAP games involve free variables body rules representing rule bodies PSS composed ground atoms Therefore similarity bodies trivially deﬁned equality However use uniﬁcation similarity measure SNAP games resulted poorer set equivalence classes obtained PSS Further research concentrate development appropriate similarity measures order overcome problem 5 Discussion In section discuss main pitfalls faced development research reported present paper This discussion follows order information processed attention mechanisms symbolic learning protocol behaviours The attention mechanisms robust scenario Very occasionally utterance missed spoken softly works environments noise background On visual attention mechanism based motion followed number static frames works 100 time objects placed scene quickly motion lasts 3 frames 05 seconds Once features extracted interesting objects sounds continuous unsupervised methods cluster data items It appear ing simple objects words main purpose work create synthetic agent integrates learning perceptual task levels general assumptions possible It trivial classify complex ob jects paper perfect way supervised classiﬁer speciﬁc feature descriptors model based approach However classifying number examples unknown number classes unsupervised manner open prob lem The ideas presented treat video audio largely way input signal interesting perceptual objects extracted clustered Presently clustering We form clusterings choose works best context use symbolic reasoning decide clustering best based protocols learn We like explore feeding information symbolic learning phase order relabel classes perceptual object belongs based context learned protocol This allow classiﬁers relearned new labels supervision The main limitation vision presented reliable colour classiﬁcation We inexpensive USB webcams struggle capture colour easily distinguish different coloured objects This led large number misclassiﬁ cations colours objects However allowed investigate behaves presence noise In situations examples locally sparse examples particular utterance PROGOL tends overgeneralise data particularly noisy examples present This understandable generally PRO GOL learn rules right speciﬁcity examples SNAP2 run 1 sound 132 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 complete rules learned game average utterance class examples The use PROGOL inductive logic programming allowed formulation protocol models positive examples In summary learned protocols generalise temporal sequences state observations generalise equality perceptual classes objects ground symbols perceptual objects autonomously synthetic agent interact environment Learning complex temporal dependency protocols step development research The present work assumes successor8 predicate temporal relation This extended include range tempo ral relations allow greater range scenarios captured McCallums work Nearest Sequence Memory 24 instancebased state identiﬁcation useful way proceed order capture variable length temporal dependencies states The complexity symbolic learning task reduced negative examples supplied constraining search space We wanted incorporate supervi sion framework necessary chose learn negative positive examples The symbolic protocols learned construct accurate sets equivalence classes overclustered sets utterance symbols The criteria deciding elements included class based userdeﬁned similarity criteria formulae bodies The development automatic procedures choosing criteria subject current research 51 Future work There ways work extended Outside game playing scenarios presented generalise approach human haviour trafﬁc scenes sporting activities learning task likely challenging It possible learn observation industrial inspection tasks accept reject industrial parts based unsupervised learning perceptual categories symbolic models learned ILP In examples particularly interesting points include dealing higher variable order temporal models actions wish learn grounded present previous states action previous timestep 8 This speciﬁcally preclude higher 1st order temporal models makes formulation unlikely CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 133 scenarios involving large number rules stochastic actions nondeterministic choice incremental learning learning information seen far adapting models ex amples seen multistage learning concepts orderings incorporated form background knowledge gained previous learning phase perceptual ob servation richer language structure similar framework We like propose game greater complexity presented paper feasible solved approach discuss adaptations framework incur order learn protocols scenarios similar behaviours present The game play cards right played TV shows involves pack playing cards sequence laid row face The ﬁrst card turned based value card contestant higher lower Then card turned contestant loses wrong continues game saying higher lower ﬁnal card correctly guess higher lower win With aim learning protocol game autonomous agent game encompasses number interesting challenges actions utterances wish learn grounded present previous state action previous timestep number rules large ﬁve values cards rules necessary stochastic actions exist middle value card turned single correct protocol followa stochastic choice higher lower fact action strictly nondeterministic incorporating concepts ordering cards form background knowledge previous learning phase perceptual observation This game simple number background concepts needed tackle Sensible approaches consider include develop ment incremental learning allowing agent learn protocol experimenting This learning search abstract formulae encompass basic mathematical concepts domain fed learning process background knowledge Preliminary results direc tion reported 36 An engine interpreting stochastic actions developed handle cases multiple distinct actions possible perceptual input Research development engine way 134 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 6 Conclusion Completely unsupervised learning rules behaviour observation crucial issue development fully autonomous agents able act appropriately real world including interacting individuals We presented novel approach learning protocol behaviours perceptual observation incorporates vision audio processing symbolic learning methods The results obtained learning protocols simple tabletop games able successfully learn perceptual classes rules behaviours real world audiovisual data autonomous manner Through selecting simple components acoustic processing able focus potential constructive interaction components For example ability resolve failings sound categorisation examining induced rulesets unexpected ﬁnding surfaced sophisticated acoustic processor start Progressively scaling approach complex games order reach level interactions number agents environment challenge future investigations Acknowledgements This work funded European Union CogVis project Contract IST200029375 We like thank Brandon Bennett Aphrodite Galata useful discussions course research anonymous reviewers comments helped improve paper References 1 S Agarwal D Roth Learning sparse representation object detection Proc European Conference Computer Vision vol 4 2002 pp 113127 2 S Aksoy C Tusk K Koperski G Marchisio Scene modeling image mining visual grammar Frontiers Remote Sensing Information Processing World Scientiﬁc Singapore 2003 pp 3562 3 JF Allen Maintaining knowledge temporal intervals Comm ACM 26 11 1983 832843 4 N Badler Temporal scene analysis Conceptual descriptions object movements PhD thesis Computer Science University Toronto 1975 5 K Barnard P Duygulu N Freitas DM Blei MI Jordan Matching words pictures J Machine Learning Res 3 2003 11071135 6 A Cangelosi D Parisi The processing verbs nouns neural networks Insights synthetic brain imaging Brain Language 89 2 2004 401408 7 RA Cole J Mariani H Uszkoreit A Zaenen V Zue Survey State Art Human Language Technology Cambridge University Press Cambridge 1996 8 S Coradeschi A Safﬁotti An introduction anchoring problem Robotics Autonomous Sys tems 43 23 2003 8596 9 J Daugman Uncertainty relation resolution space spatiol frequency orientation optimised twodimensional visual cortical ﬁlters J Optical Soc Amer 2 7 1985 11601169 10 MI Sessa F Formato G Gerla Similaritybased uniﬁcation Fund Informaticae 40 2000 393414 11 A Fern R Givan J Siskind Speciﬁctogeneral learning temporal events application learning event deﬁnitions video J Artiﬁcial Intelligence Res 17 2002 379449 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 135 12 JL Flanagan LR Rabiner Eds Speech Synthesis Dowden Hutchinson Ross Inc Stroudburg 1973 13 E Forgy Cluster analysis multivariate data Efﬁciency vs interpretability classiﬁcations Biometrics 21 1965 768 14 S HargreavesHeap Y Varoufakis Game Theory A Critical Introduction Routledge London 1995 15 S Harnard The symbol grounding problem Phys D 42 1990 335346 16 S Hongeng Unsupervised learning multiobject event classes Proc 15th British Machine Vision Conference BMVC04 London UK 2004 pp 487496 17 Y Ivanov A Bobick Recognition visual activities interactions stochastic parsing IEEE Trans Pattern Anal Machine Intell 22 8 2000 852872 18 G Karypis V Kumar Multilevel graph partitioning schemes Proc International Conference Parallel Processing vol 3 1995 pp 113122 19 E Keller Fundamentals Speech Synthesis Speech Recognition Basic Concepts StateoftheArt Future Challenges Wiley New York 1994 20 H Kollnig HH Nagel M Otte Association motion verbs vehicle movements extracted dense optical ﬂow ﬁelds Proc European Conference Computer Vision vol 2 1994 pp 338347 21 N Lavrac S Dzeroski Inductive Logic Programming Techniques Applications Ellis Horwood New York 1994 22 D Magee CJ Needham P Santos AG Cohn DC Hogg Autonomous learning cognitive agent continuous models inductive logic programming audiovisual input Proceedings AAAI Workshop Anchoring Symbols Sensor Data 2004 press 23 DR Magee Tracking multiple vehicles foreground background motion models Image Vision Comput 20 8 2004 581594 24 AR McCallum Hidden state reinforcement learning instancebased state identiﬁcation IEEE Trans Systems Man Cybernet Special issue Robot Learning 26 3 1996 464473 25 D Moore I Essa Recognizing multitasked activities video stochastic contextfree grammar Proc AAAI National Conf Artiﬁcial Intelligence 2002 26 S Muggleton Ed Inductive Logic Programming Academic Press New York 1992 27 S Muggleton Learning positive data S Muggleton Ed Proc ILP96 Lecture Notes Arti ﬁcial Intelligence vol 1314 Springer Berlin 1996 pp 358376 28 S Muggleton J Firth CProgol44 A tutorial introduction S Dzeroski N Lavrac Eds Relational Data Mining Springer Berlin 2001 pp 160188 29 HH Nagel From image sequences conceptual descriptions Image Vision Comput 6 2 1988 5974 30 B Neumann T Weiss Navigating logicbased scene models highlevel scene interpretations Proc 3rd International Conference Computer Vision SystemsICVS 2003 Springer Berlin 2003 pp 212222 31 C Pinhanez A Bobick Interval scripts programming paradigm interactive environments agents Pervasive Ubiquitous Computing 7 1 2003 121 32 LR Rabiner A tutorial hidden Markov models selected applications speech recognition Proc IEEE 77 2 1989 257286 33 L De Raedt K Kersting Probabilistic inductive logic programming Algorithmic Learning Theory 15th International Conference ALT 2004 Padova Italy 2004 pp 1936 34 D Roy Learning words syntax visual description task Computer Speech Language 16 3 2002 353385 35 D Roy A Pentland Learning words sights sounds A computational model Cognitive Sci 26 1 2002 113146 36 P Santos D Magee A Cohn D Hogg Combining multiple answers learning mathematical structures visual observation Proc 16th European Conference Artiﬁcial Intelligence ECAI04 Valencia Spain 2004 pp 1724 37 H Shin C Kim A simple effective technique partitioning IEEE Trans VLSI Syst 1 3 1993 380386 38 JM Siskind Grounding lexical semantics verbs visual perception force dynamics event logic J Artiﬁcial Intelligence Res 15 2001 3190 39 AR Smith Color gamut transform pairs Computer Graph 12 3 1978 1219 136 CJ Needham et al Artiﬁcial Intelligence 167 2005 103136 40 A Strehl J Ghosh Cluster ensemblesa knowledge reuse framework combining multiple partitions J Machine Learning Res 3 2002 583617 41 J Tsotsos J Mylopoulos HD Covvey SW Zucker A framework visual motion understanding IEEE Pattern Anal Machine Intell Special Issue Computer Analysis TimeVarying Imagery 1980 563 573 42 S Young D Kershaw J Odell D Ollason V Valtchev P Woodland The HTK Book Microsoft Corpora tion 2000 Further reading 43 T Mitchell Machine Learning McGrawHill London 1997