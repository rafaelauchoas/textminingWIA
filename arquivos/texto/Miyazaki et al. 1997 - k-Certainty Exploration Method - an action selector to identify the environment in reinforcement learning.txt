ELSEVIER Artificial Intelligence 9 1 1997 15 17 1 Artificial Intelligence kCertainty Exploration Method act ion selector identify environment reinforcement learning Kazuteru Miyazaki Masayuki Yamamura Shigenobu Kobayashi Department Computational Intelligence Systems Science Interdisciplinary Graduate School Science Engineering Tokyo Institute Technology 4259 Nagatsuta Midoriku Yokohama 226 Japan Received April 1996 revised July 1996 Abstract Reinforcement learning aims adapt agent unknown environment according rewards representative There issues handle delayed reward uncertainty Qlearning reinforcement learning method It works learn optimum policy However Qlearning needs numerous trials converge optimum policy If target environments described Markov decision processes identify statistics sensoraction pairs When build correct environment model derive optimum policy Policy Iteration Algorithm Therefore construct optimum policy identifying environments efficiently We separate learning process phases identifying environment determining optimum policy We propose kcertainty Exploration Method identifying environment After optimum policy determined Policy Iteration Algorithm We rule k certainty selected k times The kCertainty Exploration Method excepts loop rules achieve kcertainty We effectiveness comparing Qlearning original environment optimum policy varies according parameter 1997 Elsevier Science BV experiments One Suttons mazelike environment Keywords Reinforcement kCertainty Exploration Method learning Qlearning Markov decision processes Policy Iteration Algorithm author Email Corresponding Email mydistitechacjp Email kobayasidistitechacjp temfedistitechacjp 00043702971700 IIISOOO437029600062S 1997 Elsevier Science BV All rights reserved 156 K Miyazaki et al Artijicial Intelligence 91 1997 155171 1 Introduction Reinforcement learning RL kind machine learning It aims adapt unknown agent handle delayed policy efficiently Yamamura exploitation exploitation 4581112 rational policy quickly intensive intensive environment clue rewards There issues reward uncertainty The purpose RL acquire optimum type 23 divides RL systems exploration approaches One type The experience getting reward Profit sharing learn type Though intensive intensive exploitation type emphasizes representative The exploration intensive type emphasizes guarantee optimality collection experiences 21 exploration intensive optimality We regard Qlearning learn optimum policy attracted researchers Connell Ballard environment lo Takemichi 22 needs numerous complex environment In MDPs Markov decision processes MDPs Though Qlearning example Clouse Utgoff Kakazu 171 Unemi et al 3 Mahadevan 201 Whitehead precisely As trials identify environment number trials increases remarkably statistically identified gathering identified precisely Policy Iteration guarantee type If environment samples actions Algorithm PIA optimum policy environment identify 11 computational procedure based dynamic programing 151 Combining PIA algorithm efficiently achieve purpose RL efficiently priority environment If want identify number selection algorithms environment getting reward To matters worse handle delayed reward point identification action contributed times fewest The Interval Estimation Method 6 action It select useless actions gives priority In paper propose algorithm identify We effectiveness comparing Qlearning environment experiments efficiently method notations Section 3 proposes Section 2 describes problem kCertainty Exploration Method action selector efficiently Section 4 shows numerical reveal Certainty Exploration Method examples identify effectiveness environment k 2 The domain 21 The target problem Consider agent unknown environment At time step gets informa sensors chooses action As result gets reward environment A pair sensory called trial The function called policy The policy maximizes tion environment sequence actions input action maps sensory expected called optimum policy called ncle Executing reward action actions rule inputs K Miyazaki et al Artificial Intelligence 91 1997 155l 71 157 Table 1 Classification existing work variables state transitions discrete continuous Markov Class 1 Class 2 nonMarkov Class 3 Class 4 In general learn based imperfect limited addition environment correct action issues classify There lot works assume information Furthermore handle getting reward input reward In existing work RL shown Table 1 property state transitions treated stochastic processes sensory sensory delayed MaGun immediately lead state action In case environment corresponds 2 extends RL Linear Quadratic Regulation LQR belongs Markovian dynamical input operator Bradtke non works Tan 181 Tenenberg et al 191 treat environments Though environment numerical state examples transition Many works assume range variables senses set discrete attributevalue varieties Though work Lin 9 level Bradtke al 7 propose Stochastic Hill Climbing assures environments investigates continuous variables 2 pairs performs action treats continuous discrete In case agent discrete remains prototype LQR domain Kimura et variables rationality continuous Here treat Class 1 Table 1 Class 1 good domain class method traditional works approach lot works general framework RL After showing effectiveness In subsection position 22 The general framework approach RL Fig 1 After In paper divide selector Learner conjicting set rules match current sensory selector decides learner reinforces acquired targets RL systems action selector learner state recognizer parts State recognizer Action input The action set The rewards based executed actions Class 1 restricted given priori parameters rewards The design conflicting state recognizer environment environment execute rule senses The purpose RL acquire optimum policy efficiently Therefore In paper classify RL systems efficiency optimality pursue approaches An approach pursues optimality called exploration emphasized representative RL We classify guarantees optimality identify environment MDPs We framework Qlearning intensive efficiently Qlearning intensive exploration type known type Fig 2 158 K Miyazaki et al Artificial Intelligence 91 1997 155I 71 Fig 1 Framework reinforcement learning systems action 2 4 I 1 reward I m Fig 2 Framework learning systems baed Qlearning directly stateaction Qvalue accumulated In Qlearning environment Qlearning To matters worse know decide action selectors convergence An approach interactions pair trials acquire optimum policy Qvalues evaluates accelerate needs numerous environment Qvalues pursues exploitation efficiency called intensive identify getting reward Profit sharing types Though learn rational acquire optimum type makes accounts experiences representative method exploitation policy quickly conditions policy intensive 11121 In Class 1 environment optimum policy Therefore efficiently environment identified precisely PIA build algorithm identifies purpose RL achieved easily In section propose kCertainty Exploration Method action kCertainty Exploration efficiently Combining environment identify selector Method PIA possible acquire optimal policy efficiently K Miyazaki et alArtcial Intelligence 91 1997 155I 71 159 0 sensory input 0 current sensory input O kCertainty rule 0 kuncertainty rule Fig 3 An example kcertain looped rule 3 kCertainty Exploration Method 31 The basic idea We propose algorithm identify efficient possible identification environment environment efficiently select rules uniformly It important We rule kcertainty rule kcertainty kuncertainty A sensory We current sensory sensed time known state input current state sensory selected k times We input regarded state input Considering case returns rule current state We rule kcertain looped rule rule leads surely rules For example rule 0 rule 1 Fig 3 select current state rule 1 kcertain looped rule current state selection kcertain kcertain kcertain constructed loop If select kcertain looped rule interrupted uniform looped rules candidate selection rules It basic concept selection paper kcertain In subsection propose identify selector rules Furthermore propose action looped Exploration Method environment learn optimum policy learning efficiently kCertainty Exploration Method exception kcertain kCertainty based 32 The proposed method The algorithm kCertainty Exploration Method shown Fig 4 It consists action selector based kCertainty Exploration controls level environment identification updating certainty level k kCertainty Exploration state transition probabilities estimated kCertainty Exploration builds maximum likelihood model expectation rewards ry q T g 160 K Miyazaki et al Artcial Intelligence 91 1997 155171 procedure kcertainty exploration method begin luncertainty known rules kCertainty kkl rules current state kl begin kuncertainty rules current state select rules random rise flags known states states current state lf kuncertainty rules rules transfer state flag flag state state flag select rules transfer state flag random end end Fig 4 Algorithm kCertainty Exploration Method p I number transitions state j executing rule number executing rule state sum immediate rewards executing rule state number executing rule state kCertainty Exploration uses flags kcertain state The number flags unknown looped rules candidate advance unknown We algorithm selection A flag assigned kCertainty Exploration number state environment If kuncertain rules current state rules necessary avoid repeatedly random Otherwise flags known states raised Flags states select kuncertain got Furthermore got This process transfer selects random rules transfer rules Therefore flagdown states flagdown states meet kuncertain flag got Rules flags states transfer selecting kcertain flagdown continued selected rules To rules states algorithm How update k The kCertainty Exploration Method controls environment select new state perceived k reset initial value Because new state increased level rules kcertainty k set k 1 If sampling number identification level k Initially k set 1 If known states transit certainty selectively K Miyazaki et al Artificial Intelligence 91 I 997 155l 71 161 Fig 5 Framework learning systems based kCertainty Exploration Method A learning based kCertainty Exploration Method We leam ing based kCertainty Exploration Method Fig 5 It divided parts identifying environment deciding policy The identifying environment kCertainty Exploration Method The deciding policy PIA When rules kcertainty PIA applied This policy reliable sense rules select k times 33 An example We behavioral example kCertainty Exploration Method Consider case agent execute actions senses SO time Fig 6 1st Initially k set 1 Since rules luncertain example rule 0 selected random Then new sensory input S3 sensed Fig 62nd Since rules luncertain S3 example rule 2 selected random S3 sensed Fig 63rd Rule 2 lcertain S3 Therefore rule 3 sure selected As result SO sensed gets reward time Similarly rule 1 rule 5 selected Fig 64th 5th SO sensed Fig 66th Since rules SO lcertain decide rule selected SO In case kCertainty Exploration Method works lcertain looped rules candidate selection First flags known states SO SI S3 raised The flag Sl existence luncertain rule Sl Therefore selects rule 1 senses Sl Fig 67th Note case sensory input sensed stochastic state transitions Since rule 5 lcertain Sl rule 6 sure selected As result new 162 K Miyazaki et al Artijicial Intelligence 91 1997 155l 71 8th 10th 4th 5th 11th 6th L w f thQx w 12th SOSl sensory input w rule V reward Fig 6 Execution kCertainty Exploration Method sensory example sensed input S2 sensed Fig 68th rule 7 selected random Fig 69th S2 Since rules luncertain It gets reward time SO Since rules SO lcertain kCertainty Exploration Method works flags SO Sl S2 S3 raised The flag looped lcertain Sl rules First S2 existence luncertain flag Sl flagdown Sl rule 6 selected S2 sensed Fig 612th looped rules Then lth Since rule 7 S2 rule 8 sure selected Then SO sensed rule S2 Furthermore kCertainty Exploration Method works selects rule 1 senses Sl Fig 610th lcertain Fig 6l state Therefore S2 Similarly lcertain transfer K Miyazaki et alArtijcial Intelligence 91 1997 155l 71 163 All rules lcertain If use PIA optimum policy selects rule 0 SO rule 3 S3 The concrete algorithm PIA shown Appendix A If want update identification levels environment repeat processes k 2 34 Features kCertainty Exploration Method The kCertainty Exploration Method following features 1 Eficient identijication environment We expect identify environ ment efficiently kcertain looped rules candidate selection 2 Complete identification environment deterministic MDPs If rules deterministic MDPs environment identified lcertain completely 3 Progressive identification environment stochastic MDPs In stochastic MDPs control identification levels environment updating certainty level k 4 Independency reward The kCertainty Exploration Method influenced positions rewards It reflects belief rewards lead efficient identification environment 5 Palynomial computational costs The numerical cost memory 0mn2 cost time 0 mn3 m number actions n number sensory inputs They polynomial time PIA optimum policy polynomial time 141 The kCertainty Exploration Method features In section effectiveness kCertainty Exploration Method comparing methods 4 Evaluation kCertainty Exploration Method 41 Estimation multireturn environment The kCertainty Exploration Method identifies environment efficiently kcertain looped rules candidate selection We confirm effectiveness feature By way consider criteria general learning One acquire optimum policy efficiently decrease numerical costs deciding action The corresponds decreasing number trials decreasing time spent decide action In general RL systems important Because trial unknown environment accompanies risks predicted easily On hand numerical costs easy predict Therefore evaluate IU systems based number trials We action selector selects fewest sampling rule current state Minimal Select Method Though kCertainty Exploration Method slightly complex method Minimal Select Method simple method However shows exponential explosion number trials environment 164 K Miyazaki et al ArhBcial Intelligence 91 1997 155l 71 G Sl 52 s sL v SOSl sensory input W rule V reward Fig 7 A multireturn environment Considering environment multireturn environment shown forces return Fig 7 agent selects special SO immediately We environment rules environment If want raise certainty level k n 1 n 2 n 1 kCertainty Exploration Method 32 1 Minimal Select Method required number trials The multireturn environment shown easily Therefore bear exponential explosion Minimal Select Method 42 Comparison Qlearning mazelike environment The kCertainty Exploration Method identify terministic MDPs In subsection confirm comparison Qlearning mazelike environment 161 environment effectiveness feature completely 421 A mazelike environment We mazelike environment states At time step senses state selects action right goal state G reward returns 161 Fig 8 The agent distinguish black walls When left It start state S reaches The shortest path S G needs fourteen steps There variations The agent learn optimum policy finding acquire optimum policy number trials shortest path environment We compare learning based kCertainty Exploration Method Qlearning 422 Performance kCertainty Exploration Method We average number trails standard deviation acquire optimum policy Table 2 We 100 trials different random seeds We optimum policy rules achieved kcertainty Since environment Table 2 coincides stochastic state transitions K Miyazaki et alArtcial Intelligence 91 1997 15171 165 Fig 8 A mazelike environment Table 2 Comparison kcertainty Exploration Method aad Qlearning environment Fig 8 average standard deviation kCertainty Exploration Qlearning 59862 478042 39750 212446 number trials 2certainty 15035 standard deviation 3977 rules lcertainty The number trials rules If use kCertainty Exploration Method agent moves black walls state Because kCertainty Exploration Method excepts kcertain looped rules Therefore suppress explosion trials acquire optimal policy large state spaces 423 Comparison Qlearning Qlearning uses roulette selection proportion Qvalues action selector Initial Qvalues set 100 discount factor 09 reward value 1000 learning rate Qlearning 05 These determined preliminary experiments Qlearning needs correct propagation rewards converge Qvalues It needs nu merous trials propagate reward G S states environment As result Qlearning requires times trials learning comparison learning based kCertainty Exploration Method Through numerical example confirm effectiveness learning based kCertainty Exploration Method deterministic MDPs 43 Comparison Qlearning stochastic MDP In stochastic MDPs kCertainty Exploration Method control identification level environment updating certainty level k Furthermore influenced positions rewards identify environment We confirm features original environment optimum policy varies according parameter 166 K Miyazaki et al Artificial Intelligence 91 1997 155l 71 b SclSl sensory input W rule V reward Fig 9 A stochastic state transition environment 43 I A stochastic state transition environment au environment We consider shown SO S2 executing probability rule 3 2000 rewards rewards executing Fig 9b 1000 rewards executing rule 7 If p 05 optimum policy contains p 05 contains acquire optimum policy learning Method Qlearning p 01 03 07 09 Fig 9 A parameter rule 0 In Fig 9a p state transition agent 1000 S2 In selects rule 0 moves rule 3 2000 rewards executing rule 1 rule 3 On hand number trials based kCertainty Exploration rule 0 rule 5 rule 7 We compare 432 Perjormance kCertainty Exploration Method Fig 10 shows acquisition number rate optimal policy plotted based kCertainty Exploration Method We level random seeds Fig 11 shows distribution certainty trials learning 100 trials different k acquisition rate optimal policy achieves 10 The acquisition kCertainty Exploration Method state transition rates optimal policy Figs 9a b rates If p approaches 05 influenced positions rewards Only acquisition probability effects K Miyazaki et al Artificial Intelligence 91 1997 155l 71 167 0 Q j h g 12 22 g 3 Y 50 40 3 o 20 10 0 o I I I I I I I 0 100 200 300 400 500 600 700 number trials Fig 10 Behavior kCertainty Exploration Method environment Fig 9 agent tends mistake 05 boundary optimal policy In case kCertainty Exploration Method raises certainty level k Therefore distribution certainty level k p 03 p 07 spreads comparison p 01 p 09 shown Fig 11 433 Comparison Qlearning Figs 12 13 acquisition rate optimal policy plotted number trials Qlearning environment Figs 9a b respectively Qlearning uses roulette selection proportion Qvalues action selector Initial Qvalues set 100 discount factor 09 reward value 1000 learning rate Qlearning 05 They determined preliminary experiments In general Qlearning acquires optimum policy Qvalues Qvalues reflect expected reward action Therefore Qlearning influenced positions Qvalue rule 0 reinforced rule 1 rewards In Fig 9a agent getting reward executing rule 0 Therefore optimum policy easily p 05 On hand Fig 9b optimum policy easily p 05 Qvalue rule 1 reinforced rule 0 168 K Miyazaki et alArtiial Intelligence 91 1997 155l 71 A s 100 g fir 50 0 10 20 k 30 h 100 s H 5o I 1 0 10 k 20 30 iL Foe7 1 10 20 30 k Fo3 10 20 30 k Fig 11 Frequency distributions certainty level k environment Fig 9 Qlearning strictly influenced positions rewards The kCertainty Exploration Method influenced Therefore feasible changes 5 Conchsions We proposed kCertainty Exploration Method action selector identify environment efficiently Combining kCertainty Exploration Method PIA possible acquire optimal policy efficiently We effectiveness kCertainty Exploration Method comparing methods K Miyazaki et al Artijcial Intelligence 91 I 997 155I 71 169 FOS 0 100 200 300 400 500 600 700 number trials Fig 12 Behavior Qlearning environment Fig 9a The kCertainty Exploration Method consider state transition probabil ity Therefore guarantee efficiency stochastic MDPs We propose Certainty Exploration Method extension kCertainty Ex ploration Method stochastic MDPs 131 The fLCertainty Exploration Method realizes efficient identification environment considering state transition probabilities As future work want extend kCertainty Exploration Method nonMDPs learning dynamical After like consider multiagent reinforcement environments multireward environments Appendix A In following Policy Iteration Algorithm Step 1 Select policy Step 2 For policy resolve following equations Wi 12 m m Wi r yCqWj jl i12 112 170 K Miyazaki et al Arttjicial Intelligence 91 1997 155171 1 o 2 9 o E 80 8 _ oa 3 0 8 E g t z 3 8 3 7 o 60 50 40 30 20 1 o 0 o 0 100 1000 1200 1400 number trials fig 13 Behavior Qlearning environment Fig 9b m number states ki action state y discount factor Step 3 Calculate following equations wi lygn YkCwj jl 12 m n number actions Step 4 If Wi wi policy Wi Wi set k Step 3 maximized Step 2 optimum policy ki If return References 1 DP Bertsekas Dynamic programming stochastic control R Bellman ed Marhematics Science Engineering 125 Academic Press New York 1976 21 SJ Bradtke Reinforcement learning applied linear quadratic regulation Proceedings NIPSj 1993 3 JA Clouse RE Utgoff A teaching method reinforcement learning Proceedings Ninth International Conference Machine Learning Aberdeen Scotland 1992 92101 K Miyazaki et al Arttjicial Intelligence 91 1997 155I 71 171 4 JJ Grefenstette Credit assignment rule discovery systems based genetic algorithms Mach Learn 3 1988 225245 5 JH Holland JS Reightman Cognitive systems based adaptive algorithms DA Waterman E HayesRoth eds PatternDirected Inference Systems Academic Press New York 1987 61 LP Kaelbling An adaptable mobile robot Proceedings 1st European Conference Artificial Life 1991 4147 7 H Kimura M Yamamura S Kobayashi Reinforcement learning Stochastic Hill Climbing discounted reward Proceedings 12th International Conference Machine Learning lake Tahoe CA 1995 295303 8 GE Liepins MR Hilliard M Palmer G Rangarajan Alternatives classifier credit assignment Proceedings IJCAI89 Detroit MI 1989 756761 9 L Lin Scaling reinforcement learning robot control Proceedings 10th International Conference Machine Learning Amherst MA 1993 182189 lo S Mahadevan J Connell Automatic programming behaviorbased robots reinforcement learning Proceedings AAAI91 Anaheim CA 1991 774780 111 K Miyazaki M Yamamura S Kobayashi A theory profit sharing reinforcement learning I Japan Sot Artificial Intelligence 9 4 1994 580587 Japanese 121 K Miyazaki M Yamamura S Kobayashi On rationality profit sharing reinforcement learning Proceedings 3rd International Conference Fuzzy Logic Neural Nets Soft Computing 1994 285288 131 K Miyazaki M Yamamura S Kobayashi Certainty Exploration Method action selector extension kCertainty Exploration Method stochastic identify environment agentan MDPs J Japan Sot Artijcial Intelligence appear Japanese 141 CH Papadimitriou JN Tsitsiklis The complexity Markov decision processes Math Oper Res 1987 441450 151 SP Singh Transfer learning composing solutions elemental sequential tasks Mach Learn 8 1992 323339 161 RS Sutton Reinforcement learning architectures animats Proceedings 1st International Conference Simulation Adaptive Behavior 1990 337343 171 Y Takemichi Y Kakazu An acquisition action strategy multilayered Qlearning Proceedings 7th Annual Conference JSAI 1993 9396 181 M Tan Multiagent reinforcement learning independent vs cooperative agents Proceedings 10th International Conference Machine Learning Amherst MA 1993 330337 191 J Tenenberg J Karlsson S Whitehead Learning task decomposition Proceedings 2nd International Conference Simulation Adaptive Behavior 1993 337343 20 T Unemi M Nagayoshi N Hiyayama T Nade K Yano Y Masujima Evolutionary differentiation case study optimizing parameter values Qlearning genetic algorithm learning abilitiesa Proceedings 4th International Workshop Artificial Life 1994 331336 21 CJCH Watkins P Dayan Technical note Qlearning Mach Learn 8 1992 5568 22 SD Whitehead DH Ballard Active perception reinforcement learning Proceedings 7th International Conference Machine Learning Austin TX 1990 179188 23 M Yamamura Reinforcement learning J Japan Sot Artcial Intelligence 8 6 1993 833834 Japanese