Artiﬁcial Intelligence 247 2017 415439 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Modelbased contextual policy search dataeﬃcient generalization robot skills Andras Kupcsik ab Prahlad Vadakkepat Gerhard Neumann c National University Singapore Department Electrical Computer Engineering 4 Engineering Drive 3 Singapore 118571 Singapore b National University Singapore School Computing 13 Computing Drive Singapore 117417 Singapore c Technische Universität Darmstadt Fachbereich Informatik Fachgebiet Intelligente Autonome Systeme Hochschulstr 10 D64289 Darmstadt Germany d MaxPlanck Institute Intelligent Systems Spemannstrasse 38 72076 Tübingen Germany e Imperial College London Department Computing 180 Queens Gate London SW7 2AZ United Kingdom Marc Peter Deisenroth e Jan Peters cd Loh Ai Poh r t c l e n f o b s t r c t Article history Received revised form 29 September 2014 Accepted 24 November 2014 Available online 2 December 2014 Keywords Robotics Reinforcement learning Contextual policy search Modelbased policy search Robot skill generalization Gaussian processes Movement primitives Robot table tennis Robot hockey In robotics lowerlevel controllers typically robot solve speciﬁc task ﬁxed context For example lowerlevel controller encode hitting movement context deﬁnes target coordinates hit However learning problems context change task executions To adapt policy new context utilize hierarchical approach learning upperlevel policy generalizes lowerlevel controllers new contexts A common approach learn upperlevel policies use policy search However majority current contextual policy search approaches modelfree require high number interactions robot environment Modelbased approaches known signiﬁcantly reduce robot experiments current modelbased techniques applied straightforwardly problem learning contextual upperlevel policies They rely speciﬁc parametrizations policy reward function unrealistic contextual policy search formulation In paper propose novel modelbased contextual policy search algorithm able generalize lowerlevel controllers dataeﬃcient Our approach based learned probabilistic forward models information theoretic policy search Unlike current algorithms method require assumption parametrization policy reward function We complex simulated robotic tasks real robot experiment proposed learning framework speeds learning process orders magnitude comparison existing methods learning high quality policies 2014 Elsevier BV All rights reserved 1 Introduction Learning successful alternative handdesigning robot controllers solve complex tasks robotics Algorithms learn controllers need important challenges consideration First robots typically operate Corresponding author elelohapnusedusg AP Loh prahladnusedusg P Vadakkepat neumanniastudarmstadtde G Neumann Email addresses kupcsikcompnusedusg A Kupcsik mdeisenrothimperialacuk MP Deisenroth petersiastudarmstadtde J Peters httpdxdoiorg101016jartint201411005 00043702 2014 Elsevier BV All rights reserved 416 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Fig 1 KUKA lightweight arm shooting hockey pucks highdimensional continuous stateaction spaces Thus learning algorithm scale higher dimensional robot tasks Second running experiments real robots typically high cost An experiment rollout time consuming usually requires expert supervision lead robot damage Thus learning algorithm required operate limited number evaluations Furthermore learning real robots safety important factor To avoid robot environmental damage learning algorithm provide robot controllers generate robot trajectories close explored safe trajectory space Lastly robot skills able adapt changing environmental conditions For example task deﬁned throwing ball varying target positions controller adapted current target position In following refer task variables context s In throwing example context represented target position throw In paper introduce new modelbased policy search method generalize learned skill new context For example learned throw ball speciﬁc location want generalize skill throw ball multiple locations Policy Search PS methods successful Reinforcement Learning RL algorithms learning com plex movement tasks robotics 3138222321830202713 PS algorithms typically optimize parameters ω parametrized control policy generates control commands robot policy obtains maximum reward A common approach parametrize policy use compact representation movement moderate parameters movement primitives 1721 In approaches movement primitives parameters ω specify shape desired trajectory The policy deﬁned trajectory tracking controller follows desired trajectory Such desired trajectory represented single parameter vector ω solve speciﬁc task characterized context vector s The goal contextual policy search learn choose parameter vector ω control policy function context s To convenient deﬁne different levels policies policy search At lower level control policy speciﬁes controls robot function state The lower level policy parametrized parameter vector ω The lowerlevel policy example implemented movement primitive 17 On upperlevel policy chooses parameters ω lowerlevel policy We denote policy upperlevel policy Given current task description s upperlevel policy chooses parameters ω lowerlevel policy The lower level policy subsequently executed given parameters ω episode Although PS algorithms applied learn large variety robot skills paper focus learning strokebased movements throwing hitting Most existing contextual policy search methods modelfree 2027 try optimize policy estimating model robot environment Modelfree PS algorithms execute rollouts real robot evaluate parameter vectors ω These evaluations ﬁnally improve policy Most modelfree PS algorithms require hundreds thousands real robot interactions converging high quality policy For robot learn ing problems data ineﬃciency impractical executing real robot experiments time consuming requires expert supervision lead robot wear robot damage It shown dataeﬃciency policy search methods considerably improved learning forward models robot environment These models predict experiment outcome allows robust eﬃcient policy updates We refer al gorithms modelbased policy search algorithms 10143421829 However current modelbased policy search methods PILCO 912 suffer severe limitations hard apply methods learning generalized robot skills PILCO uses computationally demanding deterministic approximate inference techniques assume speciﬁc struc ture reward function lowerlevel policy These assumptions hold applications occur contextual policy search hindered use modelbased policy search learning contextual upper level policies Moreover deterministic approximate inference method adds bias prediction experiment outcome Recently PILCO algorithm extended learning generalized lowerlevel controllers 11 Promising results demonstrated learning robot controllers hitting box stacking tasks Still PILCO suffers restrictions structure reward function lowerlevel controllers A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 417 In paper introduce new modelbased policy search method relaxes assumptions current model based approaches eﬃciently generalize lower levelrobot control policies new contexts s We rely contextual extension Relative Entropy Policy Search REPS informationtheoretic PS algorithm 30 REPS maximizes expected reward upperlevel policy staying close observed data given parameter samples old upperlevel policy Our approach extend REPS modelbased method Due closeness bound REPS suited extension REPS explore areas parameter space seen data learned models poor quality We learn probabilistic forward models robot environment exploit expert knowledge task setup decompose model environment simpler dynamic contact models We use learned forward models generate artiﬁcial experiment outcomes subsequently use policy updates The beneﬁt models twofold First artiﬁcial samples policy updates signiﬁcantly improve dataeﬃciency learning framework Second use models compute expected return avoiding risk sensitive bias original REPS algorithm We resulting algorithm Gaussian Process Relative Entropy Policy Search GPREPS We complex simulated robotic tasks real experiment GPREPS reduces real robot interactions orders magnitude learning policies higher quality This paper extension results published 24 novel contributions follows First present detailed description technical background thoroughly discuss related work Second present derivation contextual REPS algorithm Third present qualitative comparison momentmatching sampling GP models terms prediction accuracy computation times Fourth compare eﬃciency sparse GP model learning robot dynamics Finally present novel evaluation GPREPS table tennis learning scenario simulated Biorob robot arm In following present problem formulation Section 2 In Section 3 discuss related work In Section 4 introduce contextual extension REPS learn upperlevel policies In Section 5 introduce GPREPS algorithm explain model learning trajectory prediction integrated policy updates contextual REPS In Section 6 experimental results Section 7 concludes work 2 Problem formulation In paper denote state robot environment x Typically state composed joint angles q Rd joint velocities q Rd d number degrees freedom1 contain external variables position ball The vector control torques u Rd computed lowerlevel policy u π x ω2 parametrized ω cid3 The space lowerlevel policy parameters denoted cid3 A typical approach policy search use simple parametrizations lowerlevel policy small number parameters Depending task use linear feedback controllers movement primitives 17 torque proﬁles 28 Such parametrizations easier learn example neural network controllers usually limited solve single task In order generalize lowerlevel controllers different contexts s need choose different parameters ω context The trajectory robot deﬁned set stateaction pairs time step episode length T τ x1 u1 xT uT In formulation upperlevel policy represented search distribution cid3 π ω It typically deﬁned Gaussian π ω N ωμω cid5ωWe consider episodebased policy search framework 202733 search optimal upperlevel policy solving cid2 π ω arg max π cid3 π ωRωdω 1 cid3 expected episode reward denoted Rω τ pτ ωRτ dτ Rτ trajectory reward pτ ω probability trajectory given parameters ω lowlevel controller In cases reward function deﬁned sum immediate rewards rxt ut Rτ T t1 rxt ut However objectives deﬁned function trajectory cid4 As example consider throwing task Fig 2a robot throw ball speciﬁc target position maintaining balance The robot executes throwing motion controller π x ω releases ball speciﬁc time point tr Then record ball trajectory τ b contains ball position time step bt bxt b ytT For throwing task reward function deﬁned minimum distance ball trajectory target position p Rτ mint cid4p bt cid42 Additionally include objectives reward function t1xt xgT xt xg xg target state torque penalty In contextual policy search 2127 goal generalize lowerlevel control policy u π x ω multiple contexts The context vector s learning problem deﬁned set variables fully specify task It typically contains objectives agent throwing task refer target position target s px p yT contain properties environment weight mass needs lifted We assume continuous context variable drawn unknown distribution s μs beginning t ut deviation target conﬁguration T t1 uT cid4 cid4 T 1 In paper assume rotational robot joints 2 In paper deterministic lowerlevel controller Alternatively use stochastic controller u π ux ω typically necessary exploration implemented upperlevel policy 418 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Fig 2 The ball throwing task The 4link robot throw ball target position px p y maintaining balance b The learning framework modelfree contextual PS algorithms contextual REPS algorithm introduced Section 4 We sample lowerlevel policy parameter upperlevel policy ω π ωs observed context s Subsequently evaluate ω robot obtain reward For policy update obtain N rollouts episode context variable fully observable The context distribution μs deﬁned learning problem Solving contextual problem standard episodebased PS approach require learn πsω context s clearly tedious data ineﬃcient approach Instead follow hierarchical approach learn upperlevel policy π ωs provides lowerlevel controller parametrization ω given context s Our goal ﬁnd optimal policy π ωs maximizes expected reward cid2 cid2 π ωs arg max π μs s ω π ωsRsωdωds 2 Rsω denotes expected reward executing lowerlevel policy parameter ω context s As dy namics robot environment stochastic reward Rsω given expected reward trajectories cid5 Rsω Eτ Rτ ss ω cid6 cid2 pτ s ωRτ sdτ τ 3 The probability trajectory pτ s ω depends context reward function Rτ s depend context s Using motivating example throwing task deﬁne reward function minimal distance target deﬁned context Rτ s mint cid4s btcid42 context deﬁnes target position throw s px p yT We illustrate basic concept modelfree contextual policy search Fig 2b In beginning episode lowerlevel policy parameter drawn π ωs given observed context variable s μs Subsequently policy parameter evaluated robot reward obtained We collect N sample rollouts use update policy 3 Related work While policy search highly successful recent years multiple approaches acquire robot skills Standard reinforcement learning tools rely Markov Decision Process MDP framework share goal policy search maximize reward interacting environment However major disadvantage MDP formulation number states number actionvalue functions grow exponentially state dimensionality As robots typically operate high dimensional continuous stateaction spaces standard RL techniques diﬃcult apply success The goal methods RL closely relate optimal control However signiﬁcant difference assumed prior knowledge In optimal control exact model controllable structure model usually assumed known While learn models data way paper optimal control solution obtained linear systems Gaussian noise For systems optimal control rely approximations lead poor quality optimal policy Policy search avoids approximations directly search policy parameter space likely produce policies higher quality In following brief overview related work ﬁelds robot skill generalization contextual modelfree policy search methods modelbased policy search robot learning Robot skill generalization Robot skill generalization investigated researchers recent years 4015726 2027 In 1540 robot skills represented Dynamic Movement Primitives 17 DMPs The DMPs generalized demonstrated trajectories First DMP parameters ω extracted human experts demonstration spe ciﬁc context s Using multiple demonstrations library contextparameter pairs s ωN i1 built Subsequently DMP parameters ω query context s chosen regression techniques While generalization proven A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 419 accurate humanoid reaching grasping drumming parameters improved reinforcement learning Thus quality reproduced skill inherently depends quality quantity expert demonstration To account skill improvement policy search methods applied 2027 For example Kober et al 20 proposed Cost regularized Kernel Regression CRKR algorithm learn DMP parameters throwing dart different targets robot table tennis However CRKR scale higher dimensional learning problems uncorrelated ex ploration strategy Another PS algorithm proposed 27 robot skill generalization The algorithm uses probabilistic approach based variational inference solve underlying RL problem The proposed method able generalize lowerlevel controller balances 4DOF planar robot upright position random initial push How solution proposed PS algorithm computed closed form upperlevel policies computationally costly obtain Recently Mixture Movement Primitives MoMP algorithm introduced 26 robot table tennis The MoMP approach ﬁrst initializes library movement primitives contexts human demonstrations Then given query context gating network combine demonstrated DMPs single executed robot The gating network parameters adapted depending successful ment primitives given context Promising results real robot table tennis presented 26 However formulation learning problem required lot prior knowledge unclear algorithm scales domains prior knowledge applicable Finally general robot skill learning framework proposed da Silva et al 7 The proposed approach separates generalization policy learning classiﬁcation regression problem The resulting policy mixture ﬁnite number local policies However choosing local policies straightforward separating generalization policy improvement step distinct algorithms data ineﬃcient counter intuitive The contextual REPS algorithm presented paper highly related hierarchical HiREPS algorithm presented 8 HiREPS applied learn multiple options solve given task It learning versatile robot skills tetherball game While contextual REPS algorithm presented paper seen special case HiREPS option state transitions HiREPS algorithm 8 modelfree needs evaluations real robot Modelfree policy search algorithms The key concept modelfree policy search algorithms update policy solely based parameterreward samples obtained real assumption knowledge dynamics Modelfree policy search methods coarsely categorized according exploration policy update strategy Exploration implemented level actions add exploration noise controls time step level parameters ω control policy Some earliest successful policy search methods policy gradient PG algorithms 42537 PG algorithms use likelihoodratio trick compute expected performance gradient The policy parameters updated gra dient user deﬁned learning rate Some important extensions PG algorithm introduction natural policy gradient 313 The natural policy gradient bounds relative entropy KullbackLeibler KL divergence subsequent policies second order approximation KL divergence Due bound algorithm achieves uniform convergence parameter space typically results increased learning speed com parison standard PG algorithms Natural policy gradient algorithms currently commonly PG algorithms While early PG algorithms developed actionbased exploration ideas adopted parameterbased exploration algorithms 433533 One signiﬁcant challenge PG methods choice appropriate learning rate crucial good performance usually diﬃcult ﬁnd An alternative approach learn upperlevel policies treat policy search problem latent variable estimation problem use Expectation Maximization solve 27204121 In episodic MonteCarlo EM approaches 204121 new upperlevel policy parameters Momentprojection reward weighted old policy essentially weighted maximum likelihood estimation new policy parameters Alternatively perform Informationprojection reward weighted old policy 27 By avoid typical problem MCEM algorithms averaging multiple modes reward weighted parameter distribution However Iprojection new policy parameters computed closed form policies 27 The idea avoiding failed low quality experiments exploration investigated Grollman Billard 16 They assume experts demonstration task unsuccessful gives idea solution look like Thus robot try skills exactly failed demonstrations similar The distribution exploratory trajectories encoded Donut Mixture Model DMM regarded pseudoinverse Gaussian Mixture Model 16 representing failed trajectory distribution A signiﬁcant advantage EMbased PS methods compared PG algorithms user deﬁned learning rate required applied learn contextual upperlevel policies 20 To combine uniform convergence property natural gradient approaches 31 closed form policy update EMbased PS approaches infor mation theoretic Relative Entropy Policy Search REPS algorithm proposed 30 The REPS algorithm limits information loss subsequent policies maximizing expected reward resulting smooth convergence The required parameter REPS upper bound information loss signiﬁcantly easier choose learning rate PG algorithms We investigate details contextual extension REPS Section 3 420 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Modelbased policy search methods To improve dataeﬃciency model free methods modelbased approaches proposed literature 912344 Modelbased methods learn forward model controllable sys tem environment subsequently simulations predict experiment outcome While 1 time dependent forward model typically learn dynamic model real hardware 34429101814 Timedependent models fail generalize unseen situations provide accurate models observed trajec tories 1 able predict dynamics accurately especially state includes unobserved variables Forward models typically provide long term trajectory predictions The common approach learn discretetime stochastic state transition model xt1 f xt ut cid6 measurement data cid6 N 0 cid5 iid Gaussian noise A common method learn nonlinear models dynamics use Locally Weighted Bayesian T β cid6 state transition t uT t T LWBR applied learn Regression LWBR algorithm 43429 LWBR learns local linear models xt1 1 xT parameter vector β reestimated locally query point y xT uT forward model helicopter 429 inverted pendulum 34 T measurement data xi1 xi uiK An alternative successful method use Gaussian Process GP models proven eﬃcient learning stochastic model dynamics 9101814 With GP models 32 compute posterior distribution suc cessor state xt1 closed form given query input xT t uT i1 As GP models t integrate model uncertainty model biased GP models provide variance pre diction The varianceuncertainty prediction typically decreases number data points neighborhood Consequently avoid overly conﬁdent predictions unexplored state spaces The state art modelbased policy search algorithm Probabilistic Inference Learning Control PILCO algorithm 912 uses GP models PILCO ﬁrst learns GP model dynamics robot Subsequently predicts expected trajectory variance distribution future rewards following current control policy However computing successor state distribution GP models given nondeterministic query input straightforward PILCO solves problem matching ﬁrst second moment predictive distribution Using approximated trajectory distribution reward distribu tion PILCO computes gradient long term rewards wrt controller parameters closed form This process repeated optimal policy current GP model Subsequently policy executed real robot obtain new measurement data improve learned forward models PILCO successfully applied learning controllers lowcost robot arm 10 robotic unicycle 9 unprecedented dataeﬃciency Recently applied imitation learning 14 However PILCO directly optimizes lowerlevel controller parameters straightforwardly applied learn upperlevel policies Moreover class representable lowlevel controllers restricted functions Gaussian distribution mapped closed form 4 Contextual episodebased REPS The intuition REPS 30 maximize expected reward staying close observed data balance exploration experience loss The constraint staying close data implemented bounding relative entropy called KullbackLeibler KL Divergence old trajectory distribution trajectory distribution induces new policy want estimate The use KLbound provides intuitive way deﬁne explorationexploitation tradeoff With small KLbound favor exploration continue explore old exploration policy Hence obtain slower learning speed likely ﬁnd good solutions With high cid3 favor exploitation The resulting policy greedy reduce exploration We fast learning speed quality solution worse average premature convergence algorithm 8 In episodic learning setting context s parameter ω uniquely determine trajectory distribution 8 For reason trajectory distribution abstracted joint distribution parameter vector ω context s ps ω μsπ ωs To bound relative entropy consecutive trajectory distributions REPS uses constraint cid2 dsdω cid3 4 ps ω log sω ps ω qs ω ps ω represent updated qs ω previously contextparameter distribution The parameter cid3 R upper bound relative entropy A smaller value cid3 results conservative policy updates higher cid3 leads faster converging policies cid3 As context distribution μs deﬁned learning problem chosen learning algorithm ω ps ω μs s satisﬁed However case continuous context variables constraints inﬁnite number instances constraint To optimization problem tractable require match feature averages instead single probability values cid2 psφsds ˆφ 5 s cid3 ω ps ωdω The feature vector denoted φs ˆφ denotes observed average feature vector ps For example feature vector contains linear quadratic terms context constraint translates A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 421 matching mean variance distributions ps μs The contextual episodebased REPS learning problem given cid2 cid2 max p st sω cid2 cid2 sω cid2 cid2 sω cid2 cid2 sω ps ωRsωdsdω ps ω log ps ω qs ω dsdω cid3 ps ωφsdsdω ˆφ ps ωdsdω 1 6 The emerging constrained optimization problem solved Lagrange multiplier method The closed form solution new distribution given ps ω qs ω exp 7 cid7 cid8 Rsω V s η Here V s θ T φs context dependent baseline η θ Lagrangian parameters Subtracting baseline reward corrected context dependent allows evaluate parameter ω independently context s The temperature parameter η scales advantage term relative entropy bound met policy update The Lagrangian parameters optimizing dual function cid7cid2 cid2 cid7 cid8 cid8 gη θ η log qs ω exp sω Rsω V s η dsdω ηcid3 θ T ˆφ 8 The dual function convex θ η corresponding gradients obtained closed form In difference recent EMbased policy search methods 2127 exponential weighting emerges relative entropy bound require additional assumptions For details derivation refer Appendix A 41 Samplebased REPS As relationship contextpolicy parameter pair s ω corresponding expected reward Rsω known sample evaluations approximate integral given dual function 824 We denote μs Subsequently sample evaluations rollouts To execute ith rollout ﬁrst observe context s lowerlevel controller parameter upperlevel policy ωi π ωs Finally execute lowerlevel policy parametrization ωi sω 1 N approximate integral given dual function samples The samplebased approximation dual function given Appendix A Eq A8 sω This process repeated N rollouts s obtain Ri ωi Ri context s As sampled controller parameters old policy samples generated qs ω Using optimized Lagrangian parameters θ η compute probabilities updated contextparameter distribution ﬁnite set samples cid8 cid7 9 exp p Ri sω V s η However order generate new samples need parametric model estimate π ωs Thus estimate parameters model given samples p weight samples For example linear Gaussian model π ωs N ωa As cid5 compute parameters A cid5 weighted maximum likelihood estimation cid9 cid10 aT A T cid5 cid11 cid121 S T P S S T P B cid4 N i1 p iωi μiωi μiT cid4 N i1 pi μi As 1 NT context matrix ˆs ˆs iT T B ω1 ωNT parameter matrix S ˆs P ii p diagonal weighting matrix When updating policy parameters restricted use N samples To improve accuracy policy updates deﬁne qs mixture H policies 1 s 10 11 12 422 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Table 1 In iteration contextual REPS algorithm collect dataset Dk s sωi1N performing rollouts real For REPS algorithm reuse H datasets combine dataset D Finally update policy optimizing dual function dataset D computing sample weights performing weighted maximum likelihood ML estimate obtain new parametric policy π ωs ωi Ri Contextual REPS Algorithm Input relative entropy bound cid3 initial policy π ωs number policy updates K number old datasets reusing data H k 1 K 1 N μs 1 N Observe context s observe trajectory τ Execute policy ωi π ωs Compute rewards Ri sω Rτ s sωi1N ωi R New dataset Dk s Reuse old datasets D Dhhmax1kHk Update policy Optimize dual function D Eq A8 η θ argminηcid9θ cid9 gηcid9 θ cid9 D Compute sample weighting exp sω θ T φs η Update policy π ωs weighted ML Ri D p Eqs 1011 p sample D end Output policy π ωs reuse old samples need importance weighting The modelfree contextual REPS algorithm summarized Table 1 However modelfree REPS produces biased policy updates expected reward Rsω evaluated single rollout This bias seen looking exponential sample weighting REPS given Eq 9 For example actions a1 a2 expected reward a1 lower expected reward a2 However variance reward a1 higher samples a1 higher reward samples a2 REPS prefer a1 Thus resulting policy riskseeking behavior want general avoid The bias inherent PS methods based weighting samples exponential function example PoWER 21 CrKR 20 PI23 38 5 Modelbased contextual policy search Our main motivation use models contextual policy search twofold First want improve data eﬃciency modelfree REPS artiﬁcial rollouts Second want obtain accurate estimate expected reward Rsω given contextpolicy parameter pair avoid bias samplebased REPS formulation The expected reward Rsω Eτ Rτ ss ω estimated multiple samples trajectory distribution pτ s ω Rsω 1 L Lcid13 l1 cid11 R τ l cid12 s 13 trajectories generated learned forward models simulation assume trajectorydependent reward function Rτ s known We generate M artiﬁcial contextparameter samples estimate context distribution upper level policy Using learned models evaluate artiﬁcial samples simulation To gain beneﬁt learned forward models use structural knowledge task decompose monolithic forward model smaller forward models easier learn For example throwing ball robot learn individual forward models predict movement robot initial conﬁguration ball released robot ﬂight ball We decomposition forward models ball throwing example Fig 3b illustration experiment shown Fig 3a In Fig 4 learning framework GPREPS algorithm One key difference compared model free case evaluate policy realrobot obtain measurement data Using ball throwing example measurement data consist joint trajectory τ r trajectory thrown ball τ b Using obtained measurement data learn models dynamics discrete events releasing ball Note modelbased formulation use solely artiﬁcial samples update policy Thus case unbiased models avoid possibly noisy reward samples evaluated real eliminate risk sensitive bias inherent REPS algorithm Additionally increase number artiﬁcial samples signiﬁcantly 3 The PI2 claimed unbiased 38 However case noise controlled unrealistic assumption A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 423 Fig 3 The illustration throwing task b The decomposition experiment individual forward models First learned robot dynamics model obtain robot trajectory τ r following lowerlevel policy ut π xt ω starting state x0 Subsequently learned release model predicts initial state ball b0 joint state time tr xtr Afterwards learned ball ﬂight model provides ball trajectory τ b Finally reward computed context s target position ball trajectory τ b torques predicted τ r Fig 4 The learning framework modelbased contextual PS GPREPS We use real robot evaluations obtain measurement data learn problem speciﬁc models Finally evaluate artiﬁcial samples simulation learned models These artiﬁcial samples policy update N cid10 M improve accuracy policy updates Due recent success Gaussian Process models reduce model bias learning complex dynamics 9 use GP models learn forward models robot environment Therefore method called Gaussian Process Relative Entropy Policy Search GPREPS 51 Gaussian Process REPS upperlevel policy π ωs In iteration ﬁrst collect measurement data robot environment For data collection observe Subsequently use lowerlevel sample parameters ωi context s control policy π x ωi obtain trajectory sample τ It important evaluate suﬃciently samples obtain measurement data GP models produce accurate predictions relevant state space Therefore repeat data collection step N times To favor dataeﬃciency want N low possible learning high quality models In experiments usually choose higher N ﬁrst iteration N 20 obtain accurate GP model After ﬁrst policy update decrease N signiﬁcantly lower value N 1 This way updating GP models relevant measurement data compromising dataeﬃciency We retrain GP models far obtained measurement data The GPREPS algorithm summarized Table 2 In prediction step predict rewards M randomly sampled contextpolicy parameter pairs We refer samples artiﬁcial samples To generate artiﬁcial samples need obtain estimate context distribution ˆμs observed data Depending learning problem typically approximate context distribution Gaussian uniform distribution Given observed context variables siN i1 ﬁt distribution parameters j maximum likelihood estimation After updating estimated context distribution ˆμs draw context parameter s ˆμs artiﬁcial sample Subsequently sample upperlevel policy ω j π ωs j produce L sample trajectories τ jl j given contextparameter pair To update policy ﬁrst minimize dual function gη θ Eq A8 artiﬁcially generated samples compute new weight p Eq 9 artiﬁcial sample Note use solely artiﬁcial contextparameter samples s j1 update policy use expected reward REPS algorithm instead single sample estimate reward Consequently models produce unbiased rewards Rsω ﬁnal policy unbiased Finally update policy weighted maximum likelihood estimate Eqs 1011 In following explain learn models sample trajectories l 1 L corresponding rewards Rτ jl s j ω jM j 424 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Table 2 In iteration GPREPS algorithm collect data environment observing context executing policy Using observed data update models subsequently use generate M artiﬁcial samples We obtain expected reward sample sampling L trajectories averaging trajectory rewards Finally update policy optimizing dual function computing sample weights GPREPS Algorithm Input relative entropy bound cid3 initial policy π ωs number policy updates K k 1 K Collect Data Observe context s Execute policy ωi π ωs μs 1 N Train forward models data estimate ˆμs j 1 M Predict Rewards j ˆμs Draw context s Draw lowerlevel parameters ω j π ωs Predict L trajectories τ l Compute R j Construct artiﬁcial dataset Update Policy j ω j s l Rτ l jL j s D s j ω j R sω cid4 j j j sω j1M Optimize dual function D Eq A8 η θ argminηcid9θ cid9 gηcid9 θ cid9 D p Compute sample weighting j R j j exp sω θ T φs η Update policy π ωs weighted ML j j 1 M D p Eqs 1011 end Output policy π ωs 52 Learning GP forward models We use forward models simulate trajectory τ given context s lowerlevel policy parameters ω We learn f yt ut cid6 y xT bT T composed state robot forward model given yt1 state environment b instance position ball The vector cid6 denotes zeromean Gaussian noise In order simpler models easier learn To simplify learning task decompose forward model f exploit prior structural knowledge robot interacts environment For example learning play table tennis game use prior knowledge trajectory ball described free dynamics contacts table racket Gaussian Processes eﬃcient nonparametric Bayesian regression tools 32 explicitly represent model uncertainty For learning model set training data given D v w iN i1 v w training input target values We use individual GP models output dimension assume scalar target values w subsequent discussion For new test input v predictive distribution pwv D posterior Gaussian process Gaussian N wμ σ 2 mean variance cid11 cid121 μ kT σ 2 σ 2 cid3 K σ 2 cid3 I kv v kT w cid11 K σ 2 cid3 I cid121 k 14 15 respectively I identity matrix K N N kernel matrix elements K j kv v j k denotes kernel vector query input ki kv v The parameter σ 2 cid3 represents variance noise cid3 The covariance function k deﬁnes similarity measure input data We use squared exponential function covariance function cid11 k v v cid12 cid9 σ 2 f exp cid7 v v 1v v cid9T L 2 cid9 cid8 L diagl2 diagonal matrix containing bandwidth parameters squared exponential kernel The f represents variance function We refer parameters l σ f σcid3 hyperparameters GP To parameter σ 2 obtain accurate predictions hyperparameters set properly To optimize hyperparameters GP maximizing marginal loglikelihood gradientbased optimizers 32 The computational complexity optimizing hyperparameters dominated ON 3 matrix inversion Eqs 1415 To reduce computational demands use sparse GP models 3639 GPREPS regarded combination information theoretic PS contextual PS modelbased RL address important problem learning real world robot skill eﬃciently Current modelbased policy search methods rely A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 425 deterministic approximate inference methods While approximate inference eﬃcient computing policy gra dient suffers severe limitations The structure policy representations reward functions limited speciﬁc type functions approximation cause severe bias policy update GPREPS uses sampling evaluate new contextparameter pair By use sampling rely assumption policy representation reward function sample policy evaluate reward function given state action pair Due increased generality eﬃcient modelbased policy search wider range applications including contextual policy search application introduced paper However types structured policies possible For example learn mixture experts models similar introduced HiREPS method 8 53 Trajectory reward predictions Using learned GP forward model need predict expected reward cid2 Rsω τ ˆpτ ω sRτ sdτ 16 given parameter vector ω executed context s The expectation trajectories estimated learned forward models To obtain trajectory distribution closed form time step compute GP predictive distribution cid2 cid2 pxt1 pxt1xt utpxt utdxtdut 17 xt ut However query input Gaussian N xT T μxu cid5xu model f nonlinear predictive distribution state pxt1 nonGaussian Thus general obtain analytic solution pxt1 To overcome problem use samples solve integral Using sample state xt ﬁrst compute control ut π xt ω subsequently sample state predictive distribution pxt1xt ut We repeat procedure obtain complete trajectory t uT t An alternative approach solve integral Eq 17 use moment matching 910 Moment matching computes ﬁrst second moment predictive distribution pxt1 approximates Gaussian Moment matching deterministic approximate inference technique provides closed form solution However Gaussian approxi mation predictive distribution result biased trajectory distribution pτ ω s biased estimate expected reward Rsω Furthermore moment matching class lowerlevel controllers restricted functions map Gaussian analytically linear controllers squared exponentials trigonometric functions Thus policies infeasible example policies hard limit controls Such hard limit needs approximated use trigonometric functions Nevertheless obtaining trajectory ward distribution moment matching method choice particular gradient based policy search 9 accurate analytical estimates policy gradient required When sampling lowerlevel policy class restricted torque limits applied ease On hand accurately approximate trajectory distribution number sample trajectories L relatively high typically needs increase dimensionality As moment matching computationally highly demanding question arises method implemented eﬃciently Note sampling multiple trajectories time consists simple computations large matrices computations executed parallel Thus speed computations signiﬁcantly high throughput processors GPUs This particularly effective evaluating multiple artiﬁcial samples GPREPS Such parallelization straightforward moment matching approach In following evaluate sampling approach trajectory prediction compare moment matching algorithm terms accuracy computation time 531 Quantitative comparison sampling moment matching To compare moment matching sampling evaluated approaches predicting joint trajectory 4link simulated nonlinear pendulum The lowerlevel policy given linear trajectorytracking PD controller We 1500 observed data points 300 pseudoinputs train sparse GP models The prediction horizon set 100 time steps We sampled 1000 trajectories estimated Gaussian state distribution pxt time step samples These distributions ground truth Subsequently compute KullbackLeibler divergence KLpxt pxt approximations pxt obtained samples moment matching This procedure increasing number samples sampling approach To improve accuracy comparison evaluated prediction 100 independently chosen starting state varying initial variance To facilitate comparison prediction methods normalized KL divergence values moment matching prediction accuracy remains t1 KLpxtp M M xt 100 p M M xt state distribution predicted moment matching The constant cid4 100 426 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Fig 5 The sampling accuracy sampling increasing number samples In experiments 95 accuracy moment matching met sampling 50 samples prediction However cases sample 20 trajectories reach moment matching performance b Comparison computation speed moment matching samplingbased longterm prediction 50 sampled trajectories needed reach accuracy moment matching Over 7000 samples created GPU implementation computation time needed moment matching approach result shown Fig 5a As ﬁgure shows best 95 experiments required approximately 50 sample trajec tories reach accuracy moment matching approach However cases 75 sample 20 trajectories reach moment matching performance The inaccuracies moment matching ap proach results outliers nonGaussian state distributions violate Gaussian approximation assumption moment matching We evaluated computation time approaches For sampling approach evaluated computation time different types parallelization As observed Fig 5b single CPU core implementation outperforms moment matching produce approximately 1000 samples computation time This number increased 7000 samples highend graphics card We showed sample trajectories needed meet accuracy moment matching approach able generate thousands trajectories computation time Thus sampled trajectories results moderate speedup main stream computers improved GPU imple mentation In addition improved computation speed sampling avoids approximations involved moment matching approach example trigonometric functions approximate torque limits 9 Sampling produces unbi ased estimates expected reward improve accuracy prediction increasing number samples Nevertheless moment matching favorable algorithms require computation gradient state distribution wrt policyparameters PILCO 9 532 Comparison Gaussian process models When GP models trajectory prediction computation times consideration As discussed earlier training time hyperparameters standard GP approach scales cubically number training samples The prediction time posterior mean scales linearly computation posterior variance scales quadratically number training samples When learning dynamic models high sampling rate number training samples quickly increase thousands computation time impractically large To mitigate computational demand learning accurate models sparse Gaussian process methods proposed 3639 In general sparse GP methods maintain set M N highly representative training samples N total number training samples When sparse methods training time scales OM2 N posterior mean variance prediction time scales OM OM2 respectively When learning GP forward models numerical problems emerge In order learn accurate models need thousands training samples v w When training samples matrix K σ 2 cid3 I GP pre diction model training overly high condition number Thus computing inverse matrix result numerical problems easily lead inaccurate trajectory prediction To avoid problem common strategy add noise training target data w w cid3add 1 N This naturally increase value σcid3 decrease condition number K σ 2 add 2 stdw The additive noise proven eﬃcient training data standard deviation σadd 10 balancing prediction accuracy numerical instability cid3 I In experiments added iid Gaussian noise cid3add N 0 σ 2 In following compare accuracy reward prediction control task sparse GP method pseudo inputs 36 standard GP approach 32 We tested sparse method presented 39 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 427 Fig 6 Comparison standard sparse GP approach Top left increased additive noise offers higher numerical stability lower prediction accuracy Top right obtain good model 50 hyperparameter optimization steps Bottom left sparse method diﬃculties capturing stochasticity increasing control noise Bottom right increasing training data clearly positive effect prediction performance ran numerical problems method The control problem balance simulated planar 4link pen dulum upright position The lowerlevel control policy set pendulum robustly balanced optimal upright position random set initial positions upright position We collect measure ment data executing certain experiment rollouts Then train GP models standard sparse model 36 measurement data Subsequently use GP models predict reward 50 contextparameter pairs 20 time steps We use 20 trajectories contextparameter pair The corresponding reward single trajectory given r ˆτ t1xt xrT xt xr xr represents upright position Finally measure accuracy GP models computing average quadratic error mean reward prediction Esωe2 Esωˆrs ω rs ω2 ˆrs ω denotes mean predicted reward rs ω real reward context parameter pair cid4 T First investigate inﬂuence additive noise prediction performance We set additive 2 stdw α scaler Second investigate hyperparameter optimization steps noise σadd α10 required learn accurate models In experiment evaluate models capture stochasticity dynamics adding noise control input Finally investigate models generalize limited training data For ﬁrst experiments use 50 sample trajectories learn model varied number sample trajectories experiment In experiment vary parameter remaining parameters optimal value We set optimal values GP models provide best prediction performance In particular chosen 2 stdw α 1 We optimized hyperparameters standard deviation additive noise value σadd 10 models 150 optimization steps assumed 05 Nm standard deviation additive torque noise We 50 observed trajectories training total 1000 training data points For sparse method 25 observed data points pseudo inputs M N4 The results model comparison tasks seen Fig 6 With increasing additive noise factor gain numerical stability accuracy reward prediction decreases slightly However observed sparse method overﬁts data results worse performance higher additive noise factor When comparing hyperparameter optimization steps conclude optimization 50 steps obtain accurate models However small overﬁtting effect sparse models continue optimization When add additional control input noise standard GP approach capture uncertainty However additive noise experiment sparse method tends overﬁt data produces inaccurate predictions Finally increasing number sample trajectories clearly positive effect prediction accuracy However training time steeply increases higher training data 428 6 Results A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 We evaluated dataeﬃcient contextual policy search method contextfree comparison task contextual motor skill learning tasks In contextfree task compare GPREPS approach competing PILCO method simulated 4link pendulum balancing task In contextual learning tasks learn throw ball distinct targets simulated 4link robot In second task 7DoF robot arm learn target puck hockey game Here present simulated results real robot results We compare approach CrKR 20 contextual modelfree REPS In experiment use GPREPS learn play table tennis simulated robot arm As lowerlevel controllers needs scale anthropomorphic robotics implement Dynamic Movement Primitive 17 approach brieﬂy review 61 Dynamic Movement Primitives To parametrize lowerlevel policy use extension 19 Dynamic Movement Primitives DMPs introduced 17 A dynamic movement primitive deﬁned second order dynamical acts like springdamper activated nonlinear forcing function f cid11 xt τ 2αx zt τ αz zt βxg xt xt cid12 τ 2 f zt v 18 19 constant parameters αx βx αz Typically separate DMP joint robot The phase variable zt acts internal clock movement It shared joints synchronizes joints It simulated separate ﬁrst order dynamical initialized z0 1 It converges 0 t drives nonlinear forcing function f The parameter g unique point attractor The springdamper modulated function f zt v φzt T v linear weights v nonlinear phase zt The weights v specify shape movement initialized expert demonstrations The basis functions φ izt 1 K activate weights trajectory evolves The basis functions deﬁned φizt cid4 expzt ci22σ 2 zt j1 expzt c j22σ 2 K j ci center basis center σi bandwidth The squared exponential basis functions multiplied zt f vanishes t Thus t DMP behave linear stable point attractor g The speed trajectory execution regulated time scaling factor τ R The weight parameters v DMP initialized observed trajectories xobs xobs xobs solving cid11 v Φ T Φ cid121Φ T Y Y 1 τ 2 cid11 xobs αx βxg xobs xobs cid12 20 Φt φ T zt matrix basis vectors time step t In robot skill learning task adapt weight parameters v goal attractor g time scaling factor τ optimize trajectory Additionally adapt ﬁnal desired velocity g movement extension given 19 To reduce dimensionality learning problem usually learn subset DMP hyperparameters For example learning return balls table tennis initialize ﬁx weights v expert demonstration Subsequently adapt goal attractor g ﬁnal velocity g DMPs maximize reward After obtaining desired trajectory DMP trajectory followed feedback controller lowerlevel control policy In presented tasks motor primitive executed predeﬁned time For detailed description DMP framework refer 19 62 Exploiting reward models prior knowledge A simple approach improve dataeﬃciency modelfree contextual REPS algorithm use known reward model Rτ s evaluate single outcome trajectory multiple contexts s Such strategy possible evaluated trajectories τ depend context variables s For example context speciﬁes desired target throwing ball use ball trajectory evaluate reward multiple targets s 63 4link pendulum balancing task In task goal ﬁnd PD controller balances simulated 4link planar pendulum upright position The pendulum total length 2 m total mass 70 The reward sample trajectory cid4 t Q xt xt deviation upright position xT sum quadratic rewards trajectory Rτ s T t time t We chose initial state distribution upright position Gaussian We compare GPREPS A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 429 Table 3 Required experience achieve reward limits 4link balancing problem Higher rewards require better policies Reward limit Required experience 100 10 15 PILCO 1018 s 1146 s 1218 s GPREPS 1068 s 2052 s 3850 s REPS 1425 s 2300 s 4075 s Fig 7 The learning progress algorithms computational times Note time taken sample evaluation robot included computational time Thus modelfree REPS outperforms modelbased approaches However realworld learning problems sample evaluation robot time consuming lead robot wear modelbased methods preferred For evaluation use standard desktop PC quadcore Intel i5 CPU highend GPU Nvidia GTX Titan high doubleprecision computational power modelfree REPS 30 PILCO 912 state art modelbased policy search algorithm As PILCO learn contextual policies learn contextfree upperlevel policies REPS GPREPS fair comparison Thus upperlevel policy given Gaussian π ω N ωμ cid5 The lowerlevel controller represented PD controller ut GxT T The gain matrix G 48 obtained reshaping parameter vector ω321 We initialize models t modelbased approaches 6 seconds real experience collected random policy We use sparse GP models 36 PILCO GPREPS x T t In Table 3 required real experience reach certain reward limits As shown table GPREPS requires orders magnitude fewer trials REPS converge optimal solution PILCO achieved faster convergence gradientbased optimizer computes greedy updates GPREPS continued explore The difference PILCO decreased updating policy data collections However difference negligible compared difference modelfree method Despite fact PILCO GPREPS signiﬁcantly outperformed modelfree REPS terms dataeﬃciency modelbased algorithms typically require higher computational time policy update In Fig 7 learning progress computational time required algorithms In evaluation consider time taken algorithms omit time taken policy evaluation robot In regard REPS superior compared modelbased algorithms samples evaluated robot Note comparison heavily biased favoring modelfree method Nevertheless experiment gives intuition scale real world computational times working real robots For GPREPS PILCO use GP models model training takes signiﬁcant time Note learning scenario 1 second real experience corresponds 50 data points 12 dimensional training input 8 dimensional training target Other model training modelbased methods require additional computational time policy update For GPREPS use sampling approach evaluate 500 artiﬁcial policy parameter samples predicted trajectory consists 100 steps use 20 trajectories single parametrization In words GPREPS requires prediction 10000 trajectories policy update evaluated eﬃciently GPUs Fig 5b For PILCO hand use parallelization straightforwardly computations involved momentmatching approach gradient computations Additionally PILCO repeatedly recomputes policy gradient converges local minimum Thus computational times vary signiﬁcantly PILCO As ﬁgure GPREPS requires roughly order magnitude higher computational time compared modelfree REPS PILCO number varies 15 2 orders magnitude Although computational times generally higher modelbased algorithms especially GP models realworld scenario modelbased approaches learn task faster compared modelfree methods Some real robot experiments require minutes evaluate learning times impractically large modelfree algorithms Furthermore aforementioned concerns robot experiments robot wear requirement expert supervision directly addressed modelfree methods 430 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 64 Ballthrowing task In task 4link robot learn throw ball target position The target position s x y uni formly distributed speciﬁed range learn contextual upperlevel policy π ωs The context varied x 5 15 m y 0 3 m The lengths masses links set l 05 05 10 10 m m 175 175 265 85 respectively The robot coarsely models human joints representing ankle knee hip shoulder In experiment GPREPS ﬁnd DMP shape parameters v throwing ball multiple targets maintaining balance The reward function deﬁned Rτ s c1 min t cid4bt scid42 c2 cid13 cid13 fcxt c3 u T t ut t t The ﬁrst term punishes minimum distance ball trajectory b target s We learning problem challenging penalizing joint angles unrealistic humanlike throwing motion Thus second term describes punishment term force robot stay given joint limits humanlike throwing motion learned The penalty term deﬁned fcxt xl xtT Ilxl xt xu xtT Iuxu xt Il Iu diagonal weighting matrices diagonal elements value 0 joint limit constraints violated 1 joint limits violated cid14 cid14 Il ii 0 1 xti xli xti xli Iu ii 0 1 xti xui xti xui The joint angle angular velocity limits deﬁned ql qu 08 25 01 π T rad 08 005 2 π T rad l qT T xu qT l ql qu 50 50 50 50T rads 50 50 50 50T rads u qT ﬁnally xl qT u experiment set reward weighting factors c1 102 c2 103 c3 10 T The term reward function favors energyeﬃcient movement In 8 As lowerlevel controllers DMPs 10 basis functions joint We modiﬁed shape parameters ﬁxed ﬁnal position velocity DMP upright position zero velocity In addition lowerlevel policy contained release time tr ball free parameter resulting 41dimensional parameter vector ω After generating reference trajectory DMPs use PD trajectory tracker controller generate control inputs ut To produce trajectory rollouts modelbased GPREPS learn distinct models environment The ﬁrst model represents dynamics robot We use observed state transitions training samples model The second model predict initial position velocity ball release time tr The GP model represents free dynamics ball ﬂight It predict trajectory ball initial state predicted second GP model The policy π ωs initialized robot expected throw ball approximately 5 m main taining balance led high penalties We policy applying contextfree REPS GPREPS learned accurately hit target given range targets Fig 8 shows learned motion sequence different targets The displacement targets s 13 3T m raise 05 m maximal error smaller 10 cm The policy chose different DMP parametrizations release times different target positions To illustrate effect target positions s1 6 1 m s2 12 3 m Fig 8 When target farther away robot showed distinctive throwing movement released ball slightly later The learning curves REPS GPREPS shown Fig 9 In addition evaluated REPS known reward model Rτ s generate additional samples randomly sampled contexts s We denote experiments extra context We evaluated GPREPS learning expected reward model directly Rsω f s ω function context parameters ω GP model denoted direct Additionally investigated learning scenario observe individual terms decomposed reward function learn models predict individual terms policy parameters ω context s The terms observe distance penalty Rdist c1 mint cid4bt scid42 state constraint penalty term R x c2 t ut This experiment represents transition standard GPREPS GPREPS direct approach use limited expert knowledge task Thus method easy implement new test setting In following refer approach GPREPS reward decomposition t fcxt torque penalty term R u c3 t u T cid4 cid4 Fig 9 shows REPS converged good solution 5000 episodes cases In instances observed premature convergence resulting suboptimal performance The performance REPS improved extra samples generated known reward model extra context In case REPS converged good A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 431 Fig 8 Throwing motion sequence The robot releases ball speciﬁed release time hits different targets high accuracy Fig 9 Learning curves ballthrowing problem The shaded regions represent standard deviation rewards 20 independent trials GPREPS converged 3040 interactions environment REPS required 5000 interactions Using reward model generate additional samples REPS led better ﬁnal policies compete GPREPS terms learning speed Learning direct reward model Rsω f s ω yielded faster learning modelfree REPS quality ﬁnal policy limited solutions For GPREPS sampled trajectories initially obtain conﬁdent GP model We evaluated sample policy update subsequently updated learned forward models We 500 artiﬁcial samples 20 sample trajectories sample obtain expectation GPREPS converged good solution cases 3040 real evaluations Directly learning Rsω resulted improved learning speed observed highly varying average lower quality resulting policies GPREPS direct However observe ﬁnal results GPREPS reward decomposition approach better GPREPS direct worse GPREPS Interestingly ﬁnal solution consistently better REPS Thus conclude limited expert knowledge task provide better results compared modelfree REPS Note approach easy implement novel test scenario This results conﬁrms intuition decomposing forward model multiple components simpliﬁes learning task 641 Inﬂuence number artiﬁcial samples In following investigate inﬂuence number artiﬁcial samples update policy learning performance To obtain accurate estimation distribution ps ω interested high number artiﬁcial samples However computation time policy updates linearly increases number artiﬁcial samples generated GP models While long policy update interval inﬂuence data eﬃciency GPREPS practical point view prefer low possible affecting learning performance In Fig 10a learning curves GPREPS different artiﬁcial samples As ﬁgure shows increasing artiﬁcial samples positive inﬂuence learning performance However difference 500 samples Using lower number artiﬁcial samples resulted lower quality ﬁnal policies highly varying performance 642 Learning stochastic dynamics For learning problems dynamics robot environment stochastic variance result ing trajectory τ variance reward Rτ s considerably large As discussed earlier 432 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Fig 10 The learning curves GPREPS different artiﬁcial samples An increasing artiﬁcial samples clearly improves performance However observe difference learning eﬃciency 500 samples Using lower number artiﬁcial samples resulted lower quality ﬁnal policies b The reward learned ﬁnal policy level dynamics stochasticity By GP models GPREPS able capture stochasticity higher noise Thus policy update able approximate expected reward On hand REPS tends produce worse ﬁnal policies uses reward samples real distribution expected reward modelfree REPS algorithm typically require single rollout evaluation given contextparameter pair assume expected outcome While reasonable assumption deterministic problems real world robot tasks contain level stochasticity GPREPS avoids problem learning stochastic outcomes averaging multiple samples contextparameter pair evaluation Eq 13 In following demonstrate effect noise dynamics learning performance robot throwing task We implement stochasticity dynamics adding Gaussian noise initial state ball release time In real robot experiment stochasticity emerge unknown dynamics ball release robot hand The standard deviation initial position ball set α75 cm initial velocity set α30 cms α scaler We investigate effect increasing noise learning performance varying α 0 1 In Fig 10b quality converged policy REPS GPREPS increasing level stochas ticity dynamics As ﬁgure shows GPREPS outperforms REPS deterministic case However increasing level stochasticity quality ﬁnal policy learned REPS gets signiﬁcantly worse The reason phenomena REPS uses noisy samples reward instead expected reward In contrast forward models GPREPS learns stochasticity able approximate expected reward Even signiﬁcant noise α 1 GPREPS able avoid converging local optima provides good quality policies relatively low variance 65 Robot hockey simulated environment In task learn robot hockey game KUKA lightweight robot arm Fig 1 The goal robot shoot hockey puck attached hockey stick target puck located certain distance The robot target puck hitting puck denote control puck The initial position target puck bx b yT varied dimensions experiments As additional goal require displacement experiments Thus target puck dt close possible desired distance d robot learn shoot provided puck direction target puck appropriate force The simulated hockey task depicted Fig 11 The context variable deﬁned s bx b y d We vary distance d T We ﬁrst evaluated method simulation We encoded hitting motion DMP The weight parameters set imitation learning We learn ﬁnal position g ﬁnal velocity g time scaling parameter τ DMP As robot seven degrees freedom 15dimensional parameter vector ω lowerlevel policy For trajectory tracking inverse dynamics controller We chose initial position target puck uniformly distributed robots base displacements bx 15 25 m b y 05 1 m The desired displacement context 0 1 m The reward function parameter d cid15 cid15 uniformly distributed d cid15 cid15dT d Rτ s min cid4xt bcid42 2 t consists terms equal weighting The ﬁrst term penalizes missing target puck located position b bx b yT control puck trajectory x1T The second term penalizes error desired displacement target puck dT resulting displacement target puck shot A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 433 Fig 11 Robot hockey task The robot shoots control puck target puck target puck speciﬁed distance Both initial The learned skill location target puck bx b y T desired distance d different contexts s shown robot learned place target puck desired distance puck varied The context given s bx b y d Fig 12 Learning curves robot hockey task GPREPS able learn task 120 interactions environment modelfree version REPS able ﬁnd highquality solutions b The GPREPS learning curve real robot arm The error bars represent standard deviation 5 independent evaluations In robot hockey task modeling contact stick control puck challenging curved shape hockey stick When shooting puck stick push hit puck multiple times To avoid extensive modeling contacts use GP model directly predict state puck constant distance 02 m x direction robots base contact stick puck longer possible We use solely DMP parameters ω input model We learn model free dynamics sliding pucks predicting puck trajectories For predicting contact pucks assume know radius pucks predict contact happening For modeling effect contact learn separate GP model predicts state pucks contact given state pucks contact We compared GPREPS modelfree REPS CrKR 20 stateoftheart modelfree contextual policy search method Furthermore evaluated GPREPS decomposing experiment directly predict reward Rsω GP model policy parameter ω context s input GPREPS direct The resulting learning curves shown Fig 12a GPREPS learned task 120 interactions environment modelfree version REPS needed approximately 10000 interactions Moreover policies learned modelfree REPS lower quality The GPREPS direct algorithm resulted faster convergence modelfree REPS version resulting policies lower quality CrKR uses kernelbased representation policy For fair comparison linear kernel CrKR The results CrKR compete modelfree REPS We believe reason worse performance CrKR lies uncorrelated exploration strategy The resulting policy CrKR Gaussian diagonal covariance matrix REPS estimates covariance matrix Moreover CrKR use informationtheoretic bound determine weightings samples The learned movement shown Fig 11 different contexts After 100 evaluations GPREPS placed target puck accurately desired distance displacement error 5 cm We evaluated performance GPREPS hockey task real KUKA lightweight arm Fig 1 A Kinect sensor track position pucks frame rate 30 Hz We smoothed trajectories pre processing step Butterworth ﬁlter We slightly changed context variable ranges meet physical constraints test environment We decreased range position variables dimensions bx 15 2 m 0 06 m We kept b y 04 08 m robots base Furthermore decreased desired distance range d 434 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Fig 13 The BioRob Robot b The table tennis learning setup The incoming ball ﬁx initial position random initial velocity v v x v y v zT The velocity distribution deﬁned incoming ball lands inside Landing zone The goal robot hit coming ball return position b bx b y T distributed uniformly inside Return zone The context variable contains initial velocity ball target return position s v T bT T reward function unchanged slightly altered modeling environment Due low sampling fre quency Kinect sensor receive information exact contact model pucks To avoid errors coming crude model exchange contact model model directly predicts displacement second puck incoming pucks relative position velocity The resulting learning curve GPREPS shown Fig 12b As robot adapts lowerlevel policy parameters optimum small low number interactions real environment The ﬁnal reward slightly different compared simulated environment altered modeling slightly distorted measurement data Kinect sensor Despite effects GP models average uncertainty produce accurate predictions expected rewards 66 Robot table tennis In task learn hitting strokes table tennis game simulated Biorob 25 arm Fig 13a The robot mounted linear axis moving horizontal plane The robot rotational joints resulting 8 actuated joints A racket mounted endeffector robot The Biorob lightweight tendondriven robot arm small weight perform highly dynamic movements The simulated robot seen Fig 13b The construction real robot platform ongoing work In simulation simulated ball standard ballistic ﬂight model air drag neglected simulating spin measurement noise The goal robot return incoming ball target position opponents table However incoming ball changing initial velocity v return target position b varied uniformly opponents table Thus context deﬁned s v x v y v z bx b yT For illustration task Fig 13 We chose range initial velocities incoming ball bounces forehand table To learn task use DMPs initialize DMP weight parameters kinesthetic teachin movement resembles forehand hitting motion We learn ﬁnal positions ﬁnal velocities DMP trajectories furthermore τ timescaling parameter starting time point movement altogether 18 parameters The initialized policy able execute demonstrated movement adapting incoming ball We decompose experiment ﬁve distinct models With ﬁrst model predict landing position landing velocity landing time incoming ball observed initial velocities v ball Such model suﬃcient modeling want learn return balls land exactly table The second model predicts trajectory ball given position velocity predicted ﬁrst model The fourth model predicts trajectory orientation racket mounted endeffector To avoid complex modeling 8DOF robot dynamics use time dependent GP models directly predict position orientation quaternion representation policy parameters To create time dependent models ﬁt linear basis function model 40 local basis functions φt dimension trajectory racket basis functions depend execution time trajectory The task GP predict weights basis functions given policy parameters ω The training input model lowerlevel policy parameter ω training target local model weight ν Finally ﬁfth model predicts landing position returned ball case contact detected SVM classiﬁer linear kernel The input classiﬁer relative velocity ball racket position orientation racket time point absolute distance racket ball minimal For contact model use training input data classiﬁer observed landing position p px p y training target A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 435 Fig 14 The learning curves table tennis experiment REPS able learn good policy 4000 evaluations learns suboptimal policy hits ball net When GPREPS directly predicts reward contextpolicy parameter pairs GPREPS direct resulting policies showed highly varying performance getting stuck local optima However GPREPS exploit given structure experiment provided consistent performance ﬁnal policy able return ball 30 cm target position avoiding hitting ball net This behavior learned 150 evaluations Fig 15 Animation shots different targets different serving positions ball learned GPREPS The reward function deﬁned sum penalties missing ball missing target return position Rτ s c1 min cid4τ b τ rcid42 c2cid4b pcid42 21 c c1 c2T weighting parameters τ b τ r incoming ball racket trajectories b target p returned ball landing position As learning good contact model racket ball requires samples ﬁrst iterations focus learning hit ball As soon learned good hitting stroke contact samples use learned contact model provide conﬁdent predictions Thus change weighting parameters c c1 c2T beginning learning c2 negligible compared c1 add extra constant penalty term By algorithm focuses learning hit ball After collecting samples learn good contact model set c1 c2 disable constant penalty term Now algorithm focuses hitting ball returning close target position b Note use c1 c2 modelfree algorithms We compared GPREPS modelfree REPS modelbased REPS directly predict reward contextpolicy parameter pairs The learning curves depicted Fig 14 We clearly GPREPS outperforms algorithms GPREPS consistently provides high quality policies 150 evaluations The ﬁnal policy avoids hitting net displacement desired target return position remains 30 cm An example complete experiment outcome prediction depicted Fig 16 When GPREPS direct approach use prior knowledge structure experiment obtain policies stuck local optima Typically observe policies hit ball net miss ball The modelfree REPS approach results good performance general disadvantage dataineﬃcient Modelfree REPS requires 4000 evaluations average learn policy consistently returns ball instances hitting ball net An animation different strikes learned GPREPS shown Fig 15 This experiment concludes GPREPS applicable learn complex robotic tasks presence contact models general diﬃcult learn In future work investigate use GP modelbased version HiREPS 8 learn forehand backhand hitting motion Furthermore order learn play 436 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Fig 16 An example prediction outcome table tennis experiment The ﬁrst second model predicts initial position velocity incoming ball trajectory bouncing black lines The fourth model predicts position blue lines orientation depicted racket After detecting contact predict returned position ball red diamonds When compare predictions real experiment trajectories red green lines high accuracy predictions The models learned 100 experiment rollouts sample 10 trajectories capture stochasticity For interpretation references color ﬁgure legend reader referred web version article general table tennis game extend context allows balls varying initial positions evaluate GPREPS real robot platform 67 Initialization limitations upperlevel policy To ensure safety eﬃciency learning process properly initialize upper level policy pa rameters In experiments linear Gaussian model representing upper level policy π ωs N ωa s A cid5 parameters A cid5 Initially set linear model A 0 zero obtain offset pa rameter imitation learning We set initial exploration covariance cid5 diagonal matrix initial exploration parameter space exploration safe robot As REPS typically decreases explo ration variance policy update step cid5 collapses policy parameters converge ﬁnal solution Thus initial cid5 chosen carefully optimal solution range initial exploration range Model based solutions gradientbased policy updates PILCO scale higher dimensions neglect exploration problem With REPS usually learn policies parameters computation covariance matrix gets intractable Thus currently GPREPS applied complex robot learning tasks 100 parameters 68 Learning real robots For evaluations presented paper robot simulators task KUKA lightweight arm For cases convenient use robot simulator demonstrate learning eﬃciency ulti mately goal provide good real world results As simulators typically use hand tuned models robot environment skills learned simulators perform poorly real robot This inaccuracies hand crafted models limited expert knowledge lack model adaptation In approach propose learn models interacting environment The resulting data driven modeling approach addresses inaccuracies resulting imperfectly tuned models lack accurate expert knowledge Moreover able adapt learned models continuously updating model parameters recent measurement data We chose Gaussian Process regression modeling approach signiﬁcantly reduce bias coming limited representational power mathematical models When working real robot experiments obtaining measurement data model learning im portant challenge With real world experiments measured quantities typically corrupted noise need ﬁltered reliable estimate Furthermore devices lower sampling frequency provide measurement data ultimately increases complexity learned model Interpolation help build reliable mod els introduce higher model bias To avoid poor quality models advisable validate learned models simulate experiment rollouts In tasks necessary thoroughly model experiment order infer reward rollout suﬃcient predict relevant quantities computing reward policy context parameters We illustrated idea ball throwing task decomposed reward function solved regression problems obtain models This approach typically lower accuracy signiﬁcantly faster A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 437 computational time assuming longterm trajectory prediction GP models necessary On hand presumably lower quality models number required evaluations slightly increase Nevertheless new experiment advisable begin simpler modeling approach reduce modeling effort expert knowledge required 7 Conclusion We presented GPREPS novel modelbased contextual policy search algorithm based GP models information theoretic policy search We learn upper level policy eﬃciently generalizes lower level policy parameters ω multiple contexts GPREPS based REPS informationtheoretic policy search algorithm It exploits learned probabilis tic forward models robot environment predict expected rewards artiﬁcially generated data points For evaluating expected reward GPREPS samples trajectories learned models Unlike deterministic inference meth ods stateofthe art approaches policy evaluation trajectory sampling easy implement easy parallelize limit policy class reward model With simulated real robot experiments demonstrated GPREPS signiﬁcantly reduces required measurement data learn high quality policies compared stateoftheart model free contextual policy search approaches Moreover GP models able incorporate model uncertainty produce accurate trajectory distri butions Thus GPREPS avoid risk learning noisy reward samples results bias modelfree REPS formulation The increased data eﬃciency makes GPREPS applicable learning contextual policies realrobot tasks Since existing modelbased policy search methods applied contextual setup GPREPS allows new applications modelbased policy search Acknowledgement The research leading results received funding European Communitys Seventh Framework Pro gramme FP720072013 grant agreement 270327 CompLACS Marc Peter Deisenroth supported Imperial College Junior Research Fellowship Appendix A Derivation contextual episodebased REPS The constrained optimization problem episodebased REPS contextual policy search given cid2 cid2 max p st sω cid2 cid2 sω cid2 cid2 sω cid2 cid2 sω ps ωRsωdsdω ps ω log ps ω qs ω dsdω cid3 ps ωφsdsdω ˆφ ps ωdsdω 1 We write Lagrangian corresponding constrained optimization problem form cid2 cid2 cid7 cid2 cid2 Lp η θ ps ωRsωdsdω η sω cid7 cid2 cid2 cid3 ps ω log cid7 cid2 cid2 sω cid8 ps ω qs ω dsdω cid8 cid8 θ T ˆφ sω ps ωφsdsdω λ 1 ps ωdsdω sω By setting gradient Lp η θ wrt ps ω zero obtain solution ps ω qs ω exp cid7 Rsω θ T φs η cid8 exp cid8 cid7 η λ η cid3cid3 base line V s θ T φs Due constraint cid9cid2 cid2 cid7 cid8 cid7 exp η λ η qs ω exp sω Rsω θ T φs η cid8 cid101 dsdω sω ps ωdsdω 1 A1 A2 A3 A4 A5 The dual function obtained setting solution ps ω Lagrangian After rearranging terms obtain gη θ λ η λ ηcid3 θ T ˆφ η log exp ηcid3 θ T ˆφ A6 cid7 cid8 η λ η 438 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 Setting Eq A5 dual eliminate λ parameter obtain dual function cid7cid2 cid2 gη θ η log qs ω exp cid7 Rsω θ T φs η cid8 cid8 dsdω ηcid3 θ T ˆφ A7 sωi1N context parameter pairs sampled qs ω integral Using dataset D s dual function approximated sω ωi Ri gη θ D η log cid16 cid7 exp 1 N Ncid13 i1 Ri sω θ T φs η cid8cid17 ηcid3 θ T ˆφ A8 The dual function convex η θ 30 To solve original optimization problem need minimize gη θ D η 0 6 solve constrained optimization problem easier solve We use solver problems interior point algorithm For eﬃcient optimization dual corresponding gradients dual required They given gη θ η gη θ θ cid3 log cid4 ˆφ References cid11 Z ωi s cid12 1 N Ncid13 i1 cid4 N i1 Z s η N i1 Z s cid4 N ωiφs i1 Z si ωi cid11 Z s ωiRi cid4 sω θ T φs N i1 Z si ωi cid7 cid12 ωi exp Ri sω θ T φs η cid8 A9 A10 1 P Abbeel M Quigley AY Ng Using inaccurate models reinforcement learning Proceedings International Conference Machine Learning 2006 Robotics Automation 1997 Conference Robotics Automation 2001 ICML 2000 pp 4148 2 CG Atkeson JC Santamaría A comparison direct modelbased reinforcement learning Proceedings International Conference 3 JA Bagnell JC Schneider Covariant policy search Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI 2003 4 JA Bagnell JG Schneider Autonomous helicopter control reinforcement learning policy search methods Proceedings International 5 J Baxter P Bartlett Reinforcement learning POMDPs direct gradient ascent Proceedings 17th Intl Conference Machine Learning 6 S Boyd L Vandenberghe Convex Optimization Cambridge University Press 2004 7 BC da Silva GD Konidaris AG Barto Learning parameterized skills Proceedings International Conference Machine Learning ICML June 2012 TATS 2012 Machine Learning 2011 Systems 2011 Automation ICRA 2014 8 C Daniel G Neumann J Peters Hierarchical relative entropy policy search International Conference Artiﬁcial Intelligence Statistics AIS 9 MP Deisenroth CE Rasmussen PILCO modelbased dataeﬃcient approach policy search Proceedings International Conference 10 MP Deisenroth CE Rasmussen D Fox Learning control lowcost manipulator dataeﬃcient reinforcement learning Robotics Science 11 MP Deisenroth P Englert J Peters D Fox Multitask policy search robotics Proceedings 2014 IEEE International Conference Robotics 12 MP Deisenroth D Fox CE Rasmussen Gaussian processes dataeﬃcient learning robotics control IEEE Trans Pattern Anal Mach Intell 2013 httpdxdoiorg101109TPAMI2013218 PrePrints 13 MP Deisenroth G Neumann J Peters A survey policy search robotics Found Trends Robot 2 12 2013 1142 14 P Englert A Paraschos J Peters MP Deisenroth Modelbased imitation learning probabilistic trajectory matching Proceedings 2013 IEEE International Conference Robotics Automation ICRA 2013 15 A Gams A Ude Generalization example movements dynamic systems International Conference Humanoid Robots Humanoids IEEE 16 DH Grollman A Billard Donut I learning failed demonstrations Proceedings IEEE International Conference Robotics Au 2009 pp 2833 tomation 2011 17 AJ Ijspeert S Schaal Learning attractor landscapes learning motor primitives Advances Neural Information Processing Systems NIPS 2003 18 J Ko D Klein Gaussian processes reinforcement learning identiﬁcation control autonomous blimp Proceedings Interna 19 J Kober K Mülling O Kroemer CH Lampert B Schölkopf J Peters Movement templates learning hitting batting Proceedings tional Conference Robotics Automation ICRA 2007 International Conference Robotics Automation 2010 20 J Kober E Oztop J Peters Reinforcement learning adjust robot movements new situations Robotics Science Systems RSS 2010 21 J Kober J Peters Policy search motor primitives robotics Mach Learn 2010 133 22 N Kohl P Stone Policy gradient reinforcement learning fast quadrupedal locomotion Proceedings International Conference Robotics 23 P Kormushev S Calinon DG Caldwell Robot motor skill coordination EMbased reinforcement learning Proceedings International 24 A Kupcsik MP Deisenroth J Peters G Neumann Dataeﬃcient contextual policy search robot movement skills Proceedings National 25 T Lens Physical humanrobot interaction lightweight elastic tendon driven robotic arm modeling control safety analysis PhD thesis TU Automation 2003 Conference Intelligent Robots Systems IROS 2010 Conference Artiﬁcial Intelligence AAAI 2013 Darmstadt Department Computer Science July 4 2012 2011 ICML ICML 2009 2009 mental Robotics MIT Press 2004 A Kupcsik et al Artiﬁcial Intelligence 247 2017 415439 439 26 K Muelling J Kober O Kroemer J Peters Learning select generalize striking movements robot table tennis Int J Robot Res 3 2013 263279 27 G Neumann Variational inference policy search changing situations Proceedings International Conference Machine Learning ICML 28 G Neumann W Maass J Peters Learning complex motions sequencing simpler motion templates International Conference Machine Learning 29 AY Ng H Jin Kim MI Jordan S Sastry Inverted autonomous helicopter ﬂight reinforcement learning International Symposium Experi 30 J Peters K Mülling Y Altun Relative entropy policy search Proceedings National Conference Artiﬁcial Intelligence AAAI 2010 31 J Peters S Schaal Reinforcement learning motor skills policy gradients Neural Netw 4 2008 682697 32 CE Rasmussen CKI Williams Gaussian Processes Machine Learning Adaptive Computation Machine Learning The MIT Press 2005 33 T Rückstieß F Sehnke T Schaul D Wierstra S Yi J Schmidhuber Exploring parameter space reinforcement learning PALADYN J Behav Robot 34 JG Schneider Exploiting model uncertainty estimates safe dynamic control learning Advances Neural Information Processing Systems NIPS 1 1 2010 1424 1997 35 F Sehnke C Osendorfer T Rückstieß A Graves J Peters J Schmiedhuber Parameterexploring policy gradients Neural Netw 23 2010 551559 36 E Snelson Z Ghahramani Sparse Gaussian processes pseudoinputs Advances Neural Information Processing Systems NIPS 2006 37 RS Sutton D McAllester S Singh Y Mansour Policy gradient methods reinforcement learning function approximation Adv Neural Inf 38 E Theodorou J Buchli S Schaal Reinforcement learning motor skills high dimensions path integral approach Proceedings Interna Process Syst 12 2000 10571063 tional Conference Robotics Automation ICRA 2010 39 M Titsias Variational learning inducing variables sparse gaussian processes J Mach Learn Res Proc Track 5 2009 567574 40 A Ude A Gams T Asfour J Morimoto Taskspeciﬁc generalization discrete periodic dynamic movement primitives IEEE Trans Robot 26 5 41 D Wierstra T Schaul J Peters J Schmidhuber Fitness expectation maximization Parallel Problem Solving Nature PPSN X Lecture Notes Computer Science SpringerVerlag 2008 pp 337346 42 RJ Williams Simple statistical gradientfollowing algorithms connectionist reinforcement learning Mach Learn 8 1992 229256 43 S Yi D Wierstra T Schaul J Schmidhuber Stochastic search natural gradient Proceedings 26th International Conference Machine Learning ICML 2009 pp 11611168 2010 800815