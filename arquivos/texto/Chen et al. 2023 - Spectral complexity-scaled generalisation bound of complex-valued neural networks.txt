Artiﬁcial Intelligence 322 2023 103951 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint Spectral complexityscaled generalisation bound complexvalued neural networks Haowen Chen abc Fengxiang He db Department Mathematics ETH Zürich 8092 Zürich Switzerland b JD Explore Academy JDcom Inc Beijing 100176 China c Department Mathematics Faculty Science The University Hong Kong Hong Kong Special Administrative Region d Artiﬁcial Intelligence Applications Institute School Informatics University Edinburgh Edinburgh EH8 9AB United Kingdom e School Computer Science Faculty Engineering The University Sydney Darlington NSW 2008 Australia Shiye Lei e Dacheng Tao eb r t c l e n f o b s t r c t Article history Received 12 November 2021 Received revised form 22 May 2023 Accepted 27 May 2023 Available online 5 June 2023 Keywords Complexvalued neural networks Generalisation Spectral complexity Complexvalued neural networks CVNNs widely applied ﬁelds primarily signal processing image recognition Few studies focused generalisation CVNNs vital ensure performance CVNNs unseen data This study ﬁrst prove generalisation bound complexvalued neural networks The bounds increase spectral complexity increases dominant factor product spectral norms weight matrices Furthermore work provides generalisation bound CVNNs trained sequential data affected spectral complexity Theoretically bounds derived Maurey Sparsiﬁcation Lemma Dudley entropy integral We conducted empirical experiments datasets including MNIST ashionMNIST CIFAR10 CIFAR100 Tiny ImageNet IMDB training complexvalued convolutional neural networks The Spearman rankorder correlation coeﬃcient corresponding pvalues datasets provide strong proof statistically signiﬁcant correlation spectral complexity network generalisation ability measured spectral norm product weight matrices The code available httpsgithub com LeavesLei cvnn _ generalization 2023 The Authors Published Elsevier BV This open access article CC BYNCND license httpcreativecommons org licenses bync nd 4 0 1 Introduction Complexvalued neural networks CVNNs garnered signiﬁcant attention ﬁelds signal processing 12 voice processing 3 image reconstruction 4 To reduce complex operations natural link CVNNs dimensional realvalued neural networks fewer degrees freedom 56 A complex number consists real imaginary alternatively expressed amplitude phase When performing computations complex numbers distinct arithmetic operations applied separately real imaginary parts Several recent studies endeavoured investigate different properties CVNNs built basic algorithms implementation For example Nitta 78910 proved orthogonality decision boundary complexvalued neu Corresponding author Artiﬁcial Intelligence Applications Institute School Informatics University Edinburgh Edinburgh EH8 9AB United Kingdom Email address FHeedacuk F He httpsdoiorg101016jartint2023103951 00043702 2023 The Authors Published Elsevier BV This open access article CC BYNCND license http creativecommons org licenses bync nd 4 0 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 rones addressed redundancy problem parameters CVNNs extended backpropagation algorithm complex numbers Trabelsi et al 11 organised essential components complexvalued deep neural networks complex convolutions complex batch normalisation complex weight initialisation Empirical studies conducted examine experimental performance CVNNs Hirose Yoshida 5 different neural networks including CVNNs process signals different coherence Nitta 7 computational cost CVNNs display higher learning speed realvalued neural networks Previous studies shown satisfactory experimental performance CVNNs However lack theoretical analysis generalisation ability This gap understanding motivated derive generalisation bound CVNNs This ﬁrst study provide theoretical evidence generalisation performance CVNNs We propose novel upper bounds positively correlate spectral complexity CVNNs trained independent identically distributed iid sequential data The spectral complexityscaled upper bounds suggest direct correlation generalisation ability CVNNs spectral norm product complexvalued weight matrices From empirical perspective experiments conducted investigate inﬂuence spectral complexity generalisation ability Speciﬁcally trained CVNNs stochastic gradient descent SGD standard datasets CIFAR10 CIFAR100 MNIST FashionMNIST Tiny ImageNet IMDB Excess risks collected analysis When training error zero datasets excess risk equals test accuracy informative expressing gen eralisation ability In addition change spectral norm product weight matrices primarily contributes change spectral complexity simulate spectral complexity Our experimental results demonstrate strong correlation spectralnorm product excess risk consistent theoretical analysis The code available httpsgithub com LeavesLei cvnn _generalization The remainder paper organised follows Section 2 presents motivation research pro vides review related work Section 3 provides introduction preliminaries complexvalued neural networks Section 4 presents theoretical results Section 5 presents experimental results In Section 6 comparison CVNNs realvalued neural networks explore novelties advantages proposed bound In Section 7 practical applications proposed theorems spectral normalisation algorithms discussed 2 Motivation related works Complex values widely adopted different neural networks biological 12 computational 713 representational advantages 1415 From biological perspective Reichert Serre 12 proposed complexvalued neuronal unit ap propriate abstraction modelling activity neurones brain realvalued unit To better process cortical information modelling mechanism consider ﬁring rate spike timing In incorporating elements deep neural networks amplitude complexvalued neuron represents ﬁring rate phase represents spike timing When inputs excitatory complexvalued neuron similar dissimilar phase information magnitude net input increase decrease depending phases similar correspond synchronous asynchronous situations respectively The incorporation complex values deep neural networks helps construct richer versatile representations Regarding computational aspect Danihelka et al 13 combined long shortterm memory LSTM concept holographic reduced representations complex values increase eﬃciency information retrieval Experiments showed method achieves faster learning speed multiple memorisation tasks Nitta 7 extended propagation algorithm complex values preserving basic idea realvalued backpropagation updates conducted real imaginary parts Through experiments demonstrated time complexity learning speed complex backpropagation deﬁnitely faster real speed learning rate low 05 Complexvalued neural networks provide advantages realvalued neural networks terms representational ability Arjovsky et al 14 proposed unitary recurrent neural network RNN unitary matrices weight matrix circumvent wellstudied gradient vanishing gradient exploding issues The unitary matrix generalised form orthogonal matrices complex ﬁeld absolute value eigenvalue 1 Compared orthogonal matrix complexvalued matrix richer representation particularly applications discrete Fourier Transform Wisdom et al 15 proposed fullcapacity unitary RNNs improving performance unitary evolution RNN uRNN Given advantages applications CVNNs increasing number researchers investigating properties complexvalued neural networks provide basic framework implementation CVNNs Nitta 8 demonstrated decision boundary twolayered complexvalued network orthogonal threelayered network decision boundary nearly orthogonal This reﬂects computational power versatility complex values In work Trabelsi et al 11 provided building blocks complexvalued deep neural networks including complex batch normalisation complex weight initialisation strategies They compared performances different activation functions datasets CIFAR10 CIFAR100 SVHN 2 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 While studies presenting empirical results generalisation performance complexvalued neural net works 75 lack theoretical evidence support ﬁndings Therefore study aims present ﬁrst upper bound generalisation error CVNNs Various complexity measures proposed derive upper bound generalisation error realvalued neural networks VCdimension Rademacher complexity 16 Bartlett et al 17 proved marginbased multi class generalisation bound based covering number Rademacher complexity These tools work Compared Bartlett et al 17s work work deﬁnes spectral complexity CVNNs deﬁning spectral norm complexvalued matrix providing generalisation bounds complexvalued neural networks processing regression tasks 3 Preliminaries This section introduces complexvalued neural networks CVNNs presents notations theoretical analysis 31 Model construction Each layer CVNN consists complexvalued neurones described The input signals weight pa rameters threshold values output signals complex numbers complexvalued neurones Assuming complexvalued neurone n linked m neurones previous layer net input neurone n described follows T n input mcid2 i1 W Xin Hn Here T n input denotes complexvalued network input neurone n W denotes weight connecting n neurones previous layer Xin denotes complexvalued input signal neurone neurone n Hn denotes threshold value neurone n If denote ReT n input input respectively output neurone n input real imaginary parts T n input amplitude phase T n input ImT n cid3 cid3 cid3 θ n input cid3 cid3 cid3T n respectively described follows cid4 T n output fr ReT n cid5 input cid4 cid5 input ReT n f T n output f p e cid4 cid5 θ n input cid4cid3 cid3 cid3T n input cid3 cid5 cid3 cid3 fa 1 2 Equation 1 describes output derived applying activation function separately real imaginary parts Equation 2 describes situation activation function applied amplitude phase fr activation function applied real f activation function applied imaginary f p activation function applied phase fa activation function applied amplitude 32 Complexvalued activation functions Several forms complexvalued activation functions corresponding realvalued functions proposed Arjovsky et al 14 proposed modReLU activation function preserves phase information applies realvalued ReLU function amplitude This function described follows modReLUz ReLUz beiθz cid6 z b z z 0 z b 0 In formula z denotes amplitude complex number z b R denotes threshold amplitude z Nitta 10 applied hyperbolic tangent function real imaginary parts input complex number propose following activation function σ z tanhRez tanhImz 1 tanhu def expu expuexpu expu u R These functions represent main types complexvalued activation functions applied real imaginary parts applied amplitude phase values There variations activation functions 3 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 zReLU CReLU 18 These different activation functions different properties terms satisfying CauchyRiemann equations Therefore important carefully select activation functions based speciﬁc task hand characteristics data 33 Basic notations deﬁnitions Suppose S z1 y1 z2 y2 z3 y3 zn ynzi Z Cd Z yi Y CdY training sample set yi corresponding label zi d Z dY dimensions z y separately We deﬁne D distribution following zi yi Assume network L layers ith layer ρi Lipschitz activation function σi Cdi Cdi activation functions CReLU function hyperbolic tangent function Their Lipschitz properties proven Appendix A weight matrix Ai Cdi1di applied input matrix passed previous layer Let σi0 0 A A1 A2 A L FA function computed CVNNs F Az σL A LσL1 A L1 σ1 A1z cid7 cid8 output FAz CdL For input data z1 z2 zn matrix Z Cnd formed collecting zi ith row Therefore output neu ral network expressed FAZ T ith column FAzi assumed d0 d Z d dL dY W maxd0 d2 dL To avoid ambiguity necessary clarify deﬁnition complexvalued matrix norm The norm complex matrix Ai j Cdk deﬁned norm corresponding realvalued matrix follows cid10 cid9 cid9 cid11cid9 cid9 p cid9 cid9 cid10cid3 cid3 Ai j cid3 cid3 cid11cid9 cid9 cid5 p Ai j Ai j denotes matrix jth entry Ai j In paper L2 norm calculated entrywise means L2 matrix norm deﬁned Frobenius norm cid12cid2 cid2 cid3 cid3 Ai j cid3 cid32 cid8 Acid8 2 cid5 j Moreover cid8cid8σ denotes spectral norm cid8 Acid8σ sup 1 cid8vcid8 2 cid13 cid8 A vcid8 2 λmax A A denotes Hermitian transpose A λmax denotes largest absolute value eigenvalues A Meanwhile A cid8 Acid8 pq deﬁned cid4cid9 cid9 A1 cid8 Acid8 cid5 cid9 cid9 cid9 pq cid9 cid9 p cid9 cid9 A2 cid9 cid9 p cid9 cid9 Am cid9 cid9 cid5cid9 cid9 cid9 q p A Cdm To investigate generalisation ability suﬃces derive high probability bound generalisation error E z yD lF Az y 1 n ncid2 i1 lF Azi yi lFAz y Z Y R denotes loss function usually set lF Az y F Az y2 Finally spectral complexity RA neural network FA deﬁned follows 17 RA ρi cid8 Aicid8σ cid14 Lcid15 i1 cid16 cid10 cid9 cid9 cid9 A cid923 21 cid8 Aicid823 σ Lcid2 i1 32 This complexity measure plays crucial role generalisation bound presented section 4 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 4 Main theorems proof sketch 41 Generalisation bound In section main theorems study presented Theorem 1 Independent identically distributed data Let S z1 y1 z2 y2 z3 y3 zn yn sample dataset size n elements drawn independently identically distribution D Given activation functions σi σi ρi Lipschitz σi0 0 weight matrices A A1 A2 A L stated Section 33 probability 1 δ corresponding complexvalued neural network satisfy following Ez yDlF Az y 1 n ncid2 i1 lF Azi yi 36 cid8Z cid8 2 8M n 3 2 2 ln2W lnnRA n cid12 ln 2 δ 2n 3M lFAz y FAz y2 denotes loss function lFAz y M z y It observed explicit occurrence combinatorial parameters L depth neural networks However upper bound depends L implicitly RA formed layers weight matrix norm Lipschitz constant layers activation function The proof provided Appendix B proof sketch presented Section 42 Theorem 2 Sequential data Consider S z1 y1 z2 y2 z3 y3 zn yn sample dataset ztt1 sequence random data adopted ﬁlter Att1 Given activation functions σi σi ρi Lipschitz σi0 0 weight matrices A A1 A2 A L stated Section 33 probability 1 δ corresponding complexvalued neural network satisfy following 1 n ncid2 t1 E l zt yt At1 l zt yt 24 cid8Z cid8 2 8M n 2 ln2W lnnRA n cid12 M ln 2 δ 2n lz y FAz y2 denotes loss function lz y M z y Theorem 2 illustrates generalisation capability complexvalued neural networks dealing sequential data The proof sketch theorem omitted main text exists overlap Theorem 1 proof shown Appendix D In Appendix present deﬁnitions sequential Rademacher com plexity sequential covering number sequential Dudley entropy integral proposed work Rakhlin et al 19 42 Proof sketch In section provide proof Theorem 1 following lemmas The proof presented steps I An upper bound covering number obtained N Z A A Cdm cid8 Acid8 cid8 Lemma 1 II Starting single layer applying induction method upper bound covering number entire network derived This result illustrated Lemma 2 III The upper bound Rademacher complexity derived Dudley entropy integral covering number bound combining bound Theorem 3 qs In preparation proof ﬁrst state Theorem 3 crucial tool step III This theorem derives generalisation bound regression case L p loss function Rademacher complexity Let recall theorem presented Mohri et al 16 Theorem 3 16 Let l Z Y R L p loss function bounded M 0 F hypothesis set family G x y cid12 lFAx y FA F δ probability 1 δ following inequality holds cid12 E x yD lx y 1 m mcid2 i1 l xi yi 2 ˆRS G 3M log 2 δ 2m 5 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 ˆRS G denotes empirical Rademacher complexity family G Obviously bound generalisation error suﬃces derive upper bound Rademacher complexity loss function family G x y cid12 lFAx y FA F realised ﬁrst second steps In following paragraphs illustrate proof steps Step I In step obtain matrix covering set matrix products Z A Z represents data matrix passed present layer A instantiated weight matrix L2 norm Lemma 1 derived follows Lemma 1 p q r s conjugate exponents p 2 Let b cid8 positive real numbers let d m positive integers Impose constraint norm Z cid8Z cid8 p cid24 b Therefore cid23 cid22 cid5 cid4cid21 ln N Z A A Cdm cid8 Acid8qs cid8 cid8 cid82 a2b2m2r cid82 ln4dm The proof Lemma 1 based Maurey sparsiﬁcation lemma This lemma inspired cover targeting set sparsifying convex hull complexvalued matrices constructed product rescaled data j Moreover prove Theorem 1 constraints imposed A21 matrix Z 20 standard matrices ei eT q 2 s 1 instead A2 helps avoid occurrence combinatorial numbers L W outside log term upper bound 17 Step II After obtaining matrix covering upper bound Step I idea extended proof entire network covering number upper bound obtaining Lemma 2 The proof Lemma 2 relies induction Lemma 1 Lemma 2 σ1 σ2 σ3 σL ﬁxed activation functions σi ρi Lipschitz The spectral norm bound matrix Ai denoted si matrix 21 norm denoted bi 1 2 L Given Z ﬁxed data matrix Z Cnd row denotes group data points cid8 ln N F cid8 cid8 cid82 cid21 cid7 cid8 cid7 cid8 cid8Z cid82 4W 2 2 ln cid82 Lcid15 j1 j ρ2 s2 j cid14 cid25 Lcid2 cid163 cid26 23 bi si cid22 i1 cid9 cid9 cid9 cid9 A cid10 F complexvalued neural networks FA W denotes maximum d0 d1 dL A A1 A L cid8 Aicid8σ si bi FA Z T 21 family outputs generated feasible choices In general separate proof Lemma 2 parts The ﬁrst determines relationship entire network upper bound matrix covering bounds previous L layers addressed Appendix B2 Lemma 6 The second combines Lemmas 1 6 provide Lemma 2 induction technique Step III Since deriving bound covering number network N F cid8 cid8cid8 2 suﬃcient We derive upper bound empirical Rademacher complexity loss function family ˆRS G It natural connect concepts Dudley entropy integral However preparation work required satisfy condition standard Dudley entropy integral The standard Dudley entropy integral illustrates relationship N G cid8 cid8cid8 2 upper bounds N G cid8 cid8cid8 2 N F cid8 cid8cid8 2 ˆRS G Hence Lemma 3 Lemma 3 Given family F FAZ A B1 BL family G z y cid12 l FAz y FA F covering number families satisfy N F cid8 cid8cid8 2 N G cid8 cid8cid8 2 Let l FAz y cid8FAz ycid8 2 3 Moreover range loss function adopt lie 0 1 Lemma 4 calculates covering number rescaling Lemma 4 If coeﬃcient α 0 multiplied targeting set G distance constant cid8 covering number remains unchanged N G cid8 cid8cid8 2 N αG α cid8 cid8cid8 2 Here αG represents set obtained scaling α element G 6 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Using Lemmas 3 4 Rademacher complexity G bounded Dudley entropy integral We prove Theorem 1 substituting ˆRS G Theorem 3 value upper bound obtained A detailed proof provided Appendix B3 5 Results experiments In section present experimental results training complexvalued neural networks SGD datasets MNIST FashionMNIST CIFAR10 CIFAR100 Tiny ImageNet IMDB Before presenting experimental results summary upper bounds derived Theorems 1 2 Both independent identically distributed sequential data cases showed derived upper bound scales spectral complexity complexvalued neural network RA cid14 Lcid15 i1 ρi cid8 Aicid8σ cid16 cid10 cid9 cid9 cid9 A cid923 21 cid8 Aicid823 σ Lcid2 i1 32 cid14cid25 cid28 cid16 L i1 cid14cid25 cid10 cid2632 cid9 cid9 cid9 A cid923 21 cid8 Ai cid823 σ cid9 cid9 cid9 A cid923 21 cid8 Ai cid823 σ L i1 cid10 cid28 equivalent factor assume change change RA cid4cid27 cid5 L i1 ρi cid8 Aicid8σ The formula spectral norm RA consists parts Lipschitz constant neural network factor related sum quotients weight matrix norms Since norms cid4cid27 cid5 L i1 ρi cid8 Aicid8σ cid14cid25 cid28 L i1 cid10 cid9 cid9 cid9 A cid923 21 cid8 Ai cid823 σ cid16 cid2632 remains interval C1 C2 constants C1 C2 Therefore cid16 cid2632 minor Then training process dominates activation functions ρi remain unchanged Therefore use change spectral norm product simulate changing trend RA Lipschitz constant neural network This Lipschitz constants cid8 Aicid8σ L i1 cid27 51 Spectral norm weight matrix First calculation spectral norm complex weight matrix convolutional layer demonstrated Considering complexvalued kernel W X Y layer X Y realvalued kernels convolutional kernel corresponds matrix transformation 21 realvalued weight matrices kernels X Y derived denoted C D Hence complexvalued weight matrix A layer expressed C Di Then deﬁnition spectral norm complexvalued matrix A cid13 cid8 A vcid82 λmax A A cid8 Acid8σ sup cid8vcid821 cid29 λmax cid7 C T C D T D C T D C D T cid8 Here A A Hermitian matrix real eigenvalues 52 Complexvalued convolutional neural networks In experiments trained complexvalued convolutional neural networks CVCNN complexvalued multilayer perceptrons CVMLP different datasets The IMDb dataset training CVMLP The proposed complex valued neural network architectures described Appendix E2 The CVCNN architecture consists types layers convolutional maxpooling fully connected layers The layer CVMLP The convolutional maxpooling layers CVCNN analogous hidden layers CVMLPs case realvalued neural networks 22 The operations convolutional maxpooling layers expressed matrixvector multiplication shown 23 To analyze results considered convolutional maxpooling layers separately 521 Matrix multiplication interpretation convolution operations First interpret convolution process matrix multiplication The convolutional layer usually receives input matrix dimensions h1 w 1 d1 3D matrix length h1 width w 1 height d1 Further kernels deﬁned follows Assuming number output channels d2 d2 kernels written matrix dimensions h2 w 2 d1 Finally assume output matrix dimensions h3 w 3 d2 Fig 2 shows input kernel output matrices convolution process 7 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 The relationship output input matrix dimensions expressed follows w 3 w 1 w 2 2p 1s h3 h1 h2 2p 1s p denotes padding number s indicates stride number Without loss generality assume p 0 s 1 The step ﬂatten input matrix vector write d2 3D kernel matrices 2D matrix Because input matrix dimension h1 w 1 d1 ﬂattened vector x Rh1 w1d1 cid5 cid4 x 11 x1 x1 12 xd hw xd1 11 xd1 h1w 1 T Here xd In terms constructing kernel matrix ﬁrst consider convolutional matrix K hw denotes h w dth entry input matrix 0 h h1 0 w w 1 0 d d1 cid4 h1d ki ki hd formed vector hwd denotes h w dth entry ith kernel 0 h h2 0 w w 2 Rw2 ki hd 0 d d1 0 d2 The convolutional matrix K formed follows hd induced vector ki cid4 h1d ki ki Rw2 ki hw2d hw2d hd cid5 cid5 h2d ki h1d ki ki h1d ki ki 0 0 h3d h2d ki 0 h3d ki h1d ki hw 2d ki h2d 0 ki hw 2d ki h3d K hd Toeplitz matrix Rw1w21w1 The second step construct convolutional matrix K Rh1h21w1w21h1 w1 constructed considering K ki 0 0 hw 2d d induced matrix K hd K d Toeplitz block matrix hd components We write K d follows K 2d K 1d K K 3d h2d 2d K 1d K K 0 1d K K 0 3d 2d 0 0 K h2d K 3d K 0 0 h2d Therefore entire convolutional matrix K Rd2h1h21w1w21h1 w1d1 induced d2 kernels viewed block matrix taking K d components It constructed follows K K 1 1 K 2 1 K d2 1 K 1 2 K 2 2 K d2 2 K 1 d1 K 2 d1 K d2 d1 Consequently regard K weight matrix convolutional layer apply spectral norm calculate generalisation bound For maxpooling layer exists weight matrix entries 0 1 depending entry x 11 1 d d1 reserved weight matrix M reserved maxpooling process For instance xd Rd1h1 w1d1 maxpooling layer 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 M sparse matrix ih1 w 1 entry equals 1 In conclusion CVCNNs type complexvalued neural network ﬁts theoretical analysis presented Section 4 8 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Fig 1 Plots excess risk spectral norm product SN prod functions epoch The right yaxis denotes spectral norm product left yaxis denotes excess risk Fig 2 The input matrix kernel output matrix convolution process 53 Results The architectures complexvalued neural networks study described Appendix E2 The datasets MNIST FashionMNIST CIFAR10 CIFAR100 Tiny ImageNet IMDB Descriptions datasets presented Appendix E1 We trained CVCNNs SGD MNIST FashionMNIST CIFAR10 CIFAR100 Tiny ImageNet investi gate generalisation bound derived Theorem 1 CVNN trained IMDB investigate generalisation bound derived Theorem 2 sequential training data The results presented Fig 1 Fig 1 displays excess risk spectral norm product function epoch In addition performed Spearman rank order correlation tests excess risks spectral norm products MNIST FashionMNIST CIFAR10 CIFAR100 IMDB Tiny ImageNet Spearmans rankorder correlation coeﬃcients SCCs pvalues correlation spectral norm product generalisation ability statistically signiﬁcant p 00051 listed Table 1 These results strongly support theoretical ﬁndings 1 The deﬁnition statistically signiﬁcant versions p 005 p 001 This paper rigorous approach p 0005 9 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Table 1 SCC p values spectral norm product excess risk CIFAR10 SCC p CIFAR100 SCC p MNIST SCC p 099 IMDB 3703 10 228 080 4124 10 23 099 9044 10 142 FashionMNIST Tiny ImageNet SCC p SCC p SCC p 099 6118 10 194 099 3703 10 142 099 4060 10 125 6 Comparison realvalued neural networks 61 Theoretical analysis In previous studies Bartlett et al 17 proved generalisation bound realvalued neural networks RVNNs positively correlated spectral complexity In work spectral complexity deﬁned product realvalued weight matrix spectral norms deﬁned following formula Ai denotes weight matrices realvalued neural networks Mi denotes reference matrices cid16 cid9 cid923 21 cid9 cid9 A Lcid2 Lcid15 cid10 cid10 32 cid14 RA ρi cid8 Aicid8σ i1 i1 M cid8 Aicid823 σ The generalisation bound presented CVNNs similar realvalued neural networks RVNNs pre sented 17 Theorem 4 illustrates relationship generalisation bounds RVNNs spectral complexity cid7 Theorem 4 17 Let nonlinearities σ1 σL reference matrices given σi ρi Lipschitz σi0 0 Then x y x1 y1 xn yn drawn iid probability distribution Rd 1 k probability 1 δ xi yin i1 margin γ 0 network FA Rd Rk weight matrices A A1 A L satisfy M1 M L cid8 cid14 Pr arg max j F Ax j cid13 y Rγ F A O cid8Xcid82 RA γ n Rγ f n 1 cid28 1 f xi yi γ max jcid13 yi f xi j cid8 Xcid82 cid16 ln1δ n lnW cid29cid28 cid8xicid82 2 4 The main difference generalisation bounds CVNNs RVNNs lies spectral complexity spectral norm weight matrix deﬁned In theorem redeﬁne spectral norm complexvalued matrix instead reformulating complexvalued matrix constrained realvalued matrix calculate spectral norm By preserve unique structure complexvalued matrices avoid increasing computational costs Reformu lating complexvalued matrix realvalued matrix result considerable increase matrix dimensions making calculation spectral norm complicated Additionally contribution lies proving gener alisation bound positively correlated deﬁned spectral complexity inspired consider effectiveness spectral normalisation CVNNs discussed Section 7 62 Empirical analysis Empirical comparisons CVNNs RVNNs conducted forms literature For instance Trabelsi et al 11 controlled number parameters RVNNs CVNNs tradeoff width depth Nitta 9 compared learning speed CVNNs RVNNs architecture controlled number parameters However challenging impose fair constraint CVNNs RVNNs comparison abundance studies realvalued neural networks compared limited work investigating best architecture complexvalued neural networks Although study presents methods roughly compares generalisation ability CVNNs RVNNs results conclusive evidence indicate type neural network better A fair comparison performances remains open problem In study compared performance CVNNs different RVNNs The ﬁrst type RVNN number parameters CVNN second type architecture CVNN The results presented Fig 3 10 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Fig 3 Change excess risk number epochs complexvalued realvalued neural networks trained different datasets The solid line represents performance complexvalued neural networks The dashed line represents performance realvalued neural networks number parameters The dotted line represents performance realvalued neural networks architecture As shown Figure 3 diﬃcult draw conclusions comparing generalisation performance complex valued realvalued neural networks Controlling architecture generalisation ability realvalued neural networks superior complexvalued neural networks However controlling number parameters performance CVNNs better trained CIFAR10 CIFAR100 7 Applications In section demonstrate spectral regularisation algorithm practical application theory Be cause generalisation bound complexvalued neural networks correlates positively product spectral norms weight matrices natural investigate adding regularisation term spectral norm loss function decreases excess risk reallife training complexvalued neural networks We conducted experiments MNIST FashionMNIST CIFAR10 CIFAR100 IMDB TingImageNet The empirical results applying spectral regularisation algorithm decreases excess risk signiﬁcantly fully supporting idea The spectral regularisation algorithm presented Algorithm 1 The empirical results presented Fig 4 8 Conclusions In study propose generalisation bounds complexvalued neural networks iid sequential data Our analysis shows bounds scale spectral complexity includes spectral norm product weight matrices factor We provide empirical evidence support theoretical ﬁndings Our work contributes understanding generalisation ability complexvalued neural networks encourages exploration unique properties Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper 11 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Fig 4 Plots excess risk function epoch The right yaxis denotes spectral norm product left yaxis denotes excess risk The solid line denotes CVNNs spectral regularisation dashed line denotes results adding spectral regularisation Algorithm 1 SGD complexvalued spectral regularisation Input Initialised network parameter θ 0 training epoch N spectral regularisation factor λ learning rate α Output Optimised network parameter θ N t 0 N 1 Sample minibatch xi yi k l 1 L i1 training data Express lth layer weight matrix Al Cl Dl Cl Dl realvalued matrices Compute layerwise complexvalued spectral norm cid8 Alcid8σ l Dl Cl D T C T l Cl D T C T l Dl λmax cid29 cid7 cid7 l cid8 cid8 end Compute Lt 1 Update θ t1 θ t αθ t cid28 k k Lt i1 cid11 fθ t xi yi λ L i1 cid8 Ai cid8σ cid28 end Data availability Data available request Acknowledgements Mr Shiye Lei supported Australian Research Council Projects FL170100117 IH180100002 Appendix A Lipschitz properties activation function In section goal prove types activation functions widely complexvalued neural networks Lipschitz continuous The ﬁrst introduced 10 σ1z tanhRez itanhImz 12 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 This activation function applies hyperbolic tangent function real imaginary Since derivative hyperbolic tangent function upper bounded 1 σ1 1Lipschitz coordinate view real imaginary different coordinates Then cid9 cid9σ1 z1 σ1 cid10 cid8cid9 cid9 cid7 cid15 1 z p tanh Re z1 tanh cid8cid8cid11 cid7 Re cid7 cid15 1 z p cid7 Re z1 Re cid9 cid9 cid15 cid9 cid9zi z p cid8cid8 cid7 cid15 1 z cid7 p Im z1 Im cid10 tanh Im z1 tanh cid7 cid8cid8 1 p p cid15 1 z cid8cid8cid11 1 p p cid7 Im cid7 cid15 1 z The ﬁrst inequality holds hyperbolic tangent function 1Lipschitz The second type activation function C ReLU function introduced 11 σ2z ReLU Rez ReLU Imz This function operates separately real imaginary The proof process similar ﬁrst ReLU function 1Lipschitz cid8cid9 cid9 cid7 cid9 cid9σ1 z1 σ1 cid10 cid15 1 z p tanh Re z1 tanh cid8cid8cid11 cid7 Re cid7 cid15 1 z p cid7 Re z1 Re cid9 cid9 cid15 cid9 cid9zi z p cid8cid8 cid7 cid15 1 z cid7 p Im z1 Im cid10 tanh Im z1 tanh cid7 cid8cid8 1 p p cid15 1 z cid8cid8cid11 1 p p cid7 Im cid7 cid15 1 z The type activation function σ3z tanhz expiθ θ argz If write z x yi use vector notation represent real imaginary operation Reσ3z Imσ3z tanhx2 y2 tanhx2 y2 1 2 1 2 x x2 y2 y x2 y2 1 2 1 2 Notice cid3 cid3 cid3 cid3tanhx2 y2 cid3 cid3 cid3 cid3 cid3 cid3 1 1 2 1 x2 y2 1 2 Hence following inequality 1 1 cid3 cid3 cid3 cid3tanhx2 cid3 cid3 cid3 cid3 cid3tanhx2 cid3 cid3 cid3 cid3 cid3tanhx2 cid3 cid3 cid3 cid3 cid3 cid3 1 y2 1 tanhx2 1 y2 1 x2 1 1 2 x2 1 x2 1 y2 1 1 2 y2 1 1 2 y2 1 1 2 x1 y2 1 x1 y2 1 x2 y2 1 1 2 1 2 1 2 x2 1 cid3 cid3 cid3 cid3 cid3 1 2 x1 x2 tanhx2 2 y2 2 1 2 tanhx2 1 y2 1 1 2 x2 y2 2 x2 y2 1 1 2 1 2 x2 2 x2 1 tanhx2 2 y2 2 1 2 cid3 cid3 cid3 cid3 cid3 1 2 y2 1 tanhx2 1 y2 1 x2 1 1 2 1 2 x2 y2 x2 2 2 tanhx2 x2 2 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 Noted assume gx tanhx x calculation 13 y2 2 1 2 cid3 cid3 cid3 cid3 cid3 1 2 x2 2 y2 2 cid3 cid3 cid3 bounded 1 Hence gx 1Lipschitz cid3g cid15x H Chen F He S Lei et al Therefore Artiﬁcial Intelligence 322 2023 103951 1 2 cid3 cid3 cid3 cid3 cid3 x2 y2 2 2 y2 2 1 2 1 2 y2 1 tanhx2 1 y2 1 2 x2 2 x2 1 y2 1 1 2 1 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3x2 1 cid3 cid3 cid3 cid3 cid3 tanhx2 x2 2 cid3 cid3 cid3 x2 cid3 cid3 cid3 cid3 cid3 y2 2 1 2 1 2 x2 y2 2 y2 2 x2 1 y2 1 x2 1 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 y2 x2 1 2 1 2 x2 2 x1 x2 2 x2 y2 1 2 y1 y2 2 x2 y2 1 2 αx1 x2 α y1 y1 x2 x2 1 x2 1 1 1 y2 2 y2 2 x2 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3 1 2 1 2 x1 x2 y1 y2 constant α x2 α Then bound ﬁrst coordinate cid3 cid3 cid3 cid3tanhx2 cid3 1 y2 1 1 2 x1 y2 1 1 2 x2 1 tanhx2 2 y2 2 1 2 cid3 cid3 cid3 cid3 cid3 x2 y2 2 1 2 x2 2 α 1x1 x2 α y1 y1 Without loss generality second coordinate bounded αx1 x2 α 1 y1 y1 Finally cid8σ3z1 σ3z2cid8 cid7 p α 1x1 x2 α y1 y2p αx1 x2 α 1 y1 y2p cid8 1 p cid7 Mx1 x2p M y1 y2p cid8 1 p 2α 1 cid8z1 z2cid8 p M 2α 1p z1 x1 iy1 z2 x2 iy2 Hence proved σ3z tanhzexpiθ Lipschitz continuous Appendix B Proof Theorem 1 B1 Proof Lemma 1 Before proving Lemma 1 ﬁrst introduce Maureys sparsiﬁcation lemma 2417 Lemma 5 Maureys sparsiﬁcation lemma 24 In Hilbert space H equipped norm consider f H f αi cid13 0 Then positive integer k exist non negative ncid28 i1 αi gi gi H αi positive real numbers α integers k1 k2 kn ncid28 i1 ki k ncid28 i1 cid9 cid9 cid9 cid9 cid9 f α k cid9 cid9 2 cid9 cid9 cid9 ki gi ncid2 i1 α k ncid2 i1 αi cid8gicid82 α2 k cid8gicid82 max cid9 cid9 cid9 cid9 cid9 ncid2 i1 αi α ncid2 i1 ki k gi 2 cid9 cid9 cid9 cid9 cid9 ncid2 i1 gi αi k cid8gicid82 α k cid8gicid82 max 14 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Proof Deﬁne k iid random variable W 1 W 2 W k P W 1 α gi αi α Let W cid28 k i1 W k Therefore EW EW 1 f Hence Ecid8 f W cid82 1 k2 Ecid16 f W kcid2 f W icid17 i1 kcid2 i1 kcid2 i1 1 k2 E cid8 f W icid82 Ecid8 f W 1cid82 Ecid8W 1cid82 cid8 f cid82 1 k 1 k 1 k ncid2 i1 α2 k Ecid8W 1cid82 αi kα α2 cid8gicid82 cid8gicid82 max Since random variable minimal value value expectation exist cid28 k ii W sequence k numbers l1 l2 lk 1 2 nk W α gli W cid8W f cid82 α2 k cid8gicid82 max To ﬁnish proof assign integer ki mentioned lemma ki kcid2 j1 cid2 1gl j As Bartlett et al 17 indicated Maurey sparsiﬁcation lemma discussed L1 norm case Zhang 20 generalized lemma create bounds nonL1 norm cases applicable proof Lemma 1 Proof Lemma 1 Given data matrix Z Cnd rescaling column matrix Z matrix Y Cnd cid9 cid9 cid9 cid9Z j Y j Z j Set N 4dm k a2b2m2rcid82 V 1 V 2 V N cid21 σ Y eieT j cid21 σ Y cieT j α am1rcid8 Xcid8p To construct appropriate convex hull deﬁne cid22 σ 1 1 1 2 d j 2 m cid22 σ 1 1 1 2 d j 2 m cid6 cid6 α k α k C Ncid2 i1 kcid2 ki V ki 0 Ncid2 i1 ki k V lm l1 lm Nk m1 ki cid5 cid28 k m1 1lmi Here ei deﬁnes ddimensional standard vector e j deﬁnes mdimensional standard vector ci deﬁnes ddimensional vector ith entry equals 1 entries equal 0 15 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Because way V deﬁned p 2 max cid8V icid8 2 max cid8Y eicid8 2 cid8Y cicid8 2 max cid8Y eicid8 2 max cid8 Xeicid8 cid8 Xeicid8 2 p 1 The ﬁrst equality deﬁnition complexvalued vector norms second equality holds Next suﬃces prove C cover monotonicity matrix norm terms p cid9 cid9 cid9Z A α k cid8 k1 kN N i1 ki V cid9 cid9 cid9 cid28 2 Deﬁne M Rdm element row j equals cid9 cid9 Z j cid9 cid9 p 0 Z A A Cdm cid8 Acid8qs 1 To prove desire bound Z A Y M cid19 A cid19 represents Hadamard product cid25cid9 cid4cid9 cid9 cid9 Z1 cid9 cid8Mcid8pr cid9 cid9 Zd cid9 cid9 cid9 cid9 cid9 cid9 cid9 cid9 p p cid5cid9 cid9 cid9 p cid9 cid9 cid9 cid4cid9 cid9 Z1 cid9 cid9 m1r cid9 cid9 Zd cid9 cid9 p cid5cid9 cid9 cid9 p p 1p Z p j dcid2 ncid2 m1r i1 j1 m1rcid8Z cid8p cid9 cid9 cid9 cid9 cid9 cid4cid9 cid9 Z1 p cid9 cid9 Zd 1p cid5cid9 cid9 cid9 cid9 cid9 p cid26cid9 cid9 cid9 cid9 r p m1r cid9 cid9 Z j cid9 cid9p p dcid2 j1 Hence denote S M cid19 A cid8Scid81 cid16M Acid17 cid8Mcid8prcid8 Acid8qs m1rcid8Z cid8pa α We Z A lies convex hull related V 1 V 2 V N Z A Y M dcid2 mcid2 cid4 cid7 cid8 Re Mi j eie cid10 j Im cid7 cid8 Mi j cieT j cid5 Y i1 j1 dcid2 mcid2 cid8Mcid81 cid14 cid7 Re Mi j cid8Mcid81 cid8 cid4 cid5 Y eie cid10 j cid8 cid4 cid7 Im Mi j cid8Mcid81 Y cie cid10 j cid16 cid5 i1 j1 α conv V 1 V N conv V 1 V N denotes convex hull formed V 1 V 2 V N Finally Lemma 5 exist nonnegative integers k1 k2 kN cid9 cid9 cid9 cid9 cid9 Z A α k Ncid2 i1 ki V cid9 cid9 cid9 cid9 cid9 2 2 cid9 cid9 cid9 cid9 cid9Y M α k Ncid2 i1 ki V cid9 cid9 cid9 cid9 cid9 2 2 cid8V icid8 2 α2 max k a2m2rcid8Z cid82 p k cid82 Hence C covering desire set Since cardinality set C equals Nk target inequality cid4cid21 ln N Z A A Cdm cid8 Acid8qs cid22 cid23 cid5 cid8 cid8 cid82 cid24 a2b2m2r cid82 ln4dm cid2 16 H Chen F He S Lei et al B2 Proof Lemma 2 Artiﬁcial Intelligence 322 2023 103951 cid9 cid9 cid9 cid9 Ai Z Ai As stated section lemma shall proved mathematical induction The basic idea follows Denotes Z data set passing i1th layer ith layer Z 0 Z T According Lemma 1 assume ﬁxed speciﬁc layer exists sequence covering matrices A0 A1 Ai1 i1 previous layers covering matrix Ai cid8 cid8 0 As consequence input data i1th layer shall Z i1 σi1 Ai Z Z i1 σi1Ai Z cid9 cid9 cid9 cid9 Ai Z Ai cid9 cid9 Z 2 2 cid7cid9 cid9 cid9 Ai Z Ai cid9 Z 2 cid9 cid9 cid9 cid9 Z Z 2 ρi ρi ρi cid8 Aicid8σ cid9 cid9 Z i1 Z i1 cid9 cid9 Ai ρicid8i Z Ai Z Z cid9 cid9 cid8 2 2 Since ﬁrst term righthand depends inductive hypothesis intuitively covering number upper bound depends product spectral norms covering matrices The detailed proof illustrated follows spaces equipped cid8cid8 ﬁrst layers input Z V 1 constraint cid8 Z cid8 We ﬁrst deﬁne sequences vector space V 1 V 2 V L W 2 W 3 W L1 The ﬁrst sequence vector W For layers input matrix Z V V second sequences equipped cid8cid8 B Moreover assumptions Ai viewed linear operator V W i1 norm linear V operation deﬁned cid8 Aicid8 V W sup Z V 1 cid8 Ai Z cid8 W ci σi treated mapping W V ρi lipschitz property means cid9 cid9σiz σiz cid15 cid9 cid9 ρi V cid9 cid9z z cid9 cid9 cid15 W With preparations claim following lemma based similar lemma raised Bartlett et al 17 Lemma 6 17 Assume sequence positive numbers cid81 cid8L Lipschitz nonlinear mappings σ1 σL σi ρi Lipschitz linear operator norm bounds c1 cL described given Suppose sequence matrices A A1 A L lies B1 BL Bi classes satisfying property Ai Bi cid8 Aicid8 ci l j1 ρlcl complexvalued neural network images F Let data Z given cid8Z cid8 FAZ A B1 BL cid8Z cid8 B following covering number bound B Then deﬁne τ jL cid8 jρ j V W cid28 cid27 V L V N F τ cid8cid8 V Lcid15 i1 cid7 sup A1 Ai1 B ji A j j cid8 cid4cid21 N cid7 Ai F A1 Ai1 cid8Z Ai Bi cid8i cid8 cid8W cid22 cid5 Proof The lemma proved Mathematical induction A sequence covering set F1 F2 FL constructed Fi covers W Base case When i1 F1 constructed according Lemma 1 F1 N A1 Z A1 B1 cid81 cid8 cid8W N1 Inductive Hypothesis Assume ﬁnd cid8n covering Fn set 0 AnFA1An1 Z An Bn 1 Fn ncid15 l1 Nl Induction Step For element F Fn construct cid8n1cover Gn1F An1σnF An1 Bn1 Since covers proper meaning F An1 F A1 AnZ matrices A1 An B1 Bn follows cid70 N An1 F A1 An Z An1 Bn1 1 cid8 cid8n1 cid8 cid8W Gn1F sup A1 An B cid7 cid8 ji A j Nn1 j 17 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Lastly form cover 2 Fn1 Gn1F F Fn cardinality satisﬁes Fn1 Fn Nn1 n1cid15 l1 Nl Deﬁne H σLF F FL Its trivial cardinality H FL Then suﬃces H covering F If ﬁx A1 A L satisfying constraints recursively denote F 1 A1 Z W 2 G σi F V i1 F i1 Ai1G W i2 In words need prove exist G L H cid9 cid9G L G L cid9 cid9 V τ cid9 cid9 Ai G i1 F cid9 cid9 W cid8i set G σi cid8 cid7 F Base case Set G 0 Z Inductive hypothesis Choose F Fi Induction Step cid9 cid9G i1 G i1 cid9 cid9 V ρi1 ρi1 ρi1 cid8 Ai1cid8 cid9 cid9 cid9F i1 F i1 cid9 W cid9 cid9 cid9F i1 Ai1 cid9 G cid9 cid9G G icid15 V W cid2 W ρi1ci1 cid8 jρ j ρlcl l j1 ji i1cid15 cid2 cid8 jρ j ρlcl G F i1 cid9 cid9 W ρi1 cid9 cid9 cid9 cid9 Ai1 ρi1cid8i1 V ρi1cid8i1 ji1 l j1 γ Hence proved cid2 To prove Lemma 2 key idea apply result Lemma 1 Lemma 6 Proof Lemma 2 To begin assume setting However prove Lemma 2 cid8cid8 operator norm set spectral norm cid8 Aicid8 deﬁned 2 cid8 Aicid8σ Also sequence number cid81 cid82 cid8L cid8cid8 cid8cid8 V W W V cid8i cid27 αicid8 ji ρ j s j ρi αi 1 α cid26 23 cid25 bi si α cid25 Lcid2 j1 cid26 23 b j s j By setting ﬁnd γ deﬁned Lemma 6 satisﬁes cid2 Lcid15 cid8 jρ j τ ρlsl jL l j1 cid2 jL α jcid8 cid8 Then cid7 FS cid8 cid8 cid82 cid8 ln N Lcid2 i1 Lcid2 i1 cid4cid21 ln N cid7 Ai F A1 Ai1 cid5 cid4 cid8 cid10 Z cid22 Ai Bi cid8i cid8 cid82 cid7 sup A1 Ai1 B ji A j j cid8 cid7 sup A1 Ai1 B ji A j j cid8 cid253 ln N cid7 F A1 Ai1 cid5cid10 cid4 cid8 cid10 Z cid10 Ai cid9 cid9 cid9 A cid9 cid9 cid9 cid10 21 18 cid5 4 cid26 bi cid8i cid8 cid82 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Lcid2 i1 cid7 sup A1 Ai1 B ji A j j cid8 cid9 cid9 cid9F cid7 b2 A1 Ai1 cid82 cid8cid10 cid7 cid8 cid10 Z cid9 cid9 2 cid9 2 cid4 cid5 ln 4W 2 The ﬁrst equality holds use L2 norms Hence covering number matrix transpose cid9 cid9 cid9F To simplify formula upper bound cid8cid10 cid9 cid9 cid9 cid10 Z cid7 2 cid8 cid7 A1 Ai1 2 cid9 cid9 cid9 cid9F cid7 A1 Ai1 cid5cid10 cid4 cid8 cid10 Z cid9 cid9 cid9F cid7 cid9 cid9 cid9 cid9 2 cid4 cid8 cid10 Z cid5cid9 cid9 cid9 2 cid4 A1 Ai1 cid4 cid5 cid10 Z σi10cid82 Ai1 F cid8σi1 cid9 cid9 cid9 Ai1 F ρi1 cid7 A1 Ai2 cid8 cid4 cid7 cid8 A1 Ai2 cid9 cid9 cid9F cid7 A1 Ai2 cid5 cid10 X cid4 cid8 cid9 cid9 0 cid9 2 cid5cid9 cid9 cid9 cid10 Z 2 ρi1 cid8 Ai1cid8 σ Inductively cid9 cid9 cid9 cid9F cid7 max j A1 Ai1 cid5cid10 cid4 cid8 cid10 Z e j cid9 cid9 cid9 cid9 2 cid8Z cid82 cid9 cid9 cid9 cid9 A j ρ j σ i1cid15 j1 Finally obtain cid7 ln N FS cid8 cid8 cid82 Lcid2 cid8 cid27 b2 cid8Z cid82 2 ji ρ2 j cid82 cid9 cid9 A j cid9 cid92 σ cid4 cid5 ln 4W 2 cid8 cid7 sup A1 Ai1 B ji A j j cid27 b2 B2 i1 Lcid2 i1 cid7 cid7 B2 ln B2 ln cid4 cid5 ln 4W 2 j s2 j ji ρ2 cid82 cid8 cid27 4W 2 L j1 ρ2 j s2 j cid82 cid8 cid27 4W 2 cid82 L j1 ρ2 j s2 j Lcid2 i1 cid4 b2 s2 α2 cid5 α3 cid2 B3 Proof Theorem 1 As stated section main theorem prove Theorem 1 Dudley Entropy Integral The standard Dudley Entropy Integral introduces method obtain Rademacher complexity bound covering number 16 Theorem 5 16 Let F realvalued function class taking values 0 1 assume 0 F Then cid8 cid7 R FS inf α0 4α n 12 n n6 cid29 α cid7 log N FS ε cid8 cid82 cid8 dε Proof 17 Let N N arbitrary let εi N cid7 FS εi cid8 cid82 cid8 n2 i1 N For let V denote cover achieving f F v V cid16 12 f xt vt2 εi cid14 ncid2 t1 V N cid7 FS εi cid8 cid82 cid8 For ﬁxed f F let v f denote nearest element V Then 19 H Chen F He S Lei et al cid14 ncid2 cid16 Artiﬁcial Intelligence 322 2023 103951 E sup f F cid14 εi f xt t1 ncid2 E E E cid14 cid14 sup f F sup f F sup f F t1 ncid2 t1 ncid2 t1 cid4 cid8t f xt v N t cid5 f cid16 cid5 f cid4 cid8t f xt v N t cid16 cid8t v 1 t f N1cid2 ncid2 cid4 cid8t cid14 i1 t1 N1cid2 E sup f F i1 v t f v i1 t ncid2 t1 cid4 cid8t cid5 f ncid2 t1 v t f v i1 t cid16 cid8t v 1 t f cid16 cid5 f For term observe suﬃces V 1 0 implies ncid2 cid2 f F t1 E sup cid8 cid8t v 1 t f 0 The ﬁrst term handled CauchySchwarz follows ncid2 cid2 cid4 cid8t f xt v N t cid5 f f F cid8t2 t1 8 9 9 sup f F ncid2 cid7 t1 f xt v N t cid8 f 2 E sup cid8 8 9 9 E ncid2 t1 nεN Last care terms form ncid2 cid4 cid8t t1 E sup cid8 v t f v i1 t cid5 f 0 1 For let W ncid2 cid2 f F t1 E sup cid8 v f v i1 f f F E sup wW f v i1 cid5 f v t cid4 t cid8t cid8t wt ncid2 t1 Then W V V i1 V i12 furthermore 8 9 9 sup wW ncid2 t1 w 2 t sup f F cid9 cid9 cid9 cid9 cid9v f v i1 f cid9 2 cid9 cid9 cid9 cid9 sup cid9v f f x1 f xn cid9 f F cid9 cid9 cid9 cid9 cid9 f x1 f xn v i1 f cid9 2 2 sup f F εi εi1 3εi1 With observation standard Massart ﬁnite class lemma 16 implies ncid2 sup wW E cid8 8 9 9 2 sup wW cid8t wt t1 ncid2 t1 wt2 log W 3 cid13 2 log W iεi1 6 cid13 log V i1εi1 20 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Collecting terms establishes Ecid8 sup f F ncid2 t1 cid8t f xt εN N1cid2 n 6 cid29 εi1 log N cid7 FS εi1 cid8 cid82 cid8 i1 Ncid2 εN n 12 cid29 εi εi1 log N cid7 FS εi cid8 cid82 cid8 i1 n6 cid29 n 12 εN εN1 cid7 log N FS ε cid8 cid82 cid8 dε Finally select α 0 N largest integer εN1 α Then εN 4εN2 4α n6 cid29 n 12 εN cid7 log N FS ε cid8 cid82 cid8 dε 4α n 12 εN1 n6 cid29 log N α cid7 FS ε cid8 cid82 cid8 dε cid2 However worth noticing N F cid8 cid8cid8 2 directly Theorem 3 obtain upper bound ˆRS G Hence raise Lemma 3 Lemma 4 applicable We shall ﬁrst prove lemmas Proof Lemma 3 Consider H F cover family F satisﬁes cardinality H equals covering number F Then FA F corresponding h H cid8FAZ hZ cid8 2 cid8 Then consider cid8FAZ Y cid8 G 2 cid8FAZ Y cid8 2 cid8hZ Y cid8 2 cid8FAZ Y hZ Y cid8 2 cid8FAZ hZ cid8 cid8 2 Therefore trivial H cid8hZ Y cid8 Hence covering number G covering number F cid2 h H cover G cardinality H equals H 2 Proof Lemma 4 Consider H G cover family G satisﬁes cardinality H equals covering number G For g G exist h H cid8α g αhcid8 2 α cid8g hcid8 2 Therefore αH cover αG αcid8 Vice versa αH cover αG H cover G Hence Lemma 4 proved cid2 After preparations proof Theorem 1 given follows cid21 Proof Theorem 1 Consider family F FA z A A1 A L cid8 Aicid8σ si G z y cid12 l F Az y F A F As consequence Lemma 3 cid7 N FS cid8 cid8 cid82 cid8 N cid7 GS cid8 cid8 cid82 cid8 cid9 cid9 A cid22 cid9 cid9 bi 21 family cid10 loss function set lFAz y cid8FAz ycid8 2 Since standard Dudley Entropy Integral requires value loss function located interval 0 1 assumption lFAz y M holds given data set rescale loss function 1 M 21 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Deﬁne G 0 z y cid12 1 N G Mcid8 cid8 cid82 N M l FAz y FA F G cid8 cid8 cid82 cid7 cid8 1 Lemma 4 indicates Therefore cid7 N ln N GS cid8 cid8 cid82 cid7 GS cid8 cid8 cid82 cid8 cid7 cid8Z cid82 4W 2 2 ln M2cid82 cid8 cid7 N cid8 FS Mcid8 cid8 cid82 cid7 cid8 cid8 ln N Lcid15 j1 j ρ2 s2 j FS Mcid8 cid8 cid82 cid26 cid14 cid25 Lcid2 23 cid163 bi si cid5 cid25 i1 L j1 s2 j ρ2 j If denote R cid8Z cid82 2 ln As stated Theorem 3 cid8 cid4cid27 cid7 4W 2 ˆRS G E σ sup g G E σ sup gG 1 n 1 n ncid2 i1 ncid2 i1 σi gzi σi 1 M gzi cid28 cid26 3 23 cid4 cid5 L i1 bi si ln N cid7 GS cid8 cid8 cid82 cid8 R M2cid82 cid7 ln N GS ε cid8 cid82 cid8 dε 1 M ˆRS G cid14 inf α0 inf α0 inf α0 4α n 4α n 4α n 12 n n6 cid29 α n6 12 n α 12 R Mn ln dε R M2cid82 cid16 n α To upper bound neater simple choice α 1 n ˆRS G 4M n32 18 cid8 Z cid8 2 2 ln2W ln nRA n Plugging upper bound Theorem 2 desired result obtained cid2 Appendix C PAC learnability complexvalued neural networks In section desire present proof shows complexvalued neural networks PAClearnable We denote f S empirical error minimizer f S arg f F l f zi yi Similarly f min 1 n expected error ncid28 i1 l f zi yi R f R f respectively represents expected error empirical minimizer f arg f F error min E 1 n ncid28 i1 The concept PAClearnable deﬁned follows Deﬁnition 1 PAClearnable Let F hypothesis set A PAClearnable algorithm exists polynomial function poly cid8 0 δ 0 distributions D Z following holds sample size m poly1cid8 1δ n size c P SDm R f S R f cid8 1 δ Here f f S deﬁned 22 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Corollary 1 Deﬁne loss function lFAz y FAz y2 upperbounded constant M For complexvalued neural network FAz σL A LσL1 A L1 σ1 A1z activation functions σi ρi Lipschitz PAClearnable Proof It suﬃces prove R f S R f cid8 generalization upper bound high probability Since R f S R f R f S R f S R f S R f R f S R f S R f R f inequality holds f S empirical error minimizer R f S R f cid3 cid3 cid3R f S R f S R f R f cid3 cid3 cid3 cid3 cid3 cid3 cid3 cid3R f S R f S cid3R f R f cid3 cid3 cid3 cid3R f R f 2 sup f F Hence P R f S R f cid8 P cid14 cid3 cid3 cid3 cid8 cid3R f R f 2 sup f F cid16 As Theorem 1 f F P cid3 cid3 cid3 8M cid3R f R f n 3 2 1 δ 36 Z 2 2ln2W lnnRA n cid12 ln 2 δ 2n 3M Notice statements equivalent cid3 cid3 cid3 cid8 cid3R f R f 2 cid3 cid3 cid3 cid8 cid3R f R f 2 f F sup f F Hence claim 36 Z 2 cid8 2 8M n 3 2 2ln2W lnnRA n cid12 ln 2 δ 2n 3M cid14 P sup f F cid3 cid3 cid3 cid8 cid3R f R f 2 cid16 1 δ P R f S R f cid8 1 δ Hence conclusion n 8 cid83 8M 36 cid8Z cid8 2 cid13 2 ln 2W RA 3M cid12 3 ln 2 δ 2 P R f S R f cid8 1 δ Therefore PAClearnability complexvalued neural networks proved cid2 Appendix D Generalization sequential data In section aim proving Theorem 2 Theorem 2 shows extension generalization sequential data case Therefore sequential analogues complexities 19 presented section complete proof 23 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 D1 Sequential Rademacher complexity In case classical complexity measure use expectation supremum Rademacher process deﬁne Rademacher complexity In sequential Rademacher case intuition similar Rakhlin et al 19 illustrated binary tree process analogue Rademacher process coincides Rademacher process iid assumption behaves differently general The notion tree deﬁned following A Z valued tree z depth n rooted complete binary tree nodes labelled elements Z We identify tree z sequence z1 zn labelling functions zi 1i1 cid12 Z provide labels node Here z1 Z label root tree zi 1 label node obtained following path length 1 root 1 indicating right 1 indicating left A path length n given sequence cid8 cid81 cid8n 1n For brevity shall write ztcid8 understood zt depends preﬁx cid81 cid8t1 cid8 Given tree z function f Z cid12 R deﬁne composition f z realvalued tree given labelling functions f z1 f zn 19 Therefore deﬁnition sequential Rademacher complexity stated Deﬁnition 2 Deﬁnition 2 19 For Z valued tree z depth n sequential Rademacher complexity function class Gsq zt yt cid12 l FAz y FA F deﬁned follows 1 n sup f F ncid2 t1 cid8tl f ztcid8 yt Rsq n Gsq z E Rsq n Gsq sup z Rsq n Gsq z Here cid8t Rademacher variables taking value 1 1 equal probability D2 Sequential Rademacher complexity generalization bound When investigating relation generalization error Rademacher complexity following theo rem Theorem 6 Given function class F sample S z1 y1 z2 y2 zn yn zi yi iid data points 1 n ncid2 i1 E sup f F f zi yi E f E f zi yi E f 2RF 1 n sup f F ncid2 i1 RF ERS F For sequential Rademacher complexity Rakhlin et al 19 proved similar theorem Theorem 7 Given function class F sample S z1 y1 z2 y2 zn yn z1 y1 sequential data points following inequality holds E 1 n sup f F ncid2 t1 E f zt yt At1 f zt yt 2Rsq n F Rsq n F denotes sequential Rademacher complexity If function class F bounded f F cid8 f cid8 M generalization error 1 n sharply concentrated expectation leads Corollary 2 cid8 f zt yt cid28 n t1 cid7 E f zt yt At1 Corollary 2 Assume target function class f F cid8 f cid8 M Given sample S z1 y1 z2 y2 zn yn z1 y1 sequential data points probability 1 δ following inequality holds cid12 1 n ncid2 t1 E f zt yt At1 f zt yt 2Rsq n F M log 2 δ 2n 24 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Proof This corollary consequence McDiarmids Inequality Theorem 6 By McDiarmids Inequality cid8 f cid8 M P cid5F Ecid5F t 2 exp cid7 E f zt yt At1 f zt yt cid28 n t1 cid5F 1 n plexity upper bound cid2 cid4 2nt2M2 cid5 cid8 Then Theorem 6 sequential Rademacher com As consequence Corollary 1 necessary bound sequential Rademacher complexity want prove generalization upper bound This leads introduction sequential Dudley Entropy Integral D3 Sequential Dudley entropy integral Before stating sequential Dudley Entropy Integral ﬁrst present deﬁnition sequential covering number 19 Deﬁnition 3 Sequential covering number A set C sequential αcover respect cid11p norm F RZ depth n tree z f F cid8 1n c C st ctcid8 f ztcid8p α cid14 1 n ncid2 t1 cid16 1p The sequential covering number function class F given tree z deﬁned N sq p α F z min p α F n supz N sq 0 1 C C αcover wrt cid11pnorm F z p α F z N sq Rakhlin et al 19 provides sequential version Dudley Entropy Integral follows Theorem 8 Sequential Dudley Entropy Integral For p 2 sequential Rademacher complexity function class F 1 1Z Z valued tree depth n satisﬁes 16 cid29 Rsq n F inf α 4α 12 n log N sq 2 δ F ndδ α Notice classical αcover F regard l2 norm denote V given data matrix Z FA F exist v V cid8FAZ vZ cid8 2 α Since given set sequential data cid8FAZ vZ cid8 2 cid12 ncid28 t1 FAzt vzt 2 Hence cid8FAZ vZ cid8 2 α 8 9 9 1 n ncid2 t1 FAzt vzt2 α n Hence consequence Lemma 2 ln N sq 2 α n Gsq n ln N G α cid8cid8 2 cid8Z cid8 2 ln 4W 2 α2 sq A R Gsq denotes loss function family sequential data set R case sequential data set G denotes loss function family iid data set sq A denotes spectral complexity CVNNs Hence cid7 ln N sq 2 α Gsq n cid8 cid8Z cid82 ln 4W 2 nα2 sq A R 25 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 Supplementary Table E2 Detailed model architectures different datasets MNISTFashionMNISTCIFAR10CIFAR100 5 5 10 maxpool 2 2 5 5 20 maxpool 2 2 fc500 Tiny ImageNet 5 5 10 maxpool 2 2 5 5 20 2 maxpool 2 2 fc500 IMDB fc500 fc200 abs fc10100 softmax abs fc200 softmax abs fc2 softmax D4 Proof Theorem 2 As consequence previous Sections D1D3 cid7 ln N sq 2 cid8 Gsq n cid8 cid8Z cid82 ln 4W 2 ncid82 sq A R cid8 R Therefore Lemma 4 sequential Dudley Entropy Integral derive following bound sequential Rademacher complexity n Gsq 4M Rsq n 12 ln n RA Mn M denotes upper bound loss function After plugging inequality Corollary 2 desired bound stated Theorem 2 Appendix E Additional experiments details The section provides additional details experiments The code available httpsgithub com LeavesLei cvnn _generalization E1 Datasets Our experiments conducted datasets MNIST 25 FashionMNIST 26 CIFAR10 CIFAR100 27 IMDB 28 Tiny ImageNet 29 The details datasets shown follows MNIST consists 60 000 training images 10 000 test images 10 different classes It downloaded httpyann lecun com exdb mnist FashionMNIST consists 60 000 training images 10 000 test images 10 different classes It loaded httpsgithub com zalandoresearch fashion mnist CIFAR10 consists 50 000 training images 10 000 test images 10 different classes CIFAR100 data CIFAR10 images CIFAR100 belong 100 classes CIFAR10 CIFAR100 downloaded httpswwwcs toronto edu kriz cifarhtml IMDB movie reviews sentiment classiﬁcation dataset training test sets consists 25 000 movie reviews 2 different classes It downloaded httpai stanford edu amaas data sentiment Tiny ImageNet consists 100 000 training images 10 000 test images 200 different classes It loaded httpcs231n stanford edu tinyimagenet 200 zip For image datasets MNIST FashionMNIST CIFAR10 CIFAR100 Tiny ImageNet normalize pixel images datasets range 0 1 feeding neural network For IMDB dataset perform data preprocessing following httpsgithub com manavgakhar imdbsentiment blob master IMdB _sentiment _ analysis _project ipynb E2 Model architectures We employ Python package complexPyTorch 30 implement CVNNs include complexvalue CNNs complexvalue MLPs The detailed architectures CVNNs presented Supplementary Table E2 parameters network architectures complex values layer In Supplementary Table E2 5 5 10 denotes convolutional layer 5 5 kernel size 10 output channels The strides convolutional layers set 1 fc500 denotes fullyconnected layer output features 26 H Chen F He S Lei et al Artiﬁcial Intelligence 322 2023 103951 500 All convolutional layers fullyconnected layers followed ReLU layer layer abs absolute layer computes absolute value element input convert complex values real values E3 Implementation details This section provides additional implementation details experiments Model training We employ SGD optimize models momentum 09 Training strategy MNIST FashionMNIST Every model trained SGD 100 epochs batch size set 1024 learning rate ﬁxed 001 Training strategy CIFAR10 CIFAR100 Models trained SGD 100 epochs batch size set 128 learning rate ﬁxed 001 Training strategy IMDB Models trained SGD 100 epochs batch size set 512 The learning rate initialized 001 decayed 02 40 epoch Training strategy Tiny ImageNet Models trained SGD 100 epochs batch size set 128 The learning rate initialized 001 decayed 02 40 epoch References 1 SL Goh DP Mandic Nonlinear adaptive prediction complexvalued signals complexvalued prnn IEEE Trans Signal Process 53 2005 18271836 2 A Hirose R Eckmiller Behavior control coherenttype neural networks carrierfrequency modulation IEEE Trans Neural Netw 7 1996 3 H Sawada R Mukai S Araki S Makino Polar coordinate based nonlinear function frequencydomain blind source separation IEICE Trans Fundam 4 EK Cole JY Cheng JM Pauly SS Vasanawala Analysis deep complexvalued convolutional neural networks mri reconstruction arXiv preprint 5 A Hirose S Yoshida Generalization characteristics complexvalued feedforward neural networks relation signal coherence IEEE Trans Neural 10321034 Electron Commun Comput Sci 86 2003 590596 arXiv2004 01738 2020 Netw Learn Syst 23 2012 541551 6 A Hirose ComplexValued Neural Networks vol 400 Springer Science Business Media 2012 7 T Nitta An extension backpropagation algorithm complex numbers Neural Netw 10 1997 13911415 8 T Nitta Orthogonality decision boundaries complexvalued neural networks Neural Comput 16 2004 7397 9 T Nitta On inherent property decision boundary complexvalued neural networks Neurocomputing 50 2003 291303 10 T Nitta Redundancy parameters complexvalued neural network Neurocomputing 49 2002 423428 11 C Trabelsi O Bilaniuk Y Zhang D Serdyuk S Subramanian JF Santos S Mehri N Rostamzadeh Y Bengio CJ Pal Deep complex networks arXiv 12 DP Reichert T Serre Neuronal synchrony complexvalued deep networks arXiv preprint arXiv1312 6115 2013 13 I Danihelka G Wayne B Uria N Kalchbrenner A Graves Associative long shortterm memory International Conference Machine Learning preprint arXiv1705 09792 2017 PMLR 2016 pp 19861994 14 M Arjovsky A Shah Y Bengio Unitary evolution recurrent neural networks International Conference Machine Learning PMLR 2016 pp 11201128 15 S Wisdom T Powers J Hershey J Le Roux L Atlas Fullcapacity unitary recurrent neural networks Adv Neural Inf Process Syst 29 2016 48804888 16 M Mohri A Rostamizadeh A Talwalkar Foundations Machine Learning MIT Press 2018 17 P Bartlett DJ Foster M Telgarsky Spectrallynormalized margin bounds neural networks arXiv preprint arXiv1706 08498 2017 18 N Guberman On complex valued convolutional neural networks arXiv preprint arXiv1602 09046 2016 19 A Rakhlin K Sridharan A Tewari Sequential complexities uniform martingale laws large numbers Probab Theory Relat Fields 161 2015 111153 20 T Zhang Statistical analysis multicategory large margin classiﬁcation methods J Mach Learn Res 5 2004 12251251 21 PC Guo A Frobenius norm regularization method convolutional kernels avoid unstable gradient problem arXiv preprint arXiv190711235 2019 2017 22 A Botalb M Moinuddin U AlSaggaf SS Ali Contrasting convolutional neural network cnn multilayer perceptron mlp big data analysis 2018 International Conference Intelligent Advanced System ICIAS IEEE 2018 pp 15 23 YH Geum AK Rathie H Kim Matrix expression convolution generalized continuous form Symmetry 12 2020 1791 24 G Pisier Remarques sur résultat non publié B Maurey Séminaire Analyse Fonctionnelle 1981 pp 112 25 Y LeCun L Bottou Y Bengio P Haffner Gradientbased learning applied document recognition Proc IEEE 86 1998 22782324 26 H Xiao K Rasul R Vollgraf Fashionmnist novel image dataset benchmarking machine learning algorithms arXiv preprint arXiv1708 07747 27 A Krizhevsky G Hinton Learning multiple layers features tiny images Technical Report Citeseer 2009 28 AL Maas RE Daly PT Pham D Huang AY Ng C Potts Learning word vectors sentiment analysis Proceedings 49th Annual Meeting Association Computational Linguistics Human Language Technologies Association Computational Linguistics Portland Oregon USA 2011 pp 142150 httpwwwaclweb org anthology P11 1015 29 Y Le X Yang Tiny imagenet visual recognition challenge CS 231N 7 2015 7 30 MW Matthès Y Bromberg J Rosny SM Popoff Learning avoiding disorder multimode ﬁbers Phys Rev X 11 2021 021060 https doi org 10 1103 PhysRevX 11021060 27