Artiﬁcial Intelligence 186 2012 123156 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Algorithms strategyproof classiﬁcation Reshef Meir Ariel D Procaccia b Jeffrey S Rosenschein a1 School Engineering Computer Science Hebrew University Jerusalem 91904 Israel b Computer Science Department Carnegie Mellon University 5000 Forbes Pittsburgh PA 15213 United States r t c l e n f o b s t r c t Article history Received 25 September 2011 Received revised form 12 March 2012 Accepted 26 March 2012 Available online 27 March 2012 Keywords Mechanism design Classiﬁcation Game theory Approximation The strategyproof classiﬁcation problem deals setting decision maker classify set input points binary labels minimizing expected error The labels input points reported selfinterested agents lie order obtain classiﬁer closely matches labels creating bias data motivates design truthful mechanisms discourage false reports In paper strategyproof mechanisms classiﬁcation problem restricted settings classiﬁers ii agents interested shared set input points We plausible assumptions lead strong positive results In particular demonstrate variations random dictator mechanism truthful guarantee approximately optimal outcomes respect family classiﬁers Moreover results tight sense match best possible approximation ratio guaranteed truthful mechanism We mechanisms learning classiﬁers sampled data provide PACstyle generalization bounds expected error Interestingly results applied problems context ﬁelds classiﬁcation including facility location judgment aggregation 2012 Elsevier BV All rights reserved 1 Introduction Consider learning algorithm takes labeled set samples training data input outputs binary clas siﬁer The training data typically handconstructed human experts supposed reﬂect knowledge experts current domain The basic requirement algorithm guarantee output classiﬁer minimizes number classiﬁcation errors respect truth according domain experts Standard machinelearning literature studies performance algorithms given distributions concept classes linear classiﬁers sparse noisy data However reallife situations experts personal outcome algorithm assumed truthful If expert bias learned classiﬁer favor lying reported training data longer reﬂect properties domain properties real training data Optimizing classiﬁer based corrupted data result poor classiﬁer regardless guarantees supplied learning theory assumes truthfulness We consider interrelated settings The ﬁrst setting decisiontheoretic decision based data reported multiple selfinterested agents The agents concerned binary labels set input points Put Corresponding author Tel 972 2 6585188 Email addresses reshefmeirmailhujiacil R Meir arielprocscmuedu AD Procaccia jeffcshujiacil JS Rosenschein 1 Tel 972 2 6585353 00043702 matter 2012 Elsevier BV All rights reserved httpdxdoiorg101016jartint201203008 124 R Meir et al Artiﬁcial Intelligence 186 2012 123156 way agents disagree labels points input space assume underlying distribution The utility agent respect given decision given classiﬁer number points label provided classiﬁer agrees agents label The goal decision maker choose classiﬁer maximizes social welfarethe sum utilities As results setting applied problems context ﬁelds including facility location judgment aggregation The second setting learningtheoretic variation standard Supervised Classiﬁcation problem Samples drawn distribution input space labeled experts A classiﬁcation mechanism receives sampled data input outputs classiﬁer Unlike standard setting machine learning similarly ﬁrst setting experts assumed selfinterested agents lie order increase utility This setting far involved ﬁrst deals generalization partial data dataset underlying distribution However standard assumptions learning theory learning problem effectively reduces ﬁnding classiﬁer best ﬁts available data ﬁrst setting In settings decision maker mechanism learning algorithm aims ﬁnd classiﬁer classiﬁes available data possible However agents misreport labels attempt inﬂuence ﬁnal decision favor The result decision making process based biased data completely unexpected diﬃcult analyze A truthful learning mechanism eliminates bias allows decision maker select classiﬁer best ﬁts reported data having account hidden interests agents In words guarantee agents telling truth concentrate standard goal minimizing error In order obtain truthfulness need trade optimality Our goal provide mechanisms truthful approximately optimal terms social welfare 11 Restrictions domain In recent work 29 showed unrestricted domain effectively impossible design truthful mechanisms close optimal This motivates investigation restricted domains In paper consider restrictions described 111 Restricting concept class functions A seemingly simple case concept class contains functions This equivalent binary decision based data points controlled multiple possibly selﬁsh agents decision affects agents The decision maker like decision consistent possible available data However strategic setting agents misreport data attempt inﬂuence ﬁnal decision favor As motivating example consider decision Workers committee TAs Hebrew University ongoing strike Each member committee represents department announces TAs hisher department support strike oppose A ﬁnal decision based total support strike Suppose 60 economics department opposes strike However representative economics department majors game theory She knows beneﬁt majority TAs department better state everybody objects strike2 112 Restricting dataset shared inputs Our main conceptual contribution paper leads strong positive results assumption shared inputs In decisiontheoretic setting means agents share set input points disagree labels points In learningtheoretic setting shared inputs assumption implies agents interested common distribution input space differ respect labels The ﬁrst restriction described address issue shared inputs However possible classiﬁers constant identity input points location irrelevantonly labels matter Hence ﬁrst restriction fact special case footnote 17 As shared inputs assumption weaker restriction assuming functions guarantees somewhat weaker Nevertheless hold respect concept class We believe environments requirement shared inputs satisﬁed As example consider large organization trying ﬁght congestion internal email designing smart spam ﬁlter In order train managers asked review 1000 emails sent employees mailing list shared inputs classify workrelated positive label spam negative label Whereas managers likely agree classiﬁcation messages Buy Viagra Christmas Bonus employees likely Joe Sales department goes lunch break unanimously classiﬁed Moreover manager interested ﬁltering sees spam manager try compensate mistakes colleagues misreporting real opinion 2 In attempt avoid misrepresentation major decisions usually require gathering TAs use standard voting procedure However decisions taken narrower quorum R Meir et al Artiﬁcial Intelligence 186 2012 123156 125 respect cases For example manager RD department believing 90 Sales messages utterly unimportant classify spam order reduce congestion The manager Sales suspecting general opinion department exact opposite prevent emails ﬁltered The fact users understanding learning algorithm necessarily prevent trying bias Even strategy optimal contaminates data Interestingly model binary classiﬁcation shared inputs equivalent models suggested literature problems seemingly unrelated domains including judgment aggregation partition aggregation facility location voting detailed comparison Section 13 discussion One common classiﬁcationpartition problem deciding operating hours shared resource As concrete example consider building central heating buildings common Jerusalem cities Europe Every tenant certain hours wants heat home cost shared tenants The household fee tenants transfer payoffs A classiﬁer partition day week intervals Furthermore constraints ﬁnal partition example intervals 3 hours long achieve better eﬃciency 113 Realizable datasets In cases learning facilitated know perfect classiﬁer concept class classiﬁer separates positive data points dataset negative ones Such datasets called realizable It possible labels agent realizable single classiﬁer perfect agents We study realizability seen restriction dataset affects optimality proposed mechanisms context shared input 12 Overview results We wish design classiﬁcation mechanisms achieve good outcome face strategic behavior By good outcome mean output mechanism provides approximation optimal solution3 We like mechanisms strategyproof SP agents able beneﬁt lying These key requirements formalized demonstrated examples Section 2 We begin presenting mechanisms twofunction problem Section 3 The results section serve purposes First tight worstcase analysis SP mechanisms provides picture power limitations binary decisionmaking setting Second focus simple setting allows explain subtle issues important general setting We forward simple deterministic decisionmaking mechanism group strategyproof coalitions agents gain lying gives 3approximation optimal global risk words number mislabeled points 3 times minimum number Moreover deterministic strategyproof mechanism better Interestingly circumvent result designing strategyproof randomized mechanism gives 2approximation demonstrate far randomization In Section 4 turn study general case shared inputs assumption We ﬁrst SP terministic mechanisms guarantee sublinear approximation ratio We choosing dictator random provides approximation ratio 3 expectation agents weights decision mechanism values agents case randomly select dictator according weights We drive approx imation lower nontrivial selection dictator matching known lower bound 3 2 n striking results hold respect concept class In addition datasets realizable better approximation ratio 2 2 n guaranteed In section suggested mechanisms decisiontheoretic setting ex ploited attain similar approximation results learningtheoretic setting We observe learningtheoretic setting designing strategyproof mechanisms virtually impossible additional element randomness troduced sampling input space We relax strategyproof requirements instead investigate incomparable strategic assumptions agents lie gain cid2 agents use dominant strategy exists respect speciﬁc sample We assumption randomized mechanisms run directly sampled data maintaining bounded expected error Our theorems connec tion number samples expected error mechanism case spirit PAClearning algorithms 40 121 Mechanisms payments An important remark strategyproof classiﬁcation setting standard economic moneybased mechanisms VickreyClarkeGroves VCG mechanism 32 obtain good results However setting 3 Approximation algorithms frequently domains science order overcome computational barriers While largely ignore issues computational complexity optimal algorithms typically strategyproof need approximation 126 R Meir et al Artiﬁcial Intelligence 186 2012 123156 admits strategyproof mechanisms assuming money available Achieving goals resorting payments highly desirable payments legal ethical considerations Moreover Internet environments VCG style payments notoriously diﬃcult implement banking security issues Hence follow example set previous work strategyproof learning models 10 considering approximation mechanisms require payments 13 Related work This paper lies intersection areas including mechanism design judgment aggregation learning We cluster related work areas 131 Approximate mechanism design money Mechanisms deal strategic behavior agents proposed recently large range applications While certain restrictions allow design optimal SP mechanisms 39 case approximation required This observation gave rise agenda approximate mechanism design money AMDwoM Below overview SP mechanisms machine learning problems compare work These constitute facet large variety problems AMDwoM applied Approximate mechanisms payments proposed facility location matching 315 resource allocation 181933 scheduling 23 auctions 20 132 Strategyproof learning algorithms The work closely related paper Dekel et al 10 Their work focused regression learning labels real numbers interested distances mechanisms outputs labels Except signiﬁcant difference settings study goals similar theirs Dekel et al provided upper lower bounds approximation ratio achieved supervised regression mechanisms model Notably bounds resemble bounds regression setting Moreover similar intuitions apply settings results setting analytically mapped Dekel et al concentrate mechanisms payments results hold respect speciﬁc function classes assume shared inputs Theorems 41 42 10 We demand weaker assumptions generalization theorems allowing stronger results Strategyproof regression studied PerotePeña Perote 34 They suggested mechanisms compared naive learning algorithms strategic setting Unlike Dekel et al evaluated mechanisms empirically analytically respect speciﬁc assumptions strategic behavior agents Another closely related work authors results negative ﬂavor Perote PerotePeña 35 forward model unsupervised clustering agent controls single point R2 reported location A clustering mechanism aggregates locations outputs partition set centroids They agent wants close centroid weak restrictions clustering mechanism exists beneﬁcial manipulation reasonable deterministic clustering mechanisms SP 133 Judgment partition aggregation While motivation model stems binary classiﬁcation problem machine learning similar models problems judgment aggregation In particular list binary issues decided essentially equivalent dataset binary labels Similarly suggestion split ﬁnite set parts replaced labels element set Properties mechanisms judgmentpartition aggregation discussed extensively literature 1970s 423024516 A recent paper deals explicitly manipulations Dokow Holzman 14 characterizes strategyproof aggregation rules interpreted classiﬁcation mechanisms framework Our current work differs important ways literature judgment aggregation First explicitly measure quality proposed mechanisms spirit AMDwoM enables compare SP mechanisms Second study deterministic mechanisms randomized ones We believe notion approximation use randomization common practice science contribute study standard judgment aggregation settings The current paper demonstration approach 134 Facility location In facility location problem agents report location usually metric space mechanism outputs location facility close average agents SP location mechanisms topologies suggested studied 12636 provides clear overview ﬁeld Consider dataset labeled agents binary cube dimensions correspond samples dataset It hard verify classiﬁcation shared inputs equivalent facility location binary cube label vector agent corresponds directly speciﬁc vertex cube Similarly concept class R Meir et al Artiﬁcial Intelligence 186 2012 123156 127 deﬁnes allowed labellings corresponds set vertices constitutes allowed locations A classiﬁcation mechanism seeks optimal classiﬁcation optimal vertex restricted set Although main focus context binary classiﬁcation binary cube mechanisms paper directly applied facility location problems metric space An important note typically assumed set allowed locations facility coincides possible locations agents This equivalent assumption realizability classiﬁcation model We study SP mechanisms assumption 135 Voting A ﬁnite set classiﬁers thought class candidates voting scenario experts casting votes While perspective useful example 29 preferences voting typically expressive We model preference proﬁle proper input space Suppose set candidates consider binary cube section dimension sample dataset corresponds pair candidates The allowed set vertices concept class restricts outcome vertices correspond linear order candidates The assumption realizability setting interpreted rationality voters The optimal classiﬁcation mechanism minimizes average distance voters equivalent KemenyYoung voting rule 22 Therefore SP classiﬁcation mechanisms interpreted setting strategyproof approximations KemenyYoung rule It important note strategyproofness model coincide similar requirement voting typical voting setting identity winner considered 136 Other related work There signiﬁcant body work learning face noise noise random adversarial 625 Dalvi 9 Dekel Shamir 11 study settings similar learning process modeled game classiﬁer adversary However papers goal face noisy biased data provide incentives way prevents dataset manipulated ﬁrst place Further aﬁeld worth mentioning examples literature apply machine learning techniques order resolve problems economics game theory Balcan et al 4 apply SP machine learning algorithms learn bidders valuations auctions However authors achieve truthfulness learning agents directly inﬂuenced outcome relies reported data This possible setting agents affected selected classiﬁer Other papers Procaccia et al 37 suggest learning algorithms enable better preference aggregation consider strategic behavior society Finally recent work automated mechanism design techniques machine learning 78 Although designed mechanisms required truthful learning algorithm handle private information truthfulness irrelevant 2 Model notations We start introducing model notations decisiontheoretic setting additional deﬁnitions learningtheoretic setting given subsequently 21 Binary classiﬁcation multiple experts Let X input space assume ﬁnite set subset Rd A classiﬁer concept c function c X input space labels A concept class C set concepts For example class linear separators Rd set concepts deﬁned parameters Rd b R map point x Rd x b cid2 0 Denote set agents I 1 n n cid2 2 The agents interested ﬁnite set k data points X X k In paper assume X shared agents agents equally interested data point X This plausible assumption shall allows obtain surprisingly strong results Naturally points X common knowledge Each agent private type labels points X Speciﬁcally agent I holds function Y X maps point x X label Y ix attributes x Each agent I assigned weight w reﬂects relative importance normalizing weights assume cid2 iI w 1 Let cid3cid4 cid5 x Y ix S x X cid6 partial dataset agent let S cid5S1 Sncid6 denote complete dataset S said realizable wrt concept class C c C perfectly separates positive samples negative ones If S realizable I S said individually realizable Fig 1 shows example dataset shared set points X 128 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Fig 1 An instance shared inputs Here X R2 C class linear separators R2 n 3 The data points X agents identical labels types different The best classiﬁer C respect S shown arrow marks positive halfspace separator Only rightmost dataset realizable We use common 01 loss function measure error The risk4 negative utility agent I respect concept c simply relative number errors c makes dataset Formally cid7 cid7 Ric S 1 k cid2 cx cid7 y cid3 1 k x X cid2 cid3 cx cid7 Y ix cid5x ycid6S 1 A denotes indicator function boolean expression A Note S realizable mincC Ric S 0 In contrast standard learning scenarios model ground truth objective classify way satisfactory agents Thus global risk deﬁned RI c S cid7 iI w Ric S 1 k cid7 cid7 iI x X cid3 cid2 cx cid7 Y ix w 22 Mechanism properties 2 A deterministic mechanism M receives input dataset S5 outputs classiﬁer c C Note S ﬁnite ﬁnitely different ways classify data RiMS S I RI MS S welldeﬁned This longer case learningtheoretic setting need slightly modify deﬁnitions A randomized mechanism identiﬁed probability distribution pM S C We restrict attention proba bilities ﬁnite support That dataset S mechanism M returns c C probability pMc S When measuring risk interested expected number errors mechanism makes given dataset Formally cid8 cid9 Ri MS S cid10 Ric S cid12 cid11 cid11 S EpM cid7 cC pMc S Ric S 3 global risk deﬁned analogously For complete partial dataset S cid8 S best available classiﬁer respect dataset S cid8 referred empirical risk minimizer erma common term machine learning literature Formally 4 cid8 erm cid8 S cid9 cid7 cid2 cid3 cx cid7 y argmin cC cid5x ycid6Scid8 For complete dataset denote best classiﬁer c context That S risk r S simply c r S clear c S ermS argmin cC RI c S r S RI c S S The simple mechanism computes returns ermS referred ERM mechanism block letters6 If optimal classiﬁer assume ERM returns arbitrarily Similarly mech anism returns best classiﬁer respect partial dataset speciﬁc agent ermS 1 called dictator mechanism If r said perfect Note existence perfect classiﬁer C implies partial datasets 0 c realizable converse hold We measure quality outcome mechanism standard notion multiplicative approximation 4 When dataset S consists sampled data appropriate term empirical risk This distinction signiﬁcant Sections 33 43 5 We implicitly assume information weights agents contained dataset 6 Actual algorithms compute erm raise practical problems depend domain computational complexity However problems scope paper Since erm exists number data points ﬁnite algorithm computes erm ﬁnite time Deﬁnition 21 A mechanism M αapproximation mechanism dataset S holds RI MS S cid3 α r R Meir et al Artiﬁcial Intelligence 186 2012 123156 129 S Note randomized mechanisms required attain approximation expectation necessarily high probability We emphasize real labels input points private information agent report different labels ones indicated Y We denote Y X reported labels agent We denote S cid5x Y ixcid6 x X reported partial dataset agent S cid5S 1 Sncid6 reported dataset Strategyproofness implies reporting truthful types dominant strategy agents For dataset S I let Si complete dataset partial dataset agent Deﬁnition 22 A deterministic randomized mechanism M strategyproof SP dataset S I S cid8 cid9 cid8 Ri MS S cid3 Ri cid9 MS Si S 5 Our goal design mechanisms SP guarantee low worstcase approximation ratio There inherent tradeoff strategyproofness good approximation The ERM mechanism returns ermS example 1approximation mechanism SP section On hand mechanism selects agent 1 dictator returns ermS 1 clearly SP general bad approximation agents disagree agent 1 We remark randomized mechanisms distinction strategyproofness expectation Deﬁnition 22 implies universal strategyproofness The stronger deﬁnition requires agent gain lying randomization takes place Interestingly ﬁrst weaker notion strategyproofness suﬃcient lower bounds upper bounds satisfy universal strategyproofness 3 Choosing classiﬁers In section consider simple concept class containing classiﬁers For ease exposition assume positive classiﬁer c negative classiﬁer c cx cx x X Our concept class C c c thought choosing global positive decision negative decision respectively Remark 1 Although deﬁne concept class C containing speciﬁc classiﬁers results easily extend concept class size 2 provided data point x X concepts disagree Indeed dataset concepts agree improve approximation ratio hand examples data points conﬂict Thus upper lower bounds hold We start observations allow simplify model setting Note identity data point important fraction positive negative labels agent attributes dataset We think setting agent controls different set points Xi size partial dataset proportional agents weight With interpretation model simpler weight type agent completely deﬁned number positive points negative points controls Consider TA committee example introduction We count TA single data point positive supports strike representative department reports opinions TAs The weight department case proportional number workers cid2 We denote number points controlled agent mi Xi S size dataset m S iI mi This notation section instead k We denote number positive negative data iI P points P cid5x ycid6 S y Ni mi P cid5x ycid6 S y For convenience let P N iI Ni We emphasize P NiiI contains information relevant problem replace S With alternative notations private risk concept c agent Eq 1 replacing k cid2 cid2 mi The risk simpliﬁed twofunction case c c c c Ric S 1 mi P imi Nimi cx cid7 y cid7 cid13 cid3 cid2 cid5x ycid6S We update deﬁnition global risk follows RI c S cid7 iI mi m Ric S 1 m cid7 cid2 cx cid7 y cid3 cid5x ycid6S 6 7 Similarly private risk RI c S P m c Nm c Note taking w mi case Eq 2 m special 130 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Fig 2 ERM strategyproof Agent 1 changes points negative positive changing risk minimizer c c agent 1s advantage In illustration X R2 Unfortunately choose ERM mechanism simple setting agents lie order decrease subjective risk Example 31 Illustrated Fig 2 Agent 1 controls 3 examples 2 positive 1 negative Agent 2 controls 2 examples negative Since majority negative examples ERM return c agent 1 suffer subjective risk 23 On hand agent 1 reported negative example positive ERM return c subjective risk 13 agent 1 Indeed note agents utility measured respect real labels respect reported labels It easy agent gain lying controls point For instance agent positive point ERM returns c falsely reporting negative label reinforce mechanisms decision This striking contrast regression learning setting considered Dekel et al 10 deepest technical results concern singlepointperagent scenario Despite fact ERM SP like use optimal concept order evaluate concepts mechanisms From deﬁnition erm RI r cid8 c S cid9 cid3 min cid6 RI c S RI c S 31 Deterministic mechanisms min cid13 cid14 N m P m Denote ci erm S ci c P cid2 Ni c Clearly ci best classiﬁer agent hope Consider mechanism given Mechanism 1 Mechanism 1 Projected Majority mechanism PM cid2 cid8 Based labels agent P Ni calculate ci Deﬁne agent negative agent ci c positive agent ci c Denote P cid8 cid2 N P return c end ici c mi number examples belong positive agents similarly N ici c mi m P return c cid8 cid2 cid8 cid8 Remark 2 Informally state current setting obtain similar approximation results mech anisms SP assuming agents lie beneﬁcial Nevertheless strategyproofness gives clean framework analyze mechanisms face strategic behavior When discuss learning theoretic framework obtaining strategyproofness impossible shall apply elegant type analysis We mechanism excellent gametheoretic property group strategyproof coalition players gain lying In words agent coalition strictly gains joint lie agent coalition strictly lose While technically simple ﬁrst result demonstrates key principles strategyproof mechanisms Theorem 32 Mechanism 1 3approximation groupSP mechanism Proof We ﬁrst group strategyproofness Let B I We assume loss generality agents B positive negative positive resp negative agent gain lying mechanism R Meir et al Artiﬁcial Intelligence 186 2012 123156 131 Fig 3 The examples agent datasets shown t 2 Agent 1 dataset II look like dataset III vice versa reporting false labels The goes agent 2 datasets I II returns c resp c Again loss generality agents positive Therefore agent beneﬁt lying mechanism return c truthful dataset However mechanism considers agents B positive agents truthful dataset given agent B hope inﬂuence outcome reporting cid8 majority negative examples However increases N reinforcing mechanisms decision return c It remains demonstrate approximation ratio claimed We assume loss generality We ﬁrst prove mechanism returned positive concept 14 cid8 cid2 N cid8 c c Now agent positive ci c half mechanism returned c P examples positive P cid2 1 cid8 cid2 m 2 Indeed clearly P cid2 N cid8 4 m examples positive Thus P cid7 iI P cid2 cid7 cid7 P cid2 icic icic mi 2 cid8 P 2 P cid2 P 2 cid2 m 4 cid8 P cid2 m2 Now know P N m N m P cid3 m m cid3 3P Clearly mechanism decided correctly 4 3m 4 RI c S RI c S N m r Otherwise P m2 RI c S RI c S N m m In case RI c S cid3 3r cid3 3 P 3RI c S 3r proving Mechanism 1 3approximation mechanism cid2 As 3approximation achieved trivial mechanism naturally like know possible better approximation ratio waiving SP property We case proving matching lower bound best possible approximation ratio achievable SP mechanism Note lower bound requires strategyproofness group strategyproofness Theorem 33 Let cid2 0 There 3 cid2approximation strategyproof mechanism Proof To prove bound present 3 different datasets We SP mechanism return result concept C yields approximation ratio 3 cid2 Let cid2 0 We use I 1 2 integer t tcid2 deﬁned later Note datasets m1 m2 2t 1 We deﬁne datasets follows Fig 3 illustration S I P 1 2t 1 N1 0 P 2 t N2 t 1 S II P 1 2t 1 N1 0 P 2 0 N2 2t 1 S III P 1 t 1 N1 t P 2 0 N2 2t 1 132 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Let M strategyproof mechanism Then hold MS I MS II Indeed assume ﬁrst MS I c MS II c Notice difference settings agent 2s labels If agent 2s truthful labels S I subjective erm c Therefore report labels S II negative obtain c Now MS I c MS II c agent 2 gain deviating S II S I A symmetric argument respect agent 1 settings prefers c shows MS II MS III So loss generality assume c MS I MS II MS III c symmetric arguments yield result Therefore cid9 cid8 c S III RI RI cid8 c S III cid9 N1 N2 m 3t 1 4t 2 On hand negative concept better RI r cid8 c S III cid9 t 1 4t 2 By combining equations RI c S III r 3t1 4t2 t1 4t2 3t 1 t 1 8 Let set t 3 SP mechanism approximation ratio 3 cid2 cid2 cid2 expression strictly greater 3 cid2 RI c S III 3 cid2r We conclude 32 Randomized mechanisms What let mechanism ﬂip coins Can ﬁnd SP randomized mechanism beats expectation 3approximation deterministic lower bound To answer question ﬁrst recall deﬁnition risk mechanism given 3 For simple concept class C c c randomized mechanism deﬁned probability returning positive negative concept given S Accordingly risk private global cid8 R cid9 MS S p Rc S p Rc S p p stand pMc S pMc S We start investigation SP randomized mechanisms establishing lower bound 2 approximation ratio Theorem 34 Let cid2 0 There 2 cid2approximation strategyproof randomized mechanism The proof remaining proofs section appears Appendix A cid8 We presently forward randomized SP 2approximation mechanism matching lower bound upper bound However ﬁrst propose simpler mechanism analyze fails The natural thing deterministic Projected Majority Mechanism simply select c probability calculate P cid8m We refer simple mechanism weighted random dictator mechanism WRD P reasons apparent Section 417 Unfortunately simple randomization clearly SP beat deterministic bound 3 cid2 demonstrated following example cid8m c probability N N cid8 Example 35 Consider dataset S n agents following examples agent P 1 t 1 N1 t cid8 n 1 additional agents holding 2t 1 negative examples Thus P t 1 N n 12t 1 P n 12t 1 The optimal classiﬁer makes P t 1 mistakes r m On hand expected number mistakes mechanism cid8 2t 1 N t1 cid8 m RI WRDS S cid9 cid8 t 1 P m p P p N N m t 1 2t 1 n 12t 1 n2t 1 n2t 1 2nt n t 1 n 1t 1 n n nt n t 1 2nt n t 1 n cid8 cid8 n 12t 1 t cid9 2nt n t 1 3nt 2n 2t 2 n 7 This procedure equivalent randomly selecting agent probability proportional weight preferred classiﬁer classify entire datasethence Weighted Random Dictator R Meir et al Artiﬁcial Intelligence 186 2012 123156 We approximation ratio mechanism RI WRDS S r 3nt 2n 2t 2 nt 1 t 3 2 n 133 9 Thus cid2 0 largeenough t approximation ratio worse 3 2 n cid2 Note example agents control datasets size 2t 1 A similar example crafted weighted agents merging datasets agents 2 n single heavier agent This example provide lower bound 3 2w 1 w 1 weight lighter agent Crucially adjusted intuitive randomization trick Mechanism 2 The Square Weighted Dictator Mechanism SRD cid8 cid8 Compute P Return c c probability proportional P Mechanism 1 N cid82 N cid82 respectively Theorem 36 Mechanism 2 groupSP 2approximation randomized mechanism There fact multiple ways achieve 2approximation different randomizations N In previous version paper suggested alternative randomization 28 A procedure follows special case CRD mechanism described Section 41 P cid8 cid8 33 Binary decision learning theoretic setting In section extend simple setting general machine learning framework Our previous results leveraged obtain powerful learning theoretic results Instead looking ﬁxed set examples selecting concept ﬁts best turn look sampled datasets That assume ﬁxed known distribution D X cid4X cid4 A set probability distributions set A represents agents different parts input space According shared input assumption distribution agents In addition agent I private function Y X assigns label point input space Observe Y distribution D X induces private distribution Di inputs labels Di cid4X This distribution determines type agent The new deﬁnition subjective risk naturally extends previous setting expressing errors concept makes respect distribution Di cx cid7 y Ric Ex yDi cid10cid2 cid3cid12 cid10cid2 ExD X cx cid7 Y ix cid3cid12 The global risk calculated similarly previously deﬁned weighted average private risk RI c w Ric cid7 iI For ease exposition assume section agents equal weight Thus RI c 1 n Section 43 discussing general problem use assumption8 Similarly longer compare outcome mechanism r S notion optimal risk assumes ﬁxed dataset instance learningtheoretic setting consists set distributions We deﬁne minimal risk rmin inf cC RI c 12 Although general case C open set simple twofunction setting C ﬁnite rmin minRI c RI c Note directly evaluate risk learning theoretic framework sample points agents distributions ask agents label We try minimize real global risk empirical risk proxy9 The empirical risk risk sampled dataset deﬁned previous section 8 The results section generalized varying weights sampling agent number points proportional weight large 9 This similar oracle model direct access distribution ask yesno questions The major difference model oracle lie Sphinx model better 10 11 cid2 iI Ric In 134 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Remark 3 A subtle point mechanism present strategyproof fact mechanism gets sampled data points input strategyproof Indeed single agent gives greater weight negative points according D1 case miserable chance agents sampled dataset contains positive points Thus nonzero probability agent incentive lie reporting negative labels We note allowing payments guarantee strategyproofness example contains agent This fact contradictory revelation principle 32 recall truthful mech anisms guaranteed exist direct revelation In domain direct revelation means agents asked explicitly select classiﬁer prefer However learningtheoretic setting agents reveal preferences indirectly submitting preferred labels sampled data points 331 Three gametheoretic assumptions While strategyproofness ask assumptions behavior agents allow formally analyze outcome mechanisms We exploit simple setting clarify distinction alternative gametheoretic assumptions agents behavior The cid2truthfulness assumption The ﬁrst assumption agents lie expected gain lie cid2 This assumption stronger rationality assumption decisionmaking setting demanded cid2 0 In Section 43 refer assumption Truthful Approach This approach taken example Dekel et al 10 The pure rationality assumption A second assumption agents play dominant strategy available The existence dominant strategies depends mechanism dataset allow arbitrary behavior strategy exist This assumption stronger standard rationality assumption assume agents behavior truthtelling suboptimal incomparable ﬁrst assumption In Section 43 refer assumption rational approach It important note rational approach entails agents complete knowledge distribution This implicit assumption necessary truthful approach The weak truthfulness assumption The assumption weakest requires agent truthful weakly dominant strategy gain lying An agent obeys ﬁrst second assumption called cid2truthful purely rational weakly truthful respectively Note cid2truthful agents cid2 cid2 0 purely rational agents weakly truthful means assumption weakest In section employ assumption supplies strongest results Thus results section stronger way results Dekel et al 10 regression results Section 4 classiﬁcation10 Remark 4 We offer simple scenario highlight substantial difference different assumptions Suppose employ pure rationality assumption consider following simple mechanism sample point D X let agents label single point If agent labels point positively agent positive negative Now apply Mechanism 1 Mechanism 2 This clearly gives approximation upper bounds 3 2 respectively sampled data point In contrast cid2truthfulness assumption guarantee case This suggests difference assumptions nontrivial Compare analysis ﬁrst approaches Section 43 Mechanism 3 The Binary Learning Mechanism cid15SRD agent I cid8 mi points iid D X Sample m Denote set data points Xi xi1 ximcid8 Ask agent label Xi Denote S cid5xi j Y xi j cid6m cid8 j1 end Use Mechanism 2 S S 1 Sn return SRDS 10 In fact simple variant proofs Section 43 directly applied binary decision problem special case shared inputs bounded VC dimension yielding approximation ratio close 2 However bound hold ﬁrst strategic assumptions R Meir et al Artiﬁcial Intelligence 186 2012 123156 135 The risk mechanism computed expectation risk outcome classiﬁer expectation taken randomizations sampling data points randomization performed SRD Formally private global risk cid10 R cid15SRD E XD X m cid8 R SRDS cid9cid12 13 labels X S set according strategic assumptions We presently establish theorem explicitly states number examples need sample order properly estimate real risk We expectation taken randomness sampling procedure Mecha nism 2s randomization Mechanism 3 yields close 2approximation relatively examples face strategic behavior Theorem 37 Given sampled datasets assume weak truthfulness For cid2 0 m sampling m points agent holds cid8 cid8 polynomial lnn 1 cid2 RI cid15SRD cid3 2rmin cid2 cid8 50 1 cid22 ln 10n Speciﬁcally sampling m While proof technical sketched follows Mechanism 2 SP respect sampled dataset S Thus agents sampled dataset faithfully represents true distribution agent strongly inclined c c agent beneﬁt lying weak truthfulness assumption If agent indifferent c c wish liebut crucially agent contributes little global risk cid2 suﬃce 4 Classiﬁcation shared inputs We begin analysis decisiontheoretic setting As Section 3 results later applied learningtheoretic setting In section assume agents control set data points The size dataset denoted k The total number labeled data points agents m n k However mechanisms section use single agent k effectively size input 41 Deterministic mechanisms We start examining extremely simple deterministic mechanism Recall ermS cid8 concept c C cid8 S Eq 4 Our mechanism simply lets heaviest agent dictate concept chosen minimizes risk wrt S Mechanism 4 The Heaviest Dictator Mechanism HD h argmaxiI w Let h I agent maximal weight return ermSh If erm exists return arbitrarily The mechanism clearly SP heaviest dictator h lie best concept selected agents simply ignored reason lie We following result Theorem 41 Let I n For concept class C dataset S Mechanism 4 SP 2n 1approximation mechanism Recall central negative result deterministic mechanisms nonrestricted input Theorem 42 Meir Procaccia Rosenschein 29 There exist concept classes deterministic SP mechanism approximation ratio Ωm m total size dataset We restriction shared inputs helps removing dependency size dataset approximation ratio increases linearly number agents appealing However turns deterministic mechanisms better respect concept class Indeed slight variation Theorem 42 gives following result Theorem 43 Suppose n agents shared inputs There exist concept classes deterministic SP mechanism approximation ratio Ωn weights equal 136 R Meir et al Artiﬁcial Intelligence 186 2012 123156 The proof theorem minor variation proof Theorem 42 applies GibbardSatterthwaite impossibility theorem 1738 Theorem 43 implies Mechanism 4 optimal constant generic mechanism applies concept class Of course speciﬁc concept classes better shown Section 3 One hope imposing restrictions dataset realizability enable design better SP mechanisms However recent results Ωn bound remains datasets realizable 13 42 Randomized mechanisms In order break lower bound given Theorem 43 employ simple randomization We randomization yields constant approximation ratio respect concept class assumption shared inputs course Moreover agents uniform weights mechanism improved Mechanism 5 The Weighted Random Dictator WRD mechanism select agent probability w return ermS Consider Mechanism 5 clearly SP The following theorem bounds approximation ratio different cases Theorem 44 For concept class C dataset S Mechanism 5 SP 3 2w minapproximation mechanism w min miniI w Moreover S individually realizable 2 2w minapproximation guaranteed When agents weight w min 1 n We following corollary follows directly Theorem 44 Corollary 45 Let I n assume agents equal weights For concept class C dataset S Mechanism 5 SP 3 2 n S individually realizable n approximation mechanism 2 2 The corollary follows special case results Section 422 It possible analysis Mechanism 5 tight Indeed consider outcome mechanism concept class c c In case mechanism essentially equivalent naive randomized mechanism presented Section 32 yields outcome Therefore Example 35 gives tight lower bound approximation ratio mechanism matching upper bound given Theorems 44 45 A similar example easily constructed concept class size 421 Is WRD mechanism optimal It natural ask better randomized SP mechanisms exist For speciﬁc concept classes answer question positive demonstrated Theorem 36 For general concept classes following lower bound known Theorem 46 Meir Almagor Michaely Rosenschein 27 Suppose n agents shared inputs There exist concept classes randomized SP mechanism approximation ratio 3 2 n weights equal Theorem 46 shows weights uniform WRD mechanism selecting dictator uniformly random fact optimal That SP mechanism better However mechanism suboptimal weighted datasets guarantees 3 approximation case We turn close gap presenting new mechanisms beat WRD mechanism weighted datasets matching lower bound given Theorem 46 422 Improving upper bound weighted agents Theorem 46 fact tells pick dictator random SP mechanism However free deﬁne probabilities selecting different agents agents weights account The WRD mechanism example randomization design Recall twofunction scenario performed optimal randomization SRD mechanism As ﬁrst attempt improve upper bound translate SRD mechanism current setting11 That mech anism select dictator I probability proportional w 2 Unfortunately SRD attain improvement WRD mechanism suboptimal n 3 11 We slightly abuse notation use SRD longer equivalent Mechanism 2 R Meir et al Artiﬁcial Intelligence 186 2012 123156 137 Proposition 47 There dataset S agents cid8 RI SRDS S cid9 24 r cid16 3 2 n cid17 r A similar counterexample exists individually realizable datasets approximation ratio SRD 139 n n 3 We somewhat different approach selection dictator strictly 2 2 Consider mechanisms CRD RRD small variation Mechanism 6 The Convexweight Random Dictator Mechanism CRD cid8 22w w I set p compute αw 1cid2 iI p select agent probability pi αw p return ermS cid8 cid8 Mechanism 7 The Realizableweight Random Dictator Mechanism RRD h argmaxiI w wh cid2 1 2 return ermSh cid8 w end I set p compute βw 1cid2 iI p select agent probability pi βw p return ermS 12w cid8 cid8 The CRD RRD mechanisms clearly SP probabilities unaffected reported labels Theorem 48 The following hold Mechanism 6 αw cid3 2 2 n CRD approximation ratio 1 αw 3 2 n S individually realizable approximation ratio αw 2 1 2 1 n By Theorem 46 SP mechanism better general dataset worst case CRD optimal However dataset known individually realizable CRD suboptimal RRD strictly better worst case Theorem 49 The following hold Mechanism 7 βw cid3 1 2 n RRD approximation ratio 4 3 worst case S individually realizable approximation ratio 1 βw 2 2 n Observe agents RRD simply selects heavier dictator Thus dataset realizable approx imation ratio high 3 accounts lower bound nonrealizable case The CRD mechanism matches lower bounds set weighted agents showing uniform weight case fact hardest The situation RRD mechanism similarno randomization dictators better However open question better sophisticated randomized mechanisms realizable case The natural conjecture Dokow et al proved deterministic mechanisms 13 Note weights uniform CRD RRD SRD WRD mechanisms coincide12 Thus Theorem 45 follows special case Theorems 48 49 Curiously RRD better CRD dataset known realizable general case converse true Therefore different mechanism depending assumptions dataset However mechanism decided aprioriwe select CRD RRD observing labels strategyproof 12 There tiny exception n 2 w 1 w 2 1 outcome 1approximation 2 RRD returns arbitrary dictator random However case 138 R Meir et al Artiﬁcial Intelligence 186 2012 123156 423 Applying mechanisms twofunction setting Suppose C We join positive agents negative agents construct instance cid8 N metaagents weights proportional P deﬁned Section 31 The RRD mechanism simply selects heavier metaagent equivalently PM mechanism guarantees approximation ratio 3 The CRD mechanism applied setting guarantees approximation ratio 3 2 2 It n supplies alternative 2approximation SP mechanism twofunction setting 3 2 2 cid8 43 The learningtheoretic setting In section leverage upper bounds attained decisiontheoretic setting obtain results machinelearning framework That present learning mechanism guarantees constant approximation optimal risk expectation face strategic behavior We use notations deﬁnitions introduced Section 33 preferences agent represented function Y X 13 Reinterpreting shared input assumption learningtheoretic setting assume agents probability distribution D X X reﬂects relative importance agents attribute different input points distribution D X common knowledge The private risk classiﬁer c C computed according Eq 10 cid3cid12 cid10cid2 Ric ExD X cx cid7 Y ix That according expected number errors c makes wrt distribution D X As global risk computed according Eq 11 RI c cid7 iI w iRic The goal mechanisms ﬁnd classiﬁers low risk We compare best risk attainable concepts C rmin infcC RI c Eq 12 special case deﬁnition C c c Our goal design mechanisms risk close optimal However constructing SP mechanism learns sampled data nearly impossible explained Remark 3 Hence weaken strategyproofness requirement analyze performance mechanisms ﬁrst strategic assumptions described Section 33 cid2truthfulness assumption states agents lie gain cid2 pure rationality assumption agents play weakly dominant strategy exists 431 The cid2truthfulness assumption An cid2strategyproof mechanism agents gain cid2 lying We similarly Dekel et al 10 results Section 42 employed obtain mechanism usually cid2strategyproof We focus following mechanism Mechanism 8 The Generic Learning Mechanism cid15CRD Sample k data points iid D X denote sampled points X agent I Ask agent label Xi Denote S cid5x j Y x j cid6k j1 end Use Mechanism 5 S S 1 Sn return CRDS We denote RI cid15CRD expected risk Mechanism 8 expectation taken randomness sampling randomness Mechanism 5 Eq 13 twofunction setting R cid15CRD E XD X k cid8 cid10 R CRDS cid9cid12 labels X S set according varying strategic assumptions We wish formulate theorem asserts given samples expected risk Mechanism 8 relatively small cid2truthfulness assumption The exact number samples needed depends combinatorial richness function class usually measured notion class complexity VC dimension 21 For instance VC dimension class linear separators Rd d 1 We dwell point instead assume dimension bounded 13 As theorems Section 41 results section follow special case general model agents distribu tions labels R Meir et al Artiﬁcial Intelligence 186 2012 123156 139 Theorem 410 Assume agents cid2truthful let C concept class bounded dimension For cid2 0 k n rmin cid2 polynomial 1 cid2 lnn st k data points sampled expected risk Mechanism 8 3 2 The proof sketch follows There high probability random sample good close actual agents b Whenever sample good agent agent report truthfully cid2truthfulness assumption c When sample good agents risk Mechanism 8 close risk Mechanism 5 3 2 n approximation d Otherwise risk high small effect total expected risk occurs low probability We prove Theorem 410 lines Appendix B2 supply exact upper bound number samples required theorem hold 432 The pure rationality assumption Recall pure rationality assumption agent use dominant strategy exists We consider performance Mechanism 8 Note mechanism uses dictator agent weakly dominant strategy In order observe classiﬁer ˆci minimizes risk wrt distribution Di 14 The dominant strategy agent label sampled dataset X according ˆci Note mean truthful possible ˆcix cid7 Y ix Remark 3 Theorem 411 Assume agents purely rational let C concept class bounded dimension For cid2 0 n rmin cid2 k polynomial 1 cid2 st k data points sampled expected risk Mechanism 8 3 2 Interestingly alternative assumption improved sample complexity number required samples longer depends n 1 cid2 In somewhat counterintuitive way rationality assumption provides better bounds notion truthfulness This explained fact rational selfinterested labeling dataset better proxy agents real type truthful labeling Indeed strange claim true sampling process produce set points X represents agents distribution inaccurate way15 5 Discussion We ﬁrst review results decision making setting learning theoretic setting ﬁnally present directions future research 51 Decision making setting We started studying simple case possible decisions In setting trivial mechanism group strategyproof guarantees 3approximation ratio While better determinis tic mechanisms showed speciﬁc randomization achieve 2approximation ratio maintaining groupSP property For general case showed simple randomization dictator WRD mechanism achieves best possible approximation ratio agents uniform weights falls short weighted case We presented new mechanism closes gap obtains optimal approximation results general case CRD In weighted realizable case presented mechanism matches best known results uniform weights However open question bound tight nontrivial lower bounds known We showed approximation results stand sharp contrast deterministic case deterministic mechanism guarantee constant approximation ratio The trivial selection heaviest agent dictator best deterministic SP mechanism hand Results highlight power shared inputs assumption allow break lower bounds hold general case 29 All results summarized Tables 1 2 help decision makersboth human automatedin reaching decision approximately maximizes social welfare data biased conﬂicting interests 511 Implications facility location As hinted introduction classiﬁcation model seen facility location metric spaces n bound Theorem 45 follows directly folk result particular space use binary cube In fact 2 2 14 There ﬁne issue ﬁniteness concept class deal proof 15 As explained Remark 3 revelation principle apply agents report preferences 140 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Table 1 Summary results deterministic mechanisms The corresponding theorem result appears parentheses HD PM Lower bound All classes shared inputs General datasets O n Th 41 Ωn Th 43 Realizable datasets O n Ωn 13 Binary decision O n 3 Th 32 3 Th 33 Table 2 Summary results randomized mechanisms We conjecture upper bound realizable datasets tight remains open question WRD SRD CRD RRD Best upper bound Lower bound All classes shared inputs General datasets 3 Th 44 24 Prop 47 3 2 n Th 48 cid2 3 Th 49 3 2 3 2 n CRD n Th 46 27 Realizable datasets 2 Th 44 139 2 1 2 2 n Th 48 n Th 49 2 2 n RRD Binary decision 3 2 Th 36 2 3 2 SRD CRD 2 Th 34 facility location employed example Alon et al 2 We results decision theoretic setting wider context metric spaces extending generalizing mentioned folk theorem Let cid5F dcid6 metric space16 Let F f 1 fn ﬁnite set points F point f attached F weight w reﬂecting importance Deﬁne d f F weighted average distance f F let f point minimizes distance f argmin f F d f F argmin f F cid7 w id f f icid3n We interested selecting points F close possible points The restriction selection blind That select knowing actual distances All know weights n points Clearly weights uniform better simply picking random point F The following inequality folk theorem bounds expected distance achieved process cid8 d f cid9 F 14 1 n cid7 icid3n d f F cid3 cid17 cid16 2 2 n As informally explained upper bounds approximation ratio WRD mechanism realizable Theorem 45 derived Eq 14 deﬁning metric classiﬁers reﬂecting fraction data space disagree In uniformweight realizable case WRD mechanism picks agent random risk exactly average distance agents optimal classiﬁer agents The formal details appear Appendix B supply analog nonrealizable case extend bounds weighted agents Moreover proofs mechanisms Section 4 attain speciﬁed approximation ratios general model private labels nondeterministic datasets given form private distribution X 17 The theorems Section 4 standard model presented deterministic labels follow special case 512 Implications partition judgment aggregation Given subset X Rd particular interval partitions X form metric space Informally distance partitions exactly volume disagree The set partitions allowed constitutes concept class C A similar approach Judgment aggregation problem requires additional assumptions issues agenda directly compared quantiﬁed We clarify following simple example Doctrinal paradox 14 The agenda contains logical expressions X b b Legal assignments logically consistent 1 1 1 legal 1 1 0 We naturally deﬁne C 16 It fact suﬃcient assume d pseudometric possible d f f 17 The datasets Section 3 viewed single data point nondeterministic labels The probabilities positivenegative label agent proportional P Ni respectively cid8 0 f cid7 f cid8 R Meir et al Artiﬁcial Intelligence 186 2012 123156 141 set legal assignments C 4 case The dataset S contains opinion judge correct assignment Consistency judges opinions coincides requirement S individually realizable The subtle issue apriori reason example 1 1 0 closer 1 1 1 0 0 0 However assign ﬁxed weight issue agenda judges agree natural metric shared input setting Section 4 Our suggested mechanisms randomize legal assignment closeon averageto opinions judges It important note judges disagree importance certain issues approximation welldeﬁned strategyproofness longer guaranteed Dokow Holzman 14 characterized agendas deterministic nondictatorial aggregation rules exist18 Our randomizations guarantee constant bound social welfare agenda likely families agendas characterized Dokow Holzman better outcome guaranteed We mention context recent paper Nehama 31 studies approximate judgment aggregation rules different angle considering incentives welfare Rather paper characterizes rules properties consistency approximately hold We hope explore applicability similar relaxations domains future work 52 Learningtheoretic setting In cases constant upper bound approximation ratio available showed use SP decision mechanism implement learning mechanisms bounded expected risk More precisely mechanisms sample ﬁnite number data points given distribution labeled selfinterested agents The expected risk mechanism expectation taken sampling procedure internal randomization compared expected risk given distribution best classiﬁer concept class This allows achieve approximation ratio arbitrarily close approximation guaranteed decision theoretic setting 2 classiﬁers 3 2 n provided agents sample distribution When optimal risk high 510 results useful With low optimal risk constant approximation ratio 2 3 good especially applies concept classes distributions We distinction alternative gametheoretic assumptions agents behavior showing different assumptions affect mechanism number required samples Our results learning theoretic setting contribute design algorithms function non cooperative environments We promote understanding underlying assumptions agents behavior environments affect learning process 53 Future work Future research provide answers questions left open expand young hybrid ﬁeld new directions More eﬃcient SP mechanisms crafted handle speciﬁc concept classes Further extensions SP classiﬁcation model presented considered formalizations PAClike suggested different loss functions alternative gametheoretic assumptions restrictions structure dataset It possible alter model allowing different types strategic behavior misreporting location data points labels All directions reveal new parts overall picture promote better understanding conditions SP learning place effectively This turn supply new insights results relationship areas Acknowledgements This work partially supported Israel Science Foundation grant 89805 Israel Ministry Science Tech nology grant 36797 Google InterUniversity Center Electronic Markets Auctions The authors thank Omri Abend Shaull Almagor Assaf Michaely Ilan Nehama enlightening comments drafts paper Appendix A Proofs Section 3 Theorem 34 Let cid2 0 There 2 cid2approximation strategyproof randomized mechanism Proof We use datasets proof Theorem 33 illustrated Fig 3 Let M SP randomized mechanism denote pMc S probability outputting c given S We ﬁrst mechanism chooses positive hypothesis probability datasets 18 Dokow Holzman 14 require strategyproofness different properties closely related 142 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Lemma A1 pMc S I pMc S II pMc S III Proof As proof Theorem 33 agents dataset look like dataset If pMc S I cid7 pMc S II agent 2 report labels way guarantees higher probability c Similarly pMc S II cid7 pMc S III implies agent 1 increase probability c lying cid2 Denote p pM cid8 c cid11 cid11 S I cid9 cid8 c cid9 cid11 cid11 S II cid8 c cid9 cid11 cid11 S III pM pM p pM cid8 c cid11 cid11 S I cid9 cid8 c cid9 cid11 cid11 S II cid8 c cid9 cid11 cid11 S III pM pM Without loss generality p cid2 1 2 cid8 cid8 M cid9 cid9 S III S III cid8 pRI c S III p 3t 1 4t 2 cid9 cid2 p Then cid8 cid9 c S III cid2 1 2 pRI p t 1 4t 2 RI 3t 1 4t 2 1 2 t 1 4t 2 1 2 RI r cid8 c S III cid9 t 1 4t 2 For t 1 cid2 holds RI MS III S III r 2 1 t 1 As p p symmetric argument shows RI MS I S I 2 cid2r 2 cid2 4t 2 2t 1 achieve 2 cid2approximation randomization cid2 Therefore SP mechanism Theorem 36 Mechanism 2 group strategyproof 2approximation randomized mechanism Proof Similarly Mechanism 1 Mechanism 2 clearly group SP declaring false label increase probability obtaining classiﬁer labels correctly half agents examples increasing subjective expected risk Assume loss generality N cid2 P negative classiﬁer c better Denote w N cid8 m total weight agents support c Lemma A2 1 r cid3 1w 1w r Proof The largest possible number negative examples achieved negative agents control negative examples positive agents control slight majority positive labels Formally N cid3 N cid8 P cid8 2 1 r cid3 N RI c N m m 1 1 r cid8 cid8 P 2m cid2 1w w 1 w 1 w 2 2 By dividing inequalities 1r 2 It follow r r cid3 1w 1w lemma proved cid2 cid8 cid9 RI SRDS S w 2r w 2RI c S 1 w2RI c S w 2 1 w2 1 w21 r w 2 1 w2 1 w1 wr w 2 1 w2 2r w 2r w 2r cid3 r cid3 1 12 1 w2 1w 1w r w 2 1 w2 Lemma A2 1 2w 2 2w 1 r inequality holds 2w 2 2w 1 minimum w 1 2 cid2 R Meir et al Artiﬁcial Intelligence 186 2012 123156 143 Theorem 37 Given sampled datasets assume weak truthfulness For cid2 0 m sampling m points agent holds cid8 cid8 polynomial lnn 1 cid2 RI cid15SRD cid3 2rmin cid2 Speciﬁcally sampling m cid8 50 1 cid22 ln 10n cid2 suﬃce Proof In proof differentiate real risk deﬁned learningtheoretic setting empirical risk given sample deﬁned simple setting The empirical risk denoted ˆRI c S 1 m cid7 cid2 cx cid7 y cid3 cid5x ycid6S Also simplify notation replace cid15SRD M proof Note M equally stand group strategyproof 2approximation mechanism including CRD mechanism presented 28 Without loss generality assume r RI c RI c Notice r concept mechanism returns trivially attain risk 1 2 rest proof 3cid2 cid3 r RI c 3cid2 cid3 1 2 cid3 RI c 3cid2 RI c 3cid2 6cid2 Therefore assume RI c 1 2 15 Let introduce new notations deﬁnitions Denote data set real labels S cid5xi j Y ixi jcid6 jcid3mcid8 S S1 Sn Note mechanism direct access S reported labels appear S Deﬁne G event empirical real risk differ cid2 agents formally cid11 cid11 cid11 cid2 cid11 ˆRic S Ric c c c I 16 Lemma A3 Let δ 0 If m cid8 1 2cid22 ln 2n δ probability 1 δ G occurs Proof Fix I Consider event Y ix indicator random variable Y ix We rewrite empirical real risk sum expectation variable cid10 cid12 y cid7 Y ix cid7 cid2 cid10cid2 cid3cid12 Ric ExD X ˆRic S 1 mcid8 cid3 Y ix Ex yDi 1 mcid8 x yS x yS y Since S sampled iid Di empirical risk sum independent Bernoulli random variables expectation Ric We derive Chernoff bound data set size S m cid8 Pr cid10cid11 cid11 cid11 ˆRic S Ric cid11 cid2 cid8 1 Taking m cid12 2e 2cid22m cid8 2cid22 ln 2n δ cid11 cid11 cid10 cid11 cid2 cid11 ˆRic S Ric I cid10cid11 cid11 cid12 cid11 ˆRic S Ric cid11 cid2 Pr cid3 cid12 PrG Pr cid7 iI ﬁrst inequality union bound cid2 Note cid11 cid11 cid11 cid11 ˆRic S Ric cid11 cid11 cid11 cid11 ˆRic S Ric cid3 I2e 2cid22m cid8 n δ n δ c If G occurs 16 triangle inequality holds c c c I cid11 cid11 cid11RI c ˆRI c S cid11 cid3 cid7 iI 1 n cid11 cid11 cid11Ric ˆRic S cid11 cid3 cid2 17 Using 17 bounded risk MS unfortunately mechanism access S S In order bound RI MS need know estimate agents label examples To 144 R Meir et al Artiﬁcial Intelligence 186 2012 123156 handle problem ﬁrst analyze agents gain lying deﬁne new data set S following properties agent motivation lie assess result running M S S S similar We divide I types agents I P Ni number positivenegative examples agent controls S Note P m assume loss generality agents I mechanism Agents I Mechanism 2 Mechanism 3 step 3 SP For agent deﬁne new set examples S follows For agent I denote cid8 ˆRic S Since RI c RI c prefer c lying lowers expected risk hand beneﬁt lying S reﬂect truthful preferences cid2 I cid8 I Ric 1 cid8cid8 I I cid8cid8 2 cid8 cid8 If I If I S S cid8cid8 deﬁne P P cid17cid2m cid8 cid8cid18 let S contain P positive examples m cid8 P negative ones Lemma A4 If G occurs agents I Ni cid3 P Ric cid2 Ric Proof If I cid8cid8 wlog Ric cid3 Ric 2cid2 16 P P m cid8 ˆRic S cid3 m cid8 cid8 Ric cid2 cid9 cid8 cid8 cid3 m Ric cid2 cid9 cid3 m cid8 ˆRic S Ni Ni If I cid8 according assumption Ric cid3 Ric cid3 Ric 2cid2 Moreover deﬁnition P P cid2 P m cid8cid2 Ni cid3 Ni m cid8cid2 Thus P cid2 P m cid8cid2 m cid8 ˆRic S m cid8cid2 cid2 m cid8 Ric cid2 m cid8 Ric cid2 m cid8 cid8 ˆRic S cid2 cid9 cid2 Ni m cid8cid2 cid2 Ni cid2 Lemma A4 implies G occurs agents better report S Mechanism 3 S reﬂects real preferences agent Now agent reports truthfully P P If decides lie report positive labels gain reporting P labels crucially mechanisms outcome change case The immediate result assume P cid3 P cid7 iI 1 n P cid3 cid7 iI 1 n P P expected risk M increases number positive examples probability Mechanism 3 choosing positive classiﬁer increases cid8 cid9 M S cid9 MS cid9 MS 18 cid8 cid8 cid3 RI cid3 RI RI We concentrate bounding empirical risk S Lemma A5 If G occurs c c c cid11 cid11 cid11 cid3 3cid2 cid11RI c ˆRI c S As Lemma A3 suﬃce c 19 cid8 1 cid2 P cid17m mcid8 cid8cid2cid18 cid8cid2 1 cid3 P m mcid8 Proof From 16 m ˆRI c S P mcid8 cid3 P m cid8 2cid2 ˆRI c S 2cid2 cid3 RI c cid2 2cid2 RI c 3cid2 cid2 R Meir et al Artiﬁcial Intelligence 186 2012 123156 From 15 19 ˆRI c S cid3 RI c 3cid2 cid3 RI c 3cid2 cid3 ˆRI c S So c empirically best concept S Mechanism 2 guarantees cid8 ˆRI M S S cid9 cid3 2 ˆRI c S Furthermore risk Mechanism 3 convex combination risk c c 19 cid8 cid9 M S RI cid3 ˆRI cid8 M S S cid9 3cid2 145 20 21 22 Finally 18 22 21 20 order G occurs cid9 cid8 cid8 cid8 cid9 MS RI cid9 cid3 ˆRI M S cid3 RI cid8 RI c 3cid2 cid3 2 cid9 M S S 3cid2 2r 3cid2 cid3 2 ˆRI c S 3cid2 9cid2 If G occur risk exceed 1 Thus applying Lemma A3 δ cid2 cid2cid8 cid2cid82 ln 10n cid2cid8 50 1 cid8 RI cid15SRD cid3 PrG cid9 9cid2 2r PrG1 cid3 2r 9cid2 cid2 cid3 2r cid2cid8 10 ﬁnd m cid8 required cid2 Appendix B Proofs Section 4 B1 Proofs upper bounds shared inputs Sections 41 42 We formulate prove results somewhat general model preferences agent encoded distribution deterministic function The new model extends presented Section 4 components data points receive attention b preferences agent reﬂect uncertainty indeterminism label speciﬁc data point The theorems Section 4 follow easily special case In addition use distributions makes proofs generalization section Section 43 easier natural For purpose replace proﬁle ﬁnite datasets S cid5S1 Sncid6 proﬁle distributions F cid5F 1 Fncid6 X The marginal distributions X We denote marginal FX measure agents different parts input space Let H set deterministic functions h X In particular C H We adjust deﬁnition private global risk handle distributions The private risk h H agent wrt proﬁle F deﬁned cid3cid12 Rih F Ecid5x ycid6F hx cid7 y cid10cid2 As usual global risk deﬁned cid7 RI h F w iRih F iI As discrete datasets F said realizable wrt concept class C H concept c C Every distribution p X induces nondeterministic function f p X labels Formally Pr f px x Ecid5x ycid6p y x convenience denote probability f px 0 1 Similarly Ecid5x ycid6p cid12 cid10 y x We denote F set nondeterministic functions Note H F concept class C cid8 f px 1 f px Pr cid9 f px x Ric F 0 subset F A special case p F case f f p conveys preferences agent We assume agents prefer ences independent agents cid7 j x X y y cid8 cid8 Pr f ix y f jx y cid8 Pr f ix y cid11 cid9 cid11 x cid8 cid11 cid9 cid11 x cid8 Pr f jx y cid8 cid11 cid9 cid11 x 23 Deﬁnition B1 We deﬁne distance classiﬁers wrt ﬁxed distribution F X cid4X space label differently Formally cid8 d f f cid9 cid8 cid8 cid9 cid8 f f d F X ExF X cid10 cid8 Pr f x cid7 f cid8 x cid11 cid11 x cid9cid12 24 146 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Let C H concept class following holds c C j I d f j c R jc F The proof Eq 25 follows R jc F Ecid5x ycid6F j cid10cid2 cx cid7 y cid3cid12 ExF X cid18 cid7 cid2 y x cx cid7 y cid19 cid3 y Pr yF j cid3cid12 cid10 cid2 cid3 EF X EF X EF X EF X cid9cid2 f jx cid8 cid10 Pr cid8 cid10 Pr cid8 cid10 Pr cx cid7 cid11 cid11 x f jx f jx cx f jx cid7 cx cid11 cid11 x cid9cid12 cx cid7 cid8 cid3 Pr cid8 Pr cid2 f jx cx cid7 cid11 cid9 cid11 x dc f j argmincC RI c F 24 f jx cid9cid2 cid11 cid11 x f jx cx cid3cid12 cx cid7 cid11 cid9cid12 cid11 x Recall ci argmincC Ric F c As special case Eq 25 j I cid9 cid8 dci f j R jci F cid20 ci argmin cid21 dc f cC 25 26 27 The following lemma seen formalization statement decisionmaking setting equivalent facility location metric space binary cube Lemma B2 d reﬂexive nonnegative symmetric satisﬁes triangle inequality f f d f f ExF X cid8 f Proof Nonnegativity symmetry trivial Pr f x cid7 f x x ExF X cid8cid8 F Note disagreement f f cid8 Pr f x cid7 f f x cid7 f cid11 cid9 cid11 x Pr x cid8 cid8cid8 cid8 cid3 Pr 0 0 reﬂexive We prove triangle inequality Let cid8cid8 requires disagrees f x X cid8 cid8 x f x f cid11 cid8 cid9 cid11 x Pr x f cid8cid8 cid11 cid9 cid11 x x cid7 f x cid8 cid8 Pr cid8cid8 x f x f cid11 cid9 cid11 x cid8 x f cid8 x cid7 f cid8cid8 x cid11 cid9 cid11 x cid8 f x cid7 f cid8 cid8 cid9 cid8cid8 d f f ExF X ExF X Thus triangle inequality holds cid2 f x cid7 f f x cid7 f cid8 cid8 cid10 Pr cid8 cid10 Pr cid8cid8 x x cid9cid12 cid11 cid11 x cid11 cid9cid12 cid11 x cid8 cid10 cid3 ExF X Pr cid8 cid10 ExF X Pr f x cid7 f cid8 x cid7 f f cid11 cid9 cid11 x cid11 cid11 x cid8 x cid8cid8 x cid8 Pr cid9cid12 f cid8 cid8cid8 cid8 x cid7 f cid9 cid8 d f f d cid9cid12 cid11 cid11 x cid8cid8 f cid9 x cid8 cid8 f Lemma B3 cid7 iI cid7 iI Proof w iRI ci F cid7 cid7 j w w jdci f j w iRI ci F cid7 w iRI ci F cid7 cid16cid7 w j cid17 w jR jci F cid7 cid7 j w w jdci f j cid2 Lemma B4 cid7 cid7 j w w jd f f j cid3 2 2wminr R Meir et al Artiﬁcial Intelligence 186 2012 123156 147 Proof cid7 cid7 w w jd f f j cid7 cid7 w w jd f f j d f f 0 j cid7 cid7 cid8 cid8 d f c cid9 jcid7i cid8 c d cid9cid9 f j w w j triangle inequality jcid7i cid7 cid8 w id f c cid9cid7 jcid7i cid7 cid7 cid8 w w jd f j c cid9 w j cid8 w id f c cid9 1 w cid7 jcid7i cid16cid7 cid8 w w jd f j c cid9 w id cid8 f c cid17 cid9 cid8 cid8 d cid8 cid8 d w w cid9 f c 1 w r j cid8 w id cid9cid9 f c cid9 f c 1 w min r w mind cid9cid9 cid8 f c cid3 cid3 cid7 cid7 cid7 1 wmin cid7 cid16 cid8 w d f c cid7 cid9 r w w min cid7 cid8 w id f c cid17 cid9 1 wminr r w minr 2 2wminr cid2 Note Eq 14 derived special case lemma weights uniform We use lemmas bound approximation ratio mechanism extended setting We begin simpler deterministic mechanism Theorem 41 cid2 Let I n For concept class C proﬁle F Mechanism 4 SP 2n 1approximation mechanism Proof We ﬁrst ﬁnd lower bound r cid9 cid9 cid7 RI r cid8 c F cid8 c F w iRi cid2 w jd cid9 cid8 c f j iI cid2 1 n cid9 cid8 c d f j cid7 iI cid9 cid8 c f w id j heaviest 28 Then upper bound risk c j cid8 cid9 RI HDF F RI c j F cid7 w idc j f w jdc j f j cid3 w jd cid8 c f j cid9 iI cid7 cid8 d cid8 c j c cid9 cid8 c d w f cid9cid9 cid8 c j c d cid9cid7 icid7 j w cid7 cid8 c cid9 cid8 c j c d f w id cid7 icid7 j w idc j f triangle inequality cid9cid7 icid7 j w r cid16 iI r icid7 j cid9 n 1 n cid8 dc j f j d cid8 c 2d f j cid9 n cid8 c j c n 1 n 1 n cid17 cid9cid9 w j cid2 1 n cid8 f j c 27 n 1 2n r n n 12r 28 2n 1r cid2 cid3 d cid3 r cid3 r cid3 r r triangle inequality cid2 Theorem 44 w min miniI w Moreover S individually realizable 2 2w minapproximation guaranteed For concept class C dataset S Mechanism 5 SP 3 2w minapproximation mechanism 148 R Meir et al Artiﬁcial Intelligence 186 2012 123156 Proof Using lemmas cid8 RI WRDF F cid7 cid9 w iRI ci F cid7 cid7 w w jd f c j iI cid7 cid7 cid7 j cid7 cid7 j cid7 cid3 cid3 j cid8 cid9 d f f j d f j c j w w j triangle inequality cid8 d f f j d cid8 f j c cid9cid9 w w j 27 w w jd f f j cid7 cid8 w jd f j c cid9cid7 w j cid3 2 2wminr 2 2wminr cid7 j cid7 j cid8 w jd f j c cid9 cid9 cid8 c F w jR j Lemma B4 25 2 2wminr RI j cid9 cid8 c F 3 2wminr Further individually realizable proﬁle F cid8 agent j d f j c j R jc j F cid8 0 25 case cid8 cid8 WRD RI cid9 cid8 F F cid9 cid8 cid7 iI cid8 ci F cid9 cid8 cid3 w iRI cid7 cid7 j w w jd f f j cid3 2 2wminr Thus proof Theorem 44 Theorem 44 special case complete cid2 Proposition 47 There dataset S agents cid8 RI SRDS S cid9 24 r cid16 3 2 n cid17 r Example B5 We set concept class C c c Assume wlog agent indifferent concepts dictates c concept Let S1 S2 positive S3 contains exactly half negative samples We set agents weights follows w 1 w 2 029 w 3 042 RI c S 021 However SRD mechanism selects agent 3 Observe ﬁrst RI c S 079 r 0422 029202920422 concept c probability cid9 cid8 0511 Therefore RI SRDS S 051 079 049 021 05058 24 021 24 r proves lower bound Proposition B6 There individually realizable dataset S agents cid8 cid9 RI SRDS S 139 r cid17 cid16 2 2 n r Example B7 We C c c Let S1 S2 positive S3 negative We set agents weights follows w 1 w 2 0363 w 3 0274 We RI c S 0763 r RI c S 0274 The SRD mechanism selects agent 3 probability 02742 036320363202742 cid8 0222 Therefore cid9 RI SRDS S 0222 0763 0778 0274 0382 139 0274 139 r proves lower bound realizable case Theorem 48 cid2 The following hold Mechanism 6 wrt proﬁle F αw cid3 2 2 n CRD approximation ratio αw 1 3 2 n S individually realizable approximation ratio αw 2 1 2 1 n R Meir et al Artiﬁcial Intelligence 186 2012 123156 Proof We ﬁrst prove αw cid3 2 2 n Let gx 1 22x Note g convex Also cid2 iI w 1 cid3 1 n cid7 iI w 2 cid3 1 αw 1 cid7 cid8 p iI cid16cid7 cid2 g iI 1 2 21n cid2 cid7 iI w cid17 1 2 2w cid7 iI w gw w w 1 cid2 2 2 iI w 2 Jensens inequality 29 149 29 αw cid3 2 2 n We denote d f f cid8 number disagreements f f classiﬁer C closest c C minimizes dc f For c holds cid8 f ci denote labels agent RI c F w iRic F w idc f cid7 iI cid7 iI cid3 2d f c piRI ci F Note dci c cid7 cid9 cid8 RI CRDF F iI cid7 cid16cid7 iI cid7 jcid7i cid16cid7 cid3 closer f ci c cid7 cid7 pi w jdci f j iI jI cid17 pi w jdci f j pi w idci f cid8 d cid8 ci c cid9 cid8 c d f j cid9cid9 pi w j pi w id cid8 c f cid17 cid9 iI cid7 jcid7i cid8 ci c pid cid9cid7 w j cid7 cid7 cid9 cid8 c f j pi w jd iI αw cid3 αw cid7 iI cid7 iI w 21 w cid8 2d w 2 cid7 jcid7i cid8 ci c d jI iI cid9 1 w cid7 cid9 cid8 c f j w jd f c cid8 c f j w jd jI cid9 αw 1RI cid9 cid8 c F cid8 c f j w jd cid9cid7 pi iI cid7 jI cid9 αw 1 cid16 cid3 3 2 n jI cid17 r Now realizable case f ci cid7 cid7 cid8 RI CRDF F piRI ci F cid9 cid7 w jd f f j pi cid7 cid7 pi w jd f f j iI jcid7i iI cid7 cid7 iI f c cid9 jI cid8 d cid8 cid8 d cid9cid9 f j c pi w j TI jcid7i iI cid7 cid8 iI cid7 pid f c cid8 pid f c cid9cid7 cid7 cid7 cid8 pi w jd f j c cid9 w j jcid7i cid9 1 w iI cid7 jcid7i cid8 r pi F w id cid8 cid3 iI αw cid7 iI w 21 w cid8 d f c cid9 iI 1 w r F cid9cid9 cid8 f c cid7 pi w id f c iI cid9 150 R Meir et al Artiﬁcial Intelligence 186 2012 123156 cid7 cid8 w id f c cid9 r F cid8 pi w id f c cid9 cid7 iI cid8 αw 2 αw 2 cid16 cid3 iI r F r F cid7 cid9 pi w id f c cid17 1 r F cid3 iI cid16 2 1 n cid17 r F αw 2 completes proof cid2 Theorem 49 The following hold Mechanism 7 βw cid3 1 2 n RRD approximation ratio 4 3 worst case S individually realizable approximation ratio 1 βw 2 2 n Proof Let qx 1 12x Note q convex cid7 cid7 cid7 1 βw cid8 p 1 1 2w w iqw iI w cid17 iI iI cid16cid7 cid2 q iI w w 1 cid2 1 2 iI w 2 Jensens inequality cid2 1 1 21n 29 βw cid3 1 2 n For upper bound need following Lemma B8 For I pi cid3 2w Proof Let hx x 12x Note h convex Thus Jensens inequality cid16 1 n 1 cid7 jcid7i hw j cid2 h 1 n 1 cid7 jcid7i cid17 cid16 cid17 w j h 1 w n 1 Next cid7 jI w j 1 2w j w 1 2w cid7 jcid7i w j 1 2w j cid16 w 1 2w cid17 30 cid7 jcid7i hw j cid2 w 1 2w w 1 2w cid2 w 1 2w n 1h n 1 1 w n 1 1w n1 1 2 1w n1 w Eq 30 w 1 2w 1 w 1 2 1w n1 12 1 2 12 n1 1 2w 1 2 n2 n1 w 1 2w 1 2 Therefore pi βw p cid8 cid16cid7 jI w j 1 2w j cid171 w 1 2w 1 w 12w 1 2 w 1 2w w w 12w 2 w w w 1 2 2w cid2 R Meir et al Artiﬁcial Intelligence 186 2012 123156 151 We bound risk RRD We skip steps detailed upper bound proof CRD mechanism cid8 RI RRDS S cid9 cid7 piRI ci S cid7 cid8 ci c pid cid9cid7 w j cid7 cid7 cid9 cid8 c f j pi w jd iI βw cid3 βw βw βw cid7 iI cid7 iI cid7 cid16 iI cid7 d w 1 2w 2w i1 w 1 2w w i1 2w 1 2w cid9 cid8 d w id f c βw iI S βw cid8 w id f c cid7 iI cid9 βwr cid7 cid3 2 iI cid8 ci c cid9 jcid7i 1 w iI cid8 c w jd jI f j cid7 jI cid9cid7 pi iI cid8 cid9 f c r S cid8 d f c cid9 w cid17 cid9 cid8 d f c r S 1 2w cid8 cid9 r S d f c cid7 iI w 1 2w cid9 cid8 d f c w 1 2w 2r S 2r S 2r iI S 4r S S cid3 cid7 cid8 pid f c cid9 2r S r iI In realizable case recall f ci cid7 cid7 cid7 cid8 RI RRDS S piRI ci S pi cid9 w jd f f j cid7 cid7 pi w jd f f j iI cid7 cid7 iI f c cid9 jI cid8 d cid8 cid8 d cid9cid9 f j c pi w j TI iI jcid7i jcid7i iI cid7 cid8 pid f c cid9cid7 cid7 cid7 cid8 pi w jd f j c cid9 w j iI cid7 cid8 pid f c cid9 jcid7i 1 w iI cid7 jcid7i cid8 r pi S w id cid9cid9 cid8 f c cid3 cid7 iI w 1 2w cid8 w id f c cid9 r S iI βw βw βw cid7 iI cid7 iI cid7 iI cid9 iI 1 w βw f c cid8 d f c cid9 r S cid8 d w 1 2w w i1 2w 1 2w cid9 cid8 w id f c r S βwr cid17 r S cid16 2 2 n 1 βwr S cid3 S r S proves upper bound cid2 B2 Proofs generalization results Section 43 As Section 33 distinguish notation Rc risk wrt ﬁxed input distribution ˆRc S empirical risk wrt sampled dataset S For proofs section need following fundamental result machine learning theory Theorem B9 Vapnik Chervonenkis 41 Let m st m V C cid22 log cid16 cid17 V C cid22δ Let S dataset contains m data points sampled iid distribution D cid4X Y Then probability 1 δ c C cid8cid11 cid11 cid11Rc ˆRc S cid11 cid2 cid9 31 V C constant depends concept class C distribution D property problem 152 R Meir et al Artiﬁcial Intelligence 186 2012 123156 V C known VCdimension C introduced 41 We formal deﬁnition V C However detailed accessible overviews VC theory PAC learning abundant example 12 While V C large inﬁnite cases known ﬁnite commonly concept classes linear classiﬁers Theorem 410 Assume agents cid2truthful let C concept class bounded dimension For cid2 0 k n rmin cid2 polynomial 1 cid2 lnn st k data points sampled expected risk Mechanism 8 3 2 Proof Let S cid5 X Y Xcid6 partial dataset agent true private labels Denote Q Q icid2 event c C cid8cid11 cid11 cid11Ric ˆRic S cid11 cid2 cid9 32 We emphasize Q property S random samples S event Q holds hold Our proof sketch reformulated follows Q happens simultaneously high probability b Whenever Q occurs agent report truthfully cid2truthfulness assumption c When Q occur risk Mechanism 8 bounded 3 2 d Otherwise risk high small effect total expected risk n rmin cid2 Let δ 0 As S iid random sample Di Theorem B9 Q occurs probability 1 δ provided samples Also union bound probability event j Q j 1 δcid8 δ δcid8 n Lemma B10 If Q occurs agent gain 2cid2 lying Proof Assume agent selected mechanism trivially true We denote ˆci C concept returned mechanism reports truthfully ˆci argmincC Let c ˆRic S cid8 C cid8 c Riˆci Ri cid9 cid8 Riˆci ˆRiˆci S ˆRiˆci S Ri cid3 cid11 cid11 cid11 cid11Riˆci ˆRiˆci S cid11 cid11 ˆRi cid2 cid2 2cid2 32 S cid2 cid8 c cid9 cid8 cid8 c Ri cid9 cid8 cid8 c cid9cid11 cid11 cid8 ˆci empirically optimal By Lemma B10 gain 2cid2 reporting c cid8 By taking cid2 cid2cid8 2 complete proof parts b Now c assume Q Thus Lemma B10 cid2truthfulness assumption agents truthful proof sketch S S Lemma B11 If S holds Q occurs I cid9 cid8 c S S ˆRI S argmincC cid3 rmin cid2 ˆRI c S c Proof For c C Ric ˆRic S cid2 Eq 32 Therefore cid7 ˆRic S cid3 ˆRI c S ˆRic S S S cid8 c cid7 ˆRI pi pi cid9 cid7 cid8 pi Ric cid2 cid9 RI c cid2 particular ˆRI c iI S S rmin cid2 cid2 iI iI We bound expected risk mechanism We denote cMS random classiﬁer returned Mechanism 6 input S For random variable A EM A S expectation A random dictator selection ﬁxed dataset S Similarly ES A expectation A random sampling given selected dictator cid10 RI ES cid9 cid11 cid8 cid11 j Q j cMS cid8 cid10 cid10 EM cMS RI changing order randomizations cid9 cid11 cid11 j Q j cid12 cid11 cid11 j Q j cid8 cMS cid9 cid11 cid11 S EM cid10 RI ES cid12cid12 E cid10 cid12 cid12 R Meir et al Artiﬁcial Intelligence 186 2012 123156 153 cid10 RI cid8 ˆciS cid9 cid11 cid11 j Q j cid12 piES cid10 ˆRI cid8 ˆciS S cid9 cid2 cid11 cid11 j Q j cid12 piES 32 cid7 iI cid7 iI cid7 cid3 cid10 ˆRI cid8 ˆciS S piES cid8 cMS S 3 2 n 3 2 n cid17 ˆRI cid17 cid12 cid9 cid11 cid11 j Q j cid12cid12 cid2 cid2 cid9 cid11 cid11 j Q j cid9 cid11 cid8 cid11 j Q j c cid19cid19 S S cid19cid19 rmin cid2 cid16 cid11 cid11 j Q j cid17 3 2 n iI EM cid10 ES cid18 cid10 ˆRI cid18cid16 cid3 EM ES cid18cid16 cid18 ES cid17 cid3 EM cid16 3 2 n rmin cid2 cid2 cid3 rmin 4cid2 cid2 Theorem 48 cid2 Lemma B11 cid16 3 2 n cid17 rmin cid2cid8 proves c proof sketch Finally bound total risk mechanism taking d account cid10 RI RI cid15CRD E cid9cid12 cid12cid12 cid12 cid8 cid10 cMS RI Pr j Q jES ES cid10 cid10 EM RI cid10 EM Pr j Q jES cid9 cid11 cid10 cid11 S RI cid9 cid11 cid10 cid11 S RI cid17 cid8 cMS cid8 cMS cid10 EM cid3 ES cid10 EM ES cid16 3 2 n cid9 cid11 cid10 cid8 cid11 S EM cMS cid12 cid11 cid9 cid11 cid8 cid11 j Q j cid11 S cMS cid12 cid11 cid9 cid11 cid8 cid10 cid11 j Q j cid11 S cMS RI cid12 cid11 cid12 cid11 j Q j cid8 1 δ cid12 cid11 cid11 j Q j cid8 δ cid17 cid16 3 2 rmin cid2cid8cid8 n cid8 cid2cid8 rmin δ cid3 cid12 cid12 agents truthful case required cid2 We conclude computing exact number samples needed Mechanism 8 cid2truthfulness assumption Lemma B12 If k 64 V C cid22 log256 V C n cid23 RI cid15CRD cid3 rmin cid2 cid17 cid16 3 2 n Proof From Theorem B9 S j V C V C cid22δ PrQ jcid2 δ union bound holds 4n unfolding residues proof cid22 log cid9 cid8 cid2 Q j nδ cid8 Pr j I Q j cid9cid9 cid8 cid2 cid3 cid7 jI Taking cid2 cid2 8 δ cid2 cid17 cid16 3 2 n cid16 3 2 n rmin cid3 cid17 rmin 4 cid16 3 2 n cid17 rmin 4cid2 2nδ cid2 8 2n cid16 3 2 n cid2 4n cid17 rmin cid2 RI cid15CRD cid3 cid16 log V C cid22 cid17 V C cid22δ V C cid282 log cid16 V C cid282cid24n cid17 64 V C cid22 cid16 log 256 cid17 V C n cid23 cid2 Theorem 411 Assume agents purely rational let C concept class bounded dimension For cid2 0 n rmin cid2 k polynomial 1 cid2 st k data points sampled expected risk Mechanism 8 3 2 154 R Meir et al Artiﬁcial Intelligence 186 2012 123156 cid2 n Proof Note private distributions D1 D2 Dn induce global joint distribution input space deﬁned D i1 w iDi We alternatively deﬁne rmin minimal risk concept wrt distribution D rmin infcC Ex yDcx cid7 y We like analyze outcome Mechanism 8 compare empirical risk actual risk However technical problem directly S deﬁned proof Theorem 410 sampled iid Di D In order prove theorem introduce virtual mechanism Mechanism 9 This mechanism generates truthful dataset S iid sample joint distribution D Mechanism 9 The Virtual Learning Mechanism Sample k data points iid D X assume dataset X Mechanism 8 point x X Select agent probability w Add cid5x j Y xcid6 S end return c S ermS The output Mechanism 9 c S best concept C real dataset S Note S iid sample D actual mechanism Mechanism 8 access real labels Y term virtual mechanism We denote T T cid2 event cid8 c cid9 S RI rmin 2cid2 33 Similarly Q j previous proof T property S occurrence depends sampling Lemma B13 If k kδ cid2 large PrT δ Proof This immediate corollary Theorem B9 As c sampled iid D c C ˆRI cid2 cid3 ˆRI c S cid2 RI c cid2 cid2 cid9 S S S cid8 c cid8 c RI cid9 argmincC ˆRI c S C bounded dimension S holds probability 1 δ large k In particular cid8 PrT Pr RI cid8 c cid9 S cid9 rmin 2cid2 1 δ cid2 It clear approximate c S mechanism access S For purpose deﬁne new concept class C X C projection C X Formally let H X H class dichotomies X h st h X 19 C X C H X In words C X contains dichotomies X allowed C Denote S dataset reported labels agent ˆci best concept wrt dataset That S C X ˆc j C X agents This case S cid5x Y ixcid6x X ˆci argmincC S S j labeled versions set X Thus classiﬁer computed wrt S S j dichotomy X S minimizes function depends labels We deﬁne c argmincC X RI c Clearly RI c cid3 RI c member C X Thus T occurs inequality ˆRic S Observe c S c RI c rmin 2cid2 34 holds directly special case 33 We approximate c generalized variant Theorem 48 appears appendix Consider proﬁle F cid5D1 Dncid6 This valid proﬁle shared inputs concept c C Rc Rc F private global risk alike Lemma B14 Let j selected dictator ˆc j argmin cCX R jc argmin cCX R jc F ˆR jc S j Since assumed j purely rational label examples X Proof Recall ˆc j argmincC way minimize private risk From way Mechanism 8 works concepts C X returned c C X labeling X st c returned This labeling Y c simply x X yx cx Thus argmincC X R jc best agent j hope achieve reporting appropriate labels Y j cid2 19 Put differently H X partition H equivalence classes according outcome X X R Meir et al Artiﬁcial Intelligence 186 2012 123156 We apply Theorem 48 F class C X getting cid17 cid17 p jRI ˆc j F cid3 r F RI c cid16 3 2 n cid16 3 2 n cid7 jI 155 35 To holds observe left term expected risk Mechanism 6 input proﬁle F concept class C X c globally optimal classiﬁer input We emphasize Eq 35 holds independently sampling selection Finally bound risk result concept RI cid15CRD ES cid12cid12 cid11 cid11 S cid10 EM PrT ES cid10 cid10 EM cid3 ES RI cM cid18cid7 cid8 ˆc jS cid10 RI cM cid11 cid10 cid10 cid11 S EM RI cM cid12 cid11 cid11 cid12 cid11 T cid11 S cid9 cid11 cid11 T ES w jRI cid12 PrT ES cid12 cid11 cid11 T δ 1 Lemma B13 cid19 cid10 EM cid10 RI cM cid11 cid11 S cid12 cid11 cid11 T cid12 cid18cid16 jI 3 2 n cid17 ES cid17 cid3 ES cid16 3 2 n cid16 3 2 n δ cid19 δ 35 cid17 RI cid8 cS cid9 cid11 cid11 T cid10 rmin 2cid2 cid12 cid11 cid11 T δ rmin 2cid2 δ 34 cid17 rmin 6cid2 δ cid16 3 2 n By taking δ cid2 cid2cid8 7 proof complete Similarly Lemma B12 follows Theorem B9 taking cid16 cid17 k 49 V C cid22 log 343 V C cid23 suﬃcient Mechanism 8 work pure rationality assumption cid2 References 1 N Alon M Feldman AD Procaccia M Tennenholtz Strategyproof approximation minimax networks Mathematics Operations Re search 35 3 2010 513526 2 N Alon M Feldman AD Procaccia M Tennenholtz Walking circles Discrete Mathematics 310 23 2010 34323435 3 I Ashlagi F Fischer I Kash AD Procaccia Mix match Proceedings 11th ACM Conference Electronic Commerce ACMEC 2010 pp 305314 4 MF Balcan A Blum JD Hartline Y Mansour Mechanism design machine learning Proceedings 46th Symposium Foundations Computer Science FOCS 2005 pp 605614 5 JP Barthèlemy B Leclerc B Monjardet On use ordered sets problems comparison consensus classiﬁcations Journal Classiﬁca tion 3 1986 187224 6 NH Bshouty N Eiron E Kushilevitz PAC learning nasty noise Theoretical Computer Science 288 2 2002 255275 7 V Conitzer T Sandholm Complexity mechanism design Proceedings 18th Annual Conference Uncertainty Artiﬁcial Intelligence UAI 2002 pp 103110 8 V Conitzer T Sandholm An algorithm automatically designing deterministic mechanisms payments Proceedings 3rd Interna tional Joint Conference Autonomous Agents MultiAgent Systems AAMAS 2004 pp 128135 9 N Dalvi P Domingos Mausam S Sanghai D Verma Adversarial classiﬁcation Proceedings 10th International Conference Knowledge Discovery Data Mining KDD 2004 pp 99108 10 O Dekel F Fischer AD Procaccia Incentive compatible regression learning Journal Computer System Sciences 76 2010 759777 11 O Dekel O Shamir Good learners evil teachers Proceedings 26th International Conference Machine Learning ICML 2009 pp 216 223 12 L Devroye L Györﬁ G Lugosi A Probabilistic Theory Pattern Recognition SpringerVerlag New York 1997 13 E Dokow M Feldman R Meir I Nehama Mechanism design discrete lines cycles Proceedings 13th ACM Conference Electronic Commerce ACMEC 2012 forthcoming 14 E Dokow R Holzman Aggregation binary evaluations Journal Economic Theory 145 2010 495511 15 S Dughmi A Ghosh Truthful assignment money Proceedings 11th ACM Conference Electronic Commerce ACMEC 2010 pp 325334 16 P Fishburn A Rubinstein Aggregation equivalence relations Journal Classiﬁcation 3 1986 6165 17 A Gibbard Manipulation voting schemes Econometrica 41 1973 587602 18 M Guo V Conitzer Strategyproof allocation multiple items agents payments priors Proceedings 9th Interna tional Joint Conference Autonomous Agents MultiAgent Systems AAMAS 2010 pp 881888 19 M Guo V Conitzer D Reeves Competitive repeated allocation payments Proceedings 5th International Workshop Internet Network Economics WINE 2009 pp 244255 156 R Meir et al Artiﬁcial Intelligence 186 2012 123156 20 P Harrenstein MM Weerdt V Conitzer A qualitative Vickrey auction Proceedings 10th ACM Conference Electronic Commerce ACMEC 2009 pp 197206 21 MJ Kearns UV Vazirani An Introduction Computational Learning Theory MIT Press 1994 22 J Kemeny Mathematics numbers Daedalus 88 1959 577591 23 E Koutsoupias Scheduling payments Proceedings 4th Symposium Algorithmic Game Theory SAGT 2011 press 24 B Leclerc Eﬃcient binary consensus functions transitively valued relations Mathematical Social Sciences 8 1984 4561 25 D Lowd C Meek P Domingos Foundations adversarial machine learning Manuscript 2007 26 P Lu X Sun Y Wang ZA Zhu Asymptotically optimal strategyproof mechanisms twofacility games Proceedings 11th ACM Conference Electronic Commerce ACMEC 2010 pp 315324 27 R Meir S Almagor A Michaely JS Rosenschein Tight bounds strategyproof classiﬁcation Proceedings 10th International Joint Conference Autonomous Agents MultiAgent Systems AAMAS Taipei Taiwan 2011 pp 319326 28 R Meir AD Procaccia JS Rosenschein Strategyproof classiﬁcation constant hypotheses A tale functions Proceedings 23rd AAAI Conference Artiﬁcial Intelligence AAAI 2008 pp 126131 29 R Meir AD Procaccia JS Rosenschein On limits dictatorial classiﬁcation Proceedings 9th International Joint Conference Au tonomous Agents MultiAgent Systems AAMAS 2010 pp 609616 30 B Mirkin On problem reconciling partitions H Blalock Ed Quantitative Sociology International Perspectives Mathematical Statis tical Modeling Academic Press New York 1975 31 I Nehama Approximate judgement aggregation Proceedings 7th International Workshop Internet Network Economics WINE 2011 pp 302313 32 N Nisan Introduction mechanism design scientists N Nisan T Roughgarden E Tardos V Vazirani Eds Algorithmic Game Theory Cambridge University Press 2007 Chapter 9 33 A Othman E Budish T Sandholm Finding approximate competitive equilibria Eﬃcient fair course allocation Proceedings 9th Interna tional Joint Conference Autonomous Agents MultiAgent Systems AAMAS 2010 pp 873880 34 J Perote J PerotePeña Strategyproof estimators simple regression Mathematical Social Sciences 47 2004 153176 35 J PerotePeña J Perote The impossibility strategyproof clustering Economics Bulletin 4 23 2003 19 36 AD Procaccia M Tennenholtz Approximate mechanism design money Proceedings 10th ACM Conference Electronic Commerce ACMEC 2009 pp 177186 37 AD Procaccia A Zohar Y Peleg JS Rosenschein The learnability voting rules Artiﬁcial Intelligence 173 1213 2009 11331149 38 M Satterthwaite Strategyproofness Arrows conditions Existence correspondence theorems voting procedures social welfare func tions Journal Economic Theory 10 1975 187217 39 J Schummer RV Vohra Mechanism design money N Nisan T Roughgarden E Tardos V Vazirani Eds Algorithmic Game Theory Cambridge University Press 2007 Chapter 10 40 L Valiant A theory learnable Communications ACM 27 1984 41 VN Vapnik AY Chervonenkis On uniform convergence relative frequencies events probabilities Theory Probability Applications 16 2 1971 264280 42 R Wilson On theory aggregation Journal Economic Theory 10 1975 8999