Artiﬁcial Intelligence 246 2017 5385 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Hierarchical semiMarkov conditional random ﬁelds deep recursive sequential data Truyen Tran Center Pattern Recognition Data Analytics Deakin University Geelong Australia b Adobe Research Adobe USA Dinh Phung Hung Bui b Svetha Venkatesh r t c l e n f o b s t r c t Article history Received 20 January 2015 Received revised form 12 February 2017 Accepted 14 February 2017 Available online 24 February 2017 Keywords Deep nested sequential processes Hierarchical semiMarkov conditional random ﬁeld Partial labelling Constrained inference Numerical scaling 1 Introduction We present hierarchical semiMarkov conditional random ﬁeld HSCRF generalisation linearchain conditional random ﬁelds model deep nested Markov processes It parameterised conditional loglinear model polynomial time algorithms learning inference We derive algorithms partiallysupervised learning constrained inference We develop numerical scaling procedures handle overﬂow problem We depth HSCRF reduced semi Markov conditional random ﬁelds Finally demonstrate HSCRF applications recognising human activities daily living ADLs indoor surveillance cameras ii nounphrase chunking The HSCRF capable learning rich hierarchical models reasonable accuracy fully partially observed data cases 2017 Elsevier BV All rights reserved Modelling hierarchical depth complex stochastic processes important application domains In deep hier archy level abstraction lower level details 14 This paper studies recursively sequential processes level sequence node sequence decomposed subsequence ﬁner grain 2 Consider example frequent activity performed human eatbreakfast It include series speciﬁc activities like enterkitchen gotocupboard takecereal washdishes leavekitchen Each speciﬁc activity decomposed ﬁner details Similarly natural language processing NLP syntax trees inherently hierarchical In partial parsing task known nounphrase NP chunking 5 syntactic levels sentence nounphrases partofspeech POS tags In setting sentence sequence NPs nonNPs phrase sub sequence POS tags A popular approach deal hierarchical data build cascaded model level modelled separately output lower level input level right 6 For instance NP chunking approach ﬁrst builds POS tagger constructs chunker incorporates output tagger This approach suboptimal POS tagger takes information NPs chunker aware reasoning tagger In contrast nounphrase informative infer POS tags belonging phrase As result layered approach suffer socalled cascading error problem errors introduced lower layer propagate higher levels Corresponding author Email address truyentrandeakineduau T Tran httpdxdoiorg101016jartint201702003 00043702 2017 Elsevier BV All rights reserved 54 T Tran et al Artiﬁcial Intelligence 246 2017 5385 A holistic approach build joint representation levels Formally given data sequence z need model infer deep nested semantic x The main problem choose appropriate representation x inference eﬃcient An important class representation hierarchical hidden Markov model HHMM 2 An HHMM nested hidden Markov network HMM sense state sub HMM Although HMMs represent ﬁrstorder Markov processes HHMMs offer higherorder interaction HHMMs generative models joint distribution Prx z data generating distribution Prz x simpliﬁed eﬃcient inference semantic Prx z An alternative model discriminative distribution Prx z directly modelling data Prz This effective arbitrary longrange interdependent data features incorporated model The popular class probabilistic structured output methods conditional random ﬁelds CRFs 7 early models ﬂat Deep variants introduced past decade including dynamic CRFs DCRF 8 hierarchical CRFs 910 stacked CRFs 11 However methods require ﬁxed predeﬁned hierarchy suitable problems automatically inferred topologies To end construct novel discriminative model called Hierarchical SemiMarkov Conditional Random Field HSCRF1 The HSCRF offers nested semantic similar HHMM parameterised undirected loglinear model The HSCRF generalises linearchain CRFs 7 semiMarkov CRFs 13 To concrete let return NP chunking example The problem modelled threelevel HSCRF root represents sentence second level NP process level POS process The root processes conditioned sequence words sentence Under discriminative modelling rich contextual information simply encoded features including starting ending phrase phrase length distribution words falling inside phrase effectively encoded On hand encoding diﬃcult HHMMs We proceed address important issues First represent HSCRFs dynamic graphical model 14 effectively encodes hierarchical temporal semantics For parameter learning eﬃcient algorithm based Asymmetric InsideOutside 15 introduced For inference generalise Viterbi algorithm decode semantics observational sequence The common assumptions discriminative learning inference training data learning fully labelled test data inference labelled We propose relax assumptions training labels partially available Likewise labels given inference algorithm automatically adjust meet new constraints We demonstrate effectiveness HSCRFs applications segmenting labelling activities daily living ADLs indoor environment ii jointly modelling nounphrases partofspeeches shallow parsing Our experimental results ﬁrst application HSCRFs capable learning rich hierarchical activities good accuracy exhibit better performance compared DCRFs ﬂatCRFs Results partially supervised case demonstrate signiﬁcant reduction training labels results models perform reasonably We observing small labels signiﬁcantly increase accuracy decoding In shallow parsing HSCRFs achieve higher accuracy standard CRFbased techniques recent DCRFs To summarise paper claim following contributions Introducing novel Hierarchical SemiMarkov Conditional Random Field HSCRF model complex hierarchical nested Markovian processes discriminative framework Developing eﬃcient generalised Asymmetric InsideOutside AIO algorithm fullsupervised learning Generalising Viterbi algorithm decoding probable semantic labels structure given observational sequence Addressing problem partiallysupervised learning constrained inference Constructing numerical scaling algorithm prevent numerical overﬂow Demonstration applicability HSCRFs modelling human activities domain home video surveil lance shallow parsing English The rest paper organised follows Section 2 reviews Conditional Random Fields Hierarchical Hidden Markov Models Section 3 continues HSCRF model deﬁnition Section 4 deﬁnes building blocks required common ference tasks Section 5 presents generalised Viterbi algorithm Parameterisation estimation follow Section 6 Learning inference partially available labels addressed Section 7 Section 8 presents method numerical scaling prevent numerical overﬂow Section 9 documents experimental results Section 11 concludes paper 1 Preliminary version published NIPS08 12 T Tran et al Artiﬁcial Intelligence 246 2017 5385 55 Table 1 Notations paper Notation Description cid3 cid3 xdd j edd j ζ ds j c j t τ d r s u v w Rdsz j π ds ui Adsz uvi Edsz ui cid5 ζ z Sd cid6ds j ˆcid6ds j cid7ds j ˆcid7ds j αds j u λds j u δ I ψϕ cid3 starting time ending time j inclusive cid3 Subset state variables level d level d Subset ending indicators level d level d Set state variables ending indicators sub model rooted sd level d spanning substring j Contextual clique Time indices Set ending time indices τ d ed starting time ending time j inclusive 1 State Statepersistence potential state s level d spanning j Initialisation potential state s level d time initialising substate u Transition level d time state u v parent s Ending potential state z level d time receiving return control child u The global potential particular conﬁguration ζ given observation sequence z The set state symbols level d The symmetric inside mass state s level d spanning substring j The symmetric inside mass state s level d spanning substring j The symmetric outside mass state s level d spanning substring j The symmetric outside mass state s level d spanning substring j The asymmetric inside mass parent state s level d starting having childstate u returns control parent transits new childstate j The asymmetric outside mass counterpart asymmetric inside mass αds Indicator functions Potential functions j u 2 Preliminaries This section presents foundations proposed HSCRF built conditional random ﬁelds hierarchical hidden Markov models For later reference deﬁne mathematical notations Table 1 21 Conditional random ﬁelds Denote G V E graph V set vertices E set edges Associated vertex state variable xi Let x joint state variable x xiiV Conditional random ﬁelds CRFs 7 deﬁne conditional distribution given observation z follows Prx z 1 Z z cid2 c φcxc z 1 cid4 cid3 c index cliques graph φcxc z nonnegative potential function deﬁned clique c Z z c φcxc z partition function Let x set observed state variables empirical distribution Q x w parameter vector x Learning CRFs typically maximising log likelihood w arg max w L w arg max w Q x log Prx z w cid5 x The gradient loglikelihood computed cid6 L w cid5 cid5 Q x log φcxc z cid5 Prxc z log φcxc z cid7 2 3 xc Thus inference needed CRF parameter estimation computation clique marginals Prxcz c x Typically CRFs parameterised loglinear models φcxc z expw fxc z f feature vector cid6 cid3 w weight vector Let Fx z c fxc z global feature Eq 3 written follows 56 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Fig 1 The state transition diagram HHMM L cid5 Q x cid6 cid5 fxc z x Q xF E PrxzF E c cid5 xc cid7 Prxc zfxc z 4 5 Thus gradientbased maximum likelihood learning loglinear setting boils estimating feature expecta tions known expected suﬃcient statistics ESS 211 Learning partial labels Let x v h v set visible variables h set hidden variables The incomplete loglikelihood gradient given cid5 L Q x log Prv z cid5 x Q x log cid5 h Prv h z Q xlog Z v z log Z z x cid5 x Z v z cid3 h cid4 c φcvc hc z The gradient reads L Ehvz Fv h z Exz Fx z cid5 cid5 cid5 Q x Prhc v zfvc hc z x c hc 212 Sequential models Prxc zfxc z cid5 xc 6 7 The popular form CRFs linearchain order n n typically small integer This allows fast estimation sequence clique marginals Prxc z forwardbackward procedure time complexity O length T K states T K n1 cid12 cid13 A generalisation chainCRF semiMarkov CRF SemiCRF 13 ﬁrstorder Markovian segments non L Markovian states A forwardbackward procedure adapted accordingly time complexity O maximum segment length In Appendix C SemiCRF special case proposed HSCRF T L K 2 cid12 cid13 22 Hierarchical hidden Markov models Hierarchical HMMs generalisation HMMs 16 state HHMM subHHMM Thus HHMM nested Markov chain In temporal evolution HHMM child Markov chain terminates returns control parent Nothing terminated child chain carried forward Thus parent state abstracts belonging Upon receiving return control parent transits new parent terminates Fig 1 illustrates state transition diagram twolevel HHMM At level parent states A B Parent A children ch A 1 2 3 B chB 4 5 6 7 At level transitions A B normal directed Markov chain Under parent transitions child states depend direct parent A B There special ending states represented shaded nodes Fig 1 signify termination Markov chains At time step child Markov chain child emit observational symbol shown The temporal evolution HHMM represented dynamic Bayesian network DBN ﬁrst 17 Fig 2 depicts DBN structure 3 levels Associated state ending indicator signify termination 0 state state Denote xd t continues xd xd t transits new state transits There hierarchical t t state ending indicator level d time t respectively When ed t t ed t1 And ed 1 state xd xd t T Tran et al Artiﬁcial Intelligence 246 2017 5385 57 Fig 2 Dynamic Bayesian network representation HHMMs Fig 3 The shared topological structure cid3 cid3 consistency rules strictly observed Whenever state persists ed t persist ed t ed t cid3 d Similarly state ends ed t 0 d cid3 d 1 d Inference learning HHMMs follow InsideOutside algorithm probabilistic contextfree grammars Overall algorithm OK 3 D T 3 time complexity K maximum size state space level D depth model T model length When representing DBN stack states x1D collapsed megastate big HMM fore inference carried OK 2D T time This eﬃcient shallow model D small problematic deep model D large 0 states 1 states end t 3 Model deﬁnition In section deﬁne general HSCRF hierarchically nested Markov process Speciﬁc loglinear parameterisation presented Sec 61 In HSCRF like generative counterpart HHMM Sec 22 parent state embeds child Markov chain states turn contain child Markov chains The family relation deﬁned topology state hierarchy depth D 1 The model set states Sd level d 1 D Sd 1K d For state sd Sd 1 d D topological structure deﬁnes set children chsd Sd1 Conversely child sd1 set parents pasd1 Sd Unlike original HHMMs child states belong exclusively parent HSCRFs allow arbitrary sharing children parents For example Fig 3 chs1 1 1 2 3 pas3 1 1 2 4 This helps avoid explosive number substates D large leading fewer parameters possibly training data time The shared topology investigated context HHMMs 15 The temporal evolution nested Markov processes sequence length T operates follows As soon state created level d D initialises child state level d 1 The initialisation continues downward reaching level As soon child process level d 1 ends returns control parent level d case d 1 parent transits new parent state returns grandparent level d 1 In hierarchical nesting lifespan child process belongs exclusively lifespan parent For example parent j time persists time j At time parent initialises child state sd1 process level d starts new state sd continues ends time k j child state transits new child state sd1 k1 The child process exits time j returning control parent sd j transit new parent state sd j1l end j returning control grandparent level d 1 j Upon receiving control parent state sd We formally specify nested Markov processes Let introduce multilevel temporal graphical model length T D levels starting 1 D Fig 4 At level d 1 D time index 1 T 58 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Fig 4 The multilevel temporal model Adapted 18 Table 2 Hierarchical constraints The state persists course evolution e1 0 When state ﬁnishes descendants ﬁnish ed When state persists ancestors persist ed When state transits parent remain unchanged ed The states persists e D All states end T e1D 1 1 T 1 1T 1 T 1 implies ed1D 1 0 implies e1d1 1 ed1 0 0 Fig 5 An example statepersistence subgraph Adapted 18 Fig 6 Subgraphs state transition initialisation b ending c Adapted 18 node representing state variable xd signifying state xd speciﬁc constraints value assignment ending indicators summarised Table 2 ending indicator ed ends persists The nesting nature HSCRFs realised imposing Sd 1 2 K d Associated xd Thus speciﬁc value assignments ending indicators provide contexts realise evolution model states hierarchical vertical temporal horizontal directions Each context level associated state variables form contextual clique identify contextual clique types Statepersistence This corresponds life time state given level Fig 5 Speciﬁcally given context contextual clique speciﬁes lifespan j state cid15 1 0 0 1 cid17 cid16 σ persistd j xd j c cid14 c ed i1 j s xd j Statetransition This corresponds state level d 2 D time transiting new state Fig 6a Speciﬁcally xd1 i1 xd contextual clique speciﬁes transition cid14 ed1 cid15 1 ii1 c cid17 cid16 given context c xd σ transitd 0 ed i1 time parent xd1 i1 xd T Tran et al Artiﬁcial Intelligence 246 2017 5385 59 Table 3 Shorthands contextual clique potentials cid17 cid17 z z cid17 cid16 Rdsz j Adsz uvi π dsz ui Edsz ui ψ ψ ψ ψ cid16 σ persistd j cid16 σ transitd σ initd cid16 σ endd z z cid17 s xd j s xd1 i1 u xd v xd i1 s xd s xd u xd1 u xd1 Stateinitialisation This corresponds state level d 1 D 1 initialising new child state level d 1 time contextual clique speciﬁes σ initd c cid16 cid17 xd1 xd Fig 6b Speciﬁcally given context c initialisation time parent xd cid18 ed i1 cid19 1 child xd1 Stateending This corresponds state level d 1 D 1 ending time Fig 6c Speciﬁcally given context time contextual clique speciﬁes ending xd σ endd xd1 xd c cid17 cid16 cid18 cid19 c 1 ed child xd1 cid12 1T e1D x1D In HSCRF interested conditional setting entire state variables ending indicators 1T conditioned observational sequence z For example NLP observation sequence words state variables partofspeech tags phrases cid13 cid12 To capture correlation variables conditioning deﬁne positive potential function ψσ z contextual clique σ Table 3 shows notations potentials correspond contextual clique types identiﬁed Details potential speciﬁcation described Sec 61 Let ζ denote set variables satisﬁes set hierarchical constraints listed Table 2 Let τ d denote ordered set ending time indices level d τ d ed 1 The joint potential deﬁned conﬁguration product contextual clique potentials ending time indices 1 T semantic levels d 1 D 1T e1D x1D 1T cid13 cid5ζ z cid20 cid2 cid2 d1D ikik1τ d Rdsz ik1ik1 The conditional distribution given cid21 cid2 cid20 cid2 d1D1 ikτ d1ik τ d Ad1sz uvik cid21cid20 cid2 ikτ d1 π dsz uik1 cid21cid20 cid2 ikτ d1 cid21 Edsz uik 8 9 Prζ z 1 Z z ζ cid5ζ z partition function normalisation cid5ζ z cid3 Z z In follows omit z clarity implicitly use partition function Z potential cid5 It noted unconditional formulation single Z data instances In conditional setting Z z data instance z Remarks The temporal model HSCRFs presented standard graphical model 14 connectivity clique structures ﬁxed The potentials deﬁned ontheﬂy depending context assignments ending indicators Although model topology identical shared structure HHMMs 15 unrolled temporal representation undirected graph model distribution formulated discriminative way Furthermore state persistence potentials capture duration information available dynamic DBN representation HHMMs 17 The HSCRF potentials ﬁrst appear resemble clique templates discriminative relational Markov networks 19 They different cliques HSCRFs dynamic contextdependent 4 Asymmetric insideoutside algorithm This section describes core inference engine called Asymmetric InsideOutside AIO algorithm The AIO algorithm computes building blocks needed inference learning tasks including partition function timespeciﬁc marginals feature expectations 41 Building blocks conditional independence 411 Contextual Markov blankets In subsection deﬁne elements building blocks inference learning These building blocks identiﬁed given corresponding boundaries Let introduce types boundaries contextual symmetric asymmetric Markov blankets 60 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Fig 7 Symmetric Markov blanket b asymmetric Markov blanket Deﬁnition 1 A symmetric Markov blanket level d state s starting ending j following set cid16 cid15ds j xd j s edD i1 1 edD j 1 ed j1 0 cid17 Deﬁnition 2 Let cid15ds j symmetric Markov blanket deﬁne ζ ds j ζ ds j follows cid17 cid16 ζ ds j ζ ds j ζ ed1D xd1D j j1 cid16 cid17 j cid15ds ζ ds j 10 11 12 s Further deﬁne subject xd j cid16 cid17 ˆζ ds j j cid15ds ζ ds j ˆζ ds j cid16 cid17 j cid15ds ζ ds j Fig 7a shows example symmetric Markov blanket represented doublearrowed line Deﬁnition 3 A asymmetric Markov blanket level d parent state s starting child state u ending j following set cid14 cid16ds j u xd j s xd1 j u edD i1 1 ed1D j 1 ed j1 0 cid15 Deﬁnition 4 Let cid16ds j u asymmetric Markov blanket deﬁne ζ ds j u ζ ds j cid16 ζ ds j u ζ ds j u ζ j j1 xd2D xd1D cid16 j u cid16ds ζ ds ed1D j1 cid17 j u cid17 subject xd j s xd1 j u Further deﬁne cid16 cid16 ˆζ ds j u ds ˆζ j u j u cid16ds ζ ds j u cid16ds ζ ds cid17 j u cid17 j u u follows 13 14 15 16 17 Fig 7b shows example asymmetric Markov blanket represented arrowed line Remark The concepts contextual Markov blankets Markov blankets short different traditional Markov random ﬁelds Bayesian networks speciﬁc assignments subset variables collection variables 412 Conditional independence Given deﬁnitions following propositions conditional independence T Tran et al Artiﬁcial Intelligence 246 2017 5385 Proposition 1 ζ ds cid16 j ζ ds ζ ds j Pr j conditionally independent given cid15ds j ζ ds j cid17 cid17 cid16 cid16 cid17 cid15ds j Pr ζ ds j cid15ds j Pr ζ ds j cid15ds j This proposition gives rise following factorisation cid17 cid16 cid17 cid16 cid17 cid16 cid16 cid17 cid16 cid17 Prζ Pr cid15ds j Pr j ζ ds ζ ds j cid15ds j Pr cid15ds j Pr ζ ds j cid15ds j Pr ζ ds j cid15ds j Proposition 2 ζ ds cid16 j u ζ ds ζ ds Pr j u ζ ds j u conditionally independent given cid16ds j u j u cid16ds cid17 j u cid16 j u cid16ds ζ ds cid17 j u Pr cid16 Pr j u cid16ds ζ ds cid17 j u The following factorisation consequence Proposition 2 Prζ Pr cid16 Pr cid16 cid16ds cid17 j u cid17 j u cid16ds Pr Pr cid16 ζ ds j u ζ ds j cid16 j u cid16ds ζ ds cid17 u cid16ds j u cid17 cid16 j u ζ ds j Pr u cid16ds cid17 j u The proofs Propositions 1 2 given Appendix A1 61 18 19 20 21 cid16 413 Symmetric insideoutside masses j cid15ds ζ ds j ζ ds From Eq 12 ζ j cid14 cid15 cid14 ds ˆζ cid15ds cid5 cid5 j j j group local potentials Eq 8 cid14 ˆζ ds parts cid5 By grouping mean multiply local potentials belonging j certain way group local potentials belonging model Eq 8 Note ˆζ ds j contains cid15ds cid14 j group cid5 j separates ζ ds j Since cid15ds cid15 cid14 cid5 ζ ds cid15ds j ˆζ ds j cid17 cid15 cid15 cid15 cid15 cid14 By deﬁnition statepersistence clique potential Fig 3 cid5 cid14 The holds cid5 cid15 cid15ds j ds ˆζ j Rds j Thus Eq 8 replaced cid14 cid5ζ cid5 cid15 ˆζ ds j Rds j cid5 cid15 cid14 ˆζ ds j cid14 ˆζ There special cases 1 d 1 cid5 1 T This factorisation plays important role eﬃcient inference cid14 ˆζ Ds 1 s S 1 2 d D cid5 ii 1s 1T cid15 We know deﬁne quantity called symmetric inside mass cid6ds j called symmetric outside mass cid7ds j 22 cid15 1 s S D Deﬁnition 5 Given symmetric Markov blanket cid15ds deﬁned j symmetric inside mass cid6ds j symmetric outside mass cid7ds j cid6ds j cid7ds j cid5 cid14 cid15 cid5 ˆζ ds j ζ ds j cid5 cid15 cid14 cid5 ˆζ ds j ζ ds j As special cases cid71s 1T symmetric inside mass ˆcid6ds 23 24 1 1 T s S D For later use let introduce 1 s S 1 cid6Ds ii j symmetric outside mass ˆcid7ds j ˆcid6ds j Rds j cid6ds j ˆcid7ds j Rds j cid7ds j In rest paper clear context use inside mass shorthand symmetric inside mass outside mass symmetric outside mass fullinside mass fullsymmetric inside mass fulloutside mass fullsymmetric outside mass Thus Eq 22 partition function computed fullinside mass level d 1 1T Rds 1T cid5ζ ˆζ 1s 1T R1s 1T cid6ds Z cid5 cid5 cid5 cid5 cid5 cid15 cid14 ζ 1s 1T sS 1 sS 1 ζ cid5 sS 1 ˆcid61s 1T 25 62 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Table 4 Computing partition function fullinside mass fulloutside mass Z Z Z cid3 cid3 cid3 sS 1 sS D sSd ˆcid61s 1T ˆcid7Ds ii cid3 1 T jtT cid6ds cid3 i1t j cid7ds j Rds j t 1 T d 2 D 1 With similar derivation partition function computed fulloutside mass level d D Z cid5 sS D ˆcid7Ds ii 1 T In fact prove general way compute Z Appendix B cid5 cid5 cid5 Z sSd i1t jtT cid6ds j cid7ds j Rds j 26 27 t 1 T d 2 D 1 These relations summarised Table 4 Given fact ζ ds j separated rest variables symmetric Markov blanket cid15ds j Proposi tion 3 Proposition 3 The following relations hold cid17 cid16 cid15 cid14 Pr ζ ds j cid15ds j cid16 Pr ζ ds j cid15ds j cid16 Pr cid15ds j cid17 cid17 1 cid6ds j 1 cid7ds j cid5 cid5 ˆζ ds j cid15 cid14 ˆζ ds j 1 Z cid6ds j Rds j cid7ds j The proof proposition given Appendix A2 414 Asymmetric insideoutside masses 28 29 30 Recall introduced concept asymmetric Markov blanket cid16ds Let group local contextual clique potentials associated ζ ds j u cid16ds Similarly group local potentials associated ζ ds j u cid16ds cid14 j u joint potential cid5 cid14 ˆζ cid5 cid15 u ds j includes statepersistence potential Rds j j u separates ζ ds cid14 j u joint potential cid5 j u ζ ds j u cid15 ˆζ ds j u Note cid15 u ˆζ ds j Deﬁnition 6 Given asymmetric Markov blanket cid16ds mass λds j u asymmetric inside mass αds j u asymmetric outside j u deﬁned follows cid15 cid5 ˆζ ds j u αds j u cid5 cid14 λds j u ζ ds j u cid5 ζ ds j u cid14 cid5 ˆζ ds j cid15 u 31 32 The relationship asymmetric outside mass asymmetric inside mass analogous outside inside masses However small difference asymmetric outside mass owns segment xd j s associated statepersistence potential Rds j whilst outside mass cid7d js T Tran et al Artiﬁcial Intelligence 246 2017 5385 63 Fig 8 Decomposition respect symmetricasymmetric Markov blankets 42 Computing inside masses In subsection recursively compute pair inside mass asymmetric inside mass The key idea exploit decomposition asymmetric Markov blanket As shown Fig 8 outer asymmetric Markov blanket decomposed subasymmetric Markov blanket symmetric blanket 421 Computing asymmetric inside mass inside mass Assume asymmetric Markov blanket cid16ds j u child u starts t j ends j xd1 t j u ed1 t j1 0 ed1D1 t1 1 Let consider cases t t Case 1 For t denote v xd1 t1 We smaller blankets cid16ds associated child u xd1 t j parent s Fig 8 illustrates blanket decomposition The assignment ζ ds asymmetric blanket cid16ds j u symmetric blanket cid15d1u it1v associated child v ending t 1 t j j u decomposed cid16 ζ ds j u it1v ζ d1u ζ ds t j cid14 Thus joint potential cid5 cid15 ˆζ ds j u u xd1 cid15 ˆζ ds j u cid14 cid15 ˆζ ds it1v ˆζ d1u t j cid5 cid5 cid5 cid14 cid14 t j ed t1 j1 cid17 0 ed1D t1 1 factorised follows cid15 vut1 Rd1u Ad1s t j 33 34 cid15 The transition potential Ad1s persistence potential Rd1u t j vut1 enabled context c context c Case 2 For t asymmetric blanket cid16ds cid16 assignment ˆζ ds j u activated Thus ˆζ d1u j ed i1 1 ed j1 cid14 0 ed1 ed t1 t1 1 ed1D 0 ed1D cid14 ed1 t j1 1 xd t 1 xd1 t j it1v exist t 1 We following decompositions cid17 0 stateinitialisation potential π ds ui In context c state cid18 ed i1 s xd1 t1 cid15 v xd1 cid19 1 u u t1 t j cid14 cid15 ˆζ ds j u cid5 π ds ui cid5 cid15 cid14 ˆζ d1u j Rd1u j 35 Substituting Eqs 34 35 Eq 31 fact t value interval j v value Sd1 following relation cid15 cid5 ˆζ ds it1v αds j u cid5 cid5 cid5 cid5 cid14 cid5 ti1 j cid5 vSd1 cid5 ti1 j vSd1 ζ d1u ζ ds it1v t j it1v ˆcid6d1u αds t j cid15 cid14 ˆζ d1u t j vut1 Rd1u Ad1s t j cid5 cid15 cid14 ˆζ d1u j Rd1u j π ds ui cid5 ζ d1u j Ad1s vut1 ˆcid6d1u j π ds ui 36 As asymmetric inside mass α plays role forward message starting starting time ending time j There recursion asymmetric inside mass ending time j computed asymmetric inside masses ending time t 1 t 1 j There special cases asymmetric inside mass 1 j ii u ˆcid6d1s αds ii π ds ui 37 64 T Tran et al Artiﬁcial Intelligence 246 2017 5385 2 d D 1 sum index t Eq 36 allowed level D inside mass spans single index We following instead cid5 α D1s j u α D1s j1 v ˆcid6Du j j A Ds vu j1 vSd1 cid5 vSd1 α D1s j1 vR Du j j A Ds vu j1 422 Computing inside mass asymmetric inside mass 38 Notice relationship asymmetric Markov blanket cid16ds cid17 1 parent s ends j cid16ds When ed cid16 j ˆζ ds j u u xd1 ζ ds ζ ds j j cid15 cid14 ˆζ ds j u cid16 ˆζ ds j u ed ˆζ ds j Eds u j cid5 j cid14 cid5 cid15 j 1 u xd1 j j u cid15ds cid17 j u xd1 j j u symmetric blanket cid15ds j d D Then decompositions These lead factorisation 39 stateending potential Eds u j rewritten activated context c cid14 ed j cid15 1 Thus inside mass Eq 23 cid5 cid5 cid6ds j cid5 ˆζ ds j uEds u j uSd1 cid5 uSd1 cid5 uSd1 ζ ds j u Eds u j cid5 cid5 ˆζ ds j u ζ ds j u u jαds Eds j u 40 This equation holds d D When d D set cid6Ds ii 1 j T 1 s S D 1 T d 1 ensure Remark Eqs 36 37 38 40 specify leftright bottomup algorithm compute inside asymmet ric inside masses Initially level cid6Ds 1 1 T s S D Pseudocode dynamic programming ii algorithm compute inside asymmetric inside masses partition function given Algorithm 1 Algorithm 1 Computing set insideasymmetric inside masses partition function Input D T potential function values Output partition function Z cid61s 1T s S 1cid6Ds ii cid6ds j d 2 D 1 s Sd 1 j T j u d 1 D 1 u Sd1 1 j T αds 1 1 T s S D Initialise cid6Ds ii s S D 1 T At level dD1 For 1 2 T j 1 T Compute α D1s Compute cid6D1s j j u Eq 38 Eq 40 EndFor The main recursion loops bottomup forward For d D 2 D 3 1 For 1 2 T j 1 T Compute αds Compute αds Compute cid6ds ii u Eq 37 If j j u Eq 36 If j j Eq 40 If d 1 EndFor EndFor Compute Z Eq 25 T Tran et al Artiﬁcial Intelligence 246 2017 5385 65 43 Computing outside masses In subsection recursively compute symmetric outside mass asymmetric outside mass We use blanket decomposition Section 42 However time view reversed interested quantities outside blankets For example outside inner symmetric Markov blanket Fig 8 exists outer asymmetric blanket subasymmetric blanket left 431 Computing asymmetric outside mass outside mass Let examine variables ζ ds j u associated asymmetric Markov blanket cid16ds 1 j T Deﬁnition 4 For j T assume exists outer asymmetric Markov blanket cid16ds v Sd1 t j 1 T symmetric Markov blanket cid15d1v v ˆζ d1v decomposition ˆζ j1t xd1 cid14 cid14 cid15 leads following factorisation j1t right cid16ds u cid14 u ds ˆζ cid16 cid17 j j u d 1 D 1 v j u Given blankets cid5 ˆζ ds j cid5 ˆζ ds ˆζ d1v j1t j1t Ad1s Rd1v uv j cid15 v cid5 ds j cid15 u 41 The state transition potential Ad1s uv j Rd1v 1 ed1 j1t In addition exists special case state s ends j We decomposition ˆζ enabled context c context c 0 ed1 0 ed1 cid14 ed1 cid15 1 j1t1 t j j cid14 ed j cid15 1 state persistence potential cid16 ˆζ ds j u ds j u xd1 j cid17 42 following factorisation cid15 cid14 cid14 cid15 u cid5 ˆζ ds j cid5 ˆζ ds j j Eds Rds u j The ending potential Eds u j appears context c cid14 ed j cid15 1 s ends j Now relax assumption t v allow receive possible values t j T v Sd1 Thus replace Eq 32 λds j u cid5 cid5 cid5 cid5 cid14 cid5 vSd1 cid5 t j1T cid5 vSd1 t j1T ζ d1v ζ ds v j1t v ˆcid6d1v λds cid14 cid15 v cid5 ˆζ ds ˆζ d1v j1t cid15 j1t Ad1s Rd1v uv j cid5 cid15 cid14 cid5 ˆζ ds j j Eds Rds u j ζ ds j u j1t Ad1s uv j ˆcid7ds j Eds u j 43 d 2 D 2 1 j T Thus λds j u thought message passed backward j T j Here asymmetric outside mass ending j computed asymmetric outside masses ending t t j 1 T There special cases At level d 1 λds j u deﬁned 1 second term RHS Eq 43 included 1 j T At second lowest level d D 1 sum t Eq 43 ˆcid6Dv j1t deﬁned t j 1 We following relation instead λD1s j u cid5 vS D λD1s j1 v ˆcid6Dv j1 j1 A Ds uv j ˆcid7D1s j E D1s u j 44 432 Computing outside mass asymmetric outside mass Given symmetric Markov blanket cid15d1u d 1 D 1 assume exists asymmetric Markov blanket cid16ds t j u parent level d t 1 Clearly t 1 1 exists subasymmetric Markov blanket cid16ds ti1v See Fig 8 illustration j Let consider cases t t Case 1 For t enables decomposition ˆζ cid16 d1u j ˆζ ds t j u ˆζ ds ti1v u xd1 j cid17 factorisation cid15 cid14 cid5 ˆζ d1u j cid14 ˆζ ds t j cid15 u cid5 cid14 cid15 ˆζ ds ti1v cid5 Ads vui1 The state transition potential Ads vui1 activated context c cid14 ed i1 0 ed1 i1 cid15 1 leads following 45 66 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Table 5 Summary basic building blocks computed Section 42 43 cid61s cid6ds cid6Ds αds αds 1T cid71s j cid7ds ii cid7Ds ii 1 j u λds j u λds 1T s S 1 j d 2 D 1 s Sd 1 j T 1 T s S D 1 j u d 1 s S 1 u S 2 j 1 T j u d 2 D 1 s Sd u Sd1 1 j T Case 2 For t decomposition reduces ˆζ cid16 d1u j ˆζ ds j u u xd1 j cid17 leads following factorisation cid15 cid14 cid5 ˆζ d1u j cid14 ˆζ ds j cid15 u π ds ui cid5 cid19 1 46 cid18 ed i1 The stateinitialisation potential π ds ui plays role context c However decompositions factorisations hold given assumption speciﬁc values s Sd v Sd1 t 1 Without information possibilities account Substituting relations Eq 24 cid5 cid5 cid5 cid5 cid5 cid7d1u j cid14 cid15 ˆζ ds t j u cid14 cid15 ˆζ ds ti1v cid5 cid5 Ad1s vui1 cid5 cid5 cid14 cid5 ˆζ ds j cid15 u π ds ui sSd cid5 vSd1 cid5 t1i1 ζ ds t j u cid5 ζ ds ti1v ti1v Ad1s αds vui1 λds t j u sSd t1i1 vSd1 sSd ζ ds j u cid5 sSd j uπ ds λds ui 47 d 2 D 2 There special cases The ﬁrst base case d 0 cid71s 1T d 1 ﬁx index t 1 asymmetric inside mass αds RHS included 1 asymmetric outside mass λds j 1 s S 1 In second case ti1 deﬁned t 1 Also second term j u sense In second case d 1 D Remark Eqs 43 44 47 recursive topdown outsidein approach compute symmetricasymmetric outside masses We start d 1 cid71s 1 s S 1 proceed downward d D The 1T pseudocode given Algorithm 2 Table 5 summarises quantities computed Section 42 43 Algorithm 2 Computing set outsideasymmetric outside masses Input D T potential function values insideasymmetric inside masses Output outsideasymmetric outside masses Initialise cid71s uT s S 1 u S 2 1T main recursive loops topdown insideout For d 1 2 D 1 1T u E 1s 1 λ1s For 1 2 T j T T 1 Compute asymmetric outside mass λds Compute outside mass cid7ds j Eq 47 j u Eqs 43 44 EndFor EndFor Algorithm 3 summarises AIO algorithm computing building blocks partition function Algorithm 3 The AIO algorithm Input D T potential function values Output building blocks partition function Compute insideasymmetric inside masses algorithm Algorithm 1 Compute outsideasymmetric outside masses algorithm Algorithm 2 T Tran et al Artiﬁcial Intelligence 246 2017 5385 67 Table 6 Notations section Notation Description cid6maxds j ˆcid6maxds j αmaxds j cid6argds j αargds j I d u u j The optimal potential function subset variables ζ ds j The version cid6maxds The optimal potential function subset variables ζ ds The optimal child ud1 The optimal child vd1 t1 transits ud1 t j The set optimal segments level d s j j u time index t 5 The generalised Viterbi algorithm The MAP assignment ζ M A P arg max ζ Prζ z arg max ζ cid5ζ z The process computing MAP assignment similar computing partition function This similarity comes relation sumproduct maxproduct algorithm generalisation Viterbi algorithm 20 fact insideasymmetric inside procedures described Section 42 essentially sumproduct We need convert summations corresponding maximisations The algorithm twostep procedure In ﬁrst step maximum joint potential computed local maximum states ending indicators saved way These states ending indicators maintained bookkeeper In second step decode best assignment backtracking saved local maximum states We use contextual decompositions factorisations Section 42 Notations This section abuse notations uses slight modiﬁcations notations rest paper See Table 6 reference We ﬁrst step 51 Computing maximum joint potential maximal states time indices cid14 For clarity let drop notation z cid5ζ z As cid5ζ cid5 cid15 ˆζ 1s 1T R1s 1T s S 1 max ζ cid5ζ max sS 1 R1s 1T max ζ 1s 1T cid5 cid15 cid14 ˆζ 1s 1T Now subassignment ζ ds j cid14 cid15 ˆζ ds j cid5 max ζ ds j max uSd1 Eds u j max ζ ds j u cid5 cid14 cid15 ˆζ ds j u 1 1 D 1 Eq 39 leads 48 49 With slight abuse notation introduce cid6maxds j optimal potential function subset variables j αmaxds ζ ds j u optimal potential function subset variables ζ ds j u Deﬁnition 7 We deﬁne cid6maxds j αmaxds j cid15 u follows cid14 cid6maxds j ˆcid6maxds j αmaxds j cid5 ˆζ ds j max ζ ds j cid6maxds Rds j j cid15 cid14 ˆζ ds j u cid5 u max ζ ds j u 50 51 52 68 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Eqs 48 49 rewritten compactly cid15 cid14 ζ M A P cid5 cid6maxds j max sS 1 max uSd1 ˆcid6max1s 1T u jαmaxds Eds j u d 1 D 1 When d D simply set cid6maxDs From factorisation Eqs 34 35 ii 1 s S D 1 T cid14 cid15 ˆζ ds j u max cid5 max ζ ds j u max vSd1 max ti1 j Rd1u t j Ad1s vut1 max ζ ds it1v cid15 cid14 ˆζ d1u t j Rd1u j max ζ d1u j max ζ d1u t j cid5 cid14 cid15 ˆζ ds it1v cid5 cid14 ˆζ d1u j π ds ui cid5 cid15 cid28 cid29 αmaxds j u max max vSd1 max ti1 j αmaxds it1 v ˆcid6maxd1u t j Ads vut1 cid30 cid16 ˆcid6maxd1u j π d1s ui cid31 cid17 53 54 55 56 d 1 D 2 j For d D 1 scan index t interval 1 j maximum inside cid6maxDu t j deﬁned t j We following instead αmaxD1s j u max vS D αmaxD1s j1 v ˆcid6maxDu j j A Ds vu j1 There base case j context c cid18 ed i1 cid19 1 active αmaxds ii u ˆcid6maxd1u ii π ds ui 57 58 Of course interested maximum joint potentials optimal states time indices ending indicators We need bookkeepers hold quantities way With abuse notation let introduce symmetric inside bookkeeper cid6argds associated Eq 54 asymmetric inside bookkeeper αargds u associated Eqs 56 57 58 j j Deﬁnition 8 We deﬁne symmetric inside bookkeeper cid6argds j follows cid6argds j u arg maxuSd1 Eds u jαmaxds j u Similarly deﬁne asymmetric inside bookkeeper αargds j u associated Eq 56 d 1 D 2 αargds j u v t arg maxti1 jvSd1αmaxds it1 v ˆcid6maxd1u t j Ads vut1 maxvSd1ti1 j αmaxds it1 v ˆcid6maxd1u t j vut1 ˆcid6maxd1u Ads j π d1s ui j αargds j u undeﬁned For d D 1 αargds j u associated Eq 57 αargD1s j u arg maxvS D αmaxds j1 v ˆcid6maxDu j j Ads vu j1 59 60 61 62 The Eqs 53 54 56 57 58 provide recursive procedure compute maximum joint potential bottomup 1 s S D 1 T The procedure summarised leftright manner Initially set cid6maxDs Algorithm 4 ii Algorithm 4 Computing bookkeepers T Tran et al Artiﬁcial Intelligence 246 2017 5385 69 Input D T potential function values Output bookkeepers cid6arg1s 1T s S 1 1 j T cid6argds j αargds j d 2 D 1 s Sd cid6argDs u d 1 D 1 u Sd1 1 j T ii s S D 1 T Initialise cid6maxDs ii 1 1 T s S D At level dD1 For 1 2 T j 1 T Compute αmaxD1s Compute cid6maxD1s j u Eq 57 αargD1s Eq 54 cid6argD1s j j j Eq 59 u Eq 62 EndFor The main recursion loops bottomup forward For d D 2 D 3 1 For 1 2 T j 1 T If j Compute αmaxds ii u Eq 58 Else Compute αmaxds j u Eq 56 αargds ii u Eq 60 EndIf If d 1 Compute cid6maxds j Eq 54 cid6argds j Eq 59 EndIf EndFor EndFor Compute cid6max1s 1T Eq 54 cid6arg1s 1T Eq 59 52 Decoding MAP assignment The proceeding backtracking process opposite maxproduct Speciﬁcally start root proceed topdown rightleft manner The goal identify rightmost segment level Formally segment triple s j s segment label j start end time indices respectively From maximum inside cid6maxds level d identify best child u ending time j Eq 54 This gives rise maximum asymmetric inside αmaxds u Then seek best child v transits u parent s Eq 56 Since starting time t u identiﬁed ending time v t 1 We rightmost segment u t j level d 1 The procedure repeated reach starting time parent s The backtracking algorithm summarised Algorithm 5 j j Finally generalised Viterbi algorithm given Algorithm 6 Working logspace avoid numerical overﬂow With long sequence complex topology run problem numerical overﬂow numerical value maximum joint potential number representation machine To avoid work logspace instead monotonic property log function The equations logspace summarised Table 7 6 Parameter estimation In section tackle problem parameter estimation maximising conditional data likelihood Typically need parametric form deﬁned particular problem need numerical method optimisation task Here employ loglinear parameterisation commonly CRF setting Recall Section 21 estimating parameters loglinear models gradientbased methods requires computation feature expectation expected suﬃcient statistics ESS For HSCRFs need compute types ESS corresponding statepersistence statetransition stateinitialisation stateending 70 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Algorithm 5 Backtracking optimal assignment nested Markov blankets Input D T ﬁlled bookkeepers Output optimal assignment ζ M A P ˆcid6max1s arg maxsS 1 s 1T Initialise triple buckets I 1 s For d 1 2 D 1 For triple s j I d cid6argds j Let u For j 1 T I d d 2 D deﬁned Then If αargds u j αargds v t j Add triple v u t j I d1 Set j t 1 u v Else Add triple u j I d1 Break loop EndIf EndFor EndFor EndFor j bucket I d d 1 D For stored triple s create corresponding set variables xd j The joining sets optimal assignment ζ M A P 1 ed j ed s i1 1 ed j1 0 Algorithm 6 The generalised Viterbi algorithm Input D T potential function values Output optimal assignment ζ M A P Run bottomup discrete optimisation procedure described Algorithm 4 Run topdown backtracking procedure described Algorithm 5 Table 7 MAP equations logspace Logspace equations log cid6maxds j maxuSd1 log Eds u j log αmaxds j u log αmaxds j u max log ˆcid6maxd1u t j maxti1 j maxvSd1 log αmaxds log ˆcid6maxd1u j it1 log π d1s v vut1 ui log Ads log αmaxD1s j u maxvS D log αmaxD1s j1 log A Ds log ˆcid6maxDu j j vu j1 v log αmaxds ii u log ˆcid6maxd1u ii log π ds ui 61 Loglinear parameterisation Equations Eq 54 Eq 56 Eq 57 Eq 58 In HSCRF setting feature vector fd cid14 w σ σ z associated type contextual clique σ φσ d z Thus features active context corresponding contextual cliques appear cid6 exp cid15 σ σ z σ d fd For statepersistence contextual clique features incorporate stateduration start time end time j state Other feature types incorporate time index features triggered Speciﬁcally Rdsz j exp Adsz uvi exp π dsz ui exp Edsz ui exp cid14 w cid14 w cid14 w cid14 w cid6 cid6 cid15 σ persist j z cid15 z σ transit uv cid15 z cid15 z σ persistd fds σ transitd fds σ initd fds σ endd fds σ init u σ endu cid6 cid6 63 64 65 66 Denote Fd σ ζ z global feature sum active features fd given assignment ζ clique type σ Recall τ d ikm feature types given Eqs 6770 k1 set ending time indices ed ik σ z level d duration 1 T 1 The T Tran et al Artiﬁcial Intelligence 246 2017 5385 cid5 ikτ dk1 fds σ persist ik 1 ik1 z σ persist ζ z fds Fds σ persist 1 i1 z Fds σ transit uv ζ z cid5 ik τ d1ikτ d fds σ transit uv ik z Fds σ init u ζ z fds σ init uv 1 z Fds σ endu ζ z cid5 ikτ d fds σ enduv z cid5 ikτ d fds σ init uv ik 1 z Substituting global features potentials Eqs 8 9 obtain following loglinear model Prζ z 1 Z z exp w cid6 σ c Fσ c ζ z cid5 cC C persist transit init exit Again clarity presentation drop notion z implicitly assume quantity 62 ESS statepersistence features Recall Section 61 feature function statepersistence fds σ persist j active context cid15ds j ζ Thus Eq 67 rewritten cid5 cid5 Fds σ persist ζ fds σ persist jδ i1T jiT cid14 cid15ds j cid15 ζ 72 σ persist j active exists symmetric Markov The indicator function RHS ensures feature fds blanket cid15ds j assignment ζ Consider following expectation cid15 cid15cid15 cid14 cid5 E fds σ persist jδ ζ Prζ fds σ persist jδ cid14 cid15ds j ζ cid14 cid15ds j ζ 1 Z cid5 ζ cid5ζ fds σ persist jδ cid14 cid15ds j cid15 ζ Using factorisation Eq 22 rewrite cid14 E fds σ persist jδ cid15cid15 cid14 cid15ds j ζ 1 Z cid5 cid14 cid5 cid15 cid15 cid14 cid5 ˆζ ds j ˆζ ds j ζ j fds Rds σ persist jδ cid14 cid15ds j cid15 ζ Note elements inside sum RHS nonzeros assignment ζ respect persistent j cid15ds state sd cid15 cid14 j factorisation Eq 22 ζ ζ ds j Thus equation simpliﬁed j ζ ds cid15 cid15cid15 cid14 cid14 E fds σ persist jδ cid14 cid15ds j ζ 1 Z cid5 cid5 cid5 ζ ds j ζ ds j ˆζ ds j cid5 ˆζ ds j j fds Rds σ persist j 1 Z cid6ds j cid7ds j Rds j fds σ persist j Using Eq 72 obtain ESS statepersistence features cid14 cid15cid15 cid14 cid5 cid5 E fds σ persist jδ cid14 cid15ds j ζ cid15 F ds k ζ E i1T jiT cid5 cid5 1 Z i1T jiT cid6ds j cid7ds j Rds j fds σ persist j 71 67 68 69 70 71 73 74 75 76 77 78 There special cases 1 d 1 sum j ﬁx 1 j T 2 d D j 72 T Tran et al Artiﬁcial Intelligence 246 2017 5385 63 ESS transition features Recall Section 61 deﬁne fds child state ud ﬁnishes job time t transits child state vd parent sd1 sd1 running Thus Eq 68 rewritten t function active context ctransit 0 ed t σ transit uv t cid14 ed1 cid15 1 Fds σ transit uv ζ fds σ transit uv tδ cid5 t1T 1 cid14 ctransit ζ cid15 We consider following expectation cid14 ctransit ζ tδ cid5 cid15cid15 E cid14 fds σ transit uv Prζ fds σ transit uv cid14 ctransit ζ cid15 tδ ζ 1 Z cid5 ζ cid5ζ fds σ transit uv cid14 ctransit ζ cid15 tδ Assume parent s starts Since ed t 1 child v starts t 1 ends time later j t 1 We following decomposition conﬁguration ζ respects assumption cid16 ζ ˆζ d1s j v ˆζ d1s u ˆζ dv t1 j cid17 following factorisation joint potential ˆζ dv t1 j cid5ζ cid5 ˆζ d1s cid15 v cid15 u d1s j cid5 cid5 ˆζ cid14 cid14 cid14 cid15 t1 j Ads Rdv uvt The state persistent potential Rdv potential Ads t1 j uvt context ctransit enabled context c 82 83 cid14 ed t 1 ed t1 j1 0 ed j cid15 1 state transition Substituting factorisation RHS Eq 81 gives cid5 cid5 cid5 cid5 cid5 cid14 cid14 1 ˆζ d1s j cid15 v cid5 ˆζ d1s cid14 cid15 u cid5 ˆζ dv t1 j cid15 t1 j Ads Rdv uvt fds σ transit uv Z i1t jt1T ζ d1s simpliﬁed cid5 u ζ d1s j v ζ dv t1 j 1 Z cid5 cid5 i1t jt1T λd1s j vαd1s u ˆcid6dv t1 j Ads uvt fds σ transit uv t Using Eqs 79 84 obtain ESS statetransition features cid14 cid14 cid5 E fds σ transit uv cid15 tδctransit ζ E Fds σ transit uv 1 Z cid5 t1T 1 cid15 ζ t1T 1 uvt fds Ads σ transit uv cid5 cid5 t αd1s uλd1s j v ˆcid6dv t1 j When d 2 ﬁx 1 α1s j v deﬁned 1 i1t jt1T u λ1s 64 ESS initialisation features Recall Section 61 deﬁne fds d initialises child u level d 1 In event context cinit rewritten σ init u function level d triggered time parent s level activated 1 Thus Eq 69 cid19 1 cid18 ed i1 79 80 81 86 87 t 84 85 Fds σ init u ζ fds σ init u iδ cid5 i1T cid15 cid14 cinit ζ Now consider following feature expectation cid14 cid15 cinit ζ iδcinit ζ Prζ fds iδ cid5 E cid14 cid15 fds σ init u σ init u ζ 1 Z cid5 ζ cid5ζ fds σ init u iδ cid15 cid14 cinit ζ T Tran et al Artiﬁcial Intelligence 246 2017 5385 For assignment ζ enables fds following decomposition σ init u cid16 ζ ˆζ ds j u ˆζ d1u j cid17 context cinit activates emission s u feature function fds factorised σ init u cid5ζ cid5 cid14 ˆζ ds j cid14 cid15 u cid5 cid15 ˆζ d1u j Rd1u j π ds ui 73 88 Thus joint potential cid5ζ 89 Using factorisation noting elements summation RHS Eq 87 nonzeros assignments simplify RHS Eq 87 cid14 cid5 ˆζ ds j cid14 cid15 u cid5 ˆζ d1u j cid15 Rd1u j ui fds π ds σ init u 1 Z cid5 cid5 cid5 jiT ζ ds j u ζ d1u j 1 Z cid5 jiT j u ˆcid6d1u λds j ui fds π ds σ init u The summation j T fact know index Using Eqs 86 90 obtain ESS initialisation features cid14 E Fds σ init u cid15 ζ cid5 cid14 E fds σ init u cid15 iδcinit ζ i1T cid5 1 Z i1T ui fds π ds σ init u cid5 jiT j u ˆcid6d1u λds j 90 91 There special cases 1 d 1 scanning ﬁx 1 single initialisation beginning sequence 2 d D 1 ﬁx j ˆcid6Du j deﬁned j 65 ESS ending features Recall Section 61 deﬁne fds σ endu j function activated child u level d 1 returns control parent s level d time j This event enables context cend rewritten Fds σ endu ζ fds σ endu jδ cid5 j1T cid15 cid14 cend ζ Now consider following feature expectation cid14 cend ζ cid15 jδcend ζ Prζ fds jδ cid5 E cid14 cid15 fds σ endu σ endu ζ 1 Z cid5 ζ cid5ζ fds σ endu jδ cid15 cid14 cend ζ Assume state s starts ends j For assignment ζ enables fds tion following decomposition σ endu cid16 cid17 j u ˆζ ds ˆζ ds j ζ cid14 ed j cid15 1 Thus Eq 70 92 93 j respects assump This assignment context cend activates ending u Thus joint potential cid5ζ factorised cid5ζ cid5 cid14 cid15 ˆζ ds j cid14 cid15 ˆζ ds j u cid5 j Eds Rds u j Substituting factorisation summation RHS Eq 93 yields cid15 ˆζ ds j u j Eds Rds j αds u jfds j ˆcid7ds j uEds cid5 cid5 cid5 cid5 cid5 cid5 ˆζ cid14 cid15 cid14 ds j σ endu u jfds σ endu j i1 j ζ ds j ζ ds j u i1 j 94 95 96 74 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Using Eqs 92 96 obtain ESS exiting features cid14 E Fds σ endu cid15 ζ cid5 cid14 E fds σ endu jδed i1 cid15 ζ j1T cid5 1 Z j1T u jfds Eds σ endu j cid5 i1 j ˆcid7ds j αds j u 97 There special case d 1 scanning j ﬁx 1 j T 7 Partial labels learning inference So far assumed training data fully labelled testing data labels In section extend AIO handle cases assumptions hold Speciﬁcally happen training data completely labelled possibly lack labelling resources In case learning algorithm robust handle missing labels On hand inference partially obtain high quality labels external sources This requires inference algorithm responsive data 71 The constrained AIO algorithm In section consider general case ζ ϑ h ϑ visible set labels h hidden set Since HSCRF exponential model shares computation required general CRFs Eqs 6 7 We compute quantities partial logpartition function Z ϑ z partition function Z z constrained ESS EhϑzFϑ h z free ESS Eζ zFζ z The partition function free ESS computed Sections 4 6 respectively This section describes quantities Let set visible labels ϑ x e x visible set state variables e visible set ending indicators The basic idea modify procedures computing building blocks cid6ds j u address constraints imposed labels For example cid6ds implies state s level d starts persists till j terminating j Then labels xd cid10 s k j seen causing assumption inconsistent k cid6ds j zero Therefore general computation building block multiplied identity function enforces consistency labels required constraints computation block As example consider computation cid6ds j αds The symmetric inside mass cid6ds consistent following conditions satisﬁed j j αds j u 1 If state labels xd 2 If label ending indicator ed 3 If label ending indicator ed 4 If ending indicator ed k level d interval j xd i1 ed 1 k k j 1 ed j labelled ed 1 i1 k k j s 0 cid15 These conditions captured following identity function cid14 ed i1 cid14 xd ki j s When labels observed Eq 40 replaced cid14 ed ki j1 0 cid14 cid6ds j cid14 ed j 1 δ 1 cid15 cid15 cid15 δ δ δ I cid6ds j I cid14 cid6ds j cid15 cid29 cid5 cid30 j uEds αds u j cid15 98 99 uSd1 Note need explicitly enforce state consistency summation u bottomup leftright computation αds j u computed contributes sum consistent Analogously asymmetric inside mass αds j u consistent following conditions satisﬁed 1 The ﬁrst conditions symmetric inside mass cid6ds 2 If state level d time j labelled u 3 If ending indicator ed1 labelled ed1 1 j hold j j These conditions captured identity function cid15 cid14 αds j u I cid14 xd ki j s cid15 δ cid14 ed i1 cid15 1 δ cid14 ed ki j1 0 cid15 cid14 xd1 j δ cid15 u δ cid14 ed1 j cid15 1 100 δ T Tran et al Artiﬁcial Intelligence 246 2017 5385 75 Thus Eq 36 cid6 cid15 cid14 αds j u αds j u I jcid5 cid5 ki1 vSd1 ik1v ˆcid6d1u αds k j Ads vuk1 ˆcid6d1u j π d1s ui 101 cid7 k j Note need explicitly enforce state consistency summation v time consistency j u cid6d1u summation k bottomup computation αds computed contribute sum consistent Finally constrained partition function Z ϑ z computed Eq 25 given inside mass consistent observations Other building blocks symmetric outside mass cid7ds j cid6ds cid14 j complementary share d s j indicator function I analogous way Since cid7ds applied Similarly pair asymmetric inside mass αds j u asymmetric outside mass λds cid14 share d s j u indicator function I applied cid15 αds j u Once constrained building blocks computed calculate constrained ESS Section 6 modiﬁcations The difference need replace partition function Z z strained version Z ϑ z j asymmetric outside mass λds j u complementary j u computed cid15 cid6ds j 72 The constrained Viterbi algorithm Recall Generalised Viterbi Algorithm described Section 5 want ﬁnd probable conﬁguration ζ M A P arg maxζ Prζ z When variables ϑ ζ labelled necessary estimate The task estimate probable conﬁguration hidden variables h given labels hM A P arg max h Prh ϑ z arg max h cid5h ϑ z It turns constrained MAP estimation identical standard MAP respect labelled variables ϑ Since Viterbi algorithm maxproduct version AIO constrained Viterbi modiﬁed u manner constrained AIO Section 71 Speciﬁcally auxiliary quantities cid6maxs need maintain set indicator functions ensures consistency labels Eqs 98 99 αmaxs j j cid15 cid15 cid14 cid6maxds j I δ cid6maxds j I cid14 xd ki j s cid15 cid29 cid14 cid6maxds j cid14 ed i1 δ cid15 1 δ cid14 ed ki j1 0 cid30 cid15 cid14 ed j δ cid15 1 max uSd1 αmaxds j uEds u j Likewise modiﬁcations Eq 100 Eq 101 respectively cid15 cid15 cid15 cid14 αmaxds j cid15 u I cid14 xd ki j s δ 1 δ cid14 ed ki j1 0 cid14 xd1 j cid15 u δ cid14 ed1 j cid15 1 δ δ cid14 ed i1 cid28 cid14 αmaxds j u I αmaxds j cid15 u max max ki1 j max vSd1 ik1 v ˆcid6maxd1u αmaxds k j Ads vuk1 cid31 ˆcid6maxd1u j π d1s ui 102 103 Other tasks Viterbi algorithm including bookkeeping backtracking identical described Section 5 73 Complexity analysis The complexity constrained AIO constrained Viterbi upper bound OT 3 labels given It lower bound OT ending indicators known model reduces standard treestructured graphical model In general complexity decreases labels available expect subcubic time behaviour Learning requires constrained ESSes free ESSes computed Regardless labels free ESSes require cubic time Thus labels overall computation increase slightly 76 T Tran et al Artiﬁcial Intelligence 246 2017 5385 8 Numerical scaling In previous sections derived AIObased inference learning algorithms unconstrained strained models The quantities computed algorithms like insideoutside masses involve summation exponentially positive potentials The potentials estimated data upperbounded causing mag nitude masses increase exponentially fast sequence length T The magnitude goes numerical capacity machines moderate T In section present scaling method reduce numerical overﬂow problem 81 Scaling symmetricasymmetric inside masses Let revisit Eq 40 If scale asymmetric inside mass αds j u factor κ j 1 αcid3 ds j u αds j u κ j 104 symmetric inside mass cid6ds j scaled factor Similarly Eq 36 αds j u jcid5 cid5 ti1 vSd1 it1v ˆcid6d1u αds t j Ads vut1 ˆcid6d1u j π ds ui t j cid6d1u t j ˆcid6d1u addition set recursive relations Eqs 36 40 reduction level cid6Ds reduction symmetric inside mass cid6ds t 1 j reduced κ j αds j reduced factor In j j result cid6d1u t j Rd1u t j j asymmetric inside mass αds reduced factor κi 1 1 j quantities cid6ds j u d D factor 1 j αds 1 j u reduced factor Suppose cid6Ds ii cid4 i1 κi That j ˆcid6 cid3 ds 1 j αcid3 ds 1 j u ˆcid6ds 1 jcid4 j i1 κi αds 1 j u cid4 j i1 κi It follows immediately Eq 25 partition function scaled factor cid4 T i1 κi cid3 Z cid5 sS 1 ˆcid6 cid3 1s 1T cid4 Z T j1 κ j 105 106 107 cid3 1s ˆcid6 1T B1s partition function computed cid3 1s 1T cid6 1T Clearly deal log quantity avoid numerical overﬂow Thus log logZ log cid5 sS 1 ˆcid6 cid3 1s 1T Tcid5 j1 log κ j 108 cid6 cid3 1s 1T scaled appropriately One question choose set meaningful scaling factors κ jT 1 The simplest way choose relatively large number scaling factors making right choice straightforward Here natural way Assume chosen scaling factors κi j1 Using original Eqs 36 37 38 d 2 D subcomponents scaled appropriately compute partiallyscaled inside mass cid6 asymmetric inside mass αcid3cid3 ds cid5 u d 1 D 1 1 j Then scaling factor time j computed cid3cid3 ds j j 1 109 κ j αcid3cid3 1s 1 j u su The step rescale partiallyscaled variables T Tran et al Artiﬁcial Intelligence 246 2017 5385 77 cid6 αcid3cid3 ds j κ j cid3cid3 ds j κ j cid3cid3 Ds j j κ j cid6 αcid3 ds j u cid6 cid3 ds j cid6 cid3 Ds j j 1 j u s Sd d 1 D 1 s Sd d 2 D 1 s S D 110 111 112 82 Scaling symmetricasymmetric outside masses In similar fashion work set factors derivation symmetricasymmetric outside masses masses solely depend inside masses building blocks In words scaling inside masses compute scaled outside masses directly set equations described Section 43 The algorithm summarised Algorithm 7 Note order performing loops case different Algorithm 1 Algorithm 7 Scaling algorithm avoid numerical overﬂow Input D T contextual potentials Output Scaled insideasymmetric inside masses outsideasymmetric outside masses For j 1 2 T Compute αds Compute κ j Eq 109 Rescale α1s For 1 2 j 1 j u d 1 D 1 Eqs 36 37 38 1 j u Eq 110 For d 2 3 D 1 Rescale αds Rescale cid6ds j u Eq 110 j Eq 111 EndFor EndFor Rescale cid6Ds j j Eq 112 EndFor Compute true logpartition function Eq 108 Compute outsideasymmetric outside masses scaled insideasymmetric inside masses instead original insideasymmetric inside Eqs 43 47 9 Applications In section present experimental results demonstrate capacity proposed HSCRFs applications activity recognition shallow parsing 91 Recognising indoor activities In experiment evaluate HSCRFs relatively small dataset domain indoor video surveillance The task recognise indoor trajectories activities person noisy positions extracted video The data captured 21 subsequently evaluate DCRFs 22 The raw data consists 90 sequences noisy coordinates Each time step manually annotated labels complex primitive activities There complex activities preparingshortmeal havingsnack preparingnormalmeal 12 primitive activities listed Table 8 As 22 data split sets equal size training testing respectively We assume statespeciﬁc features initialisation transition exiting indicator functions For data associations embedded statepersistence potentials level use feature set 22 X Y coordinates X Y velocities speed At second level duration state use average velocities vector positions visited duration To encode duration state persistence potentials employ suﬃcient statistics gamma distribution features fks cid6t Is logcid6t fk1s cid6t Iscid6t 78 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Table 8 Primitive activities 21 No 1 2 3 4 5 6 Activity DoorCupboard CupboardFridge FridgeDining chair Dining chairDoor DoorTV chair TV chairCupboard No 7 8 9 10 11 12 Activity FridgeTV chair TV chairDoor FridgeStove StoveDining chair FridgeDoor Dining chairFridge Table 9 Accuracy fully observed labels left 50 partially observed PO labels right Alg HSCRF DCRF ﬂatCRF d 2 100 965 d 3 939 897 826 Alg POHSCRF POCRF d 2 802 d 3 904 835 Fig 9 The topology learned data Fig 10 The state transition model learned data Primitive states duplicated clarity They shared complex states For learning labels sequence provided fully case fully observed state data partially case missing state data For testing level d time t count error predicted state groundtruth 911 Fully supervised learning Firstly examine fully observed case HSCRF compared DCRF data levels ﬂatCRF level Table 9 left half shows multilevel models signiﬁcantly outperform ﬂat model b HSCRF outperforms DCRF We test ability model learn hierarchical topology state transitions Using loglinear parameter isation described Section 61 parentchild relationship topology statetransitions captured corresponding parameters Only positive parameters This state features nonnegative negative parameters mean probabilities transitions small exponential compared positive ones For transition second level complex activity level obtain negative entries This clearly matches training data sequence belongs complex activities With method able construct correct hierarchical topology Fig 9 The state transition model presented Fig 10 There wrong transition state 12 state 10 presented training data The rest correct 912 Partially supervised learning Next consider partiallysupervised learning 50 startend times segment segment labels observed second level All ending indicators known level The results reported Table 9 T Tran et al Artiﬁcial Intelligence 246 2017 5385 79 Fig 11 Performance constrained maxproduct algorithm function available information labelstartend time right half As seen 50 state labels state startend times observed model learned performing accuracy 802 904 levels 2 3 respectively 913 Prediction partial labelling We consider issue partial observed labels decoding improve prediction accuracy poorly estimated models We extract parameters 10th iteration fully observed data case The labels provided random time indexes Fig 11a shows decoding accuracy function available state labels It interesting observe moderate observed labels 2040 causes accuracy rate considerably 92 POS tagging nounphrase chunking In experiment apply HSCRF task nounphrase chunking The data CoNLL2000 shared task 5 8926 English sentences Wall Street Journal corpus training 2012 sentences testing Each word preprocessed sentence labelled labels partofspeech POS nounphrase NP There 48 POS different labels 3 NP labels BNP beginning nounphrase INP inside nounphrase O Each nounphrase generally word To reduce computational burden reduce POS tagset 5 groups noun verb adjective adverb Since HSCRFs explicitly indicate node beginning segment NP label set reduced NP nounphrase O The POS tags actually output Brills tagger 23 NPs manually labelled We extract raw features text way similar 8 However consider limited vocabulary extracted training data select words 3 occurrences This reduces vocabulary feature size signiﬁcantly We use bigrams similar selection criteria Furthermore use contextual window 5 instead 7 8 This setting gives rise 32K raw features The model feature factorised f xc z Ixcgcz Ixc binary function assignment clique variables xc gcz raw features We build HSCRF topology 3 levels root dummy node second level 2 NP states level 5 POS states For comparison implement DCRF simple sequential CRF SCRF semiMarkov CRF SemiCRF 13 The DCRF grid structure depth 2 modelling NP process POS process Since state spaces relatively small able run exact inference DCRF collapsing NP POS state spaces combined state space size 3 5 15 The SCRF SemiCRF model NP process taking POS tags input The raw feature set DCRF identical HSCRF However set shared SCRF SemiCRF little elaborate takes POS tags account 8 Although HSCRF SemiCRF capable modelling arbitrary segment duration use simple ex ponential distribution processed sequentially eﬃcient For learning use simple online stochastic gradient ascent method shown work relatively fast CRFs 24 At test time SCRF SemiCRF able use Brills POS tags input fair DCRF HSCRF predict labels inference Instead POS tags DCRF HSCRF perform constrained inference predict NP labels This boosts performance multilevel models signiﬁcantly The performance models depicted Fig 12 interested prediction nounphrases data Brills POS tags Without Brills POS tags given test time HSCRF DCRF perform worse SCRF trained POS tags This surprising Brills POS tags given case SCRF However POS tags given test time HSCRF consistently works better models In particular 80 T Tran et al Artiﬁcial Intelligence 246 2017 5385 Fig 12 Performance models Conll2000 nounphrase chunking HSCRF POS DCRF POS mean HSCRF DCRF POS given test time respectively HSCRF POS signiﬁcantly better SCRF POS included features training data small There subtle important difference The HSCRF trained knowledge POS availability test time In contrast SCRF trained knowledge good POSbased features The DCRF worse SCRF POS tags given This share observation 8 However use smaller POS tag set 8 Our explanation SCRF able use wider context given POS tags window 5 tags DCRF limited 1 POS tag NP chunk The SemiCRF theory expressive SCRF advantage current setting Recall SemiCRF special case HSCRF POS level modelled possible conclude joint modelling NP POS levels important More formally let look difference ﬂat setting SCRF SemiCRF multilevel setting DCRF HSCRF Let x xnp xpos Essentially model distribution Prx z Prxnp xpos z Prxpos z multilevel models ignore Prxpos z ﬂat models During test time multilevel models predict xnp ﬁnding maximiser Prxnp xpos z The POS model Prxpos z waste use test time However Prxpos z extra information joint distribution Prx z modelling POS process help smoother estimate NP distribution 10 Related work Hierarchical modelling stochastic processes largely categorised graphical models extending ﬂat hidden semiMarkov models HMMHsMM layered HMM 6 abstract HMM 25 hierarchical HMM HHMM 215 DBN 26 grammarbased models PCFG 27 deterministic models 2829 These models directed HSCRF undirected Higherorder extensions linearchain CRFs developed recently 3031 These methods exploit sparsity state transition eﬃcient inference shallow models Development deeper structures include dynamic CRFs DCRF 8 hierarchical CRFs 910 stacked CRFs 11 The main difference HSCRF CRF variants hierarchical topology HSCRF dynamically inferred data unlike topology predeﬁned users In term inference complexity DCRFs tractable largestate settings The hierarchical CRFs hand tractable assume ﬁxed tree structures ﬂexible adapt complex data For example nounphrase chunking problem assume prior tree structures Rather structure exists discovered model successfully built learned Our HSCRFs deal inference problem DCRFs limiting recursive processes obtaining eﬃcient inference dynamic programming InsideOutside family algorithms Furthermore generalises SemiCRFs model multilevel semantics It addresses partial labels introducing appropriate constraints InsideOutside algorithms As HHMMs special case PCFG bounded depth HSCRFs special case conditional probabilistic contextfree grammar CPCFG 3233 Like HSCRFs CPCFG requires cubic time sequence length parse sentence However contextfree grammar limit depth semantic hierarchy making unnecessarily diﬃcult map hierarchical problems form Secondly lacks graphical model representation enjoy rich set approximate inference techniques available graphical models The AIO algorithm presented Section 4 inspired AIO algorithm HHMMs 152 However loglinear parameterisation probabilistic interpretations inside outside masses T Tran et al Artiﬁcial Intelligence 246 2017 5385 81 The idea numerical scaling presented Section 8 traced Pearls messagepassing procedure 20 34 In AIO algorithms inside masses play role insideout messages In Pearls method reduce messages magnitude normalising step The overﬂow problem opposite underﬂow directed counterparts A similar idea proposed 15 HHMMs The graphical modellike dynamic representation HSCRF appears similar DBN representation HHMMs 17 somewhat resembles dynamic factor graph 3536 Chap 9 However exactly standard graphical model contextual cliques HSCRFs ﬁxed inference This makes diﬃcult apply approximate inference methods Loopy Belief Propagation LBP Gibbs sampling designed ﬁxed cliques The Gibbs sampling applied special arrangement 37 convergence guaranteed limited time preliminary experiments indicated advantage compared exact inference 11 Conclusions In paper presented novel model called Hierarchical SemiMarkov Conditional Random Field extends standard CRFs incorporate hierarchical multilevel semantics We developed graphical modellike dynamic representation HSCRF We derived eﬃcient algorithms learning inference especially ability learn inference partially given labels We demonstrated capacity HSCRFs home video surveillance data shallow parsing English text hierarchical information inherent context helps increase recognition In future work plan attack computational bottleneck largescale settings Although AIO family cubic time complexity expensive largescale application especially long sequences It desirable introduce approximation methods provide speedquality tradeoffs Our early work RaoBlackwellised Gibbs sampling shows promising results 37 We need choice precomputing potentials prior inference learning computing ontheﬂy The ﬁrst choice requires OD K 3 T 2 space signiﬁcant typical realworld problems todays computing power The second choice slow inference learning signiﬁcantly repeated computation step AIO algorithm Finally interesting good HSCRFs approximation general multilevel processes necessarily recursive HSCRF approximation DCRFs This important HSCRFs tractable DCRFs generally Acknowledgement This work partially supported TelstraDeakin Centre Excellence Big Data Machine Learning Appendix A Proofs In appendix detailed proofs propositions stated main text A1 Proof Propositions 1 2 Before proving Proposition 1 2 let introduce lemma Lemma 1 Given distribution form Prx cid5x x xa xs xb exists factorisation cid5x cid5xa xscid5xscid5xs xb xa xb conditionally independent given xs Proof We want prove Since Prxa xb xs Prxa xb xs Prxa xb xs LHS Eq A2 Prxa xb xs Prxa xs Prxb xs cid3 Prxa xb xs cid3 xaxb cid5xa xscid5xscid5xs xb cid5xa xscid5xscid5xs xb xaxb cid5xa xs cid3 cid5xa xs xa cid5xs xb cid3 cid5xs xb xb following fact cid5 xaxb cid5xa xscid5xscid5xs xb cid5xs cid29cid5 cid30cid29cid5 cid5xa xs cid30 cid5xs xb xa xb A1 A2 A3 A4 82 T Tran et al Artiﬁcial Intelligence 246 2017 5385 cid3 cid5 To prove Prxa xs cid5xa xs cid3 cid5xa xs need Prxa xs cid5xa xs normalisation xa xa xa Prxaxs 1 Using Bayes rule Prxa xs Prxa xs xb cid5xa xscid5xs cid5xa xs Prxa xs xb cid5 cid5xs xb xb A5 ignored factors depend xa A similar proof gives Prxb xs cid5xs xb cid5xs xb Combining result Eq A5 Eq A3 gives cid3 xb Eq A2 This completes proof cid2 In fact xs acts separator xa xb In standard Markov networks paths xa xb xs Now proceed proving Propositions 1 2 Given symmetric Markov blanket cid15ds ζ ds j ζ ds ζ ds j The blanket completely separates ζ ds j ζ ds j j potentials associated variables belonging j Therefore Lemma 1 ensures conditional independence j ζ ds Similarly asymmetric Markov blanket cid16ds j u separates ζ ds j u ζ ds j u variable sets conditionally independent Lemma 1 cid2 A2 Proof Proposition 3 Here want derive Eqs 28 29 30 With conditions Lemma 1 Eq A5 shown Prxa xs cid5xa xs Similarly extends cid15 cid16 cid17 cid15 cid14 Pr ζ ds j cid15ds j cid5 cid14 j cid15ds ζ ds j cid5 ˆζ ds j equivalent cid17 cid16 Pr ζ ds j cid15ds j cid3 ζ ds j 1 cid14 cid5 ˆζ ds j cid15 cid14 ˆζ ds j cid15 cid5 cid15 cid14 cid5 ˆζ ds j 1 cid6ds j The equation follows deﬁnition symmetric inside mass Eq 23 Similar procedure yield Eq 29 To prove Eq 30 notice Eq 19 says cid16 cid17 cid16 cid17 cid16 cid17 Prζ Pr cid15ds j Pr ζ ds j cid15ds j Pr ζ ds j cid15ds j equivalently Prcid15ds j Prζ cid16 Pr 1 cid15ds ζ ds j j cid7ds cid6ds j j cid5 ˆζ ds ds cid5 ˆζ j j cid5ζ cid17 1 cid16 cid15ds ζ ds j j cid17 Pr cid5 ˆζ ds j Rds j cid5 ˆζ ds j cid6ds j cid5 ˆζ ds j cid7ds j ds cid5 ˆζ j cid6ds j Rds j cid7ds j In proof proceeding use relation Eq 22 This completes proof cid2 Appendix B Computing state marginals We interested computing marginals state variables Pr cid16 cid17 cid5 cid16 cid17 cid5 cid14 cid15 Pr xd t Pr t ζ xd xd t Prζ δ xd t ζ cid13 cid12 xd t We ζ xd t cid5 1 Z ζ cid14 ζ cid15 cid5ζ δ xd t ζ A6 A7 A8 A9 A10 B1 T Tran et al Artiﬁcial Intelligence 246 2017 5385 83 Fig C13 The SemiCRFs contextual clique framework Let s xd assumption factorisation Eq 22 says t assume state s starts end j t j For conﬁguration ζ respects cid5ζ cid5 cid14 cid15 cid14 cid15 ˆζ ds j cid5 ˆζ ds j Rds j B2 B3 B4 B5 B6 Then Eq B1 s 1 Z Prxd t cid5 cid14 cid5 ˆζ ds j cid15 cid14 cid5 ˆζ ds j cid15 Rds j δ t j ζ cid5 1 Z i1t jtT cid5 cid6ds j cid7ds j Rds j The summing j fact know indices There special cases 1 d 1 scan left right indices marginals simply Prx1 t s 1 Z ˆcid61s 1T cid71s 1T 1 s S 1 2 d D start end times j PrxD t s 1 Z ˆcid7Ds tt cid6Ds tt cid3 Since cid12 1 t 1 T s S D sSd Pr cid5 s cid5 cid5 xd t cid13 1 follows Eq B3 Z cid6ds j cid7ds j Rds j sSd i1t jtT This turns general way computing partition function Some special cases shown earlier For example d 1 1 j T Eq B6 Eq 25 cid71s 1 Similarly d D j t 1T Eq B6 recovers Eq 26 cid6Ds ii 1 Appendix C SemiMarkov CRFs special case In appendix convert semiMarkov CRF SemiCRF 13 HSCRF SemiCRF interesting ﬂat segmental undirected model generalises chain CRF In SemiCRF framework Markov process operates segment level segment nonMarkovian chain nodes A chain segments Markov chain However segment potentially arbitrary length inference SemiCRFs involved chain CRFs Represented HSCRF framework Fig C13 node xt SemiCRF associated ending indicator et following contextual cliques Segmental state corresponds single segment si j essentially state persistence contextual clique context c cid19 cid18 ei1 j 1 0 0 1 HSCRFs terminology State transition similar state transition contextual clique HSCRFs corresponding context c et 1 Associated segmental state clique potential R s s s cid3 S S 1 2 K A SemiCRF threelevel HSCRF root dummy states This gives simpliﬁed way compute partition function ESS MAP assignment AIO algorithms Thus techniques developed paper numerical scaling partially observed data applied SemiCRF To consistent literature ﬂat models HMMs CRFs asymmetric insideoutside masses forwardbackward respectively Since model ﬂat need inside outside variables j state transition potential Ascid3st C1 C2 84 Forward With abuse notation let ζ s 1 j j We write forward αts cid5 cid15 cid14 α js ζ s 1 j z cid5 ζ s 1 j T Tran et al Artiﬁcial Intelligence 246 2017 5385 cid12 cid13 x1 j1 e1 j1 x j s e j 1 In words segment state s ending As result partition function written term forward cid18 cid5 cid5 cid5 cid19 Z z cid5 ζ1T z ζ s 1T z cid5 s ζ s 1T ζ1T cid5 s αT s We derive recursive relation forward Assume segment ending j starts 1 j leads following cid17 1i1 xi j s ei j1 0 ζ s s cid16 cid3 cid3 Then 1 exists decomposition ζ s 1 j factorisation cid15 cid14 ζ s 1 j z cid5 cid15 cid14 cid3 ζ s 1i1 cid5 Ascid3si1 R s j C3 The transition potential Ascid3si1 occurs context c ei1 1 segmental potential R s cid18 cid19 xi j s ei1 1 ei j1 0 cid15 j context c cid14 For 1 factorisation reduces cid5 ζ s 1 j z R s 1 j Since know starting consider possible values interval 1 j Thus Eq C1 rewritten cid5 cid5 cid5 cid15 cid14 cid3 ζ s 1i1 Ascid3si1 R s j R s 1 j cid3 αi1s Ascid3si1 R s j R s 1 j cid5 cid3 ζ s 1i1 α js i2 j cid5 scid3 cid5 i2 j scid3 Backward The backward mirrored version forward In particular let ζ s jT deﬁne backward βt s cid15 cid5 cid14 ζ s jT z β js cid5 ζ s jT Clearly partition function written term backward Z z cid5 s β1s The recursive relation backward βis cid5 cid5 jiT 1 scid3 R s j Asscid3 jβ j1s cid3 R s iT C4 C5 cid12 cid13 x j1T e jT x j s e j1 1 C6 C7 C8 Typically want limit segment maximum length L 1 T This limitation introduces special cases performing recursive computation forward backward Eqs C4 C8 rewritten follows α js βis cid5 cid5 jL1 ji1 cid5 scid3 cid5 jiiL1 jT scid3 cid3 αi1s Ascid3si1 R s j R s 1 j R s j Asscid3 jβ j1s cid3 R s iT C9 C10 Finally extend HSCRF straightforwardly allowing level states persist With relaxation nested semiMarkov CRF model sense segment Markov chain Markov chain subsegments References T Tran et al Artiﬁcial Intelligence 246 2017 5385 85 data J Mach Learn Res 8 Mar 2007 693723 119134 Computer Vision ICCV vol 2 Oct 2005 pp 12841291 1 Y Bengio learning deep architectures AI Found Trends Mach Learn 2 1 2009 1127 2 S Fine Y Singer N Tishby The hierarchical hidden Markov model analysis applications Mach Learn 32 1 1998 4162 3 G Hinton R Salakhutdinov Reducing dimensionality data neural networks Science 313 5786 2006 504507 4 R Salakhutdinov G Hinton Deep Boltzmann machines Proceedings Twelfth International Conference Artiﬁcial Intelligence Statistics AISTATS09 vol 5 2009 pp 448455 5 EFTK Sang S Buchholz Introduction CoNLL2000 shared task chunking Proceedings 2nd Workshop Learning Language Logic 4th Conference Computational Natural Language Learning Lisbon Portugal vol 7 2000 pp 127132 httpwwwcntsuaacbe conll2000chunking 6 N Oliver A Garg E Horvitz Layered representations learning inferring oﬃce activity multiple sensory channels Comput Vis Image Underst 96 2 2004 163180 7 J Lafferty A McCallum F Pereira Conditional random ﬁelds probabilistic models segmenting labeling sequence data Proceedings International Conference Machine Learning ICML 2001 pp 282289 8 C Sutton A McCallum K Rohanimanesh Dynamic conditional random ﬁelds factorized probabilistic models labeling segmenting sequence 9 L Liao D Fox H Kautz Extracting places activities GPS traces hierarchical conditional random ﬁelds Int J Robot Res 26 Jan 2007 10 S Kumar M Hebert A hierarchical ﬁeld framework uniﬁed contextbased classiﬁcation Proceedings IEEE International Conference 11 D Yu S Wang L Deng Sequential labeling deepstructured conditional random ﬁelds IEEE J Sel Top Signal Process 4 6 2010 965973 12 T Truyen D Phung H Bui S Venkatesh Hierarchical semiMarkov conditional random ﬁelds recursive sequential data TwentySecond Annual Conference Neural Information Processing Systems NIPS Vancouver Canada Dec 2008 pp 16571664 13 S Sarawagi WW Cohen SemiMarkov conditional random ﬁelds information extraction LK Saul Y Weiss L Botton Eds Advances Neural Information Processing Systems 17 MIT Press Cambridge Massachusetts 2004 pp 11851192 14 S Lauritzen Graphical Models Oxford Science Publications 1996 15 HH Bui DQ Phung S Venkatesh Hierarchical hidden Markov models general state hierarchy DL McGuinness G Ferguson Eds Proceed ings 19th National Conference Artiﬁcial Intelligence AAAI San Jose CA Jul 2004 pp 324329 16 LR Rabiner A tutorial hidden Markov models selected applications speech recognition Proc IEEE 77 2 1989 257286 17 K Murphy M Paskin Linear Time Inference Hierarchical HMMs Advances Neural Information Processing Systems NIPS vol 2 MIT Press 2002 pp 833840 18 PQ Dinh Probabilistic Film Grammar Based Methods Video Content Understanding PhD thesis Curtin University Technology 2005 19 B Taskar P Abbeel D Koller Discriminative probabilistic models relational data Proceedings 18th Conference Uncertainty Artiﬁcial Intelligence UAI02 Morgan Kaufmann 2002 pp 485492 20 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann San Francisco CA 1988 21 N Nguyen D Phung S Venkatesh HH Bui Learning detecting activities movement trajectories hierarchical hidden Markov models Proceedings IEEE Conference Computer Vision Pattern Recognition CVPR San Diego CA vol 2 Jun 2005 pp 955960 22 T Truyen D Phung H Bui S Venkatesh AdaBoostMRF boosted Markov random forests application multilevel activity recognition Com puter Vision Pattern Recognition New York USA vol 2 June 2006 pp 16861693 23 E Brill Transformationbased errordriven learning natural language processing case study partofspeech tagging Comput Linguist 21 4 24 SVN Vishwanathan NN Schraudolph MW Schmidt KP Murphy Accelerated training conditional random ﬁelds stochastic gradient methods Proceedings International Conference Machine Learning ICML 2006 pp 969976 25 HH Bui S Venkatesh G West Policy recognition abstract hidden Markov model J Artif Intell Res 17 2002 451499 26 K Murphy Dynamic Bayesian Networks Representation Inference Learning PhD thesis Computer Science Division University California Berke 27 F Pereira Y Schabes Insideoutside reestimation partially bracketed corpora Proceedings Meeting Association Computational ley Jul 2002 Linguistics ACL 1992 pp 128135 28 J Chung S Ahn Y Bengio Hierarchical multiscale recurrent neural networks arXiv preprint arXiv160901704 2016 29 S El Hihi Y Bengio Hierarchical recurrent neural networks longterm dependencies Advances Neural Information Processing Systems 8 1995 543566 NIPS95 MIT Press Cambridge MA 1996 pp 493499 Res 15 2014 9811009 30 NV Cuong N Ye WS Lee HL Chieu Conditional random ﬁeld highorder dependencies sequence labeling segmentation J Mach Learn 31 X Qian X Jiang Q Zhang X Huang L Wu Sparse higher order conditional random ﬁelds improved sequence labeling Proceedings 26th Annual International Conference Machine Learning ACM 2009 pp 849856 32 Y Miyao J Tsujii Maximum entropy estimation feature forests Proceedings Human Language Technology Conference HLT Morgan Kaufmann 33 S Clark JR Curran Loglinear models widecoverage CCG parsing Proceedings Conference Empirical Methods Natural Language 34 J Yedidia W Freeman Y Weiss Constructing freeenergy approximations generalized belief propagation algorithms IEEE Trans Inf Theory 51 7 35 FR Kschischang BJ Frey HA Loeliger Factor graphs sumproduct algorithm IEEE Trans Inf Theory 47 February 2001 498519 36 TT Truyen On Conditional Random Fields Applications Feature Selection Parameter Estimation Hierarchical Modelling PhD thesis Curtin Uni 37 T Truyen D Phung S Venkatesh H Bui MCMC hierarchical semiMarkov conditional random ﬁelds NIPS09 Workshop Deep Learning Speech Recognition Related Applications Whistler BC Canada Dec 2009 Publishers Inc 2002 pp 292297 Processing EMNLP 2003 pp 97104 2005 22822312 versity Technology 2008