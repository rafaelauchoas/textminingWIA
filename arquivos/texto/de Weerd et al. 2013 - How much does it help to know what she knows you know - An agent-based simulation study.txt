Artiﬁcial Intelligence 199200 2013 6792 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint How help know knows know An agentbased simulation study Harmen Weerd Rineke Verbrugge Bart Verheij Institute Artiﬁcial Intelligence Faculty Mathematics Natural Sciences University Groningen PO Box 407 9700 AK Groningen The Netherlands r t c l e n f o b s t r c t Article history Received 2 October 2012 Received revised form 8 May 2013 Accepted 10 May 2013 Available online 14 May 2013 Keywords Agentbased models Evolution theory mind 1 Introduction In everyday life people use theory mind explicitly attributing unobservable mental content beliefs desires intentions Humans known able use ability recursively That engage higherorder theory mind consider believe beliefs In paper use agentbased computational models investigate evolution higherorder theory mind We consider higherorder theory mind different competitive games including repeated singleshot repeated extensive form games determine advantage higherorder theory mind agents lowerorder theory mind opponents Across games ﬁnd common pattern ﬁrstorder secondorder theory mind agents clearly outperform opponents limited ability use theory mind advantage deeper recursion thirdorder theory mind limited comparison 2013 Elsevier BV All rights reserved In everyday life regularly use theory mind reasoning people know believe For example identify characters literature movies accept beliefs intentions different When telling joke speaker engages higherorder theory mind believing hearer knows speaker intend convey actual fact opinion In paper use agentbased computational models explain evolution ability reason mental content others1 In settings humans computational agents perform actions inﬂuence decisionmaking pro cess example automated negotiation 34 necessary accurately predict behaviour order respond appropriately In artiﬁcial intelligence modeling opponent explicitly achieved formal approaches example dynamic epistemic logic 56 recursive opponent modeling 7 interactive POMDPs 8 networks inﬂuence diagrams 9 game theory mind 10 iterated bestresponse models cognitive hierarchy models 11 leveln theory 1213 These models allow recursive modeling opponent modeling opponent opponentmodeling agent creating increasingly complicated models predict actions increasingly sophisti cated opponents For cognitive agents meant interact humans important know formal models cognition allow accurate modeling human reasoning models better capture type bounded rationality exhibited humans 1415 Corresponding author Tel 31 50 363 4114 fax 31 50 363 6687 Email addresses hdeweerdairugnl H Weerd rinekeairugnl R Verbrugge bverheijairugnl B Verheij 1 This research continuation 12 00043702 matter 2013 Elsevier BV All rights reserved httpdxdoiorg101016jartint201305004 68 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 11 Theory mind abilities humans animals In humans ability predict actions explicitly attributing unobservable mental content beliefs desires intentions known psychology theory mind 16 Experiments humans play games evidence humans use theory mind recursively decisionmaking process 1720 They ability secondorder theory mind reason way reason mental content For example asked search hidden object boxes participants tend ignore salient box nested belief hider believe seeker consider obvious place search hidden object box stands 21 The use higherorder secondorder theory mind allows individuals secondorder attribution Alice doesnt know Bob knows throwing surprise party The human ability higherorder theory mind wellestablished false belief tasks 172223 strategic games 182024 However use theory mind kind nonhuman species controversial matter Primates 2526 monkeys 27 goats 28 dogs 29 corvids 3031 proposed able mental content account However experiments animals behave way consistent having theory mind criticized able distinguish theory mind strategies rely mental state attribution 3233 Opponents attributing theory mind animals posit animal learned behaviour previous experiences combined simple mechanisms stress 3435 Likewise experiments animals fail ability attribute mental states criticized complex ecologically meaningful 36 12 Evolution theory mind The differences ability use theory mind humans animals raise issue reason evolution allows humans use theory mind recursively use higher order theory mind reason people understand mental content animals including chimpanzees primates appear ability Furthermore recursive opponent modeling continue indeﬁnitely humans appear use higherorder theory mind certain point 181937 In evolutionary sense costs higher orders theory mind outweigh beneﬁts One hypotheses explain emergence social cognition Machiavellian intelligence hypothesis2 38 According Machiavellian intelligence hypothesis social cognition allows individuals use deception social manipulation obtain evolutionary advantage If parallel drawn higherorder theory mind evolution higherorder theory mind favored giving individuals competitive advantage This way ability use higherorder theory mind beneﬁcial individuals trait detrimental individuals abilities In paper aim test Machiavellian intelligence hypothesis making use agentbased modeling tempt reasonably natural competitive settings higherorder theory mind advantageous agents 13 Agentbased modeling Agentbased modeling simulation technique individual agents act interact based percep tion local situation By explicitly modeling heterogeneity individual agents agentbased models represent systems complex capture equationbased modeling approaches This technique proven use fulness research tool investigate behavioral patterns emerge interactions individuals cf 3940 Among agentbased models explain ﬁghting crowds 41 trust negotiations 42 evolution agriculture 43 evolution cooperation punishment 4446 evolution lan guage 374749 In paper consider agentbased computational models investigate advantages making use higherorder theory mind The use agentbased models allows precisely control monitor mental content including application theory mind test subjects This allows simulate computational agents game settings determine extent higherorder theory mind provides individuals advantage competitors restricted use theory mind By varying game settings allows determine scenarios ability use theory mind beneﬁcial agent increasingly higher orders theory mind provide individuals increasing advantages competitors To test Machiavellian intelligence hypothesis consider number competitive zerosum games let computational agents compete determine ability use higherorder theory mind advanta geous competitive setting We consider different games First consider variations repeated singleshot 2 For discussion alternative hypotheses 37 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 69 Table 1 RPS Payoff table graph representation rockpaperscissors game The table shows payoff player choosing row action rock paper scissors possible choice player choosing column action Arrows graph read defeats For example arrow paper rock means paper defeats rock Rock Paper Scissors Rock Paper Scissors 0 1 1 1 0 1 1 1 0 rockpaperscissors RPS games The transparent setup RPS allows relate differences effectiveness higher order theory mind easily structure game The fourth game Limited Bidding involves planning multiple rounds play We consider complex extensive form game judge evolutionary advantage making use higherorder theory mind generalizes games The games described Section 2 Agents beneﬁt theory mind games considering position opponent determining mental content roles reversed This process ﬁrst described intuitively Section 3 A formal description model use presented Section 4 To determine use theory mind presents agents advantage opponents abilities placed agents different orders theory mind competition The results Section 5 Finally Section 6 provides discussion gives directions future research Throughout paper considering agents engaged competitive twoplayer game To avoid confusion refer focal agent player male opponent female 2 Game settings We investigate theory mind game settings The games strictly competitive games sense zerosum games possibility winwin situation games In games present player guarantee expected outcome zero irrespective opponent plays That value games zero By playing mixed strategy player randomly selects actions perform player prevent opponent structurally winning game However repeated games player learn regularities opponents strategy time able use advantage 21 Rockpaperscissors variations In following subsections wellknown game rockpaperscissors RPS variations The rockpaperscissors game known RoShamBo game settings ability model opponent informally demonstrated relevance applicability 5051 Although strategy consistently defeat agent plays RPS randomly agent repeatedly encounters opponent setting RPS game use regularities opponents strategy advantage In programming competitions 50 random strategy results average score The existence agents play according nonrandomizing strategy allows stronger players increase score expense weaker players The champion programming competition 2000 use strategies detect regularities opponents behaviour considered possibility opponent similar strategies model champions behaviour 51 211 Rockpaperscissors The game rockpaperscissors RPS 52 twoplayer symmetric zerosum game players simultane ously choose possible actions rock paper scissors If choose action game ends tie Otherwise player chooses rock wins chooses scissors scissors wins paper paper wins rock The game represented shown Table 1 shows payoff table graph rep resentation RPS game The matrix shows payoff player choosing row action possible choice player choosing column action In graph arrow action A action B denotes relation A defeats B RPS known unique mixedstrategy Nash equilibrium 53 player chooses options equal probability That player strategies known player improve expected outcome players play randomly choosing possible actions When agents repeatedly play RPS opponent agent randomizes actions prevents opponent taking advantages regularities strategy However randomizing prevents agent exploiting regularities opponents behaviour repeated games By correctly modeling regularities opponents behaviour agent 70 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 Table 2 ERPS Payoff table graph representation elemental rockpaperscissors game agents choose tween ﬁve different actions The table shows payoff player choosing row action wood ﬁre water earth possible choice player choosing column action Arrows graph read defeats For example arrow wood earth means wood defeats earth metal Wood Metal Fire Water Earth Wood Metal Fire Water Earth 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 Table 3 RPSLS Payoff table graph representation rockpaperscissorslizardSpock game The table shows payoff player choosing row action rock paper scissors lizard Spock possible choice player choosing column action Arrows graph read defeats For example arrow lizard Spock means lizard defeats Spock Rock Paper Scissors Lizard Spock Rock Paper Scissors Lizard Spock 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 increase score expense opponent Experimental evidence suggests human participants poor generating random sequences 5455 play RPS nonrandom way 5657 We expect ability use theory mind present agent advantage opponents abilities The champion ﬁrst international RPS programming competition 2000 use strategy detected regularities opponents behaviour considered possibility opponent similar strategy model behaviour champions 51 That program engaged theory mind attributing intention win game opponent However limited action space limit effectiveness theory mind speciﬁc particular game 212 Elemental rockpaperscissors Although simple structure RPS appealing limitation actions inﬂuence effectiveness higherorder theory mind To address issue consider elemental rockpaperscissors ERPS ERPS extends RPS includes ﬁve actions wood metal ﬁre water earth shown Table 2 The ERPS game preserves property RPS action defeated exactly response That action opponent play exists unique best response guarantees positive outcome agent3 As case RPS unique mixedstrategy Nash equilibrium ERPS randomize possible actions However increased action space ERPS increased support theory mind That expect theory mind agents perform ERPS RPS Moreover differences perfor mance theory mind agents playing ERPS compared playing RPS attributed differences structure games In particular increased performance higherorder theory mind agents ERPS indicates limited action space inﬂuences effectiveness theory mind 213 RockpaperscissorslizardSpock RockpaperscissorslizardSpock RPSLS 58 extension RPS adds actions lizard Spock actions rock paper scissors RPS Like ERPS RPSLS ﬁve actions RPSLS action wins exactly actions defeated remaining actions Table 3 shows payoff matrix graph representation RPSLS game Unlike previous games best response action RPSLS unique This means agent attributes mental content opponent result clear prediction opponent behaviour An agent predicts opponent play paper preference playing scissors lizard defeat paper equally As result agent believes opponent believe agent play paper predict 3 The payoffs elemental rockpaperscissors based overcoming cycle elements Chinese philosophy Wu Xing H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 71 Fig 1 Example way limited bidding played For interpretation references color ﬁgure reader referred web version article going play scissors lizard Similarly opponents behaviour RPSLS informative RPS ERPS After agent plays paper believed opponent play rock play Spock As result playing RPSLS repeatedly opponent provides information opponents behaviour RPS ERPS Due increased diﬃculty modeling opponent expect theory mind agents perform poorly RPSLS RPS ERPS 22 Limited Bidding Unlike rockpaperscissors variations presented previous subsections Limited Bidding LB4 game plays rounds When game starts player handed identical set 5 tokens valued 1 5 Over course 5 rounds players simultaneously choose tokens use bid round Once players choice tokens selected players revealed compared round won player selected highest value token In case draw winners The object game win rounds possible losing rounds possible However token game This forces players plan ahead strategically choose tokens available place bid For example player selects token value 5 ﬁrst round sure ﬁrst round result win opponent However means remaining 4 rounds token value 5 available player Players weigh additional probability win current round loss competitive strength later rounds results higher valued token Fig 1 shows example way LB played In case game won light blue player right Note LB possible win rounds Instead player win maximum rounds case round won opponent As result player achieve maximum score 3 LB As variations RPS described earlier player prevent opponent winning game He randomly choosing play tokens available round game Averaged repeated games mixed strategy randomizing available choices result score zero player opponent 23 Rational players In game theory common assumption player rational fact known players Moreover players assumed know knows player rational continuing fashion ad inﬁnitum In terms theory mind common knowledge rationality 6061 means players possess ability use theory mind depth order In section explain rational players play Limited Bidding game assumption common knowledge rationality For simplicity consider limited bidding game tokens In game players decide token play moments start game result ﬁrst round announced Although new information available second round choice token play round degenerate start round players token left Since players choice tokens play ﬁrst round variations subgame agents play second round game We ﬁrst consider rational agent choose start second round Since player tries maximize number rounds won minimize numbers rounds lost end game player receives payoff equal difference Table 4 lists payoffs players possible outcome game outcome represented concatenation tokens order player played Each payoff structure presented tuple x y player 1 receives payoff x player 2 receives payoff y The subgames played beginning second round represented 2by2 submatrices highlighted alternating background color Table 4 4 Limited Bidding adaptation game presented 59 72 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 Table 4 Payoff table limited bidding game tokens Each outcome game corresponds tuple table The ﬁrst value tuple payoff player second payoff player Player 1 123 132 213 231 312 321 Player 2 123 0 0 0 0 0 0 1 1 1 1 0 0 132 0 0 0 0 1 1 0 0 0 0 1 1 213 0 0 1 1 0 0 0 0 0 0 1 1 231 1 1 0 0 0 0 0 0 1 1 0 0 312 1 1 0 0 0 0 1 1 0 0 0 0 321 0 0 1 1 1 1 0 0 0 0 0 0 Table 5 Payoff table limited bidding game tokens players derived ﬁrst round players play randomly Player 1 1 2 3 Player 2 1 00 00 05 05 05 05 2 05 05 00 00 05 05 3 05 05 05 05 00 00 Note ﬁrst round game ends draw resulting subgame degenerate In case players receive zero payoff irrespective ﬁnal outcome When ﬁrst round end draw resulting subgame variation matching pennies game 62 This game known purestrategy Nash equilibrium That combination pure strategies player maximizes payoff given strategy opponent However unique mixedstrategy Nash equilibrium player plays possible strategy equal probability If players play remaining tokens 50 probability incentive switch strategies given opponent playing randomly rational agent strategy available yield better expected payoff playing randomly Due common knowledge rationality player knows reached conclusion ﬁrst round play randomly This means rewrite payoff matrix reﬂect results subgames shown Table 5 Note variation rockpaperscissors game As purestrategy Nash equilibrium unique mixedstrategy Nash equilibrium reached players play strategy equal probability That rational agents assumption common knowledge rationality solve limited bidding game playing randomly round This result holds game played tokens That prevent opponent taking advantage regularity strategy rational agents play limited bidding game randomly 24 Hypotheses effectiveness theory mind In section described different games rockpaperscissors elemental rockpaperscissors rockpaper scissorslizardSpock Limited Bidding In game rockpaperscissors agents choose possible actions defeated exactly actions The game elemental rockpaperscissors sembles RPS action defeated exactly action agents playing ERPS ﬁve different actions choose RockpaperscissorslizardSpock allows ﬁve different actions unlike ERPS action defeated exactly actions Finally Limited Bidding game spans rounds agents decide order play tokens initial set ﬁve The game RPS serves transparent base scenario determine theory mind beneﬁts agents competi tive settings We expect ability use theory mind advantageous competitive settings Speciﬁcally expect ability use higher orders theory mind allows agent outperform opponent lower order theory mind game rockpaperscissors In remainder refer expectation hypothesis H RPS The small number actions agents choose RPS limit effectiveness higherorder theory mind The ERPS game agents larger action space addresses issue We expect larger action space ERPS game allows higherorder theory mind agents outperform opponents lower order theory mind RPS game refer hypothesis H ERPS Agents use theory mind model opponent attempt predict behaviour As result theory mind likely effective opponent predictable We selected RPSLS game unique bestresponse action opponent behaviour harder predict Hypothesis H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 73 Fig 2 Example possible thought process zeroorder theory mind agent Fig 3 Example possible thought process ﬁrstorder theory mind agent HRPSLS states expect theory mind agents diﬃculty predicting opponent RPSLS perform poorly compared performance RPS ERPS Limited Bidding multistage game represents complex situation singleshot games RPS ERPS RPSLS This game selected determine theory mind advantageous results simple singleshot games translate complex setting We expect results Limited Bidding quantitatively different rockpaperscissors results qualitatively similar hypothesis H LB 3 Playing games simulationtheory mind In games described Section 2 players prevent opponent winning game playing randomly However strategy prevents agent losing game opponent prevents agent winning game As result randomizing strategy results average score RoShamBo programming competitions 50 discussed Section 21 An agent believes opponent play nonrandom way try predict opponents behaviour advantage regularities opponents strategy win game For humans way generating predictions opponent behaviour simulationtheory mind 6365 In simulationtheory mind player takes perspective opponent determines decision player position faced opponent Using implicit assumption opponents thought process accurately modeled thought process player predicts opponent decision player roles reversed In section intuition process perspectivetaking agents differ abilities explicitly model mental states illustrate affects choices playing RPS In Section 4 intuition described computational model In remainder speak ToMk agent indicate agent ability use theory mind including kth order 31 Zeroorder theory mind A zeroorder theory mind ToM0 agent unable model mental content beliefs desires intentions opponent In particular ToM0 agent unable represent opponent goals different goals When predicting opponents behaviour agent limited memory previous events The ToM0 agent intended model inexperienced frustrated player consider opponents behaviour thinking way reacts actions A ToM0 agent believes happened past good predictor going happen future This reﬂects human players tendency interpret repetition indicative pattern 66 Fig 2 illustrates possible thought process ToM0 agent If ToM0 agent remembers opponent played paper previous RPS games concludes opponent likely play paper game Given belief ToM0 agent adjust behaviour play scissors 32 Firstorder theory mind In contrast ToM0 agent ﬁrstorder theory mind ToM1 agent considers possibility opponent trying win game reacts choices ToM1 agent To predict opponents behaviour ToM1 agent puts position opponent considers information available perspective Fig 3 shows example thought process Suppose ToM1 agent remembers played paper previous RPS games opponent He realizes roles reversed 74 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 Fig 4 Example possible thought process secondorder theory mind agent remember opponent played paper conclude opponent likely playing paper play scissors The ToM1 agent ability attribute thought process opponent predict likely play scissors Given prediction ToM1 agent play rock Although ToM1 agent models opponent able use zeroorder theory mind agents setup know extent abilities opponents certainty Through repeated interaction ToM1 agent come believe opponent ToM0 agent beliefs Such opponent beliefs example play rock irrespective ToM1 agent previously played Based belief ToM1 agent choose play ToM0 agent follow thought process presented Fig 2 33 Secondorder theory mind Just ToM1 agent models opponent having zeroorder theory mind secondorder theory mind ToM2 agent models ToM1 opponent That ToM2 agent considers possibility opponent putting position modeling ToM0 agent Fig 4 depicts possible process ToM2 agent If ToM2 agent remembers opponent played paper previous encounters believe opponent predict playing scissors As result ToM2 agent predict opponent play rock case agent play paper Each additional order theory mind allows deeper recursion mental state attribution Note order theory mind represents additional model opponent behaviour corresponding prediction A ToM2 agent considers predictions opponents behaviour based application zeroorder ﬁrstorder secondorder theory mind 4 Model We implemented computational agents use simulationtheory mind play similarly intuitive scription Section 3 In section discuss implementation agents play competitive games described Section 2 The agents presented differ ability explicitly represent beliefs ability use theory mind 41 Representation games In model discuss game tuple G cid3N S A T π cid4 N j set agents denotes focal agent j denotes opponent S set possible states game A Ai A j set possible action pairs Ai set actions performed agent A j set actions performed opponent j T partial transition function T S A S describes results pair actions focal agent opponent game state π πi π j pair payoff functions πi π j S A R An instance game played players consists initial game state sequence action pairs For example Limited Bidding game state s S encodes tokens available agent available opponent This allows agents playing LB distinguish individual rounds beliefs concerning opponents gameplay conditional tokens played Fig 5 shows representation instance LB game corresponds example game shown Fig 1 In Fig 5 states represented boxes arrows state transitions Each state transition shows action pair caused transition payoff πi focal agent payoff π j opponent The game starts initial state agent opponent identical set ﬁve tokens The actions H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 75 Fig 5 Representation instance Limited Bidding game shown Fig 1 The ﬁgure shows action pairs transition game initial state ﬁnal state agent perform correspond selecting tokens available Once agent opponent selected action game transitions new state players receive payoff This process repeated ﬁnal state reached combination actions leads change game state For Limited Bidding ﬁnal state situation ﬁve rounds tokens play The game states S intended model different stages multistage game Limited Bidding Singleshot games variations rockpaperscissors described Section 2 represented game contains states S s0 s1 game transitions start state s0 end state s1 possible action pair ai j A That T s0 ai j s1 ai j A Additional transitioning new game state agents receive payoff based actions The payoff functions πi π j S A R determine payoff πis ai j focal agent payoff π js ai j opponent combination game state s S action pair ai j A Note consider zerosum games πis ai j π js ai j 42 Zeroorder theory mind agents In present setup assume agents understand game Furthermore assume agent considers possibility opponent understand game opponent believes understand game continuing fashion ad inﬁnitum This means example agents considers possible opponent perform action action space A j Similarly agent believes opponent considers possibility play action action space Ai Note assumption similar assumption rules dynamics game common knowledge 56768 requires agent understands game knows opponent understands game forth However simulated agents described limited ability use theory mind able represent opponents mental content That case possible assume rules dynamics game common knowledge Instead assume agent beliefs conﬂict common knowledge rules dynamics game Agents form beliefs b0 form probability distribution opponents actions A j game state b0a j s represents agent believes probability opponent play action j A j given game situation s S We assume b0a j s cid2 0 cid2 b0a j s 1 j A j s S s S j A j 1 2 That 1 agents assign nonnegative probability opponent playing certain action certain game state 2 probabilities assigned possible opponent action sum 1 possible game state For ToM0 agent belief structure b0 represents extent beliefs concerning opponents behaviour Given games payoff function beliefs way opponent plays game ToM0 agent able assign subjective value Φiai b0 s playing certain action ai Ai game state s S given beliefs b0 concerning opponents behaviour To determine value agent considers likely considers opponent going play action j A j If opponent play j playing ai yield agent immediate payoff cid7 T s ai j The agent πis ai j cause game forward end new state s takes account planning ahead determining maximum value achieve game reaches The combination immediate payoff πis ai j maximum value achieved state state s T s ai j weighted agent believes probability b0a j s opponent actually going play action j The value Φiai b0 s focal agent assigns playing action ai game state s based belief b0 concerning opponents behaviour given cid2 cid5 cid7 cid4cid4cid6 cid3 ai b0 s cid4 Φi b0a j s cid3 cid4 s ai j πi s ai j 3 cid3 Φi cid7 b0 T cid3 max acid7Ai j A j 76 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 Table 6 Possible mental contents ToM0 agent ToM1 agent game rockpaperscissors Mental content ToM0 agent Example 1 b Mental content ToM1 agent Example 2 Order theory mind k Order theory mind k b0R s0 b0P s0 b0S s0 0 05 03 02 ck bkR s0 bkP s0 bkS s0 0 05 03 02 1 09 04 05 01 We assume agents choose rationally given beliefs That agents choose play action ai maximizes given value function This represented decision function t cid3 ai b0 s cid3 b0 s Φi 4 cid4 cid4 t arg max ai Ai Example 1 Consider agent plays rockpaperscissors opponent The RPS game consists states S s0 s1 ﬁrst state s0 represents start game second state s1 end game The action spaces agent opponent Ai A j R P S The transition function T deﬁned T s0 ai j s1 ai Ai j A j The payoffs state s0 given Table 1 payoffs zero state s1 We consider ToM0 agent mental content listed Table 6a The agents zeroorder beliefs b0 indicate agent believes 50 probability opponent going play R 30 probability opponent going play P 20 probability opponent going play S Based zeroorder beliefs agent determine value actions R P S based expected payoff For example agent believes plays R 30 probability lose opponent played P 20 probability win opponent played S This results following values cid3 cid3 cid3 Φi Φi Φi cid4 R b0 s0 cid4 P b0 s0 cid4 S b0 s0 b0R s0 πis0 R R b0P s0 πis0 R P b0S s0 πis0 R S 05 0 03 1 02 1 01 05 1 03 0 02 1 03 05 1 03 1 02 0 02 The agent chooses play action maximum value In case cid4 cid3 b0 s0 t arg max ai Ai cid3 ai b0 s0 cid4 Φi P That ToM0 agent described Table 6a chooses play P 43 Firstorder theory mind agents A ToM1 agent attributes beliefs opponent form additional probability distribution b1 Here b1ai s represents agent believes opponent judge probability play action ai Ai game state s S However ToM1 agent zeroorder beliefs b0 opponent The decision process ToM1 agent consists roughly steps 1 making prediction ˆa 2 integrating ﬁrstorder prediction ˆa 3 selecting action maximizes agents expected payoff given integrated beliefs opponent behaviour opponent behaviour based agents ﬁrstorder beliefs b1 opponent behaviour zeroorder belief b0 1 j 1 j Let step precisely 1 First ToM1 agent makes prediction opponent behaviour based ﬁrstorder beliefs b1 Using simulation prediction action opponent play To A j maximizes value function perspective opponent theory mind agent uses decision function t 1 agent determines action ˆa j given agent believes opponent zeroorder beliefs b1 That ˆa 1 j t j cid4 cid3 b1 s arg max j A j cid3 j b1 s cid4 Φ j 5 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 77 Note Eq 5 similar Eq 4 That ToM1 agent determines prediction opponent behaviour similar way ToM0 agent determines behaviour In calculating prediction ˆa agent makes use value function beliefs b1 Note specifying ˆa agent makes single prediction opponents behaviour assigning probabilities possible opponent action This allows agent check validity 1 prediction easily comparing prediction ˆa j opponents actual behaviour However means slight differences agents value function opponent render prediction incorrect 1 j 1 j 2 A ToM1 agents ﬁrstorder theory mind provides agent prediction ˆa opponent behaviour This pre diction conﬂict zeroorder beliefs b0 The extent ﬁrstorder theory mind governs decisions agents actions determined conﬁdence 0 cid3 c1 cid3 1 ﬁrstorder theory mind accurately predicts oppo nents behaviour The value conﬁdence c1 allows agent distinguish different types opponents weights zeroorder beliefs prediction ﬁrstorder theory mind accordingly This weighting process captured belief integration function U This function integrates agents ﬁrstorder prediction ˆa j zeroorder beliefs b0 opponent behaviour Compared zeroorder beliefs b0 agents integrated belief opponent playing action ˆa increased integrated belief opponent playing action decreased Speciﬁcally 1 j 1 j cid3 b0 ˆa U cid4 1 j c1 j s cid7 1 c1 b0a j s 1 c1 b0a j s c1 j cid8 ˆa j ˆa 1 j 1 j 6 3 After integrating zeroorder beliefs b0 prediction opponent behaviour ˆa based ﬁrstorder theory mind agent chooses action play This decision analogously way ToM0 agent decides Eq 4 However ToM1 agent decides based integrated beliefs U b0 ˆa c1 opponent behaviour instead zeroorder beliefs b0 directly That ToM1 agent chooses play action given cid3 b1 s cid3 b0 ˆa cid3 b0 t t 1 j cid4 7 s s U U cid4 cid3 cid4 cid3 cid4 cid4 t 1 j c1 1 j c1 j In special case agent conﬁdence ﬁrstorder theory mind c1 0 ToM1 agents decision inﬂuenced zeroorder beliefs In case agent chooses ToM0 agent Example 2 Consider ToM1 agent plays rockpaperscissors similar agent Example 1 mental content given Table 6b The table shows ToM1 agent zeroorder beliefs b0 indicate agents beliefs concerning opponents actions ﬁrstorder beliefs b1 For example b1R s0 04 agent believes opponent believes 40 probability going play R Taking perspective opponent agent determines place That agent ﬁrst calculates value assign actions available opponent ﬁrstorder beliefs b1 actually zeroorder beliefs opponents payoffs actually payoffs cid3 cid3 cid3 Φ j Φ j Φ j cid4 cid4 R b1 s0 P b1 s0 cid4 S b1 s0 04 0 05 1 01 1 04 04 1 05 0 01 1 03 04 1 05 1 01 0 01 The agents ﬁrstorder theory mind predicts opponent select action yield highest payoff ˆa 1 j t j cid4 cid3 b1 s0 arg max j A j cid3 j b1 s0 cid4 Φ j P Using ﬁrstorder theory mind agent predicts opponent going play P Note agents prediction ˆa conﬂicts zeroorder beliefs b0 According ﬁrstorder theory mind opponent going play P agents zeroorder beliefs assign 50 probability opponent going play R To able decision agent integrates ﬁrstorder prediction zeroorder beliefs b0 In case agents conﬁdence c1 ﬁrstorder theory mind 09 This means agents integrated beliefs determined 90 prediction based ﬁrstorder theory mind 10 zeroorder beliefs 1 j cid3 b0 P 09 cid3 b0 P 09 cid3 b0 P 09 cid4 cid4 cid4 U U U R s0 1 09 b0R s0 01 05 005 P s0 1 09 b0P s0 09 093 S s0 1 09 b0S s0 01 02 002 78 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 After integrating zeroorder beliefs ﬁrstorder prediction agent believes 5 probability oppo nent going play R 93 probability opponent going play P 2 probability opponent going play S Based integrated beliefs agent determines value playing actions cid3 cid3 cid3 Φi Φi Φi R U P U S U cid4 cid3 b0 P 09 cid3 b0 P 09 cid4 cid3 b0 P 09 s0 cid4 s0 cid4 s0 cid4 cid4 005 0 093 1 002 1 091 005 1 093 0 002 1 003 005 1 093 1 002 0 088 The agent chooses play action maximum value In case cid3 cid3 b0 P 09 cid4 U cid4 s0 t arg max ai Ai cid3 ai U cid3 b0 P 09 cid4 Φi s0 cid4 S That ToM1 agent described Table 6b chooses play S 44 Secondorder theory mind agents Similar way ToM1 agent models opponent ToM0 agent ToM2 agent considers possibility opponent ToM1 agent As ToM2 agent explicit model beliefs believes opponent attributing In model beliefs represented additional belief structure b2 Using simulationtheory mind agent attributes decisionmaking process described Eq 7 opponent That agent considers game perspective opponent determines position ToM1 agent To determine opponents actions ToM2 agent needs know conﬁdence c1 ﬁrstorder theory mind In experiments assumed ToM2 agents use value 08 determine opponents behaviour playing ToM1 agent5 Based secondorder theory mind ToM2 agent predicts opponent playing square brackets readability cid3 b2 s cid8 b1 t 08 t s 8 ˆa U cid4 cid3 cid4 cid9 2 j j 2 This prediction ˆa based secondorder theory mind integrated ToM2 agents zeroorder beliefs b0 j 1 prediction ˆa based ﬁrstorder theory mind makes choice action play As ToM1 j agent ToM2 agent know order theory mind opponent playing Instead extent secondorder theory mind governs decisions ToM2 agents actions determined conﬁdence 0 cid3 c2 cid3 1 secondorder theory mind accurately predicts opponents behaviour The ToM2 agent weights integrated beliefs Eq 7 prediction opponent behaviour ˆa based secondorder theory mind As result ToM2 agents integrated beliefs opponent behaviour given cid8 b1 t cid4 08 2 j 9 U U U cid4 cid3 cid4 cid9 cid9 cid8 cid3 b0 t j cid10 c1 cid4 t j cid10 cid3 b1 s cid13 cid11cid12 1 j ˆa cid3 b2 s cid11cid12 2 j ˆa s cid13 c2 The ToM2 agent performs belief integration steps First agent integrates zeroorder beliefs b0 cerning opponents behaviour prediction ˆa based application ﬁrstorder theory mind In second step prediction ˆa makes ﬁnal choice action select based beliefs based secondorder theory mind integrated beliefs The ToM2 agent 1 j 2 j cid3 cid8 U U cid3 b0 ˆa t 1 j c1 cid4 ˆa 2 j c2 cid9 cid4 s 10 Example 3 Consider ToM2 agent plays rockpaperscissors similar Example 2 mental content given Table 7 When ToM2 agent considers opponents ﬁrstorder beliefs actions agent performs decision process ToM1 agent viewpoint opponent That calculates believes predicts based ﬁrstorder beliefs The agents model opponents ﬁrstorder beliefs captured 5 Results additional simulations different values c1 0 1 turned visually indistinguishable ones presented value c1 05 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 79 Table 7 Possible mental content ToM2 agent game rockpaperscissors Example 3 Order theory mind k ck bkR s0 bkP s0 bkS s0 0 05 03 02 1 09 04 05 01 2 01 03 03 04 b2 This agent believes opponent believe ﬁrstorder beliefs Firstly agent determines secondorder beliefs b2 actually zeroorder beliefs cid3 cid4 cid4 cid3 Φi Φi R b2 s0 P b2 s0 cid4 S b2 s0 Φi cid3 b2 s0 cid4 cid3 t 03 0 03 1 04 1 01 03 1 03 0 04 1 01 03 1 03 1 04 0 0 cid3 ai b2 s0 R Φi cid4 arg max ai Ai That ToM2 agent believes opponent predict playing R Secondly agent determines opponents prediction playing R inﬂuences zeroorder beliefs The agent explicitly model opponents conﬁdence ﬁrstorder theory mind Rather assumes value 08 conﬁdence The agent integrates ﬁrstorder beliefs b1 believes correspond opponents zeroorder beliefs prediction play R cid3 b1 R 08 cid3 b1 R 08 cid3 b1 R 08 cid4 cid4 cid4 U U U R s0 02 b1R s0 08 088 P s0 02 b1R s0 02 05 010 S s0 02 b1R s0 02 01 002 These integrated beliefs specify agent believes opponents beliefs concerning actions For example based application secondorder theory mind ToM2 agent believes opponent believes 88 probability play R From viewpoint opponent agent determines value playing possible actions given integrated beliefs opponent action cid3 cid3 cid3 Φi Φi Φi R U P U S U cid4 cid3 b1 R 08 cid3 b1 R 08 cid4 cid3 b1 R 08 s0 cid4 s0 cid4 s0 cid4 cid4 088 0 010 1 002 1 008 088 1 010 0 002 1 086 088 1 010 1 002 0 078 The action maximizes value represents agents prediction action opponent going play according secondorder theory mind cid4 cid3 b1 R 08 P t ˆa U cid3 cid4 s0 2 j j Based secondorder theory mind agent believes opponent play P To decision agent integrates zeroorder beliefs b0 ﬁrstorder prediction ˆa P Example 2 secondorder prediction ˆa P Example 2 shows agents zeroorder beliefs ﬁrstorder prediction opponent behaviour integrated Using conﬁdence c2 agent integrates belief opponent going play P In example agent conﬁdence c2 01 secondorder theory mind This results following integrated beliefs 2 j 1 j cid3 cid3 cid3 U U U U U U cid3 b0 P 09 cid3 b0 P 09 cid3 b0 P 09 cid4 cid4 cid4 P 01 P 01 P 01 cid4 cid4 cid4 R s0 09 005 0045 P s0 09 093 01 0937 S s0 09 002 0018 Based integrated beliefs agent determines value playing actions cid3 cid3 cid3 cid3 U cid3 cid3 U U R U P U S U Φi Φi Φi cid4 cid3 b0 P 09 cid3 b0 P 09 cid4 cid3 b0 P 09 P 01 cid4 cid4 s0 cid4 cid4 cid4 P 01 cid4 s0 cid4 P 01 s0 0018 0937 0919 0045 0018 0027 0937 0045 0892 80 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 The agent chooses play action maximizes value In case cid3 cid3 U U cid3 b0 P 09 cid4 t P 01 cid4 cid4 s0 arg max ai Ai cid3 cid3 ai U U cid3 b0 P 09 cid4 Φi cid4 cid4 P 01 s0 S Based integrated beliefs opponent going ToM2 agents choice play S 45 Higher orders theory mind agents For order theory mind available agent secondorder order k agent maintains additional belief structure bk These beliefs expand decision process modeling decision process k 1storder theory mind agent opponents point view The resulting prediction weighted decision process k 1storder theory mind point view For example ToM3 agent expands decision process ToM2 agent represented Eq 10 He modeling decision process ToM2 agent opponents point view That ToM3 agent calculates prediction opponent behaviour ˆa based thirdorder theory mind 3 j ˆa 3 j t j cid3 cid8 U U cid3 b1 t cid3 b2 s cid4 08 cid4 cid3 cid8 b2 t U j t cid3 b3 s cid4 08 cid9 s cid4 cid9 cid4 s 08 11 Once ToM3 agent determined prediction based thirdorder theory mind weights prediction decision process ToM2 agent represented Eq 10 The extent ToM3 agents prediction ˆa ToM2 opponents behaviour reﬂected behaviour determined conﬁdence 0 cid3 c3 cid3 1 thirdorder theory mind yields accurate predictions opponents behaviour That choice ToM3 agent given cid4cid9 3 j cid3 cid4 cid3 cid4 cid8 cid9 cid8 t U U U cid8 b0 t cid3 b1 t U cid3 b2 c1 t cid10 08 c2 cid13 cid4 cid3 b1 cid10 cid11cid12 cid13 ˆs1 cid3 b2 s cid11cid12 ˆs2 cid8 b2 t cid4 cid4 08 t cid3 U cid11cid12 ˆs3 cid3 cid8 U U t j cid10 cid3 b1 t cid4 cid3 b3 s j cid9 cid9 cid4 08 s 08 cid9cid4 c3 cid4 s cid13 46 Belief adjustment learning speed In previous subsections discussed agents different orders theory mind decide action play based current beliefs bk conﬁdence levels ck By placing position opponent viewing game perspective agent makes predictions action opponents going perform Each order theory mind available agent generates prediction The agent use accuracy predictions gain information opponents abilities repeated games adjust beliefs conﬁdence levels accordingly For example ToM2 agent learn opponent playing predicted secondorder theory mind ﬁrstorder theory mind consistently makes accurate predictions actions In case ToM2 agent start play ToM1 opponent ignore predictions secondorder theory mind altogether However important note ToM2 agent adjust behaviour advantage predictable behaviour opponent opponent trying In section agents update beliefs bk conﬁdence levels ck observe outcome game When agent plays unfamiliar opponent ﬁrst time beliefs bk initialized randomly conﬁdence levels ck initialized zero After round actual choice ai agent j opponent revealed At moment agent updates conﬁdence theory mind based accuracy predictions A ToM1 agent increases conﬁdence c1 ﬁrstorder theory mind ﬁrstorder prediction ˆa calculated Eq 5 correct In cases conﬁdence ﬁrstorder theory mind decreases This process represented update 1 j cid7 c1 1 λ c1 λ 1 λ c1 j cid8 ˆa j ˆa 1 j 1 j 12 0 cid3 λ cid3 1 agentspeciﬁc learning speed An agents learning speed indicates relative weight new informa tion determining beliefs An agent high learning speed determines opponent ToM0 agent based recent observations The ToM1 agents conﬁdence c1 ﬁrstorder theory mind reﬂects accuracy ﬁrst order theory mind recent games case An agent low learning speed depends experience built longer period time For higher orders theory mind agent additionally adjusts conﬁdences ck kthorder theory mind order k theory mind available Similar update conﬁdence c1 ﬁrstorder theory mind agent reduces conﬁdence ck kthorder theory mind corresponding prediction ˆa opponent k j H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 81 cid8 j However agent increases behaviour based application kthorder theory mind incorrect ˆa conﬁdence kthorder theory mind yields correct predictions predictions order j theory mind lower k incorrect If lower order n k theory mind ˆa agent increase conﬁdence kthorder theory mind That theory mind agents grow conﬁdent use higherorder theory mind results accurate predictions lower order theory mind This feature makes agents likely overestimate theory mind abilities opponent n j k j 1 λ ck ck ck λ 1 λ ck k j j cid8 ˆa 1 cid3 k j ˆa n j ˆa k j 13 When actual choice agent ai opponent j revealed agent updates beliefs bk Since zeroorder beliefs b0 represent agents beliefs concerning opponents behaviour beliefs updated opponents choice j This increasing belief opponent perform action j game state s S decreasing belief perform action Secondorder beliefs b2 specify agent believes opponent believe believes going That agents secondorder beliefs b2 beliefs concerning actions opponent updated choice j After update agent believes opponent believes believes strongly perform action j game state s S This true evennumbered orders theory mind available agent The belief structure bk evennumbered orders theory mind updated opponents choice j On hand oddnumbered orders theory mind actions agent These beliefs updated agents choice ai For example belief adjustment agent believes opponent believes strongly perform action ai game state s S encountered Using belief updating function U beliefs adjusted agents learning speed λ cid3 bi j λ bka j s U cid3 cid4 bi ai λ bkai s U cid4 j s ai s k j A j k odd j A j 14 15 That agent adjusts beliefs based forecasting technique exponential smoothing 69 Note adjustments apply game state s actions taken The agents learning speed λ determines quickly agent learns That higher value λ shows agent changes beliefs radically based new information At maximum λ 1 agent effectively believes action opponent performed determines future behaviour At extreme λ 0 agent learn change beliefs new information available This means agent learning speed λ 0 change behaviour The agents actively try model learning speed λ opponent Instead agent assumes opponent updates beliefs learning speed That computational agents consider possibility opponent reacts differently new information This means general beliefs agent attributes opponent structurally different actual beliefs An agent makes use theory mind considering position opponent viewpoint When agent makes use secondorder theory mind considers opponent knows viewpoint Since games consider symmetric information causes agents secondorder beliefs b2 resemble zeroorder beliefs b0 closely update That agent eventually believes ﬁrstorder theory mind opponent knows zeroorder beliefs Due restrictions learning speed λ Eqs 14 15 preserve normalization nonnegativity beliefs Similarly conﬁdences ci application ithorder theory mind remain limited range 0 1 Example 4 Consider ToM2 agent Example 3 mental content given Table 7 Once agent opponent decided action play actions revealed players receives payoff based actions Once outcome game revealed agent updates beliefs based observed Our calculations showed ToM2 agent discussed example played action ai S We assume opponent played j P agents learning speed λ 06 Table 8 lists agents predictions conﬁdences theory mind beliefs belief update De pending accuracy prediction application ithorder theory mind conﬁdence ci order theory mind increases decreases In example ﬁrstorder theory mind accurately predicted opponent play P j ˆa As result new conﬁdence c1 calculated Eq 12 1 j c1 1 λ c1 λ 1 06 09 06 096 82 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 Table 8 Beliefs conﬁdences theory mind belief update example ToM3 agent playing RPS Order theory mind Before update 0 05 03 02 1 P 09 04 05 01 2 P 01 03 03 04 ˆa j ci biR s0 biP s0 biS s0 After update 0 020 072 008 1 096 020 016 064 2 010 012 072 016 2 j P j secondorder theory mind correctly predicted opponent Table 8 shows ˆa play P However ﬁrstorder theory mind lower order secondorder theory mind ﬁrstorder theory mind correctly predicted action opponent conﬁdence c2 secondorder theory mind remains unchanged The actions actually played agent opponent change agents beliefs Each evennumbered order theory mind refers beliefs concerning opponents actions These beliefs updated reﬂect action opponent taken recently This increasing belief opponent perform action case P decreasing beliefs That agents zeroorder beliefs b0 updated update b0R s0 U b0P s0 U b0S s0 U cid4 cid3 b0 P 06 cid3 b0 P 06 cid4 cid3 b0 P 06 R s0 1 06 05 02 cid4 P s0 1 06 03 06 072 S s0 1 06 02 008 This means belief update agent believes 72 probability opponent going repeat action P round The agents secondorder beliefs b2 concern actions opponent Speciﬁcally agents secondorder beliefs b2 determine agent believes opponent believe believes actions The agent uses action j P actually performed opponent update secondorder beliefs The oddnumbered orders theory mind represent beliefs concerning agents actions These beliefs updated reﬂect agent chose action ai S For agents ﬁrstorder beliefs b1 results b1R s0 U b1P s0 U b1S s0 U cid4 cid3 b1 S 06 cid3 b1 S 06 cid4 cid3 b1 S 06 R s0 1 06 05 020 cid4 P s0 1 06 04 016 S s0 1 06 01 06 064 This means belief update agent believes opponent believes 64 probability repeat action S round 5 Results The agent model described Section 4 implemented Java performance tested settings described Section 2 For rockpaperscissors game variations game trial consisted agent plays 20 consecutive games opponent6 An agents trial score average agents game scores games trial The graphs section depict average trial score averaged 500 trials Since Limited Bidding complex game longer sequence needed learn model opponent Each trial game consisted agent plays 50 consecutive games opponent Our results qualitatively similar longer trials 100 games instead In section performance measured average trial score focal agent function learning speed λi learning speed λ j opponent The ﬁgures section simulation results 002 step learning speeds range λi λ j 0 1 We report results simulations focal agent exactly order theory mind higher opponent In simulations difference theory mind ability focal agent opponent larger order performance focal agent turned similar 6 We compared results trials 20 games longer trials 50 100 games qualitative differences H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 83 Fig 6 Average performance theory mind agents playing rockpaperscissors opponents lower order theory mind Performance averaged 500 trials 20 consecutive games Insigniﬁcant results p 001 highlighted red For interpretation references color ﬁgure legend reader referred web version article 51 Rockpaperscissors Fig 6 shows ability represent mental content affects performance agents RPS game function learning speed λi focal agent learning speed λ j opponent Higher lighter areas indicate focal agent won games lost lower darker areas opponent upper hand To emphasize shape surface grid appears plane projected surface plane zero performance appears semitransparent surface ﬁgure Red areas indicate performance signiﬁcantly different zero signiﬁcance level α 001 Fig 6a shows ToM1 agent learning speed λi 0 compete opponent When agent learn opponents behaviour loses nearly rounds Similarly opponent loses nearly rounds learn λ j 0 This shows zeroorder theory mind agents ﬁrstorder theory mind agents successfully model opponent performs action 84 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 The ﬁgure shows ToM1 agent outperforms ToM0 opponent Whenever ToM1 agents learning speed λi 01 average win rounds loses obtain positive score The ToM1 agents score particularly high opponent learn high rate case agent wins rounds When ToM0 opponent learns low rate average score ToM1 agent reduced The relatively low performance ToM1 agent slow learning opponents fact learning speed determines agents memory A ToM0 agent high learning speed adapts new situations quickly quickly forgets information previous rounds When faced unpredictable opponent ToM0 agent high learning speed choose erratically conﬁdence In case ToM0 agent believes opponent repeat action performed time met For ToM1 opponent represents predictable situation use advantage Conversely ToM0 agent low learning speed retains beliefs longer time When encountering unpredictable opponent ToM0 agent start playing little conﬁdence That probability distribution modeled b0 gradually come resemble uniform distribution This causes ToM0 agent low learning speed play action weakly believes slightly better choice rest This makes diﬃcult ToM1 opponent predict token ToM0 agent low learning speed play choice robust small deviations beliefs Although ToM1 agent performs better opponent learns quickly opponent learns slowly performance ToM1 agent largely independent quality model When ToM1 agent makes use theory mind assumes opponent reacts new information way That agent assumes opponent share learning speed However ﬁgure increase performance line equal learning speeds That cost assuming equal learning speeds low RPS Fig 6b shows performance ToM2 agent playing RPS ToM1 opponent Note Fig 6b similar Fig 6a As ToM1 agent ToM2 agent performs best playing RPS ToM1 opponent opponent learn high speed ToM2 agent diﬃculty modeling ToM1 agent learns slowly This shows application higherorder theory mind beneﬁt agent playing RPS Performance ToM2 agent playing RPS ToM1 opponent nonetheless slightly lower ToM1 agent playing RPS ToM0 opponent Figs 6a 6b suggest application higher orders theory mind beneﬁts agent However performance ToM3 agent playing RPS ToM2 agent shown Fig 6c poor comparison Although ToM3 agent outperforms ToM2 opponent lower margin The average score ToM3 agent exceeds 05 opponent learning speed zero When facing ToM2 opponent low learning speed average score ToM3 agent learns quickly negative Although ToM3 agent outperform ToM2 opponent small margin Fig 6d shows ToM4 agent longer outperforms ToM3 opponent RPS In scenario outcome game dependent agents highest learning speed longer theory mind abilities In Fig 6d seen fact ToM4 agent obtains positive outcome average learning speed λi higher learning speed λ j opponent In summary ability use theory mind beneﬁt agent game RPS As hypothesized cf hypothesis HRPS Section 24 ToM1 agent ToM2 agent outperform opponents lower order theory mind The performance ToM3 agent ToM4 agent suggests limit effectiveness application higher orders theory mind However rockpaperscissors involves possible opponent actions game leaves room unique predictions opponents action The low performance ToM3 ToM4 agents caused speciﬁc characteristics RPS game limit effectiveness application higher orders theory mind The section describes game actions order differentiate alternative explanations 52 Elemental rockpaperscissors In Section 212 introduced elemental rockpaperscissors variation classical RPS game agents choose action set ﬁve actions ERPS preserves feature RPS action defeated exactly action Differences performance theory mind agents play RPS play ERPS allow determine features game structure affect effectiveness higher orders theory mind competitive games The results ERPS shown Fig 7 Our expectation performance theory mind agents playing ERPS good performance RPS partially correct Similar results theory mind agents playing RPS Fig 7a shows ToM1 agent outperforms ToM0 opponent Fig 7b shows ToM2 agent outper forms ToM1 opponent However performance game ERPS slightly reduced compared situation playing RPS Especially agent opponent learns low speed diﬃcult theory mind agent model opponent game ERPS RPS The main qualitative difference RPS ERSP shown performance ToM3 agent performance ToM4 agent depicted Fig 7c Our results RPS showed diﬃcult ToM3 agent model oppo H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 85 Fig 7 Average performance theory mind agents playing ERPS opponents lower order theory mind Performance averaged 500 trials 20 consecutive games Insigniﬁcant results p 001 highlighted red nent correctly In Fig 6c presents relatively low performance opponent zero learning speed λ 0 In contrast Fig 7c shows ToM3 agent diﬃculty playing ERPS similar opponent Since richer action space ERPS increases performance ToM3 agent playing opponent learn structure game inﬂuences effectiveness theory mind However performance ToM3 agent playing ERPS ToM2 opponent poor comparison performance ToM1 ToM2 agents playing ERPS opponents lower order theory mind Although ToM1 ToM2 agents clearly perform opponents lower order theory mind ToM3 agent outperforms ToM2 agent small margin Fig 7d shows performance ToM4 agent playing ERPS ToM3 opponent Like ToM3 agent peak performance ToM4 agent playing opponent learning speed λ j 0 shown ﬁgure indicates ToM4 agent diﬃculty distinguishing agents learning speed zero agents lower order theory mind However ability use fourthorder theory mind present agent advantages ERPS thirdorder theory mind Fig 7d shows ToM4 agent plays ERPS ToM3 86 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 opponent obtains positive score average learning speed λi higher learning speed λ j opponent That ToM4 agent plays ERPS ToM3 opponent highest learning speed expected win In summary investigate game ERPS determine limited choice actions agents playing RPS effect advantage making use theory mind The results conﬁrm expectations cf hypothesis H ERPS Section 24 agents choose limited action space higher orders theory mind experience diﬃculty modeling opponent However limited action space explain relatively poor performance ToM3 agent playing ToM2 opponent RPS ERPS 53 RockpaperscissorslizardSpock The game rockpaperscissorslizardSpock described Section 213 variation ERPS action defeated exactly actions As result best response action unique Our expectation harder predict opponents behaviour case performance theory mind agents reduced Fig 8 shows generally case In game RPSLS advantage making use theory mind reduced compared RPS ERPS Fig 8a shows performance ToM1 agent playing RPSLS ToM0 opponent Unlike RPS ERPS ToM1 agent performs better learning speed matches learning speed opponent In Fig 8 reﬂected high scores line equal learning speeds λi λ j In case ToM1 agents model opponents beliefs matches actual beliefs However modeling opponents beliefs correctly yields agent higher score expected win cases learning speed match opponent Performance ToM1 agent particularly low opponent maximum learning speed λ j 1 In case considers agents actions previous game ignores information previous games For example ToM1 agent plays paper game RPSLS ToM0 opponent believe repeat action future games This means ToM0 opponent actions lizard scissors maximize expected payoff chooses actions 50 probability On hand ToM0 opponent learns lower speed λ j 1 completely replace beliefs new information available In case ToM0 opponent believes small probability ToM1 agent play action paper In general prevents actions having exactly expected payoffs Since agents choose action yields highest expected payoff causes ToM0 opponent choose possible actions certainty As Fig 8a shows distinct types behaviour diﬃcult ToM1 agent accurately model opponent In present model ToM1 agent learning speed λi 1 believes opponent learning speed As result believes single action maximizes opponents expected payoff However opponent maximal learning speed λ j 1 actually randomizes choice possible actions The ToM1 agent expected predict opponents behaviour incorrectly half cases Fig 8b shows performance ToM2 agent playing RPSLS ToM1 opponent Similar ToM1 agent performance ToM2 agent low playing RPSLS opponent learns maximum speed λo 1 The ToM2 agent particular diﬃculties modeling ToM1 opponent RPSLS learning speed λi low In case ToM2 agent outperformed opponent lower order theory mind However ToM2 agent average win learning speed λi 07 The low performance ToM2 agent RPSLS learns low speed translates beneﬁt ToM3 agent Fig 8c shows performance ToM3 agent playing RPSLS ToM2 opponent When opponents learning speed λo low ToM3 agent performs better RPSLS games RPS ERPS However ToM3 agent performs poorly ToM2 opponent learns quickly In particular facing ToM2 opponent learns maximal learning speed λ j 1 ToM3 agent obtains positive score average learns maximal learning speed λi 1 Similar games RPS ERPS performance ToM4 agent playing RPSLS ToM3 opponent determined player highest learning speed shown Fig 8d However unlike RPS ERPS ToM4 agent small advantage ToM3 opponent That learning speed λi ToM4 agent learning speed λ j ToM3 opponent close ToM4 agent expected win predicted chance performance In summary results game RPSLS effectiveness theory mind strongly related predictability lowerorder agents Theory mind agents perform poorly opponent indifferent possible actions behaviour predictable This conﬁrms expectations relationship performance theory mind agents predictability opponents cf hypothesis H RPSLS Section 24 54 Limited Bidding Unlike variations rockpaperscissors Limited Bidding extensive form game spans rounds Al unique bestresponse opponent action multiple responses yield positive outcome H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 87 Fig 8 Average performance theory mind agents playing RPSLS opponents lower order theory mind Performance averaged 500 trials 20 consecutive games Insigniﬁcant results p 001 highlighted red For interpretation references color ﬁgure legend reader referred web version article To determine advantage having ability explicitly represent mental states game LB agents differ order theory mind placed competition Fig 9 shows performance theory mind agents function learning speed λi focal agent learning speed λ j opponent Performance normalized range 1 means focal agent achieved maximum possible payoff 1 case opponent achieved maximum possible payoff As lighter areas highlight agent performed better opponent darker areas opponent obtained higher average score Fig 9a shows ToM1 agents predominantly obtain positive score playing ToM0 opponents A ToM1 agent performs facing opponent learn shown high scores opponents learning speed zero λ j 0 The bright area line equal learning speeds indicates advantage ToM1 agent particularly high learning speeds equal In case ToM1 agents implicit assumption opponent learning speed correct Fig 9a shows ToM1 agent fails accurately model opponent average obtain positive score learning speed λi 008 88 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 Fig 9 Average performance theory mind agents playing Limited Bidding opponents lower order theory mind Performance averaged 50 trials 50 consecutive games Insigniﬁcant results p 001 highlighted red For interpretation references color ﬁgure legend reader referred web version article As cases RPS ERPS described applying theory mind appears effective ToM1 agent playing ToM0 opponent low learning speed In LB ToM0 opponent high learning speed changes beliefs radically high conﬁdence That effect random initialization beliefs impact opponent behaviour learning speed high learning speed low For ToM1 agent ToM0 opponent high learning speed represents predictable situation use advantage Fig 9b shows ToM2 agent advantage ToM1 opponent However Fig 9b shows features Fig 9a brighter area main diagonal equal learning speeds ToM2 agents playing ToM1 opponents obtain score average 013 lower score ToM1 agents playing ToM0 agents As result ToM2 agent needs higher learning speed λi 012 order obtain average positive score playing ToM1 agent Note like ToM1 agent ToM2 agent diﬃculty obtaining advantage playing opponent low learning speed learning speed high Similar results variations RPS application ﬁrstorder secondorder theory mind present agent clear advantage opponents lower order theory mind However advantage H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 89 ToM3 agent ToM2 opponent marginal Fig 9c shows ToM3 agent barely outperforms ToM2 agent average score exceeds 01 ToM2 opponent zero learning speed Moreover appears ToM3 agent average obtain positive score learning speed λi 032 Fig 9c shows ToM2 opponent learning speed 0 λ j 01 performance ToM3 agent fall plane zero performance That ToM3 agent longer guaranteed win playing ToM2 opponent value learning speed λ j Fig 9d shows ToM4 agent fails obtain advantage kind ToM3 agent playing LB When agent opponent learns low speed game average end tie The learning speed agent learning speed opponent strong effect expected outcome game In summary agent performance LB clearly shows diminishing returns higher orders theory mind The use ﬁrstorder secondorder theory mind allows agents obtain reliable advantage opponents limited ability explicitly represent mental states However specialized thirdorder theory mind barely allows ToM3 agents outperform ToM2 agents fourthorder theory mind yield agent advantage obtained thirdorder theory mind Qualitatively results similar described RPS game Section 51 55 Summary results To determine effectiveness theory mind simulated computational theory mind agents described Section 4 playing competitive games In hypothesis H RPS Section 24 predicted higher orders theory mind beneﬁt agents competitive settings Our results support conclusion sense ability use ﬁrstorder secondorder theory mind allows agents obtain clear advantage opponents lower order theory mind However orders theory mind second additional advantage marginal This pattern results consistent variations rockpaperscissors investigated As predicted hypothesis HERPS larger action space elemental rockpaperscissors advantageous higherorder theory mind agents instances However larger action space remove diminishing returns higher orders theory mind Qualitatively similar results multistage limited bidding game conﬁrms hypothesis HLB The relatively limited advantage ToM3 agents playing ToM2 opponents appears caused model ToM2 opponent holds ToM3 agent Agents start playing ToM0 agents When ToM3 agent competition ToM2 opponent notice predictions based ﬁrstorder theory mind correct This causes agents grow conﬁdent application ﬁrstorder theory mind As result gradually start play ToM1 agents When happens predictions based ﬁrstorder theory mind accurate predictions based secondorder theory mind increasingly accurate increasing conﬁdence application secondorder theory mind Both agent opponent start playing ToM2 agents At point ToM2 opponent longer model behaviour agent That notice predictions correct Because lose conﬁdence application ﬁrstorder secondorder theory mind gradually start play ToM0 agent When ToM3 agent tries advantage playing ToM1 agent ToM2 opponent able recognize behaviour grow conﬁdent predictions based secondorder theory mind This causes ToM2 opponent constantly changing strategy hinders ToM3 agent efforts trying model behaviour The relation performance theory mind agent predictability opponents behaviour reﬂected results rockpaperscissorslizardSpock game As predicted hypothesis H RPSLS higherorder theory mind agents perform poorly game RPS ERPS 6 Discussion conclusion The Machiavellian intelligence hypothesis 38 evolution theory mind predicts competitive settings use higherorder theory mind presents individuals evolutionary advantage But beneﬁts making use higherorder theory mind outweigh costs For example settings purestrategy Nash equilibrium exists individuals use theory mind unlikely outperform individuals play Nash strategy explicitly reasoning opponents mental states In cases simple heuristics superior methods rely sophisticated cognitive abilities like theory mind 7071 However humans possess ability use higherorder theory mind suggests settings cognitively demanding skill useful For example secret codes negotiating climate change control heuristics In paper agentbased models ability use theory mind present individuals advantage opponents lack ability certain competitive settings The advantage qualitatively similar competitive games discussed included repeated singleshot 90 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 games rockpaperscissors elemental rockpaperscissors rockpaperscissorslizardSpock repeated extensive form game limited bidding To surprise results diminishing returns higher orders theory mind Although ﬁrstorder secondorder theory mind agents clearly outperform opponents limited abilities represent mental content thirdorder theory mind agents marginally outperform secondorder theory mind opponents Fourthorder theory mind beneﬁcial speciﬁc circumstances These diminishing returns higher orders theory mind related number actions available agents Increasing action space agents choose increase performance thirdorder theory mind agent competition secondorder theory mind opponent Although theory mind allows agents outperform opponents limited ability explicitly represent mental states theory mind eﬃcient use memory capacity Additional experiments simple games rockpaperscissors agent beneﬁt remembering past behaviour opponent representing mental states However complex games Limited Bidding theory mind appears beneﬁts remembering past opponent behaviour Agents capable associative learning strategies theory mind strategies choose use theory mind task simple Tasks need suﬃciently complex elicit theory mind response In model assumed agents choose action perform rationally That agents choose perform action believe yield highest possible payoff This results predictability beneﬁts theory mind agents shown results game rockpaperscissorslizardSpock When opponent indifferent actions sense actions maximize expected payoff effectiveness theory mind suffers However slight asymmetry actions action appears slightly better alternative creates focal point 72 agents In case opponent choose action believes yield better payoff However behaviour predicted higherorder theory mind agents An agent lower order theory mind able avoid falling victim opponent capable theory mind higher order choose action play completely rationally For example agents choose action perform probability proportional expected payoff Similarly utility proportional beliefs 73 beneﬁt effectiveness theory mind agents belief opponents choose action pro portionally utility In case theory mind agent reliant opponent playing completely rationally Future research reveal balance achieved exploiting weaknesses opponents actions remaining unpredictable avoid exploitation In model zeroorder theory mind agent believe opponent behaves randomly 1011 attempts model opponents behaviour assuming past actions predict future A higher order theory mind agent simultaneously updates model mental content opponent belief opponents theory mind abilities It interesting compare effectiveness theory mind direct competition classical strategies heuristics In future work aim investigate theory mind effective complex interaction settings including partners Theory mind play important role cooperative settings example teamwork mixedmotive settings negotiations cf 37 This provide insights automated agents share environment human agents automated negotiation 34 Acknowledgements This work supported Netherlands Organisation Scientiﬁc Research NWO Vici grant NWO 27780001 awarded Rineke Verbrugge project Cognitive systems interaction Logical computational models higher order social cognition We like thank anonymous reviewers helpful comments References 1 H Weerd B Verheij The advantage higherorder theory mind game limited bidding J van Eijck R Verbrugge Eds Proc Workshop Reason Other Minds Log Cogn Perspect CEUR Workshop Proceedings 2011 pp 149164 2 H Weerd R Verbrugge B Verheij Higherorder social cognition game rockpaperscissors A simulation study G Bonanno H van Ditmarsch W van der Hoek Eds Proc 10th Conf Log Found Game Decis Theory 2012 pp 218232 3 S Kraus Negotiation cooperation multiagent environments Artif Intell 94 1997 7997 4 R Lin S Kraus J Wilkenfeld J Barry Negotiating bounded rational agents environments incomplete information automated agent Artif Intell 172 2008 823851 5 R Fagin J Halpern Y Moses M Vardi Reasoning About Knowledge MIT Press Cambridge MA 1995 second edition 2003 6 H van Ditmarsch W van der Hoek B Kooi Dynamic Epistemic Logic Springer 2007 7 P Gmytrasiewicz E Durfee A rigorous operational formalization recursive modeling Proc First Int Conf MultiAgent Syst 1995 pp 125132 8 P Gmytrasiewicz P Doshi A framework sequential planning multiagent settings J Artif Intell Res 24 2005 4979 9 A Pfeffer Networks inﬂuence diagrams A formalism representing agents beliefs decisionmaking processes J Artif Intell Res 33 2008 109147 10 W Yoshida R Dolan K Friston Game theory mind PLoS Comput Biol 4 2008 e1000254 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 91 11 C Camerer T Ho J Chong A cognitive hierarchy model games Q J Econ 119 2004 861898 12 D Stahl P Wilson On players models players Theory experimental evidence Games Econ Behav 10 1995 218254 13 M Bacharach DO Stahl Variableframe leveln theory Games Econ Behav 32 2000 220246 14 H Simon A mechanism social selection successful altruism Science 250 1990 16651668 15 D Kahneman Maps bounded rationality Psychology behavioral economics Am Econ Rev 2003 14491475 16 D Premack G Woodruff Does chimpanzee theory mind Behav Brain Sci 1 1978 515526 17 J Perner H Wimmer John thinks Mary thinks Attribution secondorder beliefs 5 10 year old children J Exp Child Psychol 39 1985 437471 18 T Hedden J Zhang What think I think think Strategic reasoning matrix games Cognition 85 2002 136 19 L Flobbe R Verbrugge P Hendriks I Krämer Childrens application theory mind reasoning language J Log Lang Inf 17 2008 417442 20 B Meijering H van Rijn N Taatgen R Verbrugge I know think I think Secondorder theory mind strategic games diﬃcult Proc 33rd Annu Conf Cogn Sci Soc 2011 pp 24862491 21 V Crawford N Iriberri Fatal attraction Salience naïvete sophistication experimental HideandSeek games Am Econ Rev 2007 17311750 22 H Wimmer J Perner Beliefs beliefs Representation constraining function wrong beliefs young childrens understanding deception Cognition 13 1983 103128 23 I Apperly Mindreaders The Cognitive Basis Theory Mind Psychology Press Hove UK 2011 24 B Meijering L Van Maanen H Van Rijn R Verbrugge The facilitative effect context secondorder social reasoning Proc 32nd Annu Conf Cogn Sci Soc 2010 pp 14231429 25 M Tomasello Why We Cooperate MIT Press Cambridge MA 2009 26 M Schmelz J Call M Tomasello Chimpanzees know inferences Proc Natl Acad Sci USA 108 2011 30773079 27 J Burkart A Heschl Understanding visual access common marmosets Callithrix jacchus Perspective taking behaviour reading Anim Behav 73 2007 457469 28 J Kaminski J Call M Tomasello Goats behaviour competitive food paradigm Evidence perspective taking Behaviour 143 2006 13411356 29 J Kaminski J Brauer J Call M Tomasello Domestic dogs sensitive humans perspective Behaviour 146 2009 979998 30 N Clayton J Dally N Emery Social cognition foodcaching corvids The western scrubjay natural psychologist Philos Trans R Soc B Biol Sci 362 2007 507 31 T Bugnyar Knowerguesser differentiation ravens Others viewpoints matter Proc R Soc B Biol Sci 278 2011 634640 32 D Penn D Povinelli On lack evidence nonhuman animals possess remotely resembling theory mind Philos Trans R Soc B Biol Sci 362 2007 731 33 P Carruthers Metacognition animals A skeptical look Mind Lang 23 2008 5889 34 E van der Vaart R Verbrugge C Hemelrijk Corvid recaching theory mind A model PLoS ONE 7 2012 e32904 35 M Balter Killjoys challenge claims clever animals Science 335 2012 10361037 36 B Hare J Call M Tomasello Do chimpanzees know conspeciﬁcs know Anim Behav 61 2001 139151 37 R Verbrugge Logic social cognition The facts matter computational models J Philos Log 38 2009 649680 38 A Whiten R Byrne Machiavellian Intelligence II Extensions Evaluations Cambridge University Press Cambridge 1997 39 J Epstein Generative Social Science Studies Agentbased Computational Modeling Princeton University Press Princeton NJ 2006 40 J Epstein Agentbased computational models generative social science Complexity 4 1999 4160 41 W Jager R Popping H Van Sande Clustering ﬁghting twoparty crowds Simulating approachavoidance conﬂict J Artif Soc Soc Simul 4 2001 118 42 M Harbers R Verbrugge C Sierra J Debenham The examination informationbased approach trust Coord Organ Inst Norms Agent Syst III 2008 pp 7182 43 E van der Vaart B Boer A Hankel B Verheij Agents adopting agriculture Modeling agricultural transition Proc 9th Int Conf Anim Animats Simul Adapt Behav 2006 pp 750761 44 H Gintis Strong reciprocity human sociality J Theor Biol 206 2000 169179 45 R Boyd H Gintis S Bowles P Richerson The evolution altruistic punishment Proc Natl Acad Sci 100 2003 35313535 46 H Weerd R Verbrugge Evolution altruistic punishment heterogeneous populations J Theor Biol 290 2011 88103 47 A Cangelosi D Parisi Simulating Evolution Language Springer 2002 48 B Boer The Origins Vowel Systems Oxford University Press USA 2001 49 I Slingerland M Mulder E van der Vaart R Verbrugge A multiagent systems approach gossip evolution language Proc 31st Annu Meet Cogn Sci Soc 2009 pp 16091614 50 D Billings The ﬁrst international RoShamBo programming competition ICGA J 23 2000 4250 51 D Egnor Iocaine powder ICGA J 23 2000 3335 52 J Von Neumann Zur Theorie der Gesellschaftsspiele Math Ann 100 1928 295320 53 K Binmore Playing Real Oxford University Press Oxford UK 2007 54 W Wagenaar Generation random sequences human subjects A critical survey literature Psychol Bull 77 1972 65 55 A Rapoport D Budescu Randomization individual choice behavior Psychol Rev 104 1997 603 56 R West C Lebiere D Bothell Cognitive architectures game playing human evolution Cognition MultiAgent Interaction From Cognitive Modeling Social Simulation Cambridge University Press 2006 pp 103123 57 R Cook G Bird G Lünser S Huck C Heyes Automatic imitation strategic context Players rockpaperscissors imitate opponents gestures Proc R Soc B Biol Sci 2011 58 S Kass K Bryla Rock paper scissors Spock lizard httpwwwsamkasscomtheoriesRPSSLhtml 2009 accessed 29122012 59 E De Bono Edward Bonos Super Mind Pack Expand Your Thinking Powers Strategic Games Mental Exercises Dorling Kindersley Publishers Ltd London UK 1998 60 M Osborne A Rubinstein A Course Game Theory MIT Press Cambridge MA 1994 61 C Bicchieri Common knowledge backward induction A solution paradox Proc 2nd Conf Theor Asp Reason Knowl 1988 pp 381393 62 J Von Neumann O Morgenstern Theory Games Economic Behavior Princeton University Press Princeton NJ 1944 commemorative edition 2007 63 M Davies The mental simulation debate Philos Issues 5 1994 189218 64 S Nichols S Stich Mindreading An Integrated Account Pretence SelfAwareness Understanding Other Minds Oxford University Press USA 2003 65 S Hurley The shared circuits model SCM How control mirroring simulation enable imitation deliberation mindreading Behav Brain Sci 31 2008 122 66 R Falk C Konold Making sense randomness Implicit encoding basis judgment Psychol Rev 104 1997 301 67 J Barwise On model theory common knowledge The Situation Logic CSLI Press Stanford CA 1989 pp 201220 92 H Weerd et al Artiﬁcial Intelligence 199200 2013 6792 68 H van Ditmarsch J van Eijck R Verbrugge Common knowledge common belief J van Eijck R Verbrugge Eds Discourses Social Software Amsterdam University Press Amsterdam 2009 pp 99122 69 R Brown Smoothing Forecasting Prediction Discrete Time Series PrenticeHall Englewood Cliffs NJ 1963 70 G Gigerenzer R Hertwig T Pachur Heuristics The Foundations Adaptive Behavior Oxford University Press 2011 71 D Kahneman Thinking Fast Slow Farrar Straus Giroux New York 2011 72 R Sugden A theory focal points Econ J 105 1995 533550 73 C Bach A Perea Utility proportional beliefs httpepicenternameResearchhtml 2011 accessed 29122012