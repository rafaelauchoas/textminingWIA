Artiﬁcial Intelligence 167 2005 1330 wwwelseviercomlocateartint Word sense disambiguation pictures Kobus Barnard Matthew Johnson b Computer Science Department University Arizona USA b Department Engineering University Cambridge USA Received 25 July 2004 received revised form 21 February 2005 accepted 14 April 2005 Available online 11 August 2005 Abstract We introduce images word sense disambiguation conjunction traditional text based methods The approach based recently developed method automat ically annotating images statistical model joint probability image regions words The model learned data base images associated text To use model word sense disambiguation constrain predicted words possible senses word consideration When word prediction constrained narrow set choices possible senses reliable We report experiments resulting sense probabilities augmenting state art text based word sense disambiguation algorithm In order evaluate approach developed new corpus ImCor consists substantive portion Corel image data set associated disambiguated text drawn SemCor corpus Our experiments corpus suggest visual information useful disambiguating word senses It illustrates associated nontextual information image data help ground language meaning 2005 Elsevier BV All rights reserved Keywords Word sense disambiguation Image autoannotation Region labeling Statistical models Corresponding author Email address kobuscsarizonaedu K Barnard 00043702 matter 2005 Elsevier BV All rights reserved doi101016jartint200504009 14 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 Fig 1 Five senses bank illustrated images Corel dataset 1 Introduction A signiﬁcant portion words natural language number possible meanings senses depending context This illustrated Fig 1 arguably overused bank example A priori word bank number meanings including ﬁnancial institution step edge snow bank river bank Words spelled different meanings polysemes confuse attempts automatically attach meaning language As ambiguous words natural language texts word sense disambiguationdetermining exact sense wordshas identiﬁed important component natural language processing studied researchers leading large body literature 2427324041474950 Since words spelled resolving mean requires consideration context A purely natural language based approach considers words near question Thus bank example words like ﬁnancial money strong hints ﬁnancial institution sense meant Interestingly despite work number innovative ideas signiﬁcantly better choosing common sense remains difﬁcult 47 In paper develop method image information disambiguate senses words We posit image information orthogonal source infor K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 15 mation distinguishing senses In extreme case disambiguation nearby text impossible sentence He ate lunch bank In cases alternative sources information offer attractive possibilities grounding word meanings Even essential nontextual information capacity helpful Our method associated visual information junction text based methods Naturally images available fall nonimage methods Incorporation vision word sense disambiguation process novel approach As far know word sense disam biguation methods use document text andor additional text carrying domain document context semantic information However acknowledge related work WordNet 42 propagate sense semantic information feature based classes context multimedia information systems 1213 To use image information exploit recently developed method predicting likely words images 5922 The method based statistical model joint proba bility distribution words image region features The model learned training set images associated text Additional details provided Section 3 To use model word sense disambiguation constrain predicted words set senses word consideration In general word prediction constrained narrow set choices possible senses reliable We report experiments resulting sense probabilities augmenting state art text based word sense disambiguation algorithms In order evaluate approach necessary develop new corpus ImCor consists substantive portion Corel image data base associated disam biguated text drawn SemCor corpus We ImCor available research purposes 31 Our experiments corpus suggest visual information useful disambiguating word senses This work suggests approaches exploiting multiple data modes increase ability automatically search browse multimedia information For example text data web augmented image data Searches based text currently use information cases helpful While computational methods effectively understanding arbitrary visual data long way visual features improve rankings query results require standing For example text data better sense disambiguated image data unambiguous query better executed data 2 Disambiguating words textual content Research automatic methods disambiguating word senses resulted va riety ways surrounding text textual context infer word sense Disambiguating sense semantic problem underlying assumption word disambiguated semantically linked nearby words text tends semantically coherent Cooccurrence statistics reﬂect semantic linking searchers developed methods based statistical models senses 16 A large number methods attempt quantify linking known word semantics For 16 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 example word classes deﬁned Thesaurus integrated combined weight indicators textual context 48 Going word sense disambiguation al gorithms use semantic network WordNet 42 WordNet machinereadable dictionary covering large proportion English language 152059 words organized 115424 sets synonyms synsets It provides relationships sets commonly hypernym relationship The graph created hypernym relationships forms tree node hypernym children The path connecting words deﬁne semantic distances word sense disambiguation algorithms 2203541 Usage statistics helpful word sense disambiguation In WordNet sense number roughly corresponds decreasing common usage frequency ﬁrst WordNet sense considers commonly Going researchers exploited SemCor senseattributed corpus 28414346 SemCor short Word Net Semantic Concordance 26 consists 25 Brown corpus 25 ﬁles fully tagged partofspeech sense disambiguated A number word sense disambiguation methods compared Senseval conferences 12333 Based results second Senseval chose implement algorithm based iterative word sense disambiguation SMUaw 41 We intrigued fact choosing common sense according WordNet evaluates higher algorithms currently use 47 Thus implemented algorithm provides usage distribution senses provide additional evaluation algorithm 36 There work incorporating multiple alternative knowledge sources help disambiguate words context In 19 world knowledge derived alternative synset contexts obtained WordNet supplement learning algorithm showed marked improvement unaided version Another interesting example 44 word disambiguated feature set formed based multiple sources including speech neighboring words morphological form unordered set neighboring words local collocations verbobject syntactic relation During training disambiguated sentences mined features testing feature set obtained word compared training sets The proposal similarity directly proportional probability sense word training set correct sense test word While relied surrounding text obtain feature set testing training data potentially come number different sources This similar efforts 1137 indicate intelligent efﬁcient integration multiple knowledge sources result enhanced performance variety algorithms dealing textual analysis general word sense disambiguation particular 3 Predicting words images To integrate image information text data exploit recent work linking im ages words 5922 The general approach build statistical models occurrence image regions words A key assumption words linked K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 17 Fig 2 Illustration region labeling Each region labeled maximally probable word distrib ution entire vocabulary In word sense disambiguation task combine probability distributions regions provide annotation relevant entire image We emphasize region based approaches believe good image annotation requires reasoning image components images regions These models predict words image regions region labeling entire images autoannotation Region labeling illustrated Fig 2 To label regions probabilistic inference models provides posterior probabil ity distribution vocabulary region label region maximal probability We ﬁt models large image data sets asso ciated text Critically require words training data identiﬁed belonging particular image regions data rare These models owe previous work text domain 29 statistical ma chine translation 141538 A number additional methods linking image features words recently proposed 17243034 considered word sense disambiguation For work use models 5 In particular use dependent model D2 linear topology We use hierarchical clustering version better suited characterizing known data set suited predicting words novel images We ﬁrst segment images regions coherent color texture This sim pliﬁcation essentially data reduction step allowing semantic analysis groups pixels In work use modiﬁed version Normalized Cuts 45 seg mentation For image region compute feature vector representing color texture size position shape 5 color context 8 More speciﬁcally Size represented portion image covered region Position represented coordinates region center mass normalized image dimensions Color represented average standard deviation r RR G B g GR G B S R G B region We use color space instead RGB reduce correlation bands Texture represented average variance 16 ﬁlter responses We use 4 difference Gaussian ﬁlters different sigmas 12 oriented ﬁlters aligned 30 degree increments See 45 additional details references approach texture 18 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 Shape represented ratio area perimeter squared moment inertia center mass ratio region area convex hull Color context represented colors representing color adjacent regions restricted 90 degree wedges 8 A region feature vector referred blob 18 Our language model commonly bag words word order Various preprocessing strategies increase likelihood words connected visual attributes image regions 6 In work use subset SemCor 26 vocabulary described Section 6 To statistically link blobs words assume hidden factors concepts responsible generating words blobs associated factor This binding generation leads capacity link words blobs We assume observations image associated text generated multi ple draws hidden factors nodes Without modeling image generation compositionalregion models arbitrary conﬁguration handle images known regions different arrangementswe need model possible combinations entities For example model tigers grass tigers water tigers sand Clearly tiger model reused possible We model joint probability particular blob b word w P w b cid1 l P w lP b lP l 1 l indexes concepts P l concept prior P w l frequency ta ble P b l Gaussian distribution features We assume diagonal covariance matrix independent features ﬁtting covariance generally difﬁcult large number features This independence assumption troublesome require conditional independence given concept Intuitively concept generates image regions according particular Gaussian distribution concept Similarly generates ore words image according learned table probabilities To blob oriented expression 1 entire image assume observed blobs B yield posterior probability P l B proportional sum P l b Words generated conditioned blobs P w B cid1 l P w lP l B assumption cid1 P l B P l b b Bayes rule compute P l b P b lP l 2 3 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 19 Some manipulation 7 shows equivalent assuming word posterior image proportional sum word posteriors regions P w B Ncid1 b P w b 4 We limit sum blobs largest N blobs work N sixteen While training normalize contributions blobs words mitigate effects differing numbers blobs words training images The probability observed data W B given model P W B cid3cid1 cid2 cid4 maxNb Nb P b lP l bB l cid2 cid3cid1 wW l cid4 maxNw Nw P w lP lB 5 maxNb similarly maxNw maximum number blobs words training set image Nb similarly Nw number blobs words particular image P l B computed 3 Since know concept responsible observed blobs words training data determining maximum likelihood values model parameters P w l P b l P l tractable We estimate values parameters expectation maximization EM 21 treating hidden factors concepts respon sible blobs words missing data In EM computation alternate following steps ExpectationE Estimate expectations unobserved data previous es timates parameters In particular blob word training data estimate probability comes hidden factors concepts MaximizationM Estimate model parameters P w l P b l P l max imizing expected loglikelihood computed Estep The model particularly sensitive number concepts attempt optimize number concepts work In previous studies 569 500 concepts adequate ﬁve thousand images In work 1000 concepts experiments training sets order 18000 images 100 concepts experiment training sets order 1500 images The model generalizes learns image components These com ponents occur different conﬁgurations recognized For example possible learn sky regions images tigers predict sky ele phant images Of course predicting word elephant requires having elephants training set 20 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 4 Using word prediction sense disambiguation In context word sense disambiguation vocabulary assumed sense disambiguated Formally use extended vocabulary S contains senses words vocabulary W Notationally word bank W bank_1 bank_2 S Thus sense s S sense word w W Once model trained S use annotation process compute P s B Different annotation word sense disambiguation additional characteristic trying distinguish senses s particular word w produce number good choices S clearly difﬁcult Given word w consideration assume senses words predicted Operationally simply posterior probability senses vocabulary set corresponding w zero We rescale pos terior sums This computation yields probability word sense s given w visual context B denote P s w B Being able constrain word prediction domain makes process accurate useful Linking wordswhich carry semanticsto images difﬁcult task limiting choices generally helpful For example shown Fig 3 know words caption constrain region labeling words labeling performance increases substantively 41 Combining word prediction traditional word sense disambiguation The quantity P s w B word sense disambiguation provide results strategy It natural combine text based methods provide orthogonal source information Here assume text based method provide second estimate probability P s w W sense s w based observed words W senses known priori We discuss choice P s w W Section 42 We assume estimates relatively independent gives follow ing simple expression combining P s w B W P s w BP s w W 6 While estimates likely degree mutual information sults suggest independence useful We considered possibility estimates embody empirical sense distribution compensating provide better strategy robust results simple independence assumption 42 Traditional word sense disambiguation The probability P s w W 6 assumed come traditional text based word sense disambiguation algorithm In preliminary 6 work naïve algorithm based distances computed WordNet 42 words forming context K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 21 Fig 3 Illustration improvement regionlabeling able restrict predicted words known caption The task ﬁnd tiger regions image data base The best tiger regions shown The group determined image data group image data ﬁve keywords tiger We emphasize task precisely analogous word sense disambiguation The key point difﬁcult prediction problem easier constrain predictions small number choices words related proposed senses This algorithm produced score instead true proba bility calculated work 6 drawn 340 We performance algorithm poor leading question original results image information overshadowed 22 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 sophisticated text based WSD algorithm Thus implemented methods men tioned We ﬁrst The second algorithm proved interesting domain attempt capture usage statistics image based algorithm access training Thus need independence sources information breaks results good 421 Iterative word sense disambiguation The SMUaw algorithm recent derivative SenseLearner 39 shown perform 123 As based main textbased algorithms technique iterative word sense disambiguation presented 41 This method makes use WordNet semantically tagged corpus SemCor consists 10 algorithms act ﬁlters input data Each algorithm pipeline uses different heuristic disambiguate word moves set ambiguous words SAW set disambiguated words SDW process referred marking These procedures range removing proper nouns monosemous words connecting words certain semantic distances The original algorithm gave words deﬁnite sense based computational heuristics associated ﬁlter As approach described requires softer output modiﬁed algorithm information lost ﬁltration step contributes score sense Each procedures altered following ways original procedure italics 1 Mark proper nouns WordNet sense 1 No change 2 Mark words sense having sense No change 3 Examine usage word neighbors SemCor If count sense certain threshold remainder senses remove mark word highest sense Instead dropping counts senses threshold normalize array sense frequency counts senses scores 075 mark word sense retain distribution data 4 For sense noun SAW ﬁnd nouns occur window 10 words sense usage compile create noun contexts The sense nouncontext greatest overlap textual context word deﬁned cardinality intersection noun context words document greater highest sense threshold marked Again instead throwing away overlap data instead store entire array cardinalities normalize mark word highest threshold case 05 5 For word SAW senses semantic distance 0 synset word SDW mark sense Instead throwing away data count word semantic distance 0 given sense tabulated counts normalized substitute probabilities Again mark word likelihood threshold 05 6 Same performed SAW words SAW senses semantic distance 0 marked sense Change K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 23 7 Same ﬁfth procedure distance 1 hypernymholonym relationship Change 5 8 Same sixth procedure distance 1 Change 6 All words disambiguated process given default distribution favored common sense The end result 6 8 procedures produce softer distributions useful 6 5 ImCor In previous work 10 Corel image data set ﬁve keywords image We labeled senses keywords 16000 images identiﬁed subset 1800 images potential sense problems heuristics bias set ambiguous keywords Nevertheless ambiguity dataset sufﬁcient provide realistic testing For example word head usually ambiguous Corel dataset overwhelmingly tends way Given inadequacy existing image datasets kind work created new research corpus named ImCor This corpus links images Corel dataset sense disambiguated SemCor corpus provide new corpus links images semantically tagged text We ImCor available research purposes 31 51 Building Imcor The task hand link images text passages SemCor provide images linked text lines ﬁnd newspaper magazine setting The Corel keywords determine initial set 30 candidate images SemCor articles We developed tool facilitate human selection text image candidates Fig 4 The rater asked ﬁrst choose image appropriate text rater selected text passage article appropriate The magnitude task meant raters required build corpus We divided data overlap article The Kappa statistic agreement raters subset 0575 appreciable hoped reﬂecting subjective nature task The end result list documents associated images marked inap propriate text images illustrated article speciﬁc appropriate paragraph text article We gathered appro priate images single corpus disambiguated text captions We incorporated images associated article speciﬁc text segment assigning random sampling words article selection factor 1P P number paragraphs article The end result corpus 1633 imagetext pairings 8683 tagged speciﬁc paragraph text 1317 random samplings documents 24 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 Fig 4 A screenshot program select text passages SemCor semantically linked images The rater reads article left looks picture If picture appropriate click box lower right At point rater opportunity select text appropriate indicate image 52 Expanding ImCor While carefully sense disambiguated annotated corpus 1633 images goes far yond available relatively small purposes Therefore exploit fact semantic redundancy Corel image data 50 images planesjets similar keywords ﬁnd additional images appropriate captions ﬁrst step Any image shared keywords image paired SemCor text added corpus text This operation produced new version corpus 20153 imagetext pairings 6 Experiments For experiments produced different breakdowns corpus train ing testing sets 90 training 10 testing In corpus number images times took care ensure cases entire group assigned training testing sets For split termined vocabulary based training set We ﬁrst removed stop words corpus reduce computation We eliminated word senses occurred 20 times 50 times second experiment If produced images words K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 25 removed vocabulary recomputed iteratively needed Typical vocabulary sizes 3800 senses 3100 sense blind words 3002600 second experiment To provide idea vocabulary noted 193 senses starting letter m 137 unambiguous 56 senses Those 56 senses machinery_1 machinery_2 major_1 major_2 major_3 make_1 make_10 make_12 make_13 make_17 make_2 make_3 make_4 make_6 make_8 man_1 man_2 man_3 man_4 man_5 marvel_1 marvel_2 mass_1 mass_3 mass_4 matter_1 matter_2 matu rity_1 maturity_2 mean_1 mean_2 mean_3 measure_2 measure_3 memory_1 mem ory_2 mention_2 mention_3 mind_1 mind_2 miss_2 miss_6 moment_1 moment_2 monotonous_1 monotonous_2 month_1 month_2 moral_1 moral_2 mortal_1 mortal_2 mouth_2 mouth_3 musician_1 musician_2 Next trained word prediction model Section 3 combined image sense data We features described 16 largest image regions fewer 16 We applied model test data predict senses according 4 restricted senses word consideration described Section 4 We combined image text results described Section 41 sets ﬁnal results word sense disambiguation Fig 5 shows examples text based method gives wrong sense adding image information leads correct sense We compute performance documents ambiguous word By construction words test document sense measurement process score algorithms giving correct sense inﬂate performance ﬁgures dilute effects investigating For baseline use performance empirical distribution training set roughly 60 This harsher baseline simple common sense method surprising effective 47 empirical distribution gives common sense particular corpus investigated We omit results second text WSD method 36 roughly comparable base line score zero surprising fact given nature algorithm corpus We provide results forms In Table 1 report average absolute scores 20 samples In Table 2 report performance method exceeds baseline averaged 20 samples This controls somewhat sub set difﬁculty makes easy identify nontrivial performance results positive values The results combining sources information promising performance went method exactly trying achieve On large data set extended ImCor able increase perfor mance baseline nearly 20 yielding nearly 80 absolute performance In small seed data set performance increase modest yielding 5 improve ment In cases results statistically signiﬁcant Speciﬁcally performed paired t test performance images text exceeding text 20 samples 9 degrees freedom reﬂecting fact roughly 10 26 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 Sense tagged words plant rooting_1 developed_1 compost_1 sand_1 beneﬁt_1 good_1 ﬁnd_1 day_3 feel_2 separate_1 top_2 half_1 plant_2 b Sense tagged words water reach_1 location_1 sundown_1 herd_1 water_2 and_then_1 broad_1 grass_1 ﬂat_1 Fig 5 Two cases image information proved helpful In text based word sense disambigua tion gives canonical abstract meaning water water_1 substance Adding image information gives correct sense water_2 body water In b text gives incorrect sense plant plant_1 fac tory Adding image information gives correct sense plant_2 botanical In cases visual common sense promoted statistical model linking image features words However caution reader words study particularly visual examples clear cut Nonetheless correlates visual features word senses consistent training data testing data help disambiguate senses demonstrated quantitative results Table 1 Restricted word prediction results word sense disambiguation experiments The ﬁrst rows extended ImCor data set 20153 text passages paired images different values minimum number times word sense needs training data order considered vocabulary For completeness row result manually produced seed data set 1633 pairs data bit sparse learning method The numbers tabulated fraction times sense correctly chosen Every document processed ambiguous word Some words unam biguous algorithms score correctly words construction The results shown average 20 different breakdowns training testing The error estimated variance 20 testtraining splits 0003 ﬁrst rows 001 row Incorporating image information statistically signiﬁcant p 001 cases paired t tests Data set Full Full Seed Minimum sense count 20 50 20 Baseline 0615 0606 0571 Text 41 0683 0674 0693 Image 0791 0781 0687 Combined 6 0817 0814 0741 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 27 Table 2 Analogous results Table 1 performance increase method empiri cal distribution baseline averaged samples Comparisons based numbers accurate comparing overall performances reported Table 1 results empirical distribution controls somewhat sample difﬁculty The estimated errors numbers 0003 ﬁrst rows 001 row Data set Full Full Seed Minimum sense count 20 50 20 Text 41 0069 0068 0125 Image 0177 0175 0116 Combined 6 0202 0208 0173 Table 3 Average counts number senses correctly identiﬁed 20 samples experiments The total number ambiguous words provided ﬁrst column These results consistent previous tables map exactly numbers words ambiguous respect vocabulary counted The errors ﬁrst rows roughly 11 errors row roughly 4 All differences WSD text WSD text images signiﬁcant p 00005 Average number ambiguous words 6975 6204 697 Baseline 4506 3986 411 Text 41 4935 4390 477 Image 5082 4515 454 Combined 6 5361 4803 498 dependent samples 20 sets sampling 10 data time For experiments 1 M 0133 SE 00030 t 9 448 p 00005 2 M 0140 SE 0003 t 9 496 p 00005 3 M 0048 SE 0011 t 9 45 p 0001 We interpret results Table 1 noting run attempts ﬁnd senses 7000 words distributed 800 documents The 7000 words 20000 senses relative vocabulary Thus baseline method performing 60 speciﬁes correct sense 4200 words misses 2800 The combined method performing 80 misses half 1400 Finally Table 3 provide average counts correct sense identiﬁcation stricted words ambiguous Again experiments signiﬁcant performance increase adding image data Speciﬁcally paired t test sults images text greater text 20 samples 9 degrees freedom experiments 1 M 426 SE 104 t 9 410 p 00005 2 M 413 SE 114 t 9 362 p 00005 3 M 212 SE 40 t 9 53 p 00005 We emphasize domain constructed somewhat artiﬁcially test ideas improvement going small seed data set larger likely taking advantage structure Corel data However seed data case limited image data train corpus 28 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 pure statistically signiﬁcant improvement word sense disambiguation performance image data included 7 Conclusion The main conclusion work visual information help disambiguate senses help determine language meaning In fact small relatively friendly domain able exceed performance text based methods We able improve performance combining text imaged based information Our experiments suggest image information captured approach sufﬁciently independent textual based cues combining sources information prove fruitful A second important contribution work development new corpus Im Cor links images sense disambiguated text As linking images text important emerging research area data set help researchers area evaluate extent approaches capture semantics visual data References 1 Proceedings Senseval3 The Third International Workshop Evaluation Systems Semantic Analysis Text 2004 2 E Agirre G Rigau Word sense disambiguation conceptual density Proceedings COLING96 Copenhagen Denmark 1996 pp 1622 3 E Agirre G Rigau A proposal word sense disambiguation conceptual distance Proceedings 1st International Conference Recent Advances Natural Language Processing 1995 4 Y BarHillel The present status automatic translation languages D Booth RE Meagher Eds Advances Computers Academic Press New York 1960 pp 91163 5 K Barnard P Duygulu N Freitas D Forsyth D Blei MI Jordan Matching words pictures J Machine Learning Res 3 2003 11071135 6 K Barnard P Duygulu D Forsyth Clustering art Proceedings IEEE Conference Computer Vision Pattern Recognition vol II 2001 pp 434441 7 K Barnard P Duygulu D Forsyth Exploiting text image feature cooccurrence statistics large datasets R Veltkamp Ed Trends Advances ContentBased Image Video Retrieval Springer Berlin submitted publication 8 K Barnard P Duygulu KG Raghavendra P Gabbur D Forsyth The effects segmentation feature choice translation model object recognition IEEE Conference Computer Vision Pattern Recognition II 2003 pp 675682 9 K Barnard D Forsyth Learning semantics words pictures Proceedings International Conference Computer Vision vol II 2001 pp 408415 10 K Barnard M Johnson Word sense disambiguation pictures Proceedings HLTNAACL 2003 Workshop Learning Word Meaning NonLinguistic Data 2003 pp 15 11 J Bear J Dowding E Shriberg Integrating multiple knowledge sources detection correction repairs humancomputer dialog Meeting Association Computational Linguistics 1992 pp 5663 12 AB Benitez SF Chang Automatic multimedia knowledge discovery summarization evaluation 2003 13 AB Benitez S Chang Image classiﬁcation multimedia knowledge networks Proceedings IEEE International Conference Image Processing ICIP Barcelona Spain September 2003 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 29 14 PF Brown J Cocke SAD Pietra VJ Della Pietra F Jelinek JD Lafferty RL Mercer PS Roossin A statistical approach machine translation Computational Linguistics 16 1990 7985 15 PF Brown SA Della Pietra VJ Della Pietra RL Mercer The mathematics machine translation Parameter estimation Computational Linguistics 19 10 1993 263311 16 PF Brown S Della Pietra VJ Della Pietra RL Mercer Wordsense disambiguation statistical methods Meeting Association Computational Linguistics 1991 pp 264270 17 P Carbonetto N Freitas K Barnard A statistical model general contextual object recognition Proceedings European Conference Computer Vision I 2004 pp 350362 18 C Carson M Thomas S Belongie JM Hellerstein J Malik Blobworld A regionbased image indexing retrieval Proceedings Third International Conference Visual Information Systems Springer Berlin 1999 19 M Ciaramita T Hofmann M Johnson Hierarchical semantic classiﬁcation Word sense disambiguation world knowledge Proceedings 18th International Joint Conference Artiﬁcial Intelligence 2003 20 C Loupy M ElBèze Using clues compensate small resources available wsd Proceedings Second International Conference Language Resources Evaluation 2000 pp 219223 21 AP Dempster NM Laird DB Rubin Maximum likelihood incomplete data em algorithm J Roy Statist Soc Ser B 39 1 1977 138 22 P Duygulu K Barnard N Freitas D Forsyth Object recognition machine translation Learning lexicon ﬁxed image vocabulary Proceedings The Seventh European Conference Computer Vision vol IV 2002 pp 97112 23 P Edmonds A Kilgarriff Eds J Natural Language Engineering 9 January 2003 24 SL Feng R Manmatha V Lavrenko Multiple Bernoulli relevance models image video annotation Proceedings CVPR04 vol 2 2004 pp 10021009 25 WN Francis H Kuˇcera Frequency Analysis English Usage Lexicon Grammar Houghton Mifﬂin 1981 26 G Miller C Leacock T Randee R Bunker A semantic concordance Proceedings 3rd DARPA Workshop Human Language Technology 1993 pp 303308 27 W Gale K Church D Yarowsky One sense discourse Proceedings DARPA Workshop Speech Natural Language 1992 pp 233237 28 J Gonzalo F Verdejo I Chugur J Cigarran Indexing wordnet synsets improve text retrieval Proceedings COLINGACL 98 Workshop Usage WordNet NLP Montreal Canada 1998 pp 3844 29 T Hofmann J Puzicha Statistical models cooccurrence data Technical Report Massachusetts Institute Technology 1998 30 J Jeon V Lavrenko R Manmatha Automatic image annotation retrieval crossmedia relevance models Proceedings SIGIR 2003 pp 119126 31 M Johnson K Barnard ImCor A linking SemCor sense disambiguated text corel image data httpkobuscaresearchdataindexhtml 2004 32 A Kaplan An experimental study ambiguity context 1950 33 A Kilgarriff Senseval An exercise evaluating word sense disambiguation programs Proceedings LREC Granada May 1998 pp 581588 34 V Lavrenko SL Feng R Manmatha Statistical models automatic video annotation retrieval Proceedings International Conference Acoustics Speech Signal Processing ICASSP Mon treal May 2004 35 X Li S Szpakowicz S Matwin A wordnetbased algorithm word sense disambiguation Proceed ings IJCAI95 Montreal Quebec 1995 pp 13681374 36 D McCarthy R Koeling J Weeds J Carroll Finding predominant senses untagged text Proceedings 42nd Annual Meeting Association Computational Linguistics 2004 pp 280287 37 SW McRoy Using multiple knowledge sources word sense discrimination Computational Linguis tics 18 1 1992 130 38 D Melamed Empirical Methods Exploiting Parallel Texts MIT Press Cambridge MA 2001 30 K Barnard M Johnson Artiﬁcial Intelligence 167 2005 1330 39 R Mihalcea E Faruque Senselearner Minimally supervised word sense disambiguation words open text Proceedings ACLSIGLEX Senseval3 Barcelona Spain July 2004 40 R Mihalcea D Moldovan Word sense disambiguation based semantic density Proceedings COLINGACL Workshop Usage WordNet Natural Language Processing Systems Montreal Canada August 1998 41 R Mihalcea D Moldovan An iterative approach word sense disambiguation Proceedings Florida Artiﬁcial Intelligence Research Society Conference FLAIRS 2000 Orlando FL May 2000 pp 219223 42 GA Miller R Beckwith C Fellbaum D Gross KJ Miller Introduction wordnet An online lexical database Internat J Lexicography 3 4 1990 235244 43 A Montoyo M Palomar G Rigau Wordnet enrichment classiﬁcation systems Proceedings NAACL Workshop WordNet Other Lexical Resources Applications Extensions Customizations Carnegie Mellon University Pittsburgh USA 2001 pp 101106 44 Hwee Tou Ng Hian Beng Lee Integrating multiple knowledge sources disambiguate word sense An exemplarbased approach A Joshi M Palmer Eds Proceedings ThirtyFourth Annual Meeting Association Computational Linguistics Morgan Kaufmann San Francisco 1996 pp 4047 45 J Shi J Malik Normalized cuts image segmentation IEEE Trans Pattern Anal Machine Intell 22 9 2000 888905 46 J Stetina S Kurohashi M Nagao General word sense disambiguation method based sentential context Use WordNet Natural Language Processing Systems Proceedings Conference Association Computational Linguistics Somerset NJ 1998 pp 18 47 J Traupman R Wilensky Experiments improving unsupervised word sense disambiguation Technical Report University California Berkeley 2003 48 D Yarowsky Wordsense disambiguation statistical models Rogets categories trained large corpora Proceedings COLING92 Nantes France July 1992 pp 454460 49 D Yarowsky Unsupervised word sense disambiguation rivaling supervised methods Proceedings 33rd Conference Applied Natural Language Processing ACL 1995 pp 189196 50 V Yngve Syntax problem multiple meaning W Locke D Booth Eds Machine Translation Languages Wiley New York 1955 pp 208226