Artiﬁcial Intelligence 175 2011 673693 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint From Bidirectional Associative Memory noisetolerant robust Protein Processor Associative Memory Omer Qadir Jerry Liu Gianluca Tempesti Jon Timmis Andy Tyrrell Department Electronics University York Heslington YO10 5DD York UK r t c l e n f o b s t r c t Protein Processor Associative Memory PPAM novel architecture learning associ ations incrementally online performing fast reliable scalable heteroassociative recall This paper presents comparison PPAM Bidirectional Associative Memory BAM Koskos original training algorithm pop ular PseudoRelaxation Learning Algorithm BAM PRLAB It compares PPAM recent associative memory architecture called SOIAM Results training objectavoidance presented simulations playerstage veriﬁed actual implementations EPuck mobile robot Finally PPAM capable achieving increase performance typical weightedsum arithmetic operations arithmetic operations 2010 Elsevier BV All rights reserved Article history Received 8 March 2010 Received revised form 19 October 2010 Accepted 21 October 2010 Available online 27 October 2010 Keywords Selforganising Selfregulating Associative Memory Protein processing Heteroassociative BAM PRLAB SOIAM SABRE Mobile robotics 1 Introduction The work paper stems research parallel architectures targeted problems domain AI Although architectures exist attempt solutions problems domain 3822237321 These based Arithmetic Logic Units ALUs simply Von Neumann style processors modiﬁcations connected parallel Despite abundance AI algorithms machine learning techniques state art fails capture rich analytical properties biological beings robustness In architecture computation moved memory intelligence expected emerge memory operations ALU operations It based connectionist approach affords level inherent faulttolerance enhanced design composed large number elements individually critical overall correct operation This contrast traditional approaches faulttolerance module component designed unique hot cold spares maintained replace faulty modules In architecture faults result graceful degradation nodes having replaced It postulated architecture better suited implementing artiﬁcial intelligence traditional ALU based architectures Although portions inspired biological neural networks objective build architecture Artiﬁcial Neural Networks Note The research supported EPSRC grant FPF06219211 Corresponding author Email addresses oq500ohmyorkacuk O Qadir yl520ohmyorkacuk J Liu gt512ohmyorkacuk G Tempesti jt517ohmyorkacuk J Timmis amtohmyorkacuk A Tyrrell URL httpwwwusersyorkacukoq500 O Qadir 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201010008 674 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 current version fully satisfy original motivation designing architecture hardware suitable software implementations Traditional memory stores data unique address recall data presentation complete unique address Autoassociative memories capable retrieving piece data presentation partial information piece data Heteroassociative memories hand recall associated piece data category input presentation data category inputs Hopﬁeld networks 13 shown act autoassociative memories 6 capable remembering data observing portion data Biological neural networks hand heteroassociative memories remember completely different item presented input Bidirectional Associative Memories BAM 18 Artiﬁcial Neural Networks long performing heteroassociative recall This paper starts investigating eﬃcacy BAMs performing heteroassociative recall context AI robotics Association performed sonar values actions needed avoid obstacles robot Two different training algorithms examined generation correlation matrix BAM original BAM training algorithm outlined Kosko 18 popular PseudoRelaxation Learning Algorithm BAM PRLAB ﬁrst presented Oh Kothari 27 Even PRLAB guarantees optimal learning distinguishing values closely spaced weakness BAM Nevertheless values realistic inputs realtime noisy environment parameters optimized maximize recall small confusions result major problems mobile robot particularly online learning environment The paper goes compare structural differences Protein Processor Associative Memory PPAM BAM presents results implementations PPAM problem It discusses robustness noise tolerance PPAM architecture compares noise tolerance reported Sudo et al 35 SelfOrganising Incremental Associative Memory SOIAM The paper structured follows Section 2 brief discussion BAM PRLAB constraints advantages Section 3 outlines experimental method setup parameter tuning details experiments performed Section 4 presents results BAM implementation playerstage robot Section 5 describes PPAM ending structural comparison BAM PPAM Section 6 reviews results PPAM implementation playerstage robot Section 7 discusses results noise tolerance properties comparing SOIAM presents inferences drawn results Section 8 summarises concludes discusses future directions 2 Bidirectional Associative Memories Bidirectional Associative Memories 18 generalization Hopﬁeld networks 13 beginnings Correlation Matrix Memories 17 cited 1 BAMs long subject analysis formed basis later models Attempts ﬁnd best training algorithm include 5 uses exponential rule 33 uses genetic algorithms 45 uses descending gradient method 7 uses linear programming techniques 32 noniterative Morphological BAM 41 noniterative feedforward BAM 1 variation based binary metaoperators called AlphaBeta BAM Cao et al 4 discuss higher order BAMs time delays Lu et al 23 consider effects topology performance Hopﬁeldtype associative memories Like traditional neural networks 2layer nonlinear recurrent neural network starts training deﬁned training dataset moves actual test set The weightedsum approach means capabilities analysed proven paper mathematics Furthermore original training algorithm require batch learning easily adaptable online learning On hand results suboptimal utilization capacity Furthermore shown Kosko 18 BAM confused storing onetomany manytoone relationships In addition pairs trained correlation matrix unstable BAM likely forget previously learnt pairs Obviously desirable maximise storage capacity quantify attempts direction Tanaka et al 36 perform statistical physical analysis estimate capacity pairs retrievable allowing ﬁnite fraction retrieval error 01998N N number neurons layer layers The fact capacity BAM tied number nodes drawback terms scalability Wang Vachtsevanos 39 derive storage capacity discrete BAM veriﬁed experimental results It shows storage capacity smaller expected mislearning behavior BAM connection matrix confuses similar patterns bipolar encoding scheme Since bipolar encoding performs better average binary encoding shown Kosko 18 mislearning unavoidable In essence problem maximise number patterns superimposed memory medium weights connections correlation matrix Therefore guarantee recall training vectors orthogonal One obvious solution reencode nonorthogonal training data orthogonal Simpson 34 cited 27 Of course imply complete training set known advance assumption valid online realtime learning training data encountered sequentially complete size known advance PRLAB 27 iterative learning algorithm discrete BAMs maximises storage capacity BAM need preprocessing reencoding training data Furthermore guarantees perfect recall training pairs possible store stable states correlation matrix All training pairs examined cyclically O Qadir et al Artiﬁcial Intelligence 175 2011 673693 675 Algorithm 1 Pseudocode innate objectavoidance algorithm Input sonar values array A Output forward speed rotation speed ro s f constant forward speed forward ss constant forward speed avoid obstacles ri constant rotation speed avoid obstacles C true Ai shows distance object threshold value C true set forward speed 0 ss object left closer object right ro ri ro ri s f iteration called epoch associative pair stored stable state weights adjusted Where algorithms suffer parameter tuning issues notably deciding optimum step size maximize learning iterative algorithms PRLAB highly insensitive parameter values initial conﬁgurations To guarantee training PRLAB needs training pairs adjust weights multiple iterations Sudo et al 35 adapt PRLAB online incremental learning sequentially providing associative pairs algorithm observes pair They results BAM Hopﬁeld networks forgetting previously learned data However unfair comparison entire strength PRLAB lies fact cycles training pairs iteratively makes changes weight matrix pair stored properly 3 Experiments 31 Method A single agent placed static environment initialized simple reactive objectavoidance algorithm Al gorithm 1 controls movements If sonar values observed indicate collision imminent agent rotated left right depending direction indicates objects farthest The movement value generated sonar value observed form variables associative pair The idea running innate objectavoidance algorithm agent able switch recalled objectavoidance algo rithm seamlessly The associative memory seen observing sonar values encountered agent conjunction corresponding actions movement values generated innate objectavoidance algorithm associating The experiments performed BAM original Koskos learning method BAM PRLAB PPAM 32 Setup Playerstage 828 simulate environment The agent Pioneer P3DX Robot1 7 func tional sonars pointing relative north angular difference 225 The sonars range 5 meters area robot navigates 14 14 meters The stage driver pioneer sonars returns real values range 00 cid2 s cid2 50 Since analogue realworld data need pass analogueto digital converters perform quantization approximated discretizing sonar values 10 bins bin 05 meters Since 7 sonars makes total 107 10 million possible unique values 7dimensional variable represents sonar input Movement represented forward speed rotation speed real values The objectavoidance algorithm generates possible forward speeds slow fast pending sonar values Similarly rotation algorithm generate possible values turn left turn right straight Therefore rotation 3 discrete values forward speed 2 discrete values making 6 total possible unique values 2dimensional variable represents movement Note objectavoidance algorithm memory quantization errors cause agent stuck race conditions ﬁrst turns left right left If slow forward speed set nonzero value means agent continues forward turning turn means agent able navigate race conditions effort objectavoidance algorithm This undesirable motivation simple algorithm test future associative memory perform better algorithm given higher level goals input associative pairs 1 httpwwwactivrobotscomROBOTSp2dxhtml 676 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Fig 1 Controllers connection topology Agent Fig 2 Agent Trajectory Algorithm 1 Fig 1 illustrates topology controller BAM PPAM agent The sonar values fed analogueto digital converters output digitally encoded values As stated analoguetodigital A2D converters quantize sonar values 10 uniform discrete bins Thus output A2D converters 4bit binary values directly innate controller Algorithm 1 Depending size network forming associative memory number nodes BAM PPAM discretized values need sampled In BAM N nodes layer input vector V layer V I1 I2 I N Assuming bipolar implementation I j described I j 1 1 In 3x 7 y BAM network 3 nodes movement layer 7 nodes sonar layer N 7 sonar layer This means 4bit output A2D converters needs downsampled 1bit value 7 sonar values 7 nodes For 3x 28 y network 28 nodes sonar layer means sonar value represented 4 nodes 28 7 resampling required Similarly 12x 63 y network upsampling required The sampling function PPAM similar discussed Section 61 description architecture The advantage updown sampling realistically models quantization errors This fact perspective associative memory movement values generated innate algorithm source analogue external inputs Other resampling conversion bipolar case BAMPRLAB encoding performed input data At output corresponding updown sampling process performed required input Next movement values innate algorithm recalled movement values associative memory forwarded actuators agent The choice online usercontrol The selected movement values converted analogue values transmitted relevant motors The trajectory agent controlled objectavoidance algorithm shown Fig 2 objects shown black The end point trajectory algorithm gets stuck race condition This occurs 670 time steps time step agent observed 7D sonar value algorithm generated 2D movement value making associative pair These 670 pairs training dataset Note ﬁrstly training dataset pruned select key points instead includes observed values Secondly dataset onetomany relationship pairs movement vs sonar different sonar readings result agent continuing straight ahead This places BAM disadvantage dealing relationships known weakness Furthermore dataset guaranteed likely composed orthogonal vectors O Qadir et al Artiﬁcial Intelligence 175 2011 673693 677 opinion realtime data unlikely orthogonal realistic associative memories depend priori knowledge preprocessing training data convert orthogonal pairs 33 Tuning BAM correlation matrix The training dataset 670 pairs composed 228 unique associative pairs Each pair represented 3bit value movement 28bit value sonar 4bit value 7 sonars The perfect associative memory able learn 228 associative pairs recall perfectly agent reinitialized start coordinates Fig 2 follow trajectory exactly stuck race condition end In order best performance BAM parameters original Koskos learning algorithm PRLAB tuned Whether BAM correlation matrix BAM conﬁguration capable learning 228 pairs dependant 228 pairs orthogonal The rest section presents attempts maximize number associative pairs learnt BAM tuning parameters Once correlation matrix remember maximum number pairs control agent playerstage environment Section 34 These playerstage simulations explore effect imperfect recall BAMs compare imperfect recall PPAM 331 Koskos algorithm For original BAM learning algorithm presented Kosko 18 parameters need tuning initial values weight matrix number nodes 2 layers BAM Node conﬁgurations 3x 7 y 3x 28 y 6x 28 y 12x 28 y 12x 63 y 30x 28 y attempted 3x 7 y means 3 nodes layer 1 7 nodes layer 2 The numbers chosen multiples movement values sonar values Given BAM operates bipolar data 7 nodes layer means layer distinguish 27 128 bipolar values Therefore 3x 7 y conﬁguration represents case sonar values downsampled The 3x 28 y conﬁguration suﬃcient represent entire range values Higher node conﬁgurations test upsampling effect number pairs learnt BAM train better associative pairs farther apart Where necessary training dataset reencoded upsample downsample values For example 3x 7 y conﬁguration requires 4bit sonar values sonar downsampled 1 bit In case training dataset turned 18 unique associative pairs conﬁgurations remained 228 For higher node conﬁgurations 6x 28 y 12x 28 y reencoding essential preferable data points closer To illustrate 3 nodes range data 23 8 values maximum distance numbers 6 integer values Thus instance 4 separated 1 3 integer values possible 8 On hand 6 nodes range data 26 64 values If dataset 3 nodes reencoding upsampling case 4 separated 1 3 integer values total possible 64 For node conﬁguration initial weight values set 10 10 steps 002 The 3x 28 y node conﬁguration tested extra long range weight values range 1000 1000 steps 002 Whereas maximum number pairs learnt including almostrecalled pairs smaller range 186 228 larger range maximum number pairs learnt 187 228 indicating effect initial weights correlation matrix minor For initial weight value BAM trained increasing number associative pairs number pairs successfully recalled tested iteration A value considered perfectly recalled value recalled exactly trained Considering values trained attractors recalled value almostrecalled closest attractor correct Euclidean distance chosen measure closeness need orthogonality BAMs Since BAM uses weightedsummation Euclidean distance appropriate measures like hamming distance testing data points far apart likely confused Note detecting almostrecalled values implies postprocessing output priori knowledge attractors determine attractor closest This means memory copy associative pairs stored test recalled value associative pairs ﬁnd closest essence duplicating behaviour Despite unrealistic particularly online incremental learning method maximise performance The objective tuning determine BAM correlation matrix recall number pairs training dataset This control agent playerstage environment If correlation matrix recall pairs able follow trajectory objectavoidance algorithm exactly As mentioned initial weight conﬁguration increasing number associative pairs presented training Fig 3 shows capacity BAM varies number associative pairs training dataset increased This important step learning algorithm unlike PRLAB guarantee utilization maximum storage capacity Therefore seen plots pairs presented capacity memory begins fall Each point plots Fig 3 best initial weight value primary ﬁtness criterion maximize number pairs recalled secondary ﬁtness criterion maximize number pairs recalled perfectly Note size 8 conﬁguration 3x 7 y size 25 conﬁgurations BAM successfully recall training pairs presented However pairs presented point learnt 678 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Fig 3 Memory capacity Kosko learning Fig 4 Capacity BAM PRLAB expense pair The maximum number associative pairs learnt conﬁguration nodes initial weights 187 332 PRLAB The parameters need tuning initial values weightthreshold matrix number nodes number epochs relaxation factor λ constant ξ deciding basins attraction Oh Kothari 27 PRLAB sensitive parameter values implementations dont train parameters 35 However shown Fig 4 ﬁnd dataset entirely orthogonal training result difference 68 pairs worst case difference PRLAB Koskos original method The node conﬁgurations training dataset Koskos algorithm Details training dataset Sections 31 32 Parameters tuned steps ﬁrst performing longrange sweep larger steps second performing short range sweep smaller steps close best region longrange sweep Table 1 shows longrange sweep parameters result total 10000 iterations Table 2 shows short range sweep parameters The method perfectly recalled almostrecalled tuning Koskos algorithm Section 331 tune PRLAB The 6x 28 y node conﬁguration tested extra long range shown Table 3 Whereas maximum number pairs learnt including almostrecalled pairs longrangeshortrange search 219 extra long range search 220 Fig 4 barchart showing capacity BAM trained PRLAB The best O Qadir et al Artiﬁcial Intelligence 175 2011 673693 679 Table 1 Long range search parameters PRLAB Parameter Weightthreshold Epochs λ ξ Min 05 5 05 005 Table 2 Short range search parameters PRLAB Parameter Weightthreshold Epochs λ ξ Step 001 1 01 0002 Table 3 Extra long range search parameters Parameter Weightthreshold Epochs λ ξ Min 50 5 01 001 Max 05 70 23 02 Max 50 75 24 10 Table 4 Best parameters PRLAB Network 3x 7 y 3x 28 y 6x 28 y 12x 28 y 12x 63 y 30x 28 y Epochs 11 20 35 23 65 23 ξ 005 0120 09505 0125 02 0125 λ 05 160 154 158 05 158 Step 01 5 02 0015 Iterations 10 10 4 5 Step 01 3 01 0005 Weight 03 034 09 03 01 03 parameter values Table 4 ﬁtness criterion maximizes number pairs recalled maximizing number pairs recalled perfectly minimizing number epochs required learn associative pairs The thatched portion indicates number values almostrecalled correctly The absolute minimum number pairs learnt combination parameters depicted lightgrey bar plus thatched bar Min Almost Recalled The maximum number pairs learnt height bar Note maxalmostrecalled 0 This means correlation matrix fully optimized maximum number pairs stored pairs stored stored perfectly Euclidean distance measure recalled enhance performance Note exception 3x 7 y node conﬁguration worstcase PRLAB correlation matrix performed better Koskos original correlation matrix Furthermore trained PRLAB correlation matrices performed approximately equally indicating number associative pairs learnt BAM unaffected number nodes layers maximum number orthogonal pairs learnt reached More pairs learnt increasing orthogonality pairs Thus dataset reencoded Disregarding impli cations structure BAM assuming possible online realtime encoding ensure orthogonality realworld datasets point associative memory deal imperfect recall The objective explore behaviour BAM compare PPAM situation data imperfectly stored recalled described Section 34 333 Verifying alternate dataset An alternate dataset generated training dataset parameter tuning process explained The object verify results observed property dataset For purpose agent placed completely different environment Fig 5 shows modiﬁed environment trajectory taken agent This trajectory composed 770 time steps agent got stuck race condition end position These 770 time steps composed 283 unique training data points formed alternate dataset Tuning Koskos method PRLAB performed manner outlined Results network conﬁgurations shown Fig 6 680 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Fig 5 Trajectory alternate environment 34 Robot simulations Fig 6 Alternate dataset memory capacity The objective previous experiments determine best correlation matrix BAM If correlation matrix recall 228 associative pairs training data trivial reset agent initial starting location agent followed path objectavoidance algorithm exactly In case remain observe behaviour agent new previously unobserved environments However correlation matrix recall 228 pairs experiments environments useful Although know maximum number pairs recalled BAM conﬁguration 220 228 objective following experiments determine effects robot realworld realtime environment Note following description relevant experiments performed PPAM In ﬁrst phase agent playerstage reset initial starting position orientation allowed run 670 time steps This time movements recalled memory calculated If associations learnt correctly agent follow exact path took ﬁrst time end 670 steps end position correct orientation The second phase begins 670 steps agent placed new environments The sonar values encountered best partially observed associative memory receives inputs partially learnt This observing noisy training vector The agent placed different locations run total 763 time steps altogether Each network conﬁgurations described 3x 7 y 3x 28 y 6x 28 y 12x 28 y 12x 63 y 30x 28 y control agent playerstage simulation The best trained correlation matrix Section 332 method detect almostrecalled values implemented Note case PRLAB trained matrix incremental learning Therefore agent moves environment learns associations needs store copy entire training dataset separately associative memory copy presented associative memory weights updated Nonetheless necessary implementing incremental version Sudo et al 35 loses guaranteed learning capability PRLAB O Qadir et al Artiﬁcial Intelligence 175 2011 673693 681 3x 7 y b 3x 28 y 6x 28 y 12x 28 y Fig 7 Agent trajectory Koskos method 3x 7 y b 3x 28 y 6x 28 y 12x 28 y Fig 8 Environment input percentage previously observed Koskos algorithm Table 5 Associativity 3x 7 y Koskos network Percentage previously observed Recall conﬁdence Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct 100 838 340 802 178 764 236 857 761 543 500 87 343 657 4 BAM results 41 Koskos method The trajectory taken agent reinitialized original start point shown Fig 7 The trajectory lightgrey taken agent controlled objectavoidance algorithm dark grey trajectory taken agent controlled BAM As seen Fig 7a agent fact collision wall stalled Increasing nodes 3x 7 y 3x 28 y improved performance agent successfully avoided collisions suffered race conditions The distance travelled agent shorter agent stops encounters race condition memory ﬁrst decides turn left right Increasing nodes 3x 28 y effect trajectory This race condition occur certain set sonar inputs BAM incapable storing required associative pair stable state Fig 8 shows breakdown environmental inputs terms percentage environment observed The 100 region contains data points occur robot follows path reset initial start location heading Partially observed sonar inputs occur robot placed new environment robot strays original path Fig 7 The 857 region contains data points 6 7 sonar values observed This represents heteroassociative recall extrapolation relationships required determine output Table 5 shows associativity 3x 7 y conﬁguration Koskos learning algorithm Data broken terms percentage environmental inputs previously observed column presents data 682 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Table 6 Associativity 3x 28 y 6x 28 y 12x 28 y Koskos network Percentage previously observed Recall conﬁdence Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct 100 100 989 00 NA NA 989 857 756 622 378 59 206 823 714 459 207 307 485 350 423 571 389 410 63 100 162 100 3x 7 y b 3x 28 y 6x 28 y 12x 28 y Fig 9 Agent trajectory PRLAB corresponding bar barchart showing breakdown environmental inputs Fig 8 Therefore example 100 column Table 5 corresponds 100 bar Fig 8a Since data points 714 region columns dont exist Table 5 Recall measure conﬁdence BAM recalls values If value perfectly recalled 100 conﬁdence The distance attractor measure conﬁdence almostrecalled values Thus given BAM trained set training vector pairs T X k Y kk1S k element set T viewed attractor Furthermore recalling X X k1S attractor similarly Y Assuming BAM provided input X l generate output Y l conﬁdence output Y l measured C λ minabscid6 Y l Y kcid62k1S λ subtract operation performed Euclidean distance measure multidimensional vectors Y λ maximum possible distance attractor current network size lower bound conﬁdence measure For BAM n nodes performing recall represented λ 2n 1 Accuracy measured number memory recalls matched outputs objectavoidance algorithm Note according deﬁnition action accurate result race condition object avoidance algorithm intentionally simple This facilitates later versions PPAM future attempting improve behavior agent controlled objectavoidance algorithm Critical lists percentage memory recalls critical avoiding collision CA percentage memory recalls critical correct RA measures percentage memory recalls critical noncritical recalled 100 conﬁdence accurate recalled Therefore measure accurate memory thinks scaled accurate actually higher better RC measures number memory recalls recalled 100 conﬁdence critical actions Therefore RA critical actions The associativity tables 3x 28 y 6x 28 y 12x 28 y conﬁgurations identical shown Table 6 42 PRLAB The trajectory taken agent reinitialized original start point training PRLAB shown Fig 9 The performance decidedly better conﬁgurations particularly agent collide objects matter node conﬁguration Note increasing nodes 7 28 deteriorates performance agent gets stuck race condition This primarily overtraining The 3x 7 y training set actually operates reduced training set 18 unique pairs Thus 3x 28 y correlation matrix recall 219 pairs O Qadir et al Artiﬁcial Intelligence 175 2011 673693 683 3x 7 y b 3x 28 y c 6x 28 y d 12x 28 y Fig 10 Environment input percentage previously observed PRLAB Table 7 Associativity 3x 28 y PRLAB Percentage previously observed Recall Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct Table 8 Associativity 6x 28 y PRLAB Percentage previously observed Recall Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct Table 9 Associativity 12x 28 y PRLAB Percentage previously observed Recall Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct 100 987 987 66 900 60 100 100 993 993 67 900 60 100 100 548 548 27 800 39 100 857 991 977 881 990 880 986 857 911 956 133 100 98 100 857 994 728 611 990 609 733 714 407 345 695 364 620 141 714 674 442 750 507 776 334 714 431 459 824 362 715 306 571 662 515 500 29 378 644 571 609 609 781 500 641 100 571 585 477 538 29 447 579 228 pairs trained manytoone relationship associative pairs BAM gets confused critical set sonar inputs Further increases number nodes effect trajectory Fig 10 shows breakdown environmental inputs terms percentage environment observed Table 10 shows associativity 3x 7 y conﬁguration trained PRLAB Tables 7 8 9 associativity 3x 28 y 6x 28 y 12x 28 y conﬁgurations respectively 684 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Table 10 Associativity 3x 7 y PRLAB Percentage previously observed Recall Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct 5 Protein Processor Associative Memory 100 100 748 577 564 577 748 857 100 977 157 857 157 977 714 100 807 580 667 580 807 571 100 979 97 789 97 979 This section describes architecture learning mechanism Protein Processor Associative Memory PPAM 30 51 Biological inspiration Proteins form critical communication processing cellular level Cells receive signals terms proteins respond terms proteins Proteins cause cells change genetic expression generate proteins This forms complex network signals called Genetic Regulatory Network GRN 1916 At heart GRN DNA process transcription Details processes membrane cell extremely complex dependant physical shape structure proteins 4212 A simpliﬁed version source inspiration presented Proteins enter cell channels receptors cause conformational changes cell If protein excites cis regulatory region2 initiates process transcription DNA read protein encoded Genes code protein RNA composed codons codes speciﬁc amino acid Proteins composed variable number amino acids small 466 yeast large 34350 Titin giant protein 20 During transcription DNA read serially reading frame beginning start marker called start codon ending stop marker called stop codon The sequence information start stop codons description protein produced result receiving original protein cells input The resultant output protein output cell input cell resulting iteration transcription process Proteins cause change genetic expression means cis regulatory regions presented excitation incoming proteins altered result processing proteins Codons composed subunits called nucleotides labeled U C A G chemical names Codons trinucleotide substances combination 3 nucleotides makes codon This means total 64 43 codon combinations possible However 20 valid amino acids start stop codon Despite 64 combinations assigned amino acids startstop markers The advantage obvious considering effects point mutations alter single codon Together natural selection controls effect frameshift mutations results highly robust transcription process For details transcription process reader referred Nakamoto 26 Reil 31 As expected transcription process resulting DNA inside cell extremely long Therefore needs compressed instead meter length ﬁt inside micrometers This achieved winding DNA strands histone resulting selected number cis regulatory interfaces excited allowing patternmatching process incoming proteins This basis genetic expression differentiation For details cis regulatory regions transcription regulation histones reader referred Wolpert 40 Zhang Reinberg 44 Zhang 43 52 Principle protein processing Existing bioinspired architectures 2415299 employ reconﬁgurable fabrics store conﬁguration bit stream DNA reconﬁgure hardware Other overhead DNA biologically plausible DNA biological cell initial conﬁguration cell actively involved function cell life Section 51 The transcription process performed inside biological cell considered akin looking table answers Incoming data tested action data expected produced response expected action data looked table database DNA incoming data unique ﬁeld record identiﬁcation As seen Section 51 seemingly simple lookupbased processing actually result extremely complex GRN capable complex developmental tasks having calculate results In theory unlimited memory database lookup tables perform kind processing If lookup table contains possible answers possible combinations incoming data cell 2 This seen precondition selection process O Qadir et al Artiﬁcial Intelligence 175 2011 673693 685 Fig 11 Abstract view nodes generate function In real world scenario limited memory limited number answers stored As seen Section 51 biological cell suffers similar issue employs histones compress data Future versions PPAM architecture expected use biologically inspired solutions compress memory DNA node The PPAM envisioned network nodes receiving input multiple categories inputs associ ated Each node network receives external inputs outputs nodes neighbourhood internal inputs Fig 11 illustrates network 2 types nodes squares circles The squares node1 node2 node3 node4 node5 receive external input ﬁrst category external input example movement values circles A B C D receive external input second category external input example sonar values External inputs indicated diagonal arrows In addition nodes receive inputs neighbouring nodes indicated thinner grid lines connecting nodes Although nodes illustrated Fig 11 arranged grid note topology presented facilitate concept neighbourhood network conﬁgurations possible In experimental setup neighbourhood node category consists nodes category Input data external world time series means sequence data points sampled uniform time intervals In node input data environment input neighbouring node input looked custom Content Addressable Memory CAM compare cis regulatory excitation described Section 51 If data CAM corresponding output value generated result compare transcription resulting cis regulatory excitation described Section 51 From perspective CAM likened database ﬁelds generating output node observation particular inputs Data calculated nodes free arithmetic units Instead data considered abstract symbols long symbols uniquely identiﬁed realworld data encoded decoded correctly Although encoding requirement PPAM unique symbols order enhance robustness encoding composed symbols redundant combinations This translates following based codon encoding principles outlined Section 51 Requirement Let data S1 deﬁned composed subset symbols set S elements s1 s2 s3 sn Let S1 encoded E 1 subset symbols set E elements e1 e2 e3 em Then multiple encodings E1 E2 E3 E M possibly different subset E decode generate S 1 This advantages Firstly inherent faulttolerance transmission errors Secondly decoding results unknown strings indication faults signaled lower upper layers Note PPAM considers input data abstract symbols encoding requirement optional enhancement A node said ﬁre input node causes generate output Firing nodes selfregulate means update tables memory response inputs compare proteins causing change genetic expression altering cis regulatory region presented excitation Learning continuous separate distinct training phase Memory recall asynchronous operation learning synchronous avoid requirement asynchronous RAMs hardware If node receive input environment looks neighbourhood input word CAM determines output value generated Therefore absence category environment input node recalls view neighbours partial view relationship categories inputs Together nodes reconstruct complete picture 686 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Table 11 Learning values CAM Input pattern Output value Input pattern Output value 0x0001 0x0002 0x0004 0x5 0x6 0x7 0x0001 0x0002 0x0004 0xBDCA 0x5 0x6 0x7 0x5 Initial values CAM b Values CAM regulation Table 12 Relationship 2 multidimensional variables A D1 3 0 3 A D2 0 15 025 B D1 50 05 35 B D2 50 10 40 B D3 50 075 475 53 Learning mechanism Learning based Hebbian principle 116 A ﬁring node records snapshot neighbours indicating neighbours ﬁred conjunction To illustrate let CAM node1 Fig 11 contain values shown Ta ble 11a Let environmental inputs 0x00013 input node1 environment inputs neighbouring nodes circular nodes ﬁre values A B C D Then node1 ﬁres value 0x5 observes 0xB DC A neighbours ordering data NorthSouthEastWest It adds tuple CAM reﬂect view neighbourhood resulting table shown Table 11b Note symbol set external inputs including categories overlap symbol set word generated neigh bourhood inputs For instance concatenated word neighbours node1 0x0004 instead 0xB DC A add tuple table input pattern ﬁeld contains word 0x0004 value tuple table Thus fallacious conﬂicting record entered table stem lack relationship inputs overlap symbols 54 Resolving conﬂicts Conﬂicting tuples result input variables related relationship onetomany manytomany manytoone In case nodes observe neighbourhood inputs conjunction different output values node Consider multidimensional variables A B onetoone relationship shown Table 12 Although A A D1 A D2 onetoone relationship B B D1 B D2 B D3 imply A Di onetoone relationship B D j combination This evident Table 12 The memory individual node represents portion relationship dimension category example A Di dimensions category example B D j This means onetoone mapping dataset relationship nodes resulting conﬂicting tuples However advantage extrapolating information necessitate solving mathematical equations require ALUs Conﬂicts resolved swapping process illustrated Algorithm 2 Over time tuples observed arranged order frequent frequent The frequent tuples ﬁrst removed node runs memory Note recall variable B value 50 50 40 perspective PPAM equivalent 50 50 x x value observed CAM present nodes Thus result heteroassociative recall If hand variable B value 50 50 50 autoassociative recall B fully observed considering tuple Table 12 data point complete data point recalled partial information 55 Comparing structure BAM PPAM Table 13 compares structure kind operations required BAM PPAM BAM implemen tations assumed optimized calculations repeated results considered stored temporary memory CPU registers Memory access readwrite include access temporary locations 3 0x indicates hexadecimal notation O Qadir et al Artiﬁcial Intelligence 175 2011 673693 687 Algorithm 2 Pseudocode frequency detection algorithm Input neighbourhood ﬁring pattern pn current node ﬁring value f memory M size 2 N M0x neighbourhood pattern memory location x M1x firing value CAM memory location x j stores index record swap j unique init value 0 N pn M0i f M1i j unique init value swap tuple tuple j break loop j j unique init value add new tuple end M location x M0x pn M1x f Table 13 Comparison structure BAM PPAM BAM PPAM Connections Memory requirements Encoding requirements M nodes connected N N nodes connected M M N M N ﬂoating point numbers bipolar preferred M nodes connected N N nodes connected M worst case S M N2 bits Operations N nodes recall Multiplications Additions Comparisons Comparisons Comparisons Iterations Memory reads M M 1 1 1 0 till stable M 1 Operations N nodes store 2 M 3 2 M 2 0 1 1 Epochs S M 3 M 1 Multiplications Additions Comparisons Comparisons Comparisons Iterations Memory reads Memory writes 0 0 0 0 S till stable S 0 0 0 0 S 1 S 1 working memory Furthermore memory requirements operations stimulating nodes providing input included In addition multiplication addition operations calculating indexes weightthreshold matrix BAM included Table 13 considered similar signiﬁcantly simpler smaller number memory index counting operations PPAM Table 13 generated storing set vector pairs T X k Y kk1S For BAM X k 1 1N Y k 1 1M PPAM X k 0 1N Y k 0 1M Both BAM PPAM require fully connected networks M nodes N connections N nodes M nections The BAM requires M N connection weight matrix storing ﬂoating point numbers data realvalued case In addition requires ﬂoating point threshold value node On hand node PPAM requires M bit wide memory Y N bit wide memory X S locations deep worst case It likely lower experiments conducted worst case observed 72 locations node average requirement 12 minimum 1 location S 228 Depending encoding M N nodes network Details calcula tions required recall values store associative pairs BAM PRLAB Kosko 18 Oh Kothari 27 Note memory variables BAM real values BAM calculations require ﬂoating point arithmetic Details operations recalling values storing pairs PPAM Sec tion 5 688 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Fig 12 Environment input percentage previously observed 3x 35 y PPAM 6 PPAM experiments results 61 Experiment setup Although nonbinary nodes possible PPAM implementation uses binary nodes This sim pliﬁes implementation reduces number connections nodes binary outputs require 1bit connections This particularly useful hardware perspective The experiments performed BAM Section 3 repeated PPAM The 3x 14 y PPAM node conﬁguration equivalent 3x 7 y BAM conﬁguration following The 14 PPAM nodes divided sets 7 The seven sonar values input set nodes set receive values sonar The nodes conﬁgured nodes ﬁre generate output Since nodes mutually exclusive nodes represent binary value Therefore 14 nodes distinguish 27 binary values 3x 7 y BAM conﬁguration Similarly 70 nodes 3x 70 y PPAM network divided 10 sets 7 set receiving 7 sonar values The 10 nodes set mutually exclusive set represents decimal value 70 nodes distinguish 107 values 3x 28 y BAM conﬁguration One consideration simulating BAM 6x 28 y 12x 28 y 12x 63 y 30x 28 y node conﬁgurations test effect performance like corresponding PPAM implementation conﬁgurations use bits necessary represent sonar values movement values 62 Results Whereas process tuning correlation matrix BAM revealed possible store associative pairs BAM PPAM node conﬁgurations 3x 28 y successfully able recall 228 associative pairs training dataset Thus ﬁrst phase experiment Section 34 trivial higher PPAM node conﬁgurations experiment performed veriﬁcation results presented order comparison Fig 13 shows trajectory taken agent reinitialized original start point trolled PPAM As seen ﬁgure 3x 14 y PPAM network suffered getting stuck race condition network conﬁgurations followed trajectory original objectavoidance algorithm exactly Only results 3x 35 y conﬁguration reproduced order offer comparison More details results experiments Qadir et al 30 Fig 12 shows breakdown environmental inputs terms environment having observed Table 14 measures associativity 3x 35 y PPAM For case PPAM conﬁdence output memory directly proportional portion neighbourhood input memory If node neighbourhood N nodes receives total N words neighbours input If recall operation node able locate F words memory conﬁdence simply C F N Note imply division operation node nodes output value F The division performed normalize output implemented required companion controller based traditional ALUbased architecture 63 Faulttolerance The 3x 60 y PPAM node conﬁguration simulated test case 3x 70 y node conﬁguration network 10 nodes broken Note case 3x 70 y network trained 10 nodes stop ﬁring O Qadir et al Artiﬁcial Intelligence 175 2011 673693 689 3x 14 y b 3x 28 y 3x 35 y 3x 70 y Fig 13 Agent trajectory PPAM Table 14 Associativity 3x 35 y PPAM Percentage previously observed Recall Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct 100 100 100 154 100 154 100 857 583 972 00 NA NA 100 714 248 809 295 549 23 100 571 186 775 132 561 34 100 428 158 838 127 586 55 100 286 687 687 00 NA NA 100 Fig 14 Error path following 3x 60 y network Table 15 Associativity 3x 60 y network Percentage previously observed Recall Accuracy Critical decisions avoid collision CA critical correct RC critical decisions claimed correct RA correct claimed correct 100 100 100 155 100 155 100 857 583 972 00 NA NA 100 714 248 809 295 549 23 100 571 187 774 132 561 34 100 428 157 830 140 582 56 100 286 687 687 00 NA NA 100 10 nodes stuck 0 0 represents ﬁring encoding This lookup process node nonﬁring node node ﬁring value seen CAM The error robots path innate algorithm path PPAM shown Fig 14 yaxis error xaxis discrete time steps As seen error small small fact difference paths detected observation naked eye Table 15 measures associativity PPAM The important thing note associativity exactly 3x 35 y conﬁguration 7 Discussion When comparing associativity results biased according number vectors present category seen barchart breakdown environment Furthermore case environmental input fully observed 100 column ideal associative memory able recall correct output time conﬁdence Otherwise means memory forgotten training pair 690 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Even pairs forgotten memory able correctly indicate conﬁdence level value recalled The RA values critical measure accurate memory thinks scaled accurate actually higher better 71 Comparing Koskos method PRLAB Comparing accuracy ﬁeld associativity tables Tables 69 evident PRLAB accurate The anomalous cases 100 columns 3x 28 y 6x 28 y conﬁgurations unreliable number test vectors regions low 10 Figs 8 10 In node conﬁgurations Koskos original method PRLAB memory able successfully recall training pairs This expected results experiments conducted train correlation matrix Section 33 Using Koskos method seen 100 observed case value recalled complete conﬁdence wrong RA ﬁeld 100 observed column associativity tables 100 Although 3x 7 y PRLAB node conﬁguration suffered problem higher PRLAB nodeconﬁgurations overcame value recalled claimed correct correct Furthermore general RA measure PRLAB higher RA measure Koskos method For anomalous case RA 714 column Table 6 higher corresponding PRLAB associativity tables note best compare 6x 28 y PRLAB conﬁguration 6x 28 y Kosko conﬁguration closest terms environment input breakdown 7906 8374 Comparing PRLAB measures 334 RA compared Koskos 423 bearing mind Koskos method conﬁdent 459 recalls PRLAB conﬁdent 674 recalls PRLAB result better In addition agent controlled PRLAB able follow trajectory better agent controlled Koskos algorithm Figs 7 9 From comparison Koskos method PRLAB seen PRLAB performs better terms number associative pairs recalled Section 33 terms agents behaviour realtime environment Therefore suﬃcient compare PPAM PRLAB 72 Comparing PRLAB PPAM The trajectory agent controlled 3x 7 y PRLAB conﬁguration identical trajectory agent controlled 3x 14 y PPAM conﬁguration However higher conﬁgurations performance PPAM signiﬁcantly better Furthermore associativity Table 14 seen higher conﬁgurations accuracy PPAM 100 previously observed sonar values perfect Also RA ﬁeld indicates PPAM indicated 100 conﬁdence value recalled correct value This true 100 previously observed sonar values 286 environmental input previously observed 7 sonar values From perspective PPAM inputs 100 observed sonar values exactly 100 observed values corrupted noise Therefore 100 observed sonar values trigger heteroassociative recall This remaining sonar inputs actually value bus hardware value perspective architecture equivalent inputs absent values seen treated Since BAM operate abstract symbols uses arithmetic calculations proper noise test involve random noise pattern based known distribution Nonetheless following discussion aforementioned simpler noise test considered performance BAM expected better random noise source In addition accurate estimate performance BAM presence noise included Sudo et al 35 As seen results PPAM noise tolerant Although 3x 70 y PPAM conﬁguration equivalent 3x 28 y BAM node conﬁguration comparing 3x 35 y PPAM 3x 28 y 6x 28 y 12x 28 y BAM PPAM accuracy 809 noise reaches 286 BAM 459 best This types noise considered Sudo et al 35 noiseadded original patterns For case random noise Sudo et al 35 noise level reaches 15 accuracy drops approximately 95 SOIAM 85 KFMAMFW Kohonen Feature Map Associative Memory Fixed Weights 60 KFMAM batch learning 38 KFMAM sequential learning 0 BAM batch sequential learning Our implementation BAM PRLAB uses redundant memory store associative pairs Although defeats purpose having online associative memory result BAM displaying higher level noise tolerance 977 accuracy best case 728 worst case 15 noise The accuracy 3x 35 y PPAM 15 noise 972 Table 14 3x 70 y PPAM equivalent SOIAM 3x 28 y higher BAMs 991 30 When noise 28 SOIAM 75 accuracy lower 30 Sudo et al 35 Our implementation BAM PRLAB best 46 worst 35 accuracy PPAM hand 81 accuracy 3x 35 y conﬁguration 90 accuracy 3x 70 y case Note increase number nodes results corresponding increase performance indicating PPAM architecture easily succumb issues overtraining The second type noise indicated Sudo et al 35 faultily presented random patterns identiﬁed noise unknown patterns In SOIAM achieved generating unknown output In PPAM O Qadir et al Artiﬁcial Intelligence 175 2011 673693 691 EPuck Mobile Robot bEPuck Maze Fig 15 EPuck platform achieved conﬁdence value output Whatever pattern presented input PPAM generates output conﬁdence measure In cases input pattern unrecognisable conﬁdence measure zero zero Note results reported Sudo et al 35 include RA To knowledge results reported existing literature associative memories include measure equivalent RA measures claimed accuracy recall absolute accuracy recall In opinion measure critical determining performance associative memories 8 Conclusion A mobile robot playerstage environment controlled Bidirectional Associative Memory Protein Processor Associative Memory Two training algorithms attempted BAM original learning algorithm pre sented Kosko 18 popular PRLAB 27 guarantees recall training pairs possible store BAM correlation matrix Parameters tuned attempt maximize memory utilization BAMs Although PRLAB relatively insensitive initial parameter values claimed values worst case dataset orthogonal drop performance PRLAB level original Koskos algorithm Various network conﬁgurations BAM attempted number nodes selected based level quantization desired input pairs However conﬁgurations successfully able recall asso ciative pairs despite tuning parameters This fact BAMs require associative pairs orthogonal Therefore increasing number nodes 2 layers increased capacity BAM unable store recall pairs In fact cases increasing number nodes opposite effect associative pairs orthogonal new encoding required higher node count layer In order store pairs BAM data reencoded orthogonal 34 cited 27 training dataset parsed select critical data points orthogonal suﬃcient underlying distributionalgorithm For solutions means generating training dataset BAM requires priori knowledge associative pairs realistic requirement online realtime learning environments Furthermore requirement PPAM Equivalent PPAM BAM conﬁgurations attempted controllers agent PPAM shown perform better cases displaying higher accuracy noise tolerance Results compared SOIAM PPAM shown tolerant types noise identiﬁed Sudo et al 35 achieving higher accuracy The advantages playerstage environment utilized transferring player implementation EPuck mobile robot 25 results veriﬁed embedded environment A closeup EPuck maze shown Fig 15a overall view robot maze seen Fig 15b The PPAM architecture described Section 5 shown better associativity noise tolerance It shown faulttolerant node conﬁguration 3x 70 y degrades gracefully 10 nodes broken 30 Furthermore PPAM architecture achieves use arithmetic operations Nonetheless signiﬁcant room improvement particularly architecture issues hardware design These outlined Qadir et al 30 A future version PPAM attempted expected retain improve level associativity address hardware issues architecture suitable hardware implementation In addition parallels drawn PPAM Hamming Associative Memory 10 cited 14 future implementations expected compared performance 692 O Qadir et al Artiﬁcial Intelligence 175 2011 673693 Acknowledgements The authors like thank partners BRLUWE4 The research funded EPSRC funded SABRE5 project6 grant FPF06219211 References 1 ME AcevedoMosqueda C YáñezMárquez I LópezYáñez Alphabeta bidirectional associative memories based translator International Journal Computer Science Network Security 6 2006 190194 2 J Amaral J Ghosh An associative memory architecture concurrent production systems Proc IEEE International Conference Systems Man Cybernetics Humans Information Technology 3 1994 22192224 3 G Anzellotti R Battiti I Lazzizzera G Soncini A Zorat A Sartori G Tecchiolli P Lee Totem highly parallel chip triggering applications inductive learning based reactive tabu search International Journal Modern Physics C 6 1995 555560 4 J Cao J Liang J Lam Exponential stability highorder bidirectional associative memory neural networks time delays Physica D Nonlinear Phenomena 199 2004 425436 5 S Chen H Gao W Yan Improved exponential bidirectional associative memory Electronics Letters 33 1997 223224 6 B Coppin Neural Networks 1st ed Jones Bartlett Publishers Inc USA 2004 pp 291326 Chapter 11 7 TD Eom SK Oh JJ Lee Guaranteed recall training pairs exponential bidirectional associative memory Electronics Letters 37 2001 153154 8 BP Gerkey RT Vaughan A Howard The playerstage project Tools multirobot distributed sensor systems Proceedings 11th International Conference Advanced Robotics 2003 pp 317323 9 AJ Greenstead AM Tyrrell Extrinsic evolvable hardware RISA architecture Proceedings ICES 2007 7th International Conference Evolvable Hardware 2007 pp 244255 10 M Hassoun P Watta The hamming associative memory relation exponential capacity dam IEEE International Conference Neural Networks vol 11 1996 pp 583587 11 DO Hebb The Organization Behavior A Neuropsychological Theory new ed Wiley New York 1949 12 P Hogeweg Computing organism interface informatic dynamic processes S Kumar PJ Bentley Eds On Growth Form Computers 1st ed Academic Press Amsterdam London 2003 13 JJ Hopﬁeld Neural networks physical systems emergent collective computational abilities Proceedings National Academy Sciences United States America vol 79 1982 pp 25542558 14 N Ikeda P Watta M Artiklar MH Hassoun A twolevel hamming network high performance associative memory Neural Networks 14 2001 11891200 15 AH Jackson AM Tyrrell Asynchronous embryonics reconﬁguration Proceedings 4th International Conference Evolvable Systems vol 2210 Springer BerlinHeidelberg 2001 pp 8899 16 HD Jong J Geiselmann D Thieffry Qualitative modeling simulation developmental regulatory networks S Kumar PJ Bentley Eds On Growth Form Computers 1st ed Academic Press Amsterdam London 2003 17 T Kohonen Correlation matrix memories IEEE Transactions Computers C 21 1972 353359 18 B Kosko Bidirectional associative memories IEEE Transactions Systems Man Cybernetics 18 1988 4960 19 S Kumar PJ Bentley An introduction computational development S Kumar PJ Bentley Eds On Growth Form Computers 1st ed Academic Press Amsterdam London 2003 20 S Labeit B Kolmerer Titins Giant proteins charge muscle ultrastructure elasticity Science 270 1995 293296 21 P Lee E Costa S McBader L Clementel A Sartori LogTOTEM A logarithmic neural processor implementation FPGA fabric Interna tional Joint Conference Neural Networks 2007 IJCNN 2007 2007 pp 27642769 22 SW Lee JT Kim H Wang DJ Bae KM Lee JH Lee JW Jeon Architecture RETE network hardware accelerator realtime contextaware B Gabrys RJ Howlett LC Jain Eds KES 1 Lecture Notes Computer Science vol 4251 Springer 2006 pp 401408 23 J Lu J He J Cao Z Gao Topology inﬂuences performance associative memory neural networks Physics Letters A 354 2006 335343 24 D Mange M Sipper A Stauffer G Tempesti Toward robust integrated circuits The embryonics approach Proceedings IEEE 88 2000 516543 25 F Mondada M Bonani X Raemy J Pugh C Cianci A Klaptocz S Magnenat JC Zufferey D Floreano A Martinoli The epuck robot designed education engineering P Gonçalves P Torres C Alves Eds Proceedings 9th Conference Autonomous Robot Systems Competitions vol 1 IPCB Instituto Politécnico Castelo Branco Portugal 2009 pp 5965 26 T Nakamoto Evolution universality mechanism initiation protein synthesis Gene 432 2009 16 27 H Oh S Kothari Adaptation relaxation method learning bidirectional associative memory IEEE Transactions Neural Networks 5 1994 576583 28 J Owen How use playerstage On playerstage website httpplayerstagesourceforgenet 2009 29 L Prodan G Tempesti D Mange A Stauffer Embryonics electronic stem cells ICAL 2003 Proceedings Eighth International Conference Artiﬁcial Life MIT Press Cambridge MA USA 2003 pp 101105 30 O Qadir J Liu J Timmis G Tempesti A Tyrrell Principles protein processing selforganising associative memory Proceedings IEEE Congress Evolutionary Computation CEC 2010 Barcelona Spain 2010 31 T Reil Artiﬁcial genomes models gene regulation S Kumar PJ Bentley Eds On Growth Form Computers 1st ed Academic Press Amsterdam London 2003 32 GX Ritter JL Diazde Leon P Sussner Morphological bidirectional associative memories Neural Networks 12 1999 851867 33 D Shen JB Cruz Encoding strategy maximum noise tolerance bidirectional associative memory IEEE Transactions Neural Networks 16 2005 293300 34 P Simpson Bidirectional associative memory systems Technical Report GDEISGPKS02 General Dynamics Electronics Div 1988 35 A Sudo A Sato O Hasegawa Associative memory online learning noisy environments selforganizing incremental neural network IEEE Transactions Neural Networks 20 2009 964972 36 T Tanaka S Kakiya Y Kabashima Capacity Analysis Bidirectional Associative Memory 2000 4 Bristol Robotics Lab University Western England 5 Selfhealing cellular Architectures Biologicallyinspired highly Reliable Electronic systems 6 httpwwwbrlacukprojectssabreindexhtml O Qadir et al Artiﬁcial Intelligence 175 2011 673693 693 37 J Teich Invasive algorithms architectures Invasive Algorithmen und Architekturen Information Technology 50 2008 300310 38 T Toffoli CAM A highperformance Cellular Automaton Machine Physica D 10 1984 195204 39 B Wang G Vachtsevanos Storage capacity bidirectional associative memories Proc IEEE International Joint Conference Neural Networks vol 2 1991 pp 18311836 40 LL Wolpert Relationships development evolution S Kumar PJ Bentley Eds On Growth Form Computers 1st ed Academic Press Amsterdam London 2003 41 Y Wu D Pados A feedforward bidirectional associative memory IEEE Transactions Neural Networks 11 2000 859866 42 S Zhan JF Miller AM Tyrrell A developmental gene regulation network constructing electronic circuits G Hornby L Sekanina PC Haddow Eds ICES Lecture Notes Computer Science vol 5216 Springer 2008 pp 177188 43 Y Zhang Transcriptional regulation histone ubiquitination deubiquitination Genes Development 17 2003 27332740 44 Y Zhang D Reinberg Transcription regulation histone methylation interplay different covalent modiﬁcations core histone tails Genes Development 15 2001 23432360 45 G Zheng S Givigi W Zheng A new strategy designing bidirectional associative memories J Wang X Liao Z Yi Eds Advances Neural Networks ISNN 2005 Lecture Notes Computer Science vol 3496 Springer BerlinHeidelberg 2005 pp 398403