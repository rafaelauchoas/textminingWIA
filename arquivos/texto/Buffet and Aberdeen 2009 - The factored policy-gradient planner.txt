Artiﬁcial Intelligence 173 2009 722747 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint The factored policygradient planner Olivier Buffet Douglas Aberdeen b LORIAINRIA Nancy University Nancy France b Google Inc Zurich Switzerland r t c l e n f o b s t r c t Article history Received 8 October 2007 Received revised form 31 October 2008 Accepted 9 November 2008 Available online 27 November 2008 Keywords Concurrent probabilistic temporal planning Reinforcement learning Policygradient AI planning We present anytime concurrent probabilistic temporal planner CPTP includes continuous discrete uncertainties metric functions Rather relying dynamic programming approach builds methods stochastic local policy search That optimise parameterised policy gradient ascent The ﬂexibility policy gradient approach combined low memory use use function approximation methods factorisation policy allow tackle complex domains This factored policy gradient FPG planner optimise steps goal probability success attempt combination We compare FPG planner planners CPTP domains simpler better studied nonconcurrent nontemporal probabilistic planning PP domains We present FPGipc PP version planner successful probabilistic track ﬁfth international planning competition 2008 Elsevier BV All rights reserved 1 Introduction Only planners attempted handle concurrent probabilistic temporal planning CPTP domains general form These tools able produce good optimal policies relatively small problems We designed factored policy gradient FPG planner goal creating tools produce good policies realworld domains complex features Such features include metric functions resources example concurrent actions uncertainty outcomes actions uncertainty duration actions In single paragraph approach 1 use gradient ascent local policy search 2 factor policy simple approximate policies starting action 3 base policies important elements state implicitly aggregating similar states 4 estimate gradients MonteCarlo style algorithms allow arbitrary distributions 5 optionally parallelising planner The AI planning community familiar valueestimation class reinforcement learning RL algorithms RTDP 1 arguably AO 2 These algorithms represent probabilistic planning problems state space estimate longterm value utility cost choosing action state 34 The fundamental disadvantage algorithms need estimate values huge number stateaction pairs Even algorithms prune states fail scale exponential increase important states domains grow There wealth literature use function approximation estimating stateaction values 56 little adopted 7 example planning community diﬃculty interpreting approximated policies The majority work performed authors employed National ICT Australia Corresponding author Email addresses olivierbuffetloriafr O Buffet dougaberdeengooglecom D Aberdeen URLs httpwwwloriafrbuffet O Buffet httpsmlnictacomaudaa D Aberdeen 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200811008 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 723 On hand FPG planner borrows policygradient PG reinforcement learning 811 This class algorithms estimate stateaction values memory use directly related size state space Instead policygradient RL algorithms estimate gradient longterm average reward process In context stochastic shortest path problems covers probabilistic planning problems view estimating gradient longterm value initial state Gradients computed respect set realvalued parameters governing choice actions decision point These parameters summarise policy plan1 As distinct valuebased approaches function approximation parameters encode absolute value actions plans Instead encode relative merit action Hence hope achieve compact policy representation Stepping parameters direction gradient increases longterm average reward improving policy Also PG algorithms guaranteed converge local maximum approximate policy representations necessitated state space continuous inﬁnite simply large Our setting permits inﬁnite state space action durations modelled continuous distributions The policy takes form function accepts observation planning state input returns prob ability distribution currently legal actions The policy parameters modify function In words policy parameters tune shape probability distributions actions given current planning state observation In temporal planning setting action deﬁned single grounded durative action planning domain deﬁnition language PDDL 21 sense 12 A command deﬁned decision start 0 actions concurrently The command set power set actions started current decisionpoint state From deﬁnition clear size policy learning values grow exponentially number actions We combat command space explosion factoring parameterised policy simple policy action This essentially scheme explored multiagent policygradient RL setting 1314 Each action independent agentpolicy implicitly learns coordinate action policies global rewards achieving goals By number policy parametersand total memory usegrows linearly length grounded input description An advantage function approximators ability generalise learned knowledge If starting state changes exogenous event occurs modify current plan direct interaction FPG return new suggested action effectively instantly The quality suggested action depend different current state frequently encountered training This important quality mixedinitiative planning 15 At time policygradient advantage ignoring irrelevant statesbecause implicit reachability analysisbut focusing effort states relevant best policy naturally encountered frequently This feature reminiscent realtime dynamic programming 1 Our ﬁrst parameterised action policy simple linear function approximator takes truth value predicates current planning state outputs probability starting command A criticism policygradient RL methods compared searchbased plannersor valuebased RL methodsis diﬃculty translating vectors parameters human readable plan Thus second parameterised policy explore readable decision tree highlevel planning strategies Our nonconcurrent nontemporal version FPG International Planning Competition IPC considered form policy ensures action eligible actions chosen Finally elements Relational Online Policy Gradient ROPG planner 16 viewed FPG style parameterised policy We believe contribution paper exploration existing MonteCarlo local optimisation methods feed planning uncertainty In summary local optimisation factored policies framework allow good policies rich domains Sometimes cost long optimisation times local minima How demonstrate FPG approach ﬁnd optimal policies stateoftheart planning methods replanning fail There alternate local optimisation methods parameterisations yield interesting results presented considered useful examples The paper starts background section introducing Markov decision processes MDPs policygradient algorithms MDPs related work Section 3 describes FPG including example function approximators implementation details Experimental results Section 4 overall quality approach pinpointing main strengths weaknesses 2 Background We relevant background planning Markov decision processes policygradient algorithms previous probabilistic planning approaches 1 We generally prefer term policy plan mean ﬁnal output planning phase In probabilistic setting plans change policies 724 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Fig 1 A snippet XML format racing car domain showing probabilistic effect discrete probability outcome continuous probability delay 21 Concurrent probabilistic temporal planning CPTP FPGs input language temporal STRIPS fragment PDDL 21 extended probabilistic outcomes certain durations PPDDL 1718 In particular support continuous uncertain durations functions atstart atend overall conditions ﬁnite probabilistic action outcomes In addition allow effects probabilistic occur time actions duration FPGs input syntax actually XML schema designed map directly PPDDL Fig 1 Our PPDDL XML translator grounds actions ﬂattens nested probabilistic statements discrete distribution action outcomes delayed effects Grounded actions basic planning unit An action eligible begin preconditions satisﬁed It pos sible certain combinations eligible actions mutually exclusive We return possibility later Action execution begin start effects Execution proceeds probabilistic event outcome sam pled outcome effects queued appropriate times We use sampling process enumerating outcomes transpire need simulate executions plan order estimate neces sary gradients A beneﬁt approach sample continuous discrete distributions enumerating continuous distributions possible2 With N eligible actions 2N possible commands Current planners explore command space atically attempting prune commands search heuristically When combined probabilistic outcomes state space explosion cripples existing planners tens actions We deal explosion factorising overall policy independent policies action Each policy learns start associated action given current predicate values independent decisions action policies This idea simplify problem Indeed action policy approximations suﬃciently rich receive state observation learn predict decision actions act optimally The signiﬁcant reduction complexity arises approximate policies implicitly assumes similar states similar policies The FPG planner aims produce good plans rich large domains It complete sense return solution exists In particular FPG shares completeness problems temporal planners 1920 arise decisions start new actions restricted times event queued occur happenings Since examples frequent existing solutions computationally prohibitive require signiﬁcant restrictions domains Temporal Graphplan TGP style actions 20 choose attempt guaranteed completeness FPG 22 Probabilistic planning Although FPG initially developed objective handling concurrent probabilistic temporal planning CPTP problems simpliﬁed version called FPGipc participated probabilistic track ﬁfth International Planning Com petition IPC5 2006 We simpliﬁcations Section 323 This probabilistic planning PP setting seen restriction CPTP objective maximise probability reaching goal Candidate planners process PPDDL speciﬁcations adl requirements 1721 23 Previous work Previous probabilistic temporal planners include DUR 22 Prottle 4 Military Operations MO planner 23 All algorithms use optimised form dynamic programming RTDP 1 AO 2 associate values stateaction pair However requires values stored encountered state Even algorithms 2 We sample integer times maximum permitted makespan distributions reality ﬁnite extremely large O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 725 prune state space ability scale limited memory size Tempastic 24 uses generate debug repair planning paradigm It overcomes state space problem generating decision tree policies sample trajectories follow good deterministic policies repairing tree cope uncertainty This method suffer highly nondeterministic domains rare example approach permits modelling continuous distribu tions durations Prottle DUR Tempastic minimise plan duration failure probability The FPG planner allows simple tradeoffs metrics The 2004 2006 probabilistic tracks International Planning Competition IPC represent cross section recent approaches nontemporal probabilistic planning Along FPG planner entrants included FOALP Paragraph sfDP FOALP 25 solves ﬁrst order logic representation underlying domain MDP prior producing plans speciﬁc problems drawn domain Paragraph 26 based Graphplan extended probabilistic framework sfDP 27 uses symbolic form dynamic programming based Algebraic Decision Diagrams ADDs A surprisingly successful approach competition domains FFrePlan 28 winning 2004 competition A subsequent version achieved ﬁrst place 2006 competition FFrePlan uses FF heuristic 29 quickly ﬁnd potential short path goal It creating deterministic version domain Thus attempt directly optimise probability reaching goal costtogo In practice domains ability reach goal leads good performance However replanning approach perform poorly cost failure taken account 30 Policygradient RL multipleagents MDPs described 1314 providing precedent factoring policygradient RL policies independent agents action This paper builds earlier work presented 3132 The challenge learning generalise problems particular domain use function approxi mation generalisation The classical view learning planning acquire knowledge given domain 1 planning small problem instances 2 reusing knowledgesuch heuristic rulesto plan larger domains 33 In direction attempts Relational Reinforcement Learning RRL ﬁnd generic policies plan ning problems expressed ﬁrstorder logic 71634 But remains challenging task search space larger abstract notions required aboveAB numberofblocksonXN Blocksworld The Approximate Policy Iteration algorithm Classy similar FPG sense avoids computing value function heavily relies MonteCarlo simulations attempt factorise policy The Relational On line PolicyGradient ROPG 16 uses exactly policygradient algorithm FPG ROPG learns higherorder control strategy follow given state While contribution ROPG develop possible control strategies policygradient component needed learn strategies work 24 Markov decision processes policygradient algorithms We Markov decision process MDP framework overview gradient ascent policy gradient algorithms 241 Markov decision processes A ﬁnite partially observable Markov decision process consists possibly inﬁnite set states s S ﬁnite set actions c C correspond command concept probabilities Ps command c reward state rs S R3 ﬁnite set observation basis vectors o O action policies lieu complete state descriptions cid4 s c making state transition s s cid4 We trade complexity action policies state information provided observation As provide state information policies richer richer At extreme end spectrum provide unique command state essentially state command lookup table At end policy knows current state generalise states estimating best stationary policy However case FPG linear approximator construct policy observation vectors state follows Each predicate valueand predicate valuesbecomes observation bit We set bit 1 asserted predicate 0 A constant 1 observation bit provided bias element assist linear approximator Goal states occur predicates functions match PPDDL goal state speciﬁcation From failure states impossible reach goal state usually time resources run atend overall condition invalid These classes end state form set terminal states ending plan simulation Policies stochastic mapping observation vector o generated current planning state probability distribution commands Fundamentally necessary enable exploration command space Over course optimisation hope policy distribution increasingly peaked equally optimal commands Let N number grounded actions available planner For FPG command c binary vector length N An entry 1 index n 1 N means Yes begin action n 0 entry means No start action n The probability command Pc o θ conditioning θ reﬂects fact policy tuned set real 3 This work remains valid reward depends complete transition rs c s cid4 We consider simpler setting readability reasons 726 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 1 Initialisation 2 t 0 3 repeat t t 1 4 θ t θ t1 α Rθ t1 5 6 stoppingCriterioncid3 7 Return θ t Algorithm 1 Generic gradientAscentR θ 0 α cid3 valued parameters θ Rp Commands noneligible actions guaranteed probability 0 We assume stochastic policies values θ reach terminal states ﬁnite time executed s0 This enforced limiting maximum makespan plan FPGs optimisation criteria general maximises longterm average reward Rθ lim T 1 T Eθ cid2 T 1cid3 cid4 rst t0 1 expectation Eθ distribution state trajectories s0 s1 induced current joint policy In context planning instantaneous reward provides action policies measure progress goal A simple reward scheme set rs 1000 states s represent goal state 0 states To maximise Rθ goal states reached frequently possible This desired property simultaneously minimising steps goal maximising probability reaching goal failure states achieve reward There pitfalls avoid describing reward scheme For example large negative reward failure optimisation choose extend plan execution long possible reduce frequency negative rewards We provide intermediate rewards progress goal These additional shaping rewards provide im mediate reward 1 achieving goal predicate 1 goal predicate unset Shaping rewards provably admissible sense change optimal policy 35 The shaping assists convergence domains long chains actions necessary reach goal proved important achieving good results IPC domains The reward shaping helps reinforcements occur soon action moves closer distant goal goal reached This helps solve reward assignment problem 242 Gradient algorithms We want maximise Rθ gradient ascent That repeatedly computing gradients Rθ stepping param eters direction In setting gradient vector operator mapping differentiable function R Rp R R Rp Rp deﬁned Rθ1 θp 2 R θ1 R θn θ1 θp θ1 θp A gradient ascent iterative algorithm ﬁnd local maximum Rθ The principle compute sequence θ ttT following parameter values point θ t direction gradient point Algorithm 1 gives generic overview process requires differentiable function R starting point θ 0 stepsize α 0 threshold cid3 0 stopping criterion The stopping criterion typically function parameters gradients time andor number steps The details Algorithm 1 implemented constitute entire complex ﬁeld FPG actually online gradient ascent function R additional external inputs effect gradient step step In FPGs case additional inputs observation planning state ot However shall transpire Markov nature process means weighted average direct policy gradients speciﬁc observations converges approximate estimate gradient Rθ safely Algorithm 1 The gradients stochastic planning domains contain uncertainty sampling distributions actions The combination online stochastic means advanced gradient ascent al gorithms line searches BFGS conjugate methods immediately applied 36 However comment use approximate secondorder gradient ascent methods discussion Gradient optimisation methods perform local search That greedily step direction gradient reaching maximum global maximum We accept possibility gradient ascent far tractable global optimisation methods tabula rasa dynamic programming variants In practice gradient ascent achieve good results planning domain global optimiser return policy typically memory runs O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 727 243 Introduction policygradient algorithms Computing gradient longterm average reward Rθ closedloop Markov decision process entirely trivial However papers solved problem practical algorithms 891137 The beneﬁts approach compared reinforcement learning methods 1 local convergence function approximation partial observability 2 memory usage linear number parameters policy function 3 convenient optimal implicit approach exploration versus exploitation tradeoff We use Baxter Bartletts approach policygradient algorithms 37 For historical reasons begin introduction based Williams REINFORCE algorithm 8 credited ﬁrst policygradient method reinforcement learning context Horizon 1 gradient estimate Let ﬁrst consider planning problem maximum horizon length 1 This means starting random state s choosing command c executing transitioning end state s The transition generates instant reward r The criterion optimise cid4 Rθ Eθ cid11 rs c s cid4 cid12 cid3 socscid4 Ps Po s Pc o θ Ps cid13 cid14cid15 Psocscid4 cid4 s c cid16 rs c s cid4 estimated N iid samples r Rθ 1 Similarly taking gradient Rθ gives cid18 cid3 Rθ Ps Po s Pc o θ Ps cid4 s crs c s cid4 socscid4 cid3 cid11 Pc o θ Ps Po s cid12 Ps cid4 s crs c s cid4 N cid17 N k1 rk cid19 Pc o θ Pc o θ cid19 socscid4 cid3 cid18 Ps Po s socscid4 cid3 cid18 Pc o θ Pc o θ rs c s cid19 Pc o θ Pc o θ rs c s cid4 socscid4 cid18 Eθ cid19 Pc o θ Ps cid4 s crs c s cid4 cid4 Ps Po s Pc o θ Ps cid13 cid14cid15 Psocscid4 cid4 s c cid16 provides following estimate N samples Rθ 1 N Ncid3 k1 ln Pck ok θ rk noting Pckokθ Pckokθ ln Pck ok θ Horizon 1 REINFORCE algorithm In horizon 1 online gradient ascent parameter θi incremented iteration instant estimate gradient N 1 cid6θi αrei α 0 step size learning rate factor ei ln Pc o θ θi called characteristic eligibility θi Under conditions let consider following increment cid6θi αir biei αi speciﬁc learning rate parameter θi bi baseline If bi conditionally independent o αi depends θ k learning algorithm called REINFORCE algorithm Such algorithm interesting property 8 Theorem 1 Ecid6θ θ ascent direction Its inner product θ Er θ non negative zero θ Er θ 0 REINFORCE parameters Let observe appropriately decreasing step size αi chosen satisfy standard stochastic function approximation conditions 38 ensures θi converges limit value missing local optimum choice reinforcement baseline makes possible improve convergence behavior example bi r empirical value Er leads instabilities bi 0 compares instant reward usually received equivalent variance reduction gradient estimates 39 728 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 To characteristic eligibility ei computed let assume example single action available possible commands execute c 0 execute c 1 parameterised policy takes following form cid20 Pc o θ f o θ 1 f o θ c 0 c 1 f f o θ differentiable respect parameters θ1 θp One f simple logistic regression 1 1expocid2θ Then characteristic eligibility parameter θi given ln Pc o θ θi 1 Pc o θ cid21 1 f oθ 1 1 f oθ Pc o θ θi f oθ θi f oθ θi c 0 c 1 Indeﬁnite horizon REINFORCE algorithm For true policygradient approach need extend algorithm problems indeﬁnite horizon Each trial ﬁnite possibly unknown duration A ﬁrst approach given trial length N accumulate reward rtrial compute update direction parameter θi cid6θi αirtrial bi Ncid3 k1 eik eik characteristic eligibility evaluated time k Another option update time step k cid6θi αi cid22 rk bi cid23 kcid3 eit t1 The approach advantage providing online algorithm makes use instant rewards soon obtained REINFORCE algorithms exactly gradient ascent algorithms They follow estimate ascent direction estimate gradient direction step sizes different parameter Another 90 issue REINFORCEs updates weight old recent decisions settings old decisions assigned little credit current reward This prefer policygradient algorithms designed inﬁnite horizon problems In section present reinforcement learning algorithms FPG based direct estimate gradient meant inﬁnite horizon POMDPs 244 Baxter Bartletts policygradient algorithms We follow presentation Baxter Bartlett 1037 note derivations 911 differ substantially resulting algorithms vary little We begin overview policygradient approach expression exact gradient Rθ Theorem 1 Exact policygradients 37 Suppose S possible planning states4 Let P θ S S ergodic stochastic Note P θ describes Markov chain resulting transition matrix gives probability transition state s state s setting parameter vector θ Let π θ 1 S column vector gives stationary probability particular state derived unique solution π P θ π Let r 1 S vector reward state The identity matrix given I e column vector 1s With deﬁnitions cid23cid22 cid4 cid2 Rθ π θ cid22 P θ cid2 I P θ eπ θ cid231 r 3 The point wish theorem policygradients true modelbased plan ning 40 The planning domain problem speciﬁcation contain information necessary calculate π θ P θ given initial policy described θ uniformly randomly starting eligible actions We construct reward vector r described Section 241 Also P θ computed matrix inverse 3 exists Thus state 4 See 37 details required extend theorem continuous state spaces O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 729 space ﬁnite continuous durations makespan limit perform exact modelbased policy gradient optimisation repeatedly solving 3 Algorithm 1 loop This preferable suggested approach model simply create plan execution simulator use reinforcement learning However sizeable planning problems involve millions possible states making O S 3 matrix inversion 3 intractable It worth noting P θ typically sparse This fact truncated iterative solution 3 perform modelbased policygradient systems tens thousands states 40 As aside attempted exploit structured ADD algebraic decision diagram representation 3 ADDs compactly represent factored matrices standard matrix operations redeﬁned terms ADD manipulations This fact previously planning 41 Our idea create parameterised ADD allowed analytic solution 3 eﬃciently evaluated parameter values However initially compact ADD representations P θ π θ explode size solution matrix inverse 42 Because exact computation gradient intractable use MonteCarlo gradient estimates generated repeatedly simulating plan executions long Markov process Theorem 2 Estimated approximate policygradients 37 Let rt scalar reward received time rt rst Let β 0 1 discount factor Then Rθ lim β1 lim T 1 T Tcid3 t1 θ Pct ot θ Pct ot θ Tcid3 τ t1 βτ t1rτ 4 This quantity estimate exact T approximation exact β 1 The detailed derivation scope paper wish provide insight expression relates 3 In particular role discount factor β The theorem derived 3 ﬁrst establishing cid22 cid2 I P θ eπ θ cid231 r lim β1 vβ θ vβ θ 1 S vector discounted state values current policy For state s deﬁned vβ θ s Eθ βtr Xt X0 s cid4 cid2 cid3 t0 Xt random variable denoting state time t steps future Note usual deﬁnition discounted value reinforcement learning 6 Combining 3 gives Rθ lim β1 cid2 π θ cid22 cid23 P θ vβ θ The Ergodic Theorem applied turn summations implicit matrix operations state state actions observations MonteCarlo estimate single inﬁnite trajectory states states actions observations So β introduced You observe inﬁnite summation β 1 4 unbounded That inﬁnite variance arises MonteCarlo estimate This loosely thought trying assign credit reward possibly inﬁnite number actions past So β creates decaying artiﬁcial horizon long ago actions occur assigned credit achieving current reward However β 1 introduces bias 37 Thus try β close 1 possible achieving reasonable estimates gradient Alternatively observe point state reset end plan execution impose true horizon impact actions rewards 8 leave β 1 achieve unbiased estimate In practice variance reduction β 1 useful ﬁnitehorizon planning We deﬁned form policy Pct ot θ t We discuss Section 32 note parameterised policy function construct produce correct probability distribution Its log derivative exist bounded satisfy assumptions described 37 Note 4 requires looking forward time observe rewards reverse summations Rθ lim β1 lim T 1 T T 1cid3 tcid3 rt t0 τ 0 βtτ Pcτ oτ θ Pcτ oτ θ 5 This easy implement eligibility trace et place second summation shown Algo rithm 2 The eligibility trace et contains discounted sum normalised policy gradients recent commands equivalent logpolicy gradients This provide intuition algorithm works stepping parameters direction eligibility trace increase probability choosing recent commands similar observations recency weighting determined β But relative value rewards indicate increase decrease probability recent command sequences So instant gradient time step rt et 730 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 1 t 0 2 repeat 3 4 5 ot simgetObservation sample ct P ot θ t et βet1 θ Pct ot θ t Pct ot θ t simdoActionct rt simgetReward OlPomdp 6 7 8 9 10 11 12 13 stoppingCriterioncid3 14 OlPomdp 15 t1 tgt1 rt et gt 1 t t 1 θ t1 θ t αrt et return gt batch estimate Algorithm 2 policyGradθ 0 Simulator sim α Fig 2 A high level illustration parallelise FPG Each node contains separate process implementing Algorithm 2 There offered optimisation methods instant gradients 10 These correspond OlPomdp batch cases distinguished Algorithm 2 OlPomdp simple online stochastic gradient ascent θ t1 θ t αrt et scalar gain α Alternatively ConjPomdp averages rt et T steps compute batch gradient gt approximating 4 followed line search best step size α search direction5 OlPomdp considerably faster ConjPomdp highly stochastic environments tolerant stochastic gradients adjusts policy step We prefer single processor experiments However batch approach parallelising FPG shown Fig 2 Each processor runs independent simula tions current policy ﬁxed parameters Instant gradients averaged simulations obtain processor estimate gradient 4 A master process averages gradients processor broadcasts resulting search direction All processors evaluating points search direction establish best α Once master process broadcasts ﬁnal step size The process repeated aggregated gradient drops threshold 3 FPG We policygradients optimise CPTP policies We begin describing construction plan simulator 31 State space simulator FPGs planning state makespan far plan starts time 0 absolute time 5 We apply conjugation gradients described 10 gives algorithm conjugate directions collapse noise O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 731 truth value predicate function values dynamic length queue future events including outcome sampling events outcome end events effect implementation events exogenous events In particular state eligible actions satisﬁed atstart preconditions execution Recall command decision start set eligible actions While actions individually eligible starting concurrently require resources Or starting cause precondition eligible action invalidated deterministic starteffects Both examples actions consider mutually exclusive mutex We deal type conﬂict determining mutexes purpose deciding start actions For example consider mutexed outcomes This probabilistic planning means mutexes occur If occur plan execution enters failure state moving optimisation away policy The planner handles execution actions timeordered event queue When starting action atstart effects processed adding effect events queue delayed atstart effects Additionally sample outcome event scheduled point execution action duration possibly sampled continuous distribution The sampleoutcome event indicates point chance decides particular discrete outcome triggered given action This results adding corresponding effect events outcome atend effects event queue possibly additional sampled delay An action ends possible effects action occurred arbitrary deﬁnition given execution model These deﬁnitions allow predicates change time action The element state presented policy truth value predicate We trivially present additional features dynamically sized event queue However empirically state predicates good summary overall planning state This intuitive domains human designers important state predicates Note particularly dangerous supply representation future event queue policy Doing allow decisions based particular probabilistic outcome occurred foreseen real life Exogenous events handled inserting events event queue occur They include manually descheduling action effect probabilistic outcome Note permits form mixed initiative plan ning useful policy adjust unexpected events To estimate policy gradients need plan execution simulator generate trajectory planning state space It takes commands factored policy checks mutex constraints implements atstart effects queues sampleoutcome events The state update proceeds process sampleoutcome effect events queue new decision point met Decision points equate happenings occur 1 time increased decision point 2 events time step Under conditions new action chosen possibly noop best action simply proceed event The process simulating plan execution command choice described simulateTillHappening Algorithm 3 When processing events algorithm ensures running actions violated overall conditions If happens plan execution ends failure state Note making decisions happenings results FPG incomplete domains combinations effects atend conditions 1922 A simple ﬁx set maximum delay d consecutive decision points In discrete time case d 1 guarantees completeness small d introduces feasible plans Another ﬁx learn long wait decision point6 We pursued approach Only current parameters eligibility trace initial current states current observation kept memory point time The number parameters depends choice policy function approximation typically O N o N number actions o dimensionality state observations This stark contrast dynamic programming based planners expand memory states relevant planning number states generally exponential number statevariables We emphasise low memory use key advantage FPGs approach The planning problem orders magnitude larger CPTP planners attempt However timespace trade FPG require time approaches compensate retaining detailed state information 32 Choice function approximator A beneﬁt FPG approach arising use policygradients ﬂexibility choice function approxima tor It possible trade richness policy representation compactness ability trained quickly 6 This feasible continuous time case policygradient algorithms optimise controllers continuous action spaces 732 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 atn isMutex return MUTEX saddEventatn sampleoutcome stime sampleatn durationdistribution f atStartEffectsatn f delayedEffectsatn saddEvent f effect stime sample f delaydistribution stime maximum makespan soperationGoalsMet sprocessEffect f sgoal true return sfailure true return 1 atn Yes ct 2 3 4 5 6 7 8 repeat 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 sisDecisionPoint sfailure true return sprocessEffect f event snextEvent stime eventtime typeevent effect sprocessEffecteventeffect sanyEligibleActions snoEvent typeevent sampleoutcome sample outcome event f immediateEffectsout f delayedEffectsout saddEvent f effect stime sample f delaydistribution Algorithm 3 simulateTillHappeningState s Command ct On hand diﬃcult choose representation achieves best balance considerations This section provides generic approximator requirements discusses speciﬁc approximators The command ct at1 at2 N time t combination independent Yes No choices grounded action policies Each policy independent set parameters θ Rp θ 1 θ 2 θ N With independence parameters command policy factors Pct ot θ Pat1 N ot θ 1 θ N Pat1 ot θ 1 Pat N ot θ N 6 The computation logpolicy gradients factorises trivially It necessary action policies receive observation advantageous different observations different actions leading decentralised paradigm7 Similar factored policygradient approaches adopted 13 14 The main requirement action policy log Patn ot θ n differentiable bounded respect parameters choice action start atn Yes No 37 The gradient bounded In situation action policies access state information rewards policy approximations suﬃciently rich action policies able coordinate optimally given state information available This coordination natural consequence learning process individual actions taking rewarding joint actions reinforced joint actions preferred 43 We note diﬃculties arise equivalent optimal policies example policies pick single action eligible actions equally optimal In case gradient based planner stuck saddle regions The usual somewhat unsatisfactory resolution rely random initialisation slightly higher probability equivalent policy outset If action policy sees different state information optimal coordination NExphard 44 Methods coordinated RL 45 exist eﬃciently solve problems performing belief propagation agents policies The policies conditionally independent structure conditional independence exploited process eﬃcient In context policies conditionally dependent mutex relationships actions However far experimented approach We ensure action policies observation 321 Linear function approximators One simple effective action policy linear approximator mapped probabilities logistic regression func tion8 7 Although note use single shared simulator optimisation means truly decentralised planning algorithm 8 In RL function commonly expressed softmax Gibbs Boltzmann distributions O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 733 Fig 3 High level overview FPGs decision making loop showing linear function approximators Patn Yes ot θ n 1 cid10 t θ n 1 expo Patn No ot θ n 1 Patn Yes ot θ n 7 Recall observation vector o vector representing current predicate truth values plus constant bias True predicates 1 entry false predicates 0 The bias entry constant 1 allows linear approximator represent decision boundary pass origin If dimension observation vector o set parameters θ n thought o vector represents approximator weights action n The required logpolicy instant gradients parameter θ θ n θ n Patn Yes ot θ n Patn Yes ot θ n θ n Patn No ot θ n Patn No ot θ n cid22 cid10 ot exp t θ n o cid23 Patn Yes ot θ n ot Patn Yes ot θ n 8 These logpolicy gradients added eligibility trace Algorithm 2 line 5 based yesno decisions action Looping calculation eligible actions computes normalised gradient probability joint command 6 Fig 3 illustrates scheme Initially parameters usually set 0 giving uniformly random policy encouraging exploration command space To increase longterm average reward policy parameters gradually adjust prefer commands Typically policy start converge function gives high probability commands established good If FPG maintains commands similar probabilities given particular observation indicative FPG unable determine command better This insuﬃcient state information encoded o parameterisation simple commands equivalent long run 322 Trees experts To demonstrate ﬂexibility FPG approach develop alternative parameterised policy function This policy task switching collection known expert policy strategies As described achieved partlydeﬁned decision tree decision nodes tuned stochastic gradient ascent Rather start uniform policy given selection heuristic policies work range domains For example probabilistic setting access replanner optimal nonconcurrent planner naïve planner attempts run eligible commands Indeed best planner invoke depend current state overall domain The decision tree policies described simple mechanism allow FPG switch high level expert policies We assume user declares initial tree available policies The leaves represent policy follow branch nodes represent decision rules policy follow We learn 734 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Fig 4 Decision tree action policy cid24 rules In factored setting action decision tree policy All actions start template tree adapt independently Whether start action decided starting root node following path tree visiting set decision nodes D At node apply hardcoded branch selection rule sample stochastic branch rule parameterised policy node Assuming conditional independence decisions node probability reaching action leaf l equals product branch probabilities node Pa l o θ n Pd cid4 o θ nd 9 dD d represents current decision node d represents node visited tree The probability branch cid4 o θ nd functions differentiable function followed result hardcoded rule 1 The individual Pd parameters linear approximator Parameter adjustments simple effect pruning parts tree represent poor policies region state space cid4 For example nodes A D F H Fig 4 represent hardcoded rules switch deterministically Yes No branches based truth statement node current state Nodes B C E G parameterised select branches result learning In example probability choosing left right branches single parameter logistic function independent observations For action n decision node C action duration matters PYes o θnC PYes θnC 1 expθnC 1 In general policy pruning function current state The log derivatives Yes No decisions given 8 noting case o scalar constant 1 The normalised action probability gradient node added eligibility trace independently We product terms 9 cancel taking gradient log expression If parameters converge way prunes Fig 4 dashed branches policy action IS eligible probability action success NOT matter duration action DOES matter action IS fast start start Thus encode highly expressive policies parameters parameters case Fig 4 This approach allows extensive use control knowledge FPG simply switch experts Even ignored example state taken account useful 323 Approximators nontemporal probabilistic planning We discus particular case nonconcurrent nontemporal probabilistic planning PP domains The probabilistic track international planning competition IPC based class domains In PP choose action currently eligible actions This contrast choosing set actions contribute command Even somewhat different problem CPTP similar factorisation applied The factored policy uses parameterised policy function action But instead turning output Yes No probability distribution action logistic regression compute single probability distribution eligible actions The probability choosing n time t O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Pat n ot θ cid17 cid10 t θ n expo cid10 t θ k kEot expo Eot set eligible actions The normalised derivative Pat n ot θ Pat n ot θ cid22 ot U n P ot θ cid23cid2 735 10 U n unit vector 1 element n P ot θ treated N dimensional probability vector Also simulateTillHappening function substantially simpler We implement effects action sample outcome implement effects Time event queue longer elements state 324 The relational online policy gradient The relational online policy gradient ROPG described 16 It FPG planner optimisation component ROPG cast example FPG policy parameterisation ROPG relational reinforcement learning RRL approach solving probabilistic nonMarkovian decision processes RRL computes policies expressed relational language work range problems drawn domain The state observation spaces vary problem instances higherorder representations capture commonalities concise way The aim learn policies small number problems generalise problems domain An initial reasoning phase discovers candidate relational control strategies In second phase policygradient learning discover use strategy ROPG uses exactly Algorithm 2 order optimise choice control strategy current observation ROPG chooses action step sense resembles FPGs IPC policy Section 323 However output parameterised policy choice strategy evaluate resolved deterministically produce grounded action In sense ROPG closely resembles single layer tree experts policy Section 322 Assume N control strategies choose available current time step The observations ot provided ROPG thought binary vectors giving eligibility strategy according relational representation current state In fact nonMarkovian nature domains observations control rules rewards histories instead current state The ROPG parameterised policy expressed Pat n ot θ κn ot cid17 expθ n N k1 κk ot expθ k κn ot 1 element n ot nonzero 0 The use κ restricts policy choose eligible strategies analogous use function Eot 10 The results shown 16 demonstrate effective strategy automatically evaluating generated control strategies 33 The policygradient algorithm Algorithm 4 completes description FPG showing implement gradient estimate 4 planning factored action policies assuming complex CPTP case The algorithm works repeatedly simulating plan executions 1 initial state represents makespan 0 plan confused step number t algorithm 2 policies receive observation ot current state st 3 policy representing eligible action emits probability starting 4 action policy samples Yes No issued joint command 5 plan state transition sampled Section 31 6 planner receives global reward new state 7 OlPomdp parameters immediately updated αrt et parallel planning rt et averaged T steps passed additional line search phase controlled master process Note link planning simulator line 9 If simulator indicates action impossible mutex constraint planner successively disables action command according arbitrary lexical ordering command eligible This mutex elimination process considered systems dynamics Some actions automatically cancelled mutexed actions inﬂuence policygradient algorithm While like dangerously ad hoc scheme wrong decision longterm average reward suffer choice actions led mutex discouraged future More studied approaches dealing mutexes permit faster learning Examples include coordinated RL 45 provide eﬃcient methods include dependencies action policies 736 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 1 Set s0 initial state t 0 et 0 init θ 0 randomly 2 repeat 3 4 5 6 7 et1 βet ot simgetObservation eligible action Evaluate action policy n Patn Yes No o θ tn Sample atn Yes atn No et1 et1 θ Patn oθ tn Patn oθ tn 8 st1 simulateTillHappeningst ct MUTEX arbitrarily disable action ct mutex 9 10 11 12 13 14 15 stoppingCriterioncid3 rt simgetReward θ t1 θ t αrt et1 st1isTerminalState st1 s0 t t 1 Algorithm 4 FPG based OlPomdp Line 8 computes normalised gradient sampled action probability adds gradient nth actions parameters eligibility trace Because planning inherently episodic alternatively set β 1 reset et time terminal state encountered However empirically setting β 095 performed better resetting et probably variance reduction associated lower β The gradient parameters relating action n 0 We compute Patn ot θ n gradients actions unsatisﬁed preconditions If actions chosen begin issue noop action If event queue process happening time simply incremented 1 ensure possible policies eventually reach maximum makespan reset state 331 The Q learning variant FPGs approach raises question valuebased reinforcement learning algorithm factored approxi mator work FPGs factorisation easy extend nontemporal nonconcurrent case The result Factored Q learning planner FQL identical FPG policygradient algorithm replaced standard Q λlearning algorithm linear approximator learn Q value associated action Contrary FPG simple way trading exploration exploitation Our choice learning use cid3greedy policy cid3 following sigmoid function going 095 005 allocated learning period We explored settings λ learning rate FQL convergence guarantees Even λ 0 Q learning linear function approximation diverge exploration strategies 46 34 Implementation details This section devoted engineering details needed achieve good performance FPG Unless speciﬁed details FPG CPTP FPGipc PP 341 Grounding actions variables Because PPDDL uses ﬁrstorder constructs preliminary step creating function approximators ground domain That build set relevant actions Ar actions reachable initial state set relevant predicates P r predicates value change relevant actions Exact reachability analysis determine precise sets Ar P r expensive Instead simple iterative process compute supersets A P following relaxed reachability analysis The relaxation similar employed planners 47 It relies fact PDDL based notion atoms variables predicates States described list currently truepositive atoms Two sets grounded actions initially set new eligible actions An set processed actions A p The reachability analysis starts putting initially true atoms set P shown Algorithm 5 alternates adding new eligible actions An based atoms P positive preconditions determine action eligible searching new atoms add P based actions An moved A p processed positive effects O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 737 1 Initialisation 2 An new eligible actions 3 A p processed eligible actions 4 P atoms s0 5 repeat An An 6 Pick An 7 An Ana 8 P P atomsInEffectsa 9 cid25 A p A p 10 11 An eligibleActionsP A p cid25 cid25 Algorithm 5 ground The process stops An 342 Progress estimator In FPGipc reward function shaped simple progress estimator The progress estimator counts goals facts added deleted transition This possibly negative number f multiplied coeﬃcient ρ compute additional reward rprogress Provided net progress reward episode 0 ensured adjusting ﬁnal goal state reward shaping alter overall objective function 35 This approach works goals speciﬁed conjunction facts satisfy It improved tackle complex goals involving metric comparisons disjunctions 343 Saving computation time rewards rare Domains probabilistic track IPC5 reward reaching goal So time rt 0 particularly progress estimator This makes possible avoid delay computations OlPomdp follows update parameter vector θ rt cid12 0 action remains ineligible r 0 discount β corresponding eligibility vector ea increment counter action eligible r 0 discount ea β performing normal update reset zero This process compatible use baseline rewardie subtracting running average r reward instant reward rt commonly RL reduce variance gradient estimates However gain additional simulated plan executions generated saved computation time cid25 344 Saving computation time actions Section 341 covers creating superset relevant actions Let denote set A Ar Ai r denotes relevant subset irrelevant subset It useless perform computations actions Ai eligible eligibility traces remain null But know advance actions belong Ai In vein observe actions eligibility vector ea remains null episode long eligible During episode actions remain ineligible long time So computations avoided storing actions eligible Irrelevant actions eligible The result restarting initial state set hasBeenEligiblea false A action eligible set hasBeenEligiblea true eligible update corresponding eligibility vector ea parameter vector θ remain 0 We exploit strategy reduce memory usage If suspects irrelevant actions A allocate memory actions parameter vector eligibility vector action encountered ﬁrst time 345 Software The reinforcement learning routines provided LibPG C library written authors multiple research projects It makes use Boost C libraries matrix operations optionally ATLAS basic linear algebra routines library faster mathematics In CPTP case Algorithm 3 implemented scratch C class domains described 21 As dis cussed XML domains problems writing PPDDL XML translations necessary This decision ease parsing transforming XML range programming languages In PP case tied LibPG MDPSim package oﬃcial simulator IPC 738 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Table 1 Main parameter settings FPGipc IPC5 FQL Other TD discount factors experimented λ 08 λ 1 nearidentical results FPG FQL step size α 000005 α 0001 discount factor β 085 discount 085 λ NA λ 0 success reward rs 1000 rs 1000 progress reward rprogress 100 rprogress 100 Table 2 Summary nontemporal domain results IPC5 benchmarks Values plan simulations reach goal 30 runs A dash indicates planner run failed produce results Domain BW Exploding BW Tire Zeno Drive Elevator Pitchcatch Schedule Random 4 Experiments FOALP sfDP FPG Paragraph FFreplan 100 24 82 100 29 31 7 63 43 75 27 63 76 23 54 65 31 91 7 9 1 5 86 52 82 100 71 93 54 51 100 FQL 0 7 11 7 11 1 0 54 10 All domains source code following experiments available httpfpgloriafr 41 Probabilistic planning Because probabilistic planning setting International Planning Competition simpler widely known ﬁrst present experiments performed FPGipc In section refer FPGipc simply FPG confusion possible 411 IPC One beneﬁt IPC offer variety benchmarks recognised AI planning community This main reason developing FPGipc Many approximations FPG designed cope combinatorial action space little reason believe FPG competitive nontemporal domains In competition time limit 40 minutes problem instance including optimisation evaluation Table 2 shows overall summary results IPC5 domain planner These results include FFreplan FQL run competition following similar rules FQL slight advantage run powerful machine competition server Core2 DUO 32 GHz limited optimisation time 10 minutes evaluation time FPGipcs FQLs parameters manually tuned best results domains IPC4 preliminary benchmarks IPC5 tuned competition problems These parameters given Table 1 The results based 9 PPDDL speciﬁed domains averaged 15 instances domain tested 30 simulations plans instance This simulations reliably measure performance planner competition time constraints required small number Many domains Blocksworld BW classical deterministic domains noise added effects FPG FFreplan prove robust variety domains planners They able run problem instances return results Software maturity explains performance FPG FFreplan largely based stable software FPGMDPSim LibPG FFreplanFF planners bugfree time competition On domains Paragraph FOALPwhich optimal planners unlike FPG FFreplanoutperformed planners problem instances We defer 48 details FQL turns competitive problems Drive Random Schedule Tireworld domains9 We surmise FPG successful FQL FQL aims approximating Q value stateaction pairs generally suﬃcient FPG simply return good action state policies deterministic This consistent experiments Tetris game learning ranking function eﬃcient learning value function 49 We note empirical evidence There situations additional information encoded values advantageous learning allowing faster convergence PG methods 9 Problems Exblocksworldp03 Zenoworldp01 trivial initial state goal state O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 739 Table 3 FPGs instability mean standard deviation success rate N 30 runs problems α 00001 Domain Blocksworld p10 Zenoworld p05 Schedule p09 Mean StdDev 670 419 397 150 449 446 Min 0 26 43 Max 100 70 51 Note 610 policies goal 90 known high variance We come domain planning context The Schedule domain FQL FPG perform equally problem instance FQL diﬃcult tune FPG exploration policy set explicitly method decay exploration correct rate tied step size value λ For FQL tried λ 0 08 1 The variance ﬁnal results signiﬁcant We quote results λ 0 standard version Qlearning It probable tuning result signiﬁcant improvements FQLs results FPG requires tuning step size discount factor However process simpler FQLs amounting tuning discount factor small step size increasing step size convergence unstable domains 412 Instabilities Because randomised algorithm FPG ﬁnd different results time run different random seed They instance fall different local optima reach locally optimal policy learning insuﬃcient time converge diverge use inappropriately large step size In domains IPC Blocksworld Zenotravel Elevators lesser extent Random ExplodingBlocksworld clear divide easy hard problem instances FPG achieves 100 success rate easy ones near 0 hard ones Table 3 presents experimental results problem instances identiﬁed leading FPG unstable It gives problem average success rate standard deviation minimal maximal success rates obtained 10 repeats optimisation evaluation cycle We implement automatic random restarts robust results problems Yet solution avoided time constraints competition On easy problems restarts ﬁnd similar solutions hard problems restarts fail It eﬃcient strategy single complete gradient ascent problem problem case failure 413 Validating FPGs speed tricks Section 343 presented ideas avoiding useless computations To illustrate beneﬁt FPG run times problem The random seed time learning process identical Optimisation limited 15 minutes different combinations speedup tricks Fig 5 shows case computation time function number simulation steps start optimisation For example grounded actions eligible calculated statistics IPC5 Blocksworld p07 120 200 actions eligible single point time episode max episode length 750 1770 2310 actions turn eligible point episode 414 Beneﬁts progress estimator The eﬃciency progress estimator illustrated IPC5 Blocksworld problem p07 Fig 6 shows learning curves FPG different weights ρ progress estimator While reward case success set rs 1000 progress reward number goal facts added subtracted multiplied ρ 0 1 10 100 Fig 6a shows average number goals reached simulation step Fig 6b shows average reward simulation step Rθ All curves averages 10 runs error bars showing standard deviation This problem hard progress estimator necessary order FPG initially ﬁnd goal bootstrap good policy The learning time limited 15 minutes Because uniformly scaling rewards equivalent scaling stepsize bigger r p values lead faster learning But resulting policy 900 seconds better r p 10 r p 100 favours shortterm rewards linked progresses longterm nonguaranteed rewards linked goal states Plus error bars higher standard deviation r p 100 FPG learning fast We present ﬁgures different xaxes purpose number simulation steps performed 900 sec onds depends setting The fastest setting r p 0 receiving reward good way avoid excessive computations 740 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Fig 5 Speedup gained avoiding useless parameter updates r r indicate speedup based observing reward e e indicate speedup relying action eligible 415 When FFreplan fails A second set domains instance demonstrates domains introduced 30 challenging replanners They characterised short plans having high failure probability Deterministic planners like FF look shortest path goal ability avoid dangerous paths Replanners fail optimise cost togo measured IPC5 Results shown Table 4 The optimal Paragraph planner problem size large Triangletire4 particular shows threshold domain approximate probabilistic planning approach required order ﬁnd good policy To summarise probabilistic planning results FPG appears good compromise scalability planning approaches capacity optimal probabilistic planner perform reasoning uncertainty Note shall FPGs limitations describing performance CPTP domains 42 CPTP These experiments compare FPG earlier probabilistic temporal planners Prottle 4 Military Operations MO planner 23 The MO planner uses LRTDP Prottle uses hybrid AO LRTDP They require storage state values attempt prune large branches state space The Prottle planner advantage good heuristics prune state space The MO planner use heuristics We present results criteria probability reaching goal state average makespan including ex ecutions end failure longterm average reward FPG However planner uses subtly different optimisation criteria FPG maximises average reward step R 1000 1Prfail steps plan execution related makespan steps average number decision points Prottle minimises probability failure MO minimises costpertrial based weighted combination Pfailure makespan resource sumption It clear FPGs minimisation steps helps minimise makespan Steps occur decision points The way minimise decision points start actions possible single command maximising concurrency It easy adapt reward function emphasise criteria resource consumption The ﬁrst domains Probabilistic Machine Shop MS 3 Multiple subactions shape paint polish need performed different objects different machines possibly parallel Not machines capable action work object concurrently Objects need transported machine different sub actions The version based Mach6 largest variant 3 subsequently modiﬁed exactly O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 741 Number successes simulation step function wall clock time Fig 6 FPGs learning curve Blocksworld problem p07 settings progress estimator success reward rs 1000 b Average reward simulation step function simulation time Table 4 Summary results nonreplannerfriendly domains Values plan simulations reach goal minimum 30 runs A dash indicates planner run failed produce results typically memory constraints A starred result indicates theoretical upper limit FFreplan failed reach experiments Domain Climber Bus fare Tritire 1 Tritire 2 Tritire 3 Tritire 4 Paragraph 100 100 100 100 100 3 FPG 100 22 100 92 91 68 FFreplan 62 1 50 13 3 08 Prottle 100 10 original Prottle experiments10 It 9 durative actions 13 predicates expand 38 grounded actions 28 grounded predicates The maximum makespan 20 Prottle 15 Maze MZ 4 Maze based idea moving connected rooms ﬁnding keys necessary unlock closed doors There doors keys different colours possible try unlocking doors time 10 Some actions nested probabilistic events outcomedependant durations 742 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Table 5 Results 3 benchmark domains The experiments MO FPG repeated 100 times Success percentage successful executions MS makespan R ﬁnal longterm average reward Time optimisation time seconds FPGL FPG linear network FPGT FPG tree experts Problem Algorithm Success MS MS MS MS MS MS MS MS MS MZ MZ MZ MZ MZ MZ MZ MZ MZ TP TP TP TP TP TP TP TP PitStop PitStop PitStop 500 500 500 FPGL FPGL FPGT FPGT Prottle MO random naïve FPGL FPGL FPGT FPGT Prottle MO MO random naïve FPGL FPGL FPGT FPGT Prottle MO random naïve FPGL random naïve FPGT random naïve 986 999 300 350 971 07 00 191 853 803 847 822 921 928 235 92 344 667 656 667 202 04 00 1000 710 00 975 234 305 Out memory Out memory 66 55 13 13 18 20 55 69 55 57 80 82 13 16 18 18 18 18 15 19 20180 12649 66776 158 765 736 R 118 166 209 214 01 00 134 130 136 115 164 86 298 305 302 301 10 00 142 410 00 156 0231 0100 Time 532 600 439 600 272 371 440 29 17 10 71 72 340 600 258 181 442 41 3345 grab keys Actions duration 1 2 nested probabilistic outcomes There 6 durative actions 7 predicates expand 165 grounded actions 207 grounded predicates Plans fail makespan reaches 20 units Prottle 10 Despite large number actions problem easy solve compared Machine Shop Teleport TP 4 Objects teleport locations objects successfully linked destina tion There fast slow forms teleporting slow higher probability success There ﬁxed number links ﬁxed source location One change destination multiple links time try teleport oneself stable link The problem actions outcomedependent durations It 3 durative actions 3 predicates expand 63 grounded actions 24 grounded predicates Plans fail makespan reaches 25 units Prottle 20 We additionally introduce novel domains illustrate particular strengths FPG PitStop A proofofconcept continuous duration uncertainty domain representing alternative pit stop strategies car race 2stop strategy versus 3stop For strategy pitstop racing action deﬁned The 3stop strategy shorter racing pitting time pit stop injects 20 laps worth fuel The 2stop strategy longer pit times injects 30 laps worth fuel The goal complete 80 laps The pitstop actions modelled Gaussian durations The racing actions ﬁxed minimum time discrete outcomes probability 05 clear track adds exponentially distributed delay encountering backmarkers adds normally distributed delay Thus domain includes continuous durations discrete outcomes metric functions fuel counter lap counters 500 To provide demonstration scalability parallelisation generated 500 grounded actions 250 predicates domain follows goal state required 18 predicates true Each action outcomes 6 effects 10 chance effect negative Two independent sequences actions generated potentially lead goal state makespan 1000 possible routes goal There 40 types resource 200 units Each action requires maximum 10 units 5 types potentially consuming half occupied resources permanently Resources limit actions start Our experiments combination 1 FPG linear network FPGL action policies 2 FPG tree FPGT action policy shown Fig 4 3 MO planner 4 Prottle 5 random policy starts eligible actions coin toss 6 naïve policy attempts start eligible actions command O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 743 Fig 7 Three decisiontree action subpolicies extracted ﬁnal Maze policy Returning Yes means start action Fig 8 Relative convergence longterm average reward R failure probability makespan single linear network FPG optimisation Machine Shop The yaxis common scale units All experiments limited 10 minutes Other parameters described Table 7 In particular single gradient step size α selected single highest value ensured reliable convergence 100 runs domains Experiments section conducted dedicated 24 GHz Pentium IV processor 1 GB ram The results summarised Table 5 Reported success percentage makespan estimated 10000 simulated executions optimised plan Prottle results taken directly 4 quoting highest probability success result FPG MO optimisationsevaluations repeated 100 times investigate impact local minima The FPG MO results mean result 100 runs unbarred results single best run 100 measured probability success The small differences mean best results indicate local minima severe The random naïve experiments designed demonstrate optimisation necessary good results In general Table 5 shows FPG competitive Prottle MO planner FPG achieves best performance Machine Shop The poor performance Prottle Teleport domain202 success compared FPGs 656is Prottles short maximum permitted makespan 20 time units At 25 units required achieve higher success probability We observe FPGs linear action policy generally performs slightly better tree takes longer optimise This expected given linear actionpolicy represent richer class policies expense parameters In fact surprising decision tree domains Machine Shop reduces success rate 30 compared 99 linear policy achieves We explored types policy decision tree structure Fig 4 produces The pruned decision tree policy grounded Maze actions shown Fig 7 The ﬁrst action pruned runs The action runs eligible The second action interesting FPG decided useful action predicate set faster eligible action Over 165 Maze actions majority optimised start start optimised The case indicates actions active plan long The good performance simplistic treepolicy indicates test domainsand possibly othersa large planning effort deciding grounded actions useful ﬁnal plan Machine shop exhibited signiﬁcant difference linear decision tree action policy indicating complex domain FPG simple decision tree like generate control knowledge reasoning based planners For example preprocessing stage FPG determine actions immediately discarded 744 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 Table 6 FPGs results optimisation terminated 75 mean R achieved Table 5 Problem Algorithm Success MS MS MS MS MZ MZ MZ MZ TP TP TP TP FPGL FPGL FPGT FPGT FPGL FPGL FPGT FPGT FPGL FPGL FPGT FPGT 953 999 296 341 807 853 803 842 654 670 653 669 MS 71 65 13 14 55 56 55 70 18 19 18 18 R 89 89 16 16 100 100 102 102 224 224 226 226 Time 37 32 10 14 13 22 2 2 35 4 23 1 Fig 9 Convergence times 500 action experiment run parallel mode varying number processors The numbers curve total aggregated simulation steps taken Table 5 shows Prottle achieves good results faster Maze Machine Shop The apparently faster Prottle op timisation asymptotic convergence FPG criterion optimise longterm average reward fails increase 5 Rθ estimations 10000 steps In reality good policies achieved long convergence criterion To demonstrate plotted progression single optimisation run FPGL Machine Shop domain Fig 8 The failure probability makespan settle near ﬁnal values reward approximately R 85 mean longterm average reward obtainable domain R 118 In words tail end FPG optimisation removing unnecessary noops To demonstrate anytime nature FPG optimisation stopped 75 chosen arbitrarily average reward obtained stopping criterion Table 5 The new results Table 6 reduction optimisation times orders magnitude little drop performance ﬁnal policies The experimental results continuous time PitStop domain FPGs ability optimise mixtures discrete continuous uncertainties Results 500 action domain shown running parallel version FPG algorithm 16 processors No CPTP planner know capable running domains scale As expected observed optimisation times dropped inversely proportional number processors 16 processors This shown Fig 9 However single processor parallel version requires double time OlPomdp This eﬃcient use stepbystep gradients somewhat communication overheads parallelisation As number processors increases observe overheads grow ﬁxed problem size point adding processors decrease performance O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 745 Table 7 Parameter settings discussed text Param θ α α β cid3 cid3 T T Val 5 5 0 1 10 5 10 095 1 00 06 6 106 1 106 Table 8 Success probability XOR problem Domain XOR FPGlinear cid1374 5 10cid14 43 When FPG fails Opt All FPG FPGL FPGT FPG MO Prottle Parallel FPGT Parallel FPGT FPGMLP cid1375 5 072cid14 cid13100 028cid14 Notes Initial θ Both LT LRTDP Param Prottle Param For search dir For line search FFreplan cid13100 10cid14 As reinforcement learning algorithm relying function approximation FPG fail function approxi mator capable representing good policy This easy demonstrate FPGipc XOR problem deﬁned x y randomly initialised boolean predicates action leads plan success x cid12 y plan fails action b leads plan success x cid12 y plan fails Using linear function approximators best policies opt action 1 4 possible states action remaining states This results 75 probability success But function approximation richer adding hidden layera standard multilayer perceptron MLPwith squashed hidden units represent policy achieves 100 success Experiments conducted FPGlinear FPGMLP FFreplan We observed problem FPG verges local optima When hidden layers second layer parameters initialised randomly This FPGlinear FPGMLP run 100 times problem resulting scores automatically clustered identify local optima number clusters initialised 2 Table 8 lists identiﬁed clusters algorithm A cluster described tuple cid13mean standard deviation weightcid14 As expected FFreplan ﬁnds goal FPGlinear happens stable results conﬁrming theoretical value 75 success This 75 value policy unsurprisingly local optimum FPGMLP reaches 100 success probability 28 optimisations This toy problem illustrates fact linear network able represent optimal solution exhibits situation gradient ascent tends fall local optima Greater tuning number hidden units step size probably overcome problem tuning likely highly domain speciﬁc undesirable automated planning Another solution enriching observation space classical strategy complex preconditions conditionaleffects appear planning problem duplicate action copy simple preconditions 50 On hand conducted experiments MLPs IPC problems achieved better perfor mance We suspect appropriate encoding observation linear approximator produce good policies domains There domains FPG easily represent optimal policy diﬃcult learn As studied Section 414 progress estimator crucial achieving good results domains Blocksworld How larger blocks world domains proved diﬃcult FPG This hints domains generally challenging FPG They characterised requiring long chains correct actions obtain rewards A random policy essentially walks state space long time achieving reward In cases failure mode FPG optimise forever achieving gain longterm reward Essentially stays perpetually ﬂat region gradient space However domains challenging planners great motivation reasoning structure exists domains Blocksworld 746 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 5 Discussion future work FPG diverges traditional planning approaches key ways search plans directly local optimisation procedures approximate policy representation factored collection parameterised policies Apart reducing representation complexity approach allows policies generalise states encountered training This important feature FPGs learning approach We obtain similar effect restricting input policies state That concentrate predicate values If action useful given observation o generally useful similar observations similar states FPG scales sense memory use step computation times grow linearly domain However total number gradient steps needed convergence function mixing time underlying POMDP grow exponentially number state variables How compute mixing time arbitrary MDP open problem turn hints diﬃculty assessing advance hardness arbitrary planning problem In recent years policygradient algorithms developed better use samples employing approximate second order gradient ascent andor use critic value estimates 5152 They particular obtaining samples high cost However algorithms time complexity cost O k2 gradient step k number parameters plus observation dimensions In setting samples obtained fast simulator Using available time generate samples better performing O10002 matrix operations step taken simulator It strange use planning model information build plan execution simulator We avoid intractable computations large models However believe hybrid dynamic programmingRL algorithms highly desirable achieve best approaches This achieved RaoBlackwellisation gradient estimates reducing variance known perstep transition probabilities We believe fast eﬃcient methods nonprobabilistic planning bootstrap probabilistic planners This ways including replanning One method experimented uses FFreplan suggest actions help FPG reach goal domains random actions tend reach goal 53 Over time FPG takes FFreplan choice actions optimising probabilistic structure 6 Conclusion To conclude FPG demonstration MonteCarlo local optimisation methods supplement AI planning methods This particularly true domains involve large inﬁnite state spaces uncertainty continuous quantities time Ultimately believe hybrid planninglearning approaches stateofthe art complex domains Acknowledgements This work supported National ICT Australia funded Australian Governments Backing Australias Ability program Centre Excellence program This project funded Australian Defence Science Tech nology Organisation Thank Sylvie Thiébaux Iain Little helpful insights We wish thank organisers IPC5 probabilistic track opportunity test FPG subsequent feedback References 1 A Barto S Bradtke S Singh Learning act realtime dynamic programming Artiﬁcial Intelligence 72 1995 81138 2 E Hansen S Zilberstein LAO 3 Mausam DS Weld Concurrent probabilistic temporal planning Proceedings Fifteenth International Conference Automated Planning A heuristic search algorithm ﬁnds solutions loops Artiﬁcial Intelligence 129 2001 3562 Scheduling ICAPS05 Monterey CA 2005 4 I Little D Aberdeen S Thiébaux Prottle A probabilistic temporal planner Proceedings Twentieth American National Conference Artiﬁcial Intelligence AAAI05 2005 5 DP Bertsekas JN Tsitsiklis NeuroDynamic Programming Athena Scientiﬁc 1996 6 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press Cambridge MA ISBN 0262193981 1998 7 A Fern S Yoon R Givan Approximate policy iteration policy language bias Solving relational Markov decision processes Journal Artiﬁcial Intelligence Research 25 2006 85118 8 RJ Williams Simple statistical gradientfollowing algorithms connectionist reinforcement learning Machine Learning 8 1992 229256 9 RS Sutton D McAllester S Singh Y Mansour Policy gradient methods reinforcement learning function approximation Advances Neural Information Processing Systems NIPS99 vol 12 MIT Press 2000 pp 10571063 10 J Baxter P Bartlett L Weaver Experiments inﬁnitehorizon policygradient estimation Journal Artiﬁcial Intelligence Research 15 2001 351381 11 H Kimura K Miyazaki S Kobayashi Reinforcement learning POMDPs function approximation Proceedings Fourteenth International Conference Machine Learning ICML97 Morgan Kaufmann 1997 pp 152160 12 M Fox D Long PDDL21 An extension PDDL expressing temporal planning domains Journal Artiﬁcial Intelligence Research 20 2003 61124 13 L Peshkin KE Kim N Meuleau LP Kaelbling Learning cooperate policy search Proceedings Sixteenth Conference Uncertainty Artiﬁcial Intelligence UAI00 2000 14 N Tao J Baxter L Weaver A multiagent policygradient approach network routing Proceedings Eighteenth International Conference Machine Learning ICML01 Morgan Kaufmann 2001 15 M AiChang J Bresina L Charest A Chase JC jung Hsu A Jonsson B Kanefsky P Morris K Rajan J Yglesias B Chaﬁn W Dias PF Maldague MAPGEN Mixedinitiative planning scheduling Mars exploration rover mission IEEE Intelligent Systems 19 1 2004 812 O Buffet D Aberdeen Artiﬁcial Intelligence 173 2009 722747 747 16 C Gretton Gradientbased relational reinforcementlearning temporally extended policies Proceedings Seventeenth International Confer ence Automated Planning Scheduling ICAPS07 2007 17 HLS Younes ML Littman PPDDL10 An extension PDDL expressing planning domains probabilistic effects Tech Rep CMUCS04167 Carnegie Mellon University October 2004 18 HLS Younes Extending PDDL model stochastic decision processes Proceedings ICAPS03 Workshop PDDL 2003 19 W Cushing S Kambhampati Mausam D Weld When temporal planning temporal Proceedings International Joint Conference Artiﬁcial Intelligence IJCAI07 Hyderabad India 2007 20 Mausam P Bertoli D Weld Planning durative actions stochastic domains Journal Artiﬁcial Intelligence Research 21 B Bonet B Givan Results probabilistic track 5th international planning competition Not Proceedings Fifth International Planning Competition IPC5 2006 22 Mausam DS Weld Probabilistic temporal planning uncertain durations Proceedings Sixteenth International Conference Automated Planning Scheduling ICAPS06 2006 23 D Aberdeen S Thiébaux L Zhang Decisiontheoretic military operations planning Proceedings Fourteenth International Conference Automated Planning Scheduling ICAPS04 2004 pp 402411 24 HLS Younes RG Simmons Policy generation continuoustime stochastic domains concurrency Proceedings Fourteenth Interna tional Conference Automated Planning Scheduling ICAPS04 2004 25 S Sanner C Boutilier Practical linear valueapproximation techniques ﬁrstorder MDPs Proceedings TwentySecond Conference Uncertainty Artiﬁcial Intelligence UAI06 2006 26 I Little S Thiébaux Concurrent probabilistic planning graphplan framework Proceedings Sixteenth International Conference Automated Planning Scheduling ICAPS06 2006 27 P Fabiani F TeichteilKönigsbuch Symbolic focused dynamic programming planning uncertainty Proceedings IJCAI05 Workshop Reasoning Uncertainty Robotics RUR05 2005 28 S Yoon A Fern B Givan FFReplan baseline probabilistic planning Proceedings Seventeenth International Conference Automated Planning Scheduling ICAPS07 2007 29 J Hoffmann B Nebel The FF planning Fast plan generation heuristic search Journal Artiﬁcial Intelligence Research 14 2001 253302 30 I Little S Thiébaux Probabilistic planning vs replanning Proceedings ICAPS07 Workshop International Planning Competition Past Present Future 2007 31 D Aberdeen Policygradient methods planning Advances Neural Information Processing Systems NIPS05 vol 18 MIT Press 2006 32 D Aberdeen O Buffet Concurrent probabilistic temporal planning policygradients Proceedings Seventeenth International Conference Automated Planning Scheduling ICAPS07 Providence USA 2007 33 Y Xu A Fern S Yoon Discriminative learning beamsearch heuristics planning Proceedings Twentieth International Joint Conference Artiﬁcial Intelligence IJCAI07 2007 34 S Dzeroski LD Raedt K Driessens Relational reinforcement learning Machine Learning 43 2001 752 35 A Ng D Harada S Russell Policy invariance reward transformations Theory application reward shaping Proceedings Sixteenth International Conference Machine Learning ICML99 1999 36 TG Nicol N Schraudolph Conjugate directions stochastic gradient descent Proceedings International Conference Artiﬁcial Neural Networks ICANN02 Lecture Notes Computer Science vol 2415 SpringerVerlag 2002 pp 13511356 37 J Baxter PL Bartlett Inﬁnitehorizon policygradient estimation Journal Artiﬁcial Intelligence Research 15 2001 319350 38 A Benveniste M Metivier P Priouret Adaptive Algorithms Stochastic Approximation SpringerVerlag 1990 39 E Greensmith P Bartlett J Baxter Variance reduction techniques gradient estimates reinforcement learning Journal Machine Learning Research 5 2004 14711530 40 D Aberdeen J Baxter Scaling internalstate policygradient methods POMDPs Proceedings Nineteenth International Conference Machine Learning ICML02 Morgan Kaufmann Sydney Australia 2002 41 J Hoey R StAubin A Hu C Boutilier SPUDD Stochastic planning decision diagrams Proceedings Fifteenth Conference Uncertainty Artiﬁcial Intelligence UAI99 1999 pp 279288 42 J Nicholls Algebraic decision diagrams reinforcement learning Tech rep Australian National University honours thesis September 2005 43 C Boutilier Sequential optimality coordination multiagent systems Proceedings 16th International Joint Conference Artiﬁcial Intelligence IJCAI99 1999 44 D Bernstein S Zilberstein N Immerman The complexity decentralized control Markov decision processes Proceedings Sixteenth Conference Uncertainty Artiﬁcial Intelligence UAI00 2000 45 C Guestrin MG Lagoudakis R Parr Coordinated reinforcement learning Proceedings Nineteenth International Conference Machine Learning ICML02 Morgan Kaufmann Publishers Inc 2002 pp 227234 46 GJ Gordon Reinforcement learning function approximation converges region Advances Neural Information Processing Systems NIPS00 vol 13 2001 pp 10401046 47 A Blum M Furst Fast planning planning graph analysis Artiﬁcial Intelligence 90 1997 281300 48 O Buffet D Aberdeen The factored policy gradient planner Proceedings Fifth International Planning Competition IPC5 2006 httpwwwldcusbvebonetipc5 results proceedings 49 B Scherrer A Boumaza C Thiery Personal communication 2008 50 C Anderson D Smith D Weld Conditional effects graphplan Proceedings International Conference Artiﬁcial Intelligence Planning Scheduling AIPS98 1998 51 J Peters S Vijayakumar S Schaal Natural actorcritic Proceedings Sixteenth European Conference Machine Learning ECML05 Lecture Notes Computer Science vol 3720 SpringerVerlag 2005 52 S Kakade A natural policy gradient Advances Neural Information Processing Systems NIPS03 vol 14 2003 53 O Buffet D Aberdeen FFFPG Guiding policygradient planner Proceedings Seventeenth International Conference Automated Planning Scheduling ICAPS07 Providence USA 2007