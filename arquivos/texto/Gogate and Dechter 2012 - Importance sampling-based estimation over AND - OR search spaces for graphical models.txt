Artiﬁcial Intelligence 184185 2012 3877 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Importance samplingbased estimation ANDOR search spaces graphical models Vibhav Gogate Rina Dechter ab Department Computer Science The University Texas Dallas Richardson TX 75080 USA b Donald Bren School Information Computer Sciences University California Irvine CA 92697 USA r t c l e n f o b s t r c t It known accuracy importance sampling improved reducing variance sample mean variance reduction schemes subject research In paper introduce family variance reduction schemes generalize sample mean conventional OR search space ANDOR search space graphical models The new ANDOR sample means allow trading time space variance At end ANDOR sample tree mean time space complexity conventional OR sample tree mean smaller variance At end ANDOR sample graph mean requires time space compute smallest variance Theoretically variance smaller ANDOR space ANDOR sample mean deﬁned larger virtual sample size compared OR sample mean Empirically demonstrate ANDOR sample mean far closer true mean OR sample mean 2012 Elsevier BV All rights reserved Article history Received 23 March 2010 Received revised form 24 February 2012 Accepted 1 March 2012 Available online 3 March 2012 Keywords Probabilistic inference Approximate inference Graphical models Importance sampling Bayesian networks Constraint networks Markov networks Model counting Variance reduction 1 Introduction Importance sampling 12 general scheme approximate weighted counting tasks deﬁned graphical models computing probability evidence Bayesian network computing partition func tion Markov network counting number solutions constraint network The main idea transform counting summation task expectation special distribution called proposal distribution Then algorithm generates samples proposal distribution approximates expectation weighted average samples The weighted average called sample mean It known accuracy estimate inversely proportional variance sample mean signiﬁcant research focused reducing variance 23 To effect paper propose family variance reduction schemes context graphical models called ANDOR importance sampling The central idea ANDOR importance sampling exploit problem decomposition introduced conditional dependencies graphical model Recently graphbased problem decompositions introduced systematic search graphical models 45 captured notion ANDOR search spaces 6 The usual way performing search systematically possible instantiations variables organized OR search tree In ANDOR search additional AND nodes interleaved OR nodes capture decomposition conditionally independent sub problems Corresponding author Email addresses vgogatehltutdallasedu V Gogate dechtericsuciedu R Dechter 00043702 matter 2012 Elsevier BV All rights reserved doi101016jartint201203001 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 39 We propose organize generated samples partial cover ANDOR search tree yielding ANDOR sample tree Likewise OR sample tree portion OR search tree covered samples The main intuition moving OR space ANDOR space AND nodes combine samples independent components yield virtual increase sample size For example X conditionally independent Y given Z consider N samples X independently Y given value Z yielding effective virtual sample size N 2 instead input N Since variance reduces number samples increases cf 23 sample mean computed ANDOR sample tree smaller variance computed OR sample tree We idea step look ANDOR search graph 6 target compiling given set samples Since ANDOR search graph captures conditional independencies ANDOR search tree partial cover corresponding generated samples yields larger virtual sample size As result sample mean computed ANDOR sample graph smaller variance computed ANDOR sample tree timewise factor However computing ANDOR sample graph mean expensive factor O w O N space wise w treewidth N number samples Thus ANDOR sample tree graph means allow trading time space accuracy We provide thorough empirical evaluation comparing impact exploiting varying levels problem decom positions ANDOR tree ANDOR graph variety probabilistic deterministic benchmark networks Our experiments demonstrate ANDOR sample tree mean slightly better conventional OR sample tree mean terms accuracy ANDOR sample graph mean clearly superior ANDOR sample tree mean The rest paper organized follows In section present preliminaries background In Section 3 deﬁne ANDOR sample tree mean Section 4 prove smaller variance OR sample tree mean The ANDOR sample graph mean deﬁned Section 5 Section 6 presents empirical results conclude Section 7 The research presented paper based Gogate Dechter 78 2 Preliminaries background We denote variables upper case letters X Y values variables lower case letters x y Sets variables denoted bold upper case letters X X1 Xn We denote D Xi set possible values Xi D Xi called domain Xi Xi xi simply xi variable clear context denotes assignment xi D Xi Xi X x simply x denotes sequence assignments variables X x X1 x1 X2 x2 Xn xn DX denotes Cartesian product domains variables X DX D X1 D Xn We denote projection assignment x set S X xS Given assignment y z partition Y Z X x y z denotes composition assignments subsets cid2 cid2 xDX xDX ExQ X random variable X respect distribution Q deﬁned ExQ X VarQ X X deﬁned VarQ X VarQ X Var X identity Q clear context xX The expected value x X xQ x The variance x X x ExQ X2 Q x To simplify write ExQ X Ex X xDX denotes sum possible conﬁgurations variables X cid2 xnD Xn For brevity abuse notation write xi D Xi x2D X2 x1D X1 cid2 cid2 xi Xi cid2 cid2 cid2 cid2 cid2 cid2 We denote discrete functions upper case letters F H C I scope set arguments function F scopeF Given assignment y superset Y scopeF abuse notation write F yscopeF F y Deﬁnition 1 Graphical models Markov networks A discrete graphical model Markov network denoted G 3 tuple cid5X D Fcid6 X X1 Xn ﬁnite set variables D D X1 D Xn ﬁnite set domains D Xi domain variable Xi F F 1 Fm ﬁnite set discretevalued nonnegative functions called potentials The graphical model represents joint distribution P G X deﬁned P Gx 1 Z mcid3 i1 F ix Z normalization constant called partition function It given Z cid4 mcid3 xX i1 F ix We refer Z weighted counts 1 2 The primary queries Markov networks computing partition function weighted counts computing The marginal probability P G Xi xi ratio weighted counts Formally let I xi Diracdelta function marginal probability P G Xi xi scope Xi deﬁned follows 40 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 cid5 I xi x 1 x xi 0 Then deﬁnition P G xi given cid2 P Gxi I xi xP Gx cid4 xX cid6 m j1 F jx xX I xi x Z 3 Notice numerator Eq 3 weighted counts graphical model obtained augmenting G I xi Thus algorithms computing weighted counts computing P G xi Each graphical model associated primal graph captures dependencies present model Deﬁnition 2 Primal graph The primal graph graphical model G cid5X D Fcid6 undirected graph GX E variables G vertices edge variables appear scope function 21 Bayesian constraint networks Deﬁnition 3 Bayesian belief networks A Bayesian network graphical model B cid5X D G Pcid6 G X E directed acyclic graph set variables X Each function P P conditional probability table deﬁned P Xipai pai scopeP Xi set parents Xi G The primal graph Bayesian network called moral graph When entries CPT 0 1 called deterministic functional CPT An evidence E e instantiated subset variables A Bayesian network represents following joint probability distribution P Bx ncid3 i1 P ixXi xpai By deﬁnition given Bayesian network B probability evidence P Be given P Be cid4 ncid3 cid7 P y eXi cid8 cid8y epai cid9 yXE i1 4 5 It easy Eqs 2 5 P Be equivalent weighted counts Z evidence instantiated Bayesian network Another important query Bayesian network computing conditional marginal probability P Bxie query variable Xi X E Deﬁnition 4 Constraint networks A constraint network graphical model R cid5X D Ccid6 C C1 Cm set constraints Each constraint Ci 01 function deﬁned scope Given assignment x constraint said satisﬁed Cix 1 A constraint expressed pair cid5R Sicid6 R relation deﬁned scope Ci contains tuples Cisi 1 The primal graph constraint network called constraint graph A solution constraint network assignment values variables satisﬁes constraints The primary query constraint network determine solution ﬁnd Another important query counting number solutions K constraint network deﬁned K cid4 mcid3 xX i1 Cix 6 K clearly identical weighted counts constraint network The Boolean satisﬁability problem deﬁnes special type constraint network variables Xi X binary domain 0 1 False True constraints speciﬁed clauses A clause disjunction literals literal variable negation For example X1 X2 X3 clause deﬁned literals X1 X2 X3 denotes negation Given assignment clause satisﬁed literals set True For example clause X1 X2 X3 satisﬁed given assignment X1 0 X2 0 X3 0 satisﬁed given assignment X1 0 X2 0 X3 1 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 41 22 ANDOR search spaces graphical models Fig 1 ANDOR search spaces graphical models Given graphical model G cid5X D Fcid6 compute weighted counts accumulating probabilities weights traversing search space instantiated variables In simplest case algorithm traverses OR search tree nodes represent states space partial assignments This traditional search space capture structural properties underlying graphical model Introducing AND nodes search space capture conditional independencies graphical model The ANDOR search space wellknown problem solving approach developed area heuristic search 9 exploits problem structure decompose search space The ANDOR search space graphical models intro duced Dechter Mateescu 6 It guided pseudo tree spans original graphical model Deﬁnition 5 Pseudo tree Given undirected graph G V E called pseudo tree edge E cid8 E backarc connects node ancestor T cid8 directed rooted tree T V E deﬁned nodes Deﬁnition 6 ANDOR search tree Given graphical model G cid5X D Fcid6 primal graph G pseudo tree T G associated ANDOR search tree denoted ψT alternating levels AND OR nodes The OR nodes labeled Xi correspond variables The AND nodes labeled xi correspond value assignments The structure ψT based T Its root OR node labeled root T The children OR node Xi AND nodes labeled assignments xi The children AND node xi OR nodes labeled children Xi T Semantically OR nodes represent alternative assignments AND nodes represent problem decomposition independent subproblems need solved When pseudo tree chain ANDOR search tree coincides regular OR search tree Deﬁnition 7 Solution subtree A solution subtree ANDOR search tree graph contains root node For OR node contains child nodes AND nodes contains child nodes Example 1 Fig 1a shows constraint network 3coloring problem 4 variables A possible pseudo tree constraint network given Fig 1b Fig 1c shows OR search tree Fig 1d shows ANDOR search tree guided pseudo tree given Fig 1b 42 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Fig 2 Assigning weights ORtoAND arcs ANDOR search tree To compute weighted counts ANDOR search tree need annotate ORtoAND arcs weights derived functions F product weights arcs solution subtree assignment x equal m i1 F ix We formalize notion weighted ANDOR tree 6 cid6 Deﬁnition 8 Weighted ANDOR tree Given graphical model G cid5X D Fcid6 ANDOR search tree pseudo tree T weight wa b arc OR node AND node b labeled Xi b labeled xi product functions F F fully instantiated assignment root Xi A weighted ANDOR tree ANDOR tree annotated weights Example 2 Fig 2a shows Bayesian network Fig 2b shows pseudo tree Fig 2c shows conditional probability tables Fig 2d shows weighted ANDOR search tree based pseudo tree Bayesian network Note AND children OR nodes having edge label zero drawn extended Functions having zeros range express notion inconsistent assignments The weighted counts computed traversing weighted ANDOR tree DFS manner computing value nodes leaves root 6 deﬁned The value node weighted counts subtree roots Deﬁnition 9 Value node computing weighted counts The value node deﬁned recursively follows The value leaf AND node 1 Let chin denote set child nodes node n let vn denote value If n OR node vn cid7 n n cid7 n cid4 w v cid9 cid9 cid8 cid8 ncid8chin V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 43 wn n cid8 weight arc OR node n child node n cid8 If n AND node vn cid3 v ncid8chin cid9 cid7 n cid8 Proposition 1 The value root node weighted ANDOR tree equal weighted counts Proof See 6 proof cid2 A node n ANDOR search tree represents subproblem graphical model restricted assignment values path root n The ANDOR search tree contain nodes root identical subproblems These nodes uniﬁable merged yielding search graph size smaller ANDOR search tree Traversing ANDOR search graph requires additional memory A depth ﬁrst search algorithm cache previously com puted results retrieve subproblem encountered Some uniﬁable nodes identiﬁed based context express set ancestor variables pseudo tree completely determine conditioned subproblem 6 cid8 context node Xi T denoted Deﬁnition 10 Context Given pseudo tree T X E primal graph GX E contextT Xi set ancestors Xi T ordered descendingly connected G Xi descendants Xi Example 3 Fig 1b shows pseudo tree node annotated context The context nodes C B D A C C B respectively Deﬁnition 11 Context minimal ANDOR graph Given ANDOR search tree OR nodes n1 n2 context uniﬁable variable label Xi assignments contexts identical In words y z denote partial assignment variables path root n1 n2 respectively restriction context Xi satisﬁes ycontextT Xi zcontextT Xi n1 n2 uniﬁable The context minimal ANDOR graph obtained ANDOR search tree merging context uniﬁable OR nodes Example 4 Fig 1e shows context minimal ANDOR graph constructed ANDOR tree Fig 1d merging context uniﬁable nodes 23 Importance sampling approximating weighted counts Importance sampling 110 Monte Carlo simulation technique estimating sum function F domain The main idea express sum expectation easytosample distribution Q called proposal trial importance distribution Then generate samples Q estimate expectation equals sum weighted average samples weight sample x F xQ x The weighted average called sample mean Following prior work cf 11 assume proposal distribution speciﬁed product form dering o X1 Xn variables Namely Q x i1 Q ixix1 xi1 Q Bayesian network CPTs Q 1 Q n Q expressed Bayesian network easy generate samples following logic sampling scheme 12 cid6 n i1 Q xi x1 xi1 cid6 n Algorithm 1 Logic Sampling 1 Input A distribution Q x 2 Output An assignment x sampled Q 3 begin 4 5 6 7 8 Sample xi Q Xi x x x xi x For 1 n Return x 9 end Moreover assume CPT Q Q speciﬁed polynomial space Formally ncid3 Q x Q ixix1 xi1 ncid3 Q ixiyi i1 Yi X1 Xi1 Yi assumed bounded constant1 i1 7 1 Let p maxi Y Then time complexity generating sample Q O np n number variables 44 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Deﬁnition 12 Unbiased asymptotically unbiased estimate Given probability distribution Q quantity θ deﬁned Q estimate θ N function N random samples drawn Q unbiased estimate θ ExQ θ N θ θ N asymptotically unbiased estimate θ limN ExQ θ N θ By deﬁnition unbiased estimate θ asymptotically unbiased However converse true We denote estimates θ drawing hat line θ cid10θ The notion unbiasedness characterizes performance estimator terms mean squared error MSE cid11 MSEQ θ ExQ cid11 ExQ θ θ2 cid12 cid11 θ 2 cid12 ExQ θ 2 cid12 cid11 ExQ θ2 2ExQ θ θ θ 2 cid12 The bias θ deﬁned BiasQ θ ExQ θ θ The variance θ deﬁned VarQ θ ExQ cid12 cid11 θ 2 ExQ θ 2 From deﬁnitions bias variance meansquared error MSEQ θ VarQ θ BiasQ θ 2 8 9 10 In words mean squared error estimator equal bias squared plus variance An unbiased estimator zero bias Therefore reduce MSE estimator constant bias reducing variance Next weighted counts Z estimated importance sampling Consider expression Z Eq 2 Z cid4 mcid3 xX i1 F ix Given proposal distribution Q Z cid4 xX cid6 m i1 F ix Q x Q x ExQ cid6 cid13 cid6 m i1 F ix 0 Q x 0 rewrite Eq 11 follows m i1 F ix Q x cid14 Given independent identically distributed iid samples x1 xN generated Q estimate Z cid10Z N 1 N Ncid4 k1 cid6 m i1 F ixk Q xk 1 N Ncid4 k1 cid9 cid7 xk w wx cid6 m i1 F ix Q x 11 12 13 weight sample x It easy cid10Z N unbiased ExQ cid10Z N Z Thus mean squared error reduced reducing variance given VarQ wx N VarQ cid10Z N 14 Therefore VarQ cid10Z N reduced increasing number samples N reducing variance i1 F ix sample x drawn Q wx Z weights It easy Q x yielding optimal zero variance estimator However making Q x m i1 F ix NPhard order small MSE practice recommended Q close possible distribution tries approximate case proportional cid6 cid6 cid6 m m i1 F ix Next present importance sampling algorithm estimating marginal probability P G xi Recall P G xi deﬁned P Gxi cid2 cid6 m j1 F jx xX I xi x cid6 cid2 m j1 F jx xX 15 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 45 Fig 3 A Bayesian network CPTs Given proposal distribution Q x rewrite Eq 15 follows P Gxi cid2 cid6 xX I xi x cid6 cid2 m xX m j1 F jx Q x Q x Ex j1 F jx Q x Q x cid6 m j1 F j x cid12 cid11 I xi x cid11 cid6 Ex Q x m j1 F j x Q x cid12 ExI xi xwx Exwx 16 Given independent identically distributed iid samples x1 xN generated Q estimate P G xi cid2 cid10P GN xi 1 N cid2 N k1 I xi xkwxk cid2 N k1 wxk 1 N N k1 I xi xkwxk cid2 N k1 wxk 17 cid10P GN xi ratio sample means The numerator equals sample mean samples containing Xi xi denominator sample mean samples estimate Z Thus samples estimating Z straightforward manner estimating P G xi However cid10P GN xi unbiased estimate P G xi Excid10P GN xi cid14 P G xi Yet asymptotically unbi ased limN Excid10P GN xi P G xi bias goes increase sample size Unfortunately variance cid10P GN xi harder analyze ratio 3 Liu 3 suggests measure called effective sample size ESS analyze accuracy asymptotically unbiased estimate It deﬁned ESS N 1 VarQ wx 18 The interpretation ESS N samples proposal distribution worth ESS samples ideal proposal m i1 F ix The higher ESS higher accuracy ESS distribution ideal proposal distribution proportional increased increasing sample size N decreasing variance VarQ wx cid6 In summary accuracy estimates Z P G xi obtained importance sampling improved increasing sample size N reducing variance weights In sections new schemes ANDOR tree ANDOR graph importance sampling improve accuracy virtually increasing sample size N We focus theoretical analysis empirical evaluation estimating weighted counts Z noting analysis results extended straight forward manner estimating marginal probabilities P G xi 3 ANDOR tree importance sampling We start discussing computing expectation parts forms backbone ANDOR importance sampling We present ﬁrst version based ANDOR tree search Note proofs appear body paper deferred Appendix B 31 Estimating expectation parts In Eq 12 expectation function deﬁned set variables computed summing Cartesian product domains variables This method clearly ineﬃcient account condi tional independencies graphical model illustrate Consider tree Bayesian network given Fig 3 Let A B b evidence By deﬁnition probability evidence P b given cid4 xyz X Y Z P b P zP xzP axP yzP b y 19 46 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Let Q x y z Q zQ xzQ yz proposal distribution We express P b terms Q P b cid4 xyz X Y Z P zP xzP axP yzP b y Q zQ xzQ yz Q zQ xzQ yz We apply simple symbolic manipulations rewrite Eq 20 P b cid4 z Z P zQ z Q z cid4 x X P xzP axQ xz Q xz cid4 yY P yzP b yQ yz Q yz By deﬁnition conditional expectation2 cid13 Ex cid13 Ex P xzP ax Q xz cid14 cid8 cid8 cid8 z cid4 x X P xzP axQ xz Q xz P yzP b y Q yz cid14 cid8 cid8 cid8 z cid4 yY P yzP b yQ yz Q yz Substituting Eqs 22 23 Eq 21 P b cid13 Ex cid4 z Z P z Q z P xzP ax Q xz cid14 cid8 cid8 cid8 z cid13 Ex P yzP b y Q yz cid14 cid8 cid8 cid8 z Q z By deﬁnition expectation rewrite Eq 24 20 21 22 23 24 cid14cid14 cid8 cid8 cid8 z cid13 cid14 cid8 cid8 cid8 z cid13 P b Ex cid13 Ex P z Q z P yzP b y Q yz We refer equations form 25 expectation parts If domain size variables d 10 exam ple computing P b Eq 20 require summing d3 103 1000 terms computing P b Eq 25 require summing d d2 d2 10 102 102 210 terms P xzP ax Q xz 25 Ex We estimate P b Eq 25 Assume given N samples z1 x1 y1 zN xN y N generated Q Let 0 1 domain Z let Z 0 Z 1 sampled N0 N1 times respectively We deﬁne sets S j kk 1 N zk j j 0 1 store indices samples value j assigned Z We estimate Ex P xzP ax z replacing expectation sample average These z Ex P yzP b y Q xz Q yz unbiased estimates denoted cid10g X Z j cid10gY Z j j 0 1 given cid10g X Z j 1 N j cid10gY Z j 1 N j cid4 iS j cid4 iS j P xi Z jP axi Q xi Z j P yi Z jP b yi Q yi Z j 26 27 Substituting unbiased estimates Ex P xzP ax estimate P b Q xz z Ex P yzP b y Q yz z Eq 25 following unbiased cid13 cid10P b Ex P z Q z cid14 cid10g X Z jcid10gY Z j 28 Given samples z1 zN generated Q Z estimate cid10P b replacing expectation Eq 28 following sample average cid10P aoisa b 1 N Ncid4 i1 P zi Q zi cid7 cid7 cid9 cid10gY zi zi cid9 cid10g X 29 2 Because expectation taken respect component proposal distribution denominator write ExQ X Ex X clarity V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 47 cid10P aoisa b stands ANDOR estimate P b Since Z 0 Z 1 sampled N0 N1 times respec tively collect samples Z j j 0 1 rewrite Eq 28 cid10P aoisa b 1 N 1cid4 j0 N j P Z j Q Z j cid10g X Z jcid10gY Z j 30 It easy Excid10P aoisa b P b cid10P aois unbiased Conventional importance sampling hand estimate P b follows cid10P isa b 1 N Ncid4 i1 P ziP xiziP yiziP axiP b yi Q ziQ xiziQ yizi As collect samples Z j j 0 1 rewrite Eq 31 cid10P isa b 1 N 1cid4 j0 N j P Z j Q Z j cid15 1 N j cid4 iS j P xi Z jP yi Z jP axiP b yi Q xi Z jQ yi Z j cid16 For simplicity denote cid10g XY Z j 1 N j cid4 iS j P xi Z jP yi Z jP axiP b yi Q xi Z jQ yi Z j rewrite Eq 32 cid10P isa b 1 N 1cid4 j0 N j P Z j Q Z j cid10g XY Z j It easy cid10g XY Z j unbiased estimate Ex P xzP yzP axP b y z Q xzQ yz cid12 cid11 cid10g XY Z j Ex cid13 Ex P xzP yzP axP b y Q xzQ yz cid14 cid8 cid8 cid8 z 31 32 33 34 Let compare cid10P aois given Eq 30 cid10P given Eq 33 The difference cid10P aois compute product cid10g X Z j cid10gY Z j instead cid10g X Y Z j The product cid10g X Z j cid10gY Z j combines estimate separate quantities deﬁned random variables X Z z Y Z z respectively generated samples While conventional importance sampling estimate quantity deﬁned joint random variable X Y Z z generated samples Because samples X Z z Y Z z considered independently Eq 30 N j samples drawn joint random variable X Y Z z Eq 33 correspond N j N j N 2 j virtual samples Eq 30 Since variance goes sample size increases new estimation technique accurate conventional approach 32 Estimating weighted counts ANDOR sample tree We generalize example ANDOR search tree 6 We deﬁne ANDOR sample tree restriction ANDOR search tree generated samples On ANDOR sample tree deﬁne new sample mean yields unbiased estimate weighted counts We start required deﬁnitions Deﬁnition 13 Bucket function Given graphical model G cid5X D Fcid6 rooted pseudo tree T X E bucket function Xi relative T denoted B T Xi product functions G mention Xi mention variables descendants Xi T Example 5 Fig 4 shows possible pseudo tree nonevidence variables Bayesian network given Fig 3 Each variable pseudo tree annotated bucket function3 The bucket function Z P Z P Z function mentions Z mention descendants X Y Z The bucket function X P X P X Z Y P bY P Y Z 3 Note pseudo tree deﬁned nonevidence variables probability evidence equals weighted counts evidence instantiated Bayesian network After instantiating A CPT P A X yields function P X having scope X Similarly instantiating B b CPT P BY yields function P bY having scope Y Thus actual functions computing weighted counts probability evidence P Z P XZ P X P Y Z P bY 48 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Fig 4 A pseudo tree Bayesian network given Fig 3a variable annotated bucket function Fig 5 Four samples drawn uniform proposal distribution Q X Y Z Q XQ Y Q Z Q Z 0 Q Z 1 12 Q X 0 Q X 1 Q X 2 13 Q Y 0 Q Y 1 Q Y 2 13 b The samples arranged ANDOR search tree Dotted edges nodes sampled Each arc OR node AND node labeled weight frequency Deﬁnition 14 Deﬁnition 14 ANDOR sample tree Given graphical model G cid5X D Fcid6 pseudo tree T X E proposal distribution i1 Q ixiyi Yi contextT Xi4 sequence samples S Q deﬁned relative pseudo tree Q x complete ANDOR search tree ψT ANDOR sample tree ψT S obtained ψT removing nodes corresponding edges appear S cid6 n Let πn denote path root ψT S node n let Aπn denote assignment sequence path πn We deﬁne arclabel arc n m OR node n AND node m ψT S Xi labels n xi labels m pair cid5wn m n mcid6 wn m B T Xi n m frequency arc Namely equal number times partial assignment Aπm occurs weight arc n m B T Xi bucket function Xi Deﬁnition 13 xi Aπn Q xi Aπn S An OR sample tree ANDOR sample tree deﬁned relative chain pseudo tree Example 6 Consider Bayesian network given Fig 3 Assume proposal distribution Q X Y Z Q X Q Y Q Z uniform distribution Fig 5b shows ANDOR search tree Bayesian network Fig 5a shows hypothetical random samples drawn Q The ANDOR sample tree obtained removing dotted edges nodes sampled ANDOR search tree Each arc OR node AND node ANDOR sample tree labeled appropriate frequencies weights according Deﬁnition 14 For instance consider 4 For simplicity assume Yi subset context Xi When proposal distribution speciﬁed externally obey constraint In case construct pseudo tree graph G obtained combining primal graphs proposal distribution graphical model V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 49 Fig 6 Value computation ANDOR sample tree Deﬁnition 15 Each OR AND node annotated value By deﬁnition value leaf AND node 1 shown avoid clutter The ANDOR sample tree mean equals value root OR node Z 012096 arc corresponding Z 0 X 1 leftmost sampled arc The assignment Z 0 X 1 appears samples given Fig 5a Therefore frequency arc 1 Given evidence A 0 bucket function associated X P xz P A 0x Fig 4 Since proposal distribution uniform Q xz 13 X values domain weight arc B T X 1 Z 0 Q X 1 Z 0 P X 1 Z 0 P A 0 X 1 Q X 1 Z 0 04 02 13 024 Fig 5b shows derivation weight arcZ 1 We compute approximation node values mimicking value computation ANDOR sample tree Deﬁnition 15 Value node Given ANDOR sample tree graph ψT S value node n denoted vn deﬁned recursively follows The value leaf AND node 1 If n AND node vn cid3 cid9 cid7 n cid8 v ncid8chin n OR node cid2 vn ncid8chin n n cid2 cid8 wn n ncid8chin n ncid8 cid8 vn cid8 We value OR node n equal unbiased estimate conditional expectation subproblem conditioned assignment root n Theorem 1 Deﬁnition 16 ANDOR sample tree mean The ANDOR sample tree mean value root node ANDOR sample tree Example 7 Fig 6 shows values nodes computed Deﬁnition 15 running example For instance value AND node equals product values child OR nodes corresponding Z 0 039 0255 009945 The derivation value root OR node labeled Z shown Fig 6 The value OR nodes X Y given Z j 0 1 cid10g X Z j cid10gY Z j respectively deﬁned Eqs 26 27 The value root node labeled Z ANDOR sample tree mean equal sample mean computed parts Eq 30 Theorem 1 The ANDOR sample tree mean unbiased estimate weighted counts Next OR sample tree mean equal conventional importance sampling sample mean given Eq 13 Recall OR sample tree deﬁned relative chain pseudo tree We convert pseudo tree T chain pseudo tree T topological linearization T Formally forming chain topological DFS ordering T We refer T cid8 cid8 50 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Algorithm 2 ANDOR tree importance sampling Input A graphical mode G cid5X D Fcid6 pseudo tree T X E proposal distribution deﬁned relative T Q X Output ANDOR sample tree mean cid6 n i1 Q XicontextT Xi 1 Generate samples S x1 xN Q 2 Build ANDOR sample tree ψT S relative S T 3 Initialize labeling functions cid5wn m n mcid6 arc OR node n AND node m Deﬁnition 14 Start Value computation phase 4 Initialize value leaf AND nodes 1 5 node n leaves root ψT S Let chin denote child nodes node n 6 denote value node vn n AND node cid6 ncid8chin vn vn cid8 7 8 9 10 cid2 n vn cid8 chin nn cid2 n cid8wnn cid8 chin nncid8 cid8vn cid8 End Value computation phase 11 return vroot node ψT S Deﬁnition 17 Topological linearization pseudo tree A topological linearization pseudo tree T X E chain pseudo cid8 cid8 nodes X Y X X ancestor Y T X ancestor Y T tree T cid8X E Theorem 2 Given graphical model G cid5X D Fcid6 pseudo tree T proposal distribution Q X iid samples drawn Q topological linearization T based T equals conventional importance sampling estimate cid10Z N deﬁned Eq 13 i1 Q XicontextT Xi N T ANDOR sample tree mean computed OR sample tree cid8 cid8 cid6 n Algorithm ANDOR tree importance sampling presented Algorithm 2 In steps 13 algorithm generates samples Q stores ANDOR sample tree The algorithm computes ANDOR sample tree mean ANDOR sample tree recursively leaves root steps 410 value computation phase We summarize complexity Algorithm 2 following theorem Theorem 3 Complexity ANDOR tree importance sampling Given N iid samples drawn proposal distribution Q graphical model n variables pseudo tree having depth h time complexity computing ANDOR sample tree mean O nN space complexity O h Note Algorithm 2 separated sampling estimation value computation phases pedagogical reasons It possible interleave phases generating ANDOR sample tree ﬂy depthﬁrst sampling This variant described Appendix A Algorithm 3 33 Estimating conditional probabilities ANDOR sample tree To compute estimate conditional probability P G xi ANDOR importance sampling compute ANDOR sample tree means value computation phase output ratio The denominator equals ANDOR sample tree mean computed Algorithm 2 To compute numerator set values child AND nodes Xi labeled xi zero compute ANDOR sample tree mean Both numerator denominator computed pass maintaining values node corresponding numerator corresponding denominator Clearly estimate P G xi obtained way asymptotically unbiased computational complexity Algorithm 2 It possible compute marginal probabilities variables network performing value com putation passes upward downward ANDOR sample tree The passes mimic computation updating beliefs ANDOR search tree 613 Each node stores values summarizing information ancestors descendants excluding descendants summarizing information descendants The value node upward pass summarizes information descendants computed The downward value node recursively computed follows Deﬁnition 18 Downward value node The downward value root OR node 1 The downward value internal OR node n having AND parent n cid9 cid3 given cid9 cid8 u cid7 n cid8 cid7 n cid8cid8 v ncid8cid8chincid8 ncid8cid8cid14n V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 51 chin ANDnode n having parent n cid8 set child nodes n cid8 given cid8 vn cid8cid8 upward value n cid8cid8 Deﬁnition 15 The downward value cid2 cid8 n cid8 nwn cid8n mchincid8 ncid8 m Given upward downward values nodes ANDOR sample tree compute estimate marginal probability values variables graphical model following equation cid10P Gxi cid2 nnxi vnun vroot 35 nxi set AND nodes corresponding value xi vroot upward value root node The estimate cid10P G xi asymptotically unbiased ratio unbiased estimates 103 The numerator unbiased estimate weighted counts graphical model augmented indicator function cid5 cid9 cid7 cid8 x I xi cid8 1 xi x 0 The denominator unbiased estimate weighted counts Since size ANDOR sample tree bounded O nN straightforward Theorem 4 Given N iid samples drawn proposal distribution Q graphical model n variables time space complexity computing estimate marginal probabilities ANDOR sample tree O nN Comparing space complexities estimating weighted counts marginal probabilities Theorem 3 Theorem 4 respectively estimating marginal probabilities space intensive factor O nNh Conventional OR importance sampling hand incurs space complexity estimating This yields space versus variance tradeoff In summary section deﬁned ANDOR sample tree mean showed yields unbiased estimate weighted counts Z We proved conventional importance sampling sample mean equals OR sample tree mean We provided algorithm computing ANDOR sample tree mean proved time complexity conventional importance sampling sample mean 4 Variance reduction In section prove ANDOR sample tree mean smaller variance OR sample tree mean In words prove variance ANDOR sample tree mean smaller conventional importance sampling sample mean deﬁned Eq 12 In fact prove general result Speciﬁcally deﬁne iterative process constructing topological linearization ANDOR sample tree mean deﬁned relative partial linearization iteration 1 smaller equal variance deﬁned relative partial linearization iteration We begin formally deﬁning partial linearizations Deﬁnition 19 Topological linearization pseudo tree wrt node Given pseudo tree T X E topological linearization T wrt node X X pseudo tree T X obtained follows If X child node T X equals T Otherwise let T X T let C arbitrary child node X Let O 1 O k set child nodes X including C Replace edge X O T X edge C O It easy Proposition 2 A topological linearization pseudo tree obtained successively applying topological linearization nodes convergence nodes child node Example 8 Fig 7a shows pseudo tree Each pseudo tree shown Figs 7b7e obtained applying topological linearization node pseudo tree left Fig 7e shows chain pseudo tree structure changed applying topological linearization nodes 52 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Fig 7 A pseudo tree b Pseudo tree obtained applying topological linearization node X1 pseudo tree given c The pseudo tree obtained applying topological linearization node X2 pseudo tree given b d The pseudo tree obtained applying topological linearization node X4 pseudo tree given c e The chain pseudo tree obtained applying topological linearization node X3 pseudo tree given d Fig 8 Pseudo tree T b ANDOR sample tree ψT S based pseudo tree given c Pseudo tree T X obtained topological linearization T wrt X d ANDOR sample tree ψT X S based pseudo tree given c We variance ANDOR sample tree mean deﬁned relative S T smaller equal ANDOR sample tree mean deﬁned relative S T X T X topological linearization T respect X Since chain pseudo tree obtained successively applying topological linearization nodes main theorem follows immediately general result Lemma 1 Given pseudo tree T X E graphical model G cid5X D Fcid6 topological linearization T X T wrt node X X sequence samples S x1 xN drawn proposal distribution Q deﬁned relative T variance ANDOR sample tree mean deﬁned relative S T smaller equal variance ANDOR sample tree mean deﬁned relative S T X V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 53 Proof Without loss generality let assume following X root T child nodes U W b x1 xd u1 ud w 1 wd domains X U W respectively c Q X U W Q X Q U X Q W X proposal distribution X U W Let x j appear N j times S let x j u1 w 1 x j u N j w N j samples having X x j Let ψT S ψT X S denote ANDOR sample trees deﬁned relative T S T X S respectively T T X ψT S ψT X S shown Fig 8 For notational convenience let U W denote random variables corresponding value child nodes U W respectively x j ψT S Fig 8 Let U T deﬁned follows T W U T B T U x j ui Q uix j cid9 cid7 ui v T B T U bucket function U v T ui value AND node corresponding x j ui W T B T W x j w Q w ix j cid9 cid7 w v T 36 37 B T W bucket function W v T w value AND node corresponding x j w Note U T W T unbiased estimates value child OR nodes U W respectively x j ANDOR search tree based T By deﬁnition Deﬁnition 15 value AND node labeled x j ψT S given v T x j v T U v T W 1 N j 1 N j N jcid4 i1 N jcid4 i1 B T U x j ui Q uix j cid9 cid7 ui v T 1 N j N jcid4 i1 B T W x j w Q w ix j cid9 cid7 w v T U T 1 N j N jcid4 i1 W T Eqs 36 37 By deﬁnition value AND node labeled x j ψT X S v T X x j v T X U N jcid4 1 N j 1 N j i1 N jcid4 i1 B T U x j ui Q x j ui cid9 cid7 ui v T X B T U x j ui Q x j ui cid9 cid7 ui v T v T X cid9 cid7 W 38 39 40 41 42 43 The step follows fact value AND node corresponding x j ui ψT X S product value AND node corresponding x j ui ψT S value OR node labeled W Fig 8 Namely v T X ui v T ui v T X W For notational convenience deﬁne W T X v T X cid7 cid9 W Note W T X search tree based T X From Eqs 36 43 44 unbiased estimate value W ANDOR search tree based T ANDOR v T X x j 1 N j N jcid4 i1 U T W T X 45 Since ANDOR sample tree mean equals value root node labeled X prove lemma prove Varv T X cid2 Varv T X X From law total variance5 cid12cid12 cid12cid12 cid11 Var cid11 Var cid12 v T X cid11 cid11 Var Ex cid11 cid12 Var v T X X v T x j cid11 Ex v T X x j cid11 Ex cid12cid12 cid11 Var v T x j cid11 cid11 Ex Var v T X x j cid12cid12 44 46 47 5 The law states variance random variable A expressed terms conditional variance expectation wrt random variable B follows Var A VarEx AB ExVar AB 54 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 From Eqs 46 47 order prove Varv T X cid2 Varv T X X suﬃces prove cid11 Ex cid12 v T x j cid11 Ex cid12 v T X x j cid11 Var cid12 v T x j cid11 cid2 Var cid12 v T X x j We prove parts turn By deﬁnition cid11 Ex cid12 v T x j cid17 Ex 1 N j N jcid4 i1 U 1 N j cid18 N jcid4 W i1 N jcid4 N jcid4 1 N 2 j ExUW 1 1 i1 i1 cid11 Ex cid12 v T X x j ExUWN j N j 1 N 2 j ExUW cid17 cid18 UW 1 N j N jcid4 i1 ExUW N jcid4 i1 1 ExUWN j Ex 1 N j 1 N j ExUW From Eqs 51 55 cid11 Ex cid12 v T x j cid11 Ex cid12 v T X x j This proves ﬁrst Next prove second By deﬁnition cid11 Var cid12 v T X x j cid17 Var cid18 UW 1 N j N jcid4 i1 1 N 2 j VarUW N jcid4 i1 1 VarUWN j 1 N 2 j VarUW N j 48 49 50 51 52 53 54 55 56 57 58 59 Notice random variables U W conditionally independent given X x j Goodman 14 provides expression variance product independent random variables Formally A B independent random variables Var A B given Var A B Var AExB2 VarBEx A2 Var AVarB Using expression derive expression Varv T x j shown cid11 Var cid12 v T x j cid17 Var cid17 Var 1 N j 1 N j cid17 Var U 1 N j cid17 cid18 U Ex N jcid4 i1 N jcid4 i1 cid18 N jcid4 i1 W N jcid4 1 N j i1 cid17 cid18 1 N j N jcid4 i1 U Var 1 N j cid182 cid17 W Var cid18 N jcid4 i1 W 1 N j N jcid4 i1 cid18 cid17 W Ex cid182 1 N j N jcid4 i1 U 60 61 62 It easy cid17 cid18 Var 1 N j N jcid4 i1 U VarU N j V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 55 cid17 Ex 1 N j N jcid4 i1 cid18 U ExU Similarly easy cid17 Var 1 N j N jcid4 i1 cid18 W VarW N j cid17 Ex 1 N j N jcid4 i1 cid18 W ExW Substituting values Eq 62 cid11 Var cid12 v T x j VarUExW2 N j VarWExU2 N j VarUVarW N 2 j Using formula variance products independent random variables given Eq 60 Eq 59 cid11 Var cid12 v T X x j VarUExW2 N j VarWExU2 N j VarUVarW N j 63 64 Notice righthand sides Eqs 63 64 differ term denominator term Eq 63 N j Eq 64 Thus N j 1 Varv T x j Varv T X x j assuming Q N 2 j equal posterior distribution N j 1 Varv T x j Varv T X x j This proves second proof follows cid2 As mentioned earlier chain pseudo tree obtained applying topological linearizations nodes T following theorem follows immediately Lemma 1 Theorem 5 Given pseudo tree T topological linearization T T set samples S variance ANDOR sample tree mean deﬁned ANDOR sample tree ψT S smaller equal variance ANDOR sample tree mean deﬁned ANDOR sample tree ψT cid8S In words variance ANDOR sample tree mean smaller equal variance OR sample tree mean cid8 41 Remarks variance reduction From proof Lemma 1 given pseudo tree T topological linearization T X wrt X value x D X sampled value AND node x ANDOR sample tree deﬁned relative T equal corresponding value ANDOR sample tree deﬁned relative T X result variance We tie variance reduction number virtual ANDOR tree samples deﬁned recursively Deﬁnition 20 Virtual samples ANDOR sample tree Given ANDOR sample tree based set samples S number virtual samples associated leaf AND node l number times path root l sampled S The number virtual samples rooted internal AND node equals product number virtual samples rooted child OR nodes The number virtual samples rooted OR node n equals sum number virtual samples rooted child AND nodes The number virtual samples ANDOR sample tree equals number virtual samples rooted root OR node Note leaf node ANDOR sample tree sampled number virtual samples equals number solution subtrees Deﬁnition 7 Example 9 Figs 9a 9b samples arranged OR sample tree ANDOR sample tree respectively The 4 samples correspond 8 virtual samples ANDOR sample tree The ANDOR sample tree includes example assignment C 0 B 2 D 1 A 1 present OR sample tree samples rooted B conditionally independent samples rooted D given C The following propositions immediate deﬁnition virtual ANDOR tree samples proof Lemma 1 obtained applying topological linearization times different nodes Proposition 3 Given pseudo tree T pseudo tree T T set samples S number virtual ANDOR tree samples ψT S greater equal number virtual ANDOR tree samples ψT cid8S cid8 56 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Fig 9 A chain pseudo tree corresponding topological linearization pseudo tree given Fig 1b Figures b c 4 samples arranged OR sample tree ANDOR sample tree respectively The OR sample tree given b deﬁned relative pseudo tree given The ANDOR sample tree given c deﬁned relative pseudo tree given Fig 1b Each node tree annotated number virtual samples rooted node We ANDOR sample tree represents 8 virtual samples OR sample tree represents 4 samples Proposition 4 Given pseudo tree T pseudo tree T obtained applying topological linearization times different nodes T set samples S number virtual ANDOR tree samples ψT S strictly greater number virtual ANDOR tree samples ψT cid8S variance ANDOR sample tree mean deﬁned relative ψT S strictly smaller variance ANDOR sample tree mean deﬁned relative ψT cid8S assuming proposal distribution Q equal P G cid8 In summary proved given set samples variance ANDOR sample tree mean equal variance ANDOR sample tree mean deﬁned relative topological linearization wrt node pseudo tree particular variance OR sample tree mean We demonstrated variance reduction tied number virtual samples Speciﬁcally showed variance reduction occurs AND nodes child nodes appear given set samples times 5 ANDOR sample graph mean Next powerful strategy estimating sample mean ANDOR space moving ANDOR trees ANDOR graphs 6 The idea similar ANDOR graph search merge nodes ANDOR sample tree uniﬁable based context Deﬁnition 10 form ANDOR sample graph This results larger number virtual samples Deﬁnition 21 ANDOR sample graph Given pseudo tree T set samples S ANDOR sample graph obtained ANDOR sample tree ψT S merging OR nodes context The frequency arc n m OR node n labeled Xi AND node m changed account merging based context weight n m remains In particular frequency arc n m ANDOR sample graph equals number times partial assignment Aπm Xi contextT Xi appears S6 Deﬁnition 22 ANDOR sample graph mean The ANDOR sample graph mean value root node ANDOR sample graph Deﬁnition 23 Number virtual samples ANDOR sample graph The number virtual samples ANDOR sample graph number virtual samples rooted root node Example 10 Fig 10 shows ANDOR sample graph obtained ANDOR sample tree given Fig 9c merging context uniﬁable nodes Notice context A B Therefore AND node B 0 1 2 merge child OR nodes labeled A ANDOR sample tree yielding ANDOR sample graph The ANDOR sample 6 Recall Deﬁnition 14 Aπm denotes assignment sequence path πm root ψT S m Also recall notation AπmXi contextT Xi denotes projection assignment Aπm set Xi contextT Xi V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 57 Fig 10 ANDOR sample graph obtained ANDOR sample tree given Fig 9c merging context uniﬁable nodes The ANDOR sample tree graph based pseudo tree given Fig 1b The context A B C D B C C respectively Each node annotated number virtual samples rooted node The ANDOR sample graph represents 12 virtual samples Compare ANDOR sample tree given Fig 9c represents 8 samples graph represents 12 virtual samples compared ANDOR sample tree represents 8 virtual samples The ANDOR sample graph includes example sample C 0 B 2 D 1 A 0 virtual samples ANDOR sample tree Clearly Proposition 5 The number virtual ANDOR graph samples greater equal number virtual ANDOR tree samples based underlying pseudo tree Since ANDOR sample graph captures virtual samples variance ANDOR sample graph mean smaller variance ANDOR sample tree mean Formally Theorem 6 The variance ANDOR sample graph mean equal ANDOR sample tree mean The algorithm computing ANDOR sample graph mean identical ANDOR sample tree mean Steps 410 Algorithm 2 The difference store samples perform value computations ANDOR sample graph instead ANDOR sample tree Theorem 7 Complexity computing ANDOR sample graph mean Given graphical model n variables pseudo tree T space maximum context size treewidth w complexity O nN N samples time complexity ANDOR graph sampling O nN w 6 Experiments In section demonstrate empirically moving OR space ANDOR space improves accuracy estimates function time The section organized follows We ﬁrst implementation details experimental set results probabilistic deterministic constraint benchmark networks 61 Experimental setup As mentioned earlier strength ANDORbased estimates samples estimates based generated importance sampling scheme Therefore order demonstrate impact ANDOR estimation nontrivial setting generate samples stateoftheart importance sampling techniques IJGPIS 1516 IJGPSampleSearch 1719 IJGPIS uses output generalized belief propagation scheme called Iterative Join Graph Propagation IJGP construct proposal distribution It shown belief propagation schemes applied original graph 58 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Fig 11 Figure showing scope experimental study The algorithm names abbreviated follows IJGPIS stands IJGPbased importance sampling IJGPSS stands IJGPbased SampleSearch ortree aotree aograph stand OR tree ANDOR tree ANDOR graph respectively minﬁll hmetis orderings constructing pseudo trees For example aographIJGPSSminﬁll stands IJGPbased SampleSearch uses ANDOR graph constructed minﬁll ordering deriving estimates clusters nodes yield good approximation true posterior available choices 2022 sampling output obvious choice 231516 details IJGP 2024 generalized belief propagation scheme parametrized constant called ibound yielding class algorithms IJGPi complexity exponential tradeoff accuracy complexity As increases accuracy generally increases When equals treewidth graphical model IJGPi exact We use ibound 10 set number iterations 10 experiments ensure IJGP terminates reasonable time 5 minutes requiring bounded space The variance accuracy ANDOR sample tree mean highly dependent height pseudo tree ANDOR sample graph mean dependent treewidth pseudo tree We experimented alternatives constructing pseudo tree based minﬁll ordering based hyper graph partitioning hmetis software7 henceforth called hmetis ordering In earlier studies 425 shown minﬁll ordering generally yields pseudo trees having smaller treewidth compared alternatives hmetis ordering yields pseudo trees having smaller height Finally networks having substantial determinism generate samples IJGPbased SampleSearch IJGPSS 1719 instead IJGPIS It known networks pure importance sampling generates useless zero weight samples eventually rejected SampleSearch overcomes rejection problem explicitly searching nonzero weight sample yielding eﬃcient sampling scheme heavily deterministic databases It shown SampleSearch importance sampling scheme generates samples modiﬁcation proposal distribution backtrackfree wrt constraints Thus derive ANDOR sample tree graph means samples generated SampleSearch need replace proposal distribution backtrackfree distribution computing sample weight We evaluated algorithms weighted counting task deﬁned mixed probabilistic deterministic networks probability evidence Bayesian network counting solutions constraint network We experimented ﬁve sets benchmarks alarm networks b grid networks c linkage networks d coding networks e graph coloring networks modeled satisﬁability problems The linkage coding graph coloring networks strong deterministic relationships generated samples IJGPbased SampleSearch IJGPSS On remaining networks alarm grids IJGPIS Fig 11 shows benchmarks algorithms experimented We organize results subsections In subsection results instances exact weighted counts known Section 63 results instances exact weighted counts known The reason separation difference evaluation criteria 62 Results networks exact value weighted counts known 621 Evaluation criteria For networks exact weighted counts known measure performance comparing log relative error exact weighted counts approximate ones If Z exact value Z approximate value weighted counts logrelative error deﬁned cid8 cid8 cid8 cid8 Δ 65 cid8 cid8 cid8 cid8 logZ logZ logZ 7 Available httpwwwuserscsumnedukarypismetishmetis V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 59 Table 1 Results alarm networks Table showing average sample means cid10Z relative standard deviation sample means RSD average logrelative error Δ ﬁve runs ortreeIJGPISminﬁll aotreeIJGPISminﬁll aographIJGPISminﬁll ortreeIJGPIShmetis aotreeIJGPIShmetis aographIJGPIShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Z h w 624e06 14 20 796e18 15 25 246e04 15 24 478e03 16 24 966e10 19 27 199e06 19 25 359e18 12 17 184e19 13 19 429e26 13 22 963e08 14 21 408e03 14 21 272e04 14 22 Instance n k f e c BN_10 85 2 85 17 0 BN_11 105 2 105 46 0 BN_12 90 2 90 11 0 BN_13 125 2 125 9 0 BN_14 115 2 115 30 0 BN_15 120 2 120 19 0 BN_4 100 2 100 51 0 BN_5 125 2 125 55 0 BN_6 125 2 125 71 0 BN_7 95 2 95 30 0 BN_8 100 2 100 9 0 BN_9 105 2 105 13 0 ortree IJGPIS minﬁll cid10Z RSD Δ 623e06 008 120e04 797e18 015 411e05 246e04 034 257e04 479e03 029 416e04 963e10 056 181e04 199e06 043 253e04 359e18 023 448e05 184e19 009 255e05 429e26 026 318e05 960e08 025 185e04 407e03 028 433e04 272e04 020 258e04 aotree IJGPIS minﬁll cid10Z RSD Δ 624e06 005 487e05 796e18 026 499e05 246e04 020 138e04 478e03 010 230e04 966e10 024 904e05 199e06 023 140e04 359e18 008 185e05 184e19 009 304e05 429e26 012 154e05 960e08 021 169e04 408e03 027 362e04 271e04 018 255e04 aograph IJGPIS minﬁll cid10Z RSD Δ 624e06 005 474e05 796e18 033 577e05 246e04 015 103e04 478e03 008 174e04 966e10 030 120e04 199e06 019 120e04 359e18 007 230e05 184e19 010 290e05 429e26 013 212e05 961e08 019 148e04 408e03 026 390e04 272e04 016 214e04 h w 16 22 18 23 18 23 15 23 20 26 19 26 13 18 15 20 13 18 15 20 14 21 15 21 ortree IJGPIS hmetis cid10Z RSD Δ 623e06 019 163e04 795e18 099 201e04 246e04 020 155e04 479e03 028 529e04 969e10 054 267e04 198e06 042 290e04 359e18 008 253e05 183e19 011 640e05 429e26 029 377e05 962e08 037 135e04 408e03 023 301e04 272e04 021 276e04 aotree IJGPIS hmetis cid10Z RSD Δ 624e06 013 889e05 795e18 036 778e05 246e04 014 134e04 478e03 010 240e04 968e10 045 210e04 199e06 030 166e04 359e18 008 237e05 183e19 009 766e05 429e26 020 253e05 963e08 015 704e05 408e03 009 135e04 272e04 007 150e04 aograph IJGPIS hmetis cid10Z RSD Δ 624e06 013 881e05 795e18 036 572e05 246e04 012 123e04 479e03 012 276e04 968e10 044 191e04 199e06 027 137e04 359e18 008 226e05 183e19 007 634e05 429e26 013 194e05 963e08 020 108e04 408e03 009 126e04 272e04 006 142e04 We compute log relative error instead usual relative error probability evidence extremely 10 solution counts large 1010 relative error exact approximate small 10 answer arbitrarily close 1 need large number digits distinguish results Tables 16 contain results On instance ran algorithm 5 times For algorithm report average logrelative error Δ average sample means cid10Z 8 5 runs We report relative standard deviation RSD 5 runs RSD deﬁned follows Let Scid10Z standard deviation cid10Z average sample means k runs solver 8 The sample mean recovered logrelative error exact value weighted counts Eq 65 We report table readers convenience 60 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 RSD 100 Scid10Z cid10Z 66 Relative Standard Deviation RSD measure precision accuracy It unitless quantity allows compare standard deviation quantities different means cid10Z meaningfully It especially relevant given schemes roughly accuracy In case prefer scheme having smaller RSD Also RSD small 2 indicates scheme small sample variance likely proposal distribution good approximation true posterior 2 Notation tables Table 1 reference The ﬁrst column shows instance statistical information instance number variables n average domain size k number functions f number evidence nodes e number deterministic functions constraints c The second column gives exact value weighted counts Z known treewidth w height h pseudo tree minﬁll hmetis orderings respectively Columns 38 average sample mean cid10Z relative standard deviation RSD average logrelative error Δ 5 runs solvers The average logrelative error best performing scheme problem instance highlighted bold 622 Results alarm networks Our ﬁrst benchmark domain alarm networks UAI 2006 evaluation 26 To create networks ﬁxed number copies burglar alarm graph described Pearls book 12 created One graph copies connected previously considered copies probability Each variable randomly set hidden observed Table 1 shows results We following observations First instances ANDOR sample tree graph means slightly better terms logrelative error OR sample tree mean Second logrelative error RSD values schemes small indicating proposal distribution close posterior dis tribution Third cases RSD ANDOR sample tree graph means smaller OR sample tree mean Finally performance schemes use minﬁll hmetis ordering incomparable minﬁll better times hmetis better Fig 12 logrelative error vs time plots schemes randomly chosen alarm networks We clearly superior anytime performance ANDOR sample tree graph means compared OR sample tree graph means Note importance sampling anytime algorithm needs sample estimate weighted counts Moreover accuracy estimate improves samples drawn 623 Results grid networks The grid networks available authors Cachet SAT model counter 27 A grid Bayesian network s s grid directed edges node neighbors right The upperleft node source bottomright node sink The deterministic ratio p parameter specifying fraction nodes deterministic functional The grid instances designated p s For example instance 5018 indicates grid size 18 18 50 nodes deterministic Evidence networks set random Tables 2 3 4 results The results similar alarm networks instances ANDOR graph schemes hmetis minﬁllbased superior terms accuracy RSD ANDOR tree schemes turn slightly superior OR tree scheme Again performance hmetisbased minﬁllbased schemes incomparable ordering scheme strictly dominate The logrelative error vs time plots largest grid instances value deterministic ratio shown Figs 13 14 15 respectively We clearly superior anytime performance ANDOR graph schemes hmetisbased minﬁllbased compared ANDOR tree OR tree schemes The ANDOR tree scheme slightly better OR tree scheme 624 Results linkage networks The linkage instances Bayesian networks model likelihood computation pedigree 28 These networks 7772315 nodes average domain size 9 The linkage networks generated converting biological linkage analysis data Bayesian Markov network Linkage analysis statistical method mapping genes chromosome 29 This useful practice identifying disease genes The input ordered list loci L1 Lk1 allele frequencies locus pedigree individuals typed loci The goal linkage analysis evaluate likelihood candidate vector θ1 θk recombination fractions input pedigree locus order The component θi candidate recombination fraction loci Li Li1 The pedigree data represented Bayesian network types random variables genetic loci variables represent genotypes individuals pedigree genetic loci variables individual locus paternal allele maternal allele phenotype variables selector variables auxiliary variables represent gene ﬂow pedigree Fig 16 represents fragment network describes parents child interactions simple 2loci analysis The genetic loci variables individual locus j denoted Li jp Li jm Variables Xi j S jp S jm denote phenotype variable paternal selector variable maternal selector variable individual locus j respectively The conditional probability tables correspond selector variables V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 61 Fig 12 Logrelative error versus time plots randomly chosen alarm networks Fig 13 Logrelative error versus time plots largest Grid instances Deterministic ratio 50 parameterized recombination ratio θ The remaining tables contain deterministic information It shown given pedigree data computing likelihood recombination fractions equivalent computing probability evidence Bayesian network model problem details consult 28 Table 5 shows results linkage networks UAI 2006 evaluation 26 The ANDOR graph estimates closer exact value P e ANDOR tree OR tree estimates BN_69 instance 62 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Table 2 Results Grid instances Deterministic ratio 50 Table showing average sample means cid10Z relative standard deviation sample means RSD average logrelative error Δ ﬁve runs ortreeIJGPISminﬁll aotreeIJGPISminﬁll aographIJGPISminﬁll ortreeIJGPIS hmetis aotreeIJGPIShmetis aographIJGPIShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Z h w 753e07 8 31 721e05 8 37 183e04 13 38 468e06 11 49 893e05 19 55 888e04 25 62 208e04 14 46 121e03 28 77 Instance n k f e c 50125 144 2 144 10 62 50145 196 2 196 10 93 50155 225 2 225 10 111 50165 256 2 256 10 125 50175 289 2 289 10 138 50185 324 2 324 10 153 50195 361 2 361 10 172 50205 400 2 400 10 190 ortree IJGPIS minﬁll cid10Z RSD Δ 750e07 051 312e04 722e05 025 208e04 177e04 531 624e03 471e06 588 378e03 785e05 1000 151e02 999e04 2540 346e02 209e04 300 306e03 186e03 6380 969e02 aotree IJGPIS minﬁll cid10Z RSD Δ 752e07 013 944e05 720e05 016 159e04 182e04 097 971e04 468e06 178 113e03 804e05 443 114e02 987e04 5090 411e02 207e04 258 244e03 318e03 10100 106e01 aograph IJGPIS minﬁll cid10Z RSD Δ 753e07 020 819e05 721e05 012 962e05 181e04 033 765e04 469e06 028 250e04 893e05 214 180e03 889e04 100 123e03 208e04 106 101e03 121e03 459 477e03 h w 14 21 9 18 17 29 20 30 19 36 24 41 17 31 27 46 ortree IJGPIS hmetis cid10Z RSD Δ 753e07 008 312e05 721e05 006 490e05 182e04 097 870e04 467e06 131 907e04 929e05 263 443e03 833e04 3420 376e02 204e04 111 208e03 931e04 6170 847e02 aotree IJGPIS hmetis cid10Z RSD Δ 753e07 005 436e05 721e05 003 215e05 182e04 039 382e04 467e06 056 303e04 885e05 339 313e03 840e04 390 789e03 208e04 044 406e04 114e03 3070 357e02 aograph IJGPIS hmetis cid10Z RSD Δ 753e07 005 432e05 721e05 003 215e05 183e04 047 427e04 467e06 035 213e04 893e05 068 630e04 889e04 178 225e03 208e04 032 293e04 120e03 207 306e03 Fig 14 Logrelative error versus time plots largest Grid instances Deterministic ratio 75 ANDOR tree scheme best On instances BN_69 BN_74 accuracy schemes roughly ANDOR graph estimates smallest RSD Again ANDOR tree estimates slightly better OR tree estimates instances V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 63 Table 3 Results Grid instances Deterministic ratio 75 Table showing average sample means cid10Z relative standard deviation sample means RSD average logrelative error Δ ﬁve runs ortreeIJGPISminﬁll aotreeIJGPISminﬁll aographIJGPISminﬁll ortreeIJGPIS hmetis aotreeIJGPIShmetis aographIJGPIShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Z h w 415e03 11 41 720e04 9 39 438e05 10 47 370e04 10 78 352e05 10 39 500e03 11 44 132e03 14 65 215e05 17 67 743e06 19 52 310e05 22 84 675e04 29 79 Instance n k f e c 75165 256 2 256 10 193 75175 289 2 289 10 217 75185 324 2 324 10 245 75195 361 2 361 10 266 75205 400 2 400 10 299 75215 441 2 441 10 331 75225 484 2 484 10 361 75235 529 2 529 10 406 75245 576 2 576 10 442 75255 625 2 625 10 455 75265 676 2 676 10 506 ortree IJGPIS minﬁll cid10Z RSD Δ 414e03 088 134e03 717e04 278 292e03 439e05 119 807e04 368e04 519 485e03 352e05 179 130e03 500e03 242 364e03 123e03 547 114e02 212e05 2680 181e02 690e06 1720 124e02 235e05 2990 317e02 348e04 11200 169e01 aotree IJGPIS minﬁll cid10Z RSD Δ 418e03 078 136e03 721e04 158 154e03 439e05 055 417e04 372e04 433 411e03 351e05 061 461e04 501e03 072 117e03 131e03 528 627e03 230e05 1300 993e03 698e06 558 607e03 276e05 1970 194e02 555e04 15200 189e01 aograph IJGPIS minﬁll cid10Z RSD Δ 416e03 051 679e04 721e04 074 782e04 438e05 027 185e04 370e04 090 913e04 352e05 073 477e04 503e03 087 175e03 132e03 213 277e03 214e05 116 900e04 721e06 184 250e03 313e05 307 184e03 650e04 829 974e03 h w 17 24 17 33 15 27 18 31 13 26 16 33 15 31 26 42 20 39 25 45 29 53 ortree IJGPIS hmetis cid10Z RSD Δ 412e03 030 133e03 714e04 111 144e03 439e05 234 201e03 333e04 180 135e02 352e05 105 785e04 507e03 310 545e03 131e03 228 258e03 214e05 560 416e03 753e06 368 229e03 343e05 3640 273e02 465e04 4710 675e02 aotree IJGPIS hmetis cid10Z RSD Δ 415e03 023 428e04 716e04 030 625e04 438e05 171 127e03 327e04 069 155e02 353e05 054 401e04 504e03 122 240e03 132e03 091 106e03 210e05 103 215e03 746e06 119 684e04 295e05 563 484e03 494e04 1660 445e02 aograph IJGPIS hmetis cid10Z RSD Δ 415e03 018 353e04 716e04 042 743e04 437e05 148 119e03 327e04 075 154e02 353e05 046 354e04 504e03 118 175e03 132e03 111 135e03 213e05 167 124e03 761e06 139 205e03 311e05 266 217e03 680e04 712 651e03 In Fig 17 logrelative error vs time plots randomly chosen linkage instances The ANDOR graph scheme exhibits superior anytime performance compared ANDOR tree scheme turn superior OR tree scheme Next present results pedigree linkage instances UAI 2008 evaluation 30 These linkage Bayesian networks evidence instantiated yielding unnormalized Bayesian network Markov net work In subsection report results 10 20 instances solved exactly UAI 2008 evaluation The results remaining 10 instances presented subsection Table 6 shows results Fig 18 shows log relative error versus time plots randomly chosen instances Again similar picture ANDOR graph scheme superior schemes 63 Results networks exact weighted counts known When exact results available evaluating capability approximation algorithm problematic quality approximation close approximation exact assessed To allow comparison hard instances evaluate power sampling schemes yielding good lowerbound approximations quality compared higher better exact solution available 64 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Table 4 Results Grid instances Deterministic ratio 90 Table showing average sample means cid10Z relative standard deviation sample means RSD average logrelative error Δ ﬁve runs ortreeIJGPISminﬁll aotreeIJGPISminﬁll aographIJGPISminﬁll ortreeIJGPIS hmetis aotreeIJGPIShmetis aographIJGPIShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Instance n k f e c 90245 576 2 576 10 528 90255 625 2 625 10 553 90265 676 2 676 10 597 90305 900 2 900 10 792 90345 1156 2 1156 10 1048 90385 1444 2 1444 10 1300 90425 1764 2 1764 10 1593 90465 2116 2 2116 10 1904 90505 2500 2 2500 10 2264 Z h w 390e05 6 35 217e02 7 25 267e07 4 16 394e03 8 45 131e02 11 59 708e04 11 60 470e03 14 65 213e02 19 87 120e02 16 79 ortree IJGPIS minﬁll cid10Z RSD Δ 387e05 078 894e04 217e02 041 976e04 267e07 035 258e04 392e03 290 398e03 111e02 1200 382e02 606e04 2930 359e02 483e03 6550 111e01 180e02 11100 234e01 795e03 3340 103e01 aotree IJGPIS minﬁll cid10Z RSD Δ 389e05 024 340e04 217e02 033 801e04 267e07 028 159e04 392e03 094 145e03 121e02 702 198e02 734e04 398 619e03 436e03 1430 229e02 163e02 2440 770e02 109e02 1340 281e02 aograph IJGPIS minﬁll cid10Z RSD Δ 390e05 023 216e04 217e02 032 755e04 267e07 029 173e04 394e03 062 814e04 130e02 184 330e03 728e04 277 414e03 446e03 524 965e03 218e02 1350 217e02 118e02 610 117e02 h w 11 18 23 50 14 23 12 23 16 26 17 32 16 49 27 51 23 60 ortree IJGPIS hmetis cid10Z RSD Δ 390e05 019 166e04 217e02 038 946e04 266e07 014 880e05 394e03 076 125e03 132e02 1580 287e02 549e04 631 353e02 380e03 4320 762e02 144e02 12100 288e01 978e03 2520 618e02 aotree IJGPIS hmetis cid10Z RSD Δ 390e05 015 113e04 216e02 039 928e04 266e07 018 850e05 395e03 041 623e04 128e02 421 724e03 576e04 210 284e02 452e03 1390 215e02 621e02 16000 189e01 986e03 2470 534e02 aograph IJGPIS hmetis cid10Z RSD Δ 390e05 015 112e04 216e02 034 814e04 266e07 017 875e05 395e03 035 543e04 128e02 293 681e03 574e04 147 290e02 450e03 681 121e02 284e02 4360 105e01 104e02 862 332e02 Fig 15 Logrelative error versus time plots largest Grid instances Deterministic ratio 90 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 65 Fig 16 A fragment Bayesian network genetic linkage analysis Fig 17 Logrelative error versus time plots sample linkage instances UAI 2006 evaluation 66 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Table 5 Results linkage instances UAI 2006 evaluation Table showing average sample means cid10Z relative standard deviation sample means RSD average logrelative error Δ ﬁve runs ortreeIJGPSSminﬁll aotreeIJGPSSminﬁll aographIJGPSSminﬁll ortreeIJGP SShmetis aotreeIJGPSShmetis aographIJGPSShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Z h w 528e54 36 52 200e71 35 110 512e111 35 80 421e150 33 107 226e113 42 97 375e45 32 70 588e91 32 116 493e110 37 139 688e79 22 114 Instance n k f e c BN_69 777 7 777 78 402 BN_70 2315 5 2315 159 1290 BN_71 1740 6 1740 202 920 BN_72 2155 6 2155 252 1130 BN_73 2140 5 2140 216 1115 BN_74 749 6 749 66 374 BN_75 1820 5 1820 155 1000 BN_76 2155 7 2155 169 1130 BN_77 1020 9 1020 135 507 ortree IJGPSS minﬁll cid10Z RSD Δ 258e55 445 246e02 781e77 8290 781e02 152e116 20000 600e02 101e156 14700 501e02 263e122 22100 948e02 246e46 10100 344e02 609e97 8680 682e02 308e123 9270 127e01 256e86 9270 982e02 aotree IJGPSS minﬁll cid10Z RSD Δ 295e55 1050 236e02 120e75 18500 678e02 151e116 19000 583e02 697e156 19200 470e02 551e122 22300 948e02 351e46 15200 330e02 743e97 12100 681e02 415e123 8620 123e01 445e86 8230 954e02 aograph IJGPSS minﬁll cid10Z RSD Δ 290e55 255 237e02 344e75 2530 534e02 185e113 5870 226e02 253e150 6790 220e03 329e118 20100 498e02 209e46 7020 298e02 107e95 9950 547e02 141e118 10100 799e02 152e84 8160 739e02 h w 27 55 44 90 39 101 35 88 39 93 34 67 37 76 38 108 57 125 ortree IJGPSS hmetis cid10Z RSD Δ 239e55 2780 255e02 737e74 8310 358e02 139e115 13600 486e02 271e153 8370 226e02 109e123 16100 959e02 727e48 21800 914e02 979e98 17400 808e02 842e121 21300 107e01 336e83 10600 612e02 aotree IJGPSS hmetis cid10Z RSD Δ 283e55 2110 240e02 147e73 8410 332e02 111e115 12200 475e02 413e153 8060 213e02 280e123 20100 952e02 490e48 14800 779e02 703e98 14500 801e02 177e120 21600 107e01 169e83 8410 614e02 aograph IJGPSS hmetis cid10Z RSD Δ 289e55 645 237e02 655e74 2320 353e02 663e112 21100 166e02 131e150 6480 375e03 133e121 20700 815e02 928e47 7770 395e02 147e94 3160 401e02 900e120 16800 955e02 376e81 6460 301e02 Speciﬁcally exact weighted counts known compare lower bounds obtained combining sample means output schemes Markov inequality based lower bounding scheme presented 31 Such lower bounding schemes 32 input set unbiased sample means b real number 0 α 1 output lower bound weighted counts Z correct probability greater α Formally given set unbiased sample means use following theorem probabilistic lower bound weighted counts Z Theorem 8 See 3231 Let cid10Z1 cid10Z2 cid10Zr unbiased sample means r independent runs solver Let 0 α 1 constant let β 1 1 r Let Zlb given 1α Zlb 1 β r min i1 cid10Z Then Zlb lower bound Z probability greater α 67 In experiments set α 099 r 5 run algorithm ﬁve times lower bounds correct r 2512 Note evaluate algorithms terms lower probability greater 099 β 1 bounds higher lower bound better corresponding scheme 1α 1 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 67 Fig 18 Logrelative error versus time plots sample linkage instances UAI 2008 evaluation 631 Results linkage instances Table 7 shows results 10 linkage instances UAI 2008 evaluation exact weighted counts known Note cell report lower bound Zlb average sample mean cid10Z 9 RSD 5 runs We clearly instances ANDOR graph scheme yields substantially higher lower bounds ANDOR tree scheme turn yields higher lower bounds OR tree scheme The RSD ANDOR graph scheme smaller schemes 632 Results random coding networks The random coding networks class linear block codes 33 They represented fourlayer belief networks The second layer correspond input information bits parity check bits respectively Each parity check bit represents XOR function input bits Input parity check nodes binary output nodes realvalued Each layer number nodes code rate R K N 12 K number input bits N number transmitted bits Given number input bits K 128 number parents P 4 XOR bit channel noise variance σ 040 coding network structure generated randomly picking parents XOR node Then input signal simulated assuming uniform random distribution information bits corresponding values parity check bits computed assignment output nodes generated assuming adding Gaussian noise information parity check bit Table 8 shows results Unlike benchmarks ANDOR graph scheme slightly better ANDOR tree OR tree schemes The improvement accuracy small IJGPbased proposal dis tribution close exact posterior distribution indicated relatively smaller RSD 2 instances 9 Note average sample mean shown sake convenience reader It relevant making comparisons performance schemes exact weighted counts known 68 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Table 6 Results linkage instances UAI 2008 evaluation Table showing average sample means cid10Z relative standard deviation sample means RSD average logrelative error Δ ﬁve runs ortreeIJGPSSminﬁll aotreeIJGPSSminﬁll aographIJGPSSminﬁll ortreeIJGP SShmetis aotreeIJGPSShmetis aographIJGPSShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Z h w 781e15 20 51 718e79 24 93 234e30 25 58 278e39 29 56 169e116 26 82 184e84 27 89 263e117 29 56 564e55 17 69 632e103 26 87 173e31 27 52 Instance n k f e c pedigree1 334 2 334 0 121 pedigree18 1184 2 1184 0 386 pedigree20 437 2 437 0 147 pedigree23 402 2 402 0 130 pedigree25 1289 2 1289 0 396 pedigree30 1289 2 1289 0 413 pedigree37 1032 2 1032 0 333 pedigree38 724 2 724 0 263 pedigree39 1272 2 1272 0 354 pedigree42 448 2 448 0 156 ortree IJGPSS minﬁll cid10Z RSD Δ 770e15 327 913e04 194e82 11900 486e02 909e34 11400 133e01 264e39 1680 169e03 485e125 22200 905e02 320e87 22400 701e02 250e119 10000 188e02 361e56 21200 401e02 783e109 21600 702e02 162e31 879 138e03 aotree IJGPSS minﬁll cid10Z RSD Δ 792e15 504 109e03 133e82 12500 508e02 121e32 20600 123e01 252e39 1040 114e03 405e122 17300 555e02 147e87 22300 692e02 125e117 1790 283e03 527e56 1880 191e02 134e106 13400 435e02 156e31 345 144e03 aograph IJGPSS minﬁll cid10Z RSD Δ 783e15 098 273e04 404e79 6820 451e03 218e30 2340 293e03 290e39 555 468e04 145e116 2170 851e04 697e85 2430 516e03 118e117 568 301e03 145e55 1720 110e02 557e103 802 545e04 174e31 326 345e04 h w 19 41 31 70 27 50 24 43 47 86 28 66 47 56 53 69 31 62 27 50 ortree IJGPSS hmetis cid10Z RSD Δ 745e15 912 239e03 117e81 12300 426e02 407e33 21800 165e01 237e39 1510 205e03 288e121 17500 481e02 250e85 21800 245e02 147e119 20100 253e02 283e62 8520 141e01 371e104 8280 132e02 173e31 1060 127e03 aotree IJGPSS hmetis cid10Z RSD Δ 769e15 166 570e04 153e81 11900 388e02 911e32 19600 846e02 272e39 292 355e04 198e118 15400 231e02 801e85 12200 120e02 126e117 9640 459e03 156e55 13800 172e02 310e103 11500 532e03 161e31 630 105e03 aograph IJGPSS hmetis cid10Z RSD Δ 775e15 188 523e04 701e79 3010 142e03 143e30 3840 807e03 272e39 306 344e04 471e117 3210 497e03 163e84 6970 303e03 117e117 1740 305e03 113e55 8420 147e02 539e103 1100 693e04 172e31 595 634e04 Consequently OR sample tree mean accurate Our results consistent previous studies 222120 demonstrated generalized belief propagation yields good approximation true posterior random coding networks 633 Results graph coloring problems Our ﬁnal domain 4coloring problems generated Joseph Culbersons ﬂat graph coloring generator10 Here interested counting number solutions graph coloring instance Table 9 shows results We observe ANDOR tree graph sampling schemes yield higher lower bounds OR tree sampling schemes 64 Summary experiments In summary experiments ANDOR sample graph mean substantially superior terms accuracy precision ANDOR sample tree mean turn slightly superior OR sample tree mean In particular problem size gets larger instances harder exact inference ANDOR graph scheme orders magnitude superior As expected proposal distribution close posterior distribution example results alarm networks difference performance OR ANDOR 10 Available httpwwwcsualbertacajoeColoring V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 69 Table 7 Results linkage instances UAI 2008 evaluation Table showing average sample means cid10Z relative standard deviation sample means RSD lower bound cid10Zlb weighted counts 99 conﬁdence ﬁve runs ortreeIJGPSSminﬁll aotreeIJGPSSminﬁll aographIJGPSSminﬁll ortreeIJGPSShmetis aotreeIJGPSShmetis aographIJGPSShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Instance n k f e c pedigree13 1077 2 1077 0 343 pedigree19 793 2 793 0 286 pedigree31 1183 2 1183 0 389 pedigree34 1160 2 1160 0 348 pedigree40 1030 2 1030 0 351 pedigree41 1062 2 1062 0 346 pedigree44 811 2 811 0 287 pedigree51 1152 2 1152 0 383 pedigree7 1068 2 1068 0 315 pedigree9 1118 2 1118 0 386 Z h w 36 115 23 121 38 124 37 109 29 122 34 92 30 97 42 92 36 96 28 108 ortree IJGPSS minﬁll cid10Z RSD Zlb 240e44 13600 243e46 121e66 21300 915e70 464e81 12700 466e82 427e76 8860 191e77 aotree IJGPSS minﬁll cid10Z RSD Zlb 677e44 13800 889e47 682e67 20600 117e69 289e78 19600 199e80 776e73 14300 845e74 138e97 21900 771e102 140e97 21800 130e101 805e85 21500 851e89 298e65 14600 154e66 380e78 19300 114e81 135e72 17100 502e75 501e83 21600 574e87 435e83 13900 444e86 148e64 21200 220e66 419e77 21800 253e80 130e71 7810 171e72 220e83 19700 772e87 aograph IJGPSS minﬁll cid10Z RSD Zlb 746e33 6760 470e34 766e62 12500 814e63 558e73 8790 467e74 161e67 17700 222e69 316e92 16800 208e94 403e78 13500 649e80 325e64 3600 593e65 947e75 12500 310e76 270e66 12000 792e68 255e80 6840 304e81 h w 48 72 35 63 43 77 39 69 36 86 39 74 31 59 46 87 39 74 36 68 ortree IJGPSS hmetis cid10Z RSD Zlb 185e35 6330 309e37 813e66 9480 785e68 502e75 20900 535e77 381e70 21800 200e73 218e97 9930 304e98 676e85 13900 112e86 150e64 13100 100e65 424e79 10900 462e81 281e70 10000 841e72 514e84 17800 288e86 aotree IJGPSS hmetis cid10Z RSD Zlb 106e35 7640 397e37 133e65 14300 717e68 521e75 19200 404e77 142e68 11400 351e71 352e97 14700 143e98 722e84 9510 260e85 420e64 13900 170e65 485e77 22200 372e80 233e67 10100 408e70 403e83 21600 448e86 aograph IJGPSS hmetis cid10Z RSD Zlb 960e32 15900 237e33 528e62 4700 681e63 268e73 6780 913e75 830e66 20000 348e68 352e92 9920 712e95 835e78 8910 865e79 130e64 2810 332e65 662e76 8760 491e77 332e66 1850 906e67 205e80 11000 190e81 estimates ANDOR tree ANDOR graph estimates We experimented orderings based minﬁll second based hmetis constructing pseudo trees It known observe minﬁll superior generating small treewidth pseudo trees hmetis superior generating small height pseudo trees We orderings comparable terms accuracy estimation yield different proposal distributions relative accuracy compared posterior distribution understood point We leave issue future research 7 Discussion related work 71 Relation graphbased variance reduction schemes The work presented related work Hernandez Moral 34 Kjærulff 35 Dawid et al 36 perform samplingbased inference junction tree organize samples way virtual samples generated The main idea papers perform message passing junction tree substituting messages hard compute exactly samplingbased approximations Kjærulff 35 Dawid et al 36 use Gibbs sampling Hernandez Moral 34 use importance sampling approximate messages Another related work Bouckaert et al 37 use search trees implement stratiﬁed sampling eﬃciently Similar recent works RaoBlackwellized sampling 383915 variance reduction achieved junction tree based sampling schemes exact computations dictated RaoBlackwell theorem ANDOR estimation based 70 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Table 8 Results random coding networks UAI 2006 evaluation Table showing average sample means cid10Z relative standard deviation sample means RSD lower bound cid10Zlb weighted counts 99 conﬁdence ﬁve runs ortreeIJGPSSminﬁll aotreeIJGPSS minﬁll aographIJGPSSminﬁll ortreeIJGPSShmetis aotreeIJGPSShmetis aographIJGPSShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Instance n k f e c BN_126 512 2 512 256 384 BN_127 512 2 512 256 384 BN_128 512 2 512 256 384 BN_129 512 2 512 256 384 BN_130 512 2 512 256 384 BN_131 512 2 512 256 384 BN_132 512 2 512 256 384 BN_133 512 2 512 256 384 BN_134 512 2 512 256 384 Z h w 55 65 54 71 49 68 53 66 53 63 53 66 51 64 55 67 55 64 ortree IJGPSS minﬁll cid10Z RSD Zlb 207e56 064 815e57 304e58 765 112e58 486e48 024 193e48 418e62 116 164e62 378e58 087 149e58 230e54 054 908e55 658e65 1940 200e65 235e54 095 926e55 610e57 020 243e57 aotree IJGPSS minﬁll cid10Z RSD Zlb 205e56 076 808e57 306e58 088 120e58 487e48 017 193e48 419e62 073 165e62 379e58 044 150e58 229e54 057 905e55 690e65 484 260e65 237e54 244 928e55 612e57 028 243e57 aograph IJGPSS minﬁll cid10Z RSD Zlb 205e56 065 809e57 307e58 057 121e58 487e48 017 193e48 419e62 079 165e62 379e58 054 150e58 229e54 032 908e55 695e65 442 260e65 237e54 232 929e55 612e57 032 243e57 h w 54 66 57 66 52 61 52 66 52 63 51 63 51 66 55 65 53 62 ortree IJGPSS hmetis cid10Z RSD Zlb 205e56 057 808e57 306e58 406 116e58 487e48 015 194e48 566e62 3110 152e62 379e58 070 150e58 230e54 098 901e55 100e64 7970 183e65 235e54 106 924e55 611e57 053 242e57 aotree IJGPSS hmetis cid10Z RSD Zlb 204e56 039 810e57 307e58 176 120e58 487e48 008 194e48 419e62 908 156e62 379e58 047 150e58 230e54 093 908e55 221e64 16100 204e65 236e54 088 934e55 611e57 026 242e57 aograph IJGPSS hmetis cid10Z RSD Zlb 204e56 019 811e57 306e58 123 120e58 487e48 008 194e48 422e62 433 157e62 380e58 039 150e58 231e54 082 911e55 720e65 2510 212e65 236e54 117 931e55 611e57 026 242e57 fundamentally different principle achieves variance reduction conditional independence derive virtual samples In fact Gogate 16 Gogate Dechter 40 variance reduction RaoBlackwellization orthogonal achieved ANDORbased estimation combined achieve variance reduction 72 Hoeffdings U statistics ANDORestimates closely related cross match estimates 41 based Hoeffdings U statistics To derive crossmatch estimates original function set variables divided marginal functions deﬁned subset variables Then marginal function sampled independently crossmatch sample mean derived considering possible combinations samples For example k marginal functions m samples taken function cross match sample mean computed mk combinations It shown Kong et al 41 cross match sample mean lower variance conventional sample mean similar work The caveat cross match estimates requires exponentially time O mk compute estimates compared O m conventional estimates making direct application infeasible large values k So authors suggest resampling possible O mk samples hope estimates based resampled samples smaller variance conventional Unlike cross match estimates complex ANDOR estimates w treewidth compared conventional estimates require extra resampling step times expensive time wise w V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 71 Table 9 Results 4coloring instances generated Joseph Culbersons ﬂat graph coloring generator Table showing average sample means cid10Z relative standard deviation sample means RSD lower bound cid10Zlb weighted counts 99 conﬁdence ﬁve runs ortree IJGPSSminﬁll aotreeIJGPSSminﬁll aographIJGPSSminﬁll ortreeIJGPSShmetis aotreeIJGPSShmetis aographIJGPSShmetis 1 hour CPU time Exact minﬁll ordering hmetis ordering Instance n k f e c 4coloring1 400 2 2026 0 2026 4coloring2 400 2 2205 0 2205 4coloring3 800 2 4065 0 4065 4coloring4 800 2 4419 0 4419 4coloring5 1200 2 6455 0 6455 4coloring6 1200 2 6641 0 6641 Z h w 71 87 95 113 144 171 196 225 260 301 290 321 ortree IJGPSS minﬁll cid10Z RSD Zlb 116e37 2160 366e36 141e30 5840 395e29 313e72 3720 528e71 797e63 10600 645e62 442e98 7960 660e97 166e90 12600 111e88 aotree IJGPSS minﬁll cid10Z RSD Zlb 111e37 1730 372e36 117e30 3580 350e29 343e72 3080 900e71 822e63 12700 776e62 464e99 20300 665e97 107e90 10800 222e88 aograph IJGPSS minﬁll cid10Z RSD Zlb 205e37 2450 577e36 292e30 5770 493e29 196e73 7600 364e72 584e64 3180 177e64 160e101 19600 185e99 856e91 12700 652e90 h w 67 90 84 105 129 160 165 196 232 272 264 307 ortree IJGPSS hmetis cid10Z RSD Zlb 144e37 2630 385e36 129e30 5750 267e29 430e72 6010 252e71 208e63 5650 206e62 597e97 12000 537e96 257e89 10900 282e88 aotree IJGPSS hmetis cid10Z RSD Zlb 183e37 3640 451e36 363e30 15300 178e29 491e72 6900 289e71 246e63 5060 445e62 510e97 5780 426e96 259e89 8060 360e88 aograph IJGPSS hmetis cid10Z RSD Zlb 223e37 2410 651e36 155e30 1800 453e29 112e73 5700 264e72 286e64 7630 375e63 404e99 10000 163e98 153e91 13900 118e90 73 Problem large sample sizes Given space complexity computing ANDOR sample graph mean marginal probabilities O nN reader think samples drawn algorithms run memory One perform multistage adaptive sampling circumvent problem Here stage stop storing samples pre speciﬁed memory limit reached Then ANDOR sample graph mean computed stored samples samples discarded repeating process stipulated time bound expires samples drawn The ﬁnal sample mean simply average sample means computed stage It obvious ﬁnal sample mean smaller variance OR sample tree mean accurate compared ANDOR sample graph mean 74 Impact determinism context speciﬁc independence ANDOR sampling based simple viewpoint generated samples This especially use ful graphical model structural features determinism context speciﬁc independence CSI 42 It easy problem generating sample nonzero weight useful sample graphical model deterministic dependencies equivalent problem ﬁnding model satisﬁability formula 43 Because problem NPcomplete useful samples generated Many schemes proposed literature generating samples hard graphical models probabilistic deterministic relation ships 4423451746 Out demonstrated prior work 19 SampleSearch currently best performing alternative In paper showed deterministic networks linkage analysis domain ANDOR sam ple graph mean computed samples generated SampleSearch substantially accurate OR sample tree mean This shows power ANDOR estimation hard deterministic graphical models Another advantage ANDOR sample graph mean networks advantage implicit conditional independen cies elucidated primal graph 4748 These implicit dependencies increase virtual sample size resulting improved accuracy 8 Conclusion The primary contribution paper viewing importance samplingbased estimation context ANDOR search spaces graphical models 6 Speciﬁcally viewed sampling partial exploration ANDOR search 72 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 space called ANDOR sample tree deﬁned process computing unbiased sample mean ANDOR sample tree We proved conventional sample mean running average equal computing ANDOR sample mean OR sample tree impervious problem decomposition Arranging samples ANDOR sample tree sensitive problem decomposition yields virtual samples better sample mean smaller variance Since ANDOR sample tree mean time complexity slightly space overhead OR sample tree mean preferred We extended ANDOR sample tree mean ANDOR sample graph mean utilizes problem decomposition merging identical subtrees The ANDOR sample graph yields virtual samples ANDOR sample tree reduces variance However computing ANDOR sample graph mean requires factor O w time O N times space introduces time space versus accuracy tradeoffs We focused empirical investigation task computing probability evidence Bayesian network partition function Markov network The main aim evaluation compare impact exploiting varying levels graph decompositions OR tree b ANDOR tree c ANDOR graph accuracy sample mean Our results demonstrated conclusively cases scheme exploits decomposition ANDOR sample graph mean consistently superior Our results ANDOR sample tree mean slightly better terms accuracy OR sample tree mean Future work The ANDOR sampling framework leaves plenty avenues future work For instance ANDOR sampling RaoBlackwellization orthogonal nature combination needs explored Some initial results combination presented ﬁrst authors thesis 16 recent conference paper 40 A second line future work based observation ANDOR sampling framework utilizes conditional independencies partially uncovered primal graph graphical model It known primal graph captures subset conditional independencies New unknown independencies elucidated sampling ANDOR space ANDOR sampling theory know sampling error decrease utilize How guide sampling uncover unknown independencies open problem A line future research develop ANDOR estimators sampling techniques Gibbs sampling 49 stratiﬁed sampling 37 A fourth line future work developing memory eﬃcient algorithms estimating posterior marginal probabilities As discussed Section 33 unlike conventional OR tree estimator requires O n space ANDOR estimator proposed paper memory intensive complexity O nN requires storing ANDOR sample tree memory Acknowledgements This work supported NSF award numbers IIS1065618 IIS0331707 IIS0412854 IIS 0713118 NIH grant R01HG004175 The authors like thank anonymous reviewers associate editor valuable comments suggestions substantially helped improve earlier drafts Appendix A Algorithm interleaving sampling estimation Algorithm 3 Interleaved ANDOR tree importance sampling IAOTS Input A graphical mode G cid5X D Fcid6 pseudo tree T X E proposal distribution deﬁned relative T Q X cid6 n i1 Q Xi contextT Xi integer N 0 variable Xi assignment x Output The value OR node corresponding Xi ANDOR sample tree deﬁned relative G x T Q 1 v Xi 0 2 Generate N samples Xi Q Xi xcontextT Xi 3 Let xi1 xid set values Xi sampled N j 0 times 4 j 1 d 5 6 Xi leaf node T vxi j 1 7 8 9 10 11 cid8 x xi j x Let C1 C p set child nodes Xi T cid6 cid8 vxi j k1 IAOTSG T Q N j Ck x p v Xi v Xi B T Xi xi j x Q xi j xcontextT Xi N j vxi j 12 return v Xi N In section present recursive algorithm interleaves sampling ANDORbased estimation Al gorithm 3 The algorithm takes input graphical model G pseudo tree T proposal distribution Q X cid6 i1 Q XicontextT Xi variable Xi sampled integer N denotes number times Xi sam n pled current assignment x The algorithm returns value OR node corresponding Xi ANDOR V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 73 sample tree deﬁned relative G x T Q First given current assignment x algorithm generates N samples Xi Q XixcontextT Xi Then forloop computes numerator value OR node Deﬁnition 15 corresponding Xi iterating sampled values xi j Xi computing values vxi j The algorithm com putes vxi j follows If Xi leaf node T deﬁnition Deﬁnition 15 vxi j equals 1 If Xi leaf node deﬁnition vxi j equals product values child OR nodes child OR node corresponding child node Ck Xi T The value OR node corresponding Ck turn computed calling algorithm recursively Finally algorithm returns value OR node corresponding Xi It easy Theorem 9 Given graphical model G cid5X D Fcid6 pseudo tree T proposal distribution Q X i1 Q XicontextT Xi integer N Algorithm IAOTSG T Q N X1 correctly computes ANDOR sample tree mean X1 root node T cid6 n Appendix B Proofs Proof Theorem 1 We prove induction value OR node n unbiased estimate conditional expectation subproblem rooted n conditioned assignment root n Consider expression weighted counts Z cid4 mcid3 xX i1 F ix B1 Let T X E pseudo tree pathT Xi set variables path root node Xi note pathT Xi include Xi T B T Xi bucket function Deﬁnition 13 Xi wrt T For assignment x ncid3 i1 B T Xi xi xpathT Xi mcid3 i1 F ix Recall xpathT Xi projection assignment x subset pathT Xi X Substituting Eq B2 Eq B1 cid4 ncid3 Z B T Xi xi xpathT Xi B2 B3 xX i1 cid6 n Let Q x express Q i1 Q ixiyi proposal distribution Yi contextT Xi Because contextT Xi pathT Xi Q x ncid3 i1 Q ixixYi ncid3 i1 Q ixixpathT Xi We express Z Eq B3 terms Q Z cid4 ncid3 xX i1 B T Xi xpathT Xi Q ixixpathT Xi Q ixixpathT Xi B4 B5 Using notation xi x1 xi xipathT X j projection xi pathT X j migrating functions left summation variables reference rewrite Eq B5 Z cid4 x1 X1 B T X1 x1 Q 1x1 Q 1x1 cid4 xi Xi cid4 xn Xn B T Xi xi xi1pathT Xi Q ixixi1pathT Xi Q ixixi1pathT Xi B T Xi xi xn1pathT Xn Q nxnxn1pathT Xn Q nxnxn1pathT Xn B6 74 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Using deﬁnition expectation conditional expectation rewrite Eq B6 cid13 Z Ex cid13 Ex B T X1 x1 Q 1x1 cid13 B T Xn xn xn1pathT Xn Ex Q nxnxn1pathT Xn B T Xi xi xi1pathT Xi Q ixixi1pathT Xi cid14 cid8 cid8 cid8 xn1pathT Xn cid8 cid8 cid8 xi1pathT Xi cid14 cid14 B7 Let chi Xi set children Xi pseudo tree T let denote component conditional expectation node Xi assignment xi1pathT Xi V Xi Xixi1pathT Xi V Xi Xixi1pathT Xi recursively deﬁned follows cid13 V Xi Xixi1pathT Xi Ex B T Xi xi xi1pathT Xi Q ixixi1pathT Xi cid3 V X j X jxi xi1pathT Xi cid14 cid8 cid8 cid8 xi1pathT Xi X j chi Xi It easy Z equals V X1 X1 cid13 Z Ex B T X1 X1 Q 1X1 cid3 cid14 V X j X jx1 V X1 X1 X j chi X1 B8 B9 We derive unbiased estimate V Xi Xixi1pathT Xi Assume X j chi Xi T unbiased estimate V X j X jxi xi1pathT Xi denoted cid10v X j X jxi xi1pathT Xi Assume given xi1pathT Xi Q Xixi1pathT Xi By replacing conditional expectation sample average generated N samples x1 following unbiased estimate V Xi Xixi1pathT Xi xN cid10v Xi Xixi1pathT Xi 1 N Ncid4 a1 B T Xi xa Q ixa xi1pathT Xi xi1pathT Xi cid3 cid7 X j cid10v X j cid8 cid8xa xi1pathT Xi cid9 X j chi Xi B10 Assume domain Xi xi1 xik Also assume value xi j sampled Ni j times By collecting cid2 k a1 Nia rewrite Eq B10 samples value xi j generated substituting N cid10v Xi Xixi1pathT Xi cid2 k a1 Nia xiaxi1pathT Xi B T Xi Q xiaxi1pathT Xi cid6 X j chi Xi cid2 k a1 Nia cid10v X j X jxia xi1pathT Xi B11 Next given ANDOR sample tree ψT S samples S cid10v Xi Xixi1pathT Xi derived value OR node n ψT S labeled Xi Aπn xi1pathT Xi equal cid10v Xi Xixi1pathT Xi Let denote kth child AND node mk By deﬁnition frequencies weights arcs n ma given n ma Nia wn ma B T Xi xia Aπn Q ixia Aπn B T Xi xia xi1pathT Xi Q ixiaxi1pathT Xi By deﬁnition value AND node ma given vma cid3 cid9 cid7 n cid8 v ncid8chima Similarly deﬁnition value OR node n given cid2 k cid2 k vn a1 n ma wn ma vma cid2 k a1 n ma cid6 a1 n ma wn ma cid2 k a1 n ma ncid8chima vn cid8 B12 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 75 Substituting expressions n ma wn ma Eq B12 vn cid2 k a1 Nia cid6 xiaxi1pathT Xi B T Xi Q xiaxi1pathT Xi cid2 k a1 Nia ncid8chima vn cid8 B13 Assuming vn yielding vn cid10v Xi Xixi1pathT Xi Namely proved value child OR node n OR node n equal unbiased estimate conditional expectation subproblem rooted n value OR node n unbiased estimate conditional expectation subproblem rooted n cid8 cid10v X j X jxia xi1pathT Xi righthand sides Eqs B11 B13 equal child AND node cid8 cid8 Since result true OR node value root OR node equal unbiased estimate Z V X1 X1 wanted prove cid2 Proof Theorem 2 We prove theorem induction nodes pseudo tree T X E Base Case Here prove statement theorem true n 1 Assume T variable X1 obtained topological linearization T coincides T Given samples 1 generated proposal distribution Q 1 X1 bucket function B T cid8 X1 X1 Deﬁnition 13 In case chain pseudo tree T S x1 conventional importance sampling estimate 1 xN cid8 cid10Z 1 N Ncid4 i1 B T cid8 X1 xi 1 Q 1xi 1 B14 Let x11 x1k domain X1 N1 j number times value x1 j appears S collecting samples value x1 j generated substituting N Eq B14 cid2 k a1 N1a cid2 k a1 N1a cid8X1 Q 1x1a cid10Z x1a B T j 1 k Then cid2 k a1 N1a rewrite B15 cid8 Since T node OR sample tree based T OR node denoted n Let m1 mk child AND nodes n By deﬁnition value leaf AND nodes 1 weight frequency arcs n ma given n ma N1a wn ma B T cid8 X1 x1a Q 1x1a Also deﬁnition value OR node n given vn cid2 k a1 n mawn ma cid2 k a1 n ma x1a cid8X1 B T Q 1x1a cid2 k a1 N1a cid2 k a1 N1a B16 From Eqs B15 B16 cid10Z vn proves base case Next prove induction case Induction case In case assume statement theorem true n variables X1 Xn prove true n 1 variables X1 Xn1 Consider pseudo tree T n 1 variables Xn1 root Let T cid8 chain pseudo tree corresponding topological linearization T By deﬁnition T T root node Xn1 cid8 n xN n1 generated proposal distribution Q Xn Xn1 conventional n x1 Given samples S x1 n1 xN importance sampling estimate given n1 j1 B T cid8 X j xi cid6 n1 j1 Q jxi n xi n xi cid10Z 1 N n1 n1 Ncid4 cid6 i1 Let Xn1 k values domain given xn11 xn1k Nn1 j number times value xn1 j appears S Let Sxn1 j S subset samples mention value xn1 j Then collecting samples value xn1 j generated substituting N xk nxn1a xn1a cid2 k a1 Nn1a rewrite Eq B17 cid2 k a1 Nn1a cid6 n cid8X j j1 B T cid6 n j1 Q j xk n cid8Xn1 Q n1xn1a xkSxn1 j 1 Nn1a xn1a cid2 B T cid11 cid12 cid10Z cid2 k a1 Nn1a B17 B18 76 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 Without loss generality let X1 child Xn1 T From induction case assumption quantity brackets Eq B18 equal value OR node labeled X1 given xn1a Let OR node denoted ra We rewrite Eq B18 cid8 cid10Z cid2 k a1 Nn1a cid2 k a1 Nn1a xn1a cid8Xn1 B T Q n1xn1a vra B19 Consider root OR node denoted r OR sample tree labeled Xn1 r k child AND nodes m1 mk turn child OR node By deﬁnition value AND node product values child nodes Since AND node ma 1 k child OR node denoted ra OR sample tree value ma equal value ra Namely vma vra By deﬁnition weights arcs r ma 1 k given r ma Nn1a wr ma B T cid8 Xn1 xn1a Q n1xn1a By deﬁnition value root OR node r denoted vr given vr cid2 k a1 r ma wr ma vma cid2 k a1 r ma cid2 k a1 Nn1a cid2 k a1 Nn1a xn1a cid8Xn1 B T Q n1xn1a vra B20 From Eqs B19 B20 cid10Z vr proves induction case Therefore principle induc tion proof follows cid2 Proof Theorem 3 Because N samples generated number nodes ANDOR sample tree bounded O nN Because node ANDOR sample tree processed value computation phase Algorithm 2 node processed constant time overall time complexity O nN We perform depth ﬁrst search traversal ANDOR sample tree build ﬂy In case store current search path maximum size bounded depth h pseudo tree Therefore space complexity O h cid2 Proof Theorem 6 Given pseudo tree T set samples S ANDOR sample graph value OR node denoted nAOG labeled Xi computed subset samples assignment contextT Xi Xi ANDOR sample tree value corresponding OR node nAOT computed subset samples assignment variables path root Xi denoted pathT Xi Because contextT Xi pathT Xi value nAOG based larger equal number samples compared value nAOT Because larger virtual sample size variance value nAOG equal variance value nAOT cid2 Proof Theorem 7 Let X j child node Xi T Given N samples maximum context size w edges emanating AND nodes corresponding Xi OR nodes labeled X j bounded O N w O nN w number ANDOR sample graph Since edge visited value computation phase overall time complexity To store N samples takes O nN space space complexity O nN cid2 References 1 AW Marshall The use multistage sampling schemes Monte Carlo computations Symposium Monte Carlo Methods 1956 pp 123140 2 RY Rubinstein Simulation Monte Carlo Method John Wiley Sons Inc 1981 3 J Liu Monte Carlo Strategies Scientiﬁc Computing SpringerVerlag New York 2001 4 A Darwiche Recursive conditioning Artiﬁcial Intelligence 126 12 2001 541 5 F Bacchus S Dalmao T Pitassi Value elimination Bayesian inference backtracking search Proceedings Nineteenth Conference Uncertainty Artiﬁcial Intelligence 2003 pp 2028 6 R Dechter R Mateescu ANDOR search spaces graphical models Artiﬁcial Intelligence 171 23 2007 73106 7 V Gogate R Dechter ANDOR importance sampling Twenty Third Conference Uncertainty Artiﬁcial Intelligence 2008 pp 212219 8 V Gogate R Dechter Approximate solution sampling counting ANDOR spaces Proceedings Fourteenth International Conference Principles Practice Constraint Programming 2008 pp 534538 9 NJ Nilsson Principles Artiﬁcial Intelligence Morgan Kaufmann 1982 10 J Geweke Bayesian inference econometric models Monte Carlo integration Econometrica 57 6 1989 13171339 11 J Cheng MJ Druzdzel AISBN An adaptive importance sampling algorithm evidential reasoning large Bayesian networks Journal Artiﬁcial Intelligence Research 13 2000 155188 V Gogate R Dechter Artiﬁcial Intelligence 184185 2012 3877 77 12 J Pearl Probabilistic Reasoning Intelligent Systems Morgan Kaufmann 1988 13 A Darwiche A differential approach inference Bayesian networks Journal ACM 50 2003 280305 14 LA Goodman On exact variance products Journal American Statistical Association 55 292 1960 708713 15 V Gogate R Dechter Approximate inference algorithms hybrid Bayesian networks discrete constraints Proceedings Twenty First Annual Conference Uncertainty Artiﬁcial Intelligence 2005 pp 209216 16 V Gogate Sampling algorithms probabilistic graphical models determinism PhD thesis Computer Science University California Irvine USA 2009 17 V Gogate R Dechter SampleSearch A scheme searches consistent samples Proceedings Eleventh Conference Artiﬁcial Intelligence Statistics 2007 pp 147154 18 V Gogate R Dechter Approximate counting sampling backtrackfree search space Proceedings Twenty Second Conference Artiﬁcial Intelligence 2007 pp 198203 19 V Gogate R Dechter SampleSearch Importance sampling presence determinism Artiﬁcial Intelligence 175 2 2011 694729 20 R Dechter K Kask R Mateescu Iterative join graph propagation Proceedings Eighteenth Conference Uncertainty Artiﬁcial Intelligence 2002 pp 128136 21 KP Murphy Y Weiss MI Jordan Loopy belief propagation approximate inference An empirical study Proceedings Fifteenth Conference Uncertainty Artiﬁcial Intelligence 1999 pp 467475 22 JS Yedidia WT Freeman Y Weiss Free constructing energy approximations generalized belief propagation algorithms IEEE Transactions Information Theory 51 2004 22822312 23 C Yuan MJ Druzdzel Importance sampling algorithms Bayesian networks Principles performance Mathematical Computer Model ing 43 910 2006 11891207 24 R Mateescu K Kask V Gogate R Dechter Joingraph propagation algorithms Journal Artiﬁcial Intelligence Research 37 2010 279328 25 R Marinescu ANDOR search strategies combinatorial optimization graphical models PhD thesis Computer Science University California Irvine USA 2008 26 J Bilmes R Dechter Evaluation probabilistic inference systems UAI06 Available online httpsslieewashingtonedubilmes uai06InferenceEvaluation 2006 27 T Sang P Beame HA Kautz Performing Bayesian inference weighted model counting Proceedings The Twentieth National Conference Artiﬁcial Intelligence 2005 pp 475482 28 M Fishelson D Geiger Optimizing exact genetic linkage computations Proceedings Seventh Annual International Conference Research Computational Molecular Biology 2003 pp 114121 29 J Ott Analysis Human Genetic Linkage The Johns Hopkins University Press Baltimore Maryland 1999 30 A Darwiche R Dechter A Choi V Gogate L Otten Results probabilistic inference evaluation UAI08 Available online httpgraphmod icsucieduuai08EvaluationReport 2008 31 V Gogate B Bidyuk R Dechter Studies lower bounding probability evidence Markov inequality Proceedings Twenty Third Conference Uncertainty Artiﬁcial Intelligence 2007 pp 141148 32 CP Gomes J Hoffmann A Sabharwal B Selman From sampling model counting Proceedings Twentieth International Joint Conference Artiﬁcial Intelligence 2007 pp 22932299 33 K Kask R Dechter A general scheme automatic generation search heuristics speciﬁcation dependencies Artiﬁcial Intelligence 129 12 2001 91131 34 LD Hernandez S Moral Mixing exact importance sampling propagation algorithms dependence graphs International Journal Approximate Reasoning 12 8 1995 553576 35 U Kjærulff HUGS Combining exact inference Gibbs sampling junction trees Proceedings Eleventh Conference Uncertainty Artiﬁcial Intelligence 1995 pp 368375 36 AP Dawid U Kjaerulff SL Lauritzen Hybrid Propagation Junction Trees Advances Intelligent Computing IPMU ISBN 3540601163 1994 pp 8597 37 RR Bouckaert E Castillo JM Gutiérrez A modiﬁed simulation scheme inference Bayesian networks International Journal Approximate Reasoning 14 1 1996 5580 38 B Bidyuk R Dechter Cutset Sampling Bayesian Networks Journal Artiﬁcial Intelligence Research 28 2007 148 39 MA Paskin Sample propagation Advances Neural Information Processing Systems 2003 pp 425432 40 V Gogate R Dechter On combining graphbased variance reduction schemes Proceedings Thirteenth International Conference Artiﬁcial Intelligence Statistics 2010 pp 257264 41 Augustine Kong Jun S Liu Wing Hung Wong The properties crossmatch estimate split sampling The Annals Statistics 25 6 1997 24102432 ISSN 00905364 42 C Boutilier N Friedman M Goldszmidt D Koller Contextspeciﬁc independence Bayesian networks Proceedings Twelfth Annual Confer ence Uncertainty Artiﬁcial Intelligence 1996 pp 115123 43 GF Cooper The computational complexity probabilistic inference Bayesian belief networks Artiﬁcial Intelligence 42 23 1990 393405 44 C Yuan MJ Druzdzel An importance sampling algorithm based evidence prepropagation Proceedings Nineteenth Conference Uncer tainty Artiﬁcial Intelligence 2003 pp 624631 45 S Moral A Salmerón Dynamic importance sampling Bayesian networks based probability trees International Journal Approximate Reason ing 38 3 2005 245261 46 H Yu R Engelen Arc refractor methods adaptive importance sampling large Bayesian networks evidential reasoning International Journal Approximate Reasoning 51 7 2010 800819 47 M Chavira A Darwiche On probabilistic inference weighted model counting Artiﬁcial Intelligence 172 67 2008 772799 48 V Gogate P Domingos Formulabased probabilistic inference Proceedings TwentySixth Conference Uncertainty Artiﬁcial Intelligence 2010 pp 210219 49 S Geman D Geman Stochastic relaxations Gibbs distributions Bayesian restoration images IEEE Transaction Pattern analysis Machine Intelligence PAMI6 6 1984 721742