Artiﬁcial Intelligence 197 2013 3955 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Transfer learning heterogeneous collaborative ﬁltering domains Weike Pan Qiang Yang Department Computer Science Engineering Hong Kong University Science Technology Clearwater Bay Kowloon Hong Kong r t c l e n f o b s t r c t Article history Received 6 December 2010 Received revised form 6 December 2012 Accepted 12 January 2013 Available online 11 February 2013 Keywords Transfer learning Collaborative ﬁltering Missing ratings A major challenge collaborative ﬁltering CF techniques recommender systems data sparsity caused missing noisy ratings This problem CF domains ratings expressed numerically 5star grades We assume 5star ratings unordered bins instead ordinal relative preferences We observe lack information numerical ratings additional auxiliary data form binary ratings This especially true given users easily express preferences expressed likes dislikes items In paper explore use binary auxiliary preference data help reduce impact data sparsity CF domains expressed numerical ratings We solve problem transferring rating knowledge auxiliary data source binary form likes dislikes target numerical rating matrix In particular solution model numerical ratings ratings expressed like dislike principled way We present novel framework Transfer Collective Factorization TCF construct shared latent space collectively learn datadependent effect separately A major advantage TCF approach previous bilinear method collective matrix factorization able capture data dependent effect sharing dataindependent knowledge This allows increase overall quality knowledge transfer We present extensive experimental results demonstrate effectiveness TCF sparsity levels improvements approach compared stateoftheart methods 2013 Elsevier BV All rights reserved 1 Introduction Data sparsity major challenge collaborative ﬁltering 23943 Sparsity refers fact observed ratings 5star grades useritem rating matrix overﬁtting easily happen use prediction model missing values test data However observe auxiliary data form like dislike easily obtained favoreddisfavored data Moviepilot1 Qiyi2 digbury data Tudou3 loveban data Lastfm4 Want seeNot interested data Flixster5 It convenient users express preferences instead numerical ratings The question ask paper Corresponding author Email addresses weikepcseusthk W Pan qyangcseusthk Q Yang 1 httpwwwmoviepilotde 2 httpwwwqiyicom 3 httpwwwtudoucom 4 httpwwwlastfm 5 httpwwwﬂixstercom 00043702 matter 2013 Elsevier BV All rights reserved httpdxdoiorg101016jartint201301003 40 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 Table 1 Matrix illustration related work transfer learning collaborative ﬁltering Note SoRec CMF CBT RMGM applied general problem settings matrices types alignments Methods Training data Auxiliary data SoRec user 39 CMF item 52 CBT aligned 31 RMGM aligned 32 R UVT Knowledge sharing U U1 Value domain U V U1 V1 DR DR U V U Rnd V Rmd R UVT Knowledge sharing V V2 Value domain U V U2 V2 DR DR U V U Rnd V Rmd R1 U1VT 1 R2 U2VT 2 R UBVT Knowledge sharing B B3 Value domain U V U3 V3 D01 D01 U V U 0 1nd U1 1 V 0 1md V1 1 R3 U3B3VT 3 R UBVT Knowledge sharing B B3 Value domain U V U3 V3 D01 D01 U V U 0 1nd U1 1 V 0 1md V1 1 R3 U3B3VT 3 advantage auxiliary knowledge form binary ratings alleviate sparsity problem numerical ratings build ratingprediction model To best knowledge previous work answered question jointly model target data numerical ratings auxiliary data binary ratings There prior works numerical ratings implicit data rated 2835 purchased 57 help boost prediction performance Among previous works Koren 28 uses implicit data rated offsets factorization model Liu et al 35 adapt collective matrix factorization CMF approach 52 integrate implicit data rated Zhang Nie 57 convert implicit data simulated purchases userbrand matrix userside meta data representing brand loyalty useritem matrix purchased However previous works consider use auxiliary data form like dislike type binary ratings collaborative ﬁltering transfer learning framework Most existing transfer learning methods collaborative ﬁltering consider auxiliary data perspectives includ ing userside transfer 3911583855 itemside transfer 52 knowledgetransfer related aligned data 31 32 We illustrate ideas knowledge sharing matrix factorization view shown Table 1 We representative methods 39523132 Table 1 details starting nontransfer learning method probabilistic matrix factorization PMF 47 Probabilistic matrix factorization The PMF 47 latent factorization model LFM 4 seeks appropriate lowrank U Rnd V Rmd approximation R UVT missing value predicted ˆrui U u V T userspeciﬁc itemspeciﬁc latent feature matrices respectively The optimization problem PMF follows 474 EIU V αRU V min UV 1 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 EIU V 1 cid2 2 cid2 cid2 n u1 m i1 yuirui U u V T 2 1 2 cid5Y cid6 R UVT cid52 F loss function RU V 1 2 cid2 n u1 m i1 cid5V icid52 1 2 cid5Ucid52 F cid5Vcid52 F regularization term avoid overﬁtting 41 cid5U ucid52 Social recommendation SoRec 39 proposed alternatively factorize target rating matrix R userside social network matrix R1 constraint sharing userspeciﬁc latent feature matrix U U1 Table 1 The objective function formalized follows 39 EIU V EIU V1 αRU V V1 min UV1U 2 U V DR RU V V1 1 2 cid5Ucid52 F cid5Vcid52 F cid5V1cid52 F regularization term latent variables Collective matrix factorization CMF 52 proposed alternatively factorize target rating matrix R itemside content matrix R2 constraint sharing itemspeciﬁc latent feature matrix V V2 Table 1 This approach similar SoRec 39 different auxiliary data The optimization problem CMF stated follows 52 EIU V EIU2 V αRU V U2 min UVU2 3 U V DR RU V U2 1 2 cid5Ucid52 F cid5Vcid52 F cid5U2cid52 F regularization term avoid overﬁtting Codebook transfer The CBT 31 method consists codebook construction expansion steps It achieves knowledge transfer assumption auxiliary target data share common clusterlevel rating pattern B B3 Table 1 1 Codebook construction Assume U3 V3 D01 userspeciﬁc itemspeciﬁc membership indicator matrices auxiliary rating matrix R3 obtained coclustering algorithms NMF 19 The constructed 3 R3V3kcid3 denotes summation ratings codebook represented B3 UT users user cluster k items item cluster cid3 UT 3 R3 0V3kcid3 denotes number ratings users user cluster k items item cluster cid3 elementwise division cid7 resembles idea normalization B3kcid3 average rating users user cluster k items item cluster cid3 3 R3 0V3 31 UT 3 R3V3 cid7 UT 2 Codebook expansion The codebook expansion problem formalized follows 31 EBU V st U V D01 min UV 4 cid5Y cid6 R UBVT cid52 EBU V 1 F Bregularized square loss function B B3 codebook constructed 2 auxiliary data R3 In 31 alternating greedysearch algorithm proposed solve combinatorial optimization problem Eq 4 choices U uk 1 V icid3 1 select entry located k cid3 Thus predicted rating ˆrui UBVT ui Bkcid3 average rating users user B UBVT ui U uBV T cluster k items item cluster cid3 auxiliary data Ratingmatrix generative model RMGM 32 derived extended FMM generative model 50 write matrix factorization manner min UVBU3V3 EBU V EBU3 V3 st U V U3 V3 D01 5 EBU V Bregularized loss function given Eq 4 We RMGM different CBT learns U V U3 V3 alternatively relaxes hard membership requirement imposed indicator matrix U 0 1nd A soft indicator matrix RMGM 32 U 0 1nd In paper consider situation auxiliary data following information aligned users items target rating matrix auxiliary binary rating matrix This assumption gives precise information mapping auxiliary target data lead higher performance having knowledge We illustrate idea assumptions matrices Table 2 problem setting proposed solution novel different previous ones shown Table 1 We discuss novelty sequel Our idea extends ideas previous conference papers topic 4342 extensively ex tended Compared preliminary works extended following aspects First included new analysis transfer learning methods collaborative ﬁltering perspective matrix factorization Section 1 Sec ond provided detailed derivations equations Section 3 Third included experimental results reported Section 4 Finally added related works associated discussions given Section 5 42 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 Table 2 Matrix illustration Transfer Collective Factorization Variants TCF Training data Auxiliary data CMTF frontal R UBVT Knowledge sharing U U V V Value domain U V U V DR DR U V U Rnd V Rmd CSVD frontal R UBVT Knowledge sharing U U V V Value domain U V U V D D U V U Rnd UT U I V Rmd VT V I R U B VT R U B VT The organization paper follows We formal deﬁnition problem Section 2 solution Section 3 We present experimental results realworld data sets Section 4 discuss related work Section 5 Finally concluding remarks future works Section 6 2 Heterogeneous collaborative ﬁltering problems 21 Problem deﬁnition In target data useritem numerical rating matrix R ruinm 1 2 3 4 5 nm q observed ratings question mark denotes missing value unobserved value Note ob served rating values R considered unordered bins limited 5star grades instead real numbers We use indicator matrix Y yuinm 0 1nm denote entry u observed yui 1 yui 0 ui yui q Similarly auxiliary data useritem binary rating matrix R ruinm 0 1 nm q observations value denotes observed like value zero denotes observed dislike value The question mark denotes missing value Similar target data corresponding indicator matrix Y yuinm 0 1nm yui q Note oneone mapping users items R R Our goal predict missing values R transferring rating knowledge R Note binary ratings different implicit data 283557 represented 1 nm implicit data corresponds positive observations cid2 cid2 ui 22 Challenges Our problem setting novel challenging In particular enumerate following challenges problem setting Fig 1 1 How use existing correspondences users items domains given relationships important serve bridge domains Some previous solutions proposed correspondences 3132 imprecise Other works correspondence information additional constraints userspeciﬁc itemspeciﬁc latent feature matrices 3952 2 What transfer transfer raised 41 Previous works address question include approaches transfer knowledge latent features adaptive way 43 collective way 3952 Some works direction include transfer clusterlevel rating patterns 31 adaptive manner collective manner 32 3 How model datadependent effect numerical ratings binary ratings sharing dataindependent knowl edge This question important clearly auxiliary target data different distributions different semantic meanings From Table 1 solutions 39523132 proposed different problem settings compared shown Table 2 Fig 1 More speciﬁcally aforementioned challenges problem setting W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 43 Fig 1 Graphical model Transfer Collective Factorization transfer learning collaborative ﬁltering Note use set userspeciﬁc latent feature vectors set itemspeciﬁc latent feature vectors target data auxiliary data approaches 3952 capture datadependent information methods 3132 use existing correspondence information 23 Overview solution We propose principled matrixbased transferlearning framework referred Transfer Collective Factorization jointly factorizes data matrices parts userspeciﬁc latent feature matrix itemspeciﬁc latent feature matrix datadependent inner matrices Speciﬁcally main idea solution major steps First factorize target numerical rating matrix R UBVT auxiliary binary rating matrix R U B VT constraints sharing userspeciﬁc latent feature matrix U U itemspeciﬁc latent feature matrix V V Table 2 matrix illustration Second learn inner matrices B B separately domain capture domaindependent information semantic meaning distributions numerical ratings binary ratings different As alternative interpretation inner matrices B B considered datadependent correlations rows U columns VT target data auxiliary data respectively Those major steps iterated richer interactions knowledge sharing 1354 reach convergence locally optimal state The intuition approach users items domains likely latent feature matrices U U V V domain differences datadependent information left inner matrices B B In summary major contributions 1 We use correspondences users items source target domains We allow aligned users items share userspeciﬁc latent feature matrix itemspeciﬁc latent feature matrix respectively 2 We construct shared latent space address transfer question matrix trifactorization trilinear method collective way address transfer question 3 We model datadependent effect binary ratings numerical ratings learning inner matrices trilinear method separately 3 Transfer collective factorization 31 Model formulation We assume user rating item target data rui generated userspeciﬁc latent feature vector U u R1du itemspeciﬁc latent feature vector V R1dv datadependent effect denoted B Rdu dv Note formulation different PMF formulation 47 contains U u V Similarly graphical model shown Fig 1 signiﬁcant extension graphical model PMF 47 U u u 1 n V 1 m shared bridge data B B designed capture datadependent effect We ﬁx d du dv notation simplicity sequel We deﬁne conditional distribution 44 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 pruiU u B V αr N cid3 ruiU uBV T α1 r cid4 cid5 αxμ2 2 N xμ α1 Gaussian distribution mean μ precision α We deﬁne prior distributions U u V B pU uαu N U u0 α1 v I pBβ N B0 βq1I We logposterior function latent variables U Rnd B Rdd V Rmd Bayesian inference u I pV iαv N V i0 α1 α 2π exp log pU B VR αr αu αv β mcid6 cid7 ncid6 log cid8 pruiU u B V αrpU uαupV iαv pBβ yui u1 ncid6 i1 mcid6 cid7 N log cid3 ruiU uBV T α1 r cid4 N cid3 cid4 U u0 α1 u I N cid3 cid4 V i0 α1 v I N cid3 B0 βq 1I cid4cid8 yui u1 ncid9 i1 mcid9 yui i1 u1 cid5 cid10 αr 2 cid3 rui U uBV T cid4 2 αu 2 cid5U ucid52 F αv 2 cid5V icid52 F β 2q cid5Bcid52 F C cid11 C ln αr 2π ln αu 2π ln αv 2π ln cid5 cid5 cid5 ncid9 mcid9 u1 i1 cid10 1 2 yui cid3 rui U uBV T cid4 β 2qπ constant Setting αr 1 cid11 2 αu 2 cid5U ucid52 F αv 2 cid5V icid52 F β 2 cid5Bcid52 F Similarly auxiliary data logposterior function matrix trifactorization trilinear model log pU B V R αr αu αv β To jointly maximize logposterior functions log pU B VR αr αu αv β λ log pU B V R αr αu αv β max UVB B st U V D λ 0 tradeoff parameter balance target auxiliary data D value domain latent variables D DR U Rnd V Rmd D DR UT U I VT V I effect ﬁnding latent ics 1843 noise reduction 627 SVD Thus variants TCF CMTF collective matrix trifactorization DR CSVD collective SVD D Although 2DSVD Tucker2 20 factorize sequence matrices achieve goal missingvalue prediction sparse observation matrices accomplished proposed approach Finally obtain following equivalent minimization problem TCF ncid9 mcid9 cid10 min UVB B u1 λ yui i1 ncid9 mcid9 u1 i1 cid3 rui U uBV T cid4 1 2 2 αu 2 cid5U ucid52 αv 2 cid5V icid52 cid11 cid10 1 2 yui cid3 rui U u BV T cid4 2 αu 2 cid5U ucid52 αv 2 cid5V icid52 cid11 β 2 cid5Bcid52 F U V D λ β 2 cid5 Bcid52 F st 6 To solve optimization problem Eq 6 ﬁrst collectively factorize data matrices R R learn U V We estimate B B separately We transfer knowledge latent feature matrices U V collective factorization rating matrices R R For reason approach Transfer Collective Factorization 32 Learning TCF Learning U V CMTF Given B V userspeciﬁc latent feature matrix U Eq 6 obtained analytically Theorem 1 Given B V obtain userspeciﬁc latent feature matrix U closed form W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 2 αu 2 cid5U ucid52 αv 2 cid5V icid52 β 2 cid5Bcid52 F λ cid2 m i1 yui 1 2 rui U u BV T 2 αu 2 45 cid5U ucid52 Proof Let f u αv cid5V icid52 β 2 2 m 2 rui U uBV T i1 yui 1 cid2 cid5 Bcid52 F mcid9 f u U u cid7cid3 rui U uBV T cid4 V iBT αu U u cid8 yui i1 λ mcid9 cid7cid3 yui rui U u BV T cid4 V BT αu U u cid8 i1 mcid9 cid3 yuirui V iBT λ yui cid4 rui V BT i1 αu U u mcid9 yui λ yui U u i1 mcid9 cid3 i1 yuiBV T V iBT λ yui BV T V BT cid4 0 update rule U u Setting f u U u U u bu C cid2 1 u C u m i1 yuiBV T V iBT λ yui BV T V BT αu userspeciﬁc latent feature matrix U analytically cid2 We U u Eq 7 independent users latent features given B V obtain cid2 m i1 yui λ yuiI bu cid2 m i1 yuirui V iBT λ yui 7 rui V BT Similarly given B U latent feature vector V item estimated closed form itemspeciﬁc latent feature matrix V obtained analytically 1 V bi C cid2 n u1 yuiBT U T C uU uB λ yui BT U T uU u B αv cid2 n u1 yui λ yuiI bi cid2 n u1 yuirui U uB λ yui rui U u B 8 The closedform update rule Eq 7 Eq 8 considered generalization alternating square ALS approach 4 Note Bell Koren 4 consider bilinear model single matrix different trilinear models matrices Learning U V CSVD Since constraints D similar effect regularization remove regularization terms Eq 6 reach simpliﬁed optimization problem cid4cid12 cid122 F R U BVT R UBVT cid12 cid12 Y cid6 cid12 cid12Y cid6 cid4cid12 cid122 F 1 cid3 cid3 2 min UV st UT U I λ 2 VT V I Let f 1 2 f U cid5Y cid6 R UBVT cid52 F cid4cid4 cid3 Y cid6 UBVT R cid3 λ 2 cid5 Y cid6 R U BVT cid52 cid3 cid3 Y cid6 U BVT R VBT λ cid4cid4 V BT F We gradients U follows Then variable U learned gradient descent algorithm Grassmann manifold 211027 U U γ cid3 I UUT cid4 f U U γ U We γ obtained analytically following theorem Theorem 2 The step size γ Eq 10 obtained analytically Proof Plugging update rule Eq 10 objective function Eq 9 cid12 cid12Y cid6 cid12 cid12 Y cid6 cid3 gγ 1 2 λ 2 cid12 cid12Y cid6 cid12 cid12 Y cid6 1 2 λ 2 cid7 R U γ UBVT cid7 R U γ U BVT cid3 cid4 R UBVT γ Y cid6 cid8cid12 cid122 F cid8cid12 cid122 F cid3 R U BVT cid4 γ Y cid6 U BVT UBVT cid3 cid4cid12 cid122 F cid4cid12 cid122 F 9 10 46 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 Denoting t1 Y cid6 R UBVT t1 Y cid6 R U BVT t2 Y cid6 UBVT t2 Y cid6 U BVT gγ 1 cid5t1 γ t2cid52 λ 2 2 cid5t1 γ t2cid52 F F gradient cid3 γ tr t T 2 t2 cid3 t T 1 t2 tr cid4 gγ γ cid4 cid4 cid3 cid7 t T tr 1 t2 λ γ tr cid3 t T 2 cid4cid8 t2 obtain γ trt T trt T 1 t2λ trt T 2 t2λ trt T t2 t2 2 1 setting gγ γ 0 cid2 Similarly update rule itemspeciﬁc latent feature matrix V V V γ V 11 V I VVT f V f V Y cid6 UBVT RUB λ Y cid6 U BVT RU B Note previous works 1027 use gradient descent approach Grassmann manifold But study singlematrix factorization problem adopt different learning algorithm Grassmann manifold searching step size γ Learning B B Given U V estimate B B separately data target data Let F R UBVT cid2 n u1 cid5U ucid52 αv i1 yui 1 F cid5Bcid52 cid2 m 2 rui U uBV T mcid9 ncid9 cid4 2 2 αu cid10 cid3 rui U uBV T yui 1 2 cid4 2 cid5V icid52 β cid11 2 β 2 cid5Bcid52 F 2 cid3 R UBVT F u1 i1 cid12 cid12Y cid6 1 2 cid3 R UBVT cid4cid12 cid122 F β 2 cid5Bcid52 F Thus obtain following equivalent minimization problem β 2 R UBVT cid12 cid12Y cid6 cid4cid12 cid122 F cid5Bcid52 F min B 1 2 cid3 12 datadependent parameter B estimated exactly estimating w corresponding T Rd21 large vector concatenated columns square SVM problem w vecB B T u V Rd21 Hence obtain matrix B The instances constructed xui rui yui 1 xui vecU T following leastsquare SVM problem 1 B T d min w 1 2 cid5r Xwcid52 F β 2 cid5wcid52 F 13 X xui T Rpd2 ratings R Setting w XT r Xw β w 0 yui 1 data matrix r 1 2 3 4 5p1 corresponding observed cid3 XT X βI cid41 XT r w 14 Note B w considered linear compact operator 1 solved eﬃciently existing offthe shelf tools Finally solve optimization problem Eq 6 alternatively estimating B B U V The complete algo rithm given Fig 2 Note scale target matrix R rui rui1 4 yui 1 u 1 n 1 m order remove value range difference data sources We adopt random initialization U V CMTF SVD results 17 R CSVD 33 Analysis Each substep updating B B U V Fig 2 monotonically decrease objective function Eq 6 ensure convergence local minimum We use validation data set determine convergence condition tune parameters Section 43 The time complexity TCF baseline methods Section 4 obtained follows AF O q ii PMF 47 O K qd2 K maxn md3 iii cPMF 47 O K qcd2 K maxn md3 iv SVD 48 O nm ﬁlls missing ratings average values v PCC 45 O n2 vi OptSpace 27 O K qd3 K d6 vii CMF 52 O K maxq qd2 K maxn md3 viii TCF O K maxq qd3 K d6 K number iterations convergence q q q q n m number nonmissing entries matrix R R respectively c average number raters item R d number latent features Note TCF algorithm sped stochastic sampling stochastic gradient descent algorithm distributed computing More speciﬁcally step estimating B B CMTF CSVD equivalent W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 47 Input The target useritem numerical rating matrix R auxiliary useritem binary rating matrix R targetuseritem indicator matrix Y auxiliary useritem indicator matrix Y Output The shared userspeciﬁc latent feature matrix U shared itemspeciﬁc latent feature matrix V inner matrix model target datadependent information B inner matrix model auxiliary datadependent information B Step 1 Scale ratings R rui rui1 4 Step 2 Initialize U V randomly initialize U V CMTF initialize U V CSVD SVD 17 results R Step 3 Estimate B B shown Eq 14 Step 4 Update U V B B repeat yui 1 u 1 n 1 m repeat Step 411 Fix B V update U CMTF shown Eq 7 CSVD shown Eq 10 Step 412 Fix B U update V CMTF shown Eq 8 CSVD shown Eq 11 Convergence Step 42 Fix U V update B B shown Eq 14 Convergence Fig 2 The algorithm Transfer Collective Factorization square SVM existing offtheshelf tools use stochastic sampling stochastic gradient descent method 8 distributed algorithms 14 Second step estimating U V CMTF distributed PMF CMF For example B V given user latent feature vector U u independent users ﬁts MPI message passing interface framework 4 Experimental results Our experiments designed verify following hypotheses We believe transfer learning effective address ing data sparsity problem collaborative ﬁltering smoothing methods competitive baselines task missingvalue prediction sparse rating matrix In particular believe proposed transfer learning methods CMTF CSVD perform better baseline algorithms b believe transfer learning method CMTFlink better nontransfer learning methods PMF 47 SVD 48 OptSpace 27 c believe transfer learning method CMTF better CMFlink inner matrices B B CMTF capture datadependent information d believe transfer learning method CSVD better CMTF orthonormal constraints CSVD selectively transfer useful knowledge noise reduction We verify hypotheses Section 43 41 Data sets evaluation metrics We evaluate proposed method movie rating data sets Moviepilot Netﬂix6 compare stateoftheart baseline algorithms Subset Moviepilot data The Moviepilot rating data contains 45 106 ratings values 0 100 given 10 105 users 25 104 movies 46 The data set experiments constructed follows 1 ﬁrst randomly extract 2000 2000 dense rating matrix R Moviepilot data We normalize ratings rui 25 1 new rating range 1 5 2 randomly split R training test sets T R T E 50 ratings respectively T R T E u rui N N 1 5 1 cid2 u cid2 n 1 cid2 cid2 m T E kept unchanged different average number observed ratings user ui yuinm levels 02 04 06 4 8 12 16 randomly sampled T R training different sparsity 08 correspondingly 3 randomly pick 40 observed ratings average T R user construct auxiliary data matrix R To simulate heterogeneous auxiliary target data adopt preprocessing approach 51 R relabeling ratings value rui cid2 3 R 0 dislike ratings value rui 3 1 like The overlap R R cid2 yuinm 0026 0062 0096 013 correspondingly cid2 ui yui 6 httpwwwnetﬂixcom 48 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 Table 3 Description subset Moviepilot data n m 2000 subset Netﬂix data n m 5000 Data set Moviepilot subset Netﬂix subset target training target test auxiliary target training target test auxiliary Form 1 5 1 5 0 1 1 2 3 4 5 1 2 3 4 5 0 1 Sparsity 1 114 2 1 113 2 Subset Netﬂix data The Netﬂix rating data contains 108 ratings values 1 2 3 4 5 given 48 105 users 18 104 movies The data set experiments constructed follows 1 use target data previous work 43 dense 5000 5000 rating matrix R Netﬂix data speciﬁcally 43 ﬁrst identify 5000 movies appearing MovieLens7 Netﬂix movie title select 10 000 frequent users 5000 popular items Netﬂix 5000 items paper movies appearing MovieLens Netﬂix 5000 users paper frequent 5000 users 2 randomly split R training test sets T R T E 50 ratings respectively T E kept unchanged different average number observed ratings user 10 20 30 40 randomly sampled T R training different sparsity levels 02 04 06 08 correspondingly 3 randomly pick 100 observed ratings average T R user construct auxiliary data matrix R To simulate heterogeneous auxiliary target data adopt preprocessing approach 51 R relabeling 1 2 3 ratings R 0 dislike 4 5 ratings 1 like The overlap R R yuinm 0035 0071 011 014 correspondingly ui yui cid2 The ﬁnal data sets8 summarized Table 3 Evaluation metrics We adopt evaluation metrics Mean Absolute Error MAE Root Mean Square Error RMSE cid9 MAE rui ˆruiT E uiruiT E cid13 cid9 RMSE uiruiT E rui ˆrui2T E rui ˆrui true predicted ratings respectively T E number test ratings In experi ments run 3 random trials generating required number observed ratings T R averaged results reported 42 Baselines parameter settings We compare TCF method ﬁve nontransfer learning methods average ﬁlling method AF Pearson cor relation coeﬃcient PCC 45 PMF 47 SVD 48 OptSpace 27 learning methods auxiliary data CMF 52 logistic link function CMFlink constrained PMF cPMF 47 We study following average ﬁlling AF methods ˆrui ru ˆrui ri ˆrui ru ri2 ˆrui bu ri ˆrui ru bi ˆrui r bu bi 7 httpwwwgrouplensorgnode73 8 The data code downloaded httpwwwcseusthkweikepTCFdatacodezip W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 49 Fig 3 Logistic link function σ x 1 1expγ x05 cid2 cid2 yuirui cid2 ru cid2 yuirui ri u yui average rating item bu ui yui global average rating We use ˆrui r bu bi performs best experiments In order compare commonly average ﬁlling methods report results ˆrui ru ˆrui ri yui average rating user u ri u yuirui ru u yuirui u yui bias item r yui bias user u bi ui yuirui cid2 cid2 cid2 For SVD 48 adopt approach 5star numerical rating predictions reported best 48 Speciﬁcally convert original rating matrix R R follows 48 cid2 cid2 cid2 cid14 rui rui rui ru ri ru yui 1 rated yui 0 rated ru user average rating ri item average rating aforementioned average ﬁlling methods apply SVD 548 matrix R R UΣVT ﬁnally rating user u item predicted follows 48 ˆrui ru U uΣ V T average rating ru added prediction rule For PCC data matrices sparse use set neighboring users prediction rule For PMF cPMF SVD OptSpace CMFlink TCF ﬁx latent feature number d 10 For PMF different tradeoff parameters αu αv 001 01 1 tried cPMF different tradeoff parameters αu αv αw 001 01 1 tried CMFlink different tradeoff parameters αu αv 001 01 1 λ 001 01 1 tried CMTF β ﬁxed 1 different tradeoff parameters αu αv 001 01 1 λ 001 01 1 tried CSVD different tradeoff parameters λ 001 01 1 tried 123451 4 123451 4 logistic link function σ U u V T embed To alleviate data heterogeneity 0 1 ded auxiliary data matrix factorization CMF cid10 yui Ncid9 Mcid9 u1 i1 min UV Ncid9 Mcid9 λ yui 1 2 cid10 cid3 rui U u V T cid4 2 αu 2 cid5U ucid52 αv 2 cid5V icid52 cid11 cid3 rui σ cid3 U u V T cid4cid4 1 2 2 αu 2 cid5U ucid52 αv 2 cid11 cid5V icid52 σ x i1 u1 1expγ x05 Fig 3 different parameters γ 1 10 20 tried 1 For cPMF 47 integrate auxiliary data follows min UVW ncid9 mcid9 u1 i1 yui 1 2 cid15 cid16 cid16 rui U u mcid9 yu j W j cid17 mcid9 cid18 cid182 yu j V T αu 2 cid5U ucid52 αv 2 cid5V icid52 αw 2 j1 cid19 j1 mcid9 cid5W jcid52 j1 U Rnd userspeciﬁc latent feature matrix V Rmd itemspeciﬁc latent feature matrix W Rmd called latent similarity constraint matrix 47 Once learned model parameters predict rating 50 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 Table 4 Prediction performance subset Moviepilot data Table 3 AF ˆrui r bu bi AF user ˆrui ru AF item ˆrui ri PCC45 SVD 48 PMF 47 cPMF constrained PMF 47 OptSpace 27 CMFlink CMF 52 logistic link function variants Transfer Collective Factorization TCF CMTF TCF CSVD Numbers boldface 07087 Italic 07415 best second best results methods respectively Metrics Methods MAE RMSE AF AF user AF item PCC PMF cPMF SVD OptSpace CMFlink TCF CMTF TCF CSVD AF AF user AF item PCC PMF cPMF SVD OptSpace CMFlink TCF CMTF TCF CSVD Sparsity 02 tr 3 val 1 0794200047 0826900081 0812600035 0795600237 0811800014 0836800012 0826200081 1346500352 0995600149 0741500018 0708700035 1039100071 1086700120 1061500053 1039500358 1033000012 1090600016 1086900121 1718900314 1302400170 0944900018 0929800038 04 tr 7 val 1 0725900022 0781900041 0772100014 0778500102 0779400009 0768100011 0779600039 0797100031 0763200005 0702100020 0686000023 095580002 1020600054 1007300012 1021700091 1012300013 0990000004 1021000053 1061100062 1006600036 0910900013 0903900018 06 tr 11 val 1 0695600017 0764300018 0754100011 0721500211 0760200009 0752600013 0760300017 0754100039 0712100007 0687100013 0674300048 0917700017 0992900025 0983600009 0958200261 0983200009 0967900013 0993600024 0995200024 0936600007 0896700011 0889800052 08 tr 15 val 1 0679800010 0755900011 0744900002 0676600095 0751300005 0746200007 0750500013 0726000024 0690500007 0677600006 0661200028 0897700002 0980200015 0972200003 0900500125 0970600003 0959900004 0981300014 0954300042 0907200009 0887500003 0874400033 user u item ˆrui U u produces better results like dislike yu j W j yu jV T m j1 m j1 cid2 cid2 In experiments cPMF use auxiliary data like 43 Summary experimental results We randomly sample n ratings rating user average training data R use validation set determine tradeoff parameters αu αv αw β λ number iterations convergence PMF cPMF OptSpace CMFlink TCF For AF PCC SVD training set validation set combined set training data The results test data unavailable training reported Tables 4 5 We following observations 1 For smoothing method average ﬁlling AF best variant ˆrui ru bu bi competitive sparse rating data commonly average ﬁlling methods ˆrui ru ˆrui ri worse There reasons advantages AF First average ﬁlling strong baseline especially small dense subsets Netﬂix Moviepilot data Second PMF cPMF advantages useritem rating matrix large data set Netﬂix competition improved tune parameters ﬁner granularity 2 For matrix factorization methods orthonormal constraint including SVD OptSpace SVD better OptSpace sparsity lower cid2 06 Moviepilot cid2 04 Netﬂix OptSpace beats SVD rating matrix denser explained different strategies adopted SVD OptSpace missing ratings SVD ﬁlls missing ratings average values help extremely sparse rating matrix hurt performance rating matrix denser 3 For sparsity problem collaborative ﬁltering transfer learning attractive technique The proposed transfer learning methods CMTF CSVD perform signiﬁcantly better baselines sparsity levels b For transfer learning method CMFlink signiﬁcantly better nontransfer learning methods PMF SVD OptSpace sparsity levels extremely sparse case 02 Moviepilot worse AF explained heterogeneity auxiliary binary rating data target numerical rating data usefulness smoothing AF sparse data For PMF cPMF cPMF auxiliary data better PMF cases c For transfer learning methods CMTF CMFlink CMTF performs better CMFlink cases shows advantages modeling datadependent effect inner matrices B B CMTF W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 51 Table 5 Prediction performance subset Netﬂix data Table 3 AF ˆrui r bu bi AF user ˆrui ru AF item ˆrui ri PCC 45 SVD 48 PMF 47 cPMF constrained PMF 47 OptSpace 27 CMFlink CMF 52 logistic link function variants Transfer Collective Factorization TCF CMTF TCF CSVD Numbers boldface 07405 Italic 07589 best second best results methods respectively Metrics Methods MAE RMSE AF AF user AF item PCC PMF cPMF SVD OptSpace CMFlink TCF CMTF TCF CSVD AF AF user AF item PCC PMF cPMF SVD OptSpace CMFlink TCF CMTF TCF CSVD Sparsity 02 tr 9 val 1 0776500006 0806000021 0853500007 0823300228 0887900008 0849100181 0805500021 0827600004 0799400017 0758900175 0740500007 0985500004 1020800015 1070800011 1046200326 1077900001 1060600199 1020200014 1067600020 1020400013 0965300198 0950200005 04 tr 19 val 1 0742900006 0786500010 0837200005 0788800418 0846700006 0814700006 0784600010 0781200040 0750800008 0719500055 0708000002 0942700007 0992100012 1047700005 1004100518 1047300004 1012500007 0990600012 1008900024 0955200009 0917100063 0907400004 06 tr 29 val 1 0730800005 0779800009 0830400002 0771400664 0808700188 0812200005 0775700009 0757200027 0736500004 0703100005 0694800007 0927700006 0983400004 1038600004 0984100848 1020500112 1006600006 0979800005 0975000010 0936900004 0897100005 0890300006 08 tr 39 val 1 0724600003 0776700003 0827000001 0778800516 0764200003 0786400057 0771100002 0741800038 0729500003 0696200009 0687700007 0920000002 0979100002 1033900001 0993400662 0969100007 0993000044 0974100004 0954300037 0927700004 0888400007 0880900005 d For variants TCF transfer learning method CSVD improves performance CMTF cases shows effect noise reduction orthonormal constraints UT U I VT V I To study effectiveness selective transfer noise reduction TCF compare performance CMTF CSVD different sparsity levels different auxiliary data sparsity 1 2 3 subset Netﬂix data The results shown Fig 4 We CSVD performs better CMTF cases shows advantage CSVD transferring useful knowledge There fundamental question transfer learning 41 transfer related negative transfer 40 For problem setting Fig 1 negative transfer 40 happen density auxiliary binary ratings lower target numerical ratings semantic meaning auxiliary binary ratings completely different target numerical ratings However work assume auxiliary binary ratings denser target numerical ratings ratings related differences Thus assumption negative transfer likely happen In fact negative transfer observed empirical studies 5 Related works SVD Lowrank singular value decomposition SVD principal component analysis PCA 525 widely informa tion retrieval data mining ﬁnd latent topics 18 reduce noise 6 These solutions applied collaborative ﬁltering 24227444853291027 Among works apply noniterative SVD PCA matrix preprocessing remove missing values 242274448 works 5329 use iterative SVD matrix expectationmaximization EM procedure Still works 1027 missing ratings unknown directly optimize objective function observed ratings Our strategy similar 1027 missing ratings unknown We use representative methods SVD 48 OptSpace 27 baselines experiments The differences approach previously published SVDbased methods identiﬁed aspects First missing ratings unknown previous works preprocess rating matrix obtain matrix PCA SVD applied Second use auxiliary data target rating data transfer learning techniques aforementioned works target rating matrix PMF PMF 47 recently proposed method missingvalue prediction single matrix reduced TCF Eq 6 D DR λ 0 β 0 B I The RSTE model 38 generalizes PMF factorizes single rating 52 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 Fig 4 Prediction performance TCF CMTF CSVD Netﬂix different sparsity levels different auxiliary data matrix regularization term userside social data different twomatrix factorization model The PLRM model 57 generalizes PMF model incorporate numerical ratings implicit purchasing data meta data social network information consider explicit auxiliary data like dislike Mathematically PLRM model considering numerical ratings implicit feedback considered special case TCF framework CMTF D DR learning algorithm different CMTF closedform solutions steps CMF CMF 52 proposed jointly factorizing matrices constraints sharing itemspeciﬁc latent features SoRec 39 proposed sharing userspeciﬁc latent features CMF SoRec reduced TCF Eq 6 D DR β 0 B B I requiring oneside latent feature matrix user R UVT R U VT item R UVT R UVT However problem setting shown Fig 1 users items aligned To alleviate data heterogeneity CMF SoRec embed logistic link function auxiliary data matrix factorization experiments There differences TCF CMF First TCF trilinear method R UBVT R U BVT inner matrices B B designed capture domaindependent information CMF bilinear method applied studied problem Fig 1 Second introduce orthonormal constraints variant TCF CSVD empirically proved effective noise reduction CMF constraints effect Finally learning algorithms TCF CSVD TCF CMTF CMF different DPMF Dependent probabilistic matrix factorization DPMF 2 multitask version PMF based Gaussian processes proposed incorporating homogeneous heterogeneous information sharing inner covariance matrices userspeciﬁc itemspeciﬁc latent features The slice sampling algorithm DPMF time consuming medium sized problems problems studied experiments CST Coordinate transfer CST 43 recently proposed transfer learning method collaborative ﬁltering transfer coordinate auxiliary CF matrices target adaptive way CST performs coordinate constructed auxiliary data dense target data sparse 43 However auxiliary target data dense constructing shared latent feature matrices collective way TCF perform better collective behavior brings richer interactions bridging data sources 1354 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 53 Table 6 Summary related work transfer learning collaborative ﬁltering Knowledge transfer Algorithm style transfer PMF 47 family NMF 30 family Covariance Latent features Codebook Latent features Adaptive Collective CST 43 CBT 31 DPMF 2 SoRec 39 CMF 52 TCF RMGM 32 WNMCTF 56 Parallel PMF family CMF DPMF corresponding NMF 30 family nonnegative constraints 1 Trilinear method WNMCTF 56 proposed factorize matrices useritem itemcontent user demographics 2 codebook sharing methods CBT 31 RMGM 32 considered adaptive collective extensions 5019 RMGMOT 33 followup work RMGM 32 studies effect user preferences time sharing clusterlevel rating patterns temporal domains This work focused homogeneous user feedbacks 5star grades instead heterogeneous user feedbacks Models NMF family usually better interpretability learned latent feature matrices U V CBT 31 RMGM 32 considered memberships corresponding users items ranking models 28 collaborative ﬁltering PMF family We summarize related work Table 6 perspective having nonnegative constraints latent variables transfer transfer learning 41 Clustering relational data Long et al 3637 study clustering problem matrix missing values different problem setting missing rating prediction idea sharing common subspace latent feature matrices similar Cohn et al 15 study document clustering content information auxiliary information documentdocument link information matrices termdocument documentdocument missing values Banerjee et al 3 study clustering relational data missing values missing entries imputed zeros approach takes missing values unknown aims missing rating prediction Logistic loss function matrix factorization There matrix factorization methods logistic loss functions binary rating data 162649 There reasons use loss functions First different loss functions logistic loss function binary PCA 162649 vertical research direction focus developing transfer learning solutions study issue future work Second diﬃcult justify logistic loss function 162649 factorization auxiliary binary rating matrix square loss function target numerical rating matrix objective functions totally different meanings scales userspeciﬁc latent feature matrix U domains comparable similar V cause diﬃculty knowledge sharing We illustrate loss functions bellow cid8 cid7 rui log ˆrui 1 rui log1 ˆrui vs cid3 rui U u V T cid4 2 rui 0 1 true binary rating ˆrui σ U u V T sigmoid function logistic link function 0 1 predicted rating σ θ 1 1expθ Furthermore address heterogeneities numerical ratings binary ratings scaled 5star numerical ratings range 0 1 introduced sigmoid link function logistic link function instead logistic loss function follows Section 4 rui ˆrui2 vs ˆrui σ U u V T cid4 cid3 rui U u V T 0 1 predicted rating 2 To sum differences proposed transfer learning solution works include following First focus missing rating prediction instead clustering 36 Second study auxiliary data user feedbacks instead content information 52 Third leverage auxiliary data frontal instead user 11 item 52 Fourth missing ratings unknown instead negative feedbacks zeros 3 order optimize objec tive function speciﬁcally observed ratings Fifth introduce orthonormal constraints instead nonnegative constraints 56 resemble effect noise reduction Sixth design collective algorithm instead adaptive algorithm richer interactions auxiliary domain target domain 1354 Seventh transfer knowl edge latent features aligned users items instead sharing compressed knowledge clusterlevel 54 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 rating patterns 3132 covariance matrix 2 Finally extend trilinear base model instead bilinear model 52 capture domainindependent knowledge domaindependent effect In summary ﬁrst points illustrate novelty proposed problem setting points novelty designed algorithm 6 Conclusions future work In paper investigate address sparsity problem collaborative ﬁltering transfer learning solu tion Speciﬁcally present novel transfer learning framework Transfer Collective Factorization transfer knowledge auxiliary data explicit binary ratings like dislike alleviates data sparsity problem target numerical ratings Note assume 5star ratings unordered bins instead ordinal relative preferences Our method constructs shared latent space U V collective manner captures datadependent effect learning inner matrices B B separately selectively transfers useful knowledge noise reduction introducing orthonor mal constraints The novelty algorithm includes generalizing transfer learning methods collaborative ﬁltering principled way Experimental results TCF performs signiﬁcantly better stateoftheart baseline algorithms sparsity levels The problem setting TCF Fig 1 forheterogeneous explicit user feedbacks novel widely applicable ap plications useritem representation recommender systems Examples include querydocument information retrieval authorword academic publications usercommunity social network services 59 locationactivity ubiquitous computing 60 drugprotein biomedicine For future work study extend transfer learning framework additional areas include theoretical analysis largerscale experiments In particular address pure coldstart recommendation prob lem users rating sparse learning matrix completion 27 partial correspondence users items 34 distributed implementation MPI framework adaptive transfer learning 12 collaborative ﬁltering complex user feedbacks different rating distributions different loss functions 1626 Acknowledgements We thank support RGCNSFC Joint Research Grant N_HKUST62409 Hong Kong RGC Grant 621211 We thank anonymous reviewers detailed helpful comments References 1 Jacob Abernethy Francis Bach Theodoros Evgeniou JeanPhilippe Vert A new approach collaborative ﬁltering Operator estimation spectral regularization J Mach Learn Res 10 June 2009 803826 2 Ryan P Adams George E Dahl Iain Murray Incorporating information probabilistic matrix factorization Gaussian processes Uncer tainty Artiﬁcial Intelligence UAI 2010 pp 19 3 Arindam Banerjee Sugato Basu Srujana Merugu Multiway clustering relation graphs SIAM International Conference Data Mining SDM 2007 4 Robert M Bell Yehuda Koren Scalable collaborative ﬁltering jointly derived neighborhood interpolation weights Proceedings 2007 Seventh IEEE International Conference Data Mining IEEE Computer Society Washington DC USA 2007 pp 4352 5 Michael W Berry Svdpack A fortran77 software library sparse singular value decomposition Technical report Knoxville TN USA 1992 6 Michael W Berry Susan T Dumais Gavin W OBrien Using linear algebra intelligent information retrieval SIAM Rev 37 December 1995 573595 7 Daniel Billsus Michael J Pazzani Learning collaborative information ﬁlters Proceedings Fifteenth International Conference Machine Learning ICML98 Morgan Kaufmann Publishers Inc San Francisco CA USA 1998 pp 4654 8 Léon Bottou Largescale machine learning stochastic gradient descent Yves Lechevallier Gilbert Saporta Eds Proceedings 19th International Conference Computational Statistics COMPSTAT2010 Springer Paris France August 2010 pp 177187 9 John S Breese David Heckerman Carl Myers Kadie Empirical analysis predictive algorithms collaborative ﬁltering Technical report MSRTR98 12 1998 10 Nicoletta Del Buono Tiziano Politi A continuous technique weighted lowrank approximation problem International Conference Com putational Science Applications ICCSA 2004 pp 988997 11 Bin Cao Nathan Nan Liu Qiang Yang Transfer learning collective link prediction multiple heterogenous domains International Conference Machine Learning ICML 2010 pp 159166 12 Bin Cao Sinno Jialin Pan Yu Zhang DitYan Yeung Qiang Yang Adaptive transfer learning TwentyFourth Conference Artiﬁcial Intelligence AAAI 2010 13 Rich Caruana Multitask learning Mach Learn 28 July 1997 4175 14 Edward Y Chang Hongjie Bai Kaihua Zhu Hao Wang Jian Li Zhihuan Qiu PSVM Parallel support vector machines incomplete Cholesky factor ization Scaling Machine Learning Parallel Distributed Approaches Cambridge Univ Press 2011 15 David Cohn Deepak Verma Karl Pﬂeger Recursive attribute factoring Neural Information Processing Systems NIPS 2006 pp 297304 16 Michael Collins S Dasgupta Robert E Schapire A generalization principal components analysis exponential family Neural Information Processing Systems NIPS 2001 pp 617624 17 Paolo Cremonesi Yehuda Koren Roberto Turrin Performance recommender algorithms topn recommendation tasks Proceedings Fourth ACM Conference Recommender Systems RecSys10 ACM New York NY USA 2010 pp 3946 18 Scott C Deerwester Susan T Dumais Thomas K Landauer George W Furnas Richard A Harshman Indexing latent semantic analysis J Am Soc Inf Sci 41 6 1990 391407 19 Chris Ding Tao Li Wei Peng Haesun Park Orthogonal nonnegative matrix trifactorizations clustering Proceedings 12th ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD06 ACM New York NY USA 2006 pp 126135 20 Chris HQ Ding Jieping Ye 2dimensional singular value decomposition 2d maps images SIAM International Conference Data Mining SDM 2005 pp 3243 W Pan Q Yang Artiﬁcial Intelligence 197 2013 3955 55 21 Alan Edelman Tomás A Arias Steven T Smith The geometry algorithms orthogonality constraints SIAM J Matrix Anal Appl 20 2 1999 303353 22 Danyel Fisher Kris Hildrum Jason Hong Mark Newman Megan Thomas Rich Vuduc Swami poster session A framework collaborative ﬁltering algorithm development evaluation Proceedings 23rd Annual International ACM SIGIR Conference Research Development Information Retrieval SIGIR00 ACM New York NY USA 2000 pp 366368 23 David Goldberg David Nichols Brian M Oki Douglas Terry Using collaborative ﬁltering weave information tapestry Commun ACM 35 December 1992 6170 24 Ken Goldberg Theresa Roeder Dhruv Gupta Chris Perkins Eigentaste A constant time collaborative ﬁltering algorithm Inf Retr 4 July 2001 133151 25 Gene H Golub Charles F Van Loan Matrix Computations 3rd ed Johns Hopkins University Press Baltimore MD USA 1996 26 Geoffrey J Gordon Generalized2 linear2 models Neural Information Processing Systems NIPS 2002 pp 577584 27 Raghunandan H Keshavan Andrea Montanari Sewoong Oh Matrix completion noisy entries J Mach Learn Res 11 August 2010 20572078 28 Yehuda Koren Factor neighbors Scalable accurate collaborative ﬁltering ACM Trans Knowl Discov Data 4 1 January 2010 124 29 Miklós Kurucz András A Benczúr Balázs Torma Methods large scale svd missing values KDDCup 2007 2007 30 Daniel D Lee H Sebastian Seung Algorithms nonnegative matrix factorization Neural Information Processing Systems NIPS 2001 pp 556 562 31 Bin Li Qiang Yang Xiangyang Xue Can movies books collaborate Crossdomain collaborative ﬁltering sparsity reduction International Joint Conferences Artiﬁcial Intelligence IJCAI 2009 pp 20522057 32 Bin Li Qiang Yang Xiangyang Xue Transfer learning collaborative ﬁltering ratingmatrix generative model International Conference Machine Learning ICML 2009 pp 617624 33 Bin Li Xingquan Zhu Ruijiang Li Chengqi Zhang Xiangyang Xue Xindong Wu Crossdomain collaborative ﬁltering time IJCAI 2011 pp 2293 2298 34 Tao Li Vikas Sindhwani Chris HQ Ding Yi Zhang Bridging domains words Opinion analysis matrix trifactorizations SIAM International Conference Data Mining SDM 2010 pp 293302 35 Nathan N Liu Evan W Xiang Min Zhao Qiang Yang Unifying explicit implicit feedback collaborative ﬁltering Proceedings 19th ACM International Conference Information Knowledge Management CIKM10 ACM New York NY USA 2010 pp 14451448 36 Bo Long Zhongfei Mark Zhang Xiaoyun Wú Philip S Yu Spectral clustering multitype relational data Proceedings 23rd International Conference Machine Learning ICML06 ACM New York NY USA 2006 pp 585592 37 Bo Long Zhongfei Mark Zhang Philip S Yu A probabilistic framework relational clustering Proceedings 13th ACM SIGKDD International Conference Knowledge Discovery Data Mining KDD07 ACM New York NY USA 2007 pp 470479 38 Hao Ma Irwin King Michael R Lyu Learning recommend explicit implicit social relations ACM Trans Intell Syst Technol 2 3 May 2011 119 39 Hao Ma Haixuan Yang Michael R Lyu Irwin King Sorec Social recommendation probabilistic matrix factorization ACM Conference Information Knowledge Management CIKM 2008 pp 931940 40 Leslie Pack Kaelbling Michael T Rosenstein Zvika Marx To transfer transfer Neural Information Processing Systems NIPS 2005 41 Sinno Jialin Pan Qiang Yang A survey transfer learning IEEE Trans Knowl Data Eng 22 10 2010 13451359 42 Weike Pan Nathan N Liu Evan W Xiang Qiang Yang Transfer learning predict missing ratings heterogeneous user feedbacks International Joint Conferences Artiﬁcial Intelligence IJCAI 2011 pp 23182323 43 Weike Pan Evan W Xiang Nathan N Liu Qiang Yang Transfer learning collaborative ﬁltering sparsity reduction TwentyFourth Conference Artiﬁcial Intelligence AAAI 2010 pp 230235 44 Michael H Pryor The effects singular value decomposition collaborative ﬁltering Technical report Hanover NH USA 1998 45 Paul Resnick Neophytos Iacovou Mitesh Suchak Peter Bergstrom John Riedl Grouplens An open architecture collaborative ﬁltering netnews Computer Supported Cooperative Work CSCW 1994 pp 175186 46 Alan Said Shlomo Berkovsky Ernesto W De Luca Putting things context Challenge contextaware movie recommendation Proceedings Workshop ContextAware Movie Recommendation CAMRa10 ACM New York NY USA 2010 pp 26 47 Ruslan Salakhutdinov Andriy Mnih Probabilistic matrix factorization Neural Information Processing Systems NIPS 2008 pp 12571264 48 Badrul M Sarwar George Karypis Joseph A Konstan John T Riedl Application dimensionality reduction recommender A case study ACM WEBKDD WORKSHOP 2000 49 Andrew I Schein Lawrence K Saul Lyle H Ungar A generalized linear model principal component analysis binary data Proceedings 9th International Workshop Artiﬁcial Intelligence Statistics 2003 50 Luo Si Rong Jin Flexible mixture model collaborative ﬁltering International Conference Machine Learning ICML 2003 pp 704711 51 Vikas Sindhwani SS Bucak J Hu A Mojsilovic A family nonnegative matrix factorizations oneclass collaborative ﬁltering RIA Workshop ACM Conference Recommender Systems 2009 52 Ajit P Singh Geoffrey J Gordon Relational learning collective matrix factorization Proceedings 14th ACM SIGKDD International Confer ence Knowledge Discovery Data Mining KDD08 ACM New York NY USA 2008 pp 650658 53 Nathan Srebro Tommi Jaakkola Weighted lowrank approximations International Conference Machine Learning ICML 2003 pp 720727 54 Charles Sutton Andrew McCallum Composition conditional random ﬁelds transfer learning Proceedings Conference Human Lan guage Technology Empirical Methods Natural Language Processing HLT05 Association Computational Linguistics Stroudsburg PA USA 2005 pp 748754 55 Vishvas Vasuki Nagarajan Natarajan Zhengdong Lu Berkant Savas Inderjit Dhillon Scalable aﬃliation recommendation auxiliary networks ACM Trans Intell Syst Technol 3 1 October 2011 120 56 Jiho Yoo Seungjin Choi Weighted nonnegative matrix cotrifactorization collaborative prediction Proceedings 1st Asian Conference Machine Learning Advances Machine Learning ACML09 SpringerVerlag Berlin Heidelberg 2009 pp 396411 57 Yi Zhang Jiazhong Nie Probabilistic latent relational model integrating heterogeneous information recommendation Technical report School Engineering UCSC 2010 58 Yu Zhang Bin Cao DitYan Yeung Multidomain collaborative ﬁltering Uncertainty Artiﬁcial Intelligence UAI 2010 pp 725732 59 Shiwan Zhao Michelle X Zhou Xiatian Zhang Quan Yuan Wentao Zheng Rongyao Fu Who Social mapbased recommenda tion contentcentric social web sites ACM Trans Intell Syst Technol 3 1 October 2011 123 60 Yu Zheng Xing Xie Learning travel recommendations usergenerated gps traces ACM Trans Intell Syst Technol 2 1 January 2011 129