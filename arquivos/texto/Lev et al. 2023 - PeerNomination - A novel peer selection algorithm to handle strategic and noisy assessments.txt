Artiﬁcial Intelligence 316 2023 103843 Contents lists available ScienceDirect Artiﬁcial Intelligence journal homepage wwwelseviercomlocateartint PeerNomination A novel peer selection algorithm handle strategic noisy assessments Omer Lev Nicholas Mattei b Paolo Turrini c Stanislav Zhydkov d Department Industrial Engineering Management BenGurion University Negev Israel b Department Computer Science Tulane University USA c Department Computer Science University Warwick United Kingdom d Mathematics Institute University Warwick United Kingdom r t c l e n f o b s t r c t Article history Received 30 November 2021 Received revised form 12 June 2022 Accepted 21 December 2022 Available online 29 December 2022 Keywords Peer selection Strategyproofness Optimality Noisy opinions Reweighting In peer selection group agents choose subset winners peerreviewed grants prizes We Condorcet view aggregation problem assuming objective groundtruth ordering agents We study agents noisy perception ground truth assessments truthful inaccurate Our goal select best set agents according underlying ground truth looking potentially unreliable assessments peers Besides potentially unreliable allow agents selfinterested attempting inﬂuence outcome decision favour Hence focused tackling problem impartial strategyproof peer selection prevent agents manipulating reviews selecting deserving individuals presence noisy evaluations We propose novel impartial peer selection algorithm PeerNomination aims fulﬁl desiderata We provide comprehensive theoretical analysis recall PeerNomination prove properties including impartiality monotonicity We provide empirical results based simulations effectiveness compared stateoftheart impartial peer selection algorithms We investigate robustness PeerNomination levels noise reviews In order maintain good performance conditions extend PeerNomination weights reviewers informally capture notion reliability reviewer We theoretically new algorithm preserves strategyproofness empirically weights help identify noisy reviewers increase selection performance1 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 Corresponding author Email addresses omerlevbguacil O Lev nsmatteitulaneedu N Mattei pturriniwarwickacuk P Turrini szhydkovwarwickacuk S Zhydkov 1 This paper signiﬁcant extension IJCAI 2020 contribution 29 limited study PeerNomination noise Besides exploring role weights peer selection presence noise extend unweighted version novel theoretical experimental results httpsdoiorg101016jartint2022103843 00043702 2022 The Authors Published Elsevier BV This open access article CC BY license httpcreativecommonsorglicensesby40 O Lev N Mattei P Turrini et al 1 Introduction Artiﬁcial Intelligence 316 2023 103843 Peer evaluation selection agents rate choose subset award prize pillars quality assessment scientiﬁc contexts While current methods rely expert panels ideally impartial selection process 742 increasing need alternative mechanisms procedure reliable cheap An important approach achieve goal agents submitted proposals review set reviewers This particularly relevant open online courses 37 hiring professional graders prohibitively expensive Indeed large AI venues IJCAI NeurIPS implementing portion requiring authors submit papers agree reviewers papers The importance improving peer reviewing procedures brought light 2014 NeurIPS experiment 2443 papers submitted NeurIPS 2014 10 reviewed twice independent committees aston ishingly agreed half accepted papers Whether outcome bias incompetence wellthought disagreement unclear What clear current solutions suffer undesirable features The exploding number papers AI general science venues spurred improving aspects peer review process including assignment biases 322519 review quality 48 reviewer training 46 quality reviewers discussions overview Shah 42 Other studies bias evalua tive processes brought fore extent impact inaccurate assessments peer reviewing example 4945 Finding high quality mechanisms peer review critical step helping review process large conferences 4 grant reviewing 33 online courses 47 domains Researchers algorithmic game theory computational social choice worked peer selection problem past decade focusing accurate strategyproof algorithms including Partition 1 Credible Subset 23 ExactDollarPartition EDP 4 provide overview algorithms Section 3 All algorithms Condorcet view aggregation problem exists apriori groundtruth ranking agents wish select ranked agents possible given access agents noisy reports 52 While raises obvious philosophical challenges ground truth represent direct access follow view allows quantitative analysis performance peer selection algorithms objective comparison Many existing algorithms survey Section 3 highlight tradeoffs forced pursuit dual goal impartiality optimality Some require set reviewing agents partitioned clusters review 4 sacriﬁce exactness ability select given number agents consistently 23 With PeerNomination algorithm presented paper sacriﬁce exactness able achieve new stateoftheart performance Additionally existing algorithms seek alleviate problem noisy inputs uniﬁed strategyproof mechanism When earlier work engage noisy reports limited empirical testing relatively low noise Mallows model ϕ 05 4 yields fairly minor changes agents reports shown Section 23 We instead concerned algorithms handle signiﬁcant level noise maintaining strategyproofness high quality selection important missing aspect literature Ideally like algorithm capable identifying inaccurate reviewers reducing inﬂuence ﬁnal selection agents reports guide We example try downgrade reviewers differ However problems approach ﬁrst noise diﬃcult establish consensus actually second metalevel reweighting exploited strategically Simple reweighting strategyproof consider example agent harshly reviewing agent b b reviewing agent c Agent b beneﬁt reviewing agent c way present agent unreliable agent lowering impact report agent agent b weights computed based correlations evaluations Merriﬁeld Saari 33 On hand mechanism able identify agent b source noise increase overall quality selection While reweight agents maintaining strategyproofness 4751 wish achieve increased selection quality strategyproofness The algorithm present paper able achieve tasks state art performance 11 Contribution We present PeerNomination impartial strategyproof peer selection method scenarios n agents view reviewed m goal selecting k Each proposal2 identify proposing agent considered independently selected falls k n m majority reviewers partial rankings probabilistic completion number integer Performing selection dependently relaxes exactness requirement algorithm guaranteed select exactly k agents time However mild assumptions algorithm select exactly k agents expectation Unlike wellknown 2 For sake clarity agent referred reviewer mean context reviewing use word proposal referring agent reviewed 2 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 peer reviewing methods ExactDollarPartition EDP PeerNomination rely clustering reviewers submitting complete rankings allowing ﬂexibility deployed We compare performance PeerNomination underlying ground truth ranking agent rank ings drawn according Mallows Model 2852 deriving expected recall analytically3 Furthermore extend PeerNomination use reviewer weights order handle noisy inaccurate agents To explicitly formulate reliability weights reviewers way violate strategyproofness use information reweight scores PeerNomination weights able handle high levels noise reviewers act adver sarially We analytically weighting schemes improve overall quality selection signiﬁcantly Finally empirically compare method peer selection mechanisms analytic performance bounds unknown number wellknown classiﬁcation measures Our results PeerNomination im proves current best performance terms recall known literature relies milder assumptions underlying reviewer graph This suggests relaxing exactness requirement peer selection outcomes improved quality accepted set Moreover empirically PeerNomination weights able signiﬁcantly improve quality peer selection PeerNomination weights variety noise parameters Paper outline In Section 2 provide formal deﬁnitions problem concepts required algorithm precisely description noise model empirical testing In Section 3 number previously proposed algorithms peer selection sets method apart Section 4 introduces PeerNomination different weighting schemes assignment procedure We use Section 5 derive analytic results PeerNomination strategyproofness expected recall Finally Section 6 PeerNomination test stateoftheart strategyproof peer selection algorithm EDP 4 measure performance realistic setting 2 Preliminaries In peer selection problem agents represented set positive integers N Agents ground truth 1 2 n As common peer reviewing literature consistent Condorcet theory voting 52 assume ground truth agents share deﬁne linear order N In words simplifying assumption agents assess accurately report ranking4 To provide general realistic setup use noise model gives agent distorted view ground truth Assuming noisy reviewers requires nuanced notion truthfulness agent true potentially faulty perception In general peer review process consists steps 1 assignment proposals reviewed agent 2 submission reviews reviewing agents 3 aggregation submitted reviews We formalise steps focusing notions needed study PeerNomination algorithm Adopting academic peer review terminology refer agents reviewers context giving reviews proposals context reviewed Review assignment In peer reviewing agents assigned review work A desirable stipulation review assignment agent review We typically expect agent review similar number proposals proposals receive similar number reviews Formally review assignment function A N 2 N Ai This gives reviewer set proposals evaluate Ai reviewers bundle For peer selection procedure need refer set agents assigned review particular proposal j Slightly abusing notation denote reviewers proposal 1 j N j Ai proposals panel Given integer m assignment called mregular j A N Ai A 1i m N In practice m tends small constant respect n representing assumption reviewer limited reviewing capacity This makes mregular assignments desirable distribute workload evenly For reason simplify theoretical analysis assume assignments mregular rest paper Note assumption needed algorithm work In settings conference peer review wish view assignment light agent bids 25 sim ilarity scores 1232 measure assignment quality Xu et al 53 In work like US National Science Foundation 33 peer review classroom setting 16 assume 3 As explain Section 61 paper use binary classiﬁcation deﬁnition recall measure performance Recall calculated proportion positives selected algorithm This measure Aziz et al 4 peer selection literature usually referred accuracy We decide use term recall avoid confusion accuracy distinct deﬁnition classiﬁcation measure 4 This assumption simplistic account diversity views common scientiﬁc debates It thought idealisation scientiﬁc communities methodological debates epistemic views heart discussion participants agree objective value proposal subject careful examination From technical point view standard assumption Condorcet views voting 52 allows easy theoretical empirical analysis performance peer selection algorithms 3 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 1 2 5 3 4 Fig 1 A peer review assignment The ground truth ranking agents given agent number agent 1 best agent 5 worst 1 cid5 2 cid5 3 cid5 4 cid5 5 assignment independent ground truth ranking An interesting direction future work incorpo rate PeerNomination larger framework assumptions information available assignment algorithm However paper assumptions Agents reviews We assume reviewers evaluations represented rank ordering bundle Given mregular review assignment A agent reports ranking bijection σi Ai 1 m A ranking called truthful consistent perceived ground truth In setting noise truthful ranking ranking consistent ground truth accessed In setting noise generally truthful ranking ranking consistent agents individual perception given noise model The collection rankings called proﬁle denoted σ σ1 σn A truthful proﬁle proﬁle truthful rankings The set possible proﬁles truthful denoted cid4 Most peer selection algorithms use rankings provided reviewers directly ExactDollarPartition However coming rankings typically poses higher cognitive load reviewers increases chance inaccurate assessments Additionally settings student peer evaluation assignments discussed De Alfaro Shavlovsky 16 agents outright refuse provide partial complete rankings preferring instead indicate work acceptable In contrast algorithm takes inspiration approval voting require ranking submitted set approved agents As common approval voting 9 voters simply yes candidate potentially subject quota We approvals nominations We expand idea allowing non integer quotas noninteger representing partial nomination Given mregular review assignment A quota q 0 nomination vector agent function σ Ai 0 1 q cid6qcid7 σ 1 q cid6qcid7 1 In words reviewers simply submit nomina tions ﬁll quota choose additional nomination partial Under proposed algorithm partial nomination resolved probabilistically 1 cid6qcid7 σ 1 Example 1 The directed graph Fig 1 represents 2regular assignment n 5 agents noise ground truth ranking agents 1 cid5 2 cid5 3 cid5 4 cid5 5 For example agent 1 reviewing agents 3 5 review bundle ranking consistent ground truth 3 5 If nomination quota 1 truthful strategy agent 1 nominate 3 leave 5 If instead nomination quota increased 12 agent 1 employ partial nominations Their truthful strategy nominate agent 3 fully agent 5 partially Example 1 shows use partial nominations constitute backbone main algorithm Under PeerNomination partial nomination quota directly translate probability nominated Aggregation The ﬁnal step process aggregate rankings select set winners Typically speciﬁed number winners k However mechanisms forgo requirement return possibly smaller set winners 3 possibly winners 23 Formally given review assignment A proﬁle σ cid4 integer k n peer selection mechanism function f cid4 2 The mechanism called exact σ cid4 f σ k N Example 2 Let Example 1 focus nomination quota 1 If agent submits truthful ranking proposals receive 2 2 1 0 0 nominations respectively Assume target number winners k 2 If agents truthful select agent 1 2 However note agent 3 untruthfully nominate agent 4 instead agent 1 giving following nomination distribution 1 2 1 1 0 makes agent 3 tie second place agents 1 4 If select uniformly random agent 3 increase chance selection 3 manipulating review keeping k 2 agents winners violating strategyproofness deﬁned 0 1 Section 22 4 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 2 When ﬁrstorder weights weight reviewer affected rankings received bundle j1 j2 j3 green arrows rankings red arrows Hence reviewers panels affect weight On contrary reviewers outside panels bear effect w For interpretation colours ﬁgures reader referred web version article cid9cid9 cid9 An alternative approach select agents nomination This way agent 3 accepted regardless nominate required 2 nominations agent 3 selected independent review This approach strategyproof comes cost exactness select agents instead Example 2 shows aggregation protocols satisfy desirable properties particular strategyproofness exactness In paper develop algorithm relaxes exactness achieve strategyproofness However mild assumptions exactness guaranteed expectation We shall noisier scenarios employment weighting schemes helpful discriminate inaccurate reviewers empirically achieve higher recall 21 Weighting schemes Peer selection takes place presence noise types reviewer bias andor submitting short hastily prepared reviews 42 presence legitimately different evaluations quality proposals Hence desire algorithms able work situations high noise agents inaccurate assessment agents algorithms strict stiﬂe dissenting viewpoints In order provide reasonable outcome employ weighting schemes mitigate noise possible These schemes assign weights agent based reviews reﬂect little agree reviewers A weighting scheme function w cid4 0 1n When review proﬁle σ obvious context denote reviewer weight agent w A weighting scheme ﬁrstorder w depends comparative σk j Ak Thus rankings panels More formally w depends rankings reviewer inﬂuence weights copanelists reviewers panel illustrated Fig 2 We require weighting schemes deterministic preserve form neutrality input weighting scheme reviewers weights j Ai cid2 Notice Google PageRank 3622 example uses weights ﬁrstorder inﬂuence ranking propagated indeﬁnitely This detrimental algorithm change single ranking attempt strategising propagate reviewer weights making hard ensure strategyproofness For reason use ﬁrstorder weighting schemes PeerNomination 22 Properties Since agent wants proposal selected incentives selfinterested agents peer selection problem align socially optimal outcome selection best k agents according ground 5 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 3 Typical number errors nominations proposals actually k nominate 3 9 proposals function dispersion parameter n m ground truth ordering committed reviewer called strategyproof impartial agent f σ truth A peer selection algorithm f proﬁle σ f σ1 σ σn σ truthful In words deviating truthful reporting beneﬁcial agent In probabilistic context require agents able increase probability selected In addition strategyproofness mechanism like maintains properties anonymity permuting agents makes difference nonimposition set k accepted papers possible output monotonicity receiving better scores decrease probability selection 23 Noise model To model inaccuracies reviewers assessments assume agent associated noisy observation ground truth according Mallows model 28 Mallows models widely compare performance peer selection algorithms empirically 294 far studied mild levels noise signiﬁcantly affect reported rankings cid9 ϕ K T RR cid9 K T R R The Mallows model parameterised dispersion parameter ϕ 0 1 reference linear ranking R Given R cid9 ϕ model induces probability distribution permutations R probability linear order R cid9 Kendallτ distance R R πRφR The Kendallτ distance counts number pairwise disagreements rankings 21 Hence probability agent reporting additional pairwise disagreement reference ranking decreases exponentially Note vary dispersion parameter ϕ 0 1 probability distribution linear rankings moves concentrated R uniform possible rankings In simulations Section 6 ground truth reference ranking sample noisy ranking agent ϕ speciﬁed An important feature Mallows model sampled eﬃciently 275230 allows generate unique reviewer proﬁle experiment cid9 1R In addition test weighting schemes settings reviewers random actively contrarian ground truth Since Mallows model produces random rankings worst case ϕ 1 introduce simple extension agents tend ground truth Formally given reference ranking R cid9 dispersion parameter ϕ 1 2 extended Mallows model samples ranking R 2 ϕK T R 1 reverse linear order R probability πR12φR cid9 R For example set ϕ 12 assume agent reverse ground truth reference ranking sam ples ranking Mallows model ϕ 08 Thus distribution moves smoothly concentrated ground truth reverse ground truth uniform 1 It worth noting Mallows model behaves nonlinearly respect number errors committed reviewer In setup number errors proposals nominated fall outside k m n ground truth illustrated Fig 3 Unless dispersion parameter close 1 reviewers commit errors average Moreover signiﬁcant probability 3 nominations wrong arises consider ϕ 1 following contrarian extension cid9 3 Related work Using evaluations peers rank select winners problem broad CS AI including numerous practical domains conference journal grant reviewing large scale course grading group decision making Brought fore Merriﬁeld Saari 33 allocate telescope time US National Science Foundation problem deeply rooted economics extensive work continuous case Clippel et al 14 6 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 agents allocate fractions rewards discrete variants Dollar Raﬄe Dollar Partition suggested Aziz et al 4 In discrete case AI community work Alon et al 1 partition onwards Later Credible Subset method 23 mechanism examines possibility manipulations accounts suggested Despite strategyproofness inexact return selection shown happen signiﬁcant number cases 3 A prominent recent algorithm ExactDollarPartition EDP 4 provides exactness cost randomness remaining strategyproof improving main earlier algorithms A study Caragiannis et al 11 provides optimal nonimpartial algorithms ordinal peer ranking setting close considered paper In particular simple Borda mechanism optimal setting noise provide way construct optimal algorithm speciﬁc Mallowslike noise model This provides good benchmark testing impartial peer selection algorithms For example results Mattei et al 29 unweighted PeerNomination approaches recall Borda settings Similar algorithms multiagent systems communities include voting rules aggregate ranks kPartite 20 Committee Rule 20 DivideandRank 53 algorithms Others focus proving bounds quality given rank aggregation scheme noisy partial observations 10 Yet methods approvalbased focus single agent selection Permutation 17 Slicing 8 A key application area peer evaluation mechanisms education problems reviewer reliability bias extensively studied 38 We motivated evidence ﬁelded peer evaluation mechanisms showing students unwilling strictly rank assignments 16 rely scores passfail marks approvals Within conference journal reviewing ecosystem growing detecting strategic behaviour reviewers 4532 debiasing calibrating differences scores reviewers 4825 We calibration debiasing identifying suboptimal behaviour agents populations looking effect rescaling Outside peer selection extensive work machine learning information retrieval preference learning communities learning rank problem inferring likely ranking possibly noisy observations 26 These works include learning noise models parameters Mallows model use inferring latent preferences agents 2652 This great practical information retrieval wishes rank webpages based user clicks 41 combining labelling multiple sources construction datasets 51 However systems concern strategyproofness key focus study The notion weights systems applications example ﬁeld recommender systems reviewers customers incentive submit untruthful ratings 39 similar approach taken reputation 40 platforms Google Search form PageRank algorithm 3622 4 PEERNOMINATION In section formally present PeerNomination including design implementation reweighting mech anism We discuss tradeoffs arise mechanism introduction weights Finally present speciﬁc examples weighting schemes PeerNomination The complete PeerNomination algorithm given Algorithm 1 41 The PeerNomination algorithm A usual requirement peer selection mechanisms return set exactly size k 4120 Some ap proaches investigated relaxing assumption 323 notably Bjelde et al 6 relaxation lead better approximation optimal selection winners We use intuition relaxing exactness requirement improve recall PeerNomination returns winning set size approximately k expectation PeerNomination works follows suppose agent reviews reviewed m agents If agent true k apriori ground truth overall n agents expect ranked k proportion k n m review bundle majority agents review proposal reviewing agents report rankings perfectly We agent nominated reviewer k n proportion reviewers declared ranking review bundle We refer k n m nomination quota n m unlikely integer consider proposal nominated certain cid6 k n mcid7 proposals particular review bundle cid6xcid7 denotes positive real number x If proposal n mcid7 decimal position cid6 k nomination quota For illustration Fig 4 n mcid7 1 consider proposal nominated probability k n m cid6 k As k We use reviewers nominations induced rankings select winners As discussed ﬁrst use weighting scheme compute reviewer weights reviewers aim detect inaccurate reviewers assign lower weight We measuring reviewer disagrees copanellists consistent particular reviewers ranking agents reviewing proposal We compare 7 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 4 Each reviewer nominates quota agents n m ε Initialise nomCount 0 A 1 j σi j cid6nomQuotacid7 Algorithm 1 PeerNomination Input Assignment A review proﬁle σ target quota k slack parameter ε reviewer weights w1 wn Output Accepting set S 1 Set nomQuota k 2 j N 3 4 5 6 7 8 9 10 11 12 13 14 end 15 return S increment nomCount w probability nomQuota cid6nomQuotacid7 increment nomCount w σi j cid6nomQuotacid7 1 end nomCount A1 j w 2 S j end end cid3 b c cid11 Count nominations j received cid11 Select j weighted majority c e b d f Fig 5 A nonstrategyproof assignment Left strategyproof Right Algorithm 2 returns type Unit weighing scheme w 1 agents agents identical weights The weighting schemes discussed Section 43 The ﬁnal stage PeerNomination consists selecting proposal winner achieves weighted majority nominations In Unit case translates majority reviewers nominating A crucial observation proposal considered independently selection algorithm guaranteed return exactly k agents However algorithm select set size approximately k reviewers submit reviews close ground truth We PeerNomination truthtelling equilibrium outcome PeerNomination strategyproof The PeerNomination algorithm presented Algorithm 1 Note algorithm introduce slack parameter ε input extends nomination quota serves ﬁnetune algorithm performance As discuss Section 52 necessary settings achieve expected size k winning set 42 Review assignment One goals PeerNomination strategyproof impartial Theorem 1 How fairly straightforward nonweighted Unit case introduction weighting schemes requires care illustrated following example Example 3 Consider example Fig 5 Left agents b c reviewing arrows directed reviewer reviewed proposal Note agent b reviewing agent c In scenario agent b impact weight given agent manipulating evaluation agent c For example agent b expects agent nominate c choose nominate c discrediting Hence b inﬂuence agent role determining agent b themself selected 8 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Algorithm 2 EulerBased Assignment For simplicity present algorithm n This loss generality If n odd add dummy agent run algorithm n 1 agents m 1 assignments We remove dummy agent leaving degree m m 1 Input Set n agents review number m n4 Output Antitransitive mregular assignment A 1 Initialise G V E V n E 2 Partition V X Y X Y n2 3 x X 4 cid11 Make 2mregular bipartite graph GVE 1 2m arg min y y E E x y deg y y Y 5 6 end 7 8 end 9 Set eulerCycle HierholzersAlgorithmG 10 Set A n E A E A 11 v v i1 eulerCycle E A E A v v i1 12 13 end 14 return A cid11 Use Hierholzers alg ﬁnd Euler cycle cid11 Orient edges direction Euler The example shows cases mechanism strategyproof We review assignments avoid We henceforth refer review assignment avoids construction Fig 5 anti transitive Formally review assignment antitransitive agents b c reviews b b reviews c review c We present algorithm generating random antitransitive review assignments Section 51 algorithm correct Proposition 1 antitransitivity makes PeerNomination strategyproof Theorem 1 Algorithm 2 works follows ﬁrst randomly partitions agents 2 equally sized sets creates 2mregular bipartite graph based partition It orients edges traversing Euler tour graph Fig 5 Right yields mregular directed graph Notice Algorithm 2 Algorithm 1 depend studied isolation 43 Weighting schemes In section present weighting schemes addition Unit weighting scheme evaluate reliability reviewers Each weighting schemes satisﬁes ﬁrst order requirement described Section 21 Informally weighting schemes propagates information link review graph ensure PeerNomination remains strategyproof assignment generated Algorithm 2 The ﬁrst contain aggressiveness parameter allows ﬁnetune wish lower weights reviewers scheme identiﬁes problematic While weighting schemes consider ones based expectation maximisation EM algo rithms including GLAD 50 DawidSkene 15 PageRank 3622 methods satisfy ﬁrstorder requirement render PeerNomination strategyproof weighting schemes For reason consider methods paper focus strategyproof methodologies exploring loss recall strategyproof versus nonstrategyproof methods interesting direction future work Distance Distance weights distance agents review reviewers This distance calcu lated averaging individual differences reviewers nominated proposal Formally average distance reviewer reviewers di 1 σ j σ l j Then m2 1 diγ γ aggressiveness parameter exaggerates weights better distance weight w dist discrimination agents This understood equivalent Hamming distance framework computed nominations given reviewer given copanelists l A1 j j Ai cid3 cid3 Majority Errors Let nomination proposal error reviewer minority opinion reviewer nominates proposal majority panel nominate proposal majority rounding partial nominations closest integer More formally let majσ j cid4 cid3 1 0 A1 j σ j m2 majority agent j proﬁle σ Then deﬁne number errors reviewer errσ The weight deﬁned w majerr m δ aggressiveness parameter 1 δerrσ cid3 j Ai 1σ jcid16majσ j 9 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Step Step applies step function error rate errσ m deﬁned Majority scheme We choose thresholds t1 t2 error rate reaches t1 reduce weight reviewer 05 error rate reaches t2 reduce weight 0 Additionally scale threshold nomination quota plays bigger role error detection size review bundle m Formally w 1 05 0 errσ n t1 errσ k t1 n k t2 Unit We refer version PeerNomination weights ignored w 1 Unit PeerNomination 5 Theoretical analysis In section provide theoretical analysis PeerNomination We ﬁrst prove PeerNomination satisﬁes important axiomatic properties notably strategyproofness We derive analytic expressions expected recall output size PeerNomination case Unit weights Lastly provide motivation effectiveness weighting schemes introducing simpliﬁed model peer reviewing setting showing detecting inaccurate reviewers improve recall peer selection 51 Axiomatic properties Assigning weights reviewers based reviews introduces complexity potential manipulation illustrated Example 3 Thankfully consider ﬁrstorder weighting schemes need introduce simple condition review assignment maintain strategyproofness PeerNomination We ﬁrst condition suﬃcient prove Algorithm 2 guarantees Of course assignmentgenerating algorithm guarantees condition guarantee strategyproofness PeerNomination Theorem 1 PeerNomination strategyproof review assignment antitransitive weighing scheme ﬁrstorder Proof First consider Unit case weight set 1 independently reviews Under PeerNomination agent selected depends solely reviews receive reviewers interaction reviews Since agent reviews themself affect chances selection Now consider general case reviewers assigned weights based nominations The weights introduce interaction agents guarantee strategyproofness wish affect weight reviewers proposal improving chances selection Assume review assignment antitransitive weighing scheme ﬁrstorder Say agent proposal Since weighting scheme ﬁrstorder agent inﬂuence weights agent copanelists That inﬂuence weights agent j proposal cid11 j reviewing cid11 But assignment antitransitive agent j reviewer agent proposal Therefore change implemented nominations inﬂuence chances selected cid2 We observe Algorithm 2 instance assignment guarantees antitransitive assignment exist Indeed discussed Section 2 assume notions assignment quality work However interesting direction future work investigate antitransitive assignment requirement interact assignment quality 53 Proposition 1 Algorithm 2 produces antitransitive review assignment Proof We need agents j k review assignment reviews j j reviews k review k Consider review assignment produced Algorithm 2 suppose agents j k reviews j j reviews k All agents partitioned balanced bipartite graph X Y beginning algorithm loss generality assume X Since graph remains bipartite algorithm ﬁnal assignment simply orients edges j Y k X Hence k graph X edge In words review k cid2 We want algorithm monotonic having better reviews hurt chances selection 10 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Table 1 Review proﬁles lead agent selected nomination quota increased Reviewer 1 2 3 4 4 4 2 3 2 1 1 1 3 3 4 2 Weight Reviewer 0 1 1 1 1 2 3 4 4 4 2 3 Weight 0 0 0 0 2 1 1 1 3 3 4 2 b Proposition 2 PeerNomination monotonic Proof Suppose j reviewed compare probability selecting j given original review vs modiﬁed j ranked higher Note w 0 nomination nonnegative impact proposal There cases ii iii j inside integer nomination quota original ranking j modiﬁcation completely outside nomination quota modiﬁed review In cases j certain nominated nominated respectively probability change j moves fractional nominee nominee increasing chances nomination 1 kq cid6kqcid7 increasing chances selection j moves nominated fractionally nominated increasing chance nomination kq cid6kqcid7 increasing chances selection In cases js chances selection decrease completing proof cid2 Unit PeerNomination committee monotonic increasing target quota k hurt chances selection agent However longer true weighting schemes Proposition 3 PeerNomination Unit weights committee monotonic Proof Fix review assignment proﬁle suppose increase target number agents select k This increases nomination quota reviewer review proﬁle ﬁxed agents sum nominations decrease So previously selected proposal selected cid2 Proposition 4 PeerNomination committee monotonic Proof As counterexample present instance peerreview problem weighting scheme Consider instance given Table 1 objective underlying ground truth 1 cid5 2 cid5 3 cid5 4 Here set 4 agents reviews n 4 m 3 We augment PeerNomination simple weighting scheme reviewers nominations nominated reviewers set weight 1 set weight 0 In words disagreement reviewers discredit reviewers Now suppose k 4 3 giving nomination quota 1 review proﬁle given Table 1a Reviews accurate agent 3 placed agent 4 2 As shown table reweighting agent 1 nominated 3 times complete agreement agent 2 nominated However agent 1 nominated agent 2 weight set 0 effectively nullifying nomination Hence agent 1 selected Now suppose extend k 8 3 giving nomination quota 2 shown Table 1b Now reviewer nominates 2 proposals reviewer 1 proposal nominated Hence according weighting schemes reviewer receives weight 0 resulting nominations selection5 cid2 In addition PeerNomination trivially anonymous permuting agents matter satisﬁes nonimposition k proposals selected given appropriate nomination 52 Expected size slack parameter In order understand interplay number selected agents slack parameter derive expected size winning set returned PeerNomination function n m k assuming Unit weights 5 Technically PeerNomination uses nonstrict majority selection agent achieves selection threshold 0 However practical implementation select agents receive 0 nominations 11 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 noise The expression reached proposition face insightful But allow draw parameters effects Proposition 5 Assume Unit PeerNomination run peer review instance parameters n m k truthful proﬁle noise Then probability selection agent ground truth position r given P accept R r mcid9 icid17m2cid18 cid10 m cid11 r1 qrmi qi qrn m k agents probability nominated reviewer qr cid6kqcid7cid9 y1 P Y y R r cid13 cid12 kq cid6kqcid7 P Y cid6kqcid7 1 R r 1 2 Proof Recall algorithm run mregular assignment assume reviews truthful We assume assignment sampled uniformly review bundle equally likely assigned reviewer First consider probability position y sample size m given position r ground truth ranking When drawing sample need choose y 1 individuals r 1 agent r ground truth choose m y n r worse In total expected choosing m 1 agents n 1 Hence P Y y R r cid10 cid11cid10 r 1 y 1 n r m y cid11cid14cid10 cid11 n 1 m 1 Y random variable representing position review bundle R random variable representing ground truth position In order proceed analysis need simplifying assumption probability independent bundle This true general6 empirical data shows effect negligible large n Denote nomination quota kq k n m recall given review bundle cid6kqcid7 agents nominated certain position nominated probability kq cid6kqcid7 Hence probability nominated bundle position r ranking independently qr cid6kqcid7cid9 y1 P Y y R r cid13 cid12 kq cid6kqcid7 P Y cid6kqcid7 1 R r Since review bundle regarded Bernoulli trial probability qr accepted agent nominated cid17m2cid18 times probability accepted position r given cumulative Binomial distribution P accept R r mcid9 icid17m2cid18 cid10 m cid11 r1 qrmi cid2 qi An illustration acceptance probabilities function ground truth position shown Fig 6 We agents inside k certain accepted outside k certain rejected The width interval k probability away extremes dictated m Higher m reduces uncertainty providing trials agent narrows interval We use derived probability acceptance calculate expected size accepting set Since individual accepted independently probability P accept R r contributes 1 size accepted expected size given Eaccepting size ncid9 r1 P accept R r The complexity expression makes diﬃcult analyse explicitly However Fig 7a shows typical behaviour expected size function m7 We observe approaches k m increases However small values m 6 Suppose peer selection instance n 5 m 3 agents labelled ground truth position There 5 review bundles total agent Agent 1 3 placed ﬁrst according accurate reviewers Now consider agent 2 The probability placed ﬁrst according calculation 1 2 bundles contain agent 1 probability agent 2 placing ﬁrst bundles 0 7 Note ﬁgures yaxis begins 26 variations milder cursory look implies 12 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 6 Probability accepted algorithm given position ranking n 130 k 30 Fig 7 Expected size accepting set returned algorithm n 130 k 30 varying m b Expected recall accepting size n 130 m 9 ε 015 varying k The green line shows expected accepting size blue line shows recall c The slack parameter ε required achieve expected accepting size k We computed ε method outlined Section 54 n 130 k 30 varying m expected size vary signiﬁcantly k especially m odd recall agents need clear majority case making selection diﬃcult To tackle issues introduce additional parameter ε slack parameter allows control size accepting set ﬁnely If ε set nonzero value usually positive extend nomination quota review bundle Usually increment simply contributes probability fractional nominee nominated For example setting n 130 m 9 k 30 Fig 7a shows expected size slightly 27 aim 30 Setting ε 013 yields expected size close 30 For practical applications ε 005 015 shown Fig 7c meaning original algorithm wellbehaved Note contrast inexact mechanisms literature Credible Subset return solutions positive probability 23 Dollar Partition method return additional agents number clusters 3 The analysis assumes reviewers accurate weights set 1 If assumptions fail provide guarantees expected size accepting set It straightforward construct marginal cases selected worst case scenario example 13 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 8 n 100 k 25 m 8 Each plot shows sum nominations agent respective ground truth position averaged 1000 simula tions The red line shows selection threshold m 2 Example 4 Consider setting 3 agents reviewing suppose want select individual n 3 m 2 k 1 Suppose agent 1 reviews 2 3 agent 2 reviews 3 1 agent 3 reviews 1 2 The nomination quota ε 0 2 3 agent ranked ﬁrst place Hence agent selected probability 2 3 independently exists realisation selected selected To ensure algorithm returns reasonable number agents expectation need assumptions population agents Suppose large number agents n common acceptance rate 20 k 02n If reviewers random expect nominations spread evenly agents agent receiving 02m nominations expectation Hence agents achieve threshold m 2 selected However realistic scenario reviewers able discriminate agents likely good number agents reach required acceptance threshold Fig 8 The unfavourable performance noisy settings motivation reviewer weights seen particular empirical Section 6 Also note deﬁnition algorithm stipulate ε input One tempted calculate ε collecting reviews order adjust output size exactly k undesirable reasons First run algorithm nondeterministic impossible ﬁnd value ε guarantees output size run Second importantly eliminate strategyproofness agent estimate reporting particular untruthful review force mechanism increase ε increase chances selection In section derive expected recall algorithm Section 54 practical guidance setting slack parameter 53 Expected recall In Section 52 derived probability acceptance given position ground truth ranking assuming noise reviewers reported rankings In section modify expression Proposition 5 include ε slack parameter To update nomination quota computing qr Equation 2 Hence let kε q kq ε qε r cid6kε cid7cid9 q y1 P Y y R r cid12 kε q cid13 cid7 cid6kε q P Y cid6kε q 1cid7 R r 3 This gives P εaccept R r ground truth position simply replacing qr Equation 1 qε size given similar expression r The expected Eaccepting size ncid9 r1 P εaccept R r 4 In principle derive expected performance algorithm However algorithms output inexact multiple performance measures consider case classiﬁcation algorithms 5 For example care agents k according ground truth selected recall 14 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 want select agents outside k false positive rate We focus referred accuracy Aziz et al 4 The connection classiﬁcation metrics exact deﬁnitions explored Section 61 In following theorem provide analytic expression expected recall PeerNomination Theorem 2 The expected recall Unit PeerNomination setting n m k Erecall 1 k kcid9 r1 P εaccept R r P εaccept R r probability acceptance agent ground truth position r given 3 Proof Consider single run PeerNomination Let Xi random variable Xi 1 agent accepted algorithm Xi 0 Since agent k positions contributes 1 recall 0 recall equal 1 k i1 Xi Now Xi Bernoulli random variable8 E Xi P Xi 1 Finally 1ik Xi 1 k cid3 n i1 cid3 k Erecall 1 k kcid9 r1 P Xr 1 1 k kcid9 r1 P εaccept R r required cid2 Again complexity expressions hinders theoretical analysis Fig 7b shows typical output different values k While performance appears good isolation important compare PeerNomination peer selection mechanisms Section 6 54 Using slack parameter practice The analytic expression expected accepting size PeerNomination given Equation 4 allows derive practical way estimate slack parameter ε Given setting n m k let f ε Eaccepting size given Equation 4 Since want f ε k estimate required slack parameter simply need ﬁnd root function gε f ε k Since f highly nonlinear continuous analytic solution root unlikely easy compute good approximation root obtained quickly obtainable However f rootﬁnding algorithm Brents method Indeed Section 6 use method estimate ε running PeerNomination 55 Effect weights The constructive use weighting schemes Algorithm 1 depends ability identifying accurate inaccurate reviewers identiﬁcation reweight reviews Not knowing ground truth means identiﬁcation accurateinaccurate agents depend comparing different agents submitted rankings nominations If agents accurate reweighting agents needed proportion accurate agents drops problem diﬃcult Still large majority agents accurate correct opinion usually majority inaccurate agents providing random rankings However number accurate agents low agents actively malicious identiﬁcation impossible metric evaluate agents To provide intuition conceptual underpinnings algorithm present simpliﬁed setting algorithm simple conservative weighting scheme able improve PeerNomination We start mregular assignment agent types A meaning agent accurate reviewer A meaning agent inaccurate Recall PeerNomination proposal selected majority reviewers approve We simple dynamic weighting scheme relying knowing times agent minority good chance ﬂipping decision A agents A agents improving PeerNomination Let Aagent identiﬁed inaccurate hold minority opinion j panels We want ﬁnd probability following event 1 agents panel consists Amajority 2 Aagents majority identiﬁed inaccurate We wish use simpliﬁed model derive mathematical expressions probability surely identifying bad reviewer seeing worthwhile improvement algorithm While mathematical expressions reach easily analyzable examining empirically shows simple model reach meaningful values 8 As explained Section 52 Xi exactly independent simplifying assumption reasonable large n 15 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 9 The probability identifying inaccurate agent m 9 threshold identiﬁcation j Given noise model let q probability agent type A Then probability agent reviewed majority A agents majority size k qBk P Amajority size k cid6m2cid7k1 qmcid6m2cid7k q cid10 cid11 m cid6m2cid7 k In case like identify k A agents inaccurate order nullify votes We ﬁnd probability Aagents majority size qB P Amajority cid17m2cid18cid9 qbk cid6m2cid7cid9 k1 i0 cid11 1 qiqmi cid10 m Our simple weighting algorithm labels agent Aagent minority j panels majority panel decreasing weight change The probability event qdet given cumulative binomial probability keeping mind probability Amajority panel conditioned fact contain Aagent cid9 A q P Amajority A cid6m2cid7cid9 cid11 cid10 m 1 1 qiqm1i qdet i0 m1cid9 j cid11 cid10 m 1 q cid9 Ai1 q cid9 Am1i Notice A inaccurate adversarial case ﬂip nomination need k However safer approach means need detect 2k Aagents correct decision The probability correcting event given following expression P correction event qbk cid17m2cid18cid9 cid6m2cid7kcid9 cid10 k1 i2k cid11 cid6m2cid7 k det1 qdet qi cid6m2cid7ki As desired signiﬁcant probability correction event illustrated Fig 9 As seen wide variety q j conservative weighting scheme produces reasonably high probability improving reviews As shall designed weighting schemes Section 43 examined simulations better results achieved It noted produce analogous probabilities weighting scheme incorrectly identifying A agents Aagents However large j j m2 majority A agents q n2 number smaller beneﬁt reweighting positive 6 Empirical analysis In section use experimental framework demonstrate PeerNomination outperforms mechanisms proposed In draw novel connection inexact peer selection literature classiﬁcation machine learning 5 16 O Lev N Mattei P Turrini et al 61 Classiﬁcation measures Artiﬁcial Intelligence 316 2023 103843 The usual intuitive way measure accuracy exact peerselection mechanism counting agents k positions ground truth selected proportion k agents selected This allows compare exact peerselection mechanisms Aziz et al 4 However comparison inexact mechanisms obviously Since accepting set guaranteed exactly size k output k agents artiﬁcially increase performance inexact mechanism opposite smaller output One option measure performance proportion output size approach overrate outputs accurate smaller k Inexactness allows view peer selection classiﬁcation problem selection means positive classiﬁcation We view selected agents true k true positives nonselected agents outside true k true negatives We apply standard classiﬁcation performance measures 5 recall precision PeerNomination analyse performance More formally let S set agents selected algorithm S agents true k true positives TP Similarly use S FP S FP Hence deﬁne TP S negatives FN r S rankr k n S TN r S rankr k set selected r S rankr k false positives S TP true negatives TN r S rankr k n k FP false We look typical performance metrics Positive Predictive Value PPV aka Precision True Positive Rate TPR aka Recall False Positive Rate FPR deﬁned follows PPV TP TPR TP FPR FP TP FP TP FN TN FP To nomination quota affects parameters PeerNomination use PrecisionRecall PR ReceiverOperator Characteristic ROC curves These curves trade sensitivity TRP inclusivity FPR We use slack parameter ε sensitivity threshold akin probability threshold machine learning literature 18 So vary ε nomination quota varies 0 m measure Precision Recall False Positive Rate value An example presented Fig 11 The curves trade sensitivity TRP inclusivity FPR As follow ROC curve corresponds gradually increasing nomination quota TPR increases quickly That adding extra agents improve signiﬁcantly selection true k proposals On hand achieve TPR 08 FPR close 0 This shows select 80 proposals true k concentrate selecting undeserving individuals fall outside true k While curves interesting want able compare peerselection mechanisms important direction ﬁnding generalizable way constructing curves peerselection mechanisms 62 Experimental setup We extend testing framework developed Aziz et al 4 methods PrefLib 31 As Aziz et al 4 set n 120 tested algorithm values k m The test values k 15 20 25 30 35 test values m 5 119 For setting tested algorithms noise levels For comparison included PeerNomination paired weighting schemes introduced Section 43 including Unit ExactDollarPartition stateoftheart strategyproof peer selection algorithm Mattei et al 29 provides comparison Unit PeerNomination peer selection algorithms lownoise setting Both ExactDollarPartition PeerNomination Algorithm 2 rely partitionbased assignments However Algorithm 2 partitions agents 2 clusters ExactDollarPartition tends perform best number clusters 4 For reason decided generate mregular assignments l 4 clusters algorithm Aziz et al 4 This weighted PeerNomination nonstrategyproof ensures performance ExactDollarPartition crippled ensure fair comparison In practice performance PeerNomination depend type assignment We include performance PeerNomination paired Algorithm 2 separately Fig 14 In fact observe PeerNomination tends perform slightly better assignment generated Algorithm 2 We use different settings model noise reviewers random reviewers contrarian adversarial reviewers In settings partition population accurate inaccurate random contrarian reviewers generate individual noisy rankings Mallows noise 28 discussed Section 23 In random case dispersion parameter accurate reviewers ϕ 05 random reviewers ϕ 1 In contrarian case values 08 12 respectively In experiments gradually increase proportion inaccurate reviewers test robustness algorithms noise Note reviewers individual rankings noisy assume reporting truthful respect beliefs Since tested algorithms strategyproof truthful proﬁle equilibrium 9 In Figs 12 13 14 present results k 25 m 9 representative settings 17 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 10 Spearman correlation weights different weighting schemes underlying ϕ agent The bars represent mean standard deviation 1000 simulations weights To summarise single simulation consists following steps 1 Generate random mregular assignment matching reviewers proposals 2 Determine type accurate inaccurate rankings reviewer bundle proposals Mallows model 3 Run algorithm generated instance measure performance precision recall The experiment repeated 1000 times setting average recall calculated giving high conﬁdence results For PeerNomination theoretical estimates ε achieve right expected size accepting set The error bars Figs 12 13 represent 1 standard deviation We observe test ExactDollarPartition given access partial noisy rankings Mallows model PeerNomination simple rule reviewers approve half reported order Hence sults PeerNomination capable performing better ExactDollarPartition presence information An interesting direction future work complete analysis possible reporting spaces partial rankings rankings utilities approvals impact reports overall algorithm performance In testing setup adopted slightly different procedure order ensure fair comparison In simu lation generate random instance run PeerNomination target k input measure actual size output run EDP actual winning set size input target size k EDP This ensures simulation algorithms return number winning proposals The results comparison presented Fig 13 63 Results 631 Random reviewers Fig 12a compares performance PeerNomination selected weighting schemes presented Section 43 ExactDollarPartition It observed proportion accurate reviewers high 08 10 range tested weighting schemes practically improvement Unit This setting barely noise weighting schemes behave desired overﬁt It observed weighting schemes outperform ExactDollarPartition When proportion random reviewers rises 04 06 range Unit PeerNomination underperforming compared weighting schemes At 02 imbalance decreases fur ther For instance PeerNomination Distance 228 times accurate PeerNomination Unit We advantage weighting schemes EDP evident 04 setting despite lower output size PeerNomination Distance achieves higher recall The ability weighting schemes discriminate reviewers supported Fig 10 demonstrates metrics strongly correlate underlying ϕ This means metrics able identify inaccurate reviewer reasonable certainty In general weighting schemes better Unit keeping output size close desired k Distance keeping output size consistent levels noise At time Units output size reduced drastically factor 4 explained Fig 8 noisier reviewers tend agree nominations meaning nominations spread proposals reach selection threshold In addition greater recall Distance weighting schemes indicates additional selected agents usually deserving ones Between weighting schemes Distance manages gain advantage noise levels increase ﬁnegrained aggressive decreases weight inaccurate reviewers severely compared schemes This allows Distance identify inaccurate reviewers maintain consistent output size It noted 18 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 11 ROC PR curves PeerNomination They computed empirically n 120 m 8 k 25 Fig 12 Performance comparison n 120 k 25 m 9 19 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 13 Results forced size experiment PeerNomination run ﬁrst set EDPs target size guaranteed return number agents The parameters set n 120 k 25 m 9 noise low Distance tends slightly worsen performance For instance 1 staying EDP PeerNomination Distance achieves 4 lower recall PeerNomination Unit 632 Contrarian reviewers When work settings contrarian reviewers weighting schemes effective The results study shown Fig 12b Notice analyse proportion accurate reviewers 05 keeping contrarians minority Again low noise levels 1 results match expected observations previous paragraph As proportion contrarian reviewers rises 07 point observe schemes outperform EDP Distance reaching 20 performance increase compared EDP Interestingly moderate levels noise 07 08 Distance shows similar performance case random reviewers Even reviewers average diverging easier detect Even half population contrarian Distance gets impressively close theoretical maximum 50 shown Fig 12b Beyond point contrarian point view majority way retrieve original ground truth It ﬁnally worth noting graphs Figs 12a 12b PeerNomination tends return slightly larger k set average noiseless setting usually 1 additional agent This im pression results inﬂate performance PeerNomination compared exact algorithm 20 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 Fig 14 Performance comparison n 120 k 25 m 9 contrarian setting PeerNomination weighting schemes Here assignment iteration generated Algorithm 2 ensuring mechanisms strategyproof ExactDollarPartition provided extra agents chosen correctly Section 633 perform fair com parison shows makes negligible difference settings 633 Fair tests The fair testing setup described allows test inexact algorithm output target number agents Hence chose compare best performing weighting scheme PeerNomination Distance ExactDollarPartition The results fair test Fig 13 align previous ﬁndings In noiseless setting ExactDollarPartition gain advantage PeerNomination paired Distance likely overﬁtting weighting scheme However level noise increases Distance gets clear advantage ExactDollarPartition For instance 40 reviewers contrarian PeerNomination sees 34 increase recall ExactDollarPartition The advantage particularly stark contrarian setting Distance beneﬁts greatly ability identify reweight inaccurate reviewers 7 Discussion conclusions We proposed novel strategyproof peer selection algorithm PeerNomination weighs reviewers based perceived accuracy The basis reweighting observation cases ones reviews correlated quality use correlation improve recall overall algorithm We develop weighting methods showing straightforward ones reach high quality outcomes high levels noise reported rankings reviewers Hence shown PeerNomination achieves stateoftheart performance problem peer selection Given PeerNomination constructed modular way variety weighting evaluation methods developed particular settings noise models This modularity allows multiple directions future development One possible direction future work exploring weighting schemes developed far optimal As seen results different schemes perform best different conditions For example aggressive approach come majority reviewers inaccurate lose forgiving weighting noise reported rankings Indeed Distance aggressive approach promising weighting scheme share good reviewers high Majority Step simpler easier calculate administer While examined weighting schemes variants Mallows model weighing schemes behave differently different distributions In particular Random Utility Models RUMs 2 extensively social choice provide interesting alternative consider Moreover particular peer review setting assumptions sources noise possible develop weighing scheme adapts particular situation Even proposed weighting schemes free parameters optimised Review ers assessment patterns learnt time use information input PeerNomination The use convolutional neural networks infer peer assessment patterns currently study 34 On hand weighting schemes easily decoupled form PeerNomination adapted strategyproof peer selection mechanisms For example considering good performance ExactDollarPartition noisy conditions interesting beneﬁts reweighting sim 21 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 ilar way PeerNomination Note run test explicitly paper adapting ExactDollarPartition use reweighting schemes require completely redesign ExactDollarPartition clustering assignment mechanisms Observe PeerNomination Algorithm 2 ensures particular structure allocation papers algorithm work ExactDollarPartition Speciﬁcally kpartition balanced assignment procedure ExactDollarPartition 3 partitions result case illustrated Fig 5 causing proposed weighting schemes impartial ExactDollarPartition There avenues additional theoretical work summarising quantifying effects strategyproofness peer review mechanisms effects weighting schemes We seen different tradeoffs relaxing exactness imposing constraints review assignment employed ensure strategyproofness It important spec ify assumptions precisely possible quantify effect performance peer selection theoretically For example direction future work lies evaluating nonstrategyproof reweighting methods ones based ex pectation maximisation EM algorithms including GLAD 50 DawidSkene 15 PageRank 3622 While methods maintain ﬁrstorder requirement strategyproofness setting interesting evaluate recall methods compared strategyproof methods In context peer reviewing strategyproofness nonnegotiable desideratum strove designing algorithm presence noise Our assignment weights selected way incentive reviewers gain advantage discrediting reviewers submitting insincere reports peer selection mechanisms practice allow 33 We use nonstrategyproof systems benchmark compare performance algorithm ideal optimum advocate use systems practice Although scope paper believe study nonstrategyproof systems lead important discoveries terms cost strategyproofness sacriﬁcing terms optimality order deploy systems reviewers incentive lie This lead acceptable weakenings strategyproofness optimality gains proved signiﬁcant Currently theoretical guarantees strategyproof peer selection algorithms produce close optimal results respective constraints Likewise know relaxation strategyproofness way obtain signiﬁcant gains terms recall Hence unclear focus lie improving recall current mechanisms developing new mechanisms rely weaker assumptions We believe important future direction research peer selection If exactness strategyproofness objective little reason Bordalike mech anism pairwise Shah Wainwright 44 complete rankings 13 Additionally past work including Mattei et al 29 Aziz et al 4 Kahng et al 20 provide empirical comparison Borda strategyproof mechanisms sheds light tradeoff exactness strategyproofness Theoretical quantiﬁcation tradeoffs exactness impartiality optimality exciting direction future work Additionally use strategyproofness mechanisms question exactly agents incentivised report 45 An interesting future direction analyse mechanisms mechanism design viewpoint 35 align incentives agents way strategyproof Finally lack realworld data analysis ﬁeld peer selection The challenge lies domains considered academic peer review best available ground truth acquired subjective opinions For instance evaluate submissions major conference independent expert panel evaluate agree ranking thousands papers guarantee best possible approximation ground truth Nevertheless validation realworld data better idea true performance current mechanisms help create realistic artiﬁcial models Declaration competing The authors declare known competing ﬁnancial interests personal relationships appeared inﬂuence work reported paper References 1 N Alon F Fischer A Procaccia M Tennenholtz Sum strategyproof selection selectors Proceedings 13th Conference Theoretical Aspects Rationality Knowledge TARK 2011 pp 101110 2 H Azari D Parks L Xia Random utility theory social choice Adv Neural Inf Process Syst 2012 25 3 H Aziz O Lev N Mattei JS Rosenschein T Walsh Strategyproof peer selection mechanisms analyses experiments D Schuurmans MP Wellman Eds AAAI AAAI Press 2016 pp 397403 4 H Aziz O Lev N Mattei JS Rosenschein T Walsh Strategyproof peer selection randomization partitioning apportionment Artif Intell 275 2019 295309 httpsdoi org 10 1016 j artint 2019 06 004 5 CM Bishop Pattern Recognition Machine Learning Springer 2006 6 A Bjelde F Fischer M Klimm Impartial selection power choices ACM Trans Econ Comput 5 4 2017 120 httpsdoi org 7 J Bohannon Whos afraid peer review Science 342 6154 2013 6065 8 N Bousquet S Norin A Vetta A nearoptimal mechanism impartial selection Proceedings 10th International Workshop Internet Network Economics WINE Lecture Notes Computer Science LNCS 2014 pp 133146 9 S Brams P Fishburn Approval voting Am Polit Sci Rev 72 1978 831847 10 1145 3107922 22 O Lev N Mattei P Turrini et al Artiﬁcial Intelligence 316 2023 103843 10 I Caragiannis GA Krimpas AA Voudouris Aggregating partial rankings applications peer grading massive online open courses Pro ceedings 2015 International Conference Autonomous Agents Multiagent Systems AAMAS ACM 2015 pp 675683 11 I Caragiannis GA Krimpas AA Voudouris How effective simple ordinal peer grading ACM Trans Econ Comput 8 3 2020 https doi org 10 1145 3412347 12 L Charlin RS Zemel C Boutilier A framework optimizing paper matching UAI 2011 Proceedings TwentySeventh Conference Uncertainty Artiﬁcial Intelligence AUAI Press 2011 pp 8695 13 W Chen R Zhou C Tian C Shen On topk selection mwise partial rankings Borda counting IEEE Trans Signal Process 70 2022 20312045 14 G Clippel H Moulin N Tideman Impartial division dollar J Econ Theory 139 2008 176191 15 AP Dawid AM Skene Maximum likelihood estimation observer errorrates em algorithm J R Stat Soc Ser C Appl Stat 28 1 1979 2028 pp 803820 16 L De Alfaro M Shavlovsky Crowdgrader tool crowdsourcing evaluation homework assignments Proceedings 45th ACM Technical Symposium Computer Science Education ACMCACM 2014 pp 415420 17 F Fischer M Klimm Optimal impartial selection Proceedings 15th ACM Conference Economics Computation ACMEC 2014 18 PA Flach Machine Learning The Art Science Algorithms That Make Sense Data Cambridge University Press 2012 httpwwwcambridge org academic subjects computerscience pattern recognition machine learning machine learning art science algorithms sense data 19 S Jecmen H Zhang R Liu NB Shah V Conitzer F Fang Mitigating manipulation peer review randomized reviewer assignments H Larochelle M Ranzato R Hadsell M Balcan H Lin Eds Annual Conference Neural Information Processing Systems 2020 NeurIPS 2020 20 A Kahng Y Kotturi C Kulkarni D Kurokawa A Procaccia Ranking wily people rank Proceedings 32nd AAAI Conference Artiﬁcial Intelligence AAAI 2018 21 MG Kendall A new measure rank correlation Biometrika 30 12 1938 8193 22 JM Kleinberg Authoritative sources hyperlinked environment J ACM 46 5 1999 604632 23 D Kurokawa O Lev J Morgenstern AD Procaccia Impartial peer review Proceedings 24th International Conference Artiﬁcial Intelligence IJCAI15 AAAI Press 2015 pp 582588 httpdl acm org citation cfm id 2832249 2832330 24 J Langford The NIPS experiment httpscacm acm org blogs blog cacm 181996 nips experiment fulltext 2015 25 JW Lian N Mattei R Noble T Walsh The conference paper assignment problem order weighted averages assign indivisible goods Proceedings 32nd AAAI Conference Artiﬁcial Intelligence AAAI 2018 pp 11381145 26 TY Liu Learning Rank Information Retrieval Springer Science Business Media 2011 27 T Lu C Boutilier Learning mallows models pairwise preferences Proceedings 28th International Conference International Conference Machine Learning ICML 2011 pp 145152 28 CL Mallows Nonnull ranking models I Biometrika 44 12 1957 114130 29 N Mattei P Turrini S Zhydkov Peernomination relaxing exactness increased accuracy peer selection Proc International Joint Conference Artiﬁcial Intelligence IJCAI 2020 pp 393399 ijcaiorg 30 N Mattei T Walsh PrefLib library preferences Proceedings 3rd International Conference Algorithmic Decision Theory ADT 2013 31 N Mattei T Walsh A PrefLibOrg retrospective lessons learned new directions U Endriss Ed Trends Computational Social Choice AI 32 R Meir J Lang J Lesca N Kaminski N Mattei A marketinspired bidding scheme peer review paper assignment Proceedings 35th AAAI httpwwwpreﬂib org Access Foundation 2017 pp 289309 Conference Artiﬁcial Intelligence AAAI 2021 10 1111 j 1468 4004 2009 50416 x 33 M Merriﬁeld D Saari Telescope time tears distributed approach peer review Astron Geophys 50 4 2009 416420 httpsdoi org 34 AA Namanloo J Thorpe A SalehiAbari Improving peer assessment graph convolutional networks httpsarxivorg abs 211104466 2021 35 N Nisan T Roughgarden E Tardos VV Vazirani Algorithmic Game Theory Cambridge University Press Cambridge 2007 36 L Page S Brin R Motwani T Winograd The PageRank Citation Ranking Bringing Order Web Technical Report 199966 Stanford InfoLab 1999 httpilpubs stanford edu 8090 422 previous number SIDLWP19990120 37 C Piech J Huang Z Chen C Do A Ng D Koller Tuned models peer assessment moocs arXiv preprint arXiv13072579 2013 38 C Piech J Huang Z Chen CB Do AY Ng D Koller Tuned models peer assessment moocs Proceedings 6th International Conference 39 P Resnick R Sami The inﬂuence limiter provably manipulationresistant recommender systems Proceedings 2007 ACM Conference Educational Data Mining EDM 2013 pp 153160 Recommender Systems 2007 pp 2532 40 J Sabater C Sierra Review computational trust reputation models Artif Intell Rev 24 1 2005 3360 41 T Schnabel A Swaminathan PI Frazier T Joachims Unbiased comparative evaluation ranking functions Proceedings 2016 ACM Interna tional Conference Theory Information Retrieval 2016 pp 109118 42 NB Shah KDD 2021 tutorial systemic challenges solutions bias unfairness peer review F Zhu BC Ooi C Miao Eds KDD 21 The 27th ACM SIGKDD Conference Knowledge Discovery Data Mining Virtual Event Singapore August 1418 2021 ACM 2021 pp 40664067 43 NB Shah B Tabibian K Muandet I Guyon U Von Luxburg Design analysis NIPS 2016 review process J Mach Learn Res 19 1 2018 19131946 44 NB Shah MJ Wainwright Simple robust optimal ranking pairwise comparisons J Mach Learn Res 18 1 2017 72467283 45 I Stelmakh NB Shah A Singh Catch I detecting strategic behaviour peer assessment arXiv preprint arXiv2010 04041 2020 46 I Stelmakh NB Shah A Singh H Daumé III A novicereviewer experiment address scarcity qualiﬁed reviewers large conferences Thirty Fifth AAAI Conference Artiﬁcial Intelligence AAAI Press 2021 pp 47854793 47 T Walsh The PeerRank method peer assessment Proceedings 21st European Conference Artiﬁcial Intelligence ECAI Prague Czech Republic 2014 pp 909914 48 J Wang NB Shah Your 2 1 3 9 handling arbitrary miscalibrations ratings E Elkind M Veloso N Agmon ME Taylor Eds Proceedings 18th International Conference Autonomous Agents MultiAgent Systems AAMAS IFAAMAS 2019 pp 864872 49 J Wang I Stelmakh Y Wei NB Shah Debiasing evaluations biased evaluations arXiv preprint arXiv2012 00714 2020 50 J Whitehill P Ruvolo T Wu J Bergsma J Movellan Whose vote count optimal integration labels labelers unknown expertise Proceedings 22nd International Conference Neural Information Processing Systems 2009 pp 20352043 51 J Whitehill Tf Wu J Bergsma J Movellan P Ruvolo Whose vote count optimal integration labels labelers unknown expertise Advances Neural Information Processing Systems 2009 pp 20352043 52 L Xia Learning DecisionMaking Rank Data Synthesis Lectures Artiﬁcial Intelligence Machine Learning Morgan Claypool 2019 53 Y Xu H Zhao X Shi NB Shah On strategyproof conference peer review Proceedings 28th International Joint Conference Artiﬁcial Intelligence IJCAI Macau 2019 pp 616622 23