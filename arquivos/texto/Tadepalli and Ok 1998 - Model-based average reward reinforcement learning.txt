ELSEVIER Artificial Intelligence 100 1998 177224 Artificial Intelligence Modelbased average reward reinforcement learning Prasad Tadepalli DoKyeong Ok b2 Department Computer Science Oregon State University Corvallis OR 97331 USA Korean Army Computer Center NonSanSi DuMaMyeon BuNamRi iOBox 29 ChungNam 320919 South Korea Received 27 September 1996 revised 15 December 1997 Abstract RL Reinforcement improve performance domains discounted natural criterion study programs environment Most RL methods optimize Learning rewards punishments dis receiving total reward received agent counted optimize average reward time step In paper introduce modelbased Average reward Reinforcement Learning method called Hlearning converges quickly domain scheduling simulated Automatic robustly Guided Vehicle explores unexplored parts state space choosing greedy actions respect current value function We Autoexploratory HLearning performs better previously larger state spaces extend learn action models reward functions form dynamic Bayesian networks approximate effective faster AGV scheduling value function local linear regression We extensions converge significantly making tasks 1998 Published Elsevier Science BV AGV We introduce version Hlearning strategies To scale Hlearning studied exploration automatically counterpart Hlearning requirement space reducing Keywords Machine networks Linear regression AGV scheduling learning Reinforcement learning Average reward Modelbased Exploration Bayesian author Email Corresponding Most work authors Oregon State University Email okdounitelcokr tadepallcsorstedu 00043702981900 PII SOOO4370298000022 1998 Published Elsevier Science BV All rights reserved 178 P 7hdepulli D OkArtificial Intelligence 100 1998 177224 1 Introduction receives equivalent scheduling learner study programs rewards punishments Reinforcement Learning EU task receiving improve perfor environment RL tasks elevator scheduling optimization money learning good procedures including Qlearning 31 optimize automatic tasks jobshop learning ARTDP 181 In words reward mance successful including realworld 1 14447 Most approaches reinforcement Adaptive RealTime Dynamic Programming counted reward time step considered immediately Discounted ward interpreted situation run terminated etary aspect probability average counted average ward action sequences 461 total dis received fraction reward received time step Another given time reason However domains mon time scales domains received time step Even people dis optimize learning total better algorithms reason discounted sequence actions rewards Hence choose use reinforcement immediate The natural domains aiming criterion earn state compared criterion finite infinite learning optimize termination criterion motivated domains fixed probability wellsuited 21261 One reinforcement model interested reward reward like encourages learner factor discounting While mathematically convenient domains isnt natural interpretation longterm benefits reward decreases optimization averagereward sacrifice discount shortterm gains impact action choice longterm exponentially time Hence discounted optimization argued optimizes 1 21261 This raises appropriate nearly close question discounted RL methods appropriate average lead suboptimal policies Nevertheless reward discount required optimize discounted average reward use optimize sufficiently factor total reward version shortterm undiscounted 31 We compare Hlearning In paper Averagereward RL ARL method called Hlearning Programming Adaptive RealTime Dynamic discounted simulated Automatic Guided Vehicle ARTDP task scheduling dling robot manufacturing Our results Hlearning ARTDP icy optimizes different ARTDP fails converge converges counterpart ARTDP AGV material han competitive factor optimal pol average reward When shortterm longterm optimal policies policy factor high Like ARTDP unlike Schwartzs Rlearning modelbased 381 Hlearning models We AGV scheduling domain Hlearning steps Rlearning 37 Singhs ARL algorithms learns uses explicit action reward fewer competitive CPU time This consistent discounted small discount optimal averagereward discount slowly converges I Tadepdli D OkArtificiul Intelligence 100 1998 177224 179 previous RL 328 results comparisons modelbased modelfree discounted Like RL methods Hlearning needs exploration optimal policy including recencybased 45 Other meth random actions preferring IE method Kaelbling strategies studied recently executed occasionally RL visit states visited counterbased reward functions Koenig Simmons uncertainty A number exploration executing executing actions ods Interval Estimation idea incorporate representation value function states optimism high values gradually decreasing explore automatically executing greedy actions We introduce version Hlearning exploring respect converges quickly random counterbased current value function We Autoexploratory Hlearning Hlearning strategies unexplored parts state space taking greedy action better average reward compared Boltzmann recencybased value function uses optimism uncertainty property automatically methods encourage 17201 By initializing actionpenalty ARL methods exploration learner stored table require scale large state spaces Modelbased methods space like Hlearning time training additional reward essential problem having explicitly functions spaceintensive To scale ARL store action models domains form design networks fully specify domain models approximate action models value function Dynamic Bayesian networks successfully past represent compact 12351 In cases possible action models way small number parameters sufficient We extended Hlearning conditional probabilities method increases action models dominates learning time takes network structure input learns network This reduces space requirements domains learning speed convergence learning piecewise reinforcement Hlearning tasks especially performance linear To advantage implemented combines learning Bayesian networkbased described small Bayesian networks uniform action models reward Many reward functions optimal value func structure large regions state space In domains tion Hlearning value function approximation method based local linear regression Local linear regression action models synergistically improves tasks Combining Autoexploratory Hlearning action model value function approximation faster convergence The rest paper leads learning method follows Section 2 introduces Markov Deci form basis RL motivates Averagereward RL Section 3 ARTDP Rlearning sion Problems introduces Hlearning compares ing task Section 4 introduces Autoexploratory Hlearning previously Bayesian networks local linear regression value function issues Section 7 summary AGV schedul use dynamic action models respectively Section 6 discussion related work future research domains producing effective organized schemes Section 5 demonstrates AGV scheduling studied exploration approximate compares 180 P 72uiepdii D OkArtijicitrl Irltelligencr 100 1998 177224 2 Background We assume applicable described discrete learners environment modeled Markov Decision Process MDP An MDP set S n states discrete set denoted Ui actions A The set actions called admissible The Markovian assumption means action u given state E S results state j fixed probability Pi u There finite immediate reward ru sequence discrete steps A policy p mapping pi E Ui We consider policies stationary policies state j Time treated states actions change time called executing action u state resulting state 2 I Discounted reinforcement learning Suppose stochastic accumulating rpso expected value variable agent policy p goes states SO st time 0 total reward rpso t random variable Hence candidate total reward received t starting time t tends sum unbounded The discounted RL methods factor y total reward given t probability If environment optimize time horizon finite multiplying 0 y 1 In words SO E rp SO t Unfortunately infinite expected discounted reward discount expected successive rsksk usk optimize t cLi ffisO ECykrSk I1 kO 1 It known exists optimal discounted policy p value function starting states SO policies pu It shown satisfy following maximizes recurrence 36 relation 2 relation fi uiriu rpijP ES RealTime Dynamic Programming RTDP solves recurrence step righthand state value current updating equation RTDP assumes r given Adaptive Realtime Dynamic Programming action model probabilities estimates righthand literature method based method simultaneously called learns value function real values updating recurrence reward functions relation learn action models p reward functions uses state value function current online experience ARTDP estimates 31 In dynamic programming It model action reward models explicitly uses certainty equivalence control 6 The modelfree RL methods Qlearning model learning combine step learning value function stateaction pairs value function learning I 7irdepulli D OkArtificial Intelligence 100 1998 177224 181 action u represents The value Q u state executing action u state following correct Qvalues satisfy following relationship fp discounted optimal policy Hence total reward QLlO ru rpfj iES 3 In Qlearning Q u updated time action u executed execution results updated immediate following rule reward r nlF1 transition state If state j Q u Qiu Qiu Prln rrj Qiu 4 0 y 1 discount value state j detined maxEui Q j exploration state infinitely guaranteed strategy optimal policy ensures learning converge 46 factor 0 1 learning If learning executes admissible rate upj algorithm action uses rate 3 appropriately decayed Qlearning 22 The problems discounting Discounted reinforcement learning wellstudied methods Qlearning optimization given earn fixed probability ARTDP shown converge suitable conditions theory prac motivated domains reward interpreted tice Discounted run money terminated like learning properties As Schwartz pointed use reinforcement researchers use learning methods totals domains expected bigger cases The following example evaluate reward time step rewards 37 Discounting favor smaller shortterm systems different natural measureaverage domains tends sacrifice time However domains optimize discounted rewards undesirable longterm illustrates shown domain In Multiloop lengths The agent choose loops state S The average reward step I 65 12 loop 2 97 I 29 loop 3 129 133 33 1 loop loop 4 According averagereward loop 4 getting optimality highest average reward 133 Fig I loops different best policy criterion But discounted optimal policy different based value discount factor y Let Reward reward end loop Length total number loop steps loop Then total reward policy u following discounted 0 yLhmsRewayji 0 y2bnthnRej J simplified 182 P Tudepolll 0 OkAmcirrl Intelligence 100 I 998 177224 loop 1 loop 2 loop 3 loop 4 Fig I The Multiloop domain Discounted learning methods converge suboptimal policy E 2 d 8 9 ZJ I s 13 12 11 1 09 08 07 ARTDP w gamma098 ARTDP w changing gamma ARTDP WI gamma06 A 06 f 0 I 1 I I I 20000 40000 60000 80000 100000 Steps Fig 2 Online average reward step 10 random exploration Multiloop domain Each point mean 30 trials 8000 steps In particular rewards loop optimal discounted policy loop 3 097 loop 1 y 085 loop 2 085 f y 094 follow 094 y 097 loop 4 y 3 097 Hence y greater policy optimizing Fig 2 shows discountin g total reward optimizes results running ARTDP y On Xaxis average reward previous 8000 steps averaged 30 trials For comparison average reward modelbased ARL method called Hlearning section domain different values online way shown solid line For exploration taken Yaxis number steps average reward computed described average convergence I Tudepullr D OkArt1 Intelligence 100 1998 177224 183 state S ARTDP converged loop 1 random actions taken 01 probability yO8 loop 2 yO9 loop 3 yO95 loop 4 yO98 The Hlearning loop 4 fewest number steps shown solid algorithm line Fig 2 This experimental discounted When suboptimal different depending small factor policy yield optima1 policy maximizing factor average reward value discounting optimal discounted result confirms total reward discount selected As domain discounted learning methods optimize increased increasing 09 ARTDP converged loop 2 098 loop selected reward high value y 3 097 But high value y causes learning methods low slow seen Fig 2 Since discounted converge high value y value y need fewer steps optimal averagereward policy faster starting expect converge To approach works value low value y slowly 08 098 running ARTDP y started y gradually I 095 08 increased converged steps ARTDP converged The result Fig 2 shows changing y makes ARTDP slower y fixed highest value 098 In summary discounted converged loop thousand consecutive gain average reward leads shortsighted policies poor perfor discounted converge maximize mance learning methods optimal averagereward slowly Moreover starting convergence naturally Qlearning We later problems discounting factor high policy factor low If discount small y slowly loop 3 It assumed lead poor performance real world domains actual optimization ARTDP cases learning slows discount increasing criterion loop arise 3 Averagereward reinforcement learning We start standard Markov Decision Problems denotes r SO t random variable agent uses policy p starting Recall t Learning reward step time t t f For given starting state SO policy EL denoted p SO defined SO In Averagereward Reinforcement ARL seek optimize average expected MDP total reward received introduced Section 2 time ppsO fimm fEsa t 5 We states communicate state reaching set states set Nonrecurrent states form single recurrent communicate communicate states set stationary policy It unichain policy A recurrent set states transient An MDP states called ergoc policy positive probability 184 t fideptrlli D OkArtrinl Intelligence 100 1998 177224 stationary policy gives rise single recurrent set states possibly transient states 341 For unichain MDPs L independent policy denoted pp maximizes pp optimal policy mean longterm expected reward time step starting state SO We gain policy p policy p consider From specified use term problem finding gainoptimal average gainoptimal policy 31 Derivation Hlearning gain policy pp independent time t The total reward X conveniently denoted pp starting t qs Ed starting state total state s exists denoted offset Although lim oo ES exist periodic policies ES defined liml f cl es 61 It called bias state s interpreted expected total reward starting state s p u t expected longterm total reward Even expected time t policy timedependent Cesarolimit hs advantage reward time t average Suppose goes state j policy p In time step worth reward pp reward r A Hence following equation average gained immediate bias values state j policy J satisfy The gainoptimal policy p maximizes righthand equation state 61 Theorem 1 Howard h S satisfy For MDP recurrence relation exist scalar p realvalued function vi E S hi F ru epuhj I p Further gain optimal policy u attains maximum state p follows Bellman equation In going reward ri u action Eq 7 explained gained immediate u optimal opposed equal ru p Averagereward RL problem state instead average reward p After convergence best state j Intuitively longterm state j equal difference bias value state state difference expected bias value state j p Hence advantage riu expected I Ttrdeprrlli D OkArriciul Intelligence 100 1998 177224 185 h lO Fig 3 A simple MDP illustrates Bellman equation I 2 3 4 5 6 current state Let state Yiml immediate reward received taken k resulting action greedy action Take exploratory action N Ni I Ni n k Ni k 1 pka Ni n kNi ra cu inwr ra Nk All actions u E Ui GreedyActions maximize ru CT Pi0G1 If E GreedyActions P 1 apara hi hk b 7 hi mauiriu 8 ik Cy pijuW P Fig 4 The Hlearning algorithm The agent executes steps 18 state Notice solution adding constant set optimal policies relative differences arbitrary MDPs reference recurrent Eq 7 yields infinite number solutions hvalues However sets hvalues result state determined u optimal action values h Setting state 0 guarantees unique hvalue unichain solution For example Fig 3 agent select state 0 For domain p 1 optimal policy choosing good 0 h 1 0 h2 1 h3 2 actions goodmove recurrence badmove state 0 If arbitrarily satisfy h 1 2 equals difference action state 3 optimal average reward 1 relative value iteration method In Whites set hO relations Eq 7 For example ence state set 0 resulting equations solved synchronous hvalue arbitrarily chosen refer successive difference h3 reward optimal immediate 186 P Ttrdeprrlli D OkArtificial Intelligence 100 1998 177224 61 Unfortunately approximation dates p Eq 7 converge solve p Hlearning estimates online asynchronous 51 Hence rewards version algorithm instead Eq 7 Fig 4 The algorithm Fig 4 executed number times 1 executed step current state N u number times current greedy policy Y 1 initializes N u j explicitly stores algorithm implementation Before starting 0 GreedyActions state initialized set admissible denotes resulted state j Our array GreedyActions variables actions state Hlearning seen cross Schwartzs Rlearning discounted averagereward modelfree modelbased probabilities mates It employs mates true values updating equation pii learning method Adaptive RTDP ARTDP learning method Like ARTDP Hlearning rewards T straightforward maximum principle certainty equivalence hvalue current state according 371 computes 31 esti likelihood current esti 8 One nice properties Hlearning optimal policy matter exploration executed long action state learns learning exploration optimality TDA designed executed learning learned policy This learn shared Qlearning ARTDP The learned unlike temporal difference methods strategy sufficiently policy value function strategy effects speed optimal policy 391 modelfree Rlearning Eq 7 split parts Just Qlearning defining hj mx Rj u 10 R u represents gainoptimal When action u executed equation policy expected bias value action u executed followed Initially state Rvalues set 0 update state value R u updated Ri u I PRL u Prill hj P 11 3 learning state p estimate average rate Y immediate reward current greedy policy reward obtained j In I Tudeplli D OkArtiiuI Intelligence 100 1998 177224 187 state need piu Rvalues explicitly greedy action u maximizes immediate learn reward value Ri u Rlearning functions TiU action models need action selection updating As RL methods Hlearning agent makes exploratory estimation p slightly complicated Simply averaging greedy moves necessarily maximize visited converge state movesmoves intended ensure exploratory moves Hlearning moves rewards nonexploratory moves making greedy moves Rlearning greedy action u maximizes Hence p estimated cumulatively greedy action u executed following equation cy learning accumulate average estimate reward righthand state resulting rate righthand infinitely suboptimal Eq 7 training Without immediate exploratory policy However visits state similar rewards states Instead use method 371 From Eq 7 p TU hi fan pjuhj averaging YU hi hj state j Thus p updated p p curu hi zj p 12 Indeed converge Hlearning similar algorithms apply algorithm role exploration reward TU action gainoptima1 policy nonergodic immediate averagin g adjusted Hlearning proved mains interested need add exploration form original MDP infinitely Another difference updates p averaging sequence Later present experimental reason Thirdly reference slows Hlearning gorithm B case rectness suitable conditions convergence easier theory Jalali Fergusons Algorithm B 161 This algorithm ergodic MDPs Since domains trans ergodic making sure state visited Balgorithm action ri hi hj crucial algorithms recurrent change Al cor original proof In case hvalues bounded Algorithm B chooses arbitrary hvalue cases To extend 0 We proof convergence changes preserve exploration Unfortunately disputed3 proof based stochastic approximation independent state permanently difference significant performances correctness convergence shows immediate evidence involved reward grounds 32 AGV Scheduling Automatic Guided vehicles learning algorithms small AGV domain called Delivery domain shown plants performance modern manufacturing location 27 To compare transport materials AGVs 3 D Bertsekas private communication 188 I Tadeplli D OkArtificial Inrellipxce 100 1998 177224 elt 1 cl Job Package Moving Obstacle AGV Job generator 2 Conveyorbelt 2 Fig 5 The Delivery domain Fig 5 There job generators left AGV destination conveyor belts right Each job generator produces queue soon The AGV loads carries single job time conveyor belt Each job generator generate type 1 job K destination belt 1 type 2 job 1 unit reward units reward delivered job 1 p generator 1 4 delivered generator 2 belt 2 The probability jobs puts generating load moveup movedown The AGV moves lanes 5 positions actions time donothing changelane unload To load job AGV position queue To unload job proper conveyor belt To domain interesting added It randomly moves instant stay right lane stand The AGV obstacle single time step job standing If obstacle collides AGV AGV delivering state remains unchanged There collisions obstacle penalty 5 moving obstacle specified job numbers XY AGV obstacle job number AGV We assume learning states domain The goal AGV A state coordinates queue hold single job complete state observable There total 540 different maximize policy gainoptimal received unit queues locations average reward time By varying ratio jobs andor reward optimal policy generators generators produce jobs queue 2 frequently time steps needed needed produce queue 1 frequently compensates extra distance policy given different values p q K job mixes produced job changed For example K 1 job type 1 jobs low rates p q AGV unload number queue 1 belt 2 But job generators jobs increased value job 1 best jobs type 1 high rate K 5 AGV unload type 2 jobs queue 2 belt 2 smaller It general hard predict queue 1 queue 2 transport learning For P Tadepalli D OkAmjScial Intelligence 100 1998 177224 189 33 Experimental results Our experiments based comparing Hlearning ARTDP Qlearning R Balgorithm Jalali Ferguson Delivery domain experiments Delivery domain p 05 q 00 In words generator 1 produces types jobs equal probability generator 2 produces result comparing Hlearning ARTDP Qlearning Rlearning K 5 We chose sets domain parameters situations Experiments qualitatively different 3 I reported situations AGV domain K 1 wider range domain parameters type 2 jobs We present illustrate chosen uniformly random exploration tune The parameters initial state In experiments 1 7 greedy action randomly Each experiment repeated 30 trials algorithm Every trial started strategy chosen probability actions While available average reward step computed 10000 steps K 1 learning methods tuned best performance Strictly speaking y problem algorithm discounted methods adjustable parameter For random probability 77 01 action training 40000 steps K 5 Hlearning parameters trial error definition However goal domain gainoptimal approach K 1 case ARTDP parameter p 005 y 09 Rlearning ARTDP y 099 Qlearning cy 0005 The results shown l3 005 y 099 Rlearning Fig 6 p 001 cy 005 For K 5 case p 001 parameter learning y 09 parameters Balgorithm policy treated Qlearning discounted optimization learning policy closer gainoptimal type 1 jobs small value y coincides gainoptimal When K 1 jobs reward serve generator 2 produces type 2 jobs Since destination generator jobs icy We type domains shortrange policy ter setting delivery domain longterm optimal policy In case modelbased ARTDP converges difference formance discounted optimal pol domains discounted optimal policy For parame serving queue 2 shortterm optimal policy discounted method faster Hlearning negligible All methods Rlearning gainoptimal policy slightly I jobs type 2 The gainoptimal When K set 5 AGV receives times reward unloading policy serve queue 1 time queues type 2 jobs dis obstacle 1 serving generator 2 difficulty y set jobs type jobs counted optimal policy y 09 Even close return jobs belt 2 ARTDP sees shortterm opportunity belt 2 Whenever transport high reward jobs Hence policy y 09 To overcome 1 This policy conflicts located near conveyorbelt generates AGV serves gainoptimal 1 failing generator generator 190 P 7i D OkArtijicid Intelligence 100 1998 177224 E g I 5 2 z 0 025 02 015 01 005 025 005 0 0 Q wbeta05 R wbetaOOl alpha 005 b gamma9 x Steps 500000 1 e06 Steps 15e06 2e06 Fig 6 Average reward step Hlearning Balgorithm ARTDP Qlearning Rlearning Delivery domain estimated 30 trials random exploration 7 01 Top p 05 y 00 K 1 Average reward estimated 10000 steps Bottom p 05 q 00 K 5 Average reward estimated 40000 steps value y discounted optimal policy gain policy 099 With optimal policy Even undiscounted gainoptimal discounted methods need optimal policy As Qlearning 01 Hlearning 018 Thus averagereward Balgorithm 2 million longer infer discounted Balgorithm discount learning methods ARTDP Qlearning Jalali Ferguson steps Since training Fig 6 longrange high rate learn domain ARTDP trials getting gain Rlearning able policy gain higher time higher exploration served queue 2 exclusively factor yO99 outperformed discounted learning methods ARTDP Qlearning finding learning methods Hlearning Rlearning significantly P Tudepdli D OkArtcial Intelligence 100 1998 177224 191 average reward policy Hlearnin g Rlearning took training steps converge gainoptimal Rlearning gain Hlearning Somewhat surprisingly optimize allowed hvalue reference Balgorithm Balgorithm updating p Hlearning suggests crucial Balgorithm unable random exploration served queues 30 trials But learned policy Hlearning Balgorithm designed gainoptimal prevented policy Since grounding version Eq 12 state 0 difference way p updated The poor performance adjust immediate rewards low average reward modelbased ARTDP gainoptimal The main y reduces optimal action selection Since initial steps true optimum Meanwhile result reason policy K 5 discounted optimal policy y 099 high value far rewards relevant makes temporally effect discounting rewards takes long time propagate converge takes long time discounted methods action rewards dominate shortterm selecting The reason modelbased learning methods converge propagate information fewer steps model step taking update This learn store action models explicitly free learning methods expectation possible states given action requires CPUtime increases learning ARTDP Qlearning Fig 7 shows 40000 steps random exploration 71 01 All parameters methods Fig 6 H time online average reward 30 trials learning update So compared results Each point function CPU learning methods performance Rlearning When K 1 Qlearning converged Rlearning slowest Hlearning mance When K 5 converge ods Hlearning beginning Rlearning Hlearning gainoptimal Rlearning discounted converged gainoptimal policy ARTDP showed learning methods ARTDP Qlearning policy Even Hlearning averagereward shortest time perfor learning meth good performance gainoptimal policy slightly faster The results experiment shortrange domains discounted better policy Hlearning Rlearning respect optimal policy coincides gainoptimal ARTDP Qlearning However time But gainoptimal long converge gain Hlearning cases parameter performs number steps modelfree methods slightly better performance respect CPU longrange gainoptimal policy policy y low converge achieves higher average reward fewer steps methods policy discounted methods ARTDP Qlearning optimal policy conflicts domains discounted In different robustness changes domain parameters p 4 K Delivery domain We experimented total 75 different domain parameter settings varying p q K Hlearning compared ARTDP y 09 099 0999 Hlearning respect tuning experiment tested 192 I Ttrdeptrlli D OkAriJicitr Intelligence 100 I 998 177224 ARTDP WI gamma09 Q WI beta005 gamma09 R wl betaOOl alpha005 x x 0 0 I I I I I I I I 10000 20000 CPU Time 001 sec 30000 40000 50000 03 025 02 015 01 E g d 8 E E z 6 ARTDP WI gamma099 Q WI beta005 gammaz099 R w betaOOl alpha0005 o x I 50000 I 100000 CPU Time 001 sec 8 150000 II 200000 Fig 7 Average Delivery domain estimated 30 trials rewards step plotted CPU time Hlearning ARTDP Qlearning Rlearning random exploration 7 0 I Top 17 05 q 00 estimated 10 seconds CPU time Bottom p 05 q 00 reward K 1 Average K 5 Average reward estimated 40 seconds CPU time Hlearning configurations optimal policy 75 cases ARTDP 61 cases We able best y value 75 different configurations 099 able considered optimal policy domain roughly divided In configurations groups50 ARTDP y 09 Hlearning comparable They gainoptimal policy ARTDP remaining 25 longrange gainoptimal slowly success drastically In fewer steps Hlearning ARTDP y 09 policy Increasing y 099 helped ARTDP gainoptimal policy Increasing y 0999 decreased slowed configurations 300000 steps rate ARTDP convergence Hlearning shortrange slightly 3 I 1 I Tudepalli D OkArtcial Intelligence 100 1998 177224 193 In summary experiments domain parameters indicate discounted changes gainoptimal policy goal gainoptimal Learning methods preferable Mahadevan compared Qlearning lator domain 241 better maze domain Hlearning cases converges robust respect fewer steps suggest counterpart Thus experiments policies Averagereward Reinforcement discounted methods Our results consistent robot simu perform Rlearning Rlearning tuned 4 Exploration Recall Hlearning infinitely fortunately reward maximize reward needs exploratory training actions executed exclusively order avoid converging actions ensure state visited suboptimal policies Un exploratory purpose lead decreased agents current knowledge fully exploit In section version Hlearning learning executing optimism algorithm actionpenalty AH automatically explores current greedy actions Our approach promising uncertainty Koenig Simmonss method representing Kaelblings similar scheme 171820 called Autoexploratory H parts state space based idea IE functions Interval Estimation reward 4 I Autoexploratory HLearning Recall ergodic MDPs stationary policy guaranteed visit states ensures domains interested shown sufficient exploration section Unfortunately better exploration Hence primarily choosing greedy action respect ergodic MDPs pair states communicate gain stationary policy nonunichain MDP constant depends initial state In MDPs current value function strategy speed convergence nonergodic general multichain 341 Hence consider restricted classes MDPs An MDP communicating pair states j stationary policy communicate Contrast stationary policy For example Delivery domain Serving generators states A weakly communicating MDP allows set states transient stationary policy Although initial state gain optimal policy AHlearning works p upper bound optimal gain It initializing p high value slowly gain optimal policy Instead estimate average reward current greedy policy interpret p aspired average reward learner The aspired average reward decreases slowly ergodic AGV visiting communicating MDP 34 gain stationary policy weakly communicating MDP depends fact time prevents general communicating reducing exploits 194 P Eldeplli D OkArt1 Intelligence 100 1998 177224 move0OS move0OS moveoo Fig 8 The Twostate domain The notation action r p arc node I 2 indicates action I p probability state 2 r immediate reward executed decreases time When actual average reward current greedy policy increases difference aspired value average reward current greedy policy AHlearning converges average suboptimal weakly communicating MDPs strict superset unichains globally optimal policy adjust initial value aspired average reward rate falls gainoptima1 policies greedy policy AHlearning reward learning converges To ensure AHlearning applicable There reasons Hlearning needs exploration Inadequate reward models learn correct h values accuracy making affect policy learn accurate action adversely exploration suboptimal converge changes updated The key observation design AHlearning greedy policy pu gain Consider happens maxEuir current value p affects states current greedy policy Let p u states hvalues current suboptimal current value p pp Recall hi p Ignoring C puhj sum immediate tend current greedy policy higher np p p z n steps likely rewards policy hvalues states current It possible hvalues states visited policy policy change able set states current greedy policy If optimal policy learned involves going Twostate MDP Fig 8 communicating states visited greedy policy increase average increase stay Since circumstances greedy policy This illustrated executing hvalues implies p updated clearly In states multichain stay keeps 50 probability There immediate There reward 1 stay action state 2 In domain stay state 2 pp 2 state currently actions available stay state state 1 reward 2 stay action reward action changes optimal policy y taking action state 1 When actions chosen greedily Hlearning approximately state 2 executed stay action state 1 If stay action state 1 executed state 2 receives reward I updates h 1 1 h 1 p finds gainoptima1 policy trials stay action half trials domainthose I Edepalli D OkArtcial Intelligence 100 1998 177224 195 continue execute stay The greedy policy p increases action choice results converges stay action suboptimal policy value h 1 update finally p converges state 1 If p pp 1 1 Since greedy state 1 Hlearning visits state 2 MDP assumed Now consider happens eventually visited Thus states higher hvalues reachable ignoring p pp u current greedy policy In hvalues states current greedy case argument policy decrease average This means eventually states outside set states visited greedy policy hvalues higher visited greedy policy Since recurrent hvalues affect gain long p pp policy p This suggests changing Hlearning pa weakly communicating states decreasing transient danger getting stuck suboptimal starts high initial pvalue high gets gain suboptimal policy In fact p changes changing rate determined Hence p initially higher previous argument work adjust LY p changes low hvalues This starting sufficiently initial constantly pp To slowly compared initial avalue values pe 0 Hfil Hence Hlearning Ho gradually We denote Hlearning In preceding discussion ignored decreases continuously smaller states 0 decaying pvalue pu So far considered turn effect accuracy action models For rest discussion useful utility R n state action pair define effect lack exploration hvalues We 1 p Rvalue maximize assumes state actions takes state 1 greedy actions following run H 602 Twostate domain action stay state 1 It reduces h 1 R 1 stay state Hence step 1 Consider takes agent executes action step Assume 50 failure rate With limited experience actions state state 1 stay reward 1 0 Hence R 1 stay I h 1 p 0 h 1 p R 1 continues determines execute stay keeps decreasing agent state 2 Therefore model Unfortunately The solution value h 1 Even h 2 h 1 correct action model learn correct keeps executing stay turn makes impossible problem fixed changing pu 0 AH starts high pe low 0 AHT stores Rvalues explicitly Rvalues state effectively updated time In cf Eq 13 updated following update equation hvalue makes converge learning In Hlearning updating AHlearning Ri action u taken state incorrect action models called Autoexploratory implemented HLearning 196 P 7kiedi D OkArtG1 Intelligence 100 1998 177224 1 Take greedy action action E Ui maximizes R current state rinrnl immediate reward received state Let k resulting 2 NiaNialNiakNiukl 3 ptu NL kNi 4 ru riu riflll riuNiu 5 p 6at 1 aparu al 7 Ria ru piuhj 8 itk p hj maxu Rju Fig 9 The AHlearning algorithm The agent executes steps l8 state Riu ra iuj P I 14 In AHlearning p higher value executed action Therefore current state forcing execute actions greedy actions R gain current greedy policy decreased Rvalues actions remain actions appear unexecuted forced explore actions Thus AHlearning learn correct models optimal policy executing eventually best set user AHlearning Fig 9 p cy initially implementation shown The algorithm AHlearning defined parameters po LYO Unlike Hlearning explicitly executed action p updated simply taking nongreedy hvalues updated Eq 14 shows plot Rvalues actions store current greedy policy There greedy updating p executed actions greedy reward estimate The Rvalues stored explicitly average immediate need distort check action Fig 10a immediate domain All initial Rvalues 0 Because initial value p updating Rvalues lower beginning Thus executed time Therefore AH602 learn accurate action models executing state reduces error measure decreased higher Twostate domain single run AH602 Twostate action little actions executed p significantly 2 R 2 stay p 0 But Rvalues reward action rapidly rarely chosen best action times As p gets close significantly finds gainoptimal immediate reward Thus r2 stay policy reduces Fig IOb shows online average reward 100 trials AH602 H602 Ho Fig 8 When actions chosen greedily AH602 AH search space effectively executing greedy actions On learning methods AH Twostate domain optimal policy optimal policy learning explores simple Twostate domain previously discussed 57 69 trials respectively This confirms hypothesis shown 100 trials tested Ho H6o2 I Tadepalli D OkArtijicial Intelligence 100 1998 177224 197 20 40 60 80 100 Steps 200 400 600 800 1000 Steps E s d w E s s 22 2 18 16 14 12 1 0 Fig IO The R values p values AHh single trial b 20 steps AHh H2 Ho meanaveraged 100 trials online mean rewards TwoState domain learning execute nongreedy including Hlearning ARTDP Qlearning gainoptimal actions results informal gain suboptimal high value LY sufficiently Rlearning need occasionally policy reasoning conjecture policy p main learning initializing AHlearning gain lower small value gainoptimal policy However p policy time learning AHlearning converge From empirical tained higher sufficiently converges suboptimal policy 42 Experimental results AHlearning In section present empirical evidence finding policy higher average reward converging quickly effectiveness illustrate AHlearning 198 I Tudepdli D CJkArtfirict Intellipm 100 I 998 177224 previously studied exploration methods We use Delivery compared domain Fig 5 We compared AHlearning exploration methods exploration Boltzmann counterbased In random exploration actions small probability greedy action exploration action u chosen 1 7 maximizes R chosen maximizes 7 With high probability random action selected uniformly exploration recencybased random exploration 45 admissible exploration state In counterbased Ri 6 ci c pijUCj ci In Boltzmann number times state visited 6 small positive constant exploration action selected state probability eKitrlP Cue RiuP parameter maximizes Ri exploration number state E small positive constant rate initial values rates decay tuned trial error best 3 temperature randomness action selected steps In cases Their performance parameters 7 6 3 E decayed constant action executed In recencybased ni f qm The parameters Delivery domain p q K set 05 00 5 particularly domain Fig 6b Proper exploration reasons rewards gain For reasons suboptimal policy It gave best performance pa 2 0 00002 stochastic suboptimal difficult maintain p consistently AHlearning following takes steps propagate high optimal policies gain close higher gain gainoptimal domain policy important important second Fig 11 shows online average rewards 30 trials AH200002 HOOO Ho greedy actions Ho 4 different exploration methods random exploration initial 77 014 decayed 000031 1000 steps counter based exploration initial S 007 decayed rate 000021 1000 steps Boltzmann 03 decreased 00003 1000 steps recencybased exploration initial E 005 reduced 00004 1000 steps exploration initial tuning po improved When actions greedily chosen Ho optimal policy slightly appears Boltzmann faster exploration methods AH recencybased methods Recencybased random exploration By proper worse better explorations dives optimism uncertainty low value beginning attributed domain counterbased worse AHlearning significantly exploration I Tadepulli D OklAmjCul Inrelligence 100 1998 177224 194 025 0l 005 0 005 01 015 0 50000 100000 150000 200000 250000 300000 350000 400000 450000 500000 Fig Il The online mean rewards IOK steps averaged 30 trials AH2WW2 H Ho greedy actionselection H recencybased counterbased random Boltzman exploration strategies Delivery domain 11 05 y 00 K 5 It interesting curves methods AHlearning compare AHlearning RL methods respect CPU Fig 12 time The performance copied Fig 7 We tested AHlearning K 1 5 added results Fig 12 Since methods AHlearning 7 01 final online average AHlearning Perhaps CPU time methods unlike Hlearning AHlearning significantly random exploration lower optimal policy In particular longrange domain faster Rlearning undecayed rewards significantly importantly AHlearning converged proper These results suggest state space effectively AHlearning involve automatically maximum p higher explores schemes Although p adjusted One way track currently known reward stateaction pairs maxi Ru reinitialize changes parameters p LY appears initialization value p AHlearning exploration immediate tuning 5 Scalingup averagereward reinforcement learning tablebased RL methods including Hlearning programming need space proportional function For interesting causing combinatorial realworld domains explosion All dynamic value enormous algorithms number states AHlearning based store number states time space requirements In fact tablebased RL methods need space exponential 200 F Tudepalli D OkArtijiciul Intelligence 100 1998 177224 Q w beta005 gamma09 R wl betaOOl alpha005 x 0 0 10000 20000 30000 CPU Time 001 sec 40000 50000 0 50000 100000 CPU Time 001 sec 150000 200000 Fig 12 Average rewards versus training time AH Ms H ARTDP Qlearning Rlearning K 1 b average rewards versus training time AH xK1 Ho ARTDP Qlearning Rlearning K 5 Each point mean average reward 40000 steps 30 trials p 05 y 00 All methods AHlearning use random exploration 7 0 I function store number machines In addition states value values individual jobs transported number AGVs completely number state variables compartmentalize particular state absolutely influence action choose similar state state spaces huge agent expect In realistic domains experience state learn appropriate action Thus important states experienced similar states generalize experienced This hypothesized value function usually finding approximation tablebased If space functions best action algorithms learn There function approximation methods studied including neural network learning 8211 clustering discounted 261 memorybased 281 locally weighted regression 29361 Two characteristics AGV RL literature methods F Tudepalli D OkAmjSciul Intelligence 100 1998 177224 201 specific implies Hlearning value function value function scheme able generalize In AGV domain I scheduling domain attracted local linear regression method choice First features state domain location AGV important locations Any function approximation varies linearly AGV large regions Second reward region state space optimal action effects immediate changes region L v L11e A allU I LUIIIILLLcs 1 LIIC fiiu v I 11s rb uue vecause upmuai action changes coordinates large geometrically AGV constant gives constant linear _I J c rL_ AC rPL I_ rL_ _r_ 1 __r_ feature values current state constant iS reward usually 0 need space action models reward Section 2 The space requirement domain model number state variables Dynamic Bayesian networks In domain models way small number stateaction store models defined exponential successfully cases parameters learning methods Hlearning fully specify domain models learning methods need store value function domain model past represent combination While modelfree pair modelbased networks possible sufficient piecewise storing contiguous immediate design 12351 regions 5 I Model generalization Bayesian networks One disadvantages modelbased methods like Hlearning explicitly lot space The space requirement Onm domain n number states 112 number large domains represent In section network learning methods networks assuming parameters depending 0n learn action reward models consumes models storing storing stochasticity actions To scale modelbased domain models dynamic Bayesian adapt Hlearning strrrtre criven _ _____ _I D __ We assume DBN Bayesian network t action time directed acyclic graph nodes represent probability probabilities The probability associated CPTs algorithms learning given prior knowledge structure CPTs table state described set discrete valued features A dynamic feature values represents relationships feature values time t 1 A Bayesian network CPT different values node conditioned event given evidence random variables conditional associated node The CPT node describes values parents determined network 351 Since network compute action models learning reduces mustrate dynamic Bayesian network representation job generator left conveyorbelt Siipperyiane Fig 13a There domain right The job generator generates new job The goal AGV unloading JobonAGV loads job zone A state domain described job AGV 0 1 AGV location loading zone unload repeatedly AGV immediately load job AGVLot 202 P fildeptlli D OkArttjickzl lntellipnce IOU 1998 177224 Loading Zone AGV Lane Unloading Zone Job generator Conveyorbelt 0 Job Package Fig I The Slipperylane domain fop b dynamic Bayesian network stay load unload unloading lane entering There actions forward backward The action entering forward correct lane 1 Plve destination probability P incorrect direction probability slippery The actions obvious meanings The AGV gets positive reward 5 unloads job gets penalty 0005 moves job In cases reward 0 loading zone backward executed moves If lane zone shows DBN domain Typically Fig 13b set corresponds nodes features state given action representation explicitly differences differences corresnnndino y_D currefit ff3tilrPS _ ___ state DBN consists sets features state second set features Instead simplified states shown The features state easily computed adding fl represent moving changes 1 left Because independent unloading JobonAGV JobonAGV AAGVLot respectively In domain AJobonAGV 0 actions AAGVLot In Fig 13 AJobonAGV AGVLot 1 right 0 staying JobonAGV loading moving unload actions admissible AJobonAGV onAGV Action parents AAGVLot dependent JobonAGV reward immediate iearning values 1 load value AGVLot given Action But Job Action Since action receives negative expected AGV job reward JobonAGV tine CPTs dynamic Bayesian network exampies straightforward network directly observable network structure probability features known Consider fraction stateaction pairs PfUIfLr f value u cases parents desired values For AAGVLot I given JobonAGV 1 Action example f parents node fk ok approximated Action parents direction node ExpReward fk The conditional appropriate probability denotes fi P Tudepnlli D OkArrificiui Intelligence 100 1998 177224 203 fraction cases AGV moved right job executed reduces consequence If n number different AGV locations repre domain models 10n 4 32 An sentation learning domain models important faster Unfortunately time dominated time takes learn value function time takes iearn iiiodeis This true Deiivery domain But domains space requirement reduction easy Bayesian network learning tiie domain Slipperylane accurate domain model mance Bayesian networkbased models demonstrably shown Section 531 crucial perfor learning expedite policy learning This 52 Value function approximation In section value function approximation method based local linear regression like ARTDP important conditions nonlinear AGV scheduling value function able generalize h function Second In follows assume task Slipperylane We chose local linear regression features state location AGV Since LLR approximation method reasons domain section AGV AGV approximate independent exact location AGV h linear respect AGV location features large set locations large regions immediate First important typically effectively locations domains reward usually given features state action Under function piecewise state constant linear respect small number linear respect features However learning methods nnmuimtinn discounted learnin yy memory In linear piecewise features change arbitrarily value function discounted linear sum squares errors points respect surface neighborhood state represented Our value function features This h features function set piecewise value function set exempiars seiect set states hvaiues picked learning algorithm The value function stored exemplars set m data points output features values k linear nonlinear store value function regression fit linear surface features n k nonlinear generalizing set k linear limited approximation point prediction Locally Weighted Regression linear Hence piecewise linear functions represent data points chosen needed Let assume Instead representing points piecewise k dimensions IO In local LWR large weights interpolated minimized regression given similar 22936 introduce infinitely linear rnilrl r e _ D apPr rrnr 2nd QIJ rengire Suppose need estimate hp state p values x1 x If p exemplars selects n features stored value features k linear estimate Otherwise exemplars 204 I Trrdeplli D OkArtijiciul Intelligence 100 1998 177224 features kdimensional selecting neighbors hvalue features Out subspaces nearest neighbor p dimension way reduces In words exemplars picks nearest exemplar p space centered p If large errors states far p close measured values p nonlinear differ k linear 2k orthants k 1 example selected We extrapolating The distance states differ linear features Euciidean values k features exemplars uses values selected neighbors set maximum reducing p uses iinear regression fit values minimum values respectively This step useful distances different neighbors errors large differences distance After seiecting squares If predicted value 2 neighbors maximum potentially predict minimum hvalue greater If p neighbors 2k orthants number dimensions nonlinear neighbors continued estimated features 2k orthants selected k I If fails 0 At time exemplar currently stored share values k reduced 1 nearest holds This hvalue neighbors condition hvalues adjacent stored exemplars Since tolerance state space differ p normalized multiplying updated hvalue neighbors Whenever nearest neighbors exemplar hjvalue iarop nllmhpr nf xmnlnrc w stored ifi oinnino 2nd y ze PVP id hvalue estimated given states stored hvalues selected nearest j safely deleted stored possible js nearest neighbors include constant parameter E p An exemplar new exemplar selected approximate Without greater checks immediate u stored heuristic tolerance set described reward av LYL 1 wy bna We approximates value function version Hlearning Fig 14 shows algorithm LBHlearning learns Bayesian network action models LBHlearning local linear regression exemplar set It initializes expected optimum gain Y low value The ex p value higher plicitly stored hvalues exemplars denoted h values estimated LLR denoted A The prob abilities pj u inferred standard Bayesian algorithms The parameters Bayesian networks updated network Section 51 The exemplars incrementally dated prediction hvaiues states descriiied previous paragraphs Bayesian networks ri u transition UpdateBNmodel described immediate inference rewards Our algorithm similar edited nearest neighbor improve size representation exemplars nearest neighbor approaches small 11314 One problem highly sensitive predictive accuracy prune unnecessary algorithms collect ones edited noise tend P Tadepalli R OkArtificial Intelligence 100 1998 177224 205 1 Take exploratory action n greedy action maximizes CT pjahj received Let 1 resulting state rinlm immediate ria reward 2 UpdateBNmodel 3 If greedy action 1 rIl p 1 ap Lyra hI b cy 2 hi 4 5 maxriu P If hi E Exemplars delete Let Neighbors nearest neighbors 2k orthants surrounding differ values k linear features PijUhj CC 6 If lu hi 1 EP u maxjENeighbors hj u minjcNeighbors hj Add u Exemplars b For j E Neighbors plars Jz j 7 il hj computed temporarily hj EP delete j hj Exem removing j hj Exemplars Fig 14 LBHlearning uses Bayesian networks representing action models local linear rerrrewinn lnnroximating __o__ ___ _ __ Lrr__ _ value function Steps 17 executed agent state 11 remaining points linear noise Even intermediate store noise points interpolated Our algorithm suffer problem target value function fact piecewise learner sees noisy linear stages learning This small number pieces updated backing values step time value function adjacent places 4 states keeps locally consistent intermediate value function piecewise appears examples T 1 TT I__ J_IKt llearrg LDlYlItXilLI111g ill t GLGIGU L fiLrnyarly r TlTT ______ __ L_ t_4l ALBHlearning initializing hvalues Thus ALBHlearning approximation It maintains In section evaluate different versions Hlearning domains ALBHlearning separate set exemplars gainoptimal converges p high value explicitly storing Rvalues uses Bayesian networks local linear regression l t __ I _ T DLI IIIGLIIIIIIg instead policy taking greedy actions action store tuples Ri including LBHlearning locally linear 53 Experimental results In section experimental mains We compare named extensions A autoexploration B learning Bayesian network versions Hlearning Each version different AGVscheduling performance results 4 There fact kinds locality locality state space locality Cartesian space inhabited AGV The fact notions coincide domain important premise argument 206 t Tudeplli D OkArtijicial Intellipme 100 I 998 177224 action models L approximating sion value function local linear regres We compare performance ARTDP Bayesian network action models regression local linear Since LBARTDP parameter p tolerance normalized pi smallest dimensions This gave better performance ARTDP Pi state linear local slope value function constant extensions LBARTDP tolerance Because learning iiigh valile p gives good perfoimance algorithms based Hlearning gorithms v 01 probability The experiments regression linear applicability demonstrate scalability domains multiple start high value p In experiments autoexploratory al random actions greedy actions synergy Bayesian network models local space time learning methods component algorithms autoexploratory linear features 531 Improving performance Hlearning demonstrate synergy local linear The goal experiment regression Bayesian network domain We use Slipperylane Fig 13b learning shown representing works locations tant Pl set 06 60 probability zone job loading zone job AGV set 30 To model Fig 13a The dynamic Bayesian net domain models The number impor unloading learning AGV moves significantly scaling Hlearning The parameters method tuned trial error The pvalues 01 005 002 001 001 001 respectively LHlearning BHlearning AHlearning ALBHlearning Hlearning The avalues initialized LHlearning fQrtnr fnr I RARTnP au YuI_ILu_ ULl initialized LBHlearning LBHlearning rlicrnllnt ULL 30 trials method ted offline 1OOK steps 3 random convergence main fast average Since P close 05 model stored _ __ parameterized different BHlearning learning Hlearning interesting performance __ BHlearning approximations AHlearning learning Since AHlearning BAHlearning updates Rvalue stateaction LHlearning 0001 0005 00001 0001 0001 0001 For LBARTDP E set 1 1 2 respectively The respectively The _ urwrimmtc yU WPW I rewatwl r WRE QPt tn 0 95 _ random starting state In Fig 15 plot initial reward 30 trials estimate based 3 runs start states We chose offline estimation online reliably measure average reward learning domain models converged quickly important models tables LHlearning converged reason value function LBHlearning clearly superior demonstrating synergy domain Thus H approximation Most kinds faster Also ALBHlearning BAHlearning converged AHlearning faster BAHlearning converged faster explored domain effectively learned models H faster converged slightly faster AHlearning Because ALBHlearning pair step LBHlearning updates P Tudepalli D OkArticial Intelligence 100 1998 177224 207 002 0018 t I I 4 40000 steps 60000 80000 100000 Fig IS Offline average LBHuX2 BH LHn reward ALBH BAH2K AHOW random exploration LBARTDP random exploration 7 01 runs 1OOK steps 3 random 3 offline trial evaluation In averaged 30 trials start states point shows hvalue state step ALBHlearning learning LBARTDP slower LBHlearning ALBHlearning BAHlearning AHlearning converged little slower LBH faster BHlearning Hlearning size methods stores 4 states use auto end value stores 6 7 states Bayesian network models BH hand store values 60 states Fig 16b store stores 6 7 states linear LHlearning correspond These mn meth af nlnmtnrv Y Fig 16a average exemplar smooth piecewise In end LBHlearning exploration locations slipperylane LBARTDP function ARTDP value function learning chnwc vprzo __ _ exemplars Rvalue state action However 17 exemplars Rvalues BAHlearning Rvalues ppmnlar b Wy methods Hlearning ci _ Fig 16a end ALBHlearning approximate stores store 122 exemplars AHlearning stores LBHlearning stores 67 exemplars hvalues ALBHlearning functions actions taken frequently actions smooth Therefore 136 exemplars Rvalues Because greedy actions suboptimal value uses approximation methods LBHlearning ALBHlearning proportion total exemplars iinear regression function piecewise need exemplars stores greater LBHlearning All Hlearning methods local vaiue autoexploratory methods ALBHlearning fully converged end store exempiars store accurately beginning linear These results suggest local linear regression prove time space requirements faster discounted counterpart Hlearning Bayesian network models im converge 208 I Tudepdli D OkArtificitrl Intelligence 100 1998 177224 40000 60000 80000 100000 Steps 80 60 0 0 20000 40000 60000 80000 100000 Steps Fig 16 Number exemplars BH b AHW _ tl tktxb JJHKs J01 LBARTDP fjAH2nl ALBHX All methods random exploration 4 01 Each point mean 30 trials 532 Scaling LBHlearning dotnain size The goal experiment section learning rlnmoin Fig 17a UVIIIUI ALBHlearning 1 I u LV U ta tn An thic varying demonstrate scaling LBH size domain We use Loop There AGV job generators 1 2 destination AGVLot JobonAGV jobs generates 2 Each generator state described 5 featuresLane atGen2with denotes tal number possible tions 1 2n5 short lane number locations AGV lanes 2n5 lane 1 1 n5 locations JobatGenl conveyor belts 1 belt 05 probability A Job 1 4 loca takes values lanes Jobon Fig 17a The lane 1 AGVLot denoted n There n5 obvious meanings The variable Lane takes values AGVs location shown I Tudepulli D OkArtificial Intelligence 100 1998 177224 209 14 59 60 job generator I Lane 3 7oGb_it I Conveyorbelt 2 Job generator 2 Fig 17 The Loop domain b dynamic Bayesian network indicates job destination JobatGen2 1 2 depending size state space 0 AGV job If AGV job 1 2 destina moveforward AGV job JobatGenl tion job waiting generators Therefore n x 3 x 2 x 2 12n The AGV 4 actions moveforward movebackward unload The AGV action job zones optimal average livery proportional receives reward Oln ward OO2n The goal AGV conveyorbelts Whenever ated Fin action movebackward load loading zones To proper AGV loads job job generator new job gener action unload unloading n immediate reward correct belt gets smaller n If AGV delivers If delivers 6 L I Uaa CnI u nncu YUJVUICI IVIL 11 ULLULaI ZSV I PW jobs job generators wrong belt rewards action load thir Anmain Avnmir job RIVPCDII ntwnrli rhnwc 17h flne IUUCUIU footllro AGVLot Link abstracts end locations lane middle locations feature distinguishes succinctly model AGVs motion AGVLot given Link Lane Action store domain models ments feature Link takes 3 values end1 end2 ends This space require end locations lane rest loop Since AAGVLot representation 4 52 1056 useful independent reduces We random exploration 7 01 LBHlearning AHlearning ALBHlearning experiment use autoexploration We set E 1 Hlearning 210 P 7ideIdli D OkArticinl Intelligence 100 1998 177224 025 02 H25 0 20000 40000 60000 Steps 025 c 02 015 01 005 0 L 40000 Steps _I 60000 80000 100000 rewards step H LBH average Fig 18 Online 5000 steps Each point mean 30 trials HCJn2 125 LBH2W 125 total number AGV locations E ALBHlearning superscripts b LBH 111 25 LBH1nl750 LBHom75 p czo number Loop domain estimated H25 H iMs50 H1000275 HhO loo LBHotW 100 parentheses set 1 sm chnwn ad II UV LULU L CLIUL UU LL UL4 _ __ varied n 25 125 steps 25 The parameters algorithm nf M SW IWWWI hxr ad I The online average rewards Hlearning AHlearning estimated online 5000 steps averaged 30 trials LBHlearning shown ALBHlearning _ y_ __ __ _ O nf Fioc rnntinnc rrnr trial shown Fig 18 Fig 19 The values shown value 19 18 As convergence Hlearning longer sistently better n The convergence travel longer distances ALBHlearning takes speeds LBHlearning AHlearning larger values n ALBHlearning difference new information However LBHlearning number steps convergence grows slowly increases AGV E Tadepalli D OkArtcial Intelligence 100 1998 177224 211 0 20000 40000 60000 a0000 100000 Steps 025 0 0 20000 40000 60000 80000 100000 Steps average rewards 5000 i9 Oniine estimated Fig AH AHn000850 mean 30 AHWX 75 AHa2 IOO AH2X 125 b ALBH om325 ALBHm50 ALBH7W2 75 ALBH22 100 ALBH2002 125 number superscripts total number AGV locations E ALBHlearning steps Each point pa set 1 AH00225 parentheses ALBH Loop domain trials step average online attributable H AH Also higher Hlearning difference LBHlearning ods buiitin decay mechanism Section 42 illustrate potentially decayed useful parts state space rewards AHlearning respectively LBHlearning factors First rates exploration Hlearning experiment Perhaps autoexploratory methods effective importantiy ALBHlearning value n This autoexploratory meth experiments exploring The numbers stored exemplars LBHlearning 30 trials shown Fig 20 comparison The bigger AHlearning value n larger number exemplars ALBHlearning averaged exemplars stored Hlearning stored 212 I Tadeplli D OkArrifcial Intelligence 100 1998 177224 a00 600 2500 Steps 0 0 20000 40000 60000 60000 100000 Steps Fig 20 Number exemplars Hlearning LBHlearning random exploration oniy autoexpioration ioop domain 7 01 b AHlearning ALBHlearning The results mean averages 30 trials AHlearning For LBHlearning Hlearning remains constant increasing n ALBHlearning absolute number exemplars increasing especially n This suboptimal Rvalue increases function actions absolute number exemplars like LBHlearning performs slightly LBHlearning smooth h function F Tudepalli D OkArtcial Intelligence 100 I 998 177224 213 gc LI m 50 45 2 z 5 0 r s rl E k D s 0 0 6 35 30 25 20 15 10 5 I I I I I I 1 ALElHmaxralio ALEiHfinalratio _ LBHmaxratio LBHfinalratio _ t 0 _ Q __y____ tX_ 01 0 I 20 I I Tit total ntnber 80 I I I 100 120 140 AGV locationsn Fig 21 The maximum ALBHlearning 30 trials final ratios stored exemplars function domain size II Loop domain The results mean averages possible exemplars LBHleaming usually ALBHlearning indicate ALBHmaxratio ALBHfinalratio Fig 21 shows percentage number stored exemplars total locations n AGV maximum values ratio exemplars function total number possible indicate domain LBHmaxratio learning LBHlearning LBHfinalratio values ratio LBHlearning ALBHlearning step The ratio maximum 388 LBHlearning 443 gradually n 125 The final ratios lower maximum end lot smoother tal0r greedy artinnr rrfiuu 1u 1 I stores exemplars ALBHlearning increases locations n 25 value function nrtinnc SWP fully converged Hence beginning Because ALBHlearning Fig 21 clearly shows LBHlearning 159 ALBHlearning total number AGV itc Ivajues subontimsl store far fewer exemplars 94 LBHlearning Y __VU __ LBHlearning ALBHlearning beginning AHlearning Hlearning ratios reduces IZ smoother LBHlearning LBHlearning Hlearning The value functions Hlearning LBHlearning AHlearning ALBHlearning optimal value function AH random actions state space evenly AHlearning explore greedy actions thoroughly end training shown Fig 22 comparison The value functions Hlearning learning ALBHlearning 01 probability ALBHlearning usually suboptimal smooth piecewise random expioration iinear Vaiue function forces agent visit states uniformly Due local linear regression Hlearnings value function ALBH LBHlearnings value function Thus LBH learnings value learnings solid value lines Fig 22 regions state space A learning method based Hlearning value function function function smoother closest optimal value function AHlearnings smoother explore shown 214 P Tudelulli D OkArtcicll Intellienue 100 1998 177224 ptimal Value Function 0 10 20 30 40 AGV 50 location 60 70 80 90 al 5 12 8 6 0 0 10 20 30 40 AGV 60 50 location 70 80 90 b AH ALBH Fig 22 The kvalues function AGV AGV job generator I job 2 generator 2 job 1 n 100 The location shown Fig 17a The optimal value function shown solid lines plots AGV location H LBH The results Hlearning memory use domain LBHlearning ALBHlearning AHlearning domain size terms learning scale better speed 533 Scaling multiple linear features In section demonstrate LBHlearning ALBHlearning Grid domain shown Fig 23aj iinear dimensions At corner 15 x 15 grid job generator opposite corner conveyorbelt The AGV action admissible obstacle middle obstacles The AGV load unload action available destination actions movenorth wall represent movesouth moveeast The dark squares movewestif AGV wants location P Tadepalli D OkArtcial Intelligence 100 1998 177224 215 Job AGV obstacle Job generator zCz maLEze Fig 23 The Grid domain b dynamic Bayesian network reward step 1 Fig 23b appropriate average Grid domain Action variable domain model 3076 24 corner It receives reward 58 delivers job optimal effect available action depends shows dynamic Bayesian network space requirements In fact reduces compact storing Each point Fig 24 online average reward 30 trials calculated effect setting E 04 07 10 13 5000 steps It shows reward LBHlearning random exploration v 01 average finds reward ALBHlearning With E 04 07 10 LBHlearning average policy faster Hlearning With E 13 speed decreases gainoptimal policy policy Similarly ALBHlearning faster AHlearning suboptimal policy e high random exploratory actions Thus average E 04 07 10 finds gainoptimal high value E makes However ALBHlearning e 13 converges reward remains zero suboptimal converge Fig 25 shows number exemplars stored LBHlearning ALBHlearning T IT 1 Lamlearning E E high ing methods converge space The value functions LBHlearning sufficient exemplars suboptimal exploration usuaiiy 13 case When E high ALBH learning store fewer exempiars higher vaiues learn state smooth store large number policy sufficiently ALBHlearning explore state space makes 216 P Tadepalli II OkArtcial Intelligence 100 1998 177224 06 LBH WI epsilonOrl LBH wl epsilork07 LBH w epsilon1 O LBH w epsilon1 3 H 40000 60000 80000 100000 Steps ALBH w epsilon04 ALBH WI epsilon07 ALBH w epsilon1 O o ALBH w epsilonl3 _ AH 0 20000 40000 60000 80000 100000 Steps Fig 24 Online average rewx v 01 b AH8JKW3 mean 30 trials 5000 steps step iJJH32 j random expioration ALBH 5n7 exploration Grid domain Each point taj 3 The learning domain AGVs multiple final value Hlearning AHlearning functions Fig 26 comparison shown true optimal value function Hlearning ALBHlearning location LBHlearning LBHlearning ALBH optimal value function For linear respect value function piecewise approximate Since obstacles linear features successfully local linear regression middle grid states hvalues plots Fig 26 Like experimental Some arbitrary vaiues iower tnan tine hvaiues states surrounding locations Fig 22 value functions corresponding LBH ALBH AH respectively assigned Loop domain H LBH left half Fig 26 smoother right half The value functions H approximation autoexploratory smoother versions learning results P Tudepalli D OkArtificial Intelligence 100 1998 177224 217 500 I I LBH w epsilon04 LBH WI epsilon07 LBH wl epsilon1 O o x LBH w epsilonl3 200 150 100 50 0 0 20000 40000 60000 80000 IOOGQO Steps 1800 I I 1 1 ALBH w epsilow04 ALBH w epsilonO7 ALBH w epsilon1 O 0 ALBH wl epsilond3 AH 600 400 200 I I 0 20000 40000 I 60000 80000 I 100000 Steps Fig 25 Average 9 exploration domain number exemplars 30 trials HN3 LBHv 01 b AHlxx ALBHS00007 exploration random Grid The results section demonstrate local linear regression Bayesian network model learning extend twodimensional spaces significantly reduce learning time memory requirements 6 Discussion future work The basic premise paper realworld domains demand optimizing average reward time step work Reinforcement Learning focused optimizing discounted total reward Because discounting encourages learner sacrifice longterm benefits shortterm gains discounted RL domains lead suboptimal policies We presented variety algorithms based H 218 P Tudepctlli D OkArtificial lntelligencr 100 1998 177224 Fig 26 The value functions Hlearning ALBHlearning Hlearning isset middle LBHlearning right I comparison use random exploration 7 0 I E LBHlearning left AHlearning right LBHlearning optimal value function middle left Grid domain ALBHlearning learning modelbased method designed step demonstrated parts work include usefulness 411 321 421 optimize AGV scheduling gain average reward time tasks Earlier presentations learning Fig 27 shows family algorithms obtained adding autoexploration Bayesian Hlearning We choose domain needs available Based experiments network model combination resources prior knowledge following recommendations extensions local linear regression depending I Tadepulli D OkArtiJicial Intelligence 100 1998 177224 219 Fig 27 The family algorithms based Hlearning l When know upper bound pmax gain use autoexploration p initialized higher pmax l When afford tuning exploration parameters use Hlearning small p 0 LY 1 exploration strategy l When limited space time use local linear regression l When structures Bayesian network action models available Bayesian network action models use learn parameters y C 117 Ml L4 _ approaches reinforcement learning methods Learning point view fnr SWPOPIPWW nntimintinn I UwUbv _ There extensive body literature averagereward optimization dynamic 61534 Mahadevan gives useful survey literature 241 Schwartz Singh present model There RW nt bact twn __ wv proved converge 4161 Bertsekass algorithm Average stochastic shortest path algorithm slowly changing edge immediate programming Reinforcement frpp RT alonrithmc 1v I_ averagereward suitable conditions reward RL problem costs rewards relation immediate equivalent h values p estimated online averaging h value reference state online averaging rewards Hlearning The basic Hlearning adjusted similar expioration p Ru p gain current greedy policy Riu reward executing action u state Hence uses recurrence Algorithm B Jalali Ferguson 46 The edge costs essentially ignored Jaiaii Ferguson negated averageadjusted 161 The main difference Section 3 updating based converting Eq 7 immediate algorithm In paper mainly Biasoptimality gainoptimality gainoptimality total reward obtained entering recurrent All gainoptimal It seeks 3437 Schwartzs Toptimality concerned averagereward optimality refined notion expected state gainoptimal maximizes policy policies biasoptimal Hlearning Rlearning 220 P Tndepulli D OkArtcial Intelligence 100 1998 177224 unichain MDPs set states states gainoptimal including nonrecurrent infinitely exploration strategy To biasoptimal necessary select biasoptimal state refined criteria Mahadevan biasoptimal policies based solving set nested recurrence relations iz W state iz vaiues aione sufficient policy Bias optimal policies appear significant advantage operations systems studied control queuing policies ones Rlearning biasoptimal recurrent visited general unichains gainoptimal Hlearning 23251 His method vaiues biasoptimal domains literature admission 23251 Autoexploratory Hlearning policies rise states policies extends general unichains actions determine research belongs set exploration The general techniques idea initialize state stateaction wellexplored pair effect domains updates minimax high utility Koenig Simmons value function giving negative penalty pairs state achieve Qvalues stateaction admissibility A algorithms called action executed closely Qvalue execute action highest Qvalue case Qvalue forever remain higher state Since Qvalues actions best In execute executed action decrease value actually chance reward uncertainty 201 In deterministic functions appears better optimism explored relatively classified value function sufficiently known simply zeroinitializing action scheme ensures true optimal values condition The approaches effects Qvalues actions nonunderestimating case Qvalue eventually available potentially Kwlhlinmr LUVL In analogy frequently real value Hence choosing state This gives best actions Tntmwal Ertimatinn 0 mLVL U YYLIUL idea applicable interval value function bounds confidence confidence confidence Hence picking actions effect intervals satisfy implies actions LLLIY au yuuI encouraging ITE methnrl A_ stochastic domains state picks actions nn 2 tnnw mnhictirntd vmcinn y vIYV 171 It maintains confidence upper interval high probability upper bounds If true value state stateaction pair maximize intervals llIv property high probability admissibility upper bound desired autoexploratory algorithm exploration ic hacd In AHlearning righthand giving high penalty OS Since p subtracted equivalent __ lzul effect achieved initializing R values AHlearning Simmons method optimal gain stops exploring ically characterizing problem The idea Autoexploratory However p fluctuates Rlearning p high value update equation action Koenig loop Theoret important open learning experiments Rlearning value conditions convergence AHlearning learning adapted modelfree optimai poiicy p converges As converges states optimal Y initialized Hlearning preliminary R Tudepalli D OkArrijicial Intelligence 100 1998 177224 221 update learning work small However small LY consequence Another possibility confidence maintain confidence interval interval R values IE method slowing learning p use upper bound algorithms 328 j Once propagate converge observed based modelfree controi poiicy iearned Q simpler need realtime reinforcement 46 Modelfree algorithms easier implement Most learning update procedures However experience independent action models learned update value function considering state actually learn stumbling representing practical Dynamic Bayesian networks number researchers represent action models useful learning shorten dynamic Bayesian network prior knowledge learns conditional probability tables One important networks automatically learn explicit action models fairiy quickiy possible states action reached They plan 40 However space simulated blocks explicitly time Our current method uses structure experience modelbased theoretic planning represent learn structure decision compactly transition matrices consumes Dyna architecture 7121930 We showed widely future research problems action models reinforcement learning information algorithms We showed piecewise linearity hfunction It member family regression Locally Weighted Regression technique sound statistical basis takes account LLR Local Linear Regression niques regression locality target function As result places complex LWR reinforcement including learning LWR effectively exploited tech 229 LWR 36 Our results suggest smooth applications fit functions juggling robot places There successful 1 1 R wneriallv Vrv lintnr rwmwcinn _ 6 2 prp_isincr nnnrnarh tn nnnrnvimatinn yAA e lr combines approxi synergistically lnral _ Averagereward RL We showed mating domain models Bayesian networks However local method scale number dimensions value function conveyor belts feature selection methods instead use aggressive approximation methods neural networks regression Another location AGV realvalued number AGVs machines combine like extend work domains features state space especially trees scale learning It necessary dimensions important problem larger domains domains nonlinear 91121 given To apply methods fullscale AGV scheduling need able handle __ number 01 AtiVs muitipie AGVs Since AGVs stiii expensive minimizing needed floor AGVs single agent scale positive scheduling AGV sets actions AGVs There including Critess work 11431 reinforcement bank elevators Tans results crossproduct results important practical problem Treating learning hunterprey set actions available multiagent simulation factory 222 P Tadepcdli D OkArrijicinl intelligence 100 1998 177224 experiments Our preliminary policy learned mapping Both AGVs share update policy This approach converges AGVs single agent simple domain 2 AGVs indicate optimal global state space actions single AGV optimal faster global optimal policy value treating follow function Another important assumption n _I_ rromems observable Recently Markov Decision solving POMDPs currently appear building form highlevel area progress domainspecific nA1sTn rumors needs relaxing state fully work extend RL Partially Observable best aigorithms large problems We believe learning algorithms ILL 53J unrortunateiy impractical prior knowledge features constraints value function mm 011 7 1 essential 7 Conclusions Reinforcement learning proved successful reinforcement number domains learning methods including currently sensitive prohibitively total reward family algorithms natural criterion averagereward RL method called Hlearning lead suboptimal exploration average optimize average policies strategy We presented optimize discounted RL methods AGV scheduling employing reward discount realworld domains However popular optimize discounted domains time step We showed ward factor empirically demonstrated modelbased learning method usefulness We presented previously outperforms form learning method exploit prior knowledge structure network time dynamic Bayesian npprlprl tn learn thm WP 11cn rhnwd va11w fmnctinnc nf Hlnmino cm VU C LVULI LIVLLB V c L L a_ lv _ vCVI y _ __ b linear effectively domains large number dimensions multiagent RL main prob lems variety realworld tasks action models improve scaling analysis We believe autoexploratory studied regression Several open problems laid foundation apply averagereward including theoretical strategies We showed version space approximated reinforcement exploration study learning based local Acknowledgments n __ acknowledge We gratefully support NSF grant number r support UNK grant number N666i495lUXI thank Chris Aikeson Leemon Baird Andrew Barto Dimitri Bertsekas Tom Dietterich Leslie Kaelbling Jude Shavlik Sridhar Mahadevan Toshi Minoura Andrew Moore Martin Puterman Satinder Singh Rich Sutton topic We thank Sandeep Seri help testing programs We thank reviewers suggestions paper excellent IRI9520243 discussions interesting xr_ I idepnlli D OkArtificiuI Intelligence 100 1998 177224 223 References 11 I DW Aha D Kibler MK Albert Instancebased learning algorithms Machine Learning 6 1991 3766 121 CG Atkeson AW Moore S Schaal Locally weighted learning Artificial Intelligence Review I I 1997 I l73 131 AG Barto SJ Bradtke SP Singh Learning act realtime dynamic programming Artificial Intelligence 73 1995 81138 IdI D Rprtsekns I I _ I__ A_ new valueiteration method averaop aI Technical Report LIDSP2307 MIT Boston MA 1995 rnst _L v rvb rlvnlmir nrnornmmino b F nmhlmm 5 DP Bertsekas Distributed dynamic programming 161 DP Bertsekas Dynamic Programming Optimal Control Athena Scientific Belmont MA 1995 IEEE Trans Automatic Control 27 3 1982 171 C Boutilier R Dearden M Goldszmidt Exploiting structure policy construction Proceedings 14th International Joint Conference Artificial Intelligence IJCAI95 Montreal Que 1995 18 1 J Boyan AW Moore Generalizing reinforcement learning safely approximating value function Proceedings Neural Information Processing Systems 1994 191 L Brieman JH Friedman RA Olshen CJ Stone Classification Regression Trees Wadsworth International Group Belmont MA 1984 IO GC Canavos Applied Probability Statistical Methods Little Brown Company Boston MA 1984 1 1 I RH Crites AG Barto hl_ I rFr Dl C r renlrl 0rcrllJ zua IIlldLIII Improving elevator performance reinforcement A m 17jr oJ 8 hlT Drr 0I_rl0 1111 r EiJJ Urrrruge learning ronr Advances I 121 T Dean K Kanazawa A model reasoning persistence causation Computational intelligence 5 3 1989 142150 13 I CW Gates The reduced nearest neighbor L 14 1 PE Hart The condensed nearest neighbor 1 IS RA Howard Dynamic Programming rule IEEE Trans Inform Theory 1972 431433 rule IEEE Trans Inform Theory 14 1968 515516 Markov Processes MIT Press Wiley Cambridge MA 1960 16 A Jalali M Ferguson Computationally efficient adaptive control algorithms markov chains IEEE Proceedings 28th Conference Decision Control Tampa FL 1989 I7 I LP Kaelbling Learning 1181 LP Kaelbling ML Littman AW Moore Reinforcement Embedded Systems MIT Press Cambridge MA 1990 learning survey J Artificial Jntelligence Research 4 1996 237285 191 U Kjaerulff A computational scheme reasoning dynamic probabilistic networks Proceedings 8th Conference Uncertainty Aniticiai inteiiigence i992 j iii29 1201 S Koenig RG Simmons The effect representation knowledge goaldirected exploration reinforcementlearning algorithms Machine Learning 22 1996 227250 21 I LJ Lin Selfimproving reactive agents based reinforcement learning planning teaching Machine Learning 8 1992 29332 I 122 ML Littman A Cassandra LP Kaelbling Learning policies partially observable environments scaling Proceedings International Machine Learning Conference San Fransisco CA 1995 pp 362370 I23 I S Mahadevan An average reward reinforcement learning algorithm computing biasoptimal policies Proceedings National Conference Artificial Intelligence AAAI96 Portland OR 1996 124 S Mahadevan Average reward reinforcement learning foundations algorithms empirical results Machine Learning 22 1996 lS9 195 125 I S Mahadevan Sensitive discount optimality Unifying discounted average reward reinforcement learning Proceedings international Machine Learning Conference Bari Italy 1996 26 S Mahadevan J Connell Automatic programming behaviorbased Intelligence 55 1992 3 I I365 learning Artificial robots reinforcement I271 WL Maxwell JA Muckstadt Design automatic guided vehicle systems Institute Industrial Engineers Trans 14 2 1982 I 14124 1281 AW Moore AG Atkeson Prioritized sweeping Reinforcement learning data time Machine Learning J 13 1993 1033130 224 P Ttrdeplli D OkArtijicicd Intelligence 100 1998 177224 1291 AW Moore CG Atkeson S Schaal Locally weighted learning control Artificial Intelligence Review I1 1997 75I 13 I30 1 AE Nicholson JM Brady The data association problem monitoring robot vehicles dynamic belief networks ECAI 92 10th European Conference Artificial Intelligence Proceedings Vienna Austria Wiley New York 1992 pp 689693 13 I D Ok A study modelbased average reward reinforcement learning PhD Thesis Technical Report 96302 Department Computer Science Oregon State University Corvallis OR 1996 1321 D Ok P Tadepalli Autoexploratory average reward reinforcement learning Proceedings National Conference Artificial Intelligence AAAI96 Portland OR 1996 I33 I R Parr S Russell Approximating optimal policies partially observable stochastic domains Proceedings National Conference Artificial Intelligence AAAI94 Seattle WA 1994 pp IO88 1093 I34 1 ML Puterman Markov Decision Processes Discrete Dynamic Stochastic Programming John Wiley 1351 L361 1371 I381 1391 1401 1411 1421 1431 1441 I451 1461 1471 New York 1994 S Russell P Norvig Artihcial Intelligence A Modern Approach PrenticeHall Englewood Cliffs NJ 1995 S Schaal C Atkeson Robot juggling implementation memorybased learning IEEE Control Systems Vol 14 1994 pp 5771 A Schwartz A reinforcement learning method maximizing undiscounted rewards Proceedings 10th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1993 SP Singh Reinforcement rlrrr rlvceeungs LULIVIILLI LUIIIILC 11 TLLIIICILLI uarrrlgrrrcr rrrrr71 hTnrnl rrrtra rnrrllr IA n n CIA _ Ac learning algorithms averagepayoff markovian decision processes Seatte WA MIT PiESS Cambridge MA 1994 RS Sutton Learning predict methods temporal differences Machine Learning 3 1988 944 RS Sutton Integrating architectures learning planning reacting based approximating dynamic programming Proceedings Seventh International Conference Machine Learning Austin TX 1990 P Tadepalli D Ok Hlearning A reinforcement learning method optimizing undiscounted average reward Technical Report 9430l Department Computer Science Oregon State University 1994 P Tadepalli D Ok Scaling average reward reinforcement learning approximating domain models value function Proceedings 13th International Conference Machine Learning 1996 M Tan Multiagent reinforcement learning independent vs cooperative agents Proceedings 10th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1993 G Tesauro Practical issues temporal difference learning Machine Learning 8 34 __ __ 1992 257277 S Thrun The role exploration learning control Handbook Intelligent Control Neural Fuzzy Adaptive Approaches Van Nostrand Reinhold New York 1994 CJCH Watkins P Dayan Qlearning Machine Learning 8 1992 279292 W Zhang T Dietterich A reinforcement learning approach jobshop scheduling Proceedings 14th International Joint Conference Artificial Intelligence IJCAI95 Montreal Que 1995