Artiﬁcial Intelligence 173 2009 11011132 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint A probabilistic plan recognition algorithm based plan tree grammars Christopher W Geib Robert P Goldman b University Edinburgh School Informatics 2 Buccleuch Place Edinburgh EH8 9LW United Kingdom b SIFT LLC 211 N First St Suite 300 Minneapolis MN 55401 USA r t c l e n f o b s t r c t We present PHATT algorithm plan recognition Unlike previous approaches plan recognition PHATT based model plan execution We clariﬁes diﬃcult issues plan recognition including execution multiple interleaved root goals partially ordered plans failing observe actions We present PHATT algorithms theoretical basis implementation based tree structures We investigate algorithms complexity analytically empirically Finally present PHATTs integrated constraint reasoning parametrized actions temporal constraints 2009 Elsevier BV All rights reserved Article history Received 28 November 2007 Received revised form 31 October 2008 Accepted 21 January 2009 Available online 24 March 2009 Keywords Plan recognition Bayesian methods Probabilistic grammars Task tracking Intent inference Goal recognition Action grammars 1 Introduction There increasing need automated systems understand goals plans human users Applica tions need understanding include assistive systems elderly network security insider threat detection agent based systems As develop systems ﬁnd early work plan recognition simplifying assumptions restrictive effective application domains Some simplifying assumptions include The observed agent pursuing single plan time The observed agents plans totally ordered Failing observe action means observed observer arbitrary subset actual actions The actions plan explicit temporal relations The plan representation purely propositional That actions parameters1 While limitations addressed individually new plan recognition PHATT ﬁrst provide solution issues single framework PHATT takes different approach problem intent inference2 systems approach allows address issues time Corresponding author Email addresses cgeibinfedacuk CW Geib rpgoldmansiftinfo RP Goldman 1 For example action goes home train station action goes train station home action moves arbitrary locations 2 Plan recognition referred task tracking intent recognition intent inference We use terms interchangeably 00043702 matter 2009 Elsevier BV All rights reserved doi101016jartint200901003 1102 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Most early work plan recognition treated plans patterns matched data recipes actions executed Unlike approaches PHATT based model plan execution simply elegantly captures key aspects plan recognition problem The critical observation approach goal driven agents actions consistent goals enabled actions taken We set actions agent execute given goals actions performed pending set Putting execution plans pending sets center plan recognition model build stochastic model plan execution develop probabilistic algorithm recognizing plans Like prior work assuming agents observed actively deceitful Deceitful agents attempt dissemble misdirect actions deliberately confuse observing agent directly achieve goals We discussing address issues The rest paper following structure We ﬁrst present example domain short review prior work particular attention limitations previous work PHATT addresses Sections 2 3 Section 4 describes intuitions PHATT algorithm followed theoretical core paper Sections 5 8 Section 5 formalizes plan libraries terms leftmost plan trees Section 6 provides abstract topdown algorithm closely parallels generative model providing smooth transition probability model ﬁrst step actual implementation Section 7 provides formal probability model use explanations produced plan trees Section 8 gives bottomup algorithm approximates topdown algorithm discusses implementation limitations The rest paper covers evaluating PHATT algorithm extensions Section 9 discusses algorithms formal complexity Section 10 covers empirical complexity results studies algorithms scalability Section 11 explains PHATTs use variables temporal constraints improve eﬃciency Finally Section 12 concludes paper discussion topics think particularly interesting areas future work 2 Background Most plan recognition algorithms require input plan library implicitly speciﬁes set plans recognized PHATT 202225 based model execution simple hierarchical plans 16 In framework plan libraries partially ordered ANDOR trees ANDnodes represent methods achieving particular task children ANDnode performed order perform parent task The children constrained performed particular order set possible orders annotating pairwise ordering constraints As example Fig 1 displays small example plan library taken network security domain In case attacker motivated toplevel ANDnode goals bragging Brag able boast hisher success crackers theft information Theft denial service DoS attacking network consuming resources longer serve legitimate objectives Attackers wish achieve bragging rights ﬁrst scan target network vulnerabilities scan attempt gain control getctrl They motivated exploiting control gain On hand attackers wish steal information scan vulnerabilities control target exploit control steal data getdata Finally attacker wishes DoS target need scan identify vulnerability carry DoS attack dosattack ORnodes plan library represent places agent choose number alternate methods achieve task Only children ORnode need performed order parent action achieved For reason ordering constraints children ORnode allowed For example Fig 1 ORnode dosattack possible children synﬂood synﬂood bind DoS attack bindDoS ping Death pingof death executed perform dosattack Since ORnodes represent choices plan refer choice points plan use terms interchangeably Fig 1 An example set plans In ﬁgure ANDnodes represented undirected arc lines connecting parent node children ORnodes arc Ordering constraints plans represented directed arcs ordered actions For example action scan executed getctrl executed getdata perform Theft CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1103 Our representation plans partially ordered ANDOR trees similar Hierarchical Task Network HTN represen tation Ghallab Nau Traversos recent textbook 24 pp 244245 account action preconditions postconditions Note example plan library displaying variety phenomena inter ested discussing uptodate realistic plan library security domain This library simply illustrates use method decomposition represented ANDnodes choice points represented ORnodes ordering constraints sibling actions We use plan library running example article 3 Previous work plan recognition Attempts perform plan recognition old artiﬁcial intelligence years large number methods applied plan recognition Some methods include rulebased systems parsing conventional stochastic graphcovering Bayesian nets costbased abduction Early approaches paid little atten tion choosing different explanatory hypotheses Either problem isolated separate problem particular early rulebased approaches ﬁnessed graphcovering approaches Our Bayesian approach addresses issue directly earlier Bayesian approaches Many approaches achieved computational eﬃciency limiting expressiveness plans particularly imposing rigid assumptions type num ber plans ordering steps Few approaches able account evidence failure observe actions critical domains 43 Cohen Perrault Allen 12 distinguish kinds plan recognition intended keyhole plan recognition In intended recognition agent cooperative actions intent understood For example tutor demonstrating procedure trainee provide case intended recognition In keyhole recognition recognizer simply watching normal actions ambivalent agent These cases arise example systems intended watch human user imperceptibly offer assistance appropriate context possible The earliest work plan recognition 4955 rulebased researchers attempted come inference rules capture nature plan recognition However underlying formal model rule sets diﬃcult maintain scale In 1986 Kautz Allen KA published article Generalized Plan Recognition 36 framed work plan recognition date KA deﬁned problem keyhole plan recognition problem identifying minimal set toplevel actions suﬃcient explain set observed actions Plans represented plan graph toplevel actions root nodes expansions actions unordered sets child actions representing plan decomposition To ﬁrst approximation problem plan recognition problem graph covering KA formalized view plan recognition terms McCarthys circumscription 41 Kautz presented approximate implementation approach recast problem computing vertex covers plan graph 35 This method eﬃcient exploits eﬃciency assumption observed agent attempting toplevel goal given time Furthermore account differences priori likelihood different goals Observing agent going airport algorithm views air travel terrorist attack equally likely explanations explain cover observations equally To best knowledge Charniak ﬁrst argue plan recognition best understood speciﬁc case general problem abduction reasoning best explanation 8 Charniak Goldman CG 10 argued viewing plan recognition abduction best Bayesian probabilistic inference Bayesian inference supports preference minimal explanations case equally likely hypotheses correctly handles explanations complexity different likelihoods For example set observations equally explained hypotheses theft bragging theft simple probability theory minor assumptions tell simpler hypothesis likely On hand hypotheses air travel terrorist attack explained observations equally prior probabilities dominate air travel seen likely explanation CG knowledgebased model construction KBMC 54 build Bayesian networks expressing particular plan recognition story understanding problems solved networks posterior probability explanations Previous systems handle failures observe actions Some systems assumed set actions observed complete failure observe action required achieve objective G suﬃcient reason conclude agent trying achieve objective G effectively application closed world assumption Other systems simply treated set observations arbitrary subset set actions actually executed For CG followed focus plan recognition story understanding 9 In human communication stories radically compressed omitting steps reader hearer infer based explicitlymentioned material background knowledge For example reading interesting story Jack went supermarket He paid groceries went home reader assume occurrence complicated steps plan shopping The reader assume reason Jack locate desired groceries pick store shelf In cases plan recognizer assume observing subset actuallyoccurring events Observers situations know actions carried use knowl edge Consider plan library Fig 1 What happen observed actions consistent scan getctrl 1104 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Assuming priori probabilities plan recognition conclude Brag Theft equally good explanations However time goes sees actions seeing actions contribute getdata certain Brag right explanation Theft Theft right explanation sooner later seen additional actions service getdata Systems like CG KA capable reasoning like start model plan execution time As result represent fact action observed In general systems solutions First assert action occur second silent action occurred implying failed notice action action occurred Neither solutions satisfying Parsingbased approaches plan recognition promise greater eﬃciency approaches cost making strong assumptions ordering plan steps Vilain 53 presented theory plan recognition parsing based KAs theory3 Vilain actually propose parsing solution plan recognition problem Instead reduces limited cases plan recognition parsing order investigate complexity KAs theory The major problem parsing model plan recognition treat partiallyordered plans interleaved plans Both partial ordering interleaving plans result exponential increase size required grammar issues addressed implementing PHATT We discuss relationship plan recognition parsing Section 9 More recently Pynadath Wellman PW proposed plan recognition method probabilistic based parsing They represent plan libraries probabilistic contextfree grammars PCFGs extract Bayes networks PCFGs interpret observation sequences Unfortunately approach suffers limitations plan interleaving Vilains PW propose probabilistic contextsensitive grammars PCSGs overcome prob lem diﬃcult deﬁne probability distribution PCSG 47 We return later paper discuss differences algorithm PW There large promising work Hierarchical Hidden Markov Models HHMMs 6 Conditional Random Fields CRF 383952 related approaches 2742 These approaches offer eﬃciency advantages parsing approaches additional advantages incorporating likelihood information sup porting machine learning automatically acquire plan models The ﬁrst work know area provided Bui 6 proposed model plan recognition based variant Hidden Markov Models HMMs A similar HMM serves foundation work Buis work based model plan execution address case multiple goals The work CRFs similar approaches title activity recognition 2738404252 promising recognized solving different problem addressed The early work area carefully chose term activity behavior recognition distinguish plan recognition The distinction activity recognition plan recognition difference recognizing single possibly complex activity recognizing relationships set activities result complete plan Much work activity recognition seen discretizing sequence possibly noisy intermittent lowlevel sensor readings coherent actions treated inputs plan recognition Much work activity recognition deﬁnes problem labeling element sequence obser vations single unstructured activity label While labels varying degrees abstraction process address activity labels combined construct complex structures representing larger plans like produced PHATT The distinction activity recognition plan recognition similar distinction natural language processing NLP community tagging identifying speech tags individual words task CRFs shown good 39 points parsing combines words speech tags sentences While problems related distinct problems activity recognition plan recognition Our work PHATT focused ﬁrmly plan recognition activity recognition Several researchers interested keyhole recognition improve team coordination That agents team recognize teammates better cooperate coordinate They able learn shared environment For example member military squad sees teammate ducking cover infer threat takes precautions Huber et al 31 present approach keyhole plan recognition coordinating teams Procedural Reasoning System PRS based agents Their approach like CG based KBMC They developed approach automatically generating belief networks plan recognition PRS knowledge areas hierarchical reactive plans The important difference work theirs obtain simpler structure working plan representation directly instead generating intermediate representation belief network inhibitory links Further clear handle interleaving multiple plans development plans time Kaminka et al 34 present algorithm keyhole recognition teams agents This work number nice properties probabilistic rooted model plan execution considers question handle missing 3 This ﬁrst attempt cast plan recognition parsing 51 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1105 Fig 2 A simple model plan execution A given set plans deﬁnes initial pending set actions enabled execution initial state At time step action pending set chosen execution ChooseForExec The execution chosen action makes progress executing plans enables actions To model time step generate new pending set enabled actions based previous pending set executed action observations state changes However differs work signiﬁcantly different model plan execution assumes agent pursuing single plan time Finally work differs work devoting great deal effort knowledge team social structures conventions infer overall team behavior AvrahamiZilberbrand Kaminka 1 reported closely related work In order draw contrasts clearly return discuss work provided intuitions algorithm 4 Intuition PHATT takes Bayesian approach plan recognition Our Bayesian reasoning customary based stochastic generative model phenomena reasoned For plan recognition requires building stochastic model process choosing executing plans making observations executing plans This model gives probability plan P plan4 probability observations given plan P obsplan We invert model observations hypotheses underlying plans reason model probability plans given observations P planobs Bayes law In section provide intuitions PHATTs model plan execution process inverted infer plans A detailed formal treatment given following sections The central idea PHATT plan execution model plans executed dynamically result action agent takes time step critically depends actions previously taken Intuitively moment executing agent like execute actions contribute current goals successfully execute enabled prior execution predecessor actions plan We set actions contribute agents current plans enabled previous actions pending set These actions pending execution agent With idea mind build model plan execution First agent chooses set goals To achieve goals agent chooses set methods commits set choices ORnodes plan library Before agent begins acting subset actions plans prerequisite actions These actions form initial pending set agent From initial set agent chooses action execution After agent performs action agents pending set changed Some actions removed pending set action executed example actions added pending set actions enabled execution previous action The agent choose action new pending set process choosing action building new pending sets repeats agent stops performing actions ﬁnishes plans This process illustrated Fig 2 We probabilistically simulate model plan execution sampling agents goals plans repeatedly choosing elements resulting pending sets generating future pending sets later actions selected We note probabilistic model plan execution HMM observer agents goals choices ORnodes plans pending sets To use model perform probabilistic plan recognition observations agents actions input invert generation process build explanation observed actions By hypothesizing goals plans agent stepping forward observation trace generate possible sequence pending sets When reach end set observations assignment observed action hypothesized plan achieves agents hypothesized goals sequence pending sets consistent observed actions This collection plan structures pending sets single complete explanation observations 4 We unpack meant probability plan later 1106 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 We assume input plan library augmented probabilities speciﬁcally prior probabilities root goals method choice probabilities probabilities picking elements pending sets In paper simplifying assumptions simplify derivation probabilities It noted simplifying assumptions essential functioning PHATT fairly easily relaxed We provide brief discussion happens assumptions violated Section 84 On basis probabilities compute probability explanation Since taking probabilis tic approach want compute conditional probability particular explanation exp given set observations obs Using Bayes rule conditional probability explanation P expobs P exp obsP obs We exploit equivalent formulation assuming mutually exclusive exhaustive set explanations P expobs P exp obs cid2 obs P expi cid2 P expP obsexp P expiP obsexpi denominator sums probability mass explanations produce probability observations In work explanation allowed contain multiple hypothesized goals provide formal deﬁni tions terms section Since compute probability single explanation observations build complete set possible explanations observations compute probability compute single goals conditional probability summing probability mass explanations contain goal P goalobs P expi obs cid2 expi goalexpi In previous work 25 presented direct implementation model exactly presented We prover Pooles Probabilistic Horn Abduction PHA logic 4445 We provided PHA rules described generative model PHA prover able use formulation generative model plan library sequence observations perform probabilistic plan recognition topdown manner hypothesizing set possible root goals generating pending sets explanations described We discuss topdown algorithm Section 6 However required assuming single instance possible root goal This provided ﬁnite hypothesis space required topdown algorithm This unusual assumption plan recognition thought acceptable However later applications assumption restrictive Domains like network security regularly multiple instances root goal active time 21 To meet needs developed bottomup algorithm PHATT This algorithm performs essentially inference directly manipulates tree structures representing probabilistic explanations resolution theorem proving Being bottomup required single instance assumption We discuss provided formal speciﬁcation topdown bottomup versions algorithm terms tree structures With intuitions hand worth noting AvrahamiZilberbrand Kaminka 1 similar approach differ subtle points While maintain set hypotheses manner work instead model plan execution pending sets foundation work check consistency observed actions previous hypotheses This allows solve problems address allow recognize tasks depend critically pending set including handling negative evidence seeing actions In order compute probabilities explanations AvrahamiZilberbrand Kaminka suggested use HMMs area future work In following sections ﬁrst formally deﬁne explanations pending sets based plan library tree structures We manner similar deﬁnitions context free grammarsCFGs 28 extend handle partial ordering actions 5 Formalizing explanations The foundation plan recognition collection plans recognized These plans speciﬁed formal language In section ﬁrst deﬁne language specifying plans form plan library provide number deﬁnitions terms tree structures algorithms built plans deﬁned plan library The critical structures set generating trees constructed plan library build plan structures This set deﬁnitions culminate formal deﬁnition explanations speciﬁcation algorithm generation based plans plan library In case formal deﬁnitions provide intuitions try aid reader CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1107 51 Plan tree grammars Deﬁnition 51 We deﬁne plan library tuple PL cid5Σ NT R P cid6 Σ ﬁnite set basic actions terminal symbols NT TNT NNT ﬁnite set nonterminal symbols TNT NNT R distinguished subset intendable root nonterminal symbols R NT P set production rules form A α φ A NT A TNT α single terminal symbol σ Σ φ In addition following conditions A TNT A σ1 P A σ2 P cid12 σ1 σ2 A B TNT A σ P B σ P cid12 A B σ Σ A TNT A σ P instead A NNT 1 α string symbols NT 2 φ jαi α j αi α j refer ith jth symbols α respectively Following traditional CFG based encodings hierarchical plans plan library deﬁnes set production rules P distinguished set nonterminal symbols R representing root goals plans recognized expanded sequences terminal nonterminal symbols NT By repeatedly applying rules nonterminals given root goal symbol reduced sequence contains terminal symbols Σ This sequence terminal symbols represents observable actions instance high level goal Our deﬁnition diverges traditional CFGs ways First terminal grammar plan tree grammars distinguished nonterminal captured set TNT maps uniquely terminal symbol The set NNT captures nonterminals grammar uniquely map terminal We discuss Deﬁnition 53 Second plan tree grammars explicit ordering constraints production rules CFG deﬁned relation These constraints indicate actions performed speciﬁc order In traditional CFGs ordering symbols production indicates required ordering plan In grammars symbols right hand production rule assumed unordered ordering relation production states Our grammar formalism similar work IDLP grammars 32 grammar formalisms separate ordering constraints decomposition We Section 9 Note root goals members R appear right hand production That root nodes permitted appear derivation tree required appear derivation tree To tie representations use paper production rules formulation plan library correspond ANDnodes plan tree shown Fig 1 ORnodes Fig 1 captured production rule nonterminal For example productions Theft scan getctrl getdata 1 22 3 scan zonetrans ipsweep portsweep 1 21 3 getctrl getctrllocal getctrl getctrlremote getdata snifferinstall defaultlogin capture ﬁrst levels plan Theft shown Fig 1 rules getctrl capturing ORnode Deﬁnition 52 Given rule ρ A β φ βi leftmost symbol child A given ρ cid2 j j φ Intuitively set leftmost symbols given rule symbols ﬁrst expansion nonterminal rule No action right hand rule ordered symbols rules ordering constraints Note deﬁnition require unique leftmost symbol rule We denote set leftmost symbols rule ρ Lρ We use Rρ denote set symbols leftmost ρ Ie ρ A β φ Rρ β Lρ interpreted set difference Deﬁnition 53 Given plan library PL cid5Σ NT R P cid6 terminal symbol σ Σ deﬁne leftmost tree T deriv ing σ tree 1 Every node T labeled symbol Σ NT 2 Every interior node T labeled symbol NT 1108 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 3 If interior node n T labeled A children labels β1 βk ρ P ρ A β1 βk φ node n additionally annotated ρ children n labeled symbols Rρ children leftmost child n children 4 There single distinguished node frontier T labeled terminal symbol node labeled σ We foot tree T denote footT Leftmost trees left branching trees frontier contains single terminal symbol In case intendable root nonterminal tree capturing leftmost spine expansion root nonterminal ending speciﬁed terminal symbol Note inclusion T N T set nonterminal symbols deﬁnition plan library allows guarantee create leftmost trees single terminal symbol frontier Leftmost trees correspond closely minimal leftmost depthﬁrst derivation trees speciﬁc terminal tra ditional CFGs The difference grammars ordering relation deﬁned plan library determine methodsnonterminals leftmost We use leftmost trees build explanations elements explanatory hypotheses To ﬁrst deﬁne generating set trees particular plan library deﬁne process trees composed produce derivations sequences observations Deﬁnition 54 A set leftmost trees said generating plan library PL cid5Σ NT R P cid6 contains leftmost trees derive action Σ rooted nonterminal NT We denote generating set GPL refer members generating trees To combine trees generating set build larger trees deﬁne substitution This process frontier nonterminal existing tree replaced leftmost tree rooted nonterminal symbol obeying ordering constraints deﬁned plan library Deﬁnition 55 Given tree T init frontier nonterminal node m let node n ms parent node assume n labeled A annotated rule A β1 βk φ Let ms label βi assume given leftmost tree root labeled βi T βi We T βi substituted m T init resulting T res case j j φ frontier subtree T init rooted ns child labeled β j contains terminal symbols T res tree obtained replacing βi T βi In order tree substituted nonterminal existing tree portion original trees frontier precedes substitution site completely expanded contain terminal symbolsbasic actions This guarantees ordering constraints contained original grammar met Aside partial ordering require ment portion plan precedes substitution site fully expanded deﬁnition substitution Joshis deﬁnition tree adjunction 33 Note set nonterminals TNT production rules rewrite elements TNT single elements Σ included deﬁnition plan library permit uniform treatment substitution generating trees To emphasize fact majority trees dealing built repeated substitution following work natural language processing refer trees derivation trees distinguish original leftmost trees generating set We derivation trees partial trees frontier contains nonterminals In remainder discussion PHATT algorithm multiple identical partial trees need distinguish Accordingly talk particular tree instances distinguished general tree deﬁnitions plan library having rigid designator associated indexing purposes Thus following discussion couched terms introducing new tree instances substitution particular tree instances We position formally deﬁne substitution set The substitution set fulﬁll role pending set referred introduction Since want algorithm support multiple root goals multiple instances root goal deﬁne substitution set relative given set partially expanded derivation tree instances Deﬁnition 56 Given plan library PL set partial derivation tree instances D representing plan instances PL deﬁne substitution set D represented PSD set tree instances T GPL substituted tree D Each tree instance PSD indexed tree D substituted particular nonterminal tree substituted In special case D deﬁne PSD The substitution set set containing instance tree GPL substituted tree D CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1109 The pending sets discussed earlier extracted substitution sets While initial discussion pending sets couched terms actions deﬁnition produced actually terms tree structures support inclusion particular action agents plan For reason possible multiple tree instances substitution set foot labeled terminal action symbol Such tree instances unique designed substitute different nonterminals derivation trees making different commitments achieve plan Our original pending set set terminal symbols occurring foot tree substitution set Deﬁnition 57 Given plan library PL cid5Σ NT R P cid6 set possibly partial derivation trees D substitution set D PSD deﬁne PendingSetD cid3 cid4 xt PSD x foott For example PHATT seen partial input zonetrans considering explanation single root goal Brag substitution set IPSWEEP ipsweep PORTSWEEP portsweep5 From extract pending set ipsweep portsweep Since pending set substitution set closely related use terms interchangeably difference critical Finally formally deﬁne explanation series observations Intuitively explanation set observations pair contains 1 A forest possibly partial derivation trees representing set plan instances observed action assigned terminal symbol obeying ordering constraints plan library We note set derivation trees order explicitly support multiple root goals interleaved plan execution 2 The sequence substitution sets production particular derivation trees As Section 7 substitution sets computing probability explanation helpful explicitly carry deﬁnition Therefore Deﬁnition 58 We deﬁne explanation sequence observations σ1 σn cid5Dn PSD0 PSDncid6 Dn possibly forest possibly partial derivation trees terminal leaves set σ1 σn PSD0 PSDn series substitution sets construction Dn In rest discussion use terms possible partial explanations wish emphasize explanation processed observed actions Keep mind possible later observations inconsistent earlier choices explanation rule explanation entire set observations With collection deﬁnitions hand position provide formal topdown algorithm computing explanation set observations 6 Topdown algorithm building explanations trees In section provide nondeterministic algorithm explanation generation As nondeterministic algorithms idealized picture program ﬁrst guesses observed agent intends veriﬁes hypothesis matching observations hypothesis This algorithm close earlier implementation 25 While algorithm practical closely parallels generative model provides good basis probability discussion follows bridge bottomup PHATT algorithm We deﬁne process explanationbuilding stages construction initial hypothesis progression hypothesis incorporating new observation We want algorithm support multiple root goals multiple instances root goal deﬁne initial goal hypothesis set root goal instances Deﬁnition 61 An initial goal hypothesis D0 set instances nonterminals R ordering constraints Since ordering constraints elements initial goal hypothesis available imme diately trees substituted 5 Note IPSWEEP member NNT corresponding terminal ipsweep similarly PORTSWEEP 1110 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Procedure 61 Topdown explanation generation PROCEDURE Explainσ1 σn CHOOSE initial goal hypothesis D0 R E cid5D0 PSD0cid6 LOOP FOR 1 n DO CHOOSE T new PSD i1 footT new σi D SubstituteT new D i1 E cid5D PSD0 PSD icid6 END LOOP RETURN E The procedure ﬁnding single explanation works ﬁrst CHOOSE operation select initial goal hypothesis After computing substitution set set goal instances algorithm loops sequence observations second CHOOSE operation select elements substitution set substitution current set derivation trees incrementally produce derivation trees pending sets explanation We overstress fact Explain nondeterministic algorithm The CHOOSE operations nondeterministic choice operations To resolve operations single explanation use search Keep mind selection initial goal hypothesis determines goal instances pursued observed agent topdown algorithm effectively generate test algorithm The ﬁrst CHOOSE operation generating hypothesis root goals inner loop testing observations conform hypothesis A naive searchbased implementation algorithm faces formidable search problem To ﬁnd single expla nation large number hypotheses possible goals plans pursued achieve considered Most account observed actions abandoned The handling search left implicit nondeterministic nature CHOOSE operators Worse number goals priori bounded space explanatory hypotheses theoretically inﬁnite While practical abstract topdown algorithm helps providing crisp deﬁnition search space aid discussion probability model explanations cover section After discussion probability model return discuss bottomup algorithm building explanations incrementally based observations closer implementation PHATT align cleanly probability model 7 The probability model Recall Section 4 PHATT compute P exp obs P expP obsexp explanation exp given set observations obs We ﬁrst term P exp probability agent having hypothesized goals plans derivation trees explanation The second term P obsexp probability observed actions chosen associated sequence substitution sets We break probability P exp terms term probability agent hypothesized set root goals P goals second term probability given set plans chosen achieve goals P plansgoals Note assume observed actions conditionally independent given goals We assume probability root goal independent goals explanation This results following formula Formula 71 P exp obs P goalsP plansgoalsP obsexp The ﬁrst term P goals prior probability set root goal instances adopted actor In PHATT prior root goal given plan library represent probability agent adopting goal G P G In order compute probability set goals product probabilities We note modeling goal selections independent unrealistic discussion assumption later section To address issue multiple instances goal deﬁne P G probability agent adopts instance G keeps sampling Therefore probability exactly n instances G P Gn1 P G geometric distribution This certainly incorrect intuition probability multiple instances single goal goes far rapidly However practice oversimpliﬁcation benign effect prior evidence underestimated decline probability suﬃcient good results The theory accommodate sophisticated probability models fewer independence assumptions add signiﬁcant computational cost Letting G exp represent number instances goal G explanation exp following formula CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1111 Formula 72 P goals cid5 G exp P G cid6 1 P G cid7 cid5 cid7 cid6 1 P G Ggoals G goals rewritten Formula 73 P goals cid5 G exp P G cid5 cid7 cid6 1 P G Ggoals GR Note second term formula product set intendable root goals R actually constant explanations The second term Formula 71 P plansgoals probability agent choosing particular means achieve goal In model determined choices ORnodes derivation trees Therefore nonterminalsubgoal multiple production rules PHATT probability given rule expand speciﬁc nonterminal For example Fig 1 cyber attacker use synﬂood bindDoS pingofdeath denial service attack PHATT distribution likely possible attacks given agent going commit denial service attack Typically assumed production rule A α φ plan library equally likely given agent attempting achieve subgoal left hand Ie P A α j φ j A P A αk φk A j k Therefore deﬁne A symbol A NNT number rules P A left hand This allows write following formula Formula 74 P plansgoals cid5 Aplans 1 A This product deﬁned nonterminal ORnodes plan forest choice points plan forest Note uniformity assumption required framework One specify nonuniform distribution choices case methods likely The term P obsexp probability particular sequence string actions executed agent carrying plan If single root goal plans actions totally ordered conventional CFG unique sequence plan term zero However partial orders multiple interleaved plans term probability observed sequence actions selected sequence substitution sets To compute probability actions chosen substitution sets order assume actions substitution set equally likely Thus particular substitution set time k explanation exp probability speciﬁc element set given 1PSk Again note uniformity assumption required Any distribution This choice conditioned state world hypothesized root goals plans agent This results following formula Formula 75 P obsexp P obs1expP obs2exp obs1 P obsnexp obs1 obsn1 P obs1exp 1PSD0 obs1 PSD0 0 P obsiobs0 obsi1 1PSD i1 0 P obsexp cid8 cid8 cid8 cid8PSD 1 ncid5 i1 obsi PSD i1 With formulas hand rewrite Formula 71 time Formula 76 P exp obs K cid5 G exp P G cid5 K constant Ggoals cid9 GR 1 P G Aplans 1 A ncid5 i1 cid8 cid8 cid8 cid8PSD 1 1112 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Remember given formula probability explanation conditional probability particular goalG computed summing probability mass explanations expi contain goal dividing total probability mass associated explanations That Deﬁnition 71 P Gobs cid2 expi Grootsexpi P expi obs cid2 exp P exp obs denominator sums probability explanations observations numerator sums proba bility explanations goal G occurs Recall denominator prior probability observations usually The use different probabilities differentiates work work probabilistic grammars 4753 Most prob abilistic context free sensitive grammar PCFGPCSG research included use single probability grammar rule capture likely given nonterminal expanded grammar rule This leaves term substitution sets making diﬃcult systems address partially ordered plans multiple concurrent plans partial observability Simplifying assumptions PHATT makes number simplifying assumptions uniformity assumptions probability parameters One ask effect assumptions We room address issue experimentally fair evidence diagnostic applications Bayesian systems robust inaccuracy parameters 46 additionally analytic information assess sensitivity conclusions probability parameters 7 We drawn research following assessment Before discuss PHATTs sensitivity assumptions parameters like stress uniformity assumptions PHATT makes inherent algorithm These assumptions relaxed cases signiﬁcantly complicating PHATT In following discussion discussing effect simplifying assumptions applied PHATTs probability parameters discuss relaxed To recapitulate kinds probability parameters PHATT subject simplifying assump tions 1 prior probabilities goals 2 probability choosing method alternative methods single sub goal 3 probability choosing speciﬁc action set currently pending actions We discuss turn Prior probabilities goals PHATTs performance sensitive prior probabilities intendable root nodes extent input ambiguous Eg example library Fig 1 theres distribution uniform root goals scanrelated actions extent priors So prior probability Brag 2 Theft DoS 1 posterior probability given seen zonetrans ignoring possibility multiple plans P Bragzonetrans 5 P Theftzonetrans P DoSzonetrans 25 However observations problem tend correct Indeed rule multiplegoal explanations appearance datagathering DoSrelated actions eliminate Brag goal entirely If permit multiplegoal explanations posterior probability Brag DoS seen subsequence ending synﬂood approximately 2 11 2 1 17 versus 112 83 DoS The uniformity assumption prior probabilities goals easiest assumption change PHATT implementation Since priors weighting factor applied explanation PHATT need modi ﬁcation accommodate nonuniform priors There issues related uniform priors assumption root goals The ﬁrst issue assumption root goals independent For goals absurd example pairs brewing tea making toast bungee jumping brewing tea unlikely independent The probably positively correlated negatively Positive correlations handled relatively easily introducing new toplevel goals activate combinations original goals For example afternoon tea goal method involved brewing tea making toast PHATTs model friendly negatively correlated goals In order accommodate elaborate version approach positively correlated goals In elaborate version specify probabilities combinations subgoals contexts For example extreme sports context quiet afternoon context turn resp goals like bungee jumping resp goals like afternoon tea Doing require slightly machinery wouldnt change probability computations CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1113 root goal probabilities need computed However bottomup goal introduction method need modiﬁed interact context structures One determine application application basis kind machinery necessary note example unlikely lot action sequences cause PHATT believe agent actually bungee jumping brewing tea independence assumption probably benign The second issue model probability multiple instances root goals hypergeometric distribution root goal This likely overstates probability multiple instances root goals However PHATT implementation doesnt consider explanations primed speciﬁc observations Section 81 details This limits overestimate cases evidence hypothesis Probabilities methods given subgoals Here effect incorrectness assumptions cause confusion extent observation sequence ambiguous For example consider case PHATT seen input considering equally likely explanations root goal A root goal B plan proceed C D Under uniformity assumption PHATT C D continue treat A B equally likely However consider case C likely given root goal A versus B In case true posterior odds A versus B proportional odds choosing C given A versus given B For example probability choosing C given A 9 probability choosing C given B 1 PHATT substantially disambiguating observation Such deviation uniformity arise sample plan library multiple ways getctrl Suppose braggarts likely unsophisticated script kiddies prone gross exploits data thief likely precautions remain undetected We considered problems kind deviation uniformity happens nonuniform root goal priors easy modify PHATT probabilities account Once probabilities simple weighting factors PHATT Probability distributions substitution sets Recall PHATT assumes agent equally likely execute enabled actions For domains going inaccurate In domains agents likely operate depthﬁrst way persist working subgoals single goal satisﬁed On hand timepressured domains multiple simultaneous goals consider shortorder chef example agent timeslice multitask It possible speciﬁc goals methods achieving situational features like weather effect selection action execution In cases true probability action selection affected features question PHATTs probabilities substantially However hard extend PHATT features ac count It simply matter storing relevant features explanation compute probabilities Depending domain advanced modeling helpful signiﬁcant compu tational cost little impact It area future work identify complex modeling worth We provided formal deﬁnitions topdown algorithm probability model plan recognition based model plan execution However pointed Section 6 topdown algorithm requires exhaustive search inﬁnite search space To eliminate problem section outline similar bottomup algorithm introduces root goal hypothesis suggested observed actions circumventing need initial goal hypothesis 8 Practical explanation building 81 Bottom explanations By taking bottomup approach explanation generation introduce goals explanation observing action consistent initially hypothesizing set root goals Taking approach requires additional deﬁnition Deﬁnition 81 We deﬁne set intendable trees I set generating trees rooted intendable nonterminal symbol cid3 I T GPL rootT R cid4 rootT denotes label root node tree T Every tree I derives ﬁrst action plan rooted distinguished intendable root nonterminals Note multiple trees I root leftmost child interior nodes tree different 1114 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 In algorithm started explanationﬁnding process choosing initial goal hypothesis In bottomup algorithm hand initial hypothesis set add new tree instances copied elements I set derivation trees new goals associated plans suggested observed actions Procedure 81 Bottomup single explanation generation PROCEDURE Explainσ1 σn E cid5 cid6 LOOP FOR 1 n DO cid5D T PS0 PSi1cid6 E CHOOSE 1 T new PSi1 footT new σi DTnew SubstituteT new DT PSnew PSD T new E cid5DTnew PS0 PSi1 PSnewcid6 OR 2 T new I footT new σi DTnew DT T new PSnew PSDTnew cid17 cid17 PS 0 PS i1 E cid5DTnew PS BackpatchPSrootT new PS0 PSi1 cid17 0 PS cid17 i1 PSnewcid6 END LOOP RETURN E END PROCEDURE PROCEDURE BackpatchPSR PS0 PSm T addition t I roott R LOOP FOR 0 m DO PSi T addition cid17 PS END LOOP cid17 cid17 RETURN PS 0 PS m END PROCEDURE The bottomup algorithm Procedure 81 dynamic programming algorithm maintains current set derivation trees based set derivation trees previous iteration A single set derivation trees build explanations suﬃcient support probability computations need sequence substitution sets The substitution sets derived sequence partial derivation trees maintaining sequence substitution sets provides PHATT savings time memory The topdown algorithm Procedure 61 initially guessed set goals expanded plan trees downward featured nondeterministic CHOOSE operations However Procedure 81 choices moved single point algorithm complex choice There possibilities First observed action contribute goals derivation trees case 1 CHOOSE operator way topdown algorithm Second observed action introduce entirely new plan entirely new root goal case 2 CHOOSE operator selecting T new I6 Backpatching explanation While algorithm introduces new goals hypotheses needed conceptually goals That models view agent goals time ﬁrst action selected initial substitution set simply chosen execute actions contributed plan goal Since algorithm adds goals needed new goal introduced prior substitution sets ex planation contained trees plan Thus prior substitution sets incorrect To later use substitution sets correctly compute probability explanation backpatched account presence new goal prior time points To requires adding tree instances prior substitution sets intendable trees new goal root These amended substitution sets computed BackpatchPS function case 2 The cost bounded number observations relatively inexpensive algorithm We note backpatch operation relatively simple set union rapid careful bookkeeping 6 It possible new root goal instance root goal present set derivation trees CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1115 Fig 3 The process building single explanation observations associated pending sets Starting upper left moving clockwise pending set grouped observation follows partial derivation trees result observation The set intendable trees root goal identiﬁed stored execution removing cost computing set Thus cost operation reduced O n dominated costs algorithm Keep mind operation happens new root goals introduced explanation Thus case agent single goal happens exactly ﬁrst observation An example Here PHATT construct explanations sequence observations zonetrans ipsweep zonetrans Fig 3 shows construction derivation trees related substitution sets We note save space ﬁgure represented trees substitution sets pairs trees foot terminal root instance symbols The ﬁrst zonetrans observation introduces instance intendable tree goal DoS indexed 1 We note introduction new root goal requires backpatch initially pending set tree zonetrans observation DoS root goal The requirement backpatch indicated asterisk substitution set results new substitution set time t0 Pendingt0 zonetrans DoS1 The addition plan adds trees substitution set actions ipsweep portsweep enabled plan This plan extended explain ipsweep observed Finally zonetrans observed algorithm introduces second instance goal DoS This results number additions substitution set The asterisk ﬁnal substitution set indicates previous partial explanations backpatched include initial actions DoS2 Thus explaining observations rounds backpatching ﬁnal explanation observations contain derivation trees shown pane Fig 3 following sequence substitution sets Pendingt0 zonetrans DoS1 zonetrans DoS2 Pendingt1 ipsweep DoS1 portsweep DoS1 zonetrans DoS2 Pendingt2 portsweep DoS1 zonetrans DoS2 Pendingt3 portsweep DoS1 ipsweep DoS2 portsweep DoS2 Note addition zonetrans DoS2 element prior substitution sets addition zonetrans DoS1 element initial set result backpatching We compute probability particular explanation multiplying terms described Formula 71 First multiply priors root goal instances DoS Second multiply probability choice points plans In case term choice points plans 1116 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Third ﬁnally multiply probability associated action chosen execution substitution set given uniformity assumption If assume P DoS 6 result following cid6 example zonetrans ipsweep zonetrans cid7 P 06 06 10 05 0333 05 002999 Keep mind possible explanations observations process repeated explanations compute conditional probability given goal Completeness The topdown algorithm clearly complete sense generate possible explanations observation trace Completeness complicated question bottomup algorithm Where observation sequences concerned bottomup algorithm complete By observation se quence mean observation sequence contains set actions generated set plan trees If Procedure 81 given sequence input ﬁnd corresponding explanation A sketch proof follows root goal g actual explanation observations exists tree T I observation σ j rootT g footT σ j Processing loop σ j cause second 2 clause choose operator Procedure 81 insert T set derivation trees All structure plan g added substitution operations Deﬁnition 55 Those substitutions executed ﬁrst 1 clause choose operator Procedure 81 appropriate σ s processed Thus observation traces Procedure 81 complete This notion completeness necessarily right incremental plan recognition Ideally like algorithm complete sense given observation sequence p algorithm constructs explana tions possible complete observation sequences ω st ω p ωcid17 7 For previous approaches KA limit hypothesis spaces plans generated single root goal deﬁnition like makes sense However PHATTs explanation space ﬁnite going able realize sense completeness concrete implementation For example consider case single observation zonetrans original plan library Fig 1 The bottomup PHATT algorithm consider instance root goals zonetrans initial action plans meaning set hypothesized goals Brag Theft DoS However single action consistent inﬁnite number explanations Brag Theft Brag Theft DoS agent multiple goals actions ωcid17 While Procedure 81 doesnt satisfy strong sense incremental completeness provides weaker sense incremental completeness follows For observation sequence p completion ω p ωcid17 algorithm explanation ω This follows construct partial explanation E completed E proof completeness observation sequences We strengthen claim incremental completeness follows Given observation sequence p completion ω p ωcid17 algorithm ﬁnd explanations E completed E explanation ω p contains action contributing plan E E cid17 cid17 cid17 82 The algorithm Finally present pseudocode algorithm combines construction explanations computation probabilities explanation Note ﬁrst second terms Formula 76 goals choice node probabilities computed explanation constructed ﬁnal term computed ﬁnal set nested loops explanations Procedure 82 Full algorithm PROCEDURE ExplainAndComputeProbσ1 σn Initialize data structures D0 E Emptyqueue Enqueuecid5D0 PSD0 1 1cid6 E Loop observations LOOP FOR 1 n DO Loop explanations queue WHILE NonemptyE DO cid17 Emptyqueue E cid5DT PS0 PSi1 probroots probchoicescid6 DequeueE Consider existing plans observation extend LOOP FOR EACH T new PSi1 footT new σi DTnew SubstituteT new DT 7 Where concatenation operator CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1117 PSnew PSDTnew localprobchoices probchoices PTST new Enqueuecid5DTnew PS0 PSi1 PSnew probroots localprobchoicescid6 E cid17 END FOR EACH LOOP Consider new plans observation introduce LOOP FOR EACH T new I footT new σi cid17 i1 cid17 0 PS DTnew DT T new PSnew PSDTnew PS localprobchoices probchoices PTST new localprobroots probroots P rootT new Enqueuecid5DTnew PS END FOR EACH LOOP cid17 0 PS BackpatchPSrootT new PS0 PSi1 cid17 i1 PSnew localprobroots localprobchoicescid6 E cid17 END WHILE cid17 E E END LOOP result Compute probability explanation WHILE NonemptyE DO cid5DT PS0 PSn probroots probchoicescid6 DequeueE localprobpend 1 LOOP FOR 1 n 1 DO localprobpend localprobpend 1PSi END LOOP probhypoth probroots probchoices localprobpend result result cid5DT PS0 PSn probhypothcid6 END WHILE RETURN result The functions Emptyqueue Nonempty Enqueue Dequeue standard functions queue data structures Elements queues fourtuples cid5D T PS0 PSi probroots probchoicescid6 ﬁrst element DT set derivation trees second sequence substitution sets probroots product probabilities root goals probchoices product probabilities choices As case function BackpatchPS updates prior substitution sets adding trees present goal introduced T new known earlier The function PTS gives probability associated choice nodes T new Since choice probability tree constant tree computed oﬄine Once set explanations built ﬁnal nested loops iterate explanation computing probability contribution substitution sets Note ﬁnal loop doesnt include contribution PSn action selected substitution set observed Finally root node probabilities probabilities choices probabilities drawing pending sets localprobpend multiplied returned probability explanation 83 Implementation differences The implementation PHATT follows algorithm closely variations enhance eﬃciency PHATT maintain complete substitution sets sizes actually needed signiﬁcant time memory savings PHATT caches intendable trees order avoid recomputing signiﬁcant spacefor time tradeoff PHATT leaves probabilities missing goals goals dont appear explanation simply constant factor explanations Finally PHATT allows bounded recursion explanations We discuss issues Storing sizes substitution sets Given uniformity assumptions computing likelihood particular action chosen substitution set needed computation size pending set Thus maintaining complete substitution sets implementation stores backpatches sizes sets Caching intendable trees Procedure 82 separate inner loops search extensions known trees search new trees added Actually PHATT combines pending set I form single set This number beneﬁts 1118 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Fig 4 An explanation including derivation trees substitution sets set observations zonetrans 1 ipsweep 2 zonetrans 3 portsweep 4 pingofdeath 5 making use PScore simplify explanation construction First PHATT search spaces single loop common treatment process extending explanation This greatly simpliﬁes control algorithm small cost clarity Second PHATT continuously update substitution set PSD current derivation trees D computing pending set scratch time This enables PHATT rapidly ﬁnd set possible insertion points observed action To aid PHATT precomputes maintains intendable trees ﬁxed set trees added pending set contains single instance element I Deﬁnition 82 Core hypothesis set PS core I As simple example generating explanation following PHATT algorithm plan library shown Fig 1 explanations algorithm sequence observations zonetrans 1 ipsweep 2 zonetrans 3 portsweep 4 pingofdeath 5 shown Fig 4 An expression action t indicates action performed time t use counting numbers time indexes In Fig 4 displaying actual generating trees hypothesis set element hypothesis set represented pair contains leftmost child generating tree root goal tree element substituted Note presence core hypothesis set PScore initial substitution set plan recognition begins The presence PS core subset pending sets affect probability computations size pending set crucial term computing probability explanation However size PS core constant simple bookkeeping allows address All required subtracting size PS core size pending sets computing probability explanation Approximating goal probabilities Rather Formula 73 PHATT drops terms probability goals GR 1 P G8 Keep mind term constant explanations explanation change relative likelihood goal cid9 Bounded recursion In enumerating substitution sets able enumerate complete set leftmost trees It follows PHATT implementation support unbounded recursion plan grammar This recursive deﬁnitions possible formalism recursion ﬁxed depth This represent problem practice bounded agents general arbitrarily deep plans For example demonstration domain recursion required 8 Note original PHA speciﬁcation implementation theory reported 25 simplifying assumption That reasons impractically slow CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1119 84 PHATT addressing issues plan recognition Having completed discussion PHATT algorithm implementation section gives ﬁnal brief discus sion issues plan recognition PHATT algorithm easier discuss algorithm Accuracy As verifying correctness implementation conducted simple studies algorithms accuracy In tests algorithm performed expected PHATT able correctly identify present root goals compute correct conditional probabilities assumptions This true single mul tiple root goal situations wide number different kinds plans Section 10 paper number different kinds plan structures number parameters explored considering algorithms runtime For parameter settings discuss ﬁrst veriﬁed systems ability accurately draw conclusions problem settings Multiple root goals Going simply handling multiple root goals handling multiple instances root goal unusual plan recognition systems In fact applications 1330 allow user goal time let multiple instances goal However real world domains simply unacceptable assumption Consider cyber security domain Fig 1 In real world common determined cyber attacker launch multiple different attacks single host multiple instances attack achieve single goal This number reasons diversity target susceptibility attack success likelihood create confusion Thus domain common multiple instances goal pursued different similar identical instances plans For example explanation presented Fig 4 explain second observation zonetrans time 3 consistent second DoS goal Any complete algorithm plan recognition consider possibility multiple interleaved instances goal pursued single agent time Most previous work discounted possibility single agent pursuing multiple instances root goal time However domains simply valid assumption Partially ordered plans As pointed Section 5 formal underpinnings language PHATTs plan libraries makes partial order explicit language result require perform exponential unfolding grammar cover possible orderings plans PHATT takes pains generation pending sets particular explanation enforce ordering constraints particular plan This allows PHATT seriously idea partial ordering plan execution maintaining expressiveness plan language The eﬃciencies result underestimated Section 9 Negative evidence People regularly failure observe actions consistent hypothesized goal evidence goal pursued However previous work plan recognition captured intuition With model plan recognition couched terms plan execution PHATT able partially address problem In fact naturally falls way probabilities computed pending set term explanation For example suppose plan library intendable roots A B C Further assume B deﬁned plan library production B AD 1 2 This means observed plan A plan B Any sequence actions achieving A proper preﬁx plan B D represents remaining actions B A Now suppose set observations want explain actually case agent A C B C set goals Any explanations account actions root goal A identical assign ments observations explanations root goal B A completed Now consider computing probability explanations After seen action A pending set A C explanation smaller pending set B C explanation A C pending set actions C case B C actions D pending set Making uniformity assumption larger size pending set lowers probability B C explanation time action D observed Thus actions observed seeing action contributes D conditional probability favored explanation A C increase If subsequently action contributes D A C ruled hypothesis explain action Thus B C explanation matter unlikely correct conditional probability increase Although PHATT handle negative evidence outlined PHATT seen action contributes plan order consider The handling negative evidence described place context ﬁrst seen evidence plans question Thus correctly handle case sketched complete plan A proper preﬁx plan treated correctly explicitly hypothesize absence goals evidence Overloading actions Some prior work plan recognition allowed action contribute plan In contrast PHATT assumes action contribute goal It relatively easy extend 1120 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 explanation building process allow single observation explained multiple elements substitution set In case selecting single element algorithm explore subsets trees account observed action This signiﬁcantly increase size search space Another signiﬁcant problem probability model situations Identifying appropriate probability model case open research question Hostile agents As mentioned introduction PHATT assumes observed agents actively hostile inference goals plans We work partially observable domains PHATT 20 applicable recognizing hostile agent hidden actions However great deal work required treatment Clearly hostile agent attempting distract observer set root goals fulﬁll independence assumptions The agent carefully choose sets goals designed obfuscate true goals agent This require addressing uniformity independence assumptions root goals selection actions pending sets This exciting area future work Plan language expressiveness While use temporal constraint based reasoning typed variable systems studied areas science technologies brought bear languages plan library speciﬁcations In applications critical reason temporal relations actions plan Plan library engineering facilitated use plan variables form method goal schemas We included extensions PHATT discussed Section 11 9 Complexity In paper present analytic empirical results complexity PHATT algorithm Previous results complexity planrecognition demonstrate PHATT algorithm NPHard 3553 PHATT hard approaches like Kautz use heuristic methods ﬁnd single approximately optimal9 explanation set observations However ask PHATT compute posterior probability explanations given set observations additional NPcomplete computation compute posterior probabilities This essentially Bayes net probabilistic inference problem shown NPcomplete 14 91 Finding explanation First ﬁnding explanation observation trace framework NPcomplete Explanationﬁnding NP In Section 6 provided nondeterministic algorithm ﬁnding explanation ob servation trace We algorithm NP follows The outer loop algorithm executed observed action Accordingly need actions loop executed polynomial time These operations 1 Choose element substitution set foot matches observation 2 Substitute new tree partial explanation 3 Update substitution set The complexity step 1 proportional number internal nodes tree constant k captures maximum number rules single nonterminal Substitution possible internal nodes tree 2nk choices Step 2 requires traversal original tree copy traversal substituted tree insert copy new copy original tree This time linear size tree At worst tree fewer 3n nodes n nodes NNT n leaf nodes n internal nodes10 Finally updating substitution set step 3 walking frontier tree substituted The size frontier smaller n number leaves tree end algorithm Ergo algorithm nondeterministic polynomial time Explanationﬁnding NPhard We explanationﬁnding NPhard reduction 3dimensional match ing 18 pp 5053 3dimensional matching deﬁned follows given sets W X Y set triples M speciﬁes acceptable threeway marriage triple M cid5w x ycid6 w W x X y Y A lution subset M covers W X Y exactly A 3dimensional matching problem translated plan recognition problem follows The sets W X Y translated string elements W ﬁrst ele ments X elements Y For triple cid5w x ycid6 M grammar rule s w x y w x x y 9 In terms size explanation 10 We forbid epsilon productions productions simply rewrite nonterminal CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1121 s intendable root The translation linear size input problem readily seen ﬁnd explanation problem exists corresponding 3dimensional matching The complexity arguments topdown explanationﬁnding carry bottomup explanation ﬁnding algorithm apply implemented algorithm Examination NPhardness result shows apply bottomup explanationﬁnding explanations correspond 3D match solutions bottomup explana tions Relation parsing Viewing PHATTs explanation generation parsing helps identify problem dif ﬁcult solve deterministically Here Vilains results plan recognition parsing 53 helpful There aspects PHATTs explanation grammars diﬃcult parse The ﬁrst use partial orders rules These allow encode equivalent factorial number contextfree productions single rule Barton shown parsing rules NPcomplete 17 A sketch argument follows context free grammars CFGs rules annotated partial orders subsume unordered CFGs UCFGs Vertex cover reduced UCFG parsing The worst case recognition problem minimallyconstrained leads largest explosion number rules considered Above issue partial ordering rules ability interleave strings generated different root nodes grammar Nederhof Satta Shieber analyze complexity parsing CFGs augmented shuﬄe operator 32 They shuﬄe makes worstcase complexity parsing task exponential O P k g q n3 The n3 factor conventional CFG parsing complexity P size grammar q size state space automaton parsing rule k maximum width shuﬄing performed grammar g term measures memory added parser order handle interleaving reduced q k giving O P k q e 2 kk q n3 Shuﬄing width accumulates additively parse trees subtrees shuﬄed Nederhof et al argue parsing applications k relatively small constant However worst case plan library k O dl d depth grammar l maximum number subtasks righthand size rule As partial ordering problem diﬃcult plan library relatively unconstrained temporally toplevel goals mixed single trace 92 Explanations probability We shown problem ﬁnding explanation set observations NPcomplete worst cases arise relatively unconstrained plan library requires maintain large set hypotheses Recall Deﬁnition 71 PHATTs job ﬁnding conditional probability given goal diﬃcult requires computing set explanations We know general problem computing posterior probabilities Bayes net Pcomplete 14 Unfortu nately general result applies special purpose computations PHATT Since know ﬁnding single explanation topdown bottomup NPcomplete PHATTs probability computation problem capable counting solutions 3Dmatching problem explosion size encoding shown probability computation problem Pcomplete This easy We engineer probabilities rules 3Dmatching problem ensure solutions equally probable In case reciprocal probability explanations tell number solutions 3Dmatching problem problem Pcomplete Some plan recognition researchers attempted ease computational burden trying ﬁnd single best possibly approximately best explanation MAP solution 112648 Unfortunately ﬁnding MAP assignment variables belief network NPhard 1450 93 Explanation combinatorics If PHATT ﬁnd explanations critical complexity consideration rate set explanations grows function input set observations plan library In particular identify features plan library cause number possible explanations increase input length bounds effects Given algorithm generating explanations single observation increase decrease leave unchanged number explanations11 That observation inconsistent current explanations case explanation explanations pruned number explanations decreases observation consistent explanations consideration case number explanations remains increases The critical question elements pending set observed action foot It helpful deﬁne terms discussion 11 We talk number explanations exact cumbersome number explanatory hypotheses consideration 1122 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Deﬁnition 91 We deﬁne attachment points observation σ explanation cid5Dn PSD0 PSDncid6 set nonterminal instances A exists tree T PSDn rootT A footT σ Attachment points nonterminal symbols explanation new tree added expla nation account observation We process adding single observation attachment point explaining observation To explore new explanations result single observation consider case single observation single attachment point generalize multiple attachment points We use T σ B refer generating tree foot terminal symbol σ root nonterminal B It possible tree superscript tree As said σ observed algorithm create explanation T σ B substitution set Thus N elements substitution set T 1 σ B explain action σ contributing B N explanations result extending single initial explanation Keep mind attachment point observation explanation discarded inconsistent number explanations decrease σ B T N The question ask large N In general number trees depends grammar One cause multiple elements substitution set share common root foot pair cases recursion happens beginning plan In cases imagine situations require build trees share root leaf symbols differ number times recursive production invoked tree As result build generating trees plan grammar bounded depth recursion allow grammar This effect limiting number generating trees plan library In case bound placed recursion m single explanation extended exactly m ways generating trees addressing recursion Another reason multiple trees share common root foot pair presence ORnodes plan library Since ORnode captures fact multiple ways expand given nonterminal worst case construct plan libraries alternative expansions share ﬁrst action For given plan library ORnode largest number alternatives We denote number alternatives MaxOrBF Since worst case ORnodes plans expansion MaxOrBFalternatives number trees resulting size N speciﬁc root leaf pair bounded MaxOrBFMaxD MaxDis maximum length path root leaf original plan library Note deﬁnition MaxD m recursive plans contained plan library Since MaxOrBFMaxD bounds N pairing observed leaf particular nonterminal plan bound overall size substitution set light following deﬁnition Deﬁnition 92 We deﬁne APσ exp number attachment points current explanation exp current observation Note APσ exp includes nonterminals existing plans nonterminal roots intro duce new plans In case APσ exp MaxOrBFMaxD bounds size substitution set growth number explanations observation This conﬁrms intuitions number explanations grow rapidly worst case These worstcase results suggest turn away universal claims PHATTs performance focus actual performance encountered particular cases In following section change focus turn empirical evaluation PHATT number test cases manipulating key problem parameters 10 Empirical complexity scalability results We conducted series experiments based Common LISP implementation PHATT algorithm signed allow understand critical factors determining runtime PHATT algorithm Our initial hypothesis number roots planlibrary large effect runtime al gorithm fact believed features plan library impact Our results verify hypothesis While number root plans measurable effect systems runtime experiments showed partial ordering plan especially beginning plan far dramatic effect runtime 101 Experimental design Our experiments measuring runtime Allegro Common LISP 70 implementation PHATT algorithm conducted Sun Sunﬁre880 8 GB main memory 750 MHz CPUs afforded large number replications 1000 Note measured CPU time msec exclusive time operating processes CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1123 Factor Description Levels order depth methodBF choiceBF roots Types action ordering constraints Plan depth Method branching factor Choice point branching factor Number root goals total partial unord 3 4 5 6 3 4 3 4 10 100 200 400 600 800 1000 Fig 5 Experimental factors Fig 6 Graphical representations different ordering cases experiments total unord Note absence partial case Each order partial plan randomly built child action having constraint pointing child node Cycles order partial plans prevented construction We identiﬁed ﬁve features plan libraries believed signiﬁcant effect runtime PHATT algorithm The kind interaction ordering constraints plans depth plans plan library number intendable roots plan library branching factor methods branching factor choice points The following detailed deﬁnitions experimental factors order This indication type ordering constraints exist actions methods ANDnodes plan library A graphical representation shown Fig 6 Total actions totally ordered Each action single ordering constraint action precedes One plan designated ﬁrst action All actions plan ordered unordered respect Last plan designated action All actions plan ordered unordered respect Partial Each action single ordering constraint This constraint orders action randomly chosen action deﬁnition Cyclic orderings prevented generation This means methods vary totally ordered completely unordered This speciﬁcally included approximate real world plan libraries In cases actions totally ordered completely unordered Such plan ordering constraints totally ordered case Unord All actions unordered respect depth This measure depth plan trees plan library In plan trees choice points ORnodes methods ANDnodes alternate levels In cases root deﬁned ORnode roots This measures number plan root nodes plan library 10 100 200 400 600 800 1000 roots respectively methodBF This determines number actions branching factor method deﬁnition ANDnode choiceBF This determines number actions branching factor choice point ORnode These factors values summarized Fig 5 The discussion experiment document features tested factor held constant All actions plan libraries unique Thus action observed actually ambiguity root intention action contribute The recognize leverage fact inherently reduce runtime algorithm However general plans libraries lower runtimes plan libraries greater ambiguity plans ambiguous plans explanations Keep mind reduction ambiguity rule possibility instance given plan Therefore chose simpliﬁcation allow inferences space possible explanations effects factors algorithms runtime We return discuss later For experiment separate plan library generated experimental conditions To test plan library generated test data set containing thousand test casesTo generate test case possibly duplicate roots selected random plan library For roots legal plan linearization plan generated following plan library The complete plan instances randomly interleaved maintaining ordering constraints individual plans resulting single test case To run single test PHATT started loaded plan library Then test case internal clock started PHATT presented observed action sequence processing sequence PHATT computed probability distribution root goals At point clock halted CPU time measured recorded 1124 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Fig 7 Average runtime vs plan depth Fig 8 Average observation runtime plan depth ﬁxed Note runtimes decrease ordering plans increases earlier plan Also note signiﬁcant increase completely unordered case test case There test cases runtime algorithm registered zero In cases millisecond listed runtime test case 102 First experiment We ﬁrst explored algorithms average runtime scales depth plans plan library We collected runtimes following conditions The order factor ﬁxed Total methodBF factor ﬁxed choiceBF ﬁxed depth factor varying number roots thousand Fig 7 shows resulting average observation runtime msec vs plan tree depth Given log scale runtimes clear exponential trend plan libraries thousand root nodes Since know runtime building explanations depends size plans size plan depends exponentially depth tree branching factor result surprising We held methodBF choiceBF factors constant respectively remaining exper iments We felt acceptable test plans complete trees limit effect branching factors dominated depth tree While exponential relationship kept mind working algorithm practical experience suggests signiﬁcant problem Our application experience suggests depth hierarchical plans real world applications limited relatively small value 103 Second experiment Next collected runtimes factorial experiment order depth roots factors values Fig 8 plots average observation runtime measured msec log scale number root goals order values depth ﬁxed Note data depth ﬁve collected showed trends discuss depth case We note case complete plans interleaved CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1125 presented None experiments reported run partial plans However reporting average observation runtimes ﬁgures representative average case partial plans The ﬁrst thing notices data algorithm scaling linearly number plan roots plan library This validated orders magnitude encouraging use large domains Note results Partial runtimes dip 200 800 overall trend linear We believe dip runtimes caused natural variability complexity partially ordered plans examining effect The graph shows Order factor profound effect means Unordered plans exhibit highest means partially ordered plans relatively high means However difference order total obvious Given similarity total interested determining signiﬁcant statistical difference total levels order matter partial unord tell great deal effect ordering constraints runtime algorithm The Tukey HSD method analysis variance data second experiment test contrasts veriﬁed lines Fig 8 represent statistically signiﬁcant differences algorithms runtime Our analysis PHATT algorithm light results shows difference total order levels caused maintaining larger substitution sets In order cases initial action plan seen actions enabled added substitution set In total cases single action enabled substitution set This means multiple explanations possibility case size average substitution set larger order cases order total cases Computing maintaining larger substitution sets causes increase runtime In section similar line reasoning allow explain signiﬁcantly higher runtimes unord order levels 104 The cost multiple explanations Given previous discussion surprising examination PHATT output data points order unord cases shows large number explanations Since actions ordered PHATTs algorithm unable conclude particular subset actions plan instance Remember PHATT create maintain multiple instances root goal single explanation Thus case unordered plans maintain explanations consistent possible subsets actions contributing different plan instances This includes possibility action contributes separate plan instance This contrasts sharply total levels In cases ordering actions license single explanation In fact cases unique ﬁrst action single explanation observations The difference cases accounted larger substitution sets maintained levels ﬁrst action observed Our hypothesis multiple possible ﬁrst actions plan increases number maintained explanations cause signiﬁcant increases runtime conﬁrmed runtimes produced order test cases In case large set explanations collapse single explanation ﬁnal action observed Keep mind actions plans unordered respect actions required executed ﬁnal action Thus sees ﬁnal action consistent explanation observations contribute single root goal As result exhibit higher runtimes order total test cases The test cases number ordering constraints test cases removing variable possible cause Further constraints positioned test cases average size pending sets removing possible cause Since runtimes signiﬁcantly greater conclude increased runtimes cases result larger number explanations produced plans causal structure 105 Early closing plans Close inspection raw runtimes order cases previous experiment reveal interesting rela tionship There signiﬁcant gap cluster test cases worst runtimes rest test cases See Fig 9 Inspection worst test cases showed shared common property In case ﬁnal actions test ﬁnal actions component plans test case For example consider following abstract ordered observation stream plans b c steps a1 b1 c1 a2 b2 c2 b3 c3 a3 c4 b4 a4 note actions series c4 b4 a4 ﬁnal actions respective plans We plans property late closing test cases The test case worst runtime closed plan step earlier In example equivalent swapping actions c4 a3 We cases early closing test 1126 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Fig 9 Raw total runtimes order depth 4 test cases Note line connects average runtime test cases A signiﬁcant gap runtimes appears open space column results average runtimes number plans test set increases 400 Fig 10 Average total runtimes early plan closing test cases cases This small change observation stream making second difference algorithms runtime suggested ﬁnal experiment 106 Third experiment To determine early closure plans caused reduction number explanations reduced runtimes collected runtimes test cases follow ﬁve abstract test cases shown 1 a1 b1 a2 b2 b3 a3 c1 c2 c3 b4 a4 c4 2 a1 b1 a2 b2 b3 a3 c1 c2 b4 c3 a4 c4 3 a1 b1 a2 b2 b3 a3 c1 b4 c2 c3 a4 c4 4 a1 b1 a2 b2 b3 a3 b4 c1 c2 c3 a4 c4 5 a1 b1 a2 b2 b3 b4 a3 c1 c2 c3 a4 c4 In test case notice b4 action moves time step earlier observation sequence Speciﬁcally took individual late closing case order depth 4 1000 plan library generated ﬁve observation streams moving action corresponding b4 earlier observation stream Each test cases presented PHATT 8 times runtimes averaged Fig 10 graphs average total runtimes test cases As ﬁnal action b plan moves closer closer beginning observation stream runtime test case drops Given previous discussion impact unordered actions plan cause relatively clear Since instance order level ﬁrst actions plan unordered respect As result PHATT assume actions particular goalplan contribute single instance plan However actions ordered ﬁnal action plan PHATT CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1127 sees ﬁnal action plan throw explanation observed actions plan contributing single instance Thus PHATT presented b4 action eliminate explanation involves single instance plan b This results radical reduction number possible explanations observation subsequent reduction overhead associated keeping explanations active In summary major conclusions draw set experiments The average runtime algorithm scaling linearly number roots plan library The feature plan library signiﬁcant effect algorithms runtime ordering constraints plan library followed number roots plan library followed actual depth plan trees Plan libraries ordering constraints represent upper bound worst case ordering factor Plans early ordering constraints result signiﬁcantly reduced runtimes plans number ordered initial actions Ordering constraints end plans signiﬁcantly reduce algorithms runtime Maintenance large number possible explanations signiﬁcant cost algorithm We continuing empirical analysis PHATT algorithm We conducting experiments study effect plan branching factors incomplete trees algorithms runtime While study needed current results promising application PHATT algorithm 11 Extensions real world use Thus far plans discussed propositional However propositional representations limiting real world applications Therefore extended PHATTs actions typed arguments temporal constraints This sec tion brieﬂy outline extensions For discussion use action speciﬁcation language implementation This language equivalent deﬁnitions provided Section 5 For example Section 5 deﬁned ﬁrst level plan Theft Theft scan getctrl getdata 1 22 3 In language use written obvious mapping defmethod Theft scan getctrl getdata defprecond 2 1 defprecond 3 2 endm This representation allow easily annotate actions plans additional arguments constraints 111 Typed action variables PHATT supports limited form typed action arguments This requires extending representation language plan library variables deﬁning method handling binding propagation values plan recognition algorithm Consider adding typed arguments example defmethod Theft file type datafile location type scan location getctrl location getdata file location defprecond 2 1 defprecond 3 2 endm This deﬁnition extends previous example specifying typed variable speciﬁc ﬁle stolen stolen We note use common deﬁnition indicates coreference argument Thus location subactions deﬁnition Rather supporting fullblown ﬁrst order representation variables function argument place holders sup porting value propagation coreference The values arguments bound process explaining observation assumed existentially quantiﬁed In probabilistic model like PHATT supporting universal quantiﬁcation arguments require signiﬁcant extra probabilistic machinery Such build probability distribution set possible assignments variables greatly increasing algorithms runtime Instead variables bound ﬁrst action refers added explanation 1128 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 These bindings propagate upwards actions higher levels plan library enabling binding variables lower levels determine bindings actions observed For example scan argument location When PHATT builds explanation structure speciﬁc observation scan action binds location variable propagates bindings explanation Theft action Given location bound observation getctrl action involved different binding location incompatible previously observed scan action pruned To aid process PHATT requires arguments action propagated explanation occur subactions parent actions deﬁnition Adding types PHATT variables enables pruning explanations Consider example restricted location argument type ﬁle type dataﬁle These type restrictions prevent PHATT plan recognize cases theft involve stealing applications dont happen network PHATT supports simple negation types This allows domain designer specify type object bound speciﬁc variable This keyword type deﬁnition This type negation limited value removes single possible type consideration However coupled binding variables valuable Variables PHATT bound speciﬁc observations observation generated speciﬁc sensor imposes limits possible values argument This means arguments observation fall limited range values This limits scope action argument subset types making type negation stronger tool Consider following case defmethod Theft file type application location type scan location getctrl location getdata file location defprecond 2 1 defprecond 3 2 endm In case observation getdata action assumed report identiﬁer ﬁle ﬁrst argument Since action arguments explanation bound explaining observation data action argument constrained ﬁle By negating type application limited set electronic ﬁles restricted set nonapplication ﬁles We implemented ideas augmenting PHATT ﬂexible argument type framework The plan library designer speciﬁes types computing type equivalence subsumption We tested framework implementing different type systems PHATT 1 A simple set ﬁxed types use equality testing subsumption 2 handbuilt type hierarchy subsumption test 3 FaCT29 All type systems successfully integrated PHATT However type systems fully implemented extensively tested excellent results formally evaluated additional runtime memory requirements imposed use variable type systems Typed arguments allow PHATT prune explanations result signiﬁcant reductions PHATTs search space Further careful bookkeeping cost binding propagating variables minimal cases constant cost This means typed arguments enable signiﬁcant reductions PHATTs overall runtime However depends critically strength complexity type vary considerably pending speciﬁc features plan library Thus magnitude savings results vary greatly depending representational choices construction plan library cost subsumption tests type To summarize All variables PHATT plan libraries assumed existentially quantiﬁed bound observation Propagation variable binding starts observations propagates upwards The support explicit noncodesignation constraints That PHATT uses variable deﬁnition assumed refer entity method explicitly stating variables different names refer entity While support type negation type variables type variable codesignation currently sup ported Especially domains hierarchical argument types cases helpful able specify actions arguments type concerned actual type instance CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1129 112 Temporal constraints Temporal constraints effective pruning space possible explanations Many tasks simple temporal relations subtasks considering explanations violate bounds unreasonable To allow PHATT prune possible explanations based kinds temporal constraints Interval Constraint Engine ICE 35 integrated PHATT ICE solver Simple Temporal Problems STP 15 A STP temporal constraint problem constraints form T cid2 T j C T T j timepoints At heart ICE BellmanFord shortest path algorithm 237 incremental maintenance spanning trees giving tightest bounds earliest startlatest end times The complexity algorithm O mn m number vertices n number edges temporal graph However incremental mode allowing online addition constraints critical need evolving explanations built PHATT In practice ICE handle incremental updates near constant time making excellent choice integration PHATT In order use ICE PHATT allowed temporal constraints restricted result STP To end PHATT supports kinds temporal constraints overall duration intersibling constraints A duration constraint captures overall duration single action For example people taking hour lunch unusual We code rules recognizing normal lunch events agent violates requirement PHATT recognize normal lunch event It lunch plan abandoned special occasion case PHATT consider normal lunch event This kind limitation captured simple restriction duration lunch action requires longer hour lunch Note duration constraints placed actions subactions actually force sub actions overlap Consider case action subactions parent action constrained 3 time units actions takes longer time unit subactions overlapped time explanation inconsistent Intersibling temporal constraints constrain temporal distance beginning end time points sibling actions Consider case starting car turning ignition key depressing gas If gas depressed short proximity beginning key turning action car start The battery wear gas start car In case temporal constraint needed start times sibling actions Again actions seen correct temporal relation PHATT consider explanation actions ascribes car starting goal By specifying temporal constraints basis beginning end time actions case ordered actions implicitly deﬁning maximum time duration actions Consider case sibling actions α β sequentially ordered Now add constraint βend 5 time units αbegin α β individually 5 time units sum Both duration intersibling constraints representable timePoint1 cid2 timePoint2 C time points case duration constraint begin time end time single action case intersibling constraint begin end times actions siblings action deﬁnition It restriction allows enforce temporal constraints form STP Thus temporal constraints restricted form STP know use ICEs near constant time processing eﬃciently prune explanations violate temporal restrictions deﬁned plan Our experience ICE integrated PHATT suggests measurable effect algorithms runtime dominate effects domain features discussed Section 10 12 Future work conclusions We presented plan recognition algorithm PHATT exploits Bayesian model The Bayesian model clariﬁes number knotty issues plan recognition including reasoning best explanation negative evidence agents multiple concurrent goals We presented implementation PHATT algorithm uses precomputed ex planation trees greater eﬃciency given analytical experimental complexity results We discussed enhancements algorithm incorporate temporal type constraint management techniques Our work opens interesting directions future investigation We think following seven areas particu larly interesting Goal abandonment Most real agents inﬁnitely persistent In face failures simply inattention abandon goals Most previous plan recognition systems taken account We developed technique based Bayesian reasoning model mismatch identifying goal abandonment 23 This approach makes use kind decrease probability discussed negative evidence identify goal abandoned We continuing explore impact approach runtime 1130 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 Partial observability In applications impossible actions agent carries There actions simply seen For example factories partially automated In factories control actions performed console actions readily observed However tasks performed ﬁeld operators turning valves wrenches actions directly perceived plan recognition In cases noisy sensors observation sequence stochastic function actual action sequence For example track agents motion GPS wiﬁ triangulation lose track agent moves indoors leaves area wireless access points For reason begun look problems partial observability 19 As matter theory trivial extend model handle partial observability So far considered set observations complete accurate trace action sequence P exp obs P expP obsexp We augment observation model P exp obs P expP actsexpP obsacts assuming observation probabilities depend action observed While theoretical extension straightforward implementing theory raised number diﬃcult issues Even simple deterministic models observations example ﬁeld operator actions cause set hypotheses explode way similar plan abandonment For stochastic models need develop procedures inverting observation models account context currently active explanatory hypotheses Work layered HMM approaches helpful However localization example illustrates approach inverting observation model sensitive models speciﬁc structure case wiﬁ localization example particular characteristics geometry radio performance Inﬂuence state Our model discussed effect state plan recognition However obviously state world profound effect goals adopted plans achieve goals order actions selected pending sets We interested number simple ways state variables inﬂuence probabilistic models level features We interested ways observed state changes infer performance unobserved actions Failures State modeling necessary order plan recognition contexts actions methods fail achieve desired ends In addition incorporating state effects need sophisticated model agents internal state So far able treat agents chosen complete decomposition plan time zero Now need complex model incorporate choices occur later order sensitive state time method choice We include models agents react perceive methods failed We currently working semantic web domain agents need gather information resource availability suitability commit resources Execution traces domain necessarily include cases agent attempts verify resource r suitable use goal ﬁnd change new method based resource r cid17 Hostile agents intended recognition It odd grouped reasoning hostile agents intended recognition agents areas exactly opposite objectives However applications important thing common raise gametheoretic concerns That cases agent executing plan reasoning reasoning plan recognizer Hostile agents try turn reasoning force wrong conclusions intended recognition agent trying force recognition right conclusions conventional methods explicitly eliminating alternatives consideration ambiguity We great deal work area hostile agents notably security19 gametheoretic considerations Instead assumed hostile agents speciﬁc reasoning opponents try standard gambits elude detection For domain opponents scripts actual intelligent agents simplifying assumption works fairly gameplaying approach obviously insuﬃcient Learning As AI approaches PHATT suffers knowledge engineering bottleneck It diﬃcult build maintain plan libraries recognition reliably assess probability parameters Fortunately probabilistic systems PHATT terribly sensitive priors extreme cases terrorism versus air travel example gross effects However library construction problem grave There great deal work training HMMs applications speech recognition natural language processing work begun adopted plan recognition The PHATT algorithm shows connection good oldfashioned plan recognition expressive models HMMs We hope lead techniques learning expressive models type PHATT CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 1131 Reasoning types quantiﬁcation equality In Section 11 discussed use types handling parametrized actions However PHATT limited extremely simple uses parametrization notably treated simply ﬁltering variable bindings simply propagated uniﬁcation In cases need complicated reasoning For example plan air travel involves taking train airport constraint destinationof train trip equal trainstationof startingairportof air travel Note introduction trainstationof function means constraint directly handled simple symbol uniﬁcation Charniak Goldman extensive work kind equality reasoning context plan recognition story understanding 9 Not equality reasoning arise problems ﬁnd prior probability plans conditioned values assigned parameters Consider example plan library action going Going Antarctica going lower probability event going nearest supermarket Problems quantiﬁcation arise domains like semantic web softbots variables drawn nonﬁnite practically inﬁnite domains quantiﬁcation example ﬁles created destroyed Our current work planning semantic web leading examine issues Acknowledgements This article supported DARPAIPTO Air Force Research Laboratory Wright Labs contract number FA865006C7606 based earlier work supported DARPAITO Air Force Research Laboratory Contract No F3060299C0077 The work supported European Commission EU Cognitive Systems project PACOPLUS FP62004IST4027657 References 1 D AvrahamiZilberbrand GA Kaminka Fast complete symbolic plan recognition Proceedings International Joint Conference Artiﬁcial Intelligence 2005 2 R Bellman On routing problem Quarterly Applied Mathematics 16 1 1958 8790 3 M Boddy Temporal reasoning planning scheduling Lessons learned A Tate Ed Advanced Planning Technology AAAI Press 1996 4 M Boddy J Carcioﬁni G Hadden Scheduling partial orders causal model Proceedings Space Applications Research Workshop Johnson Space Flight Center 1992 5 M Boddy R Goldman Empirical results scheduling dynamic backtracking Proceedings International Symposium Artiﬁcial Intelli gence Robotics Automation Space 1994 6 HH Bui S Venkatesh G West Policy recognition abstract hidden Markov model Journal Artiﬁcial Intelligence Research 17 2002 451499 7 H Chan A Darwiche When numbers matter Journal Artiﬁcial Intelligence Research 17 2002 265287 8 E Charniak D McDermott Introduction Artiﬁcial Intelligence AddisonWesley Reading MA 1987 9 E Charniak RP Goldman Plan recognition stories life M Henrion R Schachter J Lemmer Eds Uncertainty Artiﬁcial Intelligence 5 Elsevier Science Publishing Co Inc New York NY 1990 pp 343351 10 E Charniak RP Goldman A Bayesian model plan recognition Artiﬁcial Intelligence 64 1 1993 5379 11 E Charniak SE Shimony Costbased abduction MAP explanation Artiﬁcial Intelligence 66 2 1994 345374 12 PR Cohen CR Perrault JF Allen Beyond question answering W Lehnert M Ringle Eds Strategies Natural Language Processing Lawrence Erlbaum Associates 1981 pp 245274 13 C Conati AS Gertner K VanLehn MJ Druzdzel Online student modeling coached problem solving Bayesian networks Proceedings Sixth International Conference User Modeling 1997 14 GF Cooper The computational complexity probabilistic inference Bayesian belief networks Artiﬁcial Intelligence 42 1990 393405 15 R Dechter I Meiri J Pearl Temporal constraint networks Artiﬁcial Intelligence 49 1 1991 6195 16 K Erol J Hendler DS Nau UMCP A sound complete procedure hierarchical task network planning Proceedings Second International Conference Artiﬁcial Intelligence Planning Systems AIPS 94 1994 pp 249254 17 JG Edward Barton On complexity IDLP parsing Computational Linguistics 11 4 1985 205218 18 MR Garey DS Johnson Computers Intractability WH Freeman Company New York 1979 19 C Geib Plan recognition A Kott W McEneaney Eds Adversarial Reasoning Chapman HallCRC 2006 20 CW Geib RP Goldman Partial observability collaborative task tracking Proceedings AAAI 2001 Fall Symposium Collaborative Task Tracking 2001 21 CW Geib RP Goldman Plan recognition intrusion detection systems Proceedings DISCEX II 2001 22 CW Geib RP Goldman Probabilistic plan recognition hostile agents Proceedings FLAIRS 2001 Conference 2001 23 CW Geib RP Goldman Recognizing plangoal abandonment Proceedings IJCAI 2003 2003 24 M Ghallab D Nau P Traverso Automated Planning Theory Practice Morgan Kaufmann Publishers Inc 2004 25 RP Goldman CW Geib CA Miller A new model plan recognition Proceedings 1999 Conference Uncertainty Artiﬁcial Intelligence 1999 26 JR Hobbs ME Stickel DE Appelt PA Martin Interpretation abduction Artiﬁcial Intelligence 63 12 1993 69142 27 A Hoogs AA Perera Video activity recognition real world Proceedings Conference American Association Artiﬁcial Intelli gence 2008 2008 pp 15511554 28 JE Hopcroft JD Ullman Introduction Automata Theory Languages Computation Addison Wesley 1979 29 I Horrocks Using expressive description logic FaCT ﬁction Proceedings 7th International Conference Principles Knowledge Representation Reasoning KR 98 1998 30 E Horvitz J Breese D Heckerman D Hovel K Rommelse The Lumiere project Bayesian user modeling inferring goals needs software users Proceedings 14th Conference Uncertainty Artiﬁcial Intelligence 1998 31 MJ Huber EH Durfee MP Wellman The automated mapping plans plan recognition Proceedings Twelfth National Conference Artiﬁcial Intelligence 1994 pp 344351 32 M Jan Nederhof G Satta SM Shieber Partially ordered multiset contextfree grammars IDLP parsing Proceedings Eighth International Workshop Parsing Technologies Nancy France April 2003 pp 171182 1132 CW Geib RP Goldman Artiﬁcial Intelligence 173 2009 11011132 33 A Joshi Y Schabes Treeadjoining grammars Handbook Formal Languages vol 3 Springer Verlag 1997 pp 69124 34 G Kaminka D Pynadath M Tambe Monitoring teams overhearing A multiagent planrecognition approach Journal Artiﬁcial Intelligence Research 17 1 2002 83135 35 H Kautz A formal theory plan recognition implementation PhD thesis University Rochester 1991 36 H Kautz JF Allen Generalized plan recognition Proceedings Conference American Association Artiﬁcial Intelligence AAAI86 1986 pp 3238 37 JLR Ford DR Fulkerson Flows Networks Princeton University Press Princeton 1962 38 L Liao D Fox H Kautz Learning inferring transportation routines Proceedings AAAI 2004 2004 39 L Liao D Fox H Kautz Extracting places activities GPS traces hierarchical conditional random ﬁelds International Journal Robotics Research 26 2007 119134 40 L Liao D Fox HA Kautz Locationbased activity recognition relational Markov networks Proceedings 19th International Joint Confer ence Artiﬁcial Intelligence 2005 pp 773778 41 J McCarthy Circumscription A form nonmonotonic reasoning Artiﬁcial Intelligence 13 1980 2739 171172 42 J Modayil T Bai H Kautz Improving recognition interleaved activities Proceedings Tenth International Conference Ubiquitous Computing 2008 43 M Peot R Shachter Learning dont observe Uncertainty Artiﬁcial Intelligence Proceedings Fourteenth Conference San Francisco CA Morgan Kaufmann Publishers 1998 pp 439446 44 D Poole Logic programming abduction probability A topdown anytime algorithm estimating prior posterior probabilities New Genera tion Computing 11 34 1993 377400 45 D Poole Probabilistic Horn abduction Bayesian networks Artiﬁcial Intelligence 64 1993 81129 46 M Pradhan M Henrion G Provan BD Favero K Huang The sensitivity belief networks imprecise probabilities An experimental investigation Artiﬁcial Intelligence 85 12 1996 363397 47 D Pynadath M Wellman Probabilistic statedependent grammars plan recognition Uncertainty Artiﬁcial Intelligence Proceedings Sixteenth Conference 2000 pp 507514 48 E Santos Jr A linear constraint satisfaction approach costbased abduction Artiﬁcial Intelligence 65 1 1994 128 49 C Schmidt N Sridharan J Goodson The plan recognition problem An intersection psychology artiﬁcial intelligence Artiﬁcial Intelligence 11 1978 4583 50 SE Shimony Finding MAPs belief networks Is NPhard Artiﬁcial Intelligence 68 2 1994 399410 51 CL Sidner Plan parsing intended response recognition discourse Computational Intelligence 1 1 1985 110 52 DL Vail MM Veloso Feature selection activity recognition multirobot domains Proceedings Conference American Association Artiﬁcial Intelligence 2008 2008 pp 14151420 53 M Vilain Deduction parsing Proceedings Conference American Association Artiﬁcial Intelligence 1991 1991 pp 464470 54 MP Wellman JS Breese RP Goldman From knowledge bases decision models Knowledge Engineering Review 7 1 1992 3553 55 R Wilensky Planning Understanding AddisonWesley 1983