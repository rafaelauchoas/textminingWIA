Artiﬁcial Intelligence 175 2011 378415 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint The welldesigned logical robot Learning experience observations Situation Calculus Fiora Pirri DIS Sapienza University Rome Italy r t c l e n f o b s t r c t Article history Received 21 January 2007 Received revised form 11 March 2010 Accepted 14 March 2010 Available online 3 April 2010 Keywords Visual perception Action space Action recognition Parametric probability model Learning knowledge Inference visual perception knowledge representation Theory action Learning theory action visual perception The welldesigned logical robot paradigmatically represents words McCarthy abilities robotchild reveal structure reality language thought In paper partially support McCarthys hypothesis showing early perception trigger inference process leading language thought We deﬁning systematic transformation structures different formal languages sharing signature kernel actions states Starting early vision visual features encoded descriptors mapping space features space actions The densities estimated space form observation layer hidden states model labelling identiﬁed actions observations states action preconditions effects The learned parameters specify probability space ﬁrstorder probability model Finally transform probability model model Situation Calculus learning phase reiﬁed axioms preconditions effects actions course axioms expressed language thought This shows albeit partially underlying structure perception brought logical language 2010 Elsevier BV All rights reserved To John surprising innate abilities 1 Introduction 11 Foreword In paper I discuss McCarthys exploration relation appearance reality welldesigned child 7779 McCarthys arguments opinion twofold The ﬁrst addresses need axiomatise appearance reality relation language thought second addressing archetype inner abilities contests ability current learning methods explain appearancereality relation My thesis early learning perception necessary tune knowledge formation I argue perception reality local particular shaped learning parameters appearance events parameters provide structure symbolic representation supporting inference appearance reality Other cited papers works McCarthys web site arguments learning taken personal conversations 18 years friendship Email address ﬁorapirridisuniroma1it 00043702 matter 2010 Elsevier BV All rights reserved doi101016jartint201004016 F Pirri Artiﬁcial Intelligence 175 2011 378415 379 12 The welldesigned child In notes The welldesigned child 7779 John McCarthy argues Lockean view knowledge built sensations adequate calls designer stance theme taken Dennett 31 paradigmatically understand predict structure behaviour biological artiﬁcial systems In far idea innate knowledge world useful AI work putting robots cognitive science philosophy look evidence evolved humans This designer stance Evidence evolution eﬃciency 7779 supersedes point view shared behaviourism psychology pos itivist philosophy knowledge proceeds sensations Evolution gradually shaped optimal predispositions humans determining correspondence innate mental structure facts world The criticism Lockean child addressed mindbody problem instead stepping aside McCarthy offers bold view robotchild experiment AI possibly psychological replicating intelligent behaviour child So innate learned McCarthy recognises learning central experiment design grounded meanings independent sensations facts child learns world based innate mental structure 7779 Speciﬁcally innate mental structure based interdependent aspects 1 world characteristics appearance reality continuity motion gravity relations 2 mental characteristics central decision making incompleteness appearance senses 3 innate abilities introspection noise rejection principle mediocrity These innate mental structures evolved survival control purposes necessitated adaptation A baby nately equipped deal outperform Lockean baby They investigated neurophysiological studies example known vestibular specialised sensors canals otoliths detect gravity direction disambiguate spatial orientation localisation Likewise specialised motion sensitive area temporal cortex control locomotion perception spatial relations Most important connection McCarthy calls principle mediocrity imitation innate ability oneself place explained mirror neurons discovered Rizzolatti colleagues 1036339 Psychological neurological neurophysiological ﬁndings function structure brain justify mentioned innate abilities Innate structures step view Russell 105 sensations determine knowledge The question designer stance possible language thought McCarthy words represent innate childs brain structure means logical sentences 13 Appearance Reality language thought McCarthy discusses implementation innate structures relation appearance reality persistent objects spatial temporal continuity perception language thought In notes Appearance Reality challenge machine learning 78 complements Appearance Reality included Welldesigned child McCarthy says classifying experience inadequate model human learning inadequate robotic applications He argues mere classiﬁcation discover sense uncover reveal hidden reality appearance To unlock reality appearance McCarthy suggests brute force logical attitude making relations appearancereality explicit proposes axiomatise relation formal language Situation Calculus follows cid2 holds cid3 appearsappearance object s 1 So single object accurate description term appearance Instances term appearance need recursive deﬁnition intended interpretation These descriptions term appear ance qualia1 sense attributes pure perceptual experience 7044 designate quantiﬁable attributes physical objects projected retina necessarily evoking sensory experience A wellknown argument relation innate knowledge abilities learned knowledge Marys Room 565732 interesting compare McCarthys arguments We recall Jacksons knowledge argument following Mary perfect scientist colours knows causeeffect laws regulate human perception colours utterance colour nouns object whatsoever 5657 When eventually capable colours learn Would experience inducing knowledge different current knowledge hypothesis complete Daniel Dennett 32 rejects argument Mary derives incremental knowledge allowed colours based fact knowledge complete Furthermore having reformulated argument Mary perfect logician 3355 argues Mary ﬁnally colours offered blue banana able discover deceived knows 1 Note 76 McCarthy proposes basic consciousness propositions images scenes objects 380 F Pirri Artiﬁcial Intelligence 175 2011 378415 bananas look like 32 Dennetts statement 3355 clarify abstract precondition knowledge argument describing best circumstances Marys RoboMarys knowledge complete infer applying useful lemmas actually knowledge equivalent deductive closure What matters Mary RoboMary deduce like red complete physical knowledge use ones physical knowledge way acquire knowledge like colour 33 Obviously RoboMary complete knowledge 33 perceptual experience form knowledge acquisition whatsoever unnecessary irrelevant In fact deﬁnition added complete theory making inconsistent However far visual perception concerned pointed complete axiomatisation partial structure concerning colour perception include arithmetic true appearances derived Clearly McCarthy proposing complete axiomatisation appearancereality relation Nevertheless concrete question brute force logical attitude term designating appearance physical object relation appear logical truth accounting innate brain structure colourselective perceptive ﬁelds chromatic adaptation Purkinje effect correlated phenomena light change background changes 14 From language thought learning An immediate concrete problem robotchild uses sensors A robotchild endowed sound visual sensors proprioperception sensors moving The term designating appearance posed 1 match continuous stimuli obtained sensor instruments It possible appearance term accurate accurate variability object phenomenology variability instruments taken care amanuensis instructor know subject matter little program hardware works 77 So typical situation mismatch term designating appearance measurements robotchild way language thought sample data instrument obtain adequate model current stimulus Even innate abilities hardwired sensors pathway sensors reasoning explained Perceptual stimuli driven appearance informing robotchilds reasoning undergo complex hardly predictable processing described logical sentences In view variability appearance induces stimulus gradient learning experience solicits innate abilities determine possibly new correlations events objects An innate ability example ﬁnding hidden parameters partially perceivable phenomenon eliciting prediction future appearance A single statement appearances objects physical objects represented logical objects values variables terms 7779 reduce power scientiﬁc ﬁndings Kantian synthetic truth 60 Whereas Sloman note 111112 tradeoff deterministic probabilistic causation Humean versus Kantian view Furthermore appearance categorisation language thought requires language phenomena denoted ontology An enumeration objects phenomena parsimony principle entia non sunt multiplicanda praeter necessitatem The parsimony principle instead applied learning experience When look scene distinguish entities designate detailed acknowledgement unnamed phenomena allows abstract concepts ﬁnd correlations indicate causal power infer deep essential structure scene Designation appearance language result episodic inference producing knowledge causation perceptual evidence correlation It remains open determine appropriate explanation appearancereality relation Recall started design robotchild chance learning experience education In developmental studies Gopnik Meltzoff colleagues argue childrens theories converge accurate descriptions learning mechanisms analogous scientiﬁctheory formation 47 46459380 On basis view problem modelling relation appearance reality relation science experimental data In way appearancereality problem transformed problem causation prediction Sloman 111 112 discussing Gopnik colleagues view child tiny scientist proposes cleavage Humean Kantian attitude causation The ﬁrst involving direct experience cooccurrence correlations compared Gopnik Shulz 47 consider childrens learning mechanisms That Humean view speciﬁes correlations physical objects terms probabilities causation On hand Kantian view concerns inner structure causation affects universal deterministic laws He argues child starts Humean approach correlation ends Kantian causal structure environment Sloman foresees progression attitude Typically science causation starts Humean acquire deep mathematical theory going use Kantian concept causation These issues close reiﬁcation steps explained Quine 97 initiated observation sentences accom plished sketch epistemological settings Observation sentences occasion sentences vehicle perceptual F Pirri Artiﬁcial Intelligence 175 2011 378415 381 stimuli formation scientiﬁc theory The empirical signiﬁcance observation sentences relation scientiﬁc theory sensory evidence 95 For Quine folly separate synthetic sentences hold contingently experience analytic ones hold come He speaks varying distance sensory periphery statements closer experience stimuli measurements centre held highly theoretical statements physics logic ontology 94 Highly theoretical statements constitute backlog scientiﬁc theory To cope prediction Quine introduces observation categorical doctrine explain bearing sensory stimulation 9697 Although discussion limited different notions causation involved learning mechanisms place initially perception experience observations 15 Commonsense learning experience At point modelling appearancereality relation reduced problem scientiﬁc theory formation natural steps perceptual stimulus elicitation hypotheses conﬁrmation These steps lead description appearance underlying correlation objects possibly counting refutation As generality problem addressed early approaches commonsense reasoning foundational Programs commonsense 75 recent reviews Davies Morgenstern 35 In fact welldesigned robotchild McCarthy intends explicitly AI experiment commonsense world However huge literature commonsense AI approaches effectively faced appearancereality relation Here distinguish attempts formalise perception learning Approaches seeking formalise ception stimulus speciﬁc meaning observation variability earliest taken Levesque 67 Bacchus Halpern Levesque 8 9 These thorough attempts epistemologically formalise sensory data In particular 8 ﬁrst meaningful attempt explicate appearancereality relation simulated Similarly works 68 87102 While prodigious literature learning perception vision statistical learning commu nities case knowledge representation commonsense communities Often learning perception confused learning representation perception If representation perception provided speciﬁc terms predicates symbolic structure dealing measure scale dimension quantities left learning left understanding real problem learned appearance mapped appearance reality Nevertheless bear mind McCarthy starting point robotchild experiment innate mental structures child paramount designer stance notion child tiny scientist McCarthys designer stance implemented taking care crucial passage That providing plausible repre sentation patterns sensory stimuli ﬁrst classiﬁed computational models pattern recognition An empirical method learning experience formally care inductive attitude sense men tioned Polya 89 abstraction determination spatiotemporal world appearance obtained shifting forms empirical reasoning language thought This coincides Quines argu ment 94 shifting sensory periphery centre categorical observations 97 16 The designer stance Dennetts design stance concerned predicting purpose function entity design McCarthys designer stance concerned features intelligent implement haviour Thus designer stance predicts behaviour deﬁning rules governing robotchilds function The design database actions begins specifying signature spelling words denote actions objects properties ﬂuents formalise set tasks This representation context In designer mind abstraction layer schematisation agent activity execution laws formalised set axioms The designer jumping learning steps separates distinctly essential inessential according judgement expressed form axioms These judgements form instructions robotsystem tasks execution This schema induced data designer theorises execution process The structure data considered On hand try upstream conjecture general setting designer stance The design learning process starts perception ends ontology actions exempliﬁed Fig 1 Here supposed designer shows task example opening door speak language seen task So begin designating action names unlock push door But language selfexplaining time point uttered action names learner looking sequence On hand pointing effective time learner able infer enormous data gathered stimuli events single speciﬁc action indicated designer uttering action So added 382 F Pirri Artiﬁcial Intelligence 175 2011 378415 Fig 1 Structure learning model observations axioms task open door inference processes inferring actions movements inferring correlation actions terms actions Showing pointing successful children dogs cats learn open door looking adult humans A dog understands essential concepts aperture needs created quickly pass The actions door needs unlocked turning lowering handle pushed direction How dog infer concepts having hands mouth knowing actions afforded objects like handle door To implement interpretation designer stance hinted formalise continuous learning process perceptual stimulus language thought Indeed simple task opening door learned steps pathway perceptual stimulus categorisation The ﬁrst step concerned early visual perception eliciting number actions sequence movements This step leads learning categorise action large number simpler movements correlate actions basis speed movements positions space distances centres mass brightness The second step concerned understanding local causal relations actions This step identify induction hypothesis extends basic terms signature language quantities learned ﬁrst step mapping speciﬁc parameters probability space The step inductive generalisation concerned establishing rules learned example end task door open Starting number videos different people opening door actually door sequence events sequence frames illustrating hand opening door assess basic laws cause effect specify task Although simple example gives substantial insight crucial issue critical steps process observations leads formation abstract concepts like cause effect laws involved simple task The important aspect transformation quantitative information observations qualitative judgements laws The information collected observations lies highdimensional space colours intensity velocity positions simplest way reduce dimension qualitative assessment underpinning properties observed events use probability functions Stochastic variables true Bayesian conception far entities convey beliefs observations expressed measures terms priors posterior probabilities epistemic states named words speciﬁed properties For example probability certain motion direction given parameters class belief certain action executed produced effects Our idea illustrate aid opening door task data underlying observations explained stochastic variables denoting values actions states These values initially proba bility events example sequence movements speciﬁed descriptors assessed visual perception Further induced densities generate action space gathering movements main actions named For F Pirri Artiﬁcial Intelligence 175 2011 378415 383 Fig 2 A set frames showing sequence moves hand opening door task open door example sequence 50 frames showing hand lowering handle door 50 movements grouped unique action lowering handle signature naming chosen actions provided When actions leads states signature expanded terms functions given abstract representation predicates formulae logical language The role probabilistic logic processes introduce provide structure observations obtain representation formal language conjugate measures truth values In ﬁnal transformation measures eliminated ontology fully expressed language thought That relationships actions states expressed formal language contingent observations speciﬁc measures gathered example taken general And point designer stance begun designing causal effect laws governing speciﬁc task like opening door prediction behaviours based results learning inner structure actions preconditions effects speciﬁc perceptual experience The designer stance allow pathway prediction limited particular postulated general categories designer 17 Paper organisation In following sections discuss process learning task opening door generality The task illustrated observer ideally robotchild number video sequences similar Fig 2 Thus section Section 2 describes early perception analysis devoted deﬁnition action space shaping descriptors mainly motion analysis 72531716 Section 3 shows preliminary formalisation observations prove effectiveness action space estimating density grouping mixtures principal component analysers 104113114 These mixtures form observation model HMM described Section 4 focusing action space dynamic actions Further Section 5 map probability space learned distribution parameters domain probability model processes states actions formalised ﬁrstorder language To end transform hidden Markov model M ﬁrstorder model M processes Section 5 Differently approaches statistical logic probability domain 7496 focus setting parametric distribution domain interpretation language In fact introduce method ﬁtting learned parameters formulae language A typical problem probabilistic logics known probabilities come speciﬁed priori obtained uniform distributions structures counting measures In contrast approaches transformations deal stochastic processes determine 384 F Pirri Artiﬁcial Intelligence 175 2011 378415 learned distribution learned observations map domain ﬁrstorder interpretation Our approach different work example 24258399 107 directly map computational learning model ﬁrstorder domain In sense formalisation ﬁrstorder probability model closer concept BLOG 81 inasmuch domain inherits distribution suitably ﬁxing interpretation atoms according transformation terms statistical model This clearly extended graphical models sampling methods Finally Section 6 probability ﬁrstorder model M transformed model M basic theory actions language Situation Calculus Here generalisation accomplished learning purposes probabilities discarded 2 Features descriptors action space The study human actions motion analysis received good deal attention recent years We refer reader recent reviews 54829265 idea progress application motion analysis understanding human actions behaviour Beside focused speciﬁc example contribution approach respect early phase designer stance mainly speciﬁcation mapping feature space eigen action space The eigen action space obtained grouping motion features actions combining smoothed functions velocity estimating features descriptors density mixtures probabilistic principal component analysers The ﬁrst step design obtain set descriptors motion hand opening door A video sequence task open door composed 200 280 frames 3540 fps image size 640 420 Fig 2 Thus sequence volume 3D matrix 640 420 T frames It t 1 T For sampled video sequence new volumes generated The ﬁrst result early segmentation obtained combining attentionbased motion detection 13 intensitybased meanshift tracking 2122 The result segmentation sequence images noninteresting pixels respect motion saliency blurred uniformly set null This segmented volume denoted GT binarisation analysis deﬁned cid4 δGx y t 1 Gx y t 0 x y t 1 T 0 2 That δ T G like GT pixel values different null set 1 Two volumes generated GT optical ﬂow vector components The optical ﬂow problem amounts computing displacement ﬁeld images image intensity variation Here assumed 2D velocity projection image plane spacetime path 3D point The optical ﬂow vector w u v 1 computed successive images image sequence natural constraint based principle brightness constancy The optical ﬂow frame video sequence computed according Eq 36 A3 returns ﬂow vector w u v t pixel image Brox 16 Namely optical ﬂow vectors w u v t represented time step t images V t U t Fig 3 illustrates direction ﬂow vector w pixel segmented images size arrows pixel x y t given magnitude The ﬂow main feature descriptors image volume Descriptors compact representation relevant features subbehaviours speciﬁed motion direction magnitude Thus descriptors events inducing grouping observed subbehaviours actions For example motion concentrated ﬁngers second frame ﬁrst row Fig 4 sequence frames holding property induces perceptual grouping motionlets single action fact action grasping The principal directions velocity vectors time t t 1 T smoothed velocity magnitude obtained follows dirx y t π δGx y t 1 2k cid5 θ j 2 j 1π 2k cid5cid6 cid5 k arctan cid7 V x y t U x y t cid7 1 π cid8 cid9 cid5 k arctan V x y t U x y t cid10cid7 cid7 1 π j 1 2k 3 2 Mx y t cid2cid2 V x y t2 U x y t2 cid3 12 cid3cid2 cid3 δGx y t Here cid3zcid4 cid5zcid6 respectively ceiling ﬂoor operators dirx y t x y element time t image principal directions velocity segmented binarised image δG x y t Mx y t x y element time t magnitude velocity applied binarised segmented image Furthermore V x y t x y element image v component optical ﬂow time t t 1 T U x y t x y element image u component optical ﬂow time t δG x y t x y element binarised image time t Here k half number principal directions So 12 principal directions k 6 θ j speciﬁc angles wrt xaxis F Pirri Artiﬁcial Intelligence 175 2011 378415 385 Fig 3 Frames corresponding shown Fig 2 illustrating observations descriptors Section 2 Here optical ﬂow shown highlighted arrows indicating motion direction Fig 4 The upper row shows directions optical ﬂow grouped according ﬁrst equation 3 12 directions The lower row illustrates magnitude velocity obtained optical ﬂow The images sequence 272 frames Finally smoothed magnitude Mgt Mt cid5 gσ Mt magnitude image given 3 time t gσ Gaussian kernel size 3 3 variance σ 06 cid5 convolution operation Fig 4 illustrates upper row main directions 4 frames taken sequence lower row magnitude Mgt frames The centre mass extrema velocity magnitude segmented smoothed image time step t t 1 T located cM t 1 α cid11cid12 x y cid13 cid14 cid13 z1 z2 Mgx y t cid2 Mgz1 z2 t 4 Here α normalisation factor depends size arg maxx y Mgx y t 386 F Pirri Artiﬁcial Intelligence 175 2011 378415 Fig 5 The graph shows ﬂow motion directions expressed radians set 58 sampled frames 272 frames video sequence Another crucial aspect action description concept objectrelated actions 66 identiﬁed number regions segmented image This simply devised labelling 8connect regions binary image δG t These active regions values equal 0 For example hand handle distinct regions point hand grasps handle key hole distinct region point door opened To better capture hand action motion tracking extreme edge region highest magnitude values fall identiﬁed time t These coincide hand time step Therefore described feature space feature descriptor time step t frame formed 10 values motion directions dirt Eq 31 1 value time 2 values centre mass location extrema velocity magnitude 1 2 cM Eq 4 1 value integration velocity magnitude frame obtained integrating image Mgt Eq 32 2 values centre mass region cM lies different location cM 1 2 1 value number scalar active regions number nonoverlapping objects image viewer vantage point This data set forms frame video sequence set descriptors instantaneous actions 17dimensional vector Y 3 Deﬁning testing action space mixtures PPCA The descriptors introduced deﬁne action space We perceived motion initial estimate Indeed perceived motion direction resultant motion direction time t To cap ture meaningful difference motion speed direction tell action ends action begins necessary smooth velocity ﬁeld Both resultants time integrated magnitude velocity smoothed Smoothing obtained Laplace movingaverage ﬁlters Let n jθ j t x y dirx y t θ j We indicate velocity components according dirx y t Eq 3 principal directions dirθ ju t cosθ jMgx y t dirθ jv t sinθ jMgx y t cid15 cid16 cid16 j dirθ jv tn jθ j t j dirθ ju tn jθ j t cid17 NRDt arctan cid16 cid16 NIMt cid7 x cid16 cid16 x y Mgx y t y Mgx y tcid7 5 zcid9z Then smoothed functions cid8NDRt cid8NIMt obtained respectively convolving NRDt Here cid7zcid7 moving average kernel size 3 NIMt moving average kernel size 5 negative Laplace ﬁlter Fig 5 illustrates plot NRDt NIMt smoothed functions cid8NDRt cid8NIMt 58 uniformly sampled frames video sequence 272 frames smoothed functions represented dashed lines The extrema gradients smoothed functions time evolution motion divided 5 main regions action space formed 5 principal actions Two video sequences A 245 frames B 272 frames A testing B training Each frame sequences transformed descriptor 17dimensional vector Yt according speciﬁcation given previous section sequence T frames data matrix size T 17 obtained Let denote time intervals appearance motion ascribed jth action j 1 N α j We expect action labelled j described Y specifying frame taken time interval α j F Pirri Artiﬁcial Intelligence 175 2011 378415 387 Fig 6 The graph shows sampled probabilities 17dimensional vectors coding 100 samples taken video 272 frames PPCA mixture The lines graph indicate xaxis ground truth action observed speciﬁc frame 1 100 Note 1D sampled probabilities approximately zero speciﬁc action observed proving coding effective cid10 cid10α j Y P Yα j P Y descriptor time interval Thus use data training sequence B ﬁrst clustering establishing correspondence action labelling time intervals First data 100 frames randomly selected B partitioned N 5 initial clusters cluster actions For ith cluster 1 N sample covariance Σi sample mean μi estimated data proportion ci cluster Let Λi diagλi1 λi17 λi1 cid2 λi2 cid2 λi17 eigenvalues Σi values corresponding eigenvectors deﬁne eigen action space jth action The observed data reassigned clusters maximise likelihood observation eigen action space Using PPCA clustering 114 principal subspace data possible maximumlikelihood estimation parameters Gaussian latent variable model 113 This model building dynamic action model section Thus Y expressed linear transformation unobserved ρdimensional variable z Y Az μ cid13 Here A 17 ρ linear transformation matrix μ 17 1dimensional vector locating observations cid13 residual error combination The space observations set follows Rρ YnT n1 RD znT n1 6 Here D dimension Y T number frames 272 ρ dimension latent space For details estimation Appendix A4 A correspondence time intervals mixture parameters established follows The timelabels αi grouped ground truth according region deﬁned smoothed direction magnitude optical ﬂow deﬁned That frames t 1 30 labelled a1 approachhandle frames t 31 85 a2 grasphandle frames t 86 133 a3 pullDownhandle frames t 134 210 a4 pushdoor frames t 211 272 a5 opendoor Note action names manually forced method generate action space action denotations Now consider mixture component g j parameters c j μ j C j C j A jAcid9 g j labelled action label j given training sequence B time interval α j σ 2 j j I Appendix A4 Then cid16 tα j cid16 tα j Yt μ jcid9 C Yt μkcid9C 1 Yt μ j j 1 k Yt μk 1 k cid13 j In words action space labelling achieved time intervals speciﬁed training sequence Given labelling test predict correct label j descriptor Yt t 1 245 test f Yt sequence A This veriﬁed responsibility mixture component g j pYtg j The sampled probabilities mixture PPCA action cluster shown Fig 6 The ﬁgure shows probability sampled descriptors observed frame time t greater zero frames labelled speciﬁc action This proves frame coding 17dimensional descriptors vector Y effective computation 4 From feature space space actions states In previous sections shown obtain good set descriptors model observed actions providing action space In particular proved effectiveness descriptors PPCA noting eigen action space formed ρdimensional eigenspace spanned latent variable z We seen pdf 388 F Pirri Artiﬁcial Intelligence 175 2011 378415 descriptor Gaussian variance maximised eigenspace likelihood maximal descriptor comes space action represents However PPCA capture dynamic actions timespace relation Given observation sequence Y1 YT observation description visual process want ﬁnd observation Yt explained condition effect action predicts These conditions effects unobservable states We hypothesis appearance action starting ending depends scale motion evolution light change space location afforded objects deﬁnition Yt Since descriptors capture interaction action ending action beginning fact unobserved terms visual features frame Thus state unobserved records executability action following sense transition state state speciﬁes conditions action starts conditions rated end action Thus according results previous section 5 states 5 actions A continuous observation HMM suitable dynamic model actions states requiring estimation transition matrix P states distribution π initial states mixture parameters Ψ modelling local evolution actions respect observed sequences interactions HMM useful chain observed directly process processes Xi Y iicid20 Y iicid20 observed More precisely Deﬁnition 1 Consider bivariate discrete time process continuous observations Yi Xiicid20 Xiicid20 chain Yiicid20 set continuous random variables Let Xiicid20 satisfy independence properties Markov chain Appendix A4 let Yi conditionally independent Y j cid13 j Xk k cid13 j given speciﬁed assignment variable Xk depends The model M bivariate process identiﬁed parameters vector π P Ψ γ 1 π P Markov chain 2 Ψ family parameters mixtures normal densities specifying state emissions Given M mixture components N states probability observation Y state j according mixture PPCAHMM b jY cid2 c jkN Mcid11 k1 Yμ jk A jkAcid9 jk σ 2 jk I cid3 j 1 N 7 Here c jk probability kth mixture state j μ jk mean kth normal density mixture state j A jk σ 2 jk variances observed hidden parameters kth normal density state j speciﬁed mixture PPCA Eqs 44 45 46 Appendix A41 3 γ family parameters mixtures PPCA densities determine test action space Section 3 The reestimation procedure model parameters π Ψ P HMMs Gaussian observation densities scribed 987159 based Expectation Maximisation EM concern action space The adaptation reestimation procedure mixture PPCA HMMPPCA described Appendix A4 An example HMM openthedoor problem described paper 17dimensional observations 5 states 5 mixtures PPCA formed components illustrated Fig 7 mixtures represented 3D ellipsoids ﬁgures illustrate mixture matrix left case dimension 5 3 element mixture gain c jk indicating probability state j mixture component k chosen right transition matrix P The states HMM labelled terms specifying preconditionseffect action We recall useful facts estimation sequences HMMs continuous observation densities given model M states S 1 The probability state sequence ωT s j1 s jT P ωT cid11 s j S π s j1 T 1cid18 k1 P s jk1 s jk 8 2 The probability observation sequence Y1 YT given speciﬁc state sequence ωT s1 sT given indepen dence observations pY1 YT ωT Tcid18 j1 pY js j Tcid18 j1 b jY j 9 F Pirri Artiﬁcial Intelligence 175 2011 378415 389 Fig 7 On left steps motion perception deﬁnition descriptors leading eigen action space On right estimated HMM mixtures probabilistic PCA The HMM composed 5 states estimated descriptors The emission 17dimensional Gaussian mixture 3 components The HMMgraph upper illustrates 3dimensional mixture components obtained posterior distribution pzY latent variables The HMMgraph illustrates left mixture matrix 3d probability mixture component given state On right graph shows transition matrix probability transit state represented xaxis state speciﬁed colour Table 1 Transition matrix sequence images sampled video describing task open door Transition matrix Far Athandle Holdinghandle Unlocked DoorOpened Far 867E01 130E12 266E203 000E00 000E00 Athandle 133E01 868E01 110E31 651E123 000E00 Holdinghandle 382E107 132E01 858E01 157E25 000E00 Unlocked 715E237 523E106 142E01 862E01 276E35 DoorOpened 000E00 000E00 556E194 138E01 100E00 3 The joint probability observation sequence Y1 YT speciﬁc state sequence ωT s1 sT P Y1 YT ωT Tcid18 j1 pY js jP ωT Tcid18 j1 b jY jP ωT 4 The probability observation sequence given state sequence ﬁxed length T pY1 YT pY1 YT ωT MP ωT cid11 s j S π s j1 b j1 Y j1 T 1cid18 k1 P s jk1 s jk b jk1 Y jk1 10 11 An HMM M tell aspects combination actions states The probability observing action speciﬁc state amounts estimation expected number times action observed executed state Likewise probability transition state si state s j depends times temporal sequences frames presented action probably seen executed state si effectively seen Thus learning terms basic local cases effective looking sequence motions induced actions video sequence possible learn sequence precondition action probability action leads speciﬁc state Here examples Let M cid14π P Ψ γ cid15 HMM learned according estimation process described transition matrix P illustrated Table 1 mixture gain matrix illustrated Table 2 Given sequence descriptors Yi specifying observations outcomes learned process order 1 The probability action generated state rationally makes action possible precon dition grasp handle hand handle likelihood grasping according Ψ parameters maximised state labelled Athandle 390 F Pirri Artiﬁcial Intelligence 175 2011 378415 Table 2 The mixtures proportion matrix element ci j indicates proportion jth mixture component activated state Mixture components matrix Athandle Holdinghandle Unlocked DoorOpened Far c1i 3653E01 2763E01 3118E01 2440E01 5536E01 c2i 2693E01 4408E01 2425E01 3005E01 2677E01 c3i 3654E01 2829E01 4458E01 4555E01 1787E01 Fig 8 The graphs illustrate sampled probabilities sequence observations showing grasphandle left sequence observations showing pushdoor right The graph shows probability maximal state 2 grasphandle state 4 pushdoor expected 2 The likelihood sequence states s1 sT state action maximal probability executed 3 The probability observing presumably correct sequence actions likelihood approachhandle grasphandle pullDownhandle pushdoor opendoor 4 The probability given correct sequence actions correct sequence states obtained probability end sequence door opened likelihood state denoted DoorOpened These questions easily answered model M according estimation process HMM parameters described First note item 1 consequence maximumlikelihood estimation parameters mixture covariance obtained PPCA On hand item 2 checked Viterbi algorithm Example 1 To labelling correct illustrate 1 sampled values pdfs sequence 58 frames action grasphandle 58 frames illustrate movements concurring formation action grasphandle 2 sampled values sequence 32 frames action pushdoor given 5 states It seen graphs Fig 8 likelihood maximal second Athandle fourth Unlockeddoor states respectively expected When sequence motions repeated action grasphandle observed learned model M generates sequence states starting ﬁrst state Far transiting state Athandle remaining state This fact estimated prior actually M arg maxx π x s1 transition probability Far Athandle relevant Table 1 The maximum likelihood sequence actions grasphandle close 0 In case sequence actions pushdoor learned model M generates sequence states starting ﬁrst state Far remaining state way transition states maximum likelihood close 0 Thus unwanted sequences created Finally consider case sequence frames motions illustrating correct sequence actions The obtained sequence states expected 10 experiments performed test data log likelihood computed Viterbi algorithm 116 That list best states computed Viterbi maximising step t P Y1 Yt X1 Xt x jMpi jb jYt assigning list best states argument maxi 1 s2 mum Thus given list best states x15 x0 s2 5 superscript indicates states repetition T 1 t1 P xt1xt 26516E004 Indeed sequences ended DoorOpened The stationary dis P DoorOpenedX π x0 tribution illustrated transition obtained T 87 3 s3 4 s4 2 s4 cid19 The stationary distribution described Appendix A2 ensures ﬁnite property F Pirri Artiﬁcial Intelligence 175 2011 378415 391 Deﬁnition 2 Let M HMM P irreducible satisﬁes conditions Lemma 5 Ap pendix A3 M ﬁnite property At given example learning task opening door seen speciﬁc HMM model M obtained sequence starting parameters set Each model long trained according correct observed sequence starting hand approaching handle ending door opened lead expected sults slight variations Thus class models tuned task speciﬁc observations sequence For parameters estimation Appendices A3 A4 5 Firstorder parametric models In section start care induction step We settled basic step learning parameters probability space actions hidden values specifying states Hidden state transitions learned likewise likelihood observations given states These set parameters π P Ψ γ initial data set ﬁnite set sequences induced transformations initial parameters train model described previous sections There inﬁnite number possible models set initial parameters learning task requires observations Continuing open door example look generalisation step transforming parameter space learned small number sequences general rules behaviours In subsections learned parameters extend simple signature action terms new signature encoding predicates functions terms We language obtained correspondence formulae ﬁeld terms ﬁrstorder probability structure random variables HMM 51 Probability structures Usually assume real world events described random variables behaving according unknown distribution estimated observed cases possibly prior belief distribution In words random variable practice want determine distribution In studies probabilistic logic problem faced axiomatisation pushed forward assess properties probabilistic inference For example Keisler 61 shows ﬁrstorder probabilistic logic quantiﬁers possible assert random variables X1 X2 independent Indeed McCarthy 74 argues The information necessary assign numerical probabilities ordinarily available Therefore formalism required numerical probabilities epistemologically inadequate2 exactly lack methods feed parameter learning formalisms reasoning Since early studies ﬁrstorder probability Gaifman 40 Krauss Scott 110 Keisler 6261 Hoover 52 wealth research dedicated integration logic probability Nowadays seminal contributions computational purposes works Abadi Bacchus Fagin Halpern 36761493848 introducing ﬁrst order probabilistic logic ﬁrstorder quantiﬁers real valued formulae More recently streams research logic probability integration appeared concerning probabilistic logic programs Poole 90 likewise 249983 connected learning stochastic logic programs Koller colleagues 64 42 introduced probability relational models Domingo colleagues introduced Markov logics 4 general overview 34 Milch Russell 81 introduced ﬁrstorder models unknown objects These examples wealth approaches enumerate The clear direction adapting ﬁrstorder probability languages statistical needs computational learning way round lift statistical learning ﬁrstorder logic Here aim different Here aim embedding distribution learned ﬁrstorder probability model More precisely interested learn probability logic example PRISM 106 use parameters feed logic inductive step language thought The task build ﬁrstorder probability model accepts formulae enunciating events HMM describes random variables probability observed action given sequence states probability state given sequence actions comparing probabilities events This natural approach preserves declarativerelational structure logic prepares ground embedding learning viewed early computational process reasoning For example concerned model construction proof theory axioma tisation For aspects mainly refer seminal works Gaifman Halpern Bacchus 41748 In fact main results Bacchus Halpern work speciﬁcation proof theory complete alternative restrictive conditions That logic complete The domain assumed countable measure functions ﬁeldvalued nonArchimedean gener ated algebra events ﬁnitely additive 76 2 Bacchus 6 refers sentence 392 F Pirri Artiﬁcial Intelligence 175 2011 378415 b The collection axioms includes proper axiomatisation reals language excludes random variables domain bounded size 49 c The language includes random variables domain bounded size Archimedean property given 76 In fact Halpern chooses axiomatise reals theory real closed ﬁeld Bacchus admits measure functions ranging nonArchimedean ordered ﬁelds sense logic admits inﬁnitesimals34 Let consider ﬁrstorder language Lp probabilities domain presented 76 The language Lp formed formulae sentences ﬁrstorder logic plus ﬁeld terms αcid16xcid16x αcid16x ﬁrstorder formula free variables cid16x probability term constructor A statistical probability structure Lp M cid14A F Πn μnnωcid15 A D I classical ﬁrstorder structure F totally ordered ﬁeld numbers n Πn ﬁeld subsets Dn μn probability function Πn range F The language L1Φ presented 49 like Lp ﬁeld terms range reals F real closed ﬁeld having ﬁrstorder properties ﬁeld real numbers ﬁeld terms denoted wcid16xα A structure L1Φ M D π X μ D domain π interpretation X σ algebra D μ probability function X essentially counting measure In section substantially refer work 749 Let start learned HMM M cid14S H π P Ψ γ cid15 S ﬁnite set states H set observations ﬁxed dimension respect model For example task open door observations 17dimensional vectors parameters Ψ tuned 17dimensional space shown Section 4 However assume variables 1dimensional simplify notation Finally set parameters formed transition matrix P mixture parameters Ψ initial distribution parameter π selecting initial state S action space parameters γ We deﬁne canonical parametric parameters π P Ψ given probability structure M M follows 511 The signature The signature cid17M language includes terms sort state denote set S states terms sort observations denote set H observations Note states observations induce random variables Appendix A1 domain states ﬁnite dimension n domain observations R To sorts add terms N W endowed constructor W domain countable sequences states sort sequence states w S Finally signature includes terms sort natural numbers indexing Notation Indices denoted natural numbers n m t When refer states elements domain denote s referred variables denoted x possibly indexed Sequences states denoted w use w denote variable sort sequence states term xn x0 When refer observations domain denote h referred variables sort observations denoted y The language includes monadic predicate symbols R state si S taking argument sequence W binary predicates A j observation action taking argument observation h H state si S binary predicate symbols O ji deﬁned conjunction R A j arguments observation sequence states Finally language includes relation cid3 abbreviating sequences To sorts add real line R assume represented Matlab Borel σ ﬁeld real line BR Sorts taking values reals include observations ﬁeld terms mixture gain matrix c S 0 1 0 1 normal distribution N μik AikAcid9 I μik Aik σik elements Ψ similarly γ random variables Xi Y 1 2 Sorts mapping reals include initial distribution π states π S 0 1 transition matrix P S S 0 1 σ 2 ik ik 512 Formulae language Firstorder formulae deﬁned usual Field terms formulae including ﬁeld terms deﬁned follows classical laws connectives quantiﬁers implicitly assumed Deﬁnition 3 A ﬁeldbase α ﬁeld term deﬁned inductively follows 1 If y x w respectively terms sort observation sort state sequence states A j y x R iw O y w ﬁeldbases cid10 terms sort W w w w w w w 2 If w w 3 If α1 α2 ﬁeldbases αi α1 α2 ﬁeldbases cid10 cid10 ﬁeldbases 3 The Archimedean property says set reals positive upper bound x R x 0 n N xn 1 4 See Hammond 50 discussion advantages nonArchimedean ordered ﬁeld game theory F Pirri Artiﬁcial Intelligence 175 2011 378415 393 4 If αx ﬁeldbase x free variable possibly varying sort state observation sequence xαx ﬁeldbase Deﬁnition 4 A ﬁeld term constructor α cid22 0 1 α ﬁeldbase formula A ﬁeld term deﬁned follows 1 If xi x j terms sort states Pxi x j ﬁeld term likewise π xi μi j σi j 1 n j 1 k ﬁeld terms Also Xix 1 2 T ﬁeld terms If y term sort observation N yμi j σi j Y j y j cid2 1 ﬁeld terms In particular atomic ﬁeld terms 2 If α1 α2 ﬁeldbases free variables varαi V 1 2 V sort state sequence states αiV discrete probability term V ranges observations αiV continuous probability term deﬁned ﬁeld terms 3 α1V α2V 4 αiV α jV cid10 ﬁeld term cid10 α1V α2V cid10 α1V α2V cid10 1 αiV ﬁeld terms Deﬁnition 5 If f z ﬁeld term z variable sort state observation sequence states p 0 1 formulae formed ﬁeld terms follows f z p f z p f z p formulae shall abbreviate cid3 cid2 cid10 f z cid2 gz 1 2 If f z gz 3 If φz ψz 4 If ϕz formula z variable occurring free possibly z sort cid10 ﬁeld terms f z cid3 gz cid10 formulae including ﬁeld terms φz ψz cid10 φz formulae cid10 formulae state observation zϕz formula To terms measure terms η X X index speciﬁed products added detailed section 513 Domain probability space The domain D partitioned following sets ﬁnite space S states space H observations taking value R space W S S S T sequences states denoted ω T cid3 N Let cid80 σ ﬁeld formed subsets S cid8 cid80 cid80 cid8T 0 Given distribution π initial distribution S transition matrix P speciﬁed HMM parameters consider discrete random variables X1 X2 Markov property deﬁned probability space mapping S T S Xtω s jt A probability measure ηs cid8 cid22 R satisfying Markov property cid2cid12 ηs X1ω s j1 Xtω s jt cid14 j1S cid11 cid3 s j1 s jt π s j1 t1cid18 i1 Ps ji1 s ji 12 ηs countably additive measure 15 Since sum initial density π 1 P stochastic matrix W cid8 ηs probability space H set observations domain R Let Θ smallest σ ﬁeld generated Borel sets R The elements Θ Borel sets denoted B Let H Θ measure space measure η On space H Θ η consider elements H random variables Y j identities The probability measure measure space induced random variables deﬁned cid20 ηB f y dy B Θ f y B Mcid11 k1 cid2 ckN yμk AkAcid9 k σ 2 k I cid3 cid2 μk Ak σ 2 k γ cid3 13 Here f y density speciﬁed mixture PPCA deﬁned Section 3 Appendix A4 Eq 40 N normal density When random events speciﬁed respect ﬁxed state s S η extended ηI product space H W σ ﬁeld Θ cid8 formed measurable rectangles B Q B Θ Q cid8 That E Θ cid8 Q ω h ω E lies cid8 B h h ω E lies Θ On measure space H W Θ cid8 density random variables bi taking arguments y implicitly state si cid20 ηI B bi y dy B Θ bi y B Mcid11 k1 cid2 cikN yμik AikAcid9 ik σ 2 ik cid2 cid3 I μik Aik σ 2 ik Ψ 1 N cid3 14 t cid4 394 F Pirri Artiﬁcial Intelligence 175 2011 378415 Here N number states M number mixture components When B R bi integrates 1 B ηI 0 Indeed H W Θ cid8 ηI probability space We introduce structure M follows Deﬁnition 6 A probability structure ﬁrstorder parameters ﬁxed HMM model M S H π P Ψ γ M D Ψ Φ I Here D domain D S W H Ψ set parameters deﬁned HMM M S H π P Ψ γ Φ probability space deﬁned cid2 cid3 W cid8 ηs H Θ H S Θ cid8 ηI Φ 15 Here cid8 Θ cid8 sigmaﬁelds deﬁned ηs ηI associated probability measures Finally I interpretation signature deﬁned follows 1 Interpretation predicates R A j O ji j 1 N N number states number observation actions Let Xi S T cid22 S discrete random variable cid13 cid13 X1ω arg max R I cid21 cid12 ω π s Xt ω si t cid2 s cid14 1 N 16 Let N number observation actions mixture components observations independently states number states M number mixture components observations state Let z R Acid5 j I h cid13 cid13 cid13 cid13 cid22 h μ j2 σ 2 j cid3 z cid2 cid3 j 1 N μ j σ j 13 Let z R cid4 A j si I h cid13 cid13 cid13 cid13 h μik2 σ 2 ik wt1cid15 cid3 z h Acid5 I j k 1 M cid22 cid2 cid3 j 1 N μ jk σ jk 14 cid12 O ji I cid14h sit cid13 cid13 h A j sit I sit wt1 R I cid14 j 1 N 17 18 2 Interpretation ﬁeld terms Let v assignment free variables cid14cid3 cid2 cid3cid3 cid13 cid2cid12 cid2 cid23 cid24I v ηs R iwt v wt cid11 X1ω Xtω cid2 I t cid2 cid13 ω R cid3 t1cid18 π X1ω s j1 cid2 P Xk1ω s jk1 cid13 cid13 Xkω s jk cid3 s jt si 19 Let Y j H cid22 H identity vwt X1ω Xt ω k1 cid23 A j y si cid24I v ηI cid2 cid2cid12 h cid13 cid13 v cid3 yY jh A j si I 1 N cid14cid3 cid20 bih dh B Θ B Finally cid23 cid24I v O ji cid23 A j R cid24I v 3 The ﬁeld terms π P N interpreted 20 21 In particular I ensures distribution domain agree model M This established follow ing Lemma 1 Given HMM M S H P π Ψ γ ﬁnite property Deﬁnition 2 exists probability structure M D Ψ Φ I domain D S H W R probability space Φ generated D atoms terms interpreted according Mparameters P π Ψ Furthermore according given interpretation I ﬁeldbased atom ϕ corresponding measurable set ﬁeld term ϕ intended distribution Proof See Appendix A5 proof Lemma 1 cid2 We shown interpretation atoms fully determines distribution ﬁeld terms structure M HMM M Before showing extend formulae illustrate lemma implemented example F Pirri Artiﬁcial Intelligence 175 2011 378415 395 Fig 9 Structures M M example opening door interpretation signature Example 2 In example illustrate implemented interpretation atoms considered task assignments free variables5 evaluation atomic ﬁeld terms For details refer reader proof Lemma 1 Appendix A5 Let M π P Ψ γ learned HMM structure M ﬁrstorder parametric structure speciﬁed Deﬁnition 6 The signature language LM resumed Fig 9 The implementation considered based Matlab Prolog interface suitably implemented The atomic ﬁeld terms evaluated Matlab appealing functions implemented HMM M data results reported Sections 3 4 Each predicate R 1 N R Far Athandle Holdinghandle Unlockeddoor Openeddoor takes argument sort sequence states Its interpretation deﬁned set sequences states W nth element ﬁrst noutcomes si n length 87 In fact P87 stationary distribution model M Sequences start state s1 assuming π s1 1 The interpretation R speciﬁed Eq 16 Now consider Farwt given assignment v free variable vwt X1ω s1 X2ω s j2 X3ω s1 j15 follows Farwt true M If vwt X1ω s1 X2ω s j2 X3ω s3 j15 Farwt satisﬁed M Thus wt indicates sequence length t considering event experiment repeated t times Xiω indicates outcome time The probability measure ﬁeld term Farwt Mv speciﬁed 19 cid23 Farwt cid24Mv π X1 s1 cid2 cid3 PX2 s1 X1 s1PX3 s1 X2 s1 cid2 cid3 PX2 s2 X1 s1PX3 s1 X2 s2 PX2 s3 X1 s1PX3 s1 X2 s3 22 Consider predicates A j Each predicate A j actions speciﬁed following action names j 1 N number states equals number observed A j approach grasphandle pullDownhandle pushdoor opendoor An observation action predicate A j takes argument element sort observation h H denoted variable y element sort state s S denoted variable x Its interpretation Eq 17 deﬁned events speciﬁed 5 We indicate assignment free variable z domain element d vzd vz d 396 F Pirri Artiﬁcial Intelligence 175 2011 378415 I j state si To ensure values centred mixture generated A descriptors real valued variables having Mahalanobis distance mean covariance σ 2 z 17 Deﬁnition 6 The domain A j generated follows Let U R set values generated normal distributions parameters ﬁxed HMM M Take U interval B z delimited values y U having Mahalanobis distance according parameters μiu σiu Ψ u 1 M speciﬁed value z assign I j ﬁrst check Mahalanobis A distance mixture speciﬁed Eq 13 In general assume z decided according distances means different mixtures For example μ ju μku means component u mixtures states sk s j require μ ju μku2 σ 2 ku z This ensure interpretation observations overlap modes The probability measure ηI ﬁeld term distribution function random variable Y j domain A j si Given assignment v yh A ﬁeld term A jh si evaluated density bih speciﬁed 14 1 derivative distribution function juσ 2 I j Finally denotation predicate O ji intended Poss jiaction state j indicates observation action state The action meant observation action descriptor The interpretation O ji conjunction A j R deﬁned 18 Its ﬁeld term deﬁned 21 We extend interpretation formulae stated following theorem Theorem 1 Let M probability structure ﬁrstorder probability space Φ Let Φ extended probability space Φcid5 I ηs ηI n n cid2 1 space product For ﬁeldbase formula ϕ exists measurable set product measures ηn distribution agree HMM s ηn Proof See Appendix A5 proof Theorem 1 cid2 We extend relation cid25 ﬁeld terms formulae mentioning ﬁeld terms follows let P ηs ηI ηs ηI let Γϕ measurable set ϕ cid24 cid23 ϕcid16x cid2 p 1 M v cid25 cid24 cid23 ϕcid16x cid2 p 2 M v cid25 y cid24 cid23 ψcid16yϕcid16x 4 M v cid25 cid2 p iff P Γϕ cid2 p iff M v yd cid25 iff M v cid25 cid24 cid23 ϕcid16x cid2 p d D right sort cid24 cid23 ψcid16y ϕcid16x cid24 cid23 ϕx cid2 p 23 Proposition 1 Let v v cid10 agree assignments free variables M v cid25 ϕ iff M v cid10 cid25 ϕ Proof See Appendix A5 proof Lemma 1 cid2 Theorem 2 Let M S H π P Ψ γ exists probability model M structure M M M agree distribution domain Proof See Appendix A5 proof Theorem 2 cid2 Example 3 Let continue Example 2 illustrated Fig 9 Let assume interpretation atom speciﬁed described Example 2 according Lemma 1 Let binary predicate O ji Poss ji indicating action A j observable executable ﬁnal state sni nlength sequence Let y y variables sort observation Consider sequences ending state labelled Unlockeddoor Athandle We want verify likelihood observing Pushdoor state Unlockeddoor likelihood observing Pushdoor state Athandle We express fact follows Let wn1 wm1 variables sort sequence states referring sequences length m 1 n 1 m 3 n 4 cid10 cid23 cid24 Poss jk y xn wn1 Pushdoor y xn Unlockeddoorxn wn1 Pushdoor cid24 Athandlexm wm1 xm wm1 cid23 Poss jk xm y y cid3 cid3 cid2 cid2 cid10 cid10 24 Let assume assignment free variables sort sequence states follows vw 3 X1ω s1 X2ω s2 X3ω s3 vw 2 X1ωcid10 s1 X2ωcid10 s2 terms sort state vx3s3 vx4s4 Furthermore assume π s1 1 s1 ﬁrst state labelled Far Let h H Y jh z M cid25 Pushdoorv yh s2 cid10h s4 We recall previous example Pushdoor predicate veriﬁed parametric M cid25 Pushdoorv y model M according Mahalanobis distance atomic ﬁeld term evaluated Eq 7 The pdfs b4h bUnlockeddoor h b2h b Athandle h return likelihood ﬁeld terms Pushdoorh s2δ Pushdoorh s4δ F Pirri Artiﬁcial Intelligence 175 2011 378415 397 In implemented model observation 17dimensional vector projected 3D space section actually reduced space 1D stochastic variables For example consider real 17D observation ˆh Hence ˆh 17dimensional descriptor The results projected h 005 048 047 047 055 cid9 cid2 cid2 h h b2 cid3cid24M cid10 s4 cid2 cid3 cid2 cid10 b3 h h cid2 h 0002 cid23 Pushdoor cid3 cid10 cid3cid24M b4 0274 A4 cid3 cid10 cid2 h cid2 h b1 cid3 cid3 cid23 cid10 cid10 cid10 cid10 s4 0001 b5 cid24Mv π s1 cid23 Unlockeddoorx4 w 3 3cid18 i1 Psi si1 0006 cid23 Athandlex3 w 2 cid24Mv π s1 2cid18 i1 Psi si1 0192 cid23 Poss44 y x4 w 3 cid24M b4 cid2 h cid10 cid3 π s1 3cid18 i1 Psi si1 0006 0274 00016 On hand according assignment v cid23 Poss42 y x3 w 2 cid24M b4hπ s1 2cid18 i1 Psi si1 0001 0192 00002 25 26 Now deﬁnition O ji product rule Bayes rule obtain righthand 24 reduces 26 00002 Hence gather likely action Pushdoor executed state Unlockeddoor likely precondition observed action We ﬁnally note absence proof theory use Bayes rule semantic level Indeed models parametric canonical proof theory deﬁned modulo parameters In fact given class A HMM models M1 M2 observed event exist class B parametric probability models M1 M2 corresponding A according Theorem 2 Now given classes structures model group events formulae true structures independently variation parameters structure Thus proof theory lifting individual parametric models class models The analysis issue step research 6 The ﬁnal induction step concluding Situation Calculus At step purpose complete induction obtaining simple theory action This step attained mapping formulae true parametric probability model formulae Situation Calculus Learning concluded cycle reasoning step Recently research efforts directed problem formalising learning steps theory actions planning purposes For example Amir Chang 3 introduced dynamic model actions solving problem simulta neous learning ﬁltering partial observations The model constructs aware real states respect expected ones The model aims similar investigate induction structures rely logic ﬁrstorder probabilistic logic On hand Pasula colleagues 86 propose framework learning suitably represent new concepts new rules handling action effects plan ning domains So particular objective 86 learn transition model This language Skolem constants ﬁrstorder rules actions combined probabilistic rules As approach determinis tic model action learned However 86 learning construct perception effort representation level adding noise Therefore clear combination rules allows reasoning learned structure The differences approach respect methodology formalism For example use operators model learning induction process based different phases On hand 86 assess process set rules embedded speciﬁc logical formalisms Furthermore approach learning precisely experience action model actually constructed online shown examples 86 approach online learning addressed In fact learning rules initialised designer completely learned method Under view similar 86 logical learning studied connection logic works For example connection logic programming language SLP Stochastic Logic Programs Muggleton 11883 Cussens 24 PRISM Programming statistical logic Sato Kameya 107 On hand BLOG 81 offers representation paradigm HMM com puted embed parameters ﬁrstorder formulae offers interesting methods sampling interpretations Learning axioms action ontology regarded discovery problem 398 F Pirri Artiﬁcial Intelligence 175 2011 378415 The Situation Calculus SC 74100101 optimal purpose illustrated example provides language dynamic theory action Despite deterministic SC models ﬁrstorder Markov property actually model nth order Markov property models timehomogeneity sequences situations histories actions inﬁnite tree choices 101 We recall Situation Calculus earlier formalised McCarthy Hayes 74 extended Reiter order simple solution frame problem exploiting deﬁnitional strength ﬁrstorder logic appealing metalanguage The version Situation Calculus provided initially Reiter 100 McCarthy calls Toronto Situation Calculus multisorted language powered second order induction axiom In particular language sorted distinguished disjoint domains Act Obj Sit Actions Objects Situations comes axiomatisation situations denoted Σ inducing mentioned tree rooted special situation called S0 Situations bounded countable second order axiom speciﬁed Σ forcing structures language intended ones Reiter solution frame problem introduced axiom schemata classiﬁed sets Duna axioms specifying unique names actions Dap axioms specifying preconditions actions Dss succes sor state axioms These sort axiom schemata suitably elaborated obtain dynamic theory actions meaningful specifying initial conditions state affairs axioms telling true S0 state world instant actions clock This initial description called Ds0 In 88 shown initial theory called DS0 consistent additions axioms turns conservative extensions DS0 Duna given induction A structure LSC deﬁned follows Deﬁnition 7 A structure M language LSC deﬁned D ISC D domain partitioned sets object X actions A situations S ISC interpretation The signature language LSC formed countable set variables sort constant S0 sort situation function A S cid22 S countable set distinguished predicates called ﬂuents taking argument term sort situation special ﬂuent symbol Poss To set predicate function symbol added The language LSC formed terms formulae signature usual connectives quantiﬁers extended include ﬁrstorder predicate variables sort ﬂuent The discovery problem generate simple theory actions task open door parametric model M To end build structure M speciﬁed language DS0 set axioms Dss Duna obtained sentences maximal probability values M Let specify signature LSC basis signature LM follows Deﬁnition 8 Signature LSC The signature LSC deﬁned signature LM follows 1 Variables sort object action situation freely introduced 2 A new constant symbol S0 sort situation introduced 3 Constants sort object freely introduced names element devised image sequence6 4 For predicate symbol A j LM zeroary unary function symbol sort action introduced predicate symbol R LM analogous unary binary ﬂuent symbol introduced 5 For predicate symbols O ji LM j 1 m m number actions 1 N N number states binary predicate symbol Poss introduced Given structure M LSC suitable construction 88 possible ensure M satisﬁes founda tional axioms Σ Appendix A6 Deﬁnition 66 Let M structure LSC satisfying foundational axioms Σ introduced symbol A j R O j v v assignments free variables deﬁne cid10 M v cid25 A jz cid13 Aiz M v cid25 R iz S0 M v cid25 Poss cid2 cid3 Aiz S0 j cid13 j iff M v cid10 cid25 R ix v cid10 iff M v v cid10 cid25 Ai y x v cid10 x arg max sS π s π s x arg max sS cid23 yh arg max hB cid10 Ai y x cid24Mv δ cid10 27 cid10 y v Here v initial state structures HMM M Ai y xMv cid10x respectively elements domain observation states M π assigns probability cid10 subscript δ indicates density bs y δ 6 In paper constants denoting objects derived learning identiﬁed number interesting elements image recorded descriptors interpretation To adequately develop issue necessary extend visual step tagging add suitable sound analysis tagging F Pirri Artiﬁcial Intelligence 175 2011 378415 399 vxs Appendix A proof Lemma 1 We deﬁned Duna DS0 ensured M model Σ Duna DS0 Now given structure M Σ Duna DS0 LSC extend ﬁrstorder probability model M We obtain M set successor state axioms ssa set action precondition axioms apa axioms added consistently deﬁned initial axiomatisation consequence relative consistency theorem 88 The relative consistency theorem fact says starting structure model set Σ Duna DS0 structure model set Dap Dss deﬁned language 88 Generating axioms M considering ﬁrstorder M In fact clear relying ﬁrstorder information recorded structure M insuﬃcient information structure complete On hand probabilistic information conspicuous The learned likelihood observed executed actions sequence likelihood states tell connect actions transitions states It turns preconditions clearly established effect actions reasoned referring projecting state This presented Going Eq 27 arg maxvcid10 yB Ai y skMv δ cid10 Ai maximal according state sk assignment v More precisely cid10 indicates value B Borel set R density Deﬁnition 9 A j y skMv δ maximal M precisely sk iff v yh B 1 A j y skMv 2 A j y skMv δ δ cid10 skMv A j y cid2 A j y suMv δ cid10 B v y su su cid13 sk cid10h δ R ix wt maximal likely M vxsi 3 R isi wt cid2 R js j wt si cid13 s j The observation values A j y skMv arg maxhB A j y skδ δ maximal M assignment v deﬁned v yh The deﬁnition extended terms including Pi j According learned structure M state si S exists action ﬁeld term maximal indepen dently process sequence leads si example Fig 6 This precise following lemma Lemma 2 For t exists likely state R ix wt M v cid25 R ix wt exists A j A j y xMv maximal precisely vxs s interpretation R independently assignment v wt maximal Furthermore A j y xMv δ δ Proof See Appendix A6 proof Lemma 2 cid2 In similar way maximality ﬁeld terms concerning state predicates transitions Lemma 3 For likely state R ux wt M v cid25 R ux wt exists observation action A j y x transition P Xt2 x cid10 x wt maximally cid10 Xt1 x extends R ux wt R ix P Xt2 x That A j y xMv cid10 Xt1 x maximal vxsu δ Proof See Appendix A6 proof Lemma 3 cid2 An immediate corollary property Corollary 1 For state predicate R ix x unique transition P Xt2 x Xt1 x cid10 A j y x cid10δ Am y xδMv maximal vxsi cid10 x cid10 wt M v cid25 R ix x cid10 extends R ux cid10 wt R ix x cid10 wt exist observation actions Am A j cid10 wt maximally That P Xt2 x Xt1 Proof See Appendix A6 proof Corollary 1 cid2 We following remarks concerning observations Given state R ux wt wt sequence t 1 T cases occur observed action A j y x cid10 relatively state 400 F Pirri Artiﬁcial Intelligence 175 2011 378415 maximal x vx vx 1 Remaining A j y x cid10Mv δ remains state cid10Mv δ incoming state cid10 maximal x 2 Entering A j y x cid10 enters state x 3 Exiting A j y xMv maximal x P Xt2 x abandons x state execution A j δ cid10 P Xt2 x Xt1 x maximal process P Xt2 x Xt1 x cid10 maximal vx cid10 cid13 vx In case process cid10 Xt1 x maximal vx cid10 cid13 vx Hence process Note importance A j y xδ maximal x action observed according parameters estimation distributions learned models Thus observed action executed state speciﬁes action preconditions However effects speciﬁed Lemma 3 joint observed action transition Since successor state axiom captures persistence properties need use properties ensure somewhat analogous persistence property follows Deﬁnition 10 Persistence Let wt ﬁxed sequence vwt X1ω s j1 Xtω s jt wtMv R jt1 x M v cid25 R jwt state predicate wt t k R itk x wtkP Xtk1 x cid10 R jwt When R jt x x x happens t happen t 1 said exit wt deﬁnition Xt1 cid10 wtk happens t happen t 1 said enter wt R jt2 x x said happen wt t cid10 Xtk x P Xt x cid10 A state predicate property R j cid10 wtP Xt2 x Xt1 x A predicate R j happens ﬁxed sequence wt t k k cid2 0 persistent M v cid25 R jx wt R ix wtMv cid10 maximal t cid10 x cid10 wt exist actions A j y 1 When enters wt t 2 form R ix x cid10 cid10δ Am y xδ maximal x cid10 x cid10 Am y x Moreover terms A j y cid10 wt happens wt t 1 persistent 2 When exits wt maybe future reappearance exist actions Aq y A j y predicate state R ux cid10 x cid10 Am y x M v cid25 x vxsu u cid10 x cid10 Ak y x terms cid2 x respectively Furthermore cid10 wt x cid3 cid2 cid3 cid2 cid3 M v cid25 Aq cid10 y cid10 x cid10 wt cid10si vxsu u cid13 However possible delay exit wt t t k Ak y x R R x x cid10 maximal x That vx k 1 M v cid25 Ak cid2 cid3 cid10 y x cid2 cid10 wt x cid3 cid2 R R cid10 x x cid10 wt cid3 These conditions expressed maximality term cid10 wt cid3 cid24 A j y x cid23 R ix x wt Ak cid3cid24 cid2 cid10 y cid23 cid2 x x R M v x 28 The persistence predicate state stated Theorem 3 Given M assignment v vwt ˆwt ˆwt X1ω s j1 Xtω s jt sequence x x cid10 ˆwtMv persistent cid10 ˆwt R ix x t cid2 0 R exists Proof See Appendix A6 proof Theorem 3 cid2 Given results ﬁnally specify obtain action precondition axioms successor state free variables denoting domain object M z cid10cid10 cid10 axioms learned theory note variables z z speciﬁed M 1 M v cid10 cid25 sPoss cid2 A jz s cid3 cid2 cid3 cid10 z s R iff M v cid25 x A j y x R ix wt A j y siMv cid2 Am cid3 z doa s cid10 cid25 sR z A j cid2 cid3 cid2 z cid10cid10 δ cid10 2 M v maximal precisely si chosen x cid10cid10 cid3 z cid13 R iz s cid2 cid23 iff M v cid25 R R ix x cid3 cid2 cid3cid24 cid23 cid2 cid10 wt A j x x cid10 wt persistent vwt X1ω s j1 Xtω s jt x x wt Am 0 x R y cid10 y x cid2 cid10 cid3cid3cid24 t cid3 T P 29 F Pirri Artiﬁcial Intelligence 175 2011 378415 401 So interesting note universal quantiﬁers sort situation LSC introduced action maximality persistence domain W structure M On hand quantiﬁers actions ﬁxed equality successor state axioms fact justiﬁed choice argument v y respect maximality Note variables sort objects freely arbitrarily added according comment item 3 concerning signature LSC We obtained basic theory actions basis learned initial perception Example 4 A basic theory actions task open door According deﬁnitions action theory turns following added arbitrarily variables constants sort object The signature speciﬁed Example 2 illustrated Fig 9 subscripts removed All actions situation variables universally quantiﬁed gathered Initial database DS0 Farz S0 z door coincide ﬁrst state models M M arg maxx π s1 label R1 denotation Far Action Precondition axioms cid2 cid2 cid2 cid2 sPoss sPoss sPoss sPoss approachz s cid3 Farz s cid3 cid3 graspz s Atz s cid3 pullDownz s Holdingz s pushz s Unlockedz s Successor state axiom cid2 cid3 z doa s cid3 z doa s cid2 sFar cid2 s At sHolding sUnlocked cid2 sOpened cid13 approachz Farz s approachz Atz s cid3 z doa s cid2 graspz Holdingz s cid3 pushz Unlockedz s z doa s cid3 z doa s openz Openedz s 30 31 It remains prove conditions M Eqs 29 deﬁned That need ensure successor state axioms obtained uniquely parametric probability model ﬁrstorder In words ambiguities choice action precondition axioms successor state axioms This stated following theorem Theorem 4 Let M ﬁrstorder parametric probability structure deﬁned Deﬁnition 6 v assignment free variables Let vwt X1ω s j1 Xtω s jt let R ix wt predicate state M v cid25 R ix wt 1 There action A j y x maximal vxs s interpretation R 1 N Hence action precondition A j uniquely determined R 2 There exists action A j possibly Am ensuring persistence satisﬁability R 1 N Hence cid10 wt A j y x R ix x successor state axiom R uniquely determined maximal values R ix x wt Am y cid10 xMv Proof See Appendix A6 proof Theorem 4 cid2 The method provided ensures given ﬁrstorder probability structure M possible construct uniquely theory actions derive initial database action precondition axioms successor state axioms It interesting note probability model M possible establish properties states stronger classical HMM states observations speciﬁed ﬁrstorder properties This allowed obtain axioms basic theory actions Situation Calculus We need fact M deal observations single variables There limits First example poor The main reason fact able learn signature labels HMM Furthermore method established assign elements sort object domain possible element domain 402 F Pirri Artiﬁcial Intelligence 175 2011 378415 sort object The limited example limited vocabulary impossible deﬁne axioms unobserved ﬂuents For example closing door observed axioms mentioning action close Openeddoor Opendoor cid13 Closedoor Openeddoors Thus axioms naive Another important limitation order construct theory maximality densities associated ﬁeld terms observation actions required These requirements far restricted simple cases lack clear condition effect actions Indeed observed precondition action effects stated maximality requirement However determining maximality diﬃcult expensive task 7 Comparisons conclusions We proposed completely new method perception discovers genuine theory actions Situation Calculus We wanted design logical robotchild requires different steps modelling starting pure visual path The transformation requires building concept observation action Then induction process build computational model observations actions states ﬁnally action theory reasoning The transformations shown start trials concerning observed events phenomena HMM model M observed events Then parametric probability model M ﬁnally probability model ﬁrstorder model M These inductive steps underlying process learning reality appear ance 78 At beginning robotchild observe sequence phenomena learns measures realworld features forming perception actions It learns observe constructing motion space actions modelled computational structure example HMM model M The terms computational structure volatile strength explain events observed phenomena Thus step state properties relations terms according given signature These structure embedding parameters relations assessing connections relations probability model M These properties enable robotchild hypotheses formed seen At point able induce regularities hidden reality appearance The robotchild observed events affected uncertainty events subject randomness canonical model proves speciﬁc sample observations independently trials performed Therefore hazardous induce general laws hypotheses This fact achieved step induction obtaining theory actions Still crucial problem effectively inducing general laws remains open provided proof theory parametric probability model Actually induction process needs vocabulary signature denoting observations obtain faced It interesting ﬁnd combine visual audio learning connote observation word following suitable transformation obtain theory actions signature learned Another limitation object learned Nevertheless completely explicated details steps passages generate theory actions visual observations We hinted crucial passage developed However shown respect McCarthy argument learning reality appearance possible directly perception The novelty approach resides aspects First transformation methodology faced mainly communities vision knowledge representation learning integrated Further interpretation recognition gestures active research area approach based building multidimensional descriptor training continuous observations PPCAHMM build action space new Finally construction probability ﬁrstorder model visual processes embedding parameters knowledge new Despite novelty ﬁrst research attempt generate knowledge learning observations certainly ﬁrst directly perception builds formal theory actions The concept observation vague contexts In approaches observation connected perception structured representation code example 43117 similar approaches learning burden problem building data On hand robot learning sensing vision wide research area robot actions learned assessing improving robot performances A particular proliﬁc research area connected robot learning learning imitation example works 73295827181912 Other approaches favoured discovery planning aspects inform actions stressing biological inspiration These example approaches Cohen colleagues 20108109 Although approaches introduce clear connection discovery learning effect actions attempt generate theory allow robot reason learned instead purpose approach A survey robot learning facing current state art recently provided 5 F Pirri Artiﬁcial Intelligence 175 2011 378415 403 A quick comparison research approaches action learning perception Section 2 methodology approach discussed Appendix A Apart works probabilistic logic cited Section 5 works studied stochastic processes problem verifying sentence process satisﬁed logical language These approaches mainly based temporal propositional logic monadic ﬁrstorder logic beginning works Vardi 115 From 115 huge literature germinated probabilistic model checking automatic veriﬁcation processes starting CTL logics DTMC MDPs PCTL Model checking algorithms developed discrete time Markov chains work Alur 2 Hansson Jonsson 51 Markov decision processes works 142328 These examples vast literature probabilistic model checking essentially based propositional logic Further Beauquier Rabinovich Slissenko studied model checking speciﬁc class ﬁrstorder formulae representing liveness properties Markov chains 11 work inspired work Halpern probabilistic logic 36 4937 possible worlds approach Hidden Markov models studied connection logic programming example language SLP Stochastic Logic Programs Muggleton 11883 Cussens 24 PRISM Programming statistical logic Sato Kameya 107 These approaches attracted statistical learning computation HMM particular case representation problem concerned probabilistic logic On hand BLOG 81 offers representation paradigm HMM computed embed parameters ﬁrstorder formulae offers interesting methods sampling interpretations Indeed Poole notes 91 probability speciﬁes semantic construction representation knowledge Poole brieﬂy discusses possible representation HMM language ICL Independent Choice Logic 90 special case Dynamic Belief Networks A discussion close relationship existing frameworks combining probabilities logic presented Cussens 26 On hand number formalisms devised combining probabilistic reasoning logical represen tation machine learning This integration nowadays strong research area 99 De Raedt Kersting Venn diagram intersecting probability learning logic kernel probabilistic logic learning provide overview different formalisms developed recent years The analysis concerned stochastic logic programs Our approach differs respects cited ones First shown computations valid speciﬁc HMM M equivalent obtained probability model M ﬁrmly believe computational steps parameters carried computational model Example 2 Our view supersede logic easily achieved computational learning models build induction process A limitation approach considered construction canonical model HMM Furthermore assume ﬁnite property HMM models More understanding needed prove properties concerning parameters embedding lifting ﬁrstorder logic The implemented transformation theory actions The visual process implemented original script segmentation motion detection optical ﬂow realised Matlab This implementation creates matrix 17dimensional vectors standing processed frame The implementation PPCAHMM Matlab extends wellknown implementations mixture probabilistic PCA reestimation parameters PPCAHMM discussed Appendix A4 To end widely basic work implementation provided Kevin Murphy 84 primitives mixtures Gaussian provided Netlab 85 The probability model ﬁrstorder implemented embedding Prolog Matlab explained Example 2 To conclude mechanisation process learning reality appearance lack automatic introduction signature require integration voice image tagging Acknowledgements I like express gratitude recall number people First I like thank organisers Leora Sheila Vladimir support contributors issue Vladimir actually ﬁrst person introduce Situation Calculus 20 years ago I like thank PhD student Mario Gianni helped implement interface PrologMatlab Finally I like recall Ray Reiter ongoing inﬂuence formal AI community Situation Calculus love John This special issue incomplete contribution Appendix A In Appendix A1 introduce preliminaries probability space measure space In Appendix A2 introduce basic concepts Markov chains Further Appendix A3 introduce details concerning optical ﬂow problem Section 2 features descriptors In Appendix A4 introduce mixture principal component analysers 113114 model action space descriptors extension reestimation procedure mixtures PPCA Hidden Markov Models continuous observation densities In Appendix A5 Appendix A6 report proofs Sections 5 6 404 F Pirri Artiﬁcial Intelligence 175 2011 378415 A1 Probability space A measurable space pair Ω F Ω set F σ ﬁeld subsets Ω The collection F subsets Ω σ ﬁeld set F A1 A2 elements F union F F closed operation taking countable unions A F complement A A F A probability space triple Ω F P st Ω sample space event ﬁeld set F Ω certain event If P measure domain P σ ﬁeld sets G F P G deﬁned Hence P probability measure Ω F P F cid22 0 1 satisﬁes conditions P 0 P Ω 1 cid25 cid26 P Ai cid21 i1 cid11 i1 P Ai cid13 j Ai A j function X Ω Θ Given probability space Ω F P measurable space Θ B random variable X Ω F measurable F F B cid13 cid13 Xω F That inverse image X measured events events original σ algebra The probability measure induced X 1F ω Ω X F cid14 cid12 cid2cid12 cid13 cid13 Xω B cid14cid3 ω P X B Θ distribution function F X cid12 cid14 cid13 cid13 Xx cid3 z x F X z A2 Markov chains We recall chain discrete time stochastic process deﬁned family random variables Xttcid20 indexed discrete time T discrete timeindex set That chain function taking arguments time t T states sample space S X j X j indicates random variable varying states time values X j states X j si indicates random variable time j takes value state si S Given probability space process Xiicid20 discrete time Markov chain P Xk1 s X0 s j0 Xk s jk P Xk1 s Xk s jk 32 Property 32 Markov property A Markov chain homogeneous transition depend time step That P Xi1 sm Xi sk P X1 sm X0 sk 33 We shall consider discrete time homogeneous Markov chains A model M Markov chain Xiicid20 identiﬁed parameters π P 1 π stochastic vector sums 1 entry nonnegative deﬁning initial distribution states 2 P state transition matrix stochastic matrix row stochastic vector deﬁning probability states states All entries P kind P Xk s j Xk1 si si s j S abbreviated pi j Since assume set states ﬁnite P dimension n n S n At time 1 probability state determined initial distribution π The matrix Pn nstep transition matrix The distribution Markov chain π P time n given stochastic vector pn pn1P π Pn For purpose shall deal transition matrices irreducible states communicate parti tioned set absorbing states persistent P Xn si X0 si 1 set nonpersistent states state q nonpersistent transient starting q probability returning 1 path nonpersistent state persistent A vector p called stationary distribution chain p stochastic vector pP p The distribution said stationary pPn pPn1 p When P irreducible states nonnull persistent stationary distribution exists unique Let P irreducible Tr transient states As absorbing states Lemma 4 Let P ﬁnite states As accessible nonabsorbing states absorption states certain F Pirri Artiﬁcial Intelligence 175 2011 378415 405 Lemma 5 Let P ﬁnite transition matrix partitioned transient states Tr absorbing states As absorbing states accessible transient state exists path absorbing state Then exists n Pn Pnk k 0 cid24 cid23 Proof Let Pm mstep matrix absorption absorbing states holds previous lemma Then Pm expressed Q Here I identity matrix absorbing states st qii 1 elements R step probabilities nonabsorbing absorbing states O matrix zeroes On hands P I O expressed P elements Q step probabilities nonabsorbing states It easy R Q QP Q cid2 I O R O cid24 cid23 Lemma 6 Let P stochastic matrix m m let Pnk Pn k 0 1 row ri Pnk stochastic vector stationary distribution Pn P Proof Let Pn Q clearly Q Q Q Pnn Pn let ri ith row Q 2 ri qi1 qim cid27 mcid11 k1 qikqk1 mcid11 k1 cid28 qikqkm ri Q Since Q stochastic ri stochastic stationary distribution Pn ri riPnk pnkP k ri stationary distribution chain cid2 A3 Optical ﬂow The optical ﬂow problem amounts compute displacement ﬁeld images image intensity variation Here assumed 2D velocity projection image plane spacetime path 3D point The optical ﬂow vector w u v 1 computed successive images image sequence natural constraint based principle brightness constancy That point pixel moving object change intensity frames Ix y t Ix u y v t 1 optical ﬂow constraint I xu I y v It 0 34 Here I z image partial derivative respect z We single constraint unknown u v ﬂow vector w Usually local methods 72 introduce square estimation small window nearby pixels overcome problem However brightness patterns independently motion deformable object like hand hard recover velocities Global methods combine optical ﬂow constraint global smoothness term These methods based minimisation energy functional Eu v 53 cid2 I xu I y v It2 α cid2 u2 v2 cid3cid3 dx dy smoothness term α cid2 u2 v2 cid3 35 cid20 Eu v R α regularisation parameter This functional minimised solving corresponding EulerLagrange PDE equations y u I x I y v I y It 17 reﬂecting boundary conditions 0 cid8u 1αI 2 Recently Brox 167 extended mentioned global approach applying multiresolution strategy image sequence obtaining better approximation optimum energy functional Eu v E data Esmooth x u I x I y v I x It 0 cid8v 1αI 2 cid20 EDatau v cid13 cid2cid13 cid132 γ cid13Ix w Ix cid13 cid13 cid13 Ix w Ix cid132 cid3 Ψ dx 36 R w u v 1cid9 Here x x y 1cid9 Ψ concave function γ weight E smoothu v derived Esmoothu v integrates sum function taking care outliers spatiotemporal image gradient Eu v min imised numerical approximation A4 Probabilistic PCA action space HMM Probabilistic PCA 113114 build mixtures principal component analysers clustering descriptors Yt action space account mixtures observations state HMM PPCA deﬁnes probability model relating sets variables Ddimensional vector observations ρdimensional vector unobserved variables 7 See httpperceptioninrialpesfrcharimywebSoftwareﬂow_documentationpdf 406 F Pirri Artiﬁcial Intelligence 175 2011 378415 Given Gaussian noise model N 0 σ 2I cid13 Gaussian prior model N z0 Iρ latent variable z marginal distribution Y Gaussian More precisely 113104 let B Acid9A σ 2Iρ cid2 P zY N P Yz N cid2 pY N cid2 zB1Acid9 YAz μ σ 2ID Yμ AAcid9 σ 2ID Y μ σ 2B1 cid3 cid3 cid3 37 Here Iρ identity ρ ρ matrix The maximumlikelihood estimation D ρ matrix A relating latent variables observations 113 AM L Uρ cid2 Λρ σ 2Iρ cid3 12Iρ 38 Here Uρ D ρ vector formed eigenvectors corresponding greatest eigenvalue λ1 λρ sample variance Σ data Λρ ρ ρ diagonal matrix eigenvalues Σ σ average variance discarded dimension The ML estimation residual variance 113 σ 2 M L 1 D q Dcid11 jq1 λ j 39 The mixture PPCA deﬁned model Ak ck μk σ 2 distribution observation Yt form k k 1 N N number clusters f Yt cid2 ckN Mcid11 k1 Ytμk AkAcid9 k σ 2 k I cid3 t 1 T 40 cid16 cid16 Here ck mixture proportion The parameters mixture PPCA approximated reestimation procedure given 104113114 EM approach taken maximise loglikelihood completedata LC k1 wtk lnck pYt ztk reestimation steps follows 114 Let Y1 Yt observation sequence zt latent variable z time t tth row let Bk Acid9 Iρ Let N ﬁxed N 5 The estimation step accounts computation expectation ELC expectation latent variables Ak σ 2 k T t1 M k Acid9 Eztk B1 k cid23 cid9 σ 2 E ztkz k tk cid24 k Yt μk B1 k EztkEztkcid9 41 In maximisation step ELC maximised respect ck μk Ak σ 2 kth mixture generate Yt Here k 1 N according initial clustering descriptors deﬁned Section 3 k posterior responsibility γtk pkYt 42 This obtained initially clustering smoothed functions sample mean variance obtained cluster Then Ak σ 2 k estimated follows 114 cid16 ˆAk T t1 γtkYt ˆμkEztkcid9 cid16 cid9 t1 γtkEztkz tk cid29 T ˆσ 2 k cid16 1 T t1 γtk D Tcid11 t1 γtkcid7Yt ˆμkcid72 2 Tcid11 t1 γtkEztkcid9 ˆAcid9 k Yt ˆμk γtktr cid2 cid23 E ztkz cid9 tk cid3 cid24 ˆAcid9 k ˆAk cid30 43 Tcid11 t1 Here ˆX new estimated variable D space dimension While ˆμk ˆck obtained classical EM 1030 For visualization projection data Yt mean posterior distribution latent variables A41 PPCAHMM An HMM continuous observation densities suitable dynamic model actions states requiring estimate transition matrix P states distribution π initial states mixture parameters Ψ modelling local evolution actions respect observed sequences interactions In HMM observations estimated respect state observed N mixtures given N states So observation mixtures f described previous section observation mixture state shall specify Now PPCCHMM differently M mixture components sequence length T F Pirri Artiﬁcial Intelligence 175 2011 378415 407 observations Yt Then probability Yt deﬁned according discussed PPCA model Here mixture state j 1 N μ jk c jk A jk σ 2 jk b jYt cid2 c jkN Mcid11 k1 Yμ jk A jkAcid9 jk σ 2 jk I cid3 t 1 T j 1 N 44 Here A jk μ jk σ 2 jk speciﬁed particular Eqs 39 38 The reestimation procedure model parameters π Ψ P HMM Gaussian observation densities provided 987159 based EM To adapt reestimation procedure mixture PPCA HMMPPCA states added So begin posterior responsibility γt j k This deﬁned sequence observations Y1 Yt pk j Y1 YT forward backward variables αt j βt j 69 follows γt j k cid16 cid5 cid7cid5 αt jβt j N j1 αt jβt j cid16 c jkN Ytμ jk A jkAcid9 k1 c jkN Ytμ jk A jkAcid9 σ 2 I jk σ 2 jk M jk jk cid7 I 45 Analogously deﬁnitions A jk σ 2 chain parameters deﬁned follows jk given 43 likewise μ jk c jk restated γt j k Finally ˆπ j Mcid11 k1 γ1 j k ˆPi j cid16 T 1 t1 T 1 t1 αt jPi jb jYt1βt1 j cid16 N j1 αt jPi jb jYt1βt1 j1 cid16 46 A5 Proofs Section 5 Firstorder parametric models Lemma 1 Given HMM M S H P π Ψ γ ﬁnite property Deﬁnition 2 exists probability structure M ﬁrstorder domain D S H W R probability space Φ generated D atoms terms interpreted according Mparameters P π Ψ Furthermore according given interpretation I ﬁeldbased atom φ corresponding measurable set ﬁeld term φ intended distribution HMM Proof Let M structure domain formed sorts H S W probability space Deﬁnition 6 Where probability space mentions σ ﬁelds cid8 Θ Θ cid8 probability measures ηs ηI An assignment v domain element d appropriate sort free variables indicated vx d vxd maps free variables respective domains D vxs maps variable x domain S states vwt X1ω Xtω maps variable denoting sequence states sequence s j1 s jt chosen random variables ω ω S T v yY h maps variable denoting observation domain H intended reals R 1 The parameters states By HMM model M following sets parameters available M π P cik 1 N j 1 M Ψ μi j σi j Ai ji1N j1M 3 The parameters observations state 4 The parameters observations independently states γ μ j σ j A j j1N 2 The parameters linking states observations 47 These interpreted ﬁeld terms R To simplify shall consider parameters σik covariances normal pdfs Let ﬁrst consider interpretation atoms R ﬁeld terms We measurable sets probability speciﬁed HMM M Let T P stationary distribution P transition matrix M let T T P By construction cid8T cid80 cid80 cid8t 0 consists subsets St S cid80 μ0 measure space cid80 σ ﬁeld subsets S Let Q t cid8t 0 X1 Xt sequence discrete random variables Markov property X1ω arg max π s X2ω s j2 Xt1ω s jt1 Xtω si ω S T Q t 48 Q t cid13 cid13 cid12 cid14 cid2 cid3 s Then Q 0 Hence deﬁnition R iI t cylinder base Q t measurable Q t cid8t cid14 π s Xt ω si t cid2 R 1 N I cid21 cid12 ω 49 cid13 cid13 X1ω arg max cid31 s t follows R iI s s1 ﬁrst state Let s j1 s j2 s jt1 si sequence M 8 P s j1 s jt cid19 cid8 R iI W measurable set Now let assume arg maxs π s 1 s j S π s j1 t Q t cid16 T 1 s jk p s1 s j1 Thus let ω S T interpretation R 48 k1 P s jk1 I ω R Q t X1ω s1 X2ω s j2 Xt1ω s jt1 Xtω si iff ω Q t iff cid3 cid2 t cid2 50 408 F Pirri Artiﬁcial Intelligence 175 2011 378415 Hence v assignment wt free variable sort sequence t cid2 cid23 cid24I v ηs R iwt cid2cid12cid2 cid3cid14cid3 iff X1ω s1 X2ω s j2 Xt1ω s jt1 Xtω si I iff ω R cid2 v ηs cid3 wts1 s j2 s jt1 si cid2cid12cid2 Q t iff X1ω s1 Xt1ω s jt1 Xtω si cid11 T 1cid18 π s j1 P s jk1 s j S k1 s jk p cid3cid14cid3 51 Which follows fact Xi deﬁned probability space Markov property ηs product measure Therefore ηs probability measure distribution discrete random variables Markov property HMM M domain space S parameters π P Consider atom A j si ﬁxed state si domain H σ ﬁeld Θ The elements H reals consider random variables Y j H cid22 H identity For state si S number predicates A j state labelling observed actions taking argument real number H real number actually vector value descriptor action Now let Y1 Yt sequence observations Y j arrays real variables We assume instead vectors single real variables h HMM M likelihood sequence h1 ht given ﬁxed sequence states s j1 s jt according Eq 9 P h1 hts j1 s jt tcid18 i1 b ji hi 52 In particular consider single observation given sequence s j1 s jt likelihood observation given sequence states P hts j1 s jt P hts jt b ji ht independence observation states b ji hi model M deﬁned 7 On hand deﬁnition interpretation A j z R cid4 A j si I h cid13 cid13 cid13 cid13 cid3 z h Acid5 j k 1 M j 1 N cid22 h μik2 σ 2 ik 53 54 j depends initial mixture parameters speciﬁed γ Clearly h A j siI Where h Acid5 choice z cid2 0 Then A j si H A j si Θ Therefore let h B j A j siI Eqs 14 20 depends Given Y jh h cid23 A j y si cid24I v ηI cid2 cid2cid12 h cid13 cid13 v cid3 yY jh A j si I 1 N cid14cid3 cid20 bih dh B j 55 If B j h h δh bihδh δh 0 probability density speciﬁed 53 Then follows derivative ηI h denote A jh siI required density bih M M In particular δ interval stochastic variable h chosen follows value density exactly models distribution induced random variables agree param It follows measurable set A eters Ψ HMM The interpretation binary predicates O j given follows M v cid25 O j y x wt1 cid2 cid2 v x wt si X1ω Xtω iff v yh A I si ω R cid2 cid3 vxsi I 56 I cid3cid3 Hence O j Θ cid8 measurable set Finally given assignment v free variables different sorts v yh vxsi vwω distribution O ji deﬁned follows F Pirri Artiﬁcial Intelligence 175 2011 378415 409 cid23 O ji y x wt cid24Mv cid23 cid23 A j y x R ix wt cid24Mvcid23 A j y x cid23 bih R isi wt cid24Mv R ix wt cid24Mv cid25 cid11 t1cid18 bihπ X1 sq1 cid24Mv PXk1 sqk1 cid26 Xk sqk Psi Xt1 sqt1 cid2 57 k1 Theorem 1 Let M probability structure ﬁrstorder probability space Φ Let Φ extended probability space Φcid5 I ηs ηI n n cid2 1 space product For ﬁeldbase formula ϕ exists measurable set product measures ηn distribution agree HMM s ηn Proof First note ηn I conditions Fubini theorem satisﬁed ηI probability measure Thus integral products considered s like ηs ηs product measure Thus concerned ηn Let cid8α Θα Γα measurable sets atom α according Lemma 1 We prove statement induction structure ﬁeldbase formulae dimension product space For basic case If ϕw R iw ϕ y x A j y si ϕ y x w O ji y x w cid8ϕω Θϕh s Γϕh s ω statement veriﬁed Lemma 1 If ϕw αw statement follows cid8 σ ﬁeld cid8c Analogously ϕ y α y statement follows fact Θ σ ﬁeld Let n 2 αω cid8 ηs 1 Case Let ϕw w cid10 αw βw cid10 Let vw X1ω Xnω vw Xnω subsequence X1ωcid10 Xmωcid10 independent By induction hypothesis measurable sets cid8αω cid8β ωcid10 If cid8αω cid8β ωcid10 cid8β cid8ϕ cid8 belong cid8 ii Otherwise cid8αω cid8β ωcid10 cid8ϕ cid8 belong cid8 Hence ηs cid10 X1ωcid10 Xmωcid10 Then X1ω 2 Case b Let ϕ y y cid10 α y β y cid10 Let v yh v y cid10h cid10 Now E Θ 2 B hh h cid10 E Θ B h η2 I cid10h h ηI ηI particular η2 I B B 3 Case c If ϕcid16z α y w β y cid10 E Θ By induction hypothesis Θαh Θ Θβ h Θ E Θ 2 Θϕh h cid10 dy dy cid10 cid10 ωcid10 Γαh ω Θ cid8 cid10 according v Γαh ω Γβ h cid10 ωcid10 Θ cid8 argument case b obtain Γϕcid16z Θ 2 cid8 measures BBcid10 b y y cid10 cid10 w cid10b y cid10 cid10 Θ 2 Γβ h η2 I ηs accordingly 4 Case d If ϕ wα induction hypothesis set Γα measurable product space Since α free variables probability space space space product probability measure value 1 0 For n 2 proof going cases ad considering product measures affected permutations formulae cid2 Proposition 1 Let v v cid10 agree assignments free variables M v cid25 ϕ iff M v cid10 cid25 ϕ Proof By induction structure ϕ considering cases ϕ mentions ﬁeld terms cid2 Theorem 2 Let M S H π P Ψ γ exists probability model M probability space Φcid5 extending Φ M M agree distribution domain Proof By Lemma 1 given HMM M probability model M ﬁrstorder probability space Φ atomic ﬁeld terms distribution terms HMM If probability space extended Φcid5 obtained product measures unique deﬁned products probability measures We shall consider cases joint terms HMM 1 Let O y1 ym sequence observations The probability O M given π si1 bi1 y1Psi2 si1 bim ymPsim sim1 58 cid11 si1 sim S 410 F Pirri Artiﬁcial Intelligence 175 2011 378415 Thus term requires represent sequences states length m To represent sequences need deﬁne suitable set variables First let Q m formed ω subsequence length vw m state si Let U m provides variable sort sequence element Q m ωm Xmω si Q m m X1ω Xmω X1ω Xmω Q m U m w m ϕ N cid2 O ji y j w m cid3 59 j1 i1N U m w m Let θ cid5 y1 ym ωcid5 cid2 cid2 cid3 ϕ ηm I ηs cid11 Γϕ cid31 Q m cid3 θ cid5 ωcid5 π s j1 b j1 y1Ps j2 s j1 b jm ymPs jm s jm1 s j1 s jm ωcid5 2 Let p stationary distribution Markov chain M pPt pP p pt cid13 0 Consider tk denoting sequence deﬁned previous item sequence w U t variable w X1ω Xt1ω ending state si cid23 t w t cid3cid24 cid3cid24 cid2 cid2 cid23 R w t R w tk pt π s j1 Pt 60 By interpretation R 49 3 Let P h1 hm s1 sm M joint probability sequence observations sequence states 11 Consider formula form ψ y1 ym x1 xm wm1 ϕw 1 wm Since predicate mentioning observations states O ji O ji y x w R ix w A j y x formula reduced normal form m i1 A j yi xi m i1 R ixi wm1 Then joint probability observations given states ηs cid24 ψ y1 ym x1 xm ϕw 1 wm cid2 cid23 ηm I Theorem 1 23 Furthermore cid2 M v cid25 cid2 M v cid25 cid3 R iw R jw cid3 O y w O j y w j cid13 j j cid13 j cid3 ΓψϕΓϕ 61 62 Simply deﬁnition interpretation given Lemma 1 Hence M required model cid2 A6 Proofs Section 6 Final induction step concluding Situation Calculus We recall basic theory actions formed set sentences Σ DS0 Duna Dap Dss 63 Where Σ set foundational axioms described DS0 set formulae mention situation terms mention situation term S0 Duna set unique action axioms action Ai A j Ai cid13 A j Dap axioms specifying preconditions execute action Posspushdoor s Unlockeddoor 64 Finally Dss axioms specifying effect action comes true execution action example pushdoor cid13 closedoor Openeddoors 65 Openeddoor cid3 cid2 doa s The set foundational axioms Σ cid10 cid3 s cid3 S0 cid2 s s s cid3 s cid3 cid2 cid10 doa s cid2 P P S0 asP s P s cid10 cid10 cid10 cid10 s s cid3 doa s F Pirri Artiﬁcial Intelligence 175 2011 378415 411 s P s 66 In following M refers probability structure M refers structure Situation Calculus Thus context M A j predicate taking values domain observations domain states language Situation Calculus A j refers action signature deﬁned Deﬁnition 8 We shall assume σ 2 ik variance kth component ith univariate mixture mixtures modes components Furthermore shall assume mixture b j j 1 N component variance σ 2 jk τ τ suitable threshold 001 Lemma 2 For t exists likely state R ix wt M v cid25 R ix wt exists A j A j y xMv maximal Furthermore A j y xMv maximal precisely vxs s interpretation R independently assignment v wt δ δ M cid16 k1 γti k For t 1 R isi likely state R isi cid2 R js j Proof By reestimation formulae arg max1cid3icid3N M cid25 R isi By Lemma 1 reestimation formula πi likely state bi y1 maximal Choose bi j μi j arg maxv y1μi j B bi y1 Therefore v y1μi j A j y1 siMv likely state speciﬁed si j cid13 clearly k1 γ1i k α1i ˆπibi y1 Hence R isi ik component argument maximising bi maximal si R isi k cikN μik σ 2 HMM M likely state time t cid16 cid16 M δ Let t 1 By reestimation formula γti k 1 N γti k 1 λ αtiβti cid2 cikN 1 bi yt cid3 yt μik σ 2 ik Here λ normalisation factor We ﬁx βti 1 state αti αt1qPqi bi yt cid26 cid25 Ncid11 q1 Replacing αti deﬁnition simplifying summing k obtain cid26 cid25 cid26 cid25 Mcid11 k1 γti k 1 λ Ncid11 αt1qPqi q1 k1 Mcid11 cid2 cikN yt μik σ 2 ik Ncid11 cid3 1 λ αt1Pqi bi yt q1 67 68 69 N cid16 k cikN yt μik σ 2 By reestimation formula transition matrix coeﬃcients 46 Pqi depends bi yt term q1 αt1Pqi maximal bi yt maximal 1 N Then obtain likely action choose bi 1 λ cid16 ik component argument maximising bi j μi j arg maxv yt μi j B bi yt k1 γt k Hence R isi wt depend wt Therefore v ytμi j A j yt siMv likely state A j yt si likely action Clearly maximality A j yt siMv M cid25 R isi wt cid2 maximal si si arg max1cid3icid3N cid16 M δ δ Lemma 3 For likely state R ux wt M v cid25 R ux wt exists observation action A j y x unique transition P Xt2 x cid10 Xt1 x extends R ux wt R ix cid10 x wt maximally That A j y xMv δ P Xt2ω x cid10 Xt1ω x maximal vxsu Proof If M v cid25 R ux wt vxsu Lemma 2 given su exists A j maximal su Now property transition matrix exists sk P Xt2 sk Xt1 su 0 choose maximal transition statement holds k cid2 Corollary 1 For state predicate R ix x unique transition P Xt2 x Xt1 x cid10 A j y x cid10δ Am y xδMv maximal vxsi cid10 x cid10 wt M v cid25 R ix x cid10 extends R ux cid10 wt R ix x cid10 wt exist observation actions Am A j cid10 wt maximally That P Xt2 x Xt1 Proof Let M v cid25 R ix x transition P Xt2 si Xt1 su su M v cid25 R ux cid10 wt v map Xt2 si Now argument Lemma 3 exists cid10 wt v mapping Xt1 su Choose observation 412 F Pirri Artiﬁcial Intelligence 175 2011 378415 action Am pdf maximal si observation action A j pdf maximal su v yh v y Lemma 2 Hence Lemma 3 A j y suδP Xt1 si Xt1 suMv extends R ux cid10 wt maximally R ix x pdf Am maximal si statement follows cid2 cid10h cid10 cid10 wt Theorem 3 Given M assignment v vwt ˆwt ˆwt X1ω s j1 Xtω xs jt sequence x x cid10 ˆwtMv persistent cid10 ˆwt R ix x t cid2 0 R exists Proof Note construction transition matrix irreducible transient states connected absorbing state probability state happens wt greater zero states By Lemma 2 Deﬁnition 10 state R ix wt happens ˆwt t k M v cid25 R jx wt R ix wtMv maximal vxsi Furthermore Lemma 2 action A j y x A j y xδ maximal vxsi In fact proof Lemma 2 shown R maximal action A j maximal si R reestimation formulae I Let assume R ix x cid10 wtMv t cid2 0 maximal M v cid25 R ix x cid10 wt We deﬁne following construction given choice R cid23 cid24 A j y si δ Ut2 si Tt2 arg max v yh j A cid2 cid12 si x Mt2 R Ut1 arg I j cid10 wt max 1cid3ucid3Nvxcid10su cid3 cid23 cid14 A jTt2 si cid2 cid10 ˆwt x cid3 R u cid2 cid3cid24 Am cid10 y Ut1 cid23 Tt1 arg max v ycid10hm A cid12 cid24 δ cid3 I m cid2 Ut1 x R u Mt1 Mt2 U 1 arg max 1cid3qcid3NvxsqS cid2 cid23 cid2cid23 cid2 Rq y U 1 xcid5 cid3 X2 U 2 X1 xcid5 cid3cid3 cid3cid24 cid2 P cid24 δ T 1 arg max v yhk A cid12 M1 M2 Ak I k RqU 1 AmT 1 U 1 v cid3cid14 cid2 xcid5U 1 PXt2 si Xt1 Ut1 cid10cid10 wt1 AmTt1 Ut1 v cid2 cid10 x Ut1 cid3cid14 Let v deﬁned M1 easy construction Lemma 2 M v cid25 Mcid5 Mcid5 set atoms M1 Furthermore n n 1 t 2 v ynh j Tn vxnsu Un A jh j su maximal R usu wn1 Now let consider 70 cid2 cid2 cid3 A j1 h j1 si1 A jt2 h jt2 sit2 cid3 R i1 si1 R it2 sit2 By construction ﬁeld term atoms sequences maximal We induction t given construction R 1 n persistent according Deﬁnition 10 We item 1 2 term introduced Eq 28 maximal For basic case item 1 2 straightforward R entering wt t 1 Thus 28 reduces maximality R iU 1 Let t 1 Again 1 obviously veriﬁed construction inductive hypothesis Hence R iU t2 U t1 wt A jTt2 U t2 AmTt1 U t1 R uU t1 wt maximal R iU t2 U t1 wt A jTt2 U t2 maximal Consider 2 Suppose t 2 change In M1 actions AqTt1 U t1 AkTt2 U t2 terms maximal U t1 U t2 respectively Because M1 satisﬁed M Since hypothesis change M v cid25 AqTt1 Ut1 AkTt2 Ut2 R iUt1 wt R iUt2 Ut1 wt We need delay possible If U t1 absorbing state change occur transient null deﬁnition transition matrix Pii 0 We suppose delay affect observation AqTt1 U t1δ maximal R iU t1 wt maximal R iU t1 U t1 wt From fact fact AkTt2 U t2δ maximal state U t2 cid13 U t1 follows 1 AkT 1 U 1δ 0 We note Corollary 1 AqTt1 U t1δP Xt2 U t1 Xt1 U t1 AqTt1 U t1δ extends maximally R iU t1 wt R iU t1 U t1 wt possible delay change M v cid25 AkTt2 Ut2 R iUt1 wt R iUt1 Ut1 wt F Pirri Artiﬁcial Intelligence 175 2011 378415 413 The delay like having added t 1 t k k 2 U t1 Tt1 Mt1 ktimes R iU t1 wt R iU t1 U t1 wt M1 maximal But Aq right choice y A j item 1 renaming states R isi U t1 wt A jTt2 si R isi si wt AqTt2 si considering normalisation factors cid23 cid24 R isi si wt A jTt2 si cid24 R isi Ut1 wt AkTt2 si maximal That choosing Ak like having chosen maximal action let R remaining wt cid2 cid23 Theorem 4 Let M ﬁrstorder parametric probability structure deﬁned Deﬁnition 6 v assignment free variables Let vwt X1ω Xtω x let R ix wt predicate state M v cid25 R ix wt ω R I 1 There action A j y x maximal vxs s interpretation R Hence action precondition A j uniquely determined R 2 There exists action A j possibly Am ensuring persistence satisﬁability R Hence successor state axiom R uniquely determined maximal values R ix x cid10 wt A j y x R ix x wt Am y cid10 x Proof 1 follows Lemma 2 Eq 29 2 follows Theorem 3 Eq 29 More precisely Theorem 3 given ﬁxed wt t cid3 Tp R ix wtMv likely state t 1 Then 1 By Lemma 2 t cid2 0 choose likely state R ix wt action A j y x maximal state add action precondition axioms possibly free variables z z cid3 cid3 cid2 cid2 sPoss A jz s R cid10 z s cid10 sort object 2 By Theorem 3 item time t cid2 0 choose likely state action A j y x maximal state according construction speciﬁed Theorem 3 Further R correspond absorbing state action Am according Theorem 3 induces change R exits wt Then cid10 x maximal Then Am action add successor state cid10 wt A j y x R ix x wt Am y R ix x cid10cid10 axioms possibly free variables z z z sort object cid2 cid10cid10 cid3 cid3 cid2 cid2 cid10cid10 cid10 cid10 A j z z Am z cid13 R iz s cid2 sR cid3 z doa s References 1 Martin Abadi Joseph Y Halpern Decidability expressiveness ﬁrstorder logics probability IEEE Symposium Foundations Computer Science FOCS89 1989 pp 148153 2 Rajeev Alur Costas Courcoubetis David L Dill Modelchecking probabilistic realtime systems extended abstract Automata Languages Programming 18th International Colloquium ICALP91 Proceedings 1991 pp 115126 3 Eyal Amir Allen Chang Learning partially observable deterministic action models J Artif Intell Res JAIR 33 2008 349402 4 Corin R Anderson Pedro Domingos Daniel S Weld Relational Markov models application adaptive web navigation KDD 2002 pp 143152 5 Brenna D Argall Sonia Chernova Manuela Veloso Brett Browning A survey robot learning demonstration Robot Auton Syst 57 5 2009 469483 6 Fahiem Bacchus Lp A logic statistical information M Henrion RD Shachter LN Kanal JF Lemmer Eds Uncertainty Artiﬁcial Intelli gence vol 5 NorthHolland Amsterdam 1990 pp 314 7 Fahiem Bacchus Representing Reasoning Probabilistic Knowledge A Logical Approach Probabilities MIT Press Cambridge MA USA 1990 8 Fahiem Bacchus Joseph Y Halpern Hector J Levesque Reasoning noisy sensors effectors Situation Calculus Reasoning Uncertainty Robotics 1995 pp 218220 9 Fahiem Bacchus Joseph Y Halpern Hector J Levesque Reasoning noisy sensors effectors Situation Calculus Artiﬁcial Intelli gence 111 12 1999 171208 10 Leonard E Baum Ted Petrie George Soules Norman Weiss A maximization technique occurring statistical analysis probabilistic functions Markov chains Ann Math Statist 41 1970 164171 11 Danièle Beauquier Alexander Moshe Rabinovich Anatol Slissenko A logic probability decidable modelchecking CSL 2002 pp 306321 12 Anna Belardinelli Fiora Pirri Andrea Carbone Bottomup gaze shifts ﬁxations learning imitation IEEE Transactions Systems Man Cybernetics Part B 37 2 2007 256271 13 Anna Belardinelli Fiora Pirri Andrea Carbone Motion saliency maps spatiotemporal ﬁltering Proc 5th International Workshop Attention Cognitive Systems WAPCV 2008 2008 pp 717 14 Andrea Bianco Luca Alfaro Model checking probabilistic nondeterministic systems FSTTCS Foundations Software Technology Theoretical Computer Science 15th Conference Proceedings 1995 pp 499513 15 Patrick Billingsley Probability Measure Wiley 1995 16 T Brox B Rosenhahn D Cremers Contours optic ﬂow prior knowledge Cues capturing 3D human motion videos Human Motion Understanding Modeling Capture Animation Springer 2007 17 Andrés Bruhn Joachim Weickert Christoph Schnörr LucasKanade meets HornSchunck Combining local global optic ﬂow methods International Journal Computer Vision 61 3 2005 211231 18 Carlos A Acosta Calderon Husheng Hu Robotic societies Elements learning imitation Proc 21st IASTED International Conference Applied Informatics Innsbruck Austria 2003 pp 315320 19 Antonio Chella Haris Dindo Ignazio Infantino Learning highlevel tasks imitation IROS 2006 pp 36483654 20 Paul R Cohen Tim Oates Carole R Beal Niall M Adams Contentful mental states robot baby AAAIIAAI 2002 pp 126131 414 F Pirri Artiﬁcial Intelligence 175 2011 378415 21 Dorin Comaniciu Peter Meer Mean shift A robust approach feature space analysis IEEE Trans Pattern Anal Mach Intell 24 5 2002 603619 22 Dorin Comaniciu Visvanathan Ramesh Peter Meer Kernelbased object tracking IEEE Trans Pattern Anal Mach Intell 25 5 2003 564575 23 Costas Courcoubetis Mihalis Yannakakis The complexity probabilistic veriﬁcation J ACM 42 4 1995 857907 24 James Cussens Parameter estimation stochastic logic programs Machine Learning 44 3 2001 245271 25 James Cussens At interface inductive logic programming statistics ILP 2004 pp 23 26 James Cussens Integrating separating Combining probability logic ICL PRISM SLPs Technical report APRIL project report York 2005 27 Cynthia Breazeal Daphna Buchsbaum Jesse Gray David Gatenby Bruce Blumberg Learning Towards imitation bootstrap social understanding robots Artiﬁcial Life 11 12 January 2005 3162 28 Luca Alfaro Temporal logics speciﬁcation performance reliability STACS 97 14th Annual Symposium Theoretical Aspects Computer Science Proceedings 1997 pp 165176 29 Yiannis Demiris Gillian Hayes Imitation Dual Route Process Featuring Predictive Learning Components A BiologicallyPlausible Computa tional Model MIT Press 2002 30 Arthur P Dempster Nan M Laird Donald B Rubin Maximum likelihood incomplete data EM algorithm Journal Royal Statistical Society 39 1 1977 138 31 Daniel C Dennett Brainstorms Philosophical Essays Mind Psychology MIT Press 1978 32 Daniel C Dennett Consciousness Explained Little Brown Boston 1991 33 Daniel C Dennett What RoboMary knows Torin Alter Sven Walter Eds Phenomenal Concepts Phenomenal Knowledge New Essays Consciousness Physicalism 2006 34 Thomas G Dietterich Pedro Domingos Lise Getoor Stephen Muggleton Prasad Tadepalli Structured machine learning The years Machine Learning 73 1 2008 323 35 Davis Ernest Leora Morgenstern Introduction Progress formal commonsense reasoning Artiﬁcial Intelligence 153 12 2004 112 36 Ronald Fagin Joseph Y Halpern Reasoning knowledge probability Proc Second Conference Theoretical Aspects Reasoning Knowledge Asilomar CA 1988 pp 277293 37 Ronald Fagin Joseph Y Halpern Reasoning knowledge probability J ACM 41 2 1994 340367 38 Ronald Fagin Joseph Y Halpern Nimrod Megiddo A logic reasoning probabilities Information Computation 87 12 1990 78128 39 L Fogassi PF Ferrari B Gesierich S Rozzi F Chersi G Rizzolatti Parietal lobe From action organization intention understanding Science 308 2005 662667 40 Haim Gaifman Concerning measures ﬁrst order calculi Israel Journal Mathematics 2 1964 118 41 Haim Gaifman A theory higher order probabilities TARK 1986 pp 275292 Already presented NSF Irvine 1985 42 Lise Getoor Nir Friedman Daphne Koller Benjamin Taskar Learning probabilistic models relational structure ICML 2001 pp 170177 43 Yolanda Gil Learning experimentation Incremental reﬁnement incomplete planning domains ICML 1994 pp 8795 44 Nelson Goodman The Structure Appearance Harvard University Press Cambridge MA 1951 45 Alison Gopnik Andrew N Meltzoff PK Kuhl The Scientists Crib Harper Collins New York 2000 46 Alison Gopnik Andrew N Meltzoff Words Thoughts Theories MIT Press Cambridge MA 1997 47 Alison Gopnik Laura Schulz Mechanisms theoryformation young children Trends Cognitive Science 8 8 2004 371377 48 Joseph Y Halpern An analysis ﬁrstorder logics probability Artiﬁcial Intelligence 46 3 1990 311350 49 Joseph Y Halpern David A McAllester Likelihood probability knowledge Comput Intelligence 5 3 1990 151160 50 Peter Hammond Elementary nonArchimedean representations probability decision theory games Probability Probabilistic Causal ity Kluwer Academic 1994 51 Hans Hansson Bengt Jonsson A logic reasoning time reliability Formal Asp Comput 6 5 1994 512535 52 Douglas N Hoover Probability logic Annals Mathematical Logic 14 1978 287315 53 Berthold KP Horn Brian G Schunck Determining optical ﬂow Artiﬁcial Intelligence 17 13 1981 185203 54 Weiming Hu Tieniu Tan Liang Wang Steve Maybank A survey visual surveillance object motion behaviors IEEE Transactions Systems Man Cybernetics 34 2004 334352 55 Frank Jackson Peter Ludlow Yujin Nagasawa Daniel Stoljar Theres Something Mary Essays Phenomenal Consciousness Frank Jacksons Knowledge Argument MIT Press Bradford Book 2004 56 Frank C Jackson Epiphenomenal qualia Philosophical Quarterly 32 127 1982 127136 57 Frank C Jackson What Mary didnt know Journal Philosophy 83 5 1986 291295 58 Alex Pentland Tony Jebara Statistical imitative learning perceptual data 2nd International Conference Development Learning ICDL02 2002 59 BingHwang Juang Stephen E Levinson M Mohan Sondhi Maximum likelihood estimation multivariate mixture observations Markov chains IEEE Transactions Information Theory 32 2 1986 307 60 Immanuel Kant Critique Pure Reason Riga 17811787 61 H Jerome Keisler Probability quantiﬁers J Barwise S Feferman Eds ModelTheoretic Logics Springer New York 1985 pp 509556 62 H Jerome Keisler Hyperﬁnite Model Theory NorthHolland Amsterdam 1977 63 E Kohler C Keysers MA Umiltà L Fogassi V Gallese G Rizzolatti Hearing sounds understanding actions Action representation mirror neurons Science 297 2002 846848 64 Daphne Koller Structured probabilistic models Bayesian networks AAAIIAAI 1998 pp 12101211 65 V Krüger D Kragic A Ude C Geib The meaning action A review action recognition mapping Advanced Robotics 21 13 2007 1473 1501 66 M Land N Mennie J Rusted The roles vision eye movements control activities daily living Perception 28 1999 13111328 67 Hector J Levesque Knowledge action ability Situation Calculus TARK 1994 pp 14 68 Hector J Levesque What planning presence sensing AAAIIAAI vol 2 1996 pp 11391146 69 SE Levinson LR Rabiner MM Sondhi An introduction application theory probabilistic functions Markov process automatic speech recognition Bell Systems Technical Journal 62 4 1983 10351074 70 Clarence Irving Lewis Mind World Order C Scribners Sons New York 1929 71 Louis A Liporace Maximum likelihood estimation multivariate observations Markov sources IEEE Transactions Information Theory 28 5 1982 729734 72 Bruce D Lucas Takeo Kanade An iterative image registration technique application stereo vision IJCAI 1981 pp 674679 73 Maja Mataric SensoryMotor Primitives Basis Imitation Linking Perception Action Biology Robotics MIT Press 2002 74 John McCarthy Patrick J Hayes Some philosophical problems standpoint artiﬁcial intelligence B Meltzer D Michie Eds Machine Intelligence vol 4 Edinburgh Univ Press Edinburgh Scotland 1969 pp 463502 F Pirri Artiﬁcial Intelligence 175 2011 378415 415 75 John McCarthy Programs common sense Teddington Conference Mechanization Thought Processes httpwwwformalstanford edujmcmcc59html 1959 76 John McCarthy Todd Moodys zombies Journal Consciousness Studies 2 4 1995 77 John McCarthy AI papers preparation The welldesigned child 1999 78 John McCarthy Notes AI Appearance reality A challenge machine learning 1999 79 John McCarthy The welldesigned child Artiﬁcial Intelligence 172 18 2008 20032014 80 Andrew N Meltzoff W Prinz The Imitative Mind Development Evolution Brain Bases Cambridge University Press 2002 81 Brian Milch Bhaskara Marthi Stuart J Russell David Sontag Daniel L Ong Andrey Kolobov BLOG Probabilistic models unknown objects IJCAI 2005 pp 13521359 82 Thomas B Moeslund Adrian Hilton Volker Kreger A survey advances visionbased human motion capture analysis Computer Vision Image Understanding 104 23 2006 90126 83 Stephen Muggleton Learning stochastic logic programs Electron Trans Artif Intell 4 B 2000 141153 84 Kevin Murphy Hidden Markov model HMM toolbox Matlab httpwwwcsubccamurphykSoftwareHMMhmmhtml 1998 85 Ian T Nabney Netlab Algorithms Pattern Recognition Springer 2001 86 Hanna M Pasula Luke S Zettlemoyer Leslie Pack Kaelbling Learning symbolic models stochastic domains J Artif Intell Res JAIR 29 2007 309352 87 Fiora Pirri Alberto Finzi An approach perception theory actions Part I Electron Trans Artif Intell 3 C 1999 1961 88 Fiora Pirri Ray Reiter Some contributions metatheory Situation Calculus J ACM 46 3 1999 325361 89 George Polya Induction Analogy Mathematics A Guide Art Plausible Reasoning Mathematics Plausible Reasoning vol 1 Prince ton University Press 1954 eighth printing 1973 90 David Poole The independent choice logic modelling multiple agents uncertainty Artiﬁcial Intelligence Journal 94 12 1997 756 91 David Poole Logic knowledge representation Bayesian decision theory First International Conference Computational Logic 2000 Invited paper 92 Ronald Poppe Visionbased human motion analysis An overview Computer Vision Image Understanding 108 12 2007 418 93 Daniel J Povinelli Folk Physics Apes The Chimpanzees Theory How The World Works Oxford University Press 2000 94 Willard Van Orman Quine Main trends recent philosophy Two dogmas empiricism The Philosophical Review 60 1 1951 2043 95 Willard Van Orman Quine Theories Things second ed Harvard University Press 1981 96 Willard Van Orman Quine Quiddities An Intermittently Philosophical Dictionary Harvard University Press 1987 97 Willard Van Orman Quine Pursuit Truth ed Harvard University Press 1996 98 Lawrence R Rabiner A tutorial hidden Markov models selected applications speech recognition Proceedings IEEE 77 1989 257286 99 Luc De Raedt Kristian Kersting Probabilistic logic learning SIGKDD Explor Newsl 5 1 2003 3148 100 Raymond Reiter Proving properties states Situation Calculus Artiﬁcial Intelligence Journal 64 2 1993 337351 101 Raymond Reiter Knowledge Action Logical Foundations Specifying Implementing Dynamical Systems MIT Press 2001 102 Raymond Reiter On knowledgebased programming sensing Situation Calculus ACM Trans Comput Log 2 4 2001 433457 103 G Rizzolatti L Fogassi V Gallese Neurophysiological mechanism underlying understanding imitation action Nature Neurosci 2 2001 661670 104 Sam T Roweis EM algorithms PCA SPCA NIPS 1997 105 Bertrand Russell The Problems Philosophy 1912 106 Taisuke Sato A glimpse symbolicstatistical modeling prism J Intell Inf Syst 31 2 2008 161176 107 Taisuke Sato Yoshitaka Kameya Parameter learning logic programs symbolicstatistical modeling J Artif Intell Res JAIR 15 2001 391454 108 Matthew D Schmill Tim Oates Paul R Cohen Learning planning operators realworld partially observable environments AIPS 2000 pp 246 253 109 Matthew D Schmill Michael T Rosenstein Paul R Cohen Paul E Utgoff Learning relevant effects actions mobile robot Agents 1998 pp 247253 110 Dana Scott Peter Krauss Assigning probabilities logical formulas Aspects Inductive Logic NorthHolland 1966 pp 219264 111 Aaron Sloman Whats wrong priority dynamical systems hypothesis 2005 112 Aaron Sloman Two views child scientist Humean Kantian 2006 113 Michael E Tipping Christopher M Bishop Probabilistic principal component analysis Journal Royal Statistical Society Series B 61 1999 611622 114 Michael E Tipping Christopher M Bishop Mixtures probabilistic principal component analysers Neural Computation 11 2 1999 443482 115 Moshe Y Vardi Automatic veriﬁcation probabilistic concurrent ﬁnitestate programs FOCS IEEE 1985 pp 327338 116 Andrew J Viterbi Error bounds convolutional codes asymptotically optimal decoding algorithm IEEE Trans Inform Theory IT13 1967 260269 117 Wang Xuemei Learning observation practice An incremental approach planning operator acquisition ICML 1995 pp 549557 118 Hiroaki Watanabe Stephen Muggleton Learning stochastic logical automaton JSAI Workshops 2005 pp 201211