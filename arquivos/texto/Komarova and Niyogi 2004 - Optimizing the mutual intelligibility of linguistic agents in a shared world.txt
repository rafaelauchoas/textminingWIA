Artiﬁcial Intelligence 154 2004 142 wwwelseviercomlocateartint Optimizing mutual intelligibility linguistic agents shared world Natalia Komarova ab Partha Niyogi c Institute Advanced Study Einstein Drive Princeton NJ 08540 USA b Department Applied Mathematics University Leeds Leeds LS2 9JT UK c Department Computer Science University Chicago Chicago IL 60637 USA Received 4 October 2001 received revised form 11 May 2003 Abstract cid2 We consider problem linguistic agents communicate shared world We develop formal notion language set probabilistic associations form lexical syntactic meaning semantic general applicability Using notion deﬁne natural measure mutual intelligibility F L Lcid2 agents language L L We proceed investigate important questions framework 1 Given language L language Lcid2 maximizes mutual intelligibility L We ﬁnd surprisingly Lcid2 need L present algorithms approximating Lcid2 arbitrarily 2 How learn optimally communicate user language L L unknown outset learner allowed ﬁnite number linguistic interactions user L We possible algorithms calculate explicit bounds number interactions needed 3 Consider population linguistic agents learn evolve time Will community converge shared language nature language We characterize evolutionarily stable states population linguistic agents gametheoretic setting Our analysis signiﬁcance number areas natural artiﬁcial communication studies design learning evolution linguistic communication systems 2003 Published Elsevier BV Keywords Linguistic agents Optimal communication Language learning Language evolution Game theory Multiagent systems Corresponding author Email address niyogicsuchicagoedu P Niyogi 00043702 matter 2003 Published Elsevier BV doi101016jartint200308005 2 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 1 Introduction Consider linguistic agents shared world The agents desire communicate different messages meanings Such situation arises number different contexts natural artiﬁcial communication systems important cases able quantify rate success information transfer words mutual intelligibility agents Each agent possesses communicative device language allows relate code signal message form meaning syntax semantics depending context communication arises If share language language expressive unambiguous mutual intelligibility high If hand share language languages inexpressive ambiguous mutual intelligibility lower This case real world paper present analysis situation We view languages probabilistic associations form meaning develop natural measure intelligibility F L1 L2 languages L1 L2 generalization similar function introduced 10 We ask following question biologicalculturaltechnological advantage agent increase intelligibility rest population ways The task increasing intelligibility reduces ultimately related subproblems Given language L language Lcid2 maximizes mutual intelligibility F L Lcid2 way communication shared world What acquisition mechanismslearning algorithms serve task improving intelligibility What consequences individual language acquisition behavior population dynamics communicative efﬁciency interacting population linguistic agents In paper create mathematical framework address questions analytically We ﬁnd surprisingly optimal language Lcid2 need L present algorithm approximating Lcid2 arbitrarily Section 3 The optimal language Lcid2 learned inherited individual parents In case ﬁnd bounds performance appropriate learning algorithms Section 4 In case study resulting population dynamics context evolutionary language game Section 5 11 Communicability animal human machine communication The simplest situation communicability readily deﬁned corresponds case language viewed association matrix A Such matrix simply links referents signals If M referents N signals A N M matrix The entries aij deﬁne relative strength association signal meaning j The matrix A characterizes behavior linguistic agent production mode produce signals corresponding particular meaning proportion strength association ii comprehension mode N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 3 interpret particular signal meanings proportion association strengths The speciﬁc settings scheme useful description include animal communication human languages artiﬁcial languages For instance makes sense talk lexical matrix formal description human mental vocabular ies It introduced arbitrary relations discrete words discrete concepts human languages 10141926 36 Bayesian perspec tive Each column lexical matrix corresponds particular word meaning concept row corresponds particular word form word image In Saus surean terminology arbitrary sign lexical matrix provides link signiﬁé signiﬁant 28 An equivalent lexical matrix basis animal communication deﬁnes relation animal signals speciﬁc meanings 4 8153132 A classic example alarm calls primates There ﬁnite number referents coded acoustic signals decoded appropriately recipients Inﬁnite association matrices description human languages 1325 Human grammars mediate complex mapping form meaning There space possible signals set strings sentences ﬁnite syntactic alphabet set possible meanings set strings semantic alphabet Most crucially sets possible sentences meanings inﬁnite This accounts inﬁnite expressibility human grammars In artiﬁcial intelligence problem arises different settings A number studies emerged linguistic agents interact simulated worlds studies coherent coordinated communication ultimately emerges example 21221233335 Much kind research employs simulation methodology Artiﬁcial Life In paper create mathematical framework kinds problems derive number analytic results We study language coordination gametheoretic setting results consequences Nash equilibria problems related research multiagent systems game theoretic foundations 137 In design natural language understanding systems goal develop able communicate human The statistical approach problem assumes underlying probabilistic model human source This probabilistic model recovered learned data randomly drawn samples case corpus linguistics statistical language modeling 3 16 overviews point view interactive exchanges semantic reinforcement 711 The primary implication paper optimal communication language user require learn language different target source 12 Main results context previous work Here outline main sets results presented respectively Sections 3 4 5 4 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 121 How maximize mutual intelligibility Let consider population agents assume language An evolutionary process described individuals reproduce offspring innate language acquire language basis interaction population This process ﬁrst explicitly modeled 10 later 23 21 In approach works discrete moment time randomly chosen individual replaced new learns language population 10 generations overlap It clear choice learning procedure offspring inﬂuence evolutionary dynamics ensues particular population converge maintain reasonably coherent language Several basic learning mechanisms considered The imitator simply learns averaged language population production comprehension mode The calculator Hurford called obverter 23 Bayesian learner 21 copy language population constructs best response adopts production behavior best understood population comprehension strategy best decoder population maximizing communicative efﬁciency population The Saussurean learner 10 imitates production mode population adopts comprehension behavior maximizes chances understanding It turns imitators coordinated communication unstable population learners Saussurean learners better performance obverters efﬁcient setting 23 21 Starting randomly chosen initial condition population obverters quickly develops highly coordinated communicative reaches state signals meanings related onetoone fashion plus isolated synonyms homonyms A peculiar feature imitators obverters production comprehension modes completely decorrelated1 Before perfect coordination language reached obverters ﬁnd speaking strange language Imagine case language sentences s1 s2 meanings m1 m2 A pathological linguistic agent use s1 communicate meaning m1 s2 communicate m2 production mode interpret s1 mean m2 s2 mean m1 comprehension mode Such linguistic agent self contradictory associations form meaning In paper avoid internal contradictions requiring linguistic agents production comprehension modes linked common association matrix In motivating considerations mind First cognitive standpoint natural symmetric consideration form meaning treat language relation form meaning separate functional mappings production comprehension Second computational standpoint 1 In contexts obverter deﬁned differently A general deﬁnition obverter performs maximize comprehension assumption hearers reception behavior N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 5 common association matrix provides compact representation language production comprehension modes easily derived The Saussurean learner satisﬁes criterion obverters reached coordinated language violate constraint 20 The communicating neural networks provide link production comprehension modes 202930 In paper execute comprehensive analysis situation The ﬁrst question address obverter algorithm carried selfconsistency constraint imposed learners2 This requires understand best response arbitrary language exists approximate We demonstrate following If language L selfconsistent general possible use obverter procedure ﬁnding best response In words comprehension behavior production behavior designed separately maximize commu nicative efﬁciency obey selfconsistency requirement If language L fully coordinated deﬁnes onetoone correspondence signals meanings best response exists equal language L Next suppose language L selfconsistent fully coordinated Then general possible ﬁnd best response approximate given accuracy ε Finally suppose language L fully coordinated communication noisy Then mild restrictions magnitude noise ﬁnd best response slightly stronger conditions language L Incidentally ﬁrst statements suggests obverter mechanism learning population individuals Even agent selfconsistent language average language population selfconsistent The obverter procedure newly introduced agent learn randomly chosen individual parent individual chosen proportionally linguistic performance 122 Learning optimal language A second set results relate problem learning self consistent language purpose optimal communication chosen teacher parent Since teachers language known outset learner obtain relevant estimates ﬁnite number interactions teacher This situation arises number artiﬁcial intelligence settings machine learning approach taken acquire language communicative purposes For example statistical language modeling spoken language understanding human machine language learning robotic communication systems robots machines natural applications In 2 The precise deﬁnition selfconsistency exists probability measure space signals meanings common production comprehension modes 6 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 cases particularly statistical language modeling tacitly assumed collecting corpus sampling target language reconstructing basis corpus sufﬁciently good strategy designing natural language based humancomputer interaction In light results presented paper assumption mistaken Speciﬁcally We consider different frameworks learning learning information learner access sentence meaning interactions ii learning partial information learner direct access sentence The meaning directly accessible learner knows communication ultimately successful We present algorithms learn communicate optimally settings We present explicit bounds number examples interactions needed agent able learn self consistent language yields communicability arbitrarily close optimal high probability In partial information setting number examples seen proportional N 2M 2γ 2 N number distinct signals M number distinct meanings In information setting meanings directly observable number examples reduces quantity proportional N 2γ 2 In cases γ margin parameter characterizes learning difﬁculty teachers language It interesting compare approach approach taken studies populations neural networks Oliphant 21 Smith 29 numerical simulations investigate dynamics iterative learning model While address question convergence maximum communicability teacherlearner pair looked convergence population networks optimal communication By varying update rules individual networks able learning bias onetoone mapping meanings signals led emergence coordinated communication In setting individual necessarily optimize communication ability current population individual learning strategy eventually facilitated high long term communicability outcome 123 Communicability evolutionary language game Finally examine implications communicability function language game framework There considerable recent activity work computational models evolution natural languages animal communication 8122325 In models based selective ﬁtness communicability function determines payoff different languages Individuals communicate receive high payoff translates biological ﬁtness reproductive success individuals higher ﬁtness produce children learn language Alternatively assume individuals high payoff high reputation standing group inﬂuential language teachers The assumption language performance measured function F contributes rate language spread N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 7 The function F deﬁnes equilibria language game equation In 38 equilibria called Nash equilibria production comprehension modes independent In paper results continue hold requirement selfconsistency imposed In words stable neutrally stable states language attained production comprehension modes cognitively related We characterize evolutionarily stable strategies ESS 18 language It proved strict ESS correspond fully coordinated languages However kind evolutionarily signiﬁcant state called weak ESS stable modulo random drift This state observed numerical studies language systems including mentioned 102123 In paper analytically prove If frequency occurrence events subjects communication shared world exactly uniform events occur exactly frequency weak ESS characterized perfectly coordinated languages isolated synonyms homonyms Section 53 precise deﬁnition In general case nonuniform frequencies events isolated synonyms possible homonyms unstable The result means ambiguous languages evolutionarily unstable Indeed true homonyms reduce communicability potential language isolated synonyms On hand commonly observed human languages numerous homonyms true synonymy extremely rare To resolve apparent contradiction remember presence context Indeed relevant communicative accuracy individuals deﬁned word utterance Therefore entries lexical matrix words slightly larger objects roughly deﬁned words context As soon accept level description results mathematical model correctly following observation human languages practically true homonyms remain homonyms presence contextual clues hand contextdependent synonyms common To examples let ﬁrst consider lexical homonymy fall autumn fall This complete homonymy level words level larger utterances clearly utterances fall fall confused Similarly words beautiful fair regarded true synonyms level words consider utterances beautiful lady fair lady interchangeable This illustrates point context taken account synonyms possible homonyms unstable The rest paper structured follows In Section 2 develop general notion association matrices probability measures cross product forms meanings We measure communicative efﬁciency mutual intelligibility naturally deﬁned In Section 3 construct approximating family languages converges optimal communicator We examine extension Appendix B study communication perfect language noisy 8 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 channel We continue examining implications results learning theory Section 4 discuss algorithms learning communicate present bounds sample complexity Finally Section 5 implications evolution discussed particular classify Nash equilibria characterize possible evolutionarily stable strategies Conclusions Section 6 2 Communicability linguistic systems 21 Basic notions We regard linguistic association form meaning Let S N set possible linguistic forms sentences signals M N set possible semantic objects meanings referents Note depending context elements S words codes expressions forms signals sentences The elements M meanings messages events referents We use general term signals elements S meanings elements M The sets S M need ﬁnite essential enumerable The reason sets S M viewed countable human languages discrete nature language In lexical setting S set words naturally countable countability M meanings assured categorization In case human grammars let S Σ 1 set possible strings syntactic alphabet Σ1 M Σ 2 set possible strings semantic alphabet Σ2 Note case S M inﬁnite We deﬁne communication language probability measure µ S M Note case ﬁnite languages human artiﬁcial lexicons animal communication systems µ related association matrix A means simple rescaling Let enumerate possible signals elements set S s1 s2 s3 possible meanings elements M m1 m2 m3 The coding decoding schemes agent contained measure µ following manner Each user µ characterized encoding matrix P decoding matrix Q cid1 cid2 Pij µsimj µsi mj 0 cid1 Qj µmisj µsj mi 0 p µsp mj p µsp mj 0 cid2 p µsj mp p µsj mp 0 cid2 cid2 1 2 Both P Q matrices easily interpreted Pij simply probability producing signal si given wishes convey meaning mj Similarly Qij probability interpreting expression si mean mj user Matrices analogous P Q introduced 10 explicitly related common measure µ An effective connection P Q employed particular learning mechanism called Saussurean 1021 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 9 Remarks 1 The user language characterized production mode matrix P comprehension mode matrix Q This captures fact given particular meaning different ways express Correspondingly given particular signal unique interpretation Thus ambiguities sentence interpretation polysemy lexical semantics incorporated 2 A measure µ uniquely deﬁnes corresponding P Q matrices The converse generally true given P Q matrices possible ﬁnd µ correct encoding decoding matrices An example 2 2 matrices P Q I cid3 cid4 µ1 µ2 cid3 12 0 13 0 0 12 0 23 cid4 Clearly µ1 µ2 lead P Q In order avoid ambiguities introduce equivalence classes measures We measures µ1 µ2 equivalent µ1 µ2 corresponding P Q matrices equal P 1 P 2 Q1 Q2 3 For probability measure µ let introduce s S m M st µs m 0 Sµ cid5 cid6 This deﬁnes set signals production comprehension linguistic agent In sense formal language theory set formed syntactic expressions In fact set Sµ normally called language Our deﬁnition language measure µ contains notion language support µ Similarly deﬁne Mµ cid5 m M s S st µs m 0 cid6 This deﬁnes set meanings expressible linguistic agent If Mµ M meanings expressed If Mµ proper subset M meanings left unexpressed 4 The probability measure µ sets Sµ Mµ matrices P Q humans animals arise highly structured systems brain In fact clear human languages objects vary arbitrarily A signiﬁcant activity generative linguistics attempts characterize nature structure variation exists natural languages world 22 Probability events communicability function The communicating agents immersed world need communicate messages arises corresponding events occur shared world Thus deﬁne measure σ set possible meanings M according agents need communicate meanings Given communication 10 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 systems languages µ1 µ2 probability event occurs meaning successfully communicated µ1 µ2 given P 1 2 µ1sj miµ2misj cid7 cid7 σ mi j Similarly compute probability event successfully communi cated µ2 µ1 cid7 cid7 P 2 1 µ2sj miµ1misj σ mi j We deﬁne effective communicability function µ1 µ2 cid8 F µ1 µ2 1 2 P 1 2 P 2 1 cid9 In matrix notation written cid10 cid9 F µ1 µ2 1 2 cid8 cid8 Q2 P 1Λ cid9 T tr cid8 cid8 P 2Λ Q1 cid9cid11 cid9 T 3 tr Λ diagonal matrix Λii σ mi P Qi refer coding decoding matrices associated measure µi Note trP 1ΛQ2T simply probability event occurs successfully communicated user µ1 user µ2 Remarks 1 The function F µ1 µ2 average probability µ1 µ2 understand way communication mode The function F µ1 µ2 symmetrical respect arguments If µ1 probability measure support diagonal elements S M P Q matrices identity communicative efﬁciency 1 2 F µ1 µ1 communicability identical linguistic agents We 0 F µ1 µ1 cid1 1 For different agents µ1 µ2 0 cid1 F µ1 µ2 F µ2 µ1 cid1 1 3 The marginals µm σ m equal In words language agent simply given µ conditional probabilities associated The probability agents communicate different meanings determined language external world agents grounded Therefore agents high communicative efﬁciency world low communicative efﬁciency 4 A function similar communicability function introduced Hurford 10 However meanings treated equal probabilities uniform measure σ function suitable inﬁnite matrices N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 11 3 Reaching highest communicability Let assume languages given language µ0 According deﬁnition 3 language µ cid7 F µ0 µ 1 2 σj ij cid10 µ0si mj µmj si µsi mj µ0mj si Let deﬁne best response language µ F µ0 µ sup µ F µ0 µ cid11 4 5 In follows present algorithm building best response language sense approaches best response In particular best response need exist However arbitrarily good response constructed We construct sequence languages µε ε 0 F µ0 µε arbitrarily close supµ F µ0 µthe maximum possible mutual intelligibility user µ0 user allowable language The interesting question ﬁnding best response noisy environment considered Appendix B 31 A special case ﬁnite languages In order argument transparent possible ﬁrst simplifying assumptions The effect relaxing assumptions demonstrated Section 32 For assume following conditions satisﬁed The languages ﬁnite matrices size N M ii The distribution σ uniform σi 1M iii The measure µ0 satisﬁes property unique maxima exist unique p0i unique r0i µ0simp0i max p µ0si mp µ0misr0i max r µ0mi sr 6 The condition states exists strictly element column µ0sm row µ0ms biggest element column row Let maximize terms right hand expression 4 separately First ﬁnd matrix Q cid7 cid7 µ0simj Q ij max Q ij ij µ0si mj Qij 7 maximize matrices Q elements nonnegative sum row This results following deﬁnition Q 1 µ0simj maxp µ0si mp 0 Q ij 8 cid1 12 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 In words order construct best decoder Q need ﬁnd largest elements rows µ0sm ones correspondent slots Q The rest entries matrix Q zero This deﬁned operation property unique maxima Similarly ﬁnd matrix P cid7 ij P ij µ0mj si max P Pij µ0mj si cid7 ij maximize matrices P elements nonnegative sum column The best encoder P given cid1 P ij 1 µ0mj si maxp µ0mj sp 0 9 maximize column matrix µ0ms Now best encoder best decoder language µ0 Finding matrices P Q completes task obverter 23 However setting matrices independent need related common measure If measure µ existed µsm P µms Q satisfy Eq 5 deﬁning best response It turns general µ exist However exists measure approaches performance P Q arbitrarily close It convenient use following short hand notation P 0 ij µ0simj Q0 ij µ0mj si We ready formulate following Theorem 31 For ﬁnite language µ0 satisfying property unique maxima uniform probability distribution σ cid8 P 0Q F µ0 µ 12M tr T P Q0T cid9 sup µ In order prove statement need µ cid8 F µ0 µ cid1 12M tr P 0QT P Q0T cid9 b exists family languages µε cid12 cid12sup µ lim ε0 F µ0 µ F µ0 µε cid12 cid12 0 The proof immediately follows deﬁnitions best decoder best encoder The rest subsection devoted developing algorithmic proof b Given matrices Q P build family measures µε µεms Q µεsm P 10 lim ε0 lim ε0 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 13 This trivial task demonstrated following example Suppose P Q matrices given cid3 cid3 cid4 cid4 P 1 0 0 1 Q 0 1 1 0 It clear ﬁnd measure µε satisfy conditions 10 pair P Q Fortunately turns situations like arise In order prove need consider auxiliary matrices 311 The auxiliary matrix absence loops Let deﬁne auxiliary matrix X following way ij 0 cid1 Xij 1 P Q ij 0 This means matrix X contains nonzero entries slots matrices P Q contains nonzero entry Now let draw lines connecting ones X matrix belong row ones X matrix belong column We obtain disjoint graphs Let refer ones X matrix vertices Lemma 32 Suppose ﬁnite measure µ0 property unique maxima Graphs constructed described contain closed loops Proof Let assume exists closed loop It looks like polygon right angles Let consider turning points points simultaneously belong horizontal vertical line Suppose 2K vertices number We refer vertices xαi βj pair integers αi βj gives coordinates vertex Clearly 1 cid1 j cid1 K Without loss generality let xα1β1 connected xα1β2 horizontal line Then xα1β2 connected xα2β2 vertical line xαK β1 connected xα1β1 vertical line closing loop Fig 1 K 3 It possible exactly half vertices corresponds ones P matrix restto ones Q matrix If vertex correspond Q matrix corresponding slot P matrix zero vice versa This direct consequence property unique maxima Let suppose Q 0 alternative P 1 0 case proof remains similar This means Q Q 0 construction 8 nonzero element row Q matrix Then element P 1 corresponding vertex present α1β2 X matrix This leads P 0 positive element column P matrix Eq 9 This argument continued loop The Q elements loop alternating 0 1 elements P matrix Fig 1 1 P α1β1 α1β2 α1β1 α2β2 α1β1 α1β1 We conclude P 0 construction positive elements Q matrix correspond largest elements corresponding rows P 0 matrix Similarly obtain 2K inequalities P 0 α1β2 α1β1 14 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Fig 1 No loops graphs P 0 αi βi P 0 Q0 αi1βi1 αi βi1 Q0 1 cid1 cid1 K αi βi1 11 12 set αK1 α1 βK1 β1 In Fig 1 maximum elements rows P 0 columns Q0 marked crosses The arrows indicate direction larger elements We 1112 incompatible In order write ij Mi Mi sum elements ith row matrix µ0 k µ0si mk Then rewrite P 0 ij terms Q0 M µ0si mj Q0 cid2 Mi P 0 ij cid2 µ0si mj k µ0sk mj cid2 Q0 ij Mi k Q0 kj Mk System 1112 presented closed chain inequalities Q0 Q0 α1β1 Q0 α1β2 Q0 α2β2 Q0 α2β3 cid2 cid2 cid2 cid2 k Q0 kβ1 k Q0 kβ2 k Q0 kβ2 k Q0 kβ3 cid2 Mk Mk Mk Mk Q0 α1β2 Q0 α2β2 Q0 α2β3 Q0 α3β3 Mk k Q0 kβi k Q0 kβi1 Mk Q0 αi βi Q0 αi βi1 cid2 Q0 αi βi1 Q0 αi1βi1 Q0 αK βK Q0 αK β1 cid2 cid2 kβK k Q0 k Q0 kβ1 Mk Mk Q0 αK β1 Q0 α1β1 13 14 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 15 From ﬁrst inequalities know Q0 α1β1 Q0 α2β2 cid2 cid2 k Q0 kβ1 k Q0 kβ2 Mk Mk Q0 pair similarly derive cid2 α1β1 Q0 k Q0 kβ1 k Q0 kβ3 Continuing chain Kth step cid2 α3β3 Mk Mk cid2 Mk Q0 α1β1 Q0 k Q0 kβ1 k Q0 Using inequalities ﬁnally obtain Q0 proves closed loops matrix X αK βK α1β1 Mk cid2 kβK Q0 α1β1 This contradiction 312 Constructing matrix µε Now systematically build matrix µε From Lemma 32 follows connect vertices matrix X horizontal vertical lines resulting disjoint graphs contain closed loops Some graphs consist vertex For graphs perform following procedure Take pair vertices If connected horizontal vertical line refer corresponding entries Q matrix P matrix One otherzero Draw arrow graph element corresponding zero element corresponding Repeat pairs vertices Next starting vertex replace corresponding element X matrix ε following arrows replacing elements X entries form εk integer k increases decreases vertex depending direction arrow Lemma 32 closed loops graphs matrix X We resulting matrix Aε The measure µε obtained renormalizing elements matrix Aε µεsi mj Aε ij Aε kl 15 kl Remark 33 In algorithm powers small parameter ε εk assign vertices matrix X More generally use functions ε fkε limε0 fkεfk1ε 0 Thus family µε families 313 Proof Theorem 31 We ready complete proof Theorem 31 b Proof Let Eq 10 holds In order ﬁnd entries µεsm need normalize column matrix µε elements sum Obviously cid13 cid7 16 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 column contain segment graphs By construction biggest element segment graph corresponds positive element Q In limit ε 0 elements vanishingly small comparison biggest resulting column µεsm matrix identical corresponding column P matrix The argument holds rows µεms matrix limit rows Q matrix Thus conclude algorithm Section 312 leads constructing family measures µε satisfy requirements Theorem 31 Example 34 Consider following 5 5 matrix 23 90 2 64 1 42 81 42 92 8 53 77 60 50 2 88 15 68 73 59 39 48 66 65 37 µ0 1 1245 16 For language supµ F µ0 µ 3946225 In Fig 2 calculated P Q matrices construct X Aε matrices The family µε given 0 1 0 0 0 0 0 0 ε 0 1 31 ε ε2 ε 0 ε2 0 0 0 0 0 0 1 0 1 0 ε 0 µε As ε 0 F µ0 µε supµ F µ0 µ Remark 35 If let µ µεε0 µε evaluated 0 note µ cid17 µ0 general Further F µ0 µ supµ F µ0 µ Thus limε0 µε µ F µ0 µ limε0 F µ0 µε supµ F µ0 µ This consequence discontinuity deﬁnition communicability function F L1 L2 Namely conditional probabilities entering deﬁnition 4 discontinuous elements column row µ zero Eqs 12 Thus value F µ0 µε jump ε 0 32 General languages Now demonstrate effect relaxing assumptions iii Section 31 321 Multiple maxima neutral vertices If condition iii previous section satisﬁed language µ0 possess property unique maxima deﬁnitions 8 9 changed For instance µ0skmα1 µ0skmαn maximal values kth row matrix µ0sm γn γ1 Q Q α1k αnk N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 17 Fig 2 Construction Aε Example 34 We ﬁrst form P 0 Q0 matrices normalizing columns rows µ0 respectively step shown Then construct best encoder P identifying maximal elements columns Q0 best decoder Q identifying maximal elements rows P 0 ﬁgure Next combine positive elements vertices P create auxiliary matrix X The vertices X belong column row connected In order deﬁne direction arrows refer matrices P If vertices connected vertical line ﬁnd corresponding elements P matrix encircled direction arrow P matrix Similarly vertices connected horizontal line ﬁnd corresponding elements Q matrix encircled direct arrow Q matrix Finally build Aε matrix replacing ones X matrix powers ε The powers ε arranged way connected graphs arrows point smaller entry larger entry Note example P compatible single measure Q Q Q cid2 ij µ0si mj Q n i1 γi 1 γ1 γn arbitrary positive numbers restriction cid2 ij Eq 7 depend The result evaluating function values coefﬁcients γi The argument repeated P Next note closed loops possible case Lemma 32 modiﬁed Let generalize procedure assigning direction graphs case language µ0 possess property unique maxima We assign direction segments graph corresponding rows P 0 columns Q0 unique maximum We corresponding vertices neutral vertices For vertices neutral proceed vertices connected horizontal vertical line arrow points larger element Q P matrix For closed loop auxiliary matrix X deﬁne direction segment positive negative clockwise counterclockwise The direction zero arrow We direction changes sign changes positive negative negative positive Instead Lemma 32 18 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Lemma 36 The following true loops auxiliary matrix X contain neutral vertex b direction graph changes sign identically zero Proof Statement proved assuming neutral vertices loop applying Lemma 32 To prove statement b let assume direction arrows loop positive neutral Then repeat argument Lemma 32 write chain equationsinequalities similar 1314 The difference inequalities fact equals sign More precisely segment positive zero direction correspond We immediately contradiction signs strict inequalities change direction This proves statement b The statement Theorem 31 holds case perfect encoder decoder redeﬁned indicated The algorithm building best response language stays similar We assign powers ε nodes power decreases direction arrows For adjacent neutral nodes power ε arbitrary weights assigned neutral nodes If loop present possible assign powers ε consistent way statement b Lemma 36 It necessary use noninteger powers Example 37 Consider following 3 3 matrix µ0 1 43 cid20 cid21 8 2 5 2 10 2 2 9 3 It easy calculate P 0 Q0 matrices cid20 cid20 cid21 P 0 813 524 13 213 512 13 13 38 313 Q0 815 13 57 17 314 914 17 18 cid21 215 17 17 We language possess property unique maxima column Q0 contains maximal elements We cid21 cid21 cid20 cid20 cid20 cid21 P 1 0 0 0 1 0 0 γ 1 γ Q X 19 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 The directed graph contains loop neutral vertices marked N 1 1 N N N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 19 The graph changes direction loop Applying algorithm obtain following Aε matrix 0 1 ε Aε 1 0 0 0 γ1ε γ2ε cid21 cid20 For positive numbers γ12 family satisﬁes conditions Theorem 31 approaches best communicability Note γ1γ2 γ limit ε 0 matrices P Q recovered Eqs 19 Note general possible ﬁnd Aε matrix rise given P Q conditions arbitrary neutral coefﬁcients P Q matrices imposed Lemma 53 322 Nonuniform distributions Before considered uniform distributions σi 1M condition ii Now let assume general distribution It turns argument changes little Namely deﬁnition 8 cid1 Q ij 1 µ0simj σj maxp µ0si mpσp 0 20 similarly case unique maxima property deﬁnition 9 stays In proof Lemma 32 argument follows logics change comes inequalities 11 Pαi βi σi Pαi βi1σi1 21 However multipliers σl canceled loop statements Lemmas 32 36 remain true case algorithm building best response 323 Inﬁnite matrices Finally deal restriction Section 31 First let deﬁnition 4 makes sense case inﬁnite matrices We cid7 1 2 cid1 1 2 cid8 µ0simj µmj si µsimj µ0mj si cid7 cid8 µ0simj µsimj cid9 1 cid9 Since σ measure cid2 Now deﬁne following quantities j σj 1 leads conclusion F L0 L cid1 1 Ai sup j P 0 ij σj Bi sup j Q0 j The generalization Theorem 31 case inﬁnite matrices given Theorem 38 For inﬁnite matrices supµ F µ0 µ 1 2 cid2 iAi Bi 20 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 It straightforward µ F µ0 µ cid1 1 2 iAi Bi In order conclude proof Theorem 38 necessary construct family languages µε limε0 supµ F µ0 µ F µ0 µε 0 This Appendix A cid2 324 The existence best response To end section address issue From Theorem 31 extensions follows exists family languages µε limε F µ0 µε cid2 µ F µ0 µ Can happen supremum actually reached language From previous considerations clear language µ satisﬁes Eq 5 obeys µsm P µms Q P Q unique like case property unique maxima satisﬁed The question existence µ answered following Theorem 39 For language µ0 limiting measure µ exists auxiliary matrix X satisﬁes following property vertices matrix X share row column neutral vertices Proof The easy given condition theorem apply algorithm building family µε Section 312 extension Section 321 observe powers ε simply canceled normalize matrix µε This means µε depend ε know satisﬁes conditions Theorem 31 conclude µε µ To ﬁnish proof need condition theorem satisﬁed µ exist Let assume adjacent nonneutral vertices b matrix X Without loss generality let assume connected horizontal line This means Q 1 vertices vertex 0 vertex b Therefore µ exists positive entry On hand P 1 vertex b zero vertex Therefore matrix µ zero vertex leads contradiction Corollary 310 If P Q extended permutation matrices square permutation matrices extra rows columns consisting entirely zeros µ deﬁned P properly normalized Corollary 311 If property unique maxima satisﬁed µ exists unique 4 Implications learning From preceding discussion clear order maximize mutual intelligibility language user characterized measure µ necessary N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 21 use different measure µ µ cid17 µ This fact implications learning evolution populations linguistic agents Let ﬁrst consider problem agent trying learn language order communicate agent language characterized measure µ Recall µ best response exist arbitrarily close approximation µε ε exist Therefore learners task estimate µε What degree accuracy ε useful necessary depend particular application mind Since measure µ unknown learner outset natural learning scenarios depending information available learner interaction 1 Full information This corresponds situation learner able sample µ directly sentence meaning pairs Thus teacher speaks sentence meaning directly accessible The strategy learner estimate µ derive P Q matrices ultimately µε procedure described previous sections 2 Partial information In natural settings meaning directly accessible In words learner hears sentence intended meaning latent What learner reasonably access interpretation sentence successful On basis information learner derive optimal communication strategy We refer learning partial information Note assume learner hearer receives weak reinforcement communicative exchange This similar setting selﬁsh games developed work Steels pursued 40 simulations There variants learner strong reinforcement extreme form told true meaning failed communicative exchange alternative corrective feedback We explore strong reinforcement setting Thus 1 information 2 partial information suggest different frameworks learning case learner estimate P Q matrices teacher 41 Estimating P An important task learner estimate Q derivable P matrix teacher Recall cid1 Q ij 1 σj µsi mj maxp σpµsi mp 0 411 Learning information The learner case access s m sentencemeaning pairs time teacher produces sentence We deﬁne event Aij Teacher produces si communicate mj 22 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 The probability event Aij simply σj µsimj Therefore teacher produces n sentence meaning pairs random independent identically distributed ratio ˆaij n kij n empirical estimate probability event Aij By law large numbers n ˆaij n σj µsi mj probability 1 For case consideration bound rate convergence occurs For example applying Hoeffdings inequality cid10cid12 cid12 ˆaij n σj µsimj cid12 cid12 ε cid11 P cid1 2 e ε2n2 This convergence guaranteed ﬁxed j In general learner estimate collection events The total number events given total number possible sentence meaning pairs As let assume N possible sentences M possible meanings Therefore NM different events probabilities need estimated The collection events Aij 1 N j 1 M disjoint For ﬁnite collection events derive uniform law large numbers Let event Eij Eij cid12 cid12 ˆaij n σj µsimj cid12 cid12 ε Then union bound obtain cid24 Eij cid1 cid22cid23 P ij cid7 ij P Eij cid1 NM2 e ε2n2 Therefore cid22cid23 cid24 P Eij ij cid10 j cid12 cid12 ˆaij n σj µsimj cid12 cid12 cid1 ε cid11 1 NM2 e ε2n2 P Thus high probability depending number examples n empirical estimates ˆaij n close σj µsimj respectively Estimating σj µsimj s step estimating Q matrix required optimal communication 412 Learning partial information Now consider setup 2 learner access meaning directly guess meaning told event guess correct incorrect Thus learner access asymmetric information guess correct learner knows true intended meaning guess incorrect learner merely knows meaning As turns dramatically change state affairs To let learner guess meaning uniformly random Thus probability 1M learner chooses meaning mj Each time teacher produces N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 23 sentence intended meaning successfully communicated Deﬁne event Aij Teacher produces si Learner guesses mj Communication successful M σj µsimj The event Aij observable The probability event Aij simply 1 learner knows sentence uttered teacher ii meaning learner assigned sentence iii communication successful Therefore n sentences produced teacher learner count kij number times event Aij occurred empirical estimate probability Aij ˆaij n kij n By argument ˆaij n converges probability 1 M σj µsimj rates provided Hoeffding bounds Since M ﬁxed advance known allows learner guess σj µsimj j arbitrarily Let little precise rates convergence The learners estimate σj µsimj M ˆaij ˆaij deﬁned Therefore cid22cid12 cid12 cid12 ˆaij 1 cid12 M cid10cid12 cid12M ˆaij σj µsimj ε2n2M2 σj µsimj cid12 cid12 ε cid12 cid12 cid12 cid12 cid1 2 e ε M P P cid24 cid11 Thus conﬁdence εgood estimate σj µsimj poorer By argument case 2 uniform bound follows ε2n2M2 cid12 cid12M ˆaij σj µsi mj 1 NM2 e cid12 cid12 cid1 ε j 22 P cid10 cid11 42 Estimating Q Let consider task estimating P derivable Q matrix teacher The arguments previous section apply Recall cid1 P ij 1 µmj si maxp µmj sp 0 421 Learning information Here learner direct access meaning assigned teacher sentence Therefore learner need pick sentence uniformly random probability 1N produce teacher hear Let deﬁne event Aij Learner produces si Teacher interprets mj The event Aij observable trial The probability occurs given N µmj si After n trials learner speaks manner learner simply 1 counts number kij times event Aij occurs estimate 1 N µmj si kij n Therefore cid22cid12 cid12 cid12 ˆaij 1 cid12 N ε2n2N 2 cid12 cid12 cid12 cid12 ε µmj si cid1 2 e P cid24 24 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Using arguments cid10 P j cid12 cid12MN ˆaij µmj si cid12 cid12 cid1 ε cid11 1 NM2 e ε2n2N 2 422 Learning partial information The learner simply picks sentence meaning pair uniformly random probability 1NM Deﬁne event Aij Learner produces si mj Communication successful The event Aij observable learner trial The probability event Aij NM µmj si After n trials learner speaks learner counts number kij 1 times event Aij occurs Therefore cid22cid12 cid12 cid12 ˆaij 1 cid12 NM P µmj si cid1 2 e ε2n2M2N 2 cid24 cid12 cid12 cid12 cid12 ε Using arguments cid10 P j cid12 cid12MN ˆaij µmj si cid12 cid12 cid1 ε cid11 1 NM2 e ε2n2M2N 2 23 43 Sample complexity bounds Now pieces determine number learning events need occur high probability learner able develop language εgood communicability Let teachers measure µ We assume µ P Q matrices unique rowwise columnwise maxima respectively First let introduce margin maximum value clears values row column respectively This margin play important role determining number learning events Deﬁnition 1 For let j arg max j σj µsi mj j let j arg max µmj si Then deﬁne margin γ largest real number σj µsimj cid2 σj µsimj γ j cid17 j µmj si j cid2 µmj si γ cid17 j N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 25 431 Learning partial information We described estimate Q P matrices following theorem provides bound number examples needed ensure correct estimates Theorem 41 If total number n interactions teacher learner partial information greater 64M 2N 2γ 2 log4MNδ high probability 1 δ learner construct measure arbitrarily good communicability teacher Proof Let n2 interactions teacher speaks learner listens n2 interactions form The learner constructs estimates σj µsi mj µmj si manner described previously Let estimates denoted ˆpij ˆqij respectively By setting ε γ 4 Eqs 22 23 obtain cid12 cid12 cid1 γ 4 cid12 cid12 ˆpij σj µsimj 1 2NM e γ 2n64M2 j P cid11 cid10 cid10 cid12 cid12 ˆqij µmj si cid12 cid12 cid1 γ 4 cid11 P j 1 2NM e Using fact P A B cid2 P A P B 1 probability greater 1 2NMeγ 2n64M2N 2 eγ 2n64M2 estimates ˆpij ˆqij γ 4 true values The learner chooses Q P estimated matrices Let ﬁrst consider case Q For learner desires obtain j γ 2n64M2N 2 given j arg max j σj µsi mj The learner chooses ˆji arg max j ˆpij claim ˆji j immediately µsimj cid2 σ ˆji σj µsim ˆji γ In order prove assume case Then σ ˆji cid2 ˆpi ˆji µsim ˆji However following chain inequalities γ 4 cid2 σj leads contradiction This argument holds ˆji j Q matrix identiﬁed exactly Similarly P matrix identiﬁed exactly γ 4 cid2 ˆpij µsimij γ 2 The thing remains ensure n large occurs high probability We cid8 2NM e γ 2n64M2N 2 e γ 2n64M2 cid9 cid1 4NM e γ 2n64M2N 2 cid1 δ This satisﬁed n 64M 2N 2γ 2 log4MNδ Thus probability greater 1 δ P Q identiﬁed exactly Now procedure approximating measure applied 26 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Remarks 1 The number examples seen function M N γ The margin γ depends teachers language µ determines sense easy estimate Q P matrices learner It characterizes learning difﬁculty µ setting 2 Finite matrices applicable settings alarm calls animal communication systems lexical learning human linguistic systems For example 27 discusses problem learning mappings signals meanings variety schemes associative learning Bayesian estimation 3 Inﬁnite matrices learnable general In fact inﬁnite dimensional spaces known unlearnable 39 constraints required space possible measures teachers language belongs There ways explore reasonable constraints linguistic measures One possibility pursue approach Chomsky 5 restrict range variation possible syntactic forms possible measures µ Another possibility pursue theory compositional semantics 6 meanings larger units like phrases sentences derived compositional rules applied meanings smaller units like morphemes words Thus true learning task learn meanings words appropriately apply compositional rules syntactic forms Since words ﬁnite reduces inﬁnite case ﬁnite A possibility use context heavily claim learning proceeds context context case based fashion particular context ﬁnite number possible forms interpretations A proper development issues subject research scope current paper 4 The constants bound sample complexity tightened order essentially correct For example let interactions symmetric numbers sentences learner produces receives It easy check favorable bound obtained learner speaks N 2 times listens In case 32M 2N 2 1γ 2 log4MNδ interactions 432 Learning information For completeness let state number interactions needed learn setting 1 This given following Theorem 42 If total number n interactions teacher learner information greater 64N 2γ 2 log4MNδ high probability 1 δ learner construct measure arbitrarily good communicability teacher The proof similar previous theorem omit reason It noteworthy learning information requires M 2 fewer interactions learn This surprising meanings accessible larger number M N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 27 different concepts greater difference learning partial information 5 Implications evolution In section address question evolutionary signiﬁcance communicability This application different contexts First artiﬁcial intelligence way create communicating machines start population agents suboptimal communication let evolve learn This general approach pursued forms researchers evolutionary computation genetic algorithms artiﬁcial life Since goal increase information transfer function F conveniently play role score different communication systems Based agents higher intelligibility arranged proceed agents lower intelligibility score gradually eliminated The main question process ultimately lead coherent communication In follows develop formalism allow characterize possible outcomes dynamical Second gametheoretic approaches relevant biological evolution simple innate signaling systems animal world In setting function F contributes biological ﬁtness individuals The framework developed obvious drawbacks assumption signaler receiver equally interested successful transfer information necessarily case natural settings However studying basic properties evolutionarily stable states simplest explain certain aspects evolution approach later extended include sophisticated scenarios clashes signalers receivers Finally application human languages evolutionary theory potentially contribution Here innate genetic endowment considered selective pressure learned culturally transmitted languages 1325 For natural selection act language ability reward successful communication links language biological ﬁtness We assume successful communication leads payoff speaker hearer In spirit evolutionary game theory link payoff reproductive success 9 17 Individuals communicate successfully increased survival probabilities leave offspring An alternative interpretation kind dynamics individuals acquire higher standing reputation group leave followers learn language This simpliﬁed model leaving potentially important aspects serves logical tool reasoning evolution human language The inferred properties evolutionarily stable states compared observed properties human languages The results comparison shed light role communicability evolution human language 28 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Fig 3 Nash equilibria ESS 51 Preliminaries Let characterize attracting states evolutionary means payoff function F µ µcid2 It useful recall important deﬁnitions classical game theory 9 Language µ strict Nash equilibrium F µ µ F µ µcid2 µcid2 cid17 µ It Nash equilibrium F µ µ cid2 F µ µcid2 µcid2 cid17 µ Language µ called evolutionarily stable state ESS 18 µ Nash µcid2 F µ µ F µ µcid2 F µ µ F µcid2 µcid2 Language µ weak ESS ﬁnal strict inequality relaxed weak F µ µ cid2 F µcid2 µcid2 It shown 38 language ESS strict Nash equilibrium Fig 3 It clear µcid2 strict Nash equilibrium iff exists unique best response equal µcid2 From algorithm ﬁnding best response given paper follows strict Nash equilibria square matrices given permutation languages accordance 38 In presence nontrivial transition matrix noisy environment derive following result permutation languages strict Nash equilibria T matrix diagonally dominant column wise rowwise Appendix B It interesting conditions T matrix satisﬁed permutation languages longer stable However situations correspond kind noise changes signals recognition hardly considered relevant natural settings These results indicate perfectly coordinated systems homonyms synonyms evolutionarily stable Strict Nash equilibria exhaust set important rest points In order picture need characterize weak ESS Once reached weak ESS random drift possible change average communicability 52 Nash equilibria The classiﬁcation Nash languages uniform σ distributions 38 reproduce result need use later N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 29 Theorem 51 38 For uniform probability distribution σ language Nash supports P Q matrices coincide row column P Q matrix contains distinct values zero3 The case general distributions considered Section 54 An example Nash language given Example 52 Consider language cid2cid2 P Qcid2cid2 a1 a2 0 0 0 0 0 a3 0 a5 a1 0 0 a4 0 c1 c1 0 0 0 cid2 c3 0 0 0 c2 0 c3 0 c2 0 i1 P cid2cid2 N ij a1 a2 0 0 0 c4 c4 0 0 0 0 a2 a3 0 0 a1 0 a3 0 0 0 a2 a3 0 0 0 c5 c5 0 0 cid2 c6 0 c7 0 c6 c7 0 0 0 0 j 1 Qcid2cid2 M ij 0 0 0 a4 a5 0 0 0 c8 c8 The conditions 1 1 lead c1 1 2 c4 c2 c3 1 c8 c5 1 2 c6 ai 1 2 c6 c8 1 2 24 This language satisﬁes conditions Theorem 51 Nash equilib rium We need check P Q matrices Nash equilibria related common measure We following useful Lemma 53 The P Q matrices Nash languages Theorem 51 correspond common measure µ Proof We simply construct measure µ Let form diagonal M M matrix DQ DQ ii value nonzero elements ith column Q Similarly diagonal N N matrix DP element DP ii value nonzero elements ith row P We P XDP Q DQX X auxiliary matrix P Q ones support Q P zeros Let deﬁne A DP XDQ 25 3 These conditions derived methods Section 321 arbitrariness algorithm presence neutral vertices 30 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 It easy check matrix µ obtained proper normalization A corresponds matrices P Q Let assume µ Nash exists language µcid2 F µ µ F µ µcid2 F µcid2 mucid2 F µ µ There selective pressures language µ replaced language µcid2 A reliable equilibrium states given weak ESS If weak ESS change state random drift takes long time large populations 24 Therefore important able characterize weak ESS language It observed 38 Nash equilibria correspond weak ESS Here derive speciﬁc conditions Nash equilibria weak ESS languages 53 Weak ESS uniform σ distributions We language µ synonyms homonyms column row matrix µ positive entries Let synonym homonym isolated corresponding rows columns matrix µ contain positive elements Example 54 Consider language µ 1 2 β 0 α 1 α β 0 0 0 0 0 0 0 0 0 0 γ 1 γ 26 The synonyms µ34 µ44 isolated 3rd 4th rows contain positive entries The homonyms µ12 µ13 isolated entries columns 2 3 We synohomonyms sets elements possible ﬁnd chain elements connecting given elements synonym synonym homonymhomonym relationships Example 52 contains synohomonyms Suppose synonyms homonyms language µ isolated ones We following observation Observation 55 For µ best response µ function F µ µ depend actual entries matrices simply equal 1M times number effective elements count synonyms corresponding meaning effective element homonyms expressed word effective element To illustrate note Example 54 effective elements synonyms counted homonyms counted Thus F µ µ 3 4 The following statement holds N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 31 Theorem 56 For uniform probability distribution σ language µ weak ESS kind synonyms homonyms isolated synonyms homonyms Proof The proof contains parts First assume language satisﬁes conditions theorem prove Nash ii µcid2 F µ µcid2 F µ µ F µcid2 µcid2 cid1 F µ µ Then languages Theorem 56 weak ESS The languages µ Theorem 56 Nash obey conditions Theorem 51 Next construction auxiliary matrix X contains closed loops turns Theorem 39 best response exists From Observation 55 follows F µ µ F µ µ F µ µ µ weak ESS To conclude proof need Nash language ESS Let suppose language µcid2cid2 satisfy conditions Theorem 56 Then contains sets synohomonyms We exists language µ satisfying conditions Theorem 56 F µcid2cid2 µcid2cid2 F µcid2cid2 µ F µ µ F µcid2cid2 µcid2cid2 27 Using algorithm ﬁnding best response P Q matrices language µcid2cid2 support language symmetries It easy check choice P Q best response µ unique Let identify groups synohomonyms µ generally represented submatrices size k l 1 cid1 k cid1 N 1 cid1 l cid1 M rows columns consisting entirely zeros Each submatrices considered separately maximize contribution function F µ µ Below assume simplicity group synohomonyms generalization multiple groups straightforward Now build µ F µ µ maxµ F µ µ clarity useful consult example considered In support matrices P Q X let identify largest extended permutation matrix belongs support It unique size bigger mkl mink l This skeleton matrix cid25X support matrix µ Next need sure support cid25X contains element row column submatrixotherwise F µcid2cid2 µcid2cid2 cid17 F µcid2cid2 µ This adding elements matrix X skeleton permutation matrix row column missing cid25X It easy check resulting k l matrix contain isolated synonyms homonyms To build cid25P cid25Q matrices cid25X simply sure satisfy standard normalization conditions This leaves entries corresponding isolated homonyms cid25Q matrix synonyms cid25P matrix undeﬁned All left measure µ condition 27 holds Let consider function F µ µ µ best response µcid2cid2 F µ µ linear function arguments entries matrices P Q A linear function reach maximum boundary domain For row Q matrix entries lie simplex The maximum reached elements rest zero Because restriction supports matrices P Q coincide 32 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 nonzero entries corresponding vertices simplexes form permutation matrix Note value function F µ µ depend value entries corresponding isolated synonymshomonyms This means construction µ deﬁned corresponds maximum function F µ µ inequality 27 holds We conclude language µcid2cid2 containing synohomonyms ﬁnd matrix µ satisfying conditions Theorem 56 F µ µ F µcid2cid2 µ F µcid2cid2 µcid2cid2 Theorem 56 proven To illustrate Theorem 56 algorithm consider Example 52 It presents Nash language satisfy conditions Theorem 56 contains syno homonyms Its best response deﬁned following matrices P Q α1 α2 0 0 0 a1 b1 0 0 0 0 0 α3 0 α5 0 0 a2 0 b2 β1 0 0 α4 0 a3 0 0 b3 0 γ1 β2 0 0 0 a4 b4 0 0 0 0 γ2 β3 0 0 0 a5 b5 0 0 δ1 0 γ3 0 0 a6 0 b6 0 0 0 δ2 δ3 0 0 0 a7 b7 0 0 0 0 0 β4 β5 0 0 0 a8 b8 usual normalization restrictions plus condition P Q identical support The normalization conditions state elements columns P rows Q belong simplexes a1 a3 a4 a6 1 Some entries matrices P Q allowed zero rows columns consisting entirely zeros If holds µcid2cid2 µ 52 matter entries µ follows Eqs 24 normalization conditions P Q The function F µ µ hand depends entries µ reaches maximum corners simplexes In order ﬁnd maximum need identify largest permutation matrix support µ Fig 4 elements encircled solid lines Then sure zero rows columns adding elements cid25X encircled dotted line The maximum µ value F µ µ 5 number effective entries Observation 55 Fig 4 isolated homonyms underlined The matrices cid25P cid25Q obtained cid25X cid25P 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 33 Fig 4 Building matrix µ Example 52 cid25Q 0 1 0 0 0 x1 0 0 0 0 0 0 0 0 1 x1 x2 x3 x4 1 The corresponding measure µ satisﬁes 27 We conclude language Example 52 weak ESS 0 0 0 1 0 0 1 0 0 0 0 0 x2 0 0 0 0 x4 0 0 0 0 x3 0 0 54 Weak ESS general σ distributions First generalize Theorem 51 case nonuniform σ We Theorem 51cid2 Let Λ diagonal matrix elements Λii σi Then language Nash supports P P Λ Q matrices coincide row column P Λ Q matrix contains distinct values zero Example 57 For N 2 M 3 σi given 12 14 14 language cid3 cid3 cid4 cid4 P 12 1 12 0 cid3 0 1 Q cid4 P Λ 14 14 14 0 0 14 13 23 13 0 0 23 Nash equilibrium Next let generalize results weak ESS As saw previous section uniform σ distributions weak ESS contain isolated synonyms homonyms This means evolutionary stuck sub optimal state average communicative efﬁciency smaller consequence having homonyms language andor able express meanings Below ambiguous languages stable degenerate case uniform probability distribution σ As soon lift degeneracy homonyms disappear language 34 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Theorem 58 For nonuniform σ distributions language µ weak ESS kind synonyms isolated synonyms It contain homonyms Proof First Nash language nonuniform σ distributions contain isolated homonyms Let assume exists Nash language µ contains string l isolated homonyms The fact µ Nash means µ best response F µ µ supµcid2 F µ µcid2 Let follow algorithm Section 322 construct X matrix In order X matrix contain sting homonyms Q matrix contain means P Λ matrix contain string l identical elements This turn means P matrix contain string aσ1 aσl Since values σ1 σl means elements string aσ1 aσl matrix P identically equal zero remember columns P sum Therefore matrix µ contain nonzero elements string homonyms isolated contradiction Now need language µcid2cid2 contains synohomonyms language µ F µcid2cid2 µ F µcid2cid2 µcid2cid2 F µ µ F µcid2cid2 µcid2cid2 We proceed building language µ exactly proof Theorem 56 The difference emerges consider function F µ µ Before value depend values elements corresponded isolated homonyms Say string α1 αl cid25Q matrix corresponding elements l i1 αi equal Now enter linear combination entered cid2 l i1 σi αi order maximize contribution αk 1 αj 0 j cid17 k σk largest σi Of course means support resulting matrix cid25Q smaller support Qcid2cid2 use language maximizer However σk 1 ε σj εl 1 By choosing ε small ﬁnd language µ satisfying 27 cid2 We conclude weak ESS languages contain homo nyms Note Nash equilibria case nonuniform σ distributions contain isolated synonyms The important difference isolated synonyms introduce ambiguity language We conclude case general distributions ESS weak ESS correspond states perfect coherence ambiguities present language The possible source reduction average communicability function come poverty absence certain meanings language Remark 59 Theorems 56 58 proven assume matrices P Q connected common matrix µ proofs longer ones presented N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 35 6 Conclusions We considered linguistic agents characterized language measure µ signalmeaning space The mutual intelligibility agents characterized natural way simple probability transmit signals successfully ways We studied problem optimizing mutual intelligibility linguistic agents shared environment It turned given language µ0 language leads mutual intelligibility higher achieved µ0 The exceptions languages correspond Nash equilibria instance permutation languages Moreover family languages µε exists leads optimization intelligibility ε 0 We identiﬁed algorithm construct languages external noise The results consequence learning theory It apparent order maximize intelligibility offspring better learning best response languages simply copying language parents population We identiﬁed algorithms learning task calculated efﬁciency From evolutionary prospective identify languages correspond evolutionary stable strategies language game It turns strict ESS stable equilibria languages relate signals meanings onetoone way The weak ESS neutral equilibria dynamics contain isolated synonyms homonyms means ambiguity language exception degenerate case meanings occur exactly frequency Appendix A Inﬁnite matrices Here present algorithmic proof Theorem 2 inﬁnite languages The main difﬁculty best decoder best encoder deﬁned way ﬁnite matrices formulas 8 9 In present case need equivalent matrices P Q constructed We prove following Lemma A1 For ε0 exists integer N pair cid25P cid25Q N N matrices cid12 cid12 cid12 cid12 cid12 sup µ cid12 cid12 cid12 cid12 cid12 sup µ 1 2 1 2 cid7 cid7 l1 cid7 σl σl k1 cid7 l1 k1 PklQ0 kl 1 2 P 0 klQkl 1 2 cid12 cid12 cid12 cid12 cid12 ε0 cid12 cid12 cid12 cid12 cid12 ε0 cid25P klQ0 kl P 0 kl cid25Q kl Ncid7 Ncid7 l1 Ncid7 σl σl k1 Ncid7 l1 k1 A1 A2 36 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Remark A2 The matrix cid25P said ε0 best decoder matrix cid25Q ε0 best encoder Finding matrices equivalent controlling behavior respectively second ﬁrst terms expression F µ0 µ F µ0 µ 1 2 cid7 cid7 cid10 σl l1 k1 klQkl PklQ0 P 0 kl cid11 Proof First language µ ε1 0 exists integer L cid12 cid12 cid12 cid12 cid12 cid12 cid12 cid12 cid12 cid12 1 2 1 2 cid7 cid7 l1 cid7 σl σl k1 cid7 l1 k1 P 0 klQkl 1 2 PklQ0 kl 1 2 cid12 cid12 cid12 cid12 cid12 ε1 cid12 cid12 cid12 cid12 cid12 ε1 P 0 klQkl PklQ0 kl µcid7 cid7 l1 Lcid7 σl σl k1 cid7 l1 k1 A3 A4 A5 This σl measure tails 1 cid2 2 lL σl kl small adjusting L The inequalities klQkl 1 2 k1 PklQ0 k1 P 0 lL σl cid2 cid2 cid2 hold supremum values terms Let concentrate ﬁrst term F L0 L For language L ε1 0 K1 cid12 cid12 cid12 cid12 cid12 1 2 Lcid7 cid7 σl l1 k1 klQkl 1 P 0 2 cid12 cid12 cid12 cid12 cid12 ε1 P 0 klQkl Lcid7 K1cid7 σl l1 k1 A6 P 0 kl measure index k Thus behavior ﬁrst term F L0 L controlled inﬁnity limiting range l L range k K1 introduces error smaller 2ε1 For 1 cid1 k cid1 K1 let deﬁne lk P 0 kl assume simplicity property unique maxima holds Then set nearly best decoder cid25Q maxl P 0 klk cid1 cid25Q kl l lk 1 0 cid2L 1 2 l1 σl Clearly supL obtain cid12 cid12 cid12 cid12 cid12 sup L cid7 σl cid7 1 2 l1 k1 cid2K1 k1 P 0 kl Qkl 1 2 A7 cid2L l1 σl cid2K1 k1 P 0 kl cid25Q kl Therefore P 0 klQkl 1 2 cid12 cid12 cid12 cid12 cid12 2ε1 P 0 kl cid25Q kl Lcid7 K1cid7 σl l1 k1 A8 Next turn second term F µ0 µ Its behavior harder control k kl measure respect index k However ml following construction We note l l1 σl il contained 0 1 Therefore direction Q0 cid2L approach supL 1 cid1 l cid1 L sequence Q0 k1 PklQ0 1l Q2l Q0 cid2 1 2 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 37 supk Q0 kl ε1 Now deﬁne nearly best A9 cid2L l1 σl supk Q0 kl Therefore set K2 1 2 ﬁnd kl Q0 encoder follows kll cid1 cid25P kl k kl 1 0 cid2 cid2L We supµ maxi ki obtain cid7 Lcid7 1 2 l1 σl k1 PklQ0 kl PklQ0 kl 1 2 cid12 cid12 cid12 cid12 cid12 ε1 cid25P klQ0 kl Lcid7 K2cid7 σl l1 k1 σl l1 k1 By combining inequality A5 1 2 cid7 cid7 σl l1 k1 PklQ0 kl 1 2 Lcid7 K2cid7 σl l1 k1 cid25P klQ0 kl cid12 cid12 cid12 cid12 cid12 2ε1 Next let N maxL K1 K2 It possible Ncid7 K1cid7 Ncid7 Lcid7 l1 Lcid7 σl σl k1 K2cid7 l1 k1 P 0 kl cid25Q kl 1 2 cid25P klQ0 kl 1 2 P 0 kl cid25Q kl cid25P klQ0 kl l1 Ncid7 σl σl k1 Ncid7 l1 k1 cid12 cid12 cid12 cid12 cid12 ε1 cid12 cid12 cid12 cid12 cid12 ε1 cid12 cid12 cid12 cid12 cid12 sup µ 1 2 cid12 cid12 cid12 cid12 cid12 sup µ cid12 cid12 cid12 cid12 cid12 cid12 cid12 cid12 cid12 cid12 1 2 1 2 Finally set ε1 ε03 Combining formulas A8 A13 obtain inequality A2 Combining formulas A11 A12 obtain inequality A1 Now present proof Theorem 38 inﬁnite matrices Proof Following algorithm ﬁnite matrices developed Section 31 let construct family N N languages Lε cid12 cid12 cid12 cid12F µ0 µε 1 cid12 2 Ncid7 Ncid7 σl l1 k1 cid8 P 0 kl cid25Q kl cid25P klQ0 kl cid9 cid12 cid12 cid12 cid12 cid12 ε0 Combining inequalities A1 A2 Lemma A1 obtain cid12 cid12 3ε0 F µ0 µ F µ0 µε cid12 cid12sup µ Thus conclude family languages µε satisﬁes conditions Theorem 38 A generalization case language µ0 property unique maxima straightforward Appendix B Noisy channel From discussion Section 3 clear perfect languages association matrix permutation matrix communicability F µ0 µ0 1 A10 A11 A12 A13 A14 A15 38 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 Therefore best language communicate perfect language language However imagine agent needs communicate perfect language user µ0 noisy channel What optimal language µ communication In section use assumption Section 31 We consider memoryless transmission media introduce M N matrix T Tij probability signal j received listener given signal conveyed speaker We ncid7 j 1 Tij 1 Now F function rewritten following way P 0T TQT cid8 P T TQ0T cid9cid11 cid10 tr tr F µ0 µ 1 2 cid9 cid8 Let introduce effective encoding decoding matrices language µ0 cid25P 0 P 0T T cid25Q0 Q0T We obtain F µ0 µ 1 2 cid8 cid10 tr cid25P 0QT cid9 cid8 tr P cid25Q0T cid9cid11 B1 This deﬁnition formally similar deﬁnition noiseless transmission matrices cid25P 0 cid25Q0 necessarily related common association matrix In case P 0 Q0 identity matrices cid25P 0 T T cid25Q0 T Given matrix T like optimize function F µ0 µ languages µ Let maximize terms expression B1 separately The best encoder Q given picking maximum elements row matrix cid25P 0 matrix T T The best decoder P given picking maximum elements column matrix cid25Q0 matrix T Therefore P QT B2 If language µ P P Q Q F µ0 µ supµ F µ0 µ In general possible ﬁnd language However certain restrictions T matrix approach desired communicability We matrix T rowwise diagonally dominant 1 cid1 cid1 M Tii Tij j cid17 We prove following Theorem B1 If µ0 permutation language T diagonally dominated rowwise supµ F µ0 µ 12M trP 0T TQT P T TQ0T The proof follows logics given Section 31 The key factor closed loops auxiliary matrix combining positive entries P Q This established N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 39 Lemma B2 If µ0 permutation language T matrix rowwise diagonally dominant closed loops auxiliary matrix Proof Consider closed loop α1 β1 going α1 β2 ultimately αK βK ﬁnally α1 β1 Without loss generality assume node α1 β1 corresponds 1 Q matrix Immediately follows Pα1β1 Pα1β β But P T T β Tβ1α1 Tβα1 B3 Now consider α1 β2 For node Q corresponding entry 0 P corresponding entry 1 Since P obtained taking maxima columns Q Q0 Q0 Q T α α1β2 αβ2 Tα1β2 Tαβ2 α B4 Matrix T rowwise diagonally dominant Tii Tik k cid17 Thus Eqs B3 B4 diagonal dominance property Tβ1β1 Tβ1α1 Tα1α1 Tα1β2 Tβ2β2 Now continue α2 β2 use logics We Tβ2β2 Tβ2α2 Tα2α2 Tα2β3 Tβ3β3 This repeated eventually obtain TβK1βK1 TβK βK ﬁnally TβK βK Tβ1β1 This leads contradiction From Lemma B2 follows T dominated diagonal approach supµ F µ0 µ arbitrarily close choosing appropriate language µ The proof Theorem B1 straightforward What interesting languages high communicability µ0 necessarily identity matrices Here Example B3 Consider following 3 3 T matrix cid20 T 046 045 009 03 04 03 047 005 048 cid21 40 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 For matrix cid21 cid20 P 0 0 1 1 0 0 0 0 1 Q cid20 0 0 1 0 0 0 cid21 1 0 1 Then auxiliary matrix X combined positive elements P Q matrices given X cid20 0 1 1 cid21 1 1 0 0 0 1 It symmetrical contains closed loops construct following family languages Aε cid20 0 ε2 ε ε2 0 0 cid21 ε 0 1 As ε tends zero language Aε tends best response perfect language µ0 noisy channel T Finally note µ0 permutation language T rowwise diagonally dominated supµ F µ0 µ cid1 12M trP 0T TQT P T TQ0T inequality strict demonstrated Example B4 The language µ0 transition matrix T given cid20 µ0 078 003 058 072 094 020 034 062 040 cid21 cid20 T 072 000 028 028 043 029 002 041 057 cid21 It turns best decoder best encoder case given P cid20 0 1 0 0 0 1 cid21 1 0 0 Q cid20 1 0 0 1 0 0 cid21 0 0 1 leads following auxiliary matrix closed loop 1 1 X cid20 1 1 0 cid21 0 1 1 0 1 1 1 1 1 1 This suggests ﬁnding best encoder best decoder help optimize communicability function N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 41 References 1 C Boutilier Y Shoham MP Wellman Economic principles multiagent systems editorial Artiﬁcial Intelligence 94 12 1997 16 2 EJ Briscoe Language Evolution Language Acquisition Formal Computational Models Cambridge University Press Cambridge 2000 3 E Charniak Statistical Language Learning MIT Press Cambridge MA 1993 4 DL Cheney RM Seyfarth How Monkeys See World Inside Mind Another Species University Chicago Press Chicago IL 1990 5 NA Chomsky Language Problems Knowledge MIT Press Cambridge MA 1986 6 DR Dowty RE Wall S Peters Introduction Montague Semantics Kluwer Academic Dordrecht 1980 7 AL Gorin SE Levinson AN Gertner Adaptive acquisition spoken language Proc ICASSP91 Toronto ON 1991 pp 805808 8 MD Hauser The Evolution Communication MIT Press Cambridge MA 1997 9 J Hofbauer K Sigmund Evolutionary Games Replicator Dynamics Cambridge University Press Cambridge 1998 10 JR Hurford Biological evolution Saussurean sign component language acquisition device Lingua 77 1989 187222 11 C Isbell C Shelton M Kearns S Singh P Stone A social reinforcement learning agent Proc Agents 2001 Montreal QB 2001 pp 377384 12 S Kirby Syntax learning The cultural evolution structured communication population induction algorithms D Floreano JD Nicoud F Mondada Eds Advances Artiﬁcial Life 5th European Conference Lausanne Switzerland Lecture Notes Computer Science vol 1674 Springer Berlin 1999 pp 694703 13 NL Komarova P Niyogi MA Nowak Evolutionary dynamics grammar acquisition J Theor Biol 209 1 2001 4359 14 NL Komarova MA Nowak Evolutionary dynamics lexical matrix Bull Math Biol 63 3 2001 451485 15 JM Macedonia CS Evans Variation mammalian alarm systems problem meaning animal signals Ethnol 93 1993 177197 16 C Manning H Schutze Foundations Statistical Natural Language Processing MIT Press Cambridge MA 1999 17 J Maynard Smith Evolution Theory Games Cambridge University Press Cambridge UK 1982 18 J Maynard Smith G Price The logic animal conﬂict Nature 246 1973 1518 19 GA Miller The Science Words Scientiﬁc American Library New York 1996 20 M Oliphant The dilemma Saussurean communication BioSystems 37 12 1996 3138 21 M Oliphant Formal approaches innate learned communication Laying foundations language PhD Thesis Univ California San Diego CA 1997 22 M Oliphant The learning barrier Moving innate learned systems communication Adaptive Behavior 7 1999 371384 23 M Oliphant J Batali Learning emergence coordinated communication Center Research Language Newsletter 11 1 1997 24 MA Nowak An evolutionarily stable strategy inaccessible J Theor Biol 142 1990 237241 MA Nowak Stochastic strategies prisoners dilemma Theor Pop Biol 38 1990 93112 25 MA Nowak NL Komarova P Niyogi Evolution universal grammar Science 291 2001 114118 26 T Regier B Corrigan R Cabasan A Woodward M Gasser L Smith The emergence words Proceedings 23rd Annual Conference Cognitive Science Society Edinburgh 2001 27 T Regier Emergent constraints wordlearing A computational review Trends Cognitive Sciences 7 2003 263268 28 F Saussure C Bally A Sechehaye Eds Course General Linguistics Duckworth London 1983 Translated annotated Roy Harris 29 K Smith The cultural evolution communication population neural networks Connection Sci 14 1 2002 6584 42 N Komarova P Niyogi Artiﬁcial Intelligence 154 2004 142 30 K Smith The transmission language Models biological cultural evolution PhD Thesis University Edinburgh 2003 31 WJ Smith The Behavior Communicating Harvard University Press Cambridge MA 1977 32 WJ Smith The behavior communicating years DH Owings MD Beecher NS Thompson Eds Perspectives Ethnology vol 10 Plenum Press New York 1997 pp 751 33 L Steels Selforganizing vocabularies C Langston Ed Proceedings ALife V Nara Japan 1996 34 L Steels F Kaplan Spontaneous lexicon change Proceedings COLINGACL Montreal QB 1998 pp 12431249 35 L Steels P Vogt Grounding adaptive language games robotic agents P Husbands I Harvey Eds Proceedings Fourth European Conference Artiﬁcial Life MIT Press Cambridge MA 1997 36 JB Tenenbaum F Xu Word learning Bayesian inference Proceedings 22nd Annual Conference Cognitive Science Society Philadelphia PA 2000 37 F Tohme T Sandholm Coalition formation processes belief revision bounded rational self interested agents J Logic Comput 9 6 1999 793815 38 PE Trapa MA Nowak Nash equilibria evolutionary language game J Math Biol 41 2000 172 188 39 VN Vapnik Statistical Learning Theory Wiley New York 1998 40 P Vogt H Coumans Investigating social interaction strategies bootstrapping lexicon development J Artiﬁcial Soc Social Simul 6 1 2003