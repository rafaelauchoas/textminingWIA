Artificial Intelligence 78 1995 327354 Artificial Intelligence Localization homing combinations model views Ronen Basri avl Ehud Rivlin b23 Department Applied Mathematics The Weizmann Institute Science Rehovot 76100 Israel Computer Science Department Technion Haifa 32CO0 Israel Received August 1993 revised November 1994 Abstract identifying Navigation honing involves recognizing environment positioning current position environment act environment reaching particular positions We present method localization act computing exact coordinates recognizing robot environment act returning previously visited position visual input The method based representing scene set 2D views predicting appearances novel views linear combinations model views The method accurately approximates appearance scenes weakperspective projection Analysis projection experimental results demonstrate cases approximation sufficient accurately scene When weakperspective approximation invalid larger number models acquired iterative solution account perspective distortions employed The method advantages approaches It uses relatively rich representations representations 2D 3D localization single 2D view calibration The principal method applied localization positioning problems simple qualitative algorithm homing derived method Corresponding author Email ronenwisdomweizmannacil This report describes research Massachusetts Institute Technology Artificial Intelligence Laboratory McDonnellPew Center Cognitive Neuroscience Support laboratorys artificial intelligence research provided Advanced Research Projects Agency Department Defense Office Naval Research Contract NOOO1491J4038 Email ehudrcstechnionaciI 3 This repolt describes research University Maryland Computer Vision Laboratory Center Automation Research The second author suppolted Defense Advanced Research Projects Agency ARPA Order No 8459 US Army Engineer Topographic Laboratories Contract DACA7692C0009 00043702950950 1995 Elsevier Science BV All rights reserved SSDIOOO4370295000216 328 R Basri E Rivlin Artk fntelligence 78 1995 327354 1 Introduction Basic tasks autonomous robot navigation localization positioning act recognizing environment assigning homing consistent locations different Localization labels robot sense position placespecific previously visited position coordinate environment positioning Positioning act computing task complementary coordinates localization specified room 911 Homing task returning 15 meters northwest table r A method localization positioning homing visuallyguided navigation presented The method based 201 represents systems images Localization achieved comparing model views The position robot computed analyzing linear combination solution homing problem scheme observed presented aligns model scenes sets 2D image linear combinations coefficients image Also simple qualitative Visuallyguided navigation systems classified according utilized We distinguish represent images scene representation These invariant representations 3D models Systems generate large range transformations projecting measurements observed straightforward way images comparing image data lower dimensional data Localization obtained types representations type scene signatures scene set signatures usually invariant relatively obtained set subspace computing signatures signatures stored signatures achieved generating representations Sarachik McDermott erates signatures Braunegg map obtained projecting signatures 61 use blurred averaged orientations 171 computes stores dimensions navigated offices Engelson 141 gen regions image 4 recovers depth map scene generates occupancy 3D edges floor Hong et al 9 generate images scene signatures Nelson edges different panoramic views scene projecting 1D circle Other systems store complete 3D descriptions scene To recognize relates transformation I use trinocular images Ayache Faugeras systems recover incoming 3D structure scene 151 use stereo align positions image Fennema images Grayscaled location landmarks stereo image model set landmarks transformation 3D models scene landmarks generated derive et al 71 compare templates selected recover depth map observed stereo scene model recover et al located relates sequences 2D model scene In order model computed means correlation tracking compared model Onoguchi The method presented explicit 3D descriptions 2D images Predicting model views paper generate signatures scene However scene scene represented sets appearances novel views obtained combining Homing recently addressed studies Nelson 141 Zipser 22 R Basri E RivlinArtificial Intelligence 78 1995 327354 329 proposed handle problem generating signatures scene single images storing vectors directing robot target location At runtime robot encounters signature similar stored signatures follows precomputed direction vectors associated signatures Hong et al 9 perform homing comparing signatures obtained panoramic view scene similar signature obtained target location The robot instructed bring observed signature target signature alignment The method homing presented paper differs previous algorithms use signatures represent scene Homing achieved moving robot align observed images scene image taken target position Like 91 algorithm computes direction motion fly The algorithm qualitative nature designed gradually bring current target images alignment The rest paper organized follows The method localization presented Section 2 propose method works accurately weakperspective approximation iterative scheme account perspective distortions Positioning addressed Section 3 algorithm homing described Section 4 Constraints imposed motion robot result special properties indoor environments reduce complexity method presented This topic covered Section 5 Experimental results follow 2 Localization The problem localization defined follows given P 2D image place M set stored models model M E M P matches M One problem localization address variability images viewpoint changes The inexactness practical systems makes difficult robot return specified position subsequent visits The visual data available robot visits varies accordance viewing position robot A localization able recognize scenes different positions orientations Another problem changes scene At subsequent visits place look different changes arrangement objects introduction new objects removal In general objects tend static While chairs books moved tables closets pictures tend change position frequently walls guaranteed static Static cues naturally reliable mobile ones Confining static cues cases result failure recognize scene insufficient cues The attempt rely static cues ignore dynamic cues We interested recognize environment different viewing positions update representations dynamically accommodate changes scene A common approach handling problem recognition different viewpoints comparing stored models observed environment 330 R Bnsri E RivlinArfQScia Intelligence 78 1995 327354 approach recovered compensated viewpoint number studies object recognition alignment based Linear Combinations parts In Section 2 I weakperspective handling problem localization Below localization divided works approximation The second Section 22 proposes method This approach called alignment We apply 201 The presentation basic large perspective distortions 3810131819 scheme 21 Localization weakperspective assumption ycoordinates xcoordinates correspondence feature points points An object The scheme localization image contains following Given image construct view case modeled set views points views predicted linear The appearance novel view object stored views The coefficients linear combinations recovered small number mode1 points corresponding vectors points contains environment ordered applying combination image points To verify match predicted appearance image object recognized line segments First viewercentered models composed 2D views observed predicted Formally compared actual twofold ones scene Second novel appearances simple accurate way weakperspective given P 2D image scene M verification The advantage method match A large number points objectcentered model M E M objective E R It shown scheme accurately predicts objects weakperspective limitations projection model discussed set stored models P cf LYjM constants appearance rigid 201 The orthographic projection scale later paper representations projection projection More concretely weakperspective given let p x yi Zi 1 6 n set n object points Under image position p xi y points projection x STIIX sy12y STIJZ r y sr21x sr22yi smz t 1 rij components ty amounts horizontal form obtain vector equation 3 x 3 rotation matrix s scale factor tx respectively Rewriting vertical translation I x srllx srl2y srl37 tl y n21x sr22y sr23z tl 2 xy z x y E R vectors x yi zi xi y coordinates lll lT Consequently respectively xy E spanxy zl 3 R Basri E RivlinArtcial Intelligence 78 1995 327354 331 z vector depth coordinates words x y belong Notice subspace This spanned scene supply y2 location vectors n points ala2a3aq linearly vectors blb263b4 fact Section 22 A fourdimensional fourdimensional linear subspace R projected points belongs space vectors space Two views See 1 I Denote xt y x2 independent 16201 images exist coefficients x alxi Y bixt bzy 63x2 b41 a2y agx2 a41 4 Note vector y depends vectors Since R rotation matrix quadratic constraints coefficients following satisfy aaaibbibz 2blb3 ala3rll 2bzb3 a2a3r12 albl a2b2 ah alb3 asbl r11 a263 a362r12 0 5 constraints transformation To derive recovered This weakperspective constraints mations affine ones This usually prevent generally scenes fairly different ignored model views image Alternatively rigid transfor localization successful case confuse Note incorporate Points visible model We extend See images scene appear model images model points images occlusion models additional points taking 201 excluded quadratic model aligns quadratic environment To summarize model effect noise After coefficients constraints stored Localization set images correspondence images For example spot modeled corresponding views The corresponding achieved recovering observed linear combination image The coefficients determined model points corresponding image points solving linear set equations Three points sufficient determine coefficients considered Additional constraints points reduce recovered use predict appearance points model stage The predicted appearance verify match When example testing possible matches quadruples feature points image model In case worstcase time complexity km4n4m k number models considered m number model points n number image points typical m number points considered alignment constraints proposed coefficients unconstrained applying reduce complexity recovering feature points process compared ignored verification This complexity recovery coefficients reduced considerably schemes This complexity quadratic constraints Section 5 A method actual image model All localization transformation quadruples described 211 332 R Basri E RivlinArtijicial Intelligence 78 1995 327354 The recovery alignment coefficients defined follows Denote M XiYX211 matrix model points let b denote vectors coefficients Mx b My 6 7 M Mf M Mt MTM MT pseudoinverse points Note recovery stage M x y contain coordinates points recovery process hypothesized match The sensitivity determined condition number M The robustness recovery process increased choosing quadruples model points arising nonplanar set matches additional points errors recovery process generate overdetermined extending configurations types features assign weights In scheme distinguish static semistatic dynamic cues To handle weights points scene stages In recovery stage let w vector different reliability We use different criteria number occurrences higher points recovering weights assigned determine subsequent visits height points tend static The weights incorporated model points let W gw model points verification coefficients reflecting WMWx b WMWy In verification matched positions stage distances predicted positions model features image weighed according uses viewercentered models w representations Our scheme localization small transformations avoid need handle occlusions It number advantages methods represent composed images scene First viewercentered models threedimensional models cover relatively scene If viewpoints scene appears different occlusions utilize new model viewpoints Second viewercentered models easier build maintain images model images correspondences By limiting correspondence If large changed visits new model constructed portions environment simply replacing old images new ones ones The models contain epipolar constraints motion methods objectcentered transformation build 212 The number models required scene A complex scene depends complexity require relatively require relatively location appears represent sufficient cover scene possible viewing positions containing aspects large number views In practice navigation rough small number models Specifically represent robot recognize appearance room threshold One model case See Section 5 scheme An analysis weakperspective robot need access routes For example recognize environment room environment weakperspective One problem localization approximation scheme assumption R Basri E RivlinArtificial Intelligence 78 1995 327354 333 given Appendix A In contrast problem object recognition assume objects small relative distance camera localization environment surrounds robot perspective distortions neglected The limitations weakperspective modeling discussed math rest paper It shown ematically empirically practical cases weakperspective sufficient enable accurate localization The main reason problem localization require accurate measurements entire image requires identifying sufficient number spots guarantee accurate naming If spots relatively close center image depth differences create relatively small case looking wall line sight nearly perpendicular wall perspective distortions relatively small identify scene high accuracy Also views related translation parallel image plane form linear space perspective distortions large This case simplifications discussed Section 5 By weakperspective avoid stability problems frequently occur spective computations We compute alignment coefficients looking relatively narrow field view The entire scheme viewed accumulative process Rather acquiring images entire scene comparing scene model 41 recognize scene image image spot spot accumulate sufficient convincing evidence indicates identity place When perspective distortions relatively large weakperspective insufficient model environment approaches One possibility construct larger number models possible changes familiar novel views small Alternatively iterative computation applied compensate distortions Such iterative method described Section 22 22 Handling perspective distortions The scheme presented accurately handles changes viewpoint assuming images obtained weakperspective projection Error analysis experimental results demonstrate practical cases assumption valid In cases perspective distortions large handled weakperspective approxima tion matching model image facilitated ways One possibility avoid cases large perspective distortion augmenting library stored models additional models In relatively dense library usually exists model related image sufficiently small transformation avoiding distortions The second alternative improve match model image iterative process In section consider second option The suggested iterative process based Taylor expansion perspective coordinates As described expansion results polynomial consisting terms approximated linear combinations views The term series represents orthographic approximation The process resembles method matching 3D points 2D points described recently DeMenthon Davis 51 In case method applied 2D models 3D 334 R Basri E RivlinArtcial Intelligence 78 1995 327354 ones In application approximated 3D coordinates model points provided model views instead x y fXZ pZ projection image f denotes focal length Consider object point following Taylor image point An X E Z expansion lZ depth value Zc c Z O 1 lkk k Zk The Taylor series describing position point x given 9 Notice zero term contains kth term series orthographic approximation x Denote Ack Ak fX _ zo 11 A recursive definition series given l Initialization xO A 0 c zo l Iterative step Xk Xkl Ak xck represents highestorder term xck kthorder approximation x Ack represents approximation orthographic model views According combinations approximating X Z step model points image points The general estimate xO AO solving X Z expressed linear Eq 4 We apply procedure best aligns following First case Then step linear combination idea orthographic R Bash E Rivlin Artificial Intelligence 78 1995 327354 335 iteration improve factor estimate seeking linear combination best estimates Denote x E R vector image point coordinates denote p xIYnll 12 13 position points II x 4 matrix containing P PTP PT pseudoinverse denote coefficients combination f constant vectors computed values x A kth step An iterative procedure align model images Denote Also linear X Z values Since ZQ linear combination Denote xc Ack P assume P overdetermined kth step Pack represents computed step approximate merged described model image computed l Initialization Solve orthographic approximation xO 40 PaO 0 Iterative step _ I k x 4 aw pqk L_ A1 A pa A Xkl Ak vector operations 8 defined avoid improve false matches The method presented meant overall match effects One problem applying errors perspective distortion In kinds errors One possible model image reducing perspective false matches method mistake priori general distinguish way orthographic distortions Then extend points deviate predetermined eccentricity point image expected depth value Appendix A Finally obtained reasonable perspective image bound The bound determined analysis If poor match iterative procedure match This procedure guarantees set feature points matching model points following procedure First apply solution allowing iterative procedure solution evaluate convergence applying repeat run 336 R Bash 15 RivlinArtijicial Intelligence 78 1995 327354 solution disadvantage increasing polynomialtime correspondence probabilistic methods reduce complexity additional cues stereo color texture previous knowledge sonar detect large variations perspective distortion anticipated orthographic solution Heuristics combinatorics instruments problem relative 3 Positioning problem recovering fixed coordinate Positioning specified room coordinates expressed respect section derive position robot alignment exact position robot This position In case location model views acquired associated model associated environment position coefficients image P align We assume model composed images Pt P2 relative position local given Given novel robots position ization By considering relative recovered To recover absolute position robot room absolute positions model views provided Note computation Positions coefficients linear combination assuming unit focal length image coordinates model image normalized model images Assume P2 obtained average distance camera t7 The coordinates point s Denote combinations corresponding model points following way t t t t scaling PI scene s given x y written linear P world coordinates PI rotation R translation x alxl u2y1 u3x2 04 J hxi b2y1 t 13x2 04 Substituting x2 obtain x UlXl a2y1 3WlXl sr12yj sr13z1 tx u4 v blxl b2yI b3srIIxI srl2yI sr131 tx b4 rearranging equations obtain x a3w1x1 a2 u3w2yI a3sr13zl a3tx u4 Y h bwIIxI h b3srl2yl b3sr13a b3t a4 14 15 16 equations derive parameters transformation Using model image Assume scaling s Using orthonormality obtained rotation U translation t constraint derive scale factor image 4 al k a3w1 a2 u3sr12 u3srl3 uaus22u3sur u2rl2 17 R Basri E Rivlin Artcial Intelligence 78 1995 327354 337 Note extract scale factor applying constraint bs s 6 6 bv 2b3Sblrl b2r12 18 We use equations verify weakperspective approximation valid The orthogonality constraint Eq 5 purpose From Equations 16 17 deriving components translation vector t obtain position robot image relative position model views Ax a3tx a4 Ay b3tv h Azt1f 19 Note AZ derived change scale object The rotation matrix U PI P given Ull al a3wl sil a2 a3sn2 U12 St a3w3 u13 Sll U21 U22 h b21 sn b2 b22 8 b3 St23 U3 Sll 20 As mentioned position robot computed relative position camera model image PI acquired AX AZ represent motion robot PI P rest parameters represent 3D rotation elevation To obtain relative position transformation parameters model views PI P2 required Consequently positioning unlike localization requires calibration model images One note results positioning process depend precision alignment coefficients erroneous bad choice correspondences invalid orthographic approximation In cases errors coefficients recovery Ax Ay depend linearly errors AZ inversely dependent errors This sensitivity AZ typical processes recovering depth stereo motion We note positioning general performed localization achieved estimate coefficients improved large number points Section 4 presents alternative process lead robot desired positions use feedback sensitive errors require calibration model images 4 Homing The homing problem defined follows Given image called target image position location image observed One way solve problem extract exact position target image 338 R Busri E RivlinArtijiciul Intelligence 78 1995 327354 obtained qualitative robot observes Unlike transformation direct robot approach Under position approach position In section interested computed environment extracts direction target Instead location recovery exact approach method presented require model views In words assume We assume given model environment new images moving image The robot allowed begin assuming horizontally moving platform degrees freedom robot allowed rotate translate horizontally The validity constraint section shall consider homing computation determines step robot acquires new image aligns alignment step The algorithm identifiable point moves circular path line sight point coincides line sight corresponding target image In second stage robot advances target target We vertical axis Section 5 Later 3D case Below simple target location At time stages In stage robot fixates fixation point point forward retreats backward model By comparing target image robot determines coefficients coefficients path terminates target location discussed divided reaches Given model composed images PI P2 P2 obtained rotation Given target translation expressed Fig 1 Yaxis angle cy horizontal image P P obtained tt scale s Using Eq 4 translation PI similar Pi t scale factor s rotation angle 0 position target point xy y 21 The rest coefficients given coefficients zero platform moves horizontally In fact al ssincu 6 sinff trst sin 6 u4 t _ ___ ssina CI st sin 0 s sin Y b2 s 22 The derivation At given Appendix B time step robot acquires image aligns model image P obtained result rotation angle 4 translation Assume t scale s The position point xI y expressed 23 X ClXl c3x2 4 I d2Y coefficients given R Basri E Rivlin Artificial Intelligence 78 I 995 327354 339 Fig 1 Illustration hommg task PI Pz model images separated angle Y The target image separated PI angle 8 robot positioned angle 4 PI C sy sin ff 4 sina c4 t tsp sin 4 ssinff c3 sp sin 4 s sin c d2 s The step performed robot determined That s s sina 4 sin ssina 19 sin6 ssinacot4 cotf3 24 25 The robot reduce absolute value 6 The direction motion depends sign LY The robot deduce direction moving slightly checking increase decrease 6 The results defined follows The robot moves right left depending motion direction ISI step Ax motion reduces A new image P acquired new position x Since motion image Denote image plane depth values point views Pp P identical We want rotate camera return original position The angle rotation jl deduced fixated point equation fixated point located parallel xP xcospsinfi 27 This equation solutions We chose counters translation angle rotation In time step new picture P replaces P translation left keeps right small camera rotate 340 R Basri E RivlinArtijicial Inrelligence 78 1995 327354 procedure focus repeated 6 vanishes The resulting path circular point Once coincides retreat backward satisfies determine robot arrives position target adjust position image 4 6 6 0 advance line sight forward line sight Several measures term c3us direction motion example 2 a3 _ J sf 28 lines sight coincide The objective stage bring measure 1 t scaling s Given target A similar process formulated images Pr PI P2 obtained vector tion U translation image aligns result rotation U translation circular path attempting terms t scaling minimize image Pt Pt obtained 3D case Given model composed Pi rotation matrix R translation PI rota s As time step robot acquires image P obtained takes absolute value t scaling simultaneously s Again robot model Assume As shown Appendix B 30 k 1 4 sign exists circular path decreases term 113 depends model constant putation The signs ponents current target image Note nents determine values lar path direction searching com depend rotation com rotation compo absolute sought circu Efficient methods pa Sks simultaneously possible directions discussed maximizes possible directions example searching The direction pointing terms simultaneously change Once robot arrives position 0 k 1 4 corresponding equal U U This shown following claim image P corresponding current rotation matrix target image Pt Claim o k 1 4 implies I U R Bad E RivlidArtificial Intelligence 78 1995 327354 341 Proof 61 0 implies 41 7v u13 a11 u13 Sz 0 implies 42 _ 43 Ml2 u13 As result following vectors identical Notice rows U U normalized versions vectors clearly equal 4 J43 ullv1213 Similarly 83 84 0 implies middle rows U U equal 4 u3 u21vu22u23 row rotation matrix given cross product rows obtain Consequently robot reaches position Sk vanish line sight robot coincides line sight target image In order reach target position robot advance forward retreat backward adjust position line sight Again measure purpose c3 a3 sp sr 31 lines sight coincide The objective stage bring measure 1 5 Imposing constraints Localization positioning require large memory great deal online computation A large number models stored enable robot navigate manipulate relatively large complicated environments The computational cost modelimage comparison high context path history available number required comparisons large To reduce computational cost number constraints employed These constraints advantage 342 R Busri E RivlinArijicinl Intelligence 78 1995 327354 structure robot navigation properties indoor environments natural properties task This section examines constraints One thing attempt set models reduce build order avoid performing looks satisfy perpendicular obtained relatively deep iterative computations condition When perspective distortions subsets views related translation parallel effect perspective distortions Views environment scene usually consider modeling plane roughly equal images considered expressed linear combinations large perspective distortions This apparent Xi x Z let K y projected point applying rigid transformation Assuming Z Zi obtain large image In case depth values points novel views model views presence following derivation Let image xi yi fXiZ I 6 II point projected line sight shown assuming f 1 fiZi Zx rllX r12Y r13Z f Zy r21X r22X r232 Iv Dividing Z obtain x IIX r12y I r13 t Z I yJ r214 r22yi r23 rJz Rewriting vector equation form gives x rilx r12y t131 tC y r2lx ml t 32 33 34 x y x y vectors xi yi xi yi values respectively 1 vector Is z case novel views obtained translation parallel linear combinations weakperspective image plane expressed vector lZ values Consequently vectors An indoor environment usually provides motion camera constrained XZplane instead degrees freedom support rotation vertical degrees constraint Such motion genera1 case Under align mode1 image For example robot flat horizontal required coefficients u2 01 b3 b4 0 Three points coefficients solving linear Two possible fact motion constrains images This fact guide task correspondence considered Another advantage quadratic constraints Consequently I axis translation freedom fewer correspondences Eq 4 required required considering epipolar seeking lines determine horizontal motion Objects indoor environments ticular relatively static objects appear roughly planar settings tend located walls Such objects In par include R Basri E RivlinArticial Intelligence 78 1995 327354 343 closets tables When shelves pictures valid example windows projection line sight roughly perpendicular views described 2D affine transformation space views scene reduced wall robot assumption relatively distant transformation orthographic wall Eq 4 The dimension x atxt azy a41 y blXl bzy 641 35 1x3 b3 0 Only view sufficient model scene Most officelike indoor environments environment rooms Not points involves maneuvering places robot faces number options important places navigation include thresholds rooms beginnings tend store models environment composed rooms connected corridors corridors entering important direction ends corridors A changing In indoor environment points equally Navigating exiting Junctions places navigation stepping room task include confined example includes decision One important property shared junctions adjacent corridor When robot relatively threshold room It relatively narrow place small areas Consider enter separates room room common behavior enter room avoid The images identifying relevant thresholds narrow views related exclusively rotation relatively easy model view recover The position points This apparent Its Now consider position view obtained rotation R camera The location p new view given assuming following derivation Consider point p X I Z set views room entrance Provided vertical axis Under perspective projection given x y fXZ pZ novel views recovered model view door looking rotation f 1 XTYI rll r12Y 32 121x r22Y r23z r3X r32Y r33z r31x r32y r33z implying x y rlI x n2y r31xr32yr33r31xr32yr33 r13 r21 x r22y r23 relation Depth factor determining simpler rotations Yaxis considered 36 37 views Eq 37 x y xcosLy sina Y xsinff cosa xsincrcosa 38 LY angle rotation correspondence In case LY recovered merely single 344 R Basri E RivlinArtijicial lnfelligence 78 1995 327354 6 Experiments The method implemented images taken indoor environment Images offices A B similar structures taken Panasonic applied Fig 2 Two model views office A Fig 3 Lines extracted upper blocks Right picture contains image Left picture contains search blocks The lines extracted lines Hough transform procedure Fig 4 Matching model office A image office A left matching model office B image right R Basri E Rivlin Artificial Intelligence 78 I 995 32 7354 345 Fig 5 Matching model office A image office obtained relatively large motion forward right camera focal length 700 pixels Semistatic objects heavy furniture pictures distinguish offices Fig 2 shows mode1 views office A The views taken distance 4m wall Candidates correspondence picked following method The image divided equalsize blocks Candidates picked upper blocks assuming upper portion image likely contain static features scene In block dominant lines selected ranked Hough transform procedure A line ranked sum gradient values points The results process shown Fig 3 Feature points obtained intersecting obtained lines Using extracted feature points recovering coefficients linear combination aligns model image method similar 8 lo Quadruples image points matched quadruples model points match mode1 image correspondence evaluated The best match obtained selected The results aligning mode1 views images offices seen Fig 4 The left image contains overlay predicted image white lines constructed linearly combining views actual image office A A good match achieved The right image contains overlay predicted image constructed model office B image office A Because offices share similar structure static cues wall corners perfectly aligned The semistatic cues match features image Fig 5 shows matching mode1 office A image office obtained relatively large motion forward 2m 15m Although distances relatively short perspective distortions negligible good match mode1 image obtained The experiment shows application iterative process presented Sec tion 22 cases large perspective distortion noticeable Fig 6 shows mode1 views Fig 7 shows results matching linear combination mode1 views image office In case image taken Fig 6 Two model views office C Fig 7 Matching model image obtained relatively large motion Perspective distortions seen table board hanger upper right close distance perspective distortions neglected The effects noticed right corner board edges iterative effects reduced procedure iterations shown relatively perspective distortions hanger right Perspective process The results applying Fig 8 Another corridor accurate match results demonstrate model views corridor Fig 10 left scene Here set experiments applied noticeable Nevertheless deep structure corridor perspective distortions large portions image alignment shows overlay Fig 9 shows model views image corridor It seen linear combination parts relatively distant align perfectly Fig 10 right shows matching corridor model image obtained relatively half relatively near features longer corridor far edges match Fig 11 align shows result applying iterative process reducing perspective distortions scene The process converged iterations length Because perspective distortions near door edges The relatively large motion The experimental results demonstrate method achieves accurate localization R Basri E RivlinArtifcial Intelligence 78 1995 327354 347 Fig 8 The results applying iterative process reduce perspective distortions left right iterations Fig 9 Two model views corridor Fig 10 Matching corridor model images corridor The right image obtained relatively large motion forward half corridor length right Note results alignment picture taken roughly conditions Eq 34 left better conditions violated right 348 R Basri E RivlinArtijicial Intelligence 78 1995 327354 Fig I 1 Results applying iterative process reduce perspective distortions iterations cases method iterative computation improve fails large perspective distortions quality match 7 Conclusions We presented method localization linear combinations positioning scene set 2D views predicting visual input The method appearance model views The method accurately projection Analysis cases results demonstrate scenes weakperspective experimental appearances sufficient invalid larger number models acquired iterative scene When weakperspective accurately based representing novel views approximates projection approximation approximation solution employed account perspective distortions Using method presented solution homing problem The solution advantage 2D representation The homing process simple qualitative manner Specifically transformation model images takes image domain recovery require The method presented rich representations paper advantages existing methods 3D single 2D view calibration The basic positioning problems Future work includes indexing complexity localization process building maps localization problem acquisition reduce models constructing representations maintenance 2D It uses relatively localization method handling methods visual input Appendix A Projection modelerror analysis In appendix estimate method assumes weakperspective accurate perspective projection model We start deriving error obtained localization method The assumption error projection model We compare R Basri E RivlidArtifcial Intelligence 78 1995 327354 349 true perspective image orthographic approximation compute error implied assuming weakperspective projection model image A point X Y Z projected perspective model x y fXZ pZ image f denotes focal length Under weakperspective model point approximated 2 9 sX sY s scaling factor The best estimate s scaling factor given s fa Zc average depth observed environment Denote error E 12 xl The error expressed EljX1 Changitq image coordinates E xzl E 1 Z x z1 I I A1 A21 A3 A4 The error small measured feature close optical axis estimate depth Zc close real depth Z This supports basic intuition images low depth variance fixated regions regions near center image obtained perspective distortions relatively small identify scene high accuracy Figs A1 A2 depth ratio ZZc function x E 10 20 pixels Table A1 shows number examples function The allowed depth variance ZZc computed function x tolerated error E For example 10 pixel error tolerated field view f50 pixels equivalent allowing depth variations 20 From discussion apparent model aligned image results alignment judged differently different points image The farther away point center discrepancy tolerated prediction actual image A pixel error position x 50 equivalent 10 pixel error position x 100 So far considered discrepancies weakperspective perspective projections points The accuracy scheme depends validity weakperspective projection model views incoming image In rest section develop error term scheme assuming model views incoming image obtained perspective projection The error obtained scheme given E 1x axI cx2 dl A51 350 R Basri Rivlin Artcial Intelligence 78 1995 327354 Fig A I ZZ function X E IO pixels 0 100 150 200 250 300 Fig A2 ZS function x E 20 pixels Table A1 Allowed depth ratios ZZ function x half width field considered E pixels error allowed d F 2s SO IS 100 5 I2 I1 I 07 I os IO I 4 I 2 113 11 IS 16 13 I 2 115 20 18 I 4 I 21 12 scheme accurately predicts appearances points weakperspective Since projection satisfies A 4 bj ci2 d accented model pictures letters represent orthographic depth ratios roughly equal approximations Assume A61 47 R Basri E RivlinArrcial Intelligence 78 1995 327354 351 This condition camera satisfied translates example image plane Using fact model images xfif_fx ZJ _f z ZfJz z obtain E Ix axl byI cx2 dl A81 A9 center frame difference The error depends terms The gets smaller image points closer depth ratios model image gets smaller The second gets smaller translation gets smaller model gets close orthographic analysis weakperspective projection model component Following depth variations scene relatively center image We conclude environment positioning linear combinations low fixating distinguished scheme localization concentrates parts Appendix B Coefficients values homing In appendix derive explicit values coefficients linear combi case horizontal motion Consider point p x y z projected PI ro t scale factor s P tp scale s nations weakperspective tation obtained The position p images images PI P2 P P2 obtained Yaxis angle translation Yaxis angle 8 translation PI rotation given XltYl KY x2y2 cosaszsinfy 2 sxcosszsinBtsy The point x y expressed linear combination points 352 R Basri E RivlinArtijicial Intelligence 78 1995 327354 I x UlXl u2x2 u3 y Rewriting equations sxcosBszsin8t arasxcoscuszsincuft a3 spy Equating values coefficients sides equations obtain S cos 0 u1 QS cosY r a2tt a3 s sin 0 UPS sin LY s 0 coefficients given ut sp sin Y sina 14 t t sin 0 s sin Y a3 s sin 0 sn sin b s Similarly derive terms describing model composed images PI P2 image Pt P obtained rotation U translation x y expressed 3D case Given PI fr fV tf scaling st position target point coefficients t t _ t x alxl a2yj I u3x2 14 yr hxl bz_m bm bq Using Eq 16 Section 3 obtain coefficients given h stu23 ST13 a4 t ytu13 t so3 x9 Similarly tpx t tZ scaling sp position point x yp expressed PI rotation U translation image P obtained given t X CIXI c2yt c3x2 C4 y IXI d2y1 x2 d4 coefficients given R Basri E RivlinArtificial Intelligence 78 1995 327354 353 cIspuu3 dspuu3 cisuu spu2u3r St d3 w3 d4 tpg ety w13 c3 sr13 c4 t t w13 sr13 We define terms 63_ s Cl 3 a1 a3 j2c2_a2 a3 3 Substituting coefficients obtain sn3 u21 23 u22 G sr3 References III 121 131 141 51 161 171 81 191 101 Image Image Understanding Workshop 1992 875884 orthographic perspective projections recognizing world locations stereo vision AITR1229 MIT N Ayache OD Faugeras Maintaining representations environment mobile robot IEEE Trans Robotics Automation 5 1989 804819 R Basri On uniqueness correspondence Proceedings R Basri S Ullman The alignment objects smooth surfaces Comput Vision Graph Process Image Understanding 57 3 1993 33 I345 DJ Braunegg Marvela Cambridge MA 1990 DF DeMenthon LS Davis Modelbased object pose 25 lines code Proceedings 2nd European Conference Computer Vision Genova Italy 1992 SP Engelson DV McDermott Image signatures place recognition map construction Proceedings SPIE Symposium Intelligent Robotic Systems Boston MA 199 1 C Fennema A Hanson E Riseman RJ Beveridge R Kumar Modeldirected mobile robot navigation IEEE Trans Syst Man Cybernetics 20 1990 1352 1369 MA Fischler RC Belles Random sample consensus paradigm model fitting application image analysis automated cartography Commun ACM 24 198 1 38 I395 J Hong X Tan B Pinette R Weiss EM Riseman Imagebased homing IEEE Control Systems 1992 3844 DP Huttenlocher S Ullman Object recognition alignment Int J Comput Vision 5 2 1990 195212 1 l J Koenderink A van Doom Affine structure motion J Optical Sot America 8 2 1991 377385 121 I Lawn R Cipolla Epipole estimation affine motion parallax Proceedings British Machine Vision Conference 1993 379388 354 K Basri E HivlinArtificial Intelligence 78 1995j 327354 I3 1 DG Lowe Threedimensional object recognition single twodimensional images Robotics Research Technical Report 202 Courant 1985 Institute Mathematical Sciences New York University 1 141 RN Nelson Visual homing associative memory DARPA Image Understanding Workshop 1989 245262 I5 1 K Onoguchi M Watanabe Y Okamoto Y Kuno H Asada A visual navigation local map Proceedings lntrrnutional Conference Robotics Automation result Basri Ullman Technical Report 900503 IRST information multi Cincinnati OH 1990 767774 recognition I6 1 T Poggio 3D object Italy 1990 Povo 17 KB Sarachik Visual navigation constructing utilizing simple maps indoor environment AITR1113 MIT Cambridge MA 1989 1 18 1 DW Thompson JL Mundy Three dimensional model matching unconstrained viewpoint Proceedings Inrernafionul Confererwe Robotics ctnd Auromation Raleigh NC 1987 208220 1 19 1 S Ullman Aligning 193254 pictorial descriptions approach object recognition Crpifion 32 1989 1201 S Ullman R Basri Recognition Intell 13 I99 I 992 1006 Mrrchine linear combinations models IEEE Trans Pattern Ann 21 1 D Wilkes S Dickinson E Rivlin R Basri Navigation based network 2D images Proceedings Infernutionul Conference Puttern Recognition ICPR94 Jerusalem Israel 1994 1221 D Zipser Biologically JL McClelland plausible models place recognition goal location DE Rumelhart PDP Group eds Purullel Distributed Processing Explorurions Murostrucfure Coqnirion 2 Psychologiccd Biologicul Models MIT Press Cambridge MA 1986 43247 I