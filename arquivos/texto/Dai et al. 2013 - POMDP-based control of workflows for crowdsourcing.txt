Artiﬁcial Intelligence 202 2013 5285 Contents lists available SciVerse ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint POMDPbased control workﬂows crowdsourcing Peng Dai Christopher H Lin Mausam Daniel S Weld Department Computer Science Engineering University Washington Seattle WA 98195 United States r t c l e n f o b s t r c t Article history Received 20 December 2011 Received revised form 8 June 2013 Accepted 9 June 2013 Available online 20 June 2013 Keywords PartiallyObservable Markov Decision Process POMDP Planning uncertainty Crowdsourcing 1 Introduction Crowdsourcing outsourcing tasks crowd unknown people workers open rapidly rising popularity It heavily numerous employers requesters solving wide variety tasks audio transcription content screening labeling training data machine learning However quality control tasks continues key challenge high variability worker quality In paper value decisiontheoretic techniques problem optimizing workﬂows crowdsourcing In particular design AI agents use Bayesian network learning inference combination PartiallyObservable Markov Decision Processes POMDPs obtaining excellent costquality tradeoffs We use techniques distinct crowdsourcing scenarios 1 control voting answer binarychoice question 2 control iterative improvement workﬂow 3 control switching alternate workﬂows task In scenario design Bayes net model relates worker competency task diﬃculty worker response quality We design POMDP task solution provides dynamic control policy We demonstrate usefulness models agents live experiments Amazon Mechanical Turk We consistently achieve superior quality results nonadaptive controllers incurring equal cost 2013 Elsevier BV All rights reserved The ready availability Internet world farreaching consequences Not widespread online connectivity revolutionized communication politics entertainment aspects everyday life enabled ability easily unite large groups people world crowd common purpose This ability crowdsource turn opened radical new possibilities resulted novel successes like Wikipedia1 encyclopedia written crowd platforms rapidly impacting worlds laborforce bringing online marketplaces provide work ondemand We believe Crowdsourcing act taking tasks traditionally performed employee contractor sourcing group crowd people community form open call2 potential revolutionize informationprocessing services coupling human workers intelligent machines productive workﬂows 21 Corresponding author Current aﬃliation Google Inc 1600 Amphitheater Pkwy Mountain View CA 94043 Email addresses daipengcswashingtonedu P Dai chrislincswashingtonedu CH Lin mausamcswashingtonedu Mausam weldcswashingtonedu DS Weld 1 httpenwikipediaorg 2 httpenwikipediaorgwikiCrowdsourcing 00043702 matter 2013 Elsevier BV All rights reserved httpdxdoiorg101016jartint201306002 P Dai et al Artiﬁcial Intelligence 202 2013 5285 53 Fig 1 A handwriting recognition task successfully solved Mechanical Turk iterativeimprovement workﬂow Workers shown text written human iterations deduced message minimal errors highlighted Figure adapted 37 While word crowdsourcing coined 2006 area grown rapidly economic signiﬁcance emergence generalpurpose platforms Amazons Mechanical Turk3 taskspeciﬁc sites centers4 program ming jobs5 37473324 Crowdsourced workers motivated variety incentives Common incentives include entertainment context playing games purpose 6310 contribution science6 sense community monetary rewards While work applicable crowdsourcing scenarios focus ﬁnancially motivated microcrowdsourcing crowdsourcing small jobs exchange monetary payments Labor markets like oDesk handle medium largesized tasks requiring diverse range skills microcrowdsourcing popular requesters use concert smallsized jobs handle wide variety higherlevel tasks audio transcription language translation calorie counting 42 helping blind people navigate unknown surroundings 7 It immensely popular workers developed developing countries 49 On popular microcrowdsourcing platforms like Mechanical Turk requesters use workﬂows series steps complete tasks For instance simple image classiﬁcation task Is lion picture workﬂow simple asking workers binarychoice question suﬃce For diﬃcult task like handwriting recognition task shown Fig 1 requester create complicated workﬂow like iterative improvement workers incrementally reﬁne solutions improvement necessary 37 A plethora research shows simple binary classiﬁcation workﬂows microcrowdsourcing achieve highquality results 5860 Simi larly iterative improvement ﬁndﬁxverify 3 workﬂows shown yield excellent output individual workers err But use workﬂows raises important questions For example designing workﬂow handle problems like handwriting recognition task shown Fig 1 know answers questions like 1 What optimal number iterations task 2 How ballots voting 3 How answers change depending workers skill levels Our paper offers answers questions constructing AI agents workﬂow optimization control We study distinct crowdsourcing scenarios We start considering problem dynamically controlling aggregation simplest possible workﬂow binary classiﬁcation Next extend model control signiﬁcantly complex iterative improvement workﬂow Finally model dynamically switch alternative workﬂows given task We use shared toolbox techniques crowdsourcing scenarios key components First propose probabilistic model worker responses This model relates worker ability task diﬃculty worker response quality means Bayesian network Second view problem workﬂow control problem decision theoretic planning execution cast PartiallyObservable Markov Decision Process POMDP 6 The Bayes net provides model POMDP POMDP policy controls workﬂow obtain highquality output costeﬃcient manner We important contributions First provide principled solution dynamically decide number votes task based exact history workers worked task instance answers notion 3 httpmturkcom 4 httpliveopscom 5 httptopcodercom 6 httpgalaxyzooorg Audubon Christmas Bird Count 54 P Dai et al Artiﬁcial Intelligence 202 2013 5285 varying problem diﬃculty We provide framework answering complex questions larger workﬂows number iterations iterative improvement workﬂow The commonality approaches diverse tasks suggests underlying abstraction exploited optimizing controlling different kinds workﬂows Our experiments consistently outperform best known baselines large margins obtaining 50 error reduction scenario multiple workﬂows 30 cost savings iterative improvement Moreover work reveals surprising discoveries First iterative improvement scenario AI agent proposes unexpected voting policy predicted human expert Section 4 Second demonstrate judiciously alternative workﬂows task better single best workﬂow While base idea novel fact dynamic switching workﬂows automatically obtain highquality results surprising discovery The rest paper structured follows Section 2 formally deﬁnes Markov Decision Processes Partially Observable Markov Decision Processes POMDP Then review propose planning algorithms solving POMDPs Section 3 details model control aggregation simple binary classiﬁcation workﬂow ﬁrst agent TurKontrol0 Section 4 extends model agent iterative improvement workﬂows create second agent TurKontrol We present simulationbased investigation performance TurKontrol illustrate learn model parameters ﬁnally demonstrate usefulness TurKontrol Mechanical Turk real tasks Section 5 details model availability multiple workﬂows design agent Agen tHunt dynamically selects best workﬂow use We illustrate learn model parameters demonstrate usefulness AgentHunt simulation Mechanical Turk Finally present related work propose future work conclusions We note software packages implementations available general use httpcswashingtonedunode7714 2 Background 21 Markov decision processes AI researchers typically use Markov Decision Processes MDPs formulate fullyobservable decision making uncertainty problems 40 Deﬁnition 1 An MDP fourtuple cid3S A T Rcid4 S ﬁnite set discrete states A ﬁnite set actions T S A S 0 1 transition function describing probability taking action given state result state R S A R reward taking action state An agent executes actions discrete time steps starting initial state At step distinct state s S The agent execute action set actions A receives reward Rs The action cid7 stochastically The transition process exhibits Markov property new state s takes new state s cid7s The model independent previous states given current state s The transition probability deﬁned T assumes observability executing action transitioning stochastically state governed T agent knowledge state cid7 Any solution MDP problem form policy Deﬁnition 2 A policy π S A MDP mapping state space action space A policy static action taken state time step π s indicates action execute state s To solve MDP need ﬁnd optimal policy π S A probabilistic execution plan achieves maximum expected utility sum rewards We evaluate policy π value function set values satisfy following equation cid5 cid5s V π s R cid3 s π s β V π cid4 1 cid2 cid2 cid3 cid3 cid2 s s cid7 cid7 Tπ s β 0 1 discount factor controls value future rewards Any optimal policys value function scid7S satisfy following Bellman equations cid6 V s max A Rs β cid2 cid3 cid5 cid5s cid7 s V cid2 cid7 s T cid7 cid3 cid4 scid7S 2 P Dai et al Artiﬁcial Intelligence 202 2013 5285 The corresponding optimal policy extracted value function cid4 cid7 cid6 π s argmax Rs β A T s scid7S cid2 cid3 cid7 cid5 cid5s V cid3 cid2 s cid7 Given implicit optimal policy π Qfunction S A R form optimal value function V measure superiority action value stateaction pair s value state s immediate action performed Deﬁnition 3 The Q followed π More concretely Q s Rs β cid4 cid2 cid3 cid5 cid5s cid7 s V cid2 cid3 cid7 s T scid7S Therefore optimal value function expressed V s max A Q s 22 Solving MDP 55 3 4 5 Many optimal MDP algorithms based dynamic programming A simple powerful algorithm value eration 2 basis heuristic search approximate algorithms It ﬁrst initializes value function arbitrarily Then values updated iteratively operator called Bellman backup create successively better approximations value state iteration The Bellman residual state absolute difference state value Bellman backup Value iteration stops value function converges In implementation typically signaled Bellman error largest Bellman residual states predeﬁned threshold A simple way approximate solutions large MDPs use Monte Carlo MC simulation algorithms Such algorithms repeatedly perform simulation trials originating initial state During simulation actions chosen based heuristic function h resulting states chosen stochastically based transition probabilities action Different heuristics depending desired tradeoff exploration exploitation For example heuristic purely exploits knowledge value function pick greedy action maximizes value function On hand heuristic solely explores choose random action One simulation trial terminates terminal state7 encountered At termination Q values visited stateaction pairs updated reverse order Upper Conﬁdence Bounds Applied Trees UCT 29 MC algorithm power demonstrated application challenging problems like Go 18 Realtime Strategy Games 1 UCT uses heuristic considers actions proven valuable underexplored branches case better policies It remembers total number times state s visited ns number times action picked s visited nsa Its heuristic function hUCT s deﬁned follows cid8 hUCT s Q s κ 2 ln ns nsa 6 The second term increases value actions visited frequently The exploration parameter κ balance exploration exploitation Theoretical error bounds given suﬃcient number trials proven 29 23 PartiallyObservable Markov Decision Processes PartiallyObservable MDPs POMDPs 25 provide ﬂexible framework model decisionmaking uncertainty relaxing assumption agent complete knowledge world noisy observations current state This generality model singleagent realworld problems perfect information world rarely available agent Deﬁnition 4 A POMDP sixtuple cid3S A O T P Rcid4 S ﬁnite set discrete states A ﬁnite set actions 7 A terminal state state positiveprobability transition 56 P Dai et al Artiﬁcial Intelligence 202 2013 5285 O ﬁnite set observations T S A S 0 1 transition function describing probability taking action given state result state P S O 0 1 observation function describing probability taking action given state result observation R S A R reward taking action state We POMDP extends MDP adding ﬁnite set observations O corresponding observation model P Since agent unable directly observe worlds current state maintains probability distribution states called belief state b reﬂecting estimate likelihood bs 0 1 cid4 sS bs 1 7 bs probability current state s inferred previous belief state recent action resulting observation A POMDP cast MDP letting state space MDP space possible belief states Since transition belief states completely determined action resulting observation resulting model fullyobservable MDP vastly larger space Even POMDP deﬁned world states s1 s2 space belief states inﬁnite belief state arbitrary combination bs1 p bs2 1 p p real number 0 1 Solving POMDP hard problem For simplest case set A S O ﬁnite Sondik 59 proves optimal value function ﬁnitehorizon problem piecewise linear convex PWLC Therefore value iteration type algorithms converge problems However number reachable belief states grows exponentially number observations O The complexity update iteration exponential total number observations 25 For inﬁnitehorizon POMDP needs perform inﬁnite number iterations worst case total number reachable belief states inﬁnite Therefore problem undecidable 38 worst case Porta et al 45 later proved ﬁnitehorizon continuous state POMDPs PWLC Researchers seek approximation methods solve POMDPs eﬃciently Pointbased value iteration 614454 successful approach The basic idea approximate value function belief space B estimating B B For belief states values approximately estimated according values ﬁxed sampled subset values sampled space inferred integration 45 mixture Gaussians representation 8 For tractable ﬁnitehorizon problem algorithm iteratively computes approximately optimal thorizon value function The PWLC properties discrete POMDP guarantee error value function computed pointbased algorithms bounded 45 24 Planning algorithms We present algorithms use paper solving POMDPs 241 Limited lookahead Our ﬁrst decisionmaking algorithm lstep lookahead The goal evaluate sequences l decisions ﬁnd best sequence To determine value given sequence l decisions calculate expected utility stopping lth decision based agents current belief Based expected utility estimations agent picks sequence best utility executes ﬁrst action sequence repeats process terminal state reached 242 A discretized POMDP algorithm We try discretizationbased POMDP algorithm ADBS short approximation discretiza tion belief space The method approximates belief distribution values mean standard deviation Similar techniques robot navigation domain 50 ADBS discretizes range variable small equalsized intervals With discretization known bounds later bound vari ables application space belief states ﬁnite ADBS performs reachability search initial state build MDP model approximated belief space It optimally solves discretized MDP value itera tion 243 A UCT variant For algorithm use variant UCT applied space states simpliﬁed history actions observations condensed histories deﬁne Section 44 Although condensed histories unfortunate property different histories represent belief state approximation technique excels ways It saves computation need maintain posterior beliefs Maintaining beliefs tricky P Dai et al Artiﬁcial Intelligence 202 2013 5285 57 Fig 2 Worker accuracies given intrinsic diﬃculty error parameters γ With ﬁxed diﬃculty greater workers γ value likely worker makes mistake problems state space continuous belief continuous state space nontrivial represent update For reason implementing historybased POMDPs easier Finally makes execution faster directly map history action Monte Carlo simulationbased POMDP algorithms proved effective large problems 56 3 Decisiontheoretic optimization binary classiﬁcation workﬂow We begin study decisiontheoretic control crowdsourcing We ﬁrst consider simplest work ﬂows binary classiﬁcation binary ballot job These workﬂows present workers question ask pick best answer choices For instance workﬂow workers picture ask human face picture Then method aggregation like majority vote account worker variability Such workﬂows collect training data vision algorithm The agents control problem deﬁned follows As input agent given task agent asked return correct answer In pursuit goal agent allowed create jobs crowdsourcing platform We model agents decision problem POMDP To ﬁrst deﬁne generative model worker responses 31 Probabilistic model binary classiﬁcation response We assume workers diligent answer ballots best abilities In words assume workers adversarial Additionally assume workers collaborate We deﬁne workers accuracy cid2 ad γ 1 2 1 1 dγ cid3 8 d 0 1 intrinsic diﬃculty task motivate shortly γx 0 error parameter worker As workers error parameter andor workﬂows diﬃculty increases approaches 12 suggesting worker randomly guessing On hand parameters decrease approaches 1 worker produces correct answer As γ decreases accuracy curve concave expected accuracy increases ﬁxed d Fig 2 illustration accuracy The answer bw worker w error parameter γw provides governed following equations P b w vd ad γw P b w cid10 vd 1 ad γw 9 10 Fig 7 illustrates plate notation generative model encodes Bayes Net responses W workers T tasks The correct answer v diﬃculty parameter d error parameter γ inﬂuence ﬁnal answer b worker provides observed variable We note given assumption workers collaborate believe workers responses bw independent given true answer v However following subtlety Even 58 P Dai et al Artiﬁcial Intelligence 202 2013 5285 different workers collaborating mistake worker changes error probability mistake gives evidence question intrinsically hard diﬃcult right Thus shown graphical model assumption worker responses independent additionally given diﬃculty d With generative model deﬁne POMDP 32 The POMDP Deﬁnition 5 The POMDP simple binary classiﬁcation workﬂow sixtuple cid3S A R T O P cid4 S d v d 0 1 v 0 1 d diﬃculty task v true answer A create job submit true submit false R S A R speciﬁed T S A S 0 1 d v d v cid11 1 All probabilities 0 O true false Boolean response worker P S O 0 1 deﬁned generative model The reward function maintains value submitting correct answer penalty submitting incorrect answer Additionally maintains cost TurKontrol0 incurs creates job We modify reward function match desired budgets accuracies We note POMDP purely sensingPOMDP None actions changes state POMDP In crowdsourcing platforms Mechanical Turk preselect workers job However order specify observation probabilities deﬁned generative model need access future workers parameters To simplify computation POMDP calculates observation probabilities average γ In words assumes future worker average worker Formally future worker error parameter equal γ 1 W w γw W number known workers However agent knowledge worker accuracy order use accurate estimates γ updating belief state In particular answers question correctly according agents belief good worker γw decrease error question γw increase Moreover increasedecrease amounts depend diﬃculty question The following simple update strategy work cid9 1 If worker answers question diﬃculty d correctly γw γx dδ 2 If worker makes error answering question γw γw 1 dδ We use δ represent learning rate slowly reduce time accuracy worker approaches asymptote More sophisticately update parameters Bayesian updates ﬂavor discussed Sec tion 34 33 Simulated experiments We evaluate model simulation agent uses majorityvote strategy assess viability POMDP control simple workﬂow crowdsourcing The POMDP manage belief state cross product Boolean answer continuous variable task diﬃculty Since solving POMDP continuous state space challenging discretize diﬃculty possible values leading world state space size 2 11 22 To solve POMDPs run ZMDP package8 300 s default Focused RealTime Dynamic Programming search strategy 57 On run simulator draws diﬃculty uniformly We ﬁx reward returning correct answer 0 vary reward penalty returning incorrect answer following values 10 100 1000 We set cost creating job simulated worker 1 We use discount factor 09999 POMDP POMDP solver converges quickly We ensure majority vote uses number ballots average number ballots TurKontrol0 uses Diﬃculties drawn uniformly randomly workers γ drawn Normal 10 02 For setting reward values run 1000 simulations report mean net utilities Fig 3 percent reduction error TurKontrol0 achieves comparing accuracy majority voting Fig 4 We model works average diﬃculty task diﬃcult easy variance wide To gain insight investigate effectiveness model varying settings task diﬃculty 8 httpwwwcscmuedutreyzmdp P Dai et al Artiﬁcial Intelligence 202 2013 5285 59 Fig 3 In simulation importance answer correctness increases TurKontrol0 outperforms agent majorityvote strategy increasing margin Fig 4 In simulation importance answer correctness increases TurKontrol0 increases percent reduction error achieves comparing accuracy majority voting We run 1000 simulations equally spaced diﬃculty settings starting d 01 Workers γ drawn Normal 10 02 set reward incorrect answer 1000 However assume TurKontrol0 knowledge worker γ receives observations new workers access average γ To place agent unfavorable testing conditions possible compare TurKontrol0 agent MV uses following adaptive majorityvote strategy For diﬃculty setting determine average ballots TurKontrol0 uses Then set majorityvote strategy use ballots 1 Fig 5 shows percent reduction error TurKontrol0 achieves comparing accuracy MV Since TurKontrol0 achieves equal better accuracy MV MV uses equal ballots TurKontrol0 achieves equal better net utility MV Interestingly ﬁgure shows illuminating trend TurKontrol0 achieves slightly better results MV extremes diﬃculty spectrum shows larger gains middle diﬃculty settings Such result intuitively makes sense following reasons When problems easy need dynamic Similarly problems extremely diﬃcult TurKontrol0 knowledge workers However problem diﬃculty middle range workers agree quickly ability agent dynamic decisions contributes higher accuracy higher net utility 34 Learning model Extensive simulation results previous section usefulness decisiontheoretic techniques simple binary classiﬁcation workﬂow This section addresses learning model real Mechanical Turk data 13 We use following task Given image pair descriptions ask workers select better description This learning problem challenging large number parameters sparse noisy training data We begin deﬁning supervised learning problem Fig 6 presents generative model ballot jobs observe ballots true answer diﬃculty task We seek learn error parameters γ γw parameter worker w To generate training data task select T images corresponding pairs descriptions 60 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 5 The percent reduction error TurKontrol0 achieves comparing accuracy MV Fig 6 A plate model ballot jobs supervised learning setting b represents ballot outcome γ workers individual error parameter d diﬃculty job v truth value job Shaded nodes represent observed variables post W copies ballot job task We use biw denote worker ws ballot ith question Let v denote ﬁrst artifact ith pair better second di denote diﬃculty answering question The ballot answer worker depends error parameter diﬃculty job d real truth value v For learning problem collect values v d T ballot questions consensus human experts treat values observed In experiments assume γ drawn uniform prior model incorporate informed priors9 We use standard maximum posteriori approach estimate γ parameters P γ b w d P γ P bγ w d 11 Under uniform prior conditional independence different workers given diﬃculty truth value task Eq 11 simpliﬁed P γ b w d P bγ w d Tcid10 Wcid10 i1 w1 P biw γw di v 12 Taking log maximum posteriori problem transformed following optimization problem Constants d1 dT v 1 v T b11 bT W Variables γ1 γW Maximize Tcid4 Wcid4 i1 w1 cid11 cid12 P biw γw di v log Subject 9 We tried priors penalized extreme values help experiments P Dai et al Artiﬁcial Intelligence 202 2013 5285 61 Fig 7 A plate model ballot jobs unsupervised learning setting b represents ballot outcome γ workers individual error parameter d diﬃculty job w truth value job Shaded nodes represent observed variables We try unsupervised learning algorithm The plate notation shown Fig 7 We dont assume knowledge correct answers diﬃculties Instead adopt EMstyle algorithm based Whitehill et als learning mech anism 68 We initialize diﬃculty values error parameters corresponding average values learned supervised learning algorithm We ﬁnd applying prior distributions parameters help use prior During expectation step compute probability true answers given workers answers current values diﬃculty error parameters During maximization step update diﬃculty error parameters based likelihood function Eq 12 341 Experiments ballot model We evaluate effectiveness learning procedures majorityvote baseline image description task We select 20 pairs descriptions T 20 collect sets ballots 50 workers 5 spammers detected manually dropped learning W 45 We spend 450 supervised learning process We solve optimization problem NLopt package10 implement unsupervised learning algorithm based Whitehills framework 68 We run ﬁvefold crossvalidation experiment We 45th image pairs learn error parameters use parameters estimate true ballot answer images ﬁfth fold Our supervised learning algorithm obtains accuracy 8001 unsupervised learning algorithm obtains accuracy 800 majority voting baseline obtains accuracy 80 We investigate similar accuracies ballots frequently missed models mass opinion differs expert labels We compare conﬁdence degree belief correctness answer approaches For majorityvote baseline calculate conﬁdence dividing number votes inferred correct answer total number votes For supervised learning approach use average posterior probability inferred answer The average conﬁdence values derived supervised learning approach higher majority vote 822 636 Thus approaches achieve accuracy 45 votes ballot model superior belief answer Although conﬁdence values different ballot models learned supervised unsupervised learning offer distinct advantage simple majorityvoting baseline given large number votes In hindsight result surprising large number workers In work researchers shown simple average large number nonexperts beats expert opinion 58 However rarely resources use 45 voters question consider effect varying number available voters For image pair randomly sample replacement 50 000 sets 3 11 bal lots compute average accuracies approaches Fig 8 shows model learned supervised unsupervised learning algorithms consistently outperforms majorityvote baseline Further applying supervised learning algorithm consistently achieves higher accuracy applying unsupervised learning algorithm shows usefulness expert labeling The unsupervised learning algorithm gradually catches number votes increases distinguishes majority voting In contrast model learned supervised learning outperforms majority voting wide margin With 11 votes able achieve accuracy 793 close accuracy achieved 45 votes Also supervised ballot model 5 votes achieves accuracy similar achieved majority vote 11 votes Our ballot model signiﬁcantly reduces number votes money needed given desired accuracy Since unsupervised learning algorithm require labeled data useful data labeling expen sive 10 httpabinitiomiteduwikiindexphpNLopt 62 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 8 Accuracies ballot model applying supervised unsupervised learning majority vote random voting sets different sizes averaged 50000 random sets size The models generated learning algorithms achieve higher accuracy majority vote Using supervised learning ballot model achieves signiﬁcantly higher accuracy majority vote p 001 Fig 9 Flowchart iterative text improvement task reprinted 37 4 Decisiontheoretic optimization iterative improvement workﬂow 41 Motivation example We decisiontheoretic control iterative improvement workﬂow 12 introduced Little et al 37 This workﬂow depicted Fig 9 While ideas paper applicable complex workﬂows choose apply representative number ﬂows commercial use today time moderately complex making ideal ﬁrst investigation Iterative text improvement works follows There initial job presents worker image requests English description images contents A subsequent iterative process consists improvement job multiple ballot jobs In improvement job different worker shown image current description requested generate improved English description Fig 22 Next n cid2 1 ballot jobs posted Which text best describes picture See Figs 23 24 25 26 user interface design Based majority opinion best description selected loop continues Little et al shown iterative process generates better descriptions ﬁxed allocating total reward single author Little et al support opensource toolkit TurKit provides highlevel mechanism deﬁning moderately complex iterative workﬂows votingcontrolled conditionals However TurKit builtin methods monitoring accuracy workers automatically determine ideal number voters estimate appropriate number iterations returns diminish 42 Problem overview The agents control problem workﬂow like iterative text improvement deﬁned follows As input agent given initial artifact job description requesting agent asked return artifact maximizes payoff based quality submission In initial model assume requesters express utility function U quality dollars To accomplish task agent allowed post improvement job ballot job To fully specify problem deﬁne quality means Intuitively highquality better things type For engineered artifacts including English descriptions artifact high quality diﬃcult improve Therefore deﬁne quality artifact follows Let quality q 0 1 An artifact quality q means average dedicated worker probability 1 q improving artifact The agent exactly knows quality artifact At best estimate q based domain dynamics observations like ballot results Therefore model agents control problem POMDP Since quality real number state space POMDP continuous 8 P Dai et al Artiﬁcial Intelligence 202 2013 5285 63 Fig 10 Computations needed TurKontrol0 control iterativeimprovement workﬂow 43 The POMDP Deﬁnition 6 The POMDP iterative improvement workﬂow sixtuple cid3S A T R O P cid4 cid7q q cid7 0 1 S q q A create ballot job create improvement job submit best artifact Rq q Rq q T deﬁned O true false Boolean answer received ballot question P deﬁned cid7 submit reward received submitting artifact quality maxq q cid7 create cost creating job cid7 431 The transition function We ﬁrst note underlying state POMDP changes agent requests improvement The qualities artifacts change agent requests votes Votes change belief state agent Suppose currently artifacts α αcid7cid7 cid7cid7 Sup unknown qualities q q pose loss generality agent posts improvement job α worker x submits artifact αcid7 suggested improvement α q quality denoted q depends initial quality q Moreover higher accuracy worker x improve q depends x In order determine state compute P q worker x improves artifact quality q We cid7 probability learn conditional distribution later The current state q q cid7q x However agent know priori worker submit improvement assume P q purpose lookahead worker average ability cid7q x conditional distribution q cid7cid7 transitions new state q q Therefore current state q q Since αcid7 cid7cid7 cid7 cid7 cid7 cid7 432 The observation function Our observation function relevant agent requests ballot jobs If current hidden state q q agent observe vote α αcid7 cid7 The observation function deﬁned cid7 Since ballot job simply instantiation binary classiﬁcation task use deﬁned corresponding artifacts α αcid7 P oq q generative model deﬁning diﬃculty task To deﬁnition observe diﬃculty ballot job depends relative closeness qualities underlying state If artifacts similar quality diﬃcult judge better Thus deﬁne relationship diﬃculty qualities cid3 cid2 q q cid7 d 1 cid5 cid5q q cid5 cid5M cid7 M diﬃculty constant 433 Discussion Fig 10 highlevel ﬂow describing planners decisions At step track belief qualities q q previous α current artifact αcid7 respectively Each improvement attempt vote gives new information reﬂected quality posteriors These distributions depend accuracy workers incrementally estimate based previous work Our POMDP lets answer questions like 1 terminate voting phase switching attention artifact improvement 2 artifacts best basis subsequent improvements 3 stop iterative process submit result requester We note general POMDP model belief quality previous artifact posterior α change based ballots comparing new artifact Why case We following subtle important point If improvement worker good accuracy unable create better αcid7 improvement phase α high quality easily improvable Under evidence 13 cid7 64 P Dai et al Artiﬁcial Intelligence 202 2013 5285 increase quality estimation α Similarly voting workers unanimously thought αcid7 α αcid7 likely incorporates signiﬁcant improvements α qualities reﬂect knowledge better 434 Updating diﬃculty worker accuracy To update knowledge quality worker voting phase agent ﬁrst estimates expected diﬃculty voting estimates quality follows d 1cid13 1cid13 0 0 cid3 cid2 q q cid7 d P qP cid3 cid2 q cid7 dq dq cid7 1cid13 1cid13 cid2 0 0 cid5 cid5q q cid5 cid5Mcid3 cid7 1 cid3 cid2 q cid7 dq dq cid7 P qP 14 Then uses d Section 32 believes correct answer update worker quality records discussed 435 Implementation For eﬃcient storage computation TurKontrol0 employ piecewise constantpiecewise linear value function representations use particle ﬁlters Although approximate techniques popular literature eﬃciently maintaining continuous distributions 3917 provide arbitrarily close approximations Because equations require double integrals timeconsuming Eq 14 compact representations help overall eﬃciency implementation cid14 Our piecewise constant function distribution density function P q divides domain ﬁxed number f q dq value interval l u In implementation continuous intervals uses average value function represented array values value equals constant function value designated interval 1 ul u l 44 Simulations This section aims empirically answer following questions POMDPsolving algorithms introduced Section 24 1 For simple lookahead approach deep agents lookahead best tradeoff computation time utility 2 How ABDS algorithm perform scale 3 What error UCT algorithm history approximation 4 How UCT algorithm perform 5 What best planner TurKontrol0 6 Does TurKontrol0 better decisions compared nonadaptive workﬂow 7 Can planner outperform agent following wellinformed ﬁxed policy 8 How planner handle workers effective completing ballot jobs improvement jobs 441 Experimental setup We deﬁne reward submitting artifact quality q convex function Rq 1000 eq1 e1 R0 0 R1 1000 We assume quality initial artifact follows Beta distribution Beta1 9 implies mean quality ﬁrst artifact 01 We model results improvement task manner akin ballot tasks We know higher quality artifact likely artifact improved Suppose quality current artifact q deﬁne cid7q x Beta10μQ cid7qx 101 μQ cid7qx mean μQ cid7qx conditional distribution P q μQ cid7qx q 05 cid11 1 q cid2 axq 05 cid3 q cid2 axq 1 cid3cid12 15 Thus resulting distribution qualities inﬂuenced workers accuracy hardness improvement indicated quality original artifact q We ﬁx ratio costs improvements ballots cimpcb 3 ballots time We set diﬃculty constant M 05 In simulation runs build pool 1000 workers error coeﬃcients γw follow bell shaped distribution ﬁxed mean γ We distinguish accuracies performing improvement answering ballot half γw worker w answering ballot answering ballot easier task worker higher accuracy11 442 The lstep lookahead algorithm We ﬁrst try ﬁnd optimal depth lstep lookahead algorithm presented earlier When l 2 algorithm considers following set action sequences cid3stopcid4 cid3ballot stopcid4 cid3improvement stopcid4 cid3ballot ballotcid4 cid3ballot improvementcid4 cid3improvement ballotcid4 cid3improvement improvementcid4 We run 10 000 simulation trials average error coeﬃcient γ 1 pairs improvement ballot costs 30 10 3 1 03 01 trying ﬁnd best lookahead depth l lstep lookahead algorithm Fig 11 shows 11 For simplicity reasons simulations assume worker γw We relax assumption run real experiments Mechanical Turk P Dai et al Artiﬁcial Intelligence 202 2013 5285 65 Table 1 Average utility TurKontrol2 performance ADBS algorithms different resolution discrete interval lengths sets improvement ballot costs 30 10 3 1 03 01 TurKontrol2 outperforms ADBS algorithms settings Costs TurKontrol2 ADBS 30 10 3 1 03 01 264906 445721 466820 Interval length S s0 s0 s0 V V V 002 3577 253951 365866 382066 001 41 072 253951 366798 385698 0005 809 420 253951 367674 387366 Fig 11 Average utility TurKontrol lookahead depths calculated 10 000 simulation trials sets improvement ballot costs 30 10 3 1 03 01 Longer lookahead produces better results 2step lookahead good costs relatively high 30 10 average utility achieved different lookahead depths denoted TurKontroll Note perfor mance gap TurKontrol1 TurKontrol2 curves TurKontrol3 TurKontrol4 generally overlap We observe costs high process usually ﬁnishes iterations performance difference TurKontrol2 deeper step lookaheads negligible Since additional step lookahead creases computational overhead order magnitude limit TurKontrols lookahead depth 2 subsequent experiments 443 The ADBS algorithm Next try ADBS algorithm The states MDP fourtuples cid3μ σ μcid7 σ cid7cid4 average standard devi ation hidden quality Since quality real number 0 1 μ 0 1 σ 0 1 ﬁnitely discretize intervals We try different resolutions discretization constant interval lengths run average error coeﬃcient γ 1 pairs improvement ballot costs 30 10 3 1 03 01 Table 1 lists average utility TurKontrol2 10 000 simulations results incorporated Fig 11 For ADBS algorithm report size reachable belief states resolution S value initial state V s0 calculated value iteration We ﬁnd TurKontrol2 outperforms ADBS settings Moreover smaller costs bigger performance gap The poor performance ADBS probably errors generated approximation discretization Also notice reﬁned discretization reachable state space grows quickly approximately rate order magnitude doubly reﬁned interval optimal value initial state increases slowly This indicates error likely comes approximation opposed discretization We try interval length 00025 algorithm terminates prematurely reachability search runs memory indicating limited scalability ADBS algorithm This experiment shows simple POMDP method work 2step lookahead algorithm draw conclusion POMDP algorithms work planning problem 444 The UCT algorithm Finally try UCT variant We ﬁrst evaluate suboptimality introduced condensed history approximation An example condensed history Step 1 An initial artifact α1 provided Step 2 The agent creates improvement HIT obtains α2 Step 3 The agent creates ballot job receives vote α2 better α1 Step 4 The agent submits α2 Since worker identities ignored belief state space size drastically reduced In new TurKontrol2 version TurKontrol2 regard worker anonymous worker average accuracy γ We denote new variant TurKontrol2 anonymous Note 66 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 12 Average utility UCT algorithm varying number trials compared average utility 10 000 simulation trials TurKontrol2 horizontal lines represent xaxes improvement ballot costs 30 10 3 1 middle 03 01 UCT achieves higher average utility costs low case policy nonadaptive uncertainty workers accuracies We perform 10 000 simulations following conﬁgurations12 1 γ 2 ballot accuracies higher improvement accuracies TurKontrol2 gets 45 utility TurKon trol2 anonymous 2 γ 1 ballot accuracies improvement accuracies TurKontrol2 gets 37 utility TurKon trol2 anonymous This shows average γ bad approximation reduces expected utility TurKontrol2 5 cases We expect produce better results approximation reﬁned group workers accuracy In investigating UCT algorithm use ﬁxed κ 50 dynamically decreasing learning parameter θ As action visited frequently effect randomness decreases weight 02log2 10 nsa given new observation Fig 12 plots average utility UCT algorithm function number trials completed 10 000 500 000 The horizontal lines xaxes represent average utility TurKontrol2 data Fig 11 First notice performance UCT improves increase number completed trials We observe UCT outperforms TurKontrol2 10 000 trials problems costs small underperforms TurKontrol2 problem costs high This behavior probably TurKontrol2 performs close optimal high cost problem We ﬁnd UCT consistently underperforms 12 We intentionally choose cases favorable cases TurKontrol2 ﬁnd following subsections P Dai et al Artiﬁcial Intelligence 202 2013 5285 67 Fig 13 Average utility control policies averaged 10 000 simulation trials varying mean error coeﬃcient γ Top We set workers better accuracies ballot jobs improvement jobs Bottom We set workers equally accurate ballot improvement jobs TurKontrol2 produces best policy case TurKontrol3 comparing Fig 11 This TurKontrol3 computes closetooptimal values UCTs performance inﬂuenced random noises search trials This experiment shows UCT useful quickly ﬁnding good suboptimal policy limited power ﬁnding closetooptimal policy 445 Choosing best planning algorithm From comparing results algorithms lstep lookahead ADBS UCT conclude lstep lookahead eﬃcient planning algorithm domain 446 The effect poor workers We consider effect worker accuracy effectiveness agent control policies Using ﬁxed costs 30 10 compare average net utility control policies The ﬁrst TurKontrol2 The second TurKit nonadaptive policy literature 37 performs iterations possible ﬁxed allowance 400 experiment depleted iteration requests ballots invoking ﬁrst disagree Our policy TurKontrolﬁxed combines elements decision theory ﬁxed policy After simulating behavior TurKontrol2 compute integer mean number iterations μimp mean number ballots μb use values drive ﬁxed control policy μimp iterations μb ballots Thus represents nonadaptive policy parameters tuned costs worker accuracies Fig 13 shows decisiontheoretic methods work better TurKit policy partly TurKit runs iterations needed A Students ttest shows differences statistically signiﬁcant p 001 We note performance TurKontrolﬁxed similar TurKontrol2 workers inaccurate γ 4 Indeed case TurKontrol2 executes nearly ﬁxed policy In cases TurKontrolﬁxed consistently underperforms TurKontrol2 A Students ttest conﬁrms differences statistically signiﬁcant γ 4 We attribute difference fact dynamic policy makes better use ballots requests ballots late iterations harder improvement tasks errorprone The biggest performance gap policies manifests γ 2 TurKontrol2 generates 197 utility TurKontrolﬁxed 447 Robustness face bad voters As ﬁnal study consider sensitivity previous policies increasingly noisy voters Speciﬁcally repeat previous experiment error coeﬃcient γw workers improvement ballot behavior Fig 13 We previously set error coeﬃcient ballots half γw model fact voting easier Fig 13 shape Fig 13 lower overall utility Once TurKontrol2 continues achieve highest average utility settings Interestingly utility gap TurKontrol variants TurKit consistently bigger γ previous experiment In addition γ 1 TurKontrol2 68 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Table 2 Qualities 10 artifacts collected methods deﬁnition simulation direct expert estimation averaged worker estimation The averaged worker estimation cheapest method provides comparable results methods Def simulation Avg worker est Expert est α1 005 012 01 α2 053 051 05 α3 048 043 04 α4 062 045 06 α5 033 044 04 α6 041 036 02 α7 010 027 03 α8 04 056 05 α9 074 063 09 α10 032 044 04 generates 25 utility TurKontrolﬁxed bigger gap seen previous experiment A Students ttest shows differences TurKontrol2 TurKontrolﬁxed signiﬁcant γ 2 differences TurKontrol variants TurKit signiﬁcant settings 45 Learning improvement model Extensive simulation results previous section usefulness decisiontheoretic techniques iterative improvement workﬂows This section addresses learning model real Mechanical Turk data 13 We ex plored learning ballot model Section 34 learn improvement model To learn effect worker trying improve artifact ﬁrst need method determining ground truth quality arbitrary artifact 451 Estimating artifact quality gold standards Since quality partiallyobservable statistical measure consider ways approximate simulating deﬁnition direct expert estimation averaged worker estimation Our ﬁrst technique simply simulates deﬁnition We ask W workers improve artifact α use multiple ballots l judge improvement We deﬁne quality α 1 minus fraction workers able improve Unfortunately method requires W W l jobs order estimate quality single artifact slow expensive practice As alternative direct expert estimation complex We teach statistically sophisticated scientist deﬁnition quality ask estimate quality nearest decile13 Our ﬁnal method averaged worker estimation similar averages judgments Mechanical Turk workers scoring jobs These scoring jobs provide deﬁnition quality examples mapped 010 integer scale workers asked score artifacts See Figs 27 28 29 user interface design We collect data 10 images Web use Mechanical Turk generate multiple descriptions We select description image carefully ensuring chosen descriptions span wide range language ﬂuency We modiﬁed description obtain felt hard improve accounting highquality region When simulating deﬁnition average W 22 workers14 We use single expert direct expert estimation average 10 worker scores averaged worker estimation See Table 2 Our hope following work 58 averaged worker estimation deﬁnitely cheapest method proves compa rable expert estimates especially simulated deﬁnition Indeed ﬁnd methods produce similar results They agree best worst artifacts average expert worker estimates 01 score produced simulating deﬁnition We conclude averaged worker estimation equally effective additionally easier economical 1 cent scoring job adopt method assess qualities subsequent experiments 452 Actually learning model Finally approach learning model improvement phase Our objective estimate cid7q w probability distribution describes quality q worker w improves arti new artifact αcid7 cid7 P q fact α quality q Moreover learn prior P q cid7q order model work previously unseen worker There main challenges learning model ﬁrst functions twodimensional continuous space second training data scant noisy To alleviate diﬃculties break task learning steps 1 learn mean value quality regression 2 ﬁt conditional density function given mean We second learning task tractable choosing parametric representations functions Our solution follows following steps 1 Generate improvement job contains T original artifacts α1 αT 2 Crowdsource W workers improve artifact generate W T new artifacts 3 Estimate qualities qi q cid7 iw artifacts set previous section qi denotes quality new artifact produced worker w This data training data quality αi q cid7 iw 13 The consistency type subjective rating carefully evaluated literature 11 14 We collected 24 sets improvements workers improved 3 artifacts results dropped analysis P Dai et al Artiﬁcial Intelligence 202 2013 5285 69 4 Learn workerdependent distribution P q 5 Learn workerindependent distribution P q cid7q w participating worker w cid7q act prior unseen workers We steps learning algorithms We ﬁrst estimate mean worker ws im provement distribution denoted μq q cid7 w cid7 w We assume μq linear function quality original artifact15 By introducing μQ separate variance workers ability improving artifacts quality variance training data starting artifacts different qualities To learn perform linear regression training data cid7 aw q bw line regression standard error e w truncate values outside qi q iw This yields q 0 1 cid7 w cid7 w To model workers variance improving artifacts quality consider parametric represen cid7q w Triangular Beta Truncated Normal While clearly making approximation restricting attention tations P q distributions signiﬁcantly reduces parameter space makes learning problem tractable Note assume mean distributions given line regression We consider distribution turn Triangular The triangularshaped probability density function ﬁxed vertices 0 0 1 0 We set xcoordinate vertex μq q yielding following probability density function fq qq cid7 w cid7 w cid7 w cid3 cid2 q cid7 w f Q cid7 w q cid7 2q w μq q cid7 w cid7 21q w 1μq q cid7 w q cid7 w μq cid7 w q q cid7 w cid2 μq cid7 w q 16 Beta We wish Beta distributions mean μq standard deviation proportional e w Therefore train constant c1 gradient descent maximizes loglikelihood observing training data workers16 resulting distribution Beta c1 q The error e w appears denominator cid7 e w w parameters Beta distribution approximately inversely related standard deviation 1 μq q c1 e w μq cid7 w cid7 w Truncated Normal As set mean μq trained maximize loglikelihood training data This yields distribution Truncated Normalμq truncated interval 0 1 standard deviation c2 e w c2 constant 2e2 w q c2 cid7 w cid7 w cid7 average improved quality ith artifact mean q q We use similar approaches learn workerindependent model P q cid7 q cid7q training data form cid7 qi iw workers Denote standard cid7 aq b The Triangular distribution deﬁned deviation set σq exactly For distributions standard deviations depend σq We assume conditional standard deviation σqcid7q quadratic q infer unknown conditional standard deviation existing ones running quadratic regression As use gradient descent train constants Beta Truncated Normal distributions As start linear regression q qi qi cid7 cid7 We seek determine distributions best models data employ leaveoneout cross val idation We set number original artifacts number workers T W 10 spend 1650 cid7 data collection The algorithm iteratively trains training examples qi workerindependent q case measures probability density observing tenth We score model summing log probability densities Our results Beta distribution c1 376 best conditional distribution workerdependent mod els For workerindependent model intercept 035 slope 034 linear regression Truncated Normal c2 100 performs best We suspect case workers average performance Truncated Normal thinner tail Beta In cases Triangular distribution performs worst This probably Triangular assumes linear probability density reality workers tend provide reasonably consistent results translates higher probabilities conditional mean We use best performing distributions subsequent experiments In case returning worker use corresponding workerdependent model Otherwise assume worker performs averagely instead use workerindependent model 46 TurKontrol Mechanical Turk Now learned POMDP model ﬁnal evaluation assesses beneﬁts dynamic workﬂow controlled TurKontrol versus nonadaptive workﬂow originally TurKit 37 similar monetary 15 While obviously approximation ﬁnd surprisingly close R2 082 workerindependent model 16 We use Newtons method 1000 random restarts Initial values chosen uniformly real interval 0 1000 70 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 14 Average qualities 40 descriptions generated TurKontrol TurKit monetary consumption TurKontrol generates statisticallysigniﬁcant higherquality descriptions TurKit sumption settings We aim answer following questions 1 Is signiﬁcant quality difference artifacts produced TurKontrol TurKit 2 What qualitative differences workﬂows As run experiments image description task We use 40 fresh pictures Web employ iterative improvement generate descriptions For picture restrict worker task setting nonadaptive dynamic We set user interfaces identical settings randomize order conditions presented workers order eliminate human learning effects Altogether 655 participating workers 57 settings We devise automated rules detect spammers We reject improvement job new artifact identical original We reject ballot scoring jobs returned quickly worker reasonable judgment Note need learn model new worker assigning jobs instead uses cid7q prior These parameters incrementally updated TurKontrol workerindependent parameters γ P q obtains information Recall TurKontrol performs decisiontheoretic control based userdeﬁned reward function We deﬁne reward submitting artifact quality q Rq 25q experiments We set cost improvement job 5 cents ballot job 1 cent We use 3step lookahead algorithm controller Under parameters TurKontrolworkﬂows run average 625 iterations average 232 ballots iteration costing 46 cents image description average We use TurKits original nonadaptive policy ballots requests ballot ﬁrst voters disagree We compute number iterations TurKit total money spent matches TurKontrols Since number comes 647 compare cases TurKit6 6 iterations TurKit7 7 iterations TurKit67 weighted average equalizes monetary consumption For ﬁnal description create scoring job multiple workers score descriptions Fig 14 compares artifact qualities generated TurKontrol TurKit67 40 images We note points y x line indicating dynamic workﬂow produces superior descriptions Furthermore quality produced TurKontrol greater average TurKits difference statistically signiﬁcant p 001 TurKit6 p 001 TurKit67 p 005 TurKit7 Students ttest Using parameters TurKontrol generates highestquality descriptions average quality 067 TurKit67s average quality 060 furthermore generates worst descriptions qualities 03 Finally standard deviation TurKontrol lower 009 TurKits 012 These results demonstrate overall superior perfor mance decisiontheoretic control live workﬂows While 11 average quality increase produced TurKontrol statistically signiﬁcant wonder material To better illustrate importance quality include experiment We run nonadaptive TurKit policy ad ditional improvement iterations produces artifacts average quality equal produced TurKontrol Fixing quality threshold TurKit policy run average 876 improvements compared 625 improve ment iterations TurKontrol As result nonadaptive policy spends 287 money TurKontrol achieve quality results Note ﬁnal artifact quality linear number iterations total cost Intuitively easier improve artifact quality low high We qualitatively study TurKontrols behavior compared TurKits ﬁnd interesting difference use ballots Fig 15 plots average number ballots iteration number Since TurKits ballot policy ﬁxed consistently uses 245 ballots iteration TurKontrol hand uses ballots intelligently In ﬁrst improvement iterations TurKontrol bother ballots expects workers improve artifact As iterations increase TurKontrol increases use ballots artifacts harder improve later iterations TurKontrol needs information deciding artifact promote iteration The eighth iteration interesting exception point improvements rare ﬁrst voter rates new artifact loser TurKontrol believes verdict P Dai et al Artiﬁcial Intelligence 202 2013 5285 71 Fig 15 Average number ballots nonadaptive dynamic workﬂows TurKontrol makes intelligent use ballots Fig 16 An image description example It took TurKontrol 6 improvement jobs 14 ballot jobs reach ﬁnal version This Gene Hackman scene ﬁlm The Conversation plays man paid secretly record peoples private conversations He squatting bathroom gazing tape recorder concealed blue toolbox placed hotel motel commode paper strip toilet seat He left image gray jacket commode right picture His ﬁngertips rest lid commode He wearing black court white shirt He glasses It took nonadaptive workﬂow 6 improvement jobs 13 ballot jobs reach version A thought repairing Image shows person named Gene Hackman thinking repair toilet hotel room He opened tool box contains plier screw diver wires He looks seriously tool box thinking tool willuse Wearing grey coat sits toilet seat resting gently toilet seat Besides ballots intelligently believe TurKontrol adds kinds reasoning First seven pictures TurKontrol ﬁnished 5 iterations higher qualities TurKits This suggests quality tracking working Perhaps agreement voters TurKontrol able infer description quality high warrant termination Secondly TurKontrol ability track individual workers affects posterior calculations For example instance TurKontrol decided trust ﬁrst vote worker superior accuracy reﬂected low error parameter We expect repetitive tasks enormously valuable ability TurKontrol able construct informed worker models superior decisions We present image description example Fig 16 It interesting note processes managed ﬁnd origin image However TurKontrol version exempliﬁes better use language factuality level In retrospect ﬁnd nonadaptive workﬂow probably wrong ballot decision sixth iteration decision critical voters consulted TurKontrol hand reached decision 6 unanimous votes stage 5 Decisiontheoretic optimization multiple workﬂows Now explored decisiontheoretic control single workﬂow consider scenario quester 36 In exploration simplicity consider binary classiﬁcation workﬂows However note ideas present equally applicable complex workﬂows like iterative improve ment workﬂow considered earlier 72 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 17 Workers answer b depends diﬃculty question d generated δ workers error parameter γ questions true answer v There W workers complete T tasks solved set K different workﬂows b observed variable To useful switch workﬂows consider process task designers use design workﬂows Typically quantitatively experiment alternative workﬂows accomplish task choose single production runs workﬂow achieves best performance early testing In simplest case alternative workﬂows differ user interfaces instructions cases workers asked solve problem different set steps Unfortunately seemingly natural design paradigm achieve potential crowdsourcing Selecting single best workﬂow suboptimal alternative workﬂows compose synergistically attain higherquality results While given workﬂow best performance average necessarily best problem Suppose gathering answers task wishes increase ones conﬁdence results workﬂow invoked Due fact different alternative workﬂow offer independent evidence signiﬁcantly bolster ones conﬁdence answer If best workﬂow giving mixed results task alternative workﬂow best way disambiguate Instead selecting priori best workﬂow better solution reason potential synergy Our POMDP model automatically tracks expected accuracy alternative workﬂows switching away average best workﬂow intermediate results lead conclude superior particular instance task hand 51 Probabilistic model multiple workﬂows We extend generative model worker responses incorporate existence multiple workﬂows complete task It includes multiple workﬂowspeciﬁc error parameters worker workﬂowspeciﬁc diﬃculties Now K alternative workﬂows worker use arrive answer Let dk 0 1 denote inherent 0 worker ws error parameter workﬂow k Notice diﬃculty completing task workﬂow k let γ k w worker K error parameters Having parameters worker incorporates insight workers perform asked question way visually asked different way asked English command language great The accuracy worker w adk γ k w probability produces correct answer workﬂow k We rewrite original deﬁnition worker accuracy accordingly cid3 cid2 dk γ k w 1 2 cid2 cid2 1 1 dk cid3 cid3γ k w 17 Fig 17 illustrates plate notation generative model encodes Bayes Net responses W workers T tasks solved set K alternative workﬂows The correct answer v diﬃculty parameter d error parameter γ inﬂuence ﬁnal answer b worker provides observed variable d generated δ K dimensional random variable describing joint distribution workﬂow diﬃculties The answer w worker w error parameter γ k bk w provides task workﬂow k governed following equations cid2 bk w cid2 bk w P P cid3 cid3 cid5 cid5dk cid5 cid5dk v cid10 v cid3 cid2 dk γ k w cid2 dk γ k w 1 cid3 18 19 As underlying assumption given workﬂow diﬃculty dk true answer v bk w s inde pendent δ encodes assumption workﬂows independent The fact workﬂow easy imply related workﬂow easy Finally assume workers collaborate adversarial purposely submit incorrect answers Now discuss dynamically switch workﬂows obtain highest accuracy given task P Dai et al Artiﬁcial Intelligence 202 2013 5285 73 52 A decisiontheoretic agent Fig 18 AgentHunts decisions executing task In section answer following question Given speciﬁc task accomplished alternative workﬂows design agent leverage availability alternatives dynamically switching order achieve highquality solution We design automated agent named AgentHunt uses following POMDP Deﬁnition 7 The POMDP sixtuple cid3S A T R O P cid4 S d1 d2 dK vdk 0 1 v 0 1 dk diﬃculty kth workﬂow v true answer A ballot1 ballot2 ballotK submit true submit false R S A R described T Identity map O true false Boolean answer received ballot question P S O 0 1 deﬁned generative model As POMDP describing simple binary classiﬁcation workﬂow reward function maintains value submitting correct answer penalty submitting incorrect answer Additionally maintains cost AgentHunt incurs creates job We modify reward function match desired budgets accuracies Fig 18 ﬂowchart decisions AgentHunt As assume future worker average worker In words given workﬂow k future worker error parameter equal γ k 1 W w γ k w W number workers Also submitting answer AgentHunt updates records workers participated task believes correct answer following modiﬁed update rules For worker w 1 dkα submitted answer workﬂow k γ k w worker answer incorrectly α learning rate Any worker AgentHunt seen previously begins average γ k dkα worker answer correctly γ k w γ k w γ k w cid9 53 Learning model In order behave optimally AgentHunt needs learn γ values average worker error parameters γ joint workﬂow diﬃculty prior δ initial belief We consider unsupervised approaches learning oﬄine batch learning online RL 531 Oﬄine learning In approach ﬁrst collect training data having set workers complete set tasks set workﬂows This generates set worker responses b Since true answer values v unknown option supervised learning experts label true answers diﬃculties However option requires signiﬁcant expert time upfront instead use EM algorithm learn parameters jointly For EM purposes simplify model removing joint prior δ treat variables d γ parameters In Estep parameters ﬁxed compute posterior probabilities hidden true answers pvt b d γ task t The Mstep uses probabilities maximize standard expected complete loglikelihood Q d γ Q d γ E cid11 cid12 ln pv bd γ 20 expectation taken v given old values γ d After estimating hidden parameters AgentHunt compute γ k workﬂow k taking average learned γ k parameters Then learn δ ﬁt Truncated Multivariate Normal distribution learned d This diﬃculty prior determines initial belief state AgentHunt We complete initial belief state assuming correct answer distributed uniformly 2 alternatives 74 P Dai et al Artiﬁcial Intelligence 202 2013 5285 532 Online reinforcement learning Oﬄine learning model expensive temporally monetarily Moreover sure training data necessary agents ready act realworld An ideal AI agent learn acting realworld tune parameters acquires knowledge producing meaningful results We modify AgentHunt build RL twin AgentHuntRL able accomplish tasks right box AgentHuntRL starts uniform priors diﬃculties workﬂows When begins new task uses existing parameters recompute best policy uses policy guide set decisions After completing task AgentHuntRL recalculates maximumlikelihood estimates parameters γ d EM The updated parameters deﬁne new POMDP agent computes new policy future tasks This relearning POMDPsolving timeconsuming relearn resolve completing task We easily speed process solving tasks launching relearning phase As RL AgentHuntRL tradeoff taking possibly suboptimal actions order learn model world exploration taking actions believes optimal exploitation AgentHuntRL uses modiﬁcation standard cid10greedy approach 62 With probability cid10 AgentHuntRL uniformly choose suboptimal actions The exception submit answer believes incorrect help learn world 54 Experiments This section addresses following questions 1 In practice value gained switching different workﬂows task 2 What tradeoff cost accuracy 3 Previous decision theoretic crowdsourcing systems required initial training phase reinforcement learning provide similar beneﬁts training We choose NLP labeling task create K 2 alternative workﬂows described low To answer ﬁrst questions compare agents TurKontrol0 refer TurKontrol stateoftheart controller optimizing execution single best workﬂow AgentHunt switch workﬂows dynamically We ﬁrst compare simulation allow agents control live workers Amazon Mechanical Turk We answer question comparing AgentHunt AgentHuntRL 541 Implementation The POMDP manage belief state cross product Boolean answer continuous variables diﬃculties workﬂows Since solving POMDP continuous state space challenging discretize diﬃculty possible values leading world state space size 2 11 11 242 To solve POMDPs run ZMDP package17 300 s default Focused RealTime Dynamic Programming search strategy 57 Since cache complete POMDP policy advance AgentHunt control workﬂows real time Since discretized diﬃculty modify learning process slightly After learn values d round values nearest discretizations construct histogram count number times state appears training data Then use implicit joint distribution agents starting belief state smooth adding 1 bin Laplace smoothing 542 Evaluation task NER tagging In order test agents select task needed colleagues labeling training data namedentity recognition NER 52 NER tagging common problem NLP information extraction given body text Barack Obama thinks research bad subsequence text Barack Obama speciﬁes entity output set tags classify type entity person politician Since machine learning techniques create production NER systems large amounts labeled data form described needed Obtaining accurate training data minimal cost excellent test methods In consultation NER domain experts develop workﬂows task Fig 19 Both workﬂows begin providing users body text entity like Nixon concluded ﬁve days private talks Chinese leaders Beijing The ﬁrst workﬂow called WikiFlow ﬁrst uses Wikiﬁcation 4146 ﬁnd set possible Wikipedia articles describing entity Nixon ﬁlm Richard Nixon It displays articles including ﬁrst sentence article asks workers choose best describes entity Finally returns Freebase18 tags associated Wikipedia article selected worker The second workﬂow TagFlow asks users choose best set Freebase tags directly For example Free base tags associated Nixon ﬁlm filmfilm tags associated Richard Nixon includes peopleperson governmentus congressperson TagFlow displays tag sets corresponding different options asks worker choose tag set best describes entity mentioned sentence 17 httpwwwcscmuedutreyzmdp 18 wwwfreebasecom P Dai et al Artiﬁcial Intelligence 202 2013 5285 75 Fig 19 In NER task TagFlow considerably harder WikiFlow tags similar The correct tag set location Washington state county citytown 543 Experimental setup First gather training data Mechanical Turk We generate 50 NER tasks For task submit 40 identical WikiFlow jobs 40 identical TagFlow jobs Mechanical Turk At 001 job total cost 6000 including Amazon commission Using EM technique calculate average worker accuracies γ WF γ TF corresponding Wiki Flow TagFlow respectively Somewhat surprise ﬁnd γ TF 0538 γ WF 0547 average workers TagFlow slightly easier WikiFlow Note result implies AgentHunt create TagFlow job begin task We note difference γ TF γ WF inﬂuences switching behavior AgentHunt Intuitively ifAgentHunt given workﬂows average diﬃculties apart AgentHunt reluctant switch harder workﬂow Because ﬁnd TagFlow jobs slightly easier experiments set TurKontrol creates TagFlow jobs We use training data construct agents initial beliefs 544 Experiments simulation We ﬁrst run agents simulated environment On run simulator draws states agents initial belief distributions We ﬁx reward returning correct answer 0 vary reward penalty returning incorrect answer following values 10 100 1000 10 000 We set cost creating job simulated worker 1 We use discount factor 09999 POMDP For setting reward values run 1000 simulations report mean net utilities Fig 21 We ﬁnd stakes low agents behave identically However penalty incorrect answer increases AgentHunts ability switch workﬂows allows capture utility TurKon trol As expected agents submit increasing number jobs importance answer correctness rises process accuracies rise Fig 20 However agents accurate TurKontrol increase accuracy compensate exponentially growing penalties Instead AgentHunt experiences sublinear decline net utility TurKontrol sees exponential drop Fig 21 A Students ttest shows settings penalty 10 differences systems average net utilities statistically signiﬁcant When penalty 10 p 04 reward settings p 00001 Thus ﬁnd simulation AgentHunt outperforms TurKontrol metrics We analyze systems behaviors qualitatively As expected AgentHunt starts creating TagFlow job γ TF γ WF implies TagFlows lead higher worker accuracy average Interestingly AgentHunt 76 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 20 In simulation importance answer correctness increases agents converge 100 percent accuracy AgentHunt quickly Fig 21 In simulation importance answer correctness increases AgentHunt outperforms TurKontrol everincreasing margin Table 3 Comparisons accuracies costs net utilities agents run Mechanical Turk AgentHunt TurKontrol Avg accuracy Avg cost Avg net utility 9245 581 1336 8585 421 1835 TurKontrol300 8491 626 2135 available workﬂows creates fewer actual jobs TurKontrol correct answers increasingly im portant We split problems categories better understand agents behaviors 1 TagFlow easy 2 TagFlow hard WikiFlow easy 3 Both workﬂows diﬃcult In ﬁrst case agents terminate quickly AgentHunt spends little money requests WikiFlow jobs doublecheck learns TagFlow jobs In second case TurKontrol creates enormous number jobs decides submit answer AgentHunt terminates faster quickly deduces TagFlow hard switches creating easy WikiFlow jobs In case AgentHunt expectedly creates jobs TurKontrol terminating AgentHunt worse TurKontrol correctly deduces gathering information unlikely help 545 Experiments Mechanical Turk We run agents real data gathered Mechanical Turk We generate 106 new NER tasks experiment use gold labels supplied single expert Since simulations agents spend average money reward incorrect answer 100 use reward value realworld experiments As Table 3 shows AgentHunt fares remarkably better realworld TurKontrol A Students ttest shows difference average net utilities agents statistically signiﬁcant p 003 However TurKontrol spends leading naturally wonder difference utility accounted cost discrepancy Thus modify reward incorrect answer 300 TurKontrol create TurKontrol300 spends money AgentHunt P Dai et al Artiﬁcial Intelligence 202 2013 5285 77 Table 4 Comparisons accuracies costs net utilities agents run Mechanical Turk Avg accuracy Avg cost Avg net utility AgentHunt 9245 581 1336 AgentHuntRL 9340 725 1385 But modiﬁcation accuracy AgentHunt higher A Students ttest shows difference average net utilities AgentHunt TurKontrol300 statistically signiﬁcant p 001 showing realworld given similar budgets AgentHunt produces signiﬁcantly better results TurKontrol Indeed AgentHunt reduces error TurKontrol 45 error TurKontrol300 50 Surprisingly accuracy TurKontrol300 lower TurKontrol despite additional jobs attribute statistical variance 546 Adding reinforcement learning Finally compare AgentHunt AgentHuntRL AgentHuntRLs starting belief state uniform distribution world states assumes γ k 1 workﬂows To encourage exploration set cid10 01 We test 106 tasks described Table 4 reproduces results AgentHunt Table 3 alongside AgentHuntRL We AgentHuntRL achieves slightly higher accuracy AgentHunt difference net utilities statistically signiﬁcant p 04 means AgentHuntRL comparable AgentHunt suggesting AgentHunt perform box mode needing training phase 6 Related work Modeling repeated labeling face noisy workers received signiﬁcant attention ﬁrst use decision theory dynamically poll labels necessary Dawid et al 14 Romney et al 48 ﬁrsts incorporate worker accuracy model improve label quality Raykar et al 47 propose model parameters worker accuracy depend true answer Whitehill et al 67 address concern worker labels modeled independent given problem diﬃculty Welinder et al 66 design multidimensional model workers takes account competence expertise annotator bias Kamar et al 27 extract features task hand use Bayesian Structure Learning learn worker response model Parameswaran et al 43 conduct policy search ﬁnd optimal dynamic control policy respect constraints like cost accuracy Karger et al 28 develop algorithm generate graph represents assignment tasks workers infers correct answers based lowrank matrix approximation Given assumptions tasks share equal level diﬃculty workers correct randomly guess analytically prove optimality algorithm minimizing budget target error rate respect possible algorithm assumptions Our methods use general models fall outside assumptions provide utility maximization guarantees POMDP model inherits Wauthier et al 64 treat entire crowdsourcing pipeline data collection learning Bayesian framework Kajino et al 26 formulate repeated labeling problem convex optimization problem Lin et al 35 address case requesters enumerate possible answers worker solution space inﬁnitely large Other innovative workﬂows designed complex tasks example ﬁndﬁxverify intelligent edi tor 5 iterative dual pathways speechtotext transcription 34 counting calories food plate 42 Lasecki et al 32 design allows multiple users control interface realtime Control switched users depending better Kulkarni et al 30 crowd help design execution complex workﬂows Ipeirotis et al 23 observe workers tend bias multiplechoice annotation tasks They learn confusion matrix model error distribution individual workers However model assumes workers errors completely independent model handles situations workers correlated errors intrinsic diﬃculty task Kulkarni et al 31 design alternative crowdsourcing platform marketplace Instead intelligently routes work workers controls accuracy quality Ho et al 20 consider assign tasks workers ar rival Huang et al 22 look problem designing task budget time constraints They illustrate ap proach imagetagging task By wisely setting variables reward task number labels requested image increase number useful tags acquired Donmez et al 16 observe workers accuracies change time fatigue mood task similarity Rzeszotarski et al 51 propose use microbreaks overcome fatigue effects As approaches orthogonal like integrate methods fu ture Shahaf Horvitz 53 develop HTNplanner style decomposition algorithm ﬁnd coalition workers different skill sets solve task Some members coalition machines humans different skills 78 P Dai et al Artiﬁcial Intelligence 202 2013 5285 command different prices In contrast work Shahaf Horvitz consider methods learning models workers The beneﬁts combining disparate workﬂows previously observed Babbages Law Errors suggests accuracy numerical calculations increased comparing outputs methods 19 However previous work workﬂows combined manually AgentHunt embodies ﬁrst method automatically evaluating potential synergy dynamically switching workﬂows 7 Discussion future work We believe AI potential impact growing thousands requesters use crowdsourcing processes eﬃcient Our work extended important directions realizing vision We list Task economic uncertainty We far assumed price task constant input However ﬁguring actual value task important problem This problem diﬃcult highly dynamic nature crowdsourcing markets At supply end value job inﬂuenced workers expertise motivation work personal interests First plan explore approaches estimate value based passively observing workﬂow activity As ﬁrst step develop models allow estimation effective hourly wage job This problem nontrivial given sources variance including network delay workers technology adequacy worker multitasking Second explore active algorithms estimating models These approaches actively experiment payment choices order eﬃciently learn model Further active passive cases plan extend model handle tasks time constraints This require modeling relationship payment task completion speed Incentives Paying optional bonus exceptional work viable way motivating highquality results forming longterm collaborations competent workers Questions need answered 1 When pay bonus 2 Who receive bonus 3 How bonus An intuitive way pay portion monetary savings good result Another form incentive involves recognition leader boards Investigations need carried determine combination bonus forms magnitude effective Here plan study passive active approaches modeling incentive structure workﬂows A universal worker model A workers productivity varies time This fatigue level variance mood work eﬃciency day skillfulness acquired experiences similar tasks We plan generalize worker model integrating temporal parameters 1 time day day week year 2 workers consecutive working hours 3 cumulative hours type tasks Further performance crowdsourcing lowered noisy answers provided spammers malicious workers An important aspect model support inference spamming malicious intent Example selection reinforcement learning Our work consider problem carefully curating reinforce ment learning process online oﬄine For example given set tasks know large subset tasks solve update parameters know subset tasks use order All factors affect ability agent learn parameters accurately quickly We wish investigate questions future research Additive utility function Our utility function additive task instance treated independently The total utility achieve set tasks equal sum utilities achieve task Such assumption hold scenarios trying train machine learning classiﬁer crowdsourced labels Here active learning formalism appropriate 155569 Abstractness utility To work usable vast majority able elicit utility function users This issue approach decisiontheoretic formalizations The concept util ity abstract easily constructed domain knowledge An extension work consider ways derive utilities budgetary accuracy constraints far concrete comprehensible Alternatively cast utility elicitation problem 9 Generalpurpose crowdsourcing control Our longterm research goal automatically control workﬂows consisting broader range tasks 65 To achieve goal need specify general language workﬂows develop methods translating workﬂow speciﬁcations decision problems We build worker model task type However believe crowdsourcing tasks captured relatively small number task classes tasks discrete alternatives content generation By having task library share parameters similar P Dai et al Artiﬁcial Intelligence 202 2013 5285 79 tasks Given new task transfer knowledge related tasks library simplifying expediting model training process We currently developing generalpurpose workﬂow controller called CLOWDER given new workﬂow automatically convert POMDP control requester Such software extremely important realizing vision techniques easily accessible requesters advanced AI knowledge 8 Conclusions This work applies modern planning machine learning techniques quality control problem crowdsourcing new rapidlygrowing method accomplishing tasks demonstrates usefulness decision theory realworld setting controlling dynamic workﬂows successfully achieve statisticallysigniﬁcant higherquality results traditional nonadaptive workﬂows In particular following contributions19 1 As complex workﬂows commonplace crowdsourcing regularly employed highquality output introduce exciting new application artiﬁcial intelligence control workﬂows 2 We use POMDPs model single workﬂows multiple workﬂows deﬁne generative models gov ern observation models processes Our agents implement mathematical framework use optimize control selection execution workﬂows Extensive simulations realworld experiments agents robust variety scenarios parameter settings achieve higher utilities previous nonadaptive policies 3 We present eﬃcient cheap mechanisms learning model parameters limited noisy training data We validate parameters independently learned models signiﬁcantly outperform popular majority vote baseline resources constrained Acknowledgements Thanks anonymous reviewers Jonathan Bragg helpful comments manuscript Conversations Lydian Chilton Greg Little informed thoughts crowdsourcing This work supported WRFTJ Cable Professorship Oﬃce Naval Research grants N000141210211 N000140610147 National Science Foundation grants IIS 1016713 IIS 1016465 Appendix A Snapshots tasks Fig 22 Snapshot improvement HIT 19 Software packages implementations available general use httpcswashingtonedunode7714 80 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 23 Snapshot complete ballot HIT P Dai et al Artiﬁcial Intelligence 202 2013 5285 81 Fig 24 Snapshot instruction portion ballot HIT Fig 25 Snapshot example portion ballot HIT Fig 26 Snapshot actual task portion ballot HIT 82 P Dai et al Artiﬁcial Intelligence 202 2013 5285 Fig 27 Snapshot instruction portion scoring HIT Fig 28 Snapshot example portion scoring HIT P Dai et al Artiﬁcial Intelligence 202 2013 5285 83 References Fig 29 Snapshot actual task portion scoring HIT 1 RadhaKrishna Balla Alan Fern UCT tactical assault planning realtime strategy games IJCAI 2009 pp 4045 2 R Bellman Dynamic Programming Princeton University Press Princeton NJ 1957 3 Michael S Bernstein Joel Brandt Robert C Miller David R Karger Crowds seconds Enabling realtime crowdpowered interfaces UIST 2011 4 Michael S Bernstein Greg Little Robert C Miller Björn Hartmann Mark S Ackerman David R Karger David Crowell Katrina Panovich Soylent A word processor crowd inside UIST 2010 pp 313322 5 Michael S Bernstein Greg Little Robert C Miller Björn Hartmann Mark S Ackerman David R Karger David Crowell Katrina Panovich Soylent A word processor crowd inside UIST 2010 6 Dimitri P Bertsekas Dynamic Programming Optimal Control vol 1 2nd ed Athena Scientiﬁc 2000 7 Jeffrey P Bigham Chandrika Jayant Hanjie Ji Greg Little Andrew Miller Robert C Miller Robin Miller Aubrey Tatarowicz Brandyn White Samuel White Tom Yeh Vizwiz Nearly realtime answers visual questions UIST 2010 pp 333342 8 Emma Brunskill Leslie Kaelbling Tomas LozanoPerez Nicholas Roy Continuousstate POMDPs hybrid dynamics Symposium Artiﬁcial Intelligence Mathematics 2008 9 Urszula Chajewska Daphne Koller Ronald Parr Making rational decisions adaptive utility elicitation AAAI 2000 10 Seth Cooper Firas Khatib Adrien Treuille Janos Barbero Jeehyung Lee Michael Beene Andrew LeaverFay David Baker Zoran Popovic Foldit players Predicting protein structures multiplayer online game Nature 446 2010 756760 11 Dan Cosley Shyong K Lam Istvan Albert Joseph A Konstan John Riedl Is seeing believing How recommender interfaces affect users opinions Proceedings SIGCHI Conference Human Factors Computing Systems 2003 pp 585592 12 Peng Dai Mausam Daniel S Weld Decisiontheoretic control crowdsourced workﬂows AAAI 2010 84 P Dai et al Artiﬁcial Intelligence 202 2013 5285 13 Peng Dai Mausam Daniel S Weld Artiﬁcial intelligence artiﬁcial intelligence AAAI 2011 14 AP Dawid AM Skene Maximum likelihood estimation observer errorrates em algorithm Appl Stat 28 1 1979 2028 15 Pinar Donmez Jaime G Carbonell Jeff Schneider Eﬃciently learning accuracy labeling sources selective sampling KDD 2009 16 Pinar Donmez Jaime G Carbonell Jeff Schneider A probabilistic framework learn multiple annotators timevarying accuracy SIAM International Conference Data Mining SDM 2010 pp 826837 17 Arnaud Doucet Nando Freitas Neil Gordon Sequential Monte Carlo Methods Practice Springer 2001 18 Sylvain Gelly Yizao Wang Exploration exploitation UCT Monte Carlo NIPS Online Trading Exploration Exploitation Workshop 2006 19 David Alan Grier Error identiﬁcation correction human computation Lessons WPA HCOMP 2011 20 ChienJu Ho Jennifer Wortman Vaughan Online task assignment crowdsourcing markets AAAI 2012 21 Leah Hoffmann Crowd control Comm ACM 52 3 2009 1617 22 Eric Huang Haoqi Zhang David C Parkes Krzysztof Z Gajos Yiling Chen Toward automatic task design A progress report Proceedings ACM SIGKDD Workshop Human Computation 2010 pp 7785 23 Panagiotis G Ipeirotis Foster Provost Jing Wang Quality management Amazon Mechanical Turk Proceedings ACM SIGKDD Workshop Human Computation 2010 pp 6467 24 Hyun Joon Jung Matthew Lease Spam worker ﬁltering featuredvoting based consensus accuracy improvement Proc CrowdConf 2011 25 Leslie Pack Kaelbling Michael L Littman Anthony R Cassandra Planning acting partially observable stochastic domains Artiﬁcial Intelli gence 101 12 1998 99134 26 Hiroshi Kajino Yuta Tsuboi Hisashi Kashima A convex formulation learning crowds AAAI 2012 27 Ece Kamar Severin Hacker Eric Horvitz Combining human machine intelligence largescale crowdsourcing AAMAS 2012 28 David R Karger Sewoong Oh Devavrat Shah Budgetoptimal crowdsourcing lowrank matrix approximations Allerton 2011 29 Levente Kocsis Csaba Szepesvári Bandit based Monte Carlo planning ECML 2006 pp 282293 30 Anand Kulkarni Matthew Can Bjorn Hartmann Collaboratively crowdsourcing workﬂows Turkomatic Proceedings CSCW 2012 31 Anand Kulkarni Philipp Gutheim Prayag Narula David Rolnitzky Tapan Parikh Bjoern Hartmann Mobileworks Designing quality managed crowdsourcing architecture HCOMP 2012 32 Walter S Lasecki Kyle I Murray Samuel White Robert C Miller Jeffrey P Bigham Realtime crowd control existing interfaces Proceedings UIST 2011 33 Edith Law Luis von Ahn Human Computation Synthesis Lectures Artiﬁcial Intelligence Machine Learning Morgan Claypool Publishers 2011 34 Beatrice Liem Haoqi Zhang Yiling Chen An iterative dual pathway structure speechtotext transcription HCOMP 2011 35 Christopher H Lin Mausam Mausam Daniel S Weld Crowdsourcing control Moving multiple choice UAI 2012 36 Christopher H Lin Mausam Daniel S Weld Dynamically switching synergistic workﬂows crowdsourcing AAAI 2012 37 Greg Little Lydia B Chilton Max Goldman Robert C Miller TurKit Tools iterative tasks Mechanical Turk KDD Workshop Human Computation 2009 pp 2930 38 Omid Madani Steve Hanks Anne Condon On undecidability probabilistic planning related stochastic optimization problems Artiﬁcial Intelligence 147 12 2003 534 39 Mausam Emmanuel Benazera Ronen I Brafman Nicolas Meuleau Eric A Hansen Planning continuous resources stochastic domains IJCAI 2005 pp 12441251 40 Mausam Andrey Kolobov Planning Markov Decision Processes An AI Perspective Morgan Claypool 2012 41 David Milne Ian H Witten Learning link Wikipedia Proceedings ACM Conference Information Knowledge Management 2008 42 John Noronha Eric Hysen Haoqi Zhang Krzysztof Z Gajos Platemate Crowdsourcing nutrition analysis food photographs UIST 2011 43 Aditya Parameswaran Hector GarciaMolina Hyunjung Park Neoklis Polyzotis Aditya Ramesh Jennifer Widom Crowdscreen Algorithms ﬁltering data humans VLDB 2010 44 Joelle Pineau Geoffrey Gordon Sebastian Thrun Anytime pointbased approximations large POMDPs J Artiﬁcial Intelligence Res 27 1 2006 335380 45 Josep M Porta Nikos Vlassis Matthijs TJ Spaan Pascal Poupart Pointbased value iteration continuous POMDPs J Mach Learn Res 7 2006 23292367 46 Lev Ratinov Dan Roth Doug Downey Mike Anderson Local global algorithms disambiguation Wikipedia Proceedings Annual Meeting Association Computational Linguistics 2011 47 Vikas C Raykar Shipeng Yu Linda H Zhao Gerardo Valadez Learning crowds J Mach Learn Res 11 2010 12971322 48 A Kimball Romney Susan C Weller William H Batchelder Culture consensus A theory culture informant accuracy Am Anthropol 88 2 1986 313338 49 Joel Ross Lilly Irani M Six Silberman Andrew Zaldivar Bill Tomlinson Who crowdworkers Shifting demographics Mechanical Turk CHI 2010 50 Nicholas Roy Sebastian Thrun Coastal navigation mobile robots Advances Neural Processing Systems 12 1999 pp 10431049 51 Jeffery Rzeszotarski Ed Chi Praveen Paratosh Peng Dai And completely different Introducing microbreaks crowdsourcing workﬂows Submission 2013 52 E Tjong Kim Sang F De Meulder Introduction CoNNL2003 shared task Languageindependent named entity recognition Proceedings CoNNL 2003 53 Dafna Shahaf Eric Horvitz Generalized markets human machine computation AAAI 2010 54 Guy Shani Ronen I Brafman Solomon Eyal Shimony Prioritizing pointbased POMDP solvers IEEE Trans Syst Man Cybern Part B Cybern 38 6 2008 15921605 55 Victor S Sheng Foster Provost Panagiotis G Ipeirotis Get label Improving data quality data mining multiple noisy labelers Proceedings Fourteenth ACM SIGKDD International Conference Knowledge Discovery Data Mining 2008 56 David Silver Joel Veness Monte Carlo planning large POMDPs NIPS 2010 pp 21642172 57 Trey Smith Reid G Simmons Focused realtime dynamic programming MDPs Squeezing heuristic AAAI 2006 58 Rion Snow Brendan OConnor Daniel Jurafsky Andrew Y Ng Cheap fast good Evaluating nonexpert annotations natural language tasks EMNLP 2008 pp 254263 59 Edward J Sondik The optimal control partially observable Markov processes PhD thesis Stanford 1971 60 Alexander Sorokin David Forsyth Utility data annotation Amazon Mechanical Turk Computer Vision Pattern Recognition Workshop 2008 pp 18 61 Matthijs TJ Spaan Nikos Vlassis Perseus Randomized pointbased value iteration POMDPs J Artiﬁcial Intelligence Res 24 2005 195220 62 Richard S Sutton Andrew G Barto Reinforcement Learning The MIT Press 1998 63 Luis von Ahn Games purpose IEEE Comput Mag June 2006 9698 P Dai et al Artiﬁcial Intelligence 202 2013 5285 85 64 Fabian L Wauthier Michael I Jordan Bayesian bias mitigation crowdsourcing NIPS 2011 65 Daniel S Mausam Mausam Peng Dai Human intelligence needs artiﬁcial intelligence HCOMP 2011 66 Peter Welinder Steve Branson Serge Belongie Pietro Perona The multidimensional wisdom crowds NIPS 2010 67 Jacob Whitehill Paul Ruvolo Jacob Bergsma Tingfan Wu Javier Movellan Whose vote count Optimal integration labels labelers unknown expertise NIPS 2009 68 Jacob Whitehill Paul Ruvolo Tingfan Wu Jacob Bergsma Javier Movellan Whose vote count Optimal integration labels labelers unknown expertise Proc NIPS 2009 pp 20352043 69 Yan Yan Romer Rosales Glenn Fung Jennifer G Dy Active learning crowds ICML 2011