Artiﬁcial Intelligence 172 2008 14701494 Contents lists available ScienceDirect Artiﬁcial Intelligence wwwelseviercomlocateartint Enhanced qualitative probabilistic networks resolving tradeoffs Silja Renooij Linda C van der Gaag Department Information Computing Sciences Utrecht University PO Box 80089 3508 TB Utrecht The Netherlands r t c l e n f o b s t r c t Article history Received 18 October 2006 Received revised form 9 April 2008 Accepted 14 April 2008 Available online 20 April 2008 Keywords Probabilistic reasoning Qualitative reasoning Tradeoff resolution Qualitative probabilistic networks designed overcome extent quantiﬁcation problem known probabilistic networks Qualitative networks abstract numerical probabilities quantitative counterparts signs summarise probabilistic inﬂuences variables One major drawbacks qualitative abstractions coarse level representation provide indicating strengths inﬂuences As result tradeoffs modelled network remain unresolved inference We present enhanced formalism qualitative probabilistic networks provide ﬁner level representation An enhanced qualitative probabilistic network differs basic qualitative network distinguishes strong weak inﬂuences Now strong inﬂuence combined inference conﬂicting weak inﬂuence sign net inﬂuence readily determined Enhanced qualitative networks purely qualitative nature basic qualitative networks allow resolving tradeoffs inference 2008 Elsevier BV All rights reserved 1 Introduction The formalism probabilistic networks introduced 1980s 26 intuitively appealing formalism capturing knowledge complex problem domains uncertainties involved Associated formalism powerful algorithms reasoning uncertainty mathematically correct way These algorithms probabilistic inference allow causal reasoning diagnostic reasoning casespeciﬁc reasoning probabilistic inference known NPhard 7 Applications probabilistic networks areas medical diagnosis prognosis planning monitoring vision information retrieval example 12452031 A probabilistic network basically concise representation joint probability distribution set statistical variables It consists acyclic directed graph encoding relevant variables domain application probabilistic interrelationships Associated variable set conditional probability distributions describing relationship variable predecessors graph The ﬁrst task constructing probabilistic network identify important domain variables values interdependencies This knowledge modelled directed graph referred networks qualitative The ﬁnal task obtain probabilities constitute networks quantitative As conditional probability distributions stated variable graph number required probabilities large small applications While construction qualitative probabilistic network generally considered feasible quantiﬁcation far harder task Probabilistic information available literature data insuﬃcient unusable domain experts relied assess This research partly supported Netherlands Organisation Scientiﬁc Research NWO We like thank Hans Bodlaender advice relating complexity issues addressed paper Corresponding author Email addresses siljacsuunl S Renooij lindacsuunl LC van der Gaag 00043702 matter 2008 Elsevier BV All rights reserved doi101016jartint200804001 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1471 required probabilities 14 Unfortunately experts uncomfortable having provide probabilities Moreover problems bias encountered directly eliciting probabilities experts widely known 19 The usually large number probabilities required probabilistic network consequence tends pose major obstacle application 1418 To mitigate quantiﬁcation bottleneck extent qualitative probabilistic networks intro duced 34 Qualitative networks essence qualitative abstractions probabilistic networks Like probabilistic network qualitative network encodes variables probabilistic relationships directed graph However relationships represented variables quantiﬁed conditional probabilities prob abilistic network relationships summarised qualitative abstraction qualitative signs capturing stochastic dominance The probabilistic information captured signs robust exact numbers easily obtained domain experts 10 Elicitation methods end designed 33 Originally beneﬁts qualitative probabilistic networks included complexity inference reasoning qualitative probabilistic network eﬃcient algorithm available based idea propagating combining signs 11 In practice nowadays complexity probabilistic inference problem qualitative probabilistic networks shifted construction validation phase probabilistic networks reallife application domains As assessment probabilities required hard task performed probabilistic networks graph considered robust Now assessing signs inﬂuences modelled graph qualitative network obtained exploited studying projected probabilistic networks reasoning behaviour prior assessment probabilities Patterns qualitative inﬂuences recognise different types causal interaction noisyor greatly simplify quantiﬁcation effort 24 In addition qualitative signs ways constraints quantiﬁcation For example interpreting signs continuous subintervals probability interval constraints impose conditional probability distributions involved stepwise quantiﬁcation probabilistic network conditional probability table certain variable ﬁlled interval associated direct inﬂuences variable tightened 28 These semiqualitative probabilistic networks include assessments based probabilistic logic credal sets 6 More recently signs qualitative probabilistic networks constrain probabilities learned small data sets 31517 At somewhat higher level constraints imposed qualitative inﬂuences bound entire space pos sible joint probability distributions networks variables 13 Finally qualitative signs verifying monotonicity properties probabilistic network 32 explanation qualitative probabilistic networks rea soning processes 10 Given increasing variety useful applications qualitative probabilistic networks important derive information possible networks Qualitative probabilistic networks nature coarse level representation Inﬂuential relationships variables modelled positive negative zero ambiguous indication strengths provided quantiﬁed network One major drawbacks coarse level representation ease ambiguous sign arises inference Ambiguous signs typically arise tradeoffs A qualitative network models tradeoff nodes networks digraph connected multiple parallel reasoning chains conﬂicting signs In absence notion strength inﬂuences qualitative networks provide resolving tradeoffs Inference qualitative network reallife domain application consequence introduces ambiguous signs Moreover ambiguous sign generated spread major parts network Although incorrect ambiguous signs provide information whatsoever inﬂuence variable useful practice Ambiguous results inference averted enhancing formalism qualitative probabilistic networks provide ﬁner level representation Roughly speaking ﬁner level tradeoffs resolved inference The ﬁner levels typically come price higher computational com plexity inference The problem tradeoff resolution qualitative networks addressed researchers relation work Related work section paper In short S Parsons ex ample introduced concept categorical inﬂuence inﬂuence serves increase probability 1 inﬂuence decreases probability 0 serves resolve tradeoff involved 25 Parsons studied use orderofmagnitude reasoning context qualitative probabilistic networks 25 CL Liu MP Wellman designed methods resolving tradeoffs based idea reverting numer ical probabilities necessary 23 While tradeoffs resolved use categorical inﬂuences methods Liu Wellman provide resolving tradeoff require availability fully quantiﬁed proba bilistic network To provide qualitative tradeoff resolution resorting numerical probabilities designed intu itively appealing formalism enhanced qualitative networks An enhanced qualitative probabilistic network differs basic qualitative network introduces notion relative strength distinguishing strong weak ﬂuences The distinction strong weak inﬂuences intuitive domain experts problems providing interpreting associated signs Now tradeoff modelled enhanced network positive inﬂuence example known stronger conﬂicting negative inference conclude net inﬂuence positive Tradeoff resolution inference builds idea strong inﬂuences dominate conﬂicting weak inﬂuences To provide inference enhanced network generalised signpropagation 1472 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 algorithm basic qualitative networks deal strong weak inﬂuences This generalisation straightforward establish properties basic signpropagation algorithm based provided en hanced network The new inference algorithm takes account effect variable diminishes variables apart networks graph takes account variable affect variable multiple pathways differing strengths To maintain correct strengths indirect inﬂuences algorithm additional bookkeeping result eﬃcient inference algorithm basic qualitative networks The paper organised follows In Section 2 provide preliminaries ﬁelds probabilistic networks qualitative networks introduce notational conventions In Section 3 present new formalism enhanced qualitative probabilistic networks In Section 4 properties enhanced networks build new signpropagation algorithm Section 5 provides example inference enhanced qualitative probabilistic network discusses complexity issues concerning signpropagation Related work reviewed Section 6 The paper rounded conclusions directions future research Section 7 2 Preliminaries In section brieﬂy review probabilistic networks qualitative counterparts 21 Probabilistic networks A probabilistic network models domain application basically representing concise way joint probability distribution set statistical variables relevant application domain 26 A probabilistic network B G Pr encodes acyclic directed graph G V G AG relevant variables probabilistic interrelation ships Each node A V G represents statistical variable ﬁnite set values We assume total order values variable Variables indicated capital letters We restrict binary valued variables write denote A true denote A false As onetoone correspondence variables nodes use terms node variable interchangeably The probabilistic relationships represented variables captured digraphs set arcs AG In formally speaking arc A B G represent inﬂuential relationship variables A B designating B effect cause A Given arc A B node A called immediate predecessor node B node B called successor node A We write π A denote set predecessors node A G π A denote set ancestors similarly σ A denote set successors node A σ A denote descendants Two variables A B said connected simple trail G iff connected simple path underlying undirected graph G Absence arc variables digraph probabilistic network means variables inﬂuence directly conditionally independent More formally probabilistic independence read digraph means dseparation criterion builds concept blocking 26 A trail variables said blocked available evidence includes observed variable outgoing arc unobserved variable incoming arcs observed descendants Two variables said dseparated trails blocked case considered conditionally independent given available evidence A trail blocked called active If active trail connects Markovblanket neighbours variables sharing arc common child variables said active neighbours Associated variable A V G networks digraph G set conditional probability distributions Pr A π A strengths dependences A immediate predecessors These conditional probabilities provide information necessary uniquely deﬁning joint probability distri bution networks variables probabilistic network B G Pr deﬁnes distribution Pr V G cid2 Pr cid3 V G cid4 cid2 Pr cid3 A π A AV G respects independences portrayed digraph G Since probabilistic network captures unique joint probability distribution provides computing prior posterior probability variables To end algorithms available 2226 We piece ﬁctitious medical knowledge serve example domain application paper Example 21 Our example application domain pertains effects administering antibiotics patient involves ﬁve statistical variables interrelationships Node A represents patient taking antibiotics Node T models patient suffering typhoid fever node D represents presence absence diarrhoea patient node H represents patient dehydrated Node F conclude describes composition bacterial ﬂora patients intestines changed Typhoid fever change bacterial ﬂora possible causes diarrhoea Diarrhoea turn cause dehydration Antibiotics cure S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1473 Fig 1 The Antibiotics domain captured probabilistic network b qualitative probabilistic network b typhoid fever killing bacteria cause infection As result probability patient contracting diarrhoea decreases However antibiotics change composition intestinal bacterial ﬂora increasing risk diarrhoea Fig 1a depicts probabilistic network captures knowledge domain 22 Qualitative probabilistic networks Qualitative probabilistic networks bear strong resemblance quantitative counterparts 34 Instead representing joint probability distribution set statistical variables relevant application domain represents qualitative constraints distribution A qualitative probabilistic network Q G Δ comprises acyclic digraph G V G AG modelling variables probabilistic relationships Moreover set arcs AG digraph models probabilistic independence Instead conditional probability distributions qualitative probabilistic network associates digraph set Δ qualitative inﬂuences qualitative synergies A qualitative inﬂuence variables expresses values variable inﬂuence probabilities values variable direction shift distribution higher values likely likely indicated sign inﬂuence A positive qualitative inﬂuence variable A variable B example expresses observing higher value A makes higher value B likely regardless inﬂuences B 34 Deﬁnition 22 Let G V G AG acyclic digraph let Pr joint probability distribution V G respects independences G Let A B variables G A B AG Then variable A positively inﬂuences variable B arc A B written S A B iff Prb ax Prb ax cid2 0 combination values x set π B A predecessors B A A negative qualitative inﬂuence denoted S zero qualitative inﬂuence denoted S 0 deﬁned analogously replacing cid2 formula cid3 respectively If inﬂuence variable A variable B monotonic unknown ambiguous denoted S A B With arc digraph qualitative probabilistic network qualitative inﬂuence associated Variables inﬂuence directly arcs exert indirect inﬂuences The deﬁnition qualitative inﬂuence trivially extends indirect inﬂuences inﬂuences active trails We denote indirect inﬂuence sign δ active trail t variable A variable B ˆSδ A B t From term trail refer simple trail basically consisting concatenation arcs subgraph containing simple trails variables The type trail said consist composition simple trails The set variables trail t denoted V t The set inﬂuences qualitative probabilistic network exhibits convenient properties constitute basis eﬃcient algorithm qualitative probabilistic inference 34 The property symmetry guarantees network includes inﬂuence Sδ A B includes SδB A sign δ 0 The property transitivity asserts qualitative inﬂuences active trail headtohead nodes nodes incoming arcs trail combine indirect inﬂuence sign determined operator Table 1 The property composition asserts multiple qualitative inﬂuences variables parallel active trails combine composite inﬂuence sign determined operator From Table 1 observe combining nonambiguous qualitative inﬂuences operator yield ambiguous result Such ambiguity fact results parallel inﬂuences opposite signs combined We tradeoff reﬂected conﬂicting inﬂuences resolved Note contrast operator operator introduce ambiguities combining signs The operators Table 1 adhere standard algebraic properties commutativity associativity distributivity In addition inﬂuences qualitative probabilistic network includes synergies model interactions triples variables An additive synergy example captures joint inﬂuence variables common succes 1474 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 Table 1 The operators 0 0 0 0 0 0 0 0 0 0 0 0 sor 34 A positive additive synergy variables A B variable C speciﬁcally expresses joint inﬂuence A B C greater separate inﬂuences Deﬁnition 23 Let G V G AG acyclic digraph let Pr joint probability distribution V G respects independences G Let A B C variables G A C B C AG Then variables A B exhibit positive additive synergy C iff Prc abx Prc abx Prc abx Prc abx cid2 0 combination values x set π C A B predecessors C A B Negative zero ambiguous additive synergies deﬁned analogously If variables A B common successor C observation value variable C serves activate trail A C B The observation induces dependence A B This dependence represented qualitative inﬂuence A B vice versa Such induced inﬂuence commonly known intercausal inﬂuence The sign intercausal inﬂuence captured sign product synergy associated variables involved observation A product synergy expresses value variable inﬂuences probabilities values variable view given value variable 12 A negative product synergy A B C value c example expresses given c high value A renders high value B likely reasoning pattern known explaining away 26 Deﬁnition 24 Let G V G AG acyclic digraph let Pr joint probability distribution V G respects independences G Let A B C variables G A C B C AG Then variables A B exhibit negative product synergy variable C value c denoted X A B c iff Prc abxPrc abx Prc abxPrc abx cid3 0 combination values x set π C A B predecessors C A B Positive zero ambiguous product synergies deﬁned analogously With triple variables A B C V G A C B C AG additive synergy product synergies associated Note product synergy deﬁned possible value C These qualitative synergies trivially extended trails exhibit symmetry transitivity composition properties For details refer 2734 Example 25 We consider qualitative probabilistic network representation Antibiotics domain Fig 1b The ﬁgure displays signs qualitative inﬂuences arcs additive synergy curve node D product synergies dotted edge The qualitative inﬂuence S A T speciﬁes difference conditional probabilities Prt Prt zero Indeed conditional probabilities speciﬁed variable T probabilistic network representation domain Fig 1a Prt Prt 001 035 034 cid3 0 Similar observations hold S A F S D H From conditional probabilities speciﬁed variable D Prd t f Prd t f 095 015 080 cid2 0 Prd t f Prd t f 080 001 079 cid2 0 obey constraints posed S quantiﬁed network In networks variables T F exert positive additive synergy Y variable D T D We similarly ﬁnd qualitative inﬂuence S F D preserved T F D Prd t f Prd t f Prd t f Prd t f 001 cid2 0 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1475 procedure PropagateObservationQ O sign Observed V V G signV 0 PropagateSign O sign procedure PropagateSigntrail messagesign signto signto messagesign trail trail active neighbour V given O Observed linksign sign induced inﬂuence V messagesign signto linksign V trail signV cid10 signV messagesign PropagateSigntrail V messagesign Fig 2 The signpropagation algorithm qualitative probabilistic inference Either value D addition induces negative intercausal inﬂuence variables T F probabilistic network representation For example Prd t f Prd t f Prd t f Prd t f 012 cid3 0 For reasoning qualitative probabilistic network eﬃcient algorithm available MJ Druzdzel M Hen rion 11 algorithm termed signpropagation algorithm summarised pseudocode Fig 2 The basic idea algorithm trace effect observing variables value probabilities values variables network messagepassing neighbouring nodes In essence algorithm computes sign net inﬂuence active trails newly observed variable variables network building properties symmetry transitivity composition inﬂuences For variable summarises net inﬂuence nodesign indicates direction shift variables probability distribution occasioned new observation The signpropagation algorithm takes input qualitative probabilistic network Q set Observed previously observed variables variable O observation available sign sign new observation value true value false Prior propagation new observation variables V nodesign signV set 0 For newly observed variable O appropriate sign entered network The observed variable updates nodesign signsum original sign entered sign It reports change sign active neighbours variables Markovblanket reached trail blocked set Observed This notiﬁcation passing message containing appropriate sign signproduct variables current nodesign sign linksign inﬂuence associated arc induced intercausal link traverses Each message records origin variable trail information prevent messages passed nodes visited trail Upon receiving message variable updates nodesign signsum current nodesign signto sign messagesign message received The variable sends copy message neighbours need reconsider nodesigns In variable changes sign copy appropriate sign adds trail origin copy Note process repeated network trails messages passed recorded Also note messages travel simple trails suﬃcient record nodes trails During signpropagation variables visited need change nodesign A nodesign change twice 0 From observation variable visited twice inference The algorithm guaranteed halt For proof algorithms correctness refer reader 11 We illustrate signpropagation algorithm means running example Example 26 We consider qualitative Antibiotics network Fig 1b Suppose patient taking antibiotics This observation entered network updating nodesign variable A Variable A propagates message sign T Variable T updates nodesign sends message sign D Variable D updates sign sends message sign H Variable H updates nodesign sends messages neighbours need update sign Variable D pass sign F trail T D F active Variable A sends message sign F Variable F updates nodesign accordingly passes message sign D Variable D receives additional sign This sign combined previously updated nodesign results ambiguous nodesign D Note ambiguous sign arises tradeoff represented variable D D sends message sign H updates sign Note network contained additional variables variables D andor H variables ended nodesign inference 1476 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 3 The enhanced formalism Qualitative probabilistic networks capture knowledge problem domain coarse level representation Qualitative inﬂuences variables example captured simple signs indication strengths As consequence tradeoff encountered inference remain unresolved In section present new formalism qualitative probabilistic networks allows ﬁner level representation enable resolving tradeoffs extent In new formalism enhance qualitative probabilistic networks associating indication relative strength inﬂuences Now example encountering tradeoff inference positive inﬂuence known stronger conﬂicting negative conclude combined inﬂuence positive effectively resolving tradeoff In enhanced qualitative probabilistic network distinguish strong weak inﬂuences Intuitively strong inﬂuence variable A variable B inﬂuence stronger weak inﬂuence network property cid5 cid5 cid5Prb ax Prb ax cid5 cid2 cid5 cid5 cid5Prd c y Prd c y cid5 holds variables C D weak inﬂuence combination values x y sets X Y relevant predecessors The basic idea partition set direct inﬂuences network disjoint sets way inﬂuence subset stronger inﬂuence subset To end introduce cutoff value α serves partition set direct qualitative inﬂuences set inﬂuences capture absolute difference probabilities α set inﬂuences model absolute difference α An inﬂuence subset termed strong inﬂuence inﬂuence subset termed weak inﬂuence Deﬁnition 31 Let G V G AG acyclic digraph let Pr joint probability distribution V G respects independences G Let A B variables G A B AG Let α 0 1 cutoff value The inﬂuence variable A variable B arc A B strongly positive denoted S A B iff Prb ax Prb ax cid2 α combination values x set π B A predecessors B A The inﬂuence variable A variable B arc weakly positive denoted S A B iff 0 cid3 Prb ax Prb ax cid3 α combination values x Strongly negative qualitative inﬂuences denoted S deﬁned analogously zero qualitative inﬂuences ambiguous qualitative inﬂuences deﬁned basic qualitative probabilistic networks In special case inﬂuence variable A variable B difference probabilities Prb ax Prb ax equals α x inﬂuence strong weakly negative qualitative inﬂuences denoted S A product synergy deﬁned strongly negative induces strongly negative intercausal inﬂuence Weakly negative strongly positive weakly positive product synergies deﬁned analogously zero product synergies ambiguous product synergies deﬁned basic qualitative networks For additive synergies distinction weak strong slightly complicated Since additive synergies signpropagation contribute resolution tradeoffs consider paper Upon abstracting quantiﬁed probabilistic network enhanced qualitative probabilistic network cutoff value α need chosen explicitly This cutoff value typically vary application application possible choose cutoff value values α 0 α 1 yield trivial partitioning set inﬂuences In reallife applications enhanced qualitative probabilistic networks cutoff value need established explicitly The partitioning strong weak inﬂuences elicited directly domain experts involved construction network Example 32 We consider Fig 1 showing qualitative quantitative probabilistic network representations Antibiotics domain Example 21 An enhanced qualitative probabilistic network representation domain given Fig 3 showing qualitative inﬂuences involved In addition qualitative inﬂuences strongly negative product synergy variables T F D d weakly negative product synergy D d We note basic signs qualitative inﬂuences enhanced representation consistent signs qualitative inﬂuences basic qualitative network representation Fig 1b We enhanced representation consistent completely quantiﬁed representation Fig 1a choose cutoff value α 030 From S A T conclude Prt Prt negative absolute value α 030 Prt Prt cid3 0 cid5 cid5 cid5 034 cid2 α cid5Prt Prt S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1477 Fig 3 The enhanced qualitative Antibiotics network A F We similarly ﬁnd conditional probabilities associated variables F D consistent S D H respectively For inﬂuence variable T variable D observe Prd t F Prd t F cid2 0 regardless S value F Prd t f Prd t f 080 Prd t f Prd t f 079 T D similarly ﬁnd exceed level cutoff value α We S F D The signs product synergies exhibited variables T F variable D presence S value D equal signs corresponding intercausal inﬂuences The intercausal inﬂuences deﬁned terms differences Pr f tx Pr f tx x represents different combinations values variables D A These probabilities network Example 21 applying Bayes rule list ease reference Pr f tx Pr f tx x da x da x da x da 054 052 020 017 094 092 049 041 For sign intercausal inﬂuence variable T variable F given value d D Pr f tda Pr f tda 040 cid3 α Pr f tda Pr f tda 040 cid3 α We conclude intercausal inﬂuence corresponding product synergy strongly negative X T F d similarly conﬁrm X T F d In enhanced formalism semantics sign inﬂuence slightly changed basic qualitative probabilistic network sign inﬂuence represents sign differences probability enhanced qual itative network sign addition captures relative magnitude differences These relative magnitudes correctly preserved inference indirect inﬂuences variables considered In fact demon strate Sections 42 43 strength direct inﬂuence deﬁned relative cutoff value α strengths indirect inﬂuences described terms polynomial expression α To capture polynomial introduce multiplicationindex list augment signs indirect inﬂuences list Before deﬁning multiplicationindex list augmented signs observe general nth order polynomial α written ncid6 i0 ci αi cid6 ci αi cid6 ci αi i0n ci 0 i0n ci 0 For purposes suﬃces consider polynomials α coeﬃcients ci Z c0 cid2 0 Since exponents non negative represent polynomial listing exponent ci times indication associated term added subtracted This list possibly negated exponents constitutes multiplication index list Deﬁnition 33 A multiplicationindex list I multiset i1 n cid2 1 index j I j 1 n integer The multiplicationindex list I captures polynomial α cid6 cid6 αi j αi j j cid20 j 0 A polynomial α captured multiplicationindex list I short denoted αI 1478 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 As example consider polynomial αI 2 α3 1 written α3 α3 α0 The multiset 3 3 0 deﬁnes multiplicationindex list I captures polynomial A sign augmented multiplicationindex list displayed attaching multiplicationindex list superscript omitting curly braces sake readability For example weakly positive sign multiplication index list I i1 written i1in I short The following deﬁnition formally describes meaning indirect inﬂuence augmented sign Deﬁnition 34 Let G V G AG acyclic digraph variables A B connected active trail t Let Pr joint probability distribution V G respects independences G Let α 0 1 cutoff value The inﬂuence variable A variable B trail t strongly positive multiplicationindex list I denoted ˆS A B t iff I Prb ax Prb ax cid2 αI cid2 0 cid7 combination values x subset X B t weakly positive multiplicationindex list I denoted ˆS I A B t iff CV t A π C V t relevant ancestors B The inﬂuence A 0 cid3 Prb ax Prb ax cid3 αI combination values x X Strongly weakly negative inﬂuences multiplicationindex list deﬁned analogously Zero biguous inﬂuences deﬁned basic qualitative probabilistic networks augmented multiplication indices We like remark multiplicationindex list introduce augment signs enhanced network inference The list solely purpose computation possible intend output signs augmented indices user 4 Enabling inference enhanced network For inference basic qualitative probabilistic network eﬃcient algorithm available We recall Section 2 algorithm builds idea propagating signs network combining operators We recall algorithm exploits properties symmetry transitivity parallel composition inﬂuences In section generalise idea signpropagation inference enhanced qual itative probabilistic network taking account strength inﬂuences Upon initiating inference signs inﬂuences associated arcs digraph enhanced network interpreted having single multipli cation index equal 1 In Section 41 address property symmetry followed discussion enhancement operators provide properties transitivity parallel composition strong weak inﬂuences Sections 42 43 respectively 41 The property symmetry In basic qualitative probabilistic network property symmetry guarantees variable A exerts inﬂuence variable B variable B exerts inﬂuence sign variable A As result signs propagated inference arc directions In enhanced qualitative network basic qualitative network inﬂuence reverse positive negative zero ambiguous The symmetry property hold regard strength inﬂuence reverse strongly positive qualitative inﬂuence example weakly positive inﬂuence vice versa There ways ensuring enhanced network inference signs propagated directions arc elicit signs inﬂuences direction arc explicitly alternatively use positive negative signs ambiguous strength signs strength unknown 0 1 Both alternatives beneﬁts drawbacks The option straightforward symmetric coun terpart positive inﬂuence example ambiguously positive inﬂuence represent 0 However signs unknown strength useful information lost opt explicitly specifying signs inﬂuences arc directions Upon explicitly specifying signs care taken signs speciﬁed arc consistent For example going details derive following deﬁnition qualitative inﬂuence property symmetry arc A B predecessors X A predecessors Y B A Pra xy lies inbetween Prb xy Prb xy qualitative inﬂuence B A necessarily stronger qualitative inﬂuence A B weaker In A B case S B A example inconsistent S S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1479 Fig 4 A fragment network With respect intercausal inﬂuences note regarded qualitative inﬂuence observations hold respect signs inﬂuences 42 The property transitivity For propagating qualitative signs active trails enhanced qualitative probabilistic network enhance operator deﬁned purpose basic qualitative networks apply strong weak inﬂuences We recall operator provides multiplying signs inﬂuences In basic qualitative probabilistic network inﬂuence essence captures difference probabilities Combining inﬂuences property transitivity amounts determining sign product differences In formalism enhanced qualitative probabilistic networks associated explicit notion strength inﬂuences It evident strengths need taken consideration multiplying signs operator To address signproduct signs enhanced qualitative probabilistic network consider network fragment shown Fig 4 The fragment includes active trail composed variables A B C qualitative inﬂuences In addition X denotes set predecessors B A Y set predecessors C B The following lemma indicates strength indirect inﬂuence A C given trail equals product strengths inﬂuences A B B C Lemma 41 Let G V G AG acyclic digraph A B C V G A B B C active trail variables A C Let Pr joint probability distribution V G respects independences G Then cid3 cid2 Prc Prc b y cid3 cid2 Prb ax Prb ax Prc axy Prc axy combination values x set variables X π B A combination values y set Y π C B Proof We observe G variable C independent variables A X given B Y addition variable B independent variable Y given A X By conditioning B ﬁnd Prc axy Prc axy Prc abxy Prb axy Prc abxy Prb axy Prc abxy Prb axy Prc abxy Prb axy cid2 cid3 Prc Prc b y cid2 cid3 Prc Prc b y Prb ax Prc b y cid3 cid2 Prb ax Prc b y Prc Prc b y cid3 cid2 Prb ax Prb ax cid2 Similar lemmas hold strengths inﬂuences possible active trail variables A C obtained reversing arcs Fig 4 introducing headtohead node trail The lemma easily extended apply situation A B B C respectively connected indirect active trails direct arcs We like note existence additional parallel active trails variables A C handled operator disregarded The differences Prc axy Prc axy combinations values xy serve indicate strength indirect inﬂuence variable A variable C We informally investigate differences property stated Lemma 41 Suppose qualitative inﬂuences A B B C strongly positive B C Let α cutoff value distinguishing strong weak inﬂuences From A B S S expression stated lemma ﬁnd Prc axy Prc axy cid2 α α α2 combination values xy set variables X Y Since α cid3 1 α2 cid3 α Upon multiplying signs strong direct inﬂuences sign results indicates indirect inﬂuence necessarily stronger weak direct inﬂuence Similar observations apply strongly negative inﬂuences Now suppose 1480 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 qualitative inﬂuences network fragment Fig 4 weakly positive S indirect inﬂuence variable A variable C ﬁnd A B S B C For 0 cid3 Prc axy Prc axy cid3 α α α2 combination values xy Similar observations apply weakly negative inﬂuences While indirect inﬂuence resulting product strong inﬂuences compared weak direct inﬂuence observation indirect inﬂuence strong indirect inﬂuence results product weak inﬂuences Finally suppose qualitative inﬂuence network fragment Fig 4 weakly positive B C We ﬁnd indirect inﬂuence A B S strongly positive example S variable A variable C 0 0 α cid3 Prc axy Prc axy cid3 α 1 α combination values xy Similar observations apply combinations weak strong inﬂuences We strength indirect inﬂuence resulting product strong weak inﬂuence comparable strength weak direct inﬂuence From previous observations conclude provide comparing indirect qualitative inﬂuences different trails respect strengths required tradeoff resolution preserve information concerning number times signs multiplied 421 Enhancing operator We employ multiplicationindex list deﬁned Section 3 retain information strengths signs multiplied To able combine information different multiplicationindex lists deﬁne sum operation lists Deﬁnition 42 Let I J multiplicationindex lists Then multiplicationindex list I J multiset cid8cid2 j sgni sgn j I j J cid9 sgn Z 1 1 deﬁned sgnz z cid2 0 1 1 cid3 cid10 As example consider polynomials αI I 1 2 α J J 1 3 multiplicationindex list I J 2 3 4 5 actually captures polynomial αI J αI α J cid2 α11 α21 α13 α23 α1 α3 αI J αI α J cid2 α1 α2 cid3 cid3 Table 2 deﬁnes enhanced operator shapes transitivity property qualitative inﬂuences enhanced network From table readily seen 0 signs essence combine basic qualitative probabilistic network difference handling multiplication indices The following proposi tion shows operator correctly captures sign transitive combination weakly positive inﬂuences Proposition 43 Let Q G Δ enhanced qualitative probabilistic network Let A B C variables G exist active trail t1 A B active trail t2 B C concatenation t1 t2 active trail A C Let I J multiplicationindex lists Then B C t2 cid13 ˆS A B t1 ˆS A C t1 t2 I J J ˆS I Proof Let Pr joint probability distribution V G respects independences G let α 0 1 cutoff value distinguishing strong weak inﬂuences We start assuming multiplication index lists I J consist single index j respectively Then weakly positive inﬂuence ˆS A B t1 variable A variable B expresses 0 cid3 Prb ax Prb ax cid3 αi I Table 2 The enhanced operator I I 0 I I J I J I 0 I I J J J I J 0 I J J 0 0 0 0 0 0 0 J J I J 0 I J J J I J I 0 I I J 0 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1481 combination values x set X positive qualitative inﬂuence ˆS J B C t2 variable B variable C expresses cid7 DV t1 A π D V t1 relevant ancestors B Similarly weakly 0 cid3 Prc Prc b y cid3 α j combination values y set Y relevant ancestors C For indirect inﬂuence variable A variable C ﬁnd Lemma 41 0 cid3 Prc axy Prc axy cid3 αi α j αi j combination values xy set X Y More general observe strength resulting inﬂuence lies 0 product polynomial expressions α captured multiplicationindex lists I J respectively We conclude ˆS A C t1 t2 cid2 I J From proposition appropriate entry Table 2 conclude weakly positive inﬂuences enhanced operator correctly captures sign transitive combination Similar observations hold transitive combination weak inﬂuences strong inﬂuences positive negative The following proposition shows operator Table 2 correctly captures sign transitive combination weakly positive strongly positive inﬂuence Proposition 44 Let Q A B C t1 t2 t1 t2 I J previous proposition Then I ˆS A B t1 ˆS J B C t2 cid13 ˆS I A C t1 t2 Proof Let Pr α previous proof We start assuming multiplicationindex lists I J consist single index j respectively Then weakly positive inﬂuence ˆS A B t1 variable A variable B expresses I 0 cid3 Prb ax Prb ax cid3 αi combination values x set X qualitative inﬂuence ˆS J B C t2 variable B variable C expresses cid7 DV t1 A π D V t1 relevant ancestors B The strongly positive α j cid3 Prc Prc b y cid3 1 combination values y set Y relevant ancestors C For indirect inﬂuence variable A variable C ﬁnd Lemma 41 0 cid3 Prc axy Prc axy cid3 αi 1 combination values xy set X Y More general observe strength resulting inﬂuence lies 0 1 times polynomial expression α captured multiplicationindex list I We conclude ˆS A C t1 t2 cid2 I From proposition appropriate entry Table 2 conclude weakly positive strongly positive inﬂuence enhanced operator correctly captures sign transitive combination Similar ob servations hold transitive combination weak inﬂuence strong inﬂuence positive negative The proofs signs transitive combinations inﬂuences stated Table 2 analogous proofs Propositions 43 44 43 The property parallel composition For combining multiple qualitative inﬂuences variables parallel active trails enhanced qualita tive probabilistic network enhance operator deﬁned purpose basic qualitative networks apply strong weak inﬂuences We recall operator provides summing signs inﬂuences We fur ther recall adding signs conﬂicting inﬂuences inference basic qualitative network represented tradeoff resolved ambiguous inﬂuence results In formalism enhanced qualitative probabilistic networks associated explicit notion strength inﬂuences These strengths taken consideration summing signs inﬂuences resolve tradeoffs For example tradeoff encountered inference negative inﬂuence known stronger conﬂicting positive conclude combined inﬂuence negative forestalling ambiguous results Upon addressing property transitivity previous section argued product inﬂuences yield indirect inﬂuence weaker inﬂuences built We sum inﬂuences contrast result stronger inﬂuence To address signsum signs enhanced qualitative 1482 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 Fig 5 Another network fragment probabilistic network consider network fragment shown Fig 5 The fragment includes active trails variables A C captures direct inﬂuence A C indirect inﬂuence B In addition set X denotes set predecessors B A Y set predecessors C A B The following lemma relates strength net inﬂuence variable A variable C strengths inﬂuences built Lemma 45 Let G V G AG acyclic digraph A B C V G A B B C A C active trails variables A C Let Pr joint probability distribution V G respects independences G Then Prc axy Prc axy cid3 cid2 Prc aby Prc ab y Prb ax Prc ab y cid3 cid2 Prc aby Prc ab y Prb ax Prc ab y combination values x set X predecessors B A combination values y set Y predecessors C A B Proof The proof property stated lemma similar Lemma 41 cid2 Similar lemmas hold strengths net inﬂuences A C combinations multiple parallel trails obtained reversing arcs Fig 5 long trails remain active A similar lemma formulated situations arcs Fig 5 replaced active trails We like note existence additional parallel trails variables A C handled repeated application composition property disregarded The differences Prc axy Prc axy combinations values xy serve indicate sum strengths direct inﬂuence indirect inﬂuence variable A variable C If arcs network fragment Fig 5 associated weakly positive inﬂuence example ﬁnd Prc axy Prc axy cid3 α α2 Building Lemma 45 prove property shortly From inequality observe parallel composition weak inﬂuences sign result net inﬂuence stronger weak direct inﬂuence Its relation strong inﬂuence unknown So basic sign resulting inﬂuence known unambiguously strength readily expressible simple power α Alternatively arcs network fragment Fig 5 associated strongly positive inﬂuence ﬁnd Prc axy Prc axy cid2 α α2 observe parallel composition strong inﬂuences sign results net inﬂuence slightly stronger strong direct inﬂuence From observations parallel composition inﬂuences result inﬂuence strength expressed single power cutoff value α losing information entire polynomial α required In remainder section present ways capturing strengths parallel inﬂuences deﬁning different operators summing signs The ﬁrst enhanced operator discussed Section 431 keeps track entire polynomials α means multiplicationindex list The second operator works signs single multiplication indices In case minimise bookkeeping necessary inference discarding higherorder terms polynomial expression α leaving single αterm power taken single multiplication index resulting sign higherorder terms discarded introducing possible error sign unknown strength yielded This second operator called simple enhanced operator s brieﬂy described Section 432 Obviously application soperator result loss available information adding signs inference 431 The enhanced operator We employ multiplicationindex list deﬁned Section 3 retain information strengths possibly conﬂicting signs summed To able compare strengths signs captured multiplication index lists deﬁne additional list operations S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1483 Table 3 The enhanced operator signs multiplicationindex lists I I 0 I I J I J J J b I J I cid3 J b I J J cid3 I c I J I cid3 J d I J J cid3 I J I I J J c 0 I I 0 I I J J I J I J d J J I J Deﬁnition 46 Let I J multiplicationindex lists let αI α J polynomials α captured lists I J respectively Then multiplicationindex list I multiset I multiplicationindex list I J multiset I J I cid3 J iff αI α J cid2 0 The deﬁnition deﬁnes negation operator negates index list applied union operator combines elements multisets single multiset comparison operator cid3 captures idea general lower order polynomials α 0 1 correspond stronger signs As example consider polynomials αI I 1 3 α J J 2 3 I 1 3 captures polynomial αI αI I J 1 2 3 3 I cid3 J αI α J α α2 cid2 0 Table 3 deﬁnes enhanced operator shapes composition property inﬂuences enhanced qualitative network From table readily seen 0 signs combine basic qualitative probabilistic network difference handling multiplication indices The following propositions different situations operator correctly captures sign combination parallel inﬂuences proofs combinations inﬂuences similar The ﬁrst proposition pertains situation weakly positive inﬂuences parallel trails combined Proposition 47 Let Q G Δ enhanced qualitative probabilistic network Let A C variables G let t1 t2 parallel active trails G A C t1 cid15 t2 trail composition Let I J multiplicationindex lists Then I ˆS A C t1 ˆS J A C t2 cid13 ˆS I J A C t1 cid15 t2 Proof Let Pr joint probability distribution V G respects independences G Let α 0 1 cut value distinguishing strong weak inﬂuences For ease exposition assume trail t1 consists single arc trail t2 consists arcs A B B C variable B network fragment Fig 5 Additional trails A C handled repeated application composition property disregarded We recall arc associated inﬂuence multiplication index 1 I 1 We recall Lemma 45 gives net inﬂuence variable A variable C trail composition t1 cid15 t2 We write equation Lemma 45 difference functions f h cid3 Prc aby Prc ab y cid11cid2 cid3 cid2 Prb ax cid12 Prb ax Prc ab y cid3 Prc aby Prc ab y cid3 cid2 Prb ax cid12 Prb ax Prc ab y Prc axy Prc axy h f cid11cid2 value combinations x y set X predecessors B A set Y predecessors C A B respectively We note functions f h linear respective parameter We assume positive inﬂuence trail t2 composed separate positive inﬂuences From inﬂuence variable B variable C positive functions f h linearly increasing larger gradient function h depicted Fig 6 fact ﬁgure gradient function f arbitrary choice From positive direct inﬂuence variable A variable C f 0 cid2 h0 f 1 cid2 h1 We functions f h intersect If inﬂuences trail t2 negative functions f h decreasing similar observations apply To determine sign composite inﬂuence variable A variable C consider sign difference functions f h We observe functions f h expressed terms 1484 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 Fig 6 Possible functions f Prb ax hPrb ax Fig 7 The functions f Prb ax hPrb ax depicted single graph gradient f gradienth b gradienth gradient f b different parameters parameters varied independently difference restricted sign qualitative inﬂuence variable A variable B Under constraint allowed compare function values f h different parameters For ease comparison depicted purpose functions f h single graph Fig 7 Since positive indirect inﬂuence trail t2 composed positive inﬂuences possible situa tions 1 S 2 S 3 S A B S A B S A B S B C B C B C In ﬁrst situations ˆS A C t2 J 2 Here consider situation proofs situations similar As direct inﬂuence variable A variable B weakly positive 0 cid3 Prb ax Prb ax cid3 α Therefore investigating difference functions f h satisfy following constraints A C t2 J 1 situation 3 ﬁnd ˆS J J parameter Prb ax function f greater equal parameter Prb ax function h difference parameters greater α We constraints difference f Prb ax hPrb ax greater equal zero To end consider graph Fig 7a similar observations hold graph Fig 7b Under given constraints minimal difference f Prb ax hPrb ax attained f 0 h0 We ﬁnd Prc axy Prc axy cid2 f 0 h0 Prc ab y Prc ab y The minimal difference positive result direct inﬂuence A C positive The sign composite inﬂuence variable A variable C positive The maximal difference f Prb ax hPrb ax attained f 1 h1 α Once exploiting information signs direct inﬂuences weakly positive difference equals Prc axy Prc axy cid3 f 1 h1 α Prc aby Prc ab y cid3 cid2 Prc aby Prc ab y 1 α Prc aby Prc aby α cid3 cid2 Prc aby Prc ab y cid3 α α α α α2 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1485 We conclude composite inﬂuence variable A variable C weakly positive multiplicationindex list 1 2 More general ﬁnd strength composite inﬂuence lies zero sum polyno mials α captured multiplicationindex lists I J respectively conclude composite inﬂuence equals ˆS A C t1 cid15 t2 cid2 I J From proposition appropriate entry Table 3 conclude weakly positive inﬂuences enhanced operator correctly captures sign composition Similar observations hold composition weakly negative signs The proposition addresses situation strongly positive inﬂuences parallel trails combined composite inﬂuence Proposition 48 Let Q A C t1 t2 t1 cid15 t2 I J previous proposition Then I ˆS A C t1 ˆS J A C t2 cid13 ˆS I J A C t1 cid15 t2 Proof The proof proceeds similar fashion proof Proposition 47 details provided Appendix A cid2 From proposition appropriate entry Table 3 conclude strongly positive inﬂuences enhanced operator correctly captures sign composition Similar observations hold composition strongly negative signs The proposition addresses combination strongly positive weakly positive inﬂuence proof Appendix A Proposition 49 Let Q A C t1 t2 t1 cid15 t2 I J previous proposition Then I ˆS A C t1 ˆS J A C t2 cid13 ˆS I A C t1 cid15 t2 From proposition appropriate entry Table 3 deduce weakly strongly posi tive inﬂuence enhanced operator correctly captures sign composition Similar observations hold composition strongly negative weakly negative inﬂuence The main reason enhancing qualitative probabilistic networks notion strength provide ﬁner level representation allows resolving tradeoffs inference Tradeoff resolution essence amounts associating unambiguous basic sign composite inﬂuence built conﬂicting inﬂuences parallel active trails The proposition provides combination conﬂicting inﬂuences describes type tradeoff typically resolved Proposition 410 Let Q A C t1 t2 t1 cid15 t2 I J previous proposition Then I cid3 J I ˆS A C t1 ˆS J A C t2 ˆS I J A C t1 cid15 t2 Proof Let Pr α We use functions f h deﬁned proof Proposition 47 Depending sign inﬂuence variable B variable C functions f h linearly increasing linearly decreasing functions We assume functions increasing implies inﬂuence variable B variable C positive We assume gradient function f larger gradient function h depicted graph Fig 7a Similar observations apply graph Fig 7b decreasing functions We distinguish cases I II proof Proposition 49 I trail t1 consists single arc trail t2 consists arcs A B B C variable B II trail t1 consists arcs A B B C trail t2 consists single arc First address case I strongly positive direct inﬂuence variable A variable C From assumptions indirect negative inﬂuence trail t2 composed negative inﬂuence A B positive inﬂuence B C More speciﬁcally following situations 1 S 2 S 3 S A B S A B S A B S B C B C B C 1486 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 The indirect inﬂuence variable A variable C trail t2 associated sign J J 2 situation 1 J 1 situations 2 3 To establish sign composite inﬂuence A C ﬁrst establish minimal difference functions f h We begin considering situations 1 3 described Since inﬂuence variable A variable B weakly negative parameters Prb ax Prb ax functions f h respectively satisfy following constraints parameter Prb ax function f difference parameters α smaller equal parameter Prb ax function h From Fig 7a observe constraints minimal difference f h attained f 0 hα The minimal difference Prc axy Prc axy cid2 f 0 hα Prc ab y Prc ab y cid3 cid2 Prc aby Prc ab y α The difference ﬁrst terms α strongly positive direct inﬂuence A C The difference terms captured inﬂuence B C weakly positive situation 1 strongly positive situation 3 In situation 1 Prc axy Prc axy cid2 α α α situation 3 ﬁnd Prc axy Prc axy cid2 α 1 α 0 We consider situation 2 described The strongly negative inﬂuence variable A variable B imposes following constraints parameters f h parameter Prb ax function f difference parameters α smaller parameter Prb ax function h From Fig 7a observe constraints minimal difference f h attained f 0 h1 Prc axy Prc axy cid2 f 0 h1 Prc ab y Prc ab y cid3 cid2 Prc aby Prc ab y We Prc axy Prc axy cid2 α α 0 For situations 1 2 3 maximum difference functions f h attained f 1 α h1 The maximum difference Prc axy Prc axy cid3 f 1 α h1 Prc aby Prc aby cid2 cid3 Prc aby Prc ab y α We ﬁnd maximum difference 1 situations 1 2 1 α situation 3 We conclude case I composite inﬂuence variable A variable C positive α I α J ˆS A C t1 cid15 t2 We address case II Since indirect inﬂuence trail t1 strongly positive composed strong direct inﬂuences Recall assume inﬂuence variable B variable C positive strong inﬂuences positive I J S A B S B C A C As proposition addresses situations I cid3 J assume resulting indirect inﬂuence S weakly negative direct inﬂuence variable A variable C multiplication index 2 The observations result following constraints 2 function f parameter Prb ax function f lies function h f 0 cid3 h0 f 1 cid3 h1 greater parameter Prb ax function h difference α functions f h linearly increasing functions We assume gradient f larger h similar observations holding opposite case To establish sign composite inﬂuence variable A variable B investigate minimal maximal differences functions f h Under constraints ﬁnd minimal difference f h attained f 1 h0 equals Prc axy Prc axy cid2 f 1 h0 Prc aby Prc ab y Prc ab y Prc ab y From strongly positive inﬂuence variable B variable C Prc aby Prc ab y cid2 α weakly negative inﬂuence variable A variable C 0 cid2 Prc ab y Prc ab y cid2 α2 The minimal difference equals α α2 Similarly maximal difference functions f h attained f α h0 equals α2 We conclude case II composite inﬂuence variable A variable C positive α I α J ˆS A C t1 cid15 t2 I J S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1487 To summarise possible situations multiplicationindex list I strong sign equal multiplicationindex list J weak sign composite inﬂuence variable A variable C positive equals ˆS A C t1 cid15 t2 Note I J guarantee composite inﬂuence positive cid2 I J From proposition appropriate entry Table 3 observe strongly positive inﬂuence multiplicationindex list I weakly negative inﬂuence multiplicationindex list J I cid3 J enhanced operator correctly captures sign composition Similar observations apply combinations strong weak conﬂicting inﬂuences We conclude certain conditions composition conﬂicting strong weak inﬂuences enhanced operator leads unambiguous result level basic sign composite inﬂuence The enhanced operator serves resolve tradeoffs inference From Table 3 observe multiplicationindex lists tend grow size combining signs These lists simpliﬁed large extent For example list I 1 2 1 3 captures polynomial α α2 α α3 α2 α3 simpliﬁed I 2 3 That complementing indices removed long result multiplicationindex list For example list I 1 1 represents constant 0 α0 1 A list form I n n represented given form actual value n irrelevant nonzero integer value changing lists meaning In case strong sign augmented multiplicationindex list form n n equivalent representation given weak sign single multiplication index 0 represent inﬂuence strength zero So simplify multiplicationindex list form n n example replace sign nn sign 0 changing meaning We ﬁnally note multiplicationindex list true multiset duplicates removed example I 1 1 represents polynomial α α equals 2 α simply α Although simplifying multiplicationindex lists inference save large bookkeeping spared bookkeeping approximating polynomial expressions α single term The section brieﬂy describes safely manage loss information incurred approximation 432 The simple enhanced soperator In section introduce simpliﬁed version enhanced operator assumes multiplication index lists consist single positive index This single index result essentially discarding higherorder terms polynomial expression α captures strength sign If higherorder terms discarded introducing possible error sign unknown strength denoted 0 0 yielded equivalent positive negative sign respectively basic qualitative probabilistic network The simple enhanced soperator deﬁned Table 4 multiplicationindex lists signs reduced single indices When comparing table Table 3 enhanced operator previous section note following differences 1 combining strong signs having basic sign multiplication index resulting sign minimum multiplication indices combined signs instead concatenation 2 combining weak signs having basic sign longer preserve information conclude resulting sign strong weak sign unknown strength results 3 combining strong weak sign conﬂicting basic signs unambiguously conclude basic sign multiplication index strong sign smaller weak sign know strength The fact simple enhanced soperator correctly captures sign combination parallel inﬂuences follows directly proofs propositions previous section The proof Proposition 48 example demonstrates strength sign results combining nonconﬂicting strong signs multiplica tion indices j respectively αi α j cid2 αmini j Similarly proof Proposition 47 Table 4 The simple enhanced s operator signs single multiplication indices s j j 0 0 j c 0 m j j b 0 m mini j 0 cid3 j b 0 j cid3 c 0 cid3 j d 0 j cid3 j j 0 j d j j m 1488 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 strength sign results combining nonconﬂicting weak signs multiplication indices j respectively αi α j considered unknown Finally proof Proposition 410 shows strength sign results combining strong sign multiplication index conﬂicting weak sign multiplication index j αi α j considered unknown We stress contrary purely ambiguous signs signs unknown strength valuable necessarily spread network occur inference We conclude application simple enhanced s operator enhanced operator results computational overhead qualitative inference Due loss information level strengths signs application simple soperator result tradeoffs resolved 5 Probabilistic inference revisited In Section 3 introduced formalism enhanced qualitative probabilistic networks In Section 4 enhanced standard operators combining signs inﬂuences inference addressed propagation signs direction arcs Building new enhanced operators basic signpropagation algorithm probabilistic inference qualitative network generalised straightforwardly apply enhanced networks instead standard operators enhanced operators propagating combining signs In section illustrate application resulting algorithm versions enhanced operator means running example qualitative networks associated example reproduced Fig 8 In addition discuss algebraic properties enhanced operators affect inference results Finally brieﬂy discuss complexity issues concerning different versions signpropagation algorithm 51 Inference enhanced operators The idea signpropagation algorithm basically establish net inﬂuence observed variable variables qualitative probabilistic network multiply sign net inﬂuence sign observation return effect observation variables For ease implementation algorithm starts sending sign observation observed variable incorporating effect observation messages subsequently sent Due algebraic properties basic operators actual implementation affect results In section demonstrate enhanced qualitative network enhanced operators adhere algebraic properties ensure order signs combined affect result combination As consequence multiplying sign net inﬂuence sign observation lead different result obtained directly incorporating sign observation messages sent observed variable To disturb computation signs net inﬂuences little possible propose entering observation identity sign respect strength More speciﬁcally require sign s arbitrary sign t result s t strength t We note Table 2 signs 0 0 suitable purpose taken represent constants 1 1 respectively We present example illustrates signpropagation enhanced operators Example 51 We consider qualitative Antibiotics network reproduced Fig 8a Recall entering sign variable A results inference basic signpropagation algorithm ambiguous sign variable D turn causes ambiguous sign variable H Now consider enhanced Antibiotics network reproduced Fig 8b signs speciﬁed taken hold direction corresponding arcs We recall initially inﬂuences associated arcs networks digraph signs multiplicationindex 1 We apply signpropagation algorithm enhanced operators We enter sign 0 variable A reﬂecting positive observation A Variable A propagates message sign 0 1 1 variable T Variable T updates nodesign 1 sends message sign 1 1 2 variable D Variable D b Fig 8 The qualitative Antibiotics network b enhanced version S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1489 updates nodesign 2 sends message sign 2 1 3 variable H Variable H updates sign accordingly sends messages Variable A sends message sign 0 1 1 variable F Variable F updates sign passes message sign 1 1 2 variable D Variable D receives additional sign 2 Variable D combine signs received parallel trails originating A The result combination depends enhanced operator More speciﬁcally signpropagation algorithm employs fully enhanced operator variable D updates sign 2 2 22 computes variable H message sign 22 1 33 On hand simple enhanced soperator applied variable D updates sign 2 s 2 0 computes message sign 0 1 0 variable H Variable H need sign update current sign correct regardless operator 3 33 3 3 s 0 3 The variables D H send messages algorithm halts From example ﬁrst glance results signpropagation fully enhanced operator similar results simple enhanced soperator nodesign node D differing This illusion caused speciﬁc example network With operator nodesign variable D form ii fact trails conﬂicting inﬂuences length recall nodesign captures information negative sign 0 unknown strength returned soperator results inference essence differ operators If conﬂicting trails different lengths difference operators important situ ations algorithm simple enhanced soperator leads nodesign 0 algorithm fully enhanced operator result nodesign I J I cid10 J sign captures information ambiguous negative sign aid resolving tradeoffs We stress contrary purely ambiguous signs signs unknown strength necessarily spread network occur In addition signs convey useful information basic sign inﬂuence variables We conclude basic framework qualitative networks tradeoffs resolved inference result ambiguous net inﬂuence enhanced qualitative probabilistic networks allow resolving tradeoffs 52 Algebraic properties As result algebraic properties basic operators qualitative inference basic qualitative probabilistic network nodesigns computed variables network depend order variables receive messages neighbours Unfortunately observation longer holds enhanced qualitative network nodesigns computed fact different depending order messages received Given enhanced operators correctly capture transitivity parallel composition properties qualitative inﬂuences computed nodesigns correct informative The following proposition states algebraic properties enhanced operators adhere proofs given Appendix A Proposition 52 Consider enhanced operators deﬁned Tables 2 3 4 combining signs enhanced qualitative network Then enhanced operator commutative enhanced operator associative enhanced soperators commutative The enhanced soperators combining signs parallel inﬂuences longer associative result loss information combining trails having strong weak conﬂicting inﬂuences This illustrated following example cid2 cid3 ii cid2 cid3 cid2 0 s cid3 We stress combinations example lead correct results regardless operator s ﬁrst informative second Heuristics example separately adding positive negative signs combining designed prevent unnecessary ambiguous results order combination Such heuristics increase complexity inference Finally observe enhanced operator distributes operator soperator Compare example following 1490 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 cid2 cid2 cid3 cid3 cid3 cid2 2i i2i cid2 or0 s cid3 Again stress results correct differ level informativeness respect strength resulting sign 53 Complexity probabilistic inference For quantitative probabilistic networks general exact computation probabilities NPhard 7 The algorithms probabilistic inference probabilistic network known behave polynomially certain restrictions concerning topology networks digraph In general sparser digraph better algorithms perform The basic signpropagation algorithm inference basic qualitative network worstcase runtime complexity polynomial number nodes networks digraph regardless digraphs topology In singly nected digraph pair nodes connected single simple trail Upon signpropagation variable A visited receive single sign signproduct sign observation signs asso ciated arcs trail A observed variable In multiply connected graph nodes connected simple trail As result variable visited times number active simple trails variable observed variable receive sign inﬂuence trails To limit possibly exponential number visits variable basic propagation algorithm exploits fact node signs change twice 0 As variables need visited twice visited variable inspects constructs message variables basic propagation algorithm halts number operations polynomial number nodes networks digraph The basic formalism qualitative probabilistic networks allow resolving tradeoffs combining ﬂicting inﬂuences basic operator immediately results ambiguous nodesign From example previous section enhanced soperators provide resolving tradeoffs ad ditional information carried enhanced augmented signs The possibility resolving tradeoffs comes expense eﬃciency signpropagation This surprising qualitative tradeoff resolution known NPhard 23 The main difference signpropagation basic qualitative probabilistic network sign propagation enhanced network multiply connected digraphs limit visits variable longer applies Although variables basic enhanced nodesign change timesfrom zero strong weak ambiguousthe multiplicationindex lists associated sign require updating time variable inspected The difference fully enhanced operator simple enhanced soperator case multiplicationindex list sign restricted single index require update Using simple enhanced soperator turn eﬃcient practice Further research required determine exact complexity class inference enhanced qualitative network belongs determine enhanced soperators differ complexity Recent results indicate networks enhanced qualitative signs translated intervals intervalpropagation NPhard 21 We conclude exists tradeoff information present inference results sign propagation complexity propagation algorithm Inference basic signpropagation algorithm runtime complexity polynomial number nodes qualitative networks digraph leads ambiguous results network models tradeoff Inference enhanced operators exponential enable resolving tradeoffs resorting numerical information 6 Related work The problem tradeoff resolution framework qualitative networks addressed different researchers In section brieﬂy review related work S Parsons introduced concept categorical inﬂuence 25 A categorical inﬂuence qualitative inﬂuence serves increase probability 1 decrease probability 0 disregarding inﬂuences For exam A B variable A variable B deﬁned Prb ax 1 relevant ple positive categorical inﬂuence S variables X A categorical inﬂuence serves resolve tradeoff involved capture terministic relationships nodes reallife applications relationships exist Parsons studied use relative absolute orderofmagnitude reasoning context qualitative probabilistic net works 25 Using relative orderofmagnitude romk 8 Parsons relates different qualitative inﬂuences specifying qualitative inﬂuence respectively negligible respect distant comparable close inﬂuence The use relative orders magnitude serves relate strengths different inﬂuences requires speciﬁcation relation pairs inﬂuences instead notion strength inﬂuence In addition vague interpretations terms relations illdeﬁned makes reasoning intuitive For absolute orderofmagnitude reasoning Parsons proposes method revolves propagation abstract intervals 1 1 correspond labels like Strongly Positive S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1491 Weakly Positive Two different sets labels required modelling inﬂuences associated arcs networks digraph modelling changes occur nodes graph comparable node signs The intervals corresponding set labels overlap span interval 1 1 The boundaries intervals actually quantiﬁed set α β approach comparable treatment cutoff value Probabilistic inference based propagating combining abstract intervals interval comparisons required end cid2int α β cid2int γ δ iff α cid2 γ β cid2 δ Note interval considered larger operator fact overlap To prevent considerable loss information assumptions actual values interval boundaries κ calculus 30 considered absolute orderofmagnitude proposed qualitative ver sion probabilistic reasoning M Goldszmidt J Pearl 16 Using probabilistic interpretation κ calculus probabilities abstracted κ values κ value n indicates associated probability order magnitude cid10n inﬁnitesimal number cid10 This interpretation subsequently applied context probabilistic networks replacing conditional probabilities κ values computing posterior κ values κ calculus 9 More recently interpretation approach enhance expressiveness qualitative probabilistic networks 29 With approach interval κ values associated sign inﬂuence capture possible strengths These κ intervals propagated qualitative networks signs As result way κ values deﬁned propagation results guaranteed correct inﬁnitesimal probabilities Another drawback use κ values deﬁnition intuitive values hard domain experts specify interpret Categorical inﬂuences orderofmagnitude reasoning κ calculus purely qualitative nature serve resolving tradeoffs CL Liu MP Wellman designed methods resolving tradeoffs based idea reverting numerical probabilities necessary 23 They propose reason probabilistic network qualitative way exploiting eﬃciency signpropagation reverting quantiﬁcation tradeoff leads ambiguous result Two methods described resolving tradeoff The ﬁrst method provides incrementally applying numeric inference point qualitative reasoning produce decisive result That tradeoff variables resolved numerically abstracted net qualitative inﬂuence variables The second method amounts estimating bounds net inﬂuence trails rise tradeoff These bounds compute qualitative sign net inﬂuence The methods presented Liu Wellman resolve tradeoff present network require availability fully speciﬁed nu merical probabilistic network As methods interesting use construction phase probabilistic network We like mention approaches dealing uncertainty qualitative way proposed literature As approaches tailored speciﬁcally qualitative probabilistic reasoning use framework qualitative probabilistic networks review 7 Conclusions research Qualitative probabilistic networks overcome extent quantiﬁcation problem known probabilistic networks Qualitative networks essence qualitative abstractions quantitative counterparts probabilistic network relationships variables quantiﬁed probabilities relationships expressed qualitative signs qualitative probabilistic networks As result coarse level representation qualitative networks lack expressive power allows resolving tradeoffs way probabilistic networks Since qualitative probabilistic networks recognised useful tools different stages construc tion veriﬁcation quantitative probabilistic networks reallife application domains feel important qualitative formalism expressive possible order derive information possible qualita tive network The formalism enhanced qualitative networks provides step making qualitative networks applicable providing tradeoff resolution qualitative way To end distinguished strong weak inﬂuences We enhanced multiplication addition operators guarantee transitivity parallelcomposition properties inﬂuences Unfortunately additional expressiveness enhancement comes expense property symmetry inﬂuences strength inﬂuence concerned To handle asymmetry inﬂuences strength proposed specifying inﬂuences arc With enhancements generalised basic signpropagation algorithm apply enhanced qualitative networks We shown formalism provides resolving tradeoffs qualitative way having fall numerical information To distinguish weak strong inﬂuences introduced additional signs augmented signs multiplicationindex lists Since diﬃcult domain experts interpret meaning lists indices intention output augmented signs The multiplication indices merely internally tradeoff resolution output inference basic qualitative network basic sign variable indicates net inﬂuence observation variable positive negative zero ambiguous If desirable additional level strength added introducing example signs additional cutoff value redeﬁning operators This render operators far complex Alternatively signs 1492 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 multiplication index 1 allowed arcs enhanced networks digraph option implemented directly current enhanced framework Both options require domain experts able distinguish levels strength When signpropagation algorithm enhanced operators eﬃcient basic sign propagation algorithm In fact inference theory infeasible Further research necessary determine actual complexity signpropagation enhanced operators reallife qualitative networks Two approaches bound complexity inference The ﬁrst approach amounts posing limit number signadditions performed single variable If limit reached nodesign variable changed basic sign 0 basic signpropagation algorithm propagation Note approach lead weaker correct results The approach use enhanced signs small parts network parts tradeoffs reside In constructing enhanced network focus multiply connected parts networks digraph ask domain experts possible parallel trails variables consist conﬂicting inﬂuences If enhanced signs elicited inﬂuences trails During inference tradeoff locally resolved enhanced signpropagation algorithm basic sign net inﬂuence propagation basic signpropagation algorithm Another advantage local computation enhanced signs requires local speciﬁcation signs As consequence elicitation signs domain experts compare differences strengths small sets inﬂuences Since correctly specifying strengths harder experts correctly specifying basic sign inﬂuence local speciﬁcation enhanced signs resulting signs prone error Local speciﬁcation allows different interpretations strong weak inﬂuences different parts network allows different cutoff values implicitly different parts network This addition simplify elicitation signs domain experts We conclude including notion strength logical extension original formalism qualitative probabilis tic networks We formalised notion strength shown cope qualitative probabilistic inference mathematically correct way As enhanced expressiveness qualitative probabilistic net works albeit expense convenient properties symmetry inﬂuences algebraic properties operators combining signs complexity inference Although research issues required enhancement broadened range possible applications qualitative probabilistic networks Appendix A Additional proofs propositions Proof Proposition 48 I J I J The proof proceeds similar fashion proof Proposition 47 We assume positive inﬂuence trail t2 composed separate positive inﬂuences similar observations applying inﬂuences negative Since indirect inﬂuence variable A variable C trail t2 strongly positive composed strongly positive direct inﬂuences We S A B S B C J A C t2 J 2 We investigate difference functions f h S deﬁned proof Proposition 47 Since inﬂuence variable A variable B strongly positive difference parameters f h α To establish minimum difference f Prb ax hPrb ax consider graph Fig 7a similar observations hold graph Fig 7b Under constraint mentioned readily seen minimal difference f Prb ax hPrb ax attained f α h0 We ﬁnd Prc axy Prc axy cid2 f α h0 cid2 cid3 Prc aby Prc ab y α Prc ab y Prc ab y cid2 α2 α We conclude composite inﬂuence variable A variable C strongly positive multiplicationindex list 1 2 More general ﬁnd strength composite inﬂuence sum polynomials α captured multiplicationindex lists I J respectively conclude composite inﬂuence equals ˆS A C t1 cid15 t2 cid2 I J Proof Proposition 49 I J I We distinguish different cases I trail t1 consists single arc trail t2 consists arcs A B B C variable B II trail t1 consists arcs A B B C trail t2 consists single arc S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 1493 For cases proof proceeds similar fashion proof Proposition 47 First address case I As assume indirect weakly positive inﬂuence variable A variable C trail t2 composed separate weakly positive inﬂuences proofs possible situations analogous To establish minimal difference functions f h deﬁned proof Proposition 47 consider graph Fig 7a Since inﬂuence variable A variable B weakly positive difference parameters f h α Under constraint minimal difference f Prb ax hPrb ax attained f 0 h0 We ﬁnd Prc axy Prc axy cid2 f 0 h0 Prc ab y Prc ab y Since direct inﬂuence variable A variable C strongly positive Prc axy Prc axy cid2 α We conclude composite inﬂuence variable A variable C strongly positive multiplicationindex list I 1 conclude composite inﬂuence equals ˆS A C t1 cid15 t2 case I I We consider minimal difference functions f h case II We assume indirect positive inﬂuence A C trail t1 composed separate positive inﬂuences similar observations applying inﬂuences negative Since indirect inﬂuence A C strongly positive Table 2 separate inﬂuences A B B C strongly positive We S A B S B C I ˆS A C t1 I 2 Since inﬂuence variable B variable C positive functions f h linearly increasing Since inﬂuence A B strongly positive parameter Prb ax function f greater parameter Prb ax function h difference α To establish minimum difference f Prb ax hPrb ax consider graph Fig 7a similar observations applying graph Fig 7b Under constraints mentioned observe minimal difference f Prb ax hPrb ax attained f α h0 We ﬁnd cid3 cid2 Prc aby Prc ab y Since direct inﬂuence A C weakly positive 0 cid3 Prc ab y Prc ab y cid3 α We conclude composite inﬂuence variable A variable C strongly positive multiplicationindex list I 2 conclude composite inﬂuence equals ˆS Prc axy Prc axy cid2 f α h0 α Prc ab y Prc ab y A C t1 cid15 t2 case II cid2 I Proof Proposition 52 algebraic properties The fact enhanced soperators commutative follows directly symmetry respective tables To prove enhanced operator associative distinguish number cases We ﬁrst observe property trivially holds signs enhanced operator 0 Now consider combining operator signs positive negative weak strong Since combining strong signs respective multiplicationindex lists results strong sign augmented sum multiplicationindex lists order combination signs affect resulting sign property associativity holds The argument applies combining weak signs Now consider case enhanced operator combine single weak sign δ I strong signs δδ J δδ K If signs positive cid3 cid2 δ I δδ J cid2 δδ J δδ K δ I δδ K δ I δδ K δ I cid3 δ I δδ J K δ I Similar results hold regardless signs involved positive negative Finally consider case enhanced operator combine weak signs δ I δ J single strong sign δδ K If signs positive cid3 cid2 δ I δ J cid2 δ J δδ K δ I δδ K δ I J δδ K δ I J δ I δ J δ I J cid3 similar results holding regardless signs involved positive negative We conclude based observations commutativity operator operator associa tive cid2 References 1 B Abramson ARCO1 An application belief networks oil market BD DAmbrosio P Smets PP Bonissone Eds Proceedings Seventh Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Mateo California 1991 pp 18 2 B Abramson J Brown A Murphy RL Winkler Hailﬁnder A Bayesian forecasting severe weather International Journal Forecasting 12 1996 5771 1494 S Renooij LC van der Gaag Artiﬁcial Intelligence 172 2008 14701494 3 EE Altendorf AC Restiﬁcar TG Dietterich Learning sparse data exploiting monotonicity constraints F Bacchus T Jaakkola Eds Pro ceedings TwentyFirst Conference Uncertainty Artiﬁcial Intelligence AUAI Press Corvallis Oregon 2005 pp 1825 4 S Andreassen M Woldbye B Falck SK Andersen MUNIN A causal probabilistic network interpretation electromyographic ﬁndings J Mc Dermott Ed Proceedings Tenth International Conference Artiﬁcial Intelligence Morgan Kaufmann Publishers Los Altos California 1987 pp 366372 5 IA Beinlich HJ Suermondt RM Chavez GF Cooper The alarm monitoring A case study probabilistic inference techniques belief networks J Hunter J Cookson J Wyatt Eds Proceedings Second Conference Artiﬁcial Intelligence Medicine SpringerVerlag Berlin 1989 pp 247256 6 CP Campos FG Cozman Belief updating learning semiqualitative probabilistic networks F Bacchus T Jaakkola Eds Proceedings TwentyFirst Conference Uncertainty Artiﬁcial Intelligence AUAI Press Corvallis Oregon 2005 pp 1825 7 GF Cooper The computational complexity probabilistic inference Bayesian belief networks Artiﬁcial Intelligence 42 1990 393405 8 P Dague Symbolic reasoning relative orders magnitude R Bajcsy Ed Proceedings Thirteenth International Joint Conference Artiﬁcial Intelligence Morgan Kaufmann Publishers San Mateo California 1993 pp 15091514 9 A Darwiche M Goldszmidt On relation kappa calculus probabilistic reasoning Proceedings Tenth Conference Uncer tainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Francisco California 1994 pp 145153 10 MJ Druzdzel Probabilistic Reasoning decision support systems From computation common sense PhD Thesis Department Engineering Public Policy Carnegie Mellon University Pittsburgh Pennsylvania 1993 11 MJ Druzdzel M Henrion Eﬃcient reasoning qualitative probabilistic networks R Fikes W Lehnert Eds Proceedings Eleventh National Conference Artiﬁcial Intelligence AAAI Press Menlo Park California 1993 pp 548553 12 MJ Druzdzel M Henrion Intercausal reasoning uninstantiated ancestor nodes D Heckerman A Mamdani Eds Proceedings Ninth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Francisco California 1993 pp 317325 13 MJ Druzdzel LC van der Gaag Elicitation probabilities belief networks Combining qualitative quantitative information Ph Besnard S Hanks Eds Proceedings Eleventh Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Francisco California 1995 pp 141148 14 MJ Druzdzel LC van der Gaag Building probabilistic networks Where numbers come fromGuest editors introduction IEEE Transactions Knowledge Data Engineering 12 2000 481486 15 A Feelders LC van der Gaag Learning Bayesian network parameters order constraints International Journal Approximate Reasoning 42 2006 3753 16 M Goldszmidt J Pearl Reasoning qualitative probabilities tractable D Dubois MP Wellman B DAmbrosio P Smets Eds Proceed ings Eighth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Mateo California 1992 pp 112120 17 EM Helsper LC van der Gaag AJ Feelders WLA Loeffen PL Geenen ARW Elbers Bringing order Bayesiannetwork construction Pro ceedings Third International Conference Knowledge Capture ACM Press New York 2005 pp 121128 18 AL Jensen Quantiﬁcation experience DSS mildew management winter wheat MJ Druzdzel LC van M Henrion FV Jensender Gaag Eds Working Notes IJCAI Workshop Building Probabilistic Networks Where Do Numbers Come From AAAI Press 1995 pp 2331 19 D Kahneman P Slovic A Tversky Judgment Uncertainty Heuristics Biases Cambridge University Press Cambridge 1982 20 M Korver PJF Lucas Converting rulebased expert belief network Medical Informatics 18 1993 219241 21 J Kwisthout G Tel Complexity results enhanced qualitative probabilistic networks M Studeny J Vomlel Eds Proceedings Third Workshop Probabilistic Graphical Models Prague 2006 pp 171178 22 SL Lauritzen DJ Spiegelhalter Local computations probabilities graphical structures application expert systems Journal Royal Statistical Society Series B 50 1988 157224 23 CL Liu MP Wellman Incremental tradeoff resolution qualitative probabilistic networks GF Cooper S Moral Eds Proceedings Four teenth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Francisco California 1998 pp 338345 24 PJF Lucas Bayesian network modelling qualitative patterns Artiﬁcial Intelligence 163 2005 233263 25 S Parsons Reﬁning reasoning qualitative probabilistic networks Ph Besnard S Hanks Eds Proceedings Eleventh Conference Uncer tainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Francisco California 1995 pp 427434 26 J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann Publishers Palo Alto California 1998 27 S Renooij Qualitative approaches quantifying probabilistic networks PhD Thesis Institute Information Computing Sciences Utrecht Uni versity The Netherlands 2001 28 S Renooij LC van der Gaag From qualitative quantitative probabilistic networks A Darwiche N Friedman Eds Proceedings Eighteenth Conference Uncertainty Artiﬁcial Intelligence Morgan Kaufmann Publishers San Francisco California 2002 pp 422429 29 S Renooij S Parsons P Pardieck Using kappas indicators strength qualitative probabilistic networks TD Nielsen NL Zhang Eds Proceedings Seventh European Conference Symbolic Quantitative Approaches Reasoning Uncertainty Lecture Notes Artiﬁcial Intelligence 2003 pp 8799 30 W Spohn A general nonprobabilistic theory inductive reasoning RD Shachter TS Levitt LN Kanal JF Lemmer Eds Uncertainty Artiﬁcial Intelligence 4 Elsevier Amsterdam 1990 pp 149158 31 LC van der Gaag S Renooij CLM Witteman BMP Aleman BG Taal Probabilities probabilistic network A casestudy oesophageal carcinoma Artiﬁcial Intelligence Medicine 25 2002 123148 32 LC van der Gaag HL Bodlaender A Feelders Monotonicity Bayesian networks M Chickering J Halpern Eds Proceedings Twentieth Conference Uncertainty Artiﬁcial Intelligence AUAI Press Arlington Virginia 2004 pp 569576 33 LC van der Gaag EM Helsper Deﬁning classes inﬂuences acquisition probability constraints Bayesian networks R López Mántaras L Saitta Eds Proceedings Sixteenth European Conference Artiﬁcial Intelligence IOS Press Amsterdam 2004 pp 11011102 34 MP Wellman Fundamental concepts qualitative probabilistic networks Artiﬁcial Intelligence 44 1990 257303