Artiﬁcial Intelligence 112 1999 181211 Between MDPs semiMDPs A framework temporal abstraction reinforcement learning Richard S Sutton acid3 Doina Precup b Satinder Singh ATT LabsResearch 180 Park Avenue Florham Park NJ 07932 USA b Computer Science Department University Massachusetts Amherst MA 01003 USA Received 1 December 1998 Abstract Learning planning representing knowledge multiple levels temporal abstraction key longstanding challenges AI In paper consider challenges addressed mathematical framework reinforcement learning Markov decision processes MDPs We extend usual notion action framework include optionsclosedloop policies taking action period time Examples options include picking object going lunch traveling distant city primitive actions muscle twitches joint torques Overall options enable temporally abstract knowledge action included reinforcement learning framework natural general way In particular options interchangeably primitive actions planning methods dynamic programming learning methods Qlearning Formally set options deﬁned MDP constitutes semiMarkov decision process SMDP theory SMDPs provides foundation theory options However interesting issues concern interplay underlying MDP SMDP SMDP theory We present results cases 1 results planning options execution interrupt options perform better planned 2 introduce new intraoption methods able learn option fragments execution 3 propose notion subgoal improve options All results precursors existing literature contribution paper establish simpler general setting fewer changes existing reinforcement learning framework In particular results obtained committing ruling particular approach state abstraction hierarchy function approximation macro utility problem cid211 1999 Published Elsevier Science BV All rights reserved cid3 Corresponding author 0004370299 matter cid211 PII S 0 0 0 4 3 7 0 2 9 9 0 0 0 5 2 1 1999 Published Elsevier Science BV All rights reserved 182 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Keywords Temporal abstraction Reinforcement learning Markov decision processes Options Macros Macroactions Subgoals Intraoption learning Hierarchical planning SemiMarkov decision processes 0 Introduction Human decision making routinely involves choice temporally extended courses action broad range time scales Consider traveler deciding undertake journey distant city To decide beneﬁts trip weighed expense Having decided choices leg ﬂy drive taxi arrange ride Each steps involves foresight decision way smallest actions For example taxi involve ﬁnding telephone dialing digit individual muscle contractions lift receiver ear How understand automate ability work ﬂexibly multiple overlapping time scales Temporal abstraction explored AI early 1970s primarily context STRIPSstyle planning 18202129343746495160 76 Temporal abstraction focus appealing aspect qualitative modeling approaches AI 615333662 explored robotics control engineering 179253961 In paper consider temporal abstraction framework reinforcement learning Markov decision processes MDPs This framework popular AI ability deal naturally stochastic environments integration learning planning 34132264 Reinforcement learning methods proven effective number signiﬁcant applications 1042507077 MDPs conventionally conceived involve temporal abstraction tem porally extended action They based discrete time step unitary action taken time t affects state reward time t C 1 There notion course action persisting variable period time As consequence conventional MDP methods unable advantage simplicities efﬁciencies avail able higher levels temporal abstraction On hand temporal abstraction introduced reinforcement learning variety ways 2811121416192628 3132384044455356575963686971737882 In present paper generalize simplify previous cotemporaneous works form compact uni ﬁed framework temporal abstraction reinforcement learning MDPs We answer question What minimal extension reinforcement learning framework allows general treatment temporally abstract knowledge action In second paper use new framework develop new results generalizations previous results One keys treating temporal abstraction minimal extension reinforcement learning framework build theory semiMarkov decision processes SMDPs pioneered Bradtke Duff 5 Mahadevan et al 41 Parr 52 SMDPs special kind MDP appropriate modeling continuoustime discreteevent systems The actions SMDPs variable amounts time intended model temporallyextended courses action The existing theory SMDPs RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 183 speciﬁes model results actions plan However existing SMDP work limited temporally extended actions treated indivisible unknown units There attempt SMDP theory look inside temporally extended actions examine modify structure terms lowerlevel actions As tried suggest essence analyzing temporally abstract actions AI applications goal directed behavior involves multiple overlapping scales decisions modiﬁed In paper explore interplay MDPs SMDPs The base problem consider conventional discretetime MDP 1 consider courses action MDP results state transitions extended variable duration We use term options 2 courses action include primitive actions special case Any ﬁxed set options deﬁnes discretetime SMDP embedded original MDP suggested Fig 1 The panel shows state trajectory discrete time MDP middle panel shows larger state changes continuous time SMDP panel shows levels analysis superimposed use options In case underlying base MDP regular singlestep transitions options deﬁne potentially larger transitions like SMDP number discrete steps All usual SMDP theory applies superimposed SMDP deﬁned options addition explicit interpretation terms underlying MDP The SMDP actions options longer black boxes policies base MDP examined changed learned planned right The ﬁrst paper Sections 13 develops ideas formally fully The ﬁrst sections review reinforcement learning framework present generalization temporally extended action Section 3 focuses link SMDP theory illustrates speedups planning learning possible use temporal abstraction The rest paper concerns ways going SMDP analysis options change learn internal structure terms MDP Section 4 considers problem effectively combining given set options single overall policy For example robot predesigned controllers servoing joints positions picking objects visual search face difﬁcult problem coordinate switch behaviors 1732353940436179 Sections 5 6 concern intraoption learninglooking inside options learn simultaneously options consistent fragment experience Finally Section 7 illustrate notion subgoal improve existing options learn new ones 1 In fact base SMDP technical changes framework larger step away standard framework 2 This term deserve explanation In previous work terms including macro actions behaviors abstract actions subcontrollers structures closely related options We introduce new term avoid confusion previous formulations informal terms The term options meant generalization actions use formally primitive choices It ﬁrst inappropriate option connote course action nonprimitive exactly intention We wish treat primitive temporally extended actions similarly prefer 184 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Fig 1 The state trajectory MDP small discretetime transitions SMDP comprises larger continuoustime transitions Options enable MDP trajectory analyzed way 1 The reinforcement learning MDP framework In section brieﬂy review standard reinforcement learning framework discretetime ﬁnite Markov decision processes MDPs forms basis extension temporally extended courses action In framework learning agent interacts environment discrete lowestlevel time scale t D 0 1 2 On time step t agent perceives state environment st 2 S basis chooses primitive action 2 Ast In response action environment produces step later numerical reward rt C1 state st C1 It convenient suppress differences available actions states possible let A D S s2S As denote union action sets If S A ﬁnite environments transition dynamics modeled onestep statetransition probabilities pa ss0 D Pr cid8 st C1 D s0 cid12 cid12 st D s D cid9 onestep expected rewards r s D E cid8 rt C1 cid12 cid12 st D s D cid9 s s0 2 S 2 As These sets quantities constitute onestep model environment The agents objective learn Markov policy mapping states probabilities taking available primitive action cid25 S cid2 A T0 1U maximizes expected discounted future reward state s V cid25 s D E D E cid12 cid8 cid12 st D s cid25 rt C1 C cid13 rt C2 C cid13 2rt C3 C cid1 cid1 cid1 cid12 cid9 cid8 cid12 st D s cid25 rt C1 C cid13 V cid25 st C1 cid20 X r s ss0V cid25 s pa cid25s C cid13 cid21 0 X cid9 D a2As s0 1 2 cid25s probability policy cid25 chooses action 2 As state s cid13 2 T0 1U discountrate parameter This quantity V cid25 s called value state RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 185 s policy cid25 V cid25 called statevalue function cid25 The optimal statevalue function gives value state optimal policy V cid25 s cid8 V cid3s D max cid25 D max a2As D max a2As E cid20 rt C1 C cid13 V cid3st C1 X r s C cid13 ss0V cid3s0 pa s0 cid9 cid12 cid12 st D s D cid21 3 4 Any policy achieves maximum 3 deﬁnition optimal policy Thus given V cid3 optimal policy easily formed choosing state s action achieves maximum 4 Planning reinforcement learning refers use models environment compute value functions optimize improve policies Par ticularly useful regard Bellman equations 2 4 recursively relate value functions If treat values V cid25 s V cid3s unknowns set Bellman equations s 2 S forms equations unique solution fact V cid25 V cid3 given 1 3 This fact key way temporaldifference dynamic programming methods estimate value functions There similar value functions Bellman equations stateaction pairs states particularly important learning methods The value taking action state s policy cid25 denoted Qcid25 s expected discounted future reward starting s taking henceforth following cid25 cid8 Qcid25 s D E D r s X rt C1 C cid13 rt C2 C cid13 2rt C3 C cid1 cid1 cid1 ss0V cid25 s pa C cid13 X s0 X 0 D r s C cid13 pa ss0 s0 a0 cid25s0 a0Qcid25 s0 a0 cid12 cid12 st D s D cid25 cid9 This known actionvalue function policy cid25 The optimal actionvalue function Qcid3s D max cid25 D r s C cid13 Qcid25 s X pa ss0 max a0 Qcid3s0 a0 s0 Finally tasks episodic nature involving repeated trials episodes ending reset standard state state distribution Episodic tasks include special terminal state arriving state terminates current episode The set regular states plus terminal state denoted SC Thus s0 pa ss0 general ranges set SC S stated earlier In episodic task values deﬁned expected cumulative reward termination inﬁnite future equivalently consider terminal state transition forever reward zero There undiscounted averagereward formulations simplicity consider For details background reinforcement learning 72 186 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 2 Options As mentioned earlier use term options generalization primitive actions include temporally extended courses action Options consist components policy cid25 S cid2 A T0 1U termination condition cid12 SC T0 1U initiation set I cid18 S An option hI cid25 cid12i available state st st 2 I If option taken actions selected according cid25 option terminates stochastically according cid12 In particular Markov option executes follows First action selected according probability distribution cid25st cid1 The environment makes transition state st C1 option terminates probability cid12st C1 continues determining C1 according cid25st C1 cid1 possibly terminating st C2 according cid12st C2 3 When option terminates agent opportunity select option For example option named openthedoor consist policy reaching grasping turning door knob termination condition recognizing door opened initiation set restricting consideration open thedoor states door present In episodic tasks termination episode terminates current option cid12 maps terminal state 1 options The initiation set termination condition option restrict range application potentially useful way In particular limit range options policy needs deﬁned For example handcrafted policy cid25 mobile robot dock battery charger deﬁned states I battery charger sight The termination condition cid12 deﬁned 1 outside I robot successfully docked A subpolicy servoing robot arm particular joint conﬁguration similarly set allowed starting states controller applied termination condition indicating target conﬁguration reached tolerance unexpected event taken subpolicy outside domain application For Markov options natural assume states option continue states option taken fs cid12s 1g cid18 I In case cid25 need deﬁned I S Sometimes useful options timeout terminate period time elapsed failed reach particular state This possible Markov options termination decisions solely basis current state long option executing To handle cases allow semiMarkov options policies termination conditions choices dependent prior events option initiated In general option initiated time t determines actions selected number steps k terminates st Ck At intermediate time cid28 t 6 cid28 t C k decisions Markov option depend scid28 decisions semiMarkov option depend entire preceding sequence st rt C1 st C1 C1 rcid28 scid28 events prior st scid28 We sequence history t cid28 denote ht cid28 We denote set histories 3 The termination condition cid12 plays role similar cid12 cid12models 71 opposite sense That cid12s paper corresponds 1 cid0 cid12s 71 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 187 cid10 In semiMarkov options policy termination condition functions possible histories cid25 cid10 cid2 A T0 1U cid12 cid10 T0 1U SemiMarkov options arise options use detailed state representation available policy selects options hierarchical abstract machines 5253 MAXQ 16 Finally note hierarchical structures options select options rise higherlevel options semiMarkov lowerlevel options Markov SemiMarkov options include general range possibilities Given set options initiation sets implicitly deﬁne set available options Os state s 2 S These Os like sets available actions As We unify kinds sets noting actions considered special case options Each action corresponds option available available I D fs 2 As g lasts exactly step cid12s D 1 8s 2 S selects cid25s D 1 8s 2 I Thus consider agents choice time entirely options persist single time step temporally extended The refer singlestep primitive options multistep options Just case actions convenient suppress s2S Os denote set differences available options states We let O D available options S Our deﬁnition options crafted like actions possible adding possibility temporally extended Because options terminate deﬁned way consider sequences way consider sequences actions We consider policies select options instead actions model consequences selecting option model results action Let consider turn Given options b consider taking sequence consider ﬁrst taking terminates b terminates omitting b altogether terminates state outside bs initiation set We options composed yield new option denoted ab corresponding way behaving The composition Markov options general semiMarkov Markov actions chosen differently ﬁrst option terminates The composition semiMarkov options semiMarkov option Because actions special cases options compose produce deterministic action sequence words classical macrooperator More interesting purposes policies options When initiated state st Markov policy options cid22 S cid2 O T0 1U selects option o 2 Ost according probability distribution cid22st cid1 The option o taken st determining actions terminates st Ck time new option selected according cid22st Ck cid1 In way policy options cid22 determines conventional policy actions ﬂat policy cid25 D ﬂatcid22 Henceforth use unqualiﬁed term policy policies options include ﬂat policies special case Note policy Markov options selects Markov corresponding ﬂat policy unlikely Markov options multistep temporally extended The action selected ﬂat policy state scid28 depends scid28 option followed time depends stochastically entire history ht cid28 policy initiated 188 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 time t 4 By analogy semiMarkov options policies depend histories way semiMarkov policies Note semiMarkov policies specialized nonstationary policies Whereas nonstationary policies depend arbitrarily preceding events semiMarkov policies depend events particular time Their decisions determined solely event subsequence time present independent events preceding time These ideas lead natural generalizations conventional value functions given policy We deﬁne value state s 2 S semiMarkov ﬂat policy cid25 expected return given cid25 initiated s V cid25 s defD E cid8 rt C1 C cid13 rt C2 C cid13 2rt C3 C cid1 cid1 cid1 cid12 cid12 Ecid25 s t cid9 Ecid25 s t denotes event cid25 initiated s time t The value state general policy cid22 deﬁned value state corresponding ﬂat policy V cid22s Ddef V ﬂatcid22s s 2 S Actionvalue functions generalize optionvalue functions We deﬁne Qcid22s o value taking option o state s 2 I policy cid22 Qcid22s o defD E cid8 rt C1 C cid13 rt C2 C cid13 2rt C3 C cid1 cid1 cid1 cid12 cid12 Eocid22 s t cid9 5 ocid22 composition o cid22 denotes semiMarkov policy ﬁrst follows o terminates starts choosing according cid22 resultant state For semiMarkov options useful deﬁne Eo h t event o continuing h time t h history ending st In continuing actions selected history preceded st That selected according oh cid1 o terminates t C 1 probability cid12hat rt C1st C1 o terminate C1 selected according ohat rt C1st C1 cid1 With deﬁnition 5 holds s history state This completes generalization temporal abstraction concept value functions given policy In section similarly generalize concept optimal value functions 3 SMDP optiontooption methods Options closely related actions special kind decision problem known semiMarkov decision process SMDP 58 In fact MDP ﬁxed set options SMDP state formally Although fact follows immediately deﬁnitions present theorem highlight state explicitly conditions consequences Theorem 1 MDP C Options D SMDP For MDP set options deﬁned MDP decision process selects options executing termination SMDP 4 For example options picking object putting object specify different actions intermediate state action taken depends option followed RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 189 Proof Sketch An SMDP consists 1 set states 2 set actions 3 pair state action expected cumulative discounted reward 4 welldeﬁned joint distribution state transit time In case set states S set actions set options The expected reward nextstate transittime distributions deﬁned state option MDP options policy termination condition cid25 cid12 These expectations distributions deﬁned MDPs Markov options semiMarkov state reward time dependent option state initiated The transit times options discrete simply special case arbitrary real intervals permitted SMDPs 2 This relationship MDPs options SMDPs provides basis theory planning learning methods options In later sections discuss limitations theory treatment options indivisible units internal structure section focus establishing beneﬁts assurances provides We establish theoretical foundations survey SMDP methods planning learning options Although formalism slightly different results essence taken adapted prior work including classical SMDP work 544525765687174 75 A result similar Theorem 1 proved Parr 52 In Sections 47 present new methods improve SMDP methods Planning options requires model consequences Fortunately appropriate form model options analogous r ss0 deﬁned earlier actions known existing SMDP theory For state option started kind model predicts state option terminate total reward received way These quantities discounted particular way For option o let Eo s t denote event o initiated state s time t Then reward model o state s 2 S s pa r o s D E rt C1 C cid13 rt C2 C cid1 cid1 cid1 C cid13 kcid01rt Ck 6 t C k random time o terminates The stateprediction model o state s po ss0 D 1X kD1 ps 0 kcid13 k 7 s0 2 S ps0 k probability option terminates s0 k steps ss0 combination likelihood s0 state o terminates Thus po measure delayed outcome relative cid13 We kind model multitime model 5455 describes outcome option single time potentially different times appropriately combined 5 5 Note deﬁnition state predictions options differs slightly given earlier actions action simply corresponding Under new deﬁnition model transition state s s0 transition probability transition probability times cid13 Henceforth use new deﬁnition given 7 cid12 cid12 Eo s t cid9 cid8 190 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Using multitime models write Bellman equations general policies options For Markov policy cid22 statevalue function written cid8 rt C1 C cid1 cid1 cid1 C cid13 kcid01rt Ck C cid13 kV cid22st Ck k duration ﬁrst option selected cid22 cid12 cid12 Ecid22 s t V cid22s D E cid9 cid21 ss0V cid22s0 po 8 X D cid22s o o2Os cid20 r o s C X s0 Bellman equation analogous 2 The corresponding Bellman equation value option o state s 2 I Qcid22s o D E D E cid8 rt C1 C cid1 cid1 cid1 C cid13 kcid01rt Ck C cid13 kV cid22st Ck cid26 rt C1 C cid1 cid1 cid1 C cid13 kcid01rt Ck cid12 cid12 Eo s t cid9 X C cid13 k D r o s C X s0 o02Os po ss0 o02O s0 cid22st Ck o0Qcid22st Ck o0 X cid22s0 o0Qcid22s0 o0 cid12 cid12 cid12 Eo s t cid27 9 Note equations specialize given earlier special case cid22 conventional policy o conventional action Also note Qcid22s o D V ocid22s Finally generalizations optimal value functions optimal Bellman equations options policies options Of course conventional optimal value functions V cid3 Qcid3 affected introduction options ultimately primitive actions options Nevertheless interesting know restricted set options include actions For example planning ﬁrst consider highlevel options order ﬁnd approximate plan quickly Let denote restricted set options O set policies selecting options O cid5O Then optimal value function given select O V cid3 Os defD max V cid22s cid222cid5O cid8 D max o2Os E rt C1 C cid1 cid1 cid1 C cid13 kcid01rt Ck C cid13 kV cid3 Ost Ck cid12 cid12 Eo s t cid9 cid20 cid21 C X k duration o taken s Os0 cid12 cid12 Eo s s0 r C cid13 kV cid3 ss0V cid3 po r o s cid8 cid9 Os0 E D max o2Os D max o2Os 10 11 Eo s denotes option o initiated state s Conditional event usual random variables s0 state o terminates r cumulative discounted reward way k number time steps elapsing s s0 The value functions Bellman equations optimal option values RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 191 Qcid3 Os o D E Qcid22s o defD max cid222cid5O cid8 rt C1 C cid1 cid1 cid1 C cid13 kcid01rt Ck C cid13 kV cid3 Ost Ck k duration o s n rt C1 C cid1 cid1 cid1 C cid13 kcid01rt Ck C cid13 k max o02OstCk D E X D r o s C Qcid3 po ss0 max o02O s0 Os0 o0 Os0 o0 cid12 cid12 cid12 Eo s Qcid3 s0 n r C cid13 k max o02O s0 D E cid12 cid12 Eo s t cid9 Qcid3 Ost Ck o0 cid12 cid12 cid12 Eo s t o o 12 r k s0 reward number steps state taking o 2 Os O V cid22cid3 Given set options O corresponding optimal policy denoted cid22cid3 Os states s 2 S If V cid3 O policy achieves V cid3 O models options known optimal policies formed choosing proposition maximizing options 10 11 Or Qcid3 O known optimal policies model choosing state s proportion options o Qcid3 Os o0 In way computing approximations V cid3 O key goals planning learning methods options Os o D maxo0 Qcid3 O s D V cid3 O Qcid3 31 SMDP planning With deﬁnitions MDP set options O formally comprises SMDP standard SMDP methods results apply Each Bellman equations options 8 9 10 12 deﬁnes equations unique solution corresponding value function These Bellman equations update rules dynamicprogramminglike planning methods ﬁnding value functions Typically solution methods problem maintain approximation V cid3 Os o states s 2 S options o 2 Os For example synchronous value iteration SVI options starts arbitrary approximation V0 V cid3 O computes sequence new approximations fVkg X Os Qcid3 cid21 Vks D max o2Os cid20 r o s C s02S ss0Vkcid01s0 po 13 s 2 S The optionvalue form SVI starts arbitrary approximation Q0 Qcid3 O computes sequence new approximations fQkg Qks o D r o s C po ss0 max o02O s0 Qkcid01s 0 0 o X s02S s 2 S o 2 Os Note algorithms reduce conventional value iteration algorithms special case O D A Standard results SMDP theory guarantee processes converge general semiMarkov options limk1 Vk D O limk1 Qk D Qcid3 V cid3 O O 192 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Fig 2 The rooms example gridworld environment stochastic celltocell actions roomtoroom hallway options Two hallway options suggested arrows labeled o1 o2 The labels G1 G2 indicate locations goals experiments described text The plans policies temporally abstract options approximate sense O maximum possible V cid3 On achieve V cid3 hand models ﬁnd correct guaranteed achieve V cid3 O We value achievement property planning options This contrasts planning methods abstract state space generally guaranteed achieve planned values models correct As simple illustration planning options consider rooms example gridworld environment rooms shown Fig 2 The cells grid correspond states environment From state agent perform actions left right stochastic effect With probability 23 actions cause agent cell corresponding direction probability 13 agent moves instead directions probability 19 In case movement agent wall agent remains cell For consider case rewards zero state transitions In rooms provide builtin hallway options designed agent room hallway cells leading room A hallway options policy cid25 follows shortest path room target hallway minimizing chance stumbling hallway For example policy hallway option shown Fig 3 The termination condition cid12s hallway option zero states s room 1 states outside room including hallway states The initiation set I comprises states room plus nontarget hallway state leading room Note options deterministic Markov options policy deﬁned outside initiation set We denote set hallway options H For option o 2 H provide priori ss0 s 2 I s0 2 S assuming goal state accurate model r o ss0 nominally large order jIj cid2 jSj Note transition models po s po RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 193 Fig 3 The policy underlying hallway options Fig 4 Value functions formed iterations planning synchronous value iteration primitive options multistep hallway options The hallway options enabled planning proceed roombyroom cellbycell The area disk cell proportional estimated value state disk ﬁlls cell represents value 10 fact sparse relatively little memory order jIj cid2 2 actually needed hold nonzero transitions state adjacent hallway states 6 Now consider sequence planning tasks navigating grid designated goal state particular hallway state labeled G1 Fig 2 Formally goal state state actions lead terminal state reward C1 Throughout paper discount cid13 D 09 rooms example As planning method SVI given 13 sets options O The initial value function V0 0 goal state initialized correct value V0G1 D 1 shown leftmost panels Fig 4 6 The offtarget hallway states exceptions possible outcomes target hallway neighboring state offtarget room 194 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 This ﬁgure contrasts planning original actions O D A planning hallway options original actions O D H The upper ﬁgure shows value function ﬁrst iterations SVI primitive actions The region accurately valued states moved cell iteration iterations states initial arbitrary value zero In lower ﬁgure shown corresponding value functions SVI hallway options In ﬁrst iteration states rooms adjacent goal state accurately valued second iteration states accurately valued Although values continued change small amounts subsequent iterations complete optimal policy known time Rather planning stepbystep hallway options enabled planning proceed higher level roombyroom faster This example particularly favorable case use multistep options goal state hallway target state options Next consider case coincidence goal lies middle room state labeled G2 Fig 2 The hallway options models previous experiment In case planning models hallway options completely solve task agent hallways goal state Fig 5 shows value functions ﬁve iterations SVI hallway options primitive options corresponding actions O D A H In ﬁrst iterations accurate values propagated G2 cell iteration models corresponding primitive options After iterations ﬁrst hallway state reached subsequently roomtoroom planning multistep hallway options dominated Note state lower Fig 5 An example goal different subgoal hallway options Planning SVI options O D A H Initial progress models primitive options actions iteration roomtoroom planning dominated greatly accelerated planning RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 195 right corner given nonzero value iteration This value corresponds plan ﬁrst going hallway state goal overwritten larger value corresponding direct route goal iteration Because multistep options close approximation correct value function fourth iteration states steps goal given nonzero values time We SVI example particularly simple planning method makes potential advantage multistep options clear In large problems SVI impractical number states large complete iterations In practice necessary selective states updated options considered states considered These issues resolved multistep options greatly aggravated Options provide tool dealing ﬂexibly Planning options necessarily complex planning actions For example ﬁrst experiment described primitive options hallway options state hallway options needed considered In addition models primitive options generated possible successors nonzero probability multistep options generated Thus planning multistep options actually computationally cheaper conventional SVI case In second experiment case use multistep options greatly increase computational costs In general course guarantee multistep options reduce overall expense planning For example Hauskrecht et al 26 shown adding multistep options actually slow SVI initial value function optimistic Research deterministic macrooperators identiﬁed related utility problem macros 2023 244776 Temporal abstraction provides ﬂexibility greatly reduce computational complexity opposite effect indiscriminately Nevertheless issues scope paper consider 32 SMDP value learning The problem ﬁnding optimal policy set options O addressed learning methods Because MDP augmented options SMDP apply SMDP learning methods 541445253 Much planning methods discussed option viewed indivisible opaque unit When execution option o started state s jump state s0 o terminates Based experience approximate optionvalue function Qs o updated For example SMDP version onestep Qlearning 5 SMDP Qlearning updates option termination Qs o Qs o C cid11 h r C cid13 k max o02O s0 Qs 0 0 o cid0 Qs o k denotes number time steps elapsing s s0 r denotes cumulative discounted reward time implicit stepsize parameter cid11 depend arbitrarily states option time steps The estimate Qs o converges 196 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Fig 6 Performance SMDP Qlearning rooms example goals sets options After 100 episodes data points averages groups 10 episodes trends clearer The step size parameter optimized nearest power 2 goal set options The results shown cid11 D 18 cases O D H G1 cid11 D 116 O D A H G2 cid11 D 14 Os o s 2 S o 2 O conditions similar conventional Q Qcid3 learning 52 easy determine optimal policy described earlier As illustration applied SMDP Qlearning rooms example goal G1 G2 Fig 2 As case planning different sets options A H A H In cases options selected set according greedy method That options usually selected random maximal option value ot Qst ot D maxo2Ost Qst o probability option instead selected randomly available options The probability random action 01 experiments The initial state episode upperleft corner Fig 6 shows learning curves goals sets options In cases multistep options enabled goal reached quickly ﬁrst episode With goal G1 methods maintained advantage conventional Qlearning experiment presumably exploration The results similar goal G2 H method performed worse long term This best solution requires steps primitive options hallway options ﬁnd best solution running hallways stumbles G2 For reason advantages A H method A method reduced 4 Interrupting options SMDP methods apply options treated opaque indivisible units More interesting potentially powerful methods possible looking inside options altering internal structure rest paper In section ﬁrst step altering options useful This area working simultaneously terms MDPs SMDPs relevant We analyze options terms SMDP use MDP interpretation change produce new SMDP RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 197 In particular section consider interrupting options terminate naturally according termination conditions Note treating options indivisible units SMDP methods limiting unnecessary way Once option selected methods require policy followed option terminates Suppose determined optionvalue function Qcid22s o policy cid22 stateoption pairs s o encountered following cid22 This function tells following cid22 committing irrevocably option reevaluate commitment step Suppose time t midst executing option o If o Markov st compare value continuing o Qcid22st o value interrupting o selecting new option according cid22 V cid22s D q cid22s qQcid22s q If highly valued interrupt o allow switch If simple actions classical policy improvement theorem 27 assure new way behaving better Here prove generalization semiMarkov options The ﬁrst empirical demonstration effectimproved performance interrupting temporally extended substep based value function planning higher levelmay Kaelbling 31 Here formally prove improvement general setting P In following theorem characterize new way behaving following policy cid220 original policy cid22 new set options cid220s o0 D cid22s o s 2 S Each new option o0 corresponding old option o terminates switching better continuing according Qcid22 In words termination condition cid120 o0 o cid120s D 1 Qcid22s o V cid22s We cid220 interrupted policy cid22 The theorem slightly general require interruption state This weakens requirement Qcid22s o completely known A important generalization theorem applies semi Markov options Markov options This generalization result intuitively accessible ﬁrst reading Fortunately result read restricted Markov case simply replacing occurrence history state set histories cid10 set states S Theorem 2 Interruption For MDP set options O Markov policy cid22 S cid2 O T0 1U deﬁne new set options O0 onetoone mapping option sets follows o D hI cid25 cid12i 2 O deﬁne corresponding o0 D hI cid25 cid120i 2 O0 cid120 D cid12 history h ends state s Qcid22h o V cid22s choose set cid120h D 1 Any histories termination conditions changed way called interrupted histories Let interrupted policy cid220 s 2 S o0 2 O0 cid220s o0 D cid22s o o option O corresponding o0 Then s V cid22s s 2 S V cid220 ii If state s 2 S nonzero probability encountering interrupted history initiating cid220 s V cid220 s V cid22s 198 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Proof Shortly arbitrary start state s executing option given interrupted policy cid220 following policy cid22 worse following policy cid22 In words following inequality holds cid21 X 0 cid22 s o 0 cid20 r o0 s C X s0 o0 V cid22s D X o cid22s o 0 po0 ss0V cid22s cid20 r o s C X s0 cid21 ss0V cid22s po 0 14 If true use expand lefthand repeatedly replacing xx0V cid22x0U occurrence V cid22x left corresponding In limit lefthand V cid220 o0 cid220x o0Tr o0 x proving V cid220 V cid22 x0 po0 To prove inequality 14 note s cid220s o0 D cid22s o ss0V cid22s0 po po0 ss0V cid22s0 r o r o0 s X X 15 P P C C C s s0 follows Let cid0 denote set interrupted histories cid0 D fh 2 cid10 cid12h 6D cid120hg Then X r o0 s C po0 ss0V cid22s0 D E cid8 r C cid13 kV cid22s0 cid12 cid12 Eo0 s h 2 cid0 cid9 s0 s0 cid8 r C cid13 kV cid22s0 cid12 cid12 Eo0 s h 2 cid0 cid9 C E s0 r k state cumulative reward number elapsed steps following option o s h history s s0 Trajectories end encountering history cid0 encounter history cid0 occur probability expected reward executing option o state s Therefore continue trajectories end encountering history cid0 option o termination follow policy cid22 E cid9 0 cid12 cid8 cid12 Eo 0 r C cid13 kV cid22s cid2 cid8 cid12s0 r C cid13 kV cid22s0 X ss0V cid22s0 po C E C s h 2 cid0 cid3 C D r o s s0 cid0 1 cid0 cid12s0 cid1cid2 r C cid13 kQcid22h o cid3 cid12 cid12 Eo0 s h 2 cid0 cid9 option o semiMarkov This proves 14 h 2 cid0 Qcid22h o 6 V cid22s0 Note strict inequality holds 15 Qcid22h o V cid22s0 history h 2 cid0 ends trajectory generated o0 nonzero probability 2 As application result consider case cid22 optimal policy given set Markov options O We discussed planning learning determine optimal value functions V cid3 O optimal policy cid22cid3 O achieves This best O Qcid3 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 199 changing O SMDP deﬁned O best possible achievable MDP V cid3 D V cid3 A But course typically wish work directly primitive actions A computational expense The interruption theorem gives way improving cid22cid3 O little additional computation stepping outside O That step interrupt current option switch new option valued highly according Qcid3 O Checking options typically vastly expense time step involved combinatorial process computing Qcid3 O In sense interruption gives nearly free improvement SMDP planning learning method computes Qcid3 O intermediate step In extreme case interrupt step switch greedy option option state highly valued according Qcid3 O polling execution 16 In case options followed step superﬂuous However options play role determining Qcid3 O basis greedy switches recall multistep options enable O quickly Qcid3 Section 3 Thus multistep Qcid3 options actually followed step provide substantial advantages computation theoretical understanding Fig 7 shows simple example Here task navigate start location goal location continuous twodimensional state space The actions movements 001 direction current state Rather work lowlevel actions inﬁnite number introduce seven landmark locations space For landmark deﬁne controller takes landmark direct path cf 48 Each controller applicable limited range states case certain distance corresponding landmark Each controller deﬁnes option circular region controllers landmark options initiation set controller policy arrival target landmark termination condition We denote set seven landmark options O Any action 001 goal location transitions terminal state discount rate cid13 1 reward cid01 transitions makes minimumtime task One landmarks coincides goal possible reach goal picking O The optimal policy O runs landmark landmark shown line upper panel Fig 7 This optimal solution SMDP deﬁned O best picking options But course better options followed way landmark The trajectory shown line Fig 7 cuts corners shorter This interrupted policy respect SMDPoptimal policy The interrupted policy takes 474 steps start goal good optimal policy primitive actions 425 steps better nominal additional cost SMDP optimal policy takes 600 steps The statevalue functions V cid22 D V cid3 policies shown lower Fig 7 Note values interrupted policy greater values original policy A related larger application interruption idea mission planning uninhabited air vehicles given 75 O V cid220 Fig 8 shows results example controllersoptions dynamics The task mass dimension rest position 0 rest position 2 200 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Fig 7 Using interruption improve navigation landmarkdirected controllers The task navigate S G minimum time options based controllers run seven landmarks black dots The circles region landmark controllers operate The line shows SMDP solution optimal behavior uses controllers interrupting line shows corresponding solution interruption cuts corners The lower panels statevalue functions SMDP interrupted solutions minimum time There option takes way 0 2 option takes 0 1 option takes position greater 05 2 Both options control precisely target position zero velocity terminating correct D 00001 Using options best ﬁrst precisely rest 1 ﬁrst option reaccelerate 2 second option This SMDPoptimal solution slower corresponding interrupted solution shown Fig 8 Because need slow nearzero velocity 1 takes 200 time steps interrupted solution takes 121 steps RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 201 Fig 8 Phasespace plot SMDP interrupted policies simple dynamical task The mass moving dimension xtC1 D xt C PxtC1 PxtC1 D Pxt C cid0 0175 Pxt xt position Pxt velocity 0175 coefﬁcient friction action applied force Two controllers provided options drives position zero velocity xcid3 D 1 xcid3 D 2 Whichever option followed time t target position xcid3 determines action taken according D 001xcid3 cid0 xt 5 Intraoption model learning In section introduce new method learning model r o ss0 option o given experience knowledge o I cid25 cid12 Our method requires cid25 deterministic option Markov For semiMarkov option general approach execute option termination times state s recording case resultant state s0 cumulative discounted reward r elapsed time k These outcomes averaged approximate expected values s po r o ss0 given 6 7 For example incremental learning rule update model execution o s po br o s Dbr o s C cid11Tr cid0br o s U 16 cid2 cid3 cid13 kcid14s0x cid0 bp o sx bp o sx D bp o sx C cid11 17 x 2 SC cid14s0x D 1 s0 D x 0 stepsize parameter cid11 constant depend state option time For example cid11 1 divided number times o experienced s updates maintain estimates sample averages experienced outcomes However averaging SMDP modellearning methods like SMDP valuelearning methods based jumping initiation termination option ignoring happens way In special case o primitive option SMDP model learning methods reduce learn conventional onestep models actions One disadvantage SMDP modellearning methods improve model option option terminates Because nonterminating options applied option timethe option 202 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 executing time For Markov options special temporaldifference methods learn usefully model option option terminates We intraoption methods learn option fragment experience option Intraoption methods learn option executing long selections consistent option Intraoption methods examples offpolicy learning methods 72 learn consequences policy actually behaving according Intraoption methods simultaneously learn models different options experience Intraoption methods introduced 71 prediction problem single unchanging policy control case consider 74 Just Bellman equations value functions Bellman equations models options Consider intraoption learning model Markov option o D hI cid25 cid12i The correct model o related X a2As D r o s D cid25s aE cid0 cid8 r C cid13 1 cid0 cid12s0 cid9 cid1 r o s0 r s0 reward state given action taken state s X cid21 X cid0 1 cid0 cid12s0 cid1 r o s0 pa ss0 cid20 r s C s0 cid25s a2As po sx D D X a2As X a2As cid25s acid13 E cid8cid0 cid1 1 cid0 cid12s0 po s0x C cid12s0cid14s0x cid9 cid25s X s0 pa ss0 cid2cid0 1 cid0 cid12s cid1 po s0x 0 C cid12s 0 cid3 cid14s0x s x 2 S How turn Bellmanlike equations update rules learning model First consider action taken st way selected consistent o D hI cid25 cid12i selected distribution cid25st cid1 Then Bellman equations suggest temporaldifference update rules cid1 br o stC1 1 cid0 cid12st C1 rt C1 C cid13 br o st cid0br o st C cid11 br o st 18 cid0 cid3 cid2 bp o st x bp o st x C cid11 cid2 cid0 cid13 1 cid0 cid12st C1 cid1 bp o stC1x C cid13cid12st C1cid14stC1x cid0 bp o st x ss0 r o cid3 19 ss0 br o x 2 SC bp o s estimates po s respectively cid11 positive stepsize parameter The method onestep intraoption model learning applies updates option consistent action taken Of course simplest intraoption modellearning method Others possible eligibility traces standard tricks offpolicy learning 71 As illustration consider model learning rooms example SMDP intra option methods As assume hallway options given RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 203 Fig 9 Model learning SMDP intraoption methods Shown average maximum I absolute errors learned true models averaged hallway options 30 repetitions experiment The lines labeled SMDP 1t SMDP method sample averages cid11 D 14 assume models given learned In experiment rewards selected according normal probability distribution standard deviation 01 mean different stateaction pair The means selected randomly beginning run uniformly Tcid01 0U interval Experience generated selecting randomly state possible options pos sible actions goal state In SMDP modellearning method Eqs 16 17 applied option terminated intraoption modellearning method Eqs 18 19 applied step options consistent action taken step In example options deterministic consis tency action selected means simply option selected action For method tried range values stepsize parameter cid11 D 12 14 18 116 Results shown Fig 9 value best method happened cid11 D 14 cases For SMDP method results stepsize parameter set model estimates sample averages best possible performance method lines labeled 1t The ﬁgure shows average maximum errors stateoption space method averaged options 30 repetitions experiment As expected intraoption method able learn signiﬁcantly faster SMDP methods 6 Intraoption value learning We turn intraoption learning option values optimal policies options If options semiMarkov SMDP methods described Section 32 feasible methods semiMarkov option completed evaluated But options Markov willing look inside consider intraoption methods Just case model learning 204 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 intraoption methods value learning potentially efﬁcient SMDP methods extract training examples experience For example suppose learning approximate Qcid3 Os o o Markov Based execution o t t Ck SMDP methods extract single training example Qcid3 Os o But o Markov sense initiated steps t t C k The jumps intermediate si st Ck valid experiences o experiences improve estimates Qcid3 Osi o Or consider option similar o selected actions terminated step later t C k C 1 t C k Formally different option formally executed experience learning relevant In fact option learn experience slightly related occasionally selecting actions generated executing option This idea offpolicy trainingto use experience occurs learn possible options irrespective role generating experience To best use experience like offpolicy intraoption versions valuelearning methods Qlearning It convenient introduce new notation value stateoption pair given option Markov executing arrival state cid1 Os o0 Qcid3 Os o D 1 cid0 cid12s Qcid3 U cid3 Os o C cid12s max o02O cid0 Then write Bellmanlike equations relate Qcid3 Os o expected values U cid3 Os0 o s0 immediate successor s initiating Markov option o D hI cid25 cid12i s cid3 Os o D Q X cid25s aE cid8 r C cid13 U 0 cid3 Os o cid9 cid12 cid12 s a2As X a2As D cid25s cid20 r s C X s0 cid21 ss0U cid3 pa Os0 o 20 r immediate reward arrival s0 Now consider learning methods based Bellman equation Suppose action taken state st produce state st C1 reward rt C1 selected way consistent Markov policy cid25 option o D hI cid25 cid12i That suppose selected according distribution cid25st cid1 Then Bellman equation suggests applying offpolicy onestep temporaldifference update Qst o Qst o C cid11 rt C1 C cid13 U st C1 o cid0 Qst o 21 cid3 cid2cid0 cid1 U s o D cid0 1 cid0 cid12s cid1 Qs o C cid12s max o02O Qs o0 The method onestep intraoption Qlearning applies update rule option o consistent action taken Note algorithm potentially dependent order options updated update U s o depends current values Qs o options o0 If options policies RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 205 deterministic concept consistency clear case prove convergence Extensions stochastic options topic current research Theorem 3 Convergence intraoption Qlearning For set Markov options O deterministic policies onestep intraoption Qlearning converges probability 1 optimal Qvalues Qcid3 O option regardless options executed learning provided action gets executed state inﬁnitely Proof Sketch On experiencing transition s r 0 s0 option o picks action state s intraoption Qlearning performs following update Qs o Qs o C cid11s o cid2 r 0 C cid13 U s0 o cid0 Qs o cid3 Our result follows directly Theorem 1 30 observation expected value update operator r 0 C cid13 U s0 o yields contraction proved cid12 cid12 cid9 cid12 cid12E D C cid8 r 0 C cid13 U s0 o cid12 X cid12 cid12 cid12r s cid12 cid12 cid12 cid12r s cid12 cid12 cid12 cid12 s0 X pa ss0 X hcid0 C s0 D 6 cid0 Qcid3 Os o pa ss0U s 0 cid3 o cid0 Q Os o cid12 cid12 cid12 cid12 pa ss0U s 0 o cid0 r s cid0 pa ss0U 0 cid3 Os o cid12 cid12 cid12 cid12 X s0 1 cid0 cid12s cid1cid0 0 Qs 0 cid3 o cid0 Q Os 0 o cid1 cid3 Os Q 0 0 o icid12 cid12 cid12 cid12 s0 X 6 s0 6 cid13 max s00o00 Qs 0 0 o 0 C cid12s cid12 cid12Qs max o02O 00 00 o pa ss0 max s00o00 cid12 cid12Qs00 o00 cid0 Qcid3 00 cid3 cid0 Q Os cid12 cid12 Os00 o00 cid0 max o02O cid12 cid12 00 o 2 As illustration applied intraoption method rooms example time goal rightmost hallway cell G1 Fig 2 Actions selected randomly equal probability primitives The update 21 applied ﬁrst primitive options hallway options consistent action The hallway options updated clockwise order starting hallways faced current state The rewards experiment previous section Fig 10 shows learning curves demonstrating effective learning option values selecting corresponding options Intraoption versions reinforcement learning methods Sarsa TDcid21 eligibilitytrace versions Sarsa Qlearning straightforward experience The intraoption Bellman equation 20 intraoption samplebased planning 206 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 Fig 10 The learning option values intraoption methods selecting options Experience generated selecting randomly actions goal G1 Shown left value greedy policy averaged states 30 repetitions experiment compared value optimal policy The right panel shows learned option values state G2 approaching correct values 7 Subgoals learning options Perhaps important aspect working MDPs SMDPs options making SMDP actions changed We seen way changing termination conditions Perhaps fundamental changing policies consider brieﬂy section It natural think options achieving subgoals kind adapt options policy better achieve subgoal For example option openthedoor natural adapt policy time effective efﬁcient opening door generally useful It possible subgoals learn independently offpolicy learning method Qlearning 1731386678 In section develop idea options framework illustrate learning hallway options rooms example We assume subgoals given address larger question source subgoals A simple way formulate subgoal option assign terminal subgoal value gs state s subset states G cid18 S These values indicate desirable option terminate state G For example learn hallway option rooms task target hallway assigned subgoal value C1 hallway states outside room assigned subgoal value 0 Let Og denote set options terminate G cid12s D 0 s 2 G cid12s D 1 s 2 G Given subgoalvalue function g G R deﬁne g s options o 2 Og expected value new statevalue function denoted V o cumulative reward option o initiated state s plus subgoal value gs0 state s0 terminates discounted appropriately Similarly deﬁne new actionvalue function Qo g s actions 2 As options o 2 Og gs D V ao Finally deﬁne optimal value functions subgoal g Qcid3 Qo V o gs V cid3 g s D max o2Og g s gs D max o2Og RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 207 Fig 11 Learning subgoalachieving hallway options random behavior Shown left error Qg s Qcid3 g s averaged s 2 I 2 A 30 repetitions The right panel shows learned state values maximum action values options state G2 approaching correct values Finding option achieves maximums optimal option subgoal deﬁned subtask For Markov options subtask Bellman equations methods learning planning original task For example onestep tabular Qlearning method updating estimate Qgst Qcid3 h gst Qgst Qgst C cid11 rt C1 C cid13 max Qgst C1 cid0 Qgst st C1 2 G Qgst Qgst C cid11 cid2 rt C1 C cid13 gst C1 cid0 Qgst cid3 st C1 2 G As simple example applied method learn policies hallway options rooms example Each option assigned subgoal values C1 target hallway 0 states outside options room including offtarget hallway The initial state upper left corner actions selected randomly equal probability goal state The parameters cid13 D 09 cid11 D 01 All rewards zero Fig 11 shows learned values hallway subgoals reliably approaching ideal values 8 Conclusion Representing knowledge ﬂexibly multiple levels temporal abstraction potential greatly speed planning learning large problems We introduced framework context reinforcement learning MDPs This context enables handle stochastic environments closedloop policies goals general way possible classical AI approaches temporal abstraction Our framework clear learned interpreted mechanically shown exhibiting simple procedures learning planning options learning models options creating new options subgoals 208 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 The foundation theory options provided existing theory SMDPs associated learning methods The fact set options deﬁnes SMDP provides rich set planning learning methods convergence theory immediate natural general way analyzing mixtures actions different time scales This theory offers lot interesting cases involve interrupting constructing decomposing options constituent parts It intermediate ground MDPs SMDPs richest possibilities new algorithms results In paper broken ground touched issues far left Key issues transfer subtasks source subgoals integration state abstraction remain incompletely understood The connection options SMDPs provides foundation addressing issues Finally paper emphasized temporally extended action interesting note implications temporally extended perception It common recognize action perception intimately linked To objects room label locate know opportunities afford action door open chair sit book read person talk If temporally extended actions modeled options models options correspond perceptions Consider robot learning recognize battery charger The useful concept set states successfully dock charger exactly produced model docking option These kinds actionoriented concepts appealing tested learned robot external supervision shown paper Acknowledgement The authors gratefully acknowledge substantial help received colleagues shared related results ideas long period paper preparation especially Amy McGovern Ron Parr Tom Dietterich Andrew Fagg B Ravindran Manfred Huber Andy Barto We thank Leo Zelevinsky Csaba Szepesvári Paul Cohen Robbie Moll Mance Harmon Sascha Engelbrecht Ted Perkins This work supported NSF grant ECS9511805 grant AFOSRF496209610254 Andrew Barto Richard Sutton Doina Precup acknowledges support Fulbright foundation Satinder Singh supported NSF grant IIS9711753 An earlier version paper appeared University Massachusetts Technical Report UMCS1998074 References 1 EG Araujo RA Grupen Learning control composition complex environment Proc 4th International Conference Simulation Adaptive Behavior 1996 pp 333342 2 M Asada S Noda S Tawaratsumida K Hosada Purposive behavior acquisition real robot vision based reinforcement learning Machine Learning 23 1996 279303 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 209 3 AG Barto SJ Bradtke SP Singh Learning act realtime dynamic programming Artiﬁcial Intelligence 72 1995 81138 4 C Boutilier RI Brafman C Geib Prioritized goal decomposition Markov decision processes Toward synthesis classical decision theoretic planning Proc IJCAI97 Nagoya Japan 1997 pp 1162 1165 5 SJ Bradtke MO Duff Reinforcement learning methods continuoustime Markov decision problems Advances Neural Information Processing Systems 7 MIT Press Cambridge MA 1995 pp 393400 6 RI Brafman M Tennenholtz Modeling agents qualitative decision makers Artiﬁcial Intelligence 94 1 1997 217268 7 RW Brockett Hybrid models motion control systems Essays Control Perspectives Theory Applications Birkhäuser Boston MA 1993 pp 2953 8 L Chrisman Reasoning probabilistic actions multiple levels granularity Proc AAAI Spring Symposium DecisionTheoretic Planning Stanford University 1994 9 M Colombetti M Dorigo G Borghi Behavior analysis training A methodology behavior engineering IEEE Trans Systems Man Cybernet Part B 26 3 1996 365380 10 RH Crites AG Barto Improving elevator performance reinforcement learning Advances Neural Information Processing Systems 8 MIT Press Cambridge MA 1996 pp 10171023 11 P Dayan Improving generalization temporal difference learning The successor representation Neural Computation 5 1993 613624 12 P Dayan GE Hinton Feudal reinforcement learning Advances Neural Information Processing Systems 5 Morgan Kaufmann San Mateo CA 1993 pp 271278 13 T Dean LP Kaelbling J Kirman A Nicholson Planning time constraints stochastic domains Artiﬁcial Intelligence 76 12 1995 3574 14 T Dean SH Lin Decomposition techniques planning stochastic domains Proc IJCAI95 Montreal Quebec Morgan Kaufmann San Mateo CA 1995 pp 11211127 See Technical Report CS9510 Brown University Department Computer Science 1995 15 GF DeJong Learning plan continuous domains Artiﬁcial Intelligence 65 1994 71141 16 TG Dietterich The MAXQ method hierarchical reinforcement learning Machine Learning Proc 15th International Conference Morgan Kaufmann San Mateo CA 1998 pp 118126 17 M Dorigo M Colombetti Robot shaping Developing autonomous agents learning Artiﬁcial Intelligence 71 1994 321370 18 GL Drescher Made Up Minds A Constructivist Approach Artiﬁcial Intelligence MIT Press Cambridge MA 1991 19 C Drummond Composing functions speed reinforcement learning changing world Proc 10th European Conference Machine Learning Springer Berlin 1998 20 O Etzioni Why PRODIGYEBL works Proc AAAI90 Boston MA MIT Press Cambridge MA 1990 pp 916922 21 RE Fikes PE Hart NJ Nilsson Learning executing generalized robot plans Artiﬁcial Intelligence 3 1972 251288 22 H Geffner B Bonet Highlevel planning control incomplete information POMDPs Proc AIPS98 Workshop Integrating Planning Scheduling Execution Dynamic Uncertain Environments 1998 23 J Gratch G DeJong A statistical approach adaptive problem solving Artiﬁcial Intelligence 88 12 1996 101161 24 R Greiner I Jurisica A statistical approach solving EBL utility problem Proc AAAI92 San Jose CA 1992 pp 241248 25 RL Grossman A Nerode AP Ravn H Rischel Hybrid Systems Springer New York 1993 26 M Hauskrecht N Meuleau C Boutilier LP Kaelbling T Dean Hierarchical solution Markov decision processes macroactions Uncertainty Artiﬁcial Intelligence Proc 14th Conference 1998 pp 220229 27 R Howard Dynamic Programming Markov Processes MIT Press Cambridge MA 1960 28 M Huber RA Grupen A feedback control structure online learning tasks Robotics Autonomous Systems 22 34 1997 303315 29 GA Iba A heuristic approach discovery macrooperators Machine Learning 3 1989 285317 210 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 30 T Jaakkola MI Jordan S Singh On convergence stochastic iterative dynamic programming algorithms Neural Computation 6 6 1994 11851201 31 LP Kaelbling Hierarchical learning stochastic domains Preliminary results Proc 10th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1993 pp 167173 32 Zs Kalmár Cs Szepesvári A Lörincz Module based reinforcement learning Experiments real robot Machine Learning 31 1998 5585 Autonomous Robots 5 1998 273295 special joint issue 33 J Kleer JS Brown A qualitative physics based conﬂuences Artiﬁcial Intelligence 24 13 1984 783 34 RE Korf Learning Solve Problems Searching MacroOperators Pitman Publishers Boston MA 1985 35 JR Koza JP Rice Automatic programming robots genetic programming Proc AAAI92 San Jose CA 1992 pp 194201 36 BJ Kuipers Commonsense knowledge space Learning experience Proc IJCAI79 Tokyo Japan 1979 pp 499501 37 JE Laird PS Rosenbloom A Newell Chunking SOAR The anatomy general learning mechanism Machine Learning 1 1986 1146 38 LJ Lin Reinforcement learning robots neural networks PhD Thesis Carnegie Mellon University Technical Report CMUCS93103 1993 39 P Maes R Brooks Learning coordinate behaviors Proc AAAI90 Boston MA 1990 pp 796802 40 S Mahadevan J Connell Automatic programming behaviorbased robots reinforcement learning Artiﬁcial Intelligence 55 23 1992 311365 41 S Mahadevan N Marchalleck T Das A Gosavi Selfimproving factory simulation continuoustime averagereward reinforcement learning Proc 14th International Conference Machine Learning 1997 pp 202210 42 P Marbach O Mihatsch M Schulte JN Tsitsiklis Reinforcement learning admission control routing integrated service networks Advances Neural Information Processing Systems 10 Morgan Kaufmann San Mateo CA 1998 pp 922928 43 MJ Mataric Behaviorbased control Examples navigation learning group behavior J Experi ment Theoret Artiﬁcial Intelligence 9 23 1997 323336 44 A McGovern RS Sutton Macroactions reinforcement learning An empirical analysis Technical Report 9870 University Massachusetts Department Computer Science 1998 45 N Meuleau M Hauskrecht KE Kim L Peshkin LP Kaelbling T Dean C Boutilier Solving large weakly coupled Markov decision processes Proc AAAI98 Madison WI 1998 pp 165172 46 S Minton Learning Search Control Knowledge An ExplanationBased Approach Kluwer Academic Dordrecht 1988 47 S Minton Quantitative results concerning utilty explanationbased learning Artiﬁcial Intelligence 42 23 1990 363391 48 AW Moore The partigame algorithm variable resolution reinforcement learning multidimensional spaces Advances Neural Information Processing Systems 6 MIT Press Cambridge MA 1994 pp 711718 49 A Newell HA Simon Human Problem Solving PrenticeHall Englewood Cliffs NJ 1972 50 J Nie S Haykin A Qlearning based dynamic channel assignment technique mobile communication systems IEEE Transactions Vehicular Technology appear 51 N Nilsson Teleoreactive programs agent control J Artiﬁcial Intelligence Res 1 1994 139158 52 R Parr Hierarchical control learning Markov decision processes PhD Thesis University California Berkeley 1998 53 R Parr S Russell Reinforcement learning hierarchies machines Advances Neural Information Processing Systems 10 MIT Press Cambridge MA 1998 pp 10431049 54 D Precup RS Sutton Multitime models reinforcement learning Proc ICML97 Workshop Modeling Reinforcement Learning 1997 55 D Precup RS Sutton Multitime models temporally abstract planning Advances Neural Information Processing Systems 10 MIT Press Cambridge MA 1998 pp 10501056 56 D Precup RS Sutton SP Singh Planning closedloop macro actions Working Notes 1997 AAAI Fall Symposium Modeldirected Autonomous Systems 1997 pp 7076 RS Sutton et al Artiﬁcial Intelligence 112 1999 181211 211 57 D Precup RS Sutton SP Singh Theoretical results reinforcement learning temporally abstract options Proc 10th European Conference Machine Learning Springer Berlin 1998 58 ML Puterman Markov Decision Problems Wiley New York 1994 59 M Ring Incremental development complex behaviors automatic construction sensorymotor hierarchies Proc 8th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1991 pp 343347 60 ED Sacerdoti Planning hierarchy abstraction spaces Artiﬁcial Intelligence 5 1974 115135 61 S Sastry Algorithms design hybrid systems Proc International Conference Information Sciences 1997 62 ACC Say S Kuru Qualitative identiﬁcation Deriving structure behavior Artiﬁcial Intelligence 83 1 1996 75141 63 J Schmidhuber Neural sequence chunkers Technische Universität München TR FKI14891 1991 64 R Simmons S Koenig Probabilistic robot navigation partially observable environments Proc IJCAI 95 Montreal Quebec Morgan Kaufmann San Mateo CA 1995 pp 10801087 65 SP Singh Reinforcement learning hierarchy abstract models Proc AAAI92 San Jose CA MITAAAI Press Cambridge MA 1992 pp 202207 66 SP Singh Scaling reinforcement learning learning variable temporal resolution models Proc 9th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1992 pp 406415 67 SP Singh The efﬁcient learning multiple task sequences Advances Neural Information Processing Systems 4 Morgan Kaufmann San Mateo CA 1992 pp 251258 68 SP Singh Transfer learning composing solutions elemental sequential tasks Machine Learning 8 34 1992 323340 69 SP Singh AG Barto RA Grupen CI Connolly Robust reinforcement learning motion planning Advances Neural Information Processing Systems 6 Morgan Kaufmann San Mateo CA 1994 pp 655 662 70 SP Singh D Bertsekas Reinforcement learning dynamic channel allocation cellular telephone systems Advances Neural Information Processing Systems 9 MIT Press Cambridge MA 1997 pp 974980 71 RS Sutton TD models Modeling world mixture time scales Proc 12th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1995 pp 531539 72 RS Sutton AG Barto Reinforcement Learning An Introduction MIT Press Cambridge MA 1998 73 RS Sutton B Pinette The learning world models connectionist networks Proc 7th Annual Conference Cognitive Science Society 1985 pp 5464 74 RS Sutton D Precup S Singh Intraoption learning temporally abstract actions Proc 15th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1998 pp 556564 75 RS Sutton S Singh D Precup B Ravindran Improved switching temporally abstract actions Advances Neural Information Processing Systems 11 MIT Press Cambridge MA 1999 pp 10661072 76 M Tambe A Newell P Rosenbloom The problem expensive chunks solution restricting expressiveness Machine Learning 5 3 1990 299348 77 GJ Tesauro Temporal difference learning TDGammon Comm ACM 38 1995 5868 78 T Thrun A Schwartz Finding structure reinforcement learning Advances Neural Information Processing Systems 7 Morgan Kaufmann San Mateo CA 1995 pp 385392 79 M Uchibe M Asada K Hosada Behavior coordination mobile robot modular reinforcement learning Proc IEEERSJ International Conference Intelligent Robots Systems 1996 pp 1329 1336 80 CJCH Watkins Learning delayed rewards PhD Thesis Cambridge University 1989 81 M Wiering J Schmidhuber HQlearning Adaptive Behavior 6 2 1997 219246 82 LE Wixson Scaling reinforcement learning techniques modularity Proc 8th International Conference Machine Learning Morgan Kaufmann San Mateo CA 1991 pp 368372