Artiﬁcial Intelligence 172 (2008) 179–203

www.elsevier.com/locate/artint

Phase transition in a random NK landscape model ✩

Sung-Soon Choi a, Kyomin Jung b, Jeong Han Kim c,∗,1

a School of Computer Science and Engineering, Seoul National University, Seoul, 151-742 Korea
b Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
c Department of Mathematics, Yonsei University, Seoul, 120-749 Korea

Received 31 March 2006; received in revised form 26 March 2007; accepted 13 June 2007

Available online 27 June 2007

Abstract

An analysis for the phase transition in a random NK landscape model, NK(n, k, z), is given. This model is motivated from
population genetics and the solubility problem for the model is equivalent to a random (k + 1)-SAT problem. Gao and Culberson
[Y. Gao, J. Culberson, An analysis of phase transition in NK landscapes, Journal of Artiﬁcial Intelligence Research 17 (2002)
√
309–332] showed that a random instance generated by NK(n, 2, z) with z > z0 = 27−7
is asymptotically insoluble. Based on
4
empirical results, they conjectured that the phase transition occurs around the value z = z0. We prove that an instance generated
by NK(n, 2, z) with z < z0 is soluble with positive probability by providing a polynomial time algorithm. Using branching process
arguments, we prove again that an instance generated by NK(n, 2, z) with z > z0 is asymptotically insoluble. The results show the
phase transition around z = z0 for NK(n, 2, z). In the course of the analysis, we introduce a generalized random 2-SAT formula,
which is of self interest, and show its phase transition phenomenon.
© 2007 Elsevier B.V. All rights reserved.

5

Keywords: NK landscape; Fitness function; Solubility; Phase transition; Satisﬁability problem

1. Introduction

1.1. NK landscape models

A ﬁtness landscape is a function that assigns each genetic composition (genotype) with the ﬁtness of the expression
(phenotype) of the genetic composition in an environment. The ﬁtness landscape sometimes refers to its graphical
representation as the word “landscape” indicates. The notion of ﬁtness landscape was ﬁrst introduced by Wright [47]
for the analysis of population genetics. Afterwards, mathematical models to study the evolution on ﬁtness landscape
have been proposed by many researchers including Franklin and Lewontin [20], Lewontin [34], Ewens [17], Kauffman
and Weinberger [30], and Macken and Perelson [35]. Among them, the NK model proposed by Kauffman [28] has
attracted considerable attention. The NK model generates ﬁtness landscapes with correlation structures in which we

✩ This work was partially carried on in Microsoft Research and partially supported by a grant of Research Institute of Mathematics funded by
Microsoft Korea.
* Corresponding author.

E-mail addresses: sschoi@soar.snu.ac.kr (S.-S. Choi), kmjung@mit.edu (K. Jung), jehkim@yonsei.ac.kr (J.H. Kim).

1 This work was partially supported by Yonsei University Research Funds 2006-1-0078 and 2007-1-0025, and by the second stage of the Brain
Korea 21 Project in 2007, and by the Korea Research Foundation Grant funded by the Korean Government (MOEHRD) (KRF-2006-312-C00455).

0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2007.06.002

180

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

can control the degree of interactions between genes and so, indirectly, the ruggedness and correlation degrees of the
landscapes.

An NK landscape is a real-valued function deﬁned on the set of binary n-tuples, {0, 1}n, which is of the form

f (x1, x2, . . . , xn) =

(cid:3)
(cid:4)
xi, Π(xi)

.

fi

n(cid:2)

i=1

It is a summation of local ﬁtness functions fi ’s, where each fi depends on its main variable xi and the variables in
the neighborhood of xi . Here the neighborhood Π(xi) is a subset of the set {x1, x2, . . . , xn} \ {xi} and its size |Π(xi)|
is k. Two ways have been introduced to choose the variables in the neighborhood Π(xi); adjacent neighborhood and
random neighborhood. In the NK models with adjacent neighborhood, Π(xi) consists of the closest k variables (with
a certain tie-break) to the main variable xi with respect to the indices modulo n. In the NK models with random
neighborhood, Π(xi) is composed of the k variables chosen uniformly at random from {x1, x2, . . . , xn} \ {xi}. Local
ﬁtness functions are constructed independently of each other. For each local ﬁtness function, a random value from a
probability distribution is assigned for each input. In general, it is independently (or nearly independently) assigned
for each of 2k+1 inputs and its expectation has small absolute value. In the Kauffman’s original model, the uniform
distribution between zero and one was used as the underlying distribution for local ﬁtness functions. Later, it has been
replaced with various probability distributions in the contexts of analysis and applications, inducing variants of the
NK model.

The name, “NK landscape”, embodies two parameters n and k. In terms of biology, each variable xi is regarded as
a gene. The parameter n represents the number of genes that an organism has. The local ﬁtness function fi quantiﬁes
the ﬁtness of a character that is determined (or expressed in a biological term) by the gene xi affected by k other genes
in Π(xi). A genotype of an organism is the values of genes xi ’s. Strictly speaking, the phenotype corresponding to
a genotype is the characters expressed by the genotype. In practice, especially in this paper, the phenotype may be
regarded as an organism that has the characters.

Generally, the parameter k plays a role in controlling the degree of interactions between genes. The larger the value
of k is, the more genes interact one another in constructing the ﬁtness landscape. Consider the case that k is small.
Given two genotypes (or assignments) with the identical values for most of the genes, most of fi ’s produce the same
values for the genotypes. Since the values of fi ’s are small relatively to the overall ﬁtness f in absolute value, the two
genotypes have similar ﬁtnesses, which implies that the landscape has strong correlation structure. On the other hand,
if k is n − 1, each fi has (nearly) independent values for the two genotypes, which induces the landscape consisting
of 2n (nearly) independent random values. Through experiments in the original NK model, Kauffman suggested that
the ruggedness of the landscape generally increases as k increases [28].

Kauffman [28] further analyzed various features of adaptive walks in the original NK model. Weinberger [42] and
Fontana et al. [19] carried out more detailed analysis of such walks. The asymptotic properties of the global and local
optima in NK landscapes were analyzed in various random NK landscape models. The differences between models are
mainly due to the underlying distributions for local ﬁtness functions. Evans and Steinsaltz [16], Durrett and Limic [13],
Skellet et al. [39], and Kaul and Jacobson [31,32] used the exponential, negative exponential, uniform, and both of
normal and uniform distributions in their works, respectively. Weinberger [43] and, later, Wright et al. [46] studied
the computational complexities of problems related to NK landscapes. Gao and Culberson [22] showed a treewidth
result for NK landscapes in a probabilistic way.

NK models have been used in biology, physics, and so on. In biology, NK models explain evolutions of biological
objects including amino acid sequences [29,30,35], protein or RNA sequences [6,18,19,37,41], and molecular quasi-
species [14]. NK models have been served as a reference point for understanding the properties of those biological
objects. In statistical physics, models of spin-glasses are investigated from the viewpoint of NK models in [42]. The
evolution of organizations in a business environment is modeled based on an NK model [33]. NK models have been
used as a benchmark for evaluating various encoding schemes and genetic operators on the evolutionary algorithm
and comparing them in the evolutionary computation area [5,24,36]. They have been also served as a basis for the
design of problem difﬁculty measures for evolutionary algorithms [26,40] and the design of epistasis measures [38].

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

181

1.2. Solubility problem and phase transition in a random NK model

One of the most interesting questions regarding NK landscape is the solubility problem that asks whether there
exists a genotype, or an assignment of values xi ’s, that maximizes the values of all local ﬁtness functions. In other
words, the problem asks whether there exists an organism that perfectly ﬁts to a given environment, which seems to
be the most natural question regarding the NK landscape. For the problem it is enough to consider binary local ﬁtness
functions having only two values 0 and 1, since one may replace each local ﬁtness function by an auxiliary binary
ﬁtness function that is 1 if and only if the value of the local ﬁtness function is its maximum. An NK landscape f is
called soluble if there is such an assignment. Otherwise, it is insoluble.

Weinberger [43] and Wright et al. [46] proved that the problem for the NK landscapes with arbitrary neighborhood
is NP-complete for k (cid:2) 2. To investigate the difﬁculties of the solubility problems for typical NK landscapes with ran-
dom neighborhood, Gao and Culberson [21] proposed two random NK landscape models and provided results about
the phase transition in the models. A phase transition in probabilistic combinatorial theory refers to the phenomenon
that the probability of a property being satisﬁed in the random model rapidly changes as the order parameter changes
around a certain value. The two random models are the uniform probability model and the ﬁxed ratio model inspired
by the two random graph models of Erd˝os-Rényi type, G(n, p) and G(n, m), respectively. In the uniform probability
model, the ﬁtness value of each input for a local ﬁtness function is independently assigned to zero with probability p
and one with probability 1 − p. This process is independently repeated for each local ﬁtness function. It was shown
that an instance generated by this model is asymptotically insoluble or, if it is soluble, a solution can be found in
polynomial time with high probability. However, unless p decreases very rapidly with n, it is easy to see that, with
high probability, a random instance has a local ﬁtness function that takes zero values for all inputs. This makes the
random instance insoluble with high probability. For this reason, the model is not desirable as a model for representing
typical instances.

The ﬁxed ratio model overcomes the drawback of the uniform probability model by ﬁxing the ratio of zero values
for each local ﬁtness function. The ﬁxed ratio model NK(n, k, z) is as follows. The value of z ranges in [0, 2k+1]. If z
is an integer, for each local ﬁtness function fi , we choose z tuples of 2k+1 possible assignments uniformly at random
without replacement and independently of other fj ’s. Then fi = 0 for those tuples and fi = 1 for the other tuples. If z
is not an integer so that z = (cid:4)z(cid:5) + h (0 < h < 1), we specify the ﬁtness values of (cid:4)(1 − h)n(cid:5) local ﬁtness functions as
if they were local ﬁtness functions in NK(n, k, (cid:4)z(cid:5)) and those of the rest of the local ﬁtness functions as if they were
in NK(n, k, (cid:4)z(cid:5) + 1). Another way to specify the ﬁtness values of local ﬁtness functions is that we regard each local
ﬁtness function as if it were a local ﬁtness function in NK(n, k, (cid:4)z(cid:5)) with probability 1 − h and in NK(n, k, (cid:4)z(cid:5) + 1)
with probability h, independently of all others. For example, if z = 2 + h, then each local ﬁtness function has zero
values for two random assignments with probability 1 − h and for three random assignments with probability h. This
new model is denoted by NK(n, 2, z). It is easy to see that NK(n, 2, z) is essentially the same as NK(n, 2, z).
√
For k = 2, it was proved [21] that an instance generated by the ﬁxed ratio model with z > z0 = 27−7
5
4

≈ 2.837
is almost always insoluble, where “an event An almost always occurs” means that limn→∞ Pr[An] = 1. And it was
empirically suggested that the instances generated by the model with z < z0 are soluble and the solutions are found in
polynomial time with probability close to one. From these, Gao and Culberson conjectured that the phase transition
takes place around z = z0 in the ﬁxed ratio model with k = 2.

1.3. Contribution and approach

In this paper, we prove that an instance generated by the ﬁxed ratio model with z < z0 is soluble with positive
probability by providing a polynomial time algorithm. This settles the conjecture in an afﬁrmative way. Using branch-
ing process arguments, we also give an another way to prove that an instance generated by the model with z > z0 is
almost always insoluble:

Theorem 1. If 0 < z < z0, then there exists α > 0 depending on z such that the probability of NK(n, 2, z) being
soluble is at least α as n goes to inﬁnity. If z > z0, then NK(n, 2, z) is almost always insoluble.

Though it is a very interesting question, we have no idea whether α can be arbitrarily close to 1 or not.

182

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

Let an NK landscape f =

To prove Theorem 1, we reduce the solubility problem of an NK landscape to the (k + 1)-SAT problem as in [21].
For given Boolean variables, the variables and their complements are called literals. Two literals are strictly distinct if
their underlying variables are different. A k-clause is a disjunction of k strictly distinct literals and a k-SAT formula is
a conjunction of k-clauses. Given a k-SAT formula F , the k-SAT problem is to ask whether there is a truth assignment
satisfying F .

(cid:5)
n
i=1 fi(xi, Π(xi)). For each local ﬁtness function fi , we construct the (k + 1)-clauses
with the literals of the main variable and neighborhood variables of fi such that fi is equal to zero only for the
assignments that do not satisfy one of the clauses. For example, suppose that a local ﬁtness function fi(xi, xj , xk)
has zero value only when (xi, xj , xk) is one of (0, 0, 0), (0, 1, 0), and (1, 1, 0). Then, we construct three 3-clauses
(xi ∨ xj ∨ xk), (xi ∨ xj ∨ xk), and (xi ∨ xj ∨ xk) for fi(xi, xj , xk). We take the conjunction of all the (k + 1)-clauses
obtained from all the fi ’s to construct a (k + 1)-SAT formula F . It is easy to check that f is soluble if and only if F
is satisﬁable. Thus, it is sufﬁcient to consider the phase transition for the satisﬁability of the 3-SAT formula F .

There have been many studies for the phase transition of the satisﬁability of the random 3-SAT formula, in which
the 3-clauses are chosen independently and uniformly at random. (See, for example, [1–3,12,25,27].) In verifying
lower bounds of the threshold, many results were obtained by applying variants of the unit clause algorithm that were
ﬁrst analyzed by Chao and Franco [8,9]. We will apply a variant of the unit clause algorithm to the 3-SAT formula
reduced from the random NK landscape NK(n, 2, z) in the subcritical region of the phase transition. In Section 2,
we describe the unit clause algorithm and investigate some properties of the reduced 3-SAT formula that should be
considered when the unit clause algorithm is applied to it. The properties suggest that it is useful to consider four
types of random 2-clauses or random equalities (of truth values of variables).

In Section 3, we introduce a generalized random 2-SAT formula consisting of the random 2-clauses and the random
equalities presented in Section 2. It generalizes the well-known random 2-SAT formula in which the 2-clauses are
chosen independently and uniformly at random [7,10]. After a parameter D is introduced, a threshold phenomenon
result is obtained: A random 2-SAT formula generated by the model is satisﬁable with positive probability if D < 1
and almost always unsatisﬁable if D > 1. It turns out that the threshold is not sharp.

In Section 4, we provide the threshold phenomenon result for the satisﬁability of the reduced 3-SAT formula,
or equivalently, the proof of Theorem 1. To obtain the result for the subcritical region, we use similar approaches
developed in Section 3. For the supercritical region, we introduce another random 2-SAT model, which is similar to
the generalized random 2-SAT model presented in Section 2. The formula generated according to the model consists
of random 2-clauses resolved from the 3-SAT formula reduced from NK(n, 2, z).

2. The unit clause algorithm

In the subcritical region, we will apply a variant of unit clause algorithm to the 3-SAT formula F (n, 2, z) reduced
from a random instance of NK(n, 2, z), and show that the algorithm ﬁnds a satisfying assignment with positive prob-
ability. The 3-clauses in the formula are to be regarded as ordered 3-tuples and (copies of) literals came from main
variables are placed in the ﬁrst coordinate of the corresponding 3-clauses. Those (copies of) literals are called main
(copies of) literals.

Now we consider the unit clause algorithm (UC). UC takes as input a formula F over n variables and outputs
a satisfying assignment of F , or outputs “Cannot determine”. UC consists of one loop of n iterations and in each
iteration of the loop, UC chooses a literal l contained in a unit clause chosen uniformly at random among all the unit
clauses. If there is no unit clause, it chooses a literal l uniformly at random among all the literals not assigned truth
values and sets l to be true. Then all the clauses containing l are satisﬁed and all the clauses containing ¯l are shortened
to the clauses without ¯l. UC fails to produce a satisfying assignment if and only if a 0-clause, a clause with no literal,
is created.

Fig. 1 describes the pseudo code of UC. For a literal l, let var(l) be the underlying variable of l. For a set V =
{x1, . . . , xn} of Boolean variables, let L(V ) denote the set of 2|V | literals on the variables of V . For i (cid:2) 0, let Ci(t)
denote the collection of all the i-clauses of F at the end of the tth iteration. When F = F (n, 2, z), C3(0) is the
collection of all the clauses of F and the other Ci(0)’s are empty. In general, it is easy to see that

Ci(t + 1) =

(cid:6)

(cid:7)
c | (c ∈ Ci(t), l /∈ c, and ¯l /∈ c) or (c ∨ ¯l) ∈ Ci+1(t)

.

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

183

UC (F )
// F : a formula

V ← {x1, x2, . . . , xn};
S ← ∅;
for t = 0, 1, . . . , n − 1
if |C1(t)| (cid:12)= 0

choose a literal l uniformly at random from C1(t);
V ← V − {var(l)};

else

choose a literal l uniformly at random from L(V );
V ← V − {var(l)};

satisfy all clauses of F containing l;
remove ¯l from all clauses of F ;
S ← S ∪ {l};

if C0(n) = ∅ output solution S;
else output “cannot determine”;

Fig. 1. Pseudo code of the unit clause algorithm.

When we apply UC to F (n, 2, z), there are three main distinctive properties to be considered. First, there may be a
pair of 3-clauses of the form (l1 ∨ l2 ∨ l3) and (l1 ∨ l2 ∨ l3). If l3 is set to be 1, then two clauses (l1 ∨ l2) and (l1 ∨ l2)
would be created. The conjunction of the two clauses is equivalent to the unit clause (l2). So we will regard it as
the unit clause. This property is called sublimation. If a local ﬁtness function is the conjunction of three clauses like
(l1 ∨ l2 ∨ l3), (l1 ∨ l2 ∨ l3), (l1 ∨ l2 ∨ l3) and l1 is set to be 1, we have three 2-clauses (l2 ∨ l3), (l2 ∨ l3), (l2 ∨ l3). By
the sublimation, we meant that these three clauses are replaced by two unit clauses (l2) and (l3), as the conjunction
of the three 2-clauses is logically equivalent to the conjunction of the two unit clauses. (Though the conjunction of
(l2 ∨ l3) and (l2 ∨ l3) induces l2 = l3, we do not add the equality since it is redundant by the fact that the conjunction
of the two unit clauses implies the equality.) If we apply UC without sublimation, UC almost always fails to satisfy
F , as shown in the following. Consider a pair of 3-clauses of the form (l1 ∨ l2 ∨ l3) and (l1 ∨ l2 ∨ l3) in C3(0). In the
process of UC, with constant probability, l3 is set to be true before var(l1) and var(l2) are set to have truth values,
which induces two clauses (l1 ∨ l2) and (l1 ∨ l2). Again, with constant probability, l2 is set to be true before var(l1) is
set to have a truth value and the two clauses are reduced to a pair of unit clauses (l1) and (¯l1). Then, UC fails to satisfy
F . Since there are (cid:4)(n) pairs of 3-clauses of this form in C3(0), without sublimation, it is not difﬁcult to show that
UC almost always fails to satisfy F .

Second, there may be a pair of 3-clauses of the form (l1 ∨ l2 ∨ l3) and (l1 ∨ l2 ∨ l3). Again, if ¯l3 is set to be true,
then two clauses (l1 ∨ l2) and (l1 ∨ l2) would be produced. The conjunction of the two clauses is equivalent to the
equality l1 = l2. Third, the main (copies of) literals from different local ﬁtness functions are strictly distinct. This fact
turns out to increase the threshold value compared to the case without main literals.

In the process of UC, 2-clauses are produced from the 3-clauses of F . Some pair of 2-clauses will become equalities
by the second property. Due to the third property, 2-clauses with main variables will appear so that the literals in their
ﬁrst places are strictly distinct. Similarly, equalities with main variables will appear too. Pairs of two clauses like
(l1 ∨ l2) and (l1 ∨ l2) do not appear because of the sublimation property. Motivated by these facts, we will separately
consider a generalized random 2-SAT formula consisting of random 2-clauses and equalities, both with and without
main variables.

As explained in Section 3, unit clauses consisting of main (copies of) literals and unit clauses consisting of other
(copies of) literals have different properties. So we will consider two types of unit clauses. Unit clauses consisting of
main literals and the (copies of) literals therein are to be colored red. The other unit clauses and the (copies of) literals
therein are to be colored blue. Then Fig. 2 is the ﬂow diagram of clauses in the process of UC.

3. A generalized random 2-SAT formula

In this section, we deﬁne a generalized random 2-SAT formula and examine its satisﬁability. As mentioned in
Section 2, the generalized random 2-SAT formula has four types of random 2-clauses or equalities. Here 2-clauses
and equalities are to be regarded as ordered pairs. The ﬁrst type consists of typical uniform random clauses, that is,

184

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

Fig. 2. Flow diagram of clauses in the process of UC.

clauses chosen uniformly at random among all the 2-clauses. The second type consists of uniform random equalities
over all the literals. The third and fourth types are the same as the ﬁrst and the second types, respectively, except that
the copy of literals in the ﬁrst places of the clauses or the equalities are pairwise strictly distinct. Those copies of literals
are called main literals. Let c1, c2, c3 and c4 be nonnegative real numbers with c3 + c4 (cid:3) 1. Denote Fi = Fi(n, ci) the
conjunction of cin 2-clauses or equalities of type i, 1 (cid:3) i (cid:3) 4. Denoted by F (n, c1, c2, c3, c4) is the conjunction of
the four random formulae with pairwise strictly distinct main literals.

If c2 = c3 = c4 = 0, it is well known [10,11,23] that F (n, c1, 0, 0, 0) is almost always satisﬁable if c1 < 1 and

almost always unsatisﬁable if c1 > 1. It turns out that the parameter

D = c1 + 2c2 + c3 + 2c4 − (c3 + 2c4)2

4

plays a similar role in the general case, as D essentially determines the branching ratio. Roughly speaking, the branch-
ing ratio is the expected number of unit clauses produced when a literal is set to be true. This is why a variant of UC
succeeds with positive probability if D < 1, and it almost always fails if D > 1. More precisely, we have the following
theorem.

Theorem 2. If D < 1, then there exists α > 0 depending on ci ’s so that the probability of F (n, c1, c2, c3, c4) being
satisﬁable is at least α as n goes to inﬁnity. If D > 1, then the random formula is almost always unsatisﬁable.

It is not difﬁcult to show that α cannot be arbitrarily close to 1 if c2 > 0 or c4 > 0. When c2 > 0, let Xij k be
the random variable representing that the equalities xi = xj , xj = xk, and xk = xi exist among the c2n uniform
] −
random equalities, and let X =
]) = O(1/n). Hence, by Chebyshev inequality [4], Pr[X > 0] is bounded below by some positive
]E[Xi2j2k2
E[Xi1j1k1
constant and so the probability of F (n, c1, c2, c3, c4) being satisﬁable is bounded above by some constant less than 1.
Similar argument holds for the case when c4 > 0.

Xi,j,k. Then, E[X] = (cid:4)(1) and Var[X] = E[X2] − E[X]2 =

(E[Xi1j1k1 Xi2j2k2

(cid:5)

(cid:5)

Theorem 2, in particular, says that the existence of main literals makes the random formula easier to be satisﬁed.
For example, if there are 0.1n uniform random 2-clauses and n random 2-clauses with main literals, then the random
formula is satisﬁable with positive probability. On the other hand, if there are 1.1n uniform random 2-clauses, the
random formula is almost always unsatisﬁable. If one equality is regarded as its corresponding two 2-clauses then
c1 + 2c2 + c3 + 2c4 represents the total number of 2-clauses. The extra term −(c3 + 2c4)2/4 is the effect of the
existence of main literals.

3.1. Subcritical region

Now we prove the ﬁrst part of Theorem 2. Without loss of generality, we may assume c1 + c2 > 0 and c3 + c4 > 0.
Otherwise, some uniform random 2-clauses or random 2-clauses with main literals might be added to F while the
conditions D < 1 and c3 + c4 (cid:3) 1 are kept. Here we deﬁne some notations. “At time t” means after t times of iteration
of UC, or equivalently, after t literals have been set. Let V (t) denote the set of variables not assigned truth values at

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

185

UC with switching server policy (F )
// F : a formula
S ← ∅;
for t = 0, . . . , n − 1

χ(t) ← 1 with probability p, χ(t) ← 0 otherwise;
if χ(t) = 1

if B(t) (cid:12)= ∅

choose a unit clause (l) uniformly at random from B(t);

else

choose a literal l uniformly at random from L(V (t));

if χ(t) = 0

if R(t) (cid:12)= ∅

choose a unit clause (l) uniformly at random from R(t);

else

if V (t) − VM (t) = ∅

output “cannot determine” and terminate;

else

choose a literal l uniformly at random from L(V (t) − VM (t));

satisfy clauses of F containing l;
remove all the copies of ¯l and sublimate if possible;
S ← S ∪ {l};

if C0(n) = ∅ output solution S;
else output “cannot determine”;

Fig. 3. Pseudo code of UCS.

time t. For 1 (cid:3) i (cid:3) 4, let Fi(t) denote the conjunction of remaining 2-clauses or equalities of Fi at time t. Deﬁne
|Fi(t)| to be the number of 2-clauses or equalities in Fi(t). Let F (t) = F1(t) ∧ F2(t) ∧ F3(t) ∧ F4(t).

As in Section 2, unit clauses consisting of main (copies of) literals and the main (copies of) literals themselves are
colored red. The other unit clauses and the (copies of) literals therein are colored blue. Let B(t) and R(t) denote the
set of blue unit clauses and red unit clauses at time t, respectively. Let VM (t) denote the set of the underlying variables
of the main literals of F3(t) and F4(t).

As mentioned, we apply a variant of UC that uses a different literal selection policy. Think of UC as an imaginary
server whose task is satisfying one unit clause, if any, at each time. We regard B(t) and R(t) as two task queues that
the server works for. The server will work for one queue at a time and the queue selection is made randomly with a
given probability p, which will be speciﬁed later. We call this modiﬁed UC UC with switching server policy (UCS).
Fig. 3 describes the pseudo code of UCS. Note that if c3 + c4 = 1 and p < 1, then UCS may encounter the case that a
literal in L(V (t) − VM (t)) must be chosen while V (t) − VM (t) is empty. We ﬁrst consider the case that c3 + c4 < 1.
Let

(cid:8)

p =

c1 + 2c2 +
c1 + 2c2 + c3 + 2c4 +

(cid:8)

(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)

(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)

.

(1)

Note that 0 < p < 1. We deﬁned p so that the expected number of blue unit clauses produced at each time is less than
p and the expected number of red unit clauses produced at each time is less than 1 − p. Using these facts and by a
coupling argument, we will show that, with positive probability, no 0-clause is produced until (1 − (cid:6))n variables are
assigned truth values, for a small constant (cid:6) > 0. When (1 − (cid:6))n variables are assigned truth values, the remaining
formula is very sparse and it is easy to show that the formula is satisﬁable with positive probability.

It is not hard to see that at each time t, F (t) has the same distribution as F (n − t, c1(t), c2(t), c3(t), c4(t)), where
ci(t) = |Fi(t)|/(n − t): Suppose a variable is set to be 1 or 0. For the literals of the variable appeared in the ﬁrst
coordinates or in the second coordinates of 2-clauses or equalities without main variables, the other literals are uniform
random among all remaining literals. For the literals of the variable appeared in the second coordinates of 2-clauses
and equalities with main variables, we ﬁrst specify the number a of such 2-clauses and equalities and then, conditioned
on the number, all a main variables are equally likely to be in the ﬁrst coordinate of the 2-clauses and equalities. This
may be also illustrated using a card game described in [2].

186

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

The distributions of the numbers of blue and red unit clauses produced at each time highly depend on the sizes
of Fi(t)’s. So, we ﬁrst show that |Fi(t)|’s are highly predictable using Wormald’s theorem [45]. Let H (t) denote the
history of Fi(t)’s, i.e., the matrix (cid:15) (cid:16)F (0), . . . , (cid:16)F (t)(cid:17), where (cid:16)F (t) = (F1(t), . . . , F4(t)). The distributions of |Fi(t +
1)| − |Fi(t)| (1 (cid:3) i (cid:3) 4) conditioned on H (t) play important role here.

Lemma 1. For any small (cid:6) > 0, we have for all 0 (cid:3) t (cid:3) (1 − (cid:6))n,

(cid:9)(cid:10)
(cid:10)
(cid:10)F1(t + 1)
(cid:10) −
(cid:10)
(cid:9)(cid:10)
(cid:10) −
(cid:10)F2(t + 1)
(cid:9)(cid:10)
(cid:10)
(cid:10)F3(t + 1)
(cid:10) −
(cid:9)(cid:10)
(cid:10)
(cid:10)F4(t + 1)
(cid:10) −

E

E

E

E

(cid:10)
(cid:10)
(cid:11)
(cid:10)F1(t)
(cid:10) | H (t)
(cid:10)
(cid:10)
(cid:11)
(cid:10)F2(t)
(cid:10) | H (t)
(cid:10)
(cid:10)
(cid:11)
(cid:10)F3(t)
(cid:10) | H (t)
(cid:10)
(cid:10)
(cid:11)
(cid:10)F4(t)
(cid:10) | H (t)

= − 2|F1(t)|
n − t
= − 2|F2(t)|
n − t

,

,

= −(1 + p)

= −(1 + p)

|F3(t)|
n − t
|F4(t)|
n − t

+ o(1),

+ o(1).

Proof. Suppose that UCS sets a literal l to be true at time t. Then |F1(t +1)|−|F1(t)| = −X1, where X1 is the number
of 2-clauses of F1(t) that contain l or l. For a 2-clause (l1 ∨ l2) of F1(t), Pr[l1 = l or l1 = l or l2 = l or l2 = l] = 2
n−t .
And the 2-clauses of F1(t) are independent from one another. So X1 has a binomial distribution Bin[|F1(t)|, 2
].
n−t
The same argument can be applied to see that |F2(t + 1)| − |F2(t)| = −X2, where X2 has a binomial distribution
Bin[|F2(t)|, 2
]. Also, |F3(t + 1)| − |F3(t)| = −Y3 − Z3, where Y3 is the number of 2-clauses of F3(t) whose main
n−t
literals are equal to l or ¯l, and Z3 is the number of 2-clauses of F3(t) whose second literals are equal to l or ¯l. Here
we divide into two cases according to the value of χ(t), which is deﬁned in Fig. 3. First, suppose that χ(t) = 1. Then
|F3(t)|
since l and ¯l are equal to at most one of the main literals of F3(t).
Y3 has Bernoulli distribution with density
n−t
And Z3 has a binomial distribution Bin[|F3(t)|,
] according to whether Y3 = 0 or
1
(n−t−1)
Y3 = 1. Suppose that χ(t) = 0. Then Y3 = 0 because l and ¯l cannot be equal to any of main literals of F3(t). And
Z3 has a binomial distribution Bin[|F3(t)|,
]. The distribution and the expectation of |F4(t + 1)| − |F4(t)| are
obtained in the same way. So the lemma follows. (cid:2)

] or Bin[|F3(t)| − 1,

1
(n−t−1)

1
(n−t−1)

Now we state Wormald theorem. A function g is said to satisfy a Lipschitz condition on an open set D0 ⊂ Rk+1 if
|ui − vi|, for all (u1, . . . , uk+1)

there exists a constant L > 0 such that |g(u1, . . . , uk+1) − g(v1, . . . , vk+1)| (cid:3) L
and (v1, . . . , vk+1) in D0.

k+1
i=1

(cid:5)

Theorem 3 (Wormald). For 1 (cid:3) j (cid:3) m, where m is a ﬁxed number, let Yj (t) (which also depends on n) be a sequence
of real-valued random variables such that for all j , all t with 0 (cid:3) t (cid:3) t0 = t0(n), and n, |Yj (t)| (cid:3) C0n for some con-
stant C0. Let H (t) denote the history of sequences, i.e. the matrix (cid:15) (cid:16)Y (0), . . . , (cid:16)Y (t)(cid:17), where (cid:16)Y (t) = (Y1(t), . . . , Ym(t)).
Let D0 be some bounded connected open set of Rm+1 containing the closure of {(0, z1, . . . , zm) | zj = Yj (0)
n ,
1 (cid:3) j (cid:3) m, for some n}. Let gj : Rm+1 → R, 1 (cid:3) j (cid:3) m, and suppose that the followings are true for some t0 = t0(n).

(cid:9)
E

(i) For all j and uniformly over all 0 (cid:3) t < t0,
(cid:11)
Yj (t + 1) − Yj (t) | H (t)
(ii) For all j and uniformly over all 0 (cid:3) t < t0,
(cid:10)
(cid:9)(cid:10)
(cid:11)
(cid:10) > n
(cid:10)Yj (t + 1) − Yj (t)
5 | H (t)

= gj

Pr

1

= o(n

−3).

(cid:3)
t/n, Y1(t)/n, . . . , Ym(t)/n

(cid:4)

+ o(1).

(iii) For each j , gj is continuous and satisﬁes a Lipschitz condition on D0.

Then the followings hold.

(a) For (0, ˆz(1), . . . , ˆz(m)) ∈ D0 the system of differential equations

dzj
ds

= gj (s, z1, . . . , zm),

1 (cid:3) j (cid:3) m

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

187

has a unique solution in D0 for zj : R → R passing through zj (0) = ˆz(j ), 1 (cid:3) j (cid:3) m, and which extends to points
arbitrarily close to the boundary of D0.

(b) Almost always Yj (t) = zj ( t

solution in (a) with ˆz(j ) = Yj (0)

n ) · n + o(n) uniformly for 0 (cid:3) t (cid:3) min{σ n, t0} and for each i, where zj (s) is the
n , and σ = σ (n) is the supremum of those s to which the solution can be extended.

Using Lemma 1 and applying Wormald theorem, we may have

(cid:13)

(cid:12)

Lemma 2. For any small (cid:6) > 0, almost always we have uniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n,
|F2(t)|
n − t
|F4(t)|
n − t

1 − t
n
(cid:12)
1 − t
n

|F1(t)|
n − t
|F3(t)|
n − t

1 − t
n
1 − t
n

+ o(1),
(cid:13)

+ o(1),

+ o(1),

+ o(1).

= c1

= c2

= c4

= c3

(cid:13)

(cid:12)

(cid:12)

(cid:13)

p

p

(2)

Proof. To apply Wormald theorem to our situation, let m = 4, Yi(t) = |Fi(t)| (1 (cid:3) i (cid:3) 4), C0 = c1 + c2 + c3 + c4,
and t0 = (1 − (cid:6))n. Let
(cid:6)
(s, z1, z2, z3, z4) | −(cid:6) < s < 1, −(cid:6) < zi < ci + (cid:6)

D0 =

(cid:7)

,

and

g1(s, z1, z2, z3, z4) = − 2z1
1 − s

,

g3(s, z1, z2, z3, z4) = −(1 + p)

z3
1 − s

,

g2(s, z1, z2, z3, z4) = − 2z2
1 − s

,

g4(s, z1, z2, z3, z4) = −(1 + p)

z4
1 − s

.

By the expectations and distributions of |Fi+1(t)| − |Fi(t)|, the conditions in Wormald theorem are easily satisﬁed

directly. Then we get ϕi : [0, 1 − (cid:6)] → R, the solution of the following system of differential equations,

⎧

⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩

dϕ1
dx
dϕ2
dx
dϕ3
dx
dϕ4
dx

= − 2ϕ1(x)
1−x
= − 2ϕ2(x)
1−x
= −(1 + p) ϕ3(x)
1−x
= −(1 + p) ϕ4(x)
1−x

ϕ1(0) = c1,
ϕ2(0) = c2,
ϕ3(0) = c3,
ϕ4(0) = c4

such that, almost always |Fi(t)| = ϕi( t
ϕ1(x) = c1(1 − x)2, ϕ2(x) = c2(1 − x)2, ϕ3(x) = c3(1 − x)1+p, and ϕ4(x) = c4(1 − x)1+p. (cid:2)

n ) · n + o(n) holds uniformly for 0 (cid:3) t (cid:3) (1 − (cid:6))n and 1 (cid:3) i (cid:3) 4. Note that

As limx→1

(cid:5)

4
i=1

ϕi (x)
1−x

= 0, we may choose (cid:6) > 0 so that almost always the total number of 2-clauses and equalities

remaining at time t = (1 − (cid:6))n is less than 0.01(cid:6)n. From Lemma 2, almost always the following holds:
(cid:10)
(cid:10)
(cid:10)F4(t)
(cid:10) (cid:3) (c3 + c4)(n − t) + o(n) < n − t =

(cid:10)
(cid:10)
(cid:10) =
(cid:10)VM (t)

(cid:10)
(cid:10)
(cid:10)F3(t)
(cid:10) +

(cid:10)
(cid:10)
(cid:10),
(cid:10)V (t)

uniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. So, for 0 (cid:3) t (cid:3) (1 − (cid:6))n, almost always UCS does not encounter the case that
χ(t) = 0 but V (t) − VM (t) is empty.

For 1 (cid:3) i (cid:3) 4, let bi(t) be the number of blue unit clauses coming from Fi(t) at time t and let ri(t) be the number
of red unit clauses coming from Fi(t) at time t. We will obtain the expectations and distributions of bi(t)’s and
ri(t)’s, conditioned on |Fi(t)|’s. Suppose that UCS sets a literal l to be true at time t. Then b1(t) is the number of
2-clauses in F1(t) that contain ¯l. And r1(t) = 0 since there is no main literal in F1(t). Note that, for each 2-clause
(l1 ∨ l2) ∈ F1(t), Pr[¯l = l1 or ¯l = l2] = 1
n−t . And the 2-clauses in F1(t) are independent from one another. So b1(t)
]. The same argument can be applied to have that r2(t) = 0 and b2(t) has
has a binomial distribution Bin[|F1(t)|,
1
(n−t)
a binomial distribution Bin[|F2(t)|,
]. For b3(t), observe that b3(t) is the number of 2-clauses in F3(t) whose
2
(n−t)
main literals are ¯l. Here we consider two cases according to the value of χ(t). First, suppose that χ(t) = 1. Then
|F3(t)|
2(n−t) since ¯l may be equal to at most one of the main literals in F3(t).
b3(t) has a Bernoulli distribution with density
When χ(t) = 0, b3(t) = 0 since ¯l cannot be equal to any of the main literals in F3(t) and hence only unit clauses

188

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

with main literals are produced. For r3(t), observe that r3(t) is the number of 2-clauses in F3(t) whose second literals
are equal to ¯l. Suppose that l is not strictly distinct with one of main variables of F3(t). Then, since exactly one
2-clause in F3(t) has l or ¯l as main literal, r3(t) has a binomial distribution Bin[|F3(t)| − 1,
]. Otherwise,
r3(t) has a binomial distribution Bin[|F3(t)|,
]. Thus, the distribution of r3(t) is a linear combination of
Bin[|F3(t)| − 1,
]. The same argument can be applied to obtain the distributions of
|F4(t)|
b4(t) and r4(t). If χ(t) = 1, b4(t) has a Bernoulli distribution with density
(n−t) and if χ(t) = 0, then b4(t) = 0. And
] if l is not strictly distinct with one of the main variables
r4(t) has a binomial distribution Bin[|F4(t)| − 1,
]. Thus, the distribution of r4(t) is a linear
of F4(t). Otherwise, r4(t) has a binomial distribution Bin[|F4(t)|,
1
(n−t−1)
combination of Bin[|F4(t)| − 1,
].

1
2(n−t−1)
1
2(n−t−1)

] and Bin[|F4(t)|,

] and Bin[|F3(t)|,

1
2(n−t−1)

1
2(n−t−1)

1
(n−t−1)

i (t)’s and r ∗

i (t)’s (and r ∗
] and r ∗

Observe that bi(t)’s and ri(t)’s (over t) are dependent random variables, which makes our analysis difﬁcult. For-
tunately, the dependency is weak and it is possible to bypass this obstacle using couplings. It is not hard to see that
we may take mutually independent random variables b∗
i (t)’s) as follows: The random variable b∗
1(t) has
a binomial distribution Bin[(1 + δ)c1n(1 − t
1 (t) = 0, where δ > 0 is a small constant deﬁned later.
Though b∗
i (t)’s, i = 2, 3, 4, may be similarly deﬁned, we just list their distributions for completeness. The
] and r ∗
random variable b∗
2
n )2,
3(t) is χ times
(n−t)
n )p, and r ∗
a random variable that has a Bernoulli distribution with density (1 + δ) c3
2 (1 − t
3 (t) has a binomial distribu-
tion Bin[(1 + δ)c3n(1 − t
1
4(t) is χ times a random variable that has a Bernoulli distribution
2(n−t−1)
n )p, and r ∗
with density (1 + δ) c3
]. We couple
bi(t) and b∗
i (t)). In
particular, almost always

4 (t) has a binomial distribution Bin[(1 + δ)c4n(1 − t
i (t)) so that if |Fi(t)|’s satisfy Eq. (2), then bi(t) (cid:3) b∗

2(t) has a binomial distribution Bin[(1 + δ)c1n(1 − t

2 (1 − t
i (t), (and ri(t) and r ∗

n )1+p,
1
(n−t−1)
i (t) (and ri(t) (cid:3) r ∗

2 (t) = 0. And, b∗

]. Finally, b∗

n )1+p,

1
(n−t)

n )2,

1
(n−t−1)

1
(n−t−1)

∗
i (t),

bi(t) (cid:3) b

ri(t) (cid:3) r
uniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. The expectations of b∗
(cid:18)

(cid:19)

(cid:19)

(cid:18)

∗
i (t)

E[b∗
E[r ∗

i (t)]
i (t)]

= T

∗
i (t) ·

p
1 − p

+ o(1),

i (t) and r ∗

i (t) are as follows:

where

,

(cid:19)

1
0

(cid:13) (cid:18)

(cid:12)
∗
1 (t) = c1(1 + δ)
T
(cid:12)
∗
3 (t) = c3(1 + δ)

1 − t
n
1 − t
1
1
n
(cid:5)
i (t), and r ∗(t) =
4
(cid:5)
always, b(t) (cid:3) b∗(t) and r(t) (cid:3) r ∗(t) for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. By letting T ∗(t) =

1
2
1
2
(cid:5)
4
i=1 bi(t), r(t) =

(cid:12)
∗
2 (t) = c2(1 + δ)
(cid:12)
∗
4 (t) = c4(1 + δ)
(cid:5)

1 − t
n
1 − t
n

i=1 ri(t), b∗(t) =

Then for b(t) =

i=1 b∗

2 2
0 0
(cid:18)

1
0
(cid:18)

(cid:13) (cid:18)

0
1
2

(cid:13)

(cid:13)

(cid:19)

T

T

T

p

p

,

4

(cid:19)

,

(cid:19)

.

0
1
(cid:5)
i=1 r ∗
4
i=1 T ∗

4

(cid:18)

(cid:19)

E[b∗(t)]
E[r ∗(t)]

(cid:18)

(cid:19)

= T

∗

(t) ·

p
1 − p

+ o(1).

i (t), it is clear that, almost
i (t), we have that

Lemma 3. There exists a constant δ > 0 such that, E[b∗(t)] < p − δ and E[r ∗(t)] < 1 − p − δ for all 0 (cid:3) t (cid:3) (1 − (cid:6))n.

(cid:9)

(cid:11)

(cid:9)

(cid:11)

Proof. Since T ∗(t) ·
each pair of the entries, it sufﬁces to show that
(cid:19)

(cid:3) T ∗(0) ·

p
1−p

p
1−p

(cid:19)

(cid:18)

(cid:18)

, where the inequality for the vectors means that the inequality holds for

∗

(0) ·

T

p
1 − p

<

p
1 − p

.

Clearly,

∗

T

(0) = (1 + δ)

(cid:18)

c1 + 2c2 + 1
2 c3 + c4
1

2 c3 + c4

c1 + 2c2
2 c3 + c4
1

(cid:19)

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

189

has two nonnegative eigenvalues,

c1 + 2c2 + c3 + 2c4 ±

(1 + δ)

(cid:8)

(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)

2

.

(cid:9)

Let λ(δ) be the larger one. Since D < 1 implies λ(0) < 1, we may choose a small constant δ so that λ(δ) < 1. Note
that

(cid:11)

is an eigenvector of T ∗(0) corresponding to λ(δ). So,
(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:18)

(cid:19)

p
1−p

∗

(0) ·

T

p
1 − p

= λ(δ)

p
1 − p

<

p
1 − p

,

as desired. (cid:2)

Then using these facts, we show that almost always the sizes of

|R(t)| are O(n). In the course of
that, we use a simpliﬁed version of Lazy-server lemma, which was introduced by Achlioptas [1]. Suppose that there is
a server so that the probability that the server would work at time t is w(t) and, if it works, it can handle one task per
unit time. And the expected number of tasks that arrive to the server at time t is z(t). Then Lazy-server lemma says
that if z(t) is bounded above by w(t) uniformly for all t, then almost always the sum of sizes of the task queue over
all t would be bounded linearly.

|B(t)| and

(cid:5)

(cid:5)

Lemma 4 (Lazy-server lemma). Let Z(0), Z(1), . . . be a sequence of random variables and denote z(t) = E[Z(t)].
Let W (0), W (1), . . . be a sequence of independent Bernoulli random variables with density w(t), i.e. W (t) = 1 with
probability w(t), and 0 otherwise. Let Q(0), Q(1), . . . be a sequence of random variables deﬁned by Q(0) = 0 and
Q(t + 1) = max(Q(t) − W (t), 0) + Z(t). Assume that

(i) there exist a constant ρ > 0 such that for all t (cid:2) 0,

z(t) < w(t) − ρ;

(ii) there exist constants a, b, c > 0 such that for any ﬁxed 0 (cid:3) j1 (cid:3) j2 and β > 0,

(cid:20)

j2(cid:2)

Pr

t=j1

Z(t) > (1 + β)

(cid:21)

(cid:22)

z(t)

< exp

−aβb

(cid:22)

j2(cid:2)

(cid:23)

(cid:23)

c

z(t)

.

t=j1

j2(cid:2)

t=j1

Then there exists constant C and K depending on ρ, a, b, c such that for every m (cid:2) 1,

(cid:20)

m−1(cid:2)

(cid:21)

Pr

Q(t) > Cm

= O(m

−2),

(cid:9)
Pr

t=0

max
0(cid:2)t<m

(cid:11)
Q(t) > logK m

= O(m

−2).

From the Lazy-server lemma, we have

Lemma 5. We almost always have

(1−(cid:6))n(cid:2)
(cid:10)
(cid:10)
(cid:10) < Cn,
(cid:10)B(t)

t=0
(1−(cid:6))n(cid:2)
(cid:10)
(cid:10)
(cid:10) < Cn,
(cid:10)R(t)

t=0

(cid:10)
(cid:10)
(cid:10) < logK n,
(cid:10)B(t)

max
0(cid:2)t(cid:2)(1−(cid:6))n

(cid:10)
(cid:10)
(cid:10) < logK n,
(cid:10)R(t)

max
0(cid:2)t(cid:2)(1−(cid:6))n

for some constants C, K.

190

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

Proof. We apply Lazy-server lemma with Z(t) = b∗(t), W (t) = χ(t), w(t) = p for 0 (cid:3) t (cid:3) (1 − (cid:6))n. Condition (i)
follows by Lemma 3. And condition (ii) follows by Chernoff bound. So almost always for some constants C1 = C1((cid:6))
and K1 = K1((cid:6)),
(1−(cid:6))n(cid:2)

Q(t) < C1n and

max
0(cid:2)t(cid:2)(1−(cid:6))n

t=0

Q(t) < logK1 n.

Note that |B(t + 1)| = max(|B(t)| − χ(t), 0) + b(t), and almost always b(t) (cid:3) b∗(t) for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. So, by
induction, almost always |B(t)| (cid:3) Q(t),

(1−(cid:6))n(cid:2)

(cid:10)
(cid:10)
(cid:10) < C1n,
(cid:10)B(t)

and

t=0

(cid:10)
(cid:10)
(cid:10) < logK1 n.
(cid:10)Q(t)

max
0(cid:2)t(cid:2)(1−(cid:6))n

Same argument can be applied to obtain that almost always there exist constants C2 = C2((cid:6)) and K2 = K2((cid:6)) such

that

(1−(cid:6))n(cid:2)

t=0

(cid:10)
(cid:10)
(cid:10) < C2n and
(cid:10)R(t)

(cid:10)
(cid:10)
(cid:10) < logK2 n.
(cid:10)R(t)

max
0(cid:2)t(cid:2)(1−(cid:6))n

Then for C = C((cid:6)) = C1 + C2 and K = K((cid:6)) = max{K1 + 1, K2 + 1}, almost always

(1−(cid:6))n(cid:2)

(cid:10)
(cid:3)(cid:10)
(cid:10) +
(cid:10)B(t)

(cid:10)
(cid:10)
(cid:10)
(cid:10)R(t)

(cid:4)

t=0

< Cn and

max
0(cid:2)t<(1−(cid:6))n

(cid:10)
(cid:3)(cid:10)
(cid:10) +
(cid:10)B(t)

(cid:10)
(cid:10)
(cid:10)
(cid:10)R(t)

(cid:4)

< logK n.

(cid:2)

Now we prove that with positive probability no 0-clause is produced until t = (1 − (cid:6))n. Under the condition that

no 0-clause is produced until time t − 1, the probability that the same holds until time t is at least

(cid:12)

1 −

1
2(n − t − 1)

(cid:13)|B(t)|(cid:12)

1 −

|R(t)|
2(n − t − 1)

(cid:13)

(cid:12)
1 − 2
(cid:6)n

(cid:2)

(cid:13)|B(t)|+|R(t)|

,

(3)

for the literals in R(t) are strictly distinct. Therefore, the probability that no 0-clause is produced until t = (1 − (cid:6))n is
at least
(cid:12)

(cid:13)(cid:5)

(cid:13)

1 − 2
(cid:6)n

n−(cid:6)n
t=0 (|B(t)|+|R(t)|)

(cid:12)
1 − 2
(cid:6)n

(cid:2)

Cn

= e

− 2C

(cid:6) + o(1).

We complete the proof of the ﬁrst part of Theorem 2 by showing that when no 0-clause is produced until t =
(1 − (cid:6))n, the remaining formula at t = (1 − (cid:6))n is satisﬁable with positive probability. Remind that the total number
of 2-clauses and equalities remaining at the time is almost always less than 0.01(cid:6)n. Think of a multi graph G deﬁned
by the following rules. The vertices represent the Boolean variables in V ((1 − (cid:6))n). For each 2-clause or equality of
F ((1 − (cid:6))n), we set an edge between the two vertices that appear in the 2-clause or equality. Notice that if G is acyclic
and no pair of unit clauses at time t = (1 − (cid:6))n belongs to the same connected component, then there exists a truth
assignment of V ((1 − (cid:6))n) which satisﬁes F ((1 − (cid:6))n).

Note that

Pr[G has a cycle] (cid:3) E[number of cycles of G] =

(cid:2)

possible cycles CG

Pr[CG appears in G].

The last summand is at most
(cid:12)

(cid:13)

(cid:6)n(cid:2)

k(cid:2)

(cid:10)
(cid:10)2VM

(cid:3)
(1 − (cid:6))n

(cid:12)

(cid:4)(cid:10)
(cid:10)j ((cid:6)n)k−j

(cid:12)

(cid:13)
j

1
(cid:6)n − 1

|F1(n − (cid:6)n)| + |F2(n − (cid:6)n)|
(cid:4)
(cid:3)
(cid:6)n
2

(cid:13)

k−j

,

where k is the length of cycle CG, and j is the number of edges from F3((1 − (cid:6))n) or F4((1 − (cid:6))n). This summand is
at most

(cid:12)

(cid:13)(cid:12)

(cid:13)

(cid:12)

j

0.02

(cid:6)n
(cid:6)n − 1

0.02

(cid:6)n
(cid:6)n − 1

(cid:13)

k−j

< 0.1.

k
j

k
j

k=2

j =0

∞(cid:2)

k(cid:2)

k=2

j =0

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

191

(cid:3)

(cid:4)

Let Ai ’s be the connected components of G. Since there are (cid:6)n vertices and at most 0.01(cid:6)n edges in G, we can
show that, almost always, |Ai| = O(log n) for all i and thus
|Ai|2 = O(n log2 n) by using a similar argument as in
the well-known result in random graphs [15]. Hence, for each pair of the existing unit clauses (v1) and (v2) at time
t = (1 − (cid:6))n, the probability that v1 and v2 are in Ai for some i is O
. So the probability that there is a pair
(cid:4)
logK n
of unit clauses at time t = (1 − (cid:6))n that belong to the same connected component is at most
= o(1).
O
2
Therefore, with probability higher than 0.9, the remaining formula at the time is satisﬁable.

log2 n
n

log2 n
n

(cid:5)
i

Now consider the case that c3 + c4 = 1. Since V (0) − VM (0) is empty in this case as mentioned above, UCS
may encounter the case that χ(t) = 0 but V (t) − VM (t) is empty unless p = 1. However, when we set p as in (1),
UCS may not encounter the case that χ(t) = 0 but V (t) − VM (t) is empty: Initially, |V (0) − VM (0)| = 0. At each
step t of the ﬁrst δn steps, if χ(t) = 1, then the expected change of |V (t) − VM (t)| is 1 + O(δ), as one uniform
random literal eliminates 2 + O(δ) 2-clauses or equalities with main literals, in expectation. The other effects are
small enough if δ is small enough. If χ(t) = 0, then the expected change is O(δ), as a nonmain literal eliminates
1 + O(δ) 2-clause or equality with main literal, in average. Thus, at each step, |V (t) − VM (t)| increases by p + O(δ),
in average, and hence |V (t) − VM (t)| > 0 for t (cid:2) 1 with positive probability. Notice that UCS produces 0-clause in
the ﬁrst δn steps with probability O(δ) (cf. Lazy-server lemma). Therefore, with positive probability, UCS proceeds
to the ﬁrst δn steps without encountering χ(t) = 0 and V (t) − VM (t) = ∅. After t = δn steps, it is easy to see that
c3(t) + c4(t) (cid:3) (1 − (cid:6)0)(n − t) for some constant (cid:6)0 > 0, which is covered in the previous case.

(cid:3)

(cid:4)

(cid:3)

3.2. Supercritical region

For a 2-SAT formula F , setting a literal x to be true produces the unit clause (l) after x is removed from each
clause (x ∨ l) in F . Again, setting the literal l to be true yields other unit clauses and the process repeats for other unit
clauses, if any. The process terminates if there is no more unit clause. This process is called an implication process
starting from x, or simply an implication process if the identity of x is clear in the context. Strictly speaking, an
implication process depends on the order of unit clauses chosen. We assume that there is a ﬁxed order among all
the unit clauses, as our argument below does not depend on a particular order. For the generalized random 2-SAT
formula we are dealing with, there are two types of unit clauses colored blue and red as explained in the proof for
the subcritical region. So we call the process in which blue and red unit clauses are distinguished an implication
process with two types. If the implication process starting from x produces a 0-clause, there does not exist a satisfying
assignment setting x to be true for the formula F . In addition, if the implication process starting from x also produces a
0-clause, there does not exist a satisfying assignment setting x to be true (equivalently setting x to be false) for F and,
consequently, the formula F is unsatisﬁable. In the following, we will prove the unsatisﬁability of F (n, c1, c2, c3, c4)
with D > 1 by showing that there almost always exists such a variable x that both of x and x produce 0-clauses in the
random formula. To maintain the independence among the implications in the implication process, once a 2-clause or
an equality is used in the process, we remove it from the formula and do not consider it in the subsequent process.

Before investigating the implication process with two types, we consider a branching process with two types, a
simpler than but very close to the implication process. In the branching process, there are two types of organisms, say
colored blue and red, and each type produces both types of organisms according to a certain probability distribution
independently of the other. For an organism x, an organism is in the ﬁrst generation if it is produced by x. In general,
an organism is in the kth generation if it is produced by an organism in the (k − 1)th generation. We use the term
“after k generations” when all organisms of kth or less generation are exposed. Suppose that a blue organism produces
a blue and c red organisms in expectation and a red organism produces b blue and d red organisms in expectation.
Then, the branching ratios of the process are represented by a 2 × 2 matrix

(cid:18)

(cid:19)

A =

a
c

b
d

.

We have the following result.

Lemma 6. Suppose a two-type branching process with the branching ratio matrix

(cid:18)

(cid:19)

A =

a
c

b
d

,

192

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

with a (cid:2) d, and the larger eigenvalue of A is larger than 1. Then, there is a constant κ such that the expected number
of the blue organisms that are produced after κ generations, starting from one blue organism, is larger than 1.

Proof. Let Bb(k) (Rb(k), resp.) be the expected numbers of blue (red, resp.) organisms produced after k generations,
starting from one blue organism and Br (k) (Rr (k), resp.) be the expected numbers of blue (red, resp.) organisms
produced after k generations, starting from one red organism. Then, by induction on k,

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

Bb(k)
Rb(k)

= Ak

1
0

,

and

Br (k)
Rr (k)

= Ak

0
1

.

Let λA be the larger eigenvalue of A and [u, v] be the corresponding eigenvector with u + v = 1, that is,

λA = a + d +

(a − d)2 + 4bc

,

(cid:8)

2

and

(cid:18)

(cid:19)

(cid:18)

u
v

= μ

(cid:8)

a − d +

(a − d)2 + 4bc
2c

(cid:19)

,

where μ is a positive constant.

If b = 0 or c = 0, a (cid:2) d and λA > 1 imply that Bb(1) = a > 1. Thus we may take κ = 1. Suppose that b > 0 and

c > 0. We ﬁrst choose the minimum integer K such that (λA)K min{u, v} > 2, i.e., set

K =

(cid:24)

log

2
min{u,v}

(cid:25)

log λA

+ 1.

If Bb(K) > 1, we can set κ = K. If Rr (K) > 1, it is easy to see that there is a constant α so that Rr (αK) > 1.1/(bc).
As, in expectation, one blue organism produces c red organisms, and one red organisms produces Rr (αK) red organ-
isms after αK generations, and then each of those red organism produces b blue organisms, we have bcRr (αK), which
is at least 1.1, blue organisms in expectation after αK + 2 generations. Suppose now Bb(K) (cid:3) 1 and Rr (K) (cid:3) 1. Note
that

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

u
v

AK

1
0
On the other hand, as [u, v] is an eigenvector of A,

= uAK

+ vAK

0
1

=

u · Bb(K) + v · Rr (K)
u · Rb(K) + v · Rr (K)

(cid:19)

.

(cid:18)

(cid:19)

(cid:18)

AK

u
v

=

(λA)K u
(λA)K v

(cid:19)

.

As (λA)K min{u, v} > 2 and all of u, v, Bb(K), and Rr (K) are less than or equal to 1, Br (K) > 1 and Rb(K) > 1 and
hence

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

Bb(2K)
Rb(2K)

= A2K

1
0

= AK AK

1
0

=

Bb(K)2 + Br (K) · Rb(K)
Bb(K) · Rb(K) + Rb(K) · Rr (K)

(cid:19)

.

Therefore, Bb(2K) (cid:2) Br (K)Rb(K) > 1 and we can set κ = 2K. (cid:2)

It is well known that a branching process with branching ratio larger than one continues forever with positive

probability [4]. From Lemma 6, we have an analogy for branching processes with two types.

Corollary 1. Suppose a two-type branching process with the branching ratio matrix satisfying the condition in
Lemma 6. Then, the branching process, starting from one blue organism, continues forever with positive probabil-
ity.

Now we prove the second part of Theorem 2. Consider a random formula F = F (n, c1, c2, c3, c4) with D > 1
and the implication process in F . Involved in the implication process, a κ-generation implication process (shortly,
κ-implication process) is deﬁned as follows. The κ-implication process consists of ordinary rounds followed by one
supplemental round. It starts from a literal chosen uniformly at random (or equivalently, the literal in a blue unit

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

193

clause). In the ﬁrst ordinary round, the unit clauses in the κth or less generations from the literal are all exposed and
the literals in them are assigned truth values. In each subsequent ordinary round, if there remain blue unit clauses, one
of them is chosen, the literal in it is set, and the literals in the unit clauses in the κth or less generations are all assigned
their truth values. Otherwise, the process is terminated. If the process proceeds by the ﬁrst n2/3 ordinary rounds, all
the literals in the existing blue unit clauses are assigned their truth values in the supplemental round and the process
is terminated.

Lemma 7. Given F = F (n, c1, c2, c3, c4) with D > 1, for some constant κ, the κ-implication process in F starting
from a variable x chosen uniformly at random proceeds to the supplemental round with positive probability.

Proof. Consider the implication process in F starting from a variable x chosen uniformly at random. A blue or red
unit clause being assigned at time t in the implication process may be regarded as if UCS sets a literal with χ(t) = 1
or 0. As in proving Lemma 2, we see that, almost always,

(cid:10)
(cid:10)
(cid:10)Fi(t)
(cid:10) =

(cid:10)
(cid:10)
(cid:10)Fi(0)
(cid:10) + o(n) = cin + o(n)

(4)
(1 (cid:3) i (cid:3) 4) uniformly for all 0 (cid:3) t (cid:3) n1−(cid:6) for arbitrarily small (cid:6) > 0. For the implication ratios in the process, let
ˆa(t) and ˆc(t) ( ˆb(t) and ˆd(t), resp.) be the numbers of blue and red unit clauses produced by a blue (red, resp.) unit
clause at time t. Recall that b(t) and r(t), the numbers of blue and red unit clauses coming from F (t) at time t in the
process of UCS, were investigated in the previous section. In fact, ˆa(t) and ˆc(t) have the same distributions as b(t)
and r(t) with χ(t) = 1, respectively. And, ˆb(t) and ˆd(t) have the same distributions as b(t) and r(t) with χ(t) = 0,
]
ˆai(t), where ˆa1(t) and ˆa2(t) have binomial distributions Bin[|F1(t)|, 1
respectively. For example, ˆa(t) =
n−t
|F4(t)|
and Bin[|F2(t)|, 2
], respectively, and ˆa3(t) and ˆa4(t) have Bernoulli distributions with densities
,
n−t
n−t
respectively. Similarly, the distributions of ˆb(t), ˆc(t), and ˆd(t) can be obtained from the results in the previous section.
Combined with Eq. (4), we see that the implication ratio matrix at time t is almost always
(cid:19)

|F3(t)|
2(n−t) and

(cid:5)
4
i=1

(cid:18)

T (t) =

c1 + 2c2 + c3+2c4
c3+2c4
2

2

c1 + 2c2
c3+2c4
2

+ o(1)

uniformly for all 1 (cid:3) t (cid:3) n1−(cid:6) .

Consider the branching process with two types whose branching ratio matrix is

A = (1 − δ)

(cid:18)

c1 + 2c2 + c3+2c4
c3+2c4
2

2

c1 + 2c2
c3+2c4
2

(cid:19)

.

The eigenvalues of A are

c1 + 2c2 + c3 + 2c4 ±

(1 − δ)

(cid:8)

(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)

2

.

Let ˆλ(δ) be the larger one. Since D > 1 implies ˆλ(0) > 1, we choose δ > 0 so that ˆλ(δ) > 1. Then, by Lemma 6, there
is a constant κ such that the expected number of the blue organisms produced after κ generations, starting from one
blue organism, is larger than one in the branching process. Note that, for sufﬁciently large n, the entries of T (t) are
larger than or equal to the corresponding entries of A for all 1 (cid:3) t (cid:3) n1−(cid:6) . By coupling the implication process with
the branching process and using Corollary 1, we see that the κ-implication process in F proceeds to the supplemental
round with positive probability. (cid:2)

Consider the κ-implication process in F starting from x chosen uniformly at random, where κ is speciﬁed as in
Lemma 7. Now consider the condition that the κ-implication process proceeds to the supplemental round. Let the
strictly distinct literals in the blue unit clauses existing in the beginning of the supplemental round be y1, . . . , yL,
where L is the number of the literals. Since the process almost always produces in expectation more than one blue
unit clause in each of the n2/3 ordinary rounds, by the large deviation result [4], L is almost always (cid:4)(n2/3). Let the
variables that have not occurred in the ordinary rounds be z1, . . . , zM , where M is the number of such variables and
almost always n + o(n). Note that D > 1 implies c1 > 0 or c2 > 0. Suppose that c1 > 0. Denote by ˆF1 the formula
consisting of the 2-clauses remaining in F1 in the beginning of the supplemental round. Then, | ˆF1| is almost always

194

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

c1n + o(n). Deﬁne the random variable Xi for 1 (cid:3) i (cid:3) M such that Xi = 1 if the unit clauses (zi) and (zi) are
produced from the clauses in ˆF1 by setting yj ’s and Xi = 0 otherwise. Deﬁne the random variable Xi,1 (Xi,2, resp.)
for 1 (cid:3) i (cid:3) M such that Xi,1 = 1 (Xi,2 = 1, resp.) if the unit clause (zi) ((zi), resp.) is produced from the clauses in
ˆF1 by setting yj ’s and Xi,1 = 0 (Xi,2 = 0, resp.) otherwise. Note that
ˆF1 consists of 2-clauses chosen uniformly at
(cid:20)s. Thus,
random among all the 2-clauses over the literals yi ’s, yi ’s, zi ’s, and zi

(cid:9)
neither (yj ∨ zi) nor (zi ∨ yj ) are not in ˆF1 for all 1 (cid:3) j (cid:3) L
Pr[Xi,1 = 0] = Pr

(cid:11)

(cid:12)

(cid:13)| ˆF1|

=

1 − 2L
U
U )| ˆF1| and Pr[Xi,1 = 0 and Xi,2 = 0] =

,

where U = (2M + 2L)(2M + 2L − 2). Similarly, Pr[Xi,2 = 0] = (1 − 2L
(1 − 4L

U )| ˆF1|. Hence,
Pr[Xi = 1] = Pr[Xi,1 = 1 and Xi,2 = 1]

(cid:12)

= 1 − Pr[Xi,1 = 0 or Xi,2 = 0]
(cid:12)
1 − 2L
U

= 1 −

(cid:13)| ˆF1|

(cid:12)
1 − 4L
U

−

2

(cid:13)| ˆF1|(cid:13)

.

(5)

Again, consider the condition that L = (cid:4)(n2/3), M = n + o(n), and | ˆF1| = c1n + o(n), which hold almost always.
(cid:3)| ˆF1|
(cid:4)
U )2 − (cid:4)(n−1) = (cid:4)(n−2/3) by the binomial expansion with
( L

M
i=1 Xi . Since E[Xi] = Pr[Xi = 1] = 8

(cid:5)

2

Let X =
Eq. (5),

E[X] = (cid:4)(M · n

−2/3) = (cid:4)(n1/3).

And, we see that Var[Xi] = E[X2
i

] − E[Xi]2 = (cid:4)(n−2/3) − (cid:4)(n−4/3) = (cid:4)(n−2/3). Since

E[XiXj ] = Pr[Xi = 1 and Xj = 1] = 1 − Pr[Xi,1 = 0 or Xi,2 = 0 or Xj,1 = 0 or Xj,2 = 0],

which is

1 −

and

(cid:12)(cid:12)
4
1

(cid:13)(cid:12)

1 − 2L
U

(cid:13)| ˆF1|

(cid:12)
4
2

−

(cid:13)(cid:12)

1 − 4L
U

(cid:13)| ˆF1|

(cid:12)

+

(cid:13)(cid:12)
4
3

1 − 6L
U

(cid:13)| ˆF1|

(cid:12)

(cid:13)(cid:12)
4
4

−

1 − 8L
U

(cid:13)| ˆF1|(cid:13)

,

E[Xi] = Pr[Xi = 1] = 1 −

2

the binomial expansion says that

(cid:12)

(cid:12)
1 − 2L
U

(cid:13)| ˆF1|

(cid:12)

−

1 − 4L
U

(cid:13)| ˆF1|(cid:13)

,

Cov[Xi, Xj ] = E[XiXj ] − E[Xi]E[Xj ] = E[XiXj ] − E[Xi]2 = −(cid:4)(n

−7/3).

Thus,

Var[X] =

M(cid:2)

i=1

Var[Xi] +

(cid:2)

i(cid:12)=j

Cov[Xi, Xj ] = (cid:4)(M · n

−2/3) − (cid:4)(M 2 · n

−7/3) = (cid:4)(n1/3).

By Chebyshev inequality [4],
Pr[X = 0] (cid:3) Var[X]
E[X]2

= (cid:4)(n

−1/3).

Hence, a 0-clause is almost always produced in the supplemental round. For the case that c1 = 0 and c2 > 0, this fact
can be obtained in a similar way by considering the equalities remaining in F2.

Now we have that the κ-implication process in F starting from a variable x chosen uniformly at random produces
a 0-clause with positive probability. Suppose that the process produces a 0-clause. In the case that F consists of
equalities only, the fact that setting x to be true produces a 0-clause implies that setting x to be true also produces
a 0-clause. In the other cases, we remove all the 2-clauses, which contain the underlying variables of the literals
occurred in the κ-implication process, from F , except for the 2-clauses that contain the literal x. Since there remain

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

195

(cid:4)(n) uniform random 2-clauses or uniform random equalities almost always, setting x to be true produces a blue
unit clause in at most two generations with positive probability and we have another κ-implication process starting
from the blue unit clause. Note that, for the new process, the distributions mentioned in the above argument almost
always do not change asymptotically. Hence, by the same argument, the new process produces a 0-clause with positive
probability. In summary, with positive probability, 0-clauses are produced from both x and x for a variable x.

In the case that the κ-implication process starting from a variable x does not produce any 0-clause, we remove
all the 2-clauses and equalities that contain the underlying variables of the literals occurred in the process. Then, we
choose one of the remaining variables uniformly at random and consider the κ-implication processes starting from
the variable and its negation. Since the expectation of the number of the exposed variables in a κ-implication process
is O(n2/3), the probability that the number of the exposed variables is greater than n3/4 is O(n−1/12) by Markov
inequality [4]. This means that, when we consider the successive κ-implication processes starting from (cid:4)(log n)
different literals, the distributions mentioned in the above argument almost always do not change asymptotically. So
the probability, that there does not occur a variable such that 0-clauses are produced from the variable and its negation
until log n variables are successively chosen, approaches to zero as n goes to inﬁnity. This means that the random
formula F is almost always unsatisﬁable.

4. Solubility of NK(n, 2, z)

In this section, we prove Theorem 1 for the model NK(n, 2, z). This is enough as NK(n, 2, z) is essentially the
√
same as NK(n, 2, z). Recall z0 = 27−7
≈ 2.837. In the ﬁrst subsection, the result for the subcritical region z < z0 is
4
proven. The next subsection is for another proof for the supercritical region. By the monotonicity of the solubility of
NK(n, 2, z), it is enough to consider cases 2 < z < z0 and z0 < z < 3.

5

4.1. Subcritical region

As in NK(n, 2, z), a 3-SAT formula F can be reduced from a random instance f of NK(n, 2, z). More precisely, a
3-SAT formula Lj is reduced from each local ﬁtness function fj of f and F is the conjunction of Lj ’s. We call Lj
a local formula. Then each local formula consists of two 3-clauses with probability 1 − h and three 3-clauses with
probability h where z = 2 + h, independently of all other local formulae. Main variables or its negations appearing
in a local formula are called main (copies of) literals of the local formula. Note that any pair of main literals from
different Lj ’s are strictly distinct. As in the generalized random 2-SAT model, we apply UCS to F and show that
UCS succeeds to ﬁnd a satisfying assignment of F with positive probability. In the process of UCS, there appear
four types of 2-clauses or equalities as presented in the generalized 2-SAT problem. Denoted by Fi(t) (1 (cid:3) i (cid:3) 4)
is the 2-SAT formula consisting of the 2-clauses or equalities of type i at time t. Denoted by F5(t) is the 3-SAT
formula consisting of the remaining local formulae at time t. Let |Fi(t)| be the number of the 2-clauses or equalities
in Fi(t) and |F5(t)| be the number of the local formulae in F5(t) at time t. Then it is clear that Fi(0) is empty for
1 (cid:3) i (cid:3) 4 and |F5(0)| = n. Let VM (t) be the set of the underlying variables of the main literals of F3(t), F4(t) and
F5(t). We consider that in the process of UCS, unit clauses consisting of main literals and the copies of literals therein
are colored red and other unit clauses and the copies of literals therein are colored blue. As in Section 3, we let B(t)
(R(t), respectively) be the set of blue (red, respectively) unit clauses at time t.

For this problem, we run UCS with

p = p(t) = p0 − t
10n

,

√

5−1
2

where p0 =
≈ 0.618. As one can see later, we deﬁned p(t) so that the expected number of blue (red, re-
spectively) unit clauses produced at each time t is uniformly bounded above by p(t) (1 − p(t), respectively) for
1 (cid:3) t (cid:3) (1 − (cid:6))n, where (cid:6) is a small constant. Then by a coupling argument and Lazy-server lemma, we obtain that
|R(t)| are O(n). Then we show that, with positive probability, no 0-clause is
the sizes of
produced until t = (1 − (cid:6))n. At t = (1 − (cid:6))n, the remaining formula is sparse enough that it is satisﬁable with positive
probability.

|B(t)| and

(1−(cid:6))n
t=0

(1−(cid:6))n
t=0

(cid:5)

(cid:5)

As in the case of generalized 2-SAT problem, it is not hard to see that at each time t, F1(t) consists of uniform
random 2-clauses over V (t), and F2(t) consists of uniform random equalities over V (t). The formula F3(t) consists

196

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

of random 2-clauses with main variables over V (t), and F4(t) consists of random equalities with main variables over
V (t), where the main literals in F3(t) and F4(t) and F5(t) are pairwise strictly distinct: For clauses and equalities in
Fi(t), i = 1, . . . , 4, one may use the same argument as in the case of the generalized 2-SAT problem. Once a variable
is set to be 0 or 1, a local formula may be reduced to 0, 1, 2 or 3 2-clauses before sublimation. If three 2-clauses are
created, then they may be further reduced to two unit clauses. (See the sublimation procedure described in Section
2.) Clearly, those unit clauses have the desired randomness by the same argument. If two 2-clauses are created, then
they are further reduced to a unit clause or an equality, but not both and, in either case, the desired randomness holds.
If one 2-clause is created, then it clearly has the randomness. The case that no 2-clause is created has no need to be
concerned.

Suppose a variable is set to be 1 or 0. For the literals of the variable appeared in the ﬁrst coordinates or in other
coordinates of clauses or equalities without main variables, the other literals are uniform random among all remaining
literals. For the literals of the variable appeared in the second or third coordinates of clauses and equalities with main
variables, we ﬁrst specify the number a of such clauses and equalities and then, conditioned on the number, all a main
variables are equally likely to be in the ﬁrst coordinate of the clauses and equalities and the other variables in such
3-clauses are uniform random.

It is not hard to see that at each time t, F (t) has the same distribution as F (n − t, c1(t), c2(t), c3(t), c4(t)), where
ci(t) = |Fi(t)|/(n − t): Suppose a variable is set to be 1 or 0. For the literals of the variable appeared in the ﬁrst
coordinates or in the second coordinates of 2-clauses or equalities without main variables, the other literals are uniform
random among all remaining literals. For the literals of the variable appeared in the second coordinates of 2-clauses
and equalities with main variables, we ﬁrst specify the number a of such 2-clauses and equalities and then, conditioned
on the number, all a main variables are equally likely to be in the ﬁrst coordinate of the 2-clauses and equalities. This
may be also illustrated using a card game described in [2].

Let bi(t) and ri(t) be the numbers of blue and red unit clauses coming from Fi(t) at time t, respectively. As
mentioned in Section 2, during the execution of UCS, there occur some sublimations. So we also consider b5(t)
(r5(t), respectively), the number of blue (red, respectively) unit clauses produced by sublimations at time t.

As in the generalized 2-SAT model, we investigate E[|Fi(t + 1)| − |Fi(t)|], E[bi(t)] and E[ri(t)] (1 (cid:3) i (cid:3) 5) and
use Wormald theorem to obtain approximations of |Fi(t)|, and then obtain approximations of E[bi(t)], and E[ri(t)].
For 1 (cid:3) i (cid:3) 4, let ui(t) be the number of 2-clauses or equalities that come from F5(t) to Fi(t) at time t, and di(t) be the
number of 2-clauses or equalities that are removed from Fi(t) at time t. Then for 1 (cid:3) i (cid:3) 4, E[|Fi(t + 1)| − |Fi(t)|] =
E[ui(t)] − E[di(t)]. As we already obtained E[di(t)], E[bi(t)] and E[ri(t)] (1 (cid:3) i (cid:3) 4) in Section 3, we only need to
obtain E[ui(t)] (1 (cid:3) i (cid:3) 4), E[b5(t)], E[r5(t)] and E[|F5(t + 1)| − |F5(t)|].

Lemma 8. For any small (cid:6) > 0, we have for all 0 (cid:3) t (cid:3) (1 − (cid:6))n,

(cid:10)
(cid:9)(cid:10)
(cid:10) −
(cid:10)F1(t + 1)
(cid:9)(cid:10)
(cid:10)
(cid:10)F2(t + 1)
(cid:10) −
(cid:10)
(cid:9)(cid:10)
(cid:10) −
(cid:10)F3(t + 1)
(cid:9)(cid:10)
(cid:10)
(cid:10)F4(t + 1)
(cid:10) −
(cid:9)(cid:10)
(cid:10)
(cid:10)F5(t + 1)
(cid:10) −

E

E

E

E

E

(cid:11)

(cid:11)

(cid:11)

(cid:11)

(cid:11)

(cid:10)
(cid:10)
(cid:10)F1(t)
(cid:10)
(cid:10)
(cid:10)
(cid:10)F2(t)
(cid:10)
(cid:10)
(cid:10)
(cid:10)F3(t)
(cid:10)
(cid:10)
(cid:10)
(cid:10)F4(t)
(cid:10)
(cid:10)
(cid:10)
(cid:10)F5(t)
(cid:10)

= − 2|F1(t)|
n − t
= − 2|F2(t)|
n − t

+ p(t)

(cid:12)

+ p(t)

= −

= −

= −

(cid:3)
1 + p(t)

(cid:3)
1 + p(t)

(cid:3)
2 + p(t)

(cid:4) |F3(t)|
n − t
(cid:4) |F4(t)|
n − t
(cid:4) |F5(t)|
n − t

(cid:12)

(cid:13)

4
7
1
14

+

,

,

|F5(t)|
n − t
(cid:13)
|F5(t)|
n − t
(cid:13)

− h
7
+ h
14
(cid:12)
− 2h
8
7
7
(cid:13)
+ h
1
7
7

(cid:12)

|F5(t)|
n − t
|F5(t)|
n − t

+

+ o(1).

+ o(1),

+ o(1),

Proof. Let L be a local formula of F5(t). Recall that the probability that L is a conjunction of two (or three) 3-clauses
is 1 − h (or h) independently of all other local formulae. Denoted by |L| is the number of 3-clauses of L. We write
m(L) for the main variable of L. Suppose a literal l is set to be true at time t. For convenience, we deﬁne an index
I (L, l) to be an ordered pair (a, b), where a = 1(2, respectively) means that l and the main (a neighborhood) variable
of L are not strictly distinct, and b indicates how many copies of l appears in the 3-clauses of L. For example,
I (L, l) = (1, 2) means that m(L) and l are not strictly distinct and l appears in two 3-clauses of L.

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

197

Now we obtain E[u1(t)]. Note that one uniform random 2-clause is produced from L only when |L| = 2, χ(t) =
1, I (L, l) = (1, 1) or |L| = 3, χ(t) = 1, I (L, l) = (1, 2). So the corresponding expected numbers of new uniform
random 2-clauses that come from F5(t) are

(1 − h) × p(t) ×

|F5(t)|
n − t

× 4 · 4
28

and h × p(t) ×

|F5(t)|
n − t

×

and hence

(cid:9)
(cid:11)
u1(t)
E

(cid:12)

= p(t)

(cid:13)

|F5(t)|
n − t

.

4
7

− h
7

(cid:4)

(cid:3)
4
4
2
56

,

Similarly, contributions for u2(t) from L are made only when |L| = 2, χ = 1, I (L, l) = (1, 0) or |L| = 3, χ =
1, I (L, l) = (1, 1). So the corresponding expected numbers of new uniform random equalities that come from F5(t)
are

(1 − h) × p(t) ×

|F5(t)|
n − t

× 2
28

and h × p(t) ×

|F5(t)|
n − t

× 2 · 4
56

,

and hence

(cid:9)
(cid:11)
u2(t)
E

(cid:12)

= p(t)

1
14

+ h
14

(cid:13)

|F5(t)|
n − t

.

For u3(t), the cases are |L| = 2, χ = 1, I (L, l) = (2, 1) and |L| = 2, χ = 0, I (L, l) = (2, 1) and |L| = 3, χ =
1, I (L, l) = (2, 2) and |L| = 3, χ = 0, I (L, l) = (2, 2). The corresponding expected numbers of new random
2-clauses with main literals that come from F5(t) are

(1 − h) × p(t) × 2|F5(t)|
n − t

× 4 · 4
28

,

(1 − h) ×

(cid:4)
(cid:3)
1 − p(t)

× 2|F5(t)|
n − t − 1

× 4 · 4
28

and

h × p(t) × 2|F5(t)|
n − t

×

(cid:4)

4 ·

(cid:3)
4
2
56

,

(cid:4)
(cid:3)
1 − p(t)

h ×

× 2|F5(t)|
n − t − 1

×

4 ·

(cid:3)
(cid:4)
4
2
56

,

and hence

(cid:9)
(cid:11)
u3(t)
E

=

(cid:12)

8
7

− 2h
7

(cid:13)

|F5(t)|
n − t

+ o(1).

For u4(t), the cases are |L| = 2, χ = 1, I (L, l) = (2, 0)) and |L| = 2, χ = 0, I (L, l) = (2, 0) and |L| = 3, χ = 1,

I (L, l) = (2, 1) and |L| = 3, χ = 0, I (L, l) = (2, 1). The corresponding expected numbers are

(1 − h) × p(t) × 2|F5(t)|
n − t

× 2
28

,

(1 − h) ×

(cid:4)
(cid:3)
1 − p(t)

× 2|F5(t)|
n − t − 1

× 2
28

and

h × p(t) × 2|F5(t)|
n − t

× 2 · 4
56

,

(cid:4)
(cid:3)
1 − p(t)

h ×

× 2|F5(t)|
n − t − 1

× 2 · 4
56

,

and hence

(cid:9)
(cid:11)
u4(t)
E

=

(cid:12)

(cid:13)

|F5(t)|
n − t

1
7

− h
7

+ o(1).

Similarly,
(cid:11)
(cid:9)
b5(t)
E

=

(cid:9)
(cid:11)
r5(t)
E

=

(cid:12)
(cid:4)
(cid:3)
1 + p(t)
(cid:12)

(cid:13)

1
7

+ 2h
7

+ 2h
1
7
7
|F5(t)|
n − t

(cid:13)

|F5(t)|
n − t

+ o(1),

+ o(1),

and

(cid:9)(cid:10)
(cid:10)
(cid:10)F5(t + 1)
(cid:10) −

E

(cid:11)

(cid:10)
(cid:10)
(cid:10)F5(t)
(cid:10)

= −

(cid:3)
2 + p(t)

(cid:4) |F5(t)|
n − t

+ o(1).

(cid:2)

198

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

For any small constant (cid:6) > 0 we can apply Wormald’s theorem to approximate |Fi(t)| uniformly for 1 (cid:3) t (cid:3)

(1 − (cid:6))n. For the solution ϕi(x) : [0, 1 − (cid:6)] → R of the following system of differential equations,
ϕ5(x)
1−x
(cid:4)

⎧

(cid:4)

⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩

dϕ1
dx
dϕ2
dx
dϕ3
dx
dϕ4
dx
dϕ5
dx

= − 2ϕ1(x)
1−x
= − 2ϕ2(x)
1−x

(cid:3)
+ (p0 − 0.1x)
(cid:3)
+ (p0 − 0.1x)
= −(1 + p0 − 0.1x) ϕ3(x)
1−x
= −(1 + p0 − 0.1x) ϕ4(x)
1−x
= −(2 + p0 − 0.1x) ϕ5(x)
1−x

4
7
1
14
+

+

− h
7
+ h
14
(cid:3)
− 2h
7
(cid:4)
+ h
7

8
7
1
7

(cid:3)

ϕ5(x)
1−x
(cid:4)

ϕ5(x)
1−x
ϕ5(x)
1−x

ϕ1(0) = 0,
ϕ2(0) = 0,
ϕ3(0) = 0,
ϕ4(0) = 0,
ϕ5(0) = 1,

almost always
(cid:10)
(cid:10)
(cid:10)Fi(t)
(cid:10) = ϕi

(cid:12)

(cid:13)

t
n

· n + o(n)

uniformly for all 1 (cid:3) t (cid:3) (1 − (cid:6))n and 1 (cid:3) i (cid:3) 5.

(cid:5)
5
i=1

ϕi (1−(cid:6))

Now we choose (cid:6) > 0 so that

(cid:6) < 0.01, i.e., the total number of 2-clauses, equalities, and local formulae

remaining at time t = (1 − (cid:6))n is almost always less than 0.01(cid:6)n.

Lemma 9. There exists a constant δ > 0 such that, E[b(t)] < p(t) − δ and E[r(t)] < 1 − p(t) − δ, almost always and
uniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n.

Proof. The expectations of bi(t) and ri(t) are as follows.
(cid:18)

(cid:18)

(cid:19)

(cid:19)

E[bi(t)]
E[ri(t)]

= Ti(t) ·

p(t)
1 − p(t)

+ o(1),

where

T1(t) =

T3(t) =

(cid:19)

,

T2(t) =

,

(cid:18)

T4(t) =

(cid:19)

(cid:18)

(cid:18)

1 1
0 0

(cid:19)

|F1(t)|
(n − t)
|F3(t)|
(n − t)

( 1
7

1
0
2
1
1
2
2
+ 2h
7 )|F5(t)|
(n − t)

|F2(t)|
(n − t)
|F4(t)|
(n − t)

(cid:18)

(cid:19)

2 2
0 0
(cid:18)

,

(cid:19)

1 0
1 1

,

Let b(t) =
(cid:18)

T5(t) =
(cid:5)
5
i=1 bi(t) and r(t) =
(cid:19)
p(t)
1 − p(t)

E[b(t)]
E[r(t)]

= T (t) ·

(cid:18)

.

2 1
1 1
(cid:5)
5
i=1 ri(t). Then for T (t) =
(cid:19)

(cid:5)
5
i=1 Ti(t),

+ o(1).

So

(cid:18)

E[b(t)] = 1
n − t

+ p(t)
(cid:12)

= 1
n − t

(cid:9)
E

(cid:11)
r(t)

(cid:12)

(cid:13)

(cid:10)
(cid:10)
(cid:10) + 2
(cid:10)F1(t)
(cid:12)

+

|F3(t)|
2
(cid:10)
(cid:10)
(cid:10) + 2
(cid:10)F3(t)

(cid:10)
(cid:10)
(cid:10)F2(t)
(cid:10) +
(cid:10)
(cid:10)
(cid:10) +
(cid:10)F4(t)
(cid:12)
(cid:10)
(cid:10)
(cid:10)F4(t)
(cid:10) +

1
7
(cid:12)
1
7
1
7

(cid:10)
(cid:10)
+ 2h
(cid:10)
(cid:10)F5(t)
7
(cid:13)
(cid:10)
(cid:10)
+ 2h
(cid:10)
(cid:10)F5(t)
7
(cid:13)
(cid:10)
(cid:10)
+ 2h
(cid:10)
(cid:10)F5(t)
7

(cid:13)

(cid:13)(cid:19)

+ o(1),

+ o(1).

(6)

Note that

(cid:18)

p0
1 − p0

(cid:20)

(cid:19)

=

(cid:21)

√

(

5−1)
2
√
1 − (

5−1)
2

is an eigenvector of

(cid:12)

T (0) =

1
7

+ 2h
7

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

199

(cid:13) (cid:18)

(cid:19)

2 1
1 1

and the corresponding eigenvalue is less than one if and only if z < z0.

From (6), by letting
(cid:12)
ϕb(x) = 1
1 − x

ϕ1(x) + 2ϕ2(x) +

and

(cid:12)

ϕr (x) = 1
1 − x

ϕ3(x) + 2ϕ4(x) +

(cid:12)

(cid:12)

1
7

+ 2h
7

(cid:13)

ϕ5(x) + p0 − 0.1x

2

(cid:12)

ϕ3(x) + 2ϕ4(x) +

(cid:12)

1
7

+ 2h
7

(cid:13)

(cid:13)(cid:13)

ϕ5(x)

(cid:13)

(cid:13)

ϕ5(x)

,

1
7

+ 2h
7

we obtain that E[b(t)] = ϕb(t/n) + o(1) and E[r(t)] = ϕr (t/n) + o(1). So, if we show that ϕb(x) < p0 − 0.1x and
ϕr (x) < 1 − (p0 − 0.1x) for 0 (cid:3) x < 1, we obtain the required result. By inserting the formulae for dϕi
dx ’s and by
the fact that ϕi(x)’s are nonnegative, we obtain that ϕb(0) < p0, ϕ(cid:20)
b (x) < 0 for 0 (cid:3) x < 1. So
ϕb(x) < p0 − 0.1x. Similarly, ϕr (0) < 1 − p0, ϕ(cid:20)
r (0) < 0.1 and ϕ(cid:20)(cid:20)
r (x) < 0. So ϕr (x) < 1 − (p0 − 0.1x). It is worth
noting that intuitively we have deﬁned p(t) so that p(nx) is a line between the curves ϕb(x) and 1 − ϕr (x), using the
facts that ϕb(x) and ϕr (x) are convex. (cid:2)

b(0) < −0.1 and ϕ(cid:20)(cid:20)

Now as in the generalized random 2-SAT model with D < 1 and c3 + c4 = 1, for some small constant δ > 0, at each
step t of the ﬁrst δn steps |V (t) − VM (t)| increases in average. Hence by the same argument, with positive probability
UCS proceeds to the ﬁrst δn steps without encountering χ(t) = 0 and V (t) − VM (t) = ∅, and without producing any
0-clauses.

Note that |V (t)| = n − t and almost always, |VM (t)| = |F3(t)| + |F4(t)| + |F5(t)| = n(ϕ3(t/n) + ϕ4(t/n) +
ϕ5(t/n)) + o(n). And we can obtain that for all 0 < x < 1 − (cid:6), ϕ3(x) + ϕ4(x) + ϕ5(x) < 1 − x, by using the formulae
for dϕi
dx ’s (i = 3, 4, 5) and the fact that ϕi(x)’s are nonnegative. So under the condition that UCS proceeds to the ﬁrst
δn steps, almost always UCS does not encounter the case that χ(t) = 0 and V (t) − VM (t) = ∅ for δn (cid:3) t (cid:3) (1 − (cid:6))n.
Then as in the generalized random 2-SAT model with D < 1, using the coupling argument and the Lazy-server
(cid:5)
|R(t)| are bounded by O(n). Now we show that
lemma, we obtain that almost always
with positive probability no 0-clause is produced until t = (1 − (cid:6))n. Under the condition that no 0-clause is produced
until time t − 1, the probability that the same holds until time t is at least

|B(t)| and

(1−(cid:6))n
t=1

(1−(cid:6))n
t=1

(cid:5)

(cid:12)

1 −

1
2(n − t − 1)

(cid:13)|B(t)|(cid:12)

1 −

|R(t)|
2(n − t − 1)

(cid:13)

(cid:12)

(cid:2)

1 − 2
(cid:6)n

(cid:13)|B(t)|+|R(t)|

.

(7)

Hence, the probability that no 0-clause is produced until t = (1 − (cid:6))n is at least
(cid:12)
1 − 2
(cid:6)n

(cid:12)
1 − 2
(cid:6)n

n−(cid:6)n
t=0 (|B(t)|+|R(t)|)

(cid:6) + o(1),

= e

(cid:13)(cid:5)

− 2C

(cid:2)

Cn

(cid:13)

which is a positive constant.

Now we only need to show that under the condition that no 0-clause is produced until t = (1 − (cid:6))n, the remaining
formula at time t = (1 − (cid:6))n is satisﬁable with positive probability. Remind that from our choice of (cid:6), the total number
of 2-clauses, equalities, and local formulae remaining at time t = (1 − (cid:6))n is almost always less than 0.01(cid:6)n. For each
local formula remaining as in the original form at time t = (1 − (cid:6))n, we consider conjunction of 2-clauses obtained
by removing the main literals of the local formula. Then, apply the sublimation or replace two 2-clauses like (l1 ∨ l2),
(l1 ∨ l2) by the equality l1 = l2, but not both. (It is easy to check that only one of the sublimation and the replacement
is needed.) As in the proof of the generalized random 2-SAT model with D < 1, we think of the following multi-graph
G. The vertices represent the Boolean variables in V ((1 − (cid:6))n). For each 2-clause or equality at time t = (1 − (cid:6))n
including equality reduced from the local formula, we set an edge between the two variables that appear in the 2-clause
or equality. Notice that if G is acyclic, and no pair of unit clauses at time t = (1 − (cid:6))n belongs to the same connected
component of G, then there exists a truth assignment of V ((1 − (cid:6))n) which satisﬁes the remaining formula at time
t = (1 − (cid:6))n. The same argument used to analyze the generalized random 2-SAT model with D < 1 yields the desired
events occur simultaneously with positive probability.

200

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

4.2. Supercritical region

As in the previous section, let F be the 3-SAT formula reduced from a random instance f of NK(n, 2, z). For
z0 < z < 3, local formulae in F consist of two 3-clauses or three 3-clauses. A local formula with two 3-clauses
may have the form of (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3), which is equivalent to the 2-clause (l2 ∨ l3). According to the
position of the negation, the 2-clause is a uniform random 2-clause or a random 2-clause with main literal. A local
formula with three 3-clauses may have the form of (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3), which is equivalent to
(l1 ∨ l2) ∧ (l1 ∨ l3). We call such a conjunction of two 2-clauses a cherry as the shape of the relations among literals
looks like it. According to the positions of the negations, the literal appearing twice in a cherry may be a main literal
or not. If it is a main literal, the cherry is called a symmetric cherry. Otherwise, it is called an asymmetric cherry.
A local formula with three 3-clauses may have the form of (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3), which implies
the 2-clause (l2 ∨ l3). (The converse does not hold.) The 2-clause is a uniform random 2-clause or a random 2-clause
with main literal according to the positions of the negations.

This way, we resolve the local formulae in F into four types of 2-clauses and cherries: uniform random 2-clauses,
asymmetric cherries, random 2-clauses with main literals, and symmetric cherries. Note that the unsatisﬁability of the
2-SAT formula consisting of the 2-clauses and cherries obtained from the resolution implies the unsatisﬁability of F .
In fact, we prove the unsatisﬁability of F by showing the unsatisﬁability of the 2-SAT formula. To do this, we deﬁne
a generalized random 2-SAT formula and examine its satisﬁability. The second part of Theorem 1 is obtained as a
corollary.

The generalized random 2-SAT formula has four types of random 2-clauses and cherries. The ﬁrst type consists of
uniform random 2-clauses. The second type consists of random asymmetric cherries of the form (ri ∨ui1)∧(ui1 ∨ui2),
where ui1 and ui2 being strictly distinct with ri are chosen uniformly at random. The third type consists of random
2-clauses with main literals. The fourth type consists of random symmetric cherries of the form (ri ∨ ui1) ∧ (ri ∨ ui2),
where ui1 and ui2 being strictly distinct with ri are chosen uniformly at random. The copies of the literals ri ’s in
cherries and the literals in the ﬁrst places in random 2-clauses with main literals are pairwise strictly distinct and
they are called main literals in the random 2-SAT formula. Let c1, c2, c3 and c4 be nonnegative real numbers with
c2 + c3 + c4 (cid:3) 1. Denote Fi = Fi(n, ci) the conjunction of cin 2-clauses or cherries of type i (1 (cid:3) i (cid:3) 4). Denoted
by F (n, c1, c2, c3, c4) is the conjunction of the four random formulae with pairwise strictly distinct main literals. The
parameter

D = (c1 + c2) + (c2 + c3 + 2c4) − (c2 + c3 + 2c4)2

4

essentially determines the branching ratio in F (n, c1, c2, c3, c4). The satisﬁability of F (n, c1, c2, c3, c4) can be de-
scribed in terms of D as follows.

Theorem 4. If D < 1, then there exists α > 0 depending on ci ’s so that the probability of F (n, c1, c2, c3, c4) being
satisﬁable is at least α as n goes to inﬁnity. If D > 1, then the random formula is almost always unsatisﬁable.

Theorem 4 can be proved in a similar way as the proof of Theorem 2. We present the proof for the second part of

Theorem 4 in Appendix for the completeness of the proof in this section.

Now we prove the second part of Theorem 1. As mentioned above, a local formula in F is resolved into a uniform
random 2-clause (l2 ∨l3) when it has the form of (l1 ∨l2 ∨l3)∧(l1 ∨l2 ∨l3) or (l1 ∨l2 ∨l3)∧(l1 ∨l2 ∨l3)∧(l1 ∨l2 ∨l3).
If we let z = 2 + h (z0 − 2 < h < 1), this takes place with probability 1
7 . So, the probability that a
local formula is resolved into a uniform random 2-clause is 1
7 . In a similar way, we see the following: The probability
of a local formula being resolved into an asymmetric cherry is 2
7 h, the probability for a 2-clause with main literal
is 2
7 h. Then, the expected numbers of uniform random 2-clauses,
asymmetric cherries, 2-clauses with main literals, and symmetric cherries obtained from F are 1
7 n, and
1
7 hn, respectively.

7 , and the probability for a symmetric cherry is 1

7 (1 − h) = 1

Since each local formula is created independently of other local formulae, the numbers of 2-clauses and cherries
obtained from F are highly concentrated around their expectations. Let c1 = 1
7 h, and
ci(δ) = ci − δ for 1 (cid:3) i (cid:3) 4. Then, by the large deviation result [4], the numbers of 2-clauses and cherries of four

7 h, c3 = 2

7 , c4 = 1

7 , c2 = 2

7 h + 1

7 hn, 2

7 n, 2

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

201

types are almost always larger than c1(δ)n, c2(δ)n, c3(δ)n, and c4(δ)n, respectively, for arbitrarily small δ > 0. Since
z0 − 2 < h < 1 implies that c2 + c3 + c4 (cid:3) 1 and D > 1, we may choose δ > 0 so that c2(δ) + c3(δ) + c4(δ) (cid:3) 1 and

(cid:3)
(cid:4)
c1(δ) + c2(δ)

(cid:4)
(cid:3)
c2(δ) + c3(δ) + 2c4(δ)

+

− (c2(δ) + c3(δ) + 2c4(δ))2
4

> 1.

Then, the second part of Theorem 4 implies that the random 2-SAT formula resolved from F is almost always unsat-
isﬁable and the proof completes.

In that the 2-SAT formula resolved from F is exploited in the proof, our approach is similar to that of Gao and
Culberson [21]. However, they counted the number of unsatisﬁable subformulae in the resolved 2-SAT formula, which
leads to complex computation. We used branching process arguments to derive the proof in more intuitive and natural
way.

5. Conclusion and remarks

In this paper, we analyzed the phase transition in NK landscape on the ﬁxed ratio model, NK(n, 2, z). We also
proposed a generalized random 2-SAT model and introduced a corresponding parameter D. Then a phase transition
result for the model is obtained, that is, if D < 1, the formula is satisﬁable with positive probability, and if D > 1,
the formula is almost always unsatisﬁable. For the proof of the subcritical region, we provided a variant of the unit
clause algorithm, the unit clause algorithm with switching server policy, and analyzed it. For the supercritical region,
a branching process argument was used.

Using a similar argument as in the generalized random 2-SAT model, it was proved that a random instance gener-
√
ated by NK(n, 2, z) with z < z0 = 27−7
is soluble with positive probability. To the best of our knowledge, this is
4
the ﬁrst mathematical result that describes the behavior of NK(n, 2, z) with z < z0. We also present an another way
to prove that a random instance generated by NK(n, 2, z) with z > z0 is almost always insoluble using a branching
process argument. This approach is a novel one and is simpler than that of Gao and Culberson. From these results, we
established the threshold value, z0, of the phase transition in NK(n, 2, z).

We believe that our approach used for NK(n, k, z) with k = 2 works for general k (cid:2) 3 to obtain at least partial

5

results for the phase transition phenomenon.

It is expected that NK(n, k, z) with z < z0 is soluble with probability close to 1. One of typical ways to prove such
a result is to use the sharp threshold criterion developed by Friedgut [44]. If the criterion is satisﬁed, then our theorem
would have implied the desired result. Though we have tried to show that our case satisﬁes the criterion, we are unable
to prove it. A nontrivial idea seems to be required for a rigorous proof.

Appendix A. The proof of the second part of Theorem 4

(cid:5)

Here, we prove the second part of Theorem 4. Consider a random formula F = F (n, c1, c2, c3, c4) with D > 1 and
the implication process in F starting from a literal x chosen uniformly at random. For 1 (cid:3) i (cid:3) 4, let Fi(t) denote
the conjunction of remaining 2-clauses or cherries of Fi at time t. Deﬁne |Fi(t)| to be the number of 2-clauses or
cherries in Fi(t). Let F (t) = F1(t) ∧ F2(t) ∧ F3(t) ∧ F4(t). We ﬁrst investigate the implication ratios in the process
conditioned on Fi(t)’s. Let a(t) and c(t) (b(t) and d(t), resp.) be the numbers of blue and red unit clauses produced
by a blue (red, resp.) unit clause at time t. For 1 (cid:3) i (cid:3) 4, let bi,b(t) and ri,b(t) (bi,r (t) and ri,r (t), resp.) be the numbers
of blue and red unit clauses produced from Fi(t) at time t by a blue (red, resp.) unit clause. Then, a(t) =
bi,b(t),
c(t) =
ri,r (t).

bi,r (t), and d(t) =

ri,b(t), b(t) =

For bi,b(t)’s, we see that b1,b(t) has a binomial distribution Bin[|F1(t)|, 1
n−t

of a random variable with a Bernoulli distribution with density
distribution Bin[|F2(t)|, 1
n−t
random variable that has a Bernoulli distribution with density
and r4,b(t) have binomial distributions Bin[|F2(t)|,
For bi,r (t)’s, b1,r (t) and b2,r (t) have binomial distributions Bin[|F1(t)|, 1
n−t

], b3,b(t) has a Bernoulli distribution with density

], Bin[|F3(t)|,

1
2(n−t)

1
2(n−t)

], b2,b(t) is a linear combination
|F2(t)|
2(n−t) and a random variable with a binomial
|F3(t)|
2(n−t) , and b4,b(t) is two times a
|F4(t)|
2(n−t) . For ri,b(t)’s, r1,b(t) = 0 and r2,b(t), r3,b(t),
], respectively.
], and Bin[|F4(t)|, 1
n−t
], respectively,
1
n−t−1

] and Bin[|F2(t)|,

(cid:5)

(cid:5)

(cid:5)

202

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

and b3,r (t) = b4,r (t) = 0. For ri,r (t)’s, r1,r (t) = 0 and r2,r (t), r3,r (t), and r4,r (t) have binomial distributions
Bin[|F2(t)|,

], and Bin[|F4(t)|,

], Bin[|F3(t)|,

], respectively.

1
2(n−t−1)

1
2(n−t−1)

Using similar arguments as in the proof of Theorem 2, we see that the implication ratio matrix at time t is almost

1
n−t−1

always

(cid:18)

T (t) =

c1 + c2 + c2+c3+2c4
c2+c3+2c4
2

2

c1 + c2
c2+c3+2c4
2

(cid:19)

+ o(1)

uniformly for all 1 (cid:3) t (cid:3) n1−(cid:6) for arbitrarily small (cid:6) > 0. Consider the branching process with two types whose
branching ratio matrix is

(cid:18)

A = (1 − δ)

c1 + c2 + c2+c3+2c4
c2+c3+2c4
2

2

c1 + c2
c2+c3+2c4
2

(cid:19)

.

The eigenvalues of A are

c1 + 2c2 + c3 + 2c4 ±

(1 − δ)

(cid:8)

(c1 + c2)2 + 2(c1 + c2)(c2 + c3 + 2c4)

2

.

Let λ(δ) be the larger one. Since D > 1 implies λ(0) > 1, we choose δ > 0 so that λ(δ) > 1. Then, by coupling
the implication process with the branching process and using Corollary 1, we see that, for some constant κ, the
κ-implication process starting from x proceeds to the supplemental round with positive probability. Conditioned that
the κ-implication process proceeds to the supplemental round, the second moment method says that a 0-clause is
almost always produced in the supplemental round. Hence, we have that the κ-implication process starting from
x produces a 0-clause with positive probability. By considering successive κ-implication processes starting from
(cid:4)(log n) different literals, we can show that a 0-clause is almost always produced from F in the same way as the
proof of Theorem 2.

References

[1] D. Achlioptas, Setting two variables at a time yields a new lower bound for random 3-SAT, in: Proceedings of the 32nd Annual ACM

Symposium on Theory of Computing, 2000, pp. 28–37.

[2] D. Achlioptas, Lower bounds for random 3-SAT via differential equations, Theoretical Computer Science 265 (1–2) (2001) 159–185.
[3] D. Achlioptas, G.B. Sorkin, Optimal myopic algorithms for random 3-SAT, in: Proceedings of the 41st Symposium on the Foundations of

Computer Science, 2000, pp. 590–600.

[4] N. Alon, J. Spencer, The Probabilistic Method, Wiley, New York, 1992.
[5] L. Altenberg, NK ﬁtness landscapes, in: T. Bäck, D. Fogel, Z. Michalewicz (Eds.), Handbook of Evolutionary Computation, Oxford University

Press, 1997.

[6] C. Amitrano, L. Peliti, M. Saber, Population dynamics in a spin-glass model of chemical evolution, Journal of Molecular Evolution 29 (1989)

513–525.

[7] B. Bollobás, C. Borgs, J. Chayes, J.H. Kim, D.B. Wilson, The scaling window of the 2-SAT transition, Random Structures and Algorithms 18

(2001) 201–256.

[8] M.T. Chao, J. Franco, Probabilistic analysis of two heuristics for the 3-satisﬁability problem, SIAM Journal on Computing 15 (4) (1986)

1106–1118.

[9] M.T. Chao, J. Franco, Probabilistic analysis of a generalization of the unit-clause literal selection heuristics for the k-satisﬁability problem,

Information Science 51 (3) (1990) 289–314.

[10] V. Chvátal, B. Reed, Mick gets some (the odds are on his side), in: Proceedings of the 33th Annual Symposium on Foundations of Computer

Science, 1992, pp. 620—627.

[11] W. Fernandez de la Vega, On random 2-SAT, Unpublished manuscript, 1992.
[12] O. Dubois, Y. Boufkhad, J. Mandler, Typical random 3-SAT formulae and the satisﬁability threshold, in: Proceedings of the 11th Annual

ACM–SIAM Symposium on Discrete Algorithms, 2000, pp. 126–127.

[13] R. Durrett, V. Limic, Rigorous results for the NK model, Annals of Probability 31 (4) (2003) 1713–1753.
[14] M. Eigen, J. McCaskill, P. Schuster, The molecular quasispecies, Advanced Chemical Physics 75 (1989) 149–263.
[15] P. Erd˝os, A. Rényi, On the evolution of random graphs, in: Publication of the Mathematical Institute of the Hungarian Academy of Science,

1960, pp. 17–61.

[16] S.N. Evans, D. Steinsaltz, Estimating some features of NK ﬁtness landscapes, Annals of Applied Probability 12 (2002) 1299–1321.
[17] W. Ewens, Mathematical Population Genetics, Springer-Verlag, 1979.
[18] H. Flyvbjerg, B. Lautrup, Evolution in a rugged ﬁtness landscape, Physical Review A 46 (1992) 6714–6723.
[19] W. Fontana, P.F. Stadier, E.G. Bornberg-Bauer, T. Griesmacher, I.L. Hofacker, M. Tacker, P. Tarazona, E.D. Weinberger, P. Schuster, RNA

folding and combinatory landscapes, Physical Review E 47 (1993) 2083–2099.

S.-S. Choi et al. / Artiﬁcial Intelligence 172 (2008) 179–203

203

[20] I. Franklin, R. Lewontin, Is the gene the unit of selection? Genetics 65 (1970) 707–734.
[21] Y. Gao, J. Culberson, An analysis of phase transition in NK landscapes, Journal of Artiﬁcial Intelligence Research 17 (2002) 309–332.
[22] Y. Gao, J. Culberson, On the treewidth of NK landscapes, in: Proceedings of the Genetic and Evolutionary Computation Conference, 2003,

pp. 948–954.

[23] A. Goerdt, A threshold for unsatisﬁability, Journal of Computer and System Sciences 53 (3) (1996) 469–486.
[24] W. Hordijk, A measure of landscapes, Evolutionary Computation 4 (4) (1997) 335–360.
[25] S. Janson, Y.C. Stamatiou, M. Vamvakari, Bounding the unsatisﬁability threshold of random 3-SAT, Random Structures and Algorithms 17

(2000) 103–116.

[26] T. Jones, S. Forrest, Fitness distance correlation as a measure of problem difﬁculty for genetic algorithms, in: Proceedings of the Sixth

International Conference on Genetic Algorithms, 1995, pp. 184–192.

[27] A. Kaporis, L. Kirousis, Y. Stamatiou, M. Vamvakari, M. Zito, The unsatisﬁability threshold revisited, submitted for publication.
[28] S.A. Kauffman, Adaptation on rugged ﬁtness landscapes, in: D. Stein (Ed.), Lectures in the Sciences of Complexity, Addison Wesley, 1989,

pp. 527–618. Santa Fe Institute Studies in the Sciences of Complexity.

[29] S.A. Kauffman, S. Levin, Towards a general theory of adaptive walks on rugged landscapes, Journal of Theoretical Biology 128 (1987) 11–45.
[30] S.A. Kauffman, E.D. Weinberger, A.S. Perelson, Maturation of the immune response via adaptive walks on afﬁnity landscapes, in: A.S. Perel-

son (Ed.), Theoretical Immunology I, Addison Wesley, 1988. Santa Fe Institute Studies in the Sciences of Complexity.

[31] H. Kaul, S.H. Jacobson, Global optima results for the Kauffman NK model, Mathematical Programming 106 (2) (2006) 319–338.
[32] H. Kaul, S.H. Jacobson, New global optima results for the Kauffman NK model: Handling dependency, Mathematical Programming (2006),

in press.

[33] D.A. Levinthal, Adaptation on rugged landscapes, Management Science 43 (1997) 934–950.
[34] R. Lewontin, The Genetic Basis of Evolutionary Change, Columbia University Press, 1974.
[35] C.A. Macken, A.S. Perelson, Protein evolution on rugged landscapes, Proceedings of the National Academic of Science, USA 86 (1989)

6191–6195.

[36] P. Merz, B. Freisleben, On the effectiveness of evolutionary search in high-dimensional NK-landscapes, in: Proceedings of the IEEE Interna-

tional Conference on Evolutionary Computation, 1998, pp. 741–745.

[37] P. Schuster, P.F. Stadler, Landscapes: Complex optimization problems and biopolymer structures, Computational Chemistry 18 (1994) 295–

324.

[38] D.I. Seo, Y.H. Kim, B.R. Moon, New entropy-based measures of gene signiﬁcance and epistasis, in: Proceedings of the Genetic and Evolu-

tionary Computation Conference, 2003, pp. 1345–1356.

[39] B. Skellet, B. Cairns, N. Geard, B. Tonkes, J. Wiles, Maximally rugged NK landscapes contain the highest peaks, in: Proceedings of the

Genetic and Evolutionary Computation Conference, 2005, pp. 579–584.

[40] T. Smith, P. Husbands, P. Layzell, M. O’Shea, Fitness landscapes and evolvability, Evolutionary Computation 10 (1) (2002) 1–34.
[41] E.D. Weinberger, A more rigorous derivation of some properties of uncorrelated ﬁtness landscapes, Journal of Theoretical Biology 134 (1988)

125–129.

[42] E.D. Weinberger, Local properties of Kauffman’s NK model, a tuneably rugged energy landscape, Physical Review A 44 (10) (1991) 6399–

6413.

[43] E.D. Weinberger, NP completeness of Kauffman’s NK model, a tuneably rugged ﬁtness landscape, Technical Report 96-02-003, Santa Fe

Institute, Santa Fe, 1996.

[44] E. Friedgut, Sharp thresholds of graph properties, and the k-SAT problem, Journal of the American Mathematical Society 12 (1999) 1017–

1054 (with appendix by J. Bourgain).

[45] N.C. Wormald, Differential equations for random processes and random graphs, Annals of Applied Probability 5 (4) (1995) 1217–1235.
[46] A.H. Wright, R.K. Thompson, J. Zhang, The computational complexity of NK ﬁtness functions, IEEE Transactions on Evolutionary Compu-

tation 4 (4) (2000) 373–379.

[47] S. Wright, The roles of mutation, inbreeding, crossbreeding, and selection in evolution, in: Proceedings of the Sixth International Congress on

Genetics, vol. 1, 1932, pp. 356–366.

