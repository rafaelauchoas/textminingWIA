Artiﬁcial Intelligence 166 (2005) 81–139

www.elsevier.com/locate/artint

Knowledge and communication:
A ﬁrst-order theory ✩

Ernest Davis

Courant Institute, New York University, New York, NY 10012, USA

Received 1 July 2004; accepted 2 May 2005

Available online 22 June 2005

Abstract

This paper presents a theory of informative communications among agents that allows a speaker to
communicate to a hearer truths about the state of the world; the occurrence of events, including other
communicative acts; and the knowledge states of any agent—speaker, hearer, or third parties—any of
these in the past, present, or future—and any logical combination of these, including formulas with
quantiﬁers. We prove that this theory is consistent, and compatible with a wide range of physical the-
ories. We examine how the theory avoids two potential paradoxes, and discuss how these paradoxes
may pose a danger when this theory are extended.
 2005 Elsevier B.V. All rights reserved.

Keywords: Communication; Knowledge; Logic; Paradox

1. Introduction

In constructing a formal theory of communications between agents, the issue of expres-
sivity enters at two different levels: the scope of what can be said about the communica-
tions, and the scope of what can be said in the communications. Other things being equal,
it is obviously desirable to make both of these as extensive as possible. Ideally, a theory
should allow a speaker to communicate to a hearer truths about the state of the world;
the occurrence of events, including other communicative acts; the knowledge states of any

✩ The research reported in this paper was supported in part by NSF grant IIS-0097537.

E-mail address: davise@cs.nyu.edu (E. Davis).

0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2005.05.002

82

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

agent—speaker, hearer, or third parties; any of these in the past, present, or future; and any
logical combination of these. This paper presents a theory that achieves pretty much that.

A few examples of what can be expressed in our representation:

(1) Alice tells Bob that all her children are asleep.
(2) Alice tells Bob that she does not know whether he locked the door.
(3) Alice tells Bob that if he ﬁnds out who was in the kitchen at midnight, then he will

know who killed Colonel Mustard.

(4) Alice tells Bob that no one had ever told her she had a sister.
(5) Alice tells Bob that he has never told her anything she did not already know.

The above examples illustrate many of the expressive features of our representation:

• Example 1 shows that the content of a communication may be a quantiﬁed formula.
• Example 2 shows that the content of a communication may refer to knowledge and

ignorance of past actions.

• Example 3 shows that the content of a communication may be a complex formula

involving both past and future events and states of knowledge.

• Examples 4 and 5 show that the content of a communication may refer to other com-
munications. They also show that the language supports quantiﬁcation over agents and
over the content of a communication, and thus allows the content to be partially char-
acterized, rather than fully speciﬁed.

Moreover, our theory supports basic inference from these kinds of representations. For
example, given that Alice tells Bob that no one has ever told her that she has a sister, and
that Alice knows that, if she did have a sister, someone would have told her, it is possible
to infer that Alice knows that she does not have a sister. The proof from our theory of this
and similar sample inferences and the representation of the ﬁve statements above is given
in Section 4.

Following the research programme of [8,21,24,25], the primary purpose of this paper is
to develop a representation for expressing commonsense knowledge about knowledge and
communication, with the ultimate intention that this representation or something similar,
could be used to carry out symbolic reasoning in this domain. A secondary purpose is
to develop an object-level theory, expressible in the language, that will justify as broad a
range as possible of commonsensically obvious inference in the domain, while entailing as
few as possible commonsensically absurd consequences. The success of the language and
theory is demonstrated in terms of their ability to capture a large number and a broad range
of examples of commonsensically obvious inferences. We are not here concerned with
specialized applications, such as distributed systems; with subtle philosophical nuance; or
with efﬁciency of inference in an implemented reasoning engine. Potentially, this theory
could ﬁnd practical application as a logical foundation either for planning communications
in a multi-agent system, or for a theory of speech acts to be used in interpreting dialogue
or engaging in dialogue with human speakers.

Since our theory allows communications that refer to other communications, and even
communications that refer to themselves, there is clearly a danger of running into para-

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

83

doxes of vicious self-reference. It is therefore particularly important to establish that the
theory is consistent. We prove a meta-theorem that the theory is indeed consistent; in fact,
that it is consistent with a wide range of domain-speciﬁc physical theories and axioms
of knowledge acquisition. We discuss two particular apparent paradoxes—an analogue of
Russell’s paradox, and the “unexpected hanging” paradox—and we show how our theory
manages to side-step these.

We should note at the outset the limitations of our theory. The theory deals only with
informative acts (and not, for example, with requests) and assumes that the following con-
ditions are true and universally known: If AS communicates Q to AH, then

(1) AS knows that Q is true at the time that he initiates the communication.
(2) From the time that he initiates the communication, AS knows that he is carrying out a
communication; he knows that the content is Q; and he knows that the recipient is AH.
(3) Similarly, when the communication is complete, AH knows that he has received a com-

munication; he knows that the content was Q; and he knows that the sender was AS.

(4) When the communication is complete, AS knows that the communication is complete

and AH knows the time at which the communication was initiated.

The paradigmatic example of a form of communication satisfying conditions (2), (3),

and (4) is direct speech.1 Another example could be mail, assuming that

• All messages are time-stamped with the time of sending, and signed by the sender.
• There is a universally known maximal delay D between the time of sending and the
time of receiving a message. (“Receiving” here means the time when the hearer reads
the message, not the time that it arrives in his mailbox.)

In this case, if we deﬁne a communication to be “complete” at the time of sending plus D,
then the above conditions are met.

Many aspects of the theory can be applied to communications that do not meet condi-
tion (4), but I have not been able to ﬁnd a plausible axiomatization of this more general
case that I can prove to be consistent. Also, I cannot prove that the theory is consistent
unless time is taken to be discrete. These are discussed further in Section 8.

The paper proceeds as follows: Section 2 reviews the theories of time and of knowledge,
which are not new here. Section 3 presents our language and axioms of communica-
tion. Section 4 is the core of this paper; it illustrates the power of the theory by showing
how it supports the representation of the ﬁve sample statements above and three example
commonsense inferences. Sections 5 and 6 describe two apparent paradoxes—a paradox
analogous to Russell’s paradox and the “unexpected hanging” paradox—and explain why
these do not cause inconsistencies in the theory. Section 7 gives the statement of Theo-
rems 1 and 2, which assert that the theory is internally consistent and compatible with a
wide range of physical theories. Sections 8 and 9 discuss related work. Section 10 dis-

1 Under assumptions that are reasonable, though not universally valid: e.g. that the speaker knows what he
will say when he begins speaking, and that the speaker and hearer have common knowledge that the hearer will
correctly understand the message.

84

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

cusses open problems and summarizes our conclusions. Appendix A gives the proofs of
Theorems 1 and 2.

2. Framework

We use a situation-based, branching theory of time; an interval-based theory of multi-
agent actions; and a possible-worlds theory of knowledge. This is all well known, so the
description below is brief.

2.1. Time and action

We use a situation-based theory of time. Time can be either continuous2 or discrete, but
it must be branching, like the situation calculus. The branching structure is described by
the partial ordering “S1 < S2”, meaning that there is a timeline containing S1 and S2 and
S1 precedes S2. It is convenient to use the abbreviations “S1 (cid:1) S2” and “ordered(S1, S2).”
The predicate “holds(S, Q)” means that ﬂuent Q holds in situation S.

Each agent has, in various situations, a choice about what action to perform next, and
the time structure includes a separate branch for each such choice. Thus, the statement that
action E is feasible in situation S is expressed by asserting that E occurs from S to S1 for
some S1 > S.

Following McDermott [26], actions are represented as occurring over an interval; the
predicate occurs(E, S1, S2) states that action E occurs starting in S1 and ending in S2.
However, the whole theory could be recast without substantial change into the situation
calculus extended to permit multiple agents, after the style of Reiter [36]. The advantage
of using the “occurs” representation is the much greater ease of extensibility. The situation
calculus was developed for domains where a single agent executes a single atomic action
in each situation to bring about the next situation; and extending the situation calculus to
allow multiple agents, exogenous change, real-valued time, concurrent actions, extended
actions, and alternative characterizations of actions involves a series of representational
extensions that are somewhat awkward and hard to integrate [36]. By contrast, all of these
can be subsumed in the “occurs” representation, though ﬁnding a correct axiomatization
of a theory with these features can still be difﬁcult.

Table 1 shows the axioms of our temporal theory. Throughout this paper, we use a
sorted ﬁrst-order logic with equality, where the sorts of variables are indicated by their
ﬁrst letter. The sorts are clock-times (T ), situations (S), Boolean ﬂuents (Q), actions (E),
agents (A), and actionals (Z). An actional is a characterization of an action without spec-
ifying the agent. For example, the term “puton(blocka, table)” denotes the actional of
someone putting block A on the table. The term “do(john, puton(blocka, table))” denotes
the action of John putting block A on the table. Free variables in a formula are assumed to
be universally quantiﬁed.

2 As will be discussed below, I have not proven the theory consistent for continuous theories of time. However,
nothing in the form of the representation inherently excludes a continuous model of time; and I conjecture that
the theory is, actually, consistent with a continuous model of time.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

85

Table 1
Temporal axioms

Primitives
T 1 < T 2—Time T 1 is earlier than T 2.
S1 < S2—Situation S1 precedes S2, on the same time line. (We overload the <

symbol.)

time(S)—Function from a situation to its clock time.
holds(S, Q)—Fluent Q holds in situation S.
occurs(E, S1, S2)—Action E occurs from situation S1 to situation S2.
do(A, Z)—Function. The action of agent A doing actional Z.

Deﬁnitions

TD.1 S1 (cid:1) S2 ≡ S1 < S2 ∨ S1 = S2.
TD.2 ordered(S1, S2) ≡

S1 < S2 ∨ S1 = S2 ∨ S2 < S1.
TD.3 feasible(E, S) ⇔ ∃S2 occurs(E, S, S2).

Axioms

T.1 T 1 < T 2 ∨ T 2 < T 1 ∨ T 1 = T 2.
T.2 ¬[T 1 < T 2 ∧ T 2 < T 1].
T.3 T 1 < T 2 ∧ T 2 < T 3 ⇒ T 1 < T 3.
(Clock times are linearly ordered.)

T.4 S1 < S2 ∧ S2 < S3 ⇒ S1 < S3. (Transitivity)
T.5 (S1 < S ∧ S2 < S) ⇒ ordered(S1, S2).

(Forward branching)

T.6 S1 < S2 ⇒ time(S1) < time(S2).

(The ordering on situations is consistent with the orderings of their clock times.)

T.7 ∀S,T 1∃S1 ordered(S, S1) ∧ time(S1) = T 1.

(Every time line contains a situation for every clock time.)

T.8 occurs(E, S1, S2) ⇒ S1 < S2.
(Events occur forward in time.)

T.9 [occurs(E, S1, S2) ∧ S1 < SX < S2 ∧ SX < SY ] ⇒

∃SZ SX < SZ ∧ ordered(SY, SZ) ∧ occurs(E, S1, SZ).
(If action E starts to occur on the time line that includes SY , then it completes on that time line (Fig. 1).)

Note that in our model of time, each feasible action and its consequences are represented
by a branch in the time structure. Thus the time structure incorporates everything that can
possibly happen. We do not single out one particular time line or branch as the history
that will actually happen. This will be important in our discussion of the paradox of the
unexpected hanging.

2.2. Knowledge

As ﬁrst proposed by Moore [28,29] and widely used since, knowledge is represented
by identifying temporal situations with epistemic possible worlds and positing a relation
of knowledge accessibility between situations. The relation k_acc(A, S, SA) means that
situation SA is accessible from S relative to agent A’s knowledge in S; that is, as far as
A knows in S, the actual situation could be SA. The statement that A knows φ in S is
represented by asserting that φ holds in every situation that is knowledge accessible from S
for A. As is well known, this theory enables the expression of complex interactions of

86

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Fig. 1. Axiom T.9. If the time structure has the form on the left, then it has one of the forms on the right.

knowledge and time; one can represent both knowledge about change over time and change
of knowledge over time.

Again following Moore [29], the state of agent A knowing what something is is ex-
pressed by using a quantiﬁer of larger scope than the universal quantiﬁcation over acces-
sible possible worlds. For example, the statement, “In situation s1, John knows who the
President is” is expressed by asserting that there exists a unique individual who is the Pres-
ident in all possible worlds accessible for John from s1.

∃X ∀S1A k_acc(john, s1, S1A) ⇒ holds(S1A, president(X))

For convenience, we posit an S5 logic of knowledge; that is, the knowledge accessibility
relation, restricted to a single agent, is in fact an equivalence relation on situations. This is
expressed in axioms K.1, K.2, and K.3 in Table 2. Three important further axioms govern
the relation of time and knowledge.

K.4. Axiom of memory: if A knows φ in S, then in any later situation, he remembers that

he knew φ in S.

K.5. A knows all the actions that he has begun, both those that he has completed and those
that are ongoing. That is, he knows a standard identiﬁer for these actions; if Bob is
dialing (212) 998-3123 on the phone, he knows that he is dialing (212) 998-3123 but

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

87

Fig. 2. Axiom K.6.

he may not know that he is calling Ernie Davis. At any time, A knows what actions
are feasible for him now.

K.6. Knowledge accessibility relations do not cross in the time structure. (Fig. 2.) In a
discrete theory of time, axiom K.6 is a consequence of the axiom of memory K.4.
(Knowledge accessibility relations that violate this condition have sometimes been
used in the literature for agents who do not satisfy the axiom of memory.)

The theory includes a form of common knowledge, restricted to two agents. Agents A1
and A2 have shared knowledge of φ if they both know φ, they both know that they
both know φ and so on.3 We represent this by deﬁning a further accessibility relation,
“sk_acc(A1, A2, S, SA)” (SA is accessible from S relative to the shared knowledge of A1
and A2). This is deﬁned as the transitive closure of links of the form k_acc(A1, ·,·) together
with links of the form k_acc(A2, ·,·). (Of course, transitive closure cannot be exactly de-
ﬁned in a ﬁrst-order theory; axioms K.7 and K.8 deﬁne an approximation that is adequate
for our purposes.)

3. Communication

We now introduce the function “inform”, taking two arguments, an agent AH and a
ﬂuent Q. The term “inform(AH, Q)” denotes the actional of informing AH that Q; the
term “do(AS, inform(AH, Q))” thus denotes the action of speaker AS informing AH that
Q. Our theory here treats “do(AS, inform(AH, Q))” as a primitive action; in a richer
theory, it would be viewed as an illocutionary description of an underlying locutionary
act (not here represented)—the utterance or writing or broadcasting of a physical sig-
nal.

We also add a second actional “communicate(AH)”. This alternative characterization of
a communicative act, which speciﬁes the hearer but not the content of the communication,
enables us to separate out physical constraints on a communicative act from contentive
constraints. Thus, we allow a purely physical theory to put constraints on the occurrence

3 In [10], we need to use common knowledge by a general set of agents. The modiﬁcations to the representation
and the axioms needed to support this are entirely straightforward.

88

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Table 2
Axioms of knowledge

Primitives
k_acc(A, SA, SB)—SB is accessible from SA relative to A’s knowledge in SA.
sk_acc(A1, A2, SA, SB)—SB is accessible from SA relative to the shared

knowledge of A1 and A2 in SA.

Axioms

K.1 ∀A,SA k_acc(A, SA, SA).
K.2 k_acc(A, SA, SB) ⇒ k_ acc(A, SB, SA)
K.3 k_acc(A, SA, SB) ∧ k_ acc(A, SB, SC) ⇒ k_acc(A, SA, SC).

(K.1 through K.3 sufﬁce to ensure that the knowledge of each agent obeys an S5 logic: what he knows is
true, if he knows φ he knows that he knows it; if he does not know φ, he knows that he does not know it.)

K.4 [k_acc(A, S2A, S2B) ∧ S1A < S2A] ⇒

∃S1B S1B < S2B ∧ k_acc(A, S1A, S1B).
(Axiom of memory: If agent A knows φ at any time, then at any later time he knows that φ was true.)

K.5 [occurs(do(A, Z), S1A, S2A) ∧ S1A (cid:1) SA ∧
ordered(SA, S2A) ∧ k_acc(A, SA, SB)] ⇒
∃S1B,S2B occurs(do(A, Z), S1B, S2B) ∧
S1B (cid:1) SB ∧
[S2A < SA ⇒ S2B < SB] ∧
[S2A = SA ⇒ S2B = SB] ∧
[SA < S2A ⇒ SB < S2B] ∧
[S1A = SA ⇒ S1B = SB].
(An agent knows which actions he has completed, which actions he has begun, and which actions are now
feasible.)

K.6 ¬∃A,S1A,S1B,S2A,S2B

S1A < S2A ∧ S1B < S2B ∧ k_acc(A, S1A, S2B) ∧ k_acc(A, S2A, S1B).

(Knowledge accessibility links do not cross in the time structure (Fig. 2).)

K.7 sk_acc(A1, A2, SA, SB) ⇔

[k_acc(A1, SA, SB) ∨ k_acc(A2, SA, SB) ∨
sk_acc(A1, A2, SB, SA) ∨
sk_acc(A2, A1, SA, AB) ∨
∃SC sk_acc(A1, A2, SA, SC) ∧ sk_acc(A1, A2, SC, SB)].
(Deﬁnition of sk_acc as a equivalence relation, symmetric in A1, A2, that includes the k_acc links for the
two agents A1, A2.)

K.8 (Induction from k_acc links to sk_acc links.) Let Φ(S) be a formula with a free situational variable S. Then

the closure of the following formula is an axiom:
[∀AS,AH[[∀SA,SBΦ(SA) ∧ k_acc(AS, SA, SB) ⇒ Φ(SB)] ∧

[∀SA,SBΦ(SA) ∧ k_ acc(AH, SA, SB) ⇒ Φ(SB)]] ⇒
[∀SA,SBΦ(SA) ∧ sk_ acc(AS, AH, SA, SB) ⇒ Φ(SB)].

of a communication, or even to posit physical effects of a communication, but these must
be independent of the information content of the communication.

We posit ﬁve axioms of communication, summarized in Table 3. Some of these are
straightforward; others much less so. We discuss them below in increasing order of com-
plexity. We also put forward a sixth axiom, a frame axiom for ignorance, but its status is
more dubious, for reasons that we will discuss.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

89

Table 3
Axioms of communication

I.1 Any inform act is a communication.

occurs(do(AS,inform(AH, Q)), S1, S2) ⇒
occurs(do(AS,communicate(AH)), S1, S2).

I.2 If a speaker AS can communicate with a hearer AH, then AS can inform AH of some speciﬁc Q if and only

if AS knows that Q holds at the time he begins speaking.

feasible(do(AS, communicate(AH)), S1) ⇒
[∀Q feasible(do(AS, inform(AH, Q)), S1) ⇔

[∀S1A k_acc(AS, S1, S1A) ⇒ holds(S1A, Q)]]

I.3 If AS informs AH of Q from S1 to S2, then in S2, AH knows that AS has informed him of Q.

∀S1,S2,S2A [occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AH, S2, S2A)] ⇒
∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A) ∧ k_acc(AH, S1, S1A)

I.4 If AS informs AH of Q1 over [S1, S2] and the shared knowledge of AS and AH in S1 implies that
holds(S1, Q1) ⇔ holds(S1, Q2), then AS has also informed AH of Q2 over [S1, S2]. Conversely, the two
actions “do(AS,inform(AH, Q1))” and “do(AS,inform(AH, Q2))” co-occur only if Q1 and Q2 are related in
this way.

occurs(do(AS, inform(AH, Q1)), S1, S2) ⇒
[occurs(do(AS, inform(AH, Q2)), S1, S2) ⇔
[∀S1A ¯sk_acc(AS, AH, S1, S1A) ⇒
[holds(S1A, Q1) ⇔ holds(S1A, Q2)]]]

I.5 Axiom of comprehension: any property of situations that can be stated in the language is a ﬂuent.

Let α(S) be a ﬁrst-order formula that contains exactly one free variable S of sort “situation” and that does
not contain Q as a free variable. (α may have other free variables of other sorts.) Then the closure of the
following formula is an axiom:

∃Q ∀S holds(S, Q) ⇔ α(S).

(The closure of a formula β is β scoped by universal quantiﬁcations of all its free variables.)

I.6 Frame axiom for ignorance. See the discussion in Section 3.5 below.

3.1. Relation between informing and communication

Axiom I.1. Any inform act is a communication.

occurs(do(AS, inform(AH, Q)), S1, S2) ⇒
occurs(do(AS, communicate(AH)), S1, S2)

Axiom I.2. If a speaker AS can communicate with a hearer AH, then AS can inform AH of
some speciﬁc Q if and only if A knows that Q holds at the time he begins speaking.

[feasible(do(AS, communicate(AH)), S1)] ⇒
[∀Q feasible(do(AS, inform(AH, Q)), S1) ⇔

[∀S1A k_acc(AS, S1, S1A) ⇒ holds(S1A, Q)]]

By virtue of these two axioms, the preconditions for AS informing AH that Q are just that
it is feasible for AS to communicate to AH and that AS knows that Q is true. The content Q

90

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

may not affect the feasibility in any other way. Axiom I.1 further guarantees that any other
physical constraints over communications, such as the duration of a communication or its
physical effects, must apply also to inform acts; that is, that the physical characteristics of
any inform act must be consistent with the physical constraints on communications. These
axioms do not rule out the possibility that the content could affect other physical aspects of
the inform act—for example, that a complex content takes longer to communicate than a
simple content—but I have not shown that any such constraints lead to a consistent theory.
Note that axiom I.2 requires, conversely, that any ﬂuent Q that is known to be true
can be communicated; that is, there is a branch in the time structure corresponding to the
communication of Q.

3.2. Epistemic effect of communication

Since we require the strong conditions mentioned in Section 1, we can posit the follow-

ing axiom:4

Axioms I.3. If AS informs AH of Q from S1 to S2, then in S2, AH knows that AS has
informed him of Q.

∀S1,S2,S2A [occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AH, S2, S2A)] ⇒
∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A) ∧ k_acc(AH, S1, S1A)

Lemmas 3.1 and 3.2 are important consequences of I.3 together with the preceding

axioms:

Lemma 3.1. If AS informs AH of Q, then, when the communication is complete, AS and
AH have shared knowledge that the communication has taken place.

occurs(do(AS, inform(AH, Q)), S1, S2) ∧ sk_acc(AS, AH, S2, S2A) ⇒
∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A)

Proof. By K.5, AS knows when he has completed a communication.

occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AS, S2, S2A) ⇒
∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A)

By I.3, AH knows when he has received a communication.

occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AH, S2, S2A) ⇒
∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A)

Choosing Φ(S) to be the formula “occurs(do(AS, inform(AH, Q))”, the formula in

Lemma 3.1 then follows from axiom K.8. (cid:1)

4 The statement of this axiom in the KR-2004 paper [9] was not correct.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

91

Lemma 3.2. If AS informs AH of Q then, when the communication is complete, then AS
and AH have shared knowledge that Q was true when the communication began.

occurs(do(AS, inform(AH, Q)), S1, S2) ∧ sk_acc(AS, AH, S2, S2A) ⇒
∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A) ∧ holds(S1A, Q)

Proof. Let as, ah, q, s0, s1, s2a satisfy the left side of the above implication.

By Lemma 3.1 there exists s1a such that occurs(do(as, inform(ah, q)), s1a, s2a).
By K.1, k_acc(as, s1a, s1a).
By I.2, holds(s1a, q). (cid:1)

3.3. Axiom of comprehension

The axiom of comprehension states that there is a ﬂuent corresponding to any property
of situations deﬁnable in the language. The content of this axiom therefore depends on the
overall language L. We state this as an axiom schema, i.e., an inﬁnite set of axioms.

Axiom I.5. The comprehension axiom for ﬂuents in a language L is this: let α(S) be a ﬁrst-
order formula that contains exactly one free variable S of sort “situation” and that does not
contain Q as a free variable. (α may have other free variables of other sorts.) Then the
closure of the following formula is an axiom:

∃Q ∀0S holds(S, Q) ⇔ α(S)

(The closure of a formula β is β scoped by universal quantiﬁcations of all its free vari-
ables.)

What this axiom states is that, given any property α(S), there exists a ﬂuent Q that
holds on just those situation satisfying α. Moreover, the property α may be parameterized
by free variables. The form of this axiom is modelled on the formulation of the “separation”
or “subset” axiom of ZF set theory as given in, for example, [15], which states that, given
any set B and property α, there exists a subset C ⊂ B of all the elements in B satisfying α.
In axiom I.5, the set of all situations corresponds to B and the ﬂuent Q corresponds to the
subset C. Further discussion is given in Appendix A, particularly in Lemma A.21.

Let us ﬁrst discuss the signiﬁcance of free variables in the formula α. The reason to
allow free variables that are not situations is to deal with examples such as the following:
We want to be able to posit that a speaker can say, for example, that some speciﬁc block
is either red or blue without requiring that the language L have a constant symbol for each
block, or even a formula that uniquely identiﬁes each block.5

5 You might well ask, “If you cannot refer to the block in L, how is the speaker talking about it?” Perhaps he
is pointing. Perhaps he is using a richer language with more constant symbols. The language L does not have
to be the language that the speaker is actually using; it is a language in which we, externally, describe what the
speaker is saying. It is not a very important point, but it does make the theory more elegant and easier to use if
one assumes that a speaker can refer de re to any entity other than a situation.

92

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

This axiom achieves this. We choose α(S) to be the formula “holds(S, red(X)) ∨

holds(S, blue(X))”. The axiom schema then state

∀X ∃Q ∀S holds(S, Q) ⇔ holds(S, red(X)) ∨ holds(S, blue(X))

That is, for every object X there is a ﬂuent Q that corresponds to the situations in which X
is either red or blue.

The reason to exclude formulas that have other situational free variables in addition to S
is that it does not seem to mean anything to have this kind of de re reference to situations.
A situation is meaningful only in relation to the current situation; there is no other way to
meaningfully refer to a situation. It may be noted that the consistency proof for the theory
(Theorem 1 below) does not depend on this restriction.

The reason for the condition that Q not appear free in the formula is that, otherwise, we
could choose α(S) to be the formula ¬holds(S, Q), in which case the axiom would give us
∃Q ∀S holds(S, Q) ⇔ ¬holds(S, Q), which is obviously not satisﬁable.6 Note, however,
that if a different variable name is chosen, there is no problem with having a free variable
of sort “ﬂuent”. For example, if we choose α(S) to be the formula ¬holds(S, Q1), then
the schema yields the axiom ∀Q1 ∃Q ∀S holds(S, Q) ⇔ ¬holds(S, Q1) which is entirely
reasonable.

The content of the comprehension axiom depends on the overall language L. In general,
one supposes that the language L will contain many domain and problem speciﬁc symbols
beyond those that are used in the axioms enumerated here. Theorem 1 shows that these
axioms are consistent when L is a physical language augmented with the symbols from the
theory of knowledge and communication described here. In [10] we consider a language
that includes also agent commitments and requests. In that setting, the above formulation
of the axiom turns out to be too strong; we have to limit the comprehension axiom to apply
only to formulas that do not include symbols describing commitment and requests.

In view of this comprehension axiom, axiom K.8 could be restated as a single axiom

(rather than an axiom schema) as follows:

K.8.A. ∀Q,AS,AH [[∀S,SA holds(S, Q) ∧ k_acc(AS, S, SA) ⇒ holds(SA, Q)] ∧

[∀S,SA holds(S, Q) ∧ k_acc(AH, S, SA) ⇒ holds(SA, Q)]] ⇒
[∀S,SA holds(S, Q) ∧ sk_acc(AS, AH, S, SA) ⇒ holds(SA, Q)].

However we did not use this formulation originally because we did not want K.8 to be

dependent on I.5.

3.4. Independent actions

In a temporal representation, like ours, that permits the concurrent execution of actions,
it does not sufﬁce just to describe what actions can be executed; one must also, to greater or
lesser extent, describe what combinations of actions can be executed concurrently. At the
minimum, if two actions are independent, it should be possible to execute the one without

6 I am extremely grateful to the anonymous reviewer who pointed this out.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

93

the other. In the case of “inform” acts, the natural axiom would be that, if AS knows φ,
then he can choose to carry out the single act of informing AH of φ and not doing anything
else. One might suppose that this could be expressed in the following two axioms:

WRONG.1 feasible(do(AS, inform(AH, Q)), S1) ⇒

∃S2 occurs(do(AS, Z), S1, S2) ⇔ Z = do(AS, inform(AH, Q)).

WRONG.2 do(AS1, inform(AH1, Q1)) = do(AS2, inform(AH2, Q2)) ⇒

AS1 = AS2 ∧ AH1 = AH2 ∧ Q1 = Q2.

However, as my labels subtly suggest,7 this is not an acceptable formulation. In fact, as

we shall show in Section 5, these are inconsistent with the axiom of comprehension I.5.

The problem, intuitively, is this: The comprehension axiom asserts that there exists a
ﬂuent for every set of situations; axiom WRONG.1 asserts that there exists a separate
branch in time for every ﬂuent. Therefore, if you try to construct a model of these axioms
combined, you ﬁrst have to construct all sets of situations; then add branches for each of
these, which gives a whole bunch more resultant situations; these in turn generate vast
numbers of new sets of situations . . . . There is no way to make this construction converge.
(I’m being a little loose here, but one can make this tight. The decisive proof that this
cannot be made to work is the “misled” paradox of Section 5.)

Therefore, we have to weaken axiom WRONG.1.8 The approach we take is as follows:
In general, it is only necessary to distinguish an occurrence of action A1 from an occur-
rence of action A2 if they have different causal consequences. For instance, in the blocks
world, if all you are interested in is the sequence of towers that are formed, then all that
matters in discriminating actions is the ending position of the block being moved; the tra-
jectory through which it moves is immaterial.

Now, in the case of informative acts, the causal consequence of concern is the effect on
knowledge states. Assuming axiom I.3, the main effect of AS informing AH of Q is that,
when the communication is complete, AS and AH have shared knowledge that Q held at
the beginning of the communication. Therefore, if Q1 and Q2 are two informative contents
such that the effects on the shared knowledge of AS and AH following a communication
of Q1 from AS to AH are the same as those effects following a communication of Q2,
then we can treat the communication of Q1 and the communication of Q2 as the same
action; they, so to speak, attain the same end state via different trajectories. And a sufﬁ-
cient condition to ensure this is that AS and AH have shared knowledge at the start of the
communication that Q1 and Q2 are equivalent.

For example, if Jack and Jane share the knowledge that George Bush is the President
and that 1600 Pennsylvania Avenue is the White House, then the action of Jack informing

7 One thing I have learned in twenty years of teaching is that, if you write down a wrong formula on the
blackboard for purposes of discussion, you have to label it WRONG in large letters. Otherwise, students copy it
into their notebooks . . . . Similarly, if someone is skimming through this paper looking for formal axioms, I do
not want him to use these.
8 Weakening axiom WRONG.2 does not work. In fact, WRONG.2 ends up being true in the model we will
construct, but its truth won’t actually matter much once we have correctly reformulated WRONG.1.

94

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Jane that Bush is at the White House is identical to the act of Jack informing Jane that the
President is at 1600 Pennsylvania Avenue. If they do not share this knowledge, then these
two acts are different.

This, then, is our axiom: The event of AS informing AH of Q1 and the event of AS
informing AH of Q2 co-occur over an interval [S1, S2] if and only if AS and AH have
shared knowledge in S1 that Q2 if and only if Q1,

I.4. occurs(do(AS, inform(AH, Q1)), S1, S2) ⇒
[occurs(do(AS, inform(AH, Q2)), S1, S2) ⇔
[∀S1A sk_acc(AS, AH, S1, S1A) ⇒

[holds(S1A, Q1) ⇔ holds(S1A, Q2)]]].

As we shall see in Section 5, in a discrete model of time this is sufﬁcient to avoid the
contradiction.

Note: The above axiom is not sufﬁcient to rule out models in which the informative
actions of one agent constrain the concurrent actions of another agent. The easiest way to
insure independence between agents is to posit an axiom of “anti-synchrony” that no two
agents begin two actions at the same time [36].

T.10. occurs(do(A1, Z1), S1, S2) ∧ occurs(do(A2, Z2), S1, S3) ⇒ A1 = A2.

However, since this axiom is part of the physical theory, and not all physical theories may
wish to use it, we have not made it part of our standard set of temporal axioms.

Two alternative formulations of axiom I.4 should be mentioned. We can weaken I.4 to
read that communicating Q1 and Q2 co-occur just if they coincide over all situations of
the same time as the beginning of the situation.

I.4.A. occurs(do(AS, inform(AH, Q1)), S1, S2) ⇒
[occurs(do(AS, inform(AH, Q2)), S1, S2) ⇔
[∀S1A time(S1A) = time(S1) ⇒

[holds(S1A, Q1) ⇔ holds(S1A, Q2)]]].

The consistency proof in Appendix A requires only a small modiﬁcation to deal with this
new version. However, this version seems to me harder to justify than the previous version.
A second alternative, which is in effect equivalent to axiom I.4.A, is to use the axioms
WRONG.1 and WRONG.2 and modify the comprehension axiom to state that there is a
ﬂuent corresponding to every property of situations at some particular time T :

I.5.B. Let α(S) be a ﬁrst-order formula in L with exactly one free variable S of sort
“situation”, in which the variable Q does not appear free. (α may have other free variables
of other sorts.) Then the closure of the following formula is an axiom:

∀T ∃Q ∀S holds(S, Q) ⇔ α(S) ∧ time(S) = T

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

95

3.5. The frame inference

Finally, it would be desirable to carry out the frame inference over knowledge and ig-

norance.

The frame axiom over knowledge is just the axiom of memory, axiom K.4: if A knows
in S that φ is true, then he remembers in all later situations that φ was true. Since we
have no actions or events that cause forgetting, this simple formulation sufﬁces. Note that
“knowing φ” is represented as “all worlds in which φ is false are inaccessible.” Hence
preserving knowledge amounts to saying that if situation SB is inaccessible from SA then
any temporal descendant of SB is inaccessible from the corresponding descendant of SA.

The frame axiom over ignorance is the reverse: Given that A does not know φ in S0, and
given that nothing occurs between S0 and S1 that would cause him to learn φ, we wish to
infer that he still does not know φ in S1. Since “not knowing φ in S” is represented as “there
are possible worlds accessible from S in which φ is false,” this frame inference should
have the following general form: If S0A is accessible from S0, S1 > S0, S1A > S0A,
and as far as A’s sources of knowledge are concerned, the interval between S0 and S1 is
indistinguishable from the interval between S0A and S1A, then S1A is accessible from S1.
Stating this formally is mostly a matter of collecting all the necessary sources of
knowledge. Our theory requires that agent A gains knowledge in S under the following
circumstances

(1) If A begins action E in S1, and S2 is on a branch in which E is executed, then in S2,
A knows that E is executed. If E is completed at or before S2, then in S2 A knows
when it was completed.

(2) If action E is feasible for A in situation S, then A knows that E is feasible in S.
(3) If A receives a communication from AS in S then A knows in S that he has received a

communication.

We also assume that there are domain-speciﬁc axioms of knowledge production. In
an S5 logic, it is reasonable to assume that these are all of the following form: In all
situations S, A knows whether Φ(A, S), where Φ is a formula that can refer only to present
or past physical states or to past (but not present) knowledge states.9 Formally, we impose
the following conditions on Φ(A, S):

• The only free variables in Φ(A, S) are A and S.
• If S1 is a quantiﬁed variable other than S appearing in Φ, and S1 is used as either the
second-to-last or last argument for either k_acc or sk_acc, then the quantiﬁcation of
S1 imposes the restriction S1 < S.

• If S1 is a quantiﬁed variable other than S appearing in Φ, and S1 is not used as an ar-
gument for either k_acc or sk_acc, then the quantiﬁcation of S1 imposes the restriction
S1 (cid:1) S.

9 Actually, I conjecture that these restrictions are not necessary, and that it is consistent to allow Φ to be any
formula, but I have not proven it.

96

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Table 4
Frame action for ignorance

I.6: [k_acc(A, S0A, S0B) ∧ S0A < S1A ∧ S0B < S1B ∧ time(S1B) = time(S0B) ∧

(1)

[∀S2A,S3A,Z [occurs(do(A, Z), S2A, S3A) ∧ S2A (cid:1) S1A ∧ S0A < S3A ∧

ordered(S1A, S3A)] ⇒
∃S2B,S3B occurs(do(A, Z), S2B, S3B) ∧ time(S2B) = time(S2A) ∧

[[S1A < S3A ∧ S1B < S3B] ∨
[S3B (cid:1) S1B ∧ time(S3B) = time(S3A)]]] ∧

(2)

[∀S2B,S3B,Z [occurs(do(A, Z), S2B, S3B) ∧ S2B (cid:1) S1B ∧ S0B < S3B ∧

ordered(S1B, S3B)] ⇒
∃S2A,S3A occurs(do(A, Z), S2A, S3A) ∧ time(S2A) = time(S2B) ∧

[[S1B < S3B ∧ S1A < S3A] ∨
[S3A (cid:1) S1A ∧ time(S3A) = time(S3B)]]] ∧

(3)

[∀S2A,S3A,AS,Q [occurs(do(AS, inform(A, Q)), S2A, S3A) ∧ S3A (cid:1) S1A] ⇒

∃S2B,S3B occurs(do(AS, inform(A, Q)), S2B, S3B) ∧ S3B (cid:1) S1B ∧

time(S2B) = time(S2A) ∧ time(S3B) = time(S3A)] ∧

(4)

[∀S2B,S3B,AS,Q [occurs(do(AS, inform(A, Q)), S2B, S3B) ∧ S3B (cid:1) S1B] ⇒

∃S2A,S3A occurs(do(AS, inform(A, Q)), S2A, S3A) ∧ S3A (cid:1) S1A ∧

time(S2A) = time(S2B) ∧ time(S3A) = time(S3A)] ∧

[∀S2A,S2B [S2A (cid:1) S1A ∧ S2B (cid:1) S1B ∧ time(S2A) = time(S2B)] ⇒

(cid:1)

i

[Φi (S2A) ⇔ Φi (S2B)]]

(5)

]

⇒ k_acc(A, S1A, S1B).

Thus we assume the existence of a ﬁnite collection of axioms of the form

∀A,S [[∀SAk_acc(A, S, SA) ⇒ Φi(A, S)] ∨
[∀SA k_acc(A, S, SA) ⇒ ¬Φi(A, S)]]

For example, Scherl and Levesque [38,39] propose the use of an action “SENSEQ” which
informs the actor whether ﬂuent Q is true. We can achieve that in the above framework by
choosing Φ(A, S) to be the condition that A has executed SENSEQ and Q holds:

Φ(A, S) ⇔ ∃S1 occurs(SENSEQ, S1, S) ∧ holds(S, Q)

We now posit that every agent always knows whether Φ(A, S). Since, by axiom K.5, an
agent always knows whether he has executed SENSEQ, it follows that, if an agent has
executed SENSEQ, then he knows whether Q is true.

So now we can state the frame axiom I.6 asserting that if none of the above conditions

has been met, then a knowledge accessibility relation persists. (Table 4.)

That is: suppose that S0B is knowledge accessible from S0A relative to A, S1A fol-
lows S0A, S1B follows S1A, S1A and S1B have the same clock-time, and the following
conditions are met:

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

97

(1) If A executes actional Z, either completing it or starting it between S1A or beginning
it at S2A, then he executes the same action at the corresponding times in the interval
[S0B, S1B]. (If the action ends after S1A and S1B, then the clock-times of the endings
need not be the same.)

(2) The reverse of 1; if A executes an action in the “B” interval then he executes the same

action at the corresponding time in the “A” interval.

(3) If AS tells A of Q and completes this action between S0A and S1A, then the same

thing happens between S0B and S1B.

(4) The reverse of (3): If AS tells A of Q and completes this action between S0B and S1B,

then the same thing happens between S0A and S1A.

(5) All of the facts Φi have the same truth value from S0A to S1A.

Then nothing that A knows about has occurred to distinguish the interval [S0B, S1B] from
the interval [S0A, S1A], and therefore S1B is knowledge accessible from S1A.

Well, there it is. It is not a candidate for any “Top 10 most elegant axioms” lists.
A more serious problem is that it does not give us what we want. What we want is: Given
that in s0, Sam does not know whether Herbert Hoover invented the vacuum cleaner (P ),
and given that the only thing that happens between s0 and s1 is that Jack tells Sam that tea
is selling for $2 a pound in Shanghai (Q), we should be able to infer that Jack still does not
know whether Herbert Hoover invented the vacuum cleaner. But that inference is not valid.
The problem is that it is consistent with the givens that Sam originally knows ¬P ⇔ Q,
and so, when Jack tells him Q he ﬁnds out ¬P . Alternatively, Sam may originally know
the weaker statement, “If Jack knows Q, then P ;” again, when Jack tells him Q he can
infer that Jack knows Q and therefore P .

The problem here is not with the frame axiom; the frame axiom is ﬁne. The problem is
with the speciﬁcation of the initial state. You need to add the condition that the agent does
not know anything except the givens. Halpern [17] presents a multi-agent model in which
an agent knows only a speciﬁc collection of statements and their logical consequences,
and nothing more about the world including other agents’ knowledge (more precisely, he
presents a collection of such theories corresponding to different models of knowledge);
and similar studies have been done by Levesque [22]. The problem, though, is that these
theories only work in the case where we can specify everything that the agent knows. In
most real cases, we do not know everything that Sam knows, but we still want to make the
inference. How this inference can be characterized is entirely an open question; and once it
is solved (perhaps non-monotonically) it is unclear whether it would use axiom I.6 at all. It
would be hard to ﬁnd any plausible commonsense inference problems where axiom I.6 was
useful. (In [7], I have studied a special case of this frame inference where the occurrence
of an event is physicially hidden from an agent, and it is therefore possible to infer that the
agent remains ignorant of it.)

4. Sample inferences

We now illustrate the power of the above theory by showing how the sample scenarios

in the introduction can be represented, and how three toy inferences can be justiﬁed.

98

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Table 5
Notational extensions

Deﬁnitions

KD.1 holds(S, know(A, Q1)) ≡ [∀SA k_acc(A, S, SA) ⇒ holds(SA, Q1)].
KD.2 holds(S, not(Q1)) ≡ ¬holds(S, Q1).
KD.3 holds(S, know_whether(A, Q1)) ≡

holds(S, know(A, Q1)) ∨ holds(S, know(A, not(Q1)))

KD.4 Let β(S) be a formula with a free variable S (and possibly other free variables). Let µ be a variable of sort
“ﬂuent” that does not appear free in β and let Φ(µ) be a formula. The expression “Φ(λ(S)β(S))” should
be expanded to read

∃µ [∀S holds(S, µ) ⇔ β(S)] ∧ Φ(µ)

In an expression with multiple lambda expressions, the expressions should be expanded from left to right,
from outside to inside.

To help make the representations more readable and more elegant, we will begin by
deﬁning four further notations (Table 5). First we deﬁne “know(A, Q)” as a function map-
ping agent A and ﬂuent Q to the ﬂuent of A knowing that Q holds in S; that is, Q holds in
all situations accessible from S (deﬁnition KD.1).

The existence of such a ﬂuent is guaranteed by the comprehension axiom. Let α(S)
be the open formula “∀SA k_acc(A, S, SA) ⇒ holds(SA, Q1)”. Then the comprehension
schema asserts

∀A,Q1 ∃Q ∀S holds(S, Q) ⇔ ∀SA k_acc(A, S, SA) ⇒ holds(SA, Q1)

For any particular A and Q1, the ﬂuent Q satisfying this property has the property we
need for know(A, Q1). Note that, in this construal “know” is a garden-variety ﬁrst-order
function both in its syntax and its semantics.

Second, we deﬁne “not(Q)” as the function mapping ﬂuent Q to the ﬂuent of Q not
holding. Third, we deﬁne “know_whether(A, Q)” as a function mapping agent A and ﬂuent
Q to the ﬂuent of A knowing whether or not Q is true. Again, the existence of such ﬂuents
is guaranteed by the comprehension axiom, and these are simple ﬁrst-order functions.

The ﬁnal notation is a macro extension to ﬁrst-order syntax (“syntactic sugar”). We will
use expressions of the form λ(S)β(S) to denote the ﬂuent that holds in situation S just if
formula β holds of S. Thus, for examples, the ﬂuent that Joe has just completed putting
block A on B can be denoted by the expression

λ(S)∃S0 occurs(do(joe, puton(a, b)), S0, S)

The statement that Sam knows in situation s1 that Joe has just completed putting block A
onto B can thus be expressed

holds(s1, know(sam, λ(S)∃S0 occurs(do(joe, puton(a, b)), S0, S)))

These lambda expressions are deﬁned within our theory as macros that expand into ﬁrst-
order formulas. (It should be emphasized that we are not here deﬁning a general lambda
calculus, just lambda expressions with one situational argument and a ﬂuent value.) The
expansion rule is given in deﬁnition KD.4 in Table 5.

For example, the formula

holds(s1, know(sam, λ(S)∃S0 occurs(do(joe, puton(a, b)), S0, S)))

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

99

expands to the formula

∃Q [∀S holds(S, Q) ⇔ ∃S0 occurs(do(joe, puton(a, b)), S0, S)] ∧

holds(s1, know(sam, Q))

Using the deﬁnition of “know”, this is equivalent to

∃Q [∀S holds(S, Q) ⇔ ∃S0 occurs(do(joe, puton(a, b)), S0, S)] ∧

∀S1A k_acc(sam, s1, S1A) ⇒ holds(S1A, Q)

Since the existence of a ﬂuent Q satisfying this ﬁrst line is guaranteed by the compre-

hension axiom, this is equivalent to

∀S1A k_acc(sam, s1, S1A) ⇒

∃S0 occurs(do(joe, puton(a, b)), S0, S1A).

In the examples that follow, we will give both the compacted representation (with

“know” and lambda expressions) and the expanded versions without them.

4.1. Sample representations

We illustrate the expressive power of our representation using the examples from the

introduction.

4.1.1. Sample representation 1

Alice tells Bob that all her children are asleep.

occurs(do(alice, inform(bob,

λ(S) ∀C holds(S, child(C, alice)) ⇒ holds(S, asleep(C)))),

s0, s1)

In expanded form:

∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧

∀S holds(S, Q) ⇔
[∀C holds(S, child(C, alice)) ⇒ holds(S, asleep(C))]

4.1.2. Sample representation 2

Alice tells Bob that she does not know whether he locked the door.

occurs(do(alice, inform(bob,

λ(S) holds(S, not(know_whether(alice,

λ(SA) ∃S1A,S2A occurs(do(bob, lock_door), S1A, S2A) ∧ S1A < SA

))))),

s0, s1)

100

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Expanding and rearranging gives

∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧

∀S holds(S, Q) ⇔

[∃SA k_acc(alice, S, SA) ∧

∃S1A,S2A S1A < S2A < SA ∧

occurs(do(bob, lock_door), S1A, S2A)] ∧

[∃SA k_acc(alice, S, SA) ∧

¬∃S1A,S2A S1A < S2A < SA ∧

occurs(do(bob, lock_door), S1A, S2A)]

4.1.3. Sample representation 3

Alice tells Bob that if he ﬁnds out who was in the kitchen at midnight, then he will
know who killed Colonel Mustard. (Note: The interpretation below assumes that exactly
one person was in the kitchen at midnight.)

occurs(do(alice, inform(bob,

λ(S) ∀SA [S < SA ∧

∃PK holds(SA, know(bob,

λ(SC) ∃S3C S3C < SC∧ time(S3C) = midnight ∧

holds(S3C, in(PK, kitchen))))]

⇒
∃PM holds(SA, know(bob,

λ(SB) ∃S2B,S3B S3B < SB ∧

occurs(do(PM, kill(mustard)), S2B, S3B))))),

s0, s1)

Expanding and rearranging gives:

∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧

∀S holds(S, Q) ⇔
∀S2 [S2 > S ∧

∃PK ∀S2A k_acc(bob, S2, S2A) ⇒

∃S3A S3A < S2A ∧ midnight(time(S3A)) ∧
holds(S3A, in(PK, kitchen))] ⇒
[∃PM ∀S2B k_acc(bob, S2, S2B) ⇒
∃S3B,S4B S3B < S4B < S2B ∧

occurs(do(PM, murder(mustard)), S3B, S4B)]

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

101

4.1.4. Sample representation 4

Alice tells Bob that no one had ever told her she had a sister.

occurs(do(alice, inform(bob,

λ(S) ¬∃AP,S1,S2 S2 < S ∧

occurs(do(AP, inform(alice,

λ(SA) ∃P 2 holds(SA, sister(P 2, alice))))
S1, S2)))

s0, s1)

Expanding and rearranging,

∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧

∀S holds(S, Q) ⇔

¬∃S2,S3,Q1,P 1 S2 < S3 < S ∧

occurs(do(P 1, inform(alice, Q1)), S2, S3) ∧
∀SX holds(SX, Q1) ⇒ ∃P 2 holds(SX, sister(P 2, alice))

4.1.5. Sample representation 5

Alice tells Bob that he has never told her anything she didn’t already know.

occurs(do(alice, inform(bob,

λ(S) ∀S2,S3,Q S3 (cid:1) S ∧ occurs(S2, S3, do(bob, inform(alice, Q))) ⇒

holds(S2, know(alice, Q))))

s0, s1)

Expanding and rearranging gives:

∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧

∀S holds(S, Q) ⇔

∀S2,S3,Q1

[S2 < S3 (cid:1) S ∧
occurs(do(bob, inform(alice, Q1)), S2, S3)] ⇒
∀S2A k_acc(alice, S2, S2A) ⇒ holds(S2A, Q1)

4.2. Sample inferences

We next illustrate the inferential power of the above theory with three toy problems.

102

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

4.2.1. Sample inference 1

Given:

X.1: Sam knows in s0 that it will be sunny on July 4.

holds(s0, know(sam, λ(S) ∀SJ S < SJ ∧ time(SJ) = july4 ⇒ holds(SJ, sunny)))

Expanding gives

∀S0A,SJA [k_acc(sam, s0, S0A) ∧ S0A < SJA ∧ time(SJA) = july4]

⇒ holds(SJA, sunny)

X.2: In any situation, if it is sunny, then Bob can play tennis.

∀S holds(S, sunny) ⇒ feasible(occurs(do(bob, tennis), S))

X.3: Sam can always communicate with Bob.

∀S1 feasible(do(sam, communicate(bob)), S1).

Infer:

X.P: Sam knows that there is an action he can do (e.g., tell Bob that it will be sunny) that
will cause Bob to know that he will be able to play tennis on July 4.

holds(s0, know(sam, λ(S)

∃Z feasible(do(sam, Z), S) ∧

∀S2A occurs(do(sam, Z), S, S2A) ⇒
holds(S2A, know(bob, λ(S2B)

∀S3B S2B < S3B ∧ time(S3B) = july4 ⇒
feasible(do(bob, tennis), S3B)))))

Expanding gives

k_acc(sam, s0, S0A) ⇒
∃Z feasible(S0A, do(sam, Z)) ∧

∀S2A,S2B,S3B [occurs(do(sam, Z), S0A, S2A) ∧ k_acc(bob, S2A, S2B) ∧

S2B < S3B ∧ time(S3B) = july4] ⇒

feasible(do(bob, tennis), S3B)

Proof. By the comprehension axiom I.5 there is a ﬂuent q1 that holds in any situation S
just if it will be sunny on July 4 following S.
P.1: ∀S holds(S, q1) ⇔ [∀S1[S < S1 ∧ time(S1) = july4] ⇒ holds(S1, sunny)].
Let z1 = inform(bob, q1). By axioms I.2, X.1, and X.3, do(sam, z1) is feasible in s0.

P.2: feasible(do(sam, z1), s0).

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

103

By axiom K.5, Sam knows in s0 that do(sam, z1) is feasible.
P.3: ∀S0A k_acc(sam, s0, S0A) ⇒ feasible(do(sam, z1), S0A).
Let s0a be any situation such that k_acc(sam, s0, s0a). By P.3, there exists a situation s1a
such occurs(do(sam, z1), s0a, s1a). Let s2a be any situation such that occurs(do(sam, z1),
s0a, s2a). Let s2b be any situation such that k_acc(bob, s2a, s2b).

By Lemma 3.2, there exists s1b such that occurs(do(sam, z1), s1b, s2b) and holds(s1b,
q1). Let s3b be any situation such that s2b < s3b and time(s3b) = july4. By T.8 and T.4,
s1b < s3b. By P.2, holds(s3b, sunny). By X.2, feasible(do(bob, tennis), s3b). Applying
universal abstraction over s0a, s2a, s2b, and s3b and existential abstraction over z1 and s1a
gives us formula X.P. (cid:1)

4.3. Sample inference 2

Given:

Y.1: Bob confesses to Alice that he has cheated on her.

occurs(do(bob, inform(alice,

λ(S) ∃S2,S3 S3 < S ∧ occurs(do(bob, cheat), S2, S3))),

s0, s1)

This expands to

∃Q occurs(do(bob, inform(alice, Q)), s0, s1) ∧
∀S holds(S, Q) ⇔ ∃S2,S3 S3 < S ∧ occurs(do(bob, cheat), S2, S3)

Y.2: Alice responds that Bob has never told her anything she did not already know.

As in sample representation 5, above, in expanded form this is:

∃Qoccurs(do(alice, inform(bob, Q)), s1, s2) ∧

∀S holds(S, Q) ⇔

∀S3,S4,Q1

[S3 < S4 (cid:1) S ∧ occurs(do(bob, inform(alice, Q1)), S3, S4)] ⇒
∀S3A k_acc(alice, S3, S3A) ⇒ holds(S3A, Q1)

Y.P: Bob now knows that Alice had already known, before he spoke, that he had cheated
on her.

holds(s2, know(bob,

λ(S2A) ∃S0A,S1A,Q1 S1A < S2A ∧

occurs(do(bob, inform(alice, Q1)), S0A, S1A) ∧
holds(S0A, know(alice,

λ(S0B) ∃S3B,S4B S4B < S0B ∧

occurs(do(bob, cheat), S3B, S4B))))).

104

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Expanding and rearranging gives:

∀S2A k_acc(bob, s2, S2A) ⇒

∃S0A,S1A,Q1 S1A < S2A ∧ occurs(do(bob, inform(alice, Q1)), S0A, S1A) ∧

[∀S0B k_acc(alice, S0A, S0B) ⇒
∃S3B,S4B S4B < S0B ∧ occurs(do(bob, cheat), S3B, S4B)]

Proof. Let q1 be the content of Bob’s statement in Y.1, and let q2 be the content of Alice’s
statement in Y.2. By axiom I.5, both these ﬂuents exist.

By K.4, Bob knows in s2 that he has informed Alice of q1.

Q.1: ∀S2A k_acc(bob, s2, S2A) ⇒

∃S0A,S1A S1A < S2A ∧ occurs(do(bob, inform(alice, q1)), S0A, S1A).

By Lemma 3.2, Bob knows in s2 that q2 held when Alice started to speak.

Q.2: k_acc(bob, s2, S2A) ⇒

∃S1A occurs(do(alice, inform(bob, q2)), S1A, S2A) ∧ holds(S1A, q2).

Let s2a be any situation such that k_acc(bob, s2, s2a), and let s1a be a corresponding value
of S1A satisfying Q.2. Then holds(s1a, q2).

By deﬁnition of q2, we have that in s1a, whenever Bob had previously told Alice any-

thing (Q3), she had already known it.

Q.3: ∀S3,S4,Q3 [S3 < S4 (cid:1) s1a ∧ occurs(do(bob, inform(alice, Q3)), S3, S4)] ⇒

∀S3A k_acc(alice, S3, S3A) ⇒ holds(S3A, Q1).

By K.4 and Y.3, Bob knows in s1 that he has informed Alice of q1.

Q.4: ∀S1A k_acc(bob, s1, S1A) ⇒

∃S0A occurs(do(bob, inform(alice, q1)), S0A, S1A).

In particular, therefore, Q.4 is true of S1A = s1a.
Q.5: ∃S0A occurs(do(bob, inform(alice, q1)), S0A, s1a).

Let s0a be a situation satisfying Q.5. Applying Q.3, with S3 →s0z, S4 →s1a, and

Q3 →q1, gives
Q.6. ∀S0B k_acc(alice, s0a, S0B) ⇒ holds(S0B, q1). (cid:1)

Applying the deﬁnition of q1, we get the desired result.

4.4. Sample inference 3

Given:

Z.1: Anne does not know that she has a sister.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

105

¬holds(s0, know(anne, λ(S) ∃Y holds(S, sister(Y, anne))))

This expands to

¬[∀S0A k_acc(anne, s0, S0A) ⇒ ∃Y holds(S0A, sister(Y, anne))]

Z.2: Anne knows that, if she had a sister, someone would have told her about him.

holds(s0, know(anne,

λ(S) ∀Y holds(S, sister(Y, anne)) ⇒

∃S1,S2,AS S2 < S ∧ occurs(do(AS, inform(anne, sister(Y, anne))), S1, S2)))

Expanding and rearranging,

∀S0A k_acc(anne, s0, S0A) ⇒

∀Y holds(S0A, sister(Y, anne)) ⇒
∃S1A,S2A,AS S2A (cid:1) S0A ∧

occurs(do(AS, inform(anne, sister(Y, anne))), S1A, S2A)

Z.3: Sisterhood is forever.

S0 < S1 ∧ holds(S0, sister(X, Y )) ⇒ holds(S1, sister(X, Y ))

Infer: Anne knows that she has no sister.

holds(s0, know(anne, λ(S) ¬∃Y holds(S, sister(Y, anne))))

Expands to:
Z.4: ∀S0A k_acc(anne, s0, S0A) ⇒ ¬∃Y holds(S0A, sister(Y, anne)).

Note: This is a monotonic variant of the “auto-epistemic” inference [30].

Proof by contradiction. Suppose that Z.4 is false and Anne does not know that she does
not has a sister—in other words, as far as she knows she might have a sister.
R.1: ∃S0A,Y k_acc(anne, s0, S0A) ∧ holds(S0A, sister(Y, anne)).
Let sb and yb be values satisfying R.1. Thus k_acc(anne, s0, sb) and holds(sb, sister(yb,
anne)). By Z.2, in sb someone would have already told her that she had a sister.
R.2: ∃S1A,S2A,AS S2A (cid:1) sb ∧ occurs(do(AS, inform(anne, sister(yb, anne))), S1A, S2A).
By Lemma 3.2, Anne would know in sb that she had previously had a sister.

R.3: ∀SC k_acc(anne, sb, SC) ⇒

∃S1C S1C < SC ∧ holds(S1C, sister(yb, anne)).

Let s0x be any situation such that k_acc(anne, s0, s0x). By K.2 and K.3 k_acc(anne, sb,
s0x). By R.3 and X.3, holds(s0x, sister(yb, anne)). Applying universal abstraction to s0x
we have
R.4: ∀S0X k_acc(anne, S0, S0X) ⇒ holds(S0X, sister(yb, anne)).
But this contradicts X.1. (cid:1)

106

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

5. Paradox

The following Russell-like paradox seems to threaten our theory:10
Paradox: Let Q be a ﬂuent. Suppose that over interval [S0, S1], agent a1 carries out the
action of informing a2 that Q holds. Necessarily, Q must hold in S0, since agents are not
allowed to lie (axiom I.2). Let us say that this communication is immediately obsolete if
Q no longer holds in S1. For example, if it is raining in s0, the event of a1 telling a2 that
it is raining occurs over [s0, s1], and it has stopped raining in s1, then this communication
is immediately obsolete. Now let us say that a1 has “misled” a2 in S if S is the end of
an immediately obsolete communication. (There is no suggestion intended here, of course,
that a2 has any false beliefs.) Since “a1 having misled a2” is a property of a situation, by
the comprehension axiom it should be deﬁnable as a ﬂuent. Symbolically,

holds(S, misled(A1, A2)) ≡
∃Q,A1,A2,S0 occurs(do(A1, inform(A2, Q)), S0, S) ∧ ¬holds(S, Q)

Now, suppose that, as above, in s0 it is raining; from s0 to s1, a1 tells a2 that it is raining;
and in s1 it is no longer raining and a1 knows that it is no longer raining. Then a1 knows
that “misled(a1, a2)” holds in s1. Therefore, (axiom I.2) it is feasible for a1 to tell a2 that
“misled(a1, a2)” holds in s1. Suppose that, from s1 to s2, a1 informs a2 that “misled(a1,
a2)” holds. The question is now, does “misled(a1, a2)” hold in s2? Well, if it does, then
what was communicated over [s1, s2] still holds in s2, so “misled(a1, a2)” does not hold;
but if it does not, then what was communicated no longer holds, so “misled(a1, a2)” does
hold in s2.

The ﬂaw in this argument is that it presupposes the independence axiom WRONG.1
(p. 93) that we rejected earlier. The argument presumes that if ﬂuent Q1 (cid:10)= Q2, and do(A1,
inform(A2, Q1)) occurs from s1 to s2, then do(A1, inform(A2, Q2)) does not occur. (Our
English description of the argument used the phrase “what was communicated between s1
and s2“, which presupposes that there was a unique content that was communicated.) But
axiom I.4 asserts that many different ﬂuents are communicated in the same act. Therefore,
the argument collapses.

In particular, as we shall show, the clock time (in the sense of “the number of situa-
tions that have elapsed since the start of time”) is always common knowledge among all
agents (Theorem 3, Appendix A). Now, let q1 be any ﬂuent, and suppose that occurs(do(a1,
inform(a2, q1)), s1, s2). Let t1 = time(q1) and let q2 be the ﬂuent deﬁned by the formula

∀S holds(S, q2) ⇔ holds(S, q1) ∧ time(S) = t1

By assumption, it is shared knowledge between a1 and a2 that holds(s1, q2) ⇔ holds(s1,
q1). Hence, by axiom I.4, occurs(do(a1, inform(a2, q2)), s1, s2). But by construction q2
does not hold in s1; hence the occurrence of do(a1, inform(as, q2)) from s1 to s2 is imme-
diately obsolete. Therefore “misled(a1, a2)” holds any time a1 communicates with a2.

10 The comprehension axiom in itself, without the “inform” acts, does not lead to Russell’s paradox, because
a ﬂuent is being deﬁned as in terms of a property of situations, so that there is no circularity. Formally, we will
construct a set of situations, and then use the standard Zermelo–Fraenkel separation axiom to deﬁne a ﬂuent as a
subset. See Lemma A.21, p. 136.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

107

Changing the deﬁnition of misled to use the universal quantiﬁer, thus:

holds(S, misled(A1, A2)) ≡
∀Q,A1,A2 occurs (do(A1, inform(A, Q)), S0, S) ∧ ¬holds(S, Q)

does not rescue the contradiction. One need only change the deﬁnition of q2 above to be

∀S holds(S, q2) ⇔ holds(S, q1) ∨ time(S) (cid:10)= t1

Clearly, the new deﬁnition of “misled(a1, a2)” never holds after any informative act.

Of course, if we extend the theory to include the underlying locutionary act, then this
paradox may well return, as the locutionary act that occurs presumably is unique. However,
as the content of a locutionary act is a quoted string, we can expect to have our hands full
of paradoxes in that theory; this “misled” paradox will not be our biggest problem [31].

6. Unexpected hanging

The well-known paradox of the unexpected hanging (also known as the surprise exam-
ination) [16,34] can be formally expressed in our theory; however, the paradox does not
render the theory inconsistent. (The analysis below is certainly not a philosophically ade-
quate solution to the paradox, merely an explanation of how our particular theory manages
to side-step it.)

The paradox can be stated as follows:

A judge announces to a prisoner, “You will be hung at noon within 30 days; however,
that morning you will not know that you will be hung that day”. The prisoner reasons
to himself, “If they leave me alive until the 30th day, then I will know that morning that
they will hang me that day. Therefore, they will have to kill me no later than the 29th
day. So if I ﬁnd myself alive on the morning of the 29th day, I can be sure that I will be
hung that day. So they will have to kill me no later than the 28th day . . . . So they cannot
kill me at all!”

On the 17th day, they hung him at noon. He did not know that morning that he would

be hung that day.

Let kill_today be the ﬂuent that the prisoner will be hung today. Then the judge’s state-

ment can be represented as follows:

occurs(do(judge, inform(prisoner,

λ(S) ∀SX [S < SX ∧ time(SX) = time(S) + 31] ⇒
∃SH S < SH < SX ∧ holds(SH, kill_today) ∧
¬holds(SH, know(prisoner, kill_ today)))),

s0, s1)

Expanding and rearranging, this becomes

108

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

∃Q occurs(do(judge, inform(prisoner, Q)), s0, s1) ∧

∀S holds(S, Q) ⇔

∃SH,SHA S < SH < SX ∧ holds(SH, kill_today) ∧

k_acc(prisoner, SH, SHA) ∧ ¬holds(SHA, kill_today)

We further posit the axioms that the prisoner has never been killed before s0, and that if

an agent has never been killed, he knows that he has never been killed.11

¬∃S S < s0 ∧ holds(S, kill_today).
∀S2 [∀S1 S1 < S2 ⇒ ¬holds(S1, kill_today)] ⇒
holds(S2, know(prisoner, λ(S) ∀S1 S1 < S ⇒ ¬holds(S1, kill_today)))

Let UHlang be the judge’s statement in English and let UHlogic be the ﬂuent Q that the
judge communicates. Let “kill(K)” be the proposition that the prisoner will be killed no
later than the Kth day. It would appear that UHlang is true; that the judge knows that in s0
that it is true, and that UHlogic means the same as UHlang. By Axiom I.2, if the judge knows
that UHlogic holds in s0, then he can inform the prisoner of it. How, then, does our theory
avoid contradiction?

The ﬁrst thing to note is that the prisoner cannot know UHlogic. There is simply no
possible worlds structure in which the prisoner knows UHlogic. The proof is exactly iso-
morphic to the sequence of reasoning that prisoner goes through. Therefore, by Lemma 3.2
above, the judge cannot inform the prisoner of UHlogic; if he did, the prisoner would know
it to be true.

The critical point is that there is a subtle difference between UHlang and UHlogic. The
statement UHlang asserts that the prisoner will not know kill_today—this means even after
the judge ﬁnishes speaking. In our theory, however, one can only communicate properties
of the situation at the beginning of the speech act and there is no way to refer to what
will happens as distinguished from one could happen. So what UHlogic asserts is that the
prisoner will not know kill_today whatever the judge decides to say or do in s0.

In fact, it is easily shown that either [the judge does not know in s0 that UHlogic is
true], or [UHlogic is false]. It depends on what the judge knows in s0. Let us suppose that
in s0, it is inevitable that the prisoner will be killed on day 17 (the executioner has gotten
irrevocable orders). There are two main cases to consider.

• Case 1: All the judge knows is kill(K), for some K > 17. Then the most that the judge
can tell the prisoner is kill(K). In this case, UHlogic is in fact true in s0, but the judge
does not know that it is true, because as far as the judge knows, it is possible that (a)
he will tell the prisoner kill(K) and (b) the prisoner will be left alive until the Kth day,
in which case the prisoner would know kill_today on the morning of the Kth day.

11 In S5, it is a logical consequence of this axiom that if he has been killed, he knows he has been killed; but that
is beyond the scope of this paper.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

109

• Case 2: The judge knows kill(17). In that case, UHlogic is not even true in s0, because
the judge has the option of telling the prisoner kill(17), in which case the prisoner will
know kill_today on the morning of the 17th day.

Again, we do not claim that this is an adequate solution to the philosophical problem,
merely an explanation of how our formal theory manages to remain consistent and side-
step the paradox. In fact, in the broader context the solution is not at all satisfying, for
reasons that may well become serious when the theory is extended to be more powerful.
There are two objections. First, the solution depends critically on the restriction that agents
cannot talk about what will happen as opposed to what can happen; in talking about the
future, they cannot take into account their own decisions or commitments about what they
themselves are planning to do. One can extend the outer theory so as to be able to represent
what will happen—in [10], we essentially do this—but then the comprehension axiom I.5
must be restricted so as to exclude this from the scope of ﬂuents that can be the content of
an “inform” act. We do not see how this limitation can be overcome.

The second objection is that it depends on the possibility of the judge telling the prisoner
kill(17) if he knows this. Suppose that we eliminate this possibility? Consider the follow-
ing scenario: The judge knows kill(17), but he is unable to speak directly to the prisoner.
Rather, he has the option of playing one of two tape recordings; one says “kill(30)” and the
other says UHlogic. Now the theory is indeed inconsistent. Since the prisoner cannot know
UHlogic it follows that the judge cannot inform him of UHlogic; therefore the only thing that
the judge can say is “kill(30)”. But in that case, the formula UHlogic is indeed true, and the
judge knows it, so he should be able to push that button.

To axiomatize this situation we must change axiom I.2 to assert that the only possible

inform acts are kill(30) and UHlogic.

Within the context of our theory, it seems to me that the correct answer is “So what?”
Yes, you can set up a Rube Goldberg mechanism that creates this contradiction, but the
problem is not with the theory, it is with the axiom that states that only these two inform
acts are physically possible.

(Those readers, if any, who work through the proof of Theorem 1 in Appendix A may
wonder what prevents this constraint from being incorporated into the construction of
u-situations. After all, all that this amounts to is drastically restricting the class of “in-
form” acts that are added on. The answer is that which of the “inform” acts are allowed
to exist now depends on the interpretation of a formula in the extended language, and
that therefore the construction now involves a vicious cycle. See further the comments on
Lemma A.21.)

In a wider context, though, this answer will not serve. After all, it is physically possi-
ble to create this situation, and in a sufﬁciently rich theory of communication, it will be
provable that you can create this situation. However, such a theory describing the physical
reality of communication must include a theory of locutionary acts; i.e., sending signals of
quoted strings. As mentioned above such a theory will run into many paradoxes; this one
is probably not the most troublesome.

110

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

7. Consistency

Two paradoxes have come up, but the theory has side-stepped them both. How do we
know that the next paradox won’t uncover an actual inconsistency in the theory? We can
eliminate all worry about paradoxes once and for all by proving that the theory is consis-
tent. We do this by constructing a model satisfying the theory. More precisely, we construct
a fairly broad class of models, establishing (informally) that the theory is not only consis-
tent but does not necessitate any weird or highly restrictive consequences. (Just showing
soundness with respect to a model or even completeness is not sufﬁcient for this. For in-
stance, if the theory were consistent only with a model in which every agent was always
omniscient, and inform acts were therefore no-ops, then the theory would be consistent but
not of any interest.)

As usual, establishing soundness has three steps: deﬁning a model, deﬁning an inter-
pretation of the symbols in the model, and establishing that the axioms are true under the
interpretation.

Our class of models is (apparently) more restrictive than the theory;12 that is, the theory
is not complete with respect to this class of models. The major additional restrictions in
our model are:

I. Time must be discrete. We believe that this restriction can be lifted with minor modi-
ﬁcations to the axioms, but this is beyond the scope of this paper. We hope to address
it in future work.

II. Time must have a starting point; it cannot extend inﬁnitely far back. It would seem to
be very difﬁcult to modify our proof to remove this constraint; at the current time, it
seems to depend on the existence of highly non-standard models of set theory.

III. A knowledge accessibility link always connects two situations whose time is equal,
where “time” measure the number of clock ticks since the start. In other words, all
agents always have common knowledge of the time. In a discrete structure, this is a
consequence of the axiom of memory. Therefore, it is not, strictly speaking, an addi-
tional restriction; rather, it is a non-obvious consequence of restriction (I). If we extend
the construction to a non-discrete time line, some version of this restriction must be
stated separately.

There are also more minor restrictions; for example, we will deﬁne shared knowledge
to be the true transitive closure of knowledge, which is not expressible in a ﬁrst-order
language.

Theorem 1 below states that the axioms in this theory are consistent with essentially any
physical theory that has a model over discrete time with a starting point state and physical
actions.

Deﬁnition 1. A physical language is a ﬁrst-order language containing the sorts “situa-
tions”, “agents”, “physical actionals”, “physical actions”, “physical ﬂuents”, and “clock

12 The only way to be sure that the theory is more general than the class of models is to prove that it is consistent
with a broader class of models.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

111

times”; containing the non-logical symbols, “<”, “do”, “occurs”, “holds”, “time”, and
“communicate”; and excluding the symbols, “k_acc”, “inform”, and “sk_acc”.

Deﬁnition 2 (This is Deﬁnition A.6 of Appendix A). Let L be a physical language, let T be
a theory over L. T is an acceptable physical theory (i.e. acceptable for use in Theorem 1
below) if there exists a model M and an interpretation I of L over M such that the
following conditions are satisﬁed:

(1) I maps the sort of clock times to the positive integers, and the relation T 1 < T 2 on

clock times to the usual ordering on integers.
(2) Axioms T.1–T.9 in Table 1 are true in M under I.
(3) Theory T is true in M under I.
(4) The theory is consistent with the following constraint: In any situation S, if any
communication act is feasible, then arbitrarily many physically indistinguishable com-
munication acts are feasible.

(5) If α is a predicate symbol in L with more than one situational argument, then
α(X1 . . . Xk) holds only if all the situations among X1 . . . Xk are ordered with re-
spect to <. (Note that this condition holds both when α is “<” and α is “occurs”.)
If β(X1 . . . Xk) is a function symbol, then the above condition holds for the relation
Xk+1 = β(X1 . . . Xk).

Condition (4) no doubt seems complex, strange, and restrictive. But in fact any physical
model can be easily transformed into one satisfying this condition: take the original model
and, wherever a communicative act occurs, make an inﬁnite number of identical copies of
the subtree following the branch where the act occurs. Moreover, most reasonable physical
theories T will accept this transformation, or can be straightforwardly transformed into
theories that will accept this transformation. In fact, therefore, condition (4) is not a sub-
stantial restriction on T . The reason it is needed is that, without this condition, the physical
theory could include an axiom like, “In any situation S there is only one situation S1 such
that occurs(do(AS, communicate(AH), S, S1)” whereas our theory demands that there must
exist many such situations corresponding to the different informative acts possible in S.

Condition (5) is a technical one needed for the proof. We do not know of any reasonable
causal theories that contain predicates that do not satisfy this condition and that cannot be
deﬁned in terms of simpler predicates that satisfy this condition. We do not know whether
(5) is necessary for the conclusion of Theorem 1 to hold; however, we have not been able
to construct a proof without it.

(The KR-2004 paper claims that condition (4) can be stated in a ﬁrst-order axiom
schema. This is in error. More precisely, I have not found any ﬁrst-order axiom schema
that can be used to instantiate condition (4) that I can prove to be sufﬁcient for the theorem
below.)

Theorem 1. Let T be an acceptable physical theory, and let U be T together with axioms
K.1–K.8 and I.1–I.5. Then U is consistent.

The proof of Theorem 1 is given in Appendix A.

112

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

It is possible to strengthen Theorem 1 by adding in domain-speciﬁc axioms of knowl-
edge acquisition and the associated frame axiom over accessibility relation, as described in
Section 3.5, plus conditions on the initial knowledge and ignorance of the agents. Speciﬁ-
cally, we have the following theorem:

Theorem 2. Let T be an acceptable physical theory, and let U be the union of :

A. T .
B. Axioms K.1–K.7 and I.1–I.5.
C. A collection of domain-speciﬁc knowledge acquisition axioms of the form speciﬁed in

Section 3.5.

D. The frame axiom I.6 associated with the axioms in (C).
E. Any set of axioms K specifying knowledge or ignorance at time 0 as long as:
i. The axioms in K do not refer to any situations of time later than 0.
ii. The axioms in K are consistent with T , axioms K.1–K.3, K.5 (as regards knowing

the feasibility of actions at time 0); and the axioms in (C).

Then U is consistent.

In Appendix A, we sketch how the proof of Theorem 1 is modiﬁed to give a proof of

Theorem 2.

8. Related work

The theory presented here was originally developed as part of a larger theory of multi-
agent planning [10]. That theory includes requests as speech acts as well as informative
speech acts. However, our analysis of informative acts there was not as deep or as extensive
in scope.

As far as we know, this is the ﬁrst attempt to characterize the content of communication
as a ﬁrst-order property of possible worlds. Morgenstern [31] develops a theory in which
the content of communication is a string of characters. A number of BDI models incorpo-
rate various types of communication. The general BDI model was ﬁrst proposed by Cohen
and Perrault [4]; within that model, they formalized illocutionary acts such as “Request”
and “Inform” and perlocutionary acts such as “Convince” using a STRIPS-like represen-
tation of preconditions and effects on the mental states of the speaker and hearer. Cohen
and Levesque [5] extend and generalize this work using an full modal logic of time and
propositional attitudes. Here, speech acts are deﬁned in terms of their effects; a request, for
example, is any sequence of actions that achieves the speciﬁed effect in the mental state of
the hearer.

Update logic (e.g. [2,33]) combines dynamic logic with epistemic logic, introducing the
dynamic operator [A!]φ, meaning “φ holds after A has been truthfully announced”. The
properties of this logic have been extensively studied. Baltag et al. [1] extend this logic to
allow communication to a subset of agents, and to allow “suspicious” agents. Colombetti
[6] proposes a timeless modal language of communication, to deal with the interaction of

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

113

intention and knowledge in communication. Parikh and Ramanujam [32] present a the-
ory of messages in which the meaning of a message is interpreted relative to a proto-
col.

There is a large literature on the applications of modal logics of knowledge to a multi-
agent systems. For example, Sadek et al. [37] present a ﬁrst-order theory with two modal
operators Bi(φ) and Ii(φ) meaning “Agent i believes that φ” and “Agent i intends that
φ” respectively. An inference engine has been developed for this theory, and there is an
application to automated telephone dialogue that uses the inference engine to choose ap-
propriate responses to requests for information. However, the temporal language associated
with this theory is both limited and awkward; it seems unlikely that the theory could be
applied to problems involving multi-step planning. (The dialogue application requires only
an immediate response to a query.)

The multi-agent communication languages KQML [13] and FIPA [14] provide rich sets
of communication “performatives”. KQML was never tightly deﬁned [40]. FIPA has a for-
mal semantics deﬁned in terms of the theory of Sadek et al. [37] discussed above. However,
the content of messages is unconstrained; thus, the semantics of the representation is not
inherently connected with the semantics of the content, as in our theory.

Other modal theories of communication, mostly propositional rather than ﬁrst-order, are

discussed in [23,35,41].

9. Fagin, Halpern, Moses, and Vardi

The theory of runs and messages, developed by Fagin, Halpern, Moses, and Vardi
(FHMV) in their book Reasoning about Knowledge [12] and many papers, presents a con-
structive model of a system of agents. Each agent is characterized as a inﬁnite sequence.
The state of agent A at time T is the preﬁx of the ﬁrst T elements of the corresponding
sequence. The global state of the system at time T is the tuple of the states of all the agents
at time T. Two global system states Q1 and Q2 are knowledge accessible relative to A if
the state of A is the same in Q1 and Q2. A message is an event that modiﬁes the state of
the sender when it is sent and the state of the recipient when received. There is a protocol
that governs under what circumstances a sender may send a speciﬁed message. Messages
may be given a semantics, and agents can be prohibited from sending messages that they
know to be false.

Thus, the FHMV theory deals with much the same issues as our theory, and arrives at
many of the same rules: axioms K.1–K.3, K.7, and K.8 are valid in all FHMV models, and
FHMV have extensively studied classes of models in which axioms K.4, K.6, I.3, I.6, and
the forward implication in I.2 are valid.

Nonetheless there are many major differences between FHMV and our theory. We
divide these differences for the most part into three categories: differences in purpose,
differences in the model, and differences in the representation language. These three cate-
gories interact strongly.

114

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

9.1. Differences in purpose

The central objective of FHMV is to establish a theory for characterizing distributed
systems in terms of the “knowledge” of the components and the communications between
them. Such a theory can be used as the foundation for the formal analysis of such systems;
e.g. proving that a given class of systems is safe, in some sense; that a speciﬁed protocol
achieves a speciﬁed goal; that a given state of knowledge is inevitable or unattainable;
and so on. These proofs might be carried out automatically by reasoning in terms of the
formal language, but more often FHMV seem to be thinking about proofs carried out by
human reasoners reasoning directly about the model. In most cases, the construction of the
model is the critical issue; the deﬁnition of a formal language and statement of axioms is
secondary, or peripheral. Indeed, in [20] Halpern and Vardi argue strongly in favor of a
model-based as opposed to axiom-based approach to automated reasoning.

By contrast, our central objective here is, primarily, to deﬁne a formal language capable
of representing a wide range of statements about knowledge in commonsense domains,
and, secondarily, to demonstrate the power of this language by formulating axioms sufﬁ-
cient to justify commonsense inferences. Ultimately, the language and axioms could serve
as the basis for a representation and rule set in a symbolic knowledge base. The model
is secondary, as is evidenced by the fact that it is only described in the appendix; it is
constructed only in order to enable us to prove that our representation is coherent and our
theory is consistent. (For that reason, the inelegance, not to say ugliness, of our model does
not much matter.)

This difference also underlies our different attitudes toward completeness proofs.
FHMV construct models that are elegant and interesting in themselves; it is therefore a
worthwhile enterprise looking for axiom sets that characterize them exactly. But our model
is constructed out of scotch tape and toothpicks to ﬁt the axioms: what would be gained
by ﬁnding a complete axiom set, even if it were possible? After all, since the theory is
ﬁrst-order and consistent, we can be sure that there exists a class of models with respect to
which the theory is complete; namely, the class of all models satisfying the theory.

Another difference is that FHMV are much more interested in the properties of com-
munication itself and communication channels, and have studied in depth the properties of
systems with unreliable channels or with unknown delays. By contrast, we have been con-
tent to deal only with the case of direct speech, or, more generally, communication across
a reliable channel of ﬁxed delay.

9.2. Differences in model

The key difference between the two models might, at ﬁrst glance, seem to be a rather
technical one: FHMV uses a linear model of time13 whereas we use a branching model
of time. But that difference has many ramiﬁcations. In a linear model of time, one cannot
speak of an actor having options of many different possible actions. Therefore, it is not

13 There are a few exceptions; [18,19,27] consider FHMV models with branching time. However, even in these,
no axioms are given that require that there ever exists more than one branch in a situation; that is, these theories
permit time to branch but do not require it.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

115

possible in the FHMV model to reason about what an agent can accomplish or commu-
nicate. Inferences such as sample inference 3 (that Sam can cause Bob to know that he
will be able to play tennis) and axioms such as the forward implication of I.2 (that AS
can inform AH of anything that AS knows to be true) are not merely invalid in the FHMV
model; it is essentially impossible to formulate them in that setting. Indeed, almost all the
predictive theorems in the FHMV theory are universal, asserting that a system must attain
particular conditions, or cannot attain them; there are few existential theorems, asserting
that a system can attain a particular condition. The comprehension axiom over ﬂuents, in
this setting, can be made true, but is essentially irrelevant; since any particular system con-
tains only a restricted set of messages, the only ﬂuents that need to exist are those that are
the content of these messages.

For that reason, the FHMV model cannot be applied to automated planning under the
usual logical analysis. The usual logical analysis of the planning problem of states that a
deterministic14 plan P correctly achieves goal G starting in situation S0, if (1) P is feasible
starting in S0; that is, there exists an S1 > S0 such that P is executed over [S0, S1]; and
(2) for all such S1, G is achieved over [S0, S1]. But in a linear model of time, (1) can never
be true of two alternative but mutually exclusive plans.

Another difference in the model, reﬂecting to FHMV’s interest in communication chan-
nels, is that where we have a single action “do(AS, inform(AH, Q))” which involves both
the speaker and the hearer, FHMV separate this into two parts: one agent sends a message,
then later another agent receives it. The FHMV model is much more general.

9.3. Differences in formal language

There are many differences between the FHMV formal language and our formal lan-
guages. To some extent, this reﬂects the difference in the model; to some extent it reﬂects
the difference in purpose; to some extent it is a matter of personal preference in represen-
tation style. The representational choices all interact, which makes it difﬁcult to separate
out the different motivations behind the different choices. Among the most conspicuous
differences are:

• FHMV use modal languages of time and knowledge where we use a ﬁrst-order lan-
guage. The modal logic formulation has the advantage of supporting interesting theo-
rems about computability in the case where the base language is propositional.

• FHMV very rarely use an explicit representation of actions and events. They occasion-
ally raise the possibility of using a dynamic logic, in which there is a modal operator
corresponding to each action.

• The content of a communication is not a ﬁrst-order entity in FHMV. Indeed, there is
no representation of a message whose form reﬂects the meaning of the message; the
meanings of message are set by a meta-level operator σ .

• Agents enter the formal language of FHMV primarily as subscripts on the modal op-

erator. It is therefore not possible to quantify over agents.

14 The logical analysis of non-deterministic plans such as partially ordered plans is more complex, but also
require non-linear models of time [3].

116

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

• In general, FHMV aim at a very spare formal language; since our objective is to max-

imize expressivity, we tend to aim at a very rich one.

9.4. Other differences

One ﬁnal difference: FHMV like to study, not a one single theory at a time, but a sheaf
of variant theories, whereas we have presented a single theory. This difference, I think, is
mostly a stylistic difference in research method, and is not closely related to any of the
other differences.

Indeed, FHMV present a set of taxonomic categorizations of different theories of knowl-
edge and communication [11]. In terms of that taxonomy, our theory has the following
characteristics:

• The system is synchronous.
• Knowledge is cumulative.
• The environment does not determine the initial state of the agents.
• The system is not required to be history independent.
• Process state transitions are not independent of the environment, or of the initial envi-

ronment.

• The system is not deterministic.
• The primitive propositions are not determined either by the current global state or by

the initial global state.

• Neither the primitive propositions nor the class of agents is required to be ﬁnite.

10. Conclusions and open problems

We have developed a theory of communications which allows the content of an infor-
mative act to include quantiﬁers and logical operators and to refer to physical states, events
including other informative acts, and states of knowledge; all these in the past, present,
or possible futures. We have proven that this theory is consistent, and compatible with a
wide range of physical theories. We have examined how the theory avoids two potential
paradoxes, and discussed how these paradoxes may pose a danger when these theories are
extended. Elsewhere [10] we have shown that the theory can be integrated with a similarly
expressive theory of multi-agent planning.

The major technical problem that follows naturally on this work is to ﬁnd ways to relax
the limitations enumerated in Section 1 while preserving the consistency of the theory. Let
us discuss what is involved here a little.

Two related restrictions are particularly signiﬁcant in terms of limiting the scope of
applications of this theory: ﬁrst, that the sender AS knows when a communication has been
received (a consequence of axiom K.5) and that the hearer knows when the communication
was sent (a consequence of axiom I.3). To relax this restriction, it would be necessary, as in
FHMV and similar theories, to separate the action “do(AS, inform(AH, Q))” into an action
of sending a message and an exogenous event of receiving it. The difﬁculty is that we

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

117

have not found a reasonable reformulation of axiom I.4 in a way that we can prove avoids
paradox.

The restriction that the sender and recipient know each other is one that, in practice, is
often enough violated, and it would certainly be interesting to relax this. If you relax this
condition, then a timed communication (i.e., one satisfying I.4) gives rise to anonymous
shared knowledge. That is, the speaker and hearer know that they share the knowledge
of the content; they just do not know who they are sharing the knowledge with. (Or one
knows and the other does not.) This is analogous to common knowledge among non-rigid
sets [12, Section 6.4] but the different setting here raises different issues.

The restriction to discrete time obviously impedes the integration of this theory with
physical theories that use continuous time. The problem is that the construction of the
model in our consistency proof is inherently iterative over time, and there does not seem
to be any easy way to modify this iterative structure. The proof will work if one makes
strong assumptions about the discreteness of communicative acts; e.g., one posits that it
is only physically possible to begin a communication in a situation whose clock time is
a non-negative integer. It is conceivable that such a theory would sufﬁce for most ap-
plications; one would have to look over examples of reasoning that integrate continuous
physical reasoning with communication, which I have not yet done. I would conjecture
that axioms K.1–K.7 and I.1–I.6 are in fact consistent with a continuous model of time,
without modiﬁcation, and without the need to impose strong conditions on the physics of
communication, but I am certainly far from a proof.
Other, more far-reaching, problems include:

• Our work on integrating the theory here with a theory of planning [10] involves some
rather restrictive constraints on the protocols between agents. We would like to study
how the theory can be modiﬁed to weaken these.

• To my mind, the brass ring in this ﬁeld would be to integrate the above theory of
illocutionary acts, which describes the content of communications, with a theory of
locutionary acts, which would describe the form of communications. Achieving a the-
ory that is both general and consistent would be a major accomplishment.

Acknowledgement

The work described here comes out of and builds upon a project done in collaboration
with Leora Morgenstern, stemming from a benchmark problem that she proposed. Thanks
also to the reviewers for suggestions and corrections.

Appendix A. Proof of Theorem 1

This appendix contains a proof of Theorem 1. Speciﬁcally, we prove that if T is a
physical theory over integer-valued time satisfying a few, not very restrictive, constraints,
then T is consistent with our axioms of knowledge and of communication.

118

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Outline of appendix: in Section A.1 we give a formal deﬁnition of what we mean by a
physical theory. In Section A.2, we show how a model of a physical theory can be extended
to incorporate knowledge relations and informative actions. In Section A.3, we deﬁne the
interpretation of our theory over the new model. In Section A.4, we prove that this in-
terpretation over this model satisﬁes both the original physical theory and the axioms of
knowledge and communication.

A.1. A physical theory

A physical theory is a set of constraints on actions and ﬂuents. A communicative action
may have physical preconditions, effects, or other constraints, but these may not depend on
the content of the communication. That is, from the physical point of view, communicative
actions are distinguished only by the identity of the speaker and hearer, not the content.
Physical theories do not refer to knowledge states.

Our objective here is to prove that any reasonable physical theory is consistent with
our theory of knowledge and communication. To do this, we have to ensure that the two
theories “join up”, so to speak; speciﬁcally, that the physical theory does not impose
any constraints that are incompatible with the epistemic theory. There are three potential
sources of trouble.

• Axioms I.1, I.2, and I.4 together imply that, if AS can communicate with AH then, in
general, there are a large number of different possible communicative acts that AS can
perform. Speciﬁcally, in any situation S, if Q1 and Q2 are ﬂuents such that (a) AS
knows that both Q1 and Q2 hold; but (b) it is not shared knowledge between AS
and AH that Q1 ⇔ Q2, then the act of AS informing AH that Q1 different from the
act of AS informing AH that Q2. The physical theory could make this impossible by
asserting that only a small number of different communicative acts are feasible in S.
For instance, the statement that only two different communicative acts are feasible in
s0 could be stated in the formula

∃S1a,S1b occurs(do(as, communicate(ah)), s0, S1a) ∧
occurs(do(as, communicate(ah)), s0, S1b) ∧ S1a (cid:10)= S1b ∧
∀S1 occurs(do(as, communicate(ah)), s0, S1) ⇒ [S1 = S1a ∨ S1 = S1b]

To block this, we impose condition (4) in Deﬁnition A.6 below: a physical theory
must be consistent with the constraint that, if any communicative action is feasible in a
situation, then inﬁnitely many physically indistinguishable actions are feasible in that
situation.

• Axiom I.5 asserts the existence of a large number of ﬂuents. The physical theory could
assert that only a limited class of ﬂuents exist. E.g., the following axiom asserts that
the only ﬂuents have the form “on(A, B)” where A and B are blocks.

∀Q ∃A,B block(A) ∧ block(B) ∧ Q = on(A, B)

This is not at all far-fetched; one approach to the frame problem is to assert “The only
ﬂuents changed by action A are Q1 . . . Qk”, which leads to the same kind of problem.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

119

We get around this problem by distinguishing between physical ﬂuents and general
ﬂuents, and requiring that a physical theory can only refer to physical ﬂuents.

• Similarly, the theory of communication requires the existence of actionals “inform(AH,
Q)” and of actions “do(AS, inform(AH, Q)).” We have to make sure that the physical
theory does not simply prohibit these; e.g. assert that the only possible actionals have
the form “communicate(AH)” and “puton(A, B)”. To insure this, we require that the
physical theory can only refer to physical actions and actionals.

Deﬁnition A.1. A physical language is a ﬁrst-order language containing the sorts “situa-
tions”, “agents”, “physical actionals”, “physical actions”, “physical ﬂuents”, and “clock
times”; containing the non-logical symbols, “<”, “do”, “occurs”, “holds”, “time”, and
“communicate”; and excluding the symbols, “k_acc”, “inform”, and “sk_acc”. (The lan-
guage may or may not contain any sort or non-logical symbol other than those mentioned
above.)

Deﬁnition A.2. Let L be a physical language. Let M be a model and let I be an interpre-
tation of L in M. Let s0 and s1 be situations in M. Situation s1 is a successor of s0 if
s0 < s1 and there is no situation sm such that s0 < sm < s1.

Here, and in subsequent deﬁnitions, we implicitly use I to apply nomenclature from L
to entities in M. More formal statements of the condition “s0 < s1” above would be,
“The pair (cid:11)s0, s1(cid:12) ∈ I(‘<’)” or “The open formula SA < SB is true in M under I under
the valuation SA → s0, SB → s1”. We will use the shorter form when it is clear; when
necessary, we will be more precise.

Deﬁnition A.3. Let L, M, I be as above. Let s0, s1 be situations in M. We say that s1 is
a communication successor of s0 if s1 is a successor of s0 and there exist agents as,ah and
a situation sz such that s1 (cid:1) sz and occurs(do(as,communicate(ah)),s0,sz).

Deﬁnition A.4. Let L, M, I be as above. Let τ be a function from M to itself which
is one-to-one and onto. The function τ is said to be a situational automorphism if the
following conditions hold:

(1) If X is not a situation, then τ (X) = X.
(2) Let α be a predicate symbol in L with k arguments or a function symbol with k − 1
arguments. Note that, under standard Tarskian semantics, I(α) is a set of k-tuples of
elements of M. A tuple (cid:11)x1 . . . xk(cid:12) ∈ I(α) if and only if (cid:11)τ (x1) . . . τ (xk)(cid:12) ∈ I(α).

Deﬁnition A.5. Two situations SA and SB are indistinguishable if the following holds: Let
SSA be the part of the time structure following SA and SSB be the part of the time structure
following SB.

SSA = {S ∈ M | SA (cid:1) S}
SSB = {S ∈ M | SB (cid:1) S}

120

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Then there exists a situational automorphism τ over M such that τ (SSA) = SSB, τ (SSB) =
SSA, and for any situation S which is not in SSA and SSB, τ (S) = S.

Deﬁnition A.6. Let L be a physical language, and let T be a theory over L. T is an ac-
ceptable physical theory (i.e., acceptable for our discussion here) if there exists a model M
and an interpretation I of L over M such that the following conditions are satisﬁed:

(1) I maps the sort of clock times to the positive integers, and the relation T 1 < T 2 on

clock times to the usual ordering on integers.

(2) M satisﬁes axioms T.1–T.9 in Table 1 under T , where T.8 and T.9 are restricted to

physical actions.

(3) M satisﬁes theory T under I.
(4) For any situations s0, s1 and agents as, ah in M, if s1 is a communication successor
of s0, then there are inﬁnitely many successors of s0 that are physically indistinguish-
able from s1.

(5) If α is a predicate symbol in L with more than one situational argument, then
α(X1 . . . Xk) holds only if all the situations among X1 . . . Xk are ordered with re-
spect to <. (Note that this condition holds both when α is “<” and α is “occurs”.)
If β(X1 . . . Xk) is a function symbol, then the above condition holds for the relation
Xk+1 = β(X1 . . . Xk).

We can now state precisely the theorem that is the objective of this appendix.

Theorem 1. Let T be an acceptable physical theory, and let A be T together with ax-
ioms K.1–K.8 and I.1–I.5, and with T.8 and T.9 extended to arbitrary actions. Then A is
consistent.

Sections A.2–A.4 give the proof of this theorem.

A.2. Model construction

Sketch of model construction

The main sticking point of the proof is as follows: In order to satisfy the comprehension
axiom, we must deﬁne a ﬂuent to be any set of situations. However, if Q is a ﬂuent, then
the act of AS informing AH of Q in S1 generates a new situation; and if we generate a
separate “inform” act for each ﬂuent, then we would have a unsolvable vicious circularity.
We are rescued here by axiom I.4 together with the theorem, proven in Theorem 3 be-
low, that, in a discrete time structure satisfying the axiom of memory (K.4), knowledge
accessibility relations can only connect situations of the same time, and therefore the cur-
rent time is always common knowledge between all agents. Let q1 be any ﬂuent that holds
in situation s1. By axiom I.4, if AS informs AH of q1 over the interval [s1, s2] and AS
and AH have shared knowledge that q1 ⇔ q2 in s1, then the same act can be characterized
as AS informing AH of q2. Let t1 = time(s1). Let q2 be the ﬂuent such that holds(S, q2)
⇔ holds(S, q1) ∧ time(S) = t1. Then AS and AH have shared knowledge in s1 that q1 is
equivalent to q2. Therefore, it sufﬁces to generate an occurrence of an inform act starting

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

121

Table A.1
Construction of a model

Constructing a model
procedure model_construct(in T : an acceptable physical theory;

M: a model of theory T )

return a structure of u-situations over which we will deﬁne

a model of the extended theory.

for each p-situation PS in M, construct a u-situation US.

Label PHYS(US) = PS, time(US) = 0.

for (each agent A), deﬁne the relation K_ACC(A, ·,·)

to be some equivalence relation over the u-situations constructed above.

for (K = 0 to ∞) do {

for (each u-situation S of time K) do {

for (each p-situation PS following PHYS(S) in M)

construct a new u-situation S1 and mark PHYS(S1) = PS;

for (each pair of agents AS, AH) do {

if (in M there is an act starting in S of AS communicating to AH)
then {

SSL := the set of u-situations knowledge-accessible from S

relative to the knowledge of AS;

SSU := the set of u-situations knowledge-accessible from S
relative to the shared knowledge of AS and AH;

for (each set SS that is a subset of SSU and a superset of SSL) do {

construct an action “do(AS,inform(AH,SS))” starting in S;
construct a successor S1 of S corresponding to the execution of this action;
label PHYS(S1) to be a u-situation in M following a communicate action in PHYS(S);

}
} } }

use the axioms of knowledge to construct a valid set of

knowledge accessibility relations over the new u-situations

} return (the set of u-situations plus the set of knowledge accessibility relations)

in S1 only for ﬂuents like q2 that specify the current time, and such a ﬂuent can be iden-
tiﬁed with a set of situations of the same time as S1. This limitation allow us to break the
circularity in the construction of situations and informative acts: the content of informa-
tive acts starting at time K is a subset of the situations whose time is K; informative acts
starting in time K generate situations whose time is K + 1.

Therefore, we can use the “algorithm” shown in Table A.1 to construct a model of the
theory A. The main difference between the model M of theory T and the model U of A is
that U contains many more situations. To avoid confusion, we will call the situations of M
“p-situations” and call the situations of U “u-situations”. Each u-situation US has a corre-
sponding p-situation, denoted PHYS(US), which is physically indistinguishable from US.
The difference is that US may associate speciﬁc contents with some of the communication
actions that precede PHYS(US).

Theorem 3. If the set of clocktimes is equal to the positive integers, then for any situations
SA, SB, if k_acc(A, SA, SB) then time(SA) = time(SB).

122

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Proof. Suppose that time(SA) < time(SB) = k. By axioms T.7, T.6 and T.5, there exist
situations SB0 < SB1 < · · · < SBk−1 < SB such that time(SBi) = i. By axiom K.4 there
exist SA0 . . . SAk−1 such that k_acc(A, SAi, SBi ), SAi−1 < SAi and SAk−1 < SA; but this is
impossible, since time(SA) < k. (cid:1)

Formal construction of the model

The deﬁnitions in this section essentially amount to a formalized re-statement of the

“algorithm” in Table A.1.

Let L be a physical language. Let T be an acceptable physical theory over L. Let M
be a model and let I be an interpretation of L satisfying the conditions of Deﬁnition A.6.
The remaining deﬁnitions in this section are relative to a ﬁxed choice of L, T , M,

and I.

For convenience, for each symbol τ in T , including sorts, we use the same symbol in
block capitals to denote the image of τ under I; this is an individual, a subset, a mapping, or
a relation over M. Thus, for example, AGENTS is the image under I of the sort “agents”;
TIME is the image under I of the function symbol “time” and so on.

We now proceed to building up the set of u-situations. This construction is recursive

over time. Naturally, the base case is at time 0.

The most important and complex part of the construction is the wider class of situations

that we will need. In general a u-situation US is a pair (cid:11)S1, MM(cid:12) where:

• S1 is a p-situation. We will write S1 = PHYS(US).
• MM is a set of 4-tuples (cid:11)AS, AH, USSQ, SX(cid:12). AS and AH are agents; USSQ is a
set of u-situations; and SX is a p-situation such that SX < S1 and such that OC-
CURS(DO(AS, COMMUNICATE(AH)), SX, SZ), for some SZ that is ordered with
respect to S1. Such a tuple asserts that an action of AS informing AH of content USSQ
began in a u-situation USX < US. We write MM = MM(US).

It will be convenient to posit the existence of an atomic entity INFORM, which is not

in M, and of an entity DO.

Deﬁnition A.7. Let PS be a p-situation such that TIME(PS) = 0. A u-situation at time 0 is
a pair of the form US = (cid:11)PS, ∅(cid:12). The function ANCESTOR(US) maps a u-situation US to
a set of u-situations, the ancestors of US in the time structure.

Deﬁnition A.8. A time structure of depth 0 TS is a pair:

• The set of u-situations U_SITS = {(cid:11)PS, ∅(cid:12) | PS ∈ SITUATIONS, TIME(PS) = 0} with

one u-situation for each p-situation at time 0.

• A function K_ACC mapping any agent A ∈ AGENTS to an equivalence relation over

U_SITS.

Deﬁnitions A.9 through A.15 are mutually recursive over the depth k, successively

building up the model forward in time.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

123

Deﬁnition A.9. Let TS be a time structure of depth K. Let US be a u-situation of time K
in TS. Let S1 = PHYS(US). Let MM be a collection of 4-tuples as described above. Let
S2 be a successor to S1. The simple successor to US parallel to S2 is the pair (cid:11)S2,MM(cid:12).

Deﬁnition A.10. Let TS = (cid:11)U_SITS, K_ACC(cid:12), US, S1, MM be as above. Let AS and
AH be agents. A possible communicative content from AS to AH is a set of u-situations
USSQ of time K in U_SITS satisfying the following: let USSL be the set of u-situations
USA in TS such that (cid:11)US1, USA(cid:12) ∈ K_ACC(AS). Let USSU be the set of u-situations
USA in USSL such that there exist US0 = US, US1, US2, . . . , USN = USA, such that
for each J, (cid:11)USJ , USJ +1(cid:12) is either in K_ ACC(AS) or in K_ACC(AH). Then USSL ⊆
USSQ ⊆ USSU.

The 4-tuple (cid:11)AS, AH, USSQ, S1(cid:12) is called an inform indicator starting in S1.

Deﬁnition A.11. Let TS, US, S1, MM be as above. Let S2 be a successor of S1. Let
I = (cid:11)AS, AH, USSQ, S1(cid:12) be an inform indicator starting in S1. I possibly leads toward S2
if there exists SZ (cid:2) S2 such that OCCURS(DO(AS, COMMUNICATE(AH)), S1, SZ). An
informative sheaf in US toward S2 is a set MMX of inform indicators in US toward S2 such
that no two elements of MMX have the same speaker and the same hearer. An informative
successor to US toward S2 is a pair (cid:11)S2, MM2(cid:12) where MM2 is the union of MM with some
informative sheaf in US toward S2.

Deﬁnition A.12. Let TS, US, S1, S2 be as above. A u-successor set for US toward S2 is
the union of

• The simple successor to US, S2.
• A set USS of informative successors to US, S2 with the following property: if M is
any inform indicator in S1, then there exists an element (cid:11)S2, MM(cid:12) ∈ USS such that
M ∈ MM. That is, every inform indicator is attached to at least one successor of US.

A u-successor of a u-situation at time K is a u-situation at time K + 1. If US1 is a u-
successor of US then ANCESTOR(US1) = ANCESTORS(US) ∪ {US}.

Deﬁnition A.13. Let TS be a time-structure of depth K. A u-situation successor space
for TS is the union over [all u-situations US of depth K in TS] and [all successors S2 of
PHYS(US))] of some u-successor set for US, S2.

Deﬁnition A.14. Let TS = (cid:11)U_SITS, K_ACC(cid:12) be a time-structure of depth K. Let USA
and USB be u-situations of depth K in TS. Let US1A be a u-successor of USA and let
US1B be a u-successor of USB. Let A be an agent. Then US1B is possibly knowledge
accessible from US1A relative to A if all the following conditions hold:

• (cid:11)USA, USB(cid:12) ∈ K_ACC(A).
• For any actional Z and p-situations SXA, SYA, if OCCURS(DO(A, Z), SXA, SYA)

and SXA (cid:1) PHYS(USA), then

124

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

– If SYA < PHYS(USA), then there exist SXB, SYB such that
OCCURS(DO(A, Z), SXB, SYB) and SYB < PHYS(USB).

– If SYA = PHYS(USA), then there exists SXB such that

OCCURS(DO(A, Z), SXB, PHYS(USB)).

– If SXA < PHYS(USA) < SYA, then there exist SXB, SYB such that
OCCURS(DO(A, Z), SXB, SYB) and SXB < PHYS(USB) < SYB.

– If SXA = PHYS(USA) < SYA, then there exists SYB such that

OCCURS(DO(A, Z), PHYS(USB), SYB).

• If there exists a tuple (cid:11)AS, A, USSQ, SX(cid:12) in MM(USA) and

OCCURS(DO(AS, COMMUNICATE(AH)), SX, PHYS(USA)) then there exists a p-
situation SXB and a tuple (cid:11)AS, A, USSQ, SXB(cid:12) in MM(USB) and
OCCURS(DO(AS, COMMUNICATE(AH)), SXB, PHYS(USB)). (That is, if AS has
completed informing A of USSQ, then A knows that AS has completed informing him
of USSQ.)

Deﬁnition A.15. Let TS be a time-structure of depth K. A possible successor to TS is a
pair TS1 = (cid:11)U_SITS1, K_ACC1(cid:12) where

• U_SITS1 is a u-situation successor space for TS;
• for each agent A ∈ AGENTS, K_ACC1(A) is an equivalence relation over U_SITS1,
which is a subset of the relation, “USB is possibly knowledge accessible from USA”.
(Note that, since all the conditions on “possibly knowledge accessible” have the form
“Some property holds on US1A iff the corresponding property holds on US1B”, the
relation “possibly knowledge accessible relative to (A)” is itself always an equivalence
relation.)

TS1 is said to be of depth K + 1.

Finally, we let this construction go from time 0 to inﬁnity.

Deﬁnition A.16. Let TS0 = (cid:11)U_SITS0, K_ACC0(cid:12), TS1 = (cid:11)U_SITS1, K_ACC1(cid:12), . . . be a
sequence such that TS0 is a time structure of depth 0 and for each i, TSi+1 is a possible
successor for TSi . Then the pair

TS∞ = (cid:11)U_SITS∞, K_ ACC∞(cid:12) =

i
is a communicative model extension of M, I .

A.3. Interpretation

(cid:2)(cid:3)

(cid:4)

(cid:3)

U_SITSi,

K_ ACCj

j

Let L, M and I be as in the previous section. Let W be the language L combined with

the following additional elements:

• The sorts “ﬂuent”, “actional” and “actions”, which are super-categories of the sort

“physical ﬂuent”, “physical action”, and “physical actional”, respectively.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

125

• The symbols “k_acc”, “sk_acc”, and “inform”.

Let TS∞ = (cid:11)U_SITS∞, K_ ACC∞(cid:12) be a communicative model extension of M, I .

In this section, we deﬁne an interpretation J of W in terms of constructions over TS∞
and M. For notational convenience, we will write the image of a symbol under J by
writing it in lower-case boldface; thus, for example, sk_acc = J (“sk_acc”). We will use or-
dinary Roman font where symbols are used in preﬁx notation and are interpreted under J .
For example, if we write “occurs(E, S1, S2)” we mean the interpretation of “occurs” un-
der J . Note that, if a symbol is in L, then its interpretation under I may be different than
its interpretation under J .

We will ﬁrst discuss the construction of J informally and then proceed to the formal

deﬁnition.

The ﬁrst issue is ﬂuents. On the one hand, axiom I.5 asserts that every property of
situations α(S) has an associated ﬂuent Qα such that Qα holds in just those situations
satisfying S. The usual extensionalizing trick, therefore, is to identify Qα with the set of
u-situations satisfying α; generally, to identify ﬂuents with sets of situations. On the other
hand, to extend the theory T to the new model, we must make sure that every physical
ﬂuent in T is still a ﬂuent in the new theory. Moreover it is possible that T involves the
existence of two different ﬂuents that are in fact coextensional in terms of the situations
where they hold, but differ in terms of some other property of interest to T . Therefore, we
deﬁne a general ﬂuent as a pair of a label and a set of u-situations. For a physical ﬂuent
that is, so to speak, grandfathered from T , the label is just the physical ﬂuent; for all other
ﬂuents, the label is immaterial. A physical ﬂuent Q holds in u-situation S just if Q holds
in PHYS(S).

The second issue is the occurrence of actions. For physical actions, as for physical
ﬂuents, we use the “PHYS” mapping to guide us; a physical action E occurs from US1 to
US2 if E occurs from PHYS(US1) to PHYS(US2). For informative events, there are two
steps. First, axiom I.4 asserts that “do(as, inform(ah, q1))” and “do(as, inform(ah, q2))”
co-occur from us1 to us2 if the intersection of q1 with the set of u-situations that are sk-
accessible relative to as,ah from us1 is the same as the intersection of us2 with that set.
Second, the occurrence from us1 to us2 of the act “do(as, inform(ah, q0))” where q0 is a
subset of the sk-accessible situations is indicated in the second (MM) ﬁeld of the u-situation
us1.

Finally for simplicity we assume that there are no “pointless coincidences” between M
and the constructions we will use in J . That is to say: It is conceivable that M itself
happens to contain, as an entity, some tuple that we will want to deﬁne as an entity in
the denotation of J . Such a coincidence would cause propositions to be true and false in
ways that we do not intend. One could block this by modifying Deﬁnition A.20 below as
follows: Wherever the deﬁnition constructs an tuple, add an additional element that is not
an element of M (e.g., M itself). That will block any such coincidences. For the sake of
readability, I have omitted these.

Otherwise, the deﬁnition is pretty much straightforward.

Deﬁnition A.17. A general ﬂuent is a pair (cid:11)LABEL, USS(cid:12) where LABEL is either a phys-
ical ﬂuent or 0, and USS is a set of u-situations.

126

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Deﬁnition A.18. For any PF in PHYSICAL-FLUENTS, deﬁne PF_MAP(PF) to be the pair
(cid:11)PF, {US | US ∈ U-SITUATIONS ∧ HOLDS(PHYS(US), PF)}(cid:12).

Deﬁne PF_IMAGES = {PF_MAP(PF) | PF ∈ PHYSICAL-FLUENTS}.

Deﬁnition A.19. We deﬁne a general mapping “U2P_MAP” from constructions over TS∞
to entities in M as follows:

• If U is a u-situation, then U2P_MAP(U) = PHYS(U).
• If U = (cid:11)PF,USS(cid:12) ∈ PF_IMAGES then U2P_MAP(U) = PF.
• If U ∈ M then U2P_MAP(U) = U.
• Else U2P_MAP(U) is undeﬁned.

In reading Deﬁnition A.20 below, keep in mind that, in the standard Tarskian semantics
for ﬁrst-order logic, the denotation of a function or a predicate symbol is a set of tuples.
Similarly, we take the denotation of a sort to be a set of entities.

Deﬁnition A.20 (Long). Let L, M, I, W, U be as above. We deﬁne the function J over the
sorts and symbols of W as follows:

Sorts:

J (the sort “clock time”) = the non-negative integers.
J (the sort “agent”) = I(“agent”).
J (the sort “situation”) = the set of u-situations in U .
J (the sort “ﬂuent”) = the set of general ﬂuents.
J (the sort “physical ﬂuent”) = PF_IMAGES.
J (the sort “physical actional”) = I(“physical actional”).
J (the sort “physical action’) = I(“physical action”).
Let informative_actionals ≡ {(cid:11)INFORM, AH, Q(cid:12) | AH ∈ agent ∧ Q ∈ ﬂuent}.
Let informative_actions ≡ {(cid:11)DO, A, Z(cid:12) | Z ∈ informative_ actionals}.
J (the sort “actional”) = I(“physical actional”) ∪ informative_actionals.
J (the sort “action”) = I(“physical action”) ∪ informative_actions.
If σ is any other sort used in L, then J (σ ) = I(σ ).

Non-logical symbols:

J (“<”) (as a predicate on clock times) = the usual ordering on integers.
J (“<”) (as a predicate on situations) = {(cid:11)S1, S2(cid:12) | S1, S2 ∈ situation and S1 is
an ancestor of S2}.
J (“holds”) = {(cid:11)S, Q(cid:12) | S ∈ situation, Q = (cid:11)PF, USS(cid:12) ∈ ﬂuent and S ∈ USS}.
J (“time”) = {(cid:11)S, T (cid:12) | S ∈ situation, T ∈ clock time and S is of time T }.
J (“communicate”) = I(“communicate”).
J (“do”) = I(“do”)∪{(cid:11)A, Z, (cid:11)DO, A, Z(cid:12)(cid:12) | A ∈ agent and Z ∈ informative_actionals}.
J (“inform”) = {(cid:11)AH, Q, (cid:11)INFORM, AH, Q(cid:12)(cid:12) | AH ∈ agent and Q ∈ ﬂuent}.
J (“k_acc”) = {(cid:11)A, S1, S2(cid:12) | A ∈ agents and (cid:11)S1, S2(cid:12) ∈ K_ACC∞(A)}.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

127

J (“sk_acc”) =
{(cid:11)AS, AH, SA, SB(cid:12) |

exists(S0 = SA, S1 . . . Sk = SB) such that

for (i = 1 . . . k) either k_acc(AS, Si−1, Si ) or k_acc(AH, Si−1, Si )

}.

J (“occurs”) =
{(cid:11)E,US1,US2(cid:12) |

E ∈ action and US1, US2 ∈ situation and U S1 < U S2 and
either [E ∈ I(“physical action”) and OCCURS(E, PHYS(US1), PHYS(US2))] or

[there exist (A,AH ∈ agent; Q1, Q2 ∈ ﬂuent; USS1, USS2) such that
E = (cid:11)DO, AS, (cid:11)INFORM, AH, Q1(cid:12)(cid:12) and
Q1 = (cid:11)PF1, USS1(cid:12), Q2 = (cid:11)PF2, USS2(cid:12);
USS2 = {US ∈ USS1 | (cid:11)AS, AH, US1, US(cid:12) ∈ sk_acc},
OCCURS(DO(AS, COMMUNICATE(AH)), PHYS(US1), PHYS(US2)) and
(cid:11)AS, AH, USS2, PHYS(US1)(cid:12) ∈ MM(US2)
]

}.

Let α be any symbol in L other than those enumerated above. I(α) is a set of tu-
ples of entities in M. A tuple T (cid:18) is a replacement for tuple T if, for each index I ,
U2P_MAP(T (cid:18)[I ]) = T [I ]. Then J (α) is the set of all replacements R for the tuples
in I(α), such that any two situations in R are ordered under J (“<”).

Deﬁnition A.21. The model U is the union of clocktime, agent, situation, ﬂuent, actional,
action and M.

Note that the function U2P_MAP(X) is deﬁned for exactly those entities X which are
in J (σ ) where σ is one of the sorts in the physical language (clock times, situations,
agents, physical ﬂuents, physical actionals, physical actions, and other sorts in L).

A.4. Soundness

Throughout this section: Let L be a physical language. Let T be an acceptable physical
theory over L. Let M be a model and let I be an interpretation of L in M that satisﬁes T .
Let U and J be deﬁned as above.

We will assume that L is strongly sorted; in particular, that every variable in L is labelled
with its sort. A valuation over variables in L is required to respect the sort constraint. That
is, if µi is a variable of sort σi , and V is a valuation of µi in M then V(µi) ∈ I(σi). If W
is a valuation of µi in U then W(µi) ∈ J (σi).

Lemma A.1. For every p-situation PS in M there exists a u-situation US in U such that
PHYS(US) = PS.

Proof. By induction on TIME(PS). If TIME(PS) = 0 then there exists a correspond-
ing u-situation by Deﬁnition A.8. Suppose the statement is true for all PS such that

128

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

TIME(PS) = k. Let PS1 be a p-situation such that TIME(PS1) = k + 1. By axiom T.7,
PS1 is the successor of some situation PS0 such that TIME(PS0) = k. By the induction
hypothesis, there is a situation US0 such that PHYS(US0) = PS1. By Deﬁnition A.9 there
is a simple successor US1 of US0 such that PHYS(US1) = PS1. (cid:1)

Lemma A.2. For any u-situation U, TIME(PHYS(U)) = TIME(U). For any two u-
situations U 1, U 2 if U 1 < U 2 then PHYS(U 1) < PHYS(US2).

Proof. Immediate from the deﬁnition of J (“<”) in Deﬁnition A.20 and the deﬁnition of
“ANCESTORS” in Deﬁnitions A.7 and A.12. (cid:1)

Lemma A.3. Let µ1 . . . µk be variables in L. Let V be a valuation mapping each vari-
able µi into I(σi). Then there exists a valuation W into U such that U2P_MAP(W(µi)) =
V(µi).

Proof. Immediate from Lemma A.1 together with the construction of U2P_MAP and the
fact that, for each sort σ , U2P_MAP maps an element of J (σ ) to an element of I(σ ). (cid:1)

Lemma A.4. Let α(µ1 . . . µk) be a predicate symbol in L, including equality, where
into U . Deﬁne V(µi) =
µ1 . . . µk have sorts in L. Let W be a valuation from µi
U2P_MAP(W(µi )). Then α(µ1 . . . µk) holds in U under J , W if and only if
(a)
α(µ1 . . . µk) holds in M under I, V and (b) any two situations W(µi) and W(µj ) are
ordered under J (“<”).

Proof. We must consider separately the cases where α is (A) equality over non-situations;
(B) equality over situations; (C) the symbol “<” over clock times; (D) the symbol “<”
over situations; (E) the symbol “occurs”; (F) the symbol “holds”; (G) any other predicate
symbol in L.

(A) Equality over non-situations: from Deﬁnitions A.19 and A.20.
(B) Equality over situations: following Deﬁnitions A.19 and A.20, this amounts to the
claim that US1 = US2 if and only if PHYS(US1) = PHYS(US2) and US1 and US2
are ordered. The implication from left to right is trivial. For the implication from right
to left, consider that, if US1 and US2 are ordered but US1 (cid:10)= US2, then either US1 <
US2 or US2 < US1. If US1 < US2, then time(US1) < time(US2) so by Lemma A.2,
PHYS(US1) (cid:10)= PHYS(US2); and likewise if US2 < US1.

(C) The symbol “<” over clock times: from the fact that the interpretation is the same

under J as under I (Deﬁnition A.20).

(D) The symbol “<” over situations: analogous to (B) above.
(E) The symbol “occurs”. By Deﬁnition A.20, if E is a physical action then occurs(E, S1,

S2) occurs under J if and only if occurs(E, PHYS(S1), PHYS(S2)) under I.

(F) Let µ1, µ2 be variables of sorts “situation” and “physical ﬂuent” respectively. Let
PF = V(µ1). Since U2P_MAP(W(µ2)) = V(µ2) = PF, by Deﬁnition A.19 W(µ2) ∈
PF_IMAGES, which, by Deﬁnitions A.18 and A.19, means that W(µ2) = (cid:11)PF, {US ∈

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

129

U-SITUATIONS | HOLDS(PHYS(US), PF)}(cid:12) By Deﬁnition A.20 it follows that
(cid:11)W(µ1), W(µ2)(cid:12) ∈ J (“holds”) if and only if (cid:11)V(µ1), PF)(cid:12) ∈ I(“holds”).
(G) α is any other predicate symbol in L. Immediate from Deﬁnition A.20. (cid:1)

Lemma A.5. Let β(µ1 . . . µk) be a function symbol in L, where µ1 . . . µk have sorts
in L. Let W be a valuation from µi
into U such that, for any two situational vari-
ables µp and µq , W(µp) and W(µq ) are ordered with respect to J (“<”). Deﬁne
V(µi) = U2P_MAP(W(µi)). Then the value of β(µ1 . . . µk) in M under I, V is the image
under U2P_MAP of the value of β(µ1 . . . µk) in U under J , W.

Proof. As in the proof of Lemma A.4, we must consider separately the cases where β
is (A) the function symbol “do”; (B) the function symbol “time”; (C) any other function
symbol in L.

(A) By Deﬁnitions A.19 and A.20, if A is an agent and Z is a physical actional then
U2P_MAP(J (do(A, Z))) = J (do(A, Z)) = I(do(A, Z)) = I(do(U2P_MAP(A),
U2P_MAP(Z))). (Again, we are mildly abusing notation.)

(B) By Deﬁnitions A.19 and A.20, if US is a u-situation then U2P_MAP(J (time(US))) =

J (time(US)) = I(time(PHYS(US))) = I(time(U2P_MAP(US))).

(C) Let β be any other function symbol. Let (cid:11)x1 . . . xk, y(cid:12) be any tuple where the xi and
y are entities in the image under J of the sorts in L. Then by the last part of Deﬁni-
tion A.20, (cid:11)x1 . . . xk, y(cid:12) ∈ J (β) iff (cid:11)U2P_MAP(x1) . . . U2P_MAP(xk), U2P_MAP(y)(cid:12)
∈ I(β).
But for any terms γ1 . . . γk and any valuation W from the variables in the γ ’s
to U , the denotation of β(γ1 . . . γk) under J , W is equal to y just if the tuple
(cid:11)J (γ1) . . . J (γk), y(cid:12) is in J (β); and likewise for I.

Unfortunately, U2P_MAP does not preserve truth-values of predicates over unordered
u-situations; it is possible that U2P_MAP(US1) = U2P_MAP(US2) even though US1 (cid:10)=
US2, or that U2P_MAP(US1) < U2P_MAP(US2) even if US1 and US2 are unordered.
There is, moreover, in general no way to modify U2P_MAP to preserve inequality, since
the cardinality of the set of u-situations may be larger than the cardinality of p-situations.
Therefore, in establishing below that if an open formula with inequalities or orderings is
satisﬁable in J then it is also satisﬁable in I, it is necessary to continuously “patch” the
mapping U2P_MAP by mapping a u-situation US into some p-situation that is physically
indistinguishable from U2P_MAP(US). Fortunately, we had the foresight to provide our-
selves with plenty of these. Stating this exactly is a little involved; Deﬁnitions A.22–A.24
and Corollary A.6 through Lemma A.9 accomplish this.

Deﬁnition A.22. Let τ be a function from U to itself which is one-to-one and onto. The
function τ is said to be a physical automorphism over U if the following conditions hold:

(1) If X is not a u-situation, then τ (X) = X.

130

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

(2) Let α(µ1 . . . µk) be any atomic formula in L with free variables µ1 . . . µk. Let W and
Y be valuations from µi to U such that Y(µi) = τ (W(µi)). Then Y satisﬁes α only if
W satisﬁes α.

Note that condition (2) only applies to formulas in the physical language L, not in the

broader language.

Deﬁnition A.23. Let S1, S2 be either two p-situations or two u-situations. Situation S is
the latest common ancestor (LCA) of S1 and S2, if S (cid:1) S1, S (cid:1) S2 and S is the latest
situation with that property. Since the order relation on situations is a forest of trees, any
two situations have at most one latest common ancestor.

Deﬁnition A.24. Let (cid:11)µ1 . . . µk(cid:12) be a k-tuple of variables. Let W be a valuation of the µ’s
to U and let V be a valuation of the µ’s to M. V is said to be an image of W if the following
conditions hold:

• If µ is not a situational variable, then V(µ) = U2P_MAP(W(µ)).
• There exists a physical automorphism τ over U such that, for each pair of situational
variables µi, µj , if S is the latest common ancestor of W(µi ), W(µj ) then PHYS(τ (S))
is the LCA of V(µi ), V(µj ); and if W(µi ) and W(µj ) have no common ancestor, then
V(µi ) and V(µj ) have no common ancestor.

We say that the automorphism τ establishes the correspondence between W and V.

Corollary A.6. Let µ1 . . . µk, W, V, and τ be as in Deﬁnition A.24. For each i, V(µi) =
U2P_MAP(τ (W(µi))).

If µi

is a situational variable,

then applying Deﬁnition A.24 and choosing
j = i, since W(µi ) is the LCA of W(µi) and itself, we have U2P_MAP(τ (W(µi))) =
PHYS(τ (W(µi))) = LCA(V(µi), V(µi)) = V(µi). If µi is not a situational variable, then
the result is immediate.

Lemma A.7. Let µ1 and µ2 be situational variables in L. Let W and V be valuations of
µ1, µ2 to U and M respectively, and let V be an image of W. Then W(µ1) = W(µ2) if
and only if V(µ1) = V(µ2) and W(µ1) < W(µ2) if and only if V(µ1) < V(µ2).

Proof. Let τ be an automorphism that establishes the correspondence between W and V.
If W(µ1) = W(µ2) then V(µ1) = V(µ2), since V(µ) = PHYS(τ (W(µ))) and is thus a
function of W(µ). If W(µ1) < W(µ2) then by Lemma A.2, V(µ1) < V(µ2).

Suppose that V(µ1) = V(µ2). Thus, LCA(V(µ1), V(µ2)) = V(µ1) = V(µ2). By Deﬁ-

nition A.24 LCA(W(µ1), W(µ2)) = W(µ1) = W(µ2).

Suppose that V(µ1) < V(µ2). Thus, LCA(V(µ1), V(µ2)) = V(µ1). By Deﬁnition A.24,
LCA(W(µ1), W(µ2)) = W(µ1). Therefore W(µ1) (cid:1) W(µ2). Since V(µ1) (cid:10)= V(µ2),
it follows from the earlier part of this lemma that W(µ1) (cid:10)= W(µ2); hence W(µ1) <
W(µ2). (cid:1)

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

131

Lemma A.8. Let α(µ1 . . . µk) be a predicate symbol in L. Let W be a valuation of the µ’s
to U and let V be an image of W. Then α holds in U under W if and only if α holds in M
under V.

Proof. Let τ be an automorphism that establishes the correspondence between W and V.
Let Q(µi) = τ (W(µi)). By Deﬁnition A.22, α(µ1 . . . µk) holds under J , W if and only if
it holds under J , Q. By Lemma A.4, α(µ1 . . . µk) holds under J , Q if and only if it holds
under I, V and for any two situational variables µa, µb, Q(µa) and Q(µb) are ordered. By
Lemma A.7, Q(µa) and Q(µb) are ordered if and only if V(µa) and V(µb) are ordered;
and by condition (5) of Deﬁnition A.6, α(µ1 . . . µk) holds under I, V only if V(µa) and
V(µb) are ordered. Putting these together, it follows that α(µ1 . . . µk) holds under J , W if
and only if it holds under I, V. (cid:1)

Lemma A.9. Let β(µ1 . . . µk) be a function symbol in L, and let µk+1 be another variable.
Let W be a valuation of the µ’s to U and let V be an image of W. Then the equation
µk+1 = β(µ1 . . . µk) holds in U under W if and only if it holds in M under V.

Proof. Exactly analogous to the proof of Lemma A.8, substituting Lemma A.5 for
Lemma A.4. (cid:1)

Lemma A.10. Let α(µ1 . . . µk) be a quantiﬁer-free formula in L. Let W be a valuation of
the µ’s to U and let V be an image of W. Then α holds in U under W if and only if α holds
in M under V.

Proof. Straightforward structural induction over the form of α, using Lemmas A.8
and A.9. (cid:1)

Lemma A.11. Let µ1 . . . µk be variables whose sorts are in L. Let W be a valuation from
variables µ1 . . . µk to U and let V be an image of W. (We will include here the case where
k = 0; in that case, W and V are null valuations.) Let µk+1 be a new variable of sort σk+1.

(1) Let A be an entity in J (σk+1). Let W(cid:18) = W ∪ {µk+1 → A}. Then there exists B in M

such that V(cid:18) = V ∪ {µk+1 → B} is an image of W(cid:18).

(2) Let B be an entity in I(σk+1). Let V(cid:18) = V ∪ {µk+1 → B}. Then there exists A in U

such that V(cid:18) is an image of W(cid:18) = W ∪ {µk+1 → A}.

Proof. Let τ be a physical automorphism over U that establishes the correspondence of W
and V. If the sort of µk+1 is not a situation, then both (1) and (2) are trivial; one can
take A = B, leave the automorphism τ unchanged, and the result is immediate from the
deﬁnitions. Therefore, we may assume that the sort of µk+1 is a situation, and therefore A
is a u-situation and B is a p-situation. Without loss of generality, renumber the variables
µ1 . . . µk so that µ1 . . . µm are situational variables and the rest are not situational variables.
In both halves of the lemma, in order to show that W(cid:18) is an image of V(cid:18) we must exhibit

an automorphism τ (cid:18) that establishes this correspondence.

Let us write PT(S) = PHYS(τ (S)), and Si = W (µi) for i = 1 . . . m.

132

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Part 1. There are three cases:

Case A. m = 0. In this case, one can choose B = PHYS(A), and τ (cid:18) to be the identity

automorphism.

Case B. Suppose that A (cid:1) Si for some i. Let τ (cid:18) = τ , and let B = PT(A). For any j , let S

be the LCA of Sj and A. There are four cases:
B.i. Sj (cid:1) A. In this case S = Sj . Since W is an image of V under τ , PT(S) =

PT(Sj ) = V(µj ).

B.ii. A (cid:1) Sj . In this case S = A. Since τ is an automorphism, τ (S) (cid:1) τ (Sj ). By
Lemma A.2, PT(S) = PT(A) (cid:1) PT(Sj ) so PT(S) is the LCA of PT(A) and
PT(Sj )

B.iii. A and Sj are unordered but have LCA S. Then S is the LCA of Si and Sj , so
PT(S) is the LCA of PT(Si ) and PT(Sj ). Since PT(S) < PT(A) (cid:1) PT(Si),
it follows that PT(S) is the LCA of PT(A) and PT(Sj ).

B.iv. A and Sj have no common ancestor. Hence Si and Sj have no com-
mon ancestor. Hence PT(Si ) and PT(Sj ) have no common ancestor. Hence
PT(A) < PT(Si) and PT(Sj ) have no common ancestor.

Case C. Suppose that A does not precede any of the Sj . Consider the set LL = {LCA(A,
S1) . . . LCA(A, Sm)}. If LL is non-empty, let S be the latest situation in LL. We
have three cases:

C.i. LL is empty; that is, none of the Sj are ordered with respect to A. Then none
of the values of τ (Sj ) are ordered with respect to τ (A), so by Lemma A.4,
none of the values of PT(Sj ) are ordered with respect to PT(A). Hence, we
may choose τ (cid:18) = τ and B = PT(A).

C.ii. S is equal to one of the Si . Then for each Sj , LCA(A, Sj ) = LCA(Si, Sj ).

Thus, again, we may choose τ (cid:18) = τ and B = PHYS(τ (A)).

C.iii. S is not equal to any of the Si . Note that at least there must be one of the
Sj > S; call this Sx . Let Q be the successor of S such that Q (cid:1) A. There
are two cases:
C.iii.a. Q is not a communicative successor of S. Then τ (Q) is not a com-
municative successor of τ (S). For any Sj , if S < Sj , let Qj be
the successor of S such that Qj (cid:1) Sj . By the construction in Def-
initions A.9–A.12, it follows that PT(Q) is not equal to PT(Qj ).
Therefore PT(S) is the LCA of PT(A) and PT(Sj ). If Sj is not
ordered with respect to S, then the LCA of Sj and A is the
same as the LCA of Sj and Sx (or neither of these LCA’s ex-
ists), so again LCA(PT(A), PT(Sj )) = LCA(PT(Sx), PT(Sj )) =
PHYS(LCA(τ (Sx), τ (Sj ))) = PHYS(LCA(τ (A), τ (Sj ))). There-
fore we can choose τ (cid:18) = τ and B = PHYS(τ (A)).

C.iii.b. Q is a communicative successor of S. Here, ﬁnally, is the case
where τ may need to be modiﬁed. Let Q1 . . . Qp be all the suc-
cessors of S that precede one of the Si . By property (4) of Deﬁ-
nition A.6, there are inﬁnitely many successors of PT(S) that are
physically indistinguishable from PT(Q). Let C be one such that is
not equal to PT(Qi ) for any i. Let ω be the automorphism of M that

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

133

interchanges the subtree of p-situations following C with the sub-
tree of p-situations following PT(Q) and leaves the rest of M the
same (see Deﬁnition A.5). Let τ (cid:18) = τ ◦ ω. Let B = PHYS(τ (cid:18)(A)).
Now, suppose Sj > S. Then the LCA of Sj and A = S. Since
PHYS(τ (cid:18)(A)) is a descendant of C, which is a successor of
PHYS(τ (S)) and PHYS(τ (cid:18)(Sj )) is a descendant of PHYS(τ (Qj )
which is a different successor of PHYS(τ (S)), it follows that
the LCA(PHYS(τ (cid:18)(A)), PHYS(τ (cid:18)(Sj ))) = PHYS(τ (S)). Alterna-
tively, if Sj is not ordered with respect to S, then we still have
LCA(PHYS(τ (A)), PHYS(τ (Sj ))) = PHYS(LCA(τ (A), τ (Sj ))),
by exactly the same argument as in case C.iii.a.

Part 2. The proof of Part 2 is exactly analogous to that of Part 1, but going in the opposite
direction. (cid:1)

Lemma A.12. Let α be a prenex formula in L with m quantiﬁers and k free variables
µ1 . . . µk. Let W be a valuation from variables µ1 . . . µk to U and let V be an image of W.
Then α is true under J , W if and only if it is true under I, V.

Proof. By induction on m, the number of quantiﬁers.
If m = 0, then the statement is just Lemma A.10.
Suppose the statement is true for all formulas with m quantiﬁers. Let α be a formula

with m + 1 quantiﬁers. There are four cases:

Case 1: α is true under J , W and α has the form “∃Xβ(X)”, where β is a formula with
m quantiﬁers and k + 1 free variables. Since α is true, there exists an entity
A ∈ U and a valuation W(cid:18) = W ∪ {X → A} such that β is true under J , W(cid:18). By
Lemma A.11 there exists a valuation V(cid:18) that is an image of W(cid:18). By the inductive
hypothesis, β is true under I, V(cid:18). Hence α (that is, ∃Xβ) is true under I, V.
Case 2: α is true under I, V and α has the form “∃Xβ(X)”. Since α is true, there exists an
entity B ∈ M and a valuation V(cid:18) = V ∪ {X → B} such that β is true under I, V(cid:18).
By Lemma A.11 there exists a valuation W(cid:18) such that V(cid:18) is an image of W(cid:18). By
the inductive hypothesis, β is true under J , W(cid:18). Hence α is true under J , W.

Case 3: α is true under J , W and α has the form “∀Xβ(X)”. Let γ be the transformation
into prenex form of ¬α. Then γ is false under J , W, and γ has the form “∃Xδ”
where δ is the prenex form of ¬β. By the contrapositive to case 2 above, γ is false
under I, V; hence α is true under I, V.

Case 4: α is true under I, V and α has the form “∀Xβ(X)”. Exactly analogous to case (4),

but using the contrapositive to case 1. (cid:1)

Corollary A.13. All the physical axioms of T , axioms T.1–T.7, and axioms T.8 and T.9
restricted to physical actions are true in U under interpretation J .

Proof. Immediate from Lemma A.12, taking k = 0 and using the fact that the axioms in T
and axioms T.1–T.9 are true in M (by deﬁnition of M). (cid:1)

134

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Lemma A.14. If PS1 = PHYS(US1) and PS1 and PSZ are ordered, then there exists USZ
such that US1 and USZ are ordered, and PSZ = PHYS(USZ).

Proof. If PS1 = PSZ then USZ = US1.

If PSZ < PS1, then let USZ be the ancestor of US1 at time TIME(PSZ).
If PS1 < PSZ, then let s1 = PS1, s2 . . . sk = PSZ be p-situations such that si+1 is a
successor of si . Using Deﬁnition A.9 iteratively, let US2 be the simple successor to US1
parallel to PS2, let US3 be the simple successor to US2 parallel to PS3, and so on. Then
USk satisﬁes the desired conditions on USZ. (cid:1)

Lemma A.15. Axioms T.8 extended to general actions and K.1–K.8 are true in U under J .

(I’m just bunching together the axioms whose proof is easy.)

Proof.

Immediate from the deﬁnition of J (“occurs”) (Deﬁnition A.20).

T.8.
K.1–K.3. Immediate from Deﬁnition A.15, which requires K-ACC(A) to be an equiva-

lence relation on u-situations.

K.4–K.6. Immediate from Deﬁnition A.14, which restricts the “possibly accessible” on
situations that hold on the left-hand side of each of these relations to those that
satisfy the conditions on the right-hand side of these implications; plus Deﬁni-
tion A.15, which states that the actual knowledge accessibility relation are a subset
of the possibly accessible relations.

K.7,K.8. Immediate from the deﬁnition of J (“sk_acc”) in Deﬁnition A.20. (cid:1)

Lemma A.16. Axiom I.1 is true in U under J .

Proof. By the deﬁnition of J (“occurs”) in Deﬁnition A.20, if occurs(do(AS, inform(AH,
Q)),US1, US2) then there exist QA, PF1, USS1, PFA, USSA) such that Q1 = (cid:11)PF1, USS1(cid:12),
QA = (cid:11)PFA, USS2(cid:12), USSA = {US ∈ USS1 | (cid:11)AS, AH, US1, US(cid:12) ∈ k_acc,}, and (cid:11)AS, AH,
USS2, S1, PHYS(US2)(cid:12) ∈ MM(US2). Let USQ be the successor of US1 such that
USQ (cid:1) US2. By Deﬁnition A.9, (cid:11)AS, AH, USS2, S1, PHYS(US2)(cid:12) ∈ MM(USQ). By
Deﬁnitions A.10 and A.11, OCCURS(DO(AS, COMMUNICATE(AH)), PHYS(US1),
PHYS(US2)). By Deﬁnition A.20, occurs(do(AS, communicate(AH)), US1, US2). (cid:1)

Lemma A.17. Axiom T.9 extended to general actions is true in U under J .

Proof. Let US1, US2, USX, and USY be u-situations and E an event such that occurs(E,
US1, US2), US1 < USX < US2 and USX < USY. Let S1 = PHYS(US1) and S2 =
PHYS(US2). By Deﬁnition A.20, E is either a physical action or an informative action.
The case where E is a physical action is covered in Corollary A.13. Suppose that E is an in-
formative action; let E = (cid:11)DO, AS, (cid:11)INFORM, AH, Q1(cid:12)(cid:12). By Deﬁnition A.20 there exist
QA, PF1, USS1, PFA, USSA such that Q1 = (cid:11)PF1, USS1(cid:12), QA = (cid:11)PFA, USS2(cid:12), USSA =
{US ∈ USS1 | (cid:11)AS, AH, US1, US(cid:12) ∈ k_acc}, and (cid:11)AS,AH,USSA,S1(cid:12) ∈ MM(US2). By

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

135

Deﬁnition A.9, (cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(USX). By Axiom T.9 applied to the ac-
tion DO(AS, COMMUNICATE(AH)) there exists a situation SZ such that ordered(SZ,
PHYS(SY)), SZ > SX, and OCCURS(DO(AS, COMMUNICATE(AH), S1, SZ). By
Lemma A.14, there exists USZ such that PHYS(USZ) = SZ and USZ is ordered with
respect to USY. It follows that USZ > USX and that (cid:11)AS, AH, USS2, S1(cid:12) ∈ MM(USZ).
By Deﬁnition A.20, occurs(do(AS, inform(AH, Q)), US1, USZ). (cid:1)

Lemma A.18. Axiom I.2 is true in U under J .

Proof. Let AS, AH be agents, let US1, US2 be u-situations, and let Q = (cid:11)PF, USSQ(cid:12)
be a general ﬂuent. Let US1ACC = {USA | (cid:11)AS, AH, US1, USA(cid:12) ∈ sk_acc}, the set of
situations accessible from US1 in the shared knowledge of AS and AH. Let USSA =
USSQ ∩ US1ACC, the set of situations satisfying Q that are knowledge accessible from S1,
relative to the shared knowledge of AS and AH. Let S1 = PHYS(US1).

Suppose that occurs(do(AS, inform(AH, Q)), US1, US2). By Deﬁnition A.20 (denota-
tion of “occurs”), the tuple (cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(US2). Let USY be the successor
of US1 that is an ancestor of US2. By Deﬁnitions A.9, A.11, and A.12 it follows that
MM(USY) contains the tuple (cid:11)AS, AH, USSA, S1(cid:12). By Deﬁnition A.10, USSA is a pos-
sible communicative content for S1 from AS to AH; hence, by Deﬁnition A.10, every
situation that is knowledge accessible from US1 relative to AS is an element of USSA and
therefore an element of USSQ ⊃ USSA. By Deﬁnition A.20 (“holds”) Q holds in every
situation accessible from US1.

Conversely,

if Q holds in every situation accessible from S1,

then USSA is a
possible communicative content from AS to AH. Suppose that OCCURS(DO(AS,
COMMUNICATE(AH)), S1, S2). Let SY be the successor of S1 such that SY (cid:1) S2.
By Deﬁnition A.12,
there exists an informative successor USY of US1 such that
(cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(USY). By Axiom T.9 there exists a situation USZ (cid:2) USY
such that OCCURS(DO(AS, COMMUNICATE(AH)), US1, USZ). By Deﬁnitions A.9,
A.11, A.12 (cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(USZ). By Deﬁnition A.20, occurs(do(AS, in-
form(AH,Q)), S1, SZ). (cid:1)

Lemma A.19. Axiom I.3 is true in U under J .

Proof. Assume that occurs(do(AS, inform(AH, Q)), US1, US2) and that k_acc(AH, US2,
US2A). We need to prove that there exists a situation US1A such that occurs(do(AS,
inform(AH, Q)), US1A, US2A) and k_acc(AH, US1, US1A).

Deﬁne USSA as in the proof of Lemma A.18. By Deﬁnition A.20 (denotation
of “occurs”) since occurs(do(AS, inform(AH, Q)), US1, US2) it follows that the tu-
ple (cid:11)AS, AH, USSA, PHYS(US1)(cid:12) ∈ MM(US2) and OCCURS(DO(AS, COMMUNI-
CATE(AH)), PHYS(US1), PHYS(US2)). By Deﬁnition A.15, since k_acc(AH, US2,
US2A), US2A is possibly knowledge accessible from US2 relative to AH. By Deﬁni-
tion A.14, the tuple (cid:11)AS, AH, USSA, PS1A(cid:12) ∈ MM(US2A) for some p-situation PS1A <
PHYS(US2A), and OCCURS(DO(AS, COMMUNICATE(AH)), PS1A, PHYS(US2A)).
By Theorem 3 and axiom K.8, any two situations that are sk_acc are at the same
time. Hence, all the situations in USSA are at the same time, and by Deﬁnition A.10

136

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

this time must be equal to TIME(US1) and to TIME(US1A). Hence TIME(US1) =
TIME(US1A). By axiom A.4, since k_acc(AH, US2, US2A), US1 < US2, US1A < US2A
and TIME(US1) = TIME(US1A), it follows that k_acc(AH, US1, US1A). Hence, the set
of situations that are accessible relative to the shared knowledge of AS and AH is the same
starting from US1 as starting from US1A. Hence the act of AS informing AH of Q starting
in US1A uses the tuple (cid:11)AS, AH, USSA, PS1A(cid:12). Thus by Deﬁnition A.20, occurs(do(AS,
inform(AH, Q)), US1A, US2A). (cid:1)

Lemma A.20. Axiom I.4 is true in U under J .

Proof. Suppose that occurs(do(AS, inform(AH, QX)), US1, US2) and that QY is a ﬂuent.
Let US3 be the successor of US1 such that US3 (cid:1) US2. Let

QX = (cid:11)PFX, USSQX(cid:12)
QY = (cid:11)PFY, USSQY(cid:12)
QXA = USSQX ∩ {USA | sk_acc(AS, AH, US1, USA)}
QYA = USSQY ∩ {USA | sk_acc(AS, AH, US1, USA)}

and

By Deﬁnition A.20, (cid:11)AS, AH, QXA, PHYS(US1)(cid:12) ∈ MM(US2). By Deﬁnitions A.9,
A.11, A.12, (cid:11)AS, AH, QXA, PHYS(US1)(cid:12) ∈ MM(US3).

I. (Left to right in the two-way implication.)

Suppose that occurs(do(AS, inform(AH, QY)), US1, US2). By the same argument as
above (cid:11)AS, AH, QYA, PHYS(US1)(cid:12) ∈ MM(US3). But by Deﬁnition A.11, US3 con-
tains at most one inform indicator with starting point PHYS(US1), speaker AS, and
hearer AH. Hence QXA = QYA. That is, if situation USA is accessible from US1 rel-
ative to the shared knowledge of AS and AH, then QX holds in USA iff QY holds in
USA.

II. (Right to left in the two-way implication.)

If it is the case that

∀S1A sk_acc(AS, AH, S1, S1A) ⇒ [holds(S1A, QX) ⇔ holds(S1A, QY)]
then QXA = QYA, so by Deﬁnition A.20, occurs(do(AS, inform(AH, QY)), US1,
US2). (cid:1)

Lemma A.21. Axiom I.5 (the comprehension axiom) is true in U under J .

Proof. Immediate from Deﬁnitions A.17 and A.20, using the comprehension axiom of
Zermelo–Fraenkel set theory (also known as the “subset” or “separation” axiom). Since
there exists a well-deﬁned set U-SITS of all situations, the ZF axiom asserts that every
such formula deﬁnes a subset of U-SITS. See, for example, [15, p. 36]. (cid:1)

Considering how problematic the comprehension axiom would seem to be it may be
surprising that it has a one-step proof. In fact, one might say that the whole construction we
went through in Section A.3 is precisely tailored so that the comprehension axioms should

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

137

have a one-step proof. Nonetheless the reader may well have legitimate worries about such
a powerful axiom, that are hardly assuaged by the above proof. Let me therefore discuss
further how this whole construction works.

The key point is this: There is no circularity whatever in the whole structure of deﬁ-
nitions given in Section 3. The structure of u-situations is built up iteratively forward in
time. The label on an “inform” action A is a set of u-situations contemporaneous with the
start of A; it gives rise to a new u-situations at the next point in time. Iterating from 1 to
inﬁnity gives us a well-deﬁned and ﬁxed set U of all u-situations. Deﬁnition A.17 deﬁnes
a ﬂuent as a subset of U . Deﬁnition A.20 deﬁnes the occurrence of an inform action in
terms of these ﬂuents and of the labels on the actions. More generally, Deﬁnition A.20
deﬁnes the denotation of every symbol in W extensionally, in terms of structures over U
and M and the interpretation I; no aspect of J is deﬁned in terms of J itself (except as a
convenient abbreviation). Having adopted Deﬁnition A.20, J is now ﬁxed, and it is ﬁxed
which ﬂuents satisfy which formulas under J .

But is not it inherently circular to say, for example,

q1 is the ﬂuent such that
∀S holds(S, q1) ⇔ ∃AS,AH,S2,Q occurs(do(AS, inform(AH, Q)), S, S2)

considering that the quantiﬁcation over Q contains q1 itself? Not at all, no more than
saying

0 is the number such that, ∀XX + 0 = X

when the quantiﬁcation over X includes 0 itself. The formula above is just a description of
q1, and the axioms are sufﬁcient to guarantee that a q1 satisfying this deﬁnition exists.

Theorem 1. Let T be an acceptable physical theory, and let A be T together with ax-
ioms K.1–K.8 and I.1–I.5, and with T.8 and T.9 extended to arbitrary actions. Then A is
consistent.

Proof. We have shown that a model and an interpretation satisfying A can be con-
structed. (cid:1)

Theorem 2. Let T be an acceptable physical theory, and let U be the union of :

A. T .
B. Axioms K.1–K.7 and I.1–I.5.
C. A collection of domain-speciﬁc knowledge acquisition axioms of the form speciﬁed in

Section 3.5.

D. The frame axiom I.6 associated with the axioms in (C).
E. Any set of axioms K specifying the presence or absence of k_acc relations among

situations at time 0 as long as:
i. The axioms in K do not refer to any situations of time later than 0.
ii. The axioms in K are consistent with T , axioms K.1–K.3, K.5 (as regards knowing

the feasibility of actions at time 0), and the axioms in (C).

Then U is consistent.

138

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

Proof (Sketch). The proof of Theorem 1 needs to be modiﬁed as follows:

• In Deﬁnition A.8, initialize the K_ACC function at time 0 to satisfy the union of the

axioms in (E) with the axioms enumerated in E.ii.

• In Deﬁnition A.14, add to the conditions on US1B being possibly knowledge accessi-

ble from US1A:

For each axiom in (C) of the form “A always knows whether Φi(A, S),” the condi-
tion Φi(US1B) ⇔ Φi(US1A) must hold.

• Modify the second bullet in Deﬁnition A.15 to read, “For each agent A, K_ACC1(A)
is the relation over u-situations, ‘US1B is knowledge accessible from US1A relative
to A’ ”.

The proof that the additional axioms enumerated in Theorem 2 are satisﬁed is then

straightforward. (cid:1)

References

[1] A. Baltag, L. Moss, S. Solecki, The logic of public announcements: Common knowledge and private suspi-

cions, Unpublished, 2002, http://www.cs.indiana.edu/ftp/techreports/TR534.html.

[2] J. van Benthem, ‘One is a Lonely Number’: On the logic of communication, ILLC Tech Report 2003-07,

Institute for Logic, Language and Computation, University of Amsterdam, 2003.

[3] D. Chapman, Planning for conjunctive goals, Artiﬁcial Intelligence 32 (1987) 333–378.
[4] P.R. Cohen, C.R. Perrault, Elements of a plan-based theory of speech acts, Cognitive Sci. 3 (3) (1979) 177–

212.

[5] P.R. Cohen, H. Levesque, Intention is choice with commitment, Artiﬁcial Intelligence 42 (2–3) (1990) 213–

261.

[6] M. Colombetti, A modal logic of intentional communication, Math. Social Sci. 38 (1999) 171–196.
[7] E. Davis, Inferring ignorance from the locality of visual perception, in: Proceedings of AAAI-88, St. Paul,

MN, AAAI Press/MIT Press, Cambridge, MA, 1988, pp. 786–790.

[8] E. Davis, Representations of Commonsense Knowledge, Morgan Kaufmann, San Mateo, CA, 1990.
[9] E. Davis, A ﬁrst-order theory of communicating ﬁrst-order formulas, in: D. Dubois, C. Welty, M. Williams
(Eds.), Proceedings of the Ninth International Conference on Principles of Knowledge Representation and
Reasoning (KR2004), Whistler, BC, AAAI Press, Menlo Park, CA, 2004, pp. 235–245.

[10] E. Davis, L. Morgenstern, A ﬁrst-order theory of communication and multi-agent plans, J. Logic Comput.,

in press.

[11] R. Fagin, J. Halpern, M. Vardi, What can machines know? On the properties of knowledge in distributed

systems, J. ACM 39 (1992) 328–376.

[12] R. Fagin, J. Halpern, Y. Moses, M. Vardi, Reasoning about Knowledge, MIT Press, Cambridge, MA, 1995.
[13] T. Finin, J. Weber, M. Genesereth, D. McKay, J. McGuire, R. Pelavin, S. Shapiro, C. Beck, Speciﬁcation
of the KQML agent communication language, DARPA Knowledge Sharing Initiative External Interfaces
Working Group, 1993.

[14] FIPA, The foundation for intelligent physical agents, http://www.ﬁpa.org/, 2001.
[15] A. Fraenkel, Y. Bar-Hillel, A. Levey, A Foundations of Set Theory, second ed., Noord-Hollandsche U.M.,

Amsterdam, 1973.

[16] M. Gardner, The Unexpected Hanging and Other Mathematical Diversions, Chicago University Press,

Chicago, IL, 1991.

[17] J. Halpern, A theory of knowledge and ignorance for many agents, J. Logic Comput. 7 (1) (1997) 79–108.

E. Davis / Artiﬁcial Intelligence 166 (2005) 81–139

139

[18] J. Halpern, M. Vardi, The complexity of reasoning about knowledge and time in asynchronous systems, in:

Proc. 20th ACM Symp. on Theory of Computation, 1988, pp. 53–65.

[19] J. Halpern, M. Vardi, The complexity of reasoning about knowledge time, I: lower bounds, J. Comput.

Systems Sci. 38 (1) (1989) 195–237.

[20] J. Halpern, M. Vardi, Model checking vs. theorem proving: A manifesto, in: J. Allen, R. Fikes, E. Sandewall
(Eds.), Proceedings of the Second International Conference on Principles of Knowledge Representation and
Reasoning (KR1991), Morgan Kaufmann, San Mateo, CA, 1991, pp. 325–334.

[21] P. Hayes, The naive physics manifesto, in: D. Michie (Ed.), Expert Systems in the Micro-Electronic Age,

Edinburgh University Press, Edinburgh, 1978.

[22] H. Levesque, All I know: A study in auto-epistemic logic, Artiﬁcial Intelligence 42 (1990) 263–309.
[23] A. Lomuscio, M. Ryan, A spectrum of modes of knowledge sharing between agents, in: N. Jennings,
Y. Lespérance (Eds.), Intelligent Agents VI: Agent Theories, Architectures, and Languages, in: Lecture
Notes in Artiﬁcial Intelligence, vol. 1757, Springer, Berlin, 2000, pp. 13–26.

[24] J. McCarthy, Programs with common sense, in: M. Minsky (Ed.), Semantic Information Processing, MIT

Press, Cambridge, MA, 1968, pp. 403–418.

[25] J. McCarthy, P. Hayes, Some philosophical problems from the standpoint of artiﬁcial intelligence, in:
B. Meltzer, D. Michie (Eds.), Machine Intelligence, vol. 4, Edinburgh University Press, Edinburgh, 1969,
pp. 463–502.

[26] D. McDermott, A temporal logic for reasoning about processes and plans, Cognitive Sci. 6 (1982) 101–155.
[27] R. van der Meyden, K. Wong, Complete axiomatizations for reasoning about knowledge and branching time,

Studia Logica 75 (1) (2003) 93–123.

[28] R. Moore, Reasoning about Knowledge and Action, Tech. Note, vol. 191, SRI International, Menlo Park,

CA, 1980.

[29] R. Moore, A formal theory of knowledge and action, in: J. Hobbs, R. Moore (Eds.), Formal Theories of the

Commonsense World, ABLEX Publishing, Norwood, NJ, 1985, pp. 319–358.

[30] R. Moore, Semantical considerations on nonmonotonic logic, Artiﬁcial Intelligence 25 (1985) 75–94.
[31] L. Morgenstern, Foundations of a logic of knowledge, action, and communication, PhD thesis, NYU, 1988.
[32] R. Parikh, R. Ramanujam, A knowledge-based semantics of messages, J. Logic Language Inform. 12 (4)

(2003) 453–467.

[33] J. Plaza, Logics of public announcements, in: Z.W. Ras (Ed.), Proc. 4th International Symposium on Method-

ologies for Intelligent Systems, Charlotte, NC, North-Holland, Amsterdam, 1989.

[34] W.V.O. Quine, On a so-called paradox, Mind 62 (1953) 65–67.
linear
[35] A.S. Rao, Decision procedures

in:
M. Wooldridge, J. Müller, M. Tambe (Eds.), Intelligent Agents II: Agent Theories, Architectures, and Lan-
guages, in: Lecture Notes in Artiﬁcial Intelligence, vol. 1037, Springer, Berlin, 1995, pp. 33–48.

time belief-desire-intention logics,

for propositional

[36] R. Reiter, Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems,

MIT Press, Cambridge, MA, 2001.

[37] M.D. Sadek, P. Bretier, F. Panaget, ARTIMIS: Natural dialogue meets rational agency, in: M. Pollack (Ed.),
Proceedings of IJCAI-97, Nagoya, Japan, Morgan Kaufmann, San Francisco, CA, 1997, pp. 1030–1035.
[38] R. Scherl, H. Levesque, The frame problem and knowledge producing actions, in: Proceedings of AAAI-93,

AAAI Press/MIT Press, Cambridge, MA, 1993, pp. 689–695.

[39] R. Scherl, H. Levesque, Knowledge, action, and the frame problem, Artiﬁcial Intelligence 144 (1) (2003)

1–39.

[40] M. Wooldridge, An Introduction to MultiAgent Systems, Wiley, New York, 2002.
[41] M. Wooldridge, A. Lomuscio, Reasoning about visibility, perception, and knowledge, in: N. Jennings,
Y. Lespérance (Eds.), Intelligent Agents VI: Agent Theories, Architectures, and Languages, in: Lecture
Notes in Artiﬁcial Intelligence, vol. 1757, Springer, Berlin, 2000, pp. 1–12.

