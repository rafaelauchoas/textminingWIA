ELSEVIER 

Attificial 

intelligence  87  ( 1996)  75-143 

Artificial 
Intelligence 

From  statistical  knowledge  bases 
to  degrees  of  belief  * 

Fahiem  Bacchus  a,1, Adam  J.  Grove bp2, Joseph  Y. HalpernCv3, 
Daphne  Keller d,* 
a Computer  Science  Department,  University of  Waterloo,  Waterloo. Ont.,  Canada  N2L  3Gl 
b NEC  Research  Institute,  4  Independence  Way? Princeton,  NJ  08540,  USA 
’  IBM  Almaden  Research  Center.  650  Harry  Road,  San  Jose,  CA  95120-6099,  USA 
d  Computer  Science  Department,  Stanford  Universiry, Stanford,  CA  94305,  USA 

Received  June  1994;  revised  December  1995 

Abstract 

An  intelligent  agent  will  often  be  uncertain  about  various  properties  of  its  environment,  and 
when  acting  in  that  environment  it  will  frequently  need  to quantify  its  uncertainty.  For  example,  if 
the  agent  wishes  to  employ  the  expected-utility  paradigm  of  decision  theory  to  guide  its  actions, 
it  will  need  to  assign  degrees  of  belief  (subjective  probabilities) 
to  various  assertions.  Of  course, 
these  degrees  of  belief  should  not  be  arbitrary,  but  rather  should  be  based  on  the  information 
available  to  the  agent.  This  paper  describes  one  approach  for  inducing  degrees  of  belief  from 
very  rich  knowledge  bases,  that  can  include  information  about  particular  individuals,  statistical 
correlations,  physical  laws,  and  default  rules.  We  call  our  approach  the  random-worlds  method. 
The  method  is  based  on  the  principle  of  indifference:  it  treats  all  of the  worlds  the  agent  considers 
possible  as being  equally  likely.  It is able  to integrate  qualitative  default  reasoning  with  quantitative 
probabilistic  reasoning  by  providing  a  language  in  which  both  types  of  information  can  be  easily 
expressed.  Our  results  show  that  a  number  of  desiderata  that  arise  in  direct  inference  (reasoning 

in  the  International 

* A  preliminary version  of  this  paper  appeared 
Intelligence, 
Joint  Conference  on  Artificial 
1993  [ 51.  Some  of  this  work  was  performed  while  Adam  Grove  was  at  Stanford  University 
and  at  IBM 
Almaden  Research  Center,  and  while  Daphne  KoIIer  was  at  U.C.  Berkeley  and  at  IBM  Almaden  Research 
their  NSERC  and  IRIS 
Center.  This  research  has  been  supported 
programs,  by  the  Air  Force  Office  of  Scientific  Research 
by  an 
IBM  Graduate  Fellowship, 
and  by  a  University  of  California  President’s  Postdoctoral  Fellowship.  The  United 
States  Government 
* Corresponding 
I E-mail: 
2 E-mail:  grove@research.nj.nec.com. 
D E-mail:  halpem@almaden.ibm.com. 

( AFGSR)  under  Contract  F49620-91-C-0080, 

author.  E-mail:  koller@cs.stanford.edu. 

in  part  by  the  Canadian  Government 

to  reproduce  and  distribute 

fbacchus@logos.uwaterloo.ca. 

for  governmental 

is  authorized 

purposes. 

through 

reprints 

0004-3702/96/$15.00 
PII  SOOO4-3702( 

Copyright  @  1996  Elsevier  Science  B.V.  All  rights  reserved. 

96)  00003-3 

16 

E  Bucchus  er  cd. /Art$ciul 

Intelhgence 

87  (1996)  75-143 

to  conclusions  about  individuals) 

information 

from  statistical 
from  the  semantics  of  random  worlds.  For  example, 
of  reasoning 
assumptions 
intuitive  semantics  of  random  worlds  allow  the  method 
scope  of  many  other  nondeductive 

such  as  specificity, 
of  independence. 

reasoning  systems. 

Furthermore, 

inheritance, 

and  default  reasoning  follow  directly 
important  patterns 
and  default 
the  expressive  power  of  the  language  used  and  the 
the 

random  worlds  captures 
to  irrelevant 

to  deal  with  problems 

that  are  beyond 

information, 

indifference 

1.  Introduction 

its 
for  a 
of 

Consider 

an  agent  with  a  knowledge 

base,  KB,  who  has  to  make  decisions 
a  doctor  may  need  to  decide  on  a  treatment 

about 

information 

information, 

information, 

including: 
first-order 

e.g.,  “80%  of  patients  with  jaundice 
e.g.,  “all  patients  with  hepatitis  have  jaundice”; 
typically  have  a  fever”;  and  informa- 
the 

in  the  world.  For  example, 

statistical 
information, 

e.g.,  “patients  with  hepatitis 

the  particular  patient  at  hand,  e.g.,  “Eric  has  jaundice”. 
complete 
base  will  not  contain 

actions 
particular  patient,  say  Eric.  The  doctor’s  knowledge  base  might  contain 
different 
types, 
have  hepatitis”; 
default 
tion  about 
knowledge 
For  example, 
the  efficacy  of  a  treatment  will  almost  certainly  depend  on  the  disease, 
for  the  doctor 
More  generally, 
[ 49,67]), 
(see,  e.g., 
events.  For  example, 
as  “Eric  has  hepatitis”.  This  paper  describes  one  particular  method 
agent  to  use  its  knowledge  base  to  assign  degrees  of  belief 
call  this  method 

an  agent  must  assign  probabilities, 
the  doctor  may  wish  to  assign  a  degree  of  belief 

likelihood 
tools  for  decision  making 

the  doctor  may  be  uncertain 

the  random-worlds  method. 

individual. 
that  Eric  has.  Since 
it  is  important 
of  various  possibilities. 
theory 
such  as  decision 
or  degrees  of  belief,  to  various 
to  an  event  such 
that  allows  such  an 
in  a  principled  manner;  we 

about  a  particular 

to  apply  standard 

the  exact  disease 

In  most  cases, 

the  relative 

to  be  able 

to  quantify 

information 

about 

typically  by  attempting 

[ 641,  and  the  various  approaches 

There  has  been  a  great  deal  of  work  addressing 

inference  deals  with  the  problem  of  deriving  degrees  of  belief 

to  find  a  suitable 
the  degree  of  belief.  For  instance,  a  suitable 

aspects  of  this  genera1  problem. 
relevant  are  the  work  on  direct  inference, 
Two  large  bodies  of  work  that  are  particularly 
to  nonmonotonic  reasoning. 
going  back  to  Reichenbach 
from  statistical 
Direct 
reference  class  whose  statistics 
information, 
reference  class  for 
can  be  used  to  determine 
is 
the  patient  Eric  might  be  the  class  of  all  patients  with  jaundice.  While  direct  inference 
concerned  with  statistical  knowledge, 
reasoning,  on  the  other 
hand,  deals  mostly  with  knowledge  bases  that  contain  default  rules.  As  we  shall  argue, 
none  of  the  systems  proposed 
reasoning 
interested 
order,  default,  and  statistical 
hand,  can  deal  with  such  complex  knowledge  bases,  and  handles  several  paradigmatic 
problems 

can  deal  adequately  with 
in.  In  particular,  none  can  handle  rich  knowledge  bases  that  may  contain 

bases  we  are 
first- 
approach,  on  the  other 

the  large  and  complex  knowledge 

the  field  of  nonmonotonic 

in  both  nonmonotonic 

The  random-worlds 

and  reference-class 

or  nonmonotonic 

reference-class 

information. 

for  either 

reasoning 

reasoning 

We  now  provide  a  brief  overview  of  the  random-worlds 
in  the  knowledge  base  is  expressed 
language  augments 
[3].  Bacchus’s 

information 
by  Bacchus 

in  a  variant  of  the  language 
first-order 

introduced 
logic  by  allowing  statements 

remarkably  well. 
approach.  We  assume 

that  the 

E  Bacchus  et  al./Artificial 

Intelligence  87  (1996)  75-143 

77 

of  the  form  I]Hep(x)  1 Juun(x) 
have  hepatitis.  Notice,  however, 
unintended) 
consequence 
5.  To  avoid 
this  problem,  we  use  approximate 
IlfWx)  I Jaun(x) IL x  0.8,  read  “approximately 
hepatitis”. 
close  to  80%:  i.e.,  within  some  tolerance  T  of  0.8. 

Intuitively, 

that 

Not  only  does  the  use  of  approximate 

[Ix = 0.8,  which  says  that  80%  of  patients  with  jaundice 
this  statement  has  the  (probably 

that  in  finite  models 

the  number  of  patients  with  jaundice 

equality 

rather 

is  a  multiple  of 
than  equality,  writing 
80%  of  patients  with  jaundice  have 
is 

this  says  that  the  proportion  of  jaundiced  patients  with  hepatitis 

equality  solve  the  problem  of  unintended 
it  lets  us  express  default 

advantage: 

information. 

con- 

that  “Almost 

significant 
such  as  “Birds 

it  has  another 
sequences, 
We  interpret  a  statement 
sertion 
as  I]Fly(x)  1 Bird(x)II,  M  1.  This  interpretation 
applying  probabilistic 
of  these  approaches, 
Having  described 

semantics 

all  birds 

to  nonmonotonic 

and  Section  6  for  further  discussion. 
the  language 

fly”.  Using  approximate 

typically 

fly”  as  expressing 

the  statistical 
equality,  we  can  represent 

as- 
this 
to  various  approaches 
[ 591  for  an  overview 

is  closely  related 
logic;  see  Pearl 

for  assigning  degrees  of  belief  (which  are  essentially 

is  the  Bayesian  paradigm.  There,  one  assumes  a  space  of  possibilities 

need  to  decide  how  to  assign  degrees  of  belief  given  a  knowledge  base.  Perhaps 
widely  used  framework 
probabilities) 
a  probability  distribution  over  this  space  (the  prior  distribution), 
probabilities 
use  this  approach,  we  must  specify 
it.  In  Bayesian 
done  in  general. 
difficulty  of  making 
historic  unpopularity 
Our  approach 

in  which  our  knowledge  base  is  expressed,  we  now 
the  most 
subjective 
and 
and  calculates  posterior 
(in  our  case,  the  knowledge  base).  To 
over 
and  the  distribution 
the  space  of  possibilities 
little  consensus 
as  to  how  this  should  be 
is  that  these  decisions  are  subjective.  The 
for  the 

to  have  been  an  important 
in  symbolic  AI  [ 541. 
that  the  KB  contains  all  the  knowledge 

there  is  relatively 
the  usual  philosophy 
seems 
approach 

these  decisions 
of  the  Bayesian 

is  different.  We  assume 

reasoning, 
Indeed, 

on  what  is  known 

by  conditioning 

reason 

that  any  knowledge 

is  already 

has,  and  we  allow  a  very  expressive 
This  assumption  means 
distribution 
construction 
probability 
assertion 
the  resulting  posterior  distribution. 

included 
of  a  space  of  possibilities 

space,  we  can  use  the  Bayesian 

language 

the  agent  has  that  could 

so  as  to  make  this  assumption 

the  agent 
reasonable. 
the  prior 
in  the  KB.  As  a  consequence,  we  give  a  single  uniform 
over  it.  Once  we  have  this 
the  probability 
of  an 
the  probability  of  rp using 

and  a  distribution 
approach: 

to  compute 

influence 

(p given  KB,  we  condition  on  KB,  and  then  compute 

between 

the  probability 

to  degrees  of  belief 

space ?  One  general 

in  terms  of  a  probability 

So  how  do  we  choose 

[ 301,  is  to  give  semantics 

strategy,  discussed  by 
distri- 
the 
statistical  assertions  and  degrees  of  belief.  As  we  suggested  above, 
I Juun(n)  [Ix M 0.8  is  true  or  false  in  a  particular 

Halpern 
bution  over  a  set  of  possible  worlds,  or  first-order  models.  This  semantics  clarifies 
distinction 
a  statistical  assertion 
world,  depending 
other  hand,  a  degree  of  belief 
semantics  only  with  respect 
tribution  over  them.  There 
agent’s  KB  and  the  distribution 
ever,  we  clearly  want  there  to  be  some  connection. 
base  her  degrees  of  beliefs  on  her  information 

in  that  world.  On  the 
has 
dis- 
in  the 
between 
over  worlds  that  determines  her  degrees  of  belief.  How- 
In  particular,  we  want  the  agent  to 
about  the  world,  including  her  statistical 

in  a  particular  world-it 
to  the  entire  set  of  possible  worlds  and  a  probability 

such  as  I(Hep(x) 
on  how  many  jaundiced 
is  neither 

patients  have  hepatitis 
true  nor  false 

is  no  necessary 

the  information 

connection 

78 

E  Bacchus 

et  cd. /Arrificiul 

Intelligence 

87 

(I  996)  75-143 

information.  As  this  paper  shows, 
for  accomplishing 

this. 

the  random-worlds  method 

is  a  powerful 

technique 

To  define  our  probability 

space,  we  have  to  choose  an  appropriate 

worlds.  Given  some  domain  of  individuals,  we  stipulate 
the  set  of  all  first-order  models  over  this  domain.  That  is,  a  possible  world  corresponds 
a  particular  way  of  interpreting 
In  our  context,  we  can  assume 
In  fact,  without 

the  symbols 
in  the  agent’s  vocabulary  over  the  domain. 
that  the  “true  world”  has  a  finite  domain,  say  of  size  N. 

loss  of  generality,  we  assume 

that  the  domain 

is  { 1,.  . . , N}. 

set  of  possible 
that  the  set  of  worlds  is  simply 
to 

distribution 

Having  defined 

the  probability 

that  all  the  possible  worlds  are  equally 

This  can  be  viewed  as  an  application 

over  this  set.  For  this,  we  give  perhaps 

a  probability 
definition:  we  assume 
has  the  same  probability). 

space  (the  set  of  possible  worlds),  we  must  construct 
the  simplest  possible 
likely  (that  is,  each  world 
of  the  principle  of 
in  her 
to  prefer  one  world  over  the  other.  It 
the  principle  of 

that  all  the  agent  knows 

is  incorporated 

reasonable 
(sometimes 

to  view  all  worlds  as  equally 

as  part  of  the  very  definition 

Since  we  are  assuming 
in&j@-ence. 
knowledge  base,  the  agent  has  no  a  priori  reason 
is  therefore 
indifference 
promoted 
formalized  by  Jacob  Bernoulli 
and  applied  with  considerable 
It  later  fetl  into  disrepute  as  a  general  definition  of  probability, 
is  applied 
existence  of  paradoxes 
probability 
and  effective  way  of  assigning  degrees  of  belief  in  certain  contexts,  and  in  particular, 
the  context  where  we  restrict  our  attention 

likely.  Interestingly, 
the  principle  of  in.wfJicient  reason)  was  originally 
the  field  was  originally 
further 

to  infinite  or  continuous 
can  be  a  natural 
in 

spaces.  We  claim,  however,  that  the  principle  of  indifference 

(See  [ 291  for  a  historical  discussion.) 

the  principle  was  later  popularized 

to  a  finite  collection  of  worlds. 

that  arise  when  the  principle 

largely  because  of  the 

of  probability  when 

success  by  Laplace. 

and  others; 

also  called 

Combining 

our  choice  of  possible  worlds  with  the  principle  of  indifference,  we  obtain 
in  rp given  KB  by  condi- 
of 
It  is  easy  to  see  that,  since  each  world  is  equally 
in  p  given  KB  is  the  fraction  of  possible  worlds  satisfying 

our  prior  distribution.  We  can  now  induce  a  degree  of  belief 
tioning  on  KB  to  obtain  a  posterior  distribution 
q  according 
to  this  new  distribution. 
likely, 
KB  that  also  satisfy  cp. 

the  degree  of  belief 

and  then  computing 

the  probability 

One  problem  with  the  approach  as  stated  so  far  is  that,  in  general,  we  do  not  know  the 

the  limiting  value  of  this 

to  be  large.  We  therefore  approximate 

domain  size  N.  Typically,  however,  N  is  known 
the  degree  of  belief  for  the  true  but  unknown  N  by  computing 
degree  of  belief  as  N  grows  large.  The  result 

and  for  the  most  part  restrict 

[ 7 1 ]  and  of  Paris  and  Vencovska 

[ 361  and  Carnap 
first-order 

in  the  work 
these  authors  focus  on  knowledge  bases 
their  attention 

is  our  random-worlds  method. 
The  key  ideas  in  the  approach  are  not  new.  Many  of  them  can  be  found 
[ I 1, 121,  although 
information, 

of  Johnson 
that  contain  only 
to  unary  predicates.  Related  approaches  have  been  used  in  the  more  recent  works  of 
Shastri 
lan- 
technically  quite 
guage.  Chuaqui’s 
reasoning  upon 
different 
investigate 
notions  of  indifference 
in  this  paper.  For  example,  Carnap,  and 
very  different 
from 
in  induc- 
others  who  later  continued 
the 
tive  learning 

recent  work  [ 141  is  also  relevant.  His  work,  although 
the  idea  of  basing  a  theory  of  probabilistic 

those  we  examine 
to  develop  his  ideas,  were  very  much 

interested 
laws).  While  we  believe 

[ 571,  in  the  context  of  a  unary  statistical 

and  symmetry.  The  works  of  Chuaqui 

the  problem  of  learning  universal 

from  ours,  shares 

and  Carnap 

(especially 

issues 

E  Bacchus  et  aI./Artificial 

Intelligence  87  (1996)  75-143 

79 

question  of  learning 
on  understanding 
and  default  rules  to  inferences 
describe 

reflect 

is  very  important 

(and  generalizing) 

this  different  emphasis. 

the  process  of  going 

(see  Section  7.3))  we  have  largely  concentrated 
information 

from  statistical 
individuals.  Many  of  the  new  results  we 

about  particular 

reasoning 

reasonable 

Fortunately, 

and  the  ability 

Having  defined 

seems  entirely  adequate, 

intuitions 
Interestingly, 

the  method,  how  do  we  judge 

regarding  what  answers  are  intuitively 

in  some  strong 
types  of  queries. 

reference-class 
for  these  problems 

(of  both  types)  espouse  some  form  of  preference 

its  reasonableness? 
there  are  two  large  bodies  of  work  on  related  problems 

as  we 
from  which  we  can 
and  default  reasoning.  While  none  of  the  solu- 
the  years  of  research  have 
for 

mentioned, 
draw  guidance: 
tions  suggested 
resulted 
certain 
In  particular,  most  systems 
specific 
information 
random-worlds 
these  properties 
where  there  is  a  specific  piece  of  statistical 
to  determine 
different  desiderata, 
ference 
to  irrelevant 
worlds  provides 
specificity  and  irrelevance  heuristics.  Thus,  the  random-worlds  method 
that  can  deal  with  rich  knowledge  bases  and  still  produce 
erful  one, 
people  have  identified  as  being 

these  intuitions  often  lead  to  identical  desiderata. 
for  more 
information.  We  show  that  the 
In  fact,  in  the  case  of  random  worlds, 
theorems.  We  prove  that,  in  those  cases 
be  used 
that  should  “obviously” 
random  worlds  does  in  fact  use  this  information.  The 
and  an  indif- 
follow  as  easy  corollaries.  We  also  show  that  random 
in  many  other  contexts,  not  covered  by  the  standard 
is  indeed  a  pow- 
that 
the  answers 

approach  satisfies 
follow  from  two  much  general 

such  as  a  preference 
information 

to  ignore 
these  desiderata. 

a  degree  of  belief, 

for  more  specific 

information 

information 

reasonable 

irrelevant 

answers 

about 

in  context. 

The  rest  of  this  paper 

is  that  the  random-worlds 

in  the  work  on  reference 

reasoning.  Since  one  of  our  major  claims 

is  organized 
themes  and  problems 

the  random-worlds  method 
theorems 

this  will  help  set  our  work 
in  detail. 

In  the  next  two  sections,  we  outline 
classes  and  on 
approach 
In  Section  4, 
In  Section  5,  we  state  and  prove 
and  show  how 
the  problem  of 

some  of  the  major 
default 
solves  many  of  these  problems, 
we  describe 
a  number  of  general 
various  desiderata 
calculating  degrees  of  belief.  Using  results  from  [ 281,  we  demonstrate  a  close  connection 
between 
in  the  case  of  unary  knowledge  bases. 
Based  on  this  connection,  we  show  that  in  many  cases  of  interest  a  maximum-entropy 
computation 
that  the  maximum-entropy 
embedded 
of  the  random-worlds  method 
Section  8. 

can  be  used  to  calculate  an  agent’s  degree  of  belief.  Furthermore,  we  show 
[23]  can  be 

in  our  framework.  Finally,  we  discuss  some  possible  criticisms  and  limitations 

the  properties  of  the  approach, 
In  Section  6  we  discuss 

random  worlds  and  maximum  entropy 

in  Section  7  and  the  possible 

follow  from  these  theorems. 

impact  of  the  method 

considered 

to  default 

reasoning 

approach 

in 

in 

the  most  appropriate  ones. 
as  follows. 

2.  Reference  classes 

Strictly  speaking, 

the  only  necessary 

frequencies 
and  proportions 
is  the  simple  mathematical 
practice  we  usually  hope  for  a  deeper  connection: 

between  objective  knowledge  about 
on  the  one  hand  and  degrees  of  belief  on  the  other  hand 
the  axioms  of  probability.  But  in 
fact  that  they  both  obey 
the  latter  should  be  based  on  the 

relationship 

80 

t?  Bucchus  et  ul.  /Artijiciul 

Intelligence 

87  (I  996)  75-143 

the  random-worlds 

to  connect  statistical 

in  some  “reasonable”  way.  Of  course, 

is  precisely  a  theory  of  how  this  connection 

that  we  are 
approach 
former 
can  be  made.  But  our  approach 
advocating 
is  far  from  the  first  to  attempt 
and  degrees  of  belief. 
Most  of  the  earlier  work  is  based  on  the  idea  of  finding  a  suitable  reference  class.  In  this 
that  this  approach,  while 
section,  we  review  some  of  this  work  and  show  why  we  believe 
it  has  some 
as  a  general  methodology. 
(See  also  [8]  for  further  discussion  of  this  issue.)  We  go  into  some  detail  here,  since 
the  issues  that  arise  provide  some  motivation 
for  the  results  that  we  prove  later  regarding 
our  approach. 

is  inadequate 

information 

properties, 

reasonable 

intuitively 

2.1.  The  basic  approach 

The  earliest  sophisticated 

between  objective  sta- 
and  degrees  of  belief,  and  the  basis  for  most  subsequent  proposals, 

attempt  at  clarifying 

the  connection 

[ 641.  Reichenbach 

describes 

the  idea  as  follows: 

tistical  knowledge 
is  due  to  Reichenbach 

If  we  are  asked  to  find  the  probability 
must  first  incorporate 
or  event  may  be  incorporated 
considering 
can  be  compiled. 

the  narrowest 

[smallest] 

in  many 

the  case  in  a  suitable 

reference  class.  An  individual 

holding 

for  an  individual 

reference  classes 

reference  class  for  which  suitable 

future  event,  we 
thing 
. . ,.  We  then  proceed  by 
statistics 

suppose 

in  the  individual 

event  with  the  statistics 

that  we  want  to  determine 

in  this  quote,  Reichenbach’s 

approach  was  to  equate 
the  chosen 
from 

Although  not  stated  explicitly 
the 
degree  of  belief 
reference 
class.  As  an  example, 
(i.e.,  a  degree 
of  belief) 
that  Eric,  a  particular  patient  with  jaundice, 
has  the  disease  hepatitis.  The 
individual  Eric  is  a  member  of  the  class  of  all  patients  with  jaundice.  Hence, 
particular 
following  Reichenbach,  we  can  use  the  class  of  all  such  patients  as  a  reference  class, 
and  assign  a  degree  of  belief  equal  to  our  statistics  concerning 
the  frequency  of  hepatitis 
among 
is  80%,  then  we  would  assign  a  degree 
of  belief  of  0.8  to  the  assertion 

this  class.  If  we  know  that  this  frequency 

that  Eric  has  hepatitis. 

a  probability 

Reichenbach’s 

reference  class 

approach  consists  of  ( 1)  the  postulate 

from 
to  infer  a  degree  of  belief  with  the  same  numerical  value, 
this  reference  class  from  a  number  of 

that  we  use  the  statistics 

as  to  how  to  choose 

(2)  some  guidance 

reference  classes.  We  consider  each  point 

in  turn. 

a  particular 
and 
competing 

In  general,  a  reference  class  is  simply  a  set  of  domain 

particular 
statistics”. 
$(x) 
reason  about  belongs 

individual 
In  our  framework,  we  may  take  the  set  of  individuals 

the 
about  whom  we  wish  to  reason  and  for  which  we  have  “suitable 
a  formula 
c  we  wish  to 
that  the  particular 
to  the  class  is  then  represented  by  the  logical  assertion  $(c).  s  But 

to  be  a  reference  class.  The  requirement 

that  contains 

individuals4 

individual 

satisfying 

4 These  “individuals”  might  be  complex  objects  (such  as  sequences  of  coin  tosses)  depending  on  what  we 

take  as  primitive 

in  our  ontology. 

s Although  the  examples  in  this section  deal  with  reasoning  about  single  individuals,  in  general  both  reference- 

class  reasoning  and  random  worlds  can  be  applied  to  queries  such  as  “Did  Eric  infect  Tom”,  which  involve 

reasoning  about  a  number  of  individuals  simultaneously. 

In  such  cases  the  reference  classes  will  consist  of 

sets  of  ruples  of  individuals. 

E 

Bacchus 

et  al. 

/Artificial 

Intelligence 

87 

(1996) 

75-143 

81 

what  does  the  phrase  “suitable 
statistic” 
proportion 
some 
logical 
Then,  under 

to  be  a  closed 
or  frequency 
assertion 
this 

interpretation, 

I 9(x) 

and  Ill 

both  e(c) 
we  know 
property  $, 
the  proportion 
that  this  is  the  appropriate 
conclude  Pr(  rp( c)  > E  [a,  /I], 
(o  is  between 
(Y and  p.  Note 
depends  both  on  the  formula  q(x) 

statistics”  mean?  Suppose 

that  is  nontrivial, 

interval 
lies.  More  precisely, 

consider 

i.e.,  that  is  not  [ 0,  11,  in  which 

for  now  we  take  a  “suitable 
the 
some  query  p(c),  where  cp is 

q(x) 

individual 

IL  E  l~,Pl, 

f or  some  nontrivial 

and  c  is  a  constant,  denoting 
is  a  reference 

some 
class  for  this  query 

in  the  domain. 
if  we  know 
[a,  p].  That  is, 
that  possess 
that  also  have  property  p  is  between  LY and  /?.  If  we  decide 
approach,  we  would 
that  c  has  property 
class  for  the  query  p(c) 

i.e.,  the  probability 
that  the  appropriate 

reference  class  then,  using  Reichenbach’s 

the  class  of  individuals 

(degree  of  belief) 

interval 

and  on  the  individual 

reference 
c. 

that  c  has  property  $,  and  that  among 

in  general  be  many 

reference 

suppose  we  know  both 

that  are  ar- 

classes 
fit  (c)  and  &(c), 

Ilsp(x)  I t,bl (x)  IIx  E  [a,,  PI]  and 
In  this  case  both  +1 (x)  and  $2 (x)  are  reference  classes 
and,  depending  on  the  values  of  the  o’s  and  p’s,  they  could  assign  conflicting 

information: 

Given  a  query  a(c), 
for 
appropriate 

there  will 
it.  For  example, 

two  pieces  of  statistical 

guably 
and  we  have 
(\pp( x)  I $2 (x)  IJx E  [ a2, &I. 
for  q(c) 
degrees  of  belief 
deal  with  the  problem  of  how  to  choose  a  single 
ble  classes.  Reichenbach 
preferring 
most  specific) 
class  $1 (x) 
would  take  the  statistics 

recommended 
In  this  example, 

class. 

to  (p(c).  The  second  part  of  Reichenbach’s 

approach 

is  intended 

to 
reference  class  from  a  set  of  possi- 
the  smallest,  or 
(i.e., 
the  narrowest 
so  that  the 
if  we  know  Vx (et  (x)  +  @z(x)), 
approach,  we 
that 

is  a  subset  of  the  class  @Z(X),  then,  using  Reichenbach’s 

from  the  more  specific  reference  class  $1 (x)  and  conclude 

Pr((o(c)) 

E  [W,Pll. 

These 

two  parts  of  Reichenbach’s 

approach-using 

statistics 

taken  from  a  class  as  a 

reasonable 

in  any  absolute 

degree  of  belief  about  an  individual 
are  generally 
Of  course,  even  on  the  simplest 
be  “correct” 
widespread 
random-worlds 
applied 
random-worlds 
a  result, 
approach. 

approach  derives 

agreement 

to  simple 

approach 

and  preferring 

statistics  from  more  specific  classes- 

and  intuitively 

compelling  when  applied 

to  simple  examples. 

examples  Reichenbach’s 

sense.  Nevertheless, 

as  to  the  reasonableness 

agrees  with  both  aspects  of  Reichenbach’s 

(and  uncontroversial) 

examples.  Unlike 

these  intuitive  answers 

that  there 

it  is  impressive 

strategy  cannot  be  said 

to 
is  such 
of  the  answers.  As  we  show  later,  the 
approach  when 
the 
that  approach,  however, 
from  more  basic  principles.  As 

it  is  able  to  deal  well  with  more  complex  examples 

that  defeat  Reichenbach’s 

Despite 

statistic” 

its  successes,  Reichenbach’s 

of  preferring  more  specific 

approach  has  several  serious  problems.  For  one 
it  is  clear 
is  not  easy.  For  another, 
to  deal  with 
that  arise  with  a  rich  knowledge  base.  Nevertheless,  much  of  the  work  on 
[ 41, 
classes  by 
classes.  As  a 
all  suffer  from  a  similar  set  of  difficulties,  which  we  now 

thing,  defining  what  counts  as  a  “suitable 
that  the  principle 
the  cases 
connecting 
statistical 
421  and  of  Pollock 
elaborating 
result, 
discuss. 

information 
and  degrees  of  belief, 
[ 611,  has  built  on  Reichenbach’s 

the  manner 
these  later  approaches 

including 
ideas  of  reference 

in  which  choices  are  made  between 

that  of  Kyburg 

rarely  suffices 

information 

reference 

82 

E  Bacclu~~  et  d./Art$cial 

lnielligence 

87  (1996)  75-143 

2.2.  identifying 

reference  classes 

In 

(‘. 

I e 

like 

class 

Recall 

to  conclude 

and  IIHep(x) 

is  a  legitimate 

this  case  Jaw(x) 

arise.  Assume  we  know  Jaun(Eric) 

reference 
that  Pr(Hep(Eric)) 

that  we  took  a  reference  class  to  be  simply  a  set  for  which  we  have  “suitable 
serve  as  a  reference 
z 
/ Jaun(x)ll, 

statistics”.  But  if  any  set  of  individuals  whatsoever  can  potentially 
class  then  problems 
0.8. 
Therefore,  we  would 
member  of  the  more  speciJc  class  of  jaundiced  patients  without  hepatitis 
(Eric} 
are  quite  a  few  jaundiced 
the  proportion  of  patients 
conclusion 
most  specific 
specific  class 
suggests 
must  satisfy  additional 

for  the  query  Hep(Eric). 
=  0.8.  But  Eric  is  also  a 
together  with 
x)  ) Vn  =  Eric).  If  there 
for 
0%.  Thus,  the 
= 0.8  is  disallowed  by  the  rule  instructing  us  to  use  the 
find  a  more 
incorrect  answer.  This  example 

reference  class.  In  fact,  it  seems 
that  will  give  a  different  and  intuitively 
take  an  arbitrary 

patients  without  hepatitis, 
in  this  class  with  hepatitis: 

t  e  c  ass  defined  by  the  formula 

then  we  have  excellent  statistics 

that  we  can  almost  always 

to  be  a  reference  class; 

it  is  approximately 

that  Pr(Hep(Eric)) 

(Jaun(  x)  A 3fep( 

set  of  individuals 

that  we  cannot 

criteria. 

.,  h 

it 

1 

including 

the  problematic 

the  problem  completely.  Furthermore, 

Kyburg  and  Pollock  deal  with  this  difficulty  by  placing  restrictions  on  the  set  of  allow- 
that,  although  different,  have  the  effect  of  disallowing  disjunctive 
class  described  above.  This  approach  suf- 

able  reference  classes 
reference  classes, 
fers  from  two  deficiencies.  First,  as  Kyburg  himself  has  observed 
do  not  eliminate 
reference  classes  may  prevent  us  from  making 
example, 
appears  only 
(EEJ),  and  French-Canadians 
population,  Tay-Sachs  occurs 
using 
the  statement 
ence  classes  are  disallowed, 
reasoning. 

these  restrictions 
the  set  of  allowable 
full  use  of  the  information  we  have.  For 
by  the  predicate  73) 
extraction 
the  afflicted 
this  fact 
refer- 
in 

(represented 
Jews  of  east-European 
from  a  certain  geographic  area  (FC).  Within 
in  2%  of  the  babies.  The  agent  might  represent 

1 EZY(x) 
then  the  agent  would  not  be  able  to  use  this  information 

inherited  disease  Tay-Sachs 
in  babies  of  two  distinct  populations: 

/I.( =  0.02.  However, 

the  genetically 

if  disjunctive 

restricting 

V  FC(x) 

[41], 

IITS 

It  is  clear 

that 

if  one 

takes 

the  reference-class 

approach 

to  generating 

degrees  of 

belief,  some  restrictions 
Unfortunately, 
random-worlds 
not  forced  to  confront 

this  issue. 

on  what  constitutes 

a  legitimate 

reference  class  are  inevitable. 

it  seems  that  the  current  approaches 
to  this  problem  are  inadequate.  The 
approach  does  not  depend  on  the  notion  of  a  reference  class,  and  so  is 

2.3.  Competing 

reference  classes 

the  reference-class 

the  set  of  “legitimate” 

if  the  problem  of  defining 

classes  can  be 
Even 
the  problem  of  choosing 
the 
resolved, 
approach  must  still  address 
“right”  class  out  of  the  set  of  legitimate  ones.  The  solution 
to  this  problem  has  typically 
been  to  posit  a  collection  of  rules  indicating  when  one  reference  class  should  be  preferred 
the  most 
over  another.  The  basic  criterion 
specific  class.  But  even 
it  is  not 
that  we  know  that  between  70%  and  80%  of 
always  appropriate.  Assume, 
is  a  magpie, 
birds  chirp  and 

that  between  0%  and  99%  of  magpies  chirp.  If  Tweety 

is  the  one  we  already  mentioned: 

choose 
rule  applies, 

this  specificity 

in  the  cases 

for  example, 

to  which 

reference 

E  Bacchus et al. /Artificial Intelligence 87 (1996) 75-143 

83 

rule  would 

tell  us  to  use  the  more  specific  reference  class,  and  conclude 
is  certainly  not 
it  is  not  very  meaningful.  Had  the  0.99  been  a  1,  the  interval  would  have  been 
this  class  and  used  the  more  detailed  statistics 

the  specificity 
that  Pr(  Chilps(  Tweery)  )  E  [ 0,0.99].  Although 
trivial, 
trivial,  and  we  could  have  then  ignored 
of  [ 0.7,0.8] 

derived  from  the  class  of  birds. 

the  interval 

[0,0.99] 

This 

intuition 

that  says 

from  birds 

for  magpies 

the  specificity 

)  E  [ 0.7,0.8]. 

in  his  statistics 

is  an  alternative 

that  if  the  statistics 

in  general  with  respect 

The  knowledge  base  above  might  be  appropriate 

for  someone  who  knows  little  about 
than  in  his  statistics 
[0.7,0.8]  C_ [0,0.99],  we  know  nothing 

that  magpies  are  actually  different 

then  we  should  use  them.  That  is, 
is  captured 
intuition 

magpies,  and  so  feels  less  confidence 
for  the  class  of  birds  as  a  whole.  But  since 
to 
that  indicates 
chirping.  There 
for  the  less 
specific  reference  class  (the  class  of  birds)  are  more  precise,  and  they  do  not  contradict 
the  statistics 
we  should  conclude 
and  generalized 
Unfortunately, 

for  the  more  specific  class  (magpies), 
that  Pr(  Chirps(  Se&y) 
in  Kyburg’s  strength  rule. 
neither 
in  most  cases. 

rule  nor  its  extension  by  Kyburg’s  strength 
the  agent  generally  has  several 

are  adequate 
comparable 
systems  such  as  Kyburg’s  and  Pollock’s  simply  give  no  useful  answer  in  these  cases.  For 
and  is  a  heavy  smoker,  and 
example, 
that  Fred  has  high  cholesterol 
If  this  is  the  only  suitable 
that  15%  of  people  with  high  cholesterol  get  heart  disease. 
=  0.15. 
reference 
that  9%  of 
On 
informa- 
heavy  smokers  develop  heart  disease 
tion  about 
is  the 
that  rely  on  finding  a  single  reference  class 
single 
generate  a  trivial 
in 
that  Fred  will  contract  heart  disease 
this  case.  For  example,  Kyburg’s  system  will  generate 
[ 0, 1 ]  for  the  degree 
of  belief. 

(according 
suppose  we  then  acquire 

to  the  problem,  so  that  neither  rule  applies.  Reference-class 

the  class  of  people  with  both  attributes). 

then 
class, 
the  other  hand, 

to  all  the  systems)  Pr(Heurt-disease(F)) 

right  reference  class,  so  approaches 

range  for  the  degree  of  belief 

(but  still  have  no  nontrivial 

In  this  case,  neither  class 

In  typical  examples, 

suppose  we  know 

classes  relevant 

the  additional 

the  interval 

information 

rule 
in- 

statistical 

seems 

evidence 

is  geared 

for  its  actions 

Giving  up  completely 

in  the  face  of  conflicting 

propriate.  The  entire  enterprise  of  generating  degrees  of  belief 
the  agent  with  some  guidance 
deduction 

is  insufficient 
inferences.  The  presence  of  conflicting 

to  provide  a  definite  answer.  That 
information 

plausible 
agent  no  longer  needs  guidance.  When  we  have  several  competing 
none  of  which  dominates 
has  been  proposed, 
then 
bination  of  the  corresponding 
approach  does 
sonable  way,  giving  well-motivated 
would  fail. 

to  us  to  be  inap- 
to  providing 
(in  the  form  of  degrees  of  belief)  when 
is,  the  aim  is  to  generate 
that  the 
classes, 
rule 
that 
be  some  com- 

does  not  mean 
reference 
to  specificity  or  any  other 

statistical  values.  As  we  show  later,  the  random-worlds 

the  degree  of  belief  should  most  reasonably 

in  a  rea- 
approach 

the  others  according 

answers  even  when 

the  reference-class 

reference  classes 

indeed  combine 

from  conflicting 

the  values 

2.4.  Other  types  of  information 

We  have  already  pointed  out  the  problems 

approach 
if  more  than  one  reference  class  bears  on  a  particular  problem.  A  more  subtle  problem 

that  arise  with  the  reference-class 

84 

I?  Bucchus  et  (11. /Art$ciul 

Inrelligence  87  (1996)  75-143 

information 

information 

to  consider  only 

it  is  not  sufficient 

in  cases  where  there  is  relevant 

for  some  other  formula  u.  Then  we  would  want  Pr(p(c)) 

that  is  not  in  the  form  of  a 
to  be  a  reference  class  for  a  query  about 
l/q(x)  1 1,4(x) /Ix. 
about 
the  query  v(c).  Suppose  we  also  know 

is  encountered 
reference  class.  We  have  said  that  for  Q(x) 
p(c)  we  must  know  rl/(c)  and  have  some  statistical 
However, 
p(c)  @  a(c) 
But  this  implies 
that  all  of  the  reference  classes  for  (T(C)  are  relevant  as  well,  because 
anything  we  can  infer  about  Pr(a(  c)  )  tells  us  something  about  Pr(  cp( c)  ).  Both  Pollock 
all  of  the  reference  classes  for  any 
[61]  and  Kyburg 
the  case 
formula  g  such  that  (T(C)  @  (p(c) 
that  Pr(  a(  c)  )  6  Pr(  q(  c)  ),  nor  the 
where  it  is  known 
that  Pr(  a(  c)  )  >  Pr(  qp( c)  ). 
case  where  it  is  known 
Thus,  if  we  have  a  rich 
it  can  become  very 
reference  classes  or  even  to  define  what  qualifies  as  a 
hard  to  locate  all  of  the  possible 
reference  class. 
possible 

that  q(c)  3  LT( c),  which  implies 

that  a(c)  +  p(c),  which  implies 

[42]  deal  with  this  by  considering 

(D(C)  and  its  implications, 

they  do  not  consider 

is  known.  However, 

theory  about 

=  Pr(cr(c)). 

2.5.  Discussion 

on 

A  comparison 

random-worlds 

and  reference-class 

information.  The  reference-class 

that  we  can  always 
reference  class, 

in  the  knowledge  base.  A  strategy  based  on  identifying 

the  assumption 
the  statistics  over  a  single 

between 
approaches  can  be  made 
in  terms  of  the  use  of  local  versus  global 
approach 
is  predicated 
focus  on  a  single  piece  of  in- 
formation, 
all  the  relevant 
that  summarizes 
information 
relevant 
(“local”) 
this  to 
be  a  general 
information  we  have  avail- 
approach  are  not 
able.  In  this  sense, 
surprising.  When  generating  degrees  of  belief  from  a  rich  knowledge  base,  it  will  not 
always  be  possible 
that  captures  all  of  the  relevant 
information. 

for  the  use  of  all  the  (“global”) 
encountered 

datum  can  offer  great  efficiency,  but  of  course  we  should  not  expect 

by  the  reference-class 

to  find  a  single 

the  difficulties 

substitute 

reference 

a  single 

class 

to  remember 

that  although 

It  is  important 

class  seems 
it  arises  as  part  of  one  proposed  solutiotz  strategy  for  the  problem  of  computing 

intuitive, 
degrees  of  belief.  The  notion  of  reference  classes 
problem,  and  there  is  no  reason  for  it  to  necessarily  be  part  of  the  solution. 
we  have  tried  to  argue,  making 
solves. 

is  not  part  of  the  description  of  the 
Indeed,  as 
than  it 

it  part  of  the  solution  can  lead  to  more  problems 

the  notion  of  a  reference 

reference 

to  locate  a  single 

class  vanish.  Rather, 

the  “right” 
into  account 

Our  approach  makes  no  attempt 

class).  Thus,  all  of  the  problems  described  above 

local  piece  of  information 
that  arise  from  trying 

(a 
lo- 
that 
in  a  uniform  man- 
that  agree  with 
in  those  special  cases  where  there  is  a  single  appropriate 
in  many  situations 
these  answers  are  ob- 
from  the  simple  semantics  of  random  worlds,  with  no  ad  hoc  rules  and 

reference 
cate 
takes 
all  of  the  information 
ner.  As  we  shall  see,  the  random-worlds 
the  reference-class 
reference 
where  no  single 
tained  directly 
assumptions. 

in  the  knowledge  base 
approach  generates 

local  piece  of  information 

suffices.  Furthermore, 

it  uses  a  semantic 

to  give  reasonable 

class.  However, 

it  continues 

construction 

approach 

answers 

answers 

E  Bacchus  et al. /Artificial  Intelligence  87  (1996)  75-143 

85 

3.  Default  reasoning 

interpretation 

rather  vague,  criteria 

reasoning.  Evaluating 

of  defaults,  provides  a  well-motivated 
is  hard  because 

One  main  claim  of  this  paper  is  that  the  random-worlds  method  of  inference,  coupled 
and  successful 
there  are  many, 
In  particular,  not  all 
(such 
for  a  default 
there  are  certain 
for  the  success  of  a  new  nonmono- 

systems:  different  applications 
interpretations 

therefore  need 
that  have  gained  acceptance 

to  satisfy  different  desiderata.  Nevertheless, 

that  one  can  consider. 

require  different 

for  all  default 

such  a  claim 

as  measures 

for  success 

reasoning 

in  [53]) 

inference 

(more  often 

the  “right”  answers 

involve  getting 
than  not  involving 

system.  Some  are  general  properties  of  nonmonotonic 

(see 
to  a  small 
a  bird  called  “Tweety”).  As 
this  has  made  an  “objective”  validation  of  proposed 
to  say  the  least.  In  this  section,  we  survey  some  of  the  desired  proper- 
and  the  associated  problems  and  issues.  Of  course,  our  survey 
are  the  semantics  of  defaults,  basic 
expressive  power,  and  the 

The  areas  we  consider 

and  irrelevance, 

inheritance 

inference, 

reasoning 

with  our  statistical 
system  of  default 
sometimes 
criteria  are  appropriate 
as  some  of  the  ones  outlined 
rule,  and 
desiderata 
tonic 
Section  3.2).  Most,  on  the  other  hand, 
set  of  standard  examples 
we  claim  at  the  end  of  this  section, 
systems  difficult, 
ties  for  default  reasoning 
cannot  be  comprehensive. 
properties  of  default 
lottery  paradox. 

3.1.  Semantics  of  defaults 

to  discuss 
fashion 
about 
that  incorporate 

It  is  possible 
tremely  abstract 
some  assumptions 
sider  systems 
In  general, 
intuitive 
mally,  usually,  probably, 
ally  used  differs  significantly 
some  construct 
write 

a  default 
interpretation 

of  this 

rule 

is  that 

systems 

reasoning 

some  properties 

of  default 
(see  Section  3.2),  but  for  other  properties  we  need 
the  type  of  system  being  considered. 

in  an  ex- 
to  make 
In  particular,  we  con- 
some  notion  of  a  default  rule,  which  we  now  explain. 
,  whose 
(nor- 
the  syntax  actu- 
systems  have 
logic  [65]  we  would 

is  an  expression 
if  A  holds 

from  case  to  case,  most  default 

for  that  individual.6  While 

that  has  the  form  A(x) 

in  Reiter’s  default 

etc.)  B  holds 

for  some 

individual 

reasoning 

instance, 

typically 

-+  B(x) 

x  then 

type.  For 

A(x) 

: B(x) 

B(x) 

’ 

while  in  a  circumscriptive 

framework 

[ 521,  we  might  use 

‘v’x (A(x)  A  -Ah(x) 

=+ B(x)), 

Ab(  x) . Theories  based  on  first-order  conditional 

while  circumscribing 
do  use  the  syntax  A(x)  +  B(x).  As  we  said  in  the  introduction, 
the  statistical  assertion 
is  captured  using 
framework 
inference  have  a  notion  of  a  default 

While  most  systems  of  default 

this  default 

logic 

[ 151  often 

in  the  random-worlds 
(Ix M  1. 

]I B(  x)  1 A(x) 

rule,  not  all  of 
the  issue  of  what  the  rule  means.  In  particular,  while  all  systems  describe 

them  address 

6 We use  -+  for  a default  implication,  reserving  =S for  standard  material  implication. 

86 

F: Bucchus  et  al. /Arti’ciul 

Intelligence  87  (1996)  75-143 

how  a  default 
unintuitive 
semantics) 
becomes  very  difficult 
For  example, 
typically  G’s”.  Under 
inconsistent. 
simultaneously 
of  contradiction. 

rule  should  be  used,  some  do  not  ascribe  semantics 
to  such  rules.  Without  a  good,  intuitive 
to  judge 
as  we  mentioned 

above,  one  standard 

the  reasonableness 

semantics 
of  a  collection  of  defaulL  statements. 

(or  ascribe  only 
it 
for  defaults 

this  reading, 

reading  of  q~ 4  Cc, is  “p’s  are 
the  pair  of  defaults  A  --+ B  and  A  -+  7B  should  be 
logic,  A  -+  B  and  A  +  TB  can  be 
there  is  no  relevant  notion 
because 

such  as  Reiter’s  default 
they  are  not  “contradictory” 

In  approaches 
adopted; 

In  contrast,  our  approach  does  give  semantics 

to  defaults. 

that  covers 
Such  an  approach  enables  us,  among  other 

information, 

first-order 

default 

of  a  collection 

of  defaults  and  to  see  whether  a  default 

of  defaults.  Of  other  existing 

logic  come  closest 

to  achieving 

this  (see 

theories, 
[IO] 

information, 
things, 

In  fact,  we  use  a  single 
and  sta- 
the 
logically 
follows 
those  based  on  conditional 
for  further  discussion  of  this 

to  verify 

information. 

logic  and  semantics 
tistical 
consistency 
from  a  collection 
or  modal 
point). 

3.2.  Properties  of  default  inference 

As  we  said,  default 
on  a  number  of  important 
improve  upon 
Lehmann, 
relation  of  a  default 
such  an  inference 
work. 

and  Magidor 

reasoning 

systems  have  typically  been  measured  by  testing 
examples.  Recently,  a  few  tools  have  been  developed 

this  approach.  Gabbay 
[39]) 
reasoning 

introduced 
system,  with  respect 

[ 181  (and 

later  Makinson 
the  idea  of  investigating 

relation  might  possess.  Makinson 

to  certain  general  properties 

that 
[ 511  gives  a  detailed  survey  of  this 

them 
that 
[ 501  and  Kraus, 
the  input/output 

b 

reached 

to  assume 

clearly  depends  on  the  default 

to  this  theory.  Suppose  q  ‘is  a  default  conclusion 

to  the  particular  default  approach  being  considered. 

The  idea  is  simple.  Fix  a  theory  of  default  reasoning  and  let  KB  be  some  knowledge 
from  KB 
In  this  case,  we  write 
It  is 
in  the  same  logical 
if 
that  the  defaults  are 
i_  )  and  that  both  KB  and  p  are  first-order 
the  circumscriptive 

base  appropriate 
according 
KB  i_  p.  The  relation 
necessary 
language,  and  that  the  language  has  a  notion  of  valid  implication.  Thus,  for  example, 
we  are  considering 
fixed  (and 
or  propositional 
policy  must  also  be  fixed  and 
beginning  of  Section  3.3.) 

logic  or  e-semantics,  we  must  assume 

that  KB  and  q  are  both  expressed 

in  the  case  of  circumscription, 

(See  also  the  discussion 

theory  being  considered. 

formulas.  Similarly, 

into  the  notion  of 

in  this  context 

incorporated 

incorporated 

into  k 

default 

at  the 

With  this  machinery  we  can  state  a  few  desirable  properties  of  default 

in  a 
of  the  (very  diverse)  details  of  such  theories.  There  are  five 

theories 

that  have  been  viewed  as  being  particularly  desirable 

[ 391: 

l  Right  Weakening.  If  qo +  I++ is  logically  valid  and  KB  k  p,  then  KB  i_  @. 
l  Reflexivity.  KB  k  KB. 

l  Left  Logical  Equivalence. 

If  KB  H  KB’  is  logically  valid,  then  KB  /-- p  if  and  only 

if  KB’  b  40. 

l  Cut.  If  KB  k  6, and  KB  A  0  f-- cp then  KB  i_  p. 
l  Cautious  Monotonic&. 

If  KB  k  6  and  KB  k  p  then  KB  A  6’ b  40. 

way  that  is  independent 
properties  of  k 

E  Bacchus  et  al.  /Artificial 

Intelligence 

87  (1996)  75-143 

87 

the  scope  of  this  paper 

to  stress  Cut  and  Cautious  Monotonicity, 

it  is  beyond 

While 
want 
results.  They 
from  KB,  where  “safely” 
(via  k  )  from  KB  A  B  is  precisely 

to  defend 
since 

these  criteria 
they  will  be  useful 

(see 

[ 39]),  we  do 
in  our  later 
0  that  we  can  derive 
derivable 

is  interpreted 

to  mean 

that  the  set  of  conclusions 

the  same  as  that  derivable 

from  KB  alone. 

tell  us  that  we  can  safely  add  to  KB  any  conclusion 

As  shown 

in  [ 391,  numerous  other  conditions 

can  be  derived  from  these  properties. 

For  example,  we  can  prove: 

l And.  If  KB  t_  40 and  KB  k  I+? then  KB  k  cp A  t+b. 

Other  plausible  properties,  however,  do  not  follow  from  these  basic  five.  For  example, 
the  following  property  captures 

reasoning  by  cases: 

l  Or.  If  KB  t_  p  and  KB’  k  cp, then  KB  V KB’  k  rp. 
Perhaps 

the  most  interesting  property 

[ 391.  Note 

is  what  has  been  called  RationaE  Monotonicity 

that  does  not  follow  from  the  basic  five  prop- 
that  the  property  of 
that  KB  /-  cp implies  KB  A  0  b  qo, 
should  satisfy 

erties 
(full)  monotonicity,  which  we  do  not  want,  says 
no  matter  what  0  is.  It  has  been  argued 
that  default 
property 
one  situation  where  we  may  believe 
“irrelevance”, 
While 
that  8  should  not  affect  the  conclusions  we  can  derive  from  KB  is  if  6  is  not  implausible 
given  KB,  i.e.,  if  it  is  not  the  case  that  KB  b  -8 
following  property  asserts  that  monotonicity 
knowledge  base: 

(see  Section  3.3  for  an  example).  The 
holds  when  adding  such  a  formula  8  to  our 

in  those  cases  where  0  is  “irrelevant” 
to  characterize 

the  same 
between  KB  and  rp. 

reasoning 
to  the  connection 

it  is  difficult 

l Rational  Monotonic&y. 

If  KB  k  ~0 and  it  is  not  the  case  that  KB  k  -8, 

then  KB  A 

0  i-  50. 

Rational  Monotonicity 

is  a  fairly  strong  property,  and 

agreed  upon  (see  [ 5 1 ]  for  a  discussion, 
and  Magidor 
people,  notably  Lehmann 
of  this  principle.  One  advantage  of  Rational  Monotonicity 
noncontroversial 
involving  property 
further 
slightly  weakened  version  of  Rational  Monotonicity. 

patterns  of  reasoning 
in  the  next  section.  As  is  demonstrated 

not  universally 
and  some  weakened  versions).  However,  several 
for  the  desirability 
[47],  have  argued  strongly 
is  that  it  covers  some  fairly 
this 
inheritance.  We  explore 
in  Section  5.1,  our  approach  satisfies  a 

is  certainly 

theories.  There  are  certainly  applications 

The  set  of  properties  we  have  discussed  provides  a  simple,  but  useful,  system  for  clas- 
in  which  some  of  the  properties 
it  does  not  satisfy 
[ 511.  (We  briefly  discuss  one  of 
logic  in  the  next  section.)  Nevertheless,  many 
if 

sifying  default 
are  inappropriate;  Reiter’s  default 
Cautious  Monotonicity,  Or,  or  Rational  Monotonicity 
the  consequent 
of  default 
people  would  argue  that  the  five  core  properties  given  above  constitute 
incomplete, 

is  still  popular  even  though 

for  mainstream  default 

set  of  desiderata 

disadvantages 

a  reasonable, 

theories. 

logic 

3.3.  Specijcity 

and  inheritance 

As  we  have  pointed  out,  systems  of  default 

reasoning  have  particular  mechanisms 

for  expressing 
other 
a  particular 
we  denote 

default 

rules.  A  collection 

information) 

forms  a  default 

theory 

default 

theory  KBd,f  might  contain 

this  by  writing 

(A(x) 

-+  B(x)) 

(perhaps 

in  conjunction  with 
of  such  rules 
(or  default  knowledge  base).  For  example, 
the  default  “A’s  are  typically  B’s”; 
is  used 

E  KBd+  A  default 

theory  K&f 

88 

F: Bucchus et nl./Artijicial Intelligence 87 (1996)  75-143 

system 

indicate 

in  order 

reasoning 

from  various  premises 
the  above  default  might 

to  reason 
by  a  default 
to  default 
conclusions.  For  example,  a  theory  K&,f  containing 
infer  B(c) 
from  A(c).  Let  k&f 
generated  by  a  particular 
default  reasoning 
that  this  default 
indicates 
the  default 
reasoning 
theory  KBd+  In  this  section  we  examine  some  additional  properties  we  might  like  FJ~~ 
to  satisfy. 
Clearly, 

the  input/output 
system  that  uses  K&j+  Thus,  A(c)  k&t  B(c) 
is  able 

the  presence  of  a  default 

the  premise  A(c) 

to  conclude  B(c) 

relationship 

system 

using 

from 

rule 
system  will 
unless 

(or  should) 

in  a  theory  does  not  necessarily  mean 
apply 
is  known  about 
for  any  default 

that 
to  any  par- 
that 
indi- 
reasoning 

special 
requirement 

that  rule 

something 

to  be  an  obvious 

reasoning 
individual.  Nevertheless, 
the  following 

the  associated  default 
ticular 
vidual, 
system: 

seems 

l  Direct  lnfereuce 

for  Defaults. 

If  (A(x) 

+  B(x)) 

E  KBd,t  and  KBd,f  contains  no 

assertions  mentioning 

c,  then  A(c)  kdet  B(c) 
has  been  previously  discussed  by  Poole 

This  requirement 
erty  of  Conditioning.  We  have  chosen  a  different  name 
directly 

to  earlier  notions  arising 

in  work  on  direct  inference. 

. 

[ 631,  who  called 

it  the  prop- 
the  property  more 

that  relates 

We  view  Direct  Inference 

for  Defaults  as  stating  a  (very  weak)  condition 

theory  should  behave  on  some  of  the  simpler  problems 

default 
classes  and  default  properties.  Consider 
default  knowledge  base  KBflY is 

the  following 

standard  example, 

for  how  a 
involving  hierarchies  of 
in  which  our 

Bird(x) 

+  Fly(x) 

, 

fenguin(  x)  -+  -Fly(X) 

, 

‘dx (Penguin(x) 

+  Bird(x) 

) 

Should  Tweety 

the  penguin 
or  the  property  of  not  flying 

inherit 
from 

the  property  of  flying 

the  class  of  birds, 
the  class  of  penguins?  For  any  system  satisfying 

from 

lrlference 

treats  universals 

in  a  reasonable  manner, 

for  Defaults  we  must  have  Penguin(  Tweety)  hX 

lFly(  Tweety).  So 
Direct 
to 
long  as  the  system 
Penguin(  Tweety)  A Bird(  Tweety)  htJ  lF!\(  Tweety).  Thus  we  see  that  if  a  system  satis- 
satisfies  a  form  of  speciJicit_v---the 
fies  Direct  Inference  for  Defaults, 
then  it  automatically 
preference 
is,  of  course,  di- 
that  we  saw  in  the  context 
rectly 
of  reference-class 
in 
default  reasoning. 

for  more  specific  defaults.  Specificity 

is  one  of  the  least  controversial 

for  more  specific  subsets 

this  will  be  equivalent 

reasoning.  Specificity 

in  default  reasoning 

to  the  preference 

desiderata 

related 

logic  and  circumscription 

In  approaches  such  as  default  reasoning  or  circumscription, 

the  most  obvious  encoding 
of  these  defaults  satisfies  neither  Direct  Inference  for  Defaults  nor  specificity.  However, 
default 
to 
arrange  specificity 
in  default  logic,  this  can  be  done  by  means 
[ 661.  There  is  a  cost  to  doing  this,  however:  adding  a  default  rule 
of  nonnormal  defaults 
to  enforce 
can  require 
the  desired  precedences. 

that  all  older  default  rules  be  reexamined, 

are  certainly  powerful  enough 

if  we  wish.  For  example, 

and  possibly  changed, 

for  us  to  be  able 

Direct  Inference  for  Defuults  is  a  weak  principle, 

is  no  default 

that  fits  the  case  at  hand  perfectly.  Suppose  we  learn 

since  in  most  interesting  cases  there 
is  a 

that  Tweety 

E  Bacchus  et  al./Art$cial 

Intelligence  87  (1996)  75-143 

89 

yellow  penguin.  Should  we  still  conclude  that  Tweety  does  not  fly?  That  is,  should  we 
conclude  Penguin( Tweety) AYellow(  Tweety)  bJ  ~Fly(Tweety)?  Most people  would  say 
we  should,  because  we  have  been  given  no  reason  to  suspect  that  yellowness  is relevant 
to  flight.  In  other  words,  in  the  absence  of  more  specific  information  about  yellow 
penguins  we  should  use  the  most  specific  superclass  for  which  we  do  have  knowledge, 
namely  penguins.  The  inheritance  property,  i.e.,  the  ability  to  inherit  defaults  from 
superclasses,  is  a  second  criterion  for  successful  default  reasoning,  and  is  not  provided 
by  Direct  Inference for  Defaults. 

In  some  sense,  we  can  view  Rational  Monotonicity  as  providing  a  partial  solution 
to  this  problem  [47].  If  a  nonmonotonic  reasoning  system  satisfies  Rational  Mono- 
tonicity  in  addition  to  Direct  Inference  for  Defaults  then  it  does  achieve  inheritance 
in  a  large  number  of  examples.  For  instance,  we  have  already  observed  that  Direct 
Inference  for  Defaults  gives  Penguin( Tweety)  by  ~Fly(  Tweety),  given  KBfr,,. Since 
KBp!  gives  us  no  reason  to  believe  that  yellow  penguins  are  unusual,  any  reason- 
able  default  reasoning  system  would  have  Penguin( Tweety)  /&tj, yYellow(  Tweety) .  From 
these  two  statements,  Rational  Monotonicity  allows  us  to  conclude  Penguin( Tweety)  A 
Yellow( Tweety)  by  lFly(  Tweety),  as desired. 

However,  Rational  Monotonicity  is  still  insufficient  for  inheritance  reasoning  in  gen- 
eral.  Suppose  we  add  the  default  Bird(x)  +  Warm-blooded(x) 
to  KBJ~. We  would 
surely  expect  Tweety  to be warm-blooded.  However,  Rational  Monotonicity  cannot  be ap- 
plied  here.  To see  why, observe  that Bitd(  Tweety)  h4y Warm-blooded( Tweety) ,  while  we 
want  to  conclude  that  Bird( Tweety)  A Penguin( Tweety)  ty4y Warm-blooded( Tweety) . ’ 
We  could  use  Rational  Monotonicity  to  go  from  the  first  statement  to  the  second, 
if  we  could  show  that  Bird(Tweety)  h? 
lPenguin(  Tweety)  .  However,  most  default 
reasoning  systems  do  not  support  this  statement.  In  fact,  since  penguins  are  excep- 
tional  birds  that  do  not  fly,  it  is  not  unreasonable  to  conclude  the  contrary,  i.e.,  that 
Bini(  Tweety)  hy 
.  Thus,  Rational  Monotonicity  cannot  be  used  to 
conclude  that  Tweety  the  penguin  is  warm-blooded. 

lPenguin(Tweety) 

It  seems  undesirable  that  if  a  subclass  is  exceptional  in  any  one  respect,  then  inher- 
itance  of  all  other  properties  is  blocked.  However,  it  can  be  argued  that  this  blocking 
of  inheritance  to  exceptional  subclasses  is reasonable.  Since  penguins  are  known  to  be 
exceptional  birds  perhaps  we  should  be  cautious  and  not  allow  them  to  inherit  any  of 
the  normal  properties  of  birds.  But  even  if  we  accept  this  argument,  there  are  many 
examples  which  demonstrate  that  the  complete  blocking  of  inheritance  to  exceptional 
subclasses  yields  an  inappropriately  weak  theory  of  default  reasoning.  For  example, 
suppose  we  add  to  KBpy the  default  Yellow(x)  ---f Easy-to-see(x).  This  differs  from 
standard  exceptional  subclass  inheritance  in  that  yellow  penguins  are  not  known  to  be 
exceptional  members  of  the  class  of  yellow  things.  That  is,  while  penguins  are  known  to 
be  somewhat  unusual  birds  (and  so perhaps  the  normal  properties  of  birds  should  not  be 
inherited), 
there  is  no  reason  to  suppose  that  yellow  penguins  are  different  from  other 
yellow  objects.  Nevertheless,  Rational  Monotonicity  does  not  suffice  even  in  this  less 
controversial  case.  Indeed,  there  are  well-known  systems  that  satisfy  Rational  Mono- 

‘In  any  system  that  treats  universals  reasonably,  this  is  clearly  equivalent  to  the  assertion  we  are  really 

interested  in:  Penguin(  liueety)  /yy  Warm-blooded( Tweety). 

90 

E  Bucchus  et  d/Artificial 

lntell~gence  87  (1996)  75-143 

tonicity  but  cannot  conclude 
This  problem  has  been  called 

that  Tweety, 
the  drowning  problem 

[ 2,9]. 

the  yellow  penguin, 

is  easy  to  see  [ 47,601. 

Theories  of  default 

reasoning  have  had  considerable 

from  superclasses 
to  inherit 
the  problem  of  inheritance 
particular, 
cult.  While  some  recent  propositional 
exceptional 
discuss 

in  the  next  section, 

inheritance 

subclass 

difficulty 

in  capturing  an  ability 
that  can  deal  properly  with  all  of  these  different  cases.  In 
subclasses  has  been  the  most  diffi- 
to  exceptional 
theories  have  been  more  successful  at  dealing  with 
[ 19,2  I,  231,  they  encounter  other  difficulties,  which  we 

3.4.  Expressivity 

In  the  effort  to  discover  basic  techniques  and  principles 

languages 

based  on  propositional 

[20,23],  modal  approaches 

logics  [ IO],  are  usually  considered 

logic  and  Delgrande’s 

conditional 

that  tends  to  decouple 

logic,  For 
such  as  autoepistemic 

for  default  reasoning,  people 
instance,  E- 
[55], 
logic 
framework.  Others, 
[ 151,  use  a  first-order 
reasoning  and 

logic 
the  issues  of  first-order 

in  a  propositional 

this  below.  Of  the  better-known 

systems,  circumscription 

and  variants 

looked  at  weak 

have  often 
semantics 
and  conditional 
such  as  Reiter’s  default 
language,  but  with  a  syntax 
default  reasoning;  we  discuss 
seems 
logic. 

to  have  the  ability,  at  least  in  principle,  of  making 

the  richest  use  of  first-order 

It  seems  uncontroversial 

language.  Sophisticated 

that,  ultimately,  a  system  of  default  reasoning 
knowledge 

should  be  built 
systems  almost 
around  a  powerful 
invariably  use  languages  with  the  expressive  power  of  some  large  fragment  of  first-order 
logic, 
the  knowledge  we  have 
about  almost  any  interesting  domain  without 
predicates 
by  default  within 

and  first-order  quantifiers.  We  would  also  like  to  reason 

the  expressive  power  provided  by  nonunary 

It  is  hard  or  impractical 

if  not  much  more. 

logically  as  well  as 

representation 

to  encode 

to  integrate 

the  same  system,  and  to  allow  perhaps  even  richer 
first-order 
of  our  approach 
language.  One  difficulty 

languages. 
logic  and  defaults  completely. 

for  other  approaches 

is  its  ability 

that  are  intended 

to  apply 

statement 

that  birds 

to  all  individuals. 
typically 

about  different  birds.  Let  us  examine  how  some  existing 

to  express  both 

In  fact, 
types 
concerns 
suppose 
fly,  and  be  able  to  use  this 
systems  do 

For  instance, 

It  has  not  been  easy 

in  a  single 

one  of  the  major  contributions 
information 
of 
“open”  defaults, 
we  wish 
when  reasoning 
this. 

to  make  a  general 

approaches, 

In  propositional 

the  usual  strategy 

(see,  for  example, 

[ 2 I ]  and  the  references 

is  to  claim  that  there  are  different 

types 
therein).  General  defaults,  such 
such  as  Tweety, 
the  context,  For  Tweety, 
for  a  general 
in  the 
approaches  have  more  expressive  power  in  this  regard.  For  example, 
4  Fly(x).  That 
is  a  bird  can  then  be  written  Bird(  Tweet?).  which  seems  much  more  natural. 
instances 

of  knowledge 
as  Bird  ---f Fly,  are  in  one  class.  When  we  reason  about  an  individual, 
its  properties  are  described  by  knowledge 
the  context  might  be  Bird  A  Yellow.  In  a  sense, 
property  when  used  in  a  default  and  talks  about  Tweety 
context.  First-order 
Reiter’s  default 
Tweety 
The  default 
(such  as  Bird(  Tweety) 

logic  uses  defaults  with  free  variables,  e.g.,  Bird(x) 

itself  is  treated  essentially 

the  symbol  Bird  stands 

(say)  when  it  appears 

in  a  different  class, 

--i  Fly(  Tweety)  ). 

all  substitution 

as  a  schema, 

implying 

E  Bacchus  et  al.  /Artijicial 

Intelligence 

87  (1996)  75-143 

91 

One  example  shows  the  difficulties  with  both  of  these  approaches.  Suppose  we  know 

that: 

Elephants 

typically 

like  zookeepers. 

Fred  is  a  zookeeper,  but  elephants 

typically  do  not  like  Fred. 

Clyde 

is  an  elephant. 

Eric  is  a  zookeeper. 

this 

to  determine 

as  “Does  Clyde 

strategy  of  classifying 

information  we  can  apply  specificity 

to 
Using 
like  Fred  ?’  (No)  or  “Does  Clyde  like  Eric”  (Yes).  But 
such  questions 
the  propositional 
to  fail  here.  Is  “Elephants 
typically  do  not  like  Fred”  a  general  default,  or  an  item  of  contextual  knowledge?  Since 
it  talks  about  elephants 
it  does  not 
in  general  and  also  about  one  particular 
fit  either  category  well.  In  a  rich  first-order 
between 
one). 

zookeeper, 
there  is  no  clear-cut  distinction 

facts  and  general  knowledge 

(nor  do  we  believe 

there  should  be 

knowledge 

reasonable 

language, 

answers 

specific 

seems 

Next,  consider 

the  first-order 
not  work  at  all.  One  substitution 

substitutional 
instance  of 

approach. 

It  is  easy  to  see  that  this  does 

Elephant(x)  A  Zookeeper(  y)  ---t Likes(  x,  y) 

is 

Elephant(x) 

A Zookeeper(  Fred)  +  Likes(  x,  Fred), 

which  will  contradict 

the  second  default.  Of  course,  we  could  explicitly  exclude  Fred: 

Elephant(x) 

A Zookeeper(  y)  A y  #  Fred 

-+  Likes(  x,  y) . 

However,  explicit  exclusion 
defaults,  mentioned 
edge  base, 
the  knowledge  base.  Hence, 
bases. 

is  similar 

less  specific 
the  modularity  of  the  knowl- 
i.e.,  the  form  of  a  default  becomes  dependent  on  what  other  defaults  are  in 
are  highly 

to  the  process  of  explicitly  disabling 

section,  Both  destroy 

for  large  knowledge 

these  techniques 

in  the  previous 

impractical 

is  similar 

they  suggest 

The  zookeeper 

for  open  defaults.  Rather, 

by  a  set  of  rules  provided 

example 
the  solution 

to  an  example  given  by  Lehmann 

the  “meaning” 
for  manipulating 

and  Magidor 
to  this  problem  does  not  provide  an  explicit 
is  implicitly 
of  an  open  default 

[ 461.  However, 
interpretation 
determined 
cope  with  the  zookeeper  example,  but  the  key  step  in  the  application  of  these  rules 
argument 
the  use  of  Rational  Monotonicity.  More  precisely,  Lehmann 
infer 
can 
applies 
and  yet 
by  default 
since 
cannot 
we  know  nothing  whatsoever  about  Clyde  or  Eric.  Now,  however,  we  can  apply  Rational 
(i.e.,  add  to  the  premises) 
Monotonicity 
that  x  =  Clyde  A  y  =  Eric,  while  still  concluding  Likes(  x,  y).  Finally,  ReJlexivity,  Right 

(i.e.,  Elephant(x) 
infer  either  x  #  CZyde  or  y  #  Eric.  The  latter  certainly 

such  defaults.  These  rules  can 
is 

the  premise  Elephant(x) 
A  Zookeeper 

to  systems  which,  given 

twice,  which  effectively 

allows  us  to  assume 

that  Likes(x,  y) 

seem  reasonable 

and  Magidor’s 

b  Likes(x,  y)) 

A  Zookeeper( 

92 

I?  Bacchus  et  ul./Arb$cial 

Inlelligence 87  (1996)  75-143 

Weakening,  and  Left  Logical  Equivalence 
y;  we  obtain 

can  be  used  to  justify 

substituting 

for  x  and 

Elephant(  Clyde)  A Zookeeper(  Eric)  k  Likes(  Clyde,  Eric), 

that  Fred 

by  default 

and  in  fact  it  is  easy  to  argue  analogously 

is  that  this  argument  will  typically 
is  unusual 

as  desired.  The  key  point 
do  have  reason 
to  believe 
conclude 
that  Likes(  Clyde,  Fred), 
TLikes(  Clyde,  Fred)  using 
in  this  example,  we  have, 
blocked  by  “irrelevant” 
tional 
approach  will  not  be  able  conclude 
able. 

fail  for  Fred,  because  we 
(and  so,  in  many  systems,  we  could 
that  y  #  Fred).  Thus,  as  we  would  hope,  we  cannot  conclude 
that  we  conclude 
the  second  default.  But  while  Rational  Monotonicity  helps 
it  is  easily 
in  Section  3.3,  already 
to  be  excep- 
then  Lehmann  and  Magidor’s 
is  surely  undesir- 

For  example, 
to  zookeeping), 

in  some  way  (even  one  unrelated 

that  he  is  liked  by  Clyde.  This 

exceptionality. 

is  known 

its  main 

if  Eric 

failing: 

seen 

Thus, 
perhaps 
default 

to  be  very  hard  to  interpret  generic 

it  seems 
the  best-known 
logic.  There  are,  of  course,  others;  we  close  by  mentioning 

(open)  defaults  properly.  This 
the  expressive  power  of  various  approaches 

issue  regarding 

one. 

is 
to 

Morreau 
individuals 

[56]  has  discussed 
satisfying 

a  certain  default”.  For  example, 

the  assertion: 

the  usefulness 

of  being  able  to  refer 

to  “the  class  of 

Typically,  people  who  normally  go  to  bed  late  normally 

rise  late, 

refers  to  “the  class  of  people  who  normally  go  to  bed  late”.  The  structure  of  this  assertion 
is  essentially: 

(Day(y) 

+  To-bed-late(  x,  y)  )  -+  (Day(  y’)  --+ Rises-late(  x,  y’)  ) . 

this  example, 

and  conclusion 

are  descriptions 

theories.  Reiter’s  default 

is  a  default  whose  precondition 

theories  of  conditional 
they  are  as  yet 

This 
of  people  whose 
behaviors  are  themselves  defined  using  defaults.  Such  defaults  appear  to  pose  problems 
logic  cannot  express  such  defaults. 
for  most  existing  default 
logic 
And  while  some 
can 
express 
incapable 
from  nested  defaults  of  this  type.  Circumscription, 
configured 
not  obvious 
is  clearly  a  difference  between 
go  to  bed  late  rise  late  (i.e.,  the  next  morning)“; 
written: 

of  generating 
reasonable 
on  the  other  hand,  could  perhaps  be 
is 
there 
the  above  default  and  the  one  “Typically,  people  who 
the  latter  statement  could  be 

to  cope  with  this  example,  but  precisely  how  this  could  be  accomplished 
to  us.  We  also  note  that  the  example  has  many  variants.  For  instance, 

(for  example, 

inferences 

those  of 

[ 10,151) 

formally, 

(Day(y)  A  To-bed-late(  x,  y)  )  +  Rises-late(  x,  Next-day(  y)  ) . 

There  are  also  other  variations.  We  would 
them  all.  The  real  issue  here  is  that  we  need  to  define  various  properties  of  individuals, 
and  while  many  of  these  properties  can  be  expressed 
refer  to  defaults  explicitly.  This  argues,  yet  again,  that  it  is  a  mistake 
language 

logic,  others  need  to 
to  have  a  different 

than  the  one  used  for  other  knowledge. 

to  express  and  reason  correctly  with 

in  first-order 

for  defaults 

like 

E  Bacchus  et  al./Art@cial  Intelligence  87  (1996)  75-143 

93 

3.5.  The  lottery  paradox 

The  lottery  paradox  [40]  addresses 

It  provides 

a  challenging 

interact. 
reasoning 
three  here. 

system.  There  are  a  number  of  issues 

test  of  the  intuitions 

the  issue  of  how  different  default  conclusions 
and  semantics  of  any  default 
raised  by  this  paradox;  we  consider 

i.e., 

theories 

is  to  deny 

conjunction, 

to  contradict 

First,  imagine 

that  default  conclusions 

the  existence  of  multiple 

theories,  we  are  not  aware  of  work  taking 

to  be  several  options.  Clearly,  one  solution 

that  a  large  number  N  of  people  buy  tickets  to  a  lottery 

in  which  there 
is  only  one  winner.  For  a  particular  person  c,  it  seems  sensible 
to  conclude  by  default 
that  c  does  not  win  the  lottery.  But  we  can  argue  this  way  for  every  individual,  which 
the  fact  that  someone  definitely  will  win.  Of  course  some  theories, 
seems 
languages,  do  not  have  enough  expressive  power 
such  as  those  based  on  propositional 
to  even  state  this  version  of  the  problem.  Among 
that  can  state  it,  there  would 
seem 
are  closed  under  arbitrary 
from  explicitly 
probabilistic 
(although 
related).  Without 
syntactic 
about  all  N  individuals 
follows  by  default.  Circumscription, 
of  the  problem  would  result  in  multiple  extensions, 
is  one  extension  where  c  is  the  winner.  While 
only  allows  us  to  conclude 
able 
problem 
buy  a  lottery 
not  win. 

to  give  up  on  the  And  rule.  But  aside 
this  approach 
is  certainly 
too  dependent  on  merely 
is  to  prevent  a  theory  from  reasoning 
that  TWinner(c) 
representation 

thus,  we  would  not  be 
is  that  the  lottery 
if  you 
reasoning: 
that  you  will 

application 
ticket  you  should  continue  your  life  under 

features  of  a  problem.  Another  solution 
at  once 

to  conclude  TWinner(c).  The  problem  with  these  “solutions” 

[ 171.  Finally,  one  can  simply  deny 

for  instance,  does  this:  the  standard 

in  theories  such  as  Reiter’s 

such  that  for  each  individual 

that  hold  in  all  extensions; 

this  seems  reasonable, 

is  a  danger  of  being 

to  be  an  extremely 

logical  closure, 

the  assumption 

circumscription 

reasonable 

of  default 

extensions 

c,  there 

seems 

things 

there 

argues 

related 

in  fact  it  seems 

for).  The  desire 

list  of  benchmark 

Second,  a  closely 

for  these  universal 

although  not  in  many  other  theories,  we  get  both  universal  conclusions 

it  possible 
that  Vx(  (Ticket(x)  A  x  #  c)  =+ lWinner(  x))? 

is  raised  by  Lifschitz’s 
problems 
for  instance  Ticket(x)  +  TWinner(x),  and  no  other 
if 
that  the  lottery  has  more  than  one  winner, 
In 

issue 
[48].  Suppose  we  have  a  default, 
knowledge.  Should  Vx( Ticket(  x)  =+ 7 Winner(x)  )  be  a  default  conclusion?  Likewise, 
we  know  Winner(c)  but  consider 
should  we  nevertheless 
conclude 
circumscription, 
(as  Lifschitz 
troversial; 
However,  as  Lifschitz  observes, 
ition:  how  can  we  conclude 
each  individual 
default,  no  one  wins?  The  concern 
logically 
Reiter’s  default 
the  random-worlds 
Finally,  Poole 
the  issue  of  named 
birds  we  are  likely 

con- 
rules  to  have  some  exceptions. 
this  latter  intu- 
that,  by  default, 
that,  by 
the  latter  conclusion  will  be 
entailed  whether  we  wish  it  or  not.  Because  of  its  treatment  of  open  defaults, 
logic  does  not  suffer  from  this  difficulty.  As  we  shall  see,  neither  does 

c  is  not  a  winner,  and  yet  not  also  reach  the  universal  conclusion 
is  that,  in  many  systems, 

[ 631  has  considered  a  variant  of  the  lottery  paradox 

from  the  default  Ticket(x)  4  ~Winner(x) 

that  avoids  entirely 
the  types  of 

In  his  version, 
such  as: 

that  we  often  expect  default 

individuals. 
to  encounter, 

there  is  a  formula  describing 

there  is  a  technical  difficulty 

in  following 

is  certainly 

conclusions 

approach. 

94 

E  Bacchus  et  (11. /Arhfiicial 

Intelligence 

87  (I  996)  75-143 

Vx(Bird(x) 

++  (Emu(x)  VPenguin(x)  V~~~VCunary(x))). 

flies? 

typically 

typically 

such  as  birds 

If  we  conclude 

to  give  an  exhaustive 

flies,  then  the  default  “Birds 

the  fact  he  must  be  exceptional 

that  he  can, 
that  he  is  a  typical  bird 

fly,  but  penguins 
assert  that  every  other  species  of  bird  is  excep- 
in  some  way.  Now  suppose  all  we  know  is  that  Bird(  Tweety).  Can  we  conclude 
then  a  similar  argument  would  also 
this  would 
in  all  other  respects.  But 
that 
in  some  respect.  If  we  do  not  conclude 
ignored.  Poole 
re- 
such  as 
for  Defaults),  must  be 
is  the  possibility 
of 
Is  it 
or  inconsistent. 
all  of  which 
to 
is  nothing 
semantics 
to 
of  certain  sets 
to  determine 
is  how  our  approach  avoids  Poole’s  version  of  the  lottery 

We  then  add  to  the  knowledge  base  defaults 
typically  do  not  fly,  and  we  similarly 
tional 
that  Tweety 
allow  us  to  conclude 
contradict 
Tweety 
uses  such  examples 
act  to  the  lottery  paradox.  He  shows 
closure  under  conjunction 
sacrificed.  Perhaps 
declaring 
to  say  that  the  class  of  birds 
really  reasonable 
are  exceptional? 
In  many 
prevent  one  from  asserting 
defaults,  we  may  be  able 
of  defaults.  This, 
indeed, 
paradox. 

is  the  union  of  subclasses 
there 
this.  But  in  a  theory  which  gives  reasonable 

analysis  of  how  various  systems  might 

that  in  any  theory,  some  desideratum, 

or  “conditioning” 
interesting 

of  defaults  are  inadmissible 

that  certain  combinations 

fly”  has  been  effectively 

such  as  Reiter’s  default 

“way  out”  he  discusses 

the  incompatibility 

and  justify 

the  most 

theories, 

Inference 

(Direct 

logic, 

3.6.  Discussion 

In  this  section,  we  have  presented 

a  limited 

list  of  desiderata 

that  seem  appropriate 
that 

some  key  problems  and  issues 

it  is  interesting 
system 

that 
in  a  satisfactory  way.  Although  we  can  (and  do)  show  that 
on  this  list,  we  would  like  to 
to  the  best  of 

fashion.  Unfortunately, 

reasoning 

that 

reasoning 

random  worlds 

there  does  not  seem 

to  be  a  single  default 

in  a  more  comprehensive 

system,  and  have  discussed 

there  is  (as  yet)  no  general 

for  a  default 
must  be  resolved  by  such  a  system.  While  our  list  may  be  limited, 
to  point  out 
fulfills  all  these  desiderata 
random  worlds  does,  in  fact,  achieve  all  the  requirements 
validate 
our  knowledge, 
systems. 
solve 
problems 
aspects  of  the  problem  and  defining  our  intuitions 
substitute 
the  drowning  problem 
to  provide 
long.  While  we  do  not  attempt 
in  Section  5  we  prove  a  number  of  general 
theorems  provide  a  precise 
approach.  These 

such  a  general 
theorems  concerning 
formulation 

in  [ 481).  While  such  examples  are  often  important 

In  particular, 
these  particular 

evaluation 
examples 

for  a  comprehensive 

correctly?” 

framework 

tends 

still 

in  identifying 
in  these  cases,  they  are  clearly  not  a 
framework.  Had  there  been  such  a  framework,  perhaps 
for  so 
in  this  paper, 

framework 

from  Section  3.3  would  not  have  remained  undiscovered 

to  be  on  the  level  of  “Does 
(see, 

for  evaluating  default  reasoning 
this  theory 
the  list  of  benchmark 
interesting 

for  example, 

such  as  Direct 
for  Defaults,  and  show  that  they  hold  for  random  worlds.  Other  properties 
from  these 
immediately 
approach  deals  well  with  the  paradigm 
theorem,  rather  than  by  a  case-by- 

Inference 
such  as  specificity 
theorems.  Thus,  our  proof  that  the  random-worlds 
examples 
case  analysis. 

follows  from  a  general 

in  default  reasoning 

and  exceptional 

of  properties 

inheritance 

subclass 

follow 

the  random-worlds 

E  Bacchus  et  al.  /Artijicial 

Intelligence 

87  (I  996)  75-143 

95 

4.  The  formalism 

4.1.  The  language 

We  are  interested 

in  a  formal 

logical 

language 

and  first-order 

information.  We  therefore  define  a  statistical 

that  allows  us  to  express  both  statistical 
I?, 

language 

is  a  variant  of  a  language  designed  by  Bacchus 
let  @  be  a  finite 

first-order  vocabulary, 

consisting 

[3].  For  the  remainder  of  the 
and 

of  predicate, 

function, 

symbols,  and  let  X  be  a  set  of  variables. 

information 
which 
paper, 
constant 

standard 

]]+(x)llX 

the  term 

first-order 

Our  statistical 

language  augments 

as  a  rational  number  between  0  and  1,  that  represents 

quantifier.  For  a  formula  $(x), 
be  interpreted 
domain  elements 
subscript  and  in  the  formula 
y,  the  proportion  of  domain  elements 
for  a  fixed  x,  the  proportion  of  domain  elements  whose  child  is  x;  and  I I Child( x,  y)  1 Ix,? 
describes 

logic  with  a  form  of  statistical 
expression. 
It  will 
the  proportion  of 
in  the 
for  a  fixed 
that  are  children  of  y;  I IChild(x,  y)  I jy  describes, 

cl/(x).  We  actually  allow  an  arbitrary  set  of  variables 
(/I. Thus,  for  example,  1 IChild( x,  y)  1 Ix describes, 

the  proportion  of  pairs  of  domain  elements 

that  are  in  the  child  relation. 

is  a  proportion 

satisfying 

We  also  allow  proportion  expressions  of  the  form  I]$(  x)  1 6J( x)  JIx, which  we  call  con- 

ditional proportion  expressions.  Such  an  expression 
of  domain  elements 
tional  number 
expressions 

is  closed  under  addition  and  multiplication. 

is  also  considered 

satisfying  $  from  among  those  elements  satisfying  8.  Finally,  any  ra- 

to  be  a  proportion  expression,  and  the  set  of  proportion 

is  intended 

to  denote 

the  proportion 

are  sometimes 

in  the  introduction, 

expressions.  As  we  argued 

that  the  number  of  jaundiced  patients 

implication.  We  therefore  use  the  approach  described 

One  important  difference  between  our  syntax  and  that  of  [ 31  is  the  use  of  approximate 
exact 
inappropriate.  Consider  a  statement  such  as  “80%  of  patients 
If  this  statement  appears  in  a  knowledge  base,  it  is  almost 
there  as  a  summary  of  a  large  pool  of  data.  It  is  clear  that  we  do  not  mean 
this 
is  a  multiple  of  five,  which  is  surely 
in  [ 28,381,  and 
family  of 

equality  to  compare  proportion 
comparisons 
with  jaundice  have  hepatitis”. 
certainly 
that  exactly  80%  of  all  patients  with  jaundice  have  hepatitis.  Among  other  things, 
would  imply 
not  an  intended 
compare  proportion 
connectives 
less  than  or  equal”).  8  For  example,  we  can  express 
patients  have  hepatitis”  by  the  proportion  formula 
intuition  behind 
be  interpreted 
sample  variations, 
of  information, 
connectives.  A  formula 
says  that  both 
]]Fly(x) 
the  notion  of  “approximately”  may  be  different 

I Jaun(x)  [IX ~1  0.8.  The 
should 
error, 
tolerance  will  differ  for  various  pieces 
equals” 

) Bird(x)II,  XI  1 A  IIFly(x)  1 Bat(x)II,  ~2  1 
1,  but 
(IF/y(x) 

(instead  of  =  and  <)  one  of  an  infinite 
. .  (“i-approximately 

equal”  or  “i-approximately 
the  statement  “80%  of  jaundiced 

expressions  using 
zi  and  di,  for  i =  1,2,3,. 

is  that  each  comparison 
for  measurement 

such  as  [[Fly(x) 
( Bird(x)II, 
and 

subscripts  on  the  “approximately 

) Bat(x)II, 
in  each  case. 

the  semantics  of  approximate 

so  our  logic  allows  different 

and  so  on.  The  appropriate 

are  approximately 

some  small 

to  account 

tolerance 

equality 

factor 

using 

lIHep(x) 

We  can  now  give  a  recursive  definition  of  the  language  C=. 

8 In  141  the  use  of  approximate  equality  was  suppressed  in  order  to  highlight  other  issues. 

96 

E  Bucchus  et  al./Ardjkial 

Inrelligence  87  (1996)  75-143 

Definition  4.1.  The  set  of  terms  in  Lc”  is  the  least  set  containing  X  and  the  constant 
symbols 
(so  that  if  f  is  a  function  symbol 
in  9  of  arity  r,  and  rt , 
The  set  of  proportion 

. , t,  are  terms,  then  so  is  f(tt  , . . . , t,)). 
is  the  least  set  that 
expressions 

in  @ that  is  closed  under  function  application 

(a)  contains 

the  rational  numbers, 

(b)  contains  proportion 

terms  of  the  form  II@llx  and  [Ifi  IOjlx,  for  formulas 

rJ,e  E 

C”  and  a  finite  set  of  variables  X  2  X,  and 

(c) 

is  closed  under  addition  and  multiplication. 

The  set  of  formulas 

in  C” 

is  the  least  set  that 

(a)  contains 
symbol 

atomic 

formulas 
in  @U  {=}  of  arity  r  and  tl, 

. .  , t,  are  terms, 

of  the  form  R( tl,  . . . , tr),  where  R  is  a  predicate 

(b)  contains  proportion 
are  proportion 

expressions 

and  i  is  a  natural  number,  and 

formulas  of  the  form  5  zi  5’  and  C  di  l’,  where  C  and  l’ 

(c) 

is  closed  under  conjunction, 

negation,  and  first-order  quantification. 

Notice 
pressions. 
observed 
sion  binds 
quantification. 

allows  arbitrary  nesting  of  quantifiers 

that  this  definition 
In  Section  4.3  we  demonstrate 

ex- 
the  expressive  power  of  the  language.  As 
in  [ 31,  the  appearance  of  a  variable  x  in  the  subscript  of  a  proportion  expres- 
indeed,  we  can  view  Il.llx  as  a  new  type  of 

the  variable  x  in  the  expression; 

and  proportion 

connective 

comparisons 

the  semantics 

to  define 
are  fairly  straightforward. 

and  conditional 
5  Ei  5’  to  mean 

We  now  need 
of  the  definitions 
approximate 
proximate 
is  within  some  very  small,  but  unknown, 
tolerance  vector  7’ =  (q,r2,. 
are  within 
well-defined 
know  appropriate 
section.) 

ri  of  each  other.  (Note 

that,  although 
formal  semantics,  one  might  object 

proportion 

is  very  close 

The  two  features 

that  cause  problems 

expressions.  We  interpret 

of  the  logic.  As  we  shall  see  below,  most 
are 
the  ap- 
it 
this  using  a 
factor.  We  formalize 
5  Mi  5’  if  the  values  of  JJ and  5’ 
the  use  of  tolerance  vectors 
leads  to 
that  in  practice  we  generally  will  not 
to  the  next 

to  5’.  More  precisely, 

to  this  objection 

tolerance 
. .),  7;  >  0.  Intuitively 

tolerance  values.  We  defer  our  response 

that  f 

proportion 

conditional 

expressions 

A  difficulty 

arises  when 

interpreting 
the  problem  of  conditioning 
semantics 

because  we 
is, 
to 
is  used  rather 
than 
[ 301,  we  can  eliminate 

for  II@ I 611 x  even  when 

to  deal  with 
to  define 

in  X  that  would  satisfy  B.  When  standard  equality 

on  an  event  of  measure  0.  That 
there  are  no  assignments 

is  easily  overcome.  Following 
altogether  by  viewing  a  statement  such  as  ll$  I O/lx  = 

need 
we  need 
the  variables 
approximate 
this  problem 
equality, 
conditional  proportion  expressions 
a  as  an  abbreviation 
interpretation 
that  formulas 
same  approach 
following 
an  undesirable  way  with  the  semantics  of  approximate 
approach  does  not  preserve 
approximately 

for  II+  A ~91 Ix  =  al(0l[x.  This  approach  agrees  with  the  standard 
the  convention 

of  conditionals 
if  llt9llx  #  0.  If  l/e/lx  =  0,  it  enforces 
such  as  I[$  I Bllx  =  a  or  lit,!? ( 011 x  <  (Y are  true  for  any  CY. We  used  the 
as  the 
in 
this 
is 

in  [28],  where  we  allowed  approximate 
this  interpretation 

semantics  of  conditional 

equality.  Unfortunately, 

of  conditional 

In  particular, 

the  standard 

comparisons. 

can  interact 

proportions 

example 

if  llellx 

equality 

shows, 

0. 

E  Bacchus  et  al/Artificial  Intelligence  87  (1996)  75-143 

97 

Example  4.2.  Consider 

the  knowledge  base:  9 

KB  =  ( 1 IPenguin 

) IX XI  0)  A  (IIFly(x) 

I Penguin(x) 

IIx =2  0). 

this  to  mean 

We  expect 
to  0  in  large  domains), 
small.  However, 
we  obtain 

if  we  attempt 
the  knowledge  base 

that  the  proportion  of  penguins 

is  very  small 

but  also  that  the  proportion  of  fliers  among  penguins 

to  interpret  conditional 

proportions 

(arbitrarily 

close 
is  also  very 
as  discussed  above, 

KB’=  (IIPenguin(x)II,  ~1  0)  A 

(IIW(x)  APewWx)ll, 

z2  0’ 

IIPenguin(x)II,), 

which 

is  equivalent 

to 

(IIPenguin(x)II. 

=I  0)  A  (IIFly(x)  APenguin(x)II, 

x2  0). 

simply  asserts 

that  the  proportion  of  penguins 

last  formula 

This 
flying  penguins 
penguins. 
the  process  of  multiplying 
intended 

interpretation 

In  fact, 

are  both  small,  but  says  nothing  about 

the  world  where  all  penguins 

out  across  an  approximate 

of  the  formulas. 

and  the  proportion  of 
the  proportion  of  fliers  among 
fly  is  consistent  with  KB’.  Clearly, 
the 
connective  does  not  preserve 

Because  of  this  problem,  we  cannot 

treat  conditional  proportions 

them  as  primitive  expressions 

like 

to  maintain 

that  avoids 

the  conventions 

instead  have  added 
have  to  give  them  a  semantics 
would 
Namely, 
of  elements 
want  all  formulas  of  the  form  Ilp(x> 
There  are  a  number  of  ways  of  accomplishing 
the  simplest,  but  it  introduces  machinery 

satisfying 

e(x) 

that  also  satisfy  p(x). 

as  abbreviations 

and 
in  the  language.  Of  course,  we  now 
illustrated  by  Example  4.2.  We 

the  problem 

used  when  we  had  equality 

in  the  language. 
the  fraction 
llO(x)llX  =  0,  we 
I 13(x> /Ix xi  a  or  11+$x> I 0(x)11,  di  LY to  be  true. 
this.  The  route  we  take  is  perhaps  not 

In  worlds  where 

in  worlds  where  Il8(x)  )IX #  0,  we  want  I/q(x)  1 t!?(x) [Ix to  denote 

that  will  be  helpful 

later. 

We  give  semantics 

to  the  language  J!?  by  providing  a  translation 

from  formulas 

in  Lc” 

to  formulas 
L=  is  essentially 
equality.  More  precisely, 
in  Definition  4.1,  except 

in  a  language  L=  whose  semantics 

the  language  of  [30], 

that  uses  true  equality 

is  more  easily  described.  The  language 
rather  than  approximate 
to  the  definition  of  Lx  given 

the  definition  of  L=  is  identical 
that: 

l  we  use  =  and  <  instead  of  zi  and  i,, 
l  we  allow 

the  set  of  proportion 

expressions 

just  rational  numbers), 

to  include  arbitrary 

real  numbers 

(not 

l  we  do  not  allow  conditional 
l we  assume 

that  Cc= has  a  special 

proportion 

expressions, 

As  we  shall  see,  the  variable  ei  is  used  to  interpret 
xi  and  ii.  We  view  an  expression 
an  abbreviation 

for  the  expression  obtained  by  multiplying 

out. 

family  of  variables  ei,  interpreted  over  the  reals. 
equality  connectives 

the  approximate 
in  Lc= that  uses  conditional  proportion  expressions 

as 

9 We  remark 

that,  here  and 

in  our  examples  below, 

However,  we  use  different 
different  measurements 

are  known 

to  be  the  same. 

subscripts 

for  different  approximate 

the  actual  choice  of  subscript 
unless 

comparisons 

for  x 
the  tolerances 

is  unimportant. 

for  the 

98 

F  Bncchus  et  ul.  /Arri$ciul 

Intelligence 

87  (I  996)  75-143 

for  LX  is  quite  straightforward, 

the  lines  of  [30].  Recall 
and  follows 
to  &=  in  terms  of  worlds,  or  finite  first-order  models.  For  any 
IV, let  W,+,(G)  consist  of  all  worlds  with  domain  D  =  { I,.  . . , N)  over 

The  semantics 

that  we  give  semantics 
natural  number 
the  vocabulary  Cp. 
Now,  consider 

a  world  W  E  WN(  @I,  a  valuation  V  :  X 

-+  { 1, 

. , N} 

for  the 

in  X,  and  a  tolerance  vector  ?.  We  simultaneously 

to  each  proportion 
[ [  1 (w,,!g  and  to  each  formula  4  a  truth  value  with  respect 
standard,  so  we  omit 
are  completely 
are  interpreted  using  V,  each  tolerance  variable  F, 

assign 

variables 
the  tolerance 

variables 
expression  3  a  real  number 
to  (M! L!?).  Most  of  the  clauses  of  the  definition 
them  here.  In  particular, 
is  interpreted 
as  denoting 
using  W,  the  Boolean  connectives 
dard  fashion,  and  when  interpreting 
multiplication, 
tion  terms.  Recall 
that  we  need 
expression 

to  deal  only  with  unconditional 

that  we  eliminate  conditional 

]]fillX ,,..... l,L  (for  ii  <  i2  < 

and  <  are  given 

proportion 

<  ix),  then 

r,,  the  predicates  and  constants 
and  the  first-order  quantifiers  are  defined 

expressions, 

their  standard  meaning. 

It  remains 

are  interpreted 
in  the  stan- 
the  real  numbers,  addition, 
to  interpret  propor- 
out,  so 

proportion 
proportion 

terms  by  multiplying 
terms.  If  i 

is  the  proportion 

Thus, 

if  W  E  W,(G), 

the  proportion 

expression 

II$]/+ 

..,x,k denotes 

the  fraction  of 

the  Nk  k-tuples  of  domain  elements 
in  D  that  satisfy  ~4  in  the  world  W.  For  exam- 
ple,  [lIC~~i~~(x,~)ll.l~~!~~ is  the  fraction  of  domain  elements  d  that  are  children  of 
V(Y). 

We  now  show  how  a  formula  x  E  C=  can  be  associated  with  a  formula  x*  E  C=. 

We  proceed  as  follows: 
l  every  proportion 

formula 

i  3,  [’ 

in  x 

is  (recursively) 

replaced  by  5  - 

l’  6 

0  Zery  proportion 

formula 

IJ k+  5’  in  x  t IS  recursively) 

( 

replaced  by  the  conjunction 

(<  -  5’  <  Gil  A 
l  finally,  conditional 
by  multiplying 

out. 

(5’ 

- 
5  <  &;I, 
proportion 

expressions 

are  eliminated 

as  in  [301’s  semantics, 

allows  us  to  embed  Lc”  in  C=.  Thus,  for  the  remainder  of  the  paper,  we 
of  C=.  We  can  now  easily  define  the  semantics  of  formulas 
/=  ,y  iff  (W; Y?)  k  x*.  It  is  sometimes  useful 
into  the  formula  x*.  Thus, 

This  translation 
regard  Lx  as  a  sublanguage 
in  C”:  for  x  E  Lx.  we  say  that  ( W,  Y?) 
to  incorporate  particular  values  for  the  tolerances 
represent 
according 

that  results  from  x*  if  each  variable  E;  is  replaced  by  ri,  its  value 

the  formula 
to  7:  lo 

let  x[fl 

Typically  we  are  interested 

that  is,  formulas  with  no  free  variables. 
In  that  case,  it  is  not  hard  to  show  that  the  valuation  plays  no  role.  Thus,  if  x  is  closed, 
we  write  (M! ?)  /=  x  rather  than  ( W, K?) 

in  closed  sentences, 

/= ,y. 

“‘Note 
numbers 

that  some  of  the  tolerances  7,  may  be  irrational; 
in  the  proportion 

expressions  of  C=. 

it  is  for  this  reason 

that  we  allowed  arbitrary 

real 

E  Bacchus  et  al./Artijicial 

Intelligence 

87  (1996)  75-143 

99 

4.2.  Degrees  of  belief 

in 

the 

the  probability 

As  we  explained 

introduction,  we  give  semantics 

all  worlds  of  size  N  to  be  equally 

to  degrees  of  belief  by 
on  KB,  and  then 
In  the  previous 
in  a  world  of  size  N 
to  be  the  number 
in  W,v(  @)  such  that  ( W, 73  +  x.  Since  we  are  taking  all  worlds  to  be  equally 

considering 
checking 
section,  we  defined  what  it  means  for  a  sentence  x  to  be  satisfied 
using  a  tolerance  vector  7’. Given  N  and  +‘, we  define  #  world&(x) 
of  worlds 
likely, 

of  4p over  the  resulting  probability  distribution. 

in  rp given  KB  with  respect 

the  degree  of  belief 

likely,  conditioning 

to  WN  and  7’ is 

Pr$(cp  1 KB)  = 

#  worlds;  (rp A KB) 

#  worldsc(  KB) 

’ 

If  #  worlds$(  KB)  = 0,  this  degree  of  belief 

is  not  well  defined.  ” 

Typically,  we  know  neither  N  nor  7’ exactly.  All  we  know 

like 
)  KB).  Notice 

is  that  N  is  “large”  and 
to  take  our  degree  of  belief  in  (p  given  KB 
that  the  order  of  the  two  limits  over  7’ 

lim,+G  appeared 
equality,  since  the  result  would  be  equivalent 

last,  then  we  would  gain  nothing  by 

to  treating  approximate 

limN_+,  Prk(4p 

that  7’ is  “small”.  Thus,  we  would 
to  be  limi+a 
and  N  is  important. 
using  approximate 
equality  as  exact  equality. 

If  the  limit 

implies 

In  particular, 

for  arbitrarily 

is  not  sufficient; 

things,  eventual 

This  definition,  however, 

small  7’ and  sufficiently 
consistency 

large  N,  #  worldsiN 
that  the  KB  is  satisfiable 

the  limit  may  not  exist.  We  observed  above 
that  Prz(cp  1 KB)  is  not  always  well  defined. 
it  may  be  the  case  that  for 
certain  values  of  7’, Prg  ( C,O ) KB) 
large  N.  In  order 
is  not  well  defined 
to  deal  with  this  problem  of  well  definedness,  we  define  KB  to  be  eventually  consistent 
if  for  all  sufficiently 
>  0.  Among 
in  finite  domains 
other 
that  “there  are  exactly  7  domain 
large  size.  For  example, 
of  arbitrarily 
elements” 
of  the  paper,  we  assume 
consistent. 
is  not  eventually 
that  all  knowledge  bases  are  eventually 
consistent, 

a  KB  stating 
I2  For  the  remainder 
consistent. 
the  limit  may  not  exist.  For  example, 

it  may  be 
the  case  that  for  some  i,  Pri(  v,  1 KB)  oscillates  between  LY +  ri  and  (Y -  Ti  as  N  gets 
large.  In  this  case,  for  any  particular  7’, the  limit  as  N  grows  will  not  exist.  However, 
it 
seems  as  if  the  limit  as  7’ grows  small  “should”, 
about  CY go  to  0.  We  avoid  such  problems  by  considering 
than  the  limit.  For  any  set  S  c  R,  the  infimum  of  S,  infS, 
is  the  limit  of  the  infimums; 
of  S.  The  liminf  of  a  sequence 

in  this  case,  be  (Y, since  the  oscillations 

is  the  greatest 
that  is, 

rather 
lower  bound 

the  limsup  and  liminf, 

if  KB  is  eventually 

Even 

lim  inf  uN  = 
N-CC 

$mminf(ai: 

i  >  N}. 

(,y)  rather than # worlds; 

I* Strictly  speaking,  we should  write # worlds”,.’ 
(,y),  since the number also depends 
on  the  choice  of  @. Indeed,  we  do  so  in  the  one  place  where  this  dependence  matters  (Theorem  5.27).  The 
degree  of  belief  is,  however  unaffected  by  expansions  of  the  vocabulary. That  is,  if  @’ >  @ then  the  degree 
of  belief  Pri(9  1 KB) is  the  same  under  the  vocabulary  @’  as  it  is  under  @. 
I2 Of  course, 
lim  N  - 
enough 
the  (known) 

If  we  are  fortunate 
small,  we  can  simply  compute  degrees  of  belief  using 

in  this  case  one  probably  would  not  want  to  consider 
size,  and  it  is  reasonably 

the  domain 
fixed  value  of  N. 

co  anyway. 

to  know 

100 

F:  Bacchus  er  al.  /Artificial 

Intelligence 

87  (1996)  75-143 

for  any  sequence  bounded 

exists 
is  defined  analogously,  where  sups  denotes 

from  below,  even 

UN =  lim  inf,,, 
is  always  bounded 

liminf 
limsup 

The 
The 
If  limhr,,  UN does  exist,  then  IimN,, 
for  any  ?,  the  sequence  PrL(sp  (  KB) 
limsup 
of  nonexistence 
definition. 

liminf 

and 

always  exist.  Thus,  we  do  not  have  to  worry  about 

for  particular  values  of  7:  We  can  now  present 

if  the  limit  does  not. 
the  least  upper  bound  of  S. 
UN =  lim  supN+oo  UN.  Since, 
the 
the  problem 
the  final  form  of  our 

from  above  and  below, 

Definition  4.3.  If 

lim.  liminfPr$(p 
N-m 
i-0 

/ KB) 

and 

lim  limsupPr$(Ip 
i-6 

N-m 

/  KB) 

both  exist  and  are  equal, 
is  defined  as  the  common 

then  the  degree  of  belief  in  (o given  KB,  written  Pr, 

((p  1 KB), 

limit;  otherwise  Pr,(p 

1 KB)  does  not  exist. 

We  point  out  that,  even  using 

this  definition, 

of  belief  does  not  exist.  However,  as  some  of  our  examples 
the  nonexistence 
related 
and  5.3  and  [ 281.) 

to  the  existence  of  multiple  extensions  of  a  default 

of  a  degree  of  belief  can  be  understood 

there  are  many  cases  where  the  degree 
show,  in  many  situations 
and  is  sometimes 
(See  Sections  4.3 

intuitively, 
theory. 

We  remark 

[ 7 I]  used  a  somewhat 

that  Shastri 
of  belief.  His  language  does  not  allow 
but  does  allow  us  to  talk  about 
predicate.  He  then  gives  a  definition  of  degree  of  belief  similar 
notion  of  approximate 
assumption  we  wish  to  avoid),  he  does  not  have  to  deal  with  limits  as  we  do. 

that  satisfy  a  given 
to  ours.  Since  he  has  no 
in  his  language,  and  presumes  a  fixed  domain  size  (an 

to  defining  degrees 
information, 

the  direct  expression  of  statistical 

the  number  of  domain 

similar  approach 

individuals 

equality 

4.3.  Statistical 

interpretation 

for  defaults 

the  appropriate 

is  not  straightforward. 

in  the  introduction, 
information 

finding 
as  is  well  known, 

statements.  However, 
For  example, 

there  are  many  similarities  between  direct  infer- 
and  default  reasoning.  To  capitalize  on  this  observation, 
system,  we  need  to  interpret 
interpre- 
statistical 
“Birds 
if  we  interpret 
fly”  as  “Most  (i.e.,  more  than  50%  of)  birds  fly”,  then  we  get  a  default  system 
such  as  the  And  rule,  discussed 
in  a  straightforward  way  does  not  help. 
[ 201,  suggested  an  interpre- 
this  is  done  using  extreme 
close  to  1:  i.e.,  within  1 -  E 
the  limit  as  E --f  0.  The  basic  system  derived  from  this  idea 

As  we  mentioned 
ence  from  statistical 
and  to  be  able  to  use  random  worlds  as  a  default  reasoning 
defaults  as  statistical 
tation 
typically 
that  fails  to  satisfy  some  of  the  most  basic  desiderata, 
in  Section  3.2.  Using  a  higher  fixed  threshold 
More  successfully,  Adams 
tation  of  defaults  based  on  “almost  all”.  In  their  framework, 
probabilities-conditional 
for  some  E,  and  considering 
is  called  e-semantics.  Later,  stronger  systems 
based  on  the  same  probabilistic 

(that  are  able  to  make  more  inferences) 

[ I 1, and  later  Geffner  and  Pearl 

that  are  arbitrarily 

probabilities 

The  intuition  behind  E-semantics 

since 

the  language  used  in  these  approaches 

idea  were  introduced 
and  its  extensions 

(see  Pearl 
seems 
is  propositional, 

[ 591  for  a  survey). 
to  be  statistical.  However, 
cannot  be 

this  intuition 

E  Bacchus  et al. /Artificial  Intelligence  87  (1996)  75-143 

101 

the 

Indeed, 

between 

statement 

the  approximate 

these  approaches 

explicitly.  Recall 

this  intuition  more  directly 

typically  make  no  distinction 

that  we  interpret  a  statement 

using 
the  use  of  an  approximate 

in  our  approach,  since  we  can  make 
typically 

expressed  directly. 
statistical  nature  of  the  default  and  the  degree  of  belief  nature  of  the  default  conclusion. 
We  are  able  to  capture 
this  distinction 
fly”  statistically, 
(Thus, 
purely  a  technical 
defaults  as  a  generalization 
first-order  case.  The  connection 
issue  of  representation: 
the  generalization 
approach  of  Goldszmidt,  Morris,  and  Pearl 
in  more  detail 
is  discussed 
approach  can  be  embedded 

xi  1  for  some  i. 
is  not 
of 
to  the 
the 
there  is  a  deeper  sense  in  which  we  can  view  our  approach  as 

connective 
Clearly,  we  can  view  our  statistical 

interpretation 
of  defaults 
extends  beyond 

\]Fly(x)  1 Binf(x)\I, 
to  compare  proportion 

in  Section  6,  where 
in  our  framework. 

the  maximum-entropy 
setting.  This 

of  one  of  the  extensions  of  e-semantics,  namely 

between  our  work  and  e-semantics 

of  the  extreme  probabilities 

that  this  maximum-entropy 

to  the  first-order 

such  as  “Birds 

convenience.) 

interpretation 

it  is  shown 

expressions 

issue 

[23], 

Of  course, 

the  fact  that  our  syntax 

that  simply  cannot  be  expressed 

information 
that  a  propositional 
earlier 
contextual  knowledge  has  difficulty 
Section  3.4).  This  example 

approach 

in  any  propositional 

is  so  rich  allows  us  to  express  a  great  deal  of 
approach.  We  observed 
and 
(see 

between  default  knowledge 
example 

in  dealing  with  the  elephant-zookeeper 

that  distinguishes 

is  easily  dealt  with  in  our  framework. 

Example  4.4.  The  following  knowledge  base,  KB likes, is  a  formalization  of  the  elephant- 
zookeeper  example.  Recall, 
typi- 
typically  do  not  like  Fred.  As  discussed  earlier, 
cally  like  zookeepers,  but  (b)  elephants 
simply  expressing 
this  example 
In  our  framework 
can  be  expressed  as  follows: 

this  problem  concerns 

can  be  a  challenge. 

that  (a)  elephants 

this  knowledge 

the  defaults 

]]likes(x,  y)  ( Elephant(x)  A Zookeeper(y)ll.,, 

q  1 A 

]llikes(  x,  Fred)  1 Elephant(x)  IIx ~2  0  A 

Zookeeper(  Fred)  A Elephant(  Clyde)  A Zookeeper(  Eric). 

Furthermore, 

our  interpretation 

of  defaults  allows  us  to  deal  well  with 

interactions 

between 

first-order  quantifiers 

and  defaults. 

Example  4.5.  We  may  know  that  people  who  have  at  least  one  tall  parent  are  typically 
in  our  language: 
tall.  This  default  can  be  expressed 

[(Tall(x)  I3y  (Child(x,y)  A Tall(y))ll,  xi  1. 

We  can  also  define  defaults  over  classes 

themselves  defined  using  default 

rules 

(as 

discussed  by  Morreau 

[56]). 

Example  4.6. 
default  “Typically, 
press 
who  normally 

In  Section  3.4,  we  discussed 
people  who  normally 

the  problem  of  expressing 
late  normally 

rise 

go  to  bed 

this  default  we  can  simply  use  nested  proportion 
late  are  those  who  rise 

late  most  days; 

rise 

the  nested 
late”.  To  ex- 
individuals 
the 
they  are  the  x’s  satisfy- 

statements: 

102 

E  Bucchus  et  al./ArtiJicial 

Intelligence 

87  (1996)  75-143 

the  individuals  who  normally  go  to 
ing  IIRises-la&(x,  y)  1 Day(y) 
bed  late  are  the  x’s  satisfying 
II,.! ~2  1. Thus  we  can  cap- 
ture  the  default  by  saying  most  x’s  that  go  to  bed  late  also  rise  late,  as  in  the  knowledge 
base  KBlute: 

IIY MI  1.  Similarly, 
/[To-bed-Zute(x, y’)  1 Duy(y’) 

/[Rises-late(x, 

II 

y)  I Day(y) 

II?. = I  1 / I/To-bed-lute(  x,  y’)  I Duy(  y’)  IIYf “2  1  .~ z3  I. 

!I 

On  the  other  hand, 
late  (i.e., 

the  next  morning)” 

can  be  expressed  as: 

the  related  default 

that  “Typically,  people  who  go  to  bed  late  rise 

I/Rises-lute(x,Next-day(y) 

1 / Day(y)  A  Ta-bed-lute(n,y)ll.,?.  %:I 1, 

which 

is  clearly  different 

from  the  first  default. 

5.  Properties  of  random  worlds 

We  now  show  that  the  random-worlds  method  validates 

reasoning 
several  desirable 
in  Sections  2 and  3. It  is  worth  noting 

require  any  additional 

in  Section  4.2;  none  of  these  patterns 

including  essentially  all  of  those  discussed 

to  the  method.  We  also  note  that  all  the  results 
in  its  full  generality: 
nonunary 
(including 

patterns, 
that  all  of  these  reasoning  patterns  follow  from  the  basic  definition  of  the  random-worlds 
to 
method  given 
in  this  section  hold  for  our 
be  added 
function  and  predicate 
language 
symbols 
and  proportion 
statements.  Finally,  we  note  that  the  theorems  we  state  are  not  the  most  general  ones 
for  which  the  conditions  of  the  theorems 
possible. 
do  not  hold,  but  random  worlds  still  gives  the  intuitively  plausible  answer.  We  could  find 
theorems 
to  find 
other  results  whose  conditions 
large  class  of  examples.  We  discuss 

the  formulas  can  contain  arbitrary 
predicates), 

are  easy  to  state  and  check,  and  yet  cover  an  interestingly 

It  is  quite  easy  to  construct  examples 

this  issue  again  in  Section  7.4. 

it  seems  to  be  fairly  difficult 

and  have  nested  quantifiers 

that  deal  with  additional 

cases,  although 

structure 

5.1.  Rundom  worlds  and  default  reasoning 

In  this  subsection,  we  focus  on  formulas  which  are  assigned  degree  of  belief  1. Given 
the  statistical 
of  Section  4.3),  we  say  that  p  is  a  default  conclusion  from  KB,  and  write 
satisfies  all  the 
two 

any  knowledge  base  KB  (which  can,  in  particular, 
interpretation 
KB  i_,,  cp, if  Pr,(  9  I  KB)  =  1.  As  we  now  show,  the  relation 
basic  properties  of  default 
somewhat  more  general 

in  Section  3.2.  We  start  by  proving 

include  defaults  using 

inference  discussed 

results. 

k-, 

Proposition  5.1.  If  /= KB  u  KB’,  then  Pr,(p 
cp. '3 

I KB)  =  Pr,(p 

I KB’)  for a~lformulas 

I3 By  Pr,( 
value,  or  neither  exists.  Proposition  5.2  should  be  interpreted  analogously. 

cp (  KB)  =  F’r,(  cp ( KB’)  we  man 

that  either  both  degrees  of  belief  exist  and  have  the  same 

E  Bacchus  et  al./Artifcial 

Intelligence  87  (1996)  75-143 

103 

Proof.  By  assumption, 
for  all  N  and  7’, Pr;  (p  1 KB)  and  Pr;  (sp  1 KB’)  are  equal.  Therefore, 
equal. 

the  same  set  of  worlds  satisfy  KB  and  KB’.  Therefore, 
the  limits  are  also 

precisely 

0 

Proposition  5.2. 

If  KB  t-,  8,  then  Pr,  ( (P I KB)  = Pr,  (cp 1 KB  *  0)  for  any  rP. 

Proof.  Fix  N  and  7’. Then,  by  the  standard  properties  of  conditional  probability,  we  get 

By  assumption,  P&(8 
to  Pr, 
the  second  summand 

(q  ( KB A 0).  Since  Pr;( 

( KB)  tends  to  1 when  we  take  limits,  so  the  first  summand 
-49  1 KB)  has  limit  0  and  Prz(  50 I KB  A 4) 

tends 
is  bounded, 

tends  to  0.  The  result  follows. 

Cl 

Theorem  5.3.  The  relation  b, 
Cut,  Left  Logical  Equivalence,  Or,  Reflexivity, and  Right  Weakening. 

satisjes  the properties  of And,  Cautious Monotonic@, 

Proof. 

And:  As  we  mentioned 

in  Section  3.2,  this  follows 

from  the  other  properties  proved 

below. 

Cautious  Monotonicity  and  Cut:  These  follow 
Left  Logical  Equivalence:  Follows 
immediately 
Or:  Assume  Pr,(4p 

( KB)  =  Pr,(rp 

immediately 

from  Proposition 

5.2. 

from  Proposition  5.1. 

I KB’)  =  1,  so  that  Pr,(-cp 

1 KB)  =  Pr,(lrp 

1 

KB’)  =  0.  Fix  N  and  ?.  Then 

PrG(lp 

1 KBV  KB’) 

=Prc(ylpA(KBVKB’) 

I KBVKB’) 

<P~~(-~AKBIKBVKB’)+P~~(~YAKB’IKBVKB’) 

<Prc(Tq 

I KB)  +Prg(lq 

I KB’). 

Taking 

limits,  we  conclude 

that  Pr,(  79 

I  KB  V  KB’)  =  0.  It  follows 

that 

(KB  V 

KB’)  t-, 

4~. 

RefZexivity: Because  we  restrict  our  attention 
(KB  I KB)  is  well  defined.  But  then  Pr, 

to  KB’s  that  are  eventually 
(KB  I KB)  is  clearly  equal  to  1. 

Pr, 

consistent, 

Right  Weakening:  Suppose  Pr,(  40 I  KB)  =  1.  If  k  p  +  p’,  then  the  set  of  worlds 
for  any  N  and  ?, 

satisfying  C$ is  a  superset  of  the  set  of  worlds  satisfying  C,D. Therefore, 
I$,( 

(p’ 1 KB)  2  Pr$ (40 ) KB)  . Taking 

limits,  we  obtain 

that 

1  2  Pr,((p’ 

I KB)  >  Pr,(qp 

) KB)  =  1, 

and  so  necessarily  Pr, 

(40’ ( KB)  =  1.  Cl 

Besides  demonstrating 

that  k,.,,,  satisfies  the  minimal  standards  of  reasonableness 

a  default 

inference 

relation, 

these  properties,  particularly 

the  stronger 

for 
form  of  Cut  and 

104 

F:  Bacchus  et  al.  /Artificial 

Intelligence 

87  (I  996)  75-143 

proved 

in  Proposition 

5.2,  will  prove  quite  useful 

In  particular,  many  of  our 

in  computing 
Cautious  Monotonicity 
degrees  of  belief,  especially  when  combined  with  some  other  properties  we  prove  below 
show  how  random 
(see  also  Section  7.4). 
forms. 
worlds  behaves 
into  one  that 
Sometimes 
does,  simply  by  extending  KB  with  some  of  its  default  conclusions.  We  then  appeal 
to 
Proposition 
the  new  knowledge  base  instead  of  the  old  one.  The 
other  rules  are  also  useful,  as  shown 
example 

for  knowledge 
a  KB  that  does  not  satisfy 

in  the  following  analysis  of  Poole’s  “broken-arm” 

5.2  to  justify  using 

bases  and  queries 

these  requirements 

that  have  certain 

can  be  changed 

restricted 

results 

[ 621. 

later 

Suppose  we  have  predicates 
respectively, 

indicating, 

that  the  left  arm  is  usable, 

LeftUsable, 

LeftBroken, 

the  right  arm 

is  usable,  and 

the  right  arm 

is  broken.  Let  KB:, 

RightUsable, 
the  left  arm  is  bro- 
consist  of  the 

Example  5.4. 
RightBroken, 
ken, 
statements 

l  IlLeftUsable(x)/l, 

~1  1,  IILefUsable(  x)  I LeftBroken 

I(*  M:!  0  (left  arms  are 

typically  usable,  but  not  if  they  are  broken), 

l  IIRightUsable(x)II, 

“3  1,  IIRightUsable(x) 

( RightBroken(x)(I, 

~4  0  (right  arms 

are  typically  usable,  but  not  if  they  are  broken). 

Now,  consider  KB,,  =  (KB&_,, A  (LeftBroken  (Eric)  V  RightBroken(  Eric)  ) ) ; that  is,  we 
know  that  Eric  has  a  broken  arm.  Poole  observes 
logic, 
that  if  we  use  Reiter’s  default 
and  in  that  extension,  both  arms  are  usable. 
there  is  precisely  one  extension  of  KB,,, 
However, 

it  can  be  shown 

that 

KB:,,,  A  LeftBroken(  Eric)  bvnL. -LeftlJsable(  Eric) 

(see  Theorem  5.6  below)  and  hence 

(using  right  Weakening) 

that 

KB:,,,  A  LeftBroken(  Eric) 

i_,.,,, TLeftUsable(  Eric)  V  TRightUsable(Eric); 

the  same  conclusion 
that 
follows 

is  obtained 

from  KB:,,  A  RightBroken(Eric). 

By  the  Or  rule, 

it 

K&r,,,  F,,  TLeftUsable(  Eric)  V  TRightUsable(  Eric). 

Using  similar 

reasoning,  we  can  also  show  that 

KB,,.,,,  i_,  LeftUsable(  Eric)  V  RightUsable(  Eric). 

By  applying 
arms  is  usable,  but  we  draw  no  conclusions 

the  And  rule,  we  conclude  by  default  from  KB,, 

as  to  which  one  it  is. 

that  exactly  one  of  Eric’s 

The 

final  property  mentioned 

that 
p. 
Rational  Monotonicity 
it 
Random  worlds 
satisfies  Rational  Monotonic&y  except  in  those  situations  where  limits  fail  to  exist.  I4  If 
Pr,(  40 I KB  A  0)  does  exist  it  must  be  equal  to  I,  i.e.,  we  must  have  (KB  A  0)  k, 
40 

in  Section  3.2  is  Rational  Monotonicity.  Recall 
then  (KB  A  19) b, 

p  and  KB  p,,,,  -8 
form  of  Rational  Monotonicity. 

that  if  KB  b, 

In  particular, 

a  weakened 

asserts 

satisfies 

I4  As  we  discuss 

later  in  Section  5.3  there  arc  often  intuitive  reasons  for  the  nonexistence  of  limits. 

E  Bacchus et al./Artifcial 

Intelligence 87 (1996)  75-143 

105 

cp  entails  that  Pr,(  4p 1 KB)  exists.  But  Rational  Monotonicity’s 

as  desired.  Sometimes,  however,  this  limit  does  not  exist.  Note  that  the  assumption 
that  KB  t-, 
other 
assumption,  that  KB  p, 
)  KB)  has  a  value  less  than  one 
or  if  this  degree  of  belief  does  not  exist.  It  is  the  latter  “incompatibility”  of  8  with 
KB  that  is  a  potential  source  of  problems.  In  this  case  the  combination  of  KB  and  0 
may  fail  to  assign  a  limiting  degree  of  belief  to  rp even  though  KB  by  itself  did.  The 
following  theorem  summarizes  the  status  of  Rational  Monotonic@ 
in  the  random-worlds 
approach. 

-4  holds  if  either  Pr,(B 

Theorem  5.5.  Assume 
that  KB  k,.,,,  4p and  KB  p, 
that  Pr,  (p  1 KB  A  0)  exists.  Moreover,  a  s@cient 
exist  is  that  Pr,  (8  1 KB)  exists. 

4.  Then  KB  A  0  k,.,,,  4p provided 
to 
condition  for  F’r,(q 

1 KB  A  0) 

Proof.  Longer  proofs,  including  the  proof  of  this  result,  are  in  the  Appendix.  0 

5.2.  Specificity  and  inheritance 

in  random  worlds 

One  way  of  using  random  worlds  is to  derive  conclusions  about  particular  individuals, 
based  on  general  statistical  knowledge.  This  is, of course,  the type  of reasoning  reference- 
class  theories  were  designed  to  deal  with.  Recall,  these  theories  aim  to  discover  a single 
summarizes  all the  relevant 
piece  of  data-the 
information.  This  idea  is  also  useful  in  default  reasoning,  where  we  sometimes  want  to 
find  a single  appropriate  default.  Random  worlds  rejects  this  idea  as a  general  approach, 
but  supports  it  as  a  valuable  heuristic  in  special  cases. 

statistics  for  a single  reference  class-that 

In  this  section,  we  give  two  theorems  covering  some  of  the  cases  where  random 
worlds  agrees  with  the  basic  philosophy  of  reference  classes.  Both  results  concern 
spec$city-the 
idea  of  using  the  “smallest”  relevant  reference  class  for  which  we  have 
statistics.  However,  both  results  also  allow  some  indifference  to  irrelevant  information. 
In  particular,  the  second  theorem  also  covers  certain  forms  of  inheritance 
(as  described 
in  Section  3.3).  The  results  cover  almost  all  of  the  noncontroversial  applications  of 
specificity  and  inheritance  that  we  are  aware  of,  and  do  not  seem  to  suffer  from  any  of 
the  commonly  found  problems  such  as the  disjunctive  reference-class  problem  (see  Sec- 
tion  2.2).  Because  our  theorems  are derived  properties  rather  than postulates,  consistency 
is  assured  and  there  are  no  ad  hoc  syntactic  restrictions  on  the  choice  of  possible  refer- 
ence  classes.  We  remark  that  Shastri  [ 7 1 ]  has  also  observed  that  irrelevance  properties 
hold  in  his  framework. 

Our first,  and simpler,  result  is basic  direct  inference,  where  we have  a single  reference 
class  that  is precisely  the  “right  one”.  That  is, assume  that  the  assertion  G(c)  represents 
everything  the  knowledge  base  tells  us  about  the  constant  c.  In  this  case,  we  can  view 
the  class  defined  by  #I(x)  as  the  class  of  all  individuals  who  are  “just  like  c”.  If  we 
have  adequate  statistics  for  the  class  9(x), 
then  we  should  clearly  use  this  information. 
For  example,  assume  that  all  we  know  about  Eric  is  that  he  exhibits  jaundice,  and  let 
$  represent  the  class  of  patients  with  jaundice.  If  we  know  that  80%  of  patients  with 
jaundice  exhibit  hepatitis,  then  basic  direct  inference  would  dictate  a  degree  of  belief 
of  0.8  in  Eric  having  hepatitis.  We  would,  in  fact,  like  this  to  hold  regardless  of  any 

106 

F:  Bucchus  et  ul.  /ArtiJicial 

Intelligence 

87  (1996)  75-143 

information  we  might  have  in  the  knowledge  base.  For  example,  we  may  know 
other 
the  proportion  of  hepatitis  among  patients 
in  general,  or  that  patients  with  jaundice  and 
fever  typically  have  hepatitis.  But  if  all  we  know  about  Eric  is  that  he  has  jaundice,  we 
would  still  like  to  use  the  statistics 
regardless  of 
the  additional 

for  the  class  of  patients  with  jaundice, 

information. 

intended 

it  may  be.)  Clearly, 

in  order  for  the  result 

I KB)  = a”.  (Here,  KB’  is  simply 

asserts  the  following:  “If  we  are  interested 

Our  result  essentially 
in  q(c), 
that  Pr,(p(c) 

in  obtaining  a  degree 
and  the  KB  is  of  the  form  1+4(c) A  I/q(x)  1 G(x)  II*  z  LY A KB’,  then 
to  denote 
the  rest 
to  hold,  we  must  make 

of  belief 
conclude 
of  KB,  whatever 
certain  assumptions.  The  assumptions  we  consider  can  be  viewed  as  ensuring 
represents  all  the  information  we  have  about  c.  First,  for  obvious 
KB’  does  not  mention  c.  However, 
not  appear  in  either  q(x)  or  $(x) 
that  p(x) 
is  Q(x)  V  x  =  c,  +4(x) 
result  held  in  this  case,  we  would  erroneously 
since  p(c) 
holds 
the  constant  c  cannot  appear 

reasons,  we  require  that 
that  c  does 
this  is  not  enough;  we  also  need  to  assume 
suppose 
To  understand  why  c  cannot  appear  in  p(x), 
is  true,  and  the  KB  is  119(x)  I truell,  =I  0.5.  If  the 
((o( c)  I KB)  = 0.5.  But 
I  KB)  =  1.  To  see  why 

tautologically,  we  actually  obtain  Pr,(qo(c) 

is  (P(x)  A x  #  c)  V-P(x), 

that  $(x) 

that  e(c) 

conclude 

that  Pr, 

suppose 

in  $(x), 

is  P(X),  and  the  KB  is  e(c)  A  I/P(x) 

/ cc/(x)llX  z:2  0.5.  Again, 

q(x) 
we  would  be  able  to  conclude 
-P(c), 

so  in  fact  Pr,(P(c) 

that  Pr,(  P(c) 

( KB)  = 0. 

/ KB)  = 0.5.  But  +9(c)  is  equivalent 

if  the  result  held, 
to 

As  we  now  show,  these  assumptions 

In  the  following, 

the  theorem  generalizes 
than  one  individual 
theorem). 
variables  and  distinct  constants, 
all  of  the  free  variables 
new  formula 
constants  not  among 

formed  by  substituting 

suffice  to  guarantee 

the  basic  principle 
at  a  time  (as  is  demonstrated 
let  x’=  {xl,. 

result.  In  fact, 
the  desired 
to  properties  and  classes  dealing  with  more 
the 
. . , ck}  be  sets  of  distinct 
that 
the 
each  Xi  by  c;  in  p.  Note  that  p  may  contain  other 

. , xk}  and  c’=  {cl,. 
respectively.  Furthermore,  we  use  ~(2) 

to  indicate 
in  the  formula  9  are  in  2,  and  we  use  qo(c3  to  denote 

in  some  of  the  examples 

following 

the  ci’s;  these  are  unaffected  by  the  substitution. 

Theorem  5.6.  Let  KB  be  a  knowledge  buse  of  the  form  @(a  A  KB’,  and  assume 
for  all  suficiently 

small  tolerance  vectors  7’, 

that 

If  no  constant 
[a,  p]  , provided 

in  c’ appears 

in  KB’,  in  q(i), 
the  degree  of  belief  exists.  Is 

or  in  e(2), 

then  Pr,(q(?‘) 

I  KB)  E 

Proof.  See  the  Appendix. 

0 

Theorem  5.6  refers 
inferred 
from 
base  contains 

the  knowledge 
the  relevant 

information 

to  any  statistical 

information 

about 

(lqo(x’) 1 +4(x’) 117 that  can  be 

base.  An  important 

special  case  is  when 

the  knowledge 

I5 The  degree  of  belief  may  not  exist  since 
limi_G 

Pri  (~0  1 KB).  However, 

lim  SU~~_~ 

lie  in  the  interval  [a,  81.  A  similar  remark  holds  for  many  of  our  later  results. 

explicitly. 

lim,~lim 

inf,,,_m  PrL(p 

/  KB)  may  not  be  equal 

to 

it  follows  from  the  proof  of  the  theorem  that  both  these  limits 

E  Bacchus  et  al.  /Artificial 

Intelligence 

87  (1996)  7.5-143 

107 

Corollary  5.7.  Let  KB’  be  the  conjunction 

rl/(c3  A  (Q! 5i  ll9(3 

I +(3ll? 

ij  P) . 

Let  KB  be  a  knowledge  base  of  the form  KB’  A KB”  such  that no  constant  in  c’appears 
in  KB”,  in  cp( 2)  ,  or  in  1//( 2).  Then,  if  the  degree  of  belief  exists,  we  have 

Pr,(so(c’) 

I KB)  E  [a,Pl. 

Proof.  Let  E  >  0,  and  let  7’be  sufficiently 
(a  3;  (\~(a 
Theorem  5.6,  Pr, 
is  necessarily 

small  so  that  Ti, rj  <  E. For  this  ?,  the  formula 
I G(Z)  11~ 3.i  P)  implies  11dx3  I fiC.3  11~ E  [a  -  E, p  +  ~1.  Therefore,  by 
(p(  ?)  I KB)  E  [a  -  E, p  +  E] . But  since  this  holds  for  any  E >  0,  it 

the  case  that  Pr,(  p(  Z)  I KB)  E  [a,  p]  .  0 

It  is  interesting 

to  note  one  way  in  which  this  result  diverges  from  the  reference-class 

the  exact  value  of  the  degree  of  belief  within 

and  that  our  knowledge  base  KB  is  as 
1 

indeed  conclude 

5.7.  While  we  can 

that  Pr,(cp(Z) 
interval  depends 
in  the  knowledge  base.  Thus,  while  random  worlds  certainly 
the 
is 
if  the  interval 
then  we  may  not  care  exactly  where 

$  p,  it  does  not  necessarily 
the  other  hand, 

base  altogether.  On 
in  particular,  when  (Y =  p), 

ignore 
[ a,P] 

I $(x)I(, 

this 

information 

of  Corollary 

the  information 

paradigm.  Suppose  we  consider  a  query  p(c), 
in  the  hypothesis 
KB)  E  [a,  /3], 
on  the  other 
uses 
rest  of  the  knowledge 
sufficiently 
small  (and, 
in  the  interval 
in  KB’,  and  use  the  single  piece  of  “local” 
belief. 

the  degree  of  belief 

lip(x) 

LY <i 

lies.  In  this  case,  we  can  ignore  all  the  information 

information 

for  computing 

the  degree  of 

We  now  present  a  number  of  examples 

that  demonstrate 

the  behavior  of  the  direct 

inference 

result. 

Example  5.8.  Consider 
earlier.  In  the  notation  of  Corollary  5.7: 

a  knowledge  base  describing 

the  hepatitis  example  discussed 

K&p  =  Jaun(Eric)  A  IIHep(x)  1 Jaun(x)  IIx XI  0.8, 

and 

KBhr,, =  KBL,  A  IJHep(x)ll,  52  0.05  A  l(Hep(x) 

I Jaun(x)  AFever(x)ll. 

x:!  1. 

Then  Pr,(Hep(Eric) 
classes 
are  also  ignored, 

(whether  more  general  or  more  specific) 

I  KBhep)  =  0.8  as  desired; 

information 

reference 
is  ignored.  Other  kinds  of  information 

about  other 

for  example, 

information 

about  other  individuals.  Thus, 

Pr,(Hep(Eric) 

I K&,  A Hep(Tom))  = 0.8. 

Although 

it  is  nothing  but  an  immediate 

application 

that  the  principle  of  Direct  Inference  for  Defaults  (Section  3.3) 

of  Theorem  5.6,  it  is  worth 
is  satisfied 

remarking 
by  random  worlds: 

Corollary  5.9.  Suppose  KB  implies  I(q( 2)  ( (/I (2)  112 Xi  1, and  no  constant  in c’appears 
I KB  A$(c?))  =  1. 
in  KB,  rp, or  +.  Then  Pr,((p(c?) 

108 

E  Bacchus  et al. /Artificial  Intelligence  87  (I 996)  75-143 

As  discussed 
fication  hierarchies 

are  possible. 

in  Section  3.3,  this  shows  that  simple  forms  of  reasoning 

about  classi- 

Example 
of  defaults: 

5.10.  The  knowledge  base  KBpI,. from  Section  3.3  is,  under  our  interpretation 

IIFfy 

/ Bird(x)/(, 

=I  1 A 

II/+(X) 

1 Penguins/, 

x2  0  A 

‘v’x( Penguin(x) 

+  Bird(x)  1. 

Then  Pr,(F1y( 
Tweety 

Tweets)  1 KBJ!  A  fenguirz(  Tweery)  )  =  0.  That 

that 
the  penguin  does  not  fly,  even  though  he  is  also  a  bird  and  birds  generally  do 

is,  we  conclude 

fly. 

Given 

this  preference 

for  the  most  specific 

reference  class,  one  might  wonder  why 

random  worlds  does  not  encounter 
Section  2.2).  The  following  example,  based  on  the  example 
one  answer. 

the  problem  of  disjunctive 

reference 

(see 
from  Section  2.2,  provides 

classes 

Example 
and  consider 

the  disjunctive 

reference  class 

5.11.  Recall 

the  knowledge  base  KB&,,  from 

the  hepatitis  example  above, 

Clearly,  as  the  domain  size  grows  large,  lIHep(x) 
0.  t6  Therefore, 

for  any  fixed  E >  0, 

/ e(x) 

/Ix  becomes  arbitrarily  close  to 

Pr,(l/Hep(x) / G(X) IIA E [O-E] 1 K%p) = 1. 

We  can  construct  a  new  knowledge  base 

KB v/7efp =  KBj,,, 

A 

IIHep(x) 

/  @I(x) 

II.r  E 

lk~l. 

a  more  specific 

/  KB&,)  =  Pr,(  Hep(Eric) 

/== I++( Eric).  Hence,  KBv/,,,,  contains 

than  Jaw(x) 
that  Pr,(  Hep(  Eric) 

this  to  be  equal  to  0.8.  So  random  worlds  avoids  using 

Furthermore,  KBvt,,,, 
class  for  Hep(Eric) 
we  know 
ple  5.8  we  showed 
disjunctive 
this  class.  Theorem  5.6  does  not  apply  here  because 
tions 
the  constant  Eric.  Another  way  of  seeing 
random-worlds 
statistics  are  true  in  almost  all  worlds.  Hence  $(x)‘s 
the  sets  of  worlds  that  determine 

reference 
with  very  different  statistics.  Yet,  by  Proposition  5.2, 
/  KBvhcp),  and  in  Exam- 
the  spurious 
from 
the  class  cl/(x)  explicitly  men- 
that  the  class  G(x)  does  not  affect  the 
i.e.,  these 
statistics  places  no  constraints  on 
the  degree  of  belief.  As  we  shall  see  in  Example  5.22, 

even  in  a  knowledge  base  that  explicitly 

that  its  statistics  are  not  informative, 

includes  statistics 

is  to  observe 

computation 

class  $(x) 

I6 This  actually 
jaundiced  patients  without  hepatitis 

relies  on  the  fact  that,  with  high  probability, 

the  proportion 

(as  the  domain  size  grows)  of 

is  nonzero.  We  do  not  prove  this  fact  here;  see  126.57  1. 

E  Bacchus  et  al./Artificial 

Intelligence  87  (I  996)  75-143 

109 

when  we  do  have  informative 
the  class  is  disjunctive. 

statistics 

for  a  class,  those  statistics  can  be  used,  even  if 

As  we  have  said,  we  are  not  limited 

to  unary  predicates,  nor  to  examining 

only  one 

individual 

at  a  time. 

the  elephant-zookeeper 

two  queries.  First,  assume 

in  Section  3.4.  As  we  now  show,  the  natural  representation 

(cI(x,  y)  =  Elephant(x) 
in  KBlikes,  we  can  conclude 

Example  5.12.  In  Example  4.4,  we  showed  how  to  formalize 
example  discussed 
indeed  yields 
are  interested 
of  pairs 
default 
we  examine  whether  or  not  Clyde 
default 
in  KBlike,,  we  can  conclude 
that  we  cannot  apply  Corollary  5.9  to  the  first  default 
likes  Fred.  The  conditions 
used  elsewhere 

of  KBIikes 
that  we 
the  answers  we  expect.  We  consider 
in  finding  out  whether  Clyde  likes  Eric.  In  this  case,  we  can  use  the  class 
A  Zookeeper(  y).  Applying  Corollary  5.9  to  the  first 
Clyde,Eric)  1 KBlikes) =  1.  Second, 
that  Pr,(Likes( 
likes  Fred.  Applying  Corollary  5.9  to  the  second 
(Likes  (Clyde,  Fred)  1 KBlikes) =  0.  Note 
that  Pr, 
that  Clyde 
is 

in  the  knowledge  base. 

in  KBlikes to  conclude 

are  violated,  because 

the  constant  Fred 

of  the  corollary 

The  same  principles 

continue 

to  hold  for  more  complex  sentences; 

for  example,  we 

can  mix  first-order 

logic  and  statistical  knowledge 

arbitrarily 

and  we  can  nest  defaults. 

the  default:  “People  who 
Example  5.13.  In  Example  4.5,  we  showed  how  to  express 
have  at  least  one  tall  parent  are  typically 
tall”.  If  we  have  this  default,  and  also  know 
that  3y  (ChiZd(Alice,  y)  A Tall(y)  )  (Alice  has  a  tall  parent),  Corollary  5.9  tells  us  that 
we  can  conclude  by  default 

that  Tall(AZice). 

Example  5.14.  In  Example  4.6,  we  showed  how  the  default  “Typically,  people  who 
normally  go  to  bed  late  normally 
the 
knowledge  base  KBrate. Let  KBj,,  be 

rise  late”  can  be  expressed 

in  our  language  using 

KB,,,  A  /ITo-bed-hte(Alice,y’) 

1 Duy(y’)IIY~  ~2  1. 

By  Corollary  5.9,  Alice 

typically 

rises  late.  That  is, 

Pr,( 

IIRises-lute(Alice,  y)  I Day(y) 

IIY xi  1  I KBj,,,)  =  1. 

By  Cautious  Monotonicity  and  Cut, we can  add  this  conclusion 
to  KB;,,.  By  Corollary  5.9  again,  we  then  conclude 
late  on  any  particular  day,  say  Tomorrow.  So,  for  instance: 

(which 
that  Alice  can  be  expected 

is  itself  a  default) 
to  rise 

Pr, 

(Rises-Zute(Alice,  Tomorrow)  1 KB$,  A Duy(  Tomorrow) > =  1. 

In  all  the  examples  presented 

class 

the  right  reference 
Theorem  5.6  and  its  corollaries 
information 
base  KBh, 

is  not  detailed  enough 
the  hepatitis 
from 

to  match  our  knowledge 

so  far  in  this  section,  we  have  statistics 
the  individual(s) 
about 
this.  Unfortunately, 

for  precisely 
in  question; 
in  many  cases  our  statistical 
the  knowledge 

for  Theorem  5.6  to  apply.  Consider 

require 

example.  Here  we  have  statistics 

for  the  occurrence  of 

110 

I?  Bacchs 

er  ul.  /Arrificial 

Intelligence 

87 

(1996) 

78-143 

the  class  of  patients  who  are  just 

among 

in  Hep(Eric). 

a  degree  of  belief 

hepatitis 
to  induce 
KB,,,  A Tufl(  Eric).  Since  we  do  not  have  statistics 
tall  patients, 
ignore  Td(Eric). 
solve  this  problem 
currently  have.  Nevertheless, 
the  less  controversial 

examples 

found 

But  what  entitles  us  to  ignore  Td(Eric) 
in  complete  generality 

like  Eric,  so  we  can  use 
the  knowledge 

these 
base 
for  the  frequency  of  hepatitis  among 
like  to  be  able  to 
To 
than  we 
including  many  of 

and  not  Jaun(Eric)‘? 

our  next  theorem  covers  many  cases, 
in  the  default  reasoning 

the  results  we  have  seen  so  far  do  not  apply.  We  would 

requires  a  better  theory  of  irrelevance 

But  now  consider 

literature. 

The  theorem  we  present  deals  with  a  knowledge  base  KB  that  defines  a  “minimal” 

class  $0  with  respect 
information 
suppose 

regarding 
that,  among 

other  classes  are  strictly 

reference 
gives  statistical 
qi(x). 
Further 
minimal-all 
states 
that 
induce  a  degree  of  belief 
we  are  allowed 
be  treated  as  being 
example: 

to  the  query  p(c).  More  precisely, 

there 

assume 

is  one  class  &(x) 

larger  or  entirely  disjoint 

that  KB 
/Ip(  x)  1 ti;(  x)  jIx  f or  a  number  of  different  classes 
is 
these  classes, 
it.  Our  result 
[Ix  to 
j $0(x) 
is  that 
result 
information  will 
is  best  explained  using  an 

for  lisp(x) 
this  such  an  interesting 

from 

that 

to  know  1?1o~e about  c  than  just  +e(  c)  ;  any  extra 

if  we  also  know  @c(c),  we  can  use  the  statistics 

irrelevant.  This  pattern  of  reasoning 

in  p(c).  What  makes 

Example 
about  birds  and  animals; 
domain.  Moreover,  KBt(lX,,,,,,m, contains 
ability  of  various 

types  of  animals: 

5.15.  Assume  we  have  a  knowledge  base  KBtrrxon,,,,lX containing 

information 

in  particular,  KB,olo,lo,l,\. contains  a  taxonomic  hierarchy  of  this 
the  swimming 

the  following 

information 

about 

JjSwims(  x)  1 Penguin(x) 

(I,, %I  0.9  A 

IlSwims(  x)  / Spari-ow(  x)  jl.r z2  0.0  I A 

&!hvims(x) 

I Bird(x)ll, 

zj  0.05  A 

IISwims(x) 

I Animal(x) 

11, E=J 0.3  A 

~pvims(x) 

1 Fish(x)//., 

=5  I. 

then  in  order  to  determine  whether  Opus  swims 
If  we  also  know  that  Opus  is  a  penguin, 
classes  are  either 
the  best  reference  class  is  surely 
larger  (in  the  case  of  birds  or  animals), 
(in  the  case  of  sparrows  and  fish). 
This  is  the  case  even  if  we  know  that  Opus  is  a  black  penguin  with  a  large  nose.  That  is, 
Opus  inherits 
the  class 
of  individuals 

for  the  minimal  class  @o-penguins-even 

the  class  of  penguins.  The  remaining 

the  statistics 
just 

like  Opus  is  smaller 

or  disjoint 

than  Qe. 

though 

in  (P(X)  appear 

That  random  worlds  validates 

this  intuition 

requires 

that  no  symbol 

of  a  unique  minimal 

theorem 
in  statistics  of  the  form  119(x)  / $(x) 
assumption 
violation  of  this  condition, 
$(.x) 
identify 
will  not  notice 

is  in  fact  a  reference 

this.  Obviously 

reference 

II*  f or  various  $(x).  This 

is  necessary 
reference  class  to  be  a  practical  one.  Suppose 

is  formalized 

in  the  knowledge  base  other 

in  the  next  theorem.  This 
than 
for  our 
that,  in 
(P(X)).  Clearly 
is  100%).  But  if  we 
IIX, we 
I $(x) 
assumption  needs  to  consider  all  reference 

for  terms  of  the  form  Ill 

the  knowledge  base  contains  Vx(@(n)  + 
(where 
class  for  r&x) 

the  statistic 

classes  only  by  looking 

the  minimality 

E  Bacchus  et  al./Art#cial 

Intelligence 

87  (1996)  75-143 

111 

irrespective  of  syntactic 

classes, 
and  nonobvious  ways  to  constrain 
assuming 
in  explicit  statistical 
that  addresses  cases  in  which  this  assumption 

that  the  only  mention  of  information 

assertions.  Of  course, 

statistics 

form.  But  because  first-order 

relating 

to  rp( x),  we  simplify 
that  might  be  related  to  p(x) 

logic  provides  many  subtle 
the  issue  by 
is  contained 
to  find  a  result 

it  would  be  very  interesting 

is  not  true. 

Theorem  5.16.  Let  c  be  a  constant  and  let  KB  be  a  knowledge  base  satisfying 
following 

conditions: 

the 

(4  KB  k 
(b)  for  any  expression  of  the  form 

fin, 

I]qp(x> 1 I)(X)  IIx  in  KB,  it  is  the  case  that  either 

* 

or 

ccl(x)) 

that  KB 
function,  and  constant) 

KB  l= V.~$O(X) 
the  (predicate, 
the  left-hand  side  of  the  conditionals 
condition 
the  constant  c  does  not  appear 

(b)  , 

that  for  all  s@iciently 

small  tolerance  vectors  7’: 

in  the  formula  cp( x). 

k  W@O(X) 
symbols 

* 

+(x) 

), 

in  p(x) 

appear 

in  the  proportion 

expressions  described 

in  KB  only  on 
in 

(c) 

(d) 
Assume 

KB[77  k  1140(x) I $otx)Ilx  E  [a,Pl. 

Then  Pr, 

(cp( c)  I KB)  E  [a,  B]  , provided 

the  degree  of  belief  exists. 

Proof.  See  the  Appendix. 

0 

Again, 

the  following 

analogue 

to  Corollary  5.7  is  immediate: 

Corollary  5.17.  Let  KB’  be  the  conjunction 

$O(c) 

A  (a  5i 

IlP(X) 

I  +O(X)llx 

5j  P). 

Let  KB  be  a  knowledge  base  of  the  form  KB’  A  KB”  that  satisfies  conditions 
and  (d)  of  Theorem  5.16.  Then,  if  the  degree  of  belief  exists, 

(b), 

(c), 

Pr,(qo(c) 

I KB) E [a,Pl. 

This  theorem  and  corollary  have  many  useful  applications. 

from  Example  5.8.  In  that  example,  we  supposed 

the  knowledge 

Example  5.18.  Consider 
and  hepatitis 
about  Eric  contained 
that  Eric’s  hospital 
more  realistic 
this  fact.  Theorem  5.16  allows  us  to  ignore 

in  the  knowledge 

to  assume 

bases  KB& 

and  KBt,, 

base  was  that  Eric  has  jaundice. 

records  contain  more  information 
this  information 

concerning 

jaundice 
that  the  only  information 
It  is  clearly 
than  just 
in  a  large  number  of  cases. 

For  example, 

Pr, 

( Hep  ( Eric) 

I KBLeP A  Fever(  Eric)  A  Tall(  Eric)  )  = 0.8, 

as  desired.  On  the  other  hand, 

Pr, 

( Hep  ( Eric) 

I KBt,,  A  Fever(  Eric)  A  Tall(  Eric)  )  =  1. 

II2 

F:  Bacchus  et  (11. /Arr$cial 

Intelligence 

87  (1996)  75-143 

that  KBh, 

(Recall 
not.)  This  shows  why  it  is  important 
for  the  query  p.  The  most  specific 

includes 

A  Tall(Eric) 

KB;,,,,  A  Fever(Eric) 
to  K&c,,  A Fever(Eric) 
latter  case, 
the  less  specific 
cases  Theorem  5.16  allows  us  to  ignore 
theorem  does  not  allow  us  to  conclude 

A  Tull(Eric) 

reference 

that 

that  we  identify 
reference 
is  ((Hep(x) 

I[Hep(x)  1 Juun(x)  A  Fever(x)  11, ~2  1,  while  KB&  does 
the  most  specific  reference  class 
for  Hep(Eric) 
to 
(Ix z=]  0.8,  while  with  respect 
IIx ~2  1.  In  the 
classes  Juun  and  true  are  ignored,  and  in  both 
the  extra  information  Tall(Eric).  Note  that  the 

/ Juun(x)  A Fever(x) 

statistic 
j Juun(x) 

it  is  IIHep(x) 

with  respect 

Pr, 

(Hep(  Eric) 

( KBhr,,  A  Tall(  Eric  1)  = 0.8 

is  no  longer 

The  class  Juun 
statistics 
fact,  reached  by  random  worlds. 

for  the  more  specific  class  Juun  A  Fever.  Nevertheless, 

the  unique  most  specific  reference  class,  since  we  also  have 
is,  in 

this  conclusion 

in  Section  3.3,  various 

As  discussed 
default  reasoning 
cases  (which  can  also  be  seen  as  applications 

in 
as  well.  To  begin  with,  we  note  that  Theorem  5.16  covers  the  simpler 

inheritance  properties  are  considered  desirable 

of  Rational  Monotonicity) 

: 

Example  5.19.  In  simple  cases,  Theorem  5.16  shows  that  random  worlds  is  able  to  apply 
defaults 
information.  For  example, 
using  the  knowledge  base  KBfr?  (see  Example  5.10): 

in  the  presence  of  “obviously 

irrelevant”  additional 

Pr, 

( HJ(  Tweet)‘)  / Kf3hY A Penguin(  Tweety)  A  kllow(  Tweety)  )  =  0. 

That  is,  Tweety 

the  yellow  penguin 

is  still  not  able  to  fly. 

Theorem  5.16  also  validates  more  difficult 

in  which  a  class  that  is  exceptional 

in  one  respect  can  nevertheless 

reasoning  patterns 
theories.  In  particular,  we  validate  exceptional 

that  have  caused  prob- 
subclass 
inherit 

lems  for  many  default  reasoning 
inheritance, 
other  unrelated  properties: 

Example  5.20.  If  we  consider 
get: 

the  property  of  warm-bloodedness 

as  well  as  flight,  we 

Pr, 

Wurm-blooded(Tweety) 

Ij Warm-btooded(x) 

1 Bird(x) 

III ~3 

I 

=  ” 

KBfY  A Penguin(  Tweety)  A 

Knowing 
assuming 

that  Tweety  does  not  fly  because  he  is  a  penguin  does  not  prevent  us  from 
that  he  is  like  typical  birds  in  other  respects. 

The  drowning  problem  variant  of  the  exceptional 

subclass 

inheritance  problem 

is  also 

covered  by  the  theorem. 

Example  5.21.  Suppose  we  know,  as  in  Section  3.3,  that  yellow  things  tend  to  be  highly 
visible.  Then: 

F: Bacchus et al./Arti&ial 

Intelligence 87  (1996)  75-143 

113 

Pr, 

Easy-to-see(  Tweety) 

KB~~Y A  Penguin(  Tweety)  A  Yellow(  Tweety)  A 

I(Easy-to-see(x) 

1 Yelfow(x)ll.  ~3  1 

=  1. 

Here,  all  that  matters 
an  exceptional 

is  that  Tweety 
bird  at  that,  is  rightly 

is  a  yellow  object.  The  fact  that  he  is  a  bird,  and 
ignored. 

Notice 

that,  unlike  Theorem  5.6,  the  conditions 

of  Theorem  5.16  do  not  extend 

to 

let  KB’  be  ] ]Hep(  x)  A +Zep(y) 

lies  in  the  ability  of  the  language 

in  q(  c?),  where  c’ is  a  tuple  of  constants.  Roughly  speaking, 
between  different 
to  create  connections 
IIX,y MI  0.2.  By 
) KB’)  = 0.2. 
(Hep(  Tom)  A-+Zep(  Eric) 
1 KB’  A  Tom  = Eric)  = 0.  The  additional 
regarding  Tom  and  Eric  cannot  be  ignored.  A  version  of  Theorem  5.16 
attempt 
This 
to  the  use  of  equality,  but 
related  only 

inferring  degrees  of  belief 
the  reason 
constants 
in  the  tuple.  For  example, 
Theorem  5.6  (taking  $0 (xi,  x2)  to  be  true),  Pr, 
But,  of  course,  Pr,(Hep( 
information 
where  we  replaced  c  by  c’ would 
example  might  suggest 
more  complex  examples 
As  a  final  example 

that  this  is  a  problem 
that  do  not  mention  equality  can  also  be  constructed. 

in  this  section,  we  revisit  the  issue  of  disjunctive 

Tom)  A  +lep(Eric) 

this  information. 

incorrectly 

to  ignore 

in  Example 
problem. 

random  worlds  does  not  suffer 

5.11, 
In  Section  2.2,  we  observed 

As  we  saw 
reference-class” 
problem  by  simply  outlawing 
such  classes  are  sometimes  useful.  The  next  example  demonstrates 
does,  in  fact,  treat  disjunctive 

reference  classes  appropriately. 

disjunctive 

reference 

classes,  which 

that  some  systems  avoid 
is  problematic, 

this 
as 
that  random  worlds 

reference  classes. 
the  “disjunctive 

from 

Example  5.22.  Recall 
reference 
represented, 

classes 

for  Tay-Sachs  disease.  The  corresponding 
as  the  knowledge  base  KB: 

in  our  framework, 

that  in  Section  2.2  we  gave  an  example 

involving  disjunctive 

statistical 

information  was 

/(TS( x)  I EEJ(x)  V  K(x) 

IIX xl  0.02. 

Given  a  baby  Eric  of  eastern-European 

extraction,  Theorem  5.16  shows  us  that 

Pr, 

(TS(  Eric)  1 KB  A  EEJ(  Eric)  )  = 0.02. 

is  able  to  use  the  information 
it  to  an  individual 

known 

That  is,  random  worlds 
erence  class,  and  apply 
inheritance 
ing  to  which  of  the  two  populations 
reference  classes  are  treated 

it  also  deals  with  the  case  where  we  have  additional 

to  be  in  the  class; 

derived  from  the  disjunctive 

ref- 
through 
determin- 
belongs.  Thus,  disjunctive 

information 

indeed, 

in  the  same  manner  as  other  potential 

reference  classes. 

this  specific 

individual 

reasoning  covered  by  the  results 

The  type  of  specificity  and  inheritance 
inheritance 

reasoning.  While 

are  special  cases  of  general 
random  worlds  does  support  many  noncontroversial 
theorem  asserting 
a  more  general 
the  existence  of  numerous  divergent 
[ 721)  .  We  are  currently  working 
the  case  in  which  we  have  an  inheritance  hierarchy  of  defaults  and  universal 

subtle 
for  inheritance 

is  surprisingly 

and  intuitions 

this  claim 

semantics 

theorems 

in  this  section 
that 
show 
instances  of  such  reasoning,  proving 
(partly  because  of 

these 

towards  stating  and  proving  such  a  general  claim,  for 

implications. 

reasoning 

114 

17 Bacclzu.~  et  ui.  /Artificial 

Intelligence 

87  (1996)  75-143 

it  is  easy 

in  an  arbitrary 

to  see  that  random  worlds  does  not  validate  general 
(i.e.,  where  some  statistical 
than  1,  and  so  do  not  state  defaults).  We  discuss  why  this  happens 
in  all 

that  we  should  not  want  simple 

inheritance 

statistical 

context 

in  Example  5.25,  and  argue 

the  other  hand, 
reasoning 

On 
inheritance 
values  are  less 
below, 
contexts  anyway. 

5.3.  Competing 

reference  classes 

In  previous 

sections  we  have  always  been  careful 

candidates 

there  was  an  obviously 
fortunate.  Reference-class 
competing 
problem,  because 
competing 
competing 
the  second  between 
and  the  last  between 
not  applicable. 

classes. 
information. 

the  degrees  of  belief 

“best” 

for  the  best  class.  However, 

theories  usually  cannot  give  useful  answers  when 

examples 

to  consider 

in  which 
reference  class.  In  practice,  we  will  not  always  be  this 
there  are 
random  worlds  does  not  have  this 
of  the  values  for 
three 
types  of 
and  accuracy, 
that  is  too  general, 
is 

reference  classes,  so  that  the  specificity  principle 

that  is  too  specific  and  information 

it  defines  can  be  combinations 

terms, 
specificity 

conflicts  between 

in  very  general 

In  this  section  we  examine, 
The  first  concerns 

information 

incomparable 

We  discussed 

to  use  the  tighter 

that,  to  assign  a  degree  of  belief 

the  conflict  between  specificity  and  accuracy 

lem  was  noticed  by  Kyburg,  who  addresses 
tion  2.3,  we  argued 
be  able 
specific  reference  class.  As  we  observed,  Kyburg’s  strength  rule  attempts 
intuition.  As  the  following 
intuition 
(without 
a  chain.  ” 

in  Section  2.3.  This  prob- 
rule.  In  Sec- 
to  Chirps(  Tweety),  we  should 
it  is  associated  with  a  less 
this 
this 
requiring  any  special  rules),  at  least  when  the  reference  classes  form 

to  capture 
the  random-worlds  method  also  captures 

this  issue  with  his  strength 

result  shows, 

[0.7,0.8] 

interval 

though 

even 

Theorem 

5.23. 

Suppose  KB  has  the  form 

x  (a,  36,  Ill 
i=l 

I @ii(X)lIx  5r,  pi,  A@I CC) A KB’, 

and,  for  all  i,  KB  b  ‘vx(I/J~(x)  +  ccl,+l(x)) 
no  symbol  appearing 
[ Cy.i, fl,j] 
for  some  j, 
Then,  if  it  exists, 

in  q(x) 
is  the  tightest 

appears 

/2 ~(ll+l(x)ll, 

~1  0).  Assume  also  that 
that, 
in  KB’  or  in  any  @i(c).  Further  suppose 
interval.  That  is, for  all  i  #  j,  ai  <  ‘Yj  <  ei  <  p;. 

Pr,((p(c) 

/ KB)  E  IQi,Pjl. 

Proof.  See  the  Appendix. 

0 

to  cases  where 

rule  also  applies 

”  Kyburg’s 
the  reference  classes  do  not  form  a  chain.  The  random-worlds 
if  we  know  that  only  20%  of  Republicans 
method  disagrees  with  the  strength 
banker,  Kyburg’s 
are  pacifists, 
strength 
is  0.2.  On  the  other  hand,  the 
random-worlds  method  would  view  this  as  two  pieces  of  evidence  counting  against  Morgan  being  a  pacifist; 
it  can  be  shown 

are  pacifists, 
that  our  degree  of  belief  that  Morgan 

that  this  would  result  in  a  degree  of  belief  less  than  0.2. 

rule  in  these  cases.  For  example, 

that  only  20%  of  bankers 

rule  would  conclude 

and  that  Morgan 

is  a  Republican 

is  a  pacifist 

I?  Bacchus  et  al.  /Artificial 

Intelligence 

87  (1996)  75-143 

115 

Example  5.24.  The  example  described 
following  knowledge  base  KBchirps: 

in  Section  2.3  is  essentially 

captured  by  the 

0.7  51  [[Chirps(x) 

1 Bird(x)I(,  52  0.8  A 

0  53  (IChirps 

I Magpie(x) 

Ilx  54  0.99  A 

Vx  (Mugpie(  x)  +  Bird(x)  )  A 

Magpie(  Tweety) . 

It  follows 

from  Theorem  5.23  that  Pr, 

(Chirps(  Tweety)  ( KBchirps) E  [0.7,0.8], 

I8 

Next,  we  consider 

a  different  way  in  which  competing 

reference  classes  can  arise: 

when  one  reference  class  is  too  specific,  and  the  other  too  general. 

Example  5.25.  We  illustrate 
[ 251.  Consider  KBmgpie: 

the  problem  with  a  example  based  on  one  of  Goodwin’s 

(IChirps 

) Bird(x)lj,  Ml  0.9  A 

IIChirps( x)  1 Magpie(x)  A  Mody(  X)  IIx  ~2  0.2  A 

Vx  (Mugpie(  x)  +  Bird(x)  )  A 

Magpie(  Tweety)  . 

reference  class  in  these  theories.  Using  such  approaches, 

argues 

theories  would 

is  not  known 

the  information 

Reference-class 
since  Tweety 
legitimate 
would  be  0.9.  Goodwin 
ignore 
base 
are  generally  moody.  But 
that  magpies  generally 
assumption.  The  random-worlds 
of  belief 
that  Tweety 
than  0.9. 

the  question 

leaves 

typically 
to  be  moody, 

ignore 

the  information 

about  moody  magpies: 

the  class  of  moody  magpies 

is  not  even  a 
the  degree  of  belief 
should  we 

that  this  is  not  completely 

reasonable-why 

about  moody  magpies?  Tweety  could  be  moody 

In  fact, 

open). 
ignoring 
are  not  moody. 

the  second  statistic 

(the  knowledge 
it  is  consistent  with  KBmgpie  that  magpies 
to  assuming 
in  effect  amounts 
It  seems  hard  to  see  why  this  is  a  reasonable 
and  the  degree 
is  less 

to  have  a  value  which 

intuition, 

approach  supports  Goodwin’s 

flies,  given  KBnmg+  can  be  shown 

This  example 

illustrates  a  general  phenomenon: 

if  we  do  not  have  a  “most  appropriate” 
reference  class  ( in  the  sense  of  Theorem  5.6),  then  random  worlds  combines 
information 
from  more  specific  classes  as  well  as  from  more  general  classes.  Hence,  as  we  mentioned 
reasoning: 
in  the  previous 
inheritance 
and  ignore  subclasses. 
pure 
inheritance 
We  agree  with  Goodwin 
reasoning 
unintuitive 

random  worlds  does  not  always  validate 
look 

especially  when  we  are  dealing  with  quantitative 

to  superclasses 
inheritance 

that  this  property  of  pure 

can  lead 
information. 

reasoning  would  always 

conclusions, 

section, 

to 

‘s Strictly  speaking,  a  direct  application  of  Theorem  5.23  would  require 
-tIIMagpie(r)llx 
this  actually  follows  by  default.  Hence,  by  Proposition  5.2, we  can  consider  this  to  be  in  KB,hi~ps. 

contains 
%i  0).  But  the  maximum-entropy  techniques  of  Section  6  can  be  used  to  show  that 

that  KB,hirps 

116 

F: Bacchus 

et  ul.  /Artificial 

Intelligence 

87 

(1996) 

75-143 

of  the  behavior  of  random  worlds 

The  third  and  most  important 

type  of  conflict 
in  Section  2.3,  this  case  is  likely 

references 

the  complete 

characterization 

to  be  complex, 

the  following 
classes  are  essentially 

classes.  As  we  argued 
While 
would  appear 
competing 
here  by  assuming 
member: 
theorem 
reference  classes  1+5 and  @’ is  small  relative 
II@(x)  A V@‘(x) 1 V+(X) II,,  M  0  and  [1$(x)  A $‘(x) 
omit  the  details  of  this  extension  here. 

the 
to  the  case  where  we  simply  assume 

the  overlap  between 
c  addressed 

that 
individual 

to  come  up  often 

is  when  we  have  incomparable 

theorem 
disjoint.  We  capture  “essentially 

reference 
in  practice. 
in  such  cases 
illustrates  what  happens  when  the 
disjoint” 
these  classes  consists  of  precisely  one 
the  following 
competing 
that  is,  where 
1 $‘(x)~~.,  z  0.  For  simplicity,  we 

the  overlap  between 

to  the  sizes  of  the  two  classes; 

that 

in  our  query.  We  can  generalize 

It  turns  out 

for  combining 

the  issue  of  combining 

random  worlds  provides 

,  and  assume  we  have  competing 

of  the  property  P  in  the  different  competing 

an 
evidence:  Dempster’s 

this  assumption, 
technique 
rule  addresses 

independent 
rule  of  com- 
independent  pieces 
reference  classes 
for  this  query.  In  this  case,  the  different  pieces  of  evidence  are 
reference  classes.  More 
IIA M  a;  as  giving 
the  dif- 
these 

that,  under 
derivation  of  a  well-known 
[ 681.  Dempster’s 
bination 
of  evidence.  Consider  a  query  P(c) 
that  are  all  appropriate 
the  proportions 
precisely, 
if  &i(c)  holds,  we  can  view  the  fact  that  I/ P(x) 
evidence  of  weight  cy; in  favor  of  P(c).  The  fact  that  the  intersection 
ferent  classes 
pieces  of  evidence; 
Under 
of  evidence 
Dempster’s 

is  “small”  means 
thus, 
this  interpretation,  Dempster’s 

to  obtain  an  appropriate  degree  of  belief 
rule  is  6  :  [0,  I]”  + 

the  different  pieces 
in  P(c).  The  function  used  in 

that  almost  disjoint  samples  were  used  to  compute 

rule  tells  us  how  to  combine 

[ 0,  1 1, defined  as  follows: 

to  view  them  as  being 

it  is  perhaps 

independent. 

reasonable 

between 

) #i(x) 

As  the  following 
Since  6  is  undefined 
that  this  is  not  the  case  in  the  theorem. 

theorem  shows, 

if  some  LYE are  equal 

this  is  also  the  answer  obtained  by  random  worlds. 
to  I  while  others  are  equal  to  0,  we  assume 

Theorem  5.26.  Let  P  be  a  unaty  predicate,  and  consider  a  knowledge  base  KB  of  the 
following 

form:  I9 

,,I 

!,I 

/j 

(l/P(X) 

I +il(X)llx  ";%A$m))A 

A 

3!X($i(X) 

A+,j(x))* 

;=I 

!,j=l,i#; 

where  either  ai  <  1 for  all  i  =  1,. 
P  nor  c  appear  anywhere 

in  the  formulas 

fli (x)  , then 

, rn,  or  ai  >  0 for  all  i =  1,. 

. , m.  Then,  if  neither 

Pr,(P(c) 

(KB)  =S(a~,...,a,~,). 

Proof.  See  the  Appendix. 

0 

I9 Here, Ll!x stands for  “there  exists  a  unique  x  such  that 

.” 

E  Bacchus  et  al. /Artificial  Intelligence  87  (1996)  75-143 

117 

We  illustrate 
information-the 
of  belief 
both  a  Quaker  and  a  Republican, 
of  pacifists  within  both  classes.  This 
reference  classes 

to  the  assertion 

“Nixon 

this  theorem  on  what  is,  perhaps, 

Nixon  Diamond  [ 661.  Suppose  we  are  interested 

the  most  famous  example  of  conflicting 
in  assigning  a  degree 
is 

is  a  pacifist”.  Assume 
and  we  have  statistical 

that  Nixon 
for  the  proportion 
information 
is  an  example  where  we  have  two  incomparable 

that  we  know 

for  the  same  query.  More  formally,  assume 

that  KBN~~,, is 

IIPacr$st(x) 

1 Quaker(x)  [IX ~1  cy A 

IIPuci@t(x)  1 Repubhzn(x) 

III 32  p  A 

Quuker(  Nixon)  A Republicun(  Nixon)  A 

3!x  (Quaker(x)  A Republicun(  x)  ) , 

for  (0. 
that  our  information 

1 KBN~**)  always  exists  and  its  value  is  equal  to  olB+(l~~)C,_p). 

(Y and  p  for  the  two  reference  classes. 

on  the  values 

/3  =  0.5,  so  that  the  information 

and  that  (D is  Pucifst(Nixon).  2o The  degree  of  belief  Pr,(p 
values,  depending 
(0,  l},  then  Pr,(q 
for  example, 
I  KBN~~,,) =  a:  the  data  for  Quakers 
(9 
Pr, 
If  the  evidence  given  by  the  two  reference  classes 
(q  ( KBN~“,,)  E  [a,  /3] :  some  intermediate 
Pr, 
the  two  reference 
belief 
of  belief  would  be  around  0.94.  This  has  a  reasonable 
independent 
get  even  more  support 

classes  provide  evidence 
(Y and 

than  both 

is  greater 

for  Republicans 
is  used  to  determine 
is  conflicting-a 
is  chosen. 

value 

if  (Y =  p  =  0.8,  then 
explanation: 
bodies  of  evidence,  both  supporting  q,  when  we  combine 

/?.  For  example, 

in  the  same  direction, 

then 

I KBN~~,,) takes  different 
If  {cY,~}  # 
If, 
is  neutral,  we  get  that 
the  degree  of  belief. 
>  0.5  >  P-then 

If,  on  the  other  hand, 
the  degree  of 
the  degree 
if  we  have 
two 
them  we  should 

are 

typically 

pacifists”. 

Now,  assume 

that  “Quakers 

for  Republicans 

this  corresponds 

In  our  framework, 

value)  dominates.  But  what  happens 
for  the  two  reference 
does  not  exist.  This 

from  Theorem  5.26  that  Pr,( 

know 
assigning  LY =  1.  If  our  information 
then  it  follows 
(i.e.,  an  “extreme” 
conflicting 
defaults 
limiting 
probability 
depends  on  the  way  in  which 
that  the  “almost  all”  in  the  statistical 
to  “all” 
limit 
the  default.  Thus,  in  this  case,  the  first  conjunct 
than  the  second  conjunct.  Symmetrically, 
hand, 

is  1. We  can  view  the  magnitude 

classes. 7  It  turns  out  that,  in  this  case, 
is  because 

is  not  entirely  quantitative.  For  example,  we  may 
to 
is  not  a  default,  so  that  p  >  0, 
cp I  KBN~~,,) =  1.  As  expected,  a  default 
in  the  case  where  we  have 
the 
is  nonrobust:  its  value 
if  71  <  72,  so 
is  much  closer 
the 
then 
the  strength  of 
represents  a  default  with  higher  priority 
is  0.  On  the  other 

to  0.  More  precisely, 
of  the  first  conjunct 
is  closer 

of  the  tolerance  as  representing 

the  tolerances  7’tend 
interpretation 
in  the  second  conjunct 

if  ri  >>  72,  then  the  limit 

if  ri  =  72,  then  the  limit 

the  “almost  none” 

to  “none”, 

the  limit 

is  l/2. 

than 

of  this  limit 

The  nonexistence 

fact  that  we  obtain  different 
closely 
reasoning 

is  not  simply  a  technical  artifact  of  our  approach.  The 
limiting  degrees  of  belief  depending  on  how  7’ goes  to  0  is 
to  the  existence  of  multiple  extensions  in  many  other  theories  of  default 
and  the  existence 
instance, 

[65]  ).  Both  nonrobustness 

related 
(for 

in  default 

logic 

X’ As  pointed  out  above,  Theorem  5.26  can  be  generalized 
Quaker  Republican, 

it  is  sufficient 

to  assert  that  there  are  very  few  Quaker  Republicans. 

so  that  instead  of  asserting 

that  Nixon 

is  the  only 

I18 

E  Bocchus  el  al./Arrijiciul 

intelligence  87  (1996)  75-143 

It  is 

incompleteness 

suggest  a  certain 

of  our  knowledge. 

in  order  to  resolve 

than  one  extension 

the  type  of  information 

that,  in  the  presence  of  conflicting  defaults,  we  often  need  more  information 
the  conflict.  Our  approach 
that  would  suffice  to  reach  a 

of  more 
well  known 
about  the  strength  of  the  different  defaults 
has  the  advantage  of  pinpointing 
decision.  Note  that  our  formalism  does  give  us  an  explicit  way  to  state  in  this  example 
that  the  two  extensions 
that  generate 
them  have  equal  strength;  namely,  we  can  use  ~1  to  capture  both  default  statements, 
I /2,  as  expected.  However, 
rather  than  using  pi  and  ~2.  In  this  case,  we  get  the  answer 
It  is  not 
it  is  not  always  appropriate 
difficult 
the 
to  extend  our  language 
relative  sizes  of  the  components 

to  allow  the  user  to  prioritize  defaults,  by  defining 
7,  of  the  tolerance  vector. 

that  defaults  have  equal  strength. 

likely,  by  asserting 

that  the  defaults 

to  conclude 

are  equally 

As  we  mentioned,  Theorem  5.26  tells  us  only  how  to  combine 

is  small.  Shastri 

statistics 

from  com- 
in  the  very  special  case  where  the  intersection  of  the  different 
in  the  same 
for 
are  also 
is  in  fact  a 

special  case:  he  assumes 
the  statistics 
class, 

]  describes 
that,  in  addition 

for  P  in  the  general  population 

entropy.  Maximum 

[ 71,  pp.  331-332 

is  based  on  maximum 

to  the  statistics 

a  result 

entropy 

tool  for  computing 

degrees  of  belief,  provided  we  restrict 

that  involve  only  unary  predicates  and  are  well  behaved 

to  knowledge 
in  a  sense  made  precise 

classes 

reference  classes 

peting 
reference 
spirit,  but  for  a  different 
P  within  each  reference 
known.  Shastri’s 
very  general 
bases 
in  [ 281.  (See  the  discussion 

result 

in  Section  6.) 

5.4.  Independence 

As  we  have  seen  so  far,  random  worlds  captures  many  of  the  natural  reasoning  heuris- 
is  a  default  assumption 
unless  we  know  otherwise.  Random 
this  principle  as  well,  in  many  cases,  It  is,  in  general,  very  hard  to  give 
tests  for  when  a  knowledge  base  forces  two  properties 

in  the  literature.  Another  heuristic 

to  be  dependent. 

independent 

tics  that  have  been  proposed 
that  all  properties  are  probabilistically 
worlds  captures 
simple  syntactic 
The  following 
relationship 
Consider 

theorem  concerns  one  very  simple  scenario  where  we  can  be  sure  that  no 

is  forced. 
two  disjoint  vocabularies  @ and  @‘, and  two  respective  knowledge  base  and 

query  pairs:  KB,  q  E  L(  @),  and  KB’,  4p’ E  C( @‘) . We  can  prove  that 

Pr,(qA& 

KBAKB’) 

=Pr,(plKB) 

xP~,((D’(KB’). 

the  two 
That  is,  if  there  is  no  connection  between 
queries  will  be  independent: 
is  the  product  of  their 
probabilities.  We  now  prove  a  slightly  more  general  case,  where  the  two  queries  are 
both  allowed 

to  refer  to  some  constant  c. 

in  the  two  vocabularies, 

of  their  conjunction 

the  probability 

the  symbols 

Theorem  5.27.  Let  @I  and  @2  be  two  subvocabularies 
for  the  constant  c.  Consider  KB,,  L,OI E L(@l )  and  KBz,  qp2 E  L(&), 

of  cf  that  are  disjoint  except 
Then 

Pr,(w  A  ‘~2  I KBI  AK&)  =Pr,(m 

I KBI)  x Pr,(n 

I KB2). 

Proof.  See  the  Appendix. 

0 

E  Bacchus  et  al.  /Art@cial 

Intelligence 

87  (1996)  75-143 

119 

Although 

very  simple, 

this 

theorem  allows  us  to  deal  with  such  examples 

as  the 

following: 

Example  5.28.  Consider 
40%  of  hospital  patients  are  over  60  years  old  and  that  Eric  is  a  patient. 

the  knowledge  base  K&,,, 

and  a  knowledge  base  stating 

that 

KB ,60  dLf ~)Over60( x)  1 Patient(x)  III M5 0.4  A Putient(Eric) 

. 

Then 

Pr, 

( Hep ( Eric)  A Over60 ( Eric)  1 KBl,,  A KB>60) 

= Pr,(Hep(Eric) 

I KBh,)  x  Pr,(  Over60(Eric) 

I K&,jo)  = 0.8  x  0.4  = 0.32. 

In  the  case  of  a  unary  vocabulary 

(i.e.,  one  containing 

Theorem  5.27  can  be  proved  using 

the  connection 

constants), 
worlds  method  and  maximum 
fact  that  using  maximum 
above  proves 
as  well. 

entropy,  which  we  discuss 
entropy  often  leads  to  probabilistic 

that,  with  random  worlds, 

this  phenomenon 

between 

only  unary  predicates 

and 
the  random- 
in  Section  6.  It  is  a  well-known 
The  result 
case 

in  the  nonunary 

independence. 

appears 

We  remark 

that  the  connection  between  maximum  entropy  and  independence 

times  overstated.  For  example,  neither  maximum 
probabilistic 

independence 

in  examples 

like  the  following: 

entropy  nor  random  worlds 

is  some- 
lead  to 

Example  5.29.  Consider 

the  knowledge  base  KB,  describing 

a  domain  of  animals: 

IIBlack(x)  1 Bird(x)II,  ~1  0.2~ 

((Bird(x)(l,  ~2  0.1. 

the  proportion  of  black  animals 

In  this  case,  our  degree  of  belief 

for  Bird  and  Black  to  be  probabilistically 

If  this 
It  is  perfectly  consistent 
to  be  the  same  as  that 
were  the  case,  we  would  expect 
in  BZack( Clyde),  for  some  arbitrary 
of  black  birds. 
this  is  not  the  case.  Since  all  the  predicates 
animal  Clyde,  would  also  be  0.2.  However, 
in  Section  6  to  show 
here  are  unary  we  can  use  maximum-entropy 
about  Clyde 
that  Pr, 
being  black,  except  for  a  slight  bias  due  to  the  fact  that  he  might  be  a  bird  and  in  that 
case  is  unlikely 

techniques  discussed 
is,  we  are  almost 

I  KB)  =  0.47.  That 

(Black(  Clyde) 

to  be  black. 

independent. 

indifferent 

5.5.  The  lottery paradox  and  unique  names 

In  Section  3.5  we  discussed 

the  lottery paradox  and  the  challenge 

it  poses  to  theories 

of  default 

reasoning.  How  does  random  worlds  perform? 

To  describe 

the  original  problem 

in  our  framework, 

let  Ticket(x)  hold  if  x  purchased 

a  lottery 

ticket.  Consider 

the  knowledge  base  consisting  of 

KB  =  3!x  Winner(x)  A Vx  (Winner(x)  +  Ticket(x)  ) . 

That 
ticket. 

is,  there 

is  a  unique  winner, 

a  lottery 
the  size  of  the  lottery,  say  N,  we  can  add  to  our  knowledge 

to  win  one  must  purchase 

in  order 

and 

If  we  also  know 

120 

E  Brrcchus  et  ul. /Art&G1 

Intelligence  87  (1996)  75-143 

base  the  assertion  EINx Ticket(x) 
assertion  can  easily  be  expressed 
for  simplicity 
belief 

that  each  individual  buys  at  most  one  lottery 

stating 

in  first-order 

that  there  are  precisely  N  ticket  holders. 

(This 
logic  using  equality.)  We  also  assume 
ticket.  Then  our  degree  of 

that  the  individual  denoted  by  a  particular  constant  c  wins  the  lottery 

is 

Pr, 

( Winner(c) 

( KB  A  ?X  Ticket(x)  A  Ticket(c) 

)  =  $. 

answers. 

It  is  true  that  we  do  not  get  the  default  conclusion 

that  .so/~eorlr  wins  will  obviously  be  I.  We  would  argue  that  these 
that  c  does 
framework  can  and  does 
the  conclusion 
this  is  not  a  serious  problem 
in  systems  which  either  reach  a  default  conclusion  or  not,  with  no  possibilities 

Our  degree  of  belief 
are  reasonable 
not  win  (i.e.,  degree  of  belief  0).  But  since  our  probabilistic 
express 
(unlike 
in  between). 

that  c  is  very  unlikely 

to  win, 

If  we  do  not  know 

the  exact  number  of  ticket  holders,  but  have  only 

the  qualitative 

then  our  degree  of  belief  that  c  wins  the  lottery  is 
1 

information 
that  this  number 
is  “large”, 
( Winner(c)  1 KB  A Ticket(c) 
(3x  Winner(x) 
simply  Pr, 
KB)  =  1.  In  this  case  we  do  conclude  by  default 
individual  will  not 
win,  although  we  still  have  degree  of  belief  1 that  someone  does  win.  This  shows  that 
and 
the  tension  Lifschitz 
yet  not  concluding 
setting 
such  as  ours. 

the  universal  does  in  fact  have  a  solution 

)  =  0,  although,  as  before,  Pr, 

sees  between  concluding 

a  fact  for  any  particular 

that  any  particular 

in  a  probabilistic 

individual 

of  defaults), 

“exceptional” 

interpretation, 

is  exceptional 

just  as  would 

the  set  of  birds 

fly”  with  “Birds 

implies  “makes  up  a  negligible 

in  which  a  class  (such  as  Bird(x) 

of  defaults, 
So  under  our 

to  be  equal  to  the  union  of  a  number  of  subclasses 

fraction  of  the  whole  set.  We  view  the  inconsistency 

in  at  least  one  respect.  However,  using  our  statistical 

the  user  that  this  collection  of  defaults  cannot  all  be  true  of  the  world 

Poole’s  example 
into  a  finite  number  of  subclasses, 

Finally,  we  consider  where  random  worlds  fils  into  Poole’s  analysis  of  the  lottery  para- 
) 
(Penguin(x),  Errzu( x)  , . .  ), 
in- 
fraction  of  the 
is  inconsistent:  we  cannot 
each  of  which  makes  up  a 
in  this  case  as  a  feature: 
(given 
of  the  default  “Birds 

dox.  Recall,  his  argument  concentrated  on  examples 
is  known 
each  of  which 
terpretation 
population”. 
partition 
negligible 
it  alerts 
our  interpretation 
typically 

typically  do  not  fly”  or  “No  bird  flies”. 
Our  treatment  of  Poole’s  example  clearly  depends  on  our  interpretation  of  defaults.  For 
IIx k  a 
sub- 
fraction 
of  default  not  based  on  “almost  all” 
it  entails  giving  up  many  of  the  attractive  properties  of 
assigned  a  degree  of  belief 
solution  would  be  to 
interprets  “almost 
thus 

instance,  we  could  interpret 
for  some  appropriately 
classes  (such  as  penguins  which  are  nonflying  birds)  can  include  a  nonvanishing 
of  the  domain.  While  allowing 
does  make  Poole’s  KB  consistent, 
the  =  1  representation 
1, and  the  properties 
use  the  approach  presented 
all”  as  “arbitrarily 
allows  us  to  get  the  benefits  associated  with  this  interpretation). 
is  inconsistent, 
maintain  consistency. 

it  takes  “almost  all”  to  mean  “within  T  of  I”,  for  7  large  enough 

is  less  than  1.  In  this  case,  “exceptional” 

to  I”  whenever  such  an  interpretation 

(such  as  having  default  conclusions 

in  Theorem  5.3).  An  alternative 

in  [ 381.  Roughly  speaking, 

the  default  “Birds  typically 

If  this  interpretation 

chosen  cy which 

the  inconsistency 

an  interpretation 

fly”  as  l/Fly(x) 

this  approach 

is  consistent 

summarized 

/ Bird(x) 

close 

(and 

to 

F:  Bacchus  et  aL/Artijiciul 

Intelligence 

87  (1996)  75-143 

121 

that  any 

We  conclude 

two  constants 

denote  different  objects. 

this  section  by  remarking 
reasoning 

on  another  property  of  the  random-worlds 
are  often  simplified 
should 

In  random  worlds, 
If  cl  and  c:!  are  not  mentioned 

by  using 
the  unique 
(but  perhaps  only  by 
is  a  strong  automatic  bias 
= 
in  KB,  then  Pr,(ct 
[ 28,  Lemma  D.1  ]  for  a  formal  proof  of  this  fact).  Of  course,  when 
for  which  this  result 
It  is  hard  to 

=  c2  1 (cl  =  ~3.) V  (c:!  =  cg) V  (cl  =  cg))  =  l/3. 
theorem  saying  precisely  when  the  bias  towards  unique  names  overrides 
that 
[48]  are  correctly  handled  by  random 

method.  Applications 
of  default 
names  assumption,  which  says 
default) 
towards  unique  names. 
c2  1 KB)  =  0  (see 
we  know  something 
fails;  for  instance  Pr,(ct 
give  a  general 
other  considerations. 
Lifschitz  has  given  concerning 
worlds.  For  instance,  Lifschitz’s  problem  Cl 

that  both  of  the  “benchmark” 

about  ct  and  c2  it  is  possible 

there 
anywhere 

However,  we  note 

to  find  examples 

unique  names 

examples 

is: 

( 1)  Different  names  normally  denote  different  people. 
(2)  The  names  “Ray”  and  “Reiter”  denote 
(3)  The  names  “Drew”  and  “McDermott” 

the  same  person. 
denote 

the  same  person. 

The  desired  conclusion 

here  is: 

a  The  names  “Ray”  and  “Drew”  denote  different  people. 

Random  worlds  gives  us  this  conclusion.  That  is, 

Pr, 

(Ray  #  Drew  ( Ray = Reiter  A Drew  =  McDermott)  =  1. 

Furthermore,  we  do  not  have  to  explicitly 

state  a  unique  names  default. 

6.  Random  worlds  and  maximum 

entropy 

The  principle  of  maximum  entropy  is  a  well-known 

idea,  useful 

for  certain 

types 
p  over  a 
[70] 
in 

distribution 

reasoning.  Briefly, 

the  entropy  of  a  probability 

the  amount  of  “information” 
theory;  note 

of  probabilistic 
finite  space  0  is  defined  as  H(p)  =  -  CUE0  p (CO) ln(k(o)). 
distribution, 
that  the  entropy  measures 
has  the  maximum 
the  sense  of  information 
possible  entropy.  The  principle  of  maximum 
in  which 
we  have  some  constraints  on  a  probability  distribution,  which  may  have  many  solutions, 
but  where  we  must  decide  on  one  particular  consistent  distribution.  The  principle  asserts 
the  one  that  should  be  adopted 
that  among 
is  the  (hopefully 
it  incorporates 
the  least  additional 

having  maximum  entropy,  because 

that  the  uniform  distribution 

[ 341  addresses  situations 

unique)  distribution 

It  has  been  argued 

those  distributions 

in  a  probability 

the  constraints, 

themselves. 

satisfying 

entropy 

beyond 
entropy 

information 
No  explicit  use  of  maximum 
they  are  both  tools  for  reasoning 
by  the  two  methods  are  seemingly 
surprising  and  very  close  connection  between 
entropy  provided 

the  constraints 
is  made  by  random  worlds.21 

Indeed,  although 
the  classes  of  problems  considered 

about  probabilities, 

disjoint.  Nevertheless, 

it  turns  out  that  there 

is  a 

the  random-worlds 

approach  and  maximum 

that  the  language  consists  only  of  unary  predicates  and  constants. 

In 

21 In  fact,  the  postulate  of  uniform  probability 
application 
unrelated 

to  the  connection  we  discuss 

entropy.  However, 

of  maximum 

in  the  rest  of  this  section. 

over  worlds 

(i.e., 

indifference) 

can  be  seen  as  a  degenerate 

in  the  context  of  this  paper, 

this  is  rather  uninteresting 

and  is 

this  connection.  This  result 

it  hints  at  effective 

this  section  we  briefly  describe 
simply  because 
the  unary  case.  However,  as  we  discuss  below, 
interesting 
that  the  maximum-entropy 
embedded 

reasons  as  well.  For 
approach 

in  our  framework. 

to  default 

for  other 

computational 

techniques 
the  connection 

instance,  we  use 
reasoning, 

is  of  considerable 

for  random  worlds 
to  random  worlds 

interest 
in 
is 
to  show 
in  [23],  can  be 

the  connection 

considered 

To  understand 

the  connection 

to  maximum  entropy,  suppose 

, Pk  together  with  some  constant  symbols. 
symbols  or  higher  arity  predicates.)  We  can  consider 

the  language  consists  of 
the  unary  predicate  symbols  PI . . 
(Thus,  we 
the 
do  not  allow  either  function 
2”  CUO~ZS that  can  be  formed  from  these  predicate  symbols,  namely, 
the  formulas  of  the 
form  Q)  A.  A Qk,  where  each  Q;  is  either  P,  or  IPi.  Then  the  knowledge  base  KB  can 
be  viewed  as  simply  placing  constraints  on  the  proportion  of  domain  elements  satisfying 
/ P?(x)  IIx =  l/2  says  that  the  proportion 
each  atom.  For  example, 
is  twice  the  proportion 
of  the  domain  satisfying 
languages 
satisfying 
(only) 
it  can  be  shown 
form  from  which 
on  the  possible  proportions  of  atoms  can  be  simply  derived.  Details  of  this 
constraints 
and  all  other  specific 
we  are  about 
(e.g.. 
physics 

the  general  phenomenon 
in  many  places,  such  as  [ 57,7  I ]  and  in  statistical 

both  PI  and  Pz  as  conjuncts.  For  unary 
in  a  canonical 

the  formula 
some  atom  containing  PI  as  a  conjunct 

that  every  formula  can  he  rewritten 

to  discuss 
[ 441  ). 

results  can  be  found 

in  [ 281,  although 

atoms  containing 

is  addressed 

l/P) (x) 

The  set  of  constraints 

is.  each  vector 

S(KB).  That 
constraints  defined  by  KB  (where 
our  language 

contains  only 

in  S(KB). 

generated  by  KB  defines  a  subset  of  [O,  I]“,  which  we  call 
,pz~),  is  a  solution  of  the 
/jr  is  the  proportion  of  atom  i).  For  example,  suppose 
{PI,  Pl},  so  that  there  are  four 

the  two  predicate 

say  I?’ =  (p).  . 

symbols 

atoms 

A,  =  P,  A  P2. 

A2  =  P,  i:  -P2. 

A3  =  -P,  A  Pz. 

A4  = TP,  A  -7P> 

Let  KB  =  YxP,(x) 
constrains  both  ye  and  p4  (the  proportion  of  domain  elements 
A4)  to  be  0.  The  second  conjunct 

/\Pz)~(.Y);~~  :>I  0.3.  The 

forces  p)  to  he  (approximately) 

A  //PI(X) 

first  conjunct 

of  KB  clearly 
satisfying  atoms  A3  and 

at  most 

l/3.  Thus, 

S(KB)  = {(p,,. 

.,p4)  t  IO.  I I’:  PI  s  0.3.p~  =/A$  =o,p,  +p2  =  I}. 

the  vector 

The  connection 

between  maximum 

entropy  and  the  random-worlds  method 

is  based 
ir”. 
in  W.  Each  vector  d  can  be  viewed  as  a  probability  distribution 

on  the  following  observations.  With  every  world  W.  we  can  associate 
where  p,‘”  is  IIAi(x)jj, 
over  the  space  of  atoms  A 1. 
such  vector.  We  define 
point  p’ E  S( KB).  What 
for  those  I?’ where  some  p,  is  not  an  integer  multiple  of  l/N, 
for  those  p’ which  are  “possible”. 
there  are  vastly  more  worlds  W  for  which  /7”” IS “near” 
S(KB) 
for  all  sufficiently 
point(s) 

. ADA ;  WC can  therefore  associate  an  entropy  with  each 
the  errtr’ol>y of  W  to  be  the  entropy  of  p”.  Now,  consider  some 
is  the  number  of  worlds  W  E  WN  such  that  p””  =  I??  Clearly, 
is  0.  However, 
as  eNH(ii?.  Hence, 
point  of 
if, 
result: 
the  maximum-entropy 

the  maximum-entropy 
than  there  are  worlds  elsewhere.  This  allows  us  to  prove  the  following 
small  7’, a  formula  0  is  true  in  all  worlds  around 

this  number  grows  asymptotically 

(0  ! KB)  =  I. 

the  answer 

of  S(KB), 

then  Pr, 

E  Bacchus  et al. /Art$cial 

Intelligence  87  (1996)  75-143 

123 

In  the  above  example, 

the  maximum-entropy 

point 

in  S(KB) 

is  p”  =  (0.3,0.7,0,0). 

5.2  to  conclude 

tells  us  that  the  size  of  atom  A1  is  (approximately) 

to  p”,  we  conclude 

that,  for  any  formula  40, Pr,( 

this  formula  certainly 
that  Pr, 

less 
some  small  fixed  E  and  the  formula  0[  E]  = 
holds  at  all  worlds  W 
(01 E]  )  KB)  =  1.  This 
cp 1 KB)  = 
this  holds  for  4p =  P2( c) .  But  now,  we  can  use 
(P?(c)  1 KE)  E  [0.3  -  E, 0.3  +E] 
. Since  this  holds 
In 
it  to  more  complex  examples.  These 
as  a  basis  for  computing 

j  KB)  =  0.3,  as  desired. 

that  Pr,(  P2( c) 

computation 

and  generalize 

close 

base  only 

to  l/3.  But  now,  consider 
E  [0.3  -  ~,0.3  +  E].  Since 
is  sufficiently 

Our  knowledge 
than  or  equal 
II&(x)\\, 
where  p” 
allows  us  to  use  Proposition 
Pr,  (4p 1 KB  A  0 [E]  ) .  In  particular, 
direct  inference 
that  Pr, 
for  all  sufficiently 
[28]  we  formalize 
techniques 
degrees  of  belief.  The  resulting  procedure 
in  Section  5.  Furthermore, 
results 
for  computing  maximum 
entropy 
technique  of  potential  practical  significance. 

small  E,  we  conclude 
this  argument 

to  conclude 

allow  us  to  use  a  maximum-entropy 

applies 

to  many  cases  not  covered  by  our 
since  we  can  take  advantage  of  existing  algorithms 
(see 

[ 221  and  the  references 

therein),  we  obtain  a 

entropy 

for  many 

is  important 

to  maximum 

implications.  Maximum 

in  AI  and  elsewhere.  Two  highly  relevant  works  are  the  application 

reasons,  aside  from  its 
for  proba- 
to 
[ 711  and  to  default  reasoning  by  Goldszmidt,  Morris, 
hierarchies  by  Shastri 
[23]. 
and 
rather  than  be  seen  as  an  ad  hoc  heuristic.  Random  worlds,  resting  on  the  ba- 

technique  be  well  understood 

entropy  has  been  a  popular 

that  such  a  popular 

It  is  desirable 

technique 

The  connection 

reasoning 

computational 
bilistic 
inheritance 
and  Pearl 
motivated, 
sic  principle  of  indifference, 
than  the  usual 

provides  motivation  which  some  may  find  more  convincing 

information-theoretic 

justifications. 

Not  only  does  the  random-worlds  method  provide  motivation 

for  maximum  entropy, 

it 

of  it.  As  discussed  above,  there  is  a  strong  connection 

in  the  unary  case  (see  also 
In  fact,  restricted  versions  of  some  of  our  results  from  Section  5  can  be  proved 
are  more 

(see  [ 7 1 ] ) . But  our  combinatorial 

approach  and  maximum 

proof  techniques 

entropy 

than  the  ones  based  directly  on  entropy.  The  limitations 
in  detail  in  [ 281) 
once  we 

inescapable,  because 

(as  we  discuss 

is  inherently 

inapplicable 

entropy 

that  maximum 

entropy 

the  random-worlds 

can  be  viewed  as  a  generalization 
between 
[28]). 
using  maximum 
general 
of  maximum 
it  is  reasonable 
move  beyond  unary  predicates. 
Finally,  our  results  connecting 

in  fact,  simpler) 
entropy  are  perhaps 
to  conjecture 

(and, 

use  to  help  clarify 
applying  probabilistic 
vious  work  has  been  the  formalism  of  c-semantics 
here. 

the  connection 
semantics 

between 

random  worlds  to  maximum  entropy  can  also  be  put  to 
to 
to  default  reasoning.  The  mainstay  of  most  of  this  pre- 
[ 201.  We  briefly  review  E-semantics 

random  worlds  and  previous  approaches 

consisting 

a  language 

of  propositional 

variables  PI,.  . 

, Pk)  and  default 

Consider 
propositional 
are  typically  C’s”),  where  B  and  C  are  propositional 
propositional  worlds,  corresponding 
to  the  possible 
p  on  R,  we  define  p(B) 
distribution 
Given  a  probability 
of  worlds  where  B  is  true.  We  say  that  a  distribution 
ifp(C)B)>l-E. 

formulas 

(over  some  finite  set  of 
rules  of  the  form  B  -+  C  (read  “B’s 
formulas.  Let  0  be  the  set  of  2k 

truth  assignments 

to  the  variables. 
to  be  the  probability  of  the  set 
f..~ c-satisjes  a  default  rule  B  --* C 

A  parameterized  probability  distribution  (PPD) 

is  a  collection  {,u~}~,o  of  probability 

over  0.  parameterized 

by  E.  A  PPD  {,LL~)~,o e-satisfies  a  set  R  of  default 
distributions 
rules  if  for  every  E,  ,uu, e-satisfies  every  rule  r  E  R.  A  set  R  of  default  rules  c-entails 
B  -  C  if  for  every  PPD  that  E-satisfies  R.  limc,~ 

,uu,(C  1 B)  =  I. 

As  shown 

in  [ 201,  e-entailment 
associated  with  default  reasoning, 
However,  e-entailment 
quence 
inference  given  in  Section  3.2.  Hence.  among  other  limitations, 
irrelevant 

typically 
information. 
[ I 1, the  conse- 
satisfies  only  the  five  basic  properties  of  default 
to  ignore 

possesses  a  number  of  reasonable  properties 
including 

is  very  weak.  In  particular,  as  shown  by  Adams 

so  it  cannot  perform  any  inheritance 

relation  defined  by  e-entailment 

for  more  specific 

it  has  no  ability 

a  preference 

information, 

reasoning. 

In  order 

to  restrict 

it  is  necessary 

{,u~,~}~,~~  (see 

to  obtain  additional  desirable  properties, 

of  admissible  PPD’s.  Goldszmidt,  Morris,  and  Pearl 
PPD:  the  maxinzum-entropy  PfD 
nical  details).  A  rule  B  +  C  is  defined 
lim,,o  ~z,~(  C  j B)  =  1. The  notion  of  ME-plausible 
in  [23],  where  it  is  shown 
ing  some  ability 
to  ignoring 
provided 
(see  also  [24]  ). 
Our  results 

the  class 
[ 231  focus  attention  on  a  single 
and  tech- 
if 
of  R 
in  detail 
to  inherit  all  the  nice  properties  of  c-entailment  while  hav- 
are 
irrelevant 
of  a  set  of  rules  in  certain  cases 

[ 231  for  precise  definitions 
consequeme 
is  analyzed 

to  be  an  ME-plausible 

information.  Equally 

the  ME-plausible 

for  computing 

consequences 

consequence 

importantly, 

algorithms 

relating 

random  worlds 
the  approach  of  [23]  can  be  embedded 
We  simply  convert  all  default 

to  maximum 

in  our  framework 

entropy  can  be  used  to  show  that 
in  a  straightforward  manner. 
rules  r  of  the  form  B  -+  C  into  formulas  of  the  form 
1,  where  @B is  the  formula  obtained  by  replacing  each 
that 
variable  p,  in  B  with  P,(X).  Note  that  the  formulas 
relation  XI,  since  the 
that  propositional 
assertions 
theorem 

equality 
rules.  Note  also 
rules  become 

statistical 
the  following 

this  translation,  we  obtain 

l/$k(x) 

1 h(X)//., 

this  conversion 

@. =def 
=I 
occurrence  of  the  propositional 
arise  under 
approach  of  [23]  uses 
the  same  E  for  all  default 
variables  become  unary  predicates.  Hence,  default 
about  classes  of  individuals.  Under 
(which 

is  proved,  and  discussed 

in  more  detail, 

all  use  the  same  approximate 

in  I28  J ). 

Theorem 
any  .set ‘R  of  defeasible 

6.1.  Let  c  be  u  comtant 

synbol.  U.&g 

rules,  B  4  C  is  an  ME-plausible 

the  tramlation  described  above,  for 
consequence  qf  R  iff 

Pr, 

f/+(c) 

/  /j 

rER 

Hence,  all  the  computational 

consequence 

techniques  and  results  described 

relation.  Examples  demonstrating 

special  case  of  our  approach.  Furthermore. 
over  to  the  ME-plausible 
given  in  [ 23 1, but  we  can  now  use  Theorem  5.16  to  provide  a  formal  characterization 
some  of  the  inheritance 
that  our  translation 
of  individuals 
individual 
intuitive 
Section  3.4). 

in  [23]  carry  over  to  this 
unary  versions  of  all  of  our  theorems  carry 
inheritance  were 
of 
It  should  also  be  noted 
about  classes 
about  a  particular 
to  be  c).  This  is  in  keeping  with  the 
(see 

of  rules  and  context  used  by  propositional 

(whose  name  we  have  arbitrarily  chosen 

properties  of  this  consequence 

default  systems 

and  it  converts 

into  statistical 

interpretations 

i.e..  B,  into 

the  context, 

the  default 

information 

assertions 

converts 

relation. 

rules 

E  Bacchus  et  al.  /Art$cial 

Intelligence 

87  (1996)  75-143 

125 

We  stress  that  the  assumption 
in  Theorem  6.1.  Geffner 

equality 
that  we  use  the  same  approximate 
[ 211  gives  an  example  of  an  anomalous 

relation 
conclusion 

is 

to  R, 

reasonable, 

is  more  specific 

in  the  system  of  [23].  Suppose 

the  rule  set  ‘R  consists  of  the  two  rules 

In  this  case,  the  rule  P  A  S  A  R  -+  Q  is  not  an  ME-plausible 
as  we  have  evidence 

for  Q  (P  A  S)  and 
than  the  other.  However, 
then  P  A  S  A  R  -+  Q  does  become  an  ME- 

crucial 
obtained 
P  A  S  -+  Q  and  R  +  -Q. 
of  R.  This  seems 
consequence 
against  Q  (R),  and  neither  piece  of  evidence 
if  we  add  the  new  rule  P  -+  -Q 
plausible  consequence 
of  R.  This  behavior  seems  counterintuitive, 
of  [231’s  use  of  the  same  E  for  all  of  the  rules.  Intuitively,  what  is  occurring  here  is 
that  prior  to  the  addition  of  the  rule  P  -+  -Q, 
the  sets  P(x)  A  S(x)  and  R(x)  were  of 
comparable 
, 
since  almost  all  P’s  are  -Q’s,  whereas  almost  all  P  A  S’s  are  Q’s.  The  size  of  the  set 
the  default  for  the  e-smaller  class  P  A S 
R(x),  on  the  other  hand, 
equality 
now  takes  precedence  over  the  class  R.  Once  we  allow  a  family  of  approximate 
connectives, 
to  a  different  E,  we  are  no  longer  forced  to  derive 
this  conclusion.  An  appropriate  choice  of  ri  can  make  the  default  I] 7Q<  x)  ( R(x)  IJx Mi  1 
the  number  of  Q’s  in  the 
so  strong 
subsetP(x)AS(x)AR(x),ismuchsma11erthanthesizeofthesetP(x)AS(x)AR(x). 
In  this  case, 
specific 
in  the  Nixon  Diamond.  That  is,  we  draw  no  conclusions 

takes  precedence  over  the  rule  P  A  S  -+  Q.  With  no 
as 

about  the  relative  strengths  of  the  defaults  we  get  nonrobustness, 

size.  The  new  rule  forces  P(x)  A  S(x) 

that  the  number  of  Q’s  in  the  set  R(x), 

about  P  A  S  A  R  -+  Q. 

each  one  corresponding 

is  unaffected.  Hence, 

and  is  a  consequence 

the  rule  R  -+  -Q 

to  be  an  e-small 

subset  of  P(x) 

information 

and  hence 

7.  Problems 

The  principle  of  indifference 

and  maximum 

icism.  Any  such  criticism 
is  important 
we  consider  problems 
learning  and  computation. 

that  we  examine 
relating 

is,  at  least  potentially, 
the  difficulties 
to  causal  reasoning, 

relevant 

entropy  have  both  been  subject 

to  random  worlds.  Hence, 

to  crit- 
it 
In  this  section, 
language  dependence,  acceptance, 

that  people  have  found. 

7.1.  Causal  and  temporal 

information 

The  random-worlds  method  can  use  knowledge  bases  which 

include  statistical, 

about  actions,  using  causal  and  temporal 

is  this  language 

sufficient?  We  suspect 

traditional  knowledge 

for  most 

of  adequacy 

information.  When 

can  be  subtle.  This 

order,  and  default 
is,  in  fact,  adequate 
the  question 
domain  of  reasoning 
there  would 
includes 
such  languages, 
finding  a  representation  with  sufficient  expressivity 
to  know  whether 
about  causal 
used  with  the  most  straightforward 

choosing 
time  explicitly. 
a  world  might  model  an  entire 

to  be  no  difficulty 
to  talk  about 

seem 
the  ability 

representations 

reasoning. 

is  certainly 

information. 

representation 

a  suitable 
In  the  semantics 

tasks.  Nevertheless, 
the  case  for  the  important 
In  principle, 
that 
to  many 
temporal  sequence  of  events.  However, 
is  only  part  of  the  problem:  we  need 
reflect  our  intuitions 
results  when 

first-order  vocabulary 
appropriate 

of  temporal  knowledge. 

It  turns  out  that  random  worlds  gives  unintuitive 

the  degrees  of  belief  we  derive  will  correctly 

first- 
that  it 

126 

IT  L(trcr~lrus r/  trl. /AI-trficxtl 

I~rltrllipwc~r  87  (19%)  75-143 

(in 

shown 

related 

information 

information 

information 

information, 

to  maximum 

[ 33,581.  Hence, 

naively.  On  the  other  hand,  Hunter 

it  is  represented 
representation 

This  observation 
is  closely 

provided 
that  by  using  an  appropriate 

[ 33 ]  has  shown  that  maximum- 
appropri- 

is  not  really  a  new  one.  As  we  have  observed, 
entropy 

the  random-worlds 
the  context  of  a  unary  knowledge 
techniques  has  been  that  they  seem 
it  is  not  surprising 
if  we  represent  causal  and 

method 
base).  One  significant  criticism  of  maximum-entropy 
to  have  difficulty  dealing  with  causal 
that  the  random-worlds  method  also  gives  peculiar  answers 
temporal 
entropy  methods  can  deal  with  causal 
ately.  We  have  recently 
Hunter’s,  but  nevertheless  distinct). 
causal 
tion  and  explanation 
Baycsian 
frame  problem 
some  of‘ the  standard  problems 

to 
the  random-worlds  method  can  also  deal  well  with 
allows  us  to  (a)  deal  with  predic- 
in 
information 
to  the 
in  the  situation  calculus  154 1. In  particular,  our  proposal  deals  well  with 

(321. 
the  fact  we 
want  to  emphasize  here  is  that  there  may  bc  more  than  one  reasonable  way  to  represent 
our  knowledge  of  a  given  domain.  When  one  formulation  does  not  work  as  we  expect, 
It  will  often  turn  out  that  the 
we  can  look  for  other  ways  of  representing 
new  representation 
that  were  ignored  by  the 
naive  representation. 
of  reasoning 

in  the  area.  such  as  the  Yale  Shooting  Problem 

[ 58 1. and  Cc)  provide  a  clean  and  concise 

captures  some  subtle  aspects  of  the  domain, 

that  this  is  the  case  with  our  alternative 

The  details  of  the  proposal  are  beyond 

to  this  issue  a  number  of  times  below. 

the  scope  of  this  paper.  However, 

161.  Indeed,  our  representation 

about  actions.)  WC return 

of  the  type  implicit 

causal  networks 

represent  causal 

the  problem. 

(We  believe 

formulation 

problems, 

solution 

(related 

(b) 

As  WC saw  above. 

information 

dence:  causal 
shows 
that  choosing 
context  of  the  random-worlds 

approach. 

random  worlds  suftbrs 

dcpen- 
is  treated  correctly  only  if  it  is  represented  appropriately.  This 
in  the 
of  our  knowledge 

Irom  ;I  problem  ot‘ representation 

representation 

is  important 

the  “right” 

dependence 

representation 

is  a serious  problem  because, 

In  some  ways.  this  representation 

in  practice, 
or  not?  Before 
this.  WC note  that  the  situation  with  random  worlds  is  actually  not  as  bad  as  it 

how  can  we  know  whether  WC have  chosen  a  good 
addressing 
might  be.  As  WC pointed  out  in  Section  5. I.  the  random-worlds 
to  merely  syntactic  changes 
always  result  in  the  same  degrees  of  belief.  So  if  a  changed  representation 
answers, 
it  can  only  be  because  WC have  changed 
a  different  ontology.  or  the  new  representation  might  model 
level  of  detail  and  accuracy.  The  representation 
concerns  more  than  mere  syntax.  This  gives  us  some  hope  that  the  phenomenon 
understood 

is  not  sensitive 
in  the  knowledge  hasc:  logically  equivalent  knowledge  bases 
gives  different 
the  semantics:  we  might  be  using 
the  world  with  a  different 
exhibited  by  random  worlds 
can  be 

and,  at  least  in  some  cases,  be  seen  to  be  entirely  appropriate. 

dependence 

approach 

irrelevant 

Unfortunately, 

seemingly 
concern 
pose 
Pr,,  ( White(c) 
Blw 

things.  Perhaps 

changes  can  affect 

it  does  seem  as  if  random  worlds  really 

is  too  sensitive;  minor  and 
examples 
sup- 
and  we  take  KB  to  be  true.  Then 
/ KB)  =  I /2.  On  the  other  hand,  if  we  refine  7 White  by  adding  Red  and 
then 

that  -1White  is  their  disjoint  union, 

and  having  KB’  assert 

changes.  For  instance, 

the  most  disturbing 

to  definitional 

language 

is  Mite 

in  our 

lnnguagr  dependence,  or  sensitivity 

the  only  predicate 

to  our  language 

dependence.  Unfortunately,  while  interval-valued 

if  we  generalize  our  concept  of  “degree  of  belief”,  say  to  intervals 

E  Bacchus  et  al.  /Artificial 

intelligence 

87  (1996)  75-143 

127 

Pr,(  White(  c)  1 KB’)  =  l/3.  The  fact  that  simply  expanding 
the  language  and  giving  a 
definition  of  an  old  notion 
(Red  and  Blue)  can 
affect  the  degree  of  belief  seems  to  be  a  serious  problem.  There  are  several  approaches 
to  dealing  with  this  issue. 

in  terms  of  the  new  notions 

( TWhite) 

One  approach 

to  dealing  with  representation 

independence 

that  does  not  suffer  from  it.  To  do  this,  it  is  important 

is  to  search  for  a  method 
to 

independent. 

debate  about 

of  computing  degrees  of  belief 
have  a  formal  definition  of  representation 
we  can  investigate  whether 
that  are  representation 
with  our  use  of  point-valued 
a  few  very  weak  assumptions) 
that  act  like  probabilities 
considerable 
Perhaps 
point  values 
representation 
well  be  representation 
they  do  not  solve 
representation 
essentially 
independent 
Another 

independence 
interesting 

in  [ 0,  11,  we  can  address 

in  their  sense. 

nondeductive 

independent 

response 

any 

the  problem.  Halpern 

there  are  nontrivial  approaches 

For  example,  one  might 

independence.  Once  we  have  such  a  definition, 
to  generating  degrees  of  belief 
lies 
think  that  the  problem 
that  (under 
degrees  of  belief 
In  fact,  there  has  been 

that  gives  point-valued 
independent. 
forced  by  point-valued 

probabilities. 

any  approach 

cannot  be  representation 
the  “excess  precision” 

degrees  of  belief.  After  all,  it  is  fairly  obvious 

rather  than 
these  concerns,  as  well  as  avoid  the  problem  of 
degrees  of  belief  might 
than  random  worlds, 
of 
that 

in  [ 311,  provide  a  definition 
and  show 

and  Koller, 

in  many  more  circumstances 

reasoning, 
cannot  be  representation 

inference  procedure 

in  the  context  of  probabilistic 

learning 

inductive 

dependence 

in  machine 

of  induction 

as  reflecting 

is  to  accept 

at  our  disposal. 

that  representation 

this,  but  to  declare 

is  indeed  a  significant 

inevitable 
surprised 

of  the  world 
have 

i.e.,  that  the  choice  of  an  appropriate  vocabulary 

into  colors.  Researchers 
that  bias 
long 

if  this  is  the  case  we  would  hope  to  have  a  good  intuitive  understanding 

is  an 
realized 
reasoning.  So  we  should  not  be  completely 

is 
one, 
justified, 
In  our  example  above,  we 
which  does  encode  some  of  the  information 
the  bias  of  the  reasoner  with  respect 
can  view 
the  choice  of  vocabulary 
the 
and 
to  the  partition 
component 
of 
philosophy 
effective 
if  it  turns  out 
that  the  related  problem  of  finding  degrees  of  belief  should  also  depend  on  the  bias. 
of 
Of  course, 
like  to  give  the 
how  the  degrees  of  belief  depend  on  the  bias.  In  particular,  we  would 
knowledge  base  designer 
representation. 
the  “appropriate” 
in  the  context  of  random  worlds. 
This  is  an  important 
independence  with  respect 
A  third  response 
this  approach,  consider  another 
is  a  bird, 
(who  may  or  may  not  be  a  bird).  One  obvious  way 
is  to  have  a  language  with  predicates  Bird  and  Fly,  and 

to  a  large  class  of  queries 
example.  Suppose 
and  Opus 
this  information 
to  represent 
take  the  KB  to  consist  of  the  statements 
( KB)  =  0.5. 
It  is  easy  to  see  that  Pr, 
language,  one  that  uses  the  basic 
But  suppose 
take  KB’  to  consist  of  the  statements 
predicates  Bird  and  FlyingBird.  We  would 
1) FlyingBird(  x)  1 Bird(x) 
) 
We  now  get  Pr,(  FlyingBird(  Tweezy)  ] KB’)  =  0.5  and  Pr,(  Bird(  Opus)  1 KB’)  =  2/3. 
that  Tweety  Aies  is  0.5  in  both  cases.  In  fact,  one  can 
Note  that  our  degree  of  belief 

JIx x  0.5,  Bird(  Tweety),  and  Vlx( FlyingBird(  x)  +  Bird(x) 

that  we  know  that  only  about  half  of  birds  can  fly,  Tweety 

(Fly(  Tweety)  ( KB)  =  0.5  and  Pr,(Bird(  Opus) 

and  seemingly  difficult  problem 

(see  also  [ 3 1 ] ) . To  understand 

Jlx x  0.5  and  Bird(Tweety). 

is  to  prove  representation 

is  some  other  individual 

that  we  had  chosen 

to  use  a  different 

some  guidelines 

to  the  problem 

to  selecting 

) Bird(x) 

jlFly(x) 

then 

128 

E  Bacchus 

et  d. 

/Artijcial 

Intelligence 

87 

(1996) 

75-143 

that  this  conclusion 

is  robust  against  many  “reasonable” 

representation 

give  an  argument 
changes.  On 
two  representations.  Arguably, 
language  dependent 
contain 
it  would  be  useful 
recognizing 

information 

sufficient 

to  characterize 
that  not  all  queries  will  be. 

the  other  hand,  our  degree  of  belief 

that  Opus 

the  fact  that  our  degree  of  belief 

in  the 
is  a  bird  is 
is  a  direct  reflection  of  the  fact  that  our  knowledge  base  does  not 
that 
independent,  while 

it  a  single  “justified”  value.  This  suggests 

that  are  language 

is  a  bird  differs 

those  queries 

that  Opus 

to  assign 

7.3.  Acceptance  and  learning 

The  most  fundamental 

base  KB  and  wish 
not  considered 

assumption 

in  this  paper 
to  calculate  degrees  of  belief 

how  one  comes 

to  know  KB  in  the  first  place.  That 
as  knowledge‘?  We  do  not  have  a  good  answer 
since 

it  seems  plausible 

information 
is  unfortunate, 

accept 
This 
and  computing  degrees  of  belief  should  be  interrelated. 
that  perhaps  we  might  accept  assertions 
argued 
For  example, 
we  are  is  not  entirely 
color.  Nevertheless, 
accept 

suppose  we  observe  a  block  b  that  appears 
sure  that  the  block  is  indeed  white; 

if  our  confidence 

in  White(b) 

it  in  KB). 

it  (and  so  include 
The  problem  of  acceptance 

is  that  we  are  given  a  knowledge 
this  knowledge.  We  have 
relative 
is,  when  do  we 
to  this  question. 
that  the  processes  of  gaining  knowledge 
[43]  has 
In  particular,  Kyburg 
strongly. 
that  are  believed  sufficiently 
to  be  white.  It  could  be  that 
it  might  be  some  other  light 
threshold,  we  might 

exceeds  some 

is  well  known 

in  such  examples,  concerned  with  what  we  learn  directly 
[ 351.  But  the  problem  of  acceptance  we 
language.  Under  what 

than  usual,  because  of  our  statistical 

in  philosophy 

is  a  statement  such  as  lJFl~(n-) 

/lx M 0.9  accepted  as  knowledge? 

this  as  an  objective 
could  examine 
it  seems 

j Bird(x) 
statement  about 

the  world, 

it  is  unrealistic 

to 
in  the  world  and  count  how  many 
all  the  birds 
that  this  statistical 
in  KB  if 
large)  sample  of  birds  and  about  90%  of  the  birds  in 
to  be  typical,  and  we  then 
so 
has  statistics  similar 

statement  would  appear 

the  full  population 

the  sample 

is  assumed 

that,  with  high  confidence, 

that  90%  of  all  birds  fly.  This  would  be  in  the  spirit  of  Kyburg’s  suggestion 

inspects  a  (presumably 

fly.  Then  a  leap  is  made: 

from  the  senses, 
face  is  even  more  difficult 
circumstances 
Although  we  regard 
suppose 
that  anyone 
of  them  fly.  In  practice, 
someone 
this  sample 
conclude 
long  as  we  believe 
to  those  of  the  sample. 

the  sampling 

the  random-worlds  method  by  itself  does  not  support 

this  leap,  at  least 
Unfortunately, 
in  the  most  obvious  way.  That  is,  suppose  we  represent 
not  if  we  represent 
our  sample  using  a  predicate  S.  We  could 
the  fact  that  90%  of  a  sample 
of  birds  fly  as  [[Fly(x)  1 Bird(x)  A  S(x)jj,  M  0.9.  If  the  KB  consists  of  this  fact  and 
) KB)  =  0.9,  but  it  is  not.  In  fact, 
Bird(  Tweety),  we  might  hope  that  Prco(Fly(Tweety) 
random  worlds 
it 
maintains 
random  worlds 

treats  the  birds  in  S  and  those  outside  S  as  two  unrelated  populations; 

that  a  bird  not  in  S  will  fy.  22  Intuitively, 

is  not  treating  S  as  a  random  sample. 

the  default  degree  of  belief 

then  represent 

( l/2) 

22 A  related  observation,  that  random  worlds  cannot  do  learning  (although  in  a  somewhat  different  sense),  was 
made  by  Carnap  [ 121,  who  apparently 
this  reason. 

lost  his  enthusiasm  for  (his  version  of)  random  worlds  for  precisely 

E 

Bacchus 

et  al.  /Artificial 

Intelligence 

87 

(1996) 

75-143 

129 

Of  course, 

the  failure  of  the  obvious  approach  does  not  imply  that  random  worlds 

individuals 

representation. 

statistics.  As  was  the  case  for  causal  reasoning, 

incapable  of  learning 
be  to  find  an  appropriate 
fact  that  different 
of  each  other.  If  we  see  that  an  animal 
structure  and  so,  by  this  mechanism, 
If  we  see  a  giraffe, 
issue  is  subtle. 
in  general 
it  does  about  other  giraffes,  and  a  good  representation 
this. 

hint  at  properties  of  other  animals.  But  clearly 
less  about 
this  tells  us  much 

is  tall,  it  may  tell  us  something  about  its  genetic 
this 
the  height  of  animals 
reflect 
should 

Perhaps  we  need  a  representation 
their  properties  completely 

do  not  acquire 

than 

is 
the  solution  may 
reflecting 
the 
independently 

that  it  is  appropriate 

While  we  still  hope 

about  degrees  of  belief 

to  the  knowledge  base.  Thus, 

such  as  Pr(  J]Tall(x)  1 Animal(x) 
the  sampling 

random  worlds,  we  can 
to  find  ways  of  doing  sampling  within 
also  look  for  other  ways  of  coping  with  the  problem  of  learning.  One  idea  is  to  add 
in  a 
statements 
then  we  might 
sample  are  tall,  and  we  believe 
/IX ~1  0.2)  2  0.9  to  the  KB.  Although 
add  a  statement 
that 
this  does  not  “automate” 
it  allows  us  to  use  our  belief 
a  sample 
to  this  fact.  In 
the  agent  that  a  sample 
particular, 
is,  in  fact,  biased.  Adding  degrees  of  belief  would  also  let  us  deal  with  the  problem  of 
acceptance,  mentioned 
that  block  b  is 
white,  but  are  not  certain,  we  could  write  Pr(  White(  b))  2  0.9.  We  then  do  not  have  to 
fix  an  arbitrary 

is  likely 
this  representation 

to  be  representative,  without  committing 

allows  further  evidence 

to  learn  this  statistic, 

if  20%  of  animals 

of  this  subsection. 

at  the  beginning 

If  we  believe 

to  convince 

procedure, 

absolutely 

threshold 

Adding  degree  of  belief 

for  acceptance. 
statements 
to  now,  all  the  assertions  we  allowed 
in  a  given  world.  This 
semantics 
to  handle  such  a  statement 
distribution 
different 
equivalent. 

is  not 
for  degrees  of  belief 

approaches 

to  doing 

base 

to  a  knowledge 
in  a  knowledge  base  were  either 
the  case  for  a  degree  of  belief  statement. 
involve 

step.  Up 
true  or  false 
Indeed,  our 
looking  at  sets  of  possible  worlds.  Thus,  in  order 

is  a  nontrivial 

appropriately,  we  would  need  to  ensure 

that  our  probability 

over  the  possible  worlds  satisfies 

the  associated 

constraint.  A  number  of 

this  are  discussed 

in  [7],  and  shown 

to  be  essentially 

in 

all  propensities 

that  a  third  of  them  do?  Roughly 

for  dealing  with 
[4]  called 

the  existence  of  a  parameter  denoting 
are  equally 

Yet  another  approach 
random  worlds  presented 
worlds  has  a  strong  bias  towards  believing 
property,  and  this  is  not  always  reasonable.  Why  should 
all  birds  fly  than 
approach  postulates 
to  fly.  Initially, 
information 
other  birds.  As  shown 
samples.  We  can  also  show  [ 37 ]  that  the  random-propensities 
same  attractive  properties 
inference, 
problems. 
representative 
would  conclude 
an  important 

is  tall.  Addressing 
investigation. 

and  irrelevance.  Unfortunately, 

specificity, 
In  particular, 

the  propensity  of  birds 

that  almost  everything 

the  random-propensities 

it  learns  “too  often”, 

issue  that  deserves 

samples.  Given 

in  [4], 

further 

about 

is  to  use  a  variant  of 
the  learning  problem 
the  random-propensities  approach.  Random 
that  exactly  half  the  domain  has  any  given 
that  half  of 

it  be  more 

likely 

speaking, 

the  random-propensities 

likely.  Observing 
to  fly,  and  hence  about 

method  does,  indeed, 

the  “propensity” 

of  a  bird 
a  flying  bird  gives  us 
the  flying  ability  of 
learn  from 
approach  has  many  of  the 
in  particular  direct 
has  its  own 
that  are  not 

random  propensities 

i.e.,  even  from  arbitrary  subsets 

the  assertion  “All  giraffes  are  tall”,  random  propensities 

that  we  have  shown  for  random  worlds, 

this  problem  appropriately 

is 

Our  goal 

in  this  research  has  been 

to  understand 

in  first-order  probabilistic 

and  default 

reasonable 

to  ignore  or  downplay 

theory 

turns  out  to  be  impractical 
for  approximations 

in  a  starch 

for  computational 
and  heuristics. 

some  of  the  fundamental 
issues 
reasoning.  Until  such  issues  are  under- 
If 
reasons,  we  can 

about  computation. 

concerns 

in  [27].  computing 
in  general.  This 

degrees  of  belief  according 
is  not  surprising: 

to  random  worlds 

is, 

problem.  Note  that,  in  spite  of  its  undecidability, 

vicwcd  as  a  powerful  and  useful  tool.  We  believe 

is  analogous.  Random  worlds 

lirst-order 
our  language  extends 
Although  unfortunate,  we  do  not  view  this 
logic 
first-order 
that  the  situation  with 
it  is 

is  not  just  a  computational 

tool; 

is  undecidable.” 

it  is  perhaps 

involved 
stood, 
an  ideal  normative 
still  use  it  as  guidance 

As  WC show 

intractable 
indeed. 
logic.  for  which  validity 
as  an  insurmountable 
is  nevertheless 
random  worlds 
inherently 

interesting  because  of  what  it  can  tell  us  about  probabilistic 

reasoning. 

techniques 

that  work  efticiently 

is  also  hard  in  general, 

and  constants.  We  showed 

But  even  in  terms  of  computation. 

the  situation  with  random  worlds  is  not  as  bleak  as 
it  might  seem.  We  have  presented  one  class  of  much  more  tractable  knowledge  bases: 
in  [28]  and  in  Section  6 
those  using  only  unary  predicates 
that.  in  this  case,  we  can  often  use  maximum  entropy  as  a  computational 
degrees  of  belief.  While  computing  maximum  entropy 
heuristic 
this  class  of  problems 
be  expressed  using  unary  predicates, 
example, 
(set 
predicates 
that  statisticians 
event 
cases  where  statistics  are  used,  we  have  a  basic  unit  in  mind  (an  individual, 
(predicates  1 we  consider  are  typically 
household,  etc.).  and  the  properties 
single  unit  (i.e.,  unary  predicates).  Thus,  results  concerning 
in  practice. 
for  unary  knowledge  bases  are  quite  significant 

tool  in  deriving 
there  are 
in  practical  cases.  As  we  have  already  claimed, 
is  an  important  one.  In  general,  many  properties  of  interest  can 
they  express  properties  of  individuals.  Fat 
state 
in  such  predicates  as  quantum 
typically  use  only  unary 
In  fact,  a  good  case  can  be  made 
since  an 
in  a  sample  space  can  be  identified  with  a  unary  predicate  1691.  Indeed,  most 
a  family,  a 
to  a 
computing  degrees  of  belief 

[ 131  such  as  symptoms 
tend  to  reformulate 

in  physics  applications  we  are  interested 

and  diseases. 
all  problems 

in  terms  of  unary  predicates, 

Similarly,  AI  applications 

and  expert  systems 

relative 

[ Ifi]). 

since 

Even  for  nonunary  knowledge  bases,  there  is  hope.  The  intractability 

in 
[ 27)  use  knowledge  bases 
to  mimic  a  Turing  machine 
that  l’orcc  the  possible  worlds 
computation.  Typical  knowledge  bases  do  not  usually  encode  Turing  machines!  There 
may  therefore  be  many  cases 
specific 
structure,  which  may  simplify  computation.  This 
domains 
seems  quite  possibly 
involve 
reasoning 

in  which  computation 

in  certain  problems 

impose  additional 

to  be  the  case. 

about  action. 

proofs  given 

in  particular, 

In  particular, 

is  practical. 

typically 

that 

Furthermore, 

as  we  have  seen,  WC (XXII compute  degrees  of  belief  in  many 

cases.  In  particular,  we  have  presented  a  number  of  theorems 
of  belief  are  for  certain 
theorems  hold  for  our  language  in  its  full  generality, 

interesting 
that  tell  us  what  the  degrees 
important  classes  of  knowledge  bases  and  queries.  Most  of  these 
predicates. 

including 

nonunary 

”  Although. 

in  fact.  finding  degrees of  belief  wing  random  worlds  is  even IPZOW intractable  than  the  problem 

of  deciding  validity 

in  first-order 

logic 

E  Bacchus  et  al.  /Artificial 

Intelligence 

87  (I  996)  75-143 

131 

results 

We  believe 
be  more  “irrelevance” 
be  ignored.  Such  results  could 
simpler 
examples 
cases  where  no  single 

that  combining 

that  many  more  such  results  could  be  found.  Particularly 
that  tell  us  when 
then  be  used  to  reduce  apparently 

interesting  would 
large  parts  of  the  knowledge  base  can 
to 
apply.  We  have  already  seen  in  some  of  the 
in 

complex  problems 

results  can  often  let  us  compute  degrees  of  belief 

different 
result  suffices. 

to  which  other  techniques 

forms, 

Nevertheless, 

there  are  many  natural  knowledge  bases  that  fail  to  meet  the  syntactic 

re- 

strictions 
literature 
cannot  understand 
special-purpose 
required  arguments 

required  by  the  theorems  we  have  provided. 
includes  many  quite  complicated 

In  particular, 

the  default-reasoning 

examples,  and  we  have  often  found 

that  we 

random  worlds’  behavior  on  these  without  some 

(often  nontrivial) 

arguments. 

(Interestingly, 

it  seems 

to  us  that  the  complexity 

of  the 

is  correlated  with  how  controversial 

the  example 

is!) 
that  of  characterizing 

strategy: 

Of  course, 

that  confronts 

these  difficulties 

suggest  a  research 

technical  challenges 

that  a  major  obstacle 

at  one  of  the  interesting 

restrictions,  whose  purpose  was  to  ensure 

the 
behavior  of  the  random-worlds  method  on  ever  larger  classes  of  examples.  We  close 
such  a  research 
by  hinting 
agenda. 
is  simply 
It  turns  out  (perhaps  not  that  surprisingly) 
that  we  had  to  impose 
the  richness  of  our  language.  Consider  Theorem  5.16.  Recall 
rather  severe  syntactic 
that  we  could  identify 
relevant  to  a  property  p(x).  The  conditions  were  made  so  strong 
all  the  subformulas 
it  is  easy,  in  a  language  with  as  much  expressive  power  as  ours,  to 
in  general 
because 
logical 
in  which  one  part  of  the  knowledge  base  places  nontrivial 
examples 
construct 
unrelated  concept.  We 
or  probabilistic 
certainly  believe 
in  a 
reasoning 
much  more  comprehensive 
result.  But  it  appears 
in  a  way  that  does  not  admit  contrived 
to  be  hard  to  state  clean,  checkable, 
tools 
counterexamples. 
irrelevant. 
are  provided 
In  fact, 
the  properties  of  a 
the  results  of  [37]  can  be  viewed  as  steps  towards  characterizing 
prior  distribution 
that 
such  results  apply 
method. 

in  this  regard  is  made  in  [ 371,  where  additional 
formulas 

on  a  superficially 
that  random  worlds  can  do  inheritance 

that  lead  to  results  such  as  Theorems  5.6  and  5.16,  and  show 

Some  progress 
for  testing  whether 

than  suggested  by  this  particular 

can  be  treated  as  being 

the  uniform  prior  used 

in  the  random-worlds 

(i.e.,  syntactically) 

to  priors  other 

and  irrelevance 

constraints 

conditions 

fashion 

than 

8.  Summary 

The  random-worlds 

approach 

for  probabilistic 

reasoning 

intuitive 
semantics, 

ideas:  possible  worlds  and  the  principle  of  indifference. 
features: 
l  It  can  deal  with  very  rich  knowledge  bases  that  involve  quantitative 

it  has  many  attractive 

is  derived 

from 

two  very 
In  spite  of  its  simple 

the  form  of  statistics,  qualitative 
information.  The  language 
such  as  the  representation 

information 
is  sufficiently  powerful 
of  nested  defaults. 

in 
in  the  form  of  defaults,  and  first-order 
for  even  fairly  esoteric  demands 

information 

l  It  uses  a  simple  and  well-motivated 
semantics 

allow  us  to  examine 

statistical 

interpretation 
the  reasonableness 

for  defaults.  The  cor- 
of  a  default  with 

to  our  entire  knowledge  base,  including  other  default  rules. 

responding 
respect 

132 

E  Buccl~us  et  al.  /ArtiJicictl 

lnlelligence 

87  (I  996)  75-143 

to  ignore 

the  ability 

like  a  preference 

l  It  validates  many  desirable  properties, 

the  ability 
these  properties 

tion, 
names, 
tantly, 
worlds.  In  particular,  ad  hoc  assumptions. 
no  part  in  the  definition  of  the  method. 

informa- 
of  unique 
to  combine  different  pieces  of  evidence,  and  more.  Most  impor- 
from  the  very  simple  semantics  of  random 
these  properties,  play 

a  default  assumption 

for  more  specific 

arise  naturally 

information, 

to  realize 

irrelevant 

designed 

l  It  avoids  many  of  the  problems 

(such  as  the  disjunctive 

soning 
that  have  plagued 
class  inheritance 
hard  to  avoid  problems  which. 

that  have  plagued  systems  of  reference-class 

rea- 
and  many  of  the  problems 
problem) 
(such  as  exceptional 
reasoning 
sub- 
and  the  lottery  paradox).  Many  systems  have  been  forced  to  work 

systems  of  nonmonotonic 

reference-class 

in  fact.  never  even  arise  for  random  worlds, 
important 

reasoning 

l  The  random-worlds 

approach  subsumes 
them  to  the  case  of  first-order 

several 
logic.  In  particular, 

generalizes 
tive  reasoning,  probabilistic 
the  principle  of  maximum 
But  it  is  far  more  powerful 

reasoning, 

theories  of  nonmonotonic 
entropy,  some  rules  of  evidence  combination, 

certain 

than  any  of  these  individual 

systems. 

systems,  and 
it  encompasses  deduc- 
inference, 
and  more. 

As  we  saw  in  Section  7,  there  are  certainly 

some  problems  with  the  random-worlds 
that  these  problems  are  far  from  insuperable.  But,  even  conceding 

for  the  moment, 

the  substantial 

success  of  random  worlds  supports  a  few 

method.  We  believe 
these  problems 
general  conclusions. 
One  conclusion 

concerns 

that  degrees  of  belief  provide  a  powerful  model  for  understanding 
instance, 

the  role  of  statistics  and  degrees  of  belief.  The  difference  be- 
the  two,  is  at  the  heart  of  our  work.  People  have 
rational 
shows 
in  almost 
in  which  we  have  complete  statistical  knowledge 
is,  of  course,  dealt  with  appropriately  by  random  worlds,  But  more 
(which  need  not  be  precise)  can  still  be 

to  assign  degrees  of  belief,  using  a  principled 

theory).  The  random-worlds 

through  decision 

information 

technique, 

approach 

include  defaults  and/or 

to  give  useful  answers,  Likewise,  completely 
a  rich  first-order 

nonnumeric 
theory  of  some  application 

(for 

tween  these,  and  the  problem  of  relating 
long  realized 
behavior 
that  it  is  possible 
any  circumstance.  The  ideal  situation. 
concerning 
realistically, 
utilized  by  random  worlds 
data,  which  may 
domain, 
the  user’s  knowledge 
reasoning  paradigm  we  know  of‘. 

can  bc  used.  Probabilistic 
base.  Indeed, 

even  partial  statistical 

a  domain, 

reasoning  need  not  make  unrealistic 
in  a  sense 

less  demands 

it  makes 

demands  of 
than  any  other 

to  our  next,  mom  general 

seemingly 
leads 
forms  of representation  and  reasoning  c(ltr  (and,  we  believe,  should)  be  unified. 
this  goal 
specialized 

that  we  can  take  a  large  step  towards 

(with  clear  semantics) 

listed  above  suggest 

conclusion,  which 

that  subsumes 

is  that  many 

language 

finding  a  powerful 

This 
disparate 
The  first  two  points 
by  simply 
representations. 
using  nested  defaults,  or  combining 
random-worlds 
diverse 
Clearly 
“defaults”  which  have  no  interesting 
think 
and  default 

types  of  reasoning 
this  is  not  always  possible; 

that  our  work  demonstrates 

reasoning  method 

reasoning 

defaults  and  statistics) 

The  advantages  we  have  Sound  (such  as  a  clear  and  general  way  of 
apply  even  if  one  rejects  the 
is  only  part  of  the  answer.  Can 
really  be  seen  as  aspects  of  a  single  more  general  system? 
of 
there  are  surely  some  interpretations 
to  statistics  whatsoever.  However,  we 

for  instance, 
connection 

itself.  But  the  language 

that  the  alleged  gap  between  probabilistic 

reasoning 

is  much  narrower 

than  is  often 

thought. 

In  fact,  the  success  of 

E  Bacchus  et  al./Artijicial 

intelligence 

87  (1996)  75-143 

133 

random  worlds  encourages 
representation 

paradigms 

us  to  hope 

that  a  synthesis  between  different  knowledge 

is  possible 

in  most  of  the  interesting  domains. 

Appendix  A.  Proofs  of  results 

Theorem  5.5.  Assume  that  KB  k, 
that  I%,( 
exist  is  that Pr, 

(8  ) KB)  exists. 

9  ) KB  A 0)  exists.  Moreover,  a  sujficient  condition for  Pr,(p 

rp and  KB  p, 

70.  Then  KB  A f3 b,.,,,  rp provided 
1 KB  A 0)  to 

Proof.  Since  KB  p,,  4,  Pr, 
there  exists  some  E  >  0  for  which  we  can  construct 
follows:  N’  is  an  increasing 

sequence  of  domain 

(-0  1 KB)  #  1,  so  that  Pr, 

sizes,  li 

(0  1 KB)  #  0.  Therefore, 

a  sequence  of  pairs  N’,?’ 

as 
sequence  of 

is  a  decreasing 

tolerance  vectors,  and  Pr$,  (0  ) KB)  >  E.  For  these  pairs  N’,  ?  we  can  conclude  using 
standard  probabilistic 

reasoning 

that 

Since  Pr, 

(~4p  1 KB)  =  0,  it  follows 

that  limi,m  Prg,  (asp 

j  KB)  =  0.  Moreover,  we 

know 

that  for  all  i,  Pr$(B  1 KB)  >  E  >  0.  We  can  therefore 

take  the  limit  as  i  +  00, 

and  conclude 
must  be  1. 

that  limi,oo  Prc,(  1~  ( KB  A 8)  =  0.  Thus, 

if  Proo(  (o 1 KB  A 0)  exists, 

it 

For  the  second  half  of  the  theorem,  suppose 

that  Pr,(B 

) KB)  exists.  Since  KB k,.,,, 

+I, we must  have  that  Pr, 
all  N  sufficiently 
that  Prz(e 

large  (where  “sufficiently 

(0  I KB)  = p  >  0.  Therefore, 

small  and 
for  all  7’ sufficiently 
large”  may  depend  on  73,  we  can  assume 

[ KB)  >  E >  0.  But  now,  for  any  such  pair  N, 7’ we  can  again  prove  that 

the  limit,  we  obtain 

that  Pr,( 

-~rp I KB  A 0)  must  also  have  a  limit  that  must  be 

Taking 
0.  Hence  Pr, 

(v,  ) KB  A  0)  =  1,  as  desired. 

0 

Theorem  5.6.  Let  KB  be  a  knowledge  base  of  the form  $(c’)  A  KB’,  and  assume  that 
for  all  suficiently  small  tolerance  vectors  ?, 

KBt7’1 t= l(d-3 

I9Cx’)lIa  E  CwPl. 

If  no  constant  in  c’ appears  in  KB’,  in  p(  x’),  or  in  $(x’), 
[a,  p]  , provided  the  degrees  of  belief  exist. 24 

then  Pr,( 

cp( Z)  I  KB)  E 

24 The  degree  of  belief  may  not  exist 

since 

lim?,G 

liminfhr,, 

Pr;(  cp  1 KB)  may  not  be  equal 

to 

lim i-ii 

lim  SU~~_~  Prc(  rp  ) KB).  However, 

it  follows 

from  the  proof  of  the  theorem 

that  both  these  limits 

lie  in  the  interval 

[a, p]  A  similar  remark  holds  for  many  of  our  later  results. 

Proof.  First,  fix  any  sufficiently 
for  which  KB[?] 
satisfy  KB[71 
bility  of  (p(?) 
probability 

small  tolerance  vector  ?,  and  consider  a  domain  size  N 
the  size  N  worlds  that 
the  proba- 

is  to  partition 

into  disjoint  clusters  and  then  prove  that,  within  each  cluster, 
is  in  the  range 
is  in  this  range  also. 

[cY,/I].  From  this,  we  can  show  that  the  (unpartitioned) 

is  satisfiable.  The  proof  strategy 

if  and  only 

The  size  N  worlds  satisfying  KlI[  ?]  arc  partitioned 

cluster 
except  for  the  constants 
bc  the  denotation  of’ J/( .a)  inside  the  cluster.  That  is.  if  W  is  a  world  in  the  cluster, 

in  (::  Now  consider  one  such  cluster,  and  let  A  2  {I,. 

if  they  agree  on  the  denotation  of  all  symbols 

so  that  two  worlds  are  in  the  same 
in  the  vocabulary  @ 
, N}X 
then 

the  cluster, 

else  is  fixed  throughout 

in  c  and  the  denotation  of 
the  set  A  is  the  same  in  all  worlds  W  of 
let  B  C  A  be  the  denotation  of  cp(.?)  A @(x’)  in  the  cluster.  Since 
/Ip(Xt)  / $(x’)  117 E  [a,  ,f3], we 
in  KB  except 
for  C: There  is  precisely 

Note  that,  since  $(2’,  does  not  mention  any  of  the  constants 
everything 
the  cluster.  Similarly, 
the  worlds  in  the  cluster  all  satisfy  KB(  ?I,  and  KB[  ?]  k 
know  that  /Bl/lA/  E  [ a,/3].  S’ lnce  none  of  the  constants 
for  the  statement  @Cc’). each  k-tuple 
one  world 
in  the  cluster 
this  Ihrm.  Among 
Therefore, 

those  corresponding 
in  the  cluster  satisfying  p(Z) 

in  the  cluster  are  of 
in  B  satisfy  ~(3. 

for  each  such  denotation, 

in  A  is  a  legal  denotation 

the  fraction  of  worlds 

is  \Bl/lAi  E  [a,p]. 

those  worlds,  only 

in  c”are  mentioned 

and  all  worlds 

to  tuples 

The  probability  Pri(cp(c3 

1 KB)  IS ;I weighted  average  of  the  probabilities  within 

the 

individual 

clusters,  so  it  also  has  to  be  in  the  range  [cr,p]. 

It  follows  that  lim  infN___  PrG(cp(c3 
[ CZ, p].  Since 

in  the  range 
both  limits 

/ KB)  and  limsup,_,Pr~(~(c~ 

this  holds  for  every  sufficiently 

small  7:  we  conclude 

/ KB)  are  also 
that  if 

lim  liminfPri(p(?) 
f-6 

hfix 

/ KB) 

and 

lim  limsupPrd(p 
.A-+  x 
i  4i 

/ KB) 

exist  and  are  equal, 

then  Pr, 

((p(F’l  1 KB)  has  to  be  in  the  range  [a,P], 

as  desired. 

Cl 

Theorem 

5.16.  Let  c  be  u  comtmt  md 

let  KB  be  a  knowledge  buse  sati?fying  the 

,following  conditions: 

(2)  KB  +  (j/o(c). 
(b) 

function,  mu1  cmstarrt) 

for  my  expressiorr  of  the ,fi,rrrr Jjp( I) 
KB/=Vx(r,&,(.r) 
the  (predicate, 
the  left-hand  side  qf  the  cmditionals 
conditiorl 
the  constant  c  does  rrot uppear 
that  ,for  all  suficiently  mull 

tolermce 

( b ) . 

(c) 

(d) 
Assume 

in  the  jhrmulu  p(x). 
vectors  ?I 

=+$(.I-))  orthutKB~Vx(~~(x) 

j-@(x)). 

/ $(  x)  // ,  in  KB,  it  is  the  cue 

that  either 

vynlbols  in  q(x) 

appear 

in  the  proportion  expressions  described 

in  KB  only  011 
it1 

Therl  Pr, 

( (D( c)  1 KB)  E  [ CY. /3 1. provided 

the  degree  oj  belief  exists. 

E  Bucchus  et  al.  /Arti$cial 

Intelligence 

87  (I  996)  75-143 

135 

in  this  theorem. 
sets  of  worlds  satisfying 

Proof.  This  theorem 
That  is,  for  each  domain  size  N  and  tolerance  vector  7’, we  partition 
N  satisfying  KB[?l 
into  clusters  and  prove  that,  within  each  cluster, 
q(c) 
clusters  are  defined  quite  differently 

is  proved  with  the  same  general  strategy  we  used  for  Theorem  5.6. 
the  worlds  of  size 
of 
the  probability 
the 
this  suffices  to  prove  the  result.  However 

is  in  the  interval 

.  As  before, 

[(Y, p] 

We  define 

the  clusters  as  maximal 

the  following 

three  con- 

ditions: 

(1)  All  worlds 

in  a  cluster  must  agree  on  the  denotation  of  every  vocabulary 

symbol 
they  agree  on 
in  q(x).  Note  that,  in  particular, 
c.  They  must  also  agree  as  to  which  elements 

except  possibly 
the  denotation 
satisfy  cl/o (x)  ;  let  this  set  be  Ao. 

those  appearing 
of  the  constant 

(2)  The  denotation 

of  symbols 

in  9  must  also  be  constant, 

. . , N}  -  Ao.  Then 

a  member  of  A0  is  involved.  More  precisely, 
ments  {I,. 
f  of  arity  r  appearing 
in  q(x), 
if  dl,..., 
d,,d,+l  E  & 
then  R(dl,.. 
f(dl,. . . ,d,)  =  dr+l  in  W  iff  f(dl,. 
means 
that  for  any  constant 
in  W,  then  it  must  denote  d’  in  W’. 

symbol  c’  appearing 

symbol  R  or  function 

except  possibly  when 
let  &  be  the  set  of  domain  ele- 
symbol 
and  for  all  worlds  W’  and  W  in  the  cluster, 
in  W,  and 
this 
,  if  it  denotes  d’  E  & 

. ,d,)  holds 
. . ,d,)  =  dr+l  in  W’.  In  particular, 

in  W’  iff  it  holds 

in  p(x) 

for  any  predicate 

(3)  All  .worlds 

in  the  cluster  are  isomorphic  with  respect 

symbols 
if  W  and  W’  are  two  worlds  in  the  cluster,  then  there  exists 
in  q.  More  precisely, 
some  permutation  m  of  the  domain  such  that  for  any  predicate  symbol  R  appear- 
.  . . , d,) 
ing  in  q(x) 
holds  in  W  iff  R(n-(  dl)  ,  . . . , n(  d,)  )  holds  in  W’,  and  similarly 
for  function  sym- 
if  it  denotes 
bols.  In  particular, 
d’  in  W,  then  it  denotes  v(d’) 

and  any  domain  elements  dl,  .  . . , d,  E  (1,. 

symbol  c’  appearing 

to  the  vocabulary 

for  any  constant 

. . , N},  R(dl, 

in  (p(x), 

in  W’. 

It  should  be  clear  that  clusters  so  defined  are  mutually  exclusive  and  exhaustive. 

sense, 

to  prove 

to  the  elements 

We  now  want 

is,  in  a  precise 

that  each  cluster 

in  (p get  the  same  interpretation 

in  Ao.  That  is,  let  r  be  any  permutation 

respect 
is  the  identity  on  any  element  outside  of  A0  (i.e., 
W  be  any  world 
appearing 
the  symbols 
as  described 
an  immediate 
implies  condition 
W’  /=  KB[  Fj .  Because  of  condition 
that  vocabulary 
fail  to  happen  only 
if  some  expression 
in  W’.  We  show  that  this  is  impossible. 

symmetric  with 
of  the  domain  which 
for  any  d  6  Ao,  r(d)  =  d).  Let 
in  our  cluster,  and  let  W’  be  the  world  where  all  the  symbols  not 
of 
r 
is 
of  the  definition  of  W’;  the  restriction  on  the  choice  of  rr 
It  remains  only  to  prove  that 
in  the  statement  of  the  theorem,  and  the  fact 
in  W  and  in  W’,  this  can 
in  W  and 

as  they  do  in  W,  while  the  interpretation 
in  W  by  applying 
(1) 

(c) 
symbols  not  in  rp have  the  same  denotation 

that  W’  is  also  in  the  cluster.  Condition 

appearing 
above.  We  want 

in  q  is  obtained 
to  prove 

(3)  holds  by  definition. 

from  their  interpretation 

[Ix  has  different  values 

(2)  ;  condition 

J(cp( x)  1 $(x) 

consequence 

It  is  easy 

to  see  that  for  all  domain 

(W’,  KF)  I=  9(x) 
p  get  the  same  interpretation 
that  mentions  only  the  symbols  appearing 
structure  of  p’  can  be  used  to  show  that  (W,  V!7’)  k  p’(x) 

iff 
(  h w  ere  V  is  a  valuation  mapping  x  to  d),  since  the  symbols  not  in 
in  both  W  and  W’.  On  the  other  hand,  if  cp’ is  a  formula 
induction  on  the 

in  (p, then  a  straightforward 

iff  (W’,  T  o  Vf’)  k  q’(x), 

(K  V?,J  b  9(x> 

d,  we  have 

elements 

where  7~ o  V  is  the  valuation 
satisfying  p(x) 
be  the  set  of  domain  elements 
show  that  jB  fl  A//IA\  =  In-(B)  n  Al/IA/  or,  equivalently, 

is  the  set  of  elements 
satisfying  @(.x1  for  worlds 

that  maps  x  to  rr(V(n) 

in  W,  then  r(B) 

that 

).  Thus,  if  B  is  the  set  of  elements 
in  W’.  Let  A 
in  this  cluster.  We  want  to 

satisfying  q(x) 

there  are  only 

satisfying  p(x)  A $(x) 

two  cases:  either  KB  k  Yx(&(x) 
=+  $(x)), 
/=  Yx($~(x) 

in  which  case  A0  and  4  are  disjoint,  or  KB 

the  set  01  domain  elements 
(h) 

By  our  observations 
above, 
is  z-(B)  fl A.  By  condition 
-$(.Y)). 
that  A,)  i  A.  In  the  first  case.  since  TT is  the  identity  of  Ao,  it  is  easy 
rr(  B)  (3 A  =  B  n  A,  and  we  are  done. 
permutation 
that  W’  does  satisfy  KB[  ?I.  and 
the  cluster 
we  have  now  proved 
contains  precisely  all  such  worlds. 

to  consist  only  of  worlds 
that  all  worlds 

that  arc  isomorphic 
formed 

of  A  into  itself,  so  we  must  still  have  /r(B) 

in  this  way  are  in  the  cluster, 

n  Al  =  IB  n  Al.  We  conclude 
in  the  cluster.  Since  we  restricted 
to  W  in  the  above  sense,  and 
the  cluster 

in  W’ 
+ 
so 
to  see  that 
is  a 

In  the  second  case,  because  A0  C  A,  r 

is  therefore  also 

Having  defined 

the  clusters.  we  want 

that  the  degree  of  belief  of  p(c) 
[a,  p]  when  we  look  at  any  single  cluster.  By  assumption,  KB[q 

in  the  range 
//q(x) 
elements  of  A0  that  satisfy  p(x) 
KB  also  entails 
element  d  in  A”.  Condition 
of  C’ is  the  same  for  all  worlds 
and 
that  every  permutation 
has  a  corresponding  world 
IBJ  are  possible  denotations 
symmetry, 
precisely 
of  p(c) 

they  are  all  equally 
(BI/IAol  of  the  worlds 
in  any  one  cluster 

(d)  says  that  c  does  not  appear 

to  show 

the  denotation 

is  in  the  interval 

for  each  world  in  the  cluster, 

[a,@].  Moreover,  by  condition 

is 
/= 
the  subset  of  the 
(a), 
of  c  is  some  domain 
in  (D, and  so  the  denotation 
in  the  cluster.  Now  consider  a  world  W  in  the  cluster, 
in  W.  We  have  shown 
elements  constant) 
subsets  B’  of  size 
because  of 
in 

the  remaining 
In  particular,  all  possible 

in  the  cluster.  Furthermore, 

likely.  It  follows  that  the  fixed  element  d  satisfies  p(x) 

in  A0  (leaving 

satisfy  p(x) 

in  worlds 

in  the  cluster. 
for  cp(x) 

in  the  cluster.  Since  IBI/IAoI  E  [cy, PI, 

the  probability 

let  B  be  the  subset  of  A0  whose  members 

of  the  elements 

the  assertion  @o(c).  Therefore, 

j $0 (x)  /I., t  [a,  /? 1.  Therefore, 

is  in  this  range  also. 

As  in  Theorem  5.6,  the  truth  of  this  fact  for  each  cluster 

implies 

and  at  the  limit. 
sufficiently 

In  particular, 

small  ?,  we  conclude 

since  KB[?+l  k 
that  Pr,,(  p(  c)  j KB)  E  [a,  /?I,  if  the  limit  exists. 

/ (//o(.x)JJ,  E  [a,/?] 

lJ~(x) 

its  truth  in  general 
for  every 
0 

Theorem  5.23.  Suppose  KB  has  the  form 

and,for 
no  symbol  appearing 
for  some  ,j,  [ aj,  pj] 
Then.  if  it  exists, 

all  i,  KB  /== ‘v’x(~,$(x~ 

in  q(x) 
is  the  tightest 

appears 

IS  &I(X)) 

/‘I -~(IIsl/~(x)lI.,  MI  0).  Assume  also  that 
that, 
in  KB’  or  in  any  @i(c).  Further  suppose 
interval.  That  is, for  all  i  #  j,  LY; <  ai  <  /3j  <  /3i. 

Pr,(v(c) 

I KB)  E  [ai.P,l 

E  Bacchus  et  al./Artijicial 

Intelligence  87  (1996)  75-143 

137 

Proof.  The  proof  of  the  theorem 
the  form 

is  based  on  the  following 

result.  Consider  any  KB  of 

+ll~‘(x)II,  Ml  0) A 
Wrcl’(x) =+- ti(x))  A 
a  ie Ildx>  I 9(x)  II+ ir  PA 
KB' , 

where  none  of  KB’,  $I( x)  , t,b’ (x)  mention  any  symbol  appearing 
E >  0, 

in  p(x) 

. Then,  for  any 

Pr,(a 

-  E Q  I\v(x) 

I9’(~>11,  <  P  +  E 1 KB)  =  1. 

in  spirit  to  Theorem  5.16,  where  we  proved  that  (under 

Note  that  this  is  quite  similar 
an  individual 

certain  conditions) 
that  is,  the  degree  of  belief  is  derived  from  these  statistics.  Not  surprisingly, 
the  new  result 
is  similar 
for  many  of  the  details. 

; 
the  proof  of 
to  that  of  Theorem  5.16,  and  we  refer  the  reader  to  that  proof 

the  statistics  over  e(x) 

c  satisfying  $(c) 

“inherits” 

playing 
ti(x) 
cluster  and  let  A  be  the 

We  begin  by  clustering  worlds  exactly  as  in  the  earlier  proof,  with 

any  particular 

the  role  of  the  earlier  $0 (x).  Now  consider 
corresponding 
denotation  of  $(x) 
is  some  y  such  that  cr  -  TP <  y  <  j3  +  T,..  (Recall 
associated  with 
denotation 
in  A  ranges  over  subsets  of  A  of  size  ylA\.  From 
Theorem  5.16,  we  know  that  there  is,  in  fact,  an  equal  number  of  worlds 
corresponding 

comparisons  Z[  and  x,.  in  KB). 

to  every  such  subset. 

the  approximate 

of  q(x) 

. In  the  cluster,  the  proportion  of  A  that  satisfies  q(x) 
that  T!  and  TV are  the  tolerances 

the 
the  proof  of 
in  the  cluster 

In  this  cluster, 

Now  let  A’  be  the  denotation  of  e’(x) 

in  the  cluster 

(recall 

that  it  follows 

in  a  cluster  have  the  same  denotation 
in  computing 

of  the  clusters 

that  all  worlds 

y’  E  [ 0,  1 ] ,  we  are  interested 

in  the  cluster  such  that  the  proportion  of  p(x) 

construction 
I,V (n)  ) .  For  a  proportion 
worlds 
above,  it  follows  that  this  is  a  purely  combinatorial 
subset  A’  of  size  n’,  how  many  ways  are  there  of  choosing  yn  elements 
elements 
using 
without 
distribution 
mean  y  and  variance 

it  behaves  according 
[ 451)  . We  can  thus  conclude 

replacement.  25  Hence, 
(see,  for  example, 

that  the  distribution  of  y’n’ 

the  observation 

question:  given  a  set  A  of  size  n  and  a 
the 
this 
is  derived  from  a  process  of  sampling 
to  the  well-known  hypergeometric 

(representing 
for  which  q(x)  holds)  so  that  y’n’  elements  come  from  A’?  We  estimate 

in  A’  is  y’.  From  our  discussion 

that  y’  is  distributed  with 

from  the 
for 
the  fraction  of 

Y(l-y)(n-n’) 
(n-1)n’ 

<  y(l-y) 
nr 
’ 

<  _1_ 
’  4n” 

Since  KB  +  ~(~~t+b’(x)~~,  MI  0),  we  know 
large.  Now,  consider 
tends 

to  0  as  N  grows 

that  n’  =  IA’\  >  TIN.  Thus, 

this  variance 

the  event:  “a  world 

in  the  cluster  has  a 

in  fact,  a  number  of  ways 

25 There  are, 
technique.  We  can  do  this  because,  at  this  point  in  the  proof, 
predicates  or  not;  we  can  therefore 

safely  apply 

techniques 

to  solve 

this  problem.  One  alternative 

is  to  use.  an  entropy-based 
it  no  longer  matters  whether  KB  uses  nonunary 

that  usually  only  work  in  the  unary  case. 

from  above  by  some  small  probability 

this  is  bounded 

proportion  of  p(x)  within  A’  which  is  not  in  the  interval 
inequality, 
only  on  71 N.  That 
proportion 
general.  More  precisely, 
INS  T,  E, p  +  T,  +  E 1 is  at  most  />h.  But  this  probability 
infinity.  Therelhrc. 

the  I‘raction  of  overall  worlds 

is.  the  fraction  of  worlds 

is  at  most  r>,v.  Since 

this 

in  each  cluster 

is  the  case  for  every  cluster, 

[Y--E,  Y+E]“.  By  Chebychev’s 
I)~  which  depends 
the  “wrong” 
in 

that  have 

it  is  also 

true 
j qJ~‘(x)ll.,  @ 
to 

goes  to  0  as  N  tends 

for  which  J/p(x) 

Pri,  ( a  --  T, 

uljpc.\r 

~~I/‘(.~-),/,-..~+~,+EIKL~)=I 

As  i  4 C? WC can  simply  omit  T,  ;~nd  7,.  proving 
to  prove  the  theorem 
in  the  statement  of  the  theorem: 

It  is  now  a  simple  matter 
cation  KU”  of  the  KB  given 

the  required 
itself.  Consider 

result. 

the  following  modifi- 

l,, 
A  (u, 
r=, 

j,, 

l/p(x) 

/ tl/,(.v;/, 

7, 

fi’,, 

,“‘$I  (c)  A KB’. 

the  statistics 

where  WC eliminate 
more  specific  reference  classes  ).  From  Theorem  5.16  we  can  conclude 
KB”)  E  [ cu,,p,  J  (the  conditions 
know,  from  the  result  above. 

in  $,;  (the 
( (D( c)  j 
of  that  theorem  are  clearly  satisfied).  But  we  also 

that  for  each  $,.  for  i  <  ,j: 

l’or the  rct’ercncc  classes 

that  are  contained 

that  Pr, 

Pr,(cv; 

-E 

s 

/(p(x)  _ (//‘(.vJ/~,  C  0:  -  t  KB”)  =  I 

For  sufficiently 

small  E >  0,  the  assertion 

that 

ffl 

E <  I(cp(.i)  / t/7(.\  I/:,  I_ /l3, -  c 

logically 

implies 

that 

a,  51,  Ilds) 

/ $‘(.i-)/I,  _I!,, p,. 

so  that  this  latter  assertion 
1  (given  KB”) 
probability 

also  has  probability 
in  the  finite  conjunction 

1  given  KB”.  We  therefore  also  have 

We  can  now  apply  Thcorcm  5.2  to  conclude 
KB”  without  affecting  any  of  the  degrees  ol‘  belief.  But  the  knowledge  base  resulting 
from  adding 

that  WC can  add  this  finite  conjunction 

the  original  KB.  We  conclude 

to  KB”  is  precisely 

this  conjunction 

that 

to 

Pr,(p(c) 

1 KB)  =Pr,(gt(,) 

/ KB”) 

‘x  Iu,.~,], 

as  required. 

0 

Theorem 
follo~tGg  form: 

5.26.  Let  P  be  u  UIILI~J predicate,  md  cotlsider  a  blowledge  base  KB  of  the 

E  Bacchus 

et  al.  /Artificial 

Intelligence 

87 

(1996) 

75-143 

139 

/i 

(llp(X) 

( ccli(X>ll xxi~iA\(cri(C))A 

i 

q!X($i(X) 

A$j(X>)9 

i=l 

i.j=l. i#,j 

whereeither~~<1foralli=1,...,m,orLu~>Oforalli=1,...,m.Then,ifneither 
P  nor  c  appear  anywhere 

in  the  formulas  $i  (x)  ,  then 

Proof.  Assume  without 
theorems,  we  prove 
consider  any  7’such 
domain  size  N,  we  divide  the  worlds  of  size  N  satisfying  KB[q 
the  probability 
that,  within  each  cluster, 

that  Lyi >  0  for  i  =  1,.  . . , m.  As  in  previous 
loss  of  generality 
the  result  by  dividing 
into  clusters.  More  precisely, 
that  q  -  ri  >  0.  Let  /3i =  min(cui  +  ri,  1) .  For  any  such  7’ and  any 
into  clusters,  and  prove 
-  71,.  . . , an, - 
to 
this  suffices 

[S(q 
at  these  points, 

, f3,)  1.  Since  6  is  a  continuous 

is  in  the  interval 

the  worlds 

function 

of  q(c) 

the  worlds  satisfying  KB[?j 

into  maximal  clusters 

that  satisfy 

the  fol- 

in  a  cluster  must  agree  on  the  denotation  of  every  vocabulary 

for  P.  In  particular, 
, m,  let  Ai  denote 

the  denotations 
the  denotation 

of  $1 (x), 
of  +i (x) 

. . . , &!(x> 
in  the  cluster,  and 

symbol 
is  fixed.  For 
let  ni 

~“,),wh9... 
prove  the  theorem. 
We  partition 

lowing 

three  conditions: 

(1)  All  worlds 
except 
i  =  l,... 
denote 
(2)  All  worlds 
X=(1,... 

/Ail. 

in  a  cluster  must  have 
, N}  -  IJzt  Ai. 

the  same  denotation 

of  P  for  elements 

in 

elements 
cluster  satisfy  KB[  3, 

(3)  For  all  i  =  I,...,  m,  all  worlds 

ri  satisfying  P  within  each  set  Ai.  Note  that,  since  all  worlds 

in  the  cluster  must  have  the  same  number  of 
in  the 

it  follows 

that  ri/ui  E  [ (Yi -  Ti, pi] 

Now,  consider 

a  cluster  as  defined  above.  The  assumptions 
defined  by  the  numbers 

constraints 

the  sets  Al,. 

the  proportion 
on  the  denotation 
of  P  satisfying 

that,  besides 
constraints 
denotations 
in  this  cluster.  Our  assumptions 
the  number  of  elements  of  A;  for  which  P  has  not  yet  been  chosen 
that  satisfy  P(c) 
disjoint  except  for  d,  the  choice  of  P  within  each  Ai  can  be  made  independently 
other  choices.  Therefore, 

are  possible.  Let  d  be  the  denotation  of  c 
that  d  is  the  only  member  of  Ai  n  A,i.  Hence, 
is  ni  -  1. In  worlds 
the  Ai  are 
of  the 

of  P  within 
these  constraints 
guarantee 

I  of  these  elements  must  satisfy  P.  Since 

in  the  cluster  where  P(c)  holds  is 

the  number  of  worlds 

. . , A,,.  Therefore, 

,  precisely 

ri  - 

for  i =  1,.  . . , m. 
of  the  theorem 
imply 
ri,  there  are  no  other 
all  possible 

fi(::I:) 

i=l 

Similarly, 

the  number  of  worlds 

in  the  cluster  for  which  P(c)  does  not  hold  is 

fi  (y  1). 

i=l 

Therefore, 

the  fraction  of  worlds 

in  the  cluster  satisfying  P(c) 

is: 

Since  6  is  easily 
1.,/H,  E  1 cr, -  7,.  p,  1,  we  must  have  that  S( 1-1 .ittl  , 

to  he  monotonically 

seen 

increasing 

in  each  of  its  arguments 

and 

, ~,,,/a,,,)  is  in  the  interval 

I8la, 

-  71.. 

. a,,, ~~ T,,, 1. S(B,. 

. pm 1 J 

Using  the  same  argument  as  in  the  previous 
0 
the  desired 

result. 

theorems  and  the  continuity  of  8,  we  deduce 

Theorem  5.27.  Let  @I  atld  @z  he  two  su0~mabuluries 
the  cormant  c.  Consider  KBI  , ~1  t  C(  @I )  and  KB2,92  E L(  ~02). Then 

of  @ that  are  disjoint  except  for 

Pr,((pt  Apz 

/ KBI  AK&) 

=Pr,(cpt 

1 KB,)  xPr,(qop 

/ KB?). 

Proof.  Fix  N,  ?,  and  d  with 
Worlds$‘.*([) 

I  6  d  &  N.  Given  a  vocabulary 

p  containing 

c,  let 

consist  of  all  worlds  in  WN C ty 1 such  that  (w  ?)  /=  [  and  the  denotation 

of  L’ in  W  is  d.  and  let  #  \vorlds~i.“([) 

=  I+vorlds$i.y([)/.2h 

It  should  be  clear  that 

for  each  choice  of  d,  the  sets  world.~$i.‘” (5)  have  equal  size.  Thus,  #  worlds? 

(5)  = 

N#  world.$“*(,$). 

If51  is  a  formula 

in  @t  and  (2  is  a  formula 

in  @2, then  there  is  clearly 

a  bijection  between  worlds$i.@‘U’b’ (51  A &  )  and  worlds$i:@’  (4t  )  x  worlds;“@ 

It 
that  #  worlds”i.@1u@2 (51  A 52 )  = #  worlds;i’@” (51)  x  #  worlds$i,@D2 (&).  Since 

follows 

(52). 

#  worlds?([) 

= N#  torlds:‘,:i.“(<).  we  immediately  get 

#  \vorldsF’ 

(~1  A  KBl  )  x  #  worlds?’ 

((~2 A  KB2) 

= 

#  worlds$@  (KB,  )  x  #  worlds$@’ ( KB2 ) 

=Pr:*l(qt 

1 KB,)  x  PrF’(p2 

j KB2). 

x  Note  that  we  are  careful 

to  mention 

the  vocabulary  111 the  superscript  here,  rather  than  suppressing 

it  as  we 

have  up  to  now.  This  is  because  the  vocabulary  used  plays  a  significant 

role  in  this  proof. 

F:  Bacchus  ef  al./Arti$cial  Intelligence  87  (1996)  75-143 

141 

Taking 

limits,  we  get  that 

PrzU@P’(p,  A(P:!  [ KB!  AK&)  =Prz((o, 

1 KBl) 

.Prz(rp;? 

(KB2). 

As  observed 

in  [ 27 J,  for  all  formulas  p  and  KB,  if  @ _> CD’, then 

PrL(p 

1 KB)  =PrE(p) 

KB). 

(Intuitively, 
numerator 

this 

is  because 

the  effect  of  changing 

the  vocabulary 

cancels  out  in  the 

and  denominator.)  We  thus  get 

Pr,(ql 

A 402 I KBI  A K&j  =  Pr,(rpi 

I K&) 

.Pr,(e 

I K&j, 

as  desired. 

0 

References 

[ 11  E.  Adams,  The  Logic  of  Conditionals  (Reidel,  Dordrecht, 
121  N.  Asher,  Extensions 

sense  entailment, 

for  common 

1975). 

in:  Proceedings  IJCAI  Workshop on  Conditionals 

in  Knowledge  Representation  (1993)  26-41. 

[ 31  E  Bacchus,  Representing  and  Reasoning  with Probabilistic  Knowledge  (MIT  Press,  Cambridge,  MA, 

1990). 

[4  1 F.  Bacchus,  A.J.  Grove,  J.Y.  Halpem  and  D.  Koller,  From  statistics 

to  belief, 

in:  Proceedings  AAAI-92, 

San  Jose,  CA  (1992)  602-608. 

1  E  Bacchus,  A.J.  Grove, 

and  D.  Keller,  Statistical 

J.Y.  Halpem 
IJCAI-93,  Washington,  DC 

( 1993)  563-569; 

foundations 
available 

for  default 
by  anonymous 

reasoning, 
from 

ftp 

in:  Proceedings 
logos.uwaterloo.ca/pub/bacchus 
1 F.  Bacchus,  A.J.  Grove, 

or  via  WWW  at  http:/  /logos.uwaterloo.ca. 

J.Y.  Halpem 

and  D.  Koller,  Forming 

beliefs 

in:  Proceedings  AAAI-94,  Seattle,  WA 
logos.uwaterloo.ca/pub/bacchus 

or  via  WWW  at  http:/llogos.uwaterloo.ca. 

(1994) 

222-229; 

available 

[7]  E  Bacchus,  A.J.  Grove,  J.Y.  Halpem  and  D.  Koller,  Generating 

Tenth  Annual  Conference  on  Uncerkkty 
ftp  from 

logos.uwaterloo.ca/pub/bacchus 

new  beliefs  from  old,  in:  Proceedings 
in Artificial Intelligence  ( 1994)  37-45;  available  by  anonymous 

or  via  WWW  at  http:Illogos.uwaterloo.ca. 

[ 8 1 E  Bacchus,  A.J.  Grove,  J.Y.  Halpem  and  D.  Koller,  A  response 

to:  Believing  on  the  basis  of  evidence, 

Compur.  Intell.  10  (1994)  21-25. 

[9]  S.  Benferhat,  C.  Cayrol,  D.  Dubois,  J.  Lang  and  H.  Prade, 

Inconsistency  management 

and  prioritized 

syntax-based 

entailment, 
[ 101  C.  Boutilier,  Conditional 

in:  Proceedings  IJCAI-93,  Chambery 
logics 

for  default 

reasoning 

(1993)  640-645. 

and  belief  revision,  PhD  thesis,  Department 

of 

Computer  Science,  University  of  Toronto,  Toronto,  Ont.  ( 199 1). 

[ 111  R.  Camap,  Logical  Foundations  of  Probability 
I123 R.  Carnap,  The  Continuum  of  Inductive  Methods  (University  of  Chicago  Press,  Chicago, 
1131  PC.  Cheeseman,  A  method  of  computing 
in:  Proceedings  IJCAI-83,  Karlsruhe 

(University  of  Chicago  Press,  Chicago, 

generalized  Bayesian  probability 

198-202. 
[ 141  R.  Chuaqui,  Truth,  Possibility, nnd  Probability:  New Logical  Foundations  of Probabilify and  Starisrical 

values  for  expert  systems, 

IL,  1950). 

IL,  1952). 

(1983) 

Inference  (North-Holland,  Amsterdam, 

199 1) 

[ 1.51 J.P.  Delgrande,  An  approach 

to  default  reasoning  based  on  a  first-order  conditional 

logic:  revised  report, 

Artif:  Intell.  36  (1988)  63-90. 

[ 16 1 K.G.  Denbigh  and  J.S.  Denbigh,  Entropy  in  Relation  to Incomplete  Knowledge 

(Cambridge  University 

Press,  Cambridge, 
[ 171  D.W.  Etherington, 
(1991)  221-261. 

1985). 
S.  Kraus  and  D.  Perlis,  Nonmonotonicity 

and  the  scope  of  reasoning,  Artif.  Intell.  2 

about  a  changing  world, 
from 
by 

anonymous 

ftp 

I IX  I  D.  Gabbay.  Theoretical 

foundations 

lor  nonmonotonic 

Pnwerditlgs 
( Springer,  Berlin,  1984) 

of  rhe  NATO  Ad~w~c~rd  Stud, 

lnstiru~ 

in:  K.R.  Apt,  ed., 
reasoning 
ou  Logic.r  and  Models  of  Concurrent  Systems 

in  expert  systems. 

I I9  I  H.  Geffner.  Dqfimlt  Rectsorrirf~:,  (‘~rrrstri  trrld  (‘mtli~rowtrl  Theories 
1201 H.  Geffner  and  J.  Pearl.  A 

for  reasoning  with  defaults. 

framework 

(MIT  Press,  Cambridge,  MA,  1992). 

in:  H.E.  Kvburg  Jr,  R.  Loui  and 

G.  Carlson.  eds.,  Knou&d<q?  Rq~rerenrc~tior~  t/m/  Ikf~mihle 

Remming 

( Kluwer  Academic  Publishers, 

Dordrecht. 

1990)  145-265 

t-1. Gcffncr  and  J.  Pearl,  Condltronal 

entailment. 

bridging 

two  approaches 

to  default 

reasoning. 

Arr!f. 

lur  calculating  maximum  entropy  distributions,  Master’s  Thesis.  MIT 

I!rrcJ// 

53  ( 1992)  X39-244 
S.A  Goldman,  Eflicient  method\ 
EECS  Depamltent,  Cambridge..  MA  c 19X7 J 
Xl. Goldszmidt, 
/‘r-0(  c,<‘dr,l,qs AAAI-90.  Boston.  MA  C 1900~ 6-K-65.! 
hl. Goldszmidt.  P  Moms  and  J  Pearl.  A  maximum  entropy  approach 
Tniti\ 

/rite//.  15  I I993  I 770-232. 

P  MolTis  and  J.  Pearl.  A  max~n~um  entropy  approach 

,hro/.  Mtrch 

f’tr/wrrf 

to  nonmonotonic 

reasoning, 

in: 

to  nonmonotonic 

reasoning. 

IEEE 

S.L).  Goodwin.  Second  order  direct 

inference: 

:I  reference  class  selection  policy. 

/IX 

./.  Erper/  S~,FI. Rrs. 

~A/I/‘/.  5  (  1992)  185-2  10. 

A.J  Grove.  J.Y.  Halpern  and  I).  Keller,  Asgmptol~c  conditronal  probabilities: 

the  unary  case.  Research 

Kept.  RJ  9563, 

IBM 

(  1993);  a&o-  SIAM 

./.  CO~~/XI/.  (to appear). 
conditional 

A.J.  Grove.  J.Y.  Halpern  and 

Research  Rept.  RJ  9564. 

IBM 

I)  Keller.  Asymptotic 
( I993  ): also:  ./.  S~rnDo/ic~ Logic 

(to  appear). 

probabilities: 

the  non-unary 

case, 

4.5.  Grove,  J.Y.  Halpern  and  D  Keller,  Random  worlds  and  maximum  entropy,  J.  Artif 
( 1994)  33-88. 
I.  Hacking.  Tire  Enwrgenc~r  of  I’rrhbi/ir~ 

( Cambridge  University  Press,  Cambridge.  1975). 

/n/e//.  Res.  2 

J.Y.  Halpem.  An  analysis  of  tirst-order 

logics  of  probability,  Arr$ 

Intel/.  46  (  1990)  3 I  I-350. 

in  probabilistic 

inference. 

in:  Proceediqs 

UC/II- 

J.Y.  Halpem  and  D.  Keller.  Representation  dependence 
Y5.  Montreal,  Que.  ( 1995)  1853-l  860 
S.  Hanks  and  S.  McDermott, 

Nonmonotonic 

logic  and  temporal  projection,  Aroj. 

/ntr//.  33 

(1987) 

379-412. 

I>.  Hunter,  Causality  and  maxm1um  entropy  updating. 

E.T.  Jaynes.  Where  do  we  stand  on  maximum 

/nr.  J.  Apprm.  Retr.sonm~  3  ( 1989)  379-406. 
cnlropy?.  m:  R.D.  Levine  and  M.  Tribus.  eds.,  T/w 

Moximurrr  Entropy 

fi~rmu/ism 

( MIT  Press,  Cambridge,  MA,  1978) 

IS-  I  18. 

R.C.  Jeffrey,  Probable  knowledge, 

Scierlce:  The  f’wblenr 

oj’hducfi~v 

in: 
/H,~IC’  ( North-Holland, 

Amsterdam, 

1968) 

I S7-  185. 

I.  Lakatos.  ed..  /nremrrrioncd  Colloquium 

in  the  P/tr/o.w/~/~~  of 

W.E.  Johnson,  Probability: 

the  deductive  and  inductive  problems,  Mind  41  (  164) 

(  1932)  409-423. 

D.  Koller  and  J.Y.  Halpem, 

Irrelevance  and  conditioning 

in  first-order  probabilistic 

logic:  available  via 

WWW 

from  http://robotics.stanford.edu/daphne 

(38 I  Il.  Keller  and  J.Y  Halpem.  A  logic 

for  approximate 

reasoning, 

in:  B.  Nebel,  C.  Rich  and  W.  Swartout. 

on  Prir~c~iples  of  KamVedge  Represenmtior~ 

cm/ 

cds..  Procwdiugs 
‘Third 
Ren.wrrirlg.  Cambridge,  MA  ( 1992  )  I S3-  164. 
S.  Kraus,  D.  Lehmann  and  M.  Magidor,  Nonmonotonic 

/n/err~trtiontr/  Cmfererrw 

I391 

1401 

141  I 
1421 
1431 
1441 
1451 

1461 

reasoning.  preferential  models  and  cumulative 

logics.  Arf$ 

/n/e//.  44  (  1990)  167-207. 

H.E  Kyburg  Jr,  Prhrbi/if_v 

cmd  fhe  Im,yic  o/  Rnmnctl  Brlwj 

(Wesleyan  University  Press,  Middletown, 

CT,  1961). 

H.E.  Kyburg  Jr,  T//e  hj,qictr/ 
H.E.  Kyburg  Jr,  The  reference  class.  f’hilos.  Sci.  50  ( 1983)  374-397. 
H.E.  Kyburg  Jr.  Full  beliefs.  Theory  Lkisim 

fi~unrltrrio~~s  ((1 Smm/icrt/  hjereuce 

25  (  1988)  137-162. 

(Reidel,  Dordrecht, 

1974). 

L.D.  Landau,  Stchrictrl 

Ph_vsk.s  I  ( Pergamon.  Oxford,  1980). 

R.J.  Larsen  and  M.L.  Mark.  Arr  /n/roductiort 
Hall,  Englewood  Cliffs,  NJ,  I98  I ). 
Il.  Lehmann 
Proceedirrgs  Third  Conferewe 
CA  ( 1990)  57-72 

and  M.  Magidor.  Prefcrentlal 

fo  Mtrrhrr~mtic~nl  Sruristics  urld  i/s  Applic~chns 

(Prentice- 

IO&X\: 

the  predicate  calculus 

case, 

in:  R.  Parikh,  ed.. 

on  7%eoreticx/  A.s/wc~rs of  Rectsoning 

trbou~  Ktwvledge, 

Pacific  Grove. 

E  Bacchus  et  (11. /Artificial  Intelligence  87  (1996)  75-143 

143 

[471 

[481 

r491 
[501 

1511 

problems 

in  Artifificial 

Intellelligence 

and  E.  Sandewall, 

for  formal  non-monotonic 

reasoning,  version  2.00, in:  M.  Reinfrank, 

eds.,  Non-Monotonic  Reasoning:  2nd  International 

and  M.  Magidor,  What  does  a  conditional  knowledge  base  entail?,  Art8  Intell.  55  (1992) 

D.  Lehmann 
I-60. 
V.  Lifschitz,  Benchmark 
J.  de  Kleer,  M.L.  Ginsberg 
Workshop,  Lecture  Notes 
R.D. Lute  and  H.  Raiffa,  Games  and  Decisions  (Wiley,  New  York,  1957). 
D.  Makinson,  General 
E.  Sandewall, 
lntellelligence 
D.  Makinson,  General  patterns 
in:  D.  Gabbay,  C.  Hogger  and  J.  Robinson, 
eds.,  Handbook  of  Logic  in Artificial Intelligence  cmd Logic  Programming  3  (Oxford  University  Press, 
Oxford,  1994)  35-l  10. 
J.  McCarthy,  Circumscription-a 
J.  McCarthy,  Applications 

J.  de  Kleer,  M.L.  Ginsberg  and 
eds.,  Non-Monotonic  Reasoning:  2nd  International  Workshop, Lecture  Notes  in  Artificial 
346  (Springer,  Berlin,  1989) 

reasoning,  Artif  Intell.  13  ( 1980)  27-39. 

346  (Springer,  Berlin,  1989)  202-2  19. 

knowledge,  Artif  Intell.  28 

form  of  non-monotonic 

theory  of  cumulative 

of  circumscription 

in:  M.  Reinfrank, 

in  nonmonotonic 

to  formalizing 

common-sense 

reasoning, 

inference, 

I-  18. 

and  PJ.  Hayes,  Some  philosophical 

(1986)  86-l  16. 
intelligence, 
J.  McCarthy 
in:  B.  Meltzer  and  D.  Michie,  eds.,  Machine  Intelligence  4  (Edinburgh  University  Press,  Edinburgh, 
1969)  463-502. 
R.C.  Moore,  Semantical 
M.  Morreau,  The  conditional 

logic,  Art8  Intell.  25  ( 1985)  75-94. 
in:  Proceedings  of  the  IJCAI  W~~rksh~~p on 

on  nonmonotonic 
logic  of  generalizations, 

from  the  standpoint  of  artificial 

considerations 

problems 

Conditionals  in  Knowledge  Representation  ( 1993)  108-I  18. 
J.B.  Paris  and  A.  Vencovska,  On  the  applicability 

of  maximum 

1571 

entropy 

to  inexact 

reasoning, 

Int.  .I. 

semantics 

for  nonmonotonic 

Appron.  Reasoning  3  ( 1989)  l-34. 
1.  Pearl,  Probabilistic  Reasoning  in  Intelligent  Systems  (Morgan  Kaufmann,  San  Francisco,  CA,  1988). 
I.  Pearl,  Probabilistic 
in:  R.J.  Brachman,  H.J. 
Levesque  and  R.  Reiter,  eds.,  Proceedings  First  International  Conference  on  Principles  of  Knowledge 
Representation  and  Reasoning,  Toronto,  Ont. 
in:  G.  Shafer  and  I.  Pearl, 
eds.,  Readings  in  Uncertain  Reasoning  (Morgan  Kaufmann,  San  Francisco,  CA,  1990)  699-710. 
J.  Pearl,  System  Z:  a  natural  ordering  of  defaults  with  tractable  applications 
in:  M.  Vardi,  ed.,  Proceedings  Third  Conference  on  Theoreticul  Aspects  of  Reasoning  about  Knowledge, 
Pacific  Grove,  CA  (1990) 
J.L.  Pollock,  Noniic  Probabilities  and  the  Foundations  of  Induction  (Oxford  University  Press,  Oxford, 
1990). 

(1989)  505-516;  Reprinted 

to  nonmonotonic 

a  survey, 

reasoning: 

reasoning, 

121-135. 

1521 
1531 

1541 

[551 
1561 

[581 
[591 

[GO 

161 

[ 621  D.  Poole,  What  the  lottery  paradox 

in:  R.J.  Brachman,  H.J.  Levesque  and 
R.  Reiter,  eds.,  Proceedings  First  International  Conference  on  Principles  of  Knowledge  Representation 
und  Reasoning,  Toronto,  Ont.  ( 1989)  333-340. 

tells  us  about  default  reasoning, 

[ 63 1 D.  Poole,  The  effect  of  knowledge  on  belief:  conditioning, 

specificity  and  the  lottery  paradox 

in  default 

reasoning,  Artif  Intell.  49  (1991)  282-307. 

[ 641  H.  Reichenbach,  Theory  of  Probability  (University  of  California  Press,  Berkeley,  CA,  1949) 
[65]  R.  Reiter,  A  logic  for  default 
[66]  R.  Reiter  and  G.  Criscuolo,  On  interacting  defaults, 

reasoning,  Artif  Intell.  13  (1980)  81-132. 

in:  Proceedings  IJCAI-RI,  Vancouver,  BC  ( 1981) 

270-276. 

I67  1 L.J.  Savage,  Foundations  of  Statistics  (Wiley,  New  York,  1954) 
168  I  G. Shafer,  A  Mathematical  Theory  of  Evidence  (Princeton  University  Press,  Princeton,  NJ,  1976). 
(691  G.  Shafer,  Personal  communication, 
[ 701  C.  Shannon  and  W.  Weaver,  The  Mathematicul  Theory  of  Communication  (University  of  Illinois  Press, 

1993. 

Champaign, 

IL,  1949). 

171  1 L.  Shastti,  Default  reasoning 
Intell.  39  (1989)  285-355. 

in  semantic  networks:  a  formalization 

of  recognition 

and  inheritance,  Artif 

I72  I  D.S.  Touretzky,  J.F.  Horty  and  R.H.  Thomason,  A  clash  of  intuitions: 

the  current  state  of  nonmonotonic 

multiple 

inheritance 

systems, 

in:  Proceedings  IJCAI-87,  Milan  ( 1987)  476-482. 

