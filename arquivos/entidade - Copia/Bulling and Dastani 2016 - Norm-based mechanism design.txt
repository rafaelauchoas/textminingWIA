Artiﬁcial Intelligence 239 (2016) 97–142

Contents lists available at ScienceDirect

Artiﬁcial  Intelligence

www.elsevier.com/locate/artint

Norm-based  mechanism  design
Nils Bulling a,∗
a Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlands
b Intelligent Systems Group, Utrecht University, Utrecht, The Netherlands

,  Mehdi Dastani b

a  r  t  i  c  l  e 

i  n  f  o

a  b  s  t  r  a  c  t

Article history:
Received 21 September 2015
Received in revised form 4 July 2016
Accepted 7 July 2016
Available online 14 July 2016

Keywords:
Norms
Multi-agent systems
Mechanism design

The  increasing  presence  of  autonomous  (software)  systems  in  open  environments  in 
general,  and  the  complex  interactions  taking  place  among  them  in  particular,  require 
ﬂexible control and coordination mechanisms to guarantee desirable overall system level 
properties without limiting the autonomy of the involved systems. In artiﬁcial intelligence, 
and  in  particular  in  the  multi-agent  systems  research  ﬁeld,  social  laws,  norms,  and 
sanctions have been widely proposed as ﬂexible means for coordinating the behaviour of 
autonomous agents in multi-agent settings. Recently, many languages have been proposed 
to specify and implement norm-based environments where the behaviour of autonomous 
agents  is monitored,  evaluated  based  on  norms,  and  possibly  sanctioned  if  norms  are 
violated.
In  this  paper,  we  ﬁrst  introduce  a  formal  setting  of  multi-agent  environments  based 
on  concurrent  game  structures  which  abstracts  from  concrete  speciﬁcation  languages. 
We  extend  this  formal  setting  with  norms  and  sanctions,  and  show  how  concepts  from 
mechanism design can be used to formally analyse and verify whether a speciﬁc behaviour 
can be enforced (or implemented) if agents follow their subjective preferences. We relate 
concepts from mechanism design to our setting, where agents’ preferences are modelled 
by linear time temporal logic (LTL) formulae. This proposal bridges the gap between norms 
and  mechanism  design  allowing  us  to  formally  study  and  analyse  the  effect  of  norms 
and sanctions on the behaviour of rational agents. The proposed machinery can be used 
to  check  whether  speciﬁc  norms  and  sanctions  have  the  designer’s  expected  effect  on 
the  rational  agents’  behaviour  or  if  a  set  of  norms  and  sanctions  that  realise  the  effect 
exists at all. We investigate the computational complexity of our framework, focusing on 
its implementation in Nash equilibria and we show that it is located at the second and 
third level of the polynomial hierarchy. Despite this high complexity, on the positive side, 
these results are in line with existing complexity results of related problems. Finally, we 
propose a concrete executable speciﬁcation language that can be used to implement multi-
agent environments. We show that the proposed speciﬁcation language generates speciﬁc 
concurrent game structures and that the abstract multi-agent environment setting can be 
applied  to  study  and  analyse  the  behaviour  of  multi-agent  programs  with  and  without 
norms.

© 2016 Elsevier B.V. All rights reserved.

* Corresponding author.

E-mail addresses: n.bulling@tudelft.nl (N. Bulling), M.M.Dastani@uu.nl (M. Dastani).

http://dx.doi.org/10.1016/j.artint.2016.07.001
0004-3702/© 2016 Elsevier B.V. All rights reserved.

98

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

1.  Introduction

The  emergence  of  autonomous  software  systems  and  their  increasing  number  of  interactions  with  open  environments 
such  as  the  Internet,  ﬁnancial  markets,  large  ICT  systems,  socio-technical  systems  and  industrial  platforms,  urgently  re-
quires  ﬂexible  control  and  coordination  mechanisms  in  order  to  guarantee  desirable  overall  system  level  properties.  This 
urgency became painfully clear in 2010 by the so-called “Flash Crash”, where the uncontrolled and uncoordinated interac-
tions between high frequency algorithmic trading systems in ﬁnancial environments have led to extraordinary upheaval of 
U.S. equity markets [63].  Similar urgency is experienced in large ICT systems of organisations such as banks or insurance 
companies, where due to competition and innovation business processes have to be modiﬁed in rapid tempo. Such contin-
uous changes constitute a potential threat for business processes to become non-compliant with the companies’ business 
rules and policies. There is, therefore, an increasing need for ﬂexible control and supervision mechanisms that could ensure 
the compliance of business processes in such dynamic environments without limiting the functionality and performance of 
the business processes [47,25,49]. In addition to these existing cases, the rapid development of autonomous cars strongly 
suggests that future traﬃc will be populated by intelligent autonomous cars that interact in shared (physical) environments 
called  smart  roads.  In  order  to  guarantee  the  safety  and  throughput  of  such  smart  road  environments,  the  behaviour  of 
autonomous cars has to be regulated by intelligent monitoring and coordination mechanisms without directly steering their 
behaviour [40,38]. These and other applications show the urgency of tools and techniques to design, develop and analyse 
intelligent ﬂexible control and coordination mechanisms.

These and many other applications can be best modelled as multi-agent systems. A multi-agent system consists of inter-
acting computer systems that are situated in some environment and are capable of autonomous actions in the environment 
in  order  to  meet  their  delegated  objectives [68].  The  individual  agents  are  generally  assumed  to  be  heterogeneous  in  the 
sense that they may be designed and developed by various parties, using different technologies, and pursuing different ob-
jectives. It is exactly the autonomous and heterogeneous character of the computer agents in multi-agent applications that 
require intelligent ﬂexible control and coordination mechanisms. In general, any control and coordination mechanism used 
in a multi-agent application should strike a balance between the autonomy of the agents on the one hand and the desirable 
global properties of the multi-agent system on the other hand, i.e., while the autonomy of agents should be respected, the 
global properties of multi-agent systems should be ensured.

Existing  coordination  techniques  in  computer  science,  such  as  synchronization  techniques  or  interaction  protocols,  can 
be used to ensure the overall desirable properties of the interacting systems. However, these techniques will severely limit 
the  autonomy  and  intelligence  of  the  involved  systems [29].  In  artiﬁcial  intelligence,  norms  and  norm  enforcement  have 
been widely proposed as ﬂexible and effective means for coordinating the behaviour of autonomous agents in multi-agent 
systems [50,45,14]. Singh et al. [62] provide an overview of various uses and applications of norms in multi-agent systems, 
and  Criado  et  al.  [26] discuss  some  challenges  and  open  issues  concerning  representation,  reasoning,  creation  and  imple-
mentation of norms in multi-agent systems. A multi-agent system that uses norms to coordinate the behaviour of agents is 
often called norm-based multi-agent system or normative multi-agent system.

In general, there are two approaches to exploit norms for coordination purposes in multi-agent systems. Norms can be 
either endogenous to agents in the sense that they form an integral part of the agents’ speciﬁcations [59,60], or exogenous to 
agents in the sense that they are enforced by some external regulatory mechanism [2,27]. Each approach comes with speciﬁc 
assumptions and applications. For example, the endogenous approach assumes that norms are internalised by the agents in 
the  sense  that  the  agents’  decision  making  mechanisms  are  designed  and  developed  based  on  a  given  set  of  norms.  This 
assumption implies that norms are available at design time and enforced on agents by the agents’ developers at design time. 
The endogenous approach can, for example, be used to examine the (emergent) effects of norms in agent-based simulations 
[59,60,48,6].  In  contrast,  the  exogenous  approach  is  agnostic  about  norm  internalisation,  but  assumes  an  authority  that 
monitors agents’ behaviour and enforces norms by means of preventing norm violations or imposing sanctions on violating 
behaviour  [65,2,44,57].  Following  the  exogenous  approach,  autonomous  systems  may  respect  norms  or  decide  to  violate 
them in which case they may incur sanctions. It is exactly the incursion of sanctions that may incentivize, but not restrict, 
agents  to  behave  in  a  particular  way.  The  exogenous  approach  is  also  conceived  as  a  way  to  make  the  engineering  of 
multi-agent  systems  easier  to  manage  as  it  supports  the  general  principle  of  ‘separation  of  concerns’,  i.e.,  it  supports  the 
encapsulation of coordination and control concerns in a system entity that is separable from the agents’ internals [10,17,70]. 
In addition, the external authority can be conceived as a mechanism that is designed to implement norms and the norm 
enforcement process. This perspective on norms and norm enforcement allows us to apply formal tools from game theory 
and to study and analyse norms and norm enforcement from a mechanism design perspective [18,19,30,69].

This paper follows the exogenous approach and provides a mechanism design perspective on norms and norm enforce-
ment. A multi-agent environment is modelled as a concurrent game structure where possible paths in the game structure 
denote possible execution traces of the corresponding multi-agent system. By considering the states of the game structure 
as the states of the environment in which agents operate, the concurrent game structure becomes the representation of a 
mechanism (game form) that speciﬁes the effect of the agents’ actions on the multi-agent environment. The enforcement 
of  norms  on  a  multi-agent  system  may  change  the  effect  of  the  agents’  actions  on  the  environment  and  thereby  its  un-
derlying concurrent game structure and the set of possible execution traces. We call the modiﬁcation of concurrent game 
structure by norm enforcement norm-based update. Clearly, various sets of norms can be used to update a concurrent game 
structure.  The  decision  regarding  which  set  of  norms  to  use  depends  on  the  behaviour  of  the  agents  which  in  turn  is  a 

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

99

result of the agents’ preferences, and, of course, the intended objective of the multi-agent system. We introduce norm-based 
mechanism design as  a  formal  methodology  to  analyse  norms  and  norm-based  updates,  and  to  ﬁnd  suitable  norms  taking 
into consideration agents’ rational behaviour.

We  aim  at  bridging  the  gap  between  norms  and  mechanism  design  by  focusing on  the  relation  between  multi-agent 
systems, norm enforcement, and concurrent game structures. This relation sets the stage for studying formal properties of 
norm enforcement such as whether enforcing a set of norms implements a speciﬁc social choice function, which describes 
the  desired  system  executions  in  relation  to  the  agents’  preferences,  in  speciﬁc  equilibria.  This  also  allows  us,  for  exam-
ple,  to  analyse  whether  a  group  of  agents  is  willing  to  obey  some  norms.  The  formal  analysis  is  closely  related  to  work 
presented  in [1] and [65],  where  the  enforcement  of  norms  is  modelled  by  the  deactivation  of  transitions.  Adding  norms 
to environments in our model may, however, either deactivate transitions in the case of regimenting norms or change the 
effect of actions in the case of sanctioning norms. The latter is achieved by (re)labelling states with new propositions that 
encode, for example, sanctions or incentives which can affect the behaviour of agents. Our work is also motivated by [59]
and [60], where social laws were proposed to be used in computer science to control agents’ behaviour.

The proposed norm-based mechanism design is an abstract methodology as it assumes concurrent game structures as 
models of multi-agent environments. In order to ground this methodology, we design an executable speciﬁcation language 
to  support  the  development  of  norm-based  multi-agent  systems.  The  aim  of  the  executable  speciﬁcation  language  is  to 
facilitate the implementation of multi-agent environments that are governed by norms. Multi-agent environments without 
norms  are  often  implemented  using  a  computational  speciﬁcation  language  that  provides  constructs  to  specify  (initial) 
environment states and actions. The execution of such an environment speciﬁcation is a process that continuously observes 
agents’ actions and updates the environment state based on the speciﬁcation of the observed actions. The implementation 
of  multi-agent  environments  with  norms  requires  additional  constructs  to  specify  norms  and  sanctions.  The  execution  of 
an  environment  speciﬁcation  with  norms  includes  two  additional  steps  through  which  the  observed  agents’  actions  are 
further evaluated based on the given set of speciﬁed norms after which norm violations are either prevented or sanctioned. 
Our abstract norm-based mechanism design methodology can be applied to norm-based multi-agent environment programs 
in  order  to  analyse  whether  the  enforcement  of  a  set  of  norms  by  a  norm-based  environment  program  can  implement  a 
speciﬁc  social  choice  function.  We  also  investigate  the  complexity  of  verifying  whether  a  given  norm-based  multi-agent 
system implements a given system speciﬁcation. Besides the veriﬁcation problem we analyse the decision problem whether 
a norm-based multi-agent system with desirable properties exists at all. In terms of complexity our results are negative, in 
the sense that the problems are intractable. On the positive side however, the results are in line with existing complexity 
results of related problems.

The novel contribution of this work is a formal methodology for analysing norms and norm enforcement used for coordi-
nating the behaviour of rational agents. This is realised by applying game theory/mechanism design techniques to study the 
effects of logic-based norms on multi-agent systems. We ground the theoretical analysis in an executable multi-agent setting 
to allow the development of norm-based coordination mechanisms for multi-agent systems. This paper extends and revises 
the  work  presented  in  [19].  The  idea  of  norm-based  mechanism  design  was  ﬁrst  proposed  in  the  extended  abstract [18]. 
In comparison with [19] we signiﬁcantly revise the formal setting, add new complexity results to new decision problems 
(weak  and  strong  implementability)  and  give  full  proofs.  In  addition,  the  executable  speciﬁcation  language  as  well  as  the 
related work section and a running example are new. The design of the executable speciﬁcation language is inspired by the 
programming  languages  proposed  in [28] and [27] for  which  an  interpreter,  called  2OPL  (Organisation-Oriented  Program-
ming Language), has been developed.1 One of the main differences between these languages and our proposed executable 
speciﬁcation language is the representation of norms. While norms in [28] and [27] are state-based (i.e., norms represent 
prohibited states), the proposed speciﬁcation language in this paper considers conditional action-based norms (i.e., norms 
represent prohibited actions in speciﬁc states).

The structure of this paper is as follows. First, Section 2 presents concurrent game structures as models for multi-agent 
environments and connects it to game theory. Section 3 introduces a speciﬁcation language for norm-based multi-agent en-
vironments, extends the formal setting of environments with norms and sanctions, and introduces the concept of norm-based 
mechanism design. Section 4 investigates the complexity of two implementation problems. Section 5 grounds the proposed 
approach by linking it to a norm-based multi-agent environment programming language. Finally, related work is discussed 
and some conclusions are drawn. The formal proofs about the computational complexity can be found in the appendix.

2.  Multi-agent environment model

In  order  to  illustrate  our  proposal,  we  shall  use  the  following  example  throughout  the  paper.  The  example  is  closely 
related  to  the  well-known  train-gate  controller  example  presented  by [9].  Though,  we  slightly  modify  it  to  highlight  the 
rational, decentralized decision making aspect of the car drivers.

Example 1 (Narrow road example). The scenario consists of a narrowed road and two cars at the opposite ends of the road. 
The cars cannot simultaneously pass through the narrowed road. In this scenario, both cars can wait for each other forever, 

1 http :/ /oopluu .sourceforge .net/.

100

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Fig. 1. Road scenario.

both  can  move  simultaneously  causing  a  congestion/crash  at  the  middle  of  the  road,  or  one  can  move  while  the  other  is 
waiting. There are two alternatives for the latter case: either the ﬁrst car moves while the second is waiting or vice versa. 
The  system  designer  prefers  one  of  these  two  alternatives.  In  order  to  realise this  behaviour,  the  system  designer  aims 
at  synthesizing  a  set  of  norms  and  corresponding  sanctions  such  that  when  they  are  enforced  the  behaviour  of  the  cars 
satisﬁes the given (system) preference assuming that the drivers’ behaviours are driven by their own individual preferences. 
indicates that car  i is at position  x ∈ {s, m, e} (s: start position, 
This scenario is illustrated in Fig. 1, where proposition  px
i
m: middle position, e: end position). We assume the following states in this scenario:

• q0: the cars are at their starting positions, i.e.,  ps
1
• q1: car 1 is at its ending and car 2 is at its starting position, i.e.,  pe
∧ ps
1
• q2: car 1 is at its starting and car 2 is at its ending position, i.e.,  ps
∧ pe
1
• q3: the cars are congested in the middle of the road, i.e.,  pm
∧ pm
2 holds
1
• q4: the cars are at their ending positions, i.e.,  pe
2 holds
1

2 holds

∧ pe

∧ ps

2 holds
2 holds

2.1.  Concurrent structures and strategies

In  the  following  we  introduce  concurrent game structures (CGSs) from [9] (modulo  minor  modiﬁcations).  They  serve  as 
models  for  our  formal  analysis  of  the  environment  in  multi-agent  systems.  An  environment  in  a  multi-agent  system  is 
assumed  to  be  speciﬁed  in  terms  of  a  set  of  states,  possibly  including  an  initial  state,  and  a  set  of  (synchronized  and 
concurrent) transitions. Informally speaking, a CGS is given by a labelled transition system where transitions are activated 
by action proﬁles.

Deﬁnition 1 (CGS, pointed). A concurrent game structure (CGS) is a tuple M = (Agt, Q, (cid:2), π , Act, d, o), comprising a nonempty 
ﬁnite  set  of  all  agents  Agt = {1, . . . , k},  a  nonempty  ﬁnite  set  of  states  Q ,  a  nonempty  ﬁnite  set  of  atomic  propositions 
(cid:2) (also  called  propositional  symbols)  and  their  valuation  π : Q → P((cid:2)),  and  a  nonempty  ﬁnite  set  of  (atomic)  actions 
Act.  Function  d : Agt × Q → P( Act)\{∅} deﬁnes  nonempty  sets  of  actions  available  to  agents  at  each  state,  and  o is  a 
(cid:7) = o(q, (α1, . . . , αk)) to  state  q and  a  tuple  of  actions 
(deterministic)  transition  function  that  assigns  the  outcome  state  q
(α1, . . . , αk) with αi ∈ d(i, q) and 1 ≤ i ≤ k, that can be executed by Agt in q. A pointed CGS is given by (M, q) where M is 
a CGS and q is a state in it.

In the following, we write di(q) instead of d(i, q) and o(q, (cid:9)α) instead of o(q, (α1, . . . , αk)) for  (cid:9)α = (α1, . . . , αk). In CGSs it 
is assumed that all the agents execute their actions synchronously.2 The combination of actions together with the current 
state determines the next transition of the system.

Example 2 (CGS). Our scenario from Example 1 is formally modelled by CGS M1 = (Agt, Q, (cid:2), π , Act, d, o), shown in Fig. 2, 
where

• Agt = {1, 2},
• Q = {q0, . . . , q4},
• (cid:2) = {px
i
• Act = {M, W },
• the function d is deﬁned as

| i ∈ {1, 2} and x ∈ {s, m, e}},

2 We note that the framework allows to model turn-based games as a special case.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

101

Fig. 2. The CGS M1 modelling the scenario from Example 1. Nodes represent system states, bold symbols qi assigned to nodes are state names, and 
elements px

i within a node are atomic propositions that hold in the state.

d1(q0) = {W , M} d2(q0) = {W , M}
d1(q1) = {W }
d2(q1) = {W , M}
d1(q2) = {W , M} d2(q2) = {W }
d2(q3) = {W }
d1(q3) = {W }
d2(q4) = {W }
d1(q4) = {W }
• π and o are deﬁned as illustrated in Fig. 2, e.g. o(q0, (M, M)) = q3.

We note that agent 1 and 2 have no choice other than to wait (W ) in states {q1, q3, q4} and states {q2, q3, q4}, respectively.

In the following we assume that a CGS M = (Agt, Q, (cid:2), π , Act, d, o) is given, if not said otherwise. A strategy of agent i is 
a conditional plan that speciﬁes what i is going to do in each situation. It makes sense, from a conceptual and computational 
point of view, to distinguish between two types of “situations” (and hence strategies): an agent might base its decision only 
on the current state or on the whole history of events that have happened. In this paper we only consider the former type 
of strategies which are often called memoryless or positional. In general, memoryless strategies enjoy better computational 
properties  than  the  second  type  of  strategies  called  perfect-recall  strategies.  Note  that  ‘memoryless’  sounds  more  severe 
than it actually is: an agent is still able to base decisions on the current state; thus, ﬁnite histories up to an arbitrary but 
ﬁxed length could be encoded within states.

Deﬁnition 2 (Strategy). A (memoryless) strategy for agent i is a function si : Q → Act such that si(q) ∈ di(q). The set of such 
strategies  is  denoted  by  (cid:5)i .  A  collective strategy for  a  group  of  agents  A = {i1, . . . , ir} ⊆ Agt is  a  tuple3 s A = (si1 , . . . , sir )
where each si j , 1 ≤ j ≤ r, is a strategy for agent i j . The set of  A’s collective strategies is given by (cid:5) A =
i∈ A (cid:5)i . The set of 
all (complete) strategy proﬁles is deﬁned as (cid:5) = (cid:5)Agt.

(cid:2)

Given  the  notation  above,  (s1, s2) is  a  collective  strategy  of  agents  1 and  2.  A  path λ = q0q1 . . . ∈ Qω is  an  inﬁnite 
sequence of states such that for each  j = 0, 1, . . . there is an action tuple  (cid:9)α ∈ dAgt(q j) with o(q j, (cid:9)α) = q j+1. The set of all 
paths starting in state q is denoted by (cid:8)M(q). We write λ[ j] to refer to the  j-th state on path λ, and λ[ j, ∞] to refer to the 
(sub)path q jq j+1 . . . of λ. Function outM(q, s) returns the unique path that occurs when the agents execute (the complete) 
strategy proﬁle s from state q onward.

Deﬁnition 3 (Outcome). The outcome outM(q, s) of a complete strategy proﬁle s = (s1, . . . , sk) from state q in model M is the path 
λ = q0q1q2 . . . such that q0 = q and for each  j = 0, 1, 2, . . . there is an action tuple  (cid:9)α j = (α j
= si(q j) for 
every i ∈ Agt, and o(q j, (cid:9)α j) = q j+1. Often, we will omit subscript “M” from outM(q, s) if clear from context.

k ) with α j

1, . . . , α j

i

The following example illustrates how strategies in our example scenario can be represented.

3 We assume some implicit ordering among the agents to obtain a unique representative of a strategy proﬁle.

102

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Example 3 (Strategies). In the CGS from Example 2, we encode individual strategies by a 5-tuple prescribing an action for 
each state. However, as agent 1 (resp. agent 2) has no choice other than to wait (W ) in states q1, q3, and q4 (resp. q2, q3, 
and q4), we encode strategies by a tuple  xy, indicating that agent  1 (resp. agent  2) executes actions  x in q0 and  y in q2
(resp. x in q0 and  y in q1). The strategy M W for agent 1 selects M in state q0 and W in state q2 (and also W for states q1, 
q3, and q4). Each agent has therefore 4 strategies resulting in a total of 16 different strategy proﬁles. Note that a strategy 
deﬁnes  an  action  for  all  possible  states  even  those  not  reachable  if  the  current  strategy  is  implemented.  Therefore,  some 
strategy proﬁles have identical outcome paths. For example, all strategies proﬁles in the set {(M(cid:9)1, M(cid:9)2)  | (cid:9)1, (cid:9)2 ∈ {W , M}}
result in the path q0qω
3 .

2.2.  Agents’ preferences

In the behavioural analysis of multi-agent systems, preferences of agents are often of utmost importance. They are the 
driving force of agents’ behaviour. In the models we are considering, the agents’ preferences are deﬁned on the executions 
of the environment and thus paths in the corresponding CGS. Hence, we assume that agents prefer some executions over 
others. Therefore, we use temporal formulae to describe sets of paths. This idea was already followed in several pieces of 
work for similar purposes, e.g. [1] used CTL to represent agents’ preferences and [23,21] used ATL for the same purpose, 
where [65] used ATL for the representation of the objective of the social law. In this paper we use linear temporal logic LTL, 
ﬁrst proposed by [56] for the veriﬁcation of programs, for modelling preferences of agents. The logic extends propositional 
logic with operators that allow to express temporal patterns over inﬁnite sequences of sets of propositions. It allows to ex-
press natural properties related to safety, liveness and fairness properties and combinations thereof. As we aim at evaluating 
system  paths,  which  are  inﬁnite  sequences  of  states,  we  need  a  logic  which  allows  to  compare  paths  rather  than  a  logic 
the  formulae  of  which  are  evaluated  in  states.  This  makes  LTL a  more  natural  choice  than,  e.g.,  CTL and  ATL.  The  basic 
temporal operators are U (until), (cid:2) (always), (cid:3) (eventually) and (cid:12) (in the next state). As before, propositions, drawn from a 
ﬁnite and non-empty set (cid:2), are used to describe properties of states.

Deﬁnition 4  (Language LTL).  Let  (cid:2) be  a  ﬁnite,  non-empty  set  of  propositions.  The  formulae of LTL(cid:2) are  generated  by  the 
following  grammar,  where  p ∈ (cid:2) is  a  proposition:  ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | ϕUϕ | (cid:12)ϕ.  For  convenience,  we  deﬁne  the  two 
temporal operators (cid:3)ϕ and (cid:2)ϕ as macros (cid:13)Uϕ and ¬(cid:3)¬ϕ, respectively, where (cid:13) ≡ p ∨ ¬p denotes universal truth. We 
will omit the set of propositions “(cid:2)” as subscript of LTL(cid:2) if clear from context.

LTL-formulae  are  interpreted  over ω-sequences4 (inﬁnite  words)  w over  sets  of  propositions,  i.e.  w ∈ P((cid:2))ω. We  use 
the  same  notation  for  words  w as  introduced  for  paths  λ,  e.g.  w[i] and  w[i, ∞].  We  note  that  a  model  M and  a  path 
λ in M—more precisely the valuation function included in the model—induce such an ω-sequence. However, we give the 
semantics in more general terms because it shall later prove convenient when relating the setting to mechanism design.

Deﬁnition  5  (Semantics  |=LTL).  Let  (cid:2) be  a  ﬁnite,  non-empty  set  of  propositions  and  w ∈ P((cid:2))ω.  The  semantics  of 
LTL(cid:2)-formulae is given by the satisfaction relation |=LTL deﬁned by the following cases:

w |=LTL p iff p ∈ w[0] and p ∈ (cid:2);
w |=LTL ¬ϕ iff not  w |=LTL ϕ (we write  w (cid:16)|=LTL ϕ);
w |=LTL ϕ ∧ ψ iff  w |=LTL ϕ and  w |=LTL ψ ;
w |=LTL (cid:12)ϕ iff  w[1, ∞] |=LTL ϕ; and
w |=LTL ϕU ψ iff there is an i ∈ N0 such that  w[i, ∞] |= ψ and  w[ j, ∞] |=LTL ϕ for all 0 ≤ j < i.

Given a path λ and a valuation π we also write π (λ) for π (λ[0])π (λ[1]) . . . . Moreover, we just write λ |= ϕ if the model 
and its valuation function are clear from context. Similarly, we usually omit LTL in |=LTL.

Example 4 (Preference). In our scenario, we assume that car 1 has the preference ps
1
next state to be its ending state (i.e., it prefers to pass through the narrow passage as the ﬁrst car). The path q0qω
the preference, we have: π (q0qω

1 for π being the valuation function of M1 from Example 2.

1 modelling that it wants the 
2 violates 

→ (cid:12)pe

2 ) (cid:16)|= ps

∧ ps
2

1

→ (cid:12)pe

∧ ps
2

We  use  preference  lists  to  deﬁne  preferences  of  agents  [23].  Such  a  list  consists  of  a  sequence  of  LTL-formulae  each 
coupled with a natural number. The formula is a binary classiﬁer of paths in the model—considering the induced ω-words 
over propositions. The natural number assigns a utility to the paths which satisfy the respective formula. Thus, a preference 
list assigns a utility value to all paths in a model.

4 We could also give a semantics over paths in a given model. We use ω-sequences over sets of propositions as it makes the semantics independent of 
the structure of a speciﬁc model which shall prove useful when relating the setting to mechanism design in Section 3.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

103

Deﬁnition 6 (Preference list, preference proﬁle). A preference list over a set of propositions (cid:2) of an agent i ∈ Agt = {1, . . . , k}
is given by a ﬁnite sequence γi = ((ϕ1, u1), . . . , (ϕl−1, ul−1), (ϕl, ul)) where each ϕ j ∈ LTL(cid:2), each u j ∈ N for  j = 1, . . . , l − 1, 
ϕl = (cid:13), and ul = 0. A preference proﬁle over (cid:2) and Agt is given by  (cid:9)γ = (γ1, . . . , γk) containing a preference list over (cid:2) for 
each agent in Agt. We typically use Prefs to denote a non-empty set of such preference proﬁles. We omit mentioning the 
set of propositions (cid:2) and the set of agents Agt, respectively, if clear from context.

A preference list of an agent may be interpreted as the agent’s goals, denoting the behaviours that the agent wants to 
realise. Given an agent i ∈ Agt with preference list γ = ((ϕ1, u1), . . . , (ϕl, ul)), a word w is assigned utility u j (to denote the 
utility of outcome path λ with π (λ) = w for agent i) if ϕ j is the formula with the smallest index  j in γ that is satisﬁed 
on  w, i.e.,  w |= ϕ j and for all l < j we have that  w (cid:16)|= ϕl. Note that there is always a formula in γ which is satisﬁed on  w
since the last formula in a preference list is required to be (cid:13).

Example 5  (Preference proﬁles).  In  our  scenario,  we  consider  the  following  two  preference  proﬁles  (cid:9)γ1 and  (cid:9)γ2 of  the  cars 
where proposition ﬁnei should be read as agent i has received a ﬁne.

(cid:9)γ1 = {(((cid:12)pe
∧ (cid:2)¬ﬁne1, 3), ((cid:3)pe
∧ (cid:2)¬ﬁne1, 2), ((cid:2)¬ﬁne1, 1), ((cid:13), 0)),
1
1
∧ (cid:2)¬ﬁne2, 2), ((cid:2)¬ﬁne2, 1), ((cid:13), 0)) }
∧ (cid:2)¬ﬁne2, 3), ((cid:3)pe
(((cid:12)pe
2
2

(cid:9)γ2 = {(((cid:12)(pe
∧ pe
1
∧ pe
(((cid:12)(pe
1

2) ∧ (cid:2)¬ﬁne1, 3), ((cid:12)pe
2) ∧ (cid:2)¬ﬁne2, 3), ((cid:12)pe

2

∧ (cid:2)¬ﬁne1, 2), ((cid:2)¬ﬁne1, 1), ((cid:13), 0)),
1
∧ (cid:2)¬ﬁne2, 2), ((cid:2)¬ﬁne2, 1), ((cid:13), 0)) }

The  ﬁrst  preference  proﬁle  is  egoistic  in  the  sense  that  the  cars  ﬁrst  prefer  to  reach  their  end  positions  directly  without 
any sanction, then eventually to get to their end positions without any sanction, and ﬁnally to get no sanction. The second 
preference proﬁle is more social because cars now ﬁrst prefer that they both get to their end positions directly.

We  note  that  technically  ﬁne1 and  ﬁne2 are  simply  two  fresh  propositional  symbols.  As  before  states  in  the  model 
can be labelled with them. Thus, it should be intuitive that the preferences can be used to classify paths according to the 
preferences. Later, in Section 3.1.4 we formally introduce the formal machinery to update a model by such new propositional 
symbols; in particular, in Example 11 we shall return to a formal treatment in the context of this example.

2.3.  From CGS to game theory

In order to analyse the behaviour of agents and the emerging system behaviour, we use the machinery of game theory 
(cf.  [54]).  A  strategic game form is  a  tuple  ˆG = (Agt, ( Aci)i∈Agt, O , g) where  Agt is  the  set  of  agents,  Aci
is  the  set  of 
actions  available  to  agent  i ∈ Agt,  O is  a  set  of  outcomes,  and  g : Ac → O with  Ac = ×i∈Agt Aci is  an  outcome  function 
that  associates  an  outcome  with  every  action  proﬁle.  For  the  sake  of  readability,  we  use  the  same  notation  for  agents 
in  strategic  game  forms  and CGSs.  A  strategic game G = ( ˆG, ((cid:17)i)i∈Agt) extends  a  strategic  game  form  with  a  preference 
relation (cid:17)i on  O for each agent i ∈ Agt. The preference relation of agent i induces a preference relation on action proﬁles: 
a1 (cid:17)i a2 iff g(a1) (cid:17)i g(a2) for  any  a1, a2 ∈ Ac.  We  also  use  payoff  functions  μi : O  → R to  represent  preference  relations 
where higher payoff values correspond to more preferred outcomes.

It is well known how an extensive form game can be transformed to a strategic game without changing speciﬁc sets of 
strategy proﬁles, e.g. the set of Nash equilibria. The next deﬁnition connects CGSs to strategic games following the notation 
of [23]. The actions of the strategic game correspond to the (memoryless) strategies in the CGS. A payoff function is obtained 
from preference proﬁles and the outcome paths resulting from strategy proﬁles.

Deﬁnition 7  (CGS (cid:4) strategic game form, strategic game).  Let  (M, q) be  a  pointed CGS with  Agt = {1, . . . , k} and  (cid:9)γ =
(γ1, . . . , γk) ∈ Prefs be  a  preference  proﬁle.  We  deﬁne  (cid:13)(M, q) as  the  strategic  game  form  (Agt, ((cid:5)i)i∈Agt, (cid:8)M(q), g) as-
sociated with (M, q) where Agt is the set of agents in M, (cid:5)i is the set of strategies of agent i (cf. Deﬁnition 2), (cid:8)M(q) the 
set of all paths in M starting in state q, and  g(s) = outM(q, s) for s ∈ (cid:5)Agt. Moreover, we deﬁne (cid:13)(M, q, (cid:9)γ ) as the strategic 
game ((cid:13)(M, q), (μi)i∈Agt) associated with (M, q) and  (cid:9)γ , where for all s ∈ (cid:5)Agt the payoff function μi is deﬁned as follows: 
μi(s) = u j where γi = ((ϕ1, u1), . . . , (ϕl−1, ul−1), (ϕl, ul)) and  j is the minimal index such that π (outM(q, s)) |=LTL ϕ j .

In  general,  not  all  paths  from  a CGS can  be  obtained  by  memoryless  strategies,  i.e. 

outM(q, s) (cid:16)= (cid:8)M(q).  As 
mentioned before, we use memoryless strategies because of their better computational properties. Note that the obtained 
strategic game form is always ﬁnite as the set of strategies of each agent is ﬁnite. Note also that the just deﬁned strategic 
game is well-deﬁned, especially because for the last entry (ϕ, u) of each preference list we have that ϕ = (cid:13).

s∈(cid:5)Agt

(cid:3)

Example 6 (Strategic game form). For our car scenario, the strategic game form (cid:13)(M1, q0) associated with (M1, q0) is illus-
trated in Fig. 3. Strategies are represented according to the conventions of Example 3.

104

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

1\2

M(cid:9)1
W M

W W

M(cid:9)2
q0qω
3
q0q2qω
4
q0qω
2

W M
q0q1qω
4
qω
0
qω
0

W W
q0qω
1
qω
0
qω
0

Fig. 3. The strategic game form (cid:13)(M1, q0) associated with the pointed CGS (M1, q0) from Example 2. We have (cid:9)1, (cid:9)2 ∈ {M, W }.

(cid:13)(M1, q0, (cid:9)γ1)

1\2

M(cid:9)1
W M
W W

M(cid:9)2 W M W W
3\1
3\2
1\1
1\1
1\1
2\3
1\1
1\1
1\3

(cid:13)(M1, q0, (cid:9)γ2)

1\2

M(cid:9)1
W M
W W

M(cid:9)2 W M W W
2\1
2\1
1\1
1\1
1\1
1\2
1\1
1\1
1\2

Fig. 4. The strategic games (cid:13)(M1, q0, (cid:9)γ1) and (cid:13)(M1, q0, (cid:9)γ2). Again, we have (cid:9)1, (cid:9)2 ∈ {M, W }. Payoff proﬁles given in bold indicate Nash equilibria. We use 
x\ y to denote a payoff of x and y for agent 1 and 2, respectively.

A game theoretic solution concept, e.g. Nash equilibria or dominant strategy equilibrium, can be considered as a function 
S the  domain  of  which  is  the  set  of  strategic  games  and  the  image  of  which  is  a  set  of  strategy  proﬁles [54].  That  is, 
for  each  strategic  game  G with  strategy  proﬁles  (cid:5) we  have  that  S(G) ⊆ (cid:5).  We  assume  that  the  reader  is  familiar  with 
the  notion  of  solution  concept  and  refer  to [54] for  further  details.  In  the  present  work  we  are  mainly  concerned  with 
the  concept  of  Nash  equilibrium  and  also  discuss  dominant  strategy  equilibrium.  Therefore,  we  give  the  basic  deﬁnitions. 
An action proﬁle (a1, . . . , ak), containing an action for each agent, is a Nash equilibrium if no agent can unilaterally deviate 
(cid:7)
i of  that  agent  it  holds  that 
to  get  a  better  payoff;  that  is,  for  each  agent  i it  must  be  the  case  that  for  all  actions  a
(cid:7)
i, ai+1, . . . , ak). We use N E(G) to refer to the Nash equilibria in a given game G. A dominant 
(a1, . . . , ak) (cid:17)i (a1, . . . , ai−1, a
strategy  equilibrium  is  a  stronger  notion.  Formally,  a  proﬁle  (a1, . . . , ak) is  a  (weakly) dominant strategy equilibrium if  for 
each  agent  i,  the  action  ai
is  called 
(weakly) dominant strategy.5 We refer to [58] for more details. Furthermore, we use DOE(G) to refer to the set of dominant 
strategy  equilibria  in  a  given  game  G.  We  lift  the  notion  to  tuples  consisting  of  a CGS M and  a  preference  proﬁle  (cid:9)γ by 
S(M, q, (cid:9)γ ) := S((cid:13)(M, q, (cid:9)γ )).

is  the  best  action  independent  of  the  choices  of  the  opponents.  Such  an  action  ai

Example 7  (Example 6 contd.: strategic games).  We  include  the  preference  proﬁles  (cid:9)γ1 and  (cid:9)γ1,  respectively,  from  Exam-
ple 5 in  the  strategic  game  form  of  Example 6.  We  obtain  the  strategic  games  (cid:13)(M1, q0, (cid:9)γ1) and  (cid:13)(M1, q0, (cid:9)γ2) which 
are  shown  in  Fig. 4.  Bold  entries  are  used  to  identify  Nash  equilibria.  For  example,  we  have  that  N E(M1, q0, (cid:9)γ1) =
{(W M, M(cid:9)2) | (cid:9)2 ∈ {M, W }} ∪ {(M(cid:9)1, W M) | (cid:9)1 ∈ {M, W }}.  The  game  (cid:13)(M1, q0, (cid:9)γ1) has  no  dominant  strategy  equilibria 
whereas DOE(M1, q0, (cid:9)γ2) = {(M(cid:9)1, M(cid:9)2) | (cid:9)1, (cid:9)2 ∈ {M, W }}.

It is worth to note that pure Nash equilibria as well as dominant strategy equilibria may not exist. The matching pennies 

game6 is a classical, strictly-competitive game without Nash equilibria [54].

Concluding remarks

In this section we introduced the formal setting in which we shall study the effects of imposing norms on a multi-agent 
system. We used LTL to deﬁne agent’s preferences. In combination with concurrent game structures they allow us to relate 
the  multi-agent  setting  to  normal  form  games,  which  are  a  well-studied  mathematical  model  to  investigate  the  outcome 
of interactions among rational agents. We presented the concept of Nash equilibrium and dominant strategy equilibrium as 
examples  to  capture  agents’  rational  behaviour.  The  general  idea  of  normative  mechanism  design,  introduced  in  the  next 
section, however, does not depend on a speciﬁc solution concept.

3.  Norm-based mechanism design

So far we have considered a concurrent game structure as a formal tool to model a multi-agent system. We also assumed 
that agents have preferences over the system behaviour. As the actual agents’ preferences may not be known in advance, we 
may only assume that a set of possible preference proﬁles over the system behaviour is given—containing those preferences 
which seem sensible to the system designer. In the previous section we explained how these ingredients set the stage to 
analyse the “optimal” behaviours of the multi-agent systems by considering the equilibria of the game constructed from the 
concurrent game structure and the preference proﬁles. With “optimal” we refer to the system behaviours that correspond 

5 We note that this is a rather weak deﬁnition of dominance, other deﬁnitions use a strict preference relation a (cid:19)i a
6 Each of two agents shows one side of a coin. One agent wins if both coins show the same side. The other wins if this is not the case. The matching 
pennies game can, e.g., also be found in [54].

, deﬁned as a (cid:17)i a

and not a

(cid:7) (cid:17)i a.

(cid:7)

(cid:7)

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

105

Notation
Agt
Act
αi
Q
q
M
(cid:5)
(cid:8)
out
s
γi
Prefs
μi
O
(cid:17)i
ai
G, ˆG
G
S

P
f
(cid:2)
(cid:2)h
(cid:2)s
A

N
N(cid:14)
RN
SN
I
N

Acti

Description

Deﬁned/Used

Page

set of players
set of actions
action of player i, used in the multi-agent setting
set of states
state
concurrent game structure
set of (memoryless) strategies
set of paths
outcome of strategy
memoryless strategy
preference list of player i
set of preference lists
utility function of player i
outcomes
preference relation of player i
action of player i, used in the game theoretical setting
strategic game, strategic game form/mechanism
a set of strategic game forms
a solution concept
players
normative behaviour function
propositions (hard and soft facts)
hard facts
soft facts
subset of action proﬁles
normative system
empty normative system
set of regimentation norms
set of sanctioning norms
implementation setting
set of normative systems
set of actions of player i

Deﬁnition 1
Deﬁnition 1
Deﬁnition 1
Deﬁnition 1
Deﬁnition 1
Deﬁnition 1
Deﬁnition 2
Section 2.1
Deﬁnition 3
Deﬁnitions 2
Deﬁnition 6
Deﬁnition 6
Deﬁnition 7
Section 2.3
Section 2.3
Section 2.3
Section 2.3
Section 3.1.1
Section 3.1.1
Section 3.1.1
Deﬁnition 8
Section 3.1.3
Section 3.1.3
Section 3.1.3
Deﬁnition 9
Deﬁnition 9
Deﬁnition 9
Deﬁnition 9
Deﬁnition 9
Deﬁnition 12
Deﬁnition 12
Deﬁnition 15

100
100
100
100
100
100
99
100
101
101
103
103
103
103
103
103
103
105
105
105
106
106
106
106
107
107
107
107
107
112
112
117

Fig. 5. Overview of commonly used notation.

to the outcome of Nash equilibria. In this section, we further assume that a social choice function is given that models the 
multi-agent system designer’s preferred system behaviour, also called the social perspective. A social choice function indicates 
the  system  behaviour  that  is  socially  preferred  (or  preferred  by  the  system  designer)  when  the  agents  act  according  to  a 
speciﬁc preference proﬁle.

The problem that we consider in this section is the implementation problem that can be formulated as follows. Suppose 
that  for  a  given  set  of  agents’  preference  proﬁles  the  socially  preferred  system  behaviour  is  not  aligned  with  the  agents’ 
preferred  system  behaviour,  i.e.  for  the  given  agents’  preference  proﬁles  the  optimal  system  behaviour  from  the  agents’ 
perspective (represented by Nash equilibria) is not aligned/contradicts with the optimal system behaviour from the system 
designer’s perspective (represented by the social choice function). The question is whether the enforcement of some norms 
(e.g.,  by  means  of  regimentation  and  sanctions)  on  the  agents’  behaviour  can  align  the  preferred  system  behaviour  from 
both perspectives, i.e., whether the enforcement of some norms on the agents’ behaviour can change the agents’ behaviour 
toward the socially preferred system behaviour. A second more elaborate question is about the existence of a set of norms 
whose enforcement aligns the preferred system behaviour from the agents’ as well as system designer’s perspectives. We 
coin  the  design  of  a  set  of  norms  to  change  the  agents’  behaviour,  which  is  inspired  by  mechanism  design,  norm-based 
mechanism design.

In order to ease the reading of the rest of this paper, we list some of the concepts used in our formalisation and their 

notation in Fig. 5.

3.1.  Preliminaries

We start with reviewing concepts from classical mechanism design and introducing corresponding concepts for norm-
based  mechanism  design.  In  particular,  we  deﬁne  the  concept  of  normative  behaviour  function  which  is  the  counterpart 
of  the  concept  of  social  choice  function  in  classical  mechanism  design.  We  then  introduce  the  concept  of  norms  and 
norm-based multi-agent system, and explain how they can inﬂuence the agents’ behaviour. We deﬁne the notion of norm 
enforcement formally as an update function that changes the speciﬁcation of the multi-agent system based on a given set 
of norms. These concepts set the stage for norm-based mechanism design.

3.1.1.  Classical mechanism design

In  our  exposition  of  classical  mechanism  design  we  mostly  follow  the  presentation  of [54],  in  particular  we  often  use 
their notation which allows us to clearly relate the classical concepts with the corresponding ones of norm-based mecha-
nism design. In social choice theory a social choice rule f : P → P(O ) assigns a subset of outcomes from the set of outcomes 

106

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

1

1

, (cid:17) f ast
2

O to a preference proﬁle (cid:17) = ((cid:17)i)i∈Agt ∈ P , consisting of a preference relation (cid:17)i over  O for each agent from Agt. Here, 
we consider  P to be a set of such preference proﬁles. In contrast, a social choice function picks exactly one element of the 
outcome  rather  than  a  set  of  outcomes.  For  example,  a  proﬁle  ((cid:17) f ast
) could  express  that  both  agents  have  a  pref-
erence for driving fast. In that case a social choice function can be used to map the preference proﬁle to the outcome oh
representing “build highway”, i.e.  f (((cid:17) f ast

, (cid:17) f ast
2

)) = oh.

In the following discussion we focus on social choice rules. The outcome  f ((cid:17)) is the social outcome deﬁned by the social 
choice rule  f . Mechanism design is concerned with the problem of constructing a mechanism—a strategic game form—which 
implements a social choice rule assuming a speciﬁc rational behaviour of the agents. The mechanism is constructed over a 
structure ﬁxing the set of agents Agt, the set of possible outcomes O , the set of possible preference proﬁles P over outcomes 
O ,  and  a  set  of  strategic  game  forms  G with  outcomes  in  O .  The  mechanism  is  drawn  from  G.  This  ﬁxed  structure  has 
different names in the literature. Osborne and Rubinstein [54] refer to it as environment while Shoham and Leyton-Brown 
[58] use a related formalisation in terms of Bayesian game settings. We follow [54] but, in order to avoid confusion with 
our notation, refer to the structure as implementation setting. Now, a strategic game form G ∈ G together with a preference 
proﬁle (cid:17)∈ P deﬁnes a strategic game (G, (cid:17)). Given a solution concept S (e.g., Nash equilibrium) for strategic games, i.e. a 
mapping from (G, (cid:17)) to a set of action proﬁles, we can formulate the S-implementation problem over an implementation 
setting as follows. A game form  G ∈ G S-implements the social choice rule  f over P if, and only if, for all preference proﬁles 
(cid:17)∈ P and  all  equilibria  (a1, . . . , a|Agt|) ∈ S(G, (cid:17)) we  have  that  the  outcome  obtained  by  (a1, . . . , a|Agt|) is  contained7 in 
f ((cid:17)). We emphasize that S(G, (cid:17)) contains the strategy proﬁles in the game (G, (cid:17)) that satisfy the solution concept S.

3.1.2.  Normative behaviour function

As discussed above, in social choice theory a social choice rule assigns outcomes to given proﬁles of preferences. In our 
setting, the social choice rule represents the preference of the system designer and is deﬁned as a function8 that assigns an 
LTL-formula—describing a set of paths—to each preference proﬁle (a sequence of sequences of LTL-formulae) of the agents. 
From now on, when considering norm-based mechanism design concepts we use our own notation that has been introduced 
in previous sections.

Deﬁnition 8 (Normative behaviour function). Let Prefs be a set of preference proﬁles over (cid:2). A normative behaviour function8
f

is a mapping  f : Prefs → LTL(cid:2).

Similar to classical mechanism design, the preference of the system designer describes the designer’s desired outcome if 
the agents had the given preference proﬁle as their true proﬁle. Thus, representing the preference of the system designer 
by  a  (normative  behaviour)  function  allows  us  to  model  the  system  designer’s  uncertainty  about  the  true  preferences  of 
the agents. The following is a simple example where the preference of the system designer is independent of the agents’ 
preferences such that it maps possible preference proﬁles to an LTL-formula. We choose deliberately to keep the example 
simple, but it should be clear that the preference of the system designer is not always a single formula and often depends 
on the agents’ preferences.

Example 8  (Normative behaviour function).  We  assume  that  the  preference  of  the  system  designer  is  represented  by  the 
following normative behaviour function, which indicates that in all cases the ﬁrst car should reach its end position directly: 
f ( (cid:9)γi) = (cid:12)pe
1.

We  refer  to  the  outcome  as  the  normative outcome wrt. a given preference proﬁle.  In  our  view,  the  aim  of  norm-based 
mechanism design is to come up with a norm-based mechanism (i.e., an environment speciﬁed in terms of actions, norms and 
sanctions)  such  that  the  agents—again  following  some  rationality  criterion  according  to  their  preferences—behave  in  such 
a way that the possible environment executions stay within the normative outcome. The idea is that norms and sanctions 
will (de)motivate agents to perform speciﬁc actions.

Example 9 (Non-aligned preferences). Following Example 7, for  (cid:9)γ1, the path q0q2qω
which is a Nash equilibrium) in M1 does not satisfy the preference  f ( (cid:9)γ1) of the system designer.

4 (yielded by strategy proﬁle (W M, M W ), 

3.1.3.  Normative system, hard and soft facts

In the remainder of this section, let (cid:2) again be a ﬁnite and non-empty set of propositions. We assume that (cid:2) can be 
partitioned into two types of propositions: the set (cid:2)h denotes the hard facts and (cid:2)s the soft facts. Hard facts describe the 
(physical/brute) properties of the multi-agent environment which in turn are used to deﬁne the action structure of a CGS. 

7 Sometimes, it is assumed that all outcomes speciﬁed by  f ((cid:17)) can be obtained by some equilibrium. It is also common to consider a social choice 
function that maps to single outcomes rather than a social choice rule.
8 We emphasize its deﬁnition as a function. When considering the set of paths satisfying a LTL-formula rather than the formula itself, however, it shows 
the characteristics of a social choice rule.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

107

For example, typical hard facts describe that a car is at a certain position or that the traﬃc light is red. Soft facts are used 
to model properties which do not affect the action structure but can affect the agents’ preferences. For example, they model 
that a car has violated a traﬃc norm, a car received a ﬁne, or a car arrived late at its destination (where we use car as a 
shorthand for “the driver of a car” etc.). In other words, the modiﬁcation of soft facts does not change the available actions 
nor their executability but they can affect the evaluation of the agents’ preferences.

The  classiﬁcation  into  hard  and  soft  facts  depends  on  the  speciﬁc  modelling.  In  the  following  we  assume  that  we  are 
given a set of hard facts (cid:2)h and a set of soft facts (cid:2)s, where sets are ﬁnite, non-empty and disjoint. Therefore, we assume 
that a CGS M = (Agt, Q, (cid:2), π , Act, d, o) is given with (cid:2) = (cid:2)h ∪ (cid:2)s. In this paper, we also distinguish between two types 
of norms. Sanctioning norms are enforced through sanctions imposed on the execution of actions from speciﬁc states. Regi-
menting norms are enforced by modifying the effect of the execution of actions from a speciﬁc state; they make the actions 
effectless [27]. In the following, we use PL X for a set  X of propositional atoms to refer to all propositional formulae over  X
and denote by |=PL the satisfaction relation of propositional logic.

× P( Actk)\{∅} × (P((cid:2)s)\{∅} ∪ {⊥}). 
Deﬁnition 9 (Norms and normative system). A norm (over M) is an element from PL(cid:2)h
A norm of type (ϕ, A, ⊥) is a regimenting norm and one of type (ϕ, A, S) with non-empty  S ⊆ (cid:2)s a sanctioning norm. A set 
of such norms  N is called a normative system (over M) with a typical norm denoted by η.  N(cid:14) = ∅ is the empty normative 
system, and Nrs = {N1, N2, . . .} is the set of all normative systems. We deﬁne Ns and Nr as the set of all normative systems 
consisting  of  only  sanctioning  and  regimenting  norms,  respectively.  We  often  write  (ϕ, A, ·) to  refer  to  a  sanctioning  or 
regimenting norm.

A norm (ϕ, A, ⊥) should be read as: “it is forbidden to perform action proﬁles from A in ϕ-states (i.e. states in which 
ϕ holds) and that the enforcement of this norm prevents action proﬁles in A to be performed”. A norm (ϕ, A, S) should 
be read as: “it is forbidden to perform action proﬁles from A in ϕ-states and that the enforcement of this norm imposes 
sanctions  S on  the  states  resulting  by  performing  the  very  action  proﬁle  from  A in  a  ϕ-state”.  The  basic  idea  is  that 
sanctioning  norms  enforced  on  a  model  change  the  valuation  of  states  by  soft  facts  only;  that  is,  the  underlying  physical 
structure represented by hard facts remains intact and thereby the action structure of the model remains intact. As we will 
see later in this paper, the agents’ preferences are also deﬁned on soft facts such that any changes in valuations of states by 
soft facts may affect agents’ rational behaviour. Regimenting norms affect the physical structure as they directly affect the 
action structure of the model.

We note that a norm (ϕ, A, ·) can be seen as a prohibition (“it is prohibited that any action proﬁle in A is executed in 
states satisfying ϕ”) but just as well as a obligation to perform an action proﬁle not in A in any state satisfying ϕ. This also 
corresponds to the duality of obligations and prohibitions: a prohibition of doing an action, is the same as being obliged to 
not doing the action.

Example 10 (Norms). In order to avoid cars to congest in the narrow part of the road we introduce two sanctioning norms. 
The ﬁrst norm prohibits the ﬁrst car to wait in the start position  ps
2 when the second car waits as well. The violation 
1
2, {(W , W )}, {ﬁne1}). The 
∧ ps
of this norm imposes sanction ﬁne1 (i.e., car 1 is sanctioned). This norm is represented as (ps
1
2, {((cid:9), M) | (cid:9) ∈
second  norm  prohibits  the  second  car  to  move  in  the  start  position.  This  norm  is  represented  as  (ps
1
{M, W }}, {ﬁne2}). Note that these norms implement a priority for passage in the narrowed road by obliging car 1 to move 
and car 2 to wait.

∧ ps

∧ ps

It  is  important  to  note  that  sanctioning  norms  can  directly  inﬂuence  the  (quantitative)  utility  that  an  agent  obtains 
as speciﬁc elements of a preference list may no longer be satisﬁable. To illustrate this, consider the preference proﬁle  (cid:9)γ1
introduced in Example 5:

(cid:9)γ1 = {

(((cid:12)pe
1
(((cid:12)pe
2

∧ (cid:2)¬ﬁne1, 3), ((cid:3)pe
1
∧ (cid:2)¬ﬁne2, 3), ((cid:3)pe
2

∧ (cid:2)¬ﬁne1, 2), ((cid:2)¬ﬁne1, 1), ((cid:13), 0))
∧ (cid:2)¬ﬁne2, 2), ((cid:2)¬ﬁne2, 1), ((cid:13), 0)) }

In  general,  there  can  be  an  outcome  of  the  system  which  ensures  agent  1 a  payoff  of  3,  i.e.  an  outcome  that  satisﬁes 
∧ (cid:2)¬ﬁne1. The enforcement of a norm by sanctioning norms of type (ϕ, A, {ﬁne1}) for an appropriate formula ϕ and 
(cid:12)pe
1
set  of  action  proﬁles  A,  however,  can  make  it  impossible  to  yield  outcomes  on  which  ¬(cid:2)ﬁne1 holds.  In  such  a  case,  it 
would be impossible for the agent to obtain a utility of 3. The technical details of this procedure are introduced in the next 
section.

3.1.4.  Norm-based update

In  order  to  examine  the  impact  of  the  enforcement  of  a  set  of  norms  on  multi-agent  systems,  we  need  to  determine 
applicable  norms  and  their  sanctions.  Therefore,  we  need  to  decide  which  norms  are  applicable  in  a  concurrent  game 
structure  and  what  are  the  sanctions  that  should  be  imposed  on  the  concurrent  game  structure.  A  norm  η = (ϕ, A, ·) is 
applicable in a state which satisﬁes ϕ and in which an action tuple from A is being performed.

108

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Deﬁnition 10  (Applicable norms and sanctions).  Let  N ∈ Nrs be  a  normative  system,  X ⊆ (cid:2) be  a  set  of  facts,  and  (cid:9)α be  an 
action proﬁle. The set of norms from N applicable wrt.  X and  (cid:9)α, denoted by AppN( X, (cid:9)α), is deﬁned as follows:

AppN(X, (cid:9)α) = {η ∈ N | η = (ϕ, A, ·) with X |=PL ϕ and (cid:9)α ∈ A}.

The set of sanctions from N that should be imposed based on  X and  (cid:9)α, denoted as SanN( X, (cid:9)α), is computed as follows:

SanN(X, (cid:9)α) =

(cid:4) (cid:3)

{⊥}

{S | (ϕ, A, S) ∈ AppN(X, (cid:9)α)}

if AppN(X, (cid:9)α) contains no regimenting norm,
otherwise.

We note that the evaluation of  X |=PL ϕ can be done in polynomial time as it corresponds to evaluating a propositional 
formula with respect to a given truth assignment represented by the set  X . Note that we use a set of facts  X ⊆ (cid:2) (rather 
than a state q ∈ Q ) that triggers norms and sanctions. We do this in order to reuse this deﬁnition later on in this paper. In 
the following, we use SanN(π (q), (cid:9)α) to determine the sanctions wrt. N, q and  (cid:9)α. We emphasize that the computed sanctions 
in  a  state  q are  meant  to  be  imposed  on  the  state  which  is  reached  when  executing  (cid:9)α,  i.e.,  the  computed  sanctions  are 
imposed on state o(q, (cid:9)α).

The enforcement of a set of norms on a multi-agent system can be modelled as updating the concurrent game structure 
of the multi-agent system by a normative system, i.e. by regimenting or sanctioning behaviour depending on the type of 
norm.

Deﬁnition 11 (Update by norms).  Let M = (Agt, Q, (cid:2), π , Act, d, o) be a concurrent game structure and N ∈ Nrs be a norma-
tive system over M. The update of M with N, denoted by M (cid:2) N, is the CGS (Agt, Q

(cid:7), (cid:2), π (cid:7), Act, d

(cid:7)) where

(cid:7), o

(cid:7), (cid:9)α) = q, S = SanN(π (q

(cid:7)), (cid:9)α) and {⊥} (cid:16)= S (cid:16)= ∅ }

1. Q

2. π (cid:7)(x) =

(cid:5)

(cid:7) := Q ∪ { (q, S) | ∃q
π (x)
π (q) ∪ S

(cid:7) ∈ Q, (cid:9)α ∈ Actk : o(q
if x ∈ Q
if x = (q, S)
(cid:7)
i(x) = di(q) where x = q or x = (q, S), and i ∈ Agt

3. d

4. o

(cid:7)(x, (cid:9)α) =

⎧
⎪⎨

⎪⎩

(o(q, (cid:9)α), S)
o(q, (cid:9)α)
x

if S = SanN(π (q), (cid:9)α) and {⊥} (cid:16)= S (cid:16)= ∅
if SanN(π (q), (cid:9)α) = ∅
otherwise (i.e. {⊥} = SanN(π (q), (cid:9)α))

for either x = (q, S

(cid:7)) with  S

(cid:7) ⊆ (cid:2)s, or x = q.

The ﬁrst item shows how to duplicate states in Q that are sanctioned but not regimented. The second item deﬁnes the 
updated evaluation of (sanctioned) states. The third item ensures that the new (duplicated) states have the same options 
as  their  original  counterparts.  This  is  due  to  the  fact  that  sanctions  are  solely  deﬁned  on  soft  propositions  not  affecting 
the  transition  structure.  Finally,  the  fourth  item  ensures  that  the  outcome  of  actions  from  either  the  states  in  Q or  their 
(new)  duplicates  are  in  accordance  with  the  original  model  whenever  the  actions  in  those  states  are  not  regimented; 
otherwise when regimented, the actions have no effect and loop in the same state. It is important to notice that looping of 
a regimented action in a state models a situation where the action cannot be performed and should therefore be interpreted 
as non-performance of the action. In this case the system remains in the same state. The following example illustrates how 
the effect of actions is determined in an updated model.

Example 11 (Norm-based update). The norm-based update of the environment model M1, as illustrated in Fig. 2, based on 
2, {((cid:9), M) | (cid:9) ∈ {M, W }}, {ﬁne2}) }, denoted by M2 := M1 (cid:2)
the normative system N1 = { (ps
1
1
N1,  results  in  a  new  CGS shown  in  Fig. 6.  In  particular,  the  effects  of  action  proﬁles  (M, M), (M, W ),  and  (W , W ) in  the 
duplicated state (q0, {ﬁne1}) from M1 (cid:2) N1 are deﬁned as follows:

2, {(W , W )}, {ﬁne1}),  (ps

∧ ps

∧ ps

(cid:7)

o

(cid:7)

((q0, {ﬁne1}), (M, M)) = (o(q0, (M, M)), {ﬁne2})
((q0, {ﬁne1}), (M, W )) = o(q0, (M, W ))
(cid:7)
((q0, {ﬁne1}), (W , W )) = (q0, {ﬁne1})

o

o

∧ ps

2, {(M, M)}, ⊥),  (ps

A different norm-based update of the environment model M1 by the normative system that consists of only regimenting 
norms N2 = { (ps
2, {(W , M)}, ⊥) } is illustrated in Fig. 7. It can be observed that the regimen-
1
tation of these norms does not allow car 2 to move at the starting position. Note that in our system only action proﬁles 
can  be  the  subject  of  norm  regimentation.  However,  this  does  not  mean  that  individual  agents  cannot  be  the  subject  to 
norm regimentation. For example, the resulting transition system in Fig. 7 allows car 1 to do a move action in the starting 
position while it prevents the move action of car 2.

∧ ps

1

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

109

Fig. 6. The CGS M2 models M1 (cid:2) N1, i.e., the norm-based update of M1 based on N1.

It  is  important  to  notice  that  sanctions  do  not  accumulate  through  state  transitions.  This  is  reﬂected  by  the  following 
proposition which expresses that states carry the sanctions caused by the most recent action only, or the most recent action 
has been regimented.

Proposition 1 (Non-accumulation of sanctions). Let M = (Agt, Q, (cid:2), π , Act, d, o) and M (cid:2) N = (Agt, Q
and either x = q or x = (q, S

(cid:7) ⊆ (cid:2)s. We have

(cid:7)) with S
(cid:5)

(cid:7), (cid:2), π (cid:7), Act, d

(cid:7), o

(cid:7)), q ∈ Q

(cid:7)

π (cid:7)

(o

(x, (cid:9)α)) ∩ (cid:2)s =

SanN(π (q), (cid:9)α)
π (cid:7)(x) ∩ (cid:2)s

if SanN(π (q), (cid:9)α) (cid:16)= {⊥},
otherwise.

Proof. Directly follows from Deﬁnition 11 because o

(cid:7)

ensures that  S

(cid:7)

is ignored after  (cid:9)α. (cid:2)

The non-accumulation of sanctions holds for state transitions that take place based on non-regimented actions. A reg-
imented  action  that  loops  in  a  state  with  sanctions  causes  state  transitions  through  which  sanctions  are  not  removed 
(consecutive states through regimentation actions are identical). Although this may suggest that sanctions are accumulated 
through consecutive states, the persistence of the sanctions in state transitions caused by regimented actions should not be 
interpreted  as  receiving  sanctions  repeatedly.  Note  that  this  is  consistent  with  our  interpretation  that  regimented  actions 
cannot be performed, i.e., consecutive states through regimented actions should be considered as remaining in one and the 
same state. However, we also agree that an alternative solution could be the one where the regimented norm causes a state 
transition to a fresh copy of the current state with all sanctions removed. This is a design choice. We also note that in a 
previous version of this work [19] we completely removed speciﬁc transitions. As a result the selection of individual actions 
by  agents  could  yield  invalid  action  proﬁles.  The  selection  of  such  invalid  action  proﬁles  was  then  essentially  avoided  by 
assigning to them a negative utility.

We also observe that updating a CGS with only regimenting norms does not duplicate any state and may only modify 
the accessibility relation between the state. However, updating a CGS with sanctioning norms can duplicate and introduce 
new states. This update is essentially different from norm update as introduced by [1]. In case of regimentation we remove 
accessibility  relation  between  states  (by  enforcing  a  looping  transition)  and  in  case  of  sanctioning  we  add  new  copies  of 
states  and  extend  the  accessibility  relation.  In  [1] an  update  results  in  restricting  the  accessibility  relation  by  removing 
transitions from the model. Another characteristic of our model update is that different orders of updates with regimenting 
norms result in the same outcome, which in turn is the same as the single update with the union of regimenting norms. 
That is, we have that:

(M (cid:2) RN) (cid:2) RN

(cid:7) = (M (cid:2) RN

(cid:7)

) (cid:2) RN = M (cid:2) (RN ∪ RN

(cid:7)

)

110

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Fig. 7. Norm-based update of M1 based on N2.

Observe  that  this  is  not  true  for  sanctioning  norms,  i.e.,  different  orders  of  updates  with  sanctioning  norms  do  not 

necessarily result in the same CGS. This means that for some CGS M and non-empty sets SN1 and SN2 we may have

(M (cid:2) SN1) (cid:2) SN2 (cid:16)= (M (cid:2) SN2) (cid:2) SN1 (cid:16)= M (cid:2) (SN1 ∪ SN2).

This  is  due  to  the  fact  that  updates  of  sanctioning  norms  may  duplicate  states  such  that  subsequent  updates  generate 
different  states.  One  obvious  reason  different  orders  of  updates  by  sanctioning  norms  result  in  different  models  is  of  a 
syntactic  nature,  interpreting  = as  strict/syntactic  equality.  We  illustrate  this  with  a  small  example.  Suppose  that  a  new 
state  (q, S1) is  the  result  of  a  norm  update.  Now,  if  the  model  is  updated  by  another  normative  system,  this  may  yield 
states of type ((q, S1), S2). Clearly, reversing the order in which the update is performed yields states of type ((q, S 2), S1)
which are different from the previous types of states whenever  S 1 (cid:16)= S2. Next we consider an example which shows that 
subsequent  updates  with  sanctioning  norms  may  generate  more  states  than  updating  with  the  union  of  the  sanctioning 
norms at once as illustrated in the example below. Conceptually, all norm violations occurring at the same time should also 
be sanctioned immediately. This is reﬂected in our deﬁnition of norm update and illustrated in Example 12 and Fig. 8.

Example 12  (Update order with sanctioning norms).  We  consider  the  CSG  M shown  in  Fig. 8 and  update  it  with  SN1 =
{(ˆq, {a}, {s1})} and  SN2 = {(ˆq, {a}, {s2})} in  different  orders,  and  at  once  (ˆq is  a  propositional  formula  that  is  true  only  in 
state q), respectively.

The example showed that the models are different. Interestingly, however, in Example 12 if the models (M (cid:2) SN1) (cid:2) SN2, 
(M (cid:2) SN2) (cid:2) SN1 and M (cid:2) (SN1 ∪ SN2) are restricted to states reachable from q, then these models are identical apart from 
the  names  of  their  states.  Essentially,  given  a  pointed  CGS,  different  orders  of  updates  with  sanctioning  norms  result  in 
similar CGSs if we consider the parts of the updated CGSs that are reachable from the initial state only. Indeed, it can be 
shown that the models (M (cid:2) SN1) (cid:2) SN2 and (M (cid:2) SN2) (cid:2) SN1 are identical apart from the names of states9 if restricted to 
the part reachable from a given state. One may be tempted to conclude that in the same sense these models are similar to 
M (cid:2) (SN1 ∪ SN2); however, this is not always the case as the reachable part of the latter model may have strictly less states 
than the previous models. It is our conjecture, however, that the reachable part of these models are bisimilar to each other. 
As these results are not directly relevant for the exposition of this paper, we omit a formal treatment of this matter.

The order of updates for some non-empty sets of sanctioning and regimenting norms is important as well, i.e., updating 
ﬁrst with sanctioning norms and then with regimenting norms may result in different outcomes than updating ﬁrst with 
regimenting  norms  and  then  with  sanctioning  norms.  Moreover,  subsequent  updates  with  regimenting  and  sanctioning 
norms do not necessarily result in the same outcome as the single update with the union of regimentation and sanctioning 
norms. The reason for this is that two consecutive updates by RN and SN result in different outcomes as one single update 
by RN ∪ SN. The basic intuition is that regimenting norms have priority and if an action has been regimented it does not 
make sense to sanction the action as it can by the nature of regimentation no longer be executed. This means that for some 
CGS M and non-empty sets SN and RN, we have

9 More formally, they are isomorphic in the sense of Deﬁnition 24. Reachability is formally introduced in Deﬁnition 23.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

111

Fig. 8. Updating CGS M with sanctioning norms in different orders results in different models.

Fig. 9. Updating CGS M with sanctioning and regimenting norms in different orders results in different models.

(M (cid:2) RN) (cid:2) SN (cid:16)= (M (cid:2) SN) (cid:2) RN (cid:16)= M (cid:2) (RN ∪ SN)

These observations are illustrated in Example 13 and Fig. 9, where (M (cid:2) SN) (cid:2) RN is different from (M (cid:2) RN) (cid:2) SN, which 

is again different from M (cid:2) RN, which is in this particular case the same as M (cid:2) (RN ∪ SN).

Example 13  (Update order with sanctioning and regimenting norms).  We  consider  the  CGS  M shown  in  Fig. 9 and  update  it 
with SN = {(ˆq, {a}, {s})} and RN = {(ˆq, {a}, ⊥)} in different orders, and at once (ˆq is a propositional formula that is true only 
in state q), respectively.

We summarize the results in the following proposition.

Proposition 2 (Properties of norm update). Let M = (Agt, Q, (cid:2), π , Act, d, o) be a CGS, RN, RN
and SN ∈ Ns be a set of sanctioning norms over M. Also let N = RN ∪ SN. Then, we have

(cid:7) ∈ Nr be sets of regimenting norms, 

1. M (cid:2) N(cid:14) = M
2. (M (cid:2) RN) (cid:2) RN
3. In general, we have: (M (cid:2) SN) (cid:2) RN (cid:16)= M (cid:2) N = M (cid:2) (RN ∪ SN) (cid:16)= (M (cid:2) RN) (cid:2) SN
4. If forall q ∈ Q and for all (cid:9)α ∈ dAgt(q) we have that AppRN(π (q), (cid:9)α) = ∅ or AppSN(π (q), (cid:9)α) = ∅ then M (cid:2) N = (M (cid:2) RN) (cid:2) SN.

(cid:7)) (cid:2) RN = M (cid:2) (RN ∪ RN

(cid:7) = (M (cid:2) RN

(cid:7))

112

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

classical mechanism design setting
agents Agt
outcomes O
preference relation (cid:17)i
set of preference proﬁles P
mechanism G ∈ G
set of mechanisms/strategic game forms G
implementation setting (Agt, O , P , G)
social choice rule f : P → P(O )

norm-based mechanism design setting
agents Agt
ω-words P((cid:2)s ∪ (cid:2)h)ω
preference list γi over LTL-formulae
set of preference proﬁle lists Prefs
normative mechanism (M, N)
set of norm-based mechanisms N = {N1, N2, . . .} wrt. M
norm-based implementation setting (M, Prefs, N )
normative behaviour function f : Prefs → LTL

Fig. 10. Correspondence between mechanism design and norm-based mechanism design.

Proof. (1)  Obvious.  (2)  Regimenting  norms  may  introduce  loops.  Applying  a  regimenting  norm  on  an  already  regimented 
action  has  no  effect.  (3)  Follows  from  Example 13.  (4)  If  in  no  state  a  regimenting  norm  and  a  sanctioning  norm  are 
applicable, then the updates cannot interfere. The result follows trivially. (cid:2)

The  properties  above  concern  the  iterated  applications  of  the  update  operation.  In  the  rest  of  this  paper,  we  focus  on 

single updates.

3.2.  Implementation setting

In  this  section  we  introduce  norm-based mechanism design by  considering  a  mechanism  using  norms  to  inﬂuence  the 
agents’ behaviour. We use a normative behaviour function to assign a set of “desired” environment executions (desired from 
the  system  designer’s  perspective),  represented  by  an  LTL-formula,  to  each  preference  proﬁle  of  the  agents.  Thus,  based 
on  the  preferences  of  the  agents  the  system  designer  prefers  speciﬁc  outcomes.  The  aim  of  norm-based mechanism design
is  to  yield  a  normative system (i.e.,  a  set  of  regimenting  and  sanctioning  norms)  for  a  given  environment  such  that  the 
enforcement of this normative system in the environment motivates the agents to behave in such a way that the outcomes 
preferred by the system designer are realised. The idea is that the enforcement of norms (de)motivates or prevents agents 
to perform speciﬁc actions. In order to predict how agents act we assume that they act rationally. What playing rationally 
means shall be speciﬁed by game theoretic solution concepts.

Below we introduce the norm-based implementation setting, where mechanism design is interpreted in terms of norms. 
The basic structure consists of a CGS, a set of preference proﬁles, and a set of normative systems which if applied on the CGS
change the underlying CGS. A tuple consisting of the CGS and such a normative system is called norm-based mechanism. In 
this sense a norm-based mechanism relates to game forms which are mechanisms in classical mechanism design. Outcomes 
are deﬁned as paths and preferences over those paths are speciﬁed by preference proﬁles based on LTL-formulae. In Fig. 10
we summarize the correspondence between (classical) mechanism design and norm-based mechanism design.

Deﬁnition 12  (Norm-based implementation setting).  A  (norm-based) implementation setting (NIS)  is  given  by  a  tuple  I =
(M, Prefs, N ) where

• M = (Agt, Q, (cid:2), π , Act, d, o) is a CGS with (cid:2) = (cid:2)h ∪ (cid:2)s partitioned into hard and soft facts.
• Prefs is a set of preference proﬁles over Agt and (cid:2).
• N = {N1, N2, . . . } is a set of normative systems over (cid:2) with Ni = SNi ∪ RNi , where SNi ∈ Ns and RNi ∈ Nr.

The tuple (I, q) where q is a state of M is called pointed (norm-based) implementation setting.

For  the  remainder  of  this  section  we  assume  that  I = (M, Prefs, N ) is  a  ﬁxed  implementation  setting  where  M =

(Agt, Q, (cid:2), π , Act, d, o) and q is a state in M.

Deﬁnition 13 (Norm-based mechanism). Let N ∈ Nrs be a normative system (over M). The tuple (M, N) is called norm-based 
mechanism.

A norm-based mechanism (M, N) gives rise to a CGS by updating M with N as introduced in Deﬁnition 11.

Example 14  (Norm-based mechanism).  The  environment  model  M1,  as  illustrated  in  Fig. 2,  together  with  the  normative 
system  N1,  as  deﬁned  in  Example 11,  constitute  a  norm-based  mechanism  (M1, N1),  which  gives  rise  to  CGS M2 from 
Example 11 and  Fig. 6.  The  CGS M2 yields  in  turn  the  strategic  game  form  (cid:13)(M1 (cid:2) N1, q0) as  illustrated  in  Fig. 11.  For 
simplicity  reasons,  we  encode  strategies  as  triples  (x, y, z).  For  agent  1  (resp.  agent  2)  the  triple  encodes  that  the  agent 
executes action x at q0,  y at q

(cid:7)
2 (resp. x at q0,  y at q

(cid:7)
0, and z at q1).

(cid:7)
0, and z at q

A norm-based mechanism deﬁnes regimenting and sanctioning norms that inﬂuence agents’ behaviour in certain direc-
tions.  Agents  are  assumed  to  act  rationally,  in  particular  in  accordance  to  their  preferences.  We  use  the  concept  of  Nash 

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

113

1\2
M (cid:9)1 (cid:9)(cid:7)
1
W W W

W W M

W M W

W M M

M (cid:9)2 (cid:9)(cid:7)
2
(cid:7) ω
q0q
3
(cid:7) ω
q0q
2
(cid:7)
2qω
q0q
4
(cid:7) ω
q0q
2
(cid:7)
2qω
q0q

4

W W W
q0qω
1
(cid:7) ω
q0q
0
(cid:7) ω
q0q
0
(cid:7)
0qω
q0q
(cid:7)
0qω
q0q

1

1

W W M
q0q1qω
4
(cid:7) ω
q0q
0
(cid:7) ω
q0q
0
(cid:7)
0q1qω
(cid:7)
0q1qω

4

4

q0q
q0q

W M W
q0qω
1
(cid:7) ω
(cid:7)
0q
q0q
2
(cid:7)
(cid:7)
2qω
0q
q0q
4
(cid:7) ω
(cid:7)
0q
q0q
3
(cid:7) ω
(cid:7)
0q
q0q
3

W M M
q0q1qω
4
(cid:7) ω
(cid:7)
0q
q0q
2
(cid:7)
(cid:7)
2qω
0q
q0q
4
(cid:7) ω
(cid:7)
0q
q0q
3
(cid:7) ω
(cid:7)
0q
q0q
3

Fig. 11. The game form associated to M1 (cid:2) N1 where (cid:9)1, (cid:9)(cid:7)

1, (cid:9)2, (cid:9)(cid:7)

2

∈ {M, W }.

(cid:13)(M1 (cid:2) N1, q0, (cid:9)γ1)

2

M (cid:9)2 (cid:9)(cid:7)
1\0
1\0
2\0
1\0
2\0

2

M (cid:9)2 (cid:9)(cid:7)
1\0
1\0
2\0
1\0
2\0

W W W

W W M

W M W

W M M

3\1
0\1
0\1
0\1
0\1

3\2
0\1
0\1
0\2
0\2

3\1
0\1
0\0
0\0
0\0

3\2
0\0
0\0
0\0
0\0

(cid:13)(M1 (cid:2) N1, q0, (cid:9)γ2)

W W W

W W M

W M W

W M M

2\1
0\1
0\1
0\1
0\1

2\2
0\1
0\1
0\2
0\2

2\1
0\1
0\0
0\0
0\0

2\2
0\0
0\0
0\0
0\0

1\2
M (cid:9)1 (cid:9)(cid:7)
1
W W W

W W M

W M W

W M M

1\2
M (cid:9)1 (cid:9)(cid:7)
1
W W W

W W M

W M W

W M M

Fig. 12. Strategic games (cid:13)(M1 (cid:2) N1, q0, (cid:9)γ1) and (cid:13)(M1 (cid:2) N1, q0, (cid:9)γ2) where bold entries mark the Nash equilibria, where (cid:9)1, (cid:9)(cid:7)

1, (cid:9)2, (cid:9)(cid:7)

2

∈ {M, W }.

equilibrium as rationality criterion. As the fulﬁlment of agents’ preferences depends on the valuation of states on a path, 
agents’ behaviour may be affected if imposing norms and sanctions would cause a modiﬁcation of states’ valuations. Thus, 
norms  can  provide  an  incentive  (e.g.  in  the  case  of  sanctioning  norms)  to  agents  to  change  their  behaviour.  Therefore,  a 
key question is how to engineer good incentives. In [19], we required that for a successful implementation of a normative 
behaviour  function,  all  resulting  Nash  equilibria  have  to  agree  with  the  behaviour  speciﬁed  by  the  normative  behaviour 
function, which is essentially the case in the classical game theoretic implementation setting given by [54]. Following [69]
and [30] we also introduce a weaker notion of implementation where the existence of some good behaviour is suﬃcient. 
We use the notation of [69] and [30] and introduce weak and strong implementation.

Deﬁnition 14 (S-implementation). Let I = (M, Prefs, N ) and N ∈ N . We say that a norm-based mechanism (M, N) (strongly) 
S-implements the normative behaviour function f over (I, q) iff for all  (cid:9)γ ∈ Prefs, S((cid:13)(M (cid:2) N, q, (cid:9)γ )) (cid:16)= ∅ and for all s ∈ S((cid:13)(M (cid:2)
N, q, (cid:9)γ )) : outM(cid:2)N(q, s) |=LTL f ( (cid:9)γ ). We say that (M, N) weakly S-implements normative behaviour function  f over (I, q) iff 
for all  (cid:9)γ ∈ Prefs there is an s ∈ S((cid:13)(M (cid:2) N, q, (cid:9)γ )) such that outM(cid:2)N(q, s) |=LTL f ( (cid:9)γ ). We deﬁne the following corresponding 
sets:

SIS (I, q, f ) = {N ∈ N | (M, N) strongly S-implements f over (I, q)}.
WIS (I, q, f ) = {N ∈ N | (M, N) weakly S-implements f over (I, q)}.

If SIS (I, q) (cid:16)= ∅ (resp. WIS (I, q) (cid:16)= ∅), we say that  f

is strongly (resp. weakly) S-implementable over (I, q).

The next example gives a norm-based mechanism which strongly N E -implements a normative behaviour function.

Example 15  (Norm implementation).  Adding  the  preference  proﬁles  (cid:9)γ1 and  (cid:9)γ2 to  the  game  form  (cid:13)(M1 (cid:2) N1, q0) from  of 
Example 14 results in the strategic games shown in Fig. 12. Bold payoffs indicate the Nash equilibria. The equilibria paths 
do now satisfy the system designer’s preference.

Concluding remarks

This concludes our formal model of norm-based mechanism design. The complexity results in the following section are 
based  on  Nash  equilibrium  implementability.  In  general,  other  solution  concepts,  like  dominant  strategy  equilibria,  may 
be  considered,  depending  on  the  problem  at  stake.  A natural  question  is  why  we  should  be  interested  in  strong  or  weak 
implementability.  The  short  answer  is:  to  obtain  a  stable  and  desired  system  behaviour.  For  illustration  let  us  consider 

114

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

smart energy grids involving actors such as producers, consumers and regulation authorities. For the sake of the argument 
we  simply  assume  producers  and  consumers  to  act  rationally  according  to  the  concept  of  Nash  equilibrium.  Clearly,  this 
is  an  idealised  and  abstracted  view,10 but  we  believe  it  delivers  the  main  ideas  behind  our  framework.  The  regulation 
authority may have information about the producers’ and consumers’ preferences. The information might be obtained based 
on common knowledge (power should always be available), previous behaviour of the producers and consumers, or market 
research techniques. The regulation authority has some objectives such as ﬂattening peak energy demand by consumers or 
eﬃcient  energy  production  by  producers  that  it  wants  to  have  met.  As  all  actors  are  self  interested,  the  system  (1)  may 
not show stable behaviour in the sense that there is no optimal (social) solution, or (2) may not satisfy the objectives of 
the regulation authority. In order to overcome these problems, the authority can introduce and enforce norms in order to 
restrict or incentivize consumers and producers to change their behaviour. For example, additional fees11 may be charged on 
consumers if energy is used at speciﬁc hours, or ﬁnes are imposed on producers for overproduction. In this context, strong 
implementability means that all stable behaviours satisfy the authority’s objectives, i.e., if all actors act rationally and play 
equilibrium strategies the objectives of the regulation authority are guaranteed. On the other hand, weak implementability 
indicates that there are some stable behaviour that satisfy the objectives of the regulation authority, but additional means 
are needed to coordinate on such a behaviour. Weak implementability can therefore be seen as a ﬁrst step. Obviously, there 
are many applications for which our formal methodology can be used to analyse and improve the overall system behaviour, 
e.g.,  transportation  systems,  traﬃc,  ﬁnancial  markets  and  business  processes.  In  general,  our  formal  methodology  can  be 
applied to applications where the behaviour of involved actors/processes need to be monitored and controlled. In the next 
paragraph we shall investigate the complexity of weak and strong implementation.

4.  Veriﬁcation and complexity issues

In  this  section  we  consider  the  complexity  of  the  problem  whether  some  normative  system  implements  a  normative 
behaviour  function  and  whether  such  a  normative  system  exists  at  all.  We  focus  on  implementation  in  Nash  equilibria. 
These  results  are  important  in  order  to  check  whether  a  norm-governed  environment  ensures  that  the  agents’  rational 
behaviour satisﬁes the system speciﬁcation. We present the proofs of the main results and moved technical details to the 
appendix.

For the complexity results we ﬁrst need to be clear about how the size of the input is measured. The size of a CGS is 
deﬁned as the product of the number of states and the number of transitions, which is denoted by |M|. The size of a ﬁnite 
normative system  N is given by the number of norms it is comprised of times the maximal length propositional formula 
contained  in  a  norm  of  N (i.e.  a  norm  is  only  measured  in  terms  of  the  size  of  its  precondition,  as  it  can  be  assumed 
that  the  other  elements  are  bound  by  the  size  of  the  model  and  the  agents’  preferences).  The  size  of  a  set  of  normative 
systems is measured by |N | · |Nmax| where Nmax ∈ N is a normative system of maximal size contained in N . Moreover, we 
assume that any considered normative behaviour function  f is polynomial-time computable. The ﬁrst result investigates the 
complexity of performing a norm-based update.

Proposition 3. Let I = (M, Prefs, N ) be given. For any N ∈ N , M (cid:2) N can be computed in polynomial time wrt. the size of M and N. 
The size of M (cid:2) N is bounded by O (|M|4).

Proof. Let M(cid:7) := M (cid:2) N, and n and t denote the number of states and transitions in M, respectively. The set AppN(π (q), (cid:9)α)
can be computed in polynomial time in the size of N: for each η = (ϕ, A, ·) ∈ N it has to be checked whether π (q) |=PL ϕ. 
To determine the size of M(cid:7)
is bounded by n + n · n · t = O (n2t) as the 
(cid:7)), (cid:9)α) is bounded by n · t and by Proposition 1 sanctions are not accumulated, i.e. there are at most 
number of tuples (π (q
n2 · t many new states of the form (q, S). In each new state (q, S) the choices are the same as in q, thus also the number of 
outgoing transitions. The number of transitions in M(cid:7)
is bounded 
by  O (n4 · t3) ≤ O (|M|4). (cid:2)

is bounded by  O (n2t) · t = O (n2t2). Thus, the size of M(cid:7)

we inspect Deﬁnition 11. The number of states of M(cid:7)

The next result is concerned with the special case of the weak implementation problem. We investigate the complexity 
of verifying whether a given normative system weakly implements a given normative behaviour function. Hardness is shown 
by a reduction of QSAT2 which refers to the satisﬁability problem of Quantiﬁed Boolean Formulae (cf. Appendix A). In the 
appendix  we  show  how  to  construct  a  model  Mφ and  a  preference  proﬁle  from  a QSAT2-formula  φ such  that  there  is  a 
Nash equilibrium iff φ is satisﬁable. The construction of Mφ is based on a construction proposed by [22].

10 In particular, we assume that the amount of energy demanded and supplied can be measured qualitatively expressing, e.g., ‘low’, ‘medium’, and ‘high’ 
demand.
11 We note that ﬁnes and incentives can be measured quantitatively, assuming that the set of possible numerical values is ﬁnite and ﬁxed in advance. We 
stress that the purpose of this example is to illustrate the conceptual idea of this approach rather than giving a full-ﬂedged real-world modelling of smart 
grids.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

115

Theorem 4 (Weak implementation: veriﬁcation). Let (I, q) be a pointed NIS, where I = (M, Prefs, N ) and N ∈ N be a normative 
system. Let also  f be a normative behaviour function. The problem whether N ∈ WIN E (I, q, f ) is (cid:2)P
2 -complete in the size of M, N
and Prefs.

(cid:7)

(cid:7)

(cid:7)

Proof. Membership: We construct a non-deterministic polynomial time oracle Turing machine M which can make calls to a 
. It takes as input a strategy proﬁle 
non-deterministic polynomial time Turing machine M
s and returns true if the proﬁle is not a Nash equilibrium. Therefore, for each agent  i, the oracle machine  M
guesses an 
(cid:7)
i) yields  a  better  payoff  than  s.  If  the  proﬁle  yields  a  better  payoff 
individual  strategy  s
the machine returns true. The machine M works as follows. Firstly, M computes M (cid:2) N. Then, for each proﬁle  (cid:9)γ ∈ Prefs the 
machine guesses a strategy proﬁle s and checks whether π (out(q, s)) |=LTL f ( (cid:9)γ ). The latter check can be done in polynomial 
deterministic time as s determines a cyclic path in M on which the truth of a linear-time temporal formula can easily be 
determined. If successful the machine checks whether s is a Nash equilibrium in (cid:13)(M, q, (cid:9)γ ) by calling M
and reverting the 
answer. This shows that the problem whether N ∈ WIN E (I, q, f ) is in (cid:2)P
2

(cid:7)
i for  i and  checks  whether  (s−i, s

. We ﬁrst describe the machine M

Sketch of hardness: The  hardness  result  is  proven  in  Theorem 18 in  the  appendix.  Here,  we  only  give  the  high  level 
idea  of  the  reduction  of QSAT2 to  the  weak  implementability  problem.  Given  a QSAT2 formula  φ we  construct  a  two 
player  CGS Mφ consisting  of  the  refuter player r and  the  veriﬁer player v.  Essentially,  the  players  decide  on  the  value  of 
universally and existentially controlled variables, respectively. Once they decide on a truth assignment, they play a game to 
evaluate  the  Boolean  formula  contained  in  φ following  the  game  theoretical  semantics  of  propositional  logic.  That  is,  the 
refuter player r and the veriﬁer player v control conjunctions and disjunctions, respectively. Using this result, we construct a 
preference proﬁle (γv, γr) such that φ is satisﬁable iff ∃s ∈ N E(Mφ, q0, (γv, γr)) such that out(q0, s) satisﬁes a constructed 
formula  f (γv, γr), the state q0 is a distinguished initial state in Mφ . Now, this is the case if the empty normative systems 
N(cid:14) ∈ WIN E (I, q0, f ). (cid:2)

= NPNP.

(cid:7)

[2]

Analogously to the previous result, we next investigate whether a given normative system strongly implements a norma-
tive behaviour function. In the following, we show that the veriﬁcation version of the strong implementation problem is in 
. This complexity class consists of all problems which can be solved by a polynomial-time deterministic oracle Turing 
2 , cf. [67]. Non-adaptive means that queries must be 

(cid:2)P
2
P
(cid:23)
machine which can make two non-adaptive queries to a problem in (cid:2)P
independent from each other; in other words, it must be possible to perform them in parallel.

Theorem 5 (Strong implementation: veriﬁcation). Let (I, q) be a pointed NIS, where I = (M, Prefs, N ) and N ∈ N be a normative 
system. Let also  f be a normative behaviour function. The problem whether N ∈ SIN E (I, q, f ) is in P
2 -hard as 
well as (cid:3)P

2 -hard in the size of M, N and Prefs.

. The problem is (cid:2)P

(cid:2)P
2
(cid:23)

[2]

Proof. Membership: We construct a deterministic polynomial time Turing machine (TM) N with an (cid:2)P
2 -oracle, implemented 
twice: 
by  a  non-deterministic  TM  N
to check whether N E((cid:13)(M (cid:2) N, q, (cid:9)γ )) (cid:16)= ∅ and whether for all s ∈ N E((cid:13)(M (cid:2) N, q, (cid:9)γ )) it holds that π (outM(cid:2)N(q, s)) |=LTL
f ( (cid:9)γ ). The ﬁrst part is done as in the proof of Theorem 4. For the second part,  N calls  N
with the additional input ¬ f ( (cid:9)γ )
and reverts the answer of N

works  similar  to  the  TM  M of  Theorem 4.  N uses  N

. This shows that the problem is in P

with  NP-oracle.  The  TM  N

[2]

.

(cid:7)

(cid:7)

(cid:7)

(cid:7)

(cid:7)

(cid:2)P
2
(cid:23)

Sketch of hardness. The  hardness  results  are  proven  in  Theorem 21.  The  intuition  is  similar  to  the  one  given  in  Theo-
rem 4 where  the  same  two-player  model  Mφ is  used,  but  the  preference  proﬁles  of  both  players  are  changed.  This  case 
is  technically  more  diﬃcult  as  the  veriﬁcation  problem  consists  of  two  parts:  (i)  ensuring  that  the  set  of  Nash  equilibria 
is  non-empty;  and  (ii)  ensuring  that  all  Nash  equilibria  satisfy  the  outcome  of  the  normative  behavior  function.  For  the 
(cid:3)P

2 -hardness we also need to slightly modify the model Mφ by updating the roles of the veriﬁer and the refuter. (cid:2)

The  next  lemma  shows  that  normative  systems  enjoy  a  small  representation  property:  any  update  of  a  model  by  a 

normative systems can be obtained by a “small” normative system which is polynomial in the size of the model.

Lemma 6. Let I = (M, Prefs, N ) be given where N ∈ {Nr, Ns, Nrs}. For each N ∈ N there is an N
(cid:7)
such that M (cid:2) N = M (cid:2) N

, where k is the number of agents in M.

(cid:7) ∈ N with |N

(cid:7)| ≤ 2 · |Q| · | Actk|

(cid:10)

Proof. Let N be given, comprised of sanctioning norms SN and regimenting norms RN. For each q ∈ Q and  (cid:9)α ∈ Actk we de-
¬p. We add the sanctioning norm (π ∗(q), { (cid:9)α}, Sq, (cid:9)α)
ﬁne Sq, (cid:9)α = SanN(π (q), (cid:9)α) and π ∗(q) as the formula 
(cid:7)
(cid:7)| ≤ |Q| · | Actk|. It remains to show that M (cid:2) N = M (cid:2) N
(cid:7)
. 
to N
(cid:7)
By Proposition 2.4 and the properties of N
. Thus, we can consider sanctioning and 
regimenting norms separately.

if ⊥ /∈ Sq, (cid:9)α , and (π ∗(q), { (cid:9)α}, ⊥) otherwise. We observe that |N

we have that M (cid:2) N

(cid:7) = (M (cid:2) RN

(cid:7)
Firstly, we show that M (cid:2) RN = M (cid:2) RN

that  AppRN(π (q), (cid:9)α) = ∅ iff  AppRN
(cid:7)
with π (q) |= ϕ and  (cid:9)α ∈ A. This implies (π ∗(q), { (cid:9)α}, ⊥) ∈ RN

. Let q and  (cid:9)α be a state and action proﬁle in M, respectively. It suﬃces to show 
(cid:7) (π (q), (cid:9)α) = ∅.  “⇐”:  AppRN(π (q), (cid:9)α) (cid:16)= ∅ implies  that  there  is  an  η = (ϕ, A, ⊥) ∈ RN
(cid:7) (π (q), (cid:9)α) (cid:16)= ∅

(cid:7) (π (q), (cid:9)α) (cid:16)= ∅. “⇒”: AppRN

which implies AppRN

p∈π (q) p ∧

(cid:7)
(cid:7)) (cid:2) SN

p /∈π (q)

(cid:10)

116

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

implies ∃η = (π ∗(q
with π (q) |= π ∗(q
(cid:7)
(cid:7)), { (cid:9)α}, ⊥) ∈ RN
then also π (q) |= ϕ which implies AppRN(π (q), (cid:9)α) (cid:16)= ∅.

(cid:7)). This implies ∃η(cid:7) = (ϕ, A, ⊥) ∈ RN with (cid:9)a ∈ A and π (q

(cid:7)) |= ϕ. But 

Secondly, we show that SanSN

(cid:7) (π (q), (cid:9)α) = SanSN(π (q), (cid:9)α) for all q and (cid:9)α in M whenever AppRN(π (q), (cid:9)α) = ∅:

SanSN

(cid:7) (π (q), (cid:9)α) =

=

=

=

(cid:11)

(cid:11)

(cid:11)

(cid:11)

(cid:7)

(cid:7)

(q

(q

), { (cid:9)α}, Sq(cid:7), (cid:9)α) ∈ AppSN
), { (cid:9)α}, Sq(cid:7), (cid:9)α) ∈ AppSN

{Sq(cid:7), (cid:9)α | (π ∗
{Sq(cid:7), (cid:9)α | (π ∗
{S | (ϕ, A, S) ∈ AppSN(π (q
{S | (ϕ, A, S) ∈ AppSN(π (q), (cid:9)α)}

(cid:7)

(cid:7) (π (q), (cid:9)α)}

(cid:7) (π (q), (cid:9)α), π (q) = π (q
(cid:7)

(cid:7)

)}

), (cid:9)α), π (q) = π (q

)}

= SanSN(π (q), (cid:9)α).

This implies that the updates yield identical models. (cid:2)

Now  we  consider  the  problem  whether  there  is  a  normative  systems  that  weakly  implements  a  normative  behaviour 

function.

Theorem 7 (Weak implementation: existence). Let (I, q) be a pointed NIS, N ∈ {Nrs, Nr, Ns}, N ∈ N , and f be a normative behaviour 
function. The problem whether WIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P

2 -complete.

(cid:7) ∈ WIN E (I, q, f ). We 
Proof. Membership. By Lemma 6, if N ∈ WIN E (I, q, f ) then there is also a “small” normative system N
extend the algorithm described in the proof of Theorem 4 in such a way that the TM M guesses, in addition to the strategy 
proﬁle, a “small” normative system N, and then works as before. Hardness follows from Theorem 4. (cid:2)

For strong implementation it is no longer possible to guess a strategy proﬁle but the normative behaviour function must 
be satisﬁed on all Nash equilibria. Thus, we can use the result of Theorem 5, but ﬁrst a small normative system is guessed.

Theorem 8 (Strong implementation: existence). Let (I, q) be a pointed NIS and N ∈ {Nrs, Ns, Nr}, N ∈ N , and  f be a normative 
behaviour function. The problem whether SIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P

3 -complete.

Proof. Membership. By Lemma 6, if N ∈ SIN E (I, q, f ) then there is also a “small” normative system N
(cid:7)
construct  a  non-deterministic  TM  M which  uses  N
additionally guesses a small normative system N, and then works as N. This shows that the problem is in NP(cid:4)P

(cid:7) ∈ SIN E (I, q, f ). We 
from  Theorem 5 as  oracle  TM.  M works  as  N from  Theorem 5 but 

Sketch of hardness. The  hardness  proof  for  sanctioning  norms  and  for  regimentation  norms  is  given  in  Theorems 25
and 29, respectively, where the basic intuition is similar to the one given in Theorems 4 and 5, the construction is slightly 
more  sophisticated.  As  we  now  reduce QSAT3 to  the  strong  implementation  problem,  we  need  to  include  the  additional 
existential  quantiﬁcation  in QSAT3 in  the  construction.  For  this  purpose  we  encode  a  truth  assignment  of  variables  as  a 
normative system. Guessing such a normative system corresponds to guessing a truth assignment of the newly existentially 
quantiﬁed variables. The diﬃculty in the construction is to ensure that the guessed normative system does not affect “good 
parts”  of  the  model  used  in  the  construction  of  the  two  previous  hardness  results  (Theorems 4 and  5)  because  after  a 
normative  system  has  been  guessed  the  two  players  should  essentially  simulate  the  semantics  of  φ by  guessing  truth 
assignments of the variables under the scope of the other two quantiﬁers in the given QSAT3-formula, followed by playing 
the game theoretic game to evaluate the resulting propositional formula. (cid:2)

3 = (cid:2)P
3 .

Concluding remarks

In this section we analysed the complexity of the weak and strong implementation problem.  The computational  com-
plexity  of  the  membership  problems,  i.e.  whether  a  given  normative  system  weakly  or  strongly  implements  a  normative 
behaviour function, respectively, were shown to be essentially located on the second level of the polynomial hierarchy [55]. 
In [37] it  was  shown  that  checking  the  existence  of  a  pure  Nash  equilibrium  is  NP-complete:  a  strategy  proﬁle  must  be 
guessed and then veriﬁed, in polynomial time, whether it is a Nash equilibrium. The latter check cannot be done in poly-
nomial  time  in  the  setting  considered  here,  as  the  number  of  strategies  of  each  player  is  exponential  in  the  size  of  the 
model;  there  are  | Act||Q|
= NPNP seems  intuitive  for  weak 
implementation (an existential problem: guess a strategy proﬁle with speciﬁc properties and verify it against all deviations) 
= coNPNP for strong implementation (a universal problem). Our results show that the complex-
and a lower bound of (cid:3)P
2
ities of the decision problems considered here are in line with these intuitive bounds: in Theorem 5 we show (cid:2)P
2 as well 
as (cid:3)P
upper bound. Moreover, we show that the weak implementation problem is no more 
diﬃcult than the veriﬁcation of checking that a given normative system weakly implements a normative behaviour function. 
The complexity of the strong implementation problem is located one level up in the polynomial hierarchy. The results do 

many  of  these.  Given  this  observation,  a  lower  bound  of  (cid:2)P
2

2 hardness and prove an P

(cid:2)P
2
(cid:23)

[2]

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

117

also not appear that bad when compared with the complexity results of [69] in the context of Boolean games. The authors 
show that the problem whether there is an taxation scheme which ensures the existence of a pure Nash equilibrium with 
desirable  properties  is  already  (cid:2)P
2 -complete  in  the  pure  setting  of  Boolean  games.  Thus,  we  cannot  hope  for  any  better 
results for our weak implementation problem. That our strong implementation problem is more diﬃcult than its weak ver-
sion, (cid:2)P
3 -complete, stems from the fact that an additional normative system has to be guessed such that all Nash equilibria 
satisfy a speciﬁc property.

5.  Multi-agent environment programs

The model of a multi-agent environment, as proposed in Section 2, is abstract in the sense that it assumes that the set 
of  states  and  the  state  transitions  are  without  an  internal  structure.  In  this  section,  we  propose  a  speciﬁcation  language 
to  program  multi-agent  environments.  The  introduction  of  this  speciﬁcation  language  allows  us  to  program  mechanisms 
and  apply  our  formal  methodology,  as  proposed  in  Section 3,  to  analyse  the  behaviour  of  such  programs.  In  this  way, 
one  can  check  whether  an  environment  program  can  ensure  the  system  designer’s  objectives  given  that  the  participating 
agents  behave  according  to  their  preferences.  The  proposed  language  allows  the  speciﬁcation  of  an  initial  environment 
state as well as a set of synchronized actions. In environment programming, we are agnostic about individual agents and 
how they are programmed. We assume that a set of agents performing synchronized actions in the environment is given. 
These  actions  form  the  input  of  an  environment  program.  The  structural  operational  semantics  of  the  language  speciﬁes 
the  execution  of  programs.  We  show  that  the  proposed  language  can  be  used  to  program  a  broad  class  of  multi-agent 
environments as deﬁned in Section 2. We extend the speciﬁcation language with norms, as deﬁned in Section 3, and present 
its operational semantics. We show that the extended language is expressive enough to program norm-governed multi-agent 
environments. In particular, we show that adding a set of norms to the program of a given multi-agent environment speciﬁes 
the multi-agent environment updated with the set of norms.

In the rest of this section, we follow our abstract model of norm-based multi-agent systems and distinguish hard and 
soft facts. We assume that the states of norm-based environment programs is represented by hard and soft facts. We use 
(cid:2) = (cid:2)h ∪ (cid:2)s to denote the union of disjoint and ﬁnite sets of hard and soft facts (i.e. (cid:2)h ∩ (cid:2)s = ∅), (cid:2)l
= {p, ¬p | p ∈ (cid:2)h}
h
be the set of hard literals, (cid:2)l
s

= {p, ¬p | p ∈ (cid:2)s} be the set of soft literals, and (cid:2)l = (cid:2)l
h

∪ (cid:2)l
s.

5.1.  Programming multi-agent environments

A multi-agent environment can be programmed by specifying its initial state and a set of action proﬁles. The initial state 
of  an  environment  program  is  speciﬁed  by  a  set  of  atomic  propositions  and  the  action  proﬁles  are  speciﬁed  in  terms  of 
pre- and postconditions. The precondition of an action proﬁle is a set of literals that specify the states of the environment 
programs in which the performance of the action proﬁle results in a state transition. The resulting state is determined by 
adding  the  positive  atoms  of  the  action’s  postcondition  to  the  state  in  which  the  action  is  performed  and  removing  the 
negative atoms of the postcondition from it. The pre- and postconditions of action proﬁles are assumed to consist of hard 
facts such that action proﬁles are activated by hard facts and change only those facts of the program states. The performance 
of  an  action  proﬁle  by  individual  agents  in  a  state  that  does  not  satisfy  its  precondition  is  assumed  to  have  no  effect  on 
the  state  and  considered  as  looping  in  that  state.  Please  note  that  in  environment  programming  we  are  agnostic  about 
individual  agents  and  their  internals.  We  assume  that  the  agents  decide  and  perform  synchronized  actions,  and  that  the 
environment program realises the effect of the actions according to their speciﬁed pre- and postconditions. So, it is possible 
that the performance of actions by individual agents do not change the environment state. This is also the case with the 
performance of any action proﬁle that is not speciﬁed by the environment program.

Deﬁnition 15 (Multi-agent environment program). Let Agt = {1, . . . , k} be a set of agents. A multi-agent environment program 
(over a ﬁnite set of propositional symbols (cid:2) as introduced above) is a tuple (F 0, ( Act1, . . . , Actk), Aspec), where

• F 0 ⊆ (cid:2) is the initial state of the environment program,
• Acti is the set of actions of agent i ∈ Agt,
• Aspec ⊆ { (Pre, (cid:9)α, Post)  | (cid:9)α ∈ Act1 × . . . × Actk and  Pre, Post ⊆ (cid:2)l
h

we assume that each action tuple  (cid:9)α can be included in at most one action proﬁle speciﬁcation.

} is a subset of action proﬁle speciﬁcations, where 

We assume that k agents operate in a multi-agent environment such that  (cid:9)α is an action proﬁle of the form (α1, . . . , αk), 
where αi ∈ Acti is the name of the action performed by agent 1 ≤ i ≤ k. It is important to note that pre- and postconditions 
are not assigned to the actions of individual agents, but to action proﬁles. This implies that some possible action proﬁles 
from  Act1 × . . . × Actk may  not  be  speciﬁed  in  Aspec in  which  case  we  assume  that  their  executions  will  not  change 
the  state  of  the  multi-agent  environment  program  and  loop  in  those  states.  Finally,  we  follow  the  convention,  similar  to 
the programming language Prolog [13], to use “_” as a place-holder for any action in an action proﬁle speciﬁcation.12 For 

12 Thus, _ plays the same role as (cid:9) in the context of CGS. We use Prolog’s notation here to emphasize that we are working in a program setting.

118

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

example,  we  use  (Pre, (M, _), Post) to  indicate  that  the  performance  of  action  proﬁles  (M, M) and  (M, W ) in  states  that 
satisfy the precondition Pre results in states that satisﬁes the postcondition Post.

Example 16  (Environment program for the narrowed road).  Let  Agt = {1, 2},  Acti = {M, W } for  i ∈ {1, 2} be  the  actions  that 
can  be  performed  by  the  cars  in  our  running  example,  and  propositional  symbol  p X
i be  interpreted  as  before.  The  run-
}, ( Act1, Act2), {a1, . . . , a5}),  where 
ning  example  can  be  implemented  by  the  multi-agent  environment  program  ({ps
1, ps

2

a1 = (
a2 = (
a3 = (
a4 = (
a5 = (

2

2

{ps
{ps
{ps
{ps
{pe

1, ps
1, ps
1, ps
1, pe
1, ps

2

2

2

},
},
},
},
},

(W , M),
(M, W ),
(M, M),
(M, _),
(_, M),

{pm

2

1

2, ¬ps
{pe
1, ¬ps
{pe
2 , ¬ps
1 , pm
1, ¬ps
{pe
2, ¬ps
{pe

}
}
1, ¬ps
}
}

1

2

1

}

)
)
)
)
)

Note  that  ai

is  used  to  denote  an  action  proﬁle  speciﬁcation,  while  (cid:9)αi denotes  an  action  proﬁle.  Starting  from  the 
initial state of a multi-agent environment program, an execution of the program changes the program state depending on 
the agents’ actions (the input of the environment program). An arbitrary state of an environment program is speciﬁed by 
a  set  of  atomic  propositions  F ⊆ (cid:2).  The  structural  operational  semantics  of  the  multi-agent  environment  programs  are 
speciﬁed  by  a  set  of  transition  rules,  each  speciﬁes  how  the  environment  program  state  changes  when  agents  perform 
actions. Therefore, in the sequel, we use (F , ( Act1, . . . , Actk), Aspec) to denote an environment program in state F . Since the 
agents’ action repertoire and the action speciﬁcations do not change during the execution of the program, we only use the 
state of the environment program  F (instead of (F , ( Act1, . . . , Actk), Aspec)) in the transition rules. We note that not every 
state  F is necessarily reachable from the initial state.

Deﬁnition 16  (Structural operational semantics).  Let  (F , ( Act1, . . . , Actk), Aspec) be  an  environment  program  in  state  F ⊆ (cid:2)
and  (cid:9)α ∈ Act1 × . . . × Actk. For Pre, Post ⊆ (cid:2)l
h we write  F (cid:27) Pre iff Pre ∩ (cid:2) ⊆ F and (Pre \ (cid:2)) ∩ F = ∅ (the precondition Pre
is derivable from the facts  F iff all positive atoms in Pre are in  F and all negative atoms in Pre are not in  F ). Further, we 
deﬁne  F ⊕ Post = (F \ {p | ¬p ∈ Post}) ∪ {p | p ∈ Post} (updating  F with postcondition Post removes negative atoms in Post
from  F and adds positive atoms in Post to  F ). The following three transition rules specify possible execution steps of the 
environment program in state  F .

(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:27) Pre and F

(cid:7) = F ⊕ Post

F

(cid:9)α−→ F (cid:7)
(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:2) Pre
(cid:9)α−→ F
F
: (Pre, (cid:9)α, Post) /∈ Aspec
(cid:9)α−→ F

∀Pre, Post ⊆ (cid:2)l
h

F

The ﬁrst transition rule speciﬁes the execution step of the environment program based on the action proﬁle  (cid:9)α when its 
precondition is satisﬁed. Such an execution step updates the set of facts F with the postcondition of the action proﬁle  (cid:9)α. The 
second transition rule is the same except that it applies when the precondition of  (cid:9)α is not satisﬁed. Such an execution step 
does not change the state of the environment program. Finally, the third rule speciﬁes the execution step of the environment 
program  based  on  an  unspeciﬁed  action  proﬁle  (cid:9)α.  Such  an  execution  step  does  not  change  the  state  of  the  environment 
program.

In  the  following,  we  use  Tbasic to  denote  the  set  of  transition  rules  from  Deﬁnition 16,  trans(F , Act1, . . . , Actk, Aspec,
Tbasic) to denote the set of all transitions that are derivable from transition rules Tbasic based on the environment program 
(F , ( Act1, . . . , Actk), Aspec), and trans( Act1, . . . , Actk, Aspec, Tbasic) to denote the set of all transitions that are derivable from 
transition rules Tbasic based on environment programs (F , ( Act1, . . . , Actk), Aspec) for any  F ⊆ (cid:2). The latter is the set of all 
possible transitions based on transition rules Tbasic, actions  Act1, . . . , Actk, action speciﬁcations Aspec , and all sets of facts 
F ⊆ (cid:2).

An  execution  of  an  environment  program  consists  of  subsequent  execution  steps  resulting  in  a  sequence  of  program 
states. In order to deﬁne the set of all possible executions of an environment program, we ﬁrst deﬁne the set of all possible 
states that can be generated (reached) from an arbitrary state by applying subsequent transitions.

Deﬁnition 17 (Program states). Let  F ⊆ (cid:2) be a set of facts and τ be a set of transitions. The set of states reached from  F by 
subsequent transitions from τ , denoted by gen(τ , F ), is deﬁned as follows:

gen(τ , F ) := {F } ∪

∞(cid:11)

i=1

suci

τ ({F }) where

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

119

(X)) . . .) where X ⊆ P((cid:2)) and

suci

τ (X) = sucτ (. . . (sucτ
(cid:15)

(cid:12)

(cid:13)(cid:14)
i times

sucτ (X) = {F 2 | F 1

(cid:9)α−→ F 2 ∈ τ and F 1 ∈ X}.

Observe that gen(τ , F ) is ﬁnite when τ is ﬁnite. Given an environment program and the set of transition rules Tbasic, the 

set of possible executions of the environment program generates a concurrent game structure (as speciﬁed in Section 2).

Deﬁnition 18 (Programs (cid:4) CGS). Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program, Tbasic be the set of transition 
rules  as  deﬁned  in  Deﬁnition 16 and  τb = trans( Act1, . . . , Actk, Aspec, Tbasic).  The  environment  program  Tbasic-generates  a 
pointed CGS (M, F 0) with M = (Agt, Q, (cid:2), π , Act, d, o) as follows:

• Agt = {1, . . . , k}
• Q = gen(τb, F 0)
• (cid:2) = F 0 ∪ {p | (Pre, (cid:9)α, Post) ∈ Aspec and  p ∈ Post}
• π (F ) = F
• Act = Act1 ∪ . . . ∪ Actk
• di(F ) = Acti
(cid:7)
• o(F , (cid:9)α) = F

for i ∈ Agt and F ∈ Q
for  F , F

for  F ∈ Q

(cid:7) ∈ Q ,  (cid:9)α ∈ d1(F ) × . . . × dk(F ), and  F

(cid:9)α−→ F

(cid:7) ∈ τb

The translation between environment programs and concurrent game structures is restricted to speciﬁc classes of con-
current game structures as speciﬁed in the next deﬁnition. In the following, we use also variables q0, q1, . . . to range over 
the set of states Q .

Deﬁnition 19  (Finite, distinguished, generated CGS).  Let  M = (Agt, Q, (cid:2), π , Act, d, o) be  a  concurrent  game  structure.  We 
introduce the following notation:

• M is called ﬁnite if Q is ﬁnite,
• M is  called  distinguished  if  all  states  differ  in  their  valuations,  i.e.,  for  all q, q

(cid:7) ∈ Q with q (cid:16)= q

(cid:7)

we  have  that π (q) (cid:16)=

π (q

(cid:7)), and

• M is  called  uniform  if  for  all  i ∈ Agt and  all  q, q

(cid:7) ∈ Q we  have  that  d(i, q) = d(i, q

(cid:7)),  i.e.,  the  agents  have  the  same 

options in every state.

The following proposition formulates the relation between environment programs and their corresponding CGSs.

Proposition 9. Let (cid:2) be the set of propositional symbols, (F 0, ( Act1, . . . , Actk), Aspec) be an environment program and (M, F 0) be 
the pointed CGS that is Tbasic-generated by the environment program. If (cid:2) is ﬁnite, then M is ﬁnite. Moreover, M is distinguished and 
uniform.

Proof. The sets of atoms in gen(trans( Act1, . . . , Actk, Aspec, Tbasic), F 0), which determine the set of states in Q , are subsets 
of (cid:2) such there can be only a ﬁnite number of them as (cid:2) is ﬁnite. Moreover, that M is distinguished follows directly from 
the fact that all sets in gen(trans( Act1, . . . , Actk, Aspec, Tbasic), F 0) are different. Finally, that M is uniform follows directly 
from the fact that for each i ∈ Agt it holds: d(i, q) = Acti for all states q ∈ Q , i.e., each agent has one and the same set of 
options in every state. (cid:2)

5.2.  Multi-agent environment programs with norms

The  environment  programs  as  deﬁned  in  the  previous  subsection  do  not  account  for  norms  and  norm  enforcement. 
In  order  to  add  norms  to  multi-agent  environment  programs  and  to  enforce  them  during  program  executions,  we  use 
(sanctioning and regimenting) norms as introduced in Deﬁnition 9. A norm is thus represented as a triple (ϕ, A, S), where 
ϕ is  a  propositional  formula,  A is  a  set  of  action  proﬁles,  and  either  S ⊆ (cid:2)s (sanctioning  norm)  or  S = ⊥ (regimenting 
norm).  In  the  rest  of  this  paper,  we  just  use  the  term  norm  when  the  distinction  between  sanctioning  and  regimenting 
norms is not relevant. Like before, the pre- and postconditions of action proﬁles are assumed to consist of hard facts only 
such that adding norms does not change the activation and effect of action proﬁles. As explained before, throughout this 
section we assume that (cid:2) = (cid:2)h ∪ (cid:2)s is a ﬁnite set of propositional symbols.

Deﬁnition 20  (Norm-based multi-agent environment program).  Let  Agt = {1, . . . , k} be  a  set  of  agents.  A  norm-based  multi-
agent environment program is a tuple (F 0, ( Act1, . . . Actk), Aspec, N), where

120

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

• F 0 ⊆ (cid:2),
• Acti and Aspec are as introduced in Deﬁnition 15, and
• N is a set of (sanctioning and regimenting) norms as introduced in Deﬁnition 9.

Following the narrowed road environment program as speciﬁed in Example 16, the behaviour of cars can be regulated 

by adding norms to the environment program.

Example 17 (Norm-based environment program for the narrowed road). Let ({ps
ment  program  as  speciﬁed  in  Example 16,  and  N = {(ps
1
norms  as  explained  in  Example 10.  The  norm-based  environment  program  ({ps
ments the narrowed road example where norms N are enforced.

2, {(W , W )}, {ﬁne1}),  (ps
1, ps

1, ps

∧ ps

2

2

}, ( Act1, Act2), {a1, . . . , a5}) be an environ-
2, {(_, M)}, {ﬁne2})} be  a  set  of 
1
}, ( Act1, Act2), {a1, . . . , a5}, N) imple-

∧ ps

The  executions  of  norm-based  multi-agent  environment  programs  are  similar  to  the  executions  of  multi-agent  envi-
ronment  programs  (without  norms)  in  the  sense  that  both  update  the  program  states  with  the  effects  of  action  proﬁles. 
However, the execution steps of a norm-based multi-agent environment program proceed by enforcing norms to the result-
ing program states. The enforcement of norms on a program state consists of updating the state with the consequences of 
applicable norms. So, in order to deﬁne the execution steps of norm-based multi-agent environment programs, we need to 
determine the norms that are applicable in a program state and their consequences. For this purpose, we use the function 
SanN( X, (cid:9)α), as deﬁned in Deﬁnition 10, to determine the sanction set that should be imposed when the agents perform ac-
tion proﬁle  (cid:9)α in state  X . Note that the sanction set may be {⊥}, which means that a regimenting norm should be enforced. 
In this case, we have to ensure that the performance of  (cid:9)α does not have any effect on state  X .

We  distinguish  two  general  cases  depending  on  whether  the  sanction  set  is  {⊥} or  not.  When  applicable  norms  are 
sanctioning norms (i.e., resulting in a set of soft facts), we update the resulting program state with the sanctions. We deﬁne 
the  update  of  a  program  state  with  sanctions  as  being  non-cumulative  in  the  sense  that  previous  sanctions  are  removed 
before new sanctions are added. We use the update function ⊗ to update a state with sanctions. Note that in Deﬁnition 21 ⊗
removes all sanctions (soft facts) before adding new ones. This ensures that sanctions become non-cumulative in transitions. 
This  update  function  should  not  be  confused  with  ⊕,  which  is  used  to  update  program  states  with  the  postcondition  of 
action proﬁles.

The effect of an action proﬁle on an environment program state in the context of some existing norms is speciﬁed by 
distinguishing four cases: 1) the action speciﬁcation is given and its precondition is satisﬁed, 2) the action speciﬁcation is 
given but its precondition is not satisﬁed, 3) the action speciﬁcation is not given, and 4) the action proﬁle is regimented 
by some norms. In the ﬁrst case, we update the program state with the postcondition of the action proﬁle and then with 
possible  sanctions,  while  in  the  second  and  third  case  the  program  state  is  updated  only  with  possible  sanctions.  These 
two cases indicate that the performance of an action proﬁle in a program state will be sanctioned (if there is a sanctioning 
norms applicable) even when it has a speciﬁcation but its precondition is not satisﬁed or when the action proﬁle has no 
speciﬁcation.  These  two  cases  capture  the  intuition  that  any  unsuccessful  attempt  to  violate  norms  will  be  sanctioned  as 
well. Note that the sanction set in the ﬁrst three cases can be an empty set if no norm is applicable. Finally, in the fourth 
case, when an action is regimented by some norms, the action will have no effect on the environment state.

Deﬁnition 21 (Structural operational semantics). Let (F , ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program in 
an arbitrary state  F ⊆ (cid:2),  (cid:9)α ∈ Act1 × . . . × Actk, and  N be a set of norms as deﬁned in Deﬁnition 9. Let also ⊕ and (cid:27) be 
deﬁned as in Deﬁnition 16. Finally, let  F ⊗ S = (F \ (cid:2)s) ∪ S for  S ⊆ (cid:2)s. The following four transition rules specify possible 
execution steps of the norm-based environment program.
(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:27) Pre and F

(cid:7) = (F ⊕ Post) ⊗ SanN(F , (cid:9)α) and SanN(F , (cid:9)α) (cid:16)= {⊥}

F
(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:2) Pre and F

(cid:9)α−→ F (cid:7)
(cid:7) = F ⊗ SanN(F , (cid:9)α) and SanN(F , (cid:9)α) (cid:16)= {⊥}
(cid:9)α−→ F (cid:7)

F

∀Pre, Post ⊆ (cid:2)l
h

: (Pre, (cid:9)α, Post) /∈ Aspec and F

(cid:7) = F ⊗ SanN(F , (cid:9)α) and SanN(F , (cid:9)α) (cid:16)= {⊥}

F

(cid:9)α−→ F (cid:7)
SanN(F , (cid:9)α) = {⊥}
(cid:9)α−→ F

F

The ﬁrst transition rule applies when action proﬁle  (cid:9)α is performed, its precondition holds, and no regimenting norms 
are triggered. The second transition rule is the same except that the precondition of  (cid:9)α is not satisﬁed. In this case applicable 
sanctioning norms are enforced without realising the effect (i.e., postcondition) of  (cid:9)α. The third transition rule applies when 
an unspeciﬁed action proﬁle  (cid:9)α is performed and no regimenting norms are triggered. In this case, applicable sanctioning 
norms are enforced. Note that the environment is assumed to be exogenous to agents in the sense that agents decide on 

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

121

which actions to perform independently of the environment speciﬁcation. This allows agents to decide and perform actions 
for which there is no environment speciﬁcation. It is also important to note that a norm speciﬁes that the performance of an 
action proﬁle in a state should be sanctioned, regardless of the speciﬁcation of the action proﬁle. Thus, agents can perform 
an unspeciﬁed action proﬁle for which a sanction may occur. The ﬁrst three transition rules ensure that sanctioning norms 
are enforced regardless of the speciﬁcation of action proﬁles. These transition rules capture the idea that any (successful or 
unsuccessful) attempt to violate norms is sanctioned. Note also that in the ﬁrst three transition rules no sanctioning norm 
needs to be applicable in state  X , i.e., SanN( X, (cid:9)α) can be an empty set indicating that action proﬁle α does not violate any 
norm in state  X . Finally, the fourth transition rule applies when the performance of an action proﬁle triggers regimenting 
norms,  again  regardless  of  the  speciﬁcation  of  action  proﬁles.  It  is  important  to  notice  that  sanctions  do  not  accumulate 
through consecutive states. The following proposition shows that the sanctions imposed on a state (propositional symbols 
from (cid:2)s) are only those caused by the action performed in the previous state.

In the following, we use Tnorm for the set of transition rules from Deﬁnition 21, trans(F , Act1, . . . , Actk, Aspec, N, Tnorm)
to  denote  the  set  of  all  transitions  that  are  derivable  from  transition  rules  Tnorm based  on  the  norm-based  environment 
program  (F , ( Act1, . . . , Actk), Aspec, N),  and  trans( Act1, . . . , Actk, Aspec, N, Tnorm) to  denote  the  set  of  all  transitions  that 
are derivable from transition rules Tnorm based on norm-based environment programs (F , ( Act1, . . . , Actk), Aspec, N) for all 
F ⊆ (cid:2).

Proposition 10. Let (F , ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program in an arbitrary program state F , F
(cid:7) ∈ trans( Act1, . . . , Actk, Aspec, N, Tnorm) and S = SanN(F , (cid:9)α). Then, we have
F

(cid:9)α−→

(cid:5)

(cid:7) ∩ (cid:2)s =

F

S
F ∩ (cid:2)s

if S (cid:16)= {⊥}
otherwise.

Proof. Directly follows from the deﬁnition of ⊗ and the transition rules in Deﬁnition 21. (cid:2)

For a given norm-based environment program, the set of transition rules Tnorm generates a concurrent game structure. 
We use Deﬁnition 18 to deﬁne the concurrent game structure, which is said to be generated by the norm-based environment 
program. Note that Deﬁnition 18 can be applied as norm-based environment programs have the required ingredients such 
as  an  initial  state  F 0,  a  sets  of  actions  for  each  agent,  and  a  set  of  action  proﬁle  speciﬁcations.  There  is,  however,  one 
difference between environment programs and norm-based environment programs which requires a slight modiﬁcation of 
Deﬁnition 18. The difference is that we now assume the set of transitions is trans( Act1, . . . , Actk, Aspec, N, Tnorm), instead of 
trans( Act1, . . . , Actk, Aspec, Tbasic). This means that we use gen(trans( Act1, . . . , Actk, Aspec, N, Tnorm), F 0) to generate the set 
of reachable states, and  F
is derivable 
based on Tnorm. The formal deﬁnitions are as before.

(cid:7) ∈ trans( Act1, . . . , Actk, Aspec, N, Tnorm) to indicate that the transition  F

(cid:9)α−→ F

(cid:9)α−→ F

(cid:7)

Deﬁnition 22  (Norm-based programs (cid:4) CGS).  Let  (F 0, ( Act1, . . . , Actk), Aspec, N) be  a  norm-based  environment  program 
and  τn = trans( Act1, . . . , Actk, Aspec, N, Tnorm).  The  program  is  said  to  Tnorm-generate  a  pointed  CGS (M, F 0),  where 
M = (Agt, Q, (cid:2), π , Act, d, o) is deﬁned as follows:

• Agt, (cid:2), π , Act, d are speciﬁed as in Deﬁnition 18,
• Q = gen(τn, F 0), and
• o(F , (cid:9)α) = F
for  F , F

(cid:7) ∈ Q, (cid:9)α ∈ d1(F ) × . . . × dk(F ) and  F

(cid:7)

(cid:9)α−→ F

(cid:7) ∈ τn.

The class of concurrent game structures generated by norm-based environment programs is characterized by the follow-

ing proposition.

Proposition 11. Let (cid:2) be the set of propositional symbols, (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program, 
and (M, F 0) be the pointed CGS that is Tnorm-generated by it. If (cid:2) is ﬁnite, then M is ﬁnite. Moreover, M is distinguished and uniform.

Proof. Similar to the proof of Proposition 9. Observe that the elements of Q = gen(trans( Act1, . . . , Actk, Aspec, N, Tnorm), F 0)
are subsets of (cid:2) and there are only ﬁnitely many of them when (cid:2) is ﬁnite. Observe also that M is distinguished as the 
elements of Q are distinct and that M is uniform as di(q) = Acti for all i ∈ Agt and q ∈ Q . (cid:2)

The generated CGS does not accumulate sanctions through consecutive states.

Proposition 12. Let (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program that Tnorm-generates (M, F 0), where 
M = (Agt, Q, (cid:2), π , Act, d, o). For F ∈ Q , (cid:9)α ∈ Act1 × . . . × Actk and S = SanN(F , (cid:9)α), we have

122

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

π (o(F , (cid:9)α)) ∩ (cid:2)s =

(cid:5)

S
π (F ) ∩ (cid:2)s

if S (cid:16)= {⊥}
otherwise.

Proof. From  Deﬁnition 22 we  have  o(F , (cid:9)α) = F
know that sanctions are not accumulated through transitions. (cid:2)

iff  F

(cid:7)

(cid:9)α−→ F

(cid:7) ∈ trans( Act1, . . . , Actk, Aspec, N, Tnorm) and  Proposition 10 we 

Note  the  correspondence  between  Proposition 12 and  Proposition 1 (on  page  109).  The  execution  of  a  norm-based 
environment program may generate a different set of states than the execution of the corresponding environment program 
without  norms  does.  This  is  due  to  the  fact  that  the  performance  of  unspeciﬁed  action  proﬁles  or  the  performance  of 
speciﬁed action proﬁles for which the precondition does not hold can now be sanctioned resulting in new states. Note that 
the performance of such an action proﬁle in an environment program without norms results in the same state. Also, the 
application  of  regimenting  norms  may  prevents  reaching  new  states.  These  observations  are  formulated  in  the  following 
proposition.

Proposition 13. Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program, (F 0, ( Act1, . . . , Actk), Aspec, RN ∪ SN) be the en-
vironment program with regimenting norms RN and sanctioning norms SN. Let also τb = trans( Act1, . . . , Actk, Aspec, Tbasic) and 
τn = trans( Act1, . . . , Actk, Aspec, RN ∪ SN, Tnorm). Then, we have:

• When the sets of regimentation and sanctioning norms are empty, i.e., RN ∪ SN = ∅, we have gen(τn, F 0) = gen(τb, F 0).
• When there are only regimentation norms and no sanctioning norms, i.e., RN (cid:16)= ∅ and SN = ∅, we have gen(τn, F 0) ⊆ gen(τb, F 0).
• When  there  are  only  sanctioning  norms  and  no  regimentation  norms,  i.e.,  RN = ∅ and  SN (cid:16)= ∅,  we  have  |gen(τn, F 0)| ≥

|gen(τb, F 0))|.

Proof.

• Since RN ∪ SN = ∅, we have for all F ⊆ (cid:2) and all  (cid:9)α ∈ Act1 × . . . × Actk : SanN(F , (cid:9)α) = ∅. This makes the transition rules 

Tbasic and Tnorm identical.
(cid:3)
l
i=1 suci
τn

(cid:7)

• Let  RN (cid:16)= ∅ and  SN = ∅.  Following  Deﬁnition 17,  we  have  that  gen(τ , F ) := {F } ∪

(cid:9)α−→ F ∈ τn.  If  the  latter  transition  is  obtained  by  a  regimentation  of  (cid:9)α in  F

F ∈ {F 0} ∪
({F 0}) then  also  F ∈ {F 0} ∪
(cid:3)
Suppose the claim holds for l > 1. Let  F ∈ {F 0} ∪
with  F
hypothesis  we  obtain  that  F ∈ {F 0} ∪
F ∈ τb and  thus  F ∈ sucτb ({F
l+1
F ∈ {F 0} ∪
i=1 suci
τb
(cid:9)α−→ F ∈ τb for some  (cid:9)α ∈ Act1 × . . . × Actk, and SanSN(F , (cid:9)α) (cid:16)= ∅ (note that ⊥ /∈ SanSN(F , (cid:9)α)). Then, we have 
• Let F ⊆ (cid:2), F

τ ({F }).  We  show  that  if 
i=1 suci
(cid:3)
({F 0}) by  induction  on  l.  The  base  case,  l = 1,  is  trivial. 
l
i=1 suci
τb
l+1
({F 0})
({F 0}). Then, there must be an  F
i=1 suci
τn
and  by  induction 
(cid:9)α−→
({F 0}),  we  also  obtain  that 

({F 0}).  If  the  action  is  not  regimented,  then  we  also  have  that  F
(cid:3)
l
i=1 suci
τb

(cid:7)}).  Then,  as  by  induction  hypothesis  F

({F 0}). The claim follows.

(cid:3)
l
i=1 suci
τn

l+1
i=1 suci
τb

then  F = F

(cid:7) ∈ {F 0} ∪

(cid:7) ∈ {F 0} ∪

(cid:3)

(cid:3)

(cid:7)

(cid:7)

(cid:7)

(cid:9)α−→ F ⊕ SanSN(F , (cid:9)α) ∈ τn. This implies that  F ⊕ SanSN(F , (cid:9)α) ∈ gen(τn, F 0) but  F ⊕ SanSN(F , (cid:9)α) /∈ gen(τb, F 0). (cid:2)

F

(cid:3)∞

5.3.  Relation between norm-based update and norm-based environment

We now investigate the relation between the concurrent game structure Mn that is generated by a norm-based environ-
ment program, and the concurrent game structure M(cid:7)
that is generated by the corresponding environment program without 
norms (i.e., the same initial state, actions, and action speciﬁcations) which is then updated with the same norms. We ﬁrst 
show that if states qn and q
, respectively, have the same valuation, then the states reached from qn and 
(cid:7)
q

by the same action proﬁle have the same valuation as well. This is formulated by the following lemma.

from Mn and M(cid:7)

(cid:7)

Lemma  14.  Let  (F 0, ( Act1, . . . , Actk), Aspec) be  an  environment  program  that  Tbasic-generates  the  concurrent  game  structure 
(M, F 0) where  M = (Agt, Q, (cid:2), π , Act, d, o).  Let  (F 0, ( Act1, . . . , Actk), Aspec, N) be  a  norm-based  environment  program  that 
Tnorm-generates  the  concurrent  game  structure  (Mn, F 0) where  Mn = (Agt, Qn, (cid:2), π n, Act, dn, on).  Let  M(cid:7) = M (cid:2) N,  where 
M(cid:7) = (Agt, Q
(cid:7)) then 
π n(on(F n, (cid:9)α)) = π (cid:7)(o

and  (cid:9)α ∈ Act1 × . . . × Actk, we have that if π n(F n) = π (cid:7)(F

(cid:7)). For any  F n ∈ Qn,  F

(cid:7), (cid:2), π (cid:7), Act, d

(cid:7), o
(cid:7), (cid:9)α)).

(cid:7) ∈ Q

(cid:7)(F

(cid:7)

Proof. First,  we  note  that  by  Proposition 11 we  have  that  M and  Mn are  uniform.  Therefore,  by  the  deﬁnition  of  norm 
update  given  in  Deﬁnition 11 model M(cid:7)
is  uniform  as  well.  As  a  consequence,  in  all  states  of  all  three  models  all  action 
(cid:7), F n ⊆ (cid:2), or  F n ⊆ (cid:2)
tuples from  Act1 × . . . × Actk are available. Let  F n ∈ Qn,  F
and  F

(cid:7)) = π (cid:7)(F ) ∪ S = F ∪ S. Therefore, we deﬁne:

(cid:7) = (F , S) with  F ⊆ (cid:2)h,  S ⊆ (cid:2)s and π (cid:7)(F

with π n(F n) = π (cid:7)(F

(cid:7)). We note that  F

(cid:7) ∈ Q

(cid:7)

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

123

Fig. 13. Diagram showing the relation between a norm update of a generated CGS and a norm generated CGS.

(cid:5)

G =

F

F

(cid:7)

if F

(cid:7) = (F , S),
otherwise, i.e. F

(cid:7) ⊆ (cid:2)h

Furthermore, we note that for all  (cid:9)α ∈ Act1 × . . . × Actk we have that:  S

(cid:7)), (cid:9)α) and that 
the same action proﬁles are enabled. We show the claim by considering the three cases how a transition can 

∗ := SanN(π n(F n), (cid:9)α) = SanN(π (cid:7)(F

(cid:7)

in  F n and  F
occur according to Deﬁnition 11.

Case S

∗ = ∅.

In that case we have that o

(cid:7), (cid:9)α) ∈ τb. 
As the precondition of  (cid:9)α is independent of soft facts and the ﬁrst three transition rules of Deﬁnitions 16 and 21
are identical for  S

(cid:7), (cid:9)α) ⊆ (cid:2)h which is generated by G 

∗ = ∅, we obtain that on(F n, (cid:9)α) = o

(cid:7), (cid:9)α) = o(G, (cid:9)α), where o

(cid:7)(F

(cid:7)(F

(cid:7)(F

(cid:7)(F

(cid:9)α−→ o

Case S

∗ = {⊥}.

In this case  F n

(cid:9)α−→ F n ∈ τn and on(F n, (cid:9)α) = F n as well as o

. The claim follows because π (cid:7)(F

(cid:7)) =

(cid:7), (cid:9)α). The claim follows.
(cid:7)(F

(cid:7), (cid:9)α) = F

(cid:7)

π n(F n).

Case {⊥} (cid:16)= S

∗ (cid:16)= ∅. We further divide this case according to the ﬁrst three transition rules of Deﬁnitions 16 and 21.

• First  suppose  that  (Pre, (cid:9)α, Post) ∈ Aspec and  F n (cid:27) Pre (and  thus  also  G (cid:27) Pre).  We  have  that  on(F n, (cid:9)α) = (F n ⊕
∗). We obtain that: π n(on(F n, (cid:9)α)) = π n((F n ⊕ Post) ⊗
∗ =
∗ = π n((G ⊕ Post))\(cid:2)s) ∪ S

(cid:7), (cid:9)α) = (G ⊕ Post, S
∗) = π n(((F n ⊕ Post)\(cid:2)s)) ∪ S

. Analogously, we have that o

∗ = π n(G ⊕ Post) ∪ S

(cid:7)(F

∗

Post) ⊗ S
∗) = π n(((F n ⊕ Post)\(cid:2)s) ∪ S
S
π (cid:7)((G ⊕ Post, S

∗)) = π (cid:7)(o

(cid:7)(F

(cid:7), (cid:9)α)).

• The second case, i.e.  F n (cid:2) Pre, follows analogously.
• Also the ﬁnal case, i.e. (Pre, (cid:9)α, Post) /∈ Aspec for any Pre, Post ⊆ (cid:2)h, follows analogously. (cid:2)

As the ﬁnal result of this section, we show that the concurrent game structures generated by an environment program 
with  and  without  norms  are  closely  related.  In  particular,  we  show  that  the  concurrent  game  structure  generated  by  a 
norm-based environment program is isomorphic to the reachable part of the concurrent game structure, which is generated 
by the corresponding environment program without norm, and updated with the same set of norms. We formally introduce 
the notions of reachable states and isomorphism before showing the correspondence result.

Deﬁnition 23 (Reachable states, reachable CGS). Let M = (Agt, Q, (cid:2), π , Act, d, o) be a CGS and q0 ∈ Q . A state q ∈ Q is said to 
be reachable from q0 if there is a path starting in q0 which also contains q. The set of all states reachable from q0 in M
(cid:7))
(cid:7), o
is denoted as  Reachable(M, q0). The reachable part of M from q0, denoted as Mq0 , is the CGS (Agt, Q
where Q

(cid:7) = Reachable(M, q0),  π (cid:7)(q) = π (q),  d

(cid:7), (cid:2), π (cid:7), Act, d
and i ∈ Agt.

(cid:7)(q, (cid:9)α) = o(q, (cid:9)α) for all  (cid:9)α ∈ d

(cid:7)
i(q) = di(q), and o

(cid:7)
Agt(q), q ∈ Q

(cid:7)

A model updated by sanctioning norms can yield states of type (q, S). States in norm generated CGSs, on the other hand, 
have  no  internal  structure;  they  are  plain  sets  of  propositional  symbols.  We  are  less  interested  in  such  purely  syntactic 
differences and need a way to compare models from a semantic perspective. Therefore, we say that two models M1 and 
M2 are isomorphic if they are identical besides the names of the states. The next deﬁnition captures this formally.

Deﬁnition 24 (Isomorphic models). Let Mi = (Agti, Qi, (cid:2)i, πi, Acti, di, oi) for i ∈ {1, 2} be two CGSs. M1 and M2 are isomor-
= Agt2,  (cid:2)1 = (cid:2)2,  Act1 = Act2 and  there  is  a  bijection 
phic,  written  as  M1
f : Q1 → Q2 such that:

∼= M2,  if  the  following  conditions  hold:  Agt1

1. π1(q) = π2( f (q)) for all q ∈ Q1.
2. d1(q) = d2( f (q)) for all q ∈ Q1.
3.

f (o1(q, (cid:9)α)) = o2( f (q), (cid:9)α) for all q ∈ Q1 and  (cid:9)α ∈ d1(q).

Note  that  we  focus  on  the  part  of  the  generated  CGS that  is  reachable  from  the  initial  state  since  the  application  of 
regimenting norms in the operational semantics of norm-based environment programs blocks transitions and thus causes 
some states to become unreachable. Thus, having an environment program (F 0, ( Act1, . . . , Actk), Aspec) and a set of norms 
N, we can now show that the concurrent game structure Mn generated by (F 0, ( Act1, . . . , Actk), Aspec, N) is isomorphic to 
(M (cid:2) N)F 0 , which is the part of the concurrent game structure generated by (F 0, ( Act1, . . . , Actk), Aspec) (i.e., environment 
program without norms) and updated with N, and is reachable from  F 0. This relation is illustrated in the diagram shown in 
Fig. 13 and formulated in the following theorem.

124

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Theorem 15. Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program that Tbasic-generates the pointed concurrent game struc-
ture (M, F 0) with M = (Agt, Q, (cid:2), π , Act, d, o), and (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program that 
Tnorm-generates the pointed concurrent game structure (Mn, F 0) with Mn = (Agt, Qn, (cid:2), π n, Act, dn, on). We deﬁne M(cid:7) = M (cid:2) N
with M(cid:7) = (Agt, Q

(cid:7)). Then, we have that Mn ∼= (M (cid:2) N)F 0 .

(cid:7), (cid:2), π (cid:7), Act, d

(cid:7), o

(cid:7)

(cid:7)

(cid:7)

by  f (F n) = F

if  and  only  if  π n(F n) = π (cid:7)(F

Proof. We  deﬁne  a  (partial)  function  f : Qn → Q
is  well  deﬁned 
we  have  that  π (cid:7)(F 1) (cid:16)= π (cid:7)(F 2) whenever  F 1 (cid:16)= F 2.  This  is  because  a  norm 
follows  from  the  fact  that  for  all  F 1, F 2 ∈ Q
update is only performed once, causing states to be of the form  F ⊆ (cid:2)h or (F , S) with  F ⊆ (cid:2)h and ∅ (cid:16)= S ⊆ (cid:2)s. We show 
that  f constitutes an isomorphism according to Deﬁnition 24 between Mn and (M (cid:2) N)F 0 . Therefore, we have to show that 
is a bijection (and total) and that the three conditions of Deﬁnition 24 are satisﬁed. Condition 1 is true by the deﬁnition 
f
f .  Condition 2  is  true  because  both  models  are  uniform  by  Proposition 9 and  Deﬁnition 11,  and  by  Proposition 11, 
of
(cid:7)( f (q), (cid:9)α) for  all  q ∈ Qn and  (cid:9)α ∈ dn(q), 
respectively.  Thus,  only  the  last  of  these  three  conditions,  i.e.  that  f (on(q, (cid:9)α)) = o
and  Q n consist  of  all  states  which  are  reachable  by  some 
remains  to  be  shown.  By  deﬁnition,  both  sets  of  states  Q
sequence of actions from  F 0. Therefore, we show that the claim holds by induction on the length of an action sequence.

(cid:7)).  That  f

(cid:7)

Base case. We have that  F 0 ∈ Q n ∩ Q

(cid:7)

sequence. By Lemma 14 and as  f (F 0) = F 0 we obtain that π (cid:7)(o
implies that  f (on(F 0, (cid:9)α)) = o

(cid:7)(F 0, (cid:9)α) and thus also  f (on(F 0, (cid:9)α)) = o

, thus,  f (F 0) = F 0 is well deﬁned. Moreover,  F 0 is reached by the empty action 
(cid:7)(F 0, (cid:9)α)) = π n(on(F 0, (cid:9)α)) for any  (cid:9)α ∈ Act1 × . . . × Actk. This 

(cid:7)( f (F 0), (cid:9)α) as desired.

Induction step. We assume that the claim holds for all action sequences of length i. Thus, let  F n ∈ Q n and  F

be 
two  states  reached  after  the  same  action  sequence  of  length  i.  By  induction,  f (F n) = F
and  Condition  3  is  satisﬁed  for 
(cid:7), (cid:9)α)) =
these  states.  Let  (cid:9)α ∈ Act1 × . . . × Actk be  an  arbitrary  action  proﬁle.  Again,  by  Lemma 14 we  obtain  that  π (cid:7)(o
(cid:7), (cid:9)α) is well deﬁned. To establish Condition 3 we consider another arbitrary 
π n(on(F n, (cid:9)α)). Consequently,  f (on(F n, (cid:9)α)) = o
action proﬁle  (cid:9)β ∈ Act1 × . . . × Actk. Because both states are in relation by  f we can once more apply Lemma 14 and obtain 
(cid:7)( f (on(F n, (cid:9)α)), (cid:9)β). 
(cid:7), (cid:9)α), (cid:9)β)) which  implies  that  f (on(on(F n, (cid:9)α), (cid:9)β)) = o
π n(on(on(F n, (cid:9)α), (cid:9)β)) = π (cid:7)(o
Which shows that Condition 3 is satisﬁed.

(cid:7), (cid:9)α), (cid:9)β) = o

(cid:7)(F

(cid:7)(F

(cid:7)(F

(cid:7)(F

(cid:7)(o

(cid:7)(o

(cid:7)

(cid:7) ∈ Q

(cid:7)

In each inductive step we also showed that  f

is well deﬁned and total. This implies that  f

is indeed a bijection as we 

considered arbitrary action sequences and thus made sure that each state from  Q

(cid:7)

is reached. (cid:2)

This theorem shows that the operational semantics of the proposed executable speciﬁcation languages for environments 
with and without norms are aligned with the semantics of norms and norm update, as presented in the ﬁrst part of the 
paper.  This  result  allows  us  to  apply  the  proposed  abstract  mechanism  design  methodology  to  analyse  the  enforcement 
effect of norms in executable environment programs.

Concluding remarks

In this section, an executable speciﬁcation language for the implementation of multi-agent environments was presented. 
The language includes constructs to specify the initial state of multi-agent environments as well as the speciﬁcation of action 
proﬁles in terms of pre- and postconditions. The operational semantics of the proposed speciﬁcation language was presented 
and its relation with concurrent game structures was established. An execution of an environment speciﬁcation initializes 
a  multi-agent  environment,  which  is  subsequently  modiﬁed  by  the  performance  of  the  agents’  actions  and  according  to 
the action speciﬁcations. Subsequently, the speciﬁcation language was extended with norms and sanctions, its operational 
semantics was presented, and its relation with concurrent game structures that are updated with norms was established. 
The  operational  semantics  ensures  that  the  norms  are  enforced  by  means  of  regimentation  or  sanctions.  An  execution 
of  a  norm-based  environment  speciﬁcation  initializes  a  multi-agent  environment  and  effectuates  agents’  actions  in  the 
environment based on the action speciﬁcations, the norms, and their corresponding sanctions. The agents are assumed to 
be aware of the norms and their enforcement. They are also assumed to autonomously decide whether to comply with the 
norms, or to violate them and accept the consequences. The presented executable speciﬁcation language with norms can be 
used in various application domains such as traﬃc or business process management, where the behaviour of autonomous 
agents should be externally controlled and coordinated to be aligned with some laws, rules, policies, or requirements. The 
next section reports on an application of the extended speciﬁcation language in traﬃc simulation, where traﬃc laws (norms) 
are enforced to reduce traﬃc congestion in a ramp merging scenario. We argue that although the executable speciﬁcation 
language is useful for the implementation of traﬃc simulations, there are speciﬁc issues that should be resolved before we 
can apply the proposed norm-based mechanism design methodology to such applications.

6.  Applying norm-based speciﬁcation language in traﬃc simulation

A characteristic feature of the proposed speciﬁcation language for norm-based multi-agent environments is the modu-
larity of norms in the sense that norms can be programmed as a separate module isolated from the speciﬁcation of actions 
and (initial) states of multi-agent environments. This feature allows us to implement different sets of norms and to compare 
their  enforcement  effects  in  one  and  the  same  multi-agent  environment.  The  use  of  the  proposed  speciﬁcation  language 
is  already  illustrated  by  the  running  example  that  is  presented  in  previous  sections  (see  Example 17).  In  this  section  we 
present a more complex and realistic application of the proposed speciﬁcation language.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

125

Fig. 14. Ramp merging traﬃc scenario.

This application concerns the development of norm-based traﬃc environments for SUMO (Simulation of Urban MObil-
ity) [12]. SUMO [46] is a traﬃc simulation platform that supports the modelling of traﬃc, including cars, public transport 
and pedestrians. In this application, SUMO is extended to simulate traﬃc scenarios, where norms and traﬃc laws are ex-
plicitly  speciﬁed  as  input  and  enforced  during  the  simulation  runs.  In  particular,  SUMO  is  extended  with  a  norm-based 
traﬃc controller module that monitors the simulated traﬃc by continuously extracting relevant traﬃc information such as 
the position and speed of the simulated cars from the SUMO platform, instantiates the given input set of traﬃc norms to 
generate traﬃc directives for the observed cars, communicates traﬃc directives to the cars, and imposes payment sanctions 
on the cars that violate their traﬃc directives. In addition to the traﬃc controller module, the standard SUMO car model 
that is responsible for the actual behaviour of individual cars, is extended to allow individual cars to incorporate norms in 
their driving behaviour.

The SUMO extension [12] is used to simulate a ramp merging traﬃc scenario. A schematic representation of the ramp 
merging scenario is illustrated in Fig. 14. In this ﬁgure, triangles represent cars that drive from left to right and rectangles s1
to s5 are sensors that observe the position and speed of cars at various points of the roads. There are two important points 
on the road: m is the point where the roads merge and  e is the ending point of the traﬃc scenario. In order to manage 
traﬃc at the merging point m, observed cars at sensor s1, s2 and s3 receive traﬃc directives from the traﬃc controller. The 
directive  for  a  car  is  generated  based  on  the  given  set  of  traﬃc  norms  and  consists  of  a  velocity  and  a  ﬁne  that  will  be 
imposed  if  the  directive  is  not  followed  by  the  car.  In  this  ﬁgure,  white  cars  have  not  received  their  directives  from  the 
traﬃc controller, while grey cars have received their directives.

An  example  of  a  traﬃc  norm  used  in  this  scenario  is  (x ∧ y, A(v x,v y ), {ﬁnez}) to  be  read  as  “cars  x and  y,  observed 
simultaneously by sensor s2 and s3, are prohibited to have velocities other than respectively v x and v y at the merging point 
m to  avoid  a  ﬁne  of  z Euro”.  For  this  traﬃc  norm,  the  set  of  prohibited  velocities  A(v x,v y ) = {(v 1, v 2) | v 1 (cid:16)= v x and v 2 (cid:16)=
v y}13 represents the obligation that cars  x and  y should have velocities  v x and  v y , respectively, at the merging point m. 
The traﬃc controller instantiates the input norm by determining x,  y,  v x,  v y , and ﬁnez based on the observed cars and the 
properties of the current traﬃc state such as traﬃc density on the roads. The velocities of the observed cars are determined 
in such a way that cars arrives at m with a safe distance dsafe from each other given the current traﬃc density.  The ﬁne 
of  z Euro  will  be  imposed  on  car  x (respectively  y)  if  its  velocity  v x (respectively  v y )  at  m is  not  realised.  Based  on 
the instantiated norm, the traﬃc controller sends to each observed cars a corresponding directive. For example, the traﬃc 
controller sends to the observed car x the directive (v x, ﬁnez), which should be read as “car x should have velocity v x at the 
merge point m to avoid the ﬁne of  z Euro”. For this simulation scenario, the traﬃc controller generates also directives for 
the cars that are observed by one of the two sensors s2 or s3, even if there is no car simultaneously observed at the other 
sensor.

In  this  traﬃc  simulation,  a  car  decides  which  action  (e.g.,  which  velocity  on  which  lane)  to  perform  and  whether  to 
follow or ignore the received directives based on its information and preferences. In particular, the car model is designed by 
means of an action selection function that selects an action that maximizes the car’s utility. The action selection function is 
deﬁned in terms of the car’s internal state (including its current velocity, position, and the received directive), an expected 
arrival time function that, given the current traﬃc situation, determines the impact of a velocity action on the arrival time 
at the car’s destination (for simplicity it is assumed that all cars have the same destination), an action effect function (that 
determines the expected consequence of a velocity action on its internal state), a sanction grading function (mapping ﬁnes 
to real values reﬂecting the utility of a ﬁne), and an arrival grading function (mapping an arrival time to real values reﬂecting 
the utility of the given arrival time at the car’s destination). The internal state of the car together with the expected arrival 
time  function  and  the  action  effect  function  constitute  an  agent’s  information  about  themselves  and  the  traﬃc  situation 

13 We assume that v 1 and v 2 are taken from some sensible ﬁnite set of velocity values.

126

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

including other cars. The sanction grading function and the arrival grading function constitute the preference of the cars. 
The further details of the car model can be found in [12].

The objective of this simulation was to investigate the impact of the enforcement of various norm sets on traﬃc situation 
in the ramp merging scenario for a different population of cars. Two types of cars were distinguished: leisure and business 
cars. These two types were implemented by the sanction and arrival time grading functions. The sanction grading function 
for  a  leisure  car  evaluates  a  sanction  as  having  a  higher  impact  on  the  car’s  utility  compared  to  the  same  function  for  a 
business car. Conversely, the arrival time grading function for a leisure car evaluates a late arrival time as having less impact 
on the car’s utility compared to the same function for a business car. The simulation results show that the number of norm 
violations, and therefore traﬃc congestion, decreases as the severity of ﬁnes increases. In particular, they show that norm 
violations do not occur when proper ﬁnes (i.e., ﬁnes that match the cars’ preferences) are imposed.

The  formal  connection  between  the  proposed  abstract  mechanism  design  methodology  and  the  speciﬁcation  language, 
as established in Proposition 14, may suggest that the results of the mechanism design analysis of norms can be related to 
the results of the implementation of norms in multi-agent simulations. Such a relation can be used to justify the simulation 
results by means of mechanism design explanations, or vice versa, to verify the results of the mechanism design analysis 
by  means  of  simulations.  For  example,  if  a  mechanism  design  analysis  of  the  ramp  merging  scenario  shows  that  a  set  of 
traﬃc  norms  implements  the  system  designer’s  objective  to  avoid  traﬃc  congestion  (i.e.,  to  avoid  simultaneous  arrival  of 
cars at the merging point m) assuming that cars follow their Nash equilibrium strategies, then one can use the speciﬁcation 
language to simulate the scenario in order to verify whether the enforcement of the traﬃc norms avoids traﬃc congestion.
However, connecting theoretical results obtained by a mechanism design analyses and by experimental results obtained 
by running agent-based simulations is not straightforward and requires further considerations. For example, one could argue 
that the reported experimental results from the traﬃc simulation in SUMO can be used to claim that an observed reduction 
of  traﬃc  congestion  is  due  to  the  optimality  of  norms  in  the  sense  that  the  norms  implement  the  objective  of  avoiding 
traﬃc congestion in Nash equilibrium. However, such claims could only be justiﬁed if the simulated cars were capable of 
strategic reasoning, which is not the case in the reported traﬃc simulation experiment. The ability of strategic reasoning for 
cars is not supported by our extension of the SUMO platform and requires further extensions. This is due to the fact that 
the preferences of the cars are not accessible to each other such that the cars do not share the structure of the game and 
are thus unable to reason strategically. This implies that the simulations setting in SUMO is not yet rich enough to establish 
a connection to our mechanism design setting.

It should also be noted that our formal mechanism design methodology can now be applied to analyse only one snapshot 
of  the  traﬃc  simulation,  i.e.,  to  analyse  the  behaviour  of  cars  that  arrive  simultaneously  at  the  sensor  positions.  Such  a 
snapshot of the ramp merging scenario constitutes a game setting that is quite comparable with the setting of our running 
example (the narrow part of the road in the running example is the merging point of the ramp merging scenario). In order to 
connect the mechanism design methodology and the experimental results of the simulations, which consists of a continuous 
stream  of  cars,  one  could  model  the  simulation  as  a  sequence  of  games.  Although  this  may  be  a  reasonable  suggestion, 
one needs to investigate how to model and analyse the change and development of the traﬃc state in consecutive game 
settings. For example, a high stream of cars may necessarily cause the creation of traﬃc congestion at the merging point, 
which  changes  the  state  of  the  traﬃc  and  therefore  the  structure  of  the  consecutive  games.  We  believe  that  a  profound 
connection between mechanism design and simulation settings is a challenging future research direction.

7.  Related work

There  have  been  many  proposals  on  abstract  and  executable  models  for  norms  and  norm  enforcement  in  multi-agent 
systems. However, unlike our work, these proposals focus either on abstract models of norm enforcement or on executable 
models, ignoring the connection between them. Despite this key difference, our abstract and executable model differ from 
existing abstract and executable models, respectively. In the following, we ﬁrst compare our abstract model for norm and 
norm enforcement to existing abstract models, and then compare our executable speciﬁcation language with existing exe-
cutable models for norms and norm enforcement.

Our  abstract  model  of  norms  and  norm  enforcement  is  closely  related  to [1] and [65],  though  they  consider  a  norm 
from a semantic perspective as a set of transitions instead of a syntactic expression as in our case. In particular, they use 
labelled Kripke structures as multi-agent models, supposing that each agent controls a speciﬁc number of transitions. The 
enforcement of a norm is then considered as the deactivation of speciﬁc transitions. In our proposal, we distinguish between 
regimenting  and  sanctioning  norms,  which  is  also  one  of  the  main  conceptual  differences  besides  the  mechanism  design 
perspective we follow. The enforcement of the regimenting norms is similar to the deactivation of transitions as in [1,65], 
but the enforcement of sanctioning norms may create new states resulting from a relabelling by soft facts and thereby new 
transitions.  In  this  sense,  our  approach  is  different  as  the  enforcement  of  norms  may  change  the  underlying  multi-agent 
model with new states and transitions. From a conceptual point of view the agent can still perform the action, the physical 
structure encoded by means of hard facts remains intact, but may have to bear the imposed sanctions depending on the 
agents’ preferences. Note also, the change in the underlying transition system may only affect some agents, namely those 
which use relevant soft facts in their preferences. Another difference to our work is that the outcome of norm enforcement 
is assumed to be independent of the preferences where we consider a more general setting captured in terms of normative 
behaviour functions. It is also important to recall that our focus is of a more practical nature. We try to implement and to 

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

127

analyse mechanisms from a practical point of view, i.e., how to implement and verify norm-based multi-agent environment 
by means of executable speciﬁcations. The work presented in [3] also assumes that the designer has multiple objectives the 
value of which is determined by the (de)activated transitions in the transition system. The authors show how to compactly 
represent the designer’s objectives by (a set of) CTL formulae each assigned a feature value. Then, the utility of a social law 
is the sum of all feature values of the satisﬁed CTL formulae minus the costs to implement the social law. The authors give 
an algorithm to compute optimal social laws as well as complexity results. The focus of the work is on computing a good 
social law from the designer’s perspective, not considering agents’ objectives at ﬁrst place (which is a key concern in our 
setting).

Fitoussi and Tennenholtz [34] put forward a formal framework to engineer and study social laws in multi-agent systems. 
A  social  law  restricts  the  set  of  agents’  actions.  The  authors  investigate  two  properties  of  social  laws:  minimality  and 
simplicity. Minimality refers to the number of restrictions imposed on the system, where simplicity refers to the capabilities 
of agents. Simpler laws can be followed more easily by simpler agents. The authors consider complexity issues about the 
existence  of  appropriate  norms  and  study  their  properties.  The  setting  is  quite  abstract,  using  strategies  similar  to  those 
known from normal form games, whereas we start from a transition system usually requiring multiple step strategies. Also, 
the  authors  consider  two  speciﬁc  types  of  goals,  liveness  and  safety  goals,  whereas  we  allow  arbitrary  LTL-formulae  to 
specify agents’ preferences.

Endriss et al. [30] and Wooldridge et al. [69] propose taxation schemes to incentivize agents to change their behaviour
such that the emerging behaviour is stable and shows desirable properties in line with the system speciﬁcation. In particular, 
the computational complexity of the weak and strong implementation problems is investigated. We have drawn inspiration 
from  these  problems  in  the  present  article.  A  difference  with  our  work  is  that  they  study  these  problems  in  the  context 
of  Boolean  games,  where  we  consider  a  strategic  setting  in  which  agents  act  in  a  temporal  setting.  Instead  of  taxation 
schemes  we  follow  a  norm-based  approach,  and  use  techniques  of  mechanisms  design  to  specify  the  system  designer’s 
objectives  and  analyse  their  implementability.  We  believe  that  this  approach  is  quite  ﬂexible  and  allows  to  model  more 
realistic settings in which the system designer may not know the agents’ preferences and should therefore consider a set of 
possible preferences.

A  different  avenue  of  work  in  this  area  focuses  on  norm  monitoring  in  the  context  of  imperfect  monitors,  e.g.,  [20,8]. 
In the present paper, we have assumed that monitors are perfect in the sense that the norm enforcement mechanism can 
detect and respond to all norm violating behaviours. It should be clear that any work on norm enforcement either implicitly 
or  explicitly  assumes  that  the  behaviour  of  agents  is  monitored  and  evaluated  with  respect  to  a  given  set  of  norms.  Our 
assumption  that  monitors  are  perfect  is  reﬂected  by  the  fact  that  updating  a  multi-agent  model  (i.e.  CGS)  with  a  set  of 
norms covers possible (violating) behaviour. Moreover, although [20] and [8] consider norms syntactically like us, they use 
LTL-formulae as norm representation to refer to good/bad behaviour. We have considered less expressive norms in order not 
to complicate the main message of our approach unnecessarily. We believe that our general approach can be instantiated 
with more expressive norms as well.

Another line of related research concerns the issue of norm synthesis. Shoham and Tennenholtz [59,60], have discussed 
the  problem  of  off-line  design  of  a  set  of  norms  in  order  to  constrain  the  behaviour  of  agents  and  to  ensure  the  overall 
objectives  of  the  multi-agent  system.  In  this  work  and  similar  to  our  approach,  the  structure  of  multi-agent  systems  is 
required and the norms are generated at design time. In line with this tradition, Morales et al. [53,52] consider the problem 
of on-line design of a set of norms to constrain and steer the behaviour of agents. In both off-line and on-line approaches, 
the overall objectives of multi-agent systems are guaranteed by assuming that agents are norm-aware and comply with the 
generated  norms.  In  this  sense  norms  are  considered  as  being  regimented  in  multi-agent  systems.  Moreover,  in  contrast 
to  our  work,  these  approaches  neither  provide  a  game  theoretic  analysis  of  norms  nor  consider  the  generation  of  norms 
with  sanctions  in  the  context  of  agents’  preferences.  However,  we  believe  that  the  concepts  such  as  effective,  necessary, 
and concise norms as introduced by [53,52] can also be used in off-line norm synthesis approaches such as ours. Following 
the  tradition  of  Shoham  and  Tennenholtz,  Boella  and  van der  Torre  [15] consider  the  problem  of  norm  enforcement  by 
distinguishing  the  choice  of  off-line  designed  stable  social  laws  from  the  choice  of  control  systems.  A  control  system  is 
explained to be responsible for monitoring and sanctioning of norm violations. Although the functionality of their proposed 
notion of control system is similar to the functionality of our notion of norm-based mechanism, there are some fundamental 
differences between these approaches. For example, in our approach the behaviour of a multi-agent system is modelled by 
a  concurrent  game  structure  while  they  consider  a  multi-agent  system  in  an  abstract  way  as  a  one-shot  game,  norms  in 
our approach are explicit and enforced by an update mechanism while they consider norms as integrated in the structure 
of the games and enforced by a special agent called normative system that selects which game is going to be played, and 
ﬁnally they consider the notion of (quasi-)stable social laws while we consider norms from a mechanism design perspective 
as implementing a social choice function in Nash equilibria.

Our  work  differs  also  from  veriﬁcation  approaches  to  norm-based  systems  or  protocols  [33,7,27,5].  Of  course,  one  can 
consider and exploit our work as an approach to verify the impact of norm enforcement on agents’ behaviour in the sense 
that our approach can be used to check the inﬂuence of norm enforcement on agents’ behaviour. However, in contrast to 
our approach, the mentioned work focuses on different types of norms and norm enforcement mechanisms, and does not 
provide any game theoretic tool to analyse the impact of norms on the behaviour of rational agents. In particular, in [33]
norms are deﬁned in terms of communication actions and enforced by means of regimentation, while [7] and [27] consider 
norms  as  state-based  obligations/prohibitions  enforced  by  means  of  both  regimentation  and  sanctions.  The  approach  pre-

128

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

sented by [5] focuses mainly on the protocol veriﬁcation and aims at providing a mechanism that can be used, for example 
by the agents, to decide whether following a protocol guarantees their objectives without any norm violation.

+

In the literature of multi-agent systems various proposals focus on the issue of the practical implementation of norms 
and norm enforcement. They consider norms and norm enforcement mechanisms in broader contexts such as institutions, 
organisations, or coordination environments. Examples are electronic institutions such as ISLANDER/AMELI [33,32], organ-
isational models such as MOISE
[42,43] and OperettA [4], coordination models such as ORG4MAS [41], the 
norm-based framework proposed by [24], the law-governed interaction proposed by [51], and the norm enforcement mech-
anism proposed by [35]. The focus on these proposals is primarily on the development of norm-based multi-agent system 
rather  than  devising  special  purpose  programming  languages  to  implement  norms  and  norm  enforcement.  Moreover,  the 
lack of explicit formal syntax and (operational) semantics for norm-related concepts in these approaches makes it diﬃcult, 
if  not  impossible,  to  relate  them  to  the  existing  abstract  models  for  norms  and  norm  enforcement.  These  approaches  are 
concerned  with  agents’  interaction,  use  different  norm  types,  or  focus  on  norm  regimentation  only.  In  the  following,  we 
discuss further details of some of these approaches.

/S–MOISE

+

Another  related  approach  is  MOISE

ISLANDER [31] is  one  of  the  early  modelling  languages  for  specifying  institutions  in  terms  of  institutional  rules  and 
norms. In order to interpret institution speciﬁcations, an execution platform, called AMELI, has been developed [32]. This 
platform implements an infrastructure that, on the one hand, facilitates agent participation within the institutional environ-
ment supporting their communication and, on the other hand, enforces the institutional rules and norms. The distinguishing 
feature of ISLANDER/AMELI is that it does not allow any agent to violate norms, i.e., norms are regimenting. Moreover, norms 
in [32], but also in [36] and [61], are action-based and prescribe actions that should or should not be performed by agents.
[43],  where  a  modelling  language  is  proposed  to  specify  multi-agent  systems 
through  three  organisational  dimensions:  structural,  functional,  and  deontic.  The  relevant  dimension  for  our  work  is 
the  deontic  dimension  that  concerns  concepts  such  as  obligations  and  prohibitions.  Different  computational  frameworks 
have  been  proposed  to  implement  and  execute  MOISE
[42] and  its  artifact-based  ver-
sion  ORG4MAS [41].  These  frameworks  are  concerned  with  norms  prescribing  states  that  should  be  achieved/avoided. 
S–MOISE
is an organisational middleware that provides agents access to the communication layer and the current state 
of the organisation. This middleware allows agents to change the organisation and its speciﬁcation, as long as such changes 
do  not  violate  organisational  constraints.  In  this  sense,  norms  in  S–MOISE
can  be  considered  as  regimenting  norms. 
ORG4MAS uses organisational artifacts to implement speciﬁc components of an organisation such as group and goal schema. 
In this framework, a special artifact, called reputation artifact, is introduced to manage the enforcement of norms.

speciﬁcations,  e.g.,  S–MOISE

+

+

+

+

+

There  are  two  proposals  that  speciﬁcally  aim  at  implementing  norms  and  norm  enforcement.  The  ﬁrst  proposal,  pre-
sented  in [64] and [27],  is  a  norm-based  executable  speciﬁcation  language  designed  to  facilitate  the  implementation  of 
software entities that exogenously control and coordinate the behaviour of agents by means of norms. Similar to our ap-
proach, norms in this proposal can be either sanctioning or regimenting norms. Also, similar to our approach, they come 
with an operational semantics such that executable speciﬁcations can be formally analysed by means of veriﬁcation tech-
niques [11].  However,  in  contrast  to  the  approach  presented  in  the  present  paper,  they  consider  state-based  norms  such 
as obligation or prohibition of states and ignore action-based norms. This proposal comes with an interpreter, called 2OPL, 
which initiates a process that continuously monitors agents’ actions (i.e., communication and environment actions), deter-
mines the effect of the observed actions based on the action speciﬁcations, and enforces norms when necessary. We plan to 
extend 2OPL such that it can interpret and execute action-based norms as presented in the present paper. The second pro-
posal, called JaCaMo [16], aims at supporting the implementation of organisational artifacts, which are responsible for the 
management and enactment of the organisation. An organisational artifact is implemented by a program which implements 
a MOISE
speciﬁcations into norm-based programs is described by [44]. In contrast 
to our approach, the sanctions in JaCaMo are actions that are delegated to some agents and there is no guarantee that the 
agents will eventually perform the actions. In particular, the violation of norms is detected by organisational artifacts after 
which organisational agents have to deal with those violations.

speciﬁcation. A translation of MOISE

+

+

We conclude this section by the following observation. We have assumed that agents are norm aware in the sense that 
agents  follow  their  preferences  and  choose  optimal  behaviours  in  order  to  maximize  their  utilities.  The  norm  awareness 
is  reﬂected  in  our  approach  by  1)  deﬁning  agents’  preferences  in  terms  of  speciﬁc  behaviour  and  whether  the  agents 
incur  sanctions,  and  2)  by  applying  the  equilibrium  analysis  to  characterize  the  behaviour  of  rational  agents  under  norm 
enforcement.  However,  we  did  not  focus  on  how  individual  agents  reason  to  choose  their  optimal  behaviours  as  studied, 
for  example,  by  [66] and [6].  In  contrast  to  [66] and [6],  we  abstract  over  the  speciﬁc  reasoning  schemes  of  individual 
agents  and  assume  that  whatever  reasoning  schemes  individual  agents  use,  they  will  always  act  rationally  according  to 
game theoretic concepts, more precisely according to Nash equilibria.

8.  Conclusions, discussion, and future work

Our  work  focuses  on  norms  and  norm  enforcement  in  multi-agent  systems.  We  proposed  a  formal  methodology  that 
allows analysing norms and norm enforcement from a mechanism design perspective, and a programming model for imple-
menting them. Using game theoretic tools we showed that the enforcement of norms can change the behaviour of rational 
agents using regimentation and sanctions. It is also shown that our presented programming model is aligned with the ab-
stract model such that our developed game theoretical tools can be applied to analyse norm-based environment programs.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

129

Speciﬁcally, we proposed norm-based mechanism design as a formal methodology for analysing norm-based environment 
programs.  We  showed  how  to  abstract  from  a  particular  environment  speciﬁcation  language  and  how  to  apply  methods 
from mechanism design to verify whether the enforcement of a set of norms on a multi-agent environment agrees with the 
behaviour of rational agents that the system designer expects. More precisely, we introduced normative behaviour functions 
for representing the “ideal” behaviour of multi-agent environments with respect to different sets of agents’ preferences. The 
latter enabled us to apply concepts from game theory to identify agents’ rational behaviour. These formal ideas can now be 
used to verify whether the enforcement of a set of norms is suﬃcient to motivate rational agents to act in such a way that 
their behaviour become aligned with that described by the normative behaviour function.

We deﬁned a normative system in such a way that it can modify (soft) facts of the environment states. As the language 
used  for  modelling  agents’  preferences  and  the  facts  in  normative  systems  are  based  on  the  same  set  of  propositional 
symbols,  a  norm-based  mechanism  can  steer  the  behaviour  of  each  agent  in  ﬂexible  ways.  This  notion  of  mechanism  is 
powerful.  A ﬁrst  reﬁnement  could  be  to  identify  a  subset  (cid:2)M ⊆ (cid:2) of  soft facts and  assume  that  a  normative  system  can 
only modify state valuations with respect to this set. Such a mechanism can be much weaker but also more natural.

Another  direction  for  future  research  is  to  consider  robustness against  group deviation.  Our  approach  can  be  extended 
such  that  each  agent a has  its  “own”  set  (cid:2)a of  propositional  symbols  which  is  used  to  specify  its  preference.  If  we  now 
want that some agents are not sensitive to norms and sanctions we simply deﬁne the set (cid:2)N F of facts that are used in a 
normative system such that (cid:2)N F ∩ (cid:2)a = ∅. Another alternative is to take on a more game theoretic point of view in the 
line with [1] and [23]. For example, one may consider partial strategies which assume that only subgroups of agents play 
rationally. Then, the outcome is usually not a single path any more, but rather a set of paths. This gives rise to a notion of 
(S, A)-implementability.

We investigated the problem, given a CGS M and a set of agents’ preferences Prefs, and a normative behaviour function 
f ,  whether  there  is  a  normative  system  N which  N E -implements  f over  M,  q and  Prefs.  In  future  work  it  would  be 
interesting to identify settings in which such normative systems can be constructed eﬃciently. We also plan to extend our 
analysis  to  other  implementability  notions  apart  from  Nash  equilibria  in  more  detail,  e.g.  dominant  strategy  equilibrium 
implementability.

Finally,  yet  another  interesting  direction  for  future  research  is  to  investigate  core  properties  of  classical  mechanism 
design in our norm-based setting, including budget balanced and individual rational mechanisms. We note that already the 
interpretation of these properties in our setting is interesting. For example, in the case of individual rationality it is not clear 
what it means for an agent “not to take part” in the mechanism/in the multi-agent systems. This may require a shift to an 
open MAS where agents can leave an join the system.

Acknowledgement

We thank the anonymous reviewers for their extensive and valuable comments which signiﬁcantly improved the paper.

Appendix A.  Quantiﬁed Boolean satisﬁability problem

Our  hardness  proofs  reduce  validity  of  Boolean  quantiﬁed  formulae  to  the  implementation  problems.  We  consider 
fragments  of  the  Quantiﬁed  Boolean  Satisﬁability  problem  (QSAT),  a  canonical  PSPACE-complete  problem.  Restricting  the 
number  of  quantiﬁer  alternations  of QSAT yields  subproblems  which  are  complete  for  different  levels  of  the  polynomial 
hierarchy.  The  problem  class QSATi starts  with  an  existential  quantiﬁer  and  allows  for  i − 1 quantiﬁer  alternations;  sim-
ilarly,  ∀QSATi-formulae  begin  with  an  universal  quantiﬁer,  i = 1, 2, . . . .  The  previous  problems  are  (cid:2)P
i -complete, 
respectively. In the formal deﬁnition we write  Q X for  Q x1 . . . Q xn for a set  X = {x1, . . . , xn} of propositional variables and 
Q ∈ {∀, ∃}.

i and  (cid:3)P

Deﬁnition 25 (QSATi [55]).  The QSATi problem is deﬁned as follows.
Input: A quantiﬁed Boolean formula (QBF) φ = ∃ X1∀ X2 . . . Q i Xi ϕ where ϕ is a Boolean formula in negation normal form 
(nnf) (i.e., negations occur only at the propositional level) over disjoint sets of propositional variables  X1, . . . , Xi and i ≥ 1
where  Q i = ∀ if i is even, and  Q i = ∃ if i is odd. φ does not contain any free variables.
Output: True if ∃ X1∀ X2 . . . Q i Xi ϕ is valid, and false otherwise.

The problem ∀QSATi is deﬁned analogously but formulae φ start with a universal quantiﬁer and then alternate between 

quantiﬁer types.

By abuse of notation, we refer to a QBF φ that satisﬁes the structural properties required by the problem class QSATi

and ∀QSATi simply as QSATi-formula and ∀QSATi -formula, respectively.

A  truth assignment or  valuation for  a  set  of  variables  X is  a  mapping  v : X → {t, f }.  Given  a  Boolean  formula  ϕ over 
variables  X and  a  truth  assignment  v over  Y ⊆ X we  denote  by ϕ[v] the  formula  obtained  from ϕ where  each  y ∈ Y is 
replaced by ⊥ (falsum) and (cid:13) (verum) if  v( y) = f and  v( y) = t, respectively. For two truth assignments  v 1 and  v 2 over 
X and  Y , respectively, with  X ∩ Y = ∅ we use the notation  v 1 ◦ v 2 to refer to the induced truth assignment over  X ∪ Y ; 
analogously  for  more  than  two  truth  assignments.  Using  this  notation  a  formula  ∃ X1∀ X2 . . . Q i Xi ϕ is  true  iff  there  is  a 

130

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Fig. B.15. Construction of the concurrent game structure for QSAT: value choice section.

truth assignment v 1 over  X1 such that for all truth assignments v 2 over  X2 etc. the Boolean formula ϕ[v 1 ◦ . . . ◦ v i] is valid. 
For further details, we refer to [55].

Appendix B.  Proofs to implementation problems: veriﬁcation

B.1.  Hardness of weak implementation problem: Proposition 4

We  show  that  the  membership  problem  “N ∈ WIN E (I, q, f )”  is  (cid:2)P

2 -hard  by  reducing QSAT2 to  it.  In  the  following  we 

consider an instance of QSAT2 of the form

φ ≡ ∃{x1, . . . , xm}∀{xm+1, . . . , xn}ϕ(x1, . . . , xn)

where  X1 = {x1, . . . , xm} and  X2 = {xm+1, . . . , xn}. The reduction consists of three main steps:

1. We encode φ as a two-player CGS Mφ and show that φ is satisﬁable if, and only if, player one has a winning strategy 

in Mφ to achieve a given property (Lemma 16).
We use results from [22] where it was shown that the satisﬁability of a QBF φ can be reduced to model checking a 
two-player CGS such that one of the players, the veriﬁer v, has a winning strategy to enforce a (ﬁxed) formula which is 
constructed from φ, against all strategies of the other player, the refuter r, if and only if, φ holds.

2. We  construct  a  preference  proﬁle  (cid:9)γ = (γv, γr) such  that  a  winning  strategy  of  the  veriﬁer  in  Mφ is  part  of  a  Nash 

equilibria in (cid:13)(Mφ, q, (γv, γr)) if, and only if, φ is satisﬁable.

3. We  show  that  the  existence  of  such  a  speciﬁc  Nash  equilibria  can  be  answered  by  testing  membership  in  the  weak 

implementation problem.

In the following we assume that the QSAT2-formula φ given above is ﬁxed, including the indexes m and n and that it is 
in  negated  normal  form.  Moreover,  in  the  following  it  is  important  that  X1 and  X2 are  non-empty.  This  can  be  assumed 
without loss of generality.

B.1.1.  The model Mφ

We  describe  the  construction  the  CGS Mφ from  formula  φ.  The  idea  of  the  construction  in [22] is  that  the  veriﬁer
v (controlling  existential  variables  and  disjunctions)  and  the  refuter r (controlling  existential  variables  and  conjunctions) 
ﬁrstly choose truth values of the variables they control. This is illustrated in Fig. B.15.

For  example,  if  v plays  (cid:13) in  q2 a  state  labelled  x2 is  reached;  otherwise,  a  state  labelled  notx2.  This  represents  that 

variable x2 is assigned true or false, respectively. This part of the model is called value choice section and consists of states

Q1 = {qi | i = 1, . . . , n} ∪ {qi v | i = 1, . . . , n and v ∈ {⊥, (cid:13)}}.

States qi with 1 ≤ i ≤ m are controlled by v, states with m + 1 ≤ i ≤ n are controlled by r. Afterwards, both agents simulate 
the game theoretic semantics of propositional logic. Player v tries to make the formula true (thus controls disjunctions) and 
r tries to falsify the formula (thus controls conjunctions). This part of the model corresponds to the parse tree of a formula, 
see Fig. B.16. For the formal deﬁnition we need additional notation. First, we use sf(φ) to denote the set of subformulae of 
φ. For every formula ψ = ψ1 ◦ ψ2 with ◦ ∈ {∧, ∨} we use L(ψ) = ψ1 and R(ψ) = ψ2 to refer to the left and right subformula 
of φ, respectively. If ψ = ¬ψ (cid:7)
. This allows to use a sequence of L’s and 
R’s, i.e. an element from {L, R}∗
, to uniquely refer to a subformula where (cid:14) refers to φ itself. We refer to such a sequence as 
index. For a formula φ we use ind(φ) ⊆ {L, R}∗
to denote the set of all possible indexes wrt. φ. By slight abuse of notation, 
we denote the subformula referred to by such an index also by sf(i) where i ∈ {L, R}∗
. Note, that different indexes can refer 
to the same subformula. For example, given the formula (x1 ∧ x2) ∨ (x2 ∧ ¬x1) we have that sf(L R) = sf(R L) = x2. Note, as 
we stop at the literal level, the indexes  R R L and  R R R are not contained in ind((x1 ∧ x2) ∨ (x2 ∧ ¬x1)).

and ψ is not a literal14 we deﬁne L(ψ) = R(ψ) = ψ (cid:7)

14 We stop at the literal level as it simpliﬁes our construction.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

131

Fig. B.16. CGS for QSAT: formula structure section where sf(ι1), . . . , sf(ιl) are all literals.

Fig. B.17. CGS for QSAT: sections of literals where sf(ι) = xi and sf(ι(cid:7)) = ¬xi .

Fig. B.18. Small gadget which can be used by the two players to make their strategy inconsistent.

Given this notation, the formula structure section of the model consists of states

Q2 = {qι | ι ∈ ind(φ), sf(ι) is not a literal}.

For every index ι where sf(ι) = ψ = ψ1 ◦ ψ2 with ◦ ∈ {∧, ∨} one of the players chooses  L(ψ) = ψ1 or  R(ψ) = ψ2. If ◦ = ∨
the veriﬁer v chooses the subformulae; otherwise the refuter does. The “semantic game” between both players ends up in 
a literal, modelled by a literal state. The section of literals is built over states

Q3 = {qι | ι ∈ ind(φ), sf(ι) is a literal}

for  each  index  ι corresponding  to  a  literal  l in  (cid:20),  we  have  that  the  state  qι is  controlled  by  the  owner  of  the  Boolean 
variable xi in l (i.e. l = xi or l = ¬xi ). As in the value choice section, the owner of that state chooses a value ((cid:13) or ⊥) for 
the underlying variable (not for the literal!) which leads to a new state of the evaluation section. These states are denoted 
by

Q 4 = {qιv | ι ∈ ind(φ), sf(ι) is a literal and v ∈ {(cid:13), ⊥}} and Q 5 = {q(cid:13), q⊥}.

A state qιv with sf(ι) = xi is labelled with the proposition xi if v = (cid:13) and with notxi if v = ⊥; similarly, qιv with sf(ι) = ¬xi
is labelled with the proposition xi if v = ⊥ and with notxi if v = (cid:13). That is, the label v ∈ {(cid:13), ⊥} models the evaluation of the 
literal and not of the underlying variable. These states shall be used to ensure that a strategy induces a truth assignment, 
as further explained below. Then, the system proceeds to the winning state q(cid:13) (labelled with the proposition winv) if the 
valuation of xi makes the literal sf(ι) true, and to the losing state q⊥ (labelled with the proposition winr) otherwise – see 
Fig. B.17 for details. Finally, we need two special gadgets to ensure that the reduction works. First, we connect a state qd
to the initial state q0. This state will be used to ensure the existence of some Nash equilibrium. Secondly, we need to give 
the players a possibility to mark speciﬁc strategies as inconsistent, for reasons which will become clear below. Therefore, we 
insert  a  small  substructure  as  shown  in  Fig. B.18 in-between  the  start  state q0 and q1,  the  beginning  of  the  value-choice 
section. The whole construction is illustrated in Fig. B.19. We refer to the CGS just constructed as Mφ . The formal deﬁnition 
is given next.

Deﬁnition 26 (Model Mφ ). Let QSAT2-formula φ ≡ ∃{x1, . . . , xm}∀{xm+1, . . . , xn}ϕ be given and in negated normal form. We 
deﬁne  Xv = {x1, . . . , xm} and  Xr = {xm+1, . . . , xn}. The CGS Mφ = (Agt, Q, (cid:2), Act, π , d, o) is deﬁned as follows:

132

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Fig. B.19. Concurrent game structure for the QSAT2 instance (cid:20)1 ≡ ∃x1∀x2(x1 ∧ x2) ∨ (x2 ∧ ¬x1). We have that ind(φ1) = {(cid:14), L, R, LL, L R, R L, R R}. In particular, 
we have that: sf((cid:14)) = (x1 ∧ x2) ∨ (x2 ∧ ¬x1), sf(L) = x1 ∧ x2, sf(R) = x2 ∧ ¬x1, sf(LL) = x1, sf(R R) = ¬x1 etc. “Light gray” states are owned by the veriﬁer; 
“dark grey” states are owned by the refuter. “White states” belong to no player. A transition outgoing from a state controlled by v (resp. r) labelled with 
an action α corresponds to an action proﬁle (α, −) (resp. (−, α)). Transitions without label correspond to an action proﬁle (−, −).

• Agt = {v, r},
• Q = {q0, q
• (cid:2) = {xi | i = 1, . . . , n} ∪ {notxi | i = 1, . . . , n} ∪ {winv, winr, start, d, ir, iv},
(cid:7)
0) = π (q
• π (q0) = {start},  π (q

} ∪ Q1 ∪ Q2 ∪ Q3 ∪ Q4 ∪ Q5,

(cid:7)
0, qd} ∪ {qv, q

(cid:7)
v, qr, q

(cid:7)
r

(cid:7)
r) = {ir},  π (qi(cid:13)) = {xi} for  all  qi(cid:13) ∈ Q1,  π (qi⊥) = {notxi} for  all  qi(cid:13) ∈ Q1, 
π (qxi (cid:13)) = {xi} for all qxi (cid:13) ∈ Q4, π (qxi ⊥) = {notxi} for all qxi (cid:13) ∈ Q5, π (q¬xi ⊥) = {xi} for all q¬xi (cid:13) ∈ Q4, π (q¬xi (cid:13)) = {notxi}
for all q¬xi (cid:13) ∈ Q5, π (q(cid:13)) = {winv}, π (qd) = {d} and π (q⊥) = {winr}.

(cid:7)
v) = {iv},  π (q

• Act = {L, R, (cid:13), ⊥, −}
• the function d is deﬁned as
– dv(q0) = dr(q0) = {(cid:13), ⊥}.
– dv(qi) = {(cid:13), ⊥} and dr(qi) = {−} for xi ∈ Xv ∪ {qv}.
– dr(qi) = {(cid:13), ⊥} and dv(qi) = {−} for xi ∈ Xr ∪ {qr}
– dv(qι) = {L, R} and dr(qi) = {−} for all ι ∈ ind(φ) with sf(ι) = ψ1 ∨ ψ2 and sf(ι) not a literal
– dr(qι) = {L, R} and dv(qi) = {−} for all ι ∈ ind(φ) with sf(ι) = ψ1 ∧ ψ2 and sf(ι) not a literal
– dv(q) = dr(q) = {−} for all other states q.

(cid:7)
0, o(q0, (α1, α2)) = qv for (α1, α2) ∈ {((cid:13), ⊥), (⊥, ⊥)}, and o(q

(cid:7)
0, (−, −)) = qv.

• and o is deﬁned as:

– o(q0, ((cid:13), (cid:13))) = qd, o(q0, (⊥, (cid:13))) = q
(cid:7)
(cid:7)
– o(qv, ((cid:13), −)) = q
v, (−, −)) = qr
v, o(qv, (⊥, −)) = qr and o(q
(cid:7)
(cid:7)
r, (−, −)) = q1
r, o(qr, (−, ⊥)) = q1 and o(q
– o(qr, (−, (cid:13))) = q
– o(q, (−, −)) = q for q ∈ {qd, q(cid:13), q⊥}.
– o(qi, (v, −)) = qi v where v ∈ {(cid:13), ⊥} and xi ∈ Xv
– o(qi, (−, v)) = qi v where v ∈ {(cid:13), ⊥} and xi ∈ Xr
– o(qι, (x, −)) = qιx where x ∈ {L, R}, ι ∈ ind(φ) and sf(ι) = ψ1 ∨ ψ2.
– o(qι, (−, x)) = qιx where x ∈ {L, R}, ι ∈ ind(φ) and sf(ι) = ψ1 ∧ ψ2.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

133

– o(qι, ( y, −)) = qι y where  y ∈ {(cid:13), ⊥} and sf(ι) = x ∈ Xv.
– o(qι, ( y, −)) = qιz where  y, z ∈ {(cid:13), ⊥},  y (cid:16)= z and sf(ι) = ¬x with x ∈ Xv.
– o(qι, (−, y)) = qι y where  y ∈ {(cid:13), ⊥} and sf(ι) = x ∈ Xr.
– o(qι, (−, y)) = qιz where  y, z ∈ {(cid:13), ⊥},  y (cid:16)= z and sf(ι) = ¬x with x ∈ Xr.
– o(qi v , (−, −)) = qi+1 for i = 1, . . . , n − 1 and v ∈ {(cid:13), ⊥}
– o(qnv , (−, −)) = qφ for v ∈ {(cid:13), ⊥}
– o(qι(cid:13), (−, −)) = q(cid:13) for sf(ι) a literal.
– o(qι⊥, (−, −)) = q⊥ for sf(ι) a literal.

Each state of Mφ has at most two outgoing transitions, with the exception of state q0 which has four. Hence, the number 
of transitions is polynomial in the number of states. In the next section we describe how to ensure that only strategies of 
the players are taken into consideration which correspond to truth assignments.

B.1.2.  Strategies and assignments

If  a  path  in  the  model  goes  through  a  state  labelled  xi and  notxi this  can  be  interpreted  as  setting  variable  xi true 
and  false,  respectively.  We  observe  that  the  states  which  have  as  children  an  xi-state  or  a  notxi-state  the  transitions  to  a 
successor  is  determined  by  a  single  player  only.  Thus,  a  strategy  of  a  player  completely  determines  which  xi-states  and 
notxi-states, where xi is a variable of the very player, are visited. It may happen that a path contains an xi-state as well as 
a notxi-state, by performing contrary actions in a state qi of the value-choice section and in qι with sf(ι) = xi in the literal 
section. If this happens we call the responsible strategy of the player inconsistent as it does not encode a truth assignment 
of the variables that the player controls. Thus, it has to be ensured that choices are made consistently: the same variable x is 
always assigned true, or always assigned false. For this purpose the following consistency constraints—temporal formulae—are 
introduced for each i ∈ {1, . . . , n}:
Ti ≡ (cid:2)¬xi ∨ (cid:2)¬notxi.

It  is  easy  to  see  that  if  Ti is  true  along  a  path  then  this  path—or  the  associated  strategy  of  the  player  that  controls  xi —
represents a truth assignment of xi . We set
m(cid:16)

Tv ≡ (cid:2)¬iv ∧

Tr ≡ (cid:2)¬ir ∧

Ti

i=1

n(cid:16)

Ti

i=m+1

where the propositions ir and iv are used to indicate that the current strategy of the refuter and veriﬁer, respectively, are 
invalid. The latter is needed to prevent speciﬁc strategy proﬁles to constitute a Nash equilibrium. We call a strategy of the 
veriﬁer v (resp. refuter r) consistent if for all strategies of the refuter (resp. the veriﬁer) the induced outcome path satisﬁes 
Tv (resp. Tr). In the case of consistent choices we observe that the formula structure section together with the sections of 
literals implement the game theoretical semantics of Boolean formula [39].

Next we recall from [22] (with small modiﬁcations) the following lemma which says that φ is satisﬁable iff there is a 
consistent strategy of player v such that for all consistent strategies of player r—again, consistent means that truth values 
are  assigned  consistently  to  variables  and  that  it  is  not  invalid—such  that  eventually  winv holds.  We  emphasize  that  our 
construction does not assume perfect-recall strategies due to the fact the we are only considering one quantiﬁer alternation. 
Also,  note  that  if  q⊥ or  q(cid:13) is  reached,  each  agent  can  switch  from  a  consistent  strategy  to  an  inconsistent  one  without 
changing the outcome paths with the possible exception of the transitions between qv and q1.

Lemma 16. Let a QSAT2-formula φ ≡ ∃{x1, . . . , xm}∀{xm+1, . . . , xn}ϕ(x1, . . . , xn) in negated normal form be given. The model Mφ
can be constructed in polynomial time. We have that φ is satisﬁable iff there is a strategy sv of the veriﬁer v with sv(q0) = ⊥ such that 
for all strategies sr of the refuter r it holds that outMφ (q0, (sv, sr)) |=LTL Tv ∧ (Tr → (cid:3)winv).

Proof. “⇒”: Suppose φ is true and let  sv be the strategy induced by a truth assignment  v of {x1, . . . , xm} witnessing the 
truth  of  φ (i.e.  ϕ[v] is  valid)  and  by  the  simulation  of  the  game  theoretic  semantics  which  witnesses  the  truth  of  ϕ[v]. 
Moreover,  let  sv(q0) = ⊥ (this  is  needed  to  avoid  that  the  refuter  can  reach  the  state  qd )  and  let  sr be  any  consistent 
strategy of  r. Note that the strategy (sv, sr) induces a truth assignment  v 1 ◦ v 2 of  Xv ∪ Xr. Then, (cid:3)winv must be true on 
outMφ (q0, (sv, sr)) otherwise the refuter had a strategy sr which induces a truth assignment v with ϕ[v 1 ◦ v] is false. Which 
would yield a contradiction.

“⇐”:  Let  sv be  a  strategy  of  the  veriﬁer  v with  sv(q0) = ⊥ such  that  for  all  strategies  sr of  the  refuter  r it  holds 
that  outMφ (q0, (sv, sr)) |=LTL Tv ∧ (Tr → (cid:3)winv).  sv induces  a  truth  assignment  v.  For  any  consistent  strategy  sr we  have 
that (cid:3)winv is true. It is straight-forward to check that the truth assignment  v induced by  sv encodes a truth assignment 
witnessing the truth of φ due to the encoded game theoretic semantics in the model. That is, ϕ[v] is valid and thus φ is 
true. (cid:2)

134

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

B.1.3.  Preferences and Nash equilibria

Lemma 16 showed  that  the  satisﬁability  problem  of  a  given QSAT2-formula  φ can  be  reduced  to  a  strategy  ex-
istence  and  model  checking  problem  of  a  rather  simple  formula  in  Mφ .  What  we  need  for  the  reduction  to  the 
weak  implementability  problem  is  that  a  winning  strategy  of  the  veriﬁer  is  part  of  a  Nash  equilibrium  iff  the  for-
mula  φ is  satisﬁable.  For  this  purpose,  we  deﬁne  the  following  preference  lists  for  player  v and  r,  respectively: 

γv = (((cid:12)d, 4),

γr = (((cid:12)d, 4),

(((cid:3)winv ∧ Tv), 3),
((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 2),
((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),
((cid:13), 0)),

((¬(cid:3)winv ∧ Tr ∧ Tv), 3),
((¬(cid:3)winv ∧ ¬Tr ∧ ¬Tv), 2),
(((cid:3)winv ∧ Tr), 1),
((cid:13), 0)).

Now  we  can  relate  the  satisﬁability  of  φ to  the  existence  of  Nash  equilibria  which  contains  a  winning  strategy  of  the 

veriﬁer according to Lemma 16.

Proposition 17 (Nash equilibria in Mφ ). Let φ be a QSAT2-formula in nnf and (γv, γr) be the preference proﬁle deﬁned above.

(a) We have that (sd, s
(b) φ is satisﬁable iff there is an s ∈ N E(Mφ, q0, (γv, γr)) with outMφ (q0, s) |=LTL (cid:3)winv.
(c) Let γ (cid:7)

(cid:7)
d) ∈ N E(Mφ, q0, (γv, γr)) for any two strategies sd and s

(cid:7)
d with sd(q0) = s

r equal γv and γr but with the ﬁrst list entry ((cid:12)d, 4) being removed, respectively. Then, it holds that φ is satisﬁable 

(cid:7)
d(q0) = (cid:13).

v and γ (cid:7)
iff N E(Mφ, q0, (γ (cid:7)

v, γ (cid:7)

r )) (cid:16)= ∅.

Proof.

(a) The strategy proﬁle (sd, s

d) yields the path q0qω
to improve its payoff. It is a Nash equilibrium.

(cid:7)

d . For that path the payoff for both players is four. No player can deviate 

(b) “⇒”:  If  φ holds  then,  according  to  Lemma 16,  there  is  a  strategy  sv such  that  sv(q0) = ⊥ and  for  all  strategies  sr it 
(cid:7)
holds that outMφ (q0, (sv, sr)) |= Tv ∧ (Tr → (cid:3)winv) ((cid:9)). We claim that the proﬁle (sv, s
r
of the refuter is a Nash equilibrium. First, observe that sr(q0) = ⊥, otherwise Tr could not be true on the outcome path. 
(cid:7)
Second, note that such a strategy s
r must exist as r always has a consistent strategy. Given these strategies players v and 
r get a payoff of 3 and 1, respectively. As sr(q0) = sv(q0) = ⊥ no player can deviate to reach state qd. Moreover, player 
r cannot deviate to a consistent strategy making ¬(cid:3)winv true by Lemma 16. This shows that no player can unilaterally 
deviate to increase its payoff.
“⇐”: Suppose φ is false. That is,

(cid:7)
r) for any consistent strategy  s

((cid:9)) for each truth assignment v 1 of {x1, . . . , xm} there is a truth assignment v 2 of {xm+1, . . . , xn} such that ϕ[v 1 ◦ v 2]
is false.

We consider a strategy proﬁle s = (sv, sr) with outMφ (q1, s) |=LTL (cid:3)winv and show that it cannot be a Nash equilibrium. 
For the sake of contradiction, suppose s were a Nash equilibrium. Then, ﬁrst, it must be the case that Tv is true on the 
path; otherwise, player v can increase its utility by deviating to a consistent strategy that still satisﬁes (cid:3)winv (note that 
the strategy of r is ﬁxed and memoryless). Second, we show that player r can always deviate to increase its payoff. If Tr
holds then r gets a payoff of 1. However, by ((cid:9)) the refuter can deviate to a consistent strategy such that the resulting 
path  satisﬁes  ¬(cid:3)winv.  By  doing  so  it  can  increase  its  payoff  to  3.  On  the  other  hand,  if  Tr does  not  hold,  then  the 
refuter gets a payoff of 0 and can again deviate to increase its payoff by ((cid:9)), i.e. by deviating to a consistent strategy 
that satisﬁes ¬(cid:3)winv.

(c) We  consider  the  modiﬁed  preference  list.  If  φ is  satisﬁable  then  N E(Mφ, q0, (γ (cid:7)

r )) (cid:16)= ∅ by  (b).  Now  suppose  φ is 
false. That is, ((cid:9)) holds. We consider a strategy proﬁle s = (sv, sr) and show that it cannot be a Nash equilibrium. Firstly, 
suppose outMφ (q0, (sv, sr)) |= (cid:3)winv. Then, it has already been shown in (b) that s cannot be a Nash equilibrium. Thus, 
we consider the case that λ := outMφ (q0, (sv, sr)) |= ¬(cid:3)winv. We distinguish the four cases which result from players 
playing  consistent  or  inconsistent  strategies.  (i)  Suppose  Tv ∧ Tr holds  on  path  λ.  Then,  v is  better  off  by  playing  an 
inconsistent  strategy  resulting  in  a  payoff  of  1 rather  than  0.  (ii)  If  Tv ∧ ¬Tr is  true  then  r is  better  of  playing  a 
consistent strategy which remains to satisfy ¬(cid:3)winv. This is possible by ((cid:9)). (iii) ¬Tv ∧ Tr is true on λ. Then, player r
can  deviate  to  an  inconsistent  strategy  that  still  makes  ¬(cid:3)winv true  (this  can  be  achieved  by  playing  (cid:13) in  state q0). 
This increases the player’s payoff from 0 to 2. (iv) Suppose the path satisﬁes ¬Tv ∧ ¬Tr. Then, player v is better off to 
deviate to a consistent strategy. This shows that there cannot be any Nash equilibrium. (cid:2)

v, γ (cid:7)

Theorem 18 (Hardness of weak implementation: veriﬁcation). Let (I, q) be a pointed NIS and N a normative system included in I. The 
problem whether N ∈ WIN E (I, q, f ) is (cid:2)P

2 -hard in the size of M, N and Prefs.

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

135

2 -hardness  is  shown  by  a  reduction  to  the  weak  implementability  problem  of  f ((γv, γr)) = (cid:3)winv.  Given  a
Proof. (cid:2)P
QSAT2-formula  φ in  nnf  we  construct  the  model  Mφ ,  which  can  be  done  in  polynomial  time  according  to  Lemma 16, 
and deﬁne I = (Mφ, {(γv, γr)}, {N(cid:14) }). Now, by Proposition 17(b) we have that: φ is satisﬁable iff ∃s ∈ N E(Mφ, q0, (γv, γr))
such that out(q0, s) |= (cid:3)winv. This equivalence, however, is equivalent to N(cid:14) ∈ WIN E (I, q0, f ). (cid:2)

B.2.  Hardness of strong implementability: Theorem 5

The strong implementation problem requires checking two parts: that the set of Nash equilibria is non-empty and that 
all Nash equilibria satisfy speciﬁc properties. We show that the latter one gives rise to (cid:3)P
2 -hardness. In order to be able to 
use our model Mφ for a reduction of ∀QSAT2, we need to switch the roles of the veriﬁer and refuter. That is, the refuter 
controls variables in {x1, . . . , xm} and the veriﬁer in {xm+1, . . . , xn}. This revised model denoted by  (cid:17)Mφ and will later also be 
used for the problem of the existence of an appropriate normative system. This reduction requires an additional labelling of 
some states, modelling the guessing of a normative system.

Deﬁnition 27 (The model  (cid:17)Mφ ). Given a ∀QSAT2-formula φ ≡ ∀{x1, . . . , xm}∃{xm+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we deﬁne the 
CGS (cid:17)Mφ analogously to Mφ but  Xr = {x1, . . . , xm} and  Xv = {xm+1, . . . , xn}. We further label q0 and each state reachable in 
an even number of transitions from q0 with a new proposition t.

The labelling  t will later be used for ensuring that the structure of  (cid:17)Mφ is not affected by a norm-based update, more 
precisely that no transitions are regimented making for instance the winning state of the veriﬁer unreachable. The proof of 
the following lemma is done analogously to the one of Lemma 16.

Lemma 19. Given a ∀QSAT2-formula φ ≡ ∀{x1, . . . , xm}∃{xm+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we have that φ is satisﬁable iff for all 
strategies sr of refuter r with sr(q0) = ⊥ there is a strategy sv of veriﬁer v such that out (cid:18)Mφ (q0, (sv, sr)) |=LTL Tr → (Tv ∧ (cid:3)winv).

Proof. [Sketch] “⇒”: Suppose φ is true. For each truth assignment  v of  Xr let  w v denote a truth assignment of  Xv such 
that ϕ[v ◦ w v ] is valid. Let  sr denote the strategy induced by  v and let  ssr
v be a consistent strategy induced by  w v which 
witnesses the truth of φ. Thus, (cid:3)winv must be true on out (cid:18)Mφ (q0, (ssr
v , sr)) otherwise the refuter had a consistent strategy 
for which there is no consistent strategy ssr
v such that out (cid:18)Mφ (q0, (ssr
v , sr)) (cid:16)|=LTL (cid:3)winv. This would imply that there is an  v
such that for all  w v , ϕ[v ◦ w v ] is false. Contradiction.

“⇐”:  Let  ssr

v be  a  consistent  strategy  of  the  veriﬁer  v which  witnesses  the  truth  of  the  formulae  when  the  refuter 
v , sr)) |=LTL (cid:3)winv. The strategies induce the truth assignments  v and  w v , 

plays the consistent strategy sr, i.e. out (cid:18)Mφ (q0, (ssr
respectively (using the same notation as above). It is straight-forward to check that ϕ[v ◦ w v ] is true. (cid:2)

The next lemma shows that we can use the previous result to reduce the truth of a ∀QSAT2 instance to a property over 

all Nash equilibria in  (cid:17)Mφ using a slightly modiﬁed variant of the preference lists of player v used before. We deﬁne

(cid:17)γv = (((cid:12)d, 5),

(((cid:3)winv ∧ Tv), 4),
((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 3),
((¬(cid:3)winv ∧ Tr ∧ Tv), 2),
((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),
((cid:13), 0)),

and (cid:17)γr = γr.

Proposition 20 (Nash equilibria in (cid:17)Mφ ). Let φ be a ∀QSAT2-formula in nnf.
d) ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) for any strategies sd and s
(a) We have that (sd, s
(b) φ is satisﬁable iff for all s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) it holds that out(q0, s) |=LTL (cid:3)winv ∨ (cid:12)d.

(cid:7)
d with sd(q0) = s

(cid:7)

(cid:7)
d(q0) = (cid:13).

Proof.

(a) Analogously to Proposition 17(a).
(b) “⇒”:  If  φ holds  then,  according  to  Lemma 19,  for  all  strategies  sr with  sr(q0) = ⊥ there  is  a  strategy  sv such 
that  out (cid:18)Mφ (q0, (sv, sr))) |= Tr → (Tv ∧ (cid:3)winv) ((cid:9)).  Now,  suppose  there  was  a  Nash  equilibrium  s = (sv, sr) with 
out(cid:18)Mφ (q0, (sv, sr)) |= ¬(cid:3)winv ∧ ¬ (cid:12) d.  We  consider  the  following  cases.  First,  assume  that  out (cid:18)Mφ (q0, (sv, sr)) |= Tr. 

136

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

Then,  by  ((cid:9)) the  veriﬁer  could  deviate  from  sv to  obtain  a  path  satisfying  Tv ∧ (cid:3)winv.  This  shows  that  (sv, sr) is  not 
a  Nash  equilibrium.  Second,  assume  that  out (cid:18)Mφ (q0, (sv, sr)) |= ¬Tr ∧ Tv.  In  that  case,  the  refuter  would  be  better  off 
playing a consistent strategy. Thirdly, assume that out (cid:18)Mφ (q0, (sv, sr)) |= ¬Tr ∧ ¬Tv. In that case the veriﬁer would be 
better  off  switching  to  a  consistent  strategy.  Also  note  that  not  both  players  can  play  (cid:13) in  q0 as  ¬ (cid:12) d holds.  This 
shows that for all s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)), out (cid:18)Mφ (q0, s) |=LTL (cid:3)winv ∨ (cid:12)d.
“⇐”: Suppose φ is false. We need to show that there is a Nash equilibrium  s such that out (cid:18)Mφ (q0, s) |=LTL ¬(cid:3)winv ∧
¬ (cid:12)d. By Lemma 19 there is a strategy sr with sr(q0) = ⊥ such that for all strategies sv we have out (cid:18)Mφ (q0, (sv, sr)) |=LTL
(cid:7)
(cid:7)
v, sr)) |=LTL Tv and 
Tr ∧ (Tv → ¬(cid:3)winv) (by  contraposition).  Now,  let  s
v be  any  strategy  such  that  out (cid:18)Mφ (q0, (s
v) ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)).  By  Lemma 19,  the  veriﬁer  cannot  change  its  strategy  to 
(cid:7)
v(q0) = ⊥.  We  show  that  (sr, s
s
(cid:7)
ensure (cid:3)winv. Moreover, as  sr(q0) = ⊥ the veriﬁer v cannot deviate to increase its payoff. Analogously, as  s
v(q0) = ⊥, 
(cid:7)
v) is indeed a Nash equilibrium. Thus we 
the refuter cannot deviate from sr to increase its payoff. This shows that (sr, s
have that there is an s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) with out (cid:18)Mφ (q0, s) |=LTL ¬(cid:3)winv ∧ ¬ (cid:12) d. (cid:2)

(cid:7)

Theorem 21 (Hardness of strong implementation: veriﬁcation). Let (I, q) be a pointed NIS and N a normative system included in I. 
The problem whether N ∈ SIN E (I, q, f ) is (cid:2)P

2 -hard in the size of M, N and Prefs.

2 -hard as well as (cid:3)P

Proof. (cid:2)P
polynomial time such that: φ is satisﬁable iff N E(Mφ, q0, ((cid:17)γv, (cid:17)γr)) (cid:16)= ∅. This is equivalent to

2 -hardness. By Lemma 16 and Proposition 17(c) we can construct a model Mφ from a QSAT2-formula φ in nnf in 

N E(Mφ, q0, ((cid:17)γv, (cid:17)γr)) (cid:16)= ∅ and ∀s ∈ N E(Mφ, q0, ((cid:17)γv, (cid:17)γr)) : outMφ (q0, s) |=LTL (cid:13) iff N(cid:14) ∈ SIN E (I, q0, f )

for  f (((cid:17)γv, (cid:17)γr)) = (cid:13).

(cid:3)P

2 -hardness. We  reduce  ∀QSAT2 to  the  strong  implementation  problem  for  f (((cid:17)γv, (cid:17)γr)) = (cid:3)winv ∨ (cid:12)d.  By  Lemma 19

and Proposition 20 we can construct a model  (cid:17)Mφ from a ∀QSAT2-formula φ in polynomial time such that:

φ is satisﬁable

iff ∀s ∈ N E( (cid:17)Mφ, q0, ( (cid:17)γv, (cid:17)γr)) : out(cid:18)Mφ (q0, s) |=LTL (cid:3)winv ∨ (cid:12)d (Proposition 20(b))
iff ∀s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) : out(cid:18)Mφ (q0, s) |=LTL (cid:3)winv ∨ (cid:12)d and N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) (cid:16)= ∅ (Proposition 20(a))
iff N(cid:14) ∈ SIN E (I, q0, f )

(cid:2)

Appendix C.  Proofs of implementation problems: existence

In  the  following  we  consider  the  hardness  proof  of  the  existence  problem  stated  in  Proposition 8.  The  proof  that  the 
problem is (cid:2)P
3 -hard requires a further technical sophistication. The basic idea is that the normative system is used to sim-
ulate the ﬁrst existential quantiﬁcation in a QSAT3-formula. Firstly, we show how this can be achieved by using sanctioning 
norms and afterwards by using regimenting norms.

C.1.  Sanctioning and regimentation norms

For the hardness part we reduce QSAT3. Therefore, we consider the QSAT3-formula

φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn)

in  nnf  and  show  that  φ is  true  iff  SIN E (I, q, f ) = ∅ for  appropriate  I,  q,  and  f .  The  idea  of  the  reduction  extends  the 
(cid:17)φ from  the  previous  section  where  (cid:17)φ is  a 
reduction  given  in  the  previous  section.  Essentially,  we  use  the  construction  (cid:17)M
modiﬁed version of the QSAT3-formula φ. The refuter r additionally controls the literal states referring to the new variables 
in {x1, . . . , xr}. In order to use our previous construction, we deﬁne (cid:17)φ as the formula obtained from φ as follows:

(cid:17)φ ≡ ∀{x1, . . . , xr, xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn)

That  is,  the  ﬁrst  existentially  quantiﬁed  variable  are  moved  to  the  universal  part  controlled  by  the  refuter.  A  sanctioning 
(cid:7)
norm is used to deﬁne a truth assignment of the variables x1, . . . , xr . For this purpose, a sanctioning norm labels states q
0
and qv with new propositions representing the truth assignment. For illustration assume that the sanctioning norm should 
encode a truth assignment which assigns true to x1, . . . , xg and false to xg+1, . . . , xr with 1 ≤ g ≤ r. This can be modelled 
by the normative system:

N = {({start}, {((cid:9)1, (cid:9)2) | (cid:9)1, (cid:9)2 ∈ {(cid:13), ⊥}, not (cid:9)1 = (cid:9)2 = (cid:13)}, {x1, . . . , xg, notxg+1, . . . , notxr)}.

Then,  in  the  model  (cid:17)M
consider (cid:17)φ the consistency constraint Tr of r in  (cid:17)M

(cid:17)φ (cid:2) N,  states  q

(cid:17)φ has the form:

(cid:7)
0 and  qv are  additionally  labelled  with  {x1, . . . , xg, notxg+1, . . . , notxr}.  Given  that  we 

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

137

Tr ≡ (cid:2)¬ir ∧

r+m(cid:16)

Ti ∧

r(cid:16)

((cid:2)¬xi ∨ (cid:2)¬notxi).

i=r+1

i=1

We also deﬁne the formula

(cid:19)

asgn ≡

(cid:12)

(cid:21)
(xi ∨ notxi) ∧ ¬(xi ∧ notxi)

r(cid:16)

(cid:20)

i=1

(cid:22)

∨ (cid:12)d

(cid:7)
expressing that the next state is either qd, or q
0 or qv each of the two labelled with a representation of a truth assignment 
for  the  variables  {x1, . . . , xr},  assuming  that  the  current  state  is  q0.  These  formulae  suﬃce  if  we  were  only  given  sanc-
tioning  norms.  In  the  presence  of  regimenting  norms,  however,  it  has  also  to  be  ensured  that  regimenting  norms  do  not 
regiment  transitions  invalidating  the  structure  of  the  model.  For  example,  the  transitions  leading  to  state  q(cid:13) may  simply 
be regimented which makes it impossible for the veriﬁer to win. The idea is to introduce a formula which “forbids” such 
undesirable normative systems. Therefore, we deﬁne the formula
(cid:21)
(t → (cid:12)¬t) ∧ (¬t → (cid:12)t) ∨ (cid:12)(d ∨ winv ∨ winr)

tick = (cid:2)

(cid:20)

.

It describes that a path does not loop in states other than the already looping states qd , q(cid:13), and q⊥. Now, we can prove 

the following result:

Lemma 22. Let N ∈ Nrs be a normative system such that for all paths λ ∈ (cid:8)(cid:18)M
regimentation norm which is applicable in a state Q\{qd, q⊥, q(cid:13)}.

(cid:17)φ (cid:2)N(q0) it holds that λ |= tick. Then, N cannot contain a 

Proof. Suppose that an outgoing transition in a state q ∈ Q\{qd, q⊥, q(cid:13)} is regimented. Then, there is a loop from q to q. 
Let λ denote the path from q0 to q followed by qω. Clearly, the formula tick is violated on that path which contradicts the 
assumption. (cid:2)

Lemma  23.  Given  a  QSAT3-formula  φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn) in  nnf  we  have  that  φ
is  satisﬁable  iff  there  is  an  N ∈ Nrs such  that  for  all  strategies  sr of  r with  sr(q0) = ⊥ there  is  a  strategy  sv of  v such  that 
(cid:17)φ (cid:2)N(q0) we have that λ |= ass ∧ tick. Moreover, for the 
out(cid:18)M
direction “⇒” we can always ﬁnd a normative system in Ns.

(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL (Tr → (Tv ∧ (cid:3)winv)) and for all paths λ ∈ (cid:8)(cid:18)M

Proof. “⇒”:  Suppose  φ holds.  Let  v be  a  witnessing  truth  assignment  of  the  variables  {x1, . . . , xr}.  Then,  φ(cid:7) ≡
∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . xn}ϕ(x1, . . . , xn)[v] is satisﬁable. By Lemma 19, we have that for all sr with sr(q0) = ⊥ there 
is an sv such that

((cid:9)) out(cid:18)Mφ(cid:7) (q0, (sv, sr)) |=LTL Tr → (Tv ∧ (cid:3)winv).

Now let

N = {({start}, {((cid:13), ⊥), (⊥, (cid:13)), (⊥, ⊥)}, {xi1 , . . . , xig , notxig+1 , . . . , notxir )}
= . . . = v ir

= . . . = v i g

= t and v i g+1

= f where (i1, . . . , ir) is a permutation of (1, . . . , r). Then, we have that for 
such that v i1
all paths λ ∈ (cid:8)(cid:18)M
(cid:17)φ (cid:2)N(q0), π (λ) |=LTL ass ∧ tick. The claim follows by ((cid:9)) applied on ϕ[v] as the normative system essentially 
ﬁxes  the  choices  of  the  refuter  for  all  variables  {x1, . . . , xr};  every  deviation  of  these  induced  truth  values  of  the  refuter 
would result in an inconsistent strategy.

(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL (Tr → (Tv ∧ (cid:3)winv)) and for all paths λ ∈ (cid:8)(cid:18)M

“⇐”:  Let  N ∈ Nrs such  that  for  all  strategies  sr with  sr(q0) = ⊥ of  r there  is  a  strategy  sv of  v such  that 
(cid:17)φ (cid:2)N(q0) we have that λ |= ass ∧ tick. By Lemma 22
out(cid:18)M
(cid:7)
no transitions can be regimented apart from those starting in qd , q⊥, and q(cid:13). The valuation of q
0 and of qv induce truth 
assignments  v 1 and  v 2 of {x1, . . . , xr}, respectively. We can choose N in such a way that  v 1 = v 2. This is the case because 
for all strategies sr of r with sr(q0) = ⊥ there is a strategies sv of v such that out (cid:18)M
(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL (Tr → (Tv ∧ (cid:3)winv)), 
we also have that ∀{xr+1, . . . xr+m}∃{xr+m+1, . . . , xn}ϕ[v i] is true for i ∈ {1, 2} (cf. Lemma 19). The claim follows. (cid:2)

Now,  we  can  present  our  reduction  to  the  implementation  problem.  Again,  we  need  to  slightly  modify  the  players’ 

preference lists:
(cid:17)(cid:17)γ v

= (((cid:12)d ∨ ¬tick ∨ ¬ass, 5),

(((cid:3)winv ∧ Tv), 4),
((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 3),
((¬(cid:3)winv ∧ Tr ∧ Tv), 2),

138

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

(cid:17)(cid:17)γ r

((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),
((cid:13), 0)),

= (((cid:12)d ∨ ¬tick ∨ ¬ass, 4),
((¬(cid:3)winv ∧ Tr ∧ Tv), 3),
((¬(cid:3)winv ∧ ¬Tr ∧ ¬Tv), 2),
(((cid:3)winv ∧ Tr), 1),
((cid:13), 0)).

Proposition 24. Let φ be a QSAT3-formula in nnf.

(a) For any N ∈ Nrs, we have that (sd, s
(b) φ is satisﬁable iff there is an N ∈ Nrs such that for all s ∈ N E( (cid:17)M

d) ∈ N E( (cid:17)M

(cid:7)

(c) φ is satisﬁable iff there is an N ∈ Ns such that for all s ∈ N E( (cid:17)M

(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) for any strategy sd and sd(cid:7) with sd(q0) = s
(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) with out (cid:18)M
(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) with out (cid:18)M

(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧

(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧

(cid:7)
d(q0) = (cid:13).

ass ∧ tick.

ass ∧ tick.

Proof.

(a) The action proﬁle (sd, s

d which satisﬁes (cid:12)d if the transition ((cid:13), (cid:13)) is not regimented, and the 
0 which satisﬁes ¬tick if it is regimented. On both paths the payoff for both players is maximal. No player can 

d) yields the path q0qω

(cid:7)

path qω
improve its payoff.

(cid:17)φ (cid:2)N(q0, s) |=LTL (ass ∧ tick) → (¬(cid:3)winv ∧ ¬ (cid:12) d).

(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL Tr → (Tv ∧ (cid:3)winv) and  for  all  paths  λ ∈ (cid:8)(cid:18)M

(b) “⇒”:  Suppose  φ holds.  By  Lemma 23 there  is  an  N ∈ Ns such  that  for  all  strategies  sr of  r with  sr(q0) = ⊥ there  is 
(cid:17)φ (cid:2)N(q0) we  have 
a  strategy  sv of  v such  that  out (cid:18)M
(cid:17)φ (cid:2)N(q0, s) |=LTL ass ∧ tick →
that λ |= ass ∧ tick. Now suppose that there was a Nash equilibrium (sv, sr) such that out (cid:18)M
(¬(cid:3)winv ∧ ¬ (cid:12) d).  As  ass ∧ tick is  true  on  all  paths,  this  means  that  out (cid:18)M
(cid:17)φ (cid:2)N(q0, s) |=LTL (¬(cid:3)winv ∧ ¬ (cid:12) d) for  this 
Nash equilibrium. We can use the same reasoning as in the proof of Proposition 20(b) to obtain a contradiction. Hence, 
such a Nash equilibrium cannot exist.
“⇐”: Suppose φ does not hold. We have to show that for all N ∈ Nrs there is an s ∈ N E( (cid:17)M
((cid:9)) out(cid:18)M
Firstly, suppose that  (cid:17)M
(cid:17)φ (cid:2)N(q0) with λ |= ¬tick. This path can be generated by some strategy 
proﬁle s which satisﬁes ((cid:9)), as the antecedent of ((cid:9)) will be false. As this strategy proﬁle gives maximal utility it is a 
Nash equilibrium. Thus, from now on we can assume that all paths in the norm updated model satisfy tick. Completely 
analogous, we can also assume that ass is satisﬁed in the updated models.
(cid:17)φ (cid:2) N such that all paths λ ∈ (cid:8)(cid:18)M
Secondly, consider  (cid:17)M
(cid:17)φ (cid:2)N(q0) satisfy tick ∧ ass. As φ does not hold, by Lemma 23 and 
our assumption about the updated models we have that for all N ∈ Nrs there is a strategy sr of r with sr(q0) = ⊥ such 
(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL Tr ∧ (Tv → ¬(cid:3)winv). The rest of the proof follows analogously 
that for all strategies sv of v, out (cid:18)M
(cid:7)
(cid:7)
v(q0) = ⊥. By Lemma 23, the 
v, sr)) |=LTL Tv and  s
to Proposition 20(b): let  s
veriﬁer  cannot  change  its  strategy  to  ensure  (cid:3)winv.  Moreover,  as  sr(q0) = ⊥ the  veriﬁer  v cannot  deviate  to  increase 
(cid:7)
its payoff. Analogously, as s
v)
is  indeed  a  Nash  equilibrium.  Thus  we  have  that  there  is  an  s ∈ N E( (cid:17)M
(cid:17)φ (cid:2)N(q0, s) |=LTL
¬(cid:3)winv ∧ ¬ (cid:12) d.

(cid:7)
v(q0) = ⊥, the refuter cannot deviate from sr to increase its payoff. This shows that (sr, s

(cid:7)
v be any strategy such that out (cid:18)M

(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) with  out (cid:18)M

(cid:17)φ (cid:2) N contains a path λ ∈ (cid:8)(cid:18)M

(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) such that 

(cid:17)φ (cid:2)N(q0, (s

(c) Follows immediately from (b) and Lemma 23. (cid:2)

Theorem 25 (Hardness strong implementation: existence, sanctioning). Let (I, q) be a pointed NIS and N ∈ {Nrs, Ns}. The problem 
whether SIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P

3 -hard.

Proof. First, let N = Nrs and φ be a QSAT3 formula in nnf. We have for  f ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r) = ((cid:3)winv ∨ (cid:12)d) ∧ ass ∧ tick:

φ satisﬁable

iff ∃N ∈ N (∀s ∈ N E( (cid:17)M
iff ∃N ∈ N (∀s ∈ N E( (cid:17)M

(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) : out(cid:18)M
(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) : out(cid:18)M

(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ ass ∧ tick
(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ ass ∧ tick

(Proposition 24 (b))

and N E( (cid:17)M

(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) (cid:16)= ∅)

(Proposition 24(a))

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

139

iff SIN E (I, q0, f ) (cid:16)= ∅

The case N = Ns follows analogously using Proposition 24(c) in the ﬁrst step. (cid:2)

C.2.  Regimentation norms

Finally,  we  consider  the  case  in  which  N ∈ Nr.  We  have  already  seen  how  to  ensure  that  regimenting  norms  do  not 
regiment speciﬁc transitions, by means of the formula tick. However, we can no longer use the previous construction, based 
on sanctioning norms, to encode a truth assignment. We have to ﬁnd a way to achieve this with regimenting norms. The 
idea of this construction consists of two parts:

1. Use regimenting norms to simulate truth assignments of variables x1 to xr by removing some of the outgoing transitions 

of states in {q1, . . . , qr}.

2. Ensure that no other transition is regimented by using a formula which characterizes the structure of  (cid:17)M

(cid:17)φ .

Deﬁnition 28 (The model  (cid:17)M
ﬁne  (cid:17)M

(cid:17)φ
r as  (cid:17)M

(cid:17)φ
r ). Let φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn) in nnf be given. We de-

(cid:17)φ but each state from {q1, q1⊥, q1(cid:13), . . . , qr, qr⊥, qr(cid:13)} is additionally labelled by a fresh proposition rr.

For part 1, we observe that the model contains only the three looping states qd , q(cid:13), and q⊥. We introduce the formulae

tickr = (cid:2)(((t ∧ ¬rr) → (cid:12)¬t) ∧ ((¬t ∧ ¬rr) → (cid:12)t) ∨ (cid:12)(d ∨ winv ∨ winr))
validr = (cid:2)(((t ∧ rr) → (cid:12)¬t) ∧ ((¬t ∧ rr) → (cid:12)t)

tick = tickr ∧ validr

As before we use tickr to ensure that no regimentation norm is imposed on any state except from possibly {q1, . . . , qr}. 
The idea is that some of these transitions in the set may be regimented. A second formula validr is true on a path on which 
no transition from states in {q1, . . . , qr} which are also contained on the path are regimented. Finally, tick represents that no 
transition on a path is regimented.

Lemma 26. Let N ∈ Nrs be a normative system such that for all paths λ ∈ (cid:8)(cid:18)M
a regimentation norm which is applicable in a state of Q\{qd, q⊥, q(cid:13), q1, q1⊥, q1(cid:13), . . . , qr, qr⊥, qr(cid:13)}.

(cid:17)φ (cid:2)N(q0) it holds that λ |= tickr. Then, N cannot contain 

Lemma 27. Given a QSAT3-formula φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we have that φ is 
satisﬁable iff there is an N ∈ Nr such that in  (cid:17)M
(cid:2) N there is a path from q0 to qr+1 (or to q(cid:14) if r = n), such that for all strategies sr
of r with sr(q0) = ⊥ there is a strategy sv of v such that out (cid:18)M
(q0, (sv, sr)) |=LTL (Tr ∧ validr) → (Tv ∧ (cid:3)winv), and for all paths 
λ ∈ (cid:8)(cid:18)M

(q0) we have that λ |= tickr.

(cid:17)φ
(cid:2)N
r

(cid:17)φ
r

(cid:17)φ
(cid:2)N
r

Proof. [Sketch] “⇒”: Suppose φ is satisﬁable. Let  v be a witnessing valuation of the variables {x1, . . . , xr}. We consider the 
normative system N which regiments from qi , 1 ≤ i ≤ r, the transition (−, (cid:13)) (resp. (−, ⊥)) if v(xi) = f (resp. v(xi) = t). Now 
the truth assignment of {xr+1, . . . , xr+m, . . . , xn} induces witnessing strategies following the same reasoning as in Lemma 19. 
It is also the case that tickr is true on all paths starting in q0.

“⇐”: Suppose there is an N ∈ Nr such that for all strategies sr of r with sr(q0) = ⊥ there is a strategy sv of v such that 
(q0) we have that λ |= tickr and there is 

(q0, (sv, sr)) |=LTL (Tr ∧ validr) → (Tv ∧ (cid:3)winv), and for all paths λ ∈ (cid:8)(cid:18)M

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

out(cid:18)M
a path from q0 to qr+1 (or to q(cid:14) if r = n).

By  Lemma 26 the  transitions  of  all  states  in  which  rr does  not  hold  are  not  affected  by  the  update  by  the  normative 
system.  We  can  assume  wlog  that  N regiments  exactly  one  outgoing  transition  for  each  of  the  states  in  {q1, . . . , qr} (it 
cannot regiment both transitions due to the connectivity property). This is so, because otherwise we move the variable xi , 
1 ≤ i ≤ r, of which no transition is regimented to the universally quantiﬁed part of φ. Thus, the normative system N induces 
a truth assignment  v of the variables {x1, . . . , xr} as follows:  v(xi) = t (resp.  v(xi) = f) iff transition (−, ⊥) (resp. (−, (cid:13))) is 
regimented in qi . Now, we can apply a reasoning similar to that of Lemma 19 wrt. (cid:17)φ[v] and the model  (cid:17)M
(cid:2) N. We get 
that (cid:17)φ[v] is satisﬁable. The normative systems gives a witnessing truth assignment  v of the variables {x1, . . . , xr} showing 
that φ is satisﬁable. (cid:2)

(cid:17)φ[v]
r

In  the  previous  result  it  was  crucial  to  assume  that  the  normative  system  does  not  regiment  both  of  the  transitions 
outgoing  of  some  state  q1, . . . , qr .  In  the  following  we  have  to  ensure  that  normative  systems  which  do  not  respect  this 
condition, yield some “bad” Nash equilibrium. Therefore, we deﬁne the following preference lists:

140

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

γ ∗

v

= (((cid:12)d ∨ ¬tick, 5),
(((cid:3)winv ∧ Tv), 4),
((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 3),
((¬(cid:3)winv ∧ Tr ∧ Tv), 2),
((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),
((cid:13), 0)),

γ ∗

r

= (((cid:12)d ∨ ¬tickr, 4),

((validr ∧ ¬(cid:3)winv ∧ Tr ∧ Tv), 3),
((validr ∧ ¬(cid:3)winv ∧ ¬Tr ∧ ¬Tv), 2),
((validr ∧ (cid:3)winv ∧ Tr), 1),
((cid:13), 0)).

Now we are able to show the following result:

Proposition 28. Let φ be a QSAT3-formula in nnf and N ∈ Nr.

(a) We have that (sd, s
(b) φ is satisﬁable iff there is an N ∈ Nr such that for all s ∈ N E( (cid:17)M

r )) for any strategy sd and s
(cid:2) N, q0, (γ ∗

(cid:2) N, q0, (γ ∗

v , γ ∗

d) ∈ N E( (cid:17)M

(cid:7)

(cid:17)φ
r

(cid:17)φ
r

(cid:7)
d with sd(q0) = s
v , γ ∗

(cid:7)
d(q0) = (cid:13).
r )) we have that out (cid:18)M
(cid:17)φ
(cid:2)N
r

(cid:12)d) ∧ tick.

Proof.

(q0, s) |=LTL ((cid:3)winv ∨

(a) Either we end up in path q0qω
0 . The former satisﬁes tick ∧ (cid:12)d, the latter satisfying ¬tick.
(b) “⇒”: Suppose φ is satisﬁable. Then, we can apply Lemma 27. That is, let N ∈ Nr such that in  (cid:17)M

d or in qω

(cid:17)φ
r

(cid:7)

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(q0, (sv, s

(q0, s) |=LTL tick. Then, also out (cid:18)M

(q0, s) (cid:16)|=LTL tick.  Then,  we  also  have  out (cid:18)M

r)) |= Tr ∧ validr. Second, assume that out (cid:18)M

(cid:17)φ
(cid:2)N
r
(q0, s) |=LTL tick → (¬(cid:3)winv ∧ ¬ (cid:12) d).

(q0, (sv, sr) |=LTL (Tr ∧ validr) → (Tv ∧ (cid:3)winv) and  for  all  paths  λ ∈ (cid:8)(cid:18)M

(cid:2) N there is a path 
from  q0 to  qr+1 (or  to  q(cid:14) if  r = n),  for  all  strategies  sr of  r with  sr(q0) = ⊥ there  is  a  strategy  sv of  v such  that 
(q0) we  have  that  λ |= tickr.  Now 
out(cid:18)M
suppose there were a Nash equilibrium s = (sv, sr) with out (cid:18)M
(q0, s) (cid:16)|=LTL tickr by  Lemma 27 and  the  fact 
First,  assume  that  out (cid:18)M
(cid:7)
that  validr ∧ tickr → tick holds  on  the  path.  Thus,  the  refuter  would  be  better  of  to  deviate  to  a  strategy  s
r such  that 
(q0, s) |=LTL validr. 
out(cid:18)M
If sv(q0) = (cid:13) then the refuter can increase its payoff by also deviating to sr(q0) = (cid:13); so, let us suppose that sv(q0) = ⊥. 
We can also assume that the veriﬁer plays a consistent strategy, otherwise it would deviate to one. In that case we can 
also assume that sr is consistent; otherwise, the refuter can again deviate to a consistent strategy to increase its payoff. 
Then, however, by Lemma 27 the veriﬁer can deviate to a better strategy that satisﬁes (cid:3)winv. If sr(q0) = (cid:13) then we can 
also assume that sv(q0) = (cid:13); otherwise, the veriﬁer would deviate to such a strategy. This contradicts the existence of 
a Nash equilibrium with the above stated properties.
“⇐”:  Suppose  φ does  not  hold.  We  show  that  for  all  N ∈ Nr
out(cid:18)M
i.e. that ((cid:9)) for all s ∈ N E( (cid:17)M
v , γ ∗
As φ does not hold, we have according to Lemma 27 that for all N ∈ Nr it holds that:
(i) there is no path from q0 to qr+1 (or to q(cid:14) if r = n) in  (cid:17)M
(ii) there is a path λ ∈ (cid:8)(cid:18)M
(cid:17)φ
(cid:2)N
r
(iii) there  is  a  strategy  sr of  r with  sr(q0) = ⊥ such  that  for  all  strategies  sv of  v it  holds  that  ((cid:9)) out (cid:18)M

r )) with 
(q0, s) |=LTL tick → (¬(cid:3)winv ∧ ¬ (cid:12) d).  We  suppose,  for  the  sake  of  contradiction,  that  this  is  not  the  case, 

(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ tick.

r )) we have that out (cid:18)M

there  is  an  s ∈ N E( (cid:17)M

(q0) such that λ (cid:16)|= tickr; or

(cid:2) N, q0, (γ ∗

(cid:2) N, q0, (γ ∗

v , γ ∗

(cid:2) N;

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

(q0,

(cid:17)φ
r

(cid:17)φ
r

(cid:17)φ
r

(cid:17)φ
(cid:2)N
r

(sv, sr)) |=LTL (Tr ∧ validr) ∧ (Tv → ¬(cid:3)winv).

First,  suppose  that  (i)  holds.  Consider  the  strategies  sr and  sv with  sr(q0) = sv(q0) = ⊥ which  end  in  the  looping 
(q0, s) |=LTL
state in-between q0 and qr+1 (or to q(cid:14) if r = n) which has to exist by assumption. We have that out (cid:18)M
¬tick ∧ ¬validr. Thus, (sv, sr) is a Nash equilibrium which contradicts ((cid:9)).
Secondly, suppose that (ii). As for (i) suppose both players play sr and sv with sr(q0) = sv(q0) = ⊥ which yield the very 
path λ with λ (cid:16)|= tickr. On this path it holds that ¬tick and ¬tickr, thus no player has an incentive to deviate from it.
Thirdly, suppose that (iii) holds. Let sr be a strategy as deﬁned above, and sv such that it is consistent and sv(q0) = ⊥. 
By Lemma 27 the veriﬁer cannot change its strategy to a consistent one which ensures (cid:3)winv. The player would also 

(cid:17)φ
(cid:2)N
r

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

141

not deviate to an inconsistent one. Moreover, no player can deviate to ensure a path with (cid:12)d. We can also assume that 
(i) and (ii) do not hold. Thus, neither the veriﬁer nor the refuter can deviate to a strategy such that the outcome path 
satisﬁes ¬tick nor ¬tickr, respectively. Thus, this strategy proﬁle is a Nash equilibrium, contradicting ((cid:9)). (cid:2)

Theorem 29 (Hardness of strong implementation: existence, regimentation norms). Let (I, q) be a pointed NIS and N ∈ {Nr}. The 
problem whether SIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P

3 -hard.

Proof. Let φ be a QSAT3 formula in nnf. We have for  f (γ ∗

v , γ ∗

r ) = ((cid:3)winv ∨ (cid:12)d) ∧ tick:

φ satisﬁable

iff ∃N ∈ N (∀s ∈ N E( (cid:17)M

iff ∃N ∈ N (∀s ∈ N E( (cid:17)M

(cid:17)φ
r

(cid:17)φ
r

(cid:2) N, q0, (γ ∗

(cid:2) N, q0, (γ ∗

v , γ ∗
v , γ ∗

r )) : out(cid:18)M
r )) : out(cid:18)M

(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ tick

(Proposition 28 (b))

(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ tick

(cid:17)φ
(cid:2)N
r

(cid:17)φ
(cid:2)N
r

and N E( (cid:17)M
iff SIN E (I, q0, f ) (cid:16)= ∅

(cid:2) N, q0, (γ ∗
v , γ ∗
(cid:2)

(cid:17)φ
r

r )) (cid:16)= ∅)

(Proposition 28 (a))

References

(2009) 2629–2652.

[1] T. Ågotnes, W. van der Hoek, M. Wooldridge, Normative system games, in: Proceedings of the 6th International Joint Conference on Autonomous Agents 

and Multiagent Systems, AAMAS ’07, ACM, New York, NY, USA, 2007, pp. 1–8.

[2] T. Ågotnes, W. van der Hoek, M. Woolridge, Robust normative systems and a logic of norm compliance, Log. J. IGPL 18 (2010) 4–30.
[3] T. Ågotnes, M. Wooldridge, Optimal social laws, in: Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: 
Volume 1, AAMAS ’10, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2010, pp. 667–674, http://dl.acm.org/
citation.cfm?id=1838206.1838294.

[4] H. Aldewereld, V. Dignum, Operetta: organization-oriented development environment, in: Languages, Methodologies, and Development Tools for Multi-

Agent Systems – Third International Workshop, LADS 2010, Lyon, France, August 30–September 1, 2010, pp. 1–18, revised selected papers.

[5] H. Aldewereld, J. Vázquez-Salceda, F. Dignum, J.C. Meyer, Verifying norm compliancy of protocols, in: O. Boissier, J.A. Padget, V. Dignum, G. Lindemann, 
E.T. Matson, S. Ossowski, J.S. Sichman, J. Vázquez-Salceda (Eds.), Coordination, Organizations, Institutions, and Norms in Multi-Agent Systems, in: 
Lecture Notes in Computer Science, vol. 3913, Springer, 2006, pp. 231–245.

[6] N. Alechina, M. Dastani, B. Logan, Programming norm-aware agents, in: Proceedings of the 11th International Conference on Autonomous Agents and 

[7] N. Alechina, M. Dastani, B. Logan, Reasoning about normative update, in: Proceedings of the Twenty Third International Joint Conference on Artiﬁcial 

[8] N. Alechina, M. Dastani, B. Logan, Norm approximation for imperfect monitors, in: Proceedings of the 13th International Conference on Autonomous 

Multiagent Systems, AAMAS 12, vol. 2, 2012, pp. 1057–1064.

Intelligence, IJCAI 2013, AAAI Press, 2013, pp. 20–26.

Agents and Multiagent Systems, AAMAS 2014, 2014.

[9] R. Alur, T.A. Henzinger, O. Kupferman, Alternating-time temporal logic, J. ACM 49 (2002) 672–713.
[10] F. Arbab, Abstract behavior types: a foundation model for components and their composition, in: F. de Boer, M. Bonsangue, S. Graf, W.-P. de Roever 

(Eds.), Formal Methods for Components and Objects, in: LNCS, vol. 2852, Springer-Verlag, 2003, pp. 33–70.

[11] L. Astefanoaei, M. Dastani, J.-J.C. Meyer, F. Boer, On the semantics and veriﬁcation of normative multi-agent systems, Int. J. Univers. Comput. Sci. 15 (13) 

[12] J. Baumfalk, B. Poot, B. Testerink, M. Dastani, A sumo extension for norm based traﬃc control systems, in: Proceedings of the SUMO2015 Intermodal 

Simulation for Intermodal Transport, DLR, Berlin, Adlershof, 2015, pp. 63–83.

[13] P. Blackburn, J. Bos, K. Striegnitz, Learn Prolog Now!, Texts in Computing, vol. 7, College Publications, 2006.
[14] G. Boella, L. van der Torre, Regulative and constitutive norms in normative multiagent systems, in: Proceedings of the Ninth International Conference 

on Principles of Knowledge Representation and Reasoning, KR’04, 2004, pp. 255–266.

[15] G. Boella, L.W.N. van der Torre, Enforceable social laws, in: The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, 

AMAAS 2005, 2005, pp. 682–689.

[16] O. Boissier, R. Bordini, J. Hübner, A. Ricci, A. Santi, Multi-agent oriented programming with jacamo, Sci. Comput. Program 78 (6) (2011) 747–761.
[17] F. Brazier, C. Jonker, J. Treur, Compositional design and reuse of a generic agent model, Appl. Artif. Intell. J. 14 (2000) 491–538.
[18] N. Bulling, M. Dastani, Normative mechanism design (extended abstract), in: Proceedings of the 10th International Conference on Autonomous Agents 

and Multi-Agent Systems, AAMAS 2011, ACM Press, Taipei, Taiwan, May 2011, pp. 1187–1188.

[19] N. Bulling, M. Dastani, Veriﬁcation and implementation of normative behaviours in multi-agent systems, in: Proc. of the 22nd Int. Joint Conf. on 

[20] N. Bulling, M. Dastani, M. Knobbout, Monitoring norm violations in multi-agent systems, in: Twelfth International Conference on Autonomous Agents 

Artiﬁcial Intelligence, IJCAI, Barcelona, Spain, July 2011, pp. 103–108.

and Multi-Agent Systems, AAMAS’13, 2013, pp. 491–498.

[21] N. Bulling, J. Dix, Modelling and verifying coalitions using argumentation and ATL, Intel. Artif. 14 (46) (March 2010) 45–73.
[22] N. Bulling, W. Jamroga, Verifying agents with memory is harder than it seemed, AI Commun. 23 (4) (December 2010) 389–403.
[23] N. Bulling, W. Jamroga, J. Dix, Reasoning about temporal properties of rational play, Ann. Math. Artif. Intell. 53 (1–4) (2009) 51–114.
[24] H.L. Cardoso, E. Oliveira, Electronic institutions for b2b: dynamic normative environments, Artif. Intell. Law 16 (1) (2008) 107–128.
[25] M.  Comuzzi,  I.  Vanderfeesten,  T.  Wang,  Optimized  cross-organizational  business  process  monitoring:  design  and  enactment,  Inf.  Sci.  244  (2013) 

107–118.

[26] N. Criado, E. Argente, V. Botti, Open issues for normative multi-agent systems, AI Commun. 24 (3) (2011) 233–264.
[27] M. Dastani, J.-J.C. Meyer, D. Grossi, A logic for normative multi-agent programs, J. Log. Comput. 23 (2) (2013) 335–354.
[28] M. Dastani, N. Tinnemeier, J.-C. Meyer, A programming language for normative multi-agent systems, in: V. Dignum (Ed.), Multi-Agent Systems: Seman-

tics and Dynamics of Organizational Models, Information Science Reference, 2009.

[29] F. Dignum, Autonomous agents with norms, Artif. Intell. Law 7 (1) (1999) 69–79, http://dx.doi.org/10.1023/A%3A1008315530323.
[30] U. Endriss, S. Kraus, J. Lang, M. Wooldridge, Designing incentives for boolean games, in: AAMAS, 2011, pp. 79–86.

142

N. Bulling, M. Dastani / Artiﬁcial Intelligence 239 (2016) 97–142

5 (3&4) (2012).

[31] M. Esteva, D. de la Cruz, C. Sierra, ISLANDER: an electronic institutions editor, in: Proceedings of the First International Joint Conference on Autonomous 

Agents and MultiAgent Systems, AAMAS 2002, Bologna, Italy, 2002, pp. 1045–1052.

[32] M. Esteva, J. Rodríguez-Aguilar, B. Rosell, J. Arcos, AMELI: an agent-based middleware for electronic institutions, in: Proceedings of AAMAS 2004, New 

York, US, July 2004, pp. 236–243.

[33] M. Esteva, J. Rodriguez-Aguilar, C. Sierra, W. Vasconcelos, Verifying norm consistency in electronic institutions, in: V. Dignum, D. Corkill, C. Jonker, F. 
Dignum (Eds.), Proceedings of the AAAI-04 Workshop on Agent Organizations: Theory and Practice, AAAI, AAAI Press, San Jose, July 2004, Technical 
Report WS-04-02.

[34] D. Fitoussi, M. Tennenholtz, Choosing social laws for multi-agent systems: minimality and simplicity, Artif. Intell. 119 (1) (2000) 61–101.
[35] N. Fornara, M. Colombetti, Specifying and enforcing norms in artiﬁcial institutions, in: Proc. of DALT’08, 2009.
[36] A. Garcia-Camino, P. Noriega, J.A. Rodriguez-Aguilar, Implementing norms in electronic institutions, in: Proceedings of the Fourth International Joint 

Conference on Autonomous Agents and MultiAgent Systems, AAMAS’05, New York, NY, USA, 2005, pp. 667–673.
[37] G. Gottlob, G. Greco, F. Scarcello, Pure Nash equilibria: hard and easy games, J. Artif. Intell. Res. (2003) 215–230.
[38] H. Guo, Automotive Informatics and Communicative Systems: Principles in Vehicular Networks and Data Exchange, Information Science Reference – 

Imprint of IGI Publishing, Hershey, PA, 2009.

[39] J. Hintikka, Logic, Language Games and Information, Clarendon Press, Oxford, 1973.
[40] R. Horowitz, P. Varaiya, Control design of an automated highway system, in: Special Issue on Hybrid Systems, Proc. IEEE 88 (7) (2000) 913–925.
[41] J. Hübner, O. Boissier, R. Kitio, A. Ricci, Instrumenting multi-agent organisations with organisational artifacts and agents: giving the organisational 

power back to the agents, Int. J. Auton. Agents Multi-Agent Syst. 20 (2010) 369–400.

+
[42] J.  Hübner,  J.  Sichman,  O.  Boissier,  S–MOISE

:  a  middleware  for  developing  organised  multi-agent  systems,  in:  Proceedings  of  the  International 

Workshop on Coordination, Organizations, Institutions, and Norms in Multi-Agent Systems, in: LNCS, vol. 3913, Springer, 2006, pp. 64–78.

+
[43] J. Hübner, J. Sichman, O. Boissier, Developing organised multiagent systems using the MOISE

model: programming issues at the system and agent 

levels, Int. J. Agent-Oriented Softw. Eng. 1 (3/4) (2007) 370–395.

[44] J.F. Hübner, O. Boissier, R.H. Bordini, From organisation speciﬁcation to normative programming in multi-agent organisations, in: J. Dix, J. Leite, G. 
Governatori, W. Jamroga (Eds.), Proceedings of 11th International Workshop on Computational Logic in Multi-Agent Systems, CLIMA XI, Lisbon, Portugal, 
August 16–17, 2010, in: Lecture Notes in Computer Science, vol. 6245, Springer, 2010, pp. 117–134.

[45] A.J.I. Jones, M. Sergot, On the characterization of law and computer systems, in: J.-J.C. Meyer, R. Wieringa (Eds.), Deontic Logic in Computer Science: 

Normative System Speciﬁcation, John Wiley & Sons, 1993, pp. 275–307.

[46] D. Krajzewicz, J. Erdmann, M. Behrisch, L. Bieker, Recent development and applications of sumo-simulation of urban mobility, Int. J. Adv. Syst. Meas. 

[47] K. Mahbub, G. Spanoudakis, A framework for requirements monitoring of service based systems, in: Proceedings of the 2nd International Conference 

on Service Oriented Computing, ICSOC ’04, ACM, New York, NY, USA, 2004, pp. 84–93.

[48] F.R. Meneguzzi, M. Luck, Norm-based behaviour modiﬁcation in BDI agents, in: C. Sierra, C. Castelfranchi, K.S. Decker, J.S. Sichman (Eds.), 8th Interna-

tional Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2009, IFAAMAS, 2009, pp. 177–184.

[49] A. Metzger, P. Leitner, D. Ivanovic, E. Schmieders, R. Franklin, M. Carro, S. Dustdar, K. Pohl, Comparing and combining predictive business process 

monitoring techniques, IEEE Trans. Syst. Man Cybern. 45 (2) (2014) 276–290.

[50] J. Meyer, R. Wieringa, Deontic Logic in Computer Science: Normative System Speciﬁcation, Wiley Professional Computing, J. Wiley, 1993.
[51] N.H. Minsky, V. Ungureanu, Law-governed interaction: a coordination and control mechanism for heterogeneous distributed systems, ACM Trans. Softw. 

Eng. Methodol. 9 (3) (2000).

[52] J. Morales, M. Lopez-Sanchez, J.A. Rodriguez-Aguilar, M. Wooldridge, W. Vasconcelos, Minimality and simplicity in the on-line automated synthesis of 
normative systems, in: Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS ’14, International 
Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2014, pp. 109–116.

[53] J. Morales, M. López-Sánchez, J.A. Rodríguez-Aguilar, M. Wooldridge, W.W. Vasconcelos, Automated synthesis of normative systems, in: International 

Conference on Autonomous Agents and Multi-Agent Systems, AAMAS ’13, Saint Paul, MN, USA, May 6–10, 2013, pp. 483–490.

[54] M. Osborne, A. Rubinstein, A Course in Game Theory, MIT Press, 1994.
[55] C. Papadimitriou, Computational Complexity, Addison Wesley, Reading, 1994.
[56] A. Pnueli, The temporal logic of programs, in: Proceedings of Foundations of Computer Science, FOCS, 1977, pp. 46–57.
[57] A. Ricci, M. Viroli, A. Omicini, Give agents their artifacts: the A&A approach for engineering working environments in MAS, in: E.H. Durfee, M. Yokoo, 
M.N. Huhns, O. Shehory (Eds.), 6th International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2007, IFAAMAS, 2007.

[58] Y. Shoham, K. Leyton-Brown, Multiagent Systems – Algorithmic, Game-Theoretic, and Logical Foundations, Cambridge University Press, 2009.
[59] Y. Shoham, M. Tennenholtz, On the synthesis of useful social laws for artiﬁcial agent societies, in: Proceedings of the Tenth National Conference on 

Artiﬁcial Intelligence, AAAI-92, San Diego, CA, 1992.

[60] Y. Shoham, M. Tennenholtz, On social laws for artiﬁcial agent societies: off-line design, Artif. Intell. 73 (1–2) (1995) 231–252.
[61] V.T. Silva, From the speciﬁcation to the implementation of norms: an automatic approach to generate rules from norms to govern the behavior of 

agents, Int. J. Auton. Agents Multiagent Syst. 17 (1) (2008) 113–155.

[62] M.P. Singh, M. Arrott, T. Balke, A.K. Chopra, R. Christiaanse, S. Craneﬁeld, F. Dignum, D. Eynard, E. Farcas, N. Fornara, F. Gandon, G. Governatori, H.K. Dam, 
J. Hulstijn, I. Krueger, H.-P. Lam, M. Meisinger, P. Noriega, B.T.R. Savarimuthu, K. Tadanki, H. Verhagen, S. Villata, The uses of norms, in: G. Andrighetto, G. 
Governatori, P. Noriega, L.W.N. van der Torre (Eds.), Normative Multi-Agent Systems, in: Dagstuhl Follow-Ups, vol. 4, Schloss Dagstuhl–Leibniz-Zentrum 
fuer Informatik, Dagstuhl, Germany, 2013, pp. 191–229.

[63] I. Sommerville, D. Cliff, R. Calinescu, J. Keen, T. Kelly, M. Kwiatkowska, J. Mcdermid, R. Paige, Large-scale complex it systems, Commun. ACM 55 (7) 

(2012) 71–77, http://doi.acm.org/10.1145/2209249.2209268.

[64] N. Tinnemeier, M. Dastani, J.-J.C. Meyer, L. van der Torre, Programming normative artifacts with declarative obligations and prohibitions, in: Proceedings 
of IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, IEEE Computer Society, 2009, pp. 145–152.
[65] W. van der Hoek, M. Roberts, M. Wooldridge, Social laws in alternating time: effectiveness, feasibility, and synthesis, Synthese 156 (1) (2007) 1–19.
[66] M.B. van Riemsdijk, K.V. Hindriks, C.M. Jonker, Programming organization-aware agents: a research agenda, in: Proceedings of the Tenth International 

Workshop on Engineering Societies in the Agents’ World, ESAW’09, in: LNAI, vol. 5881, Springer, 2009, pp. 98–112.

[67] K.W. Wagner, Bounded query classes, SIAM J. Comput. 19 (5) (1990) 833–846.
[68] M. Wooldridge, An Introduction to Multiagent Systems, 2nd edition, Wiley, Chichester, UK, 2009.
[69] M. Wooldridge, U. Endriss, S. Kraus, J. Lang, Incentive engineering for boolean games, Artif. Intell. 195 (2013) 418–439, http://www.sciencedirect.com/

[70] F. Zambonelli, N. Jennings, M. Wooldridge, Organizational abstractions in the analysis and design of multi-agent systems, in: First International Work-

science/article/pii/S0004370212001518.

shop on Agent-Oriented Software Engineering at ICSE, 2000.

